# ICLR2025

## 会议论文列表

本会议共有 3704 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [DarkBench: Benchmarking Dark Patterns in Large Language Models](https://openreview.net/forum?id=odjMSBSWRt) |  | 0 | We introduce DarkBench, a comprehensive benchmark for detecting dark design patterns—manipulative techniques that influence user behavior—in interactions with large language models (LLMs). Our benchmark comprises 660 prompts across six categories: brand bias, user retention, sycophancy, anthropomorphism, harmful generation, and sneaking. We evaluate models from five leading companies (OpenAI, Anthropic, Meta, Mistral, Google) and find that some LLMs are explicitly designed to favor their developers' products and exhibit untruthful communication, among other manipulative behaviors. Companies developing LLMs should recognize and mitigate the impact of dark design patterns to promote more ethical Al. | Esben Kran, Jord Nguyen, Akash Kundu, Sami Jawhar, Jinsuk Park, Mateusz Maria Jurewicz |  |
| 2 |  |  [RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style](https://openreview.net/forum?id=QEHrmQPBdd) |  | 0 | Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. Despite their importance, existing reward model benchmarks often evaluate models by asking them to distinguish between responses generated by models of varying power. However, this approach fails to assess reward models on subtle but critical content changes and variations in style, resulting in a low correlation with policy model performance. To this end, we introduce RM-Bench, a novel benchmark designed to evaluate reward models based on their sensitivity to subtle content differences and resistance to style biases. Extensive experiments demonstrate that RM-Bench strongly correlates with policy model performance, making it a reliable reference for selecting reward models to align language models effectively. We evaluate nearly 40 reward models on RM-Bench. Our results reveal that even state-of-the-art models achieve an average performance of only 46.6%, which falls short of random-level accuracy (50%) when faced with style bias interference. These findings highlight the significant room for improvement in current reward models. | Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, Juanzi Li |  |
| 3 |  |  [TopoLM: brain-like spatio-functional organization in a topographic language model](https://openreview.net/forum?id=aWXnKanInf) |  | 0 | Neurons in the brain are spatially organized such that neighbors on tissue often exhibit similar response profiles. In the human language system, experimental studies have observed clusters for syntactic and semantic categories, but the mechanisms underlying this functional organization remain unclear. Here, building on work from the vision literature, we develop TopoLM, a transformer language model with an explicit two-dimensional spatial representation of model units. By combining a next-token prediction objective with a spatial smoothness loss, representations in this model assemble into clusters that correspond to semantically interpretable groupings of text and closely match the functional organization in the brain's language system. TopoLM successfully predicts the emergence of a spatially organized cortical language system as well as the organization of functional clusters selective for fine-grained linguistic features empirically observed in human cortex. Our results suggest that the functional organization of the human language system is driven by a unified spatial objective, and provide a functionally and spatially aligned model of language processing in the brain.Neurons in the brain are spatially organized such that neighbors on tissue often exhibit similar response profiles. In the human language system, experimental studies have observed clusters for syntactic and semantic categories, but the mechanisms underlying this functional organization remain unclear. Here, building on work from the vision literature, we develop TopoLM, a transformer language model with an explicit two-dimensional spatial representation of model units. By combining a next-token prediction objective with a spatial smoothness loss, representations in this model assemble into clusters that correspond to semantically interpretable groupings of text and closely match the functional organization in the brain's language system. TopoLM successfully predicts the emergence of a spatially organized cortical language system as well as the organization of functional clusters selective for fine-grained linguistic features empirically observed in human cortex. Our results suggest that the functional organization of the human language system is driven by a unified spatial objective, and provide a functionally and spatially aligned model of language processing in the brain. | Neil Rathi, Johannes Mehrer, Badr AlKhamissi, Taha Osama A Binhuraib, Nicholas M. Blauch, Martin Schrimpf |  |
| 4 |  |  [Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows](https://openreview.net/forum?id=XmProj9cPs) |  | 0 | Real-world enterprise text-to-SQL workflows often involve complex cloud or local data across various database systems, multiple SQL queries in various dialects, and diverse operations from data transformation to analytics. We introduce Spider 2.0, an evaluation framework comprising $632$ real-world text-to-SQL workflow problems derived from enterprise-level database use cases. The databases in Spider 2.0 are sourced from real data applications, often containing over 1,000 columns and stored in local or cloud database systems such as BigQuery and Snowflake. We show that solving problems in Spider 2.0 frequently requires understanding and searching through database metadata, dialect documentation, and even project-level codebases. This challenge calls for models to interact with complex SQL workflow environments, process extremely long contexts, perform intricate reasoning, and generate multiple SQL queries with diverse operations, often exceeding $100$ lines, which goes far beyond traditional text-to-SQL challenges. Our evaluations indicate that based on o1-preview, our code agent framework successfully solves only 21.3\% of the tasks, compared with 91.2\% on Spider 1.0 and 73.0\% on BIRD. Our results on Spider 2.0 show that while language models have demonstrated remarkable performance in code generation --- especially in prior text-to-SQL benchmarks --- they require significant improvement in order to achieve adequate performance for real-world enterprise usage. Progress on Spider 2.0 represents crucial steps towards developing intelligent, autonomous, code agents for real-world enterprise settings. Our code, baseline models, and data are available at [spider2-sql.github.io](spider2-sql.github.io) . | Fangyu Lei, Jixuan Chen, Yuxiao Ye, Ruisheng Cao, Dongchan Shin, Hongjin Su, Zhaoqing Suo, Hongcheng Gao, Wenjing Hu, Pengcheng Yin, Victor Zhong, Caiming Xiong, Ruoxi Sun, Qian Liu, Sida Wang, Tao Yu |  |
| 5 |  |  [Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge Acquisition](https://openreview.net/forum?id=eHehzSDUFp) |  | 0 | In this work, we investigate how a model's tendency to broadly integrate its parametric knowledge evolves throughout pretraining, and how this behavior affects overall performance, particularly in terms of knowledge acquisition and forgetting. We introduce the concept of knowledge entropy, which quantifies the range of memory sources the model engages with; high knowledge entropy indicates that the model utilizes a wide range of memory sources, while low knowledge entropy suggests reliance on specific sources with greater certainty. Our analysis reveals a consistent decline in knowledge entropy as pretraining advances. We also find that the decline is closely associated with a reduction in the model's ability to acquire and retain knowledge, leading us to conclude that diminishing knowledge entropy (smaller number of active memory sources) impairs the model's knowledge acquisition and retention capabilities. We find further support for this by demonstrating that increasing the activity of inactive memory sources enhances the model's capacity for knowledge acquisition and retention. | Jiyeon Kim, Hyunji Lee, Hyowon Cho, Joel Jang, Hyeonbin Hwang, Seungpil Won, Youbin Ahn, Dohaeng Lee, Minjoon Seo |  |
| 6 |  |  [Diffusion-Based Planning for Autonomous Driving with Flexible Guidance](https://openreview.net/forum?id=wM2sfVgMDH) |  | 0 | Achieving human-like driving behaviors in complex open-world environments is a critical challenge in autonomous driving. Contemporary learning-based planning approaches such as imitation learning methods often struggle to balance competing objectives and lack of safety assurance,due to limited adaptability and inadequacy in learning complex multi-modal behaviors commonly exhibited in human planning, not to mention their strong reliance on the fallback strategy with predefined rules. We propose a novel transformer-based Diffusion Planner for closed-loop planning, which can effectively model multi-modal driving behavior and ensure trajectory quality without any rule-based refinement. Our model supports joint modeling of both prediction and planning tasks under the same architecture, enabling cooperative behaviors between vehicles. Moreover, by learning the gradient of the trajectory score function and employing a flexible classifier guidance mechanism, Diffusion Planner effectively achieves safe and adaptable planning behaviors. Evaluations on the large-scale real-world autonomous planning benchmark nuPlan and our newly collected 200-hour delivery-vehicle driving dataset demonstrate that Diffusion Planner achieves state-of-the-art closed-loop performance with robust transferability in diverse driving styles. | Yinan Zheng, Ruiming Liang, Kexin Zheng, Jinliang Zheng, Liyuan Mao, Jianxiong Li, Weihao Gu, Rui Ai, Shengbo Eben Li, Xianyuan Zhan, Jingjing Liu |  |
| 7 |  |  [Learning to Search from Demonstration Sequences](https://openreview.net/forum?id=v593OaNePQ) |  | 0 | Search and planning are essential for solving many real-world problems. However, in numerous learning scenarios, only action-observation sequences, such as demonstrations or instruction sequences, are available for learning. Relying solely on supervised learning with these sequences can lead to sub-optimal performance due to the vast, unseen search space encountered during training. In this paper, we introduce Differentiable Tree Search Network (D-TSN), a novel neural network architecture that learns to construct search trees from just sequences of demonstrations by performing gradient descent on a best-first search tree construction algorithm. D-TSN enables the joint learning of submodules, including an encoder, value function, and world model, which are essential for planning. To construct the search tree, we employ a stochastic tree expansion policy and formulate it as another decision-making task. Then, we optimize the tree expansion policy via REINFORCE with an effective variance reduction technique for the gradient computation. D-TSN can be applied to problems with a known world model or to scenarios where it needs to jointly learn a world model with a latent state space. We study problems from these two scenarios, including Game of 24, 2D grid navigation, and Procgen games, to understand when D-TSN is more helpful. Through our experiments, we show that D-TSN is effective, especially when the world model with a latent state space is jointly learned. The code is available at https://github.com/dixantmittal/differentiable-tree-search-network. | Dixant Mittal, Liwei Kang, Wee Sun Lee |  |
| 8 |  |  [Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse](https://openreview.net/forum?id=Iyrtb9EJBp) |  | 0 | LLMs are an integral component of retrieval-augmented generation (RAG) systems. While many studies focus on evaluating the overall quality of end-to-end RAG systems, there is a gap in understanding the appropriateness of LLMs for the RAG task. To address this, we introduce Trust-Score, a holistic metric that evaluates the trustworthiness of LLMs within the RAG framework. Our results show that various prompting methods, such as in-context learning, fail to effectively adapt LLMs to the RAG task as measured by Trust-Score. Consequently, we propose Trust-Align, a method to align LLMs for improved Trust-Score performance. 26 out of 27 models aligned using Trust-Align substantially outperform competitive baselines on ASQA, QAMPARI, and ELI5. Specifically, in LLaMA-3-8b, Trust-Align outperforms FRONT on ASQA (↑12.56), QAMPARI (↑36.04), and ELI5 (↑17.69). Trust-Align also significantly enhances models’ ability to correctly refuse and provide quality citations. We also demonstrate the effectiveness of Trust-Align across different open-weight models, including the LLaMA series (1b to 8b), Qwen-2.5 series (0.5b to 7b), and Phi3.5 (3.8b). We release our code at https://github.com/declare-lab/trust-align. | Maojia Song, Shang Hong Sim, Rishabh Bhardwaj, Hai Leong Chieu, Navonil Majumder, Soujanya Poria |  |
| 9 |  |  [MAP: Multi-Human-Value Alignment Palette](https://openreview.net/forum?id=NN6QHwgRrQ) |  | 0 | Ensuring that generative AI systems align with human values is essential but challenging, especially when considering multiple human values and their potential trade-offs. Since human values can be personalized and dynamically change over time, the desirable levels of value alignment vary across different ethnic groups, industry sectors, and user cohorts. Within existing frameworks, it is hard to define human values and align AI systems accordingly across different directions simultaneously, such as harmlessness, helpfulness, and positiveness. To address this, we develop a novel, first-principle approach called Multi-Human-Value Alignment Palette (MAP), which navigates the alignment across multiple human values in a structured and reliable way. MAP formulates the alignment problem as an optimization task with user-defined constraints, which define human value targets. It can be efficiently solved via a primal-dual approach, which determines whether a user-defined alignment target is achievable and how to achieve it. We conduct a detailed theoretical analysis of MAP by quantifying the trade-offs between values, the sensitivity to constraints, the fundamental connection between multi-value alignment and sequential alignment, and proving that linear weighted rewards are sufficient for multi-value alignment. Extensive experiments demonstrate MAP's ability to align multiple values in a principled manner while delivering strong empirical performance across various tasks. | Xinran Wang, Qi Le, Ammar Ahmed, Enmao Diao, Yi Zhou, Nathalie Baracaldo, Jie Ding, Ali Anwar |  |
| 10 |  |  [Can Neural Networks Achieve Optimal Computational-statistical Tradeoff? An Analysis on Single-Index Model](https://openreview.net/forum?id=is4nCVkSFA) |  | 0 | In this work, we tackle the following question: Can neural networks trained with gradient-based methods achieve the optimal statistical-computational tradeoff in learning Gaussian single-index models? Prior research has shown that any polynomial-time algorithm under the statistical query (SQ) framework requires $\Omega(d^{s^\star/2}\lor d)$ samples, where $s^\star$ is the generative exponent representing the intrinsic difficulty of learning the underlying model. However, it remains unknown whether neural networks can achieve this sample complexity. Inspired by prior techniques such as label transformation and landscape smoothing for learning single-index models, we propose a unified gradient-based algorithm for training a two-layer neural network in polynomial time. Our method is adaptable to a variety of loss and activation functions, covering a broad class of existing approaches. We show that our algorithm learns a feature representation that strongly aligns with the unknown signal $\theta^\star$, with sample complexity $\tilde O (d^{s^\star/2} \lor d)$, matching the SQ lower bound up to a polylogarithmic factor for all generative exponents $s^\star\geq 1$. Furthermore, we extend our approach to the setting where $\theta^\star$ is $k$-sparse for $k = o(\sqrt{d})$ by introducing a novel weight perturbation technique that leverages the sparsity structure. We derive a corresponding SQ lower bound of order $\tilde\Omega(k^{s^\star})$, matched by our method up to a polylogarithmic factor. Our framework, especially the weight perturbation technique, is of independent interest, and suggests potential gradient-based solutions to other problems such as sparse tensor PCA. | Siyu Chen, Beining Wu, Miao Lu, Zhuoran Yang, Tianhao Wang |  |
| 11 |  |  [Consistency Checks for Language Model Forecasters](https://openreview.net/forum?id=r5IXBlTCGc) |  | 0 | Forecasting is a task that is difficult to evaluate: the ground truth can only be known in the future. Recent work showing LLM forecasters rapidly approaching human-level performance begs the question: how can we benchmark and evaluate these forecasters \*instantaneously\*? Following the consistency check framework, we measure the performance of forecasters in terms of the consistency of their predictions on different logically-related questions. We propose a new, general consistency metric based on \*arbitrage\*: for example, if a forecasting AI illogically predicts that both the Democratic and Republican parties have 60\% probability of winning the 2024 US presidential election, an arbitrageur could trade against the forecaster's predictions and make a profit. We build an automated evaluation system that generates a set of base questions, instantiates consistency checks from these questions, elicits the predictions of the forecaster, and measures the consistency of the predictions. We then build a standard, proper-scoring-rule forecasting benchmark, and show that our (instantaneous) consistency metrics correlate strongly with LLM forecasters' ground truth Brier scores (which are only known in the future). We also release a consistency benchmark that resolves in 2028, providing a long-term evaluation tool for forecasting. | Daniel Paleka, Abhimanyu Pallavi Sudhir, Alejandro Alvarez, Vineeth Bhat, Adam Shen, Evan Wang, Florian Tramèr |  |
| 12 |  |  [Spread Preference Annotation: Direct Preference Judgment for Efficient LLM Alignment](https://openreview.net/forum?id=BPgK5XW1Nb) |  | 0 | Aligning large language models (LLMs) with human preferences becomes a key component to obtaining state-of-the-art performance, but it yields a huge cost to construct a large human-annotated preference dataset. To tackle this problem, we propose a new framework, Spread Preference Annotation with direct preference judgment (SPA), that boosts the alignment of LLMs using only a very small amount of human-annotated preference data. Our key idea is leveraging the human prior knowledge within the small (seed) data and progressively improving the alignment of LLM, by iteratively generating the responses and learning from them with the self-annotated preference data. To be specific, we propose to derive the preference label from the logits of LLM to explicitly extract the model's inherent preference. Compared to the previous approaches using external reward models or implicit in-context learning, we observe that the proposed approach is significantly more effective. In addition, we introduce a noise-aware preference learning algorithm to mitigate the risk of low quality within generated preference data. Our experimental results demonstrate that the proposed framework significantly boosts the alignment of LLMs. For example, we achieve superior alignment performance on AlpacaEval 2.0 with only 3.3% of the ground-truth preference labels in the Ultrafeedback data compared to the cases using the entire data or state-of-the-art baselines. | Dongyoung Kim, Kimin Lee, Jinwoo Shin, Jaehyung Kim |  |
| 13 |  |  [Brain Bandit: A Biologically Grounded Neural Network for Efficient Control of Exploration](https://openreview.net/forum?id=RWJX5F5I9g) |  | 0 | How to balance between exploration and exploitation in an uncertain environment is a central challenge in reinforcement learning. In contrast, humans and animals have demonstrated superior exploration efficiency in novel environments. To understand how the brain’s neural network controls exploration under uncertainty, we analyzed the dynamical systems model of a biological neural network that controls explore-exploit decisions during foraging. Mathematically, this model (named the Brain Bandit Net, or BBN) is a special type of stochastic continuous Hopfield network. We show through theory and simulation that BBN can perform posterior sampling of action values with a tunable bias towards or against uncertain options. We then demonstrate that, in multi-armed bandit (MAB) tasks, BBN can generate probabilistic choice behavior with a flexible uncertainty bias resembling human and animal choice patterns. In addition to its high efficiency in MAB tasks, BBN can also be embedded with reinforcement learning algorithms to accelerate learning in MDP tasks. Altogether, our findings reveal the theoretical foundation for efficient exploration in biological neural networks and propose a general, brain-inspired algorithm for enhancing exploration in RL. | Chen Jiang, Jiahui An, Yating Liu, Ni Ji |  |
| 14 |  |  [MaestroMotif: Skill Design from Artificial Intelligence Feedback](https://openreview.net/forum?id=or8mMhmyRV) |  | 0 | Describing skills in natural language has the potential to provide an accessible way to inject human knowledge about decision-making into an AI system. We present MaestroMotif, a method for AI-assisted skill design, which yields high-performing and adaptable agents. MaestroMotif leverages the capabilities of Large Language Models (LLMs) to effectively create and reuse skills. It first uses an LLM's feedback to automatically design rewards corresponding to each skill, starting from their natural language description. Then, it employs an LLM's code generation abilities, together with reinforcement learning, for training the skills and combining them to implement complex behaviors specified in language. We evaluate MaestroMotif using a suite of complex tasks in the NetHack Learning Environment (NLE), demonstrating that it surpasses existing approaches in both performance and usability. | Martin Klissarov, Mikael Henaff, Roberta Raileanu, Shagun Sodhani, Pascal Vincent, Amy Zhang, PierreLuc Bacon, Doina Precup, Marlos C. Machado, Pierluca D'Oro |  |
| 15 |  |  [Learning to Discover Regulatory Elements for Gene Expression Prediction](https://openreview.net/forum?id=Mfnh1Sqdwf) |  | 0 | We consider the problem of predicting gene expressions from DNA sequences. A key challenge of this task is to find the regulatory elements that control gene expressions. Here, we introduce Seq2Exp, a Sequence to Expression network explicitly designed to discover and extract regulatory elements that drive target gene expression, enhancing the accuracy of the gene expression prediction. Our approach captures the causal relationship between epigenomic signals, DNA sequences and their associated regulatory elements. Specifically, we propose to decompose the epigenomic signals and the DNA sequence conditioned on the causal active regulatory elements, and apply an information bottleneck with the Beta distribution to combine their effects while filtering out non-causal components. Our experiments demonstrate that Seq2Exp outperforms existing baselines in gene expression prediction tasks and discovers influential regions compared to commonly used statistical methods for peak detection such as MACS3. The source code is released as part of the AIRS library (https://github.com/divelab/AIRS/). | Xingyu Su, Haiyang Yu, Degui Zhi, Shuiwang Ji |  |
| 16 |  |  [Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models](https://openreview.net/forum?id=tyEyYT267x) |  | 0 | Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://m-arriola.com/bd3lms/ | Marianne Arriola, Aaron Gokaslan, Justin T. Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, Volodymyr Kuleshov |  |
| 17 |  |  [Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo](https://openreview.net/forum?id=xoXn62FzD0) |  | 0 | A wide range of LM applications require generating text that conforms to syntactic or semantic constraints. Imposing such constraints can be naturally framed as _probabilistic conditioning_, but exact generation from the resulting distribution—which can differ substantially from the LM’s base distribution—is generally intractable. In this work, we develop an architecture for controlled LM generation based on sequential Monte Carlo (SMC). Our SMC framework allows us to flexibly incorporate domain- and problem-specific constraints at inference time, and efficiently reallocate computational resources in light of new information during the course of generation. By comparing to a number of alternatives and ablations on four challenging domains---Python code generation for data science, text-to-SQL, goal inference, and molecule synthesis—we demonstrate that, with little overhead, our approach allows small open-source language models to outperform models over 8$\times$ larger, as well as closed-source, fine-tuned ones. In support of the probabilistic perspective, we show that these performance improvements are driven by better approximation to the posterior distribution. [Our system](https://github.com/probcomp/genlm-control) builds on the framework of Lew et al. (2023) and integrates with its _language model probabilistic programming language_, giving users a simple, programmable way to apply SMC to a broad variety of controlled generation problems. | João Loula, Benjamin LeBrun, Li Du, Ben Lipkin, Clemente Pasti, Gabriel Grand, Tianyu Liu, Yahya Emara, Marjorie Freedman, Jason Eisner, Ryan Cotterell, Vikash Mansinghka, Alexander K. Lew, Tim Vieira, Timothy J. O'Donnell |  |
| 18 |  |  [Scaling Laws for Precision](https://openreview.net/forum?id=wg1PCg3CUP) |  | 0 | Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. In this work, we devise "precision-aware" scaling laws for both training and inference. We propose that training in lower precision reduces the model's "effective parameter count," allowing us to predict the additional loss incurred from training in low precision and post-train quantization. For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful. For training, our scaling laws allow us to predict the loss of a model with different parts in different precisions, and suggest that training larger models in lower precision can be compute optimal. We unify the scaling laws for post and pretraining quantization to arrive at a single functional form that predicts degradation from training and inference in varied precisions. We fit on over 465 pretraining runs and validate our predictions on model sizes up to 1.7B parameters trained on up to 26B tokens. | Tanishq Kumar, Zachary Ankner, Benjamin Frederick Spector, Blake Bordelon, Niklas Muennighoff, Mansheej Paul, Cengiz Pehlevan, Christopher Ré, Aditi Raghunathan |  |
| 19 |  |  [Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance](https://openreview.net/forum?id=SPS6HzVzyt) |  | 0 | Large Language Model's are instruction-finetuned to enhance their ability to follow user instructions and better comprehend input context. Still, they often struggle to follow the input context, especially when it contradicts model's parametric knowledge. This manifests as various failures, such as hallucinations where a model inserts outdated or unwarranted facts into its response. In this work, we observe an intriguing phenomenon: the context reliance of the model decreases as instruction finetuning progresses, $\textit{despite an initial expected increase}$. We call this phenomenon as the $\textbf{context-parametric inversion}$. This is surprising, as one would expect instruction tuning to improve the model's ability to follow input instructions. We observe this behavior on multiple general purpose instruction tuning datasets such as TULU, Alpaca and Ultrachat, across multiple model families like Llama, Mistral and Pythia. We perform various controlled studies to eliminate some simple hypothesis for this observed behavior and isolate what datapoints cause this counter-intuitive behavior. We then analyze the phenomenon theoretically, to explain why context reliance varies across the trajectory of finetuning. We tie the observed context-parametric inversion to the properties of the finetuning data, which provides us with some potential mitigation strategies that provide limited but insightful gains. | Sachin Goyal, Christina Baek, J. Zico Kolter, Aditi Raghunathan |  |
| 20 |  |  [Inference Scaling for Long-Context Retrieval Augmented Generation](https://openreview.net/forum?id=FSjIrOm1vz) |  | 0 | The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs’ ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG. | Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui Wang, Michael Bendersky |  |
| 21 |  |  [Scaling LLM Test-Time Compute Optimally Can be More Effective than Scaling Parameters for Reasoning](https://openreview.net/forum?id=4FWAwZtd2n) |  | 0 | Enabling LLMs to improve their outputs by using more test-time compute is a critical step towards building self-improving agents that can operate on open-ended natural language. In this paper, we scale up inference-time computation in LLMs, with a focus on answering: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on performance, but also on the future of LLM pretraining and how to tradeoff inference-time and pre-training compute. Little research has attempted to understand the scaling behaviors of test-time inference methods, with current work largely providing negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models (PRMs); and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a "compute-optimal" scaling strategy, which acts to, as effectively as possible, allocate test-time compute per prompt in an adaptive manner. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling for math reasoning problems by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model. | Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar |  |
| 22 |  |  [Capturing the Temporal Dependence of Training Data Influence](https://openreview.net/forum?id=uHLgDEgiS5) |  | 0 | Traditional data influence estimation methods, like influence function, assume that learning algorithms are permutation-invariant with respect to training data. However, modern training paradigms—especially for foundation models using stochastic algorithms and non-convergent, multi-stage curricula—are sensitive to data ordering, thus violating this assumption. This mismatch renders influence functions inadequate for answering some critical questions in current machine learning: How can we differentiate the influence of the same data contributing at different stages of training? More generally, how can we capture the dependence of data influence on the optimization trajectory during training? To address this gap, we formalize the concept of \emph{trajectory-specific leave-one-out (LOO) influence}, which quantifies the impact of removing a data point from a specific iteration during training, accounting for the exact sequence of data encountered and the model's optimization trajectory. However, exactly evaluating the trajectory-specific LOO presents a significant computational challenge. To address this, we propose \emph{data value embedding}, a novel technique enabling efficient approximation of trajectory-specific LOO. Specifically, we compute a training data embedding that encapsulates the cumulative interactions between data and the evolving model parameters. The LOO can then be efficiently approximated through a simple dot-product between the data value embedding and the gradient of the given test data. As data value embedding captures training data ordering, it offers valuable insights into model training dynamics. In particular, we uncover distinct phases of data influence, revealing that data points in the early and late stages of training exert a greater impact on the final model. These insights translate into actionable strategies for managing the computational overhead of data selection by strategically timing the selection process, potentially opening new avenues in data curation research. | Jiachen T. Wang, Dawn Song, James Zou, Prateek Mittal, Ruoxi Jia |  |
| 23 |  |  [Self-Improvement in Language Models: The Sharpening Mechanism](https://openreview.net/forum?id=WJaUkwci9o) |  | 0 | Recent work in language modeling has raised the possibility of “self-improvement,” where an LLM evaluates and refines its own generations to achieve higher performance without external feedback. It is impossible for this self-improvement to create information that is not already in the model, so why should we expect that this will lead to improved capabilities? We offer a new theoretical perspective on the capabilities of self-improvement through a lens we refer to as “sharpening.” Motivated by the observation that language models are often better at verifying response quality than they are at generating correct responses, we formalize self-improvement as using the model itself as a verifier during post-training in order to ‘sharpen’ the model to one placing large mass on high-quality sequences, thereby amortizing the expensive inference-time computation of generating good sequences. We begin by introducing a new statistical framework for sharpening in which the learner has sample access to a pre-trained base policy. Then, we analyze two natural families of self improvement algorithms based on SFT and RLHF. We find that (i) the SFT-based approach is minimax optimal whenever the initial model has sufficient coverage, but (ii) the RLHF-based approach can improve over SFT-based self- improvement by leveraging online exploration, bypassing the need for coverage. We view these findings as a starting point toward a foundational understanding that can guide the design and evaluation of self-improvement algorithms. | Audrey Huang, Adam Block, Dylan J. Foster, Dhruv Rohatgi, Cyril Zhang, Max Simchowitz, Jordan T. Ash, Akshay Krishnamurthy |  |
| 24 |  |  [Data Shapley in One Training Run](https://openreview.net/forum?id=HD6bWcj87Y) |  | 0 | Data Shapley offers a principled framework for attributing the contribution of data within machine learning contexts. However, the traditional notion of Data Shapley requires re-training models on various data subsets, which becomes computationally infeasible for large-scale models. Additionally, this retraining-based definition cannot evaluate the contribution of data for a specific model training run, which may often be of interest in practice. This paper introduces a novel concept, In-Run Data Shapley, which eliminates the need for model retraining and is specifically designed for assessing data contribution for a particular model of interest. In-Run Data Shapley calculates the Shapley value for each gradient update iteration and accumulates these values throughout the training process. We present several techniques that allow the efficient scaling of In-Run Data Shapley to the size of foundation models. In its most optimized implementation, our method adds negligible runtime overhead compared to standard model training. This dramatic efficiency improvement makes it possible to perform data attribution for the foundation model pretraining stage. We present several case studies that offer fresh insights into pretraining data's contribution and discuss their implications for copyright in generative AI and pretraining data curation. | Jiachen T. Wang, Prateek Mittal, Dawn Song, Ruoxi Jia |  |
| 25 |  |  [Computationally Efficient RL under Linear Bellman Completeness for Deterministic Dynamics](https://openreview.net/forum?id=hyfe5q5TD0) |  | 0 | We study computationally and statistically efficient Reinforcement Learning algorithms for the \*linear Bellman Complete\* setting. This setting uses linear function approximation to capture value functions and unifies existing models like linear Markov Decision Processes (MDP) and Linear Quadratic Regulators (LQR). While it is known from the prior works that this setting is statistically tractable, it remained open whether a computationally efficient algorithm exists. Our work provides a computationally efficient algorithm for the linear Bellman complete setting that works for MDPs with large action spaces, random initial states, and random rewards but relies on the underlying dynamics to be deterministic. Our approach is based on randomization: we inject random noise into least squares regression problems to perform optimistic value iteration. Our key technical contribution is to carefully design the noise to only act in the null space of the training data to ensure optimism while circumventing a subtle error amplification issue. | Runzhe Wu, Ayush Sekhari, Akshay Krishnamurthy, Wen Sun |  |
| 26 |  |  [Linear Representations of Political Perspective Emerge in Large Language Models](https://openreview.net/forum?id=rwqShzb9li) |  | 0 | Large language models (LLMs) have demonstrated the ability to generate text that realistically reflects a range of different subjective human perspectives. This paper studies how LLMs are seemingly able to reflect more liberal versus more conservative viewpoints among other political perspectives in American politics. We show that LLMs possess linear representations of political perspectives within activation space, wherein more similar perspectives are represented closer together. To do so, we probe the attention heads across the layers of three open transformer-based LLMs (Llama-2-7b-chat, Mistral-7b-instruct, Vicuna-7b). We first prompt models to generate text from the perspectives of different U.S. lawmakers. We then identify sets of attention heads whose activations linearly predict those lawmakers' DW-NOMINATE scores, a widely-used and validated measure of political ideology. We find that highly predictive heads are primarily located in the middle layers, often speculated to encode high-level concepts and tasks. Using probes only trained to predict lawmakers' ideology, we then show that the same probes can predict measures of news outlets' slant from the activations of models prompted to simulate text from those news outlets. These linear probes allow us to visualize, interpret, and monitor ideological stances implicitly adopted by an LLM as it generates open-ended responses. Finally, we demonstrate that by applying linear interventions to these attention heads, we can steer the model outputs toward a more liberal or conservative stance. Overall, our research suggests that LLMs possess a high-level linear representation of American political ideology and that by leveraging recent advances in mechanistic interpretability, we can identify, monitor, and steer the subjective perspective underlying generated text. | Junsol Kim, James Evans, Aaron Schein |  |
| 27 |  |  [Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs](https://openreview.net/forum?id=FBkpCyujtS) |  | 0 | Large Language Models (LLMs) generate text by sampling the next token from a probability distribution over the vocabulary at each decoding step. Popular sampling methods like top-p (nucleus sampling) often struggle to balance quality and diversity, especially at higher temperatures which lead to incoherent or repetitive outputs. We propose min-p sampling, a dynamic truncation method that adjusts the sampling threshold based on the model's confidence by using the top token's probability as a scaling factor. Our experiments on benchmarks including GPQA, GSM8K, and AlpacaEval Creative Writing show that min-p sampling improves both the quality and diversity of generated text across different model families (Mistral and Llama 3) and model sizes (1B to 123B parameters), especially at higher temperatures. Human evaluations further show a clear preference for min-p sampling, in both text quality and creativity. Min-p sampling has been adopted by popular open-source LLM frameworks, including Hugging Face Transformers, VLLM, and many others, highlighting its considerable impact on improving text generation quality. | Nguyen Nhat Minh, Andrew Baker, Clement Neo, Allen G. Roush, Andreas Kirsch, Ravid ShwartzZiv |  |
| 28 |  |  [Joint Graph Rewiring and Feature Denoising via Spectral Resonance](https://openreview.net/forum?id=zBbZ2vdLzH) |  | 0 | When learning from graph data, the graph and the node features both give noisy information about the node labels. In this paper we propose an algorithm to \*\*j\*\*ointly \*\*d\*\*enoise the features and \*\*r\*\*ewire the graph (JDR), which improves the performance of downstream node classification graph neural nets (GNNs). JDR works by aligning the leading spectral spaces of graph and feature matrices. It approximately solves the associated non-convex optimization problem in a way that handles graphs with multiple classes and different levels of homophily or heterophily. We theoretically justify JDR in a stylized setting and show that it consistently outperforms existing rewiring methods on a wide range of synthetic and real-world node classification tasks. | Jonas Linkerhägner, Cheng Shi, Ivan Dokmanic |  |
| 29 |  |  [Iterative Nash Policy Optimization: Aligning LLMs with General Preferences via No-Regret Learning](https://openreview.net/forum?id=Pujt3ADZgI) |  | 0 | Reinforcement Learning with Human Feedback (RLHF) has achieved great success in aligning large language models (LLMs) with human preferences. Prevalent RLHF approaches are reward-based, following the Bradley-Terry (BT) model assumption, which may not fully capture the complexity of human preferences. In this paper, we explore RLHF under a general preference framework and approach it from a game-theoretic perspective. Specifically, we formulate the problem as a two-player game and propose a novel online algorithm, iterative Nash policy optimization (INPO). The key idea is to let the policy play against itself via no- regret learning, thereby approximating the Nash policy. Unlike previous methods, INPO bypasses the need for estimating the expected win rate for individual responses, which typically incurs high computational or annotation costs. Instead, we introduce a new loss objective that is directly minimized over a preference dataset. We provide theoretical analysis for our approach and demonstrate its effectiveness through experiments on various representative benchmarks. With an LLaMA-3-8B-based SFT model, INPO achieves a 42.6% length-controlled win rate on AlpacaEval 2.0 and a 37.8% win rate on Arena-Hard, showing substantial improvement over the state-of-the-art online RLHF algorithms. | Yuheng Zhang, Dian Yu, Baolin Peng, Linfeng Song, Ye Tian, Mingyue Huo, Nan Jiang, Haitao Mi, Dong Yu |  |
| 30 |  |  [Progressive Compression with Universally Quantized Diffusion Models](https://openreview.net/forum?id=CxXGvKRDnL) |  | 0 | Diffusion probabilistic models have achieved mainstream success in many generative modeling tasks, from image generation to inverse problem solving. A distinct feature of these models is that they correspond to deep hierarchical latent variable models optimizing a variational evidence lower bound (ELBO) on the data likelihood. Drawing on a basic connection between likelihood modeling and compression, we explore the potential of diffusion models for progressive coding, resulting in a sequence of bits that can be incrementally transmitted and decoded with progressively improving reconstruction quality. Unlike prior work based on Gaussian diffusion or conditional diffusion models, we propose a new form of diffusion model with uniform noise in the forward process, whose negative ELBO corresponds to the end-to-end compression cost using universal quantization. We obtain promising first results on image compression, achieving competitive rate-distortion-realism results on a wide range of bit-rates with a single model, bringing neural codecs a step closer to practical deployment. Our code can be found at https://github.com/mandt-lab/uqdm. | Yibo Yang, Justus C. Will, Stephan Mandt |  |
| 31 |  |  [Accelerated training through iterative gradient propagation along the residual path](https://openreview.net/forum?id=JDm7oIcx4Y) |  | 0 | Despite being the cornerstone of deep learning, backpropagation is criticized for its inherent sequentiality, which can limit the scalability of very deep models. Such models faced convergence issues due to vanishing gradient, later resolved using residual connections. Variants of these are now widely used in modern architectures. However, the computational cost of backpropagation remains a major burden, accounting for most of the training time. Taking advantage of residual-like architectural designs, we introduce Highway backpropagation, a parallelizable iterative algorithm that approximates backpropagation, by alternatively i) accumulating the gradient estimates along the residual path, and ii) backpropagating them through every layer in parallel. This algorithm is naturally derived from a decomposition of the gradient as the sum of gradients flowing through all paths, and is adaptable to a diverse set of common architectures, ranging from ResNets and Transformers to recurrent neural networks. Through an extensive empirical study on a large selection of tasks and models, we evaluate Highway-BP and show that major speedups can be achieved with minimal performance degradation. | Erwan Fagnou, Paul Caillon, Blaise Delattre, Alexandre Allauzen |  |
| 32 |  |  [Tight Lower Bounds under Asymmetric High-Order Hölder Smoothness and Uniform Convexity](https://openreview.net/forum?id=fMTPkDEhLQ) |  | 0 | In this paper, we provide tight lower bounds for the oracle complexity of minimizing high-order Hölder smooth and uniformly convex functions. Specifically, for a function whose $p^{th}$-order derivatives are Hölder continuous with degree $\nu$ and parameter $H$, and that is uniformly convex with degree $q$ and parameter $\sigma$, we focus on two asymmetric cases: (1) $q > p + \nu$, and (2) $q < p+\nu$. Given up to $p^{th}$-order oracle access, we establish worst-case oracle complexities of $\Omega\left( \left( \frac{H}{\sigma}\right)^\frac{2}{3(p+\nu)-2}\left( \frac{\sigma}{\epsilon}\right)^\frac{2(q-p-\nu)}{q(3(p+\nu)-2)}\right)$ in the first case with an $\ell_\infty$-ball-truncated-Gaussian smoothed hard function and $\Omega\left(\left(\frac{H}{\sigma}\right)^\frac{2}{3(p+\nu)-2}+ \log\log\left(\left(\frac{\sigma^{p+\nu}}{H^q}\right)^\frac{1}{p+\nu-q}\frac{1}{\epsilon}\right)\right)$ in the second case, for reaching an $\epsilon$-approximate solution in terms of the optimality gap. Our analysis generalizes previous lower bounds for functions under first- and second-order smoothness as well as those for uniformly convex functions, and furthermore our results match the corresponding upper bounds in this general setting. | Site Bai, Brian Bullins |  |
| 33 |  |  [ShEPhERD: Diffusing shape, electrostatics, and pharmacophores for bioisosteric drug design](https://openreview.net/forum?id=KSLkFYHlYg) |  | 0 | Engineering molecules to exhibit precise 3D intermolecular interactions with their environment forms the basis of chemical design. In ligand-based drug design, bioisosteric analogues of known bioactive hits are often identified by virtually screening chemical libraries with shape, electrostatic, and pharmacophore similarity scoring functions. We instead hypothesize that a generative model which learns the joint distribution over 3D molecular structures and their interaction profiles may facilitate 3D interaction-aware chemical design. We specifically design ShEPhERD, an SE(3)-equivariant diffusion model which jointly diffuses/denoises 3D molecular graphs and representations of their shapes, electrostatic potential surfaces, and (directional) pharmacophores to/from Gaussian noise. Inspired by traditional ligand discovery, we compose 3D similarity scoring functions to assess ShEPhERD’s ability to conditionally generate novel molecules with desired interaction profiles. We demonstrate ShEPhERD’s potential for impact via exemplary drug design tasks including natural product ligand hopping, protein-blind bioactive hit diversification, and bioisosteric fragment merging. | Keir Adams, Kento Abeywardane, Jenna C. Fromer, Connor W. Coley |  |
| 34 |  |  [Restructuring Vector Quantization with the Rotation Trick](https://openreview.net/forum?id=GMwRl2e9Y1) |  | 0 | Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress a continuous input to a discrete latent space and reconstruct it with minimal distortion. They operate by maintaining a set of vectors---often referred to as the codebook---and quantizing each encoder output to the nearest vector in the codebook. However, as vector quantization is non-differentiable, the gradient to the encoder flows _around_ the vector quantization layer rather than _through_ it in a straight-through approximation. This approximation may be undesirable as all information from the vector quantization operation is lost. In this work, we propose a way to propagate gradients through the vector quantization layer of VQ-VAEs. We smoothly transform each encoder output into its corresponding codebook vector via a rotation and rescaling linear transformation that is treated as a constant during backpropagation. As a result, the relative magnitude and angle between encoder output and codebook vector becomes encoded into the gradient as it propagates through the vector quantization layer and back to the encoder. Across 11 different VQ-VAE training paradigms, we find this restructuring improves reconstruction metrics, codebook utilization, and quantization error. | Christopher Fifty, Ronald Guenther Junkins, Dennis Duan, Aniketh Iyengar, Jerry Weihong Liu, Ehsan Amid, Sebastian Thrun, Christopher Ré |  |
| 35 |  |  [Interpreting Emergent Planning in Model-Free Reinforcement Learning](https://openreview.net/forum?id=DzGe40glxs) |  | 0 | We present the first mechanistic evidence that model-free reinforcement learning agents can learn to plan. This is achieved by applying a methodology based on concept-based interpretability to a model-free agent in Sokoban -- a commonly used benchmark for studying planning. Specifically, we demonstrate that DRC, a generic model-free agent introduced by [Guez et al. (2019)](https://arxiv.org/abs/1901.03559), uses learned concept representations to internally formulate plans that both predict the long-term effects of actions on the environment and influence action selection. Our methodology involves: (1) probing for planning-relevant concepts, (2) investigating plan formation within the agent's representations, and (3) verifying that discovered plans (in the agent's representations) have a causal effect on the agent's behavior through interventions. We also show that the emergence of these plans coincides with the emergence of a planning-like property: the ability to benefit from additional test-time compute. Finally, we perform a qualitative analysis of the planning algorithm learned by the agent and discover a strong resemblance to parallelized bidirectional search. Our findings advance understanding of the internal mechanisms underlying planning behavior in agents, which is important given the recent trend of emergent planning and reasoning capabilities in LLMs through RL. | Thomas Bush, Stephen Chung, Usman Anwar, Adrià GarrigaAlonso, David Krueger |  |
| 36 |  |  [Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization](https://openreview.net/forum?id=kX8h23UG6v) |  | 0 | A long-standing belief holds that Bayesian Optimization (BO) with standard Gaussian processes (GP) --- referred to as standard BO --- underperforms in high-dimensional optimization problems. While this belief seems plausible, it lacks both robust empirical evidence and theoretical justification. To address this gap, we present a systematic investigation. First, through a comprehensive evaluation across twelve benchmarks, we found that while the popular Square Exponential (SE) kernel often leads to poor performance, using Mat\'ern kernels enables standard BO to consistently achieve top-tier results, frequently surpassing methods specifically designed for high-dimensional optimization. Second, our theoretical analysis reveals that the SE kernel’s failure primarily stems from improper initialization of the length-scale parameters, which are commonly used in practice but can cause gradient vanishing in training. We provide a probabilistic bound to characterize this issue, showing that Mat\'ern kernels are less susceptible and can robustly handle much higher dimensions. Third, we propose a simple robust initialization strategy that dramatically improves the performance of the SE kernel, bringing it close to state-of-the-art methods, without requiring additional priors or regularization. We prove another probabilistic bound that demonstrates how the gradient vanishing issue can be effectively mitigated with our method. Our findings advocate for a re-evaluation of standard BO’s potential in high-dimensional settings. | Zhitong Xu, Haitao Wang, Jeff M. Phillips, Shandian Zhe |  |
| 37 |  |  [Limits to scalable evaluation at the frontier: LLM as judge won't beat twice the data](https://openreview.net/forum?id=NO6Tv6QcDs) |  | 0 | High quality annotations are increasingly a bottleneck in the explosively growing machine learning ecosystem. Scalable evaluation methods that avoid costly annotation have therefore become an important research ambition. Many hope to use strong existing models in lieu of costly labels to provide cheap model evaluations. Unfortunately, this method of using models as judges introduces biases, such as self-preferencing, that can distort model comparisons. An emerging family of debiasing tools promises to fix these issues by using a few high quality labels to debias a large number of model judgments. In this paper, we study how far such debiasing methods, in principle, can go. Our main result shows that when the judge is no more accurate than the evaluated model, no debiasing method can decrease the required amount of ground truth labels by more than half. Our result speaks to the severe limitations of the LLM-as-a-judge paradigm at the evaluation frontier where the goal is to assess newly released models that are possibly better than the judge. Through an empirical evaluation, we demonstrate that the sample size savings achievable in practice are even more modest than what our theoretical limit suggests. Along the way, our work provides new observations about debiasing methods for model evaluation, and points out promising avenues for future work. | Florian E. Dorner, Vivian Yvonne Nastl, Moritz Hardt |  |
| 38 |  |  [DEPT: Decoupled Embeddings for Pre-training Language Models](https://openreview.net/forum?id=vf5aUZT0Fz) |  | 0 | Language Model pre-training uses broad data mixtures to enhance performance across domains and languages. However, training on such heterogeneous text corpora requires extensive and expensive efforts. Since these data sources vary significantly in lexical, syntactic, and semantic aspects, they cause negative interference or the \`\`curse of multilinguality''. To address these challenges we propose a communication-efficient pre-training framework, DEPT. Our method decouples embeddings from the transformer body while simultaneously training the latter on multiple data sources without requiring a shared vocabulary. DEPT can: (1) train robustly and effectively under significant data heterogeneity, (2) minimize token embedding parameters to only what the data source vocabulary requires, while cutting communication costs in direct proportion to both the communication frequency and the reduction in parameters, (3) enhance transformer body plasticity and generalization, improving both average perplexity (up to 20%) and downstream task performance, and (4) enable training with custom optimized vocabularies per data source. We demonstrate DEPT's potential via the first vocabulary-agnostic federated pre-training of billion-scale models, reducing communication costs by orders of magnitude and embedding memory by 4-5x. | Alex Iacob, Lorenzo Sani, Meghdad Kurmanji, William F. Shen, Xinchi Qiu, Dongqi Cai, Yan Gao, Nicholas Donald Lane |  |
| 39 |  |  [Homomorphism Expressivity of Spectral Invariant Graph Neural Networks](https://openreview.net/forum?id=rdv6yeMFpn) |  | 0 | Graph spectra are an important class of structural features on graphs that have shown promising results in enhancing Graph Neural Networks (GNNs). Despite their widespread practical use, the theoretical understanding of the power of spectral invariants --- particularly their contribution to GNNs --- remains incomplete. In this paper, we address this fundamental question through the lens of homomorphism expressivity, providing a comprehensive and quantitative analysis of the expressive power of spectral invariants. Specifically, we prove that spectral invariant GNNs can homomorphism-count exactly a class of specific tree-like graphs which we refer to as \emph{parallel trees}. We highlight the significance of this result in various contexts, including establishing a quantitative expressiveness hierarchy across different architectural variants, offering insights into the impact of GNN depth, and understanding the subgraph counting capabilities of spectral invariant GNNs. In particular, our results significantly extend \citet{arvind2024hierarchy} and settle their open questions. Finally, we generalize our analysis to higher-order GNNs and answer an open question raised by \citet{zhang2024expressive}. | Jingchu Gai, Yiheng Du, Bohang Zhang, Haggai Maron, Liwei Wang |  |
| 40 |  |  [RB-Modulation: Training-Free Stylization using Reference-Based Modulation](https://openreview.net/forum?id=bnINPG5A32) |  | 0 | We propose Reference-Based Modulation (RB-Modulation), a new plug-and-play solution for training-free personalization of diffusion models. Existing training-free approaches exhibit difficulties in (a) style extraction from reference images in the absence of additional style or content text descriptions, (b) unwanted content leakage from reference style images, and (c) effective composition of style and content. RB-Modulation is built on a novel stochastic optimal controller where a style descriptor encodes the desired attributes through a terminal cost. The resulting drift not only overcomes the difficulties above, but also ensures high fidelity to the reference style and adheres to the given text prompt. We also introduce a cross-attention-based feature aggregation scheme that allows RB-Modulation to decouple content and style from the reference image. With theoretical justification and empirical evidence, our test-time optimization framework demonstrates precise extraction and control of \*content\* and \*style\* in a training-free manner. Further, our method allows a seamless composition of content and style, which marks a departure from the dependency on external adapters or ControlNets. See project page: https://rb-modulation.github.io/ for code and further details. | Litu Rout, Yujia Chen, Nataniel Ruiz, Abhishek Kumar, Constantine Caramanis, Sanjay Shakkottai, WenSheng Chu |  |
| 41 |  |  [Kinetix: Investigating the Training of General Agents through Open-Ended Physics-Based Control Tasks](https://openreview.net/forum?id=zCxGCdzreM) |  | 0 | While large models trained with self-supervised learning on offline datasets have shown remarkable capabilities in text and image domains, achieving the same generalisation for agents that act in sequential decision problems remains an open challenge. In this work, we take a step towards this goal by procedurally generating tens of millions of 2D physics-based tasks and using these to train a general reinforcement learning (RL) agent for physical control. To this end, we introduce Kinetix: an open-ended space of physics-based RL environments that can represent tasks ranging from robotic locomotion and grasping to video games and classic RL environments, all within a unified framework. Kinetix makes use of our novel hardware-accelerated physics engine Jax2D that allows us to cheaply simulate billions of environment steps during training. Our trained agent exhibits strong physical reasoning capabilities in 2D space, being able to zero-shot solve unseen human-designed environments. Furthermore, fine-tuning this general agent on tasks of interest shows significantly stronger performance than training an RL agent \*tabula rasa\*. This includes solving some environments that standard RL training completely fails at. We believe this demonstrates the feasibility of large scale, mixed-quality pre-training for online RL and we hope that Kinetix will serve as a useful framework to investigate this further. | Michael T. Matthews, Michael Beukman, Chris Lu, Jakob Nicolaus Foerster |  |
| 42 |  |  [OptionZero: Planning with Learned Options](https://openreview.net/forum?id=3IFRygQKGL) |  | 0 | Planning with options -- a sequence of primitive actions -- has been shown effective in reinforcement learning within complex environments. Previous studies have focused on planning with predefined options or learned options through expert demonstration data. Inspired by MuZero, which learns superhuman heuristics without any human knowledge, we propose a novel approach, named \*OptionZero\*. OptionZero incorporates an \*option network\* into MuZero, providing autonomous discovery of options through self-play games. Furthermore, we modify the dynamics network to provide environment transitions when using options, allowing searching deeper under the same simulation constraints. Empirical experiments conducted in 26 Atari games demonstrate that OptionZero outperforms MuZero, achieving a 131.58% improvement in mean human-normalized score. Our behavior analysis shows that OptionZero not only learns options but also acquires strategic skills tailored to different game characteristics. Our findings show promising directions for discovering and using options in planning. Our code is available at https://rlg.iis.sinica.edu.tw/papers/optionzero. | PoWei Huang, PeiChiun Peng, Hung Guei, TiRong Wu |  |
| 43 |  |  [Instant Policy: In-Context Imitation Learning via Graph Diffusion](https://openreview.net/forum?id=je3GZissZc) |  | 0 | Following the impressive capabilities of in-context learning with large transformers, In-Context Imitation Learning (ICIL) is a promising opportunity for robotics. We introduce Instant Policy, which learns new tasks instantly from just one or two demonstrations, achieving ICIL through two key components. First, we introduce inductive biases through a graph representation and model ICIL as a graph generation problem using a learned diffusion process, enabling structured reasoning over demonstrations, observations, and actions. Second, we show that such a model can be trained using pseudo-demonstrations – arbitrary trajectories generated in simulation – as a virtually infinite pool of training data. Our experiments, in both simulation and reality, show that Instant Policy enables rapid learning of various everyday robot tasks. We also show how it can serve as a foundation for cross-embodiment and zero-shot transfer to language-defined tasks. | Vitalis Vosylius, Edward Johns |  |
| 44 |  |  [What should a neuron aim for? Designing local objective functions based on information theory](https://openreview.net/forum?id=CLE09ESvul) |  | 0 | In modern deep neural networks, the learning dynamics of individual neurons are often obscure, as the networks are trained via global optimization. Conversely, biological systems build on self-organized, local learning, achieving robustness and efficiency with limited global information. Here, we show how self-organization between individual artificial neurons can be achieved by designing abstract bio-inspired local learning goals. These goals are parameterized using a recent extension of information theory, Partial Information Decomposition (PID), which decomposes the information that a set of information sources holds about an outcome into unique, redundant and synergistic contributions. Our framework enables neurons to locally shape the integration of information from various input classes, i.e., feedforward, feedback, and lateral, by selecting which of the three inputs should contribute uniquely, redundantly or synergistically to the output. This selection is expressed as a weighted sum of PID terms, which, for a given problem, can be directly derived from intuitive reasoning or via numerical optimization, offering a window into understanding task-relevant local information processing. Achieving neuron-level interpretability while enabling strong performance using local learning, our work advances a principled information-theoretic foundation for local learning strategies. | Andreas Christian Schneider, Valentin Neuhaus, David Alexander Ehrlich, Abdullah Makkeh, Alexander S. Ecker, Viola Priesemann, Michael Wibral |  |
| 45 |  |  [Cross-Entropy Is All You Need To Invert the Data Generating Process](https://openreview.net/forum?id=hrqNOxpItr) |  | 0 | Supervised learning has become a cornerstone of modern machine learning, yet a comprehensive theory explaining its effectiveness remains elusive. Empirical phenomena, such as neural analogy-making and the linear representation hypothesis, suggest that supervised models can learn interpretable factors of variation in a linear fashion. Recent advances in self-supervised learning, particularly nonlinear Independent Component Analysis, have shown that these methods can recover latent structures by inverting the data generating process. We extend these identifiability results to parametric instance discrimination, then show how insights transfer to the ubiquitous setting of supervised learning with cross-entropy minimization. We prove that even in standard classification tasks, models learn representations of ground-truth factors of variation up to a linear transformation under a certain DGP. We corroborate our theoretical contribution with a series of empirical studies. First, using simulated data matching our theoretical assumptions, we demonstrate successful disentanglement of latent factors. Second, we show that on DisLib, a widely-used disentanglement benchmark, simple classification tasks recover latent structures up to linear transformations. Finally, we reveal that models trained on ImageNet encode representations that permit linear decoding of proxy factors of variation. Together, our theoretical findings and experiments offer a compelling explanation for recent observations of linear representations, such as superposition in neural networks. This work takes a significant step toward a cohesive theory that accounts for the unreasonable effectiveness of supervised learning. | Patrik Reizinger, Alice Bizeul, Attila Juhos, Julia E. Vogt, Randall Balestriero, Wieland Brendel, David A. Klindt |  |
| 46 |  |  [Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues](https://openreview.net/forum?id=UvTo3tVBk2) |  | 0 | Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and DeltaNet have emerged as efficient alternatives to Transformers for long sequences. However, both Transformers and LRNNs struggle to perform state-tracking, which may impair performance in tasks such as code evaluation. In one forward pass, current architectures are unable to solve even parity, the simplest state-tracking task, which non-linear RNNs can handle effectively. Recently, Sarrof et al. (2024) demonstrated that the failure of LRNNs like Mamba to solve parity stems from restricting the value range of their diagonal state-transition matrices to $[0, 1]$ and that incorporating negative values can resolve this issue. We extend this result to non-diagonal LRNNs such as DeltaNet. We prove that finite precision LRNNs with state-transition matrices having only positive eigenvalues cannot solve parity, while non-triangular matrices are needed to count modulo $3$. Notably, we also prove that LRNNs can learn any regular language when their state-transition matrices are products of identity minus vector outer product matrices, each with eigenvalues in the range $[-1, 1]$. Our experiments confirm that extending the eigenvalue range of Mamba and DeltaNet to include negative values not only enables them to solve parity but consistently improves their performance on state-tracking tasks. We also show that state-tracking enabled LRNNs can be pretrained stably and efficiently at scale (1.3B parameters), achieving competitive performance on language modeling and showing promise on code and math tasks. | Riccardo Grazzi, Julien Siems, Arber Zela, Jörg K. H. Franke, Frank Hutter, Massimiliano Pontil |  |
| 47 |  |  [Attention as a Hypernetwork](https://openreview.net/forum?id=V4K9h1qNxE) |  | 0 | Transformers can under some circumstances generalize to novel problem instances whose constituent parts might have been encountered during training, but whose compositions have not. What mechanisms underlie this ability for compositional generalization? By reformulating multi-head attention as a hypernetwork, we reveal that a composable, low-dimensional latent code specifies key-query specific operations. We find empirically that this latent code is predictive of the subtasks the network performs on unseen task compositions, revealing that latent codes acquired during training are reused to solve unseen problem instances. To further examine the hypothesis that the intrinsic hypernetwork of multi-head attention supports compositional generalization, we ablate whether making the hypernetwork-generated linear value network nonlinear strengthens compositionality. We find that this modification improves compositional generalization on abstract reasoning tasks. In particular, we introduce a symbolic version of the Raven's Progressive Matrices human intelligence test, which gives us precise control over the problem compositions encountered during training and evaluation. We demonstrate on this task how scaling model size and data enables compositional generalization in transformers and gives rise to a functionally structured latent space. | Simon Schug, Seijin Kobayashi, Yassir Akram, João Sacramento, Razvan Pascanu |  |
| 48 |  |  [Transformers Provably Solve Parity Efficiently with Chain of Thought](https://openreview.net/forum?id=n2NidsYDop) |  | 0 | This work provides the first theoretical analysis of training transformers to solve complex problems by recursively generating intermediate states, analogous to fine-tuning for chain-of-thought (CoT) reasoning. We consider training a one-layer transformer to solve the fundamental $k$-parity problem, extending the work on RNNs by \citet{Wies23}. We establish three key results: (1) any finite-precision gradient-based algorithm, without intermediate supervision, requires substantial iterations to solve parity with finite samples. (2) In contrast, when intermediate parities are incorporated into the loss function, our model can learn parity in one gradient update when aided by \emph{teacher forcing}, where ground-truth labels of the reasoning chain are provided at each generation step. (3) Even without teacher forcing, where the model must generate CoT chains end-to-end, parity can be learned efficiently if augmented data is employed to internally verify the soundness of intermediate steps. Our findings, supported by numerical experiments, show that task decomposition and stepwise reasoning naturally arise from optimizing transformers with CoT; moreover, self-consistency checking can improve multi-step reasoning ability, aligning with empirical studies of CoT. | Juno Kim, Taiji Suzuki |  |
| 49 |  |  [Oscillatory State-Space Models](https://openreview.net/forum?id=GRMfXcAAFh) |  | 0 | We propose Linear Oscillatory State-Space models (LinOSS) for efficiently learning on long sequences. Inspired by cortical dynamics of biological neural networks, we base our proposed LinOSS model on a system of forced harmonic oscillators. A stable discretization, integrated over time using fast associative parallel scans, yields the proposed state-space model. We prove that LinOSS produces stable dynamics only requiring nonnegative diagonal state matrix. This is in stark contrast to many previous state-space models relying heavily on restrictive parameterizations. Moreover, we rigorously show that LinOSS is universal, i.e., it can approximate any continuous and causal operator mapping between time-varying functions, to desired accuracy. In addition, we show that an implicit-explicit discretization of LinOSS perfectly conserves the symmetry of time reversibility of the underlying dynamics. Together, these properties enable efficient modeling of long-range interactions, while ensuring stable and accurate long-horizon forecasting. Finally, our empirical results, spanning a wide range of time-series tasks from mid-range to very long-range classification and regression, as well as long-horizon forecasting, demonstrate that our proposed LinOSS model consistently outperforms state-of-the-art sequence models. Notably, LinOSS outperforms Mamba and LRU by nearly 2x on a sequence modeling task with sequences of length 50k. | T. Konstantin Rusch, Daniela Rus |  |
| 50 |  |  [Latent Bayesian Optimization via Autoregressive Normalizing Flows](https://openreview.net/forum?id=ZCOwwRAaEl) |  | 0 | Bayesian Optimization (BO) has been recognized for its effectiveness in optimizing expensive and complex objective functions. Recent advancements in Latent Bayesian Optimization (LBO) have shown promise by integrating generative models such as variational autoencoders (VAEs) to manage the complexity of high-dimensional and structured data spaces. However, existing LBO approaches often suffer from the value discrepancy problem, which arises from the reconstruction gap between input and latent spaces. This value discrepancy problem propagates errors throughout the optimization process, leading to suboptimal outcomes. To address this issue, we propose a Normalizing Flow-based Bayesian Optimization (NF-BO), which utilizes normalizing flow as a generative model to establish one-to-one encoding function from the input space to the latent space, along with its left-inverse decoding function, eliminating the reconstruction gap. Specifically, we introduce SeqFlow, an autoregressive normalizing flow for sequence data. In addition, we develop a new candidate sampling strategy that dynamically adjusts the exploration probability for each token based on its importance. Through extensive experiments, our NF-BO method demonstrates superior performance in molecule generation tasks, significantly outperforming both traditional and recent LBO approaches. | Seunghun Lee, Jinyoung Park, Jaewon Chu, Minseo Yoon, Hyunwoo J. Kim |  |
| 51 |  |  [Energy-based Backdoor Defense Against Federated Graph Learning](https://openreview.net/forum?id=5Jc7r5aqHJ) |  | 0 | Federated Graph Learning is rapidly evolving as a privacy-preserving collaborative approach. However, backdoor attacks are increasingly undermining federated systems by injecting carefully designed triggers that lead to the model making incorrect predictions. Trigger structures and injection locations in Federated Graph Learning are more diverse, making traditional federated defense methods less effective. In our work, we propose an effective Federated Graph Backdoor Defense using Topological Graph Energy (FedTGE). At the local client level, it injects distribution knowledge into the local model, assigning low energy to benign samples and high energy to the constructed malicious substitutes, and selects benign clients through clustering. At the global server level, the energy elements uploaded by each client are treated as new nodes to construct a global energy graph for energy propagation, making the selected clients' energy elements more similar and further adjusting the aggregation weights. Our method can handle high data heterogeneity, does not require a validation dataset, and is effective under both small and large malicious proportions. Extensive results on various settings of federated graph scenarios under backdoor attacks validate the effectiveness of this approach. | Guancheng Wan, Zitong Shi, Wenke Huang, Guibin Zhang, Dacheng Tao, Mang Ye |  |
| 52 |  |  [Reasoning Elicitation in Language Models via Counterfactual Feedback](https://openreview.net/forum?id=VVixJ9QavY) |  | 0 | Despite the increasing effectiveness of language models, their reasoning capabilities remain underdeveloped. In particular, causal reasoning through counterfactual question answering is lacking. This work aims to bridge this gap. We first derive novel metrics that balance accuracy in factual and counterfactual questions, capturing a more complete view of the reasoning abilities of language models than traditional factual-only based metrics. Second, we propose several fine-tuning approaches that aim to elicit better reasoning mechanisms, in the sense of the proposed metrics. Finally, we evaluate the performance of the fine-tuned language models in a variety of realistic scenarios. In particular, we investigate to what extent our fine-tuning approaches systemically achieve better generalization with respect to the base models in several problems that require, among others, inductive and deductive reasoning capabilities. | Alihan Hüyük, Xinnuo Xu, Jacqueline R. M. A. Maasch, Aditya V. Nori, Javier González |  |
| 53 |  |  [CAX: Cellular Automata Accelerated in JAX](https://openreview.net/forum?id=o2Igqm95SJ) |  | 0 | Cellular automata have become a cornerstone for investigating emergence and self-organization across diverse scientific disciplines. However, the absence of a hardware-accelerated cellular automata library limits the exploration of new research directions, hinders collaboration, and impedes reproducibility. In this work, we introduce CAX (Cellular Automata Accelerated in JAX), a high-performance and flexible open-source library designed to accelerate cellular automata research. CAX delivers cutting-edge performance through hardware acceleration while maintaining flexibility through its modular architecture, intuitive API, and support for both discrete and continuous cellular automata in arbitrary dimensions. We demonstrate CAX's performance and flexibility through a wide range of benchmarks and applications. From classic models like elementary cellular automata and Conway's Game of Life to advanced applications such as growing neural cellular automata and self-classifying MNIST digits, CAX speeds up simulations up to 2,000 times faster. Furthermore, we demonstrate CAX's potential to accelerate research by presenting a collection of three novel cellular automata experiments, each implemented in just a few lines of code thanks to the library's modular architecture. Notably, we show that a simple one-dimensional cellular automaton can outperform GPT-4 on the 1D-ARC challenge. | Maxence Faldor, Antoine Cully |  |
| 54 |  |  [Proteina: Scaling Flow-based Protein Structure Generative Models](https://openreview.net/forum?id=TVQLu34bdw) |  | 0 | Recently, diffusion- and flow-based generative models of protein structures have emerged as a powerful tool for de novo protein design. Here, we develop \*Proteina\*, a new large-scale flow-based protein backbone generator that utilizes hierarchical fold class labels for conditioning and relies on a tailored scalable transformer architecture with up to $5\times$ as many parameters as previous models. To meaningfully quantify performance, we introduce a new set of metrics that directly measure the distributional similarity of generated proteins with reference sets, complementing existing metrics. We further explore scaling training data to millions of synthetic protein structures and explore improved training and sampling recipes adapted to protein backbone generation. This includes fine-tuning strategies like LoRA for protein backbones, new guidance methods like classifier-free guidance and autoguidance for protein backbones, and new adjusted training objectives. Proteina achieves state-of-the-art performance on de novo protein backbone design and produces diverse and designable proteins at unprecedented length, up to 800 residues. The hierarchical conditioning offers novel control, enabling high-level secondary-structure guidance as well as low-level fold-specific generation. | Tomas Geffner, Kieran Didi, Zuobai Zhang, Danny Reidenbach, Zhonglin Cao, Jason Yim, Mario Geiger, Christian Dallago, Emine Küçükbenli, Arash Vahdat, Karsten Kreis |  |
| 55 |  |  [Residual Deep Gaussian Processes on Manifolds](https://openreview.net/forum?id=JWtrk7mprJ) |  | 0 | We propose practical deep Gaussian process models on Riemannian manifolds, similar in spirit to residual neural networks. With manifold-to-manifold hidden layers and an arbitrary last layer, they can model manifold- and scalar-valued functions, as well as vector fields. We target data inherently supported on manifolds, which is too complex for shallow Gaussian processes thereon. For example, while the latter perform well on high-altitude wind data, they struggle with the more intricate, nonstationary patterns at low altitudes. Our models significantly improve performance in these settings, enhancing prediction quality and uncertainty calibration, and remain robust to overfitting, reverting to shallow models when additional complexity is unneeded. We further showcase our models on Bayesian optimisation problems on manifolds, using stylised examples motivated by robotics, and obtain substantial improvements in later stages of the optimisation process. Finally, we show our models to have potential for speeding up inference for non-manifold data, when, and if, it can be mapped to a proxy manifold well enough. | Kacper Wyrwal, Andreas Krause, Viacheslav Borovitskiy |  |
| 56 |  |  [Learning to Discretize Denoising Diffusion ODEs](https://openreview.net/forum?id=xDrFWUmCne) |  | 0 | Diffusion Probabilistic Models (DPMs) are generative models showing competitive performance in various domains, including image synthesis and 3D point cloud generation. Sampling from pre-trained DPMs involves multiple neural function evaluations (NFEs) to transform Gaussian noise samples into images, resulting in higher computational costs compared to single-step generative models such as GANs or VAEs. Therefore, reducing the number of NFEs while preserving generation quality is crucial. To address this, we propose LD3, a lightweight framework designed to learn the optimal time discretization for sampling. LD3 can be combined with various samplers and consistently improves generation quality without having to retrain resource-intensive neural networks. We demonstrate analytically and empirically that LD3 improves sampling efficiency with much less computational overhead. We evaluate our method with extensive experiments on 7 pre-trained models, covering unconditional and conditional sampling in both pixel-space and latent-space DPMs. We achieve FIDs of 2.38 (10 NFE), and 2.27 (10 NFE) on unconditional CIFAR10 and AFHQv2 in 5-10 minutes of training. LD3 offers an efficient approach to sampling from pre-trained diffusion models. Code is available at https://github.com/vinhsuhi/LD3. | Vinh Tong, DungTrung Hoang, Anji Liu, Guy Van den Broeck, Mathias Niepert |  |
| 57 |  |  [Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment](https://openreview.net/forum?id=kGvXIlIVLM) |  | 0 | Classifier-Free Guidance (CFG) is a critical technique for enhancing the sample quality of visual generative models. However, in autoregressive (AR) multi-modal generation, CFG introduces design inconsistencies between language and visual content, contradicting the design philosophy of unifying different modalities for visual AR. Motivated by language model alignment methods, we propose Condition Contrastive Alignment (CCA) to facilitate guidance-free AR visual generation. Unlike guidance methods that alter the sampling process to achieve the ideal sampling distribution, CCA directly fine-tunes pretrained models to fit the same distribution target. Experimental results show that CCA can significantly enhance the guidance-free performance of all tested models with just one epoch of fine-tuning (1% of pretraining epochs) on the pretraining dataset. This largely removes the need for guided sampling in AR visual generation and cuts the sampling cost by half. Moreover, by adjusting training parameters, CCA can achieve trade-offs between sample diversity and fidelity similar to CFG. This experimentally confirms the strong theoretical connection between language-targeted alignment and visual-targeted guidance methods, unifying two previously independent research fields. | Huayu Chen, Hang Su, Peize Sun, Jun Zhu |  |
| 58 |  |  [TimeMixer++: A General Time Series Pattern Machine for Universal Predictive Analysis](https://openreview.net/forum?id=1CLzLXSFNn) |  | 0 | Time series analysis plays a critical role in numerous applications, supporting tasks such as forecasting, classification, anomaly detection, and imputation. In this work, we present the time series pattern machine (TSPM), a model designed to excel in a broad range of time series tasks through powerful representation and pattern extraction capabilities. Traditional time series models often struggle to capture universal patterns, limiting their effectiveness across diverse tasks. To address this, we define multiple scales in the time domain and various resolutions in the frequency domain, employing various mixing strategies to extract intricate, task-adaptive time series patterns. Specifically, we introduce TimeMixer++, a general-purpose TSPM that processes multi-scale time series using (1) multi-resolution time imaging (MRTI), (2) time image decomposition (TID), (3) multi-scale mixing (MCM), and (4) multi-resolution mixing (MRM) to extract comprehensive temporal patterns. MRTI transforms multi-scale time series into multi-resolution time images, capturing patterns across both temporal and frequency domains. TID leverages dual-axis attention to extract seasonal and trend patterns, while MCM hierarchically aggregates these patterns across scales. MRM adaptively integrates all representations across resolutions. TimeMixer++ achieves state-of-the-art performance across 8 time series analytical tasks, consistently surpassing both general-purpose and task-specific models. Our work marks a promising step toward the next generation of TSPMs, paving the way for further advancements in time series analysis. | Shiyu Wang, Jiawei Li, Xiaoming Shi, Zhou Ye, Baichuan Mo, Wenze Lin, Shengtong Ju, Zhixuan Chu, Ming Jin |  |
| 59 |  |  [RMP-SAM: Towards Real-Time Multi-Purpose Segment Anything](https://openreview.net/forum?id=1pXzC30ry5) |  | 0 | Recent segmentation methods, which adopt large-scale data training and transformer architecture, aim to create one foundation model that can perform multiple tasks. However, most of these methods rely on heavy encoder and decoder frameworks, hindering their performance in real-time scenarios. To explore real-time segmentation, recent advancements primarily focus on semantic segmentation within specific environments, such as autonomous driving. However, they often overlook the generalization ability of these models across diverse scenarios. Therefore, to fill this gap, this work explores a novel real-time segmentation setting called real-time multi-purpose segmentation. It contains three fundamental sub-tasks: interactive segmentation, panoptic segmentation, and video instance segmentation. Unlike previous methods, which use a specific design for each task, we aim to use only a single end-to-end model to accomplish all these tasks in real-time. To meet real-time requirements and balance multi-task learning, we present a novel dynamic convolution-based method, Real-Time Multi-Purpose SAM (RMP-SAM). It contains an efficient encoder and an efficient decoupled adapter to perform prompt-driven decoding. Moreover, we further explore different training strategies and one new adapter design to boost co-training performance further. We benchmark several strong baselines by extending existing works to support our multi-purpose segmentation. Extensive experiments demonstrate that RMP-SAM is effective and generalizes well on proposed benchmarks and other specific semantic tasks. Our implementation of RMP-SAM achieves the optimal balance between accuracy and speed for these tasks. The code is released at \url{https://github.com/xushilin1/RAP-SAM} | Shilin Xu, Haobo Yuan, Qingyu Shi, Lu Qi, Jingbo Wang, Yibo Yang, Yining Li, Kai Chen, Yunhai Tong, Bernard Ghanem, Xiangtai Li, MingHsuan Yang |  |
| 60 |  |  [Steering Protein Family Design through Profile Bayesian Flow](https://openreview.net/forum?id=PSiijdQjNU) |  | 0 | Protein family design emerges as a promising alternative by combining the advantages of de novo protein design and mutation-based directed evolution.In this paper, we propose ProfileBFN, the Profile Bayesian Flow Networks, for specifically generative modeling of protein families. ProfileBFN extends the discrete Bayesian Flow Network from an MSA profile perspective, which can be trained on single protein sequences by regarding it as a degenerate profile, thereby achieving efficient protein family design by avoiding large-scale MSA data construction and training. Empirical results show that ProfileBFN has a profound understanding of proteins. When generating diverse and novel family proteins, it can accurately capture the structural characteristics of the family. The enzyme produced by this method is more likely than the previous approach to have the corresponding function, offering better odds of generating diverse proteins with the desired functionality. | Jingjing Gong, Yu Pei, Siyu Long, Yuxuan Song, Zhe Zhang, Wenhao Huang, Ziyao Cao, Shuyi Zhang, Hao Zhou, WeiYing Ma |  |
| 61 |  |  [GeSubNet: Gene Interaction Inference for Disease Subtype Network Generation](https://openreview.net/forum?id=ja4rpheN2n) |  | 0 | Retrieving gene functional networks from knowledge databases presents a challenge due to the mismatch between disease networks and subtype-specific variations. Current solutions, including statistical and deep learning methods, often fail to effectively integrate gene interaction knowledge from databases or explicitly learn subtype-specific interactions. To address this mismatch, we propose GeSubNet, which learns a unified representation capable of predicting gene interactions while distinguishing between different disease subtypes. Graphs generated by such representations can be considered subtype-specific networks. GeSubNet is a multi-step representation learning framework with three modules: First, a deep generative model learns distinct disease subtypes from patient gene expression profiles. Second, a graph neural network captures representations of prior gene networks from knowledge databases, ensuring accurate physical gene interactions. Finally, we integrate these two representations using an inference loss that leverages graph generation capabilities, conditioned on the patient separation loss, to refine subtype-specific information in the learned representation. GeSubNet consistently outperforms traditional methods, with average improvements of 30.6%, 21.0%, 20.1%, and 56.6% across four graph evaluation metrics, averaged over four cancer datasets. Particularly, we conduct a biological simulation experiment to assess how the behavior of selected genes from over 11,000 candidates affects subtypes or patient distributions. The results show that the generated network has the potential to identify subtype-specific genes with an 83% likelihood of impacting patient distribution shifts. | Ziwei Yang, Zheng Chen, Xin Liu, Rikuto Kotoge, Peng Chen, Yasuko Matsubara, Yasushi Sakurai, Jimeng Sun |  |
| 62 |  |  [Exploring The Loss Landscape Of Regularized Neural Networks Via Convex Duality](https://openreview.net/forum?id=4xWQS2z77v) |  | 0 | We discuss several aspects of the loss landscape of regularized neural networks: the structure of stationary points, connectivity of optimal solutions, path with non-increasing loss to arbitrary global optimum, and the nonuniqueness of optimal solutions, by casting the problem into an equivalent convex problem and considering its dual. Starting from two-layer neural networks with scalar output, we first characterize the solution set of the convex problem using its dual and further characterize all stationary points. With the characterization, we show that the topology of the global optima goes through a phase transition as the width of the network changes, and construct counterexamples where the problem may have a continuum of optimal solutions. Finally, we show that the solution set characterization and connectivity results can be extended to different architectures, including two layer vector-valued neural networks and parallel three-layer neural networks. | Sungyoon Kim, Aaron Mishkin, Mert Pilanci |  |
| 63 |  |  [Global Convergence in Neural ODEs: Impact of Activation Functions](https://openreview.net/forum?id=AoraWUmpLU) |  | 0 | Neural Ordinary Differential Equations (ODEs) have been successful in various applications due to their continuous nature and parameter-sharing efficiency. However, these unique characteristics also introduce challenges in training, particularly with respect to gradient computation accuracy and convergence analysis. In this paper, we address these challenges by investigating the impact of activation functions. We demonstrate that the properties of activation functions—specifically smoothness and nonlinearity—are critical to the training dynamics. Smooth activation functions guarantee globally unique solutions for both forward and backward ODEs, while sufficient nonlinearity is essential for maintaining the spectral properties of the Neural Tangent Kernel (NTK) during training. Together, these properties enable us to establish the global convergence of Neural ODEs under gradient descent in overparameterized regimes. Our theoretical findings are validated by numerical experiments, which not only support our analysis but also provide practical guidelines for scaling Neural ODEs, potentially leading to faster training and improved performance in real-world applications. | Tianxiang Gao, Siyuan Sun, Hailiang Liu, Hongyang Gao |  |
| 64 |  |  [MoDeGPT: Modular Decomposition for Large Language Model Compression](https://openreview.net/forum?id=8EfxjTCg2k) |  | 0 | Large Language Models (LLMs) have significantly advanced AI with their exceptional performance across a wide range of tasks. However, their extensive computational requirements restrict their use on devices with limited resources. While recent compression methods based on low-rank matrices show potential solutions, they often suffer from significant loss of accuracy or introduce substantial overhead in parameters and inference time. In this paper, we introduce Modular De- composition (MoDeGPT), a new, efficient, and structured compression framework that overcomes these limitations. MoDeGPT jointly decomposes pairs of consecu- tive subcomponents within Transformer blocks, reduces hidden dimensions through output reconstruction on a larger structural scale than conventional low-rank meth- ods, and repurposes three classical matrix decomposition algorithms—Nyström approximation, CR decomposition, and SVD—to ensure bounded errors in our novel decomposition approach. Our experiments show that MoDeGPT, without relying on backward propagation, consistently matches or surpasses the performance of prior techniques that depend on gradient information, while achieving a 98% reduction in compute costs when compressing a 13B-parameter model. On LLaMA-2/3 and OPT models, MoDeGPT retains 90-95% of zero-shot performance with compression rates of 25-30%. The compression process can be completed on a single GPU in a few hours, boosting inference throughput by up to 46%. | ChiHeng Lin, Shangqian Gao, James Seale Smith, Abhishek Patel, Shikhar Tuli, Yilin Shen, Hongxia Jin, YenChang Hsu |  |
| 65 |  |  [MIND over Body: Adaptive Thinking using Dynamic Computation](https://openreview.net/forum?id=EjJGND0m1x) |  | 0 | While the human brain efficiently handles various computations with a limited number of neurons, traditional deep learning networks require a significant increase in parameters to improve performance. Yet, these parameters are used inefficiently as the networks employ the same amount of computation for inputs of the same size, regardless of the input's complexity. We address this inefficiency by introducing self-introspection capabilities to the network, enabling it to adjust the number of used parameters based on the internal representation of the task and adapt the computation time based on the task complexity. This enables the network to adaptively reuse parameters across tasks, dynamically adjusting the computational effort to match the complexity of the input. We demonstrate the effectiveness of this method on language modeling and computer vision tasks. Notably, our model achieves 96.62\% accuracy on ImageNet with just a three-layer network, surpassing much larger ResNet-50 and EfficientNet. When applied to a transformer architecture, the approach achieves 95.8\%/88.7\% F1 scores on the SQuAD v1.1/v2.0 datasets at negligible parameter cost. These results showcase the potential for dynamic and reflective computation, contributing to the creation of intelligent systems that efficiently manage resources based on input data complexity. | Mrinal Mathur, Barak A. Pearlmutter, Sergey M. Plis |  |
| 66 |  |  [From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions](https://openreview.net/forum?id=QKBu1BOAwd) |  | 0 | Tool learning enables Large Language Models (LLMs) to interact with external environments by invoking tools, serving as an effective strategy to mitigate the limitations inherent in their pre-training data. In this process, tool documentation plays a crucial role by providing usage instructions for LLMs, thereby facilitating effective tool utilization. This paper concentrates on the critical challenge of bridging the comprehension gap between LLMs and external tools due to the inadequacies and inaccuracies inherent in existing human-centric tool documentation. We propose a novel framework, DRAFT, aimed at Dynamically Refining tool documentation through the Analysis of Feedback and Trials emanating from LLMs' interactions with external tools. This methodology pivots on an innovative trial-and-error approach, consisting of three distinct learning phases: experience gathering, learning from experience, and documentation rewriting, to iteratively enhance the tool documentation. This process is further optimized by implementing a diversity-promoting exploration strategy to ensure explorative diversity and a tool-adaptive termination mechanism to prevent overfitting while enhancing efficiency. Extensive experiments on multiple datasets demonstrate that DRAFT's iterative, feedback-based refinement significantly ameliorates documentation quality, fostering a deeper comprehension and more effective utilization of tools by LLMs. Notably, our analysis reveals that the tool documentation refined via our approach demonstrates robust cross-model generalization capabilities. | Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, JiRong Wen |  |
| 67 |  |  [LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization](https://openreview.net/forum?id=VpWki1v2P8) |  | 0 | Low-rank adaption (LoRA) is a widely used parameter-efficient finetuning method for LLM that reduces memory requirements. However, current LoRA optimizers lack transformation invariance, meaning the updates depending on how the two LoRA factors are scaled or rotated. This deficiency leads to inefficient learning and sub-optimal solutions in practice. This paper introduces LoRA-RITE, a novel adaptive matrix preconditioning method for LoRA optimization, which can achieve transformation invariance and remain computationally efficient. We provide theoretical analysis to demonstrate the benefit of our method and conduct experiments on various LLM tasks with different models including Gemma 2B, 7B, and mT5-XXL. The results demonstrate consistent improvements against existing optimizers. For example, replacing Adam with LoRA-RITE during LoRA fine-tuning of Gemma-2B yielded 4.6% accuracy gain on Super-Natural Instructions and 3.5% accuracy gain across other four LLM benchmarks (HellaSwag, ArcChallenge, GSM8K, OpenBookQA). | JuiNan Yen, Si Si, Zhao Meng, Felix X. Yu, Sai Surya Duvvuri, Inderjit S. Dhillon, ChoJui Hsieh, Sanjiv Kumar |  |
| 68 |  |  [Scaling and evaluating sparse autoencoders](https://openreview.net/forum?id=tcsZt9ZNKD) |  | 0 | Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents. We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, we find modifications that result in few dead latents, even at the largest scales we tried. Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity. We also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects. These metrics all generally improve with autoencoder size. To demonstrate the scalability of our approach, we train a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens. We release training code and autoencoders for open-source models, as well as a visualizer. | Leo Gao, Tom Dupré la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, Jeffrey Wu |  |
| 69 |  |  [ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement](https://openreview.net/forum?id=YUYJsHOf3c) |  | 0 | Post-training Large Language Models (LLMs) with explicit reasoning trajectories can enhance their reasoning abilities. However, acquiring such high-quality trajectory data typically demands meticulous supervision from humans or superior models, which can be either expensive or license-constrained. In this paper, we explore how far an LLM can improve its reasoning by self-synthesizing reasoning paths as training data without any additional supervision. Existing self-synthesizing methods, such as STaR, suffer from poor generalization to out-of-domain (OOD) reasoning tasks. We hypothesize it is due to that their self-synthesized reasoning paths are too task-specific, lacking general task-agnostic reasoning guidance. To address this, we propose \*\*Reasoning Generalist via Self-Improvement (ReGenesis)\*\*, a method to \*self-synthesize reasoning paths as post-training data by progressing from abstract to concrete\*. More specifically, ReGenesis self-synthesizes reasoning paths by converting general reasoning guidelines into task-specific ones, generating reasoning structures, and subsequently transforming these structures into reasoning paths, without the need for human-designed task-specific examples used in existing methods. We show that ReGenesis achieves superior performance on all in-domain and OOD settings tested compared to existing methods. For six OOD tasks specifically, while previous methods exhibited an average performance decrease of approximately 4.6% after post training, ReGenesis delivers around 6.1% performance improvement. We also conduct an in-depth analysis of our framework and show ReGenesis is effective across various language models and design choices. | Xiangyu Peng, Congying Xia, Xinyi Yang, Caiming Xiong, ChienSheng Wu, Chen Xing |  |
| 70 |  |  [Feedback Favors the Generalization of Neural ODEs](https://openreview.net/forum?id=cmfyMV45XO) |  | 0 | The well-known generalization problem hinders the application of artificial neural networks in continuous-time prediction tasks with varying latent dynamics. In sharp contrast, biological systems can neatly adapt to evolving environments benefiting from real-time feedback mechanisms. Inspired by the feedback philosophy, we present feedback neural networks, showing that a feedback loop can flexibly correct the learned latent dynamics of neural ordinary differential equations (neural ODEs), leading to a prominent generalization improvement. The feedback neural network is a novel two-DOF neural network, which possesses robust performance in unseen scenarios with no loss of accuracy performance on previous tasks. A linear feedback form is presented to correct the learned latent dynamics firstly, with a convergence guarantee. Then, domain randomization is utilized to learn a nonlinear neural feedback form. Finally, extensive tests including trajectory prediction of a real irregular object and model predictive control of a quadrotor with various uncertainties, are implemented, indicating significant improvements over state-of-the-art model-based and learning-based methods. | Jindou Jia, Zihan Yang, Meng Wang, Kexin Guo, Jianfei Yang, Xiang Yu, Lei Guo |  |
| 71 |  |  [Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs](https://openreview.net/forum?id=QWunLKbBGF) |  | 0 | Large Language Models (LLMs) are increasingly deployed as chatbots, yet their ability to personalize responses to user preferences remains limited. We introduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize and adhere to user preferences in long-context conversational setting. PrefEval comprises 3,000 manually curated user preference and query pairs spanning 20 topics. PrefEval contains user personalization or preference information in both explicit and implicit preference forms, and evaluates LLM performance using a generation and a classification task. With PrefEval, we have evaluated 10 open-sourced and proprietary LLMs in multi-session conversations with varying context lengths up to 100k tokens. We benchmark with various prompting, iterative feedback, and retrieval-augmented generation methods. Our benchmarking effort reveals that state-of-the-art LLMs face significant challenges in following users' preference during conversations. In particular, in zero-shot settings, preference following accuracy falls below 10\% at merely 10 turns (~3k tokens) across most evaluated models. Even with advanced prompting and retrieval methods, preference following still deteriorates in long-context conversations. Furthermore, we show that fine-tuning on PrefEval significantly improves performance. We believe PrefEval serves as a valuable resource for measuring, understanding, and enhancing LLMs' proactive preference following abilities, paving the way for personalized conversational agents. | Siyan Zhao, Mingyi Hong, Yang Liu, Devamanyu Hazarika, Kaixiang Lin |  |
| 72 |  |  [STAR: Synthesis of Tailored Architectures](https://openreview.net/forum?id=HsHxSN23rM) |  | 0 | Iterative improvement of model architectures is fundamental to deep learning: Transformers first enabled scaling, and recent advances in model hybridization have pushed the quality-efficiency frontier. However, optimizing architectures remains challenging and expensive, with a variety of automated or manual approaches that fall short, due to limited progress in the design of search spaces and due to the simplicity of resulting patterns and heuristics. In this work, we propose a new approach for the synthesis of tailored architectures (STAR). Our approach combines a novel search space based on the theory of linear input-varying systems, supporting a hierarchical numerical encoding into architecture genomes. STAR genomes are automatically refined and recombined with gradient-free, evolutionary algorithms to optimize for multiple model quality and efficiency metrics. Using STAR, we optimize large populations of new architectures, leveraging diverse computational units and interconnection patterns, improving over highly-optimized Transformers and striped hybrid models on the frontier of quality, parameter size, and inference cache for autoregressive language modeling. | Armin W. Thomas, Rom N. Parnichkun, Alexander Amini, Stefano Massaroli, Michael Poli |  |
| 73 |  |  [Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models](https://openreview.net/forum?id=WCRQFlji2q) |  | 0 | Hallucinations in large language models are a widespread problem, yet the mechanisms behind whether models will hallucinate are poorly understood, limiting our ability to solve this problem. Using sparse autoencoders as an interpretability tool, we discover that a key part of these mechanisms is entity recognition, where the model detects if an entity is one it can recall facts about. Sparse autoencoders uncover meaningful directions in the representation space, these detect whether the model recognizes an entity, e.g. detecting it doesn't know about an athlete or a movie. This shows that models can have self-knowledge: internal representations about their own capabilities. These directions are causally relevant: capable of steering the model to refuse to answer questions about known entities, or to hallucinate attributes of unknown entities when it would otherwise refuse. We demonstrate that despite the sparse autoencoders being trained on the base model, these directions have a causal effect on the chat model's refusal behavior, suggesting that chat finetuning has repurposed this existing mechanism. Furthermore, we provide an initial exploration into the mechanistic role of these directions in the model, finding that they disrupt the attention of downstream heads that typically move entity attributes to the final token. | Javier Ferrando, Oscar Balcells Obeso, Senthooran Rajamanoharan, Neel Nanda |  |
| 74 |  |  [Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces](https://openreview.net/forum?id=AP0ndQloqR) |  | 0 | Advances in reinforcement learning (RL) have led to its successful application in complex tasks with continuous state and action spaces. Despite these advances in practice, most theoretical work pertains to finite state and action spaces. We propose building a theoretical understanding of continuous state and action spaces by employing a geometric lens to understand the locally attained set of states. The set of all parametrised policies learnt through a semi-gradient based approach induce a set of attainable states in RL. We show that training dynamics of a two layer neural policy induce a low dimensional manifold of attainable states embedded in the high-dimensional nominal state space trained using an actor-critic algorithm. We prove that, under certain conditions, the dimensionality of this manifold is of the order of the dimensionality of the action space. This is the first result of its kind, linking the geometry of the state space to the dimensionality of the action space. We empirically corroborate this upper bound for four MuJoCo environments and also demonstrate the results in a toy environment with varying dimensionality. We also show the applicability of this theoretical result by introducing a local manifold learning layer to the policy and value function networks to improve the performance in control environments with very high degrees of freedom by changing one layer of the neural network to learn sparse representations. | Saket Tiwari, Omer Gottesman, George Konidaris |  |
| 75 |  |  [When is Task Vector Provably Effective for Model Editing? A Generalization Analysis of Nonlinear Transformers](https://openreview.net/forum?id=vRvVVb0NAz) |  | 0 | Task arithmetic refers to editing the pre-trained model by adding a weighted sum of task vectors, each of which is the weight update from the pre-trained model to fine-tuned models for certain tasks. This approach recently gained attention as a computationally efficient inference method for model editing, e.g., multi-task learning, forgetting, and out-of-domain generalization capabilities. However, the theoretical understanding of why task vectors can execute various conceptual operations remains limited, due to the highly non-convexity of training Transformer-based models. To the best of our knowledge, this paper provides the first theoretical characterization of the generalization guarantees of task vector methods on nonlinear Transformers. We consider a conceptual learning setting, where each task is a binary classification problem based on a discriminative pattern. We theoretically prove the effectiveness of task addition in simultaneously learning a set of irrelevant or aligned tasks, as well as the success of task negation in unlearning one task from irrelevant or contradictory tasks. Moreover, we prove the proper selection of linear coefficients for task arithmetic to achieve guaranteed generalization to out-of-domain tasks. All of our theoretical results hold for both dense-weight parameters and their low-rank approximations. Although established in a conceptual setting, our theoretical findings were validated on a practical machine unlearning task using the large language model Phi-1.5 (1.3B). | Hongkang Li, Yihua Zhang, Shuai Zhang, PinYu Chen, Sijia Liu, Meng Wang |  |
| 76 |  |  [Learning and aligning single-neuron invariance manifolds in visual cortex](https://openreview.net/forum?id=kbjJ9ZOakb) |  | 0 | Understanding how sensory neurons exhibit selectivity to certain features and invariance to others is central to uncovering the computational principles underlying robustness and generalization in visual perception. Most existing methods for characterizing selectivity and invariance identify single or finite discrete sets of stimuli. Since these are only isolated measurements from an underlying continuous manifold, characterizing invariance properties accurately and comparing them across neurons with varying receptive field size, position, and orientation, becomes challenging. Consequently, a systematic analysis of invariance types at the population level remains under-explored. Building on recent advances in learning continuous invariance manifolds, we introduce a novel method to accurately identify and align invariance manifolds of visual sensory neurons, overcoming these challenges. Our approach first learns the continuous invariance manifold of stimuli that maximally excite a neuron modeled by a response-predicting deep neural network. It then learns an affine transformation on the pixel coordinates such that the same manifold activates another neuron as strongly as possible, effectively aligning their invariance manifolds spatially. This alignment provides a principled way to quantify and compare neuronal invariances irrespective of receptive field differences. Using simulated neurons, we demonstrate that our method accurately learns and aligns known invariance manifolds, robustly identifying functional clusters. When applied to macaque V1 neurons, it reveals functional clusters of neurons, including simple and complex cells. Overall, our method enables systematic, quantitative exploration of the neural invariance landscape, to gain new insights into the functional properties of visual sensory neurons. | Mohammad Bashiri, Luca Baroni, Ján Antolík, Fabian H. Sinz |  |
| 77 |  |  [Feedback Schrödinger Bridge Matching](https://openreview.net/forum?id=k3tbMMW8rH) |  | 0 | Recent advancements in diffusion bridges for distribution transport problems have heavily relied on matching frameworks, yet existing methods often face a trade-off between scalability and access to optimal pairings during training. Fully unsupervised methods make minimal assumptions but incur high computational costs, limiting their practicality. On the other hand, imposing full supervision of the matching process with optimal pairings improves scalability, however, it can be infeasible in most applications. To strike a balance between scalability and minimal supervision, we introduce Feedback Schrödinger Bridge Matching (FSBM), a novel semi-supervised matching framework that incorporates a small portion ($<8$% of the entire dataset) of pre-aligned pairs as state feedback to guide the transport map of non-coupled samples, thereby significantly improving efficiency. This is achieved by formulating a static Entropic Optimal Transport (EOT) problem with an additional term capturing the semi-supervised guidance. The generalized EOT objective is then recast into a dynamic formulation to leverage the scalability of matching frameworks. Extensive experiments demonstrate that FSBM accelerates training and enhances generalization by leveraging coupled pairs' guidance, opening new avenues for training matching frameworks with partially aligned datasets. | Panagiotis Theodoropoulos, Nikolaos Komianos, Vincent Pacelli, GuanHorng Liu, Evangelos A. Theodorou |  |
| 78 |  |  [TetSphere Splatting: Representing High-Quality Geometry with Lagrangian Volumetric Meshes](https://openreview.net/forum?id=8enWnd6Gp3) |  | 0 | We introduce TetSphere Splatting, a Lagrangian geometry representation designed for high-quality 3D shape modeling. TetSphere splatting leverages an underused yet powerful geometric primitive -- volumetric tetrahedral meshes. It represents 3D shapes by deforming a collection of tetrahedral spheres, with geometric regularizations and constraints that effectively resolve common mesh issues such as irregular triangles, non-manifoldness, and floating artifacts. Experimental results on multi-view and single-view reconstruction highlight TetSphere splatting's superior mesh quality while maintaining competitive reconstruction accuracy compared to state-of-the-art methods. Additionally, TetSphere splatting demonstrates versatility by seamlessly integrating into generative modeling tasks, such as image-to-3D and text-to-3D generation. | Minghao Guo, Bohan Wang, Kaiming He, Wojciech Matusik |  |
| 79 |  |  [Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents](https://openreview.net/forum?id=kxnoqaisCT) |  | 0 | Multimodal large language models (MLLMs) are transforming the capabilities of graphical user interface (GUI) agents, facilitating their transition from controlled simulations to complex, real-world applications across various platforms. However, the effectiveness of these agents hinges on the robustness of their grounding capability. Current GUI agents predominantly utilize text-based representations such as HTML or accessibility trees, which, despite their utility, often introduce noise, incompleteness, and increased computational overhead. In this paper, we advocate a human-like embodiment for GUI agents that perceive the environment entirely visually and directly perform pixel-level operations on the GUI. The key is visual grounding models that can accurately map diverse referring expressions of GUI elements to their coordinates on the GUI across different platforms. We show that a simple recipe, which includes web-based synthetic data and slight adaptation of the LLaVA architecture, is surprisingly effective for training such visual grounding models. We collect the largest dataset for GUI visual grounding so far, containing 10M GUI elements and their referring expressions over 1.3M screenshots, and use it to train UGround, a strong universal visual grounding model for GUI agents. Empirical results on six benchmarks spanning three categories (grounding, offline agent, and online agent) show that 1) UGround substantially outperforms existing visual grounding models for GUI agents, by up to 20\% absolute, and 2) agents with UGround outperform state-of-the-art agents, despite the fact that existing agents use additional text-based input while ours only uses visual perception. These results provide strong support for the feasibility and promises of GUI agents that navigate the digital world as humans do. | Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, Yu Su |  |
| 80 |  |  [Progressive distillation induces an implicit curriculum](https://openreview.net/forum?id=wPMRwmytZe) |  | 0 | Knowledge distillation leverages a teacher model to improve the training of a student model. A persistent challenge is that a better teacher does not always yield a better student, to which a common mitigation is to use additional supervision from several “intermediate” teachers. One empirically validated variant of this principle is progressive distillation, where the student learns from successive intermediate checkpoints of the teacher. Using sparse parity as a sandbox, we identify an implicit curriculum as one mechanism through which progressive distillation accelerates the student’s learning. This curriculum is available only through the intermediate checkpoints but not the final converged one, and imparts both empirical acceleration and a provable sample complexity benefit to the student. We then extend our investigation to Transformers trained on probabilistic context-free grammars (PCFGs) and real-world pre-training datasets (Wikipedia and Books). Through probing the teacher model, we identify an analogous implicit curriculum where the model progressively learns features that capture longer context. Our theoretical and empirical findings on sparse parity, complemented by empirical observations on more complex tasks, highlight the benefit of progressive distillation via implicit curriculum across setups. | Abhishek Panigrahi, Bingbin Liu, Sadhika Malladi, Andrej Risteski, Surbhi Goel |  |
| 81 |  |  [Rethinking Reward Modeling in Preference-based Large Language Model Alignment](https://openreview.net/forum?id=rfdblE10qm) |  | 0 | The Bradley-Terry (BT) model is a common and successful practice in reward modeling for Large Language Model (LLM) alignment. However, it remains unclear \*why\* this model --- originally developed for multi-player stochastic game matching --- can be adopted to convert pairwise response comparisons to reward values and make predictions. Especially given the fact that only a limited number of prompt-response pairs are sparsely compared with others. In this paper, we first establish the convergence rate of BT reward models based on deep neural networks using embeddings, providing a theoretical foundation for their use. Despite theoretically sound, we argue that the BT model is not a necessary choice from the perspective of downstream optimization, this is because a reward model only needs to preserve the correct ranking predictions through a monotonic transformation of the true reward. We highlight the critical concept of \*order consistency\* in reward modeling and demonstrate that the BT model possesses this property. Moreover, we propose a simple and straightforward upper-bound algorithm, compatible with off-the-shelf binary classifiers, as an alternative order-consistent reward modeling objective. To offer practical insights, we empirically evaluate the performance of these different reward modeling approaches across more than 12,000 experimental setups, using $6$ base LLMs, $2$ datasets, and diverse annotation designs that vary in quantity, quality, and pairing choices in preference annotations. | Hao Sun, Yunyi Shen, JeanFrancois Ton |  |
| 82 |  |  [Copyright-Protected Language Generation via Adaptive Model Fusion](https://openreview.net/forum?id=kRoWeLTpL4) |  | 0 | The risk of language models reproducing copyrighted material from their training data has led to the development of various protective measures. Among these, inference-time strategies that impose constraints via post-processing have shown promise in addressing the complexities of copyright regulation. However, they often incur prohibitive computational costs or suffer from performance trade-offs. To overcome these limitations, we introduce Copyright-Protecting Model Fusion (CP-Fuse), a novel approach that combines models trained on disjoint sets of copyrighted material during inference. In particular, CP-Fuse adaptively aggregates the model outputs to minimize the reproduction of copyrighted content, adhering to a crucial balancing property to prevent the regurgitation of memorized data. Through extensive experiments, we show that CP-Fuse significantly reduces the reproduction of protected material without compromising the quality of text and code generation. Moreover, its post-hoc nature allows seamless integration with other protective measures, further enhancing copyright safeguards. Lastly, we show that CP-Fuse is robust against common techniques for extracting training data. | Javier Abad, Konstantin Donhauser, Francesco Pinto, Fanny Yang |  |
| 83 |  |  [Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse Controls to Any Diffusion Model](https://openreview.net/forum?id=ny8T8OuNHe) |  | 0 | ControlNets are widely used for adding spatial control to text-to-image diffusion models. However, when it comes to controllable video generation, ControlNets cannot be directly integrated into new backbones due to feature space mismatches, and training ControlNets for new backbones can be a significant burden for many users. Furthermore, applying ControlNets independently to different frames can not effectively maintain object temporal consistency. To address these challenges, we introduce Ctrl-Adapter, an efficient and versatile framework that adds diverse controls to any image/video diffusion models through the adaptation of pretrained ControlNets. Ctrl-Adapter offers strong and diverse capabilities, including image and video control, sparse-frame video control, fine-grained patch-level multi-condition control, zero-shot adaptation to unseen conditions, and supports a variety of downstream tasks beyond spatial control, including video editing, video style transfer, and text-guided motion control. With six diverse U-Net/DiT-based image/video diffusion models (SDXL, PixArt-α, I2VGen-XL, SVD, Latte, Hotshot-XL), Ctrl-Adapter matches the performance of pretrained ControlNets on COCO and achieves the state-of-the-art on DAVIS 2017 with significantly lower computation (< 10 GPU hours). | Han Lin, Jaemin Cho, Abhay Zala, Mohit Bansal |  |
| 84 |  |  [BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models](https://openreview.net/forum?id=fAAaT826Vv) |  | 0 | Predictive models often need to work with incomplete information in real-world tasks. Consequently, they must provide reliable probability or confidence estimation, especially in large-scale decision-making and planning tasks. Current large language models (LLMs) are insufficient for accurate estimations, but they can generate relevant factors that may affect the probabilities, produce coarse-grained probabilities when the information is more complete, and help determine which factors are relevant to specific downstream contexts. In this paper, we make use of these capabilities of LLMs to provide a significantly more accurate probabilistic estimation. We propose BIRD, a novel probabilistic inference framework that aligns a Bayesian network with LLM abductions and then estimates more accurate probabilities in a deduction step. We show BIRD provides reliable probability estimations that are 30% better than those provided directly by LLM baselines. These estimates further contribute to better and more trustworthy decision making. | Yu Feng, Ben Zhou, Weidong Lin, Dan Roth |  |
| 85 |  |  [LaMPlace: Learning to Optimize Cross-Stage Metrics in Macro Placement](https://openreview.net/forum?id=YLIsIzC74j) |  | 0 | Machine learning techniques have shown great potential in enhancing macro placement, a critical stage in modern chip design. However, existing methods primarily focus on \*online\* optimization of \*intermediate surrogate metrics\* that are available at the current placement stage, rather than directly targeting the \*cross-stage metrics\*---such as the timing performance---that measure the final chip quality. This is mainly because of the high computational costs associated with performing post-placement stages for evaluating such metrics, making the \*online\* optimization impractical. Consequently, these optimizations struggle to align with actual performance improvements and can even lead to severe manufacturing issues. To bridge this gap, we propose \*\*LaMPlace\*\*, which \*\*L\*\*earns \*\*a\*\* \*\*M\*\*ask for optimizing cross-stage metrics in macro placement. Specifically, LaMPlace trains a predictor on \*offline\* data to estimate these \*cross-stage metrics\* and then leverages the predictor to quickly generate a mask, i.e., a pixel-level feature map that quantifies the impact of placing a macro in each chip grid location on the design metrics. This mask essentially acts as a fast evaluator, enabling placement decisions based on \*cross-stage metrics\* rather than \*intermediate surrogate metrics\*. Experiments on commonly used benchmarks demonstrate that LaMPlace significantly improves the chip quality across several key design metrics, achieving an average improvement of 9.6\%, notably 43.0\% and 30.4\% in terms of WNS and TNS, respectively, which are two crucial cross-stage metrics that reflect the final chip quality in terms of the timing performance. | Zijie Geng, Jie Wang, Ziyan Liu, Siyuan Xu, Zhentao Tang, Shixiong Kai, Mingxuan Yuan, Jianye Hao, Feng Wu |  |
| 86 |  |  [miniCTX: Neural Theorem Proving with (Long-)Contexts](https://openreview.net/forum?id=KIgaAqEFHW) |  | 0 | Real-world formal theorem proving often depends on a wealth of context, including definitions, lemmas, comments, file structure, and other information. We introduce $\texttt{miniCTX}$, which tests a model's ability to prove formal mathematical theorems that depend on new context that is not seen during training. $\texttt{miniCTX}$ contains theorems sourced from real Lean projects and textbooks, each associated with a context that can span tens of thousands of tokens. Models are tasked with proving a theorem given access to code from the theorem's repository, which contains context that is needed for the proof. As a baseline for $\texttt{miniCTX}$, we tested fine-tuning and prompting methods that condition theorem proving on preceding context. Both approaches substantially outperform traditional methods that rely solely on state information. We found that this ability to use context is not captured by previous benchmarks such as $\texttt{miniF2F}$. Alongside $\texttt{miniCTX}$, we offer $\texttt{ntp-toolkit}$ for automatically extracting and annotating theorem proving data, making it easy to add new projects into $\texttt{miniCTX}$ to ensure that contexts are not seen during training. $\texttt{miniCTX}$ offers a challenging and realistic evaluation of neural theorem provers. | Jiewen Hu, Thomas Zhu, Sean Welleck |  |
| 87 |  |  [BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions](https://openreview.net/forum?id=YrycTjllL0) |  | 0 | Task automation has been greatly empowered by the recent advances in Large Language Models (LLMs) via Python code, where the tasks range from software engineering development to general-purpose reasoning. While current benchmarks have shown that LLMs can solve tasks using programs like human developers, the majority of their evaluations are limited to short and self-contained algorithmic tasks or standalone function calls. Solving challenging and practical tasks requires the capability of utilizing \*\*diverse function calls as tools\*\* to efficiently implement functionalities like data analysis and web development. In addition, using multiple tools to solve a task needs compositional reasoning by accurately understanding \*\*complex instructions\*\*. Fulfilling both of these characteristics can pose a great challenge for LLMs. To assess how well LLMs can solve challenging and practical tasks via programs, we introduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained tasks. To evaluate LLMs rigorously, each task encompasses 5.6 test cases with an average branch coverage of 99%. In addition, we propose a natural-language-oriented variant of BigCodeBench, BigCodeBench-Instruct, that automatically transforms the original docstrings into short instructions containing only essential information. Our extensive evaluation of 60 LLMs shows that \*\*LLMs are not yet capable of following complex instructions to use function calls precisely, with scores up to 60%, significantly lower than the human performance of 97%\*\*. The results underscore the need for further advancements in this area. | Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, James Hoang, Armel Randy Zebaze, Xiaoheng Hong, WenDing Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, et al. |  |
| 88 |  |  [Towards a Complete Logical Framework for GNN Expressiveness](https://openreview.net/forum?id=pqOjj90Vwp) |  | 0 | Designing expressive Graph neural networks (GNNs) is an important topic in graph machine learning fields. Traditionally, the Weisfeiler-Lehman (WL) test has been the primary measure for evaluating GNN expressiveness. However, high-order WL tests can be obscure, making it challenging to discern the specific graph patterns captured by them. Given the connection between WL tests and first-order logic, some have explored the logical expressiveness of Message Passing Neural Networks. This paper aims to establish a comprehensive and systematic relationship between GNNs and logic. We propose a framework for identifying the equivalent logical formulas for arbitrary GNN architectures, which not only explains existing models, but also provides inspiration for future research. As case studies, we analyze multiple classes of prominent GNNs within this framework, unifying different subareas of the field. Additionally, we conduct a detailed examination of homomorphism expressivity from a logical perspective and present a general method for determining the homomorphism expressivity of arbitrary GNN models, as well as addressing several open problems. | Tuo Xu |  |
| 89 |  |  [Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think](https://openreview.net/forum?id=DJSZGGZYVi) |  | 0 | Recent studies have shown that the denoising process in (generative) diffusion models can induce meaningful (discriminative) representations inside the model, though the quality of these representations still lags behind those learned through recent self-supervised learning methods. We argue that one main bottleneck in training large-scale diffusion models for generation lies in effectively learning these representations. Moreover, training can be made easier by incorporating high-quality external visual representations, rather than relying solely on the diffusion models to learn them independently. We study this by introducing a straightforward regularization called REPresentation Alignment (REPA), which aligns the projections of noisy input hidden states in denoising networks with clean image representations obtained from external, pretrained visual encoders. The results are striking: our simple strategy yields significant improvements in both training efficiency and generation quality when applied to popular diffusion and flow-based transformers, such as DiTs and SiTs. For instance, our method can speed up SiT training by over 17.5$\times$, matching the performance (without classifier-free guidance) of a SiT-XL model trained for 7M steps in less than 400K steps. In terms of final generation quality, our approach achieves state-of-the-art results of FID=1.42 using classifier-free guidance with the guidance interval. | Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, Saining Xie |  |
| 90 |  |  [Classic but Everlasting: Traditional Gradient-Based Algorithms Converge Fast Even in Time-Varying Multi-Player Games](https://openreview.net/forum?id=t8FG4cJuL3) |  | 0 | Last-iterate convergence behaviours of well-known algorithms are intensively investigated in various games, such as two-player bilinear zero-sum games. However, most known last-iterate convergence properties rely on strict settings where the underlying games must have time-invariant payoffs. Besides, the limited known attempts on the games with time-varying payoffs are in two-player bilinear time-varying zero-sum games and strictly monotone games. By contrast, in other time-varying games, the last-iterate behaviours of two classic algorithms, i.e., extra gradient (EG) and optimistic gradient (OG) algorithms, still lack research, especially the convergence rates in multi-player games. In this paper, we investigate the last-iterate behaviours of EG and OG algorithms for convergent perturbed games, which extend upon the usual model of time-invariant games and incorporate external factors, such as vanishing noises. Using the recently proposed notion of the tangent residual (or its modifications) as the potential function of games and the measure of proximity to the Nash equilibrium, we prove that the last-iterate convergence rates of EG and OG algorithms for perturbed games on bounded convex closed sets are $O({1}/{\sqrt{T}})$ if such games converge to monotone games at rates fast enough and that such a result holds true for certain unconstrained perturbed games. With this result, we address an open question asking for the last-iterate convergence rate of EG and OG algorithms in constrained and time-varying settings. The above convergence rates are similar to known tight results on corresponding time-invariant games. | Yanzheng Chen, Jun Yu |  |
| 91 |  |  [DSPO: Direct Score Preference Optimization for Diffusion Model Alignment](https://openreview.net/forum?id=xyfb9HHvMe) |  | 0 | Diffusion-based Text-to-Image (T2I) models have achieved impressive success in generating high-quality images from textual prompts. While large language models (LLMs) effectively leverage Direct Preference Optimization (DPO) for fine-tuning on human preference data without the need for reward models, diffusion models have not been extensively explored in this area. Current preference learning methods applied to T2I diffusion models immediately adapt existing techniques from LLMs. However, this direct adaptation introduces an estimated loss specific to T2I diffusion models. This estimation can potentially lead to suboptimal performance through our empirical results. In this work, we propose Direct Score Preference Optimization (DSPO), a novel algorithm that aligns the pretraining and fine-tuning objectives of diffusion models by leveraging score matching, the same objective used during pretraining. It introduces a new perspective on preference learning for diffusion models. Specifically, DSPO distills the score function of human-preferred image distributions into pretrained diffusion models, fine-tuning the model to generate outputs that align with human preferences. We theoretically show that DSPO shares the same optimization direction as reinforcement learning algorithms in diffusion models under certain conditions. Our experimental results demonstrate that DSPO outperforms preference learning baselines for T2I diffusion models in human preference evaluation tasks and enhances both visual appeal and prompt alignment of generated images. | Huaisheng Zhu, Teng Xiao, Vasant G. Honavar |  |
| 92 |  |  [TANGO: Co-Speech Gesture Video Reenactment with Hierarchical Audio Motion Embedding and Diffusion Interpolation](https://openreview.net/forum?id=LbEWwJOufy) |  | 0 | We present TANGO, a framework for generating co-speech body-gesture videos. Given a few-minute, single-speaker reference video and target speech audio, TANGO produces high-fidelity videos with synchronized body gestures. TANGO builds on Gesture Video Reenactment (GVR), which splits and retrieves video clips using a directed graph structure - representing video frames as nodes and valid transitions as edges. We address two key limitations of GVR: audio-motion misalignment and visual artifacts in GAN-generated transition frames. In particular, i) we propose retrieving gestures using latent feature distance to improve cross-modal alignment. To ensure the latent features could effectively model the relationship between speech audio and gesture motion, we implement a hierarchical joint embedding space (AuMoClip); ii) we introduce the diffusion-based model to generate high-quality transition frames. Our diffusion model, Appearance Consistent Interpolation (ACInterp), is built upon AnimateAnyone and includes a reference motion module and homography background flow to preserve appearance consistency between generated and reference videos. By integrating these components into the graph-based retrieval framework, TANGO reliably produces realistic, audio-synchronized videos and outperforms all existing generative and retrieval methods. Our code, pretrained models, and datasets are publicly available at https://github.com/CyberAgentAILab/TANGO. | Haiyang Liu, Xingchao Yang, Tomoya Akiyama, Yuantian Huang, Qiaoge Li, Shigeru Kuriyama, Takafumi Taketomi |  |
| 93 |  |  [Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation](https://openreview.net/forum?id=meRCKuUpmc) |  | 0 | Current efforts to learn scalable policies in robotic manipulation primarily fall into two categories: one focuses on "action," which involves behavior cloning from extensive collections of robotic data, while the other emphasizes "vision," enhancing model generalization by pre-training representations or generative models, also referred to as world models, using large-scale visual datasets. This paper presents an end-to-end paradigm that predicts actions using inverse dynamics models conditioned on the robot's forecasted visual states, named Predictive Inverse Dynamics Models (PIDM). By closing the loop between vision and action, the end-to-end PIDM can be a better scalable action learner. In practice, we use Transformers to process both visual states and actions, naming the model Seer. It is initially pre-trained on large-scale robotic datasets, such as DROID, and can be adapted to real-world scenarios with a little fine-tuning data. Thanks to large-scale, end-to-end training and the continuous synergy between vision and action at each execution step, Seer significantly outperforms state-of-the-art methods across both simulation and real-world experiments. It achieves improvements of 13% on the LIBERO-LONG benchmark, 22% on CALVIN ABC-D, and 43% in real-world tasks. Notably, it demonstrates superior generalization for novel objects, lighting conditions, and environments under high-intensity disturbances. Code and models will be publicly available. | Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, Jiangmiao Pang |  |
| 94 |  |  [The Complexity of Two-Team Polymatrix Games with Independent Adversaries](https://openreview.net/forum?id=9VGTk2NYjF) |  | 0 | Adversarial multiplayer games are an important object of study in multiagent learning. In particular, polymatrix zero-sum games are a multiplayer setting where Nash equilibria are known to be efficiently computable. Towards understanding the limits of tractability in polymatrix games, we study the computation of Nash equilibria in such games where each pair of players plays either a zero-sum or a coordination game. We are particularly interested in the setting where players can be grouped into a small number of teams of identical interest. While the three-team version of the problem is known to be PPAD-complete, the complexity for two teams has remained open. Our main contribution is to prove that the two-team version remains hard, namely it is CLS-hard. Furthermore, we show that this lower bound is tight for the setting where one of the teams consists of multiple independent adversaries. On the way to obtaining our main result, we prove hardness of finding any stationary point in the simplest type of non-convex-concave min-max constrained optimization problem, namely for a class of bilinear polynomial objective functions. | Alexandros Hollender, Gilbert Maystre, Sai Ganesh Nagarajan |  |
| 95 |  |  [MMQA: Evaluating LLMs with Multi-Table Multi-Hop Complex Questions](https://openreview.net/forum?id=GGlpykXDCa) |  | 0 | While large language models (LLMs) have made strides in understanding tabular data, current tabular evaluation benchmarks, such as WikiTableQuestions and WikiSQL, are focus on single-table scenarios, which cannot necessarily reflect the complexity of real-world applications. To bridge this gap, we present a \textbf{M}ulti-table and Multi-hop Question Answering (MMQA) dataset to assess LLMs' understanding and reasoning capabilities in handling multi-table tasks. The MMQA dataset demands that models perform multiple inferences by drawing evidence from various tables, which are designed to be connected with each other and require models to identify and utilize relationships such as foreign and primary keys. Then, we introduce a comprehensive evaluation framework that tailors to assess LLMs' capabilities in several aspects including Multi-Table Retrieval, Text-to-SQL Generation, Multi-Table QA, Primary Key Selection, and Foreign Key Selection. Finally, we propose a novel multi-table retrieval method that achieves state-of-the-art (SOTA) performance on the MMQA dataset compared to several strong baselines. Our experiment results reveal that, compared with human performance, both open-source and commercial LLMs leave significant performance room for improvements in multi-table understanding and reasoning tasks. We believe that the MMQA benchmark will enhance and facilitate LLMs' multi-table capabilities in real-world scenarios. | Jian Wu, Linyi Yang, Dongyuan Li, Yuliang Ji, Manabu Okumura, Yue Zhang |  |
| 96 |  |  [On Scaling Up 3D Gaussian Splatting Training](https://openreview.net/forum?id=pQqeQpMkE7) |  | 0 | 3D Gaussian Splatting (3DGS) is increasingly popular for 3D reconstruction due to its superior visual quality and rendering speed. However, 3DGS training currently occurs on a single GPU, limiting its ability to handle high-resolution and large-scale 3D reconstruction tasks due to memory constraints. We introduce Grendel, a distributed system designed to partition 3DGS parameters and parallelize computation across multiple GPUs. As each Gaussian affects a small, dynamic subset of rendered pixels, Grendel employs sparse all-to-all communication to transfer the necessary Gaussians to pixel partitions and performs dynamic load balancing. Unlike existing 3DGS systems that train using one camera view image at a time, Grendel supports batched training with multiple views. We explore various optimization hyperparameter scaling strategies and find that a simple sqrt(batch-size) scaling rule is highly effective. Evaluations using large-scale, high-resolution scenes show that Grendel enhances rendering quality by scaling up 3DGS parameters across multiple GPUs. On the 4K \`\`Rubble'' dataset, we achieve a test PSNR of 27.28 by distributing 40.4 million Gaussians across 16 GPU, compared to a PSNR of 26.28 using 11.2 million Gaussians on a single GPU. Grendel is an open-source project available at: https://github.com/nyu-systems/Grendel-GS | Hexu Zhao, Haoyang Weng, Daohan Lu, Ang Li, Jinyang Li, Aurojit Panda, Saining Xie |  |
| 97 |  |  [Emergence of meta-stable clustering in mean-field transformer models](https://openreview.net/forum?id=eBS3dQQ8GV) |  | 0 | We model the evolution of tokens within a deep stack of Transformer layers as a continuous-time flow on the unit sphere, governed by a mean-field interacting particle system, building on the framework introduced in Geshkovski et al. (2023). Studying the corresponding mean-field Partial Differential Equation (PDE), which can be interpreted as a Wasserstein gradient flow, in this paper we provide a mathematical investigation of the long-term behavior of this system, with a particular focus on the emergence and persistence of meta-stable phases and clustering phenomena, key elements in applications like next-token prediction. More specifically, we perform a perturbative analysis of the mean-field PDE around the iid uniform initialization and prove that, in the limit of large number of tokens, the model remains close to a meta-stable manifold of solutions with a given structure (e.g., periodicity). Further, the structure characterizing the meta-stable manifold is explicitly identified, as a function of the inverse temperature parameter of the model, by the index maximizing a certain rescaling of Gegenbauer polynomials. | Giuseppe Bruno, Federico Pasqualotto, Andrea Agazzi |  |
| 98 |  |  [Wide Neural Networks Trained with Weight Decay Provably Exhibit Neural Collapse](https://openreview.net/forum?id=1HCN4pjTb4) |  | 0 | Deep neural networks (DNNs) at convergence consistently represent the training data in the last layer via a geometric structure referred to as neural collapse. This empirical evidence has spurred a line of theoretical research aimed at proving the emergence of neural collapse, mostly focusing on the unconstrained features model. Here, the features of the penultimate layer are free variables, which makes the model data-agnostic and puts into question its ability to capture DNN training. Our work addresses the issue, moving away from unconstrained features and studying DNNs that end with at least two linear layers. We first prove generic guarantees on neural collapse that assume \emph{(i)} low training error and balancedness of linear layers (for within-class variability collapse), and \emph{(ii)} bounded conditioning of the features before the linear part (for orthogonality of class-means, and their alignment with weight matrices). The balancedness refers to the fact that $W_{\ell+1}^\top W_{\ell+1}\approx W_\ell W_\ell ^\top$ for any pair of consecutive weight matrices of the linear part, and the bounded conditioning requires a well-behaved ratio between largest and smallest non-zero singular values of the features. We then show that such assumptions hold for gradient descent training with weight decay: \emph{(i)} for networks with a wide first layer, we prove low training error and balancedness, and \emph{(ii)} for solutions that are either nearly optimal or stable under large learning rates, we additionally prove the bounded conditioning. Taken together, our results are the first to show neural collapse in the end-to-end training of DNNs. | Arthur Jacot, Peter Súkeník, Zihan Wang, Marco Mondelli |  |
| 99 |  |  [ECD: A Machine Learning Benchmark for Predicting Enhanced-Precision Electronic Charge Density in Crystalline Inorganic Materials](https://openreview.net/forum?id=SBCMNc3Mq3) |  | 0 | Supervised machine learning techniques are increasingly being adopted to speed up electronic structure predictions, serving as alternatives to first-principles methods like Density Functional Theory (DFT). Although current DFT datasets mainly emphasize chemical properties and atomic forces, the precise prediction of electronic charge density is essential for accurately determining a system's total energy and ground state properties. In this study, we introduce a novel electronic charge density dataset named ECD, which encompasses 140,646 stable crystal geometries with medium-precision Perdew–Burke–Ernzerhof (PBE) functional data. Within this dataset, a subset of 7,147 geometries includes high-precision electronic charge density data calculated using the Heyd–Scuseria–Ernzerhof (HSE) functional in DFT. By designing various benchmark tasks for crystalline materials and emphasizing training with large-scale PBE data while fine-tuning with a smaller subset of high-precision HSE data, we demonstrate the efficacy of current machine learning models in predicting electronic charge densities. The ECD dataset and baseline models are open-sourced to support community efforts in developing new methodologies and accelerating materials design and applications. | Pin Chen, Zexin Xu, Qing Mo, Hongjin Zhong, Fengyang Xu, Yutong Lu |  |
| 100 |  |  [On the Benefits of Memory for Modeling Time-Dependent PDEs](https://openreview.net/forum?id=o9kqa5K3tB) |  | 0 | Data-driven techniques have emerged as a promising alternative to traditional numerical methods for solving PDEs. For time-dependent PDEs, many approaches are Markovian---the evolution of the trained system only depends on the current state, and not the past states. In this work, we investigate the benefits of using memory for modeling time-dependent PDEs: that is, when past states are explicitly used to predict the future. Motivated by the Mori-Zwanzig theory of model reduction, we theoretically exhibit examples of simple (even linear) PDEs, in which a solution that uses memory is arbitrarily better than a Markovian solution. Additionally, we introduce Memory Neural Operator (MemNO), a neural operator architecture that combines recent state space models (specifically, S4) and Fourier Neural Operators (FNOs) to effectively model memory. We empirically demonstrate that when the PDEs are supplied in low resolution or contain observation noise at train and test time, MemNO significantly outperforms the baselines without memory---with up to $6 \times$ reduction in test error. Furthermore, we show that this benefit is particularly pronounced when the PDE solutions have significant high-frequency Fourier modes (e.g., low-viscosity fluid dynamics) and we construct a challenging benchmark dataset consisting of such PDEs. | Ricardo Buitrago Ruiz, Tanya Marwah, Albert Gu, Andrej Risteski |  |
| 101 |  |  [SD-LoRA: Scalable Decoupled Low-Rank Adaptation for Class Incremental Learning](https://openreview.net/forum?id=5U1rlpX68A) |  | 0 | Continual Learning (CL) with foundation models has recently emerged as a promising paradigm to exploit abundant knowledge acquired during pre-training for tackling sequential tasks. However, existing prompt-based and Low-Rank Adaptation-based (LoRA-based) methods often require expanding a prompt/LoRA pool or retaining samples of previous tasks, which poses significant scalability challenges as the number of tasks grows. To address these limitations, we propose Scalable Decoupled LoRA (SD-LoRA) for class incremental learning, which continually separates the learning of the magnitude and direction of LoRA components without rehearsal. Our empirical and theoretical analysis reveals that SD-LoRA tends to follow a low-loss trajectory and converges to an overlapping low-loss region for all learned tasks, resulting in an excellent stability-plasticity trade-off. Building upon these insights, we introduce two variants of SD-LoRA with further improved parameter efficiency. All parameters of SD-LoRAs can be end-to-end optimized for CL objectives. Meanwhile, they support efficient inference by allowing direct evaluation with the finally trained model, obviating the need for component selection. Extensive experiments across multiple CL benchmarks and foundation models consistently validate the effectiveness of SD-LoRA. The code is available at https://github.com/WuYichen-97/SD-Lora-CL. | Yichen Wu, Hongming Piao, LongKai Huang, Renzhen Wang, Wanhua Li, Hanspeter Pfister, Deyu Meng, Kede Ma, Ying Wei |  |
| 102 |  |  [Improving Probabilistic Diffusion Models With Optimal Diagonal Covariance Matching](https://openreview.net/forum?id=fV0t65OBUu) |  | 0 | The probabilistic diffusion model has become highly effective across various domains. Typically, sampling from a diffusion model involves using a denoising distribution characterized by a Gaussian with a learned mean and either fixed or learned covariances. In this paper, we leverage the recently proposed covariance moment matching technique and introduce a novel method for learning the diagonal covariances. Unlike traditional data-driven covariance approximation approaches, our method involves directly regressing the optimal analytic covariance using a new, unbiased objective named Optimal Covariance Matching (OCM). This approach can significantly reduce the approximation error in covariance prediction. We demonstrate how our method can substantially enhance the sampling efficiency, recall rate and likelihood of both diffusion models and latent diffusion models. | Zijing Ou, Mingtian Zhang, Andi Zhang, Tim Z. Xiao, Yingzhen Li, David Barber |  |
| 103 |  |  [PathGen-1.6M: 1.6 Million Pathology Image-text Pairs Generation through Multi-agent Collaboration](https://openreview.net/forum?id=rFpZnn11gj) |  | 0 | Vision Language Models (VLMs) like CLIP have attracted substantial attention in pathology, serving as backbones for applications such as zero-shot image classification and Whole Slide Image (WSI) analysis. Additionally, they can function as vision encoders when combined with large language models (LLMs) to support broader capabilities. Current efforts to train pathology VLMs rely on pathology image-text pairs from platforms like PubMed, YouTube, and Twitter, which provide limited, unscalable data with generally suboptimal image quality. In this work, we leverage large-scale WSI datasets like TCGA to extract numerous high-quality image patches. We then train a large multimodal model (LMM) to generate captions for extracted images, creating PathGen-1.6M, a dataset containing 1.6 million high-quality image-caption pairs. Our approach involves multiple agent models collaborating to extract representative WSI patches, generating and refining captions to obtain high-quality image-text pairs. Extensive experiments show that integrating these generated pairs with existing datasets to train a pathology-specific CLIP model, PathGen-CLIP, significantly enhances its ability to analyze pathological images, with substantial improvements across nine pathology-related zero-shot image classification tasks and three whole-slide image tasks. Furthermore, we construct 200K instruction-tuning data based on PathGen-1.6M and integrate PathGen-CLIP with the Vicuna LLM to create more powerful multimodal models through instruction tuning. Overall, we provide a scalable pathway for high-quality data generation in pathology, paving the way for next-generation general pathology models. Our dataset, code, and model are open-access at https://github.com/PathFoundation/PathGen-1.6M. | Yuxuan Sun, Yunlong Zhang, Yixuan Si, Chenglu Zhu, Kai Zhang, Zhongyi Shui, Jingxiong Li, Xuan Gong, Xinheng Lyu, Tao Lin, Lin Yang |  |
| 104 |  |  [Training on the Test Task Confounds Evaluation and Emergence](https://openreview.net/forum?id=jOmk0uS1hl) |  | 0 | We study a fundamental problem in the evaluation of large language models that we call training on the test task. Unlike wrongful practices like training on the test data, leakage, or data contamination, training on the test task is not a malpractice. Rather, the term describes a growing set of techniques to include task-relevant data in the pretraining stage of a language model. We demonstrate that training on the test task confounds both relative model evaluations and claims about emergent capabilities. We argue that the seeming superiority of one model family over another may be explained by a different degree of training on the test task. To this end, we propose an effective method to adjust for the effect of training on the test task on benchmark evaluations. Put simply, to fine-tune each model under comparison on the same task-relevant data before evaluation. Lastly, we show that instances of emergent behavior disappear gradually as models train on the test task. Our work promotes a new perspective on the evaluation of large language models with broad implications for benchmarking and the study of emergent capabilities. | Ricardo DominguezOlmedo, Florian E. Dorner, Moritz Hardt |  |
| 105 |  |  [Subgraph Federated Learning for Local Generalization](https://openreview.net/forum?id=cH65nS5sOz) |  | 0 | Federated Learning (FL) on graphs enables collaborative model training to enhance performance without compromising the privacy of each client. However, existing methods often overlook the mutable nature of graph data, which frequently introduces new nodes and leads to shifts in label distribution. Since they focus solely on performing well on each client's local data, they are prone to overfitting to their local distributions (i.e., local overfitting), which hinders their ability to generalize to unseen data with diverse label distributions. In contrast, our proposed method, FedLoG, effectively tackles this issue by mitigating local overfitting. Our model generates global synthetic data by condensing the reliable information from each class representation and its structural information across clients. Using these synthetic data as a training set, we alleviate the local overfitting problem by adaptively generalizing the absent knowledge within each local dataset. This enhances the generalization capabilities of local models, enabling them to handle unseen data effectively. Our model outperforms baselines in our proposed experimental settings, which are designed to measure generalization power to unseen data in practical scenarios. Our code is available at https://github.com/sung-won-kim/FedLoG | Sungwon Kim, Yoonho Lee, Yunhak Oh, Namkyeong Lee, Sukwon Yun, Junseok Lee, Sein Kim, Carl Yang, Chanyoung Park |  |
| 106 |  |  [A Probabilistic Perspective on Unlearning and Alignment for Large Language Models](https://openreview.net/forum?id=51WraMid8K) |  | 0 | Comprehensive evaluation of Large Language Models (LLMs) is an open research problem. Existing evaluations rely on deterministic point estimates generated via greedy decoding. However, we find that deterministic evaluations fail to capture the whole output distribution of a model, yielding inaccurate estimations of model capabilities. This is particularly problematic in critical contexts such as unlearning and alignment, where precise model evaluations are crucial. To remedy this, we introduce the first formal probabilistic evaluation framework for LLMs. Namely, we propose novel metrics with high probability guarantees concerning the output distribution of a model. Our metrics are application-independent and allow practitioners to make more reliable estimates about model capabilities before deployment. Our experimental analysis reveals that deterministic evaluations falsely indicate successful unlearning and alignment, whereas our probabilistic evaluations better capture model capabilities. We show how to overcome challenges associated with probabilistic outputs in a case study on unlearning by introducing (1) a novel loss based on entropy optimization, and (2) adaptive temperature scaling. We demonstrate that our approach significantly enhances unlearning in probabilistic settings on recent benchmarks. Overall, our proposed shift from point estimates to probabilistic evaluations of output distributions represents an important step toward comprehensive evaluations of LLMs. | Yan Scholten, Stephan Günnemann, Leo Schwinn |  |
| 107 |  |  [MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering](https://openreview.net/forum?id=6s5uXNWGIh) |  | 0 | We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, we curate 75 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models, preparing datasets, and running experiments. We establish human baselines for each competition using Kaggle's publicly available leaderboards. We use open-source agent scaffolds to evaluate several frontier language models on our benchmark, finding that the best-performing setup — OpenAI's o1-preview with AIDE scaffolding — achieves at least the level of a Kaggle bronze medal in 16.9% of competitions. In addition to our main results, we investigate various forms of resource-scaling for AI agents and the impact of contamination from pre-training. We open-source our benchmark code https://github.com/openai/mle-bench to facilitate future research in understanding the ML engineering capabilities of AI agents. | Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Aleksander Madry, Lilian Weng |  |
| 108 |  |  [Learning Randomized Algorithms with Transformers](https://openreview.net/forum?id=UV5p3JZMjC) |  | 0 | Randomization is a powerful tool that endows algorithms with remarkable properties. For instance, randomized algorithms excel in adversarial settings, often surpassing the worst-case performance of deterministic algorithms with large margins. Furthermore, their success probability can be amplified by simple strategies such as repetition and majority voting. In this paper, we enhance deep neural networks, in particular transformer models, with randomization. We demonstrate for the first time that randomized algorithms can be instilled in transformers through learning, in a purely data- and objective-driven manner. First, we analyze known adversarial objectives for which randomized algorithms offer a distinct advantage over deterministic ones. We then show that common optimization techniques, such as gradient descent or evolutionary strategies, can effectively learn transformer parameters that make use of the randomness provided to the model. To illustrate the broad applicability of randomization in empowering neural networks, we study three conceptual tasks: associative recall, graph coloring, and agents that explore grid worlds. In addition to demonstrating increased robustness against oblivious adversaries through learned randomization, our experiments reveal remarkable performance improvements due to the inherently random nature of the neural networks' computation and predictions. | Johannes von Oswald, Seijin Kobayashi, Yassir Akram, Angelika Steger |  |
| 109 |  |  [Data Scaling Laws in Imitation Learning for Robotic Manipulation](https://openreview.net/forum?id=pISLZG7ktL) |  | 0 | Data scaling has revolutionized fields like natural language processing and computer vision, providing models with remarkable generalization capabilities. In this paper, we investigate whether similar data scaling laws exist in robotics, particularly in robotic manipulation, and whether appropriate data scaling can yield single-task robot policies that can be deployed zero-shot for any object within the same category in any environment. To this end, we conduct a comprehensive empirical study on data scaling in imitation learning. By collecting data across numerous environments and objects, we study how a policy’s generalization performance changes with the number of training environments, objects, and demonstrations. Throughout our research, we collect over 40,000 demonstrations and execute more than 15,000 real-world robot rollouts under a rigorous evaluation protocol. Our findings reveal several intriguing results: the generalization performance of the policy follows a roughly power-law relationship with the number of environments and objects. The diversity of environments and objects is far more important than the absolute number of demonstrations; once the number of demonstrations per environment or object reaches a certain threshold, additional demonstrations have minimal effect. Based on these insights, we propose an efficient data collection strategy. With four data collectors working for one afternoon, we collect sufficient data to enable the policies for two tasks to achieve approximately 90\% success rates in novel environments with unseen objects. | Fanqi Lin, Yingdong Hu, Pingyue Sheng, Chuan Wen, Jiacheng You, Yang Gao |  |
| 110 |  |  [Amortized Control of Continuous State Space Feynman-Kac Model for Irregular Time Series](https://openreview.net/forum?id=8zJRon6k5v) |  | 0 | Many real-world datasets, such as healthcare, climate, and economics, are often collected as irregular time series, which poses challenges for accurate modeling. In this paper, we propose the Amortized Control of continuous State Space Model (ACSSM) for continuous dynamical modeling of time series for irregular and discrete observations. We first present a multi-marginal Doob's $h$-transform to construct a continuous dynamical system conditioned on these irregular observations. Following this, we introduce a variational inference algorithm with a tight evidence lower bound (ELBO), leveraging stochastic optimal control (SOC) theory to approximate the intractable Doob's $h$-transform and simulate the conditioned dynamics. To improve efficiency and scalability during both training and inference, ACSSM leverages auxiliary variable to flexibly parameterize the latent dynamics and amortized control. Additionally, it incorporates a simulation-free latent dynamics framework and a transformer-based data assimilation scheme, facilitating parallel inference of the latent states and ELBO computation. Through empirical evaluations across a variety of real-world datasets, ACSSM demonstrates superior performance in tasks such as classification, regression, interpolation, and extrapolation, while maintaining computational efficiency. | Byoungwoo Park, Hyungi Lee, Juho Lee |  |
| 111 |  |  [Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates](https://openreview.net/forum?id=syThiTmWWm) |  | 0 | Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and MT-Bench, have become popular for evaluating language models due to their cost-effectiveness and scalability compared to human evaluation. Achieving high win rates on these benchmarks can significantly boost the promotional impact of newly released language models. This promotional benefit may motivate tricks, such as manipulating model output length or style to game win rates, even though several mechanisms have been developed to control length and disentangle style to reduce gameability. Nonetheless, we show that even a \*\*"null model"\*\* that always outputs a \*\*constant\*\* response (\*irrelevant to input instructions\*) can cheat automatic benchmarks and achieve top-ranked win rates: an $86.5\\%$ LC win rate on AlpacaEval 2.0; an $83.0$ score on Arena-Hard-Auto; and a $9.55$ score on MT-Bench. Moreover, the crafted cheating outputs are \*\*transferable\*\* because we assume that the instructions of these benchmarks (e.g., $805$ samples of AlpacaEval 2.0) are \*private\* and cannot be accessed. While our experiments are primarily proof-of-concept, an adversary could use LLMs to generate more imperceptible cheating responses, unethically benefiting from high win rates and promotional impact. Our findings call for the development of anti-cheating mechanisms for reliable automatic benchmarks. The code is available at https://github.com/sail-sg/Cheating-LLM-Benchmarks. | Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Jing Jiang, Min Lin |  |
| 112 |  |  [On the Hölder Stability of Multiset and Graph Neural Networks](https://openreview.net/forum?id=P7KIGdgW8S) |  | 0 | Extensive research efforts have been put into characterizing and constructing maximally separating multiset and graph neural networks. However, recent empirical evidence suggests the notion of separation itself doesn't capture several interesting phenomena. On the one hand, the quality of this separation may be very weak, to the extent that the embeddings of "separable" objects might even be considered identical when using fixed finite precision. On the other hand, architectures which aren't capable of separation in theory, somehow achieve separation when taking the network to be wide enough. In this work, we address both of these issues, by proposing a novel pair-wise separation quality analysis framework which is based on an adaptation of Lipschitz and Hölder stability to parametric functions. The proposed framework, which we name Hölder in expectation, allows for separation quality analysis, without restricting the analysis to embeddings that can separate all the input space simultaneously. We prove that common sum-based models are lower-Hölder in expectation, with an exponent that decays rapidly with the network's depth . Our analysis leads to adversarial examples of graphs which can be separated by three 1-WL iterations, but cannot be separated in practice by standard maximally powerful Message Passing Neural Networks (MPNNs). To remedy this, we propose two novel MPNNs with improved separation quality, one of which is lower Lipschitz in expectation. We show these MPNNs can easily classify our adversarial examples, and compare favorably with standard MPNNs on standard graph learning tasks. | Yair Davidson, Nadav Dym |  |
| 113 |  |  [On Conformal Isometry of Grid Cells: Learning Distance-Preserving Position Embedding](https://openreview.net/forum?id=Xo0Q1N7CGk) |  | 0 | This paper investigates the conformal isometry hypothesis as a potential explanation for the hexagonal periodic patterns in grid cell response maps. We posit that grid cell activities form a high-dimensional vector in neural space, encoding the agent's position in 2D physical space. As the agent moves, this vector rotates within a 2D manifold in the neural space, driven by a recurrent neural network. The conformal hypothesis proposes that this neural manifold is a conformal isometric embedding of 2D physical space, where local physical distance is preserved by the embedding up to a scaling factor (or unit of metric). Such distance-preserving position embedding is indispensable for path planning in navigation, especially planning local straight path segments. We conduct numerical experiments to show that this hypothesis leads to the hexagonal grid firing patterns by learning maximally distance-preserving position embedding, agnostic to the choice of the recurrent neural network. Furthermore, we present a theoretical explanation of why hexagon periodic patterns emerge by minimizing our loss function by showing that hexagon flat torus is maximally distance preserving. | Dehong Xu, Ruiqi Gao, Wenhao Zhang, XueXin Wei, Ying Nian Wu |  |
| 114 |  |  [Combatting Dimensional Collapse in LLM Pre-Training Data via Submodular File Selection](https://openreview.net/forum?id=f4gF6AIHRy) |  | 0 | Selecting high-quality pre-training data for large language models (LLMs) is crucial for enhancing their overall performance under limited computation budget, improving both training and sample efficiency. Recent advancements in file selection primarily rely on using an existing or trained proxy model to assess the similarity of samples to a target domain, such as high quality sources BookCorpus and Wikipedia. However, upon revisiting these methods, the domain-similarity selection criteria demonstrates a diversity dilemma, i.e. dimensional collapse in the feature space, improving performance on the domain-related tasks but causing severe degradation on generic performance.To prevent collapse and enhance diversity, we propose a DiverSified File selection algorithm (DiSF), which selects the most decorrelated text files in the feature space. We approach this with a classical greedy algorithm to achieve more uniform eigenvalues in the feature covariance matrix of the selected texts, analyzing its approximation to the optimal solution under a formulation of $\gamma$-weakly submodular optimization problem. Empirically, we establish a benchmark and conduct extensive experiments on the TinyLlama architecture with models from 120M to 1.1B parameters. Evaluating across nine tasks from the Harness framework, DiSF demonstrates a significant improvement on overall performance. Specifically, DiSF saves 98.5\% of 590M training files in SlimPajama, outperforming the full-data pre-training within a 50B training budget, and achieving about 1.5x training efficiency and 5x data efficiency. Source code is available at: https://github.com/MediaBrain-SJTU/DiSF.git. | Ziqing Fan, Siyuan Du, Shengchao Hu, Pingjie Wang, Li Shen, Ya Zhang, Dacheng Tao, Yanfeng Wang |  |
| 115 |  |  [Population Transformer: Learning Population-level Representations of Neural Activity](https://openreview.net/forum?id=FVuqJt3c4L) |  | 0 | We present a self-supervised framework that learns population-level codes for arbitrary ensembles of neural recordings at scale. We address key challenges in scaling models with neural time-series data, namely, sparse and variable electrode distribution across subjects and datasets. The Population Transformer (PopT) stacks on top of pretrained temporal embeddings and enhances downstream decoding by enabling learned aggregation of multiple spatially-sparse data channels. The pretrained PopT lowers the amount of data required for downstream decoding experiments, while increasing accuracy, even on held-out subjects and tasks. Compared to end-to-end methods, this approach is computationally lightweight, while achieving similar or better decoding performance. We further show how our framework is generalizable to multiple time-series embeddings and neural data modalities. Beyond decoding, we interpret the pretrained and fine-tuned PopT models to show how they can be used to extract neuroscience insights from large amounts of data. We release our code as well as a pretrained PopT to enable off-the-shelf improvements in multi-channel intracranial data decoding and interpretability. Code is available at https://github.com/czlwang/PopulationTransformer. | Geeling Chau, Christopher Wang, Sabera J. Talukder, Vighnesh Subramaniam, Saraswati Soedarmadji, Yisong Yue, Boris Katz, Andrei Barbu |  |
| 116 |  |  [KAN: Kolmogorov-Arnold Networks](https://openreview.net/forum?id=Ozo7qJ5vZi) |  | 0 | Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes ("neurons''), KANs have learnable activation functions on edges ("weights''). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability, on small-scale AI + Science tasks. For accuracy, smaller KANs can achieve comparable or better accuracy than larger MLPs in function fitting tasks. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful \`\`collaborators'' helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs. Despite the slow training of KANs, their improved accuracy and interpretability show the potential to improve today's deep learning models which rely heavily on MLPs. More research is necessary to make KANs' training more efficient. | Ziming Liu, Yixuan Wang, Sachin Vaidya, Fabian Ruehle, James Halverson, Marin Soljacic, Thomas Y. Hou, Max Tegmark |  |
| 117 |  |  [Problem-Parameter-Free Federated Learning](https://openreview.net/forum?id=ZuazHmXTns) |  | 0 | Federated learning (FL) has garnered significant attention from academia and industry in recent years due to its advantages in data privacy, scalability, and communication efficiency. However, current FL algorithms face a critical limitation: their performance heavily depends on meticulously tuned hyperparameters, particularly the learning rate or stepsize. This manual tuning process is challenging in federated settings due to data heterogeneity and limited accessibility of local datasets. Consequently, the reliance on problem-specific parameters hinders the widespread adoption of FL and potentially compromises its performance in dynamic or diverse environments. To address this issue, we introduce PAdaMFed, a novel algorithm for nonconvex FL that carefully combines adaptive stepsize and momentum techniques. PAdaMFed offers two key advantages: 1) it operates autonomously without relying on problem-specific parameters; and 2) it manages data heterogeneity and partial participation without requiring heterogeneity bounds. Despite these benefits, PAdaMFed provides several strong theoretical guarantees: 1) It achieves state-of-the-art convergence rates with a sample complexity of $\mathcal{O}(\epsilon^{-4})$ and communication complexity of $\mathcal{O}(\epsilon^{-3})$ to obtain an accuracy of $\|\|\nabla f\left(\boldsymbol{\theta}\right)\|\| \leq \epsilon$, even using constant learning rates; 2) these complexities can be improved to the best-known $\mathcal{O}(\epsilon^{-3})$ for sampling and $\mathcal{O}(\epsilon^{-2})$ for communication when incorporating variance reduction; 3) it exhibits linear speedup with respect to the number of local update steps and participating clients at each global round. These attributes make PAdaMFed highly scalable and adaptable for various real-world FL applications. Extensive empirical evidence on both image classification and sentiment analysis tasks validates the efficacy of our approaches. | Wenjing Yan, Kai Zhang, Xiaolu Wang, Xuanyu Cao |  |
| 118 |  |  [SymmetricDiffusers: Learning Discrete Diffusion on Finite Symmetric Groups](https://openreview.net/forum?id=EO8xpnW7aX) |  | 0 | The group of permutations $S_n$, also known as the finite symmetric groups, are essential in fields such as combinatorics, physics, and chemistry. However, learning a probability distribution over $S_n$ poses significant challenges due to its intractable size and discrete nature. In this paper, we introduce \*SymmetricDiffusers\*, a novel discrete diffusion model that simplifies the task of learning a complicated distribution over $S_n$ by decomposing it into learning simpler transitions of the reverse diffusion using deep neural networks. We identify the riffle shuffle as an effective forward transition and provide empirical guidelines for selecting the diffusion length based on the theory of random walks on finite groups. Additionally, we propose a generalized Plackett-Luce (PL) distribution for the reverse transition, which is provably more expressive than the PL distribution. We further introduce a theoretically grounded "denoising schedule" to improve sampling and learning efficiency. Extensive experiments show that our model achieves state-of-the-art or comparable performance on solving tasks including sorting 4-digit MNIST images, jigsaw puzzles, and traveling salesman problems. Our code is released at <https://github.com/DSL-Lab/SymmetricDiffusers>. | Yongxing Zhang, Donglin Yang, Renjie Liao |  |
| 119 |  |  [Language Representations Can be What Recommenders Need: Findings and Potentials](https://openreview.net/forum?id=eIJfOIMN9z) |  | 0 | Recent studies empirically indicate that language models (LMs) encode rich world knowledge beyond mere semantics, attracting significant attention across various fields. However, in the recommendation domain, it remains uncertain whether LMs implicitly encode user preference information. Contrary to prevailing understanding that LMs and traditional recommenders learn two distinct representation spaces due to the huge gap in language and behavior modeling objectives, this work re-examines such understanding and explores extracting a recommendation space directly from the language representation space. Surprisingly, our findings demonstrate that item representations, when linearly mapped from advanced LM representations, yield superior recommendation performance. This outcome suggests the possible homomorphism between the advanced language representation space and an effective item representation space for recommendation, implying that collaborative signals may be implicitly encoded within LMs. Motivated by the finding of homomorphism, we explore the possibility of designing advanced collaborative filtering (CF) models purely based on language representations without ID-based embeddings. To be specific, we incorporate several crucial components (i.e., a multilayer perceptron (MLP), graph convolution, and contrastive learning (CL) loss function) to build a simple yet effective model, with the language representations of item textual metadata (i.e., title) as the input. Empirical results show that such a simple model can outperform leading ID-based CF models on multiple datasets, which sheds light on using language representations for better recommendation. Moreover, we systematically analyze this simple model and find several key features for using advanced language representations: a good initialization for item representations, superior zero-shot recommendation abilities in new datasets, and being aware of user intention. Our findings highlight the connection between language modeling and behavior modeling, which can inspire both natural language processing and recommender system communities. | Leheng Sheng, An Zhang, Yi Zhang, Yuxin Chen, Xiang Wang, TatSeng Chua |  |
| 120 |  |  [HiRA: Parameter-Efficient Hadamard High-Rank Adaptation for Large Language Models](https://openreview.net/forum?id=TwJrTz9cRS) |  | 0 | We propose Hadamard High-Rank Adaptation (HiRA), a parameter-efficient fine-tuning (PEFT) method that enhances the adaptability of Large Language Models (LLMs). While Low-rank Adaptation (LoRA) is widely used to reduce resource demands, its low-rank updates may limit its expressiveness for new tasks. HiRA addresses this by using a Hadamard product to retain high-rank update parameters, improving the model capacity. Empirically, HiRA outperforms LoRA and its variants on several tasks, with extensive ablation studies validating its effectiveness. Our code is available at https://github.com/hqsiswiliam/hira. | Qiushi Huang, Tom Ko, Zhan Zhuang, Lilian Tang, Yu Zhang |  |
| 121 |  |  [A Theoretically-Principled Sparse, Connected, and Rigid Graph Representation of Molecules](https://openreview.net/forum?id=OIvg3MqWX2) |  | 0 | Graph neural networks (GNNs) -- learn graph representations by exploiting the graph's sparsity, connectivity, and symmetries -- have become indispensable for learning geometric data like molecules. However, the most used graphs (e.g., radial cutoff graphs) in molecular modeling lack theoretical guarantees for achieving connectivity and sparsity simultaneously, which are essential for the performance and scalability of GNNs. Furthermore, existing widely used graph construction methods for molecules lack rigidity, limiting GNNs' ability to exploit graph nodes' spatial arrangement. In this paper, we introduce a new hyperparameter-free graph construction of molecules and beyond with sparsity, connectivity, and rigidity guarantees. Remarkably, our method consistently generates connected and sparse graphs with the edge-to-node ratio being bounded above by 3. Our graphs' rigidity guarantees that edge distances and dihedral angles are sufficient to uniquely determine the general spatial arrangements of atoms. We substantiate the effectiveness and efficiency of our proposed graphs in various molecular modeling benchmarks. Code is available at https://github.com/shihhsinwang0214/SCHull. | ShihHsin Wang, Yuhao Huang, Justin M. Baker, YuanEn Sun, Qi Tang, Bao Wang |  |
| 122 |  |  [How much of my dataset did you use? Quantitative Data Usage Inference in Machine Learning](https://openreview.net/forum?id=EUSkm2sVJ6) |  | 0 | How much of my data was used to train a machine learning model? This is a critical question for data owners assessing the risk of unauthorized usage of their data to train models. However, previous work mistakenly treats this as a binary problem—inferring whether all-or-none or any-or-none of the data was used—which is fragile when faced with real, non-binary data usage risks. To address this, we propose a fine-grained analysis called Dataset Usage Cardinality Inference (DUCI), which estimates the exact proportion of data used. Our algorithm, leveraging debiased membership guesses, matches the performance of the optimal MLE approach (with a maximum error <0.1) but with significantly lower (e.g., $300 \times$ less) computational cost. | Yao Tong, Jiayuan Ye, Sajjad Zarifzadeh, Reza Shokri |  |
| 123 |  |  [LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior](https://openreview.net/forum?id=Wr3UuEx72f) |  | 0 | We present LARP, a novel video tokenizer designed to overcome limitations in current video tokenization methods for autoregressive (AR) generative models. Unlike traditional patchwise tokenizers that directly encode local visual patches into discrete tokens, LARP introduces a holistic tokenization scheme that gathers information from the visual content using a set of learned holistic queries. This design allows LARP to capture more global and semantic representations, rather than being limited to local patch-level information. Furthermore, it offers flexibility by supporting an arbitrary number of discrete tokens, enabling adaptive and efficient tokenization based on the specific requirements of the task. To align the discrete token space with downstream AR generation tasks, LARP integrates a lightweight AR transformer as a training-time prior model that predicts the next token on its discrete latent space. By incorporating the prior model during training, LARP learns a latent space that is not only optimized for video reconstruction but is also structured in a way that is more conducive to autoregressive generation. Moreover, this process defines a sequential order for the discrete tokens, progressively pushing them toward an optimal configuration during training, ensuring smoother and more accurate AR generation at inference time. Comprehensive experiments demonstrate LARPs strong performance, achieving state-of-the-art FVD on the UCF101 class-conditional video generation benchmark. LARP enhances the compatibility of AR models with videos and opens up the potential to build unified high-fidelity multimodal large language models (MLLMs). Project page: https://hywang66.github.io/larp/ | Hanyu Wang, Saksham Suri, Yixuan Ren, Hao Chen, Abhinav Shrivastava |  |
| 124 |  |  [MOS: Model Synergy for Test-Time Adaptation on LiDAR-Based 3D Object Detection](https://openreview.net/forum?id=Y6aHdDNQYD) |  | 0 | LiDAR-based 3D object detection is crucial for various applications but often experiences performance degradation in real-world deployments due to domain shifts. While most studies focus on cross-dataset shifts, such as changes in environments and object geometries, practical corruptions from sensor variations and weather conditions remain underexplored. In this work, we propose a novel online test-time adaptation framework for 3D detectors that effectively tackles these shifts, including a challenging $\textit{cross-corruption}$ scenario where cross-dataset shifts and corruptions co-occur. By leveraging long-term knowledge from previous test batches, our approach mitigates catastrophic forgetting and adapts effectively to diverse shifts. Specifically, we propose a Model Synergy (MOS) strategy that dynamically selects historical checkpoints with diverse knowledge and assembles them to best accommodate the current test batch. This assembly is directed by our proposed Synergy Weights (SW), which perform a weighted averaging of the selected checkpoints, minimizing redundancy in the composite model. The SWs are computed by evaluating the similarity of predicted bounding boxes on the test data and the independence of features between checkpoint pairs in the model bank. To maintain an efficient and informative model bank, we discard checkpoints with the lowest average SW scores, replacing them with newly updated models. Our method was rigorously tested against existing test-time adaptation strategies across three datasets and eight types of corruptions, demonstrating superior adaptability to dynamic scenes and conditions. Notably, it achieved a 67.3% improvement in a challenging cross-corruption scenario, offering a more comprehensive benchmark for adaptation. Source code: https://github.com/zhuoxiao-chen/MOS. | Zhuoxiao Chen, Junjie Meng, Mahsa Baktashmotlagh, Yonggang Zhang, Zi Huang, Yadan Luo |  |
| 125 |  |  [Synthetic continued pretraining](https://openreview.net/forum?id=07yvxWDSla) |  | 0 | Pretraining on large-scale, unstructured internet text enables language models to acquire a significant amount of world knowledge. However, this knowledge acquisition is data-inefficient---to learn a fact, models must be trained on hundreds to thousands of diverse representations of it. This poses a challenge when adapting a pretrained model to a small corpus of domain-specific documents, where each fact may appear rarely or only once. We propose to bridge this gap with synthetic continued pretraining: using the small domain-specific corpus to synthesize a large corpus more amenable to learning, and then performing continued pretraining on the synthesized corpus. We instantiate this proposal with EntiGraph, a synthetic data augmentation algorithm that extracts salient entities from the source corpus and then generates diverse text by drawing connections between those entities. Synthetic continued pretraining with EntiGraph enables a language model to answer questions and follow generic instructions related to the source documents without access to them. If the source documents are instead available at inference time, we show that the knowledge acquired through our approach compounds with retrieval-augmented generation. To better understand these results, we build a simple mathematical model of EntiGraph, and show how synthetic data augmentation can "rearrange" knowledge to enable more data-efficient learning. | Zitong Yang, Neil Band, Shuangping Li, Emmanuel J. Candès, Tatsunori Hashimoto |  |
| 126 |  |  [EmbodiedSAM: Online Segment Any 3D Thing in Real Time](https://openreview.net/forum?id=XFYUwIyTxQ) |  | 0 | Embodied tasks require the agent to fully understand 3D scenes simultaneously with its exploration, so an online, real-time, fine-grained and highly-generalized 3D perception model is desperately needed. Since high-quality 3D data is limited, directly training such a model in 3D is infeasible. Meanwhile, vision foundation models (VFM) has revolutionized the field of 2D computer vision with superior performance, which makes the use of VFM to assist embodied 3D perception a promising direction. However, most existing VFM-assisted 3D perception methods are either offline or too slow that cannot be applied in practical embodied tasks. In this paper, we aim to leverage Segment Anything Model (SAM) for real-time 3D instance segmentation in an online setting. This is a challenging problem since future frames are not available in the input streaming RGB-D video, and an instance may be observed in several frames so efficient object matching between frames is required. To address these challenges, we first propose a geometric-aware query lifting module to represent the 2D masks generated by SAM by 3D-aware queries, which is then iteratively refined by a dual-level query decoder. In this way, the 2D masks are transferred to fine-grained shapes on 3D point clouds. Benefit from the query representation for 3D masks, we can compute the similarity matrix between the 3D masks from different views by efficient matrix operation, which enables real-time inference. Experiments on ScanNet, ScanNet200, SceneNN and 3RScan show our method achieves state-of-the-art performance among online 3D perception models, even outperforming offline VFM-assisted 3D instance segmentation methods by a large margin. Our method also demonstrates great generalization ability in several zero-shot dataset transferring experiments and show great potential in data-efficient setting. | Xiuwei Xu, Huangxing Chen, Linqing Zhao, Ziwei Wang, Jie Zhou, Jiwen Lu |  |
| 127 |  |  [Tractable Multi-Agent Reinforcement Learning through Behavioral Economics](https://openreview.net/forum?id=stUKwWBuBm) |  | 0 | A significant roadblock to the development of principled multi-agent reinforcement learning (MARL) algorithms is the fact that desired solution concepts like Nash equilibria may be intractable to compute. We show how one can overcome this obstacle by introducing concepts from behavioral economics into MARL. To do so, we imbue agents with two key features of human decision-making: risk aversion and bounded rationality. We show that introducing these two properties into games gives rise to a class of equilibria---risk-averse quantal response equilibria (RQE)---which are tractable to compute in \emph{all} $n$-player matrix and finite-horizon Markov games. In particular, we show that they emerge as the endpoint of no-regret learning in suitably adjusted versions of the games. Crucially, the class of computationally tractable RQE is independent of the underlying game structure and only depends on agents' degrees of risk-aversion and bounded rationality. To validate the expressivity of this class of solution concepts we show that it captures peoples' patterns of play in a number of 2-player matrix games previously studied in experimental economics. Furthermore, we give a first analysis of the sample complexity of computing these equilibria in finite-horizon Markov games when one has access to a generative model. We validate our findings on a simple multi-agent reinforcement learning benchmark. Our results open the doors for to the principled development of new decentralized multi-agent reinforcement learning algorithms. | Eric Mazumdar, Kishan Panaganti, Laixi Shi |  |
| 128 |  |  [Improved Finite-Particle Convergence Rates for Stein Variational Gradient Descent](https://openreview.net/forum?id=sbG8qhMjkZ) |  | 0 | We provide finite-particle convergence rates for the Stein Variational Gradient Descent (SVGD) algorithm in the Kernelized Stein Discrepancy ($\KSD$) and Wasserstein-2 metrics. Our key insight is that the time derivative of the relative entropy between the joint density of $N$ particle locations and the $N$-fold product target measure, starting from a regular initial distribution, splits into a dominant 'negative part' proportional to $N$ times the expected $\KSD^2$ and a smaller 'positive part'. This observation leads to $\KSD$ rates of order $1/\sqrt{N}$, in both continuous and discrete time, providing a near optimal (in the sense of matching the corresponding i.i.d. rates) double exponential improvement over the recent result by~\cite{shi2024finite}. Under mild assumptions on the kernel and potential, these bounds also grow polynomially in the dimension $d$. By adding a bilinear component to the kernel, the above approach is used to further obtain Wasserstein-2 convergence in continuous time. For the case of \`bilinear + Mat\'ern' kernels, we derive Wasserstein-2 rates that exhibit a curse-of-dimensionality similar to the i.i.d. setting. We also obtain marginal convergence and long-time propagation of chaos results for the time-averaged particle laws. | Sayan Banerjee, Krishna Balasubramanian, Promit Ghosal |  |
| 129 |  |  [Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning](https://openreview.net/forum?id=gc8QAQfXv6) |  | 0 | Catastrophic forgetting (CF) poses a significant challenge in machine learning, where a model forgets previously learned information upon learning new tasks. Despite the advanced capabilities of Large Language Models (LLMs), they continue to face challenges with CF during continual learning. The majority of existing research focuses on analyzing forgetting patterns through a singular training sequence, thereby overlooking the intricate effects that diverse tasks have on model behavior. Our study explores CF across various settings, discovering that model forgetting is influenced by both the specific training tasks and the models themselves. To this end, we interpret forgetting by examining the function vector (FV), a compact representation of functions in LLMs, offering a model-dependent indicator for the occurrence of CF. Through theoretical and empirical analyses, we demonstrated that CF in LLMs primarily stems from biases in function activation rather than the overwriting of task processing functions. Leveraging these insights, we propose a novel function vector guided training methodology, incorporating a regularization technique to stabilize the FV and mitigate forgetting. Empirical tests on four benchmarks confirm the effectiveness of our proposed training method, substantiating our theoretical framework concerning CF and model function dynamics. | Gangwei Jiang, Caigao Jiang, Zhaoyi Li, Siqiao Xue, Jun Zhou, Linqi Song, Defu Lian, Ying Wei |  |
| 130 |  |  [One Step Diffusion via Shortcut Models](https://openreview.net/forum?id=OlzB6LnXcS) |  | 0 | Diffusion models and flow matching models have enabled generating diverse and realistic images by learning to transfer noise to data. However, sampling from these models involves iterative denoising over many neural network passes, making generation slow and expensive. Previous approaches for speeding up sampling require complex training regimes, such as multiple training phases, multiple networks, or fragile scheduling. We introduce Shortcut Models, a family of generative models that use a single network and training phase to produce high-quality samples in a single or multiple sampling steps. Shortcut models condition the network not only on the current noise level but also on the desired step size, allowing the model to skip ahead in the generation process. Across a wide range of sampling step budgets, shortcut models consistently produce higher quality samples than previous approaches, such as consistency models and reflow. Compared to distillation, shortcut models reduce complexity to a single network and training phase and additionally allow varying step budgets at inference time. | Kevin Frans, Danijar Hafner, Sergey Levine, Pieter Abbeel |  |
| 131 |  |  [Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment](https://openreview.net/forum?id=mtSSFiqW6y) |  | 0 | The performance of large language models (LLMs) is closely linked to their underlying size, leading to ever-growing networks and hence slower inference. Speculative decoding has been proposed as a technique to accelerate autoregressive generation, leveraging a fast draft model to propose candidate tokens, which are then verified in parallel based on their likelihood under the target model. While this approach guarantees to reproduce the target output, it incurs a substantial penalty: many high-quality draft tokens are rejected, even when they represent objectively valid continuations. Indeed, we show that even powerful draft models such as GPT-4o, as well as human text cannot achieve high acceptance rates under the standard verification scheme. This severely limits the speedup potential of current speculative decoding methods, as an early rejection becomes overwhelmingly likely when solely relying on alignment of draft and target. We thus ask the following question: Can we adapt verification to recognize correct, but non-aligned replies? To this end, we draw inspiration from the LLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers in a versatile way. We carefully design a dataset coined TokenCourt to elicit the same capability in the target model by training a compact module on top of the embeddings to produce \`\`judgements" of the current continuation. We showcase our strategy on the Llama-3.1 family, where our 8B/405B-Judge achieves a speedup of $9\times$ over Llama-405B, while maintaining its quality on a large range of benchmarks. These benefits remain present even in optimized inference frameworks, where our method reaches up to $141$ tokens/s for 8B/70B-Judge and $129$ tokens/s for 8B/405B on $2$ and $8$ H100s respectively. | Gregor Bachmann, Sotiris Anagnostidis, Albert Pumarola, Markos Georgopoulos, Artsiom Sanakoyeu, Yuming Du, Edgar Schönfeld, Ali K. Thabet, Jonas Kohler |  |
| 132 |  |  [Robustness Inspired Graph Backdoor Defense](https://openreview.net/forum?id=trKNi4IUiP) |  | 0 | Graph Neural Networks (GNNs) have achieved promising results in tasks such as node classification and graph classification. However, recent studies reveal that GNNs are vulnerable to backdoor attacks, posing a significant threat to their real-world adoption. Despite initial efforts to defend against specific graph backdoor attacks, there is no work on defending against various types of backdoor attacks where generated triggers have different properties. Hence, we first empirically verify that prediction variance under edge dropping is a crucial indicator for identifying poisoned nodes. With this observation, we propose using random edge dropping to detect backdoors and theoretically show that it can efficiently distinguish poisoned nodes from clean ones. Furthermore, we introduce a novel robust training strategy to efficiently counteract the impact of the triggers. Extensive experiments on real-world datasets show that our framework can effectively identify poisoned nodes, significantly degrade the attack success rate, and maintain clean accuracy when defending against various types of graph backdoor attacks with different properties. Our code is available at: https://github.com/zzwjames/RIGBD. | Zhiwei Zhang, Minhua Lin, Junjie Xu, Zongyu Wu, Enyan Dai, Suhang Wang |  |
| 133 |  |  [Proxy Denoising for Source-Free Domain Adaptation](https://openreview.net/forum?id=FIj9IEPCKr) |  | 0 | Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain with no access to the source data. Inspired by the success of large Vision-Language (ViL) models in many applications, the latest research has validated ViL's benefit for SFDA by using their predictions as pseudo supervision. However, we observe that ViL's supervision could be noisy and inaccurate at an unknown rate, potentially introducing additional negative effects during adaption. To address this thus-far ignored challenge, we introduce a novel Proxy Denoising (__ProDe__) approach. The key idea is to leverage the ViL model as a proxy to facilitate the adaptation process towards the latent domain-invariant space. Concretely, we design a proxy denoising mechanism to correct ViL's predictions. This is grounded on a proxy confidence theory that models the dynamic effect of proxy's divergence against the domain-invariant space during adaptation. To capitalize the corrected proxy, we further derive a mutual knowledge distilling regularization. Extensive experiments show that ProDe significantly outperforms the current state-of-the-art alternatives under both conventional closed-set setting and the more challenging open-set, partial-set, generalized SFDA, multi-target, multi-source, and test-time settings. Our code and data are available at https://github.com/tntek/source-free-domain-adaptation. | Song Tang, Wenxin Su, Yan Gan, Mao Ye, Jianwei Zhang, Xiatian Zhu |  |
| 134 |  |  [Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models](https://openreview.net/forum?id=tc90LV0yRL) |  | 0 | Language Model (LM) agents for cybersecurity that are capable of autonomously identifying vulnerabilities and executing exploits have potential to cause real-world impact. Policymakers, model providers, and researchers in the AI and cybersecurity communities are interested in quantifying the capabilities of such agents to help mitigate cyberrisk and investigate opportunities for penetration testing. Toward that end, we introduce Cybench, a framework for specifying cybersecurity tasks and evaluating agents on those tasks. We include 40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF competitions, chosen to be recent, meaningful, and spanning a wide range of difficulties. Each task includes its own description, starter files, and is initialized in an environment where an agent can execute commands and observe outputs. Since many tasks are beyond the capabilities of existing LM agents, we introduce subtasks for each task, which break down a task into intermediary steps for a more detailed evaluation. To evaluate agent capabilities, we construct a cybersecurity agent and evaluate 8 models: GPT-4o, OpenAI o1-preview, Claude 3 Opus, Claude 3.5 Sonnet, Mixtral 8x22b Instruct, Gemini 1.5 Pro, Llama 3 70B Chat, and Llama 3.1 405B Instruct. For the top performing models (GPT-4o and Claude 3.5 Sonnet), we further investigate performance across 4 agent scaffolds (structured bash, action-only, pseudoterminal, and web search). Without subtask guidance, agents leveraging Claude 3.5 Sonnet, GPT-4o, OpenAI o1-preview, and Claude 3 Opus successfully solved complete tasks that took human teams up to 11 minutes to solve. In comparison, the most difficult task took human teams 24 hours and 54 minutes to solve. Anonymized code and data are available at https://drive.google.com/file/d/1kp3H0pw1WMAH-Qyyn9WA0ZKmEa7Cr4D4 and https://drive.google.com/file/d/1BcTQ02BBR0m5LYTiK-tQmIK17_TxijIy. | Andy K. Zhang, Neil Perry, Riya Dulepet, Joey Ji, Celeste Menders, Justin W. Lin, Eliot Jones, Gashon Hussein, Samantha Liu, Donovan Julian Jasper, Pura Peetathawatchai, Ari Glenn, Vikram Sivashankar, Daniel Zamoshchin, Leo Glikbarg, Derek Askaryar, Haoxiang Yang, Aolin Zhang, Rishi Alluri, Nathan Tran, et al. |  |
| 135 |  |  [Open-YOLO 3D: Towards Fast and Accurate Open-Vocabulary 3D Instance Segmentation](https://openreview.net/forum?id=CRmiX0v16e) |  | 0 | Recent works on open-vocabulary 3D instance segmentation show strong promise but at the cost of slow inference speed and high computation requirements. This high computation cost is typically due to their heavy reliance on aggregated clip features from multi-view, which require computationally expensive 2D foundation models like Segment Anything (SAM) and CLIP. Consequently, this hampers their applicability in many real-world applications that require both fast and accurate predictions. To this end, we propose a novel open-vocabulary 3D instance segmentation approach, named Open-YOLO 3D, that efficiently leverages only 2D object detection from multi-view RGB images for open-vocabulary 3D instance segmentation. We demonstrate that our proposed Multi-View Prompt Distribution (MVPDist) method makes use of multi-view information to account for misclassification from the object detector to predict a reliable label for 3D instance masks. Furthermore, since projections of 3D object instances are already contained within the 2D bounding boxes, we show that our proposed low granularity label maps, which require only a 2D object detector to construct, are sufficient and very fast to predict prompt IDs for 3D instance masks when used with our proposed MVPDist. We validate our Open-YOLO 3D on two benchmarks, ScanNet200 and Replica, under two scenarios: (i) with ground truth masks, where labels are required for given object proposals, and (ii) with class-agnostic 3D proposals generated from a 3D proposal network. Our Open-YOLO 3D achieves state-of-the-art performance on both datasets while obtaining up to $\sim$16$\times$ speedup compared to the best existing method in literature. On ScanNet200 val. set, our Open-YOLO 3D achieves mean average precision (mAP) of 24.7% while operating at 22 seconds per scene. github.com/aminebdj/OpenYOLO3D | Mohamed El Amine Boudjoghra, Angela Dai, Jean Lahoud, Hisham Cholakkal, Rao Muhammad Anwer, Salman H. Khan, Fahad Shahbaz Khan |  |
| 136 |  |  [Safety Alignment Should be Made More Than Just a Few Tokens Deep](https://openreview.net/forum?id=6Mxhg9PtDE) |  | 0 | The safety alignment of current Large Language Models (LLMs) is vulnerable. Simple attacks, or even benign fine-tuning, can jailbreak aligned models. We note that many of these vulnerabilities are related to a shared underlying issue: safety alignment can take shortcuts, wherein the alignment adapts a model's generative distribution primarily over only its very first few output tokens. We unifiedly refer to this issue as shallow safety alignment. In this paper, we present case studies to explain why shallow safety alignment can exist and show how this issue universally contributes to multiple recently discovered vulnerabilities in LLMs, including the susceptibility to adversarial suffix attacks, prefilling attacks, decoding parameter attacks, and fine-tuning attacks. The key contribution of this work is that we demonstrate how this consolidated notion of shallow safety alignment sheds light on promising research directions for mitigating these vulnerabilities. We show that deepening the safety alignment beyond the first few tokens can meaningfully improve robustness against some common exploits. We also design a regularized fine-tuning objective that makes the safety alignment more persistent against fine-tuning attacks by constraining updates on initial tokens. Overall, we advocate that future safety alignment should be made more than just a few tokens deep. | Xiangyu Qi, Ashwinee Panda, Kaifeng Lyu, Xiao Ma, Subhrajit Roy, Ahmad Beirami, Prateek Mittal, Peter Henderson |  |
| 137 |  |  [On the Identification of Temporal Causal Representation with Instantaneous Dependence](https://openreview.net/forum?id=2efNHgYRvM) |  | 0 | Temporally causal representation learning aims to identify the latent causal process from time series observations, but most methods require the assumption that the latent causal processes do not have instantaneous relations. Although some recent methods achieve identifiability in the instantaneous causality case, they require either interventions on the latent variables or grouping of the observations, which are in general difficult to obtain in real-world scenarios. To fill this gap, we propose an \textbf{ID}entification framework for instantane\textbf{O}us \textbf{L}atent dynamics (\textbf{IDOL}) by imposing a sparse influence constraint that the latent causal processes have sparse time-delayed and instantaneous relations. Specifically, we establish identifiability results of the latent causal process based on sufficient variability and the sparse influence constraint by employing contextual information of time series data. Based on these theories, we incorporate a temporally variational inference architecture to estimate the latent variables and a gradient-based sparsity regularization to identify the latent causal process. Experimental results on simulation datasets illustrate that our method can identify the latent causal process. Furthermore, evaluations on multiple human motion forecasting benchmarks with instantaneous dependencies indicate the effectiveness of our method in real-world settings. | Zijian Li, Yifan Shen, Kaitao Zheng, Ruichu Cai, Xiangchen Song, Mingming Gong, Guangyi Chen, Kun Zhang |  |
| 138 |  |  [WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://openreview.net/forum?id=mMPMHWOdOy) |  | 0 | Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical reasoning abilities of LLMs, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses all other open-source LLMs by a substantial margin. Furthermore, WizardMath 70B even outperforms ChatGPT-3.5, Claude Instant, Gemini Pro and Mistral Medium. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance. | Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, JianGuang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Yansong Tang, Dongmei Zhang |  |
| 139 |  |  [Faster Cascades via Speculative Decoding](https://openreview.net/forum?id=vo9t20wsmd) |  | 0 | Cascades and speculative decoding are two common approaches to improving language models' inference efficiency. Both approaches interleave two models, but via fundamentally distinct mechanisms: deferral rule that invokes the larger model only for “hard” inputs, while speculative decoding uses speculative execution to primarily invoke the larger model in parallel scoring mode. These mechanisms offer different benefits: empirically, cascades offer compelling cost-quality trade-offs, often even outperforming the large model; speculative cascades offer impressive speed-ups, while guaranteeing quality-neutrality. In this paper, we leverage the best of both these approaches by designing new speculative cascading techniques that implement their deferral rule through speculative execution. We characterize the optimal deferral rule for our speculative cascades, and employ a plug-in approximation to the optimal rule. Experiments with Gemma and T5 models on a range of language benchmarks show that our approach yields better cost quality trade-offs than cascading and speculative decoding baselines. | Harikrishna Narasimhan, Wittawat Jitkrittum, Ankit Singh Rawat, Seungyeon Kim, Neha Gupta, Aditya Krishna Menon, Sanjiv Kumar |  |
| 140 |  |  [The Hidden Cost of Waiting for Accurate Predictions](https://openreview.net/forum?id=A3YUPeJTNR) |  | 0 | Algorithmic predictions are increasingly informing societal resource allocations by identifying individuals for targeting. Policymakers often build these systems with the assumption that by gathering more observations on individuals, they can improve predictive accuracy and, consequently, allocation efficiency. An overlooked yet consequential aspect of prediction-driven allocations is that of timing. The planner has to trade off relying on earlier and potentially noisier predictions to intervene before individuals experience undesirable outcomes, or they may wait to gather more observations to make more precise allocations. We examine this tension using a simple mathematical model, where the planner collects observations on individuals to improve predictions over time. We analyze both the ranking induced by these predictions and optimal resource allocation. We show that though individual prediction accuracy improves over time, counter-intuitively, the average ranking loss can worsen. As a result, the planner's ability to improve social welfare can decline. We identify inequality as a driving factor behind this phenomenon. Our findings provide a nuanced perspective and challenge the conventional wisdom that it is preferable to wait for more accurate predictions to ensure the most efficient allocations. | Ali Shirali, Ariel D. Procaccia, Rediet Abebe |  |
| 141 |  |  [Learning Dynamics of LLM Finetuning](https://openreview.net/forum?id=tPNHOoZFl9) |  | 0 | Learning dynamics, which describes how the learning of specific training examples influences the model's predictions on other examples, gives us a powerful tool for understanding the behavior of deep learning systems. We study the learning dynamics of large language models during different types of finetuning, by analyzing the step-wise decomposition of how influence accumulates among different potential responses. Our framework allows a uniform interpretation of many interesting observations about the training of popular algorithms for both instruction tuning and preference tuning. In particular, we propose a hypothetical explanation of why specific types of hallucination are strengthened after finetuning, e.g., the model might use phrases or facts in the response for question B to answer question A, or the model might keep repeating similar simple phrases when generating responses. We also extend our framework and highlight a unique \`\`squeezing effect'' to explain a previously observed phenomenon in off-policy direct preference optimization (DPO), where running DPO for too long makes even the desired outputs less likely. This framework also provides insights into where the benefits of on-policy DPO and other variants come from. The analysis not only provides a novel perspective of understanding LLM's finetuning but also inspires a simple, effective method to improve alignment performance. | Yi Ren, Danica J. Sutherland |  |
| 142 |  |  [Root Cause Analysis of Anomalies in Multivariate Time Series through Granger Causal Discovery](https://openreview.net/forum?id=k38Th3x4d9) |  | 0 | Identifying the root causes of anomalies in multivariate time series is challenging due to the complex dependencies among the series. In this paper, we propose a comprehensive approach called AERCA that inherently integrates Granger causal discovery with root cause analysis. By defining anomalies as interventions on the exogenous variables of time series, AERCA not only learns the Granger causality among time series but also explicitly models the distributions of exogenous variables under normal conditions. AERCA then identifies the root causes of anomalies by highlighting exogenous variables that significantly deviate from their normal states. Experiments on multiple synthetic and real-world datasets demonstrate that AERCA can accurately capture the causal relationships among time series and effectively identify the root causes of anomalies. | Xiao Han, Saima Absar, Lu Zhang, Shuhan Yuan |  |
| 143 |  |  [ProtComposer: Compositional Protein Structure Generation with 3D Ellipsoids](https://openreview.net/forum?id=0ctvBgKFgc) |  | 0 | We develop ProtComposer to generate protein structures conditioned on spatial protein layouts that are specified via a set of 3D ellipsoids capturing substructure shapes and semantics. At inference time, we condition on ellipsoids that are hand-constructed, extracted from existing proteins, or from a statistical model, with each option unlocking new capabilities. Hand-specifying ellipsoids enables users to control the location, size, orientation, secondary structure, and approximate shape of protein substructures. Conditioning on ellipsoids of existing proteins enables redesigning their substructure's connectivity or editing substructure properties. By conditioning on novel and diverse ellipsoid layouts from a simple statistical model, we improve protein generation with expanded Pareto frontiers between designability, novelty, and diversity. Further, this enables sampling designable proteins with a helix-fraction that matches PDB proteins, unlike existing generative models that commonly oversample conceptually simple helix bundles. Code is available at https://github.com/NVlabs/protcomposer. | Hannes Stärk, Bowen Jing, Tomas Geffner, Jason Yim, Tommi S. Jaakkola, Arash Vahdat, Karsten Kreis |  |
| 144 |  |  [More RLHF, More Trust? On The Impact of Preference Alignment On Trustworthiness](https://openreview.net/forum?id=FpiCLJrSW8) |  | 0 | The trustworthiness of Large Language Models (LLMs) refers to the extent to which their outputs are reliable, safe, and ethically aligned, and it has become a crucial consideration alongside their cognitive performance. In practice, Reinforcement Learning From Human Feedback (RLHF) has been widely used to align LLMs with labeled human preferences, but its assumed effect on model trustworthiness hasn't been rigorously evaluated. To bridge this knowledge gap, this study investigates how models aligned with general-purpose preference data perform across five trustworthiness verticals: toxicity, stereotypical bias, machine ethics, truthfulness, and privacy. Our results demonstrate that RLHF on human preferences doesn't automatically guarantee trustworthiness, and reverse effects are often observed. Furthermore, we propose to adapt efficient influence function based data attribution methods to the RLHF setting to better understand the influence of fine-tuning data on individual trustworthiness benchmarks, and show its feasibility by providing our estimated attribution scores. Together, our results underscore the need for more nuanced approaches for model alignment from both the data and framework perspectives, and we hope this research will guide the community towards developing language models that are increasingly capable without sacrificing trustworthiness. | Aaron Jiaxun Li, Satyapriya Krishna, Himabindu Lakkaraju |  |
| 145 |  |  [Geometry-aware RL for Manipulation of Varying Shapes and Deformable Objects](https://openreview.net/forum?id=7BLXhmWvwF) |  | 0 | Manipulating objects with varying geometries and deformable objects is a major challenge in robotics. Tasks such as insertion with different objects or cloth hanging require precise control and effective modelling of complex dynamics. In this work, we frame this problem through the lens of a heterogeneous graph that comprises smaller sub-graphs, such as actuators and objects, accompanied by different edge types describing their interactions. This graph representation serves as a unified structure for both rigid and deformable objects tasks, and can be extended further to tasks comprising multiple actuators. To evaluate this setup, we present a novel and challenging reinforcement learning benchmark, including rigid insertion of diverse objects, as well as rope and cloth manipulation with multiple end-effectors. These tasks present a large search space, as both the initial and target configurations are uniformly sampled in 3D space. To address this issue, we propose a novel graph-based policy model, dubbed Heterogeneous Equivariant Policy (HEPi), utilizing $SE(3)$ equivariant message passing networks as the main backbone to exploit the geometric symmetry. In addition, by modeling explicit heterogeneity, HEPi can outperform Transformer-based and non-heterogeneous equivariant policies in terms of average returns, sample efficiency, and generalization to unseen objects. Our project page is available at https://thobotics.github.io/hepi. | Tai Hoang, Huy Le, Philipp Becker, Ngo Anh Vien, Gerhard Neumann |  |
| 146 |  |  [Topological Blindspots: Understanding and Extending Topological Deep Learning Through the Lens of Expressivity](https://openreview.net/forum?id=EzjsoomYEb) |  | 0 | Topological deep learning (TDL) is a rapidly growing field that seeks to leverage topological structure in data and facilitate learning from data supported on topological objects, ranging from molecules to 3D shapes. Most TDL architectures can be unified under the framework of higher-order message-passing (HOMP), which generalizes graph message-passing to higher-order domains. In the first part of the paper, we explore HOMP's expressive power from a topological perspective, demonstrating the framework's inability to capture fundamental topological and metric invariants such as diameter, orientability, planarity, and homology. In addition, we demonstrate HOMP's limitations in fully leveraging lifting and pooling methods on graphs. To the best of our knowledge, this is the first work to study the expressivity of TDL from a topological perspective. In the second part of the paper, we develop two new classes of architectures -- multi-cellular networks (MCN) and scalable MCN (SMCN) -- which draw inspiration from expressive GNNs. MCN can reach full expressivity, but scaling it to large data objects can be computationally expansive. Designed as a more scalable alternative, SMCN still mitigates many of HOMP's expressivity limitations. Finally, we design new benchmarks for evaluating models based on their ability to learn topological properties of complexes. We then evaluate SMCN on these benchmarks as well as on real-world graph datasets, demonstrating improvements over both HOMP baselines and expressive graph methods, highlighting the value of expressively leveraging topological information. | Yam Eitan, Yoav Gelberg, Guy BarShalom, Fabrizio Frasca, Michael M. Bronstein, Haggai Maron |  |
| 147 |  |  [Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency](https://openreview.net/forum?id=weM4YBicIP) |  | 0 | With the introduction of video diffusion model, audio-conditioned human video generation has recently achieved significant breakthroughs in both the naturalness of motion and the synthesis of portrait details. Due to the limited control of audio signals in driving human motion, existing methods often add auxiliary spatial signals such as movement regions to stabilize movements, which compromise the naturalness and freedom of motion. To address this issue, we propose an end-to-end audio-only conditioned video diffusion model named Loopy. Specifically, we designed two key modules: an inter- and intra-clip temporal module and an audio-to-latents module. These enable the model to better utilize long-term motion dependencies and establish a stronger audio-portrait movement correlation. Consequently, the model can generate more natural and stable portrait videos with subtle facial expressions, without the need for manually setting movement constraints. Extensive experiments show that Loopy outperforms recent audio-driven portrait diffusion models, delivering more lifelike and high-quality results across various scenarios. Video samples are available at https://loopyavataranony.github.io/ | Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong, Yanbo Zheng |  |
| 148 |  |  [CyberHost: A One-stage Diffusion Framework for Audio-driven Talking Body Generation](https://openreview.net/forum?id=vaEPihQsAA) |  | 0 | Diffusion-based video generation technology has advanced significantly, catalyzing a proliferation of research in human animation. While breakthroughs have been made in driving human animation through various modalities for portraits, most of current solutions for human body animation still focus on video-driven methods, leaving audio-driven taking body generation relatively underexplored. In this paper, we introduce CyberHost, a one-stage audio-driven talking body generation framework that addresses common synthesis degradations in half-body animation, including hand integrity, identity consistency, and natural motion. CyberHost's key designs are twofold. Firstly, the Region Attention Module (RAM) maintains a set of learnable, implicit, identity-agnostic latent features and combines them with identity-specific local visual features to enhance the synthesis of critical local regions. Secondly, the Human-Prior-Guided Conditions introduce more human structural priors into the model, reducing uncertainty in generated motion patterns and thereby improving the stability of the generated videos. To our knowledge, CyberHost is the first one-stage audio-driven human diffusion model capable of zero-shot video generation for the human body. Extensive experiments demonstrate that CyberHost surpasses previous works in both quantitative and qualitative aspects. CyberHost can also be extended to video-driven and audio-video hybrid-driven scenarios, achieving similarly satisfactory results. | Gaojie Lin, Jianwen Jiang, Chao Liang, Tianyun Zhong, Jiaqi Yang, Zerong Zheng, Yanbo Zheng |  |
| 149 |  |  [Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model](https://openreview.net/forum?id=SI2hI0frk6) |  | 0 | We introduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data. Transfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed-modality sequences. We pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks. Our experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens. By introducing modality-specific encoding and decoding layers, we can further improve the performance of Transfusion models, and even compress each image to just 16 patches. We further demonstrate that scaling our Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds. | Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, Omer Levy |  |
| 150 |  |  [MoE++: Accelerating Mixture-of-Experts Methods with Zero-Computation Experts](https://openreview.net/forum?id=t7P5BUKcYv) |  | 0 | In this work, we aim to simultaneously enhance the effectiveness and efficiency of Mixture-of-Experts (MoE) methods. To achieve this, we propose MoE++, a general and heterogeneous MoE framework that integrates both Feed-Forward Network (FFN) and zero-computation experts. Specifically, we introduce three types of zero-computation experts: the zero expert, copy expert, and constant expert, which correspond to discard, skip, and replace operations, respectively. This design offers three key advantages: (i) \*\*Low Computing Overhead\*\*: Unlike the uniform mixing mechanism for all tokens within vanilla MoE, MoE++ allows each token to engage with a dynamic number of FFNs, be adjusted by constant vectors, or even skip the MoE layer entirely. (ii) \*\*High Performance\*\*: By enabling simple tokens to utilize fewer FFN experts, MoE++ allows more experts to focus on challenging tokens, thereby unlocking greater performance potential than vanilla MoE. (iii) \*\*Deployment Friendly\*\*: Given that zero-computation experts have negligible parameters, we can deploy all zero-computation experts on each GPU, eliminating the significant communication overhead and expert load imbalance associated with FFN experts distributed across different GPUs. Moreover, we leverage gating residuals, enabling each token to consider the pathway taken in the previous layer when selecting the appropriate experts. Extensive experimental results demonstrate that MoE++ achieves better performance while delivering 1.1$\sim$2.1$\times$ expert forward throughput compared to a vanilla MoE model of the same size, which lays a solid foundation for developing advanced and efficient MoE-related models. | Peng Jin, Bo Zhu, Li Yuan, Shuicheng Yan |  |
| 151 |  |  [Compositional Entailment Learning for Hyperbolic Vision-Language Models](https://openreview.net/forum?id=3i13Gev2hV) |  | 0 | Image-text representation learning forms a cornerstone in vision-language models, where pairs of images and textual descriptions are contrastively aligned in a shared embedding space. Since visual and textual concepts are naturally hierarchical, recent work has shown that hyperbolic space can serve as a high-potential manifold to learn vision-language representation with strong downstream performance. In this work, for the first time we show how to fully leverage the innate hierarchical nature of hyperbolic embeddings by looking beyond individual image-text pairs. We propose Compositional Entailment Learning for hyperbolic vision-language models. The idea is that an image is not only described by a sentence but is itself a composition of multiple object boxes, each with their own textual description. Such information can be obtained freely by extracting nouns from sentences and using openly available localized grounding models. We show how to hierarchically organize images, image boxes, and their textual descriptions through contrastive and entailment-based objectives. Empirical evaluation on a hyperbolic vision-language model trained with millions of image-text pairs shows that the proposed compositional learning approach outperforms conventional Euclidean CLIP learning, as well as recent hyperbolic alternatives, with better zero-shot and retrieval generalization and clearly stronger hierarchical performance. | Avik Pal, Max van Spengler, Guido Maria D'Amely di Melendugno, Alessandro Flaborea, Fabio Galasso, Pascal Mettes |  |
| 152 |  |  [Advantage Alignment Algorithms](https://openreview.net/forum?id=QFO1asgas2) |  | 0 | Artificially intelligent agents are increasingly being integrated into human decision-making: from large language model (LLM) assistants to autonomous vehicles. These systems often optimize their individual objective, leading to conflicts, particularly in general-sum games where naive reinforcement learning agents empirically converge to Pareto-suboptimal Nash equilibria. To address this issue, opponent shaping has emerged as a paradigm for finding socially beneficial equilibria in general-sum games. In this work, we introduce Advantage Alignment, a family of algorithms derived from first principles that perform opponent shaping efficiently and intuitively. We achieve this by aligning the advantages of interacting agents, increasing the probability of mutually beneficial actions when their interaction has been positive. We prove that existing opponent shaping methods implicitly perform Advantage Alignment. Compared to these methods, Advantage Alignment simplifies the mathematical formulation of opponent shaping, reduces the computational burden and extends to continuous action domains. We demonstrate the effectiveness of our algorithms across a range of social dilemmas, achieving state-of-the-art cooperation and robustness against exploitation. | Juan Agustin Duque, Milad Aghajohari, Tim Cooijmans, Razvan Ciuca, Tianyu Zhang, Gauthier Gidel, Aaron C. Courville |  |
| 153 |  |  [Scaling In-the-Wild Training for Diffusion-based Illumination Harmonization and Editing by Imposing Consistent Light Transport](https://openreview.net/forum?id=u1cQYxRI1H) |  | 0 | Diffusion-based image generators are becoming unique methods for illumination harmonization and editing. The current bottleneck in scaling up the training of diffusion-based illumination editing models is mainly in the difficulty of preserving the underlying image details and maintaining intrinsic properties, such as albedos, unchanged. Without appropriate constraints, directly training the latest large image models with complex, varied, or in-the-wild data is likely to produce a structure-guided random image generator, rather than achieving the intended goal of precise illumination manipulation. We propose Imposing Consistent Light (IC-Light) transport during training, rooted in the physical principle that the linear blending of an object's appearances under different illumination conditions is consistent with its appearance under mixed illumination. This consistency allows for stable and scalable illumination learning, uniform handling of various data sources, and facilitates a physically grounded model behavior that modifies only the illumination of images while keeping other intrinsic properties unchanged. Based on this method, we can scale up the training of diffusion-based illumination editing models to large data quantities (> 10 million), across all available data types (real light stages, rendered samples, in-the-wild synthetic augmentations, etc), and using strong backbones (SDXL, Flux, etc). We also demonstrate that this approach reduces uncertainties and mitigates artifacts such as mismatched materials or altered albedos. | Lvmin Zhang, Anyi Rao, Maneesh Agrawala |  |
| 154 |  |  [AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models](https://openreview.net/forum?id=HvSytvg3Jh) |  | 0 | Large language models (LLMs) often exhibit hallucinations, producing incorrect or outdated knowledge. Hence, model editing methods have emerged to enable targeted knowledge updates. To achieve this, a prevailing paradigm is the locating-then-editing approach, which first locates influential parameters and then edits them by introducing a perturbation. While effective, current studies have demonstrated that this perturbation inevitably disrupt the originally preserved knowledge within LLMs, especially in sequential editing scenarios. To address this, we introduce AlphaEdit, a novel solution that projects perturbation onto the null space of the preserved knowledge before applying it to the parameters. We theoretically prove that this projection ensures the output of post-edited LLMs remains unchanged when queried about the preserved knowledge, thereby mitigating the issue of disruption. Extensive experiments on various LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts the performance of most locating-then-editing methods by an average of 36.7% with a single line of additional code for projection solely. | Junfeng Fang, Houcheng Jiang, Kun Wang, Yunshan Ma, Jie Shi, Xiang Wang, Xiangnan He, TatSeng Chua |  |
| 155 |  |  [DeepLTL: Learning to Efficiently Satisfy Complex LTL Specifications for Multi-Task RL](https://openreview.net/forum?id=9pW2J49flQ) |  | 0 | Linear temporal logic (LTL) has recently been adopted as a powerful formalism for specifying complex, temporally extended tasks in multi-task reinforcement learning (RL). However, learning policies that efficiently satisfy arbitrary specifications not observed during training remains a challenging problem. Existing approaches suffer from several shortcomings: they are often only applicable to finite-horizon fragments of LTL, are restricted to suboptimal solutions, and do not adequately handle safety constraints. In this work, we propose a novel learning approach to address these concerns. Our method leverages the structure of Büchi automata, which explicitly represent the semantics of LTL specifications, to learn policies conditioned on sequences of truth assignments that lead to satisfying the desired formulae. Experiments in a variety of discrete and continuous domains demonstrate that our approach is able to zero-shot satisfy a wide range of finite- and infinite-horizon specifications, and outperforms existing methods in terms of both satisfaction probability and efficiency. Code available at: https://deep-ltl.github.io/ | Mathias Jackermeier, Alessandro Abate |  |
| 156 |  |  [On the Role of Attention Heads in Large Language Model Safety](https://openreview.net/forum?id=h0Ak8A5yqw) |  | 0 | Large language models (LLMs) achieve state-of-the-art performance on multiple language tasks, yet their safety guardrails can be circumvented, leading to harmful generations. In light of this, recent research on safety mechanisms has emerged, revealing that when safety representations or component are suppressed, the safety capability of LLMs are compromised. However, existing research tends to overlook the safety impact of multi-head attention mechanisms, despite their crucial role in various model functionalities. Hence, in this paper, we aim to explore the connection between standard attention mechanisms and safety capability to fill this gap in the safety-related mechanistic interpretability. We propose an novel metric which tailored for multi-head attention, the Safety Head ImPortant Score (Ships), to assess the individual heads' contributions to model safety. Base on this, we generalize Ships to the dataset level and further introduce the Safety Attention Head AttRibution Algorithm (Sahara) to attribute the critical safety attention heads inside the model. Our findings show that special attention head has a significant impact on safety. Ablating a single safety head allows aligned model (e.g., Llama-2-7b-chat) to respond to \*\*16$\times\uparrow$\*\* more harmful queries, while only modifying \*\*0.006\%\*\* $\downarrow$ of the parameters, in contrast to the $\sim$ \*\*5\%\*\* modification required in previous studies. More importantly, we demonstrate that attention heads primarily function as feature extractors for safety and models fine-tuned from the same base model exhibit overlapping safety heads through comprehensive experiments. Together, our attribution approach and findings provide a novel perspective for unpacking the black box of safety mechanisms in large models. | Zhenhong Zhou, Haiyang Yu, Xinghua Zhang, Rongwu Xu, Fei Huang, Kun Wang, Yang Liu, Junfeng Fang, Yongbin Li |  |
| 157 |  |  [Influence Functions for Scalable Data Attribution in Diffusion Models](https://openreview.net/forum?id=esYrEndGsr) |  | 0 | Diffusion models have led to significant advancements in generative modelling. Yet their widespread adoption poses challenges regarding data attribution and interpretability. In this paper, we aim to help address such challenges in diffusion models by extending influence functions. Influence function-based data attribution methods approximate how a model's output would have changed if some training data were removed. In supervised learning, this is usually used for predicting how the loss on a particular example would change. For diffusion models, we focus on predicting the change in the probability of generating a particular example via several proxy measurements. We show how to formulate influence functions for such quantities and how previously proposed methods can be interpreted as particular design choices in our framework. To ensure scalability of the Hessian computations in influence functions, we use a K-FAC approximation based on generalised Gauss-Newton matrices specifically tailored to diffusion models. We show that our recommended method outperforms previously proposed data attribution methods on common data attribution evaluations, such as the Linear Data-modelling Score (LDS) or retraining without top influences, without the need for method-specific hyperparameter tuning. | Bruno Kacper Mlodozeniec, Runa Eschenhagen, Juhan Bae, Alexander Immer, David Krueger, Richard E. Turner |  |
| 158 |  |  [Second-Order Min-Max Optimization with Lazy Hessians](https://openreview.net/forum?id=ijbA5swmoK) |  | 0 | This paper studies second-order methods for convex-concave minimax optimization. Monteiro & Svaiter (2012) proposed a method to solve the problem with an optimal iteration complexity of $\mathcal{O}(\epsilon^{-3/2})$ to find an $\epsilon$-saddle point. However, it is unclear whether the computational complexity, $\mathcal{O}((N+ d^2) d \epsilon^{-2/3})$, can be improved. In the above, we follow Doikov et al. (2023) and assume the complexity of obtaining a first-order oracle as $N$ and the complexity of obtaining a second-order oracle as $dN$. In this paper, we show that the computation cost can be reduced by reusing Hessian across iterations. Our methods take the overall computational complexity of $\tilde{\mathcal{O}}( (N+d^2)(d+ d^{2/3}\epsilon^{-2/3}))$, which improves those of previous methods by a factor of $d^{1/3}$. Furthermore, we generalize our method to strongly-convex-strongly-concave minimax problems and establish the complexity of $\tilde{\mathcal{O}}((N+d^2) (d + d^{2/3} \kappa^{2/3}) )$ when the condition number of the problem is $\kappa$, enjoying a similar speedup upon the state-of-the-art method. Numerical experiments on both real and synthetic datasets also verify the efficiency of our method. | Lesi Chen, Chengchang Liu, Jingzhao Zhang |  |
| 159 |  |  [Composing Unbalanced Flows for Flexible Docking and Relaxation](https://openreview.net/forum?id=gHLWTzKiZV) |  | 0 | Diffusion models have emerged as a successful approach for molecular docking, but they often cannot model protein flexibility or generate nonphysical poses. We argue that both these challenges can be tackled by framing the problem as a transport between distributions. Still, existing paradigms lack the flexibility to define effective maps between such complex distributions. To address this limitation, we propose Unbalanced Flow Matching, a generalization of Flow Matching (FM) that allows trading off sample efficiency with approximation accuracy and enables more accurate transport. Empirically, we apply Unbalanced FM on flexible docking and structure relaxation, demonstrating our ability to model protein flexibility and generate energetically favorable poses. On the PDBBind docking benchmark, our method FlexDock improves the docking performance while increasing the proportion of energetically favorable poses from 30% to 73%. | Gabriele Corso, Vignesh Ram Somnath, Noah Getz, Regina Barzilay, Tommi S. Jaakkola, Andreas Krause |  |
| 160 |  |  [Learning Distributions of Complex Fluid Simulations with Diffusion Graph Networks](https://openreview.net/forum?id=uKZdlihDDn) |  | 0 | Physical systems with complex unsteady dynamics, such as fluid flows, are often poorly represented by a single mean solution. For many practical applications, it is crucial to access the full distribution of possible states, from which relevant statistics (e.g., RMS and two-point correlations) can be derived. Here, we propose a graph-based latent diffusion model that enables direct sampling of states from their equilibrium distribution, given a mesh discretization of the system and its physical parameters. This allows for the efficient computation of flow statistics without running long and expensive numerical simulations. The graph-based structure enables operations on unstructured meshes, which is critical for representing complex geometries with spatially localized high gradients, while latent-space diffusion modeling with a multi-scale GNN allows for efficient learning and inference of entire distributions of solutions. A key finding of our work is that the proposed networks can accurately learn full distributions even when trained on incomplete data from relatively short simulations. We apply this method to a range of fluid dynamics tasks, such as predicting pressure distributions on 3D wing models in turbulent flow, demonstrating both accuracy and computational efficiency in challenging scenarios. The ability to directly sample accurate solutions, and capturing their diversity from short ground-truth simulations, is highly promising for complex scientific modeling tasks. | Mario Lino Valencia, Tobias Pfaff, Nils Thuerey |  |
| 161 |  |  [Training Language Models to Self-Correct via Reinforcement Learning](https://openreview.net/forum?id=CjwERcAU7w) |  | 0 | Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Current methods for training self-correction typically depend on either multiple models, a more advanced model, or additional forms of supervision. To address these shortcomings, we develop a multi-turn online reinforcement learning (RL) approach, SCoRe, that significantly improves an LLM's self-correction ability using entirely self-generated data. To build SCoRe, we first show that variants of supervised fine-tuning (SFT) on offline model-generated correction traces are insufficient for instilling self-correction behavior. In particular, we observe that training via SFT either suffers from a distribution mismatch between the training data and the model's own responses or implicitly prefers only a certain mode of correction behavior that is often not effective at test time. SCoRe addresses these challenges by training under the model's own distribution of self-generated correction traces and using appropriate regularization to steer the learning process into learning a self-correction strategy that is effective at test time as opposed to simply fitting high-reward responses for a given prompt. This regularization prescribes running a first phase of RL on a base model to generate a policy initialization that is less susceptible to collapse and then using a reward bonus to amplify self-correction during training. When applied to Gemini 1.0 Pro and 1.5 Flash models, we find that SCoRe achieves state-of-the-art self-correction performance, improving the base models' self-correction by 15.6% and 9.1% respectively on the MATH and HumanEval benchmarks. | Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D. CoReyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei M. Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal M. P. Behbahani, Aleksandra Faust |  |
| 162 |  |  [AI as Humanity's Salieri: Quantifying Linguistic Creativity of Language Models via Systematic Attribution of Machine Text against Web Text](https://openreview.net/forum?id=ilOEOIqolQ) |  | 0 | Creativity has long been considered one of the most difficult aspect of human intelligence for AI to mimic. However, the rise of Large Language Models (LLMs), like ChatGPT, has raised questions about whether AI can match or even surpass human creativity. We present CREATIVITY INDEX as the first step to quantify the linguistic creativity of a text by reconstructing it from existing text snippets on the web. CREATIVITY INDEX is motivated by the hypothesis that the seemingly remarkable creativity of LLMs may be attributable in large part to the creativity of human-written texts on the web. To compute CREATIVITY INDEX efficiently, we introduce DJ SEARCH, a novel dynamic programming algorithm that can search verbatim and near-verbatim matches of text snippets from a given document against the web. Experiments reveal that the CREATIVITY INDEX of professional human authors is on average 66.2% higher than that of LLMs, and that alignment reduces the CREATIVITY INDEX of LLMs by an average of 30.1%. In addition, we find that distinguished authors like Hemingway exhibit measurably higher CREATIVITY INDEX compared to other human writers. Finally, we demonstrate that CREATIVITY INDEX can be used as a surprisingly effective criterion for zero-shot machine text detection, surpassing the strongest existing zero-shot system, DetectGPT, by a significant margin of 30.2%, and even outperforming the strongest supervised system, GhostBuster, in five out of six domains. | Ximing Lu, Melanie Sclar, Skyler Hallinan, Niloofar Mireshghallah, Jiacheng Liu, Seungju Han, Allyson Ettinger, Liwei Jiang, Khyathi Raghavi Chandu, Nouha Dziri, Yejin Choi |  |
| 163 |  |  [Comparing noisy neural population dynamics using optimal transport distances](https://openreview.net/forum?id=cNmu0hZ4CL) |  | 0 | Biological and artificial neural systems form high-dimensional neural representations that underpin their computational capabilities. Methods for quantifying geometric similarity in neural representations have become a popular tool for identifying computational principles that are potentially shared across neural systems. These methods generally assume that neural responses are deterministic and static. However, responses of biological systems, and some artificial systems, are noisy and dynamically unfold over time. Furthermore, these characteristics can have substantial influence on a system’s computational capabilities. Here, we demonstrate that existing metrics can fail to capture key differences between neural systems with noisy dynamic responses. We then propose a metric for comparing the geometry of noisy neural trajectories, which can be derived as an optimal transport distance between Gaussian processes. We use the metric to compare models of neural responses in different regions of the motor system and to compare the dynamics of latent diffusion models for text-to-image synthesis. | Amin Nejatbakhsh, Victor Geadah, Alex H. Williams, David Lipshutz |  |
| 164 |  |  [Learning stochastic dynamics from snapshots through regularized unbalanced optimal transport](https://openreview.net/forum?id=gQlxd3Mtru) |  | 0 | Reconstructing dynamics using samples from sparsely time-resolved snapshots is an important problem in both natural sciences and machine learning. Here, we introduce a new deep learning approach for solving regularized unbalanced optimal transport (RUOT) and inferring continuous unbalanced stochastic dynamics from observed snapshots. Based on the RUOT form, our method models these dynamics without requiring prior knowledge of growth and death processes or additional information, allowing them to be learned directly from data. Theoretically, we explore the connections between the RUOT and Schrödinger bridge problem and discuss the key challenges and potential solutions. The effectiveness of our method is demonstrated with a synthetic gene regulatory network, high-dimensional Gaussian Mixture Model, and single-cell RNA-seq data from blood development. Compared with other methods, our approach accurately identifies growth and transition patterns, eliminates false transitions, and constructs the Waddington developmental landscape. Our code is available at: [https://github.com/zhenyiizhang/DeepRUOT](https://github.com/zhenyiizhang/DeepRUOT). | Zhenyi Zhang, Tiejun Li, Peijie Zhou |  |
| 165 |  |  [Prioritized Generative Replay](https://openreview.net/forum?id=5IkDAfabuo) |  | 0 | Sample-efficient online reinforcement learning often uses replay buffers to store experience for reuse when updating the value function. However, uniform replay is inefficient, since certain classes of transitions can be more relevant to learning. While prioritization of more useful samples is helpful, this strategy can also lead to overfitting, as useful samples are likely to be more rare. In this work, we instead propose a prioritized, parametric version of an agent's memory, using generative models to capture online experience. This paradigm enables (1) densification of past experience, with new generations that benefit from the generative model's generalization capacity and (2) guidance via a family of "relevance functions" that push these generations towards more useful parts of an agent's acquired history. We show this recipe can be instantiated using conditional diffusion models and simple relevance functions such as curiosity- or value-based metrics. Our approach consistently improves performance and sample efficiency in both state- and pixel-based domains. We expose the mechanisms underlying these gains, showing how guidance promotes diversity in our generated transitions and reduces overfitting. We also showcase how our approach can train policies with even higher update-to-data ratios than before, opening up avenues to better scale online RL agents. Project page available at: https://pgenreplay.github.io | Renhao Wang, Kevin Frans, Pieter Abbeel, Sergey Levine, Alexei A. Efros |  |
| 166 |  |  [The Geometry of Categorical and Hierarchical Concepts in Large Language Models](https://openreview.net/forum?id=bVTM2QKYuA) |  | 0 | The linear representation hypothesis is the informal idea that semantic concepts are encoded as linear directions in the representation spaces of large language models (LLMs). Previous work has shown how to make this notion precise for representing binary concepts that have natural contrasts (e.g., {male, female}) as _directions_ in representation space. However, many natural concepts do not have natural contrasts (e.g., whether the output is about an animal). In this work, we show how to extend the formalization of the linear representation hypothesis to represent features (e.g., is_animal) as _vectors_. This allows us to immediately formalize the representation of categorical concepts as polytopes in the representation space. Further, we use the formalization to prove a relationship between the hierarchical structure of concepts and the geometry of their representations. We validate these theoretical results on the Gemma and LLaMA-3 large language models, estimating representations for 900+ hierarchically related concepts using data from WordNet. | Kiho Park, Yo Joong Choe, Yibo Jiang, Victor Veitch |  |
| 167 |  |  [Generator Matching: Generative modeling with arbitrary Markov processes](https://openreview.net/forum?id=RuP17cJtZo) |  | 0 | We introduce Generator Matching, a modality-agnostic framework for generative modeling using arbitrary Markov processes. Generators characterize the infinitesimal evolution of a Markov process, which we leverage for generative modeling in a similar vein to flow matching: we construct conditional generators which generate single data points, then learn to approximate the marginal generator which generates the full data distribution. We show that Generator Matching unifies various generative modeling methods, including diffusion models, flow matching and discrete diffusion models. Furthermore, it expands the design space to new and unexplored Markov processes such as jump processes. Finally, Generator Matching enables the construction of superpositions of Markov generative models and enables the construction of multimodal models in a rigorous manner. We empirically validate our method on image and multimodal generation, e.g. showing that superposition with a jump process improves performance. | Peter Holderrieth, Marton Havasi, Jason Yim, Neta Shaul, Itai Gat, Tommi S. Jaakkola, Brian Karrer, Ricky T. Q. Chen, Yaron Lipman |  |
| 168 |  |  [No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images](https://openreview.net/forum?id=P4o9akekdf) |  | 0 | We introduce NoPoSplat, a feed-forward model capable of reconstructing 3D scenes parameterized by 3D Gaussians from unposed sparse multi-view images. Our model, trained exclusively with photometric loss, achieves real-time 3D Gaussian reconstruction during inference. To eliminate the need for accurate pose input during reconstruction, we anchor one input view's local camera coordinates as the canonical space and train the network to predict Gaussian primitives for all views within this space. This approach obviates the need to transform Gaussian primitives from local coordinates into a global coordinate system, thus avoiding errors associated with per-frame Gaussians and pose estimation. To resolve scale ambiguity, we design and compare various intrinsic embedding methods, ultimately opting to convert camera intrinsics into a token embedding and concatenate it with image tokens as input to the model, enabling accurate scene scale prediction. We utilize the reconstructed 3D Gaussians for novel view synthesis and pose estimation tasks and propose a two-stage coarse-to-fine pipeline for accurate pose estimation. Experimental results demonstrate that our pose-free approach can achieve superior novel view synthesis quality compared to pose-required methods, particularly in scenarios with limited input image overlap. For pose estimation, our method, trained without ground truth depth or explicit matching loss, significantly outperforms the state-of-the-art methods with substantial improvements. This work makes significant advances in pose-free generalizable 3D reconstruction and demonstrates its applicability to real-world scenarios. Code and trained models are available at https://noposplat.github.io/. | Botao Ye, Sifei Liu, Haofei Xu, Xueting Li, Marc Pollefeys, MingHsuan Yang, Songyou Peng |  |
| 169 |  |  [Variational Diffusion Posterior Sampling with Midpoint Guidance](https://openreview.net/forum?id=6EUtjXAvmj) |  | 0 | Diffusion models have recently shown considerable potential in solving Bayesian inverse problems when used as priors. However, sampling from the resulting denoising posterior distributions remains a challenge as it involves intractable terms. To tackle this issue, state-of-the-art approaches formulate the problem as that of sampling from a surrogate diffusion model targeting the posterior and decompose its scores into two terms: the prior score and an intractable guidance term. While the former is replaced by the pre-trained score of the considered diffusion model, the guidance term has to be estimated. In this paper, we propose a novel approach that utilises a decomposition of the transitions which, in contrast to previous methods, allows a trade-off between the complexity of the intractable guidance term and that of the prior transitions. We validate the proposed approach through extensive experiments on linear and nonlinear inverse problems, including challenging cases with latent diffusion models as priors, and demonstrate its effectiveness in reconstructing electrocardiogram (ECG) from partial measurements for accurate cardiac diagnosis. | Badr Moufad, Yazid Janati, Lisa Bedin, Alain Oliviero Durmus, Randal Douc, Eric Moulines, Jimmy Olsson |  |
| 170 |  |  [Towards Understanding Why FixMatch Generalizes Better Than Supervised Learning](https://openreview.net/forum?id=25kAzqzTrz) |  | 0 | Semi-supervised learning (SSL), exemplified by FixMatch (Sohn et al., 2020), has shown significant generalization advantages over supervised learning (SL), particularly in the context of deep neural networks (DNNs). However, it is still unclear, from a theoretical standpoint, why FixMatch-like SSL algorithms generalize better than SL on DNNs. In this work, we present the first theoretical justification for the enhanced test accuracy observed in FixMatch-like SSL applied to DNNs by taking convolutional neural networks (CNNs) on classification tasks as an example. Our theoretical analysis reveals that the semantic feature learning processes in FixMatch and SL are rather different. In particular, FixMatch learns all the discriminative features of each semantic class, while SL only randomly captures a subset of features due to the well-known lottery ticket hypothesis. Furthermore, we show that our analysis framework can be applied to other FixMatch-like SSL methods, e.g., FlexMatch, FreeMatch, Dash, and SoftMatch. Inspired by our theoretical analysis, we develop an improved variant of FixMatch, termed Semantic-Aware FixMatch (SA-FixMatch). Experimental results corroborate our theoretical findings and the enhanced generalization capability of SA-FixMatch. | Jingyang Li, Jiachun Pan, Vincent Y. F. Tan, KimChuan Toh, Pan Zhou |  |
| 171 |  |  [NeuralPlane: Structured 3D Reconstruction in Planar Primitives with Neural Fields](https://openreview.net/forum?id=5UKrnKuspb) |  | 0 | 3D maps assembled from planar primitives are compact and expressive in representing man-made environments. In this paper, we present \*\*NeuralPlane\*\*, a novel approach that explores \*\*neural\*\* fields for multi-view 3D \*\*plane\*\* reconstruction. Our method is centered upon the core idea of distilling geometric and semantic cues from inconsistent 2D plane observations into a unified 3D neural representation, which unlocks the full leverage of plane attributes. It is accomplished through several key designs, including: 1) a monocular module that generates geometrically smooth and semantically meaningful segments known as 2D plane observations, 2) a plane-guided training procedure that implicitly learns accurate 3D geometry from the multi-view plane observations, and 3) a self-supervised feature field termed \*Neural Coplanarity Field\* that enables the modeling of scene semantics alongside the geometry. Without relying on prior plane annotations, our method achieves high-fidelity reconstruction comprising planar primitives that are not only crisp but also well-aligned with the semantic content. Comprehensive experiments on ScanNetv2 and ScanNet++ demonstrate the superiority of our method in both geometry and semantics. | Hanqiao Ye, Yuzhou Liu, Yangdong Liu, Shuhan Shen |  |
| 172 |  |  [Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models](https://openreview.net/forum?id=I4e82CIDxv) |  | 0 | We introduce methods for discovering and applying \*\*sparse feature circuits\*\*. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms in neural networks. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors. | Samuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David Bau, Aaron Mueller |  |
| 173 |  |  [Retrieval Head Mechanistically Explains Long-Context Factuality](https://openreview.net/forum?id=EytBpUGB1Z) |  | 0 | Despite the recent progress in long-context language models, it remains elusive how transformer-based models exhibit the capability to retrieve relevant information from arbitrary locations within the long context. This paper aims to address this question. Our systematic investigation across a wide spectrum of models reveals that a special type of attention heads are largely responsible for retrieving information, which we dub retrieval heads. We identify intriguing properties of retrieval heads:(1) universal: all the explored models with long-context capability have a set of retrieval heads; (2) sparse: only a small portion (less than 5\%) of the attention heads are retrieval. (3) intrinsic: retrieval heads already exist in models pretrained with short context. When extending the context length by continual pretraining, it is still the same set of heads that perform information retrieval. (4) dynamically activated: take Llama-2 7B for example, 12 retrieval heads always attend to the required information no matter how the context is changed. The rest of the retrieval heads are activated in different contexts. (5) causal: completely pruning retrieval heads leads to failure in retrieving relevant information and results in hallucination, while pruning random non-retrieval heads does not affect the model's retrieval ability. We further show that retrieval heads strongly influence chain-of-thought (CoT) reasoning, where the model needs to frequently refer back the question and previously-generated context. Conversely, tasks where the model directly generates the answer using its intrinsic knowledge are less impacted by masking out retrieval heads. These observations collectively explain which internal part of the model seeks information from the input tokens. We believe our insights will foster future research on reducing hallucination, improving reasoning, and compressing the KV cache. | Wenhao Wu, Yizhong Wang, Guangxuan Xiao, Hao Peng, Yao Fu |  |
| 174 |  |  [High-Dynamic Radar Sequence Prediction for Weather Nowcasting Using Spatiotemporal Coherent Gaussian Representation](https://openreview.net/forum?id=Cjz9Xhm7sI) |  | 0 | Weather nowcasting is an essential task that involves predicting future radar echo sequences based on current observations, offering significant benefits for disaster management, transportation, and urban planning. Current prediction methods are limited by training and storage efficiency, mainly focusing on 2D spatial predictions at specific altitudes. Meanwhile, 3D volumetric predictions at each timestamp remain largely unexplored. To address such a challenge, we introduce a comprehensive framework for 3D radar sequence prediction in weather nowcasting, using the newly proposed SpatioTemporal Coherent Gaussian Splatting (STC-GS) for dynamic radar representation and GauMamba for efficient and accurate forecasting. Specifically, rather than relying on a 4D Gaussian for dynamic scene reconstruction, STC-GS optimizes 3D scenes at each frame by employing a group of Gaussians while effectively capturing their movements across consecutive frames. It ensures consistent tracking of each Gaussian over time, making it particularly effective for prediction tasks. With the temporally correlated Gaussian groups established, we utilize them to train GauMamba, which integrates a memory mechanism into the Mamba framework. This allows the model to learn the temporal evolution of Gaussian groups while efficiently handling a large volume of Gaussian tokens. As a result, it achieves both efficiency and accuracy in forecasting a wide range of dynamic meteorological radar signals. The experimental results demonstrate that our STC-GS can efficiently represent 3D radar sequences with over $16\times$ higher spatial resolution compared with the existing 3D representation methods, while GauMamba outperforms state-of-the-art methods in forecasting a broad spectrum of high-dynamic weather conditions. | Ziye Wang, Yiran Qin, Lin Zeng, Ruimao Zhang |  |
| 175 |  |  [Differential Transformer](https://openreview.net/forum?id=OvoCm1gGhN) |  | 0 | Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture for large language models. | Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, Furu Wei |  |
| 176 |  |  [Open-Vocabulary Customization from CLIP via Data-Free Knowledge Distillation](https://openreview.net/forum?id=1aF2D2CPHi) |  | 0 | Vision-language models such as CLIP have demonstrated strong zero-shot performance, but their considerable size and inefficient inference limit customizable deployment for users. While knowledge distillation is a solution, it still requires the original data, which is not always available due to copyrights and privacy concerns. For many users seeking open-vocabulary customization, Data-Free Knowledge Distillation (DFKD) emerges as a promising direction. Upon rethinking DFKD, we find that existing methods fail on CLIP due to their heavy reliance on BatchNorm layers, which are unexpectedly unusable in CLIP. Based on our findings, we adopt image-text matching to achieve DFKD for CLIP, enabling customization based on arbitrary class texts. This involves (i) inversing a surrogate dataset from CLIP based on text prompts; and (ii) distilling a student model from CLIP using the surrogate dataset. Specifically, we introduce style dictionary diversification to enhance the diversity of synthetic images. To prevent uncontrollable semantics introduced by diversification, we propose a class consistency maintaining strategy to ensure the consistency of synthetic images. Based on synthetic images with various styles, we further propose meta knowledge distillation to train the student model with good generalization ability. Moreover, we introduce a simple yet effective method to enable customization based on few example images. Comprehensive experiments showcase the superiority of our approach across twelve customized tasks, achieving a 9.33\% improvement compared to existing DFKD methods. | Yongxian Wei, Zixuan Hu, Li Shen, Zhenyi Wang, Chun Yuan, Dacheng Tao |  |
| 177 |  |  [Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement](https://openreview.net/forum?id=UHPnqSTBPO) |  | 0 | We present a principled approach to provide LLM-based evaluation with a rigorous guarantee of human agreement. We first propose that a reliable evaluation method should not uncritically rely on model preferences for pairwise evaluation, but rather assess the confidence of judge models and selectively decide when to trust its judgement. We then show that under this \*selective evaluation\* framework, human agreement can be provably guaranteed---such that the model evaluation aligns with that of humans to a user-specified agreement level. As part of our framework, we also introduce \*Simulated Annotators\*, a novel confidence estimation method that significantly improves judge calibration and thus enables high coverage of evaluated instances. Finally, we propose \*Cascaded Selective Evaluation\*, where we use cheaper models as initial judges and escalate to stronger models only when necessary---again, while still providing a provable guarantee of human agreement. Experimental results show that Cascaded Selective Evaluation guarantees strong alignment with humans, far beyond what LLM judges could achieve without selective evaluation. For example, on a subset of Chatbot Arena where GPT-4 almost never achieves 80% human agreement, our method, even while employing substantially cost-effective models such as Mistral-7B, \*guarantees\* over 80% human agreement with almost 80% test coverage. | Jaehun Jung, Faeze Brahman, Yejin Choi |  |
| 178 |  |  [Your Mixture-of-Experts LLM Is Secretly an Embedding Model for Free](https://openreview.net/forum?id=eFGQ97z5Cd) |  | 0 | While large language models (LLMs) excel on generation tasks, their decoder-only architecture often limits their potential as embedding models if no further representation finetuning is applied. Does this contradict their claim of generalists? To answer the question, we take a closer look at Mixture-of-Experts (MoE) LLMs. Our study shows that the expert routers in MoE LLMs can serve as an off-the-shelf embedding model with promising performance on a diverse class of embedding-focused tasks, without requiring any finetuning. Moreover, our extensive analysis shows that the MoE routing weights (RW) is complementary to the hidden state (HS) of LLMs, a widely-used embedding. Compared to HS, we find that RW is more robust to the choice of prompts and focuses on high-level semantics. Motivated by the analysis, we propose MoEE combining RW and HS, which achieves better performance than using either separately. Our exploration of their combination and prompting strategy shed several novel insights, e.g., a weighted sum of RW and HS similarities outperforms the similarity on their concatenation. Our experiments are conducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding Benchmark (MTEB). The results demonstrate the significant improvement brought by MoEE to LLM-based embedding without further finetuning. | Ziyue Li, Tianyi Zhou |  |
| 179 |  |  [REEF: Representation Encoding Fingerprints for Large Language Models](https://openreview.net/forum?id=SnDmPkOJ0T) |  | 0 | Protecting the intellectual property of open-source Large Language Models (LLMs) is very important, because training LLMs costs extensive computational resources and data. Therefore, model owners and third parties need to identify whether a suspect model is a subsequent development of the victim model. To this end, we propose a training-free REEF to identify the relationship between the suspect and victim models from the perspective of LLMs' feature representations. Specifically, REEF computes and compares the centered kernel alignment similarity between the representations of a suspect model and a victim model on the same samples. This training-free REEF does not impair the model's general capabilities and is robust to sequential fine-tuning, pruning, model merging, and permutations. In this way, REEF provides a simple and effective way for third parties and models' owners to protect LLMs' intellectual property together. Our code is publicly accessible at https://github.com/AI45Lab/REEF. | Jie Zhang, Dongrui Liu, Chen Qian, Linfeng Zhang, Yong Liu, Yu Qiao, Jing Shao |  |
| 180 |  |  [Flat Reward in Policy Parameter Space Implies Robust Reinforcement Learning](https://openreview.net/forum?id=4OaO3GjP7k) |  | 0 | Investigating flat minima on loss surfaces in parameter space is well-documented in the supervised learning context, highlighting its advantages for model generalization. However, limited attention has been paid to the reinforcement learning (RL) context, where the impact of flatter reward landscapes in policy parameter space remains largely unexplored. Beyond merely extrapolating from supervised learning, which suggests a link between flat reward landscapes and enhanced generalization, we aim to formally connect the flatness of the reward surface to the robustness of RL models. In policy models where a deep neural network determines actions, flatter reward landscapes in response to parameter perturbations lead to consistent rewards even when actions are perturbed. Moreover, robustness to action perturbations further enhances robustness against other variations, such as changes in state transition probabilities and reward functions. We extensively simulate various RL environments, confirming the consistent benefits of flatter reward landscapes in enhancing the robustness of RL under diverse conditions, including action selection, transition dynamics, and reward functions. The code for these experiments is available at https://github.com/HK-05/flatreward-RRL. | HyunKyu Lee, Sung Whan Yoon |  |
| 181 |  |  [LLM-SR: Scientific Equation Discovery via Programming with Large Language Models](https://openreview.net/forum?id=m2nmp8P5in) |  | 0 | Mathematical equations have been unreasonably effective in describing complex natural phenomena across various scientific disciplines. However, discovering such insightful equations from data presents significant challenges due to the necessity of navigating extremely large combinatorial hypothesis spaces. Current methods of equation discovery, commonly known as symbolic regression techniques, largely focus on extracting equations from data alone, often neglecting the domain-specific prior knowledge that scientists typically depend on. They also employ limited representations such as expression trees, constraining the search space and expressiveness of equations. To bridge this gap, we introduce LLM-SR, a novel approach that leverages the extensive scientific knowledge and robust code generation capabilities of Large Language Models (LLMs) to discover scientific equations from data. Specifically, LLM-SR treats equations as programs with mathematical operators and combines LLMs' scientific priors with evolutionary search over equation programs. The LLM iteratively proposes new equation skeleton hypotheses, drawing from its domain knowledge, which are then optimized against data to estimate parameters. We evaluate LLM-SR on four benchmark problems across diverse scientific domains (e.g., physics, biology), which we carefully designed to simulate the discovery process and prevent LLM recitation. Our results demonstrate that LLM-SR discovers physically accurate equations that significantly outperform state-of-the-art symbolic regression baselines, particularly in out-of-domain test settings. We also show that LLM-SR's incorporation of scientific priors enables more efficient equation space exploration than the baselines. | Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, Chandan K. Reddy |  |
| 182 |  |  [Backtracking Improves Generation Safety](https://openreview.net/forum?id=Bo62NeU6VF) |  | 0 | Text generation has a fundamental limitation almost by definition: there is no taking back tokens that have been generated, even when they are clearly problematic. In the context of language model safety, when a partial unsafe generation is produced, language models by their nature tend to happily keep on generating similarly unsafe additional text. This is in fact how safety alignment of frontier models gets circumvented in the wild, despite great efforts in improving their safety. Deviating from the paradigm of approaching safety alignment as prevention (decreasing the probability of harmful responses), we propose backtracking, a technique that allows language models to "undo" and recover from their own unsafe generation through the introduction of a special [RESET] token. Our method can be incorporated into either SFT or DPO training to optimize helpfulness and harmlessness. We show that models trained to backtrack are consistently safer than baseline models: backtracking Llama-3-8B is four times more safe than the baseline model (6.1\% $\to$ 1.5\%) in our evaluations without regression in helpfulness. Our method additionally provides protection against four adversarial attacks including an adaptive attack, despite not being trained to do so. | Yiming Zhang, Jianfeng Chi, Hailey Nguyen, Kartikeya Upasani, Daniel M. Bikel, Jason E. Weston, Eric Michael Smith |  |
| 183 |  |  [Rethinking the generalization of drug target affinity prediction algorithms via similarity aware evaluation](https://openreview.net/forum?id=j7cyANIAxV) |  | 0 | Drug-target binding affinity prediction is a fundamental task for drug discovery. It has been extensively explored in literature and promising results are reported. However, in this paper, we demonstrate that the results may be misleading and cannot be well generalized to real practice. The core observation is that the canonical randomized split of a test set in conventional evaluation leaves the test set dominated by samples with high similarity to the training set. The performance of models is severely degraded on samples with lower similarity to the training set but the drawback is highly overlooked in current evaluation. As a result, the performance can hardly be trusted when the model meets low-similarity samples in real practice. To address this problem, we propose a framework of similarity aware evaluation in which a novel split methodology is proposed to adapt to any desired distribution. This is achieved by a formulation of optimization problems which are approximately and efficiently solved by gradient descent. We perform extensive experiments across five representative methods in four datasets for two typical target evaluations and compare them with various counterpart methods. Results demonstrate that the proposed split methodology can significantly better fit desired distributions and guide the development of models. | Chenbin Zhang, Zhiqiang Hu, Chuchu Jiang, Wen Chen, Jie Xu, Shaoting Zhang |  |
| 184 |  |  [GridMix: Exploring Spatial Modulation for Neural Fields in PDE Modeling](https://openreview.net/forum?id=Fur0DtynPX) |  | 0 | Significant advancements have been achieved in PDE modeling using neural fields. Despite their effectiveness, existing methods rely on global modulation, limiting their ability to reconstruct local details. While spatial modulation with vanilla grid-based representations offers a promising alternative, it struggles with inadequate global information modeling and over-fitting to the training spatial domain. To address these challenges, we propose GridMix, a novel approach that models spatial modulation as a mixture of grid-based representations. GridMix effectively explores global structures while preserving locality for fine-grained modulation. Furthermore, we introduce spatial domain augmentation to enhance the robustness of the modulated neural fields against spatial domain variations. With all these innovations, our comprehensive approach culminates in MARBLE, a framework that significantly advancing the capabilities of neural fields in PDE modeling. The effectiveness of MARBLE is extensively validated on diverse benchmarks encompassing dynamics modeling and geometric prediction. | Honghui Wang, Shiji Song, Gao Huang |  |
| 185 |  |  [Data Selection via Optimal Control for Language Models](https://openreview.net/forum?id=dhAL5fy8wS) |  | 0 | This work investigates the selection of high-quality pre-training data from massive corpora to enhance LMs' capabilities for downstream usage. We formulate data selection as a generalized Optimal Control problem, which can be solved theoretically by Pontryagin's Maximum Principle (PMP), yielding a set of necessary conditions that characterize the relationship between optimal data selection and LM training dynamics. Based on these theoretical results, we introduce \*\*P\*\*MP-based \*\*D\*\*ata \*\*S\*\*election (\*\*PDS\*\*), a framework that approximates optimal data selection by solving the PMP conditions. In our experiments, we adopt PDS to select data from CommmonCrawl and show that the PDS-selected corpus accelerates the learning of LMs and constantly boosts their performance on a wide range of downstream tasks across various model sizes. Moreover, the benefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by the extrapolation of the test loss curves according to the Scaling Laws. PDS also improves data utilization when the pre-training data is limited, by reducing the data demand by 1.8 times, which helps mitigate the quick exhaustion of available web-crawled corpora. Our code, model, and data can be found at https://github.com/microsoft/LMOps/tree/main/data_selection. | Yuxian Gu, Li Dong, Hongning Wang, Yaru Hao, Qingxiu Dong, Furu Wei, Minlie Huang |  |
| 186 |  |  [Simplifying, Stabilizing and Scaling Continuous-time Consistency Models](https://openreview.net/forum?id=LyJi5ugyJx) |  | 0 | Consistency models (CMs) are a powerful class of diffusion-based generative models optimized for fast sampling. Most existing CMs are trained using discretized timesteps, which introduce additional hyperparameters and are prone to discretization errors. While continuous-time formulations can mitigate these issues, their success has been limited by training instability. To address this, we propose a simplified theoretical framework that unifies previous parameterizations of diffusion models and CMs, identifying the root causes of instability. Based on this analysis, we introduce key improvements in diffusion process parameterization, network architecture, and training objectives. These changes enable us to train continuous-time CMs at an unprecedented scale, reaching 1.5B parameters on ImageNet 512×512. Our proposed training algorithm, using only two sampling steps, achieves FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 64×64, and 1.88 on ImageNet 512×512, narrowing the gap in FID scores with the best existing diffusion models to within 10\%. | Cheng Lu, Yang Song |  |
| 187 |  |  [Dynamic Multimodal Evaluation with Flexible Complexity by Vision-Language Bootstrapping](https://openreview.net/forum?id=X1OfiRYCLn) |  | 0 | Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across multimodal tasks such as visual perception and reasoning, leading to good performance on various multimodal evaluation benchmarks. However, these benchmarks keep a static nature and overlap with the pre-training data, resulting in fixed complexity constraints and data contamination issues. This raises the concern regarding the validity of the evaluation. To address these two challenges, we introduce a dynamic multimodal evaluation protocol called Vision-Language Bootstrapping (VLB). VLB provides a robust and comprehensive assessment for LVLMs with reduced data contamination and flexible complexity. To this end, VLB dynamically generates new visual question-answering samples through a multimodal bootstrapping module that modifies both images and language, while ensuring that newly generated samples remain consistent with the original ones by a judge module. By composing various bootstrapping strategies, VLB offers dynamic variants of existing benchmarks with diverse complexities, enabling the evaluation to co-evolve with the ever-evolving capabilities of LVLMs. Extensive experimental results across multiple benchmarks, including SEEDBench, MMBench, and MME, show that VLB significantly reduces data contamination and exposes performance limitations of LVLMs. | Yue Yang, Shuibo Zhang, Kaipeng Zhang, Yi Bin, Yu Wang, Ping Luo, Wenqi Shao |  |
| 188 |  |  [Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models](https://openreview.net/forum?id=mtJSMcF3ek) |  | 0 | Self-improvement is a mechanism in Large Language Model (LLM) pre-training, post-training and test-time inference. We explore a framework where the model verifies its own outputs, filters or reweights data based on this verification, and distills the filtered data. Despite several empirical successes, a fundamental understanding is still lacking. In this work, we initiate a comprehensive, modular and controlled study on LLM self-improvement. We provide a mathematical formulation for self-improvement, which is largely governed by a quantity which we formalize as the \*\*generation-verification gap\*\*. Through experiments with various model families and tasks, we discover a scaling phenomenon of self-improvement -- a variant of the generation-verification gap scales monotonically with the model pre-training flops. We also examine when self-improvement is possible, an iterative self-improvement procedure, and ways to improve its performance. Our findings not only advance understanding of LLM self-improvement with practical implications, but also open numerous avenues for future research into its capabilities and boundaries. | Yuda Song, Hanlin Zhang, Carson Eisenach, Sham M. Kakade, Dean P. Foster, Udaya Ghai |  |
| 189 |  |  [SANA: Efficient High-Resolution Text-to-Image Synthesis with Linear Diffusion Transformers](https://openreview.net/forum?id=N8Oj1XhtYZ) |  | 0 | We introduce Sana, a text-to-image framework that can efficiently generate images up to 4096$\times$4096 resolution. Sana can synthesize high-resolution, high-quality images with strong text-image alignment at a remarkably fast speed, deployable on laptop GPU. Core designs include: (1) Deep compression autoencoder: unlike traditional AEs, which compress images only 8$\times$, we trained an AE that can compress images 32$\times$, effectively reducing the number of latent tokens. (2) Linear DiT: we replace all vanilla attention in DiT with linear attention, which is more efficient at high resolutions without sacrificing quality. (3) Decoder-only text encoder: we replaced T5 with modern decoder-only small LLM as the text encoder and designed complex human instruction with in-context learning to enhance the image-text alignment. (4) Efficient training and sampling: we propose Flow-DPM-Solver to reduce sampling steps, with efficient caption labeling and selection to accelerate convergence. As a result, Sana-0.6B is very competitive with modern giant diffusion model (e.g. Flux-12B), being 20 times smaller and 100+ times faster in measured throughput. Moreover, Sana-0.6B can be deployed on a 16GB laptop GPU, taking less than 1 second to generate a 1024$\times$1024 resolution image. Sana enables content creation at low cost. Code and model will be publicly released upon publication. | Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, Song Han |  |
| 190 |  |  [Can a MISL Fly? Analysis and Ingredients for Mutual Information Skill Learning](https://openreview.net/forum?id=xoIeVdFO7U) |  | 0 | Self-supervised learning has the potential of lifting several of the key challenges in reinforcement learning today, such as exploration, representation learning, and reward design. Recent work (METRA) has effectively argued that moving away from mutual information and instead optimizing a certain Wasserstein distance is important for good performance. In this paper, we argue that the benefits seen in that paper can largely be explained within the existing framework of mutual information skill learning (MISL). Our analysis suggests a new MISL method (contrastive successor features) that retains the excellent performance of METRA with fewer moving parts, and highlights connections between skill learning, contrastive representation learning, and successor features. Finally, through careful ablation studies, we provide further insight into some of the key ingredients for both our method and METRA. | Chongyi Zheng, Jens Tuyls, Joanne Peng, Benjamin Eysenbach |  |
| 191 |  |  [When Selection Meets Intervention: Additional Complexities in Causal Discovery](https://openreview.net/forum?id=xByvdb3DCm) |  | 0 | We address the common yet often-overlooked selection bias in interventional studies, where subjects are selectively enrolled into experiments. For instance, participants in a drug trial are usually patients of the relevant disease; A/B tests on mobile applications target existing users only, and gene perturbation studies typically focus on specific cell types, such as cancer cells. Ignoring this bias leads to incorrect causal discovery results. Even when recognized, the existing paradigm for interventional causal discovery still fails to address it. This is because subtle differences in _when_ and _where_ interventions happen can lead to significantly different statistical patterns. We capture this dynamic by introducing a graphical model that explicitly accounts for both the observed world (where interventions are applied) and the counterfactual world (where selection occurs while interventions have not been applied). We characterize the Markov property of the model, and propose a provably sound algorithm to identify causal relations as well as selection mechanisms up to the equivalence class, from data with soft interventions and unknown targets. Through synthetic and real-world experiments, we demonstrate that our algorithm effectively identifies true causal relations despite the presence of selection bias. | Haoyue Dai, Ignavier Ng, Jianle Sun, Zeyu Tang, Gongxu Luo, Xinshuai Dong, Peter Spirtes, Kun Zhang |  |
| 192 |  |  [LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias](https://openreview.net/forum?id=QQBPWtvtcn) |  | 0 | We propose the Large View Synthesis Model (LVSM), a novel transformer-based approach for scalable and generalizable novel view synthesis from sparse-view inputs. We introduce two architectures: (1) an encoder-decoder LVSM, which encodes input image tokens into a fixed number of 1D latent tokens, functioning as a fully learned scene representation, and decodes novel-view images from them; and (2) a decoder-only LVSM, which directly maps input images to novel-view outputs, completely eliminating intermediate scene representations. Both models bypass the 3D inductive biases used in previous methods---from 3D representations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar projections, plane sweeps)---addressing novel view synthesis with a fully data-driven approach. While the encoder-decoder model offers faster inference due to its independent latent representation, the decoder-only LVSM achieves superior quality, scalability, and zero-shot generalization, outperforming previous state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive evaluations across multiple datasets demonstrate that both LVSM variants achieve state-of-the-art novel view synthesis quality, delivering superior performance even with reduced computational resources (1-2 GPUs). Please see our anonymous website for more details: https://haian-jin.github.io/projects/LVSM/ | Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi, Tianyuan Zhang, Fujun Luan, Noah Snavely, Zexiang Xu |  |
| 193 |  |  [Flow Matching with General Discrete Paths: A Kinetic-Optimal Perspective](https://openreview.net/forum?id=tcvMzR2NrP) |  | 0 | The design space of discrete-space diffusion or flow generative models are significantly less well-understood than their continuous-space counterparts, with many works focusing only on a simple masked construction. In this work, we aim to take a holistic approach to the construction of discrete generative models based on continuous-time Markov chains, and for the first time, allow the use of arbitrary discrete probability paths, or colloquially, corruption processes. Through the lens of optimizing the symmetric kinetic energy, we propose velocity formulas that can be applied to any given probability path, completely decoupling the probability and velocity, and giving the user the freedom to specify any desirable probability path based on expert knowledge specific to the data domain. Furthermore, we find that a special construction of mixture probability paths optimizes the symmetric kinetic energy for the discrete case. We empirically validate the usefulness of this new design space across multiple modalities: text generation, inorganic material generation, and image generation. We find that we can outperform the mask construction even in text with kinetic-optimal mixture paths, while we can make use of domain-specific constructions of the probability path over the visual domain. | Neta Shaul, Itai Gat, Marton Havasi, Daniel Severo, Anuroop Sriram, Peter Holderrieth, Brian Karrer, Yaron Lipman, Ricky T. Q. Chen |  |
| 194 |  |  [Cut Your Losses in Large-Vocabulary Language Models](https://openreview.net/forum?id=E4Fk3YuG56) |  | 0 | As language models grow ever larger, so do their vocabularies. This has shifted the memory footprint of LLMs during training disproportionately to one single layer: the cross-entropy in the loss computation. Cross-entropy builds up a logit matrix with entries for each pair of input tokens and vocabulary items and, for small models, consumes an order of magnitude more memory than the rest of the LLM combined. We propose Cut Cross-Entropy (CCE), a method that computes the cross-entropy loss without materializing the logits for all tokens into global memory. Rather, CCE only computes the logit for the correct token and evaluates the log-sum-exp over all logits on the fly. We implement a custom kernel that performs the matrix multiplications and the log-sum-exp reduction over the vocabulary in flash memory, making global memory consumption for the cross-entropy computation negligible. This has a dramatic effect. Taking the Gemma 2 (2B) model as an example, CCE reduces the memory footprint of the loss computation from 24 GB to 1 MB, and the total training-time memory consumption of the classifier head from 28 GB to 1 GB. To improve the throughput of CCE, we leverage the inherent sparsity of softmax and propose to skip elements of the gradient computation that have a negligible (i.e. below numerical precision) contribution to the gradient. Experiments demonstrate that the dramatic reduction in memory consumption is accomplished without sacrificing training speed or convergence. | Erik Wijmans, Brody Huval, Alexander Hertzberg, Vladlen Koltun, Philipp Krähenbühl |  |
| 195 |  |  [AFlow: Automating Agentic Workflow Generation](https://openreview.net/forum?id=z5uVAKwmjf) |  | 0 | Large language models (LLMs) have demonstrated remarkable potential in solving complex tasks across diverse domains, typically by employing agentic workflows that follow detailed instructions and operational sequences. However, constructing these workflows requires significant human effort, limiting scalability and generalizability. Recent research has sought to automate the generation and optimization of these workflows, but existing methods still rely on initial manual setup and fall short of achieving fully automated and effective workflow generation. To address this challenge, we reformulate workflow optimization as a search problem over code-represented workflows, where LLM-invoking nodes are connected by edges. We introduce AFLOW, an automated framework that efficiently explores this space using Monte Carlo Tree Search, iteratively refining workflows through code modification, tree-structured experience, and execution feedback. Empirical evaluations across six benchmark datasets demonstrate AFLOW's efficacy, yielding a 5.7% average improvement over state-of-the-art baselines. Furthermore, AFLOW enables smaller models to outperform GPT-4o on specific tasks at 4.55% of its inference cost in dollars. The code is available at https://github.com/FoundationAgents/AFlow. | Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, Bingnan Zheng, Bang Liu, Yuyu Luo, Chenglin Wu |  |
| 196 |  |  [Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Models](https://openreview.net/forum?id=uAFHCZRmXk) |  | 0 | Contrastive vision-language models (VLMs), like CLIP, have gained popularity for their versatile applicability to various downstream tasks. Despite their successes in some tasks, like zero-shot object recognition, they perform surprisingly poor on other tasks, like attribute recognition. Previous work has attributed these challenges to the modality gap, a separation of image and text in the shared representation space, and to a bias towards objects over other factors, such as attributes. In this analysis paper, we investigate both phenomena thoroughly. We evaluated off-the-shelf VLMs and while the gap's influence on performance is typically overshadowed by other factors, we find indications that closing the gap indeed leads to improvements. Moreover, we find that, contrary to intuition, only few embedding dimensions drive the gap and that the embedding spaces are differently organized. To allow for a clean study of object bias, we introduce a definition and a corresponding measure of it. Equipped with this tool, we find that object bias does not lead to worse performance on other concepts, such as attributes per se. However, why do both phenomena, modality gap and object bias, emerge in the first place? To answer this fundamental question and uncover some of the inner workings of contrastive VLMs, we conducted experiments that allowed us to control the amount of shared information between the modalities. These experiments revealed that the driving factor behind both the modality gap and the object bias, is an information imbalance between images and captions, and unveiled an intriguing connection between the modality gap and entropy of the logits. | Simon Schrodi, David T. Hoffmann, Max Argus, Volker Fischer, Thomas Brox |  |
| 197 |  |  [FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference](https://openreview.net/forum?id=OfjIlbelrT) |  | 0 | Large language models (LLMs) encounter computational challenges during long-sequence inference, especially in the attention pre-filling phase, where the complexity grows quadratically with the prompt length. Previous efforts to mitigate these challenges have relied on fixed sparse attention patterns or identifying sparse attention patterns based on limited cases. However, these methods lacked the flexibility to efficiently adapt to varying input demands. In this paper, we introduce FlexPrefill, a Flexible sparse Pre-filling mechanism that dynamically adjusts sparse attention patterns and computational budget in real-time to meet the specific requirements of each input and attention head. The flexibility of our method is demonstrated through two key innovations: 1) Query-Aware Sparse Pattern Determination: By measuring Jensen-Shannon divergence, this component adaptively switches between query-specific diverse attention patterns and predefined attention patterns. 2) Cumulative-Attention Based Index Selection: This component dynamically selects query-key indexes to be computed based on different attention patterns, ensuring the sum of attention scores meets a predefined threshold. FlexPrefill adaptively optimizes the sparse pattern and sparse ratio of each attention head based on the prompt, enhancing efficiency in long-sequence inference tasks. Experimental results show significant improvements in both speed and accuracy over prior methods, providing a more flexible and efficient solution for LLM inference. | Xunhao Lai, Jianqiao Lu, Yao Luo, Yiyuan Ma, Xun Zhou |  |
| 198 |  |  [REGENT: A Retrieval-Augmented Generalist Agent That Can Act In-Context in New Environments](https://openreview.net/forum?id=NxyfSW6mLK) |  | 0 | Building generalist agents that can rapidly adapt to new environments is a key challenge for deploying AI in the digital and real worlds. Is scaling current agent architectures the most effective way to build generalist agents? We propose a novel approach to pre-train relatively small policies on relatively small datasets and adapt them to unseen environments via in-context learning, without any finetuning. Our key idea is that retrieval offers a powerful bias for fast adaptation. Indeed, we demonstrate that even a simple retrieval-based 1-nearest neighbor agent offers a surprisingly strong baseline for today's state-of-the-art generalist agents. From this starting point, we construct a semi-parametric agent, REGENT, that trains a transformer-based policy on sequences of queries and retrieved neighbors. REGENT can generalize to unseen robotics and game-playing environments via retrieval augmentation and in-context learning, achieving this with up to 3x fewer parameters and up to an order-of-magnitude fewer pre-training datapoints, significantly outperforming today's state-of-the-art generalist agents. | Kaustubh Sridhar, Souradeep Dutta, Dinesh Jayaraman, Insup Lee |  |
| 199 |  |  [MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models](https://openreview.net/forum?id=HnhNRrLPwm) |  | 0 | Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this capability remains insufficient. Existing benchmarks suffer from limitations in data scale, scope, and evaluation depth, while current evaluation metrics are often costly or biased, lacking in reliability for practical applications. To address these challenges, we introduce MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs). MMIE comprises 20K meticulously curated multimodal queries, spanning 3 categories, 12 fields, and 102 subfields, including mathematics, coding, physics, literature, health, and arts. It supports both interleaved inputs and outputs, offering a mix of multiple-choice and open-ended question formats to evaluate diverse competencies. Moreover, we propose a reliable automated evaluation metric, leveraging a scoring model fine-tuned with human-annotated data and systematic evaluation criteria, aimed at reducing bias and improving evaluation accuracy. Extensive experiments demonstrate the effectiveness of our benchmark and metrics in providing a comprehensive evaluation of interleaved LVLMs. Specifically, we evaluate eight LVLMs, revealing that even the best models show significant room for improvement, with most achieving only moderate results. We believe MMIE will drive further advancements in the development of interleaved LVLMs. | Peng Xia, Siwei Han, Shi Qiu, Yiyang Zhou, Zhaoyang Wang, Wenhao Zheng, Zhaorun Chen, Chenhang Cui, Mingyu Ding, Linjie Li, Lijuan Wang, Huaxiu Yao |  |
| 200 |  |  [Do as We Do, Not as You Think: the Conformity of Large Language Models](https://openreview.net/forum?id=st77ShxP1K) |  | 0 | Recent advancements in large language models (LLMs) revolutionize the field of intelligent agents, enabling collaborative multi-agent systems capable of tackling complex problems across various domains. However, the potential of conformity within these systems, analogous to phenomena like conformity bias and group-think in human group dynamics, remains largely unexplored, raising concerns about their collective problem-solving capabilities and possible ethical implications. This paper presents a comprehensive study on conformity in LLM-driven multi-agent systems, focusing on three aspects: the existence of conformity, the factors influencing conformity, and potential mitigation strategies. In particular, we introduce BenchForm, a new conformity-oriented benchmark, featuring reasoning-intensive tasks and five distinct interaction protocols designed to probe LLMs’ behavior in collaborative scenarios. Several representative LLMs are evaluated on BenchForm, using metrics such as conformity rate and independence rate to quantify conformity’s impact. Our analysis delves into factors influencing conformity, including interaction time and majority size, and examines how the subject agent rationalize its conforming behavior. Furthermore, we explore two strategies to mitigate conformity effects, i.e., developing enhanced persona and implementing a reflection mechanism. Several interesting findings regarding LLMs’ conformity are derived from empirical results and case studies. We hope that these insights can pave the way for more robust and ethically-aligned collaborative AI systems. Our benchmark and code are available at BenchForm. | Zhiyuan Weng, Guikun Chen, Wenguan Wang |  |
| 201 |  |  [Artificial Kuramoto Oscillatory Neurons](https://openreview.net/forum?id=nwDRD4AMoN) |  | 0 | It has long been known in both neuroscience and AI that \`\`binding'' between neurons leads to a form of competitive learning where representations are compressed in order to represent more abstract concepts in deeper layers of the network. More recently, it was also hypothesized that dynamic (spatiotemporal) representations play an important role in both neuroscience and AI. Building on these ideas, we introduce Artificial Kuramoto Oscillatory Neurons (AKOrN) as a dynamical alternative to threshold units, which can be combined with arbitrary connectivity designs such as fully connected, convolutional, or attentive mechanisms. Our generalized Kuramoto updates bind neurons together through their synchronization dynamics. We show that this idea provides performance improvements across a wide spectrum of tasks such as unsupervised object discovery, adversarial robustness, calibrated uncertainty quantification, and reasoning. We believe that these empirical results show the importance of rethinking our assumptions at the most basic neuronal level of neural representation, and in particular show the importance of dynamical representations. Code: https://github.com/autonomousvision/akorn Project page: https://takerum.github.io/akorn_project_page/ | Takeru Miyato, Sindy Löwe, Andreas Geiger, Max Welling |  |
| 202 |  |  [Booster: Tackling Harmful Fine-tuning for Large Language Models via Attenuating Harmful Perturbation](https://openreview.net/forum?id=tTPHgb0EtV) |  | 0 | Harmful fine-tuning attack poses serious safety concerns for large language models' fine-tuning-as-a-service. While existing defenses have been proposed to mitigate the issue, their performances are still far away from satisfactory, and the root cause of the problem has not been fully recovered. To this end, we in this paper show that \textit{harmful perturbation} over the model weights could be a probable cause of alignment-broken. In order to attenuate the negative impact of harmful perturbation, we propose an alignment-stage solution, dubbed Booster. Technically, along with the original alignment loss, we append a loss regularizer in the alignment stage's optimization. The regularizer ensures that the model's harmful loss reduction after the simulated harmful perturbation is attenuated, thereby mitigating the subsequent fine-tuning risk. Empirical results show that Booster can effectively reduce the harmful score of the fine-tuned models while maintaining the performance of downstream tasks. Our code is available at https://github.com/git-disl/Booster | Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Ling Liu |  |
| 203 |  |  [Unlearning-based Neural Interpretations](https://openreview.net/forum?id=PBjCTeDL6o) |  | 0 | Gradient-based interpretations often require an anchor point of comparison to avoid saturation in computing feature importance. We show that current baselines defined using static functions—constant mapping, averaging or blurring—inject harmful colour, texture or frequency assumptions that deviate from model behaviour. This leads to accumulation of irregular gradients, resulting in attribution maps that are biased, fragile and manipulable. Departing from the static approach, we propose $\texttt{UNI}$ to compute an (un)learnable, debiased and adaptive baseline by perturbing the input towards an $\textit{unlearning direction}$ of steepest ascent. Our method discovers reliable baselines and succeeds in erasing salient features, which in turn locally smooths the high-curvature decision boundaries. Our analyses point to unlearning as a promising avenue for generating faithful, efficient and robust interpretations. | Ching Lam Choi, Alexandre Duplessis, Serge J. Belongie |  |
| 204 |  |  [ChartMoE: Mixture of Diversely Aligned Expert Connector for Chart Understanding](https://openreview.net/forum?id=o5TsWTUSeF) |  | 0 | Automatic chart understanding is crucial for content comprehension and document parsing. Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in chart understanding through domain-specific alignment and fine-tuning. However, current MLLMs still struggle to provide faithful data and reliable analysis only based on charts. To address it, we propose ChartMoE, which employs the Mixture of Expert (MoE) architecture to replace the traditional linear projector to bridge the modality gap. Specifically, we train several linear connectors through distinct alignment tasks, which are utilized as the foundational initialization parameters for different experts. Additionally, we introduce ChartMoE-Align, a dataset with nearly 1 million chart-table-JSON-code quadruples to conduct three alignment tasks (chart-table/JSON/code). Combined with the vanilla connector, we initialize different experts diversely and adopt high-quality knowledge learning to further refine the MoE connector and LLM parameters. Extensive experiments demonstrate the effectiveness of the MoE connector and our initialization strategy, e.g., ChartMoE improves the accuracy of the previous state-of-the-art from 80.48% to 84.64% on the ChartQA benchmark. | Zhengzhuo Xu, Bowen Qu, Yiyan Qi, Sinan Du, Chengjin Xu, Chun Yuan, Jian Guo |  |
| 205 |  |  [Probabilistic Learning to Defer: Handling Missing Expert Annotations and Controlling Workload Distribution](https://openreview.net/forum?id=zl0HLZOJC9) |  | 0 | Recent progress in machine learning research is gradually shifting its focus towards \*human-AI cooperation\* due to the advantages of exploiting the reliability of human experts and the efficiency of AI models. One of the promising approaches in human-AI cooperation is \*learning to defer\* (L2D), where the system analyses the input data and decides to make its own decision or defer to human experts. Although L2D has demonstrated state-of-the-art performance, in its standard setting, L2D entails a severe limitation: all human experts must annotate the whole training dataset of interest, resulting in a time-consuming and expensive annotation process that can subsequently influence the size and diversity of the training set. Moreover, the current L2D does not have a principled way to control workload distribution among human experts and the AI classifier, which is critical to optimise resource allocation. We, therefore, propose a new probabilistic modelling approach inspired by the mixture-of-experts, where the Expectation - Maximisation algorithm is leverage to address the issue of missing expert's annotations. Furthermore, we introduce a constraint, which can be solved efficiently during the E-step, to control the workload distribution among human experts and the AI classifier. Empirical evaluation on synthetic and real-world datasets shows that our proposed probabilistic approach performs competitively, or surpasses previously proposed methods assessed on the same benchmarks. | Cuong C. Nguyen, ThanhToan Do, Gustavo Carneiro |  |
| 206 |  |  [A Decade's Battle on Dataset Bias: Are We There Yet?](https://openreview.net/forum?id=SctfBCLmWo) |  | 0 | We revisit the \`\`dataset classification'' experiment suggested by Torralba & Efros (2011) a decade ago, in the new era with large-scale, diverse, and hopefully less biased datasets as well as more capable neural network architectures. Surprisingly, we observe that modern neural networks can achieve excellent accuracy in classifying which dataset an image is from: e.g., we report 84.7% accuracy on held-out validation data for the three-way classification problem consisting of the YFCC, CC, and DataComp datasets. Our further experiments show that such a dataset classifier could learn semantic features that are generalizable and transferable, which cannot be explained by memorization. We hope our discovery will inspire the community to rethink issues involving dataset bias. | Zhuang Liu, Kaiming He |  |
| 207 |  |  [Knowing Your Target: Target-Aware Transformer Makes Better Spatio-Temporal Video Grounding](https://openreview.net/forum?id=WOzffPgVjF) |  | 0 | Transformer has attracted increasing interest in spatio-temporal video grounding, or STVG, owing to its end-to-end pipeline and promising result. Existing Transformer-based STVG approaches often leverage a set of object queries, which are initialized simply using zeros and then gradually learn target position information via iterative interactions with multimodal features, for spatial and temporal localization. Despite simplicity, these zero object queries, due to lacking target-specific cues, are hard to learn discriminative target information from interactions with multimodal features in complicated scenarios (e.g., with distractors or occlusion), resulting in degradation. Addressing this, we introduce a novel $\textbf{T}$arget-$\textbf{A}$ware Transformer for $\textbf{STVG}$ ($\textbf{TA-STVG}$), which seeks to adaptively generate object queries via exploring target-specific cues from the given video-text pair, for improving STVG. The key lies in two simple yet effective modules, comprising text-guided temporal sampling (TTS) and attribute-aware spatial activation (ASA), working in a cascade. The former focuses on selecting target-relevant temporal cues from a video utilizing holistic text information, while the latter aims at further exploiting the fine-grained visual attribute information of the object from previous target-aware temporal cues, which is applied for object query initialization. Compared to existing methods leveraging zero-initialized queries, object queries in our TA-STVG, directly generated from a given video-text pair, naturally carry target-specific cues, making them adaptive and better interact with multimodal features for learning more discriminative information to improve STVG. In our experiments on three benchmarks, including HCSTVG-v1/-v2 and VidSTG, TA-STVG achieves state-of-the-art performance and significantly outperforms the baseline, validating its efficacy. Moreover, TTS and ASA are designed for general purpose. When applied to existing methods such as TubeDETR and STCAT, we show substantial performance gains, verifying its generality. Code is released at https://github.com/HengLan/TA-STVG. | Xin Gu, Yaojie Shen, Chenxi Luo, Tiejian Luo, Yan Huang, Yuewei Lin, Heng Fan, Libo Zhang |  |
| 208 |  |  [Open-World Reinforcement Learning over Long Short-Term Imagination](https://openreview.net/forum?id=vzItLaEoDa) |  | 0 | Training visual reinforcement learning agents in a high-dimensional open world presents significant challenges. While various model-based methods have improved sample efficiency by learning interactive world models, these agents tend to be “short-sighted”, as they are typically trained on short snippets of imagined experiences. We argue that the primary challenge in open-world decision-making is improving the exploration efficiency across a vast state space, especially for tasks that demand consideration of long-horizon payoffs. In this paper, we present LS-Imagine, which extends the imagination horizon within a limited number of state transition steps, enabling the agent to explore behaviors that potentially lead to promising long-term feedback. The foundation of our approach is to build a $\textit{long short-term world model}$. To achieve this, we simulate goal-conditioned jumpy state transitions and compute corresponding affordance maps by zooming in on specific areas within single images. This facilitates the integration of direct long-term values into behavior learning. Our method demonstrates significant improvements over state-of-the-art techniques in MineDojo. | Jiajian Li, Qi Wang, Yunbo Wang, Xin Jin, Yang Li, Wenjun Zeng, Xiaokang Yang |  |
| 209 |  |  [OLMoE: Open Mixture-of-Experts Language Models](https://openreview.net/forum?id=xXTkbTBmqq) |  | 0 | We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present novel findings on MoE training, define and analyze new routing properties showing high specialization in our model, and open-source all our work: model weights, training data, code, and logs. | Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Evan Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, et al. |  |
| 210 |  |  [Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference under Ambiguities](https://openreview.net/forum?id=84pDoCD4lH) |  | 0 | Spatial expressions in situated communication can be ambiguous, as their meanings vary depending on the frames of reference (FoR) adopted by speakers and listeners. While spatial language understanding and reasoning by vision-language models (VLMs) have gained increasing attention, potential ambiguities in these models are still under-explored. To address this issue, we present the COnsistent Multilingual Frame Of Reference Test (COMFORT), an evaluation protocol to systematically assess the spatial reasoning capabilities of VLMs. We evaluate nine state-of-the-art VLMs using COMFORT. Despite showing some alignment with English conventions in resolving ambiguities, our experiments reveal significant shortcomings of VLMs: notably, the models (1) exhibit poor robustness and consistency, (2) lack the flexibility to accommodate multiple FoRs, and (3) fail to adhere to language-specific or culture-specific conventions in cross-lingual tests, as English tends to dominate other languages. With a growing effort to align vision-language models with human cognitive intuitions, we call for more attention to the ambiguous nature and cross-cultural diversity of spatial reasoning. | Zheyuan Zhang, Fengyuan Hu, Jayjun Lee, Freda Shi, Parisa Kordjamshidi, Joyce Chai, Ziqiao Ma |  |
| 211 |  |  [SAM 2: Segment Anything in Images and Videos](https://openreview.net/forum?id=Ha6RTeWMd0) |  | 0 | We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, the dataset, an interactive demo and code. | Nikhila Ravi, Valentin Gabeur, YuanTing Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloé Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross B. Girshick, Piotr Dollár, Christoph Feichtenhofer |  |
| 212 |  |  [A Computational Framework for Modeling Emergence of Color Vision in the Human Brain](https://openreview.net/forum?id=g3xuCtrG6H) |  | 0 | It is a mystery how the brain decodes color vision purely from the optic nerve signals it receives, with a core inferential challenge being how it disentangles internal perception with the correct color dimensionality from the unknown encoding properties of the eye. In this paper, we introduce a computational framework for modeling this emergence of human color vision by simulating both the eye and the cortex. Existing research often overlooks how the cortex develops color vision or represents color space internally, assuming that the color dimensionality is known a priori; however, we argue that the visual cortex has the capability and the challenge of inferring the color dimensionality purely from fluctuations in the optic nerve signals. To validate our theory, we introduce a simulation engine for biological eyes based on established vision science and generate optic nerve signals resulting from looking at natural images. Further, we propose a bio-plausible model of cortical learning based on self-supervised prediction of optic nerve signal fluctuations under natural eye motions. We show that this model naturally learns to generate color vision by disentangling retinal invariants from the sensory signals. When the retina contains $N$ types of color photoreceptors, our simulation shows that $N$-dimensional color vision naturally emerges, verified through formal colorimetry. Using this framework, we also present the first simulation work that successfully boosts the color dimensionality, as observed in gene therapy on squirrel monkeys, and demonstrates the possibility of enhancing human color vision from 3D to 4D. | Atsunobu Kotani, Ren Ng |  |
| 213 |  |  [PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding](https://openreview.net/forum?id=Q6a9W6kzv5) |  | 0 | Understanding the physical world is a fundamental challenge in embodied AI, critical for enabling agents to perform complex tasks and operate safely in real-world environments. While Vision-Language Models (VLMs) have shown great promise in reasoning and task planning for embodied agents, their ability to comprehend physical phenomena remains extremely limited. To close this gap, we introduce PhysBench, a comprehensive benchmark designed to evaluate VLMs' physical world understanding capability across a diverse set of tasks. PhysBench contains 10,002 entries of interleaved video-image-text data, categorized into four major domains: physical object properties, physical object relationships, physical scene understanding, and physics-based dynamics, further divided into 19 subclasses and 8 distinct capability dimensions. Our extensive experiments, conducted on 75 representative VLMs, reveal that while these models excel in common-sense reasoning, they struggle with understanding the physical world---likely due to the absence of physical knowledge in their training data and the lack of embedded physical priors. To tackle the shortfall, we introduce PhysAgent, a novel framework that combines the generalization strengths of VLMs with the specialized expertise of vision models, significantly enhancing VLMs' physical understanding across a variety of tasks, including an 18.4\% improvement on GPT-4o. Furthermore, our results demonstrate that enhancing VLMs' physical world understanding capabilities can help embodied agents such as MOKA. We believe that PhysBench and PhysAgent offer valuable insights and contribute to bridging the gap between VLMs and physical world understanding. [Project Page is here](https://physbench.github.io/) | Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Campagnolo Guizilini, Yue Wang |  |
| 214 |  |  [How new data permeates LLM knowledge and how to dilute it](https://openreview.net/forum?id=NGKQoaqLpo) |  | 0 | Large language models continually learn through the accumulation of gradient-based updates, but how individual pieces of new information affect existing knowledge, leading to both beneficial generalization and problematic hallucination, remains poorly understood. We demonstrate that when learning new information, LLMs exhibit a "priming" effect: learning a new fact can cause the model to inappropriately apply that knowledge in unrelated contexts. To systematically study this phenomenon, we introduce "Outlandish," a carefully curated dataset of 1320 diverse text samples designed to probe how new knowledge permeates through an LLM's existing knowledge base. Using this dataset, we show that the degree of priming after learning new information can be predicted by measuring the token probability of key words before training. This relationship holds robustly across different model architectures (PALM-2, Gemma, Llama), sizes, and training stages. Finally, we develop two novel techniques to modulate how new knowledge affects existing model behavior: (1) a \`\`stepping-stone'' text augmentation strategy and (2) an \`\`ignore-k'' update pruning method. These approaches reduce undesirable priming effects by 50-95% while preserving the model's ability to learn new information. Our findings provide both empirical insights into how LLMs learn and practical tools for improving the specificity of knowledge insertion in language models. Further materials: https://sunchipsster1.github.io/projects/outlandish/ | Chen Sun, Renat Aksitov, Andrey Zhmoginov, Nolan Andrew Miller, Max Vladymyrov, Ulrich Rueckert, Been Kim, Mark Sandler |  |
| 215 |  |  [UniCBE: An Uniformity-driven Comparing Based Evaluation Framework with Unified Multi-Objective Optimization](https://openreview.net/forum?id=rpwGUtTeA5) |  | 0 | Human preference plays a significant role in measuring large language models and guiding them to align with human values. Unfortunately, current comparing-based evaluation (CBE) methods typically focus on a single optimization objective, failing to effectively utilize scarce yet valuable preference signals. To address this, we delve into key factors that can enhance the accuracy, convergence, and scalability of CBE: suppressing sampling bias, balancing descending process of uncertainty, and mitigating updating uncertainty. Following the derived guidelines, we propose UniCBE, a unified uniformity-driven CBE framework which simultaneously optimize these core objectives by constructing and integrating three decoupled sampling probability matrices, each designed to ensure uniformity in specific aspects. We further ablate the optimal tuple sampling and preference aggregation strategies to achieve efficient CBE. On the AlpacaEval benchmark, UniCBE saves over 17% of evaluation budgets while achieving a Pearson correlation with ground truth exceeding 0.995, demonstrating excellent accuracy and convergence. In scenarios where new models are continuously introduced, UniCBE can even save over 50% of evaluation costs, highlighting its improved scalability. | Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Yueqi Zhang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li |  |
| 216 |  |  [NetMoE: Accelerating MoE Training through Dynamic Sample Placement](https://openreview.net/forum?id=1qP3lsatCR) |  | 0 | Mixture of Experts (MoE) is a widely used technique to expand model sizes for better model quality while maintaining the computation cost constant. In a nutshell, an MoE model consists of multiple experts in each model layer and routes the training tokens to only a fixed number of experts rather than all. In distributed training, as experts are distributed among different GPUs, All-to-All communication is necessary to exchange the training tokens among the GPUs after each time of expert routing. Due to the frequent and voluminous data exchanges, All-to-All communication has become a notable challenge to training efficiency. In this paper, we manage to accelerate All-to-All communication in MoE models from the training sample perspective, which is unexplored so far. In particular, we put forward the observation that tokens in the same training sample have certain levels of locality in expert routing. Motivated by this, we develop NetMoE, which takes such locality into account and dynamically rearranges the placement of training samples to minimize All-to-All communication costs. Specifically, we model the All-to-All communication given the sample placement and formulate an integer programming problem to deduce the optimal placement in polynomial time. Experiments with 32 GPUs show that NetMoE achieves a maximum efficiency improvement of $1.67 \times$ compared with current MoE training frameworks. | Xinyi Liu, Yujie Wang, Fangcheng Fu, Xupeng Miao, Shenhan Zhu, Xiaonan Nie, Bin Cui |  |
| 217 |  |  [TabReD: Analyzing Pitfalls and Filling the Gaps in Tabular Deep Learning Benchmarks](https://openreview.net/forum?id=L14sqcrUC3) |  | 0 | Advances in machine learning research drive progress in real-world applications. To ensure this progress, it is important to understand the potential pitfalls on the way from a novel method's success on academic benchmarks to its practical deployment. In this work, we analyze existing tabular deep learning benchmarks and find two common characteristics of tabular data in typical industrial applications that are underrepresented in the datasets usually used for evaluation in the literature. First, in real-world deployment scenarios, distribution of data often changes over time. To account for this distribution drift, time-based train/test splits should be used in evaluation. However, existing academic tabular datasets often lack timestamp metadata to enable such evaluation. Second, a considerable portion of datasets in production settings stem from extensive data acquisition and feature engineering pipelines. This can have an impact on the absolute and relative number of predictive, uninformative, and correlated features compared to academic datasets. In this work, we aim to understand how recent research advances in tabular deep learning transfer to these underrepresented conditions. To this end, we introduce TabReD -- a collection of eight industry-grade tabular datasets. We reassess a large number of tabular ML models and techniques on TabReD. We demonstrate that evaluation on both time-based data splits and richer feature sets leads to different methods ranking, compared to evaluation on random splits and smaller number of features, which are common in academic benchmarks. Furthermore, simple MLP-like architectures and GBDT show the best results on the TabReD datasets, while other methods are less effective in the new setting. | Ivan Rubachev, Nikolay Kartashev, Yury Gorishniy, Artem Babenko |  |
| 218 |  |  [Improving the Sparse Structure Learning of Spiking Neural Networks from the View of Compression Efficiency](https://openreview.net/forum?id=gcouwCx7dG) |  | 0 | The human brain utilizes spikes for information transmission and dynamically reorganizes its network structure to boost energy efficiency and cognitive capabilities throughout its lifespan. Drawing inspiration from this spike-based computation, Spiking Neural Networks (SNNs) have been developed to construct event-driven models that emulate this efficiency. Despite these advances, deep SNNs continue to suffer from over-parameterization during training and inference, a stark contrast to the brain’s ability to self-organize. Furthermore, existing sparse SNNs are challenged by maintaining optimal pruning levels due to a static pruning ratio, resulting in either under or over-pruning. In this paper, we propose a novel two-stage dynamic structure learning approach for deep SNNs, aimed at maintaining effective sparse training from scratch while optimizing compression efficiency. The first stage evaluates the compressibility of existing sparse subnetworks within SNNs using the PQ index, which facilitates an adaptive determination of the rewiring ratio for synaptic connections based on data compression insights. In the second stage, this rewiring ratio critically informs the dynamic synaptic connection rewiring process, including both pruning and regrowth. This approach significantly improves the exploration of sparse structures training in deep SNNs, adapting sparsity dynamically from the point view of compression efficiency. Our experiments demonstrate that this sparse training approach not only aligns with the performance of current deep SNNs models but also significantly improves the efficiency of compressing sparse SNNs. Crucially, it preserves the advantages of initiating training with sparse models and offers a promising solution for implementing Edge AI on neuromorphic hardware. | Jiangrong Shen, Qi Xu, Gang Pan, Badong Chen |  |
| 219 |  |  [JudgeLM: Fine-tuned Large Language Models are Scalable Judges](https://openreview.net/forum?id=xsELpEPn4A) |  | 0 | Evaluating Large Language Models (LLMs) in open-ended scenarios is challenging because existing benchmarks and metrics can not measure them comprehensively. To address this problem, we propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. We first propose a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges, as well as a new benchmark for evaluating the judges. We train JudgeLM at different scales from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its capabilities and behaviors. We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance. JudgeLM obtains the state-of-the-art judge performance on both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an agreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM also demonstrates extended capabilities in being judges of the single answer, multimodal models, multiple answers, multi-turn chat, etc. | Lianghui Zhu, Xinggang Wang, Xinlong Wang |  |
| 220 |  |  [Direct Post-Training Preference Alignment for Multi-Agent Motion Generation Model Using Implicit Feedback from Pre-training Demonstrations](https://openreview.net/forum?id=8UFG9D8xeU) |  | 0 | Recent advancements in Large Language Models (LLMs) have revolutionized motion generation models in embodied applications such as autonomous driving and robotic manipulation. While LLM-type auto-regressive motion generation models benefit from training scalability, there remains a discrepancy between their token prediction objectives and human preferences. As a result, models pre-trained solely with token-prediction objectives often generate behaviors that deviate from what humans would prefer, making post-training preference alignment crucial for producing human-preferred motions. Unfortunately, post-training alignment requires extensive preference rankings of motions generated by the pre-trained model, which are costly and time-consuming to annotate, especially in multi-agent motion generation settings. Recently, there has been growing interest in leveraging expert demonstrations previously used during pre-training to scalably generate preference data for post-training alignment. However, these methods often adopt an adversarial assumption, treating all pre-trained model-generated samples as unpreferred examples and relying solely on pre-training expert demonstrations to construct preferred examples. This adversarial approach overlooks the valuable signal provided by preference rankings among the model's own generations, ultimately reducing alignment effectiveness and potentially leading to misaligned behaviors. In this work, instead of treating all generated samples as equally bad, we propose a principled approach that leverages implicit preferences encoded in pre-training expert demonstrations to construct preference rankings among the pre-trained model's generations, offering more nuanced preference alignment guidance with zero human cost. We apply our approach to large-scale traffic simulation (more than 100 agents) and demonstrate its effectiveness in improving the realism of pre-trained model's generated behaviors, making a lightweight 1M motion generation model comparable to state-of-the-art large imitation-based models by relying solely on implicit feedback from pre-training demonstrations, without requiring additional post-training human preference annotations or incurring high computational costs. Furthermore, we provide an in-depth analysis of preference data scaling laws and their effects on over-optimization, offering valuable insights for future studies. | Thomas Tian, Kratarth Goel |  |
| 221 |  |  [Online Preference Alignment for Language Models via Count-based Exploration](https://openreview.net/forum?id=cfKZ5VrhXt) |  | 0 | Reinforcement Learning from Human Feedback (RLHF) has shown great potential in fine-tuning Large Language Models (LLMs) to align with human preferences. Existing methods perform preference alignment from a fixed dataset, which can be limited in data coverage and the resulting reward model is hard to generalize in out-of-distribution responses. Thus, online RLHF is more desirable to empower the LLM to explore outside the support of the initial dataset by iteratively collecting the prompt-response pairs. In this paper, we study the fundamental problem in online RLHF, i.e., how to explore for LLM. We give a theoretical motivation in linear reward assumption to show that an optimistic reward with an upper confidence bound (UCB) term leads to a provably efficient RLHF policy. Then, we reformulate our objective to direct preference optimization with an exploration term, where the UCB-term can be converted to a count-based exploration bonus. We further propose a practical algorithm, named Count-based Online Preference Optimization (COPO), which leverages a simple coin-flip counting module to estimate the pseudo-count of a prompt-response pair in previously collected data. COPO encourages LLMs to balance exploration and preference optimization in an iterative manner, which enlarges the exploration space and the entire data coverage of iterative LLM policies. We conduct online RLHF experiments on Zephyr and Llama-3 models. The results on instruction-following and standard academic benchmarks show that COPO significantly increases performance. | Chenjia Bai, Yang Zhang, Shuang Qiu, Qiaosheng Zhang, Kang Xu, Xuelong Li |  |
| 222 |  |  [Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence](https://openreview.net/forum?id=o1Et3MogPw) |  | 0 | The rapid advancement of large language models (LLMs) has paved the way for the development of highly capable autonomous agents. However, existing multi-agent frameworks often struggle with integrating diverse capable third-party agents due to reliance on agents defined within their own ecosystems. They also face challenges in simulating distributed environments, as most frameworks are limited to single-device setups. Furthermore, these frameworks often rely on hard-coded communication pipelines, limiting their adaptability to dynamic task requirements. Inspired by the concept of the Internet, we propose the Internet of Agents (IoA), a novel framework that addresses these limitations by providing a flexible and scalable platform for LLM-based multi-agent collaboration. IoA introduces an agent integration protocol, an instant-messaging-like architecture design, and dynamic mechanisms for agent teaming and conversation flow control. Through extensive experiments on general assistant tasks, embodied AI tasks, and retrieval-augmented generation benchmarks, we demonstrate that IoA consistently outperforms state-of-the-art baselines, showcasing its ability to facilitate effective collaboration among heterogeneous agents. IoA represents a step towards linking diverse agents in an Internet-like environment, where agents can seamlessly collaborate to achieve greater intelligence and capabilities. We will release our code to facilitate further research. | Weize Chen, Ziming You, Ran Li, Yitong Guan, Chen Qian, Chenyang Zhao, Cheng Yang, Ruobing Xie, Zhiyuan Liu, Maosong Sun |  |
| 223 |  |  [Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning](https://openreview.net/forum?id=A6Y7AqlzLW) |  | 0 | A promising approach for improving reasoning in large language models is to use process reward models (PRMs). PRMs provide feedback at each step of a multi-step reasoning trace, improving credit assignment over outcome reward models (ORMs) that only provide feedback at the final step. However, collecting dense, per-step human labels is not scalable, and training PRMs from automatically-labeled data has thus far led to limited gains. With the goal of using PRMs to improve a \*base\* policy via test-time search and reinforcement learning (RL), we ask: \`\`How should we design process rewards?'' Our key insight is that, to be effective, the process reward for a step should measure \*progress\*: a change in the likelihood of producing a correct response in the future, before and after taking the step, as measured under a \*prover\* policy distinct from the base policy. Such progress values can {distinguish} good and bad steps generated by the base policy, even though the base policy itself cannot. Theoretically, we show that even weaker provers can improve the base policy, as long as they distinguish steps without being too misaligned with the base policy. Our results show that process rewards defined as progress under such provers improve the efficiency of exploration during test-time search and online RL. We empirically validate our claims by training \*\*process advantage verifiers (PAVs)\*\* to measure progress under such provers and show that compared to ORM, they are >8% more accurate, and 1.5-5x more compute-efficient. Equipped with these insights, our PAVs enable \*\*one of the first results\*\* showing a 6x gain in sample efficiency for a policy trained using online RL with PRMs vs. ORMs. | Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, Aviral Kumar |  |
| 224 |  |  [MQuAKE-Remastered: Multi-Hop Knowledge Editing Can Only Be Advanced with Reliable Evaluations](https://openreview.net/forum?id=m9wG6ai2Xk) |  | 0 | Large language models (LLMs) can give out erroneous answers to factually rooted questions either as a result of undesired training outcomes or simply because the world has moved on after a certain knowledge cutoff date. Under such scenarios, \*knowledge editing\* often comes to the rescue by delivering efficient patches for such erroneous answers without significantly altering the rest, where many editing methods have seen reasonable success when the editing targets are simple and direct (e.g., \*\`\`what club does Lionel Messi currently play for?''\*). However, knowledge fragments like this are often deeply intertwined in the real world, making effectively propagating the editing effect to non-directly related questions a practical challenge (to entertain an extreme example: [\*"What car did the wife of the owner of the club that Messi currently plays for used to get to school in the 80s?"\*](youtube.com/watch?v=DbwiHC1Fu-E\&t=132s)). Prior arts have coined this task as \*multi-hop knowledge editing\* with the most popular dataset being MQuAKE, serving as the sole evaluation benchmark for many later proposed editing methods due to the expensive nature of constructing knowledge editing datasets at scale. In this work, we reveal that \*\*up to 33\% or 76\% of \mquake{}'s questions and ground truth labels are, in fact, corrupted in various fashions due to some unintentional clerical or procedural oversights\*\*. Our work provides a detailed audit of MQuAKE's error pattern and a comprehensive fix without sacrificing its dataset capacity. Additionally, we benchmarked almost all proposed MQuAKE-evaluated editing methods on our post-fix dataset, \*\*MQuAKE-Remastered\*\*. We observe that many methods try to overfit the original MQuAKE by exploiting some dataset idiosyncrasies of MQuAKE. We provide a guideline on how to approach such datasets faithfully and show that a simple, minimally invasive approach — \*\*GWalk\*\* — can offer beyond SOTA editing performance without such exploitation. The MQuAKE-Remastered datasets and utilities are available at [huggingface.co/datasets/henryzhongsc/MQuAKE-Remastered](https://huggingface.co/datasets/henryzhongsc/MQuAKE-Remastered) and [github.com/henryzhongsc/MQuAKE-Remastered](https://github.com/henryzhongsc/MQuAKE-Remastered), respectively. | Shaochen (Henry) Zhong, Yifan Lu, Lize Shao, Bhargav Bhushanam, Xiaocong Du, Yixin Wan, Yucheng Shi, Daochen Zha, Yiwei Wang, Ninghao Liu, Kaixiong Zhou, Shuai Xu, KaiWei Chang, Louis Feng, Vipin Chaudhary, Xia Hu |  |
| 225 |  |  [Competition Dynamics Shape Algorithmic Phases of In-Context Learning](https://openreview.net/forum?id=XgH1wfHSX8) |  | 0 | In-Context Learning (ICL) has significantly expanded the general-purpose nature of large language models, allowing them to adapt to novel tasks using merely the inputted context. This has motivated a series of papers that analyze tractable synthetic domains and postulate precise mechanisms that may underlie ICL. However, the use of relatively distinct setups that often lack a sequence modeling nature to them makes it unclear how general the reported insights from such studies are. Motivated by this, we propose a synthetic sequence modeling task that involves learning to simulate a finite mixture of Markov chains. As we show, models trained on this task reproduce most well-known results on ICL, hence offering a unified setting for studying the concept. Building on this setup, we demonstrate we can explain a model’s behavior by decomposing it into four broad algorithms that combine a fuzzy retrieval vs. inference approach with either unigram or bigram statistics of the context. These algorithms engage in a competitive dynamics to dominate model behavior, with the precise experimental conditions dictating which algorithm ends up superseding others: e.g., we find merely varying context size or amount of training yields (at times sharp) transitions between which algorithm dictates the model behavior, revealing a mechanism that explains the transient nature of ICL. In this sense, we argue ICL is best thought of as a mixture of different algorithms, each with its own peculiarities, instead of a monolithic capability. This also implies that making general claims about ICL that hold universally across all settings may be infeasible. | Core Francisco Park, Ekdeep Singh Lubana, Hidenori Tanaka |  |
| 226 |  |  [In vivo cell-type and brain region classification via multimodal contrastive learning](https://openreview.net/forum?id=10JOlFIPjt) |  | 0 | Current electrophysiological approaches can track the activity of many neurons, yet it is usually unknown which cell-types or brain areas are being recorded without further molecular or histological analysis. Developing accurate and scalable algorithms for identifying the cell-type and brain region of recorded neurons is thus crucial for improving our understanding of neural computation. In this work, we develop a multimodal contrastive learning approach for neural data that can be fine-tuned for different downstream tasks, including inference of cell-type and brain location. We utilize multimodal contrastive learning to jointly embed the activity autocorrelations and extracellular waveforms of individual neurons. We demonstrate that our embedding approach, Neuronal Embeddings via MultimOdal Contrastive Learning (NEMO), paired with supervised fine-tuning, achieves state-of-the-art cell-type classification for two opto-tagged datasets and brain region classification for the public International Brain Laboratory Brain-wide Map dataset. Our method represents a promising step towards accurate cell-type and brain region classification from electrophysiological recordings. | Han Yu, Hanrui Lyu, YiXun Xu, Charlie Windolf, Eric Kenji Lee, Fan Yang, Andrew M. Shelton, Olivier Winter, International Brain Laboratory, Eva L. Dyer, Chandramouli Chandrasekaran, Nicholas A. Steinmetz, Liam Paninski, Cole Lincoln Hurwitz |  |
| 227 |  |  [Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws](https://openreview.net/forum?id=FxNNiUgtfa) |  | 0 | Scaling laws describe the relationship between the size of language models and their capabilities. Unlike prior studies that evaluate a model's capability via loss or benchmarks, we estimate information-theoretically the number of knowledge \emph{bits} a model stores. We focus on factual knowledge represented as tuples, such as (USA, capital, Washington D.C.) from a Wikipedia page. Through multiple controlled datasets, we establish that language models can and only can store \emph{2 bits of knowledge per parameter, even when quantized to int8}, and such knowledge can be flexibly extracted for downstream applications. More broadly, we present 12 results on how (1) training duration, (2) model architecture, (3) quantization, (4) sparsity constraints such as MoE, and (5) data signal-to-noise ratio affect a model's knowledge storage capacity. | Zeyuan AllenZhu, Yuanzhi Li |  |
| 228 |  |  [Scalable Decision-Making in Stochastic Environments through Learned Temporal Abstraction](https://openreview.net/forum?id=pQsllTesiE) |  | 0 | Sequential decision-making in high-dimensional continuous action spaces, particularly in stochastic environments, faces significant computational challenges. We explore this challenge in the traditional offline RL setting, where an agent must learn how to make decisions based on data collected through a stochastic behavior policy. We present \textit{Latent Macro Action Planner} (L-MAP), which addresses this challenge by learning a set of temporally extended macro-actions through a state-conditional Vector Quantized Variational Autoencoder (VQ-VAE), effectively reducing action dimensionality. L-MAP employs a (separate) learned prior model that acts as a latent transition model and allows efficient sampling of plausible actions. During planning, our approach accounts for stochasticity in both the environment and the behavior policy by using Monte Carlo tree search (MCTS). In offline RL settings, including stochastic continuous control tasks, L-MAP efficiently searches over discrete latent actions to yield high expected returns. Empirical results demonstrate that L-MAP maintains low decision latency despite increased action dimensionality. Notably, across tasks ranging from continuous control with inherently stochastic dynamics to high-dimensional robotic hand manipulation, L-MAP significantly outperforms existing model-based methods and performs on par with strong model-free actor-critic baselines, highlighting the effectiveness of the proposed approach in planning in complex and stochastic environments with high-dimensional action spaces. | Baiting Luo, Ava Pettet, Aron Laszka, Abhishek Dubey, Ayan Mukhopadhyay |  |
| 229 |  |  [Modeling Complex System Dynamics with Flow Matching Across Time and Conditions](https://openreview.net/forum?id=hwnObmOTrV) |  | 0 | Modeling the dynamics of complex real-world systems from temporal snapshot data is crucial for understanding phenomena such as gene regulation, climate change, and financial market fluctuations. Researchers have recently proposed a few methods based either on the Schroedinger Bridge or Flow Matching to tackle this problem, but these approaches remain limited in their ability to effectively combine data from multiple time points and different experimental settings. This integration is essential in real-world scenarios where observations from certain combinations of time points and experimental conditions are missing, either because of experimental costs or sensory failure. To address this challenge, we propose a novel method named Multi-Marginal Flow Matching (MMFM). MMFM first constructs a flow using smooth spline-based interpolation across time points and conditions and regresses it with a neural network using the classifier-free guided Flow Matching framework. This framework allows for the sharing of contextual information about the dynamics across multiple trajectories. We demonstrate the effectiveness of our method on both synthetic and real-world datasets, including a recent single-cell genomics data set with around a hundred chemical perturbations across time points. Our results show that MMFM significantly outperforms existing methods at imputing data at missing time points. | Martin Rohbeck, Edward De Brouwer, Charlotte Bunne, JanChristian Huetter, Anne Biton, Kelvin Y. Chen, Aviv Regev, Romain Lopez |  |
| 230 |  |  [Joint Reward and Policy Learning with Demonstrations and Human Feedback Improves Alignment](https://openreview.net/forum?id=VCbqXtS5YY) |  | 0 | Aligning to human preferences and/or intentions is an important requirement for contemporary foundation models. To ensure alignment, popular approaches such as reinforcement learning with human feedback (RLHF) break down the task into three stages: (i) a model is computed with supervised fine-tuning (SFT) based upon large demonstrations data, (ii) a reward model (RM) is estimated based upon human feedback data, and (iii) reinforcement learning (RL) is used to further refine the SFT model by optimizing the estimated reward model. Demonstrations and human feedback data reflect human user preferences in different ways. As a result, the reward model estimate obtained from only human feedback data is likely not as accurate as a reward model estimate obtained from both demonstration and human feedback data. A policy model that optimizes the reward model estimate obtained from both demonstration and human feedback data will likely exhibit better alignment performance. We introduce a tractable algorithm for finding the reward and policy models and provide a finite-time performance guarantee. Additionally, we demonstrate the efficiency of the proposed solution with extensive experiments including alignment problems in LLMs and robotic control problems in MuJoCo. We observe that the proposed solutions outperform the existing alignment algorithm by large margins, especially when the amounts of demonstration and preference data are unbalanced. | Chenliang Li, Siliang Zeng, Zeyi Liao, Jiaxiang Li, Dongyeop Kang, Alfredo García, Mingyi Hong |  |
| 231 |  |  [Counterfactual Realizability](https://openreview.net/forum?id=uuriavczkL) |  | 0 | It is commonly believed that, in a real-world environment, samples can only be drawn from observational and interventional distributions, corresponding to Layers 1 and 2 of the \*Pearl Causal Hierarchy\*. Layer 3, representing counterfactual distributions, is believed to be inaccessible by definition. However, Bareinboim, Forney, and Pearl (2015) introduced a procedure that allows an agent to sample directly from a counterfactual distribution, leaving open the question of what other counterfactual quantities can be estimated directly via physical experimentation. We resolve this by introducing a formal definition of realizability, the ability to draw samples from a distribution, and then developing a complete algorithm to determine whether an arbitrary counterfactual distribution is realizable given fundamental physical constraints, such as the inability to go back in time and subject the same unit to a different experimental condition. We illustrate the implications of this new framework for counterfactual data collection using motivating examples from causal fairness and causal reinforcement learning. While the baseline approach in these motivating settings typically follows an interventional or observational strategy, we show that a counterfactual strategy provably dominates both. | Arvind Raghavan, Elias Bareinboim |  |
| 232 |  |  [Robustness Reprogramming for Representation Learning](https://openreview.net/forum?id=SuH5SdOXpe) |  | 0 | This work tackles an intriguing and fundamental open challenge in representation learning: Given a well-trained deep learning model, can it be reprogrammed to enhance its robustness against adversarial or noisy input perturbations without altering its parameters? To explore this, we revisit the core feature transformation mechanism in representation learning and propose a novel non-linear robust pattern matching technique as a robust alternative. Furthermore, we introduce three model reprogramming paradigms to offer flexible control of robustness under different efficiency requirements. Comprehensive experiments and ablation studies across diverse learning models ranging from basic linear model and MLPs to shallow and modern deep ConvNets demonstrate the effectiveness of our approaches. This work not only opens a promising and orthogonal direction for improving adversarial defenses in deep learning beyond existing methods but also provides new insights into designing more resilient AI systems with robust statistics. Our implementation is available at https://github.com/chris-hzc/Robustness-Reprogramming. | Zhichao Hou, MohamadAli Torkamani, Hamid Krim, Xiaorui Liu |  |
| 233 |  |  [LayerDAG: A Layerwise Autoregressive Diffusion Model for Directed Acyclic Graph Generation](https://openreview.net/forum?id=kam84eEmub) |  | 0 | Directed acyclic graphs (DAGs) serve as crucial data representations in domains such as hardware synthesis and compiler/program optimization for computing systems. DAG generative models facilitate the creation of synthetic DAGs, which can be used for benchmarking computing systems while preserving intellectual property. However, generating realistic DAGs is challenging due to their inherent directional and logical dependencies. This paper introduces LayerDAG, an autoregressive diffusion model, to address these challenges. LayerDAG decouples the strong node dependencies into manageable units that can be processed sequentially. By interpreting the partial order of nodes as a sequence of bipartite graphs, LayerDAG leverages autoregressive generation to model directional dependencies and employs diffusion models to capture logical dependencies within each bipartite graph. Comparative analyses demonstrate that LayerDAG outperforms existing DAG generative models in both expressiveness and generalization, particularly for generating large-scale DAGs with up to 400 nodes—a critical scenario for system benchmarking. Extensive experiments on both synthetic and real-world flow graphs from various computing platforms show that LayerDAG generates valid DAGs with superior statistical properties and benchmarking performance. The synthetic DAGs generated by LayerDAG enhance the training of ML-based surrogate models, resulting in improved accuracy in predicting performance metrics of real-world DAGs across diverse computing platforms. | Mufei Li, Viraj Shitole, Eli Chien, Changhai Man, Zhaodong Wang, Srinivas, Ying Zhang, Tushar Krishna, Pan Li |  |
| 234 |  |  [DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life](https://openreview.net/forum?id=PGhiPGBf47) |  | 0 | As users increasingly seek guidance from LLMs for decision-making in daily life, many of these decisions are not clear-cut and depend significantly on the personal values and ethical standards of people. We present DailyDilemmas, a dataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma presents two possible actions, along with affected parties and relevant human values for each action. Based on these dilemmas, we gather a repository of human values covering diverse everyday topics, such as interpersonal relationships, workplace, and environmental issues. With DailyDilemmas, we evaluate LLMs on these dilemmas to determine what action they will choose and the values represented by these action choices. Then, we analyze values through the lens of five theoretical frameworks inspired by sociology, psychology, and philosophy, including the World Values Survey, Moral Foundations Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and Plutchik's Wheel of Emotions. For instance, we find LLMs are most aligned with self-expression over survival in World Values Survey and care over loyalty in Moral Foundations Theory. Interestingly, we find substantial preference differences in models for some core values. For example, for truthfulness, Mixtral-8x7B neglects it by 9.7% while GPT-4-turbo selects it by 9.4%. We also study the recent guidance released by OpenAI (ModelSpec), and Anthropic (Constitutional AI) to understand how their designated principles reflect their models' actual value prioritization when facing nuanced moral reasoning in daily-life settings. Finally, we find that end users cannot effectively steer such prioritization using system prompts. | Yu Ying Chiu, Liwei Jiang, Yejin Choi |  |
| 235 |  |  [Learning-Augmented Frequent Directions](https://openreview.net/forum?id=WcZLG8XxhD) |  | 0 | An influential paper of Hsu et al. (ICLR'19) introduced the study of learning-augmented streaming algorithms in the context of frequency estimation. A fundamental problem in the streaming literature, the goal of frequency estimation is to approximate the number of occurrences of items appearing in a long stream of data using only a small amount of memory. Hsu et al. develop a natural framework to combine the worst-case guarantees of popular solutions such as CountMin and CountSketch with learned predictions of high frequency elements. They demonstrate that learning the underlying structure of data can be used to yield better streaming algorithms, both in theory and practice. We simplify and generalize past work on learning-augmented frequency estimation. Our first contribution is a learning-augmented variant of the Misra-Gries algorithm which improves upon the error of learned CountMin and learned CountSketch and achieves the state-of-the-art performance of randomized algorithms (Aamand et al., NeurIPS'23) with a simpler, deterministic algorithm. Our second contribution is to adapt learning-augmentation to a high-dimensional generalization of frequency estimation corresponding to finding important directions (top singular vectors) of a matrix given its rows one-by-one in a stream. We analyze a learning-augmented variant of the Frequent Directions algorithm, extending the theoretical and empirical understanding of learned predictions to matrix streaming. | Anders Aamand, Justin Y. Chen, Siddharth Gollapudi, Sandeep Silwal, Hao Wu |  |
| 236 |  |  [No Need to Talk: Asynchronous Mixture of Language Models](https://openreview.net/forum?id=pHOH8FVrTp) |  | 0 | We introduce SMALLTALK LM, an innovative method for training a mixture of language models in an almost asynchronous manner. Each model of the mixture specializes in distinct parts of the data distribution, without the need of high-bandwidth communication between the nodes training each model. At inference, a lightweight router directs a given sequence to a single expert, according to a short prefix. This inference scheme naturally uses a fraction of the parameters from the overall mixture model. Unlike prior works on asynchronous LLM training, our routing method does not rely on full corpus clustering or access to metadata, making it more suitable for real-world applications. Our experiments on language modeling demonstrate that SMALLTALK LM achieves significantly lower perplexity than dense model baselines for the same total training FLOPs and an almost identical inference cost. Finally, in our downstream evaluations we outperform the dense baseline on 75% of the tasks. | Anastasiia Filippova, Angelos Katharopoulos, David Grangier, Ronan Collobert |  |
| 237 |  |  [Estimating the Probabilities of Rare Outputs in Language Models](https://openreview.net/forum?id=DC8bsa9bzY) |  | 0 | We consider the problem of \*low probability estimation\*: given a machine learning model and a formally-specified input distribution, how can we estimate the probability of a binary property of the model's output, even when that probability is too small to estimate by random sampling? This problem is motivated by the need to improve worst-case performance, which distribution shift can make much more likely. We study low probability estimation in the context of argmax sampling from small transformer language models. We compare two types of methods: importance sampling, which involves searching for inputs giving rise to the rare output, and activation extrapolation, which involves extrapolating a probability distribution fit to the model's logits. We find that importance sampling outperforms activation extrapolation, but both outperform naive sampling. Finally, we explain how minimizing the probability estimate of an undesirable behavior generalizes adversarial training, and argue that new methods for low probability estimation are needed to provide stronger guarantees about worst-case performance. | Gabriel Wu, Jacob Hilton |  |
| 238 |  |  [Quality Measures for Dynamic Graph Generative Models](https://openreview.net/forum?id=8bjspmAMBk) |  | 0 | Deep generative models have recently achieved significant success in modeling graph data, including dynamic graphs, where topology and features evolve over time. However, unlike in vision and natural language domains, evaluating generative models for dynamic graphs is challenging due to the difficulty of visualizing their output, making quantitative metrics essential. In this work, we develop a new quality metric for evaluating generative models of dynamic graphs. Current metrics for dynamic graphs typically involve discretizing the continuous-evolution of graphs into static snapshots and then applying conventional graph similarity measures. This approach has several limitations: (a) it models temporally related events as i.i.d. samples, failing to capture the non-uniform evolution of dynamic graphs; (b) it lacks a unified measure that is sensitive to both features and topology; (c) it fails to provide a scalar metric, requiring multiple metrics without clear superiority; and (d) it requires explicitly instantiating each static snapshot, leading to impractical runtime demands that hinder evaluation at scale. We propose a novel metric based on the Johnson-Lindenstrauss lemma, applying random projections directly to dynamic graph data. This results in an expressive, scalar, and application-agnostic measure of dynamic graph similarity that overcomes the limitations of traditional methods. We also provide a comprehensive empirical evaluation of metrics for continuous-time dynamic graphs, demonstrating the effectiveness of our approach compared to existing methods. Our implementation is available at https://github.com/ryienh/jl-metric. | Ryien Hosseini, Filippo Simini, Venkatram Vishwanath, Rebecca Willett, Henry Hoffmann |  |
| 239 |  |  [Correlated Proxies: A New Definition and Improved Mitigation for Reward Hacking](https://openreview.net/forum?id=msEr27EejF) |  | 0 | Because it is difficult to precisely specify complex objectives, reinforcement learning policies are often optimized using proxy reward functions that only approximate the true goal. However, optimizing proxy rewards frequently leads to reward hacking: the optimized reward function ceases to be a good proxy and the resulting policy performs poorly with respect to the unspecified true reward. Principled solutions to reward hacking have been impeded by the lack of a good definition for the problem. To address this gap, we introduce a definition of reward hacking based on the correlation between proxy and true rewards for states and actions seen by a “reference policy” that breaks down under optimization. We show that this definition captures reward hacking behavior across several realistic settings, including in reinforcement learning from human feedback (RLHF). Using our formulation, we show theoretically that regularization to the reference policy can effectively prevent reward hacking. While the current practice in RLHF applies a KL penalty between action distributions for this purpose, our theory suggests regularizing the χ2 divergence between the policies’ occupancy measures can be more effective. We intuitively show the benefits of this type of regularization and demonstrate that it better mitigates reward hacking in practice across four realistic settings, including RLHF. Our code is available at https://github.com/cassidylaidlaw/orpo. | Cassidy Laidlaw, Shivam Singhal, Anca D. Dragan |  |
| 240 |  |  [Holistically Evaluating the Environmental Impact of Creating Language Models](https://openreview.net/forum?id=04qx93Viwj) |  | 0 | As the performance of artificial intelligence systems has dramatically increased, so too has the environmental impact of creating these systems. While many model developers release estimates of the power consumption and carbon emissions from the final training runs for their latest models, there is comparatively little transparency into the impact of model development, hardware manufacturing, and total water usage throughout. In this work, we estimate the real-world environmental impact of developing a series of language models, ranging from 20 million to 13 billion active parameters, trained on up to 5.6 trillion tokens each. When accounting for hardware manufacturing, model development, and our final training runs, we find that our series of models released \*\*493 metric tons\*\* of carbon emissions, equivalent to powering about 98 homes in the United States for one year, and consumed \*\*2.769 million liters of water\*\*, equivalent to about 24.5 years of water usage by a person in the United States, even though our data center is extremely water-efficient. We measure and report the environmental impact of our model development; to the best of our knowledge we are the first to do so for LLMs, and we find that model development, the impact of which is generally not disclosed by most model developers, amounted to \*\*~50%\*\* of that of training. By looking at detailed time series data for power consumption, we also find that power usage throughout training is not consistent, fluctuating between ~15% and ~85% of our hardware's maximum power draw, with negative implications for grid-scale planning as demand continues to grow. We close with a discussion on the continued difficulty of estimating the environmental impact of AI systems, and key takeaways for model developers and the public at large. | Jacob Morrison, Clara Na, Jared Fernandez, Tim Dettmers, Emma Strubell, Jesse Dodge |  |
| 241 |  |  [Mixture-of-Agents Enhances Large Language Model Capabilities](https://openreview.net/forum?id=h0ZfDIrj7T) |  | 0 | Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal, we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. In our approach, we construct a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieves state-of-art performance on AlpacaEval 2.0, Arena-Hard, MT-Bench, and FLASK, surpassing GPT-4 Omni. For example, our MoA using only open-source LLMs achieves a score of 65.1% on AlpacaEval 2.0 compared to 57.5% by GPT-4 Omni. | Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, James Zou |  |
| 242 |  |  [Answer, Assemble, Ace: Understanding How LMs Answer Multiple Choice Questions](https://openreview.net/forum?id=6NNA0MxhCH) |  | 0 | Multiple-choice question answering (MCQA) is a key competence of performant transformer language models that is tested by mainstream benchmarks. However, recent evidence shows that models can have quite a range of performance, particularly when the task format is diversified slightly (such as by shuffling answer choice order). In this work we ask: how do successful models perform formatted MCQA? We employ vocabulary projection and activation patching methods to localize key hidden states that encode relevant information for predicting the correct answer. We find that prediction of a specific answer symbol is causally attributed to a few middle layers, and specifically their multi-head self-attention mechanisms. We show that subsequent layers increase the probability of the predicted answer symbol in vocabulary space, and that this probability increase is associated with a sparse set of attention heads with unique roles. We additionally uncover differences in how different models adjust to alternative symbols. Finally, we demonstrate that a synthetic task can disentangle sources of model error to pinpoint when a model has learned formatted MCQA, and show that logit differences between answer choice tokens continue to grow over the course of training. | Sarah Wiegreffe, Oyvind Tafjord, Yonatan Belinkov, Hannaneh Hajishirzi, Ashish Sabharwal |  |
| 243 |  |  [Provable Uncertainty Decomposition via Higher-Order Calibration](https://openreview.net/forum?id=TId1SHe8JG) |  | 0 | We give a principled method for decomposing the predictive uncertainty of a model into aleatoric and epistemic components with explicit semantics relating them to the real-world data distribution. While many works in the literature have proposed such decompositions, they lack the type of formal guarantees we provide. Our method is based on the new notion of higher-order calibration, which generalizes ordinary calibration to the setting of higher-order predictors that predict _mixtures_ over label distributions at every point. We show how to measure as well as achieve higher-order calibration using access to $k$-snapshots, namely examples where each point has $k$ independent conditional labels. Under higher-order calibration, the estimated aleatoric uncertainty at a point is guaranteed to match the real-world aleatoric uncertainty averaged over all points where the prediction is made. To our knowledge, this is the first formal guarantee of this type that places no assumptions whatsoever on the real-world data distribution. Importantly, higher-order calibration is also applicable to existing higher-order predictors such as Bayesian and ensemble models and provides a natural evaluation metric for such models. We demonstrate through experiments that our method produces meaningful uncertainty decompositions in tasks such as image classification. | Gustaf Ahdritz, Aravind Gollakota, Parikshit Gopalan, Charlotte Peale, Udi Wieder |  |
| 244 |  |  [Sparse components distinguish visual pathways & their alignment to neural networks](https://openreview.net/forum?id=IqHeDe2lbl) |  | 0 | The ventral, dorsal, and lateral streams in high-level human visual cortex are implicated in distinct functional processes. Yet, deep neural networks (DNNs) trained on a single task model the entire visual system surprisingly well, hinting at common computational principles across these pathways. To explore this inconsistency, we applied a novel sparse decomposition approach to identify the dominant components of visual representations within each stream. Consistent with traditional neuroscience research, we find a clear difference in component response profiles across the three visual streams—identifying components selective for faces, places, bodies, text, and food in the ventral stream; social interactions, implied motion, and hand actions in the lateral stream; and some less interpretable components in the dorsal stream. Building on this, we introduce Sparse Component Alignment (SCA), a new method for measuring representational alignment between brains and machines that better captures the latent neural tuning of these two visual systems. We find that standard visual DNNs are more aligned with ventral than either dorsal or lateral representations. SCA reveals these distinctions with greater resolution than conventional population-level geometry, offering a measure of representational alignment that is sensitive to a system’s underlying axes of neural tuning. | Ammar I Marvi, Nancy Kanwisher, Meenakshi Khosla |  |
| 245 |  |  [Reducing Hallucinations in Large Vision-Language Models via Latent Space Steering](https://openreview.net/forum?id=LBl7Hez0fF) |  | 0 | Hallucination poses a challenge to the deployment of large vision-language models (LVLMs) in applications. Unlike in large language models (LLMs), hallucination in LVLMs often arises from misalignments between visual inputs and textual outputs. This paper investigates the underlying mechanisms of hallucination, focusing on the unique structure of LVLMs that distinguishes them from LLMs. We identify that hallucinations often arise from the sensitivity of text decoders to vision inputs, a natural phenomenon when image encoders and text decoders are pre-trained separately. Inspired by this, we introduce Visual and Textual Intervention (VTI), a novel technique designed to reduce hallucinations by steering latent space representations during inference to enhance the stability of vision features. As a task-agnostic test-time intervention, VTI can be easily applied to any problem without additional training costs. Extensive experiments demonstrate that it can effectively reduce hallucinations and outperform baseline methods across multiple metrics, highlighting the critical role of vision feature stability in LVLMs. | Sheng Liu, Haotian Ye, James Zou |  |
| 246 |  |  [Mitigating Memorization in Language Models](https://openreview.net/forum?id=MGKDBuyv4p) |  | 0 | Language models (LMs) can “memorize” information, i.e., encode training data in their weights in such a way that inference-time queries can lead to verbatim regurgitation of that data. This ability to extract training data can be problematic, for example, when data are private or sensitive. In this work, we investigate methods to mitigate memorization: three regularizer-based, three fine-tuning-based, and eleven machine unlearning-based methods, with five of the latter being new methods that we introduce. We also introduce TinyMem, a suite of small, computationally-efficient LMs for the rapid development and evaluation of memorization-mitigation methods. We demonstrate that the mitigation methods that we develop using TinyMem can successfully be applied to production-grade LMs, and we determine via experiment that: regularizer-based mitigation methods are slow and ineffective at curbing memorization; fine-tuning-based methods are effective at curbing memorization, but overly expensive, especially for retaining higher accuracies; and unlearning-based methods are faster and more effective, allowing for the precise localization and removal of memorized information from LM weights prior to inference. We show, in particular, that our proposed unlearning method BalancedSubnet outperforms other mitigation methods at removing memorized information while preserving performance on target tasks. | Mansi Sakarvadia, Aswathy Ajith, Arham Mushtaq Khan, Nathaniel C. Hudson, Caleb Geniesse, Kyle Chard, Yaoqing Yang, Ian T. Foster, Michael W. Mahoney |  |
| 247 |  |  [Effective post-training embedding compression via temperature control in contrastive training](https://openreview.net/forum?id=szRmEM8Kx5) |  | 0 | Fixed-size learned representations (dense representations, or embeddings) are widely used in many machine learning applications across language, vision or speech modalities. This paper investigates the role of the temperature parameter in contrastive training for text embeddings. We shed light on the impact this parameter has on the intrinsic dimensionality of the embedding spaces obtained, and show that lower intrinsic dimensionality is further correlated with effective compression of embeddings. We still observe a trade-off between absolute performance and effective compression and we propose temperature aggregation methods which reduce embedding size by an order of magnitude with minimal impact on quality. | Georgiana Dinu, Corey D. Barrett, Yi Xiang, Miguel Romero Calvo, Anna Currey, Xing Niu |  |
| 248 |  |  [TopoNets: High performing vision and language models with brain-like topography](https://openreview.net/forum?id=THqWPzL00e) |  | 0 | Neurons in the brain are organized such that nearby cells tend to share similar functions. AI models lack this organization, and past efforts to introduce topography have often led to trade-offs between topography and task performance. In this work, we present \*TopoLoss\*, a new loss function that promotes spatially organized topographic representations in AI models without significantly sacrificing task performance. TopoLoss is highly adaptable and can be seamlessly integrated into the training of leading model architectures. We validate our method on both vision (ResNet-18, ResNet-50, ViT) and language models (GPT-Neo-125M, NanoGPT), collectively \*TopoNets\*. TopoNets are the highest performing supervised topographic models to date, exhibiting brain-like properties such as localized feature processing, lower dimensionality, and increased efficiency. TopoNets also predict responses in the brain and replicate the key topographic signatures observed in the brain’s visual and language cortices, further bridging the gap between biological and artificial systems. This work establishes a robust and generalizable framework for integrating topography into AI, advancing the development of high performing models that more closely emulate the computational strategies of the human brain. Our project page: https://toponets.github.io | Mayukh Deb, Mainak Deb, N. Apurva Ratan Murty |  |
| 249 |  |  [INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge](https://openreview.net/forum?id=k3gCieTXeY) |  | 0 | The performance differential of large language models (LLM) between languages hinders their effective deployment in many regions, inhibiting the potential economic and societal value of generative AI tools in many communities. However, the development of functional LLMs in many languages (i.e., multilingual LLMs) is bottlenecked by the lack of high-quality evaluation resources in languages other than English. Moreover, current practices in multilingual benchmark construction often translate English resources, ignoring the regional and cultural knowledge of the environments in which multilingual systems would be used. In this work, we construct an evaluation suite of 197,243 QA pairs from local exam sources to measure the capabilities of multilingual LLMs in a variety of regional contexts. Our novel resource, INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across 44 written languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed. | Angelika Romanou, Negar Foroutan, Anna Sotnikova, Zeming Chen, Sree Harsha Nelaturu, Shivalika Singh, Rishabh Maheshwary, Micol Altomare, Mohamed A. Haggag, Imanol Schlag, Marzieh Fadaee, Sara Hooker, Antoine Bosselut, Snegha A, Alfonso Amayuelas, Azril Hafizi Amirudin, Viraat Aryabumi, Danylo Boiko, Michael Chang, Jenny Chim, Gal Cohen, Aditya Kumar Dalmia, Abraham Diress, Sharad Duwal, Daniil Dzenhaliou, Daniel Fernando Erazo Florez, Fabian Farestam, Joseph Marvin Imperial, Shayekh Bin Islam, Perttu Isotalo, Maral Jabbarishiviari, Börje F. Karlsson, Eldar Khalilov, Christopher Klamm, Fajri Koto, Dominik Krzeminski, Gabriel Adriano de Melo, Syrielle Montariol, Yiyang Nan, Joel Niklaus, Jekaterina Novikova, Johan Samir ObandoCeron, Debjit Paul, Esther Ploeger, Jebish Purbey, Swati Rajwal, Selvan Sunitha Ravi, Sara Rydell, Roshan Santhosh, Drishti Sharma, Marjana Prifti Skenduli, Arshia Soltani Moakhar, Bardia Soltani Moakhar, Ran Tamir, Ayush Kumar Tarun, Azmine Toushik Wasi, Thenuka Ovin Weerasinghe, Serhan Yilmaz, Mike Zhang |  |
| 250 |  |  [Deep Learning Alternatives Of The Kolmogorov Superposition Theorem](https://openreview.net/forum?id=SyVPiehSbg) |  | 0 | This paper explores alternative formulations of the Kolmogorov Superposition Theorem (KST) as a foundation for neural network design. The original KST formulation, while mathematically elegant, presents practical challenges due to its limited insight into the structure of inner and outer functions and the large number of unknown variables it introduces. Kolmogorov-Arnold Networks (KANs) leverage KST for function approximation, but they have faced scrutiny due to mixed results compared to traditional multilayer perceptrons (MLPs) and practical limitations imposed by the original KST formulation. To address these issues, we introduce ActNet, a scalable deep learning model that builds on the KST and overcomes some of the drawbacks of Kolmogorov's original formulation. We evaluate ActNet in the context of Physics-Informed Neural Networks (PINNs), a framework well-suited for leveraging KST's strengths in low-dimensional function approximation, particularly for simulating partial differential equations (PDEs). In this challenging setting, where models must learn latent functions without direct measurements, ActNet consistently outperforms KANs across multiple benchmarks and is competitive against the current best MLP-based approaches. These results present ActNet as a promising new direction for KST-based deep learning applications, particularly in scientific computing and PDE simulation tasks. | Leonardo Ferreira Guilhoto, Paris Perdikaris |  |
| 251 |  |  [Vision Language Models are In-Context Value Learners](https://openreview.net/forum?id=friHAl5ofG) |  | 0 | Predicting temporal progress from visual trajectories is important for intelligent robots that can learn, adapt, and improve. However, learning such progress estimator, or temporal value function, across different tasks and domains requires both a large amount of diverse data and methods which can scale and generalize. To address these challenges, we present Generative Value Learning (GVL), a universal value function estimator that leverages the world knowledge embedded in vision-language models (VLMs) to predict task progress. Naively asking a VLM to predict values for a video sequence performs poorly due to the strong temporal correlation between successive frames. Instead, GVL poses value estimation as a temporal ordering problem over shuffled video frames; this seemingly more challenging task encourages VLMs to more fully exploit their underlying semantic and temporal grounding capabilities to differentiate frames based on their perceived task progress, consequently producing significantly better value predictions. Without any robot or task specific training, GVL can in-context zero-shot and few-shot predict effective values for more than 300 distinct real-world tasks across diverse robot platforms, including challenging bimanual manipulation tasks. Furthermore, we demonstrate that GVL permits flexible multi-modal in-context learning via examples from heterogeneous tasks and embodiments, such as human videos. The generality of GVL enables various downstream applications pertinent to visuomotor policy learning, including dataset filtering, success detection, and value-weighted regression -- all without any model training or finetuning. | Yecheng Jason Ma, Joey Hejna, Chuyuan Fu, Dhruv Shah, Jacky Liang, Zhuo Xu, Sean Kirmani, Peng Xu, Danny Driess, Ted Xiao, Osbert Bastani, Dinesh Jayaraman, Wenhao Yu, Tingnan Zhang, Dorsa Sadigh, Fei Xia |  |
| 252 |  |  [Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding](https://openreview.net/forum?id=Tv36j85SqR) |  | 0 | Neural compression has brought tremendous progress in designing lossy compressors with good rate-distortion (RD) performance at low complexity. Thus far, neural compression design involves transforming the source to a latent vector, which is then rounded to integers and entropy coded. While this approach has been shown to be optimal on a few specific sources, we show that it can be highly sub-optimal on synthetic sources whose intrinsic dimensionality is greater than one. With integer rounding in the latent space, the quantization regions induced by neural transformations, remain square-like and fail to match those of optimal vector quantization. We demonstrate that this phenomenon is due to the choice of scalar quantization in the latent space, and not the transform design. By employing lattice quantization instead, we propose Lattice Transform Coding (LTC) and show that it approximately recovers optimal vector quantization at reasonable complexity. On real-world sources, LTC improves upon standard neural compressors. LTC also provides a framework that can integrate structurally (near) optimal information-theoretic designs into lossy compression; examples include block coding, which yields coding gain over optimal one-shot coding and approaches the asymptotically-achievable rate-distortion function, as well as nested lattice quantization for low complexity fixed-rate coding. | Eric Lei, Hamed Hassani, Shirin Saeedi Bidokhti |  |
| 253 |  |  [Wasserstein Distances, Neuronal Entanglement, and Sparsity](https://openreview.net/forum?id=cnKhHxN3xj) |  | 0 | Disentangling polysemantic neurons is at the core of many current approaches to interpretability of large language models. Here we attempt to study how disentanglement can be used to understand performance, particularly under weight sparsity, a leading post-training optimization technique. We suggest a novel measure for estimating neuronal entanglement: the Wasserstein distance of a neuron's output distribution to a Gaussian. Moreover, we show the existence of a small number of highly entangled "Wasserstein Neurons" in each linear layer of an LLM, characterized by their highly non-Gaussian output distributions, their role in mapping similar inputs to dissimilar outputs, and their significant impact on model accuracy. To study these phenomena, we propose a new experimental framework for disentangling polysemantic neurons. Our framework separates each layer's inputs to create a mixture of experts where each neuron's output is computed by a mixture of neurons of lower Wasserstein distance, each better at maintaining accuracy when sparsified without retraining. We provide strong evidence that this is because the mixture of sparse experts is effectively disentangling the input-output relationship of individual neurons, in particular the difficult Wasserstein neurons. | Shashata Sawmya, Linghao Kong, Ilia Markov, Dan Alistarh, Nir Shavit |  |
| 254 |  |  [Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations](https://openreview.net/forum?id=4ub9gpx9xw) |  | 0 | Large language models (LLMs) are capable of generating \*plausible\* explanations of how they arrived at an answer to a question. However, these explanations can misrepresent the model's "reasoning" process, i.e., they can be \*unfaithful\*. This, in turn, can lead to over-trust and misuse. We introduce a new approach for measuring the faithfulness of LLM explanations. First, we provide a rigorous definition of faithfulness. Since LLM explanations mimic human explanations, they often reference high-level \*concepts\* in the input question that purportedly influenced the model. We define faithfulness in terms of the difference between the set of concepts that the LLM's \*explanations imply\* are influential and the set that \*truly\* are. Second, we present a novel method for estimating faithfulness that is based on: (1) using an auxiliary LLM to modify the values of concepts within model inputs to create realistic counterfactuals, and (2) using a hierarchical Bayesian model to quantify the causal effects of concepts at both the example- and dataset-level. Our experiments show that our method can be used to quantify and discover interpretable patterns of unfaithfulness. On a social bias task, we uncover cases where LLM explanations hide the influence of social bias. On a medical question answering task, we uncover cases where LLM explanations provide misleading claims about which pieces of evidence influenced the model's decisions. | Katie Matton, Robert Osazuwa Ness, John V. Guttag, Emre Kiciman |  |
| 255 |  |  [Implicit Bias of Mirror Flow for Shallow Neural Networks in Univariate Regression](https://openreview.net/forum?id=IF0Q9KY3p2) |  | 0 | We examine the implicit bias of mirror flow in least squares error regression with wide and shallow neural networks. For a broad class of potential functions, we show that mirror flow exhibits lazy training and has the same implicit bias as ordinary gradient flow when the network width tends to infinity. For univariate ReLU networks, we characterize this bias through a variational problem in function space. Our analysis includes prior results for ordinary gradient flow as a special case and lifts limitations which required either an intractable adjustment of the training data or networks with skip connections. We further introduce \emph{scaled potentials} and show that for these, mirror flow still exhibits lazy training but is not in the kernel regime. For univariate networks with absolute value activations, we show that mirror flow with scaled potentials induces a rich class of biases, which generally cannot be captured by an RKHS norm. A takeaway is that whereas the parameter initialization determines how strongly the curvature of the learned function is penalized at different locations of the input space, the scaled potential determines how the different magnitudes of the curvature are penalized. | Shuang Liang, Guido Montúfar |  |
| 256 |  |  [WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild](https://openreview.net/forum?id=MKEHCx25xp) |  | 0 | We introduce WildBench, an automated evaluation framework designed to benchmark large language models (LLMs) using challenging, real-world user queries. WildBench consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs. For automated evaluation with WildBench, we have developed two metrics, WB-Reward and WB-Score, which are computable using advanced LLMs such as GPT-4-turbo. WildBench evaluation uses task-specific checklists to evaluate model outputs systematically and provides structured explanations that justify the scores and comparisons, resulting in more reliable and interpretable automatic judgments. WB-Reward employs fine-grained pairwise comparisons between model responses, generating five potential outcomes: much better, slightly better, slightly worse, much worse, or a tie. Unlike previous evaluations that employed a single baseline model, we selected three baseline models at varying performance levels to ensure a comprehensive pairwise evaluation. Additionally, we propose a simple method to mitigate length bias, by converting outcomes of “slightly better/worse” to “tie” if the winner response exceeds the loser one by more than K characters. WB-Score evaluates the quality of model outputs individually, making it a fast and cost-efficient evaluation metric. WildBench results demonstrate a strong correlation with the human-voted Elo ratings from Chatbot Arena on hard tasks. Specifically, WB-Reward achieves a Pearson correlation of 0.98 with top-ranking models. Additionally, WB-Score reaches 0.95, surpassing both ArenaHard’s 0.91 and AlpacaEval2.0’s 0.89 for length-controlled win rates, as well as the 0.87 for regular win rates. | Bill Yuchen Lin, Yuntian Deng, Khyathi Raghavi Chandu, Abhilasha Ravichander, Valentina Pyatkin, Nouha Dziri, Ronan Le Bras, Yejin Choi |  |
| 257 |  |  [Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators and Directionally Aligned Perturbations](https://openreview.net/forum?id=ywFOSIT9ik) |  | 0 | In this paper, we explore the two-point zeroth-order gradient estimator and identify the distribution of random perturbations that minimizes the estimator's asymptotic variance as the perturbation stepsize tends to zero. We formulate it as a constrained functional optimization problem over the space of perturbation distributions. Our findings reveal that such desired perturbations can align directionally with the true gradient, instead of maintaining a fixed length. While existing research has largely focused on fixed-length perturbations, the potential advantages of directional alignment have been overlooked. To address this gap, we delve into the theoretical and empirical properties of the directionally aligned perturbation (DAP) scheme, which adaptively offers higher accuracy along critical directions. Additionally, we provide a convergence analysis for stochastic gradient descent using $\delta$-unbiased random perturbations, extending existing complexity bounds to a wider range of perturbations. Through empirical evaluations on both synthetic problems and practical tasks, we demonstrate that DAPs outperform traditional methods under specific conditions. | Shaocong Ma, Heng Huang |  |
| 258 |  |  [Adaptive Batch Size for Privately Finding Second-Order Stationary Points](https://openreview.net/forum?id=ikkvC1UnnE) |  | 0 | There is a gap between finding a first-order stationary point (FOSP) and a second-order stationary point (SOSP) under differential privacy constraints, and it remains unclear whether privately finding an SOSP is more challenging than finding an FOSP. Specifically, Ganesh et al. (2023) claimed that an $\alpha$-SOSP can be found with $\alpha=\Tilde{O}(\frac{1}{n^{1/3}}+(\frac{\sqrt{d}}{n\epsilon})^{3/7})$, where $n$ is the dataset size, $d$ is the dimension, and $\epsilon$ is the differential privacy parameter. However, a recent analysis revealed an issue in their saddle point escape procedure, leading to weaker guarantees. Building on the SpiderBoost algorithm framework, we propose a new approach that uses adaptive batch sizes and incorporates the binary tree mechanism. Our method not only corrects this issue but also improves the results for privately finding an SOSP, achieving $\alpha=\Tilde{O}(\frac{1}{n^{1/3}}+(\frac{\sqrt{d}}{n\epsilon})^{1/2})$. This improved bound matches the state-of-the-art for finding a FOSP, suggesting that privately finding an SOSP may be achievable at no additional cost. | Daogao Liu, Kunal Talwar |  |
| 259 |  |  [Adversarial Perturbations Cannot Reliably Protect Artists From Generative AI](https://openreview.net/forum?id=yfW1x7uBS5) |  | 0 | Artists are increasingly concerned about advancements in image generation models that can closely replicate their unique artistic styles. In response, several protection tools against style mimicry have been developed that incorporate small adversarial perturbations into artworks published online. In this work, we evaluate the effectiveness of popular protections---with millions of downloads---and show they only provide a false sense of security. We find that low-effort and "off-the-shelf" techniques, such as image upscaling, are sufficient to create robust mimicry methods that significantly degrade existing protections. Through a user study, we demonstrate that \*\*all existing protections can be easily bypassed\*\*, leaving artists vulnerable to style mimicry. We caution that tools based on adversarial perturbations cannot reliably protect artists from the misuse of generative AI, and urge the development of alternative protective solutions. | Robert Hönig, Javier Rando, Nicholas Carlini, Florian Tramèr |  |
| 260 |  |  [Better autoregressive regression with LLMs via regression-aware fine-tuning](https://openreview.net/forum?id=xGs7Ch3Vyo) |  | 0 | Decoder-based large language models (LLMs) have proven highly versatile, with remarkable successes even on problems ostensibly removed from traditional language generation. One such example is solving regression problems, where the targets are real numbers rather than textual tokens. A common approach to use LLMs on such problems is to perform fine-tuning based on the cross-entropy loss, and use autoregressive sampling at inference time. Another approach relies on fine-tuning a separate predictive head with a suitable loss such as squared error. While each approach has had success, there has been limited study on principled ways of using decoder LLMs for regression. In this work, we compare different prior works under a unified view, and introduce regression-aware fine-tuning(RAFT), a novel approach based on the Bayes-optimal decision rule. We demonstrate how RAFT improves over established baselines on several benchmarks and model families. | Michal Lukasik, Zhao Meng, Harikrishna Narasimhan, YinWen Chang, Aditya Krishna Menon, Felix Yu, Sanjiv Kumar |  |
| 261 |  |  [Analyzing Neural Scaling Laws in Two-Layer Networks with Power-Law Data Spectra](https://openreview.net/forum?id=wFD16gwpze) |  | 0 | Neural scaling laws describe how the performance of deep neural networks scales with key factors such as training data size, model complexity, and training time, often following power-law behaviors over multiple orders of magnitude. Despite their empirical observation, the theoretical understanding of these scaling laws remains limited. In this work, we employ techniques from statistical mechanics to analyze one-pass stochastic gradient descent within a student-teacher framework, where both the student and teacher are two-layer neural networks. Our study primarily focuses on the generalization error and its behavior in response to data covariance matrices that exhibit power-law spectra. For linear activation functions, we derive analytical expressions for the generalization error, exploring different learning regimes and identifying conditions under which power-law scaling emerges. Additionally, we extend our analysis to non-linear activation functions in the feature learning regime, investigating how power-law spectra in the data covariance matrix impact learning dynamics. Importantly, we find that the length of the symmetric plateau depends on the number of distinct eigenvalues of the data covariance matrix and the number of hidden units, demonstrating how these plateaus behave under various configurations. In addition, our results reveal a transition from exponential to power-law convergence in the specialized phase when the data covariance matrix possesses a power-law spectrum. This work contributes to the theoretical understanding of neural scaling laws and provides insights into optimizing learning performance in practical scenarios involving complex data structures. | Roman Worschech, Bernd Rosenow |  |
| 262 |  |  [Differential learning kinetics govern the transition from memorization to generalization during in-context learning](https://openreview.net/forum?id=INyi7qUdjZ) |  | 0 | Transformers exhibit in-context learning (ICL): the ability to use novel information presented in the context without additional weight updates. Recent work shows that ICL emerges when models are trained on a sufficiently diverse set of tasks and the transition from memorization to generalization is sharp with increasing task diversity. One interpretation is that a network's limited capacity to memorize favors generalization. Here, we examine the mechanistic underpinnings of this transition using a small transformer applied to a synthetic ICL task. Using theory and experiment, we show that the sub-circuits that memorize and generalize can be viewed as largely independent. The relative \*rates\* at which these sub-circuits learn explains the transition from memorization to generalization, rather than capacity constraints. We uncover a memorization scaling law, which determines the task diversity threshold at which the network generalizes. The theory quantitatively explains a variety of other ICL-related phenomena, including the long-tailed distribution of when ICL is acquired, the bimodal behavior of solutions close to the task diversity threshold, the influence of contextual and data distributional statistics on ICL, and the transient nature of ICL. | Alex Nguyen, Gautam Reddy |  |
| 263 |  |  [Multi-session, multi-task neural decoding from distinct cell-types and brain regions](https://openreview.net/forum?id=IuU0wcO0mo) |  | 0 | Recent work has shown that scale is important for improved brain decoding, with more data leading to greater decoding accuracy. However, large-scale decoding across many different datasets is challenging because neural circuits are heterogeneous---each brain region contains a unique mix of cellular sub-types, and the responses to different stimuli are diverse across regions and sub-types. It is unknown whether it is possible to pre-train and transfer brain decoding models between distinct tasks, cellular sub-types, and brain regions. To address these questions, we developed a multi-task transformer architecture and trained it on the entirety of the Allen Institute's Brain Observatory dataset. This dataset contains responses from over 100,000 neurons in 6 areas of the brains of mice, observed with two-photon calcium imaging, recorded while the mice observed different types of visual stimuli. Our results demonstrate that transfer is indeed possible -combining data from different sources is beneficial for a number of downstream decoding tasks. As well, we can transfer the model between regions and sub-types, demonstrating that there is in fact common information in diverse circuits that can be extracted by an appropriately designed model. Interestingly, we found that the model's latent representations showed clear distinctions between different brain regions and cellular sub-types, even though it was never given any information about these distinctions. Altogether, our work demonstrates that training a large-scale neural decoding model on diverse data is possible, and this provides a means of studying the differences and similarities between heterogeneous neural circuits. | Mehdi Azabou, Krystal Xuejing Pan, Vinam Arora, Ian Jarratt Knight, Eva L. Dyer, Blake Aaron Richards |  |
| 264 |  |  [Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models](https://openreview.net/forum?id=vQhn4wrQ6j) |  | 0 | Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. In this work, we present a model merging methodology that addresses the difficulty of fine-tuning Large Language Models (LLMs) for target tasks in non-English languages, where task-specific data is often unavailable. We focus on mathematical reasoning and without in-language math data, facilitate cross-lingual transfer by composing language and math capabilities. Starting from the same pretrained model, we fine-tune separate "experts" on math instruction data in English and on generic instruction data in the target language. We then replace the top and bottom transformer layers of the math expert directly with layers from the language expert, which consequently enhances math performance in the target language. The resulting merged models outperform the individual experts and other merging methods on the math benchmark, MGSM, by 10% across four major languages where math instruction data is scarce. In addition, this layer swapping is simple, inexpensive, and intuitive, as it is based on an interpretative analysis of the most important parameter changes during the fine-tuning of each expert. The ability to successfully re-compose LLMs for cross-lingual transfer in this manner opens up future possibilities to combine model expertise, create modular solutions, and transfer reasoning capabilities across languages all post hoc. | Lucas Bandarkar, Benjamin Muller, Pritish Yuvraj, Rui Hou, Nayan Singhal, Hongjiang Lv, Bing Liu |  |
| 265 |  |  [Bayesian Optimization via Continual Variational Last Layer Training](https://openreview.net/forum?id=1jcnvghayD) |  | 0 | Gaussian Processes (GPs) are widely seen as the state-of-the-art surrogate models for Bayesian optimization (BO) due to their ability to model uncertainty and their performance on tasks where correlations are easily captured (such as those defined by Euclidean metrics) and their ability to be efficiently updated online. However, the performance of GPs depends on the choice of kernel, and kernel selection for complex correlation structures is often difficult or must be made bespoke. While Bayesian neural networks (BNNs) are a promising direction for higher capacity surrogate models, they have so far seen limited use due to poor performance on some problem types. In this paper, we propose an approach which shows competitive performance on many problem types, including some that BNNs typically struggle with. We build on variational Bayesian last layers (VBLLs), and connect training of these models to exact conditioning in GPs. We exploit this connection to develop an efficient online training algorithm that interleaves conditioning and optimization. Our findings suggest that VBLL networks significantly outperform GPs and other BNN architectures on tasks with complex input correlations, and match the performance of well-tuned GPs on established benchmark tasks. | Paul Brunzema, Mikkel Jordahn, John Willes, Sebastian Trimpe, Jasper Snoek, James Harrison |  |
| 266 |  |  [Bayesian Optimization of Antibodies Informed by a Generative Model of Evolving Sequences](https://openreview.net/forum?id=E48QvQppIN) |  | 0 | To build effective therapeutics, biologists iteratively mutate antibody sequences to improve binding and stability. Proposed mutations can be informed by previous measurements or by learning from large antibody databases to predict only typical antibodies. Unfortunately, the space of typical antibodies is enormous to search, and experiments often fail to find suitable antibodies on a budget. We introduce Clone-informed Bayesian Optimization (CloneBO), a Bayesian optimization procedure that efficiently optimizes antibodies in the lab by teaching a generative model how our immune system optimizes antibodies. Our immune system makes antibodies by iteratively evolving specific portions of their sequences to bind their target strongly and stably, resulting in a set of related, evolving sequences known as a \*clonal family\*. We train a large language model, CloneLM, on hundreds of thousands of clonal families and use it to design sequences with mutations that are most likely to optimize an antibody within the human immune system. We propose to guide our designs to fit previous measurements with a twisted sequential Monte Carlo procedure. We show that CloneBO optimizes antibodies substantially more efficiently than previous methods in realistic \*in silico\* experiments and designs stronger and more stable binders in \*in vitro\* wet lab experiments. | Alan Nawzad Amin, Nate Gruver, Yilun Kuang, Yucen Lily Li, Hunter Elliott, Calvin McCarter, Aniruddh Raghu, Peyton Greenside, Andrew Gordon Wilson |  |
| 267 |  |  [Meta-Dynamical State Space Models for Integrative Neural Data Analysis](https://openreview.net/forum?id=SRpq5OBpED) |  | 0 | Learning shared structure across environments facilitates rapid learning and adaptive behavior in neural systems. This has been widely demonstrated and applied in machine learning to train models that are capable of generalizing to novel settings. However, there has been limited work exploiting the shared structure in neural activity during similar tasks for learning latent dynamics from neural recordings. Existing approaches are designed to infer dynamics from a single dataset and cannot be readily adapted to account for statistical heterogeneities across recordings. In this work, we hypothesize that similar tasks admit a corresponding family of related solutions and propose a novel approach for meta-learning this solution space from task-related neural activity of trained animals. Specifically, we capture the variabilities across recordings on a low-dimensional manifold which concisely parametrizes this family of dynamics, thereby facilitating rapid learning of latent dynamics given new recordings. We demonstrate the efficacy of our approach on few-shot reconstruction and forecasting of synthetic dynamical systems, and neural recordings from the motor cortex during different arm reaching tasks. | Ayesha Vermani, Josue Nassar, Hyungju Jeon, Matthew Dowling, Il Memming Park |  |
| 268 |  |  [AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs](https://openreview.net/forum?id=bhK7U37VW8) |  | 0 | Jailbreak attacks serve as essential red-teaming tools, proactively assessing whether LLMs can behave responsibly and safely in adversarial environments. Despite diverse strategies (e.g., cipher, low-resource language, persuasions, and so on) that have been proposed and shown success, these strategies are still manually designed, limiting their scope and effectiveness as a red-teaming tool. In this paper, we propose AutoDAN-Turbo, a black-box jailbreak method that can automatically discover as many jailbreak strategies as possible from scratch, without any human intervention or predefined scopes (e.g., specified candidate strategies), and use them for red-teaming. As a result, AutoDAN-Turbo can significantly outperform baseline methods, achieving a 74.3% higher average attack success rate on public benchmarks. Notably, AutoDAN-Turbo achieves an 88.5 attack success rate on GPT-4-1106-turbo. In addition, AutoDAN-Turbo is a unified framework that can incorporate existing human-designed jailbreak strategies in a plug-and-play manner. By integrating human-designed strategies, AutoDAN-Turbo can even achieve a higher attack success rate of 93.4 on GPT-4-1106-turbo. | Xiaogeng Liu, Peiran Li, G. Edward Suh, Yevgeniy Vorobeychik, Zhuoqing Mao, Somesh Jha, Patrick McDaniel, Huan Sun, Bo Li, Chaowei Xiao |  |
| 269 |  |  [Towards Automated Knowledge Integration From Human-Interpretable Representations](https://openreview.net/forum?id=NTHMw8S1Ow) |  | 0 | A significant challenge in machine learning, particularly in noisy and low-data environments, lies in effectively incorporating inductive biases to enhance data efficiency and robustness. Despite the success of informed machine learning methods, designing algorithms with explicit inductive biases remains largely a manual process. In this work, we explore how prior knowledge represented in its native formats, e.g. in natural language, can be integrated into machine learning models in an automated manner. Inspired by the learning to learn principles of meta-learning, we consider the approach of learning to integrate knowledge via conditional meta-learning, a paradigm we refer to as informed meta-learning. We introduce and motivate theoretically the principles of informed meta-learning enabling automated and controllable inductive bias selection. To illustrate our claims, we implement an instantiation of informed meta-learning--the Informed Neural Process, and empirically demonstrate the potential benefits and limitations of informed meta-learning in improving data efficiency and generalisation. | Kasia Kobalczyk, Mihaela van der Schaar |  |
| 270 |  |  [BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval](https://openreview.net/forum?id=ykuc5q381b) |  | 0 | Existing retrieval benchmarks primarily consist of information-seeking queries (e.g., aggregated questions from search engines) where keyword or semantic-based retrieval is usually sufficient. However, many complex real-world queries require in-depth reasoning to identify relevant documents that go beyond surface form matching. For example, finding documentation for a coding question requires understanding the logic and syntax of the functions involved. To better benchmark retrieval on such challenging queries, we introduce BRIGHT, the first text retrieval benchmark that requires intensive reasoning to retrieve relevant documents. Our dataset consists of 1,398 real-world queries spanning diverse domains such as economics, psychology, mathematics, coding, and more. These queries are drawn from naturally occurring or carefully curated human data. Extensive evaluation reveals that even state-of-the-art retrieval models perform poorly on BRIGHT. The leading model on the MTEB leaderboard (Muennighoff et al., 2023), which achieves a score of 59.0 nDCG@10,1 produces a score of nDCG@10 of 18.0 on BRIGHT. We show that incorporating explicit reasoning about the query improves retrieval performance by up to 12.2 points. Moreover, incorporating retrieved documents from the top-performing retriever boosts question answering performance by over 6.6 points. We believe that BRIGHT paves the way for future research on retrieval systems in more realistic and challenging settings. | Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Hanyu Wang, Haisu Liu, Quan Shi, Zachary S. Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan Ö. Arik, Danqi Chen, Tao Yu |  |
| 271 |  |  [Reinforcement Learning for Control of Non-Markovian Cellular Population Dynamics](https://openreview.net/forum?id=dsHpulHpOK) |  | 0 | Many organisms and cell types, from bacteria to cancer cells, exhibit a remarkable ability to adapt to fluctuating environments. Additionally, cells can leverage memory of past environments to better survive previously-encountered stressors. From a control perspective, this adaptability poses significant challenges in driving cell populations toward extinction, and is thus an open question with great clinical significance. In this work, we focus on drug dosing in cell populations exhibiting phenotypic plasticity. For specific dynamical models switching between resistant and susceptible states, exact solutions are known. However, when the underlying system parameters are unknown, and for complex memory-based systems, obtaining the optimal solution is currently intractable. To address this challenge, we apply reinforcement learning (RL) to identify informed dosing strategies to control cell populations evolving under novel non-Markovian dynamics. We find that model-free deep RL is able to recover exact solutions and control cell populations even in the presence of long-range temporal dynamics. To further test our approach in more realistic settings, we demonstrate performant RL-based control strategies in environments with dynamic memory strength. | Josiah C. Kratz, Jacob Adamczyk |  |
| 272 |  |  [Online Reinforcement Learning in Non-Stationary Context-Driven Environments](https://openreview.net/forum?id=l6QnSQizmN) |  | 0 | We study online reinforcement learning (RL) in non-stationary environments, where a time-varying exogenous context process affects the environment dynamics. Online RL is challenging in such environments due to "catastrophic forgetting" (CF). The agent tends to forget prior knowledge as it trains on new experiences. Prior approaches to mitigate this issue assume task labels (which are often not available in practice), employ brittle regularization heuristics, or use off-policy methods that suffer from instability and poor performance. We present Locally Constrained Policy Optimization (LCPO), an online RL approach that combats CF by anchoring policy outputs on old experiences while optimizing the return on current experiences. To perform this anchoring, LCPO locally constrains policy optimization using samples from experiences that lie outside of the current context distribution. We evaluate LCPO in Mujoco, classic control and computer systems environments with a variety of synthetic and real context traces, and find that it outperforms a variety of baselines in the non-stationary setting, while achieving results on-par with a "prescient" agent trained offline across all context traces. LCPO's source code is available at https://github.com/pouyahmdn/LCPO. | Pouya Hamadanian, Arash NasrEsfahany, Malte Schwarzkopf, Siddhartha Sen, Mohammad Alizadeh |  |
| 273 |  |  [CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models](https://openreview.net/forum?id=IUmj2dw5se) |  | 0 | As Large Language Models (LLMs) are increasingly deployed to handle various natural language processing (NLP) tasks, concerns regarding the potential negative societal impacts of LLM-generated content have also arisen. To evaluate the biases exhibited by LLMs, researchers have recently proposed a variety of datasets. However, existing bias evaluation efforts often focus on only a particular type of bias and employ inconsistent evaluation metrics, leading to difficulties in comparison across different datasets and LLMs. To address these limitations, we collect a variety of datasets designed for the bias evaluation of LLMs, and further propose CEB, a Compositional Evaluation Bechmark that covers different types of bias across different social groups and tasks. The curation of CEB is based on our newly proposed compositional taxonomy, which characterizes each dataset from three dimensions: bias types, social groups, and tasks. By combining the three dimensions, we develop a comprehensive evaluation strategy for the bias in LLMs. Our experiments demonstrate that the levels of bias vary across these dimensions, thereby providing guidance for the development of specific bias mitigation methods. | Song Wang, Peng Wang, Tong Zhou, Yushun Dong, Zhen Tan, Jundong Li |  |
| 274 |  |  [TabWak: A Watermark for Tabular Diffusion Models](https://openreview.net/forum?id=71pur4y8gs) |  | 0 | Synthetic data offers alternatives for data augmentation and sharing. Till date, it remains unknown how to use watermarking techniques to trace and audit synthetic tables generated by tabular diffusion models to mitigate potential misuses. In this paper, we design TabWak, the first watermarking method to embed invisible signatures that control the sampling of Gaussian latent codes used to synthesize table rows via the diffusion backbone. TabWak has two key features. Different from existing image watermarking techniques, TabWak uses self-cloning and shuffling to embed the secret key in positional information of random seeds that control the Gaussian latents, allowing to use different seeds at each row for high inter-row diversity and enabling row-wise detectability. To further boost the robustness of watermark detection against post-editing attacks, TabWak uses a valid-bit mechanism that focuses on the tail of the latent code distribution for superior noise resilience. We provide theoretical guarantees on the row diversity and effectiveness of detectability. We evaluate TabWak on five datasets against baselines to show that the quality of watermarked tables remains nearly indistinguishable from non-watermarked tables while achieving high detectability in the presence of strong post-editing attacks, with a 100% true positive rate at a 0.1% false positive rate on synthetic tables with fewer than 300 rows. Our code is available at the following anonymized repository https://github.com/chaoyitud/TabWak. | Chaoyi Zhu, Jiayi Tang, Jeroen M. Galjaard, PinYu Chen, Robert Birke, Cornelis Bos, Lydia Y. Chen |  |
| 275 |  |  [Active Task Disambiguation with LLMs](https://openreview.net/forum?id=JAMxRSXLFz) |  | 0 | Despite the impressive performance of large language models (LLMs) across various benchmarks, their ability to address ambiguously specified problems—frequent in real-world interactions—remains underexplored. To address this gap, we introduce a formal definition of task ambiguity and frame the problem of task disambiguation through the lens of Bayesian Experimental Design. By posing clarifying questions, LLM agents can acquire additional task specifications, progressively narrowing the space of viable solutions and reducing the risk of generating unsatisfactory outputs. Yet, generating effective clarifying questions requires LLM agents to engage in a form of meta-cognitive reasoning, an ability LLMs may presently lack. Our proposed approach of active task disambiguation enables LLM agents to generate targeted questions maximizing the information gain. Effectively, this approach shifts the load from implicit to explicit reasoning about the space of viable solutions. Empirical results demonstrate that this form of question selection leads to more effective task disambiguation in comparison to approaches relying on reasoning solely within the space of questions. | Kasia Kobalczyk, Nicolás Astorga, Tennison Liu, Mihaela van der Schaar |  |
| 276 |  |  [Surprising Effectiveness of pretraining Ternary Language Model at Scale](https://openreview.net/forum?id=TJo6aQb7mK) |  | 0 | Rapid advancements in GPU computational power has outpaced memory capacity and bandwidth growth, creating bottlenecks in Large Language Model (LLM) inference. Post-training quantization is the leading method for addressing memory-related bottlenecks in LLM inference, but it suffers from significant performance degradation below 4-bit precision. This paper addresses these challenges by investigating the pretraining of low-bitwidth models specifically Ternary Language Models (TriLMs) as an alternative to traditional floating-point models (FloatLMs) and their post-training quantized versions (QuantLMs). We present Spectra LLM suite, the first open suite of LLMs spanning multiple bit-widths, including FloatLMs, QuantLMs, and TriLMs, ranging from 99M to 3.9B parameters trained on 300B tokens. Our comprehensive evaluation demonstrates that TriLMs offer superior scaling behavior in terms of model size (in bits). Surprisingly, at scales exceeding one billion parameters, TriLMs consistently outperform their QuantLM and FloatLM counterparts for a given bit size across various benchmarks. Notably, the 3.9B parameter TriLM matches the performance of the FloatLM 3.9B across all benchmarks, despite having fewer bits than FloatLM 830M. Overall, this research provides valuable insights into the feasibility and scalability of low-bitwidth language models, paving the way for the development of more efficient LLMs. | Ayush Kaushal, Tejas Vaidhya, Arnab Kumar Mondal, Tejas Pandey, Aaryan Bhagat, Irina Rish |  |
| 277 |  |  [Generating Freeform Endoskeletal Robots](https://openreview.net/forum?id=awvJBtB2op) |  | 0 | The automatic design of embodied agents (e.g. robots) has existed for 31 years and is experiencing a renaissance of interest in the literature. To date however, the field has remained narrowly focused on two kinds of anatomically simple robots: (1) fully rigid, jointed bodies; and (2) fully soft, jointless bodies. Here we bridge these two extremes with the open ended creation of terrestrial endoskeletal robots: deformable soft bodies that leverage jointed internal skeletons to move efficiently across land. Simultaneous de novo generation of external and internal structures is achieved by (i) modeling 3D endoskeletal body plans as integrated collections of elastic and rigid cells that directly attach to form soft tissues anchored to compound rigid bodies; (ii) encoding these discrete mechanical subsystems into a continuous yet coherent latent embedding; (iii) optimizing the sensorimotor coordination of each decoded design using model-free reinforcement learning; and (iv) navigating this smooth yet highly non-convex latent manifold using evolutionary strategies. This yields an endless stream of novel species of \`\`higher robots'' that, like all higher animals, harness the mechanical advantages of both elastic tissues and skeletal levers for terrestrial travel. It also provides a plug-and-play experimental platform for benchmarking evolutionary design and representation learning algorithms in complex hierarchical embodied systems. | Muhan Li, Lingji Kong, Sam Kriegman |  |
| 278 |  |  [Understanding Factual Recall in Transformers via Associative Memories](https://openreview.net/forum?id=hwSmPOAmhk) |  | 0 | Large language models have demonstrated an impressive ability to perform factual recall. Prior work has found that transformers trained on factual recall tasks can store information at a rate proportional to their parameter count. In our work, we show that shallow transformers can use a combination of associative memories to obtain such near optimal storage capacity. We begin by proving that the storage capacities of both linear and MLP associative memories scale linearly with parameter count. We next introduce a synthetic factual recall task, and prove that a transformer with a single layer of self-attention followed by an MLP can obtain 100\% accuracy on the task whenever either the total number of self-attention parameters or MLP parameters scales (up to log factors) linearly with the number of facts. In particular, the transformer can trade off between using the value matrices or the MLP as an associative memory to store the dataset of facts. We complement these expressivity results with an analysis of the gradient flow trajectory of a simplified linear attention model trained on our factual recall task, where we show that the model exhibits sequential learning behavior. | Eshaan Nichani, Jason D. Lee, Alberto Bietti |  |
| 279 |  |  [Nesterov acceleration in benignly non-convex landscapes](https://openreview.net/forum?id=YwJkv2YqBq) |  | 0 | While momentum-based optimization algorithms are commonly used in the notoriously non-convex optimization problems of deep learning, their analysis has historically been restricted to the convex and strongly convex setting. In this article, we partially close this gap between theory and practice and demonstrate that virtually identical guarantees can be obtained in optimization problems with a 'benign' non-convexity. We show that these weaker geometric assumptions are well justified in overparametrized deep learning, at least locally. Variations of this result are obtained for a continuous time model of Nesterov's accelerated gradient descent algorithm (NAG), the classical discrete time version of NAG, and versions of NAG with stochastic gradient estimates with purely additive noise and with noise that exhibits both additive and multiplicative scaling. | Kanan Gupta, Stephan Wojtowytsch |  |
| 280 |  |  [AnalogGenie: A Generative Engine for Automatic Discovery of Analog Circuit Topologies](https://openreview.net/forum?id=jCPak79Kev) |  | 0 | The massive and large-scale design of foundational semiconductor integrated circuits (ICs) is crucial to sustaining the advancement of many emerging and future technologies, such as generative AI, 5G/6G, and quantum computing. Excitingly, recent studies have shown the great capabilities of foundational models in expediting the design of digital ICs. Yet, applying generative AI techniques to accelerate the design of analog ICs remains a significant challenge due to critical domain-specific issues, such as the lack of a comprehensive dataset and effective representation methods for analog circuits. This paper proposes, $\textbf{AnalogGenie}$, a $\underline{\textbf{Gen}}$erat$\underline{\textbf{i}}$ve $\underline{\textbf{e}}$ngine for automatic design/discovery of $\underline{\textbf{Analog}}$ circuit topologies--the most challenging and creative task in the conventional manual design flow of analog ICs. AnalogGenie addresses two key gaps in the field: building a foundational comprehensive dataset of analog circuit topology and developing a scalable sequence-based graph representation universal to analog circuits. Experimental results show the remarkable generation performance of AnalogGenie in broadening the variety of analog ICs, increasing the number of devices within a single design, and discovering unseen circuit topologies far beyond any prior arts. Our work paves the way to transform the longstanding time-consuming manual design flow of analog ICs to an automatic and massive manner powered by generative AI. Our source code is available at https://github.com/xz-group/AnalogGenie. | Jian Gao, Weidong Cao, Junyi Yang, Xuan Zhang |  |
| 281 |  |  [Probabilistic Geometric Principal Component Analysis with application to neural data](https://openreview.net/forum?id=mkDam1xIzW) |  | 0 | Dimensionality reduction is critical across various domains of science including neuroscience. Probabilistic Principal Component Analysis (PPCA) is a prominent dimensionality reduction method that provides a probabilistic approach unlike the deterministic approach of PCA and serves as a connection between PCA and Factor Analysis (FA). Despite their power, PPCA and its extensions are mainly based on linear models and can only describe the data in a Euclidean coordinate system around the mean of data. However, in many neuroscience applications, data may be distributed around a nonlinear geometry (i.e., manifold) rather than lying in the Euclidean space around the mean. We develop Probabilistic Geometric Principal Component Analysis (PGPCA) for such datasets as a new dimensionality reduction algorithm that can explicitly incorporate knowledge about a given nonlinear manifold that is first fitted from these data. Further, we show how in addition to the Euclidean coordinate system, a geometric coordinate system can be derived for the manifold to capture the deviations of data from the manifold and noise. We also derive a data-driven EM algorithm for learning the PGPCA model parameters. As such, PGPCA generalizes PPCA to better describe data distributions by incorporating a nonlinear manifold geometry. In simulations and brain data analyses, we show that PGPCA can effectively model the data distribution around various given manifolds and outperforms PPCA for such data. Moreover, PGPCA provides the capability to test whether the new geometric coordinate system better describes the data than the Euclidean one. Finally, PGPCA can perform dimensionality reduction and learn the data distribution both around and on the manifold. These capabilities make PGPCA valuable for enhancing the efficacy of dimensionality reduction for analysis of high-dimensional data that exhibit noise and are distributed around a nonlinear manifold, especially for neural data. | HanLin Hsieh, Maryam Shanechi |  |
| 282 |  |  [DataEnvGym: Data Generation Agents in Teacher Environments with Student Feedback](https://openreview.net/forum?id=00SnKBGTsz) |  | 0 | The process of creating training data to teach models is currently driven by humans, who manually analyze model weaknesses and plan how to create data that improves a student model. Recent approaches using large language models (LLMs) as annotators reduce human annotation effort, but still require humans to interpret feedback from evaluations and control the LLM to produce data the student needs. Automating this labor-intensive process by creating autonomous data generation agents – or teachers – is desirable, but requires environments that can simulate the feedback-driven, iterative, closed loop of data creation. To enable rapid and scalable testing for such agents and their modules, we introduce DataEnvGym, a testbed of teacher environments for data generation agents. DataEnvGym frames data generation as a sequential decision-making task, involving an agent consisting of a data generation policy (which generates a plan for creating training data) and a data generation engine (which transforms the plan into data), inside an environment that provides feedback from a student. The agent’s end goal is to improve student model performance. Students are iteratively trained and evaluated on generated data, with their feedback (in the form of errors or weak skills) being reported to the agent after each iteration. As a general-purpose testbed, DataEnvGym includes multiple instantiations of teacher environments across three levels of structure in the state representation and action space, with varying levels of scaffolding support. More structured environments are based on automatically-inferred skills and offer a higher degree of interpretability and control over the curriculum. We support developing and testing data generation agents in four diverse tasks covering text, images, and actions (mathematics, programming, visual question answering, and tool-use) and test multiple student and teacher models. We find that example agents in our teaching environments can iteratively improve students across diverse tasks and settings. Moreover, we show that environments can teach different skill levels and can be used to test variants of key modules, pointing to directions of future work in improving data generation agents, engines, and feedback mechanisms. Project page: https://DataEnvGym.github.io. | Zaid Khan, Elias StengelEskin, Jaemin Cho, Mohit Bansal |  |
| 283 |  |  [Correcting the Mythos of KL-Regularization: Direct Alignment without Overoptimization via Chi-Squared Preference Optimization](https://openreview.net/forum?id=hXm0Wu2U9K) |  | 0 | Language model alignment methods such as reinforcement learning from human feedback (RLHF) have led to impressive advances in language model capabilities, but are limited by a widely observed phenomenon known as \*overoptimization\*, where the quality of the language model degrades over the course of the alignment process. As the model optimizes performance on an offline reward model, it overfits to inaccuracies and drifts away from preferred responses covered by the data. To discourage such distribution shift, KL-regularization is widely employed in existing offline alignment methods, but overoptimization continues to harm performance. Lending theoretical insight into the source of these empirical observations, we first show that the KL-regularization is too weak to prevent overfitting, then ask: is it possible to design an efficient algorithm that is provably robust to overoptimization? In this paper, we advance theoretical understanding of sample-efficient offline alignment and introduce a new algorithm called $\chi^2$-Preference Optimization ($\chi$PO). $\chi$PO is a one-line change to Direct Preference Optimization (DPO; Rafailov et al. 2023), that modifies only the logarithmic link function in the DPO objective. Despite this minimal change, $\chi$PO implicitly implements the principle of \*pessimism in the face of uncertainty\* via regularization with the $\chi^2$-divergence---which quantifies uncertainty more effectively than KL-regularization---and provably alleviates overoptimization, achieving sample-complexity guarantees based on \*single-policy concentrability\*, the gold standard in offline reinforcement learning. This guarantee makes $\chi$PO the first simple, yet general-purpose offline alignment algorithm that is provably robust to overoptimization. | Audrey Huang, Wenhao Zhan, Tengyang Xie, Jason D. Lee, Wen Sun, Akshay Krishnamurthy, Dylan J. Foster |  |
| 284 |  |  [Exact Certification of (Graph) Neural Networks Against Label Poisoning](https://openreview.net/forum?id=d9aWa875kj) |  | 0 | Machine learning models are highly vulnerable to label flipping, i.e., the adversarial modification (poisoning) of training labels to compromise performance. Thus, deriving robustness certificates is important to guarantee that test predictions remain unaffected and to understand worst-case robustness behavior. However, for Graph Neural Networks (GNNs), the problem of certifying label flipping has so far been unsolved. We change this by introducing an exact certification method, deriving both sample-wise and collective certificates. Our method leverages the Neural Tangent Kernel (NTK) to capture the training dynamics of wide networks enabling us to reformulate the bilevel optimization problem representing label flipping into a Mixed-Integer Linear Program (MILP). We apply our method to certify a broad range of GNN architectures in node classification tasks. Thereby, concerning the worst-case robustness to label flipping: $(i)$ we establish hierarchies of GNNs on different benchmark graphs; $(ii)$ quantify the effect of architectural choices such as activations, depth and skip-connections; and surprisingly, $(iii)$ uncover a novel phenomenon of the robustness plateauing for intermediate perturbation budgets across all investigated datasets and architectures. While we focus on GNNs, our certificates are applicable to sufficiently wide NNs in general through their NTK. Thus, our work presents the first exact certificate to a poisoning attack ever derived for neural networks, which could be of independent interest. The code is available at https://github.com/saper0/qpcert. | Mahalakshmi Sabanayagam, Lukas Gosch, Stephan Günnemann, Debarghya Ghoshdastidar |  |
| 285 |  |  [Better Instruction-Following Through Minimum Bayes Risk](https://openreview.net/forum?id=7xCSK9BLPy) |  | 0 | General-purpose LLM judges capable of human-level evaluation provide not only a scalable and accurate way of evaluating instruction-following LLMs but also new avenues for supervising and improving their performance. One promising way of leveraging LLM judges for supervision is through Minimum Bayes Risk (MBR) decoding, which uses a reference-based evaluator to select a high-quality output from amongst a set of candidate outputs. In the first part of this work, we explore using MBR decoding as a method for improving the test-time performance of instruction-following LLMs. We find that MBR decoding with reference-based LLM judges substantially improves over greedy decoding, best-of-N decoding with reference-free judges and MBR decoding with lexical and embedding-based metrics on AlpacaEval and MT-Bench. These gains are consistent across LLMs with up to 70B parameters, demonstrating that smaller LLM judges can be used to supervise much larger LLMs. Then, seeking to retain the improvements from MBR decoding while mitigating additional test-time costs, we explore iterative self-training on MBR-decoded outputs. We find that self-training using Direct Preference Optimisation leads to significant performance gains, such that the self-trained models with greedy decoding generally match and sometimes exceed the performance of their base models with MBR decoding. | Ian Wu, Patrick Fernandes, Amanda Bertsch, Seungone Kim, Sina Khoshfetrat Pakazad, Graham Neubig |  |
| 286 |  |  [Union-over-Intersections: Object Detection beyond Winner-Takes-All](https://openreview.net/forum?id=HqLHY4TzGj) |  | 0 | This paper revisits the problem of predicting box locations in object detection architectures. Typically, each box proposal or box query aims to directly maximize the intersection-over-union score with the ground truth, followed by a winner-takes-all non-maximum suppression where only the highest scoring box in each region is retained. We observe that both steps are sub-optimal: the first involves regressing proposals to the entire ground truth, which is a difficult task even with large receptive fields, and the second neglects valuable information from boxes other than the top candidate. Instead of regressing proposals to the whole ground truth, we propose a simpler approach—regress only to the area of intersection between the proposal and the ground truth. This avoids the need for proposals to extrapolate beyond their visual scope, improving localization accuracy. Rather than adopting a winner-takes-all strategy, we take the union over the regressed intersections of all boxes in a region to generate the final box outputs. Our plug-and-play method integrates seamlessly into proposal-based, grid-based, and query-based detection architectures with minimal modifications, consistently improving object localization and instance segmentation. We demonstrate its broad applicability and versatility across various detection and segmentation tasks. | Aritra Bhowmik, Pascal Mettes, Martin R. Oswald, Cees G. M. Snoek |  |
| 287 |  |  [MamKO: Mamba-based Koopman operator for modeling and predictive control](https://openreview.net/forum?id=hNjCVVm0EQ) |  | 0 | The Koopman theory, which enables the transformation of nonlinear systems into linear representations, is a powerful and efficient tool to model and control nonlinear systems. However, the ability of the Koopman operator to model complex systems, particularly time-varying systems, is limited by the fixed linear state-space representation. To address the limitation, the large language model, Mamba, is considered a promising strategy for enhancing modeling capabilities while preserving the linear state-space structure. In this paper, we propose a new framework, the Mamba-based Koopman operator (MamKO), which provides enhanced model prediction capability and adaptability, as compared to Koopman models with constant Koopman operators. Inspired by the Mamba structure, MamKO generates Koopman operators from online data; this enables the model to effectively capture the dynamic behaviors of the nonlinear system over time. A model predictive control system is then developed based on the proposed MamKO model. The modeling and control performance of the proposed method is evaluated through experiments on benchmark time-invariant and time-varying systems. The experimental results demonstrate the superiority of the proposed approach. Additionally, we perform ablation experiments to test the effectiveness of individual components of MamKO. This approach unlocks new possibilities for integrating large language models with control frameworks, and it achieves a good balance between advanced modeling capabilities and real-time control implementation efficiency. | Zhaoyang Li, Minghao Han, Xunyuan Yin |  |
| 288 |  |  [LoRA3D: Low-Rank Self-Calibration of 3D Geometric Foundation models](https://openreview.net/forum?id=LSp4KBhAom) |  | 0 | Emerging 3D geometric foundation models, such as DUSt3R, offer a promising approach for in-the-wild 3D vision tasks. However, due to the high-dimensional nature of the problem space and scarcity of high-quality 3D data, these pre-trained models still struggle to generalize to many challenging circumstances, such as limited view overlap or low lighting. To address this, we propose LoRA3D, an efficient self-calibration pipeline to \*specialize\* the pre-trained models to target scenes using their own multi-view predictions. Taking sparse RGB images as input, we leverage robust optimization techniques to refine multi-view predictions and align them into a global coordinate frame. In particular, we incorporate prediction confidence into the geometric optimization process, automatically re-weighting the confidence to better reflect point estimation accuracy. We use the calibrated confidence to generate high-quality pseudo labels for the calibrating views and fine-tune the models using low-rank adaptation (LoRA) on the pseudo-labeled data. Our method does not require any external priors or manual labels. It completes the self-calibration process on a \*\*single standard GPU within just 5 minutes\*\*. Each low-rank adapter requires only \*\*18MB\*\* of storage. We evaluated our method on \*\*more than 160 scenes\*\* from the Replica, TUM and Waymo Open datasets, achieving up to \*\*88\% performance improvement\*\* on 3D reconstruction, multi-view pose estimation and novel-view rendering. For more details, please visit our project page at https://520xyxyzq.github.io/lora3d/. | Ziqi Lu, Heng Yang, Danfei Xu, Boyi Li, Boris Ivanovic, Marco Pavone, Yue Wang |  |
| 289 |  |  [Weighted Point Set Embedding for Multimodal Contrastive Learning Toward Optimal Similarity Metric](https://openreview.net/forum?id=uSz2K30RRd) |  | 0 | In typical multimodal contrastive learning, such as CLIP, encoders produce one point in the latent representation space for each input. However, one-point representation has difficulty in capturing the relationship and the similarity structure of a huge amount of instances in the real world. For richer classes of the similarity, we propose the use of weighted point sets, namely, sets of pairs of weight and vector, as representations of instances. In this work, we theoretically show the benefit of our proposed method through a new understanding of the contrastive loss of CLIP, which we call symmetric InfoNCE. We clarify that the optimal similarity that minimizes symmetric InfoNCE is the pointwise mutual information, and show an upper bound of excess risk on downstream classification tasks of representations that achieve the optimal similarity. In addition, we show that our proposed similarity based on weighted point sets consistently achieves the optimal similarity. To verify the effectiveness of our proposed method, we demonstrate pretraining of text-image representation models and classification tasks on common benchmarks. | Toshimitsu Uesaka, Taiji Suzuki, Yuhta Takida, ChiehHsin Lai, Naoki Murata, Yuki Mitsufuji |  |
| 290 |  |  [Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar Book?](https://openreview.net/forum?id=aMBSY2ebPw) |  | 0 | Extremely low-resource (XLR) languages lack substantial corpora for training NLP models, motivating the use of all available resources such as dictionaries and grammar books. Machine Translation from One Book (Tanzer et al., 2024) suggests that prompting long-context LLMs with one grammar book enables English–Kalamang translation, an XLR language unseen by LLMs—a noteworthy case of linguistics helping an NLP task. We investigate the source of this translation ability, finding almost all improvements stem from the book’s parallel examples rather than its grammatical explanations. We find similar results for Nepali and Guarani, seen low-resource languages, and we achieve performance comparable to an LLM with a grammar book by simply fine-tuning an encoder-decoder translation model. We then investigate where grammar books help by testing two linguistic tasks, grammaticality judgment and gloss prediction, and we explore what kind of grammatical knowledge helps by introducing a typological feature prompt that achieves leading results on these more relevant tasks. We thus emphasise the importance of task-appropriate data for XLR languages: parallel examples for translation, and grammatical data for linguistic tasks. As we find no evidence that long-context LLMs can make effective use of grammatical explanations for XLR translation, we conclude data collection for multilingual XLR tasks such as translation is best focused on parallel data over linguistic description. | Seth Aycock, David Stap, Di Wu, Christof Monz, Khalil Sima'an |  |
| 291 |  |  [Retri3D: 3D Neural Graphics Representation Retrieval](https://openreview.net/forum?id=q3EbOXb4y1) |  | 0 | Learnable 3D Neural Graphics Representations (3DNGR) have emerged as promising 3D representations for reconstructing 3D scenes from 2D images. Numerous works, including Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and their variants, have significantly enhanced the quality of these representations. The ease of construction from 2D images, suitability for online viewing/sharing, and applications in game/art design downstream tasks make it a vital 3D representation, with potential creation of large numbers of such 3D models. This necessitates large data stores, local or online, to save 3D visual data in these formats. However, no existing framework enables accurate retrieval of stored 3DNGRs. In this work, we propose, Retri3D, a framework that enables accurate and efficient retrieval of 3D scenes represented as NGRs from large data stores using text queries. We introduce a novel Neural Field Artifact Analysis technique, combined with a Smart Camera Movement Module, to select clean views and navigate pre-trained 3DNGRs. These techniques enable accurate retrieval by selecting the best viewing directions in the 3D scene for high-quality visual feature embeddings. We demonstrate that Retri3D is compatible with any NGR representation. On the LERF and ScanNet++ datasets, we show significant improvement in retrieval accuracy compared to existing techniques, while being orders of magnitude faster and storage efficient. | Yushi Guan, Daniel Kwan, Jean Sebastien Dandurand, Xi Yan, Ruofan Liang, Yuxuan Zhang, Nilesh Jain, Nilesh A. Ahuja, Selvakumar Panneer, Nandita Vijaykumar |  |
| 292 |  |  [Test-time Adaptation for Cross-modal Retrieval with Query Shift](https://openreview.net/forum?id=BmG88rONaU) |  | 0 | The success of most existing cross-modal retrieval methods heavily relies on the assumption that the given queries follow the same distribution of the source domain. However, such an assumption is easily violated in real-world scenarios due to the complexity and diversity of queries, thus leading to the query shift problem. Specifically, query shift refers to the online query stream originating from the domain that follows a different distribution with the source one. In this paper, we observe that query shift would not only diminish the uniformity (namely, within-modality scatter) of the query modality but also amplify the gap between query and gallery modalities. Based on the observations, we propose a novel method dubbed Test-time adaptation for Cross-modal Retrieval (TCR). In brief, TCR employs a novel module to refine the query predictions (namely, retrieval results of the query) and a joint objective to prevent query shift from disturbing the common space, thus achieving online adaptation for the cross-modal retrieval models with query shift. Expensive experiments demonstrate the effectiveness of the proposed TCR against query shift. Code is available at https://github.com/XLearning-SCU/2025-ICLR-TCR. | Haobin Li, Peng Hu, Qianjun Zhang, Xi Peng, XitingLiu, Mouxing Yang |  |
| 293 |  |  [Universal generalization guarantees for Wasserstein distributionally robust models](https://openreview.net/forum?id=0h6v4SpLCY) |  | 0 | Distributionally robust optimization has emerged as an attractive way to train robust machine learning models, capturing data uncertainty and distribution shifts. Recent statistical analyses have proved that generalization guarantees of robust models based on the Wasserstein distance have generalization guarantees that do not suffer from the curse of dimensionality. However, these results are either approximate, obtained in specific cases, or based on assumptions difficult to verify in practice. In contrast, we establish exact generalization guarantees that cover a wide range of cases, with arbitrary transport costs and parametric loss functions, including deep learning objectives with nonsmooth activations. We complete our analysis with an excess bound on the robust objective and an extension to Wasserstein robust models with entropic regularizations. | Tam Le, Jérôme Malick |  |
| 294 |  |  [Conformal Prediction Sets Can Cause Disparate Impact](https://openreview.net/forum?id=fZK6AQXlUU) |  | 0 | Conformal prediction is a statistically rigorous method for quantifying uncertainty in models by having them output sets of predictions, with larger sets indicating more uncertainty. However, prediction sets are not inherently actionable; many applications require a single output to act on, not several. To overcome this limitation, prediction sets can be provided to a human who then makes an informed decision. In any such system it is crucial to ensure the fairness of outcomes across protected groups, and researchers have proposed that Equalized Coverage be used as the standard for fairness. By conducting experiments with human participants, we demonstrate that providing prediction sets can lead to disparate impact in decisions. Disquietingly, we find that providing sets that satisfy Equalized Coverage actually increases disparate impact compared to marginal coverage. Instead of equalizing coverage, we propose to equalize set sizes across groups which empirically leads to lower disparate impact. | Jesse C. Cresswell, Bhargava Kumar, Yi Sui, Mouloud Belbahri |  |
| 295 |  |  [In Search of Forgotten Domain Generalization](https://openreview.net/forum?id=Fk3eod9aaD) |  | 0 | Out-of-Domain (OOD) generalization is the ability of a model trained on one or more domains to generalize to unseen domains. In the ImageNet era of computer vision, evaluation sets for measuring a model's OOD performance were designed to be strictly OOD with respect to style. However, the emergence of foundation models and expansive web-scale datasets has obfuscated this evaluation process, as datasets cover a broad range of domains and risk test domain contamination. In search of the forgotten domain generalization, we create large-scale datasets subsampled from LAION---LAION-Natural and LAION-Rendition---that are strictly OOD to corresponding ImageNet and DomainNet test sets in terms of style. Training CLIP models on these datasets reveals that a significant portion of their performance is explained by in-domain examples. This indicates that the OOD generalization challenges from the ImageNet era still prevail and that training on web-scale data merely creates the illusion of OOD generalization. Furthermore, through a systematic exploration of combining natural and rendition datasets in varying proportions, we identify optimal mixing ratios for model generalization across these domains. Our datasets and results re-enable meaningful assessment of OOD robustness at scale---a crucial prerequisite for improving model robustness. | Prasanna Mayilvahanan, Roland S. Zimmermann, Thaddäus Wiedemer, Evgenia Rusak, Attila Juhos, Matthias Bethge, Wieland Brendel |  |
| 296 |  |  [TOP-ERL: Transformer-based Off-Policy Episodic Reinforcement Learning](https://openreview.net/forum?id=N4NhVN30ph) |  | 0 | This work introduces Transformer-based Off-Policy Episodic Reinforcement Learning (TOP-ERL), a novel algorithm that enables off-policy updates in the ERL framework. In ERL, policies predict entire action trajectories over multiple time steps instead of single actions at every time step. These trajectories are typically parameterized by trajectory generators such as Movement Primitives (MP), allowing for smooth and efficient exploration over long horizons while capturing high-level temporal correlations. However, ERL methods are often constrained to on-policy frameworks due to the difficulty of evaluating state-action values for entire action sequences, limiting their sample efficiency and preventing the use of more efficient off-policy architectures. TOP-ERL addresses this shortcoming by segmenting long action sequences and estimating the state-action values for each segment using a transformer-based critic architecture alongside an n-step return estimation. These contributions result in efficient and stable training that is reflected in the empirical results conducted on sophisticated robot learning environments. TOP-ERL significantly outperforms state-of-the-art RL methods. Thorough ablation studies additionally show the impact of key design choices on the model performance. | Ge Li, Dong Tian, Hongyi Zhou, Xinkai Jiang, Rudolf Lioutikov, Gerhard Neumann |  |
| 297 |  |  [On the Expressiveness of Rational ReLU Neural Networks With Bounded Depth](https://openreview.net/forum?id=uREg3OHjLL) |  | 0 | To confirm that the expressive power of ReLU neural networks grows with their depth, the function $F_n = \max (0,x_1,\ldots,x_n )$ has been considered in the literature. A conjecture by Hertrich, Basu, Di Summa, and Skutella [NeurIPS 2021] states that any ReLU network that exactly represents $F_n$ has at least $\lceil \log_2 (n+1) \rceil$ hidden layers. The conjecture has recently been confirmed for networks with integer weights by Haase, Hertrich, and Loho [ICLR 2023]. We follow up on this line of research and show that, within ReLU networks whose weights are decimal fractions, $F_n$ can only be represented by networks with at least $\lceil \log_3 (n+1) \rceil$ hidden layers. Moreover, if all weights are $N$-ary fractions, then $F_n$ can only be represented by networks with at least $\Omega( \frac{\ln n}{\ln \ln N})$ layers. These results are a partial confirmation of the above conjecture for rational ReLU networks, and provide the first non-constant lower bound on the depth of practically relevant ReLU networks. | Gennadiy Averkov, Christopher Hojny, Maximilian Merkert |  |
| 298 |  |  [On the Optimization and Generalization of Two-layer Transformers with Sign Gradient Descent](https://openreview.net/forum?id=97rOQDPmk2) |  | 0 | The Adam optimizer is widely used for transformer optimization in practice, which makes understanding the underlying optimization mechanisms an important problem. However, due to the Adam's complexity, theoretical analysis of how it optimizes transformers remains a challenging task. Fortunately, Sign Gradient Descent (SignGD) serves as an effective surrogate for Adam. Despite its simplicity, theoretical understanding of how SignGD optimizes transformers still lags behind. In this work, we study how SignGD optimizes a two-layer transformer -- consisting of a softmax attention layer with trainable query-key parameterization followed by a linear layer -- on a linearly separable noisy dataset. We identify four stages in the training dynamics, each exhibiting intriguing behaviors. Based on the training dynamics, we prove the fast convergence but poor generalization of the learned transformer on the noisy dataset. We also show that Adam behaves similarly to SignGD in terms of both optimization and generalization in this setting. Additionally, we find that the poor generalization of SignGD is not solely due to data noise, suggesting that both SignGD and Adam requires high-quality data for real-world tasks. Finally, experiments on synthetic and real-world datasets empirically support our theoretical results. | Bingrui Li, Wei Huang, Andi Han, Zhanpeng Zhou, Taiji Suzuki, Jun Zhu, Jianfei Chen |  |
| 299 |  |  [Emergent Orientation Maps - - Mechanisms, Coding Efficiency and Robustness](https://openreview.net/forum?id=rySLejeB1k) |  | 0 | Extensive experimental studies have shown that in lower mammals, neuronal orientation preference in the primary visual cortex is organized in disordered "salt-and-pepper" organizations. In contrast, higher-order mammals display a continuous variation in orientation preference, forming pinwheel-like structures. Despite these observations, the spiking mechanisms underlying the emergence of these distinct topological structures and their functional roles in visual processing remain poorly understood. To address this, we developed a self-evolving spiking neural network model with Hebbian plasticity, trained using physiological parameters characteristic of rodents, cats, and primates, including retinotopy, neuronal morphology, and connectivity patterns. Our results identify critical factors, such as the degree of input visual field overlap, neuronal connection range, and the balance between localized connectivity and long-range competition, that determine the emergence of either salt-and-pepper or pinwheel-like topologies. Furthermore, we demonstrate that pinwheel structures exhibit lower wiring costs and enhanced sparse coding capabilities compared to salt-and-pepper organizations. They also maintain greater coding robustness against noise in naturalistic visual stimuli. These findings suggest that such topological structures confer significant computational advantages in visual processing and highlight their potential application in the design of brain-inspired deep learning networks and algorithms. | Haixin Zhong, Haoyu Wang, Wei P. Dai, Yuchao Huang, Mingyi Huang, Rubin Wang, Anna Wang Roe, Yuguo Yu |  |
| 300 |  |  [How Much is Unseen Depends Chiefly on Information About the Seen](https://openreview.net/forum?id=uqWM9hBDAE) |  | 0 | The \*missing mass\* refers to the proportion of data points in an \*unknown\* population of classifier inputs that belong to classes \*not\* present in the classifier's training data, which is assumed to be a random sample from that unknown population. We find that \*in expectation\* the missing mass is entirely determined by the number $f_k$ of classes that \*do\* appear in the training data the same number of times \*and an exponentially decaying error\*. While this is the first precise characterization of the expected missing mass in terms of the sample, the induced estimator suffers from an impractically high variance. However, our theory suggests a large search space of nearly unbiased estimators that can be searched effectively and efficiently. Hence, we cast distribution-free estimation as an optimization problem to find a distribution-specific estimator with a minimized mean-squared error (MSE), given only the sample. In our experiments, our search algorithm discovers estimators that have a substantially smaller MSE than the state-of-the-art Good-Turing estimator. This holds for over 93\% of runs when there are at least as many samples as classes. Our estimators' MSE is roughly 80\% of the Good-Turing estimator's. | Seongmin Lee, Marcel Boehme |  |
| 301 |  |  [Harnessing Diversity for Important Data Selection in Pretraining Large Language Models](https://openreview.net/forum?id=bMC1t7eLRc) |  | 0 | Data selection is of great significance in pretraining large language models, given the variation in quality within the large-scale available training corpora. To achieve this, researchers are currently investigating the use of data influence to measure the importance of data instances, $i.e.,$ a high influence score indicates that incorporating this instance to the training set is likely to enhance the model performance. Consequently, they select the top-$k$ instances with the highest scores. However, this approach has several limitations. (1) Calculating the accurate influence of all available data is time-consuming. (2) The selected data instances are not diverse enough, which may hinder the pretrained model's ability to generalize effectively to various downstream tasks. In this paper, we introduce $\texttt{Quad}$, a data selection approach that considers both quality and diversity by using data influence to achieve state-of-the-art pretraining results. To compute the influence ($i.e.,$ the quality) more accurately and efficiently, we incorporate the attention layers to capture more semantic details, which can be accelerated through the Kronecker product. For the diversity, $\texttt{Quad}$ clusters the dataset into similar data instances within each cluster and diverse instances across different clusters. For each cluster, if we opt to select data from it, we take some samples to evaluate the influence to prevent processing all instances. Overall, we favor clusters with highly influential instances (ensuring high quality) or clusters that have been selected less frequently (ensuring diversity), thereby well balancing between quality and diversity. Experiments on Slimpajama and FineWeb over 7B large language models demonstrate that $\texttt{Quad}$ significantly outperforms other data selection methods with a low FLOPs consumption. Further analysis also validates the effectiveness of our influence calculation. | Chi Zhang, Huaping Zhong, Kuan Zhang, Chengliang Chai, Rui Wang, Xinlin Zhuang, Tianyi Bai, Jiantao Qiu, Lei Cao, Ju Fan, Ye Yuan, Guoren Wang, Conghui He |  |
| 302 |  |  [Adaptive Gradient Clipping for Robust Federated Learning](https://openreview.net/forum?id=03OkC0LKDD) |  | 0 | Robust federated learning aims to maintain reliable performance despite the presence of adversarial or misbehaving workers. While state-of-the-art (SOTA) robust distributed gradient descent (Robust-DGD) methods were proven theoretically optimal, their empirical success has often relied on pre-aggregation gradient clipping. However, existing static clipping strategies yield inconsistent results: enhancing robustness against some attacks while being ineffective or even detrimental against others. To address this limitation, we propose a principled adaptive clipping strategy, Adaptive Robust Clipping (ARC), which dynamically adjusts clipping thresholds based on the input gradients. We prove that ARC not only preserves the theoretical robustness guarantees of SOTA Robust-DGD methods but also provably improves asymptotic convergence when the model is well-initialized. Extensive experiments on benchmark image classification tasks confirm these theoretical insights, demonstrating that ARC significantly enhances robustness, particularly in highly heterogeneous and adversarial settings. | Youssef Allouah, Rachid Guerraoui, Nirupam Gupta, Ahmed Jellouli, Geovani Rizk, John Stephan |  |
| 303 |  |  [Imputation for prediction: beware of diminishing returns](https://openreview.net/forum?id=D1Y2XFgsPI) |  | 0 | Missing values are prevalent across various fields, posing challenges for training and deploying predictive models. In this context, imputation is a common practice, driven by the hope that accurate imputations will enhance predictions. However, recent theoretical and empirical studies indicate that simple constant imputation can be consistent and competitive. This empirical study aims at clarifying \*if\* and \*when\* investing in advanced imputation methods yields significantly better predictions. Relating imputation and predictive accuracies across combinations of imputation and predictive models on 19 datasets, we show that imputation accuracy matters less i) when using expressive models, ii) when incorporating missingness indicators as complementary inputs, iii) matters much more for generated linear outcomes than for real-data outcomes. Interestingly, we also show that the use of the missingness indicator is beneficial to the prediction performance, even in MCAR scenarios. Overall, on real-data with powerful models, imputation quality has only a minor effect on prediction performance. Thus, investing in better imputations for improved predictions often offers limited benefits. | Marine Le Morvan, Gaël Varoquaux |  |
| 304 |  |  [Joint Gradient Balancing for Data Ordering in Finite-Sum Multi-Objective Optimization](https://openreview.net/forum?id=rdAbEn5DZt) |  | 0 | In finite-sum optimization problems, the sample orders for parameter updates can significantly influence the convergence rate of optimization algorithms. While numerous sample ordering techniques have been proposed in the context of single-objective optimization, the problem of sample ordering in finite-sum multi-objective optimization has not been thoroughly explored. To address this gap, we propose a sample ordering method called JoGBa, which finds the sample orders for multiple objectives by jointly performing online vector balancing on the gradients of all objectives. Our theoretical analysis demonstrates that this approach outperforms the standard baseline of random ordering and accelerates the convergence rate for the MGDA algorithm. Empirical evaluation across various datasets with different multi-objective optimization algorithms further demonstrates that JoGBa can achieve faster convergence and superior final performance than other data ordering strategies. | Hansi Yang, James T. Kwok |  |
| 305 |  |  [Learning from negative feedback, or positive feedback or both](https://openreview.net/forum?id=4FVGowGzQb) |  | 0 | Existing preference optimization methods often assume scenarios where paired preference feedback (preferred/positive vs. dis-preferred/negative examples) is available. This requirement limits their applicability in scenarios where only unpaired feedback—for example, either positive or negative— is available. To address this, we introduce a novel approach that decouples learning from positive and negative feedback. This decoupling enables control over the influence of each feedback type and, importantly, allows learning even when only one feedback type is present. A key contribution is demonstrating stable learning from negative feedback alone, a capability not well-addressed by current methods. Our approach builds upon the probabilistic framework introduced in (Dayan and Hinton, 1997), which uses expectation-maximization (EM) to directly optimize the probability of positive outcomes (as opposed to classic expected reward maximization). We address a key limitation in current EM-based methods: they solely maximize the likelihood of positive examples, while neglecting negative ones. We show how to extend EM algorithms to explicitly incorporate negative examples, leading to a theoretically grounded algorithm that offers an intuitive and versatile way to learn from both positive and negative feedback. We evaluate our approach for training language models based on human feedback as well as training policies for sequential decision-making problems, where learned value functions are available. | Abbas Abdolmaleki, Bilal Piot, Bobak Shahriari, Jost Tobias Springenberg, Tim Hertweck, Michael Bloesch, Rishabh Joshi, Thomas Lampe, Junhyuk Oh, Nicolas Heess, Jonas Buchli, Martin A. Riedmiller |  |
| 306 |  |  [CausalRivers - Scaling up benchmarking of causal discovery for real-world time-series](https://openreview.net/forum?id=wmV4cIbgl6) |  | 0 | Causal discovery, or identifying causal relationships from observational data, is a notoriously challenging task, with numerous methods proposed to tackle it. Despite this, in-the-wild evaluation of these methods is still lacking, as works frequently rely on synthetic data evaluation and sparse real-world examples under critical theoretical assumptions. Real-world causal structures, however, are often complex, evolving over time, non-linear, and influenced by unobserved factors, making it hard to decide on a proper causal discovery strategy. To bridge this gap, we introduce CausalRivers, the largest in-the-wild causal discovery benchmarking kit for time-series data to date. CausalRivers features an extensive dataset on river discharge that covers the eastern German territory (666 measurement stations) and the state of Bavaria (494 measurement stations). It spans the years 2019 to 2023 with a 15-minute temporal resolution. Further, we provide additional data from a flood around the Elbe River, as an event with a pronounced distributional shift. Leveraging multiple sources of information and time-series meta-data, we constructed two distinct causal ground truth graphs (Bavaria and eastern Germany). These graphs can be sampled to generate thousands of subgraphs to benchmark causal discovery across diverse and challenging settings. To demonstrate the utility of CausalRivers, we evaluate several causal discovery approaches through a set of experiments to identify areas for improvement. CausalRivers has the potential to facilitate robust evaluations and comparisons of causal discovery methods. Besides this primary purpose, we also expect that this dataset will be relevant for connected areas of research, such as time-series forecasting and anomaly detection. Based on this, we hope to push benchmark-driven method development that fosters advanced techniques for causal discovery, as is the case for many other areas of machine learning. | Gideon Stein, Maha Shadaydeh, Jan Blunk, Niklas Penzel, Joachim Denzler |  |
| 307 |  |  [Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes](https://openreview.net/forum?id=Nx4PMtJ1ER) |  | 0 | Inferring the causal structure underlying stochastic dynamical systems from observational data holds great promise in domains ranging from science and health to finance. Such processes can often be accurately modeled via stochastic differential equations (SDEs), which naturally imply causal relationships via \`which variables enter the differential of which other variables'. In this paper, we develop conditional independence (CI) constraints on coordinate processes over selected intervals that are Markov with respect to the acyclic dependence graph (allowing self-loops) induced by a general SDE model. We then provide a sound and complete causal discovery algorithm, capable of handling both fully and partially observed data, and uniquely recovering the underlying or induced ancestral graph by exploiting time directionality assuming a CI oracle. Finally, to make our algorithm practically usable, we also propose a flexible, consistent signature kernel-based CI test to infer these constraints from data. We extensively benchmark the CI test in isolation and as part of our causal discovery algorithms, outperforming existing approaches in SDE models and beyond. | Georg Manten, Cecilia Casolo, Emilio Ferrucci, Søren Wengel Mogensen, Cristopher Salvi, Niki Kilbertus |  |
| 308 |  |  [Provably Reliable Conformal Prediction Sets in the Presence of Data Poisoning](https://openreview.net/forum?id=ofuLWn8DFZ) |  | 0 | Conformal prediction provides model-agnostic and distribution-free uncertainty quantification through prediction sets that are guaranteed to include the ground truth with any user-specified probability. Yet, conformal prediction is not reliable under poisoning attacks where adversaries manipulate both training and calibration data, which can significantly alter prediction sets in practice. As a solution, we propose reliable prediction sets (RPS): the first efficient method for constructing conformal prediction sets with provable reliability guarantees under poisoning. To ensure reliability under training poisoning, we introduce smoothed score functions that reliably aggregate predictions of classifiers trained on distinct partitions of the training data. To ensure reliability under calibration poisoning, we construct multiple prediction sets, each calibrated on distinct subsets of the calibration data. We then aggregate them into a majority prediction set, which includes a class only if it appears in a majority of the individual sets. Both proposed aggregations mitigate the influence of datapoints in the training and calibration data on the final prediction set. We experimentally validate our approach on image classification tasks, achieving strong reliability while maintaining utility and preserving coverage on clean data. Overall, our approach represents an important step towards more trustworthy uncertainty quantification in the presence of data poisoning. | Yan Scholten, Stephan Günnemann |  |
| 309 |  |  [Weak-to-Strong Preference Optimization: Stealing Reward from Weak Aligned Model](https://openreview.net/forum?id=f7KxfUrRSb) |  | 0 | Aligning language models (LMs) with human preferences has become a key area of research, enabling these models to meet diverse user needs better. Inspired by weak-to-strong generalization, where a strong LM fine-tuned on labels generated by a weaker model can consistently outperform its weak supervisor, we extend this idea to model alignment. In this work, we observe that the alignment behavior in weaker models can be effectively transferred to stronger models and even exhibit an amplification effect. Based on this insight, we propose a method called Weak-to-Strong Preference Optimization (WSPO), which achieves strong model alignment by learning the distribution differences before and after the alignment of the weak model. Experiments demonstrate that WSPO delivers outstanding performance, improving the win rate of Qwen2-7B-Instruct on Arena-Hard from 39.70 to 49.60, achieving a remarkable 47.04 length-controlled win rate on AlpacaEval 2, and scoring 7.33 on MT-bench. Our results suggest that using the weak model to elicit a strong model with a high alignment ability is feasible. The code is available at https://github.com/zwhong714/weak-to-strong-preference-optimization. | Wenhong Zhu, Zhiwei He, Xiaofeng Wang, Pengfei Liu, Rui Wang |  |
| 310 |  |  [Benchmarking Predictive Coding Networks - Made Simple](https://openreview.net/forum?id=sahQq2sH5x) |  | 0 | In this work, we tackle the problems of efficiency and scalability for predictive coding networks (PCNs) in machine learning. To do so, we propose a library that focuses on performance and simplicity, and use it to implement a large set of standard benchmarks for the community to use for their experiments. As most works in the field propose their own tasks and architectures, do not compare one against each other, and focus on small-scale tasks, a simple and fast open-source library, and a comprehensive set of benchmarks, would address all of these concerns. Then, we perform extensive tests on such benchmarks using both existing algorithms for PCNs, as well as adaptations of other methods popular in the bio-plausible deep learning community. All of this has allowed us to (i) test architectures much larger than commonly used in the literature, on more complex datasets; (ii) reach new state-of-the-art results in all of the tasks and dataset provided; (iii) clearly highlight what the current limitations of PCNs are, allowing us to state important future research directions. With the hope of galvanizing community efforts towards one of the main open problems in the field, scalability, we will release the code, tests, and benchmarks. | Luca Pinchetti, Chang Qi, Oleh Lokshyn, Cornelius Emde, Amine M'Charrak, Mufeng Tang, Simon Frieder, Bayar Menzat, Gaspard Oliviers, Rafal Bogacz, Thomas Lukasiewicz, Tommaso Salvatori |  |
| 311 |  |  [LiFT: Learning to Fine-Tune via Bayesian Parameter Efficient Meta Fine-Tuning](https://openreview.net/forum?id=7nyJBVCTGQ) |  | 0 | We tackle the problem of parameter-efficient fine-tuning (PEFT) of a pre-trained large deep model on many different but related tasks. Instead of the simple but strong baseline strategy of task-wise independent fine-tuning, we aim to meta-learn the core shared information that can be used for unseen test tasks to improve the prediction performance further. That is, we propose a method for {\em learning-to-fine-tune} (LiFT). LiFT introduces a novel hierarchical Bayesian model that can be superior to both existing general meta learning algorithms like MAML and recent LoRA zoo mixing approaches such as LoRA-Retriever and model-based clustering. In our Bayesian model, the parameters of the task-specific LoRA modules are regarded as random variables where these task-wise LoRA modules are governed/regularized by higher-level latent random variables, which represents the prior of the LoRA modules that capture the shared information across all training tasks. To make the posterior inference feasible, we propose a novel SGLD-Gibbs sampling algorithm that is computationally efficient. To represent the posterior samples from the SGLD-Gibbs, we propose an online EM algorithm that maintains a Gaussian mixture representation for the posterior in an online manner in the course of iterative posterior sampling. We demonstrate the effectiveness of LiFT on NLP and vision multi-task meta learning benchmarks. | Minyoung Kim, Timothy M. Hospedales |  |
| 312 |  |  [Lightweight Neural App Control](https://openreview.net/forum?id=BL4WBIfyrz) |  | 0 | This paper introduces a novel mobile phone control architecture, Lightweight Multi-modal App Control (LiMAC), for efficient interactions and control across various Android apps. LiMAC takes as input a textual goal and a sequence of past mobile observations, such as screenshots and corresponding UI trees, to generate precise actions. To address the computational constraints inherent to smartphones, we introduce a small Action Transformer (AcT) integrated with a fine-tuned vision-language model (VLM) for real-time decision-making and task execution. We evaluate LiMAC on two open-source mobile control datasets, demonstrating the superior performance of our small-form-factor approach against fine-tuned versions of open-source VLMs, such as Florence2 and Qwen2-VL. It also significantly outperforms prompt engineering baselines utilising closed-source foundation models like GPT-4o. More specifically, LiMAC increases the overall action accuracy by up to 19% compared to fine-tuned VLMs, and up to 42% compared to prompt-engineering baselines. | Filippos Christianos, Georgios Papoudakis, Thomas Coste, Jianye Hao, Jun Wang, Kun Shao |  |
| 313 |  |  [Progressive Compositionality in Text-to-Image Generative Models](https://openreview.net/forum?id=S85PP4xjFD) |  | 0 | Despite the impressive text-to-image (T2I) synthesis capabilities of diffusion models, they often struggle to understand compositional relationships between objects and attributes, especially in complex settings. Existing approaches through building compositional architectures or generating difficult negative captions often assume a fixed prespecified compositional structure, which limits generalization to new distributions. In this paper, we argue that curriculum training is crucial to equipping generative models with a fundamental understanding of compositionality. To achieve this, we leverage large-language models (LLMs) to automatically compose complex scenarios and harness Visual-Question Answering (VQA) checkers to automatically curate a contrastive dataset, ConPair, consisting of 15k pairs of high-quality contrastive images. These pairs feature minimal visual discrepancies and cover a wide range of attribute categories, especially complex and natural scenarios. To learn effectively from these error cases (i.e., hard negative images), we propose EvoGen, a new multi-stage curriculum for contrastive learning of diffusion models. Through extensive experiments across a wide range of compositional scenarios, we showcase the effectiveness of our proposed framework on compositional T2I benchmarks. | Xu Han, Linghao Jin, Xiaofeng Liu, Paul Pu Liang |  |
| 314 |  |  [On Quantizing Neural Representation for Variable-Rate Video Coding](https://openreview.net/forum?id=44cMlQSreK) |  | 0 | This work introduces NeuroQuant, a novel post-training quantization (PTQ) approach tailored to non-generalized Implicit Neural Representations for variable-rate Video Coding (INR-VC). Unlike existing methods that require extensive weight retraining for each target bitrate, we hypothesize that variable-rate coding can be achieved by adjusting quantization parameters (QPs) of pre-trained weights. Our study reveals that traditional quantization methods, which assume inter-layer independence, are ineffective for non-generalized INR-VC models due to significant dependencies across layers. To address this, we redefine variable-rate INR-VC as a mixed-precision quantization problem and establish a theoretical framework for sensitivity criteria aimed at simplified, fine-grained rate control. Additionally, we propose network-wise calibration and channel-wise quantization strategies to minimize quantization-induced errors, arriving at a unified formula for representation-oriented PTQ calibration. Our experimental evaluations demonstrate that NeuroQuant significantly outperforms existing techniques in varying bitwidth quantization and compression efficiency, accelerating encoding by up to eight times and enabling quantization down to INT2 with minimal reconstruction loss. This work introduces variable-rate INR-VC for the first time and lays a theoretical foundation for future research in rate-distortion optimization, advancing the field of video coding technology. The materials will be available at https://github.com/Eric-qi/NeuroQuant. | Junqi Shi, Zhujia Chen, Hanfei Li, Qi Zhao, Ming Lu, Tong Chen, Zhan Ma |  |
| 315 |  |  [Can Watermarked LLMs be Identified by Users via Crafted Prompts?](https://openreview.net/forum?id=ujpAYpFDEA) |  | 0 | Text watermarking for Large Language Models (LLMs) has made significant progress in detecting LLM outputs and preventing misuse. Current watermarking techniques offer high detectability, minimal impact on text quality, and robustness to text editing. However, current researches lack investigation into the imperceptibility of watermarking techniques in LLM services. This is crucial as LLM providers may not want to disclose the presence of watermarks in real-world scenarios, as it could reduce user willingness to use the service and make watermarks more vulnerable to attacks. This work is the first to investigate the imperceptibility of watermarked LLMs. We design an identification algorithm called Water-Probe that detects watermarks through well-designed prompts to the LLM. Our key motivation is that current watermarked LLMs expose consistent biases under the same watermark key, resulting in similar differences across prompts under different watermark keys. Experiments show that almost all mainstream watermarking algorithms are easily identified with our well-designed prompts, while Water-Probe demonstrates a minimal false positive rate for non-watermarked LLMs. Finally, we propose that the key to enhancing the imperceptibility of watermarked LLMs is to increase the randomness of watermark key selection. Based on this, we introduce the Water-Bag strategy, which significantly improves watermark imperceptibility by merging multiple watermark keys. | Aiwei Liu, Sheng Guan, Yiming Liu, Leyi Pan, Yifei Zhang, Liancheng Fang, Lijie Wen, Philip S. Yu, Xuming Hu |  |
| 316 |  |  [Temporal Heterogeneous Graph Generation with Privacy, Utility, and Efficiency](https://openreview.net/forum?id=tj5xJInWty) |  | 0 | Nowadays, temporal heterogeneous graphs attract much research and industrial attention for building the next-generation Relational Deep Learning models and applications, due to their informative structures and features. While providing timely and precise services like personalized recommendations and question answering, this rich information also introduces extra exposure risk for each node in the graph. The distinctive local topology, the abundant heterogeneous features, and the time dimension of the graph data are more prone to expose sensitive information and narrow down the scope of victim candidates, which calls for well-defined protection techniques on graphs. To this end, we propose a Temporal Heterogeneous Graph Generator balancing Privacy, Utility, and Efficiency, named THePUff. More specifically, we first propose a differential privacy algorithm to perturb the input temporal heterogeneous graph for protecting privacy, and then utilize both the perturbed graph and the original one in a generative adversarial setting for THePUff to learn and generate privacy-guaranteed and utility-preserved graph data in an efficient manner. We further propose 6 new metrics in the temporal setting to measure heterogeneous graph utility and privacy. Finally, based on temporal heterogeneous graph datasets with up to 1 million nodes and 20 million edges, the experiments show that THePUff generates utilizable temporal heterogeneous graphs with privacy protected, compared with state-of-the-art baselines. | Xinyu He, Dongqi Fu, Hanghang Tong, Ross Maciejewski, Jingrui He |  |
| 317 |  |  [Attention with Markov: A Curious Case of Single-layer Transformers](https://openreview.net/forum?id=SqZ0KY4qBD) |  | 0 | Attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages. To deepen our understanding of their sequential modeling capabilities, there is a growing interest in using Markov input processes to study them. A key finding is that when trained on first-order Markov chains, transformers with two or more layers consistently develop an induction head mechanism to estimate the in-context bigram conditional distribution. In contrast, single-layer transformers, unable to form an induction head, directly learn the Markov kernel but often face a surprising challenge: they become trapped in local minima representing the unigram distribution, whereas deeper models reliably converge to the ground-truth bigram. While single-layer transformers can theoretically model first-order Markov chains, their empirical failure to learn this simple kernel in practice remains a curious phenomenon. To explain this contrasting behavior of single-layer models, in this paper we introduce a new framework for a principled analysis of transformers via Markov chains. Leveraging our framework, we theoretically characterize the loss landscape of single-layer transformers and show the existence of global minima (bigram) and bad local minima (unigram) contingent on data properties and model architecture. We precisely delineate the regimes under which these local optima occur. Backed by experiments, we demonstrate that our theoretical findings are in congruence with the empirical results. Finally, we outline several open problems in this arena. | Ashok Vardhan Makkuva, Marco Bondaschi, Adway Girish, Alliot Nagle, Martin Jaggi, Hyeji Kim, Michael Gastpar |  |
| 318 |  |  [The Computational Complexity of Circuit Discovery for Inner Interpretability](https://openreview.net/forum?id=QogcGNXJVw) |  | 0 | Many proposed applications of neural networks in machine learning, cognitive/brain science, and society hinge on the feasibility of inner interpretability via circuit discovery. This calls for empirical and theoretical explorations of viable algorithmic options. Despite advances in the design and testing of heuristics, there are concerns about their scalability and faithfulness at a time when we lack understanding of the complexity properties of the problems they are deployed to solve. To address this, we study circuit discovery with classical and parameterized computational complexity theory: (1) we describe a conceptual scaffolding to reason about circuit finding queries in terms of affordances for description, explanation, prediction and control; (2) we formalize a comprehensive set of queries for mechanistic explanation, and propose a formal framework for their analysis; (3) we use it to settle the complexity of many query variants and relaxations of practical interest on multi-layer perceptrons. Our findings reveal a challenging complexity landscape. Many queries are intractable, remain fixed-parameter intractable relative to model/circuit features, and inapproximable under additive, multiplicative, and probabilistic approximation schemes. To navigate this landscape, we prove there exist transformations to tackle some of these hard problems with better-understood heuristics, and prove the tractability or fixed-parameter tractability of more modest queries which retain useful affordances. This framework allows us to understand the scope and limits of interpretability queries, explore viable options, and compare their resource demands on existing and future architectures. | Federico Adolfi, Martina G. Vilas, Todd Wareham |  |
| 319 |  |  [Topological Schrödinger Bridge Matching](https://openreview.net/forum?id=WzCEiBILHu) |  | 0 | Given two boundary distributions, the \emph{Schrödinger Bridge} (SB) problem seeks the “most likely” random evolution between them with respect to a reference process. It has revealed rich connections to recent machine learning methods for generative modeling and distribution matching. While these methods perform well in Euclidean domains, they are not directly applicable to topological domains such as graphs and simplicial complexes, which are crucial for data defined over network entities, such as node signals and edge flows. In this work, we propose the \emph{Topological Schrödinger Bridge problem} ($\mathcal{T}$SBP) for matching signal distributions on a topological domain. We set the reference process to follow some linear tractable \emph{topology-aware} stochastic dynamics such as topological heat diffusion. For the case of Gaussian boundary distributions, we derive a \emph{closed-form} topological SB ($\mathcal{T}$SB) in terms of its time-marginal and stochastic differential. In the general case, leveraging the well-known result, we show that the optimal process follows the forward-backward topological dynamics governed by some unknowns. Building on these results, we develop $\mathcal{T}$SB-based models for matching topological signals by parameterizing the unknowns in the optimal process as \emph{(topological) neural networks} and learning them through \emph{likelihood training}. We validate the theoretical results and demonstrate the practical applications of $\mathcal{T}$SB-based models on both synthetic and real-world networks, emphasizing the role of topology. Additionally, we discuss the connections of $\mathcal{T}$SB-based models to other emerging models, and outline future directions for topological signal matching. | Maosheng Yang |  |
| 320 |  |  [ThunderKittens: Simple, Fast, and Adorable Kernels](https://openreview.net/forum?id=0fJfVOSUra) |  | 0 | The challenge of mapping AI architectures to GPU hardware is creating a critical bottleneck in AI progress. Despite substantial efforts, hand-written custom kernels fail to meet their theoretical performance thresholds, even on well-established operations like linear attention. The diverse capabilities of GPUs suggests we might we need a wide variety of techniques to achieve high performance. However, our work explores if a small number of key abstractions can drastically simplify the process. We present ThunderKittens (TK), a framework for writing performant AI kernels while remaining easy to use. Our abstractions map to the three levels of the GPU hierarchy: (1) at the warp-level, we provide 16x16 matrix tiles as basic data structures and PyTorch-like operations, (2) at the thread-block level, we provide templates for asynchronously overlapping operations, and (3) at the grid-level, TK helps hide block launch, tear-down, and memory costs. We show the value of TK by providing simple & diverse kernels that match or outperform prior art. We match CuBLAS and FlashAttention-3 on GEMM and attention inference performance and outperform the strongest baselines by $10-40$\% on attention backwards, $8\times$ on state space models, and $14\times$ on linear attention. | Benjamin Frederick Spector, Simran Arora, Aaryan Singhal, Arjun Parthasarathy, Daniel Y. Fu, Christopher Ré |  |
| 321 |  |  [gRNAde: Geometric Deep Learning for 3D RNA inverse design](https://openreview.net/forum?id=lvw3UgeVxS) |  | 0 | Computational RNA design tasks are often posed as inverse problems, where sequences are designed based on adopting a single desired secondary structure without considering 3D conformational diversity. We introduce gRNAde, a geometric RNA design pipeline operating on 3D RNA backbones to design sequences that explicitly account for structure and dynamics. gRNAde uses a multi-state Graph Neural Network and autoregressive decoding to generates candidate RNA sequences conditioned on one or more 3D backbone structures where the identities of the bases are unknown. On a single-state fixed backbone re-design benchmark of 14 RNA structures from the PDB identified by Das et al. (2010), gRNAde obtains higher native sequence recovery rates (56% on average) compared to Rosetta (45% on average), taking under a second to produce designs compared to the reported hours for Rosetta. We further demonstrate the utility of gRNAde on a new benchmark of multi-state design for structurally flexible RNAs, as well as zero-shot ranking of mutational fitness landscapes in a retrospective analysis of a recent ribozyme. Experimental wet lab validation on 10 different structured RNA backbones finds that gRNAde has a success rate of 50% at designing pseudoknotted RNA structures, a significant advance over 35% for Rosetta. Open source code and tutorials are available at: github.com/chaitjo/geometric-rna-design | Chaitanya K. Joshi, Arian Rokkum Jamasb, Ramón Viñas Torné, Charles Harris, Simon V. Mathis, Alex Morehead, Rishabh Anand, Pietro Lio |  |
| 322 |  |  [A Second-Order Perspective on Model Compositionality and Incremental Learning](https://openreview.net/forum?id=OZVTqoli2N) |  | 0 | The fine-tuning of deep pre-trained models has revealed compositional properties, with multiple specialized modules that can be arbitrarily composed into a single, multi-task model. However, identifying the conditions that promote compositionality remains an open issue, with recent efforts concentrating mainly on linearized networks. We conduct a theoretical study that attempts to demystify compositionality in standard non-linear networks through the second-order Taylor approximation of the loss function. The proposed formulation highlights the importance of staying within the pre-training basin to achieve composable modules. Moreover, it provides the basis for two dual incremental training algorithms: the one from the perspective of multiple models trained individually, while the other aims to optimize the composed model as a whole. We probe their application in incremental classification tasks and highlight some valuable skills. In fact, the pool of incrementally learned modules not only supports the creation of an effective multi-task model but also enables unlearning and specialization in certain tasks. Code available at <https://github.com/aimagelab/mammoth> | Angelo Porrello, Lorenzo Bonicelli, Pietro Buzzega, Monica Millunzi, Simone Calderara, Rita Cucchiara |  |
| 323 |  |  [Lean-STaR: Learning to Interleave Thinking and Proving](https://openreview.net/forum?id=SOWZ59UyNc) |  | 0 | Traditional language model-based theorem proving assumes that by training on a sufficient amount of formal proof data, a model will learn to prove theorems. Our key observation is that a wealth of informal information that is not present in formal proofs can be useful for learning to prove theorems. For instance, humans think through steps of a proof, but this thought process is not visible in the resulting code. We present Lean-STaR, a framework for training language models to produce informal thoughts prior to each step of a proof, thereby boosting the model's theorem-proving capabilities. Lean-STaR uses retrospective ground-truth tactics to generate synthetic thoughts for training the language model. At inference time, the trained model directly generates the thoughts prior to the prediction of the tactics in each proof step. Building on the self-taught reasoner framework, we then apply expert iteration to further fine-tune the model on the correct proofs it samples and verifies using the Lean solver. Lean-STaR significantly outperform base models (43.4% → 46.3%, Pass@64). We also analyze the impact of the augmented thoughts on various aspects of the theorem proving process, providing insights into their effectiveness. | Haohan Lin, Zhiqing Sun, Sean Welleck, Yiming Yang |  |
| 324 |  |  [Spectral Compressive Imaging via Unmixing-driven Subspace Diffusion Refinement](https://openreview.net/forum?id=Q150eWkQ4I) |  | 0 | Spectral Compressive Imaging (SCI) reconstruction is inherently ill-posed because a single observation admits multiple plausible reconstructions. Traditional deterministic methods struggle to effectively recover high-frequency details. Although diffusion models offer promising solutions to this challenge, their application is constrained by the limited training data and high computational demands associated with multispectral images (MSIs), making direct diffusion training impractical. To address these issues, we propose a novel Predict-and-unmixing-driven-Subspace-Refine framework (PSR-SCI). This framework begins with a light-weight predictor that produces an initial, rough estimate of the MSI. Subsequently, we introduce a unmixing-driven reversible spectral embedding module that decomposes the MSI into subspace images and spectral coefficients. This compact representation facilitates the adaptation of pre-trained RGB diffusion models and focuses refinement processes on high-frequency details, thereby enabling efficient diffusion generation with minimal MSI data. Additionally, we design a high-dimensional guidance mechanism enforcing SCI consistency during sampling. The refined subspace image is then reconstructed back into an MSI using the reversible embedding, yielding the final MSI with full spectral resolution. Experimental results on the standard KAIST and zero-shot datasets NTIRE, ICVL, and Harvard show that PSR-SCI enhances overall visual quality and delivers PSNR and SSIM results competitive with state-of-the-art diffusion, transformer, and deep-unfolding baselines. This framework provides a robust alternative to traditional deterministic SCI reconstruction methods. Code and models are available at [https://github.com/SMARK2022/PSR-SCI](https://github.com/SMARK2022/PSR-SCI). | Haijin Zeng, Benteng Sun, Yongyong Chen, Jingyong Su, Yong Xu |  |
| 325 |  |  [CBQ: Cross-Block Quantization for Large Language Models](https://openreview.net/forum?id=eW4yh6HKz4) |  | 0 | Post-training quantization (PTQ) has played a pivotal role in compressing large language models (LLMs) at ultra-low costs. Although current PTQ methods have achieved promising results by addressing outliers and employing layer- or block-wise loss optimization techniques, they still suffer from significant performance degradation at ultra-low bits precision. To dissect this issue, we conducted an in-depth analysis of quantization errors specific to LLMs and surprisingly discovered that, unlike traditional sources of quantization errors, the growing number of model parameters, combined with the reduction in quantization bits, intensifies inter-layer and intra-layer dependencies, which severely impact quantization accuracy. This finding highlights a critical challenge in quantizing LLMs. To address this, we propose CBQ, a cross-block reconstruction-based PTQ method for LLMs. CBQ leverages a cross-block dependency to establish long-range dependencies across multiple blocks and integrates an adaptive LoRA-Rounding technique to manage intra-layer dependencies. To further enhance performance, CBQ incorporates a coarse-to-fine pre-processing mechanism for processing weights and activations. Extensive experiments show that CBQ achieves superior low-bit quantization (W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across various LLMs and datasets. Notably, CBQ only takes 4.3 hours to quantize a weight-only quantization of a 4-bit LLAMA1-65B model, achieving a commendable trade off between performance and efficiency. | Xin Ding, Xiaoyu Liu, Zhijun Tu, Yun Zhang, Wei Li, Jie Hu, Hanting Chen, Yehui Tang, Zhiwei Xiong, Baoqun Yin, Yunhe Wang |  |
| 326 |  |  [Iterative Label Refinement Matters More than Preference Optimization under Weak Supervision](https://openreview.net/forum?id=q5EZ7gKcnW) |  | 0 | Language model (LM) post-training relies on two stages of human supervision: task demonstrations for supervised finetuning (SFT), followed by preference comparisons for reinforcement learning from human feedback (RLHF). As LMs become more capable, the tasks they are given become harder to supervise. Will post-training remain effective under unreliable supervision? To test this, we simulate unreliable demonstrations and comparison feedback using small LMs and time-constrained humans. We find that in the presence of unreliable supervision, SFT still retains some effectiveness, but DPO (a common RLHF algorithm) fails to improve the model beyond SFT. To address this, we propose \*iterative label refinement\* (ILR) as an alternative to RLHF. ILR improves the SFT data by using comparison feedback to decide whether human demonstrations should be replaced by model-generated alternatives, then retrains the model via SFT on the updated data. SFT+ILR outperforms SFT+DPO on several tasks with unreliable supervision (math, coding, and safe instruction-following). Our findings suggest that as LMs are used for complex tasks where human supervision is unreliable, RLHF may no longer be the best use of human comparison feedback; instead, it is better to direct feedback towards improving the training \*data\* rather than continually training the \*model\*. Our code and data are available at https://github.com/helloelwin/iterative-label-refinement. | Yaowen Ye, Cassidy Laidlaw, Jacob Steinhardt |  |
| 327 |  |  [uniINF: Best-of-Both-Worlds Algorithm for Parameter-Free Heavy-Tailed MABs](https://openreview.net/forum?id=2pNLknCTvG) |  | 0 | In this paper, we present a novel algorithm, \`uniINF\`, for the Heavy-Tailed Multi-Armed Bandits (HTMAB) problem, demonstrating robustness and adaptability in both stochastic and adversarial environments. Unlike the stochastic MAB setting where loss distributions are stationary with time, our study extends to the adversarial setup, where losses are generated from heavy-tailed distributions that depend on both arms and time. Our novel algorithm \`uniINF\` enjoys the so-called Best-of-Both-Worlds (BoBW) property, performing optimally in both stochastic and adversarial environments \*without\* knowing the exact environment type. Moreover, our algorithm also possesses a Parameter-Free feature, \*i.e.\*, it operates \*without\* the need of knowing the heavy-tail parameters $(\sigma, \alpha)$ a-priori. To be precise, \`uniINF\` ensures nearly-optimal regret in both stochastic and adversarial environments, matching the corresponding lower bounds when $(\sigma, \alpha)$ is known (up to logarithmic factors). To our knowledge, \`uniINF\` is the first parameter-free algorithm to achieve the BoBW property for the heavy-tailed MAB problem. Technically, we develop innovative techniques to achieve BoBW guarantees for Parameter-Free HTMABs, including a refined analysis for the dynamics of log-barrier, an auto-balancing learning rate scheduling scheme, an adaptive skipping-clipping loss tuning technique, and a stopping-time analysis for logarithmic regret. | Yu Chen, Jiatai Huang, Yan Dai, Longbo Huang |  |
| 328 |  |  [Exploring Local Memorization in Diffusion Models via Bright Ending Attention](https://openreview.net/forum?id=p4cLtzk4oe) |  | 0 | Text-to-image diffusion models have achieved unprecedented proficiency in generating realistic images. However, their inherent tendency to memorize and replicate training data during inference raises significant concerns, including potential copyright infringement. In response, various methods have been proposed to evaluate, detect, and mitigate memorization. Our analysis reveals that existing approaches significantly underperform in handling local memorization, where only specific image regions are memorized, compared to global memorization, where the entire image is replicated. Also, they cannot locate the local memorization regions, making it hard to investigate locally. To address these, we identify a novel "bright ending" (BE) anomaly in diffusion models prone to memorizing training images. BE refers to a distinct cross-attention pattern observed in text-to-image diffusion models, where memorized image patches exhibit significantly greater attention to the final text token during the last inference step than non-memorized patches. This pattern highlights regions where the generated image replicates training data and enables efficient localization of memorized regions. Equipped with this, we propose a simple yet effective method to integrate BE into existing frameworks, significantly improving their performance by narrowing the performance gap caused by local memorization. Our results not only validate the successful execution of the new localization task but also establish new state-of-the-art performance across all existing tasks, underscoring the significance of the BE phenomenon. | Chen Chen, Daochang Liu, Mubarak Shah, Chang Xu |  |
| 329 |  |  [EmbedLLM: Learning Compact Representations of Large Language Models](https://openreview.net/forum?id=Fs9EabmQrJ) |  | 0 | With hundreds of thousands of language models available on Huggingface today, efficiently evaluating and utilizing these models across various downstream tasks has become increasingly critical. Many existing methods repeatedly learn task-specific representations of Large Language Models (LLMs), which leads to inefficiencies in both time and computational resources. To address this, we propose EmbedLLM, a framework designed to learn compact vector representations of LLMs that facilitate downstream applications involving many models, such as model routing. We introduce an encoder-decoder approach for learning such embedding, along with a systematic framework to evaluate their effectiveness. Empirical results show that EmbedLLM outperforms prior methods in model routing. Additionally, we demonstrate that our method can forecast a model's performance on multiple benchmarks, without incurring additional inference cost. Extensive probing experiments validate that the learned embeddings capture key model characteristics, e.g. whether the model is specialized for coding tasks, even without being explicitly trained on them. We open source our dataset, code and embedder to facilitate further research and application. | Richard Zhuang, Tianhao Wu, Zhaojin Wen, Andrew Li, Jiantao Jiao, Kannan Ramchandran |  |
| 330 |  |  [InverseBench: Benchmarking Plug-and-Play Diffusion Priors for Inverse Problems in Physical Sciences](https://openreview.net/forum?id=U3PBITXNG6) |  | 0 | Plug-and-play diffusion priors (PnPDP) have emerged as a promising research direction for solving inverse problems. However, current studies primarily focus on natural image restoration, leaving the performance of these algorithms in scientific inverse problems largely unexplored. To address this gap, we introduce \textsc{InverseBench}, a framework that evaluates diffusion models across five distinct scientific inverse problems. These problems present unique structural challenges that differ from existing benchmarks, arising from critical scientific applications such as optical tomography, medical imaging, black hole imaging, seismology, and fluid dynamics. With \textsc{InverseBench}, we benchmark 14 inverse problem algorithms that use plug-and-play diffusion priors against strong, domain-specific baselines, offering valuable new insights into the strengths and weaknesses of existing algorithms. To facilitate further research and development, we open-source the codebase, along with datasets and pre-trained models, at [https://devzhk.github.io/InverseBench/](https://devzhk.github.io/InverseBench/). | Hongkai Zheng, Wenda Chu, Bingliang Zhang, Zihui Wu, Austin Wang, Berthy Feng, Caifeng Zou, Yu Sun, Nikola Borislavov Kovachki, Zachary E. Ross, Katherine L. Bouman, Yisong Yue |  |
| 331 |  |  [What Does It Mean to Be a Transformer? Insights from a Theoretical Hessian Analysis](https://openreview.net/forum?id=3ddi7Uss2A) |  | 0 | The Transformer architecture has inarguably revolutionized deep learning, overtaking classical architectures like multi-layer perceptions (MLPs) and convolutional neural networks (CNNs). At its core, the attention block differs in form and functionality from most other architectural components in deep learning—to the extent that, in comparison to MLPs/CNNs, Transformers are more often accompanied by adaptive optimizers, layer normalization, learning rate warmup, etc. The root causes behind these outward manifestations and the precise mechanisms that govern them remain poorly understood. In this work, we bridge this gap by providing a fundamental understanding of what distinguishes the Transformer from the other architectures—grounded in a theoretical comparison of the (loss) Hessian. Concretely, for a single self-attention layer, (a) we first entirely derive the Transformer’s Hessian and express it in matrix derivatives; (b) we then characterize it in terms of data, weight, and attention moment dependencies; and (c) while doing so further highlight the important structural differences to the Hessian of classical networks. Our results suggest that various common architectural and optimization choices in Transformers can be traced back to their highly non-linear dependencies on the data and weight matrices, which vary heterogeneously across parameters. Ultimately, our findings provide a deeper understanding of the Transformer’s unique optimization landscape and the challenges it poses. | Weronika Ormaniec, Felix Dangel, Sidak Pal Singh |  |
| 332 |  |  [Beyond Squared Error: Exploring Loss Design for Enhanced Training of Generative Flow Networks](https://openreview.net/forum?id=4NTrco82W0) |  | 0 | Generative Flow Networks (GFlowNets) are a novel class of generative models designed to sample from unnormalized distributions and have found applications in various important tasks, attracting great research interest in their training algorithms. In general, GFlowNets are trained by fitting the forward flow to the backward flow on sampled training objects. Prior work focused on the choice of training objects, parameterizations, sampling and resampling strategies, and backward policies, aiming to enhance credit assignment, exploration, or exploitation of the training process. However, the choice of regression loss, which can highly influence the exploration and exploitation behavior of the under-training policy, has been overlooked. Due to the lack of theoretical understanding for choosing an appropriate regression loss, most existing algorithms train the flow network by minimizing the squared error of the forward and backward flows in log-space, i.e., using the quadratic regression loss. In this work, we rigorously prove that distinct regression losses correspond to specific divergence measures, enabling us to design and analyze regression losses according to the desired properties of the corresponding divergence measures. Specifically, we examine two key properties: zero-forcing and zero-avoiding, where the former promotes exploitation and higher rewards, and the latter encourages exploration and enhances diversity. Based on our theoretical framework, we propose three novel regression losses, namely, Shifted-Cosh, Linex(1/2), and Linex(1). We evaluate them across three benchmarks: hyper-grid, bit-sequence generation, and molecule generation. Our proposed losses are compatible with most existing training algorithms, and significantly improve the performances of the algorithms concerning convergence speed, sample diversity, and robustness. | Rui Hu, Yifan Zhang, Zhuoran Li, Longbo Huang |  |
| 333 |  |  [Adam Exploits ℓ∞-geometry of Loss Landscape via Coordinate-wise Adaptivity](https://openreview.net/forum?id=PUnD86UEK5) |  | 0 | Adam outperforms SGD when training language models. Yet this advantage is not well-understood theoretically -- previous convergence analysis for Adam and SGD mainly focuses on the number of steps $T$ and is already minimax-optimal in non-convex cases, which are both $\widetilde{O}(T^{-1/4})$. In this work, we argue that the exploitation of nice $\ell_\infty$-geometry is the key advantage of Adam over SGD. More specifically, we give a new convergence analysis for Adam under novel assumptions that loss is smooth under $\ell_\infty$-geometry rather than the more common $\ell_2$-geometry, which yields a much better empirical smoothness constant for GPT-2 and ResNet models. Our experiments confirm that Adam performs much worse when the favorable $\ell_\infty$-geometry is changed while SGD provably remains unaffected. We also extend the convergence analysis to blockwise Adam under novel blockwise smoothness assumptions. | Shuo Xie, Mohamad Amin Mohamadi, Zhiyuan Li |  |
| 334 |  |  [CBGBench: Fill in the Blank of Protein-Molecule Complex Binding Graph](https://openreview.net/forum?id=mOpNrrV2zH) |  | 0 | Structure-based drug design (SBDD) aims to generate potential drugs that can bind to a target protein and is greatly expedited by the aid of AI techniques in generative models. However, a lack of systematic understanding persists due to the diverse settings, complex implementation, difficult reproducibility, and task singularity. Firstly, the absence of standardization can lead to unfair comparisons and inconclusive insights. To address this dilemma, we propose CBGBench, a comprehensive benchmark for SBDD, that unifies the task as a generative graph completion, analogous to fill-in-the-blank of the 3D complex binding graph. By categorizing existing methods based on their attributes, CBGBench facilitates a modular and extensible framework that implements cutting-edge methods. Secondly, a single de novo molecule generation task can hardly reflect their capabilities. To broaden the scope, we adapt these models to a range of tasks essential in drug design, considered sub-tasks within the graph fill-in-the-blank tasks. These tasks include the generative designation of de novo molecules, linkers, fragments, scaffolds, and sidechains, all conditioned on the structures of protein pockets. Our evaluations are conducted with fairness, encompassing comprehensive perspectives on interaction, chemical properties, geometry authenticity, and substructure validity. We further provide insights with analysis from empirical studies. Our results indicate that there is potential for further improvements on many tasks, with optimization in network architectures, and effective incorporation of chemical prior knowledge. Finally, to lower the barrier to entry and facilitate further developments in the field, we also provide a single [codebase](https://github.com/EDAPINENUT/CBGBench) that unifies the discussed models, data pre-processing, training, sampling, and evaluation. | Haitao Lin, Guojiang Zhao, Odin Zhang, Yufei Huang, Lirong Wu, Cheng Tan, Zicheng Liu, Zhifeng Gao, Stan Z. Li |  |
| 335 |  |  [Fair Clustering in the Sliding Window Model](https://openreview.net/forum?id=VGQugiuCQs) |  | 0 | We study streaming algorithms for proportionally fair clustering, a notion originally suggested by Chierichetti et al. (2017), in the sliding window model. We show that although there exist efficient streaming algorithms in the insertion-only model, surprisingly no algorithm can achieve finite ratio without violating the fairness constraint in sliding window. Hence, the problem of fair clustering is a rare separation between the insertion-only streaming model and the sliding window model. On the other hand, we show that if the fairness constraint is relaxed by a multiplicative $(1+\varepsilon)$ factor, there exists a $(1 + \varepsilon)$-approximate sliding window algorithm that uses $\text{poly}(k\varepsilon^{-1}\log n)$ space. This achieves essentially the best parameters (up to degree in the polynomial) provided the aforementioned lower bound. We also implement a number of empirical evaluations on real datasets to complement our theoretical results. | Vincent CohenAddad, Shaofeng H.C. Jiang, Qiaoyuan Yang, Yubo Zhang, Samson Zhou |  |
| 336 |  |  [NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models](https://openreview.net/forum?id=lgsyLSsDRe) |  | 0 | Decoder-only large language model (LLM)-based embedding models are beginning to outperform BERT or T5-based embedding models in general-purpose text embedding tasks, including dense vector-based retrieval. In this work, we introduce the NV-Embed model, incorporating architectural designs, training procedures, and curated datasets to significantly enhance the performance of LLM as a versatile embedding model, while maintaining its simplicity and reproducibility.For model architecture, we propose a latent attention layer to obtain pooled embeddings, which consistently improves retrieval and downstream task accuracy compared to mean pooling or using the last <EOS> token embedding from LLMs. To enhance representation learning, we remove the causal attention mask of LLMs during contrastive training. For training algorithm, we introduce a two-stage contrastive instruction-tuning method. It first applies contrastive training with instructions on retrieval datasets, utilizing in-batch negatives and curated hard negative examples. At stage-2, it blends various non-retrieval into instruction tuning, which not only enhances non-retrieval task accuracy but also improves retrieval performance. For training data, we utilize the hard-negative mining, synthetic data generation and existing public available datasets to boost the performance of embedding model. By combining these techniques, our NV-Embed- v1 model secured the No.1 position on the Massive Text Embedding Benchmark (MTEB) (as of May 24, 2024), across 56 embedding tasks. NV-Embed-v2 has reclaimed and maintained the top spot on MTEB since August 30, 2024, demonstrating the sustained effectiveness of the proposed methods over time. Also, it achieved the highest scores in the Long Doc section and the second-highest scores in the QA section of the AIR Benchmark, which covers a range of out-of-domain information retrieval topics beyond those in MTEB. We further provide the analysis of model compression techniques for generalist embedding models. We open-source the model at: https://huggingface.co/nvidia/NV-Embed-v2 . | Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping |  |
| 337 |  |  [Differentiation and Specialization of Attention Heads via the Refined Local Learning Coefficient](https://openreview.net/forum?id=SUc1UOWndp) |  | 0 | We introduce refined variants of the Local Learning Coefficient (LLC), a measure of model complexity grounded in singular learning theory, to study the development of internal structure in transformer language models during training. By applying these refined LLCs (rLLCs) to individual components of a two-layer attention-only transformer, we gain novel insights into the progressive differentiation and specialization of attention heads. Our methodology reveals how attention heads differentiate into distinct functional roles over the course of training, analyzes the types of data these heads specialize to process, and discovers a previously unidentified multigram circuit. These findings demonstrate that rLLCs provide a principled, quantitative toolkit for developmental interpretability, which aims to understand models through their evolution across the learning process. This work advances the field of developmental interpretability by providing a mathematically rigorous approach to understanding neural networks through the lens of their learning process. More broadly, this work takes a step towards establishing the correspondence between data distributional structure, geometric properties of the loss landscape, learning dynamics, and emergent computational structures in neural networks. | George Wang, Jesse Hoogland, Stan van Wingerden, Zach Furman, Daniel Murfet |  |
| 338 |  |  [VLMaterial: Procedural Material Generation with Large Vision-Language Models](https://openreview.net/forum?id=wHebuIb6IH) |  | 0 | Procedural materials, represented as functional node graphs, are ubiquitous in computer graphics for photorealistic material appearance design. They allow users to perform intuitive and precise editing to achieve desired visual appearances. However, creating a procedural material given an input image requires professional knowledge and significant effort. In this work, we leverage the ability to convert procedural materials into standard Python programs and fine-tune a large pre-trained vision-language model (VLM) to generate such programs from input images. To enable effective fine-tuning, we also contribute an open-source procedural material dataset and propose to perform program-level augmentation by prompting another pre-trained large language model (LLM). Through extensive evaluation, we show that our method outperforms previous methods on both synthetic and real-world examples. | Beichen Li, Rundi Wu, Armando SolarLezama, Changxi Zheng, Liang Shi, Bernd Bickel, Wojciech Matusik |  |
| 339 |  |  [To Trust or Not to Trust? Enhancing Large Language Models' Situated Faithfulness to External Contexts](https://openreview.net/forum?id=K2jOacHUlO) |  | 0 | Large Language Models (LLMs) are often augmented with external contexts, such as those used in retrieval-augmented generation (RAG). However, these contexts can be inaccurate or intentionally misleading, leading to conflicts with the model’s internal knowledge. We argue that robust LLMs should demonstrate situated faithfulness, dynamically calibrating their trust in external information based on their confidence in the internal knowledge and the external context to resolve knowledge conflicts. To benchmark this capability, we evaluate LLMs across several QA datasets, including a newly created dataset featuring in-the-wild incorrect contexts sourced from Reddit posts. We show that when provided with both correct and incorrect contexts, both open-source and proprietary models tend to overly rely on external information, regardless of its factual accuracy. To enhance situated faithfulness, we propose two approaches: Self-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning (RCR). SCR enables models to self-access the confidence of external information relative to their own internal knowledge to produce the most accurate answer. RCR, in contrast, extracts explicit confidence signals from the LLM and determines the final answer using predefined rules. Our results show that for LLMs with strong reasoning capabilities, such as GPT-4o and GPT-4o mini, SCR outperforms RCR, achieving improvements of up to 24.2\% over a direct input augmentation baseline. Conversely, for a smaller model like Llama-3-8B, RCR outperforms SCR. Fine-tuning SCR with our proposed Confidence Reasoning Direct Preference Optimization (CR-DPO) method improves performance on both seen and unseen datasets, yielding an average improvement of 8.9\% on Llama-3-8B. In addition to quantitative results, we offer insights into the relative strengths of SCR and RCR. Our findings highlight promising avenues for improving situated faithfulness in LLMs. | Yukun Huang, Sanxing Chen, Hongyi Cai, Bhuwan Dhingra |  |
| 340 |  |  [Representative Guidance: Diffusion Model Sampling with Coherence](https://openreview.net/forum?id=gWgaypDBs8) |  | 0 | The diffusion sampling process faces a persistent challenge stemming from its incoherence, attributable to varying noise directions across different timesteps. Our Representative Guidance (RepG) offers a new perspective to address this issue by reformulating the sampling process with a coherent direction toward a representative target. From this perspective, classic classifier guidance reveals its drawback in lacking meaningful representative information, as the features it relies on are optimized for discrimination and tend to highlight only a narrow set of class-specific cues. This focus often sacrifices diversity and increases the risk of adversarial generation. In contrast, we leverage self-supervised representations as the coherent target and treat sampling as a downstream task—one that focuses on refining image details and correcting generation errors, rather than settling for oversimplified outputs. Our Representative Guidance achieves superior performance and demonstrates the potential of pre-trained self-supervised models in guiding diffusion sampling. Our findings show that RepG not only significantly improves vanilla diffusion sampling, but also surpasses state-of-the-art benchmarks when combined with classifier-free guidance. | AnhDung Dinh, Daochang Liu, Chang Xu |  |
| 341 |  |  [AIR-BENCH 2024: A Safety Benchmark based on Regulation and Policies Specified Risk Categories](https://openreview.net/forum?id=UVnD9Ze6mF) |  | 0 | Foundation models (FMs) provide societal benefits but also amplify risks. Governments, companies, and researchers have proposed regulatory frameworks, acceptable use policies, and safety benchmarks in response. However, existing public benchmarks often define safety categories based on previous literature, intuitions, or common sense, leading to disjointed sets of categories for risks specified in recent regulations and policies, which makes it challenging to evaluate and compare FMs across these benchmarks. To bridge this gap, we introduce AIR-BENCH 2024, the first AI safety benchmark aligned with emerging government regulations and company policies, following the regulation-based safety categories grounded in the AI Risks taxonomy, AIR 2024. AIR 2024 decomposes 8 government regulations and 16 company policies into a four-tiered safety taxonomy with 314 granular risk categories in the lowest tier. AIR-BENCH 2024 contains 5,694 diverse prompts spanning these categories, with manual curation and human auditing to ensure quality. We evaluate leading language models on AIR-BENCH 2024 uncovering insights into their alignment with specified safety concerns. By bridging the gap between public benchmarks and practical AI risks, AIR-BENCH 2024 provides a foundation for assessing model safety across jurisdictions, fostering the development of safer and more responsible AI systems. | Yi Zeng, Yu Yang, Andy Zhou, Jeffrey Ziwei Tan, Yuheng Tu, Yifan Mai, Kevin Klyman, Minzhou Pan, Ruoxi Jia, Dawn Song, Percy Liang, Bo Li |  |
| 342 |  |  [DARE the Extreme: Revisiting Delta-Parameter Pruning For Fine-Tuned Models](https://openreview.net/forum?id=avSocG0oFA) |  | 0 | Storing open-source fine-tuned models separately introduces redundancy and increases response times in applications utilizing multiple models. Delta-parameter pruning (DPP), particularly the random drop and rescale (DARE) method proposed by Yu et al., addresses this by pruning the majority of delta parameters—the differences between fine-tuned and pre-trained model weights—while typically maintaining minimal performance loss. However, DARE fails when either the pruning rate or the magnitude of the delta parameters is large. We highlight two key reasons for this failure: (1) an excessively large rescaling factor as pruning rates increase, and (2) high mean and variance in the delta parameters. To push DARE’s limits, we introduce DAREx (DARE the eXtreme), which features two algorithmic improvements: (1) DAREx-q, a rescaling factor modification that significantly boosts performance at high pruning rates (e.g., > 30% on COLA and SST2 for encoder models, with even greater gains in decoder models), and (2) DAREx-L2, which combines DARE with AdamR, an in-training method that applies appropriate delta regularization before DPP. We also demonstrate that DAREx-q can be seamlessly combined with vanilla parameter-efficient fine-tuning techniques like LoRA and can facilitate structural DPP. Additionally, we revisit the application of importance-based pruning techniques within DPP, demonstrating that they outperform random-based methods when delta parameters are large. Through this comprehensive study, we develop a pipeline for selecting the most appropriate DPP method under various practical scenarios. | Wenlong Deng, Yize Zhao, Vala Vakilian, Minghui Chen, Xiaoxiao Li, Christos Thrampoulidis |  |
| 343 |  |  [CREIMBO: Cross-Regional Ensemble Interactions in Multi-view Brain Observations](https://openreview.net/forum?id=28abpUEICJ) |  | 0 | Modern recordings of neural activity provide diverse observations of neurons across brain areas, behavioral conditions, and subjects; presenting an exciting opportunity to reveal the fundamentals of brain-wide dynamics. Current analysis methods, however, often fail to fully harness the richness of such data, as they provide either uninterpretable representations (e.g., via deep networks) or oversimplify models (e.g., by assuming stationary dynamics or analyzing each session independently). Here, instead of regarding asynchronous neural recordings that lack alignment in neural identity or brain areas as a limitation, we leverage these diverse views into the brain to learn a unified model of neural dynamics. Specifically, we assume that brain activity is driven by multiple hidden global sub-circuits. These sub-circuits represent global basis interactions between neural ensembles—functional groups of neurons—such that the time-varying decomposition of these sub-circuits defines how the ensembles' interactions evolve over time non-stationarily and non-linearly. We discover the neural ensembles underlying non-simultaneous observations, along with their non-stationary evolving interactions, with our new model, \*\*CREIMBO\*\* (\*\*C\*\*ross-\*\*R\*\*egional \*\*E\*\*nsemble \*\*I\*\*nteractions in \*\*M\*\*ulti-view \*\*B\*\*rain \*\*O\*\*bservations). CREIMBO identifies the hidden composition of per-session neural ensembles through novel graph-driven dictionary learning and models the ensemble dynamics on a low-dimensional manifold spanned by a sparse time-varying composition of the global sub-circuits. Thus, CREIMBO disentangles overlapping temporal neural processes while preserving interpretability due to the use of a shared underlying sub-circuit basis. Moreover, CREIMBO distinguishes session-specific computations from global (session-invariant) ones by identifying session covariates and variations in sub-circuit activations. We demonstrate CREIMBO's ability to recover true components in synthetic data, and uncover meaningful brain dynamics in human high-density electrode recordings, including cross-subject neural mechanisms as well as inter- vs. intra-region dynamical motifs. Furthermore, using mouse whole-brain recordings, we show CREIMBO's ability to discover dynamical interactions that capture task and behavioral variables and meaningfully align with the biological importance of the brain areas they represent. | Noga Mudrik, Ryan Ly, Oliver Rübel, Adam Shabti Charles |  |
| 344 |  |  [Learning vector fields of differential equations on manifolds with geometrically constrained operator-valued kernels](https://openreview.net/forum?id=OwpLQrpdwE) |  | 0 | We address the problem of learning ordinary differential equations (ODEs) on manifolds. Existing machine learning methods, particularly those using neural networks, often struggle with high computational demands. To overcome this issue, we introduce a geometrically constrained operator-valued kernel that allows us to represent vector fields on tangent bundles of smooth manifolds. The construction of the kernel imposes the geometric constraints that are estimated from the data and ensures the computational feasibility for learning high dimensional systems of ODEs. Once the vector fields are estimated, e.g., by the kernel ridge regression, we need an ODE solver that guarantees the solution to stay on (or close to) the manifold. To overcome this issue, we propose a geometry-preserving ODE solver that approximates the exponential maps corresponding to the ODE solutions. We deduce a theoretical error bound for the proposed solver that guarantees the approximate solutions to lie on the manifold in the limit of large data. We verify the effectiveness of the proposed approach on high-dimensional dynamical systems, including the cavity flow problem, the beating and travelling waves in Kuramoto-Sivashinsky equations, and the reaction-diffusion dynamics. | Daning Huang, Hanyang He, John Harlim, Yan Li |  |
| 345 |  |  [Budgeted Online Continual Learning by Adaptive Layer Freezing and Frequency-based Sampling](https://openreview.net/forum?id=dOAkHmsjRX) |  | 0 | The majority of online continual learning (CL) advocates single-epoch training and imposes restrictions on the size of replay memory. However, single-epoch training would incur a different amount of computations per CL algorithm, and the additional storage cost to store logit or model in addition to replay memory is largely ignored in calculating the storage budget. Arguing different computational and storage budgets hinder fair comparison among CL algorithms in practice, we propose to use floating point operations (FLOPs) and total memory size in Byte as a metric for computational and memory budgets, respectively, to compare and develop CL algorithms in the same ‘total resource budget.’ To improve a CL method in a limited total budget, we propose adaptive layer freezing that does not update the layers for less informative batches to reduce computational costs with a negligible loss of accuracy. In addition, we propose a memory retrieval method that allows the model to learn the same amount of knowledge as using random retrieval in fewer iterations. Empirical validations on the CIFAR-10/100, CLEAR-10/100, and ImageNet-1K datasets demonstrate that the proposed approach outperforms the state-of-the-art methods within the same total budget. Furthermore, we validate its effectiveness in the Multi-modal Concept incremental Learning setup with multimodal large language models, such as LLaVA-1.5-7B. Code is available at https://github.com/snumprlab/budgeted-cl. | Minhyuk Seo, Hyunseo Koh, Jonghyun Choi |  |
| 346 |  |  [Control-oriented Clustering of Visual Latent Representation](https://openreview.net/forum?id=pPQPQ7Yd58) |  | 0 | We initiate a study of the geometry of the visual representation space ---the information channel from the vision encoder to the action decoder--- in an image-based control pipeline learned from behavior cloning. Inspired by the phenomenon of \*neural collapse\* (NC) in image classification, we empirically demonstrate the prevalent emergence of a similar \*law of clustering\* in the visual representation space. Specifically, - In discrete image-based control (e.g., Lunar Lander), the visual representations cluster according to the natural discrete action labels; - In continuous image-based control (e.g., Planar Pushing and Block Stacking), the clustering emerges according to \`\`control-oriented'' classes that are based on (a) the relative pose between the object and the target in the input or (b) the relative pose of the object induced by expert actions in the output. Each of the classes corresponds to one relative pose orthant (REPO). Beyond empirical observation, we show such a law of clustering can be leveraged as an algorithmic tool to improve test-time performance when training a policy with limited expert demonstrations. Particularly, we pretrain the vision encoder using NC as a regularization to encourage control-oriented clustering of the visual features. Surprisingly, such an NC-pretrained vision encoder, when finetuned end-to-end with the action decoder, boosts the test-time performance by 10% to 35%. Real-world vision-based planar pushing experiments confirmed the surprising advantage of control-oriented visual representation pretraining. | Han Qi, Haocheng Yin, Heng Yang |  |
| 347 |  |  [When do GFlowNets learn the right distribution?](https://openreview.net/forum?id=9GsgCUJtic) |  | 0 | Generative Flow Networks (GFlowNets) are an emerging class of sampling methods for distributions over discrete and compositional objects, e.g., graphs. In spite of their remarkable success in problems such as drug discovery and phylogenetic inference, the question of when and whether GFlowNets learn to sample from the target distribution remains underexplored. To tackle this issue, we first assess the extent to which a violation of the detailed balance of the underlying flow network might hamper the correctness of GFlowNet's sampling distribution. In particular, we demonstrate that the impact of an imbalanced edge on the model's accuracy is influenced by the total amount of flow passing through it and, as a consequence, is unevenly distributed across the network. We also argue that, depending on the parameterization, imbalance may be inevitable. In this regard, we consider the problem of sampling from distributions over graphs with GFlowNets parameterized by graph neural networks (GNNs) and show that the representation limits of GNNs delineate which distributions these GFlowNets can approximate. Lastly, we address these limitations by proposing a theoretically sound and computationally tractable metric for assessing GFlowNets, experimentally showing it is a better proxy for correctness than popular evaluation protocols. | Tiago da Silva, Rodrigo Barreto Alves, Eliezer de Souza da Silva, Amauri H. Souza, Vikas Garg, Samuel Kaski, Diego Mesquita |  |
| 348 |  |  [R2-Guard: Robust Reasoning Enabled LLM Guardrail via Knowledge-Enhanced Logical Reasoning](https://openreview.net/forum?id=CkgKSqZbuC) |  | 0 | As large language models (LLMs) become increasingly prevalent across various applications, it is critical to establish safety guardrails to moderate input/output content of LLMs and ensure compliance with safety policies. Existing guardrail models, such as OpenAI Mod and LlamaGuard, treat various safety categories (e.g., self-harm, self-harm/instructions) independently and fail to explicitly capture the intercorrelations among them. This has led to limitations such as ineffectiveness due to inadequate training on long-tail data from correlated safety categories, susceptibility to jailbreaking attacks, and inflexibility regarding new safety categories. To address these limitations, we propose $R^2$-Guard, a robust reasoning enabled LLM guardrail via knowledge-enhanced logical reasoning. Specifically, $R^2$-Guard comprises two parts: data-driven guardrail models and reasoning components. The data-driven guardrail models provide unsafety probabilities of moderated content on different safety categories. We then encode safety knowledge among different categories as first-order logical rules and embed them into a probabilistic graphic model (PGM) based reasoning component. The unsafety probabilities of different categories from data-driven guardrail models are sent to the reasoning component for final inference. We employ two types of PGMs: Markov logic networks (MLNs) and probabilistic circuits (PCs), and optimize PCs to achieve precision-efficiency balance via improved graph structure. We also propose different methods to optimize the weights of knowledge. To further perform stress tests for guardrail models, we employ a pairwise construction method to construct a new safety benchmark TwinSafety, which features principled categories and presents new challenges for moderation. We show that $R^2$-Guard is effective even given unrepresentative categories or challenging jailbreaking prompts. We demonstrate the effectiveness of $R^2$-Guard by comparisons with eight strong guardrail models on six standard moderation datasets, and demonstrate the robustness of $R^2$-Guard against four SOTA jailbreaking attacks. $R^2$-Guard significantly surpasses SOTA method LlamaGuard by 12.6% on standard moderation datasets and by 59.9% against jailbreaking attacks. We further reveal that $R^2$-Guard can effectively adapt to safety category updates by simply editing the PGM reasoning graph. | Mintong Kang, Bo Li |  |
| 349 |  |  [NetFormer: An interpretable model for recovering dynamical connectivity in neuronal population dynamics](https://openreview.net/forum?id=bcTjW5kS4W) |  | 0 | Neuronal dynamics are highly nonlinear and nonstationary. Traditional methods for extracting the underlying network structure from neuronal activity recordings mainly concentrate on modeling static connectivity, without accounting for key nonstationary aspects of biological neural systems, such as ongoing synaptic plasticity and neuronal modulation. To bridge this gap, we introduce the NetFormer model, an interpretable approach applicable to such systems. In NetFormer, the activity of each neuron across a series of historical time steps is defined as a token. These tokens are then linearly mapped through a query and key mechanism to generate a state- (and hence time-) dependent attention matrix that directly encodes nonstationary connectivity structures. We analyze our formulation from the perspective of nonstationary and nonlinear networked dynamical systems, and show both via an analytical expansion and targeted simulations how it can approximate the underlying ground truth. Next, we demonstrate NetFormer's ability to model a key feature of biological networks, spike-timing-dependent plasticity, whereby connection strengths continually change in response to local activity patterns. We further demonstrate that NetFormer can capture task-induced connectivity patterns on activity generated by task-trained recurrent neural networks. Thus informed, we apply NetFormer to a multi-modal dataset of real neural recordings, which contains neural activity, cell type, and behavioral state information. We show that the NetFormer effectively predicts neural dynamics and identifies cell-type specific, state-dependent dynamic connectivity that matches patterns measured in separate ground-truth physiology experiments, demonstrating its ability to help decode complex neural interactions based on population activity observations alone. | Ziyu Lu, Wuwei Zhang, Trung Le, Hao Wang, Uygar Sümbül, Eric Todd SheaBrown, Lu Mi |  |
| 350 |  |  [Revisiting Random Walks for Learning on Graphs](https://openreview.net/forum?id=SG1R2H3fa1) |  | 0 | We revisit a simple model class for machine learning on graphs, where a random walk on a graph produces a machine-readable record, and this record is processed by a deep neural network to directly make vertex-level or graph-level predictions. We call these stochastic machines random walk neural networks (RWNNs), and through principled analysis, show that we can design them to be isomorphism invariant while capable of universal approximation of graph functions in probability. A useful finding is that almost any kind of record of random walks guarantees probabilistic invariance as long as the vertices are anonymized. This enables us, for example, to record random walks in plain text and adopt a language model to read these text records to solve graph tasks. We further establish a parallelism to message passing neural networks using tools from Markov chain theory, and show that over-smoothing in message passing is alleviated by construction in RWNNs, while over-squashing manifests as probabilistic under-reaching. We empirically demonstrate RWNNs on a range of problems, verifying our theoretical analysis and demonstrating the use of language models for separating strongly regular graphs where 3-WL test fails, and transductive classification on arXiv citation network. Code is available at https://github.com/jw9730/random-walk. | Jinwoo Kim, Olga Zaghen, Ayhan Suleymanzade, Youngmin Ryou, Seunghoon Hong |  |
| 351 |  |  [DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured LLM Inference](https://openreview.net/forum?id=2c7pfOqu9k) |  | 0 | Large language models (LLMs) are increasingly employed for complex tasks that process multiple generation calls in a tree structure with shared prefixes of tokens, including few-shot prompting, multi-step reasoning, speculative decoding, etc. However, existing inference systems for tree-based applications are inefficient due to improper partitioning of queries and KV cache during attention calculation.This leads to two main issues: (1) a lack of memory access (IO) reuse for KV cache of shared prefixes, and (2) poor load balancing.As a result, there is redundant KV cache IO between GPU global memory and shared memory, along with low GPU utilization. To address these challenges, we propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient attention algorithm with prefix-aware and load-balanced KV cache partitions. DeFT reduces the number of read/write operations of KV cache during attention calculation through \*\*KV-Guided Grouping\*\*, a method that avoids repeatedly loading KV cache of shared prefixes in attention computation. Additionally, we propose \*\*Flattened Tree KV Splitting\*\*, a mechanism that ensures even distribution of the KV cache across partitions with little computation redundancy, enhancing GPU utilization during attention computations. By reducing 73-99% KV cache IO and nearly 100% IO for partial results during attention calculation, DeFT achieves up to 2.23/3.59$\times$ speedup in the end-to-end/attention latency across three practical tree-based workloads compared to state-of-the-art attention algorithms. Our code is available at https://github.com/LINs-lab/DeFT. | Jinwei Yao, Kaiqi Chen, Kexun Zhang, Jiaxuan You, Binhang Yuan, Zeke Wang, Tao Lin |  |
| 352 |  |  [How to Find the Exact Pareto Front for Multi-Objective MDPs?](https://openreview.net/forum?id=S4dItvpvAv) |  | 0 | Multi-Objective Markov Decision Processes (MO-MDPs) are receiving increasing attention, as real-world decision-making problems often involve conflicting objectives that cannot be addressed by a single-objective MDP. The Pareto front identifies the set of policies that cannot be dominated, providing a foundation for finding Pareto optimal solutions that can efficiently adapt to various preferences. However, finding the Pareto front is a highly challenging problem. Most existing methods either (i) rely on traversing the \*continuous preference space\*, which is impractical and results in approximations that are difficult to evaluate against the true Pareto front, or (ii) focus solely on deterministic Pareto optimal policies, from which there are no known techniques to characterize the full Pareto front. Moreover, finding the structure of the Pareto front itself remains unclear even in the context of dynamic programming, where the MDP is fully known in advance. In this work, we address the challenge of efficiently discovering the Pareto front, involving both deterministic and stochastic Pareto optimal policies. By investigating the geometric structure of the Pareto front in MO-MDPs, we uncover a key property: the Pareto front is on the boundary of a convex polytope whose vertices all correspond to deterministic policies, and neighboring vertices of the Pareto front differ by only one state-action pair of the deterministic policy, almost surely. This insight transforms the global comparison across all policies into a localized search among deterministic policies that differ by only one state-action pair, drastically reducing the complexity of searching for the exact Pareto front. We develop an efficient algorithm that identifies the vertices of the Pareto front by solving a single-objective MDP only once and then traversing the edges of the Pareto front, making it more efficient than existing methods. Furthermore, the entire Pareto front can be found in $V$ iterations, where $V$ represents the number of vertices on the Pareto front. Our empirical studies demonstrate the effectiveness of our theoretical strategy in discovering the Pareto front efficiently. | Yining Li, Peizhong Ju, Ness B. Shroff |  |
| 353 |  |  [Enhancing Learning with Label Differential Privacy by Vector Approximation](https://openreview.net/forum?id=IwPXYk6BV9) |  | 0 | Label differential privacy (DP) is a framework that protects the privacy of labels in training datasets, while the feature vectors are public. Existing approaches protect the privacy of labels by flipping them randomly, and then train a model to make the output approximate the privatized label. However, as the number of classes K increases, stronger randomization is needed, thus the performances of these methods become significantly worse. In this paper, we propose a vector approximation approach for learning with label local differential privacy, which is easy to implement and introduces little additional computational overhead. Instead of flipping each label into a single scalar, our method converts each label into a random vector with K components, whose expectations reflect class conditional probabilities. Intuitively, vector approximation retains more information than scalar labels. A brief theoretical analysis shows that the performance of our method only decays slightly with K. Finally, we conduct experiments on both synthesized and real datasets, which validate our theoretical analysis as well as the practical performance of our method. | Puning Zhao, Jiafei Wu, Zhe Liu, Li Shen, Zhikun Zhang, Rongfei Fan, Le Sun, Qingming Li |  |
| 354 |  |  [VisualPredicator: Learning Abstract World Models with Neuro-Symbolic Predicates for Robot Planning](https://openreview.net/forum?id=QOfswj7hij) |  | 0 | Broadly intelligent agents should form task-specific abstractions that selectively expose the essential elements of a task, while abstracting away the complexity of the raw sensorimotor space. In this work, we present Neuro-Symbolic Predicates, a first-order abstraction language that combines the strengths of symbolic and neural knowledge representations. We outline an online algorithm for inventing such predicates and learning abstract world models. We compare our approach to hierarchical reinforcement learning, vision-language model planning, and symbolic predicate invention approaches, on both in- and out-of-distribution tasks across five simulated robotic domains. Results show that our approach offers better sample complexity, stronger out-of-distribution generalization, and improved interpretability. | Yichao Liang, Nishanth Kumar, Hao Tang, Adrian Weller, Joshua B. Tenenbaum, Tom Silver, João F. Henriques, Kevin Ellis |  |
| 355 |  |  [Multi-Draft Speculative Sampling: Canonical Decomposition and Theoretical Limits](https://openreview.net/forum?id=N1L5TgtkAw) |  | 0 | We consider multi-draft speculative sampling, where the proposal sequences are sampled independently from different draft models. At each step, a token-level draft selection scheme takes a list of valid tokens as input and produces an output token whose distribution matches that of the target model. Previous works have demonstrated that the optimal scheme (which maximizes the probability of accepting one of the input tokens) can be cast as a solution to a linear program. In this work we show that the optimal scheme can be decomposed into a two-step solution: in the first step an importance sampling (IS) type scheme is used to select one intermediate token; in the second step (single-draft) speculative sampling is applied to generate the output token. For the case of two identical draft models we further 1) establish a necessary and sufficient condition on the distributions of the target and draft models for the acceptance probability to equal one and 2) provide an explicit expression for the optimal acceptance probability. Our theoretical analysis also motives a new class of token-level selection schemes based on weighted importance sampling. Our experimental results demonstrate consistent improvements in the achievable block efficiency and token rates over baseline schemes in a number of scenarios. | Ashish J. Khisti, MohammadReza Ebrahimi, Hassan Dbouk, Arash Behboodi, Roland Memisevic, Christos Louizos |  |
| 356 |  |  [First-Person Fairness in Chatbots](https://openreview.net/forum?id=TlAdgeoDTo) |  | 0 | Evaluating chatbot fairness is crucial given their rapid proliferation, yet typical chatbot tasks (e.g., resume writing, entertainment) diverge from the institutional decision-making tasks (e.g., resume screening) which have traditionally been central to discussion of algorithmic fairness. The open-ended nature and diverse use-cases of chatbots necessitate novel methods for bias assessment. This paper addresses these challenges by introducing a scalable counterfactual approach to evaluate "first-person fairness," meaning fairness toward chatbot users based on demographic characteristics. Our method employs a Language Model as a Research Assistant (LMRA) to yield quantitative measures of harmful stereotypes and qualitative analyses of demographic differences in chatbot responses. We apply this approach to assess biases in six of our language models across millions of interactions, covering sixty-six tasks in nine domains and spanning two genders and four races. Independent human annotations corroborate the LMRA-generated bias evaluations. This study represents the first large-scale fairness evaluation based on real-world chat data. We highlight that post-training reinforcement learning techniques significantly mitigate these biases. This evaluation provides a practical methodology for ongoing bias monitoring and mitigation. | Tyna Eloundou, Alex Beutel, David G. Robinson, Keren Gu, AnnaLuisa Brakman, Pamela Mishkin, Meghan Shah, Johannes Heidecke, Lilian Weng, Adam Tauman Kalai |  |
| 357 |  |  [Can Large Language Models Understand Symbolic Graphics Programs?](https://openreview.net/forum?id=Yk87CwhBDx) |  | 0 | Against the backdrop of enthusiasm for large language models (LLMs), there is a growing need to scientifically assess their capabilities and shortcomings. This is nontrivial in part because it is difficult to find tasks which the models have not encountered during training. Utilizing symbolic graphics programs, we propose a domain well-suited to test multiple spatial-semantic reasoning skills of LLMs. Popular in computer graphics, these programs procedurally generate visual data. While LLMs exhibit impressive skills in general program synthesis and analysis, symbolic graphics programs offer a new layer of evaluation: they allow us to test an LLM's ability to answer semantic questions about the images or 3D geometries without a vision encoder. To semantically understand the symbolic programs, LLMs would need to possess the ability to "imagine" and reason how the corresponding graphics content would look with only the symbolic description of the local curvatures and strokes. We use this task to evaluate LLMs by creating a large benchmark for the semantic visual understanding of symbolic graphics programs, built procedurally with minimal human effort. Particular emphasis is placed on transformations of images that leave the image level semantics invariant while introducing significant changes to the underlying program. We evaluate commercial and open-source LLMs on our benchmark to assess their ability to reason about visual output of programs, finding that LLMs considered stronger at reasoning generally perform better. Lastly, we introduce a novel method to improve this ability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned with pre-collected instruction data on symbolic graphics programs. Interestingly, we find that SIT not only improves LLM's understanding on symbolic programs, but it also improves general reasoning ability on various other benchmarks. | Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Z. Xiao, Katherine M. Collins, Joshua B. Tenenbaum, Adrian Weller, Michael J. Black, Bernhard Schölkopf |  |
| 358 |  |  [Token Statistics Transformer: Linear-Time Attention via Variational Rate Reduction](https://openreview.net/forum?id=lXRDQsiP2v) |  | 0 | The attention operator is arguably the key distinguishing factor of transformer architectures, which have demonstrated state-of-the-art performance on a variety of tasks. However, transformer attention operators often impose a significant computational burden, with the computational complexity scaling quadratically with the number of tokens. In this work, we propose a novel transformer attention operator whose computational complexity scales linearly with the number of tokens. We derive our network architecture by extending prior work which has shown that a transformer style architecture naturally arises by "white-box" architecture design, where each layer of the network is designed to implement an incremental optimization step of a maximal coding rate reduction objective (MCR$^2$). Specifically, we derive a novel variational form of the MCR$^2$ objective and show that the architecture that results from unrolled gradient descent of this variational objective leads to a new attention module called Token Statistics Self-Attention ($\texttt{TSSA}$). $\texttt{TSSA}$ has $\textit{linear computational and memory complexity}$ and radically departs from the typical attention architecture that computes pairwise similarities between tokens. Experiments on vision, language, and long sequence tasks show that simply swapping $\texttt{TSSA}$ for standard self-attention, which we refer to as the Token Statistics Transformer ($\texttt{ToST}$), achieves competitive performance with conventional transformers while being significantly more computationally efficient and interpretable. Our results also somewhat call into question the conventional wisdom that pairwise similarity style attention mechanisms are critical to the success of transformer architectures. | Ziyang Wu, Tianjiao Ding, Yifu Lu, Druv Pai, Jingyuan Zhang, Weida Wang, Yaodong Yu, Yi Ma, Benjamin David Haeffele |  |
| 359 |  |  [Nonlinear Sequence Embedding by Monotone Variational Inequality](https://openreview.net/forum?id=U834XHJuqk) |  | 0 | In the wild, we often encounter collections of sequential data such as electrocardiograms, motion capture, genomes, and natural language, and sequences may be multichannel or symbolic with nonlinear dynamics. We introduce a method to learn low-dimensional representations of nonlinear sequence and time-series data without supervision which has provable recovery guarantees. The learned representation can be used for downstream machine-learning tasks such as clustering and classification. The method assumes that the observed sequences arise from a common domain, with each sequence following its own autoregressive model, and these models are related through low-rank regularization. We cast the problem as a convex matrix parameter recovery problem using monotone variational inequalities (VIs) and encode the common domain assumption via low-rank constraint across the learned representations, which can learn a subspace approximately spanning the entire domain as well as faithful representations for the dynamics of each individual sequence incorporating the domain information in totality. We show the competitive performance of our method on real-world time-series data with baselines and demonstrate its effectiveness for symbolic text modeling and RNA sequence clustering. | Jonathan Yuyang Zhou, Yao Xie |  |
| 360 |  |  [X-ALMA: Plug & Play Modules and Adaptive Rejection for Quality Translation at Scale](https://openreview.net/forum?id=csbf1p8xUq) |  | 0 | Large language models (LLMs) have achieved remarkable success across various NLP tasks with a focus on English due to English-centric pre-training and limited multilingual data. In this work, we focus on the problem of translation, and while some multilingual LLMs claim to support for hundreds of languages, models often fail to provide high-quality responses for mid- and low-resource languages, leading to imbalanced performance heavily skewed in favor of high-resource languages. We introduce \*\*X-ALMA\*\*, a model designed to ensure top-tier performance across 50 diverse languages, regardless of their resource levels. X-ALMA surpasses state-of-the-art open-source multilingual LLMs, such as Aya-101 and Aya-23, in every single translation direction on the FLORES-200 and WMT'23 test datasets according to COMET-22. This is achieved by plug-and-play language-specific module architecture to prevent language conflicts during training and a carefully designed training regimen with novel optimization methods to maximize the translation performance. After the final stage of training regimen, our proposed \*\*A\*\*daptive \*\*R\*\*ejection \*\*P\*\*reference \*\*O\*\*ptimization (\*\*ARPO\*\*) surpasses existing preference optimization methods in translation tasks. | Haoran Xu, Kenton Murray, Philipp Koehn, Hieu Hoang, Akiko Eriguchi, Huda Khayrallah |  |
| 361 |  |  [Not All LLM-Generated Data Are Equal: Rethinking Data Weighting in Text Classification](https://openreview.net/forum?id=oI5tZaWkF9) |  | 0 | Synthetic data augmentation via Large Language Models (LLMs) allows researchers to leverage additional training data, thus enhancing the performance of downstream tasks, especially when real-world data is scarce. However, the generated data can deviate from the real-world data, and this misalignment can bring about deficient results while applying the trained model to applications. Therefore, we proposed efficient weighted-loss approaches to align synthetic data with real-world distribution by emphasizing high-quality and diversified data generated by LLMs using merely a tiny amount of real-world data. We empirically assessed the effectiveness of our methods on multiple text classification tasks, and the results showed that leveraging our approaches on a BERT-level model robustly outperformed standard cross-entropy and other data weighting approaches, providing potential solutions to effectively leveraging synthetic data from any suitable data generator. | HsunYu Kuo, YinHsiang Liao, YuChieh Chao, WeiYun Ma, PuJen Cheng |  |
| 362 |  |  [Systems with Switching Causal Relations: A Meta-Causal Perspective](https://openreview.net/forum?id=J9VogDTa1W) |  | 0 | Most work on causality in machine learning assumes that causal relationships are driven by a constant underlying process. However, the flexibility of agents' actions or tipping points in the environmental process can change the qualitative dynamics of the system. As a result, new causal relationships may emerge, while existing ones change or disappear, resulting in an altered causal graph. To analyze these qualitative changes on the causal graph, we propose the concept of meta-causal states, which groups classical causal models into clusters based on equivalent qualitative behavior and consolidates specific mechanism parameterizations. We demonstrate how meta-causal states can be inferred from observed agent behavior, and discuss potential methods for disentangling these states from unlabeled data. Finally, we direct our analysis towards the application of a dynamical system, showing that meta-causal states can also emerge from inherent system dynamics, and thus constitute more than a context-dependent framework in which mechanisms emerge only as a result of external factors. | Moritz Willig, Tim Nelson Tobiasch, Florian Peter Busch, Jonas Seng, Devendra Singh Dhami, Kristian Kersting |  |
| 363 |  |  [Multi-Robot Motion Planning with Diffusion Models](https://openreview.net/forum?id=AUCYptvAf3) |  | 0 | Diffusion models have recently been successfully applied to a wide range of robotics applications for learning complex multi-modal behaviors from data. However, prior works have mostly been confined to single-robot and small-scale environments due to the high sample complexity of learning multi-robot diffusion models. In this paper, we propose a method for generating collision-free multi-robot trajectories that conform to underlying data distributions while using only single-robot data. Our algorithm, Multi-robot Multi-model planning Diffusion (MMD), does so by combining learned diffusion models with classical search-based techniques---generating data-driven motions under collision constraints. Scaling further, we show how to compose multiple diffusion models to plan in large environments where a single diffusion model fails to generalize well. We demonstrate the effectiveness of our approach in planning for dozens of robots in a variety of simulated scenarios motivated by logistics environments. | Yorai Shaoul, Itamar Mishani, Shivam Vats, Jiaoyang Li, Maxim Likhachev |  |
| 364 |  |  [Graph Neural Networks Can (Often) Count Substructures](https://openreview.net/forum?id=sZQRUrvLn4) |  | 0 | Message passing graph neural networks (GNNs) are known to have limited expressive power in their ability to distinguish some non-isomorphic graphs. Because of this, it is well known that they are unable to detect or count arbitrary graph substructures (i.e., solving the subgraph isomorphism problem), a task that is of great importance for several types of graph-structured data. However, we observe that GNNs are in fact able to count graph patterns quite accurately across several real-world graph datasets. Motivated by this observation, we provide an analysis of the subgraph-counting capabilities of GNNs beyond the worst case, deriving several sufficient conditions for GNNs to be able to count subgraphs and, more importantly, to be able to sample-efficiently learn to count subgraphs. Moreover, we develop novel dynamic programming algorithms for solving the subgraph isomorphism problem on restricted classes of pattern and target graphs, and show that message-passing GNNs can efficiently simulate these dynamic programs. Finally, we empirically validate that our sufficient conditions for GNNs to count subgraphs hold on many real-world datasets, providing a theoretically-grounded explanation to our motivating observations. | Paolo Pellizzoni, Till Hendrik Schulz, Karsten M. Borgwardt |  |
| 365 |  |  [Towards hyperparameter-free optimization with differential privacy](https://openreview.net/forum?id=2kGKsyhtvh) |  | 0 | Differential privacy (DP) is a privacy-preserving paradigm that protects the training data when training deep learning models. Critically, the performance of models is determined by the training hyperparameters, especially those of the learning rate schedule, thus requiring fine-grained hyperparameter tuning on the data. In practice, it is common to tune the learning rate hyperparameters through the grid search that (1) is computationally expensive as multiple runs are needed, and (2) increases the risk of data leakage as the selection of hyperparameters is data-dependent. In this work, we adapt the automatic learning rate schedule to DP optimization for any models and optimizers, so as to significantly mitigate or even eliminate the cost of hyperparameter tuning when applied together with automatic per-sample gradient clipping. Our hyperparameter-free DP optimization is almost as computationally efficient as the standard non-DP optimization, and achieves state-of-the-art DP performance on various language and vision tasks. | Ruixuan Liu, Zhiqi Bu |  |
| 366 |  |  [AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials](https://openreview.net/forum?id=EEgYUccwsV) |  | 0 | Graphical User Interface (GUI) agents hold great potential for automating complex tasks across diverse digital environments, from web applications to desktop software. However, the development of such agents is hindered by the lack of high-quality, multi-step trajectory data required for effective training. Existing approaches rely on expensive and labor-intensive human annotation, making them unsustainable at scale. To address this challenge, we propose AgentTrek, a scalable data synthesis pipeline that generates high-quality web agent trajectories by leveraging web tutorials. Our method automatically gathers tutorial-like texts from the internet, transforms them into task goals with step-by-step instructions, and employs a visual-language model (VLM) agent to simulate their execution in a real digital environment. A VLM-based evaluator ensures the correctness of the generated trajectories. We demonstrate that training GUI agents with these synthesized trajectories significantly improves their grounding and planning performance over the current models. Moreover, our approach is more cost-efficient compared to traditional human annotation methods. This work underscores the potential of guided replay with web tutorials as a viable strategy for large-scale GUI agent training, paving the way for more capable and autonomous digital agents. | Yiheng Xu, Dunjie Lu, Zhennan Shen, Junli Wang, Zekun Wang, Yuchen Mao, Caiming Xiong, Tao Yu |  |
| 367 |  |  [Bilinear MLPs enable weight-based mechanistic interpretability](https://openreview.net/forum?id=gI0kPklUKS) |  | 0 | A mechanistic understanding of how MLPs do computation in deep neural net- works remains elusive. Current interpretability work can extract features from hidden activations over an input dataset but generally cannot explain how MLP weights construct features. One challenge is that element-wise nonlinearities introduce higher-order interactions and make it difficult to trace computations through the MLP layer. In this paper, we analyze bilinear MLPs, a type of Gated Linear Unit (GLU) without any element-wise nonlinearity that neverthe- less achieves competitive performance. Bilinear MLPs can be fully expressed in terms of linear operations using a third-order tensor, allowing flexible analysis of the weights. Analyzing the spectra of bilinear MLP weights using eigendecom- position reveals interpretable low-rank structure across toy tasks, image classifi- cation, and language modeling. We use this understanding to craft adversarial examples, uncover overfitting, and identify small language model circuits directly from the weights alone. Our results demonstrate that bilinear layers serve as an interpretable drop-in replacement for current activation functions and that weight- based interpretability is viable for understanding deep-learning models. | Michael T. Pearce, Thomas Dooms, Alice Rigg, José Oramas, Lee Sharkey |  |
| 368 |  |  [Towards a Unified and Verified Understanding of Group-Operation Networks](https://openreview.net/forum?id=8xxEBAtD7y) |  | 0 | A recent line of work in mechanistic interpretability has focused on reverse-engineering the computation performed by neural networks trained on the binary operation of finite groups. We investigate the internals of one-hidden-layer neural networks trained on this task, revealing previously unidentified structure and producing a more complete description of such models in a step towards unifying the explanations of previous works (Chughtai et al., 2023; Stander et al., 2024). Notably, these models approximate equivariance in each input argument. We verify that our explanation applies to a large fraction of networks trained on this task by translating it into a compact proof of model performance, a quantitative evaluation of the extent to which we faithfully and concisely explain model internals. In the main text, we focus on the symmetric group S5. For models trained on this group, our explanation yields a guarantee of model accuracy that runs 3x faster than brute force and gives a >=95% accuracy bound for 45% of the models we trained. We were unable to obtain nontrivial non-vacuous accuracy bounds using only explanations from previous works. | Wilson Wu, Louis Jaburi, Jacob Drori, Jason Gross |  |
| 369 |  |  [Approximation algorithms for combinatorial optimization with predictions](https://openreview.net/forum?id=AEFVa6VMu1) |  | 0 | We initiate a systematic study of utilizing predictions to improve over approximation guarantees of classic algorithms, without increasing the running time. We propose a generic method for a wide class of optimization problems that ask to select a feasible subset of input items of minimal (or maximal) total weight. This gives simple (near-)linear-time algorithms for, e.g., Vertex Cover, Steiner Tree, Minimum Weight Perfect Matching, Knapsack, and Maximum Clique. Our algorithms produce an optimal solution when provided with perfect predictions and their approximation ratio smoothly degrades with increasing prediction error. With small enough prediction error we achieve approximation guarantees that are beyond the reach without predictions in given time bounds, as exemplified by the NP-hardness and APX-hardness of many of the above problems. Although we show our approach to be optimal for this class of problems as a whole, there is a potential for exploiting specific structural properties of individual problems to obtain improved bounds; we demonstrate this on the Steiner Tree problem. We conclude with an empirical evaluation of our approach. | Antonios Antoniadis, Marek Eliás, Adam Polak, Moritz Venzin |  |
| 370 |  |  [Bayesian Experimental Design Via Contrastive Diffusions](https://openreview.net/forum?id=h8yg0hT96f) |  | 0 | Bayesian Optimal Experimental Design (BOED) is a powerful tool to reduce the cost of running a sequence of experiments. When based on the Expected Information Gain (EIG), design optimization corresponds to the maximization of some intractable expected \*contrast\* between prior and posterior distributions. Scaling this maximization to high dimensional and complex settings has been an issue due to BOED inherent computational complexity. In this work, we introduce an \*pooled posterior\* distribution with cost-effective sampling properties and provide a tractable access to the EIG contrast maximization via a new EIG gradient expression. Diffusion-based samplers are used to compute the dynamics of the pooled posterior and ideas from bi-level optimization are leveraged to derive an efficient joint sampling-optimization loop, without resorting to lower bound approximations of the EIG. The resulting efficiency gain allows to extend BOED to the well-tested generative capabilities of diffusion models. By incorporating generative models into the BOED framework, we expand its scope and its use in scenarios that were previously impractical. Numerical experiments and comparison with state-of-the-art methods show the potential of the approach. | Jacopo Iollo, Christophe Heinkelé, Pierre Alliez, Florence Forbes |  |
| 371 |  |  [MorphoDiff: Cellular Morphology Painting with Diffusion Models](https://openreview.net/forum?id=PstM8YfhvI) |  | 0 | Understanding cellular responses to external stimuli is critical for parsing biological mechanisms and advancing therapeutic development. High-content image-based assays provide a cost-effective approach to examine cellular phenotypes induced by diverse interventions, which offers valuable insights into biological processes and cellular states. We introduce MorphoDiff, a generative pipeline to predict high-resolution cell morphological responses under different conditions based on perturbation encoding. To the best of our knowledge, MorphoDiff is the first framework capable of producing guided, high-resolution predictions of cell morphology that generalize across both chemical and genetic interventions. The model integrates perturbation embeddings as guiding signals within a 2D latent diffusion model. The comprehensive computational, biological, and visual validations across three open-source Cell Painting datasets show that MorphoDiff can generate high-fidelity images and produce meaningful biology signals under various interventions. We envision the model will facilitate efficient in silico exploration of perturbational landscapes towards more effective drug discovery studies. | Zeinab Navidi, Jun Ma, Esteban Miglietta, Le Liu, Anne E. Carpenter, Beth A. Cimini, Benjamin HaibeKains, Bo Wang |  |
| 372 |  |  [Uncertainty Modeling in Graph Neural Networks via Stochastic Differential Equations](https://openreview.net/forum?id=TYSQYx9vwd) |  | 0 | We propose a novel Stochastic Differential Equation (SDE) framework to address the problem of learning uncertainty-aware representations for graph-structured data. While Graph Neural Ordinary Differential Equations (GNODEs) have shown promise in learning node representations, they lack the ability to quantify uncertainty. To address this, we introduce Latent Graph Neural Stochastic Differential Equations (LGNSDE), which enhance GNODE by embedding randomness through a Bayesian prior-posterior mechanism for epistemic uncertainty and Brownian motion for aleatoric uncertainty. By leveraging the existence and uniqueness of solutions to graph-based SDEs, we prove that the variance of the latent space bounds the variance of model outputs, thereby providing theoretically sensible guarantees for the uncertainty estimates. Furthermore, we show mathematically that LGNSDEs are robust to small perturbations in the input, maintaining stability over time. Empirical results across several benchmarks demonstrate that our framework is competitive in out-of-distribution detection, robustness to noise perturbations, and active learning, underscoring the ability of LGNSDEs to quantify uncertainty reliably. | Richard Bergna, Sergio CalvoOrdoñez, Felix L. Opolka, Pietro Lio, José Miguel HernándezLobato |  |
| 373 |  |  [Simplifying Deep Temporal Difference Learning](https://openreview.net/forum?id=7IzeL0kflu) |  | 0 | $Q$-learning played a foundational role in the field reinforcement learning (RL). However, TD algorithms with off-policy data, such as $Q$-learning, or nonlinear function approximation like deep neural networks require several additional tricks to stabilise training, primarily a large replay buffer and target networks. Unfortunately, the delayed updating of frozen network parameters in the target network harms the sample efficiency and, similarly, the large replay buffer introduces memory and implementation overheads. In this paper, we investigate whether it is possible to accelerate and simplify off-policy TD training while maintaining its stability. Our key theoretical result demonstrates for the first time that regularisation techniques such as LayerNorm can yield provably convergent TD algorithms without the need for a target network or replay buffer, even with off-policy data. Empirically, we find that online, parallelised sampling enabled by vectorised environments stabilises training without the need for a large replay buffer. Motivated by these findings, we propose PQN, our simplified deep online $Q$-Learning algorithm. Surprisingly, this simple algorithm is competitive with more complex methods like: Rainbow in Atari, PPO-RNN in Craftax, QMix in Smax, and can be up to 50x faster than traditional DQN without sacrificing sample efficiency. In an era where PPO has become the go-to RL algorithm, PQN reestablishes off-policy $Q$-learning as a viable alternative. | Matteo Gallici, Mattie Fellows, Benjamin Ellis, Bartomeu Pou, Ivan Masmitja, Jakob Nicolaus Foerster, Mario Martin |  |
| 374 |  |  [The Superposition of Diffusion Models Using the Itô Density Estimator](https://openreview.net/forum?id=2o58Mbqkd2) |  | 0 | The Cambrian explosion of easily accessible pre-trained diffusion models suggests a demand for methods that combine multiple different pre-trained diffusion models without incurring the significant computational burden of re-training a larger combined model. In this paper, we cast the problem of combining multiple pre-trained diffusion models at the generation stage under a novel proposed framework termed superposition. Theoretically, we derive superposition from rigorous first principles stemming from the celebrated continuity equation and design two novel algorithms tailor-made for combining diffusion models in SuperDiff. SuperDiff leverages a new scalable Itô density estimator for the log likelihood of the diffusion SDE which incurs \*no additional overhead\* compared to the well-known Hutchinson's estimator needed for divergence calculations. We demonstrate that SuperDiff is scalable to large pre-trained diffusion models as superposition is performed \*solely through composition during inference\*, and also enjoys painless implementation as it combines different pre-trained vector fields through an automated re-weighting scheme. Notably, we show that SuperDiff is efficient during inference time, and mimics traditional composition operators such as the logical OR and the logical AND. We empirically demonstrate the utility of using SuperDiff for generating more diverse images on CIFAR-10, more faithful prompt conditioned image editing using Stable Diffusion, as well as improved conditional molecule generation and unconditional \*de novo\* structure design of proteins. https://github.com/necludov/super-diffusion | Marta Skreta, Lazar Atanackovic, Joey Bose, Alexander Tong, Kirill Neklyudov |  |
| 375 |  |  [Differentiable Integer Linear Programming](https://openreview.net/forum?id=FPfCUJTsCn) |  | 0 | Machine learning (ML) techniques have shown great potential in generating high-quality solutions for integer linear programs (ILPs). However, existing methods typically rely on a \*supervised learning\* paradigm, leading to (1) \*expensive training cost\* due to repeated invocations of traditional solvers to generate training labels, and (2) \*plausible yet infeasible solutions\* due to the misalignment between the training objective (minimizing prediction loss) and the inference objective (generating high-quality solutions). To tackle this challenge, we propose \*\*DiffILO\*\* (\*\*Diff\*\*erentiable \*\*I\*\*nteger \*\*L\*\*inear Programming \*\*O\*\*ptimization), an \*unsupervised learning paradigm for learning to solve ILPs\*. Specifically, through a novel probabilistic modeling, DiffILO reformulates ILPs---discrete and constrained optimization problems---into continuous, differentiable (almost everywhere), and unconstrained optimization problems. This reformulation enables DiffILO to simultaneously solve ILPs and train the model via straightforward gradient descent, providing two major advantages. First, it significantly reduces the training cost, as the training process does not need the aid of traditional solvers at all. Second, it facilitates the generation of feasible and high-quality solutions, as the model \*learns to solve ILPs\* in an end-to-end manner, thus aligning the training and inference objectives. Experiments on commonly used ILP datasets demonstrate that DiffILO not only achieves an average training speedup of $13.2$ times compared to supervised methods, but also outperforms them by generating heuristic solutions with significantly higher feasibility ratios and much better solution qualities. | Zijie Geng, Jie Wang, Xijun Li, Fangzhou Zhu, Jianye Hao, Bin Li, Feng Wu |  |
| 376 |  |  [Streaming Algorithms For ℓp Flows and ℓp Regression](https://openreview.net/forum?id=Kpjvm2mB0K) |  | 0 | We initiate the study of one-pass streaming algorithms for underdetermined $\ell_p$ linear regression problems of the form $$ \min_{\mathbf A\mathbf x = \mathbf b} \lVert\mathbf x\rVert_p \,, \qquad \text{where } \mathbf A \in \mathbb R^{n \times d} \text{ with } n \ll d \,, $$ which generalizes basis pursuit ($p = 1$) and least squares solutions to underdetermined linear systems ($p = 2$). We study the column-arrival streaming model, in which the columns of $\mathbf A$ are presented one by one in a stream. When $\mathbf A$ is the incidence matrix of a graph, this corresponds to an edge insertion graph stream, and the regression problem captures $\ell_p$ flows which includes transshipment ($p = 1$), electrical flows ($p = 2$), and max flow ($p = \infty$) on undirected graphs as special cases. Our goal is to design algorithms which use space much less than the entire stream, which has a length of $d$. For the task of estimating the cost of the $\ell_p$ regression problem for $p\in[2,\infty]$, we show a streaming algorithm which constructs a sparse instance supported on $\tilde O(\varepsilon^{-2}n)$ columns of $\mathbf A$ which approximates the cost up to a $(1\pm\varepsilon)$ factor, which corresponds to $\tilde O(\varepsilon^{-2}n^2)$ bits of space in general and an $\tilde O(\varepsilon^{-2}n)$ space semi-streaming algorithm for constructing $\ell_p$ flow sparsifiers on graphs. This extends to $p\in(1, 2)$ with $\tilde O(\varepsilon^{2}n^{q/2})$ columns, where $q$ is the H\"older conjugate exponent of $p$. For $p = 2$, we show that $\Omega(n^2)$ bits of space are required in general even for outputting a constant factor solution. For $p = 1$, we show that the cost cannot be estimated even to an $o(\sqrt n)$ factor in $\mathrm{poly}(n)$ space. On the other hand, if we are interested in outputting a solution $\mathbf x$, then we show that $(1+\varepsilon)$-approximations require $\Omega(d)$ space for $p > 1$, and in general, $\kappa$-approximations require $\tilde\Omega(d/\kappa^{2q})$ space for $p > 1$. We complement these lower bounds with the first sublinear space upper bounds for this problem, showing that we can output a $\kappa$-approximation using space only $\mathrm{poly}(n) \cdot \tilde O(d/\kappa^q)$ for $p > 1$, as well as a $\sqrt n$-approximation using $\mathrm{poly}(n, \log d)$ space for $p = 1$. | Amit Chakrabarti, Jeffrey Jiang, David P. Woodruff, Taisuke Yasuda |  |
| 377 |  |  [Generalized Principal-Agent Problem with a Learning Agent](https://openreview.net/forum?id=LqTz13JS2P) |  | 0 | Generalized principal-agent problems, including Stackelberg games, contract design, and Bayesian persuasion, are a class of economic problems where an agent best responds to a principal's committed strategy. We study repeated generalized principal-agent problems under the assumption that the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal. We reduce this problem to a one-shot generalized principal-agent problem where the agent approximately best responds. Using this reduction, we show that: (1) if the agent uses contextual no-regret learning algorithms with regret $\mathrm{Reg}(T)$, then the principal can guarantee utility at least $U^\* - \Theta\big(\sqrt{\tfrac{\mathrm{Reg}(T)}{T}}\big)$, where $U^\*$ is the principal's optimal utility in the classic model with a best-responding agent. (2) If the agent uses contextual no-swap-regret learning algorithms with swap-regret $\mathrm{SReg}(T)$, then the principal cannot obtain utility more than $U^\* + O(\frac{\mathrm{SReg(T)}}{T})$. But (3) if the agent uses mean-based learning algorithms (which can be no-regret but not no-swap-regret), then the principal can sometimes do significantly better than $U^\*$. These results not only refine previous results in Stackelberg games and contract design, but also lead to new results for Bayesian persuasion with a learning agent and all generalized principal-agent problems where the agent does not have private information. | Tao Lin, Yiling Chen |  |
| 378 |  |  [Targeted Attack Improves Protection against Unauthorized Diffusion Customization](https://openreview.net/forum?id=agHddsQhsL) |  | 0 | Diffusion models build a new milestone for image generation yet raising public concerns, for they can be fine-tuned on unauthorized images for customization. Protection based on adversarial attacks rises to encounter this unauthorized diffusion customization, by adding protective watermarks to images and poisoning diffusion models. However, current protection, leveraging untargeted attacks, does not appear to be effective enough. In this paper, we propose a simple yet effective improvement for the protection against unauthorized diffusion customization by introducing targeted attacks. We show that by carefully selecting the target, targeted attacks significantly outperform untargeted attacks in poisoning diffusion models and degrading the customization image quality. Extensive experiments validate the superiority of our method on two mainstream customization methods of diffusion models, compared to existing protections. To explain the surprising success of targeted attacks, we delve into the mechanism of attack-based protections and propose a hypothesis based on our observation, which enhances the comprehension of attack-based protections. To the best of our knowledge, we are the first to both reveal the vulnerability of diffusion models to targeted attacks and leverage targeted attacks to enhance protection against unauthorized diffusion customization. | Boyang Zheng, Chumeng Liang, Xiaoyu Wu |  |
| 379 |  |  [High-dimensional Analysis of Knowledge Distillation: Weak-to-Strong Generalization and Scaling Laws](https://openreview.net/forum?id=1xzqz73hvL) |  | 0 | A growing number of machine learning scenarios rely on knowledge distillation where one uses the output of a surrogate model as labels to supervise the training of a target model. In this work, we provide a sharp characterization of this process for ridgeless, high-dimensional regression, under two settings: \*(i)\* model shift, where the surrogate model is arbitrary, and \*(ii)\* distribution shift, where the surrogate model is the solution of empirical risk minimization with out-of-distribution data. In both cases, we characterize the precise risk of the target model through non-asymptotic bounds in terms of sample size and data distribution under mild conditions. As a consequence, we identify the form of the optimal surrogate model, which reveals the benefits and limitations of discarding weak features in a data-dependent fashion. In the context of weak-to-strong (W2S) generalization, this has the interpretation that \*(i)\* W2S training, with the surrogate as the weak model, can provably outperform training with strong labels under the same data budget, but \*(ii)\* it is unable to improve the data scaling law. We validate our results on numerical experiments both on ridgeless regression and on neural network architectures. | Muhammed Emrullah Ildiz, Halil Alperen Gozeten, Ege Onur Taga, Marco Mondelli, Samet Oymak |  |
| 380 |  |  [BlendRL: A Framework for Merging Symbolic and Neural Policy Learning](https://openreview.net/forum?id=60i0ksMAhd) |  | 0 | Humans can leverage both symbolic reasoning and intuitive responses. In contrast, reinforcement learning policies are typically encoded in either opaque systems like neural networks or symbolic systems that rely on predefined symbols and rules. This disjointed approach severely limits the agents’ capabilities, as they often lack either the flexible low-level reaction characteristic of neural agents or the interpretable reasoning of symbolic agents. To overcome this challenge, we introduce \*BlendRL\*, a neuro-symbolic RL framework that harmoniously integrates both paradigms. We empirically demonstrate that BlendRL agents outperform both neural and symbolic baselines in standard Atari environments, and showcase their robustness to environmental changes. Additionally, we analyze the interaction between neural and symbolic policies, illustrating how their hybrid use helps agents overcome each other's limitations. | Hikaru Shindo, Quentin Delfosse, Devendra Singh Dhami, Kristian Kersting |  |
| 381 |  |  [Uncovering Overfitting in Large Language Model Editing](https://openreview.net/forum?id=t8qcGXaepr) |  | 0 | Knowledge editing has been proposed as an effective method for updating and correcting the internal knowledge of Large Language Models (LLMs). However, existing editing methods often struggle with complex tasks, such as multi-hop reasoning. In this paper, we identify and investigate the phenomenon of Editing Overfit, where edited models assign disproportionately high probabilities to the edit target, hindering the generalization of new knowledge in complex scenarios. We attribute this issue to the current editing paradigm, which places excessive emphasis on the direct correspondence between the input prompt and the edit target for each edit sample. To further explore this issue, we introduce a new benchmark, EVOKE (EValuation of Editing Overfit in Knowledge Editing), along with fine-grained evaluation metrics. Through comprehensive experiments and analysis, we demonstrate that Editing Overfit is prevalent in current editing methods and that common overfitting mitigation strategies are ineffective in knowledge editing. To overcome this, inspired by LLMs’ knowledge recall mechanisms, we propose a new plug-and-play strategy called Learn the Inference (LTI), which introduce a Multi-stage Inference Constraint module to guide the edited models in recalling new knowledge similarly to how unedited LLMs leverage knowledge through in-context learning. Extensive experimental results across a wide range of tasks validate the effectiveness of LTI in mitigating Editing Overfit. | Mengqi Zhang, Xiaotian Ye, Qiang Liu, Shu Wu, Pengjie Ren, Zhumin Chen |  |
| 382 |  |  [Advantage-Guided Distillation for Preference Alignment in Small Language Models](https://openreview.net/forum?id=xsx3Fpo3UD) |  | 0 | Alignment techniques enable Large Language Models (LLMs) to generate outputs that align with human preferences and play a crucial role in their effectiveness. However, their impact often diminishes when applied to Small Language Models (SLMs), likely due to the limited capacity of these models. Instead of directly applying existing alignment techniques to SLMs, we propose to utilize a well-aligned teacher LLM to guide the alignment process for these models, thereby facilitating the transfer of the teacher's knowledge of human preferences to the student model. To achieve this, we first explore a straightforward approach, Dual-Constrained Knowledge Distillation (DCKD), that employs knowledge distillation with two KL-divergence constraints from the aligned teacher to the unaligned student. To further enhance the student's ability to distinguish between preferred and dispreferred responses, we then propose Advantage-Guided Distillation for Preference Alignment (ADPA), which leverages an advantage function from the aligned teacher to deliver more nuanced, distribution-level reward signals for the student's alignment. Our experimental results show that these two approaches appreciably improve the alignment of SLMs and narrow the performance gap with larger counterparts. Among them, ADPA demonstrates superior performance and achieves even greater effectiveness when integrated with DCKD. Our code is available at https://github.com/SLIT-AI/ADPA . | Shiping Gao, Fanqi Wan, Jiajian Guo, Xiaojun Quan, Qifan Wang |  |
| 383 |  |  [SVBench: A Benchmark with Temporal Multi-Turn Dialogues for Streaming Video Understanding](https://openreview.net/forum?id=Hz4BYVY8YM) |  | 0 | Despite the significant advancements of Large Vision-Language Models (LVLMs) on established benchmarks, there remains a notable gap in suitable evaluation regarding their applicability in the emerging domain of long-context streaming video understanding. Current benchmarks for video understanding typically emphasize isolated single-instance text inputs and fail to evaluate the capacity to sustain temporal reasoning throughout the entire duration of video streams. To address these limitations, we introduce SVBench, a pioneering benchmark with temporal multi-turn question-answering chains specifically designed to thoroughly assess the capabilities of streaming video understanding of current LVLMs. We design a semi-automated annotation pipeline to obtain 49,979 Question-Answer (QA) pairs of 1,353 streaming videos, which includes generating QA chains that represent a series of consecutive multi-turn dialogues over video segments and constructing temporal linkages between successive QA chains. Our experimental results, obtained from 14 models in dialogue and streaming evaluations, reveal that while the closed-source GPT-4o outperforms others, most open-source LVLMs struggle with long-context streaming video understanding. We also construct a StreamingChat model, which significantly outperforms open-source LVLMs on our SVBench and achieves comparable performance on diverse vision-language benchmarks. We expect SVBench to advance the research of streaming video understanding by providing a comprehensive and in-depth analysis of current LVLMs. Our benchmark and model can be accessed at https://yzy-bupt.github.io/SVBench. | Zhenyu Yang, Yuhang Hu, Zemin Du, Dizhan Xue, Shengsheng Qian, Jiahong Wu, Fan Yang, Weiming Dong, Changsheng Xu |  |
| 384 |  |  [Knowledge Distillation with Multi-granularity Mixture of Priors for Image Super-Resolution](https://openreview.net/forum?id=cWHonXThtM) |  | 0 | Knowledge distillation (KD) is a promising yet challenging model compression approach that transmits rich learning representations from robust but resource-demanding teacher models to efficient student models. Previous methods for image super-resolution (SR) are often tailored to specific teacher-student architectures, limiting their potential for improvement and hindering broader applications. This work presents a novel KD framework for SR models, the multi-granularity Mixture of Priors Knowledge Distillation (MiPKD), which can be universally applied to a wide range of architectures at both feature and block levels. The teacher’s knowledge is effectively integrated with the student's feature via the Feature Prior Mixer, and the reconstructed feature propagates dynamically in the training phase with the Block Prior Mixer. Extensive experiments illustrate the significance of the proposed MiPKD technique. | Simiao Li, Yun Zhang, Wei Li, Hanting Chen, Wenjia Wang, Bingyi Jing, Shaohui Lin, Jie Hu |  |
| 385 |  |  [Controlling Language and Diffusion Models by Transporting Activations](https://openreview.net/forum?id=l2zFn6TIQi) |  | 0 | The increasing capabilities of large generative models and their ever more widespread deployment have raised concerns about their reliability, safety, and potential misuse. To address these issues, recent works have proposed to control model generation by steering model activations in order to effectively induce or prevent the emergence of concepts or behaviors in the generated output. In this paper we introduce Activation Transport (AcT), a general framework to steer activations guided by optimal transport theory that generalizes many previous activation-steering works. AcT is modality-agnostic and provides fine-grained control over the model behavior with negligible computational overhead, while minimally impacting model abilities. We experimentally show the effectiveness and versatility of our approach by addressing key challenges in large language models (LLMs) and text-to-image diffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate toxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is, we show how AcT enables fine-grained style control and concept negation. | Pau Rodríguez, Arno Blaas, Michal Klein, Luca Zappella, Nicholas Apostoloff, Marco Cuturi, Xavier Suau |  |
| 386 |  |  [Efficient and Accurate Explanation Estimation with Distribution Compression](https://openreview.net/forum?id=LiUfN9h0Lx) |  | 0 | We discover a theoretical connection between explanation estimation and distribution compression that significantly improves the approximation of feature attributions, importance, and effects. While the exact computation of various machine learning explanations requires numerous model inferences and becomes impractical, the computational cost of approximation increases with an ever-increasing size of data and model parameters. We show that the standard i.i.d. sampling used in a broad spectrum of algorithms for post-hoc explanation leads to an approximation error worthy of improvement. To this end, we introduce Compress Then Explain (CTE), a new paradigm of sample-efficient explainability. It relies on distribution compression through kernel thinning to obtain a data sample that best approximates its marginal distribution. CTE significantly improves the accuracy and stability of explanation estimation with negligible computational overhead. It often achieves an on-par explanation approximation error 2-3x faster by using fewer samples, i.e. requiring 2-3x fewer model evaluations. CTE is a simple, yet powerful, plug-in for any explanation method that now relies on i.i.d. sampling. | Hubert Baniecki, Giuseppe Casalicchio, Bernd Bischl, Przemyslaw Biecek |  |
| 387 |  |  [Interleaved Scene Graphs for Interleaved Text-and-Image Generation Assessment](https://openreview.net/forum?id=rDLgnYLM5b) |  | 0 | Many real-world user queries (e.g. \*"How do to make egg fried rice?"\*) could benefit from systems capable of generating responses with both textual steps with accompanying images, similar to a cookbook. Models designed to generate interleaved text and images face challenges in ensuring consistency within and across these modalities. To address these challenges, we present ISG, a comprehensive evaluation framework for interleaved text-and-image generation. ISG leverages a scene graph structure to capture relationships between text and image blocks, evaluating responses on four levels of granularity: holistic, structural, block-level, and image-specific. This multi-tiered evaluation allows for a nuanced assessment of consistency, coherence, and accuracy, and provides interpretable question-answer feedback. In conjunction with ISG, we introduce a benchmark, ISG-Bench, encompassing 1,150 samples across 8 categories and 21 subcategories. This benchmark dataset includes complex language-vision dependencies and golden answers to evaluate models effectively on vision-centric tasks such as style transfer, a challenging area for current models. Using ISG-Bench, we demonstrate that recent unified vision-language models perform poorly on generating interleaved content. While compositional approaches that combine separate language and image models show a 111% improvement over unified models at the holistic level, their performance remains suboptimal at both block and image levels. To facilitate future work, we develop ISG-Agent, a baseline agent employing a \*"plan-execute-refine"\* pipeline to invoke tools, achieving a 122% performance improvement. | Dongping Chen, Ruoxi Chen, Shu Pu, Zhaoyi Liu, Yanru Wu, Caixi Chen, Benlin Liu, Yue Huang, Yao Wan, Pan Zhou, Ranjay Krishna |  |
| 388 |  |  [ODE-based Smoothing Neural Network for Reinforcement Learning Tasks](https://openreview.net/forum?id=S5Yo6w3n3f) |  | 0 | The smoothness of control actions is a significant challenge faced by deep reinforcement learning (RL) techniques in solving optimal control problems. Existing RL-trained policies tend to produce non-smooth actions due to high-frequency input noise and unconstrained Lipschitz constants in neural networks. This article presents a Smooth ODE (SmODE) network capable of simultaneously addressing both causes of unsmooth control actions, thereby enhancing policy performance and robustness under noise condition. We first design a smooth ODE neuron with first-order low-pass filtering expression, which can dynamically filter out high frequency noises of hidden state by a learnable state-based system time constant. Additionally, we construct a state-based mapping function, $g$, and theoretically demonstrate its capacity to control the ODE neuron's Lipschitz constant. Then, based on the above neuronal structure design, we further advanced the SmODE network serving as RL policy approximators. This network is compatible with most existing RL algorithms, offering improved adaptability compared to prior approaches. Various experiments show that our SmODE network demonstrates superior anti-interference capabilities and smoother action outputs than the multi-layer perception and smooth network architectures like LipsNet. | Yinuo Wang, Wenxuan Wang, Xujie Song, Tong Liu, Yuming Yin, Liangfa Chen, Likun Wang, Jingliang Duan, Shengbo Eben Li |  |
| 389 |  |  [Learning from End User Data with Shuffled Differential Privacy over Kernel Densities](https://openreview.net/forum?id=QjSOgxJ0hp) |  | 0 | We study a setting of collecting and learning from private data distributed across end users. In the shuffled model of differential privacy, the end users partially protect their data locally before sharing it, and their data is also anonymized during its collection to enhance privacy. This model has recently become a prominent alternative to central DP, which requires full trust in a central data curator, and local DP, where fully local data protection takes a steep toll on downstream accuracy. Our main technical result is a shuffled DP protocol for privately estimating the kernel density function of a distributed dataset, with accuracy essentially matching central DP. We use it to privately learn a classifier from the end user data, by learning a private density function per class. Moreover, we show that the density function itself can recover the semantic content of its class, despite having been learned in the absence of any unprotected data. Our experiments show the favorable downstream performance of our approach, and highlight key downstream considerations and trade-offs in a practical ML deployment of shuffled DP. | Tal Wagner |  |
| 390 |  |  [Biologically Constrained Barrel Cortex Model Integrates Whisker Inputs and Replicates Key Brain Network Dynamics](https://openreview.net/forum?id=UvfI4grcM7) |  | 0 | The brain's ability to transform sensory inputs into motor functions is central to neuroscience and crucial for the development of embodied intelligence. Sensory-motor integration involves complex neural circuits, diverse neuronal types, and intricate intercellular connections. Bridging the gap between biological realism and behavioral functionality presents a formidable challenge. In this study, we focus on the columnar structure of the superficial layers of mouse barrel cortex as a model system. We constructed a model comprising 4,218 neurons across 13 neuronal subtypes, with neural distribution and connection strengths constrained by anatomical experimental findings. A key innovation of our work is the development of an effective construction and training pipeline tailored for this biologically constrained model. Additionally, we converted an existing simulated whisker sweep dataset into a spiking-based format, enabling our network to be trained and tested on neural signals that more closely mimic those observed in biological systems. The results of object discrimination utilizing whisker signals demonstrate that our barrel cortex model, grounded in biological constraints, achieves a classification accuracy exceeds classical convolutional neural networks (CNNs), recurrent neural networks (RNNs), and long short-term memory networks (LSTMs), by an average of 8.6%, and is on par with recent spiking neural networks (SNNs) in performance. Interestingly, a whisker deprivation experiment, designed in accordance with neuroscience practices, further validates the perceptual capabilities of our model in behavioral tasks. Critically, it offers significant biological interpretability: post-training analysis reveals that neurons within our model exhibit firing characteristics and distribution patterns similar to those observed in the actual neuronal systems of the barrel cortex. This study advances our understanding of neural processing in the barrel cortex and exemplifies how integrating detailed biological structures into neural network models can enhance both scientific inquiry and artificial intelligence applications. The code is available at https://github.com/fun0515/RSNN_bfd. | Tianfang Zhu, Dongli Hu, Jiandong Zhou, Kai Du, Anan Li |  |
| 391 |  |  [FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in Conversational LLMs](https://openreview.net/forum?id=RSGoXnS9GH) |  | 0 | The increasing deployment of large language model (LLM)-based chatbots has raised concerns regarding fairness. Fairness issues in LLMs may result in serious consequences, such as bias amplification, discrimination, and harm to minority groups. Many efforts are dedicated to evaluating and mitigating biases in LLMs. However, existing fairness benchmarks mainly focus on single-turn dialogues, while multi-turn scenarios, which better reflect real-world conversations, pose greater challenges due to conversational complexity and risk for bias accumulation. In this paper, we introduce a comprehensive benchmark for fairness of LLMs in multi-turn scenarios, \*\*FairMT-Bench\*\*. Specifically, We propose a task taxonomy to evaluate fairness of LLMs cross three stages: context understanding, interaction fairness, and fairness trade-offs, each comprising two tasks. To ensure coverage of diverse bias types and attributes, our multi-turn dialogue dataset FairMT-10K is constructed by integrating data from established fairness benchmarks. For evaluation, we employ GPT-4 along with bias classifiers like Llama-Guard-3, and human annotators to ensure robustness. Our experiments and analysis on FairMT-10K reveal that in multi-turn dialogue scenarios, LLMs are more prone to generating biased responses, showing significant variation in performance across different tasks and models. Based on these findings, we develop a more challenging dataset, FairMT-1K, and test 15 current state-of-the-art (SOTA) LLMs on this dataset. The results highlight the current state of fairness in LLMs and demonstrate the value of this benchmark for evaluating fairness of LLMs in more realistic multi-turn dialogue contexts. This underscores the need for future works to enhance LLM fairness and incorporate FairMT-1K in such efforts. Our code and dataset are available at https://github.com/FanZT6/FairMT-bench. | Zhiting Fan, Ruizhe Chen, Tianxiang Hu, Zuozhu Liu |  |
| 392 |  |  [OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text](https://openreview.net/forum?id=kwqhn2VuG4) |  | 0 | Image-text interleaved data, consisting of multiple images and texts arranged in a natural document format, aligns with the presentation paradigm of internet data and closely resembles human reading habits. Recent studies have shown that such data aids multimodal in-context learning and maintains the capabilities of large language models during multimodal fine-tuning. However, the limited scale and diversity of current image-text interleaved data restrict the development of multimodal large language models. In this paper, we introduce OmniCorpus, a 10 billion-scale image-text interleaved dataset. Using an efficient data engine, we filter and extract large-scale high-quality documents, which contain 8.6 billion images and 1,696 billion text tokens. Compared to counterparts (e.g., MMC4, OBELICS), our dataset 1) has 15 times larger scales while maintaining good data quality; 2) features more diverse sources, including both English and non-English websites as well as video-centric websites; 3) is more flexible, easily degradable from an image-text interleaved format to pure text corpus and image-text pairs. Through comprehensive analysis and experiments, we validate the quality, usability, and effectiveness of the proposed dataset. We hope this could provide a solid data foundation for future multimodal model research. | Qingyun Li, Zhe Chen, Weiyun Wang, Wenhai Wang, Shenglong Ye, Zhenjiang Jin, Guanzhou Chen, Yinan He, Zhangwei Gao, Erfei Cui, Jiashuo Yu, Hao Tian, Jiasheng Zhou, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, et al. |  |
| 393 |  |  [MAGNet: Motif-Agnostic Generation of Molecules from Scaffolds](https://openreview.net/forum?id=5FXKgOxmb2) |  | 0 | Recent advances in machine learning for molecules exhibit great potential for facilitating drug discovery from in silico predictions. Most models for molecule generation rely on the decomposition of molecules into frequently occurring substructures (motifs), from which they generate novel compounds. While motif representations greatly aid in learning molecular distributions, such methods fail to represent substructures beyond their known motif set, posing a fundamental limitation for discovering novel compounds. To address this limitation and enhance structural expressivity, we propose to separate structure from features by abstracting motifs to scaffolds and, subsequently, allocating atom and bond types. To this end, we introduce a novel factorisation of the molecules' data distribution that considers the entire molecular context and facilitates learning adequate assignments of atoms and bonds to scaffolds. Complementary to this, we propose MAGNet, the first model to freely learn motifs. Importantly, we demonstrate that MAGNet's improved expressivity leads to molecules with more structural diversity and, at the same time, diverse atom and bond assignments. | Leon Hetzel, Johanna Sommer, Bastian Rieck, Fabian J. Theis, Stephan Günnemann |  |
| 394 |  |  [Broaden your SCOPE! Efficient Multi-turn Conversation Planning for LLMs with Semantic Space](https://openreview.net/forum?id=3cgMU3TyyE) |  | 0 | Large language models (LLMs) are used in chatbots or AI assistants to hold conversations with a human user. In such applications, the quality (e.g., user engagement, safety) of a conversation is important and can only be exactly known at the end of the conversation. To maximize its expected quality, conversation planning reasons about the stochastic transitions within a conversation to select the optimal LLM response at each turn. Existing simulation-based conversation planning algorithms typically select the optimal response by simulating future conversations with a large number of LLM queries at every turn. However, this process is extremely time-consuming and hence impractical for real-time conversations. This paper presents a novel approach called Semantic space COnversation Planning with improved Efficiency (SCOPE) that exploits the dense semantic representation of conversations to perform conversation planning efficiently. In particular, SCOPE models the stochastic transitions in conversation semantics and their associated rewards to plan entirely within the semantic space. This allows us to select the optimal LLM response at every conversation turn without needing additional LLM queries for simulation. As a result, SCOPE can perform conversation planning 70 times faster than conventional simulation-based planning algorithms when applied to a wide variety of conversation starters and two reward functions seen in the real world, yet achieving a higher reward within a practical planning budget. Our code can be found at: https://github.com/chenzhiliang94/convo-plan-SCOPE. | Zhiliang Chen, Xinyuan Niu, ChuanSheng Foo, Bryan Kian Hsiang Low |  |
| 395 |  |  [Spa-Bench: a comprehensive Benchmark for Smartphone Agent Evaluation](https://openreview.net/forum?id=OZbFRNhpwr) |  | 0 | Smartphone agents are increasingly important for helping users control devices efficiently, with (Multimodal) Large Language Model (MLLM)-based approaches emerging as key contenders. Fairly comparing these agents is essential but challenging, requiring a varied task scope, the integration of agents with different implementations, and a generalisable evaluation pipeline to assess their strengths and weaknesses. In this paper, we present SPA-Bench, a comprehensive SmartPhone Agent Benchmark designed to evaluate (M)LLM-based agents in an interactive environment that simulates real-world conditions. SPA-Bench offers three key contributions: (1) A diverse set of tasks covering system and third-party apps in both English and Chinese, focusing on features commonly used in daily routines; (2) A plug-and-play framework enabling real-time agent interaction with Android devices, integrating over ten agents with the flexibility to add more; (3) A novel evaluation pipeline that automatically assesses agent performance across multiple dimensions, encompassing seven metrics related to task completion and resource consumption. Our extensive experiments across tasks and agents reveal challenges like interpreting mobile user interfaces, action grounding, memory retention, and execution costs. We propose future research directions to ease these difficulties, moving closer to real-world smartphone agent applications. | Jingxuan Chen, Derek Yuen, Bin Xie, Yuhao Yang, Gongwei Chen, Zhihao Wu, Li Yixing, Xurui Zhou, Weiwen Liu, Shuai Wang, Kaiwen Zhou, Rui Shao, Liqiang Nie, Yasheng Wang, Jianye Hao, Jun Wang, Kun Shao |  |
| 396 |  |  [DeepRTL: Bridging Verilog Understanding and Generation with a Unified Representation Model](https://openreview.net/forum?id=2hcfoCHKoB) |  | 0 | Recent advancements in large language models (LLMs) have shown significant potential for automating hardware description language (HDL) code generation from high-level natural language instructions. While fine-tuning has improved LLMs' performance in hardware design tasks, prior efforts have largely focused on Verilog generation, overlooking the equally critical task of Verilog understanding. Furthermore, existing models suffer from weak alignment between natural language descriptions and Verilog code, hindering the generation of high-quality, synthesizable designs. To address these issues, we present DeepRTL, a unified representation model that excels in both Verilog understanding and generation. Based on CodeT5+, DeepRTL is fine-tuned on a comprehensive dataset that aligns Verilog code with rich, multi-level natural language descriptions. We also introduce the first benchmark for Verilog understanding and take the initiative to apply embedding similarity and GPT Score to evaluate the models' understanding capabilities. These metrics capture semantic similarity more accurately than traditional methods like BLEU and ROUGE, which are limited to surface-level n-gram overlaps. By adapting curriculum learning to train DeepRTL, we enable it to significantly outperform GPT-4 in Verilog understanding tasks, while achieving performance on par with OpenAI's o1-preview model in Verilog generation tasks. | Yi Liu, Changran Xu, Yunhao Zhou, Zeju Li, Qiang Xu |  |
| 397 |  |  [Robust Function-Calling for On-Device Language Model via Function Masking](https://openreview.net/forum?id=yVQcr4qjD6) |  | 0 | Large language models have demonstrated impressive value in performing as autonomous agents when equipped with external tools and API calls. Nonetheless, effectively harnessing their potential for executing complex tasks crucially relies on enhancements in their function-calling capabilities. This paper identifies a critical gap in existing function-calling models, where performance varies significantly across benchmarks, often due to over-fitting to specific naming conventions. To address such an issue, we introduce Hammer, a novel family of foundation models specifically engineered for on-device function calling. Hammer employs an augmented dataset that enhances models’ sensitivity to irrelevant functions and incorporates function masking techniques to minimize over-fitting. Our empirical evaluations reveal that Hammer not only outperforms larger models but also demonstrates robust generalization across diverse benchmarks, achieving state-of-the-art results. Our open-source contributions include a specialized dataset for irrelevance detection, a tuning framework for enhanced generalization, and the Hammer models, establishing a new standard for function-calling performance. | Qiqiang Lin, Muning Wen, Qiuying Peng, Guanyu Nie, Junwei Liao, Jun Wang, Xiaoyun Mo, Jiamu Zhou, Cheng Cheng, Yin Zhao, Jun Wang, Weinan Zhang |  |
| 398 |  |  [IGL-Bench: Establishing the Comprehensive Benchmark for Imbalanced Graph Learning](https://openreview.net/forum?id=uTqnyF0JNR) |  | 0 | Deep graph learning has gained grand popularity over the past years due to its versatility and success in representing graph data across a wide range of domains. However, the pervasive issue of imbalanced graph data distributions, where certain parts exhibit disproportionally abundant data while others remain sparse, undermines the efficacy of conventional graph learning algorithms, leading to biased outcomes. To address this challenge, Imbalanced Graph Learning (IGL) has garnered substantial attention, enabling more balanced data distributions and better task performance. Despite the proliferation of IGL algorithms, the absence of consistent experimental protocols and fair performance comparisons pose a significant barrier to comprehending advancements in this field. To bridge this gap, we introduce \*\*IGL-Bench\*\*, a foundational comprehensive benchmark for imbalanced graph learning, embarking on \*\*17\*\* diverse graph datasets and \*\*24\*\* distinct IGL algorithms with uniform data processing and splitting strategies. Specifically, IGL-Bench systematically investigates state-of-the-art IGL algorithms in terms of \*\*effectiveness\*\*, \*\*robustness\*\*, and \*\*efficiency\*\* on node-level and graph-level tasks, with the scope of class-imbalance and topology-imbalance. Extensive experiments demonstrate the potential benefits of IGL algorithms on various imbalanced conditions, offering insights and opportunities in the IGL field. Further, we have developed an open-sourced and unified package to facilitate reproducible evaluation and inspire further innovative research, available at: https://github.com/RingBDStack/IGL-Bench. | Jiawen Qin, Haonan Yuan, Qingyun Sun, Lyujin Xu, Jiaqi Yuan, Pengfeng Huang, Zhaonan Wang, Xingcheng Fu, Hao Peng, Jianxin Li, Philip S. Yu |  |
| 399 |  |  [Learning Equivariant Non-Local Electron Density Functionals](https://openreview.net/forum?id=FhBT596F1X) |  | 0 | The accuracy of density functional theory hinges on the approximation of non-local contributions to the exchange-correlation (XC) functional. To date, machine-learned and human-designed approximations suffer from insufficient accuracy, limited scalability, or dependence on costly reference data. To address these issues, we introduce Equivariant Graph Exchange Correlation (EG-XC), a novel non-local XC functional based on equivariant graph neural networks (GNNs). Where previous works relied on semi-local functionals or fixed-size descriptors of the density, we compress the electron density into an SO(3)-equivariant nuclei-centered point cloud for efficient non-local atomic-range interactions. By applying an equivariant GNN on this point cloud, we capture molecular-range interactions in a scalable and accurate manner. To train EG-XC, we differentiate through a self-consistent field solver requiring only energy targets. In our empirical evaluation, we find EG-XC to accurately reconstruct \`gold-standard' CCSD(T) energies on MD17. On out-of-distribution conformations of 3BPA, EG-XC reduces the relative MAE by 35% to 50%. Remarkably, EG-XC excels in data efficiency and molecular size extrapolation on QM9, matching force fields trained on 5 times more and larger molecules. On identical training sets, EG-XC yields on average 51% lower MAEs. | Nicholas Gao, Eike Eberhard, Stephan Günnemann |  |
| 400 |  |  [PaRa: Personalizing Text-to-Image Diffusion via Parameter Rank Reduction](https://openreview.net/forum?id=KZgo2YQbhc) |  | 0 | Personalizing a large-scale pretrained Text-to-Image (T2I) diffusion model is chal- lenging as it typically struggles to make an appropriate trade-off between its training data distribution and the target distribution, i.e., learning a novel concept with only a few target images to achieve personalization (aligning with the personalized target) while preserving text editability (aligning with diverse text prompts). In this paper, we propose PaRa, an effective and efficient Parameter Rank Reduction approach for T2I model personalization by explicitly controlling the rank of the diffusion model parameters to restrict its initial diverse generation space into a small and well-balanced target space. Our design is motivated by the fact that taming a T2I model toward a novel concept such as a specific art style implies a small generation space. To this end, by reducing the rank of model parameters during finetuning, we can effectively constrain the space of the denoising sampling trajectories towards the target. With comprehensive experiments, we show that PaRa achieves great advantages over existing finetuning approaches on single/multi-subject generation as well as single-image editing. Notably, compared to the prevailing fine-tuning technique LoRA, PaRa achieves better parameter efficiency (2× fewer learnable parameters) and much better target image alignment. | Shangyu Chen, Zizheng Pan, Jianfei Cai, Dinh Q. Phung |  |
| 401 |  |  [Near-Optimal Online Learning for Multi-Agent Submodular Coordination: Tight Approximation and Communication Efficiency](https://openreview.net/forum?id=i8dYPGdB1C) |  | 0 | Coordinating multiple agents to collaboratively maximize submodular functions in unpredictable environments is a critical task with numerous applications in machine learning, robot planning and control. The existing approaches, such as the OSG algorithm, are often hindered by their poor approximation guarantees and the rigid requirement for a fully connected communication graph. To address these challenges, we firstly present a $\textbf{MA-OSMA}$ algorithm, which employs the multi-linear extension to transfer the discrete submodular maximization problem into a continuous optimization, thereby allowing us to reduce the strict dependence on a complete graph through consensus techniques. Moreover, $\textbf{MA-OSMA}$ leverages a novel surrogate gradient to avoid sub-optimal stationary points. To eliminate the computationally intensive projection operations in $\textbf{MA-OSMA}$, we also introduce a projection-free $\textbf{MA-OSEA}$ algorithm, which effectively utilizes the KL divergence by mixing a uniform distribution. Theoretically, we confirm that both algorithms achieve a regret bound of $\widetilde{O}(\sqrt{\frac{C_{T}T}{1-\beta}})$ against a $(\frac{1-e^{-c}}{c})$-approximation to the best comparator in hindsight, where $C_{T}$ is the deviation of maximizer sequence, $\beta$ is the spectral gap of the network and $c$ is the joint curvature of submodular objectives. This result significantly improves the $(\frac{1}{1+c})$-approximation provided by the state-of-the-art OSG algorithm. Finally, we demonstrate the effectiveness of our proposed algorithms through simulation-based multi-target tracking. | Qixin Zhang, Zongqi Wan, Yu Yang, Li Shen, Dacheng Tao |  |
| 402 |  |  [PETRA: Parallel End-to-end Training with Reversible Architectures](https://openreview.net/forum?id=0fhzSFsGUT) |  | 0 | Reversible architectures have been shown to be capable of performing on par with their non-reversible architectures, being applied in deep learning for memory savings and generative modeling. In this work, we show how reversible architectures can solve challenges in parallelizing deep model training. We introduce PETRA, a novel alternative to backpropagation for parallelizing gradient computations. PETRA facilitates effective model parallelism by enabling stages (i.e., a set of layers) to compute independently on different devices, while only needing to communicate activations and gradients between each other. By decoupling the forward and backward passes and keeping a single updated version of the parameters, the need for weight stashing is also removed. We develop a custom autograd-like training framework for PETRA, and we demonstrate its effectiveness on standard computer vision benchmarks, achieving competitive accuracies comparable to backpropagation using ResNet-18, ResNet-34, and ResNet-50 models. | Stéphane Rivaud, Louis Fournier, Thomas Pumir, Eugene Belilovsky, Michael Eickenberg, Edouard Oyallon |  |
| 403 |  |  [BirdSet: A Large-Scale Dataset for Audio Classification in Avian Bioacoustics](https://openreview.net/forum?id=dRXxFEY8ZE) |  | 0 | Deep learning (DL) has greatly advanced audio classification, yet the field is limited by the scarcity of large-scale benchmark datasets that have propelled progress in other domains. While AudioSet is a pivotal step to bridge this gap as a universal-domain dataset, its restricted accessibility and limited range of evaluation use cases challenge its role as the sole resource. Therefore, we introduce BirdSet, a large-scale benchmark data set for audio classification focusing on avian bioacoustics. BirdSet surpasses AudioSet with over 6,800 recording hours ($\uparrow17\%$) from nearly 10,000 classes ($\uparrow18\times$) for training and more than 400 hours ($\uparrow7\times$) across eight strongly labeled evaluation datasets. It serves as a versatile resource for use cases such as multi-label classification, covariate shift or self-supervised learning. We benchmark six well-known DL models in multi-label classification across three distinct training scenarios and outline further evaluation use cases in audio classification. We host our dataset on Hugging Face for easy accessibility and offer an extensive codebase to reproduce our results. | Lukas Rauch, Raphael Schwinger, Moritz Wirth, René Heinrich, Denis Huseljic, Marek Herde, Jonas Lange, Stefan Kahl, Bernhard Sick, Sven Tomforde, Christoph Scholz |  |
| 404 |  |  [SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity Reduction](https://openreview.net/forum?id=ixMBnOhFGd) |  | 0 | Large Language Models (LLMs) have demonstrated improved generation performance by incorporating externally retrieved knowledge, a process known as retrieval-augmented generation (RAG). Despite the potential of this approach, existing studies evaluate RAG effectiveness by 1) assessing retrieval and generation components jointly, which obscures retrieval's distinct contribution, or 2) examining retrievers using traditional metrics such as NDCG, which creates a gap in understanding retrieval's true utility in the overall generation process. To address the above limitations, in this work, we introduce an automatic evaluation method that measures retrieval quality through the lens of information gain within the RAG framework. Specifically, we propose Semantic Perplexity (SePer), a metric that captures the LLM's internal belief about the correctness of the retrieved information. We quantify the utility of retrieval by the extent to which it reduces semantic perplexity post-retrieval. Extensive experiments demonstrate that SePer not only aligns closely with human preferences but also offers a more precise and efficient evaluation of retrieval utility across diverse RAG scenarios. | Lu Dai, Yijie Xu, Jinhui Ye, Hao Liu, Hui Xiong |  |
| 405 |  |  [Physics-aligned field reconstruction with diffusion bridge](https://openreview.net/forum?id=D042vFwJAM) |  | 0 | The reconstruction of physical fields from sparse measurements is pivotal in both scientific research and engineering applications. Traditional methods are increasingly supplemented by deep learning models due to their efficacy in extracting features from data. However, except for the low accuracy on complex physical systems, these models often fail to comply with essential physical constraints, such as governing equations and boundary conditions. To overcome this limitation, we introduce a novel data-driven field reconstruction framework, termed the Physics-aligned Schr\"{o}dinger Bridge (PalSB). This framework leverages a diffusion bridge mechanism that is specifically tailored to align with physical constraints. The PalSB approach incorporates a dual-stage training process designed to address both local reconstruction mapping and global physical principles. Additionally, a boundary-aware sampling technique is implemented to ensure adherence to physical boundary conditions. We demonstrate the effectiveness of PalSB through its application to three complex nonlinear systems: cylinder flow from Particle Image Velocimetry experiments, two-dimensional turbulence, and a reaction-diffusion system. The results reveal that PalSB not only achieves higher accuracy but also exhibits enhanced compliance with physical constraints compared to existing methods. This highlights PalSB's capability to generate high-quality representations of intricate physical interactions, showcasing its potential for advancing field reconstruction techniques. The source code can be found at https://github.com/lzy12301/PalSB. | Zeyu Li, Hongkun Dou, Shen Fang, Wang Han, Yue Deng, Lijun Yang |  |
| 406 |  |  [RegMix: Data Mixture as Regression for Language Model Pre-training](https://openreview.net/forum?id=5BjQOUXq7i) |  | 0 | The data mixture for large language model pre-training significantly impacts performance, yet how to determine an effective mixture remains unclear. We propose RegMix to automatically identify a high-performing data mixture by formulating it as a regression task. RegMix trains many small models on diverse data mixtures, uses regression to predict performance of unseen mixtures, and applies the best predicted mixture to train a large-scale model with orders of magnitude more compute. To empirically validate RegMix, we train 512 models with 1M parameters for 1B tokens to fit the regression model and predict the best data mixture. Using this mixture we train a 1B parameter model for 25B tokens (i.e. 1000× larger and 25× longer) which we find performs best among 64 candidate 1B parameter models with other mixtures. Furthermore, RegMix consistently outperforms human selection in experiments involving models up to 7B models trained on 100B tokens, while matching or exceeding DoReMi using just 10% of the computational resources. Our experiments also show that (1) Data mixtures significantly impact performance; (2) Web corpora rather than data perceived as high-quality like Wikipedia have the strongest positive correlation with downstream performance; (3) Domains interact in complex ways often contradicting common sense, thus automatic approaches like RegMix are needed; (4) Data mixture effects transcend scaling laws. Our code is available at https://github.com/sail-sg/regmix. | Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, Min Lin |  |
| 407 |  |  [When Attention Sink Emerges in Language Models: An Empirical View](https://openreview.net/forum?id=78Nn4QJTEN) |  | 0 | Auto-regressive language Models (LMs) assign significant attention to the first token, even if it is not semantically important, which is known as \*\*attention sink\*\*. This phenomenon has been widely adopted in applications such as streaming/long context generation, KV cache optimization, inference acceleration, model quantization, and others. Despite its widespread use, a deep understanding of attention sink in LMs is still lacking. In this work, we first demonstrate that attention sinks exist universally in auto-regressive LMs with various inputs, even in small models. Furthermore, attention sink is observed to emerge during the LM pre-training, motivating us to investigate how \*optimization\*, \*data distribution\*, \*loss function\*, and \*model architecture\* in LM pre-training influence its emergence. We highlight that attention sink emerges after effective optimization on sufficient training data. The sink position is highly correlated with the loss function and data distribution. Most importantly, we find that attention sink acts more like key biases, \*storing extra attention scores\*, which could be non-informative and not contribute to the value computation. We also observe that this phenomenon (at least partially) stems from tokens' inner dependence on attention scores as a result of softmax normalization. After relaxing such dependence by replacing softmax attention with other attention operations, such as sigmoid attention without normalization, attention sinks do not emerge in LMs up to 1B parameters. The code is available at https://github.com/sail-sg/Attention-Sink. | Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, Min Lin |  |
| 408 |  |  [PianoMotion10M: Dataset and Benchmark for Hand Motion Generation in Piano Performance](https://openreview.net/forum?id=rxVvRBgqmS) |  | 0 | Recently, artificial intelligence techniques for education have been received increasing attentions, while it still remains an open problem to design the effective music instrument instructing systems. Although key presses can be directly derived from sheet music, the transitional movements among key presses require more extensive guidance in piano performance. In this work, we construct a piano-hand motion generation benchmark to guide hand movements and fingerings for piano playing. To this end, we collect an annotated dataset, PianoMotion10M, consisting of 116 hours of piano playing videos from a bird's-eye view with 10 million annotated hand poses. We also introduce a powerful baseline model that generates hand motions from piano audios through a position predictor and a position-guided gesture generator. Furthermore, a series of evaluation metrics are designed to assess the performance of the baseline model, including motion similarity, smoothness, positional accuracy of left and right hands, and overall fidelity of movement distribution. Despite that piano key presses with respect to music scores or audios are already accessible, PianoMotion10M aims to provide guidance on piano fingering for instruction purposes. The source code and dataset can be accessed at https://github.com/agnJason/PianoMotion10M. | Qijun Gan, Song Wang, Shengtao Wu, Jianke Zhu |  |
| 409 |  |  [The Power of LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions](https://openreview.net/forum?id=ws5phQki00) |  | 0 | Stance detection holds great potential to improve online political discussions through its deployment in discussion platforms for purposes such as content moderation, topic summarisation or to facilitate more balanced discussions. Typically, transformer-based models are employed directly for stance detection, requiring vast amounts of data. However, the wide variety of debate topics in online political discussions makes data collection particularly challenging. LLMs have revived stance detection, but their online deployment in online political discussions faces challenges like inconsistent outputs, biases, and vulnerability to adversarial attacks. We show how LLM-generated synthetic data can improve stance detection for online political discussions by using reliable traditional stance detection models for online deployment, while leveraging the text generation capabilities of LLMs for synthetic data generation in a secure offline environment. To achieve this, (i) we generate synthetic data for specific debate questions by prompting a Mistral-7B model and show that fine-tuning with the generated synthetic data can substantially improve the performance of stance detection, while remaining interpretable and aligned with real world data. (ii) Using the synthetic data as a reference, we can improve performance even further by identifying the most informative samples in an unlabelled dataset, i.e., those samples which the stance detection model is most uncertain about and can benefit from the most. By fine-tuning with both synthetic data and the most informative samples, we surpass the performance of the baseline model that is fine-tuned on all true labels, while labelling considerably less data. | Stefan Sylvius Wagner, Maike Behrendt, Marc Ziegele, Stefan Harmeling |  |
| 410 |  |  [Tell me about yourself: LLMs are aware of their learned behaviors](https://openreview.net/forum?id=IjQ2Jtemzy) |  | 0 | We study \*behavioral self-awareness\*, which we define as an LLM's capability to articulate its behavioral policies without relying on in-context examples. We finetune LLMs on examples that exhibit particular behaviors, including (a) making risk-seeking / risk-averse economic decisions, and (b) making the user say a certain word. Although these examples never contain explicit descriptions of the policy (e.g. "I will now take the risk-seeking option"), we find that the finetuned LLMs can explicitly describe their policies through out-of-context reasoning. We demonstrate LLMs' behavioral self-awareness across various evaluation tasks, both for multiple-choice and free-form questions. Furthermore, we demonstrate that models can correctly attribute different learned policies to distinct personas. Finally, we explore the connection between behavioral self-awareness and the concept of backdoors in AI safety, where certain behaviors are implanted in a model, often through data poisoning, and can be triggered under certain conditions. We find evidence that LLMs can recognize the existence of the backdoor-like behavior that they have acquired through fine-tuning. | Jan Betley, Xuchan Bao, Martín Soto, Anna SztyberBetley, James Chua, Owain Evans |  |
| 411 |  |  [COPER: Correlation-based Permutations for Multi-View Clustering](https://openreview.net/forum?id=5ZEbpBYGwH) |  | 0 | Combining data from different sources can improve data analysis tasks such as clustering. However, most of the current multi-view clustering methods are limited to specific domains or rely on a suboptimal and computationally intensive two-stage process of representation learning and clustering. We propose an end-to-end deep learning-based multi-view clustering framework for general data types (such as images and tables). Our approach involves generating meaningful fused representations using a novel permutation-based canonical correlation objective. We provide a theoretical analysis showing how the learned embeddings approximate those obtained by supervised linear discriminant analysis (LDA). Cluster assignments are learned by identifying consistent pseudo-labels across multiple views. Additionally, we establish a theoretical bound on the error caused by incorrect pseudo-labels in the unsupervised representations compared to LDA. Extensive experiments on ten multi-view clustering benchmark datasets provide empirical evidence for the effectiveness of the proposed model. | Ran Eisenberg, Jonathan Svirsky, Ofir Lindenbaum |  |
| 412 |  |  [Improving Convergence Guarantees of Random Subspace Second-order Algorithm for Nonconvex Optimization](https://openreview.net/forum?id=tuu4de7HL1) |  | 0 | In recent years, random subspace methods have been actively studied for large-dimensional nonconvex problems. Recent subspace methods have improved theoretical guarantees such as iteration complexity and local convergence rate while reducing computational costs by deriving descent directions in randomly selected low-dimensional subspaces. This paper proposes the Random Subspace Homogenized Trust Region (RSHTR) method with the best theoretical guarantees among random subspace algorithms for nonconvex optimization. RSHTR achieves an $\varepsilon$-approximate first-order stationary point in $O(\varepsilon^{-3/2})$ iterations, converging locally at a linear rate. Furthermore, under rank-deficient conditions, RSHTR satisfies $\varepsilon$-approximate second-order necessary conditions in $O(\varepsilon^{-3/2})$ iterations and exhibits a local quadratic convergence. Experiments on real-world datasets verify the benefits of RSHTR. | Rei Higuchi, PierreLouis Poirion, Akiko Takeda |  |
| 413 |  |  [Revisiting text-to-image evaluation with Gecko: on metrics, prompts, and human rating](https://openreview.net/forum?id=Im2neAMlre) |  | 0 | While text-to-image (T2I) generative models have become ubiquitous, they do not necessarily generate images that align with a given prompt. While many metrics and benchmarks have been proposed to evaluate T2I models and alignment metrics, the impact of the evaluation components (prompt sets, human annotations, evaluation task) has not been systematically measured. We find that looking at only \*one slice of data\*, i.e. one set of capabilities or human annotations, is not enough to obtain stable conclusions that generalise to new conditions or slices when evaluating T2I models or alignment metrics. We address this by introducing an evaluation suite of $>$100K annotations across four human annotation templates that comprehensively evaluates models' capabilities across a range of methods for gathering human annotations and comparing models. In particular, we propose (1) a carefully curated set of prompts -- \*Gecko2K\*; (2) a statistically grounded method of comparing T2I models; and (3) how to systematically evaluate metrics under three \*evaluation tasks\* -- \*model ordering, pair-wise instance scoring, point-wise instance scoring\*. Using this evaluation suite, we evaluate a wide range of metrics and find that a metric may do better in one setting but worse in another. As a result, we introduce a new, interpretable auto-eval metric that is consistently better correlated with human ratings than such existing metrics on our evaluation suite--across different human templates and evaluation settings--and on TIFA160. | Olivia Wiles, Chuhan Zhang, Isabela Albuquerque, Ivana Kajic, Su Wang, Emanuele Bugliarello, Yasumasa Onoe, Pinelopi Papalampidi, Ira Ktena, Christopher Knutsen, Cyrus Rashtchian, Anant Nawalgaria, Jordi PontTuset, Aida Nematzadeh |  |
| 414 |  |  [Diffusion Bridge AutoEncoders for Unsupervised Representation Learning](https://openreview.net/forum?id=hBGavkf61a) |  | 0 | Diffusion-based representation learning has achieved substantial attention due to its promising capabilities in latent representation and sample generation. Recent studies have employed an auxiliary encoder to identify a corresponding representation from data and to adjust the dimensionality of a latent variable $\mathbf{z}$. Meanwhile, this auxiliary structure invokes an \*information split problem\*; the information of each data instance $\mathbf{x}_0$ is divided into diffusion endpoint $\mathbf{x}_T$ and encoded $\mathbf{z}$ because there exist two inference paths starting from the data. The latent variable modeled by diffusion endpoint $\mathbf{x}_T$ has some disadvantages. The diffusion endpoint $\mathbf{x}_T$ is computationally expensive to obtain and inflexible in dimensionality. To address this problem, we introduce Diffusion Bridge AuteEncoders (DBAE), which enables $\mathbf{z}$-dependent endpoint $\mathbf{x}_T$ inference through a feed-forward architecture. This structure creates an information bottleneck at $\mathbf{z}$, so $\mathbf{x}_T$ becomes dependent on $\mathbf{z}$ in its generation. This results in $\mathbf{z}$ holding the full information of data. We propose an objective function for DBAE to enable both reconstruction and generative modeling, with their theoretical justification. Empirical evidence supports the effectiveness of the intended design in DBAE, which notably enhances downstream inference quality, reconstruction, and disentanglement. Additionally, DBAE generates high-fidelity samples in the unconditional generation. Our code is available at https://github.com/aailab-kaist/DBAE. | Yeongmin Kim, Kwanghyeon Lee, Minsang Park, Byeonghu Na, IlChul Moon |  |
| 415 |  |  [Bundle Neural Network for message diffusion on graphs](https://openreview.net/forum?id=scI9307PLG) |  | 0 | The dominant paradigm for learning on graphs is message passing. Despite being a strong inductive bias, the local message passing mechanism faces challenges such as over-smoothing, over-squashing, and limited expressivity. To address these issues, we introduce Bundle Neural Networks (BuNNs), a novel graph neural network architecture that operates via \*message diffusion\* on \*flat vector bundles\* — geometrically inspired structures that assign to each node a vector space and an orthogonal map. A BuNN layer evolves node features through a diffusion-type partial differential equation, where its discrete form acts as a special case of the recently introduced Sheaf Neural Network (SNN), effectively alleviating over-smoothing. The continuous nature of message diffusion enables BuNNs to operate at larger scales, reducing over-squashing. We establish the universality of BuNNs in approximating feature transformations on infinite families of graphs with injective positional encodings, marking the first positive expressivity result of its kind. We support our claims with formal analysis and synthetic experiments. Empirically, BuNNs perform strongly on heterophilic and long-range tasks, which demonstrates their robustness on a diverse range of challenging real-world tasks. | Jacob Bamberger, Federico Barbero, Xiaowen Dong, Michael M. Bronstein |  |
| 416 |  |  [SynFlowNet: Design of Diverse and Novel Molecules with Synthesis Constraints](https://openreview.net/forum?id=uvHmnahyp1) |  | 0 | Generative models see increasing use in computer-aided drug design. However, while performing well at capturing distributions of molecular motifs, they often produce synthetically inaccessible molecules. To address this, we introduce SynFlowNet, a GFlowNet model whose action space uses chemical reactions and buyable reactants to sequentially build new molecules. By incorporating forward synthesis as an explicit constraint of the generative mechanism, we aim at bridging the gap between in silico molecular generation and real world synthesis capabilities. We evaluate our approach using synthetic accessibility scores and an independent retrosynthesis tool to assess the synthesizability of our compounds, and motivate the choice of GFlowNets through considerable improvement in sample diversity compared to baselines. Additionally, we identify challenges with reaction encodings that can complicate traversal of the MDP in the backward direction. To address this, we introduce various strategies for learning the GFlowNet backward policy and thus demonstrate how additional constraints can be integrated into the GFlowNet MDP framework. This approach enables our model to successfully identify synthesis pathways for previously unseen molecules. | Miruna T. Cretu, Charles Harris, Ilia Igashov, Arne Schneuing, Marwin H. S. Segler, Bruno E. Correia, Julien Roy, Emmanuel Bengio, Pietro Lio |  |
| 417 |  |  [u-μP: The Unit-Scaled Maximal Update Parametrization](https://openreview.net/forum?id=P7KRIiLM8T) |  | 0 | The Maximal Update Parametrization ($\mu$P) aims to make the optimal hyperparameters (HPs) of a model independent of its size, allowing them to be swept using a cheap proxy model rather than the full-size target model. We present a new scheme, u-$\mu$P, which improves upon $\mu$P by combining it with Unit Scaling, a method for designing models that makes them easy to train in low-precision. The two techniques have a natural affinity: $\mu$P ensures that the scale of activations is independent of model size, and Unit Scaling ensures that activations, weights and gradients begin training with a scale of one. This synthesis opens the door to a simpler scheme, whose default values are near-optimal. This in turn facilitates a more efficient sweeping strategy, with u-$\mu$P models reaching a lower loss than comparable $\mu$P models and working out-of-the-box in FP8. | Charlie Blake, Constantin Eichenberg, Josef Dean, Lukas Balles, Luke Yuri Prince, Björn Deiseroth, Andrés Felipe CruzSalinas, Carlo Luschi, Samuel Weinbach, Douglas Orr |  |
| 418 |  |  [Improved Convergence Rate for Diffusion Probabilistic Models](https://openreview.net/forum?id=SOd07Qxkw4) |  | 0 | Score-based diffusion models have achieved remarkable empirical performance in the field of machine learning and artificial intelligence for their ability to generate high-quality new data instances from complex distributions. Improving our understanding of diffusion models, including mainly convergence analysis for such models, has attracted a lot of interests. Despite a lot of theoretical attempts, there still exists significant gap between theory and practice. Towards to close this gap, we establish an iteration complexity at the order of $d^{1/3}\varepsilon^{-2/3}$, which is better than $d^{5/12}\varepsilon^{-1}$, the best known complexity achieved before our work. This convergence analysis is based on a randomized midpoint method, which is first proposed for log-concave sampling (Shen & Lee, 2019), and then extended to diffusion models by Gupta et al. (2024). Our theory accommodates $\varepsilon$-accurate score estimates, and does not require log-concavity on the target distribution. Moreover, the algorithm can also be parallelized to run in only $O(\log^2(d/\varepsilon))$ parallel rounds in a similar way to prior works. | Gen Li, Yuchen Jiao |  |
| 419 |  |  [MagicPIG: LSH Sampling for Efficient LLM Generation](https://openreview.net/forum?id=ALzTQUgW8a) |  | 0 | Large language models (LLMs) with long context windows have gained significant attention. However, the KV cache, stored to avoid re-computation, becomes a bottleneck. Various dynamic sparse or TopK-based attention approximation methods have been proposed to leverage the common insight that attention is sparse. In this paper, we first show that TopK attention itself suffers from quality degradation in certain downstream tasks because attention is not always as sparse as expected. Rather than selecting the keys and values with the highest attention scores, sampling with theoretical guarantees can provide a better estimation for attention output. To make the sampling-based approximation practical in LLM generation, we propose MagicPIG, a heterogeneous system based on Locality Sensitive Hashing (LSH). MagicPIG significantly reduces the workload of attention computation while preserving high accuracy for diverse tasks. MagicPIG stores the LSH hash tables and runs the attention computation on the CPU, which allows it to serve longer contexts and larger batch sizes with high approximation accuracy. MagicPIG can improve decoding throughput by up to $5\times$ across various GPU hardware and achieve 54ms decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a context of 96k tokens. | Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye, Yang Zhou, Jianyu Zhang, Niklas Nolte, Yuandong Tian, Matthijs Douze, Léon Bottou, Zhihao Jia, Beidi Chen |  |
| 420 |  |  [Streamlining Redundant Layers to Compress Large Language Models](https://openreview.net/forum?id=IC5RJvRoMp) |  | 0 | This paper introduces LLM-Streamline, a pioneer work on layer pruning for large language models (LLMs). It is based on the observation that different layers have varying impacts on hidden states, enabling the identification of less important layers to be pruned. LLM-Streamline comprises two parts: layer pruning, which removes consecutive layers with the lowest importance based on target sparsity, and layer replacement, a novel module that trains a lightweight network to replace the pruned layers to mitigate performance loss. Additionally, a new metric called stability is proposed to address the limitations of the widely used accuracy metric in evaluating model compression. Experiments show that LLM-Streamline outperforms both previous and concurrent state-of-the-art pruning methods in terms of both performance and training efficiency. Our code is available at \href{https://github.com/RUCKBReasoning/LLM-Streamline}{this repository}. | Xiaodong Chen, Yuxuan Hu, Jing Zhang, Yanling Wang, Cuiping Li, Hong Chen |  |
| 421 |  |  [Rethinking and Improving Autoformalization: Towards a Faithful Metric and a Dependency Retrieval-based Approach](https://openreview.net/forum?id=hUb2At2DsQ) |  | 0 | As a central component in formal verification, statement autoformalization has been widely studied including the recent efforts from machine learning community, but still remains a widely-recognized difficult and open problem. In this paper, we delve into two critical yet under-explored gaps: 1) absence of faithful and universal automated evaluation for autoformalization results; 2) agnosia of contextual information, inducing severe hallucination of formal definitions and theorems. To address the first issue, we propose \*\*BEq\*\* (_\*\*B\*\*idirectional \*\*E\*\*xtended Definitional E\*\*q\*\*uivalence_), an automated neuro-symbolic method to determine the equivalence between two formal statements, which is formal-grounded and well-aligned with human intuition. For the second, we propose \*\*RAutoformalizer\*\* (_\*\*R\*\*etrieval-augmented \*\*Autoformalizer\*\*_), augmenting statement autoformalization by _Dependency Retrieval_, retrieving potentially dependent objects from formal libraries. We parse the dependencies of libraries and propose to _structurally informalise_ formal objects by the topological order of dependencies. To evaluate OOD generalization and research-level capabilities, we build a novel benchmark, _Con-NF_, consisting of 961 informal-formal statement pairs from frontier mathematical researches. Experiments validate the effectiveness of our approaches: BEq is evaluated on 200 diverse formal statement pairs with expert-annotated equivalence label, exhibiting significantly improved accuracy ($82.50\\% \mapsto 90.50\\%$) and precision ($70.59\\% \mapsto 100.0\\%$). For dependency retrieval, a strong baseline is devised. Our RAutoformalizer substantially outperforms SOTA baselines in both in-distribution ProofNet benchmark ($12.83\\% \mapsto 18.18\\%$, BEq@8) and OOD Con-NF scenario ($4.58\\%\mapsto 16.86\\%$, BEq@8). | Qi Liu, Xinhao Zheng, Xudong Lu, Qinxiang Cao, Junchi Yan |  |
| 422 |  |  [Identifiable Exchangeable Mechanisms for Causal Structure and Representation Learning](https://openreview.net/forum?id=k03mB41vyM) |  | 0 | Identifying latent representations or causal structures is important for good generalization and downstream task performance. However, both fields developed rather independently. We observe that several structure and representation identifiability methods, particularly those that require multiple environments, rely on exchangeable non--i.i.d. (independent and identically distributed) data. To formalize this connection, we propose the Identifiable Exchangeable Mechanisms (IEM) framework to unify key representation and causal structure learning methods. IEM provides a unified probabilistic graphical model encompassing causal discovery, Independent Component Analysis, and Causal Representation Learning. With the help of the IEM model, we generalize the Causal de Finetti theorem of Guo et al., 2022 by relaxing the necessary conditions for causal structure identification in exchangeable data. We term these conditions cause and mechanism variability, and show how they imply a duality condition in identifiable representation learning, leading to new identifiability results. | Patrik Reizinger, Siyuan Guo, Ferenc Huszár, Bernhard Schölkopf, Wieland Brendel |  |
| 423 |  |  [Learning Spatiotemporal Dynamical Systems from Point Process Observations](https://openreview.net/forum?id=37EXtKCOkn) |  | 0 | Spatiotemporal dynamics models are fundamental for various domains, from heat propagation in materials to oceanic and atmospheric flows. However, currently available neural network-based spatiotemporal modeling approaches fall short when faced with data that is collected randomly over time and space, as is often the case with sensor networks in real-world applications like crowdsourced earthquake detection or pollution monitoring. In response, we developed a new method that can effectively learn spatiotemporal dynamics from such point process observations. Our model integrates techniques from neural differential equations, neural point processes, implicit neural representations and amortized variational inference to model both the dynamics of the system and the probabilistic locations and timings of observations. It outperforms existing methods on challenging spatiotemporal datasets by offering substantial improvements in predictive accuracy and computational efficiency, making it a useful tool for modeling and understanding complex dynamical systems observed under realistic, unconstrained conditions. | Valerii Iakovlev, Harri Lähdesmäki |  |
| 424 |  |  [Probabilistic Neural Pruning via Sparsity Evolutionary Fokker-Planck-Kolmogorov Equation](https://openreview.net/forum?id=hJ1BaJ5ELp) |  | 0 | Neural pruning aims to compress and accelerate deep neural networks by identifying the optimal subnetwork within a specified sparsity budget. In this work, we study how to gradually sparsify the unpruned dense model to the target sparsity level with minimal performance drop. Specifically, we analyze the evolution of the population of optimal subnetworks under continuous sparsity increments from a thermodynamic perspective. We first reformulate neural pruning as an expected loss minimization problem over the mask distributions. Then, we establish an effective approximation for the sparsity evolution of the optimal mask distribution, termed the \*\*S\*\*parsity Evolutionary \*\*F\*\*okker-\*\*P\*\*lanck-\*\*K\*\*olmogorov Equation (\*\*SFPK\*\*), which provides closed-form, mathematically tractable guidance on distributional transitions for minimizing the expected loss under an infinitesimal sparsity increment. On top of that, we propose SFPK-pruner, a particle simulation-based probabilistic pruning method, to sample performant masks with desired sparsity from the destination distribution of SFPK. In theory, we establish the convergence guarantee for the proposed SFPK-pruner. Our SFPK-pruner exhibits competitive performance in various pruning scenarios. The code is available on https://github.com/mzf666/SFPK-main. | Zhanfeng Mo, Haosen Shi, Sinno Jialin Pan |  |
| 425 |  |  [Diffusion Attribution Score: Evaluating Training Data Influence in Diffusion Models](https://openreview.net/forum?id=kuutidLf6R) |  | 0 | As diffusion models become increasingly popular, the misuse of copyrighted and private images has emerged as a major concern. One promising solution to mitigate this issue is identifying the contribution of specific training samples in generative models, a process known as data attribution. Existing data attribution methods for diffusion models typically quantify the contribution of a training sample by evaluating the change in diffusion loss when the sample is included or excluded from the training process. However, we argue that the direct usage of diffusion loss cannot represent such a contribution accurately due to the calculation of diffusion loss. Specifically, these approaches measure the divergence between predicted and ground truth distributions, which leads to an indirect comparison between the predicted distributions and cannot represent the variances between model behaviors. To address these issues, we aim to measure the direct comparison between predicted distributions with an attribution score to analyse the training sample importance, which is achieved by Diffusion Attribution Score (\textit{DAS}). Underpinned by rigorous theoretical analysis, we elucidate the effectiveness of DAS. Additionally, we explore strategies to accelerate DAS calculations, facilitating its application to large-scale diffusion models. Our extensive experiments across various datasets and diffusion models demonstrate that DAS significantly surpasses previous benchmarks in terms of the linear data-modelling score, establishing new state-of-the-art performance. | Jinxu Lin, Linwei Tao, Minjing Dong, Chang Xu |  |
| 426 |  |  [Uncovering Gaps in How Humans and LLMs Interpret Subjective Language](https://openreview.net/forum?id=gye2U9uNXx) |  | 0 | Humans often rely on subjective natural language to direct language models (LLMs); for example, users might instruct the LLM to write an \*enthusiastic\* blogpost, while developers might train models to be \*helpful\* and \*harmless\* using LLM-based edits. The LLM’s \*operational semantics\* of such subjective phrases---how it adjusts its behavior when each phrase is included in the prompt---thus dictates how aligned it is with human intent. In this work, we uncover instances of \*misalignment\* between LLMs' actual operational semantics and what humans expect. Our method, TED (thesaurus error detector), first constructs a thesaurus that captures whether two phrases have similar operational semantics according to the LLM. It then elicits failures by unearthing disagreements between this thesaurus and a human-constructed reference. TED routinely produces surprising instances of misalignment; for example, Mistral 7B Instruct produces more \*harassing\* outputs when it edits text to be \*witty\*, and Llama 3 8B Instruct produces \*dishonest\* articles when instructed to make the articles \*enthusiastic\*. Our results demonstrate that humans can uncover unexpected LLM behavior by scrutinizing relationships between abstract concepts, without supervising outputs directly. | Erik Jones, Arjun Patrawala, Jacob Steinhardt |  |
| 427 |  |  [Learning local equivariant representations for quantum operators](https://openreview.net/forum?id=kpq3IIjUD3) |  | 0 | Predicting quantum operator matrices such as Hamiltonian, overlap, and density matrices in the density functional theory (DFT) framework is crucial for material science. Current methods often focus on individual operators and struggle with efficiency and scalability for large systems. Here we introduce a novel deep learning model, SLEM (strictly localized equivariant message-passing), for predicting multiple quantum operators that achieves state-of-the-art accuracy while dramatically improving computational efficiency. SLEM's key innovation is its strict locality-based design for equivariant representations of quantum tensors while preserving physical symmetries. This enables complex many-body dependency without expanding the effective receptive field, leading to superior data efficiency and transferability. Using an innovative SO(2) convolution and invariant overlap parameterization, SLEM reduces the computational complexity of high-order tensor products and is, therefore, capable of handling systems requiring the $f$ and $g$ orbitals in their basis sets. We demonstrate SLEM's capabilities across diverse 2D and 3D materials, achieving high accuracy even with limited training data. SLEM's design facilitates efficient parallelization, potentially extending DFT simulations to systems with device-level sizes, opening new possibilities for large-scale quantum simulations and high-throughput materials discovery. | Zhanghao Zhouyin, Zixi Gan, Shishir Kumar Pandey, Linfeng Zhang, Qiangqiang Gu |  |
| 428 |  |  [PhyMPGN: Physics-encoded Message Passing Graph Network for spatiotemporal PDE systems](https://openreview.net/forum?id=fU8H4lzkIm) |  | 0 | Solving partial differential equations (PDEs) serves as a cornerstone for modeling complex dynamical systems. Recent progresses have demonstrated grand benefits of data-driven neural-based models for predicting spatiotemporal dynamics (e.g., tremendous speedup gain compared with classical numerical methods). However, most existing neural models rely on rich training data, have limited extrapolation and generalization abilities, and suffer to produce precise or reliable physical prediction under intricate conditions (e.g., irregular mesh or geometry, complex boundary conditions, diverse PDE parameters, etc.). To this end, we propose a new graph learning approach, namely, Physics-encoded Message Passing Graph Network (PhyMPGN), to model spatiotemporal PDE systems on irregular meshes given small training datasets. Specifically, we incorporate a GNN into a numerical integrator to approximate the temporal marching of spatiotemporal dynamics for a given PDE system. Considering that many physical phenomena are governed by diffusion processes, we further design a learnable Laplace block, which encodes the discrete Laplace-Beltrami operator, to aid and guide the GNN learning in a physically feasible solution space. A boundary condition padding strategy is also designed to improve the model convergence and accuracy. Extensive experiments demonstrate that PhyMPGN is capable of accurately predicting various types of spatiotemporal dynamics on coarse unstructured meshes, consistently achieves the state-of-the-art results, and outperforms other baselines with considerable gains. | Bocheng Zeng, Qi Wang, Mengtao Yan, Yang Liu, Ruizhi Chengze, Yi Zhang, Hongsheng Liu, Zidong Wang, Hao Sun |  |
| 429 |  |  [Demystifying the Token Dynamics of Deep Selective State Space Models](https://openreview.net/forum?id=qtTIP5Gjc5) |  | 0 | Selective state space models (SSM), such as Mamba, have gained prominence for their effectiveness in modeling sequential data. Despite their outstanding empirical performance, a comprehensive theoretical understanding of deep selective SSM remains elusive, hindering their further development and adoption for applications that need high fidelity. In this paper, we investigate the dynamical properties of tokens in a pre-trained Mamba model. In particular, we derive the dynamical system governing the continuous-time limit of the Mamba model and characterize the asymptotic behavior of its solutions. In the one-dimensional case, we prove that only one of the following two scenarios happens: either all tokens converge to zero, or all tokens diverge to infinity. We provide criteria based on model parameters to determine when each scenario occurs. For the convergent scenario, we empirically verify that this scenario negatively impacts the model's performance. For the divergent scenario, we prove that different tokens will diverge to infinity at different rates, thereby contributing unequally to the updates during model training. Based on these investigations, we propose two refinements for the model: excluding the convergent scenario and reordering tokens based on their importance scores, both aimed at improving practical performance. Our experimental results validate these refinements, offering insights into enhancing Mamba's effectiveness in real-world applications. | Thieu N. Vo, DuyTung Pham, Xin T. Tong, Tan Minh Nguyen |  |
| 430 |  |  [Let SSMs be ConvNets: State-space Modeling with Optimal Tensor Contractions](https://openreview.net/forum?id=PkpNRmBZ32) |  | 0 | We introduce Centaurus, a class of networks composed of generalized state-space model (SSM) blocks, where the SSM operations can be treated as tensor contractions during training. The optimal order of tensor contractions can then be systematically determined for every SSM block to maximize training efficiency. This allows more flexibility in designing SSM blocks beyond the depthwise-separable configuration commonly implemented. The new design choices will take inspiration from classical convolutional blocks including group convolutions, full convolutions, and bottleneck blocks. We architect the Centaurus network with a mixture of these blocks, to balance between network size and performance, as well as memory and computational efficiency during both training and inference. We show that this heterogeneous network design outperforms its homogeneous counterparts in raw audio processing tasks including keyword spotting, speech denoising, and automatic speech recognition (ASR). For ASR, Centaurus is the first network with competitive performance that can be made fully state-space based, without using any nonlinear recurrence (LSTMs), explicit convolutions (CNNs), or (surrogate) attention mechanism. | Yan Ru Pei |  |
| 431 |  |  [MixEval-X: Any-to-any Evaluations from Real-world Data Mixture](https://openreview.net/forum?id=hpCfPEvBsr) |  | 0 | Perceiving and generating diverse modalities are crucial for AI models to effectively learn from and engage with real-world signals, necessitating reliable evaluations for their development. We identify two major issues in current evaluations: (1) inconsistent standards, shaped by different communities with varying protocols and maturity levels; and (2) significant query, grading, and generalization biases. To address these, we introduce MixEval-X, the first any-to-any, real-world benchmark designed to optimize and standardize evaluations across diverse input and output modalities. We propose multi-modal benchmark mixture and adaptation-rectification pipelines to reconstruct real-world task distributions, ensuring evaluations generalize effectively to real-world use cases. Extensive meta-evaluations show our approach effectively aligns benchmark samples with real-world task distributions. Meanwhile, MixEval-X's model rankings correlate strongly with that of crowd-sourced real-world evaluations (up to 0.98) while being much more efficient. We provide comprehensive leaderboards to rerank existing models and organizations and offer insights to enhance understanding of multi-modal evaluations and inform future research. | Jinjie Ni, Yifan Song, Deepanway Ghosal, Bo Li, David Junhao Zhang, Xiang Yue, Fuzhao Xue, Yuntian Deng, Zian Zheng, Kaichen Zhang, Mahir Shah, Kabir Jain, Yang You, Michael Shieh |  |
| 432 |  |  [Knowledge Localization: Mission Not Accomplished? Enter Query Localization!](https://openreview.net/forum?id=tfyHbvFZ0K) |  | 0 | Large language models (LLMs) store extensive factual knowledge, but the mechanisms behind how they store and express this knowledge remain unclear. The Knowledge Neuron (KN) thesis is a prominent theory for explaining these mechanisms. This theory is based on the \*\*Knowledge Localization (KL)\*\* assumption, which suggests that a fact can be localized to a few knowledge storage units, namely knowledge neurons. However, this assumption has two limitations: first, it may be too rigid regarding knowledge storage, and second, it neglects the role of the attention module in knowledge expression. In this paper, we first re-examine the KL assumption and demonstrate that its limitations do indeed exist. To address these, we then present two new findings, each targeting one of the limitations: one focusing on knowledge storage and the other on knowledge expression. We summarize these findings as \*\*Query Localization\*\* assumption and argue that the KL assumption can be viewed as a simplification of the QL assumption. Based on QL assumption, we further propose the Consistency-Aware KN modification method, which improves the performance of knowledge modification, further validating our new assumption. We conduct 39 sets of experiments, along with additional visualization experiments, to rigorously confirm our conclusions. Code will be made public soon. | Yuheng Chen, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao |  |
| 433 |  |  [Graph Sparsification via Mixture of Graphs](https://openreview.net/forum?id=7ANDviElAo) |  | 0 | Graph Neural Networks (GNNs) have demonstrated superior performance across various graph learning tasks but face significant computational challenges when applied to large-scale graphs. One effective approach to mitigate these challenges is graph sparsification, which involves removing non-essential edges to reduce computational overhead. However, previous graph sparsification methods often rely on a single global sparsity setting and uniform pruning criteria, failing to provide customized sparsification schemes for each node's complex local context. In this paper, we introduce Mixture-of-Graphs (MoG), leveraging the concept of Mixture-of-Experts (MoE), to dynamically select tailored pruning solutions for each node. Specifically, MoG incorporates multiple sparsifier experts, each characterized by unique sparsity levels and pruning criteria, and selects the appropriate experts for each node. Subsequently, MoG performs a mixture of the sparse graphs produced by different experts on the Grassmann manifold to derive an optimal sparse graph. One notable property of MoG is its entirely local nature, as it depends on the specific circumstances of each individual node. Extensive experiments on four large-scale OGB datasets and two superpixel datasets, equipped with five GNN backbones, demonstrate that MoG (I) identifies subgraphs at higher sparsity levels ($8.67\\%\sim 50.85\\%$), with performance equal to or better than the dense graph, (II) achieves $1.47-2.62\times$ speedup in GNN inference with negligible performance drop, and (III) boosts \`\`top-student'' GNN performance ($1.02\\%\uparrow$ on RevGNN+\textsc{ogbn-proteins} and $1.74\\%\\uparrow$ on DeeperGCN+\textsc{ogbg-ppa}). The source code is available at \url{https://github.com/yanweiyue/MoG}. | Guibin Zhang, Xiangguo Sun, Yanwei Yue, Chonghe Jiang, Kun Wang, Tianlong Chen, Shirui Pan |  |
| 434 |  |  [Realistic Evaluation of Deep Partial-Label Learning Algorithms](https://openreview.net/forum?id=FtX6oAW7Dd) |  | 0 | Partial-label learning (PLL) is a weakly supervised learning problem in which each example is associated with multiple candidate labels and only one is the true label. In recent years, many deep PLL algorithms have been developed to improve model performance. However, we find that some early developed algorithms are often underestimated and can outperform many later algorithms with complicated designs. In this paper, we delve into the empirical perspective of PLL and identify several critical but previously overlooked issues. First, model selection for PLL is non-trivial, but has never been systematically studied. Second, the experimental settings are highly inconsistent, making it difficult to evaluate the effectiveness of the algorithms. Third, there is a lack of real-world image datasets that can be compatible with modern network architectures. Based on these findings, we propose PLENCH, the first Partial-Label learning bENCHmark to systematically compare state-of-the-art deep PLL algorithms. We investigate the model selection problem for PLL for the first time, and propose novel model selection criteria with theoretical guarantees. We also create Partial-Label CIFAR-10 (PLCIFAR10), an image dataset of human-annotated partial labels collected from Amazon Mechanical Turk, to provide a testbed for evaluating the performance of PLL algorithms in more realistic scenarios. Researchers can quickly and conveniently perform a comprehensive and fair evaluation and verify the effectiveness of newly developed algorithms based on PLENCH. We hope that PLENCH will facilitate standardized, fair, and practical evaluation of PLL algorithms in the future. | Wei Wang, DongDong Wu, Jindong Wang, Gang Niu, MinLing Zhang, Masashi Sugiyama |  |
| 435 |  |  [BodyGen: Advancing Towards Efficient Embodiment Co-Design](https://openreview.net/forum?id=cTR17xl89h) |  | 0 | Embodiment co-design aims to optimize a robot's morphology and control policy simultaneously. While prior work has demonstrated its potential for generating environment-adaptive robots, this field still faces persistent challenges in optimization efficiency due to the (i) combinatorial nature of morphological search spaces and (ii) intricate dependencies between morphology and control. We prove that the ineffective morphology representation and unbalanced reward signals between the design and control stages are key obstacles to efficiency. To advance towards efficient embodiment co-design, we propose \*\*BodyGen\*\*, which utilizes (1) topology-aware self-attention for both design and control, enabling efficient morphology representation with lightweight model sizes; (2) a temporal credit assignment mechanism that ensures balanced reward signals for optimization. With our findings, BodyGen achieves an average \*\*60.03%\*\* performance improvement against state-of-the-art baselines. We provide codes and more results on the website: https://genesisorigin.github.io. | Haofei Lu, Zhe Wu, Junliang Xing, Jianshu Li, Ruoyu Li, Zhe Li, Yuanchun Shi |  |
| 436 |  |  [RAG-SR: Retrieval-Augmented Generation for Neural Symbolic Regression](https://openreview.net/forum?id=NdHka08uWn) |  | 0 | Symbolic regression is a key task in machine learning, aiming to discover mathematical expressions that best describe a dataset. While deep learning has increased interest in using neural networks for symbolic regression, many existing approaches rely on pre-trained models. These models require significant computational resources and struggle with regression tasks involving unseen functions and variables. A pre-training-free paradigm is needed to better integrate with search-based symbolic regression algorithms. To address these limitations, we propose a novel framework for symbolic regression that integrates evolutionary feature construction with a neural network, without the need for pre-training. Our approach adaptively generates symbolic trees that align with the desired semantics in real-time using a language model trained via online supervised learning, providing effective building blocks for feature construction. To mitigate hallucinations from the language model, we design a retrieval-augmented generation mechanism that explicitly leverages searched symbolic expressions. Additionally, we introduce a scale-invariant data augmentation technique that further improves the robustness and generalization of the model. Experimental results demonstrate that our framework achieves state-of-the-art accuracy across 25 regression algorithms and 120 regression tasks. | Hengzhe Zhang, Qi Chen, Bing Xue, Wolfgang Banzhaf, Mengjie Zhang |  |
| 437 |  |  [Theory on Mixture-of-Experts in Continual Learning](https://openreview.net/forum?id=7XgKAabsPp) |  | 0 | Continual learning (CL) has garnered significant attention because of its ability to adapt to new tasks that arrive over time. Catastrophic forgetting (of old tasks) has been identified as a major issue in CL, as the model adapts to new tasks. The Mixture-of-Experts (MoE) model has recently been shown to effectively mitigate catastrophic forgetting in CL, by employing a gating network to sparsify and distribute diverse tasks among multiple experts. However, there is a lack of theoretical analysis of MoE and its impact on the learning performance in CL. This paper provides the first theoretical results to characterize the impact of MoE in CL via the lens of overparameterized linear regression tasks. We establish the benefit of MoE over a single expert by proving that the MoE model can diversify its experts to specialize in different tasks, while its router learns to select the right expert for each task and balance the loads across all experts. Our study further suggests an intriguing fact that the MoE in CL needs to terminate the update of the gating network after sufficient training rounds to attain system convergence, which is not needed in the existing MoE studies that do not consider the continual task arrival. Furthermore, we provide explicit expressions for the expected forgetting and overall generalization error to characterize the benefit of MoE in the learning performance in CL. Interestingly, adding more experts requires additional rounds before convergence, which may not enhance the learning performance. Finally, we conduct experiments on both synthetic and real datasets to extend these insights from linear models to deep neural networks (DNNs), which also shed light on the practical algorithm design for MoE in CL. | Hongbo Li, Sen Lin, Lingjie Duan, Yingbin Liang, Ness B. Shroff |  |
| 438 |  |  [Decentralized Sporadic Federated Learning: A Unified Algorithmic Framework with Convergence Guarantees](https://openreview.net/forum?id=cznqgb4DNv) |  | 0 | Decentralized federated learning (DFL) captures FL settings where both (i) model updates and (ii) model aggregations are exclusively carried out by the clients without a central server. Existing DFL works have mostly focused on settings where clients conduct a fixed number of local updates between local model exchanges, overlooking heterogeneity and dynamics in communication and computation capabilities. In this work, we propose Decentralized Sporadic Federated Learning ($\texttt{DSpodFL}$), a DFL methodology built on a generalized notion of \*sporadicity\* in both local gradient and aggregation processes. $\texttt{DSpodFL}$ subsumes many existing decentralized optimization methods under a unified algorithmic framework by modeling the per-iteration (i) occurrence of gradient descent at each client and (ii) exchange of models between client pairs as arbitrary indicator random variables, thus capturing \*heterogeneous and time-varying\* computation/communication scenarios. We analytically characterize the convergence behavior of $\texttt{DSpodFL}$ for both convex and non-convex models and for both constant and diminishing learning rates, under mild assumptions on the communication graph connectivity, data heterogeneity across clients, and gradient noises. We show how our bounds recover existing results from decentralized gradient descent as special cases. Experiments demonstrate that $\texttt{DSpodFL}$ consistently achieves improved training speeds compared with baselines under various system settings. | Shahryar Zehtabi, DongJun Han, Rohit Parasnis, Seyyedali Hosseinalipour, Christopher G. Brinton |  |
| 439 |  |  [MAD-TD: Model-Augmented Data stabilizes High Update Ratio RL](https://openreview.net/forum?id=6RtRsg8ZV1) |  | 0 | Building deep reinforcement learning (RL) agents that find a good policy with few samples has proven notoriously challenging. To achieve sample efficiency, recent work has explored updating neural networks with large numbers of gradient steps for every new sample. While such high update-to-data (UTD) ratios have shown strong empirical performance, they also introduce instability to the training process. Previous approaches need to rely on periodic neural network parameter resets to address this instability, but restarting the training process is infeasible in many real-world applications and requires tuning the resetting interval. In this paper, we focus on one of the core difficulties of stable training with limited samples: the inability of learned value functions to generalize to unobserved on-policy actions. We mitigate this issue directly by augmenting the off-policy RL training process with a small amount of data generated from a learned world model. Our method, Model-Augmented Data for TD Learning (MAD-TD) uses small amounts of generated data to stabilize high UTD training and achieve competitive performance on the most challenging tasks in the DeepMind control suite. Our experiments further highlight the importance of employing a good model to generate data, MAD-TD's ability to combat value overestimation, and its practical stability gains for continued learning. | Claas Voelcker, Marcel Hussing, Eric Eaton, Amirmassoud Farahmand, Igor Gilitschenski |  |
| 440 |  |  [Linear Mode Connectivity in Differentiable Tree Ensembles](https://openreview.net/forum?id=UqYNPyotxL) |  | 0 | Linear Mode Connectivity (LMC) refers to the phenomenon that performance remains consistent for linearly interpolated models in the parameter space. For independently optimized model pairs from different random initializations, achieving LMC is considered crucial for understanding the stable success of the non-convex optimization in modern machine learning models and for facilitating practical parameter-based operations such as model merging. While LMC has been achieved for neural networks by considering the permutation invariance of neurons in each hidden layer, its attainment for other models remains an open question. In this paper, we first achieve LMC for soft tree ensembles, which are tree-based differentiable models extensively used in practice. We show the necessity of incorporating two invariances: subtree flip invariance and splitting order invariance, which do not exist in neural networks but are inherent to tree architectures, in addition to permutation invariance of trees. Moreover, we demonstrate that it is even possible to exclude such additional invariances while keeping LMC by designing decision list-based tree architectures, where such invariances do not exist by definition. Our findings indicate the significance of accounting for architecture-specific invariances in achieving LMC. | Ryuichi Kanoh, Mahito Sugiyama |  |
| 441 |  |  [Overcoming False Illusions in Real-World Face Restoration with Multi-Modal Guided Diffusion Model](https://openreview.net/forum?id=m9RNBZewW2) |  | 0 | We introduce a novel Multi-modal Guided Real-World Face Restoration (MGFR) technique designed to improve the quality of facial image restoration from low-quality inputs. Leveraging a blend of attribute text prompts, high-quality reference images, and identity information, MGFR can mitigate the generation of false facial attributes and identities often associated with generative face restoration methods. By incorporating a dual-control adapter and a two-stage training strategy, our method effectively utilizes multi-modal prior information for targeted restoration tasks. We also present the Reface-HQ dataset, comprising over 21,000 high-resolution facial images across 4800 identities, to address the need for reference face training images. Our approach achieves superior visual quality in restoring facial details under severe degradation and allows for controlled restoration processes, enhancing the accuracy of identity preservation and attribute correction. Including negative quality samples and attribute prompts in the training further refines the model's ability to generate detailed and perceptually accurate images. | Keda Tao, Jinjin Gu, Yulun Zhang, Xiucheng Wang, Nan Cheng |  |
| 442 |  |  [DEEM: Diffusion models serve as the eyes of large language models for image perception](https://openreview.net/forum?id=qtWjSboqfe) |  | 0 | The development of large language models (LLMs) has significantly advanced the emergence of large multimodal models (LMMs). While LMMs have achieved tremendous success by promoting the synergy between multimodal comprehension and creation, they often face challenges when confronted with out-of-distribution data, such as which can hardly distinguish orientation, quantity, color, structure, etc. This is primarily due to their reliance on image encoders trained to encode images into task-relevant features, which may lead them to disregard irrelevant details. Delving into the modeling capabilities of diffusion models for images naturally prompts the question: Can diffusion models serve as the eyes of large language models for image perception? In this paper, we propose DEEM, a simple but effective approach that utilizes the generative feedback of diffusion models to align the semantic distributions of the image encoder. This addresses the drawbacks of previous methods that solely relied on image encoders like CLIP-ViT, thereby enhancing the model's resilience against out-of-distribution samples and reducing visual hallucinations. Importantly, this is achieved without requiring additional training modules and with fewer training parameters. We extensively evaluated DEEM on both our newly constructed RobustVQA benchmark and other well-known benchmarks, POPE and MMVP, for visual hallucination and perception. In particular, DEEM improves LMM's visual perception performance to a large extent (e.g., 4\% ↑ on RobustVQA, 6.5\% ↑ on MMVP and 12.8 \% ↑ on POPE ). Compared to the state-of-the-art interleaved content generation models, DEEM exhibits enhanced robustness and a superior capacity to alleviate model hallucinations while utilizing fewer trainable parameters, less pre-training data (10\%), and a smaller base model size. Extensive experiments demonstrate that DEEM enhances the performance of LMMs on various downstream tasks without inferior performance in the long term, including visual question answering, image captioning, and text-conditioned image synthesis. | Run Luo, Yunshui Li, Longze Chen, Wanwei He, TingEn Lin, Ziqiang Liu, Lei Zhang, Zikai Song, Hamid Rokny, Xiaobo Xia, Tongliang Liu, Binyuan Hui, Min Yang |  |
| 443 |  |  [Stabilizing Reinforcement Learning in Differentiable Multiphysics Simulation](https://openreview.net/forum?id=DRiLWb8bJg) |  | 0 | Recent advances in GPU-based parallel simulation have enabled practitioners to collect large amounts of data and train complex control policies using deep reinforcement learning (RL), on commodity GPUs. However, such successes for RL in robotics have been limited to tasks sufficiently simulated by fast rigid-body dynamics. Simulation techniques for soft bodies are comparatively several orders of magnitude slower, thereby limiting the use of RL due to sample complexity requirements. To address this challenge, this paper presents both a novel RL algorithm and a simulation platform to enable scaling RL on tasks involving rigid bodies and deformables. We introduce Soft Analytic Policy Optimization (SAPO), a maximum entropy first-order model-based actor-critic RL algorithm, which uses first-order analytic gradients from differentiable simulation to train a stochastic actor to maximize expected return and entropy. Alongside our approach, we develop Rewarped, a parallel differentiable multiphysics simulation platform that supports simulating various materials beyond rigid bodies. We re-implement challenging manipulation and locomotion tasks in Rewarped, and show that SAPO outperforms baselines over a range of tasks that involve interaction between rigid bodies, articulations, and deformables. Additional details at https://rewarped.github.io/. | Eliot Xing, Vernon Luk, Jean Oh |  |
| 444 |  |  [Credal Wrapper of Model Averaging for Uncertainty Estimation in Classification](https://openreview.net/forum?id=cv2iMNWCsh) |  | 0 | This paper presents an innovative approach, called credal wrapper, to formulating a credal set representation of model averaging for Bayesian neural networks (BNNs) and deep ensembles (DEs), capable of improving uncertainty estimation in classification tasks. Given a finite collection of single predictive distributions derived from BNNs or DEs, the proposed credal wrapper approach extracts an upper and a lower probability bound per class, acknowledging the epistemic uncertainty due to the availability of a limited amount of distributions. Such probability intervals over classes can be mapped on a convex set of probabilities (a credal set) from which, in turn, a unique prediction can be obtained using a transformation called intersection probability transformation. In this article, we conduct extensive experiments on several out-of-distribution (OOD) detection benchmarks, encompassing various dataset pairs (CIFAR10/100 vs SVHN/Tiny-ImageNet, CIFAR10 vs CIFAR10-C, CIFAR100 vs CIFAR100-C and ImageNet vs ImageNet-O) and using different network architectures (such as VGG16, ResNet-18/50, EfficientNet B2, and ViT Base). Compared to the BNN and DE baselines, the proposed credal wrapper method exhibits superior performance in uncertainty estimation and achieves a lower expected calibration error on corrupted data. | Kaizheng Wang, Fabio Cuzzolin, Keivan Shariatmadar, David Moens, Hans Hallez |  |
| 445 |  |  [Discovering Temporally Compositional Neural Manifolds with Switching Infinite GPFA](https://openreview.net/forum?id=2iCIHgE8KG) |  | 0 | Gaussian Process Factor Analysis (GPFA) is a powerful latent variable model for extracting low-dimensional manifolds underlying population neural activities. However, one limitation of standard GPFA models is that the number of latent factors needs to be pre-specified or selected through heuristic-based processes, and that all factors contribute at all times. We propose the infinite GPFA model, a fully Bayesian non-parametric extension of the classical GPFA by incorporating an Indian Buffet Process (IBP) prior over the factor loading process, such that it is possible to infer a potentially infinite set of latent factors, and the identity of those factors that contribute to neural firings in a compositional manner at \textit{each} time point. Learning and inference in the infinite GPFA model is performed through variational expectation-maximisation, and we additionally propose scalable extensions based on sparse variational Gaussian Process methods. We empirically demonstrate that the infinite GPFA model correctly infers dynamically changing activations of latent factors on a synthetic dataset. By fitting the infinite GPFA model to population activities of hippocampal place cells during spatial tasks with alternating random foraging and spatial memory phases, we identify novel non-trivial and behaviourally meaningful dynamics in the neural encoding process. | Changmin Yu, Maneesh Sahani, Máté Lengyel |  |
| 446 |  |  [Presto! Distilling Steps and Layers for Accelerating Music Generation](https://openreview.net/forum?id=Gj5JTAwdoy) |  | 0 | Despite advances in diffusion-based text-to-music (TTM) methods, efficient, high-quality generation remains a challenge. We introduce Presto!, an approach to inference acceleration for score-based diffusion transformers via reducing both sampling steps and cost per step. To reduce steps, we develop a new score-based distribution matching distillation (DMD) method for the EDM-family of diffusion models, the first GAN-based distillation method for TTM. To reduce the cost per step, we develop a simple, but powerful improvement to a recent layer distillation method that improves learning via better preserving hidden state variance. Finally, we combine our step and layer distillation methods together for a dual-faceted approach. We evaluate our step and layer distillation methods independently and show each yield best-in-class performance. Our combined distillation method can generate high-quality outputs with improved diversity, accelerating our base model by 10-18x (230/435ms latency for 32 second mono/stereo 44.1kHz, 15x faster than the comparable SOTA model) — the fastest TTM to our knowledge. | Zachary Novack, Ge Zhu, Jonah Casebeer, Julian J. McAuley, Taylor BergKirkpatrick, Nicholas J. Bryan |  |
| 447 |  |  [Grounding Video Models to Actions through Goal Conditioned Exploration](https://openreview.net/forum?id=G6dMvRuhFr) |  | 0 | Large video models, pretrained on massive quantities of amount of Internet video, provide a rich source of physical knowledge about the dynamics and motions of objects and tasks. However, video models are not grounded in the embodiment of an agent, and do not describe how to actuate the world to reach the visual states depicted in a video. To tackle this problem, current methods use a separate vision-based inverse dynamic model trained on embodiment-specific data to map image states to actions. Gathering data to train such a model is often expensive and challenging, and this model is limited to visual settings similar to the ones in which data is available. In this paper, we investigate how to directly ground video models to continuous actions through self-exploration in the embodied environment -- using generated video states as visual goals for exploration. We propose a framework that uses trajectory level action generation in combination with video guidance to enable an agent to solve complex tasks without any external supervision, e.g., rewards, action labels, or segmentation masks. We validate the proposed approach on 8 tasks in Libero, 6 tasks in MetaWorld, 4 tasks in Calvin, and 12 tasks in iThor Visual Navigation. We show how our approach is on par with or even surpasses multiple behavior cloning baselines trained on expert demonstrations while without requiring any action annotations. | Yunhao Luo, Yilun Du |  |
| 448 |  |  [RESuM: A Rare Event Surrogate Model for Physics Detector Design](https://openreview.net/forum?id=lqTILjL6lP) |  | 0 | The experimental discovery of neutrinoless double-beta decay (NLDBD) would answer one of the most important questions in physics: Why is there more matter than antimatter in our universe? To maximize the chances of discovery, NLDBD experiments must optimize their detector designs to minimize the probability of background events contaminating the detector. Given that this probability is inherently low, design optimization either requires extremely costly simulations to generate sufficient background counts or contending with significant variance. In this work, we formalize this dilemma as a Rare Event Design (RED) problem: identifying optimal design parameters when the design metric to be minimized is inherently small. We then designed the Rare Event Surrogate Model (RESuM) for physics detector design optimization under RED conditions. RESuM uses a pre-trained Conditional Neural Process (CNP) model to incorporate additional prior knowledge into a Multi-Fidelity Gaussian Process model. We applied RESuM to optimize neutron shielding designs for the LEGEND NLDBD experiment, identifying an optimal design that reduces the neutron background by $(66.5 \pm 3.5)$% while using only 3.3% of the computational resources compared to traditional methods. Given the prevalence of RED problems in other fields of physical sciences, especially in rare-event searches, the RESuM algorithm has broad potential for accelerating simulation-intensive applications. | AnnKathrin Schuetz, A. W. P. Poon, Aobo Li |  |
| 449 |  |  [Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in Language Models](https://openreview.net/forum?id=GjM61KRiTG) |  | 0 | Fine-tuning large language models (LLMs) on human preferences, typically through reinforcement learning from human feedback (RLHF), has proven successful in enhancing their capabilities. However, ensuring the safety of LLMs during fine-tuning remains a critical concern, and mitigating the potential conflicts in safety and helpfulness is costly in RLHF. To address this issue, we propose a supervised learning framework called Bi-Factorial Preference Optimization (BFPO), which re-parameterizes a joint RLHF objective of both safety and helpfulness into a single supervised learning objective. In the supervised optimization, a labeling function is used to capture global preferences ranking to balance both safety and helpfulness. To evaluate BFPO, we develop a benchmark including comprehensive discriminative and generative tasks for helpfulness and harmlessness. The results indicate that our method significantly outperforms existing approaches in both safety and helpfulness. Moreover, BFPO eliminates the need for human prompting and annotation in LLM fine-tuning while achieving the same level of safety as methods that heavily rely on human labor, with less than 10\% of the computational resources. The training recipes and models will be released. | Wenxuan Zhang, Philip Torr, Mohamed Elhoseiny, Adel Bibi |  |
| 450 |  |  [Enhancing the Scalability and Applicability of Kohn-Sham Hamiltonians for Molecular Systems](https://openreview.net/forum?id=twEvvkQqPS) |  | 0 | Density Functional Theory (DFT) is a pivotal method within quantum chemistry and materials science, with its core involving the construction and solution of the Kohn-Sham Hamiltonian. Despite its importance, the application of DFT is frequently limited by the substantial computational resources required to construct the Kohn-Sham Hamiltonian. In response to these limitations, current research has employed deep-learning models to efficiently predict molecular and solid Hamiltonians, with roto-translational symmetries encoded in their neural networks. However, the scalability of prior models may be problematic when applied to large molecules, resulting in non-physical predictions of ground-state properties. In this study, we generate a substantially larger training set (PubChemQH) than used previously and use it to create a scalable model for DFT calculations with physical accuracy. For our model, we introduce a loss function derived from physical principles, which we call Wavefunction Alignment Loss (WALoss). WALoss involves performing a basis change on the predicted Hamiltonian to align it with the observed one; thus, the resulting differences can serve as a surrogate for orbital energy differences, allowing models to make better predictions for molecular orbitals and total energies than previously possible. WALoss also substantially accelerates self-consistent-field (SCF) DFT calculations. Here, we show it achieves a reduction in total energy prediction error by a factor of 1347 and an SCF calculation speed-up by a factor of 18\%. These substantial improvements set new benchmarks for achieving accurate and applicable predictions in larger molecular systems. | Yunyang Li, Zaishuo Xia, Lin Huang, Xinran Wei, Samuel Harshe, Han Yang, Erpai Luo, Zun Wang, Jia Zhang, Chang Liu, Bin Shao, Mark Gerstein |  |
| 451 |  |  [Dense Video Object Captioning from Disjoint Supervision](https://openreview.net/forum?id=auZZ2gN0ZN) |  | 0 | We propose a new task and model for dense video object captioning -- detecting, tracking and captioning trajectories of objects in a video. This task unifies spatial and temporal localization in video, whilst also requiring fine-grained visual understanding that is best described by natural language. We propose a unified model, and demonstrate how our end-to-end approach is more accurate and temporally coherent than a multi-stage pipeline combining state-of-the-art detection, tracking, and captioning models. Moreover, we propose a training strategy based on a mixture of disjoint tasks, which allows us to leverage diverse, large-scale datasets which supervise different parts of our model. Although each pretraining task only provides weak supervision, they are complementary and, when combined, result in noteworthy zero-shot ability and serve as strong initialization for additional finetuning to further improve accuracy. We carefully design new metrics capturing all components of our task, and show how we can repurpose existing video grounding datasets (e.g. VidSTG and VLN) for our new task. We show that our model improves upon a number of strong baselines for this new task. Furthermore, we can apply our model to the task of spatial grounding, outperforming prior state-of-the-art on VidSTG and VLN, without explicitly training for it. Our code is available at https://github.com/google-research/scenic. | Xingyi Zhou, Anurag Arnab, Chen Sun, Cordelia Schmid |  |
| 452 |  |  [SplatFormer: Point Transformer for Robust 3D Gaussian Splatting](https://openreview.net/forum?id=9NfHbWKqMF) |  | 0 | 3D Gaussian Splatting (3DGS) has recently transformed photorealistic reconstruction, achieving high visual fidelity and real-time performance. However, rendering quality significantly deteriorates when test views deviate from the camera angles used during training, posing a major challenge for applications in immersive free-viewpoint rendering and navigation. In this work, we conduct a comprehensive evaluation of 3DGS and related novel view synthesis methods under out-of-distribution (OOD) test camera scenarios. By creating diverse test cases with synthetic and real-world datasets, we demonstrate that most existing methods, including those incorporating various regularization techniques and data-driven priors, struggle to generalize effectively to OOD views. To address this limitation, we introduce SplatFormer, the first point transformer model specifically designed to operate on Gaussian splats. SplatFormer takes as input an initial 3DGS set optimized under limited training views and refines it in a single forward pass, effectively removing potential artifacts in OOD test views. To our knowledge, this is the first successful application of point transformers directly on 3DGS sets, surpassing the limitations of previous multi-scene training methods, which could handle only a restricted number of input views during inference. Our model significantly improves rendering quality under extreme novel views, achieving state-of-the-art performance in these challenging scenarios and outperforming various 3DGS regularization techniques, multi-scene models tailored for sparse view synthesis, and diffusion-based frameworks. The project url is https://sergeyprokudin.github.io/splatformer. | Yutong Chen, Marko Mihajlovic, Xiyi Chen, Yiming Wang, Sergey Prokudin, Siyu Tang |  |
| 453 |  |  [DeLLMa: Decision Making Under Uncertainty with Large Language Models](https://openreview.net/forum?id=Acvo2RGSCy) |  | 0 | The potential of large language models (LLMs) as decision support tools is increasingly being explored in fields such as business, engineering, and medicine, which often face challenging tasks of \*decision-making under uncertainty\*. In this paper, we show that directly prompting LLMs on these types of decision-making problems can yield poor results, especially as the problem complexity increases. To aid in these tasks, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step reasoning procedure that integrates recent best practices in scaling \*inference-time reasoning\*, drawing upon principles from decision theory and utility theory, to provide an accurate and human-auditable decision-making process. We validate our procedure on multiple realistic decision-making environments, demonstrating that DeLLMa can consistently enhance the decision-making performance of leading language models, and achieve up to a 40% increase in accuracy over competing methods. Additionally, we show how performance improves when scaling compute at test time, and carry out human evaluations to benchmark components of DeLLMa. | Ollie Liu, Deqing Fu, Dani Yogatama, Willie Neiswanger |  |
| 454 |  |  [OmniRe: Omni Urban Scene Reconstruction](https://openreview.net/forum?id=11xgiMEI5o) |  | 0 | We introduce OmniRe, a comprehensive system for efficiently creating high-fidelity digital twins of dynamic real-world scenes from on-device logs. Recent methods using neural fields or Gaussian Splatting primarily focus on vehicles, hindering a holistic framework for all dynamic foregrounds demanded by downstream applications, e.g., the simulation of human behavior. OmniRe extends beyond vehicle modeling to enable accurate, full-length reconstruction of diverse dynamic objects in urban scenes. Our approach builds scene graphs on 3DGS and constructs multiple Gaussian representations in canonical spaces that model various dynamic actors, including vehicles, pedestrians, cyclists, and others. OmniRe allows holistically reconstructing any dynamic object in the scene, enabling advanced simulations (~60 Hz) that include human-participated scenarios, such as pedestrian behavior simulation and human-vehicle interaction. This comprehensive simulation capability is unmatched by existing methods. Extensive evaluations on the Waymo dataset show that our approach outperforms prior state-of-the-art methods quantitatively and qualitatively by a large margin. We further extend our results to 5 additional popular driving datasets to demonstrate its generalizability on common urban scenes. Code and results are available at [omnire](https://ziyc.github.io/omnire/). | Ziyu Chen, Jiawei Yang, Jiahui Huang, Riccardo de Lutio, Janick Martinez Esturo, Boris Ivanovic, Or Litany, Zan Gojcic, Sanja Fidler, Marco Pavone, Li Song, Yue Wang |  |
| 455 |  |  [ZAPBench: A Benchmark for Whole-Brain Activity Prediction in Zebrafish](https://openreview.net/forum?id=oCHsDpyawq) |  | 0 | Data-driven benchmarks have led to significant progress in key scientific modeling domains including weather and structural biology. Here, we introduce the Zebrafish Activity Prediction Benchmark (ZAPBench) to measure progress on the problem of predicting cellular-resolution neural activity throughout an entire vertebrate brain. The benchmark is based on a novel dataset containing 4d light-sheet microscopy recordings of over 70,000 neurons in a larval zebrafish brain, along with motion stabilized and voxel-level cell segmentations of these data that facilitate development of a variety of forecasting methods. Initial results from a selection of time series and volumetric video modeling approaches achieve better performance than naive baseline methods, but also show room for further improvement. The specific brain used in the activity recording is also undergoing synaptic-level anatomical mapping, which will enable future integration of detailed structural information into forecasting methods. | JanMatthis Lueckmann, Alexander Immer, Alex BoYuan Chen, Peter H. Li, Mariela D. Petkova, Nirmala A. Iyer, Luuk Willem Hesselink, Aparna Dev, Gudrun Ihrke, Woohyun Park, Alyson Petruncio, Aubrey Weigel, Wyatt Korff, Florian Engert, Jeff Lichtman, Misha B. Ahrens, Michal Januszewski, Viren Jain |  |
| 456 |  |  [Lumina-T2X: Scalable Flow-based Large Diffusion Transformer for Flexible Resolution Generation](https://openreview.net/forum?id=EbWf36quzd) |  | 0 | Sora unveils the potential of scaling Diffusion Transformer (DiT) for generating photorealistic images and videos at arbitrary resolutions, aspect ratios, and durations, yet it still lacks sufficient implementation details. In this paper, we introduce the Lumina-T2X family -- a series of Flow-based Large Diffusion Transformers (Flag-DiT) equipped with zero-initialized attention, as a simple and scalable generative framework that can be adapted to various modalities, e.g., transforming noise into images, videos, multi-view 3D objects, or audio clips conditioned on text instructions. By tokenizing the latent spatial-temporal space and incorporating learnable placeholders such as \|[nextline]\| and \|[nextframe]\| tokens, Lumina-T2X seamlessly unifies the representations of different modalities across various spatial-temporal resolutions. Advanced techniques like RoPE, KQ-Norm, and flow matching enhance the stability, flexibility, and scalability of Flag-DiT, enabling models of Lumina-T2X to scale up to 7 billion parameters and extend the context window to 128K tokens. This is particularly beneficial for creating ultra-high-definition images with our Lumina-T2I model and long 720p videos with our Lumina-T2V model. Remarkably, Lumina-T2I, powered by a 5-billion-parameter Flag-DiT, requires only 35% of the training computational costs of a 600-million-parameter naive DiT (PixArt-alpha), indicating that increasing the number of parameters significantly accelerates convergence of generative models without compromising visual quality. Our further comprehensive analysis underscores Lumina-T2X's preliminary capability in resolution extrapolation, high-resolution editing, generating consistent 3D views, and synthesizing videos with seamless transitions. All code and checkpoints of Lumina-T2X are released at https://github.com/Alpha-VLLM/Lumina-T2X to further foster creativity, transparency, and diversity in the generative AI community. | Peng Gao, Le Zhuo, Dongyang Liu, Ruoyi Du, Xu Luo, Longtian Qiu, Yuhang Zhang, Rongjie Huang, Shijie Geng, Renrui Zhang, Junlin Xie, Wenqi Shao, Zhengkai Jiang, Tianshuo Yang, Weicai Ye, Tong He, Jingwen He, Junjun He, Yu Qiao, Hongsheng Li |  |
| 457 |  |  [A Periodic Bayesian Flow for Material Generation](https://openreview.net/forum?id=Lz0XW99tE0) |  | 0 | Generative modeling of crystal data distribution is an important yet challenging task due to the unique periodic physical symmetry of crystals. Diffusion-based methods have shown early promise in modeling crystal distribution. More recently, Bayesian Flow Networks were introduced to aggregate noisy latent variables, resulting in a variance-reduced parameter space that has been shown to be advantageous for modeling Euclidean data distributions with structural constraints (Song, et al.,2023). Inspired by this, we seek to unlock its potential for modeling variables located in non-Euclidean manifolds e.g. those within crystal structures, by overcoming challenging theoretical issues. We introduce CrysBFN, a novel crystal generation method by proposing a periodic Bayesian flow, which essentially differs from the original Gaussian-based BFN by exhibiting non-monotonic entropy dynamics. To successfully realize the concept of periodic Bayesian flow, CrysBFN integrates a new entropy conditioning mechanism and empirically demonstrates its significance compared to time-conditioning. Extensive experiments over both crystal ab initio generation and crystal structure prediction tasks demonstrate the superiority of CrysBFN, which consistently achieves new state-of-the-art on all benchmarks. Surprisingly, we found that CrysBFN enjoys a significant improvement in sampling efficiency, e.g., 200x speedup (10 v.s. 2000 steps network forwards) compared with previous Diffusion-based methods on MP-20 dataset. | Hanlin Wu, Yuxuan Song, Jingjing Gong, Ziyao Cao, Yawen Ouyang, Jianbing Zhang, Hao Zhou, WeiYing Ma, Jingjing Liu |  |
| 458 |  |  [DiffPuter: Empowering Diffusion Models for Missing Data Imputation](https://openreview.net/forum?id=3fl1SENSYO) |  | 0 | Generative models play an important role in missing data imputation in that they aim to learn the joint distribution of full data. However, applying advanced deep generative models (such as Diffusion models) to missing data imputation is challenging due to 1) the inherent incompleteness of the training data and 2) the difficulty in performing conditional inference from unconditional generative models. To deal with these challenges, this paper introduces DiffPuter, a tailored diffusion model combined with the Expectation-Maximization (EM) algorithm for missing data imputation. DiffPuter iteratively trains a diffusion model to learn the joint distribution of missing and observed data and performs an accurate conditional sampling to update the missing values using a tailored reversed sampling strategy. Our theoretical analysis shows that DiffPuter's training step corresponds to the maximum likelihood estimation of data density (M-step), and its sampling step represents the Expected A Posteriori estimation of missing values (E-step). Extensive experiments across ten diverse datasets and comparisons with 17 different imputation methods demonstrate DiffPuter's superior performance. Notably, DiffPuter achieves an average improvement of 8.10\% in MAE and 5.64\% in RMSE compared to the most competitive existing method. | Hengrui Zhang, Liancheng Fang, Qitian Wu, Philip S. Yu |  |
| 459 |  |  [Towards Marginal Fairness Sliced Wasserstein Barycenter](https://openreview.net/forum?id=NQqJPPCesd) |  | 0 | The Sliced Wasserstein barycenter (SWB) is a widely acknowledged method for efficiently generalizing the averaging operation within probability measure spaces. However, achieving marginal fairness SWB, ensuring approximately equal distances from the barycenter to marginals, remains unexplored. The uniform weighted SWB is not necessarily the optimal choice to obtain the desired marginal fairness barycenter due to the heterogeneous structure of marginals and the non-optimality of the optimization. As the first attempt to tackle the problem, we define the marginal fairness sliced Wasserstein barycenter (MFSWB) as a constrained SWB problem. Due to the computational disadvantages of the formal definition, we propose two hyperparameter-free and computationally tractable surrogate MFSWB problems that implicitly minimize the distances to marginals and encourage marginal fairness at the same time. To further improve the efficiency, we perform slicing distribution selection and obtain the third surrogate definition by introducing a new slicing distribution that focuses more on marginally unfair projecting directions. We discuss the relationship of the three proposed problems and their relationship to sliced multi-marginal Wasserstein distance. Finally, we conduct experiments on finding 3D point-clouds averaging, color harmonization, and training of sliced Wasserstein autoencoder with class-fairness representation to show the favorable performance of the proposed surrogate MFSWB problems. | Khai Nguyen, Hai Nguyen, Nhat Ho |  |
| 460 |  |  [Continuous Exposure Learning for Low-light Image Enhancement using Neural ODEs](https://openreview.net/forum?id=Mn2qgIcIPS) |  | 0 | Low-light image enhancement poses a significant challenge due to the limited information captured by image sensors in low-light environments. Despite recent improvements in deep learning models, the lack of paired training datasets remains a significant obstacle. Therefore, unsupervised methods have emerged as a promising solution. In this work, we focus on the strength of curve-adjustment-based approaches to tackle unsupervised methods. The majority of existing unsupervised curve-adjustment approaches iteratively estimate higher order curve parameters to enhance the exposure of images while efficiently preserving the details of the images. However, the convergence of the enhancement procedure cannot be guaranteed, leading to sensitivity to the number of iterations and limited performance. To address this problem, we consider the iterative curve-adjustment update process as a dynamic system and formulate it as a Neural Ordinary Differential Equations (NODE) for the first time, and this allows us to learn a continuous dynamics of the latent image. The strategy of utilizing NODE to leverage continuous dynamics in iterative methods enhances unsupervised learning and aids in achieving better convergence compared to discrete-space approaches. Consequently, we achieve state-of-the-art performance in unsupervised low-light image enhancement across various benchmark datasets. | Donggoo Jung, Daehyun Kim, Tae Hyun Kim |  |
| 461 |  |  [Learning to Solve Differential Equation Constrained Optimization Problems](https://openreview.net/forum?id=VeMC6Bn0ZB) |  | 0 | Differential equations (DE) constrained optimization plays a critical role in numerous scientific and engineering fields, including energy systems, aerospace engineering, ecology, and finance, where optimal configurations or control strategies must be determined for systems governed by ordinary or stochastic differential equations. Despite its significance, the computational challenges associated with these problems have limited their practical use. To address these limitations, this paper introduces a learning-based approach to DE-constrained optimization that combines techniques from proxy optimization \citep{kotary2021end} and neural differential equations \citep{chen2019neural}. The proposed approach uses a dual-network architecture, with one approximating the control strategies, focusing on steady-state constraints, and another solving the associated DEs. This combination enables the approximation of optimal strategies while accounting for dynamic constraints in near real-time. Experiments across problems in energy optimization and finance modeling show that this method provides full compliance with dynamic constraints and it produces results up to 25 times more precise than other methods which do not explicitly model the system's dynamic equations. | Vincenzo Di Vito Francesco, Mostafa Mohammadian, Kyri Baker, Ferdinando Fioretto |  |
| 462 |  |  [Fast Uncovering of Protein Sequence Diversity from Structure](https://openreview.net/forum?id=1iuaxjssVp) |  | 0 | We present InvMSAFold, an inverse folding method for generating protein sequences optimized for diversity and speed. For a given structure, InvMSAFold generates the parameters of a pairwise probability distribution over the space of sequences, capturing the amino acid covariances observed in Multiple Sequence Alignments (MSA) of homologous proteins. This allows for the efficient generation of highly diverse protein sequences while preserving structural and functional integrity. We demonstrate that this increased diversity in sampled sequences translates into greater variability in biochemical properties, highlighting the exciting potential of our method for applications such as protein design. The orders of magnitude improvement in sampling speed compared to existing methods unlocks new possibilities for high-throughput in virtual screening. | Luca Alessandro Silva, Barthélémy MeynardPiganeau, Carlo Lucibello, Christoph Feinauer |  |
| 463 |  |  [Boosting Ray Search Procedure of Hard-label Attacks with Transfer-based Priors](https://openreview.net/forum?id=tIBAOcAvn4) |  | 0 | One of the most practical and challenging types of black-box adversarial attacks is the hard-label attack, where only the top-1 predicted label is available. One effective approach is to search for the optimal ray direction from the benign image that minimizes the $\ell_p$ norm distance to the adversarial region. The unique advantage of this approach is that it transforms the hard-label attack into a continuous optimization problem. The objective function value is the ray's radius, which can be obtained via binary search at a high query cost. Existing methods use a "sign trick" in gradient estimation to reduce the number of queries. In this paper, we theoretically analyze the quality of this gradient estimation and propose a novel prior-guided approach to improve ray search efficiency both theoretically and empirically. Specifically, we utilize the transfer-based priors from surrogate models, and our gradient estimators appropriately integrate them by approximating the projection of the true gradient onto the subspace spanned by these priors and random directions, in a query-efficient manner. We theoretically derive the expected cosine similarities between the obtained gradient estimators and the true gradient, and demonstrate the improvement achieved by incorporating priors. Extensive experiments on the ImageNet and CIFAR-10 datasets show that our approach significantly outperforms 11 state-of-the-art methods in terms of query efficiency. | Chen Ma, Xinjie Xu, Shuyu Cheng, Qi Xuan |  |
| 464 |  |  [3DIS: Depth-Driven Decoupled Image Synthesis for Universal Multi-Instance Generation](https://openreview.net/forum?id=MagmwodCAB) |  | 0 | The increasing demand for controllable outputs in text-to-image generation has spurred advancements in multi-instance generation (MIG), allowing users to define both instance layouts and attributes. However, unlike image-conditional generation methods such as ControlNet, MIG techniques have not been widely adopted in state-of-the-art models like SD2 and SDXL, primarily due to the challenge of building robust renderers that simultaneously handle instance positioning and attribute rendering. In this paper, we introduce Depth-Driven Decoupled Image Synthesis (3DIS), a novel framework that decouples the MIG process into two stages: (i) generating a coarse scene depth map for accurate instance positioning and scene composition, and (ii) rendering fine-grained attributes using pre-trained ControlNet on any foundational model, without additional training. Our 3DIS framework integrates a custom adapter into LDM3D for precise depth-based layouts and employs a finetuning-free method for enhanced instance-level attribute rendering. Extensive experiments on COCO-Position and COCO-MIG benchmarks demonstrate that 3DIS significantly outperforms existing methods in both layout precision and attribute rendering. Notably, 3DIS offers seamless compatibility with diverse foundational models, providing a robust, adaptable solution for advanced multi-instance generation. The code is available at: https://github.com/limuloo/3DIS. | Dewei Zhou, Ji Xie, Zongxin Yang, Yi Yang |  |
| 465 |  |  [Strong Model Collapse](https://openreview.net/forum?id=et5l9qPUhm) |  | 0 | Within the scaling laws paradigm, which underpins the training of large neural networks like ChatGPT and Llama, we consider a supervised regression setting and establish a strong form of the model collapse phenomenon, a critical performance degradation due to synthetic data in the training corpus. Our results show that even the smallest fraction of synthetic data (e.g., as little as 1 per 1000) can still lead to model collapse: larger and larger training sets do not enhance performance. We further investigate whether increasing model size, an approach aligned with current trends in training large language models, exacerbates or mitigates model collapse. In a simplified regime where neural networks are approximated via random projections of tunable size, we both theoretically and empirically show that larger models can amplify model collapse. Interestingly, our theory also indicates that, beyond the interpolation threshold (which can be extremely high for very large datasets), larger models may mitigate the collapse, although they do not entirely prevent it. Our theoretical findings are empirically verified through experiments on language models and neural networks for images. | Elvis Dohmatob, Yunzhen Feng, Arjun Subramonian, Julia Kempe |  |
| 466 |  |  [DRoP: Distributionally Robust Data Pruning](https://openreview.net/forum?id=fxv0FfmDAg) |  | 0 | In the era of exceptionally data-hungry models, careful selection of the training data is essential to mitigate the extensive costs of deep learning. Data pruning offers a solution by removing redundant or uninformative samples from the dataset, which yields faster convergence and improved neural scaling laws. However, little is known about its impact on classification bias of the trained models. We conduct the first systematic study of this effect and reveal that existing data pruning algorithms can produce highly biased classifiers. We present theoretical analysis of the classification risk in a mixture of Gaussians to argue that choosing appropriate class pruning ratios, coupled with random pruning within classes has potential to improve worst-class performance. We thus propose DRoP, a distributionally robust approach to pruning and empirically demonstrate its performance on standard computer vision benchmarks. In sharp contrast to existing algorithms, our proposed method continues improving distributional robustness at a tolerable drop of average performance as we prune more from the datasets. | Artem M. Vysogorets, Kartik Ahuja, Julia Kempe |  |
| 467 |  |  [Co3Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion](https://openreview.net/forum?id=VaowElpVzd) |  | 0 | Generating gestures from human speech has gained tremendous progress in animating virtual avatars. While the existing methods enable synthesizing gestures cooperated by people self-talking, they overlook the practicality of concurrent gesture modeling with two-person interactive conversations. Moreover, the lack of high-quality datasets with concurrent co-speech gestures also limits handling this issue. To fulfill this goal, we first construct a large-scale concurrent co-speech gesture dataset that contains more than 7M frames for diverse two-person interactive posture sequences, dubbed $\textbf{GES-Inter}$. Moreover, we propose Co$^{\mathbf{3}}$Gesture, a novel framework that enables concurrent coherent co-speech gesture synthesis including two-person interactive movements. Our framework is built upon two cooperative generation branches conditioned on decomposed speaker audio. Specifically, to enhance the coordination of human postures w.r.t corresponding speaker audios while interacting with the conversational partner, we present a Temporal-Interaction Module ($\textbf{TIM}$). TIM can effectively model the temporal association representation between two speakers' gesture sequences as interaction guidance and fuse it into the concurrent gesture generation. Then, we devise a mutual attention mechanism to further boost learning dependencies of interacted concurrent motions, thereby enabling us to generate vivid and coherent gestures. Extensive experiments demonstrate that our method outperforms the state-of-the-art models on our newly collected GES-Inter dataset. | Xingqun Qi, Yatian Wang, Hengyuan Zhang, Jiahao Pan, Wei Xue, Shanghang Zhang, Wenhan Luo, Qifeng Liu, Yike Guo |  |
| 468 |  |  [Following the Human Thread in Social Navigation](https://openreview.net/forum?id=M8OGl34Pmg) |  | 0 | The success of collaboration between humans and robots in shared environments relies on the robot's real-time adaptation to human motion. Specifically, in Social Navigation, the agent should be close enough to assist but ready to back up to let the human move freely, avoiding collisions. Human trajectories emerge as crucial cues in Social Navigation, but they are partially observable from the robot's egocentric view and computationally complex to process. We present the first Social Dynamics Adaptation model (SDA) based on the robot's state-action history to infer the social dynamics. We propose a two-stage Reinforcement Learning framework: the first learns to encode the human trajectories into social dynamics and learns a motion policy conditioned on this encoded information, the current status, and the previous action. Here, the trajectories are fully visible, i.e., assumed as privileged information. In the second stage, the trained policy operates without direct access to trajectories. Instead, the model infers the social dynamics solely from the history of previous actions and statuses in real-time. Tested on the novel Habitat 3.0 platform, SDA sets a novel state-of-the-art (SotA) performance in finding and following humans. The code can be found at https://github.com/L-Scofano/SDA. | Luca Scofano, Alessio Sampieri, Tommaso Campari, Valentino Sacco, Indro Spinelli, Lamberto Ballan, Fabio Galasso |  |
| 469 |  |  [Don't flatten, tokenize! Unlocking the key to SoftMoE's efficacy in deep RL](https://openreview.net/forum?id=8oCrlOaYcc) |  | 0 | The use of deep neural networks in reinforcement learning (RL) often suffers from performance degradation as model size increases. While soft mixtures of experts (SoftMoEs) have recently shown promise in mitigating this issue for online RL, the reasons behind their effectiveness remain largely unknown. In this work we provide an in-depth analysis identifying the key factors driving this performance gain. We discover the surprising result that tokenizing the encoder output, rather than the use of multiple experts, is what is behind the efficacy of SoftMoEs. Indeed, we demonstrate that even with an appropriately scaled single expert, we are able to maintain the performance gains, largely thanks to tokenization. | Ghada Sokar, Johan S. ObandoCeron, Aaron C. Courville, Hugo Larochelle, Pablo Samuel Castro |  |
| 470 |  |  [Boltzmann-Aligned Inverse Folding Model as a Predictor of Mutational Effects on Protein-Protein Interactions](https://openreview.net/forum?id=lzdFImKK8w) |  | 0 | Predicting the change in binding free energy ($\Delta \Delta G$) is crucial for understanding and modulating protein-protein interactions, which are critical in drug design. Due to the scarcity of experimental $\Delta\Delta G$ data, existing methods focus on pre-training, while neglecting the importance of alignment. In this work, we propose Boltzmann Alignment technique to transfer knowledge from pre-trained inverse folding models to prediction of $\Delta\Delta G$. We begin by analyzing the thermodynamic definition of $\Delta\Delta G$ and introducing the Boltzmann distribution to connect energy to the protein conformational distribution. However, the protein conformational distribution is intractable. Therefore, we employ Bayes’ theorem to circumvent direct estimation and instead utilize the log-likelihood provided by protein inverse folding models for the estimation of $\Delta\Delta G$. Compared to previous methods based on inverse folding, our method explicitly accounts for the unbound state of the protein complex in the $\Delta \Delta G$ thermodynamic cycle, introducing a physical inductive bias and achieving supervised and unsupervised state-of-the-art (SoTA) performance. Experimental results on SKEMPI v2 indicate that our method achieves Spearman coefficients of 0.3201 (unsupervised) and 0.5134 (supervised) on SKEMPI v2, significantly surpassing the previously reported %SoTA values SoTA results of 0.2632 and 0.4324, respectively. Furthermore, we demonstrate the capability of our method in binding energy prediction, protein-protein docking, and antibody optimization tasks. Code is available at [https://github.com/aim-uofa/BA-DDG](https://github.com/aim-uofa/BA-DDG) | Xiaoran Jiao, Weian Mao, Wengong Jin, Peiyuan Yang, Hao Chen, Chunhua Shen |  |
| 471 |  |  [Improving Unsupervised Constituency Parsing via Maximizing Semantic Information](https://openreview.net/forum?id=qyU5s4fzLg) |  | 0 | Unsupervised constituency parsers organize phrases within a sentence into a tree-shaped syntactic constituent structure that reflects the organization of sentence semantics. However, the traditional objective of maximizing sentence log-likelihood (LL) does not explicitly account for the close relationship between the constituent structure and the semantics, resulting in a weak correlation between LL values and parsing accuracy. In this paper, we introduce a novel objective that trains parsers by maximizing SemInfo, the semantic information encoded in constituent structures. We introduce a bag-of-substrings model to represent the semantics and estimate the SemInfo value using the probability-weighted information metric. We apply the SemInfo maximization objective to training Probabilistic Context-Free Grammar (PCFG) parsers and develop a Tree Conditional Random Field (TreeCRF)-based model to facilitate the training. Experiments show that SemInfo correlates more strongly with parsing accuracy than LL, establishing SemInfo as a better unsupervised parsing objective. As a result, our algorithm significantly improves parsing accuracy by an average of 7.85 sentence-F1 scores across five PCFG variants and in four languages, achieving state-of-the-art level results in three of the four languages. | Junjie Chen, Xiangheng He, Yusuke Miyao, Danushka Bollegala |  |
| 472 |  |  [Student-Informed Teacher Training](https://openreview.net/forum?id=Dzh0hQPpuf) |  | 0 | Imitation learning with a privileged teacher has proven effective for learning complex control behaviors from high-dimensional inputs, such as images. In this framework, a teacher is trained with privileged task information, while a student tries to predict the actions of the teacher with more limited observations, e.g., in a robot navigation task, the teacher might have access to distances to nearby obstacles, while the student only receives visual observations of the scene. However, privileged imitation learning faces a key challenge: the student might be unable to imitate the teacher's behavior due to partial observability. This problem arises because the teacher is trained without considering if the student is capable of imitating the learned behavior. To address this teacher-student asymmetry, we propose a framework for joint training of the teacher and student policies, encouraging the teacher to learn behaviors that can be imitated by the student despite the latters' limited access to information and its partial observability. Based on the performance bound in imitation learning, we add (i) the approximated action difference between teacher and student as a penalty term to the reward function of the teacher, and (ii) a supervised teacher-student alignment step. We motivate our method with a maze navigation task and demonstrate its effectiveness on complex vision-based quadrotor flight and manipulation tasks. | Nico Messikommer, Jiaxu Xing, Elie Aljalbout, Davide Scaramuzza |  |
| 473 |  |  [OS-ATLAS: Foundation Action Model for Generalist GUI Agents](https://openreview.net/forum?id=n9PDaFNi8t) |  | 0 | Existing efforts in building GUI agents heavily rely on the availability of robust commercial Vision-Language Models (VLMs) such as GPT-4o and GeminiProVision. Practitioners are often reluctant to use open-source VLMs due to their significant performance lag compared to their closed-source counterparts, particularly in GUI grounding and Out-Of-Distribution (OOD) scenarios. To facilitate future research in this area, we developed OS-Atlas—a foundational GUI action model that excels at GUI grounding and OOD agentic tasks through innovations in both data and modeling. We have invested significant engineering effort in developing an open-source toolkit for synthesizing GUI grounding data across multiple platforms, including Windows, Linux, MacOS, Android, and the web. Leveraging this toolkit, we are releasing the largest open-source cross-platform GUI grounding corpus to date, which contains over 13 million GUI elements. This dataset, combined with innovations in model training, provides a solid foundation for OS-Atlas to understand GUI screenshots and generalize to unseen interfaces. Through extensive evaluation across six benchmarks spanning three different platforms (mobile, desktop, and web), OS-Atlas demonstrates significant performance improvements over previous state-of-the-art models. Our evaluation also uncovers valuable insights into continuously improving and scaling the agentic capabilities of open-source VLMs. | Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, Yu Qiao |  |
| 474 |  |  [Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models](https://openreview.net/forum?id=9WYMDgxDac) |  | 0 | Multimodal Large Language Models (MLLMs) exhibit promising advancements across various tasks, yet they still encounter significant trustworthiness issues. Prior studies apply Split Conformal Prediction (SCP) in language modeling to construct prediction sets with statistical guarantees. However, these methods typically rely on internal model logits or are restricted to multiple-choice settings, which hampers their generalizability and adaptability in dynamic, open-ended environments. In this paper, we introduce \*TRON\*, a \*\*t\*\*wo-step framework for \*\*r\*\*isk c\*\*o\*\*ntrol and assessme\*\*n\*\*t, applicable to any MLLM that supports sampling in both open-ended and closed-ended scenarios. \*TRON\* comprises two main components: (1) a novel conformal score to \*\*sample\*\* response sets of minimum size, and (2) a nonconformity score to \*\*identify\*\* high-quality responses based on self-consistency theory, controlling the error rates by two specific risk levels. Furthermore, we investigate semantic redundancy in prediction sets within open-ended contexts for the first time, leading to a promising evaluation metric for MLLMs based on average set size. Our comprehensive experiments across four Video Question-Answering (VideoQA) datasets utilizing eight MLLMs show that \*TRON\* achieves desired error rates bounded by two user-specified risk levels. Additionally, deduplicated prediction sets maintain adaptiveness while being more efficient and stable for risk assessment under different risk levels. | Qingni Wang, Tiantian Geng, Zhiyuan Wang, Teng Wang, Bo Fu, Feng Zheng |  |
| 475 |  |  [Exploring the Camera Bias of Person Re-identification](https://openreview.net/forum?id=SgymXhOEA5) |  | 0 | We empirically investigate the camera bias of person re-identification (ReID) models. Previously, camera-aware methods have been proposed to address this issue, but they are largely confined to training domains of the models. We measure the camera bias of ReID models on unseen domains and reveal that camera bias becomes more pronounced under data distribution shifts. As a debiasing method for unseen domain data, we revisit feature normalization on embedding vectors. While the normalization has been used as a straightforward solution, its underlying causes and broader applicability remain unexplored. We analyze why this simple method is effective at reducing bias and show that it can be applied to detailed bias factors such as low-level image properties and body angle. Furthermore, we validate its generalizability across various models and benchmarks, highlighting its potential as a simple yet effective test-time postprocessing method for ReID. In addition, we explore the inherent risk of camera bias in unsupervised learning of ReID models. The unsupervised models remain highly biased towards camera labels even for seen domain data, indicating substantial room for improvement. Based on observations of the negative impact of camera-biased pseudo labels on training, we suggest simple training strategies to mitigate the bias. By applying these strategies to existing unsupervised learning algorithms, we show that significant performance improvements can be achieved with minor modifications. | Myungseo Song, JinWoo Park, JongSeok Lee |  |
| 476 |  |  [Preference Optimization for Reasoning with Pseudo Feedback](https://openreview.net/forum?id=jkUp3lybXf) |  | 0 | Preference optimization techniques, such as Direct Preference Optimization (DPO), are frequently employed to enhance the reasoning capabilities of large language models (LLMs) in domains like mathematical reasoning and coding, typically following supervised fine-tuning. These methods rely on high-quality labels for reasoning tasks to generate preference pairs; however, the availability of reasoning datasets with human-verified labels is limited. In this study, we introduce a novel approach to generate pseudo feedback for reasoning tasks by framing the labeling of solutions to reason problems as an evaluation against associated \emph{test cases}. We explore two forms of pseudo feedback based on test cases: one generated by frontier LLMs and the other by extending self-consistency to multi-test-case. We conduct experiments on both mathematical reasoning and coding tasks using pseudo feedback for preference optimization, and observe improvements across both tasks. Specifically, using Mathstral-7B as our base model, we improve MATH results from 58.3 to 68.6, surpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and College Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3, respectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.3 on LiveCodeBench (from 21.1), surpassing Claude-3-Haiku. | Fangkai Jiao, Geyang Guo, Xingxing Zhang, Nancy F. Chen, Shafiq Joty, Furu Wei |  |
| 477 |  |  [Regularization by Texts for Latent Diffusion Inverse Solvers](https://openreview.net/forum?id=TtUh0TOlGX) |  | 0 | The recent development of diffusion models has led to significant progress in solving inverse problems by leveraging these models as powerful generative priors. However, challenges persist due to the ill-posed nature of such problems, often arising from ambiguities in measurements or intrinsic system symmetries. To address this, we introduce a novel latent diffusion inverse solver, regularization by text (TReg), inspired by the human ability to resolve visual ambiguities through perceptual biases. TReg integrates textual descriptions of preconceptions about the solution during reverse diffusion sampling, dynamically reinforcing these descriptions through null-text optimization, which we refer to as adaptive negation. Our comprehensive experimental results demonstrate that TReg effectively mitigates ambiguity in inverse problems, improving both accuracy and efficiency. | Jeongsol Kim, Geon Yeong Park, Hyungjin Chung, Jong Chul Ye |  |
| 478 |  |  [Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool Usage](https://openreview.net/forum?id=0bmGL4q7vJ) |  | 0 | The advancement of large language models (LLMs) prompts the development of multi-modal agents, which are used as a controller to call external tools, providing a feasible way to solve practical tasks. In this paper, we propose a multi-modal agent tuning method that automatically generates multi-modal tool-usage data and tunes a vision-language model (VLM) as the controller for powerful tool-usage reasoning. To preserve the data quality, we prompt the GPT-4o mini model to generate queries, files, and trajectories, followed by query-file and trajectory verifiers. Based on the data synthesis pipeline, we collect the MM-Traj dataset that contains 20K tasks with trajectories of tool usage. Then, we develop the T3-Agent via Trajectory Tuning on VLMs for Tool usage using MM-Traj. Evaluations on the GTA and GAIA benchmarks show that the T3-Agent consistently achieves improvements on two popular VLMs: MiniCPM-V-8.5B and Qwen2-VL-7B, which outperforms untrained VLMs by 20%, showing the effectiveness of the proposed data synthesis pipeline, leading to high-quality data for tool-usage capabilities. | Zhi Gao, Bofei Zhang, Pengxiang Li, Xiaojian Ma, Tao Yuan, Yue Fan, Yuwei Wu, Yunde Jia, SongChun Zhu, Qing Li |  |
| 479 |  |  [Monitoring Latent World States in Language Models with Propositional Probes](https://openreview.net/forum?id=0yvZm2AjUr) |  | 0 | Language models (LMs) are susceptible to bias, sycophancy, backdoors, and other tendencies that lead to unfaithful responses to the input context. Interpreting internal states of LMs could help monitor and correct unfaithful behavior. We hypothesize that LMs faithfully represent their input contexts in a latent world model, and we seek to extract these latent world states as logical propositions. For example, given the input context \`\`Greg is a nurse. Laura is a physicist.'', we aim to decode the propositions WorksAs(Greg, nurse) and WorksAs(Laura, physicist) from the model's internal activations. To do so we introduce _propositional probes_, which compositionally extract lexical concepts from token activations and bind them into propositions. Key to this is identifying a _binding subspace_ in which bound tokens have high similarity (Greg $\leftrightarrow$ nurse) but unbound ones do not (Greg $\not\leftrightarrow$ physicist). Despite only being trained on linguistically simple English templates, we find that propositional probes generalize to inputs written as short stories and translated to Spanish. Moreover, in three settings where LMs respond unfaithfully to the input context---prompt injections, backdoor attacks, and gender bias--- the decoded propositions remain faithful. This suggests that LMs often encode a faithful world model but decode it unfaithfully, which motivates the search for better interpretability tools for monitoring LMs. | Jiahai Feng, Stuart Russell, Jacob Steinhardt |  |
| 480 |  |  [TAID: Temporally Adaptive Interpolated Distillation for Efficient Knowledge Transfer in Language Models](https://openreview.net/forum?id=cqsw28DuMW) |  | 0 | Causal language models have demonstrated remarkable capabilities, but their size poses significant challenges for deployment in resource-constrained environments. Knowledge distillation, a widely-used technique for transferring knowledge from a large teacher model to a small student model, presents a promising approach for model compression. A significant remaining issue lies in the major differences between teacher and student models, namely the substantial capacity gap, mode averaging, and mode collapse, which pose barriers during distillation. To address these issues, we introduce $\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel knowledge distillation approach that dynamically interpolates student and teacher distributions through an adaptive intermediate distribution, gradually shifting from the student's initial distribution towards the teacher's distribution. We provide a theoretical analysis demonstrating TAID's ability to prevent mode collapse and empirically show its effectiveness in addressing the capacity gap while balancing mode averaging and mode collapse. Our comprehensive experiments demonstrate TAID's superior performance across various model sizes and architectures in both instruction tuning and pre-training scenarios. Furthermore, we showcase TAID's practical impact by developing two state-of-the-art compact foundation models: $\texttt{TAID-LLM-1.5B}$ for language tasks and $\texttt{TAID-VLM-2B}$ for vision-language tasks. These results demonstrate TAID's effectiveness in creating high-performing and efficient models, advancing the development of more accessible AI technologies. | Makoto Shing, Kou Misaki, Han Bao, Sho Yokoi, Takuya Akiba |  |
| 481 |  |  [Learning Transformer-based World Models with Contrastive Predictive Coding](https://openreview.net/forum?id=YK9G4Htdew) |  | 0 | The DreamerV3 algorithm recently obtained remarkable performance across diverse environment domains by learning an accurate world model based on Recurrent Neural Networks (RNNs). Following the success of model-based reinforcement learning algorithms and the rapid adoption of the Transformer architecture for its superior training efficiency and favorable scaling properties, recent works such as STORM have proposed replacing RNN-based world models with Transformer-based world models using masked self-attention. However, despite the improved training efficiency of these methods, their impact on performance remains limited compared to the Dreamer algorithm, struggling to learn competitive Transformer-based world models. In this work, we show that the next state prediction objective adopted in previous approaches is insufficient to fully exploit the representation capabilities of Transformers. We propose to extend world model predictions to longer time horizons by introducing TWISTER (Transformer-based World model wIth contraSTivE Representations), a world model using action-conditioned Contrastive Predictive Coding to learn high-level temporal feature representations and improve the agent performance. TWISTER achieves a human-normalized mean score of 162% on the Atari 100k benchmark, setting a new record among state-of-the-art methods that do not employ look-ahead search. We release our code at https://github.com/burchim/TWISTER. | Maxime Burchi, Radu Timofte |  |
| 482 |  |  [Multimodality Helps Few-shot 3D Point Cloud Semantic Segmentation](https://openreview.net/forum?id=jXvwJ51vcK) |  | 0 | Few-shot 3D point cloud segmentation (FS-PCS) aims at generalizing models to segment novel categories with minimal annotated support samples. While existing FS-PCS methods have shown promise, they primarily focus on unimodal point cloud inputs, overlooking the potential benefits of leveraging multimodal information. In this paper, we address this gap by introducing a multimodal FS-PCS setup, utilizing textual labels and the potentially available 2D image modality. Under this easy-to-achieve setup, we present the MultiModal Few-Shot SegNet (MM-FSS), a model effectively harnessing complementary information from multiple modalities. MM-FSS employs a shared backbone with two heads to extract intermodal and unimodal visual features, and a pretrained text encoder to generate text embeddings. To fully exploit the multimodal information, we propose a Multimodal Correlation Fusion (MCF) module to generate multimodal correlations, and a Multimodal Semantic Fusion (MSF) module to refine the correlations using text-aware semantic guidance. Additionally, we propose a simple yet effective Test-time Adaptive Cross-modal Calibration (TACC) technique to mitigate training bias, further improving generalization. Experimental results on S3DIS and ScanNet datasets demonstrate significant performance improvements achieved by our method. The efficacy of our approach indicates the benefits of leveraging commonly-ignored free modalities for FS-PCS, providing valuable insights for future research. The code is available at github.com/ZhaochongAn/Multimodality-3D-Few-Shot. | Zhaochong An, Guolei Sun, Yun Liu, Runjia Li, Min Wu, MingMing Cheng, Ender Konukoglu, Serge J. Belongie |  |
| 483 |  |  [MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark](https://openreview.net/forum?id=TeVAZXr3yv) |  | 0 | The ability to comprehend audio—which includes speech, non-speech sounds, and music—is crucial for AI agents to interact effectively with the world. We present MMAU, a novel benchmark designed to evaluate multimodal audio understanding models on tasks requiring expert-level knowledge and complex reasoning. MMAU comprises 10k carefully curated audio clips paired with human-annotated natural language questions and answers spanning speech, environmental sounds, and music. It includes information extraction and reasoning questions, requiring models to demonstrate 27 distinct skills across unique and challenging tasks. Unlike existing benchmarks, MMAU emphasizes advanced perception and reasoning with domain-specific knowledge, challenging models to tackle tasks akin to those faced by experts. We assess 18 open-source and proprietary (Large) Audio-Language Models, demonstrating the significant challenges posed by MMAU. Notably, even the most advanced Gemini 2.0 Flash achieves only 59.93% accuracy, and the state-of-the-art open-source Qwen2-Audio achieves only 52.50%, highlighting considerable room for improvement. We believe MMAU will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks. | S. Sakshi, Utkarsh Tyagi, Sonal Kumar, Ashish Seth, Ramaneswaran Selvakumar, Oriol Nieto, Ramani Duraiswami, Sreyan Ghosh, Dinesh Manocha |  |
| 484 |  |  [Higher-Order Graphon Neural Networks: Approximation and Cut Distance](https://openreview.net/forum?id=SjufxrSOYd) |  | 0 | Graph limit models, like \*graphons\* for limits of dense graphs, have recently been used to study size transferability of graph neural networks (GNNs). While most literature focuses on message passing GNNs (MPNNs), in this work we attend to the more powerful \*higher-order\* GNNs. First, we extend the $k$-WL test for graphons (Böker, 2023) to the graphon-signal space and introduce \*signal-weighted homomorphism densities\* as a key tool. As an exemplary focus, we generalize \*Invariant Graph Networks\* (IGNs) to graphons, proposing \*Invariant Graphon Networks\* (IWNs) defined via a subset of the IGN basis corresponding to bounded linear operators. Even with this restricted basis, we show that IWNs of order $k$ are at least as powerful as the $k$-WL test, and we establish universal approximation results for graphon-signals in $L^p$ distances. This significantly extends the prior work of Cai & Wang (2022), showing that IWNs—a subset of their \*IGN-small\*—retain effectively the same expressivity as the full IGN basis in the limit. In contrast to their approach, our blueprint of IWNs also aligns better with the geometry of graphon space, for example facilitating comparability to MPNNs. We highlight that, while typical higher-order GNNs are discontinuous w.r.t. cut distance—which causes their lack of convergence and is inherently tied to the definition of $k$-WL—transferability remains achievable. | Daniel Herbst, Stefanie Jegelka |  |
| 485 |  |  [Towards General-Purpose Model-Free Reinforcement Learning](https://openreview.net/forum?id=R1hIXdST22) |  | 0 | Reinforcement learning (RL) promises a framework for near-universal problem-solving. In practice however, RL algorithms are often tailored to specific benchmarks, relying on carefully tuned hyperparameters and algorithmic choices. Recently, powerful model-based RL methods have shown impressive general results across benchmarks but come at the cost of increased complexity and slow run times, limiting their broader applicability. In this paper, we attempt to find a unifying model-free deep RL algorithm that can address a diverse class of domains and problem settings. To achieve this, we leverage model-based representations that approximately linearize the value function, taking advantage of the denser task objectives used by model-based RL while avoiding the costs associated with planning or simulated trajectories. We evaluate our algorithm, MR.Q, on a variety of common RL benchmarks with a single set of hyperparameters and show a competitive performance against domain-specific and general baselines, providing a concrete step towards building general-purpose model-free deep RL algorithms. | Scott Fujimoto, Pierluca D'Oro, Amy Zhang, Yuandong Tian, Michael Rabbat |  |
| 486 |  |  [Multi-Field Adaptive Retrieval](https://openreview.net/forum?id=3PDklqqqfN) |  | 0 | Document retrieval for tasks such as search and retrieval-augmented generation typically involves datasets that are _unstructured_: free-form text without explicit internal structure in each document. However, documents can have some structure, containing fields such as an article title, a message body, or an HTML header. To address this gap, we introduce Multi-Field Adaptive Retrieval (mFAR), a flexible framework that accommodates any number and any type of document indices on _semi-structured_ data. Our framework consists of two main steps: (1) the decomposition of an existing document into fields, each indexed independently through dense and lexical methods, and (2) learning a model which adaptively predicts the importance of a field by conditioning on the document query, allowing on-the-fly weighting of the most likely field(s). We find that our approach allows for the optimized use of dense versus lexical representations across field types, significantly improves in document ranking over a number of existing retrievers, and achieves state-of-the-art performance for multi-field structured data. | Millicent Li, Tongfei Chen, Benjamin Van Durme, Patrick Xia |  |
| 487 |  |  [Scaling FP8 training to trillion-token LLMs](https://openreview.net/forum?id=E1EHO0imOb) |  | 0 | We train, for the first time, large language models using FP8 precision on datasets up to 2 trillion tokens --- a 20-fold increase over previous limits. Through these extended training runs, we uncover critical instabilities in FP8 training that were not observable in earlier works with shorter durations. We trace these instabilities to outlier amplification by the SwiGLU activation function. Interestingly, we show, both analytically and empirically, that this amplification happens only over prolonged training periods, and link it to a SwiGLU weight alignment process. To address this newly identified issue, we introduce Smooth-SwiGLU, a novel modification that ensures stable FP8 training without altering function behavior. We also demonstrate, for the first time, FP8 quantization of both Adam optimizer moments. Combining these innovations, we successfully train a 7B parameter model using FP8 precision on 256 Intel Gaudi2 accelerators, achieving on-par results with the BF16 baseline while delivering up to a $\sim$ 34 % throughput improvement. A reference implementation is supplied in https://github.com/Anonymous1252022/Megatron-DeepSpeed | Maxim Fishman, Brian Chmiel, Ron Banner, Daniel Soudry |  |
| 488 |  |  [Topograph: An Efficient Graph-Based Framework for Strictly Topology Preserving Image Segmentation](https://openreview.net/forum?id=Q0zmmNNePz) |  | 0 | Topological correctness plays a critical role in many image segmentation tasks, yet most networks are trained using pixel-wise loss functions, such as Dice, neglecting topological accuracy. Existing topology-aware methods often lack robust topological guarantees, are limited to specific use cases, or impose high computational costs. In this work, we propose a novel, graph-based framework for topologically accurate image segmentation that is both computationally efficient and generally applicable. Our method constructs a component graph that fully encodes the topological information of both the prediction and ground truth, allowing us to efficiently identify topologically critical regions and aggregate a loss based on local neighborhood information. Furthermore, we introduce a strict topological metric capturing the homotopy equivalence between the union and intersection of prediction-label pairs. We formally prove the topological guarantees of our approach and empirically validate its effectiveness on binary and multi-class datasets, demonstrating state-of-the-art performance with up to fivefold faster loss computation compared to persistent homology methods. | Laurin Lux, Alexander H. Berger, Alexander Weers, Nico Stucki, Daniel Rueckert, Ulrich Bauer, Johannes C. Paetzold |  |
| 489 |  |  [On Disentangled Training for Nonlinear Transform in Learned Image Compression](https://openreview.net/forum?id=U67J0QNtzo) |  | 0 | Learned image compression (LIC) has demonstrated superior rate-distortion (R-D) performance compared to traditional codecs, but is challenged by training inefficiency that could incur more than two weeks to train a state-of-the-art model from scratch. Existing LIC methods overlook the slow convergence caused by compacting energy in learning nonlinear transforms. In this paper, we first reveal that such energy compaction consists of two components, \emph{i.e.}, feature decorrelation and uneven energy modulation. On such basis, we propose a linear auxiliary transform (AuxT) to disentangle energy compaction in training nonlinear transforms. The proposed AuxT obtains coarse approximation to achieve efficient energy compaction such that distribution fitting with the nonlinear transforms can be simplified to fine details. We then develop wavelet-based linear shortcuts (WLSs) for AuxT that leverages wavelet-based downsampling and orthogonal linear projection for feature decorrelation and subband-aware scaling for uneven energy modulation. AuxT is lightweight and plug-and-play to be integrated into diverse LIC models to address the slow convergence issue. Experimental results demonstrate that the proposed approach can accelerate training of LIC models by 2 times and simultaneously achieves an average 1\% BD-rate reduction. To our best knowledge, this is one of the first successful attempt that can significantly improve the convergence of LIC with comparable or superior rate-distortion performance. | Han Li, Shaohui Li, Wenrui Dai, Maida Cao, Nuowen Kan, Chenglin Li, Junni Zou, Hongkai Xiong |  |
| 490 |  |  [Decomposition Polyhedra of Piecewise Linear Functions](https://openreview.net/forum?id=vVCHWVBsLH) |  | 0 | In this paper we contribute to the frequently studied question of how to decompose a continuous piecewise linear (CPWL) function into a difference of two convex CPWL functions. Every CPWL function has infinitely many such decompositions, but for applications in optimization and neural network theory, it is crucial to find decompositions with as few linear pieces as possible. This is a highly challenging problem, as we further demonstrate by disproving a recently proposed approach by Tran and Wang [Minimal representations of tropical rational functions. Algebraic Statistics, 15(1):27–59, 2024]. To make the problem more tractable, we propose to fix an underlying polyhedral complex determining the possible locus of nonlinearity. Under this assumption, we prove that the set of decompositions forms a polyhedron that arises as intersection of two translated cones. We prove that irreducible decompositions correspond to the bounded faces of this polyhedron and minimal solutions must be vertices. We then identify cases with a unique minimal decomposition, and illustrate how our insights have consequences in the theory of submodular functions. Finally, we improve upon previous constructions of neural networks for a given convex CPWL function and apply our framework to obtain results in the nonconvex case. | MarieCharlotte Brandenburg, Moritz Leo Grillo, Christoph Hertrich |  |
| 491 |  |  [Provably Accurate Shapley Value Estimation via Leverage Score Sampling](https://openreview.net/forum?id=wg3rBImn3O) |  | 0 | Originally introduced in game theory, Shapley values have emerged as a central tool in explainable machine learning, where they are used to attribute model predictions to specific input features. However, computing Shapley values exactly is expensive: for a model with $n$ features, $O(2^n)$ model evaluations are necessary. To address this issue, approximation algorithms are widely used. One of the most popular is the Kernel SHAP algorithm, which is model agnostic and remarkably effective in practice. However, to the best of our knowledge, Kernel SHAP has no strong non-asymptotic complexity guarantees. We address this issue by introducing \*Leverage SHAP\*, a light-weight modification of Kernel SHAP that provides provably accurate Shapley value estimates with just $O(n\log n)$ model evaluations. Our approach takes advantage of a connection between Shapley value estimation and agnostic active learning by employing \*leverage score sampling\*, a powerful regression tool. Beyond theoretical guarantees, we find that Leverage SHAP achieves an approximately 50% reduction in error compared to the highly optimized implementation of Kernel SHAP in the widely used SHAP library [Lundberg & Lee, 2017]. | Christopher Musco, R. Teal Witter |  |
| 492 |  |  [Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?](https://openreview.net/forum?id=Cnwz9jONi5) |  | 0 | Reward Models (RMs) are crucial for aligning language models with human preferences. Currently, the evaluation of RMs depends on measuring accuracy against a validation set of manually annotated preference data. Although this method is straightforward and widely adopted, the relationship between RM accuracy and downstream policy performance remains under-explored. In this work, we conduct experiments in a synthetic setting to investigate how differences in RM measured by accuracy translate into gaps in optimized policy performance. Our findings reveal that while there is a weak positive correlation between accuracy and downstream performance, policies optimized towards RMs with similar accuracy can exhibit quite different performance. Moreover, we discover that the way of measuring accuracy significantly impacts its ability to predict the final policy performance. Through the lens of the Regressional Goodhart effect, we recognize that accuracy, when used for measuring RM quality, can fail to fully capture the potential RM overoptimization. This underscores the inadequacy of relying solely on accuracy to reflect their impact on policy optimization. | Xueru Wen, Jie Lou, Yaojie Lu, Hongyu Lin, XingYu, Xinyu Lu, Ben He, Xianpei Han, Debing Zhang, Le Sun |  |
| 493 |  |  [Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts](https://openreview.net/forum?id=e1wDDFmlVu) |  | 0 | Deep learning for time series forecasting has seen significant advancements over the past decades. However, despite the success of large-scale pre-training in language and vision domains, pre-trained time series models remain limited in scale and operate at a high cost, hindering the development of larger capable forecasting models in real-world applications. In response, we introduce Time-MoE, a scalable and unified architecture designed to pre-train larger, more capable forecasting foundation models while reducing inference costs. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE enhances computational efficiency by activating only a subset of networks for each prediction, reducing computational load while maintaining high model capacity. This allows Time-MoE to scale effectively without a corresponding increase in inference costs. Time-MoE comprises a family of decoder-only transformer models that operate in an auto-regressive manner and support flexible forecasting horizons with varying input context lengths. We pre-trained these models on our newly introduced large-scale data Time-300B, which spans over 9 domains and encompassing over 300 billion time points. For the first time, we scaled a time series foundation model up to 2.4 billion parameters, achieving significantly improved forecasting precision. Our results validate the applicability of scaling laws for training tokens and model size in the context of time series forecasting. Compared to dense models with the same number of activated parameters or equivalent computation budgets, our models consistently outperform them by large margin. These advancements position Time-MoE as a state-of-the-art solution for tackling real-world time series forecasting challenges with superior capability, efficiency, and flexibility. Code is available at https://github.com/Time-MoE/Time-MoE | Xiaoming Shi, Shiyu Wang, Yuqi Nie, Dianqi Li, Zhou Ye, Qingsong Wen, Ming Jin |  |
| 494 |  |  [Generalization Guarantees for Representation Learning via Data-Dependent Gaussian Mixture Priors](https://openreview.net/forum?id=fGdF8Bq1FV) |  | 0 | We establish in-expectation and tail bounds on the generalization error of representation learning type algorithms. The bounds are in terms of the relative entropy between the distribution of the representations extracted from the training and "test'' datasets and a data-dependent symmetric prior, i.e., the Minimum Description Length (MDL) of the latent variables for the training and test datasets. Our bounds are shown to reflect the "structure" and "simplicity'' of the encoder and significantly improve upon the few existing ones for the studied model. We then use our in-expectation bound to devise a suitable data-dependent regularizer; and we investigate thoroughly the important question of the selection of the prior. We propose a systematic approach to simultaneously learning a data-dependent Gaussian mixture prior and using it as a regularizer. Interestingly, we show that a weighted attention mechanism emerges naturally in this procedure. Our experiments show that our approach outperforms the now popular Variational Information Bottleneck (VIB) method as well as the recent Category-Dependent VIB (CDVIB). | Milad Sefidgaran, Abdellatif Zaidi, Piotr Krasnowski |  |
| 495 |  |  [OSDA Agent: Leveraging Large Language Models for De Novo Design of Organic Structure Directing Agents](https://openreview.net/forum?id=9YNyiCJE3k) |  | 0 | Zeolites are crystalline porous materials that have been widely utilized in petrochemical industries as well as sustainable chemistry areas. Synthesis of zeolites often requires small molecules termed Organic Structure Directing Agents (OSDAs), which are critical in forming the porous structure. Molecule generation models can aid the design of OSDAs, but they are limited by single functionality and lack of interactivity. Meanwhile, large language models (LLMs) such as GPT-4, as general-purpose artificial intelligence systems, excel in instruction comprehension, logical reasoning, and interactive communication. However, LLMs lack in-depth chemistry knowledge and first-principle computation capabilities, resulting in uncontrollable outcomes even after fine-tuning. In this paper, we propose OSDA Agent, an interactive OSDA design framework that leverages LLMs as the brain, coupled with computational chemistry tools. The OSDA Agent consists of three main components: the Actor, responsible for generating potential OSDA structures; the Evaluator, which assesses and scores the generated OSDAs using computational chemistry tools; and the Self-reflector, which produces reflective summaries based on the Evaluator's feedback to refine the Actor's subsequent outputs. Experiments on representative zeolite frameworks show the generation-evaluation-reflection-refinement workflow can perform de novo design of OSDAs with superior generation quality than the pure LLM model, generating candidates consistent with experimentally validated OSDAs and optimizing known OSDAs. | Zhaolin Hu, Yixiao Zhou, Zhongan Wang, Xin Li, Weimin Yang, Hehe Fan, Yi Yang |  |
| 496 |  |  [ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability](https://openreview.net/forum?id=ztzZDzgfrh) |  | 0 | Retrieval-Augmented Generation (RAG) models are designed to incorporate external knowledge, reducing hallucinations caused by insufficient parametric (internal) knowledge. However, even with accurate and relevant retrieved content, RAG models can still produce hallucinations by generating outputs that conflict with the retrieved information. Detecting such hallucinations requires disentangling how Large Language Models (LLMs) balance external and parametric knowledge. Current detection methods often focus on one of these mechanisms or without decoupling their intertwined effects, making accurate detection difficult. In this paper, we investigate the internal mechanisms behind hallucinations in RAG scenarios. We discover hallucinations occur when the \*\*Knowledge FFNs\*\* in LLMs overemphasize parametric knowledge in the residual stream, while \*\*Copying Heads\*\* fail to effectively retain or integrate external knowledge from retrieved content. Based on these findings, we propose \*\*ReDeEP\*\*, a novel method that detects hallucinations by decoupling LLM’s utilization of external context and parametric knowledge. Our experiments show that ReDeEP significantly improves RAG hallucination detection accuracy. Additionally, we introduce AARF, which mitigates hallucinations by modulating the contributions of Knowledge FFNs and Copying Heads. | Zhongxiang Sun, Xiaoxue Zang, Kai Zheng, Jun Xu, Xiao Zhang, Weijie Yu, Yang Song, Han Li |  |
| 497 |  |  [Accelerating Goal-Conditioned Reinforcement Learning Algorithms and Research](https://openreview.net/forum?id=4gaySj8kvX) |  | 0 | Self-supervision has the potential to transform reinforcement learning (RL), paralleling the breakthroughs it has enabled in other areas of machine learning. While self-supervised learning in other domains aims to find patterns in a fixed dataset, self-supervised goal-conditioned reinforcement learning (GCRL) agents discover \*new\* behaviors by learning from the goals achieved during unstructured interaction with the environment. However, these methods have failed to see similar success, both due to a lack of data from slow environment simulations as well as a lack of stable algorithms. We take a step toward addressing both of these issues by releasing a high-performance codebase and benchmark (\`JaxGCRL\`) for self-supervised GCRL, enabling researchers to train agents for millions of environment steps in minutes on a single GPU. By utilizing GPU-accelerated replay buffers, environments, and a stable contrastive RL algorithm, we reduce training time by up to $22\times$. Additionally, we assess key design choices in contrastive RL, identifying those that most effectively stabilize and enhance training performance. With this approach, we provide a foundation for future research in self-supervised GCRL, enabling researchers to quickly iterate on new ideas and evaluate them in diverse and challenging environments. Code: [https://anonymous.4open.science/r/JaxGCRL-2316/README.md](https://anonymous.4open.science/r/JaxGCRL-2316/README.md) | Michal Bortkiewicz, Wladyslaw Palucki, Vivek Myers, Tadeusz Dziarmaga, Tomasz Arczewski, Lukasz Kucinski, Benjamin Eysenbach |  |
| 498 |  |  [Transformers Learn to Implement Multi-step Gradient Descent with Chain of Thought](https://openreview.net/forum?id=r3DF5sOo5B) |  | 0 | Chain of Thought (CoT) prompting has been shown to significantly improve the performance of large language models (LLMs), particularly in arithmetic and reasoning tasks, by instructing the model to produce intermediate reasoning steps. Despite the remarkable empirical success of CoT and its theoretical advantages in enhancing expressivity, the mechanisms underlying CoT training remain largely unexplored. In this paper, we study the training dynamics of transformers over a CoT objective on a in-context weight prediction task for linear regression. We prove that while a one-layer linear transformer without CoT can only implement a single step of gradient descent (GD) and fails to recover the ground-truth weight vector, a transformer with CoT prompting can learn to perform multi-step GD autoregressively, achieving near-exact recovery. Furthermore, we show that the trained transformer effectively generalizes on the unseen data. Empirically, we demonstrate that CoT prompting yields substantial performance improvements. | Jianhao Huang, Zixuan Wang, Jason D. Lee |  |
| 499 |  |  [SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning](https://openreview.net/forum?id=jXLiDKsuDo) |  | 0 | Recent advances in CV and NLP have been largely driven by scaling up the number of network parameters, despite traditional theories suggesting that larger networks are prone to overfitting. These large networks avoid overfitting by integrating components that induce a simplicity bias, guiding models toward simple and generalizable solutions. However, in deep RL, designing and scaling up networks have been less explored. Motivated by this opportunity, we present SimBa, an architecture designed to scale up parameters in deep RL by injecting a simplicity bias. SimBa consists of three components: (i) an observation normalization layer that standardizes inputs with running statistics, (ii) a residual feedforward block to provide a linear pathway from the input to output, and (iii) a layer normalization to control feature magnitudes. By scaling up parameters with SimBa, the sample efficiency of various deep RL algorithms—including off-policy, on-policy, and unsupervised methods—is consistently improved. Moreover, solely by integrating SimBa architecture into SAC, it matches or surpasses state-of-the-art deep RL methods with high computational efficiency across DMC, MyoSuite, and HumanoidBench. These results demonstrate SimBa's broad applicability and effectiveness across diverse RL algorithms and environments. | Hojoon Lee, Dongyoon Hwang, Donghu Kim, Hyunseung Kim, Jun Jet Tai, Kaushik Subramanian, Peter R. Wurman, Jaegul Choo, Peter Stone, Takuma Seno |  |
| 500 |  |  [Tuning Frequency Bias of State Space Models](https://openreview.net/forum?id=wkHcXDv7cv) |  | 0 | State space models (SSMs) leverage linear, time-invariant (LTI) systems to effectively learn sequences with long-range dependencies. By analyzing the transfer functions of LTI systems, we find that SSMs exhibit an implicit bias toward capturing low-frequency components more effectively than high-frequency ones. This behavior aligns with the broader notion of frequency bias in deep learning model training. We show that the initialization of an SSM assigns it an innate frequency bias and that training the model in a conventional way does not alter this bias. Based on our theory, we propose two mechanisms to tune frequency bias: either by scaling the initialization to tune the inborn frequency bias; or by applying a Sobolev-norm-based filter to adjust the sensitivity of the gradients to high-frequency inputs, which allows us to change the frequency bias via training. Using an image-denoising task, we empirically show that we can strengthen, weaken, or even reverse the frequency bias using both mechanisms. By tuning the frequency bias, we can also improve SSMs' performance on learning long-range sequences, averaging an $88.26\\%$ accuracy on the Long-Range Arena (LRA) benchmark tasks. | Annan Yu, Dongwei Lyu, Soon Hoe Lim, Michael W. Mahoney, N. Benjamin Erichson |  |
| 501 |  |  [Planning in Natural Language Improves LLM Search for Code Generation](https://openreview.net/forum?id=48WAZhwHHw) |  | 0 | While scaling training compute has led to remarkable improvements in large language models (LLMs), scaling inference compute only recently began to yield analogous gains. We hypothesize that a core missing component is a lack of diverse LLM outputs, leading to inefficient search due to models repeatedly sampling highly similar, yet incorrect generations. We empirically demonstrate that this lack of diversity can be mitigated by searching over candidate plans for solving a problem in natural language. Based on this insight, we propose PlanSearch, a novel search algorithm which shows strong results across HumanEval+, MBPP+, and LiveCodeBench (a contamination-free benchmark for competitive coding). PlanSearch generates a diverse set of observations about the problem and uses these observations to construct plans for solving the problem. By searching over plans in natural language rather than directly over code solutions, PlanSearch explores a significantly more diverse range of potential solutions compared to baseline search methods. Using PlanSearch on top of Claude 3.5 Sonnet achieves a pass@200 of 77.0% on LiveCodeBench, outperforming both the best pass-rate achieved without any search (pass@1 = 41.4%) and using standard repeated sampling on top of existing non-search models (pass@200 = 60.6%). Finally, we show that, across all models, search algorithms, and benchmarks analyzed, we can accurately predict performance gains from search as a function of the diversity over generated ideas. | Evan Z. Wang, Federico Cassano, Catherine Wu, Yunfeng Bai, William Song, Vaskar Nath, Ziwen Han, Sean M. Hendryx, Summer Yue, Hugh Zhang |  |
| 502 |  |  [Recovering Manifold Structure Using Ollivier Ricci Curvature](https://openreview.net/forum?id=aX7X9z3vQS) |  | 0 | We introduce ORC-ManL, a new algorithm to prune spurious edges from nearest neighbor graphs using a criterion based on Ollivier-Ricci curvature and estimated metric distortion. Our motivation comes from manifold learning: we show that when the data generating the nearest-neighbor graph consists of noisy samples from a low-dimensional manifold, edges that shortcut through the ambient space have more negative Ollivier-Ricci curvature than edges that lie along the data manifold. We demonstrate that our method outperforms alternative pruning methods and that it significantly improves performance on many downstream geometric data analysis tasks that use nearest neighbor graphs as input. Specifically, we evaluate on manifold learning, persistent homology, dimension estimation, and others. We also show that ORC-ManL can be used to improve clustering and manifold learning of single-cell RNA sequencing data. Finally, we provide empirical convergence experiments that support our theoretical findings. | Tristan Luca Saidi, Abigail Hickok, Andrew J. Blumberg |  |
| 503 |  |  [Improved Approximation Algorithms for k-Submodular Maximization via Multilinear Extension](https://openreview.net/forum?id=EPHsIa0Ytg) |  | 0 | We investigate a generalized form of submodular maximization, referred to as $k$-submodular maximization, with applications across the domains of social networks and machine learning. In this work, we propose the multilinear extension of $k$-submodular functions and unified Frank-Wolfe-type frameworks based on that. This continuous framework accommodates 1) monotone or non-monotone functions, and 2) various constraint types including matroid constraints, knapsack constraints, and their combinations. Notably, we attain an asymptotically optimal $1/2$-approximation for monotone $k$-submodular maximization problems with knapsack constraints, surpassing previous $1/3$-approximation results, and a factor-$1/3$ approximation for non-monotone $k$-submodular maximization problems with knapsack constraints and matroid constraints which outperforms previous $0.245$-approximation results. The foundation for our analysis stems from new insights into specific linear and monotone properties pertaining to the multilinear extension. | Huanjian Zhou, Lingxiao Huang, Baoxiang Wang |  |
| 504 |  |  [Nonlinear multiregion neural dynamics with parametric impulse response communication channels](https://openreview.net/forum?id=LbgIZpSUCe) |  | 0 | Cognition arises from the coordinated interaction of brain regions with distinct computational roles. Despite improvements in our ability to extract the dynamics underlying circuit computation from population activity recorded in individual areas, understanding how multiple areas jointly support distributed computation remains a challenge. As part of this effort, we propose a multi-region neural dynamics model composed of two building blocks: _i)_ within-region (potentially driven) nonlinear dynamics and _ii)_ communication channels between regions, parameterized through their impulse response. Together, these choices make it possible to learn nonlinear neural population dynamics and understand the flow of information between regions by drawing from the rich literature of linear systems theory. We develop a state noise inversion free variational filtering and learning algorithm for our model and show, through neuroscientifically inspired numerical experiments, how the proposed model can reveal interpretable characterizations of the local computations within and the flow of information between neural populations. We further validate the efficacy of our approach using simultaneous population recordings from areas V1 and V2. | Matthew Dowling, Cristina Savin |  |
| 505 |  |  [Diffusion On Syntax Trees For Program Synthesis](https://openreview.net/forum?id=wN3KaUXA5X) |  | 0 | Large language models generate code one token at a time. Their autoregressive generation process lacks the feedback of observing the program's output. Training LLMs to suggest edits directly can be challenging due to the scarcity of rich edit data. To address these problems, we propose neural diffusion models that operate on syntax trees of any context-free grammar. Similar to image diffusion models, our method also inverts "noise" applied to syntax trees. Rather than generating code sequentially, we iteratively edit it while preserving syntactic validity, which makes it easy to combine this neural model with search. We apply our approach to inverse graphics tasks, where our model learns to convert images into programs that produce those images. Combined with search, our model is able to write graphics programs, see the execution result, and debug them to meet the required specifications. We additionally show how our system can write graphics programs for hand-drawn sketches. Video results can be found at https://tree-diffusion.github.io. | Shreyas Kapur, Erik Jenner, Stuart Russell |  |
| 506 |  |  [Scaling up the Banded Matrix Factorization Mechanism for Large Scale Differentially Private ML](https://openreview.net/forum?id=69Fp4dcmJN) |  | 0 | Correlated noise mechanisms such as DP Matrix Factorization (DP-MF) have proven to be effective alternatives to DP-SGD in large-epsilon few-epoch training regimes. Significant work has been done to find the best correlated noise strategies, and the current state-of-the-art approach is DP-BandMF , which optimally balances the benefits of privacy amplification and noise correlation. Despite it's utility advantages, severe scalability limitations prevent this mechanism from handling large-scale training scenarios where the number of training iterations may be more than $10^4$ and the number of model parameters may exceed $10^7$. In this work, we present techniques to scale up DP-BandMF along these two dimensions, significantly extending it's reach and enabling it to effectively handle settings with over $10^6$ training iterations and $10^9$ model parameters, with no utility degradation at smaller scales. | Ryan McKenna |  |
| 507 |  |  [Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models](https://openreview.net/forum?id=cRR0oDFEBC) |  | 0 | One core capability of large language models~(LLMs) is to follow natural language instructions. However, the issue of automatically constructing high-quality training data to enhance the complex instruction-following abilities of LLMs without manual annotation remains unresolved. In this paper, we introduce AutoIF, the first scalable and reliable method for automatically generating instruction-following training data. AutoIF transforms the validation of instruction-following data quality into code verification, requiring LLMs to generate instructions, the corresponding code to verify the correctness of the instruction responses, and unit test samples to cross-validate the code's correctness. Then, execution feedback-based rejection sampling can generate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) training. AutoIF achieves significant improvements across three training algorithms, SFT, Offline DPO, and Online DPO, when applied to the advanced open-source LLMs, Qwen2 and LLaMA3, in self-alignment and strong-to-weak distillation settings. Using two widely-used and three challenging general instruction-following benchmarks, we demonstrate that AutoIF significantly improves LLM performance across a wide range of natural instruction constraints. Notably, AutoIF is the first to surpass 90\% accuracy in IFEval’s loose instruction accuracy, without compromising general, math and coding capabilities. Further analysis of quality, scaling, combination, and data efficiency highlights AutoIF's strong generalization and alignment potential. Our code are available at https://github.com/QwenLM/AutoIF | Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, Jingren Zhou |  |
| 508 |  |  [How Feature Learning Can Improve Neural Scaling Laws](https://openreview.net/forum?id=dEypApI1MZ) |  | 0 | We develop a simple solvable model of neural scaling laws beyond the kernel limit. Theoretical analysis of this model predicts the performance scaling predictions with model size, training time and total amount of available data. From the scaling analysis we identify three relevant regimes: hard tasks, easy tasks, and super easy tasks. For easy and super-easy target functions, which are in the Hilbert space (RKHS) of the initial infinite-width neural tangent kernel (NTK), there is no change in the scaling exponents between feature learning models and models in the kernel regime. For hard tasks, which we define as tasks outside of the RKHS of the initial NTK, we show analytically and empirically that feature learning can improve the scaling with training time and compute, approximately doubling the exponent for very hard tasks. This leads to a new compute optimal scaling law for hard tasks in the feature learning regime. We support our finding that feature learning improves the scaling law for hard tasks with experiments of nonlinear MLPs fitting functions with power-law Fourier spectra on the circle and CNNs learning vision tasks. | Blake Bordelon, Alexander B. Atanasov, Cengiz Pehlevan |  |
| 509 |  |  [A CLIP-Powered Framework for Robust and Generalizable Data Selection](https://openreview.net/forum?id=9bMZ29SPVx) |  | 0 | Large-scale datasets have been pivotal to the advancements of deep learning models in recent years, but training on such large datasets inevitably incurs substantial storage and computational overhead. Meanwhile, real-world datasets often contain redundant and noisy data, imposing a negative impact on training efficiency and model performance. Data selection has shown promise in identifying the most representative samples from the entire dataset, which aims to minimize the performance gap with reduced training costs. Existing works typically rely on single-modality information to assign importance scores for individual samples, which may lead to inaccurate assessments, especially when dealing with noisy or corrupted samples. To address this limitation, we propose a novel CLIP-powered data selection framework that leverages multimodal information for more robust and generalizable sample selection. Specifically, our framework consists of three key modules—dataset adaptation, sample scoring, and selection optimization—that together harness extensive pre-trained multimodal knowledge to comprehensively assess sample influence and optimize the selection results through multi-objective optimization. Extensive experiments demonstrate that our approach consistently outperforms existing state-of-the-art baselines on various benchmark datasets. Notably, our method effectively removes noisy or damaged samples from the dataset, enabling it to achieve even higher performance with less data. This indicates that it is not only a way to accelerate training but can also improve overall data quality. The implementation is available at https://github.com/Jackbrocp/clip-powered-data-selection. | Suorong Yang, Peng Ye, Wanli Ouyang, Dongzhan Zhou, Furao Shen |  |
| 510 |  |  [PABBO: Preferential Amortized Black-Box Optimization](https://openreview.net/forum?id=YhfrKB3Ah7) |  | 0 | Preferential Bayesian Optimization (PBO) is a sample-efficient method to learn latent user utilities from preferential feedback over a pair of designs. It relies on a statistical surrogate model for the latent function, usually a Gaussian process, and an acquisition strategy to select the next candidate pair to get user feedback on. Due to the non-conjugacy of the associated likelihood, every PBO step requires a significant amount of computations with various approximate inference techniques. This computational overhead is incompatible with the way humans interact with computers, hindering the use of PBO in real-world cases. Building on the recent advances of amortized BO, we propose to circumvent this issue by fully amortizing PBO, meta-learning both the surrogate and the acquisition function. Our method comprises a novel transformer neural process architecture, trained using reinforcement learning and tailored auxiliary losses. On a benchmark composed of synthetic and real-world datasets, our method is several orders of magnitude faster than the usual Gaussian process-based strategies and often outperforms them in accuracy. | Xinyu Zhang, Daolang Huang, Samuel Kaski, Julien Martinelli |  |
| 511 |  |  [Test-time Alignment of Diffusion Models without Reward Over-optimization](https://openreview.net/forum?id=vi3DjUhFVm) |  | 0 | Diffusion models excel in generative tasks, but aligning them with specific objectives while maintaining their versatility remains challenging. Existing fine-tuning methods often suffer from reward over-optimization, while approximate guidance approaches fail to optimize target rewards effectively. Addressing these limitations, we propose a training-free, test-time method based on Sequential Monte Carlo (SMC) to sample from the reward-aligned target distribution. Our approach, tailored for diffusion sampling and incorporating tempering techniques, achieves comparable or superior target rewards to fine-tuning methods while preserving diversity and cross-reward generalization. We demonstrate its effectiveness in single-reward optimization, multi-objective scenarios, and online black-box optimization. This work offers a robust solution for aligning diffusion models with diverse downstream objectives without compromising their general capabilities. Code is available at https://github.com/krafton-ai/DAS. | Sunwoo Kim, Minkyu Kim, Dongmin Park |  |
| 512 |  |  [Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures](https://openreview.net/forum?id=nGiGXLnKhl) |  | 0 | Transformers have revolutionized computer vision and natural language processing, but their high computational complexity limits their application in high-resolution image processing and long-context analysis. This paper introduces Vision-RWKV (VRWKV), a model that builds upon the RWKV architecture from the NLP field with key modifications tailored specifically for vision tasks. Similar to the Vision Transformer (ViT), our model demonstrates robust global processing capabilities, efficiently handles sparse inputs like masked images, and can scale up to accommodate both large-scale parameters and extensive datasets. Its distinctive advantage is its reduced spatial aggregation complexity, enabling seamless processing of high-resolution images without the need for window operations. Our evaluations demonstrate that VRWKV surpasses ViT's performance in image classification and has significantly faster speeds and lower memory usage processing high-resolution inputs. In dense prediction tasks, it outperforms window-based models, maintaining comparable speeds. These results highlight VRWKV's potential as a more efficient alternative for visual perception tasks. Code and models are available at~\url{https://github.com/OpenGVLab/Vision-RWKV}. | Yuchen Duan, Weiyun Wang, Zhe Chen, Xizhou Zhu, Lewei Lu, Tong Lu, Yu Qiao, Hongsheng Li, Jifeng Dai, Wenhai Wang |  |
| 513 |  |  [GOLD: Graph Out-of-Distribution Detection via Implicit Adversarial Latent Generation](https://openreview.net/forum?id=y5einmJ0Yx) |  | 0 | Despite graph neural networks' (GNNs) great success in modelling graph-structured data, out-of-distribution (OOD) test instances still pose a great challenge for current GNNs. One of the most effective techniques to detect OOD nodes is to expose the detector model with an additional OOD node-set, yet the extra OOD instances are often difficult to obtain in practice. Recent methods for image data address this problem using OOD data synthesis, typically relying on pre-trained generative models like Stable Diffusion. However, these approaches require vast amounts of additional data, as well as one-for-all pre-trained generative models, which are not available for graph data. Therefore, we propose the GOLD framework for graph OOD detection, an implicit adversarial learning pipeline with synthetic OOD exposure without pre-trained models. The implicit adversarial training process employs a novel alternating optimisation framework by training: (1) a latent generative model to regularly imitate the in-distribution (ID) embeddings from an evolving GNN, and (2) a GNN encoder and an OOD detector to accurately classify ID data while increasing the energy divergence between the ID embeddings and the generative model's synthetic embeddings. This novel approach implicitly transforms the synthetic embeddings into pseudo-OOD instances relative to the ID data, effectively simulating exposure to OOD scenarios without auxiliary data. Extensive OOD detection experiments are conducted on five benchmark graph datasets, verifying the superior performance of GOLD without using real OOD data compared with the state-of-the-art OOD exposure and non-exposure baselines. | Danny Wang, Ruihong Qiu, Guangdong Bai, Zi Huang |  |
| 514 |  |  [Fine-tuning with Reserved Majority for Noise Reduction](https://openreview.net/forum?id=ZV7CLf0RHK) |  | 0 | Parameter-efficient fine-tuning (PEFT) has revolutionized supervised fine-tuning, where LoRA and its variants gain the most popularity due to their low training costs and zero inference latency. However, LoRA tuning not only injects knowledgeable features but also noisy hallucination during fine-tuning, which hinders the utilization of tunable parameters with the increasing LoRA rank. In this work, we first investigate in-depth the redundancies among LoRA parameters with substantial empirical studies. Aiming to resemble the learning capacity of high ranks from the findings, we set up a new fine-tuning framework, \textbf{P}arameter-\textbf{Re}dundant \textbf{F}ine-\textbf{T}uning (\preft), which follows the vanilla LoRA tuning process but is required to reduce redundancies before merging LoRA parameters back to pre-trained models. Based on this framework, we propose \textbf{No}ise reduction with \textbf{R}eserved \textbf{M}ajority~(\norm), which decomposes the LoRA parameters into majority parts and redundant parts with random singular value decomposition. The major components are determined by the proposed \search method, specifically employing subspace similarity to confirm the parameter groups that share the highest similarity with the base weight. By employing \norm, we enhance both the learning capacity and benefits from larger ranks, which consistently outperforms both LoRA and other \preft-based methods on various downstream tasks, such as general instruction tuning, math reasoning and code generation. Code is available at \url{https://github.com/pixas/NoRM}. | Shuyang Jiang, Yusheng Liao, Ya Zhang, Yanfeng Wang, Yu Wang |  |
| 515 |  |  [LLaVA-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models](https://openreview.net/forum?id=oSQiao9GqB) |  | 0 | Visual instruction tuning has made considerable strides in enhancing the capabilities of Large Multimodal Models (LMMs). However, existing open LMMs largely focus on single-image tasks, their applications to multi-image scenarios remains less explored. Additionally, prior LMM research separately tackles different scenarios, leaving it impossible to generalize cross scenarios with new emerging capabilities. To this end, we introduce LLaVA-Interleave, which simultaneously tackles Multi-image, Multi-frame (video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs. To enable these capabilities, we regard the interleaved data format as a general template and compile the M4-Instruct dataset with 1,177.6k samples, spanning 4 primary domains with 14 tasks and 41 datasets. We also curate the LLaVA-Interleave Bench to comprehensively evaluate the multi-image performance of LMMs. Through extensive experiments, LLaVA-Interleave achieves leading results in multi-image, video, and 3D benchmarks, while maintaining the performance of single-image tasks. Besides, our model also exhibits several emerging capabilities, e.g., transferring tasks across different settings and modalities. | Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, Chunyuan Li |  |
| 516 |  |  [Formation of Representations in Neural Networks](https://openreview.net/forum?id=Njx1NjHIx4) |  | 0 | Understanding neural representations will help open the black box of neural networks and advance our scientific understanding of modern AI systems. However, how complex, structured, and transferable representations emerge in modern neural networks has remained a mystery. Building on previous results, we propose the Canonical Representation Hypothesis (CRH), which posits a set of six alignment relations to universally govern the formation of representations in most hidden layers of a neural network. Under the CRH, the latent representations (R), weights (W), and neuron gradients (G) become mutually aligned during training. This alignment implies that neural networks naturally learn compact representations, where neurons and weights are invariant to task-irrelevant transformations. We then show that the breaking of CRH leads to the emergence of reciprocal power-law relations between R, W, and G, which we refer to as the Polynomial Alignment Hypothesis (PAH). We present a minimal-assumption theory proving that the balance between gradient noise and regularization is crucial for the emergence of the canonical representation. The CRH and PAH lead to an exciting possibility of unifying major key deep learning phenomena, including neural collapse and the neural feature ansatz, in a single framework. | Liu Ziyin, Isaac L. Chuang, Tomer Galanti, Tomaso A. Poggio |  |
| 517 |  |  [ImpScore: A Learnable Metric For Quantifying The Implicitness Level of Sentences](https://openreview.net/forum?id=gYWqxXE5RJ) |  | 0 | Handling implicit language is essential for natural language processing systems to achieve precise text understanding and facilitate natural interactions with users. Despite its importance, the absence of a metric for accurately measuring the implicitness of language significantly constrains the depth of analysis possible in evaluating models' comprehension capabilities. This paper addresses this gap by developing a scalar metric that quantifies the implicitness level of language without relying on external references. Drawing on principles from traditional linguistics, we define "implicitness" as the divergence between semantic meaning and pragmatic interpretation. To operationalize this definition, we introduce ImpScore, a reference-free metric formulated through an interpretable regression model. This model is trained using pairwise contrastive learning on a specially curated dataset consisting of (\*implicit sentence\*, \*explicit sentence\*) pairs. We validate ImpScore through a user study that compares its assessments with human evaluations on out-of-distribution data, demonstrating its accuracy and strong correlation with human judgments. Additionally, we apply ImpScore to hate speech detection datasets, illustrating its utility and highlighting significant limitations in current large language models' ability to understand highly implicit content. Our metric is publicly available at https://github.com/audreycs/ImpScore. | Yuxin Wang, Xiaomeng Zhu, Weimin Lyu, Saeed Hassanpour, Soroush Vosoughi |  |
| 518 |  |  [ADIFF: Explaining audio difference using natural language](https://openreview.net/forum?id=l4fMj4Vnly) |  | 0 | Understanding and explaining differences between audio recordings is crucial for fields like audio forensics, quality assessment, and audio generation. This involves identifying and describing audio events, acoustic scenes, signal characteristics, and their emotional impact on listeners. This paper stands out as the first work to comprehensively study the task of explaining audio differences and then propose benchmark, baselines for the task. First, we present two new datasets for audio difference explanation derived from the AudioCaps and Clotho audio captioning datasets. Using Large Language Models (LLMs), we generate three levels of difference explanations: (1) concise descriptions of audio events and objects, (2) brief sentences about audio events, acoustic scenes, and signal properties, and (3) comprehensive explanations that include semantics and listener emotions. For the baseline, we use prefix tuning where audio embeddings from two audio files are used to prompt a frozen language model. Our empirical analysis and ablation studies reveal that the naive baseline struggles to distinguish perceptually similar sounds and generate detailed tier 3 explanations. To address these limitations, we propose ADIFF, which introduces a cross-projection module, position captioning, and a three-step training process to enhance the model’s ability to produce detailed explanations. We evaluate our model using objective metrics and human evaluation and show our model enhancements lead to significant improvements in performance over naive baseline and SoTA Audio-Language Model (ALM) Qwen Audio. Lastly, we conduct multiple ablation studies to study the effects of cross-projection, language model parameters, position captioning, third stage fine-tuning, and present our findings. Our benchmarks, findings, and strong baseline pave the way for nuanced and human-like explanations of audio differences. | Soham Deshmukh, Shuo Han, Rita Singh, Bhiksha Raj |  |
| 519 |  |  [Enhancing Compositional Text-to-Image Generation with Reliable Random Seeds](https://openreview.net/forum?id=5BSlakturs) |  | 0 | Text-to-image diffusion models have demonstrated remarkable capability in generating realistic images from arbitrary text prompts. However, they often produce inconsistent results for compositional prompts such as "two dogs" or "a penguin on the right of a bowl". Understanding these inconsistencies is crucial for reliable image generation. In this paper, we highlight the significant role of initial noise in these inconsistencies, where certain noise patterns are more reliable for compositional prompts than others. Our analyses reveal that different initial random seeds tend to guide the model to place objects in distinct image areas, potentially adhering to specific patterns of camera angles and image composition associated with the seed. To improve the model's compositional ability, we propose a method for mining these reliable cases, resulting in a curated training set of generated images without requiring any manual annotation. By fine-tuning text-to-image models on these generated images, we significantly enhance their compositional capabilities. For numerical composition, we observe relative increases of 29.3\% and 19.5\% for Stable Diffusion and PixArt-$\alpha$, respectively. Spatial composition sees even larger gains, with 60.7\% for Stable Diffusion and 21.1\% for PixArt-$\alpha$. | Shuangqi Li, Hieu Le, Jingyi Xu, Mathieu Salzmann |  |
| 520 |  |  [LoRA-Pro: Are Low-Rank Adapters Properly Optimized?](https://openreview.net/forum?id=gTwRMU3lJ5) |  | 0 | Low-rank adaptation, also known as LoRA, has emerged as a prominent method for parameter-efficient fine-tuning of foundation models. Despite its computational efficiency, LoRA still yields inferior performance compared to full fine-tuning. In this paper, we first uncover a fundamental connection between the optimization processes of LoRA and full fine-tuning: using LoRA for optimization is mathematically equivalent to full fine-tuning using a low-rank gradient for parameter updates. And this low-rank gradient can be expressed in terms of the gradients of the two low-rank matrices in LoRA. Leveraging this insight, we introduce LoRA-Pro, a method that enhances LoRA's performance by strategically adjusting the gradients of these low-rank matrices. This adjustment allows the low-rank gradient to more accurately approximate the full fine-tuning gradient, thereby narrowing the performance gap between LoRA and full fine-tuning. Furthermore, we theoretically derive the optimal solutions for adjusting the gradients of the low-rank matrices, applying them during fine-tuning in LoRA-Pro. We conduct extensive experiments across natural language understanding, dialogue generation, mathematical reasoning, code generation, and image classification tasks, demonstrating that LoRA-Pro substantially improves LoRA's performance, effectively narrowing the gap with full fine-tuning. Our code is publicly available at https://github.com/mrflogs/LoRA-Pro. | Zhengbo Wang, Jian Liang, Ran He, Zilei Wang, Tieniu Tan |  |
| 521 |  |  [Large-scale and Fine-grained Vision-language Pre-training for Enhanced CT Image Understanding](https://openreview.net/forum?id=nYpPAT4L3D) |  | 0 | Artificial intelligence (AI) shows great potential in assisting radiologists to improve the efficiency and accuracy of medical image interpretation and diagnosis. However, a versatile AI model requires large-scale data and comprehensive annotations, which are often impractical in medical settings. Recent studies leverage radiology reports as a naturally high-quality supervision for medical images, using contrastive language-image pre-training (CLIP) to develop language-informed models for radiological image interpretation. Nonetheless, these approaches typically contrast entire images with reports, neglecting the local associations between imaging regions and report sentences, which may undermine model performance and interoperability. In this paper, we propose a fine-grained vision-language model (fVLM) for anatomy-level CT image interpretation. Specifically, we explicitly match anatomical regions of CT images with corresponding descriptions in radiology reports and perform contrastive pre-training for each anatomy individually. Fine-grained alignment, however, faces considerable false-negative challenges, mainly from the abundance of anatomy-level healthy samples and similarly diseased abnormalities, leading to ambiguous patient-level pairings. To tackle this issue, we propose identifying false negatives of both normal and abnormal samples and calibrating contrastive learning from patient-level to disease-aware pairing. We curated the largest CT dataset to date, comprising imaging and report data from 69,086 patients, and conducted a comprehensive evaluation of 54 major and important disease (including several most deadly cancers) diagnosis tasks across 15 main anatomies. Experimental results demonstrate the substantial potential of fVLM in versatile medical image interpretation. In the zero-shot classification task, we achieved an average AUC of 81.3% on 54 diagnosis tasks, surpassing CLIP and supervised methods by 12.9% and 8.0%, respectively. Additionally, on the publicly available CT-RATE and Rad-ChestCT benchmarks, our fVLM outperformed the current state-of-the-art methods with absolute AUC gains of 7.4% and 4.8%, respectively. | Zhongyi Shui, Jianpeng Zhang, Weiwei Cao, Sinuo Wang, Ruizhe Guo, Le Lu, Lin Yang, Xianghua Ye, Tingbo Liang, Qi Zhang, Ling Zhang |  |
| 522 |  |  [AutoCGP: Closed-Loop Concept-Guided Policies from Unlabeled Demonstrations](https://openreview.net/forum?id=9ehJCZz4aM) |  | 0 | Training embodied agents to perform complex robotic tasks presents significant challenges due to the entangled factors of task compositionality, environmental diversity, and dynamic changes. In this work, we introduce a novel imitation learning framework to train closed-loop concept-guided policies that enhance long-horizon task performance by leveraging discovered manipulation concepts. Unlike methods that rely on predefined skills and human-annotated labels, our approach allows agents to autonomously abstract manipulation concepts from their proprioceptive states, thereby alleviating misalignment due to ambiguities in human semantics and environmental complexity. Our framework comprises two primary components: an \*Automatic Concept Discovery\* module that identifies meaningful and consistent manipulation concepts, and a \*Concept-Guided Policy Learning\* module that effectively utilizes these manipulation concepts for adaptive task execution, including a \*Concept Selection Transformer\* for concept-based guidance and a \*Concept-Guided Policy\* for action prediction with the selected concepts. Experiments demonstrate that our approach significantly outperforms baseline methods across a range of tasks and environments, while showcasing emergent consistency in motion patterns associated with the discovered manipulation concepts. Codes are available at: https://github.com/PeiZhou26/AutoCGP. | Pei Zhou, Ruizhe Liu, Qian Luo, Fan Wang, Yibing Song, Yanchao Yang |  |
| 523 |  |  [Perm: A Parametric Representation for Multi-Style 3D Hair Modeling](https://openreview.net/forum?id=WKfb1xGXGx) |  | 0 | We present Perm, a learned parametric representation of human 3D hair designed to facilitate various hair-related applications. Unlike previous work that jointly models the global hair structure and local curl patterns, we propose to disentangle them using a PCA-based strand representation in the frequency domain, thereby allowing more precise editing and output control. Specifically, we leverage our strand representation to fit and decompose hair geometry textures into low- to high-frequency hair structures, termed guide textures and residual textures, respectively. These decomposed textures are later parameterized with different generative models, emulating common stages in the hair grooming process. We conduct extensive experiments to validate the architecture design of Perm, and finally deploy the trained model as a generic prior to solve task-agnostic problems, further showcasing its flexibility and superiority in tasks such as single-view hair reconstruction, hairstyle editing, and hair-conditioned image generation. More details can be found on our project page: https://cs.yale.edu/homes/che/projects/perm/. | Chengan He, Xin Sun, Zhixin Shu, Fujun Luan, Sören Pirk, Jorge Alejandro Amador Herrera, Dominik Ludewig Michels, Tuanfeng Yang Wang, Meng Zhang, Holly E. Rushmeier, Yi Zhou |  |
| 524 |  |  [Easing Training Process of Rectified Flow Models Via Lengthening Inter-Path Distance](https://openreview.net/forum?id=RaR3ETzyKp) |  | 0 | Recent research pinpoints that different diffusion methods and architectures trained on the same dataset produce similar results for the same input noise. This property suggests that they have some preferable noises for a given sample. By visualizing the noise-sample pairs of rectified flow models and stable diffusion models in two-dimensional spaces, we observe that the preferable paths, connecting preferable noises to the corresponding samples, are better organized with significant fewer crossings comparing with the random paths, connecting random noises to training samples. In high-dimensional space, paths rarely intersect. The path crossings in two-dimensional spaces indicate the shorter inter-path distance in the corresponding high-dimensional spaces. Inspired by this observation, we propose the Distance-Aware Noise-Sample Matching (DANSM) method to lengthen the inter-path distance for speeding up the model training. DANSM is derived from rectified flow models, which allow using a closed-form formula to calculate the inter-path distance. To further simplify the optimization, we derive the relationship between inter-path distance and path length, and use the latter in the optimization surrogate. DANSM is evaluated on both image and latent spaces by rectified flow models and diffusion models. The experimental results show that DANSM can significantly improve the training speed by 30\% $\sim$ 40\% without sacrificing the generation quality. | Shifeng Xu, Yanzhu Liu, Adams WaiKin Kong |  |
| 525 |  |  [GETS: Ensemble Temperature Scaling for Calibration in Graph Neural Networks](https://openreview.net/forum?id=qgsXsqahMq) |  | 0 | Graph Neural Networks (GNNs) deliver strong classification results but often suffer from poor calibration performance, leading to overconfidence or underconfidence. This is particularly problematic in high-stakes applications where accurate uncertainty estimates are essential. Existing post-hoc methods, such as temperature scaling, fail to effectively utilize graph structures, while current GNN calibration methods often overlook the potential of leveraging diverse input information and model ensembles jointly. In the paper, we propose Graph Ensemble Temperature Scaling (GETS), a novel calibration framework that combines input and model ensemble strategies within a Graph Mixture-of-Experts (MoE) architecture. GETS integrates diverse inputs, including logits, node features, and degree embeddings, and adaptively selects the most relevant experts for each node’s calibration procedure. Our method outperforms state-of-the-art calibration techniques, reducing expected calibration error (ECE) by $\geq$ 25% across 10 GNN benchmark datasets. Additionally, GETS is computationally efficient, scalable, and capable of selecting effective input combinations for improved calibration performance. The implementation is available at https://github.com/ZhuangDingyi/GETS/. | Dingyi Zhuang, Chonghe Jiang, Yunhan Zheng, Shenhao Wang, Jinhua Zhao |  |
| 526 |  |  [Conditional Diffusion with Ordinal Regression: Longitudinal Data Generation for Neurodegenerative Disease Studies](https://openreview.net/forum?id=9UGfOJBuL8) |  | 0 | Modeling the progression of neurodegenerative diseases such as Alzheimer’s disease (AD) is crucial for early detection and prevention given their irreversible nature. However, the scarcity of longitudinal data and complex disease dynamics make the analysis highly challenging. Moreover, longitudinal samples often contain irregular and large intervals between subject visits, which underscore the necessity for advanced data generation techniques that can accurately simulate disease progression over time. In this regime, we propose a novel conditional generative model for synthesizing longitudinal sequences and present its application to neurodegenerative disease data generation conditioned on multiple time-dependent ordinal factors, such as age and disease severity. Our method sequentially generates continuous data by bridging gaps between sparse data points with a diffusion model, ensuring a realistic representation of disease progression. The synthetic data are curated to integrate both cohort-level and individual-specific characteristics, where the cohort-level representations are modeled with an ordinal regression to capture longitudinally monotonic behavior. Extensive experiments on four AD biomarkers validate the superiority of our method over nine baseline approaches, highlighting its potential to be applied to a variety of longitudinal data generation. | Hyuna Cho, Ziquan Wei, Seungjoo Lee, Tingting Dan, Guorong Wu, Won Hwa Kim |  |
| 527 |  |  [SoftCVI: Contrastive variational inference with self-generated soft labels](https://openreview.net/forum?id=PiZtlzMWUj) |  | 0 | Estimating a distribution given access to its unnormalized density is pivotal in Bayesian inference, where the posterior is generally known only up to an unknown normalizing constant. Variational inference and Markov chain Monte Carlo methods are the predominant tools for this task; however, both are often challenging to apply reliably, particularly when the posterior has complex geometry. Here, we introduce Soft Contrastive Variational Inference (SoftCVI), which allows a family of variational objectives to be derived through a contrastive estimation framework. The approach parameterizes a classifier in terms of a variational distribution, reframing the inference task as a contrastive estimation problem aiming to identify a single true posterior sample among a set of samples. Despite this framing, we do not require positive or negative samples, but rather learn by sampling the variational distribution and computing ground truth soft classification labels from the unnormalized posterior itself. The objectives have zero variance gradient when the variational approximation is exact, without the need for specialized gradient estimators. We empirically investigate the performance on a variety of Bayesian inference tasks, using both simple (e.g. normal) and expressive (normalizing flow) variational distributions. We find that SoftCVI can be used to form objectives which are stable to train and mass-covering, frequently outperforming inference with other variational approaches. | Daniel Ward, Mark Beaumont, Matteo Fasiolo |  |
| 528 |  |  [MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion](https://openreview.net/forum?id=lJpqxFgWCM) |  | 0 | Estimating geometry from dynamic scenes, where objects move and deform over time, remains a core challenge in computer vision. Current approaches often rely on multi-stage pipelines or global optimizations that decompose the problem into subtasks, like depth and flow, leading to complex systems prone to errors. In this paper, we present Motion DUSt3R (MonST3R), a novel geometry-first approach that directly estimates per-timestep geometry from dynamic scenes. Our key insight is that by simply estimating a pointmap for each timestep, we can effectively adapt DUSt3R’s representation, previously only used for static scenes, to dynamic scenes. However, this approach presents a significant challenge: the scarcity of suitable training data, namely dynamic, posed videos with depth labels. Despite this, we show that by posing the problem as a fine-tuning task, identifying several suitable datasets, and strategically training the model on this limited data, we can surprisingly enable the model to handle dynamics, even without an explicit motion representation. Based on this, we introduce new optimizations for several downstream video-specific tasks and demonstrate strong performance on video depth and camera pose estimation, outperforming prior work in terms of robustness and efficiency. Moreover, MonST3R shows promising results for primarily feed-forward 4D reconstruction. Interactive 4D results, source code, and trained models are available at: https://monst3r-project.github.io/. | Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, MingHsuan Yang |  |
| 529 |  |  [ND-SDF: Learning Normal Deflection Fields for High-Fidelity Indoor Reconstruction](https://openreview.net/forum?id=4HRRcqE9SU) |  | 0 | Neural implicit reconstruction via volume rendering has demonstrated its effectiveness in recovering dense 3D surfaces. However, it is non-trivial to simultaneously recover meticulous geometry and preserve smoothness across regions with differing characteristics. To address this issue, previous methods typically employ geometric priors, which are often constrained by the performance of the prior models. In this paper, we propose ND-SDF, which learns a Normal Deflection field to represent the angular deviation between the scene normal and the prior normal. Unlike previous methods that uniformly apply geometric priors on all samples, introducing significant bias in accuracy, our proposed normal deflection field dynamically learns and adapts the utilization of samples based on their specific characteristics, thereby improving both the accuracy and effectiveness of the model. Our method not only obtains smooth weakly textured regions such as walls and floors but also preserves the geometric details of complex structures. In addition, we introduce a novel ray sampling strategy based on the deflection angle to facilitate the unbiased rendering process, which significantly improves the quality and accuracy of intricate surfaces, especially on thin structures. Consistent improvements on various challenging datasets demonstrate the superiority of our method. | Ziyu Tang, Weicai Ye, Yifan Wang, Di Huang, Hujun Bao, Tong He, Guofeng Zhang |  |
| 530 |  |  [SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation](https://openreview.net/forum?id=UL8b54P96G) |  | 0 | Human beings are endowed with a complementary learning system, which bridges the slow learning of general world dynamics with fast storage of episodic memory from a new experience. Previous video generation models, however, primarily focus on slow learning by pre-training on vast amounts of data, overlooking the fast learning phase crucial for episodic memory storage. This oversight leads to inconsistencies across temporally distant frames when generating longer videos, as these frames fall beyond the model's context window. To this end, we introduce SlowFast-VGen, a novel dual-speed learning system for action-driven long video generation. Our approach incorporates a masked conditional video diffusion model for the slow learning of world dynamics, alongside an inference-time fast learning strategy based on a temporal LoRA module. Specifically, the fast learning process updates its temporal LoRA parameters based on local inputs and outputs, thereby efficiently storing episodic memory in its parameters. We further propose a slow-fast learning loop algorithm that seamlessly integrates the inner fast learning loop into the outer slow learning loop, enabling the recall of prior multi-episode experiences for context-aware skill learning. To facilitate the slow learning of an approximate world model, we collect a large-scale dataset of 200k videos with language action annotations, covering a wide range of scenarios. Extensive experiments show that SlowFast-VGen outperforms baselines across various metrics for action-driven video generation, achieving an FVD score of 514 compared to 782, and maintaining consistency in longer videos, with an average of 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm significantly enhances performances on long-horizon planning tasks as well. | Yining Hong, Beide Liu, Maxine Wu, Yuanhao Zhai, KaiWei Chang, Linjie Li, Kevin Lin, ChungChing Lin, Jianfeng Wang, Zhengyuan Yang, Ying Nian Wu, Lijuan Wang |  |
| 531 |  |  [Atlas Gaussians Diffusion for 3D Generation](https://openreview.net/forum?id=H2Gxil855b) |  | 0 | Using the latent diffusion model has proven effective in developing novel 3D generation techniques. To harness the latent diffusion model, a key challenge is designing a high-fidelity and efficient representation that links the latent space and the 3D space. In this paper, we introduce Atlas Gaussians, a novel representation for feed-forward native 3D generation. Atlas Gaussians represent a shape as the union of local patches, and each patch can decode 3D Gaussians. We parameterize a patch as a sequence of feature vectors and design a learnable function to decode 3D Gaussians from the feature vectors. In this process, we incorporate UV-based sampling, enabling the generation of a sufficiently large, and theoretically infinite, number of 3D Gaussian points. The large amount of 3D Gaussians enables the generation of high-quality details. Moreover, due to local awareness of the representation, the transformer-based decoding procedure operates on a patch level, ensuring efficiency. We train a variational autoencoder to learn the Atlas Gaussians representation, and then apply a latent diffusion model on its latent space for learning 3D Generation. Experiments show that our approach outperforms the prior arts of feed-forward native 3D generation. Project page: https://yanghtr.github.io/projects/atlas_gaussians. | Haitao Yang, Yuan Dong, Hanwen Jiang, Dejia Xu, Georgios Pavlakos, Qixing Huang |  |
| 532 |  |  [Min-K%++: Improved Baseline for Pre-Training Data Detection from Large Language Models](https://openreview.net/forum?id=ZGkfoufDaU) |  | 0 | The problem of pre-training data detection for large language models (LLMs) has received growing attention due to its implications in critical issues like copyright violation and test data contamination. Despite improved performance, existing methods (including the state-of-the-art, Min-K%) are mostly developed upon simple heuristics and lack solid, reasonable foundations. In this work, we propose a novel and theoretically motivated methodology for pre-training data detection, named Min-K%++. Specifically, we present a key insight that training samples tend to be local maxima of the modeled distribution along each input dimension through maximum likelihood training, which in turn allow us to insightfully translate the problem into identification of local maxima. Then, we design our method accordingly that works under the discrete distribution modeled by LLMs, whose core idea is to determine whether the input forms a mode or has relatively high probability under the conditional categorical distribution. Empirically, the proposed method achieves new SOTA performance across multiple settings (evaluated with 5 families of 10 models and 2 benchmarks). On the WikiMIA benchmark, Min-K%++ outperforms the runner-up by 6.2% to 10.5% in detection AUROC averaged over five models. On the more challenging MIMIR benchmark, it consistently improves upon reference-free methods while performing on par with reference-based method that requires an extra reference model. | Jingyang Zhang, Jingwei Sun, Eric C. Yeats, Yang Ouyang, Martin Kuo, Jianyi Zhang, Hao (Frank) Yang, Hai Li |  |
| 533 |  |  [4K4DGen: Panoramic 4D Generation at 4K Resolution](https://openreview.net/forum?id=qxRoo7ULCo) |  | 0 | The blooming of virtual reality and augmented reality (VR/AR) technologies has driven an increasing demand for the creation of high-quality, immersive, and dynamic environments. However, existing generative techniques either focus solely on dynamic objects or perform outpainting from a single perspective image, failing to meet the requirements of VR/AR applications that need free-viewpoint, 360$^{\circ}$ virtual views where users can move in all directions. In this work, we tackle the challenging task of elevating a single panorama to an immersive 4D experience. For the first time, we demonstrate the capability to generate omnidirectional dynamic scenes with 360$^{\circ}$ views at 4K (4096 $\times$ 2048) resolution, thereby providing an immersive user experience. Our method introduces a pipeline that facilitates natural scene animations and optimizes a set of 3D Gaussians using efficient splatting techniques for real-time exploration. To overcome the lack of scene-scale annotated 4D data and models, especially in panoramic formats, we propose a novel Panoramic Denoiser that adapts generic 2D diffusion priors to animate consistently in 360$^{\circ}$ images, transforming them into panoramic videos with dynamic scenes at targeted regions. Subsequently, we propose Dynamic Panoramic Lifting to elevate the panoramic video into a 4D immersive environment while preserving spatial and temporal consistency. By transferring prior knowledge from 2D models in the perspective domain to the panoramic domain and the 4D lifting with spatial appearance and geometry regularization, we achieve high-quality Panorama-to-4D generation at a resolution of 4K for the first time. Project page: https://4k4dgen.github.io/. | Renjie Li, Panwang Pan, Bangbang Yang, Dejia Xu, Shijie Zhou, Xuanyang Zhang, Zeming Li, Achuta Kadambi, Zhangyang Wang, Zhengzhong Tu, Zhiwen Fan |  |
| 534 |  |  [Samba: Synchronized Set-of-Sequences Modeling for Multiple Object Tracking](https://openreview.net/forum?id=OeBY9XqiTz) |  | 0 | Multiple object tracking in complex scenarios - such as coordinated dance performances, team sports, or dynamic animal groups - presents unique challenges. In these settings, objects frequently move in coordinated patterns, occlude each other, and exhibit long-term dependencies in their trajectories. However, it remains a key open research question on how to model long-range dependencies within tracklets, interdependencies among tracklets, and the associated temporal occlusions. To this end, we introduce Samba, a novel linear-time set-of-sequences model designed to jointly process multiple tracklets by synchronizing the multiple selective state-spaces used to model each tracklet. Samba autoregressively predicts the future track query for each sequence while maintaining synchronized long-term memory representations across tracklets. By integrating Samba into a tracking-by-propagation framework, we propose SambaMOTR, the first tracker effectively addressing the aforementioned issues, including long-range dependencies, tracklet interdependencies, and temporal occlusions. Additionally, we introduce an effective technique for dealing with uncertain observations (MaskObs) and an efficient training recipe to scale SambaMOTR to longer sequences. By modeling long-range dependencies and interactions among tracked objects, SambaMOTR implicitly learns to track objects accurately through occlusions without any hand-crafted heuristics. Our approach significantly surpasses prior state-of-the-art on the DanceTrack, BFT, and SportsMOT datasets. | Mattia Segù, Luigi Piccinelli, Siyuan Li, YungHsu Yang, Luc Van Gool, Bernt Schiele |  |
| 535 |  |  [Programming Refusal with Conditional Activation Steering](https://openreview.net/forum?id=Oi47wc10sm) |  | 0 | LLMs have shown remarkable capabilities, but precisely controlling their response behavior remains challenging. Existing activation steering methods alter LLM behavior indiscriminately, limiting their practical applicability in settings where selective responses are essential, such as content moderation or domain-specific assistants. In this paper, we propose Conditional Activation Steering (CAST), which analyzes LLM activation patterns during inference to selectively apply or withhold activation steering based on the input context. Our method is based on the observation that different categories of prompts activate distinct patterns in the model's hidden states. Using CAST, one can systematically control LLM behavior with rules like "if input is about hate speech or adult content, then refuse" or "if input is not about legal advice, then refuse." This allows for selective modification of responses to specific content while maintaining normal responses to other content, all without requiring weight optimization. We release an open-source implementation of our framework. | Bruce W. Lee, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Erik Miehling, Pierre L. Dognin, Manish Nagireddy, Amit Dhurandhar |  |
| 536 |  |  [Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders](https://openreview.net/forum?id=Y2RW9EVwhT) |  | 0 | The ability to accurately interpret complex visual information is a crucial topic of multimodal large language models (MLLMs). Recent work indicates that enhanced visual perception significantly reduces hallucinations and improves performance on resolution-sensitive tasks, such as optical character recognition and document analysis. A number of recent MLLMs achieve this goal using a mixture of vision encoders. Despite their success, there is a lack of systematic comparisons and detailed ablation studies addressing critical aspects, such as expert selection and the integration of multiple vision experts. This study provides an extensive exploration of the design space for MLLMs using a mixture of vision encoders and resolutions. Our findings reveal several underlying principles common to various existing strategies, leading to a streamlined yet effective design approach. We discover that simply concatenating visual tokens from a set of complementary vision encoders is as effective as more complex mixing architectures or strategies. We additionally introduce Pre-Alignment to bridge the gap between vision-focused encoders and language tokens, enhancing model coherence. The resulting family of MLLMs, Eagle, surpasses other leading open-source models on major MLLM benchmarks. | Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, Yilin Zhao, DeAn Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, Bryan Catanzaro, Andrew Tao, Jan Kautz, Zhiding Yu, Guilin Liu |  |
| 537 |  |  [PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training](https://openreview.net/forum?id=j4LITBSUjs) |  | 0 | This paper aims to address the challenge of hallucinations in Multimodal Large Language Models (MLLMs) particularly for dense image captioning tasks. To tackle the challenge, we identify the current lack of a metric that finely measures the caption quality in concept level. We hereby introduce HalFscore, a novel metric built upon the language graph and is designed to evaluate both the accuracy and completeness of dense captions at a granular level. Additionally, we identify the root cause of hallucination as the model's over-reliance on its language prior. To address this, we propose PerturboLLaVA, which reduces the model's reliance on the language prior by incorporating adversarially perturbed text during training. This method enhances the model's focus on visual inputs, effectively reducing hallucinations and producing accurate, image-grounded descriptions without incurring additional computational overhead. PerturboLLaVA significantly improves the fidelity of generated captions, outperforming existing approaches in handling multimodal hallucinations and achieving improved performance across general multimodal benchmarks. | Cong Chen, Mingyu Liu, Chenchen Jing, Yizhou Zhou, Fengyun Rao, Hao Chen, Bo Zhang, Chunhua Shen |  |
| 538 |  |  [CLoSD: Closing the Loop between Simulation and Diffusion for multi-task character control](https://openreview.net/forum?id=pZISppZSTv) |  | 0 | Motion diffusion models and Reinforcement Learning (RL) based control for physics-based simulations have complementary strengths for human motion generation. The former is capable of generating a wide variety of motions, adhering to intuitive control such as text, while the latter offers physically plausible motion and direct interaction with the environment. In this work, we present a method that combines their respective strengths. CLoSD is a text-driven RL physics-based controller, guided by diffusion generation for various tasks. Our key insight is that motion diffusion can serve as an on-the-fly universal planner for a robust RL controller. To this end, CLoSD maintains a closed-loop interaction between two modules — a Diffusion Planner (DiP), and a tracking controller. DiP is a fast-responding autoregressive diffusion model, controlled by textual prompts and target locations, and the controller is a simple and robust motion imitator that continuously receives motion plans from DiP and provides feedback from the environment. CLoSD is capable of seamlessly performing a sequence of different tasks, including navigation to a goal location, striking an object with a hand or foot as specified in a text prompt, sitting down, and getting up. | Guy Tevet, Sigal Raab, Setareh Cohan, Daniele Reda, Zhengyi Luo, Xue Bin Peng, Amit Haim Bermano, Michiel van de Panne |  |
| 539 |  |  [Linear SCM Identification in the Presence of Confounders and Gaussian Noise](https://openreview.net/forum?id=bjxuqI4KwU) |  | 0 | Noisy linear structural causal models (SCMs) in the presence of confounding variables are known to be identifiable if all confounding and noise variables are non-Gaussian and unidentifiable if all are Gaussian. The identifiability when only some are Gaussian remains concealed. We show that, in the presence of Gaussian noise, a linear SCM is uniquely identifiable provided that \emph{(i)} the number of confounders is at most the number of the observed variables, \emph{(ii)} the confounders do not have a Gaussian component, and \emph{(iii)} the causal structure of the SCM is known. If the third condition is relaxed, the SCM becomes finitely identifiable; more specifically, it belongs to a set of at most $n!$ linear SCMS, where $n$ is the number of observed variables. The confounders in all of these $n!$ SCMs share the same joint probability distribution function (PDF), which we obtain analytically. For the case where both the noise and confounders are Gaussian, we provide further insight into the existing counter-example-based unidentifiability result and demonstrate that every SCM with confounders can be represented as an SCM without confounders but with the same joint PDF. | Vahideh Sanjaroonpouri, Pouria Ramazi |  |
| 540 |  |  [Geometric Inductive Biases of Deep Networks: The Role of Data and Architecture](https://openreview.net/forum?id=cmXWYolrlo) |  | 0 | In this paper, we propose the \*geometric invariance hypothesis (GIH)\*, which argues that the input space curvature of a neural network remains invariant under transformation in certain architecture-dependent directions during training. We investigate a simple, non-linear binary classification problem residing on a plane in a high dimensional space and observe that&#151;unlike MLPs&#151;ResNets fail to generalize depending on the orientation of the plane. Motivated by this example, we define a neural network's \*\*average geometry\*\* and \*\*average geometry evolution\*\* as compact \*architecture-dependent\* summaries of the model's input-output geometry and its evolution during training. By investigating the average geometry evolution at initialization, we discover that the geometry of a neural network evolves according to the data covariance projected onto its average geometry. This means that the geometry only changes in a subset of the input space when the average geometry is low-rank, such as in ResNets. This causes an architecture-dependent invariance property in the input space curvature, which we dub GIH. Finally, we present extensive experimental results to observe the consequences of GIH and how it relates to generalization in neural networks. | Sajad Movahedi, Antonio Orvieto, SeyedMohsen MoosaviDezfooli |  |
| 541 |  |  [MotionAura: Generating High-Quality and Motion Consistent Videos using Discrete Diffusion](https://openreview.net/forum?id=bW9fGYo44s) |  | 0 | The spatio-temporal complexity of video data presents significant challenges in tasks such as compression, generation, and inpainting. We present four key contributions to address the challenges of spatiotemporal video processing. First, we introduce the 3D Mobile Inverted Vector-Quantization Variational Autoencoder (3D-MBQ-VAE), which combines Variational Autoencoders (VAEs) with masked modeling to enhance spatiotemporal video compression. The model achieves superior temporal consistency and state-of-the-art (SOTA) reconstruction quality by employing a novel training strategy with full frame masking. Second, we present MotionAura, a text-to-video generation framework that utilizes vector-quantized diffusion models to discretize the latent space and capture complex motion dynamics, producing temporally coherent videos aligned with text prompts. Third, we propose a spectral transformer-based denoising network that processes video data in the frequency domain using the Fourier Transform. This method effectively captures global context and long-range dependencies for high-quality video generation and denoising. Lastly, we introduce a downstream task of Sketch Guided Video Inpainting. This task leverages Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. Our models achieve SOTA performance on a range of benchmarks. Our work offers robust frameworks for spatiotemporal modeling and user-driven video content manipulation. | Onkar Kishor Susladkar, Jishu Sen Gupta, Chirag Sehgal, Sparsh Mittal, Rekha Singhal |  |
| 542 |  |  [Rare-to-Frequent: Unlocking Compositional Generation Power of Diffusion Models on Rare Concepts with LLM Guidance](https://openreview.net/forum?id=BgxsmpVoOX) |  | 0 | State-of-the-art text-to-image (T2I) diffusion models often struggle to generate rare compositions of concepts, e.g., objects with unusual attributes. In this paper, we show that the compositional generation power of diffusion models on such rare concepts can be significantly enhanced by the Large Language Model (LLM) guidance. We start with empirical and theoretical analysis, demonstrating that exposing frequent concepts relevant to the target rare concepts during the diffusion sampling process yields more accurate concept composition. Based on this, we propose a training-free approach, R2F, that plans and executes the overall rare-to-frequent concept guidance throughout the diffusion inference by leveraging the abundant semantic knowledge in LLMs. Our framework is flexible across any pre-trained diffusion models and LLMs, and can be seamlessly integrated with the region-guided diffusion approaches. Extensive experiments on three datasets, including our newly proposed benchmark, RareBench, containing various prompts with rare compositions of concepts, R2F significantly surpasses existing models including SD3.0 and FLUX by up to 28.1%p in T2I alignment. Code is available at https://github.com/krafton-ai/Rare-to-Frequent. | Dongmin Park, Sebin Kim, Taehong Moon, Minkyu Kim, Kangwook Lee, Jaewoong Cho |  |
| 543 |  |  [POTEC: Off-Policy Contextual Bandits for Large Action Spaces via Policy Decomposition](https://openreview.net/forum?id=LXftdR11io) |  | 0 | We study off-policy learning (OPL) of contextual bandit policies in large discrete action spaces where existing methods -- most of which rely crucially on reward-regression models or importance-weighted policy gradients -- fail due to excessive bias or variance. To overcome these issues in OPL, we propose a novel two-stage algorithm, called Policy Optimization via Two-Stage Policy Decomposition (POTEC). It leverages clustering in the action space and learns two different policies via policy- and regression-based approaches, respectively. In particular, we derive a novel low-variance gradient estimator that enables to learn a first-stage policy for cluster selection efficiently via a policy-based approach. To select a specific action within the cluster sampled by the first-stage policy, POTEC uses a second-stage policy derived from a regression-based approach within each cluster. We show that a local correctness condition, which only requires that the regression model preserves the relative expected reward differences of the actions within each cluster, ensures that our policy-gradient estimator is unbiased and the second-stage policy is optimal. We also show that POTEC provides a strict generalization of policy- and regression-based approaches and their associated assumptions. Comprehensive experiments demonstrate that POTEC provides substantial improvements in OPL effectiveness particularly in large and structured action spaces. | Yuta Saito, Jihan Yao, Thorsten Joachims |  |
| 544 |  |  [MetaUrban: An Embodied AI Simulation Platform for Urban Micromobility](https://openreview.net/forum?id=kFsWpSxkFz) |  | 0 | Public urban spaces such as streetscapes and plazas serve residents and accommodate social life in all its vibrant variations. Recent advances in robotics and embodied AI make public urban spaces no longer exclusive to humans. Food delivery bots and electric wheelchairs have started sharing sidewalks with pedestrians, while robot dogs and humanoids have recently emerged in the street. \*\*Micromobility\*\* enabled by AI for short-distance travel in public urban spaces plays a crucial component in future transportation systems. It is essential to ensure the generalizability and safety of AI models used for maneuvering mobile machines. In this work, we present \*\*MetaUrban\*\*, a \*compositional\* simulation platform for the AI-driven urban micromobility research. MetaUrban can construct an \*infinite\* number of interactive urban scenes from compositional elements, covering a vast array of ground plans, object placements, pedestrians, vulnerable road users, and other mobile agents' appearances and dynamics. We design point navigation and social navigation tasks as the pilot study using MetaUrban for urban micromobility research and establish various baselines of Reinforcement Learning and Imitation Learning. We conduct extensive evaluation across mobile machines, demonstrating that heterogeneous mechanical structures significantly influence the learning and execution of AI policies. We perform a thorough ablation study, showing that the compositional nature of the simulated environments can substantially improve the generalizability and safety of the trained mobile agents. MetaUrban will be made publicly available to provide research opportunities and foster safe and trustworthy embodied AI and micromobility in cities. The code and data have been released. | Wayne Wu, Honglin He, Jack He, Yiran Wang, Chenda Duan, Zhizheng Liu, Quanyi Li, Bolei Zhou |  |
| 545 |  |  [D-FINE: Redefine Regression Task of DETRs as Fine-grained Distribution Refinement](https://openreview.net/forum?id=MFZjrTFE7h) |  | 0 | We introduce D-FINE, a powerful real-time object detector that achieves outstanding localization precision by redefining the bounding box regression task in DETR models. D-FINE comprises two key components: Fine-grained Distribution Refinement (FDR) and Global Optimal Localization Self-Distillation (GO-LSD). FDR transforms the regression process from predicting fixed coordinates to iteratively refining probability distributions, providing a fine-grained intermediate representation that significantly enhances localization accuracy. GO-LSD is a bidirectional optimization strategy that transfers localization knowledge from refined distributions to shallower layers through self-distillation, while also simplifying the residual prediction tasks for deeper layers. Additionally, D-FINE incorporates lightweight optimizations in computationally intensive modules and operations, achieving a better balance between speed and accuracy. Specifically, D-FINE-L / X achieves 54.0% / 55.8% AP on the COCO dataset at 124 / 78 FPS on an NVIDIA T4 GPU. When pretrained on Objects365, D-FINE-L / X attains 57.1% / 59.3% AP, surpassing all existing real-time detectors. Furthermore, our method significantly enhances the performance of a wide range of DETR models by up to 5.3% AP with negligible extra parameters and training costs. Our code and models: https://github.com/Peterande/D-FINE. | Yansong Peng, Hebei Li, Peixi Wu, Yueyi Zhang, Xiaoyan Sun, Feng Wu |  |
| 546 |  |  [DartControl: A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control](https://openreview.net/forum?id=XNA3Mnnbvb) |  | 0 | Text-conditioned human motion generation, which allows for user interaction through natural language, has become increasingly popular. Existing methods typically generate short, isolated motions based on a single input sentence. However, human motions are continuous and can extend over long periods, carrying rich semantics. Creating long, complex motions that precisely respond to streams of text descriptions, particularly in an online and real-time setting, remains a significant challenge. Furthermore, incorporating spatial constraints into text-conditioned motion generation presents additional challenges, as it requires aligning the motion semantics specified by text descriptions with geometric information, such as goal locations and 3D scene geometry. To address these limitations, we propose \*\*DartC\*\*ontrol, in short \*\*DART\*\*, a \*\*D\*\*iffusion-based \*\*A\*\*utoregressive motion primitive model for \*\*R\*\*eal-time \*\*T\*\*ext-driven motion \*\*C\*\*ontrol. Our model, DART, effectively learns a compact motion primitive space jointly conditioned on motion history and text inputs using latent diffusion models. By autoregressively generating motion primitives based on the preceding history and current text input, DART enables real-time, sequential motion generation driven by natural language descriptions. Additionally, the learned motion primitive space allows for precise spatial motion control, which we formulate either as a latent noise optimization problem or as a Markov decision process addressed through reinforcement learning. We present effective algorithms for both approaches, demonstrating our model’s versatility and superior performance in various motion synthesis tasks. Experiments show our method outperforms existing baselines in motion realism, efficiency, and controllability. Video results and code are available at https://zkf1997.github.io/DART/. | Kaifeng Zhao, Gen Li, Siyu Tang |  |
| 547 |  |  [Mitigating Information Loss in Tree-Based Reinforcement Learning via Direct Optimization](https://openreview.net/forum?id=qpXctF2aLZ) |  | 0 | Reinforcement learning (RL) has seen significant success across various domains, but its adoption is often limited by the black-box nature of neural network policies, making them difficult to interpret. In contrast, symbolic policies allow representing decision-making strategies in a compact and interpretable way. However, learning symbolic policies directly within on-policy methods remains challenging. In this paper, we introduce SYMPOL, a novel method for SYMbolic tree-based on-POLicy RL. SYMPOL employs a tree-based model integrated with a policy gradient method, enabling the agent to learn and adapt its actions while maintaining a high level of interpretability. We evaluate SYMPOL on a set of benchmark RL tasks, demonstrating its superiority over alternative tree-based RL approaches in terms of performance and interpretability. Unlike existing methods, it enables gradient-based, end-to-end learning of interpretable, axis-aligned decision trees within standard on-policy RL algorithms. Therefore, SYMPOL can become the foundation for a new class of interpretable RL based on decision trees. Our implementation is available under: https://github.com/s-marton/sympol | Sascha Marton, Tim Grams, Florian Vogt, Stefan Lüdtke, Christian Bartelt, Heiner Stuckenschmidt |  |
| 548 |  |  [Sharpness-Aware Minimization Efficiently Selects Flatter Minima Late In Training](https://openreview.net/forum?id=aD2uwhLbnA) |  | 0 | Sharpness-Aware Minimization (SAM) has substantially improved the generalization of neural networks under various settings. Despite the success, its effectiveness remains poorly understood. In this work, we discover an intriguing phenomenon in the training dynamics of SAM, shedding light on understanding its implicit bias towards flatter minima over Stochastic Gradient Descent (SGD). Specifically, we find that \*SAM efficiently selects flatter minima late in training\*. Remarkably, even a few epochs of SAM applied at the end of training yield nearly the same generalization and solution sharpness as full SAM training. Subsequently, we delve deeper into the underlying mechanism behind this phenomenon. Theoretically, we identify two phases in the learning dynamics after applying SAM late in training: i) SAM first escapes the minimum found by SGD exponentially fast; and ii) then rapidly converges to a flatter minimum within the same valley. Furthermore, we empirically investigate the role of SAM during the early training phase. We conjecture that the optimization method chosen in the late phase is more crucial in shaping the final solution's properties. Based on this viewpoint, we extend our findings from SAM to Adversarial Training. | Zhanpeng Zhou, Mingze Wang, Yuchen Mao, Bingrui Li, Junchi Yan |  |
| 549 |  |  [What Makes a Good Diffusion Planner for Decision Making?](https://openreview.net/forum?id=7BQkXXM8Fy) |  | 0 | Diffusion models have recently shown significant potential in solving decision-making problems, particularly in generating behavior plans -- also known as diffusion planning. While numerous studies have demonstrated the impressive performance of diffusion planning, the mechanisms behind the key components of a good diffusion planner remain unclear and the design choices are highly inconsistent in existing studies. In this work, we address this issue through systematic empirical experiments on diffusion planning in an offline reinforcement learning (RL) setting, providing practical insights into the essential components of diffusion planning. We trained and evaluated over 6,000 diffusion models, identifying the critical components such as guided sampling, network architecture, action generation and planning strategy. We revealed that some design choices opposite to the common practice in previous work in diffusion planning actually lead to better performance, e.g., unconditional sampling with selection can be better than guided sampling and Transformer outperforms U-Net as denoising network. Based on these insights, we suggest a simple yet strong diffusion planning baseline that achieves state-of-the-art results on standard offline RL benchmarks. Code: https://github.com/Josh00-Lu/DiffusionVeteran. | Haofei Lu, Dongqi Han, Yifei Shen, Dongsheng Li |  |
| 550 |  |  [ThinK: Thinner Key Cache by Query-Driven Pruning](https://openreview.net/forum?id=n0OtGl6VGb) |  | 0 | Large Language Models (LLMs) have revolutionized the field of natural language processing, achieving unprecedented performance across a variety of applications. However, their increased computational and memory demands present significant challenges, especially when handling long sequences. This paper focuses on the long-context scenario, addressing the inefficiencies in KV cache memory consumption during inference. Unlike existing approaches that optimize the memory based on the sequence length, we identify substantial redundancy in the channel dimension of the KV cache, as indicated by an uneven magnitude distribution and a low-rank structure in the attention weights. In response, we propose ThinK, a novel query-dependent KV cache pruning method designed to minimize attention weight loss while selectively pruning the least significant channels. Our approach not only maintains or enhances model accuracy but also achieves a reduction in KV cache memory costs by over 20\% compared with vanilla KV cache eviction and quantization methods. For instance, ThinK integrated with KIVI can achieve $2.8\times$ peak memory reduction while maintaining nearly the same quality, enabling a batch size increase from 4$\times$ (with KIVI alone) to 5$\times$ when using a single GPU. Extensive evaluations on the LLaMA and Mistral models across various long-sequence datasets verified the efficiency of ThinK. Our code has been made available at https://github.com/SalesforceAIResearch/ThinK. | Yuhui Xu, Zhanming Jie, Hanze Dong, Lei Wang, Xudong Lu, Aojun Zhou, Amrita Saha, Caiming Xiong, Doyen Sahoo |  |
| 551 |  |  [Linear Spherical Sliced Optimal Transport: A Fast Metric for Comparing Spherical Data](https://openreview.net/forum?id=fgUFZAxywx) |  | 0 | Efficient comparison of spherical probability distributions becomes important in fields such as computer vision, geosciences, and medicine. Sliced optimal transport distances, such as spherical and stereographic spherical sliced Wasserstein distances, have recently been developed to address this need. These methods reduce the computational burden of optimal transport by slicing hyperspheres into one-dimensional projections, i.e., lines or circles. Concurrently, linear optimal transport has been proposed to embed distributions into $L^2$ spaces, where the $L^2$ distance approximates the optimal transport distance, thereby simplifying comparisons across multiple distributions. In this work, we introduce the Linear Spherical Sliced Optimal Transport (LSSOT) framework, which utilizes slicing to embed spherical distributions into $L^2$ spaces while preserving their intrinsic geometry, offering a computationally efficient metric for spherical probability measures. We establish the metricity of LSSOT and demonstrate its superior computational efficiency in applications such as cortical surface registration, 3D point cloud interpolation via gradient flow, and shape embedding. Our results demonstrate the significant computational benefits and high accuracy of LSSOT in these applications. | Xinran Liu, Yikun Bai, Rocio Diaz Martin, Kaiwen Shi, Ashkan Shahbazi, Bennett Allan Landman, Catie Chang, Soheil Kolouri |  |
| 552 |  |  [RelitLRM: Generative Relightable Radiance for Large Reconstruction Models](https://openreview.net/forum?id=3Oli4u6q3p) |  | 0 | We propose RelitLRM, a Large Reconstruction Model (LRM) for generating high-quality Gaussian splatting representations of 3D objects under novel illuminations from sparse (4-8) posed images captured under unknown static lighting. Unlike prior inverse rendering methods requiring dense captures and slow optimization, often causing artifacts like incorrect highlights or shadow baking, RelitLRM adopts a feed-forward transformer-based model with a novel combination of a geometry reconstructor and a relightable appearance generator based on diffusion. The model is trained end-to-end on synthetic multi-view renderings of objects under varying known illuminations. This architecture design enables to effectively decompose geometry and appearance, resolve the ambiguity between material and lighting, and capture the multi-modal distribution of shadows and specularity in the relit appearance. We show our sparse-view feed-forward RelitLRM offers competitive relighting results to state-of-the-art dense-view optimization-based baselines while being significantly faster. Our project page is available at: https://relit-lrm.github.io/. | Tianyuan Zhang, Zhengfei Kuang, Haian Jin, Zexiang Xu, Sai Bi, Hao Tan, He Zhang, Yiwei Hu, Milos Hasan, William T. Freeman, Kai Zhang, Fujun Luan |  |
| 553 |  |  [A Geometric Framework for Understanding Memorization in Generative Models](https://openreview.net/forum?id=aZ1gNJu8wO) |  | 0 | As deep generative models have progressed, recent work has shown them to be capable of memorizing and reproducing training datapoints when deployed. These findings call into question the usability of generative models, especially in light of the legal and privacy risks brought about by memorization. To better understand this phenomenon, we propose the \*manifold memorization hypothesis\* (MMH), a geometric framework which leverages the manifold hypothesis into a clear language in which to reason about memorization. We propose to analyze memorization in terms of the relationship between the dimensionalities of $(i)$ the ground truth data manifold and $(ii)$ the manifold learned by the model. This framework provides a formal standard for "how memorized" a datapoint is and systematically categorizes memorized data into two types: memorization driven by overfitting and memorization driven by the underlying data distribution. By analyzing prior work in the context of the MMH, we explain and unify assorted observations in the literature. We empirically validate the MMH using synthetic data and image datasets up to the scale of Stable Diffusion, developing new tools for detecting and preventing generation of memorized samples in the process. | Brendan Leigh Ross, Hamidreza Kamkari, Tongzi Wu, Rasa Hosseinzadeh, Zhaoyan Liu, George Stein, Jesse C. Cresswell, Gabriel LoaizaGanem |  |
| 554 |  |  [Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control](https://openreview.net/forum?id=xQBRrtQM8u) |  | 0 | Dynamical generative models that produce samples through an iterative process, such as Flow Matching and denoising diffusion models, have seen widespread use, but there have not been many theoretically-sound methods for improving these models with reward fine-tuning. In this work, we cast reward fine-tuning as stochastic optimal control (SOC). Critically, we prove that a very specific \*memoryless\* noise schedule must be enforced during fine-tuning, in order to account for the dependency between the noise variable and the generated samples. We also propose a new algorithm named \*Adjoint Matching\* which outperforms existing SOC algorithms, by casting SOC problems as a regression problem. We find that our approach significantly improves over existing methods for reward fine-tuning, achieving better consistency, realism, and generalization to unseen human preference reward models, while retaining sample diversity. | Carles DomingoEnrich, Michal Drozdzal, Brian Karrer, Ricky T. Q. Chen |  |
| 555 |  |  [ConFIG: Towards Conflict-free Training of Physics Informed Neural Networks](https://openreview.net/forum?id=APojAzJQiq) |  | 0 | The loss functions of many learning problems contain multiple additive terms that can disagree and yield conflicting update directions. For Physics-Informed Neural Networks (PINNs), loss terms on initial/boundary conditions and physics equations are particularly interesting as they are well-established as highly difficult tasks. To improve learning the challenging multi-objective task posed by PINNs, we propose the ConFIG method, which provides conflict-free updates by ensuring a positive dot product between the final update and each loss-specific gradient. It also maintains consistent optimization rates for all loss terms and dynamically adjusts gradient magnitudes based on conflict levels. We additionally leverage momentum to accelerate optimizations by alternating the back-propagation of different loss terms. We provide a mathematical proof showing the convergence of the ConFIG method, and it is evaluated across a range of challenging PINN scenarios. ConFIG consistently shows superior performance and runtime compared to baseline methods. We also test the proposed method in a classic multi-task benchmark, where the ConFIG method likewise exhibits a highly promising performance. Source code is available at https://tum-pbs.github.io/ConFIG | Qiang Liu, Mengyu Chu, Nils Thuerey |  |
| 556 |  |  [DLEFT-MKC: Dynamic Late Fusion Multiple Kernel Clustering with Robust Tensor Learning via Min-Max Optimization](https://openreview.net/forum?id=HE5JmwniHm) |  | 0 | Recent advancements in multiple kernel clustering (MKC) have highlighted the effectiveness of late fusion strategies, particularly in enhancing computational efficiency to near-linear complexity while achieving promising clustering performance. However, existing methods encounter three significant limitations: (1) reliance on fixed base partition matrices that do not adaptively optimize during the clustering process, thereby constraining their performance to the inherent representational capabilities of these matrices; (2) a focus on adjusting kernel weights to explore inter-view consistency and complementarity, which often neglects the intrinsic high-order correlations among views, thereby limiting the extraction of comprehensive multiple kernel information; (3) a lack of adaptive mechanisms to accommodate varying distributions within the data, which limits robustness and generalization. To address these challenges, this paper proposes a novel algorithm termed Dynamic Late Fusion Multiple Kernel Clustering with Robust {Tensor Learning via min-max optimization (DLEFT-MKC), which effectively overcomes the representational bottleneck of base partition matrices and facilitates the learning of meaningful high-order cross-view information. Specifically, it is the first to incorporate a min-max optimization paradigm into tensor-based MKC, enhancing algorithm robustness and generalization. Additionally, it dynamically reconstructs decision layers to enhance representation capabilities and subsequently stacks the reconstructed representations for tensor learning that promotes the capture of high-order associations and cluster structures across views, ultimately yielding consensus clustering partitions. To solve the resultant optimization problem, we innovatively design a strategy that combines reduced gradient descent with the alternating direction method of multipliers, ensuring convergence to local optima while maintaining high computational efficiency. Extensive experimental results across various benchmark datasets validate the superior effectiveness and efficiency of the proposed DLEFT-MKC. | Yi Zhang, Siwei Wang, Jiyuan Liu, Shengju Yu, Zhibin Dong, Suyuan Liu, Xinwang Liu, En Zhu |  |
| 557 |  |  [Hymba: A Hybrid-head Architecture for Small Language Models](https://openreview.net/forum?id=A1ztozypga) |  | 0 | We propose Hymba, a family of small language models featuring a hybrid-head parallel architecture that integrates attention mechanisms and state space models (SSMs) within the same layer, offering parallel and complementary processing of the same inputs. In this hybrid-head module, attention heads provide high-resolution recall, while SSM heads facilitate efficient context summarization. Additionally, we introduce learnable meta tokens, which are prepended to prompts to store critical meta information, guiding subsequent tokens and alleviating the “forced-to-attend” burden associated with attention mechanisms. Thanks to the global context summarized by SSMs, the attention heads in our model can be further optimized through cross-layer key-value (KV) sharing and a mix of global and local attention, resulting in a compact cache size without compromising accuracy. Notably, Hymba achieves state-of-the-art performance among small LMs: Our Hymba-1.5B-Base model surpasses all sub-2B public models and even outperforms Llama-3.2-3B, achieving 1.32\% higher average accuracy, an 11.67$\times$ reduction in cache size, and 3.49$\times$ higher throughput. | Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabaleshwarkar, ShihYang Liu, Matthijs Van Keirsbilck, MinHung Chen, Yoshi Suhara, Yingyan Celine Lin, Jan Kautz, Pavlo Molchanov |  |
| 558 |  |  [Severing Spurious Correlations with Data Pruning](https://openreview.net/forum?id=Bk13Qfu8Ru) |  | 0 | Deep neural networks have been shown to learn and rely on spurious correlations present in the data that they are trained on. Reliance on such correlations can cause these networks to malfunction when deployed in the real world, where these correlations may no longer hold. To overcome the learning of and reliance on such correlations, recent studies propose approaches that yield promising results. These works, however, study settings where the strength of the spurious signal is significantly greater than that of the core, invariant signal, making it easier to detect the presence of spurious features in individual training samples and allow for further processing. In this paper, we identify new settings where the strength of the spurious signal is relatively weaker, making it difficult to detect any spurious information while continuing to have catastrophic consequences. We also discover that spurious correlations are learned primarily due to only a handful of all the samples containing the spurious feature and develop a novel data pruning technique that identifies and prunes small subsets of the training data that contain these samples. Our proposed technique does not require inferred domain knowledge, information regarding the sample-wise presence or nature of spurious information, or human intervention. Finally, we show that such data pruning attains state-of-the-art performance on previously studied settings where spurious information is identifiable. | Varun Mulchandani, JungEun Kim |  |
| 559 |  |  [CubeDiff: Repurposing Diffusion-Based Image Models for Panorama Generation](https://openreview.net/forum?id=M2SsqpxGtc) |  | 0 | We introduce a novel method for generating 360° panoramas from text prompts or images. Our approach leverages recent advances in 3D generation by employing multi-view diffusion models to jointly synthesize the six faces of a cubemap. Unlike previous methods that rely on processing equirectangular projections or autoregressive generation, our method treats each face as a standard perspective image, simplifying the generation process and enabling the use of existing multi-view diffusion models. We demonstrate that these models can be adapted to produce high-quality cubemaps without requiring correspondence-aware attention layers. Our model allows for fine-grained text control, generates high resolution panorama images and generalizes well beyond its training set, whilst achieving state-of-the-art results, both qualitatively and quantitatively. | Nikolai Kalischek, Michael Oechsle, Fabian Manhardt, Philipp Henzler, Konrad Schindler, Federico Tombari |  |
| 560 |  |  [Language Model Alignment in Multilingual Trolley Problems](https://openreview.net/forum?id=VEqPDZIDAh) |  | 0 | We evaluate the moral alignment of large language models (LLMs) with human preferences in multilingual trolley problems. Building on the Moral Machine experiment, which captures over 40 million human judgments across 200+ countries, we develop a cross-lingual corpus of moral dilemma vignettes in over 100 languages called MultiTP. This dataset enables the assessment of LLMs' decision-making processes in diverse linguistic contexts. Our analysis explores the alignment of 19 different LLMs with human judgments, capturing preferences across six moral dimensions: species, gender, fitness, status, age, and the number of lives involved. By correlating these preferences with the demographic distribution of language speakers and examining the consistency of LLM responses to various prompt paraphrasings, our findings provide insights into cross-lingual and ethical biases of LLMs and their intersection. We discover significant variance in alignment across languages, challenging the assumption of uniform moral reasoning in AI systems and highlighting the importance of incorporating diverse perspectives in AI ethics. The results underscore the need for further research on the integration of multilingual dimensions in responsible AI research to ensure fair and equitable AI interactions worldwide. | Zhijing Jin, Max KleimanWeiner, Giorgio Piatti, Sydney Levine, Jiarui Liu, Fernando Gonzalez Adauto, Francesco Ortu, András Strausz, Mrinmaya Sachan, Rada Mihalcea, Yejin Choi, Bernhard Schölkopf |  |
| 561 |  |  [Century: A Framework and Dataset for Evaluating Historical Contextualisation of Sensitive Images](https://openreview.net/forum?id=1KLBvrYz3V) |  | 0 | How do multi-modal generative models describe images of recent historical events and figures, whose legacies may be nuanced, multifaceted, or contested? This task necessitates not only accurate visual recognition, but also socio-cultural knowledge and cross-modal reasoning. To address this evaluation challenge, we introduce Century -- a novel dataset of sensitive historical images. This dataset consists of 1,500 images from recent history, created through an automated method combining knowledge graphs and language models with quality and diversity criteria created from the practices of museums and digital archives. We demonstrate through automated and human evaluation that this method produces a set of images that depict events and figures that are diverse across topics and represents all regions of the world. We additionally propose an evaluation framework for evaluating the historical contextualisation capabilities along dimensions of accuracy, thoroughness, and objectivity. We demonstrate this approach by using Century to evaluate four foundation models, scoring performance using both automated and human evaluation. We find that historical contextualisation of sensitive images poses a significant challenge for modern multi-modal foundation models, and offer practical recommendations for how developers can use Century to evaluate improvements to models and applications. | Canfer Akbulut, Kevin Robinson, Maribeth Rauh, Isabela Albuquerque, Olivia Wiles, Laura Weidinger, Verena Rieser, Yana Hasson, Nahema Marchal, Iason Gabriel, William Isaac, Lisa Anne Hendricks |  |
| 562 |  |  [Determine-Then-Ensemble: Necessity of Top-k Union for Large Language Model Ensembling](https://openreview.net/forum?id=FDnZFpHmU4) |  | 0 | Large language models (LLMs) exhibit varying strengths and weaknesses across different tasks, prompting recent studies to explore the benefits of ensembling models to leverage their complementary advantages. However, existing LLM ensembling methods often overlook model compatibility and struggle with inefficient alignment of probabilities across the entire vocabulary. In this study, we empirically investigate the factors influencing ensemble performance, identifying model performance, vocabulary size, and response style as key determinants, revealing that compatibility among models is essential for effective ensembling. This analysis leads to the development of a simple yet effective model selection strategy that identifies compatible models. Additionally, we introduce the \textsc{Uni}on \textsc{T}op-$k$ \textsc{E}nsembling (\textsc{UniTE}), a novel approach that efficiently combines models by focusing on the union of the top-k tokens from each model, thereby avoiding the need for full vocabulary alignment and reducing computational overhead. Extensive evaluations across multiple benchmarks demonstrate that \textsc{UniTE} significantly enhances performance compared to existing methods, offering a more efficient framework for LLM ensembling. | Yuxuan Yao, Han Wu, Mingyang Liu, Sichun Luo, Xiongwei Han, Jie Liu, Zhijiang Guo, Linqi Song |  |
| 563 |  |  [MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code](https://openreview.net/forum?id=1Iuw1jcIrf) |  | 0 | Code has been shown to be effective in enhancing the mathematical reasoning abilities of large language models due to its precision and accuracy. Previous works involving continued mathematical pretraining often include code that utilizes math-related packages, which are primarily designed for fields such as engineering, machine learning, signal processing, or module testing, rather than being directly focused on mathematical reasoning. In this paper, we introduce a novel method for generating mathematical code accompanied with corresponding reasoning steps for continued pretraining. Our approach begins with the construction of a high-quality mathematical continued pretraining dataset by incorporating math-related web data, code using mathematical packages, math textbooks, and synthetic data. Next, we construct reasoning steps by extracting LaTeX expressions, the conditions needed for the expressions, and the results of the expressions from the previously collected dataset. Based on this extracted information, we generate corresponding code to accurately capture the mathematical reasoning process. Appending the generated code to each reasoning step results in data consisting of paired natural language reasoning steps and their corresponding code. Combining this data with the original dataset results in a 19.2B-token high-performing mathematical pretraining corpus, which we name MathCode-Pile. Training several popular base models with this corpus significantly improves their mathematical abilities, leading to the creation of the MathCoder2 family of models. All of our data processing and training code is open-sourced, ensuring full transparency and easy reproducibility of the entire data collection and training pipeline. | Zimu Lu, Aojun Zhou, Ke Wang, Houxing Ren, Weikang Shi, Junting Pan, Mingjie Zhan, Hongsheng Li |  |
| 564 |  |  [Training-Free Activation Sparsity in Large Language Models](https://openreview.net/forum?id=dGVZwyq5tV) |  | 0 | Activation sparsity can enable practical inference speedups in large language models (LLMs) by reducing the compute and memory-movement required for matrix multiplications during the forward pass. However, existing methods face limitations that inhibit widespread adoption. Some approaches are tailored towards older models with ReLU-based sparsity, while others require extensive continued pre-training on up to hundreds of billions of tokens. This paper describes TEAL (\*\*T\*\*raining-Fre\*\*e\*\* \*\*A\*\*ctivation Sparsity in \*\*L\*\*LMs), a simple training-free method that applies magnitude-based activation sparsity to hidden states throughout the entire model. TEAL achieves 40-50\% model-wide sparsity with minimal performance degradation across Llama-2, Llama-3, and Mistral families, with sizes varying from 7B to 70B. We improve existing sparse kernels and demonstrate wall-clock decoding speed-ups of up to 1.53× and 1.8× at 40\% and 50\% model-wide sparsity. TEAL is compatible with weight quantization, enabling further efficiency gains. | James Liu, Pragaash Ponnusamy, Tianle Cai, Han Guo, Yoon Kim, Ben Athiwaratkun |  |
| 565 |  |  [Poison-splat: Computation Cost Attack on 3D Gaussian Splatting](https://openreview.net/forum?id=ExrEw8cVlU) |  | 0 | 3D Gaussian splatting (3DGS), known for its groundbreaking performance and efficiency, has become a dominant 3D representation and brought progress to many 3D vision tasks. However, in this work, we reveal a significant security vulnerability that has been largely overlooked in 3DGS: the computation cost of training 3DGS could be maliciously tampered by poisoning the input data. By developing an attack named Poison-splat, we reveal a novel attack surface where the adversary can poison the input images to drastically increase the computation memory and time needed for 3DGS training, pushing the algorithm towards its worst computation complexity. In extreme cases, the attack can even consume all allocable memory, leading to a Denial-of-Service (DoS) that disrupts servers, resulting in practical damages to real-world 3DGS service vendors. Such a computation cost attack is achieved by addressing a bi-level optimization problem through three tailored strategies: attack objective approximation, proxy model rendering, and optional constrained optimization. These strategies not only ensure the effectiveness of our attack but also make it difficult to defend with simple defensive measures. We hope the revelation of this novel attack surface can spark attention to this crucial yet overlooked vulnerability of 3DGS systems. Our code is available at https://github.com/jiahaolu97/poison-splat . | Jiahao Lu, Yifan Zhang, Qiuhong Shen, Xinchao Wang, Shuicheng Yan |  |
| 566 |  |  [Gap-Dependent Bounds for Q-Learning using Reference-Advantage Decomposition](https://openreview.net/forum?id=6tyPSkshtF) |  | 0 | We study the gap-dependent bounds of two important algorithms for on-policy $Q$-learning for finite-horizon episodic tabular Markov Decision Processes (MDPs): UCB-Advantage (Zhang et al. 2020) and Q-EarlySettled-Advantage (Li et al. 2021). UCB-Advantage and Q-EarlySettled-Advantage improve upon the results based on Hoeffding-type bonuses and achieve the {almost optimal} $\sqrt{T}$-type regret bound in the worst-case scenario, where $T$ is the total number of steps. However, the benign structures of the MDPs such as a strictly positive suboptimality gap can significantly improve the regret. While gap-dependent regret bounds have been obtained for $Q$-learning with Hoeffding-type bonuses, it remains an open question to establish gap-dependent regret bounds for $Q$-learning using variance estimators in their bonuses and reference-advantage decomposition for variance reduction. We develop a novel error decomposition framework to prove gap-dependent regret bounds of UCB-Advantage and Q-EarlySettled-Advantage that are logarithmic in $T$ and improve upon existing ones for $Q$-learning algorithms. Moreover, we establish the gap-dependent bound for the policy switching cost of UCB-Advantage and improve that under the worst-case MDPs. To our knowledge, this paper presents the first gap-dependent regret analysis for $Q$-learning using variance estimators and reference-advantage decomposition and also provides the first gap-dependent analysis on policy switching cost for $Q$-learning. | Zhong Zheng, Haochen Zhang, Lingzhou Xue |  |
| 567 |  |  [Effective Interplay between Sparsity and Quantization: From Theory to Practice](https://openreview.net/forum?id=wJv4AIt4sK) |  | 0 | The increasing size of deep neural networks (DNNs) necessitates effective model compression to reduce their computational and memory footprints. Sparsity and quantization are two prominent compression methods that have been shown to reduce DNNs' computational and memory footprints significantly while preserving model accuracy. However, how these two methods interact when combined together remains a key question for developers, as many tacitly assume that they are orthogonal, meaning that their combined use does not introduce additional errors beyond those introduced by each method independently. In this paper, we provide the first mathematical proof that sparsity and quantization are non-orthogonal. We corroborate these results with experiments spanning a range of large language models, including the OPT and LLaMA model families (with 125M to 8B parameters), and vision models like ViT and ResNet. We show that the order in which we apply these methods matters because applying quantization before sparsity may disrupt the relative importance of tensor elements, which may inadvertently remove significant elements from a tensor. More importantly, we show that even if applied in the correct order, the compounded errors from sparsity and quantization can significantly harm accuracy. Our findings extend to the efficient deployment of large models in resource-constrained compute platforms to reduce serving cost, offering insights into best practices for applying these compression methods to maximize hardware resource efficiency without compromising accuracy. | Simla Burcu Harma, Ayan Chakraborty, Elizaveta Kostenok, Danila Mishin, Dongho Ha, Babak Falsafi, Martin Jaggi, Ming Liu, Yunho Oh, Suvinay Subramanian, Amir Yazdanbakhsh |  |
| 568 |  |  [One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation Using a Single Prompt](https://openreview.net/forum?id=cD1kl2QKv1) |  | 0 | Text-to-image generation models can create high-quality images from input prompts. However, they struggle to support the consistent generation of identity-preserving requirements for storytelling. Existing approaches to this problem typically require extensive training in large datasets or additional modifications to the original model architectures. This limits their applicability across different domains and diverse diffusion model configurations. In this paper, we first observe the inherent capability of language models, coined $\textit{context consistency}$, to comprehend identity through context with a single prompt. Drawing inspiration from the inherent $\textit{context consistency}$, we propose a novel $\textit{training-free}$ method for consistent text-to-image (T2I) generation, termed "One-Prompt-One-Story" ($\textit{1Prompt1Story}$). Our approach $\textit{1Prompt1Story}$ concatenates all prompts into a single input for T2I diffusion models, initially preserving character identities. We then refine the generation process using two novel techniques: $\textit{Singular-Value Reweighting}$ and $\textit{Identity-Preserving Cross-Attention}$, ensuring better alignment with the input description for each frame. In our experiments, we compare our method against various existing consistent T2I generation approaches to demonstrate its effectiveness, through quantitative metrics and qualitative assessments. Code is available at https://github.com/byliutao/1Prompt1Story. | Tao Liu, Kai Wang, Senmao Li, Joost van de Weijer, Fahad Shahbaz Khan, Shiqi Yang, Yaxing Wang, Jian Yang, MingMing Cheng |  |
| 569 |  |  [GrabS: Generative Embodied Agent for 3D Object Segmentation without Scene Supervision](https://openreview.net/forum?id=wXSshrxlP4) |  | 0 | We study the hard problem of 3D object segmentation in complex point clouds without requiring human labels of 3D scenes for supervision. By relying on the similarity of pretrained 2D features or external signals such as motion to group 3D points as objects, existing unsupervised methods are usually limited to identifying simple objects like cars or their segmented objects are often inferior due to the lack of objectness in pretrained features. In this paper, we propose a new two- stage pipeline called GrabS. The core concept of our method is to learn generative and discriminative object-centric priors as a foundation from object datasets in the first stage, and then design an embodied agent to learn to discover multiple ob- jects by querying against the pretrained generative priors in the second stage. We extensively evaluate our method on two real-world datasets and a newly created synthetic dataset, demonstrating remarkable segmentation performance, clearly surpassing all existing unsupervised methods. | Zihui Zhang, Yafei Yang, Hongtao Wen, Bo Yang |  |
| 570 |  |  [Scalable and Certifiable Graph Unlearning: Overcoming the Approximation Error Barrier](https://openreview.net/forum?id=pPyJyeLriR) |  | 0 | Graph unlearning has emerged as a pivotal research area for ensuring privacy protection, given the widespread adoption of Graph Neural Networks (GNNs) in applications involving sensitive user data. Among existing studies, certified graph unlearning is distinguished by providing robust privacy guarantees. However, current certified graph unlearning methods are impractical for large-scale graphs because they necessitate the costly re-computation of graph propagation for each unlearning request. Although numerous scalable techniques have been developed to accelerate graph propagation for GNNs, their integration into certified graph unlearning remains uncertain as these scalable approaches introduce approximation errors into node embeddings. In contrast, certified graph unlearning demands bounded model error on exact node embeddings to maintain its certified guarantee. To address this challenge, we present ScaleGUN, the first approach to scale certified graph unlearning to billion-edge graphs. ScaleGUN integrates the approximate graph propagation technique into certified graph unlearning, offering certified guarantees for three unlearning scenarios: node feature, edge and node unlearning. Extensive experiments on real-world datasets demonstrate the efficiency and unlearning efficacy of ScaleGUN. Remarkably, ScaleGUN accomplishes $(\epsilon,\delta)=(1,10^{-4})$ certified unlearning on the billion-edge graph ogbn-papers100M in 20 seconds for a 5,000 random edge removal request -- of which only 5 seconds are required for updating the node embeddings -- compared to 1.91 hours for retraining and 1.89 hours for re-propagation. Our code is available at https://github.com/luyi256/ScaleGUN. | Lu Yi, Zhewei Wei |  |
| 571 |  |  [Enhancing Pre-trained Representation Classifiability can Boost its Interpretability](https://openreview.net/forum?id=GjfIZan5jN) |  | 0 | The visual representation of a pre-trained model prioritizes the classifiability on downstream tasks, while the widespread applications for pre-trained visual models have posed new requirements for representation interpretability. However, it remains unclear whether the pre-trained representations can achieve high interpretability and classifiability simultaneously. To answer this question, we quantify the representation interpretability by leveraging its correlation with the ratio of interpretable semantics within the representations. Given the pre-trained representations, only the interpretable semantics can be captured by interpretations, whereas the uninterpretable part leads to information loss. Based on this fact, we propose the Inherent Interpretability Score (IIS) that evaluates the information loss, measures the ratio of interpretable semantics, and quantifies the representation interpretability. In the evaluation of the representation interpretability with different classifiability, we surprisingly discover that the interpretability and classifiability are positively correlated, i.e., representations with higher classifiability provide more interpretable semantics that can be captured in the interpretations. This observation further supports two benefits to the pre-trained representations. First, the classifiability of representations can be further improved by fine-tuning with interpretability maximization. Second, with the classifiability improvement for the representations, we obtain predictions based on their interpretations with less accuracy degradation. The discovered positive correlation and corresponding applications show that practitioners can unify the improvements in interpretability and classifiability for pre-trained vision models. Codes are available at https://github.com/ssfgunner/IIS. | Shufan Shen, Zhaobo Qi, Junshu Sun, Qingming Huang, Qi Tian, Shuhui Wang |  |
| 572 |  |  [TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters](https://openreview.net/forum?id=oQ4igHyh3N) |  | 0 | Transformers have become the predominant architecture in foundation models due to their excellent performance across various domains. However, the substantial cost of scaling these models remains a significant concern. This problem arises primarily from their dependence on a fixed number of parameters within linear projections. When architectural modifications (e.g., channel dimensions) are introduced, the entire model typically requires retraining from scratch. As model sizes continue growing, this strategy results in increasingly high computational costs and becomes unsustainable. To overcome this problem, we introduce Tokenformer, a natively scalable architecture that leverages the attention mechanism not only for computations among input tokens but also for interactions between tokens and model parameters, thereby enhancing architectural flexibility. By treating model parameters as tokens, we replace all the linear projections in Transformers with our token-parameter attention layer, where input tokens act as queries and model parameters as keys and values. This reformulation allows for progressive and efficient scaling without necessitating retraining from scratch. Our model scales from 124M to 1.4B parameters by incrementally adding new key-value parameter pairs, achieving performance comparable to Transformers trained from scratch while greatly reducing training costs. Code and models are available at {\color{red}\url{https://github.com/Haiyang-W/TokenFormer.git}} | Haiyang Wang, Yue Fan, Muhammad Ferjad Naeem, Yongqin Xian, Jan Eric Lenssen, Liwei Wang, Federico Tombari, Bernt Schiele |  |
| 573 |  |  [Simple yet Effective Incomplete Multi-view Clustering: Similarity-level Imputation and Intra-view Hybrid-group Prototype Construction](https://openreview.net/forum?id=KijslFbfOL) |  | 0 | Most of incomplete multi-view clustering (IMVC) methods typically choose to ignore the missing samples and only utilize observed unpaired samples to construct bipartite similarity. Moreover, they employ a single quantity of prototypes to extract the information of $\textbf{all}$ views. To eliminate these drawbacks, we present a simple yet effective IMVC approach, SIIHPC, in this work. It firstly transforms partial bipartition learning into original sample form by virtue of reconstruction concept to split out of observed similarity, and then loosens traditional non-negative constraints via regularizing samples to more freely characterize the similarity. Subsequently, it learns to recover the incomplete parts by utilizing the connection built between the similarity exclusive on respective view and the consensus graph shared for all views. On this foundation, it further introduces a group of hybrid prototype quantities for each individual view to flexibly extract the data features belonging to each view itself. Accordingly, the resulting graphs are with various scales and describe the overall similarity more comprehensively. It is worth mentioning that these all are optimized in one unified learning framework, which makes it possible for them to reciprocally promote. Then, to effectively solve the formulated optimization problem, we design an ingenious auxiliary function that is with theoretically proven monotonic-increasing properties. Finally, the clustering results are obtained by implementing spectral grouping action on the eigenvectors of stacked multi-scale consensus similarity. Experimental results confirm the effectiveness of SIIHPC. | Shengju Yu, Zhibin Dong, Siwei Wang, Pei Zhang, Yi Zhang, Xinwang Liu, Naiyang Guan, Tiejun Li, Yiuming Cheung |  |
| 574 |  |  [Stem-OB: Generalizable Visual Imitation Learning with Stem-Like Convergent Observation through Diffusion Inversion](https://openreview.net/forum?id=xaYlO03tIk) |  | 0 | Visual imitation learning methods demonstrate strong performance, yet they lack generalization when faced with visual input perturbations like variations in lighting and textures. This limitation hampers their practical application in real-world settings. To address this, we propose \*\*\*Stem-OB\*\*\* that leverages the inversion process of pretrained image diffusion models to suppress low-level visual differences while maintaining high-level scene structures. This image inversion process is akin to transforming the observation into a shared representation, from which other observations also stem. \*Stem-OB\* offers a simple yet effective plug-and-play solution that stands in contrast to data augmentation approaches. It demonstrates robustness to various unspecified appearance changes without the need for additional training. We provide theoretical insights and empirical results that validate the efficacy of our approach in simulated and real settings. \*Stem-OB\* shows an exceptionally significant improvement in real-world robotic tasks, where challenging light and appearance changes are present, with an average increase of \*\*22.2%\*\* in success rates compared to the best baseline. Please refer to [this link](https://stem-ob.github.io/) for more videos and details. | Kaizhe Hu, Zihang Rui, Yao He, Yuyao Liu, Pu Hua, Huazhe Xu |  |
| 575 |  |  [LiveBench: A Challenging, Contamination-Limited LLM Benchmark](https://openreview.net/forum?id=sKYHBTAxVa) |  | 0 | Test set contamination, wherein test data from a benchmark ends up in a newer model's training set, is a well-documented obstacle for fair LLM evaluation and can quickly render benchmarks obsolete. To mitigate this, many recent benchmarks crowdsource new prompts and evaluations from human or LLM judges; however, these can introduce significant biases, and break down when scoring hard questions. In this work, we introduce a new benchmark for LLMs designed to be resistant to both test set contamination and the pitfalls of LLM judging and human crowdsourcing. We release LiveBench, the first benchmark that (1) contains frequently-updated questions from recent information sources, (2) scores answers automatically according to objective ground-truth values, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis. To achieve this, LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-limited versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source models, as well as dozens of open-source models ranging from 0.5B to 405B in size. LiveBench is difficult, with top models achieving below 70% accuracy. We release all questions, code, and model answers. Questions are added and updated on a monthly basis, and we release new tasks and harder versions of tasks over time so that LiveBench can distinguish between the capabilities of LLMs as they improve in the future. We welcome community engagement and collaboration for expanding the benchmark tasks and models. | Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Benjamin Feuer, Siddhartha Jain, Ravid ShwartzZiv, Neel Jain, Khalid Saifullah, Sreemanti Dey, ShubhAgrawal, Sandeep Singh Sandha, Siddartha V. Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, Micah Goldblum |  |
| 576 |  |  [Anti-Exposure Bias in Diffusion Models](https://openreview.net/forum?id=MtDd7rWok1) |  | 0 | Diffusion models (DMs) have achieved record-breaking performance in image generation tasks. Nevertheless, in practice, the training-sampling discrepancy, caused by score estimation error and discretization error, limits the modeling ability of DMs, a phenomenon known as exposure bias. To alleviate such exposure bias and further improve the generative performance, we put forward a prompt learning framework built upon a lightweight prompt prediction model. Concretely, our model learns an anti-bias prompt for the generated sample at each sampling step, aiming to compensate for the exposure bias that arises. Following this design philosophy, our framework rectifies the sampling trajectory to match the training trajectory, thereby reducing the divergence between the target data distribution and the modeling distribution. To train the prompt prediction model, we simulate exposure bias by constructing training data and introduce a time-dependent weighting function for optimization. Empirical results on various DMs demonstrate the superiority of our prompt learning framework across three benchmark datasets. Importantly, the optimized prompt prediction model effectively improves image quality with only a 5\% increase in sampling overhead, which remains negligible. | Junyu Zhang, Daochang Liu, Eunbyung Park, Shichao Zhang, Chang Xu |  |
| 577 |  |  [DynamicCity: Large-Scale 4D Occupancy Generation from Dynamic Scenes](https://openreview.net/forum?id=M7KyLjuN0A) |  | 0 | Urban scene generation has been developing rapidly recently. However, existing methods primarily focus on generating static and single-frame scenes, overlooking the inherently dynamic nature of real-world driving environments. In this work, we introduce DynamicCity, a novel 4D occupancy generation framework capable of generating large-scale, high-quality dynamic 4D scenes with semantics. DynamicCity mainly consists of two key models. \*\*1)\*\* A VAE model for learning HexPlane as the compact 4D representation. Instead of using naive averaging operations, DynamicCity employs a novel \*\*Projection Module\*\* to effectively compress 4D features into six 2D feature maps for HexPlane construction, which significantly enhances HexPlane fitting quality (up to \*\*12.56\*\* mIoU gain). Furthermore, we utilize an \*\*Expansion & Squeeze Strategy\*\* to reconstruct 3D feature volumes in parallel, which improves both network training efficiency and reconstruction accuracy than naively querying each 3D point (up to \*\*7.05\*\* mIoU gain, \*\*2.06x\*\* training speedup, and \*\*70.84\%\*\* memory reduction). \*\*2)\*\* A DiT-based diffusion model for HexPlane generation. To make HexPlane feasible for DiT generation, a \*\*Padded Rollout Operation\*\* is proposed to reorganize all six feature planes of the HexPlane as a squared 2D feature map. In particular, various conditions could be introduced in the diffusion or sampling process, supporting \*\*versatile 4D generation applications\*\*, such as trajectory- and command-driven generation, inpainting, and layout-conditioned generation. Extensive experiments on the CarlaSC and Waymo datasets demonstrate that DynamicCity significantly outperforms existing state-of-the-art 4D occupancy generation methods across multiple metrics. The code and models have been released to facilitate future research. | Hengwei Bian, Lingdong Kong, Haozhe Xie, Liang Pan, Yu Qiao, Ziwei Liu |  |
| 578 |  |  [Computational Explorations of Total Variation Distance](https://openreview.net/forum?id=xak8c9l1nu) |  | 0 | We investigate some previously unexplored (or underexplored) computational aspects of total variation (TV) distance. First, we give a simple deterministic polynomial-time algorithm for checking equivalence between mixtures of product distributions, over arbitrary alphabets. This corresponds to a special case, whereby the TV distance between the two distributions is zero. Second, we prove that unless $\mathsf{NP} \subseteq \mathsf{RP}$ it is impossible to efficiently estimate the TV distance between arbitrary Ising models, even in a bounded-error randomized setting. | Arnab Bhattacharyya, Sutanu Gayen, Kuldeep S. Meel, Dimitrios Myrisiotis, Aduri Pavan, N. V. Vinodchandran |  |
| 579 |  |  [Reti-Diff: Illumination Degradation Image Restoration with Retinex-based Latent Diffusion Model](https://openreview.net/forum?id=kxFtMHItrf) |  | 0 | Illumination degradation image restoration (IDIR) techniques aim to improve the visibility of degraded images and mitigate the adverse effects of deteriorated illumination. Among these algorithms, diffusion-based models (DM) have shown promising performance but are often burdened by heavy computational demands and pixel misalignment issues when predicting the image-level distribution. To tackle these problems, we propose to leverage DM within a compact latent space to generate concise guidance priors and introduce a novel solution called Reti-Diff for the IDIR task. Specifically, Reti-Diff comprises two significant components: the Retinex-based latent DM (RLDM) and the Retinex-guided transformer (RGformer). RLDM is designed to acquire Retinex knowledge, extracting reflectance and illumination priors to facilitate detailed reconstruction and illumination correction. RGformer subsequently utilizes these compact priors to guide the decomposition of image features into their respective reflectance and illumination components. Following this, RGformer further enhances and consolidates these decomposed features, resulting in the production of refined images with consistent content and robustness to handle complex degradation scenarios. Extensive experiments demonstrate that Reti-Diff outperforms existing methods on three IDIR tasks, as well as downstream applications. | Chunming He, Chengyu Fang, Yulun Zhang, Longxiang Tang, Jinfa Huang, Kai Li, Zhenhua Guo, Xiu Li, Sina Farsiu |  |
| 580 |  |  [MaRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers](https://openreview.net/forum?id=yVeNBxwL5W) |  | 0 | In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions simpler and more natural. However, current training-free fast samplers are not directly applicable to MR Diffusion. And thus MR Diffusion requires hundreds of NFEs (number of function evaluations) to obtain high-quality samples. In this paper, we propose a new algorithm named MaRS (MR Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive semi-analytical solutions. The solutions consist of an analytical function and an integral parameterized by a neural network. Based on this solution, we can generate high-quality samples in fewer steps. Our approach does not require training and supports all mainstream parameterizations, including noise prediction, data prediction and velocity prediction. Extensive experiments demonstrate that MR Sampler maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm accelerates the sampling procedure of MR Diffusion, making it more practical in controllable generation. | Ao Li, Wei Fang, Hongbo Zhao, Le Lu, Ge Yang, Minfeng Xu |  |
| 581 |  |  [SRSA: Skill Retrieval and Adaptation for Robotic Assembly Tasks](https://openreview.net/forum?id=RInisw1yin) |  | 0 | Enabling robots to learn novel tasks in a data-efficient manner is a long-standing challenge. Common strategies involve carefully leveraging prior experiences, especially transition data collected on related tasks. Although much progress has been made for general pick-and-place manipulation, far fewer studies have investigated contact-rich assembly tasks, where precise control is essential. We introduce SRSA} (Skill Retrieval and Skill Adaptation), a novel framework designed to address this problem by utilizing a pre-existing skill library containing policies for diverse assembly tasks. The challenge lies in identifying which skill from the library is most relevant for fine-tuning on a new task. Our key hypothesis is that skills showing higher zero-shot success rates on a new task are better suited for rapid and effective fine-tuning on that task. To this end, we propose to predict the transfer success for all skills in the skill library on a novel task, and then use this prediction to guide the skill retrieval process. We establish a framework that jointly captures features of object geometry, physical dynamics, and expert actions to represent the tasks, allowing us to efficiently learn the transfer success predictor. Extensive experiments demonstrate that SRSA significantly outperforms the leading baseline. When retrieving and fine-tuning skills on unseen tasks, SRSA achieves a 19% relative improvement in success rate, exhibits 2.6x lower standard deviation across random seeds, and requires 2.4x fewer transition samples to reach a satisfactory success rate, compared to the baseline. In a continual learning setup, SRSA efficiently learns policies for new tasks and incorporates them into the skill library, enhancing future policy learning. Furthermore, policies trained with SRSA in simulation achieve a 90% mean success rate when deployed in the real world. Please visit our project webpage https://srsa2024.github.io/. | Yijie Guo, Bingjie Tang, Iretiayo Akinola, Dieter Fox, Abhishek Gupta, Yashraj Narang |  |
| 582 |  |  [SVDQuant: Absorbing Outliers by Low-Rank Component for 4-Bit Diffusion Models](https://openreview.net/forum?id=vWR3KuiQur) |  | 0 | Diffusion models can effectively generate high-quality images. However, as they scale, rising memory demands and higher latency pose substantial deployment challenges. In this work, we aim to accelerate diffusion models by quantizing their weights and activations to 4 bits. At such an aggressive level, both weights and activations are highly sensitive, where existing post-training quantization methods like smoothing become insufficient. To overcome this limitation, we propose \*SVDQuant\*, a new 4-bit quantization paradigm. Different from smoothing, which redistributes outliers between weights and activations, our approach \*absorbs\* these outliers using a low-rank branch. We first consolidate the outliers by shifting them from activations to weights. Then, we use a high-precision, low-rank branch to take in the weight outliers with Singular Value Decomposition (SVD), while a low-bit quantized branch handles the residuals. This process eases the quantization on both sides. However, naively running the low-rank branch independently incurs significant overhead due to extra data movement of activations, negating the quantization speedup. To address this, we co-design an inference engine \*Nunchaku\* that fuses the kernels of the low-rank branch into those of the low-bit branch to cut off redundant memory access. It can also seamlessly support off-the-shelf low-rank adapters (LoRAs) without re-quantization. Extensive experiments on SDXL, PixArt-$\Sigma$, and FLUX.1 validate the effectiveness of SVDQuant in preserving image quality. We reduce the memory usage for the 12B FLUX.1 models by 3.5×, achieving 3.0× speedup over the 4-bit weight-only quantization (W4A16) baseline on the 16GB laptop 4090 GPU with INT4 precision. On the latest RTX 5090 desktop with Blackwell architecture, we achieve a 3.1× speedup compared to the W4A16 model using NVFP4 precision. Our quantization library and inference engine are available at https://github.com/mit-han-lab/deepcompressor/ and https://github.com/mit-han-lab/nunchaku/, correspondingly. | Muyang Li, Yujun Lin, Zhekai Zhang, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, JunYan Zhu, Song Han |  |
| 583 |  |  [DenseMatcher: Learning 3D Semantic Correspondence for Category-Level Manipulation from a Single Demo](https://openreview.net/forum?id=8oFvUBvF1u) |  | 0 | Dense 3D correspondence can enhance robotic manipulation by enabling the generalization of spatial, functional, and dynamic information from one object to an unseen counterpart. Compared to shape correspondence, semantic correspondence is more effective in generalizing across different object categories. To this end, we present DenseMatcher, a method capable of computing 3D correspondences between in-the-wild objects that share similar structures. DenseMatcher first computes vertex features by projecting multiview 2D features onto meshes and refining them with a 3D network, and subsequently finds dense correspondences with the obtained features using functional map. In addition, we craft the first 3D matching dataset that contains colored object meshes across diverse categories. We demonstrate the downstream effectiveness of DenseMatcher in (i) robotic manipulation, where it achieves cross-instance and cross-category generalization on long-horizon complex manipulation tasks from observing only one demo; (ii) zero-shot color mapping between digital assets, where appearance can be transferred between different objects with relatable geometry. More details and demonstrations can be found at https://tea-lab.github.io/DenseMatcher/. | Junzhe Zhu, Yuanchen Ju, Junyi Zhang, Muhan Wang, Zhecheng Yuan, Kaizhe Hu, Huazhe Xu |  |
| 584 |  |  [LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression](https://openreview.net/forum?id=PpYy0dR3Qw) |  | 0 | In $D$istributed optimization and $L$earning, and even more in the modern framework of federated learning, communication, which is slow and costly, is critical. We introduce LoCoDL, a communication-efficient algorithm that leverages the two popular and effective techniques of $Lo$cal training, which reduces the communication frequency, and $Co$mpression, in which short bitstreams are sent instead of full-dimensional vectors of floats. LoCoDL works with a large class of unbiased compressors that includes widely-used sparsification and quantization methods. LoCoDL provably benefits from local training and compression and enjoys a doubly-accelerated communication complexity, with respect to the condition number of the functions and the model dimension, in the general heterogeneous regime with strongly convex functions. This is confirmed in practice, with LoCoDL outperforming existing algorithms. | Laurent Condat, Arto Maranjyan, Peter Richtárik |  |
| 585 |  |  [LeFusion: Controllable Pathology Synthesis via Lesion-Focused Diffusion Models](https://openreview.net/forum?id=3b9SKkRAKw) |  | 0 | Patient data from real-world clinical practice often suffers from data scarcity and long-tail imbalances, leading to biased outcomes or algorithmic unfairness. This study addresses these challenges by generating lesion-containing image-segmentation pairs from lesion-free images. Previous efforts in medical imaging synthesis have struggled with separating lesion information from background, resulting in low-quality backgrounds and limited control over the synthetic output. Inspired by diffusion-based image inpainting, we propose LeFusion, a lesion-focused diffusion model. By redesigning the diffusion learning objectives to focus on lesion areas, we simplify the learning process and improve control over the output while preserving high-fidelity backgrounds by integrating forward-diffused background contexts into the reverse diffusion process. Additionally, we tackle two major challenges in lesion texture synthesis: 1) multi-peak and 2) multi-class lesions. We introduce two effective strategies: histogram-based texture control and multi-channel decomposition, enabling the controlled generation of high-quality lesions in difficult scenarios. Furthermore, we incorporate lesion mask diffusion, allowing control over lesion size, location, and boundary, thus increasing lesion diversity. Validated on 3D cardiac lesion MRI and lung nodule CT datasets, LeFusion-generated data significantly improves the performance of state-of-the-art segmentation models, including nnUNet and SwinUNETR. | Hantao Zhang, Yuhe Liu, Jiancheng Yang, Shouhong Wan, Xinyuan Wang, Wei Peng, Pascal Fua |  |
| 586 |  |  [Beyond Next Token Prediction: Patch-Level Training for Large Language Models](https://openreview.net/forum?id=dDpB23VbVa) |  | 0 | The prohibitive training costs of Large Language Models (LLMs) have emerged as a significant bottleneck in the development of next-generation LLMs. In this paper, we show that it is possible to significantly reduce the training costs of LLMs without sacrificing their performance. Specifically, we introduce patch-level training for LLMs, in which multiple tokens are aggregated into a unit of higher information density, referred to as a \`patch', to serve as the fundamental text unit for training LLMs. During patch-level training, we feed the language model shorter sequences of patches and train it to predict the next patch, thereby processing the majority of the training data at a significantly reduced cost. Following this, the model continues token-level training on the remaining training data to align with the inference mode. Experiments on a diverse range of models (370M-2.7B parameters) demonstrate that patch-level training can reduce the overall training costs to 0.5$\times$, without compromising the model performance compared to token-level training. Source code: \url{https://github.com/shaochenze/PatchTrain}. | Chenze Shao, Fandong Meng, Jie Zhou |  |
| 587 |  |  [LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models](https://openreview.net/forum?id=z8sxoCYgmd) |  | 0 | With the rapid development of AI-generated content, the future internet may be inundated with synthetic data, making the discrimination of authentic and credible multimodal data increasingly challenging. Synthetic data detection has thus garnered widespread attention, and the performance of large multimodal models (LMMs) in this task has attracted significant interest. LMMs can provide natural language explanations for their authenticity judgments, enhancing the explainability of synthetic content detection. Simultaneously, the task of distinguishing between real and synthetic data effectively tests the perception, knowledge, and reasoning capabilities of LMMs. In response, we introduce LOKI, a novel benchmark designed to evaluate the ability of LMMs to detect synthetic data across multiple modalities. LOKI encompasses video, image, 3D, text, and audio modalities, comprising 18K carefully curated questions across 26 subcategories with clear difficulty levels. The benchmark includes coarse-grained judgment and multiple-choice questions, as well as fine-grained anomaly selection and explanation tasks, allowing for a comprehensive analysis of LMMs. We evaluated 22 open-source LMMs and 6 closed-source models on LOKI, highlighting their potential as synthetic data detectors and also revealing some limitations in the development of LMM capabilities. More information about LOKI can be found at https://opendatalab.github.io/LOKI/. | Junyan Ye, Baichuan Zhou, Zilong Huang, Junan Zhang, Tianyi Bai, Hengrui Kang, Jun He, Honglin Lin, Zihao Wang, Tong Wu, Zhizheng Wu, Yiping Chen, Dahua Lin, Conghui He, Weijia Li |  |
| 588 |  |  [Recognize Any Surgical Object: Unleashing the Power of Weakly-Supervised Data](https://openreview.net/forum?id=iuxaCU3DI7) |  | 0 | We present RASO, a foundation model designed to Recognize Any Surgical Object, offering robust open-set recognition capabilities across a broad range of surgical procedures and object classes, in both surgical images and videos. RASO leverages a novel weakly-supervised learning framework that generates tag-image-text pairs automatically from large-scale unannotated surgical lecture videos, significantly reducing the need for manual annotations. Our scalable data generation pipeline gathers 2,200 surgical procedures and produces 3.6 million tag annotations across 2,066 unique surgical tags. Our experiments show that RASO achieves improvements of 2.9 mAP, 4.5 mAP, 10.6 mAP, and 7.2 mAP on four standard surgical benchmarks respectively in zero-shot settings, and surpasses state-of-the-art models in supervised surgical action recognition tasks. We will open-source our code, model, and dataset to facilitate further research. | Jiajie Li, Brian R. Quaranto, Chenhui Xu, Ishan Mishra, Ruiyang Qin, Dancheng Liu, Peter C. W. Kim, Jinjun Xiong |  |
| 589 |  |  [Both Ears Wide Open: Towards Language-Driven Spatial Audio Generation](https://openreview.net/forum?id=qPx3i9sMxv) |  | 0 | Recently, diffusion models have achieved great success in mono-channel audio generation. However, when it comes to stereo audio generation, the soundscapes often have a complex scene of multiple objects and directions. Controlling stereo audio with spatial contexts remains challenging due to high data costs and unstable generative models. To the best of our knowledge, this work represents the first attempt to address these issues. We first construct a large-scale, simulation-based, and GPT-assisted dataset, BEWO-1M, with abundant soundscapes and descriptions even including moving and multiple sources. Beyond text modality, we have also acquired a set of images and rationally paired stereo audios through retrieval to advance multimodal generation. Existing audio generation models tend to generate rather random and indistinct spatial audio. To provide accurate guidance for Latent Diffusion Models, we introduce the SpatialSonic model utilizing spatial-aware encoders and azimuth state matrices to reveal reasonable spatial guidance. By leveraging spatial guidance, our model not only achieves the objective of generating immersive and controllable spatial audio from text but also extends to other modalities as the pioneer attempt. Finally, under fair settings, we conduct subjective and objective evaluations on simulated and real-world data to compare our approach with prevailing methods. The results demonstrate the effectiveness of our method, highlighting its capability to generate spatial audio that adheres to physical rules. | Peiwen Sun, Sitong Cheng, Xiangtai Li, Zhen Ye, Huadai Liu, Honggang Zhang, Wei Xue, Yike Guo |  |
| 590 |  |  [Moner: Motion Correction in Undersampled Radial MRI with Unsupervised Neural Representation](https://openreview.net/forum?id=OdnqG1fYpo) |  | 0 | Motion correction (MoCo) in radial MRI is a particularly challenging problem due to the unpredictability of subject movement. Current state-of-the-art (SOTA) MoCo algorithms often rely on extensive high-quality MR images to pre-train neural networks, which constrains the solution space and leads to outstanding image reconstruction results. However, the need for large-scale datasets significantly increases costs and limits model generalization. In this work, we propose Moner, an unsupervised MoCo method that jointly reconstructs artifact-free MR images and estimates accurate motion from undersampled, rigid motion-corrupted k-space data, without requiring any training data. Our core idea is to leverage the continuous prior of implicit neural representation (INR) to constrain this ill-posed inverse problem, facilitating optimal solutions. Specifically, we integrate a quasi-static motion model into the INR, granting its ability to correct subject's motion. To stabilize model optimization, we reformulate radial MRI reconstruction as a back-projection problem using the Fourier-slice theorem. Additionally, we propose a novel coarse-to-fine hash encoding strategy, significantly enhancing MoCo accuracy. Experiments on multiple MRI datasets show our Moner achieves performance comparable to SOTA MoCo techniques on in-domain data, while demonstrating significant improvements on out-of-domain data. The code is available at: https://github.com/iwuqing/Moner | Qing Wu, Chenhe Du, Xuanyu Tian, Jingyi Yu, Yuyao Zhang, Hongjiang Wei |  |
| 591 |  |  [UniMatch: Universal Matching from Atom to Task for Few-Shot Drug Discovery](https://openreview.net/forum?id=v9EjwMM55Y) |  | 0 | Drug discovery is crucial for identifying candidate drugs for various diseases. However, its low success rate often results in a scarcity of annotations, posing a few-shot learning problem. Existing methods primarily focus on single-scale features, overlooking the hierarchical molecular structures that determine different molecular properties. To address these issues, we introduce Universal Matching Networks (UniMatch), a dual matching framework that integrates explicit hierarchical molecular matching with implicit task-level matching via meta- learning, bridging multi-level molecular representations and task-level generalization. Specifically, our approach explicitly captures structural features across multiple levels—atoms, substructures, and molecules—via hierarchical pooling and matching, facilitating precise molecular representation and comparison. Additionally, we employ a meta-learning strategy for implicit task-level matching, allowing the model to capture shared patterns across tasks and quickly adapt to new ones. This unified matching framework ensures effective molecular alignment while leveraging shared meta-knowledge for fast adaptation. Our experimental results demonstrate that UniMatch outperforms state-of-the-art methods on the MoleculeNet and FS-Mol benchmarks, achieving improvements of 2.87% in AUROC and 6.52% in ∆AUPRC. UniMatch also shows excellent generalization ability on the Meta-MolNet benchmark. | Ruifeng Li, Mingqian Li, Wei Liu, Yuhua Zhou, Xiangxin Zhou, Yuan Yao, Qiang Zhang, Hongyang Chen |  |
| 592 |  |  [OASIS Uncovers: High-Quality T2I Models, Same Old Stereotypes](https://openreview.net/forum?id=L6IgkJvcgV) |  | 0 | Images generated by text-to-image (T2I) models often exhibit visual biases and stereotypes of concepts such as culture and profession. Existing quantitative measures of stereotypes are based on statistical parity that does not align with the sociological definition of stereotypes and, therefore, incorrectly categorizes biases as stereotypes. Instead of oversimplifying stereotypes as biases, we propose a quantitative measure of stereotypes that aligns with its sociological definition. We then propose OASIS to measure the stereotypes in a generated dataset and understand their origins within the T2I model. OASIS includes two scores to measure stereotypes from a generated image dataset: \*\*(M1)\*\* Stereotype Score to measure the distributional violation of stereotypical attributes, and \*\*(M2)\*\* WALS to measure spectral variance in the images along a stereotypical attribute. OASIS also includes two methods to understand the origins of stereotypes in T2I models: \*\*(U1)\*\* StOP to discover attributes that the T2I model internally associates with a given concept, and \*\*(U2)\*\* SPI to quantify the emergence of stereotypical attributes in the latent space of the T2I model during image generation. Despite the considerable progress in image fidelity, using OASIS, we conclude that newer T2I models such as FLUX.1 and SDv3 contain strong stereotypical predispositions about concepts and still generate images with widespread stereotypical attributes. Additionally, the quantity of stereotypes worsens for nationalities with lower Internet footprints. | Sepehr Dehdashtian, Gautam Sreekumar, Vishnu Boddeti |  |
| 593 |  |  [Instance-dependent Early Stopping](https://openreview.net/forum?id=P42DbV2nuV) |  | 0 | In machine learning practice, early stopping has been widely used to regularize models and can save computational costs by halting the training process when the model's performance on a validation set stops improving. However, conventional early stopping applies the same stopping criterion to all instances without considering their individual learning statuses, which leads to redundant computations on instances that are already well-learned. To further improve the efficiency, we propose an Instance-dependent Early Stopping (IES) method that adapts the early stopping mechanism from the entire training set to the instance level, based on the core principle that once the model has mastered an instance, the training on it should stop. IES considers an instance as mastered if the second-order differences of its loss value remain within a small range around zero. This offers a more consistent measure of an instance's learning status compared with directly using the loss value, and thus allows for a unified threshold to determine when an instance can be excluded from further backpropagation. We show that excluding mastered instances from backpropagation can increase the gradient norms, thereby accelerating the decrease of the training loss and speeding up the training process. Extensive experiments on benchmarks demonstrate that IES method can reduce backpropagation instances by 10%-50% while maintaining or even slightly improving the test accuracy and transfer learning performance of a model. | Suqin Yuan, Runqi Lin, Lei Feng, Bo Han, Tongliang Liu |  |
| 594 |  |  [Beyond Random Masking: When Dropout meets Graph Convolutional Networks](https://openreview.net/forum?id=PwxYoMvmvy) |  | 0 | Graph Convolutional Networks (GCNs) have emerged as powerful tools for learning on graph-structured data, yet the behavior of dropout in these models remains poorly understood. This paper presents a comprehensive theoretical analysis of dropout in GCNs, revealing that its primary role differs fundamentally from standard neural networks - preventing oversmoothing rather than co-adaptation. We demonstrate that dropout in GCNs creates dimension-specific stochastic sub-graphs, leading to a form of structural regularization not present in standard neural networks. Our analysis shows that dropout effects are inherently degree-dependent, resulting in adaptive regularization that considers the topological importance of nodes. We provide new insights into dropout's role in mitigating oversmoothing and derive novel generalization bounds that account for graph-specific dropout effects. Furthermore, we analyze the synergistic interaction between dropout and batch normalization in GCNs, uncovering a mechanism that enhances overall regularization. Our theoretical findings are validated through extensive experiments on both node-level and graph-level tasks across 14 datasets. Notably, GCN with dropout and batch normalization outperforms state-of-the-art methods on several benchmarks, demonstrating the practical impact of our theoretical insights. | Yuankai Luo, XiaoMing Wu, Hao Zhu |  |
| 595 |  |  [Self-supervised contrastive learning performs non-linear system identification](https://openreview.net/forum?id=ONfWFluZBI) |  | 0 | Self-supervised learning (SSL) approaches have brought tremendous success across many tasks and domains. It has been argued that these successes can be attributed to a link between SSL and identifiable representation learning: Temporal structure and auxiliary variables ensure that latent representations are related to the true underlying generative factors of the data. Here, we deepen this connection and show that SSL can perform system identification in latent space. We propose dynamics contrastive learning, a framework to uncover linear, switching linear and non-linear dynamics under a non-linear observation model, give theoretical guarantees and validate them empirically. | Rodrigo González Laiz, Tobias Schmidt, Steffen Schneider |  |
| 596 |  |  [Sparse autoencoders reveal selective remapping of visual concepts during adaptation](https://openreview.net/forum?id=imT03YXlG2) |  | 0 | Adapting foundation models for specific purposes has become a standard approach to build machine learning systems for downstream applications. Yet, it is an open question which mechanisms take place during adaptation. Here we develop a new Sparse Autoencoder (SAE) for the CLIP vision transformer, named PatchSAE, to extract interpretable concepts at granular levels (e.g., shape, color, or semantics of an object) and their patch-wise spatial attributions. We explore how these concepts influence the model output in downstream image classification tasks and investigate how recent state-of-the-art prompt-based adaptation techniques change the association of model inputs to these concepts. While activations of concepts slightly change between adapted and non-adapted models, we find that the majority of gains on common adaptation tasks can be explained with the existing concepts already present in the non-adapted foundation model. This work provides a concrete framework to train and use SAEs for Vision Transformers and provides insights into explaining adaptation mechanisms. | Hyesu Lim, Jinho Choi, Jaegul Choo, Steffen Schneider |  |
| 597 |  |  [PIED: Physics-Informed Experimental Design for Inverse Problems](https://openreview.net/forum?id=w7P92BEsb2) |  | 0 | In many science and engineering settings, system dynamics are characterized by governing partial differential equations (PDEs), and a major challenge is to solve inverse problems (IPs) where unknown PDE parameters are inferred based on observational data gathered under limited budget. Due to the high costs of setting up and running experiments, experimental design (ED) is often done with the help of PDE simulations to optimize for the most informative design parameters (e.g., sensor placements) to solve such IPs, prior to actual data collection. This process of optimizing design parameters is especially critical when the budget and other practical constraints make it infeasible to adjust the design parameters between trials during the experiments. However, existing experimental design (ED) methods tend to require sequential and frequent design parameter adjustments between trials. Furthermore, they also have significant computational bottlenecks due to the need for complex numerical simulations for PDEs, and do not exploit the advantages provided by physics informed neural networks (PINNs) in solving IPs for PDE-governed systems, such as its meshless solutions, differentiability, and amortized training. This work presents Physics-Informed Experimental Design (PIED), the first ED framework that makes use of PINNs in a fully differentiable architecture to perform continuous optimization of design parameters for IPs for one-shot deployments. PIED overcomes existing methods' computational bottlenecks through parallelized computation and meta-learning of PINN parameter initialization, and proposes novel methods to effectively take into account PINN training dynamics in optimizing the ED parameters. Through experiments based on noisy simulated data and even real world experimental data, we empirically show that given limited observation budget, PIED significantly outperforms existing ED methods in solving IPs, including for challenging settings where the inverse parameters are unknown functions rather than just finite-dimensional. | Apivich Hemachandra, Gregory Kang Ruey Lau, SeeKiong Ng, Bryan Kian Hsiang Low |  |
| 598 |  |  [AgentRefine: Enhancing Agent Generalization through Refinement Tuning](https://openreview.net/forum?id=FDimWzmcWn) |  | 0 | Large Language Model (LLM) based agents have proved their ability to perform complex tasks like humans. However, there is still a large gap between open-sourced LLMs and commercial models like the GPT series. In this paper, we focus on improving the agent generalization capabilities of LLMs via instruction tuning. We first observe that the existing agent training corpus exhibits satisfactory results on held-in evaluation sets but fails to generalize to held-out sets. These agent-tuning works face severe formatting errors and are frequently stuck in the same mistake for a long while. We analyze that the poor generalization ability comes from overfitting to several manual agent environments and a lack of adaptation to new situations. They struggle with the wrong action steps and can not learn from the experience but just memorize existing observation-action relations. Inspired by the insight, we propose a novel AgentRefine framework for agent-tuning. The core idea is to enable the model to learn to correct its mistakes via observation in the trajectory. Specifically, we propose an agent synthesis framework to encompass a diverse array of environments and tasks and prompt a strong LLM to refine its error action according to the environment feedback. AgentRefine significantly outperforms state-of-the-art agent-tuning work in terms of generalization ability on diverse agent tasks. It also has better robustness facing perturbation and can generate diversified thought in inference. Our findings establish the correlation between agent generalization and self-refinement and provide a new paradigm for future research. | Dayuan Fu, Keqing He, Yejie Wang, Wentao Hong, Zhuoma Gongque, Weihao Zeng, Wei Wang, Jingang Wang, Xunliang Cai, Weiran Xu |  |
| 599 |  |  [TabM: Advancing tabular deep learning with parameter-efficient ensembling](https://openreview.net/forum?id=Sd4wYYOhmY) |  | 0 | Deep learning architectures for supervised learning on tabular data range from simple multilayer perceptrons (MLP) to sophisticated Transformers and retrieval-augmented methods. This study highlights a major, yet so far overlooked opportunity for substantially improving tabular MLPs; namely, parameter-efficient ensembling -- a paradigm for imitating an ensemble of models with just one model. We start by describing TabM -- a simple model based on MLP and BatchEnsemble (an existing technique), improved with our custom modifications. Then, we perform a large scale evaluation of tabular DL architectures on public benchmarks in terms of both task performance and efficiency, which renders the landscape of tabular DL in a new light. In particular, we find that TabM outperforms prior tabular DL models, while the complexity of attention- and retrieval-based methods does not pay off. Lastly, we conduct a detailed empirical analysis, that sheds some light on the high performance of TabM. For example, we show that parameter-efficient ensembling is not an arbitrary trick, but rather a highly effective way to reduce overfitting and improve optimization dynamics of tabular MLPs. Overall, our work brings an impactful technique to tabular DL, analyses its behaviour, and advances the performance-efficiency tradeoff with TabM -- a simple and powerful baseline for researchers and practitioners. | Yury Gorishniy, Akim Kotelnikov, Artem Babenko |  |
| 600 |  |  [Multi-Label Test-Time Adaptation with Bound Entropy Minimization](https://openreview.net/forum?id=75PhjtbBdr) |  | 0 | Mainstream test-time adaptation (TTA) techniques endeavor to mitigate distribution shifts via entropy minimization for multi-class classification, inherently increasing the probability of the most confident class. However, when encountering multi-label instances, the primary challenge stems from the varying number of labels per image, and prioritizing only the highest probability class inevitably undermines the adaptation of other positive labels. To address this issue, we investigate TTA within multi-label scenario (ML--TTA), developing Bound Entropy Minimization (BEM) objective to simultaneously increase the confidence of multiple top predicted labels. Specifically, to determine the number of labels for each augmented view, we retrieve a paired caption with yielded textual labels for that view. These labels are allocated to both the view and caption, called weak label set and strong label set with the same size k. Following this, the proposed BEM considers the highest top-k predicted labels from view and caption as a single entity, respectively, learning both view and caption prompts concurrently. By binding top-k predicted labels, BEM overcomes the limitation of vanilla entropy minimization, which exclusively optimizes the most confident class. Across the MSCOCO, VOC, and NUSWIDE multi-label datasets, our ML--TTA framework equipped with BEM exhibits superior performance compared to the latest SOTA methods, across various model architectures, prompt initialization, and varying label scenarios. The code is available at https://github.com/Jinx630/ML-TTA. | Xiangyu Wu, Feng Yu, Yang Yang, QingGuo Chen, Jianfeng Lu |  |
| 601 |  |  [ToolGen: Unified Tool Retrieval and Calling via Generation](https://openreview.net/forum?id=XLMAMmowdY) |  | 0 | As large language models (LLMs) advance, their inability to autonomously execute tasks by directly interacting with external tools remains a critical limitation. Traditional methods rely on inputting tool descriptions as context, which is constrained by context length and requires separate, often inefficient, retrieval mechanisms. We introduce ToolGen, a paradigm shift that integrates tool knowledge directly into the LLM’s parameters by representing each tool as a unique token. This enables the LLM to generate tool calls and arguments as part of its next token prediction capabilities, seamlessly blending tool invocation with language generation. Our framework allows the LLM to access and utilize a vast amount of tools with no additional retrieval step, significantly enhancing both performance and scalability. Experimental results with over 47,000 tools show that ToolGen not only achieves superior results in both tool retrieval and autonomous task completion but also sets the stage for a new era of AI agents that can adapt to tools across diverse domains. By fundamentally transforming tool retrieval into a generative process, ToolGen paves the way for more versatile, efficient, and autonomous AI systems. ToolGen enables end-to-end tool learning and opens opportunities for integration with other advanced techniques such as chain-of-thought and reinforcement learning, thereby expanding the practical capabilities of LLMs | Renxi Wang, Xudong Han, Lei Ji, Shu Wang, Timothy Baldwin, Haonan Li |  |
| 602 |  |  [Activation Gradient based Poisoned Sample Detection Against Backdoor Attacks](https://openreview.net/forum?id=VNMJfBBUd5) |  | 0 | This work studies the task of poisoned sample detection for defending against data poisoning based backdoor attacks. Its core challenge is finding a generalizable and discriminative metric to distinguish between clean and various types of poisoned samples (e.g., various triggers, various poisoning ratios). Inspired by a common phenomenon in backdoor attacks that the backdoored model tend to map significantly different poisoned and clean samples within the target class to similar activation areas, we introduce a novel perspective of the circular distribution of the gradients w.r.t. sample activation, dubbed gradient circular distribution (GCD). And, we find two interesting observations based on GCD. One is that the GCD of samples in the target class is much more dispersed than that in the clean class. The other is that in the GCD of target class, poisoned and clean samples are clearly separated. Inspired by above two observations, we develop an innovative three-stage poisoned sample detection approach, called Activation Gradient based Poisoned sample Detection (AGPD). First, we calculate GCDs of all classes from the model trained on the untrustworthy dataset. Then, we identify the target class(es) based on the difference on GCD dispersion between target and clean classes. Last, we filter out poisoned samples within the identified target class(es) based on the clear separation between poisoned and clean samples. Extensive experiments under various settings of backdoor attacks demonstrate the superior detection performance of the proposed method to existing poisoned detection approaches according to sample activation-based metrics. | Danni Yuan, Mingda Zhang, Shaokui Wei, Li Liu, Baoyuan Wu |  |
| 603 |  |  [Causally Motivated Sycophancy Mitigation for Large Language Models](https://openreview.net/forum?id=yRKelogz5i) |  | 0 | Incorporating user preferences into large language models (LLMs) can enhance the personalization and reliability of model outputs and facilitate the application of LLMs to real-world scenarios. However, leveraging user preferences can be a double-edged sword. Recent studies have found that improper utilization can incur sycophancy, where LLMs prioritize alignment with user preferences over the correctness of their outputs. To address sycophancy in LLMs, we analyze and model the problem through the lens of structured causal models (SCMs). We attribute sycophancy to LLMs' reliance on spurious correlations between user preferences and model outputs in this paper. Based on the proposed SCMs, we develop a novel framework, termed \*\*CAUSM\*\*, to mitigate sycophancy in LLMs by exploiting a significant causal signature. Specifically, we eliminate the spurious correlations embedded in the intermediate layers of LLMs through causally motivated head reweighting, and then calibrate the intra-head knowledge along the causal representation direction. Extensive experiments are conducted across diverse language tasks to demonstrate the superiority of our method over state-of-the-art competitors in mitigating sycophancy in LLMs. | Haoxi Li, Xueyang Tang, Jie Zhang, Song Guo, Sikai Bai, Peiran Dong, Yue Yu |  |
| 604 |  |  [Compositional simulation-based inference for time series](https://openreview.net/forum?id=uClUUJk05H) |  | 0 | Amortized simulation-based inference (SBI) methods train neural networks on simulated data to perform Bayesian inference. While this strategy avoids the need for tractable likelihoods, it often requires a large number of simulations and has been challenging to scale to time series data. Scientific simulators frequently emulate real-world dynamics through thousands of single-state transitions over time. We propose an SBI approach that can exploit such Markovian simulators by locally identifying parameters consistent with individual state transitions. We then compose these local results to obtain a posterior over parameters that align with the entire time series observation. We focus on applying this approach to neural posterior score estimation but also show how it can be applied, e.g., to neural likelihood (ratio) estimation. We demonstrate that our approach is more simulation-efficient than directly estimating the global posterior on several synthetic benchmark tasks and simulators used in ecology and epidemiology. Finally, we validate scalability and simulation efficiency of our approach by applying it to a high-dimensional Kolmogorov flow simulator with around one million data dimensions. | Manuel Glöckler, Shoji Toyota, Kenji Fukumizu, Jakob H. Macke |  |
| 605 |  |  [Bayesian Treatment of the Spectrum of the Empirical Kernel in (Sub)Linear-Width Neural Networks](https://openreview.net/forum?id=O6znYvxC1U) |  | 0 | We study Bayesian neural networks (BNNs) in the theoretical limits of infinitely increasing number of training examples, network width and input space dimension. Our findings establish new bridges between kernel-theoretic approaches and techniques derived from statistical mechanics through the correspondence between Mercer's eigenvalues and limiting spectral distributions of covariance matrices studied in random matrix theory. Our theoretical contributions first consist in novel integral formulas that accurately describe the predictors of BNNs in the asymptotic linear-width and sublinear-width regimes. Moreover, we extend the recently developed renormalisation theory of deep linear neural networks, enabling a rigorous explanation of the mounting empirical evidence that hints at the theory's applicability to nonlinear BNNs with ReLU activations in the linear-width regime. From a practical standpoint, our results introduce a novel technique for estimating the predictor statistics of a trained BNN that is applicable to the sublinear-width regime where the predictions of the renormalisation theory are inaccurate. | Ouns El Harzli, Bernardo Cuenca Grau |  |
| 606 |  |  [When GNNs meet symmetry in ILPs: an orbit-based feature augmentation approach](https://openreview.net/forum?id=wVTJRnZ11Z) |  | 0 | A common characteristic in integer linear programs (ILPs) is symmetry, allowing variables to be permuted without altering the underlying problem structure. Recently, GNNs have emerged as a promising approach for solving ILPs. However, a significant challenge arises when applying GNNs to ILPs with symmetry: classic GNN architectures struggle to differentiate between symmetric variables, which limits their predictive accuracy. In this work, we investigate the properties of permutation equivalence and invariance in GNNs, particularly in relation to the inherent symmetry of ILP formulations. We reveal that the interaction between these two factors contributes to the difficulty of distinguishing between symmetric variables. To address this challenge, we explore the potential of feature augmentation and propose several guiding principles for constructing augmented features. Building on these principles, we develop an orbit-based augmentation scheme that first groups symmetric variables and then samples augmented features for each group from a discrete uniform distribution. Empirical results demonstrate that our proposed approach significantly enhances both training efficiency and predictive performance. | Qian Chen, Lei Li, Qian Li, Jianghua Wu, Akang Wang, Ruoyu Sun, Xiaodong Luo, TsungHui Chang, Qingjiang Shi |  |
| 607 |  |  [Optimal Transport for Time Series Imputation](https://openreview.net/forum?id=xPTzjpIQNp) |  | 0 | Missing data imputation through distribution alignment has demonstrated advantages for non-temporal datasets but exhibits suboptimal performance in time-series applications. The primary obstacle is crafting a discrepancy measure that simultaneously (1) captures temporal patterns—accounting for periodicity and temporal dependencies inherent in time-series—and (2) accommodates non-stationarity, ensuring robustness amidst multiple coexisting temporal patterns. In response to these challenges, we introduce the Proximal Spectrum Wasserstein (PSW) discrepancy, a novel discrepancy tailored for comparing two \textit{sets} of time-series based on optimal transport. It incorporates a pairwise spectral distance to encapsulate temporal patterns, and a selective matching regularization to accommodate non-stationarity. Subsequently, we develop the PSW for Imputation (PSW-I) framework, which iteratively refines imputation results by minimizing the PSW discrepancy. Extensive experiments demonstrate that PSW-I effectively accommodates temporal patterns and non-stationarity, outperforming prevailing time-series imputation methods. Code is available at https://github.com/FMLYD/PSW-I. | Hao Wang, Zhengnan Li, Haoxuan Li, Xu Chen, Mingming Gong, BinChen, Zhichao Chen |  |
| 608 |  |  [Video Action Differencing](https://openreview.net/forum?id=3bcN6xlO6f) |  | 0 | How do two individuals differ when performing the same action? In this work, we introduce Video Action Differencing (VidDiff), the novel task of identifying subtle differences between videos of the same action, which has numerous applications, such as coaching and skill learning. To enable development on this new task, we first create VidDiffBench, a benchmark dataset containing 549 video pairs, with human annotations of 4,469 fine-grained action differences and 2,075 timestamps indicating where these differences occur. Our experiments demonstrate that VidDiffBench poses a significant challenge for state-of-the-art large multimodal models (LMMs), such as GPT-4o and Qwen2-VL. By analyzing the failure cases of LMMs on VidDiffBench, we highlight two key challenges for this task: localizing relevant sub-actions over two videos and fine-grained frame comparison. To overcome these, we propose the VidDiff method, an agentic workflow that breaks the task into three stages: action difference proposal, keyframe localization, and frame differencing, each stage utilizing specialized foundation models. To encourage future research in this new task, we release the benchmark and code. | James Burgess, Xiaohan Wang, Yuhui Zhang, Anita Rau, Alejandro Lozano, Lisa Dunlap, Trevor Darrell, Serena YeungLevy |  |
| 609 |  |  [GANDALF: Generative AttentioN based Data Augmentation and predictive modeLing Framework for personalized cancer treatment](https://openreview.net/forum?id=WwmtcGr4lP) |  | 0 | Effective treatment of cancer is a major challenge faced by healthcare providers, due to the highly individualized nature of patient responses to treatment. This is caused by the heterogeneity seen in cancer-causing alterations (mutations) across patient genomes. Limited availability of response data in patients makes it difficult to train personalized treatment recommendation models on mutations from clinical genomic sequencing reports. Prior methods tackle this by utilising larger, labelled pre-clinical laboratory datasets (‘cell lines’), via transfer learning. These methods augment patient data by learning a shared, domain-invariant representation, between the cell line and patient domains, which is then used to train a downstream drug response prediction (DRP) model. This approach augments data in the shared space but fails to model patient-specific characteristics, which have a strong influence on their drug response. We propose a novel generative attention-based data augmentation and predictive modeling framework, GANDALF, to tackle this crucial shortcoming of prior methods. GANDALF not only augments patient genomic data directly, but also accounts for its domain-specific characteristics. GANDALF outperforms state-of-the-art DRP models on publicly available patient datasets and emerges as the front-runner amongst SOTA cancer DRP models. | Aishwarya Jayagopal, Yanrong Zhang, Robert John Walsh, Tuan Zea Tan, Anand D. Jeyasekharan, Vaibhav Rajan |  |
| 610 |  |  [RaSA: Rank-Sharing Low-Rank Adaptation](https://openreview.net/forum?id=GdXI5zCoAt) |  | 0 | Low-rank adaptation (LoRA) has been prominently employed for parameter-efficient fine-tuning of large language models (LLMs). However, the limited expressive capacity of LoRA, stemming from the low-rank constraint, has been recognized as a bottleneck, particularly in rigorous tasks like code generation and mathematical reasoning. To address this limitation, we introduce Rank-Sharing Low-Rank Adaptation (RaSA), an innovative extension that enhances the expressive capacity of LoRA by leveraging partial rank sharing across layers. By forming a shared rank pool and applying layer-specific weighting, RaSA effectively increases the number of ranks without augmenting parameter overhead. Our theoretically grounded and empirically validated approach demonstrates that RaSA not only maintains the core advantages of LoRA but also significantly boosts performance in challenging code and math tasks. Code, data and scripts are available at: https://github.com/zwhe99/RaSA. | Zhiwei He, Zhaopeng Tu, Xing Wang, Xingyu Chen, Zhijie Wang, Jiahao Xu, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Rui Wang |  |
| 611 |  |  [Scaling Speech-Text Pre-training with Synthetic Interleaved Data](https://openreview.net/forum?id=3tukjsVyrE) |  | 0 | Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant compared to text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower sampling rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in both speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13\% (Moshi) to 31\%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain. | Aohan Zeng, Zhengxiao Du, Mingdao Liu, Lei Zhang, Shengmin Jiang, Yuxiao Dong, Jie Tang |  |
| 612 |  |  [Offline Model-Based Optimization by Learning to Rank](https://openreview.net/forum?id=sb1HgVDLjN) |  | 0 | Offline model-based optimization (MBO) aims to identify a design that maximizes a black-box function using only a fixed, pre-collected dataset of designs and their corresponding scores. This problem has garnered significant attention from both scientific and industrial domains. A common approach in offline MBO is to train a regression-based surrogate model by minimizing mean squared error (MSE) and then find the best design within this surrogate model by different optimizers (e.g., gradient ascent). However, a critical challenge is the risk of out-of-distribution errors, i.e., the surrogate model may typically overestimate the scores and mislead the optimizers into suboptimal regions. Prior works have attempted to address this issue in various ways, such as using regularization techniques and ensemble learning to enhance the robustness of the model, but it still remains. In this paper, we argue that regression models trained with MSE are not well-aligned with the primary goal of offline MBO, which is to \textit{select} promising designs rather than to predict their scores precisely. Notably, if a surrogate model can maintain the order of candidate designs based on their relative score relationships, it can produce the best designs even without precise predictions. To validate it, we conduct experiments to compare the relationship between the quality of the final designs and MSE, finding that the correlation is really very weak. In contrast, a metric that measures order-maintaining quality shows a significantly stronger correlation. Based on this observation, we propose learning a ranking-based model that leverages learning to rank techniques to prioritize promising designs based on their relative scores. We show that the generalization error on ranking loss can be well bounded. Empirical results across diverse tasks demonstrate the superior performance of our proposed ranking-based method than twenty existing methods. Our implementation is available at \url{https://github.com/lamda-bbo/Offline-RaM}. | RongXi Tan, Ke Xue, ShenHuan Lyu, Haopu Shang, Yao Wang, Yaoyuan Wang, Sheng Fu, Chao Qian |  |
| 613 |  |  [From Search to Sampling: Generative Models for Robust Algorithmic Recourse](https://openreview.net/forum?id=NtwFghsJne) |  | 0 | Algorithmic Recourse provides recommendations to individuals who are adversely impacted by automated model decisions, on how to alter their profiles to achieve a favorable outcome. Effective recourse methods must balance three conflicting goals: proximity to the original profile to minimize cost, plausibility for realistic recourse, and validity to ensure the desired outcome. We show that existing methods train for these objectives separately and then search for recourse through a joint optimization over the recourse goals during inference, leading to poor recourse recommendations. We introduce GenRe, a generative recourse model designed to train the three recourse objectives jointly. Training such generative models is non-trivial due to lack of direct recourse supervision. We propose efficient ways to synthesize such supervision and further show that GenRe's training leads to a consistent estimator. Unlike most prior methods, that employ non-robust gradient descent based search during inference, GenRe simply performs a forward sampling over the generative model to produce minimum cost recourse, leading to superior performance across multiple metrics. We also demonstrate GenRe provides the best trade-off between cost, plausibility and validity, compared to state-of-art baselines. Our code is available at: https://github.com/prateekgargX/genre | Prateek Garg, Lokesh Nagalapatti, Sunita Sarawagi |  |
| 614 |  |  [Neural Wave Equation for Irregularly Sampled Sequence Data](https://openreview.net/forum?id=kbeX97jExm) |  | 0 | Sequence labeling problems arise in several real-world applications such as healthcare and robotics. In many such applications, sequence data are irregularly sampled and are of varying complexities. Recently, efforts have been made to develop neural ODE-based architectures to model the evolution of hidden states continuously in time, to address irregularly sampled sequence data. However, they assume a fixed architectural depth and limit their flexibility to adapt to data sets with varying complexities. We propose the neural wave equation, a novel deep learning method inspired by the wave equation, to address this through continuous modeling of depth. Neural Wave Equation models the evolution of hidden states continuously across time as well as depth by using a non-homogeneous wave equation parameterized by a neural network. Through d'Alembert's analytical solution of the wave equation, we also show that the neural wave equation provides denser connections across the hidden states, allowing for better modeling capability. We conduct experiments on several sequence labeling problems involving irregularly sampled sequence data and demonstrate the superior performance of the proposed neural wave equation model. | Arkaprava Majumdar, M. Anand Krishna, P. K. Srijith |  |
| 615 |  |  [ComaDICE: Offline Cooperative Multi-Agent Reinforcement Learning with Stationary Distribution Shift Regularization](https://openreview.net/forum?id=5o9JJJPPm6) |  | 0 | Offline reinforcement learning (RL) has garnered significant attention for its ability to learn effective policies from pre-collected datasets without the need for further environmental interactions. While promising results have been demonstrated in single-agent settings, offline multi-agent reinforcement learning (MARL) presents additional challenges due to the large joint state-action space and the complexity of multi-agent behaviors. A key issue in offline RL is the distributional shift, which arises when the target policy being optimized deviates from the behavior policy that generated the data. This problem is exacerbated in MARL due to the interdependence between agents' local policies and the expansive joint state-action space. Prior approaches have primarily addressed this challenge by incorporating regularization in the space of either Q-functions or policies. In this work, we propose a novel type of regularizer in the space of stationary distributions to address the distributional shift more effectively. Our algorithm, ComaDICE, provides a principled framework for offline cooperative MARL to correct the stationary distribution of the global policy, which is then leveraged to derive local policies for individual agents. Through extensive experiments on the offline multi-agent MuJoCo and StarCraft II benchmarks, we demonstrate that ComaDICE achieves superior performance compared to state-of-the-art offline MARL methods across nearly all tasks. | The Viet Bui, Thanh Hong Nguyen, Tien Mai |  |
| 616 |  |  [Probabilistic Conformal Prediction with Approximate Conditional Validity](https://openreview.net/forum?id=Nfd7z9d6Bb) |  | 0 | We develop a new method for generating prediction sets that combines the flexibility of conformal methods with an estimate of the conditional distribution $\textup{P}_{Y \mid X}$. Existing methods, such as conformalized quantile regression and probabilistic conformal prediction, usually provide only a marginal coverage guarantee. In contrast, our approach extends these frameworks to achieve approximately conditional coverage, which is crucial for many practical applications. Our prediction sets adapt to the behavior of the predictive distribution, making them effective even under high heteroscedasticity. While exact conditional guarantees are infeasible without assumptions on the underlying data distribution, we derive non-asymptotic bounds that depend on the total variation distance of the conditional distribution and its estimate. Using extensive simulations, we show that our method consistently outperforms existing approaches in terms of conditional coverage, leading to more reliable statistical inference in a variety of applications. | Vincent Plassier, Alexander Fishkov, Mohsen Guizani, Maxim Panov, Eric Moulines |  |
| 617 |  |  [Rethinking Neural Multi-Objective Combinatorial Optimization via Neat Weight Embedding](https://openreview.net/forum?id=GM7cmQfk2F) |  | 0 | Recent decomposition-based neural multi-objective combinatorial optimization (MOCO) methods struggle to achieve desirable performance. Even equipped with complex learning techniques, they often suffer from significant optimality gaps in weight-specific subproblems. To address this challenge, we propose a neat weight embedding method to learn weight-specific representations, which captures weight-instance interaction for the subproblems and was overlooked by most current methods. We demonstrate the potentials of our method in two instantiations. First, we introduce a succinct addition model to learn weight-specific node embeddings, which surpassed most existing neural methods. Second, we design an enhanced conditional attention model to simultaneously learn the weight embedding and node embeddings, which yielded new state-of-the-art performance. Experimental results on classic MOCO problems verified the superiority of our method. Remarkably, our method also exhibits favorable generalization performance across problem sizes, even outperforming the neural method specialized for boosting size generalization. | Jinbiao Chen, Zhiguang Cao, Jiahai Wang, Yaoxin Wu, Hanzhang Qin, Zizhen Zhang, YueJiao Gong |  |
| 618 |  |  [Robust Root Cause Diagnosis using In-Distribution Interventions](https://openreview.net/forum?id=l11DZY5Nxu) |  | 0 | Diagnosing the root cause of an anomaly in a complex interconnected system is a pressing problem in today’s cloud services and industrial operations. We propose In-Distribution Interventions (IDI), a novel algorithm that predicts root cause as nodes that meet two criteria: 1) Anomaly: root cause nodes should take on anomalous values; 2) Fix: had the root cause nodes assumed usual values, the target node would not have been anomalous. Prior methods of assessing the fix condition rely on counterfactuals inferred from a Structural Causal Model (SCM) trained on historical data. But since anomalies are rare and fall outside the training distribution, the fitted SCMs yield unreliable counterfactual estimates. IDI overcomes this by relying on interventional estimates obtained by solely probing the fitted SCM at in-distribution inputs. We present a theoretical analysis comparing and bounding the errors in assessing the fix condition using interventional and counterfactual estimates. We then conduct experiments by systematically varying the SCM’s complexity to demonstrate the cases where IDI’s interventional approach outperforms the counterfactual approach and vice versa. Experiments on both synthetic and PetShop RCD benchmark datasets demonstrate that IDI consistently identifies true root causes more accurately and robustly than nine existing state-of-the-art RCD baselines. Code will be released at https://github.com/nlokeshiisc/IDI_release. | Lokesh Nagalapatti, Ashutosh Srivastava, Sunita Sarawagi, Amit Sharma |  |
| 619 |  |  [Boosting Neural Combinatorial Optimization for Large-Scale Vehicle Routing Problems](https://openreview.net/forum?id=TbTJJNjumY) |  | 0 | Neural Combinatorial Optimization (NCO) methods have exhibited promising performance in solving Vehicle Routing Problems (VRPs). However, most NCO methods rely on the conventional self-attention mechanism that induces excessive computational complexity, thereby struggling to contend with large-scale VRPs and hindering their practical applicability. In this paper, we propose a lightweight cross-attention mechanism with linear complexity, by which a Transformer network is developed to learn efficient and favorable solutions for large-scale VRPs. We also propose a Self-Improved Training (SIT) algorithm that enables direct model training on large-scale VRP instances, bypassing extensive computational overhead for attaining labels. By iterating solution reconstruction, the Transformer network itself can generate improved partial solutions as pseudo-labels to guide the model training. Experimental results on the Travelling Salesman Problem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) with up to 100K nodes indicate that our method consistently achieves superior performance for synthetic and real-world benchmarks, significantly boosting the scalability of NCO methods. | Fu Luo, Xi Lin, Yaoxin Wu, Zhenkun Wang, Xialiang Tong, Mingxuan Yuan, Qingfu Zhang |  |
| 620 |  |  [Sensitivity Verification for Additive Decision Tree Ensembles](https://openreview.net/forum?id=h0vC0fm1q7) |  | 0 | Tree ensemble models, such as Gradient Boosted Decision Trees (GBDTs) and random forests, are widely popular models for a variety of machine learning tasks. The power of these models comes from the ensemble of decision trees, which makes analysis of such models significantly harder than for single trees. As a result, recent work has focused on developing exact and approximate techniques for questions such as robustness verification, fairness and explainability for such models of tree ensembles. In this paper, we focus on a specific problem of feature sensitivity for additive decision tree ensembles and build a formal verification framework for a parametrized variant of it, where we also take into account the confidence of the tree ensemble in its output. We start by showing theoretical (NP-)hardness of the problem and explain how it relates to other verification problems. Next, we provide a novel encoding of the problem using pseudo-Boolean constraints. Based on this encoding, we develop a tunable algorithm to perform sensitivity analysis, which can trade off precision for running time. We implement our algorithm and study its performance on a suite of GBDT benchmarks from the literature. Our experiments show the practical utility of our approach and its improved performance compared to existing approaches. | Arhaan Ahmad, Tanay Vineet Tayal, Ashutosh Gupta, S. Akshay |  |
| 621 |  |  [Monte Carlo Planning with Large Language Model for Text-Based Game Agents](https://openreview.net/forum?id=r1KcapkzCt) |  | 0 | Text-based games provide valuable environments for language-based autonomous agents. However, planning-then-learning paradigms, such as those combining Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably time-consuming due to extensive iterations. Additionally, these algorithms perform uncertainty-driven exploration but lack language understanding and reasoning abilities. In this paper, we introduce the Monte Carlo planning with Dynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages the language understanding and reasoning capabilities of Large Language Models (LLMs) alongside the exploratory advantages of tree search algorithms. Specifically, we enhance LLMs with in-trial and cross-trial memory mechanisms, enabling them to learn from past experiences and dynamically adjust action evaluations during planning. We conduct experiments on a series of text-based games from the Jericho benchmark. Our results demonstrate that the MC-DML algorithm significantly enhances performance across various games at the initial planning phase, outperforming strong contemporary methods that require multiple iterations. This demonstrates the effectiveness of our algorithm, paving the way for more efficient language-grounded planning in complex environments. | Zijing Shi, Meng Fang, Ling Chen |  |
| 622 |  |  [Actions Speak Louder Than Words: Rate-Reward Trade-off in Markov Decision Processes](https://openreview.net/forum?id=Za3M6OZuCU) |  | 0 | The impact of communication on decision-making systems has been extensively studied under the assumption of dedicated communication channels. We instead consider communicating through actions, where the message is embedded into the actions of an agent which interacts with the environment in a Markov decision process (MDP) framework. We conceptualize the MDP environment as a finite-state channel (FSC), where the actions of the agent serve as the channel input, while the states of the MDP observed by another agent (i.e., receiver) serve as the channel output. Here, we treat the environment as a communication channel over which the agent communicates through its actions, while at the same time, trying to maximize its reward. We first characterize the optimal information theoretic trade-off between the average reward and the rate of reliable communication in the infinite-horizon regime. Then, we propose a novel framework to design a joint control/coding policy, termed Act2Comm, which seamlessly embeds messages into actions. From a communication perspective, Act2Comm functions as a learning-based channel coding scheme for non-differentiable FSCs under input-output constraints. From a control standpoint, Act2Comm learns an MDP policy that incorporates communication capabilities, though at the cost of some control performance. Overall, Act2Comm effectively balances the dual objectives of control and communication in this environment. Experimental results validate Act2Comm's capability to enable reliable communication while maintaining a certain level of control performance. | Haotian Wu, Gongpu Chen, Deniz Gündüz |  |
| 623 |  |  [A Statistical Framework for Ranking LLM-based Chatbots](https://openreview.net/forum?id=rAoEub6Nw2) |  | 0 | Large language models (LLMs) have transformed natural language processing, with frameworks like Chatbot Arena providing pioneering platforms for evaluating these models. By facilitating millions of pairwise comparisons based on human judgments, Chatbot Arena has become a cornerstone in LLM evaluation, offering rich datasets for ranking models in open-ended conversational tasks. Building upon this foundation, we propose a statistical framework that incorporates key advancements to address specific challenges in pairwise comparison analysis. First, we introduce a factored tie model that enhances the ability to handle ties—an integral aspect of human-judged comparisons—significantly improving the model's fit to observed data. Second, we extend the framework to model covariance between competitors, enabling deeper insights into performance relationships and facilitating intuitive groupings into performance tiers. Third, we resolve optimization challenges arising from parameter non-uniqueness by introducing novel constraints, ensuring stable and interpretable parameter estimation. Through rigorous evaluation and extensive experimentation, our framework demonstrates substantial improvements over existing methods in modeling pairwise comparison data. To support reproducibility and practical adoption, we release leaderbot, an open-source Python package implementing our models and analyses. | Siavash Ameli, Siyuan Zhuang, Ion Stoica, Michael W. Mahoney |  |
| 624 |  |  [Online epsilon Net & Piercing Set for Geometric Concepts](https://openreview.net/forum?id=nNiWRRj6r9) |  | 0 | VC-dimension (Vapnik & Chervonenkis (1971)) and $\varepsilon$-nets (Haussler & Welzl (1987)) are key concepts in Statistical Learning Theory. Intuitively, VC-dimension is a measure of the size of a class of sets. The famous $\varepsilon$-net theorem, a fundamental result in Discrete Geometry, asserts that if the VC-dimension of a set system is bounded, then a small sample exists that intersects all sufficiently large sets. In online learning scenarios where data arrives sequentially, the VC-dimension helps to bound the complexity of the set system, and $\varepsilon$-nets ensure the selection of a small representative set. This sampling framework is crucial in various domains, including spatial data analysis, motion planning in dynamic environments, optimization of sensor networks, and feature extraction in computer vision, among others. Motivated by these applications, we study the online $\varepsilon$-net problem for geometric concepts with bounded VC-dimension. While the offline version of this problem has been extensively studied, surprisingly, there are no known theoretical results for the online version to date. We present the first deterministic online algorithm with an optimal competitive ratio for intervals in $\mathbb{R}$. Next, we give a randomized online algorithm with a near-optimal competitive ratio for axis-aligned boxes in $\mathbb{R}^d$, for $d\le 3$. Furthermore, we introduce a novel technique to analyze similar-sized objects of constant description complexity in $\mathbb{R}^d$, which may be of independent interest. Next, we focus on the continuous version of this problem (called online piercing set), where ranges of the set system are geometric concepts in $\mathbb{R}^d$ arriving in an online manner, but the universe is the entire ambient space, and the objective is to choose a small sample that intersects all the ranges. Although online piercing set is a very well-studied problem in the literature, to our surprise, very few works have addressed generic geometric concepts without any assumption about the sizes. We advance this field by proposing asymptotically optimal competitive deterministic algorithms for boxes and ellipsoids in $\mathbb{R}^d$, for any $d\in\mathbb{N}$. | Sujoy Bhore, Devdan Dey, Satyam Singh |  |
| 625 |  |  [SimulPL: Aligning Human Preferences in Simultaneous Machine Translation](https://openreview.net/forum?id=XBF63bHDZw) |  | 0 | Simultaneous Machine Translation (SiMT) generates translations while receiving streaming source inputs. This requires the SiMT model to learn a read/write policy, deciding when to translate and when to wait for more source input. Numerous linguistic studies indicate that audiences in SiMT scenarios have distinct preferences, such as accurate translations, simpler syntax, and no unnecessary latency. Aligning SiMT models with these human preferences is crucial to improve their performances. However, this issue still remains unexplored. Additionally, preference optimization for SiMT task is also challenging. Existing methods focus solely on optimizing the generated responses, ignoring human preferences related to latency and the optimization of read/write policy during the preference optimization phase. To address these challenges, we propose Simultaneous Preference Learning (SimulPL), a preference learning framework tailored for the SiMT task. In the SimulPL framework, we categorize SiMT human preferences into five aspects: \*\*translation quality preference\*\*, \*\*monotonicity preference\*\*, \*\*key point preference\*\*, \*\*simplicity preference\*\*, and \*\*latency preference\*\*. By leveraging the first four preferences, we construct human preference prompts to efficiently guide GPT-4/4o in generating preference data for the SiMT task. In the preference optimization phase, SimulPL integrates \*\*latency preference\*\* into the optimization objective and enables SiMT models to improve the read/write policy, thereby aligning with human preferences more effectively. Experimental results indicate that SimulPL exhibits better alignment with human preferences across all latency levels in Zh$\rightarrow$En, De$\rightarrow$En and En$\rightarrow$Zh SiMT tasks. Our data and code will be available at https://github.com/EurekaForNLP/SimulPL. | Donglei Yu, Yang Zhao, Jie Zhu, Yangyifan Xu, Yu Zhou, Chengqing Zong |  |
| 626 |  |  [Neural Interactive Proofs](https://openreview.net/forum?id=R2834dhBlo) |  | 0 | We consider the problem of how a trusted, but computationally bounded agent (a 'verifier') can learn to interact with one or more powerful but untrusted agents ('provers') in order to solve a given task. More specifically, we study the case in which agents are represented using neural networks and refer to solutions of this problem as neural interactive proofs. First we introduce a unifying framework based on prover-verifier games (Anil et al., 2021), which generalises previously proposed interaction protocols. We then describe several new protocols for generating neural interactive proofs, and provide a theoretical comparison of both new and existing approaches. Finally, we support this theory with experiments in two domains: a toy graph isomorphism problem that illustrates the key ideas, and a code validation task using large language models. In so doing, we aim to create a foundation for future work on neural interactive proofs and their application in building safer AI systems. | Lewis Hammond, Sam AdamDay |  |
| 627 |  |  [Oracle efficient truncated statistics](https://openreview.net/forum?id=ZS7UEI3vG5) |  | 0 | We study the problem of learning from truncated samples: instead of observing samples from some underlying population $p^\ast$, we observe only the examples that fall in some survival set $S \subset \mathbb{R}^d$ whose probability mass (measured with respect to $p^\ast$) is at least $\alpha$. Assuming membership oracle access to the truncation set $S$, prior works obtained algorithms for the case where $p^\ast$ is Gaussian or more generally an exponential family with strongly convex likelihood --- albeit with a super-polynomial dependency on the (inverse) survival mass $1/\alpha$ both in terms of runtime and in number of oracle calls to the set $S$. In this work we design a new learning method with runtime and query complexity polynomial in $1/\alpha$. Our result significantly improves over the prior works by focusing on efficiently solving the underlying optimization problem using a general purpose optimization algorithm with minimal assumptions. | Konstantinos Karatapanis, Vasilis Kontonis, Christos Tzamos |  |
| 628 |  |  [Drop-Upcycling: Training Sparse Mixture of Experts with Partial Re-initialization](https://openreview.net/forum?id=gx1wHnf5Vp) |  | 0 | The Mixture of Experts (MoE) architecture reduces the training and inference cost significantly compared to a dense model of equivalent capacity. Upcycling is an approach that initializes and trains an MoE model using a pre-trained dense model. While upcycling leads to initial performance gains, the training progresses slower than when trained from scratch, leading to suboptimal performance in the long term. We propose Drop-Upcycling - a method that effectively addresses this problem. Drop-Upcycling combines two seemingly contradictory approaches: utilizing the knowledge of pre-trained dense models while statistically re-initializing some parts of the weights. This approach strategically promotes expert specialization, significantly enhancing the MoE model's efficiency in knowledge acquisition. Extensive large-scale experiments demonstrate that Drop-Upcycling significantly outperforms previous MoE construction methods in the long term, specifically when training on hundreds of billions of tokens or more. As a result, our MoE model with 5.9B active parameters achieves comparable performance to a 13B dense model in the same model family, while requiring approximately 1/4 of the training FLOPs. All experimental resources, including source code, training data, model checkpoints and logs, are publicly available to promote reproducibility and future research on MoE. | Taishi Nakamura, Takuya Akiba, Kazuki Fujii, Yusuke Oda, Rio Yokota, Jun Suzuki |  |
| 629 |  |  [Black-Box Detection of Language Model Watermarks](https://openreview.net/forum?id=E4LAVLXAHW) |  | 0 | Watermarking has emerged as a promising way to detect LLM-generated text, by augmenting LLM generations with later detectable signals. Recent work has proposed multiple families of watermarking schemes, several of which focus on preserving the LLM distribution. This distribution-preservation property is motivated by the fact that it is a tractable proxy for retaining LLM capabilities, as well as the inherently implied undetectability of the watermark by downstream users. Yet, despite much discourse around undetectability, no prior work has investigated the practical detectability of any of the current watermarking schemes in a realistic black-box setting. In this work we tackle this for the first time, developing rigorous statistical tests to detect the presence, and estimate parameters, of all three popular watermarking scheme families, using only a limited number of black-box queries. We experimentally confirm the effectiveness of our methods on a range of schemes and a diverse set of open-source models. Further, we validate the feasibility of our tests on real-world APIs. Our findings indicate that current watermarking schemes are more detectable than previously believed. | Thibaud Gloaguen, Nikola Jovanovic, Robin Staab, Martin T. Vechev |  |
| 630 |  |  [ProAdvPrompter: A Two-Stage Journey to Effective Adversarial Prompting for LLMs](https://openreview.net/forum?id=tpHqsyZ3YX) |  | 0 | As large language models (LLMs) are increasingly being integrated into various real-world applications, the identification of their vulnerabilities to jailbreaking attacks becomes an essential component of ensuring the safety and reliability of LLMs. Previous studies have developed LLM assistants, known as the adversarial prompter, to automatically generate suffixes that manipulate target LLMs into generating harmful and undesirable outputs. However, these approaches often suffer from low performance or generate semantically meaningless prompts, which can be easily identified by perplexity-based defenses. In this paper, we introduce a novel two-stage method, $\texttt{ProAdvPrompter}$, that significantly improves the performance of adversarial prompters. In $\texttt{ProAdvPrompter}$, the first stage (Exploration) utilizes the loss information to guide the adversarial prompter in generating suffixes that are more likely to elicit harmful responses. Then the second stage (Exploitation) iteratively fine-tunes the prompter using high-quality generated adversarial suffixes to further boost performance. Additionally, we incorporate the prompt template to aid in the Exploration stage and propose a filtering mechanism to accelerate the training process in the Exploitation stage. We evaluate $\texttt{ProAdvPrompter}$ against the well-aligned LLMs (i.e., Llama2-Chat-7B and Llama3-chat-8B), achieving attack success rates of 99.68% and 97.12% respectively after 10 trials on the AdvBench dataset, thereby enhancing performance by $\sim 2$ times compared to previous works. Moreover, $\texttt{ProAdvPrompter}$ reduces training time by 20% on Llama3-Instruct-8B, generates more generalized adversarial suffixes, and demonstrates resilience against the perplexity defense. An ablation study further evaluates the effects of key components in $\texttt{ProAdvPrompter}$ (the prompt template and the filtering mechanism). | Hao Di, Tong He, Haishan Ye, Yinghui Huang, Xiangyu Chang, Guang Dai, Ivor W. Tsang |  |
| 631 |  |  [Ward: Provable RAG Dataset Inference via LLM Watermarks](https://openreview.net/forum?id=kVrwHLAb20) |  | 0 | RAG enables LLMs to easily incorporate external data, raising concerns for data owners regarding unauthorized usage of their content. The challenge of detecting such unauthorized usage remains underexplored, with datasets and methods from adjacent fields being ill-suited for its study. We take several steps to bridge this gap. First, we formalize this problem as (black-box) RAG Dataset Inference (RAG-DI). We then introduce a novel dataset designed for realistic benchmarking of RAG-DI methods, alongside a set of baselines. Finally, we propose Ward, a method for RAG-DI based on LLM watermarks that equips data owners with rigorous statistical guarantees regarding their dataset's misuse in RAG corpora. Ward consistently outperforms all baselines, achieving higher accuracy, superior query efficiency and robustness. Our work provides a foundation for future studies of RAG-DI and highlights LLM watermarks as a promising approach to this problem. | Nikola Jovanovic, Robin Staab, Maximilian Baader, Martin T. Vechev |  |
| 632 |  |  [SCOPE: A Self-supervised Framework for Improving Faithfulness in Conditional Text Generation](https://openreview.net/forum?id=dTkqaCKLPp) |  | 0 | Large Language Models (LLMs), when used for conditional text generation, often produce hallucinations, i.e., information that is unfaithful or not grounded in the input context. This issue arises in typical conditional text generation tasks, such as text summarization and data-to-text generation, where the goal is to produce fluent text based on contextual input. When fine-tuned on specific domains, LLMs struggle to provide faithful answers to a given context, often adding information or generating errors. One underlying cause of this issue is that LLMs rely on statistical patterns learned from their training data. This reliance can interfere with the model's ability to stay faithful to a provided context, leading to the generation of ungrounded information. We build upon this observation and introduce a novel self-supervised method for generating a training set of unfaithful samples. We then refine the model using a training process that encourages the generation of grounded outputs over unfaithful ones, drawing on preference-based training. Our approach leads to significantly more grounded text generation, outperforming existing self-supervised techniques in faithfulness, as evaluated through automatic metrics, LLM-based assessments, and human evaluations. | Song Duong, Florian Le Bronnec, Alexandre Allauzen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari |  |
| 633 |  |  [Clique Number Estimation via Differentiable Functions of Adjacency Matrix Permutations](https://openreview.net/forum?id=DFSb67ksVr) |  | 0 | Estimating the clique number in a graph is central to various applications, e.g., community detection, graph retrieval, etc. Existing estimators often rely on non-differentiable combinatorial components. Here, we propose a full differentiable estimator for clique number estimation, which can be trained from distant supervision of clique numbers, rather than demonstrating actual cliques. Our key insight is a formulation of the maximum clique problem (MCP) as a maximization of the size of fully dense square submatrix, within a suitably row-column-permuted adjacency matrix. We design a differentiable mechanism to search for permutations that lead to the discovery of such dense blocks. However, the optimal permutation is not unique, which leads to the learning of spurious permutations. To tackle this problem, we view the MCP problem as a sequence of subgraph matching tasks, each detecting progressively larger cliques in a nested manner. This allows effective navigation through suitable node permutations. These steps result in MxNet, an end-to-end differentiable model, which learns to predict clique number without explicit clique demonstrations, with the added benefit of interpretability. Experiments on eight datasets show the superior accuracy of our approach. | Indradyumna Roy, Eeshaan Jain, Soumen Chakrabarti, Abir De |  |
| 634 |  |  [Reliable and Diverse Evaluation of LLM Medical Knowledge Mastery](https://openreview.net/forum?id=TXfzH933qV) |  | 0 | Mastering medical knowledge is crucial for medical-specific LLMs. However, despite the existence of medical benchmarks like MedQA, a unified framework that fully leverages existing knowledge bases to evaluate LLMs' mastery of medical knowledge is still lacking. We propose PretexEval, a novel framework that dynamically generates reliable and diverse test samples to evaluate LLMs for any given medical knowledge base. We notice that test samples produced directly from knowledge bases by templates or LLMs may introduce factual errors and also lack diversity. To address these issues, our framework employs predicate equivalence transformations to produce a series of variants for any given medical knowledge point. Finally, these produced predicate variants are converted into textual language, resulting in a series of reliable and diverse test samples. Here, we use our proposed framework to systematically investigate the mastery of medical factual knowledge of 12 well-known LLMs, based on two knowledge bases that are crucial for clinical diagnosis and treatment. The evaluation results illustrate that current LLMs still exhibit significant deficiencies in fully mastering medical knowledge, despite achieving considerable success on some famous public benchmarks. These new findings provide valuable insights for developing medical-specific LLMs, highlighting that current LLMs urgently need to strengthen their comprehensive and in-depth mastery of medical knowledge before being applied to real-world medical scenarios. | Yuxuan Zhou, Xien Liu, Chen Ning, Xiao Zhang, Ji Wu |  |
| 635 |  |  [SWE-Search: Enhancing Software Agents with Monte Carlo Tree Search and Iterative Refinement](https://openreview.net/forum?id=G7sIFXugTX) |  | 0 | Software engineers operating in complex and dynamic environments must continuously adapt to evolving requirements, learn iteratively from experience, and reconsider their approaches based on new insights. However, current large language model (LLM)-based software agents often follow linear, sequential processes that prevent backtracking and exploration of alternative solutions, limiting their ability to rethink their strategies when initial approaches prove ineffective. To address these challenges, we propose SWE-Search, a multi-agent framework that integrates Monte Carlo Tree Search (MCTS) with a self-improvement mechanism to enhance software agents' performance on repository-level software tasks. SWE-Search extends traditional MCTS by incorporating a hybrid value function that leverages LLMs for both numerical value estimation and qualitative evaluation. This enables self-feedback loops where agents iteratively refine their strategies based on both quantitative numerical evaluations and qualitative natural language assessments of pursued trajectories. The framework includes a SWE-Agent for adaptive exploration, a Value Agent for iterative feedback, and a Discriminator Agent that facilitates multi-agent debate for collaborative decision-making. Applied to the SWE-bench benchmark, our approach demonstrates a 23% relative improvement in performance across five models compared to standard open-source agents without MCTS. Our analysis reveals how performance scales with increased inference-time compute through deeper search, providing a pathway to improve software agents without requiring larger models or additional training data. This highlights the potential of self-evaluation driven search techniques in complex software engineering environments. | Antonis Antoniades, Albert Örwall, Kexun Zhang, Yuxi Xie, Anirudh Goyal, William Yang Wang |  |
| 636 |  |  [Language Models are Advanced Anonymizers](https://openreview.net/forum?id=82p8VHRsaK) |  | 0 | Recent privacy research on large language models (LLMs) has shown that they achieve near-human-level performance at inferring personal data from online texts. With ever-increasing model capabilities, existing text anonymization methods are currently lacking behind regulatory requirements and adversarial threats. In this work, we take two steps to bridge this gap: First, we present a new setting for evaluating anonymization in the face of adversarial LLM inferences, allowing for a natural measurement of anonymization performance while remedying some of the shortcomings of previous metrics. Then, within this setting, we develop a novel LLM-based adversarial anonymization framework leveraging the strong inferential capabilities of LLMs to inform our anonymization procedure. We conduct a comprehensive experimental evaluation of adversarial anonymization across 13 LLMs on real-world and synthetic online texts, comparing it against multiple baselines and industry-grade anonymizers. Our evaluation shows that adversarial anonymization outperforms current commercial anonymizers both in terms of the resulting utility and privacy. We support our findings with a human study (n=50) highlighting a strong and consistent human preference for LLM-anonymized texts. | Robin Staab, Mark Vero, Mislav Balunovic, Martin T. Vechev |  |
| 637 |  |  [ADAM: An Embodied Causal Agent in Open-World Environments](https://openreview.net/forum?id=Ouu3HnIVBc) |  | 0 | In open-world environments like Minecraft, existing agents face challenges in continuously learning structured knowledge, particularly causality. These challenges stem from the opacity inherent in black-box models and an excessive reliance on prior knowledge during training, which impair their interpretability and generalization capability. To this end, we introduce ADAM, An emboDied causal Agent in Minecraft, which can autonomously navigate the open world, perceive multimodal context, learn causal world knowledge, and tackle complex tasks through lifelong learning. ADAM is empowered by four key components: 1) an interaction module, enabling the agent to execute actions while recording the interaction processes; 2) a causal model module, tasked with constructing an ever-growing causal graph from scratch, which enhances interpretability and reduces reliance on prior knowledge; 3) a controller module, comprising a planner, an actor, and a memory pool, using the learned causal graph to accomplish tasks; 4) a perception module, powered by multimodal large language models, enabling ADAM to perceive like a human player. Extensive experiments show that ADAM constructs a nearly perfect causal graph from scratch, enabling efficient task decomposition and execution with strong interpretability. Notably, in the modified Minecraft game where no prior knowledge is available, ADAM excels with remarkable robustness and generalization capability. ADAM pioneers a novel paradigm that integrates causal methods and embodied agents synergistically. Our project page is at https://opencausalab.github.io/ADAM. | Shu Yu, Chaochao Lu |  |
| 638 |  |  [Expected Return Symmetries](https://openreview.net/forum?id=wFg0shwoRe) |  | 0 | Symmetry is an important inductive bias that can improve model robustness and generalization across many deep learning domains. In multi-agent settings, a priori known symmetries have been shown to address a fundamental coordination failure mode known as mutually incompatible symmetry breaking; e.g. in a game where two independent agents can choose to move "left" or "right", and where a reward of +1 or -1 is received when the agents choose the same action or different actions, respectively. However, the efficient and automatic discovery of environment symmetries, in particular for decentralized partially observable Markov decision processes, remains an open problem. Furthermore, environmental symmetry breaking constitutes only one type of coordination failure, which motivates the search for a more accessible and broader symmetry class. In this paper, we introduce such a broader group of previously unexplored symmetries, which we call expected return symmetries, which contains environment symmetries as a subgroup. We show that agents trained to be compatible under the group of expected return symmetries achieve better zero-shot coordination results than those using environment symmetries. As an additional benefit, our method makes minimal a priori assumptions about the structure of their environment and does not require access to ground truth symmetries. | Darius Muglich, Johannes Forkel, Elise van der Pol, Jakob Nicolaus Foerster |  |
| 639 |  |  [Beware of Calibration Data for Pruning Large Language Models](https://openreview.net/forum?id=x83w6yGIWb) |  | 0 | As large language models (LLMs) are widely applied across various fields, model compression has become increasingly crucial for reducing costs and improving inference efficiency. Post-training pruning is a promising method that does not require resource-intensive iterative training and only needs a small amount of calibration data to assess the importance of parameters. Recent research has enhanced post-training pruning from different aspects but few of them systematically explore the effects of calibration data, and it is unclear if there exist better calibration data construction strategies. We fill this blank and surprisingly observe that calibration data is also crucial to post-training pruning, especially for high sparsity. Through controlled experiments on important influence factors of calibration data, including the pruning settings, the amount of data, and its similarity with pre-training data, we observe that a small size of data is adequate, and more similar data to its pre-training stage can yield better performance. As pre-training data is usually inaccessible for advanced LLMs, we further provide a self-generating calibration data synthesis strategy to construct feasible calibration data. Experimental results on recent strong open-source LLMs (e.g., DCLM, and LLaMA-3) show that the proposed strategy can enhance the performance of strong pruning methods (e.g., Wanda, DSnoT, OWL) by a large margin (up to 2.68%). | Yixin Ji, Yang Xiang, Juntao Li, Qingrong Xia, Ping Li, Xinyu Duan, Zhefeng Wang, Min Zhang |  |
| 640 |  |  [Herald: A Natural Language Annotated Lean 4 Dataset](https://openreview.net/forum?id=Se6MgCtRhz) |  | 0 | Verifiable formal languages like Lean have profoundly impacted mathematical reasoning, particularly through the use of large language models (LLMs) for automated reasoning. A significant challenge in training LLMs for these formal languages is the lack of parallel datasets that align natural language with formal language proofs. To address this challenge, this paper introduces a novel framework for translating the Mathlib4 corpus (a unified library of mathematics in formal language Lean 4) into natural language. Building upon this, we employ a dual augmentation strategy that combines tactic-based and informal-based approaches, leveraging the Lean-jixia system, a Lean 4 analyzer. We present the results of this pipeline on Mathlib4 as Herald (Hierarchy and Retrieval-based Translated Lean Dataset). We also propose the Herald Translator, which is fine-tuned on Herald. Herald translator achieves a 96.7\% accuracy (Pass@128) on formalizing statements in the miniF2F-test and a 23.5\% accuracy on our internal graduate-level textbook dataset, outperforming InternLM2-Math-Plus-7B (73.0\% and 7.5\%) and TheoremLlama (50.1\% and 4.0\%). Furthermore, we propose a section-level translation framework for real-world applications. As a direct application of Herald translator, we have successfully translated a template section in the Stack project, marking a notable progress in the automatic formalization of graduate-level mathematical literature. Our model, along with the datasets, are open-sourced to the public. | Guoxiong Gao, Yutong Wang, Jiedong Jiang, Qi Gao, Zihan Qin, Tianyi Xu, Bin Dong |  |
| 641 |  |  [Efficient Residual Learning with Mixture-of-Experts for Universal Dexterous Grasping](https://openreview.net/forum?id=BUj9VSCoET) |  | 0 | Universal dexterous grasping across diverse objects presents a fundamental yet formidable challenge in robot learning. Existing approaches using reinforcement learning (RL) to develop policies on extensive object datasets face critical limitations, including complex curriculum design for multi-task learning and limited generalization to unseen objects. To overcome these challenges, we introduce ResDex, a novel approach that integrates residual policy learning with a mixture-of-experts (MoE) framework. ResDex is distinguished by its use of geometry-agnostic base policies that are efficiently acquired on individual objects and capable of generalizing across a wide range of unseen objects. Our MoE framework incorporates several base policies to facilitate diverse grasping styles suitable for various objects. By learning residual actions alongside weights that combine these base policies, ResDex enables efficient multi-task RL for universal dexterous grasping. ResDex achieves state-of-the-art performance on the DexGraspNet dataset comprising 3,200 objects with an 88.8% success rate. It exhibits no generalization gap with unseen objects and demonstrates superior training efficiency, mastering all tasks within only 12 hours on a single GPU. For further details and videos, visit our project page. | Ziye Huang, Haoqi Yuan, Yuhui Fu, Zongqing Lu |  |
| 642 |  |  [DPLM-2: A Multimodal Diffusion Protein Language Model](https://openreview.net/forum?id=5z9GjHgerY) |  | 0 | Proteins are essential macromolecules defined by their amino acid sequences, which determine their three-dimensional structures and, consequently, their functions in all living organisms. Therefore, generative protein modeling necessitates a multimodal approach to simultaneously model, understand, and generate both sequences and structures. However, existing methods typically use separate models for each modality, limiting their ability to capture the intricate relationships between sequence and structure. This results in suboptimal performance in tasks that requires joint understanding and generation of both modalities. In this paper, we introduce DPLM-2, a multimodal protein foundation model that extends discrete diffusion protein language model (DPLM) to accommodate both sequences and structures. To enable structural learning with the language model, 3D coordinates are converted to discrete tokens using a lookup-free quantization-based tokenizer. By training on both experimental and high-quality synthetic structures, DPLM-2 learns the joint distribution of sequence and structure, as well as their marginals and conditionals. We also implement an efficient warm-up strategy to exploit the connection between large-scale evolutionary data and structural inductive biases from pre-trained sequence-based protein language models. Empirical evaluation shows that DPLM-2 can simultaneously generate highly compatible amino acid sequences and their corresponding 3D structures eliminating the need for a two-stage generation approach. Moreover, DPLM-2 demonstrates competitive performance in various conditional generation tasks, including folding, inverse folding, and scaffolding with multimodal motif inputs. | Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, Quanquan Gu |  |
| 643 |  |  [Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation](https://openreview.net/forum?id=moWiYJuSGF) |  | 0 | Large language models (LLMs) have recently gained much attention in building autonomous agents. However, performance of current LLM-based web agents in long-horizon tasks is far from optimal, often yielding errors such as repeatedly buying a non-refundable flight ticket. By contrast, humans can avoid such an irreversible mistake, as we have an awareness of the potential outcomes (e.g., losing money) of our actions, also known as the "world model". Motivated by this, our study first starts with preliminary analyses, confirming the absence of world models in current LLMs (e.g., GPT-4o, Claude-3.5-Sonnet, etc.). Then, we present a World-model-augmented (WMA) web agent, which simulates the outcomes of its actions for better decision-making. To overcome the challenges in training LLMs as world models predicting next observations, such as repeated elements across observations and long HTML inputs, we propose a transition-focused observation abstraction, where the prediction objectives are free-form natural language descriptions exclusively highlighting important state differences between time steps. Experiments on WebArena and Mind2Web show that our world models improve agents' policy selection without training and demonstrate our agents' cost- and time-efficiency compared to recent tree-search-based agents. | Hyungjoo Chae, Namyoung Kim, Kai Tzuiunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim, Sunghwan Kim, Dongha Lee, Jinyoung Yeo |  |
| 644 |  |  [HyperFace: Generating Synthetic Face Recognition Datasets by Exploring Face Embedding Hypersphere](https://openreview.net/forum?id=4YzVF9isgD) |  | 0 | Face recognition datasets are often collected by crawling Internet and without individuals' consents, raising ethical and privacy concerns. Generating synthetic datasets for training face recognition models has emerged as a promising alternative. However, the generation of synthetic datasets remains challenging as it entails adequate inter-class and intra-class variations. While advances in generative models have made it easier to increase intra-class variations in face datasets (such as pose, illumination, etc.), generating sufficient inter-class variation is still a difficult task. In this paper, we formulate the dataset generation as a packing problem on the embedding space (represented on a hypersphere) of a face recognition model and propose a new synthetic dataset generation approach, called HyperFace. We formalize our packing problem as an optimization problem and solve it with a gradient descent-based approach. Then, we use a conditional face generator model to synthesize face images from the optimized embeddings. We use our generated datasets to train face recognition models and evaluate the trained models on several benchmarking real datasets. Our experimental results show that models trained with HyperFace achieve state-of-the-art performance in training face recognition using synthetic datasets. Project page: https://www.idiap.ch/paper/hyperface | Hatef OtroshiShahreza, Sébastien Marcel |  |
| 645 |  |  [Language Imbalance Driven Rewarding for Multilingual Self-improving](https://openreview.net/forum?id=Kak2ZH5Itp) |  | 0 | Large Language Models (LLMs) have achieved state-of-the-art performance across numerous tasks. However, these advancements have predominantly benefited "first-class" languages such as English and Chinese, leaving many other languages underrepresented. This imbalance, while limiting broader applications, generates a natural preference ranking between languages, offering an opportunity to bootstrap the multilingual capabilities of LLM in a self-improving manner. Thus, we propose $\textit{Language Imbalance Driven Rewarding}$, where the inherent imbalance between dominant and non-dominant languages within LLMs is leveraged as a reward signal. Iterative DPO training demonstrates that this approach not only enhances LLM performance in non-dominant languages but also improves the dominant language's capacity, thereby yielding an iterative reward signal. Fine-tuning Meta-Llama-3-8B-Instruct over two iterations of this approach results in continuous improvements in multilingual performance across instruction-following and arithmetic reasoning tasks, evidenced by an average improvement of 7.46\% win rate on the X-AlpacaEval leaderboard and 13.9\% accuracy on the MGSM benchmark. This work serves as an initial exploration, paving the way for multilingual self-improvement of LLMs. | Wen Yang, Junhong Wu, Chen Wang, Chengqing Zong, Jiajun Zhang |  |
| 646 |  |  [Quantum-PEFT: Ultra parameter-efficient fine-tuning](https://openreview.net/forum?id=dgR6i4TSng) |  | 0 | This paper introduces Quantum-PEFT that leverages quantum computations for parameter-efficient fine-tuning (PEFT). Unlike other additive PEFT methods, such as low-rank adaptation (LoRA), Quantum-PEFT exploits an underlying full-rank yet surprisingly parameter efficient _quantum unitary parameterization_. With the use of Pauli parameterization, the number of trainable parameters grows only logarithmically with the ambient dimension, as opposed to linearly as in LoRA-based PEFT methods. Quantum-PEFT achieves vanishingly smaller number of trainable parameters than the lowest-rank LoRA as dimensions grow, enhancing parameter efficiency while maintaining a competitive performance. We apply Quantum-PEFT to several transfer learning benchmarks in language and vision, demonstrating significant advantages in parameter efficiency. | Toshiaki KoikeAkino, Francesco Tonin, Yongtao Wu, Frank Zhengqing Wu, Leyla Naz Candogan, Volkan Cevher |  |
| 647 |  |  [Think Then React: Towards Unconstrained Action-to-Reaction Motion Generation](https://openreview.net/forum?id=UxzKcIZedp) |  | 0 | Modeling human-like action-to-reaction generation has significant real-world applications, like human-robot interaction and games. Despite recent advancements in single-person motion generation, it is still challenging to well handle action-to-reaction generation, due to the difficulty of directly predicting reaction from action sequence without prompts, and the absence of a unified representation that effectively encodes multi-person motion. To address these challenges, we introduce Think-Then-React (TTR), a large language-model-based framework designed to generate human-like reactions. First, with our fine-grained multimodal training strategy, TTR is capable to unify two processes during inference: a thinking process that explicitly infers action intentions and reasons corresponding reaction description, which serve as semantic prompts, and a reacting process that predicts reactions based on input action and the inferred semantic prompts. Second, to effectively represent multi-person motion in language models, we propose a unified motion tokenizer by decoupling egocentric pose and absolute space features, which effectively represents action and reaction motion with same encoding. Extensive experiments demonstrate that TTR outperforms existing baselines, achieving significant improvements in evaluation metrics, such as reducing FID from 3.988 to 1.942. | Wenhui Tan, Boyuan Li, Chuhao Jin, Wenbing Huang, Xiting Wang, Ruihua Song |  |
| 648 |  |  [Rapid Selection and Ordering of In-Context Demonstrations via Prompt Embedding Clustering](https://openreview.net/forum?id=1Iu2Yte5N6) |  | 0 | While Large Language Models (LLMs) excel at in-context learning (ICL) using just a few demonstrations, their performances are sensitive to demonstration orders. The reasons behind this sensitivity remain poorly understood. In this paper, we investigate the prompt embedding space to bridge the gap between the order sensitivity of ICL with inner workings of decoder-only LLMs, uncovering the clustering property: prompts sharing the first and last demonstrations have closer embeddings, with first-demonstration clustering usually being stronger in practice. We explain this property through extensive theoretical analyses and empirical evidences. Our finding suggests that the positional encoding and the causal attention mask are key contributors to the clustering phenomenon. Leveraging this clustering insight, we introduce Cluster-based Search, a novel method that accelerates the selection and ordering of demonstrations in self-adaptive ICL settings. Our approach substantially decreases the time complexity from factorial to quadratic, saving 92% to nearly 100% execution time while maintaining comparable performance to exhaustive search. | Kha Pham, Hung Le, Man Ngo, Truyen Tran |  |
| 649 |  |  [Asymptotic Analysis of Two-Layer Neural Networks after One Gradient Step under Gaussian Mixtures Data with Structure](https://openreview.net/forum?id=tNn6Hskmti) |  | 0 | In this work, we study the training and generalization performance of two-layer neural networks (NNs) after one gradient descent step under structured data modeled by Gaussian mixtures. While previous research has extensively analyzed this model under isotropic data assumption, such simplifications overlook the complexities inherent in real-world datasets. Our work addresses this limitation by analyzing two-layer NNs under Gaussian mixture data assumption in the asymptotically proportional limit, where the input dimension, number of hidden neurons, and sample size grow with finite ratios. We characterize the training and generalization errors by leveraging recent advancements in Gaussian universality. Specifically, we prove that a high-order polynomial model performs equivalent to the non-linear neural networks under certain conditions. The degree of the equivalent model is intricately linked to both the "data spread" and the learning rate employed during one gradient step. Through extensive simulations, we demonstrate the equivalence between the original model and its polynomial counterpart across various regression and classification tasks. Additionally, we explore how different properties of Gaussian mixtures affect learning outcomes. Finally, we illustrate experimental results on Fashion-MNIST classification, indicating that our findings can translate to realistic data. | Samet Demir, Zafer Dogan |  |
| 650 |  |  [OCEAN: Offline Chain-of-thought Evaluation and Alignment in Large Language Models](https://openreview.net/forum?id=rlgplAuN2p) |  | 0 | Offline evaluation of LLMs is crucial in understanding their capacities, though current methods remain underexplored in existing research. In this work, we focus on the offline evaluation of the chain-of-thought capabilities and show how to optimize LLMs based on the proposed evaluation method. To enable offline feedback with rich knowledge and reasoning paths, we use knowledge graphs (KGs) (e.g., Wikidata5M) to provide feedback on the generated chain of thoughts. Due to the heterogeneity between LLM reasoning and KG structures, direct interaction and feedback from knowledge graphs on LLM behavior are challenging, as they require accurate entity linking and grounding of LLM-generated chains of thought in the KG. To address the above challenge, we propose an offline chain-of-thought evaluation framework, OCEAN, which models chain-of-thought reasoning in LLMs as a Markov Decision Process (MDP), and evaluate the policy’s alignment with KG preference modeling. To overcome the reasoning heterogeneity and grounding problems, we leverage on-policy KG exploration and reinforcement learning to model a KG policy that generates token-level likelihood distributions for LLM-generated chain-of-thought reasoning paths, simulating KG reasoning preference. Then we incorporate the knowledge-graph feedback on the validity and alignment of the generated reasoning paths into inverse propensity scores and propose KG-IPS estimator. Theoretically, we prove the unbiasedness of the proposed KG-IPS estimator and provide a lower bound on its variance. With the off-policy evaluated value function, we can directly enable off-policy optimization to further enhance chain-of-thought alignment. Our empirical study shows that OCEAN can be efficiently optimized for generating chain-of-thought reasoning paths with higher estimated values without affecting LLMs’ general abilities in downstream tasks or their internal knowledge. | Junda Wu, Xintong Li, Ruoyu Wang, Yu Xia, Yuxin Xiong, Jianing Wang, Tong Yu, Xiang Chen, Branislav Kveton, Lina Yao, Jingbo Shang, Julian J. McAuley |  |
| 651 |  |  [Distribution-Free Data Uncertainty for Neural Network Regression](https://openreview.net/forum?id=pDDODPtpx9) |  | 0 | Quantifying uncertainty is an essential part of predictive modeling, especially in the context of high-stakes decision-making. While classification output includes data uncertainty by design in the form of class probabilities, the regression task generally aims only to predict the expected value of the target variable. Probabilistic extensions often assume parametric distributions around the expected value, optimizing the likelihood over the resulting explicit densities. However, using parametric distributions can limit practical applicability, making it difficult for models to capture skewed, multi-modal, or otherwise complex distributions. In this paper, we propose optimizing a novel nondeterministic neural network regression architecture for loss functions derived from a sample-based approximation of the continuous ranked probability score (CRPS), enabling a truly distribution-free approach by learning to sample from the target's aleatoric distribution, rather than predicting explicit densities. Our approach allows the model to learn well-calibrated, arbitrary uni- and multivariate output distributions. We evaluate the method on a variety of synthetic and real-world tasks, including uni- and multivariate problems, function inverse approximation, and standard regression uncertainty benchmarks. Finally, we make all experiment code publicly available. | Domokos M. Kelen, Ádám Jung, Péter Kersch, András A. Benczúr |  |
| 652 |  |  [SOO-Bench: Benchmarks for Evaluating the Stability of Offline Black-Box Optimization](https://openreview.net/forum?id=bqf0aCF3Dd) |  | 0 | Black-box optimization aims to find the optima through building a model close to the black-box objective function based on function value evaluation. However, in many real-world tasks, such as the design of molecular formulas and mechanical structures, it is perilous, costly, or even infeasible to evaluate the objective function value of an actively sampled solution. In this situation, optimization can only be conducted via utilizing offline historical data, which yields offline black-box optimization. Different from the traditional goal that is to pursue the optimal solution, this paper emphasizes that the goal of offline optimization is to stably surpass the offline dataset during optimization procedure. Although benchmarks called Design-Bench already exist in this emerging field, it can hardly evaluate the stability of offline optimization and mainly provides real-world offline tasks and the corresponding offline datasets. To this end, this paper proposes benchmarks named SOO-Bench (i.e., Stable Offline Optimization Benchmarks) for offline black-box optimization algorithms, so as to systematically evaluate the stability of surpassing the offline dataset under different data distributions. Along with SOO-Bench, we also propose a stability indicator to measure the degree of stability. Specifically, SOO-Bench includes various real-world offline optimization tasks and offline datasets under different data distributions, involving the fields of satellites, materials science, structural mechanics, and automobile manufacturing. Empirically, baseline and state-of-the-art algorithms are tested and analyzed on SOO-Bench. Hopefully, SOO-Bench is expected to serve as a catalyst for the rapid developments of more novel and stable offline optimization methods. The code is available at \url{https://github.com/zhuyiyi-123/SOO-Bench}. | Hong Qian, Yiyi Zhu, Xiang Shu, Shuo Liu, Yaolin Wen, Xin An, Huakang Lu, Aimin Zhou, Ke Tang, Yang Yu |  |
| 653 |  |  [Understanding and Enhancing Safety Mechanisms of LLMs via Safety-Specific Neuron](https://openreview.net/forum?id=yR47RmND1m) |  | 0 | Safety alignment for large language models (LLMs) has become a critical issue due to their rapid progress. However, our understanding of effective safety mechanisms in LLMs remains limited, leading to safety alignment training that mainly focuses on improving optimization, data-level enhancement, or adding extra structures to intentionally block harmful outputs. To address this gap, we develop a neuron detection method to identify safety neurons—those consistently crucial for handling and defending against harmful queries. Our findings reveal that these safety neurons constitute less than $1\%$ of all parameters, are language-specific and are predominantly located in self-attention layers. Moreover, safety is collectively managed by these neurons in the first several layers. Based on these observations, we introduce a $\underline{S}$afety $\underline{N}$euron $\underline{Tun}$ing method, named $\texttt{SN-Tune}$, that exclusively tune safety neurons without compromising models' general capabilities. $\texttt{SN-Tune}$ significantly enhances the safety of instruction-tuned models, notably reducing the harmful scores of Llama3-8B-Instruction from $65.5$ to $2.0$, Mistral-7B-Instruct-v0.2 from $70.8$ to $4.5$, and Vicuna-13B-1.5 from $93.5$ to $3.0$. Moreover, $\texttt{SN-Tune}$ can be applied to base models on efficiently establishing LLMs' safety mechanism. In addition, we propose $\underline{R}$obust $\underline{S}$afety $\underline{N}$euron $\underline{Tun}$ing method ($\texttt{RSN-Tune}$), which preserves the integrity of LLMs' safety mechanisms during downstream task fine-tuning by separating the safety neurons from models' foundation neurons. | Yiran Zhao, Wenxuan Zhang, Yuxi Xie, Anirudh Goyal, Kenji Kawaguchi, Michael Shieh |  |
| 654 |  |  [Long Context Compression with Activation Beacon](https://openreview.net/forum?id=1eQT9OzfNQ) |  | 0 | Long context compression is a critical research problem due to its significance in reducing the high computational and memory costs associated with LLMs. In this paper, we propose Activation Beacon, a plug-in module for transformer-based LLMs that targets effective, efficient, and flexible compression of long contexts. To achieve this, our method introduces the following technical designs. 1) We directly compress the activations (i.e. keys and values at every layer), rather than leveraging soft prompts to relay information (which constitute a major bottleneck to encapsulate the complex information within long contexts). 2) We tailor the compression workflow, where each fine-grained input unit is progressively compressed, enabling high-quality compression and efficient computation during both training and inference. 3) We train the model through compression-based auto-regression, making full use of plain texts and instructional data to optimize the model's compression performance. 4) During training, we randomly sample a compression ratio at each step, teaching the model to support a wide range of compression configurations. Extensive evaluations are conducted on various long-context tasks whose lengths (e.g., 128K) may far exceed the maximum training length (20K), such as document understanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing methods struggle to handle these challenging tasks, Activation Beacon maintains a comparable performance to the uncompressed baseline across various scenarios, achieving a 2x acceleration in inference time and an 8x reduction of memory costs for KV cache. | Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, Zhicheng Dou |  |
| 655 |  |  [LASeR: Towards Diversified and Generalizable Robot Design with Large Language Models](https://openreview.net/forum?id=7mlvOHL6qJ) |  | 0 | Recent advances in Large Language Models (LLMs) have stimulated a significant paradigm shift in evolutionary optimization, where hand-crafted search heuristics are gradually replaced with LLMs serving as intelligent search operators. However, these studies still bear some notable limitations, including a challenge to balance exploitation with exploration, often leading to inferior solution diversity, as well as poor generalizability of problem solving across different task settings. These unsolved issues render the prowess of LLMs in robot design automation largely untapped. In this work, we present LASeR -- Large Language Model-Aided Evolutionary Search for Robot Design Automation. Leveraging a novel reflection mechanism termed DiRect, we elicit more knowledgeable exploratory behaviors from LLMs based on past search trajectories, reshaping the exploration-exploitation tradeoff with dual improvements in optimization efficiency and solution diversity. Additionally, with evolution fully grounded in task-related background information, we unprecedentedly uncover the inter-task reasoning capabilities of LLMs, facilitating generalizable design processes that effectively inspire zero-shot robot proposals for new applications. Our simulated experiments on voxel-based soft robots showcase distinct advantages of LASeR over competitive baselines. Code at https://github.com/WoodySJR/LASeR. | Junru Song, Yang Yang, Huan Xiao, Wei Peng, Wen Yao, Feifei Wang |  |
| 656 |  |  [Be More Diverse than the Most Diverse: Optimal Mixtures of Generative Models via Mixture-UCB Bandit Algorithms](https://openreview.net/forum?id=2Chkk5Ye2s) |  | 0 | The availability of multiple training algorithms and architectures for generative models requires a selection mechanism to form a single model over a group of well-trained generation models. The selection task is commonly addressed by identifying the model that maximizes an evaluation score based on the diversity and quality of the generated data. However, such a best-model identification approach overlooks the possibility that a mixture of available models can outperform each individual model. In this work, we numerically show that a mixture of generative models on benchmark image datasets can indeed achieve a better evaluation score (based on FID and KID scores), compared to the individual models. This observation motivates the development of efficient algorithms for selecting the optimal mixture of the models. To address this, we formulate a quadratic optimization problem to find an optimal mixture model achieving the maximum of kernel-based evaluation scores including kernel inception distance (KID) and Rényi kernel entropy (RKE). To identify the optimal mixture of the models using the fewest possible sample queries, we view the selection task as a multi-armed bandit (MAB) problem and propose the \*Mixture Upper Confidence Bound (Mixture-UCB)\* algorithm that provably converges to the optimal mixture of the involved models. More broadly, the proposed Mixture-UCB can be extended to optimize every convex quadratic function of the mixture weights in a general MAB setting. We prove a regret bound for the Mixture-UCB algorithm and perform several numerical experiments to show the success of Mixture-UCB in finding the optimal mixture of text and image generative models. The project code is available in the [Mixture-UCB Github repository](https://github.com/Rezaei-Parham/Mixture-UCB). | Parham Rezaei, Farzan Farnia, Cheuk Ting Li |  |
| 657 |  |  [Arithmetic Transformers Can Length-Generalize in Both Operand Length and Count](https://openreview.net/forum?id=eIgGesYKLG) |  | 0 | Transformers often struggle with \*length generalization\*, meaning they fail to generalize to sequences longer than those encountered during training. While arithmetic tasks are commonly used to study length generalization, certain tasks are considered notoriously difficult, e.g., multi-operand addition (requiring generalization over both the number of operands and their lengths) and multiplication (requiring generalization over both operand lengths). In this work, we achieve approximately 2–3× length generalization on both tasks, which is the first such achievement in arithmetic Transformers. We design task-specific scratchpads enabling the model to focus on a fixed number of tokens per each next-token prediction step, and apply multi-level versions of \*Position Coupling\* (Cho et al., 2024; McLeish et al., 2024) to let Transformers know the right position to attend to. On the theory side, we prove that a 1-layer Transformer using our method can solve multi-operand addition, up to operand length and operand count that are exponential in embedding dimension. | Hanseul Cho, Jaeyoung Cha, Srinadh Bhojanapalli, Chulhee Yun |  |
| 658 |  |  [Stealthy Shield Defense: A Conditional Mutual Information-Based Approach against Black-Box Model Inversion Attacks](https://openreview.net/forum?id=p0DjhjPXl3) |  | 0 | Model inversion attacks (MIAs) aim to reconstruct the private training data by accessing the public model, raising concerns about privacy leakage. Black-box MIAs, where attackers can only query the model and obtain outputs, are closer to real-world scenarios. The latest black-box attacks have outperformed state-of-the-art white-box attacks, and existing defenses cannot resist them effectively. To fill this gap, we propose Stealthy Shield Defense (SSD), a post-processing algorithm against black-box MIAs. Our idea is to modify the model's outputs to minimize the conditional mutual information (CMI). We mathematically prove that CMI is a special case of Information Bottleneck (IB), and thus inherits the benefits of IB---making predictions less dependent on inputs and more dependent on ground truths. This theoretically guarantees our effectiveness, both in resisting MIAs and preserving utility. To minimize CMI, we formulate a convex optimization problem and solve it via the water-filling method. Without the need to retrain the model, our defense is plug-and-play and easy to deploy. Experimental results indicate that SSD outperforms existing defenses, in terms of MIA resistance and model's utility, across various attack algorithms, private datasets, and model architectures. Our code is available at https://github.com/ZhuangQu/Stealthy-Shield-Defense. | Tianqu Zhuang, Hongyao Yu, Yixiang Qiu, Hao Fang, Bin Chen, ShuTao Xia |  |
| 659 |  |  [NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens](https://openreview.net/forum?id=uMEsKEiB7J) |  | 0 | Recent advancements in Large Language Models (LLMs) have pushed the boundaries of natural language processing, especially in long-context understanding. However, the evaluation of these models' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark tailored for evaluating LLMs with complex, extended narratives. NovelQA, constructed from English novels, offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper details the design and construction of NovelQA, focusing on its comprehensive manual annotation process and the variety of question types aimed at evaluating nuanced comprehension. Our evaluation of long-context LLMs on NovelQA reveals significant insights into their strengths and weaknesses. Notably, the models struggle with multi-hop reasoning, detail-oriented questions, and handling extremely long inputs, averaging over 200,000 tokens. Results highlight the need for substantial advancements in LLMs to enhance their long-context comprehension and contribute effectively to computational literary analysis. | Cunxiang Wang, Ruoxi Ning, Boqi Pan, Tonghui Wu, Qipeng Guo, Cheng Deng, Guangsheng Bao, Xiangkun Hu, Zheng Zhang, Qian Wang, Yue Zhang |  |
| 660 |  |  [Look Before You Leap: Universal Emergent Mechanism for Retrieval in Language Models](https://openreview.net/forum?id=eIB1UZFcFg) |  | 0 | When solving challenging problems, language models (LMs) are able to identify relevant information from long and complicated contexts. To study how LMs solve retrieval tasks in diverse situations, we introduce ORION, a collection of structured retrieval tasks spanning six domains, from text understanding to coding. Each task in ORION can be represented abstractly by a request (e.g. a question) that retrieves an attribute (e.g. the character name) from a context (e.g. a story). We apply causal analysis on 18 open-source language models with sizes ranging from 125 million to 70 billion parameters. We find that LMs internally decompose retrieval tasks in a modular way: middle layers at the last token position process the request, while late layers retrieve the correct entity from the context. After causally enforcing this decomposition, models are still able to solve the original task, preserving 70% of the original correct token probability in 98 of the 106 studied model-task pairs. We connect our macroscopic decomposition with a microscopic description by performing a fine-grained case study of a question-answering task on Pythia-2.8b. Building on our high-level understanding, we demonstrate a proof of concept application for scalable internal oversight of LMs to mitigate prompt-injection while requiring human supervision on only a single input. Our solution improves accuracy drastically (from 15.5% to 97.5% on Pythia-12b). This work presents evidence of a universal emergent modular processing of tasks across varied domains and models and is a pioneering effort in applying interpretability for scalable internal oversight of LMs. | Alexandre Variengien, Eric Winsor |  |
| 661 |  |  [A Multi-Power Law for Loss Curve Prediction Across Learning Rate Schedules](https://openreview.net/forum?id=KnoS9XxIlK) |  | 0 | Training large models is both resource-intensive and time-consuming, making it crucial to understand the quantitative relationship between model performance and hyperparameters. In this paper, we derive an empirical law that predicts pretraining loss for large language models for every intermediate training step across various learning rate schedules, including constant, cosine, and step decay schedules. Our proposed law takes a multi-power form, combining a power law based on the sum of learning rates and additional power laws to account for a loss reduction effect as learning rate decays. We validate this law extensively on Llama-2 models of varying sizes and demonstrate that, after fitting on a few learning rate schedules, it accurately predicts the loss curves for unseen schedules of different shapes and horizons. Moreover, by minimizing the predicted final pretraining loss across learning rate schedules, we are able to find a schedule that outperforms the widely-used cosine learning rate schedule. Interestingly, this automatically discovered schedule bears some resemblance to the recently proposed Warmup-Stable-Decay (WSD) schedule (Hu et al, 2024) but achieves a slightly lower final loss. We believe these results could offer valuable insights for understanding the dynamics of pretraining and for designing learning rate schedules to improve efficiency. | Kairong Luo, Haodong Wen, Shengding Hu, Zhenbo Sun, Zhiyuan Liu, Maosong Sun, Kaifeng Lyu, Wenguang Chen |  |
| 662 |  |  [LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token](https://openreview.net/forum?id=UQJ7CDW8nb) |  | 0 | The advent of real-time large multimodal models (LMMs) like GPT-4o has sparked considerable interest in efficient LMMs. LMM frameworks typically encode visual inputs into vision tokens (continuous representations) and integrate them and textual instructions into the context of large language models (LLMs), where large-scale parameters and numerous context tokens (predominantly vision tokens) result in substantial computational overhead. Previous efforts towards efficient LMMs always focus on replacing the LLM backbone with smaller models, while neglecting the crucial issue of token quantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal vision tokens. To achieve a high compression ratio of vision tokens while preserving visual information, we first analyze how LMMs understand vision tokens and find that most vision tokens only play a crucial role in the early layers of LLM backbone, where they mainly fuse visual information into text tokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to fuse visual information into text tokens in advance, thereby facilitating the extreme compression of vision tokens fed to LLM backbone into one token. LLaVA-Mini is a unified large multimodal model that can support the understanding of images, high-resolution images, and videos in an efficient manner. Experiments across 11 image-based and 7 video-based benchmarks demonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token instead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by 77%, deliver low-latency responses within 40 milliseconds, and process over 10,000 frames of video on the GPU hardware with 24GB of memory. | Shaolei Zhang, Qingkai Fang, Zhe Yang, Yang Feng |  |
| 663 |  |  [URLOST: Unsupervised Representation Learning without Stationarity or Topology](https://openreview.net/forum?id=MBBRHDuiwM) |  | 0 | Unsupervised representation learning has seen tremendous progress. However, it is constrained by its reliance on domain specific stationarity and topology, a limitation not found in biological intelligence systems. For instance, unlike computer vision, human vision can process visual signals sampled from highly irregular and non-stationary sensors. We introduce a novel framework that learns from high-dimensional data without prior knowledge of stationarity and topology. Our model, abbreviated as URLOST, combines a learnable self-organizing layer, spectral clustering, and a masked autoencoder (MAE). We evaluate its effectiveness on three diverse data modalities including simulated biological vision data, neural recordings from the primary visual cortex, and gene expressions. Compared to state-of-the-art unsupervised learning methods like SimCLR and MAE, our model excels at learning meaningful representations across diverse modalities without knowing their stationarity or topology. It also outperforms other methods that are not dependent on these factors, setting a new benchmark in the field. We position this work as a step toward unsupervised learning methods capable of generalizing across diverse high-dimensional data modalities. | Zeyu Yun, Juexiao Zhang, Yann LeCun, Yubei Chen |  |
| 664 |  |  [One-for-All Few-Shot Anomaly Detection via Instance-Induced Prompt Learning](https://openreview.net/forum?id=Zzs3JwknAY) |  | 0 | Anomaly detection methods under the 'one-for-all' paradigm aim to develop a unified model capable of detecting anomalies across multiple classes. However, these approaches typically require a large number of normal samples for model training, which may not always be feasible in practice. Few-shot anomaly detection methods can address scenarios with limited data but often require a tailored model for each class, struggling within the 'one-for-one' paradigm. In this paper, we first proposed the one-for-all few-shot anomaly detection method with the assistance of vision-language model. Different from previous CLIP-based methods learning fix prompts for each class, our method learn a class-shared prompt generator to adaptively generate suitable prompt for each instance. The prompt generator is trained by aligning the prompts with the visual space and utilizing guidance from general textual descriptions of normality and abnormality. Furthermore, we address the mismatch problem of the memory bank within one-for-all paradigm. Extensive experimental results on MVTec and VisA demonstrate the superiority of our method in few-shot anomaly detection task under the one-for-all paradigm. | Wenxi Lv, Qinliang Su, Wenchao Xu |  |
| 665 |  |  [K-HALU: Multiple Answer Korean Hallucination Benchmark for Large Language Models](https://openreview.net/forum?id=VnLhUogHYE) |  | 0 | Recent researchers and companies have been developing large language models (LLMs) specifically designed for particular purposes and have achieved significant advancements in various natural language processing tasks. However, LLMs are still prone to generating hallucinations—results that are unfaithful or inconsistent with the given input. As a result, the need for datasets to evaluate and demonstrate the hallucination detection capabilities of LLMs is increasingly recognized. Nonetheless, the Korean NLP community lacks publicly available benchmark datasets demonstrating the faithfulness of knowledge-based information. Furthermore, the few existing datasets that evaluate hallucination are limited in their access to the entire dataset, restricting detailed analysis beyond simple scoring, and are based on translated English knowledge. To address these challenges, we introduce K-HALU, a Korean benchmark designed to evaluate LLMs' hallucination detection in Korean. This benchmark contains seven domains, considering the faithfulness of statements based on knowledge documents compiled from Korean news, magazines, and books. For more strict evaluation, 40% of the dataset is structured as multiple-answer questions, requiring models to select all possible correct answers from the given options. Our empirical results show that open-source LLMs still struggle with hallucination detection in Korean knowledge, emphasizing the need for a more detailed analysis of their limitations. | Jaehyung Seo, Heuiseok Lim |  |
| 666 |  |  [Charting the Design Space of Neural Graph Representations for Subgraph Matching](https://openreview.net/forum?id=5pd78GmXC6) |  | 0 | Subgraph matching is vital in knowledge graph (KG) question answering, molecule design, scene graph, code and circuit search, etc. Neural methods have shown promising results for subgraph matching. Our study of recent systems suggests refactoring them into a unified design space for graph matching networks. Existing methods occupy only a few isolated patches in this space, which remains largely uncharted. We undertake the first comprehensive exploration of this space, featuring such axes as attention-based vs. soft permutation-based interaction between query and corpus graphs, aligning nodes vs. edges, and the form of the final scoring network that integrates neural representations of the graphs. Our extensive experiments reveal that judicious and hitherto-unexplored combinations of choices in this space lead to large performance benefits. Beyond better performance, our study uncovers valuable insights and establishes general design principles for neural graph representation and interaction, which may be of wider interest. | Vaibhav Raj, Indradyumna Roy, Ashwin Ramachandran, Soumen Chakrabarti, Abir De |  |
| 667 |  |  [Convergence and Implicit Bias of Gradient Descent on Continual Linear Classification](https://openreview.net/forum?id=DTqx3iqjkz) |  | 0 | We study continual learning on multiple linear classification tasks by sequentially running gradient descent (GD) for a fixed budget of iterations per each given task. When all tasks are jointly linearly separable and are presented in a cyclic/random order, we show the directional convergence of the trained linear classifier to the joint (offline) max-margin solution. This is surprising because GD training on a single task is implicitly biased towards the individual max-margin solution for the task, and the direction of the joint max-margin solution can be largely different from these individual solutions. Additionally, when tasks are given in a cyclic order, we present a non-asymptotic analysis on cycle-averaged forgetting, revealing that (1) alignment between tasks is indeed closely tied to catastrophic forgetting and backward knowledge transfer and (2) the amount of forgetting vanishes to zero as the cycle repeats. Lastly, we analyze the case where the tasks are no longer jointly separable and show that the model trained in a cyclic order converges to the unique minimum of the joint loss function. | Hyunji Jung, Hanseul Cho, Chulhee Yun |  |
| 668 |  |  [The Unreasonable Ineffectiveness of the Deeper Layers](https://openreview.net/forum?id=ngmEcEer8a) |  | 0 | How is knowledge stored in an LLM’s weights? We study this via layer pruning: if removing a certain layer does not affect model performance in common question-answering benchmarks, then the weights in that layer are not necessary for storing the knowledge needed to answer those questions. To find these unnecessary parameters, we identify the optimal block of layers to prune by considering similarity across layers; then, to “heal” the damage, we perform a small amount of finetuning. Surprisingly, with this method we find minimal degradation of performance until after a large fraction (up to half) of the layers are removed for some common open-weight models. From a scientific perspective, the robustness of these LLMs to the deletion of layers implies either that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge. For our study, we use parameter-efficient finetuning (PEFT) methods, specifically quantization and Low Rank Adapters (QLoRA), such that each of our experiments can be performed on a single 40GB A100 GPU. | Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, Daniel A. Roberts |  |
| 669 |  |  [Distilling Dataset into Neural Field](https://openreview.net/forum?id=nCrJD7qPJN) |  | 0 | Utilizing a large-scale dataset is essential for training high-performance deep learning models, but it also comes with substantial computation and storage costs. To overcome these challenges, dataset distillation has emerged as a promising solution by compressing the large-scale dataset into a smaller synthetic dataset that retains the essential information needed for training. This paper proposes a novel parameterization framework for dataset distillation, coined Distilling Dataset into Neural Field (DDiF), which leverages the neural field to store the necessary information of the large-scale dataset. Due to the unique nature of the neural field, which takes coordinates as input and output quantity, DDiF effectively preserves the information and easily generates various shapes of data. We theoretically confirm that DDiF exhibits greater expressiveness than some previous literature when the utilized budget for a single synthetic instance is the same. Through extensive experiments, we demonstrate that DDiF achieves superior performance on several benchmark datasets, extending beyond the image domain to include video, audio, and 3D voxel. We release the code at \url{https://github.com/aailab-kaist/DDiF}. | Donghyeok Shin, HeeSun Bae, Gyuwon Sim, Wanmo Kang, IlChul Moon |  |
| 670 |  |  [Relax and Merge: A Simple Yet Effective Framework for Solving Fair k-Means and k-sparse Wasserstein Barycenter Problems](https://openreview.net/forum?id=n8h1z588eu) |  | 0 | The fairness of clustering algorithms has gained widespread attention across various areas, including machine learning, In this paper, we study fair $k$-means clustering in Euclidean space. Given a dataset comprising several groups, the fairness constraint requires that each cluster should contain a proportion of points from each group within specified lower and upper bounds. Due to these fairness constraints, determining the optimal locations of $k$ centers is a quite challenging task. We propose a novel \`\`Relax and Merge'' framework that returns a $(1+4\rho + O(\epsilon))$-approximate solution, where $\rho$ is the approximate ratio of an off-the-shelf vanilla $k$-means algorithm and $O(\epsilon)$ can be an arbitrarily small positive number. If equipped with a PTAS of $k$-means, our solution can achieve an approximation ratio of $(5+O(\epsilon))$ with only a slight violation of the fairness constraints, which improves the current state-of-the-art approximation guarantee. Furthermore, using our framework, we can also obtain a $(1+4\rho +O(\epsilon))$-approximate solution for the $k$-sparse Wasserstein Barycenter problem, which is a fundamental optimization problem in the field of optimal transport, and a $(2+6\rho)$-approximate solution for the strictly fair $k$-means clustering with no violation, both of which are better than the current state-of-the-art methods. In addition, the empirical results demonstrate that our proposed algorithm can significantly outperform baseline approaches in terms of clustering cost. | Shihong Song, Guanlin Mo, Hu Ding |  |
| 671 |  |  [Neural Dueling Bandits: Preference-Based Optimization with Human Feedback](https://openreview.net/forum?id=VELhv9BBfn) |  | 0 | Contextual dueling bandit is used to model the bandit problems, where a learner's goal is to find the best arm for a given context using observed noisy human preference feedback over the selected arms for the past contexts. However, existing algorithms assume the reward function is linear, which can be complex and non-linear in many real-life applications like online recommendations or ranking web search results. To overcome this challenge, we use a neural network to estimate the reward function using preference feedback for the previously selected arms. We propose upper confidence bound- and Thompson sampling-based algorithms with sub-linear regret guarantees that efficiently select arms in each round. We also extend our theoretical results to contextual bandit problems with binary feedback, which is in itself a non-trivial contribution. Experimental results on the problem instances derived from synthetic datasets corroborate our theoretical results. | Arun Verma, Zhongxiang Dai, Xiaoqiang Lin, Patrick Jaillet, Bryan Kian Hsiang Low |  |
| 672 |  |  [SPORTU: A Comprehensive Sports Understanding Benchmark for Multimodal Large Language Models](https://openreview.net/forum?id=x1yOHtFfDh) |  | 0 | Multimodal Large Language Models (MLLMs) are advancing the ability to reason about complex sports scenarios by integrating textual and visual information. To comprehensively evaluate their capabilities, we introduce SPORTU, a benchmark designed to assess MLLMs across multi-level sports reasoning tasks. SPORTU comprises two key components: SPORTU-text, featuring 900 multiple-choice questions with human-annotated explanations for rule comprehension and strategy understanding. This component focuses on testing models' ability to reason about sports solely through question-answering (QA), without requiring visual inputs; SPORTU-video, consisting of 1,701 slow-motion video clips across 7 different sports and 12,048 QA pairs, designed to assess multi-level reasoning, from simple sports recognition to complex tasks like foul detection and rule application. We evaluated four prevalent LLMs mainly utilizing few-shot learning paradigms supplemented by chain-of-thought (CoT) prompting on the SPORTU-text part. GPT-4o achieves the highest accuracy of 71\%, but still falls short of human-level performance, highlighting room for improvement in rule comprehension and reasoning. The evaluation for the SPORTU-video part includes 6 proprietary and 8 open-source MLLMs. Experiments show that models fall short on hard tasks that require deep reasoning and rule-based understanding. GPT-4o performs the best with only 57.8\% accuracy on the hard task, showing large room for improvement. We hope that SPORTU will serve as a critical step toward evaluating models' capabilities in sports understanding and reasoning. The dataset is available at [https://github.com/chili-lab/SPORTU](https://github.com/chili-lab/SPORTU). | Haotian Xia, Zhengbang Yang, Junbo Zou, Rhys Tracy, Yuqing Wang, Chi Lu, Christopher Lai, Yanjun He, Xun Shao, Zhuoqing Xie, YuanFang Wang, Weining Shen, Hanjie Chen |  |
| 673 |  |  [SIM: Surface-based fMRI Analysis for Inter-Subject Multimodal Decoding from Movie-Watching Experiments](https://openreview.net/forum?id=OJsMGsO6yn) |  | 0 | Current AI frameworks for brain decoding and encoding, typically train and test models within the same datasets. This limits their utility for cognitive training (neurofeedback) for which it would be useful to pool experiences across individuals to better simulate stimuli not sampled during training. A key obstacle to model generalisation is the degree of variability of inter-subject cortical organisation, which makes it difficult to align or compare cortical signals across participants. In this paper we address this through use of surface vision transformers, which build a generalisable model of cortical functional dynamics, through encoding the topography of cortical networks and their interactions as a moving image across a surface. This is then combined with tri-modal self-supervised contrastive (CLIP) alignment of audio, video, and fMRI modalities to enable the retrieval of visual and auditory stimuli from patterns of cortical activity (and vice-versa). We validate our approach on 7T task-fMRI data from 174 healthy participants engaged in the movie-watching experiment from the Human Connectome Project (HCP). Results show that it is possible to detect which movie clips an individual is watching purely from their brain activity, even for individuals and movies \*not seen during training\*. Further analysis of attention maps reveals that our model captures individual patterns of brain activity that reflect semantic and visual systems. This opens the door to future personalised simulations of brain function. Code \& pre-trained models will be made available at https://github.com/metrics-lab/sim. | Simon Dahan, Gabriel Bénédict, Logan Zane John Williams, Yourong Guo, Daniel Rueckert, Robert Leech, Emma Claire Robinson |  |
| 674 |  |  [Why In-Context Learning Models are Good Few-Shot Learners?](https://openreview.net/forum?id=iLUcsecZJp) |  | 0 | We explore in-context learning (ICL) models from a learning-to-learn perspective. Unlike studies that identify specific learning algorithms in ICL models, we compare ICL models with typical meta-learners to understand their superior performance. We theoretically prove the expressiveness of ICL models as learning algorithms and examine their learnability and generalizability. Our findings show that ICL with transformers can effectively construct data-dependent learning algorithms instead of directly follow existing ones (including gradient-based, metric-based, and amortization-based meta-learners). The construction of such learning algorithm is determined by the pre-training process, as a function fitting the training distribution, which raises generalizability as an important issue. With above understanding, we propose strategies to transfer techniques for classical deep networks to meta-level to further improve ICL. As examples, we implement meta-level meta-learning for domain adaptability with limited data and meta-level curriculum learning for accelerated convergence during pre-training, demonstrating their empirical effectiveness. | Shiguang Wu, Yaqing Wang, Quanming Yao |  |
| 675 |  |  [Evaluating Large Language Models through Role-Guide and Self-Reflection: A Comparative Study](https://openreview.net/forum?id=E36NHwe7Zc) |  | 0 | Large Language Models fine-tuned with Reinforcement Learning from Human Feedback (RLHF-LLMs) can over-rely on aligned preferences without truly gaining self-knowledge, leading to hallucination and biases. If an LLM can better access its knowledge and know what it knows, it can avoid making false or unsupported claims. Therefore, it is crucial to evaluate whether LLMs have the ability to know what they know, as it can help to ensure accuracy and faithfulness in real-world applications. Inspired by research in Educational Psychology, surface learners who don’t really know are easily affected by teacher and peer guidance, we treat LLM as a student, incorporate role guidance in prompts to explore whether LLMs really know. Specifically, we propose a novel strategy called Role-Guided and Self-Reflection (RoSe) to fully assess whether LLM “knows it knows”. We introduce multiple combinations of different roles and strong reminder in prompts combined with self-reflection to explore what local information in prompt LLMs rely on and whether LLMs remain unaffected by external guidance with varying roles. Our findings reveal that LLMs are very sensitive to the strong reminder information. Role guidance can help LLMs reduce their reliance on strong reminder. Meanwhile, LLMs tend to trust the role of authority more when guided by different roles. Following these findings, we propose a double-calibrated strategy with verbalized confidence to extract well-calibrated data from closed-source LLM and fine-tune open-source LLMs. Extensive experiments conducted on fine-tuning open-source LLMs demonstrate the effectiveness of double-calibrated strategy in mitigating the reliance of LLMs on local information. For a thorough comparison, we not only employ public JEC-QA and openBookQA datasets, but also construct EG-QA which contains English Grammar multiple-choice question-answering and 14 key knowledge points for assessing self-knowledge and logical reasoning. | Lili Zhao, Yang Wang, Qi Liu, Mengyun Wang, Wei Chen, Zhichao Sheng, Shijin Wang |  |
| 676 |  |  [Local Loss Optimization in the Infinite Width: Stable Parameterization of Predictive Coding Networks and Target Propagation](https://openreview.net/forum?id=g6syfIrVuS) |  | 0 | Local learning, which trains a network through layer-wise local targets and losses, has been studied as an alternative to backpropagation (BP) in neural computation. However, its algorithms often become more complex or require additional hyperparameters due to the locality, making it challenging to identify desirable settings where the algorithm progresses in a stable manner. To provide theoretical and quantitative insights, we introduce maximal update parameterization ($\mu$P) in the infinite-width limit for two representative designs of local targets: predictive coding (PC) and target propagation (TP). We verify that $\mu$P enables hyperparameter transfer across models of different widths. Furthermore, our analysis reveals unique and intriguing properties of $\mu$P that are not present in conventional BP. By analyzing deep linear networks, we find that PC's gradients interpolate between first-order and Gauss-Newton-like gradients, depending on the parameterization. We demonstrate that, in specific standard settings, PC in the infinite-width limit behaves more similarly to the first-order gradient. For TP, even with the standard scaling of the last layer differing from classical $\mu$P, its local loss optimization favors the feature learning regime over the kernel regime. | Satoki Ishikawa, Rio Yokota, Ryo Karakida |  |
| 677 |  |  [Towards Faster Decentralized Stochastic Optimization with Communication Compression](https://openreview.net/forum?id=CMMpcs9prj) |  | 0 | Communication efficiency has garnered significant attention as it is considered the main bottleneck for large-scale decentralized Machine Learning applications in distributed and federated settings. In this regime, clients are restricted to transmitting small amounts of compressed information to their neighbors over a communication graph. Numerous endeavors have been made to address this challenging problem by developing algorithms with compressed communication for decentralized non-convex optimization problems. Despite considerable efforts, current theoretical understandings of the problem are still very limited, and existing algorithms all suffer from various limitations. In particular, these algorithms typically rely on strong, and often infeasible assumptions such as bounded data heterogeneity or require large batch access while failing to achieve linear speedup with the number of clients. In this paper, we introduce MoTEF, a novel approach that integrates communication compression with $\textbf{Mo}$mentum $\textbf{T}$racking and $\textbf{E}$rror $\textbf{F}$eedback. MoTEF is the first algorithm to achieve an asymptotic rate matching that of distributed SGD under arbitrary data heterogeneity, hence resolving a long-standing theoretical obstacle in decentralized optimization with compressed communication. We provide numerical experiments to validate our theoretical findings and confirm the practical superiority of MoTEF. | Rustem Islamov, Yuan Gao, Sebastian U. Stich |  |
| 678 |  |  [Group-robust Sample Reweighting for Subpopulation Shifts via Influence Functions](https://openreview.net/forum?id=aQj9Ifxrl6) |  | 0 | Machine learning models often have uneven performance among subpopulations (a.k.a., groups) in the data distributions. This poses a significant challenge for the models to generalize when the proportions of the groups shift during deployment. To improve robustness to such shifts, existing approaches have developed strategies that train models or perform hyperparameter tuning using the group-labeled data to minimize the worst-case loss over groups. However, a non-trivial amount of high-quality labels is often required to obtain noticeable improvements. Given the costliness of the labels, we propose to adopt a different paradigm to enhance group label efficiency: utilizing the group-labeled data as a target set to optimize the weights of other group-unlabeled data. We introduce Group-robust Sample Reweighting (GSR), a two-stage approach that first learns the representations from group-unlabeled data, and then tinkers the model by iteratively retraining its last layer on the reweighted data using influence functions. Our GSR is theoretically sound, practically lightweight, and effective in improving the robustness to sub- population shifts. In particular, GSR outperforms the previous state-of-the-art approaches that require the same amount or even more group labels. Our code is available at https://github.com/qiaoruiyt/GSR. | Rui Qiao, Zhaoxuan Wu, Jingtan Wang, Pang Wei Koh, Bryan Kian Hsiang Low |  |
| 679 |  |  [Endless Jailbreaks with Bijection Learning](https://openreview.net/forum?id=xP1radUi32) |  | 0 | Despite extensive safety measures, LLMs are vulnerable to adversarial inputs, or jailbreaks, which can elicit unsafe behaviors. In this work, we introduce bijection learning, a powerful attack algorithm which automatically fuzzes LLMs for safety vulnerabilities using randomly-generated encodings whose complexity can be tightly controlled. We leverage in-context learning to teach models bijective encodings, pass encoded queries to the model to bypass built-in safety mechanisms, and finally decode responses back into English. Our attack is extremely effective on a wide range of frontier language models. By controlling complexity parameters such as number of key-value mappings in the encodings, we find a close relationship between the capability level of the attacked LLM and the average complexity of the most effective bijection attacks. Our work highlights that new vulnerabilities in frontier models can emerge with scale: more capable models are more severely jailbroken by bijection attacks. | Brian R. Y. Huang, Maximilian Li, Leonard Tang |  |
| 680 |  |  [GotenNet: Rethinking Efficient 3D Equivariant Graph Neural Networks](https://openreview.net/forum?id=5wxCQDtbMo) |  | 0 | Understanding complex three-dimensional (3D) structures of graphs is essential for accurately modeling various properties, yet many existing approaches struggle with fully capturing the intricate spatial relationships and symmetries inherent in such systems, especially in large-scale, dynamic molecular datasets. These methods often must balance trade-offs between expressiveness and computational efficiency, limiting their scalability. To address this gap, we propose a novel Geometric Tensor Network (GotenNet) that effectively models the geometric intricacies of 3D graphs while ensuring strict equivariance under the Euclidean group E(3). Our approach directly tackles the expressiveness-efficiency trade-off by leveraging effective geometric tensor representations without relying on irreducible representations or Clebsch-Gordan transforms, thereby reducing computational overhead. We introduce a unified structural embedding, incorporating geometry-aware tensor attention and hierarchical tensor refinement that iteratively updates edge representations through inner product operations on high-degree steerable features, allowing for flexible and efficient representations for various tasks. We evaluated models on QM9, rMD17, MD22, and Molecule3D datasets, where the proposed model consistently outperforms state-of-the-art methods in both scalar and high-degree property predictions, demonstrating exceptional robustness across diverse datasets, and establishes GotenNet as a versatile and scalable framework for 3D equivariant Graph Neural Networks. | Sarp Aykent, Tian Xia |  |
| 681 |  |  [Efficient Automated Circuit Discovery in Transformers using Contextual Decomposition](https://openreview.net/forum?id=41HlN8XYM5) |  | 0 | Automated mechanistic interpretation research has attracted great interest due to its potential to scale explanations of neural network internals to large models. Existing automated circuit discovery work relies on activation patching or its approximations to identify subgraphs in models for specific tasks (circuits). They often suffer from slow runtime, approximation errors, and specific requirements of metrics, such as non-zero gradients. In this work, we introduce contextual decomposition for transformers (CD-T) to build interpretable circuits in large language models. CD-T can produce circuits at any level of abstraction and is the first to efficiently produce circuits as fine-grained as attention heads at specific sequence positions. CD-T is compatible to all transformer types, and requires no training or manually-crafted examples. CD-T consists of a set of mathematical equations to isolate contribution of model features. Through recursively computing contribution of all nodes in a computational graph of a model using CD-T followed by pruning, we are able to reduce circuit discovery runtime from hours to seconds compared to state-of-the-art baselines. On three standard circuit evaluation datasets (indirect object identification, greater-than comparisons, and docstring completion), we demonstrate that CD-T outperforms ACDC and EAP by better recovering the manual circuits with an average of 97% ROC AUC under low runtimes. In addition, we provide evidence that faithfulness of CD-T circuits is not due to random chance by showing our circuits are 80% more faithful than random circuits of up to 60% of the original model size. Finally, we show CD-T circuits are able to perfectly replicate original models' behavior(faithfulness = 1) using fewer nodes than the baselines for all tasks. Our results underscore the great promise of CD-T for efficient automated mechanistic interpretability, paving the way for new insights into the workings of large language models. | Aliyah R. Hsu, Georgia Zhou, Yeshwanth Cherapanamjeri, Yaxuan Huang, Anobel Y. Odisho, Peter R. Carroll, Bin Yu |  |
| 682 |  |  [How to Evaluate Reward Models for RLHF](https://openreview.net/forum?id=cbttLtO94Q) |  | 0 | We introduce a new benchmark for reward models that quantifies their ability to produce strong language models through RLHF (Reinforcement Learning from Human Feedback). The gold-standard approach is to run a full RLHF training pipeline and directly probe downstream LLM performance. However, this process is prohibitively expensive. To address this, we build a predictive model of downstream LLM performance by evaluating the reward model on proxy tasks. These proxy tasks consist of a large-scale human preference and a verifiable correctness preference dataset, in which we measure 12 metrics across 12 domains. To investigate which reward model metrics are most correlated to gold-standard RLHF outcomes, we launch an end-to-end RLHF experiment on a large-scale crowd-sourced human preference platform to view real reward model downstream performance as ground truth. Ultimately, we compile our data and findings into Preference Proxy Evaluations (PPE), the first reward model benchmark explicitly linked to post-RLHF real-world human preference performance, which we opensource for public use and further development at https://github.com/lmarena/PPE. | Evan Frick, Tianle Li, Connor Chen, WeiLin Chiang, Anastasios Nikolas Angelopoulos, Jiantao Jiao, Banghua Zhu, Joseph E. Gonzalez, Ion Stoica |  |
| 683 |  |  [An Efficient Framework for Crediting Data Contributors of Diffusion Models](https://openreview.net/forum?id=9EqQC2ct4H) |  | 0 | As diffusion models are deployed in real-world settings and their performance driven by training data, appraising the contribution of data contributors is crucial to creating incentives for sharing quality data and to implementing policies for data compensation. Depending on the use case, model performance corresponds to various global properties of the distribution learned by a diffusion model (e.g., overall aesthetic quality). Hence, here we address the problem of attributing global properties of diffusion models to data contributors. The Shapley value provides a principled approach to valuation by uniquely satisfying game-theoretic axioms of fairness. However, estimating Shapley values for diffusion models is computationally impractical because it requires retraining and rerunning inference on many subsets of data contributors. We introduce a method to efficiently retrain and rerun inference for Shapley value estimation, by leveraging model pruning and fine-tuning. We evaluate the utility of our method with three use cases: (i) image quality for a DDPM trained on a CIFAR dataset, (ii) demographic diversity for an LDM trained on CelebA-HQ, and (iii) aesthetic quality for a Stable Diffusion model LoRA-finetuned on Post-Impressionist artworks. Our results empirically demonstrate that our framework can identify important data contributors across global properties, outperforming existing attribution methods for diffusion models. | Mingyu Lu, Chris Lin, Chanwoo Kim, SuIn Lee |  |
| 684 |  |  [Decentralized Optimization with Coupled Constraints](https://openreview.net/forum?id=AJM52ygi6Y) |  | 0 | We consider the decentralized minimization of a separable objective $\sum_{i=1}^{n} f_i(x_i)$, where the variables are coupled through an affine constraint $\sum_{i=1}^n\left(\mathbf{A}_i x_i - b_i\right) = 0$. We assume that the functions $f_i$, matrices $\mathbf{A}_i$, and vectors $b_i$ are stored locally by the nodes of a computational network, and that the functions $f_i$ are smooth and strongly convex. This problem has significant applications in resource allocation and systems control and can also arise in distributed machine learning. We propose lower complexity bounds for decentralized optimization problems with coupled constraints and a first-order algorithm achieving the lower bounds. To the best of our knowledge, our method is also the first linearly convergent first-order decentralized algorithm for problems with general affine coupled constraints. | Demyan Yarmoshik, Alexander Rogozin, Nikita Kiselev, Daniil Dorin, Alexander V. Gasnikov, Dmitry Kovalev |  |
| 685 |  |  [You Only Prune Once: Designing Calibration-Free Model Compression With Policy Learning](https://openreview.net/forum?id=5RZoYIT3u6) |  | 0 | The ever-increasing size of large language models (LLMs) presents significant challenges for deployment due to their heavy computational and memory requirements. Current model pruning techniques attempt to alleviate these issues by relying heavily on external calibration datasets to determine which parameters to prune or compress, thus limiting their flexibility and scalability across different compression ratios. Moreover, these methods often cause severe performance degradation, particularly in downstream tasks, when subjected to higher compression rates. In this paper, we propose \*PruneNet\*, a novel model compression method that addresses these limitations by reformulating model pruning as a policy learning process. PruneNet decouples the pruning process from the model architecture, eliminating the need for calibration datasets. It learns a stochastic pruning policy to assess parameter importance solely based on intrinsic model properties while preserving the spectral structure to minimize information loss. PruneNet can compress the LLaMA-2-7B model in just 15 minutes, achieving over 80\% retention of its zero-shot performance with a 30\% compression ratio, outperforming existing methods that retain only 75\% performance. Furthermore, on complex multitask language understanding tasks, PruneNet demonstrates its robustness by preserving up to 80\% performance of the original model, proving itself a superior alternative to conventional structured compression techniques. | Ayan Sengupta, Siddhant Chaudhary, Tanmoy Chakraborty |  |
| 686 |  |  [FreDF: Learning to Forecast in the Frequency Domain](https://openreview.net/forum?id=4A9IdSa1ul) |  | 0 | Time series modeling presents unique challenges due to autocorrelation in both historical data and future sequences. While current research predominantly addresses autocorrelation within historical data, the correlations among future labels are often overlooked. Specifically, modern forecasting models primarily adhere to the Direct Forecast (DF) paradigm, generating multi-step forecasts independently and disregarding label correlations over time. In this work, we demonstrate that the learning objective of DF is biased in the presence of label correlation. To address this issue, we propose the Frequency-enhanced Direct Forecast (FreDF), which mitigates label correlation by learning to forecast in the frequency domain, thereby reducing estimation bias. Our experiments show that FreDF significantly outperforms existing state-of-the-art methods and is compatible with a variety of forecast models. Code is available at https://github.com/Master-PLC/FreDF. | Hao Wang, Lichen Pan, Yuan Shen, Zhichao Chen, Degui Yang, Yifei Yang, Sen Zhang, Xinggao Liu, Haoxuan Li, Dacheng Tao |  |
| 687 |  |  [SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration](https://openreview.net/forum?id=OL44KtasKc) |  | 0 | The transformer architecture predominates across various models. As the heart of the transformer, attention has a computational complexity of $O(N^2)$, compared to $O(N)$ for linear transformations. When handling large sequence lengths, attention becomes the primary time-consuming component. Although quantization has proven to be an effective method for accelerating model inference, existing quantization methods primarily focus on optimizing the linear layer. In response, we first analyze the feasibility of quantization in attention detailedly. Following that, we propose SageAttention, a highly efficient and accurate quantization method for attention. The OPS (operations per second) of our approach outperforms FlashAttention2 and xformers by about 2.1x and 2.7x, respectively. SageAttention also achieves superior accuracy performance over FlashAttention3. Comprehensive experiments confirm that our approach incurs almost no end-to-end metrics loss across diverse models—including those for large language processing, image generation, and video generation. The code is available at https://github.com/thu-ml/SageAttention. | Jintao Zhang, Jia wei, Pengle Zhang, Jun Zhu, Jianfei Chen |  |
| 688 |  |  [Neural Multi-Objective Combinatorial Optimization via Graph-Image Multimodal Fusion](https://openreview.net/forum?id=4sJ2FYE65U) |  | 0 | Existing neural multi-objective combinatorial optimization (MOCO) methods still exhibit an optimality gap since they fail to fully exploit the intrinsic features of problem instances. A significant factor contributing to this shortfall is their reliance solely on graph-modal information. To overcome this, we propose a novel graph-image multimodal fusion (GIMF) framework that enhances neural MOCO methods by integrating graph and image information of the problem instances. Our GIMF framework comprises three key components: (1) a constructed coordinate image to better represent the spatial structure of the problem instance, (2) a problem-size adaptive resolution strategy during the image construction process to improve the cross-size generalization of the model, and (3) a multimodal fusion mechanism with modality-specific bottlenecks to efficiently couple graph and image information. We demonstrate the versatility of our GIMF by implementing it with two state-of-the-art neural MOCO backbones. Experimental results on classic MOCO problems show that our GIMF significantly outperforms state-of-the-art neural MOCO methods and exhibits superior generalization capability. | Jinbiao Chen, Jiahai Wang, Zhiguang Cao, Yaoxin Wu |  |
| 689 |  |  [Have the VLMs Lost Confidence? A Study of Sycophancy in VLMs](https://openreview.net/forum?id=E2PFv7ad3p) |  | 0 | In the study of LLMs, sycophancy represents a prevalent hallucination that poses significant challenges to these models. Specifically, LLMs often fail to adhere to original correct responses, instead blindly agreeing with users' opinions, even when those opinions are incorrect or malicious. However, research on sycophancy in visual language models (VLMs) has been scarce. In this work, we extend the exploration of sycophancy from LLMs to VLMs, introducing the MM-SY benchmark to evaluate this phenomenon. We present evaluation results from multiple representative models, addressing the gap in sycophancy research for VLMs. To mitigate sycophancy, we propose a synthetic dataset for training and employ methods based on prompts, supervised fine-tuning, and DPO. Our experiments demonstrate that these methods effectively alleviate sycophancy in VLMs. Additionally, we probe VLMs to assess the semantic impact of sycophancy and analyze the attention distribution of visual tokens. Our findings indicate that the ability to prevent sycophancy is predominantly observed in higher layers of the model. The lack of attention to image knowledge in these higher layers may contribute to sycophancy, and enhancing image attention at high layers proves beneficial in mitigating this issue. | Shuo Li, Tao Ji, Xiaoran Fan, Linsheng Lu, Leyi Yang, Yuming Yang, Zhiheng Xi, Rui Zheng, Yuran Wang, Xiaohui Zhao, Tao Gui, Qi Zhang, Xuanjing Huang |  |
| 690 |  |  [Generative Adapter: Contextualizing Language Models in Parameters with A Single Forward Pass](https://openreview.net/forum?id=bc3sUsS6ck) |  | 0 | Large language models (LLMs) acquire substantial knowledge during pretraining but often need adaptation to new contexts, tasks, or domains, typically achieved through fine-tuning or prompting. However, fine-tuning incurs significant training costs, while prompting increases inference overhead. Inspired by fast weight memory, we introduce GenerativeAdapter, an effective and efficient adaptation method that encode test-time context into language model parameters with a single forward pass. GenerativeAdapter augments a frozen pretrained LM with a lightweight adapter generator, trained via self-supervised learning, to produce parameter-efficient adapters. Notably, our generator is general-purpose, i.e., one generator can adapt the corresponding base model for all langauge processing scenarios. We apply GenerativeAdapter to two pretrained LMs (Mistral-7B-Instruct and Llama2-7B-Chat) and evaluate the adapted models across knowledge acquisition from documents, learning from demonstrations, and personalization for users. In StreamingQA, our approach is effective in injecting knowledge into the LM's parameters, achieving a 63.5\% improvement in F1 score over the model with supervised fine-tuning (from $19.5$ to $31.5$) for contexts as long as 32K tokens. In the MetaICL in-context learning evaluation, our method achieves an average accuracy of $44.9$ across 26 tasks, outperforming the base model. On MSC, our method proves to be highly competitive in memorizing user information from conversations with a 4x reduction in computation and memory costs compared to prompting with full conversation history. Overall, GenerativeAdapter provides a viable solution for adapting large LMs to evolving information and providing tailored user experience, while reducing training and inference costs relative to traditional fine-tuning and prompting techniques. | Tong Chen, Hao Fang, Patrick Xia, Xiaodong Liu, Benjamin Van Durme, Luke Zettlemoyer, Jianfeng Gao, Hao Cheng |  |
| 691 |  |  [ICLR: In-Context Learning of Representations](https://openreview.net/forum?id=pXlmOmlHJZ) |  | 0 | Recent work demonstrates that structured patterns in pretraining data influence how representations of different concepts are organized in a large language model’s (LLM) internals, with such representations then driving downstream abilities. Given the open-ended nature of LLMs, e.g., their ability to in-context learn novel tasks, we ask whether models can flexibly alter their semantically grounded organization of concepts. Specifically, if we provide in-context exemplars wherein a concept plays a different role than what the pretraining data suggests, can models infer these novel semantics and reorganize representations in accordance with them? To answer this question, we define a toy “graph tracing” task wherein the nodes of the graph are referenced via concepts seen during training (e.g., apple, bird, etc.), and the connectivity of the graph is defined via some predefined structure (e.g., a square grid). Given exemplars that indicate traces of random walks on the graph, we analyze intermediate representations of the model and find that as the amount of context is scaled, there is a sudden re-organization of representations according to the graph’s structure. Further, we find that when reference concepts have correlations in their semantics (e.g., Monday, Tuesday, etc.), the context-specified graph structure is still present in the representations, but is unable to dominate the pretrained structure. To explain these results, we analogize our task to energy minimization for a predefined graph topology, which shows getting non-trivial performance on the task requires for the model to infer a connected component. Overall, our findings indicate context-size may be an underappreciated scaling axis that can flexibly re-organize model representations, unlocking novel capabilities. | Core Francisco Park, Andrew Lee, Ekdeep Singh Lubana, Yongyi Yang, Maya Okawa, Kento Nishi, Martin Wattenberg, Hidenori Tanaka |  |
| 692 |  |  [Ensembling Diffusion Models via Adaptive Feature Aggregation](https://openreview.net/forum?id=e32cI4r8Eo) |  | 0 | The success of the text-guided diffusion model has inspired the development and release of numerous powerful diffusion models within the open-source community. These models are typically fine-tuned on various expert datasets, showcasing diverse denoising capabilities. Leveraging multiple high-quality models to produce stronger generation ability is valuable, but has not been extensively studied. Existing methods primarily adopt parameter merging strategies to produce a new static model. However, they overlook the fact that the divergent denoising capabilities of the models may dynamically change across different states, such as when experiencing different prompts, initial noises, denoising steps, and spatial locations. In this paper, we propose a novel ensembling method, Adaptive Feature Aggregation (AFA), which dynamically adjusts the contributions of multiple models at the feature level according to various states (i.e., prompts, initial noises, denoising steps, and spatial locations), thereby keeping the advantages of multiple diffusion models, while suppressing their disadvantages. Specifically, we design a lightweight Spatial-Aware Block-Wise (SABW) feature aggregator that adaptive aggregates the block-wise intermediate features from multiple U-Net denoisers into a unified one. The core idea lies in dynamically producing an individual attention map for each model's features by comprehensively considering various states. It is worth noting that only SABW is trainable with about 50 million parameters, while other models are frozen. Both the quantitative and qualitative experiments demonstrate the effectiveness of our proposed method. | Cong Wang, Kuan Tian, Yonghang Guan, Fei Shen, Zhiwei Jiang, Qing Gu, Jun Zhang |  |
| 693 |  |  [Solving New Tasks by Adapting Internet Video Knowledge](https://openreview.net/forum?id=p01BR4njlY) |  | 0 | Video generative models demonstrate great promise in robotics by serving as visual planners or as policy supervisors. When pretrained on internet-scale data, such video models intimately understand alignment with natural language, and can thus facilitate generalization to novel downstream behavior through text-conditioning. However, they may not be sensitive to the specificities of the particular environment the agent inhabits. On the other hand, training video models on in-domain examples of robotic behavior naturally encodes environment-specific intricacies, but the scale of available demonstrations may not be sufficient to support generalization to unseen tasks via natural language specification. In this work, we investigate different adaptation techniques that integrate in-domain information with large-scale pretrained video models, and explore the extent to which they enable novel text-conditioned generalization for robotic tasks, while also considering their independent data and resource considerations. We successfully demonstrate across robotic environments that adapting powerful video models with small scales of example data can successfully facilitate generalization to novel behaviors. In particular, we present a novel adaptation strategy, termed \*Inverse Probabilistic Adaptation\*, that not only consistently achieves strong generalization performance across robotic tasks and settings, but also exhibits robustness to the quality of adaptation data, successfully solving novel tasks even when only suboptimal in-domain demonstrations are available. | Calvin Luo, Zilai Zeng, Yilun Du, Chen Sun |  |
| 694 |  |  [SoftMatcha: A Soft and Fast Pattern Matcher for Billion-Scale Corpus Searches](https://openreview.net/forum?id=Q6PAnqYVpo) |  | 0 | Researchers and practitioners in natural language processing and computational linguistics frequently observe and analyze the real language usage in large-scale corpora. For that purpose, they often employ off-the-shelf pattern-matching tools, such as grep, and keyword-in-context concordancers, which is widely used in corpus linguistics for gathering examples. Nonetheless, these existing techniques rely on surface-level string matching, and thus they suffer from the major limitation of not being able to handle orthographic variations and paraphrasing---notable and common phenomena in any natural language. In addition, existing continuous approaches such as dense vector search tend to be overly coarse, often retrieving texts that are unrelated but share similar topics. Given these challenges, we propose a novel algorithm that achieves soft (or semantic) yet efficient pattern matching by relaxing a surface-level matching with word embeddings. Our algorithm is highly scalable with respect to the size of the corpus text utilizing inverted indexes. We have prepared an efficient implementation, and we provide an accessible web tool. Our experiments demonstrate that the proposed method (i) can execute searches on billion-scale corpora in less than a second, which is comparable in speed to surface-level string matching and dense vector search; (ii) can extract harmful instances that semantically match queries from a large set of English and Japanese Wikipedia articles; and (iii) can be effectively applied to corpus-linguistic analyses of Latin, a language with highly diverse inflections. | Hiroyuki Deguchi, Go Kamoda, Yusuke Matsushita, Chihiro Taguchi, Kohei Suenaga, Masaki Waga, Sho Yokoi |  |
| 695 |  |  [Model merging with SVD to tie the Knots](https://openreview.net/forum?id=67X93aZHII) |  | 0 | Recent model merging methods demonstrate that the parameters of fully-finetuned models specializing in distinct tasks can be combined into one model capable of solving all tasks without retraining. Yet, this success does not transfer well when merging LoRA finetuned models. We study this phenomenon and observe that the weights of LoRA finetuned models showcase a lower degree of alignment compared to their fully-finetuned counterparts. We hypothesize that improving this alignment is key to obtaining better LoRA model merges, and propose KnOTS to address this problem. KnOTS uses the SVD to jointly transform the weights of different LoRA models into an aligned space, where existing merging methods can be applied. In addition, we introduce a new benchmark that explicitly evaluates whether merged models are general models. Notably, KnOTS consistently improves LoRA merging by up to 4.3% across several vision and language benchmarks, including our new setting. We release our code at: https://github.com/gstoica27/KnOTS. | George Stoica, Pratik Ramesh, Boglarka Ecsedi, Leshem Choshen, Judy Hoffman |  |
| 696 |  |  [PolyhedronNet: Representation Learning for Polyhedra with Surface-attributed Graph](https://openreview.net/forum?id=BpyHIrpUOL) |  | 0 | Ubiquitous geometric objects can be precisely and efficiently represented as polyhedra. The transformation of a polyhedron into a vector, known as polyhedra representation learning, is crucial for manipulating these shapes with mathematical and statistical tools for tasks like classification, clustering, and generation. Recent years have witnessed significant strides in this domain, yet most efforts focus on the vertex sequence of a polyhedron, neglecting the complex surface modeling crucial in real-world polyhedral objects. This study proposes \textbf{PolyhedronNet}, a general framework tailored for learning representations of 3D polyhedral objects. We propose the concept of the surface-attributed graph to seamlessly model the vertices, edges, faces, and their geometric interrelationships within a polyhedron. To effectively learn the representation of the entire surface-attributed graph, we first propose to break it down into local rigid representations to effectively learn each local region's relative positions against the remaining regions without geometric information loss. Subsequently, we propose PolyhedronGNN to hierarchically aggregate the local rigid representation via intra-face and inter-face geometric message passing modules, to obtain a global representation that minimizes information loss while maintaining rotation and translation invariance. Our experimental evaluations on four distinct datasets, encompassing both classification and retrieval tasks, substantiate PolyhedronNet's efficacy in capturing comprehensive and informative representations of 3D polyhedral objects. | Dazhou Yu, Genpei Zhang, Liang Zhao |  |
| 697 |  |  [SafeDiffuser: Safe Planning with Diffusion Probabilistic Models](https://openreview.net/forum?id=ig2wk7kK9J) |  | 0 | Diffusion models have shown promise in data-driven planning. While these planners are commonly employed in applications where decisions are critical, they still lack established safety guarantees. In this paper, we address this limitation by introducing SafeDiffuser, a method to equip diffusion models with safety guarantees via control barrier functions. The key idea of our approach is to embed finite-time diffusion invariance, i.e., a form of specification consisting of safety constraints, into the denoising diffusion procedure. This way we enable data generation under safety constraints. We show that SafeDiffusers maintain the generative performance of diffusion models while also providing robustness in safe data generation. We evaluate our method on a series of tasks, including maze path generation, legged robot locomotion, and 3D space manipulation, and demonstrate the advantages of robustness over vanilla diffusion models. | Wei Xiao, TsunHsuan Wang, Chuang Gan, Ramin M. Hasani, Mathias Lechner, Daniela Rus |  |
| 698 |  |  [BrainACTIV: Identifying visuo-semantic properties driving cortical selectivity using diffusion-based image manipulation](https://openreview.net/forum?id=CGON8Btleu) |  | 0 | The human brain efficiently represents visual inputs through specialized neural populations that selectively respond to specific categories. Advancements in generative modeling have enabled data-driven discovery of neural selectivity using brain-optimized image synthesis. However, current methods independently generate one sample at a time, without enforcing structural constraints on the generations; thus, these individual images have no explicit point of comparison, making it hard to discern which image features drive neural response selectivity. To address this issue, we introduce Brain Activation Control Through Image Variation (BrainACTIV), a method for manipulating a reference image to enhance or decrease activity in a target cortical region using pretrained diffusion models. Starting from a reference image allows for fine-grained and reliable offline identification of optimal visuo-semantic properties, as well as producing controlled stimuli for novel neuroimaging studies. We show that our manipulations effectively modulate predicted fMRI responses and agree with hypothesized preferred categories in established regions of interest, while remaining structurally close to the reference image. Moreover, we demonstrate how our method accentuates differences between brain regions that are selective to the same category, and how it could be used to explore neural representation of brain regions with unknown selectivities. Hence, BrainACTIV holds the potential to formulate robust hypotheses about brain representation and to facilitate the production of naturalistic stimuli for neuroscientific experiments. | Diego Garcia Cerdas, Christina Sartzetaki, Magnus Petersen, Gemma Roig, Pascal Mettes, Iris I. A. Groen |  |
| 699 |  |  [Efficient Online Reinforcement Learning Fine-Tuning Need Not Retain Offline Data](https://openreview.net/forum?id=HN0CYZbAPw) |  | 0 | The modern paradigm in machine learning involves pre-training on diverse data, followed by task-specific fine-tuning. In reinforcement learning (RL), this translates to learning via offline RL on a diverse historical dataset, followed by rapid online RL fine-tuning using interaction data. Most RL fine-tuning methods require continued training on offline data for stability and performance. However, this is undesirable because training on diverse offline data is slow and expensive for large datasets, and should, in principle, also limit the performance improvement possible because of constraints or pessimism on offline data. In this paper, we show that retaining offline data is unnecessary as long as we use a properly-designed online RL approach for fine-tuning offline RL initializations. To build this approach, we start by analyzing the role of retaining offline data in online fine-tuning. We find that continued training on offline data is mostly useful for preventing a sudden divergence in the value function at the onset of fine-tuning, caused by a distribution mismatch between the offline data and online rollouts. This divergence typically results in unlearning and forgetting the benefits of offline pre-training. Our approach, Warm-start RL (WSRL), mitigates the catastrophic forgetting of pre-trained initializations using a very simple idea. WSRL employs a warmup phase that seeds the online RL run with a very small number of rollouts from the pre-trained policy to do fast online RL. The data collected during warmup bridges the distribution mismatch, and helps \`\`recalibrate'' the offline Q-function to the online distribution, allowing us to completely discard offline data without destabilizing the online RL fine-tuning. We show that WSRL is able to fine-tune without retaining any offline data, and is able to learn faster and attains higher performance than existing algorithms irrespective of whether they do or do not retain offline data. | Zhiyuan Zhou, Andy Peng, Qiyang Li, Sergey Levine, Aviral Kumar |  |
| 700 |  |  [Making Text Embedders Few-Shot Learners](https://openreview.net/forum?id=wfLuiDjQ0u) |  | 0 | Large language models (LLMs) with decoder-only architectures have demonstrated exceptional text-generation capabilities across a variety of tasks. Some researchers have also adapted these models for text representation tasks. However, in text representation tasks, these models often face performance degradation on unseen tasks. In-context learning (ICL), which leverages examples provided in the input context, enables LLMs to handle unseen tasks effectively. Inspired by this, we aim to fully utilize the inherent properties of LLMs to enhance text representation performance across different tasks through the ICL approach. In this paper, we introduce a simple yet effective training strategy, which significantly improves text representation capabilities. Unlike previous models that prepend task instructions to the text, our method randomly samples a varying number of examples during training, endowing the embedding model with in-context learning abilities while maintaining its zero-shot capabilities. This approach does not require additional data construction or modifications to the model architecture. On the contrary, we find that some popular modifications to the model, such as bidirectional attention, can degrade performance, undermining the inherent characteristics of LLMs. We have publicly released our method at this \href{https://github.com/FlagOpen/FlagEmbedding}{repo}. | Chaofan Li, Minghao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Defu Lian, Yingxia Shao, Zheng Liu |  |
| 701 |  |  [Diverse Preference Learning for Capabilities and Alignment](https://openreview.net/forum?id=pOq9vDIYev) |  | 0 | As LLMs increasingly impact society, their ability to represent diverse perspectives is critical. However, recent studies reveal that alignment algorithms such as RLHF and DPO significantly reduce the diversity of LLM outputs. Not only do aligned LLMs generate text with repetitive structure and word choice, they also approach problems in more uniform ways, and their responses reflect a narrower range of societal perspectives. We attribute this problem to the KL divergence regularizer employed in preference learning algorithms. This causes the model to overweight majority opinions and sacrifice diversity in exchange for optimal reward. To address this, we propose Soft Preference Learning, which decouples the entropy and cross-entropy terms in the KL penalty — allowing for fine-grained control over LLM generation diversity. From a capabilities perspective, LLMs trained using Soft Preference Learning attain higher accuracy on difficult repeated sampling tasks and produce outputs with greater semantic and lexical diversity. From an alignment perspective, they are capable of representing a wider range of societal viewpoints and display improved logit calibration. Notably, Soft Preference Learning resembles, but is a Pareto improvement over, standard temperature scaling. | Stewart Slocum, Asher ParkerSartori, Dylan HadfieldMenell |  |
| 702 |  |  [Efficient Reinforcement Learning with Large Language Model Priors](https://openreview.net/forum?id=e2NRNQ0sZe) |  | 0 | In sequential decision-making (SDM) tasks, methods like reinforcement learning (RL) and heuristic search have made notable advances in specific cases. However, they often require extensive exploration and face challenges in generalizing across diverse environments due to their limited grasp of the underlying decision dynamics. In contrast, large language models (LLMs) have recently emerged as powerful general-purpose tools, due to their capacity to maintain vast amounts of domain-specific knowledge. To harness this rich prior knowledge for efficiently solving complex SDM tasks, we propose treating LLMs as prior action distributions and integrating them into RL frameworks through Bayesian inference methods, making use of variational inference and direct posterior sampling. The proposed approaches facilitate the seamless incorporation of fixed LLM priors into both policy-based and value-based RL frameworks. Our experiments show that incorporating LLM-based action priors significantly reduces exploration and optimization complexity, substantially improving sample efficiency compared to traditional RL techniques, e.g., using LLM priors decreases the number of required samples by over 90\% in offline learning scenarios. | Xue Yan, Yan Song, Xidong Feng, Mengyue Yang, Haifeng Zhang, Haitham BouAmmar, Jun Wang |  |
| 703 |  |  [Geometry-Aware Approaches for Balancing Performance and Theoretical Guarantees in Linear Bandits](https://openreview.net/forum?id=Oeb0I3JcVc) |  | 0 | This paper is motivated by recent research in the $d$-dimensional stochastic linear bandit literature, which has revealed an unsettling discrepancy: algorithms like Thompson sampling and Greedy demonstrate promising empirical performance, yet this contrasts with their pessimistic theoretical regret bounds. The challenge arises from the fact that while these algorithms may perform poorly in certain problem instances, they generally excel in typical instances. To address this, we propose a new data-driven technique that tracks the geometric properties of the uncertainty ellipsoid around the main problem parameter. This methodology enables us to formulate a data-driven frequentist regret bound, which incorporates the geometric information, for a broad class of base algorithms, including Greedy, OFUL, and Thompson sampling. This result allows us to identify and \`\`course-correct" problem instances in which the base algorithms perform poorly. The course-corrected algorithms achieve the minimax optimal regret of order $\tilde{\mathcal{O}}(d\sqrt{T})$ for a $T$-period decision-making scenario, effectively maintaining the desirable attributes of the base algorithms, including their empirical efficacy. We present simulation results to validate our findings using synthetic and real data. | Yuwei Luo, Mohsen Bayati |  |
| 704 |  |  [Langevin Soft Actor-Critic: Efficient Exploration through Uncertainty-Driven Critic Learning](https://openreview.net/forum?id=FvQsk3la17) |  | 0 | Existing actor-critic algorithms, which are popular for continuous control reinforcement learning (RL) tasks, suffer from poor sample efficiency due to lack of principled exploration mechanism within them. Motivated by the success of Thompson sampling for efficient exploration in RL, we propose a novel model-free RL algorithm, \emph{Langevin Soft Actor Critic} (LSAC), which prioritizes enhancing critic learning through uncertainty estimation over policy optimization. LSAC employs three key innovations: approximate Thompson sampling through distributional Langevin Monte Carlo (LMC) based $Q$ updates, parallel tempering for exploring multiple modes of the posterior of the $Q$ function, and diffusion synthesized state-action samples regularized with $Q$ action gradients. Our extensive experiments demonstrate that LSAC outperforms or matches the performance of mainstream model-free RL algorithms for continuous control tasks. Notably, LSAC marks the first successful application of an LMC based Thompson sampling in continuous control tasks with continuous action spaces. | Haque Ishfaq, Guangyuan Wang, Sami Nur Islam, Doina Precup |  |
| 705 |  |  [Searching for Optimal Solutions with LLMs via Bayesian Optimization](https://openreview.net/forum?id=aVfDrl7xDV) |  | 0 | Scaling test-time compute to search for optimal solutions is an important step towards building generally-capable language models that can reason. Recent work, however, shows that tasks of varying complexity require distinct search strategies to solve optimally, thus making it challenging to design a one-size-fits-all approach. Prior solutions either attempt to predict task difficulty to select the optimal search strategy, often infeasible in practice, or use a static, pre-defined strategy, e.g., repeated parallel sampling or greedy sequential search, which is sub-optimal. In this work, we argue for an alternative view using the probabilistic framework of Bayesian optimization (BO), where the search strategy is adapted dynamically based on the evolving uncertainty estimates of solutions as search progresses. To this end, we introduce Bayesian-OPRO (BOPRO)––a generalization of a recent method for in-context optimization, which iteratively samples from new proposal distributions by modifying the prompt to the LLM with a subset of its previous generations selected to explore or exploit different parts of the search space. We evaluate our method on word search, molecule optimization, and a joint hypothesis+program search task using a 1-D version of the challenging Abstraction and Reasoning Corpus (1D-ARC). Our results show that BOPRO outperforms all baselines in word search (≥10 points) and molecule optimization (higher quality and 17% fewer invalid molecules), but trails a best-k prompting strategy in program search. Our analysis reveals that despite the ability to balance exploration and exploitation using BOPRO, failure is likely due to the inability of code representation models in distinguishing sequences with low edit-distances. | Dhruv Agarwal, Manoj Ghuhan Arivazhagan, Rajarshi Das, Sandesh Swamy, Sopan Khosla, Rashmi Gangadharaiah |  |
| 706 |  |  [Context Steering: Controllable Personalization at Inference Time](https://openreview.net/forum?id=xQCXInDq0m) |  | 0 | To deliver high-quality, personalized responses, large language models (LLMs) must effectively incorporate context — personal, demographic, and cultural information specific to an end-user. For example, asking the model to explain Newton's second law with the context "I am a toddler'' should produce a response different from when the context is "I am a physics professor''. However, leveraging the context in practice is a nuanced and challenging task, and is often dependent on the specific situation or user base. The model must strike a balance between providing specific, personalized responses and maintaining general applicability. Current solutions, such as prompt-engineering and fine-tuning, require collection of contextually appropriate responses as examples, making them time-consuming and less flexible to use across different contexts. In this work, we introduce Context Steering (CoS) —a simple, training-free decoding approach that amplifies the influence of the context in next token predictions. CoS computes contextual influence by comparing the output probabilities from two LLM forward passes: one that includes the context and one that does not. By linearly scaling the contextual influence, CoS allows practitioners to flexibly control the degree of personalization for different use cases. We show that CoS can be applied to autoregressive LLMs, and demonstrates strong performance in personalized recommendations. Additionally, we show that CoS can function as a Bayesian Generative model to infer and quantify correlations between open-ended texts, broadening its potential applications. | Jerry ZhiYang He, Sashrika Pandey, Mariah L. Schrum, Anca D. Dragan |  |
| 707 |  |  [Efficient Policy Evaluation with Safety Constraint for Reinforcement Learning](https://openreview.net/forum?id=Dem5LyVk8R) |  | 0 | In reinforcement learning, classic on-policy evaluation methods often suffer from high variance and require massive online data to attain the desired accuracy. Previous studies attempt to reduce evaluation variance by searching for or designing proper behavior policies to collect data. However, these approaches ignore the safety of such behavior policies---the designed behavior policies have no safety guarantee and may lead to severe damage during online executions. In this paper, to address the challenge of reducing variance while ensuring safety simultaneously, we propose an optimal variance-minimizing behavior policy under safety constraints. Theoretically, while ensuring safety constraints, our evaluation method is unbiased and has lower variance than on-policy evaluation. Empirically, our method is the only existing method to achieve both substantial variance reduction and safety constraint satisfaction. Furthermore, we show our method is even superior to previous methods in both variance reduction and execution safety. | Claire Chen, Shuze Daniel Liu, Shangtong Zhang |  |
| 708 |  |  [Agent S: An Open Agentic Framework that Uses Computers Like a Human](https://openreview.net/forum?id=lIVRgt4nLv) |  | 0 | We present Agent S, an open agentic framework that enables autonomous interaction with computers through Graphical User Interface (GUI), aimed at transforming human-computer interaction by automating complex, multi-step tasks. Agent S addresses three key challenges in automating computer tasks: acquiring domain-specific knowledge, planning over long task horizons, and handling dynamic, non-uniform interfaces. To this end, Agent S introduces experience-augmented hierarchical planning, which learns from external knowledge search and internal experience retrieval at multiple levels, facilitating efficient task planning and subtask execution. In addition, it employs an Agent-Computer Interface (ACI) to better elicit the reasoning and control capabilities of GUI agents based on Multimodal Large Language Models (MLLMs). Evaluation on the OSWorld benchmark shows that Agent S outperforms the baseline by 9.37\% on success rate (an 83.6\% relative improvement) and achieves a new state-of-the-art. Comprehensive analysis highlights the effectiveness of individual components and provides insights for future improvements. Furthermore, Agent S demonstrates broad generalizability to different operating systems on a newly-released WindowsAgentArena benchmark. Code available at https://github.com/simular-ai/Agent-S. | Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, Xin Eric Wang |  |
| 709 |  |  [Semantic Loss Guided Data Efficient Supervised Fine Tuning for Safe Responses in LLMs](https://openreview.net/forum?id=kO0DgO07hW) |  | 0 | Large Language Models (LLMs) generating unsafe responses to toxic prompts is a significant issue in their applications. While various efforts aim to address this safety concern, previous approaches often demand substantial human data collection or rely on the less dependable option of using another LLM to generate corrective data. In this paper, we aim to take this problem and overcome limitations of requiring significant high-quality human data. Our method requires only a small set of unsafe responses to toxic prompts, easily obtained from the unsafe LLM itself. By employing a semantic cost combined with a negative Earth Mover Distance (EMD) loss, we guide the LLM away from generating unsafe responses. Additionally, we propose a novel lower bound for EMD loss, enabling more efficient optimization. Our results demonstrate superior performance and data efficiency compared to baselines, and we further examine the nuanced effects of over-alignment and potential degradation of language capabilities when using contrastive data. | Yuxiao Lu, Arunesh Sinha, Pradeep Varakantham |  |
| 710 |  |  [Adversarial Generative Flow Network for Solving Vehicle Routing Problems](https://openreview.net/forum?id=tBom4xOW1H) |  | 0 | Recent research into solving vehicle routing problems (VRPs) has gained significant traction, particularly through the application of deep (reinforcement) learning for end-to-end solution construction. However, many current construction-based neural solvers predominantly utilize Transformer architectures, which can face scalability challenges and struggle to produce diverse solutions. To address these limitations, we introduce a novel framework beyond Transformer-based approaches, i.e., Adversarial Generative Flow Networks (AGFN). This framework integrates the generative flow network (GFlowNet)—a probabilistic model inherently adept at generating diverse solutions (routes)—with a complementary model for discriminating (or evaluating) the solutions. These models are trained alternately in an adversarial manner to improve the overall solution quality, followed by a proposed hybrid decoding method to construct the solution. We apply the AGFN framework to solve the capacitated vehicle routing problem (CVRP) and travelling salesman problem (TSP), and our experimental results demonstrate that AGFN surpasses the popular construction-based neural solvers, showcasing strong generalization capabilities on synthetic and real-world benchmark instances. | Ni Zhang, Jingfeng Yang, Zhiguang Cao, Xu Chi |  |
| 711 |  |  [Tracking the Copyright of Large Vision-Language Models through Parameter Learning Adversarial Images](https://openreview.net/forum?id=K7xpl3LZQp) |  | 0 | Large vision-language models (LVLMs) have demonstrated remarkable image understanding and dialogue capabilities, allowing them to handle a variety of visual question answering tasks. However, their widespread availability raises concerns about unauthorized usage and copyright infringement, where users or individuals can develop their own LVLMs by fine-tuning published models. In this paper, we propose a novel method called Parameter Learning Attack (PLA) for tracking the copyright of LVLMs without modifying the original model. Specifically, we construct adversarial images through targeted attacks against the original model, enabling it to generate specific outputs. To ensure these attacks remain effective on potential fine-tuned models to trigger copyright tracking, we allow the original model to learn the trigger images by updating parameters in the opposite direction during the adversarial attack process. Notably, the proposed method can be applied after the release of the original model, thus not affecting the model’s performance and behavior. To simulate real-world applications, we fine-tune the original model using various strategies across diverse datasets, creating a range of models for copyright verification. Extensive experiments demonstrate that our method can more effectively identify the original copyright of fine-tuned models compared to baseline methods. Therefore, this work provides a powerful tool for tracking copyrights and detecting unlicensed usage of LVLMs. | Yubo Wang, Jianting Tang, Chaohu Liu, Linli Xu |  |
| 712 |  |  [Self-Supervised Diffusion MRI Denoising via Iterative and Stable Refinement](https://openreview.net/forum?id=wxPnuFp8fZ) |  | 0 | Magnetic Resonance Imaging (MRI), including diffusion MRI (dMRI), serves as a \`\`microscope'' for anatomical structures and routinely mitigates the influence of low signal-to-noise ratio scans by compromising temporal or spatial resolution. However, these compromises fail to meet clinical demands for both efficiency and precision. Consequently, denoising is a vital preprocessing step, particularly for dMRI, where clean data is unavailable. In this paper, we introduce Di-Fusion, a fully self-supervised denoising method that leverages the latter diffusion steps and an adaptive sampling process. Unlike previous approaches, our single-stage framework achieves efficient and stable training without extra noise model training and offers adaptive and controllable results in the sampling process. Our thorough experiments on real and simulated data demonstrate that Di-Fusion achieves state-of-the-art performance in microstructure modeling, tractography tracking, and other downstream tasks. Code is available at https://github.com/FouierL/Di-Fusion. | Chenxu Wu, Qingpeng Kong, Zihang Jiang, S. Kevin Zhou |  |
| 713 |  |  [LongMamba: Enhancing Mamba's Long-Context Capabilities via Training-Free Receptive Field Enlargement](https://openreview.net/forum?id=fMbLszVO1H) |  | 0 | State space models (SSMs) have emerged as an efficient alternative to Transformer models for language modeling, offering linear computational complexity and constant memory usage as context length increases. However, despite their efficiency in handling long contexts, recent studies have shown that SSMs, such as Mamba models, generally underperform compared to Transformers in long-context understanding tasks. To address this significant shortfall and achieve both efficient and accurate long-context understanding, we propose LongMamba, a training-free technique that significantly enhances the long-context capabilities of Mamba models. LongMamba builds on our discovery that the hidden channels in Mamba can be categorized into local and global channels based on their receptive field lengths, with global channels primarily responsible for long-context capability. These global channels can become the key bottleneck as the input context lengthens. Specifically, when input lengths largely exceed the training sequence length, global channels exhibit limitations in adaptively extend their receptive fields, leading to Mamba’s poor long-context performance. The key idea of LongMamba is to mitigate the hidden state memory decay in these global channels by preventing the accumulation of unimportant tokens in their memory. This is achieved by first identifying critical tokens in the global channels and then applying token filtering to accumulate only those critical tokens. Through extensive benchmarking across synthetic and real-world long-context scenarios, LongMamba sets a new standard for Mamba’s long-context performance, significantly extending its operational range without requiring additional training. Our code is available at https://github.com/GATECH-EIC/LongMamba. | Zhifan Ye, Kejing Xia, Yonggan Fu, Xin Dong, Jihoon Hong, Xiangchi Yuan, Shizhe Diao, Jan Kautz, Pavlo Molchanov, Yingyan Celine Lin |  |
| 714 |  |  [Robust Weight Initialization for Tanh Neural Networks with Fixed Point Analysis](https://openreview.net/forum?id=Es4RPNDtmq) |  | 0 | As a neural network's depth increases, it can improve generalization performance. However, training deep networks is challenging due to gradient and signal propagation issues. To address these challenges, extensive theoretical research and various methods have been introduced. Despite these advances, effective weight initialization methods for tanh neural networks remain insufficiently investigated. This paper presents a novel weight initialization method for neural networks with tanh activation function. Based on an analysis of the fixed points of the function $\tanh(ax)$, the proposed method aims to determine values of $a$ that mitigate activation saturation. A series of experiments on various classification datasets and physics-informed neural networks demonstrates that the proposed method outperforms Xavier initialization methods (with or without normalization) in terms of robustness across different network sizes, data efficiency, and convergence speed. Code is available at https://github.com/1HyunwooLee/Tanh-Init. | Hyunwoo Lee, Hayoung Choi, Hyunju Kim |  |
| 715 |  |  [Stiefel Flow Matching for Moment-Constrained Structure Elucidation](https://openreview.net/forum?id=84WmbzikPP) |  | 0 | Molecular structure elucidation is a fundamental step in understanding chemical phenomena, with applications in identifying molecules in natural products, lab syntheses, forensic samples, and the interstellar medium. We consider the task of predicting a molecule's all-atom 3D structure given only its molecular formula and moments of inertia, motivated by the ability of rotational spectroscopy to measure these moments. While existing generative models can conditionally sample 3D structures with approximately correct moments, this soft conditioning fails to leverage the many digits of precision afforded by experimental rotational spectroscopy. To address this, we first show that the space of $n$-atom point clouds with a fixed set of moments of inertia is embedded in the Stiefel manifold $\mathrm{St}(n, 4)$. We then propose Stiefel Flow Matching as a generative model for elucidating 3D structure under exact moment constraints. Additionally, we learn simpler and shorter flows by finding approximate solutions for equivariant optimal transport on the Stiefel manifold. Empirically, enforcing exact moment constraints allows Stiefel Flow Matching to achieve higher success rates and faster sampling than Euclidean diffusion models, even on high-dimensional manifolds corresponding to large molecules in the GEOM dataset. | Austin Henry Cheng, Alston Lo, Kin Long Kelvin Lee, Santiago Miret, Alán AspuruGuzik |  |
| 716 |  |  [Lawma: The Power of Specialization for Legal Annotation](https://openreview.net/forum?id=7El7K1DoyX) |  | 0 | Annotation and classification of legal text are central components of empirical legal research. Traditionally, these tasks are often delegated to trained research assistants. Motivated by the advances in language modeling, empirical legal scholars are increasingly turning to commercial models, hoping that it will alleviate the significant cost of human annotation. In this work, we present a comprehensive analysis of large language models’ current abilities to perform legal annotation tasks. To do so, we construct CaselawQA, a benchmark comprising 260 legal text classification tasks, nearly all new to the machine learning community. We demonstrate that commercial models, such as GPT-4.5 and Claude 3.7 Sonnet, achieve non-trivial accuracy but generally fall short of the performance required for legal work. We then demonstrate that small, lightly fine-tuned models vastly outperform commercial models. A few dozen to a few hundred labeled examples are usually enough to achieve higher accuracy. Our work points to a viable alternative to the predominant practice of prompting commercial models. For concrete legal annotation tasks with some available labeled data, researchers are likely better off using a fine-tuned open-source model. Code, datasets, and fine-tuned models are available at https://github.com/socialfoundations/lawma. | Ricardo DominguezOlmedo, Vedant Nanda, Rediet Abebe, Stefan Bechtold, Christoph Engel, Jens Frankenreiter, Krishna P. Gummadi, Moritz Hardt, Michael Livermore |  |
| 717 |  |  [OpenRCA: Can Large Language Models Locate the Root Cause of Software Failures?](https://openreview.net/forum?id=M4qNIzQYpd) |  | 0 | Large language models (LLMs) are driving substantial advancements in software engineering, with successful applications like Copilot and Cursor transforming real-world development practices. However, current research predominantly focuses on the early stages of development, such as code generation, while overlooking the post-development phases that are crucial to user experience. To explore the potential of LLMs in this direction, we propose OpenRCA, a benchmark dataset and evaluation framework for assessing LLMs’ ability to identify the root cause of software failures. OpenRCA includes 335 failures from three enterprise software systems, along with over 68 GB of telemetry data (logs, metrics, and traces). Given a failure case and its associated telemetry, the LLM is tasked to identify the root cause that triggered the failure, requiring comprehension of software dependencies and reasoning over heterogeneous, long-context telemetry data. Our results show substantial room for improvement, as current models can only handle the simplest cases. Even with the specially designed RCA-agent, the best-performing model, Claude 3.5, solved only 11.34% failure cases. Our work paves the way for future research in this direction. | Junjielong Xu, Qinan Zhang, Zhiqing Zhong, Shilin He, Chaoyun Zhang, Qingwei Lin, Dan Pei, Pinjia He, Dongmei Zhang, Qi Zhang |  |
| 718 |  |  [Wavelet-based Positional Representation for Long Context](https://openreview.net/forum?id=OhauMUNW8T) |  | 0 | In the realm of large-scale language models, a significant challenge arises when extrapolating sequences beyond the maximum allowable length. This is because the model's position embedding mechanisms are limited to positions encountered during training, thus preventing effective representation of positions in longer sequences. We analyzed conventional position encoding methods for long contexts and found the following characteristics. (1) When the representation dimension is regarded as the time axis, Rotary Position Embedding (RoPE) can be interpreted as a restricted wavelet transform using Haar-like wavelets. However, because it uses only a fixed scale parameter, it does not fully exploit the advantages of wavelet transforms, which capture the fine movements of non-stationary signals using multiple scales (window sizes). This limitation could explain why RoPE performs poorly in extrapolation. (2) Previous research as well as our own analysis indicates that Attention with Linear Biases (ALiBi) functions similarly to windowed attention, using windows of varying sizes. However, it has limitations in capturing deep dependencies because it restricts the receptive field of the model. From these insights, we propose a new position representation method that captures multiple scales (i.e., window sizes) by leveraging wavelet transforms without limiting the model's attention field. Experimental results show that this new method improves the performance of the model in both short and long contexts. In particular, our method allows extrapolation of position information without limiting the model's attention field. | Yui Oka, Taku Hasegawa, Kyosuke Nishida, Kuniko Saito |  |
| 719 |  |  [LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code](https://openreview.net/forum?id=chfJJYC3iL) |  | 0 | Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEvla, MBPP) are no longer sufficient for assessing their capabilities suffering from data contamination, overfitting, saturation, and focus on merely code generation. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which collects new problems over time from contests across three competition platforms, Leetcode, Atcoder, and Codeforces. Notably, our benchmark also focuses on a broader range of code-related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts over six hundred coding problems that were published between May 2023 and Aug 2024. We evaluate over 50 LLMs on LiveCodeBench (LCB for brevity) presenting the largest evaluation study of code LLMs on competition problems. Based on the study, we present novel empirical findings on contamination, overfitting, and holistic evaluations. We demonstrate that time-segmented evaluations serve as a robust approach to evade contamination; they are successful at detecting contamination across a wide range of open and closed models including GPT-4O, Claude, Deepseek, and Codestral. Next, we highlight overfitting and saturation of traditional coding benchmarks like HumanEvla and demonstrate LCB allows more reliable evaluations. Finally, our holistic evaluation scenarios allow for measuring the different capabilities of programming agents in isolation. | Naman Jain, King Han, Alex Gu, WenDing Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, Ion Stoica |  |
| 720 |  |  [ActionReasoningBench: Reasoning about Actions with and without Ramification Constraints](https://openreview.net/forum?id=NUD03NBDOE) |  | 0 | Reasoning about Actions and Change (RAC) has historically played a pivotal role in solving foundational AI problems, such as the frame problem. It has driven advancements in AI fields, such as non-monotonic and commonsense reasoning. RAC remains crucial for AI systems that operate in dynamic environments, engage in interactive scenarios, or rely on commonsense reasoning. Despite substantial advances made by Large Language Models (LLMs) in various AI domains, their performance in RAC remains underexplored. To address this gap, we introduce a new diagnostic benchmark, $\textbf{ActionReasoningBench}$, which encompasses 8 domains and includes questions for up to 19 action sequences. This benchmark rigorously evaluates LLMs across six key RAC dimensions: $\textit{Fluent Tracking}$, $\textit{State Tracking}$, $\textit{Action Executability}$, $\textit{Effects of Actions}$, $\textit{Numerical RAC}$, and $\textit{Composite Questions}$. LLMs demonstrate average accuracy rates of 73.55%, 65.63%, 58.73%, and 62.38% on the former four dimensions, which are frequently discussed in RAC literature. However, the performance on the latter two dimensions, which introduce complex and novel reasoning questions, the average performance of LLMs is lowered to 33.16% and 51.19%, respectively, reflecting a 17.9% performance decline. We also introduce new ramification constraints to capture the indirect effects of actions, providing deeper insights into RAC challenges. Our evaluation of state-of-the-art LLMs, including both open-source and commercial models, reveals challenges across all RAC dimensions, particularly in handling ramifications, with GPT-4o failing to solve any question and o1-preview achieving a score of only 18.4%. | Divij Handa, Pavel Dolin, Shrinidhi Kumbhar, Tran Cao Son, Chitta Baral |  |
| 721 |  |  [No Preference Left Behind: Group Distributional Preference Optimization](https://openreview.net/forum?id=bgpNJBD6Va) |  | 0 | Preferences within a group of people are not uniform but follow a distribution. While existing alignment methods like Direct Preference Optimization (DPO) attempt to steer models to reflect human preferences, they struggle to capture the distributional pluralistic preferences within a group. These methods often skew toward dominant preferences, overlooking the diversity of opinions, especially when conflicting preferences arise. To address this issue, we propose Group Distributional Preference Optimization (GDPO), a novel framework that aligns language models with the distribution of preferences within a group by incorporating the concept of beliefs that shape individual preferences. GDPO calibrates a language model using statistical estimation of the group's belief distribution and aligns the model with belief-conditioned preferences, offering a more inclusive alignment framework than traditional methods. In experiments using both synthetic controllable opinion generation and real-world movie review datasets, we show that DPO fails to align with the targeted belief distributions, while GDPO consistently reduces this alignment gap during training. Additionally, our evaluation metrics demonstrate that GDPO outperforms existing approaches in aligning with group distributional preferences, marking a significant advance in pluralistic alignment. | Binwei Yao, Zefan Cai, YunShiuan Chuang, Shanglin Yang, Ming Jiang, Diyi Yang, Junjie Hu |  |
| 722 |  |  [MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding](https://openreview.net/forum?id=CS2JWaziYr) |  | 0 | Large Language Models (LLMs) have become more prevalent in long-context applications such as interactive chatbots, document analysis, and agent workflows, but it is challenging to serve long-context requests with low latency and high throughput. Speculative decoding (SD) is a widely used technique to reduce latency losslessly, but the conventional wisdom suggests that its efficacy is limited to small batch sizes. In MagicDec, we show that surprisingly SD can achieve speedup even for a high throughput inference regime for moderate to long sequences. More interestingly, an intelligent drafting strategy can achieve better speedup with increasing batch size based on our rigorous analysis. MagicDec first identifies the bottleneck shifts with increasing batch size and sequence length, and uses these insights to deploy SD more effectively for high throughput inference. We leverage draft model with sparse KV cache to address the KV bottleneck, which scales with both sequence length and batch size. Additionally, we propose a theoretical model to select the optimal drafting strategy for maximum speedup. Our work highlights the broad applicability of speculative decoding in long-context serving, as it can enhance throughput and reduce latency without compromising accuracy. For moderate to long sequences, we demonstrate up to 2.51x speedup for LLaMA-3.1-8B when serving batch sizes ranging from 32 to 256 on various types of hardware and tasks. | Ranajoy Sadhukhan, Jian Chen, Zhuoming Chen, Vashisth Tiwari, Ruihang Lai, Jinyuan Shi, Ian EnHsu Yen, Avner May, Tianqi Chen, Beidi Chen |  |
| 723 |  |  [Parameter Expanded Stochastic Gradient Markov Chain Monte Carlo](https://openreview.net/forum?id=exgLs4snap) |  | 0 | Bayesian Neural Networks (BNNs) provide a promising framework for modeling predictive uncertainty and enhancing out-of-distribution robustness (OOD) by estimating the posterior distribution of network parameters. Stochastic Gradient Markov Chain Monte Carlo (SGMCMC) is one of the most powerful methods for scalable posterior sampling in BNNs, achieving efficiency by combining stochastic gradient descent with second-order Langevin dynamics. However, SGMCMC often suffers from limited sample diversity in practice, which affects uncertainty estimation and model performance. We propose a simple yet effective approach to enhance sample diversity in SGMCMC without the need for tempering or running multiple chains. Our approach reparameterizes the neural network by decomposing each of its weight matrices into a product of matrices, resulting in a sampling trajectory that better explores the target parameter space. This approach produces a more diverse set of samples, allowing faster mixing within the same computational budget. Notably, our sampler achieves these improvements without increasing the inference cost compared to the standard SGMCMC. Extensive experiments on image classification tasks, including OOD robustness, diversity, loss surface analyses, and a comparative study with Hamiltonian Monte Carlo, demonstrate the superiority of the proposed approach. | Hyunsu Kim, Giung Nam, Chulhee Yun, Hongseok Yang, Juho Lee |  |
| 724 |  |  [Hyperbolic Genome Embeddings](https://openreview.net/forum?id=NkGDNM8LB0) |  | 0 | Current approaches to genomic sequence modeling often struggle to align the inductive biases of machine learning models with the evolutionarily-informed structure of biological systems. To this end, we formulate a novel application of hyperbolic CNNs that exploits this structure, enabling more expressive DNA sequence representations. Our strategy circumvents the need for explicit phylogenetic mapping while discerning key properties of sequences pertaining to core functional and regulatory behavior. Across 37 out of 42 genome interpretation benchmark datasets, our hyperbolic models outperform their Euclidean equivalents. Notably, our approach even surpasses state-of-the-art performance on seven GUE benchmark datasets, consistently outperforming many DNA language models while using orders of magnitude fewer parameters and avoiding pretraining. Our results include a novel set of benchmark datasets---the Transposable Elements Benchmark---which explores a major but understudied component of the genome with deep evolutionary significance. We further motivate our work by exploring how our hyperbolic models recognize genomic signal under various data-generating conditions and by constructing an empirical method for interpreting the hyperbolicity of dataset embeddings. Throughout these assessments, we find persistent evidence highlighting the potential of our hyperbolic framework as a robust paradigm for genome representation learning. Our code and benchmark datasets are available at https://github.com/rrkhan/HGE. | Raiyan R. Khan, Philippe Chlenski, Itsik Pe'er |  |
| 725 |  |  [Deep Distributed Optimization for Large-Scale Quadratic Programming](https://openreview.net/forum?id=hzuumhfYSO) |  | 0 | Quadratic programming (QP) forms a crucial foundation in optimization, appearing in a broad spectrum of domains and serving as the basis for more advanced algorithms. Consequently, as the scale and complexity of modern applications continue to grow, the development of efficient and reliable QP algorithms becomes increasingly vital. In this context, this paper introduces a novel deep learning-aided distributed optimization architecture designed for tackling large-scale QP problems. First, we combine the state-of-the-art Operator Splitting QP (OSQP) method with a consensus approach to derive \*\*DistributedQP\*\*, a new method tailored for network-structured problems, with convergence guarantees to optimality. Subsequently, we unfold this optimizer into a deep learning framework, leading to \*\*DeepDistributedQP\*\*, which leverages learned policies to accelerate reaching to desired accuracy within a restricted amount of iterations. Our approach is also theoretically grounded through Probably Approximately Correct (PAC)-Bayes theory, providing generalization bounds on the expected optimality gap for unseen problems. The proposed framework, as well as its centralized version \*\*DeepQP\*\*, significantly outperform their standard optimization counterparts on a variety of tasks such as randomly generated problems, optimal control, linear regression, transportation networks and others. Notably, DeepDistributedQP demonstrates strong generalization by training on small problems and scaling to solve much larger ones (up to 50K variables and 150K constraints) using the same policy. Moreover, it achieves orders-of-magnitude improvements in wall-clock time compared to OSQP. The certifiable performance guarantees of our approach are also demonstrated, ensuring higher-quality solutions over traditional optimizers. | Augustinos D. Saravanos, Hunter Kuperman, Alex Oshin, Arshiya Taj Abdul, Vincent Pacelli, Evangelos A. Theodorou |  |
| 726 |  |  [Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining](https://openreview.net/forum?id=gU4ZgQNsOC) |  | 0 | Pretraining large language models (LLMs) on vast and heterogeneous datasets is crucial for achieving state-of-the-art performance across diverse downstream tasks. However, current training paradigms treat all samples equally, overlooking the importance or relevance of individual samples throughout the training process. Existing reweighting strategies, which primarily focus on group-level data importance, fail to leverage fine-grained instance-level information and do not adapt dynamically to individual sample importance as training progresses. In this paper, we introduce novel algorithms for dynamic, instance-level data reweighting aimed at improving both the efficiency and effectiveness of LLM pretraining. Our methods adjust the weight of each training sample based on its loss value in an online fashion, allowing the model to dynamically focus on more informative or important samples at the current training stage. In particular, our framework allows us to systematically devise reweighting strategies deprioritizing redundant or uninformative data, which we find tend to work best. Furthermore, we develop a new theoretical framework for analyzing the impact of loss-based reweighting on the convergence of gradient-based optimization, providing the first formal characterization of how these strategies affect convergence bounds. We empirically validate our approach across a spectrum of tasks, from pretraining 7B and 1.4B parameter LLMs to smaller-scale language models and linear regression problems, demonstrating that our loss-based reweighting approach can lead to faster convergence and significantly improved performance. | Daouda Sow, Herbert Woisetschläger, Saikiran Bulusu, Shiqiang Wang, HansArno Jacobsen, Yingbin Liang |  |
| 727 |  |  [Frame-Voyager: Learning to Query Frames for Video Large Language Models](https://openreview.net/forum?id=LNL7zKvm7e) |  | 0 | Video Large Language Models (Video-LLMs) have made remarkable progress in video understanding tasks. However, they are constrained by the maximum length of input tokens, making it impractical to input entire videos. Existing frame selection approaches, such as uniform frame sampling and text-frame retrieval, fail to account for the information density variations in the videos or the complex instructions in the tasks, leading to sub-optimal performance. In this paper, we propose Frame-Voyager that learns to query informative frame combinations, based on the given textual queries in the task. To train Frame-Voyager, we introduce a new data collection and labeling pipeline, by ranking frame combinations using a pre-trained Video-LLM. Given a video of M frames, we traverse its T-frame combinations, feed them into a Video-LLM, and rank them based on Video-LLM's prediction losses. Using this ranking as supervision, we train Frame-Voyager to query the frame combinations with lower losses. In experiments, we evaluate Frame-Voyager on four Video Question Answering benchmarks by plugging it into two different Video-LLMs. The experimental results demonstrate that Frame-Voyager achieves impressive results in all settings, highlighting its potential as a plug-and-play solution for Video-LLMs. | Sicheng Yu, Chengkai Jin, Huanyu Wang, Zhenghao Chen, Sheng Jin, Zhongrong Zuo, Xiaolei Xu, Zhenbang Sun, Bingni Zhang, Jiawei Wu, Hao Zhang, Qianru Sun |  |
| 728 |  |  [PN-GAIL: Leveraging Non-optimal Information from Imperfect Demonstrations](https://openreview.net/forum?id=0e2pcSxQJS) |  | 0 | Imitation learning aims at constructing an optimal policy by emulating expert demonstrations. However, the prevailing approaches in this domain typically presume that the demonstrations are optimal, an assumption that seldom holds true in the complexities of real-world applications. The data collected in practical scenarios often contains imperfections, encompassing both optimal and non-optimal examples. In this study, we propose Positive-Negative Generative Adversarial Imitation Learning (PN-GAIL), a novel approach that falls within the framework of Generative Adversarial Imitation Learning (GAIL). PN-GAIL innovatively leverages non-optimal information from imperfect demonstrations, allowing the discriminator to comprehensively assess the positive and negative risks associated with these demonstrations. Furthermore, it requires only a small subset of labeled confidence scores. Theoretical analysis indicates that PN-GAIL deviates from the non-optimal data while mimicking imperfect demonstrations. Experimental results demonstrate that PN-GAIL surpasses conventional baseline methods in dealing with imperfect demonstrations, thereby significantly augmenting the practical utility of imitation learning in real-world contexts. Our codes are available at https://github.com/QiangLiuT/PN-GAIL. | Qiang Liu, Huiqiao Fu, Kaiqiang Tang, Chunlin Chen, Daoyi Dong |  |
| 729 |  |  [Attributing Culture-Conditioned Generations to Pretraining Corpora](https://openreview.net/forum?id=XrsOu4KgDE) |  | 0 | In open-ended generative tasks like narrative writing or dialogue, large language models often exhibit cultural biases, showing limited knowledge and generating templated outputs for less prevalent cultures. Recent works show that these biases may stem from uneven cultural representation in pretraining corpora. This work investigates how pretraining leads to biased culture-conditioned generations by analyzing how models associate entities with cultures based on pretraining data patterns. We propose the MEMOED framework (MEMOrization from prEtraining Document) to determine whether a generation for a culture arises from memorization. Using MEMOED on culture-conditioned generations about food and clothing for 110 cultures, we find that high-frequency cultures in pretraining data yield more generations with memorized symbols, while some low-frequency cultures produce none. Additionally, the model favors generating entities with extraordinarily high frequency regardless of the conditioned culture, reflecting biases toward frequent pretraining terms irrespective of relevance. We hope that the MEMOED framework and our insights will inspire more works on attributing model performance on pretraining data. | Huihan Li, Arnav Goel, Keyu He, Xiang Ren |  |
| 730 |  |  [Semi-Parametric Retrieval via Binary Bag-of-Tokens Index](https://openreview.net/forum?id=l0fn10vSyM) |  | 0 | Information retrieval has transitioned from standalone systems into essential components across broader applications, with indexing efficiency, cost-effectiveness, and freshness becoming increasingly critical yet often overlooked. In this paper, we introduce SemI-parametric Disentangled Retrieval (SiDR), a bi-encoder retrieval framework that decouples retrieval index from neural parameters to enable efficient, low-cost, and parameter-agnostic indexing for emerging use cases. Specifically, in addition to using embeddings as indexes like existing neural retrieval methods, SiDR supports a non-parametric tokenization index for search, achieving BM25-like indexing complexity with significantly better effectiveness. Our comprehensive evaluation across 16 retrieval benchmarks demonstrates that SiDR outperforms both neural and term-based retrieval baselines under the same indexing workload: (i) When using an embedding-based index, SiDR exceeds the performance of conventional neural retrievers while maintaining similar training complexity; (ii) When using a tokenization-based index, SiDR drastically reduces indexing cost and time, matching the complexity of traditional term-based retrieval, while consistently outperforming BM25 on all in-domain datasets; (iii) Additionally, we introduce a late parametric mechanism that matches BM25 index preparation time while outperforming other neural retrieval baselines in effectiveness. | Jiawei Zhou, Li Dong, Furu Wei, Lei Chen |  |
| 731 |  |  [RTop-K: Ultra-Fast Row-Wise Top-K Selection for Neural Network Acceleration on GPUs](https://openreview.net/forum?id=PHg4rAXFVH) |  | 0 | Abstract Top-k selection algorithms are fundamental in a wide range of applications, including high-performance computing, information retrieval, big data processing, and neural network model training. In this paper, we present RTop-K, a highly efficient parallel row-wise top-k selection algorithm specifically designed for GPUs. RTop-K leverages a binary search-based approach to optimize row-wise top-k selection, providing a scalable and accelerated solution. We conduct a detailed analysis of early stopping in our algorithm, showing that it effectively maintains the testing accuracy of neural network models while substantially improving performance. Our GPU implementation of RTop-K demonstrates superior performance over state-of-the-art row-wise top-k GPU implementations, achieving an average speed-up of up to 11.49× with early stopping and 7.29× without early stopping. Moreover, RTop-K accelerates the overall training workflow of MaxK-GNNs, delivering speed-ups ranging from 11.97% to 33.29% across different models and datasets. | Xi Xie, Yuebo Luo, Hongwu Peng, Caiwen Ding |  |
| 732 |  |  [DelTA: An Online Document-Level Translation Agent Based on Multi-Level Memory](https://openreview.net/forum?id=hoYFLRNbhc) |  | 0 | Large language models (LLMs) have achieved reasonable quality improvements in machine translation (MT). However, most current research on MT-LLMs still faces significant challenges in maintaining translation consistency and accuracy when processing entire documents. In this paper, we introduce DelTA, a Document-levEL Translation Agent designed to overcome these limitations. DelTA features a multi-level memory structure that stores information across various granularities and spans, including Proper Noun Records, Bilingual Summary, Long-Term Memory, and Short-Term Memory, which are continuously retrieved and updated by auxiliary LLM-based components. Experimental results indicate that DelTA significantly outperforms strong baselines in terms of translation consistency and quality across four open/closed-source LLMs and two representative document translation datasets, achieving an increase in consistency scores by up to 4.58 percentage points and in COMET scores by up to 3.16 points on average. DelTA employs a sentence-by-sentence translation strategy, ensuring no sentence omissions and offering a memory-efficient solution compared to the mainstream method. Furthermore, DelTA improves pronoun and context-dependent translation accuracy, and the summary component of the agent also shows promise as a tool for query-based summarization tasks. The code and data of our approach are released at https://github.com/YutongWang1216/DocMTAgent. | Yutong Wang, Jiali Zeng, Xuebo Liu, Derek F. Wong, Fandong Meng, Jie Zhou, Min Zhang |  |
| 733 |  |  [Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small LLMs](https://openreview.net/forum?id=eENHKMTOfW) |  | 0 | The rise of large language models (LLMs) has created a significant disparity: industrial research labs with their computational resources, expert teams, and advanced infrastructures, can effectively fine-tune LLMs, while individual developers and small organizations face barriers due to limited resources to effectively explore the experiment space. In this paper, we aim to bridge this gap by presenting a comprehensive study on supervised fine-tuning of LLMs using instruction-tuning datasets spanning diverse knowledge domains and skills. We focus on small-sized LLMs (3B to 7B parameters) for their cost-efficiency and accessibility. We explore various training configurations and strategies across four open-source pre-trained models. We provide detailed documentation of these configurations, revealing findings that challenge several common training practices, including hyperparameter recommendations from TULU and phased training recommended by Orca. The code used for the experiments can be found here: https://github.com/instructlab/training. Key insights from our work include: (i) larger batch sizes paired with lower learning rates lead to improved model performance on benchmarks such as MMLU, MTBench, and Open LLM Leaderboard; (ii) early-stage training dynamics, such as lower gradient norms and higher loss values, are strong indicators of better final model performance, allowing for early termination of sub-optimal runs and significant computational savings; (iii) through a thorough exploration of hyperparameters like warmup steps and learning rate schedules, we provide guidance for practitioners and find that certain simplifications do not compromise performance; and (iv) we observe no significant difference in performance between phased (sequentially training on data divided into phases) and stacked (training on the entire dataset at once) strategies, but stacked training is simpler and more sample efficient. With these findings holding robustly across datasets as well as model families and sizes, we hope this study serves as a guide for practitioners fine-tuning small LLMs and promotes a more inclusive research environment for LLM development. | Aldo Pareja, Nikhil Shivakumar Nayak, Hao Wang, Krishnateja Killamsetty, Shivchander Sudalairaj, Wenlong Zhao, Seungwook Han, Abhishek Bhandwaldar, Guangxuan Xu, Kai Xu, Ligong Han, Luke Inglis, Akash Srivastava |  |
| 734 |  |  [ManiSkill-HAB: A Benchmark for Low-Level Manipulation in Home Rearrangement Tasks](https://openreview.net/forum?id=6bKEWevgSd) |  | 0 | High-quality benchmarks are the foundation for embodied AI research, enabling significant advancements in long-horizon navigation, manipulation and rearrangement tasks. However, as frontier tasks in robotics get more advanced, they require faster simulation speed, more intricate test environments, and larger demonstration datasets. To this end, we present MS-HAB, a holistic benchmark for low-level manipulation and in-home object rearrangement. First, we provide a GPU-accelerated implementation of the Home Assistant Benchmark (HAB). We support realistic low-level control and achieve over 3x the speed of prior magical grasp implementations at a fraction of the GPU memory usage. Second, we train extensive reinforcement learning (RL) and imitation learning (IL) baselines for future work to compare against. Finally, we develop a rule-based trajectory filtering system to sample specific demonstrations from our RL policies which match predefined criteria for robot behavior and safety. Combining demonstration filtering with our fast environments enables efficient, controlled data generation at scale. | Arth Shukla, Stone Tao, Hao Su |  |
| 735 |  |  [How Gradient descent balances features: A dynamical analysis for two-layer neural networks](https://openreview.net/forum?id=25j2ZEgwTj) |  | 0 | This paper investigates the fundamental regression task of learning $k$ neurons (\emph{a.k.a.} teachers) from Gaussian input, using two-layer ReLU neural networks with width $m$ (\emph{a.k.a.} students) and $m, k= \mathcal{O}(1)$, trained via gradient descent under proper initialization and a small step-size. Our analysis follows a three-phase structure: \emph{alignment} after weak recovery, \emph{tangential growth}, and \emph{local convergence}, providing deeper insights into the learning dynamics of gradient descent (GD). We prove the global convergence at the rate of $\mathcal{O}(T^{-3})$ for the zero loss of excess risk. Additionally, our results show that GD automatically groups and balances student neurons, revealing an implicit bias toward achieving the minimum \`\`balanced'' $\ell_2$-norm in the solution. Our work extends beyond previous studies in exact-parameterization setting ($m = k = 1$, (Yehudai and Ohad, 2020)) and single-neuron setting ($m \geq k = 1$, (Xu and Du, 2023)). The key technical challenge lies in handling the interactions between multiple teachers and students during training, which we address by refining the alignment analysis in Phase 1 and introducing a new dynamic system analysis for tangential components in Phase 2. Our results pave the way for further research on optimizing neural network training dynamics and understanding implicit biases in more complex architectures. | Zhenyu Zhu, Fanghui Liu, Volkan Cevher |  |
| 736 |  |  [A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement](https://openreview.net/forum?id=YaBiGjuDiC) |  | 0 | Reinforcement Learning from Human Feedback (RLHF) has become the predominant approach for aligning language models (LMs) to be more helpful and less harmful. At its core, RLHF uses a margin-based loss for preference optimization, which specifies the ideal LM behavior only in terms of the difference between preferred and dispreferred responses. In this paper, we identify a common pitfall of margin-based methods---the under-specification of ideal LM behavior on preferred and dispreferred responses individually, which results in two unintended consequences as the margin increases: (1) The probability of dispreferred (e.g., unsafe) responses may increase, resulting in potential safety alignment failures. (2) The probability of preferred responses may decrease, even when those responses are ideal. We demystify the reasons behind these problematic behaviors: margin-based losses couple the change in the preferred probability with the gradient of the dispreferred one, and vice versa, often preventing the preferred probability from increasing while the dispreferred one decreases, and thus causing a synchronized increase or decrease in both probabilities. We term this effect, inherent in margin-based objectives, gradient entanglement. Formally, we derive conditions for general margin-based alignment objectives under which gradient entanglement becomes concerning: the inner product between the gradient of preferred log-probability and the gradient of dispreferred log-probability is large relative to the individual gradient norms. Furthermore, we theoretically investigate why such inner products can be large when aligning language models and empirically validate our findings. Empirical implications of our framework further extend to explaining important differences in the training dynamics of various preference optimization algorithms and suggesting future directions for improvement. | Hui Yuan, Yifan Zeng, Yue Wu, Huazheng Wang, Mengdi Wang, Liu Leqi |  |
| 737 |  |  [Model Editing as a Robust and Denoised variant of DPO: A Case Study on Toxicity](https://openreview.net/forum?id=lOi6FtIwR8) |  | 0 | Recent alignment algorithms such as direct preference optimization (DPO) have been developed to improve the safety of large language models (LLMs) by training these models to match human behaviors exemplified by preference data. However, these methods are both computationally intensive and lacking in controllability and transparency, inhibiting their widespread use. Furthermore, these tuning-based methods require large-scale preference data for training and are susceptible to noisy preference data. In this paper, we introduce a tuning-free alignment alternative, ProFS (Projection Filter for Subspaces), and demonstrate its effectiveness under the use case of toxicity reduction. Grounded on theory from factor analysis, ProFS is a sample-efficient model editing approach that identifies a toxic subspace in the model parameter space and reduces model toxicity by projecting away the detected subspace. The toxic subspace is identified by extracting preference data embeddings from the language model, and removing non-toxic information from these embeddings. We show that ProFS is more sample-efficient than DPO, further showcasing greater robustness to noisy data. Finally, we attempt to connect tuning based alignment with editing, by establishing both theoretical and empirical connections between ProFS and DPO, showing that ProFS can be interpreted as a denoised version of a single DPO step. | Rheeya Uppaal, Apratim Dey, Yiting He, Yiqiao Zhong, Junjie Hu |  |
| 738 |  |  [Should VLMs be Pre-trained with Image Data?](https://openreview.net/forum?id=Pj4Aid3XqL) |  | 0 | Pre-trained LLMs that are further trained with image data perform well on vision-language tasks. While adding images during a second training phase effectively unlocks this capability, it is unclear how much of a gain or loss this two-step pipeline gives over VLMs which integrate images earlier into the training process. To investigate this, we train models spanning various datasets, scales, image-text ratios, and amount of pre-training done before introducing vision tokens. We then fine-tune these models and evaluate their downstream performance on a suite of vision-language and text-only tasks. We find that pre-training with a mixture of image and text data allows models to perform better on vision-language tasks while maintaining strong performance on text-only evaluations. On an average of 6 diverse tasks, we find that for a 1B model, introducing visual tokens 80\% of the way through pre-training results in a 2\% average improvement over introducing visual tokens to a fully pre-trained model. | Sedrick Keh, Jean Mercat, Samir Yitzhak Gadre, Kushal Arora, Igor Vasiljevic, Benjamin Burchfiel, Shuran Song, Russ Tedrake, Thomas Kollar, Ludwig Schmidt, Achal Dave |  |
| 739 |  |  [DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models](https://openreview.net/forum?id=VOAMTA8jKu) |  | 0 | The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor modifications, we found that state-of-the-art VLMs like GPT-4o can consistently fail in these scenarios, revealing limitations in their mathematical reasoning capabilities. In this paper, we investigate the \*\*mathematical reasoning robustness\*\* in VLMs and evaluate how well these models perform under different variants of the same question, such as changes in visual numerical values or function graphs. While several vision-based math benchmarks have been developed to assess VLMs' problem-solving capabilities, these benchmarks contain only static sets of problems and cannot easily evaluate mathematical reasoning robustness. To fill this gap, we introduce \*\*DynaMath\*\*, a dynamic visual math benchmark designed for in-depth assessment of VLMs. \*\*DynaMath\*\* includes 501 high-quality, multi-topic \*seed\* questions, \*each represented as a Python program\*. Those programs are carefully designed and annotated to enable the automatic generation of a much larger set of \*concrete\* questions, including many different types of visual and textual variations. \*\*DynaMath\*\* allows us to evaluate the generalization ability of VLMs, by assessing their performance under varying input conditions of a seed question. We evaluated 14 state-of-the-art VLMs with 5,010 generated concrete questions (10 per seed question). Our results show that the worst-case model accuracy, defined as the percentage of correctly answered seed questions in all 10 variants, is significantly lower than the average-case accuracy. In addition, many models show high consistency in answering these questions -- the incorrectness of a certain variant of a seed question is not only due to inherent randomness. Our analysis emphasizes the need to study the robustness of VLMs' reasoning abilities, and \*\*DynaMath\*\* provides valuable insights to guide the development of more reliable models for mathematical reasoning. | Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, Huan Zhang |  |
| 740 |  |  [MetaDesigner: Advancing Artistic Typography through AI-Driven, User-Centric, and Multilingual WordArt Synthesis](https://openreview.net/forum?id=Mv3GAYJGcW) |  | 0 | MetaDesigner introduces a transformative framework for artistic typography synthesis, powered by Large Language Models (LLMs) and grounded in a user-centric design paradigm. Its foundation is a multi-agent system comprising the Pipeline, Glyph, and Texture agents, which collectively orchestrate the creation of customizable WordArt, ranging from semantic enhancements to intricate textural elements. A central feedback mechanism leverages insights from both multimodal models and user evaluations, enabling iterative refinement of design parameters. Through this iterative process, MetaDesigner dynamically adjusts hyperparameters to align with user-defined stylistic and thematic preferences, consistently delivering WordArt that excels in visual quality and contextual resonance. Empirical evaluations underscore the system's versatility and effectiveness across diverse WordArt applications, yielding outputs that are both aesthetically compelling and context-sensitive. | JunYan He, ZhiQi Cheng, Chenyang Li, Jingdong Sun, Qi He, Wangmeng Xiang, Hanyuan Chen, JinPeng Lan, Xianhui Lin, Kang Zhu, Bin Luo, Yifeng Geng, Xuansong Xie, Alexander G. Hauptmann |  |
| 741 |  |  [Adaptive Rank Allocation: Speeding Up Modern Transformers with RaNA Adapters](https://openreview.net/forum?id=uAtDga3q0r) |  | 0 | Large Language Models (LLMs) are computationally intensive, particularly during inference. Neuron-adaptive techniques, which selectively activate neurons in Multi-Layer Perceptron (MLP) layers, offer some speedups but suffer from limitations in modern Transformers. These include reliance on sparse activations, incompatibility with attention layers, and the use of costly neuron masking techniques. To address these issues, we propose the Adaptive Rank Allocation framework and introduce the Rank and Neuron Allocator (RaNA) adapter. RaNA adapters leverage rank adapters, which operate on linear layers by applying both low-rank matrix decompositions and adaptive masking to efficiently allocate compute without depending on activation sparsity. This enables RaNA to be generally applied to MLPs and linear components of attention modules, while eliminating the need for expensive maskers found in neuron-adaptive methods. Notably, when compared to neuron adapters, RaNA improves perplexity by up to 7 points and increases accuracy by up to 8 percentage-points when reducing FLOPs by $\sim$44\% in state-of-the-art Transformer architectures. These results position RaNA as a robust solution for improving inference efficiency in modern Transformer architectures. | Roberto Garcia, Jerry Weihong Liu, Daniel Sorvisto, Sabri Eyuboglu |  |
| 742 |  |  [Efficient Jailbreak Attack sequences on Large Language Models via Multi-Armed Bandit-based Context switching](https://openreview.net/forum?id=jCDF7G3LpF) |  | 0 | Content warning: This paper contains examples of harmful language and content. Recent advances in large language models (LLMs) have made them increasingly vulnerable to jailbreaking attempts, where malicious users manipulate models into generating harmful content. While existing approaches rely on either single-step attacks that trigger immediate safety responses or multi-step methods that inefficiently iterate prompts using other LLMs, we introduce \`\`Sequence of Context" (SoC) attacks that systematically alter conversational context through strategically crafted context-switching queries (CSQs). We formulate this as a multi-armed bandit (MAB) optimization problem, automatically learning optimal sequences of CSQs that gradually weaken the model's safety boundaries. Our theoretical analysis provides tight bounds on both the expected sequence length until successful jailbreak and the convergence of cumulative rewards. Empirically, our method achieves a 95\% attack success rate, surpassing PAIR by 63.15\%, AutoDAN by 60\%, and ReNeLLM by 50\%. We evaluate our attack across multiple open-source LLMs including Llama and Mistral variants. Our findings highlight critical vulnerabilities in current LLM safeguards and emphasize the need for defenses that consider sequential attack patterns rather than relying solely on static prompt filtering or iterative refinement. | Aditya Ramesh, Shivam Bhardwaj, Aditya Saibewar, Manohar Kaul |  |
| 743 |  |  [A Little Goes a Long Way: Efficient Long Context Training and Inference with Partial Contexts](https://openreview.net/forum?id=TrKRpaOk8y) |  | 0 | Training and serving long-context large language models (LLMs) incurs substantial overhead. To address this, two critical steps are often required: a pretrained LLM typically undergoes a separate stage for context length extension by training on long-context data, followed by architectural modifications to reduce the overhead of KV cache during serving. This paper argues that integrating length extension with a GPU-friendly KV cache reduction architecture not only reduces training overhead during length extension, but also achieves better long-context performance. This leads to our proposed LongGen, which finetunes a pretrained LLM into an efficient architecture during length extension. LongGen builds on three key insights: (1) Sparse attention patterns, such as window attention (attending to recent tokens), attention sink (initial ones), and blockwise sparse attention (strided token blocks) are well-suited for building efficient long-context models, primarily due to their GPU-friendly memory access patterns, enabling efficiency gains not just theoretically but in practice as well. (2) It is essential for the model to have direct access to all tokens. A hybrid architecture with 1/3 full attention layers and 2/3 efficient ones achieves a balanced trade-off between efficiency and long-context performance. (3) Lightweight training on 5B long-context data is sufficient to extend the hybrid model's context length from 4K to 128K. We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its effectiveness across different scales. During training with 128K-long contexts, LongGen achieves 1.55x training speedup and reduces wall-clock time by 36%, compared to a full-attention baseline. During inference, LongGen reduces KV cache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding speedup. Compared to baselines that apply KV-cache reduction techniques to full-attention long-context LLMs, LongGen achieves substantially stronger performance not only on the Needle-in-a-Haystack retrieval task, but also on more challenging long-context reasoning tasks, including BABILong and RULER. | Suyu Ge, Xihui Lin, Yunan Zhang, Jiawei Han, Hao Peng |  |
| 744 |  |  [What Secrets Do Your Manifolds Hold? Understanding the Local Geometry of Generative Models](https://openreview.net/forum?id=etif9j1CnG) |  | 0 | Deep Generative Models are frequently used to learn continuous representations of complex data distributions by training on a finite number of samples. For any generative model, including pre-trained foundation models with Diffusion or Transformer architectures, generation performance can significantly vary across the learned data manifold. In this paper, we study the local geometry of the learned manifold and its relationship to generation outcomes for a wide range of generative models, including DDPM, Diffusion Transformer (DiT), and Stable Diffusion 1.4. Building on the theory of continuous piecewise-linear (CPWL) generators, we characterize the local geometry in terms of three geometric descriptors - scaling ($\psi$), rank ($\nu$), and complexity/un-smoothness ($\delta$). We provide quantitative and qualitative evidence showing that for a given latent vector, the local descriptors are indicative of post-generation aesthetics, generation diversity, and memorization by the generative model. Finally, we demonstrate that by training a reward model on the 'local scaling' for Stable Diffusion, we can self-improve both generation aesthetics and diversity using geometry sensitive guidance during denoising. Website: https://imtiazhumayun.github.io/generative_geometry. | Ahmed Imtiaz Humayun, Ibtihel Amara, Cristina Nader Vasconcelos, Deepak Ramachandran, Candice Schumann, Junfeng He, Katherine A. Heller, Golnoosh Farnadi, Negar Rostamzadeh, Mohammad Havaei |  |
| 745 |  |  [L-WISE: Boosting Human Visual Category Learning Through Model-Based Image Selection and Enhancement](https://openreview.net/forum?id=AoIKgHu9Si) |  | 0 | The currently leading artificial neural network models of the visual ventral stream - which are derived from a combination of performance optimization and robustification methods - have demonstrated a remarkable degree of behavioral alignment with humans on visual categorization tasks. We show that image perturbations generated by these models can enhance the ability of humans to accurately report the ground truth class. Furthermore, we find that the same models can also be used out-of-the-box to predict the proportion of correct human responses to individual images, providing a simple, human-aligned estimator of the relative difficulty of each image. Motivated by these observations, we propose to augment visual learning in humans in a way that improves human categorization accuracy at test time. Our learning augmentation approach consists of (i) selecting images based on their model-estimated recognition difficulty, and (ii) applying image perturbations that aid recognition for novice learners. We find that combining these model-based strategies leads to categorization accuracy gains of 33-72% relative to control subjects without these interventions, on unmodified, randomly selected held-out test images. Beyond the accuracy gain, the training time for the augmented learning group was also shortened by 20-23%, despite both groups completing the same number of training trials. We demonstrate the efficacy of our approach in a fine-grained categorization task with natural images, as well as two tasks in clinically relevant image domains - histology and dermoscopy - where visual learning is notoriously challenging. To the best of our knowledge, our work is the first application of artificial neural networks to increase visual learning performance in humans by enhancing category-specific image features. | Morgan Bruce Talbot, Gabriel Kreiman, James J. DiCarlo, Guy Gaziv |  |
| 746 |  |  [Predictive Uncertainty Quantification for Bird's Eye View Segmentation: A Benchmark and Novel Loss Function](https://openreview.net/forum?id=k3y0oyK7sn) |  | 0 | The fusion of raw sensor data to create a Bird's Eye View (BEV) representation is critical for autonomous vehicle planning and control. Despite the growing interest in using deep learning models for BEV semantic segmentation, anticipating segmentation errors and enhancing the explainability of these models remain underexplored. This paper introduces a comprehensive benchmark for predictive uncertainty quantification in BEV segmentation, evaluating multiple uncertainty quantification methods across three popular datasets with three representative network architectures. Our study focuses on the effectiveness of quantified uncertainty in detecting misclassified and out-of-distribution (OOD) pixels while also improving model calibration. Through empirical analysis, we uncover challenges in existing uncertainty quantification methods and demonstrate the potential of evidential deep learning techniques, which capture both aleatoric and epistemic uncertainty. To address these challenges, we propose a novel loss function, Uncertainty-Focal-Cross-Entropy (UFCE), specifically designed for highly imbalanced data, along with a simple uncertainty-scaling regularization term that improves both uncertainty quantification and model calibration for BEV segmentation. | Linlin Yu, Bowen Yang, Tianhao Wang, Kangshuo Li, Feng Chen |  |
| 747 |  |  [DiscoveryBench: Towards Data-Driven Discovery with Large Language Models](https://openreview.net/forum?id=vyflgpwfJW) |  | 0 | Can the rapid advances in code generation, function calling, and data analysis using large language models (LLMs) help automate the search and verification of hypotheses purely from a set of provided datasets? To evaluate this question, we present DiscoveryBench, the first comprehensive benchmark that formalizes the multi-step process of data-driven discovery. The benchmark is designed to systematically assess current model capabilities in discovery tasks and provide a useful resource for improving them. Our benchmark contains 264 tasks collected across 6 diverse domains, such as sociology and engineering, by manually deriving discovery workflows from published papers to approximate the real-world challenges faced by researchers, where each task is defined by a dataset, its metadata, and a discovery goal in natural language. We additionally provide 903 synthetic tasks to conduct controlled evaluations on data-driven workflows that are not covered in the manually collected split. Furthermore, our structured formalism of data-driven discovery enables a facet-based evaluation that provides useful insights into different failure modes. We evaluate several popular LLM-based reasoning frameworks using both open and closed LLMs as baselines on DiscoveryBench and find that even the best system scores only 25%. Our benchmark, thus, illustrates the challenges in autonomous data-driven discovery and serves as a valuable resource for the community to make progress. | Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Bhavana Dalvi Mishra, Abhijeetsingh Meena, Aryan Prakhar, Tirth Vora, Tushar Khot, Ashish Sabharwal, Peter Clark |  |
| 748 |  |  [Optimizing (L0, L1)-Smooth Functions by Gradient Methods](https://openreview.net/forum?id=GQ1Tc3vHbt) |  | 0 | We study gradient methods for optimizing $(L_0, L_1)$-smooth functions, a class that generalizes Lipschitz-smooth functions and has gained attention for its relevance in machine learning. We provide new insights into the structure of this function class and develop a principled framework for analyzing optimization methods in this setting. While our convergence rate estimates recover existing results for minimizing the gradient norm in nonconvex problems, our approach significantly improves the best-known complexity bounds for convex objectives. Moreover, we show that the gradient method with Polyak stepsizes and the normalized gradient method achieve nearly the same complexity guarantees as methods that rely on explicit knowledge of $(L_0, L_1)$. Finally, we demonstrate that a carefully designed accelerated gradient method can be applied to $(L_0, L_1)$-smooth functions, further improving all previous results. | Daniil Vankov, Anton Rodomanov, Angelia Nedich, Lalitha Sankar, Sebastian U. Stich |  |
| 749 |  |  [Compute-Optimal LLMs Provably Generalize Better with Scale](https://openreview.net/forum?id=MF7ljU8xcf) |  | 0 | Why do larger language models generalize better? To explore this question, we develop generalization bounds on the pretraining objective of large language models (LLMs) in the compute-optimal regime, as described by the Chinchilla scaling laws. We introduce a novel, fully empirical Freedman-type martingale concentration inequality that tightens existing bounds by accounting for the variance of the loss function. The generalization bound can be broken into three contributions: the number of parameters per token, the loss variance, and the quantization error at a fixed bitrate. As language models are scaled up, the number of parameters per data point stays constant; however, both the loss variance and the quantization error decrease, implying that larger models should have \emph{smaller} generalization gaps. We examine why larger models tend to be more quantizable from an information theoretic perspective, showing that the rate at which they can integrate new information grows slower than their capacity on the compute optimal frontier. From these findings we produce a scaling law for the generalization gap, showing that our bounds decrease in a predictable way. | Marc Anton Finzi, Sanyam Kapoor, Diego Granziol, Anming Gu, Christopher De Sa, J. Zico Kolter, Andrew Gordon Wilson |  |
| 750 |  |  [Safety-Prioritizing Curricula for Constrained Reinforcement Learning](https://openreview.net/forum?id=f3QR9TEERH) |  | 0 | Curriculum learning aims to accelerate reinforcement learning (RL) by generating curricula, i.e., sequences of tasks of increasing difficulty. Although existing curriculum generation approaches provide benefits in sample efficiency, they overlook safety-critical settings where an RL agent must adhere to safety constraints. Thus, these approaches may generate tasks that cause RL agents to violate safety constraints during training and behave suboptimally after. We develop a safe curriculum generation approach (SCG) that aligns the objectives of constrained RL and curriculum learning: improving safety during training and boosting sample efficiency. SCG generates sequences of tasks where the RL agent can be safe and performant by initially generating tasks with minimum safety violations over high-reward ones. We empirically show that compared to the state-of-the-art curriculum learning approaches and their naively modified safe versions, SCG achieves optimal performance and the lowest amount of constraint violations during training. | Cevahir Köprülü, Thiago D. Simão, Nils Jansen, Ufuk Topcu |  |
| 751 |  |  [Generalized Behavior Learning from Diverse Demonstrations](https://openreview.net/forum?id=Q7EjHroO1w) |  | 0 | Diverse behavior policies are valuable in domains requiring quick test-time adaptation or personalized human-robot interaction. Human demonstrations provide rich information regarding task objectives and factors that govern individual behavior variations, which can be used to characterize \textit{useful} diversity and learn diverse performant policies. However, we show that prior work that builds naive representations of demonstration heterogeneity fails in generating successful novel behaviors that generalize over behavior factors. We propose Guided Strategy Discovery (GSD), which introduces a novel diversity formulation based on a learned task-relevance measure that prioritizes behaviors exploring modeled latent factors. We empirically validate across three continuous control benchmarks for generalizing to in-distribution (interpolation) and out-of-distribution (extrapolation) factors that GSD outperforms baselines in novel behavior discovery by $\sim$21\%. Finally, we demonstrate that GSD can generalize striking behaviors for table tennis in a virtual testbed while leveraging human demonstrations collected in the real world. Code is available at https://github.com/CORE-Robotics-Lab/GSD. | Varshith Sreeramdass, Rohan R. Paleja, Letian Chen, Sanne van Waveren, Matthew C. Gombolay |  |
| 752 |  |  [Neural Stochastic Differential Equations for Uncertainty-Aware Offline RL](https://openreview.net/forum?id=hxUMQ4fic3) |  | 0 | Offline model-based reinforcement learning (RL) offers a principled approach to using a learned dynamics model as a simulator to optimize a control policy. Despite the near-optimal performance of existing approaches on benchmarks with high-quality datasets, most struggle on datasets with low state-action space coverage or suboptimal demonstrations. We develop a novel offline model-based RL approach that particularly shines in low-quality data regimes while maintaining competitive performance on high-quality datasets. Neural Stochastic Differential Equations for Uncertainty-aware, Offline RL (NUNO) learns a dynamics model as neural stochastic differential equations (SDE), where its drift term can leverage prior physics knowledge as inductive bias. In parallel, its diffusion term provides distance-aware estimates of model uncertainty by matching the dynamics' underlying stochasticity near the training data regime while providing high but bounded estimates beyond it. To address the so-called model exploitation problem in offline model-based RL, NUNO builds on existing studies by penalizing and adaptively truncating neural SDE's rollouts according to uncertainty estimates. Our empirical results in D4RL and NeoRL MuJoCo benchmarks evidence that NUNO outperforms state-of-the-art methods in low-quality datasets by up to 93% while matching or surpassing their performance by up to 55% in some high-quality counterparts. | Cevahir Köprülü, Franck Djeumou, Ufuk Topcu |  |
| 753 |  |  [Towards Optimal Multi-draft Speculative Decoding](https://openreview.net/forum?id=9KxnxWOBA5) |  | 0 | Large Language Models (LLMs) have become an indispensable part of natural language processing tasks. However, autoregressive sampling has become an efficiency bottleneck. Multi-Draft Speculative Decoding (MDSD) is a recent approach where, when generating each token, a small draft model generates multiple drafts, and the target LLM verifies them in parallel, ensuring that the final output conforms to the target model distribution. The two main design choices in MDSD are the draft sampling method and the verification algorithm. For a fixed draft sampling method, the optimal acceptance rate is a solution to an optimal transport problem, but the complexity of this problem makes it difficult to solve for the optimal acceptance rate and measure the gap between existing verification algorithms and the theoretical upper bound. This paper discusses the dual of the optimal transport problem, providing a way to efficiently compute the optimal acceptance rate. For the first time, we measure the theoretical upper bound of MDSD efficiency for vocabulary sizes in the thousands and quantify the gap between existing verification algorithms and this bound. We also compare different draft sampling methods based on their optimal acceptance rates. Our results show that the draft sampling method strongly influences the optimal acceptance rate, with sampling without replacement outperforming sampling with replacement. Additionally, existing verification algorithms do not reach the theoretical upper bound for both without replacement and with replacement sampling. Our findings suggest that carefully designed draft sampling methods can potentially improve the optimal acceptance rate and enable the development of verification algorithms that closely match the theoretical upper bound. | Zhengmian Hu, Tong Zheng, Vignesh Viswanathan, Ziyi Chen, Ryan A. Rossi, Yihan Wu, Dinesh Manocha, Heng Huang |  |
| 754 |  |  [Diffusion Generative Modeling for Spatially Resolved Gene Expression Inference from Histology Images](https://openreview.net/forum?id=FtjLUHyZAO) |  | 0 | Spatial Transcriptomics (ST) allows a high-resolution measurement of RNA sequence abundance by systematically connecting cell morphology depicted in Hematoxylin and eosin (H\&E) stained histology images to spatially resolved gene expressions. ST is a time-consuming, expensive yet powerful experimental technique that provides new opportunities to understand cancer mechanisms at a fine-grained molecular level, which is critical for uncovering new approaches for disease diagnosis and treatments. Here, we present $\textbf{Stem}$ ($\underline{\textbf{S}}$pa$\underline{\textbf{T}}$ially resolved gene $\underline{\textbf{E}}$xpression inference with diffusion $\underline{\textbf{M}}$odel), a novel computational tool that leverages a conditional diffusion generative model to enable in silico gene expression inference from H&E stained images. Through better capturing the inherent stochasticity and heterogeneity in ST data, $\textbf{Stem}$ achieves state-of-the-art performance on spatial gene expression prediction and generates biologically meaningful gene profiles for new H&E stained images at test time. We evaluate the proposed algorithm on datasets with various tissue sources and sequencing platforms, where it demonstrates clear improvement over existing approaches. $\textbf{Stem}$ generates high-fidelity gene expression predictions that share similar gene variation levels as ground truth data, suggesting that our method preserves the underlying biological heterogeneity. Our proposed pipeline opens up the possibility of analyzing existing, easily accessible H&E stained histology images from a genomics point of view without physically performing gene expression profiling and empowers potential biological discovery from H&E stained histology images. Code is available at: https://github.com/SichenZhu/Stem. | Sichen Zhu, Yuchen Zhu, Molei Tao, Peng Qiu |  |
| 755 |  |  [Diffusion State-Guided Projected Gradient for Inverse Problems](https://openreview.net/forum?id=kRBQwlkFSP) |  | 0 | Recent advancements in diffusion models have been effective in learning data priors for solving inverse problems. They leverage diffusion sampling steps for inducing a data prior while using a measurement guidance gradient at each step to impose data consistency. For general inverse problems, approximations are needed when an unconditionally trained diffusion model is used since the measurement likelihood is intractable, leading to inaccurate posterior sampling. In other words, due to their approximations, these methods fail to preserve the generation process on the data manifold defined by the diffusion prior, leading to artifacts in applications such as image restoration. To enhance the performance and robustness of diffusion models in solving inverse problems, we propose Diffusion State-Guided Projected Gradient (DiffStateGrad), which projects the measurement gradient onto a subspace that is a low-rank approximation of an intermediate state of the diffusion process. DiffStateGrad, as a module, can be added to a wide range of diffusion-based inverse solvers to improve the preservation of the diffusion process on the prior manifold and filter out artifact-inducing components. We highlight that DiffStateGrad improves the robustness of diffusion models in terms of the choice of measurement guidance step size and noise while improving the worst-case performance. Finally, we demonstrate that DiffStateGrad improves upon the state-of-the-art on linear and nonlinear image restoration inverse problems. Our code is available at https://github.com/Anima-Lab/DiffStateGrad. | Rayhan Zirvi, Bahareh Tolooshams, Anima Anandkumar |  |
| 756 |  |  [MADGEN: Mass-Spec attends to De Novo Molecular generation](https://openreview.net/forum?id=78tc3EiUrN) |  | 0 | The annotation (assigning structural chemical identities) of MS/MS spectra remains a significant challenge due to the enormous molecular diversity in biological samples and the limited scope of reference databases. Currently, the vast majority of spectral measurements remain in the "dark chemical space" without structural annotations. To improve annotation, we propose MADGEN (Mass-spec Attends to De Novo Molecular GENeration), a scaffold-based method for de novo molecular structure generation guided by mass spectrometry data. MADGEN operates in two stages: scaffold retrieval and spectra-conditioned molecular generation starting with the scaffold. In the first stage, given an MS/MS spectrum, we formulate scaffold retrieval as a ranking problem and employ contrastive learning to align mass spectra with candidate molecular scaffolds. In the second stage, starting from the retrieved scaffold, we employ the MS/MS spectrum to guide an attention-based generative model to generate the final molecule. Our approach constrains the molecular generation search space, reducing its complexity and improving generation accuracy. We evaluate MADGEN on three datasets (NIST23, CANOPUS, and MassSpecGym) and evaluate MADGEN's performance with a predictive scaffold retriever and with an oracle retriever. We demonstrate the effectiveness of using attention to integrate spectral information throughout the generation process to achieve strong results with the oracle retriever. | Yinkai Wang, Xiaohui Chen, Liping Liu, Soha Hassoun |  |
| 757 |  |  [Towards Federated RLHF with Aggregated Client Preference for LLMs](https://openreview.net/forum?id=mqNKiEB6pd) |  | 0 | Reinforcement learning with human feedback (RLHF) fine-tunes a pretrained large language model (LLM) using user preference data, enabling it to generate content aligned with human preferences. However, due to privacy concerns, users may be reluctant to share sensitive preference data. To address this, we propose utilizing Federated Learning (FL) techniques, allowing large-scale preference collection from diverse real-world users without requiring them to transmit data to a central server. Our federated RLHF methods (i.e., FedBis and FedBiscuit) encode each client’s preferences into binary selectors and aggregate them to capture common preferences. In particular, FedBiscuit overcomes key challenges, such as preference heterogeneity and reward hacking, through innovative solutions like grouping clients with similar preferences to reduce heterogeneity and using multiple binary selectors to enhance LLM output quality. To evaluate the performance of the proposed methods, we establish the first federated RLHF benchmark with a heterogeneous human preference dataset. Experimental results show that by integrating the LLM with aggregated client preferences, FedBis and FedBiscuit significantly enhance the professionalism and readability of the generated content. | Feijie Wu, Xiaoze Liu, Haoyu Wang, Xingchen Wang, Lu Su, Jing Gao |  |
| 758 |  |  [Overcoming Slow Decision Frequencies in Continuous Control: Model-Based Sequence Reinforcement Learning for Model-Free Control](https://openreview.net/forum?id=w3iM4WLuvy) |  | 0 | Reinforcement learning (RL) is rapidly reaching and surpassing human-level control capabilities. However, state-of-the-art RL algorithms often require timesteps and reaction times significantly faster than human capabilities, which is impractical in real-world settings and typically necessitates specialized hardware. We introduce Sequence Reinforcement Learning (SRL), an RL algorithm designed to produce a sequence of actions for a given input state, enabling effective control at lower decision frequencies. SRL addresses the challenges of learning action sequences by employing both a model and an actor-critic architecture operating at different temporal scales. We propose a "temporal recall" mechanism, where the critic uses the model to estimate intermediate states between primitive actions, providing a learning signal for each individual action within the sequence. Once training is complete, the actor can generate action sequences independently of the model, achieving model-free control at a slower frequency. We evaluate SRL on a suite of continuous control tasks, demonstrating that it achieves performance comparable to state-of-the-art algorithms while significantly reducing actor sample complexity. To better assess performance across varying decision frequencies, we introduce the Frequency-Averaged Score (FAS) metric. Our results show that SRL significantly outperforms traditional RL algorithms in terms of FAS, making it particularly suitable for applications requiring variable decision frequencies. Furthermore, we compare SRL with model-based online planning, showing that SRL achieves comparable FAS while leveraging the same model during training that online planners use for planning. | Devdhar Patel, Hava T. Siegelmann |  |
| 759 |  |  [Physics of Language Models: Part 3.2, Knowledge Manipulation](https://openreview.net/forum?id=oDbiL9CLoS) |  | 0 | Language models can store vast factual knowledge, yet their ability to flexibly use this knowledge for downstream tasks (e.g., via instruction finetuning) remains questionable. This paper investigates four fundamental knowledge manipulation tasks: \textbf{retrieval} (e.g., "What is person A's attribute X?"), \textbf{classification} (e.g., "Is A's attribute X even or odd?"), \textbf{comparison} (e.g., "Is A greater than B in attribute X?"), and \textbf{inverse search} (e.g., "Which person's attribute X equals T?"). We show that language models excel in knowledge retrieval but struggle even in the simplest classification or comparison tasks unless Chain of Thoughts (CoTs) are employed during both training and inference. Moreover, their performance in inverse knowledge search is virtually 0\%, regardless of the prompts. Our primary contribution is a \emph{controlled, synthetic experiment} that confirms these weaknesses are \emph{inherent} to language models: they cannot efficiently manipulate knowledge from pre-training data, even when such knowledge is perfectly stored in the models, despite adequate training and sufficient model size. Our findings also apply to modern pretrained language models such as GPT-4, thus giving rise to many Turing tests to distinguish Humans from contemporary AIs. | Zeyuan AllenZhu, Yuanzhi Li |  |
| 760 |  |  [Logic-Logit: A Logic-Based Approach to Choice Modeling](https://openreview.net/forum?id=vJgJSrYPe1) |  | 0 | In this study, we propose a novel rule-based interpretable choice model, {\bf Logic-Logit}, designed to effectively learn and explain human choices. Choice models have been widely applied across various domains—such as commercial demand forecasting, recommendation systems, and consumer behavior analysis—typically categorized as parametric, nonparametric, or deep network-based. While recent innovations have favored neural network approaches for their computational power, these flexible models often involve large parameter sets and lack interpretability, limiting their effectiveness in contexts where transparency is essential. Previous empirical evidence shows that individuals usually use {\it heuristic decision rules} to form their consideration sets, from which they then choose. These rules are often represented as {\it disjunctions of conjunctions} (i.e., OR-of-ANDs). These rules-driven, {\it consider-then-choose} decision processes enable people to quickly screen numerous alternatives while reducing cognitive and search costs. Motivated by this insight, our approach leverages logic rules to elucidate human choices, providing a fresh perspective on preference modeling. We introduce a unique combination of column generation techniques and the Frank-Wolfe algorithm to facilitate efficient rule extraction for preference modeling—a process recognized as NP-hard. Our empirical evaluation, conducted on both synthetic datasets and real-world data from commercial and healthcare domains, demonstrates that Logic-Logit significantly outperforms baseline models in terms of interpretability and accuracy. | Shuhan Zhang, Wendi Ren, Shuang Li |  |
| 761 |  |  [On Calibration of LLM-based Guard Models for Reliable Content Moderation](https://openreview.net/forum?id=wUbum0nd9N) |  | 0 | Large language models (LLMs) pose significant risks due to the potential for generating harmful content or users attempting to evade guardrails. Existing studies have developed LLM-based guard models designed to moderate the input and output of threat LLMs, ensuring adherence to safety policies by blocking content that violates these protocols upon deployment. However, limited attention has been given to the reliability and calibration of such guard models. In this work, we empirically conduct comprehensive investigations of confidence calibration for 9 existing LLM-based guard models on 12 benchmarks in both user input and model output classification. Our findings reveal that current LLM-based guard models tend to 1) produce overconfident predictions, 2) exhibit significant miscalibration when subjected to jailbreak attacks, and 3) demonstrate limited robustness to the outputs generated by different types of response models. Additionally, we assess the effectiveness of post-hoc calibration methods to mitigate miscalibration. We demonstrate the efficacy of temperature scaling and, for the first time, highlight the benefits of contextual calibration for confidence calibration of guard models, particularly in the absence of validation sets. Our analysis and experiments underscore the limitations of current LLM-based guard models and provide valuable insights for the future development of well-calibrated guard models toward more reliable content moderation. We also advocate for incorporating reliability evaluation of confidence calibration when releasing future LLM-based guard models. | Hongfu Liu, Hengguan Huang, Xiangming Gu, Hao Wang, Ye Wang |  |
| 762 |  |  [Unified Convergence Analysis for Score-Based Diffusion Models with Deterministic Samplers](https://openreview.net/forum?id=HrdVqFSn1e) |  | 0 | Score-based diffusion models have emerged as powerful techniques for generating samples from high-dimensional data distributions. These models involve a two-phase process: first, injecting noise to transform the data distribution into a known prior distribution, and second, sampling to recover the original data distribution from noise. Among the various sampling methods, deterministic samplers stand out for their enhanced efficiency. However, analyzing these deterministic samplers presents unique challenges, as they preclude the use of established techniques such as Girsanov's theorem, which are only applicable to stochastic samplers. Furthermore, existing analysis for deterministic samplers usually focuses on specific examples, lacking a generalized approach for general forward processes and various deterministic samplers. Our paper addresses these limitations by introducing a unified convergence analysis framework. To demonstrate the power of our framework, we analyze the variance-preserving (VP) forward process with the exponential integrator (EI) scheme, achieving iteration complexity of $\tilde{O}(d^2/\epsilon)$. Additionally, we provide a detailed analysis of Denoising Diffusion Implicit Models (DDIM)-type samplers, which have been underexplored in previous research, achieving polynomial iteration complexity. | Runjia Li, Qiwei Di, Quanquan Gu |  |
| 763 |  |  [Optimizing Neural Network Representations of Boolean Networks](https://openreview.net/forum?id=1H90Gb9rJ9) |  | 0 | Neural networks are known to be universal computers for Boolean functions. Recent advancements in hardware have significantly reduced matrix multiplication times, making neural network simulation both fast and efficient. Consequently, functions defined by complex Boolean networks are increasingly viable candidates for simulation through their neural network representation. Prior research has introduced a general method for deriving neural network representations of Boolean networks. However, the resulting neural networks are often suboptimal in terms of the number of neurons and connections, leading to slower simulation performance. Optimizing them while preserving functional equivalence --lossless optimization-- is an NP-hard problem, and current methods only provide lossy solutions. In this paper, we present a deterministic algorithm to optimize such neural networks in terms of neurons and connections while preserving functional equivalence. Moreover, to accelerate the compression of the neural network, we introduce an objective-aware algorithm that exploits representations that are shared among subproblems of the overall optimization. We demonstrate experimentally that we are able to reduce connections and neurons by up to 70% and 60%, respectively, in comparison to state-of-the-art. We also find that our objective-aware algorithm results in consistent speedups in optimization time, achieving up to 34.3x and 5.9x speedup relative to naive and caching solutions, respectively. Our methods are of practical relevance to applications such as high-throughput circuit simulation and placing neurosymbolic systems on the same hardware architecture. | Joshua Russell, Ignacio Gavier, Devdhar Patel, Edward A. Rietman, Hava T. Siegelmann |  |
| 764 |  |  [Causal Order: The Key to Leveraging Imperfect Experts in Causal Inference](https://openreview.net/forum?id=9juyeCqL0u) |  | 0 | Large Language Models (LLMs) have recently been used as experts to infer causal graphs, often by repeatedly applying a pairwise prompt that asks about the causal relationship of each variable pair. However, such experts, including human domain experts, cannot distinguish between direct and indirect effects given a pairwise prompt. Therefore, instead of the graph, we propose that causal order be used as a more stable output interface for utilizing expert knowledge. When querying a perfect expert with a pairwise prompt, we show that the inferred graph can have significant errors whereas the causal order is always correct. In practice, however, LLMs are imperfect experts and we find that pairwise prompts lead to multiple cycles and do not yield a valid order. Hence, we propose a prompting strategy that introduces an auxiliary variable for every variable pair and instructs the LLM to avoid cycles within this triplet. We show, both theoretically and empirically, that such a triplet prompt leads to fewer cycles than the pairwise prompt. Across multiple real-world graphs, the triplet prompt yields a more accurate order using both LLMs and human annotators as experts. By querying the expert with different auxiliary variables for the same variable pair, it also increases robustness---triplet method with much smaller models such as Phi-3 and Llama-3 8B outperforms a pairwise prompt with GPT-4. For practical usage, we show how the estimated causal order from the triplet method can be used to reduce error in downstream discovery and effect inference tasks. | Aniket Vashishtha, Abbavaram Gowtham Reddy, Abhinav Kumar, Saketh Bachu, Vineeth N. Balasubramanian, Amit Sharma |  |
| 765 |  |  [Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School Math Problems](https://openreview.net/forum?id=zpDGwcmMV4) |  | 0 | Language models have demonstrated remarkable performance in solving reasoning tasks; however, even the strongest models still occasionally make reasoning mistakes. Recently, there has been active research aimed at improving reasoning accuracy, particularly by using pretrained language models to "self-correct'' their mistakes via multi-round prompting. In this paper, we follow this line of work but focus on understanding the usefulness of incorporating \`\`error-correction'' data directly into the pretraining stage. This data consists of erroneous solution steps immediately followed by their corrections. Using a synthetic math dataset, we show promising results: this type of pretrain data can help language models achieve higher reasoning accuracy directly (i.e., through simple auto-regression, without multi-round prompting) compared to pretraining on the same amount of error-free data. We also delve into many details, such as (1) how this approach differs from beam search, (2) how such data can be prepared, (3) whether masking is needed on the erroneous tokens, (4) the amount of error required, (5) whether such data can be deferred to the fine-tuning stage, and many others. | Tian Ye, Zicheng Xu, Yuanzhi Li, Zeyuan AllenZhu |  |
| 766 |  |  [Reasoning of Large Language Models over Knowledge Graphs with Super-Relations](https://openreview.net/forum?id=rTCJ29pkuA) |  | 0 | While large language models (LLMs) have made significant progress in processing and reasoning over knowledge graphs, current methods suffer from a high non-retrieval rate. This limitation reduces the accuracy of answering questions based on these graphs. Our analysis reveals that the combination of greedy search and forward reasoning is a major contributor to this issue. To overcome these challenges, we introduce the concept of super-relations, which enables both forward and backward reasoning by summarizing and connecting various relational paths within the graph. This holistic approach not only expands the search space, but also significantly improves retrieval efficiency. In this paper, we propose the ReKnoS framework, which aims to Reason over Knowledge Graphs with Super-Relations. Our framework’s key advantages include the inclusion of multiple relation paths through super-relations, enhanced forward and backward reasoning capabilities, and increased efficiency in querying LLMs. These enhancements collectively lead to a substantial improvement in the successful retrieval rate and overall reasoning performance. We conduct extensive experiments on a variety of datasets to evaluate ReKnoS, and the results demonstrate the superior performance of ReKnoS over existing state-of-the-art baselines, with an average accuracy gain of 2.92% across nine real-world datasets. | Song Wang, Junhong Lin, Xiaojie Guo, Julian Shun, Jundong Li, Yada Zhu |  |
| 767 |  |  [Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process](https://openreview.net/forum?id=Tn5B6Udq3E) |  | 0 | Recent advances in language models have demonstrated their capability to solve mathematical reasoning problems, achieving near-perfect accuracy on grade-school level math benchmarks like GSM8K. In this paper, we formally study how language models solve these problems. We design a series of controlled experiments to address several fundamental questions: (1) Can language models truly develop reasoning skills, or do they simply memorize templates? (2) What is the model's hidden (mental) reasoning process? (3) Do models solve math questions using skills similar to or different from humans? (4) Do models trained on GSM8K-like datasets develop reasoning skills beyond those necessary for solving GSM8K problems? (5) What mental process causes models to make reasoning mistakes? (6) How large or deep must a model be to effectively solve GSM8K-level math questions? Our study uncovers many hidden mechanisms by which language models solve mathematical questions, providing insights that extend beyond current understandings of LLMs. | Tian Ye, Zicheng Xu, Yuanzhi Li, Zeyuan AllenZhu |  |
| 768 |  |  [Agent-Oriented Planning in Multi-Agent Systems](https://openreview.net/forum?id=EqcLAU6gyU) |  | 0 | Through the collaboration of multiple LLM-empowered agents possessing diverse expertise and tools, multi-agent systems achieve impressive progress in solving real-world problems. Given the user queries, the meta-agents, serving as the brain within multi-agent systems, are required to decompose the queries into multiple sub-tasks that can be allocated to suitable agents capable of solving them, so-called agent-oriented planning. In this study, we identify three critical design principles of agent-oriented planning, including solvability, completeness, and non-redundancy, to ensure that each sub-task can be effectively resolved, resulting in satisfactory responses to user queries. These principles further inspire us to propose AOP, a novel framework for agent-oriented planning in multi-agent systems, leveraging a fast task decomposition and allocation process followed by an effective and efficient evaluation via a reward model. According to the evaluation results, the meta-agent is also responsible for promptly making necessary adjustments to sub-tasks and scheduling. Besides, we integrate a feedback loop into AOP to further enhance the effectiveness and robustness of such a problem-solving process. Extensive experiments demonstrate the advancement of AOP in solving real-world problems compared to both single-agent systems and existing planning strategies for multi-agent systems. The source code is available at https://github.com/lalaliat/Agent-Oriented-Planning | Ao Li, Yuexiang Xie, Songze Li, Fugee Tsung, Bolin Ding, Yaliang Li |  |
| 769 |  |  [Context Clues: Evaluating Long Context Models for Clinical Prediction Tasks on EHR Data](https://openreview.net/forum?id=zg3ec1TdAP) |  | 0 | Foundation Models (FMs) trained on Electronic Health Records (EHRs) have achieved state-of-the-art results on numerous clinical prediction tasks. However, prior EHR FMs typically have context windows of $<$1k tokens, which prevents them from modeling full patient EHRs which can exceed 10k's of events. For making clinical predictions, both model performance and robustness to the unique properties of EHR data are crucial. Recent advancements in subquadratic long-context architectures (e.g. Mamba) offer a promising solution. However, their application to EHR data has not been well-studied. We address this gap by presenting the first systematic evaluation of the effect of context length on modeling EHR data. We find that longer context models improve predictive performance -- our Mamba-based model surpasses the prior state-of-the-art on 9/14 tasks on the EHRSHOT prediction benchmark. Additionally, we measure robustness to three unique, previously underexplored properties of EHR data: (1) the prevalence of \`\`copy-forwarded" diagnoses which create artificial token repetition in EHR sequences; (2) the irregular time intervals between EHR events which can lead to a wide range of timespans within a context window; and (3) the natural increase in disease complexity over time which makes later tokens in the EHR harder to predict than earlier ones. Stratifying our EHRSHOT results, we find that higher levels of each property correlate negatively with model performance (e.g., a 14% higher Brier loss between the least and most irregular patients), but that longer context models are more robust to more extreme levels of these properties. Our work highlights the potential for using long-context architectures to model EHR data, and offers a case study on how to identify and quantify new challenges in modeling sequential data motivated by domains outside of natural language. We release all of our model checkpoints and code. | Michael Wornow, Suhana Bedi, Miguel Angel Fuentes Hernandez, Ethan Steinberg, Jason Alan Fries, Christopher Ré, Sanmi Koyejo, Nigam Shah |  |
| 770 |  |  [Plastic Learning with Deep Fourier Features](https://openreview.net/forum?id=NIkfix2eDQ) |  | 0 | Deep neural networks can struggle to learn continually in the face of non-stationarity, a phenomenon known as loss of plasticity. In this paper, we identify underlying principles that lead to plastic algorithms. We provide theoretical results showing that linear function approximation, as well as a special case of deep linear networks, do not suffer from loss of plasticity. We then propose deep Fourier features, which are the concatenation of a sine and cosine in every layer, and we show that this combination provides a dynamic balance between the trainability obtained through linearity and the effectiveness obtained through the nonlinearity of neural networks. Deep networks composed entirely of deep Fourier features are highly trainable and sustain their trainability over the course of learning. Our empirical results show that continual learning performance can be improved by replacing ReLU activations with deep Fourier features combined with regularization. These results hold for different continual learning scenarios (e.g., label noise, class incremental learning, pixel permutations) on all major supervised learning datasets used for continual learning research, such as CIFAR10, CIFAR100, and tiny-ImageNet. | Alex Lewandowski, Dale Schuurmans, Marlos C. Machado |  |
| 771 |  |  [STAFF: Speculative Coreset Selection for Task-Specific Fine-tuning](https://openreview.net/forum?id=FAfxvdv1Dy) |  | 0 | Task-specific fine-tuning is essential for the deployment of large language models (LLMs), but it requires significant computational resources and time. Existing solutions have proposed coreset selection methods to improve data efficiency and reduce model training overhead, but they still have limitations: ❶ Overlooking valuable samples at high pruning rates, which degrades the coreset’s performance. ❷ Requiring high time overhead during coreset selection to fine-tune and evaluate the target LLM. In this paper, we introduce STAFF, a speculative coreset selection method. STAFF leverages a small model from the same family as the target LLM to efficiently estimate data scores and then verifies the scores on the target LLM to accurately identify and allocate more selection budget to important regions while maintaining coverage of easy regions. We evaluate STAFF on three LLMs and three downstream tasks and show that STAFF improves the performance of SOTA methods by up to 54.3% and reduces selection overhead by up to 70.5% at different pruning rates. Furthermore, we observe that the coreset selected by STAFF at low pruning rates (i.e., 20%) can even obtain better fine-tuning performance than the full dataset. | Xiaoyu Zhang, Juan Zhai, Shiqing Ma, Chao Shen, Tianlin Li, Weipeng Jiang, Yang Liu |  |
| 772 |  |  [Time-to-Event Pretraining for 3D Medical Imaging](https://openreview.net/forum?id=zcTLpIfj9u) |  | 0 | With the rise of medical foundation models and the growing availability of imaging data, scalable pretraining techniques offer a promising way to identify imaging biomarkers predictive of future disease risk. While current self-supervised methods for 3D medical imaging models capture local structural features like organ morphology, they fail to link pixel biomarkers with long-term health outcomes due to a missing context problem. Current approaches lack the temporal context necessary to identify biomarkers correlated with disease progression, as they rely on supervision derived only from images and concurrent text descriptions. To address this, we introduce time-to-event pretraining, a pretraining framework for 3D medical imaging models that leverages large-scale temporal supervision from paired, longitudinal electronic health records (EHRs). Using a dataset of 18,945 CT scans (4.2 million 2D images) and time-to-event distributions across thousands of EHR-derived tasks, our method improves outcome prediction, achieving an average AUROC increase of 23.7% and a 29.4% gain in Harrell’s C-index across 8 benchmark tasks. Importantly, these gains are achieved without sacrificing diagnostic classification performance. This study lays the foundation for integrating longitudinal EHR and 3D imaging data to advance clinical risk prediction. | Zepeng Frazier Huo, Jason Alan Fries, Alejandro Lozano, Jeya Maria Jose Valanarasu, Ethan Steinberg, Louis Blankemeier, Akshay S. Chaudhari, Curtis P. Langlotz, Nigam Shah |  |
| 773 |  |  [ExACT: Teaching AI Agents to Explore with Reflective-MCTS and Exploratory Learning](https://openreview.net/forum?id=GBIUbwW9D8) |  | 0 | Autonomous agents have demonstrated significant potential in automating complex multistep decision-making tasks. However, even state-of-the-art vision-language models (VLMs), such as GPT-4o, still fall short of human-level performance, particularly in intricate web environments and long-horizon planning tasks. To address these limitations, we introduce Reflective Monte Carlo Tree Search (R-MCTS), a novel test-time algorithm designed to enhance the ability of AI agents, e.g., powered by GPT-4o, to explore decision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating contrastive reflection, allowing agents to learn from past interactions and dynamically improve their search efficiency; and 2) using multi-agent debate to provide reliable state evaluation. Moreover, we improve the agent's performance by fine-tuning GPT-4o through self-learning, using R-MCTS generated tree traversals without any human-provided labels. On the challenging VisualWebArena benchmark, our GPT-4o-based R-MCTS agent achieves a 6% to 30% relative improvement across various tasks compared to the previous state-of-the-art. Additionally, we show that the knowledge gained from test-time search can be effectively transferred back to GPT-4o via fine-tuning. The fine-tuned GPT-4o matches 97\% of R-MCTS's performance while reducing compute usage by a factor of four at test time. Furthermore, qualitative results reveal that the fine-tuned GPT-4o model demonstrates the ability to explore the environment, evaluate a state, and backtrack to viable ones when it detects that the current state cannot lead to success. Moreover, our work demonstrates the compute scaling properties in both training - data collection with R-MCTS - and testing time. These results suggest a promising research direction to enhance VLMs' reasoning and planning capabilities for agentic applications via test-time search and self-learning. | Xiao Yu, Baolin Peng, Vineeth Vajipey, Hao Cheng, Michel Galley, Jianfeng Gao, Zhou Yu |  |
| 774 |  |  [TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models](https://openreview.net/forum?id=fCi4o83Mfs) |  | 0 | Existing benchmarks often highlight the remarkable performance achieved by state-of-the-art Multimodal Foundation Models (MFMs) in leveraging temporal context for video understanding. However, \*how well do the models truly perform visual temporal reasoning?\* Our study of existing benchmarks shows that this capability of MFMs is likely overestimated as many questions can be solved by using a single, few, or out-of-order frames. To systematically examine current visual temporal reasoning tasks, we propose three principles with corresponding metrics: (1) \*Multi-Frame Gain\*, (2) \*Frame Order Sensitivity\*, and (3) \*Frame Information Disparity\*. Following these principles, we introduce \*\*TOMATO\*\*, \*\*T\*\*emp\*\*O\*\*ral Reasoning \*\*M\*\*ultimod\*\*A\*\*l Evalua\*\*T\*\*i\*\*O\*\*n, a novel benchmark crafted to rigorously assess MFMs' temporal reasoning capabilities in video understanding. TOMATO comprises 1,484 carefully curated, \*human-annotated\* questions spanning \*six\* tasks (i.e. \*action count, direction, rotation, shape & trend, velocity & frequency, and visual cues\*), applied to 1,417 videos, including 805 self-recorded and -generated videos, that encompass human-centric, real-world, and simulated scenarios. Our comprehensive evaluation reveals a human-model performance gap of 57.3% with the best-performing model. Moreover, our in-depth analysis uncovers more fundamental limitations beyond this gap in current MFMs. While they can accurately recognize events in isolated frames, they fail to interpret these frames as a continuous sequence. We believe TOMATO will serve as a crucial testbed for evaluating the next-generation MFMs and as a call to the community to develop AI systems capable of comprehending the human world dynamics through the video modality. | Ziyao Shangguan, Chuhan Li, Yuxuan Ding, Yanan Zheng, Yilun Zhao, Tesca Fitzgerald, Arman Cohan |  |
| 775 |  |  [Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers](https://openreview.net/forum?id=yzloNYH3QN) |  | 0 | Information retrieval (IR) systems have played a vital role in modern digital life and have cemented their continued usefulness in this new era of generative AI via retrieval-augmented generation. With strong language processing capabilities and remarkable versatility, large language models (LLMs) have become popular choices for zero-shot re-ranking in IR systems. So far, LLM-based re-ranking methods rely on strong generative capabilities, which restricts their use to either specialized or powerful proprietary models. Given these restrictions, we ask: is autoregressive generation necessary and optimal for LLMs to perform re-ranking? We hypothesize that there are abundant signals relevant to re-ranking within LLMs that might not be used to their full potential via generation. To more directly leverage such signals, we propose in-context re-ranking (ICR), a novel method that leverages the change in attention pattern caused by the search query for accurate and efficient re-ranking. We assume that more relevant documents should receive more attention weights when an LLM is processing the query tokens, and leverage such signals for re-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration method using a content-free query. Due to the absence of generation, ICR only requires two ($O(1)$) forward passes to re-rank $N$ documents, making it substantially more efficient than generative re-ranking methods that require at least $O(N)$ forward passes. Our novel design also enables ICR to be applied to any LLM without specialized training while guaranteeing a well-formed ranking. Extensive experiments with two popular open-weight LLMs on standard single-hop and multi-hop information retrieval benchmarks show that ICR outperforms RankGPT while cutting the latency by more than 60% in practice. Through detailed analyses, we show that ICR's performance is specially strong on tasks that require more complex re-ranking signals, such as handling contextualization and contradiction between the query and passages, as well as information integration across multiple passages. Our findings call for further exploration on novel ways of utilizing open-weight LLMs beyond text generation. | Shijie Chen, Bernal Jimenez Gutierrez, Yu Su |  |
| 776 |  |  [Inspection and Control of Self-Generated-Text Recognition Ability in Llama3-8b-Instruct](https://openreview.net/forum?id=wWnsoLhHwt) |  | 0 | It has been reported that LLMs can recognize their own writing. As this has potential implications for AI safety, yet is relatively understudied, we investigate the phenomenon, seeking to establish: whether it robustly occurs at the behavioral level, how the observed behavior is achieved, and whether it can be controlled. First, we find that the Llama3-8b–Instruct chat model - but not the base Llama3-8b model - can reliably distinguish its own outputs from those of humans, and present evidence that the chat model is likely using its experience with its own outputs, acquired during post-training, to succeed at the writing recognition task. Second, we identify a vector in the residual stream of the model that is differentially activated when the model makes a correct self-written-text recognition judgment, show that the vector activates in response to information relevant to self-authorship, present evidence that the vector is related to the concept of \`\`self'' in the model, and demonstrate that the vector is causally related to the model’s ability to perceive and assert self-authorship. Finally, we show that the vector can be used to control both the model’s behavior and its perception, steering the model to claim or disclaim authorship by applying the vector to the model’s output as it generates it, and steering the model to believe or disbelieve it wrote arbitrary texts by applying the vector to them as the model reads them. | Christopher Ackerman, Nina Panickssery |  |
| 777 |  |  [Towards Learning High-Precision Least Squares Algorithms with Sequence Models](https://openreview.net/forum?id=snocoXIQXz) |  | 0 | This paper investigates whether sequence models can learn to perform numerical algorithms, e.g. gradient descent, on the fundamental problem of least squares. Our goal is to inherit two properties of standard algorithms from numerical analysis: (1) machine precision, i.e. we want to obtain solutions that are accurate to near floating point error, and (2) numerical generality, i.e. we want them to apply broadly across problem instances. We find that prior approaches using Transformers fail to meet these criteria, and identify limitations present in existing architectures and training procedures. First, we show that softmax Transformers struggle to perform high-precision multiplications, which prevents them from precisely learning numerical algorithms. Second, we identify an alternate class of architectures, comprised entirely of polynomials, that can efficiently represent high-precision gradient descent iterates. Finally, we investigate precision bottlenecks during training and address them via a high-precision training recipe that reduces stochastic gradient noise. Our recipe enables us to train two polynomial architectures, gated convolutions and linear attention, to perform gradient descent iterates on least squares problems. For the first time, we demonstrate the ability to train to near machine precision. Applied iteratively, our models obtain $100,000\times$ lower MSE than standard Transformers trained end-to-end and they incur a $10,000\times$ smaller generalization gap on out-of-distribution problems. We make progress towards end-to-end learning of numerical algorithms for least squares. | Jerry Weihong Liu, Jessica Grogan, Owen M. Dugan, Ashish Rao, Simran Arora, Atri Rudra, Christopher Ré |  |
| 778 |  |  [Learning Efficient Positional Encodings with Graph Neural Networks](https://openreview.net/forum?id=AWg2tkbydO) |  | 0 | Positional encodings (PEs) are essential for effective graph representation learning because they provide position awareness in inherently position-agnostic transformer architectures and increase the expressive capacity of Graph Neural Networks (GNNs). However, designing powerful and efficient PEs for graphs poses significant challenges due to the absence of canonical node ordering and the scale of the graph. In this work, we identify four key properties that graph PEs should satisfy: stability, expressive power, scalability, and genericness. We find that existing eigenvector-based PE methods often fall short of jointly satisfying these criteria. To address this gap, we introduce PEARL, a novel framework of learnable PEs for graphs. Our primary insight is that message-passing GNNs function as nonlinear mappings of eigenvectors, enabling the design of GNN architectures for generating powerful and efficient PEs. A crucial challenge lies in initializing node features in a manner that is both expressive and permutation equivariant. We tackle this by initializing GNNs with random node inputs or standard basis vectors, thereby unlocking the expressive power of message-passing operations, while employing statistical pooling functions to maintain permutation equivariance. Our analysis demonstrates that PEARL approximates equivariant functions of eigenvectors with linear complexity, while rigorously establishing its stability and high expressive power. Experimental evaluations show that PEARL outperforms lightweight versions of eigenvector-based PEs and achieves comparable performance to full eigenvector-based PEs, but with one or two orders of magnitude lower complexity. Our code is available at https://github.com/ehejin/Pearl-PE. | Charilaos I. Kanatsoulis, Evelyn Choi, Stefanie Jegelka, Jure Leskovec, Alejandro Ribeiro |  |
| 779 |  |  [Scale-Aware Contrastive Reverse Distillation for Unsupervised Medical Anomaly Detection](https://openreview.net/forum?id=HNOo4UNPBF) |  | 0 | Unsupervised anomaly detection using deep learning has garnered significant research attention due to its broad applicability, particularly in medical imaging where labeled anomalous data are scarce. While earlier approaches leverage generative models like autoencoders and generative adversarial networks (GANs), they often fall short due to overgeneralization. Recent methods explore various strategies, including memory banks, normalizing flows, self-supervised learning, and knowledge distillation, to enhance discrimination. Among these, knowledge distillation, particularly reverse distillation, has shown promise. Following this paradigm, we propose a novel scale-aware contrastive reverse distillation model that addresses two key limitations of existing reverse distillation methods: insufficient feature discriminability and inability to handle anomaly scale variations. Specifically, we introduce a contrastive student-teacher learning approach to derive more discriminative representations by generating and exploring out-of-normal distributions. Further, we design a scale adaptation mechanism to softly weight contrastive distillation losses at different scales to account for the scale variation issue. Extensive experiments on benchmark datasets demonstrate state-of-the-art performance, validating the efficacy of the proposed method. The code will be made publicly available. | Chunlei Li, Yilei Shi, Jingliang Hu, Xiao Xiang Zhu, Lichao Mou |  |
| 780 |  |  [Scalable Influence and Fact Tracing for Large Language Model Pretraining](https://openreview.net/forum?id=gLa96FlWwn) |  | 0 | Training data attribution (TDA) methods aim to attribute model outputs back to specific training examples, and the application of these methods to large language model (LLM) outputs could significantly advance model transparency and data curation. However, it has been challenging to date to apply these methods to the full scale of LLM pretraining. In this paper, we refine existing gradient-based methods to work effectively at scale, allowing us to retrieve influential examples for an 8B-parameter language model from a pretraining corpus of over 160B tokens with no need for subsampling or pre-filtering. Our method combines several techniques, including optimizer state correction, a task-specific Hessian approximation, and normalized encodings, which we find to be critical for performance at scale. In quantitative evaluations on a fact tracing task, our method performs best at identifying examples that influence model predictions, but classical, model-agnostic retrieval methods such as BM25 still perform better at finding passages which explicitly contain relevant facts. These results demonstrate a misalignment between factual \*attribution\* and causal \*influence\*. With increasing model size and training tokens, we find that influence more closely aligns with factual attribution. Finally, we examine different types of examples identified as influential by our method, finding that while many directly entail a particular fact, others support the same output by reinforcing priors on relation types, common entities, and names. We release our prompt set and model outputs, along with a web-based visualization tool to explore influential examples for factual predictions, commonsense reasoning, arithmetic, and open-ended generation for an 8B-parameter LLM. | Tyler A. Chang, Dheeraj Rajagopal, Tolga Bolukbasi, Lucas Dixon, Ian Tenney |  |
| 781 |  |  [AdvPaint: Protecting Images from Inpainting Manipulation via Adversarial Attention Disruption](https://openreview.net/forum?id=m73tETvFkX) |  | 0 | The outstanding capability of diffusion models in generating high-quality images poses significant threats when misused by adversaries. In particular, we assume malicious adversaries exploiting diffusion models for inpainting tasks, such as replacing a specific region with a celebrity. While existing methods for protecting images from manipulation in diffusion-based generative models have primarily focused on image-to-image and text-to-image tasks, the challenge of preventing unauthorized inpainting has been rarely addressed, often resulting in suboptimal protection performance. To mitigate inpainting abuses, we propose ADVPAINT, a novel defensive framework that generates adversarial perturbations that effectively disrupt the adversary’s inpainting tasks. ADVPAINT targets the self- and cross-attention blocks in a target diffusion inpainting model to distract semantic understanding and prompt interactions during image generation. ADVPAINT also employs a two-stage perturbation strategy, dividing the perturbation region based on an enlarged bounding box around the object, enhancing robustness across diverse masks of varying shapes and sizes. Our experimental results demonstrate that ADVPAINT’s perturbations are highly effective in disrupting the adversary’s inpainting tasks, outperforming existing methods; ADVPAINT attains over a 100-point increase in FID and substantial decreases in precision. | Joonsung Jeon, Woo Jae Kim, Suhyeon Ha, Sooel Son, SungEui Yoon |  |
| 782 |  |  [Mastering Task Arithmetic: τJp as a Key Indicator for Weight Disentanglement](https://openreview.net/forum?id=1VwWi6zbxs) |  | 0 | Model-editing techniques using task arithmetic have rapidly gained attention. Through task arithmetic, simply through arithmetic operations on the weights of pre-trained and fine-tuned models create desired models, such as multi-task models, models in which specific tasks are unsolvable, or domain-transferred models. However, task arithmetic faces challenges, such as poor reproducibility and the high cost associated with adjusting coefficients in the arithmetic operations on model parameters, which have limited its practical success. In this paper, we present three key contributions in the context of task addition and task negation within task arithmetic. First, we propose a new metric called $\tau$Jp which is based on the product of the task vector ($\tau$) and the Jacobian of the pre-trained model with respect to its weights. We show that $\tau$Jp has a causal relationship with the interference that occurs from arithmetic operations. Second, we show that introducing regularization to minimize $\tau$Jp significantly mitigates interference between task inference, which leads to the elimination of coefficient tuning and improved accuracy on each task. Third, in the context of incremental learning, we demonstrate that our $\tau$Jp regularization achieves more robust performance in environments where access to future tasks is unavailable, thus validating the scalability of the approach. Finally, we demonstrate that the $\tau$Jp regularizer further reinforces the performance of task arithmetic by leveraging publicly available fine-tuned models, offering practical benefits for real-world applications. Our code is available at https://github.com/katoro8989/tau-Jp_Task_Arithmetic | Kotaro Yoshida, Yuji Naraki, Takafumi Horie, Ryosuke Yamaki, Ryotaro Shimizu, Yuki Saito, Julian J. McAuley, Hiroki Naganuma |  |
| 783 |  |  [Generating CAD Code with Vision-Language Models for 3D Designs](https://openreview.net/forum?id=BLWaTeucYX) |  | 0 | Generative AI has transformed the fields of Design and Manufacturing by providing efficient and automated methods for generating and modifying 3D objects. One approach involves using Large Language Models (LLMs) to generate Computer- Aided Design (CAD) scripting code, which can then be executed to render a 3D object; however, the resulting 3D object may not meet the specified requirements. Testing the correctness of CAD generated code is challenging due to the complexity and structure of 3D objects (e.g., shapes, surfaces, and dimensions) that are not feasible in code. In this paper, we introduce CADCodeVerify, a novel approach to iteratively verify and improve 3D objects generated from CAD code. Our approach works by producing ameliorative feedback by prompting a Vision-Language Model (VLM) to generate and answer a set of validation questions to verify the generated object and prompt the VLM to correct deviations. To evaluate CADCodeVerify, we introduce, CADPrompt, the first benchmark for CAD code generation, consisting of 200 natural language prompts paired with expert-annotated scripting code for 3D objects to benchmark progress. Our findings show that CADCodeVerify improves VLM performance by providing visual feedback, enhancing the structure of the 3D objects, and increasing the success rate of the compiled program. When applied to GPT-4, CADCodeVerify achieved a 7.30% reduction in Point Cloud distance and a 5.0% improvement in success rate compared to prior work. | Kamel Alrashedy, Pradyumna Tambwekar, Zulfiqar Haider Zaidi, Megan Langwasser, Wei Xu, Matthew C. Gombolay |  |
| 784 |  |  [A Formal Framework for Understanding Length Generalization in Transformers](https://openreview.net/forum?id=U49N5V51rU) |  | 0 | A major challenge for transformers is generalizing to sequences longer than those observed during training. While previous works have empirically shown that transformers can either succeed or fail at length generalization depending on the task, theoretical understanding of this phenomenon remains limited. In this work, we introduce a rigorous theoretical framework to analyze length generalization in causal transformers with learnable absolute positional encodings. In particular, we characterize those functions that are identifiable in the limit from sufficiently long inputs with absolute positional encodings under an idealized inference scheme using a norm-based regularizer. This enables us to prove the possibility of length generalization for a rich family of problems. We experimentally validate the theory as a predictor of success and failure of length generalization across a range of algorithmic and formal language tasks. Our theory not only explains a broad set of empirical observations but also opens the way to provably predicting length generalization capabilities in transformers. | Xinting Huang, Andy Yang, Satwik Bhattamishra, Yash Raj Sarrof, Andreas Krebs, Hattie Zhou, Preetum Nakkiran, Michael Hahn |  |
| 785 |  |  [Palmbench: a comprehensive Benchmark of Compressed Large Language Models on Mobile Platforms](https://openreview.net/forum?id=xzSUdw6s76) |  | 0 | Deploying large language models (LLMs) locally on mobile devices is advantageous in scenarios where transmitting data to remote cloud servers is either undesirable due to privacy concerns or impractical due to network connection. Recent advancements have facilitated the local deployment of LLMs. However, local deployment also presents challenges, particularly in balancing quality (generative performance), latency, and throughput within the hardware constraints of mobile devices. In this paper, we introduce our lightweight, all-in-one automated benchmarking framework that allows users to evaluate LLMs on mobile devices. We provide a comprehensive benchmark of various popular LLMs with different quantization configurations (both weights and activations) across multiple mobile platforms with varying hardware capabilities. Unlike traditional benchmarks that assess full-scale models on high-end GPU clusters, we focus on evaluating resource efficiency (memory and power consumption) and harmful output for compressed models on mobile devices. Our key observations include: i) differences in energy efficiency and throughput across mobile platforms; ii) the impact of quantization on memory usage, GPU execution time, and power consumption; and iii) accuracy and performance degradation of quantized models compared to their non-quantized counterparts; and iv) the frequency of hallucinations and toxic content generated by compressed LLMs on mobile devices. | Yilong Li, Jingyu Liu, Hao Zhang, M. Badri Narayanan, Utkarsh Sharma, Shuai Zhang, Yijing Zeng, Jayaram Raghuram, Suman Banerjee |  |
| 786 |  |  [On Stochastic Contextual Bandits with Knapsacks in Small Budget Regime](https://openreview.net/forum?id=FCMpUOZkxi) |  | 0 | This paper studies stochastic contextual bandits with knapsack constraints (CBwK), where a learner observes a context, takes an action, receives a reward, and incurs a vector of costs at every round. The learner aims to maximize the cumulative rewards across $T$ rounds under the knapsack constraints with an initial budget of $B$. We study CBwK in the small budget regime where the budget $B = \Omega(\sqrt{T})$ and propose an Adaptive and Universal Primal--Dual algorithm (AUPD) that achieves strong regret performance: i) AUPD achieves $\tilde{O}((1 + \frac{\nu^\*}{\delta b})\sqrt{T})$ regret under the strict feasibility assumption without any prior information, matching the best-known bounds; ii) AUPD achieves $\tilde{O}(\sqrt{T}+ \frac{\nu^\*}{\sqrt{b}}T^{\frac{3}{4}})$ regret without strict feasibility assumption, which, to the best of our knowledge, is the first result in the literature. Here, the parameter $\nu^\*$ represents the optimal average reward; $b=B/T$ is the average budget and $\delta b$ is the feasibility/safety margin. We establish these strong results through the adaptive budget-aware design, which effectively balances reward maximization and budget consumption. We provide a new perspective on analyzing budget consumption using the Lyapunov drift method, along with a refined analysis of its cumulative variance. Our theory is further supported by experiments conducted on a large-scale dataset. | Hengquan Guo, Xin Liu |  |
| 787 |  |  [From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation](https://openreview.net/forum?id=JBXO05r4AV) |  | 0 | Recent advances in long-context large language models (LLMs) have led to the emerging paradigm of many-shot in-context learning (ICL), where it is observed that scaling many more demonstrating examples beyond the conventional few-shot setup in the context can lead to performance benefits. However, despite its promise, it is unclear what aspects dominate the benefits and whether simply scaling to more examples is the most effective way of improving many-shot ICL. In this work, we first provide an analysis on the factors driving many-shot ICL, and we find that 1) many-shot performance can still be attributed to often a few disproportionately influential examples and 2) identifying such influential examples ("optimize") and using them as demonstrations to regenerate new examples ("generate") can lead to further improvements. Inspired by the findings, we propose BRIDGE, an algorithm that alternates between the optimize step with Bayesian optimization to discover the influential sets of examples and the generate step to reuse this set to expand the reasoning paths of the examples back to the many-shot regime automatically. On Gemini, Claude, and Mistral LLMs of different sizes, we show BRIDGE led to significant improvements across a diverse set of tasks including symbolic reasoning, numerical reasoning and code generation. | Xingchen Wan, Han Zhou, Ruoxi Sun, Sercan Ö. Arik |  |
| 788 |  |  [Learning Neural Networks with Distribution Shift: Efficiently Certifiable Guarantees](https://openreview.net/forum?id=ed7zI29lRF) |  | 0 | We give the first provably efficient algorithms for learning neural networks with respect to distribution shift. We work in the Testable Learning with Distribution Shift framework (TDS learning) of Klivans et al. (2024), where the learner receives labeled examples from a training distribution and unlabeled examples from a test distribution and must either output a hypothesis with low test error or reject if distribution shift is detected. No assumptions are made on the test distribution. All prior work in TDS learning focuses on classification, while here we must handle the setting of nonconvex regression. Our results apply to real-valued networks with arbitrary Lipschitz activations and work whenever the training distribution has strictly sub-exponential tails. For training distributions that are bounded and hypercontractive, we give a fully polynomial-time algorithm for TDS learning one hidden-layer networks with sigmoid activations. We achieve this by importing classical kernel methods into the TDS framework using data-dependent feature maps and a type of kernel matrix that couples samples from both train and test distributions. | Gautam Chandrasekaran, Adam R. Klivans, Lin Lin Lee, Konstantinos Stavropoulos |  |
| 789 |  |  [Forgetting Transformer: Softmax Attention with a Forget Gate](https://openreview.net/forum?id=q2Lnyegkr8) |  | 0 | An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer's superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a "Pro" block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at [\`https://github.com/zhixuan-lin/forgetting-transformer\`](https://github.com/zhixuan-lin/forgetting-transformer). | Zhixuan Lin, Evgenii Nikishin, Xu Owen He, Aaron C. Courville |  |
| 790 |  |  [MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization](https://openreview.net/forum?id=R4q3cY3kQf) |  | 0 | Reinforcement learning (RL) algorithms aim to balance exploiting the current best strategy with exploring new options that could lead to higher rewards. Most common RL algorithms use undirected exploration, i.e., select random sequences of actions. Exploration can also be directed using intrinsic rewards, such as curiosity or model epistemic uncertainty. However, effectively balancing task and intrinsic rewards is challenging and often task-dependent. In this work, we introduce a framework, MaxInfoRL, for balancing intrinsic and extrinsic exploration. MaxInfoRL steers exploration towards informative transitions, by maximizing intrinsic rewards such as the information gain about the underlying task. When combined with Boltzmann exploration, this approach naturally trades off maximization of the value function with that of the entropy over states, rewards, and actions. We show that our approach achieves sublinear regret in the simplified setting of multi-armed bandits. We then apply this general formulation to a variety of off-policy model-free RL methods for continuous state-action spaces, yielding novel algorithms that achieve superior performance across hard exploration problems and complex scenarios such as visual control tasks. | Bhavya Sukhija, Stelian Coros, Andreas Krause, Pieter Abbeel, Carmelo Sferrazza |  |
| 791 |  |  [LancBiO: Dynamic Lanczos-aided Bilevel Optimization via Krylov Subspace](https://openreview.net/forum?id=wLmJIs1uqG) |  | 0 | Bilevel optimization, with broad applications in machine learning, has an intricate hierarchical structure. Gradient-based methods have emerged as a common approach to large-scale bilevel problems. However, the computation of the hyper-gradient, which involves a Hessian inverse vector product, confines the efficiency and is regarded as a bottleneck. To circumvent the inverse, we construct a sequence of low-dimensional approximate Krylov subspaces with the aid of the Lanczos process. As a result, the constructed subspace is able to dynamically and incrementally approximate the Hessian inverse vector product with less effort and thus leads to a favorable estimate of the hyper-gradient. Moreover, we propose a provable subspace-based framework for bilevel problems where one central step is to solve a small-size tridiagonal linear system. To the best of our knowledge, this is the first time that subspace techniques are incorporated into bilevel optimization. This successful trial not only enjoys $\mathcal{O}(\epsilon^{-1})$ convergence rate but also demonstrates efficiency in a synthetic problem and two deep learning tasks. | Yan Yang, Bin Gao, Yaxiang Yuan |  |
| 792 |  |  [Toward Understanding In-context vs. In-weight Learning](https://openreview.net/forum?id=aKJr5NnN8U) |  | 0 | It has recently been demonstrated empirically that in-context learning emerges in transformers when certain distributional properties are present in the training data, but this ability can also diminish upon further training. We provide a new theoretical understanding of these phenomena by identifying simplified distributional properties that give rise to the emergence and eventual disappearance of in-context learning. We do so by first analyzing a simplified model that uses a gating mechanism to choose between an in-weight and an in-context predictor. Through a combination of a generalization error and regret analysis we identify conditions where in-context and in-weight learning emerge. These theoretical findings are then corroborated experimentally by comparing the behaviour of a full transformer on the simplified distributions to that of the stylized model, demonstrating aligned results. We then extend the study to a full large language model, showing how fine-tuning on various collections of natural language prompts can elicit similar in-context and in-weight learning behaviour. | Bryan Chan, Xinyi Chen, András György, Dale Schuurmans |  |
| 793 |  |  [RouteLLM: Learning to Route LLMs from Preference Data](https://openreview.net/forum?id=8sSqNntaMr) |  | 0 | Large language models (LLMs) excel at a wide range of tasks, but choosing the right model often involves balancing performance and cost. Powerful models offer better results but are expensive, while smaller models are more cost-effective but less capable. To address this trade-off, we introduce a training framework for learning efficient router models that dynamically select between a stronger and weaker LLM during inference. Our framework leverages human preference data and employs data augmentation techniques to enhance performance. Evaluations on public benchmarks show that our approach can reduce costs by over 2 times without sacrificing response quality. Moreover, our routers exhibit strong generalization capabilities, maintaining performance even when routing between LLMs not included in training. This highlights the potential of our framework to deliver cost-effective, high-performance LLM solutions. | Isaac Ong, Amjad Almahairi, Vincent Wu, WeiLin Chiang, Tianhao Wu, Joseph E. Gonzalez, M. Waleed Kadous, Ion Stoica |  |
| 794 |  |  [Score-based Self-supervised MRI Denoising](https://openreview.net/forum?id=uNd289HjLi) |  | 0 | Magnetic resonance imaging (MRI) is a powerful noninvasive diagnostic imaging tool that provides unparalleled soft tissue contrast and anatomical detail. Noise contamination, especially in accelerated and/or low-field acquisitions, can significantly degrade image quality and diagnostic accuracy. Supervised learning based denoising approaches have achieved impressive performance but require high signal-to-noise ratio (SNR) labels, which are often unavailable. Self-supervised learning holds promise to address the label scarcity issue, but existing self-supervised denoising methods tend to oversmooth fine spatial features and often yield inferior performance than supervised methods. We introduce Corruption2Self (C2S), a novel score-based self-supervised framework for MRI denoising. At the core of C2S is a generalized denoising score matching (GDSM) loss, which extends denoising score matching to work directly with noisy observations by modeling the conditional expectation of higher-SNR images given further corrupted observations. This allows the model to effectively learn denoising across multiple noise levels directly from noisy data. Additionally, we incorporate a reparameterization of noise levels to stabilize training and enhance convergence, and introduce a detail refinement extension to balance noise reduction with the preservation of fine spatial features. Moreover, C2S can be extended to multi-contrast denoising by leveraging complementary information across different MRI contrasts. We demonstrate that our method achieves state-of-the-art performance among self-supervised methods and competitive results compared to supervised counterparts across varying noise conditions and MRI contrasts on the M4Raw and fastMRI dataset. The project website is available at: https://jiachentu.github.io/Corruption2Self-Self-Supervised-Denoising/. | Jiachen Tu, Yaokun Shi, Fan Lam |  |
| 795 |  |  [ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing](https://openreview.net/forum?id=4D0f16Vwc3) |  | 0 | Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to scale up model capacity without increasing the computation budget. However, vanilla TopK routers are trained in a discontinuous, non-differentiable way, limiting their performance and scalability. To address this issue, we propose ReMoE, a fully differentiable MoE architecture that offers a simple yet effective drop-in replacement for the conventional TopK+Softmax routing, utilizing ReLU as the router instead. We further propose methods to regulate the router's sparsity while balancing the load among experts. ReMoE’s continuous nature enables efficient dynamic allocation of computation across tokens and layers, while also exhibiting domain specialization. Our experiments demonstrate that ReMoE consistently outperforms vanilla TopK-routed MoE across various model sizes, expert counts, and levels of granularity. Furthermore, ReMoE exhibits superior scalability with respect to the number of experts, surpassing traditional MoE architectures. The implementation based on Megatron-LM is available at https://github.com/thu-ml/ReMoE. | Ziteng Wang, Jun Zhu, Jianfei Chen |  |
| 796 |  |  [HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks](https://openreview.net/forum?id=6fDjUoEQvm) |  | 0 | Mechanistic interpretability has made great strides in identifying neural network features (e.g., directions in hidden activation space) that mediate concepts (e.g., \*the birth year of a Nobel laureate\*) and enable predictable manipulation. Distributed alignment search (DAS) leverages supervision from counterfactual data to learn concept features within hidden states, but DAS assumes we can afford to conduct a brute force search over potential feature locations. To address this, we present HyperDAS, a transformer-based hypernetwork architecture that (1) automatically locates the token-positions of the residual stream that a concept is realized in and (2) learns features of those residual stream vectors for the concept. In experiments with Llama3-8B, HyperDAS achieves state-of-the-art performance on the RAVEL benchmark for disentangling concepts in hidden states. In addition, we review the design decisions we made to mitigate the concern that HyperDAS (like all powerful interpretabilty methods) might inject new information into the target model rather than faithfully interpreting it. | Jiuding Sun, Jing Huang, Sidharth Baskaran, Karel D'Oosterlinck, Christopher Potts, Michael Sklar, Atticus Geiger |  |
| 797 |  |  [GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models](https://openreview.net/forum?id=AjXkRZIvjB) |  | 0 | Recent advancements in Large Language Models (LLMs) have sparked interest in their mathematical reasoning capabilities. While performance on the widely popular GSM8K benchmark has improved, questions remain about whether reported evaluation metrics are reliable, and reasoning abilities of LLMs have advanced. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of models. Our findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and demonstrate that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is due to the fact that current LLMs are not capable of genuine logical reasoning; instead, they attempt to replicate the reasoning steps observed in their training data. When we add a single clause that appears relevant to the question, we observe significant performance drops (up to 65%) across all state-of-the-art models, even though the added clause does not contribute to the reasoning chain needed to reach the final answer. Overall, our work provides a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning. | Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, Mehrdad Farajtabar |  |
| 798 |  |  [SEMDICE: Off-policy State Entropy Maximization via Stationary Distribution Correction Estimation](https://openreview.net/forum?id=rJ5g8ueQaI) |  | 0 | In the unsupervised pre-training for reinforcement learning, the agent aims to learn a prior policy for downstream tasks without relying on task-specific reward functions. We focus on state entropy maximization (SEM), where the goal is to learn a policy that maximizes the entropy of the state's stationary distribution. In this paper, we introduce SEMDICE, a principled off-policy algorithm that computes an SEM policy from an arbitrary off-policy dataset, which optimizes the policy directly within the space of stationary distributions. SEMDICE computes a single, stationary Markov state-entropy-maximizing policy from an arbitrary off-policy dataset. Experimental results demonstrate that SEMDICE outperforms baseline algorithms in maximizing state entropy while achieving the best adaptation efficiency for downstream tasks among SEM-based unsupervised RL pre-training methods. | Jongmin Lee, Meiqi Sun, Pieter Abbeel |  |
| 799 |  |  [Identifiability for Gaussian Processes with Holomorphic Kernels](https://openreview.net/forum?id=FUaDMRVrbS) |  | 0 | Gaussian processes (GPs) are widely recognized for their robustness and flexibility across various domains, including machine learning, time series, spatial statistics, and biomedicine. In addition to their common usage in regression tasks, GP kernel parameters are frequently interpreted in various applications. For example, in spatial transcriptomics, estimated kernel parameters are used to identify spatial variable genes, which exhibit significant expression patterns across different tissue locations. However, before these parameters can be meaningfully interpreted, it is essential to establish their identifiability. Existing studies of GP parameter identifiability have focused primarily on Mat\'ern-type kernels, as their spectral densities allow for more established mathematical tools. In many real-world applications, particuarly in time series analysis, other kernels such as the squared exponential, periodic, and rational quadratic kernels, as well as their combinations, are also widely used. These kernels share the property of being holomorphic around zero, and their parameter identifiability remains underexplored. In this paper, we bridge this gap by developing a novel theoretical framework for determining kernel parameter identifiability for kernels holomorphic near zero. Our findings enable practitioners to determine which parameters are identifiable in both existing and newly constructed kernels, supporting application-specific interpretation of the identifiable parameters, and highlighting non-identifiable parameters that require careful interpretation. | Ameer Qaqish, Didong Li |  |
| 800 |  |  [Discovering Group Structures via Unitary Representation Learning](https://openreview.net/forum?id=Tz8Li6G2xU) |  | 0 | Discovering group structures within data poses a fundamental challenge across diverse scientific domains. A key obstacle is the non-differentiable nature of group axioms, hindering their integration into deep learning frameworks. To address this, we introduce a novel differentiable approach leveraging the representation theory of finite groups. Our method features a unique network architecture that models interactions between group elements via matrix multiplication of their representations, along with a regularizer promoting the unitarity of these representations. The interplay between the network architecture and the unitarity condition implicitly encourages the emergence of valid group structures. Evaluations demonstrate our method's ability to accurately recover group operations and their unitary representations from partial observations, achieving significant improvements in sample efficiency and a $\times 1000$ speedup over the state of the art. This work lays the foundation for a promising new paradigm in automated algebraic structure discovery, with potential applications across various domains, including automatic symmetry discovery for geometric deep learning. | Dongsung Huh |  |
| 801 |  |  [Image Watermarks are Removable using Controllable Regeneration from Clean Noise](https://openreview.net/forum?id=mDKxlfraAn) |  | 0 | Image watermark techniques provide an effective way to assert ownership, deter misuse, and trace content sources, which has become increasingly essential in the era of large generative models. A critical attribute of watermark techniques is their robustness against various manipulations. In this paper, we introduce a watermark removal approach capable of effectively nullifying state-of-the-art watermarking techniques. Our primary insight involves regenerating the watermarked image starting from a \textbf{clean Gaussian noise} via a controllable diffusion model, utilizing the extracted semantic and spatial features from the watermarked image. The semantic control adapter and the spatial control network are specifically trained to control the denoising process towards ensuring image quality and enhancing consistency between the cleaned image and the original watermarked image. To achieve a smooth trade-off between watermark removal performance and image consistency, we further propose an adjustable and controllable regeneration scheme. This scheme adds varying numbers of noise steps to the latent representation of the watermarked image, followed by a controlled denoising process starting from this noisy latent representation. As the number of noise steps increases, the latent representation progressively approaches clean Gaussian noise, facilitating the desired trade-off. We apply our watermark removal methods across various watermarking techniques, and the results demonstrate that our methods offer superior visual consistency/quality and enhanced watermark removal performance compared to existing regeneration approaches. Our code is available at \url{https://github.com/yepengliu/CtrlRegen}. | Yepeng Liu, Yiren Song, Hai Ci, Yu Zhang, Haofan Wang, Mike Zheng Shou, Yuheng Bu |  |
| 802 |  |  [Point Cluster: A Compact Message Unit for Communication-Efficient Collaborative Perception](https://openreview.net/forum?id=54XlM8Clkg) |  | 0 | The objective of the collaborative perception task is to enhance the individual agent's perception capability through message communication among neighboring agents. A central challenge lies in optimizing the inherent trade-off between perception ability and communication cost. To tackle this bottleneck issue, we argue that a good message unit should encapsulate both semantic and structural information in a sparse format, a feature not present in prior approaches. In this paper, we innovatively propose a compact message unit, namely point cluster, whose core idea is to represent potential objects efficiently with explicitly decoupled low-level structure information and high-level semantic information. Building upon this new message unit, we propose a comprehensive framework CPPC for communication-efficient collaborative perception. The core principle of CPPC is twofold: first, through strategical point sampling, structure information can be well preserved with a few key points, which can significantly reduce communication cost; second, the sequence format of point clusters enables efficient message aggregation by set matching and merging, thereby eliminating unnecessary computation generated when aligning squared BEV maps, especially for long-range collaboration. To handle time latency and pose errors encountered in real-world scenarios, we also carefully design parameter-free solutions that can adapt to different noisy levels without finetuning. Experiments on two widely recognized collaborative perception benchmarks showcase the superior performance of our method compared to the previous state-of-the-art approaches. | Zihan Ding, Jiahui Fu, Si Liu, Hongyu Li, Siheng Chen, Hongsheng Li, Shifeng Zhang, Xu Zhou |  |
| 803 |  |  [A Theoretical Analysis of Self-Supervised Learning for Vision Transformers](https://openreview.net/forum?id=Antib6Uovh) |  | 0 | Self-supervised learning has become a cornerstone in computer vision, primarily divided into reconstruction-based methods like masked autoencoders (MAE) and discriminative methods such as contrastive learning (CL). Recent empirical observations reveal that MAE and CL capture different types of representations: CL tends to focus on global patterns, while MAE adeptly captures \*\*both global and subtle local\*\* information simultaneously. Despite a flurry of recent empirical investigations to shed light on this difference, theoretical understanding remains limited, especially on the dominant architecture \*\*vision transformers\*\* (ViTs). In this paper, to provide rigorous insights, we model the visual data distribution by considering two types of spatial features: dominant global features and comparatively minuscule local features, and study the impact of imbalance among these features. We analyze the training dynamics of one-layer softmax-based ViTs on both MAE and CL objectives using gradient descent. Our analysis shows that as the degree of feature imbalance varies, ViTs trained with the MAE objective effectively learn both global and local features to achieve near-optimal reconstruction, while the CL-trained ViTs favor predominantly global features, even under mild imbalance. These results provide a theoretical explanation for distinct behaviors of MAE and CL observed in empirical studies. | Yu Huang, Zixin Wen, Yuejie Chi, Yingbin Liang |  |
| 804 |  |  [ProteinBench: A Holistic Evaluation of Protein Foundation Models](https://openreview.net/forum?id=BksqWM8737) |  | 0 | Recent years have witnessed a surge in the development of protein foundation models, significantly improving performance in protein prediction and generative tasks ranging from 3D structure prediction and protein design to conformational dynamics. However, the capabilities and limitations associated with these models remain poorly understood due to the absence of a unified evaluation framework. To fill this gap, we introduce ProteinBench, a holistic evaluation framework designed to enhance the transparency of protein foundation models. Our approach consists of three key components: (i) A taxonomic classification of tasks that broadly encompass the main challenges in the protein domain, based on the relationships between different protein modalities; (ii) A multi-metric evaluation approach that assesses performance across four key dimensions: quality, novelty, diversity, and robustness; and (iii) In-depth analyses from various user objectives, providing a holistic view of model performance. Our comprehensive evaluation of protein foundation models reveals several key findings that shed light on their current capabilities and limitations. To promote transparency and facilitate further research, we release the evaluation dataset, code, and a public leaderboard publicly for further analysis and a general modular toolkit. We intend for ProteinBench to be a living benchmark for establishing a standardized, in-depth evaluation framework for protein foundation models, driving their development and application while fostering collaboration within the field. | Fei Ye, Zaixiang Zheng, Dongyu Xue, Yuning Shen, Lihao Wang, Yiming Ma, Yan Wang, Xinyou Wang, Xiangxin Zhou, Quanquan Gu |  |
| 805 |  |  [LeanQuant: Accurate and Scalable Large Language Model Quantization with Loss-error-aware Grid](https://openreview.net/forum?id=ISqx8giekS) |  | 0 | Large language models (LLMs) have shown immense potential across various domains, but their high memory requirements and inference costs remain critical challenges for deployment. Post-training quantization (PTQ) has emerged as a promising technique to reduce memory requirements and decoding latency. However, recent accurate quantization methods often depend on specialized computations or custom data formats to achieve better model quality, which limits their compatibility with popular frameworks, as they require dedicated inference kernels tailored to specific hardware and software platforms, hindering wider adoption. Furthermore, many competitive methods have high resource requirements and computational overhead for quantizing models, making it challenging to scale them to hundreds of billions of parameters. In response to these challenges, we propose LeanQuant (Loss-error-aware network Quantization), a novel quantization method that is accurate, versatile, and scalable. In the existing popular iterative loss-error-based quantization framework, we identify a critical limitation in prior methods: the min-max affine quantization grid fails to preserve model quality due to outliers in inverse Hessian diagonals. To overcome this fundamental issue, we propose learning loss-error-aware grids, instead of using non-adaptive min-max affine grids. Our approach not only produces quantized models that are more accurate but also generalizes to a wider range of quantization types, including affine and non-uniform quantization, enhancing compatibility with more frameworks. Extensive experiments with recent LLMs demonstrate that LeanQuant is highly accurate, comparing favorably against competitive baselines in model quality, and scalable, achieving very accurate quantization of Llama-3.1 405B, one of the largest open-source LLMs to date, using two Quadro RTX 8000-48GB GPUs in 21 hours. Our code is available at https://github.com/LeanModels/LeanQuant. | Tianyi Zhang, Anshumali Shrivastava |  |
| 806 |  |  [Strategist: Self-improvement of LLM Decision Making via Bi-Level Tree Search](https://openreview.net/forum?id=gfI9v7AbFg) |  | 0 | Traditional reinforcement learning and planning require a lot of data and training to develop effective strategies. On the other hand, large language models (LLMs) can generalize well and perform tasks without prior training but struggle with complex planning and decision-making. We introduce \*\*STRATEGIST\*\*, a new approach that combines the strengths of both methods. It uses LLMs to generate and update high-level strategies in text form, while a Monte Carlo Tree Search (MCTS) algorithm refines and executes them. STRATEGIST is a general framework that optimizes strategies through self-play simulations without requiring any training data. We test STRATEGIST in competitive, multi-turn games with partial information, such as \*\*Game of Pure Strategy (GOPS)\*\* and \*\*The Resistance: Avalon\*\*, a multi-agent hidden-identity discussion game. Our results show that STRATEGIST-based agents outperform traditional reinforcement learning models, other LLM-based methods, and existing LLM agents while achieving performance levels comparable to human players. | Jonathan Light, Min Cai, Weiqin Chen, Guanzhi Wang, Xiusi Chen, Wei Cheng, Yisong Yue, Ziniu Hu |  |
| 807 |  |  [DataMan: Data Manager for Pre-training Large Language Models](https://openreview.net/forum?id=eNbA8Fqir4) |  | 0 | The performance emergence of large language models (LLMs) driven by data scaling laws makes the selection of pre-training data increasingly important. However, existing methods rely on limited heuristics and human intuition, lacking comprehensive and clear guidelines. To address this, we are inspired by \*\`\`reverse thinking''\* -- prompting LLMs to self-identify which criteria benefit its performance. As its pre-training capabilities are related to perplexity (PPL), we derive 14 quality criteria from the causes of text perplexity anomalies and introduce 15 common application domains to support domain mixing. In this paper, we train a \*\*Data\*\* \*\*Man\*\*ager (\*\*DataMan\*\*) to learn quality ratings and domain recognition from pointwise rating, and use it to annotate a 447B token pre-training corpus with 14 quality ratings and domain type. Our experiments validate our approach, using DataMan to select 30B tokens to train a 1.3B-parameter language model, demonstrating significant improvements in in-context learning (ICL), perplexity, and instruction-following ability over the state-of-the-art baseline. The best-performing model, based on the \*Overall Score l=5\* surpasses a model trained with 50% more data using uniform sampling. We continue pre-training with high-rated, domain-specific data annotated by DataMan to enhance domain-specific ICL performance and thus verify DataMan's domain mixing ability. Our findings emphasize the importance of quality ranking, the complementary nature of quality criteria, and their low correlation with perplexity, analyzing misalignment between PPL and ICL performance. We also thoroughly analyzed our pre-training dataset, examining its composition, the distribution of quality ratings, and the original document sources. | Ru Peng, Kexin Yang, Yawen Zeng, Junyang Lin, Dayiheng Liu, Junbo Zhao |  |
| 808 |  |  [EditRoom: LLM-parameterized Graph Diffusion for Composable 3D Room Layout Editing](https://openreview.net/forum?id=Y2Dh8rWwlb) |  | 0 | Given the steep learning curve of professional 3D software and the time- consuming process of managing large 3D assets, language-guided 3D scene editing has significant potential in fields such as virtual reality, augmented reality, and gaming. However, recent approaches to language-guided 3D scene editing either require manual interventions or focus only on appearance modifications without supporting comprehensive scene layout changes. In response, we propose EditRoom, a unified framework capable of executing a variety of layout edits through natural language commands, without requiring manual intervention. Specifically, EditRoom leverages Large Language Models (LLMs) for command planning and generates target scenes using a diffusion-based method, enabling six types of edits: rotate, translate, scale, replace, add, and remove. To address the lack of data for language-guided 3D scene editing, we have developed an automatic pipeline to augment existing 3D scene synthesis datasets and introduced EditRoom-DB, a large-scale dataset with 83k editing pairs, for training and evaluation. Our experiments demonstrate that our approach consistently outperforms other baselines across all metrics, indicating higher accuracy and coherence in language-guided scene layout editing. | Kaizhi Zheng, Xiaotong Chen, Xuehai He, Jing Gu, Linjie Li, Zhengyuan Yang, Kevin Lin, Jianfeng Wang, Lijuan Wang, Xin Eric Wang |  |
| 809 |  |  [LocoVR: Multiuser Indoor Locomotion Dataset in Virtual Reality](https://openreview.net/forum?id=9mBodivRIo) |  | 0 | Understanding human locomotion is crucial for AI agents such as robots, particularly in complex indoor home environments. Modeling human trajectories in these spaces requires insight into how individuals maneuver around physical obstacles and manage social navigation dynamics. These dynamics include subtle behaviors influenced by proxemics - the social use of space, such as stepping aside to allow others to pass or choosing longer routes to avoid collisions. Previous research has developed datasets of human motion in indoor scenes, but these are often limited in scale and lack the nuanced social navigation dynamics common in home environments. To address this, we present LocoVR, a dataset of 7000+ two-person trajectories captured in virtual reality from over 130 different indoor home environments. LocoVR provides accurate trajectory and precise spatial information, along with rich examples of socially-motivated movement behaviors. For example, the dataset captures instances of individuals navigating around each other in narrow spaces, adjusting paths to respect personal boundaries in living areas, and coordinating movements in high-traffic zones like entryways and kitchens. Our evaluation shows that LocoVR significantly enhances model performance in three practical indoor tasks utilizing human trajectories, and demonstrates predicting socially-aware navigation patterns in home environments. | Kojiro Takeyama, Yimeng Liu, Misha Sra |  |
| 810 |  |  [Prompting Fairness: Integrating Causality to Debias Large Language Models](https://openreview.net/forum?id=7GKbQ1WT1C) |  | 0 | Large language models (LLMs), despite their remarkable capabilities, are susceptible to generating biased and discriminatory responses. As LLMs increasingly influence high-stakes decision-making (e.g., hiring and healthcare), mitigating these biases becomes critical. In this work, we propose a causality-guided debiasing framework to tackle social biases, aiming to reduce the objectionable dependence between LLMs' decisions and the social information in the input. Our framework introduces a novel perspective to identify how social information can affect an LLM's decision through different causal pathways. Leveraging these causal insights, we outline principled prompting strategies that regulate these pathways through selection mechanisms. This framework not only unifies existing prompting-based debiasing techniques, but also opens up new directions for reducing bias by encouraging the model to prioritize fact-based reasoning over reliance on biased social cues. We validate our framework through extensive experiments on real-world datasets across multiple domains, demonstrating its effectiveness in debiasing LLM decisions, even with only black-box access to the model. | Jingling Li, Zeyu Tang, Xiaoyu Liu, Peter Spirtes, Kun Zhang, Liu Leqi, Yang Liu |  |
| 811 |  |  [GReaTer: Gradients Over Reasoning Makes Smaller Language Models Strong Prompt Optimizers](https://openreview.net/forum?id=fWRBheSJth) |  | 0 | The effectiveness of large language models (LLMs) is closely tied to the design of prompts, making prompt optimization essential for enhancing their performance across a wide range of tasks. Although recent advancements have focused on automating prompt engineering, many existing approaches rely exclusively on textual feedback, refining prompts based solely on inference errors identified by large, computationally expensive LLMs. Unfortunately, smaller models struggle to generate high-quality feedback, resulting in complete dependence on large LLM judgment. Moreover, these methods fail to leverage more direct and finer-grained information, such as gradients, due to operating purely in text space. To this end, we introduce, we introduce \*GReaTer\*, a novel prompt optimization technique that directly incorporates \*gradient information over task-specific reasoning\*. By utilizing task loss gradients, \*GReaTer\* enables self-optimization of prompts for smaller, lightweight language models (LM) without the need for costly closed-source LLMs, while maintaining reasonable prompt structures. This allows high-performance prompt optimization without dependence on massive LLMs, closing the gap between smaller models and the sophisticated reasoning often needed for prompt refinement. Extensive evaluations across diverse tasks demonstrate that \ours consistently outperforms previous methods, even those reliant on powerful LLMs. Additionally, \*GReaTer\*-optimized prompts frequently exhibit better transferability and, in some cases, boost task performance to levels comparable to or surpassing those achieved by larger language models, highlighting the effectiveness of \*"gradient over reasoning"\*-based prompt optimization. Code of \*GReaTer\* is available at: https://github.com/psunlpgroup/GreaTer | Sarkar Snigdha Sarathi Das, Ryo Kamoi, Bo Pang, Yusen Zhang, Caiming Xiong, Rui Zhang |  |
| 812 |  |  [Efficient Causal Decision Making with One-sided Feedback](https://openreview.net/forum?id=UWdPsY7agk) |  | 0 | We study a class of decision-making problems with one-sided feedback, where outcomes are only observable for specific actions. A typical example is bank loans, where the repayment status is known only if a loan is approved and remains undefined if rejected. In such scenarios, conventional approaches to causal decision evaluation and learning from observational data are not directly applicable. In this paper, we introduce a novel value function to evaluate decision rules that addresses the issue of undefined counterfactual outcomes. Without assuming no unmeasured confounders, we establish the identification of the value function using shadow variables. Furthermore, leveraging semiparametric theory, we derive the efficiency bound for the proposed value function and develop efficient methods for decision evaluation and learning. Numerical experiments and a real-world data application demonstrate the empirical performance of our proposed methods. | Jianing Chu, Shu Yang, Wenbin Lu, Pulak Ghosh |  |
| 813 |  |  [LANTERN: Accelerating Visual Autoregressive Models with Relaxed Speculative Decoding](https://openreview.net/forum?id=98d7DLMGdt) |  | 0 | Auto-Regressive (AR) models have recently gained prominence in image generation, often matching or even surpassing the performance of diffusion models. However, one major limitation of AR models is their sequential nature, which processes tokens one at a time, slowing down generation compared to models like GANs or diffusion-based methods that operate more efficiently. While speculative decoding has proven effective for accelerating LLMs by generating multiple tokens in a single forward, its application in visual AR models remains largely unexplored. In this work, we identify a challenge in this setting, which we term \textit{token selection ambiguity}, wherein visual AR models frequently assign uniformly low probabilities to tokens, hampering the performance of speculative decoding. To overcome this challenge, we propose a relaxed acceptance condition referred to as LANTERN that leverages the interchangeability of tokens in latent space. This relaxation restores the effectiveness of speculative decoding in visual AR models by enabling more flexible use of candidate tokens that would otherwise be prematurely rejected. Furthermore, by incorporating a total variation distance bound, we ensure that these speed gains are achieved without significantly compromising image quality or semantic coherence. Experimental results demonstrate the efficacy of our method in providing a substantial speed-up over speculative decoding. In specific, compared to a na\"ive application of the state-of-the-art speculative decoding, LANTERN increases speed-ups by $\mathbf{1.75}\times$ and $\mathbf{1.82}\times$, as compared to greedy decoding and random sampling, respectively, when applied to LlamaGen, a contemporary visual AR model. The code is publicly available at \url{https://github.com/jadohu/LANTERN}. | Doohyuk Jang, Sihwan Park, June Yong Yang, Yeonsung Jung, Jihun Yun, Souvik Kundu, Sungyub Kim, Eunho Yang |  |
| 814 |  |  [LICO: Large Language Models for In-Context Molecular Optimization](https://openreview.net/forum?id=yu1vqQqKkx) |  | 0 | Optimizing black-box functions is a fundamental problem in science and engineering. To solve this problem, many approaches learn a surrogate function that estimates the underlying objective from limited historical evaluations. Large Language Models (LLMs), with their strong pattern-matching capabilities via pretraining on vast amounts of data, stand out as a potential candidate for surrogate modeling. However, directly prompting a pretrained language model to produce predictions is not feasible in many scientific domains due to the scarcity of domain-specific data in the pretraining corpora and the challenges of articulating complex problems in natural language. In this work, we introduce LICO, a general-purpose model that extends arbitrary base LLMs for black-box optimization, with a particular application to the molecular domain. To achieve this, we equip the language model with a separate embedding layer and prediction layer, and train the model to perform in-context predictions on a diverse set of functions defined over the domain. Once trained, LICO can generalize to unseen molecule properties simply via in-context prompting. LICO performs competitively on PMO, a challenging molecular optimization benchmark comprising 23 objective functions, and achieves state-of-the-art performance on its low-budget version PMO-1K. | Tung Nguyen, Aditya Grover |  |
| 815 |  |  [Forking Paths in Neural Text Generation](https://openreview.net/forum?id=8RCmNLeeXx) |  | 0 | Estimating uncertainty in Large Language Models (LLMs) is important for properly evaluating LLMs, and ensuring safety for users. However, prior approaches to uncertainty estimation focus on the final answer in generated text, ignoring intermediate steps that might dramatically impact the outcome. We hypothesize that there exist key forking tokens, such that re-sampling the system at those specific tokens, but not others, leads to very different outcomes. To test this empirically, we develop a novel approach to representing uncertainty dynamics across individual tokens of text generation, and applying statistical models to test our hypothesis. Our approach is highly flexible: it can be applied to any dataset and any LLM, without fine tuning or accessing model weights. We use our method to analyze LLM responses on 7 different tasks across 4 domains, spanning a wide range of typical use cases. We find many examples of forking tokens, including surprising ones such as a space character instead of a colon, suggesting that LLMs are often just a single token away from saying something very different. | Eric J. Bigelow, Ari Holtzman, Hidenori Tanaka, Tomer David Ullman |  |
| 816 |  |  [BRAID: Input-driven Nonlinear Dynamical Modeling of Neural-Behavioral Data](https://openreview.net/forum?id=3usdM1AuI3) |  | 0 | Neural populations exhibit complex recurrent structures that drive behavior, while continuously receiving and integrating external inputs from sensory stimuli, upstream regions, and neurostimulation. However, neural populations are often modeled as autonomous dynamical systems, with little consideration given to the influence of external inputs that shape the population activity and behavioral outcomes. Here, we introduce BRAID, a deep learning framework that models nonlinear neural dynamics underlying behavior while explicitly incorporating any measured external inputs. Our method disentangles intrinsic recurrent neural population dynamics from the effects of inputs by including a forecasting objective within input-driven recurrent neural networks. BRAID further prioritizes the learning of intrinsic dynamics that are related to a behavior of interest by using a multi-stage optimization scheme. We validate BRAID with nonlinear simulations, showing that it can accurately learn the intrinsic dynamics shared between neural and behavioral modalities. We then apply BRAID to motor cortical activity recorded during a motor task and demonstrate that our method more accurately fits the neural-behavioral data by incorporating measured sensory stimuli into the model and improves the forecasting of neural-behavioral data compared with various baseline methods, whether input-driven or not. | Parsa Vahidi, Omid G. Sani, Maryam Shanechi |  |
| 817 |  |  [LLM-based Typed Hyperresolution for Commonsense Reasoning with Knowledge Bases](https://openreview.net/forum?id=wNobG8bV5Q) |  | 0 | Large language models (LLM) are being increasingly applied to tasks requiring commonsense reasoning. Despite their outstanding potential, the reasoning process of LLMs is prone to errors and hallucinations that hinder their applicability, especially in high-stakes scenarios. Several works have attempted to enhance commonsense reasoning performance of LLMs by (i) using prompting styles that elicit more accurate reasoning, (ii) utilizing the LLM as a semantic parser for a symbolic reasoner, or (iii) enforcing the LLM to simulate a logical inference rule. However, all these solutions have critical limitations: they are unable to leverage the internal commonsense knowledge of the LLM in tandem with an axiomatic knowledge base, they lack a mechanism to reliably repair erroneous inference steps, and their application is restricted to small knowledge bases that fit the context limit of the LLM. In this work, we present LLM-based Typed Hyperresolution (LLM-TH), a logical commonsense reasoning framework that leverages "theory resolution", a concept from classical logical inference which enables integrating LLMs into the "resolution" inference rule, thus mitigating reasoning errors and hallucinations and enabling verification of the reasoning procedure. LLM-TH is also equipped with a mechanism for repairing erroneous inference steps supported by theoretical guarantees. Using "Hyperresolution" and "Typed inference" schemes, we show that LLM-TH can efficiently reason over large knowledge bases consisting of tens of thousands of rules with arbitrary predicate arities. Our experiments on three diverse language-based reasoning tasks—preference reasoning, multi-domain deductive reasoning, and geographical question answering—showcase that LLM-TH, using merely a BART 406M parameter NLI entailment model, significantly reduces reasoning errors compared to baselines using Llama3-70B, Gemini1.5-Flash, GPT-3.5-Turbo, and Mixtral-46.7B. | Armin Toroghi, Ali Pesaranghader, Tanmana Sadhu, Scott Sanner |  |
| 818 |  |  [What's the Move? Hybrid Imitation Learning via Salient Points](https://openreview.net/forum?id=r0pLGGcuY6) |  | 0 | While imitation learning (IL) offers a promising framework for teaching robots various behaviors, learning complex tasks remains challenging. Existing IL policies struggle to generalize effectively across visual and spatial variations even for simple tasks. In this work, we introduce \*\*SPHINX\*\*: \*\*S\*\*alient \*\*P\*\*oint-based \*\*H\*\*ybrid \*\*I\*\*mitatio\*\*N\*\* and e\*\*X\*\*ecution, a flexible IL policy that leverages multimodal observations (point clouds and wrist images), along with a hybrid action space of low-frequency, sparse waypoints and high-frequency, dense end effector movements. Given 3D point cloud observations, SPHINX learns to infer task-relevant points within a point cloud, or \*salient points\*, which support spatial generalization by focusing on semantically meaningful features. These salient points serve as anchor points to predict waypoints for long-range movement, such as reaching target poses in free-space. Once near a salient point, SPHINX learns to switch to predicting dense end-effector movements given close-up wrist images for precise phases of a task. By exploiting the strengths of different input modalities and action representations for different manipulation phases, SPHINX tackles complex tasks in a sample-efficient, generalizable manner. Our method achieves \*\*86.7%\*\* success across 4 real-world and 2 simulated tasks, outperforming the next best state-of-the-art IL baseline by \*\*41.1%\*\* on average across \*\*440\*\* real world trials. SPHINX additionally generalizes to novel viewpoints, visual distractors, spatial arrangements, and execution speeds with a \*\*1.7x\*\* speedup over the most competitive baseline. Our website (http://sphinx-manip.github.io) provides open-sourced code for data collection, training, and evaluation, along with supplementary videos. | Priya Sundaresan, Hengyuan Hu, Quan Vuong, Jeannette Bohg, Dorsa Sadigh |  |
| 819 |  |  [PEARL: Towards Permutation-Resilient LLMs](https://openreview.net/forum?id=txoJvjfI9w) |  | 0 | The in-context learning (ICL) capability of large language models (LLMs) enables them to perform challenging tasks using provided demonstrations. However, ICL is highly sensitive to the ordering of demonstrations, leading to instability in predictions. This paper shows that this vulnerability can be exploited to design a natural attack—difficult for model providers to detect—that achieves nearly 80% success rate on LLaMA-3 by simply permuting the demonstrations. Existing mitigation methods primarily rely on post-processing and fail to enhance the model's inherent robustness to input permutations, raising concerns about safety and reliability of LLMs. To address this issue, we propose Permutation-resilient learning (PEARL), a novel framework based on distributionally robust optimization (DRO), which optimizes model performance against the worst-case input permutation. Specifically, PEARL consists of a permutation-proposal network (P-Net) and the LLM. The P-Net generates the most challenging permutations by treating it as an optimal transport problem, which is solved using an entropy-constrained Sinkhorn algorithm. Through minimax optimization, the P-Net and the LLM iteratively optimize against each other, progressively improving the LLM's robustness. Experiments on synthetic pre-training and real-world instruction tuning tasks demonstrate that PEARL effectively mitigates permutation attacks and enhances performance. Notably, despite being trained on fewer shots and shorter contexts, PEARL achieves performance gains of up to 40% when scaled to many-shot and long-context scenarios, highlighting its efficiency and generalization capabilities. | Liang Chen, Li Shen, Yang Deng, Xiaoyan Zhao, Bin Liang, KamFai Wong |  |
| 820 |  |  [SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domains?](https://openreview.net/forum?id=riTiq3i21b) |  | 0 | Autonomous systems for software engineering are now capable of fixing bugs and developing features. These systems are commonly evaluated on SWE-bench (Jimenez et al., 2024a), which assesses their ability to solve software issues from GitHub repositories. However, SWE-bench uses only Python repositories, with problem statements presented predominantly as text and lacking visual elements such as images. This limited coverage motivates our inquiry into how existing systems might perform on unrepresented software engineering domains (e.g., front-end, game development, DevOps), which use different programming languages and paradigms. Therefore, we propose SWE-bench Multimodal (SWE-bench M), to evaluate systems on their ability to fix bugs in visual, user-facing JavaScript software. SWE-bench M features 617 task instances collected from 17 JavaScript libraries used for web interface design, diagramming, data visualization, syntax highlighting, and interactive mapping. Each SWE-bench M task instance contains at least one image in its problem statement or unit tests. Our analysis finds that top-performing SWE-bench systems struggle with SWE-bench M, revealing limitations in visual problem-solving and cross-language generalization. Lastly, we show that SWE-agent’s flexible language-agnostic features enable it to substantially outperform alternatives on SWE-bench M, resolving 12% of task instances compared to 6% for the next best system. | John Yang, Carlos E. Jimenez, Alex L. Zhang, Kilian Lieret, Joyce Yang, Xindi Wu, Ori Press, Niklas Muennighoff, Gabriel Synnaeve, Karthik R. Narasimhan, Diyi Yang, Sida Wang, Ofir Press |  |
| 821 |  |  [Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems](https://openreview.net/forum?id=Y4aWwRh25b) |  | 0 | Retrieval-Augmented Generation (RAG) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation. We study the risk of datastore leakage in Retrieval-In-Context RAG Language Models (LMs). We show that an adversary can exploit LMs' instruction-following capabilities to easily extract text data verbatim from the datastore of RAG systems built with instruction-tuned LMs via prompt injection. The vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up. We also study multiple effects of RAG setup on the extractability of data, indicating that following unexpected instructions to regurgitate data can be an outcome of failure in effectively utilizing contexts for modern LMs, and further show that such vulnerability can be greatly mitigated by position bias elimination strategies. Extending our study to production RAG models, GPTs, we design an attack that can cause datastore leakage with a near-perfect success rate on 25 randomly selected customized GPTs with at most 2 queries, and we extract text data verbatim at a rate of 41\% from a book of 77,000 words and 3\% from a corpus of 1,569,000 words by prompting the GPTs with only 100 queries generated by themselves. | Zhenting Qi, Hanlin Zhang, Eric P. Xing, Sham M. Kakade, Himabindu Lakkaraju |  |
| 822 |  |  [Collab: Controlled Decoding using Mixture of Agents for LLM Alignment](https://openreview.net/forum?id=7ohlQUbTpp) |  | 0 | Alignment of Large Language models (LLMs) is crucial for safe and trustworthy deployment in applications. Reinforcement learning from human feedback (RLHF) has emerged as an effective technique to align LLMs to human preferences, and broader utilities, but it requires updating billions of model parameters which is computationally expensive. Controlled Decoding, by contrast, provides a mechanism for aligning a model at inference time without retraining. However, single-agent decoding approaches often struggle to adapt to diverse tasks due to the complexity and variability inherent in these tasks. To strengthen the test-time performance w.r.t the target task, we propose a mixture of agents-based decoding strategies leveraging the existing off-the-shelf aligned LLM policies. Treating each prior policy as an agent in the spirit of mixture of agent collaboration, we develop a decoding method that allows for inference-time alignment through a token-level selection strategy among multiple agents. For each token, the most suitable LLM is dynamically chosen from a pool of models based on a long-term utility metric. This policy-switching mechanism ensures optimal model selection at each step, enabling efficient collaboration and alignment among LLMs during decoding. Theoretical analysis of our proposed algorithm establishes optimal performance with respect to the target task represented via a target reward, for the given off-the-shelf models. We conduct comprehensive empirical evaluations with open-source aligned models on diverse tasks and preferences, which demonstrates the merits of this approach over single-agent decoding baselines. Notably, COLLAB surpasses the current SoTA decoding strategy, achieving an improvement of {up to 1.56x} in average reward and $71.89\%$ in GPT-4 based win-tie rate. | Souradip Chakraborty, Sujay Bhatt, Udari Madhushani Sehwag, Soumya Suvra Ghosal, Jiahao Qiu, Mengdi Wang, Dinesh Manocha, Furong Huang, Alec Koppel, Sumitra Ganesh |  |
| 823 |  |  [Finally Rank-Breaking Conquers MNL Bandits: Optimal and Efficient Algorithms for MNL Assortment](https://openreview.net/forum?id=kx8i1yfkRX) |  | 0 | We address the problem of active online assortment optimization problem with preference feedback, which is a framework for modeling user choices and subsetwise utility maximization. The framework is useful in various real-world applications including ad placement, online retail, recommender systems, and fine-tuning language models, amongst many others. The problem, although has been studied in the past, lacks an intuitive and practical solution approach with simultaneously efficient algorithm and optimal regret guarantee. E.g., popularly used assortment selection algorithms often require the presence of a \`\`strong reference" which is always included in the choice sets, further they are also designed to offer the same assortments repeatedly until the reference item gets selected---all such requirements are quite unrealistic for practical applications. In this paper, we designed efficient algorithms for the problem of regret minimization in assortment selection with \emph{Plackett Luce} (PL) based user choices. We designed a novel concentration guarantee for estimating the score parameters of the PL model using \`\emph{Pairwise Rank-Breaking}', which builds the foundation of our proposed algorithms. Moreover, our methods are practical, provably optimal, and devoid of the aforementioned limitations of the existing methods. | Aadirupa Saha, Pierre Gaillard |  |
| 824 |  |  [Efficient Top-m Data Values Identification for Data Selection](https://openreview.net/forum?id=lOfuvmi2HT) |  | 0 | Data valuation has found many real-world applications, e.g., data pricing and data selection. However, the most adopted approach -- Shapley value (SV) -- is computationally expensive due to the large number of model trainings required. Fortunately, most applications (e.g., data selection) require only knowing the $m$ data points with the highest data values (i.e., top-$m$ data values), which implies the potential for fewer model trainings as exact data values are not required. Existing work formulates top-$m$ Shapley value identification as top-$m$ arms identification in multi-armed bandits (MAB). However, the proposed approach falls short because it does not utilize data features to predict data values, a method that has been shown empirically to be effective. A recent top-$m$ arms identification work does consider the use of arm features while assuming a linear relationship between arm features and rewards, which is often not satisfied in data valuation. To this end, we propose the GPGapE algorithm that uses the Gaussian process to model the \emph{non-linear} mapping from data features to data values, removing the linear assumption. We theoretically analyze the correctness and stopping iteration of GPGapE in finding an $(\epsilon, \delta)$-approximation to the top-$m$ data values. We further improve the computational efficiency, by calculating data values using small data subsets to reduce the computation cost of model training. We empirically demonstrate that GPGapE outperforms other baselines in top-$m$ data values identification, noisy data detection, and data subset selection on real-world datasets. We also demonstrate the efficiency of our GPGapE in data selection for large language model fine-tuning. | Xiaoqiang Lin, Xinyi Xu, SeeKiong Ng, Bryan Kian Hsiang Low |  |
| 825 |  |  [ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery](https://openreview.net/forum?id=6z4YKr0GK6) |  | 0 | The advancements of language language models (LLMs) have piqued growing interest in developing LLM-based language agents to automate scientific discovery end-to-end, which has sparked both excitement and skepticism about the true capabilities of such agents. In this work, we argue that for an agent to fully automate scientific discovery, it must be able to complete all essential tasks in the workflow. Thus, we call for rigorous assessment of agents on individual tasks in a scientific workflow before making bold claims on end-to-end automation. To this end, we present ScienceAgentBench, a new benchmark for evaluating language agents for data-driven scientific discovery. To ensure the scientific authenticity and real-world relevance of our benchmark, we extract 102 tasks from 44 peer-reviewed publications in four disciplines and engage nine subject matter experts to validate them. We unify the target output for every task to a self-contained Python program file and employ an array of evaluation metrics to examine the generated programs, execution results, and costs. Each task goes through multiple rounds of manual validation by annotators and subject matter experts to ensure its annotation quality and scientific plausibility. We also propose two effective strategies to mitigate data contamination concerns. Using our benchmark, we evaluate five open-weight and proprietary LLMs, each with three frameworks: direct prompting, OpenHands, and self-debug. Given three attempts for each task, the best-performing agent can only solve 32.4% of the tasks independently and 34.3% with expert-provided knowledge. These results underscore the limited capacities of current language agents in generating code for data-driven discovery, let alone end-to-end automation for scientific research. | Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N. Baker, Benjamin Burns, Daniel AduAmpratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun |  |
| 826 |  |  [RazorAttention: Efficient KV Cache Compression Through Retrieval Heads](https://openreview.net/forum?id=tkiZQlL04w) |  | 0 | The memory and computational demands of Key-Value (KV) cache present significant challenges for deploying long-context language models. Previous approaches attempt to mitigate this issue by selectively dropping tokens, which irreversibly erases critical information that might be needed for future queries. In this paper, we propose a novel compression technique for KV cache that preserves all token information. Our investigation reveals that: i) Most attention heads primarily focus on the local context; ii) Only a few heads, denoted as retrieval heads, can essentially pay attention to all input tokens. These key observations motivate us to use separate caching strategy for attention heads.Therefore, we propose RazorAttention, a training-free KV cache compression algorithm, which maintains a full cache for these crucial retrieval heads and discards the remote tokens in non-retrieval heads. Furthermore, we introduce a novel mechanism involving a “compensation token” to further recover the information in the dropped tokens. Extensive evaluations across a diverse set of large language models (LLMs) demonstrate that RazorAttention achieves a reduction in KV cache size by over 70% without noticeable impacts on performance. Additionally, RazorAttention is compatible with FlashAttention, rendering it an efficient and plug-and-play solution that enhances LLM inference efficiency without overhead or retraining of the original model. | Hanlin Tang, Yang Lin, Jing Lin, Qingsen Han, Danning Ke, Shikuan Hong, Yiwu Yao, Gongyi Wang |  |
| 827 |  |  [Simple is Effective: The Roles of Graphs and Large Language Models in Knowledge-Graph-Based Retrieval-Augmented Generation](https://openreview.net/forum?id=JvkuZZ04O7) |  | 0 | Large Language Models (LLMs) demonstrate strong reasoning abilities but face limitations such as hallucinations and outdated knowledge. Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by grounding LLM outputs in structured external knowledge from KGs. However, current KG-based RAG frameworks still struggle to optimize the trade-off between retrieval effectiveness and efficiency in identifying a suitable amount of relevant graph information for the LLM to digest. We introduce SubgraphRAG, extending the KG-based RAG framework that retrieves subgraphs and leverages LLMs for reasoning and answer prediction. Our approach innovatively integrates a lightweight multilayer perceptron (MLP) with a parallel triple-scoring mechanism for efficient and flexible subgraph retrieval while encoding directional structural distances to enhance retrieval effectiveness. The size of retrieved subgraphs can be flexibly adjusted to match the query's needs and the downstream LLM's capabilities. This design strikes a balance between model complexity and reasoning power, enabling scalable and generalizable retrieval processes. Notably, based on our retrieved subgraphs, smaller LLMs like Llama3.1-8B-Instruct deliver competitive results with explainable reasoning, while larger models like GPT-4o achieve state-of-the-art accuracy compared with previous baselines—all without fine-tuning. Extensive evaluations on the WebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency, accuracy, and reliability by reducing hallucinations and improving response grounding. | Mufei Li, Siqi Miao, Pan Li |  |
| 828 |  |  [MrT5: Dynamic Token Merging for Efficient Byte-level Language Models](https://openreview.net/forum?id=VYWBMq1L7H) |  | 0 | Models that rely on subword tokenization have significant drawbacks, such as sensitivity to character-level noise like spelling errors and inconsistent compression rates across different languages and scripts. While character- or byte-level models like ByT5 attempt to address these concerns, they have not gained widespread adoption—processing raw byte streams without tokenization results in significantly longer sequence lengths, making training and inference inefficient. This work introduces MrT5 (MergeT5), a more efficient variant of ByT5 that integrates a token deletion mechanism in its encoder to dynamically shorten the input sequence length. After processing through a fixed number of encoder layers, a learned delete gate determines which tokens are to be removed and which are to be retained for subsequent layers. MrT5 effectively "merges" critical information from deleted tokens into a more compact sequence, leveraging contextual information from the remaining tokens. In continued pre-training experiments, we find that MrT5 can achieve significant gains in inference runtime with minimal effect on performance, as measured by bits-per-byte. Additionally, with multilingual training, MrT5 adapts to the orthographic characteristics of each language, learning language-specific compression rates. Furthermore, MrT5 shows comparable accuracy to ByT5 on downstream evaluations such as XNLI, TyDi QA, and character-level tasks while reducing sequence lengths by up to 75%. Our approach presents a solution to the practical limitations of existing byte-level models. | Julie Kallini, Shikhar Murty, Christopher D. Manning, Christopher Potts, Róbert Csordás |  |
| 829 |  |  [VVC-Gym: A Fixed-Wing UAV Reinforcement Learning Environment for Multi-Goal Long-Horizon Problems](https://openreview.net/forum?id=5xSRg3eYZz) |  | 0 | Multi-goal long-horizon problems are prevalent in real-world applications. The additional goal space introduced by multi-goal problems intensifies the spatial complexity of exploration; meanwhile, the long interaction sequences in long-horizon problems exacerbate the temporal complexity of exploration. Addressing the great exploration challenge posed by multi-goal long-horizon problems depends not only on the design of algorithms but also on the design of environments and the availability of demonstrations to assist in training. To facilitate the above research, we propose a multi-goal long-horizon Reinforcement Learning (RL) environment based on realistic fixed-wing UAV's velocity vector control, named VVC-Gym, and generate multiple demonstration sets of various quality. Through experimentation, we analyze the impact of different environment designs on training, assess the quantity and quality of demonstrations and their influence on training, and assess the effectiveness of various RL algorithms, providing baselines on VVC-Gym and its corresponding demonstrations. The results suggest that VVC-Gym is suitable for studying: (1) the influence of environment designs on addressing multi-goal long-horizon problems with RL. (2) the assistance that demonstrations can provide in overcoming the exploration challenges of multi-goal long-horizon problems. (3) the RL algorithm designs with the least possible impact from environment designs on the efficiency and effectiveness of training. | Xudong Gong, Dawei Feng, Kele Xu, Weijia Wang, Zhangjun Sun, Xing Zhou, Si Zheng, Bo Ding, Huaimin Wang |  |
| 830 |  |  [Commit0: Library Generation from Scratch](https://openreview.net/forum?id=MMwaQEVsAg) |  | 0 | With the goal of benchmarking generative systems beyond expert software development ability, we introduce Commit0, a benchmark that challenges AI agents to write libraries from scratch. Agents are provided with a specification document outlining the library’s API as well as a suite of interactive unit tests, with the goal of producing an implementation of this API accordingly. The implementation is validated through running these unit tests. As a benchmark, Commit0 is designed to move beyond static one-shot code generation towards agents that must process long-form natural language specifications, adapt to multi-stage feedback, and generate code with complex dependencies. Commit0 also offers an interactive environment where models receive static analysis and execution feedback on the code they generate. Our experiments demonstrate that while current agents can pass some unit tests, none can yet fully reproduce full libraries. Results also show that interactive feedback is quite useful for models to generate code that passes more unit tests, validating the benchmarks that facilitate its use. We publicly release the benchmark, the interactive environment, and the leaderboard. | Wenting Zhao, Nan Jiang, Celine Lee, Justin T. Chiu, Claire Cardie, Matthias Gallé, Alexander M. Rush |  |
| 831 |  |  [Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control](https://openreview.net/forum?id=1Njl73JKjB) |  | 0 | Disentangling model activations into human-interpretable features is a central problem in interpretability. Sparse autoencoders (SAEs) have recently attracted much attention as a scalable unsupervised approach to this problem. However, our imprecise understanding of ground-truth features in realistic scenarios makes it difficult to measure the success of SAEs. To address this challenge, we propose to evaluate SAEs on specific tasks by comparing them to supervised feature dictionaries computed with knowledge of the concepts relevant to the task. Specifically, we suggest that it is possible to (1) compute supervised sparse feature dictionaries that disentangle model computations for a specific task; (2) use them to evaluate and contextualize the degree of disentanglement and control offered by SAE latents on this task. Importantly, we can do this in a way that is agnostic to whether the SAEs have learned the exact ground-truth features or a different but similarly useful representation. As a case study, we apply this framework to the indirect object identification (IOI) task using GPT-2 Small, with SAEs trained on either the IOI or OpenWebText datasets. We find that SAEs capture interpretable features for the IOI task, and that more recent SAE variants such as Gated SAEs and Top-K SAEs are competitive with supervised features in terms of disentanglement and control over the model. We also exhibit, through this setup and toy models, some qualitative phenomena in SAE training illustrating feature splitting and the role of feature magnitudes in solutions preferred by SAEs. | Aleksandar Makelov, Georg Lange, Neel Nanda |  |
| 832 |  |  [CL-MFAP: A Contrastive Learning-Based Multimodal Foundation Model for Molecular Property Prediction and Antibiotic Screening](https://openreview.net/forum?id=fv9XU7CyN2) |  | 0 | Due to the rise in antimicrobial resistance, identifying novel compounds with antibiotic potential is crucial for combatting this global health issue. However, traditional drug development methods are costly and inefficient. Recognizing the pressing need for more effective solutions, researchers have turned to machine learning techniques to streamline the prediction and development of novel antibiotic compounds. While foundation models have shown promise in antibiotic discovery, current mainstream efforts still fall short of fully leveraging the potential of multimodal molecular data. Recent studies suggest that contrastive learning frameworks utilizing multimodal data exhibit excellent performance in representation learning across various domains. Building upon this, we introduce CL-MFAP, an unsupervised contrastive learning (CL)-based multimodal foundation (MF) model specifically tailored for discovering small molecules with potential antibiotic properties (AP) using three types of molecular data. This model employs 1.6 million bioactive molecules with drug-like properties from the ChEMBL dataset to jointly pretrain three encoders: (1) a transformer-based encoder with rotary position embedding for processing SMILES strings; (2) another transformer-based encoder, incorporating a novel bi-level routing attention mechanism to handle molecular graph representations; and (3) a Morgan fingerprint encoder using a multilayer perceptron, to achieve the contrastive learning purpose. The CL-MFAP outperforms baseline models in antibiotic property prediction by effectively utilizing different molecular modalities and demonstrates superior domain-specific performance when fine-tuned for antibiotic-related property prediction tasks. | Gen Zhou, Sugitha Janarthanan, Yutong Lu, Pingzhao Hu |  |
| 833 |  |  [SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators](https://openreview.net/forum?id=u3TL0qxLWf) |  | 0 | Large Language Models (LLMs) have transformed natural language processing, but face significant challenges in widespread deployment due to their high runtime cost. In this paper, we introduce SeedLM, a novel post-training compression method that uses seeds of a pseudo-random generator to encode and compress model weights. Specifically, for each block of weights, we find a seed that is fed into a Linear Feedback Shift Register (LFSR) during inference to efficiently generate a random matrix. This matrix is then linearly combined with compressed coefficients to reconstruct the weight block. SeedLM reduces memory access and leverages idle compute cycles during inference, effectively speeding up memory-bound tasks by trading compute for fewer memory accesses. Unlike state-of-the-art methods that rely on calibration data, our approach is data-free and generalizes well across diverse tasks. Our experiments with Llama3 70B, which is particularly challenging, show zero-shot accuracy retention at 4- and 3-bit compression to be on par with or better than state-of-the-art methods, while maintaining performance comparable to FP16 baselines. Additionally, FPGA-based tests demonstrate that 4-bit SeedLM, as model size increases, approaches a 4x speed-up over an FP16 Llama 2/3 baseline. | Rasoul Shafipour, David Harrison, Maxwell Horton, Jeffrey Marker, Houman Bedayat, Sachin Mehta, Mohammad Rastegari, Mahyar Najibi, Saman Naderiparizi |  |
| 834 |  |  [Dissecting Adversarial Robustness of Multimodal LM Agents](https://openreview.net/forum?id=YauQYh2k1g) |  | 0 | As language models (LMs) are used to build autonomous agents in real environments, ensuring their adversarial robustness becomes a critical challenge. Unlike chatbots, agents are compound systems with multiple components taking actions, which existing LMs safety evaluations do not adequately address. To bridge this gap, we manually create 200 targeted adversarial tasks and evaluation scripts in a realistic threat model on top of VisualWebArena, a real environment for web agents. To systematically examine the robustness of agents, we propose the Agent Robustness Evaluation (ARE) framework. ARE views the agent as a graph showing the flow of intermediate outputs between components and decomposes robustness as the flow of adversarial information on the graph. We find that we can successfully break latest agents that use black-box frontier LMs, including those that perform reflection and tree search. With imperceptible perturbations to a single image (less than 5% of total web page pixels), an attacker can hijack these agents to execute targeted adversarial goals with success rates up to 67%. We also use ARE to rigorously evaluate how the robustness changes as new components are added. We find that inference-time compute that typically improves benign performance can open up new vulnerabilities and harm robustness. An attacker can compromise the evaluator used by the reflexion agent and the value function of the tree search agent, which increases the attack success relatively by 15% and 20%. Our data and code for attacks, defenses, and evaluation are at https://github.com/ChenWu98/agent-attack | Chen Henry Wu, Rishi Rajesh Shah, Jing Yu Koh, Russ Salakhutdinov, Daniel Fried, Aditi Raghunathan |  |
| 835 |  |  [ACC-Collab: An Actor-Critic Approach to Multi-Agent LLM Collaboration](https://openreview.net/forum?id=nfKfAzkiez) |  | 0 | Large language models (LLMs) have demonstrated a remarkable ability to serve as general-purpose tools for various language-based tasks. Recent works have demonstrated that the efficacy of such models can be improved through iterative dialog between multiple models. While these paradigms show promise in improving model efficacy, most works in this area treat collaboration as an emergent behavior, rather than a learned behavior. In doing so, current multi-agent frameworks rely on collaborative behaviors to have been sufficiently trained into off-the-shelf models. To address this limitation, we propose ACC-Collab, an \*\*A\*\*ctor-\*\*C\*\*riti\*\*c\*\* based learning framework to produce a two-agent team (an actor-agent and a critic-agent) specialized in collaboration. We demonstrate that ACC-Collab outperforms SotA multi-agent techniques on a wide array of benchmarks. | Andrew Estornell, JeanFrancois Ton, Yuanshun Yao, Yang Liu |  |
| 836 |  |  [Greener GRASS: Enhancing GNNs with Encoding, Rewiring, and Attention](https://openreview.net/forum?id=rEQqBZIz49) |  | 0 | Graph Neural Networks (GNNs) have become important tools for machine learning on graph-structured data. In this paper, we explore the synergistic combination of graph encoding, graph rewiring, and graph attention, by introducing Graph Attention with Stochastic Structures (GRASS), a novel GNN architecture. GRASS utilizes relative random walk probabilities (RRWP) encoding and a novel decomposed variant (D-RRWP) to efficiently capture structural information. It rewires the input graph by superimposing a random regular graph to enhance long-range information propagation. It also employs a novel additive attention mechanism tailored for graph-structured data. Our empirical evaluations demonstrate that GRASS achieves state-of-the-art performance on multiple benchmark datasets, including a 20.3% reduction in mean absolute error on the ZINC dataset. | Tongzhou Liao, Barnabás Póczos |  |
| 837 |  |  [NeSyC: A Neuro-symbolic Continual Learner For Complex Embodied Tasks in Open Domains](https://openreview.net/forum?id=VoayJihXra) |  | 0 | We explore neuro-symbolic approaches to generalize actionable knowledge, enabling embodied agents to tackle complex tasks more effectively in open-domain environments. A key challenge for embodied agents is the generalization of knowledge across diverse environments and situations, as limited experiences often confine them to their prior knowledge. To address this issue, we introduce a novel framework, NeSyC, a neuro-symbolic continual learner that emulates the hypothetico-deductive model by continually formulating and validating knowledge from limited experiences through the combined use of Large Language Models (LLMs) and symbolic tools. Specifically, we devise a contrastive generality improvement scheme within NeSyC, which iteratively generates hypotheses using LLMs and conducts contrastive validation via symbolic tools. This scheme reinforces the justification for admissible actions while minimizing the inference of inadmissible ones. Additionally, we incorporate a memory-based monitoring scheme that efficiently detects action errors and triggers the knowledge refinement process across domains. Experiments conducted on diverse embodied task benchmarks—including ALFWorld, VirtualHome, Minecraft, RLBench, and a real-world robotic scenario—demonstrate that NeSyC is highly effective in solving complex embodied tasks across a range of open-domain environments. | Wonje Choi, Jinwoo Park, Sanghyun Ahn, Daehee Lee, Honguk Woo |  |
| 838 |  |  [Conformal Language Model Reasoning with Coherent Factuality](https://openreview.net/forum?id=AJpUZd8Clb) |  | 0 | Language models are increasingly being used in important decision pipelines, so ensuring the correctness of their outputs is crucial. Recent work has proposed evaluating the “factuality” of claims decomposed from a language model generation and applying conformal prediction techniques to filter out those claims that are not factual. This can be effective for tasks such as information retrieval, where constituent claims may be evaluated in isolation for factuality, but is not appropriate for reasoning tasks, as steps of a logical argument can be evaluated for correctness only within the context of the claims that precede them. To capture this, we define “coherent factuality” and develop a conformal-prediction-based method to guarantee coherent factuality for language model outputs. Our approach applies split conformal prediction to subgraphs within a "deducibility" graph that represents the steps of a reasoning problem. We evaluate our method on mathematical reasoning problems from the MATH and FELM datasets and find that our algorithm consistently produces correct and substantiated orderings of claims, achieving coherent factuality across target coverage levels. Moreover, we achieve 90\% factuality on our stricter definition while retaining 80\% or more of the original claims, highlighting the utility of our deducibility-graph-guided approach. | Maxon RubinToles, Maya Gambhir, Keshav Ramji, Aaron Roth, Surbhi Goel |  |
| 839 |  |  [Constraint-Conditioned Actor-Critic for Offline Safe Reinforcement Learning](https://openreview.net/forum?id=nrRkAAAufl) |  | 0 | Offline safe reinforcement learning (OSRL) aims to learn policies with high rewards while satisfying safety constraints solely from data collected offline. However, the learned policies often struggle to handle states and actions that are not present or out-of-distribution (OOD) from the offline dataset, which can result in violation of the safety constraints or overly conservative behaviors during their online deployment. Moreover, many existing methods are unable to learn policies that can adapt to varying constraint thresholds. To address these challenges, we propose constraint-conditioned actor-critic (CCAC), a novel OSRL method that models the relationship between state-action distributions and safety constraints, and leverages this relationship to regularize critics and policy learning. CCAC learns policies that can effectively handle OOD data and adapt to varying constraint thresholds. Empirical evaluations on the $\texttt{DSRL}$ benchmarks show that CCAC significantly outperforms existing methods for learning adaptive, safe, and high-reward policies. | Zijian Guo, Weichao Zhou, Shengao Wang, Wenchao Li |  |
| 840 |  |  [Specialized Foundation Models Struggle to Beat Supervised Baselines](https://openreview.net/forum?id=JYTQ6ELUVO) |  | 0 | Following its success for vision and text, the "foundation model" (FM) paradigm&#151;pretraining large models on massive data, then fine-tuning on target tasks&#151;has rapidly expanded to domains in the sciences, engineering, healthcare, and beyond. Has this achieved what the original FMs accomplished, i.e. the supplanting of traditional supervised learning in their domains? To answer we look at three modalities&#151;genomics, satellite imaging, and time series&#151;with multiple recent FMs and compare them to a standard supervised learning workflow: model development, hyperparameter tuning, and training, all using only data from the target task. Across these three specialized domains, we find that it is consistently possible to train simple supervised models&#151;no more complicated than a lightly modified wide ResNet or UNet&#151;that match or even outperform the latest foundation models. Our work demonstrates that the benefits of large-scale pretraining have yet to be realized in many specialized areas, reinforces the need to compare new FMs to strong, well-tuned baselines, and introduces two new, easy-to-use, open-source, and automated workflows for doing so. | Zongzhe Xu, Ritvik Gupta, Wenduo Cheng, Alexander Shen, Junhong Shen, Ameet Talwalkar, Mikhail Khodak |  |
| 841 |  |  [Efficient Biological Data Acquisition through Inference Set Design](https://openreview.net/forum?id=gVkX9QMBO3) |  | 0 | In drug discovery, highly automated high-throughput laboratories are used to screen a large number of compounds in search of effective drugs. These experiments are expensive, so one might hope to reduce their cost by only experimenting on a subset of the compounds, and predicting the outcomes of the remaining experiments. In this work, we model this scenario as a sequential subset selection problem: we aim to select the smallest set of candidates in order to achieve some desired level of accuracy for the system as a whole. Our key observation is that, if there is heterogeneity in the difficulty of the prediction problem across the input space, selectively obtaining the labels for the hardest examples in the acquisition pool will leave only the relatively easy examples to remain in the inference set, leading to better overall system performance. We call this mechanism inference set design, and propose the use of a confidence-based active learning solution to prune out these challenging examples. Our algorithm includes an explicit stopping criterion that interrupts the acquisition loop when it is sufficiently confident that the system has reached the target performance. Our empirical studies on image and molecular datasets, as well as a real-world large-scale biological assay, show that active learning for inference set design leads to significant reduction in experimental cost while retaining high system performance. | Ihor Neporozhnii, Julien Roy, Emmanuel Bengio, Jason S. Hartford |  |
| 842 |  |  [Uncertainty Herding: One Active Learning Method for All Label Budgets](https://openreview.net/forum?id=UgPoHhYQ2U) |  | 0 | Most active learning research has focused on methods which perform well when many labels are available, but can be dramatically worse than random selection when label budgets are small. Other methods have focused on the low-budget regime, but do poorly as label budgets increase. As the line between "low" and "high" budgets varies by problem, this is a serious issue in practice. We propose \*uncertainty coverage\*, an objective which generalizes a variety of low- and high-budget objectives, as well as natural, hyperparameter-light methods to smoothly interpolate between low- and high-budget regimes. We call greedy optimization of the estimate Uncertainty Herding; this simple method is computationally fast, and we prove that it nearly optimizes the distribution-level coverage. In experimental validation across a variety of active learning tasks, our proposal matches or beats state-of-the-art performance in essentially all cases; it is the only method of which we are aware that reliably works well in both low- and high-budget settings. | Wonho Bae, Danica J. Sutherland, Gabriel L. Oliveira |  |
| 843 |  |  [In-context Time Series Predictor](https://openreview.net/forum?id=dCcY2pyNIO) |  | 0 | Recent Transformer-based large language models (LLMs) demonstrate in-context learning ability to perform various functions based solely on the provided context, without updating model parameters. To fully utilize the in-context capabilities in time series forecasting (TSF) problems, unlike previous Transformer-based or LLM-based time series forecasting methods, we reformulate "time series forecasting tasks" as input tokens by constructing a series of (lookback, future) pairs within the tokens. This method aligns more closely with the inherent in-context mechanisms and is more parameter-efficient without the need of using pre-trained LLM parameters. Furthermore, it addresses issues such as overfitting in existing Transformer-based TSF models, consistently achieving better performance across full-data, few-shot, and zero-shot settings compared to previous architectures. | Jiecheng Lu, Yan Sun, Shihao Yang |  |
| 844 |  |  [More Experts Than Galaxies: Conditionally-Overlapping Experts with Biologically-Inspired Fixed Routing](https://openreview.net/forum?id=1qq1QJKM5q) |  | 0 | The evolution of biological neural systems has led to both modularity and sparse coding, which enables energy efficiency and robustness across the diversity of tasks in the lifespan. In contrast, standard neural networks rely on dense, non-specialized architectures, where all model parameters are simultaneously updated to learn multiple tasks, leading to interference. Current sparse neural network approaches aim to alleviate this issue but are hindered by limitations such as 1) trainable gating functions that cause representation collapse, 2) disjoint experts that result in redundant computation and slow learning, and 3) reliance on explicit input or task IDs that limit flexibility and scalability. In this paper we propose Conditionally Overlapping Mixture of ExperTs (COMET), a general deep learning method that addresses these challenges by inducing a modular, sparse architecture with an exponential number of overlapping experts. COMET replaces the trainable gating function used in Sparse Mixture of Experts with a fixed, biologically inspired random projection applied to individual input representations. This design causes the degree of expert overlap to depend on input similarity, so that similar inputs tend to share more parameters. This results in faster learning per update step and improved out-of-sample generalization. We demonstrate the effectiveness of COMET on a range of tasks, including image classification, language modeling, and regression, using several popular deep learning architectures. | Sagi Shaier, Francisco Pereira, Katharina von der Wense, Lawrence Hunter, Matt Jones |  |
| 845 |  |  [Can Watermarks be Used to Detect LLM IP Infringement For Free?](https://openreview.net/forum?id=KRMSH1GxUK) |  | 0 | The powerful capabilities of LLMs stem from their rich training data and high-quality labeled datasets, making the training of strong LLMs a resource-intensive process, which elevates the importance of IP protection for such LLMs. Compared to gathering high-quality labeled data, directly sampling outputs from these fully trained LLMs as training data presents a more cost-effective approach. This practice—where a suspect model is fine-tuned using high-quality data derived from these LLMs, thereby gaining capabilities similar to the target model—can be seen as a form of IP infringement against the original LLM. In recent years, LLM watermarks have been proposed and used to detect whether a text is AI-generated. Intuitively, if data sampled from a watermarked LLM is used for training, the resulting model would also be influenced by this watermark. This raises the question: can we directly use such watermarks to detect IP infringement of LLMs? In this paper, we explore the potential of LLM watermarks for detecting model infringement. We find that there are two issues with direct detection: (1) The queries used to sample output from the suspect LLM have a significant impact on detectability. (2) The watermark that is easily learned by LLMs exhibits instability regarding the watermark's hash key during detection. To address these issues, we propose LIDet, a detection method that leverages available anchor LLMs to select suitable queries for sampling from the suspect LLM. Additionally, it adapts the detection threshold to mitigate detection failures caused by different hash keys. To demonstrate the effectiveness of this approach, we construct a challenging model set containing multiple suspect LLMs on which direct detection methods struggle to yield effective results. Our method achieves over 90\% accuracy in distinguishing between infringing and clean models, demonstrating the feasibility of using LLM watermarks to detect LLM IP infringement. | Zhengyue Zhao, Xiaogeng Liu, Somesh Jha, Patrick McDaniel, Bo Li, Chaowei Xiao |  |
| 846 |  |  [Discovering Influential Neuron Path in Vision Transformers](https://openreview.net/forum?id=WQQyJbr5Lh) |  | 0 | Vision Transformer models exhibit immense power yet remain opaque to human understanding, posing challenges and risks for practical applications. While prior research has attempted to demystify these models through input attribution and neuron role analysis, there's been a notable gap in considering layer-level information and the holistic path of information flow across layers. In this paper, we investigate the significance of influential neuron paths within vision Transformers, which is a path of neurons from the model input to output that impacts the model inference most significantly. We first propose a joint influence measure to assess the contribution of a set of neurons to the model outcome. And we further provide a layer-progressive neuron locating approach that efficiently selects the most influential neuron at each layer trying to discover the crucial neuron path from input to output within the target model. Our experiments demonstrate the superiority of our method finding the most influential neuron path along which the information flows, over the existing baseline solutions. Additionally, the neuron paths have illustrated that vision Transformers exhibit some specific inner working mechanism for processing the visual information within the same image category. We further analyze the key effects of these neurons on the image classification task, showcasing that the found neuron paths have already preserved the model capability on downstream tasks, which may also shed some lights on real-world applications like model pruning. The project website including implementation code is available at https://foundation-model-research.github.io/NeuronPath/. | Yifan Wang, Yifei Liu, Yingdong Shi, Changming Li, Anqi Pang, Sibei Yang, Jingyi Yu, Kan Ren |  |
| 847 |  |  [When narrower is better: the narrow width limit of Bayesian parallel branching neural networks](https://openreview.net/forum?id=CkUHtnyhpY) |  | 0 | The infinite width limit of random neural networks is known to result in Neural Networks as Gaussian Process (NNGP) (Lee et al. (2018)), characterized by task-independent kernels. It is widely accepted that larger network widths contribute to improved generalization (Park et al. (2019)). However, this work challenges this notion by investigating the narrow width limit of the Bayesian Parallel Branching Neural Network (BPB-NN), an architecture that resembles neural networks with residual blocks. We demonstrate that when the width of a BPB-NN is significantly smaller compared to the number of training examples, each branch exhibits more robust learning due to a symmetry breaking of branches in kernel renormalization. Surprisingly, the performance of a BPB-NN in the narrow width limit is generally superior to or comparable to that achieved in the wide width limit in bias-limited scenarios. Furthermore, the readout norms of each branch in the narrow width limit are mostly independent of the architectural hyperparameters but generally reflective of the nature of the data. We demonstrate such phenomenon primarily in the branching graph neural networks, where each branch represents a different order of convolutions of the graph; we also extend the results to other more general architectures such as the residual-MLP and demonstrate that the narrow width effect is a general feature of the branching networks. Our results characterize a newly defined narrow-width regime for parallel branching networks in general. | Zechen Zhang, Haim Sompolinsky |  |
| 848 |  |  [Directional Gradient Projection for Robust Fine-Tuning of Foundation Models](https://openreview.net/forum?id=goBaGHLAdP) |  | 0 | Robust fine-tuning aims to adapt large foundation models to downstream tasks while preserving their robustness to distribution shifts. Existing methods primarily focus on constraining and projecting current model towards the pre-trained initialization based on the magnitudes between fine-tuned and pre-trained weights, which often require extensive hyper-parameter tuning and can sometimes result in underfitting. In this work, we propose $\textbf{Di}$rectional $\textbf{Gra}$dient $\textbf{P}$rojection (DiGraP), a novel layer-wise trainable method that incorporates directional information from gradients to bridge regularization and multi-objective optimization. Besides demonstrating our method on image classification, as another contribution we generalize this area to the multi-modal evaluation settings for robust fine-tuning. Specifically, we first bridge the uni-modal and multi-modal gap by performing analysis on Image Classification reformulated Visual Question Answering (VQA) benchmarks and further categorize ten out-of-distribution (OOD) VQA datasets by distribution shift types and degree (i.e. near versus far OOD). Experimental results show that DiGraP consistently outperforms existing baselines across Image Classfication and VQA tasks with discriminative and generative backbones, improving both in-distribution (ID) generalization and OOD robustness. | Chengyue Huang, Junjiao Tian, Brisa Maneechotesuwan, Shivang Chopra, Zsolt Kira |  |
| 849 |  |  [Reward Learning from Multiple Feedback Types](https://openreview.net/forum?id=9Ieq8jQNAl) |  | 0 | Learning rewards from preference feedback has become an important tool in the alignment of agentic models. Preference-based feedback, often implemented as a binary comparison between multiple completions, is an established method to acquire large-scale human feedback. However, human feedback in other contexts is often much more diverse. Such diverse feedback can better support the goals of a human annotator, and the simultaneous use of multiple sources might be mutually informative for the learning process or carry type-dependent biases for the reward learning process. Despite these potential benefits, learning from different feedback types has yet to be explored extensively. In this paper, we bridge this gap by enabling experimentation and evaluating multi-type feedback in a wide set of environments. We present a process to generate high-quality simulated feedback of six different types. Then, we implement reward models and downstream RL training for all six feedback types. Based on the simulated feedback, we investigate the use of types of feedback across ten RL environments and compare them to pure preference-based baselines. We show empirically that diverse types of feedback can be utilized and lead to strong reward modeling performance. This work is the first strong indicator of the potential of multi-type feedback for RLHF. | Yannick Metz, András Geiszl, Raphaël Baur, Mennatallah ElAssady |  |
| 850 |  |  [Token-Supervised Value Models for Enhancing Mathematical Problem-Solving Capabilities of Large Language Models](https://openreview.net/forum?id=6HcnC3pPkp) |  | 0 | With the rapid advancement of test-time compute search strategies to improve the mathematical problem-solving capabilities of large language models (LLMs), the need for building robust verifiers has become increasingly important. However, all these inference strategies rely on existing verifiers originally designed for Best-of-N search, which makes them sub-optimal for tree search techniques at test time. During tree search, existing verifiers can only offer indirect and implicit assessments of partial solutions or under-value prospective intermediate steps, thus resulting in the premature pruning of promising intermediate steps. To overcome these limitations, we propose token-supervised value models (TVMs) -- a new class of verifiers that assign each token a probability that reflects the likelihood of reaching the correct final answer. This new token-level supervision enables TVMs to directly and explicitly evaluate partial solutions, effectively distinguishing between promising and incorrect intermediate steps during tree search at test time. Experimental results demonstrate that combining tree-search-based inference strategies with TVMs significantly improves the accuracy of LLMs in mathematical problem-solving tasks, surpassing the performance of existing verifiers. | Jung Hyun Lee, June Yong Yang, Byeongho Heo, Dongyoon Han, Kyungsu Kim, Eunho Yang, Kang Min Yoo |  |
| 851 |  |  [ImProver: Agent-Based Automated Proof Optimization](https://openreview.net/forum?id=dWsdJAXjQD) |  | 0 | Large language models (LLMs) have been used to generate formal proofs of mathematical theorems in proofs assistants such as Lean. However, we often want to optimize a formal proof with respect to various criteria, depending on its downstream use. For example, we may want a proof to adhere to a certain style, be declaratively structured, or concise. Having suitably optimized proofs is also important for learning tasks, especially since human-written proofs may not optimal for that purpose. To this end, we study a new problem of automated proof optimization: rewriting a proof so that it is correct and optimizes for an arbitrary criterion, such as length or declarativity. As a first method for automated proof optimization, we present ImProver, a large-language-model agent that rewrites proofs to optimize arbitrary user-defined metrics in Lean. We find that naively applying LLMs to proof optimization falls short, and we incorporate various improvements into ImProver, such as the use of symbolic Lean context in a novel Chain-of-States technique, as well as error-correction and retrieval. We test ImProver on rewriting real-world undergraduate, competition, and research-level mathematics theorems, finding that ImProver is capable of rewriting proofs so that they are substantially shorter and more declarative in structure. | Riyaz Ahuja, Jeremy Avigad, Prasad Tetali, Sean Welleck |  |
| 852 |  |  [Learning-Guided Rolling Horizon Optimization for Long-Horizon Flexible Job-Shop Scheduling](https://openreview.net/forum?id=Aly68Y5Es0) |  | 0 | Long-horizon combinatorial optimization problems (COPs), such as the Flexible Job-Shop Scheduling Problem (FJSP), often involve complex, interdependent decisions over extended time frames, posing significant challenges for existing solvers. While Rolling Horizon Optimization (RHO) addresses this by decomposing problems into overlapping shorter-horizon subproblems, such overlap often involves redundant computations. In this paper, we present L-RHO, the first learning-guided RHO framework for COPs. L-RHO employs a neural network to intelligently fix variables that in hindsight did not need to be re-optimized, resulting in smaller and thus easier-to-solve subproblems. For FJSP, this means identifying operations with unchanged machine assignments between consecutive subproblems. Applied to FJSP, L-RHO accelerates RHO by up to 54\% while significantly improving solution quality, outperforming other heuristic and learning-based baselines. We also provide in-depth discussions and verify the desirable adaptability and generalization of L-RHO across numerous FJSP variates, distributions, online scenarios and benchmark instances. Moreover, we provide a theoretical analysis to elucidate the conditions under which learning is beneficial. | Sirui Li, Wenbin Ouyang, Yining Ma, Cathy Wu |  |
| 853 |  |  [Humanizing the Machine: Proxy Attacks to Mislead LLM Detectors](https://openreview.net/forum?id=PIpGN5Ko3v) |  | 0 | The advent of large language models (LLMs) has revolutionized the field of text generation, producing outputs that closely mimic human-like writing. Although academic and industrial institutions have developed detectors to prevent the malicious usage of LLM-generated texts, other research has doubt about the robustness of these systems. To stress test these detectors, we introduce a humanized proxy-attack (HUMPA) strategy that effortlessly compromises LLMs, causing them to produce outputs that align with human-written text and mislead detection systems. Our method attacks the source model by leveraging a reinforcement learning (RL) fine-tuned humanized small language model (SLM) in the decoding phase. Through an in-depth analysis, we demonstrate that our attack strategy is capable of generating responses that are indistinguishable to detectors, preventing them from differentiating between machine-generated and human-written text. We conduct systematic evaluations on extensive datasets using proxy-attacked open-source models, including Llama2-13B, Llama3-70B, and Mixtral-8x7B in both white- and black-box settings. Our findings show that the proxy-attack strategy effectively deceives the leading detectors, resulting in an average AUROC drop of 70.4% across multiple datasets, with a maximum drop of 95.0% on a single dataset. Furthermore, in cross-discipline scenarios, our strategy also bypasses these detectors, leading to a significant relative decrease of up to 90.9%, while in cross-language scenario, the drop reaches 91.3%. Despite our proxy-attack strategy successfully bypassing the detectors with such significant relative drops, we find that the generation quality of the attacked models remains preserved, even within a modest utility budget, when compared to the text produced by the original, unattacked source model. | Tianchun Wang, Yuanzhou Chen, Zichuan Liu, Zhanwen Chen, Haifeng Chen, Xiang Zhang, Wei Cheng |  |
| 854 |  |  [Probe Pruning: Accelerating LLMs through Dynamic Pruning via Model-Probing](https://openreview.net/forum?id=WOt1owGfuN) |  | 0 | We introduce Probe Pruning (PP), a novel framework for online, dynamic, structured pruning of Large Language Models (LLMs) applied in a batch-wise manner. PP leverages the insight that not all samples and tokens contribute equally to the model's output, and probing a small portion of each batch effectively identifies crucial weights, enabling tailored dynamic pruning for different batches. It comprises three main stages: probing, history-informed pruning, and full inference. In the probing stage, PP selects a small yet crucial set of hidden states, based on residual importance, to run a few model layers ahead. During the history-informed pruning stage, PP strategically integrates the probing states with historical states. Subsequently, it structurally prunes weights based on the integrated states and the PP importance score, a metric developed specifically to assess the importance of each weight channel in maintaining performance. In the final stage, full inference is conducted on the remaining weights. A major advantage of PP is its compatibility with existing models, as it operates without requiring additional neural network modules or fine-tuning. Comprehensive evaluations of PP on LLaMA-2/3 and OPT models reveal that even minimal probing—using just 1.5% of FLOPs—can substantially enhance the efficiency of structured pruning of LLMs. For instance, when evaluated on LLaMA-2-7B with WikiText2, PP achieves a 2.56 times lower ratio of performance degradation per unit of latency reduction compared to the state-of-the-art method at a 40\% pruning ratio. | Qi Le, Enmao Diao, Ziyan Wang, Xinran Wang, Jie Ding, Li Yang, Ali Anwar |  |
| 855 |  |  [AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents](https://openreview.net/forum?id=oWdzUpOlkX) |  | 0 | Autonomy via agents based on large language models (LLMs) that can carry out personalized yet standardized tasks presents a significant opportunity to drive human efficiency. There is an emerging need and interest in automating web tasks (e.g., booking a hotel for a given date within a budget). Being a practical use case itself, the web agent also serves as an important proof-of-concept example for various agent grounding scenarios, with its success promising advancements in many future applications. Meanwhile, much prior research focuses on handcrafting their web agent strategies (e.g., agent's prompting templates, reflective workflow, role-play and multi-agent systems, search or sampling methods, etc.) and the corresponding in-context examples. However, these custom strategies often struggle with generalizability across all potential real-world applications. On the other hand, there has been limited study on the misalignment between a web agent's observation and action representation, and the data on which the agent's underlying LLM has been pre-trained. This discrepancy is especially notable when LLMs are primarily trained for language completion rather than tasks involving embodied navigation actions and symbolic web elements. In our study, we enhance an LLM-based web agent by simply refining its observation and action space, aligning these more closely with the LLM's capabilities. This approach enables our base agent to significantly outperform previous methods on a wide variety of web tasks. Specifically, on WebArena, a benchmark featuring general-purpose web interaction tasks, our agent AgentOccam surpasses the previous state-of-the-art and concurrent work by 9.8 (+29.4%) and 5.9 (+15.8%) absolute points respectively, and boosts the success rate by 26.6 points (+161%) over similar plain web agents with its observation and action space alignment. Furthermore, on WebVoyager benchmark comprising tasks defined on real-world websites, AgentOccam exceeds the former best agent by 2.4 points (+4.6%) on tasks with deterministic answers. We achieve this without using in-context examples, new agent roles, online feedback or search strategies. AgentOccam's simple design highlights LLMs' impressive zero-shot performance on web tasks, and underlines the critical role of carefully tuning observation and action spaces for LLM-based agents. | Ke Yang, Yao Liu, Sapana Chaudhary, Rasool Fakoor, Pratik Chaudhari, George Karypis, Huzefa Rangwala |  |
| 856 |  |  [Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF](https://openreview.net/forum?id=SQnitDuow6) |  | 0 | Reinforcement learning from human feedback (RLHF) has demonstrated great promise in aligning large language models (LLMs) with human preference. Depending on the availability of preference data, both online and offline RLHF are active areas of investigation. A key bottleneck is understanding how to incorporate uncertainty estimation in the reward function learned from the preference data for RLHF, regardless of how the preference data is collected. While the principles of optimism or pessimism under uncertainty are well-established in standard reinforcement learning (RL), a practically-implementable and theoretically-grounded form amenable to large language models is not yet available, as standard techniques for constructing confidence intervals become intractable under arbitrary policy parameterizations. In this paper, we introduce a unified approach to online and offline RLHF --- value-incentivized preference optimization (VPO) --- which regularizes the maximum-likelihood estimate of the reward function with the corresponding value function, modulated by a sign to indicate whether the optimism or pessimism is chosen. VPO also directly optimizes the policy with implicit reward modeling, and therefore shares a simpler RLHF pipeline similar to direct preference optimization. Theoretical guarantees of VPO are provided for both online and offline settings, matching the rates of their standard RL counterparts. Moreover, experiments on text summarization, dialogue, and standard benchmarks verify the practicality and effectiveness of VPO. | Shicong Cen, Jincheng Mei, Katayoon Goshvadi, Hanjun Dai, Tong Yang, Sherry Yang, Dale Schuurmans, Yuejie Chi, Bo Dai |  |
| 857 |  |  [Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration](https://openreview.net/forum?id=FviefuxmeW) |  | 0 | Imitation learning is a central problem in reinforcement learning where the goal is to learn a policy that mimics the expert's behavior. In practice, it is often challenging to learn the expert policy from a limited number of demonstrations accurately due to the complexity of the state space. Moreover, it is essential to explore the environment and collect data to achieve beyond-expert performance. To overcome these challenges, we propose a novel imitation learning algorithm called Imitation Learning with Double Exploration (ILDE), which implements exploration in two aspects: (1) optimistic policy optimization via an exploration bonus that rewards state-action pairs with high uncertainty to potentially improve the convergence to the expert policy, and (2) curiosity-driven exploration of the states that deviate from the demonstration trajectories to potentially yield beyond-expert performance. Empirically, we demonstrate that ILDE outperforms the state-of-the-art imitation learning algorithms in terms of sample efficiency and achieves beyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations than in previous work. We also provide a theoretical justification of ILDE as an uncertainty-regularized policy optimization method with optimistic exploration, leading to a regret growing sublinearly in the number of episodes. | Heyang Zhao, Xingrui Yu, David Mark Bossens, Ivor W. Tsang, Quanquan Gu |  |
| 858 |  |  [ORSO: Accelerating Reward Design via Online Reward Selection and Policy Optimization](https://openreview.net/forum?id=0uRc3CfJIQ) |  | 0 | Reward shaping is critical in reinforcement learning (RL), particularly for complex tasks where sparse rewards can hinder learning. However, choosing effective shaping rewards from a set of reward functions in a computationally efficient manner remains an open challenge. We propose Online Reward Selection and Policy Optimization (ORSO), a novel approach that frames the selection of shaping reward function as an online model selection problem. ORSO automatically identifies performant shaping reward functions without human intervention with provable regret guarantees. We demonstrate ORSO's effectiveness across various continuous control tasks. Compared to prior approaches, ORSO significantly reduces the amount of data required to evaluate a shaping reward function, resulting in superior data efficiency and a significant reduction in computational time (up to 8×). ORSO consistently identifies high-quality reward functions outperforming prior methods by more than 50% and on average identifies policies as performant as the ones learned using manually engineered reward functions by domain experts. | Chen Bo Calvin Zhang, ZhangWei Hong, Aldo Pacchiano, Pulkit Agrawal |  |
| 859 |  |  [Risk-Sensitive Variational Actor-Critic: A Model-Based Approach](https://openreview.net/forum?id=irrtPRFksw) |  | 0 | Risk-sensitive reinforcement learning (RL) with an entropic risk measure typically requires knowledge of the transition kernel or performs unstable updates w.r.t. exponential Bellman equations. As a consequence, algorithms that optimize this objective have been restricted to tabular or low-dimensional continuous environments. In this work we leverage the connection between the entropic risk measure and the RL-as-inference framework to develop a risk-sensitive variational actor-critic algorithm (rsVAC). Our work extends the variational framework to incorporate stochastic rewards and proposes a variational model-based actor-critic approach that modulates policy risk via a risk parameter. We consider, both, the risk-seeking and risk-averse regimes and present rsVAC learning variants for each setting. Our experiments demonstrate that this approach produces risk-sensitive policies and yields improvements in both tabular and risk-aware variants of complex continuous control tasks in MuJoCo. | Alonso Granados Baca, Reza Ebrahimi, Jason Pacheco |  |
| 860 |  |  [L3Ms - Lagrange Large Language Models](https://openreview.net/forum?id=ULGbw2URE3) |  | 0 | Supervised fine-tuning (SFT) and alignment of large language models (LLMs) are key steps in providing a good user experience. However, the concept of an appropriate alignment is inherently application-dependent, and current methods often rely on heuristic choices to drive optimization. In this work, we formulate SFT and alignment as a constrained optimization problem: the LLM is fine-tuned on a task while being required to meet application-specific requirements, without resorting to heuristics. To solve this, we propose Lagrange Large Language Models (L3Ms), which employ logarithmic barriers to enforce the constraints. This approach allows for the customization of L3Ms across diverse applications while avoiding heuristic-driven processes. We experimentally demonstrate the versatility and efficacy of L3Ms in achieving tailored alignments for various applications. | Guneet S. Dhillon, Xingjian Shi, Yee Whye Teh, Alex Smola |  |
| 861 |  |  [Locality Alignment Improves Vision-Language Models](https://openreview.net/forum?id=qssVptHTPN) |  | 0 | Vision language models (VLMs) have seen growing adoption in recent years, but many still struggle with basic spatial reasoning errors. We hypothesize that this is due to VLMs adopting pre-trained vision backbones, specifically vision transformers (ViTs) trained with image-level supervision and minimal inductive biases. Such models may fail to encode the class contents at each position in the image, and our goal is to resolve this with a vision backbone that effectively captures both local and global image semantics. Our main insight is that we do not require new supervision to learn this capability – pre-trained models contain significant knowledge of local semantics that we can extract and use for scalable self-supervision. We propose a new efficient post-training stage for ViTs called locality alignment and a novel fine-tuning procedure called MaskEmbed that uses a masked reconstruction loss to learn semantic contributions for each image patch. We first evaluate locality alignment with a vision-only benchmark, finding that it improves a model’s performance at patch-level semantic segmentation, especially for strong backbones trained with image-caption pairs (e.g., CLIP and SigLIP). We then train a series of VLMs with and without locality alignment, and show that locality-aligned backbones improve performance across a range of benchmarks, particularly ones that involve spatial understanding (e.g., RefCOCO, OCID-Ref, TallyQA, VSR, AI2D). Overall, we demonstrate that we can efficiently learn local semantic extraction via a locality alignment stage, and that this procedure benefits VLM training recipes that use off-the-shelf vision backbones. | Ian Connick Covert, Tony Sun, James Zou, Tatsunori Hashimoto |  |
| 862 |  |  [Benign Overfitting in Out-of-Distribution Generalization of Linear Models](https://openreview.net/forum?id=6jxUsDAdAu) |  | 0 | Benign overfitting refers to the phenomenon where an over-parameterized model fits the training data perfectly, including noise in the data, but still generalizes well to the unseen test data. While prior work provides some theoretical understanding of this phenomenon under the in-distribution setup, modern machine learning often operates in a more challenging Out-of-Distribution (OOD) regime, where the target (test) distribution can be rather different from the source (training) distribution. In this work, we take an initial step towards understanding benign overfitting in the OOD regime by focusing on the basic setup of over-parameterized linear models under covariate shift. We provide non-asymptotic guarantees proving that benign overfitting occurs in standard ridge regression, even under the OOD regime when the target covariance satisfies certain structural conditions. We identify several vital quantities relating to source and target covariance, which govern the performance of OOD generalization. Our result is sharp, which provably recovers prior in-distribution benign overfitting guarantee (Tsigler & Bartlett, 2023), as well as under-parameterized OOD guarantee (Ge et al., 2024) when specializing to each setup. Moreover, we also present theoretical results for a more general family of target covariance matrix, where standard ridge regression only achieves a slow statistical rate of $\mathcal{O}(1/\sqrt{n})$ for the excess risk, while Principal Component Regression (PCR) is guaranteed to achieve the fast rate $\mathcal{O}(1/n)$, where $n$ is the number of samples. | Shange Tang, Jiayun Wu, Jianqing Fan, Chi Jin |  |
| 863 |  |  [ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models](https://openreview.net/forum?id=goFpCuJalN) |  | 0 | The use of Large Language Models (LLMs) in climate science has recently gained significant attention. However, a critical issue remains: the lack of a comprehensive evaluation framework capable of assessing the quality and scientific validity of model outputs. To address this issue, we develop \*ClimaGen\* (Climate QA Generator), an adaptive learning framework that generates question-answer pairs from graduate textbooks with climate scientists in the loop. As a result, we present \*ClimaQA-Gold\*, an expert-annotated benchmark dataset alongside \*ClimaQA-Silver\*, a large-scale, comprehensive synthetic QA dataset for climate science. Finally, we develop evaluation strategies and compare different LLMs on our benchmarks. Our results offer novel insights into various approaches used to enhance knowledge of climate LLMs. ClimaQA’s source code is publicly available at https://github.com/Rose-STL-Lab/genie-climaqa | Veeramakali Vignesh Manivannan, Yasaman Jafari, Srikar Eranky, Spencer Ho, Rose Yu, Duncan WatsonParris, Yian Ma, Leon Bergen, Taylor BergKirkpatrick |  |
| 864 |  |  [Lossy Compression with Pretrained Diffusion Models](https://openreview.net/forum?id=raUnLe0Z04) |  | 0 | We apply Theis et al. (2022)'s DiffC algorithm to Stable Diffusion 1.5, 2.1, XL, and and Flux-dev, and demonstrate that these pretrained models are remarkably capable lossy image compressors. A principled algorithm for compression using pretrained diffusion models has been understood since at least 2020 (Ho et al.), but challenges in reverse-channel coding have prevented such algorithms from ever being fully implemented. We introduce simple workarounds that lead to the first complete implementation of DiffC, which is capable of compressing and decompressing images using Stable Diffusion in under 10 seconds. Despite requiring no additional training, our method is competitive with other state-of-the-art generative compression methods at low ultra-low bitrates. | Jeremy Vonderfecht, Feng Liu |  |
| 865 |  |  [Human-inspired Episodic Memory for Infinite Context LLMs](https://openreview.net/forum?id=BI2int5SAC) |  | 0 | Large language models (LLMs) have shown remarkable capabilities, but still struggle with processing extensive contexts, limiting their ability to maintain coherence and accuracy over long sequences. In contrast, the human brain excels at organising and retrieving episodic experiences across vast temporal scales, spanning a lifetime. In this work, we introduce EM-LLM, a novel approach that integrates key aspects of human episodic memory and event cognition into LLMs with no fine-tuning, enabling them to handle practically infinite context lengths while maintaining computational efficiency. EM-LLM organises sequences of tokens into coherent episodic events using a combination of Bayesian surprise and graph-theoretic boundary refinement in an online fashion. When needed, these events are retrieved through a two-stage memory process, combining similarity-based and temporally contiguous retrieval for efficient, human-inspired access to relevant information. Experiments on the LongBench and $\infty$-Bench benchmarks demonstrate EM-LLM's superior performance, consistently outperforming the state-of-the-art retrieval model InfLLM across various baseline LLMs. In addition, EM-LLM outperforms its popular counterpart, RAG, in a wide range of tasks, while requiring similar resources. Notably, EM-LLM's performance even surpasses full-context models in most tasks, while successfully performing retrieval across 10 million tokens -- a scale computationally infeasible for such models. Finally, our analysis reveals strong correlations between EM-LLM's event segmentation and human-perceived events, suggesting parallels between this artificial system and its biological counterpart, thereby offering a novel computational framework for exploring human memory mechanisms. | Zafeirios Fountas, Martin Benfeghoul, Adnan Oomerjee, Fenia Christopoulou, Gerasimos Lampouras, Haitham BouAmmar, Jun Wang |  |
| 866 |  |  [Global Convergence of Policy Gradient in Average Reward MDPs](https://openreview.net/forum?id=2PRpcmJecX) |  | 0 | We present the first comprehensive finite-time global convergence analysis of policy gradient for infinite horizon average reward Markov decision processes (MDPs). Specifically, we focus on ergodic tabular MDPs with finite state and action spaces. Our analysis shows that the policy gradient iterates converge to the optimal policy at a sublinear rate of $O(\frac{1}{T})$, where $T$ represents the number of iterations. Performance bounds for discounted reward MDPs cannot be easily extended to average reward MDPs as the bounds grow proportional to the fifth power of the effective horizon. Recent work on such extensions makes a smoothness assumption that has not been verified. Thus, our primary contribution is in providing the first complete proof that the policy gradient algorithm converges globally for average-reward MDPs, without such an assumption. We also obtain the corresponding finite-time performance guarantees. In contrast to the existing discounted reward performance bounds, our performance bounds have an explicit dependence on constants that capture the complexity of the underlying MDP. Motivated by this observation, we reexamine and improve the existing performance bounds for discounted reward MDPs. We also present simulations that empirically validate the result. | Navdeep Kumar, Yashaswini Murthy, Itai Shufaro, Kfir Yehuda Levy, R. Srikant, Shie Mannor |  |
| 867 |  |  [What Matters in Learning from Large-Scale Datasets for Robot Manipulation](https://openreview.net/forum?id=LqhorpRLIm) |  | 0 | Imitation learning from large multi-task demonstration datasets has emerged as a promising path for building generally-capable robots. As a result, 1000s of hours have been spent on building such large-scale datasets around the globe. Despite the continuous growth of such efforts, we still lack a systematic understanding of what data should be collected to improve the utility of a robotics dataset and facilitate downstream policy learning. In this work, we conduct a large-scale dataset composition study to answer this question. We develop a data generation framework to procedurally emulate common sources of diversity in existing datasets (such as sensor placements and object types and arrangements), and use it to generate large-scale robot datasets with controlled compositions, enabling a suite of dataset composition studies that would be prohibitively expensive in the real world. We focus on two practical settings: (1) what types of diversity should be emphasized when future researchers collect large-scale datasets for robotics, and (2) how should current practitioners retrieve relevant demonstrations from existing datasets to maximize downstream policy performance on tasks of interest. Our study yields several critical insights -- for example, we find that camera poses and spatial arrangements are crucial dimensions for both diversity in collection and alignment in retrieval. In real-world robot learning settings, we find that not only do our insights from simulation carry over, but our retrieval strategies on existing datasets such as DROID allow us to consistently outperform existing training strategies by up to 70\%. | Vaibhav Saxena, Matthew Bronars, Nadun Ranawaka Arachchige, Kuancheng Wang, WooChul Shin, Soroush Nasiriany, Ajay Mandlekar, Danfei Xu |  |
| 868 |  |  [Rapidly Adapting Policies to the Real-World via Simulation-Guided Fine-Tuning](https://openreview.net/forum?id=XwUrzurG94) |  | 0 | Robot learning requires a considerable amount of high-quality data to realize the promise of generalization. However, large data sets are costly to collect in the real world. Physics simulators can cheaply generate vast data sets with broad coverage over states, actions, and environments. However, physics engines are fundamentally misspecified approximations to reality. This makes direct zero-shot transfer from simulation to reality challenging, especially in tasks where precise and force-sensitive manipulation is necessary. Thus, fine-tuning these policies with small real-world data sets is an appealing pathway for scaling robot learning. However, current reinforcement learning fine-tuning frameworks leverage general, unstructured exploration strategies which are too inefficient to make real-world adaptation practical. This paper introduces the \emph{Simulation-Guided Fine-tuning} (SGFT) framework, which demonstrates how to extract structural priors from physics simulators to substantially accelerate real-world adaptation. Specifically, our approach uses a value function learned in simulation to guide real-world exploration. We demonstrate this approach across five real-world dexterous manipulation tasks where zero-shot sim-to-real transfer fails. We further demonstrate our framework substantially outperforms baseline fine-tuning methods, requiring up to an order of magnitude fewer real-world samples and succeeding at difficult tasks where prior approaches fail entirely. Last but not least, we provide theoretical justification for this new paradigm which underpins how SGFT can rapidly learn high-performance policies in the face of large sim-to-real dynamics gaps. | Patrick Yin, Tyler Westenbroek, ChingAn Cheng, Andrey Kolobov, Abhishek Gupta |  |
| 869 |  |  [Empowering Users in Digital Privacy Management through Interactive LLM-Based Agents](https://openreview.net/forum?id=FEpAUnS7f7) |  | 0 | This paper presents a novel application of large language models (LLMs) to enhance user comprehension of privacy policies through an interactive dialogue agent. We demonstrate that LLMs significantly outperform traditional models in tasks like Data Practice Identification, Choice Identification, Policy Summarization, and Privacy Question Answering, setting new benchmarks in privacy policy analysis. Building on these findings, we introduce an innovative LLM-based agent that functions as an expert system for processing website privacy policies, guiding users through complex legal language without requiring them to pose specific questions. A user study with 100 participants showed that users assisted by the agent had higher comprehension levels (mean score of 2.6 out of 3 vs. 1.8 in the control group), reduced cognitive load (task difficulty ratings of 3.2 out of 10 vs. 7.8), increased confidence in managing privacy, and completed tasks in less time (5.5 minutes vs. 15.8 minutes). This work highlights the potential of LLM-based agents to transform user interaction with privacy policies, leading to more informed consent and empowering users in the digital services landscape. | Bolun Sun, Yifan Zhou, Haiyun Jiang |  |
| 870 |  |  [Object-Centric Pretraining via Target Encoder Bootstrapping](https://openreview.net/forum?id=7d2JwGbxhA) |  | 0 | Object-centric representation learning has recently been successfully applied to real-world datasets. This success can be attributed to pretrained non-object-centric foundation models, whose features serve as reconstruction targets for slot attention. However, targets must remain frozen throughout the training, which sets an upper bound on the performance object-centric models can attain. Attempts to update the target encoder by bootstrapping result in large performance drops, which can be attributed to its lack of object-centric inductive biases, causing the object-centric model’s encoder to drift away from representations useful as reconstruction targets. To address these limitations, we propose \*\*O\*\*bject-\*\*CE\*\*ntric Pretraining by Target Encoder \*\*BO\*\*otstrapping, a self-distillation setup for training object-centric models from scratch, on real-world data, for the first time ever. In OCEBO, the target encoder is updated as an exponential moving average of the object-centric model, thus explicitly being enriched with object-centric inductive biases introduced by slot attention while removing the upper bound on performance present in other models. We mitigate the slot collapse caused by random initialization of the target encoder by introducing a novel cross-view patch filtering approach that limits the supervision to sufficiently informative patches. When pretrained on 241k images from COCO, OCEBO achieves unsupervised object discovery performance comparable to that of object-centric models with frozen non-object-centric target encoders pretrained on hundreds of millions of images. The code and pretrained models are publicly available at https://github.com/djukicn/ocebo. | Nikola Dukic, Tim Lebailly, Tinne Tuytelaars |  |
| 871 |  |  [Modality-Specialized Synergizers for Interleaved Vision-Language Generalists](https://openreview.net/forum?id=7UgQjFEadn) |  | 0 | Recent advancements in Vision-Language Models (VLMs) have led to the emergence of Vision-Language Generalists (VLGs) capable of understanding and generating both text and images. However, seamlessly generating an arbitrary sequence of text and images remains a challenging task for the current VLGs. One primary limitation lies in applying a unified architecture and the same set of parameters to simultaneously model discrete text tokens and continuous image features. Recent works attempt to tackle this fundamental problem by introducing modality-aware expert models. However, they employ identical architectures to process both text and images, disregarding the intrinsic inductive biases in these two modalities. In this work, we introduce Modality-Specialized Synergizers (MoSS), a novel design that efficiently optimizes existing unified architectures of VLGs with modality-specialized adaptation layers, i.e., a Convolutional LoRA for modeling the local priors of image patches and a Linear LoRA for processing sequential text. This design enables more effective modeling of modality-specific features while maintaining the strong cross-modal integration gained from pretraining. In addition, to improve the instruction-following capability on interleaved text-and-image generation, we introduce LeafInstruct, the first open-sourced interleaved instruction tuning dataset comprising 184,982 high-quality instances on more than 10 diverse domains. Extensive experiments show that VLGs integrated with MoSS achieve state-of-the-art performance, significantly surpassing baseline VLGs in complex interleaved generation tasks. Furthermore, our method exhibits strong generalizability on different VLGs. | Zhiyang Xu, Minqian Liu, Ying Shen, Joy Rimchala, Jiaxin Zhang, Qifan Wang, Yu Cheng, Lifu Huang |  |
| 872 |  |  [Self-Normalized Resets for Plasticity in Continual Learning](https://openreview.net/forum?id=G82uQztzxl) |  | 0 | Plasticity Loss is an increasingly important phenomenon that refers to the empirical observation that as a neural network is continually trained on a sequence of changing tasks, its ability to adapt to a new task diminishes over time. We introduce Self-Normalized Resets (SNR), a simple adaptive algorithm that mitigates plasticity loss by resetting a neuron’s weights when evidence suggests its firing rate has effectively dropped to zero. Across a battery of continual learning problems and network architectures, we demonstrate that SNR consistently attains superior performance compared to its competitor algorithms. We also demonstrate that SNR is robust to its sole hyperparameter, its rejection percentile threshold, while competitor algorithms show significant sensitivity. SNR’s threshold-based reset mechanism is motivated by a simple hypothesis test we derive. Seen through the lens of this hypothesis test, competing reset proposals yield suboptimal error rates in correctly detecting inactive neurons, potentially explaining our experimental observations. We also conduct a theoretical investigation of the optimization landscape for the problem of learning a single ReLU. We show that even when initialized adversarially, an idealized version of SNR learns the target ReLU, while regularization based approaches can fail to learn. | Vivek F. Farias, Adam Daniel Jozefiak |  |
| 873 |  |  [Homomorphism Counts as Structural Encodings for Graph Learning](https://openreview.net/forum?id=qFw2RFJS5g) |  | 0 | Graph Transformers are popular neural networks that extend the well-known Transformer architecture to the graph domain. These architectures operate by applying self-attention on graph nodes and incorporating graph structure through the use of positional encodings (e.g., Laplacian positional encoding) or structural encodings (e.g., random-walk structural encoding). The quality of such encodings is critical, since they provide the necessary \emph{graph inductive biases} to condition the model on graph structure. In this work, we propose \emph{motif structural encoding} (MoSE) as a flexible and powerful structural encoding framework based on counting graph homomorphisms. Theoretically, we compare the expressive power of MoSE to random-walk structural encoding and relate both encodings to the expressive power of standard message passing neural networks. Empirically, we observe that MoSE outperforms other well-known positional and structural encodings across a range of architectures, and it achieves state-of-the-art performance on a widely studied molecular property prediction dataset. | Linus Bao, Emily Jin, Michael M. Bronstein, Ismail Ilkan Ceylan, Matthias Lanzinger |  |
| 874 |  |  [Real2Code: Reconstruct Articulated Objects via Code Generation](https://openreview.net/forum?id=CAssIgPN4I) |  | 0 | We present Real2Code, a novel approach to reconstructing articulated objects via code generation. Given visual observations of an object, we first reconstruct its part geometry using image segmentation and shape completion. We represent these object parts with oriented bounding boxes, from which a fine-tuned large language model (LLM) predicts joint articulation as code. By leveraging pre-trained vision and language models, our approach scales elegantly with the number of articulated parts, and generalizes from synthetic training data to real world objects in unstructured environments. Experimental results demonstrate that Real2Code significantly outperforms the previous state-of-the-art in terms of reconstruction accuracy, and is the first approach to extrapolate beyond objects' structural complexity in the training set, as we show for objects with up to 10 articulated parts. When incorporated with a stereo reconstruction model, Real2Code moreover generalizes to real-world objects, given only a handful of multi-view RGB images and without the need for depth or camera information. | Zhao Mandi, Yijia Weng, Dominik Bauer, Shuran Song |  |
| 875 |  |  [An Asynchronous Bundle Method for Distributed Learning Problems](https://openreview.net/forum?id=Kwo20MWWCb) |  | 0 | We propose a novel asynchronous bundle method to solve distributed learning problems. Compared to existing asynchronous methods, our algorithm computes the next iterate based on a more accurate approximation of the objective function and does not require any prior information about the maximal information delay in the system. This makes the proposed method fast and easy to tune. We prove that the algorithm converges in both deterministic and stochastic (mini-batch) settings, and quantify how the convergence times depend on the level of asynchrony. The practical advantages of our method are illustrated through numerical experiments on classification problems of varying complexities and scales. | Daniel Cederberg, Xuyang Wu, Stephen P. Boyd, Mikael Johansson |  |
| 876 |  |  [On Linear Representations and Pretraining Data Frequency in Language Models](https://openreview.net/forum?id=EDoD3DgivF) |  | 0 | Pretraining data has a direct impact on the behaviors and quality of language models (LMs), but we only understand the most basic principles of this relationship. While most work focuses on pretraining data's effect on downstream task behavior, we investigate its relationship to LM representations. Previous work has discovered that, in language models, some concepts are encoded "linearly" in the representations, but what factors cause these representations to form (or not)? We study the connection between pretraining data frequency and models' linear representations of factual relations (e.g., mapping France to Paris in a capital prediction task). We find evidence that the formation of linear representations is strongly connected to pretraining term frequencies; specifically for subject-relation-object fact triplets, both subject-object co-occurrence frequency and in-context learning accuracy for the relation are highly correlated with linear representations. This is the case across all phases of pretraining, i.e., it is not affected by the model's underlying capability. In OLMo-7B and GPT-J (6B), we discover that a linear representation consistently (but not exclusively) forms when the subjects and objects within a relation co-occur at least 1k and 2k times, respectively, regardless of when these occurrences happen during pretraining (and around 4k times for OLMo-1B). Finally, we train a regression model on measurements of linear representation quality in fully-trained LMs that can predict how often a term was seen in pretraining. Our model achieves low error even on inputs from a different model with a different pretraining dataset, providing a new method for estimating properties of the otherwise-unknown training data of closed-data models. We conclude that the strength of linear representations in LMs contains signal about the models' pretraining corpora that may provide new avenues for controlling and improving model behavior: particularly, manipulating the models' training data to meet specific frequency thresholds. We release our code to support future work. | Jack Merullo, Noah A. Smith, Sarah Wiegreffe, Yanai Elazar |  |
| 877 |  |  [CoRNStack: High-Quality Contrastive Data for Better Code Retrieval and Reranking](https://openreview.net/forum?id=iyJOUELYir) |  | 0 | Effective code retrieval plays a crucial role in advancing code generation, bug fixing, and software maintenance, particularly as software systems increase in complexity. While current code embedding models have demonstrated promise in retrieving code snippets for small-scale, well-defined tasks, they often underperform in more demanding real-world applications such as bug localization within GitHub repositories. We hypothesize that a key issue is their reliance on noisy and inconsistent datasets for training, which impedes their ability to generalize to more complex retrieval scenarios. To address these limitations, we introduce CoRNStack, a large-scale, high-quality contrastive training dataset for code that spans multiple programming languages. This dataset is curated using consistency filtering to eliminate noisy positives and is further enriched with mined hard negatives, thereby facilitating more effective learning. We demonstrate that contrastive training of embedding models using CoRNStack leads to state-of-the-art performance across a variety of code retrieval tasks. Furthermore, the dataset can be leveraged for training code reranking models, a largely underexplored area compared to text reranking. Our finetuned code reranking model significantly improves the ranking quality over the retrieved results. Finally, by employing our code retriever and reranker together, we demonstrate significant improvements in function localization for GitHub issues, an important component of real-world software development. | Tarun Suresh, Revanth Gangi Reddy, Yifei Xu, Zach Nussbaum, Andriy Mulyar, Brandon Duderstadt, Heng Ji |  |
| 878 |  |  [BingoGuard: LLM Content Moderation Tools with Risk Levels](https://openreview.net/forum?id=HPSAkIHRbb) |  | 0 | Malicious content generated by large language models (LLMs) can pose varying degrees of harm. Although existing LLM-based moderators can detect harmful content, they struggle to assess risk levels and may miss lower-risk outputs. Accurate risk assessment allows platforms with different safety thresholds to tailor content filtering and rejection. In this paper, we introduce per-topic severity rubrics for 11 harmful topics and build BingoGuard, an LLM-based moderation system designed to predict both binary safety labels and severity levels. To address the lack of annotations on levels of severity, we propose a scalable generate-then-filter framework that first generates responses across different severity levels and then filters out low-quality responses. Using this framework, we create BingoGuardTrain, a training dataset with 54,897 examples covering a variety of topics, response severity, styles, and BingoGuardTest, a test set with 988 examples explicitly labeled based on our severity rubrics that enables fine-grained analysis on model behaviors on different severity levels. Our BingoGuard-8B, trained on BingoGuardTrain, achieves the state-of-the-art performance on several moderation benchmarks, including WildGuardTest and HarmBench, as well as BingoGuardTest, outperforming best public models, WildGuard, by 4.3\%. Our analysis demonstrates that incorporating severity levels into training significantly enhances detection performance and enables the model to effectively gauge the severity of harmful responses. Warning: this paper includes red-teaming examples that may be harmful in nature. | Fan Yin, Philippe Laban, Xiangyu Peng, Yilun Zhou, Yixin Mao, Vaibhav Vats, Linnea Ross, Divyansh Agarwal, Caiming Xiong, ChienSheng Wu |  |
| 879 |  |  [Learn hybrid prototypes for multivariate time series anomaly detection](https://openreview.net/forum?id=8TBGdH3t6a) |  | 0 | In multivariate time series anomaly detection (MTSAD), reconstruction-based models reconstruct testing series with learned knowledge of only normal series and identify anomalies with higher reconstruction errors. In practice, over-generalization often occurs with unexpectedly well reconstruction of anomalies. Although memory banks are employed by reconstruction-based models to fight against over-generalization, these models are only efficient to detect point anomalies since they learn normal prototypes from time points, leaving interval anomalies and periodical anomalies to be discovered. To settle this problem, this paper propose a hybrid prototypes learning model for MTSAD based on reconstruction, named as H-PAD. First, normal prototypes are learned from different sizes of the patches for time series to discover interval anomalies. These prototypes in different sizes are integrated together to reconstruct query series so that any anomalies would be smoothed off and high reconstruction errors are produced. Furthermore, period prototypes are learned to discover periodical anomalies. One period prototype is memorized for one variable of the query series. Finally, extensive experiments on five benchmark datasets show the effectiveness of H-PAD. | KeYuan Shen |  |
| 880 |  |  [Reasoning with Latent Thoughts: On the Power of Looped Transformers](https://openreview.net/forum?id=din0lGfZFd) |  | 0 | Large language models have shown remarkable reasoning abilities and scaling laws suggest that large parameter count, especially along the depth axis, is the primary driver. In this work, we make a stronger claim --- many reasoning problems require a large depth but not necessarily many parameters. This unlocks a novel application of looped models for reasoning. Firstly, we show that for many synthetic reasoning problems like addition, $p$-hop induction, and math problems, a $k$-layer transformer looped $L$ times nearly matches the performance of a $kL$-layer non-looped model, and is significantly better than a $k$-layer model. This is further corroborated by theoretical results showing that many such reasoning problems can be solved via iterative algorithms, and thus, can be solved effectively using looped models with nearly optimal depth. Perhaps surprisingly, these benefits also translate to practical settings of language modeling --- on many downstream reasoning tasks, a language model with $k$-layers looped $L$ times can be competitive to, if not better than, a $kL$-layer language model. In fact, our empirical analysis reveals an intriguing phenomenon: looped and non-looped models exhibit scaling behavior that depends on their effective depth, akin to the inference-time scaling of chain-of-thought (CoT) reasoning. We further elucidate the connection to CoT reasoning by proving that looped models implicitly generate latent thoughts and can simulate $T$ steps of CoT with $T$ loops. Inspired by these findings, we also present an interesting dichotomy between reasoning and memorization, and design a looping-based regularization that is effective on both fronts. | Nikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, Sashank J. Reddi |  |
| 881 |  |  [Conservative Contextual Bandits: Beyond Linear Representations](https://openreview.net/forum?id=SThJXvucjQ) |  | 0 | Conservative Contextual Bandits (CCBs) address safety in sequential decision making by requiring that an agent's policy, along with minimizing regret, also satisfies a safety constraint: the performance is not worse than a baseline policy (e.g., the policy that the company has in production) by more than $(1+\alpha)$ factor. Prior work developed UCB-style algorithms for this problem in the multi-armed (Wu et al., 2016) and contextual linear (Kazerouni et al., 2017) settings. However, in practice the cost of the arms is often a non-linear function, and therefore existing UCB algorithms are ineffective in such settings. In this paper, we consider CCBs beyond the linear case and develop two algorithms $\mathtt{C\text{-}SquareCB}$ and $\mathtt{C\text{-}FastCB}$, using Inverse Gap Weighting (IGW) based exploration and an online regression oracle. We show that the safety constraint is satisfied in high probability and that the regret for $\mathtt{C\text{-}SquareCB}$ is sub-linear in horizon $T$, while the the regret for $\mathtt{C\text{-}FastCB}$ is first-order and is sub-linear in $L^\*$, the cumulative loss of the optimal policy. Subsequently, we use a neural network for function approximation and online gradient descent as the regression oracle to provide $\tilde{\mathcal{O}}\big(\sqrt{KT} + K/\alpha\big) $ and $\tilde{\mathcal{O}}\big(\sqrt{KL^\*} + K (1 + 1/\alpha)\big)$ regret bounds respectively. Finally, we demonstrate the efficacy of our algorithms on real world data, and show that they significantly outperform the existing baseline while maintaining the performance guarantee. | Rohan Deb, Mohammad Ghavamzadeh, Arindam Banerjee |  |
| 882 |  |  [Quantifying Generalization Complexity for Large Language Models](https://openreview.net/forum?id=jpSLXoRKnH) |  | 0 | While large language models (LLMs) have shown exceptional capabilities in understanding complex queries and performing sophisticated tasks, their generalization abilities are often deeply entangled with memorization, necessitating more precise evaluation. To address this challenge, we introduce Scylla, a dynamic evaluation framework that quantitatively measures the generalization abilities of LLMs. Scylla disentangles generalization from memorization via assessing model performance on both in-distribution (ID) and out-of-distribution (OOD) data through 20 tasks across 5 levels of complexity. Through extensive experiments, we uncover a non-monotonic relationship between task complexity and the performance gap between ID and OOD data, which we term the generalization valley. Specifically, this phenomenon reveals a critical threshold---referred to as critical complexity---where reliance on non-generalizable behavior peaks, indicating the upper bound of LLMs' generalization capabilities. As model size increases, the critical complexity shifts toward higher levels of task complexity, suggesting that larger models can handle more complex reasoning tasks before over-relying on memorization. Leveraging Scylla and the concept of critical complexity, we benchmark 28 LLMs including both open-sourced models such as LLaMA and Qwen families, and closed-sourced models like Claude and GPT, providing a more robust evaluation and establishing a clearer understanding of LLMs' generalization capabilities. | Zhenting Qi, Hongyin Luo, Xuliang Huang, Zhuokai Zhao, Yibo Jiang, Xiangjun Fan, Himabindu Lakkaraju, James R. Glass |  |
| 883 |  |  [SqueezeAttention: 2D Management of KV-Cache in LLM Inference via Layer-wise Optimal Budget](https://openreview.net/forum?id=9HK2rHNAhd) |  | 0 | Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has been considered critical to saving the cost of inference. Most of the existing KV-cache compression algorithms attempted to sparsify the sequence of tokens by taking advantage of the different importance of tokens. However, most of these methods treat all layers equally, allocating the same KV budget to each layer. This approach is suboptimal, as some layers may be less sensitive to input tokens yet still receive the same budget as others. In this work, we found that by identifying the importance of attention layers, we could optimize the KV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based on our observations regarding layer-wise importance in inference, we propose \sys to precisely optimize the allocation of KV-cache budget among layers on-the-fly and then incorporate three representative sequence-wise algorithms to compress the KV-cache for each layer with its very own budget. Specifically, we first measure each layer's importance by calculating the cosine similarity of the input prompt differences before and after the self-attention layers. Based on this similarity, we then categorize the layers into two groups and adjust their KV budgets accordingly. By optimizing the KV-cache from both sequence's and layer's dimensions, \sys achieves around 30\% to 70\% of the memory reductions and up to 2.2 $\times$ of throughput improvements in a wide range of LLMs and benchmarks. The code is available at https://github.com/hetailang/SqueezeAttention. | Zihao Wang, Bin Cui, Shaoduo Gan |  |
| 884 |  |  [Gaussian Mixture Counterfactual Generator](https://openreview.net/forum?id=lBB3eSn6fY) |  | 0 | We address the individualized treatment effect (ITE) estimation problem, focusing on continuous, multidimensional, and time-dependent treatments for precision medicine. The central challenge lies in modeling these complex treatment scenarios while capturing dynamic patient responses and minimizing reliance on control data. We propose the Gaussian Mixture Counterfactual Generator (GMCG), a generative model that transforms the Gaussian mixture model—traditionally a tool for clustering and density estimation—into a new tool explicitly geared toward causal inference. This approach generates robust counterfactuals by effectively handling continuous and multidimensional treatment spaces. We evaluate GMCG on synthetic crossover trial data and simulated datasets, demonstrating its superior performance over existing methods, particularly in scenarios with limited control data. GMCG derives its effectiveness from modeling the joint distribution of covariates, treatments, and outcomes using a latent state vector while employing a conditional distribution of the state vector to suppress confounding and isolate treatment-outcome relationships. | JongHoon Ahn, Akshay Vashist |  |
| 885 |  |  [Multi-domain Distribution Learning for De Novo Drug Design](https://openreview.net/forum?id=g3VCIM94ke) |  | 0 | We introduce DrugFlow, a generative model for structure-based drug design that integrates continuous flow matching with discrete Markov bridges, demonstrating state-of-the-art performance in learning chemical, geometric, and physical aspects of three-dimensional protein-ligand data. We endow DrugFlow with an uncertainty estimate that is able to detect out-of-distribution samples. To further enhance the sampling process towards distribution regions with desirable metric values, we propose a joint preference alignment scheme applicable to both flow matching and Markov bridge frameworks. Furthermore, we extend our model to also explore the conformational landscape of the protein by jointly sampling side chain angles and molecules. | Arne Schneuing, Ilia Igashov, Adrian W. Dobbelstein, Thomas Castiglione, Michael M. Bronstein, Bruno Correia |  |
| 886 |  |  [{τ}-bench: A Benchmark for \underline{T}ool-\underline{A}gent-\underline{U}ser Interaction in Real-World Domains](https://openreview.net/forum?id=roNSXZpUDN) |  | 0 | Existing benchmarks for language agents do not set them up to interact with human users or follow domain-specific rules, both of which are vital to safe and realistic deployment. We propose $\tau$-bench, a benchmark with two domains (retail and airline) emulating dynamic conversations between a user (simulated by language models) and a customer service agent provided with domain-specific API tools and policy guidelines. We employ a efficient and faithful evaluation process that compares the database state at the end of a conversation with the annotated goal state, and propose a new metric (pass^k) to evaluate the reliability of agent behavior over multiple trials. Our experiments show that even state-of-the-art function calling agents (gpt-4o) succeed on $<50\%$ of the tasks, and are terribly inconsistent (pass^8 < 25\% in retail). Our findings point to the need for methods that can improve the ability of agents to act consistently and reliably follow rules. | Shunyu Yao, Noah Shinn, Pedram Razavi, Karthik R. Narasimhan |  |
| 887 |  |  [DEPfold: RNA Secondary Structure Prediction as Dependency Parsing](https://openreview.net/forum?id=DpLFmc09pC) |  | 0 | RNA secondary structure prediction is critical for understanding RNA function but remains challenging due to complex structural elements like pseudoknots and limited training data. We introduce DEPfold, a novel deep learning approach that re-frames RNA secondary structure prediction as a dependency parsing problem. DEPfold presents three key innovations: (1) a biologically motivated transformation of RNA structures into labeled dependency trees, (2) a biaffine attention mechanism for joint prediction of base pairings and their types, and (3) an optimal tree decoding algorithm that enforces valid RNA structural constraints. Unlike traditional energy-based methods, DEPfold learns directly from annotated data and leverages pretrained language models to predict RNA structure. We evaluate DEPfold on both within-family and cross-family RNA datasets, demonstrating significant performance improvements over existing methods. DEPfold shows strong performance in cross-family generalization when trained on data augmented by traditional energy-based models, outperforming existing methods on the bpRNAnew dataset. This demonstrates DEPfold’s ability to effectively learn structural information beyond what traditional methods capture. Our approach bridges natural language processing (NLP) with RNA biology, providing a computationally efficient and adaptable tool for advancing RNA structure prediction and analysis | Ke Wang, Shay B. Cohen |  |
| 888 |  |  [APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding](https://openreview.net/forum?id=yUC8pU508S) |  | 0 | Context-augmented generation (CAG) techniques, including RAG and ICL, require the efficient combination of multiple contexts to generate responses to user queries. Directly inputting these contexts as a sequence introduces a considerable computational burden by re-encoding the combined selection of contexts for every request. To address this, we explore the promising potential of parallel encoding to independently pre-compute and cache each context's KV states. This approach enables the direct loading of cached states during inference while accommodating more contexts through position reuse across contexts. However, due to misalignments in attention distribution, directly applying parallel encoding results in a significant performance drop. To enable effective and efficient CAG, we propose Adaptive Parallel Encoding (\*\*APE\*\*), which brings shared prefix, attention temperature, and scaling factor to align the distribution of parallel encoding with sequential encoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98\% and 93\% sequential encoding performance using the same inputs while outperforming parallel encoding by 3.6\% and 7.9\%, respectively. It also scales to many-shot CAG, effectively encoding hundreds of contexts in parallel. Efficiency evaluation shows that APE can achieve an end-to-end 4.5$\times$ speedup by reducing 28$\times$ prefilling time for a 128K-length context. The code is available at https://github.com/Infini-AI-Lab/APE. | Xinyu Yang, Tianqi Chen, Beidi Chen |  |
| 889 |  |  [SFS: Smarter Code Space Search improves LLM Inference Scaling](https://openreview.net/forum?id=MCHuGOkExF) |  | 0 | We frame code generation as a black-box optimization problem within the code space and demonstrate how optimization-inspired techniques can enhance inference scaling over text. Based on this perspective, we propose \*\*SCATTERED FOREST SEARCH (SFS)\*\*, a novel approach that improves solution diversity during evolutionary search, thereby avoiding local optima. Our theoretical analysis illustrates how these methods improve exploration and enhance efficiency. Extensive experiments on \*HumanEval, MBPP, APPS, CodeContests,\* and \*Leetcode\* reveal significant performance gains. For instance, our method achieves a \*\*pass@1 rate of 67.1% on HumanEval+\*\* and \*\*87.2% on HumanEval with GPT-3.5\*\*, marking improvements of \*\*8.6%\*\* and \*\*4.3%\*\* over the state-of-the-art, while also halving the iterations needed to find the correct solution. Furthermore, our approach scales more efficiently than existing search techniques, including \*\*tree search, line search,\*\* and \*\*repeated sampling (Best of N)\*\*. | Jonathan Light, Yue Wu, Yiyou Sun, Wenchao Yu, Yanchi Liu, Xujiang Zhao, Ziniu Hu, Haifeng Chen, Wei Cheng |  |
| 890 |  |  [Aioli: A Unified Optimization Framework for Language Model Data Mixing](https://openreview.net/forum?id=sZGZJhaNSe) |  | 0 | Language model performance depends on identifying the optimal mixture of data groups to train on (e.g., law, code, math). Prior work has proposed a diverse set of methods to efficiently learn mixture proportions, ranging from fitting regression models over training runs to dynamically updating proportions throughout training. Surprisingly, we find that no existing method consistently outperforms a simple stratified sampling baseline in terms of average test perplexity. To understand this inconsistency, we unify existing methods into a standard framework, showing they are equivalent to solving a common optimization problem: minimize average loss subject to a method-specific mixing law---an implicit assumption on the relationship between loss and mixture proportions. This framework suggests that measuring the fidelity of a method's mixing law can offer insights into its performance. Empirically, we find that existing methods set their mixing law parameters inaccurately, resulting in the inconsistent mixing performance we observe. Using this insight, we derive a new online method named Aioli, which directly estimates the mixing law parameters throughout training and uses them to dynamically adjust proportions. Empirically, Aioli outperforms stratified sampling on 6 out of 6 datasets by an average of 0.27 test perplexity points, whereas existing methods fail to consistently beat stratified sampling, doing up to 6.9 points worse. Moreover, in a practical setting where proportions are learned on shorter runs due to computational constraints, Aioli can dynamically adjust these proportions over the full training run, consistently improving performance over existing methods by up to 12.012 test perplexity points. | Mayee F. Chen, Michael Y. Hu, Nicholas Lourie, Kyunghyun Cho, Christopher Ré |  |
| 891 |  |  [Differentiable Optimization of Similarity Scores Between Models and Brains](https://openreview.net/forum?id=vWRwdmA3wU) |  | 0 | How do we know if two systems - biological or artificial - process information in a similar way? Similarity measures such as linear regression, Centered Kernel Alignment (CKA), Normalized Bures Similarity (NBS), and angular Procrustes distance, are often used to quantify this similarity. However, it is currently unclear what drives high similarity scores and even what constitutes a "good" score. Here, we introduce a novel tool to investigate these questions by differentiating through similarity measures to directly maximize the score. Surprisingly, we find that high similarity scores do not guarantee encoding task-relevant information in a manner consistent with neural data; and this is particularly acute for CKA and even some variations of cross-validated and regularized linear regression. We find no consistent threshold for a good similarity score - it depends on both the measure and the dataset. In addition, synthetic datasets optimized to maximize similarity scores initially learn the highest variance principal component of the target dataset, but some methods like angular Procrustes capture lower variance dimensions much earlier than methods like CKA. To shed light on this, we mathematically derive the sensitivity of CKA, angular Procrustes, and NBS to the variance of principal component dimensions, and explain the emphasis CKA places on high variance components. Finally, by jointly optimizing multiple similarity measures, we characterize their allowable ranges and reveal that some similarity measures are more constraining than others. While current measures offer a seemingly straightforward way to quantify the similarity between neural systems, our work underscores the need for careful interpretation. We hope the tools we developed will be used by practitioners to better understand current and future similarity measures. | Nathan Cloos, Moufan Li, Markus Siegel, Scott L. Brincat, Earl K. Miller, Guangyu Robert Yang, Christopher J. Cueva |  |
| 892 |  |  [Provable Convergence Bounds for Hybrid Dynamical Sampling and Optimization](https://openreview.net/forum?id=FJv8VMPxWi) |  | 0 | Analog dynamical accelerators (DXs) are a growing sub-field in computer architecture research, offering order-of-magnitude gains in power efficiency and latency over traditional digital methods in several machine learning, optimization, and sampling tasks. However, limited-capacity accelerators require hybrid analog/digital algorithms to solve real-world problems, commonly using large-neighborhood local search (LNLS) frameworks. Unlike fully digital algorithms, hybrid LNLS has no non-asymptotic convergence guarantees and no principled hyperparameter selection schemes, particularly limiting cross-device training and inference. In this work, we provide non-asymptotic convergence guarantees for hybrid LNLS by reducing to block Langevin Diffusion (BLD) algorithms. Adapting tools from classical sampling theory, we prove exponential KL-divergence convergence for randomized and cyclic block selection strategies using ideal DXs. With finite device variation, we provide explicit bounds on the 2-Wasserstein bias in terms of step duration, noise strength, and function parameters. Our BLD model provides a key link between established theory and novel computing platforms, and our theoretical results provide a closed-form expression linking device variation, algorithm hyperparameters, and performance. | Matthew X. Burns, Qingyuan Hou, Michael C. Huang |  |
| 893 |  |  [CTSyn: A Foundation Model for Cross Tabular Data Generation](https://openreview.net/forum?id=Sh4FOyZRpv) |  | 0 | Generative Foundation Models (GFMs) have achieved remarkable success in producing high-quality synthetic data for images and text. However, their application to tabular data presents significant challenges due to the heterogeneous nature of table features. Current cross-table learning frameworks struggle because they lack a generative model backbone and an effective mechanism to decode heterogeneous feature values. To address these challenges, we propose the Cross-Table Synthesizer (CTSyn), a diffusion-based generative foundation model for tabular data generation. CTSyn comprises two key components. The first is an autoencoder network that consolidates diverse tables into a unified latent space. It dynamically reconstructs table values using a table schema embedding, allowing adaptation to heterogeneous datasets. The second is a conditional latent diffusion model that generates samples from the learned latent space, conditioned on the table schema. Through large-scale pre-training, CTSyn outperforms existing table synthesizers on standard benchmarks in both utility and diversity. These results position CTSyn as a promising framework for synthetic table generation and lay the groundwork for developing large-scale tabular foundation models. | Xiaofeng Lin, Chenheng Xu, Matthew Yang, Guang Cheng |  |
| 894 |  |  [Solving hidden monotone variational inequalities with surrogate losses](https://openreview.net/forum?id=4ZX2a3OKEV) |  | 0 | Deep learning has proven to be effective in a wide variety of loss minimization problems. However, many applications of interest, like minimizing projected Bellman error and min-max optimization, cannot be modelled as minimizing a scalar loss function but instead correspond to solving a variational inequality (VI) problem. This difference in setting has caused many practical challenges as naive gradient-based approaches from supervised learning tend to diverge and cycle in the VI case. In this work, we propose a principled surrogate-based approach compatible with deep learning to solve VIs. We show that our surrogate-based approach has three main benefits: (1) under assumptions that are realistic in practice (when hidden monotone structure is present, interpolation, and sufficient optimization of the surrogates), it guarantees convergence, (2) it provides a unifying perspective of existing methods, and (3) is amenable to existing deep learning optimizers like ADAM. Experimentally, we demonstrate our surrogate-based approach is effective in min-max optimization and minimizing projected Bellman error. Furthermore, in the deep reinforcement learning case, we propose a novel variant of TD(0) which is more compute and sample efficient. | Ryan D'Orazio, Danilo Vucetic, Zichu Liu, Junhyung Lyle Kim, Ioannis Mitliagkas, Gauthier Gidel |  |
| 895 |  |  [TorchTitan: One-stop PyTorch native solution for production ready LLM pretraining](https://openreview.net/forum?id=SFN6Wm7YBI) |  | 0 | The development of large language models (LLMs) has been instrumental in advancing state-of-the-art natural language processing applications. Training LLMs with billions of parameters and trillions of tokens requires sophisticated distributed systems that enable composing and comparing several state-of-the-art techniques in order to efficiently scale across thousands of accelerators. However, existing solutions are complex, scattered across multiple libraries/repositories, lack interoperability, and are cumbersome to maintain. Thus, curating and empirically comparing training recipes requires non-trivial engineering effort. This paper introduces \*\*TORCHTITAN\*\*$^1$, a PyTorch-native distributed training system that unifies and advances state-of-the-art techniques, streamlining integration and reducing engineering overhead. TORCHTITAN enables seamless application of 4D parallelism in a modular and composable manner, while featuring elastic scaling to adapt to changing computational requirements. The system provides comprehensive logging, efficient checkpointing, and debugging tools, ensuring production-ready training. Moreover, TORCHTITAN incorporates innovative hardware-software co-designed solutions, leveraging cutting-edge features like Float8 training and SymmetricMemory to maximize hardware utilization. As a flexible experimental test bed, TORCHTITAN facilitates the curation and comparison of custom recipes for diverse training contexts. By leveraging TORCHTITAN, we developed optimized training recipes for the Llama 3.1 family and provide actionable guidance on selecting and combining distributed training techniques to maximize training efficiency, based on our hands-on experiences. We thoroughly assess TORCHTITAN on the Llama 3.1 family of LLMs, spanning 8 billion to 405 billion parameters, and showcase its exceptional performance, modular composability, and elastic scalability. By stacking training optimizations, we demonstrate accelerations ranging from 65.08% on Llama 3.1 8B at 128 GPU scale (1D), 12.59% on Llama 3.1 70B at 256 GPU scale (2D), to 30% on Llama 3.1 405B at 512 GPU scale (3D) on NVIDIA H100 GPUs over optimized baselines. We also demonstrate the effectiveness of 4D parallelism in enabling long context training. $^1$ GitHub: [https://github.com/pytorch/torchtitan](https://github.com/pytorch/torchtitan) | Wanchao Liang, Tianyu Liu, Less Wright, Will Constable, Andrew Gu, ChienChin Huang, Iris Zhang, Wei Feng, Howard Huang, Junjie Wang, Sanket Purandare, Gokul Nadathur, Stratos Idreos |  |
| 896 |  |  [Chunk-Distilled Language Modeling](https://openreview.net/forum?id=nrvoWOWcyg) |  | 0 | We introduce Chunk-Distilled Language Modeling (CD-LM), an approach to text generation that addresses two challenges in current large language models (LLMs): the inefficiency of token-level generation, and the difficulty of adapting to new data and knowledge. Our method combines deep network-based LLMs with a straightforward retrieval module, which allows the generation of multi-token text chunks at a single decoding step. Our retrieval framework enables flexible construction of model- or domain-specific datastores, either leveraging the internal knowledge of existing models, or incorporating expert insights from human-annotated corpora. This adaptability allows for enhanced control over the language model's distribution without necessitating additional training. We present the CD-LM formulation along with performance metrics demonstrating its ability to improve language model performance and efficiency across a diverse set of downstream applications. Code and data will be made publicly available. | Yanhong Li, Karen Livescu, Jiawei Zhou |  |
| 897 |  |  [When does compositional structure yield compositional generalization? A kernel theory](https://openreview.net/forum?id=FPBce2P1er) |  | 0 | Compositional generalization (the ability to respond correctly to novel combinations of familiar components) is thought to be a cornerstone of intelligent behavior. Compositionally structured (e.g. disentangled) representations support this ability; however, the conditions under which they are sufficient for the emergence of compositional generalization remain unclear. To address this gap, we present a theory of compositional generalization in kernel models with fixed, compositionally structured representations. This provides a tractable framework for characterizing the impact of training data statistics on generalization. We find that these models are limited to functions that assign values to each combination of components seen during training, and then sum up these values ("conjunction-wise additivity"). This imposes fundamental restrictions on the set of tasks compositionally structured kernel models can learn, in particular preventing them from transitively generalizing equivalence relations. Even for compositional tasks that they can learn in principle, we identify novel failure modes in compositional generalization (memorization leak and shortcut bias) that arise from biases in the training data. Finally, we empirically validate our theory, showing that it captures the behavior of deep neural networks (convolutional networks, residual networks, and Vision Transformers) trained on a set of compositional tasks with similarly structured data. Ultimately, this work examines how statistical structure in the training data can affect compositional generalization, with implications for how to identify and remedy failure modes in deep learning models. | Samuel Lippl, Kim Stachenfeld |  |
| 898 |  |  [BadJudge: Backdoor Vulnerabilities of LLM-As-A-Judge](https://openreview.net/forum?id=eC2a2IndIt) |  | 0 | This paper proposes a novel backdoor threat attacking the LLM-as-a-Judge evaluation regime, where the adversary controls both the candidate and evaluator model. The backdoored evaluator victimizes benign users by unfairly assigning inflated scores to adversary. A trivial single token backdoor poisoning 1% of the evaluator training data triples the adversary's score with respect to their legitimate score. We systematically categorize levels of data access corresponding to three real-world settings, (1) web poisoning, (2) malicious annotator, and (3) weight poisoning. These regimes reflect a weak to strong escalation of data access that highly correlates with attack severity. Under the weakest assumptions - web poisoning (1), the adversary still induces a 20% score inflation. Likewise, in the (3) weight poisoning regime, the stronger assumptions enable the adversary to inflate their scores from 1.5/5 to 4.9/5. The backdoor threat generalizes across different evaluator architectures, trigger designs, evaluation tasks, and poisoning rates. By poisoning 10% of the evaluator training data, we control toxicity judges (Guardrails) to misclassify toxic prompts as non-toxic 89% of the time, and document reranker judges in RAG to rank the poisoned document first 97% of the time. LLM-as-a-Judge is uniquely positioned at the intersection of ethics and technology, where social implications of mislead model selection and evaluation constrain the available defensive tools. Amidst these challenges, model merging emerges as a principled tool to offset the backdoor, reducing ASR to near 0% whilst maintaining SOTA performance. Model merging's low computational cost and convenient integration into the current LLM Judge training pipeline position it as a promising avenue for backdoor mitigation in the LLM-as-a-Judge setting. | Terry Tong, Fei Wang, Zhe Zhao, Muhao Chen |  |
| 899 |  |  [The Value of Sensory Information to a Robot](https://openreview.net/forum?id=ikr5XomWHS) |  | 0 | A decision-making agent, such as a robot, must observe and react to any new task-relevant information that becomes available from its environment. We seek to study a fundamental scientific question: what value does sensory information hold to an agent at various moments in time during the execution of a task? Towards this, we empirically study agents of varying architectures, generated with varying policy synthesis approaches (imitation, RL, model-based control), on diverse robotics tasks. For each robotic agent, we characterize its regret in terms of performance degradation when state observations are withheld from it at various task states for varying lengths of time. We find that sensory information is surprisingly rarely task-critical in many commonly studied task setups. Task characteristics such as stochastic dynamics largely dictate the value of sensory information for a well-trained robot; policy architectures such as planning vs. reactive control generate more nuanced second-order effects. Further, sensing efficiency is curiously correlated with task proficiency: in particular, fully trained high-performing agents are more robust to sensor loss than novice agents early in their training. Overall, our findings characterize the tradeoffs between sensory information and task performance in practical sequential decision making tasks, and pave the way towards the design of more resource-efficient decision-making agents. | Arjun Krishna, Edward S. Hu, Dinesh Jayaraman |  |
| 900 |  |  [Sensor-Invariant Tactile Representation](https://openreview.net/forum?id=RnJY9WcpA3) |  | 0 | High-resolution tactile sensors have become critical for embodied perception and robotic manipulation. However, a key challenge in the field is the lack of transferability between sensors due to design and manufacturing variations, which result in significant differences in tactile signals. This limitation hinders the ability to transfer models or knowledge learned from one sensor to another. To address this, we introduce a novel method for extracting Sensor-Invariant Tactile Representations (SITR), enabling zero-shot transfer across optical tactile sensors. Our approach utilizes a transformer-based architecture trained on a diverse dataset of simulated sensor designs, allowing it to generalize to new sensors in the real world with minimal calibration. Experimental results demonstrate the method’s effectiveness across various tactile sensing applications, facilitating data and model transferability for future advancements in the field. | Harsh Gupta, Yuchen Mo, Shengmiao Jin, Wenzhen Yuan |  |
| 901 |  |  [NutriBench: A Dataset for Evaluating Large Language Models in Nutrition Estimation from Meal Descriptions](https://openreview.net/forum?id=6LtdZCyuZR) |  | 0 | Accurate nutrition estimation helps people make informed dietary choices and is essential in the prevention of serious health complications. We present NutriBench, the first publicly available natural language meal description nutrition benchmark. NutriBench consists of 11,857 meal descriptions generated from real-world global dietary intake data. The data is human-verified and annotated with macro-nutrient labels, including carbohydrates, proteins, fats, and calories. We conduct an extensive evaluation of Nutribench on the task of carbohydrate estimation, testing twelve leading Large Language Models (LLMs), including GPT-4o, Llama3.1, Qwen2, Gemma2, and OpenBioLLM models, using standard, Chain-of-Thought and Retrieval-Augmented Generation strategies. Additionally, we present a study involving professional nutritionists, finding that LLMs can provide comparable but significantly faster estimates. Finally, we perform a real-world risk assessment by simulating the effect of carbohydrate predictions on the blood glucose levels of individuals with type 1 diabetes. Our work highlights the opportunities and challenges of using LLMs for nutrition estimation, demonstrating their potential to aid professionals and laypersons and improve health outcomes. Our benchmark is publicly available at: https://mehak126.github.io/nutribench.html | Mehak Preet Dhaliwal, Andong Hua, Laya Pullela, Ryan Burke, Yao Qin |  |
| 902 |  |  [GOttack: Universal Adversarial Attacks on Graph Neural Networks via Graph Orbits Learning](https://openreview.net/forum?id=YbURbViE7l) |  | 0 | Graph Neural Networks (GNNs) have demonstrated superior performance in node classification tasks across diverse applications. However, their vulnerability to adversarial attacks, where minor perturbations can mislead model predictions, poses significant challenges. This study introduces GOttack, a novel adversarial attack framework that exploits the topological structure of graphs to undermine the integrity of GNN predictions systematically. By defining a topology-aware method to manipulate graph orbits, our approach generates adversarial modifications that are both subtle and effective, posing a severe test to the robustness of GNNs. We evaluate the efficacy of GOttack across multiple prominent GNN architectures using standard benchmark datasets. Our results show that GOttack outperforms existing state-of-the-art adversarial techniques and completes training in approximately 55% of the time required by the fastest competing model, achieving the highest average misclassification rate in 155 tasks. This work not only sheds light on the susceptibility of GNNs to structured adversarial attacks but also shows that certain topological patterns may play a significant role in the underlying robustness of the GNNs. Our Python implementation is shared at https://github.com/cakcora/GOttack. | Zulfikar Alom, Tran Gia Bao Ngo, Murat Kantarcioglu, Cuneyt Gurcan Akcora |  |
| 903 |  |  [NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics](https://openreview.net/forum?id=hJVdwBpWjt) |  | 0 | Large language models (LLMs) prompted with text and audio have achieved state-of-the-art performance across various auditory tasks, including speech, music, and general audio, showing emergent abilities on unseen tasks. However, their potential has yet to be fully demonstrated in bioacoustics tasks, such as detecting animal vocalizations in large recordings, classifying rare and endangered species, and labeling context and behavior—tasks that are crucial for conservation, biodiversity monitoring, and animal behavior studies. In this work, we present NatureLM-audio, the first audio-language foundation model specifically designed for bioacoustics. Our training dataset consists of carefully curated text-audio pairs spanning bioacoustics, speech, and music, designed to address the field's limited availability of annotated data. We demonstrate successful transfer of learned representations from music and speech to bioacoustics, and our model shows promising generalization to unseen taxa and tasks. We evaluate NatureLM-audio on a novel benchmark (BEANS-Zero) and it sets a new state of the art on several bioacoustics tasks, including zero-shot classification of unseen species. To advance bioacoustics research, we release our model weights, benchmark data, and open-source the code for training and benchmark data generation and model training. | David Robinson, Marius Miron, Masato Hagiwara, Olivier Pietquin |  |
| 904 |  |  [Privacy-Preserving Personalized Federated Prompt Learning for Multimodal Large Language Models](https://openreview.net/forum?id=Equ277PBN0) |  | 0 | Multimodal Large Language Models (LLMs) are pivotal in revolutionizing customer support and operations by integrating multiple modalities such as text, images, and audio. Federated Prompt Learning (FPL) is a recently proposed approach that combines pre-trained multimodal LLMs such as vision-language models with federated learning to create personalized, privacy-preserving AI systems. However, balancing the competing goals of personalization, generalization, and privacy remains a significant challenge. Over-personalization can lead to overfitting, reducing generalizability, while stringent privacy measures, such as differential privacy, can hinder both personalization and generalization. In this paper, we propose a Differentially Private Federated Prompt Learning (DP-FPL) approach to tackle this challenge by leveraging a low-rank factorization scheme to capture generalization while maintaining a residual term that preserves expressiveness for personalization. To ensure privacy, we introduce a novel method where we apply local differential privacy to the two low-rank components of the local prompt, and global differential privacy to the global prompt. Our approach mitigates the impact of privacy noise on the model performance while balancing the tradeoff between personalization and generalization. Extensive experiments demonstrate the effectiveness of our approach over other benchmarks. | Linh Tran, Wei Sun, Stacy Patterson, Ana L. Milanova |  |
| 905 |  |  [Optimizing 4D Gaussians for Dynamic Scene Video from Single Landscape Images](https://openreview.net/forum?id=IcYDRzcccP) |  | 0 | To achieve realistic immersion in landscape images, fluids such as water and clouds need to move within the image while revealing new scenes from various camera perspectives. Recently, a field called dynamic scene video has emerged, which combines single image animation with 3D photography. These methods use pseudo 3D space, implicitly represented with Layered Depth Images (LDIs). LDIs separate a single image into depth-based layers, which enables elements like water and clouds to move within the image while revealing new scenes from different camera perspectives. However, as landscapes typically consist of continuous elements, including fluids, the representation of a 3D space separates a landscape image into discrete layers, and it can lead to diminished depth perception and potential distortions depending on camera movement. Furthermore, due to its implicit modeling of 3D space, the output may be limited to videos in the 2D domain, potentially reducing their versatility. In this paper, we propose representing a complete 3D space for dynamic scene video by modeling explicit representations, specifically 4D Gaussians, from a single image. The framework is focused on optimizing 3D Gaussians by generating multi-view images from a single image and creating 3D motion to optimize 4D Gaussians. The most important part of proposed framework is consistent 3D motion estimation, which estimates common motion among multi-view images to bring the motion in 3D space closer to actual motions. As far as we know, this is the first attempt that considers animation while representing a complete 3D space from a single landscape image. Our model demonstrates the ability to provide realistic immersion in various landscape images through diverse experiments and metrics. Extensive experimental results are https://cvsp-lab.github.io/ICLR2025_3D-MOM/. | InHwan Jin, Haesoo Choo, SeongHun Jeong, Park Heemoon, Junghwan Kim, Ohjoon Kwon, Kyeongbo Kong |  |
| 906 |  |  [Explore Theory of Mind: program-guided adversarial data generation for theory of mind reasoning](https://openreview.net/forum?id=246rHKUnnf) |  | 0 | Do large language models (LLMs) have theory of mind? A plethora of papers and benchmarks have been introduced to evaluate if current models have been able to develop this key ability of social intelligence. However, all rely on limited datasets with simple patterns that can potentially lead to problematic blind spots in evaluation and an overestimation of model capabilities. We introduce ExploreToM, the first framework to allow large-scale generation of diverse and challenging theory of mind data for robust training and evaluation. Our approach leverages an A\* search over a custom domain-specific language to produce complex story structures and novel, diverse, yet plausible scenarios to stress test the limits of LLMs. Our evaluation reveals that state-of-the-art LLMs, such as Llama-3.1-70B and GPT-4o, show accuracies as low as 0% and 9% on ExploreToM-generated data, highlighting the need for more robust theory of mind evaluation. As our generations are a conceptual superset of prior work, fine-tuning on our data yields a 27-point accuracy improvement on the classic ToMi benchmark (Le et al., 2019). ExploreToM also enables uncovering underlying skills and factors missing for models to show theory of mind, such as unreliable state tracking or data imbalances, which may contribute to models' poor performance on benchmarks. | Melanie Sclar, Jane DwivediYu, Maryam FazelZarandi, Yulia Tsvetkov, Yonatan Bisk, Yejin Choi, Asli Celikyilmaz |  |
| 907 |  |  [Controllable Context Sensitivity and the Knob Behind It](https://openreview.net/forum?id=Igm9bbkzHC) |  | 0 | When making predictions, a language model must trade off how much it relies on its context vs. its prior knowledge. Choosing how sensitive the model is to its context is a fundamental functionality, as it enables the model to excel at tasks like retrieval-augmented generation and question-answering. In this paper, we search for a knob which controls this sensitivity, determining whether language models answer from the context or their prior knowledge. To guide this search, we design a task for controllable context sensitivity. In this task, we first feed the model a context ("Paris is in England") and a question ("Where is Paris?"); we then instruct the model to either use its prior or contextual knowledge and evaluate whether it generates the correct answer for both intents (either "France" or "England"). When fine-tuned on this task, instruct versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it with high accuracy (85-95%). Analyzing these high-performing models, we narrow down which layers may be important to context sensitivity using a novel linear time algorithm. Then, in each model, we identify a 1-D subspace in a single layer that encodes whether the model follows context or prior knowledge. Interestingly, while we identify this subspace in a fine-tuned model, we find that the exact same subspace serves as an effective knob in not only that model but also non-fine-tuned instruct and base models of that model family. Finally, we show a strong correlation between a model's performance and how distinctly it separates context-agreeing from context-ignoring answers in this subspace. These results suggest a single fundamental subspace facilitates how the model chooses between context and prior knowledge. | Julian Minder, Kevin Du, Niklas Stoehr, Giovanni Monea, Chris Wendler, Robert West, Ryan Cotterell |  |
| 908 |  |  [The 3D-PC: a benchmark for visual perspective taking in humans and machines](https://openreview.net/forum?id=UIFAJZ22ZF) |  | 0 | Visual perspective taking (VPT) is the ability to perceive and reason about the perspectives of others. It is an essential feature of human intelligence, which develops over the first decade of life and requires an ability to process the 3D structure of visual scenes. A growing number of reports have indicated that deep neural networks (DNNs) become capable of analyzing 3D scenes after training on large image datasets. We investigated if this emergent ability for 3D analysis in DNNs is sufficient for VPT with the 3D perception challenge (3D-PC): a novel benchmark for 3D perception in humans and DNNs. The 3D-PC is comprised of three 3D-analysis tasks posed within natural scene images: (i.) a simple test of object depth order, (ii.) a basic VPT task (VPT-basic), and (iii.) a more challenging version of VPT (VPT-perturb) designed to limit the effectiveness of "shortcut" visual strategies. We tested human participants (N=33) and linearly probed or text-prompted over 300 DNNs on the challenge and found that nearly all of the DNNs approached or exceeded human accuracy in analyzing object depth order. Surprisingly, DNN accuracy on this task correlated with their object recognition performance. In contrast, there was an extraordinary gap between DNNs and humans on VPT-basic. Humans were nearly perfect, whereas most DNNs were near chance. Fine-tuning DNNs on VPT-basic brought them close to human performance, but they, unlike humans, dropped back to chance when tested on VPT-perturb. Our challenge demonstrates that the training routines and architectures of today's DNNs are well-suited for learning basic 3D properties of scenes and objects but are ill-suited for reasoning about these properties like humans do. We release our 3D-PC datasets and code to help bridge this gap in 3D perception between humans and machines. | Drew Linsley, Peisen Zhou, Alekh Karkada Ashok, Akash Nagaraj, Gaurav Gaonkar, Francis E. Lewis, Zygmunt Pizlo, Thomas Serre |  |
| 909 |  |  [HelpSteer2-Preference: Complementing Ratings with Preferences](https://openreview.net/forum?id=MnfHxPP5gs) |  | 0 | Reward models are critical for aligning models to follow instructions, and are typically trained following one of two popular paradigms: Bradley-Terry style or Regression style. However, there is a lack of evidence that either approach is better than the other, when adequately matched for data. This is primarily because these approaches require data collected in different (but incompatible) formats, meaning that adequately matched data is not available in existing public datasets. To tackle this problem, we release preference annotations (designed for Bradley-Terry training) to complement existing ratings (designed for Regression style training) in the HelpSteer2 dataset. To improve data interpretability, preference annotations are accompanied with human-written justifications. Using this data, we conduct the first head-to-head comparison of Bradley-Terry and Regression models when adequately matched for data. Based on insights derived from such a comparison, we propose a novel approach to combine Bradley-Terry and Regression reward modeling. A Llama-3.1-70B-Instruct model tuned with this approach scores 94.1 on RewardBench, emerging top of more than 140 reward models as of 1 Oct 2024. This reward model can then be used with REINFORCE algorithm (RLHF) to align an Instruct model to reach 85.0 on Arena Hard, which is No. 1 as of 1 Oct 2024. We open-source this dataset (CC-BY-4.0 license) at https://huggingface.co/datasets/nvidia/HelpSteer2#preferences-new---1-oct-2024 and openly release the trained Reward and Instruct models at https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward and https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct . | Zhilin Wang, Alexander Bukharin, Olivier Delalleau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Oleksii Kuchaiev, Yi Dong |  |
| 910 |  |  [Shared-AE: Automatic Identification of Shared Subspaces in High-dimensional Neural and Behavioral Activity](https://openreview.net/forum?id=zXCnIyX9MG) |  | 0 | Understanding the relationship between behavior and neural activity is crucial for understanding brain function. An effective method is to learn embeddings for interconnected modalities. For simple behavioral tasks, neural features can be learned based on labels. However, complex behaviors, such as social interactions, require the joint extraction of behavioral and neural characteristics. In this paper, we present an autoencoder (AE) framework, called Shared-AE, which includes a novel regularization term that automatically identifies features shared between neural activity and behavior, while simultaneously capturing the unique private features specific to each modality. We apply Shared-AE to large-scale neural activity recorded across the entire dorsal cortex of the mouse, during two very different behaviors: (i) head-fixed mice performing a self-initiated decision-making task, and (ii) freely-moving social behavior amongst two mice. Our model successfully captures both \`shared features', shared across neural and behavioral activity, and \`private features', unique to each modality, significantly enhancing our understanding of the alignment between neural activity and complex behaviors. The original code for the entire Shared-AE framework on Pytorch has been made publicly available at: \url{https://github.com/saxenalab-neuro/Shared-AE}. | Daiyao Yi, Hao Dong, Michael James Higley, Anne Churchland, Shreya Saxena |  |
| 911 |  |  [Bounds on Lp Errors in Density Ratio Estimation via f-Divergence Loss Functions](https://openreview.net/forum?id=ttq44QjKda) |  | 0 | Density ratio estimation (DRE) is a core technique in machine learning used to capture relationships between two probability distributions. $f$-divergence loss functions, which are derived from variational representations of $f$-divergence, have become a standard choice in DRE for achieving cutting-edge performance. This study provides novel theoretical insights into DRE by deriving upper and lower bounds on the $L_p$ errors through $f$-divergence loss functions. These bounds apply to any estimator belonging to a class of Lipschitz continuous estimators, irrespective of the specific $f$-divergence loss function employed. The derived bounds are expressed as a product involving the data dimensionality and the expected value of the density ratio raised to the $p$-th power. Notably, the lower bound includes an exponential term that depends on the Kullback--Leibler (KL) divergence, revealing that the $L_p$ error increases significantly as the KL divergence grows when $p > 1$. This increase becomes even more pronounced as the value of $p$ grows. The theoretical insights are validated through numerical experiments. | Yoshiaki Kitazawa |  |
| 912 |  |  [Palu: KV-Cache Compression with Low-Rank Projection](https://openreview.net/forum?id=LWMS4pk2vK) |  | 0 | Post-training KV-Cache compression methods typically either sample a subset of effectual tokens or quantize the data into lower numerical bit width. However, these methods cannot exploit redundancy in the hidden dimension of the KV tenors. This paper presents a hidden dimension compression approach called Palu, a KV-Cache compression framework that utilizes low-rank projection to reduce inference-time LLM memory usage. Palu decomposes the linear layers into low-rank matrices, caches compressed intermediate states, and reconstructs the full keys and values on the fly. To improve accuracy, compression rate, and efficiency, Palu further encompasses (1) a medium-grained low-rank decomposition scheme, (2) an efficient rank search algorithm, (3) low-rank-aware quantization compatibility enhancements, and (4) an optimized GPU kernel with matrix fusion. Extensive experiments with popular LLMs show that Palu compresses KV-Cache by 50% while maintaining strong accuracy and delivering up to 1.89× speedup on the RoPE-based attention module. When combined with quantization, Palu’s inherent quantization-friendly design yields small to negligible extra accuracy degradation while saving additional memory than quantization-only methods and achieving up to 2.91× speedup for the RoPE-based attention. Moreover, it maintains comparable or even better accuracy (up to 1.19 lower perplexity) compared to quantization-only methods. These results demonstrate Palu’s superior capability to effectively address the efficiency and memory challenges of LLM inference posed by KV-Cache. Our code is publicly available at: https://github.com/shadowpa0327/Palu. | ChiChih Chang, WeiCheng Lin, ChienYu Lin, ChongYan Chen, YuFang Hu, PeiShuo Wang, NingChi Huang, Luis Ceze, Mohamed S. Abdelfattah, KaiChiang Wu |  |
| 913 |  |  [Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling](https://openreview.net/forum?id=bIlnpVM4bc) |  | 0 | Efficiently modeling sequences with infinite context length has long been a challenging problem. Previous approaches have either suffered from quadratic computational complexity or limited extrapolation ability in length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall recent memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and demonstrate that it significantly outperforms state-of-the-art models across a variety of benchmarks. Pretrained on sequences of 4K length, Samba shows improved perplexity in context lengths of up to 1M in zero-shot. When finetuned on 4K-length sequences, Samba efficiently extrapolates to a 256K context length with perfect memory recall on the Passkey Retrieval task, and exhibits superior retrieval extrapolation on the challenging Phonebook task compared to full-attention models. As a linear-time sequence model, Samba achieves a 3.73× higher throughput compared to Transformers with grouped-query attention for user prompts of 128K length, and a 3.64× speedup when generating 64K tokens with unlimited streaming. | Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, Weizhu Chen |  |
| 914 |  |  [Nova: Generative Language Models for Assembly Code with Hierarchical Attention and Contrastive Learning](https://openreview.net/forum?id=4ytRL3HJrq) |  | 0 | Binary code analysis is the foundation of crucial tasks in the security domain; thus building effective binary analysis techniques is more important than ever. Large language models (LLMs) although have brought impressive improvement to source code tasks, do not directly generalize to assembly code due to the unique challenges of assembly: (1) the low information density of assembly and (2) the diverse optimizations in assembly code. To overcome these challenges, this work proposes a hierarchical attention mechanism that builds attention summaries to capture the semantics more effectively and designs contrastive learning objectives to train LLMs to learn assembly optimization. Equipped with these techniques, this work develops Nova, a generative LLM for assembly code. Nova outperforms existing techniques on binary code decompilation by up to 14.84 -- 21.58% higher Pass@1 and Pass@10, and outperforms the latest binary code similarity detection techniques by up to 6.17% Recall@1, showing promising abilities on both assembly generation and understanding tasks. | Nan Jiang, Chengxiao Wang, Kevin Liu, Xiangzhe Xu, Lin Tan, Xiangyu Zhang, Petr Babkin |  |
| 915 |  |  [Efficient Model-Based Reinforcement Learning Through Optimistic Thompson Sampling](https://openreview.net/forum?id=Ian00SaFHg) |  | 0 | Learning complex robot behavior through interactions with the environment necessitates principled exploration. Effective strategies should prioritize exploring regions of the state-action space that maximize rewards, with optimistic exploration emerging as a promising direction aligned with this idea and enabling sample-efficient reinforcement learning. However, existing methods overlook a crucial aspect: the need for optimism to be informed by a belief connecting the reward and state. To address this, we propose a practical, theoretically grounded approach to optimistic exploration based on Thompson sampling. Our approach is the first that allows for reasoning about _joint_ uncertainty over transitions and rewards for optimistic exploration. We apply our method on a set of MuJoCo and VMAS continuous control tasks. Our experiments demonstrate that optimistic exploration significantly accelerates learning in environments with sparse rewards, action penalties, and difficult-to-explore regions. Furthermore, we provide insights into when optimism is beneficial and emphasize the critical role of model uncertainty in guiding exploration. | Jasmine Bayrooti, Carl Henrik Ek, Amanda Prorok |  |
| 916 |  |  [Latent Safety-Constrained Policy Approach for Safe Offline Reinforcement Learning](https://openreview.net/forum?id=bDt5qc7TfO) |  | 0 | In safe offline reinforcement learning, the objective is to develop a policy that maximizes cumulative rewards while strictly adhering to safety constraints, utilizing only offline data. Traditional methods often face difficulties in balancing these constraints, leading to either diminished performance or increased safety risks. We address these issues with a novel approach that begins by learning a conservatively safe policy through the use of Conditional Variational Autoencoders, which model the latent safety constraints. Subsequently, we frame this as a Constrained Reward-Return Maximization problem, wherein the policy aims to optimize rewards while complying with the inferred latent safety constraints. This is achieved by training an encoder with a reward-Advantage Weighted Regression objective within the latent constraint space. Our methodology is supported by theoretical analysis, including bounds on policy performance and sample complexity. Extensive empirical evaluation on benchmark datasets, including challenging autonomous driving scenarios, demonstrates that our approach not only maintains safety compliance but also excels in cumulative reward optimization, surpassing existing methods. Additional visualizations provide further insights into the effectiveness and underlying mechanisms of our approach. | Prajwal Koirala, Zhanhong Jiang, Soumik Sarkar, Cody H. Fleming |  |
| 917 |  |  [Large (Vision) Language Models are Unsupervised In-Context Learners](https://openreview.net/forum?id=ohJxgRLlLt) |  | 0 | Recent advances in large language and vision-language models have enabled zero-shot inference, allowing models to solve new tasks without task-specific training. Various adaptation techniques such as prompt engineering, In-Context Learning (ICL), and supervised fine-tuning can further enhance the model’s performance on a downstream task, but they require substantial manual effort to construct effective prompts or labeled examples. In this work, we introduce a joint inference framework for fully unsupervised adaptation, eliminating the need for manual prompt engineering and labeled examples. Unlike zero-shot inference, which makes independent predictions, the joint inference makes predictions simultaneously for all inputs in a given task. Since direct joint inference involves computationally expensive optimization, we develop efficient approximation techniques, leading to two unsupervised adaptation methods: unsupervised fine-tuning and unsupervised ICL. We demonstrate the effectiveness of our methods across diverse tasks and models, including language-only Llama-3.1 on natural language processing tasks, reasoning-oriented Qwen2.5-Math on grade school math problems, vision-language OpenFlamingo on vision tasks, and the API-only access GPT-4o model on massive multi-discipline tasks. Our experiments demonstrate substantial improvements over the standard zero-shot approach, including 39% absolute improvement on the challenging GSM8K math reasoning dataset. Remarkably, despite being fully unsupervised, our framework often performs on par with supervised approaches that rely on ground truth labels. | Artyom Gadetsky, Andrei Atanov, Yulun Jiang, Zhitong Gao, Ghazal Hosseini Mighan, Amir Zamir, Maria Brbic |  |
| 918 |  |  [LLMs Can Plan Only If We Tell Them](https://openreview.net/forum?id=K3KrOsR6y9) |  | 0 | Large language models (LLMs) have demonstrated significant capabilities in natural language processing and reasoning, yet their effectiveness in autonomous planning has been under debate. While existing studies have utilized LLMs with external feedback mechanisms or in controlled environments for planning, these approaches often involve substantial computational and development resources due to the requirement for careful design and iterative backprompting. Moreover, even the most advanced LLMs like GPT-4 struggle to match human performance on standard planning benchmarks, such as the Blocksworld, without additional support. This paper investigates whether LLMs can independently generate long-horizon plans that rival human baselines. Our novel enhancements to Algorithm-of-Thoughts (AoT), which we dub AoT+, help achieve state-of-the-art results in planning benchmarks out-competing prior methods and human baselines all autonomously. | Bilgehan Sel, Ruoxi Jia, Ming Jin |  |
| 919 |  |  [Feature Responsiveness Scores: Model-Agnostic Explanations for Recourse](https://openreview.net/forum?id=wsWCVrH9dv) |  | 0 | Machine learning models routinely automate decisions in applications like lending and hiring. In such settings, consumer protection rules require companies that deploy models to explain predictions to decision subjects. These rules are motivated, in part, by the belief that explanations can promote \*recourse\* by revealing information that individuals can use to contest or improve their outcomes. In practice, many companies comply with these rules by providing individuals with a list of the most important features for their prediction, which they identify based on feature importance scores from feature attribution methods such as SHAP or LIME. In this work, we show how these practices can undermine consumers by highlighting features that would not lead to an improved outcome and by explaining predictions that cannot be changed. We propose to address these issues by highlighting features based on their \*responsiveness score\*—i.e., the probability that an individual can attain a target prediction by changing a specific feature. We develop efficient methods to compute responsiveness scores for any model and any dataset. We conduct an extensive empirical study on the responsiveness of explanations in lending. Our results show that standard practices in consumer finance can backfire by presenting consumers with \*reasons without recourse\*, and demonstrate how our approach improves consumer protection by highlighting responsive features and identifying fixed predictions. | Seung Hyun Cheon, Anneke Wernerfelt, Sorelle A. Friedler, Berk Ustun |  |
| 920 |  |  [InterMask: 3D Human Interaction Generation via Collaborative Masked Modeling](https://openreview.net/forum?id=ZAyuwJYN8N) |  | 0 | Generating realistic 3D human-human interactions from textual descriptions remains a challenging task. Existing approaches, typically based on diffusion models, often produce results lacking realism and fidelity. In this work, we introduce \*InterMask\*, a novel framework for generating human interactions using collaborative masked modeling in discrete space. InterMask first employs a VQ-VAE to transform each motion sequence into a 2D discrete motion token map. Unlike traditional 1D VQ token maps, it better preserves fine-grained spatio-temporal details and promotes \*spatial awareness\* within each token. Building on this representation, InterMask utilizes a generative masked modeling framework to collaboratively model the tokens of two interacting individuals. This is achieved by employing a transformer architecture specifically designed to capture complex spatio-temporal inter-dependencies. During training, it randomly masks the motion tokens of both individuals and learns to predict them. For inference, starting from fully masked sequences, it progressively fills in the tokens for both individuals. With its enhanced motion representation, dedicated architecture, and effective learning strategy, InterMask achieves state-of-the-art results, producing high-fidelity and diverse human interactions. It outperforms previous methods, achieving an FID of $5.154$ (vs $5.535$ of in2IN) on the InterHuman dataset and $0.399$ (vs $5.207$ of InterGen) on the InterX dataset. Additionally, InterMask seamlessly supports reaction generation without the need for model redesign or fine-tuning. | Muhammad Gohar Javed, Chuan Guo, Li Cheng, Xingyu Li |  |
| 921 |  |  [Denoising Autoregressive Transformers for Scalable Text-to-Image Generation](https://openreview.net/forum?id=amDkNPVWcn) |  | 0 | Diffusion models have become the dominant approach for visual generation. They are trained by denoising a Markovian process which gradually adds noise to the input. We argue that the Markovian property limits the model’s ability to fully utilize the generation trajectory, leading to inefficiencies during training and inference. In this paper, we propose DART, a transformer-based model that unifies autoregressive (AR) and diffusion within a non-Markovian framework. DART iteratively denoises image patches spatially and spectrally using an AR model that has the same architecture as standard language models. DART does not rely on image quantization, which enables more effective image modeling while maintaining flexibility. Furthermore, DART seamlessly trains with both text and image data in a unified model. Our approach demonstrates competitive performance on class-conditioned and text-to-image generation tasks, offering a scalable, efficient alternative to traditional diffusion models. Through this unified framework, DART sets a new benchmark for scalable, high-quality image synthesis. | Jiatao Gu, Yuyang Wang, Yizhe Zhang, Qihang Zhang, Dinghuai Zhang, Navdeep Jaitly, Joshua M. Susskind, Shuangfei Zhai |  |
| 922 |  |  [Many-Objective Multi-Solution Transport](https://openreview.net/forum?id=Neb17mimVH) |  | 0 | Optimizing the performance of many objectives (instantiated by tasks or clients) jointly with a few Pareto stationary solutions (models) is critical in machine learning. However, previous multi-objective optimization methods often focus on a few objectives and cannot scale to many objectives that outnumber the solutions, leading to either subpar performance or ignored objectives. We introduce ''Many-objective multi-solution Transport (MosT)'', a framework that finds multiple diverse solutions in the Pareto front of many objectives. Our insight is to seek multiple solutions, each performing as a domain expert and focusing on a specific subset of objectives while collectively covering all of them. MosT formulates the problem as a bi-level optimization of weighted objectives for each solution, where the weights are defined by an optimal transport between objectives and solutions. Our algorithm ensures convergence to Pareto stationary solutions for complementary subsets of objectives. On a range of applications in federated learning, multi-task learning, and mixture-of-prompt learning for LLMs, MosT distinctly outperforms strong baselines, delivering high-quality, diverse solutions that profile the entire Pareto frontier, thus ensuring balanced trade-offs across many objectives. | Ziyue Li, Tian Li, Virginia Smith, Jeff Bilmes, Tianyi Zhou |  |
| 923 |  |  [An Auditing Test to Detect Behavioral Shift in Language Models](https://openreview.net/forum?id=h0jdAboh0o) |  | 0 | As language models (LMs) approach human-level performance, a comprehensive understanding of their behavior becomes crucial. This includes evaluating capabilities, biases, task performance, and alignment with societal values. Extensive initial evaluations, including red teaming and diverse benchmarking, can establish a model’s behavioral profile. However, subsequent fine-tuning or deployment modifications may alter these behaviors in unintended ways. We present an efficient statistical test to tackle Behavioral Shift Auditing (BSA) in LMs, which we define as detecting distribution shifts in qualitative properties of the output distributions of LMs. Our test compares model generations from a baseline model to those of the model under scrutiny and provides theoretical guarantees for change detection while controlling false positives. The test features a configurable tolerance parameter that adjusts sensitivity to behavioral changes for different use cases. We evaluate our approach using two case studies: monitoring changes in (a) toxicity and (b) translation performance. We find that the test is able to detect meaningful changes in behavior distributions using just hundreds of examples. | Leo Richter, Xuanli He, Pasquale Minervini, Matt J. Kusner |  |
| 924 |  |  [Radar: Fast Long-Context Decoding for Any Transformer](https://openreview.net/forum?id=ZTpWOwMrzQ) |  | 0 | Transformer models have demonstrated exceptional performance across a wide range of applications. Though forming the foundation of Transformer models, the dot-product attention does not scale well to long-context data since its time requirement grows quadratically with context length. In this work, we propose Radar, a training-free approach that accelerates inference by dynamically searching for the most important context tokens. For any pre-trained Transformer, Radar can reduce the decoding time complexity without training or heuristically evicting tokens. Moreover, we provide theoretical justification for our approach, demonstrating that Radar can reliably identify the most important tokens with high probability. We conduct extensive comparisons with the previous methods on a wide range of tasks. The results demonstrate that Radar achieves the state-of-the-art performance across different architectures with reduced time complexity, offering a practical solution for efficient long-context processing of Transformers. The code is publicly available at https://github.com/BorealisAI/radar-decoding. | Yongchang Hao, Mengyao Zhai, Hossein Hajimirsadeghi, Sepidehsadat Hosseini, Frederick Tung |  |
| 925 |  |  [Can Knowledge Editing Really Correct Hallucinations?](https://openreview.net/forum?id=hmDt068MoZ) |  | 0 | Large Language Models (LLMs) suffer from hallucinations, referring to the non-factual information in generated content, despite their superior capacities across tasks. Meanwhile, knowledge editing has been developed as a new popular paradigm to correct erroneous factual knowledge encoded in LLMs with the advantage of avoiding retraining from scratch. However, a common issue of existing evaluation datasets for knowledge editing is that they do not ensure that LLMs actually generate hallucinated answers to the evaluation questions before editing. When LLMs are evaluated on such datasets after being edited by different techniques, it is hard to directly adopt the performance to assess the effectiveness of different knowledge editing methods in correcting hallucinations. Thus, the fundamental question remains insufficiently validated: Can knowledge editing really correct hallucinations in LLMs? We proposed HalluEditBench to holistically benchmark knowledge editing methods in correcting real-world hallucinations. First, we rigorously construct a massive hallucination dataset with 9 domains, 26 topics and more than 6,000 hallucinations. Then, we assess the performance of knowledge editing methods in a holistic way on five dimensions including Efficacy, Generalization, Portability, Locality, and Robustness. Through HalluEditBench, we have provided new insights into the potentials and limitations of different knowledge editing methods in correcting hallucinations, which could inspire future improvements and facilitate progress in the field of knowledge editing. | Baixiang Huang, Canyu Chen, Xiongxiao Xu, Ali Payani, Kai Shu |  |
| 926 |  |  [SysCaps: Language Interfaces for Simulation Surrogates of Complex Systems](https://openreview.net/forum?id=1R5BcYS8EC) |  | 0 | Surrogate models are used to predict the behavior of complex energy systems that are too expensive to simulate with traditional numerical methods. Our work introduces the use of language descriptions, which we call "system captions" or SysCaps, to interface with such surrogates. We argue that interacting with surrogates through text, particularly natural language, makes these models more accessible for both experts and non-experts. We introduce a lightweight multimodal text and timeseries regression model and a training pipeline that uses large language models (LLMs) to synthesize high-quality captions from simulation metadata. Our experiments on two real-world simulators of buildings and wind farms show that our SysCaps-augmented surrogates have better accuracy on held-out systems than traditional methods while enjoying new generalization abilities, such as handling semantically related descriptions of the same test system. Additional experiments also highlight the potential of SysCaps to unlock language-driven design space exploration and to regularize training through prompt augmentation. | Patrick Emami, Zhaonan Li, Saumya Sinha, Truc Nguyen |  |
| 927 |  |  [Generative Classifiers Avoid Shortcut Solutions](https://openreview.net/forum?id=oCUYc7BzXQ) |  | 0 | Discriminative approaches to classification often learn shortcuts that hold in-distribution but fail even under minor distribution shift. This failure mode stems from an overreliance on features that are spuriously correlated with the label. We show that generative classifiers, which use class-conditional generative models, can avoid this issue by modeling all features, both core and spurious, instead of mainly spurious ones. These generative classifiers are simple to train, avoiding the need for specialized augmentations, strong regularization, extra hyperparameters, or knowledge of the specific spurious correlations to avoid. We find that diffusion-based and autoregressive generative classifiers achieve state-of-the-art performance on five standard image and text distribution shift benchmarks and reduce the impact of spurious correlations in realistic applications, such as medical or satellite datasets. Finally, we carefully analyze a Gaussian toy setting to understand the inductive biases of generative classifiers, as well as the data properties that determine when generative classifiers outperform discriminative ones. | Alexander Cong Li, Ananya Kumar, Deepak Pathak |  |
| 928 |  |  [Compute-Constrained Data Selection](https://openreview.net/forum?id=4es2oO9tw1) |  | 0 | Data selection can reduce the amount of training data needed to finetune LLMs; however, the efficacy of data selection scales directly with its compute. Motivated by the practical challenge of compute-constrained finetuning, we consider the setting in which both the cost of selecting data and training are budgeted for. We first formalize the problem of data selection with a cost-aware utility function, and model the data selection problem as trading off initial-selection cost for training gain. We run a comprehensive sweep of experiments across multiple tasks, varying compute budget by scaling finetuning tokens, model sizes, and data selection compute. Interestingly we find that many powerful data selection methods are almost never compute-optimal, and that cheaper data selection alternatives dominate both from a theoretical and empirical perspective. For compute-optimal training, we find that perplexity and gradient data selection require training-to-selection model size ratios of 5x and 10x, respectively. | Junjie Oscar Yin, Alexander M. Rush |  |
| 929 |  |  [Generalization through variance: how noise shapes inductive biases in diffusion models](https://openreview.net/forum?id=7lUdo8Vuqa) |  | 0 | How diffusion models generalize beyond their training set is not known, and is somewhat mysterious given two facts: the optimum of the denoising score matching (DSM) objective usually used to train diffusion models is the score function of the training distribution; and the networks usually used to learn the score function are expressive enough to learn this score to high accuracy. We claim that a certain feature of the DSM objective—the fact that its target is not the training distribution's score, but a noisy quantity only equal to it in expectation—strongly impacts whether and to what extent diffusion models generalize. In this paper, we develop a mathematical theory that partly explains this 'generalization through variance' phenomenon. Our theoretical analysis exploits a physics-inspired path integral approach to compute the distributions typically learned by a few paradigmatic under- and overparameterized diffusion models. We find that the distributions diffusion models effectively learn to sample from resemble their training distributions, but with \`gaps' filled in, and that this inductive bias is due to the covariance structure of the noisy target used during training. We also characterize how this inductive bias interacts with feature-related inductive biases. | John J. Vastola |  |
| 930 |  |  [Learning to Explore and Exploit with GNNs for Unsupervised Combinatorial Optimization](https://openreview.net/forum?id=vaJ4FObpXN) |  | 0 | Combinatorial optimization (CO) problems are pervasive across various domains, but their NP-hard nature often necessitates problem-specific heuristic algorithms. Recent advancements in deep learning have led to the development of learning-based heuristics, yet these approaches often struggle with limited search capabilities. We introduce Explore-and-Exploit GNN ($X^2$GNN, pronounced x-squared GNN), a novel unsupervised neural framework that combines exploration and exploitation for combinatorial search optimization: i) Exploration - $X^2$GNN generates multiple solutions simultaneously, promoting diversity in the search space; (ii) Exploitation - $X^2$GNN employs neural stochastic iterative refinement to exploit partial existing solutions, guiding the search toward promising regions and helping escape local optima. By balancing exploration and exploitation, $X^2$GNN achieves superior performance and generalization on several graph CO problems including Max Cut, Max Independent Set, and Max Clique. Notably, for large Max Clique problems, $X^2$GNN consistently generates solutions within 1.2\% of optimality, while other state-of-the-art learning-based approaches struggle to reach within 22\% of optimal. Moreover, $X^2$GNN consistently generates better solutions than Gurobi on large graphs for all three problems under reasonable time budgets. Furthermore, $X^2$GNN exhibits exceptional generalization capabilities. For the Maximum Independent Set problem, $X^2$GNN outperforms state-of-the-art methods even when trained on smaller or out-of-distribution graphs compared to the test set. Our framework offers a more effective and flexible approach to neural combinatorial optimization, addressing a key challenge in the field and providing a promising direction for future research in learning-based heuristics for combinatorial optimization. | Utku Umur Acikalin, Aaron M. Ferber, Carla P. Gomes |  |
| 931 |  |  [Fitting Networks with a Cancellation Trick](https://openreview.net/forum?id=C06kww3Qky) |  | 0 | The degree-corrected block model (DCBM), latent space model (LSM), and $\beta$-model are all popular network models. We combine their modeling ideas and propose the logit-DCBM as a new model. Similar as the $\beta$-model and LSM, the logit-DCBM contains nonlinear factors, where fitting the parameters is a challenging open problem. We resolve this problem by introducing a cancellation trick. We also propose R-SCORE as a recursive community detection algorithm, where in each iteration, we first use the idea above to update our parameter estimation, and then use the results to remove the nonlinear factors in the logit-DCBM so the renormalized model approximately satisfies a low-rank model, just like the DCBM. Our numerical study suggests that R-SCORE significantly improves over existing spectral approaches in many cases. Also, theoretically, we show that the Hamming error rate of R-SCORE is faster than that of SCORE in a specific sparse region, and is at least as fast outside this region. | Jiashun Jin, Jingming Wang |  |
| 932 |  |  [Do LLM Agents Have Regret? A Case Study in Online Learning and Games](https://openreview.net/forum?id=qn9tBYQHGi) |  | 0 | Large language models (LLMs) have been increasingly employed for (interactive) decision-making, via the development of LLM-based autonomous agents. Despite their emerging successes, the performance of LLM agents in decision-making has not been fully investigated through quantitative metrics, especially in the multi-agent setting when they interact with each other, a typical scenario in real-world LLM-agent applications. To better understand the limits of LLM agents in these interactive environments, we propose to study their interactions in benchmark decision-making settings in online learning and game theory, through the performance metric of regret. We first empirically study the no-regret behaviors of LLMs in canonical non-stochastic online learning problems, as well as the emergence of equilibria when LLM agents interact through playing repeated games. We then provide some theoretical insights into the no-regret behaviors of LLM agents, under certain assumptions on the supervised pre-training and the rationality model of human decision-makers who generate the data. Notably, we also identify (simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To further promote the no-regret behaviors, we propose a novel unsupervised training loss of regret-loss, which, in contrast to the supervised pre-training loss, does not require the labels of (optimal) actions. Finally, we establish the statistical guarantee of generalization bound for regret-loss minimization, and more importantly, the optimization guarantee that minimizing such a loss may automatically lead to known no-regret learning algorithms, when single-layer self-attention models are used. Our further experiments demonstrate the effectiveness of our regret-loss, especially in addressing the above “regrettable” cases. | Chanwoo Park, Xiangyu Liu, Asuman E. Ozdaglar, Kaiqing Zhang |  |
| 933 |  |  [Seq-VCR: Preventing Collapse in Intermediate Transformer Representations for Enhanced Reasoning](https://openreview.net/forum?id=30oIfmrcFO) |  | 0 | Decoder-only Transformers often struggle with complex reasoning tasks, particularly arithmetic reasoning requiring multiple sequential operations. In this work, we identify representation collapse in the model’s intermediate layers as a key factor limiting their reasoning capabilities. To address this, we propose Sequential Variance-Covariance Regularization (Seq-VCR), which enhances the entropy of intermediate representations and prevents collapse. Combined with dummy pause tokens as substitutes for chain-of-thought (CoT) tokens, our method significantly improves performance in arithmetic reasoning problems. In the challenging 5 × 5 integer multiplication task, our approach achieves 99.5% exact match accuracy, outperforming models of the same size (which yield 0% accuracy) and GPT-4 with five-shot CoT prompting (44%). We also demonstrate superior results on arithmetic expression and longest increasing subsequence (LIS) datasets. Our findings highlight the importance of preventing intermediate layer representation collapse to enhance the reasoning capabilities of Transformers and show that Seq-VCR offers an effective solution without requiring explicit CoT supervision. | Md Rifat Arefin, Gopeshh Subbaraj, Nicolas Gontier, Yann LeCun, Irina Rish, Ravid ShwartzZiv, Christopher Pal |  |
| 934 |  |  [Underdamped Diffusion Bridges with Applications to Sampling](https://openreview.net/forum?id=Q1QTxFm0Is) |  | 0 | We provide a general framework for learning diffusion bridges that transport prior to target distributions. It includes existing diffusion models for generative modeling, but also underdamped versions with degenerate diffusion matrices, where the noise only acts in certain dimensions. Extending previous findings, our framework allows to rigorously show that score-matching in the underdamped case is indeed equivalent to maximizing a lower bound on the likelihood. Motivated by superior convergence properties and compatibility with sophisticated numerical integration schemes of underdamped stochastic processes, we propose \*underdamped diffusion bridges\*, where a general density evolution is learned rather than prescribed by a fixed noising process. We apply our method to the challenging task of sampling from unnormalized densities without access to samples from the target distribution. Across a diverse range of sampling problems, our approach demonstrates state-of-the-art performance, notably outperforming alternative methods, while requiring significantly fewer discretization steps and almost no hyperparameter tuning. | Denis Blessing, Julius Berner, Lorenz Richter, Gerhard Neumann |  |
| 935 |  |  [BoneMet: An Open Large-Scale Multi-Modal Murine Dataset for Breast Cancer Bone Metastasis Diagnosis and Prognosis](https://openreview.net/forum?id=YH4M1Tbxfz) |  | 0 | Breast cancer bone metastasis (BCBM) affects women’s health globally, calling for the development of effective diagnosis and prognosis solutions. While deep learning has exhibited impressive capacities across various healthcare domains, its applicability in BCBM diseases is consistently hindered by the lack of an open, large-scale, deep learning-ready dataset. As such, we introduce the Bone Metastasis (BoneMet) dataset, the first large-scale, publicly available, high-resolution medical resource, which is derived from a well-accepted murine BCBM model. The unique advantage of BoneMet over existing human datasets is repeated sequential scans per subject over the entire disease development phases. The dataset consists of over 67 terabytes of multi-modal medical data, including 2D X-ray images, 3D CT scans, and detailed biological data (e.g., medical records and bone quantitative analysis), collected from more than five hundreds mice spanning from 2019 to 2024. Our BoneMet dataset is well-organized into six components, i.e., Rotation X-Ray, Recon-CT, Seg-CT, Regist-CT, RoI-CT, and MiceMediRec. We further show that BoneMet can be readily adopted to build versatile, large-scale AI models for managing BCBM diseases in terms of diagnosis using 2D or 3D images, prognosis of bone deterioration, and sparse-angle 3D reconstruction for safe long-term disease monitoring. Our preliminary results demonstrate that BoneMet has the potentials to jump-start the development and fine-tuning of AI-driven solutions prior to their applications to human patients. To facilitate its easy access and wide dissemination, we have created the BoneMet package, providing three APIs that enable researchers to (i) flexibly process and download the BoneMet data filtered by specific time frames; and (ii) develop and train large-scale AI models for precise BCBM diagnosis and prognosis. The BoneMet dataset is officially available on Hugging Face Datasets at https://huggingface.co/datasets/BoneMet/BoneMet. The BoneMet package is available on the Python Package Index (PyPI) at https://pypi.org/project/BoneMet. Code and tutorials are available at https://github.com/Tiankuo528/BoneMet. | Tiankuo Chu, Fudong Lin, Shubo Wang, Jason Jiang, Wiley JiaWei Gong, Xu Yuan, Liyun Wang |  |
| 936 |  |  [Time After Time: Deep-Q Effect Estimation for Interventions on When and What to do](https://openreview.net/forum?id=5yDS32hKJc) |  | 0 | Problems in fields such as healthcare, robotics, and finance requires reasoning about the value both of what decision or action to take and when to take it. The prevailing hope is that artificial intelligence will support such decisions by estimating the causal effect of policies such as how to treat patients or how to allocate resources over time. However, existing methods for estimating the effect of a policy struggle with \emph{irregular time}. They either discretize time, or disregard the effect of timing policies. We present a new deep-Q algorithm that estimates the effect of both when and what to do called Earliest Disagreement Q-Evaluation (EDQ). EDQ makes use of recursion for the Q-function that is compatible with flexible sequence models, such as transformers. EDQ provides accurate estimates under standard assumptions. We validate the approach through experiments on survival time and tumor growth tasks. | Yoav Wald, Mark Goldstein, Yonathan Efroni, Wouter A. C. van Amsterdam, Rajesh Ranganath |  |
| 937 |  |  [GRAIN: Exact Graph Reconstruction from Gradients](https://openreview.net/forum?id=7bAjVh3CG3) |  | 0 |  | Maria Drencheva, Ivo Petrov, Maximilian Baader, Dimitar Iliev Dimitrov, Martin T. Vechev |  |
| 938 |  |  [ALLaM: Large Language Models for Arabic and English](https://openreview.net/forum?id=MscdsFVZrN) |  | 0 |  | M. Saiful Bari, Yazeed Alnumay, Norah A. Alzahrani, Nouf M. Alotaibi, Hisham Abdullah Alyahya, Sultan Alrashed, Faisal Abdulrahman Mirza, Shaykhah Z. Alsubaie, Hassan A. Alahmed, Ghadah Alabduljabbar, Raghad Alkhathran, Yousef Almushayqih, Raneem Alnajim, Salman Alsubaihi, Maryam Al Mansour, Saad Amin Hassan, Majed Alrubaian, Ali Alammari, Zaki Alawami, Abdulmohsen AlThubaity, et al. |  |
| 939 |  |  [Execution-guided within-prompt search for programming-by-example](https://openreview.net/forum?id=PY56Wur7S0) |  | 0 |  | Gust Verbruggen, Ashish Tiwari, Mukul Singh, Vu Le, Sumit Gulwani |  |
| 940 |  |  [Lie Algebra Canonicalization: Equivariant Neural Operators under arbitrary Lie Groups](https://openreview.net/forum?id=7PLpiVdnUC) |  | 0 |  | Zakhar Shumaylov, Peter Zaika, James Rowbottom, Ferdia Sherry, Melanie Weber, CarolaBibiane Schönlieb |  |
| 941 |  |  [Extendable and Iterative Structure Learning Strategy for Bayesian Networks](https://openreview.net/forum?id=3n6DYH3cIP) |  | 0 |  | Hamid Kalantari, Russell Greiner, Pouria Ramazi |  |
| 942 |  |  [Improving Graph Neural Networks by Learning Continuous Edge Directions](https://openreview.net/forum?id=iAmR7FfMmq) |  | 0 |  | Seong Ho Pahng, Sahand Hormoz |  |
| 943 |  |  [Accelerating Task Generalisation with Multi-Level Skill Hierarchies](https://openreview.net/forum?id=KfeRfxTemB) |  | 0 |  | Thomas P. Cannon, Özgür Simsek |  |
| 944 |  |  [SiReRAG: Indexing Similar and Related Information for Multihop Reasoning](https://openreview.net/forum?id=yp95goUAT1) |  | 0 |  | Nan Zhang, Prafulla Kumar Choubey, Alexander R. Fabbri, Gabriel BernadettShapiro, Rui Zhang, Prasenjit Mitra, Caiming Xiong, ChienSheng Wu |  |
| 945 |  |  [Make Haste Slowly: A Theory of Emergent Structured Mixed Selectivity in Feature Learning ReLU Networks](https://openreview.net/forum?id=27SSnLl85x) |  | 0 |  | Devon Jarvis, Richard Klein, Benjamin Rosman, Andrew M. Saxe |  |
| 946 |  |  [Eliciting Human Preferences with Language Models](https://openreview.net/forum?id=LvDwwAgMEW) |  | 0 |  | Belinda Z. Li, Alex Tamkin, Noah D. Goodman, Jacob Andreas |  |
| 947 |  |  [SymDiff: Equivariant Diffusion via Stochastic Symmetrisation](https://openreview.net/forum?id=i1NNCrRxdM) |  | 0 |  | Leo Zhang, Kianoosh Ashouritaklimi, Yee Whye Teh, Rob Cornish |  |
| 948 |  |  [AdaWM: Adaptive World Model based Planning for Autonomous Driving](https://openreview.net/forum?id=NEu8wgPctU) |  | 0 |  | Hang Wang, Xin Ye, Feng Tao, Chenbin Pan, Abhirup Mallik, Burhaneddin Yaman, Liu Ren, Junshan Zhang |  |
| 949 |  |  [A Meta-Learning Approach to Bayesian Causal Discovery](https://openreview.net/forum?id=eeJz7eDWKO) |  | 0 |  | Anish Dhir, Matthew Ashman, James Requeima, Mark van der Wilk |  |
| 950 |  |  [Is Your Video Language Model a Reliable Judge?](https://openreview.net/forum?id=m8yby1JfbU) |  | 0 |  | Ming Liu, Wensheng Zhang |  |
| 951 |  |  [Boosting Multiple Views for pretrained-based Continual Learning](https://openreview.net/forum?id=AZR4R3lw7y) |  | 0 |  | Quyen Tran, Tung Lam Tran, Khanh Doan, Toan Tran, Dinh Q. Phung, Khoat Than, Trung Le |  |
| 952 |  |  [Matrix Product Sketching via Coordinated Sampling](https://openreview.net/forum?id=eHfq8Q3LeD) |  | 0 |  | Majid Daliri, Juliana Freire, Danrong Li, Christopher Musco |  |
| 953 |  |  [DUET: Decentralized Bilevel Optimization without Lower-Level Strong Convexity](https://openreview.net/forum?id=jxMAPMqNr5) |  | 0 |  | Zhen Qin, Zhuqing Liu, Songtao Lu, Yingbin Liang, Jia Liu |  |
| 954 |  |  [Preserving Deep Representations in One-Shot Pruning: A Hessian-Free Second-Order Optimization Framework](https://openreview.net/forum?id=eNQp79A5Oz) |  | 0 |  | Ryan Lucas, Rahul Mazumder |  |
| 955 |  |  [Chemistry-Inspired Diffusion with Non-Differentiable Guidance](https://openreview.net/forum?id=4dAgG8ma3B) |  | 0 |  | Yuchen Shen, Chenhao Zhang, Sijie Fu, Chenghui Zhou, Newell Washburn, Barnabás Póczos |  |
| 956 |  |  [Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning](https://openreview.net/forum?id=nDmwloEl3N) |  | 0 |  | Moritz Reuss, Jyothish Pari, Pulkit Agrawal, Rudolf Lioutikov |  |
| 957 |  |  [No Location Left Behind: Measuring and Improving the Fairness of Implicit Representations for Earth Data](https://openreview.net/forum?id=hSZaCIznB2) |  | 0 |  | Daniel Cai, Randall Balestriero |  |
| 958 |  |  [InvestESG: A multi-agent reinforcement learning benchmark for studying climate investment as a social dilemma](https://openreview.net/forum?id=2TasVD7FXp) |  | 0 |  | Xiaoxuan Hou, Jiayi Yuan, Joel Z. Leibo, Natasha Jaques |  |
| 959 |  |  [Discrete GCBF Proximal Policy Optimization for Multi-agent Safe Optimal Control](https://openreview.net/forum?id=1X1R7P6yzt) |  | 0 |  | Songyuan Zhang, Oswin So, Mitchell Black, Chuchu Fan |  |
| 960 |  |  [Aligning Language Models with Demonstrated Feedback](https://openreview.net/forum?id=1qGkuxI9UX) |  | 0 |  | Omar Shaikh, Michelle S. Lam, Joey Hejna, Yijia Shao, Hyundong Justin Cho, Michael S. Bernstein, Diyi Yang |  |
| 961 |  |  [See It from My Perspective: How Language Affects Cultural Bias in Image Understanding](https://openreview.net/forum?id=Xbl6t6zxZs) |  | 0 |  | Amith Ananthram, Elias StengelEskin, Mohit Bansal, Kathleen McKeown |  |
| 962 |  |  [Do Mice Grok? Glimpses of Hidden Progress in Sensory Cortex](https://openreview.net/forum?id=oYemKnlIrO) |  | 0 |  | Tanishq Kumar, Blake Bordelon, Cengiz Pehlevan, Venkatesh N. Murthy, Samuel J. Gershman |  |
| 963 |  |  [SymmCD: Symmetry-Preserving Crystal Generation with Diffusion Models](https://openreview.net/forum?id=xnssGv9rpW) |  | 0 |  | Daniel Levy, Siba Smarak Panigrahi, SékouOumar Kaba, Qiang Zhu, Kin Long Kelvin Lee, Mikhail Galkin, Santiago Miret, Siamak Ravanbakhsh |  |
| 964 |  |  [Benchmarking LLMs' Judgments with No Gold Standard](https://openreview.net/forum?id=uE84MGbKD7) |  | 0 |  | Shengwei Xu, Yuxuan Lu, Grant Schoenebeck, Yuqing Kong |  |
| 965 |  |  [The Pitfalls of Memorization: When Memorization Hurts Generalization](https://openreview.net/forum?id=vVhZh9ZpIM) |  | 0 |  | Reza Bayat, Mohammad Pezeshki, Elvis Dohmatob, David LopezPaz, Pascal Vincent |  |
| 966 |  |  [Graph Neural Networks Gone Hogwild](https://openreview.net/forum?id=WfxPVtYRlL) |  | 0 |  | Olga Solodova, Nick Richardson, Deniz Oktay, Ryan P. Adams |  |
| 967 |  |  [Benchmarking Vision Language Model Unlearning via Fictitious Facial Identity Dataset](https://openreview.net/forum?id=0y3hGn1wOk) |  | 0 |  | Yingzi Ma, Jiongxiao Wang, Fei Wang, Siyuan Ma, Jiazhao Li, Jinsheng Pan, Xiujun Li, Furong Huang, Lichao Sun, Bo Li, Yejin Choi, Muhao Chen, Chaowei Xiao |  |
| 968 |  |  [A Deep Generative Learning Approach for Two-stage Adaptive Robust Optimization](https://openreview.net/forum?id=CKXul9iX77) |  | 0 |  | Aron Brenner, Rahman Khorramfar, Jennifer Z. Sun, Saurabh Amin |  |
| 969 |  |  [Fundamental Limitations on Subquadratic Alternatives to Transformers](https://openreview.net/forum?id=T2d0geb6y0) |  | 0 |  | Josh Alman, Hantao Yu |  |
| 970 |  |  [Cocoon: Robust Multi-Modal Perception with Uncertainty-Aware Sensor Fusion](https://openreview.net/forum?id=DKgAFfCs5F) |  | 0 |  | Minkyoung Cho, Yulong Cao, Jiachen Sun, Qingzhao Zhang, Marco Pavone, Jeong Joon Park, Heng Yang, Zhuoqing Mao |  |
| 971 |  |  [Decision Tree Induction Through LLMs via Semantically-Aware Evolution](https://openreview.net/forum?id=UyhRtB4hjN) |  | 0 |  | Tennison Liu, Nicolas Huynh, Mihaela van der Schaar |  |
| 972 |  |  [Efficient Imitation under Misspecification](https://openreview.net/forum?id=fn36V5qsCw) |  | 0 |  | Nicolas A. Espinosa Dice, Sanjiban Choudhury, Wen Sun, Gokul Swamy |  |
| 973 |  |  [SV-RAG: LoRA-Contextualizing Adaptation of MLLMs for Long Document Understanding](https://openreview.net/forum?id=FDaHjwInXO) |  | 0 |  | Jian Chen, Ruiyi Zhang, Yufan Zhou, Tong Yu, Franck Dernoncourt, Jiuxiang Gu, Ryan A. Rossi, Changyou Chen, Tong Sun |  |
| 974 |  |  [Intelligence at the Edge of Chaos](https://openreview.net/forum?id=IeRcpsdY7P) |  | 0 |  | Shiyang Zhang, Aakash Patel, Syed Asad Rizvi, Nianchen Liu, Sizhuang He, Amin Karbasi, Emanuele Zappala, David van Dijk |  |
| 975 |  |  [Towards a learning theory of representation alignment](https://openreview.net/forum?id=DShqJA1Z64) |  | 0 |  | Francesco Insulla, Shuo Huang, Lorenzo Rosasco |  |
| 976 |  |  [A Unified Framework for Forward and Inverse Problems in Subsurface Imaging using Latent Space Translations](https://openreview.net/forum?id=yIlyHJdYV3) |  | 0 |  | Naveen Gupta, Medha Sawhney, Arka Daw, Youzuo Lin, Anuj Karpatne |  |
| 977 |  |  [Cauchy-Schwarz Regularizers](https://openreview.net/forum?id=KZu3xhPhke) |  | 0 |  | Sueda Taner, Ziyi Wang, Christoph Studer |  |
| 978 |  |  [Adversarially Robust Anomaly Detection through Spurious Negative Pair Mitigation](https://openreview.net/forum?id=t8fu5m8R5m) |  | 0 |  | Hossein Mirzaei, Mojtaba Nafez, Jafar Habibi, Mohammad Sabokrou, Mohammad Hossein Rohban |  |
| 979 |  |  [Holographic Node Representations: Pre-training Task-Agnostic Node Embeddings](https://openreview.net/forum?id=tGYFikNONB) |  | 0 |  | Beatrice Bevilacqua, Joshua Robinson, Jure Leskovec, Bruno Ribeiro |  |
| 980 |  |  [The Ramanujan Library - Automated Discovery on the Hypergraph of Integer Relations](https://openreview.net/forum?id=EyaH1wzmao) |  | 0 |  | Itay Beit Halachmi, Ido Kaminer |  |
| 981 |  |  [HELMET: How to Evaluate Long-context Models Effectively and Thoroughly](https://openreview.net/forum?id=293V3bJbmE) |  | 0 |  | Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, Danqi Chen |  |
| 982 |  |  [Uncovering Latent Memories in Large Language Models](https://openreview.net/forum?id=KSBx6FBZpE) |  | 0 |  | Sunny Duan, Mikail Khona, Abhiram Iyer, Rylan Schaeffer, Ila R. Fiete |  |
| 983 |  |  [Episodic Memories Generation and Evaluation Benchmark for Large Language Models](https://openreview.net/forum?id=6ycX677p2l) |  | 0 |  | Alexis Huet, Zied BenHouidi, Dario Rossi |  |
| 984 |  |  [Continual Slow-and-Fast Adaptation of Latent Neural Dynamics (CoSFan): Meta-Learning What-How & When to Adapt](https://openreview.net/forum?id=Dl3MsjaIdp) |  | 0 |  | Ryan Missel, Linwei Wang |  |
| 985 |  |  [R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference](https://openreview.net/forum?id=9VMW4iXfKt) |  | 0 |  | Zhenyu Zhang, Zechun Liu, Yuandong Tian, Harshit Khaitan, Zhangyang Wang, Steven Li |  |
| 986 |  |  [Query-based Knowledge Transfer for Heterogeneous Learning Environments](https://openreview.net/forum?id=XKv29sMyjF) |  | 0 |  | Norah Alballa, Wenxuan Zhang, Ziquan Liu, Ahmed M. Abdelmoniem, Mohamed Elhoseiny, Marco Canini |  |
| 987 |  |  [Language Models Are Implicitly Continuous](https://openreview.net/forum?id=SMK0f8JoKF) |  | 0 |  | Samuele Marro, Davide Evangelista, X. Angelo Huang, Emanuele La Malfa, Michele Lombardi, Michael J. Wooldridge |  |
| 988 |  |  [Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape View](https://openreview.net/forum?id=m51BgoqvbP) |  | 0 |  | Kaiyue Wen, Zhiyuan Li, Jason S. Wang, David Leo Wright Hall, Percy Liang, Tengyu Ma |  |
| 989 |  |  [ConceptPrune: Concept Editing in Diffusion Models via Skilled Neuron Pruning](https://openreview.net/forum?id=kSdWcw5mkp) |  | 0 |  | Ruchika Chavhan, Da Li, Timothy M. Hospedales |  |
| 990 |  |  [Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning](https://openreview.net/forum?id=v4MTnPiYXY) |  | 0 |  | Joey Hong, Anca D. Dragan, Sergey Levine |  |
| 991 |  |  [Mm-Embed: Universal Multimodal Retrieval with Multimodal LLMS](https://openreview.net/forum?id=i45NQb2iKO) |  | 0 |  | ShengChieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, Wei Ping |  |
| 992 |  |  [RNNs are not Transformers (Yet): The Key Bottleneck on In-Context Retrieval](https://openreview.net/forum?id=h3wbI8Uk1Z) |  | 0 |  | Kaiyue Wen, Xingyu Dang, Kaifeng Lyu |  |
| 993 |  |  [Regressing the Relative Future: Efficient Policy Optimization for Multi-turn RLHF](https://openreview.net/forum?id=cVyELMpMRS) |  | 0 |  | Zhaolin Gao, Wenhao Zhan, Jonathan Daniel Chang, Gokul Swamy, Kianté Brantley, Jason D. Lee, Wen Sun |  |
| 994 |  |  [Diffusing States and Matching Scores: A New Framework for Imitation Learning](https://openreview.net/forum?id=kWRKNDU6uN) |  | 0 |  | Runzhe Wu, Yiding Chen, Gokul Swamy, Kianté Brantley, Wen Sun |  |
| 995 |  |  [Behavioral Entropy-Guided Dataset Generation for Offline Reinforcement Learning](https://openreview.net/forum?id=LuT2CVrlpU) |  | 0 |  | Wesley A. Suttle, Aamodh Suresh, Carlos NietoGranda |  |
| 996 |  |  [Context-aware Dynamic Pruning for Speech Foundation Models](https://openreview.net/forum?id=u2QdCiOgwA) |  | 0 |  | Masao Someki, Yifan Peng, Siddhant Arora, Markus Müller, Athanasios Mouchtaris, Grant P. Strimel, Jing Liu, Shinji Watanabe |  |
| 997 |  |  [Infinite-Resolution Integral Noise Warping for Diffusion Models](https://openreview.net/forum?id=Y6LPWBo2HP) |  | 0 |  | Yitong Deng, Winnie Lin, Lingxiao Li, Dmitriy Smirnov, Ryan D. Burgert, Ning Yu, Vincent Dedun, Mohammad H. Taghavi |  |
| 998 |  |  [Injective flows for star-like manifolds](https://openreview.net/forum?id=Jyh0DR4fFE) |  | 0 |  | Marcello Massimo Negri, Jonathan Aellen, Volker Roth |  |
| 999 |  |  [Mixture of In-Context Prompters for Tabular PFNs](https://openreview.net/forum?id=2fojNANZSv) |  | 0 |  | Derek Qiang Xu, F. Olcay Cirit, Reza Asadi, Yizhou Sun, Wei Wang |  |
| 1000 |  |  [Policy Design in Long-run Welfare Dynamics](https://openreview.net/forum?id=d8hYXbxX71) |  | 0 |  | Jiduan Wu, Rediet Abebe, Moritz Hardt, AnaAndreea Stoica |  |
| 1001 |  |  [Dynamic Modeling of Patients, Modalities and Tasks via Multi-modal Multi-task Mixture of Experts](https://openreview.net/forum?id=NJxCpMt0sf) |  | 0 |  | Chenwei Wu, Zitao Shuai, Zhengxu Tang, Luning Wang, Liyue Shen |  |
| 1002 |  |  [Procedural Synthesis of Synthesizable Molecules](https://openreview.net/forum?id=OGfyzExd69) |  | 0 |  | Michael Sun, Alston Lo, Minghao Guo, Jie Chen, Connor W. Coley, Wojciech Matusik |  |
| 1003 |  |  [Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning](https://openreview.net/forum?id=44z7HL4mfX) |  | 0 |  | Simran Kaur, Simon Park, Anirudh Goyal, Sanjeev Arora |  |
| 1004 |  |  [P-Spikessm: Harnessing Probabilistic Spiking State Space Models for Long-Range Dependency Tasks](https://openreview.net/forum?id=Sf4ep9Udjf) |  | 0 |  | Malyaban Bal, Abhronil Sengupta |  |
| 1005 |  |  [Trained Transformer Classifiers Generalize and Exhibit Benign Overfitting In-Context](https://openreview.net/forum?id=jwsPS8yRe4) |  | 0 |  | Spencer Frei, Gal Vardi |  |
| 1006 |  |  [PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs](https://openreview.net/forum?id=bmrYu2Ekdz) |  | 0 |  | Oskar van der Wal, Pietro Lesci, Max MüllerEberstein, Naomi Saphra, Hailey Schoelkopf, Willem H. Zuidema, Stella Biderman |  |
| 1007 |  |  [Adapters for Altering LLM Vocabularies: What Languages Benefit the Most?](https://openreview.net/forum?id=KxQRHOre9D) |  | 0 |  | HyoJung Han, Akiko Eriguchi, Haoran Xu, Hieu Hoang, Marine Carpuat, Huda Khayrallah |  |
| 1008 |  |  [DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories Search](https://openreview.net/forum?id=tn2mjzjSyR) |  | 0 |  | Murong Yue, Wenlin Yao, Haitao Mi, Dian Yu, Ziyu Yao, Dong Yu |  |
| 1009 |  |  [Learning Diagrams: A Graphical Language for Compositional Training Regimes](https://openreview.net/forum?id=dqyuCsBvn9) |  | 0 |  | Mason Lary, Richard Samuelson, Alexander Wilentz, Alina Zare, Matthew Klawonn, James P. Fairbanks |  |
| 1010 |  |  [Learning Continually by Spectral Regularization](https://openreview.net/forum?id=Hcb2cgPbMg) |  | 0 |  | Alex Lewandowski, Michal Bortkiewicz, Saurabh Kumar, András György, Dale Schuurmans, Mateusz Ostaszewski, Marlos C. Machado |  |
| 1011 |  |  [No Equations Needed: Learning System Dynamics Without Relying on Closed-Form ODEs](https://openreview.net/forum?id=kbm6tsICar) |  | 0 |  | Krzysztof Kacprzyk, Mihaela van der Schaar |  |
| 1012 |  |  [An Undetectable Watermark for Generative Image Models](https://openreview.net/forum?id=jlhBFm7T2J) |  | 0 |  | Sam Gunn, Xuandong Zhao, Dawn Song |  |
| 1013 |  |  [Teaching LLMs How to Learn with Contextual Fine-Tuning](https://openreview.net/forum?id=FS2nukC2jv) |  | 0 |  | Younwoo Choi, Muhammad Adil Asif, Ziwen Han, John Willes, Rahul G. Krishnan |  |
| 1014 |  |  [Modeling dynamic social vision highlights gaps between deep learning and humans](https://openreview.net/forum?id=wAXsx2MYgV) |  | 0 |  | Kathy Garcia, Emalie McMahon, Colin Conwell, Michael F. Bonner, Leyla Isik |  |
| 1015 |  |  [Statistical Tractability of Off-policy Evaluation of History-dependent Policies in POMDPs](https://openreview.net/forum?id=Qja5s0K3VX) |  | 0 |  | Yuheng Zhang, Nan Jiang |  |
| 1016 |  |  [Large Language Models can Become Strong Self-Detoxifiers](https://openreview.net/forum?id=jY5oml9fe9) |  | 0 |  | ChingYun Ko, PinYu Chen, Payel Das, Youssef Mroueh, Soham Dan, Georgios Kollias, Subhajit Chaudhury, Tejaswini Pedapati, Luca Daniel |  |
| 1017 |  |  [STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning](https://openreview.net/forum?id=4VHiptx7xe) |  | 0 |  | Marius Memmel, Jacob Berg, Bingqing Chen, Abhishek Gupta, Jonathan Francis |  |
| 1018 |  |  [PortLLM: Personalizing Evolving Large Language Models with Training-Free and Portable Model Patches](https://openreview.net/forum?id=gyHoR6uFhU) |  | 0 |  | Rana Muhammad Shahroz, Pingzhi Li, Sukwon Yun, Zhenyu Wang, Shahriar Nirjon, ChauWai Wong, Tianlong Chen |  |
| 1019 |  |  [Robust Transfer of Safety-Constrained Reinforcement Learning Agents](https://openreview.net/forum?id=rvXdGL4pCJ) |  | 0 |  | Markel Zubia, Thiago D. Simão, Nils Jansen |  |
| 1020 |  |  [Residual Stream Analysis with Multi-Layer SAEs](https://openreview.net/forum?id=XAjfjizaKs) |  | 0 |  | Tim Lawson, Lucy Farnik, Conor Houghton, Laurence Aitchison |  |
| 1021 |  |  [The Same but Different: Structural Similarities and Differences in Multilingual Language Modeling](https://openreview.net/forum?id=NCrFA7dq8T) |  | 0 |  | Ruochen Zhang, Qinan Yu, Matianyu Zang, Carsten Eickhoff, Ellie Pavlick |  |
| 1022 |  |  [Disentangling Representations through Multi-task Learning](https://openreview.net/forum?id=yVGGtsOgc7) |  | 0 |  | Pantelis Vafidis, Aman Bhargava, Antonio Rangel |  |
| 1023 |  |  [Training Free Guided Flow-Matching with Optimal Control](https://openreview.net/forum?id=61ss5RA1MM) |  | 0 |  | Luran Wang, Chaoran Cheng, Yizhen Liao, Yanru Qu, Ge Liu |  |
| 1024 |  |  [Efficient Sparse PCA via Block-Diagonalization](https://openreview.net/forum?id=FAYIlGDBa1) |  | 0 |  | Alberto Del Pia, Dekun Zhou, Yinglun Zhu |  |
| 1025 |  |  [The Last Iterate Advantage: Empirical Auditing and Principled Heuristic Analysis of Differentially Private SGD](https://openreview.net/forum?id=DwqoBkj2Mw) |  | 0 |  | Milad Nasr, Thomas Steinke, Borja Balle, Christopher A. ChoquetteChoo, Arun Ganesh, Matthew Jagielski, Jamie Hayes, Abhradeep Guha Thakurta, Adam Smith, Andreas Terzis |  |
| 1026 |  |  [Trivialized Momentum Facilitates Diffusion Generative Modeling on Lie Groups](https://openreview.net/forum?id=DTatjJTDl1) |  | 0 |  | Yuchen Zhu, Tianrong Chen, Lingkai Kong, Evangelos A. Theodorou, Molei Tao |  |
| 1027 |  |  [GeoLoRA: Geometric integration for parameter efficient fine-tuning](https://openreview.net/forum?id=bsFWJ0Kget) |  | 0 |  | Steffen Schotthöfer, Emanuele Zangrando, Gianluca Ceruti, Francesco Tudisco, Jonas Kusch |  |
| 1028 |  |  [Support is All You Need for Certified VAE Training](https://openreview.net/forum?id=oZkqkkvdND) |  | 0 |  | Changming Xu, Debangshu Banerjee, Deepak Vasisht, Gagandeep Singh |  |
| 1029 |  |  [CONGO: Compressive Online Gradient Optimization](https://openreview.net/forum?id=4BFzTrIjPN) |  | 0 |  | Jeremy Carleton, Prathik Vijaykumar, Divyanshu Saxena, Dheeraj Narasimha, Srinivas Shakkottai, Aditya Akella |  |
| 1030 |  |  [Scalable Extraction of Training Data from Aligned, Production Language Models](https://openreview.net/forum?id=vjel3nWP2a) |  | 0 |  | Milad Nasr, Javier Rando, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne Ippolito, Christopher A. ChoquetteChoo, Florian Tramèr, Katherine Lee |  |
| 1031 |  |  [Measuring Non-Adversarial Reproduction of Training Data in Large Language Models](https://openreview.net/forum?id=590yfqz1LE) |  | 0 |  | Michael Aerni, Javier Rando, Edoardo Debenedetti, Nicholas Carlini, Daphne Ippolito, Florian Tramèr |  |
| 1032 |  |  [InsightBench: Evaluating Business Analytics Agents Through Multi-Step Insight Generation](https://openreview.net/forum?id=ZGqd0cbBvm) |  | 0 |  | Gaurav Sahu, Abhay Puri, Juan A. Rodríguez, Amirhossein Abaskohi, Mohammad Chegini, Alexandre Drouin, Perouz Taslakian, Valentina Zantedeschi, Alexandre Lacoste, David Vázquez, Nicolas Chapados, Christopher Pal, Sai Rajeswar, Issam H. Laradji |  |
| 1033 |  |  [Optimizing Posterior Samples for Bayesian Optimization via Rootfinding](https://openreview.net/forum?id=I6UbnkUveF) |  | 0 |  | Taiwo A. Adebiyi, Bach Do, Ruda Zhang |  |
| 1034 |  |  [Model-Agnostic Knowledge Guided Correction for Improved Neural Surrogate Rollout](https://openreview.net/forum?id=3ep9ZYMZS3) |  | 0 |  | Bharat Srikishan, Daniel O'Malley, Mohamed Mehana, Nicholas Lubbers, Nikhil Muralidhar |  |
| 1035 |  |  [Towards Fast, Specialized Machine Learning Force Fields: Distilling Foundation Models via Energy Hessians](https://openreview.net/forum?id=1durmugh3I) |  | 0 |  | Ishan Amin, Sanjeev Raja, Aditi S. Krishnapriyan |  |
| 1036 |  |  [Scaling Stick-Breaking Attention: An Efficient Implementation and In-depth Study](https://openreview.net/forum?id=r8J3DSD5kF) |  | 0 |  | Shawn Tan, Songlin Yang, Aaron C. Courville, Rameswar Panda, Yikang Shen |  |
| 1037 |  |  [FlowDec: A flow-based full-band general audio codec with high perceptual quality](https://openreview.net/forum?id=uxDFlPGRLX) |  | 0 |  | Simon Welker, Matthew Le, Ricky T. Q. Chen, WeiNing Hsu, Timo Gerkmann, Alexander Richard, YiChiao Wu |  |
| 1038 |  |  [End-to-end Learning of Gaussian Mixture Priors for Diffusion Sampler](https://openreview.net/forum?id=iXbUquaWbl) |  | 0 |  | Denis Blessing, Xiaogang Jia, Gerhard Neumann |  |
| 1039 |  |  [IterGen: Iterative Semantic-aware Structured LLM Generation with Backtracking](https://openreview.net/forum?id=ac93gRzxxV) |  | 0 |  | Shubham Ugare, Rohan Gumaste, Tarun Suresh, Gagandeep Singh, Sasa Misailovic |  |
| 1040 |  |  [OpenPRM: Building Open-domain Process-based Reward Models with Preference Trees](https://openreview.net/forum?id=fGIqGfmgkW) |  | 0 |  | Kaiyan Zhang, Jiayuan Zhang, Haoxin Li, Xuekai Zhu, Ermo Hua, Xingtai Lv, Ning Ding, Biqing Qi, Bowen Zhou |  |
| 1041 |  |  [SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training](https://openreview.net/forum?id=L9eBxTCpQG) |  | 0 |  | Tianjin Huang, Ziquan Zhu, Gaojie Jin, Lu Liu, Zhangyang Wang, Shiwei Liu |  |
| 1042 |  |  [Probing the Latent Hierarchical Structure of Data via Diffusion Models](https://openreview.net/forum?id=0GzqVqCKns) |  | 0 |  | Antonio Sclocchi, Alessandro Favero, Noam Itzhak Levi, Matthieu Wyart |  |
| 1043 |  |  [Dynamic Sparse Training versus Dense Training: The Unexpected Winner in Image Corruption Robustness](https://openreview.net/forum?id=daUQ7vmGap) |  | 0 |  | Boqian Wu, Qiao Xiao, Shunxin Wang, Nicola Strisciuglio, Mykola Pechenizkiy, Maurice van Keulen, Decebal Constantin Mocanu, Elena Mocanu |  |
| 1044 |  |  [Unlocking Point Processes through Point Set Diffusion](https://openreview.net/forum?id=4anfpHj0wf) |  | 0 |  | David Lüdke, Enric Rabasseda Raventós, Marcel Kollovieh, Stephan Günnemann |  |
| 1045 |  |  [Tracing Representation Progression: Analyzing and Enhancing Layer-Wise Similarity](https://openreview.net/forum?id=vVxeFSR4fU) |  | 0 |  | Jiachen Jiang, Jinxin Zhou, Zhihui Zhu |  |
| 1046 |  |  [Language Agents Meet Causality - Bridging LLMs and Causal World Models](https://openreview.net/forum?id=y9A2TpaGsE) |  | 0 |  | John Gkountouras, Matthias Lindemann, Phillip Lippe, Efstratios Gavves, Ivan Titov |  |
| 1047 |  |  [From Models to Microtheories: Distilling a Model's Topical Knowledge for Grounded Question-Answering](https://openreview.net/forum?id=JV8zULNh24) |  | 0 |  | Nathaniel Weir, Bhavana Dalvi Mishra, Orion Weller, Oyvind Tafjord, Sam Hornstein, Alexander Sabol, Peter A. Jansen, Benjamin Van Durme, Peter Clark |  |
| 1048 |  |  [Enabling Realtime Reinforcement Learning at Scale with Staggered Asynchronous Inference](https://openreview.net/forum?id=fXb9BbuyAD) |  | 0 |  | Matthew Riemer, Gopeshh Subbaraj, Glen Berseth, Irina Rish |  |
| 1049 |  |  [On the Transfer of Object-Centric Representation Learning](https://openreview.net/forum?id=bSq0XGS3kW) |  | 0 |  | Aniket Rajiv Didolkar, Andrii Zadaianchuk, Anirudh Goyal, Michael Curtis Mozer, Yoshua Bengio, Georg Martius, Maximilian Seitzer |  |
| 1050 |  |  [ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities](https://openreview.net/forum?id=lfPkGWXLLf) |  | 0 |  | Ezra Karger, Houtan Bastani, Chen YuehHan, Zachary Jacobs, Danny Halawi, Fred Zhang, Philip Tetlock |  |
| 1051 |  |  [Score-based free-form architectures for high-dimensional Fokker-Planck equations](https://openreview.net/forum?id=5qg6JPSgCj) |  | 0 |  | Feng Liu, Faguo Wu, Xiao Zhang |  |
| 1052 |  |  [ReGen: Generative Robot Simulation via Inverse Design](https://openreview.net/forum?id=EbCUbPZjM1) |  | 0 |  | Phat Nguyen, TsunHsuan Wang, ZhangWei Hong, Erfan Aasi, Andrew Silva, Guy Rosman, Sertac Karaman, Daniela Rus |  |
| 1053 |  |  [Flow-based Variational Mutual Information: Fast and Flexible Approximations](https://openreview.net/forum?id=spDUv05cEq) |  | 0 |  | Caleb Dahlke, Jason Pacheco |  |
| 1054 |  |  [Multi-agent cooperation through learning-aware policy gradients](https://openreview.net/forum?id=GkWA6NjePN) |  | 0 |  | Alexander Meulemans, Seijin Kobayashi, Johannes von Oswald, Nino Scherrer, Eric Elmoznino, Blake Aaron Richards, Guillaume Lajoie, Blaise Agüera y Arcas, João Sacramento |  |
| 1055 |  |  [Few-Class Arena: A Benchmark for Efficient Selection of Vision Models and Dataset Difficulty Measurement](https://openreview.net/forum?id=2ET561DyPe) |  | 0 |  | Bryan Bo Cao, Lawrence O'Gorman, Michael Coss, Shubham Jain |  |
| 1056 |  |  [Transformers Struggle to Learn to Search](https://openreview.net/forum?id=9cQB1Hwrtw) |  | 0 |  | Abulhair Saparov, Srushti Ajay Pawar, Shreyas Pimpalgaonkar, Nitish Joshi, Richard Yuanzhe Pang, Vishakh Padmakumar, Mehran Kazemi, Najoung Kim, He He |  |
| 1057 |  |  [Is uniform expressivity too restrictive? Towards efficient expressivity of GNNs](https://openreview.net/forum?id=lsvGqR6OTf) |  | 0 |  | Sammy Khalife, Josué TonelliCueto |  |
| 1058 |  |  [Training Neural Networks as Recognizers of Formal Languages](https://openreview.net/forum?id=aWLQTbfFgV) |  | 0 |  | Alexandra Butoi, Ghazal Khalighinejad, Anej Svete, Josef Valvoda, Ryan Cotterell, Brian DuSell |  |
| 1059 |  |  [Nonconvex Stochastic Optimization under Heavy-Tailed Noises: Optimal Convergence without Gradient Clipping](https://openreview.net/forum?id=NKotdPUc3L) |  | 0 |  | Zijian Liu, Zhengyuan Zhou |  |
| 1060 |  |  [Can We Trust Embodied Agents? Exploring Backdoor Attacks against Embodied LLM-Based Decision-Making Systems](https://openreview.net/forum?id=S1Bv3068Xt) |  | 0 |  | Ruochen Jiao, Shaoyuan Xie, Justin Yue, Takami Sato, Lixu Wang, Yixuan Wang, Qi Alfred Chen, Qi Zhu |  |
| 1061 |  |  [Self-Attention-Based Contextual Modulation Improves Neural System Identification](https://openreview.net/forum?id=JeLqFpFzwX) |  | 0 |  | Isaac Lin, Tianye Wang, Shang Gao, Shiming Tang, Tai Sing Lee |  |
| 1062 |  |  [Dataset Distillation via Knowledge Distillation: Towards Efficient Self-Supervised Pre-training of Deep Networks](https://openreview.net/forum?id=c61unr33XA) |  | 0 |  | Siddharth Joshi, Jiayi Ni, Baharan Mirzasoleiman |  |
| 1063 |  |  [Robotouille: An Asynchronous Planning Benchmark for LLM Agents](https://openreview.net/forum?id=OhUoTMxFIH) |  | 0 |  | Gonzalo GonzalezPumariega, Leong Su Yean, Neha Sunkara, Sanjiban Choudhury |  |
| 1064 |  |  [Learn-by-interact: A Data-Centric Framework For Self-Adaptive Agents in Realistic Environments](https://openreview.net/forum?id=3UKOzGWCVY) |  | 0 |  | Hongjin Su, Ruoxi Sun, Jinsung Yoon, Pengcheng Yin, Tao Yu, Sercan Ö. Arik |  |
| 1065 |  |  [Causal Representation Learning from Multimodal Biomedical Observations](https://openreview.net/forum?id=hjROBHstZ3) |  | 0 |  | Yuewen Sun, Lingjing Kong, Guangyi Chen, Loka Li, Gongxu Luo, Zijian Li, Yixuan Zhang, Yujia Zheng, Mengyue Yang, Petar Stojanov, Eran Segal, Eric P. Xing, Kun Zhang |  |
| 1066 |  |  [On the Learn-to-Optimize Capabilities of Transformers in In-Context Sparse Recovery](https://openreview.net/forum?id=NHhjczmJjo) |  | 0 |  | Renpu Liu, Ruida Zhou, Cong Shen, Jing Yang |  |
| 1067 |  |  [OvercookedV2: Rethinking Overcooked for Zero-Shot Coordination](https://openreview.net/forum?id=hlvLM3GX8R) |  | 0 |  | Tobias Gessler, Tin Dizdarevic, Ani Calinescu, Benjamin Ellis, Andrei Lupu, Jakob Nicolaus Foerster |  |
| 1068 |  |  [Sketching for Convex and Nonconvex Regularized Least Squares with Sharp Guarantees](https://openreview.net/forum?id=7liN6uHAQZ) |  | 0 |  | Yingzhen Yang, Ping Li |  |
| 1069 |  |  [TULIP: Token-length Upgraded CLIP](https://openreview.net/forum?id=r9oqHOdoHf) |  | 0 |  | Ivona Najdenkoska, Mohammad Mahdi Derakhshani, Yuki M. Asano, Nanne van Noord, Marcel Worring, Cees G. M. Snoek |  |
| 1070 |  |  [DELIFT: Data Efficient Language model Instruction Fine-Tuning](https://openreview.net/forum?id=Fty0wTcemV) |  | 0 |  | Ishika Agarwal, Krishnateja Killamsetty, Lucian Popa, Marina Danilevsky |  |
| 1071 |  |  [U-shaped and Inverted-U Scaling behind Emergent Abilities of Large Language Models](https://openreview.net/forum?id=jjfve2gIXe) |  | 0 |  | TungYu Wu, Melody Lo |  |
| 1072 |  |  [Hierarchical Autoregressive Transformers: Combining Byte- and Word-Level Processing for Robust, Adaptable Language Models](https://openreview.net/forum?id=tU074jg2vS) |  | 0 |  | Pit Neitemeier, Björn Deiseroth, Constantin Eichenberg, Lukas Balles |  |
| 1073 |  |  [Locally Connected Echo State Networks for Time Series Forecasting](https://openreview.net/forum?id=KeRwLLwZaw) |  | 0 |  | Filip Matzner, Frantisek Mráz |  |
| 1074 |  |  [Scaling Laws for Adversarial Attacks on Language Model Activations and Tokens](https://openreview.net/forum?id=YzxMu1asQi) |  | 0 |  | Stanislav Fort |  |
| 1075 |  |  [BEEM: Boosting Performance of Early Exit DNNs using Multi-Exit Classifiers as Experts](https://openreview.net/forum?id=EzrZX9bd4G) |  | 0 |  | Divya Jyoti Bajpai, Manjesh Kumar Hanawal |  |
| 1076 |  |  [Real-time design of architectural structures with differentiable mechanics and neural networks](https://openreview.net/forum?id=Tpjq66xwTq) |  | 0 |  | Rafael Pastrana, Eder Medina, Isabel M. de Oliveira, Sigrid Adriaenssens, Ryan P. Adams |  |
| 1077 |  |  [DAMO: Decoding by Accumulating Activations Momentum for Mitigating Hallucinations in Vision-Language Models](https://openreview.net/forum?id=JUr0YOMvZA) |  | 0 |  | Kaishen Wang, Hengrui Gu, Meijun Gao, Kaixiong Zhou |  |
| 1078 |  |  [Talking Turns: Benchmarking Audio Foundation Models on Turn-Taking Dynamics](https://openreview.net/forum?id=2e4ECh0ikn) |  | 0 |  | Siddhant Arora, Zhiyun Lu, ChungCheng Chiu, Ruoming Pang, Shinji Watanabe |  |
| 1079 |  |  [Mind the GAP: Glimpse-based Active Perception improves generalization and sample efficiency of visual reasoning](https://openreview.net/forum?id=iXCeQ2m6vT) |  | 0 |  | Oleh Kolner, Thomas Ortner, Stanislaw Wozniak, Angeliki Pantazi |  |
| 1080 |  |  [MMTEB: Massive Multilingual Text Embedding Benchmark](https://openreview.net/forum?id=zl3pfz4VCV) |  | 0 |  | Kenneth C. Enevoldsen, Isaac Chung, Imene Kerboua, Márton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzeminski, Genta Indra Winata, Saba Sturua, Saiteja Utpala, Mathieu Ciancone, Marion Schaeffer, Diganta Misra, Shreeya Dhakal, Jonathan Rystrøm, Roman Solomatin, Ömer Veysel Çagatan, Akash Kundu, et al. |  |
| 1081 |  |  [TexTailor: Customized Text-aligned Texturing via Effective Resampling](https://openreview.net/forum?id=1NprT9Kz0d) |  | 0 |  | Suin Lee, Daeshik Kim |  |
| 1082 |  |  [Generalization and Distributed Learning of GFlowNets](https://openreview.net/forum?id=PJNhZoCjLh) |  | 0 |  | Tiago Silva, Amauri H. Souza, Omar Rivasplata, Vikas Garg, Samuel Kaski, Diego Mesquita |  |
| 1083 |  |  [MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions](https://openreview.net/forum?id=WWXjMYZxfH) |  | 0 |  | Yekun Chai, Haoran Sun, Huang Fang, Shuohuan Wang, Yu Sun, Hua Wu |  |
| 1084 |  |  [Bonsai: Gradient-free Graph Condensation for Node Classification](https://openreview.net/forum?id=5x88lQ2MsH) |  | 0 |  | Mridul Gupta, Samyak Jain, Vansh Ramani, Hariprasad Kodamana, Sayan Ranu |  |
| 1085 |  |  [Logically Consistent Language Models via Neuro-Symbolic Integration](https://openreview.net/forum?id=7PGluppo4k) |  | 0 |  | Diego Calanzone, Stefano Teso, Antonio Vergari |  |
| 1086 |  |  [Narrowing Information Bottleneck Theory for Multimodal Image-Text Representations Interpretability](https://openreview.net/forum?id=INqLJwqUmc) |  | 0 |  | Zhiyu Zhu, Zhibo Jin, Jiayu Zhang, Nan Yang, Jiahao Huang, Jianlong Zhou, Fang Chen |  |
| 1087 |  |  [Efficient Source-Free Time-Series Adaptation via Parameter Subspace Disentanglement](https://openreview.net/forum?id=Q5Sawm0nqo) |  | 0 |  | Gaurav Patel, Christopher Michael Sandino, Behrooz Mahasseni, Ellen L. Zippi, Erdrin Azemi, Ali Moin, Juri Minxha |  |
| 1088 |  |  [WardropNet: Traffic Flow Predictions via Equilibrium-Augmented Learning](https://openreview.net/forum?id=7FHSPd3SRE) |  | 0 |  | Kai Jungel, Dario Paccagnan, Axel Parmentier, Maximilian Schiffer |  |
| 1089 |  |  [To Clip or not to Clip: the Dynamics of SGD with Gradient Clipping in High-Dimensions](https://openreview.net/forum?id=jmN1zXMq0O) |  | 0 |  | Noah Marshall, Ke Liang Xiao, Atish Agarwala, Elliot Paquette |  |
| 1090 |  |  [Discrete Diffusion Schrödinger Bridge Matching for Graph Transformation](https://openreview.net/forum?id=tQyh0gnfqW) |  | 0 |  | Jun Hyeong Kim, Seonghwan Kim, Seokhyun Moon, Hyeongwoo Kim, Jeheon Woo, Woo Youn Kim |  |
| 1091 |  |  [Breaking Mental Set to Improve Reasoning through Diverse Multi-Agent Debate](https://openreview.net/forum?id=t6QHYUOQL7) |  | 0 |  | Yexiang Liu, Jie Cao, Zekun Li, Ran He, Tieniu Tan |  |
| 1092 |  |  [Resolution Attack: Exploiting Image Compression to Deceive Deep Neural Networks](https://openreview.net/forum?id=OFukl9Qg8P) |  | 0 |  | Wangjia Yu, Xiaomeng Fu, Qiao Li, Jizhong Han, Xiaodan Zhang |  |
| 1093 |  |  [SLMRec: Distilling Large Language Models into Small for Sequential Recommendation](https://openreview.net/forum?id=G4wARwjF8M) |  | 0 |  | Wujiang Xu, Qitian Wu, Zujie Liang, Jiaojiao Han, Xuying Ning, Yunxiao Shi, Wenfang Lin, Yongfeng Zhang |  |
| 1094 |  |  [How Do Large Language Models Understand Graph Patterns? A Benchmark for Graph Pattern Comprehension](https://openreview.net/forum?id=CkKEuLmRnr) |  | 0 |  | Xinnan Dai, Haohao Qu, Yifei Shen, Bohang Zhang, Qihao Wen, Wenqi Fan, Dongsheng Li, Jiliang Tang, Caihua Shan |  |
| 1095 |  |  [Efficient Active Imitation Learning with Random Network Distillation](https://openreview.net/forum?id=GFgn2LprFR) |  | 0 |  | Emilien Biré, Anthony Kobanda, Ludovic Denoyer, Rémy Portelas |  |
| 1096 |  |  [Model-based RL as a Minimalist Approach to Horizon-Free and Second-Order Bounds](https://openreview.net/forum?id=txD9llAYn9) |  | 0 |  | Zhiyong Wang, Dongruo Zhou, John C. S. Lui, Wen Sun |  |
| 1097 |  |  [What Do You See in Common? Learning Hierarchical Prototypes over Tree-of-Life to Discover Evolutionary Traits](https://openreview.net/forum?id=4sDicVEy6M) |  | 0 |  | Harish Babu Manogaran, M. Maruf, Arka Daw, Kazi Sajeed Mehrab, Caleb Patrick Charpentier, Josef C. Uyeda, Wasila M. Dahdul, Matthew J. Thompson, Elizabeth G. Campolongo, Kaiya L. Provost, WeiLun Chao, Tanya Y. BergerWolf, Paula M. Mabee, Hilmar Lapp, Anuj Karpatne |  |
| 1098 |  |  [Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning](https://openreview.net/forum?id=9RCT0ngvZP) |  | 0 |  | Xiaochuan Li, Zichun Yu, Chenyan Xiong |  |
| 1099 |  |  [Affine Steerable Equivariant Layer for Canonicalization of Neural Networks](https://openreview.net/forum?id=5i6ZZUjCA9) |  | 0 |  | Yikang Li, Yeqing Qiu, Yuxuan Chen, Zhouchen Lin |  |
| 1100 |  |  [Has the Deep Neural Network learned the Stochastic Process? An Evaluation Viewpoint](https://openreview.net/forum?id=2U8owdruSQ) |  | 0 |  | Harshit Kumar, Beomseok Kang, Biswadeep Chakraborty, Saibal Mukhopadhyay |  |
| 1101 |  |  [LICORICE: Label-Efficient Concept-Based Interpretable Reinforcement Learning](https://openreview.net/forum?id=Mjn53GtMxi) |  | 0 |  | Zhuorui Ye, Stephanie Milani, Geoffrey J. Gordon, Fei Fang |  |
| 1102 |  |  [MIND: Math Informed syNthetic Dialogues for Pretraining LLMs](https://openreview.net/forum?id=TuOTSAiHDn) |  | 0 |  | Syeda Nahida Akter, Shrimai Prabhumoye, John Kamalu, Sanjeev Satheesh, Eric Nyberg, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro |  |
| 1103 |  |  [SMT: Fine-Tuning Large Language Models with Sparse Matrices](https://openreview.net/forum?id=GbgCRJedQ7) |  | 0 |  | Haoze He, Juncheng B. Li, Xuan Jiang, Heather Miller |  |
| 1104 |  |  [KGARevion: An AI Agent for Knowledge-Intensive Biomedical QA](https://openreview.net/forum?id=tnB94WQGrn) |  | 0 |  | Xiaorui Su, Yibo Wang, Shanghua Gao, Xiaolong Liu, Valentina Giunchiglia, DjorkArné Clevert, Marinka Zitnik |  |
| 1105 |  |  [Moral Alignment for LLM Agents](https://openreview.net/forum?id=MeGDmZjUXy) |  | 0 |  | Elizaveta Tennant, Stephen Hailes, Mirco Musolesi |  |
| 1106 |  |  [ColPali: Efficient Document Retrieval with Vision Language Models](https://openreview.net/forum?id=ogjBpZ8uSi) |  | 0 |  | Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, Pierre Colombo |  |
| 1107 |  |  [Content-Style Learning from Unaligned Domains: Identifiability under Unknown Latent Dimensions](https://openreview.net/forum?id=p60Y6o85Cj) |  | 0 |  | Sagar Shrestha, Xiao Fu |  |
| 1108 |  |  [From an LLM Swarm to a PDDL-empowered Hive: Planning Self-executed Instructions in a Multi-modal Jungle](https://openreview.net/forum?id=QAAsnSRwgu) |  | 0 |  | Kaustubh Vyas, Damien Graux, Yijun Yang, Sébastien Montella, Chenxin Diao, Wendi Zhou, Pavlos Vougiouklis, Ruofei Lai, Yang Ren, Keshuang Li, Jeff Z. Pan |  |
| 1109 |  |  [ZooProbe: A Data Engine for Evaluating, Exploring, and Evolving Large-scale Training Data for Multimodal LLMs](https://openreview.net/forum?id=T4LtGj7us1) |  | 0 |  | YiKai Zhang, Shiyin Lu, QingGuo Chen, DeChuan Zhan, HanJia Ye |  |
| 1110 |  |  [Wicked Oddities: Selectively Poisoning for Effective Clean-Label Backdoor Attacks](https://openreview.net/forum?id=1Z3C49JQVf) |  | 0 |  | Nguyen HungQuang, NgocHieu Nguyen, TheAnh Ta, Thanh NguyenTang, KokSeng Wong, Hoang ThanhTung, Khoa D. Doan |  |
| 1111 |  |  [Quantum (Inspired) D2-sampling with Applications](https://openreview.net/forum?id=tDIL7UXmSS) |  | 0 |  | Poojan Chetan Shah, Ragesh Jaiswal |  |
| 1112 |  |  [Non-Adversarial Inverse Reinforcement Learning via Successor Feature Matching](https://openreview.net/forum?id=LvRQgsvd5V) |  | 0 |  | Arnav Kumar Jain, Harley Wiltzer, Jesse Farebrother, Irina Rish, Glen Berseth, Sanjiban Choudhury |  |
| 1113 |  |  [MatExpert: Decomposing Materials Discovery By Mimicking Human Experts](https://openreview.net/forum?id=AUBvo4sxVL) |  | 0 |  | Qianggang Ding, Santiago Miret, Bang Liu |  |
| 1114 |  |  [SOAP: Improving and Stabilizing Shampoo using Adam for Language Modeling](https://openreview.net/forum?id=IDxZhXrpNf) |  | 0 |  | Nikhil Vyas, Depen Morwani, Rosie Zhao, Itai Shapira, David Brandfonbrener, Lucas Janson, Sham M. Kakade |  |
| 1115 |  |  [Causal Graphical Models for Vision-Language Compositional Understanding](https://openreview.net/forum?id=haJHr4UsQX) |  | 0 |  | Fiorenzo Parascandolo, Nicholas Moratelli, Enver Sangineto, Lorenzo Baraldi, Rita Cucchiara |  |
| 1116 |  |  [Mixture of Attentions For Speculative Decoding](https://openreview.net/forum?id=Rz0kozh3LE) |  | 0 |  | Matthieu Zimmer, Milan Gritta, Gerasimos Lampouras, Haitham BouAmmar, Jun Wang |  |
| 1117 |  |  [VL-Cache: Sparsity and Modality-Aware KV Cache Compression for Vision-Language Model Inference Acceleration](https://openreview.net/forum?id=HMrcv7Q4Ub) |  | 0 |  | Dezhan Tu, Danylo Vashchilenko, Yuzhe Lu, Panpan Xu |  |
| 1118 |  |  [Connectome Mapping: Shape-Memory Network via Interpretation of Contextual Semantic Information](https://openreview.net/forum?id=PZYr22zFyE) |  | 0 |  | Kyungsu Lee, Haeyun Lee, Jae Youn Hwang |  |
| 1119 |  |  [Learning Color Equivariant Representations](https://openreview.net/forum?id=IXyfbaGlps) |  | 0 |  | Yulong Yang, Felix O'Mahony, Christine AllenBlanchette |  |
| 1120 |  |  [Omni-MATH: A Universal Olympiad Level Mathematic Benchmark for Large Language Models](https://openreview.net/forum?id=yaqPf0KAlN) |  | 0 |  | Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, Baobao Chang |  |
| 1121 |  |  [Semantic Temporal Abstraction via Vision-Language Model Guidance for Efficient Reinforcement Learning](https://openreview.net/forum?id=zY37C8d6bS) |  | 0 |  | TianShuo Liu, XuHui Liu, Ruifeng Chen, Lixuan Jin, Pengyuan Wang, Zhilong Zhang, Yang Yu |  |
| 1122 |  |  [ScImage: How good are multimodal large language models at scientific text-to-image generation?](https://openreview.net/forum?id=ugyqNEOjoU) |  | 0 |  | Leixin Zhang, Steffen Eger, Yinjie Cheng, Weihe Zhai, Jonas Belouadi, Fahimeh Moafian, Zhixue Zhao |  |
| 1123 |  |  [Utility-Directed Conformal Prediction: A Decision-Aware Framework for Actionable Uncertainty Quantification](https://openreview.net/forum?id=iOMnn1hSBO) |  | 0 |  | Santiago CortesGomez, Carlos Miguel Patiño, Yewon Byun, Steven Wu, Eric Horvitz, Bryan Wilder |  |
| 1124 |  |  [On Targeted Manipulation and Deception when Optimizing LLMs for User Feedback](https://openreview.net/forum?id=Wf2ndb8nhf) |  | 0 |  | Marcus Williams, Micah Carroll, Adhyyan Narang, Constantin Weisser, Brendan Murphy, Anca D. Dragan |  |
| 1125 |  |  [Gaussian Differentially Private Human Faces Under a Face Radial Curve Representation](https://openreview.net/forum?id=K2Tqn8R9pu) |  | 0 |  | Carlos J. Soto, Matthew Reimherr, Aleksandra B. Slavkovic, Mark Shriver |  |
| 1126 |  |  [Diffusion Transformers for Tabular Data Time Series Generation](https://openreview.net/forum?id=bhOysNJvWm) |  | 0 |  | Fabrizio Garuti, Enver Sangineto, Simone Luetto, Lorenzo Forni, Rita Cucchiara |  |
| 1127 |  |  [Non-Equilibrium Dynamics of Hybrid Continuous-Discrete Ground-State Sampling](https://openreview.net/forum?id=BlSIKSPhfz) |  | 0 |  | Timothée G. Leleu, Sam Reifenstein |  |
| 1128 |  |  [The Belief State Transformer](https://openreview.net/forum?id=ThRMTCgpvo) |  | 0 |  | Edward S. Hu, Kwangjun Ahn, Qinghua Liu, Haoran Xu, Manan Tomar, Ada Langford, Dinesh Jayaraman, Alex Lamb, John Langford |  |
| 1129 |  |  [Bridging the Data Provenance Gap Across Text, Speech, and Video](https://openreview.net/forum?id=G5DziesYxL) |  | 0 |  | Shayne Longpre, Nikhil Singh, Manuel Cherep, Kushagra Tiwary, Joanna Materzynska, William Brannon, Robert Mahari, Naana ObengMarnu, Manan Dey, Mohammed Hamdy, Nayan Saxena, Ahmad Mustafa Anis, Emad A. Alghamdi, Vu Minh Chien, Da Yin, Kun Qian, Yizhi Li, Minnie Liang, An Dinh, Shrestha Mohanty, et al. |  |
| 1130 |  |  [Free Hunch: Denoiser Covariance Estimation for Diffusion Models Without Extra Costs](https://openreview.net/forum?id=4JK2XMGUc8) |  | 0 |  | Severi Rissanen, Markus Heinonen, Arno Solin |  |
| 1131 |  |  [One Hundred Neural Networks and Brains Watching Videos: Lessons from Alignment](https://openreview.net/forum?id=LM4PYXBId5) |  | 0 |  | Christina Sartzetaki, Gemma Roig, Cees G. M. Snoek, Iris I. A. Groen |  |
| 1132 |  |  [Value-aligned Behavior Cloning for Offline Reinforcement Learning via Bi-level Optimization](https://openreview.net/forum?id=elTJBP7Fbv) |  | 0 |  | Xingyu Jiang, Ning Gao, Xiuhui Zhang, Hongkun Dou, Yue Deng |  |
| 1133 |  |  [Fugatto 1: Foundational Generative Audio Transformer Opus 1](https://openreview.net/forum?id=B2Fqu7Y2cd) |  | 0 |  | Rafael Valle, Rohan Badlani, Zhifeng Kong, Sanggil Lee, Arushi Goel, Sungwon Kim, João Felipe Santos, Shuqi Dai, Siddharth Gururani, Aya Aljafari, Alexander H. Liu, Kevin J. Shih, Ryan Prenger, Wei Ping, ChaoHan Huck Yang, Bryan Catanzaro |  |
| 1134 |  |  [Balancing Bias in Two-sided Markets for Fair Stable Matchings](https://openreview.net/forum?id=qykpnEWf2J) |  | 0 |  | Siyuan Wu, Leong Hou U, Panagiotis Karras |  |
| 1135 |  |  [Addax: Utilizing Zeroth-Order Gradients to Improve Memory Efficiency and Performance of SGD for Fine-Tuning Language Models](https://openreview.net/forum?id=QhxjQOMdDF) |  | 0 |  | Zeman Li, Xinwei Zhang, Peilin Zhong, Yuan Deng, Meisam Razaviyayn, Vahab Mirrokni |  |
| 1136 |  |  [Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for Large Language Models](https://openreview.net/forum?id=uZgK0tcPqd) |  | 0 |  | Ángela LópezCardona, Carlos Segura, Alexandros Karatzoglou, Sergi Abadal, Ioannis Arapakis |  |
| 1137 |  |  [Revisiting Large-Scale Non-convex Distributionally Robust Optimization](https://openreview.net/forum?id=JYwVijuNA7) |  | 0 |  | Qi Zhang, Yi Zhou, Simon Khan, Ashley PraterBennette, Lixin Shen, Shaofeng Zou |  |
| 1138 |  |  [Beyond single neurons: population response geometry in digital twins of mouse visual cortex](https://openreview.net/forum?id=kSISSDUYFh) |  | 0 |  | Dario Liscai, Emanuele Luconi, Alessandro Marin Vargas, Alessandro Sanzeni |  |
| 1139 |  |  [State Space Models are Provably Comparable to Transformers in Dynamic Token Selection](https://openreview.net/forum?id=QFgbJOYJSE) |  | 0 |  | Naoki Nishikawa, Taiji Suzuki |  |
| 1140 |  |  [Consistency Models Made Easy](https://openreview.net/forum?id=xQVxo9dSID) |  | 0 |  | Zhengyang Geng, Ashwini Pokle, Weijian Luo, Justin Lin, J. Zico Kolter |  |
| 1141 |  |  [OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data](https://openreview.net/forum?id=mTCbq2QssD) |  | 0 |  | Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, Igor Gitman |  |
| 1142 |  |  [MAPS: Advancing Multi-Modal Reasoning in Expert-Level Physical Science](https://openreview.net/forum?id=GR0y0F3Ipd) |  | 0 |  | Erle Zhu, Yadi Liu, Zhe Zhang, Xujun Li, Jin Zhou, Xinjie Yu, Minlie Huang, Hongning Wang |  |
| 1143 |  |  [MAESTRO: Masked Encoding Set Transformer with Self-Distillation](https://openreview.net/forum?id=FEZOLWexPb) |  | 0 |  | Matthew Eric Lee, Jaesik Kim, Matei Ionita, Jonghyun Lee, Michelle L. McKeague, Yonghyun Nam, Irene Khavin, Yidi Huang, Victoria Fang, Sokratis Apostolidis, Divij Mathew, Shwetank, Ajinkya Pattekar, Zahabia Rangwala, Amit BarOr, Benjamin A Fensterheim, Benjamin A. Abramoff, Rennie L. Rhee, Damian Maseda, Allison R. Greenplate, et al. |  |
| 1144 |  |  [AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out Context Attribution](https://openreview.net/forum?id=9kJperA2a4) |  | 0 |  | Fengyuan Liu, Nikhil Kandpal, Colin Raffel |  |
| 1145 |  |  [Provably Safeguarding a Classifier from OOD and Adversarial Samples](https://openreview.net/forum?id=kwCHcaeHrf) |  | 0 |  | Nicolas Atienza, Johanne Cohen, Christophe Labreuche, Michèle Sebag |  |
| 1146 |  |  [Learning Chaos In A Linear Way](https://openreview.net/forum?id=Llh6CinTiy) |  | 0 |  | Xiaoyuan Cheng, Yi He, Yiming Yang, Xiao Xue, Sibo Cheng, Daniel Giles, Xiaohang Tang, Yukun Hu |  |
| 1147 |  |  [Data Taggants: Dataset Ownership Verification Via Harmless Targeted Data Poisoning](https://openreview.net/forum?id=6ldD8Y4gBQ) |  | 0 |  | Wassim Bouaziz, Nicolas Usunier, ElMahdi ElMhamdi |  |
| 1148 |  |  [Do Contemporary Causal Inference Models Capture Real-World Heterogeneity? Findings from a Large-Scale Benchmark](https://openreview.net/forum?id=Q2bJ2qgcP1) |  | 0 |  | Haining Yu, Yizhou Sun |  |
| 1149 |  |  [Harnessing Webpage UIs for Text-Rich Visual Understanding](https://openreview.net/forum?id=IIsTO4P3Ag) |  | 0 |  | Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan Xiong, Wenhu Chen, Graham Neubig, Xiang Yue |  |
| 1150 |  |  [Fast unsupervised ground metric learning with tree-Wasserstein distance](https://openreview.net/forum?id=FBhKUXK7od) |  | 0 |  | Kira Michaela Düsterwald, Samo Hromadka, Makoto Yamada |  |
| 1151 |  |  [World Model on Million-Length Video And Language With Blockwise RingAttention](https://openreview.net/forum?id=HN8V0flwJF) |  | 0 |  | Hao Liu, Wilson Yan, Matei Zaharia, Pieter Abbeel |  |
| 1152 |  |  [Quamba: A Post-Training Quantization Recipe for Selective State Space Models](https://openreview.net/forum?id=mnna9LUg7P) |  | 0 |  | HungYueh Chiang, ChiChih Chang, Natalia Frumkin, KaiChiang Wu, Diana Marculescu |  |
| 1153 |  |  [Learning to engineer protein flexibility](https://openreview.net/forum?id=L238BAx0wP) |  | 0 |  | Petr Kouba, Joan PlanasIglesias, Jirí Damborský, Jirí Sedlár, Stanislav Mazurenko, Josef Sivic |  |
| 1154 |  |  [ElasticTok: Adaptive Tokenization for Image and Video](https://openreview.net/forum?id=tFV5GrWOGm) |  | 0 |  | Wilson Yan, Volodymyr Mnih, Aleksandra Faust, Matei Zaharia, Pieter Abbeel, Hao Liu |  |
| 1155 |  |  [Trajectory-Class-Aware Multi-Agent Reinforcement Learning](https://openreview.net/forum?id=uqe5HkjbT9) |  | 0 |  | Hyungho Na, Kwanghyeon Lee, Sumin Lee, IlChul Moon |  |
| 1156 |  |  [Revisit the Open Nature of Open Vocabulary Semantic Segmentation](https://openreview.net/forum?id=2vHIHrJAcI) |  | 0 |  | Qiming Huang, Han Hu, Jianbo Jiao |  |
| 1157 |  |  [Robust Barycenter Estimation using Semi-Unbalanced Neural Optimal Transport](https://openreview.net/forum?id=CI5Cj0vktS) |  | 0 |  | Milena Gazdieva, Jaemoo Choi, Alexander Kolesov, Jaewoong Choi, Petr Mokrov, Alexander Korotin |  |
| 1158 |  |  [Inverse decision-making using neural amortized Bayesian actors](https://openreview.net/forum?id=zxO4WuVGns) |  | 0 |  | Dominik Straub, Tobias F. Niehues, Jan Peters, Constantin A. Rothkopf |  |
| 1159 |  |  [Reframing Structure-Based Drug Design Model Evaluation via Metrics Correlated to Practical Needs](https://openreview.net/forum?id=RyWypcIMiE) |  | 0 |  | Bowen Gao, Haichuan Tan, Yanwen Huang, Minsi Ren, Xiao Huang, WeiYing Ma, YaQin Zhang, Yanyan Lan |  |
| 1160 |  |  [Valid Conformal Prediction for Dynamic GNNs](https://openreview.net/forum?id=i3T0wvQDKg) |  | 0 |  | Ed Davis, Ian Gallagher, Daniel John Lawson, Patrick RubinDelanchy |  |
| 1161 |  |  [ContextGNN: Beyond Two-Tower Recommendation Systems](https://openreview.net/forum?id=nzOD1we8Z4) |  | 0 |  | Yiwen Yuan, Zecheng Zhang, Xinwei He, Akihiro Nitta, Weihua Hu, Manan Shah, Blaz Stojanovic, Shenyang Huang, Jan Eric Lenssen, Jure Leskovec, Matthias Fey |  |
| 1162 |  |  [KOR-Bench: Benchmarking Language Models on Knowledge-Orthogonal Reasoning Tasks](https://openreview.net/forum?id=SVRRQ8goQo) |  | 0 |  | Kaijing Ma, Xeron Du, Yunran Wang, Haoran Zhang, Zhoufutu Wen, Xingwei Qu, Jian Yang, Jiaheng Liu, Minghao Liu, Xiang Yue, Wenhao Huang, Ge Zhang |  |
| 1163 |  |  [Gaussian Splatting Lucas-Kanade](https://openreview.net/forum?id=dkrEoT68by) |  | 0 |  | Liuyue Xie, Joel Julin, Koichiro Niinuma, László Attila Jeni |  |
| 1164 |  |  [Beyond Content Relevance: Evaluating Instruction Following in Retrieval Models](https://openreview.net/forum?id=OlRjxSuSwl) |  | 0 |  | Jianqun Zhou, Yuanlei Zheng, Wei Chen, Qianqian Zheng, Zeyuan Shang, Wei Zhang, Rui Meng, Xiaoyu Shen |  |
| 1165 |  |  [RobustKV: Defending Large Language Models against Jailbreak Attacks via KV Eviction](https://openreview.net/forum?id=L5godAOC2z) |  | 0 |  | Tanqiu Jiang, Zian Wang, Jiacheng Liang, Changjiang Li, Yuhui Wang, Ting Wang |  |
| 1166 |  |  [From Probability to Counterfactuals: the Increasing Complexity of Satisfiability in Pearl's Causal Hierarchy](https://openreview.net/forum?id=rvvSSmGIFS) |  | 0 |  | Julian Dörfler, Benito van der Zander, Markus Bläser, Maciej Liskiewicz |  |
| 1167 |  |  [High-Dimensional Bayesian Optimisation with Gaussian Process Prior Variational Autoencoders](https://openreview.net/forum?id=SIuD7CySb4) |  | 0 |  | Siddharth Ramchandran, Manuel Haussmann, Harri Lähdesmäki |  |
| 1168 |  |  [LIFe-GoM: Generalizable Human Rendering with Learned Iterative Feedback Over Multi-Resolution Gaussians-on-Mesh](https://openreview.net/forum?id=gY08Ou8EL7) |  | 0 |  | Jing Wen, Alexander G. Schwing, Shenlong Wang |  |
| 1169 |  |  [NNsight and NDIF: Democratizing Access to Open-Weight Foundation Model Internals](https://openreview.net/forum?id=MxbEiFRf39) |  | 0 |  | Jaden Fried FiottoKaufman, Alexander Russell Loftus, Eric Todd, Jannik Brinkmann, Koyena Pal, Dmitrii Troitskii, Michael Ripa, Adam Belfki, Can Rager, Caden Juang, Aaron Mueller, Samuel Marks, Arnab Sen Sharma, Francesca Lucchetti, Nikhil Prakash, Carla E. Brodley, Arjun Guha, Jonathan Bell, Byron C. Wallace, David Bau |  |
| 1170 |  |  [Protecting against simultaneous data poisoning attacks](https://openreview.net/forum?id=rK0YJwL69S) |  | 0 |  | Neel Alex, Shoaib Ahmed Siddiqui, Amartya Sanyal, David Krueger |  |
| 1171 |  |  [Lift Your Molecules: Molecular Graph Generation in Latent Euclidean Space](https://openreview.net/forum?id=uNomADvF3s) |  | 0 |  | Mohamed Amine Ketata, Nicholas Gao, Johanna Sommer, Tom Wollschläger, Stephan Günnemann |  |
| 1172 |  |  [Neural Sampling from Boltzmann Densities: Fisher-Rao Curves in the Wasserstein Geometry](https://openreview.net/forum?id=TUvg5uwdeG) |  | 0 |  | Jannis Chemseddine, Christian Wald, Richard Duong, Gabriele Steidl |  |
| 1173 |  |  [GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation](https://openreview.net/forum?id=hPWWXpCaJ7) |  | 0 |  | Hongyin Zhang, Pengxiang Ding, Shangke Lyu, Ying Peng, Donglin Wang |  |
| 1174 |  |  [Revealing and Mitigating Over-Attention in Knowledge Editing](https://openreview.net/forum?id=4l3AH8Bhmt) |  | 0 |  | Pinzheng Wang, Zecheng Tang, Keyan Zhou, Juntao Li, Qiaoming Zhu, Min Zhang |  |
| 1175 |  |  [A Generalist Hanabi Agent](https://openreview.net/forum?id=pCj2sLNoJq) |  | 0 |  | Arjun Vaithilingam Sudhakar, Hadi Nekoei, Mathieu Reymond, Miao Liu, Janarthanan Rajendran, Sarath Chandar |  |
| 1176 |  |  [GraphEval: A Lightweight Graph-Based LLM Framework for Idea Evaluation](https://openreview.net/forum?id=5RUM1aIdok) |  | 0 |  | Tao Feng, Yihang Sun, Jiaxuan You |  |
| 1177 |  |  [Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying Questions](https://openreview.net/forum?id=cwuSAR7EKd) |  | 0 |  | Michael Jq Zhang, W. Bradley Knox, Eunsol Choi |  |
| 1178 |  |  [Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs](https://openreview.net/forum?id=1ExfUpmIW4) |  | 0 |  | Sungmin Cha, Sungjun Cho, Dasol Hwang, Moontae Lee |  |
| 1179 |  |  [What Makes Large Language Models Reason in (Multi-Turn) Code Generation?](https://openreview.net/forum?id=Zk9guOl9NS) |  | 0 |  | Kunhao Zheng, Juliette Decugis, Jonas Gehring, Taco Cohen, Benjamin NéXuanjinggrevergne, Gabriel Synnaeve |  |
| 1180 |  |  [RMB: Comprehensively benchmarking reward models in LLM alignment](https://openreview.net/forum?id=kmgrlG9TR0) |  | 0 |  | Enyu Zhou, Guodong Zheng, Binghai Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong, Jessica Fan, Yurong Mou, Rui Zheng, Tao Gui, Qi Zhang, Xuanjing Huang |  |
| 1181 |  |  [FedTMOS: Efficient One-Shot Federated Learning with Tsetlin Machine](https://openreview.net/forum?id=44hcrfzydU) |  | 0 |  | Shannon How Shi Qi, Jagmohan Chauhan, Geoff V. Merrett, Jonathon S. Hare |  |
| 1182 |  |  [From Sparse Dependence to Sparse Attention: Unveiling How Chain-of-Thought Enhances Transformer Sample Efficiency](https://openreview.net/forum?id=AmEgWDhmTr) |  | 0 |  | Kaiyue Wen, Huaqing Zhang, Hongzhou Lin, Jingzhao Zhang |  |
| 1183 |  |  [CS-Bench: A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery](https://openreview.net/forum?id=fjEZ2LPceZ) |  | 0 |  | Xiaoshuai Song, Muxi Diao, Guanting Dong, Zhengyang Wang, Yujia Fu, Runqi Qiao, Zhexu Wang, Dayuan Fu, Huangxuan Wu, Bin Liang, Weihao Zeng, Yejie Wang, Zhuoma Gongque, Jianing Yu, Qiuna Tan, Weiran Xu |  |
| 1184 |  |  [A deep inverse-mapping model for a flapping robotic wing](https://openreview.net/forum?id=254NJe9JEw) |  | 0 |  | Hadar Sharvit, Raz Karl, Tsevi Beatus |  |
| 1185 |  |  [Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal Perspective](https://openreview.net/forum?id=NWb128pSCb) |  | 0 |  | Xiangru Zhu, Penglei Sun, Yaoxian Song, Yanghua Xiao, Zhixu Li, Chengyu Wang, Jun Huang, Bei Yang, Xiaoxiao Xu |  |
| 1186 |  |  [GraphRouter: A Graph-based Router for LLM Selections](https://openreview.net/forum?id=eU39PDsZtT) |  | 0 |  | Tao Feng, Yanzhen Shen, Jiaxuan You |  |
| 1187 |  |  [CONDA: Adaptive Concept Bottleneck for Foundation Models Under Distribution Shifts](https://openreview.net/forum?id=8sfc8MwG5v) |  | 0 |  | Jihye Choi, Jayaram Raghuram, Yixuan Li, Somesh Jha |  |
| 1188 |  |  [Offline RL in Regular Decision Processes: Sample Efficiency via Language Metrics](https://openreview.net/forum?id=EW6bNEqalF) |  | 0 |  | Ahana Deb, Roberto Cipollone, Anders Jonsson, Alessandro Ronca, Mohammad Sadegh Talebi |  |
| 1189 |  |  [Think while You Generate: Discrete Diffusion with Planned Denoising](https://openreview.net/forum?id=MJNywBdSDy) |  | 0 |  | Sulin Liu, Juno Nam, Andrew Campbell, Hannes Stärk, Yilun Xu, Tommi S. Jaakkola, Rafael GómezBombarelli |  |
| 1190 |  |  [Complexity Lower Bounds of Adaptive Gradient Algorithms for Non-convex Stochastic Optimization under Relaxed Smoothness](https://openreview.net/forum?id=ZjOXuAfS6l) |  | 0 |  | Michael Crawshaw, Mingrui Liu |  |
| 1191 |  |  [Scaling Transformers for Low-Bitrate High-Quality Speech Coding](https://openreview.net/forum?id=4YpMrGfldX) |  | 0 |  | Julian D. Parker, Anton Smirnov, Jordi Pons, CJ Carr, Zack Zukowski, Zach Evans, Xubo Liu |  |
| 1192 |  |  [Learning Geometric Reasoning Networks For Robot Task And Motion Planning](https://openreview.net/forum?id=ajxAJ8GUX4) |  | 0 |  | Smail Ait Bouhsain, Rachid Alami, Thierry Siméon |  |
| 1193 |  |  [Differentially Private Steering for Large Language Model Alignment](https://openreview.net/forum?id=lLkgj7FEtZ) |  | 0 |  | Anmol Goel, Yaxi Hu, Iryna Gurevych, Amartya Sanyal |  |
| 1194 |  |  [Mixture of Parrots: Experts improve memorization more than reasoning](https://openreview.net/forum?id=9XETcRsufZ) |  | 0 |  | Samy Jelassi, Clara Mohri, David Brandfonbrener, Alex Gu, Nikhil Vyas, Nikhil Anand, David AlvarezMelis, Yuanzhi Li, Sham M. Kakade, Eran Malach |  |
| 1195 |  |  [Efficient Multi-agent Offline Coordination via Diffusion-based Trajectory Stitching](https://openreview.net/forum?id=EpnZEzYDUT) |  | 0 |  | Lei Yuan, Yuqi Bian, Lihe Li, Ziqian Zhang, Cong Guan, Yang Yu |  |
| 1196 |  |  [AdvWave: Stealthy Adversarial Jailbreak Attack against Large Audio-Language Models](https://openreview.net/forum?id=0BujOfTqab) |  | 0 |  | Mintong Kang, Chejian Xu, Bo Li |  |
| 1197 |  |  [Scaling up Masked Diffusion Models on Text](https://openreview.net/forum?id=WNvvwK0tut) |  | 0 |  | Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, Chongxuan Li |  |
| 1198 |  |  [LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations](https://openreview.net/forum?id=KRnsX5Em3W) |  | 0 |  | Hadas Orgad, Michael Toker, Zorik Gekhman, Roi Reichart, Idan Szpektor, Hadas Kotek, Yonatan Belinkov |  |
| 1199 |  |  [Selective Unlearning via Representation Erasure Using Domain Adversarial Training](https://openreview.net/forum?id=KzSGJy1PIf) |  | 0 |  | Nazanin Mohammadi Sepahvand, Eleni Triantafillou, Hugo Larochelle, Doina Precup, James J. Clark, Daniel M. Roy, Gintare Karolina Dziugaite |  |
| 1200 |  |  [Local Steps Speed Up Local GD for Heterogeneous Distributed Logistic Regression](https://openreview.net/forum?id=lydPkW4lfz) |  | 0 |  | Michael Crawshaw, Blake Woodworth, Mingrui Liu |  |
| 1201 |  |  [Robustness Auditing for Linear Regression: To Singularity and Beyond](https://openreview.net/forum?id=V5ns6uvRZ9) |  | 0 |  | Ittai Rubinstein, Samuel B. Hopkins |  |
| 1202 |  |  [Can Transformers Do Enumerative Geometry?](https://openreview.net/forum?id=4X9RpKH4Ls) |  | 0 |  | Baran Hashemi, Roderic Guigo Corominas, Alessandro Giacchetto |  |
| 1203 |  |  [Large Language Models Assume People are More Rational than We Really are](https://openreview.net/forum?id=dAeET8gxqg) |  | 0 |  | Ryan Liu, Jiayi Geng, Joshua C. Peterson, Ilia Sucholutsky, Thomas L. Griffiths |  |
| 1204 |  |  [Small-to-Large Generalization: Training Data Influences Models Consistently Across Scale](https://openreview.net/forum?id=79ZkWgY2FI) |  | 0 |  | Alaa Khaddaj, Logan Engstrom, Aleksander Madry |  |
| 1205 |  |  [Scaling Instruction-tuned LLMs to Million-token Contexts via Hierarchical Synthetic Data Generation](https://openreview.net/forum?id=BkwCrIsTbR) |  | 0 |  | Linda He, Jue Wang, Maurice Weber, Shang Zhu, Ben Athiwaratkun, Ce Zhang |  |
| 1206 |  |  [EqNIO: Subequivariant Neural Inertial Odometry](https://openreview.net/forum?id=C8jXEugWkq) |  | 0 |  | Royina Karegoudra Jayanth, Yinshuang Xu, Ziyun Wang, Evangelos Chatzipantazis, Kostas Daniilidis, Daniel Gehrig |  |
| 1207 |  |  [MANTRA: The Manifold Triangulations Assemblage](https://openreview.net/forum?id=X6y5CC44HM) |  | 0 |  | Rubén Ballester, Ernst Röell, Daniel Bin Schmid, Mathieu Alain, Sergio Escalera, Carles Casacuberta, Bastian Rieck |  |
| 1208 |  |  [Aligning Visual Contrastive learning models via Preference Optimization](https://openreview.net/forum?id=wgRQ2WAORJ) |  | 0 |  | Amirabbas Afzali, Borna Khodabandeh, Ali Rasekh, Mahyar JafariNodeh, Sepehr Kazemi Ranjbar, Simon Gottschalk |  |
| 1209 |  |  [Exponential Topology-enabled Scalable Communication in Multi-agent Reinforcement Learning](https://openreview.net/forum?id=CL3U0GxFRD) |  | 0 |  | Xinran Li, Xiaolu Wang, Chenjia Bai, Jun Zhang |  |
| 1210 |  |  [Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages](https://openreview.net/forum?id=a3g2l4yEys) |  | 0 |  | Xiang Yue, Yueqi Song, Akari Asai, Seungone Kim, Jean de Dieu Nyandwi, Simran Khanuja, Anjali Kantharuban, Lintang Sutawika, Sathyanarayanan Ramamoorthy, Graham Neubig |  |
| 1211 |  |  [Brain-inspired Lp-Convolution benefits large kernels and aligns better with visual cortex](https://openreview.net/forum?id=0LSAmFCc4p) |  | 0 |  | Jea Kwon, Sungjun Lim, Kyungwoo Song, C. Justin Lee |  |
| 1212 |  |  [Deconstructing What Makes a Good Optimizer for Autoregressive Language Models](https://openreview.net/forum?id=zfeso8ceqr) |  | 0 |  | Rosie Zhao, Depen Morwani, David Brandfonbrener, Nikhil Vyas, Sham M. Kakade |  |
| 1213 |  |  [dEBORA: Efficient Bilevel Optimization-based low-Rank Adaptation](https://openreview.net/forum?id=5M0ic2RxQZ) |  | 0 |  | Emanuele Zangrando, Sara Venturini, Francesco Rinaldi, Francesco Tudisco |  |
| 1214 |  |  [Interaction Asymmetry: A General Principle for Learning Composable Abstractions](https://openreview.net/forum?id=cCl10IU836) |  | 0 |  | Jack Brady, Julius von Kügelgen, Sébastien Lachapelle, Simon Buchholz, Thomas Kipf, Wieland Brendel |  |
| 1215 |  |  [Decoupling Angles and Strength in Low-rank Adaptation](https://openreview.net/forum?id=X1U74IwuxG) |  | 0 |  | Massimo Bini, Leander Girrbach, Zeynep Akata |  |
| 1216 |  |  [AutoG: Towards automatic graph construction from tabular data](https://openreview.net/forum?id=hovDbX4Gh6) |  | 0 |  | Zhikai Chen, Han Xie, Jian Zhang, Xiang Song, Jiliang Tang, Huzefa Rangwala, George Karypis |  |
| 1217 |  |  [How Learnable Grids Recover Fine Detail in Low Dimensions: A Neural Tangent Kernel Analysis of Multigrid Parametric Encodings](https://openreview.net/forum?id=Ge7okBGZYi) |  | 0 |  | Samuel Audia, Soheil Feizi, Matthias Zwicker, Dinesh Manocha |  |
| 1218 |  |  [AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents](https://openreview.net/forum?id=il5yUQsrjC) |  | 0 |  | Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William E. Bishop, Wei Li, Folawiyo CampbellAjala, Daniel Kenji Toyama, Robert James Berry, Divya Tyamagundlu, Timothy P. Lillicrap, Oriana Riva |  |
| 1219 |  |  [Safety Representations for Safer Policy Learning](https://openreview.net/forum?id=gJG4IPwg6l) |  | 0 |  | Kaustubh Mani, Vincent Mai, Charlie Gauthier, Annie S. Chen, Samer B. Nashed, Liam Paull |  |
| 1220 |  |  [Combining Induction and Transduction for Abstract Reasoning](https://openreview.net/forum?id=UmdotAAVDe) |  | 0 |  | WenDing Li, Keya Hu, Carter Larsen, Yuqing Wu, Simon Alford, Caleb Woo, Spencer M. Dunn, Hao Tang, WeiLong Zheng, Yewen Pu, Kevin Ellis |  |
| 1221 |  |  [CViT: Continuous Vision Transformer for Operator Learning](https://openreview.net/forum?id=cRnCcuLvyr) |  | 0 |  | Sifan Wang, Jacob H. Seidman, Shyam Sankaran, Hanwen Wang, George J. Pappas, Paris Perdikaris |  |
| 1222 |  |  [ReCogLab: a framework testing relational reasoning & cognitive hypotheses on LLMs](https://openreview.net/forum?id=yORSk4Ycsa) |  | 0 |  | Andrew Liu, Henry Prior, Gargi Balasubramaniam, Rivka Moroshko, Amir Zait, Ilia Labzovsky, Danny Karmon, Ishita Dasgupta, Kim Stachenfeld, Kenneth Marino |  |
| 1223 |  |  [MGCFNN: A Neural MultiGrid Solver with Novel Fourier Neural Network for High Wave Number Helmholtz Equations](https://openreview.net/forum?id=ThhQyIruEs) |  | 0 |  | Yan Xie, Minrui Lv, Chensong Zhang |  |
| 1224 |  |  [CirT: Global Subseasonal-to-Seasonal Forecasting with Geometry-inspired Transformer](https://openreview.net/forum?id=YslOW2SO6S) |  | 0 |  | Yang Liu, Zinan Zheng, Jiashun Cheng, Fugee Tsung, Deli Zhao, Yu Rong, Jia Li |  |
| 1225 |  |  [Accelerating neural network training: An analysis of the AlgoPerf competition](https://openreview.net/forum?id=CtM5xjRSfm) |  | 0 |  | Priya Kasimbeg, Frank Schneider, Runa Eschenhagen, Juhan Bae, Chandramouli Shama Sastry, Mark Saroufim, Boyuan Feng, Less Wright, Edward Z. Yang, Zachary Nado, Sourabh Medapati, Philipp Hennig, Michael Rabbat, George E. Dahl |  |
| 1226 |  |  [Injecting Universal Jailbreak Backdoors into LLMs in Minutes](https://openreview.net/forum?id=aSy2nYwiZ2) |  | 0 |  | Zhuowei Chen, Qiannan Zhang, Shichao Pei |  |
| 1227 |  |  [Action abstractions for amortized sampling](https://openreview.net/forum?id=ispjankYab) |  | 0 |  | Oussama Boussif, Léna Néhale Ezzine, Joseph D. Viviano, Michal Koziarski, Moksh Jain, Nikolay Malkin, Emmanuel Bengio, Rim Assouel, Yoshua Bengio |  |
| 1228 |  |  [Provence: efficient and robust context pruning for retrieval-augmented generation](https://openreview.net/forum?id=TDy5Ih78b4) |  | 0 |  | Nadezhda Chirkova, Thibault Formal, Vassilina Nikoulina, Stéphane Clinchant |  |
| 1229 |  |  [Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning](https://openreview.net/forum?id=UyU8ETswPg) |  | 0 |  | Yujian Liu, Shiyu Chang, Tommi S. Jaakkola, Yang Zhang |  |
| 1230 |  |  [Second Order Bounds for Contextual Bandits with Function Approximation](https://openreview.net/forum?id=h6ktwCPYxE) |  | 0 |  | Aldo Pacchiano |  |
| 1231 |  |  [Unveiling the Magic of Code Reasoning through Hypothesis Decomposition and Amendment](https://openreview.net/forum?id=kN25ggeq1J) |  | 0 |  | Yuze Zhao, Tianyun Ji, Wenjun Feng, Zhenya Huang, Qi Liu, Zhiding Liu, Yixiao Ma, Kai Zhang, Enhong Chen |  |
| 1232 |  |  [Mitigating Parameter Interference in Model Merging via Sharpness-Aware Fine-Tuning](https://openreview.net/forum?id=eaTqsptDPL) |  | 0 |  | Yeoreum Lee, Jinwook Jung, Sungyong Baik |  |
| 1233 |  |  [Flow: Modularized Agentic Workflow Automation](https://openreview.net/forum?id=sLKDbuyq99) |  | 0 |  | Boye Niu, Yiliao Song, Kai Lian, Yifan Shen, Yu Yao, Kun Zhang, Tongliang Liu |  |
| 1234 |  |  [S4M: S4 for multivariate time series forecasting with Missing values](https://openreview.net/forum?id=BkftcwIVmR) |  | 0 |  | Peng Jing, Meiqi Yang, Qiong Zhang, Xiaoxiao Li |  |
| 1235 |  |  [An Exploration with Entropy Constrained 3D Gaussians for 2D Video Compression](https://openreview.net/forum?id=JbRM5QKRDd) |  | 0 |  | Xiang Liu, Bin Chen, Zimo Liu, Yaowei Wang, ShuTao Xia |  |
| 1236 |  |  [ILLUSION: Unveiling Truth with a Comprehensive Multi-Modal, Multi-Lingual Deepfake Dataset](https://openreview.net/forum?id=qnlG3zPQUy) |  | 0 |  | Kartik Thakral, Rishabh Ranjan, Akanksha Singh, Akshat Jain, Mayank Vatsa, Richa Singh |  |
| 1237 |  |  [Gramian Multimodal Representation Learning and Alignment](https://openreview.net/forum?id=ftGnpZrW7P) |  | 0 |  | Giordano Cicchetti, Eleonora Grassucci, Luigi Sigillo, Danilo Comminiello |  |
| 1238 |  |  [Going Beyond Static: Understanding Shifts with Time-Series Attribution](https://openreview.net/forum?id=XQlccqJpCC) |  | 0 |  | Jiashuo Liu, Nabeel Seedat, Peng Cui, Mihaela van der Schaar |  |
| 1239 |  |  [Video In-context Learning: Autoregressive Transformers are Zero-Shot Video Imitators](https://openreview.net/forum?id=wkbx7BRAsM) |  | 0 |  | Wentao Zhang, Junliang Guo, Tianyu He, Li Zhao, Linli Xu, Jiang Bian |  |
| 1240 |  |  [HR-Extreme: A High-Resolution Dataset for Extreme Weather Forecasting](https://openreview.net/forum?id=5AtlfHYCPa) |  | 0 |  | Nian Ran, Peng Xiao, Yue Wang, Wesley Shi, Jianxin Lin, Qi Meng, Richard Allmendinger |  |
| 1241 |  |  [MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily Complex Proofs](https://openreview.net/forum?id=5ck9PIrTpH) |  | 0 |  | Andreas Opedal, Haruki Shirakami, Bernhard Schölkopf, Abulhair Saparov, Mrinmaya Sachan |  |
| 1242 |  |  [Feature-Based Online Bilateral Trade](https://openreview.net/forum?id=xnF2U0ro7b) |  | 0 |  | Solenne Gaucher, Martino Bernasconi, Matteo Castiglioni, Andrea Celli, Vianney Perchet |  |
| 1243 |  |  [Interpretable Causal Representation Learning for Biological Data in the Pathway Space](https://openreview.net/forum?id=3Fgylj4uqL) |  | 0 |  | Jesus de la Fuente Cedeño, Robert Lehmann, Carlos RuizArenas, Jan Voges, Irene MarínGoñi, Xabier Martinez de Morentin, David GomezCabrero, Idoia Ochoa, Jesper Tegnér, Vincenzo Lagani, Mikel Hernaez |  |
| 1244 |  |  [Separation Power of Equivariant Neural Networks](https://openreview.net/forum?id=RAyRXQjsFl) |  | 0 |  | Marco Pacini, Xiaowen Dong, Bruno Lepri, Gabriele Santin |  |
| 1245 |  |  [HASARD: A Benchmark for Vision-Based Safe Reinforcement Learning in Embodied Agents](https://openreview.net/forum?id=5BRFddsAai) |  | 0 |  | Tristan Tomilin, Meng Fang, Mykola Pechenizkiy |  |
| 1246 |  |  [Associative memory and dead neurons](https://openreview.net/forum?id=mkNVPGpEPm) |  | 0 |  | Vladimir Fanaskov, Ivan V. Oseledets |  |
| 1247 |  |  [The OMG dataset: An Open MetaGenomic corpus for mixed-modality genomic language modeling](https://openreview.net/forum?id=jlzNb1iWs3) |  | 0 |  | Andre Cornman, Jacob WestRoberts, Antonio Pedro Camargo, Simon Roux, Martin Beracochea, Milot Mirdita, Sergey Ovchinnikov, Yunha Hwang |  |
| 1248 |  |  [Physics-informed Temporal Difference Metric Learning for Robot Motion Planning](https://openreview.net/forum?id=TOiageVNru) |  | 0 |  | Ruiqi Ni, Zherong Pan, Ahmed H. Qureshi |  |
| 1249 |  |  [Towards Scalable Topological Regularizers](https://openreview.net/forum?id=FjZcwQJX8D) |  | 0 |  | HiuTung Wong, Darrick Lee, Hong Yan |  |
| 1250 |  |  [KaSA: Knowledge-Aware Singular-Value Adaptation of Large Language Models](https://openreview.net/forum?id=OQqNieeivq) |  | 0 |  | Fan Wang, Juyong Jiang, Chansung Park, Sunghun Kim, Jing Tang |  |
| 1251 |  |  [The Foundations of Tokenization: Statistical and Computational Concerns](https://openreview.net/forum?id=B5iOSxM2I0) |  | 0 |  | Juan Luis Gastaldi, John Terilla, Luca Malagutti, Brian DuSell, Tim Vieira, Ryan Cotterell |  |
| 1252 |  |  [Can Video LLMs Refuse to Answer? Alignment for Answerability in Video Large Language Models](https://openreview.net/forum?id=P9VdRQOyqu) |  | 0 |  | Eunseop Yoon, Hee Suk Yoon, Mark A. HasegawaJohnson, Chang D. Yoo |  |
| 1253 |  |  [Intrinsic Dimension Correlation: uncovering nonlinear connections in multimodal representations](https://openreview.net/forum?id=Qj1KwBZaEI) |  | 0 |  | Lorenzo Basile, Santiago Acevedo, Luca Bortolussi, Fabio Anselmi, Alex Rodriguez |  |
| 1254 |  |  [Regret-Optimal List Replicable Bandit Learning: Matching Upper and Lower Bounds](https://openreview.net/forum?id=0T49QbSOho) |  | 0 |  | Michael Chen, Aduri Pavan, N. V. Vinodchandran, Ruosong Wang, Lin Yang |  |
| 1255 |  |  [Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models](https://openreview.net/forum?id=odvSjn416y) |  | 0 |  | Orion Weller, Benjamin Van Durme, Dawn J. Lawrie, Ashwin Paranjape, Yuhao Zhang, Jack Hessel |  |
| 1256 |  |  [Everything, Everywhere, All at Once: Is Mechanistic Interpretability Identifiable?](https://openreview.net/forum?id=5IWJBStfU7) |  | 0 |  | Maxime Méloux, Silviu Maniu, François Portet, Maxime Peyrard |  |
| 1257 |  |  [Language Models Need Inductive Biases to Count Inductively](https://openreview.net/forum?id=s3IBHTTDYl) |  | 0 |  | Yingshan Chang, Yonatan Bisk |  |
| 1258 |  |  [General Scene Adaptation for Vision-and-Language Navigation](https://openreview.net/forum?id=2oKkQTyfz7) |  | 0 |  | Haodong Hong, Yanyuan Qiao, Sen Wang, Jiajun Liu, Qi Wu |  |
| 1259 |  |  [Bridging Context Gaps: Leveraging Coreference Resolution for Long Contextual Understanding](https://openreview.net/forum?id=cPozlf9OaF) |  | 0 |  | Yanming Liu, Xinyue Peng, Jiannan Cao, Shi Bo, Yanxin Shen, Tianyu Du, Sheng Cheng, Xun Wang, Jianwei Yin, Xuhong Zhang |  |
| 1260 |  |  [Scalable Mechanistic Neural Networks](https://openreview.net/forum?id=Oazgf8A24z) |  | 0 |  | Jiale Chen, Dingling Yao, Adeel Pervez, Dan Alistarh, Francesco Locatello |  |
| 1261 |  |  [Linear Partial Gromov-Wasserstein Embedding](https://openreview.net/forum?id=BA1eG7vCNb) |  | 0 |  | Yikun Bai, Abihith Kothapalli, Hengrong Du, Rocio Diaz Martin, Soheil Kolouri |  |
| 1262 |  |  [As Simple as Fine-tuning: LLM Alignment via Bidirectional Negative Feedback Loss](https://openreview.net/forum?id=fsX9nFwMNj) |  | 0 |  | Xin Mao, Huimin Xu, FengLin Li, Ziqi Jin, Wang Chen, Wei Zhang, Anh Tuan Luu |  |
| 1263 |  |  [E(3)-equivariant models cannot learn chirality: Field-based molecular generation](https://openreview.net/forum?id=mXHTifc1Fn) |  | 0 |  | Alexandru Dumitrescu, Dani Korpela, Markus Heinonen, Yogesh Verma, Valerii Iakovlev, Vikas Garg, Harri Lähdesmäki |  |
| 1264 |  |  [One Model Transfer to All: On Robust Jailbreak Prompts Generation against LLMs](https://openreview.net/forum?id=sULAwlAWc1) |  | 0 |  | Linbao Li, Yannan Liu, Daojing He, Yu Li |  |
| 1265 |  |  [PaPaGei: Open Foundation Models for Optical Physiological Signals](https://openreview.net/forum?id=kYwTmlq6Vn) |  | 0 |  | Arvind Pillai, Dimitris Spathis, Fahim Kawsar, Mohammad Malekzadeh |  |
| 1266 |  |  [Sensitivity-Constrained Fourier Neural Operators for Forward and Inverse Problems in Parametric Differential Equations](https://openreview.net/forum?id=DPzQ5n3mNm) |  | 0 |  | Abdolmehdi Behroozi, Chaopeng Shen, Daniel Kifer |  |
| 1267 |  |  [Captured by Captions: On Memorization and its Mitigation in CLIP Models](https://openreview.net/forum?id=5V0f8igznO) |  | 0 |  | Wenhao Wang, Adam Dziedzic, Grace C. Kim, Michael Backes, Franziska Boenisch |  |
| 1268 |  |  [CHASE-SQL: Multi-Path Reasoning and Preference Optimized Candidate Selection in Text-to-SQL](https://openreview.net/forum?id=CvGqMD5OtX) |  | 0 |  | Mohammadreza Pourreza, Hailong Li, Ruoxi Sun, Yeounoh Chung, Shayan Talaei, Gaurav Tarlok Kakkar, Yu Gan, Amin Saberi, Fatma Ozcan, Sercan Ö. Arik |  |
| 1269 |  |  [JPEG Inspired Deep Learning](https://openreview.net/forum?id=te2IdORabL) |  | 0 |  | Ahmed H. Salamah, Kaixiang Zheng, Yiwen Liu, EnHui Yang |  |
| 1270 |  |  [MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents](https://openreview.net/forum?id=K5yeB4dTtS) |  | 0 |  | Junpeng Yue, Xinrun Xu, Börje F. Karlsson, Zongqing Lu |  |
| 1271 |  |  [Divergence-Regularized Discounted Aggregation: Equilibrium Finding in Multiplayer Partially Observable Stochastic Games](https://openreview.net/forum?id=KD5nJUgeW4) |  | 0 |  | Runyu Lu, Yuanheng Zhu, Dongbin Zhao |  |
| 1272 |  |  [The Case for Cleaner Biosignals: High-fidelity Neural Compressor Enables Transfer from Cleaner iEEG to Noisier EEG](https://openreview.net/forum?id=b57IG6N20B) |  | 0 |  | Francesco S. Carzaniga, Gary Tom Hoppeler, Michael Hersche, Kaspar Schindler, Abbas Rahimi |  |
| 1273 |  |  [Preference Diffusion for Recommendation](https://openreview.net/forum?id=6GATHdOi1x) |  | 0 |  | Shuo Liu, An Zhang, Guoqing Hu, Hong Qian, TatSeng Chua |  |
| 1274 |  |  [Rethinking Evaluation of Sparse Autoencoders through the Representation of Polysemous Words](https://openreview.net/forum?id=HpUs2EXjOl) |  | 0 |  | Gouki Minegishi, Hiroki Furuta, Yusuke Iwasawa, Yutaka Matsuo |  |
| 1275 |  |  [Tool-Planner: Task Planning with Clusters across Multiple Tools](https://openreview.net/forum?id=dRz3cizftU) |  | 0 |  | Yanming Liu, Xinyue Peng, Jiannan Cao, Shi Bo, Yuwei Zhang, Xuhong Zhang, Sheng Cheng, Xun Wang, Jianwei Yin, Tianyu Du |  |
| 1276 |  |  [On a Connection Between Imitation Learning and RLHF](https://openreview.net/forum?id=2QdsjiNXgj) |  | 0 |  | Teng Xiao, Yige Yuan, Mingxiao Li, Zhengyu Chen, Vasant G. Honavar |  |
| 1277 |  |  [GIFT: Unlocking Full Potential of Labels in Distilled Dataset at Near-zero Cost](https://openreview.net/forum?id=FoF5RaA3ug) |  | 0 |  | Xinyi Shang, Peng Sun, Tao Lin |  |
| 1278 |  |  [Beyond Mere Token Analysis: A Hypergraph Metric Space Framework for Defending Against Socially Engineered LLM Attacks](https://openreview.net/forum?id=rnJxelIZrq) |  | 0 |  | Manohar Kaul, Aditya Saibewar, Sadbhavana Babar |  |
| 1279 |  |  [Controlling Space and Time with Diffusion Models](https://openreview.net/forum?id=d2UrCGtntF) |  | 0 |  | Daniel Watson, Saurabh Saxena, Lala Li, Andrea Tagliasacchi, David J. Fleet |  |
| 1280 |  |  [Geometry Image Diffusion: Fast and Data-Efficient Text-to-3D with Image-Based Surface Representation](https://openreview.net/forum?id=Glm7Kj47nN) |  | 0 |  | Slava Elizarov, Ciara Rowles, Simon Donné |  |
| 1281 |  |  [Persistent Pre-training Poisoning of LLMs](https://openreview.net/forum?id=eiqrnVaeIw) |  | 0 |  | Yiming Zhang, Javier Rando, Ivan Evtimov, Jianfeng Chi, Eric Michael Smith, Nicholas Carlini, Florian Tramèr, Daphne Ippolito |  |
| 1282 |  |  [CipherPrune: Efficient and Scalable Private Transformer Inference](https://openreview.net/forum?id=mUMvr33FTu) |  | 0 |  | Yancheng Zhang, Jiaqi Xue, Mengxin Zheng, Mimi Xie, Mingzhe Zhang, Lei Jiang, Qian Lou |  |
| 1283 |  |  [Approximating Full Conformal Prediction for Neural Network Regression with Gauss-Newton Influence](https://openreview.net/forum?id=vcX0k4rGTt) |  | 0 |  | Dharmesh Tailor, Alvaro H. C. Correia, Eric T. Nalisnick, Christos Louizos |  |
| 1284 |  |  [Adversarial Policy Optimization for Offline Preference-based Reinforcement Learning](https://openreview.net/forum?id=5Y9NT6lW21) |  | 0 |  | Hyungkyu Kang, Minhwan Oh |  |
| 1285 |  |  [Optimal Strong Regret and Violation in Constrained MDPs via Policy Optimization](https://openreview.net/forum?id=8eNLKk5by4) |  | 0 |  | Francesco Emanuele Stradi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti |  |
| 1286 |  |  [Endowing Visual Reprogramming with Adversarial Robustness](https://openreview.net/forum?id=OuLgaHEmzi) |  | 0 |  | Shengjie Zhou, Xin Cheng, Haiyang Xu, Ming Yan, Tao Xiang, Feng Liu, Lei Feng |  |
| 1287 |  |  [Systematic Relational Reasoning With Epistemic Graph Neural Networks](https://openreview.net/forum?id=qNp86ByQlN) |  | 0 |  | Irtaza Khalid, Steven Schockaert |  |
| 1288 |  |  [RetroInText: A Multimodal Large Language Model Enhanced Framework for Retrosynthetic Planning via In-Context Representation Learning](https://openreview.net/forum?id=J6e4hurEKd) |  | 0 |  | Chenglong Kang, Xiaoyi Liu, Fei Guo |  |
| 1289 |  |  [XAIguiFormer: explainable artificial intelligence guided transformer for brain disorder identification](https://openreview.net/forum?id=AD5yx2xq8R) |  | 0 |  | Hanning Guo, Farah Abdellatif, Yu Fu, N. Jon Shah, Abigail Morrison, Jürgen Dammers |  |
| 1290 |  |  [Simple, Good, Fast: Self-Supervised World Models Free of Baggage](https://openreview.net/forum?id=yFGR36PLDJ) |  | 0 |  | Jan Robine, Marc Höftmann, Stefan Harmeling |  |
| 1291 |  |  [A new framework for evaluating model out-of-distribution generalisation for the biochemical domain](https://openreview.net/forum?id=qFZnAC4GHR) |  | 0 |  | Raúl FernandezDiaz, Hoang Thanh Lam, Vanessa López, Denis C. Shields |  |
| 1292 |  |  [Long-Short Decision Transformer: Bridging Global and Local Dependencies for Generalized Decision-Making](https://openreview.net/forum?id=NHMuM84tRT) |  | 0 |  | Jincheng Wang, Penny Karanasou, Pengyuan Wei, Elia Gatti, Diego Martínez Plasencia, Dimitrios Kanoulas |  |
| 1293 |  |  [Audio Large Language Models Can Be Descriptive Speech Quality Evaluators](https://openreview.net/forum?id=U42TkrEDzb) |  | 0 |  | Chen Chen, Yuchen Hu, Siyin Wang, Helin Wang, Zhehuai Chen, Chao Zhang, ChaoHan Huck Yang, Eng Siong Chng |  |
| 1294 |  |  [Learn Your Reference Model for Real Good Alignment](https://openreview.net/forum?id=H0qIWXXLUR) |  | 0 |  | Alexey Gorbatovski, Boris Shaposhnikov, Alexey Malakhov, Nikita Surnachev, Yaroslav Aksenov, Ian Maksimov, Nikita Balagansky, Daniil Gavrilov |  |
| 1295 |  |  [Noise-conditioned Energy-based Annealed Rewards (NEAR): A Generative Framework for Imitation Learning from Observation](https://openreview.net/forum?id=DL9txImSzm) |  | 0 |  | Anish Abhijit Diwan, Julen Urain, Jens Kober, Jan Peters |  |
| 1296 |  |  [EcoFace: Audio-Visual Emotional Co-Disentanglement Speech-Driven 3D Talking Face Generation](https://openreview.net/forum?id=iDcWYtYUwX) |  | 0 |  | Jiajian Xie, Shengyu Zhang, Mengze Li, Chengfei Lv, Zhou Zhao, Fei Wu |  |
| 1297 |  |  [Elliptic Loss Regularization](https://openreview.net/forum?id=YwzxpZW3p7) |  | 0 |  | Ali Hasan, Haoming Yang, Yuting Ng, Vahid Tarokh |  |
| 1298 |  |  [Optimizing Backward Policies in GFlowNets via Trajectory Likelihood Maximization](https://openreview.net/forum?id=Xj66fkrlTk) |  | 0 |  | Timofei Gritsaev, Nikita Morozov, Sergey Samsonov, Daniil Tiapkin |  |
| 1299 |  |  [Improving Reasoning Performance in Large Language Models via Representation Engineering](https://openreview.net/forum?id=IssPhpUsKt) |  | 0 |  | Bertram Højer, Oliver Simon Jarvis, Stefan Heinrich |  |
| 1300 |  |  [Improving Uncertainty Estimation through Semantically Diverse Language Generation](https://openreview.net/forum?id=HSi4VetQLj) |  | 0 |  | Lukas Aichberger, Kajetan Schweighofer, Mykyta Ielanskyi, Sepp Hochreiter |  |
| 1301 |  |  [Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization](https://openreview.net/forum?id=CbfsKHiWEn) |  | 0 |  | Junkang Wu, Yuexiang Xie, Zhengyi Yang, Jiancan Wu, Jiawei Chen, Jinyang Gao, Bolin Ding, Xiang Wang, Xiangnan He |  |
| 1302 |  |  [PFGuard: A Generative Framework with Privacy and Fairness Safeguards](https://openreview.net/forum?id=8rbkePAapb) |  | 0 |  | Soyeon Kim, Yuji Roh, Geon Heo, Steven Euijong Whang |  |
| 1303 |  |  [SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models](https://openreview.net/forum?id=9chRqsPOGL) |  | 0 |  | Jiale Cheng, Xiao Liu, Cunxiang Wang, Xiaotao Gu, Yida Lu, Dan Zhang, Yuxiao Dong, Jie Tang, Hongning Wang, Minlie Huang |  |
| 1304 |  |  [EMMA: Empowering Multi-modal Mamba with Structural and Hierarchical Alignment](https://openreview.net/forum?id=Ev4iw23gdI) |  | 0 |  | Yifei Xing, Xiangyuan Lan, Ruiping Wang, Dongmei Jiang, Wenjun Huang, Qingfang Zheng, Yaowei Wang |  |
| 1305 |  |  [RAPID: Retrieval Augmented Training of Differentially Private Diffusion Models](https://openreview.net/forum?id=txZVQRc2ab) |  | 0 |  | Tanqiu Jiang, Changjiang Li, Fenglong Ma, Ting Wang |  |
| 1306 |  |  [Exact Computation of Any-Order Shapley Interactions for Graph Neural Networks](https://openreview.net/forum?id=9tKC0YM8sX) |  | 0 |  | Maximilian Muschalik, Fabian Fumagalli, Paolo Frazzetto, Janine Strotherm, Luca Hermes, Alessandro Sperduti, Eyke Hüllermeier, Barbara Hammer |  |
| 1307 |  |  [Physiome-ODE: A Benchmark for Irregularly Sampled Multivariate Time-Series Forecasting Based on Biological ODEs](https://openreview.net/forum?id=6ouZaBzeNO) |  | 0 |  | Christian Klötergens, Vijaya Krishna Yalavarthi, Randolf Scholz, Maximilian Stubbemann, Stefan Born, Lars SchmidtThieme |  |
| 1308 |  |  [Neuron-based Multifractal Analysis of Neuron Interaction Dynamics in Large Models](https://openreview.net/forum?id=nt8gBX58Kh) |  | 0 |  | Xiongye Xiao, Heng Ping, Chenyu Zhou, Defu Cao, Yaxing Li, Yizhuo Zhou, Shixuan Li, Nikos Kanakaris, Paul Bogdan |  |
| 1309 |  |  [Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts](https://openreview.net/forum?id=JSB171dSUU) |  | 0 |  | Guorui Zheng, Xidong Wang, Juhao Liang, Nuo Chen, Yuping Zheng, Benyou Wang |  |
| 1310 |  |  [Multi-Resolution Decomposable Diffusion Model for Non-Stationary Time Series Anomaly Detection](https://openreview.net/forum?id=eWocmTQn7H) |  | 0 |  | Guojin Zhong, Pan Wang, Jin Yuan, Zhiyong Li, Long Chen |  |
| 1311 |  |  [Gumbel Counterfactual Generation From Language Models](https://openreview.net/forum?id=TUC0ZT2zIQ) |  | 0 |  | Shauli Ravfogel, Anej Svete, Vésteinn Snæbjarnarson, Ryan Cotterell |  |
| 1312 |  |  [Decoupled Subgraph Federated Learning](https://openreview.net/forum?id=v1rFkElnIn) |  | 0 |  | Javad Aliakbari, Johan Östman, Alexandre Graell i Amat |  |
| 1313 |  |  [Decoupled Finetuning for Domain Generalizable Semantic Segmentation](https://openreview.net/forum?id=qZEdmyqCHF) |  | 0 |  | Jaehyun Pahk, Donghyeon Kwon, Seong Joon Oh, Suha Kwak |  |
| 1314 |  |  [Magnetic Preference Optimization: Achieving Last-iterate Convergence for Language Model Alignment](https://openreview.net/forum?id=PDnEDS244P) |  | 0 |  | Mingzhi Wang, Chengdong Ma, Qizhi Chen, Linjian Meng, Yang Han, Jiancong Xiao, Zhaowei Zhang, Jing Huo, Weijie J. Su, Yaodong Yang |  |
| 1315 |  |  [Cached Multi-Lora Composition for Multi-Concept Image Generation](https://openreview.net/forum?id=4iFSBgxvIO) |  | 0 |  | Xiandong Zou, Mingzhu Shen, ChristosSavvas Bouganis, Yiren Zhao |  |
| 1316 |  |  [The Effectiveness of Curvature-Based Rewiring and the Role of Hyperparameters in GNNs Revisited](https://openreview.net/forum?id=EcrdmRT99M) |  | 0 |  | Floriano Tori, Vincent Holst, Vincent Ginis |  |
| 1317 |  |  [Utilitarian Algorithm Configuration for Infinite Parameter Spaces](https://openreview.net/forum?id=CA06Nqa7CG) |  | 0 |  | Devon R. Graham, Kevin LeytonBrown |  |
| 1318 |  |  [Triples as the Key: Structuring Makes Decomposition and Verification Easier in LLM-based TableQA](https://openreview.net/forum?id=UwcZEoNP19) |  | 0 |  | Zhen Yang, Ziwei Du, Minghan Zhang, Wei Du, Jie Chen, Zhen Duan, Shu Zhao |  |
| 1319 |  |  [Words in Motion: Extracting Interpretable Control Vectors for Motion Transformers](https://openreview.net/forum?id=J9eKm7j6KD) |  | 0 |  | Ömer Sahin Tas, Royden Wagner |  |
| 1320 |  |  [ProtPainter: Draw or Drag Protein via Topology-guided Diffusion](https://openreview.net/forum?id=Nq7yKYL0Bp) |  | 0 |  | Zhengxi Lu, Shizhuo Cheng, Tintin Jiang, Yan Zhang, Min Zhang |  |
| 1321 |  |  [KinPFN: Bayesian Approximation of RNA Folding Kinetics using Prior-Data Fitted Networks](https://openreview.net/forum?id=E1m5yGMOiV) |  | 0 |  | Dominik Scheuer, Frederic Runge, Jörg K. H. Franke, Michael T. Wolfinger, Christoph Flamm, Frank Hutter |  |
| 1322 |  |  [Sketch2Diagram: Generating Vector Diagrams from Hand-Drawn Sketches](https://openreview.net/forum?id=KvaDHPhhir) |  | 0 |  | Itsumi Saito, Haruto Yoshida, Keisuke Sakaguchi |  |
| 1323 |  |  [Agents' Room: Narrative Generation through Multi-step Collaboration](https://openreview.net/forum?id=HfWcFs7XLR) |  | 0 |  | Fantine Huot, Reinald Kim Amplayo, Jennimaria Palomaki, Alice Shoshana Jakobovits, Elizabeth Clark, Mirella Lapata |  |
| 1324 |  |  [How Does Vision-Language Adaptation Impact the Safety of Vision Language Models?](https://openreview.net/forum?id=eXB5TCrAu9) |  | 0 |  | Seongyun Lee, Geewook Kim, Jiyeon Kim, Hyunji Lee, Hoyeon Chang, Sue Hyun Park, Minjoon Seo |  |
| 1325 |  |  [CR-CTC: Consistency regularization on CTC for improved speech recognition](https://openreview.net/forum?id=CIs9x2ZRgh) |  | 0 |  | Zengwei Yao, Wei Kang, Xiaoyu Yang, Fangjun Kuang, Liyong Guo, Han Zhu, Zengrui Jin, Zhaoqing Li, Long Lin, Daniel Povey |  |
| 1326 |  |  [ChroKnowledge: Unveiling Chronological Knowledge of Language Models in Multiple Domains](https://openreview.net/forum?id=whaO3482bs) |  | 0 |  | Yein Park, Chanwoong Yoon, Jungwoo Park, Donghyeon Lee, Minbyul Jeong, Jaewoo Kang |  |
| 1327 |  |  [Equivariant Denoisers Cannot Copy Graphs: Align Your Graph Diffusion Models](https://openreview.net/forum?id=onIro14tHv) |  | 0 |  | Najwa Laabid, Severi Rissanen, Markus Heinonen, Arno Solin, Vikas Garg |  |
| 1328 |  |  [Influence-Guided Diffusion for Dataset Distillation](https://openreview.net/forum?id=0whx8MhysK) |  | 0 |  | Mingyang Chen, Jiawei Du, Bo Huang, Yi Wang, Xiaobo Zhang, Wei Wang |  |
| 1329 |  |  [ELICIT: LLM Augmentation Via External In-context Capability](https://openreview.net/forum?id=CI4sCBMXjP) |  | 0 |  | Futing Wang, Jianhao Yan, Yue Zhang, Tao Lin |  |
| 1330 |  |  [RecFlow: An Industrial Full Flow Recommendation Dataset](https://openreview.net/forum?id=vVHc8bGRns) |  | 0 |  | Qi Liu, Kai Zheng, Rui Huang, Wuchao Li, Kuo Cai, Yuan Chai, Yanan Niu, Yiqun Hui, Bing Han, Na Mou, Hongning Wang, Wentian Bao, Yunen Yu, Guorui Zhou, Han Li, Yang Song, Defu Lian, Kun Gai |  |
| 1331 |  |  [Offline Hierarchical Reinforcement Learning via Inverse Optimization](https://openreview.net/forum?id=dTPz4rEDok) |  | 0 |  | Carolin Schmidt, Daniele Gammelli, James Harrison, Marco Pavone, Filipe Rodrigues |  |
| 1332 |  |  [QA-Calibration of Language Model Confidence Scores](https://openreview.net/forum?id=D2hhkU5O48) |  | 0 |  | Putra Manggala, AtalantiAnastasia Mastakouri, Elke Kirschbaum, Shiva Prasad Kasiviswanathan, Aaditya Ramdas |  |
| 1333 |  |  [ARLON: Boosting Diffusion Transformers with Autoregressive Models for Long Video Generation](https://openreview.net/forum?id=8pusxkLEQO) |  | 0 |  | Zongyi Li, Shujie Hu, Shujie Liu, Long Zhou, Jeongsoo Choi, Lingwei Meng, Xun Guo, Jinyu Li, Hefei Ling, Furu Wei |  |
| 1334 |  |  [Rethinking Shapley Value for Negative Interactions in Non-convex Games](https://openreview.net/forum?id=b24n2LS2BJ) |  | 0 |  | Wonjoon Chang, Myeongjin Lee, Jaesik Choi |  |
| 1335 |  |  [CO-MOT: Boosting End-to-end Transformer-based Multi-Object Tracking via Coopetition Label Assignment and Shadow Sets](https://openreview.net/forum?id=0ov0dMQ3mN) |  | 0 |  | Feng Yan, Weixin Luo, Yujie Zhong, Yiyang Gan, Lin Ma |  |
| 1336 |  |  [What's New in My Data? Novelty Exploration via Contrastive Generation](https://openreview.net/forum?id=IZDiRbVSVN) |  | 0 |  | Masaru Isonuma, Ivan Titov |  |
| 1337 |  |  [Durable Quantization Conditioned Misalignment Attack on Large Language Models](https://openreview.net/forum?id=41uZB8bDFh) |  | 0 |  | Peiran Dong, Haowei Li, Song Guo |  |
| 1338 |  |  [Node Identifiers: Compact, Discrete Representations for Efficient Graph Learning](https://openreview.net/forum?id=t9lS1lX9FQ) |  | 0 |  | Yuankai Luo, Hongkang Li, Qijiong Liu, Lei Shi, XiaoMing Wu |  |
| 1339 |  |  [UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation](https://openreview.net/forum?id=eLLBILFRsA) |  | 0 |  | Huimin Lu, Masaru Isonuma, Junichiro Mori, Ichiro Sakata |  |
| 1340 |  |  [MrSteve: Instruction-Following Agents in Minecraft with What-Where-When Memory](https://openreview.net/forum?id=CjXaMI2kUH) |  | 0 |  | Junyeong Park, Junmo Cho, Sungjin Ahn |  |
| 1341 |  |  [Optimality and Adaptivity of Deep Neural Features for Instrumental Variable Regression](https://openreview.net/forum?id=ReItdfwMcg) |  | 0 |  | Juno Kim, Dimitri Meunier, Arthur Gretton, Taiji Suzuki, Zhu Li |  |
| 1342 |  |  [Addressing Label Shift in Distributed Learning via Entropy Regularization​](https://openreview.net/forum?id=kuYxecnlv2) |  | 0 |  | Zhiyuan Wu, Changkyu Choi, Xiangcheng Cao, Volkan Cevher, Ali RamezaniKebrya |  |
| 1343 |  |  [Dreamweaver: Learning Compositional World Models from Pixels](https://openreview.net/forum?id=e5mTvjXG9u) |  | 0 |  | Junyeob Baek, YiFu Wu, Gautam Singh, Sungjin Ahn |  |
| 1344 |  |  [Rare event modeling with self-regularized normalizing flows: what can we learn from a single failure?](https://openreview.net/forum?id=gQoBw7sGAu) |  | 0 |  | Charles Dawson, Van Tran, Max Z. Li, Chuchu Fan |  |
| 1345 |  |  [Projection Head is Secretly an Information Bottleneck](https://openreview.net/forum?id=L0evcuybH5) |  | 0 |  | Zhuo Ouyang, Kaiwen Hu, Qi Zhang, Yifei Wang, Yisen Wang |  |
| 1346 |  |  [Schur's Positive-Definite Network: Deep Learning in the SPD cone with structure](https://openreview.net/forum?id=v1B4aet9ct) |  | 0 |  | Can Pouliquen, Mathurin Massias, Titouan Vayer |  |
| 1347 |  |  [SFESS: Score Function Estimators for k-Subset Sampling](https://openreview.net/forum?id=q87GUkdQBm) |  | 0 |  | Klas Wijk, Ricardo Vinuesa, Hossein Azizpour |  |
| 1348 |  |  [ConcreTizer: Model Inversion Attack via Occupancy Classification and Dispersion Control for 3D Point Cloud Restoration](https://openreview.net/forum?id=I4iZmsV4HM) |  | 0 |  | Youngseok Kim, Sunwook Hwang, HyungSin Kim, Saewoong Bahk |  |
| 1349 |  |  [Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models](https://openreview.net/forum?id=CbpWPbYHuv) |  | 0 |  | Zhijian Zhuo, Ya Wang, Yutao Zeng, Xiaoqing Li, Xun Zhou, Jinwen Ma |  |
| 1350 |  |  [ParamΔ for Direct Mixing: Post-Train Large Language Model At Zero Cost](https://openreview.net/forum?id=vqbd2OQnGp) |  | 0 |  | Sheng Cao, Mingrui Wu, Karthik Prasad, Yuandong Tian, Zechun Liu |  |
| 1351 |  |  [A Theory of Initialisation's Impact on Specialisation](https://openreview.net/forum?id=RQz7szbVDs) |  | 0 |  | Devon Jarvis, Sebastian Lee, Clémentine Carla Juliette Dominé, Andrew M. Saxe, Stefano Sarao Mannelli |  |
| 1352 |  |  [Manifolds, Random Matrices and Spectral Gaps: The geometric phases of generative diffusion](https://openreview.net/forum?id=KlN00vQEY2) |  | 0 |  | Enrico Ventura, Beatrice Achilli, Gianluigi Silvestri, Carlo Lucibello, Luca Ambrogioni |  |
| 1353 |  |  [AgentSquare: Automatic LLM Agent Search in Modular Design Space](https://openreview.net/forum?id=mPdmDYIQ7f) |  | 0 |  | Yu Shang, Yu Li, Keyu Zhao, Likai Ma, Jiahe Liu, Fengli Xu, Yong Li |  |
| 1354 |  |  [Multi-Task Corrupted Prediction for Learning Robust Audio-Visual Speech Representation](https://openreview.net/forum?id=WEQL5ksDnB) |  | 0 |  | Sungnyun Kim, Sungwoo Cho, Sangmin Bae, Kangwook Jang, SeYoung Yun |  |
| 1355 |  |  [Size-Generalizable RNA Structure Evaluation by Exploring Hierarchical Geometries](https://openreview.net/forum?id=QaTBHSqmH9) |  | 0 |  | Zongzhao Li, Jiacheng Cen, Wenbing Huang, Taifeng Wang, Le Song |  |
| 1356 |  |  [Semi-Supervised Vision-Centric 3D Occupancy World Model for Autonomous Driving](https://openreview.net/forum?id=rCX9l4OTCT) |  | 0 |  | Xiang Li, Pengfei Li, Yupeng Zheng, Wei Sun, Yan Wang, Yilun Chen |  |
| 1357 |  |  [Efficient Interpolation between Extragradient and Proximal Methods for Weak MVIs](https://openreview.net/forum?id=Y7slJZPGCy) |  | 0 |  | Thomas Pethick, Ioannis Mavrothalassitis, Volkan Cevher |  |
| 1358 |  |  [Generative Flows on Synthetic Pathway for Drug Design](https://openreview.net/forum?id=pB1XSj2y4X) |  | 0 |  | Seonghwan Seo, Minsu Kim, Tony Shen, Martin Ester, Jinkyoo Park, Sungsoo Ahn, Woo Youn Kim |  |
| 1359 |  |  [Integrating Protein Dynamics into Structure-Based Drug Design via Full-Atom Stochastic Flows](https://openreview.net/forum?id=9qS3HzSDNv) |  | 0 |  | Xiangxin Zhou, Yi Xiao, Haowei Lin, Xinheng He, Jiaqi Guan, Yang Wang, Qiang Liu, Feng Zhou, Liang Wang, Jianzhu Ma |  |
| 1360 |  |  [InstantSplamp: Fast and Generalizable Stenography Framework for Generative Gaussian Splatting](https://openreview.net/forum?id=xvhV3LvYTc) |  | 0 |  | Chenxin Li, Hengyu Liu, Zhiwen Fan, Wuyang Li, Yifan Liu, Panwang Pan, Yixuan Yuan |  |
| 1361 |  |  [Semantics-Adaptive Activation Intervention for LLMs via Dynamic Steering Vectors](https://openreview.net/forum?id=8WQ7VTfPTl) |  | 0 |  | Weixuan Wang, Jingyuan Yang, Wei Peng |  |
| 1362 |  |  [ADAM Optimization with Adaptive Batch Selection](https://openreview.net/forum?id=BZrSCv2SBq) |  | 0 |  | GyuYeol Kim, Minhwan Oh |  |
| 1363 |  |  [Tailoring Mixup to Data for Calibration](https://openreview.net/forum?id=3ygfMPLv0P) |  | 0 |  | Quentin Bouniot, Pavlo Mozharovskyi, Florence d'AlchéBuc |  |
| 1364 |  |  [MIRACLE 3D: Memory-efficient Integrated Robust Approach for Continual Learning on 3D Point Clouds via Shape Model Construction](https://openreview.net/forum?id=ANBuEJesgx) |  | 0 |  | Hossein Resani, Behrooz Nasihatkon |  |
| 1365 |  |  [Facilitating Multi-turn Function Calling for LLMs via Compositional Instruction Tuning](https://openreview.net/forum?id=owP2mymrTD) |  | 0 |  | Mingyang Chen, Haoze Sun, Tianpeng Li, Fan Yang, Hao Liang, Keer Lu, Bin Cui, Wentao Zhang, Zenan Zhou, Weipeng Chen |  |
| 1366 |  |  [SafeWatch: An Efficient Safety-Policy Following Video Guardrail Model with Transparent Explanations](https://openreview.net/forum?id=xjKz6IxgCX) |  | 0 |  | Zhaorun Chen, Francesco Pinto, Minzhou Pan, Bo Li |  |
| 1367 |  |  [TRENDy: Temporal Regression of Effective Nonlinear Dynamics](https://openreview.net/forum?id=NvDRvtrGLo) |  | 0 |  | Matthew Ricci, Guy Pelc, Zoe Piran, Noa Moriel, Mor Nitzan |  |
| 1368 |  |  [On Bits and Bandits: Quantifying the Regret-Information Trade-off](https://openreview.net/forum?id=0oWGVvC6oq) |  | 0 |  | Itai Shufaro, Nadav Merlis, Nir Weinberger, Shie Mannor |  |
| 1369 |  |  [SEBRA : Debiasing through Self-Guided Bias Ranking](https://openreview.net/forum?id=MyVC4X5B2X) |  | 0 |  | Adarsh Kappiyath, Abhra Chaudhuri, Ajay Kumar Jaiswal, Ziquan Liu, Yunpeng Li, Xiatian Zhu, Lu Yin |  |
| 1370 |  |  [Lambda-Skip Connections: the architectural component that prevents Rank Collapse](https://openreview.net/forum?id=1yJP5TVWih) |  | 0 |  | Federico Arangath Joseph, Jerome Sieber, Melanie Nicole Zeilinger, Carmen Amo Alonso |  |
| 1371 |  |  [Foundation Models Secretly Understand Neural Network Weights: Enhancing Hypernetwork Architectures with Foundation Models](https://openreview.net/forum?id=cADpvQgnqg) |  | 0 |  | Jeffrey Gu, Serena YeungLevy |  |
| 1372 |  |  [Mini-Monkey: Alleviating the Semantic Sawtooth Effect for Lightweight MLLMs via Complementary Image Pyramid](https://openreview.net/forum?id=71XtUhazG0) |  | 0 |  | Mingxin Huang, Yuliang Liu, Dingkang Liang, Lianwen Jin, Xiang Bai |  |
| 1373 |  |  [Multi-objective antibody design with constrained preference optimization](https://openreview.net/forum?id=4ktJJBvvUd) |  | 0 |  | Milong Ren, ZaiKai He, Haicang Zhang |  |
| 1374 |  |  [NL-Eye: Abductive NLI For Images](https://openreview.net/forum?id=2zmO1GVT0Y) |  | 0 |  | Mor Ventura, Michael Toker, Nitay Calderon, Zorik Gekhman, Yonatan Bitton, Roi Reichart |  |
| 1375 |  |  [COFlowNet: Conservative Constraints on Flows Enable High-Quality Candidate Generation](https://openreview.net/forum?id=tXUkT709OJ) |  | 0 |  | Yudong Zhang, Xuan Yu, Xu Wang, Zhaoyang Sun, Chen Zhang, Pengkun Wang, Yang Wang |  |
| 1376 |  |  [Curriculum-aware Training for Discriminating Molecular Property Prediction Models](https://openreview.net/forum?id=6DHIkLv5i3) |  | 0 |  | Hansi Yang, Quanming Yao, James Kwok |  |
| 1377 |  |  [Does Safety Training of LLMs Generalize to Semantically Related Natural Prompts?](https://openreview.net/forum?id=LO4MEPoqrG) |  | 0 |  | Sravanti Addepalli, Yerram Varun, Arun Sai Suggala, Karthikeyan Shanmugam, Prateek Jain |  |
| 1378 |  |  [Neural Context Flows for Meta-Learning of Dynamical Systems](https://openreview.net/forum?id=8vzMLo8LDN) |  | 0 |  | Roussel Desmond Nzoyem, David A. W. Barton, Tom Deakin |  |
| 1379 |  |  [Density estimation with LLMs: a geometric investigation of in-context learning trajectories](https://openreview.net/forum?id=semTHoVGsJ) |  | 0 |  | Toni J. B. Liu, Nicolas Boullé, Raphaël Sarfati, Christopher J. Earls |  |
| 1380 |  |  [Multimodal Quantitative Language for Generative Recommendation](https://openreview.net/forum?id=v7YrIjpkTF) |  | 0 |  | Jianyang Zhai, ZiFeng Mai, ChangDong Wang, Feidiao Yang, Xiawu Zheng, Hui Li, Yonghong Tian |  |
| 1381 |  |  [Coreset Spectral Clustering](https://openreview.net/forum?id=1qgZXeMTTU) |  | 0 |  | Ben Jourdan, Gregory Schwartzman, Peter Macgregor, He Sun |  |
| 1382 |  |  [Disentangled Representation Learning with the Gromov-Monge Gap](https://openreview.net/forum?id=ehr4oTe6XI) |  | 0 |  | Théo Uscidda, Luca Eyring, Karsten Roth, Fabian J. Theis, Zeynep Akata, Marco Cuturi |  |
| 1383 |  |  [Divergence-enhanced Knowledge-guided Context Optimization for Visual-Language Prompt Tuning](https://openreview.net/forum?id=6wOmHdwCC4) |  | 0 |  | Yilun Li, Miaomiao Cheng, Xu Han, Wei Song |  |
| 1384 |  |  [CheapNet: Cross-attention on Hierarchical representations for Efficient protein-ligand binding Affinity Prediction](https://openreview.net/forum?id=A1HhtITVEi) |  | 0 |  | Hyukjun Lim, Sun Kim, Sangseon Lee |  |
| 1385 |  |  [A Riemannian Framework for Learning Reduced-order Lagrangian Dynamics](https://openreview.net/forum?id=RoN6M3i7gJ) |  | 0 |  | Katharina Friedl, Noémie Jaquier, Jens Lundell, Tamim Asfour, Danica Kragic |  |
| 1386 |  |  [CARTS: Advancing Neural Theorem Proving with Diversified Tactic Calibration and Bias-Resistant Tree Search](https://openreview.net/forum?id=VQwI055flA) |  | 0 |  | XiaoWen Yang, Zhi Zhou, Haiming Wang, Aoxue Li, WenDa Wei, Hui Jin, Zhenguo Li, YuFeng Li |  |
| 1387 |  |  [Simple ReFlow: Improved Techniques for Fast Flow Models](https://openreview.net/forum?id=fpvgSDKXGY) |  | 0 |  | Beomsu Kim, YuGuan Hsieh, Michal Klein, Marco Cuturi, Jong Chul Ye, Bahjat Kawar, James Thornton |  |
| 1388 |  |  [A Theoretical Perspective: How to Prevent Model Collapse in Self-consuming Training Loops](https://openreview.net/forum?id=WttfQGwpES) |  | 0 |  | Shi Fu, Yingjie Wang, Yuzhu Chen, Xinmei Tian, Dacheng Tao |  |
| 1389 |  |  [Pareto Low-Rank Adapters: Efficient Multi-Task Learning with Preferences](https://openreview.net/forum?id=icDoYdUhRa) |  | 0 |  | Nikolaos Dimitriadis, Pascal Frossard, François Fleuret |  |
| 1390 |  |  [CryoGEN: Generative Energy-based Models for Cryogenic Electron Tomography Reconstruction](https://openreview.net/forum?id=uOb7rij7sR) |  | 0 |  | Yunfei Teng, Yuxuan Ren, Kai Chen, Xi Chen, Zhaoming Chen, Qiwei Ye |  |
| 1391 |  |  [Data Unlearning in Diffusion Models](https://openreview.net/forum?id=SuHScQv5gP) |  | 0 |  | Silas Alberti, Kenan Hasanaliyev, Manav Shah, Stefano Ermon |  |
| 1392 |  |  [Measuring And Improving Engagement of Text-to-Image Generation Models](https://openreview.net/forum?id=TmCcNuo03f) |  | 0 |  | Varun Khurana, Yaman Kumar Singla, Jayakumar Subramanian, Changyou Chen, Rajiv Ratn Shah, Zhiqiang Xu, Balaji Krishnamurthy |  |
| 1393 |  |  [Convergence of Score-Based Discrete Diffusion Models: A Discrete-Time Analysis](https://openreview.net/forum?id=pq1WUegkza) |  | 0 |  | Zikun Zhang, Zixiang Chen, Quanquan Gu |  |
| 1394 |  |  [VLAS: Vision-Language-Action Model with Speech Instructions for Customized Robot Manipulation](https://openreview.net/forum?id=K4FAFNRpko) |  | 0 |  | Wei Zhao, Pengxiang Ding, Min Zhang, Zhefei Gong, Shuanghao Bai, Han Zhao, Donglin Wang |  |
| 1395 |  |  [Teaching Human Behavior Improves Content Understanding Abilities Of VLMs](https://openreview.net/forum?id=ff2V3UR9sC) |  | 0 |  | Somesh Kumar Singh, Harini S. I, Yaman Kumar Singla, Changyou Chen, Rajiv Ratn Shah, Veeky Baths, Balaji Krishnamurthy |  |
| 1396 |  |  [Atomas: Hierarchical Adaptive Alignment on Molecule-Text for Unified Molecule Understanding and Generation](https://openreview.net/forum?id=mun3bGqdDM) |  | 0 |  | Yikun Zhang, Geyan Ye, Chaohao Yuan, Bo Han, LongKai Huang, Jianhua Yao, Wei Liu, Yu Rong |  |
| 1397 |  |  [LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://openreview.net/forum?id=pZiyCaVuti) |  | 0 |  | Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, KaiWei Chang, Dong Yu |  |
| 1398 |  |  [Reinforcement Learning from Imperfect Corrective Actions and Proxy Rewards](https://openreview.net/forum?id=JTji0Jfh5a) |  | 0 |  | Zhaohui Jiang, Xuening Feng, Paul Weng, Yifei Zhu, Yan Song, Tianze Zhou, Yujing Hu, Tangjie Lv, Changjie Fan |  |
| 1399 |  |  [Improved Sampling Algorithms for Lévy-Itô Diffusion Models](https://openreview.net/forum?id=XxCgeWSTNp) |  | 0 |  | Vadim Popov, Assel Yermekova, Tasnima Sadekova, Artem Khrapov, Mikhail Sergeevich Kudinov |  |
| 1400 |  |  [Measuring And Improving Persuasiveness Of Large Language Models](https://openreview.net/forum?id=NfCEVihkdC) |  | 0 |  | Somesh Kumar Singh, Yaman Kumar Singla, Harini S. I, Balaji Krishnamurthy |  |
| 1401 |  |  [Accelerating 3D Molecule Generation via Jointly Geometric Optimal Transport](https://openreview.net/forum?id=VGURexnlUL) |  | 0 |  | Haokai Hong, Wanyu Lin, Kc Tan |  |
| 1402 |  |  [Learning LLM-as-a-Judge for Preference Alignment](https://openreview.net/forum?id=HZVIQE1MsJ) |  | 0 |  | Ziyi Ye, Xiangsheng Li, Qiuchi Li, Qingyao Ai, Yujia Zhou, Wei Shen, Dong Yan, Yiqun Liu |  |
| 1403 |  |  [SCBench: A KV Cache-Centric Analysis of Long-Context Methods](https://openreview.net/forum?id=gkUyYcY1W9) |  | 0 |  | Yucheng Li, Huiqiang Jiang, Qianhui Wu, Xufang Luo, Surin Ahn, Chengruidong Zhang, Amir H. Abdi, Dongsheng Li, Jianfeng Gao, Yuqing Yang, Lili Qiu |  |
| 1404 |  |  [Monet: Mixture of Monosemantic Experts for Transformers](https://openreview.net/forum?id=1Ogw1SHY3p) |  | 0 |  | Jungwoo Park, Ahn Young Jin, KeeEung Kim, Jaewoo Kang |  |
| 1405 |  |  [GraphBridge: Towards Arbitrary Transfer Learning in GNNs](https://openreview.net/forum?id=gjRhw5S3A4) |  | 0 |  | Li Ju, Xingyi Yang, Qi Li, Xinchao Wang |  |
| 1406 |  |  [The KoLMogorov Test: Compression by Code Generation](https://openreview.net/forum?id=C45YqeBDUM) |  | 0 |  | Ori Yoran, Kunhao Zheng, Fabian Gloeckle, Jonas Gehring, Gabriel Synnaeve, Taco Cohen |  |
| 1407 |  |  [Towards Explaining the Power of Constant-depth Graph Neural Networks for Structured Linear Programming](https://openreview.net/forum?id=INow59Vurm) |  | 0 |  | Qian Li, Minghui Ouyang, Tian Ding, Yuyi Wang, Qingjiang Shi, Ruoyu Sun |  |
| 1408 |  |  [Hybrid Regularization Improves Diffusion-based Inverse Problem Solving](https://openreview.net/forum?id=d7pr2doXn3) |  | 0 |  | Hongkun Dou, Zeyu Li, Jinyang Du, Lijun Yang, Wen Yao, Yue Deng |  |
| 1409 |  |  [RelCon: Relative Contrastive Learning for a Motion Foundation Model for Wearable Data](https://openreview.net/forum?id=k2uUeLCrQq) |  | 0 |  | Maxwell A. Xu, Jaya Narain, Gregory Darnell, Haraldur Tómas Hallgrímsson, Hyewon Jeong, Darren Forde, Richard Andres Fineman, Karthik Jayaraman Raghuram, James Matthew Rehg, Shirley You Ren |  |
| 1410 |  |  [RevisEval: Improving LLM-as-a-Judge via Response-Adapted References](https://openreview.net/forum?id=1tBvzOYTLF) |  | 0 |  | Qiyuan Zhang, Yufei Wang, Tiezheng Yu, Yuxin Jiang, Chuhan Wu, Liangyou Li, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Fuyuan Lyu, Chen Ma |  |
| 1411 |  |  [InCoDe: Interpretable Compressed Descriptions For Image Generation](https://openreview.net/forum?id=aXwukBD6M6) |  | 0 |  | Armand Comas Massague, Aditya Chattopadhyay, Feliu Formosa, Changyu Liu, Octavia I. Camps, René Vidal |  |
| 1412 |  |  [Neuroplastic Expansion in Deep Reinforcement Learning](https://openreview.net/forum?id=20qZK2T7fa) |  | 0 |  | Jiashun Liu, Johan S. ObandoCeron, Aaron C. Courville, Ling Pan |  |
| 1413 |  |  [Efficient and Robust Neural Combinatorial Optimization via Wasserstein-Based Coresets](https://openreview.net/forum?id=F57HPKZ6KD) |  | 0 |  | Xu Wang, Fuyou Miao, Wenjie Liu, Yan Xiong |  |
| 1414 |  |  [Bio-xLSTM: Generative modeling, representation and in-context learning of biological and chemical sequences](https://openreview.net/forum?id=IjbXZdugdj) |  | 0 |  | Niklas Schmidinger, Lisa Schneckenreiter, Philipp Seidl, Johannes Schimunek, PieterJan Hoedt, Johannes Brandstetter, Andreas Mayr, Sohvi Luukkonen, Sepp Hochreiter, Günter Klambauer |  |
| 1415 |  |  [Uni2Det: Unified and Universal Framework for Prompt-Guided Multi-dataset 3D Detection](https://openreview.net/forum?id=AcVpLS86RT) |  | 0 |  | Yubin Wang, Zhikang Zou, Xiaoqing Ye, Xiao Tan, Errui Ding, Cairong Zhao |  |
| 1416 |  |  [Neural networks on Symmetric Spaces of Noncompact Type](https://openreview.net/forum?id=bwOndfohRK) |  | 0 |  | Xuan Son Nguyen, Shuo Yang, Aymeric Histace |  |
| 1417 |  |  [Graph-based Document Structure Analysis](https://openreview.net/forum?id=Fu0aggezN9) |  | 0 |  | Yufan Chen, Ruiping Liu, Junwei Zheng, Di Wen, Kunyu Peng, Jiaming Zhang, Rainer Stiefelhagen |  |
| 1418 |  |  [Rethinking Invariance in In-context Learning](https://openreview.net/forum?id=q1UyoY3MgJ) |  | 0 |  | Lizhe Fang, Yifei Wang, Khashayar Gatmiry, Lei Fang, Yisen Wang |  |
| 1419 |  |  [FlashMask: Efficient and Rich Mask Extension of FlashAttention](https://openreview.net/forum?id=wUtXB43Chi) |  | 0 |  | Guoxia Wang, Jinle Zeng, Xiyuan Xiao, Siming Wu, Jiabin Yang, Lujing Zheng, Zeyu Chen, Jiang Bian, Dianhai Yu, Haifeng Wang |  |
| 1420 |  |  [You Only Sample Once: Taming One-Step Text-to-Image Synthesis by Self-Cooperative Diffusion GANs](https://openreview.net/forum?id=T7bmHkwzS6) |  | 0 |  | Yihong Luo, Xiaolong Chen, Xinghua Qu, Tianyang Hu, Jing Tang |  |
| 1421 |  |  [Generalizing Weisfeiler-Lehman Kernels to Subgraphs](https://openreview.net/forum?id=HZgZrtIreg) |  | 0 |  | Dongkwan Kim, Alice Oh |  |
| 1422 |  |  [Automated Filtering of Human Feedback Data for Aligning Text-to-Image Diffusion Models](https://openreview.net/forum?id=8jvVNPHtVJ) |  | 0 |  | Yongjin Yang, Sihyeon Kim, Hojung Jung, Sangmin Bae, SangMook Kim, SeYoung Yun, Kimin Lee |  |
| 1423 |  |  [What is Wrong with Perplexity for Long-context Language Modeling?](https://openreview.net/forum?id=fL4qWkSmtM) |  | 0 |  | Lizhe Fang, Yifei Wang, Zhaoyang Liu, Chenheng Zhang, Stefanie Jegelka, Jinyang Gao, Bolin Ding, Yisen Wang |  |
| 1424 |  |  [KooNPro: A Variance-Aware Koopman Probabilistic Model Enhanced by Neural Process for Time Series Forecasting](https://openreview.net/forum?id=5oSUgTzs8Y) |  | 0 |  | Ronghua Zheng, Hanru Bai, Weiyang Ding |  |
| 1425 |  |  [TC-MoE: Augmenting Mixture of Experts with Ternary Expert Choice](https://openreview.net/forum?id=dsP91M4hDL) |  | 0 |  | Shen Yan, Xingyan Bin, Sijun Zhang, Yisen Wang, Zhouchen Lin |  |
| 1426 |  |  [Bayesian WeakS-to-Strong from Text Classification to Generation](https://openreview.net/forum?id=pHe4P1IVnb) |  | 0 |  | Ziyun Cui, Ziyang Zhang, Guangzhi Sun, Wen Wu, Chao Zhang |  |
| 1427 |  |  [Polyrating: A Cost-Effective and Bias-Aware Rating System for LLM Evaluation](https://openreview.net/forum?id=URPwT55i6O) |  | 0 |  | Jasper Dekoninck, Maximilian Baader, Martin T. Vechev |  |
| 1428 |  |  [The impact of allocation strategies in subset learning on the expressive power of neural networks](https://openreview.net/forum?id=upoxXRRTQ2) |  | 0 |  | Ofir Schlisselberg, Ran Darshan |  |
| 1429 |  |  [WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning](https://openreview.net/forum?id=oVKEAFjEqv) |  | 0 |  | Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Jiadai Sun, Xinyue Yang, Yu Yang, Shuntian Yao, Wei Xu, Jie Tang, Yuxiao Dong |  |
| 1430 |  |  [Surgical, Cheap, and Flexible: Mitigating False Refusal in Language Models via Single Vector Ablation](https://openreview.net/forum?id=SCBn8MCLwc) |  | 0 |  | Xinpeng Wang, Chengzhi Hu, Paul Röttger, Barbara Plank |  |
| 1431 |  |  [Adaptive Methods through the Lens of SDEs: Theoretical Insights on the Role of Noise](https://openreview.net/forum?id=ww3CLRhF1v) |  | 0 |  | Enea Monzio Compagnoni, Tianlin Liu, Rustem Islamov, Frank Norbert Proske, Antonio Orvieto, Aurélien Lucchi |  |
| 1432 |  |  [Glauber Generative Model: Discrete Diffusion Models via Binary Classification](https://openreview.net/forum?id=HyjIEf90Tn) |  | 0 |  | Harshit Varma, Dheeraj Mysore Nagaraj, Karthikeyan Shanmugam |  |
| 1433 |  |  [AutoBencher: Towards Declarative Benchmark Construction](https://openreview.net/forum?id=ymt4crbbXh) |  | 0 |  | Xiang Lisa Li, Farzaan Kaiyom, Evan Zheran Liu, Yifan Mai, Percy Liang, Tatsunori Hashimoto |  |
| 1434 |  |  [Self-Evolving Multi-Agent Collaboration Networks for Software Development](https://openreview.net/forum?id=4R71pdPBZp) |  | 0 |  | Yue Hu, Yuzhu Cai, Yaxin Du, Xinyu Zhu, Xiangrui Liu, Zijie Yu, Yuchen Hou, Shuo Tang, Siheng Chen |  |
| 1435 |  |  [BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation Experiments](https://openreview.net/forum?id=HAwZGLcye3) |  | 0 |  | Yusuf H. Roohani, Andrew H. Lee, Qian Huang, Jian Vora, Zachary Steinhart, Kexin Huang, Alexander Marson, Percy Liang, Jure Leskovec |  |
| 1436 |  |  [Privacy-Aware Lifelong Learning](https://openreview.net/forum?id=UstOpZCESc) |  | 0 |  | Ozan Özdenizci, Elmar Rueckert, Robert Legenstein |  |
| 1437 |  |  [PhyloLM: Inferring the Phylogeny of Large Language Models and Predicting their Performances in Benchmarks](https://openreview.net/forum?id=rTQNGQxm4K) |  | 0 |  | Nicolas Yax, PierreYves Oudeyer, Stefano Palminteri |  |
| 1438 |  |  [Stabilized Neural Prediction of Potential Outcomes in Continuous Time](https://openreview.net/forum?id=aN57tSd5Us) |  | 0 |  | Konstantin Hess, Stefan Feuerriegel |  |
| 1439 |  |  [UIFace: Unleashing Inherent Model Capabilities to Enhance Intra-Class Diversity in Synthetic Face Recognition](https://openreview.net/forum?id=riieAeQBJm) |  | 0 |  | Xiao Lin, Yuge Huang, Jianqing Xu, Yuxi Mi, Shuigeng Zhou, Shouhong Ding |  |
| 1440 |  |  [FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware](https://openreview.net/forum?id=l0ZzTvPfTw) |  | 0 |  | Korbinian Pöppel, Maximilian Beck, Sepp Hochreiter |  |
| 1441 |  |  [Dream to Manipulate: Compositional World Models Empowering Robot Imitation Learning with Imagination](https://openreview.net/forum?id=3RSLW9YSgk) |  | 0 |  | Leonardo Barcellona, Andrii Zadaianchuk, Davide Allegro, Samuele Papa, Stefano Ghidoni, Efstratios Gavves |  |
| 1442 |  |  [Inverse Rendering using Multi-Bounce Path Tracing and Reservoir Sampling](https://openreview.net/forum?id=KEXoZxTwbr) |  | 0 |  | Yuxin Dai, Qi Wang, Jingsen Zhu, Dianbing Xi, Yuchi Huo, Chen Qian, Ying He |  |
| 1443 |  |  [GeoILP: A Synthetic Dataset to Guide Large-Scale Rule Induction](https://openreview.net/forum?id=cfGpIcOIa5) |  | 0 |  | Si Chen, Richong Zhang, Xu Zhang |  |
| 1444 |  |  [Generating Graphs via Spectral Diffusion](https://openreview.net/forum?id=AAXBfJNHDt) |  | 0 |  | Giorgia Minello, Alessandro Bicciato, Luca Rossi, Andrea Torsello, Luca Cosmo |  |
| 1445 |  |  [Grounding Continuous Representations in Geometry: Equivariant Neural Fields](https://openreview.net/forum?id=A4eCzSohhx) |  | 0 |  | David R. Wessels, David M. Knigge, Riccardo Valperga, Samuele Papa, Sharvaree P. Vadgama, Efstratios Gavves, Erik J. Bekkers |  |
| 1446 |  |  [FOSP: Fine-tuning Offline Safe Policy through World Models](https://openreview.net/forum?id=dbuFJg7eaw) |  | 0 |  | Chenyang Cao, Yucheng Xin, Silang Wu, Longxiang He, Zichen Yan, Junbo Tan, Xueqian Wang |  |
| 1447 |  |  [Autocorrelation Matters: Understanding the Role of Initialization Schemes for State Space Models](https://openreview.net/forum?id=sZJNkorXMk) |  | 0 |  | Fusheng Liu, Qianxiao Li |  |
| 1448 |  |  [SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents](https://openreview.net/forum?id=xKDZAW0He3) |  | 0 |  | Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Xufang Luo, Hao Cheng, Dongsheng Li, Yuqing Yang, ChinYew Lin, H. Vicky Zhao, Lili Qiu, Jianfeng Gao |  |
| 1449 |  |  [Self-supervised Monocular Depth Estimation Robust to Reflective Surface Leveraged by Triplet Mining](https://openreview.net/forum?id=XdRIno98gG) |  | 0 |  | Wonhyeok Choi, Kyumin Hwang, Wei Peng, Minwoo Choi, Sunghoon Im |  |
| 1450 |  |  [An Effective Theory of Bias Amplification](https://openreview.net/forum?id=VoI4d6uhdr) |  | 0 |  | Arjun Subramonian, Samuel J. Bell, Levent Sagun, Elvis Dohmatob |  |
| 1451 |  |  [Representational Similarity via Interpretable Visual Concepts](https://openreview.net/forum?id=ih3BJmIZbC) |  | 0 |  | Neehar Kondapaneni, Oisin Mac Aodha, Pietro Perona |  |
| 1452 |  |  [DRoC: Elevating Large Language Models for Complex Vehicle Routing via Decomposed Retrieval of Constraints](https://openreview.net/forum?id=s9zoyICZ4k) |  | 0 |  | Xia Jiang, Yaoxin Wu, Chenhao Zhang, Yingqian Zhang |  |
| 1453 |  |  [On Minimizing Adversarial Counterfactual Error in Adversarial Reinforcement Learning](https://openreview.net/forum?id=eUEMjwh5wK) |  | 0 |  | Roman Belaire, Arunesh Sinha, Pradeep Varakantham |  |
| 1454 |  |  [ToolDial: Multi-turn Dialogue Generation Method for Tool-Augmented Language Models](https://openreview.net/forum?id=J1J5eGJsKZ) |  | 0 |  | Jeonghoon Shim, Gyuhyeon Seo, Cheongsu Lim, Yohan Jo |  |
| 1455 |  |  [3D-MolT5: Leveraging Discrete Structural Information for Molecule-Text Modeling](https://openreview.net/forum?id=eGqQyTAbXC) |  | 0 |  | Qizhi Pei, Rui Yan, Kaiyuan Gao, Jinhua Zhu, Lijun Wu |  |
| 1456 |  |  [ParFam - (Neural Guided) Symbolic Regression via Continuous Global Optimization](https://openreview.net/forum?id=8y5Uf6oEiB) |  | 0 |  | Philipp Scholl, Katharina Bieker, Hillary Hauger, Gitta Kutyniok |  |
| 1457 |  |  [Taming Transformer Without Using Learning Rate Warmup](https://openreview.net/forum?id=GeUK3zGreN) |  | 0 |  | Xianbiao Qi, Yelin He, Jiaquan Ye, ChunGuang Li, Bojia Zi, Xili Dai, Qin Zou, Rong Xiao |  |
| 1458 |  |  [Redefining the task of Bioactivity Prediction](https://openreview.net/forum?id=S8gbnkCgxZ) |  | 0 |  | Yanwen Huang, Bowen Gao, Yinjun Jia, Hongbo Ma, WeiYing Ma, YaQin Zhang, Yanyan Lan |  |
| 1459 |  |  [Boltzmann priors for Implicit Transfer Operators](https://openreview.net/forum?id=pRCOZllZdT) |  | 0 |  | Juan Viguera Diez, Mathias Jacob Schreiner, Ola Engkvist, Simon Olsson |  |
| 1460 |  |  [Can In-context Learning Really Generalize to Out-of-distribution Tasks?](https://openreview.net/forum?id=INe4otjryz) |  | 0 |  | Qixun Wang, Yifei Wang, Xianghua Ying, Yisen Wang |  |
| 1461 |  |  [CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences](https://openreview.net/forum?id=EQgEMAD4kv) |  | 0 |  | Ziran Qin, Yuchen Cao, Mingbao Lin, Wen Hu, Shixuan Fan, Ke Cheng, Weiyao Lin, Jianguo Li |  |
| 1462 |  |  [Revisiting Nearest Neighbor for Tabular Data: A Deep Tabular Baseline Two Decades Later](https://openreview.net/forum?id=JytL2MrlLT) |  | 0 |  | HanJia Ye, HuaiHong Yin, DeChuan Zhan, WeiLun Chao |  |
| 1463 |  |  [LoR-VP: Low-Rank Visual Prompting for Efficient Vision Model Adaptation](https://openreview.net/forum?id=5btFIv2PNb) |  | 0 |  | Can Jin, Ying Li, Mingyu Zhao, Shiyu Zhao, Zhenting Wang, Xiaoxiao He, Ligong Han, Tong Che, Dimitris N. Metaxas |  |
| 1464 |  |  [Robust Simulation-Based Inference under Missing Data via Neural Processes](https://openreview.net/forum?id=GsR3zRCRX5) |  | 0 |  | Yogesh Verma, Ayush Bharti, Vikas Garg |  |
| 1465 |  |  [Start Smart: Leveraging Gradients For Enhancing Mask-based XAI Methods](https://openreview.net/forum?id=Iht4NNVqk0) |  | 0 |  | Buelent Uendes, Shujian Yu, Mark Hoogendoorn |  |
| 1466 |  |  [DRESSing Up LLM: Efficient Stylized Question-Answering via Style Subspace Editing](https://openreview.net/forum?id=mNVR9jJYqK) |  | 0 |  | Xinyu Ma, Yifeng Xu, Yang Lin, Tianlong Wang, Xu Chu, Xin Gao, Junfeng Zhao, Yasha Wang |  |
| 1467 |  |  [LLMOPT: Learning to Define and Solve General Optimization Problems from Scratch](https://openreview.net/forum?id=9OMvtboTJg) |  | 0 |  | Caigao Jiang, Xiang Shu, Hong Qian, Xingyu Lu, Jun Zhou, Aimin Zhou, Yang Yu |  |
| 1468 |  |  [T2V2: A Unified Non-Autoregressive Model for Speech Recognition and Synthesis via Multitask Learning](https://openreview.net/forum?id=TtKN1TpvUu) |  | 0 |  | Nabarun Goswami, Hanqin Wang, Tatsuya Harada |  |
| 1469 |  |  [Physics-Informed Diffusion Models](https://openreview.net/forum?id=tpYeermigp) |  | 0 |  | JanHendrik Bastek, WaiChing Sun, Dennis M. Kochmann |  |
| 1470 |  |  [A Simple yet Effective ΔΔG Predictor is An Unsupervised Antibody Optimizer and Explainer](https://openreview.net/forum?id=IxmWIkcKs5) |  | 0 |  | Lirong Wu, Yunfan Liu, Haitao Lin, Yufei Huang, Guojiang Zhao, Zhifeng Gao, Stan Z. Li |  |
| 1471 |  |  [Identifying latent state transitions in non-linear dynamical systems](https://openreview.net/forum?id=d16mJDyQN6) |  | 0 |  | Çaglar Hizli, Çagatay Yildiz, Matthias Bethge, S. T. John, Pekka Marttinen |  |
| 1472 |  |  [Neuron based Personality Trait Induction in Large Language Models](https://openreview.net/forum?id=LYHEY783Np) |  | 0 |  | Jia Deng, Tianyi Tang, Yanbin Yin, Wenhao Yang, Xin Zhao, JiRong Wen |  |
| 1473 |  |  [Diffusion-based Neural Network Weights Generation](https://openreview.net/forum?id=j8WHjM9aMm) |  | 0 |  | Bedionita Soro, Bruno Andreis, Hayeon Lee, Wonyong Jeong, Song Chong, Frank Hutter, Sung Ju Hwang |  |
| 1474 |  |  [Improving Neural Network Accuracy by Concurrently Training with a Twin Network](https://openreview.net/forum?id=TEmE9PSC65) |  | 0 |  | Benjamin Vandersmissen, Lucas Deckers, José Oramas M. |  |
| 1475 |  |  [Sylber: Syllabic Embedding Representation of Speech from Raw Audio](https://openreview.net/forum?id=FyMjfDQ9RO) |  | 0 |  | Cheol Jun Cho, Nicholas Lee, Akshat Gupta, Dhruv Agarwal, Ethan Chen, Alan W. Black, Gopala Anumanchipalli |  |
| 1476 |  |  [How to Verify Any (Reasonable) Distribution Property: Computationally Sound Argument Systems for Distributions](https://openreview.net/forum?id=GfXMTAJaxZ) |  | 0 |  | Tal Herman, Guy N. Rothblum |  |
| 1477 |  |  [Synthesizing Programmatic Reinforcement Learning Policies with Large Language Model Guided Search](https://openreview.net/forum?id=8DBTq09LgN) |  | 0 |  | Max Liu, ChanHung Yu, WeiHsu Lee, ChengWei Hung, YenChun Chen, ShaoHua Sun |  |
| 1478 |  |  [DynFrs: An Efficient Framework for Machine Unlearning in Random Forest](https://openreview.net/forum?id=nsCOeCLR8e) |  | 0 |  | Shurong Wang, Zhuoyang Shen, Xinbao Qiao, Tongning Zhang, Meng Zhang |  |
| 1479 |  |  [GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling](https://openreview.net/forum?id=1p6xFLBU4J) |  | 0 |  | Jixun Yao, Hexin Liu, Chen Chen, Yuchen Hu, Eng Siong Chng, Lei Xie |  |
| 1480 |  |  [Emerging Safety Attack and Defense in Federated Instruction Tuning of Large Language Models](https://openreview.net/forum?id=sYNWqQYJhz) |  | 0 |  | Rui Ye, Jingyi Chai, Xiangrui Liu, Yaodong Yang, Yanfeng Wang, Siheng Chen |  |
| 1481 |  |  [Learned Reference-based Diffusion Sampler for multi-modal distributions](https://openreview.net/forum?id=fmJUYgmMbL) |  | 0 |  | Maxence Noble, Louis Grenioux, Marylou Gabrié, Alain Oliviero Durmus |  |
| 1482 |  |  [Looking Backward: Retrospective Backward Synthesis for Goal-Conditioned GFlowNets](https://openreview.net/forum?id=fNMKqyvuZT) |  | 0 |  | Haoran He, Can Chang, Huazhe Xu, Ling Pan |  |
| 1483 |  |  [Tree-Wasserstein Distance for High Dimensional Data with a Latent Feature Hierarchy](https://openreview.net/forum?id=nYjAzwor9R) |  | 0 |  | YaWei Eileen Lin, Ronald R. Coifman, Gal Mishne, Ronen Talmon |  |
| 1484 |  |  [StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via Inference-time Hybrid Information Structurization](https://openreview.net/forum?id=GhexuBLxbO) |  | 0 |  | Zhuoqun Li, Xuanang Chen, Haiyang Yu, Hongyu Lin, Yaojie Lu, Qiaoyu Tang, Fei Huang, Xianpei Han, Le Sun, Yongbin Li |  |
| 1485 |  |  [Infilling Score: A Pretraining Data Detection Algorithm for Large Language Models](https://openreview.net/forum?id=9QPH1YQCMn) |  | 0 |  | Negin Raoof, Litu Rout, Giannis Daras, Sujay Sanghavi, Constantine Caramanis, Sanjay Shakkottai, Alex Dimakis |  |
| 1486 |  |  [Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model](https://openreview.net/forum?id=VxvnV6slP0) |  | 0 |  | Longrong Yang, Dong Shen, Chaoxiang Cai, Fan Yang, Tingting Gao, Di Zhang, Xi Li |  |
| 1487 |  |  [ConvCodeWorld: Benchmarking Conversational Code Generation in Reproducible Feedback Environments](https://openreview.net/forum?id=rpouyo09V0) |  | 0 |  | Hojae Han, Seungwon Hwang, Rajhans Samdani, Yuxiong He |  |
| 1488 |  |  [Neural Causal Graph for Interpretable and Intervenable Classification](https://openreview.net/forum?id=nmvmPIi185) |  | 0 |  | Jiawei Wang, Shaofei Lu, Da Cao, Dongyu Wang, Yuquan Le, Zhe Quan, TatSeng Chua |  |
| 1489 |  |  [Loss Landscape of Shallow ReLU-like Neural Networks: Stationary Points, Saddle Escape, and Network Embedding](https://openreview.net/forum?id=ogKE7LcvW6) |  | 0 |  | Zhengqing Wu, Berfin Simsek, François Gaston Ged |  |
| 1490 |  |  [TPO: Aligning Large Language Models with Multi-branch & Multi-step Preference Trees](https://openreview.net/forum?id=O0sQ9CPzai) |  | 0 |  | Weibin Liao, Xu Chu, Yasha Wang |  |
| 1491 |  |  [VideoWebArena: Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks](https://openreview.net/forum?id=unDQOUah0F) |  | 0 |  | Lawrence Keunho Jang, Yinheng Li, Dan Zhao, Charles Ding, Justin Lin, Paul Pu Liang, Rogerio Bonatti, Kazuhito Koishida |  |
| 1492 |  |  [A Skewness-Based Criterion for Addressing Heteroscedastic Noise in Causal Discovery](https://openreview.net/forum?id=zGzs5SIwT8) |  | 0 |  | Yingyu Lin, Yuxing Huang, Wenqin Liu, Haoran Deng, Ignavier Ng, Kun Zhang, Mingming Gong, Yian Ma, Biwei Huang |  |
| 1493 |  |  [Shallow diffusion networks provably learn hidden low-dimensional structure](https://openreview.net/forum?id=KlxK4ncqWZ) |  | 0 |  | Nicholas Matthew Boffi, Arthur Jacot, Stephen Tu, Ingvar M. Ziemann |  |
| 1494 |  |  [Exploring the Design Space of Visual Context Representation in Video MLLMs](https://openreview.net/forum?id=UN6Ik6OCx8) |  | 0 |  | Yifan Du, Yuqi Huo, Kun Zhou, Zijia Zhao, Haoyu Lu, Han Huang, Xin Zhao, Bingning Wang, Weipeng Chen, JiRong Wen |  |
| 1495 |  |  [Adversarial Score identity Distillation: Rapidly Surpassing the Teacher in One Step](https://openreview.net/forum?id=lS2SGfWizd) |  | 0 |  | Mingyuan Zhou, Huangjie Zheng, Yi Gu, Zhendong Wang, Hai Huang |  |
| 1496 |  |  [Q-Adapter: Customizing Pre-trained LLMs to New Preferences with Forgetting Mitigation](https://openreview.net/forum?id=WLSrq1254E) |  | 0 |  | YiChen Li, Fuxiang Zhang, Wenjie Qiu, Lei Yuan, Chengxing Jia, Zongzhang Zhang, Yang Yu, Bo An |  |
| 1497 |  |  [Bridging the Gap between Variational Inference and Stochastic Gradient MCMC in Function Space](https://openreview.net/forum?id=bNVbOS3lrl) |  | 0 |  | Mengjing Wu, Junyu Xuan, Jie Lu |  |
| 1498 |  |  [Flash Inference: Near Linear Time Inference for Long Convolution Sequence Models and Beyond](https://openreview.net/forum?id=cZWCjan02B) |  | 0 |  | CostinAndrei Oncescu, Sanket Purandare, Stratos Idreos, Sham M. Kakade |  |
| 1499 |  |  [σ-zero: Gradient-based Optimization of ℓ0-norm Adversarial Examples](https://openreview.net/forum?id=JMPOqoe4tl) |  | 0 |  | Antonio Emanuele Cinà, Francesco Villani, Maura Pintor, Lea Schönherr, Battista Biggio, Marcello Pelillo |  |
| 1500 |  |  [On the Adversarial Vulnerability of Label-Free Test-Time Adaptation](https://openreview.net/forum?id=N0ETIi580T) |  | 0 |  | Shahriar Rifat, Jonathan D. Ashdown, Michael J. De Lucia, Ananthram Swami, Francesco Restuccia |  |
| 1501 |  |  [Strength Estimation and Human-Like Strength Adjustment in Games](https://openreview.net/forum?id=CvjXlsBLCX) |  | 0 |  | Chun Jung Chen, ChungChin Shih, TiRong Wu |  |
| 1502 |  |  [Discrete Copula Diffusion](https://openreview.net/forum?id=FXw0okNcOb) |  | 0 |  | Anji Liu, Oliver Broadrick, Mathias Niepert, Guy Van den Broeck |  |
| 1503 |  |  [Adaptive Energy Alignment for Accelerating Test-Time Adaptation](https://openreview.net/forum?id=sEMJ1PLSZR) |  | 0 |  | Wonjeong Choi, DoYeon Kim, Jungwuk Park, Jungmoon Lee, Younghyun Park, DongJun Han, Jaekyun Moon |  |
| 1504 |  |  [Unsupervised Disentanglement of Content and Style via Variance-Invariance Constraints](https://openreview.net/forum?id=Lut5t3qElA) |  | 0 |  | Yuxuan Wu, Ziyu Wang, Bhiksha Raj, Gus Xia |  |
| 1505 |  |  [InversionGNN: A Dual Path Network for Multi-Property Molecular Optimization](https://openreview.net/forum?id=nYPuSzGE3X) |  | 0 |  | Yifan Niu, Ziqi Gao, Tingyang Xu, Yang Liu, Yatao Bian, Yu Rong, Junzhou Huang, Jia Li |  |
| 1506 |  |  [Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN](https://openreview.net/forum?id=BChpQU64RG) |  | 0 |  | Pengxiang Li, Lu Yin, Shiwei Liu |  |
| 1507 |  |  [Large Language Models Often Say One Thing and Do Another](https://openreview.net/forum?id=RTHbao4Mib) |  | 0 |  | Ruoxi Xu, Hongyu Lin, Xianpei Han, Jia Zheng, Weixiang Zhou, Le Sun, Yingfei Sun |  |
| 1508 |  |  [Learning Task Belief Similarity with Latent Dynamics for Meta-Reinforcement Learning](https://openreview.net/forum?id=5YbuOTUFQ4) |  | 0 |  | Menglong Zhang, Fuyuan Qian, Quanying Liu |  |
| 1509 |  |  [Unearthing Skill-level Insights for Understanding Trade-offs of Foundation Models](https://openreview.net/forum?id=kNHVViEPWK) |  | 0 |  | Mazda Moayeri, Vidhisha Balachandran, Varun Chandrasekaran, Safoora Yousefi, Thomas Fel, Soheil Feizi, Besmira Nushi, Neel Joshi, Vibhav Vineet |  |
| 1510 |  |  [Decoupling Layout from Glyph in Online Chinese Handwriting Generation](https://openreview.net/forum?id=DhHIw9Nbl1) |  | 0 |  | Minsi Ren, YanMing Zhang, Yi Chen |  |
| 1511 |  |  [Handling Delay in Real-Time Reinforcement Learning](https://openreview.net/forum?id=YOc5t8PHf2) |  | 0 |  | Ivan Anokhin, Rishav Rishav, Matthew Riemer, Stephen Chung, Irina Rish, Samira Ebrahimi Kahou |  |
| 1512 |  |  [Which Tasks Should Be Compressed Together? A Causal Discovery Approach for Efficient Multi-Task Representation Compression](https://openreview.net/forum?id=x33vSZUg0A) |  | 0 |  | Sha Guo, Jing Chen, Zixuan Hu, Zhuo Chen, Wenhan Yang, Yu Lin, Xing Jiang, Lingyu Duan |  |
| 1513 |  |  [CrossMPT: Cross-attention Message-passing Transformer for Error Correcting Codes](https://openreview.net/forum?id=gFvRRCnQvX) |  | 0 |  | SeongJoon Park, Heeyoul Kwak, SangHyo Kim, Yongjune Kim, JongSeon No |  |
| 1514 |  |  [Diversity Empowers Intelligence: Integrating Expertise of Software Engineering Agents](https://openreview.net/forum?id=cKlzKs3Nnb) |  | 0 |  | Kexun Zhang, Weiran Yao, Zuxin Liu, Yihao Feng, Zhiwei Liu, Rithesh R. N., Tian Lan, Lei Li, Renze Lou, Jiacheng Xu, Bo Pang, Yingbo Zhou, Shelby Heinecke, Silvio Savarese, Huan Wang, Caiming Xiong |  |
| 1515 |  |  [Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon](https://openreview.net/forum?id=3E8YNv1HjU) |  | 0 |  | USVSN Sai Prashanth, Alvin Deng, Kyle O'Brien, Jyothir S. V, Mohammad Aflah Khan, Jaydeep Borkar, Christopher A. ChoquetteChoo, Jacob Ray Fuehne, Stella Biderman, Tracy Ke, Katherine Lee, Naomi Saphra |  |
| 1516 |  |  [Selective Label Enhancement Learning for Test-Time Adaptation](https://openreview.net/forum?id=3Z2flzXzBY) |  | 0 |  | Yihao Hu, Congyu Qiao, Xin Geng, Ning Xu |  |
| 1517 |  |  [MambaExtend: A Training-Free Approach to Improve Long Context Extension of Mamba](https://openreview.net/forum?id=LgzRo1RpLS) |  | 0 |  | Seyedarmin Azizi, Souvik Kundu, Mohammad Erfan Sadeghi, Massoud Pedram |  |
| 1518 |  |  [CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer](https://openreview.net/forum?id=LQzN6TRFg9) |  | 0 |  | Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, Jie Tang |  |
| 1519 |  |  [PeriodWave: Multi-Period Flow Matching for High-Fidelity Waveform Generation](https://openreview.net/forum?id=tQ1PmLfPBL) |  | 0 |  | SangHoon Lee, HaYeong Choi, SeongWhan Lee |  |
| 1520 |  |  [ADBM: Adversarial Diffusion Bridge Model for Reliable Adversarial Purification](https://openreview.net/forum?id=g0rnZeBguq) |  | 0 |  | Xiao Li, Wenxuan Sun, Huanran Chen, Qiongxiu Li, Yingzhe He, Jie Shi, Xiaolin Hu |  |
| 1521 |  |  [A Non-Contrastive Learning Framework for Sequential Recommendation with Preference-Preserving Profile Generation](https://openreview.net/forum?id=Ke2BEL4csm) |  | 0 |  | Huimin Zeng, Xiaojie Wang, Anoop Jain, Zhicheng Dou, Dong Wang |  |
| 1522 |  |  [Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models](https://openreview.net/forum?id=1GTARJhxtq) |  | 0 |  | Zachary Ankner, Cody Blakeney, Kartik Sreenivasan, Max Marion, Matthew L. Leavitt, Mansheej Paul |  |
| 1523 |  |  [Discriminator-Guided Embodied Planning for LLM Agent](https://openreview.net/forum?id=TjP1d8PP8l) |  | 0 |  | Haofu Qian, Chenjia Bai, Jiatao Zhang, Fei Wu, Wei Song, Xuelong Li |  |
| 1524 |  |  [MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning](https://openreview.net/forum?id=HVtu26XDAA) |  | 0 |  | Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, Sam Dodge, Keen You, Zhen Yang, Aleksei Timofeev, Mingze Xu, HongYou Chen, JeanPhilippe Fauconnier, Zhengfeng Lai, Haoxuan You, Zirui Wang, et al. |  |
| 1525 |  |  [Flow matching achieves almost minimax optimal convergence](https://openreview.net/forum?id=2OMyAFjiJJ) |  | 0 |  | Kenji Fukumizu, Taiji Suzuki, Noboru Isobe, Kazusato Oko, Masanori Koyama |  |
| 1526 |  |  [Pursuing Feature Separation based on Neural Collapse for Out-of-Distribution Detection](https://openreview.net/forum?id=mUXdysoxEP) |  | 0 |  | Yingwen Wu, Ruiji Yu, Xinwen Cheng, Zhengbao He, Xiaolin Huang |  |
| 1527 |  |  [MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models](https://openreview.net/forum?id=qIbbBSzH6n) |  | 0 |  | Chejian Xu, Jiawei Zhang, Zhaorun Chen, Chulin Xie, Mintong Kang, Yujin Potter, Zhun Wang, Zhuowen Yuan, Alexander Xiong, Zidi Xiong, Chenhui Zhang, Lingzhi Yuan, Yi Zeng, Peiyang Xu, Chengquan Guo, Andy Zhou, Jeffrey Ziwei Tan, Xuandong Zhao, Francesco Pinto, Zhen Xiang, et al. |  |
| 1528 |  |  [Ferret-UI 2: Mastering Universal User Interface Understanding Across Platforms](https://openreview.net/forum?id=GBfYgjOfSe) |  | 0 |  | Zhangheng Li, Keen You, Haotian Zhang, Di Feng, Harsh Agrawal, Xiujun Li, Mohana Prasad Sathya Moorthy, Jeffrey Nichols, Yinfei Yang, Zhe Gan |  |
| 1529 |  |  [MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models](https://openreview.net/forum?id=Usklli4gMc) |  | 0 |  | Wenbo Hu, JiaChen Gu, ZiYi Dou, Mohsen Fayyaz, Pan Lu, KaiWei Chang, Nanyun Peng |  |
| 1530 |  |  [Conflict-Averse Gradient Aggregation for Constrained Multi-Objective Reinforcement Learning](https://openreview.net/forum?id=ogXkmugNZw) |  | 0 |  | Dohyeong Kim, Mineui Hong, Jeongho Park, Songhwai Oh |  |
| 1531 |  |  [MotionDreamer: One-to-Many Motion Synthesis with Localized Generative Masked Transformer](https://openreview.net/forum?id=d23EVDRJ6g) |  | 0 |  | Yilin Wang, Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Xinxin Zuo, Juwei Lu, Hai Jiang, Li Cheng |  |
| 1532 |  |  [DynAlign: Unsupervised Dynamic Taxonomy Alignment for Cross-Domain Segmentation](https://openreview.net/forum?id=IdAyXxBud7) |  | 0 |  | Han Sun, Rui Gong, Ismail Nejjar, Olga Fink |  |
| 1533 |  |  [Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning](https://openreview.net/forum?id=09FiNmvNMw) |  | 0 |  | Hyun Ryu, Gyeongman Kim, Hyemin S. Lee, Eunho Yang |  |
| 1534 |  |  [To Tackle Adversarial Transferability: A Novel Ensemble Training Method with Fourier Transformation](https://openreview.net/forum?id=KW8yzAOIZr) |  | 0 |  | Wanlin Zhang, Weichen Lin, Ruomin Huang, Shihong Song, Hu Ding |  |
| 1535 |  |  [Manifold Induced Biases for Zero-shot and Few-shot Detection of Generated Images](https://openreview.net/forum?id=7gGl6HB5Zd) |  | 0 |  | Jonathan Brokman, Amit Giloni, Omer Hofman, Roman Vainshtein, Hisashi Kojima, Guy Gilboa |  |
| 1536 |  |  [Federated Domain Generalization with Data-free On-server Matching Gradient](https://openreview.net/forum?id=8TERgu1Lb2) |  | 0 |  | TrongBinh Nguyen, Duong Minh Nguyen, Jinsun Park, Viet Quoc Pham, WonJoo Hwang |  |
| 1537 |  |  [The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws](https://openreview.net/forum?id=ud8FtE1N4N) |  | 0 |  | Tian Jin, Ahmed Imtiaz Humayun, Utku Evci, Suvinay Subramanian, Amir Yazdanbakhsh, Dan Alistarh, Gintare Karolina Dziugaite |  |
| 1538 |  |  [Wayward Concepts In Multimodal Models](https://openreview.net/forum?id=74vnDs1R97) |  | 0 |  | Brandon Trabucco, Max Gurinas, Kyle Doherty, Russ Salakhutdinov |  |
| 1539 |  |  [MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer](https://openreview.net/forum?id=ExuBFYtCQU) |  | 0 |  | Yuancheng Wang, Haoyue Zhan, Liwei Liu, Ruihong Zeng, Haotian Guo, Jiachen Zheng, Qiang Zhang, Xueyao Zhang, Shunsi Zhang, Zhizheng Wu |  |
| 1540 |  |  [FedLWS: Federated Learning with Adaptive Layer-wise Weight Shrinking](https://openreview.net/forum?id=6RjQ54M1rM) |  | 0 |  | Changlong Shi, Jinmeng Li, He Zhao, Dandan Guo, Yi Chang |  |
| 1541 |  |  [Sail into the Headwind: Alignment via Robust Rewards and Dynamic Labels against Reward Hacking](https://openreview.net/forum?id=I8af9JdQTy) |  | 0 |  | Paria Rashidinejad, Yuandong Tian |  |
| 1542 |  |  [MA2E: Addressing Partial Observability in Multi-Agent Reinforcement Learning with Masked Auto-Encoder](https://openreview.net/forum?id=klpdEThT8q) |  | 0 |  | Sehyeok Kang, Yongsik Lee, Gahee Kim, Song Chong, SeYoung Yun |  |
| 1543 |  |  [Training Free Exponential Context Extension via Cascading KV Cache](https://openreview.net/forum?id=dSneEp59yX) |  | 0 |  | Jeffrey Willette, Heejun Lee, Youngwan Lee, Myeongjae Jeon, Sung Ju Hwang |  |
| 1544 |  |  [Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens](https://openreview.net/forum?id=jQP5o1VAVc) |  | 0 |  | Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, Yonglong Tian |  |
| 1545 |  |  [The AdEMAMix Optimizer: Better, Faster, Older](https://openreview.net/forum?id=jj7b3p5kLY) |  | 0 |  | Matteo Pagliardini, Pierre Ablin, David Grangier |  |
| 1546 |  |  [FIG: Flow with Interpolant Guidance for Linear Inverse Problems](https://openreview.net/forum?id=fs2Z2z3GRx) |  | 0 |  | Yici Yan, Yichi Zhang, Xiangming Meng, Zhizhen Zhao |  |
| 1547 |  |  [Scaling Long Context Training Data by Long-Distance Referrals](https://openreview.net/forum?id=tePFpDgyqg) |  | 0 |  | Yonghao Zhuang, Lanxiang Hu, Longfei Yun, Souvik Kundu, Zhengzhong Liu, Eric P. Xing, Hao Zhang |  |
| 1548 |  |  [Exploring Learning Complexity for Efficient Downstream Dataset Pruning](https://openreview.net/forum?id=FN7n7JRjsk) |  | 0 |  | Wenyu Jiang, Zhenlong Liu, Zejian Xie, Songxin Zhang, Bingyi Jing, Hongxin Wei |  |
| 1549 |  |  [Event-Driven Online Vertical Federated Learning](https://openreview.net/forum?id=FCBbh0HCrF) |  | 0 |  | Ganyu Wang, Boyu Wang, Bin Gu, Charles Ling |  |
| 1550 |  |  [DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References](https://openreview.net/forum?id=ajSmXqgS24) |  | 0 |  | Xueyi Liu, Jianibieke Adalibieke, Qianwei Han, Yuzhe Qin, Li Yi |  |
| 1551 |  |  [MELODI: Exploring Memory Compression for Long Contexts](https://openreview.net/forum?id=TvGPP8i18S) |  | 0 |  | Yinpeng Chen, DeLesley Hutchins, Aren Jansen, Andrey Zhmoginov, David Racz, Jesper Sparre Andersen |  |
| 1552 |  |  [CryoFM: A Flow-based Foundation Model for Cryo-EM Densities](https://openreview.net/forum?id=T4sMzjy7fO) |  | 0 |  | Yi Zhou, Yilai Li, Jing Yuan, Quanquan Gu |  |
| 1553 |  |  [Empowering LLM Agents with Zero-Shot Optimal Decision-Making through Q-learning](https://openreview.net/forum?id=JsVIGVntnQ) |  | 0 |  | Jiajun Chai, Sicheng Li, Yuqian Fu, Dongbin Zhao, Yuanheng Zhu |  |
| 1554 |  |  [Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA](https://openreview.net/forum?id=WwpYSOkkCt) |  | 0 |  | Sangmin Bae, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Seungyeon Kim, Tal Schuster |  |
| 1555 |  |  [Controllable Generation via Locally Constrained Resampling](https://openreview.net/forum?id=8g4XgC8HPF) |  | 0 |  | Kareem Ahmed, KaiWei Chang, Guy Van den Broeck |  |
| 1556 |  |  [Hot-pluggable Federated Learning: Bridging General and Personalized FL via Dynamic Selection](https://openreview.net/forum?id=B8akWa62Da) |  | 0 |  | Lei Shen, Zhenheng Tang, Lijun Wu, Yonggang Zhang, Xiaowen Chu, Tao Qin, Bo Han |  |
| 1557 |  |  [Stable Hadamard Memory: Revitalizing Memory-Augmented Agents for Reinforcement Learning](https://openreview.net/forum?id=We5z3UEnUY) |  | 0 |  | Hung Le, Dung Nguyen, Kien Do, Sunil Gupta, Svetha Venkatesh |  |
| 1558 |  |  [Lipschitz Bandits in Optimal Space](https://openreview.net/forum?id=i7k2sXSW1b) |  | 0 |  | Xiaoyi Zhu, Zengfeng Huang |  |
| 1559 |  |  [Fast and Slow Streams for Online Time Series Forecasting Without Information Leakage](https://openreview.net/forum?id=I0n3EyogMi) |  | 0 |  | Yingyee Ava Lau, Zhiwen Shao, DitYan Yeung |  |
| 1560 |  |  [Unlearning or Obfuscating? Jogging the Memory of Unlearned LLMs via Benign Relearning](https://openreview.net/forum?id=fMNRYBvcQN) |  | 0 |  | Shengyuan Hu, Yiwei Fu, Steven Z. Wu, Virginia Smith |  |
| 1561 |  |  [ToVE: Efficient Vision-Language Learning via Knowledge Transfer from Vision Experts](https://openreview.net/forum?id=EMMnAd3apQ) |  | 0 |  | Yuanchen Wu, Junlong Du, Ke Yan, Shouhong Ding, Xiaoqiang Li |  |
| 1562 |  |  [SaMer: A Scenario-aware Multi-dimensional Evaluator for Large Language Models](https://openreview.net/forum?id=aBnVU5DL3I) |  | 0 |  | Kehua Feng, Keyan Ding, Jing Yu, Yiwen Qu, Zhiwen Chen, Chengfei Lv, Gang Yu, Qiang Zhang, Huajun Chen |  |
| 1563 |  |  [Towards Hierarchical Rectified Flow](https://openreview.net/forum?id=6F6qwdycgJ) |  | 0 |  | Yichi Zhang, Yici Yan, Alexander G. Schwing, Zhizhen Zhao |  |
| 1564 |  |  [A Theoretical Framework for Partially-Observed Reward States in RLHF](https://openreview.net/forum?id=OjAU0LLDbe) |  | 0 |  | Chinmaya Kausik, Mirco Mutti, Aldo Pacchiano, Ambuj Tewari |  |
| 1565 |  |  [CONTRA: Conformal Prediction Region via Normalizing Flow Transformation](https://openreview.net/forum?id=pOO9cqLq7Q) |  | 0 |  | Zhenhan Fang, Aixin Tan, Jian Huang |  |
| 1566 |  |  [ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities](https://openreview.net/forum?id=cPD2hU35x3) |  | 0 |  | Peng Xu, Wei Ping, Xianchao Wu, Chejian Xu, Zihan Liu, Mohammad Shoeybi, Bryan Catanzaro |  |
| 1567 |  |  [Rethinking Artistic Copyright Infringements In the Era Of Text-to-Image Generative Models](https://openreview.net/forum?id=0OTVNEm9N4) |  | 0 |  | Mazda Moayeri, Sriram Balasubramanian, Samyadeep Basu, Priyatham Kattakinda, Atoosa Malemir Chegini, Robert Brauneis, Soheil Feizi |  |
| 1568 |  |  [Dysca: A Dynamic and Scalable Benchmark for Evaluating Perception Ability of LVLMs](https://openreview.net/forum?id=bU1JOvdXXK) |  | 0 |  | Jie Zhang, Zhongqi Wang, Mengqi Lei, Zheng Yuan, Bei Yan, Shiguang Shan, Xilin Chen |  |
| 1569 |  |  [Heavy-Tailed Diffusion Models](https://openreview.net/forum?id=tozlOEN4qp) |  | 0 |  | Kushagra Pandey, Jaideep Pathak, Yilun Xu, Stephan Mandt, Michael S. Pritchard, Arash Vahdat, Morteza Mardani |  |
| 1570 |  |  [Efficient Inference for Large Language Model-based Generative Recommendation](https://openreview.net/forum?id=ACSNlt77hq) |  | 0 |  | Xinyu Lin, Chaoqun Yang, Wenjie Wang, Yongqi Li, Cunxiao Du, Fuli Feng, SeeKiong Ng, TatSeng Chua |  |
| 1571 |  |  [Refine Knowledge of Large Language Models via Adaptive Contrastive Learning](https://openreview.net/forum?id=HqjRlT65WX) |  | 0 |  | Yinghui Li, Haojing Huang, Jiayi Kuang, Yangning Li, ShuYu Guo, Chao Qu, Xiaoyu Tan, HaiTao Zheng, Ying Shen, Philip S. Yu |  |
| 1572 |  |  [Dynamic Contrastive Skill Learning with State-Transition Based Skill Clustering and Dynamic Length Adjustment](https://openreview.net/forum?id=8egnwady4b) |  | 0 |  | Jinwoo Choi, SeungWoo Seo |  |
| 1573 |  |  [MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal Projection](https://openreview.net/forum?id=BQwsRy1h3U) |  | 0 |  | Bokai Lin, Zihao Zeng, Zipeng Xiao, Siqi Kou, TianQi Hou, Xiaofeng Gao, Hao Zhang, Zhijie Deng |  |
| 1574 |  |  [Do Stochastic, Feel Noiseless: Stable Stochastic Optimization via a Double Momentum Mechanism](https://openreview.net/forum?id=zCZnEXF3bN) |  | 0 |  | Tehila Dahan, Kfir Yehuda Levy |  |
| 1575 |  |  [Beyond Surface Structure: A Causal Assessment of LLMs' Comprehension ability](https://openreview.net/forum?id=gsShHPxkUW) |  | 0 |  | Yujin Han, Lei Xu, Sirui Chen, Difan Zou, Chaochao Lu |  |
| 1576 |  |  [Efficiently Parameterized Neural Metriplectic Systems](https://openreview.net/forum?id=uL1H29dM0c) |  | 0 |  | Anthony Gruber, Kookjin Lee, Haksoo Lim, Noseong Park, Nathaniel Trask |  |
| 1577 |  |  [Transformer Learns Optimal Variable Selection in Group-Sparse Classification](https://openreview.net/forum?id=fuoM5YDBX4) |  | 0 |  | Chenyang Zhang, Xuran Meng, Yuan Cao |  |
| 1578 |  |  [PaLD: Detection of Text Partially Written by Large Language Models](https://openreview.net/forum?id=rWjZWHYPcz) |  | 0 |  | Eric Lei, Hsiang Hsu, ChunFu Chen |  |
| 1579 |  |  [How Discrete and Continuous Diffusion Meet: Comprehensive Analysis of Discrete Diffusion Models via a Stochastic Integral Framework](https://openreview.net/forum?id=6awxwQEI82) |  | 0 |  | Yinuo Ren, Haoxuan Chen, Grant M. Rotskoff, Lexing Ying |  |
| 1580 |  |  [Balanced Ranking with Relative Centrality: A multi-core periphery perspective](https://openreview.net/forum?id=21rSeWJHPF) |  | 0 |  | Chandra Sekhar Mukherjee, Jiapeng Zhang |  |
| 1581 |  |  [Can Reinforcement Learning Solve Asymmetric Combinatorial-Continuous Zero-Sum Games?](https://openreview.net/forum?id=7YKV7zkNpX) |  | 0 |  | Yuheng Li, Panpan Wang, Haipeng Chen |  |
| 1582 |  |  [Learning Robust Representations with Long-Term Information for Generalization in Visual Reinforcement Learning](https://openreview.net/forum?id=PDtMrogheZ) |  | 0 |  | Rui Yang, Jie Wang, Qijie Peng, Ruibo Guo, Guoping Wu, Bin Li |  |
| 1583 |  |  [Zeroth-Order Fine-Tuning of LLMs with Transferable Static Sparsity](https://openreview.net/forum?id=myYzr50xBh) |  | 0 |  | Wentao Guo, Jikai Long, Yimeng Zeng, Zirui Liu, Xinyu Yang, Yide Ran, Jacob R. Gardner, Osbert Bastani, Christopher De Sa, Xiaodong Yu, Beidi Chen, Zhaozhuo Xu |  |
| 1584 |  |  [Bootstrapped Model Predictive Control](https://openreview.net/forum?id=i7jAYFYDcM) |  | 0 |  | Yuhang Wang, Hanwei Guo, Sizhe Wang, Long Qian, Xuguang Lan |  |
| 1585 |  |  [Group Ligands Docking to Protein Pockets](https://openreview.net/forum?id=zDC3iCBxJb) |  | 0 |  | Jiaqi Guan, Jiahan Li, Xiangxin Zhou, Xingang Peng, Sheng Wang, Yunan Luo, Jian Peng, Jianzhu Ma |  |
| 1586 |  |  [Revisiting Mode Connectivity in Neural Networks with Bezier Surface](https://openreview.net/forum?id=1NevL7zdHS) |  | 0 |  | Jie Ren, PinYu Chen, Ren Wang |  |
| 1587 |  |  [Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent](https://openreview.net/forum?id=VvDEuyVXkG) |  | 0 |  | Yangning Li, Yinghui Li, Xinyu Wang, Yong Jiang, Zhen Zhang, Xinran Zheng, Hui Wang, HaiTao Zheng, Fei Huang, Jingren Zhou, Philip S. Yu |  |
| 1588 |  |  [AdaGrad under Anisotropic Smoothness](https://openreview.net/forum?id=4GT9uTsAJE) |  | 0 |  | Yuxing Liu, Rui Pan, Tong Zhang |  |
| 1589 |  |  [Geometry of Long-Tailed Representation Learning: Rebalancing Features for Skewed Distributions](https://openreview.net/forum?id=GySIAKEwtZ) |  | 0 |  | Lingjie Yi, Jiachen Yao, Weimin Lyu, Haibin Ling, Raphael Douady, Chao Chen |  |
| 1590 |  |  [IMDPrompter: Adapting SAM to Image Manipulation Detection by Cross-View Automated Prompt Learning](https://openreview.net/forum?id=vl7kf0YHwj) |  | 0 |  | Quan Zhang, Yuxin Qi, Xi Tang, Jinwei Fang, Xi Lin, Ke Zhang, Chun Yuan |  |
| 1591 |  |  [Is Your Multimodal Language Model Oversensitive to Safe Queries?](https://openreview.net/forum?id=QsA3YzNUxA) |  | 0 |  | Xirui Li, Hengguang Zhou, Ruochen Wang, Tianyi Zhou, Minhao Cheng, ChoJui Hsieh |  |
| 1592 |  |  [Grounding by Trying: LLMs with Reinforcement Learning-Enhanced Retrieval](https://openreview.net/forum?id=BPAZ6yW3K7) |  | 0 |  | Sheryl Hsu, Omar Khattab, Chelsea Finn, Archit Sharma |  |
| 1593 |  |  [MMR: A Large-scale Benchmark Dataset for Multi-target and Multi-granularity Reasoning Segmentation](https://openreview.net/forum?id=mzL19kKE3r) |  | 0 |  | Donggon Jang, Yucheol Cho, Suin Lee, Taehyeon Kim, Daeshik Kim |  |
| 1594 |  |  [Meta-Continual Learning of Neural Fields](https://openreview.net/forum?id=OCpxDSn0G4) |  | 0 |  | Seungyoon Woo, Junhyeog Yun, Gunhee Kim |  |
| 1595 |  |  [Forewarned is Forearmed: Harnessing LLMs for Data Synthesis via Failure-induced Exploration](https://openreview.net/forum?id=yitH9xAHQs) |  | 0 |  | Qintong Li, Jiahui Gao, Sheng Wang, Renjie Pi, Xueliang Zhao, Chuan Wu, Xin Jiang, Zhenguo Li, Lingpeng Kong |  |
| 1596 |  |  [Needle In A Video Haystack: A Scalable Synthetic Evaluator for Video MLLMs](https://openreview.net/forum?id=ZJo6Radbqq) |  | 0 |  | Zijia Zhao, Haoyu Lu, Yuqi Huo, Yifan Du, Tongtian Yue, Longteng Guo, Bingning Wang, Weipeng Chen, Jing Liu |  |
| 1597 |  |  [Do Egocentric Video-Language Models Truly Understand Hand-Object Interactions?](https://openreview.net/forum?id=M8gXSFGkn2) |  | 0 |  | Boshen Xu, Ziheng Wang, Yang Du, Zhinan Song, Sipeng Zheng, Qin Jin |  |
| 1598 |  |  [Boosting Methods for Interval-censored Data with Regression and Classification](https://openreview.net/forum?id=DzbUL4AJPP) |  | 0 |  | Yuan Bian, Grace Y. Yi, Wenqing He |  |
| 1599 |  |  [ZETA: Leveraging Z-order Curves for Efficient Top-k Attention](https://openreview.net/forum?id=j9VVzueEbG) |  | 0 |  | Qiuhao Zeng, Jerry Huang, Peng Lu, Gezheng Xu, Boxing Chen, Charles Ling, Boyu Wang |  |
| 1600 |  |  [When Prompt Engineering Meets Software Engineering: CNL-P as Natural and Robust "APIs" for Human-AI Interaction](https://openreview.net/forum?id=SqoL14HDm0) |  | 0 |  | Zhenchang Xing, Yang Liu, Zhuo Cheng, Qing Huang, Dehai Zhao, Daniel Sun, Chenhua Liu |  |
| 1601 |  |  [DreamBench++: A Human-Aligned Benchmark for Personalized Image Generation](https://openreview.net/forum?id=4GSOESJrk6) |  | 0 |  | Yuang Peng, Yuxin Cui, Haomiao Tang, Zekun Qi, Runpei Dong, Jing Bai, Chunrui Han, Zheng Ge, Xiangyu Zhang, ShuTao Xia |  |
| 1602 |  |  [Data-centric Prediction Explanation via Kernelized Stein Discrepancy](https://openreview.net/forum?id=KlV5CkNQkl) |  | 0 |  | Mahtab Sarvmaili, Hassan Sajjad, Ga Wu |  |
| 1603 |  |  [Accurate and Scalable Graph Neural Networks via Message Invariance](https://openreview.net/forum?id=UqrFPhcmFp) |  | 0 |  | Zhihao Shi, Jie Wang, Zhiwei Zhuang, Xize Liang, Bin Li, Feng Wu |  |
| 1604 |  |  [SimXRD-4M: Big Simulated X-ray Diffraction Data and Crystal Symmetry Classification Benchmark](https://openreview.net/forum?id=mkuB677eMM) |  | 0 |  | Bin Cao, Yang Liu, Zinan Zheng, Ruifeng Tan, Jia Li, TongYi Zhang |  |
| 1605 |  |  [Human Simulacra: Benchmarking the Personification of Large Language Models](https://openreview.net/forum?id=BCP5nAHXqs) |  | 0 |  | Qiujie Xie, Qiming Feng, Tianqi Zhang, Qingqiu Li, Linyi Yang, Yuejie Zhang, Rui Feng, Liang He, Shang Gao, Yue Zhang |  |
| 1606 |  |  [Automatic Curriculum Expert Iteration for Reliable LLM Reasoning](https://openreview.net/forum?id=3ogIALgghF) |  | 0 |  | Zirui Zhao, Hanze Dong, Amrita Saha, Caiming Xiong, Doyen Sahoo |  |
| 1607 |  |  [Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning](https://openreview.net/forum?id=FiyS0ecSm0) |  | 0 |  | Zenan Li, Zhaoyu Li, Wen Tang, Xian Zhang, Yuan Yao, Xujie Si, Fan Yang, Kaiyu Yang, Xiaoxing Ma |  |
| 1608 |  |  [DUALFormer: Dual Graph Transformer](https://openreview.net/forum?id=4v4RcAODj9) |  | 0 |  | Jiaming Zhuo, Yuwei Liu, Yintong Lu, Ziyi Ma, Kun Fu, Chuan Wang, Yuanfang Guo, Zhen Wang, Xiaochun Cao, Liang Yang |  |
| 1609 |  |  [Progress or Regress? Self-Improvement Reversal in Post-training](https://openreview.net/forum?id=RFqeoVfLHa) |  | 0 |  | Ting Wu, Xuefeng Li, Pengfei Liu |  |
| 1610 |  |  [Reflexive Guidance: Improving OoDD in Vision-Language Models via Self-Guided Image-Adaptive Concept Generation](https://openreview.net/forum?id=R4h5PXzUuU) |  | 0 |  | Jihyo Kim, Seulbi Lee, Sangheum Hwang |  |
| 1611 |  |  [MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models](https://openreview.net/forum?id=6guG2OlXsr) |  | 0 |  | Pei Wang, Yanan Wu, Noah Wang, Jiaheng Liu, Xiaoshuai Song, Z. Y. Peng, Ken Deng, Chenchen Zhang, Jiakai Wang, Junran Peng, Ge Zhang, Hangyu Guo, Zhaoxiang Zhang, Wenbo Su, Bo Zheng |  |
| 1612 |  |  [The Crystal Ball Hypothesis in diffusion models: Anticipating object positions from initial noise](https://openreview.net/forum?id=GpdO9r73xT) |  | 0 |  | Yuanhao Ban, Ruochen Wang, Tianyi Zhou, Boqing Gong, ChoJui Hsieh, Minhao Cheng |  |
| 1613 |  |  [Pacmann: Efficient Private Approximate Nearest Neighbor Search](https://openreview.net/forum?id=yQcFniousM) |  | 0 |  | Mingxun Zhou, Elaine Shi, Giulia Fanti |  |
| 1614 |  |  [h4rm3l: A Language for Composable Jailbreak Attack Synthesis](https://openreview.net/forum?id=zZ8fgXHkXi) |  | 0 |  | Moussa Koulako Bala Doumbouya, Ananjan Nandi, Gabriel Poesia, Davide Ghilardi, Anna Goldie, Federico Bianchi, Dan Jurafsky, Christopher D. Manning |  |
| 1615 |  |  [Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models](https://openreview.net/forum?id=otW0TJOUYF) |  | 0 |  | Logan Matthew Cross, Violet Xiang, Agam Bhatia, Daniel L. K. Yamins, Nick Haber |  |
| 1616 |  |  [On Evaluating the Durability of Safeguards for Open-Weight LLMs](https://openreview.net/forum?id=fXJCqdUSVG) |  | 0 |  | Xiangyu Qi, Boyi Wei, Nicholas Carlini, Yangsibo Huang, Tinghao Xie, Luxi He, Matthew Jagielski, Milad Nasr, Prateek Mittal, Peter Henderson |  |
| 1617 |  |  [The "Law" of the Unconscious Contrastive Learner: Probabilistic Alignment of Unpaired Modalities](https://openreview.net/forum?id=DsIOUoZkVk) |  | 0 |  | Yongwei Che, Benjamin Eysenbach |  |
| 1618 |  |  [Graph Neural Preconditioners for Iterative Solutions of Sparse Linear Systems](https://openreview.net/forum?id=Tkkrm3pA35) |  | 0 |  | Jie Chen |  |
| 1619 |  |  [A Robust Method to Discover Causal or Anticausal Relation](https://openreview.net/forum?id=Q0s6kgrUMr) |  | 0 |  | Yu Yao, Yang Zhou, Bo Han, Mingming Gong, Kun Zhang, Tongliang Liu |  |
| 1620 |  |  [An Empirical Analysis of Uncertainty in Large Language Model Evaluations](https://openreview.net/forum?id=J4xLuCt2kg) |  | 0 |  | Qiujie Xie, Qingqiu Li, Zhuohao Yu, Yuejie Zhang, Yue Zhang, Linyi Yang |  |
| 1621 |  |  [On Large Language Model Continual Unlearning](https://openreview.net/forum?id=Essg9kb4yx) |  | 0 |  | Chongyang Gao, Lixu Wang, Kaize Ding, Chenkai Weng, Xiao Wang, Qi Zhu |  |
| 1622 |  |  [Reward Dimension Reduction for Scalable Multi-Objective Reinforcement Learning](https://openreview.net/forum?id=ssRdQimeUI) |  | 0 |  | Giseung Park, Youngchul Sung |  |
| 1623 |  |  [MuseGNN: Forming Scalable, Convergent GNN Layers that Minimize a Sampling-Based Energy](https://openreview.net/forum?id=Gq7RDMeZi4) |  | 0 |  | Haitian Jiang, Renjie Liu, Zengfeng Huang, Yichuan Wang, Xiao Yan, Zhenkun Cai, Minjie Wang, David Wipf |  |
| 1624 |  |  [Once-for-All: Controllable Generative Image Compression with Dynamic Granularity Adaptation](https://openreview.net/forum?id=z0hUsPhwUN) |  | 0 |  | Anqi Li, Feng Li, Yuxi Liu, Runmin Cong, Yao Zhao, Huihui Bai |  |
| 1625 |  |  [System 1.x: Learning to Balance Fast and Slow Planning with Language Models](https://openreview.net/forum?id=zd0iX5xBhA) |  | 0 |  | Swarnadeep Saha, Archiki Prasad, Justin ChihYao Chen, Peter Hase, Elias StengelEskin, Mohit Bansal |  |
| 1626 |  |  [Stochastic Semi-Gradient Descent for Learning Mean Field Games with Population-Aware Function Approximation](https://openreview.net/forum?id=tfO07iz0b9) |  | 0 |  | Chenyu Zhang, Xu Chen, Xuan Di |  |
| 1627 |  |  [Visually Consistent Hierarchical Image Classification](https://openreview.net/forum?id=7HEMpBTb3R) |  | 0 |  | Seulki Park, Youren Zhang, Stella X. Yu, Sara Beery, Jonathan Huang |  |
| 1628 |  |  [Second-Order Fine-Tuning without Pain for LLMs: A Hessian Informed Zeroth-Order Optimizer](https://openreview.net/forum?id=bEqI61iBue) |  | 0 |  | Yanjun Zhao, Sizhe Dang, Haishan Ye, Guang Dai, Yi Qian, Ivor W. Tsang |  |
| 1629 |  |  [Diverse Policies Recovering via Pointwise Mutual Information Weighted Imitation Learning](https://openreview.net/forum?id=6Ai8SuDsh3) |  | 0 |  | Hanlin Yang, Jian Yao, Weiming Liu, Qing Wang, Hanmin Qin, Hansheng Kong, Kirk Tang, Jiechao Xiong, Chao Yu, Kai Li, Junliang Xing, Hongwu Chen, Juchao Zhuo, Qiang Fu, Yang Wei, Haobo Fu |  |
| 1630 |  |  [An Effective Manifold-based Optimization Method for Distributionally Robust Classification](https://openreview.net/forum?id=nzjSvVZBIp) |  | 0 |  | Jiawei Huang, Hu Ding |  |
| 1631 |  |  [Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment](https://openreview.net/forum?id=h1XoHOd19I) |  | 0 |  | Jinhao Jiang, Junyi Li, Xin Zhao, Yang Song, Tao Zhang, JiRong Wen |  |
| 1632 |  |  [ACE: All-round Creator and Editor Following Instructions via Diffusion Transformer](https://openreview.net/forum?id=Bpn8q40n1n) |  | 0 |  | Zhen Han, Zeyinzi Jiang, Yulin Pan, Jingfeng Zhang, Chaojie Mao, ChenWei Xie, Yu Liu, Jingren Zhou |  |
| 1633 |  |  [Sequential Controlled Langevin Diffusions](https://openreview.net/forum?id=dImD2sgy86) |  | 0 |  | Junhua Chen, Lorenz Richter, Julius Berner, Denis Blessing, Gerhard Neumann, Anima Anandkumar |  |
| 1634 |  |  [HOPE for a Robust Parameterization of Long-memory State Space Models](https://openreview.net/forum?id=RZwtbg3qYD) |  | 0 |  | Annan Yu, Michael W. Mahoney, N. Benjamin Erichson |  |
| 1635 |  |  [Enhancing Uncertainty Estimation and Interpretability with Bayesian Non-negative Decision Layer](https://openreview.net/forum?id=xJXq6FkqEw) |  | 0 |  | Xinyue Hu, Zhibin Duan, Bo Chen, Mingyuan Zhou |  |
| 1636 |  |  [Decision Information Meets Large Language Models: The Future of Explainable Operations Research](https://openreview.net/forum?id=W2dR6rypBQ) |  | 0 |  | Yansen Zhang, Qingcan Kang, Wing Yin Yu, Hailei Gong, Xiaojin Fu, Xiongwei Han, Tao Zhong, Chen Ma |  |
| 1637 |  |  [Skill Expansion and Composition in Parameter Space](https://openreview.net/forum?id=GLWf2fq0bX) |  | 0 |  | Tenglong Liu, Jianxiong Li, Yinan Zheng, Haoyi Niu, Yixing Lan, Xin Xu, Xianyuan Zhan |  |
| 1638 |  |  [OMG: Opacity Matters in Material Modeling with Gaussian Splatting](https://openreview.net/forum?id=oeP6OL7ouB) |  | 0 |  | Silong Yong, Venkata Nagarjun Pudureddiyur Manivannan, Bernhard Kerbl, Zifu Wan, Simon Stepputtis, Katia P. Sycara, Yaqi Xie |  |
| 1639 |  |  [Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing](https://openreview.net/forum?id=Pnk7vMbznK) |  | 0 |  | Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, Bill Yuchen Lin |  |
| 1640 |  |  [Expected Sliced Transport Plans](https://openreview.net/forum?id=P7O1Vt1BdU) |  | 0 |  | Xinran Liu, Rocio Diaz Martin, Yikun Bai, Ashkan Shahbazi, Matthew Thorpe, Akram Aldroubi, Soheil Kolouri |  |
| 1641 |  |  [CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding & Reasoning Capabilities of CodeLLMs](https://openreview.net/forum?id=CahIEKCu5Q) |  | 0 |  | Dung Manh Nguyen, Thang Chau Phan, Nam Le Hai, TienThong Doan, Nam V. Nguyen, Quang Pham, Nghi D. Q. Bui |  |
| 1642 |  |  [MAGE: Model-Level Graph Neural Networks Explanations via Motif-based Graph Generation](https://openreview.net/forum?id=vue9P1Ypk6) |  | 0 |  | Zhaoning Yu, Hongyang Gao |  |
| 1643 |  |  [Prevalence of Negative Transfer in Continual Reinforcement Learning: Analyses and a Simple Baseline](https://openreview.net/forum?id=KAIqwkB3dT) |  | 0 |  | Hongjoon Ahn, Jinu Hyeon, Youngmin Oh, Bosun Hwang, Taesup Moon |  |
| 1644 |  |  [Shedding Light on Time Series Classification using Interpretability Gated Networks](https://openreview.net/forum?id=n34taxF0TC) |  | 0 |  | Yunshi Wen, Tengfei Ma, Ronny Luss, Debarun Bhattacharjya, Achille Fokoue, Anak Agung Julius |  |
| 1645 |  |  [Towards Neural Scaling Laws for Time Series Foundation Models](https://openreview.net/forum?id=uCqxDfLYrB) |  | 0 |  | Qingren Yao, ChaoHan Huck Yang, Renhe Jiang, Yuxuan Liang, Ming Jin, Shirui Pan |  |
| 1646 |  |  [Leveraging Variable Sparsity to Refine Pareto Stationarity in Multi-Objective Optimization](https://openreview.net/forum?id=Bl3e8HV9xW) |  | 0 |  | Zeou Hu, Yaoliang Yu |  |
| 1647 |  |  [OptiBench Meets ReSocratic: Measure and Improve LLMs for Optimization Modeling](https://openreview.net/forum?id=fsDZwS49uY) |  | 0 |  | Zhicheng Yang, Yiwei Wang, Yinya Huang, Zhijiang Guo, Wei Shi, Xiongwei Han, Liang Feng, Linqi Song, Xiaodan Liang, Jing Tang |  |
| 1648 |  |  [Test-time Adaptation for Regression by Subspace Alignment](https://openreview.net/forum?id=SXtl7NRyE5) |  | 0 |  | Kazuki Adachi, Shin'ya Yamaguchi, Atsutoshi Kumagai, Tomoki Hamagami |  |
| 1649 |  |  [Prediction Risk and Estimation Risk of the Ridgeless Least Squares Estimator under General Assumptions on Regression Errors](https://openreview.net/forum?id=AsAy7CROLs) |  | 0 |  | Sungyoon Lee, Sokbae Lee |  |
| 1650 |  |  [Advancing LLM Reasoning Generalists with Preference Trees](https://openreview.net/forum?id=2ea5TNVR0c) |  | 0 |  | Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Boji Shan, Zeyuan Liu, Jia Deng, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, Maosong Sun |  |
| 1651 |  |  [Pedestrian Motion Reconstruction: A Large-scale Benchmark via Mixed Reality Rendering with Multiple Perspectives and Modalities](https://openreview.net/forum?id=YOpa6dTrpt) |  | 0 |  | Yichen Wang, Yiyi Zhang, Xinhao Hu, Li Niu, Jianfu Zhang, Yasushi Makihara, Yasushi Yagi, Pai Peng, Wenlong Liao, Tao He, Junchi Yan, Liqing Zhang |  |
| 1652 |  |  [Generalizing Reasoning Problems to Longer Lengths](https://openreview.net/forum?id=zpENPcQSj1) |  | 0 |  | Changnan Xiao, Bing Liu |  |
| 1653 |  |  [DeeperForward: Enhanced Forward-Forward Training for Deeper and Better Performance](https://openreview.net/forum?id=kOYnXVQCtA) |  | 0 |  | Liang Sun, Yang Zhang, Weizhao He, Jiajun Wen, Linlin Shen, Weicheng Xie |  |
| 1654 |  |  [The Directionality of Optimization Trajectories in Neural Networks](https://openreview.net/forum?id=JY6P45sFDS) |  | 0 |  | Sidak Pal Singh, Bobby He, Thomas Hofmann, Bernhard Schölkopf |  |
| 1655 |  |  [T-Stitch: Accelerating Sampling in Pre-Trained Diffusion Models with Trajectory Stitching](https://openreview.net/forum?id=2mqb8bPHeb) |  | 0 |  | Zizheng Pan, Bohan Zhuang, DeAn Huang, Weili Nie, Zhiding Yu, Chaowei Xiao, Jianfei Cai, Anima Anandkumar |  |
| 1656 |  |  [ReNovo: Retrieval-Based \emph{De Novo} Mass Spectrometry Peptide Sequencing](https://openreview.net/forum?id=uQnvYP7yX9) |  | 0 |  | Shaorong Chen, Jun Xia, Jingbo Zhou, Lecheng Zhang, Zhangyang Gao, Bozhen Hu, Cheng Tan, Wenjie Du, Stan Z. Li |  |
| 1657 |  |  [Hotspot-Driven Peptide Design via Multi-Fragment Autoregressive Extension](https://openreview.net/forum?id=jqmptcSNVG) |  | 0 |  | Jiahan Li, Tong Chen, Shitong Luo, Chaoran Cheng, Jiaqi Guan, Ruihan Guo, Sheng Wang, Ge Liu, Jian Peng, Jianzhu Ma |  |
| 1658 |  |  [Learning How Hard to Think: Input-Adaptive Allocation of LM Computation](https://openreview.net/forum?id=6qUUgw9bAZ) |  | 0 |  | Mehul Damani, Idan Shenfeld, Andi Peng, Andreea Bobu, Jacob Andreas |  |
| 1659 |  |  [Examining Alignment of Large Language Models through Representative Heuristics: the case of political stereotypes](https://openreview.net/forum?id=7LGmXXZXtP) |  | 0 |  | Sullam Jeoung, Yubin Ge, Haohan Wang, Jana Diesner |  |
| 1660 |  |  [Learning-Augmented Search Data Structures](https://openreview.net/forum?id=N4rYbQowE3) |  | 0 |  | Chunkai Fu, Brandon G. Nguyen, Jung Hoon Seo, Ryan S. Zesch, Samson Zhou |  |
| 1661 |  |  [Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models](https://openreview.net/forum?id=TWnUgSAWNw) |  | 0 |  | Zhengfeng Lai, Vasileios Saveris, Chen Chen, HongYou Chen, Haotian Zhang, Bowen Zhang, Wenze Hu, Juan Lao Tebar, Zhe Gan, Peter Grasch, Meng Cao, Yinfei Yang |  |
| 1662 |  |  [Advancing Graph Generation through Beta Diffusion](https://openreview.net/forum?id=x1An5a3U9I) |  | 0 |  | Xinyang Liu, Yilin He, Bo Chen, Mingyuan Zhou |  |
| 1663 |  |  [Accelerated Over-Relaxation Heavy-Ball Method: Achieving Global Accelerated Convergence with Broad Generalization](https://openreview.net/forum?id=SWEqzy7IQB) |  | 0 |  | Jingrong Wei, Long Chen |  |
| 1664 |  |  [Investigating the Pre-Training Dynamics of In-Context Learning: Task Recognition vs. Task Learning](https://openreview.net/forum?id=htDczodFN5) |  | 0 |  | Xiaolei Wang, Xinyu Tang, Junyi Li, Xin Zhao, JiRong Wen |  |
| 1665 |  |  [UniWav: Towards Unified Pre-training for Speech Representation Learning and Generation](https://openreview.net/forum?id=yj9lLwMjnE) |  | 0 |  | Alexander H. Liu, Sanggil Lee, ChaoHan Huck Yang, Yuan Gong, YuChiang Frank Wang, James R. Glass, Rafael Valle, Bryan Catanzaro |  |
| 1666 |  |  [Weighted Multi-Prompt Learning with Description-free Large Language Model Distillation](https://openreview.net/forum?id=NDLmZZWATc) |  | 0 |  | Sua Lee, Kyubum Shin, Jung Ho Park |  |
| 1667 |  |  [Multiple Heads are Better than One: Mixture of Modality Knowledge Experts for Entity Representation Learning](https://openreview.net/forum?id=ue1Tt3h1VC) |  | 0 |  | Yichi Zhang, Zhuo Chen, Lingbing Guo, Yajing Xu, Binbin Hu, Ziqi Liu, Wen Zhang, Huajun Chen |  |
| 1668 |  |  [Contrastive Learning from Synthetic Audio Doppelgängers](https://openreview.net/forum?id=XRtyVELwr6) |  | 0 |  | Manuel Cherep, Nikhil Singh |  |
| 1669 |  |  [MetaMetrics: Calibrating Metrics for Generation Tasks Using Human Preferences](https://openreview.net/forum?id=slO3xTt4CG) |  | 0 |  | Genta Indra Winata, David Anugraha, Lucky Susanto, Garry Kuwanto, Derry Tanti Wijaya |  |
| 1670 |  |  [Confidence Elicitation: A New Attack Vector for Large Language Models](https://openreview.net/forum?id=aTYexOYlLb) |  | 0 |  | Brian Formento, ChuanSheng Foo, SeeKiong Ng |  |
| 1671 |  |  [AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large Language Models](https://openreview.net/forum?id=jTEKTdI3K9) |  | 0 |  | Kim SungBin, Oh HyunBin, JungMok Lee, Arda Senocak, Joon Son Chung, TaeHyun Oh |  |
| 1672 |  |  [Looking Inward: Language Models Can Learn About Themselves by Introspection](https://openreview.net/forum?id=eb5pkwIB5i) |  | 0 |  | Felix Jedidja Binder, James Chua, Tomek Korbak, Henry Sleight, John Hughes, Robert Long, Ethan Perez, Miles Turpin, Owain Evans |  |
| 1673 |  |  [Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts](https://openreview.net/forum?id=IDJUscOjM3) |  | 0 |  | Junmo Kang, Leonid Karlinsky, Hongyin Luo, Zhen Wang, Jacob A. Hansen, James R. Glass, David Daniel Cox, Rameswar Panda, Rogério Feris, Alan Ritter |  |
| 1674 |  |  [AssembleFlow: Rigid Flow Matching with Inertial Frames for Molecular Assembly](https://openreview.net/forum?id=jckKNzYYA6) |  | 0 |  | Hongyu Guo, Yoshua Bengio, Shengchao Liu |  |
| 1675 |  |  [The Semantic Hub Hypothesis: Language Models Share Semantic Representations Across Languages and Modalities](https://openreview.net/forum?id=FrFQpAgnGE) |  | 0 |  | Zhaofeng Wu, Xinyan Velocity Yu, Dani Yogatama, Jiasen Lu, Yoon Kim |  |
| 1676 |  |  [PWM: Policy Learning with Multi-Task World Models](https://openreview.net/forum?id=hOELrZfg0J) |  | 0 |  | Ignat Georgiev, Varun Giridhar, Nicklas Hansen, Animesh Garg |  |
| 1677 |  |  [Improved Diffusion-based Generative Model with Better Adversarial Robustness](https://openreview.net/forum?id=1DVgysiIt7) |  | 0 |  | Zekun Wang, Mingyang Yi, Shuchen Xue, Zhenguo Li, Ming Liu, Bing Qin, Zhiming Ma |  |
| 1678 |  |  [Self-Improving Robust Preference Optimization](https://openreview.net/forum?id=ZSdubdbOoi) |  | 0 |  | Eugene Choi, Arash Ahmadian, Matthieu Geist, Olivier Pietquin, Mohammad Gheshlaghi Azar |  |
| 1679 |  |  [EC-DIT: Scaling Diffusion Transformers with Adaptive Expert-Choice Routing](https://openreview.net/forum?id=PxlfzEePC0) |  | 0 |  | Haotian Sun, Tao Lei, Bowen Zhang, Yanghao Li, Haoshuo Huang, Ruoming Pang, Bo Dai, Nan Du |  |
| 1680 |  |  [Fine-Tuning Discrete Diffusion Models via Reward Optimization with Applications to DNA and Protein Design](https://openreview.net/forum?id=G328D1xt4W) |  | 0 |  | Chenyu Wang, Masatoshi Uehara, Yichun He, Amy Wang, Avantika Lal, Tommi S. Jaakkola, Sergey Levine, Aviv Regev, Hanchen Wang, Tommaso Biancalani |  |
| 1681 |  |  [BAMDP Shaping: a Unified Framework for Intrinsic Motivation and Reward Shaping](https://openreview.net/forum?id=tijmpS9Vy2) |  | 0 |  | Aly Lidayan, Michael D. Dennis, Stuart Russell |  |
| 1682 |  |  [Kernel-based Optimally Weighted Conformal Time-Series Prediction](https://openreview.net/forum?id=oP7arLOWix) |  | 0 |  | Jonghyeok Lee, Chen Xu, Yao Xie |  |
| 1683 |  |  [Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed GFlowNets](https://openreview.net/forum?id=Aye5wL6TCn) |  | 0 |  | Zhen Liu, Tim Z. Xiao, Weiyang Liu, Yoshua Bengio, Dinghuai Zhang |  |
| 1684 |  |  [Block Verification Accelerates Speculative Decoding](https://openreview.net/forum?id=frsg32u0rO) |  | 0 |  | Ziteng Sun, Uri Mendlovic, Yaniv Leviathan, Asaf Aharoni, Jae Hun Ro, Ahmad Beirami, Ananda Theertha Suresh |  |
| 1685 |  |  [Revisiting Source-Free Domain Adaptation: a New Perspective via Uncertainty Control](https://openreview.net/forum?id=nx9Z5Kva96) |  | 0 |  | Gezheng Xu, Hui Guo, Li Yi, Charles Ling, Boyu Wang, Grace Yi |  |
| 1686 |  |  [Generalizable Motion Planning via Operator Learning](https://openreview.net/forum?id=UYcUpiULmT) |  | 0 |  | Sharath Matada, Luke Bhan, Yuanyuan Shi, Nikolay Atanasov |  |
| 1687 |  |  [Modeling Unseen Environments with Language-guided Composable Causal Components in Reinforcement Learning](https://openreview.net/forum?id=XMgpnZ2ET7) |  | 0 |  | Xinyue Wang, Biwei Huang |  |
| 1688 |  |  [Doubly robust identification of treatment effects from multiple environments](https://openreview.net/forum?id=9vTAkJ9Tik) |  | 0 |  | Piersilvio De Bartolomeis, Julia Kostin, Javier Abad, Yixin Wang, Fanny Yang |  |
| 1689 |  |  [Variational Search Distributions](https://openreview.net/forum?id=1vrpdV9U3i) |  | 0 |  | Daniel M. Steinberg, Rafael Oliveira, Cheng Soon Ong, Edwin V. Bonilla |  |
| 1690 |  |  [Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language Models](https://openreview.net/forum?id=77gQUdQhE7) |  | 0 |  | Yinlam Chow, Guy Tennenholtz, Izzeddin Gur, Vincent Zhuang, Bo Dai, Aviral Kumar, Rishabh Agarwal, Sridhar Thiagarajan, Craig Boutilier, Aleksandra Faust |  |
| 1691 |  |  [Neural Spacetimes for DAG Representation Learning](https://openreview.net/forum?id=skGSOcrIj7) |  | 0 |  | Haitz Sáez de Ocáriz Borde, Anastasis Kratsios, Marc T. Law, Xiaowen Dong, Michael M. Bronstein |  |
| 1692 |  |  [Data-adaptive Differentially Private Prompt Synthesis for In-Context Learning](https://openreview.net/forum?id=sVNfWhtaJC) |  | 0 |  | Fengyu Gao, Ruida Zhou, Tianhao Wang, Cong Shen, Jing Yang |  |
| 1693 |  |  [Towards Bridging Generalization and Expressivity of Graph Neural Networks](https://openreview.net/forum?id=BOQpRtI4F5) |  | 0 |  | Shouheng Li, Floris Geerts, Dongwoo Kim, Qing Wang |  |
| 1694 |  |  [Intermediate Layer Classifiers for OOD generalization](https://openreview.net/forum?id=ByCV9xWfNK) |  | 0 |  | Arnas Uselis, Seong Joon Oh |  |
| 1695 |  |  [Lightweight Predictive 3D Gaussian Splats](https://openreview.net/forum?id=PbheqxnO1e) |  | 0 |  | Junli Cao, Vidit Goel, Chaoyang Wang, Anil Kag, Ju Hu, Sergei Korolev, Chenfanfu Jiang, Sergey Tulyakov, Jian Ren |  |
| 1696 |  |  [IntersectionZoo: Eco-driving for Benchmarking Multi-Agent Contextual Reinforcement Learning](https://openreview.net/forum?id=XoulHHQGFi) |  | 0 |  | Vindula Jayawardana, Baptiste Freydt, Ao Qu, Cameron Hickert, Zhongxia Yan, Cathy Wu |  |
| 1697 |  |  [SPDIM: Source-Free Unsupervised Conditional and Label Shift Adaptation in EEG](https://openreview.net/forum?id=CoQw1dXtGb) |  | 0 |  | Shanglin Li, Motoaki Kawanabe, Reinmar J. Kobler |  |
| 1698 |  |  [CoMRes: Semi-Supervised Time Series Forecasting Utilizing Consensus Promotion of Multi-Resolution](https://openreview.net/forum?id=bRa4JLPzii) |  | 0 |  | Yunju Cho, JayYoon Lee |  |
| 1699 |  |  [Transformer Block Coupling and its Correlation with Generalization in LLMs](https://openreview.net/forum?id=kvLenbZZgg) |  | 0 |  | Murdock Aubry, Haoming Meng, Anton Sugolov, Vardan Papyan |  |
| 1700 |  |  [Reconciling Model Multiplicity for Downstream Decision Making](https://openreview.net/forum?id=uy4EavBEwl) |  | 0 |  | Ally Yalei Du, Dung Daniel T. Ngo, Zhiwei Steven Wu |  |
| 1701 |  |  [Robust System Identification: Finite-sample Guarantees and Connection to Regularization](https://openreview.net/forum?id=ZNnmcddaB3) |  | 0 |  | Hyuk Park, Grani A. Hanasusanto, Yingying Li |  |
| 1702 |  |  [eQMARL: Entangled Quantum Multi-Agent Reinforcement Learning for Distributed Cooperation over Quantum Channels](https://openreview.net/forum?id=cR5GTis5II) |  | 0 |  | Alexander C. DeRieux, Walid Saad |  |
| 1703 |  |  [E(n) Equivariant Topological Neural Networks](https://openreview.net/forum?id=Ax3uliEBVR) |  | 0 |  | Claudio Battiloro, Ege Karaismailoglu, Mauricio Tec, George Dasoulas, Michelle Audirac, Francesca Dominici |  |
| 1704 |  |  [Tamper-Resistant Safeguards for Open-Weight LLMs](https://openreview.net/forum?id=4FIjRodbW6) |  | 0 |  | Rishub Tamirisa, Bhrugu Bharathi, Long Phan, Andy Zhou, Alice Gatti, Tarun Suresh, Maxwell Lin, Justin Wang, Rowan Wang, Ron Arel, Andy Zou, Dawn Song, Bo Li, Dan Hendrycks, Mantas Mazeika |  |
| 1705 |  |  [Beyond Worst-Case Dimensionality Reduction for Sparse Vectors](https://openreview.net/forum?id=e8qXTxMgPg) |  | 0 |  | Sandeep Silwal, David P. Woodruff, Qiuyi Zhang |  |
| 1706 |  |  [Steering Large Language Models between Code Execution and Textual Reasoning](https://openreview.net/forum?id=5X5Z7Ffrjb) |  | 0 |  | Yongchao Chen, Harsh Jhamtani, Srinagesh Sharma, Chuchu Fan, Chi Wang |  |
| 1707 |  |  [Misspecified Q-Learning with Sparse Linear Function Approximation: Tight Bounds on Approximation Error](https://openreview.net/forum?id=nIEjY4a2Lf) |  | 0 |  | Ally Yalei Du, Lin Yang, Ruosong Wang |  |
| 1708 |  |  [RankSHAP: Shapley Value Based Feature Attributions for Learning to Rank](https://openreview.net/forum?id=4011PUI9vm) |  | 0 |  | Tanya Chowdhury, Yair Zick, James Allan |  |
| 1709 |  |  [Machine Unlearning Fails to Remove Data Poisoning Attacks](https://openreview.net/forum?id=HaX48yksVL) |  | 0 |  | Martin Pawelczyk, Jimmy Z. Di, Yiwei Lu, Gautam Kamath, Ayush Sekhari, Seth Neel |  |
| 1710 |  |  [Residual-MPPI: Online Policy Customization for Continuous Control](https://openreview.net/forum?id=gVnJFY8nCM) |  | 0 |  | Pengcheng Wang, Chenran Li, Catherine Weaver, Kenta Kawamoto, Masayoshi Tomizuka, Chen Tang, Wei Zhan |  |
| 1711 |  |  [MCNC: Manifold-Constrained Reparameterization for Neural Compression](https://openreview.net/forum?id=VMV8gefvq8) |  | 0 |  | Chayne Thrash, Reed Andreas, Ali Abbasi, Parsa Nooralinejad, Soroush Abbasi Koohpayegani, Hamed Pirsiavash, Soheil Kolouri |  |
| 1712 |  |  [HQ-Edit: A High-Quality Dataset for Instruction-based Image Editing](https://openreview.net/forum?id=mZptYYttFj) |  | 0 |  | Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Cihang Xie, Yuyin Zhou |  |
| 1713 |  |  [Last-Iterate Convergence Properties of Regret-Matching Algorithms in Games](https://openreview.net/forum?id=LWeVVPuIx0) |  | 0 |  | Yang Cai, Gabriele Farina, Julien GrandClément, Christian Kroer, ChungWei Lee, Haipeng Luo, Weiqiang Zheng |  |
| 1714 |  |  [HARDMath: A Benchmark Dataset for Challenging Problems in Applied Mathematics](https://openreview.net/forum?id=nDTvP6tBMd) |  | 0 |  | Jingxuan Fan, Sarah Martinson, Erik Y. Wang, Kaylie Hausknecht, Jonah Brenner, Danxian Liu, Nianli Peng, Corey Wang, Michael P. Brenner |  |
| 1715 |  |  [Provable weak-to-strong generalization via benign overfitting](https://openreview.net/forum?id=4vzGQcVUG8) |  | 0 |  | David Xing Wu, Anant Sahai |  |
| 1716 |  |  [Toward Efficient Multi-Agent Exploration With Trajectory Entropy Maximization](https://openreview.net/forum?id=YvKJGYL4j7) |  | 0 |  | Tianxu Li, Kun Zhu |  |
| 1717 |  |  [Law of the Weakest Link: Cross Capabilities of Large Language Models](https://openreview.net/forum?id=TljGdvzFq2) |  | 0 |  | Ming Zhong, Aston Zhang, Xuewei Wang, Rui Hou, Wenhan Xiong, Chenguang Zhu, Zhengxing Chen, Liang Tan, Chloe Bi, Mike Lewis, Sravya Popuri, Sharan Narang, Melanie Kambadur, Dhruv Mahajan, Sergey Edunov, Jiawei Han, Laurens van der Maaten |  |
| 1718 |  |  [BenTo: Benchmark Reduction with In-Context Transferability](https://openreview.net/forum?id=ki7b0qD11r) |  | 0 |  | Hongyu Zhao, Ming Li, Lichao Sun, Tianyi Zhou |  |
| 1719 |  |  [Round and Round We Go! What makes Rotary Positional Encodings useful?](https://openreview.net/forum?id=GtvuNrk58a) |  | 0 |  | Federico Barbero, Alex Vitvitskyi, Christos Perivolaropoulos, Razvan Pascanu, Petar Velickovic |  |
| 1720 |  |  [On the self-verification limitations of large language models on reasoning and planning tasks](https://openreview.net/forum?id=4O0v4s3IzY) |  | 0 |  | Kaya Stechly, Karthik Valmeekam, Subbarao Kambhampati |  |
| 1721 |  |  [Broadening Target Distributions for Accelerated Diffusion Models via a Novel Analysis Approach](https://openreview.net/forum?id=reZKq6hjOZ) |  | 0 |  | Yuchen Liang, Peizhong Ju, Yingbin Liang, Ness B. Shroff |  |
| 1722 |  |  [Single-agent Poisoning Attacks Suffice to Ruin Multi-Agent Learning](https://openreview.net/forum?id=46xYl55hdc) |  | 0 |  | Fan Yao, Yuwei Cheng, Ermin Wei, Haifeng Xu |  |
| 1723 |  |  [Improving Pretraining Data Using Perplexity Correlations](https://openreview.net/forum?id=huuKoVQnB0) |  | 0 |  | Tristan Thrush, Christopher Potts, Tatsunori Hashimoto |  |
| 1724 |  |  [Self-Play Preference Optimization for Language Model Alignment](https://openreview.net/forum?id=a3PmRgAB5T) |  | 0 |  | Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, Quanquan Gu |  |
| 1725 |  |  [Protein Language Model Fitness is a Matter of Preference](https://openreview.net/forum?id=UvPdpa4LuV) |  | 0 |  | Cade W. Gordon, Amy X. Lu, Pieter Abbeel |  |
| 1726 |  |  [LoLCATs: On Low-Rank Linearizing of Large Language Models](https://openreview.net/forum?id=8VtGeyJyx9) |  | 0 |  | Michael Zhang, Simran Arora, Rahul Chalamala, Benjamin Frederick Spector, Alan Wu, Krithik Ramesh, Aaryan Singhal, Christopher Ré |  |
| 1727 |  |  [Flow Matching with Gaussian Process Priors for Probabilistic Time Series Forecasting](https://openreview.net/forum?id=uxVBbSlKQ4) |  | 0 |  | Marcel Kollovieh, Marten Lienen, David Lüdke, Leo Schwinn, Stephan Günnemann |  |
| 1728 |  |  [Robust Feature Learning for Multi-Index Models in High Dimensions](https://openreview.net/forum?id=aKkDY1Wca0) |  | 0 |  | Alireza Mousavi Hosseini, Adel Javanmard, Murat A. Erdogdu |  |
| 1729 |  |  [AdaIR: Adaptive All-in-One Image Restoration via Frequency Mining and Modulation](https://openreview.net/forum?id=M5t0WvjfCg) |  | 0 |  | Yuning Cui, Syed Waqas Zamir, Salman H. Khan, Alois Knoll, Mubarak Shah, Fahad Shahbaz Khan |  |
| 1730 |  |  [Discretization-invariance? On the Discretization Mismatch Errors in Neural Operators](https://openreview.net/forum?id=J9FgrqOOni) |  | 0 |  | Wenhan Gao, Ruichen Xu, Yuefan Deng, Yi Liu |  |
| 1731 |  |  [(Mis)Fitting Scaling Laws: A Survey of Scaling Law Fitting Techniques in Deep Learning](https://openreview.net/forum?id=xI71dsS3o4) |  | 0 |  | Margaret Li, Sneha Kudugunta, Luke Zettlemoyer |  |
| 1732 |  |  [Semantic Aware Representation Learning for Lifelong Learning](https://openreview.net/forum?id=WwwJfkGq0G) |  | 0 |  | Fahad Sarfraz, Elahe Arani, Bahram Zonooz |  |
| 1733 |  |  [Singular Subspace Perturbation Bounds via Rectangular Random Matrix Diffusions](https://openreview.net/forum?id=G8U2nGP3Vi) |  | 0 |  | Peiyao Lai, Oren Mangoubi |  |
| 1734 |  |  [LeanAgent: Lifelong Learning for Formal Theorem Proving](https://openreview.net/forum?id=Uo4EHT4ZZ8) |  | 0 |  | Adarsh Kumarappan, Mo Tiwari, Peiyang Song, Robert Joseph George, Chaowei Xiao, Anima Anandkumar |  |
| 1735 |  |  [Multilevel Generative Samplers for Investigating Critical Phenomena](https://openreview.net/forum?id=YcUV5apdlq) |  | 0 |  | Ankur Singha, Elia Cellini, Kim Andrea Nicoli, Karl Jansen, Stefan Kühn, Shinichi Nakajima |  |
| 1736 |  |  [Certifying Counterfactual Bias in LLMs](https://openreview.net/forum?id=HQHnhVQznF) |  | 0 |  | Isha Chaudhary, Qian Hu, Manoj Kumar, Morteza Ziyadi, Rahul Gupta, Gagandeep Singh |  |
| 1737 |  |  [Not All Heads Matter: A Head-Level KV Cache Compression Method with Integrated Retrieval and Reasoning](https://openreview.net/forum?id=FJFVmeXusW) |  | 0 |  | Yu Fu, Zefan Cai, Abedelkadir Asi, Wayne Xiong, Yue Dong, Wen Xiao |  |
| 1738 |  |  [Preble: Efficient Distributed Prompt Scheduling for LLM Serving](https://openreview.net/forum?id=meKEKDhdnx) |  | 0 |  | Vikranth Srivatsa, Zijian He, Reyna Abhyankar, Dongming Li, Yiying Zhang |  |
| 1739 |  |  [Sufficient Context: A New Lens on Retrieval Augmented Generation Systems](https://openreview.net/forum?id=Jjr2Odj8DJ) |  | 0 |  | Hailey Joren, Jianyi Zhang, ChunSung Ferng, DaCheng Juan, Ankur Taly, Cyrus Rashtchian |  |
| 1740 |  |  [Exploratory Preference Optimization: Harnessing Implicit Q\*-Approximation for Sample-Efficient RLHF](https://openreview.net/forum?id=QYigQ6gXNw) |  | 0 |  | Tengyang Xie, Dylan J. Foster, Akshay Krishnamurthy, Corby Rosset, Ahmed Hassan Awadallah, Alexander Rakhlin |  |
| 1741 |  |  [MoLEx: Mixture of Layer Experts for Fine-tuning with Sparse Upcycling](https://openreview.net/forum?id=rWui9vLhOc) |  | 0 |  | Rachel S. Y. Teo, Tan Minh Nguyen |  |
| 1742 |  |  [Faster Diffusion Sampling with Randomized Midpoints: Sequential and Parallel](https://openreview.net/forum?id=MT3aOfXIbY) |  | 0 |  | Shivam Gupta, Linda Cai, Sitan Chen |  |
| 1743 |  |  [nGPT: Normalized Transformer with Representation Learning on the Hypersphere](https://openreview.net/forum?id=se4vjm7h4E) |  | 0 |  | Ilya Loshchilov, ChengPing Hsieh, Simeng Sun, Boris Ginsburg |  |
| 1744 |  |  [Pushing the Limits of All-Atom Geometric Graph Neural Networks: Pre-Training, Scaling, and Zero-Shot Transfer](https://openreview.net/forum?id=4S2L519nIX) |  | 0 |  | Zihan Pengmei, Zhengyuan Shen, Zichen Wang, Marcus D. Collins, Huzefa Rangwala |  |
| 1745 |  |  [MuPT: A Generative Symbolic Music Pretrained Transformer](https://openreview.net/forum?id=iAK9oHp4Zz) |  | 0 |  | Xingwei Qu, Yuelin Bai, Yinghao Ma, Ziya Zhou, Ka Man Lo, Jiaheng Liu, Ruibin Yuan, Lejun Min, Xueling Liu, Tianyu Zhang, Xeron Du, Shuyue Guo, Yiming Liang, Yizhi Li, Shangda Wu, Junting Zhou, Tianyu Zheng, Ziyang Ma, Fengze Han, Wei Xue, et al. |  |
| 1746 |  |  [Improving Instruction-Following in Language Models through Activation Steering](https://openreview.net/forum?id=wozhdnRCtw) |  | 0 |  | Alessandro Stolfo, Vidhisha Balachandran, Safoora Yousefi, Eric Horvitz, Besmira Nushi |  |
| 1747 |  |  [Functional Homotopy: Smoothing Discrete Optimization via Continuous Parameters for LLM Jailbreak Attacks](https://openreview.net/forum?id=uhaLuZcCjH) |  | 0 |  | Zi Wang, Divyam Anshumaan, Ashish Hooda, Yudong Chen, Somesh Jha |  |
| 1748 |  |  [Forte : Finding Outliers with Representation Typicality Estimation](https://openreview.net/forum?id=7XNgVPxCiA) |  | 0 |  | Debargha Ganguly, Warren Richard Morningstar, Andrew Seohwan Yu, Vipin Chaudhary |  |
| 1749 |  |  [NarrativeBridge: Enhancing Video Captioning with Causal-Temporal Narrative](https://openreview.net/forum?id=bBoetBIN2R) |  | 0 |  | Asmar Nadeem, Faegheh Sardari, Robert Dawes, Syed Sameed Husain, Adrian Hilton, Armin Mustafa |  |
| 1750 |  |  [Provable unlearning in topic modeling and downstream tasks](https://openreview.net/forum?id=dh78yRFVK9) |  | 0 |  | Stanley Wei, Sadhika Malladi, Sanjeev Arora, Amartya Sanyal |  |
| 1751 |  |  [Point-based Instance Completion with Scene Constraints](https://openreview.net/forum?id=llSiIJosDj) |  | 0 |  | Wesley Khademi, Fuxin Li |  |
| 1752 |  |  [FACTS: A Factored State-Space Framework for World Modelling](https://openreview.net/forum?id=dmCGjPFVhF) |  | 0 |  | Nanbo Li, Firas Laakom, Yucheng Xu, Wenyi Wang, Jürgen Schmidhuber |  |
| 1753 |  |  [6D Object Pose Tracking in Internet Videos for Robotic Manipulation](https://openreview.net/forum?id=1CIUkpoata) |  | 0 |  | Georgy Ponimatkin, Martin Cífka, Tomás Soucek, Médéric Fourmy, Yann Labbé, Vladimír Petrík, Josef Sivic |  |
| 1754 |  |  [Towards Foundation Models for Mixed Integer Linear Programming](https://openreview.net/forum?id=6yENDA7J4G) |  | 0 |  | Sirui Li, Janardhan Kulkarni, Ishai Menache, Cathy Wu, Beibin Li |  |
| 1755 |  |  [A Generic Framework for Conformal Fairness](https://openreview.net/forum?id=xiQNfYl33p) |  | 0 |  | Aditya T. Vadlamani, Anutam Srinivasan, Pranav Maneriker, Ali Payani, Srinivasan Parthasarathy |  |
| 1756 |  |  [Concept Bottleneck Large Language Models](https://openreview.net/forum?id=RC5FPYVQaH) |  | 0 |  | ChungEn Sun, Tuomas P. Oikarinen, Berk Ustun, TsuiWei Weng |  |
| 1757 |  |  [Dual Process Learning: Controlling Use of In-Context vs. In-Weights Strategies with Weight Forgetting](https://openreview.net/forum?id=jDsmB4o5S0) |  | 0 |  | Suraj Anand, Michael A. Lepori, Jack Merullo, Ellie Pavlick |  |
| 1758 |  |  [Near-Exact Privacy Amplification for Matrix Mechanisms](https://openreview.net/forum?id=txV4dNeusx) |  | 0 |  | Christopher A. ChoquetteChoo, Arun Ganesh, Saminul Haque, Thomas Steinke, Abhradeep Guha Thakurta |  |
| 1759 |  |  [Improving Semantic Understanding in Speech Language Models via Brain-tuning](https://openreview.net/forum?id=KL8Sm4xRn7) |  | 0 |  | Omer Moussa, Dietrich Klakow, Mariya Toneva |  |
| 1760 |  |  [BitStack: Any-Size Compression of Large Language Models in Variable Memory Environments](https://openreview.net/forum?id=lBntjGbyv0) |  | 0 |  | Xinghao Wang, Pengyu Wang, Bo Wang, Dong Zhang, Yunhua Zhou, Xipeng Qiu |  |
| 1761 |  |  [ACES: Automatic Cohort Extraction System for Event-Stream Datasets](https://openreview.net/forum?id=P4XmKjXTrM) |  | 0 |  | Justin Xu, Jack Gallifant, Alistair E. W. Johnson, Matthew B. A. McDermott |  |
| 1762 |  |  [Almost Optimal Batch-Regret Tradeoff for Batch Linear Contextual Bandits](https://openreview.net/forum?id=rakhNY32vw) |  | 0 |  | Zihan Zhang, Xiangyang Ji, Yuan Zhou |  |
| 1763 |  |  [Can We Talk Models Into Seeing the World Differently?](https://openreview.net/forum?id=iVMcYxTiVM) |  | 0 |  | Paul Gavrikov, Jovita Lukasik, Steffen Jung, Robert Geirhos, Muhammad Jehanzeb Mirza, Margret Keuper, Janis Keuper |  |
| 1764 |  |  [Vision CNNs trained to estimate spatial latents learned similar ventral-stream-aligned representations](https://openreview.net/forum?id=emMMa4q0qw) |  | 0 |  | Yudi Xie, Weichen Huang, Esther Alter, Jeremy Schwartz, Joshua B. Tenenbaum, James J. DiCarlo |  |
| 1765 |  |  [HAINAN: Fast and Accurate Transducer for Hybrid-Autoregressive ASR](https://openreview.net/forum?id=LrmPGtnros) |  | 0 |  | Hainan Xu, Travis M. Bartley, Vladimir Bataev, Boris Ginsburg |  |
| 1766 |  |  [Inference Optimal VLMs Need Fewer Visual Tokens and More Parameters](https://openreview.net/forum?id=6VhDQP7WGX) |  | 0 |  | Kevin Y. Li, Sachin Goyal, João D. Semedo, J. Zico Kolter |  |
| 1767 |  |  [TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention](https://openreview.net/forum?id=EkfLaCJ7bk) |  | 0 |  | Lijie Yang, Zhihao Zhang, Zhuofu Chen, Zikun Li, Zhihao Jia |  |
| 1768 |  |  [MediConfusion: Can you trust your AI radiologist? Probing the reliability of multimodal medical foundation models](https://openreview.net/forum?id=H9UnNgdq0g) |  | 0 |  | Mohammad Shahab Sepehri, Zalan Fabian, Maryam Soltanolkotabi, Mahdi Soltanolkotabi |  |
| 1769 |  |  [Mutual Effort for Efficiency: A Similarity-based Token Pruning for Vision Transformers in Self-Supervised Learning](https://openreview.net/forum?id=GTcEe5fayC) |  | 0 |  | Sheng Li, Qitao Tan, Yue Dai, Zhenglun Kong, Tianyu Wang, Jun Liu, Ao Li, Ninghao Liu, Yufei Ding, Xulong Tang, Geng Yuan |  |
| 1770 |  |  [Towards counterfactual fairness through auxiliary variables](https://openreview.net/forum?id=GpUv1FvZi1) |  | 0 |  | Bowei Tian, Ziyao Wang, Shwai He, Wanghao Ye, Guoheng Sun, Yucong Dai, Yongkai Wu, Ang Li |  |
| 1771 |  |  [LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language Models for Referring Expression Comprehension](https://openreview.net/forum?id=PgXpOOqtyd) |  | 0 |  | Amaia Cardiel, Eloi Zablocki, Elias Ramzi, Oriane Siméoni, Matthieu Cord |  |
| 1772 |  |  [Conformalized Interactive Imitation Learning: Handling Expert Shift and Intermittent Feedback](https://openreview.net/forum?id=Ym2RNPX6la) |  | 0 |  | Michelle D. Zhao, Henny Admoni, Reid G. Simmons, Aaditya Ramdas, Andrea Bajcsy |  |
| 1773 |  |  [Bayesian Regularization of Latent Representation](https://openreview.net/forum?id=VOoJEQlLW5) |  | 0 |  | Chukwudi Paul Obite, Zhi Chang, Keyan Wu, Shiwei Lan |  |
| 1774 |  |  [AnoLLM: Large Language Models for Tabular Anomaly Detection](https://openreview.net/forum?id=7VkHffT5X2) |  | 0 |  | ChePing Tsai, Ganyu Teng, Phillip Wallis, Wei Ding |  |
| 1775 |  |  [GPS: A Probabilistic Distributional Similarity with Gumbel Priors for Set-to-Set Matching](https://openreview.net/forum?id=U0SijGsCHJ) |  | 0 |  | Ziming Zhang, Fangzhou Lin, Haotian Liu, Jose Morales, Haichong Zhang, Kazunori D. Yamada, Vijaya B. Kolachalama, Venkatesh Saligrama |  |
| 1776 |  |  [Learning Graph Quantized Tokenizers](https://openreview.net/forum?id=oYSsbY3G4o) |  | 0 |  | Limei Wang, Kaveh Hassani, Si Zhang, Dongqi Fu, Baichuan Yuan, Weilin Cong, Zhigang Hua, Hao Wu, Ning Yao, Bo Long |  |
| 1777 |  |  [Bridging the Gap Between f-divergences and Bayes Hilbert Spaces](https://openreview.net/forum?id=m5qpn0KTMZ) |  | 0 |  | Linus Lach, Alexander Willi Fottner, Yarema Okhrin |  |
| 1778 |  |  [SBSC: Step-by-Step Coding for Improving Mathematical Olympiad Performance](https://openreview.net/forum?id=wSkvf2WyYz) |  | 0 |  | Kunal Singh, Ankan Biswas, Sayandeep Bhowmick, Pradeep Moturi, Siva Kishore Gollapalli |  |
| 1779 |  |  [Transformers Can Learn Temporal Difference Methods for In-Context Reinforcement Learning](https://openreview.net/forum?id=Pj06mxCXPl) |  | 0 |  | Jiuqi Wang, Ethan Blaser, Hadi Daneshmand, Shangtong Zhang |  |
| 1780 |  |  [Balancing Act: Diversity and Consistency in Large Language Model Ensembles](https://openreview.net/forum?id=Dl6nkKKvlX) |  | 0 |  | Ahmed Abdulaal, Chen Jin, Nina Montaña Brown, Aryo Pradipta Gema, Daniel C. Castro, Daniel C. Alexander, Philip Alexander Teare, Tom Diethe, Dino Oglic, Amrutha Saseendran |  |
| 1781 |  |  [Improved Algorithms for Kernel Matrix-Vector Multiplication Under Sparsity Assumptions](https://openreview.net/forum?id=wLnls9LS3x) |  | 0 |  | Piotr Indyk, Michael Kapralov, Kshiteej Sheth, Tal Wagner |  |
| 1782 |  |  [NextBestPath: Efficient 3D Mapping of Unseen Environments](https://openreview.net/forum?id=7WaRh4gCXp) |  | 0 |  | Shiyao Li, Antoine Guédon, Clémentin Boittiaux, Shizhe Chen, Vincent Lepetit |  |
| 1783 |  |  [Is Large-scale Pretraining the Secret to Good Domain Generalization?](https://openreview.net/forum?id=wCOJpXm0Me) |  | 0 |  | Piotr Teterwak, Kuniaki Saito, Theodoros Tsiligkaridis, Bryan A. Plummer, Kate Saenko |  |
| 1784 |  |  [Grokking at the Edge of Numerical Stability](https://openreview.net/forum?id=TvfkSyHZRA) |  | 0 |  | Lucas Prieto, Melih Barsbey, Pedro A. M. Mediano, Tolga Birdal |  |
| 1785 |  |  [Learning from Imperfect Human Feedback: A Tale from Corruption-Robust Dueling](https://openreview.net/forum?id=ptjrpEGrGg) |  | 0 |  | Yuwei Cheng, Fan Yao, Xuefeng Liu, Haifeng Xu |  |
| 1786 |  |  [Linear Combination of Saved Checkpoints Makes Consistency and Diffusion Models Better](https://openreview.net/forum?id=QowsEic1sc) |  | 0 |  | Enshu Liu, Junyi Zhu, Zinan Lin, Xuefei Ning, Shuaiqi Wang, Matthew B. Blaschko, Sergey Yekhanin, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang |  |
| 1787 |  |  [Scalable Bayesian Learning with posteriors](https://openreview.net/forum?id=fifXzmzeGy) |  | 0 |  | Samuel Duffield, Kaelan Donatella, Johnathan Chiu, Phoebe Klett, Daniel Simpson |  |
| 1788 |  |  [On the Convergence of No-Regret Dynamics in Information Retrieval Games with Proportional Ranking Functions](https://openreview.net/forum?id=jJXZvPe5z0) |  | 0 |  | Omer Madmon, Idan Pipano, Itamar Reinman, Moshe Tennenholtz |  |
| 1789 |  |  [MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos](https://openreview.net/forum?id=tRNKe2Vgqt) |  | 0 |  | Xuehai He, Weixi Feng, Kaizhi Zheng, Yujie Lu, Wanrong Zhu, Jiachen Li, Yue Fan, Jianfeng Wang, Linjie Li, Zhengyuan Yang, Kevin Lin, William Yang Wang, Lijuan Wang, Xin Eric Wang |  |
| 1790 |  |  [PolyNet: Learning Diverse Solution Strategies for Neural Combinatorial Optimization](https://openreview.net/forum?id=TKuYWeFE6S) |  | 0 |  | André Hottung, Mridul Mahajan, Kevin Tierney |  |
| 1791 |  |  [MetaOOD: Automatic Selection of OOD Detection Models](https://openreview.net/forum?id=9qpdDiDQ2H) |  | 0 |  | Yuehan Qin, Yichi Zhang, Yi Nian, Xueying Ding, Yue Zhao |  |
| 1792 |  |  [MGDA Converges under Generalized Smoothness, Provably](https://openreview.net/forum?id=wgDB1QuxIA) |  | 0 |  | Qi Zhang, Peiyao Xiao, Shaofeng Zou, Kaiyi Ji |  |
| 1793 |  |  [Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models](https://openreview.net/forum?id=FhTAG591Ve) |  | 0 |  | Michael Noukhovitch, Shengyi Huang, Sophie Xhonneux, Arian Hosseini, Rishabh Agarwal, Aaron C. Courville |  |
| 1794 |  |  [RepoGraph: Enhancing AI Software Engineering with Repository-level Code Graph](https://openreview.net/forum?id=dw9VUsSHGB) |  | 0 |  | Siru Ouyang, Wenhao Yu, Kaixin Ma, Zilin Xiao, Zhihan Zhang, Mengzhao Jia, Jiawei Han, Hongming Zhang, Dong Yu |  |
| 1795 |  |  [On the Price of Differential Privacy for Hierarchical Clustering](https://openreview.net/forum?id=yLhJYvkKA0) |  | 0 |  | Chengyuan Deng, Jie Gao, Jalaj Upadhyay, Chen Wang, Samson Zhou |  |
| 1796 |  |  [GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-Time Alignment](https://openreview.net/forum?id=J0qTpmbSbh) |  | 0 |  | Yuancheng Xu, Udari Madhushani Sehwag, Alec Koppel, Sicheng Zhu, Bang An, Furong Huang, Sumitra Ganesh |  |
| 1797 |  |  [Unlocking Guidance for Discrete State-Space Diffusion and Flow Models](https://openreview.net/forum?id=XsgHl54yO7) |  | 0 |  | Hunter Nisonoff, Junhao Xiong, Stephan Allenspach, Jennifer Listgarten |  |
| 1798 |  |  [NeurFlow: Interpreting Neural Networks through Neuron Groups and Functional Interactions](https://openreview.net/forum?id=GdbQyFOUlJ) |  | 0 |  | Tue Minh Cao, Nhat HoangXuan, Hieu H. Pham, Phi Le Nguyen, My T. Thai |  |
| 1799 |  |  [Beyond FVD: An Enhanced Evaluation Metrics for Video Generation Distribution Quality](https://openreview.net/forum?id=cC3LxGZasH) |  | 0 |  | Ge Ya Luo, Gian Mario Favero, Zhi Hao Luo, Alexia JolicoeurMartineau, Christopher Pal |  |
| 1800 |  |  [MAVIS: Mathematical Visual Instruction Tuning with an Automatic Data Engine](https://openreview.net/forum?id=MnJzJ2gvuf) |  | 0 |  | Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Ziyu Guo, Yichi Zhang, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Shanghang Zhang, Peng Gao, Hongsheng Li |  |
| 1801 |  |  [LLaRA: Supercharging Robot Learning Data for Vision-Language Policy](https://openreview.net/forum?id=iVxxgZlXh6) |  | 0 |  | Xiang Li, Cristina Mata, Jongwoo Park, Kumara Kahatapitiya, Yoo Sung Jang, Jinghuan Shang, Kanchana Ranasinghe, Ryan D. Burgert, Mu Cai, Yong Jae Lee, Michael S. Ryoo |  |
| 1802 |  |  [Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning](https://openreview.net/forum?id=44CoQe6VCq) |  | 0 |  | Bahare Fatemi, Mehran Kazemi, Anton Tsitsulin, Karishma Malkan, Jinyeong Yim, John Palowitch, Sungyong Seo, Jonathan Halcrow, Bryan Perozzi |  |
| 1803 |  |  [Efficient stagewise pretraining via progressive subnetworks](https://openreview.net/forum?id=Y5LjYI4N6P) |  | 0 |  | Abhishek Panigrahi, Nikunj Saunshi, Kaifeng Lyu, Sobhan Miryoosefi, Sashank J. Reddi, Satyen Kale, Sanjiv Kumar |  |
| 1804 |  |  [Better than Your Teacher: LLM Agents that learn from Privileged AI Feedback](https://openreview.net/forum?id=st7XqFgbAH) |  | 0 |  | Sanjiban Choudhury, Paloma Sodhi |  |
| 1805 |  |  [MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding](https://openreview.net/forum?id=TrVYEZtSQH) |  | 0 |  | Fei Wang, Xingyu Fu, James Y. Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, Tianyi Lorena Yan, Wenjie Jacky Mo, HsiangHui Liu, Pan Lu, Chunyuan Li, Chaowei Xiao, KaiWei Chang, Dan Roth, Sheng Zhang, Hoifung Poon, Muhao Chen |  |
| 1806 |  |  [Online Clustering with Nearly Optimal Consistency](https://openreview.net/forum?id=NA2vUMaMOm) |  | 0 |  | T.H. Hubert Chan, Shaofeng H.C. Jiang, Tianyi Wu, Mengshi Zhao |  |
| 1807 |  |  [Graph Transformers Dream of Electric Flow](https://openreview.net/forum?id=rWQDzq3O5c) |  | 0 |  | Xiang Cheng, Lawrence Carin, Suvrit Sra |  |
| 1808 |  |  [Doubly Optimal Policy Evaluation for Reinforcement Learning](https://openreview.net/forum?id=60GeEoG5kD) |  | 0 |  | Shuze Daniel Liu, Claire Chen, Shangtong Zhang |  |
| 1809 |  |  [SoundCTM: Unifying Score-based and Consistency Models for Full-band Text-to-Sound Generation](https://openreview.net/forum?id=KrK6zXbjfO) |  | 0 |  | Koichi Saito, Dongjun Kim, Takashi Shibuya, ChiehHsin Lai, Zhi Zhong, Yuhta Takida, Yuki Mitsufuji |  |
| 1810 |  |  [Breaking the Reclustering Barrier in Centroid-based Deep Clustering](https://openreview.net/forum?id=r01fcKhzT5) |  | 0 |  | Lukas Miklautz, Timo Klein, Kevin Sidak, Collin Leiber, Thomas Lang, Andrii Shkabrii, Sebastian Tschiatschek, Claudia Plant |  |
| 1811 |  |  [The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for Open-Ended Text Generation](https://openreview.net/forum?id=Ij9ilPh36h) |  | 0 |  | Fredrik Carlsson, Fangyu Liu, Daniel Ward, Murathan Kurfali, Joakim Nivre |  |
| 1812 |  |  [Learning a Fast Mixing Exogenous Block MDP using a Single Trajectory](https://openreview.net/forum?id=41WIgfdd5o) |  | 0 |  | Alexander Levine, Peter Stone, Amy Zhang |  |
| 1813 |  |  [DataGen: Unified Synthetic Dataset Generation via Large Language Models](https://openreview.net/forum?id=F5R0lG74Tu) |  | 0 |  | Yue Huang, Siyuan Wu, Chujie Gao, Dongping Chen, Qihui Zhang, Yao Wan, Tianyi Zhou, Chaowei Xiao, Jianfeng Gao, Lichao Sun, Xiangliang Zhang |  |
| 1814 |  |  [Language-Image Models with 3D Understanding](https://openreview.net/forum?id=yaQbTAD2JJ) |  | 0 |  | Jang Hyun Cho, Boris Ivanovic, Yulong Cao, Edward Schmerling, Yue Wang, Xinshuo Weng, Boyi Li, Yurong You, Philipp Krähenbühl, Yan Wang, Marco Pavone |  |
| 1815 |  |  [Differentially Private Federated Learning with Time-Adaptive Privacy Spending](https://openreview.net/forum?id=W0nydevOlG) |  | 0 |  | Shahrzad Kiani, Nupur Kulkarni, Adam Dziedzic, Stark C. Draper, Franziska Boenisch |  |
| 1816 |  |  [Expressivity of Neural Networks with Random Weights and Learned Biases](https://openreview.net/forum?id=5xwx1Myosu) |  | 0 |  | Ezekiel Williams, Alexandre Payeur, Avery HeeWoon Ryoo, Thomas Jiralerspong, Matthew G. Perich, Luca Mazzucato, Guillaume Lajoie |  |
| 1817 |  |  [PADRe: A Unifying Polynomial Attention Drop-in Replacement for Efficient Vision Transformer](https://openreview.net/forum?id=YFxfcQMLWX) |  | 0 |  | PierreDavid Letourneau, Manish Kumar Singh, HsinPai Cheng, Shizhong Han, Yunxiao Shi, Dalton Jones, Matthew Harper Langston, Hong Cai, Fatih Porikli |  |
| 1818 |  |  [Frequency-Guided Masking for Enhanced Vision Self-Supervised Learning](https://openreview.net/forum?id=VmJdqhuTCh) |  | 0 |  | Amin Karimi Monsefi, Mengxi Zhou, Nastaran Karimi Monsefi, SerNam Lim, WeiLun Chao, Rajiv Ramnath |  |
| 1819 |  |  [Generalization v.s. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data](https://openreview.net/forum?id=IQxBDLmVpT) |  | 0 |  | Xinyi Wang, Antonis Antoniades, Yanai Elazar, Alfonso Amayuelas, Alon Albalak, Kexun Zhang, William Yang Wang |  |
| 1820 |  |  [Transformers Handle Endogeneity in In-Context Linear Regression](https://openreview.net/forum?id=QfhU3ZC2g1) |  | 0 |  | Haodong Liang, Krishna Balasubramanian, Lifeng Lai |  |
| 1821 |  |  [How Does Critical Batch Size Scale in Pre-training?](https://openreview.net/forum?id=JCiF03qnmi) |  | 0 |  | Hanlin Zhang, Depen Morwani, Nikhil Vyas, Jingfeng Wu, Difan Zou, Udaya Ghai, Dean P. Foster, Sham M. Kakade |  |
| 1822 |  |  [Comparing Targeting Strategies for Maximizing Social Welfare with Limited Resources](https://openreview.net/forum?id=0iscEAo2xB) |  | 0 |  | Vibhhu Sharma, Bryan Wilder |  |
| 1823 |  |  [Federated Granger Causality Learning For Interdependent Clients With State Space Representation](https://openreview.net/forum?id=KTgQGXz5xj) |  | 0 |  | Ayush Mohanty, Nazal Mohamed, Paritosh Ramanan, Nagi Gebraeel |  |
| 1824 |  |  [Convex Formulations for Training Two-Layer ReLU Neural Networks](https://openreview.net/forum?id=e0X9l4kecx) |  | 0 |  | Karthik Prakhya, Tolga Birdal, Alp Yurtsever |  |
| 1825 |  |  [Beyond Linear Approximations: A Novel Pruning Approach for Attention Matrix](https://openreview.net/forum?id=sgbI8Pxwie) |  | 0 |  | Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song, Yufa Zhou |  |
| 1826 |  |  [Closed-Form Merging of Parameter-Efficient Modules for Federated Continual Learning](https://openreview.net/forum?id=ROpY0qRUXL) |  | 0 |  | Riccardo Salami, Pietro Buzzega, Matteo Mosconi, Jacopo Bonato, Luigi Sabetta, Simone Calderara |  |
| 1827 |  |  [Eliminating Position Bias of Language Models: A Mechanistic Approach](https://openreview.net/forum?id=fvkElsJOsN) |  | 0 |  | Ziqi Wang, Hanlin Zhang, Xiner Li, KuanHao Huang, Chi Han, Shuiwang Ji, Sham M. Kakade, Hao Peng, Heng Ji |  |
| 1828 |  |  [From Tokens to Lattices: Emergent Lattice Structures in Language Models](https://openreview.net/forum?id=md9qolJwLl) |  | 0 |  | Bo Xiong, Steffen Staab |  |
| 1829 |  |  [Demystifying Topological Message-Passing with Relational Structures: A Case Study on Oversquashing in Simplicial Message-Passing](https://openreview.net/forum?id=QC2qE1tcmd) |  | 0 |  | Diaaeldin Taha, James Chapman, Marzieh Eidi, Karel Devriendt, Guido Montúfar |  |
| 1830 |  |  [Generalized Consistency Trajectory Models for Image Manipulation](https://openreview.net/forum?id=Zjv38dg1Hb) |  | 0 |  | Beomsu Kim, Jaemin Kim, Jeongsol Kim, Jong Chul Ye |  |
| 1831 |  |  [Fair Submodular Cover](https://openreview.net/forum?id=ULorFBST6X) |  | 0 |  | Wenjing Chen, Shuo Xing, Samson Zhou, Victoria G. Crawford |  |
| 1832 |  |  [Revealing the 3D Cosmic Web through Gravitationally Constrained Neural Fields](https://openreview.net/forum?id=Ax0i933gtp) |  | 0 |  | Brandon Zhao, Aviad Levis, Liam Connor, Pratul P. Srinivasan, Katherine L. Bouman |  |
| 1833 |  |  [Swing-by Dynamics in Concept Learning and Compositional Generalization](https://openreview.net/forum?id=s1zO0YBEF8) |  | 0 |  | Yongyi Yang, Core Francisco Park, Ekdeep Singh Lubana, Maya Okawa, Wei Hu, Hidenori Tanaka |  |
| 1834 |  |  [MotherNet: Fast Training and Inference via Hyper-Network Transformers](https://openreview.net/forum?id=6H4jRWKFc3) |  | 0 |  | Andreas C. Mueller, Carlo Curino, Raghu Ramakrishnan |  |
| 1835 |  |  [AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents](https://openreview.net/forum?id=AC5n7xHuR1) |  | 0 |  | Maksym Andriushchenko, Alexandra Souly, Mateusz Dziemian, Derek Duenas, Maxwell Lin, Justin Wang, Dan Hendrycks, Andy Zou, J. Zico Kolter, Matt Fredrikson, Yarin Gal, Xander Davies |  |
| 1836 |  |  [GTR: Improving Large 3D Reconstruction Models through Geometry and Texture Refinement](https://openreview.net/forum?id=Oxpkn0YLG1) |  | 0 |  | Peiye Zhuang, Songfang Han, Chaoyang Wang, Aliaksandr Siarohin, Jiaxu Zou, Michael Vasilkovsky, Vladislav Shakhrai, Sergei Korolev, Sergey Tulyakov, HsinYing Lee |  |
| 1837 |  |  [InstantPortrait: One-Step Portrait Editing via Diffusion Multi-Objective Distillation](https://openreview.net/forum?id=ZkFMe3OPfw) |  | 0 |  | Zhixin Lai, Keqiang Sun, FuYun Wang, Dhritiman Sagar, Erli Ding |  |
| 1838 |  |  [GPUDrive: Data-driven, multi-agent driving simulation at 1 million FPS](https://openreview.net/forum?id=ERv8ptegFi) |  | 0 |  | Saman Kazemkhani, Aarav Pandya, Daphne Cornelisse, Brennan Shacklett, Eugene Vinitsky |  |
| 1839 |  |  [Hierarchical Uncertainty Estimation for Learning-based Registration in Neuroimaging](https://openreview.net/forum?id=w8LMtFY97b) |  | 0 |  | Xiaoling Hu, Karthik Gopinath, Peirong Liu, Malte Hoffmann, Koen Van Leemput, Oula Puonti, Juan Eugenio Iglesias |  |
| 1840 |  |  [When LLMs Play the Telephone Game: Cultural Attractors as Conceptual Tools to Evaluate LLMs in Multi-turn Settings](https://openreview.net/forum?id=fN8yLc3eA7) |  | 0 |  | Jérémy Perez, Grgur Kovac, Corentin Léger, Cédric Colas, Gaia Molinaro, Maxime Derex, PierreYves Oudeyer, Clément MoulinFrier |  |
| 1841 |  |  [Advancing Out-of-Distribution Detection via Local Neuroplasticity](https://openreview.net/forum?id=1F8xTfv6ah) |  | 0 |  | Alessandro Canevaro, Julian Schmidt, Mohammad Sajad Marvi, Hang Yu, Georg Martius, Julian Jordan |  |
| 1842 |  |  [TimeInf: Time Series Data Contribution via Influence Functions](https://openreview.net/forum?id=Vz0CWFMPUe) |  | 0 |  | Yizi Zhang, Jingyan Shen, Xiaoxue Xiong, Yongchan Kwon |  |
| 1843 |  |  [DistRL: An Asynchronous Distributed Reinforcement Learning Framework for On-Device Control Agent](https://openreview.net/forum?id=LPG8pPSfQD) |  | 0 |  | Taiyi Wang, Zhihao Wu, Jianheng Liu, Jianye Hao, Jun Wang, Kun Shao |  |
| 1844 |  |  [Learning mirror maps in policy mirror descent](https://openreview.net/forum?id=n4wcdct43X) |  | 0 |  | Carlo Alfano, Sebastian Rene Towers, Silvia Sapora, Chris Lu, Patrick Rebeschini |  |
| 1845 |  |  [Understanding Optimization in Deep Learning with Central Flows](https://openreview.net/forum?id=sIE2rI3ZPs) |  | 0 |  | Jeremy Cohen, Alex Damian, Ameet Talwalkar, J. Zico Kolter, Jason D. Lee |  |
| 1846 |  |  [Mini-batch Coresets for Memory-efficient Language Model Training on Data Mixtures](https://openreview.net/forum?id=bAFVlpFQvT) |  | 0 |  | Dang Nguyen, Wenhan Yang, Rathul Anand, Yu Yang, Baharan Mirzasoleiman |  |
| 1847 |  |  [Spherical Tree-Sliced Wasserstein Distance](https://openreview.net/forum?id=FPQzXME9NK) |  | 0 |  | Hoang V. Tran, Thanh T. Chu, MinhKhoi NguyenNhat, Huyen Trang Pham, Tam Le, Tan Minh Nguyen |  |
| 1848 |  |  [Repetition Improves Language Model Embeddings](https://openreview.net/forum?id=Ahlrf2HGJR) |  | 0 |  | Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, Aditi Raghunathan |  |
| 1849 |  |  [TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking](https://openreview.net/forum?id=VIUisLx8lQ) |  | 0 |  | Danqing Wang, Jianxin Ma, Fei Fang, Lei Li |  |
| 1850 |  |  [Positive-Unlabeled Diffusion Models for Preventing Sensitive Data Generation](https://openreview.net/forum?id=jKcZ4hF4s5) |  | 0 |  | Hiroshi Takahashi, Tomoharu Iwata, Atsutoshi Kumagai, Yuuki Yamanaka, Tomoya Yamashita |  |
| 1851 |  |  [API Pack: A Massive Multi-Programming Language Dataset for API Call Generation](https://openreview.net/forum?id=f7O3hITh5s) |  | 0 |  | Zhen Guo, Adriana Meza Soria, Wei Sun, Yikang Shen, Rameswar Panda |  |
| 1852 |  |  [GNNs Getting ComFy: Community and Feature Similarity Guided Rewiring](https://openreview.net/forum?id=g6v09VxgFw) |  | 0 |  | Celia RubioMadrigal, Adarsh Jamadandi, Rebekka Burkholz |  |
| 1853 |  |  [ToddlerDiffusion: Interactive Structured Image Generation with Cascaded Schrödinger Bridge](https://openreview.net/forum?id=Jszf4et48m) |  | 0 |  | Eslam Mohamed Bakr, Liangbing Zhao, Vincent Tao Hu, Matthieu Cord, Patrick Pérez, Mohamed Elhoseiny |  |
| 1854 |  |  [MIA-Bench: Towards Better Instruction Following Evaluation of Multimodal LLMs](https://openreview.net/forum?id=7EhS3YBxjY) |  | 0 |  | Yusu Qian, Hanrong Ye, JeanPhilippe Fauconnier, Peter Grasch, Yinfei Yang, Zhe Gan |  |
| 1855 |  |  [SGD with memory: fundamental properties and stochastic acceleration](https://openreview.net/forum?id=Qzd4BloAjQ) |  | 0 |  | Dmitry Yarotsky, Maksim Velikanov |  |
| 1856 |  |  [Distance-Based Tree-Sliced Wasserstein Distance](https://openreview.net/forum?id=OiQttMHwce) |  | 0 |  | Hoang V. Tran, MinhKhoi NguyenNhat, Huyen Trang Pham, Thanh T. Chu, Tam Le, Tan Minh Nguyen |  |
| 1857 |  |  [Provable Benefit of Annealed Langevin Monte Carlo for Non-log-concave Sampling](https://openreview.net/forum?id=P6IVIoGRRg) |  | 0 |  | Wei Guo, Molei Tao, Yongxin Chen |  |
| 1858 |  |  [On Speeding Up Language Model Evaluation](https://openreview.net/forum?id=3cvwO5DBZn) |  | 0 |  | Jin Peng Zhou, Christian K. Belardi, Ruihan Wu, Travis Zhang, Carla P. Gomes, Wen Sun, Kilian Q. Weinberger |  |
| 1859 |  |  [Shifting the Paradigm: A Diffeomorphism Between Time Series Data Manifolds for Achieving Shift-Invariancy in Deep Learning](https://openreview.net/forum?id=nibeaHUEJx) |  | 0 |  | Berken Utku Demirel, Christian Holz |  |
| 1860 |  |  [Human-Aligned Chess With a Bit of Search](https://openreview.net/forum?id=bc2H72hGxB) |  | 0 |  | Yiming Zhang, Athul Paul Jacob, Vivian Lai, Daniel Fried, Daphne Ippolito |  |
| 1861 |  |  [Dualformer: Controllable Fast and Slow Thinking by Learning with Randomized Reasoning Traces](https://openreview.net/forum?id=bmbRCRiNDu) |  | 0 |  | DiJia Su, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, Qinqing Zheng |  |
| 1862 |  |  [Autoregressive Pretraining with Mamba in Vision](https://openreview.net/forum?id=PQpvhUrA1C) |  | 0 |  | Sucheng Ren, Xianhang Li, Haoqin Tu, Feng Wang, Fangxun Shu, Lei Zhang, Jieru Mei, Linjie Yang, Peng Wang, Heng Wang, Alan L. Yuille, Cihang Xie |  |
| 1863 |  |  [Task-Adaptive Pretrained Language Models via Clustered-Importance Sampling](https://openreview.net/forum?id=p6ncr0eTKE) |  | 0 |  | David Grangier, Simin Fan, Skyler Seto, Pierre Ablin |  |
| 1864 |  |  [Sparse Autoencoders Do Not Find Canonical Units of Analysis](https://openreview.net/forum?id=9ca9eHNrdH) |  | 0 |  | Patrick Leask, Bart Bussmann, Michael T. Pearce, Joseph Isaac Bloom, Curt Tigges, Noura Al Moubayed, Lee Sharkey, Neel Nanda |  |
| 1865 |  |  [BiGR: Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities](https://openreview.net/forum?id=1Z6PSw7OL8) |  | 0 |  | Shaozhe Hao, Xuantong Liu, Xianbiao Qi, Shihao Zhao, Bojia Zi, Rong Xiao, Kai Han, KwanYee K. Wong |  |
| 1866 |  |  [Training Robust Ensembles Requires Rethinking Lipschitz Continuity](https://openreview.net/forum?id=WKW5TG8ItY) |  | 0 |  | Ali Ebrahimpour Boroojeny, Hari Sundaram, Varun Chandrasekaran |  |
| 1867 |  |  [Generative Monoculture in Large Language Models](https://openreview.net/forum?id=yZ7sn9pyqb) |  | 0 |  | Fan Wu, Emily Black, Varun Chandrasekaran |  |
| 1868 |  |  [Composable Interventions for Language Models](https://openreview.net/forum?id=tu3qwNjrtw) |  | 0 |  | Arinbjörn Kolbeinsson, Kyle O'Brien, Tianjin Huang, Shanghua Gao, Shiwei Liu, Jonathan Richard Schwarz, Anurag Jayant Vaidya, Faisal Mahmood, Marinka Zitnik, Tianlong Chen, Thomas Hartvigsen |  |
| 1869 |  |  [MUSE: Machine Unlearning Six-Way Evaluation for Language Models](https://openreview.net/forum?id=TArmA033BU) |  | 0 |  | Weijia Shi, Jaechan Lee, Yangsibo Huang, Sadhika Malladi, Jieyu Zhao, Ari Holtzman, Daogao Liu, Luke Zettlemoyer, Noah A. Smith, Chiyuan Zhang |  |
| 1870 |  |  [Air Quality Prediction with Physics-Guided Dual Neural ODEs in Open Systems](https://openreview.net/forum?id=kOJf7Dklyv) |  | 0 |  | Jindong Tian, Yuxuan Liang, Ronghui Xu, Peng Chen, Chenjuan Guo, Aoying Zhou, Lujia Pan, Zhongwen Rao, Bin Yang |  |
| 1871 |  |  [A Transfer Attack to Image Watermarks](https://openreview.net/forum?id=UchRjcf4z7) |  | 0 |  | Yuepeng Hu, Zhengyuan Jiang, Moyang Guo, Neil Zhenqiang Gong |  |
| 1872 |  |  [Port-Hamiltonian Architectural Bias for Long-Range Propagation in Deep Graph Networks](https://openreview.net/forum?id=03EkqSCKuO) |  | 0 |  | Simon Heilig, Alessio Gravina, Alessandro Trenta, Claudio Gallicchio, Davide Bacciu |  |
| 1873 |  |  [LevAttention: Time, Space and Streaming Efficient Algorithm for Heavy Attentions](https://openreview.net/forum?id=90DC0IvlSs) |  | 0 |  | Ravindran Kannan, Chiranjib Bhattacharyya, Praneeth Kacham, David P. Woodruff |  |
| 1874 |  |  [ActSafe: Active Exploration with Safety Constraints for Reinforcement Learning](https://openreview.net/forum?id=aKRADWBJ1I) |  | 0 |  | Yarden As, Bhavya Sukhija, Lenart Treven, Carmelo Sferrazza, Stelian Coros, Andreas Krause |  |
| 1875 |  |  [Straight to Zero: Why Linearly Decaying the Learning Rate to Zero Works Best for LLMs](https://openreview.net/forum?id=hrOlBgHsMI) |  | 0 |  | Shane Bergsma, Nolan Simran Dey, Gurpreet Gosal, Gavia Gray, Daria Soboleva, Joel Hestness |  |
| 1876 |  |  [Adaptive Length Image Tokenization via Recurrent Allocation](https://openreview.net/forum?id=mb2ryuZ3wz) |  | 0 |  | Shivam Duggal, Phillip Isola, Antonio Torralba, William T. Freeman |  |
| 1877 |  |  [Scaling Diffusion Language Models via Adaptation from Autoregressive Models](https://openreview.net/forum?id=j1tSLYKwg8) |  | 0 |  | Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, Hao Peng, Lingpeng Kong |  |
| 1878 |  |  [Mixture of Experts Made Personalized: Federated Prompt Learning for Vision-Language Models](https://openreview.net/forum?id=xiDJaTim3P) |  | 0 |  | Jun Luo, Chen Chen, Shandong Wu |  |
| 1879 |  |  [FaithEval: Can Your Language Model Stay Faithful to Context, Even If "The Moon is Made of Marshmallows"](https://openreview.net/forum?id=UeVx6L59fg) |  | 0 |  | Yifei Ming, Senthil Purushwalkam, Shrey Pandit, Zixuan Ke, XuanPhi Nguyen, Caiming Xiong, Shafiq Joty |  |
| 1880 |  |  [GenVP: Generating Visual Puzzles with Contrastive Hierarchical VAEs](https://openreview.net/forum?id=Pd7IOswRUZ) |  | 0 |  | Kalliopi Basioti, Pritish Sahu, Tony Qingze Liu, Zihao Xu, Hao Wang, Vladimir Pavlovic |  |
| 1881 |  |  [Computing Circuits Optimization via Model-Based Circuit Genetic Evolution](https://openreview.net/forum?id=KWH4UIoQKS) |  | 0 |  | Zhihai Wang, Jie Wang, Xilin Xia, Dongsheng Zuo, Lei Chen, Yuzhe Ma, Jianye Hao, Mingxuan Yuan, Feng Wu |  |
| 1882 |  |  [VideoPhy: Evaluating Physical Commonsense for Video Generation](https://openreview.net/forum?id=9D2QvO1uWj) |  | 0 |  | Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, KaiWei Chang, Aditya Grover |  |
| 1883 |  |  [On the Completeness of Invariant Geometric Deep Learning Models](https://openreview.net/forum?id=52x04chyQs) |  | 0 |  | Zian Li, Xiyuan Wang, Shijia Kang, Muhan Zhang |  |
| 1884 |  |  [Multi-Modal and Multi-Attribute Generation of Single Cells with CFGen](https://openreview.net/forum?id=3MnMGLctKb) |  | 0 |  | Alessandro Palma, Till Richter, Hanyi Zhang, Manuel Lubetzki, Alexander Tong, Andrea Dittadi, Fabian J. Theis |  |
| 1885 |  |  [Reassessing How to Compare and Improve the Calibration of Machine Learning Models](https://openreview.net/forum?id=X0epAjg0hd) |  | 0 |  | Muthu Chidambaram, Rong Ge |  |
| 1886 |  |  [SSLAM: Enhancing Self-Supervised Models with Audio Mixtures for Polyphonic Soundscapes](https://openreview.net/forum?id=odU59TxdiB) |  | 0 |  | Tony Alex, Sara Atito, Armin Mustafa, Muhammad Awais, Philip J. B. Jackson |  |
| 1887 |  |  [Sharper Guarantees for Learning Neural Network Classifiers with Gradient Methods](https://openreview.net/forum?id=h7GAgbLSmC) |  | 0 |  | Hossein Taheri, Christos Thrampoulidis, Arya Mazumdar |  |
| 1888 |  |  [Diversity-Rewarded CFG Distillation](https://openreview.net/forum?id=lJ66m0ibQL) |  | 0 |  | Geoffrey Cideron, Andrea Agostinelli, Johan Ferret, Sertan Girgin, Romuald Elie, Olivier Bachem, Sarah Perrin, Alexandre Ramé |  |
| 1889 |  |  [Language-Assisted Feature Transformation for Anomaly Detection](https://openreview.net/forum?id=2p03KljxE9) |  | 0 |  | EungGu Yun, Heonjin Ha, Yeongwoo Nam, Bryan Dongik Lee |  |
| 1890 |  |  [LoRA-X: Bridging Foundation Models with Training-Free Cross-Model Adaptation](https://openreview.net/forum?id=6cQ6cBqzV3) |  | 0 |  | Farzad Farhadzadeh, Debasmit Das, Shubhankar Borse, Fatih Porikli |  |
| 1891 |  |  [Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection](https://openreview.net/forum?id=HE6pJoNnFp) |  | 0 |  | Yun Zhu, JiaChen Gu, Caitlin Sikora, Ho Ko, Yinxiao Liu, ChuCheng Lin, Lei Shu, Liangchen Luo, Lei Meng, Bang Liu, Jindong Chen |  |
| 1892 |  |  [How efficient is LLM-generated code? A rigorous & high-standard benchmark](https://openreview.net/forum?id=suz4utPr9Y) |  | 0 |  | Ruizhong Qiu, Weiliang Will Zeng, James Ezick, Christopher Lott, Hanghang Tong |  |
| 1893 |  |  [Topological Zigzag Spaghetti for Diffusion-based Generation and Prediction on Graphs](https://openreview.net/forum?id=mYgoNEsUDi) |  | 0 |  | Yuzhou Chen, Yulia R. Gel |  |
| 1894 |  |  [Model Equality Testing: Which Model is this API Serving?](https://openreview.net/forum?id=QCDdI7X3f9) |  | 0 |  | Irena Gao, Percy Liang, Carlos Guestrin |  |
| 1895 |  |  [Learning Structured Representations by Embedding Class Hierarchy with Fast Optimal Transport](https://openreview.net/forum?id=AnL6BuWzxa) |  | 0 |  | Siqi Zeng, Sixian Du, Makoto Yamada, Han Zhao |  |
| 1896 |  |  [DICE: Data Influence Cascade in Decentralized Learning](https://openreview.net/forum?id=2TIYkqieKw) |  | 0 |  | Tongtian Zhu, Wenhao Li, Can Wang, Fengxiang He |  |
| 1897 |  |  [SpikeLLM: Scaling up Spiking Neural Network to Large Language Models via Saliency-based Spiking](https://openreview.net/forum?id=ZadnlOHsHv) |  | 0 |  | Xingrun Xing, Boyan Gao, Zheng Liu, David A. Clifton, Shitao Xiao, Wanpeng Zhang, Li Du, Zheng Zhang, Guoqi Li, Jiajun Zhang |  |
| 1898 |  |  [UNSURE: self-supervised learning with Unknown Noise level and Stein's Unbiased Risk Estimate](https://openreview.net/forum?id=ScVnYBaSEw) |  | 0 |  | Julián Tachella, Mike E. Davies, Laurent Jacques |  |
| 1899 |  |  [ALBAR: Adversarial Learning approach to mitigate Biases in Action Recognition](https://openreview.net/forum?id=9KiE3t6CsL) |  | 0 |  | Joseph Fioresi, Ishan Rajendrakumar Dave, Mubarak Shah |  |
| 1900 |  |  [Efficient Exploration and Discriminative World Model Learning with an Object-Centric Abstraction](https://openreview.net/forum?id=hgwGi81ndj) |  | 0 |  | Anthony GXChen, Kenneth Marino, Rob Fergus |  |
| 1901 |  |  [Drama: Mamba-Enabled Model-Based Reinforcement Learning Is Sample and Parameter Efficient](https://openreview.net/forum?id=7XIkRgYjK3) |  | 0 |  | Wenlong Wang, Ivana Dusparic, Yucheng Shi, Ke Zhang, Vinny Cahill |  |
| 1902 |  |  [PooDLe🐩: Pooled and dense self-supervised learning from naturalistic videos](https://openreview.net/forum?id=dEg5SdGaiq) |  | 0 |  | Alex N. Wang, Christopher Hoang, Yuwen Xiong, Yann LeCun, Mengye Ren |  |
| 1903 |  |  [Apollo-MILP: An Alternating Prediction-Correction Neural Solving Framework for Mixed-Integer Linear Programming](https://openreview.net/forum?id=mFY0tPDWK8) |  | 0 |  | Haoyang Liu, Jie Wang, Zijie Geng, Xijun Li, Yuxuan Zong, Fangzhou Zhu, Jianye Hao, Feng Wu |  |
| 1904 |  |  [Theory, Analysis, and Best Practices for Sigmoid Self-Attention](https://openreview.net/forum?id=Zhdhg6n2OG) |  | 0 |  | Jason Ramapuram, Federico Danieli, Eeshan Gunesh Dhekane, Floris Weers, Dan Busbridge, Pierre Ablin, Tatiana Likhomanenko, Jagrit Digani, Zijin Gu, Amitis Shidani, Russell Webb |  |
| 1905 |  |  [Small Models are LLM Knowledge Triggers for Medical Tabular Prediction](https://openreview.net/forum?id=WoPovNkM5h) |  | 0 |  | Jiahuan Yan, Jintai Chen, Chaowen Hu, Bo Zheng, Yaojun Hu, Jimeng Sun, Jian Wu |  |
| 1906 |  |  [Efficient Model Editing with Task-Localized Sparse Fine-tuning](https://openreview.net/forum?id=TDyE2iuvyc) |  | 0 |  | Leonardo Iurada, Marco Ciccone, Tatiana Tommasi |  |
| 1907 |  |  [Shot2Story: A New Benchmark for Comprehensive Understanding of Multi-shot Videos](https://openreview.net/forum?id=FZv3kPHTtB) |  | 0 |  | Mingfei Han, Linjie Yang, Xiaojun Chang, Lina Yao, Heng Wang |  |
| 1908 |  |  [HGM³: Hierarchical Generative Masked Motion Modeling with Hard Token Mining](https://openreview.net/forum?id=IEul1M5pyk) |  | 0 |  | Minjae Jeong, Yechan Hwang, Jaejin Lee, Sungyoon Jung, Won Hwa Kim |  |
| 1909 |  |  [Improving Large Language Model Planning with Action Sequence Similarity](https://openreview.net/forum?id=tpGkEgxMJT) |  | 0 |  | Xinran Zhao, Hanie Sedghi, Bernd Bohnet, Dale Schuurmans, Azade Nova |  |
| 1910 |  |  [Tackling Data Corruption in Offline Reinforcement Learning via Sequence Modeling](https://openreview.net/forum?id=phAlw3JPms) |  | 0 |  | Jiawei Xu, Rui Yang, Shuang Qiu, Feng Luo, Meng Fang, Baoxiang Wang, Lei Han |  |
| 1911 |  |  [VoxDialogue: Can Spoken Dialogue Systems Understand Information Beyond Words?](https://openreview.net/forum?id=vbmSSIhKAM) |  | 0 |  | Xize Cheng, Ruofan Hu, Xiaoda Yang, Jingyu Lu, Dongjie Fu, Zehan Wang, Shengpeng Ji, Rongjie Huang, Boyang Zhang, Tao Jin, Zhou Zhao |  |
| 1912 |  |  [PQMass: Probabilistic Assessment of the Quality of Generative Models using Probability Mass Estimation](https://openreview.net/forum?id=n7qGCmluZr) |  | 0 |  | Pablo Lemos, Sammy Nasser Sharief, Nikolay Malkin, Salma Salhi, Connor Stone, Laurence Perreault Levasseur, Yashar Hezaveh |  |
| 1913 |  |  [OpenHands: An Open Platform for AI Software Developers as Generalist Agents](https://openreview.net/forum?id=OJd3ayDDoF) |  | 0 |  | Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, et al. |  |
| 1914 |  |  [Lines of Thought in Large Language Models](https://openreview.net/forum?id=zjAEa4s3sH) |  | 0 |  | Raphaël Sarfati, Toni J. B. Liu, Nicolas Boullé, Christopher J. Earls |  |
| 1915 |  |  [Knowledge Graph Finetuning Enhances Knowledge Manipulation in Large Language Models](https://openreview.net/forum?id=oMFOKjwaRS) |  | 0 |  | Hanzhu Chen, Xu Shen, Jie Wang, Zehao Wang, Qitan Lv, Junjie He, Rong Wu, Feng Wu, Jieping Ye |  |
| 1916 |  |  [Vevo: Controllable Zero-Shot Voice Imitation with Self-Supervised Disentanglement](https://openreview.net/forum?id=anQDiQZhDP) |  | 0 |  | Xueyao Zhang, Xiaohui Zhang, Kainan Peng, Zhenyu Tang, Vimal Manohar, Yingru Liu, Jeff Hwang, Dangna Li, Yuhao Wang, Julian Chan, Yuan Huang, Zhizheng Wu, Mingbo Ma |  |
| 1917 |  |  [Designing Mechanical Meta-Materials by Learning Equivariant Flows](https://openreview.net/forum?id=VMurwgAFWP) |  | 0 |  | Mehran Mirramezani, Anne S. Meeussen, Katia Bertoldi, Peter Orbanz, Ryan P. Adams |  |
| 1918 |  |  [Simulating Training Dynamics to Reconstruct Training Data from Deep Neural Networks](https://openreview.net/forum?id=ZJftXKy12x) |  | 0 |  | Hanling Tian, Yuhang Liu, Mingzhen He, Zhengbao He, Zhehao Huang, Ruikai Yang, Xiaolin Huang |  |
| 1919 |  |  [XLand-100B: A Large-Scale Multi-Task Dataset for In-Context Reinforcement Learning](https://openreview.net/forum?id=p9OsTj0nMP) |  | 0 |  | Alexander Nikulin, Ilya Zisman, Alexey Zemtsov, Vladislav Kurenkov |  |
| 1920 |  |  [How Far Are We from True Unlearnability?](https://openreview.net/forum?id=I4Lq2RJ0eJ) |  | 0 |  | Kai Ye, Liangcai Su, Chenxiong Qian |  |
| 1921 |  |  [Bridging the Gap between Database Search and De Novo Peptide Sequencing with SearchNovo](https://openreview.net/forum?id=SjMtxqdQ73) |  | 0 |  | Jun Xia, Sizhe Liu, Jingbo Zhou, Shaorong Chen, Hongxin Xiang, Zicheng Liu, Yue Liu, Stan Z. Li |  |
| 1922 |  |  [Rethinking Invariance Regularization in Adversarial Training to Improve Robustness-Accuracy Trade-off](https://openreview.net/forum?id=M9SKazbVkJ) |  | 0 |  | Futa Kai Waseda, ChingChun Chang, Isao Echizen |  |
| 1923 |  |  [BANGS: Game-theoretic Node Selection for Graph Self-Training](https://openreview.net/forum?id=h51mpl8Tyx) |  | 0 |  | Fangxin Wang, Kay Liu, Sourav Medya, Philip S. Yu |  |
| 1924 |  |  [Holistic Reasoning with Long-Context LMs: A Benchmark for Database Operations on Massive Textual Data](https://openreview.net/forum?id=5LXcoDtNyq) |  | 0 |  | Seiji Maekawa, Hayate Iso, Nikita Bhutani |  |
| 1925 |  |  [Multi-Dimensional Conformal Prediction](https://openreview.net/forum?id=loDppyW7e2) |  | 0 |  | Yam Tawachi, Bracha LauferGoldshtein |  |
| 1926 |  |  [Generating Likely Counterfactuals Using Sum-Product Networks](https://openreview.net/forum?id=rGyi8NNqB0) |  | 0 |  | Jiri Nemecek, Tomás Pevný, Jakub Marecek |  |
| 1927 |  |  [Overcoming Lower-Level Constraints in Bilevel Optimization: A Novel Approach with Regularized Gap Functions](https://openreview.net/forum?id=cyPMEXdqQ2) |  | 0 |  | Wei Yao, Haian Yin, Shangzhi Zeng, Jin Zhang |  |
| 1928 |  |  [Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization](https://openreview.net/forum?id=uaMSBJDnRv) |  | 0 |  | Noam Razin, Sadhika Malladi, Adithya Bhaskar, Danqi Chen, Sanjeev Arora, Boris Hanin |  |
| 1929 |  |  [Newton Meets Marchenko-Pastur: Massively Parallel Second-Order Optimization with Hessian Sketching and Debiasing](https://openreview.net/forum?id=Ty6TCjKNSF) |  | 0 |  | Elad Romanov, Fangzhao Zhang, Mert Pilanci |  |
| 1930 |  |  [SIMPL: Scalable and hassle-free optimisation of neural representations from behaviour](https://openreview.net/forum?id=9kFaNwX6rv) |  | 0 |  | Tom M. George, Pierre Glaser, Kim Stachenfeld, Caswell Barry, Claudia Clopath |  |
| 1931 |  |  [Diffusion-based Decoupled Deterministic and Uncertain Framework for Probabilistic Multivariate Time Series Forecasting](https://openreview.net/forum?id=HdUkF1Qk7g) |  | 0 |  | Qi Li, Zhenyu Zhang, Lei Yao, Zhaoxia Li, Tianyi Zhong, Yong Zhang |  |
| 1932 |  |  [AugKD: Ingenious Augmentations Empower Knowledge Distillation for Image Super-Resolution](https://openreview.net/forum?id=AC3713Fmhx) |  | 0 |  | Yun Zhang, Wei Li, Simiao Li, Hanting Chen, Zhijun Tu, Bingyi Jing, Shaohui Lin, Jie Hu, Wenjia Wang |  |
| 1933 |  |  [qNBO: quasi-Newton Meets Bilevel Optimization](https://openreview.net/forum?id=BTOdzCzSRg) |  | 0 |  | Sheng Fang, Yongjin Liu, Wei Yao, Chengming Yu, Jin Zhang |  |
| 1934 |  |  [Deconstructing Denoising Diffusion Models for Self-Supervised Learning](https://openreview.net/forum?id=9oMB6wnFYM) |  | 0 |  | Xinlei Chen, Zhuang Liu, Saining Xie, Kaiming He |  |
| 1935 |  |  [W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models](https://openreview.net/forum?id=YkmbJSHjj7) |  | 0 |  | Shang Wang |  |
| 1936 |  |  [Graph Neural Networks for Edge Signals: Orientation Equivariance and Invariance](https://openreview.net/forum?id=XWBE90OYlH) |  | 0 |  | Dominik Fuchsgruber, Tim Postuvan, Stephan Günnemann, Simon Geisler |  |
| 1937 |  |  [Neural Functions for Learning Periodic Signal](https://openreview.net/forum?id=GCH5leffZp) |  | 0 |  | Woojin Cho, Minju Jo, Kookjin Lee, Noseong Park |  |
| 1938 |  |  [Language Guided Skill Discovery](https://openreview.net/forum?id=i3e92uSZCp) |  | 0 |  | Seungeun Rho, Laura Smith, Tianyu Li, Sergey Levine, Xue Bin Peng, Sehoon Ha |  |
| 1939 |  |  [A Differentiable Rank-Based Objective for Better Feature Learning](https://openreview.net/forum?id=KiN7g8mf9N) |  | 0 |  | Krunoslav Lehman Pavasovic, Giulio Biroli, Levent Sagun |  |
| 1940 |  |  [RESfM: Robust Deep Equivariant Structure from Motion](https://openreview.net/forum?id=wldwEhQ7cl) |  | 0 |  | Fadi Khatib, Yoni Kasten, Dror Moran, Meirav Galun, Ronen Basri |  |
| 1941 |  |  [TweedieMix: Improving Multi-Concept Fusion for Diffusion-based Image/Video Generation](https://openreview.net/forum?id=ee2c4MEx9l) |  | 0 |  | Gihyun Kwon, Jong Chul Ye |  |
| 1942 |  |  [Filtered not Mixed: Filtering-Based Online Gating for Mixture of Large Language Models](https://openreview.net/forum?id=ecIvumCyAj) |  | 0 |  | Raeid Saqur, Anastasis Kratsios, Florian Krach, Yannick Limmer, Blanka Horvath, Frank Rudzicz |  |
| 1943 |  |  [LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances Model Merging](https://openreview.net/forum?id=J5sUOvlLbQ) |  | 0 |  | Ke Wang, Nikolaos Dimitriadis, Alessandro Favero, Guillermo OrtizJiménez, François Fleuret, Pascal Frossard |  |
| 1944 |  |  [Training-Free Message Passing for Learning on Hypergraphs](https://openreview.net/forum?id=4AuyYxt7A2) |  | 0 |  | Bohan Tang, Zexi Liu, Keyue Jiang, Siheng Chen, Xiaowen Dong |  |
| 1945 |  |  [POGEMA: A Benchmark Platform for Cooperative Multi-Agent Pathfinding](https://openreview.net/forum?id=6VgwE2tCRm) |  | 0 |  | Alexey Skrynnik, Anton Andreychuk, Anatolii Borzilov, Alexander Chernyavskiy, Konstantin S. Yakovlev, Aleksandr Panov |  |
| 1946 |  |  [TD-Paint: Faster Diffusion Inpainting Through Time-Aware Pixel Conditioning](https://openreview.net/forum?id=erWwBoR59l) |  | 0 |  | Tsiry Mayet, Pourya Shamsolmoali, Simon Bernard, Eric Granger, Romain Hérault, Clément Chatelain |  |
| 1947 |  |  [Class Distribution-induced Attention Map for Open-vocabulary Semantic Segmentations](https://openreview.net/forum?id=CMqOfvD3tO) |  | 0 |  | Dong Un Kang, Hayeon Kim, Se Young Chun |  |
| 1948 |  |  [Weak to Strong Generalization for Large Language Models with Multi-capabilities](https://openreview.net/forum?id=N1vYivuSKq) |  | 0 |  | Yucheng Zhou, Jianbing Shen, Yu Cheng |  |
| 1949 |  |  [CHiP: Cross-modal Hierarchical Direct Preference Optimization for Multimodal LLMs](https://openreview.net/forum?id=7lpDn2MhM2) |  | 0 |  | Jinlan Fu, Shenzhen Huangfu, Hao Fei, Xiaoyu Shen, Bryan Hooi, Xipeng Qiu, SeeKiong Ng |  |
| 1950 |  |  [Concept Pinpoint Eraser for Text-to-image Diffusion Models via Residual Attention Gate](https://openreview.net/forum?id=ZRDhBwKs7l) |  | 0 |  | Byung Hyun Lee, Sungjin Lim, Seunggyu Lee, Dong Un Kang, Se Young Chun |  |
| 1951 |  |  [Graph-Guided Scene Reconstruction from Images with 3D Gaussian Splatting](https://openreview.net/forum?id=56vHbnk35S) |  | 0 |  | Chong Cheng, Gaochao Song, Yiyang Yao, Qinzheng Zhou, Gangjian Zhang, Hao Wang |  |
| 1952 |  |  [Residual Kernel Policy Network: Enhancing Stability and Robustness in RKHS-Based Reinforcement Learning](https://openreview.net/forum?id=2vgcDW2blS) |  | 0 |  | Yixian Zhang, Huaze Tang, Huijing Lin, Wenbo Ding |  |
| 1953 |  |  [Speech Robust Bench: A Robustness Benchmark For Speech Recognition](https://openreview.net/forum?id=D0LuQNZfEl) |  | 0 |  | Muhammad A. Shah, David Solans Noguero, Mikko A. Heikkilä, Bhiksha Raj, Nicolas Kourtellis |  |
| 1954 |  |  [Learning High-Degree Parities: The Crucial Role of the Initialization](https://openreview.net/forum?id=OuNIWgGGif) |  | 0 |  | Emmanuel Abbe, Elisabetta Cornacchia, Jan Hazla, Donald KougangYombi |  |
| 1955 |  |  [Debiasing Federated Learning with Correlated Client Participation](https://openreview.net/forum?id=9h45qxXEx0) |  | 0 |  | Zhenyu Sun, Ziyang Zhang, Zheng Xu, Gauri Joshi, Pranay Sharma, Ermin Wei |  |
| 1956 |  |  [Residual Connections and Normalization Can Provably Prevent Oversmoothing in GNNs](https://openreview.net/forum?id=i8vPRlsrYu) |  | 0 |  | Michael Scholkemper, Xinyi Wu, Ali Jadbabaie, Michael T. Schaub |  |
| 1957 |  |  [SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs](https://openreview.net/forum?id=lqHv6dxBkj) |  | 0 |  | Mohammad Mozaffari, Amir Yazdanbakhsh, Zhao Zhang, Maryam Mehri Dehnavi |  |
| 1958 |  |  [DSBench: How Far Are Data Science Agents from Becoming Data Science Experts?](https://openreview.net/forum?id=DSsSPr0RZJ) |  | 0 |  | Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, Dong Yu |  |
| 1959 |  |  [Bringing NeRFs to the Latent Space: Inverse Graphics Autoencoder](https://openreview.net/forum?id=LTDtjrv02Y) |  | 0 |  | Antoine Schnepf, Karim Kassab, JeanYves Franceschi, Laurent Caraffa, Flavian Vasile, Jérémie Mary, Andrew I. Comport, Valérie GouetBrunet |  |
| 1960 |  |  [What Are Good Positional Encodings for Directed Graphs?](https://openreview.net/forum?id=s4Wm71LFK4) |  | 0 |  | Yinan Huang, Haoyu Peter Wang, Pan Li |  |
| 1961 |  |  [ASTrA: Adversarial Self-supervised Training with Adaptive-Attacks](https://openreview.net/forum?id=ZbkqhKbggH) |  | 0 |  | Prakash Chandra Chhipa, Gautam Vashishtha, Settur Jithamanyu, Rajkumar Saini, Mubarak Shah, Marcus Liwicki |  |
| 1962 |  |  [GUI-World: A Video Benchmark and Dataset for Multimodal GUI-oriented Understanding](https://openreview.net/forum?id=QarKTT5brZ) |  | 0 |  | Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, Huichi Zhou, Qihui Zhang, Zhigang He, Yilin Bai, Chujie Gao, Liuyi Chen, Yiqiang Li, Chenlong Wang, Yue Yu, Tianshuo Zhou, Zhen Li, Yi Gui, Yao Wan, Pan Zhou, Jianfeng Gao, Lichao Sun |  |
| 1963 |  |  [Effective and Efficient Time-Varying Counterfactual Prediction with State-Space Models](https://openreview.net/forum?id=yheQRc5xWB) |  | 0 |  | Haotian Wang, Haoxuan Li, Hao Zou, Haoang Chi, Long Lan, Wanrong Huang, Wenjing Yang |  |
| 1964 |  |  [Number Cookbook: Number Understanding of Language Models and How to Improve It](https://openreview.net/forum?id=BWS5gVjgeY) |  | 0 |  | Haotong Yang, Yi Hu, Shijia Kang, Zhouchen Lin, Muhan Zhang |  |
| 1965 |  |  [Spectral-Refiner: Accurate Fine-Tuning of Spatiotemporal Fourier Neural Operator for Turbulent Flows](https://openreview.net/forum?id=MKP1g8wU0P) |  | 0 |  | Shuhao Cao, Francesco Brarda, Ruipeng Li, Yuanzhe Xi |  |
| 1966 |  |  [Adaptive backtracking line search](https://openreview.net/forum?id=SrGP0RQbYH) |  | 0 |  | Joao V. Cavalcanti, Laurent Lessard, Ashia C. Wilson |  |
| 1967 |  |  [Breach By A Thousand Leaks: Unsafe Information Leakage in 'Safe' AI Responses](https://openreview.net/forum?id=8Rov0fjpOL) |  | 0 |  | David Glukhov, Ziwen Han, Ilia Shumailov, Vardan Papyan, Nicolas Papernot |  |
| 1968 |  |  [Efficient Off-Policy Learning for High-Dimensional Action Spaces](https://openreview.net/forum?id=JDzTI9rKls) |  | 0 |  | Fabian Otto, Philipp Becker, Ngo Anh Vien, Gerhard Neumann |  |
| 1969 |  |  [Causal Concept Graph Models: Beyond Causal Opacity in Deep Learning](https://openreview.net/forum?id=lmKJ1b6PaL) |  | 0 |  | Gabriele Dominici, Pietro Barbiero, Mateo Espinosa Zarlenga, Alberto Termine, Martin Gjoreski, Giuseppe Marra, Marc Langheinrich |  |
| 1970 |  |  [Meta Flow Matching: Integrating Vector Fields on the Wasserstein Manifold](https://openreview.net/forum?id=9SYczU3Qgm) |  | 0 |  | Lazar Atanackovic, Xi Zhang, Brandon Amos, Mathieu Blanchette, Leo J. Lee, Yoshua Bengio, Alexander Tong, Kirill Neklyudov |  |
| 1971 |  |  [JetFormer: An autoregressive generative model of raw images and text](https://openreview.net/forum?id=sgAp2qG86e) |  | 0 |  | Michael Tschannen, André Susano Pinto, Alexander Kolesnikov |  |
| 1972 |  |  [Counterfactual Concept Bottleneck Models](https://openreview.net/forum?id=w7pMjyjsKN) |  | 0 |  | Gabriele Dominici, Pietro Barbiero, Francesco Giannini, Martin Gjoreski, Giuseppe Marra, Marc Langheinrich |  |
| 1973 |  |  [Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models](https://openreview.net/forum?id=EbxYDBhE3S) |  | 0 |  | Biao Yi, Tiansheng Huang, Sishuo Chen, Tong Li, Zheli Liu, Zhixuan Chu, Yiming Li |  |
| 1974 |  |  [SigDiffusions: Score-Based Diffusion Models for Time Series via Log-Signature Embeddings](https://openreview.net/forum?id=Y8KK9kjgIK) |  | 0 |  | Barbora Barancikova, Zhuoyue Huang, Cristopher Salvi |  |
| 1975 |  |  [Graph Neural Ricci Flow: Evolving Feature from a Curvature Perspective](https://openreview.net/forum?id=7b2JrzdLhA) |  | 0 |  | Jialong Chen, Bowen Deng, Zhen Wang, Chuan Chen, Zibin Zheng |  |
| 1976 |  |  [Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents](https://openreview.net/forum?id=V4y0CpX4hK) |  | 0 |  | Hanrong Zhang, Jingyuan Huang, Kai Mei, Yifei Yao, Zhenting Wang, Chenlu Zhan, Hongwei Wang, Yongfeng Zhang |  |
| 1977 |  |  [CofCA: A STEP-WISE Counterfactual Multi-hop QA benchmark](https://openreview.net/forum?id=q2DmkZ1wVe) |  | 0 |  | Jian Wu, Linyi Yang, Zhen Wang, Manabu Okumura, Yue Zhang |  |
| 1978 |  |  [Differentiable and Learnable Wireless Simulation with Geometric Transformers](https://openreview.net/forum?id=9TClCDZXeh) |  | 0 |  | Thomas Hehn, Markus Peschl, Tribhuvanesh Orekondy, Arash Behboodi, Johann Brehmer |  |
| 1979 |  |  [Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning](https://openreview.net/forum?id=QOfWubPhdS) |  | 0 |  | Haozhe Ma, Zhengding Luo, Thanh Vinh Vo, Kuankuan Sima, TzeYun Leong |  |
| 1980 |  |  [Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements](https://openreview.net/forum?id=ERce2rgMQC) |  | 0 |  | Jingyu Zhang, Ahmed Elgohary, Ahmed Magooda, Daniel Khashabi, Benjamin Van Durme |  |
| 1981 |  |  [YouTube-SL-25: A Large-Scale, Open-Domain Multilingual Sign Language Parallel Corpus](https://openreview.net/forum?id=nFVsK3QLgs) |  | 0 |  | Garrett Tanzer, Biao Zhang |  |
| 1982 |  |  [TFG-Flow: Training-free Guidance in Multimodal Generative Flow](https://openreview.net/forum?id=GK5ni7tIHp) |  | 0 |  | Haowei Lin, Shanda Li, Haotian Ye, Yiming Yang, Stefano Ermon, Yitao Liang, Jianzhu Ma |  |
| 1983 |  |  [PhiNets: Brain-inspired Non-contrastive Learning Based on Temporal Prediction Hypothesis](https://openreview.net/forum?id=5tjdRyqnSn) |  | 0 |  | Satoki Ishikawa, Makoto Yamada, Han Bao, Yuki Takezawa |  |
| 1984 |  |  [UniCon: Unidirectional Information Flow for Effective Control of Large-Scale Diffusion Models](https://openreview.net/forum?id=uJqKf24HGN) |  | 0 |  | Fanghua Yu, Jinjin Gu, Jinfan Hu, Zheyuan Li, Chao Dong |  |
| 1985 |  |  [No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models](https://openreview.net/forum?id=b3CzCCCILJ) |  | 0 |  | Seyedmorteza Sadat, Manuel Kansy, Otmar Hilliges, Romann M. Weber |  |
| 1986 |  |  [CarbonSense: A Multimodal Dataset and Baseline for Carbon Flux Modelling](https://openreview.net/forum?id=l8zRnvD95l) |  | 0 |  | Matthew Fortier, Mats Leon Richter, Oliver Sonnentag, Christopher Pal |  |
| 1987 |  |  [MuHBoost: Multi-Label Boosting For Practical Longitudinal Human Behavior Modeling](https://openreview.net/forum?id=BAelAyADqn) |  | 0 |  | Nguyen T. Thach, Patrick Habecker, Anika R. Eisenbraun, Alex Mason, Kimberly Tyler, Bilal Khan, Hau Chan |  |
| 1988 |  |  [Unlocking the Potential of Model Calibration in Federated Learning](https://openreview.net/forum?id=Osr0KZJeTX) |  | 0 |  | YunWei Chu, DongJun Han, Seyyedali Hosseinalipour, Christopher Brinton |  |
| 1989 |  |  [AIMS.au: A Dataset for the Analysis of Modern Slavery Countermeasures in Corporate Statements](https://openreview.net/forum?id=ybfmpJiKXX) |  | 0 |  | Adriana Eufrosina Bora, PierreLuc StCharles, Mirko Bronzi, Arsène Fansi Tchango, Bruno Rousseau, Kerrie L. Mengersen |  |
| 1990 |  |  [Multi-Scale Fusion for Object Representation](https://openreview.net/forum?id=nobDw4d1k7) |  | 0 |  | Rongzhen Zhao, Vivienne Huiling Wang, Juho Kannala, Joni Pajarinen |  |
| 1991 |  |  [Factor Graph-based Interpretable Neural Networks](https://openreview.net/forum?id=10DtLPsdro) |  | 0 |  | Yicong Li, Kuanjiu Zhou, Shuo Yu, Qiang Zhang, Renqiang Luo, Xiaodong Li, Feng Xia |  |
| 1992 |  |  [A Graph Enhanced Symbolic Discovery Framework For Efficient Logic Optimization](https://openreview.net/forum?id=EG9nDN3eGB) |  | 0 |  | Yinqi Bai, Jie Wang, Lei Chen, Zhihai Wang, Yufei Kuang, Mingxuan Yuan, Jianye Hao, Feng Wu |  |
| 1993 |  |  [Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning with Knowledge-guided Retrieval Augmented Generation](https://openreview.net/forum?id=oFBu7qaZpS) |  | 0 |  | Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, Cehao Yang, Jiaxin Mao, Jian Guo |  |
| 1994 |  |  [Generation and Comprehension Hand-in-Hand: Vision-guided Expression Diffusion for Boosting Referring Expression Generation and Comprehension](https://openreview.net/forum?id=1qbZekXGrp) |  | 0 |  | Jingcheng Ke, JunCheng Chen, IHong Jhuo, ChiaWen Lin, YenYu Lin |  |
| 1995 |  |  [Optimality of Matrix Mechanism on ℓpp-metric](https://openreview.net/forum?id=fbqOEOqurU) |  | 0 |  | Zongrui Zou, Jingcheng Liu, Jalaj Upadhyay |  |
| 1996 |  |  [CLIPDrag: Combining Text-based and Drag-based Instructions for Image Editing](https://openreview.net/forum?id=2HjRezQ1nj) |  | 0 |  | Ziqi Jiang, Zhen Wang, Long Chen |  |
| 1997 |  |  [Interactive Adjustment for Human Trajectory Prediction with Individual Feedback](https://openreview.net/forum?id=DCpukR83sw) |  | 0 |  | Jianhua Sun, Yuxuan Li, Liang Chai, Cewu Lu |  |
| 1998 |  |  [Transformers Provably Learn Two-Mixture of Linear Classification via Gradient Flow](https://openreview.net/forum?id=AuAj4vRPkv) |  | 0 |  | Hongru Yang, Zhangyang Wang, Jason D. Lee, Yingbin Liang |  |
| 1999 |  |  [Alchemy: Amplifying Theorem-Proving Capability Through Symbolic Mutation](https://openreview.net/forum?id=7NL74jUiMg) |  | 0 |  | Shaonan Wu, Shuai Lu, Yeyun Gong, Nan Duan, Ping Wei |  |
| 2000 |  |  [NExUME: Adaptive Training and Inference for DNNs under Intermittent Power Environments](https://openreview.net/forum?id=SFNqrHQTEP) |  | 0 |  | Cyan Subhra Mishra, Deeksha Chaudhary, Jack Sampson, Mahmut T. Kandemir, Chita R. Das |  |
| 2001 |  |  [Agent Skill Acquisition for Large Language Models via CycleQD](https://openreview.net/forum?id=Kvdh12wGC0) |  | 0 |  | So Kuroki, Taishi Nakamura, Takuya Akiba, Yujin Tang |  |
| 2002 |  |  [Learning Graph Invariance by Harnessing Spuriosity](https://openreview.net/forum?id=UsVJlgD1F7) |  | 0 |  | Tianjun Yao, Yongqiang Chen, Kai Hu, Tongliang Liu, Kun Zhang, Zhiqiang Shen |  |
| 2003 |  |  [Spiking Vision Transformer with Saccadic Attention](https://openreview.net/forum?id=qzZsz6MuEq) |  | 0 |  | Shuai Wang, Malu Zhang, Dehao Zhang, Ammar Belatreche, Yichen Xiao, Yu Liang, Yimeng Shan, Qian Sun, Enqi Zhang, Yang Yang |  |
| 2004 |  |  [Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models](https://openreview.net/forum?id=1hQKHHUsMx) |  | 0 |  | Laura Ruis, Maximilian Mozes, Juhan Bae, Siddhartha Rao Kamalakara, Dwaraknath Gnaneshwar, Acyr Locatelli, Robert Kirk, Tim Rocktäschel, Edward Grefenstette, Max Bartolo |  |
| 2005 |  |  [From Attention to Activation: Unraveling the Enigmas of Large Language Models](https://openreview.net/forum?id=IjduZQK8gM) |  | 0 |  | Prannay Kaul, Chengcheng Ma, Ismail Elezi, Jiankang Deng |  |
| 2006 |  |  [On the Linear Speedup of Personalized Federated Reinforcement Learning with Shared Representations](https://openreview.net/forum?id=BfUDZGqCAu) |  | 0 |  | Guojun Xiong, Shufan Wang, Daniel Jiang, Jian Li |  |
| 2007 |  |  [Global Identifiability of Overcomplete Dictionary Learning via L1 and Volume Minimization](https://openreview.net/forum?id=4nrcn0YoDG) |  | 0 |  | Yuchen Sun, Kejun Huang |  |
| 2008 |  |  [KBLaM: Knowledge Base augmented Language Model](https://openreview.net/forum?id=aLsMzkTej9) |  | 0 |  | Xi Wang, Taketomo Isazawa, Liana Mikaelyan, James Hensman |  |
| 2009 |  |  [Minimalistic Predictions for Online Class Constraint Scheduling](https://openreview.net/forum?id=j8lqABLgub) |  | 0 |  | Dorian Guyot, Alexandra Anna Lassota |  |
| 2010 |  |  [Do Large Language Models Truly Understand Geometric Structures?](https://openreview.net/forum?id=FjQOXenaXK) |  | 0 |  | Xiaofeng Wang, Yiming Wang, Wenhong Zhu, Rui Wang |  |
| 2011 |  |  [DeepTAGE: Deep Temporal-Aligned Gradient Enhancement for Optimizing Spiking Neural Networks](https://openreview.net/forum?id=drPDukdY3t) |  | 0 |  | Wei Liu, Li Yang, Mingxuan Zhao, Shuxun Wang, Jin Gao, Wenjuan Li, Bing Li, Weiming Hu |  |
| 2012 |  |  [Mitigating Reward Over-Optimization in RLHF via Behavior-Supported Regularization](https://openreview.net/forum?id=PNMv4r7s1i) |  | 0 |  | Juntao Dai, Taiye Chen, Yaodong Yang, Qian Zheng, Gang Pan |  |
| 2013 |  |  [New Algorithms for the Learning-Augmented k-means Problem](https://openreview.net/forum?id=Xuyp1dGAbi) |  | 0 |  | Junyu Huang, Qilong Feng, Ziyun Huang, Zhen Zhang, Jinhui Xu, Jianxin Wang |  |
| 2014 |  |  [Bridging the Semantic Gap Between Text and Table: A Case Study on NL2SQL](https://openreview.net/forum?id=qmsX2R19p9) |  | 0 |  | Lin Long, Xijun Gu, Xinjie Sun, Wentao Ye, Haobo Wang, Sai Wu, Gang Chen, Junbo Zhao |  |
| 2015 |  |  [Ask, and it shall be given: On the Turing completeness of prompting](https://openreview.net/forum?id=AS8SPTyBgw) |  | 0 |  | Ruizhong Qiu, Zhe Xu, Wenxuan Bao, Hanghang Tong |  |
| 2016 |  |  [Noisy Test-Time Adaptation in Vision-Language Models](https://openreview.net/forum?id=iylpeTI0Ql) |  | 0 |  | Chentao Cao, Zhun Zhong, Zhanke Zhou, Tongliang Liu, Yang Liu, Kun Zhang, Bo Han |  |
| 2017 |  |  [MOFFlow: Flow Matching for Structure Prediction of Metal-Organic Frameworks](https://openreview.net/forum?id=dNT3abOsLo) |  | 0 |  | Nayoung Kim, Seongsu Kim, Minsu Kim, Jinkyoo Park, Sungsoo Ahn |  |
| 2018 |  |  [QP-SNN: Quantized and Pruned Spiking Neural Networks](https://openreview.net/forum?id=MiPyle6Jef) |  | 0 |  | Wenjie Wei, Malu Zhang, Zijian Zhou, Ammar Belatreche, Yimeng Shan, Yu Liang, Honglin Cao, Jieyuan Zhang, Yang Yang |  |
| 2019 |  |  [Fantastic Copyrighted Beasts and How (Not) to Generate Them](https://openreview.net/forum?id=ftHNJmogT1) |  | 0 |  | Luxi He, Yangsibo Huang, Weijia Shi, Tinghao Xie, Haotian Liu, Yue Wang, Luke Zettlemoyer, Chiyuan Zhang, Danqi Chen, Peter Henderson |  |
| 2020 |  |  [Verifying Properties of Binary Neural Networks Using Sparse Polynomial Optimization](https://openreview.net/forum?id=9c96mGtQVR) |  | 0 |  | Jianting Yang, Srecko Ðurasinovic, Jean B. Lasserre, Victor Magron, Jun Zhao |  |
| 2021 |  |  [ReAttention: Training-Free Infinite Context with Finite Attention Scope](https://openreview.net/forum?id=KDGP8yAz5b) |  | 0 |  | Xiaoran Liu, Ruixiao Li, Zhigeng Liu, Qipeng Guo, Yuerong Song, Kai Lv, Hang Yan, Linlin Li, Qun Liu, Xipeng Qiu |  |
| 2022 |  |  [Mitigating the Backdoor Effect for Multi-Task Model Merging via Safety-Aware Subspace](https://openreview.net/forum?id=dqMqAaw7Sq) |  | 0 |  | Jinluan Yang, Anke Tang, Didi Zhu, Zhengyu Chen, Li Shen, Fei Wu |  |
| 2023 |  |  [An Information Criterion for Controlled Disentanglement of Multimodal Data](https://openreview.net/forum?id=3n4RY25UWP) |  | 0 |  | Chenyu Wang, Sharut Gupta, Xinyi Zhang, Sana Tonekaboni, Stefanie Jegelka, Tommi S. Jaakkola, Caroline Uhler |  |
| 2024 |  |  [Fréchet Wavelet Distance: A Domain-Agnostic Metric for Image Generation](https://openreview.net/forum?id=QinkNNKZ3b) |  | 0 |  | Lokesh Veeramacheneni, Moritz Wolter, Hilde Kuehne, Juergen Gall |  |
| 2025 |  |  [RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction](https://openreview.net/forum?id=gRmWtOnTLK) |  | 0 |  | Peng Liu, Dongyang Dai, Zhiyong Wu |  |
| 2026 |  |  [SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal](https://openreview.net/forum?id=YfKNaRktan) |  | 0 |  | Tinghao Xie, Xiangyu Qi, Yi Zeng, Yangsibo Huang, Udari Madhushani Sehwag, Kaixuan Huang, Luxi He, Boyi Wei, Dacheng Li, Ying Sheng, Ruoxi Jia, Bo Li, Kai Li, Danqi Chen, Peter Henderson, Prateek Mittal |  |
| 2027 |  |  [Towards Auto-Regressive Next-Token Prediction: In-context Learning Emerges from Generalization](https://openreview.net/forum?id=gK1rl98VRp) |  | 0 |  | Zixuan Gong, Xiaolin Hu, Huayi Tang, Yong Liu |  |
| 2028 |  |  [SimPER: A Minimalist Approach to Preference Alignment without Hyperparameters](https://openreview.net/forum?id=jfwe9qNqRi) |  | 0 |  | Teng Xiao, Yige Yuan, Zhengyu Chen, Mingxiao Li, Shangsong Liang, Zhaochun Ren, Vasant G. Honavar |  |
| 2029 |  |  [Zero-cost Proxy for Adversarial Robustness Evaluation](https://openreview.net/forum?id=zHf7hOfeer) |  | 0 |  | Yuqi Feng, Yuwei Ou, Jiahao Fan, Yanan Sun |  |
| 2030 |  |  [NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation](https://openreview.net/forum?id=p66a00KLWN) |  | 0 |  | Zhiyuan Liu, Yanchen Luo, Han Huang, Enzhi Zhang, Sihang Li, Junfeng Fang, Yaorui Shi, Xiang Wang, Kenji Kawaguchi, TatSeng Chua |  |
| 2031 |  |  [PnP-Flow: Plug-and-Play Image Restoration with Flow Matching](https://openreview.net/forum?id=5AtHrq3B5R) |  | 0 |  | Ségolène Tiffany Martin, Anne Gagneux, Paul Hagemann, Gabriele Steidl |  |
| 2032 |  |  [Logicbreaks: A Framework for Understanding Subversion of Rule-based Inference](https://openreview.net/forum?id=pljYMCYDWJ) |  | 0 |  | Anton Xue, Avishree Khare, Rajeev Alur, Surbhi Goel, Eric Wong |  |
| 2033 |  |  [Periodic Materials Generation using Text-Guided Joint Diffusion Model](https://openreview.net/forum?id=AkBrb7yQ0G) |  | 0 |  | Kishalay Das, Subhojyoti Khastagir, Pawan Goyal, SeungCheol Lee, Satadeep Bhattacharjee, Niloy Ganguly |  |
| 2034 |  |  [Understanding Virtual Nodes: Oversquashing and Node Heterogeneity](https://openreview.net/forum?id=NmcOAwRyH5) |  | 0 |  | Joshua Southern, Francesco Di Giovanni, Michael M. Bronstein, Johannes F. Lutzeyer |  |
| 2035 |  |  [TODO: Enhancing LLM Alignment with Ternary Preferences](https://openreview.net/forum?id=utkGLDSNOk) |  | 0 |  | Yuxiang Guo, Lu Yin, Bo Jiang, Jiaqi Zhang |  |
| 2036 |  |  [Occlusion-aware Non-Rigid Point Cloud Registration via Unsupervised Neural Deformation Correntropy](https://openreview.net/forum?id=cjJqU40nYS) |  | 0 |  | Mingyang Zhao, Gaofeng Meng, DongMing Yan |  |
| 2037 |  |  [Operator Deep Smoothing for Implied Volatility](https://openreview.net/forum?id=DPlUWG4WMw) |  | 0 |  | Ruben Wiedemann, Antoine Jacquier, Lukas Gonon |  |
| 2038 |  |  [Re-evaluating Open-ended Evaluation of Large Language Models](https://openreview.net/forum?id=kbOAIXKWgx) |  | 0 |  | Siqi Liu, Ian Gemp, Luke Marris, Georgios Piliouras, Nicolas Heess, Marc Lanctot |  |
| 2039 |  |  [An Intelligent Agentic System for Complex Image Restoration Problems](https://openreview.net/forum?id=3RLxccFPHz) |  | 0 |  | Kaiwen Zhu, Jinjin Gu, Zhiyuan You, Yu Qiao, Chao Dong |  |
| 2040 |  |  [Open-Set Graph Anomaly Detection via Normal Structure Regularisation](https://openreview.net/forum?id=kSvoX0xdlO) |  | 0 |  | Qizhou Wang, Guansong Pang, Mahsa Salehi, Xiaokun Xia, Christopher Leckie |  |
| 2041 |  |  [QERA: an Analytical Framework for Quantization Error Reconstruction](https://openreview.net/forum?id=LB5cKhgOTu) |  | 0 |  | Cheng Zhang, Jeffrey T. H. Wong, Can Xiao, George Anthony Constantinides, Yiren Zhao |  |
| 2042 |  |  [Advancing Prompt-Based Methods for Replay-Independent General Continual Learning](https://openreview.net/forum?id=V6uxd8MEqw) |  | 0 |  | Zhiqi Kang, Liyuan Wang, Xingxing Zhang, Karteek Alahari |  |
| 2043 |  |  [Is Your Model Really A Good Math Reasoner? Evaluating Mathematical Reasoning with Checklist](https://openreview.net/forum?id=nDvgHIBRxQ) |  | 0 |  | Zihao Zhou, Shudong Liu, Maizhen Ning, Wei Liu, Jindong Wang, Derek F. Wong, Xiaowei Huang, Qiufeng Wang, Kaizhu Huang |  |
| 2044 |  |  [Beyond Circuit Connections: A Non-Message Passing Graph Transformer Approach for Quantum Error Mitigation](https://openreview.net/forum?id=XnVttczoAV) |  | 0 |  | Tianyi Bao, Xinyu Ye, Hang Ruan, Chang Liu, Wenjie Wu, Junchi Yan |  |
| 2045 |  |  [ViSAGe: Video-to-Spatial Audio Generation](https://openreview.net/forum?id=8bF1Vaj9tm) |  | 0 |  | Jaeyeon Kim, Heeseung Yun, Gunhee Kim |  |
| 2046 |  |  [Learning Long Range Dependencies on Graphs via Random Walks](https://openreview.net/forum?id=kJ5H7oGT2M) |  | 0 |  | Dexiong Chen, Till Hendrik Schulz, Karsten M. Borgwardt |  |
| 2047 |  |  [ReSi: A Comprehensive Benchmark for Representational Similarity Measures](https://openreview.net/forum?id=PRvdO3nfFi) |  | 0 |  | Max Klabunde, Tassilo Wald, Tobias Schumacher, Klaus H. MaierHein, Markus Strohmaier, Florian Lemmerich |  |
| 2048 |  |  [EvA: Erasing Spurious Correlations with Activations](https://openreview.net/forum?id=zKvrOOBouT) |  | 0 |  | Qiyuan He, Kai Xu, Angela Yao |  |
| 2049 |  |  [Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models](https://openreview.net/forum?id=e2ONKX6qzJ) |  | 0 |  | Seyedmorteza Sadat, Otmar Hilliges, Romann M. Weber |  |
| 2050 |  |  [An Online Learning Theory of Trading-Volume Maximization](https://openreview.net/forum?id=OvU9u6wS2J) |  | 0 |  | Tommaso Cesari, Roberto Colomboni |  |
| 2051 |  |  [Fast and Accurate Blind Flexible Docking](https://openreview.net/forum?id=iezDdA9oeB) |  | 0 |  | Zizhuo Zhang, Lijun Wu, Kaiyuan Gao, Jiangchao Yao, Tao Qin, Bo Han |  |
| 2052 |  |  [Efficient Discovery of Pareto Front for Multi-Objective Reinforcement Learning](https://openreview.net/forum?id=fDGPIuCdGi) |  | 0 |  | Ruohong Liu, Yuxin Pan, Linjie Xu, Lei Song, Pengcheng You, Yize Chen, Jiang Bian |  |
| 2053 |  |  [CAMEx: Curvature-aware Merging of Experts](https://openreview.net/forum?id=nT2u0M0nf8) |  | 0 |  | Viet Dung Nguyen, Minh Nguyen Hoang, Luc Q. Nguyen, Rachel S. Y. Teo, Tan Minh Nguyen, Linh Duy Tran |  |
| 2054 |  |  [Efficient Online Pruning and Abstraction for Imperfect Information Extensive-Form Games](https://openreview.net/forum?id=MTcgsz1SHr) |  | 0 |  | Boning Li, Longbo Huang |  |
| 2055 |  |  [metabench - A Sparse Benchmark of Reasoning and Knowledge in Large Language Models](https://openreview.net/forum?id=4T33izzFpK) |  | 0 |  | Alexander Kipnis, Konstantinos Voudouris, Luca M. Schulze Buschoff, Eric Schulz |  |
| 2056 |  |  [CAT-3DGS: A Context-Adaptive Triplane Approach to Rate-Distortion-Optimized 3DGS Compression](https://openreview.net/forum?id=m3KuuE2ozw) |  | 0 |  | YuTing Zhan, ChengYuan Ho, Hebi Yang, YiHsin Chen, JuiChiu Chiang, YuLun Liu, WenHsiao Peng |  |
| 2057 |  |  [On the Benefits of Attribute-Driven Graph Domain Adaptation](https://openreview.net/forum?id=t2TUw5nJsW) |  | 0 |  | Ruiyi Fang, Bingheng Li, Zhao Kang, Qiuhao Zeng, Nima Hosseini Dashtbayaz, Ruizhi Pu, Charles Ling, Boyu Wang |  |
| 2058 |  |  [EgoSim: Egocentric Exploration in Virtual Worlds with Multi-modal Conditioning](https://openreview.net/forum?id=zAyS5aRKV8) |  | 0 |  | Wei Yu, Songheng Yin, Steve Easterbrook, Animesh Garg |  |
| 2059 |  |  [Capability Localization: Capabilities Can be Localized rather than Individual Knowledge](https://openreview.net/forum?id=f6r1mYwM1g) |  | 0 |  | Xiusheng Huang, Jiaxiang Liu, Yequan Wang, Jun Zhao, Kang Liu |  |
| 2060 |  |  [Semi-Supervised CLIP Adaptation by Enforcing Semantic and Trapezoidal Consistency](https://openreview.net/forum?id=97D725GJtQ) |  | 0 |  | Kai Gan, Bo Ye, MinLing Zhang, Tong Wei |  |
| 2061 |  |  [Learning Successor Features with Distributed Hebbian Temporal Memory](https://openreview.net/forum?id=wYJII5BRYU) |  | 0 |  | Evgenii Aleksandrovich Dzhivelikian, Petr Kuderov, Aleksandr Panov |  |
| 2062 |  |  [Divergence of Neural Tangent Kernel in Classification Problems](https://openreview.net/forum?id=VEJzjAvaIy) |  | 0 |  | Zixiong Yu, Songtao Tian, Guhan Chen |  |
| 2063 |  |  [Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model](https://openreview.net/forum?id=sAYnDWaGd5) |  | 0 |  | Chaochen Gao, Xing Wu, Qi Fu, Songlin Hu |  |
| 2064 |  |  [Robust LLM safeguarding via refusal feature adversarial training](https://openreview.net/forum?id=s5orchdb33) |  | 0 |  | Lei Yu, Virginie Do, Karen Hambardzumyan, Nicola Cancedda |  |
| 2065 |  |  [The Rise and Down of Babel Tower: Investigating the Evolution Process of Multilingual Code Large Language Model](https://openreview.net/forum?id=eznTVIM3bs) |  | 0 |  | Jiawei Chen, Wentao Chen, Jing Su, Jingjing Xu, Hongyu Lin, Mengjie Ren, Yaojie Lu, Xianpei Han, Le Sun |  |
| 2066 |  |  [Unsupervised Zero-Shot Reinforcement Learning via Dual-Value Forward-Backward Representation](https://openreview.net/forum?id=0QnKnt411O) |  | 0 |  | Jingbo Sun, Songjun Tu, Qichao Zhang, Haoran Li, Xin Liu, Yaran Chen, Ke Chen, Dongbin Zhao |  |
| 2067 |  |  [To Code or Not To Code? Exploring Impact of Code in Pre-training](https://openreview.net/forum?id=zSfeN1uAcx) |  | 0 |  | Viraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli, Marzieh Fadaee, Ahmet Üstün, Sara Hooker |  |
| 2068 |  |  [Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data](https://openreview.net/forum?id=sMyXP8Tanm) |  | 0 |  | Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, Chongxuan Li |  |
| 2069 |  |  [TempMe: Video Temporal Token Merging for Efficient Text-Video Retrieval](https://openreview.net/forum?id=lVp97zZ5i8) |  | 0 |  | Leqi Shen, Tianxiang Hao, Tao He, Sicheng Zhao, Yifeng Zhang, Pengzhang Liu, Yongjun Bao, Guiguang Ding |  |
| 2070 |  |  [Predicting the Energy Landscape of Stochastic Dynamical System via Physics-informed Self-supervised Learning](https://openreview.net/forum?id=PxRATSTDlS) |  | 0 |  | Ruikun Li, Huandong Wang, Qingmin Liao, Yong Li |  |
| 2071 |  |  [Dimension Agnostic Neural Processes](https://openreview.net/forum?id=uGJxl2odR0) |  | 0 |  | Hyungi Lee, Chaeyun Jang, Dongbok Lee, Juho Lee |  |
| 2072 |  |  [Variational Bayesian Pseudo-Coreset](https://openreview.net/forum?id=0NAVeUm7sk) |  | 0 |  | Hyungi Lee, Seungyoo Lee, Juho Lee |  |
| 2073 |  |  [Biologically Plausible Brain Graph Transformer](https://openreview.net/forum?id=rQyg6MnsDb) |  | 0 |  | Ciyuan Peng, Yuelong Huang, Qichao Dong, Shuo Yu, Feng Xia, Chengqi Zhang, Yaochu Jin |  |
| 2074 |  |  [Building, Reusing, and Generalizing Abstract Representations from Concrete Sequences](https://openreview.net/forum?id=xIUUnzrUtD) |  | 0 |  | Shuchen Wu, Mirko Thalmann, Peter Dayan, Zeynep Akata, Eric Schulz |  |
| 2075 |  |  [HD-Painter: High-Resolution and Prompt-Faithful Text-Guided Image Inpainting with Diffusion Models](https://openreview.net/forum?id=6lB5qtdYAg) |  | 0 |  | Hayk Manukyan, Andranik Sargsyan, Barsegh Atanyan, Zhangyang Wang, Shant Navasardyan, Humphrey Shi |  |
| 2076 |  |  [ViBiDSampler: Enhancing Video Interpolation Using Bidirectional Diffusion Sampler](https://openreview.net/forum?id=nNYA7tcJSE) |  | 0 |  | Serin Yang, Taesung Kwon, Jong Chul Ye |  |
| 2077 |  |  [Stochastic Bandits Robust to Adversarial Attacks](https://openreview.net/forum?id=vOFx8HDcvF) |  | 0 |  | Xuchuang Wang, Maoli Liu, Jinhang Zuo, Xutong Liu, John C. S. Lui, Mohammad Hajiesmaili |  |
| 2078 |  |  [UV-Attack: Physical-World Adversarial Attacks on Person Detection via Dynamic-NeRF-based UV Mapping](https://openreview.net/forum?id=pqeWzZTrZY) |  | 0 |  | Yanjie Li, Kaisheng Liang, Bin Xiao |  |
| 2079 |  |  [TestGenEval: A Real World Unit Test Generation and Test Completion Benchmark](https://openreview.net/forum?id=7o6SG5gVev) |  | 0 |  | Kush Jain, Gabriel Synnaeve, Baptiste Rozière |  |
| 2080 |  |  [Efficient Distribution Matching of Representations via Noise-Injected Deep InfoMax](https://openreview.net/forum?id=mAmCdASmJ5) |  | 0 |  | Ivan Butakov, Alexander Semenenko, Alexander Tolmachev, Andrey Gladkov, Marina Munkhoeva, Alexey A. Frolov |  |
| 2081 |  |  [Model-agnostic meta-learners for estimating heterogeneous treatment effects over time](https://openreview.net/forum?id=QGGNvKaoIU) |  | 0 |  | Dennis Frauen, Konstantin Hess, Stefan Feuerriegel |  |
| 2082 |  |  [F3Set: Towards Analyzing Fast, Frequent, and Fine-grained Events from Videos](https://openreview.net/forum?id=vlg5WRKHxh) |  | 0 |  | Zhaoyu Liu, Kan Jiang, Murong Ma, Zhe Hou, Yun Lin, Jin Song Dong |  |
| 2083 |  |  [Training Large Language Models for Retrieval-Augmented Question Answering through Backtracking Correction](https://openreview.net/forum?id=IOg47mg74i) |  | 0 |  | Huawen Feng, Zekun Yao, Junhao Zheng, Qianli Ma |  |
| 2084 |  |  [INS: Interaction-aware Synthesis to Enhance Offline Multi-agent Reinforcement Learning](https://openreview.net/forum?id=kxD2LlPr40) |  | 0 |  | Yuqian Fu, Yuanheng Zhu, Jian Zhao, Jiajun Chai, Dongbin Zhao |  |
| 2085 |  |  [ϕ-Update: A Class of Policy Update Methods with Policy Convergence Guarantee](https://openreview.net/forum?id=fh7GYa7cjO) |  | 0 |  | Wenye Li, Jiacai Liu, Ke Wei |  |
| 2086 |  |  [SAVA: Scalable Learning-Agnostic Data Valuation](https://openreview.net/forum?id=0UCoWxPhQ4) |  | 0 |  | Samuel Kessler, Tam Le, Vu Nguyen |  |
| 2087 |  |  [Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving](https://openreview.net/forum?id=VNckp7JEHn) |  | 0 |  | Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, Yiming Yang |  |
| 2088 |  |  [Can Textual Gradient Work in Federated Learning?](https://openreview.net/forum?id=Cy5IKvYbR3) |  | 0 |  | Minghui Chen, Ruinan Jin, Wenlong Deng, Yuanyuan Chen, Zhi Huang, Han Yu, Xiaoxiao Li |  |
| 2089 |  |  [How Low Can You Go? Searching for the Intrinsic Dimensionality of Complex Networks using Metric Node Embeddings](https://openreview.net/forum?id=V71ITh2w40) |  | 0 |  | Nikolaos Nakis, Niels Raunkjær Holm, Andreas Lyhne Fiehn, Morten Mørup |  |
| 2090 |  |  [Synthesizing Realistic fMRI: A Physiological Dynamics-Driven Hierarchical Diffusion Model for Efficient fMRI Acquisition](https://openreview.net/forum?id=zZ6TT254Np) |  | 0 |  | Yufan Hu, Yu Jiang, Wuyang Li, Yixuan Yuan |  |
| 2091 |  |  [MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation](https://openreview.net/forum?id=br4H61LOoI) |  | 0 |  | Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, Jiaya Jia |  |
| 2092 |  |  [Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?](https://openreview.net/forum?id=8EtSBX41mt) |  | 0 |  | Egor Zverev, Sahar Abdelnabi, Soroush Tabesh, Mario Fritz, Christoph H. Lampert |  |
| 2093 |  |  [Risk-Sensitive Diffusion: Robustly Optimizing Diffusion Models with Noisy Samples](https://openreview.net/forum?id=b0WpXBABdu) |  | 0 |  | Yangming Li, Max Ruiz Luyten, Mihaela van der Schaar |  |
| 2094 |  |  [Adversarial Mixup Unlearning](https://openreview.net/forum?id=GcbhbZsgiu) |  | 0 |  | Zhuoyi Peng, Yixuan Tang, Yi Yang |  |
| 2095 |  |  [Learning on One Mode: Addressing Multi-modality in Offline Reinforcement Learning](https://openreview.net/forum?id=upkxzurnLC) |  | 0 |  | Mianchu Wang, Yue Jin, Giovanni Montana |  |
| 2096 |  |  [GraphArena: Evaluating and Exploring Large Language Models on Graph Computation](https://openreview.net/forum?id=Y1r9yCMzeA) |  | 0 |  | Jianheng Tang, Qifan Zhang, Yuhan Li, Nuo Chen, Jia Li |  |
| 2097 |  |  [Continuous Ensemble Weather Forecasting with Diffusion models](https://openreview.net/forum?id=ePEZvQNFDW) |  | 0 |  | Martin Andrae, Tomas Landelius, Joel Oskarsson, Fredrik Lindsten |  |
| 2098 |  |  [VTDexManip: A Dataset and Benchmark for Visual-tactile Pretraining and Dexterous Manipulation with Reinforcement Learning](https://openreview.net/forum?id=jf7C7EGw21) |  | 0 |  | Qingtao Liu, Yu Cui, Zhengnan Sun, Gaofeng Li, Jiming Chen, Qi Ye |  |
| 2099 |  |  [Breaking Free from MMI: A New Frontier in Rationalization by Probing Input Utilization](https://openreview.net/forum?id=WZ0s2smcKP) |  | 0 |  | Wei Liu, Zhiying Deng, Zhongyu Niu, Jun Wang, Haozhao Wang, Zhigang Zeng, Ruixuan Li |  |
| 2100 |  |  [HiBug2: Efficient and Interpretable Error Slice Discovery for Comprehensive Model Debugging](https://openreview.net/forum?id=l30moNjSY9) |  | 0 |  | Muxi Chen, Chenchen Zhao, Qiang Xu |  |
| 2101 |  |  [LiveXiv - A Multi-Modal live benchmark based on Arxiv papers content](https://openreview.net/forum?id=SulRfnEVK4) |  | 0 |  | Nimrod Shabtay, Felipe Maia Polo, Sivan Doveh, Wei Lin, Muhammad Jehanzeb Mirza, Leshem Choshen, Mikhail Yurochkin, Yuekai Sun, Assaf Arbelle, Leonid Karlinsky, Raja Giryes |  |
| 2102 |  |  [SEPARATE: A Simple Low-rank Projection for Gradient Compression in Modern Large-scale Model Training Process](https://openreview.net/forum?id=8HuLgtjqOD) |  | 0 |  | Hanzhen Zhao, Xingyu Xie, Cong Fang, Zhouchen Lin |  |
| 2103 |  |  [FIRING-Net: A filtered feature recycling network for speech enhancement](https://openreview.net/forum?id=TJp3LnQgSX) |  | 0 |  | Xinmeng Xu, Yiqun Zhang, Jizhen Li, Yuhong Yang, Yong Luo, Weiping Tu |  |
| 2104 |  |  [Conformal Generative Modeling with Improved Sample Efficiency through Sequential Greedy Filtering](https://openreview.net/forum?id=1i6lkavJ94) |  | 0 |  | KlausRudolf Kladny, Bernhard Schölkopf, Michael Muehlebach |  |
| 2105 |  |  [Federated Continual Learning Goes Online: Uncertainty-Aware Memory Management for Vision Tasks and Beyond](https://openreview.net/forum?id=f65RuQgVlp) |  | 0 |  | Giuseppe Serra, Florian Buettner |  |
| 2106 |  |  [GaussianBlock: Building Part-Aware Compositional and Editable 3D Scene by Primitives and Gaussians](https://openreview.net/forum?id=wrXCIsysqB) |  | 0 |  | Shuyi Jiang, Qihao Zhao, Hossein Rahmani, De Wen Soh, Jun Liu, Na Zhao |  |
| 2107 |  |  [Equivariant Masked Position Prediction for Efficient Molecular Representation](https://openreview.net/forum?id=Nue5iMj8n6) |  | 0 |  | Junyi An, Chao Qu, Yunfei Shi, Xinhao Liu, Qianwei Tang, Fenglei Cao, Yuan Qi |  |
| 2108 |  |  [Towards Understanding the Universality of Transformers for Next-Token Prediction](https://openreview.net/forum?id=yWoV4Ca6ji) |  | 0 |  | Michael Eli Sander, Gabriel Peyré |  |
| 2109 |  |  [A-Bench: Are LMMs Masters at Evaluating AI-generated Images?](https://openreview.net/forum?id=4muXQ5r8Ol) |  | 0 |  | Zicheng Zhang, Haoning Wu, Chunyi Li, Yingjie Zhou, Wei Sun, Xiongkuo Min, Zijian Chen, Xiaohong Liu, Weisi Lin, Guangtao Zhai |  |
| 2110 |  |  [IDInit: A Universal and Stable Initialization Method for Neural Network Training](https://openreview.net/forum?id=LFiaoYnP6T) |  | 0 |  | Yu Pan, Chaozheng Wang, Zekai Wu, Qifan Wang, Min Zhang, Zenglin Xu |  |
| 2111 |  |  [Tighter Privacy Auditing of DP-SGD in the Hidden State Threat Model](https://openreview.net/forum?id=xzKFnsJIXL) |  | 0 |  | Tudor Ioan Cebere, Aurélien Bellet, Nicolas Papernot |  |
| 2112 |  |  [Improving Complex Reasoning with Dynamic Prompt Corruption: A Soft Prompt Optimization Approach](https://openreview.net/forum?id=h7Qz1ulnvF) |  | 0 |  | Sinan Fan, Liang Xie, Chen Shen, Ge Teng, Xiaosong Yuan, Xiaofeng Zhang, Chenxi Huang, Wenxiao Wang, Xiaofei He, Jieping Ye |  |
| 2113 |  |  [DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework for Talking head Video Generation](https://openreview.net/forum?id=vjHySpxDsv) |  | 0 |  | Hanbo Cheng, Limin Lin, Chenyu Liu, Pengcheng Xia, Pengfei Hu, Jiefeng Ma, Jun Du, Jia Pan |  |
| 2114 |  |  [Precedence-Constrained Winter Value for Effective Graph Data Valuation](https://openreview.net/forum?id=tVRVE0OAyb) |  | 0 |  | Hongliang Chi, Wei Jin, Charu C. Aggarwal, Yao Ma |  |
| 2115 |  |  [Federated Few-Shot Class-Incremental Learning](https://openreview.net/forum?id=ZiPoAlKf9Y) |  | 0 |  | Muhammad Anwar Ma'sum, Mahardhika Pratama, Lin Liu, Habibullah, Ryszard Kowalczyk |  |
| 2116 |  |  [GlycanML: A Multi-Task and Multi-Structure Benchmark for Glycan Machine Learning](https://openreview.net/forum?id=owEQ0FTfVj) |  | 0 |  | Minghao Xu, Yunteng Geng, Yihang Zhang, Ling Yang, Jian Tang, Wentao Zhang |  |
| 2117 |  |  [Optimistic Games for Combinatorial Bayesian Optimization with Application to Protein Design](https://openreview.net/forum?id=xiyzCfXTS6) |  | 0 |  | Melis Ilayda Bal, Pier Giuseppe Sessa, Mojmir Mutny, Andreas Krause |  |
| 2118 |  |  [Iterative Substructure Extraction for Molecular Relational Learning with Interactive Graph Information Bottleneck](https://openreview.net/forum?id=3kiZ5S5WkY) |  | 0 |  | Shuai Zhang, Junfeng Fang, Xuqiang Li, Hongxin Xiang, Alan Xia, Ye Wei, Wenjie Du, Yang Wang |  |
| 2119 |  |  [Local-Prompt: Extensible Local Prompts for Few-Shot Out-of-Distribution Detection](https://openreview.net/forum?id=Ew3VifXaxZ) |  | 0 |  | Fanhu Zeng, Zhen Cheng, Fei Zhu, Hongxin Wei, XuYao Zhang |  |
| 2120 |  |  [QuaDiM: A Conditional Diffusion Model For Quantum State Property Estimation](https://openreview.net/forum?id=P7f55HQtV8) |  | 0 |  | Yehui Tang, Mabiao Long, Junchi Yan |  |
| 2121 |  |  [Aligning Human Motion Generation with Human Perceptions](https://openreview.net/forum?id=QOHgjY5KDp) |  | 0 |  | Haoru Wang, Wentao Zhu, Luyi Miao, Yishu Xu, Feng Gao, Qi Tian, Yizhou Wang |  |
| 2122 |  |  [Centrality-guided Pre-training for Graph](https://openreview.net/forum?id=X8E65IxA73) |  | 0 |  | Bin Liang, Shiwei Chen, Lin Gui, Hui Wang, Yue Yu, Ruifeng Xu, KamFai Wong |  |
| 2123 |  |  [Vision and Language Synergy for Rehearsal Free Continual Learning](https://openreview.net/forum?id=9aZ2ixiYGd) |  | 0 |  | Muhammad Anwar Ma'sum, Mahardhika Pratama, Savitha Ramasamy, Lin Liu, Habibullah, Ryszard Kowalczyk |  |
| 2124 |  |  [Towards a General Time Series Anomaly Detector with Adaptive Bottlenecks and Dual Adversarial Decoders](https://openreview.net/forum?id=aKcd7ImG5e) |  | 0 |  | Qichao Shentu, Beibu Li, Kai Zhao, Yang Shu, Zhongwen Rao, Lujia Pan, Bin Yang, Chenjuan Guo |  |
| 2125 |  |  [Finding Shared Decodable Concepts and their Negations in the Brain](https://openreview.net/forum?id=L07zWidgdW) |  | 0 |  | Cory Daniel Efird, Alex Murphy, Joel Zylberberg, Alona Fyshe |  |
| 2126 |  |  [A Conditional Independence Test in the Presence of Discretization](https://openreview.net/forum?id=gqbbL7k8BF) |  | 0 |  | Boyang Sun, Yu Yao, GuangYuan Hao, Yumou Qiu, Kun Zhang |  |
| 2127 |  |  [Innovative Thinking, Infinite Humor: Humor Research of Large Language Models through Structured Thought Leaps](https://openreview.net/forum?id=CGhgB8Kz8i) |  | 0 |  | Han Wang, Yilin Zhao, Dian Li, Xiaohan Wang, Sinbadliu, Xuguang Lan, Hui Wang |  |
| 2128 |  |  [Fantastic Targets for Concept Erasure in Diffusion Models and Where To Find Them](https://openreview.net/forum?id=tZdqL5FH7w) |  | 0 |  | Anh Tuan Bui, ThuyTrang Vu, Long Tung Vuong, Trung Le, Paul Montague, Tamas Abraham, Junae Kim, Dinh Phung |  |
| 2129 |  |  [PhyloVAE: Unsupervised Learning of Phylogenetic Trees via Variational Autoencoders](https://openreview.net/forum?id=Z8TglKXDWm) |  | 0 |  | Tianyu Xie, Harry Richman, Jiansi Gao, Frederick A. Matsen IV, Cheng Zhang |  |
| 2130 |  |  [Self-Evolved Reward Learning for LLMS](https://openreview.net/forum?id=Zonhl0c9I0) |  | 0 |  | Chenghua Huang, Zhizhen Fan, Lu Wang, Fangkai Yang, Pu Zhao, Zeqi Lin, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang |  |
| 2131 |  |  [Long-time asymptotics of noisy SVGD outside the population limit](https://openreview.net/forum?id=X7eAhXcps1) |  | 0 |  | Victor Priser, Pascal Bianchi, Adil Salim |  |
| 2132 |  |  [The Computational Complexity of Positive Non-Clashing Teaching in Graphs](https://openreview.net/forum?id=Jd3Vd7GCyq) |  | 0 |  | Robert Ganian, Liana Khazaliya, Fionn Mc Inerney, Mathis Rocton |  |
| 2133 |  |  [VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents](https://openreview.net/forum?id=2snKOc7TVp) |  | 0 |  | Xiao Liu, Tianjie Zhang, Yu Gu, Iat Long Iong, Xixuan Song, Yifan Xu, Shudan Zhang, Hanyu Lai, Jiadai Sun, Xinyue Yang, Yu Yang, Zehan Qi, Shuntian Yao, Xueqiao Sun, Siyi Cheng, Qinkai Zheng, Hao Yu, Hanchen Zhang, Wenyi Hong, Ming Ding, Lihang Pan, Xiaotao Gu, Aohan Zeng, Zhengxiao Du, Chan Hee Song, Yu Su, Yuxiao Dong, Jie Tang |  |
| 2134 |  |  [Rethinking Multiple-Instance Learning From Feature Space to Probability Space](https://openreview.net/forum?id=torbeUlslS) |  | 0 |  | Zhaolong Du, Shasha Mao, Xuequan Lu, Mengnan Qi, Yimeng Zhang, Jing Gu, Licheng Jiao |  |
| 2135 |  |  [Attribute-based Visual Reprogramming for Vision-Language Models](https://openreview.net/forum?id=j964C6y92q) |  | 0 |  | Chengyi Cai, Zesheng Ye, Lei Feng, Jianzhong Qi, Feng Liu |  |
| 2136 |  |  [Inverse Constitutional AI: Compressing Preferences into Principles](https://openreview.net/forum?id=9FRwkPw3Cn) |  | 0 |  | Arduin Findeis, Timo Kaufmann, Eyke Hüllermeier, Samuel Albanie, Robert D. Mullins |  |
| 2137 |  |  [Spreading Out-of-Distribution Detection on Graphs](https://openreview.net/forum?id=p1TBYyqy8v) |  | 0 |  | Daeho Um, Jongin Lim, Sunoh Kim, Yuneil Yeo, Yoonho Jung |  |
| 2138 |  |  [Why Does the Effective Context Length of LLMs Fall Short?](https://openreview.net/forum?id=eoln5WgrPx) |  | 0 |  | Chenxin An, Jun Zhang, Ming Zhong, Lei Li, Shansan Gong, Yao Luo, Jingjing Xu, Lingpeng Kong |  |
| 2139 |  |  [Don't Take Things Out of Context: Attention Intervention for Enhancing Chain-of-Thought Reasoning in Large Language Models](https://openreview.net/forum?id=W6yIKliMot) |  | 0 |  | Shaotian Yan, Chen Shen, Wenxiao Wang, Liang Xie, Junjie Liu, Jieping Ye |  |
| 2140 |  |  [Towards Self-Supervised Covariance Estimation in Deep Heteroscedastic Regression](https://openreview.net/forum?id=Q1kPHLUbhi) |  | 0 |  | Megh Shukla, Aziz Shameem, Mathieu Salzmann, Alexandre Alahi |  |
| 2141 |  |  [G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model](https://openreview.net/forum?id=px1674Wp3C) |  | 0 |  | Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, Lingpeng Kong |  |
| 2142 |  |  [Complementary Label Learning with Positive Label Guessing and Negative Label Enhancement](https://openreview.net/forum?id=LPRxGZ7Oax) |  | 0 |  | Yuhang Li, Zhuying Li, Yuheng Jia |  |
| 2143 |  |  [Emergence of a High-Dimensional Abstraction Phase in Language Transformers](https://openreview.net/forum?id=0fD3iIBhlV) |  | 0 |  | Emily Cheng, Diego Doimo, Corentin Kervadec, Iuri Macocco, Lei Yu, Alessandro Laio, Marco Baroni |  |
| 2144 |  |  [Collaborative Discrete-Continuous Black-Box Prompt Learning for Language Models](https://openreview.net/forum?id=sdLGY9Dj5r) |  | 0 |  | Hualin Zhang, Haozhen Zhang, Zhekai Liu, Bin Gu, Yi Chang |  |
| 2145 |  |  [Relation-Aware Diffusion for Heterogeneous Graphs with Partially Observed Features](https://openreview.net/forum?id=TPYwwqF0bv) |  | 0 |  | Daeho Um, Yoonji Lee, Jiwoong Park, Seulki Park, Yuneil Yeo, SeongJin Ahn |  |
| 2146 |  |  [Rotated Runtime Smooth: Training-Free Activation Smoother for accurate INT4 inference](https://openreview.net/forum?id=WG7GzGx3G9) |  | 0 |  | Ke Yi, Zengke Liu, Jianwei Zhang, Chengyuan Li, Tong Zhang, Junyang Lin, Jingren Zhou |  |
| 2147 |  |  [Conditional Diffusion Models are Minimax-Optimal and Manifold-Adaptive for Conditional Distribution Estimation](https://openreview.net/forum?id=NltQraRnbW) |  | 0 |  | Rong Tang, Lizhen Lin, Yun Yang |  |
| 2148 |  |  [Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance](https://openreview.net/forum?id=jjCB27TMK3) |  | 0 |  | Jiasheng Ye, Peiju Liu, Tianxiang Sun, Jun Zhan, Yunhua Zhou, Xipeng Qiu |  |
| 2149 |  |  [econSG: Efficient and Multi-view Consistent Open-Vocabulary 3D Semantic Gaussians](https://openreview.net/forum?id=qSEEQPNbu4) |  | 0 |  | Can Zhang, Gim Hee Lee |  |
| 2150 |  |  [AgentStudio: A Toolkit for Building General Virtual Agents](https://openreview.net/forum?id=axUf8BOjnH) |  | 0 |  | Longtao Zheng, Zhiyuan Huang, Zhenghai Xue, Xinrun Wang, Bo An, Shuicheng Yan |  |
| 2151 |  |  [EVA: Geometric Inverse Design for Fast Protein Motif-Scaffolding with Coupled Flow](https://openreview.net/forum?id=KHkBpvmYVI) |  | 0 |  | Yufei Huang, Yunshu Liu, Lirong Wu, Haitao Lin, Cheng Tan, Odin Zhang, Zhangyang Gao, Siyuan Li, Zicheng Liu, Yunfan Liu, Tailin Wu, Stan Z. Li |  |
| 2152 |  |  [Multi-Perspective Data Augmentation for Few-shot Object Detection](https://openreview.net/forum?id=qG0WCAhZE0) |  | 0 |  | AnhKhoa Nguyen Vu, QuocTruong Truong, VinhTiep Nguyen, Thanh Duc Ngo, ThanhToan Do, Tam V. Nguyen |  |
| 2153 |  |  [Intrinsic User-Centric Interpretability through Global Mixture of Experts](https://openreview.net/forum?id=wDcunIOAOk) |  | 0 |  | Vinitra Swamy, Syrielle Montariol, Julian Blackwell, Jibril Frej, Martin Jaggi, Tanja Käser |  |
| 2154 |  |  [VCR: A Task for Pixel-Level Complex Reasoning in Vision Language Models via Restoring Occluded Text](https://openreview.net/forum?id=s0Z4csHOoE) |  | 0 |  | Tianyu Zhang, Suyuchen Wang, Lu Li, Ge Zhang, Perouz Taslakian, Sai Rajeswar, Jie Fu, Bang Liu, Yoshua Bengio |  |
| 2155 |  |  [Mitigate the Gap: Improving Cross-Modal Alignment in CLIP](https://openreview.net/forum?id=aPTGvFqile) |  | 0 |  | Sedigheh Eslami, Gerard de Melo |  |
| 2156 |  |  [A Tight Convergence Analysis of Inexact Stochastic Proximal Point Algorithm for Stochastic Composite Optimization Problems](https://openreview.net/forum?id=n3TkrH7fEr) |  | 0 |  | Shulan Zhu, Chenglong Bao, Defeng Sun, Yancheng Yuan |  |
| 2157 |  |  [SparsyFed: Sparse Adaptive Federated Learning](https://openreview.net/forum?id=OBUQNASaWw) |  | 0 |  | Adriano Guastella, Lorenzo Sani, Alex Iacob, Alessio Mora, Paolo Bellavista, Nicholas Donald Lane |  |
| 2158 |  |  [MambaPEFT: Exploring Parameter-Efficient Fine-Tuning for Mamba](https://openreview.net/forum?id=UAKnJMIBwf) |  | 0 |  | Masakazu Yoshimura, Teruaki Hayashi, Yota Maeda |  |
| 2159 |  |  [DiffGAD: A Diffusion-based Unsupervised Graph Anomaly Detector](https://openreview.net/forum?id=AhcYq4CnfF) |  | 0 |  | Jinghan Li, Yuan Gao, Jinda Lu, Junfeng Fang, Congcong Wen, Hui Lin, Xiang Wang |  |
| 2160 |  |  [On the Relation between Trainability and Dequantization of Variational Quantum Learning Models](https://openreview.net/forum?id=TdqaZbQvdi) |  | 0 |  | Elies GilFuster, Casper Gyurik, Adrián PérezSalinas, Vedran Dunjko |  |
| 2161 |  |  [Regularizing Energy among Training Samples for Out-of-Distribution Generalization](https://openreview.net/forum?id=Lbx9zdURxe) |  | 0 |  | Yiting Chen, Qitian Wu, Junchi Yan |  |
| 2162 |  |  [Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks](https://openreview.net/forum?id=s7lzZpAW7T) |  | 0 |  | Chienyu Huang, WeiChih Chen, ShuWen Yang, Andy T. Liu, ChenAn Li, YuXiang Lin, WeiCheng Tseng, Anuj Diwan, YiJen Shih, Jiatong Shi, William Chen, ChihKai Yang, Xuanjun Chen, ChiYuan Hsiao, Puyuan Peng, ShihHeng Wang, ChunYi Kuan, KeHan Lu, KaiWei Chang, Fabian Alejandro Ritter Gutierrez, et al. |  |
| 2163 |  |  [MMFakeBench: A Mixed-Source Multimodal Misinformation Detection Benchmark for LVLMs](https://openreview.net/forum?id=D6zn6ozJs7) |  | 0 |  | Xuannan Liu, Zekun Li, PeiPei Li, Huaibo Huang, Shuhan Xia, Xing Cui, Linzhi Huang, Weihong Deng, Zhaofeng He |  |
| 2164 |  |  [DoF: A Diffusion Factorization Framework for Offline Multi-Agent Reinforcement Learning](https://openreview.net/forum?id=OTFKVkxSlL) |  | 0 |  | Chao Li, Ziwei Deng, Chenxing Lin, Wenqi Chen, Yongquan Fu, Weiquan Liu, Chenglu Wen, Cheng Wang, Siqi Shen |  |
| 2165 |  |  [Beyond Autoregression: Fast LLMs via Self-Distillation Through Time](https://openreview.net/forum?id=uZ5K4HeNwd) |  | 0 |  | Justin Deschenaux, Caglar Gulcehre |  |
| 2166 |  |  [AI Sandbagging: Language Models can Strategically Underperform on Evaluations](https://openreview.net/forum?id=7Qa2SpjxIS) |  | 0 |  | Teun van der Weij, Felix Hofstätter, Oliver Jaffe, Samuel F. Brown, Francis Rhys Ward |  |
| 2167 |  |  [Dataset Ownership Verification in Contrastive Pre-trained Models](https://openreview.net/forum?id=zeAOzn80VQ) |  | 0 |  | Yuechen Xie, Jie Song, Mengqi Xue, Haofei Zhang, Xingen Wang, Bingde Hu, Genlang Chen, Mingli Song |  |
| 2168 |  |  [Transformer Encoder Satisfiability: Complexity and Impact on Formal Reasoning](https://openreview.net/forum?id=VVO3ApdMUE) |  | 0 |  | Marco Sälzer, Eric Alsmann, Martin Lange |  |
| 2169 |  |  [Diffusion Transformer Captures Spatial-Temporal Dependencies: A Theory for Gaussian Process Data](https://openreview.net/forum?id=MbM1BqGpZu) |  | 0 |  | Hengyu Fu, Zehao Dou, Jiawei Guo, Mengdi Wang, Minshuo Chen |  |
| 2170 |  |  [Fast training and sampling of Restricted Boltzmann Machines](https://openreview.net/forum?id=3fGtV4Zfgq) |  | 0 |  | Nicolas Béreux, Aurélien Decelle, Cyril Furtlehner, Lorenzo Rosset, Beatriz Seoane |  |
| 2171 |  |  [Training-free LLM-generated Text Detection by Mining Token Probability Sequences](https://openreview.net/forum?id=vo4AHjowKi) |  | 0 |  | Yihuai Xu, Yongwei Wang, Yifei Bi, Huangsen Cao, Zhouhan Lin, Yu Zhao, Fei Wu |  |
| 2172 |  |  [ELBOing Stein: Variational Bayes with Stein Mixture Inference](https://openreview.net/forum?id=2rBLbNJwBm) |  | 0 |  | Ola Rønning, Eric T. Nalisnick, Christophe Ley, Padhraic Smyth, Thomas Hamelryck |  |
| 2173 |  |  [Learning Hierarchical Polynomials of Multiple Nonlinear Features](https://openreview.net/forum?id=UZ893n8FXr) |  | 0 |  | Hengyu Fu, Zihao Wang, Eshaan Nichani, Jason D. Lee |  |
| 2174 |  |  [Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model](https://openreview.net/forum?id=DugT77rRhW) |  | 0 |  | Yaxuan Huang, Xili Dai, Jianan Wang, Xianbiao Qi, Yixing Yuan, Xiangyu Yue |  |
| 2175 |  |  [Standardizing Structural Causal Models](https://openreview.net/forum?id=aXuWowhIYt) |  | 0 |  | Weronika Ormaniec, Scott Sussex, Lars Lorch, Bernhard Schölkopf, Andreas Krause |  |
| 2176 |  |  [OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation](https://openreview.net/forum?id=j7kdXSrISM) |  | 0 |  | Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, Ying Tai |  |
| 2177 |  |  [Model Risk-sensitive Offline Reinforcement Learning](https://openreview.net/forum?id=h6k4809xVV) |  | 0 |  | Gwangpyo Yoo, Honguk Woo |  |
| 2178 |  |  [VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents](https://openreview.net/forum?id=zG459X3Xge) |  | 0 |  | Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, Maosong Sun |  |
| 2179 |  |  [Learning system dynamics without forgetting](https://openreview.net/forum?id=rjuZyMfLSd) |  | 0 |  | Xikun Zhang, Dongjin Song, Yushan Jiang, Yixin Chen, Dacheng Tao |  |
| 2180 |  |  [Bootstrapping Language Models with DPO Implicit Rewards](https://openreview.net/forum?id=dliIIodM6b) |  | 0 |  | Changyu Chen, Zichen Liu, Chao Du, Tianyu Pang, Qian Liu, Arunesh Sinha, Pradeep Varakantham, Min Lin |  |
| 2181 |  |  [Reconsidering Faithfulness in Regular, Self-Explainable and Domain Invariant GNNs](https://openreview.net/forum?id=kiOxNsrpQy) |  | 0 |  | Steve Azzolin, Antonio Longa, Stefano Teso, Andrea Passerini |  |
| 2182 |  |  [SC-OmniGS: Self-Calibrating Omnidirectional Gaussian Splatting](https://openreview.net/forum?id=7idCpuEAiR) |  | 0 |  | Huajian Huang, Yingshu Chen, Longwei Li, Hui Cheng, Tristan Braud, Yajie Zhao, SaiKit Yeung |  |
| 2183 |  |  [SWIFT: On-the-Fly Self-Speculative Decoding for LLM Inference Acceleration](https://openreview.net/forum?id=EKJhH5D5wA) |  | 0 |  | Heming Xia, Yongqi Li, Jun Zhang, Cunxiao Du, Wenjie Li |  |
| 2184 |  |  [Fast Direct: Query-Efficient Online Black-box Guidance for Diffusion-model Target Generation](https://openreview.net/forum?id=OmpTdjl7RV) |  | 0 |  | Kim Yong Tan, Yueming Lyu, Ivor W. Tsang, YewSoon Ong |  |
| 2185 |  |  [Adaptive Transformer Programs: Bridging the Gap Between Performance and Interpretability in Transformers](https://openreview.net/forum?id=W8K8slZ73R) |  | 0 |  | QuocVinh LaiDang, Taemin Kang, Seungah Son |  |
| 2186 |  |  [SysBench: Can LLMs Follow System Message?](https://openreview.net/forum?id=KZWaxtzIRx) |  | 0 |  | Yanzhao Qin, Tao Zhang, Tao Zhang, Yanjun Shen, Wenjing Luo, Haoze Sun, Yan Zhang, Yujing Qiao, Weipeng Chen, Zenan Zhou, Wentao Zhang, Bin Cui |  |
| 2187 |  |  [DECO: Unleashing the Potential of ConvNets for Query-based Detection and Segmentation](https://openreview.net/forum?id=TWRhLAN5rz) |  | 0 |  | Xinghao Chen, Siwei Li, Yijing Yang, Yunhe Wang |  |
| 2188 |  |  [Erasing Concept Combination from Text-to-Image Diffusion Model](https://openreview.net/forum?id=OBjF5I4PWg) |  | 0 |  | Hongyi Nie, Quanming Yao, Yang Liu, Zhen Wang, Yatao Bian |  |
| 2189 |  |  [Bridging and Modeling Correlations in Pairwise Data for Direct Preference Optimization](https://openreview.net/forum?id=hRwxZmcvW9) |  | 0 |  | Yuxin Jiang, Bo Huang, Yufei Wang, Xingshan Zeng, Liangyou Li, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Wei Wang |  |
| 2190 |  |  [Deriving Causal Order from Single-Variable Interventions: Guarantees & Algorithm](https://openreview.net/forum?id=u63OVngeSp) |  | 0 |  | Mathieu Chevalley, Patrick Schwab, Arash Mehrjou |  |
| 2191 |  |  [Making Transformer Decoders Better Differentiable Indexers](https://openreview.net/forum?id=bePaRx0otZ) |  | 0 |  | Wuchao Li, Kai Zheng, Defu Lian, Qi Liu, Wentian Bao, Yunen Yu, Yang Song, Han Li, Kun Gai |  |
| 2192 |  |  [Sharpness-Aware Black-Box Optimization](https://openreview.net/forum?id=h7EwIfjxgn) |  | 0 |  | Feiyang Ye, Yueming Lyu, Xuehao Wang, Masashi Sugiyama, Yu Zhang, Ivor W. Tsang |  |
| 2193 |  |  [OmniBind: Large-scale Omni Multimodal Representation via Binding Spaces](https://openreview.net/forum?id=l2izo0z7gu) |  | 0 |  | Zehan Wang, Ziang Zhang, Minjie Hong, Hang Zhang, Luping Liu, Rongjie Huang, Xize Cheng, Shengpeng Ji, Tao Jin, Hengshuang Zhao, Zhou Zhao |  |
| 2194 |  |  [From Tokens to Words: On the Inner Lexicon of LLMs](https://openreview.net/forum?id=328vch6tRs) |  | 0 |  | Guy Kaplan, Matanel Oren, Yuval Reif, Roy Schwartz |  |
| 2195 |  |  [GLoRa: A Benchmark to Evaluate the Ability to Learn Long-Range Dependencies in Graphs](https://openreview.net/forum?id=2jf5x5XoYk) |  | 0 |  | Dongzhuoran Zhou, Evgeny Kharlamov, Egor V. Kostylev |  |
| 2196 |  |  [B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners](https://openreview.net/forum?id=P6dwZJpJ4m) |  | 0 |  | Weihao Zeng, Yuzhen Huang, Lulu Zhao, Yijun Wang, Zifei Shan, Junxian He |  |
| 2197 |  |  [AdaRankGrad: Adaptive Gradient Rank and Moments for Memory-Efficient LLMs Training and Fine-Tuning](https://openreview.net/forum?id=LvNROciCne) |  | 0 |  | Yehonathan Refael, Jonathan Svirsky, Boris Shustin, Wasim Huleihel, Ofir Lindenbaum |  |
| 2198 |  |  [ThinkBot: Embodied Instruction Following with Thought Chain Reasoning](https://openreview.net/forum?id=tFDTHA3odg) |  | 0 |  | Guanxing Lu, Ziwei Wang, Changliu Liu, Jiwen Lu, Yansong Tang |  |
| 2199 |  |  [Understanding Constraint Inference in Safety-Critical Inverse Reinforcement Learning](https://openreview.net/forum?id=B2RXwASSpy) |  | 0 |  | Bo Yue, Shufan Wang, Ashish Gaurav, Jian Li, Pascal Poupart, Guiliang Liu |  |
| 2200 |  |  [Few for Many: Tchebycheff Set Scalarization for Many-Objective Optimization](https://openreview.net/forum?id=O4N9kWwV6R) |  | 0 |  | Xi Lin, Yilu Liu, Xiaoyuan Zhang, Fei Liu, Zhenkun Wang, Qingfu Zhang |  |
| 2201 |  |  [Forget the Data and Fine-Tuning! Just Fold the Network to Compress](https://openreview.net/forum?id=W2Wkp9MQsF) |  | 0 |  | Dong Wang, Haris Sikic, Lothar Thiele, Olga Saukh |  |
| 2202 |  |  [HeadMap: Locating and Enhancing Knowledge Circuits in LLMs](https://openreview.net/forum?id=jUsrbOuQ5e) |  | 0 |  | Xuehao Wang, Liyuan Wang, Binghuai Lin, Yu Zhang |  |
| 2203 |  |  [Optimization by Parallel Quasi-Quantum Annealing with Gradient-Based Sampling](https://openreview.net/forum?id=9EfBeXaXf0) |  | 0 |  | Yuma Ichikawa, Yamato Arai |  |
| 2204 |  |  [Guaranteed Generation from Large Language Models](https://openreview.net/forum?id=8roRgrjbjv) |  | 0 |  | Minbeom Kim, Thibaut Thonet, Jos Rozen, Hwaran Lee, Kyomin Jung, Marc Dymetman |  |
| 2205 |  |  [On Rollouts in Model-Based Reinforcement Learning](https://openreview.net/forum?id=Uh5GRmLlvt) |  | 0 |  | Bernd Frauenknecht, Devdutt Subhasish, Friedrich Solowjow, Sebastian Trimpe |  |
| 2206 |  |  [MTSAM: Multi-Task Fine-Tuning for Segment Anything Model](https://openreview.net/forum?id=6N4QMbeVaO) |  | 0 |  | Xuehao Wang, Zhan Zhuang, Feiyang Ye, Yu Zhang |  |
| 2207 |  |  [On Generalization Across Environments In Multi-Objective Reinforcement Learning](https://openreview.net/forum?id=tuEP424UQ5) |  | 0 |  | Jayden Teoh, Pradeep Varakantham, Peter Vamplew |  |
| 2208 |  |  [Ultra-Sparse Memory Network](https://openreview.net/forum?id=zjeHLSiNv1) |  | 0 |  | Zihao Huang, Qiyang Min, Hongzhi Huang, Yutao Zeng, Defa Zhu, Ran Guo, Xun Zhou |  |
| 2209 |  |  [VICtoR: Learning Hierarchical Vision-Instruction Correlation Rewards for Long-horizon Manipulation](https://openreview.net/forum?id=UpQLu9bzAR) |  | 0 |  | KuoHan Hung, PangChi Lo, JiaFong Yeh, HanYuan Hsu, YiTing Chen, Winston H. Hsu |  |
| 2210 |  |  [Arithmetic Without Algorithms: Language Models Solve Math with a Bag of Heuristics](https://openreview.net/forum?id=O9YTt26r2P) |  | 0 |  | Yaniv Nikankin, Anja Reusch, Aaron Mueller, Yonatan Belinkov |  |
| 2211 |  |  [Think Thrice Before You Act: Progressive Thought Refinement in Large Language Models](https://openreview.net/forum?id=pUbbLHjCPM) |  | 0 |  | Chengyu Du, Jinyi Han, Yizhou Ying, Aili Chen, Qianyu He, Haokun Zhao, Haoran Guo, Sirui Xia, Jiaqing Liang, Zulong Chen, Liangyue Li, Yanghua Xiao |  |
| 2212 |  |  [Noise Separation guided Candidate Label Reconstruction for Noisy Partial Label Learning](https://openreview.net/forum?id=TOahfjA3sP) |  | 0 |  | Xiaorui Peng, Yuheng Jia, Fuchao Yang, Ran Wang, MinLing Zhang |  |
| 2213 |  |  [Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation](https://openreview.net/forum?id=rkzabmWl5k) |  | 0 |  | Jiahao Cui, Hui Li, Yao Yao, Hao Zhu, Hanlin Shang, Kaihui Cheng, Hang Zhou, Siyu Zhu, Jingdong Wang |  |
| 2214 |  |  [Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank Structures](https://openreview.net/forum?id=9BiVepgmWW) |  | 0 |  | Yiming Chen, Yuan Zhang, Liyuan Cao, Kun Yuan, Zaiwen Wen |  |
| 2215 |  |  [Efficient Learning with Sine-Activated Low-Rank Matrices](https://openreview.net/forum?id=cWGCkd7mCp) |  | 0 |  | Yiping Ji, Hemanth Saratchandran, Cameron Gordon, Zeyu Zhang, Simon Lucey |  |
| 2216 |  |  [Earlier Tokens Contribute More: Learning Direct Preference Optimization From Temporal Decay Perspective](https://openreview.net/forum?id=OspqtLVUN5) |  | 0 |  | Ruichen Shao, Bei Li, Gangao Liu, Yang Chen, ZhouXiang, Jingang Wang, Xunliang Cai, Peng Li |  |
| 2217 |  |  [Lasso Bandit with Compatibility Condition on Optimal Arm](https://openreview.net/forum?id=f3jySJpEFT) |  | 0 |  | Harin Lee, Taehyun Hwang, Minhwan Oh |  |
| 2218 |  |  [Equivariant Neural Functional Networks for Transformers](https://openreview.net/forum?id=uBai0ukstY) |  | 0 |  | Hoang V. Tran, Thieu Vo, An Nguyen The, Tho Tran Huu, MinhKhoi NguyenNhat, Thanh Tran, DuyTung Pham, Tan Minh Nguyen |  |
| 2219 |  |  [Improving Deep Regression with Tightness](https://openreview.net/forum?id=dkoiAGjZV9) |  | 0 |  | Shihao Zhang, Yuguang Yan, Angela Yao |  |
| 2220 |  |  [Looking Backward: Streaming Video-to-Video Translation with Feature Banks](https://openreview.net/forum?id=AMkf7h7HER) |  | 0 |  | Feng Liang, Akio Kodaira, Chenfeng Xu, Masayoshi Tomizuka, Kurt Keutzer, Diana Marculescu |  |
| 2221 |  |  [Basis Sharing: Cross-Layer Parameter Sharing for Large Language Model Compression](https://openreview.net/forum?id=gp32jvUquq) |  | 0 |  | Jingcun Wang, YuGuang Chen, IngChao Lin, Bing Li, Grace Li Zhang |  |
| 2222 |  |  [DyCAST: Learning Dynamic Causal Structure from Time Series](https://openreview.net/forum?id=WjDjem8mWE) |  | 0 |  | Yue Cheng, Bochen Lyu, Weiwei Xing, Zhanxing Zhu |  |
| 2223 |  |  [Towards Calibrated Deep Clustering Network](https://openreview.net/forum?id=JvH4jDDcG3) |  | 0 |  | Yuheng Jia, Jianhong Cheng, Hui Liu, Junhui Hou |  |
| 2224 |  |  [DisPose: Disentangling Pose Guidance for Controllable Human Image Animation](https://openreview.net/forum?id=AumOa10MKG) |  | 0 |  | Hongxiang Li, Yaowei Li, Yuhang Yang, Junjie Cao, Zhihong Zhu, Xuxin Cheng, Long Chen |  |
| 2225 |  |  [Online-to-Offline RL for Agent Alignment](https://openreview.net/forum?id=ruv3HdK6he) |  | 0 |  | Xu Liu, Haobo Fu, Stefano V. Albrecht, Qiang Fu, Shuai Li |  |
| 2226 |  |  [SynQ: Accurate Zero-shot Quantization by Synthesis-aware Fine-tuning](https://openreview.net/forum?id=2rnOgyFQgb) |  | 0 |  | Minjun Kim, Jongjin Kim, U Kang |  |
| 2227 |  |  [CollabEdit: Towards Non-destructive Collaborative Knowledge Editing](https://openreview.net/forum?id=2PzozgigiA) |  | 0 |  | Jiamu Zheng, Jinghuai Zhang, Tianyu Du, Xuhong Zhang, Jianwei Yin, Tao Lin |  |
| 2228 |  |  [RuAG: Learned-rule-augmented Generation for Large Language Models](https://openreview.net/forum?id=BpIbnXWfhL) |  | 0 |  | Yudi Zhang, Pei Xiao, Lu Wang, Chaoyun Zhang, Meng Fang, Yali Du, Yevgeniy Puzyrev, Randolph Yao, Si Qin, Qingwei Lin, Mykola Pechenizkiy, Dongmei Zhang, Saravan Rajmohan, Qi Zhang |  |
| 2229 |  |  [Reveal Object in Lensless Photography via Region Gaze and Amplification](https://openreview.net/forum?id=EV7FMBZxnx) |  | 0 |  | Xiangjun Yin, Huihui Yue |  |
| 2230 |  |  [Learning Video-Conditioned Policy on Unlabelled Data with Joint Embedding Predictive Transformer](https://openreview.net/forum?id=TqM0hifngW) |  | 0 |  | Hao Luo, Zongqing Lu |  |
| 2231 |  |  [Enhancing Graph Of Thought: Enhancing Prompts with LLM Rationales and Dynamic Temperature Control](https://openreview.net/forum?id=l32IrJtpOP) |  | 0 |  | Sunguk Shin, Youngjoon Kim |  |
| 2232 |  |  [SplineGS: Learning Smooth Trajectories in Gaussian Splatting for Dynamic Scene Reconstruction](https://openreview.net/forum?id=tMG6btjBfd) |  | 0 |  | Jihwan Yoon, Sangbeom Han, Jaeseok Oh, Minsik Lee |  |
| 2233 |  |  [A Closer Look at Machine Unlearning for Large Language Models](https://openreview.net/forum?id=Q1MHvGmhyT) |  | 0 |  | Xiaojian Yuan, Tianyu Pang, Chao Du, Kejiang Chen, Weiming Zhang, Min Lin |  |
| 2234 |  |  [Weighted-Reward Preference Optimization for Implicit Model Fusion](https://openreview.net/forum?id=fq24pEb8SL) |  | 0 |  | Ziyi Yang, Fanqi Wan, Longguang Zhong, Tianyuan Shi, Xiaojun Quan |  |
| 2235 |  |  [Unified Parameter-Efficient Unlearning for LLMs](https://openreview.net/forum?id=zONMuIVCAT) |  | 0 |  | Chenlu Ding, Jiancan Wu, Yancheng Yuan, Jinda Lu, Kai Zhang, Alex Su, Xiang Wang, Xiangnan He |  |
| 2236 |  |  [HyPoGen: Optimization-Biased Hypernetworks for Generalizable Policy Generation](https://openreview.net/forum?id=CJWMXqAnAy) |  | 0 |  | Hanxiang Ren, Li Sun, Xulong Wang, Pei Zhou, Zewen Wu, Siyan Dong, Difan Zou, Youyi Zheng, Yanchao Yang |  |
| 2237 |  |  [Conformalized Survival Analysis for General Right-Censored Data](https://openreview.net/forum?id=JQtuCumAFD) |  | 0 |  | Hen Davidov, Shai Feldman, Gil Shamai, Ron Kimmel, Yaniv Romano |  |
| 2238 |  |  [A Stochastic Approach to the Subset Selection Problem via Mirror Descent](https://openreview.net/forum?id=5K0fmGnFqP) |  | 0 |  | Dan Greenstein, Elazar Gershuni, Ilan BenBassat, Yaroslav Fyodorov, Moshe Ran, Fiana Raiber, Alex Shtoff, Oren Somekh, Nadav Hallak |  |
| 2239 |  |  [Everything is Editable: Extend Knowledge Editing to Unstructured Data in Large Language Models](https://openreview.net/forum?id=X5rO5VyTgB) |  | 0 |  | Jingcheng Deng, Zihao Wei, Liang Pang, Hanxing Ding, Huawei Shen, Xueqi Cheng |  |
| 2240 |  |  [Locality-aware Gaussian Compression for Fast and High-quality Rendering](https://openreview.net/forum?id=dHYwfV2KeP) |  | 0 |  | Seungjoo Shin, Jaesik Park, Sunghyun Cho |  |
| 2241 |  |  [Diffusion Actor-Critic: Formulating Constrained Policy Iteration as Diffusion Noise Regression for Offline Reinforcement Learning](https://openreview.net/forum?id=ldVkAO09Km) |  | 0 |  | Linjiajie Fang, Ruoxue Liu, Jing Zhang, Wenjia Wang, Bingyi Jing |  |
| 2242 |  |  [IRIS: LLM-Assisted Static Analysis for Detecting Security Vulnerabilities](https://openreview.net/forum?id=9LdJDU7E91) |  | 0 |  | Ziyang Li, Saikat Dutta, Mayur Naik |  |
| 2243 |  |  [Improving Long-Text Alignment for Text-to-Image Diffusion Models](https://openreview.net/forum?id=2ZK8zyIt7o) |  | 0 |  | Luping Liu, Chao Du, Tianyu Pang, Zehan Wang, Chongxuan Li, Dong Xu |  |
| 2244 |  |  [A Solvable Attention for Neural Scaling Laws](https://openreview.net/forum?id=wYxOMEzpkl) |  | 0 |  | Bochen Lyu, Di Wang, Zhanxing Zhu |  |
| 2245 |  |  [GPromptShield: Elevating Resilience in Graph Prompt Tuning Against Adversarial Attacks](https://openreview.net/forum?id=yCN4yI6zhH) |  | 0 |  | Shuhan Song, Ping Li, Ming Dun, Maolei Huang, Huawei Cao, Xiaochun Ye |  |
| 2246 |  |  [Unify ML4TSP: Drawing Methodological Principles for TSP and Beyond from Streamlined Design Space of Learning and Search](https://openreview.net/forum?id=grU1VKEOLi) |  | 0 |  | Yang Li, Jiale Ma, Wenzheng Pan, Runzhong Wang, Haoyu Geng, Nianzu Yang, Junchi Yan |  |
| 2247 |  |  [Bridging Compressed Image Latents and Multimodal Large Language Models](https://openreview.net/forum?id=GSUNPIw7Ad) |  | 0 |  | ChiaHao Kao, Cheng Chien, YuJen Tseng, YiHsin Chen, Alessandro Gnutti, ShaoYuan Lo, WenHsiao Peng, Riccardo Leonardi |  |
| 2248 |  |  [Learning Partial Graph Matching via Optimal Partial Transport](https://openreview.net/forum?id=uDXFOurrHM) |  | 0 |  | Gathika Ratnayaka, James Nichols, Qing Wang |  |
| 2249 |  |  [Offline RL with Smooth OOD Generalization in Convex Hull and its Neighborhood](https://openreview.net/forum?id=eY5JNJE56i) |  | 0 |  | Qingmao Yao, Zhichao Lei, Tianyuan Chen, Ziyue Yuan, Xuefan Chen, Jianxiang Liu, Faguo Wu, Xiao Zhang |  |
| 2250 |  |  [Adversarial Search Engine Optimization for Large Language Models](https://openreview.net/forum?id=hkdqxN3c7t) |  | 0 |  | Fredrik Nestaas, Edoardo Debenedetti, Florian Tramèr |  |
| 2251 |  |  [Near, far: Patch-ordering enhances vision foundation models' scene understanding](https://openreview.net/forum?id=Qro97zWC29) |  | 0 |  | Valentinos Pariza, Mohammadreza Salehi, Gertjan J. Burghouts, Francesco Locatello, Yuki M. Asano |  |
| 2252 |  |  [NeRAF: 3D Scene Infused Neural Radiance and Acoustic Fields](https://openreview.net/forum?id=njvSBvtiwp) |  | 0 |  | Amandine Brunetto, Sascha Hornauer, Fabien Moutarde |  |
| 2253 |  |  [Flow Distillation Sampling: Regularizing 3D Gaussians with Pre-trained Matching Priors](https://openreview.net/forum?id=BzsjHiBfLk) |  | 0 |  | LinZhuo Chen, Kangjie Liu, Youtian Lin, Zhihao Li, Siyu Zhu, Xun Cao, Yao Yao |  |
| 2254 |  |  [Test-Time Adaptation for Combating Missing Modalities in Egocentric Videos](https://openreview.net/forum?id=1L52bHEL5d) |  | 0 |  | Merey Ramazanova, Alejandro Pardo, Bernard Ghanem, Motasem Alfarra |  |
| 2255 |  |  [Model-based Offline Reinforcement Learning with Lower Expectile Q-Learning](https://openreview.net/forum?id=OATPSB5JK1) |  | 0 |  | Kwanyoung Park, Youngwoon Lee |  |
| 2256 |  |  [A Large-scale Dataset and Benchmark for Commuting Origin-Destination Flow Generation](https://openreview.net/forum?id=WeJEidTzff) |  | 0 |  | Can Rong, Jingtao Ding, Yan Liu, Yong Li |  |
| 2257 |  |  [Cross-Domain Off-Policy Evaluation and Learning for Contextual Bandits](https://openreview.net/forum?id=Z8dr422vtr) |  | 0 |  | Yuta Natsubori, Masataka Ushiku, Yuta Saito |  |
| 2258 |  |  [MAST: model-agnostic sparsified training](https://openreview.net/forum?id=sPuLtU32av) |  | 0 |  | Yury Demidovich, Grigory Malinovsky, Egor Shulgin, Peter Richtárik |  |
| 2259 |  |  [LOIRE: LifelOng learning on Incremental data via pre-trained language model gRowth Efficiently](https://openreview.net/forum?id=F5PlYMC5ik) |  | 0 |  | Xue Han, Yitong Wang, Junlan Feng, Wenchun Gao, Qian Hu, Chao Deng |  |
| 2260 |  |  [Continuous Diffusion for Mixed-Type Tabular Data](https://openreview.net/forum?id=QPtoBPn4lZ) |  | 0 |  | Markus Mueller, Kathrin Gruber, Dennis Fok |  |
| 2261 |  |  [Single Teacher, Multiple Perspectives: Teacher Knowledge Augmentation for Enhanced Knowledge Distillation](https://openreview.net/forum?id=DmEHmZ89iB) |  | 0 |  | Md. Imtiaz Hossain, Sharmen Akhter, Choong Seon Hong, EuiNam Huh |  |
| 2262 |  |  [SpaceGNN: Multi-Space Graph Neural Network for Node Anomaly Detection with Extremely Limited Labels](https://openreview.net/forum?id=Syt4fWwVm1) |  | 0 |  | Xiangyu Dong, Xingyi Zhang, Lei Chen, Mingxuan Yuan, Sibo Wang |  |
| 2263 |  |  [Toward Exploratory Inverse Constraint Inference with Generative Diffusion Verifiers](https://openreview.net/forum?id=0UvlnHgaii) |  | 0 |  | Runyi Zhao, Sheng Xu, Bo Yue, Guiliang Liu |  |
| 2264 |  |  [Exploring the Effectiveness of Object-Centric Representations in Visual Question Answering: Comparative Insights with Foundation Models](https://openreview.net/forum?id=DD11okKg13) |  | 0 |  | Amir Mohammad KarimiMamaghan, Samuele Papa, Karl Henrik Johansson, Stefan Bauer, Andrea Dittadi |  |
| 2265 |  |  [ADMM for Nonconvex Optimization under Minimal Continuity Assumption](https://openreview.net/forum?id=GKAQ92ua3A) |  | 0 |  | Ganzhao Yuan |  |
| 2266 |  |  [DOCS: Quantifying Weight Similarity for Deeper Insights into Large Language Models](https://openreview.net/forum?id=XBHoaHlGQM) |  | 0 |  | Zeping Min, Xinshang Wang |  |
| 2267 |  |  [Can LLMs Understand Time Series Anomalies?](https://openreview.net/forum?id=LGafQ1g2D2) |  | 0 |  | Zihao Zhou, Rose Yu |  |
| 2268 |  |  [Node-Time Conditional Prompt Learning in Dynamic Graphs](https://openreview.net/forum?id=kVlfYvIqaK) |  | 0 |  | Xingtong Yu, Zhenghao Liu, Xinming Zhang, Yuan Fang |  |
| 2269 |  |  [Physics-Informed Deep Inverse Operator Networks for Solving PDE Inverse Problems](https://openreview.net/forum?id=0FxnSZJPmh) |  | 0 |  | Sung Woong Cho, Hwijae Son |  |
| 2270 |  |  [X-Fi: A Modality-Invariant Foundation Model for Multimodal Human Sensing](https://openreview.net/forum?id=b42wmsdwmB) |  | 0 |  | Xinyan Chen, Jianfei Yang |  |
| 2271 |  |  [Entropy-based Activation Function Optimization: A Method on Searching Better Activation Functions](https://openreview.net/forum?id=7TZYM6Hm9p) |  | 0 |  | Haoyuan Sun, Zihao Wu, Bo Xia, Pu Chang, Zibin Dong, Yifu Yuan, Yongzhe Chang, Xueqian Wang |  |
| 2272 |  |  [Towards Synergistic Path-based Explanations for Knowledge Graph Completion: Exploration and Evaluation](https://openreview.net/forum?id=WQvkqarwXi) |  | 0 |  | Tengfei Ma, Xiang Song, Wen Tao, Mufei Li, Jiani Zhang, Xiaoqin Pan, Yijun Wang, Bosheng Song, Xiangxiang Zeng |  |
| 2273 |  |  [InstaRevive: One-Step Image Enhancement via Dynamic Score Matching](https://openreview.net/forum?id=G1CN7R5qwE) |  | 0 |  | Yixuan Zhu, Haolin Wang, Ao Li, Wenliang Zhao, Yansong Tang, Jingxuan Niu, Lei Chen, Jie Zhou, Jiwen Lu |  |
| 2274 |  |  [LLM Unlearning via Loss Adjustment with Only Forget Data](https://openreview.net/forum?id=6ESRicalFE) |  | 0 |  | Yaxuan Wang, Jiaheng Wei, Chris Yuhao Liu, Jinlong Pang, Quan Liu, Ankit Shah, Yujia Bao, Yang Liu, Wei Wei |  |
| 2275 |  |  [AI2TALE: An Innovative Information Theory-based Approach for Learning to Localize Phishing Attacks](https://openreview.net/forum?id=3xpTXF5ALZ) |  | 0 |  | Van Nguyen, Tingmin Wu, Xingliang Yuan, Marthie Grobler, Surya Nepal, Carsten Rudolph |  |
| 2276 |  |  [A Distributional Approach to Uncertainty-Aware Preference Alignment Using Offline Demonstrations](https://openreview.net/forum?id=RKOAU5ti1y) |  | 0 |  | Sheng Xu, Bo Yue, Hongyuan Zha, Guiliang Liu |  |
| 2277 |  |  [Bridging Jensen Gap for Max-Min Group Fairness Optimization in Recommendation](https://openreview.net/forum?id=1PDz4Ny1N2) |  | 0 |  | Chen Xu, Yuxin Li, Wenjie Wang, Liang Pang, Jun Xu, TatSeng Chua |  |
| 2278 |  |  [DaWin: Training-free Dynamic Weight Interpolation for Robust Adaptation](https://openreview.net/forum?id=L8e7tBf4pP) |  | 0 |  | Changdae Oh, Yixuan Li, Kyungwoo Song, Sangdoo Yun, Dongyoon Han |  |
| 2279 |  |  [Competitive Fair Scheduling with Predictions](https://openreview.net/forum?id=jBYQAtzp5Z) |  | 0 |  | Tianming Zhao, Chunqiu Xia, Xiaomin Chang, Chunhao Li, Wei Li, Albert Y. Zomaya |  |
| 2280 |  |  [Demystifying Online Clustering of Bandits: Enhanced Exploration Under Stochastic and Smoothed Adversarial Contexts](https://openreview.net/forum?id=421D67DY3i) |  | 0 |  | Zhuohua Li, Maoli Liu, Xiangxiang Dai, John C. S. Lui |  |
| 2281 |  |  [High-Precision Dichotomous Image Segmentation via Probing Diffusion Capacity](https://openreview.net/forum?id=vh1e2WJfZp) |  | 0 |  | Qian Yu, PengTao Jiang, Hao Zhang, Jinwei Chen, Bo Li, Lihe Zhang, Huchuan Lu |  |
| 2282 |  |  [Delta: Dense Efficient Long-Range 3D tracking for any video](https://openreview.net/forum?id=d9iHI1eimo) |  | 0 |  | Tuan Duc Ngo, Peiye Zhuang, Evangelos Kalogerakis, Chuang Gan, Sergey Tulyakov, HsinYing Lee, Chaoyang Wang |  |
| 2283 |  |  [Unsupervised Multiple Kernel Learning for Graphs via Ordinality Preservation](https://openreview.net/forum?id=6nb2J90XJD) |  | 0 |  | Yan Sun, Stanley Kok |  |
| 2284 |  |  [Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge](https://openreview.net/forum?id=3GTtZFiajM) |  | 0 |  | Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, PinYu Chen, Nitesh V. Chawla, Xiangliang Zhang |  |
| 2285 |  |  [OmnixR: Evaluating Omni-modality Language Models on Reasoning across Modalities](https://openreview.net/forum?id=jki6EFsZLw) |  | 0 |  | Lichang Chen, Hexiang Hu, Mingda Zhang, Yiwen Chen, Zifeng Wang, Yandong Li, Pranav Shyam, Tianyi Zhou, Heng Huang, MingHsuan Yang, Boqing Gong |  |
| 2286 |  |  [Planning Anything with Rigor: General-Purpose Zero-Shot Planning with LLM-based Formalized Programming](https://openreview.net/forum?id=0K1OaL6XuK) |  | 0 |  | Yilun Hao, Yang Zhang, Chuchu Fan |  |
| 2287 |  |  [MolSpectra: Pre-training 3D Molecular Representation with Multi-modal Energy Spectra](https://openreview.net/forum?id=xJDxVDG3x2) |  | 0 |  | Liang Wang, Shaozhen Liu, Yu Rong, Deli Zhao, Qiang Liu, Shu Wu, Liang Wang |  |
| 2288 |  |  [Enhancing Clustered Federated Learning: Integration of Strategies and Improved Methodologies](https://openreview.net/forum?id=zPDpdk3V8L) |  | 0 |  | Yongxin Guo, Xiaoying Tang, Tao Lin |  |
| 2289 |  |  [INFER: A Neural-symbolic Model For Extrapolation Reasoning on Temporal Knowledge Graph](https://openreview.net/forum?id=ExHUtB2vnz) |  | 0 |  | Ningyuan Li, Haihong E, Tianyu Yao, Tianyi Hu, Yuhan Li, Haoran Luo, Meina Song, Yifan Zhu |  |
| 2290 |  |  [Supervised and Semi-Supervised Diffusion Maps with Label-Driven Diffusion](https://openreview.net/forum?id=G3B5ReApDw) |  | 0 |  | Harel Mendelman, Ronen Talmon |  |
| 2291 |  |  [Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models with Flow Matching](https://openreview.net/forum?id=zKlFXV87Pp) |  | 0 |  | Enshu Liu, Xuefei Ning, Yu Wang, Zinan Lin |  |
| 2292 |  |  [Policy Decorator: Model-Agnostic Online Refinement for Large Policy Model](https://openreview.net/forum?id=e5jGTEiJMT) |  | 0 |  | Xiu Yuan, Tongzhou Mu, Stone Tao, Yunhao Fang, Mengke Zhang, Hao Su |  |
| 2293 |  |  [Precise Localization of Memories: A Fine-grained Neuron-level Knowledge Editing Technique for LLMs](https://openreview.net/forum?id=5xP1HDvpXI) |  | 0 |  | Haowen Pan, Xiaozhi Wang, Yixin Cao, Zenglin Shi, Xun Yang, Juanzi Li, Meng Wang |  |
| 2294 |  |  [Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solver](https://openreview.net/forum?id=6aHUmotXaw) |  | 0 |  | Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, Mao Yang |  |
| 2295 |  |  [On the Importance of Language-driven Representation Learning for Heterogeneous Federated Learning](https://openreview.net/forum?id=7pDI74iOyu) |  | 0 |  | Yunlu Yan, ChunMei Feng, Wangmeng Zuo, Salman H. Khan, Yong Liu, Lei Zhu |  |
| 2296 |  |  [Is Factuality Enhancement a Free Lunch For LLMs? Better Factuality Can Lead to Worse Context-Faithfulness](https://openreview.net/forum?id=asGQQc7gNo) |  | 0 |  | Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Junfeng Fang, Hongcheng Gao, Shiyu Ni, Xueqi Cheng |  |
| 2297 |  |  [TopoGaussian: Inferring Internal Topology Structures from Visual Clues](https://openreview.net/forum?id=B5PbOsJqt3) |  | 0 |  | Xiaoyu Xiong, Changyu Hu, Chunru Lin, Pingchuan Ma, Chuang Gan, Tao Du |  |
| 2298 |  |  [Tree of Attributes Prompt Learning for Vision-Language Models](https://openreview.net/forum?id=wFs2E5wCw6) |  | 0 |  | Tong Ding, Wanhua Li, Zhongqi Miao, Hanspeter Pfister |  |
| 2299 |  |  [DiffusionGuard: A Robust Defense Against Malicious Diffusion-based Image Editing](https://openreview.net/forum?id=9OfKxKoYNw) |  | 0 |  | June Suk Choi, Kyungmin Lee, Jongheon Jeong, Saining Xie, Jinwoo Shin, Kimin Lee |  |
| 2300 |  |  [On the Performance Analysis of Momentum Method: A Frequency Domain Perspective](https://openreview.net/forum?id=tznvtmSEiN) |  | 0 |  | Xianliang Li, Jun Luo, Zhiwei Zheng, Hanxiao Wang, Li Luo, Lingkun Wen, Linlong Wu, Sheng Xu |  |
| 2301 |  |  [3DMolFormer: A Dual-channel Framework for Structure-based Drug Discovery](https://openreview.net/forum?id=RgE1qiO2ek) |  | 0 |  | Xiuyuan Hu, Guoqing Liu, Can Chen, Yang Zhao, Hao Zhang, Xue Liu |  |
| 2302 |  |  [Evidential Learning-based Certainty Estimation for Robust Dense Feature Matching](https://openreview.net/forum?id=4NWtrQciRH) |  | 0 |  | Lile Cai, ChuanSheng Foo, Xun Xu, Zaiwang Gu, Jun Cheng, Xulei Yang |  |
| 2303 |  |  [Language models scale reliably with over-training and on downstream tasks](https://openreview.net/forum?id=iZeQBqJamf) |  | 0 |  | Samir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, Rui Xin, Marianna Nezhurina, Igor Vasiljevic, Luca Soldaini, Jenia Jitsev, Alex Dimakis, Gabriel Ilharco, Pang Wei Koh, Shuran Song, Thomas Kollar, et al. |  |
| 2304 |  |  [ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL](https://openreview.net/forum?id=BAglD6NGy0) |  | 0 |  | Yang Qin, Chao Chen, Zhihang Fu, Ze Chen, Dezhong Peng, Peng Hu, Jieping Ye |  |
| 2305 |  |  [Towards a Theoretical Understanding of Synthetic Data in LLM Post-Training: A Reverse-Bottleneck Perspective](https://openreview.net/forum?id=UxkznlcnHf) |  | 0 |  | Zeyu Gan, Yong Liu |  |
| 2306 |  |  [I2AM: Interpreting Image-to-Image Latent Diffusion Models via Bi-Attribution Maps](https://openreview.net/forum?id=bBNUiErs26) |  | 0 |  | Junseo Park, Hyeryung Jang |  |
| 2307 |  |  [3D StreetUnveiler with Semantic-aware 2DGS - a simple baseline](https://openreview.net/forum?id=G6aJyS0ZV0) |  | 0 |  | Jingwei Xu, Yikai Wang, Yiqun Zhao, Yanwei Fu, Shenghua Gao |  |
| 2308 |  |  [Prompt as Knowledge Bank: Boost Vision-language model via Structural Representation for zero-shot medical detection](https://openreview.net/forum?id=l0t2rumAvR) |  | 0 |  | Yuguang Yang, Tongfei Chen, Haoyu Huang, Linlin Yang, Chunyu Xie, Dawei Leng, Xianbin Cao, Baochang Zhang |  |
| 2309 |  |  [Adam-mini: Use Fewer Learning Rates To Gain More](https://openreview.net/forum?id=iBExhaU3Lc) |  | 0 |  | Yushun Zhang, Congliang Chen, Ziniu Li, Tian Ding, Chenwei Wu, Diederik P. Kingma, Yinyu Ye, ZhiQuan Luo, Ruoyu Sun |  |
| 2310 |  |  [An Engorgio Prompt Makes Large Language Model Babble on](https://openreview.net/forum?id=m4eXBo0VNc) |  | 0 |  | Jianshuo Dong, Ziyuan Zhang, Qingjie Zhang, Tianwei Zhang, Hao Wang, Hewu Li, Qi Li, Chao Zhang, Ke Xu, Han Qiu |  |
| 2311 |  |  [Learning to Contextualize Web Pages for Enhanced Decision Making by LLM Agents](https://openreview.net/forum?id=3Gzz7ZQLiz) |  | 0 |  | Dongjun Lee, Juyong Lee, Kyuyoung Kim, Jihoon Tack, Jinwoo Shin, Yee Whye Teh, Kimin Lee |  |
| 2312 |  |  [Energy-Based Diffusion Language Models for Text Generation](https://openreview.net/forum?id=sL2F9YCMXf) |  | 0 |  | Minkai Xu, Tomas Geffner, Karsten Kreis, Weili Nie, Yilun Xu, Jure Leskovec, Stefano Ermon, Arash Vahdat |  |
| 2313 |  |  [From GNNs to Trees: Multi-Granular Interpretability for Graph Neural Networks](https://openreview.net/forum?id=KEUPk0wXXe) |  | 0 |  | Jie Yang, Yuwen Wang, Kaixuan Chen, Tongya Zheng, Yihe Zhou, Zhenbang Xiao, Ji Cao, Mingli Song, Shunyu Liu |  |
| 2314 |  |  [Multi-objective Differentiable Neural Architecture Search](https://openreview.net/forum?id=9mjZ800m7Y) |  | 0 |  | Rhea Sanjay Sukthanker, Arber Zela, Benedikt Staffler, Samuel Dooley, Josif Grabocka, Frank Hutter |  |
| 2315 |  |  [Generative Representational Instruction Tuning](https://openreview.net/forum?id=BC4lIvfSzv) |  | 0 |  | Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, Douwe Kiela |  |
| 2316 |  |  [CogCoM: A Visual Language Model with Chain-of-Manipulations Reasoning](https://openreview.net/forum?id=Fg0eo2AkST) |  | 0 |  | Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, Jie Tang |  |
| 2317 |  |  [TabDiff: a Mixed-type Diffusion Model for Tabular Data Generation](https://openreview.net/forum?id=swvURjrt8z) |  | 0 |  | Juntong Shi, Minkai Xu, Harper Hua, Hengrui Zhang, Stefano Ermon, Jure Leskovec |  |
| 2318 |  |  [Jamba: Hybrid Transformer-Mamba Language Models](https://openreview.net/forum?id=JFPaD7lpBD) |  | 0 |  | Barak Lenz, Opher Lieber, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, Daniel Gissin, Daniel Jannai, Dor Muhlgay, Dor Zimberg, Edden M. Gerber, Elad Dolev, Eran Krakovsky, Erez Safahi, Erez Schwartz, Gal Cohen, et al. |  |
| 2319 |  |  [Rodimus\*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions](https://openreview.net/forum?id=IIVYiJ1ggK) |  | 0 |  | Zhihao He, Hang Yu, Zi Gong, Shizhan Liu, Jianguo Li, Weiyao Lin |  |
| 2320 |  |  [KinFormer: Generalizable Dynamical Symbolic Regression for Catalytic Organic Reaction Kinetics](https://openreview.net/forum?id=nhrXqy5d5q) |  | 0 |  | Jindou Chen, Jidong Tian, Liang Wu, ChenXinWei, Xiaokang Yang, Yaohui Jin, Yanyan Xu |  |
| 2321 |  |  [IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and Illuminations](https://openreview.net/forum?id=uuef1HP6X7) |  | 0 |  | Zhibing Li, Tong Wu, Jing Tan, Mengchen Zhang, Jiaqi Wang, Dahua Lin |  |
| 2322 |  |  [Learning 3D Perception from Others' Predictions](https://openreview.net/forum?id=Ylk98vWQuQ) |  | 0 |  | Jinsu Yoo, Zhenyang Feng, TaiYu Pan, Yihong Sun, Cheng Perng Phoo, Xiangyu Chen, Mark E. Campbell, Kilian Q. Weinberger, Bharath Hariharan, WeiLun Chao |  |
| 2323 |  |  [SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding](https://openreview.net/forum?id=8dzKkeWUUb) |  | 0 |  | Sihang Li, Jin Huang, Jiaxi Zhuang, Yaorui Shi, Xiaochen Cai, Mingjun Xu, Xiang Wang, Linfeng Zhang, Guolin Ke, Hengxing Cai |  |
| 2324 |  |  [VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks](https://openreview.net/forum?id=TE0KOzWYAF) |  | 0 |  | Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, Wenhu Chen |  |
| 2325 |  |  [MMDisCo: Multi-Modal Discriminator-Guided Cooperative Diffusion for Joint Audio and Video Generation](https://openreview.net/forum?id=agbiPPuSeQ) |  | 0 |  | Akio Hayakawa, Masato Ishii, Takashi Shibuya, Yuki Mitsufuji |  |
| 2326 |  |  [Enhanced Diffusion Sampling via Extrapolation with Multiple ODE Solutions](https://openreview.net/forum?id=rCGleSgNBK) |  | 0 |  | Jinyoung Choi, Junoh Kang, Bohyung Han |  |
| 2327 |  |  [MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers](https://openreview.net/forum?id=KGZAs8VcOM) |  | 0 |  | Yiwen Chen, Tong He, Di Huang, Weicai Ye, Sijin Chen, Jiaxiang Tang, Zhongang Cai, Lei Yang, Gang Yu, Guosheng Lin, Chi Zhang |  |
| 2328 |  |  [Anyprefer: An Agentic Framework for Preference Data Synthesis](https://openreview.net/forum?id=WpZyPk79Fu) |  | 0 |  | Yiyang Zhou, Zhaoyang Wang, Tianle Wang, Shangyu Xing, Peng Xia, Bo Li, Kaiyuan Zheng, Zijian Zhang, Zhaorun Chen, Wenhao Zheng, Xuchao Zhang, Chetan Bansal, Weitong Zhang, Ying Wei, Mohit Bansal, Huaxiu Yao |  |
| 2329 |  |  [Personalized Representation from Personalized Generation](https://openreview.net/forum?id=jw7P4MHLWw) |  | 0 |  | Shobhita Sundaram, Julia Chae, Yonglong Tian, Sara Beery, Phillip Isola |  |
| 2330 |  |  [Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View](https://openreview.net/forum?id=u8VOQVzduP) |  | 0 |  | Xuan Liu, Jie Zhang, Haoyang Shang, Song Guo, Chengxu Yang, Quanyan Zhu |  |
| 2331 |  |  [SVG: 3D Stereoscopic Video Generation via Denoising Frame Matrix](https://openreview.net/forum?id=sx2jXZuhIx) |  | 0 |  | Peng Dai, Feitong Tan, Qiangeng Xu, David Futschik, Ruofei Du, Sean Fanello, Xiaojuan Qi, Yinda Zhang |  |
| 2332 |  |  [Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws](https://openreview.net/forum?id=aqok1UX7Z1) |  | 0 |  | Yiding Jiang, Allan Zhou, Zhili Feng, Sadhika Malladi, J. Zico Kolter |  |
| 2333 |  |  [Perturbation-Restrained Sequential Model Editing](https://openreview.net/forum?id=bfI8cp8qmk) |  | 0 |  | JunYu Ma, Hong Wang, HaoXiang Xu, ZhenHua Ling, JiaChen Gu |  |
| 2334 |  |  [Incorporating Visual Correspondence into Diffusion Model for Virtual Try-On](https://openreview.net/forum?id=XXzOzJRyOZ) |  | 0 |  | Siqi Wan, Jingwen Chen, Yingwei Pan, Ting Yao, Tao Mei |  |
| 2335 |  |  [The adaptive complexity of parallelized log-concave sampling](https://openreview.net/forum?id=EeqlkPpaV8) |  | 0 |  | Huanjian Zhou, Baoxiang Wang, Masashi Sugiyama |  |
| 2336 |  |  [MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine](https://openreview.net/forum?id=IwgmgidYPS) |  | 0 |  | Yunfei Xie, Ce Zhou, Lang Gao, Juncheng Wu, Xianhang Li, HongYu Zhou, Sheng Liu, Lei Xing, James Zou, Cihang Xie, Yuyin Zhou |  |
| 2337 |  |  [CBraMod: A Criss-Cross Brain Foundation Model for EEG Decoding](https://openreview.net/forum?id=NPNUHgHF2w) |  | 0 |  | Jiquan Wang, Sha Zhao, Zhiling Luo, Yangxuan Zhou, Haiteng Jiang, Shijian Li, Tao Li, Gang Pan |  |
| 2338 |  |  [PFDiff: Training-Free Acceleration of Diffusion Models Combining Past and Future Scores](https://openreview.net/forum?id=wmmDvZGFK7) |  | 0 |  | Guangyi Wang, Yuren Cai, Lijiang Li, Wei Peng, SongZhi Su |  |
| 2339 |  |  [Consistent Flow Distillation for Text-to-3D Generation](https://openreview.net/forum?id=A51NEXIq1J) |  | 0 |  | Runjie Yan, Yinbo Chen, Xiaolong Wang |  |
| 2340 |  |  [Robust-PIFu: Robust Pixel-aligned Implicit Function for 3D Human Digitalization from a Single Image](https://openreview.net/forum?id=ftdJEiFudy) |  | 0 |  | Kennard Yanting Chan, Fayao Liu, Guosheng Lin, ChuanSheng Foo, Weisi Lin |  |
| 2341 |  |  [Descent with Misaligned Gradients and Applications to Hidden Convexity](https://openreview.net/forum?id=2L4PTJO8VQ) |  | 0 |  | Aditya Bhaskara, Ashok Cutkosky, Ravi Kumar, Manish Purohit |  |
| 2342 |  |  [On Statistical Rates of Conditional Diffusion Transformers: Approximation, Estimation and Minimax Optimality](https://openreview.net/forum?id=c54apoozCS) |  | 0 |  | Jerry YaoChieh Hu, Weimin Wu, YiChen Lee, YuChao Huang, Minshuo Chen, Han Liu |  |
| 2343 |  |  [Universal Image Restoration Pre-training via Degradation Classification](https://openreview.net/forum?id=PacBhLzeGO) |  | 0 |  | Jiakui Hu, Lujia Jin, Zhengjian Yao, Yanye Lu |  |
| 2344 |  |  [On-the-fly Preference Alignment via Principle-Guided Decoding](https://openreview.net/forum?id=cfn2O1qvxp) |  | 0 |  | Mingye Zhu, Yi Liu, Lei Zhang, Junbo Guo, Zhendong Mao |  |
| 2345 |  |  [Analysis of Linear Mode Connectivity via Permutation-Based Weight Matching: With Insights into Other Permutation Search Methods](https://openreview.net/forum?id=lYRkGZZi9D) |  | 0 |  | Akira Ito, Masanori Yamada, Atsutoshi Kumagai |  |
| 2346 |  |  [ConMix: Contrastive Mixup at Representation Level for Long-tailed Deep Clustering](https://openreview.net/forum?id=3lH8WT0fhu) |  | 0 |  | Zhixin Li, Yuheng Jia |  |
| 2347 |  |  [ADePT: Adaptive Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning](https://openreview.net/forum?id=fswihJIYbd) |  | 0 |  | Pengwei Tang, Xiaolin Hu, Yong Liu |  |
| 2348 |  |  [Dobi-SVD: Differentiable SVD for LLM Compression and Some New Perspectives](https://openreview.net/forum?id=kws76i5XB8) |  | 0 |  | Qinsi Wang, Jinghan Ke, Masayoshi Tomizuka, Kurt Keutzer, Chenfeng Xu |  |
| 2349 |  |  [Transition Path Sampling with Improved Off-Policy Training of Diffusion Path Samplers](https://openreview.net/forum?id=WQV9kB1qSU) |  | 0 |  | Kiyoung Seong, Seonghyun Park, Seonghwan Kim, Woo Youn Kim, Sungsoo Ahn |  |
| 2350 |  |  [Taming Overconfidence in LLMs: Reward Calibration in RLHF](https://openreview.net/forum?id=l0tg0jzsdL) |  | 0 |  | Jixuan Leng, Chengsong Huang, Banghua Zhu, Jiaxin Huang |  |
| 2351 |  |  [Designing Concise ConvNets with Columnar Stages](https://openreview.net/forum?id=zvaiz3FjA9) |  | 0 |  | Ashish Kumar, Jaesik Park |  |
| 2352 |  |  [Denoising Task Difficulty-based Curriculum for Training Diffusion Models](https://openreview.net/forum?id=96GMFXsbJE) |  | 0 |  | JinYoung Kim, Hyojun Go, Soonwoo Kwon, HyunGyoon Kim |  |
| 2353 |  |  [Circuit Transformer: A Transformer That Preserves Logical Equivalence](https://openreview.net/forum?id=kpnW12Lm9p) |  | 0 |  | Xihan Li, Xing Li, Lei Chen, Xing Zhang, Mingxuan Yuan, Jun Wang |  |
| 2354 |  |  [Learning Splitting Heuristics in Divide-and-Conquer SAT Solvers with Reinforcement Learning](https://openreview.net/forum?id=uUsL07BsMA) |  | 0 |  | Shumao Zhai, Ning Ge |  |
| 2355 |  |  [Provably Robust Explainable Graph Neural Networks against Graph Perturbation Attacks](https://openreview.net/forum?id=iFK0xoceR0) |  | 0 |  | Jiate Li, Meng Pang, Yun Dong, Jinyuan Jia, Binghui Wang |  |
| 2356 |  |  [StringLLM: Understanding the String Processing Capability of Large Language Models](https://openreview.net/forum?id=kTXChtaaNO) |  | 0 |  | Xilong Wang, Hao Fu, Jindong Wang, Neil Zhenqiang Gong |  |
| 2357 |  |  [Counterfactual Generative Modeling with Variational Causal Inference](https://openreview.net/forum?id=oeDcgVC7Xh) |  | 0 |  | Yulun Wu, Louie McConnell, Claudia Iriondo |  |
| 2358 |  |  [Efficient Evolutionary Search Over Chemical Space with Large Language Models](https://openreview.net/forum?id=awWiNvQwf3) |  | 0 |  | Haorui Wang, Marta Skreta, Cher Tian Ser, Wenhao Gao, Lingkai Kong, Felix StriethKalthoff, Chenru Duan, Yuchen Zhuang, Yue Yu, Yanqiao Zhu, Yuanqi Du, Alán AspuruGuzik, Kirill Neklyudov, Chao Zhang |  |
| 2359 |  |  [Preserving Diversity in Supervised Fine-Tuning of Large Language Models](https://openreview.net/forum?id=NQEe7B7bSw) |  | 0 |  | Ziniu Li, Congliang Chen, Tian Xu, Zeyu Qin, Jiancong Xiao, ZhiQuan Luo, Ruoyu Sun |  |
| 2360 |  |  [GSBAK: top-K Geometric Score-based Black-box Attack](https://openreview.net/forum?id=htX7AoHyln) |  | 0 |  | Md Farhamdur Reza, Richeng Jin, Tianfu Wu, Huaiyu Dai |  |
| 2361 |  |  [Adversarial Machine Unlearning](https://openreview.net/forum?id=swWF948IiC) |  | 0 |  | Zonglin Di, Sixie Yu, Yevgeniy Vorobeychik, Yang Liu |  |
| 2362 |  |  [Concept Bottleneck Language Models For Protein Design](https://openreview.net/forum?id=Yt9CFhOOFe) |  | 0 |  | Aya Abdelsalam Ismail, Tuomas P. Oikarinen, Amy Wang, Julius Adebayo, Samuel Don Stanton, Héctor Corrada Bravo, Kyunghyun Cho, Nathan C. Frey |  |
| 2363 |  |  [Difference-of-submodular Bregman Divergence](https://openreview.net/forum?id=vr1QdCNJmN) |  | 0 |  | Masanari Kimura, Takahiro Kawashima, Tasuku Soma, Hideitsu Hino |  |
| 2364 |  |  [Are Large Vision Language Models Good Game Players?](https://openreview.net/forum?id=c4OGMNyzPT) |  | 0 |  | Xinyu Wang, Bohan Zhuang, Qi Wu |  |
| 2365 |  |  [GoodDrag: Towards Good Practices for Drag Editing with Diffusion Models](https://openreview.net/forum?id=VtYfbvwpWp) |  | 0 |  | Zewei Zhang, Huan Liu, Jun Chen, Xiangyu Xu |  |
| 2366 |  |  [DisEnvisioner: Disentangled and Enriched Visual Prompt for Customized Image Generation](https://openreview.net/forum?id=vQxqcVGrhR) |  | 0 |  | Jing He, Haodong Li, Yongzhe Hu, Guibao Shen, Yingjie Cai, Weichao Qiu, YingCong Chen |  |
| 2367 |  |  [A Benchmark for Semantic Sensitive Information in LLMs Outputs](https://openreview.net/forum?id=p3mxzKmuZy) |  | 0 |  | Qingjie Zhang, Han Qiu, Di Wang, Yiming Li, Tianwei Zhang, Wenyu Zhu, Haiqin Weng, Liu Yan, Chao Zhang |  |
| 2368 |  |  [FlexCAD: Unified and Versatile Controllable CAD Generation with Fine-tuned Large Language Models](https://openreview.net/forum?id=Z0eiiV3Yyh) |  | 0 |  | Zhanwei Zhang, Shizhao Sun, Wenxiao Wang, Deng Cai, Jiang Bian |  |
| 2369 |  |  [Efficient Neuron Segmentation in Electron Microscopy by Affinity-Guided Queries](https://openreview.net/forum?id=Y0QqruhqIa) |  | 0 |  | Hang Chen, Chufeng Tang, Xiao Li, Xiaolin Hu |  |
| 2370 |  |  [Continuous Autoregressive Modeling with Stochastic Monotonic Alignment for Speech Synthesis](https://openreview.net/forum?id=cuFzE8Jlvb) |  | 0 |  | Weiwei Lin, Chenhang He |  |
| 2371 |  |  [PIG: Physics-Informed Gaussians as Adaptive Parametric Mesh Representations](https://openreview.net/forum?id=y5B0ca4mjt) |  | 0 |  | Namgyu Kang, Jaemin Oh, Youngjoon Hong, Eunbyung Park |  |
| 2372 |  |  [xFinder: Large Language Models as Automated Evaluators for Reliable Evaluation](https://openreview.net/forum?id=7UqQJUKaLM) |  | 0 |  | Qingchen Yu, Zifan Zheng, Shichao Song, Zhiyu Li, Feiyu Xiong, Bo Tang, Ding Chen |  |
| 2373 |  |  [PiCO: Peer Review in LLMs based on Consistency Optimization](https://openreview.net/forum?id=sfQ6XpApfS) |  | 0 |  | KunPeng Ning, Shuo Yang, Yuyang Liu, JiaYu Yao, ZhenHui Liu, Yonghong Tian, Yibing Song, Li Yuan |  |
| 2374 |  |  [Boosting Perturbed Gradient Ascent for Last-Iterate Convergence in Games](https://openreview.net/forum?id=Jrt9iWalFy) |  | 0 |  | Kenshi Abe, Mitsuki Sakamoto, Kaito Ariu, Atsushi Iwasaki |  |
| 2375 |  |  [Retrieval Augmented Diffusion Model for Structure-informed Antibody Design and Optimization](https://openreview.net/forum?id=a6U41REOa5) |  | 0 |  | Zichen Wang, Yaokun Ji, Jianing Tian, Shuangjia Zheng |  |
| 2376 |  |  [Improving Neural Optimal Transport via Displacement Interpolation](https://openreview.net/forum?id=CfZPzH7ftt) |  | 0 |  | Jaemoo Choi, Yongxin Chen, Jaewoong Choi |  |
| 2377 |  |  [Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models](https://openreview.net/forum?id=s7DkcgpRxL) |  | 0 |  | Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Yang You, Guiming Xie, Xuejian Gong, Kunlong Zhou |  |
| 2378 |  |  [UniCoTT: A Unified Framework for Structural Chain-of-Thought Distillation](https://openreview.net/forum?id=3baOKeI2EU) |  | 0 |  | Xianwei Zhuang, Zhihong Zhu, Zhichang Wang, Xuxin Cheng, Yuexian Zou |  |
| 2379 |  |  [Neural Fluid Simulation on Geometric Surfaces](https://openreview.net/forum?id=58lbAsXCoZ) |  | 0 |  | Haoxiang Wang, Tao Yu, Hui Qiao, Qionghai Dai |  |
| 2380 |  |  [RandLoRA: Full rank parameter-efficient fine-tuning of large models](https://openreview.net/forum?id=Hn5eoTunHN) |  | 0 |  | Paul Albert, Frederic Z. Zhang, Hemanth Saratchandran, Cristian Rodriguez Opazo, Anton van den Hengel, Ehsan Abbasnejad |  |
| 2381 |  |  [VOILA: Evaluation of MLLMs For Perceptual Understanding and Analogical Reasoning](https://openreview.net/forum?id=q5MUMlHxpd) |  | 0 |  | Nilay Yilmaz, Maitreya Patel, Yiran Lawrence Luo, Tejas Gokhale, Chitta Baral, Suren Jayasuriya, Yezhou Yang |  |
| 2382 |  |  [MamBEV: Enabling State Space Models to Learn Birds-Eye-View Representations](https://openreview.net/forum?id=MvEkN2ejZ1) |  | 0 |  | Hongyu Ke, Jack Morris, Kentaro Oguchi, Xiaofei Cao, Yongkang Liu, Haoxin Wang, Yi Ding |  |
| 2383 |  |  [RainbowPO: A Unified Framework for Combining Improvements in Preference Optimization](https://openreview.net/forum?id=trKee5pIFv) |  | 0 |  | Hanyang Zhao, Genta Indra Winata, Anirban Das, ShiXiong Zhang, David D. Yao, Wenpin Tang, Sambit Sahu |  |
| 2384 |  |  [Integral Performance Approximation for Continuous-Time Reinforcement Learning Control](https://openreview.net/forum?id=z21DkDDdgq) |  | 0 |  | Brent A. Wallace, Jennie Si |  |
| 2385 |  |  [Any-step Dynamics Model Improves Future Predictions for Online and Offline Reinforcement Learning](https://openreview.net/forum?id=JZCxlrwjZ8) |  | 0 |  | Haoxin Lin, YuYan Xu, Yihao Sun, Zhilong Zhang, YiChen Li, Chengxing Jia, Junyin Ye, Jiaji Zhang, Yang Yu |  |
| 2386 |  |  [Learning Interleaved Image-Text Comprehension in Vision-Language Large Models](https://openreview.net/forum?id=jZsN9zo8Qi) |  | 0 |  | Chenyu Zhou, Mengdan Zhang, Peixian Chen, Chaoyou Fu, Yunhang Shen, Xiawu Zheng, Xing Sun, Rongrong Ji |  |
| 2387 |  |  [Rethinking the role of frames for SE(3)-invariant crystal structure modeling](https://openreview.net/forum?id=gzxDjnvBDa) |  | 0 |  | Yusei Ito, Tatsunori Taniai, Ryo Igarashi, Yoshitaka Ushiku, Kanta Ono |  |
| 2388 |  |  [A transfer learning framework for weak to strong generalization](https://openreview.net/forum?id=PeLLMw3wLX) |  | 0 |  | Seamus Somerstep, Felipe Maia Polo, Moulinath Banerjee, Yaacov Ritov, Mikhail Yurochkin, Yuekai Sun |  |
| 2389 |  |  [UniGS: Unified Language-Image-3D Pretraining with Gaussian Splatting](https://openreview.net/forum?id=6U2KI1dpfl) |  | 0 |  | Haoyuan Li, Yanpeng Zhou, Tao Tang, Jifei Song, Yihan Zeng, Michael Kampffmeyer, Hang Xu, Xiaodan Liang |  |
| 2390 |  |  [Multi-Task Dense Predictions via Unleashing the Power of Diffusion](https://openreview.net/forum?id=TzdTRC85SQ) |  | 0 |  | Yuqi Yang, PengTao Jiang, Qibin Hou, Hao Zhang, Jinwei Chen, Bo Li |  |
| 2391 |  |  [Fewer May Be Better: Enhancing Offline Reinforcement Learning with Reduced Dataset](https://openreview.net/forum?id=zqtql1YmlS) |  | 0 |  | Yiqin Yang, Quanwei Wang, Chenghao Li, Hao Hu, Chengjie Wu, Yuhua Jiang, Dianyu Zhong, Ziyou Zhang, Qianchuan Zhao, Chongjie Zhang, Bo Xu |  |
| 2392 |  |  [From Commands to Prompts: LLM-based Semantic File System for AIOS](https://openreview.net/forum?id=2G021ZqUEZ) |  | 0 |  | Zeru Shi, Kai Mei, Mingyu Jin, Yongye Su, Chaoji Zuo, Wenyue Hua, Wujiang Xu, Yujie Ren, Zirui Liu, Mengnan Du, Dong Deng, Yongfeng Zhang |  |
| 2393 |  |  [ComLoRA: A Competitive Learning Approach for Enhancing LoRA](https://openreview.net/forum?id=jFcNXJGPGh) |  | 0 |  | Qiushi Huang, Tom Ko, Lilian Tang, Yu Zhang |  |
| 2394 |  |  [Adapting Multi-modal Large Language Model to Concept Drift From Pre-training Onwards](https://openreview.net/forum?id=b20VK2GnSs) |  | 0 |  | Xiaoyu Yang, Jie Lu, En Yu |  |
| 2395 |  |  [RECAST: Reparameterized, Compact weight Adaptation for Sequential Tasks](https://openreview.net/forum?id=J3H8Az3YlB) |  | 0 |  | Nazia Tasnim, Bryan A. Plummer |  |
| 2396 |  |  [PivotMesh: Generic 3D Mesh Generation via Pivot Vertices Guidance](https://openreview.net/forum?id=WAC8LmlKYf) |  | 0 |  | Haohan Weng, Yikai Wang, Tong Zhang, C. L. Philip Chen, Jun Zhu |  |
| 2397 |  |  [Faster Algorithms for Structured Linear and Kernel Support Vector Machines](https://openreview.net/forum?id=DDNFTaVQdU) |  | 0 |  | Yuzhou Gu, Zhao Song, Lichen Zhang |  |
| 2398 |  |  [Uni-Sign: Toward Unified Sign Language Understanding at Scale](https://openreview.net/forum?id=0Xt7uT04cQ) |  | 0 |  | Zecheng Li, Wengang Zhou, Weichao Zhao, Kepeng Wu, Hezhen Hu, Houqiang Li |  |
| 2399 |  |  [Ada-K Routing: Boosting the Efficiency of MoE-based LLMs](https://openreview.net/forum?id=9CqkpQExe2) |  | 0 |  | Tongtian Yue, Longteng Guo, Jie Cheng, Xuange Gao, Hua Huang, Jing Liu |  |
| 2400 |  |  [SonicSim: A customizable simulation platform for speech processing in moving sound source scenarios](https://openreview.net/forum?id=Hx2ADQLi8M) |  | 0 |  | Kai Li, Wendi Sang, Chang Zeng, Runxuan Yang, Guo Chen, Xiaolin Hu |  |
| 2401 |  |  [TIGER: Time-frequency Interleaved Gain Extraction and Reconstruction for Efficient Speech Separation](https://openreview.net/forum?id=rzx3vcvlzj) |  | 0 |  | Mohan Xu, Kai Li, Guo Chen, Xiaolin Hu |  |
| 2402 |  |  [Less is More: Masking Elements in Image Condition Features Avoids Content Leakages in Style Transfer Diffusion Models](https://openreview.net/forum?id=88JJjsLtqr) |  | 0 |  | Lin Zhu, Xinbing Wang, Chenghu Zhou, Qinying Gu, Nanyang Ye |  |
| 2403 |  |  [Neural Exploratory Landscape Analysis for Meta-Black-Box-Optimization](https://openreview.net/forum?id=EEI5R89Cmv) |  | 0 |  | Zeyuan Ma, Jiacheng Chen, Hongshu Guo, YueJiao Gong |  |
| 2404 |  |  [ACTIVE: Offline Reinforcement Learning via Adaptive Imitation and In-sample V-Ensemble](https://openreview.net/forum?id=qiluFujVc8) |  | 0 |  | Tianyuan Chen, Ronglong Cai, Faguo Wu, Xiao Zhang |  |
| 2405 |  |  [DRL: Decomposed Representation Learning for Tabular Anomaly Detection](https://openreview.net/forum?id=CJnceDksRd) |  | 0 |  | Hangting Ye, He Zhao, Wei Fan, Mingyuan Zhou, Dandan Guo, Yi Chang |  |
| 2406 |  |  [RA-TTA: Retrieval-Augmented Test-Time Adaptation for Vision-Language Models](https://openreview.net/forum?id=V3zobHnS61) |  | 0 |  | Youngjun Lee, Doyoung Kim, Junhyeok Kang, Jihwan Bang, Hwanjun Song, JaeGil Lee |  |
| 2407 |  |  [Training-free Camera Control for Video Generation](https://openreview.net/forum?id=KI1zldOFz9) |  | 0 |  | Chen Hou, Zhibo Chen |  |
| 2408 |  |  [CFD: Learning Generalized Molecular Representation via Concept-Enhanced Feedback Disentanglement](https://openreview.net/forum?id=CsOIYMOZaV) |  | 0 |  | Aming Wu, Cheng Deng |  |
| 2409 |  |  [Towards Unbiased Learning in Semi-Supervised Semantic Segmentation](https://openreview.net/forum?id=85G2t3yklD) |  | 0 |  | Rui Sun, Huayu Mai, Wangkai Li, Tianzhu Zhang |  |
| 2410 |  |  [Causal Effect Estimation with Mixed Latent Confounders and Post-treatment Variables](https://openreview.net/forum?id=qe1CsfnN1W) |  | 0 |  | Yaochen Zhu, Jing Ma, Liang Wu, Qi Guo, Liangjie Hong, Jundong Li |  |
| 2411 |  |  [NVS-Solver: Video Diffusion Model as Zero-Shot Novel View Synthesizer](https://openreview.net/forum?id=zDJf7fvdid) |  | 0 |  | Meng You, Zhiyu Zhu, Hui Liu, Junhui Hou |  |
| 2412 |  |  [q-exponential family for policy optimization](https://openreview.net/forum?id=OyyE1FDdrQ) |  | 0 |  | Lingwei Zhu, Haseeb Shah, Han Wang, Yukie Nagai, Martha White |  |
| 2413 |  |  [Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models](https://openreview.net/forum?id=1BdPHbuimc) |  | 0 |  | Zhenyu Pan, Haozheng Luo, Manling Li, Han Liu |  |
| 2414 |  |  [Efficient Alternating Minimization with Applications to Weighted Low Rank Approximation](https://openreview.net/forum?id=rvhu4V7yrX) |  | 0 |  | Zhao Song, Mingquan Ye, Junze Yin, Lichen Zhang |  |
| 2415 |  |  [MAI: A Multi-turn Aggregation-Iteration Model for Composed Image Retrieval](https://openreview.net/forum?id=gXyWbl71n1) |  | 0 |  | Yanzhe Chen, Zhiwen Yang, Jinglin Xu, Yuxin Peng |  |
| 2416 |  |  [Fat-to-Thin Policy Optimization: Offline Reinforcement Learning with Sparse Policies](https://openreview.net/forum?id=SRjzerUpB2) |  | 0 |  | Lingwei Zhu, Han Wang, Yukie Nagai |  |
| 2417 |  |  [Analytic DAG Constraints for Differentiable DAG Learning](https://openreview.net/forum?id=oCdIo9757e) |  | 0 |  | Zhen Zhang, Ignavier Ng, Dong Gong, Yuhang Liu, Mingming Gong, Biwei Huang, Kun Zhang, Anton van den Hengel, Javen Qinfeng Shi |  |
| 2418 |  |  [Decoupled Graph Energy-based Model for Node Out-of-Distribution Detection on Heterophilic Graphs](https://openreview.net/forum?id=NuVBI4wPMm) |  | 0 |  | Yuhan Chen, Yihong Luo, Yifan Song, Pengwen Dai, Jing Tang, Xiaochun Cao |  |
| 2419 |  |  [Learning Causal Alignment for Reliable Disease Diagnosis](https://openreview.net/forum?id=ozZG5FXuTV) |  | 0 |  | Mingzhou Liu, ChingWen Lee, Xinwei Sun, Xueqing Yu, Yu Qiao, Yizhou Wang |  |
| 2420 |  |  [CLDyB: Towards Dynamic Benchmarking for Continual Learning with Pre-trained Models](https://openreview.net/forum?id=RnxwxGXxex) |  | 0 |  | Shengzhuang Chen, Yikai Liao, Xiaoxiao Sun, Kede Ma, Ying Wei |  |
| 2421 |  |  [MGMapNet: Multi-Granularity Representation Learning for End-to-End Vectorized HD Map Construction](https://openreview.net/forum?id=E8S5Upr6oO) |  | 0 |  | Jing Yang, Minyue Jiang, Sen Yang, Xiao Tan, Yingying Li, Errui Ding, Jingdong Wang, Hanli Wang |  |
| 2422 |  |  [Rethinking Spiking Neural Networks from an Ensemble Learning Perspective](https://openreview.net/forum?id=ZyknpOQwkT) |  | 0 |  | Yongqi Ding, Lin Zuo, Mengmeng Jing, Pei He, Hanpu Deng |  |
| 2423 |  |  [Near-optimal Active Regression of Single-Index Models](https://openreview.net/forum?id=iF06WjHnNj) |  | 0 |  | Yi Li, Wai Ming Tai |  |
| 2424 |  |  [Accelerating Neural ODEs: A Variational Formulation-based Approach](https://openreview.net/forum?id=trV41CpAK4) |  | 0 |  | Hongjue Zhao, Yuchen Wang, Hairong Qi, Zijie Huang, Han Zhao, Lui Sha, Huajie Shao |  |
| 2425 |  |  [DiSK: Differentially Private Optimizer with Simplified Kalman Filter for Noise Reduction](https://openreview.net/forum?id=Lfy9q7Icp9) |  | 0 |  | Xinwei Zhang, Zhiqi Bu, Borja Balle, Mingyi Hong, Meisam Razaviyayn, Vahab Mirrokni |  |
| 2426 |  |  [A Theory for Token-Level Harmonization in Retrieval-Augmented Generation](https://openreview.net/forum?id=tbx3u2oZAu) |  | 0 |  | Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng |  |
| 2427 |  |  [Deep Compression Autoencoder for Efficient High-Resolution Diffusion Models](https://openreview.net/forum?id=wH8XXUOUZU) |  | 0 |  | Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Song Han |  |
| 2428 |  |  [PRISM: Privacy-Preserving Improved Stochastic Masking for Federated Generative Models](https://openreview.net/forum?id=B9kUJuWrYC) |  | 0 |  | Kyeongkook Seo, DongJun Han, Jaejun Yoo |  |
| 2429 |  |  [Re-Aligning Language to Visual Objects with an Agentic Workflow](https://openreview.net/forum?id=MPJ4SMnScw) |  | 0 |  | Yuming Chen, Jiangyan Feng, Haodong Zhang, Lijun Gong, Feng Zhu, Rui Zhao, Qibin Hou, MingMing Cheng, Yibing Song |  |
| 2430 |  |  [Step-by-Step Reasoning for Math Problems via Twisted Sequential Monte Carlo](https://openreview.net/forum?id=Ze4aPP0tIn) |  | 0 |  | Shengyu Feng, Xiang Kong, Shuang Ma, Aonan Zhang, Dong Yin, Chong Wang, Ruoming Pang, Yiming Yang |  |
| 2431 |  |  [EG4D: Explicit Generation of 4D Object without Score Distillation](https://openreview.net/forum?id=uq9TLFT7tF) |  | 0 |  | Qi Sun, Zhiyang Guo, Ziyu Wan, Jing Nathan Yan, Shengming Yin, Wengang Zhou, Jing Liao, Houqiang Li |  |
| 2432 |  |  [Large Language Models are Interpretable Learners](https://openreview.net/forum?id=hTphfqtafO) |  | 0 |  | Ruochen Wang, Si Si, Felix X. Yu, Dorothea Wiesmann Rothuizen, ChoJui Hsieh, Inderjit S. Dhillon |  |
| 2433 |  |  [On the expressiveness and spectral bias of KANs](https://openreview.net/forum?id=ydlDRUuGm9) |  | 0 |  | Yixuan Wang, Jonathan W. Siegel, Ziming Liu, Thomas Y. Hou |  |
| 2434 |  |  [Does SGD really happen in tiny subspaces?](https://openreview.net/forum?id=v6iLQBoIJw) |  | 0 |  | Minhak Song, Kwangjun Ahn, Chulhee Yun |  |
| 2435 |  |  [BaB-ND: Long-Horizon Motion Planning with Branch-and-Bound and Neural Dynamics](https://openreview.net/forum?id=JXKFPJe0NU) |  | 0 |  | Keyi Shen, Jiangwei Yu, Jose A. Barreiros, Huan Zhang, Yunzhu Li |  |
| 2436 |  |  [D2O: Dynamic Discriminative Operations for Efficient Long-Context Inference of Large Language Models](https://openreview.net/forum?id=HzBfoUdjHt) |  | 0 |  | Zhongwei Wan, Xinjian Wu, Yu Zhang, Yi Xin, Chaofan Tao, Zhihong Zhu, Xin Wang, Siqi Luo, Jing Xiong, Longyue Wang, Mi Zhang |  |
| 2437 |  |  [Scaling Laws for Downstream Task Performance in Machine Translation](https://openreview.net/forum?id=vPOMTkmSiu) |  | 0 |  | Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei Vassilvitskii, Sanmi Koyejo |  |
| 2438 |  |  [Routing Experts: Learning to Route Dynamic Experts in Existing Multi-modal Large Language Models](https://openreview.net/forum?id=vtT09dYPGI) |  | 0 |  | Qiong Wu, Zhaoxi Ke, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji |  |
| 2439 |  |  [Regret Bounds for Episodic Risk-Sensitive Linear Quadratic Regulator](https://openreview.net/forum?id=VD4PFpecG2) |  | 0 |  | Wenhao Xu, Xuefeng Gao, Xuedong He |  |
| 2440 |  |  [Ambient Diffusion Posterior Sampling: Solving Inverse Problems with Diffusion Models Trained on Corrupted Data](https://openreview.net/forum?id=qeXcMutEZY) |  | 0 |  | Asad Aali, Giannis Daras, Brett Levac, Sidharth Kumar, Alex Dimakis, Jonathan I. Tamir |  |
| 2441 |  |  [HERO: Human-Feedback Efficient Reinforcement Learning for Online Diffusion Model Finetuning](https://openreview.net/forum?id=yMHe9SRvxk) |  | 0 |  | Ayano Hiranaka, ShangFu Chen, ChiehHsin Lai, Dongjun Kim, Naoki Murata, Takashi Shibuya, WeiHsiang Liao, ShaoHua Sun, Yuki Mitsufuji |  |
| 2442 |  |  [Rational Decision-Making Agent with Learning Internal Utility Judgment](https://openreview.net/forum?id=GEBkyKZOc4) |  | 0 |  | Yining Ye, Xin Cong, Shizuo Tian, Yujia Qin, Chong Liu, Yankai Lin, Zhiyuan Liu, Maosong Sun |  |
| 2443 |  |  [Pareto Prompt Optimization](https://openreview.net/forum?id=HGCk5aaSvE) |  | 0 |  | Guang Zhao, ByungJun Yoon, Gilchan Park, Shantenu Jha, Shinjae Yoo, Xiaoning Qian |  |
| 2444 |  |  [BigDocs: An Open Dataset for Training Multimodal Models on Document and Code Tasks](https://openreview.net/forum?id=b1ivBPLb1n) |  | 0 |  | Juan A. Rodríguez, Xiangru Jian, Siba Smarak Panigrahi, Tianyu Zhang, Aarash Feizi, Abhay Puri, Akshay Kalkunte Suresh, François Savard, Ahmed Masry, Shravan Nayak, Rabiul Awal, Mahsa Massoud, Amirhossein Abaskohi, Zichao Li, Suyuchen Wang, PierreAndré Noël, Mats Leon Richter, Saverio Vadacchino, Shubham Agarwal, Sanket Biswas, et al. |  |
| 2445 |  |  [3D-Properties: Identifying Challenges in DPO and Charting a Path Forward](https://openreview.net/forum?id=9Hxdixed7p) |  | 0 |  | Yuzi Yan, Yibo Miao, Jialian Li, Yipin Zhang, Jian Xie, Zhijie Deng, Dong Yan |  |
| 2446 |  |  [MAP: Low-compute Model Merging with Amortized Pareto Fronts via Quadratic Approximation](https://openreview.net/forum?id=1v7SRWsYve) |  | 0 |  | Lu Li, Tianyu Zhang, Zhiqi Bu, Suyuchen Wang, Huan He, Jie Fu, Yonghui Wu, Jiang Bian, Yong Chen, Yoshua Bengio |  |
| 2447 |  |  [Long-tailed Adversarial Training with Self-Distillation](https://openreview.net/forum?id=vM94dZiqx4) |  | 0 |  | Seungju Cho, Hongsin Lee, Changick Kim |  |
| 2448 |  |  [JudgeBench: A Benchmark for Evaluating LLM-Based Judges](https://openreview.net/forum?id=G0dksFayVq) |  | 0 |  | Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Yuan Tang, Alejandro Cuadron, Chenguang Wang, Raluca A. Popa, Ion Stoica |  |
| 2449 |  |  [Indirect Gradient Matching for Adversarial Robust Distillation](https://openreview.net/forum?id=juKVq5dWTR) |  | 0 |  | Hongsin Lee, Seungju Cho, Changick Kim |  |
| 2450 |  |  [Towards Generalization Bounds of GCNs for Adversarially Robust Node Classification](https://openreview.net/forum?id=cp3aW7C5tD) |  | 0 |  | Wen Wen, Han Li, Tieliang Gong, Hong Chen |  |
| 2451 |  |  [REvolve: Reward Evolution with Large Language Models using Human Feedback](https://openreview.net/forum?id=cJPUpL8mOw) |  | 0 |  | Rishi Hazra, Alkis Sygkounas, Andreas Persson, Amy Loutfi, Pedro Zuidberg Dos Martires |  |
| 2452 |  |  [Task Descriptors Help Transformers Learn Linear Models In-Context](https://openreview.net/forum?id=lZNb1CVm5O) |  | 0 |  | Ruomin Huang, Rong Ge |  |
| 2453 |  |  [MallowsPO: Fine-Tune Your LLM with Preference Dispersions](https://openreview.net/forum?id=d8cnezVcaW) |  | 0 |  | Haoxian Chen, Hanyang Zhao, Henry Lam, David D. Yao, Wenpin Tang |  |
| 2454 |  |  [Distribution-Specific Agnostic Conditional Classification With Halfspaces](https://openreview.net/forum?id=KZEqbwJfTl) |  | 0 |  | Jizhou Huang, Brendan Juba |  |
| 2455 |  |  [Temporal Difference Learning: Why It Can Be Fast and How It Will Be Faster](https://openreview.net/forum?id=j3bKnEidtT) |  | 0 |  | Patrick Schnell, Luca Guastoni, Nils Thuerey |  |
| 2456 |  |  [OCCAM: Towards Cost-Efficient and Accuracy-Aware Classification Inference](https://openreview.net/forum?id=CUABD2qIB4) |  | 0 |  | Dujian Ding, Bicheng Xu, Laks V. S. Lakshmanan |  |
| 2457 |  |  [Machine Unlearning via Simulated Oracle Matching](https://openreview.net/forum?id=3vXpZpOn29) |  | 0 |  | Kristian Georgiev, Roy Rinberg, Sung Min Park, Shivam Garg, Andrew Ilyas, Aleksander Madry, Seth Neel |  |
| 2458 |  |  [Discovering Clone Negatives via Adaptive Contrastive Learning for Image-Text Matching](https://openreview.net/forum?id=My9MBsO41H) |  | 0 |  | Renjie Pan, Jihao Dong, Hua Yang |  |
| 2459 |  |  [Conformal Structured Prediction](https://openreview.net/forum?id=2ATD8a8P3C) |  | 0 |  | Botong Zhang, Shuo Li, Osbert Bastani |  |
| 2460 |  |  [Global Well-posedness and Convergence Analysis of Score-based Generative Models via Sharp Lipschitz Estimates](https://openreview.net/forum?id=r3cWq6KKbt) |  | 0 |  | Connor Mooney, Zhongjian Wang, Jack Xin, Yifeng Yu |  |
| 2461 |  |  [To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning](https://openreview.net/forum?id=w6nlcS8Kkn) |  | 0 |  | Zayne Rea Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, Greg Durrett |  |
| 2462 |  |  [Interference Among First-Price Pacing Equilibria: A Bias and Variance Analysis](https://openreview.net/forum?id=6bDJ3CIm5w) |  | 0 |  | Luofeng Liao, Christian Kroer, Sergei Leonenkov, Okke Schrijvers, Liang Shi, Nicolás Stier Moses, Congshan Zhang |  |
| 2463 |  |  [Exploiting Structure in Offline Multi-Agent RL: The Benefits of Low Interaction Rank](https://openreview.net/forum?id=AOlm45AUVS) |  | 0 |  | Wenhao Zhan, Scott Fujimoto, Zheqing Zhu, Jason D. Lee, Daniel Jiang, Yonathan Efroni |  |
| 2464 |  |  [Fully-inductive Node Classification on Arbitrary Graphs](https://openreview.net/forum?id=1Qpt43cqhg) |  | 0 |  | Jianan Zhao, Zhaocheng Zhu, Mikhail Galkin, Hesham Mostafa, Michael M. Bronstein, Jian Tang |  |
| 2465 |  |  [Post-hoc Reward Calibration: A Case Study on Length Bias](https://openreview.net/forum?id=Iu8RytBaji) |  | 0 |  | Zeyu Huang, Zihan Qiu, Zili Wang, Edoardo M. Ponti, Ivan Titov |  |
| 2466 |  |  [ObscuraCoder: Powering Efficient Code LM Pre-Training Via Obfuscation Grounding](https://openreview.net/forum?id=VYvxrD7aS0) |  | 0 |  | Indraneil Paul, Haoyi Yang, Goran Glavas, Kristian Kersting, Iryna Gurevych |  |
| 2467 |  |  [Differentiable Causal Discovery for Latent Hierarchical Causal Models](https://openreview.net/forum?id=Bp0HBaMNRl) |  | 0 |  | Parjanya Prajakta Prashant, Ignavier Ng, Kun Zhang, Biwei Huang |  |
| 2468 |  |  [SyllableLM: Learning Coarse Semantic Units for Speech Language Models](https://openreview.net/forum?id=dGSOn7sdWg) |  | 0 |  | Alan Baade, Puyuan Peng, David Harwath |  |
| 2469 |  |  [Exact Community Recovery under Side Information: Optimality of Spectral Algorithms](https://openreview.net/forum?id=zhFyKgqxlz) |  | 0 |  | Julia Gaudio, Nirmit Joshi |  |
| 2470 |  |  [Repulsive Latent Score Distillation for Solving Inverse Problems](https://openreview.net/forum?id=bwJxUB0y46) |  | 0 |  | Nicolas Zilberstein, Morteza Mardani, Santiago Segarra |  |
| 2471 |  |  [Calibrating Expressions of Certainty](https://openreview.net/forum?id=dNunnVB4W6) |  | 0 |  | Peiqi Wang, Barbara D. Lam, Yingcheng Liu, Ameneh AsgariTarghi, Rameswar Panda, William M. Wells III, Tina Kapur, Polina Golland |  |
| 2472 |  |  [Leveraging Driver Field-of-View for Multimodal Ego-Trajectory Prediction](https://openreview.net/forum?id=LLWj8on4Rv) |  | 0 |  | M. Eren Akbiyik, Nedko Savov, Danda Pani Paudel, Nikola Popovic, Christian Vater, Otmar Hilliges, Luc Van Gool, Xi Wang |  |
| 2473 |  |  [Learning General-purpose Biomedical Volume Representations using Randomized Synthesis](https://openreview.net/forum?id=xOmC5LiVuN) |  | 0 |  | Neel Dey, Benjamin Billot, Hallee E. Wong, Clinton J. Wang, Mengwei Ren, Ellen Grant, Adrian V. Dalca, Polina Golland |  |
| 2474 |  |  [Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning](https://openreview.net/forum?id=rQ7fz9NO7f) |  | 0 |  | Gang Liu, Michael Sun, Wojciech Matusik, Meng Jiang, Jie Chen |  |
| 2475 |  |  [Test-Time Ensemble via Linear Mode Connectivity: A Path to Better Adaptation](https://openreview.net/forum?id=4wk2eOKGvh) |  | 0 |  | Byungjai Kim, Chanho Ahn, Wissam J. Baddar, Kikyung Kim, Huijin Lee, Saehyun Ahn, Seungju Han, Sungjoo Suh, Eunho Yang |  |
| 2476 |  |  [Faster Inference of Flow-Based Generative Models via Improved Data-Noise Coupling](https://openreview.net/forum?id=rsGPrJDIhh) |  | 0 |  | Aram Davtyan, Leello Tadesse Dadi, Volkan Cevher, Paolo Favaro |  |
| 2477 |  |  [Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional, Black-box Systems](https://openreview.net/forum?id=PLskiLUBDW) |  | 0 |  | Daniel MacKinlay, Russell Tsuchida, Daniel Edward Pagendam, Petra Kuhnert |  |
| 2478 |  |  [Learning Multi-Index Models with Neural Networks via Mean-Field Langevin Dynamics](https://openreview.net/forum?id=WHhZv8X5zF) |  | 0 |  | Alireza Mousavi Hosseini, Denny Wu, Murat A. Erdogdu |  |
| 2479 |  |  [Adversarial Latent Feature Augmentation for Fairness](https://openreview.net/forum?id=cNaHOdvh9J) |  | 0 |  | Hoin Jung, Junyi Chai, Xiaoqian Wang |  |
| 2480 |  |  [Symbolic regression via MDLformer-guided search: from minimizing prediction error to minimizing description length](https://openreview.net/forum?id=ljAS7cPAU0) |  | 0 |  | Zihan Yu, Jingtao Ding, Yong Li, Depeng Jin |  |
| 2481 |  |  [Robust Gymnasium: A Unified Modular Benchmark for Robust Reinforcement Learning](https://openreview.net/forum?id=2uQBSa2X4R) |  | 0 |  | Shangding Gu, Laixi Shi, Muning Wen, Ming Jin, Eric Mazumdar, Yuejie Chi, Adam Wierman, Costas J. Spanos |  |
| 2482 |  |  [Multimodal Unsupervised Domain Generalization by Retrieving Across the Modality Gap](https://openreview.net/forum?id=bqoHdVMIbt) |  | 0 |  | Christopher Liao, Christian So, Theodoros Tsiligkaridis, Brian Kulis |  |
| 2483 |  |  [Dynamic Neural Fortresses: An Adaptive Shield for Model Extraction Defense](https://openreview.net/forum?id=029hDSVoXK) |  | 0 |  | Siyu Luan, Zhenyi Wang, Li Shen, Zonghua Gu, Chao Wu, Dacheng Tao |  |
| 2484 |  |  [How many samples are needed to train a deep neural network?](https://openreview.net/forum?id=q6zrZbth1F) |  | 0 |  | Pegah Golestaneh, Mahsa Taheri, Johannes Lederer |  |
| 2485 |  |  [Guided Score identity Distillation for Data-Free One-Step Text-to-Image Generation](https://openreview.net/forum?id=HMVDiaWMwM) |  | 0 |  | Mingyuan Zhou, Zhendong Wang, Huangjie Zheng, Hai Huang |  |
| 2486 |  |  [Partial Gromov-Wasserstein Metric](https://openreview.net/forum?id=sCew1tR6No) |  | 0 |  | Yikun Bai, Rocio Diaz Martin, Abihith Kothapalli, Hengrong Du, Xinran Liu, Soheil Kolouri |  |
| 2487 |  |  [Score Forgetting Distillation: A Swift, Data-Free Method for Machine Unlearning in Diffusion Models](https://openreview.net/forum?id=gjwhDHeAsz) |  | 0 |  | Tianqi Chen, Shujian Zhang, Mingyuan Zhou |  |
| 2488 |  |  [Deep MMD Gradient Flow without adversarial training](https://openreview.net/forum?id=Pf85K2wtz8) |  | 0 |  | Alexandre Galashov, Valentin De Bortoli, Arthur Gretton |  |
| 2489 |  |  [Fragment and Geometry Aware Tokenization of Molecules for Structure-Based Drug Design Using Language Models](https://openreview.net/forum?id=mMhZS7qt0U) |  | 0 |  | Cong Fu, Xiner Li, Blake Olson, Heng Ji, Shuiwang Ji |  |
| 2490 |  |  [Catastrophic Failure of LLM Unlearning via Quantization](https://openreview.net/forum?id=lHSeDYamnz) |  | 0 |  | Zhiwei Zhang, Fali Wang, Xiaomin Li, Zongyu Wu, Xianfeng Tang, Hui Liu, Qi He, Wenpeng Yin, Suhang Wang |  |
| 2491 |  |  [Learning the Optimal Stopping for Early Classification within Finite Horizons via Sequential Probability Ratio Test](https://openreview.net/forum?id=SRghq20nGU) |  | 0 |  | Akinori F. Ebihara, Taiki Miyagawa, Kazuyuki Sakurai, Hitoshi Imaoka |  |
| 2492 |  |  [CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale](https://openreview.net/forum?id=d5HUnyByAI) |  | 0 |  | ZeMing Gong, Austin T. Wang, Xiaoliang Huo, Joakim Bruslund Haurum, Scott C. Lowe, Graham W. Taylor, Angel X. Chang |  |
| 2493 |  |  [How DNNs break the Curse of Dimensionality: Compositionality and Symmetry Learning](https://openreview.net/forum?id=UvpuGrd6ey) |  | 0 |  | Arthur Jacot, Seok Hoan Choi, Yuxiao Wen |  |
| 2494 |  |  [Detecting Backdoor Samples in Contrastive Language Image Pretraining](https://openreview.net/forum?id=KmQEsIfhr9) |  | 0 |  | Hanxun Huang, Sarah Monazam Erfani, Yige Li, Xingjun Ma, James Bailey |  |
| 2495 |  |  [Contractive Dynamical Imitation Policies for Efficient Out-of-Sample Recovery](https://openreview.net/forum?id=lILEtkWOXD) |  | 0 |  | Amin Abyaneh, Mahrokh Ghoddousi Boroujeni, HsiuChin Lin, Giancarlo FerrariTrecate |  |
| 2496 |  |  [Duoduo CLIP: Efficient 3D Understanding with Multi-View Images](https://openreview.net/forum?id=iGbuc9ekKK) |  | 0 |  | HanHung Lee, Yiming Zhang, Angel X. Chang |  |
| 2497 |  |  [OmniSep: Unified Omni-Modality Sound Separation with Query-Mixup](https://openreview.net/forum?id=DkzZ1ooc7q) |  | 0 |  | Xize Cheng, Siqi Zheng, Zehan Wang, Minghui Fang, Ziang Zhang, Rongjie Huang, Shengpeng Ji, Jialong Zuo, Tao Jin, Zhou Zhao |  |
| 2498 |  |  [Optimal Learning of Kernel Logistic Regression for Complex Classification Scenarios](https://openreview.net/forum?id=WlhVRh2rQ0) |  | 0 |  | Hongwei Wen, Annika Betken, Hanyuan Hang |  |
| 2499 |  |  [Aligning Generative Denoising with Discriminative Objectives Unleashes Diffusion for Visual Perception](https://openreview.net/forum?id=rMOhA1JNPo) |  | 0 |  | Ziqi Pang, Xin Xu, YuXiong Wang |  |
| 2500 |  |  [Varying Shades of Wrong: Aligning LLMs with Wrong Answers Only](https://openreview.net/forum?id=p74CpDzw1Y) |  | 0 |  | Jihan Yao, Wenxuan Ding, Shangbin Feng, Lucy Lu Wang, Yulia Tsvetkov |  |
| 2501 |  |  [Boltzmann Semantic Score: A Semantic Metric for Evaluating Large Vision Models Using Large Language Models](https://openreview.net/forum?id=9yJKTosUex) |  | 0 |  | Ali Khajegili Mirabadi, Katherine Rich, Hossein Farahani, Ali Bashashati |  |
| 2502 |  |  [Training Nonlinear Transformers for Chain-of-Thought Inference: A Theoretical Generalization Analysis](https://openreview.net/forum?id=n7n8McETXw) |  | 0 |  | Hongkang Li, Songtao Lu, PinYu Chen, Xiaodong Cui, Meng Wang |  |
| 2503 |  |  [Competing Large Language Models in Multi-Agent Gaming Environments](https://openreview.net/forum?id=DI4gW8viB6) |  | 0 |  | Jentse Huang, Eric John Li, Man Ho Lam, Tian Liang, Wenxuan Wang, Youliang Yuan, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Michael R. Lyu |  |
| 2504 |  |  [A Causal Lens for Learning Long-term Fair Policies](https://openreview.net/forum?id=rPkCVSsoM4) |  | 0 |  | Jacob Lear, Lu Zhang |  |
| 2505 |  |  [Building Math Agents with Multi-Turn Iterative Preference Learning](https://openreview.net/forum?id=WjKea8bGFF) |  | 0 |  | Wei Xiong, Chengshuai Shi, Jiaming Shen, Aviv Rosenberg, Zhen Qin, Daniele Calandriello, Misha Khalman, Rishabh Joshi, Bilal Piot, Mohammad Saleh, Chi Jin, Tong Zhang, Tianqi Liu |  |
| 2506 |  |  [Looking into User's Long-term Interests through the Lens of Conservative Evidential Learning](https://openreview.net/forum?id=o99Yn1wN9J) |  | 0 |  | Dingrong Wang, Krishna Prasad Neupane, Ervine Zheng, Qi Yu |  |
| 2507 |  |  [Sharpness-Aware Minimization: General Analysis and Improved Rates](https://openreview.net/forum?id=8rvqpiTTFv) |  | 0 |  | Dimitris Oikonomou, Nicolas Loizou |  |
| 2508 |  |  [Learning to Steer Markovian Agents under Model Uncertainty](https://openreview.net/forum?id=IzYczpPqKq) |  | 0 |  | Jiawei Huang, Vinzenz Thoma, Zebang Shen, Heinrich H. Nax, Niao He |  |
| 2509 |  |  [Diff-PIC: Revolutionizing Particle-In-Cell Nuclear Fusion Simulation with Diffusion Models](https://openreview.net/forum?id=c9z65sDx6M) |  | 0 |  | Chuan Liu, Chunshu Wu, Shihui Cao, Mingkai Chen, James Chenhao Liang, Ang Li, Michael Huang, Chuang Ren, Ying Nian Wu, Dongfang Liu, Tong Geng |  |
| 2510 |  |  [NRGBoost: Energy-Based Generative Boosted Trees](https://openreview.net/forum?id=wQHyjIZ1SH) |  | 0 |  | João Bravo |  |
| 2511 |  |  [Scalable Universal T-Cell Receptor Embeddings from Adaptive Immune Repertoires](https://openreview.net/forum?id=wyF5vNIsO7) |  | 0 |  | Paidamoyo Chapfuwa, Ilker Demirel, Lorenzo Pisani, Javier Zazo, Elon Portugaly, H. Jabran Zahid, Julia Greissl |  |
| 2512 |  |  [Stochastic Polyak Step-sizes and Momentum: Convergence Guarantees and Practical Performance](https://openreview.net/forum?id=nuX2yPejiL) |  | 0 |  | Dimitris Oikonomou, Nicolas Loizou |  |
| 2513 |  |  [InstaTrain: Adaptive Training via Ultra-Fast Natural Annealing within Dynamical Systems](https://openreview.net/forum?id=QhhShUQIpJ) |  | 0 |  | Chuan Liu, Ruibing Song, Chunshu Wu, Pouya Haghi, Tong Geng |  |
| 2514 |  |  [Agree to Disagree: Demystifying Homogeneous Deep Ensembles through Distributional Equivalence](https://openreview.net/forum?id=XYRPm8rAGM) |  | 0 |  | Yipei Wang, Xiaoqian Wang |  |
| 2515 |  |  [Lr0.Fm: low-Resolution Zero-Shot Classification Benchmark for Foundation Models](https://openreview.net/forum?id=AsFxRSLtqR) |  | 0 |  | Priyank Pathak, Shyam Marjit, Shruti Vyas, Yogesh S. Rawat |  |
| 2516 |  |  [Reinforcement learning with combinatorial actions for coupled restless bandits](https://openreview.net/forum?id=DhH3LbA6F6) |  | 0 |  | Lily Xu, Bryan Wilder, Elias Boutros Khalil, Milind Tambe |  |
| 2517 |  |  [Scale-Free Graph-Language Models](https://openreview.net/forum?id=nFcgay1Yo9) |  | 0 |  | Jianglin Lu, Yixuan Liu, Yitian Zhang, Yun Fu |  |
| 2518 |  |  [DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads](https://openreview.net/forum?id=cFu7ze7xUm) |  | 0 |  | Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, Song Han |  |
| 2519 |  |  [Generalization in VAE and Diffusion Models: A Unified Information-Theoretic Analysis](https://openreview.net/forum?id=NGB6YNnO5o) |  | 0 |  | Qi Chen, Jierui Zhu, Florian Shkurti |  |
| 2520 |  |  [OGBench: Benchmarking Offline Goal-Conditioned RL](https://openreview.net/forum?id=M992mjgKzI) |  | 0 |  | Seohong Park, Kevin Frans, Benjamin Eysenbach, Sergey Levine |  |
| 2521 |  |  [DOPL: Direct Online Preference Learning for Restless Bandits with Preference Feedback](https://openreview.net/forum?id=2iYVBqRHK4) |  | 0 |  | Guojun Xiong, Ujwal Dinesha, Debajoy Mukherjee, Jian Li, Srinivas Shakkottai |  |
| 2522 |  |  [For Better or For Worse? Learning Minimum Variance Features With Label Augmentation](https://openreview.net/forum?id=LCL8SMGxDY) |  | 0 |  | Muthu Chidambaram, Rong Ge |  |
| 2523 |  |  [Permute-and-Flip: An optimally stable and watermarkable decoder for LLMs](https://openreview.net/forum?id=YyVVicZ32M) |  | 0 |  | Xuandong Zhao, Lei Li, YuXiang Wang |  |
| 2524 |  |  [Distributional Associations vs In-Context Reasoning: A Study of Feed-forward and Attention Layers](https://openreview.net/forum?id=WCVMqRHWW5) |  | 0 |  | Lei Chen, Joan Bruna, Alberto Bietti |  |
| 2525 |  |  [Last Iterate Convergence of Incremental Methods as a Model of Forgetting](https://openreview.net/forum?id=mSGcDhQPwm) |  | 0 |  | Xufeng Cai, Jelena Diakonikolas |  |
| 2526 |  |  [PEAR: Primitive Enabled Adaptive Relabeling for Boosting Hierarchical Reinforcement Learning](https://openreview.net/forum?id=0nJEgNpb4l) |  | 0 |  | Utsav Singh, Vinay P. Namboodiri |  |
| 2527 |  |  [Image and Video Tokenization with Binary Spherical Quantization](https://openreview.net/forum?id=yGnsH3gQ6U) |  | 0 |  | Yue Zhao, Yuanjun Xiong, Philipp Krähenbühl |  |
| 2528 |  |  [DeciMamba: Exploring the Length Extrapolation Potential of Mamba](https://openreview.net/forum?id=iWSl5Zyjjw) |  | 0 |  | Assaf BenKish, Itamar Zimerman, Shady AbuHussein, Nadav Cohen, Amir Globerson, Lior Wolf, Raja Giryes |  |
| 2529 |  |  [Quality over Quantity in Attention Layers: When Adding More Heads Hurts](https://openreview.net/forum?id=y9Xp9NozPR) |  | 0 |  | Noah Amsel, Gilad Yehudai, Joan Bruna |  |
| 2530 |  |  [Energy-Weighted Flow Matching for Offline Reinforcement Learning](https://openreview.net/forum?id=HA0oLUvuGI) |  | 0 |  | Shiyuan Zhang, Weitong Zhang, Quanquan Gu |  |
| 2531 |  |  [X-Sample Contrastive Loss: Improving Contrastive Learning with Sample Similarity Graphs](https://openreview.net/forum?id=c1Ng0f8ivn) |  | 0 |  | Vlad Sobal, Mark Ibrahim, Randall Balestriero, Vivien Cabannes, Diane Bouchacourt, Pietro Astolfi, Kyunghyun Cho, Yann LeCun |  |
| 2532 |  |  [Metamizer: A Versatile Neural Optimizer for Fast and Accurate Physics Simulations](https://openreview.net/forum?id=60TXv9Xif5) |  | 0 |  | Nils Wandel, Stefan Schulz, Reinhard Klein |  |
| 2533 |  |  [Satisficing Regret Minimization in Bandits](https://openreview.net/forum?id=5WPQIVgWCg) |  | 0 |  | Qing Feng, Tianyi Ma, Ruihao Zhu |  |
| 2534 |  |  [VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control](https://openreview.net/forum?id=0n4bS0R5MM) |  | 0 |  | Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, HsinYing Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, David B. Lindell, Sergey Tulyakov |  |
| 2535 |  |  [Structure Language Models for Protein Conformation Generation](https://openreview.net/forum?id=OzUNDnpQyd) |  | 0 |  | Jiarui Lu, Xiaoyin Chen, Stephen Zhewen Lu, Chence Shi, Hongyu Guo, Yoshua Bengio, Jian Tang |  |
| 2536 |  |  [Three Mechanisms of Feature Learning in a Linear Network](https://openreview.net/forum?id=Wh4SE2S7Mo) |  | 0 |  | Yizhou Xu, Ziyin Liu |  |
| 2537 |  |  [Matryoshka Multimodal Models](https://openreview.net/forum?id=Uhj5OxAz7I) |  | 0 |  | Mu Cai, Jianwei Yang, Jianfeng Gao, Yong Jae Lee |  |
| 2538 |  |  [CREMA: Generalizable and Efficient Video-Language Reasoning via Multimodal Modular Fusion](https://openreview.net/forum?id=3UaOlzDEt2) |  | 0 |  | Shoubin Yu, Jaehong Yoon, Mohit Bansal |  |
| 2539 |  |  [GI-GS: Global Illumination Decomposition on Gaussian Splatting for Inverse Rendering](https://openreview.net/forum?id=hJIEtJlvhL) |  | 0 |  | Hongze Chen, Zehong Lin, Jun Zhang |  |
| 2540 |  |  [TEOChat: A Large Vision-Language Assistant for Temporal Earth Observation Data](https://openreview.net/forum?id=pZz0nOroGv) |  | 0 |  | Jeremy Andrew Irvin, Emily Ruoyu Liu, Joyce Chuyi Chen, Ines Dormoy, Jinyoung Kim, Samar Khanna, Zhuo Zheng, Stefano Ermon |  |
| 2541 |  |  [Concept-ROT: Poisoning Concepts in Large Language Models with Model Editing](https://openreview.net/forum?id=RzUvkI3p1D) |  | 0 |  | Keltin Grimes, Marco Christiani, David Shriver, Marissa Catherine Connor |  |
| 2542 |  |  [SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And Video Generation](https://openreview.net/forum?id=hgTFotBRKl) |  | 0 |  | Jaehong Yoon, Shoubin Yu, Vaidehi Patil, Huaxiu Yao, Mohit Bansal |  |
| 2543 |  |  [Aligned LLMs Are Not Aligned Browser Agents](https://openreview.net/forum?id=NsFZZU9gvk) |  | 0 |  | Priyanshu Kumar, Elaine Lau, Saranya Vijayakumar, Tu Trinh, Elaine T. Chang, Vaughn Robinson, Shuyan Zhou, Matt Fredrikson, Sean M. Hendryx, Summer Yue, Zifan Wang |  |
| 2544 |  |  [Revisiting Prefix-tuning: Statistical Benefits of Reparameterization among Prompts](https://openreview.net/forum?id=QjTSaFXg25) |  | 0 |  | Minh Le, Chau Nguyen, Huy Nguyen, Quyen Tran, Trung Le, Nhat Ho |  |
| 2545 |  |  [Adapt-∞: Scalable Continual Multimodal Instruction Tuning via Dynamic Data Selection](https://openreview.net/forum?id=EwFJaXVePU) |  | 0 |  | Adyasha Maharana, Jaehong Yoon, Tianlong Chen, Mohit Bansal |  |
| 2546 |  |  [Matcha: Mitigating Graph Structure Shifts with Test-Time Adaptation](https://openreview.net/forum?id=EpgoFFUM2q) |  | 0 |  | Wenxuan Bao, Zhichen Zeng, Zhining Liu, Hanghang Tong, Jingrui He |  |
| 2547 |  |  [Adaptive Shrinkage Estimation for Personalized Deep Kernel Regression in Modeling Brain Trajectories](https://openreview.net/forum?id=peX9zpWgg4) |  | 0 |  | Vasiliki Tassopoulou, Haochang Shou, Christos Davatzikos |  |
| 2548 |  |  [DS-LLM: Leveraging Dynamical Systems to Enhance Both Training and Inference of Large Language Models](https://openreview.net/forum?id=OPSpdc25IZ) |  | 0 |  | Ruibing Song, Chuan Liu, Chunshu Wu, Ang Li, Dongfang Liu, Ying Nian Wu, Tong Geng |  |
| 2549 |  |  [AdaFisher: Adaptive Second Order Optimization via Fisher Information](https://openreview.net/forum?id=puTxuiK2qO) |  | 0 |  | Damien Martins Gomes, Yanlei Zhang, Eugene Belilovsky, Guy Wolf, Mahdi S. Hosseini |  |
| 2550 |  |  [DPaI: Differentiable Pruning at Initialization with Node-Path Balance Principle](https://openreview.net/forum?id=hvLBTpiDt3) |  | 0 |  | Lichuan Xiang, Quan NguyenTri, LanCuong Nguyen, Hoang Pham, Khoat Than, Long TranThanh, Hongkai Wen |  |
| 2551 |  |  [Learning Molecular Representation in a Cell](https://openreview.net/forum?id=BbZy8nI1si) |  | 0 |  | Gang Liu, Srijit Seal, John Arevalo, Zhenwen Liang, Anne E. Carpenter, Meng Jiang, Shantanu Singh |  |
| 2552 |  |  [Gradient descent with generalized Newton's method](https://openreview.net/forum?id=bI3fcTsKW4) |  | 0 |  | Zhiqi Bu, Shiyun Xu |  |
| 2553 |  |  [Certified Robustness Under Bounded Levenshtein Distance](https://openreview.net/forum?id=cd79pbXi4N) |  | 0 |  | Elías AbadRocamora, Grigorios Chrysos, Volkan Cevher |  |
| 2554 |  |  [Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice](https://openreview.net/forum?id=Tn8EQIFIMQ) |  | 0 |  | JianQiao Zhu, Haijiang Yan, Thomas L. Griffiths |  |
| 2555 |  |  [Memory Mosaics](https://openreview.net/forum?id=IiagjrJNwF) |  | 0 |  | Jianyu Zhang, Niklas Nolte, Ranajoy Sadhukhan, Beidi Chen, Léon Bottou |  |
| 2556 |  |  [Hierarchical World Models as Visual Whole-Body Humanoid Controllers](https://openreview.net/forum?id=7wuJMvK639) |  | 0 |  | Nicklas Hansen, Jyothir S. V, Vlad Sobal, Yann LeCun, Xiaolong Wang, Hao Su |  |
| 2557 |  |  [How to Probe: Simple Yet Effective Techniques for Improving Post-hoc Explanations](https://openreview.net/forum?id=57NfyYxh5f) |  | 0 |  | Siddhartha Gairola, Moritz Böhle, Francesco Locatello, Bernt Schiele |  |
| 2558 |  |  [Random-Set Neural Networks](https://openreview.net/forum?id=pdjkikvCch) |  | 0 |  | Shireen Kudukkil Manchingal, Muhammad Mubashar, Kaizheng Wang, Keivan Shariatmadar, Fabio Cuzzolin |  |
| 2559 |  |  [Mitigating Spurious Correlations in Zero-Shot Multimodal Models](https://openreview.net/forum?id=UsRKFYR4lM) |  | 0 |  | Shenyu Lu, Junyi Chai, Xiaoqian Wang |  |
| 2560 |  |  [Efficient Cross-Episode Meta-RL](https://openreview.net/forum?id=UENQuayzr1) |  | 0 |  | Gresa Shala, André Biedenkapp, Pierre Krack, Florian Walter, Josif Grabocka |  |
| 2561 |  |  [Exposure Bracketing Is All You Need For A High-Quality Image](https://openreview.net/forum?id=rDIf6NA5mj) |  | 0 |  | Zhilu Zhang, Shuohao Zhang, Renlong Wu, Zifei Yan, Wangmeng Zuo |  |
| 2562 |  |  [Towards Realistic Data Generation for Real-World Super-Resolution](https://openreview.net/forum?id=JkCJBoNUcU) |  | 0 |  | Long Peng, Wenbo Li, Renjing Pei, Jingjing Ren, Jiaqi Xu, Yang Wang, Yang Cao, ZhengJun Zha |  |
| 2563 |  |  [Visual Description Grounding Reduces Hallucinations and Boosts Reasoning in LVLMs](https://openreview.net/forum?id=3PRvlT8b1R) |  | 0 |  | Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Utkarsh Tyagi, Oriol Nieto, Zeyu Jin, Dinesh Manocha |  |
| 2564 |  |  [MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs](https://openreview.net/forum?id=DgaY5mDdmT) |  | 0 |  | Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, Filip Ilievski |  |
| 2565 |  |  [Partially Observed Trajectory Inference using Optimal Transport and a Dynamics Prior](https://openreview.net/forum?id=H8hO3T3DYe) |  | 0 |  | Anming Gu, Edward Chien, Kristjan H. Greenewald |  |
| 2566 |  |  [Universal Sharpness Dynamics in Neural Network Training: Fixed Point Analysis, Edge of Stability, and Route to Chaos](https://openreview.net/forum?id=VZN0irKnl0) |  | 0 |  | Dayal Singh Kalra, Tianyu He, Maissam Barkeshli |  |
| 2567 |  |  [CHAMP: Conformalized 3D Human Multi-Hypothesis Pose Estimators](https://openreview.net/forum?id=kPC83HK4br) |  | 0 |  | Harry Zhang, Luca Carlone |  |
| 2568 |  |  [ELFS: Label-Free Coreset Selection with Proxy Training Dynamics](https://openreview.net/forum?id=yklJpvB7Dq) |  | 0 |  | Haizhong Zheng, Elisa Tsai, Yifu Lu, Jiachen Sun, Brian R. Bartoldson, Bhavya Kailkhura, Atul Prakash |  |
| 2569 |  |  [Inverse Attention Agents for Multi-Agent Systems](https://openreview.net/forum?id=OaoDVZntGe) |  | 0 |  | Qian Long, Ruoyan Li, Minglu Zhao, Tao Gao, Demetri Terzopoulos |  |
| 2570 |  |  [R2Det: Exploring Relaxed Rotation Equivariance in 2D Object Detection](https://openreview.net/forum?id=EUeNr3e8AV) |  | 0 |  | Zhiqiang Wu, Yingjie Liu, Hanlin Dong, Xuan Tang, Jian Yang, Bo Jin, Mingsong Chen, Xian Wei |  |
| 2571 |  |  [Computational Limits of Low-Rank Adaptation (LoRA) Fine-Tuning for Transformer Models](https://openreview.net/forum?id=Lf5znhZmFu) |  | 0 |  | Jerry YaoChieh Hu, Maojiang Su, EnJui Kuo, Zhao Song, Han Liu |  |
| 2572 |  |  [A Simple Approach to Unifying Diffusion-based Conditional Generation](https://openreview.net/forum?id=tAGmxz1TUi) |  | 0 |  | Xirui Li, Charles Herrmann, Kelvin C. K. Chan, Yinxiao Li, Deqing Sun, Chao Ma, MingHsuan Yang |  |
| 2573 |  |  [Do LLMs "know" internally when they follow instructions?](https://openreview.net/forum?id=qIN5VDdEOr) |  | 0 |  | Juyeon Heo, Christina HeinzeDeml, Oussama Elachqar, Kwan Ho Ryan Chan, Shirley You Ren, Andrew C. Miller, Udhyakumar Nallasamy, Jaya Narain |  |
| 2574 |  |  [Fundamental Limits of Prompt Tuning Transformers: Universality, Capacity and Efficiency](https://openreview.net/forum?id=jDpdQPMosW) |  | 0 |  | Jerry YaoChieh Hu, WeiPo Wang, Ammar Gilani, Chenyang Li, Zhao Song, Han Liu |  |
| 2575 |  |  [Disentangling 3D Animal Pose Dynamics with Scrubbed Conditional Latent Variables](https://openreview.net/forum?id=i7drKWhFCo) |  | 0 |  | Joshua Huang Wu, Hari Koneru, James Russell Ravenel, Anshuman Sabath, James Michael Roach, Shaun SzeXian Lim, Michael R. Tadross, Alex H. Williams, Timothy W. Dunn |  |
| 2576 |  |  [Provable Convergence and Limitations of Geometric Tempering for Langevin Dynamics](https://openreview.net/forum?id=DZcmz9wU0i) |  | 0 |  | Omar Chehab, Anna Korba, Austin J. Stromme, Adrien Vacher |  |
| 2577 |  |  [Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training](https://openreview.net/forum?id=SIE6VFps9x) |  | 0 |  | Maximillian Chen, Ruoxi Sun, Tomas Pfister, Sercan Ö. Arik |  |
| 2578 |  |  [Can a Large Language Model be a Gaslighter?](https://openreview.net/forum?id=RQPSPGpBOP) |  | 0 |  | Wei Li, Luyao Zhu, Yang Song, Ruixi Lin, Rui Mao, Yang You |  |
| 2579 |  |  [Synergy Between Sufficient Changes and Sparse Mixing Procedure for Disentangled Representation Learning](https://openreview.net/forum?id=G1r2rBkUdu) |  | 0 |  | Zijian Li, Shunxing Fan, Yujia Zheng, Ignavier Ng, Shaoan Xie, Guangyi Chen, Xinshuai Dong, Ruichu Cai, Kun Zhang |  |
| 2580 |  |  [Edge Prompt Tuning for Graph Neural Networks](https://openreview.net/forum?id=92vMaHotTM) |  | 0 |  | Xingbo Fu, Yinhan He, Jundong Li |  |
| 2581 |  |  [Sparse Learning for State Space Models on Mobile](https://openreview.net/forum?id=t8KLjiFNwn) |  | 0 |  | Xuan Shen, Hangyu Zheng, Yifan Gong, Zhenglun Kong, Changdi Yang, Zheng Zhan, Yushu Wu, Xue Lin, Yanzhi Wang, Pu Zhao, Wei Niu |  |
| 2582 |  |  [Optimized Multi-Token Joint Decoding With Auxiliary Model for LLM Inference](https://openreview.net/forum?id=ZHhBawo3k5) |  | 0 |  | Zongyue Qin, Ziniu Hu, Zifan He, Neha Prakriya, Jason Cong, Yizhou Sun |  |
| 2583 |  |  [Unlocking Global Optimality in Bilevel Optimization: A Pilot Study](https://openreview.net/forum?id=2xvisNIfdw) |  | 0 |  | Quan Xiao, Tianyi Chen |  |
| 2584 |  |  [Selective Task Group Updates for Multi-Task Optimization](https://openreview.net/forum?id=EdNSQHaaMR) |  | 0 |  | Wooseong Jeong, KukJin Yoon |  |
| 2585 |  |  [Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting](https://openreview.net/forum?id=xgQfWbV6Ey) |  | 0 |  | Zilong Wang, Zifeng Wang, Long T. Le, Huaixiu Steven Zheng, Swaroop Mishra, Vincent Perot, Yuwei Zhang, Anush Mattapalli, Ankur Taly, Jingbo Shang, ChenYu Lee, Tomas Pfister |  |
| 2586 |  |  [Algorithmic Stability Based Generalization Bounds for Adversarial Training](https://openreview.net/forum?id=2GwMazl9ND) |  | 0 |  | Runzhi Tian, Yongyi Mao |  |
| 2587 |  |  [Tracking objects that change in appearance with phase synchrony](https://openreview.net/forum?id=m2gVfgWYDO) |  | 0 |  | Sabine Muzellec, Drew Linsley, Alekh Karkada Ashok, Ennio Mingolla, Girik Malik, Rufin VanRullen, Thomas Serre |  |
| 2588 |  |  [GSE: Group-wise Sparse and Explainable Adversarial Attacks](https://openreview.net/forum?id=d54fIsAbff) |  | 0 |  | Shpresim Sadiku, Moritz Wagner, Sebastian Pokutta |  |
| 2589 |  |  [CoTFormer: A Chain of Thought Driven Architecture with Budget-Adaptive Computation Cost at Inference](https://openreview.net/forum?id=7igPXQFupX) |  | 0 |  | Amirkeivan Mohtashami, Matteo Pagliardini, Martin Jaggi |  |
| 2590 |  |  [Adaptive Pruning of Pretrained Transformer via Differential Inclusions](https://openreview.net/forum?id=WA84oMWHaH) |  | 0 |  | Yizhuo Ding, Ke Fan, Yikai Wang, Xinwei Sun, Yanwei Fu |  |
| 2591 |  |  [Convergence of Distributed Adaptive Optimization with Local Updates](https://openreview.net/forum?id=VNg7srnvD9) |  | 0 |  | Ziheng Cheng, Margalit Glasgow |  |
| 2592 |  |  [Learning a Neural Solver for Parametric PDEs to Enhance Physics-Informed Methods](https://openreview.net/forum?id=jqVj8vCQsT) |  | 0 |  | Lise Le Boudec, Emmanuel de Bézenac, Louis Serrano, Ramon Daniel RegueiroEspino, Yuan Yin, Patrick Gallinari |  |
| 2593 |  |  [Theory on Score-Mismatched Diffusion Models and Zero-Shot Conditional Samplers](https://openreview.net/forum?id=nWT6LxbuGi) |  | 0 |  | Yuchen Liang, Peizhong Ju, Yingbin Liang, Ness B. Shroff |  |
| 2594 |  |  [Towards Interpreting Visual Information Processing in Vision-Language Models](https://openreview.net/forum?id=chanJGoa7f) |  | 0 |  | Clement Neo, Luke Ong, Philip Torr, Mor Geva, David Krueger, Fazl Barez |  |
| 2595 |  |  [Adaptive Q-Network: On-the-fly Target Selection for Deep Reinforcement Learning](https://openreview.net/forum?id=leACdxBEgv) |  | 0 |  | Théo Vincent, Fabian Wahren, Jan Peters, Boris Belousov, Carlo D'Eramo |  |
| 2596 |  |  [Adversarial Attacks on Data Attribution](https://openreview.net/forum?id=oJgIRwkIUB) |  | 0 |  | Xinhe Wang, Pingbang Hu, Junwei Deng, Jiaqi W. Ma |  |
| 2597 |  |  [Not All Prompts Are Made Equal: Prompt-based Pruning of Text-to-Image Diffusion Models](https://openreview.net/forum?id=3BhZCfJ73Y) |  | 0 |  | Alireza Ganjdanesh, Reza Shirkavand, Shangqian Gao, Heng Huang |  |
| 2598 |  |  [F-Fidelity: A Robust Framework for Faithfulness Evaluation of Explainable AI](https://openreview.net/forum?id=X0r4BN50Dv) |  | 0 |  | Xu Zheng, Farhad Shirani, Zhuomin Chen, Chaohao Lin, Wei Cheng, Wenbo Guo, Dongsheng Luo |  |
| 2599 |  |  [Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with Gaussian Distribution](https://openreview.net/forum?id=CvttyK4XzV) |  | 0 |  | Haiyan Zhao, Heng Zhao, Bo Shen, Ali Payani, Fan Yang, Mengnan Du |  |
| 2600 |  |  [InstructRAG: Instructing Retrieval-Augmented Generation via Self-Synthesized Rationales](https://openreview.net/forum?id=P1qhkp8gQT) |  | 0 |  | Zhepei Wei, WeiLin Chen, Yu Meng |  |
| 2601 |  |  [MixMax: Distributional Robustness in Function Space via Optimal Data Mixtures](https://openreview.net/forum?id=dIkpHooa2D) |  | 0 |  | Anvith Thudi, Chris J. Maddison |  |
| 2602 |  |  [Digi-Q: Learning VLM Q-Value Functions for Training Device-Control Agents](https://openreview.net/forum?id=CjfQssZtAb) |  | 0 |  | Hao Bai, Yifei Zhou, Li Erran Li, Sergey Levine, Aviral Kumar |  |
| 2603 |  |  [Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains](https://openreview.net/forum?id=JtGPIZpOrz) |  | 0 |  | Vighnesh Subramaniam, Yilun Du, Joshua B. Tenenbaum, Antonio Torralba, Shuang Li, Igor Mordatch |  |
| 2604 |  |  [MeToken: Uniform Micro-environment Token Boosts Post-Translational Modification Prediction](https://openreview.net/forum?id=noUF58SMra) |  | 0 |  | Cheng Tan, Zhenxiao Cao, Zhangyang Gao, Lirong Wu, Siyuan Li, Yufei Huang, Jun Xia, Bozhen Hu, Stan Z. Li |  |
| 2605 |  |  [GMValuator: Similarity-based Data Valuation for Generative Models](https://openreview.net/forum?id=WncnpvJk83) |  | 0 |  | Jiaxi Yang, Wenlong Deng, Benlin Liu, Yangsibo Huang, James Zou, Xiaoxiao Li |  |
| 2606 |  |  [Implicit In-context Learning](https://openreview.net/forum?id=G7u4ue6ncT) |  | 0 |  | Zhuowei Li, Zihao Xu, Ligong Han, Yunhe Gao, Song Wen, Di Liu, Hao Wang, Dimitris N. Metaxas |  |
| 2607 |  |  [UTILITY: Utilizing Explainable Reinforcement Learning to Improve Reinforcement Learning](https://openreview.net/forum?id=Tk1VQDadfL) |  | 0 |  | Shicheng Liu, Minghui Zhu |  |
| 2608 |  |  [MS-Diffusion: Multi-subject Zero-shot Image Personalization with Layout Guidance](https://openreview.net/forum?id=PJqP0wyQek) |  | 0 |  | Xierui Wang, Siming Fu, Qihan Huang, Wanggui He, Hao Jiang |  |
| 2609 |  |  [Generalization Bounds and Model Complexity for Kolmogorov-Arnold Networks](https://openreview.net/forum?id=q5zMyAUhGx) |  | 0 |  | Xianyang Zhang, Huijuan Zhou |  |
| 2610 |  |  [Backdooring Vision-Language Models with Out-Of-Distribution Data](https://openreview.net/forum?id=tZozeR3VV7) |  | 0 |  | Weimin Lyu, Jiachen Yao, Saumya Gupta, Lu Pang, Tao Sun, Lingjie Yi, Lijie Hu, Haibin Ling, Chao Chen |  |
| 2611 |  |  [Towards Homogeneous Lexical Tone Decoding from Heterogeneous Intracranial Recordings](https://openreview.net/forum?id=cWEfRkYj46) |  | 0 |  | Di Wu, Siyuan Li, Chen Feng, Lu Cao, Yue Zhang, Jie Yang, Mohamad Sawan |  |
| 2612 |  |  [Minimal Variance Model Aggregation: A principled, non-intrusive, and versatile integration of black box models](https://openreview.net/forum?id=grM2Yv49cI) |  | 0 |  | Théo Bourdais, Houman Owhadi |  |
| 2613 |  |  [Towards Continuous Reuse of Graph Models via Holistic Memory Diversification](https://openreview.net/forum?id=Pbz4i7B0B4) |  | 0 |  | Ziyue Qiao, Junren Xiao, Qingqiang Sun, Meng Xiao, Xiao Luo, Hui Xiong |  |
| 2614 |  |  [Multi-modal brain encoding models for multi-modal stimuli](https://openreview.net/forum?id=0dELcFHig2) |  | 0 |  | Subba Reddy Oota, Khushbu Pahwa, Mounika Marreddy, Maneesh Kumar Singh, Manish Gupta, Bapi Raju Surampudi |  |
| 2615 |  |  [Unlocking Efficient, Scalable, and Continual Knowledge Editing with Basis-Level Representation Fine-Tuning](https://openreview.net/forum?id=PITFO1ddeh) |  | 0 |  | Tianci Liu, Ruirui Li, Yunzhe Qi, Hui Liu, Xianfeng Tang, Tianqi Zheng, Qingyu Yin, Monica Xiao Cheng, Jun Huan, Haoyu Wang, Jing Gao |  |
| 2616 |  |  [Contextual Document Embeddings](https://openreview.net/forum?id=Wqsk3FbD6D) |  | 0 |  | John Xavier Morris, Alexander M. Rush |  |
| 2617 |  |  [Do LLMs have Consistent Values?](https://openreview.net/forum?id=8zxGruuzr9) |  | 0 |  | Naama Rozen, Liat Bezalel, Gal Elidan, Amir Globerson, Ella Daniel |  |
| 2618 |  |  [AdaManip: Adaptive Articulated Object Manipulation Environments and Policy Learning](https://openreview.net/forum?id=Luss2sa0vc) |  | 0 |  | Yuanfei Wang, Xiaojie Zhang, Ruihai Wu, Yu Li, Yan Shen, Mingdong Wu, Zhaofeng He, Yizhou Wang, Hao Dong |  |
| 2619 |  |  [Range, not Independence, Drives Modularity in Biologically Inspired Representations](https://openreview.net/forum?id=BxQkDog4ti) |  | 0 |  | Will Dorrell, Kyle Hsu, Luke Hollingsworth, Jin Hwa Lee, Jiajun Wu, Chelsea Finn, Peter E. Latham, Timothy Edward John Behrens, James C. R. Whittington |  |
| 2620 |  |  [Measuring memorization in RLHF for code completion](https://openreview.net/forum?id=Tg8RLxpMDu) |  | 0 |  | Jamie Hayes, Ilia Shumailov, William P. Porter, Aneesh Pappu |  |
| 2621 |  |  [Towards Certification of Uncertainty Calibration under Adversarial Attacks](https://openreview.net/forum?id=uuPkll6i7m) |  | 0 |  | Cornelius Emde, Francesco Pinto, Thomas Lukasiewicz, Philip Torr, Adel Bibi |  |
| 2622 |  |  [A Policy-Gradient Approach to Solving Imperfect-Information Games with Best-Iterate Convergence](https://openreview.net/forum?id=ZW4MRZrmSA) |  | 0 |  | Mingyang Liu, Gabriele Farina, Asuman E. Ozdaglar |  |
| 2623 |  |  [Improved Techniques for Optimization-Based Jailbreaking on Large Language Models](https://openreview.net/forum?id=e9yfCY7Q3U) |  | 0 |  | Xiaojun Jia, Tianyu Pang, Chao Du, Yihao Huang, Jindong Gu, Yang Liu, Xiaochun Cao, Min Lin |  |
| 2624 |  |  [Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient Transformer Models](https://openreview.net/forum?id=T26f9z2rEe) |  | 0 |  | Yongxin Guo, Zhenglin Cheng, Xiaoying Tang, Zhaopeng Tu, Tao Lin |  |
| 2625 |  |  [Agent-to-Sim: Learning Interactive Behavior Models from Casual Longitudinal Videos](https://openreview.net/forum?id=y80D4IojuY) |  | 0 |  | Gengshan Yang, Andrea Bajcsy, Shunsuke Saito, Angjoo Kanazawa |  |
| 2626 |  |  [Explaining Modern Gated-Linear RNNs via a Unified Implicit Attention Formulation](https://openreview.net/forum?id=wnT8bfJCDx) |  | 0 |  | Itamar Zimerman, Ameen Ali, Lior Wolf |  |
| 2627 |  |  [Zero-shot Model-based Reinforcement Learning using Large Language Models](https://openreview.net/forum?id=uZFXpPrwSh) |  | 0 |  | Abdelhakim Benechehab, Youssef Attia El Hili, Ambroise Odonnat, Oussama Zekri, Albert Thomas, Giuseppe Paolo, Maurizio Filippone, Ievgen Redko, Balázs Kégl |  |
| 2628 |  |  [High-quality Text-to-3D Character Generation with SparseCubes and Sparse Transformers](https://openreview.net/forum?id=rfeksadZox) |  | 0 |  | Jiachen Qian, Hongye Yang, Shuang Wu, Jingxi Xu, Feihu Zhang |  |
| 2629 |  |  [Implicit Neural Surface Deformation with Explicit Velocity Fields](https://openreview.net/forum?id=sYAFiHP6qr) |  | 0 |  | Lu Sang, Zehranaz Canfes, Dongliang Cao, Florian Bernard, Daniel Cremers |  |
| 2630 |  |  [Multi-Label Node Classification with Label Influence Propagation](https://openreview.net/forum?id=3X3LuwzZrl) |  | 0 |  | Yifei Sun, Zemin Liu, Bryan Hooi, Yang Yang, Rizal Fathony, Jia Chen, Bingsheng He |  |
| 2631 |  |  [Interpreting the Second-Order Effects of Neurons in CLIP](https://openreview.net/forum?id=GPDcvoFGOL) |  | 0 |  | Yossi Gandelsman, Alexei A. Efros, Jacob Steinhardt |  |
| 2632 |  |  [ClawMachine: Learning to Fetch Visual Tokens for Referential Comprehension](https://openreview.net/forum?id=TOtk9dTYGG) |  | 0 |  | Tianren Ma, Lingxi Xie, Yunjie Tian, Boyu Yang, Qixiang Ye |  |
| 2633 |  |  [Diff3DS: Generating View-Consistent 3D Sketch via Differentiable Curve Rendering](https://openreview.net/forum?id=aIMi2lOKIn) |  | 0 |  | Yibo Zhang, Lihong Wang, Changqing Zou, Tieru Wu, Rui Ma |  |
| 2634 |  |  [InfoGS: Efficient Structure-Aware 3D Gaussians via Lightweight Information Shaping](https://openreview.net/forum?id=Pj2qEVzufH) |  | 0 |  | Yunchao Zhang, Guandao Yang, Leonidas J. Guibas, Yanchao Yang |  |
| 2635 |  |  [Bayesian Image Regression with Soft-thresholded Conditional Autoregressive Prior](https://openreview.net/forum?id=rnL3OafDdw) |  | 0 |  | Yuliang Xu, Jian Kang |  |
| 2636 |  |  [TimeKAN: KAN-based Frequency Decomposition Learning Architecture for Long-term Time Series Forecasting](https://openreview.net/forum?id=wTLc79YNbh) |  | 0 |  | Songtao Huang, Zhen Zhao, Can Li, Lei Bai |  |
| 2637 |  |  [Preference Elicitation for Offline Reinforcement Learning](https://openreview.net/forum?id=2pJpFtdVNe) |  | 0 |  | Alizée Pace, Bernhard Schölkopf, Gunnar Rätsch, Giorgia Ramponi |  |
| 2638 |  |  [Gaussian-Det: Learning Closed-Surface Gaussians for 3D Object Detection](https://openreview.net/forum?id=DtFCIfvAFc) |  | 0 |  | Hongru Yan, Yu Zheng, Yueqi Duan |  |
| 2639 |  |  [RFMamba: Frequency-Aware State Space Model for RF-Based Human-Centric Perception](https://openreview.net/forum?id=lG9fjBLb6d) |  | 0 |  | Rui Zhang, Ruixu Geng, Yadong Li, Ruiyuan Song, Hanqin Gong, Dongheng Zhang, Yang Hu, Yan Chen |  |
| 2640 |  |  [Feature Averaging: An Implicit Bias of Gradient Descent Leading to Non-Robustness in Neural Networks](https://openreview.net/forum?id=zPHra4V5Mc) |  | 0 |  | Binghui Li, Zhixuan Pan, Kaifeng Lyu, Jian Li |  |
| 2641 |  |  [GOAL: A Generalist Combinatorial Optimization Agent Learner](https://openreview.net/forum?id=z2z9suDRjw) |  | 0 |  | Darko Drakulic, Sofia Michel, JeanMarc Andreoli |  |
| 2642 |  |  [Optimizing importance weighting in the presence of sub-population shifts](https://openreview.net/forum?id=j4gzziSUr0) |  | 0 |  | Floris Holstege, Bram Wouters, Noud P. A. van Giersbergen, Cees Diks |  |
| 2643 |  |  [Auto-GDA: Automatic Domain Adaptation for Efficient Grounding Verification in Retrieval-Augmented Generation](https://openreview.net/forum?id=w5ZtXOzMeJ) |  | 0 |  | Tobias Leemann, Periklis Petridis, Giuseppe Vietri, Dionysis Manousakas, Aaron Roth, Sergül Aydöre |  |
| 2644 |  |  [Cross the Gap: Exposing the Intra-modal Misalignment in CLIP via Modality Inversion](https://openreview.net/forum?id=VVVfuIcmKR) |  | 0 |  | Marco Mistretta, Alberto Baldrati, Lorenzo Agnolucci, Marco Bertini, Andrew D. Bagdanov |  |
| 2645 |  |  [Cross-Domain Offline Policy Adaptation with Optimal Transport and Dataset Constraint](https://openreview.net/forum?id=LRrbD8EZJl) |  | 0 |  | Jiafei Lyu, Mengbei Yan, Zhongjian Qiao, Runze Liu, Xiaoteng Ma, Deheng Ye, Jingwen Yang, Zongqing Lu, Xiu Li |  |
| 2646 |  |  [BadRobot: Jailbreaking Embodied LLM Agents in the Physical World](https://openreview.net/forum?id=ei3qCntB66) |  | 0 |  | Hangtao Zhang, Chenyu Zhu, Xianlong Wang, Ziqi Zhou, Changgan Yin, Minghui Li, Lulu Xue, Yichen Wang, Shengshan Hu, Aishan Liu, Peijin Guo, Leo Yu Zhang |  |
| 2647 |  |  [Input Space Mode Connectivity in Deep Neural Networks](https://openreview.net/forum?id=3qeOy7HwUT) |  | 0 |  | Jakub Vrábel, Ori ShemUr, Yaron Oz, David Krueger |  |
| 2648 |  |  [CircuitFusion: Multimodal Circuit Representation Learning for Agile Chip Design](https://openreview.net/forum?id=rbnf7oe6JQ) |  | 0 |  | Wenji Fang, Shang Liu, Jing Wang, Zhiyao Xie |  |
| 2649 |  |  [Accessing Vision Foundation Models via ImageNet-1K](https://openreview.net/forum?id=LC6ZtQV6u2) |  | 0 |  | Yitian Zhang, Xu Ma, Yue Bai, Huan Wang, Yun Fu |  |
| 2650 |  |  [Binary Losses for Density Ratio Estimation](https://openreview.net/forum?id=562B7aLi5X) |  | 0 |  | Werner Zellinger |  |
| 2651 |  |  [Sports-Traj: A Unified Trajectory Generation Model for Multi-Agent Movement in Sports](https://openreview.net/forum?id=9aTZf71uiD) |  | 0 |  | Yi Xu, Yun Fu |  |
| 2652 |  |  [SWEb: A Large Web Dataset for the Scandinavian Languages](https://openreview.net/forum?id=vhPE3PtTgC) |  | 0 |  | Tobias Norlund, Tim Isbister, Amaru Cuba Gyllensten, Paul Gabriel dos Santos, Danila Petrelli, Ariel Ekgren, Magnus Sahlgren |  |
| 2653 |  |  [RTDiff: Reverse Trajectory Synthesis via Diffusion for Offline Reinforcement Learning](https://openreview.net/forum?id=0FK6tzqV76) |  | 0 |  | Qianlan Yang, YuXiong Wang |  |
| 2654 |  |  [Generalization, Expressivity, and Universality of Graph Neural Networks on Attributed Graphs](https://openreview.net/forum?id=qKgd7RaAem) |  | 0 |  | Levi Rauchwerger, Stefanie Jegelka, Ron Levie |  |
| 2655 |  |  [I-Con: A Unifying Framework for Representation Learning](https://openreview.net/forum?id=WfaQrKCr4X) |  | 0 |  | Shaden Naif Alshammari, John R. Hershey, Axel Feldmann, William T. Freeman, Mark Hamilton |  |
| 2656 |  |  [Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG](https://openreview.net/forum?id=oU3tpaR8fm) |  | 0 |  | Bowen Jin, Jinsung Yoon, Jiawei Han, Sercan Ö. Arik |  |
| 2657 |  |  [Local convergence of simultaneous min-max algorithms to differential equilibrium on Riemannian manifold](https://openreview.net/forum?id=ROYSNn3vvB) |  | 0 |  | Sixin Zhang |  |
| 2658 |  |  [Explanations of GNN on Evolving Graphs via Axiomatic Layer edges](https://openreview.net/forum?id=pXN8T5RwNN) |  | 0 |  | Yazheng Liu, Sihong Xie |  |
| 2659 |  |  [Deep Random Features for Scalable Interpolation of Spatiotemporal Data](https://openreview.net/forum?id=OD1MV7vf41) |  | 0 |  | Weibin Chen, Azhir Mahmood, Michel Tsamados, So Takao |  |
| 2660 |  |  [MarS: a Financial Market Simulation Engine Powered by Generative Foundation Model](https://openreview.net/forum?id=Yqk7EyT52H) |  | 0 |  | Junjie Li, Yang Liu, Weiqing Liu, Shikai Fang, Lewen Wang, Chang Xu, Jiang Bian |  |
| 2661 |  |  [Reading Your Heart: Learning ECG Words and Sentences via Pre-training ECG Language Model](https://openreview.net/forum?id=6Hz1Ko087B) |  | 0 |  | Jiarui Jin, Haoyu Wang, Hongyan Li, Jun Li, Jiahui Pan, Shenda Hong |  |
| 2662 |  |  [A New Perspective on Shampoo's Preconditioner](https://openreview.net/forum?id=c6zI3Cp8c6) |  | 0 |  | Depen Morwani, Itai Shapira, Nikhil Vyas, Eran Malach, Sham M. Kakade, Lucas Janson |  |
| 2663 |  |  [Text2PDE: Latent Diffusion Models for Accessible Physics Simulation](https://openreview.net/forum?id=Nb3a8aUGfj) |  | 0 |  | Anthony Y. Zhou, Zijie Li, Michael Schneier, John R. Buchanan Jr., Amir Barati Farimani |  |
| 2664 |  |  [Adversarial Training Can Provably Improve Robustness: Theoretical Analysis of Feature Learning Process Under Structured Data](https://openreview.net/forum?id=inLUnCpDIB) |  | 0 |  | Binghui Li, Yuanzhi Li |  |
| 2665 |  |  [Unleashing the Potential of Vision-Language Pre-Training for 3D Zero-Shot Lesion Segmentation via Mask-Attribute Alignment](https://openreview.net/forum?id=QG31By6S6w) |  | 0 |  | Yankai Jiang, Wenhui Lei, Xiaofan Zhang, Shaoting Zhang |  |
| 2666 |  |  [KLay: Accelerating Arithmetic Circuits for Neurosymbolic AI](https://openreview.net/forum?id=Zes7Wyif8G) |  | 0 |  | Jaron Maene, Vincent Derkinderen, Pedro Zuidberg Dos Martires |  |
| 2667 |  |  [Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs](https://openreview.net/forum?id=NS1G1Uhny3) |  | 0 |  | Jonas Hübotter, Sascha Bongni, Ido Hakimi, Andreas Krause |  |
| 2668 |  |  [Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification](https://openreview.net/forum?id=hzVpZDrW73) |  | 0 |  | Wenxuan Huang, Zijie Zhai, Yunhang Shen, Shaosheng Cao, Fei Zhao, Xiangfeng Xu, Zheyu Ye, Shaohui Lin |  |
| 2669 |  |  [REBIND: Enhancing Ground-state Molecular Conformation Prediction via Force-Based Graph Rewiring](https://openreview.net/forum?id=WNIEr5kydF) |  | 0 |  | Taewon Kim, Hyunjin Seo, Sungsoo Ahn, Eunho Yang |  |
| 2670 |  |  [Improved Sampling Of Diffusion Models In Fluid Dynamics With Tweedie's Formula](https://openreview.net/forum?id=0FbzC7B9xI) |  | 0 |  | Youssef Shehata, Benjamin J. Holzschuh, Nils Thuerey |  |
| 2671 |  |  [CATCH: Channel-Aware Multivariate Time Series Anomaly Detection via Frequency Patching](https://openreview.net/forum?id=m08aK3xxdJ) |  | 0 |  | Xingjian Wu, Xiangfei Qiu, Zhengyu Li, Yihang Wang, Jilin Hu, Chenjuan Guo, Hui Xiong, Bin Yang |  |
| 2672 |  |  [Swift Hydra: Self-Reinforcing Generative Framework for Anomaly Detection with Multiple Mamba Models](https://openreview.net/forum?id=P7t2niLbvw) |  | 0 |  | Nguyen Hoang Khoi Do, Truc Nguyen, Malik Hassanaly, Raed Alharbi, Jung Taek Seo, My T. Thai |  |
| 2673 |  |  [Select before Act: Spatially Decoupled Action Repetition for Continuous Control](https://openreview.net/forum?id=PDgZ3rvqHn) |  | 0 |  | Buqing Nie, Yangqing Fu, Yue Gao |  |
| 2674 |  |  [Adversaries With Incentives: A Strategic Alternative to Adversarial Robustness](https://openreview.net/forum?id=U9j40EohfY) |  | 0 |  | Maayan Ehrenberg, Roy Ganz, Nir Rosenfeld |  |
| 2675 |  |  [BOND: Aligning LLMs with Best-of-N Distillation](https://openreview.net/forum?id=0tAXMiSufG) |  | 0 |  | Pier Giuseppe Sessa, Robert DadashiTazehozi, Léonard Hussenot, Johan Ferret, Nino Vieillard, Alexandre Ramé, Bobak Shahriari, Sarah Perrin, Abram L. Friesen, Geoffrey Cideron, Sertan Girgin, Piotr Stanczyk, Andrea Michi, Danila Sinopalnikov, Sabela Ramos Garea, Amélie Héliou, Aliaksei Severyn, Matthew Hoffman, Nikola Momchev, Olivier Bachem |  |
| 2676 |  |  [Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures](https://openreview.net/forum?id=2J18i8T0oI) |  | 0 |  | Junxuan Wang, Xuyang Ge, Wentao Shu, Qiong Tang, Yunhua Zhou, Zhengfu He, Xipeng Qiu |  |
| 2677 |  |  [Grammar Reinforcement Learning: path and cycle counting in graphs with a Context-Free Grammar and Transformer approach](https://openreview.net/forum?id=yEox25xAED) |  | 0 |  | Jason Piquenot, Maxime Berar, Romain Raveaux, Pierre Héroux, JeanYves Ramel, Sébastien Adam |  |
| 2678 |  |  [Dist Loss: Enhancing Regression in Few-Shot Region through Distribution Distance Constraint](https://openreview.net/forum?id=YeSxbRrDRl) |  | 0 |  | Guangkun Nie, Gongzheng Tang, Shenda Hong |  |
| 2679 |  |  [Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs](https://openreview.net/forum?id=f9w89OY2cp) |  | 0 |  | Zhaowei Zhang, Fengshuo Bai, Qizhi Chen, Chengdong Ma, Mingzhi Wang, Haoran Sun, Zilong Zheng, Yaodong Yang |  |
| 2680 |  |  [Revisiting a Design Choice in Gradient Temporal Difference Learning](https://openreview.net/forum?id=38BBWrXUhP) |  | 0 |  | Xiaochi Qian, Shangtong Zhang |  |
| 2681 |  |  [Pairwise Elimination with Instance-Dependent Guarantees for Bandits with Cost Subsidy](https://openreview.net/forum?id=eB7T1bqthA) |  | 0 |  | Ishank Juneja, Carlee JoeWong, Osman Yagan |  |
| 2682 |  |  [A Quantum Circuit-Based Compression Perspective for Parameter-Efficient Learning](https://openreview.net/forum?id=bB0OKNpznp) |  | 0 |  | ChenYu Liu, ChaoHan Huck Yang, HsiSheng Goan, MinHsiu Hsieh |  |
| 2683 |  |  [Leave-One-Out Stable Conformal Prediction](https://openreview.net/forum?id=Bt1vnCnAVS) |  | 0 |  | Kiljae Lee, Yuan Zhang |  |
| 2684 |  |  [Encryption-Friendly LLM Architecture](https://openreview.net/forum?id=pbre0HKsfE) |  | 0 |  | Donghwan Rho, Taeseong Kim, Minje Park, Jung Woo Kim, Hyunsik Chae, Ernest K. Ryu, Jung Hee Cheon |  |
| 2685 |  |  [Improved Training Technique for Latent Consistency Models](https://openreview.net/forum?id=PQjZes6vFV) |  | 0 |  | Quan Dao, Khanh Doan, Di Liu, Trung Le, Dimitris N. Metaxas |  |
| 2686 |  |  [On the Crucial Role of Initialization for Matrix Factorization](https://openreview.net/forum?id=YTEwJaBdh0) |  | 0 |  | Bingcong Li, Liang Zhang, Aryan Mokhtari, Niao He |  |
| 2687 |  |  [Boosting Latent Diffusion with Perceptual Objectives](https://openreview.net/forum?id=y4DtzADzd1) |  | 0 |  | Tariq Berrada, Pietro Astolfi, Melissa Hall, Marton Havasi, Yohann Benchetrit, Adriana RomeroSoriano, Karteek Alahari, Michal Drozdzal, Jakob Verbeek |  |
| 2688 |  |  [Dynamic Negative Guidance of Diffusion Models](https://openreview.net/forum?id=6p74UyAdLa) |  | 0 |  | Felix Koulischer, Johannes Deleu, Gabriel Raya, Thomas Demeester, Luca Ambrogioni |  |
| 2689 |  |  [Solving Differential Equations with Constrained Learning](https://openreview.net/forum?id=5KqveQdXiZ) |  | 0 |  | Viggo Moro, Luiz F. O. Chamon |  |
| 2690 |  |  [ClassDiffusion: More Aligned Personalization Tuning with Explicit Class Guidance](https://openreview.net/forum?id=iTm4H6N4aG) |  | 0 |  | Jiannan Huang, Jun Hao Liew, Hanshu Yan, Yuyang Yin, Yao Zhao, Humphrey Shi, Yunchao Wei |  |
| 2691 |  |  [AtomSurf: Surface Representation for Learning on Protein Structures](https://openreview.net/forum?id=ARQIJXFcTH) |  | 0 |  | Vincent Mallet, Yangyang Miao, Souhaib Attaiki, Bruno Correia, Maks Ovsjanikov |  |
| 2692 |  |  [Adversarial Training for Defense Against Label Poisoning Attacks](https://openreview.net/forum?id=UlpkHciYQP) |  | 0 |  | Melis Ilayda Bal, Volkan Cevher, Michael Muehlebach |  |
| 2693 |  |  [PersonalLLM: Tailoring LLMs to Individual Preferences](https://openreview.net/forum?id=2R7498e2Tx) |  | 0 |  | Thomas P. Zollo, Andrew Wei Tung Siah, Naimeng Ye, Ang Li, Hongseok Namkoong |  |
| 2694 |  |  [Streamlining Prediction in Bayesian Deep Learning](https://openreview.net/forum?id=pW387D5OUN) |  | 0 |  | Rui Li, Marcus Klasson, Arno Solin, Martin Trapp |  |
| 2695 |  |  [UniCO: On Unified Combinatorial Optimization via Problem Reduction to Matrix-Encoded General TSP](https://openreview.net/forum?id=yEwakMNIex) |  | 0 |  | Wenzheng Pan, Hao Xiong, Jiale Ma, Wentao Zhao, Yang Li, Junchi Yan |  |
| 2696 |  |  [Learning to Communicate Through Implicit Communication Channels](https://openreview.net/forum?id=wm5wwAdiEt) |  | 0 |  | Han Wang, Binbin Chen, Tieying Zhang, Baoxiang Wang |  |
| 2697 |  |  [Mask in the Mirror: Implicit Sparsification](https://openreview.net/forum?id=U47ymTS3ut) |  | 0 |  | Tom Jacobs, Rebekka Burkholz |  |
| 2698 |  |  [Shh, don't say that! Domain Certification in LLMs](https://openreview.net/forum?id=F64wTvQBum) |  | 0 |  | Cornelius Emde, Alasdair Paren, Preetham Arvind, Maxime Guillaume Kayser, Tom Rainforth, Thomas Lukasiewicz, Philip Torr, Adel Bibi |  |
| 2699 |  |  [Systematic Outliers in Large Language Models](https://openreview.net/forum?id=rLX7Vyyzus) |  | 0 |  | Yongqi An, Xu Zhao, Tao Yu, Ming Tang, Jinqiao Wang |  |
| 2700 |  |  [Implicit Search via Discrete Diffusion: A Study on Chess](https://openreview.net/forum?id=A9y3LFX4ds) |  | 0 |  | Jiacheng Ye, Zhenyu Wu, Jiahui Gao, Zhiyong Wu, Xin Jiang, Zhenguo Li, Lingpeng Kong |  |
| 2701 |  |  [Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning](https://openreview.net/forum?id=NRYgUzSPZz) |  | 0 |  | Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, Lingpeng Kong |  |
| 2702 |  |  [RGB-Event ISP: The Dataset and Benchmark](https://openreview.net/forum?id=BqtoARyz7Y) |  | 0 |  | Yunfan Lu, Yanlin Qian, Ziyang Rao, Junren Xiao, Liming Chen, Hui Xiong |  |
| 2703 |  |  [Diff-Prompt: Diffusion-Driven Prompt Generator with Mask Supervision](https://openreview.net/forum?id=LfghnrSJNg) |  | 0 |  | Weicai Yan, Wang Lin, Zirun Guo, Ye Wang, Fangming Feng, Xiaoda Yang, Zehan Wang, Tao Jin |  |
| 2704 |  |  [Sparse Autoencoders Reveal Temporal Difference Learning in Large Language Models](https://openreview.net/forum?id=2tIyA5cri8) |  | 0 |  | Can Demircan, Tankred Saanum, Akshay Kumar Jagadish, Marcel Binz, Eric Schulz |  |
| 2705 |  |  [FairDen: Fair Density-Based Clustering](https://openreview.net/forum?id=aPHHhnZktB) |  | 0 |  | Lena Krieger, Anna Beer, Pernille Matthews, Anneka Myrup Thiesson, Ira Assent |  |
| 2706 |  |  [Trajectory attention for fine-grained video motion control](https://openreview.net/forum?id=2z1HT5lw5M) |  | 0 |  | Zeqi Xiao, Wenqi Ouyang, Yifan Zhou, Shuai Yang, Lei Yang, Jianlou Si, Xingang Pan |  |
| 2707 |  |  [Revolutionizing EMCCD Denoising through a Novel Physics-Based Learning Framework for Noise Modeling](https://openreview.net/forum?id=vmulbBDCan) |  | 0 |  | Haiyang Jiang, Tetsuichi Wazawa, Imari Sato, Takeharu Nagai, Yinqiang Zheng |  |
| 2708 |  |  [SAM-CP: Marrying SAM with Composable Prompts for Versatile Segmentation](https://openreview.net/forum?id=UiEjzBRYeI) |  | 0 |  | Pengfei Chen, Lingxi Xie, Xinyue Huo, Xuehui Yu, Xiaopeng Zhang, Yingfei Sun, Zhenjun Han, Qi Tian |  |
| 2709 |  |  [Proactive Agent: Shifting LLM Agents from Reactive Responses to Active Assistance](https://openreview.net/forum?id=sRIU6k2TcU) |  | 0 |  | Yaxi Lu, Shenzhi Yang, Cheng Qian, Guirong Chen, Qinyu Luo, Yesai Wu, Huadong Wang, Xin Cong, Zhong Zhang, Yankai Lin, Weiwen Liu, Yasheng Wang, Zhiyuan Liu, Fangming Liu, Maosong Sun |  |
| 2710 |  |  [Can Generative AI Solve Your In-Context Learning Problem? A Martingale Perspective](https://openreview.net/forum?id=bcynT7s2du) |  | 0 |  | Andrew Jesson, Nicolas BeltranVelez, David M. Blei |  |
| 2711 |  |  [Conditional Testing based on Localized Conformal p-values](https://openreview.net/forum?id=Ip6UwB35uT) |  | 0 |  | Xiaoyang Wu, Lin Lu, Zhaojun Wang, Changliang Zou |  |
| 2712 |  |  [Text4Seg: Reimagining Image Segmentation as Text Generation](https://openreview.net/forum?id=vkakKdznFS) |  | 0 |  | Mengcheng Lan, Chaofeng Chen, Yue Zhou, Jiaxing Xu, Yiping Ke, Xinjiang Wang, Litong Feng, Wayne Zhang |  |
| 2713 |  |  [Diffusing to the Top: Boost Graph Neural Networks with Minimal Hyperparameter Tuning](https://openreview.net/forum?id=D756s2YQ6b) |  | 0 |  | Lequan Lin, Dai Shi, Andi Han, Zhiyong Wang, Junbin Gao |  |
| 2714 |  |  [Towards Robust Multimodal Open-set Test-time Adaptation via Adaptive Entropy-aware Optimization](https://openreview.net/forum?id=hj323oR3rw) |  | 0 |  | Hao Dong, Eleni N. Chatzi, Olga Fink |  |
| 2715 |  |  [On the Feature Learning in Diffusion Models](https://openreview.net/forum?id=JjdU6ysnCr) |  | 0 |  | Andi Han, Wei Huang, Yuan Cao, Difan Zou |  |
| 2716 |  |  [Deep Kernel Relative Test for Machine-generated Text Detection](https://openreview.net/forum?id=z9j7wctoGV) |  | 0 |  | Yiliao Song, Zhenqiao Yuan, Shuhai Zhang, Zhen Fang, Jun Yu, Feng Liu |  |
| 2717 |  |  [LDAdam: Adaptive Optimization from Low-Dimensional Gradient Statistics](https://openreview.net/forum?id=Zkp1GuHerF) |  | 0 |  | Thomas Robert, Mher Safaryan, IonutVlad Modoranu, Dan Alistarh |  |
| 2718 |  |  [Layerwise Recurrent Router for Mixture-of-Experts](https://openreview.net/forum?id=eWNEqdH0vk) |  | 0 |  | Zihan Qiu, Zeyu Huang, Shuang Cheng, Yizhi Zhou, Zili Wang, Ivan Titov, Jie Fu |  |
| 2719 |  |  [AutoUAD: Hyper-parameter Optimization for Unsupervised Anomaly Detection](https://openreview.net/forum?id=ErQPdaD5wJ) |  | 0 |  | Wei Dai, Jicong Fan |  |
| 2720 |  |  [Advancing Mathematical Reasoning in Language Models: The Impact of Problem-Solving Data, Data Synthesis Methods, and Training Stages](https://openreview.net/forum?id=GtpubstM1D) |  | 0 |  | Zui Chen, Tianqiao Liu, Tongqing, Mi Tian, Weiqi Luo, Zitao Liu |  |
| 2721 |  |  [DocMIA: Document-Level Membership Inference Attacks against DocVQA Models](https://openreview.net/forum?id=gNxvs5pUdu) |  | 0 |  | Khanh Nguyen, Raouf Kerkouche, Mario Fritz, Dimosthenis Karatzas |  |
| 2722 |  |  [Transformer-Squared: Self-adaptive LLMs](https://openreview.net/forum?id=dh4t9qmcvK) |  | 0 |  | Qi Sun, Edoardo Cetin, Yujin Tang |  |
| 2723 |  |  [Maintaining Structural Integrity in Parameter Spaces for Parameter Efficient Fine-tuning](https://openreview.net/forum?id=OALIb8oNfl) |  | 0 |  | Chongjie Si, Xuehui Wang, Xue Yang, Zhengqin Xu, Qingyun Li, Jifeng Dai, Yu Qiao, Xiaokang Yang, Wei Shen |  |
| 2724 |  |  [Mechanistic Permutability: Match Features Across Layers](https://openreview.net/forum?id=MDvecs7EvO) |  | 0 |  | Nikita Balagansky, Ian Maksimov, Daniil Gavrilov |  |
| 2725 |  |  [HALL-E: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis](https://openreview.net/forum?id=868masI331) |  | 0 |  | Yuto Nishimura, Takumi Hirose, Masanari Ohi, Hideki Nakayama, Nakamasa Inoue |  |
| 2726 |  |  [Zeroth-Order Policy Gradient for Reinforcement Learning from Human Feedback without Reward Inference](https://openreview.net/forum?id=cmYScmfu4Q) |  | 0 |  | Qining Zhang, Lei Ying |  |
| 2727 |  |  [Zigzag Diffusion Sampling: Diffusion Models Can Self-Improve via Self-Reflection](https://openreview.net/forum?id=MKvQH1ekeY) |  | 0 |  | Lichen Bai, Shitong Shao, Zikai Zhou, Zipeng Qi, Zhiqiang Xu, Haoyi Xiong, Zeke Xie |  |
| 2728 |  |  [Discrete Latent Plans via Semantic Skill Abstractions](https://openreview.net/forum?id=L66G39JrM4) |  | 0 |  | Haobin Jiang, Jiangxing Wang, Zongqing Lu |  |
| 2729 |  |  [Neuralized Markov Random Field for Interaction-Aware Stochastic Human Trajectory Prediction](https://openreview.net/forum?id=r3cEOVj7Ze) |  | 0 |  | Zilin Fang, David Hsu, Gim Hee Lee |  |
| 2730 |  |  [A General Framework for Producing Interpretable Semantic Text Embeddings](https://openreview.net/forum?id=23uY3FpQxc) |  | 0 |  | Yiqun Sun, Qiang Huang, Yixuan Tang, Anthony Kum Hoe Tung, Jun Yu |  |
| 2731 |  |  [Unifying Causal Representation Learning with the Invariance Principle](https://openreview.net/forum?id=lk2Qk5xjeu) |  | 0 |  | Dingling Yao, Dario Rancati, Riccardo Cadei, Marco Fumero, Francesco Locatello |  |
| 2732 |  |  [A Multiscale Frequency Domain Causal Framework for Enhanced Pathological Analysis](https://openreview.net/forum?id=6xrDPHhwD3) |  | 0 |  | Xiaoyu Cui, Weixing Chen, Jiandong Su |  |
| 2733 |  |  [WeatherGFM: Learning a Weather Generalist Foundation Model via In-context Learning](https://openreview.net/forum?id=izjNI5bcOV) |  | 0 |  | Xiangyu Zhao, Zhiwang Zhou, Wenlong Zhang, Yihao Liu, Xiangyu Chen, Junchao Gong, Hao Chen, Ben Fei, Shiqi Chen, Wanli Ouyang, XiaoMing Wu, Lei Bai |  |
| 2734 |  |  [Provable Robust Overfitting Mitigation in Wasserstein Distributionally Robust Optimization](https://openreview.net/forum?id=sq5LLWk5SN) |  | 0 |  | Shuang Liu, Yihan Wang, Yifan Zhu, Yibo Miao, XiaoShan Gao |  |
| 2735 |  |  [Direct Distributional Optimization for Provable Alignment of Diffusion Models](https://openreview.net/forum?id=Nvw2szDdmI) |  | 0 |  | Ryotaro Kawata, Kazusato Oko, Atsushi Nitanda, Taiji Suzuki |  |
| 2736 |  |  [MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses](https://openreview.net/forum?id=X9OfMNNepI) |  | 0 |  | Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou |  |
| 2737 |  |  [AnyTouch: Learning Unified Static-Dynamic Representation across Multiple Visuo-tactile Sensors](https://openreview.net/forum?id=XToAemis1h) |  | 0 |  | Ruoxuan Feng, Jiangyu Hu, Wenke Xia, Tianci Gao, Ao Shen, Yuhao Sun, Bin Fang, Di Hu |  |
| 2738 |  |  [Optimal Flow Transport and its Entropic Regularization: a GPU-friendly Matrix Iterative Algorithm for Flow Balance Satisfaction](https://openreview.net/forum?id=NtSlKEJ2DS) |  | 0 |  | Liangliang Shi, Yufeng Li, Kaipeng Zeng, Yihui Tu, Junchi Yan |  |
| 2739 |  |  [Exploring The Forgetting in Adversarial Training: A Novel Method for Enhancing Robustness](https://openreview.net/forum?id=fjPOt8QlqQ) |  | 0 |  | Xianglu Wang, Hu Ding |  |
| 2740 |  |  [From Decoupling to Adaptive Transformation: a Wider Optimization Space for PTQ](https://openreview.net/forum?id=JElN0LJMKB) |  | 0 |  | Zhaojing Wen, Qiulin Zhang, Yuan Zhang, Rudan Chen, Xichao Yang, Di Xie, Jiang Zhu |  |
| 2741 |  |  [SaLoRA: Safety-Alignment Preserved Low-Rank Adaptation](https://openreview.net/forum?id=GOoVzE9nSj) |  | 0 |  | Mingjie Li, Wai Man Si, Michael Backes, Yang Zhang, Yisen Wang |  |
| 2742 |  |  [Unsupervised Meta-Learning via In-Context Learning](https://openreview.net/forum?id=Jprs1v2wPA) |  | 0 |  | Anna Vettoruzzo, Lorenzo Braccaioli, Joaquin Vanschoren, Marlena Nowaczyk |  |
| 2743 |  |  [Enhancing Prediction Performance through Influence Measure](https://openreview.net/forum?id=KjBG4JNOc2) |  | 0 |  | Shuguang Yu, Wenqian Xu, Xinyi Zhou, Xuechun Wang, Hongtu Zhu, Fan Zhou |  |
| 2744 |  |  [Poisson-Dirac Neural Networks for Modeling Coupled Dynamical Systems across Domains](https://openreview.net/forum?id=U1DjXQeJRx) |  | 0 |  | Razmik Arman Khosrovian, Takaharu Yaguchi, Hiroaki Yoshimura, Takashi Matsubara |  |
| 2745 |  |  [Show-o: One Single Transformer to Unify Multimodal Understanding and Generation](https://openreview.net/forum?id=o6Ynz6OIQ6) |  | 0 |  | Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, Mike Zheng Shou |  |
| 2746 |  |  [Wasserstein-Regularized Conformal Prediction under General Distribution Shift](https://openreview.net/forum?id=aJ3tiX1Tu4) |  | 0 |  | Rui Xu, Chao Chen, Yue Sun, Parvathinathan Venkitasubramaniam, Sihong Xie |  |
| 2747 |  |  [Motion Control of High-Dimensional Musculoskeletal Systems with Hierarchical Model-Based Planning](https://openreview.net/forum?id=MWHIIWrWWu) |  | 0 |  | Yunyue Wei, Shanning Zhuang, Vincent Zhuang, Yanan Sui |  |
| 2748 |  |  [Multimodal Situational Safety](https://openreview.net/forum?id=I9bEi6LNgt) |  | 0 |  | Kaiwen Zhou, Chengzhi Liu, Xuandong Zhao, Anderson Compalas, Dawn Song, Xin Eric Wang |  |
| 2749 |  |  [PaCA: Partial Connection Adaptation for Efficient Fine-Tuning](https://openreview.net/forum?id=iYkhxre0In) |  | 0 |  | Sunghyeon Woo, Sol Namkung, Sunwoo Lee, Inho Jeong, Beomseok Kim, Dongsuk Jeon |  |
| 2750 |  |  [Text-to-Image Rectified Flow as Plug-and-Play Priors](https://openreview.net/forum?id=SzPZK856iI) |  | 0 |  | Xiaofeng Yang, Cheng Chen, Xulei Yang, Fayao Liu, Guosheng Lin |  |
| 2751 |  |  [Graph Assisted Offline-Online Deep Reinforcement Learning for Dynamic Workflow Scheduling](https://openreview.net/forum?id=4PlbIfmX9o) |  | 0 |  | Yifan Yang, Gang Chen, Hui Ma, Cong Zhang, Zhiguang Cao, Mengjie Zhang |  |
| 2752 |  |  [MP-Mat: A 3D-and-Instance-Aware Human Matting and Editing Framework with Multiplane Representation](https://openreview.net/forum?id=Xw0fCEMFss) |  | 0 |  | Siyi Jiao, Wenzheng Zeng, Yerong Li, Huayu Zhang, Changxin Gao, Nong Sang, Mike Zheng Shou |  |
| 2753 |  |  [Visual-O1: Understanding Ambiguous Instructions via Multi-modal Multi-turn Chain-of-thoughts Reasoning](https://openreview.net/forum?id=v9CDpLpjiE) |  | 0 |  | Minheng Ni, Yutao Fan, Lei Zhang, Wangmeng Zuo |  |
| 2754 |  |  [TTVD: Towards a Geometric Framework for Test-Time Adaptation Based on Voronoi Diagram](https://openreview.net/forum?id=5sU32OCxgZ) |  | 0 |  | Mingxi Lei, Chunwei Ma, Meng Ding, Yufan Zhou, Ziyun Huang, Jinhui Xu |  |
| 2755 |  |  [Spurious Forgetting in Continual Learning of Language Models](https://openreview.net/forum?id=ScI7IlKGdI) |  | 0 |  | Junhao Zheng, Xidi Cai, Shengjie Qiu, Qianli Ma |  |
| 2756 |  |  [Optimal Non-Asymptotic Rates of Value Iteration for Average-Reward Markov Decision Processes](https://openreview.net/forum?id=WuTczPV8WC) |  | 0 |  | Jongmin Lee, Ernest K. Ryu |  |
| 2757 |  |  [When Graph Neural Networks Meet Dynamic Mode Decomposition](https://openreview.net/forum?id=duGygkA3QR) |  | 0 |  | Dai Shi, Lequan Lin, Andi Han, Zhiyong Wang, Yi Guo, Junbin Gao |  |
| 2758 |  |  [HQGS: High-Quality Novel View Synthesis with Gaussian Splatting in Degraded Scenes](https://openreview.net/forum?id=25Zlvl7JxW) |  | 0 |  | Xin Lin, Shi Luo, Xiaojun Shan, Xiaoyu Zhou, Chao Ren, Lu Qi, MingHsuan Yang, Nuno Vasconcelos |  |
| 2759 |  |  [Cyclic Contrastive Knowledge Transfer for Open-Vocabulary Object Detection](https://openreview.net/forum?id=JU9oHs7ivN) |  | 0 |  | Chuhan Zhang, Chaoyang Zhu, Pingcheng Dong, Long Chen, Dong Zhang |  |
| 2760 |  |  [Transformer Meets Twicing: Harnessing Unattended Residual Information](https://openreview.net/forum?id=16kG5aNleS) |  | 0 |  | Laziz U. Abdullaev, Tan Minh Nguyen |  |
| 2761 |  |  [SmartPretrain: Model-Agnostic and Dataset-Agnostic Representation Learning for Motion Prediction](https://openreview.net/forum?id=Bmzv2Gch9v) |  | 0 |  | Yang Zhou, Hao Shao, Letian Wang, Steven L. Waslander, Hongsheng Li, Yu Liu |  |
| 2762 |  |  [Jump Your Steps: Optimizing Sampling Schedule of Discrete Diffusion Models](https://openreview.net/forum?id=pD6TiCpyDR) |  | 0 |  | YongHyun Park, ChiehHsin Lai, Satoshi Hayakawa, Yuhta Takida, Yuki Mitsufuji |  |
| 2763 |  |  [Trusted Multi-View Classification via Evolutionary Multi-View Fusion](https://openreview.net/forum?id=M3kBtqpys5) |  | 0 |  | Xinyan Liang, Pinhan Fu, Yuhua Qian, Qian Guo, Guoqing Liu |  |
| 2764 |  |  [3D Vision-Language Gaussian Splatting](https://openreview.net/forum?id=SSE9myD9SG) |  | 0 |  | Qucheng Peng, Benjamin Planche, Zhongpai Gao, Meng Zheng, Anwesa Choudhuri, Terrence Chen, Chen Chen, Ziyan Wu |  |
| 2765 |  |  [REFINE: Inversion-Free Backdoor Defense via Model Reprogramming](https://openreview.net/forum?id=4IYdCws9fc) |  | 0 |  | Yukun Chen, Shuo Shao, Enhao Huang, Yiming Li, PinYu Chen, Zhan Qin, Kui Ren |  |
| 2766 |  |  [Mind Control through Causal Inference: Predicting Clean Images from Poisoned Data](https://openreview.net/forum?id=ho4mNiwr2n) |  | 0 |  | Mengxuan Hu, Zihan Guan, Yi Zeng, Junfeng Guo, Zhongliang Zhou, Jielu Zhang, Ruoxi Jia, Anil Kumar S. Vullikanti, Sheng Li |  |
| 2767 |  |  [Segment Any 3D Object with Language](https://openreview.net/forum?id=ENv1CeTwxc) |  | 0 |  | Seungjun Lee, Yuyang Zhao, Gim Hee Lee |  |
| 2768 |  |  [Adaptive Camera Sensor for Vision Models](https://openreview.net/forum?id=He2FGdmsas) |  | 0 |  | Eunsu Baek, Sunghwan Han, Taesik Gong, HyungSin Kim |  |
| 2769 |  |  [Swift4D: Adaptive divide-and-conquer Gaussian Splatting for compact and efficient reconstruction of dynamic scene](https://openreview.net/forum?id=c1RhJVTPwT) |  | 0 |  | Jiahao Wu, Rui Peng, Zhiyan Wang, Lu Xiao, Luyang Tang, Jinbo Yan, Kaiqiang Xiong, Ronggang Wang |  |
| 2770 |  |  [Rethinking Audio-Visual Adversarial Vulnerability from Temporal and Modality Perspectives](https://openreview.net/forum?id=ePJrZLIqpV) |  | 0 |  | Zeliang Zhang, Susan Liang, Daiki Shimada, Chenliang Xu |  |
| 2771 |  |  [Large Convolutional Model Tuning via Filter Subspace](https://openreview.net/forum?id=E5YmIBvOqV) |  | 0 |  | Wei Chen, Zichen Miao, Qiang Qiu |  |
| 2772 |  |  [TVNet: A Novel Time Series Analysis Method Based on Dynamic Convolution and 3D-Variation](https://openreview.net/forum?id=MZDdTzN6Cy) |  | 0 |  | Chenghan Li, Mingchen Li, Ruisheng Diao |  |
| 2773 |  |  [Articulate-Anything: Automatic Modeling of Articulated Objects via a Vision-Language Foundation Model](https://openreview.net/forum?id=s3FTX4Ay55) |  | 0 |  | Long Le, Jason Xie, William Liang, HungJu Wang, Yue Yang, Yecheng Jason Ma, Kyle Vedder, Arjun Krishna, Dinesh Jayaraman, Eric Eaton |  |
| 2774 |  |  [Investigating Pattern Neurons in Urban Time Series Forecasting](https://openreview.net/forum?id=a9vey6B54y) |  | 0 |  | Chengxin Wang, Yiran Zhao, Shaofeng Cai, Gary Tan |  |
| 2775 |  |  [Tight Clusters Make Specialized Experts](https://openreview.net/forum?id=Pu3c0209cx) |  | 0 |  | Stefan K. Nielsen, Rachel S. Y. Teo, Laziz U. Abdullaev, Tan Minh Nguyen |  |
| 2776 |  |  [Trajectory-LLM: A Language-based Data Generator for Trajectory Prediction in Autonomous Driving](https://openreview.net/forum?id=UapxTvxB3N) |  | 0 |  | Kairui Yang, Zihao Guo, Gengjie Lin, Haotian Dong, Zhao Huang, Yipeng Wu, Die Zuo, Jibin Peng, Ziyuan Zhong, Xin Wang, Qing Guo, Xiaosong Jia, Junchi Yan, Di Lin |  |
| 2777 |  |  [LASER: A Neuro-Symbolic Framework for Learning Spatio-Temporal Scene Graphs with Weak Supervision](https://openreview.net/forum?id=HEXtydywnE) |  | 0 |  | Jiani Huang, Ziyang Li, Mayur Naik, SerNam Lim |  |
| 2778 |  |  [Real-Time Video Generation with Pyramid Attention Broadcast](https://openreview.net/forum?id=hDBrQ4DApF) |  | 0 |  | Xuanlei Zhao, Xiaolong Jin, Kai Wang, Yang You |  |
| 2779 |  |  [QMP: Q-switch Mixture of Policies for Multi-Task Behavior Sharing](https://openreview.net/forum?id=aUZEeb2yvK) |  | 0 |  | Grace Zhang, Ayush Jain, Injune Hwang, ShaoHua Sun, Joseph J. Lim |  |
| 2780 |  |  [Rationalizing and Augmenting Dynamic Graph Neural Networks](https://openreview.net/forum?id=thV5KRQFgQ) |  | 0 |  | Guibin Zhang, Yiyan Qi, Ziyang Cheng, Yanwei Yue, Dawei Cheng, Jian Guo |  |
| 2781 |  |  [Decoding Game: On Minimax Optimality of Heuristic Text Generation Strategies](https://openreview.net/forum?id=Wfw4ypsgRZ) |  | 0 |  | Sijin Chen, Omar Hagrass, Jason Matthew Klusowski |  |
| 2782 |  |  [DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search](https://openreview.net/forum?id=I4YAIwrsXa) |  | 0 |  | Huajian Xin, Z. Z. Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, Wenjun Gao, Haowei Zhang, Qihao Zhu, Dejian Yang, Zhibin Gou, Z. F. Wu, Fuli Luo, Chong Ruan |  |
| 2783 |  |  [Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs](https://openreview.net/forum?id=d2H1oTNITn) |  | 0 |  | Yuzhe Gu, Wenwei Zhang, Chengqi Lyu, Dahua Lin, Kai Chen |  |
| 2784 |  |  [Fine-Tuning Attention Modules Only: Enhancing Weight Disentanglement in Task Arithmetic](https://openreview.net/forum?id=dj0TktJcVI) |  | 0 |  | Ruochen Jin, Bojian Hou, Jiancong Xiao, Weijie J. Su, Li Shen |  |
| 2785 |  |  [Rethinking Self-Distillation: Label Averaging and Enhanced Soft Label Refinement with Partial Labels](https://openreview.net/forum?id=EJfLvrzh2Q) |  | 0 |  | Hyeonsu Jeong, Hye Won Chung |  |
| 2786 |  |  [Natural Language Inference Improves Compositionality in Vision-Language Models](https://openreview.net/forum?id=G3aXjVAJjU) |  | 0 |  | Paola CascanteBonilla, Yu Hou, Yang Trista Cao, Hal Daumé III, Rachel Rudinger |  |
| 2787 |  |  [KAA: Kolmogorov-Arnold Attention for Enhancing Attentive Graph Neural Networks](https://openreview.net/forum?id=atXCzVSXTJ) |  | 0 |  | Taoran Fang, Tianhong Gao, Chunping Wang, Yihao Shang, Wei Chow, Lei Chen, Yang Yang |  |
| 2788 |  |  [Isometric Regularization for Manifolds of Functional Data](https://openreview.net/forum?id=xBuURiCChw) |  | 0 |  | Hyeongjun Heo, Seonghun Oh, Jae Yong Lee, Young Min Kim, Yonghyeon Lee |  |
| 2789 |  |  [PostEdit: Posterior Sampling for Efficient Zero-Shot Image Editing](https://openreview.net/forum?id=J8YWCBPgx7) |  | 0 |  | Feng Tian, Yixuan Li, Yichao Yan, Shanyan Guan, Yanhao Ge, Xiaokang Yang |  |
| 2790 |  |  [Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models](https://openreview.net/forum?id=45rvZkJbuX) |  | 0 |  | Shicheng Xu, Liang Pang, Yunchang Zhu, Huawei Shen, Xueqi Cheng |  |
| 2791 |  |  [RefactorBench: Evaluating Stateful Reasoning in Language Agents Through Code](https://openreview.net/forum?id=NiNIthntx7) |  | 0 |  | Dhruv Gautam, Spandan Garg, Jinu Jang, Neel Sundaresan, Roshanak Zilouchian Moghaddam |  |
| 2792 |  |  [Efficient Training of Neural Stochastic Differential Equations by Matching Finite Dimensional Distributions](https://openreview.net/forum?id=d4qMoUSMLT) |  | 0 |  | Jianxin Zhang, Josh Viktorov, Doosan Jung, Emily Pitler |  |
| 2793 |  |  [Learning Diverse Attacks on Large Language Models for Robust Red-Teaming and Safety Tuning](https://openreview.net/forum?id=1mXufFuv95) |  | 0 |  | Seanie Lee, Minsu Kim, Lynn Cherif, David Dobre, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, Moksh Jain |  |
| 2794 |  |  [TIS-DPO: Token-level Importance Sampling for Direct Preference Optimization With Estimated Weights](https://openreview.net/forum?id=oF6e2WwxX0) |  | 0 |  | Aiwei Liu, Haoping Bai, Zhiyun Lu, Yanchao Sun, Xiang Kong, Xiaoming Simon Wang, Jiulong Shan, Albin Madappally Jose, Xiaojiang Liu, Lijie Wen, Philip S. Yu, Meng Cao |  |
| 2795 |  |  [Spectro-Riemannian Graph Neural Networks](https://openreview.net/forum?id=2MLvV7fvAz) |  | 0 |  | Karish Grover, Haiyang Yu, Xiang Song, Qi Zhu, Han Xie, Vassilis N. Ioannidis, Christos Faloutsos |  |
| 2796 |  |  [CBMA: Improving Conformal Prediction through Bayesian Model Averaging](https://openreview.net/forum?id=BKSeNw2HIr) |  | 0 |  | Pankaj Bhagwat, Linglong Kong, Bei Jiang |  |
| 2797 |  |  [Point-SAM: Promptable 3D Segmentation Model for Point Clouds](https://openreview.net/forum?id=yXCTDhZDh6) |  | 0 |  | Yuchen Zhou, Jiayuan Gu, Tung Yen Chiang, Fanbo Xiang, Hao Su |  |
| 2798 |  |  [Causal Discovery via Bayesian Optimization](https://openreview.net/forum?id=8muemqlnG3) |  | 0 |  | Bao Duong, Sunil Gupta, Thin Nguyen |  |
| 2799 |  |  [Towards Out-of-Modal Generalization without Instance-level Modal Correspondence](https://openreview.net/forum?id=LuVulfPgZN) |  | 0 |  | Zhuo Huang, Gang Niu, Bo Han, Masashi Sugiyama, Tongliang Liu |  |
| 2800 |  |  [Data Distillation for extrapolative protein design through exact preference optimization](https://openreview.net/forum?id=ua5MHdsbck) |  | 0 |  | Mostafa Karimi, Sharmi Banerjee, Tommi Jaakkola, Bella Dubrov, Shang Shang, Ron Benson |  |
| 2801 |  |  [Let Me Grok for You: Accelerating Grokking via Embedding Transfer from a Weaker Model](https://openreview.net/forum?id=4rEI2JdHH6) |  | 0 |  | Zhiwei Xu, Zhiyu Ni, Yixin Wang, Wei Hu |  |
| 2802 |  |  [Do LLMs estimate uncertainty well in instruction-following?](https://openreview.net/forum?id=IHp3vOVQO2) |  | 0 |  | Juyeon Heo, Miao Xiong, Christina HeinzeDeml, Jaya Narain |  |
| 2803 |  |  [TopoDiffusionNet: A Topology-aware Diffusion Model](https://openreview.net/forum?id=ZK1LoTo10R) |  | 0 |  | Saumya Gupta, Dimitris Samaras, Chao Chen |  |
| 2804 |  |  [DistillHGNN: A Knowledge Distillation Approach for High-Speed Hypergraph Neural Networks](https://openreview.net/forum?id=vzrs42hgb0) |  | 0 |  | Saman Forouzandeh, Parham Moradi, Mahdi Jalili |  |
| 2805 |  |  [Logical Consistency of Large Language Models in Fact-Checking](https://openreview.net/forum?id=SimlDuN0YT) |  | 0 |  | Bishwamittra Ghosh, Sarah Hasan, Naheed Anjum Arafat, Arijit Khan |  |
| 2806 |  |  [PIN: Prolate Spheroidal Wave Function-based Implicit Neural Representations](https://openreview.net/forum?id=Eh1QM3OK51) |  | 0 |  | Dhananjaya Jayasundara, Heng Zhao, Demetrio Labate, Vishal M. Patel |  |
| 2807 |  |  [Sort-free Gaussian Splatting via Weighted Sum Rendering](https://openreview.net/forum?id=y8uPsxR8PN) |  | 0 |  | Qiqi Hou, Randall Rauwendaal, Zifeng Li, Hoang Le, Farzad Farhadzadeh, Fatih Porikli, Alexei Bourd, Amir Said |  |
| 2808 |  |  [Subtask-Aware Visual Reward Learning from Segmented Demonstrations](https://openreview.net/forum?id=mqKVe6F3Up) |  | 0 |  | Changyeon Kim, Minho Heo, Doohyun Lee, Honglak Lee, Jinwoo Shin, Joseph J. Lim, Kimin Lee |  |
| 2809 |  |  [Not All Language Model Features Are One-Dimensionally Linear](https://openreview.net/forum?id=d63a4AM4hb) |  | 0 |  | Joshua Engels, Eric J. Michaud, Isaac Liao, Wes Gurnee, Max Tegmark |  |
| 2810 |  |  [SelectFormer in Data Markets: Privacy-Preserving and Efficient Data Selection for Transformers with Multi-Party Computation](https://openreview.net/forum?id=2cF3f9t31y) |  | 0 |  | Xu Ouyang, Felix Xiaozhu Lin, Yangfeng Ji |  |
| 2811 |  |  [Enhancing Language Model Agents using Diversity of Thoughts](https://openreview.net/forum?id=ZsP3YbYeE9) |  | 0 |  | Vijay Lingam, Behrooz OmidvarTehrani, Sujay Sanghavi, Gaurav Gupta, Sayan Ghosh, Linbo Liu, Jun Huan, Anoop Deoras |  |
| 2812 |  |  [Instructional Segment Embedding: Improving LLM Safety with Instruction Hierarchy](https://openreview.net/forum?id=sjWG7B8dvt) |  | 0 |  | Tong Wu, Shujian Zhang, Kaiqiang Song, Silei Xu, Sanqiang Zhao, Ravi Agrawal, Sathish Reddy Indurthi, Chong Xiang, Prateek Mittal, Wenxuan Zhou |  |
| 2813 |  |  [Eia: Environmental Injection Attack on Generalist Web Agents for Privacy Leakage](https://openreview.net/forum?id=xMOLUzo2Lk) |  | 0 |  | Zeyi Liao, Lingbo Mo, Chejian Xu, Mintong Kang, Jiawei Zhang, Chaowei Xiao, Yuan Tian, Bo Li, Huan Sun |  |
| 2814 |  |  [Don't stop me Now: Embedding based Scheduling for LLMS](https://openreview.net/forum?id=7JhGdZvW4T) |  | 0 |  | Rana Shahout, Eran Malach, Chunwei Liu, Weifan Jiang, Minlan Yu, Michael Mitzenmacher |  |
| 2815 |  |  [cryoSPHERE: Single-Particle HEterogeneous REconstruction from cryo EM](https://openreview.net/forum?id=n8O0trhost) |  | 0 |  | Gabriel Ducrocq, Lukas Grunewald, Sebastian Westenhoff, Fredrik Lindsten |  |
| 2816 |  |  [Contextualizing biological perturbation experiments through language](https://openreview.net/forum?id=5WEpbilssv) |  | 0 |  | Menghua Wu, Russell Littman, Jacob Levine, Lin Qiu, Tommaso Biancalani, David Richmond, JanChristian Huetter |  |
| 2817 |  |  [Rethinking Fair Representation Learning for Performance-Sensitive Tasks](https://openreview.net/forum?id=pBZntPrdrI) |  | 0 |  | Charles Jones, Fabio De Sousa Ribeiro, Mélanie Roschewitz, Daniel C. Castro, Ben Glocker |  |
| 2818 |  |  [Privacy Auditing of Large Language Models](https://openreview.net/forum?id=60Vd7QOXlM) |  | 0 |  | Ashwinee Panda, Xinyu Tang, Christopher A. ChoquetteChoo, Milad Nasr, Prateek Mittal |  |
| 2819 |  |  [Variance-Reducing Couplings for Random Features](https://openreview.net/forum?id=oJLpXraSLb) |  | 0 |  | Isaac Reid, Stratis Markou, Krzysztof Marcin Choromanski, Richard E. Turner, Adrian Weller |  |
| 2820 |  |  [Linear Transformer Topological Masking with Graph Random Features](https://openreview.net/forum?id=6MBqQLp17E) |  | 0 |  | Isaac Reid, Kumar Avinava Dubey, Deepali Jain, William F. Whitney, Amr Ahmed, Joshua Ainslie, Alex Bewley, Mithun George Jacob, Aranyak Mehta, David Rendleman, Connor Schenck, Richard E. Turner, René Wagner, Adrian Weller, Krzysztof Marcin Choromanski |  |
| 2821 |  |  [Unlearn and Burn: Adversarial Machine Unlearning Requests Destroy Model Accuracy](https://openreview.net/forum?id=5xxGP9x5dZ) |  | 0 |  | Yangsibo Huang, Daogao Liu, Lynn Chua, Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Milad Nasr, Amer Sinha, Chiyuan Zhang |  |
| 2822 |  |  [A3D: Does Diffusion Dream about 3D Alignment?](https://openreview.net/forum?id=QQCIfkhGIq) |  | 0 |  | Savva Victorovich Ignatyev, Nina Konovalova, Daniil Selikhanovych, Oleg Voynov, Nikolay Patakin, Ilya Olkov, Dmitry Senushkin, Alexey Artemov, Anton Konushin, Alexander Filippov, Peter Wonka, Evgeny Burnaev |  |
| 2823 |  |  [Non-myopic Generation of Language Models for Reasoning and Planning](https://openreview.net/forum?id=OoNazl6T7D) |  | 0 |  | Chang Ma, Haiteng Zhao, Junlei Zhang, Junxian He, Lingpeng Kong |  |
| 2824 |  |  [3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing](https://openreview.net/forum?id=7JUrBLDjCq) |  | 0 |  | Jiahua Dong, YuXiong Wang |  |
| 2825 |  |  [Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling](https://openreview.net/forum?id=3OyaXFQuDl) |  | 0 |  | Hritik Bansal, Arian Hosseini, Rishabh Agarwal, Vinh Q. Tran, Mehran Kazemi |  |
| 2826 |  |  [Distributed Speculative Inference (DSI): Speculation Parallelism for Provably Faster Lossless Language Model Inference](https://openreview.net/forum?id=cJd1BgZ9CS) |  | 0 |  | Nadav Timor, Jonathan Mamou, Daniel Korat, Moshe Berchansky, Oren Pereg, Moshe Wasserblat, Tomer Galanti, Michal GordonKiwkowitz, David Harel |  |
| 2827 |  |  [Improving Generalization and Robustness in SNNs Through Signed Rate Encoding and Sparse Encoding Attacks](https://openreview.net/forum?id=qLh6Ufvnuc) |  | 0 |  | Bhaskar Mukhoty, Hilal AlQuabeh, Bin Gu |  |
| 2828 |  |  [Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers](https://openreview.net/forum?id=M23dTGWCZy) |  | 0 |  | Chenglei Si, Diyi Yang, Tatsunori Hashimoto |  |
| 2829 |  |  [CertainlyUncertain: A Benchmark and Metric for Multimodal Epistemic and Aleatoric Awareness](https://openreview.net/forum?id=cQ25MQQSNI) |  | 0 |  | Khyathi Raghavi Chandu, Linjie Li, Anas Awadalla, Ximing Lu, Jae Sung Park, Jack Hessel, Lijuan Wang, Yejin Choi |  |
| 2830 |  |  [Generative Verifiers: Reward Modeling as Next-Token Prediction](https://openreview.net/forum?id=Ccwp4tFEtE) |  | 0 |  | Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, Rishabh Agarwal |  |
| 2831 |  |  [Towards Understanding the Robustness of Diffusion-Based Purification: A Stochastic Perspective](https://openreview.net/forum?id=shqjOIK3SA) |  | 0 |  | Yiming Liu, Kezhao Liu, Yao Xiao, Ziyi Dong, Xiaogang Xu, Pengxu Wei, Liang Lin |  |
| 2832 |  |  [COAT: Compressing Optimizer states and Activations for Memory-Efficient FP8 Training](https://openreview.net/forum?id=XfKSDgqIRj) |  | 0 |  | Haocheng Xi, Han Cai, Ligeng Zhu, Yao Lu, Kurt Keutzer, Jianfei Chen, Song Han |  |
| 2833 |  |  [Error-quantified Conformal Inference for Time Series](https://openreview.net/forum?id=RD9q5vEe1Q) |  | 0 |  | Junxi Wu, Dongjian Hu, Yajie Bao, ShuTao Xia, Changliang Zou |  |
| 2834 |  |  [Fengbo: a Clifford Neural Operator pipeline for 3D PDEs in Computational Fluid Dynamics](https://openreview.net/forum?id=VsxbWTDHjh) |  | 0 |  | Alberto Pepe, Mattia Montanari, Joan Lasenby |  |
| 2835 |  |  [Needle Threading: Can LLMs Follow Threads Through Near-Million-Scale Haystacks?](https://openreview.net/forum?id=wHLMsM1SrP) |  | 0 |  | Jonathan Roberts, Kai Han, Samuel Albanie |  |
| 2836 |  |  [SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Model](https://openreview.net/forum?id=FGMkSL8NR0) |  | 0 |  | Yue Zhang, Zhiyang Xu, Ying Shen, Parisa Kordjamshidi, Lifu Huang |  |
| 2837 |  |  [Optimal Protocols for Continual Learning via Statistical Physics and Control Theory](https://openreview.net/forum?id=rhhQjGj09A) |  | 0 |  | Francesco Mori, Stefano Sarao Mannelli, Francesca Mignacco |  |
| 2838 |  |  [Learning Generalizable Skills from Offline Multi-Task Data for Multi-Agent Cooperation](https://openreview.net/forum?id=HR1ujVR0ig) |  | 0 |  | Sicong Liu, Yang Shu, Chenjuan Guo, Bin Yang |  |
| 2839 |  |  [Visual Haystacks: A Vision-Centric Needle-In-A-Haystack Benchmark](https://openreview.net/forum?id=9JCNPFL1f9) |  | 0 |  | TsungHan Wu, Giscard Biamby, Jerome Quenum, Ritwik Gupta, Joseph E. Gonzalez, Trevor Darrell, David M. Chan |  |
| 2840 |  |  [Null Counterfactual Factor Interactions for Goal-Conditioned Reinforcement Learning](https://openreview.net/forum?id=2uPZ4aX1VV) |  | 0 |  | Caleb Chuck, Fan Feng, Carl Qi, Chang Shi, Siddhant Agarwal, Amy Zhang, Scott Niekum |  |
| 2841 |  |  [PAD: Personalized Alignment of LLMs at Decoding-time](https://openreview.net/forum?id=e7AUJpP8bV) |  | 0 |  | Ruizhe Chen, Xiaotian Zhang, Meng Luo, Wenhao Chai, Zuozhu Liu |  |
| 2842 |  |  [From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities](https://openreview.net/forum?id=3TnLGGHhNx) |  | 0 |  | Wanpeng Zhang, Zilong Xie, Yicheng Feng, Yijiang Li, Xingrun Xing, Sipeng Zheng, Zongqing Lu |  |
| 2843 |  |  [Diffusion Models as Cartoonists: The Curious Case of High Density Regions](https://openreview.net/forum?id=RiS2cxpENN) |  | 0 |  | Rafal Karczewski, Markus Heinonen, Vikas Garg |  |
| 2844 |  |  [Differentiable Rule Induction from Raw Sequence Inputs](https://openreview.net/forum?id=zDjHOsSQxd) |  | 0 |  | Kun Gao, Katsumi Inoue, Yongzhi Cao, Hanpin Wang, Yang Feng |  |
| 2845 |  |  [On the Modeling Capabilities of Large Language Models for Sequential Decision Making](https://openreview.net/forum?id=vodsIF3o7N) |  | 0 |  | Martin Klissarov, R. Devon Hjelm, Alexander T. Toshev, Bogdan Mazoure |  |
| 2846 |  |  [Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks](https://openreview.net/forum?id=BEpaPHDl9r) |  | 0 |  | Nikolaos Tsilivis, Gal Vardi, Julia Kempe |  |
| 2847 |  |  [Exploiting Distribution Constraints for Scalable and Efficient Image Retrieval](https://openreview.net/forum?id=d0tlL0ZWlu) |  | 0 |  | Mohammad Omama, Pohan Li, Sandeep P. Chinchali |  |
| 2848 |  |  [Discrete Distribution Networks](https://openreview.net/forum?id=xNsIfzlefG) |  | 0 |  | Lei Yang |  |
| 2849 |  |  [Chain-of-Thought Provably Enables Learning the (Otherwise) Unlearnable](https://openreview.net/forum?id=N6pbLYLeej) |  | 0 |  | Chenxiao Yang, Zhiyuan Li, David Wipf |  |
| 2850 |  |  [Glad: A Streaming Scene Generator for Autonomous Driving](https://openreview.net/forum?id=ZFxpclrCCf) |  | 0 |  | Bin Xie, Yingfei Liu, Tiancai Wang, Jiale Cao, Xiangyu Zhang |  |
| 2851 |  |  [Diffusion Models are Evolutionary Algorithms](https://openreview.net/forum?id=xVefsBbG2O) |  | 0 |  | Yanbo Zhang, Benedikt Hartl, Hananel Hazan, Michael Levin |  |
| 2852 |  |  [Beyond Interpretability: The Gains of Feature Monosemanticity on Model Robustness](https://openreview.net/forum?id=g6Qc3p7JH5) |  | 0 |  | Qi Zhang, Yifei Wang, Jingyi Cui, Xiang Pan, Qi Lei, Stefanie Jegelka, Yisen Wang |  |
| 2853 |  |  [Exploring a Principled Framework for Deep Subspace Clustering](https://openreview.net/forum?id=7psWohxvxp) |  | 0 |  | Xianghan Meng, Zhiyuan Huang, Wei He, Xianbiao Qi, Rong Xiao, ChunGuang Li |  |
| 2854 |  |  [HELM: Hierarchical Encoding for mRNA Language Modeling](https://openreview.net/forum?id=MMHqnUOnl0) |  | 0 |  | Mehdi YazdaniJahromi, Mangal Prakash, Tommaso Mansi, Artem Moskalev, Rui Liao |  |
| 2855 |  |  [Stochastic variance-reduced Gaussian variational inference on the Bures-Wasserstein manifold](https://openreview.net/forum?id=iMJpmcYucq) |  | 0 |  | Hoang Phuc Hau Luu, Hanlin Yu, Bernardo Williams, Marcelo Hartmann, Arto Klami |  |
| 2856 |  |  [SelKD: Selective Knowledge Distillation via Optimal Transport Perspective](https://openreview.net/forum?id=H4iVLvRusn) |  | 0 |  | Liangliang Shi, Zhengyan Shi, Junchi Yan |  |
| 2857 |  |  [MeshMask: Physics-Based Simulations with Masked Graph Neural Networks](https://openreview.net/forum?id=bFHR8hNk4I) |  | 0 |  | Paul Garnier, Vincent Lannelongue, Jonathan Viquerat, Elie Hachem |  |
| 2858 |  |  [EMOS: Embodiment-aware Heterogeneous Multi-robot Operating System with LLM Agents](https://openreview.net/forum?id=Ey8KcabBpB) |  | 0 |  | Junting Chen, Checheng Yu, Xunzhe Zhou, Tianqi Xu, Yao Mu, Mengkang Hu, Wenqi Shao, Yikai Wang, Guohao Li, Lin Shao |  |
| 2859 |  |  [Watch Less, Do More: Implicit Skill Discovery for Video-Conditioned Policy](https://openreview.net/forum?id=hgvERMkXOx) |  | 0 |  | Jiangxing Wang, Zongqing Lu |  |
| 2860 |  |  [MiniPLM: Knowledge Distillation for Pre-training Language Models](https://openreview.net/forum?id=tJHDw8XfeC) |  | 0 |  | Yuxian Gu, Hao Zhou, Fandong Meng, Jie Zhou, Minlie Huang |  |
| 2861 |  |  [Scalable Decentralized Learning with Teleportation](https://openreview.net/forum?id=AvmBgiQxxp) |  | 0 |  | Yuki Takezawa, Sebastian U. Stich |  |
| 2862 |  |  [What Matters When Repurposing Diffusion Models for General Dense Perception Tasks?](https://openreview.net/forum?id=BgYbk6ZmeX) |  | 0 |  | Guangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen, Chunhua Shen |  |
| 2863 |  |  [Scalable Discrete Diffusion Samplers: Combinatorial Optimization and Statistical Physics](https://openreview.net/forum?id=peNgxpbdxB) |  | 0 |  | Sebastian Sanokowski, Wilhelm Franz Berghammer, Haoyu Peter Wang, Martin Ennemoser, Sepp Hochreiter, Sebastian Lehner |  |
| 2864 |  |  [Towards Multiple Character Image Animation Through Enhancing Implicit Decoupling](https://openreview.net/forum?id=aqlzXgXwWa) |  | 0 |  | Jingyun Xue, Hongfa Wang, Qi Tian, Yue Ma, Andong Wang, Zhiyuan Zhao, Shaobo Min, Wenzhe Zhao, Kaihao Zhang, HeungYeung Shum, Wei Liu, Mengyang Liu, Wenhan Luo |  |
| 2865 |  |  [On the Byzantine-Resilience of Distillation-Based Federated Learning](https://openreview.net/forum?id=of6EuHT7de) |  | 0 |  | Christophe Roux, Max Zimmer, Sebastian Pokutta |  |
| 2866 |  |  [QPM: Discrete Optimization for Globally Interpretable Image Classification](https://openreview.net/forum?id=GlAeL0I8LX) |  | 0 |  | Thomas Norrenbrock, Timo Kaiser, Sovan Biswas, Ramesh Manuvinakurike, Bodo Rosenhahn |  |
| 2867 |  |  [DynaPrompt: Dynamic Test-Time Prompt Tuning](https://openreview.net/forum?id=EFZEdHB3Mp) |  | 0 |  | Zehao Xiao, Shilin Yan, Jack Hong, Jiayin Cai, Xiaolong Jiang, Yao Hu, Jiayi Shen, Cheems Wang, Cees G. M. Snoek |  |
| 2868 |  |  [EffoVPR: Effective Foundation Model Utilization for Visual Place Recognition](https://openreview.net/forum?id=NSpe8QgsCB) |  | 0 |  | Issar Tzachor, Boaz Lerner, Matan Levy, Michael Green, Tal Berkovitz Shalev, Gavriel Habib, Dvir Samuel, Noam Korngut Zailer, Or Shimshi, Nir Darshan, Rami BenAri |  |
| 2869 |  |  [Linear combinations of latents in generative models: subspaces and beyond](https://openreview.net/forum?id=n5PrId7pk5) |  | 0 |  | Erik Bodin, Alexandru I. Stere, Dragos D. Margineantu, Carl Henrik Ek, Henry Moss |  |
| 2870 |  |  [SOREL: A Stochastic Algorithm for Spectral Risks Minimization](https://openreview.net/forum?id=pdF86dyoS6) |  | 0 |  | Yuze Ge, Rujun Jiang |  |
| 2871 |  |  [Proactive Privacy Amnesia for Large Language Models: Safeguarding PII with Negligible Impact on Model Utility](https://openreview.net/forum?id=io8uRPYktn) |  | 0 |  | Martin Kuo, Jingyang Zhang, Jianyi Zhang, Minxue Tang, Louis DiValentin, Aolin Ding, Jingwei Sun, William Chen, Amin Hass, Tianlong Chen, Yiran Chen, Hai Li |  |
| 2872 |  |  [Variational Best-of-N Alignment](https://openreview.net/forum?id=W9FZEQj3vv) |  | 0 |  | Afra Amini, Tim Vieira, Elliott Ash, Ryan Cotterell |  |
| 2873 |  |  [HMoRA: Making LLMs More Effective with Hierarchical Mixture of LoRA Experts](https://openreview.net/forum?id=lTkHiXeuDl) |  | 0 |  | Mengqi Liao, Wei Chen, Junfeng Shen, Shengnan Guo, Huaiyu Wan |  |
| 2874 |  |  [Neural ODE Transformers: Analyzing Internal Dynamics and Adaptive Fine-tuning](https://openreview.net/forum?id=XnDyddPcBT) |  | 0 |  | Anh Tong, Thanh NguyenTang, Dongeun Lee, Duc Nguyen, Toan M. Tran, David Leo Wright Hall, Cheongwoong Kang, Jaesik Choi |  |
| 2875 |  |  [Adversarially Robust Out-of-Distribution Detection Using Lyapunov-Stabilized Embeddings](https://openreview.net/forum?id=GrDne4055L) |  | 0 |  | Hossein Mirzaei, Mackenzie W. Mathis |  |
| 2876 |  |  [Shape as Line Segments: Accurate and Flexible Implicit Surface Representation](https://openreview.net/forum?id=RavSZTIe2s) |  | 0 |  | Siyu Ren, Junhui Hou |  |
| 2877 |  |  [MoS: Unleashing Parameter Efficiency of Low-Rank Adaptation with Mixture of Shards](https://openreview.net/forum?id=1uLW9eYNJB) |  | 0 |  | Sheng Wang, Liheng Chen, Pengan Chen, Jingwei Dong, Boyang Xue, Jiyue Jiang, Lingpeng Kong, Chuan Wu |  |
| 2878 |  |  [Simulating Human-like Daily Activities with Desire-driven Autonomy](https://openreview.net/forum?id=3ms8EQY7f8) |  | 0 |  | Yiding Wang, Yuxuan Chen, Fangwei Zhong, Long Ma, Yizhou Wang |  |
| 2879 |  |  [Ctrl-U: Robust Conditional Image Generation via Uncertainty-aware Reward Modeling](https://openreview.net/forum?id=eC2ICbECNM) |  | 0 |  | Guiyu Zhang, Huanang Gao, Zijian Jiang, Hao Zhao, Zhedong Zheng |  |
| 2880 |  |  [Gradient correlation is a key ingredient to accelerate SGD with momentum](https://openreview.net/forum?id=2Q8gTck8Uq) |  | 0 |  | Julien Hermant, Marien Renaud, JeanFrançois Aujol, Charles Dossal, Aude Rondepierre |  |
| 2881 |  |  [Multiplicative Logit Adjustment Approximates Neural-Collapse-Aware Decision Boundary Adjustment](https://openreview.net/forum?id=II81zQUS1x) |  | 0 |  | Naoya Hasegawa, Issei Sato |  |
| 2882 |  |  [Pyramidal Flow Matching for Efficient Video Generative Modeling](https://openreview.net/forum?id=66NzcRQuOq) |  | 0 |  | Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, Zhouchen Lin |  |
| 2883 |  |  [CtrLoRA: An Extensible and Efficient Framework for Controllable Image Generation](https://openreview.net/forum?id=3Gga05Jdmj) |  | 0 |  | Yifeng Xu, Zhenliang He, Shiguang Shan, Xilin Chen |  |
| 2884 |  |  [Fast Summation of Radial Kernels via QMC Slicing](https://openreview.net/forum?id=iNmVX9lx9l) |  | 0 |  | Johannes Hertrich, Tim Jahn, Michael Quellmalz |  |
| 2885 |  |  [Enhancing Federated Domain Adaptation with Multi-Domain Prototype-Based Federated Fine-Tuning](https://openreview.net/forum?id=3wEGdrV5Cb) |  | 0 |  | Jingyuan Zhang, Yiyang Duan, Shuaicheng Niu, Yang Cao, Wei Yang Bryan Lim |  |
| 2886 |  |  [NEAR: A Training-Free Pre-Estimator of Machine Learning Model Performance](https://openreview.net/forum?id=Z8RZrvngm5) |  | 0 |  | Raphael T. Husistein, Markus Reiher, Marco Eckhoff |  |
| 2887 |  |  [Do as I do (Safely): Mitigating Task-Specific Fine-tuning Risks in Large Language Models](https://openreview.net/forum?id=lXE5lB6ppV) |  | 0 |  | Francisco Eiras, Aleksandar Petrov, Philip Torr, M. Pawan Kumar, Adel Bibi |  |
| 2888 |  |  [Epistemic Monte Carlo Tree Search](https://openreview.net/forum?id=Tb8RiXOc3N) |  | 0 |  | Yaniv Oren, Viliam Vadocz, Matthijs T. J. Spaan, Wendelin Boehmer |  |
| 2889 |  |  [ParaSolver: A Hierarchical Parallel Integral Solver for Diffusion Models](https://openreview.net/forum?id=2JihLwirxO) |  | 0 |  | Jianrong Lu, Zhiyu Zhu, Junhui Hou |  |
| 2890 |  |  [Enhance Multi-View Classification Through Multi-Scale Alignment and Expanded Boundary](https://openreview.net/forum?id=t1J2CnDFwj) |  | 0 |  | Yuena Lin, Yiyuan Wang, Gengyu Lyu, Yongjian Deng, Haichun Cai, Huibin Lin, Haobo Wang, Zhen Yang |  |
| 2891 |  |  [LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs](https://openreview.net/forum?id=3A71qNKWAS) |  | 0 |  | Yuhao Wu, Ming Shan Hee, Zhiqiang Hu, Roy KaWei Lee |  |
| 2892 |  |  [Temporal Reasoning Transfer from Text to Video](https://openreview.net/forum?id=sHAvMp5J4R) |  | 0 |  | Lei Li, Yuanxin Liu, Linli Yao, Peiyuan Zhang, Chenxin An, Lean Wang, Xu Sun, Lingpeng Kong, Qi Liu |  |
| 2893 |  |  [Interpreting Language Reward Models via Contrastive Explanations](https://openreview.net/forum?id=i8IwcQBi74) |  | 0 |  | Junqi Jiang, Tom Bewley, Saumitra Mishra, Freddy Lécué, Manuela Veloso |  |
| 2894 |  |  [Studying the Interplay Between the Actor and Critic Representations in Reinforcement Learning](https://openreview.net/forum?id=tErHYBGlWc) |  | 0 |  | Samuel Garcin, Trevor McInroe, Pablo Samuel Castro, Christopher G. Lucas, David Abel, Prakash Panangaden, Stefano V. Albrecht |  |
| 2895 |  |  [IV-mixed Sampler: Leveraging Image Diffusion Models for Enhanced Video Synthesis](https://openreview.net/forum?id=ImpeMDJfVL) |  | 0 |  | Shitong Shao, Zikai Zhou, Bai Lichen, Haoyi Xiong, Zeke Xie |  |
| 2896 |  |  [GLOMA: Global Video Text Spotting with Morphological Association](https://openreview.net/forum?id=tMKibc9Uxi) |  | 0 |  | Han Wang, Yanjie Wang, Yang Li, Can Huang |  |
| 2897 |  |  [Robust Representation Consistency Model via Contrastive Denoising](https://openreview.net/forum?id=armbJRJdrH) |  | 0 |  | Jiachen Lei, Julius Berner, Jiongxiao Wang, Zhongzhu Chen, Chaowei Xiao, Zhongjie Ba, Kui Ren, Jun Zhu, Anima Anandkumar |  |
| 2898 |  |  [CtD: Composition through Decomposition in Emergent Communication](https://openreview.net/forum?id=KlalQu2423) |  | 0 |  | Boaz Carmeli, Ron Meir, Yonatan Belinkov |  |
| 2899 |  |  [Geometry of Lightning Self-Attention: Identifiability and Dimension](https://openreview.net/forum?id=XtY3xYQWcW) |  | 0 |  | Nathan W. Henry, Giovanni Luca Marchetti, Kathlén Kohn |  |
| 2900 |  |  [PharmacoMatch: Efficient 3D Pharmacophore Screening via Neural Subgraph Matching](https://openreview.net/forum?id=27Qk18IZum) |  | 0 |  | Daniel Rose, Oliver Wieder, Thomas Seidel, Thierry Langer |  |
| 2901 |  |  [Animate Your Thoughts: Reconstruction of Dynamic Natural Vision from Human Brain Activity](https://openreview.net/forum?id=BpfsxFqhGa) |  | 0 |  | Yizhuo Lu, Changde Du, Chong Wang, Xuanliu Zhu, Liuyun Jiang, Xujin Li, Huiguang He |  |
| 2902 |  |  [Mining your own secrets: Diffusion Classifier Scores for Continual Personalization of Text-to-Image Diffusion Models](https://openreview.net/forum?id=hUdLs6TqZL) |  | 0 |  | Saurav Jha, Shiqi Yang, Masato Ishii, Mengjie Zhao, Christian Simon, Muhammad Jehanzeb Mirza, Dong Gong, Lina Yao, Shusuke Takahashi, Yuki Mitsufuji |  |
| 2903 |  |  [On the Optimal Memorization Capacity of Transformers](https://openreview.net/forum?id=UGVYezlLcZ) |  | 0 |  | Tokio Kajitsuka, Issei Sato |  |
| 2904 |  |  [Edge-aware Image Smoothing with Relative Wavelet Domain Representation](https://openreview.net/forum?id=0UO1mH3Iwv) |  | 0 |  | Huiqing Qi, Xiaoliu Luo, Tingting Li, Fang Li |  |
| 2905 |  |  [Action Sequence Augmentation for Action Anticipation](https://openreview.net/forum?id=f3CdjpPkSq) |  | 0 |  | Yihui Qiu, Deepu Rajan |  |
| 2906 |  |  [Optimal Brain Apoptosis](https://openreview.net/forum?id=88rjm6AXoC) |  | 0 |  | Mingyuan Sun, Zheng Fang, Jiaxu Wang, Junjie Jiang, Delei Kong, Chenming Hu, Yuetong Fang, Renjing Xu |  |
| 2907 |  |  [RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards](https://openreview.net/forum?id=Pnktu2PBXD) |  | 0 |  | Xinze Li, Sen Mei, Zhenghao Liu, Yukun Yan, Shuo Wang, Shi Yu, Zheni Zeng, Hao Chen, Ge Yu, Zhiyuan Liu, Maosong Sun, Chenyan Xiong |  |
| 2908 |  |  [Selective Aggregation for Low-Rank Adaptation in Federated Learning](https://openreview.net/forum?id=iX3uESGdsO) |  | 0 |  | Pengxin Guo, Shuang Zeng, Yanran Wang, Huijie Fan, Feifei Wang, Liangqiong Qu |  |
| 2909 |  |  [O(d/T) Convergence Theory for Diffusion Probabilistic Models under Minimal Assumptions](https://openreview.net/forum?id=4EjdYiNRzE) |  | 0 |  | Gen Li, Yuling Yan |  |
| 2910 |  |  [DebGCD: Debiased Learning with Distribution Guidance for Generalized Category Discovery](https://openreview.net/forum?id=9B8o9AxSyb) |  | 0 |  | Yuanpei Liu, Kai Han |  |
| 2911 |  |  [GeoX: Geometric Problem Solving Through Unified Formalized Vision-Language Pre-training](https://openreview.net/forum?id=6RiBl5sCDF) |  | 0 |  | Renqiu Xia, Mingsheng Li, Hancheng Ye, Wenjie Wu, Hongbin Zhou, Jiakang Yuan, Tianshuo Peng, Xinyu Cai, Xiangchao Yan, Bin Wang, Conghui He, Botian Shi, Tao Chen, Junchi Yan, Bo Zhang |  |
| 2912 |  |  [3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary Affordance Detection in 3D Worlds](https://openreview.net/forum?id=GThTiuXgDC) |  | 0 |  | Hengshuo Chu, Xiang Deng, Qi Lv, Xiaoyang Chen, Yinchuan Li, Jianye Hao, Liqiang Nie |  |
| 2913 |  |  [OmniKV: Dynamic Context Selection for Efficient Long-Context LLMs](https://openreview.net/forum?id=ulCAPXYXfa) |  | 0 |  | Jitai Hao, Yuke Zhu, Tian Wang, Jun Yu, Xin Xin, Bo Zheng, Zhaochun Ren, Sheng Guo |  |
| 2914 |  |  [Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement Learning](https://openreview.net/forum?id=X2x2DuGIbx) |  | 0 |  | Shijie Liu, Andrew Craig Cullen, Paul Montague, Sarah Monazam Erfani, Benjamin I. P. Rubinstein |  |
| 2915 |  |  [Two Sparse Matrices are Better than One: Sparsifying Neural Networks with Double Sparse Factorization](https://openreview.net/forum?id=DwiwOcK1B7) |  | 0 |  | Vladimír Boza, Vladimír Macko |  |
| 2916 |  |  [Tuning Timestep-Distilled Diffusion Model Using Pairwise Sample Optimization](https://openreview.net/forum?id=fXnE4gB64o) |  | 0 |  | Zichen Miao, Zhengyuan Yang, Kevin Lin, Ze Wang, Zicheng Liu, Lijuan Wang, Qiang Qiu |  |
| 2917 |  |  [LoCA: Location-Aware Cosine Adaptation for Parameter-Efficient Fine-Tuning](https://openreview.net/forum?id=4NRjdISWby) |  | 0 |  | Zhekai Du, Yinjie Min, Jingjing Li, Ke Lu, Changliang Zou, Liuhua Peng, Tingjin Chu, Mingming Gong |  |
| 2918 |  |  [Build-A-Scene: Interactive 3D Layout Control for Diffusion-Based Image Generation](https://openreview.net/forum?id=gg6dPtdC1C) |  | 0 |  | Abdelrahman Eldesokey, Peter Wonka |  |
| 2919 |  |  [Learning Dynamics of Deep Matrix Factorization Beyond the Edge of Stability](https://openreview.net/forum?id=J4Dvxv7WnG) |  | 0 |  | Avrajit Ghosh, Soo Min Kwon, Rongrong Wang, Saiprasad Ravishankar, Qing Qu |  |
| 2920 |  |  [Intent3D: 3D Object Detection in RGB-D Scans Based on Human Intention](https://openreview.net/forum?id=5GgjiRzYp3) |  | 0 |  | Weitai Kang, Mengxue Qu, Jyoti Kini, Yunchao Wei, Mubarak Shah, Yan Yan |  |
| 2921 |  |  [On the Adversarial Risk of Test Time Adaptation: An Investigation into Realistic Test-Time Data Poisoning](https://openreview.net/forum?id=7893vsQenk) |  | 0 |  | Yongyi Su, Yushu Li, Nanqing Liu, Kui Jia, Xulei Yang, ChuanSheng Foo, Xun Xu |  |
| 2922 |  |  [GenXD: Generating Any 3D and 4D Scenes](https://openreview.net/forum?id=1ThYY28HXg) |  | 0 |  | Yuyang Zhao, ChungChing Lin, Kevin Lin, Zhiwen Yan, Linjie Li, Zhengyuan Yang, Jianfeng Wang, Gim Hee Lee, Lijuan Wang |  |
| 2923 |  |  [Latent Action Pretraining from Videos](https://openreview.net/forum?id=VYOe2eBQeh) |  | 0 |  | Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Se June Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, YuWei Chao, Bill Yuchen Lin, Lars Liden, Kimin Lee, Jianfeng Gao, Luke Zettlemoyer, Dieter Fox, Minjoon Seo |  |
| 2924 |  |  [Open-CK: A Large Multi-Physics Fields Coupling benchmarks in Combustion Kinetics](https://openreview.net/forum?id=A23C57icJt) |  | 0 |  | Zaige Fei, Fan Xu, Junyuan Mao, Yuxuan Liang, Qingsong Wen, Kun Wang, Hao Wu, Yang Wang |  |
| 2925 |  |  [Motion-Agent: A Conversational Framework for Human Motion Generation with LLMs](https://openreview.net/forum?id=AvOhBgsE5R) |  | 0 |  | Qi Wu, Yubo Zhao, Yifan Wang, Xinhang Liu, YuWing Tai, ChiKeung Tang |  |
| 2926 |  |  [Morphing Tokens Draw Strong Masked Image Models](https://openreview.net/forum?id=d7q9IGj2p0) |  | 0 |  | Taekyung Kim, Byeongho Heo, Dongyoon Han |  |
| 2927 |  |  [Longhorn: State Space Models are Amortized Online Learners](https://openreview.net/forum?id=8jOqCcLzeO) |  | 0 |  | Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, Qiang Liu |  |
| 2928 |  |  [CaPo: Cooperative Plan Optimization for Efficient Embodied Multi-Agent Cooperation](https://openreview.net/forum?id=KRv9NubipP) |  | 0 |  | Jie Liu, Pan Zhou, Yingjun Du, AhHwee Tan, Cees G. M. Snoek, JanJakob Sonke, Efstratios Gavves |  |
| 2929 |  |  [SMITE: Segment Me In TimE](https://openreview.net/forum?id=KW6B6s1X82) |  | 0 |  | Amirhossein Alimohammadi, Sauradip Nag, Saeid Asgari Taghanaki, Andrea Tagliasacchi, Ghassan Hamarneh, Ali MahdaviAmiri |  |
| 2930 |  |  [Interactive Speculative Planning: Enhance Agent Efficiency through Co-design of System and User Interface](https://openreview.net/forum?id=BwR8t91yqh) |  | 0 |  | Wenyue Hua, Mengting Wan, Jagannath Shashank Subramanya Sai Vadrevu, Ryan Nadel, Yongfeng Zhang, Chi Wang |  |
| 2931 |  |  [MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models](https://openreview.net/forum?id=s5epFPdIW6) |  | 0 |  | Peng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James Zou, Huaxiu Yao |  |
| 2932 |  |  [Controllable Satellite-to-Street-View Synthesis with Precise Pose Alignment and Zero-Shot Environmental Control](https://openreview.net/forum?id=f92M45YRfh) |  | 0 |  | Xianghui Ze, Zhenbo Song, Qiwei Wang, Jianfeng Lu, Yujiao Shi |  |
| 2933 |  |  [NoVo: Norm Voting off Hallucinations with Attention Heads in Large Language Models](https://openreview.net/forum?id=yaOe2xBcLC) |  | 0 |  | Zheng Yi Ho, Siyuan Liang, Sen Zhang, Yibing Zhan, Dacheng Tao |  |
| 2934 |  |  [CPSample: Classifier Protected Sampling for Guarding Training Data During Diffusion](https://openreview.net/forum?id=LIBLIlk5M9) |  | 0 |  | Joshua Kazdan, Hao Sun, Jiaqi Han, Felix Petersen, Frederick Vu, Stefano Ermon |  |
| 2935 |  |  [Leveraging Flatness to Improve Information-Theoretic Generalization Bounds for SGD](https://openreview.net/forum?id=pSdE7PIA64) |  | 0 |  | Ze Peng, Jian Zhang, Yisen Wang, Lei Qi, Yinghuan Shi, Yang Gao |  |
| 2936 |  |  [BTBS-LNS: Binarized-Tightening, Branch and Search on Learning LNS Policies for MIP](https://openreview.net/forum?id=siHHqDDzvS) |  | 0 |  | Hao Yuan, Wenli Ouyang, Changwen Zhang, Yong Sun, Liming Gong, Junchi Yan |  |
| 2937 |  |  [Extending Mercer's expansion to indefinite and asymmetric kernels](https://openreview.net/forum?id=jZwwMxG8PO) |  | 0 |  | Sungwoo Jeong, Alex Townsend |  |
| 2938 |  |  [A Statistical Approach for Controlled Training Data Detection](https://openreview.net/forum?id=XAN8G0rvoB) |  | 0 |  | Zirui Hu, Yingjie Wang, Zheng Zhang, Hong Chen, Dacheng Tao |  |
| 2939 |  |  [Discriminating image representations with principal distortions](https://openreview.net/forum?id=ugXGFCS6HK) |  | 0 |  | Jenelle Feather, David Lipshutz, Sarah E. Harvey, Alex H. Williams, Eero P. Simoncelli |  |
| 2940 |  |  [PixWizard: Versatile Image-to-Image Visual Assistant with Open-Language Instructions](https://openreview.net/forum?id=xuQSp75HmP) |  | 0 |  | Weifeng Lin, Xinyu Wei, Renrui Zhang, Le Zhuo, Shitian Zhao, Siyuan Huang, Junlin Xie, Peng Gao, Hongsheng Li |  |
| 2941 |  |  [SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation](https://openreview.net/forum?id=uQjySppU9x) |  | 0 |  | Koichi Namekata, Sherwin Bahmani, Ziyi Wu, Yash Kant, Igor Gilitschenski, David B. Lindell |  |
| 2942 |  |  [PvNeXt: Rethinking Network Design and Temporal Motion for Point Cloud Video Recognition](https://openreview.net/forum?id=ZsU52Zkzjr) |  | 0 |  | Jie Wang, Tingfa Xu, Lihe Ding, Xinjie Zhang, Long Bai, Jianan Li |  |
| 2943 |  |  [Beyond Graphs: Can Large Language Models Comprehend Hypergraphs?](https://openreview.net/forum?id=28qOQwjuma) |  | 0 |  | Yifan Feng, Chengwu Yang, Xingliang Hou, Shaoyi Du, Shihui Ying, Zongze Wu, Yue Gao |  |
| 2944 |  |  [How Much is a Noisy Image Worth? Data Scaling Laws for Ambient Diffusion](https://openreview.net/forum?id=qZwtPEw2qN) |  | 0 |  | Giannis Daras, Yeshwanth Cherapanamjeri, Constantinos Daskalakis |  |
| 2945 |  |  [High-dimension Prototype is a Better Incremental Object Detection Learner](https://openreview.net/forum?id=6T8czSBWce) |  | 0 |  | Yanjie Wang, Liqun Chen, Tianming Zhao, Tao Zhang, Guodong Wang, Luxin Yan, Sheng Zhong, Jiahuan Zhou, Xu Zou |  |
| 2946 |  |  [HG-Adapter: Improving Pre-Trained Heterogeneous Graph Neural Networks with Dual Adapters](https://openreview.net/forum?id=AEglX9CHFN) |  | 0 |  | Yujie Mo, Runpeng Yu, Xiaofeng Zhu, Xinchao Wang |  |
| 2947 |  |  [HyperPLR: Hypergraph Generation through Projection, Learning, and Reconstruction](https://openreview.net/forum?id=TYnne6Pa35) |  | 0 |  | Weihuang Wen, Tianshu Yu |  |
| 2948 |  |  [Continuity-Preserving Convolutional Autoencoders for Learning Continuous Latent Dynamical Models from Images](https://openreview.net/forum?id=MxALfOAnXv) |  | 0 |  | Aiqing Zhu, Yuting Pan, Qianxiao Li |  |
| 2949 |  |  [SINGAPO: Single Image Controlled Generation of Articulated Parts in Objects](https://openreview.net/forum?id=OdMqKszKSd) |  | 0 |  | Jiayi Liu, Denys Iliash, Angel X. Chang, Manolis Savva, Ali Mahdavi Amiri |  |
| 2950 |  |  [Learning to Select Nodes in Branch and Bound with Sufficient Tree Representation](https://openreview.net/forum?id=gyvYKLEm8t) |  | 0 |  | Sijia Zhang, Shuli Zeng, Shaoang Li, Feng Wu, Xiangyang Li |  |
| 2951 |  |  [Weakly Supervised Video Scene Graph Generation via Natural Language Supervision](https://openreview.net/forum?id=GQgPj1H4pO) |  | 0 |  | Kibum Kim, Kanghoon Yoon, Yeonjun In, Jaehyeong Jeon, Jinyoung Moon, Donghyun Kim, Chanyoung Park |  |
| 2952 |  |  [GameArena: Evaluating LLM Reasoning through Live Computer Games](https://openreview.net/forum?id=SeQ8l8xo1r) |  | 0 |  | Lanxiang Hu, Qiyu Li, Anze Xie, Nan Jiang, Ion Stoica, Haojian Jin, Hao Zhang |  |
| 2953 |  |  [T2V-Turbo-v2: Enhancing Video Model Post-Training through Data, Reward, and Conditional Guidance Design](https://openreview.net/forum?id=BZwXMqu4zG) |  | 0 |  | Jiachen Li, Qian Long, Jian Zheng, Xiaofeng Gao, Robinson Piramuthu, Wenhu Chen, William Yang Wang |  |
| 2954 |  |  [Diffusion Policy Policy Optimization](https://openreview.net/forum?id=mEpqHvbD2h) |  | 0 |  | Allen Z. Ren, Justin Lidard, Lars Lien Ankile, Anthony Simeonov, Pulkit Agrawal, Anirudha Majumdar, Benjamin Burchfiel, Hongkai Dai, Max Simchowitz |  |
| 2955 |  |  [Horizon Generalization in Reinforcement Learning](https://openreview.net/forum?id=BH8Nrt2dPf) |  | 0 |  | Vivek Myers, Catherine Ji, Benjamin Eysenbach |  |
| 2956 |  |  [OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision](https://openreview.net/forum?id=Hlm0cga0sv) |  | 0 |  | Cong Wei, Zheyang Xiong, Weiming Ren, Xeron Du, Ge Zhang, Wenhu Chen |  |
| 2957 |  |  [From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory Representation for LLMs](https://openreview.net/forum?id=moXtEmCleY) |  | 0 |  | Alireza Rezazadeh, Zichao Li, Wei Wei, Yujia Bao |  |
| 2958 |  |  [AstroCompress: A benchmark dataset for multi-purpose compression of astronomical data](https://openreview.net/forum?id=kQCHCkNk7s) |  | 0 |  | Tuan Truong, Rithwik Sudharsan, Yibo Yang, Peter Xiangyuan Ma, Ruihan Yang, Stephan Mandt, Joshua S. Bloom |  |
| 2959 |  |  [CURIE: Evaluating LLMs on Multitask Scientific Long-Context Understanding and Reasoning](https://openreview.net/forum?id=jw2fC6REUB) |  | 0 |  | Hao Cui, Zahra Shamsi, Gowoon Cheon, Xuejian Ma, Shutong Li, Maria Tikhanovskaya, Peter Christian Norgaard, Nayantara Mudur, Martyna Beata Plomecka, Paul Raccuglia, Yasaman Bahri, Victor V. Albert, Pranesh Srinivasan, Haining Pan, Philippe Faist, Brian Rohr, Michael J. Statt, Dan Morris, Drew Purves, Elise Kleeman, et al. |  |
| 2960 |  |  [Truncated Consistency Models](https://openreview.net/forum?id=ZYDEJEvCbv) |  | 0 |  | Sangyun Lee, Yilun Xu, Tomas Geffner, Giulia Fanti, Karsten Kreis, Arash Vahdat, Weili Nie |  |
| 2961 |  |  [Elucidating the Preconditioning in Consistency Distillation](https://openreview.net/forum?id=55pCDKiS8B) |  | 0 |  | Kaiwen Zheng, Guande He, Jianfei Chen, Fan Bao, Jun Zhu |  |
| 2962 |  |  [Diffusion Bridge Implicit Models](https://openreview.net/forum?id=eghAocvqBk) |  | 0 |  | Kaiwen Zheng, Guande He, Jianfei Chen, Fan Bao, Jun Zhu |  |
| 2963 |  |  [Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling](https://openreview.net/forum?id=CTC7CmirNr) |  | 0 |  | Kaiwen Zheng, Yongxin Chen, Hanzi Mao, MingYu Liu, Jun Zhu, Qinsheng Zhang |  |
| 2964 |  |  [Synthio: Augmenting Small-Scale Audio Classification Datasets with Synthetic Data](https://openreview.net/forum?id=bR1J7SpzrD) |  | 0 |  | Sreyan Ghosh, Sonal Kumar, Zhifeng Kong, Rafael Valle, Bryan Catanzaro, Dinesh Manocha |  |
| 2965 |  |  [CREAM: Consistency Regularized Self-Rewarding Language Models](https://openreview.net/forum?id=Vf6RDObyEF) |  | 0 |  | Zhaoyang Wang, Weilei He, Zhiyuan Liang, Xuchao Zhang, Chetan Bansal, Ying Wei, Weitong Zhang, Huaxiu Yao |  |
| 2966 |  |  [PICASO: Permutation-Invariant Context Composition with State Space Models](https://openreview.net/forum?id=88TC1AWV27) |  | 0 |  | Tian Yu Liu, Alessandro Achille, Matthew Trager, Aditya Golatkar, Luca Zancato, Stefano Soatto |  |
| 2967 |  |  [Swiss Army Knife: Synergizing Biases in Knowledge from Vision Foundation Models for Multi-Task Learning](https://openreview.net/forum?id=eePww5u7J3) |  | 0 |  | Yuxiang Lu, Shengcao Cao, YuXiong Wang |  |
| 2968 |  |  [Neural Eulerian Scene Flow Fields](https://openreview.net/forum?id=0CieWy9ONY) |  | 0 |  | Kyle Vedder, Neehar Peri, Ishan Khatri, Siyi Li, Eric Eaton, Mehmet Kemal Kocamaz, Yue Wang, Zhiding Yu, Deva Ramanan, Joachim Pehserl |  |
| 2969 |  |  [DCT-CryptoNets: Scaling Private Inference in the Frequency Domain](https://openreview.net/forum?id=lPJUQsSIxm) |  | 0 |  | Arjun Roy, Kaushik Roy |  |
| 2970 |  |  [A Large-scale Training Paradigm for Graph Generative Models](https://openreview.net/forum?id=c01YB8pF0s) |  | 0 |  | Yu Wang, Ryan A. Rossi, Namyong Park, Huiyuan Chen, Nesreen K. Ahmed, Puja Trivedi, Franck Dernoncourt, Danai Koutra, Tyler Derr |  |
| 2971 |  |  [Beyond Model Collapse: Scaling Up with Synthesized Data Requires Verification](https://openreview.net/forum?id=MQXrTMonT1) |  | 0 |  | Yunzhen Feng, Elvis Dohmatob, Pu Yang, François Charton, Julia Kempe |  |
| 2972 |  |  [OmniPhysGS: 3D Constitutive Gaussians for General Physics-Based Dynamics Generation](https://openreview.net/forum?id=9HZtP6I5lv) |  | 0 |  | Yuchen Lin, Chenguo Lin, Jianjin Xu, Yadong Mu |  |
| 2973 |  |  [Graph Neural Networks Are More Than Filters: Revisiting and Benchmarking from A Spectral Perspective](https://openreview.net/forum?id=nWdQX5hOL9) |  | 0 |  | Yushun Dong, Patrick Soga, Yinhan He, Song Wang, Jundong Li |  |
| 2974 |  |  [Deep Kernel Posterior Learning under Infinite Variance Prior Weights](https://openreview.net/forum?id=usFdPd4Ghs) |  | 0 |  | Jorge Loría, Anindya Bhadra |  |
| 2975 |  |  [Differentially private optimization for non-decomposable objective functions](https://openreview.net/forum?id=F52tAK5Gbg) |  | 0 |  | Weiwei Kong, Andrés Muñoz Medina, Mónica Ribero |  |
| 2976 |  |  [Differentially private learners for heterogeneous treatment effects](https://openreview.net/forum?id=1z3SOCwst9) |  | 0 |  | Maresa Schröder, Valentyn Melnychuk, Stefan Feuerriegel |  |
| 2977 |  |  [Predicate Hierarchies Improve Few-Shot State Classification](https://openreview.net/forum?id=lxu8Vz6cLs) |  | 0 |  | Emily Jin, Joy Hsu, Jiajun Wu |  |
| 2978 |  |  [A Watermark for Order-Agnostic Language Models](https://openreview.net/forum?id=Nlm3Xf0W9S) |  | 0 |  | Ruibo Chen, Yihan Wu, Yanshuo Chen, Chenxi Liu, Junfeng Guo, Heng Huang |  |
| 2979 |  |  [What Makes a Maze Look Like a Maze?](https://openreview.net/forum?id=Iz75SDbRmm) |  | 0 |  | Joy Hsu, Jiayuan Mao, Joshua B. Tenenbaum, Noah D. Goodman, Jiajun Wu |  |
| 2980 |  |  [SV4D: Dynamic 3D Content Generation with Multi-Frame and Multi-View Consistency](https://openreview.net/forum?id=tJoS2d0Onf) |  | 0 |  | Yiming Xie, ChunHan Yao, Vikram Voleti, Huaizu Jiang, Varun Jampani |  |
| 2981 |  |  [No Free Lunch: Fundamental Limits of Learning Non-Hallucinating Generative Models](https://openreview.net/forum?id=OwNoTs2r8e) |  | 0 |  | Changlong Wu, Ananth Grama, Wojciech Szpankowski |  |
| 2982 |  |  [Accelerating Training with Neuron Interaction and Nowcasting Networks](https://openreview.net/forum?id=cUFIil6hEG) |  | 0 |  | Boris Knyazev, Abhinav Moudgil, Guillaume Lajoie, Eugene Belilovsky, Simon LacosteJulien |  |
| 2983 |  |  [Unbounded: A Generative Infinite Game of Character Life Simulation](https://openreview.net/forum?id=uy31tqVuNo) |  | 0 |  | Jialu Li, Yuanzhen Li, Neal Wadhwa, Yael Pritch, David E. Jacobs, Michael Rubinstein, Mohit Bansal, Nataniel Ruiz |  |
| 2984 |  |  [Privately Counting Partially Ordered Data](https://openreview.net/forum?id=hVTaXJ0I5M) |  | 0 |  | Matthew Joseph, Mónica Ribero, Alexander Yu |  |
| 2985 |  |  [Convergent Privacy Loss of Noisy-SGD without Convexity and Smoothness](https://openreview.net/forum?id=kjmLabjSE2) |  | 0 |  | Eli Chien, Pan Li |  |
| 2986 |  |  [Semialgebraic Neural Networks: From roots to representations](https://openreview.net/forum?id=zboCXnuNv7) |  | 0 |  | S. David Mis, Matti Lassas, Maarten V. de Hoop |  |
| 2987 |  |  [OPTAMI: Global Superlinear Convergence of High-order Methods](https://openreview.net/forum?id=Cpr6Wv2tfr) |  | 0 |  | Dmitry Kamzolov, Artem Agafonov, Dmitry Pasechnyuk, Alexander V. Gasnikov, Martin Takác |  |
| 2988 |  |  [On the Almost Sure Convergence of the Stochastic Three Points Algorithm](https://openreview.net/forum?id=N8tJmhCw25) |  | 0 |  | Taha el Bakkali el Kadi, Omar Saadi |  |
| 2989 |  |  [Metric-Driven Attributions for Vision Transformers](https://openreview.net/forum?id=rGP2jbWt0l) |  | 0 |  | Chase Walker, Sumit Kumar Jha, Rickard Ewetz |  |
| 2990 |  |  [Maximizing the Potential of Synthetic Data: Insights from Random Matrix Theory](https://openreview.net/forum?id=I9Dsq0cVo9) |  | 0 |  | Aymane El Firdoussi, Mohamed El Amine Seddik, Soufiane Hayou, Réda Alami, Ahmed Alzubaidi, Hakim Hacid |  |
| 2991 |  |  [Simple Guidance Mechanisms for Discrete Diffusion Models](https://openreview.net/forum?id=i5MrJ6g5G1) |  | 0 |  | Yair Schiff, Subham Sekhar Sahoo, Hao Phung, Guanghan Wang, Sam Boshar, Hugo Dallatorre, Bernardo P. de Almeida, Alexander M. Rush, Thomas Pierrot, Volodymyr Kuleshov |  |
| 2992 |  |  [STAMP: Scalable Task- And Model-agnostic Collaborative Perception](https://openreview.net/forum?id=8NdNniulYE) |  | 0 |  | Xiangbo Gao, Runsheng Xu, Jiachen Li, Ziran Wang, Zhiwen Fan, Zhengzhong Tu |  |
| 2993 |  |  [Constructing Confidence Intervals for Average Treatment Effects from Multiple Datasets](https://openreview.net/forum?id=BHFs80Jf5V) |  | 0 |  | Yuxin Wang, Maresa Schröder, Dennis Frauen, Jonas Schweisthal, Konstantin Hess, Stefan Feuerriegel |  |
| 2994 |  |  [Learning to Help in Multi-Class Settings](https://openreview.net/forum?id=NCgTbt2j1F) |  | 0 |  | Yu Wu, Yansong Li, Zeyu Dong, Nitya Sathyavageeswaran, Anand D. Sarwate |  |
| 2995 |  |  [Metalic: Meta-Learning In-Context with Protein Language Models](https://openreview.net/forum?id=TUKt7ag0qq) |  | 0 |  | Jacob Beck, Shikha Surana, Manus McAuliffe, Oliver Bent, Thomas D. Barrett, Juan Jose GarauLuis, Paul Duckworth |  |
| 2996 |  |  [Can We Ignore Labels in Out of Distribution Detection?](https://openreview.net/forum?id=falBlwUsIH) |  | 0 |  | Hong Yang, Qi Yu, Travis Desell |  |
| 2997 |  |  [CodePlan: Unlocking Reasoning Potential in Large Language Models by Scaling Code-form Planning](https://openreview.net/forum?id=dCPF1wlqj8) |  | 0 |  | Jiaxin Wen, Jian Guan, Hongning Wang, Wei Wu, Minlie Huang |  |
| 2998 |  |  [OATS: Outlier-Aware Pruning Through Sparse and Low Rank Decomposition](https://openreview.net/forum?id=DLDuVbxORA) |  | 0 |  | Stephen Zhang, Vardan Papyan |  |
| 2999 |  |  [Diffusion Models Are Real-Time Game Engines](https://openreview.net/forum?id=P8pqeEkn1H) |  | 0 |  | Dani Valevski, Yaniv Leviathan, Moab Arar, Shlomi Fruchter |  |
| 3000 |  |  [Efficient Perplexity Bound and Ratio Matching in Discrete Diffusion Language Models](https://openreview.net/forum?id=Mri9WIfxSm) |  | 0 |  | Etrit Haxholli, Yeti Ziya Gurbuz, Ogul Can, Eli Waxman |  |
| 3001 |  |  [Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats](https://openreview.net/forum?id=keu6sxrPWn) |  | 0 |  | Jiaxin Wen, Vivek Hebbar, Caleb Larson, Aryan Bhatt, Ansh Radhakrishnan, Mrinank Sharma, Henry Sleight, Shi Feng, He He, Ethan Perez, Buck Shlegeris, Akbir Khan |  |
| 3002 |  |  [Fast Training of Sinusoidal Neural Fields via Scaling Initialization](https://openreview.net/forum?id=Sr5XaZzirA) |  | 0 |  | Taesun Yeom, Sangyoon Lee, Jaeho Lee |  |
| 3003 |  |  [Language Models Learn to Mislead Humans via RLHF](https://openreview.net/forum?id=xJljiPE6dg) |  | 0 |  | Jiaxin Wen, Ruiqi Zhong, Akbir Khan, Ethan Perez, Jacob Steinhardt, Minlie Huang, Samuel R. Bowman, He He, Shi Feng |  |
| 3004 |  |  [Towards Semantic Equivalence of Tokenization in Multimodal LLM](https://openreview.net/forum?id=n64NYyc6rQ) |  | 0 |  | Shengqiong Wu, Hao Fei, Xiangtai Li, Jiayi Ji, Hanwang Zhang, TatSeng Chua, Shuicheng Yan |  |
| 3005 |  |  [ChemAgent: Self-updating Memories in Large Language Models Improves Chemical Reasoning](https://openreview.net/forum?id=kuhIqeVg0e) |  | 0 |  | Xiangru Tang, Tianyu Hu, Muyang Ye, Yanjun Shao, Xunjian Yin, Siru Ouyang, Wangchunshu Zhou, Pan Lu, Zhuosheng Zhang, Yilun Zhao, Arman Cohan, Mark Gerstein |  |
| 3006 |  |  [Node Similarities under Random Projections: Limits and Pathological Cases](https://openreview.net/forum?id=Frok9AItud) |  | 0 |  | Tvrtko Tadic, Cassiano O. Becker, Jennifer Neville |  |
| 3007 |  |  [Hadamrnn: Binary and Sparse Ternary orthogonal RNNs](https://openreview.net/forum?id=amOpepqmSl) |  | 0 |  | Armand Foucault, François Malgouyres, Franck Mamalet |  |
| 3008 |  |  [Style Outweighs Substance: Failure Modes of LLM Judges in Alignment Benchmarking](https://openreview.net/forum?id=MzHNftnAM1) |  | 0 |  | Benjamin Feuer, Micah Goldblum, Teresa Datta, Sanjana Nambiar, Raz Besaleli, Samuel Dooley, Max Cembalest, John P. Dickerson |  |
| 3009 |  |  [Learning Interpretable Hierarchical Dynamical Systems Models from Time Series Data](https://openreview.net/forum?id=Vp2OAxMs2s) |  | 0 |  | Manuel Brenner, Elias Weber, Georgia Koppe, Daniel Durstewitz |  |
| 3010 |  |  [Nonasymptotic Analysis of Stochastic Gradient Descent with the Richardson-Romberg Extrapolation](https://openreview.net/forum?id=Odtr1rzQMq) |  | 0 |  | Marina Sheshukova, Denis Belomestny, Alain Oliviero Durmus, Eric Moulines, Alexey Naumov, Sergey Samsonov |  |
| 3011 |  |  [SimpleTM: A Simple Baseline for Multivariate Time Series Forecasting](https://openreview.net/forum?id=oANkBaVci5) |  | 0 |  | Hui Chen, Viet Luong, Lopamudra Mukherjee, Vikas Singh |  |
| 3012 |  |  [Expand and Compress: Exploring Tuning Principles for Continual Spatio-Temporal Graph Forecasting](https://openreview.net/forum?id=FRzCIlkM7I) |  | 0 |  | Wei Chen, Yuxuan Liang |  |
| 3013 |  |  [Aligned Better, Listen Better for Audio-Visual Large Language Models](https://openreview.net/forum?id=1SYUKPeM12) |  | 0 |  | Yuxin Guo, Shuailei Ma, Shijie Ma, Xiaoyi Bao, ChenWei Xie, Kecheng Zheng, Tingyu Weng, Siyang Sun, Yun Zheng, Wei Zou |  |
| 3014 |  |  [OSCAR: Operating System Control via State-Aware Reasoning and Re-Planning](https://openreview.net/forum?id=VuTrZzrPfn) |  | 0 |  | Xiaoqiang Wang, Bang Liu |  |
| 3015 |  |  [Learning the Complexity of Weakly Noisy Quantum States](https://openreview.net/forum?id=tmSWFGpBb8) |  | 0 |  | Yusen Wu, Bujiao Wu, Yanqi Song, Xiao Yuan, Jingbo Wang |  |
| 3016 |  |  [Efficient Low-Bit Quantization with Adaptive Scales for Multi-Task Co-Training](https://openreview.net/forum?id=wA2RMD2AFq) |  | 0 |  | Boyu Liu, Haoyu Huang, Linlin Yang, Yanjing Li, Guodong Guo, Xianbin Cao, Baochang Zhang |  |
| 3017 |  |  [Qinco2: Vector Compression and Search with Improved Implicit Neural Codebooks](https://openreview.net/forum?id=2zMHHZ569S) |  | 0 |  | Théophane Vallaeys, Matthew J. Muckley, Jakob Verbeek, Matthijs Douze |  |
| 3018 |  |  [Balanced Neural ODEs: nonlinear model order reduction and Koopman operator approximations](https://openreview.net/forum?id=nA464tCGR5) |  | 0 |  | Julius Aka, Johannes Brunnemann, Jörg Eiden, Arne Speerforck, Lars Mikelsons |  |
| 3019 |  |  [Attention layers provably solve single-location regression](https://openreview.net/forum?id=DVlPp7Jd7P) |  | 0 |  | Pierre Marion, Raphaël Berthier, Gérard Biau, Claire Boyer |  |
| 3020 |  |  [Training One-Dimensional Graph Neural Networks is NP-Hard](https://openreview.net/forum?id=7BESdFZ7YA) |  | 0 |  | Robert Ganian, Mathis Rocton, Simon Wietheger |  |
| 3021 |  |  [Grounding Multimodal Large Language Model in GUI World](https://openreview.net/forum?id=M9iky9Ruhx) |  | 0 |  | Weixian Lei, Difei Gao, Mike Zheng Shou |  |
| 3022 |  |  [Zero-shot Imputation with Foundation Inference Models for Dynamical Systems](https://openreview.net/forum?id=NPSZ7V1CCY) |  | 0 |  | Patrick Seifner, Kostadin Cvejoski, Antonia Körner, Ramsés J. Sánchez |  |
| 3023 |  |  [PINP: Physics-Informed Neural Predictor with latent estimation of fluid flows](https://openreview.net/forum?id=vAuodZOQEZ) |  | 0 |  | Huaguan Chen, Yang Liu, Hao Sun |  |
| 3024 |  |  [GravMAD: Grounded Spatial Value Maps Guided Action Diffusion for Generalized 3D Manipulation](https://openreview.net/forum?id=qPzYF2EpXb) |  | 0 |  | Yangtao Chen, Zixuan Chen, Junhui Yin, Jing Huo, Pinzhuo Tian, Jieqi Shi, Yang Gao |  |
| 3025 |  |  [InstantSwap: Fast Customized Concept Swapping across Sharp Shape Differences](https://openreview.net/forum?id=UFrHWzZENz) |  | 0 |  | Chenyang Zhu, Kai Li, Yue Ma, Longxiang Tang, Chengyu Fang, Chubin Chen, Qifeng Chen, Xiu Li |  |
| 3026 |  |  [Explain Yourself, Briefly! Self-Explaining Neural Networks with Concise Sufficient Reasons](https://openreview.net/forum?id=8nuzsfiQfS) |  | 0 |  | Shahaf Bassan, Ron Eliav, Shlomit Gur |  |
| 3027 |  |  [Leveraging Submodule Linearity Enhances Task Arithmetic Performance in LLMs](https://openreview.net/forum?id=irPcM6X5FV) |  | 0 |  | Rui Dai, Sile Hu, Xu Shen, Yonggang Zhang, Xinmei Tian, Jieping Ye |  |
| 3028 |  |  [HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous Environment](https://openreview.net/forum?id=Cs6MrbFuMq) |  | 0 |  | Youhe Jiang, Ran Yan, Binhang Yuan |  |
| 3029 |  |  [FreqPrior: Improving Video Diffusion Models with Frequency Filtering Gaussian Noise](https://openreview.net/forum?id=8x0SGbCpzs) |  | 0 |  | Yunlong Yuan, Yuanfan Guo, Chunwei Wang, Wei Zhang, Hang Xu, Li Zhang |  |
| 3030 |  |  [PRDP: Progressively Refined Differentiable Physics](https://openreview.net/forum?id=9Fh0z1JmPU) |  | 0 |  | Kanishk Bhatia, Felix Koehler, Nils Thuerey |  |
| 3031 |  |  [Going Beyond Feature Similarity: Effective Dataset distillation based on Class-aware Conditional Mutual Information](https://openreview.net/forum?id=0no1Wp2R2j) |  | 0 |  | Xinhao Zhong, Bin Chen, Hao Fang, Xulin Gu, ShuTao Xia, EnHui Yang |  |
| 3032 |  |  [YOLO-RD: Introducing Relevant and Compact Explicit Knowledge to YOLO by Retriever-Dictionary](https://openreview.net/forum?id=KXDOmD7DM7) |  | 0 |  | HaoTang Tsui, ChienYao Wang, HongYuan Mark Liao |  |
| 3033 |  |  [Block-Attention for Efficient Prefilling](https://openreview.net/forum?id=7zNYY1E2fq) |  | 0 |  | Dongyang Ma, Yan Wang, Tian Lan |  |
| 3034 |  |  [Uncertainty modeling for fine-tuned implicit functions](https://openreview.net/forum?id=iZl0VqEdxa) |  | 0 |  | Anna Susmelj, Mael Macuglia, Natasa Tagasovska, Reto Sutter, Sebastiano Caprara, JeanPhilippe Thiran, Ender Konukoglu |  |
| 3035 |  |  [Learning Mask Invariant Mutual Information for Masked Image Modeling](https://openreview.net/forum?id=NoiaAT0eec) |  | 0 |  | Tao Huang, Yanxiang Ma, Shan You, Chang Xu |  |
| 3036 |  |  [UniRestore3D: A Scalable Framework For General Shape Restoration](https://openreview.net/forum?id=xPO6fwvldG) |  | 0 |  | Yuang Wang, Yujian Zhang, Sida Peng, Xingyi He, Haoyu Guo, Yujun Shen, Hujun Bao, Xiaowei Zhou |  |
| 3037 |  |  [Painting with Words: Elevating Detailed Image Captioning with Benchmark and Alignment Learning](https://openreview.net/forum?id=636M0nNbPs) |  | 0 |  | Qinghao Ye, Xianhan Zeng, Fu Li, Chunyuan Li, Haoqi Fan |  |
| 3038 |  |  [LLaMA-Omni: Seamless Speech Interaction with Large Language Models](https://openreview.net/forum?id=PYmrUQmMEw) |  | 0 |  | Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, Yang Feng |  |
| 3039 |  |  [TaskGalaxy: Scaling Multi-modal Instruction Fine-tuning with Tens of Thousands Vision Task Types](https://openreview.net/forum?id=JXgnnUC0PH) |  | 0 |  | Jiankang Chen, Tianke Zhang, Changyi Liu, Haojie Ding, Yaya Shi, Cheng Feng, Huihui Xiao, Bin Wen, Fan Yang, Tingting Gao, Di Zhang |  |
| 3040 |  |  [MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Masked Image Modeling Representations](https://openreview.net/forum?id=0PxLpVURTl) |  | 0 |  | Benedikt Alkin, Lukas Miklautz, Sepp Hochreiter, Johannes Brandstetter |  |
| 3041 |  |  [Exploiting Hidden Symmetry to Improve Objective Perturbation for DP Linear Learners with a Nonsmooth L1-Norm](https://openreview.net/forum?id=J863DxU7Sx) |  | 0 |  | Du Chen, Geoffrey A. Chua |  |
| 3042 |  |  [ESE: Espresso Sentence Embeddings](https://openreview.net/forum?id=plgLA2YBLH) |  | 0 |  | Xianming Li, Zongxi Li, Jing Li, Haoran Xie, Qing Li |  |
| 3043 |  |  [Vision-LSTM: xLSTM as Generic Vision Backbone](https://openreview.net/forum?id=SiH7DwNKZZ) |  | 0 |  | Benedikt Alkin, Maximilian Beck, Korbinian Pöppel, Sepp Hochreiter, Johannes Brandstetter |  |
| 3044 |  |  [On the Fourier analysis in the SO(3) space : the EquiLoPO Network](https://openreview.net/forum?id=LvTSvdiSwG) |  | 0 |  | Dmitrii Zhemchuzhnikov, Sergei Grudinin |  |
| 3045 |  |  [Breaking Class Barriers: Efficient Dataset Distillation via Inter-Class Feature Compensator](https://openreview.net/forum?id=X0CxfByJog) |  | 0 |  | Xin Zhang, Jiawei Du, Ping Liu, Joey Tianyi Zhou |  |
| 3046 |  |  [Robust Conformal Prediction with a Single Binary Certificate](https://openreview.net/forum?id=ltrxRX5t0H) |  | 0 |  | Soroush H. Zargarbashi, Aleksandar Bojchevski |  |
| 3047 |  |  [Selective induction Heads: How Transformers Select Causal Structures in Context](https://openreview.net/forum?id=bnJgzAQjWf) |  | 0 |  | Francesco D'Angelo, Francesco Croce, Nicolas Flammarion |  |
| 3048 |  |  [PEARL: Parallel Speculative Decoding with Adaptive Draft Length](https://openreview.net/forum?id=QOXrVMiHGK) |  | 0 |  | Tianyu Liu, Yun Li, Qitan Lv, Kai Liu, Jianchen Zhu, Winston Hu, Xiao Sun |  |
| 3049 |  |  [CityAnchor: City-scale 3D Visual Grounding with Multi-modality LLMs](https://openreview.net/forum?id=7nOl5W6xU4) |  | 0 |  | Jinpeng Li, Haiping Wang, Jiabin Chen, Yuan Liu, Zhiyang Dou, Yuexin Ma, Sibei Yang, Yuan Li, Wenping Wang, Zhen Dong, Bisheng Yang |  |
| 3050 |  |  [DriveTransformer: Unified Transformer for Scalable End-to-End Autonomous Driving](https://openreview.net/forum?id=M42KR4W9P5) |  | 0 |  | Xiaosong Jia, Junqi You, Zhiyuan Zhang, Junchi Yan |  |
| 3051 |  |  [TRACE: Temporal Grounding Video LLM via Causal Event Modeling](https://openreview.net/forum?id=14fFV0chUS) |  | 0 |  | Yongxin Guo, Jingyu Liu, Mingda Li, Qingbin Liu, Xi Chen, Xiaoying Tang |  |
| 3052 |  |  [RobuRCDet: Enhancing Robustness of Radar-Camera Fusion in Bird's Eye View for 3D Object Detection](https://openreview.net/forum?id=9xHlhKLu1h) |  | 0 |  | Jingtong Yue, Zhiwei Lin, Xin Lin, Xiaoyu Zhou, Xiangtai Li, Lu Qi, Yongtao Wang, MingHsuan Yang |  |
| 3053 |  |  [Transformers are Universal In-context Learners](https://openreview.net/forum?id=6S4WQD1LZR) |  | 0 |  | Takashi Furuya, Maarten V. de Hoop, Gabriel Peyré |  |
| 3054 |  |  [Autoregressive Video Generation without Vector Quantization](https://openreview.net/forum?id=JE9tCwe3lp) |  | 0 |  | Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, Xinlong Wang |  |
| 3055 |  |  [Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models](https://openreview.net/forum?id=rsZwwjYHuD) |  | 0 |  | Fushuo Huo, Wenchao Xu, Zhong Zhang, Haozhao Wang, Zhicheng Chen, Peilin Zhao |  |
| 3056 |  |  [Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models](https://openreview.net/forum?id=N5fVv6PZGz) |  | 0 |  | Keisuke Kamahori, Tian Tang, Yile Gu, Kan Zhu, Baris Kasikci |  |
| 3057 |  |  [Quantitative Approximation for Neural Operators in Nonlinear Parabolic Equations](https://openreview.net/forum?id=yUefexs79U) |  | 0 |  | Takashi Furuya, Koichi Taniguchi, Satoshi Okuda |  |
| 3058 |  |  [A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation](https://openreview.net/forum?id=wryFCrWB0A) |  | 0 |  | Liang Chen, Sinan Tan, Zefan Cai, Weichu Xie, Haozhe Zhao, Yichi Zhang, Junyang Lin, Jinze Bai, Tianyu Liu, Baobao Chang |  |
| 3059 |  |  [SiMHand: Mining Similar Hands for Large-Scale 3D Hand Pose Pre-training](https://openreview.net/forum?id=96jZFqM5E0) |  | 0 |  | Nie Lin, Takehiko Ohkawa, Yifei Huang, Mingfang Zhang, Minjie Cai, Ming Li, Ryosuke Furuta, Yoichi Sato |  |
| 3060 |  |  [Learning Structured Universe Graph with Outlier OOD Detection for Partial Matching](https://openreview.net/forum?id=dmjQLHufev) |  | 0 |  | Zetian Jiang, Jiaxin Lu, Haizhao Fan, Tianzhe Wang, Junchi Yan |  |
| 3061 |  |  [Layout-your-3D: Controllable and Precise 3D Generation with 2D Blueprint](https://openreview.net/forum?id=myolhJPuRI) |  | 0 |  | Junwei Zhou, Xueting Li, Lu Qi, MingHsuan Yang |  |
| 3062 |  |  [Youku Dense Caption: A Large-scale Chinese Video Dense Caption Dataset and Benchmarks](https://openreview.net/forum?id=vvi5OjPhbu) |  | 0 |  | Zixuan Xiong, Guangwei Xu, Wenkai Zhang, Yuan Miao, Xuan Wu, LinHai, Ruijie Guo, HaiTao Zheng |  |
| 3063 |  |  [ADMM for Structured Fractional Minimization](https://openreview.net/forum?id=DcZpQhVpp9) |  | 0 |  | Ganzhao Yuan |  |
| 3064 |  |  [TAU-106K: A New Dataset for Comprehensive Understanding of Traffic Accident](https://openreview.net/forum?id=Fb0q2uI4Ha) |  | 0 |  | Yixuan Zhou, Long Bai, Sijia Cai, Bing Deng, Xing Xu, Heng Tao Shen |  |
| 3065 |  |  [SAMRefiner: Taming Segment Anything Model for Universal Mask Refinement](https://openreview.net/forum?id=JlDx2xp01W) |  | 0 |  | Yuqi Lin, Hengjia Li, Wenqi Shao, Zheng Yang, Jun Zhao, Xiaofei He, Ping Luo, Kaipeng Zhang |  |
| 3066 |  |  [Rethinking Diffusion Posterior Sampling: From Conditional Score Estimator to Maximizing a Posterior](https://openreview.net/forum?id=GcvLoqOoXL) |  | 0 |  | Tongda Xu, Xiyan Cai, Xinjie Zhang, Xingtong Ge, Dailan He, Ming Sun, Jingjing Liu, YaQin Zhang, Jian Li, Yan Wang |  |
| 3067 |  |  [Benchmarking Agentic Workflow Generation](https://openreview.net/forum?id=vunPXOFmoi) |  | 0 |  | Shuofei Qiao, Runnan Fang, Zhisong Qiu, Xiaobin Wang, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen |  |
| 3068 |  |  [DreamCatalyst: Fast and High-Quality 3D Editing via Controlling Editability and Identity Preservation](https://openreview.net/forum?id=FA5ZAJlv96) |  | 0 |  | Jiwook Kim, Seonho Lee, Jaeyo Shin, Jiho Choi, Hyunjung Shim |  |
| 3069 |  |  [ADAPT: Attentive Self-Distillation and Dual-Decoder Prediction Fusion for Continual Panoptic Segmentation](https://openreview.net/forum?id=HF1UmIVv6a) |  | 0 |  | Ze Yang, Shichao Dong, Ruibo Li, Nan Song, Guosheng Lin |  |
| 3070 |  |  [DGQ: Distribution-Aware Group Quantization for Text-to-Image Diffusion Models](https://openreview.net/forum?id=ZyNEr7Xw5L) |  | 0 |  | Hyogon Ryu, NaHyeon Park, Hyunjung Shim |  |
| 3071 |  |  [NeuroLM: A Universal Multi-task Foundation Model for Bridging the Gap between Language and EEG Signals](https://openreview.net/forum?id=Io9yFt7XH7) |  | 0 |  | Weibang Jiang, Yansen Wang, BaoLiang Lu, Dongsheng Li |  |
| 3072 |  |  [N-ForGOT: Towards Not-forgetting and Generalization of Open Temporal Graph Learning](https://openreview.net/forum?id=rLlDt2FQvz) |  | 0 |  | Liping Wang, Xujia Li, Jingshu Peng, Yue Wang, Chen Zhang, Yan Zhou, Lei Chen |  |
| 3073 |  |  [Cafe-Talk: Generating 3D Talking Face Animation with Multimodal Coarse- and Fine-grained Control](https://openreview.net/forum?id=S7cWJkWqOi) |  | 0 |  | Hejia Chen, Haoxian Zhang, Shoulong Zhang, Xiaoqiang Liu, Sisi Zhuang, Yuan Zhang, Pengfei Wan, Di Zhang, Shuai Li |  |
| 3074 |  |  [MovieDreamer: Hierarchical Generation for Coherent Long Visual Sequences](https://openreview.net/forum?id=QHj2LL958o) |  | 0 |  | Canyu Zhao, Mingyu Liu, Wen Wang, Weihua Chen, Fan Wang, Hao Chen, Bo Zhang, Chunhua Shen |  |
| 3075 |  |  [The Optimization Landscape of SGD Across the Feature Learning Strength](https://openreview.net/forum?id=iEfdvDTcZg) |  | 0 |  | Alexander B. Atanasov, Alexandru Meterez, James B. Simon, Cengiz Pehlevan |  |
| 3076 |  |  [UNIP: Rethinking Pre-trained Attention Patterns for Infrared Semantic Segmentation](https://openreview.net/forum?id=Xq7gwsnhPT) |  | 0 |  | Tao Zhang, Jinyong Wen, Zhen Chen, Kun Ding, Shiming Xiang, Chunhong Pan |  |
| 3077 |  |  [Latent-EnSF: A Latent Ensemble Score Filter for High-Dimensional Data Assimilation with Sparse Observation Data](https://openreview.net/forum?id=urcEYsZOBz) |  | 0 |  | Phillip Si, Peng Chen |  |
| 3078 |  |  [Modeling Fine-Grained Hand-Object Dynamics for Egocentric Video Representation Learning](https://openreview.net/forum?id=P6G1Z6jkf3) |  | 0 |  | Baoqi Pei, Yifei Huang, Jilan Xu, Guo Chen, Yuping He, Lijin Yang, Yali Wang, Weidi Xie, Yu Qiao, Fei Wu, Limin Wang |  |
| 3079 |  |  [Rethinking Light Decoder-based Solvers for Vehicle Routing Problems](https://openreview.net/forum?id=4pRwkYpa2u) |  | 0 |  | Ziwei Huang, Jianan Zhou, Zhiguang Cao, Yixin Xu |  |
| 3080 |  |  [Scrutinize What We Ignore: Reining In Task Representation Shift Of Context-Based Offline Meta Reinforcement Learning](https://openreview.net/forum?id=Cr1XlGBGVm) |  | 0 |  | Hai Zhang, Boyuan Zheng, Tianying Ji, Jinhang Liu, Anqi Guo, Junqiao Zhao, Lanqing Li |  |
| 3081 |  |  [Federated Residual Low-Rank Adaptation of Large Language Models](https://openreview.net/forum?id=e0rQRMUhs7) |  | 0 |  | Yunlu Yan, ChunMei Feng, Wangmeng Zuo, Rick Siow Mong Goh, Yong Liu, Lei Zhu |  |
| 3082 |  |  [Strong Preferences Affect the Robustness of Preference Models and Value Alignment](https://openreview.net/forum?id=Upoxh7wvmJ) |  | 0 |  | Ziwei Xu, Mohan S. Kankanhalli |  |
| 3083 |  |  [HarmAug: Effective Data Augmentation for Knowledge Distillation of Safety Guard Models](https://openreview.net/forum?id=y3zswp3gek) |  | 0 |  | Seanie Lee, Haebin Seong, Dong Bok Lee, Minki Kang, Xiaoyin Chen, Dominik Wagner, Yoshua Bengio, Juho Lee, Sung Ju Hwang |  |
| 3084 |  |  [MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge](https://openreview.net/forum?id=v8qABSeeKO) |  | 0 |  | Yuntao Du, Kailin Jiang, Zhi Gao, Chenrui Shi, Zilong Zheng, Siyuan Qi, Qing Li |  |
| 3085 |  |  [Synergy and Diversity in CLIP: Enhancing Performance Through Adaptive Backbone Ensembling](https://openreview.net/forum?id=Zkq4fsyjfp) |  | 0 |  | Cristian Rodriguez Opazo, Ehsan Abbasnejad, Damien Teney, Hamed Damirchi, Edison MarreseTaylor, Anton van den Hengel |  |
| 3086 |  |  [A Black Swan Hypothesis: The Role of Human Irrationality in AI Safety](https://openreview.net/forum?id=7k4HVhUS9k) |  | 0 |  | Hyunin Lee, Chanwoo Park, David Abel, Ming Jin |  |
| 3087 |  |  [Local Patterns Generalize Better for Novel Anomalies](https://openreview.net/forum?id=4ua4wyAQLm) |  | 0 |  | Yalong Jiang |  |
| 3088 |  |  [Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving](https://openreview.net/forum?id=KmmNb7631I) |  | 0 |  | Jin Zhang, Flood Sung, Zhilin Yang, Yang Gao, Chongjie Zhang |  |
| 3089 |  |  [Minimax Optimal Two-Stage Algorithm For Moment Estimation Under Covariate Shift](https://openreview.net/forum?id=oc4yw7zX9T) |  | 0 |  | Zhen Zhang, Xin Liu, Shaoli Wang, Jiaye Teng |  |
| 3090 |  |  [The Labyrinth of Links: Navigating the Associative Maze of Multi-modal LLMs](https://openreview.net/forum?id=vJ0axKTh7t) |  | 0 |  | Hong Li, Nanxi Li, Yuanjie Chen, Jianbin Zhu, Qinlu Guo, Cewu Lu, YongLu Li |  |
| 3091 |  |  [Failures to Find Transferable Image Jailbreaks Between Vision-Language Models](https://openreview.net/forum?id=wvFnqVVUhN) |  | 0 |  | Rylan Schaeffer, Dan Valentine, Luke Bailey, James Chua, Cristóbal Eyzaguirre, Zane Durante, Joe Benton, Brando Miranda, Henry Sleight, Tony Tong Wang, John Hughes, Rajashree Agrawal, Mrinank Sharma, Scott Emmons, Sanmi Koyejo, Ethan Perez |  |
| 3092 |  |  [A Percolation Model of Emergence: Analyzing Transformers Trained on a Formal Language](https://openreview.net/forum?id=0pLCDJVVRD) |  | 0 |  | Ekdeep Singh Lubana, Kyogo Kawaguchi, Robert P. Dick, Hidenori Tanaka |  |
| 3093 |  |  [Sequential Stochastic Combinatorial Optimization Using Hierarchal Reinforcement Learning](https://openreview.net/forum?id=AloCXPpq54) |  | 0 |  | Xinsong Feng, Zihan Yu, Yanhai Xiong, Haipeng Chen |  |
| 3094 |  |  [CR2PQ: Continuous Relative Rotary Positional Query for Dense Visual Representation Learning](https://openreview.net/forum?id=3l6PwssLNY) |  | 0 |  | Shaofeng Zhang, Qiang Zhou, Sitong Wu, Haoru Tan, Zhibin Wang, Jinfa Huang, Junchi Yan |  |
| 3095 |  |  [Rethinking Visual Counterfactual Explanations Through Region Constraint](https://openreview.net/forum?id=gqeXXrIMr0) |  | 0 |  | Bartlomiej Sobieski, Jakub Grzywaczewski, Bartlomiej Sadlej, Matthew Tivnan, Przemyslaw Biecek |  |
| 3096 |  |  [Hidden in the Noise: Two-Stage Robust Watermarking for Images](https://openreview.net/forum?id=ll2nz6qwRG) |  | 0 |  | Kasra Arabi, Benjamin Feuer, R. Teal Witter, Chinmay Hegde, Niv Cohen |  |
| 3097 |  |  [From Lazy to Rich: Exact Learning Dynamics in Deep Linear Networks](https://openreview.net/forum?id=ZXaocmXc6d) |  | 0 |  | Clémentine Carla Juliette Dominé, Nicolas Anguita, Alexandra M. Proca, Lukas Braun, Daniel Kunin, Pedro A. M. Mediano, Andrew M. Saxe |  |
| 3098 |  |  [Dynamic Assortment Selection and Pricing with Censored Preference Feedback](https://openreview.net/forum?id=DOXnqYLCcd) |  | 0 |  | Junghun Kim, Minhwan Oh |  |
| 3099 |  |  [Release the Powers of Prompt Tuning: Cross-Modality Prompt Transfer](https://openreview.net/forum?id=SYnIf4LxAG) |  | 0 |  | Ningyuan Zhang, Jie Lu, Keqiuyin Li, Zhen Fang, Guangquan Zhang |  |
| 3100 |  |  [Temporal Flexibility in Spiking Neural Networks: Towards Generalization Across Time Steps and Deployment Friendliness](https://openreview.net/forum?id=9HsfTgflT7) |  | 0 |  | Kangrui Du, Yuhang Wu, Shikuang Deng, Shi Gu |  |
| 3101 |  |  [GOFA: A Generative One-For-All Model for Joint Graph Language Modeling](https://openreview.net/forum?id=mIjblC9hfm) |  | 0 |  | Lecheng Kong, Jiarui Feng, Hao Liu, Chengsong Huang, Jiaxin Huang, Yixin Chen, Muhan Zhang |  |
| 3102 |  |  [Group Distributionally Robust Dataset Distillation with Risk Minimization](https://openreview.net/forum?id=3JsU5QXNru) |  | 0 |  | Saeed Vahidian, Mingyu Wang, Jianyang Gu, Vyacheslav Kungurtsev, Wei Jiang, Yiran Chen |  |
| 3103 |  |  [CapeX: Category-Agnostic Pose Estimation from Textual Point Explanation](https://openreview.net/forum?id=scKAXgonmq) |  | 0 |  | Matan Rusanovsky, Or Hirschorn, Shai Avidan |  |
| 3104 |  |  [Slot-Guided Adaptation of Pre-trained Diffusion Models for Object-Centric Learning and Compositional Generation](https://openreview.net/forum?id=kZvor5aaz7) |  | 0 |  | Adil Kaan Akan, Yucel Yemez |  |
| 3105 |  |  [Strategic Classification With Externalities](https://openreview.net/forum?id=o6CXkEEttn) |  | 0 |  | Safwan Hossain, Evi Micha, Yiling Chen, Ariel D. Procaccia |  |
| 3106 |  |  [C-CLIP: Multimodal Continual Learning for Vision-Language Model](https://openreview.net/forum?id=sb7qHFYwBc) |  | 0 |  | Wenzhuo Liu, Fei Zhu, Longhui Wei, Qi Tian |  |
| 3107 |  |  [SpinQuant: LLM Quantization with Learned Rotations](https://openreview.net/forum?id=ogO6DGE6FZ) |  | 0 |  | Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, Tijmen Blankevoort |  |
| 3108 |  |  [Revisit Micro-batch Clipping: Adaptive Data Pruning via Gradient Manipulation](https://openreview.net/forum?id=pAkQhhn4vB) |  | 0 |  | Lun Wang |  |
| 3109 |  |  [Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?](https://openreview.net/forum?id=1Xg4JPPxJ0) |  | 0 |  | Yutong Yin, Zhaoran Wang |  |
| 3110 |  |  [Adding Conditional Control to Diffusion Models with Reinforcement Learning](https://openreview.net/forum?id=svp1EBA6hA) |  | 0 |  | Yulai Zhao, Masatoshi Uehara, Gabriele Scalia, SunYuan Kung, Tommaso Biancalani, Sergey Levine, Ehsan Hajiramezanali |  |
| 3111 |  |  [Spatial-Mamba: Effective Visual State Space Models via Structure-Aware State Fusion](https://openreview.net/forum?id=iDe1mtxqK5) |  | 0 |  | Chaodong Xiao, Minghan Li, Zhengqiang Zhang, Deyu Meng, Lei Zhang |  |
| 3112 |  |  [On the Optimization Landscape of Low Rank Adaptation Methods for Large Language Models](https://openreview.net/forum?id=pxclAomHat) |  | 0 |  | XuHui Liu, Yali Du, Jun Wang, Yang Yu |  |
| 3113 |  |  [Federated Class-Incremental Learning: A Hybrid Approach Using Latent Exemplars and Data-Free Techniques to Address Local and Global Forgetting](https://openreview.net/forum?id=ydREOIttdC) |  | 0 |  | Milad Khademi Nori, IlMin Kim, Guanghui Wang |  |
| 3114 |  |  [VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis](https://openreview.net/forum?id=Kb9PnkWYNT) |  | 0 |  | Yumeng Li, William H. Beluch, Margret Keuper, Dan Zhang, Anna Khoreva |  |
| 3115 |  |  [Towards Scalable Exact Machine Unlearning Using Parameter-Efficient Fine-Tuning](https://openreview.net/forum?id=oe51Q5Uo37) |  | 0 |  | Somnath Basu Roy Chowdhury, Krzysztof Marcin Choromanski, Arijit Sehanobish, Kumar Avinava Dubey, Snigdha Chaturvedi |  |
| 3116 |  |  [Understanding the Stability-based Generalization of Personalized Federated Learning](https://openreview.net/forum?id=znhZbonEoe) |  | 0 |  | Yingqi Liu, Qinglun Li, Jie Tang, Yifan Shi, Li Shen, Xiaochun Cao |  |
| 3117 |  |  [Adaptive Retention & Correction: Test-Time Training for Continual Learning](https://openreview.net/forum?id=9bLdbp46Q1) |  | 0 |  | Haoran Chen, Micah Goldblum, Zuxuan Wu, YuGang Jiang |  |
| 3118 |  |  [Unsupervised Model Tree Heritage Recovery](https://openreview.net/forum?id=QVj3kUvdvl) |  | 0 |  | Eliahu Horwitz, Asaf Shul, Yedid Hoshen |  |
| 3119 |  |  [Deep Linear Probe Generators for Weight Space Learning](https://openreview.net/forum?id=XoYdD3m0mv) |  | 0 |  | Jonathan Kahana, Eliahu Horwitz, Imri Shuval, Yedid Hoshen |  |
| 3120 |  |  [Do WGANs succeed because they minimize the Wasserstein Distance? Lessons from Discrete Generators](https://openreview.net/forum?id=7YXaOvunqo) |  | 0 |  | Ariel Elnekave, Yair Weiss |  |
| 3121 |  |  [Your Weak LLM is Secretly a Strong Teacher for Alignment](https://openreview.net/forum?id=sGqd1tF8P8) |  | 0 |  | Leitian Tao, Yixuan Li |  |
| 3122 |  |  [Beyond Canonicalization: How Tensorial Messages Improve Equivariant Message Passing](https://openreview.net/forum?id=vDp6StrKIq) |  | 0 |  | Peter Lippmann, Gerrit Gerhartz, Roman Remme, Fred A. Hamprecht |  |
| 3123 |  |  [Accelerating Diffusion Transformers with Token-wise Feature Caching](https://openreview.net/forum?id=yYZbZGo4ei) |  | 0 |  | Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, Linfeng Zhang |  |
| 3124 |  |  [Towards Unified Human Motion-Language Understanding via Sparse Interpretable Characterization](https://openreview.net/forum?id=Oh8MuCacJW) |  | 0 |  | Guangtao Lyu, Chenghao Xu, Jiexi Yan, Muli Yang, Cheng Deng |  |
| 3125 |  |  [WorkflowLLM: Enhancing Workflow Orchestration Capability of Large Language Models](https://openreview.net/forum?id=3Hy00Wvabi) |  | 0 |  | Shengda Fan, Xin Cong, Yuepeng Fu, Zhong Zhang, Shuyan Zhang, Yuanwei Liu, Yesai Wu, Yankai Lin, Zhiyuan Liu, Maosong Sun |  |
| 3126 |  |  [Intervening Anchor Token: Decoding Strategy in Alleviating Hallucinations for MLLMs](https://openreview.net/forum?id=zGb4WgCW5i) |  | 0 |  | Feilong Tang, Zile Huang, Chengzhi Liu, Qiang Sun, Harry Yang, SerNam Lim |  |
| 3127 |  |  [Estimation of single-cell and tissue perturbation effect in spatial transcriptomics via Spatial Causal Disentanglement](https://openreview.net/forum?id=Tqdsruwyac) |  | 0 |  | Stathis Megas, Daniel G. Chen, Krzysztof Polanski, Moshe Eliasof, CarolaBibiane Schönlieb, Sarah A. Teichmann |  |
| 3128 |  |  [ComPC: Completing a 3D Point Cloud with 2D Diffusion Priors](https://openreview.net/forum?id=SoUwcVplq4) |  | 0 |  | Tianxin Huang, Zhiwen Yan, Yuyang Zhao, Gim Hee Lee |  |
| 3129 |  |  [SINGER: Stochastic Network Graph Evolving Operator for High Dimensional PDEs](https://openreview.net/forum?id=wVADj7yKee) |  | 0 |  | Mingquan Feng, Yixin Huang, Weixin Liao, Yuhong Liu, Yizhou Liu, Junchi Yan |  |
| 3130 |  |  [PhysPDE: Rethinking PDE Discovery and a Physical Hypothesis Selection Benchmark](https://openreview.net/forum?id=G3CpBCQwNh) |  | 0 |  | Mingquan Feng, Yixin Huang, Yizhou Liu, Bofang Jiang, Junchi Yan |  |
| 3131 |  |  [Track-On: Transformer-based Online Point Tracking with Memory](https://openreview.net/forum?id=oRlANEuqG5) |  | 0 |  | Görkay Aydemir, Xiongyi Cai, Weidi Xie, Fatma Güney |  |
| 3132 |  |  [Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation](https://openreview.net/forum?id=C25SgeXWjE) |  | 0 |  | Chengwen Qi, Ren Ma, Bowen Li, He Du, Binyuan Hui, Jinwang Wu, Yuanjun Laili, Conghui He |  |
| 3133 |  |  [Revisiting Multi-Permutation Equivariance through the Lens of irreducible Representations](https://openreview.net/forum?id=4v4nmYWzBa) |  | 0 |  | Yonatan Sverdlov, Ido Springer, Nadav Dym |  |
| 3134 |  |  [On the Expressive Power of Sparse Geometric MPNNs](https://openreview.net/forum?id=NY7aEek0mi) |  | 0 |  | Yonatan Sverdlov, Nadav Dym |  |
| 3135 |  |  [Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)](https://openreview.net/forum?id=xkgfLXZ4e0) |  | 0 |  | Subba Reddy Oota, Akshett Rai Jindal, Ishani Mondal, Khushbu Pahwa, Satya Sai Srinath Namburi GNVV, Manish Shrivastava, Maneesh Kumar Singh, Bapi Raju Surampudi, Manish Gupta |  |
| 3136 |  |  [Immunogenicity Prediction with Dual Attention Enables Vaccine Target Selection](https://openreview.net/forum?id=hWmwL9gizZ) |  | 0 |  | Song Li, Yang Tan, Song Ke, Liang Hong, Bingxin Zhou |  |
| 3137 |  |  [Bad-PFL: Exploiting Backdoor Attacks against Personalized Federated Learning](https://openreview.net/forum?id=79nO2DPjVX) |  | 0 |  | Mingyuan Fan, Zhanyi Hu, Fuyi Wang, Cen Chen |  |
| 3138 |  |  [BrainOOD: Out-of-distribution Generalizable Brain Network Analysis](https://openreview.net/forum?id=3xqqYOKILp) |  | 0 |  | Jiaxing Xu, Yongqiang Chen, Xia Dong, Mengcheng Lan, Tiancheng Huang, Qingtian Bian, James Cheng, Yiping Ke |  |
| 3139 |  |  [WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio Language Modeling](https://openreview.net/forum?id=yBlVlS2Fd9) |  | 0 |  | Shengpeng Ji, Ziyue Jiang, Wen Wang, Yifu Chen, Minghui Fang, Jialong Zuo, Qian Yang, Xize Cheng, Zehan Wang, Ruiqi Li, Ziang Zhang, Xiaoda Yang, Rongjie Huang, Yidi Jiang, Qian Chen, Siqi Zheng, Zhou Zhao |  |
| 3140 |  |  [Integrative Decoding: Improving Factuality via Implicit Self-consistency](https://openreview.net/forum?id=gGWYecsK1U) |  | 0 |  | Yi Cheng, Xiao Liang, Yeyun Gong, Wen Xiao, Song Wang, Yuji Zhang, Wenjun Hou, Kaishuai Xu, Wenge Liu, Wenjie Li, Jian Jiao, Qi Chen, Peng Cheng, Wayne Xiong |  |
| 3141 |  |  [Structuring Benchmark into Knowledge Graphs to Assist Large Language Models in Retrieving and Designing Models](https://openreview.net/forum?id=49fIu0yDJ4) |  | 0 |  | Hanmo Liu, Shimin Di, Jialiang Wang, Zhili Wang, Jiachuan Wang, Xiaofang Zhou, Lei Chen |  |
| 3142 |  |  [Learnable Expansion of Graph Operators for Multi-Modal Feature Fusion](https://openreview.net/forum?id=SMZqIOSdlN) |  | 0 |  | Dexuan Ding, Lei Wang, Liyun Zhu, Tom Gedeon, Piotr Koniusz |  |
| 3143 |  |  [OccProphet: Pushing the Efficiency Frontier of Camera-Only 4D Occupancy Forecasting with an Observer-Forecaster-Refiner Framework](https://openreview.net/forum?id=vC7AlY1ytz) |  | 0 |  | Junliang Chen, Huaiyuan Xu, Yi Wang, LapPui Chau |  |
| 3144 |  |  [Glimpse: Enabling White-Box Methods to Use Proprietary Models for Zero-Shot LLM-Generated Text Detection](https://openreview.net/forum?id=an3fugFA23) |  | 0 |  | Guangsheng Bao, Yanbin Zhao, Juncai He, Yue Zhang |  |
| 3145 |  |  [Exact Byte-Level Probabilities from Tokenized Language Models for FIM-Tasks and Model Ensembles](https://openreview.net/forum?id=zGej22CBnS) |  | 0 |  | Buu Phan, Brandon Amos, Itai Gat, Marton Havasi, Matthew J. Muckley, Karen Ullrich |  |
| 3146 |  |  [Context-Alignment: Activating and Enhancing LLMs Capabilities in Time Series](https://openreview.net/forum?id=syC2764fPc) |  | 0 |  | Yuxiao Hu, Qian Li, Dongxiao Zhang, Jinyue Yan, Yuntian Chen |  |
| 3147 |  |  [Prototype antithesis for biological few-shot class-incremental learning](https://openreview.net/forum?id=bRqaHn3J5I) |  | 0 |  | Binghao Liu, Han Yang, Fang Wan, Fei Gu |  |
| 3148 |  |  [Growth Inhibitors for Suppressing Inappropriate Image Concepts in Diffusion Models](https://openreview.net/forum?id=w4C4z80w59) |  | 0 |  | Die Chen, Zhiwen Li, Mingyuan Fan, Cen Chen, Wenmeng Zhou, Yanhao Wang, Yaliang Li |  |
| 3149 |  |  [Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality](https://openreview.net/forum?id=AV7OXVlAyi) |  | 0 |  | Guanyu Zhou, Yibo Yan, Xin Zou, Kun Wang, Aiwei Liu, Xuming Hu |  |
| 3150 |  |  [Super(ficial)-alignment: Strong Models May Deceive Weak Models in Weak-to-Strong Generalization](https://openreview.net/forum?id=HxKSzulSD1) |  | 0 |  | Wenkai Yang, Shiqi Shen, Guangyao Shen, Wei Yao, Yong Liu, Gong Zhi, Yankai Lin, JiRong Wen |  |
| 3151 |  |  [ECHOPulse: ECG Controlled Echocardio-gram Video Generation](https://openreview.net/forum?id=i2r7LDjba3) |  | 0 |  | Yiwei Li, Sekeun Kim, Zihao Wu, Hanqi Jiang, Yi Pan, Pengfei Jin, Sifan Song, Yucheng Shi, Xiaowei Yu, Tianze Yang, Tianming Liu, Quanzheng Li, Xiang Li |  |
| 3152 |  |  [Training-Free Diffusion Model Alignment with Sampling Demons](https://openreview.net/forum?id=tfemquulED) |  | 0 |  | PoHung Yeh, KuangHuei Lee, JunCheng Chen |  |
| 3153 |  |  [MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models](https://openreview.net/forum?id=yOOJwR15xg) |  | 0 |  | Jingwei Xu, Junyu Lai, Yunpeng Huang |  |
| 3154 |  |  [ToolACE: Winning the Points of LLM Function Calling](https://openreview.net/forum?id=8EB8k6DdCU) |  | 0 |  | Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Zhengying Liu, Yuanqing Yu, Zezhong Wang, Yuxian Wang, Wu Ning, Yutai Hou, Bin Wang, Chuhan Wu, Xinzhi Wang, Yong Liu, Yasheng Wang, Duyu Tang, Dandan Tu, Lifeng Shang, Xin Jiang, Ruiming Tang, Defu Lian, Qun Liu, Enhong Chen |  |
| 3155 |  |  [Improving Data Efficiency via Curating LLM-Driven Rating Systems](https://openreview.net/forum?id=DKkQtRMowq) |  | 0 |  | Jinlong Pang, Jiaheng Wei, Ankit Shah, Zhaowei Zhu, Yaxuan Wang, Chen Qian, Yang Liu, Yujia Bao, Wei Wei |  |
| 3156 |  |  [VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning](https://openreview.net/forum?id=cpGPPLLYYx) |  | 0 |  | Yongshuo Zong, Ondrej Bohdal, Timothy M. Hospedales |  |
| 3157 |  |  [U-Nets as Belief Propagation: Efficient Classification, Denoising, and Diffusion in Generative Hierarchical Models](https://openreview.net/forum?id=sy1lbQxj9J) |  | 0 |  | Song Mei |  |
| 3158 |  |  [Controlled LLM Decoding via Discrete Auto-regressive Biasing](https://openreview.net/forum?id=Duuerhutvq) |  | 0 |  | Patrick Pynadath, Ruqi Zhang |  |
| 3159 |  |  [A Single Goal is All You Need: Skills and Exploration Emerge from Contrastive RL without Rewards, Demonstrations, or Subgoals](https://openreview.net/forum?id=xCkgX4Xfu0) |  | 0 |  | Grace Liu, Michael Tang, Benjamin Eysenbach |  |
| 3160 |  |  [Efficient Dictionary Learning with Switch Sparse Autoencoders](https://openreview.net/forum?id=k2ZVAzVeMP) |  | 0 |  | Anish Mudide, Joshua Engels, Eric J. Michaud, Max Tegmark, Christian Schröder de Witt |  |
| 3161 |  |  [Scaling Wearable Foundation Models](https://openreview.net/forum?id=yb4QE6b22f) |  | 0 |  | Girish Narayanswamy, Xin Liu, Kumar Ayush, Yuzhe Yang, Xuhai Xu, Shun Liao, Jake Garrison, Shyam A. Tailor, Jacob E. Sunshine, Yun Liu, Tim Althoff, Shrikanth Narayanan, Pushmeet Kohli, Jiening Zhan, Mark Malhotra, Shwetak N. Patel, Samy AbdelGhaffar, Daniel McDuff |  |
| 3162 |  |  [Asynchronous Federated Reinforcement Learning with Policy Gradient Updates: Algorithm Design and Convergence Analysis](https://openreview.net/forum?id=5DUekOKWcS) |  | 0 |  | Guangchen Lan, DongJun Han, Abolfazl Hashemi, Vaneet Aggarwal, Christopher Brinton |  |
| 3163 |  |  [Hummingbird: High Fidelity Image Generation via Multimodal Context Alignment](https://openreview.net/forum?id=6kPBThI6ZJ) |  | 0 |  | MinhQuan Le, Gaurav Mittal, Tianjian Meng, A S. M. Iftekhar, Vishwas Suryanarayanan, Barun Patra, Dimitris Samaras, Mei Chen |  |
| 3164 |  |  [On Designing General and Expressive Quantum Graph Neural Networks with Applications to MILP Instance Representation](https://openreview.net/forum?id=IQi8JOqLuv) |  | 0 |  | Xinyu Ye, Hao Xiong, Jianhao Huang, Ziang Chen, Jia Wang, Junchi Yan |  |
| 3165 |  |  [Personalized Visual Instruction Tuning](https://openreview.net/forum?id=sAxdIJ4l6z) |  | 0 |  | Renjie Pi, Jianshu Zhang, Tianyang Han, Jipeng Zhang, Rui Pan, Tong Zhang |  |
| 3166 |  |  [Neural Approximate Mirror Maps for Constrained Diffusion Models](https://openreview.net/forum?id=vgZDcUetWS) |  | 0 |  | Berthy Feng, Ricardo Baptista, Katherine L. Bouman |  |
| 3167 |  |  [Chain-of-region: Visual Language Models Need Details for Diagram Analysis](https://openreview.net/forum?id=M6fYrICcQs) |  | 0 |  | Xue Li, Yiyou Sun, Wei Cheng, Yinglun Zhu, Haifeng Chen |  |
| 3168 |  |  [pMoE: Prompting Diverse Experts Together Wins More in Visual Adaptation](https://openreview.net/forum?id=scozdyKzET) |  | 0 |  | Shentong Mo, Xufang Luo, Dongsheng Li |  |
| 3169 |  |  [MMAD: A Comprehensive Benchmark for Multimodal Large Language Models in Industrial Anomaly Detection](https://openreview.net/forum?id=JDiER86r8v) |  | 0 |  | Xi Jiang, Jian Li, Hanqiu Deng, Yong Liu, BinBin Gao, Yifeng Zhou, Jialin Li, Chengjie Wang, Feng Zheng |  |
| 3170 |  |  [TSC-Net: Prediction of Pedestrian Trajectories by Trajectory-Scene-Cell Classification](https://openreview.net/forum?id=Xmh5gdMfRJ) |  | 0 |  | Bo Hu, TatJen Cham |  |
| 3171 |  |  [RRM: Robust Reward Model Training Mitigates Reward Hacking](https://openreview.net/forum?id=88AS5MQnmC) |  | 0 |  | Tianqi Liu, Wei Xiong, Jie Ren, Lichang Chen, Junru Wu, Rishabh Joshi, Yang Gao, Jiaming Shen, Zhen Qin, Tianhe Yu, Daniel Sohn, Anastasia Makarova, Jeremiah Zhe Liu, Yuan Liu, Bilal Piot, Abe Ittycheriah, Aviral Kumar, Mohammad Saleh |  |
| 3172 |  |  [Vertical Federated Learning with Missing Features During Training and Inference](https://openreview.net/forum?id=OXi1FmHGzz) |  | 0 |  | Pedro Valdeira, Shiqiang Wang, Yuejie Chi |  |
| 3173 |  |  [High-Quality Joint Image and Video Tokenization with Causal VAE](https://openreview.net/forum?id=aRD1NqcXTC) |  | 0 |  | Dawit Mureja Argaw, Xian Liu, Qinsheng Zhang, Joon Son Chung, MingYu Liu, Fitsum Reda |  |
| 3174 |  |  [Joint Fine-tuning and Conversion of Pretrained Speech and Language Models towards Linear Complexity](https://openreview.net/forum?id=90Db4RUBc7) |  | 0 |  | Mutian He, Philip N. Garner |  |
| 3175 |  |  [Proximal Mapping Loss: Understanding Loss Functions in Crowd Counting & Localization](https://openreview.net/forum?id=7p8CcxP1Xc) |  | 0 |  | Wei Lin, Jia Wan, Antoni B. Chan |  |
| 3176 |  |  [Bandit Learning in Matching Markets with Indifference](https://openreview.net/forum?id=7ENakslm9J) |  | 0 |  | Fang Kong, Jingqi Tang, Mingzhu Li, Pinyan Lu, John C. S. Lui, Shuai Li |  |
| 3177 |  |  [Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering](https://openreview.net/forum?id=j6fsbpAllN) |  | 0 |  | Ziyu Zhao, Tao Shen, Didi Zhu, Zexi Li, Jing Su, Xuwu Wang, Fei Wu |  |
| 3178 |  |  [EgoExo-Gen: Ego-centric Video Prediction by Watching Exo-centric Videos](https://openreview.net/forum?id=8J2DrrWDKE) |  | 0 |  | Jilan Xu, Yifei Huang, Baoqi Pei, Junlin Hou, Qingqiu Li, Guo Chen, Yuejie Zhang, Rui Feng, Weidi Xie |  |
| 3179 |  |  [UniGEM: A Unified Approach to Generation and Property Prediction for Molecules](https://openreview.net/forum?id=Lb91pXwZMR) |  | 0 |  | Shikun Feng, Yuyan Ni, Yan Lu, ZhiMing Ma, WeiYing Ma, Yanyan Lan |  |
| 3180 |  |  [SmartRAG: Jointly Learn RAG-Related Tasks From the Environment Feedback](https://openreview.net/forum?id=OCd3cffulp) |  | 0 |  | Jingsheng Gao, Linxu Li, Ke Ji, Weiyuan Li, Yixin Lian, Yuzhuo Fu, Bin Dai |  |
| 3181 |  |  [Self-Supervised Diffusion Models for Electron-Aware Molecular Representation Learning](https://openreview.net/forum?id=UQ0RqfhgCk) |  | 0 |  | Gyoung S. Na, Chanyoung Park |  |
| 3182 |  |  [HShare: Fast LLM Decoding by Hierarchical Key-Value Sharing](https://openreview.net/forum?id=Tb5PY5vwp6) |  | 0 |  | Huaijin Wu, Lianqiang Li, Hantao Huang, Tu Yi, Jihang Zhang, Minghui Yu, Junchi Yan |  |
| 3183 |  |  [Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations?](https://openreview.net/forum?id=lCasyP21Bf) |  | 0 |  | Letitia Parcalabescu, Anette Frank |  |
| 3184 |  |  [Generalizability of Neural Networks Minimizing Empirical Risk Based on Expressive Power](https://openreview.net/forum?id=8wAL9ywQNB) |  | 0 |  | Lijia Yu, Yibo Miao, Yifan Zhu, XiaoShan Gao, Lijun Zhang |  |
| 3185 |  |  [Limits of Deep Learning: Sequence Modeling through the Lens of Complexity Theory](https://openreview.net/forum?id=DhdqML3FdM) |  | 0 |  | Nikola Zubic, Federico Soldà, Aurelio L. Sulser, Davide Scaramuzza |  |
| 3186 |  |  [It Helps to Take a Second Opinion: Teaching Smaller LLMs To Deliberate Mutually via Selective Rationale Optimisation](https://openreview.net/forum?id=NHxwxc3ql6) |  | 0 |  | Sohan Patnaik, Milan Aggarwal, Sumit Bhatia, Balaji Krishnamurthy |  |
| 3187 |  |  [Heavy-Tailed Diffusion with Denoising Levy Probabilistic Models](https://openreview.net/forum?id=SYmUS6qRub) |  | 0 |  | Dario Shariatian, Umut Simsekli, Alain Oliviero Durmus |  |
| 3188 |  |  [MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models](https://openreview.net/forum?id=WsgEWL8i0K) |  | 0 |  | Fanqing Meng, Jin Wang, Chuanhao Li, Quanfeng Lu, Hao Tian, Tianshuo Yang, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, Ping Luo, Kaipeng Zhang, Wenqi Shao |  |
| 3189 |  |  [From Layers to States: A State Space Model Perspective to Deep Neural Network Layer Dynamics](https://openreview.net/forum?id=msD4DHZzFg) |  | 0 |  | Qinshuo Liu, Weiqin Zhao, Wei Huang, Yanwen Fang, Lequan Yu, Guodong Li |  |
| 3190 |  |  [VAE-Var: Variational Autoencoder-Enhanced Variational Methods for Data Assimilation in Meteorology](https://openreview.net/forum?id=utz99dx2RN) |  | 0 |  | Yi Xiao, Qilong Jia, Kun Chen, Lei Bai, Wei Xue |  |
| 3191 |  |  [Towards Domain Adaptive Neural Contextual Bandits](https://openreview.net/forum?id=LNkMWCEssX) |  | 0 |  | Ziyan Wang, Xiaoming Huo, Hao Wang |  |
| 3192 |  |  [Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion](https://openreview.net/forum?id=TEkoMEjf7E) |  | 0 |  | Zhenwei Wang, Tengfei Wang, Zexin He, Gerhard Petrus Hancke, Ziwei Liu, Rynson W. H. Lau |  |
| 3193 |  |  [FLIP: Flow-Centric Generative Planning as General-Purpose Manipulation World Model](https://openreview.net/forum?id=B2N0nCVC91) |  | 0 |  | Chongkai Gao, Haozhuo Zhang, Zhixuan Xu, Zhehao Cai, Lin Shao |  |
| 3194 |  |  [Causal Graph Transformer for Treatment Effect Estimation Under Unknown Interference](https://openreview.net/forum?id=foQ4AeEGG7) |  | 0 |  | Anpeng Wu, Haiyi Qiu, Zhengming Chen, Zijian Li, Ruoxuan Xiong, Fei Wu, Kun Zhang |  |
| 3195 |  |  [Can One Modality Model Synergize Training of Other Modality Models?](https://openreview.net/forum?id=5BXWhVbHAK) |  | 0 |  | JaeJun Lee, Sung Whan Yoon |  |
| 3196 |  |  [Aligned Datasets Improve Detection of Latent Diffusion-Generated Images](https://openreview.net/forum?id=doBkiqESYq) |  | 0 |  | Anirudh Sundara Rajan, Utkarsh Ojha, Jedidiah Schloesser, Yong Jae Lee |  |
| 3197 |  |  [RocketEval: Efficient automated LLM evaluation via grading checklist](https://openreview.net/forum?id=zJjzNj6QUe) |  | 0 |  | Tianjun Wei, Wei Wen, Ruizhi Qiao, Xing Sun, Jianghong Ma |  |
| 3198 |  |  [PolaFormer: Polarity-aware Linear Attention for Vision Transformers](https://openreview.net/forum?id=kN6MFmKUSK) |  | 0 |  | Weikang Meng, Yadan Luo, Xin Li, Dongmei Jiang, Zheng Zhang |  |
| 3199 |  |  [Learning Gain Map for Inverse Tone Mapping](https://openreview.net/forum?id=GtHRhpgpzB) |  | 0 |  | Yinuo Liao, Yuanshen Guan, Ruikang Xu, Jiacheng Li, Shida Sun, Zhiwei Xiong |  |
| 3200 |  |  [SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression](https://openreview.net/forum?id=LNYIUouhdt) |  | 0 |  | Xin Wang, Yu Zheng, Zhongwei Wan, Mi Zhang |  |
| 3201 |  |  [CG-Bench: Clue-grounded Question Answering Benchmark for Long Video Understanding](https://openreview.net/forum?id=le4IoZZHy1) |  | 0 |  | Guo Chen, Yicheng Liu, Yifei Huang, Baoqi Pei, Jilan Xu, Yuping He, Tong Lu, Yali Wang, Limin Wang |  |
| 3202 |  |  [Parameter and Memory Efficient Pretraining via Low-rank Riemannian Optimization](https://openreview.net/forum?id=i0zzO7Hslk) |  | 0 |  | Zhanfeng Mo, LongKai Huang, Sinno Jialin Pan |  |
| 3203 |  |  [Latent Radiance Fields with 3D-aware 2D Representations](https://openreview.net/forum?id=vL9t9tpKli) |  | 0 |  | Chaoyi Zhou, Xi Liu, Feng Luo, Siyu Huang |  |
| 3204 |  |  [Memory Efficient Transformer Adapter for Dense Predictions](https://openreview.net/forum?id=vJkktqyU8B) |  | 0 |  | Dong Zhang, Rui Yan, Pingcheng Dong, KwangTing Cheng |  |
| 3205 |  |  [Policy Optimization under Imperfect Human Interactions with Agent-Gated Shared Autonomy](https://openreview.net/forum?id=LfekK1E0QE) |  | 0 |  | Zhenghai Xue, Bo An, Shuicheng Yan |  |
| 3206 |  |  [Multiview Equivariance Improves 3D Correspondence Understanding with Minimal Feature Finetuning](https://openreview.net/forum?id=CNO4rbSV6v) |  | 0 |  | Yang You, Yixin Li, Congyue Deng, Yue Wang, Leonidas J. Guibas |  |
| 3207 |  |  [Scale-aware Recognition in Satellite Images under Resource Constraints](https://openreview.net/forum?id=QIxFo9mFwR) |  | 0 |  | Shreelekha Revankar, Cheng Perng Phoo, Utkarsh Mall, Bharath Hariharan, Kavita Bala |  |
| 3208 |  |  [IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation](https://openreview.net/forum?id=4w99NAikOE) |  | 0 |  | Xinchen Zhang, Ling Yang, Guohao Li, Yaqi Cai, Jiake Xie, Yong Tang, Yujiu Yang, Mengdi Wang, Bin Cui |  |
| 3209 |  |  [MambaQuant: Quantizing the Mamba Family with Variance Aligned Rotation Methods](https://openreview.net/forum?id=KI45uDnmzv) |  | 0 |  | Zukang Xu, Yuxuan Yue, Xing Hu, Dawei Yang, Zhihang Yuan, Zixu Jiang, Zhixuan Chen, Jiangyong Yu, Chen Xu, Sifan Zhou |  |
| 3210 |  |  [ST-GCond: Self-supervised and Transferable Graph Dataset Condensation](https://openreview.net/forum?id=wYWJFLQov9) |  | 0 |  | Beining Yang, Qingyun Sun, Cheng Ji, Xingcheng Fu, Jianxin Li |  |
| 3211 |  |  [PPT: Patch Order Do Matters In Time Series Pretext Task](https://openreview.net/forum?id=7zwIEbSTDy) |  | 0 |  | Jaeho Kim, Kwangryeol Park, Sukmin Yun, Seulki Lee |  |
| 3212 |  |  [Rethinking Graph Neural Networks From A Geometric Perspective Of Node Features](https://openreview.net/forum?id=lBMRmw59Lk) |  | 0 |  | Feng Ji, Yanan Zhao, Kai Zhao, Hanyang Meng, Jielong Yang, Wee Peng Tay |  |
| 3213 |  |  [KiVA: Kid-inspired Visual Analogies for Testing Large Multimodal Models](https://openreview.net/forum?id=vNATZfmY6R) |  | 0 |  | Eunice Yiu, Maan Qraitem, Anisa Noor Majhi, Charlie Wong, Yutong Bai, Shiry Ginosar, Alison Gopnik, Kate Saenko |  |
| 3214 |  |  [Learning from weak labelers as constraints](https://openreview.net/forum?id=2BtFKEeMGo) |  | 0 |  | Vishwajeet Agrawal, Rattana Pukdee, MariaFlorina Balcan, Pradeep Kumar Ravikumar |  |
| 3215 |  |  [Bisimulation Metric for Model Predictive Control](https://openreview.net/forum?id=F07ic7huE3) |  | 0 |  | Yutaka Shimizu, Masayoshi Tomizuka |  |
| 3216 |  |  [Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling](https://openreview.net/forum?id=EgJhwYR2tB) |  | 0 |  | Wenda Xu, Rujun Han, Zifeng Wang, Long T. Le, Dhruv Madeka, Lei Li, William Yang Wang, Rishabh Agarwal, ChenYu Lee, Tomas Pfister |  |
| 3217 |  |  [NUDGE: Lightweight Non-Parametric Fine-Tuning of Embeddings for Retrieval](https://openreview.net/forum?id=MYw74B77KQ) |  | 0 |  | Sepanta Zeighami, Zac Wellmer, Aditya G. Parameswaran |  |
| 3218 |  |  [Private Mechanism Design via Quantile Estimation](https://openreview.net/forum?id=JQQDePbfxh) |  | 0 |  | Yuanyuan Yang, Tao Xiao, Bhuvesh Kumar, Jamie H. Morgenstern |  |
| 3219 |  |  [Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community Retrieval](https://openreview.net/forum?id=8fLgt7PQza) |  | 0 |  | Pengcheng Jiang, Cao Xiao, Minhao Jiang, Parminder Bhatia, Taha A. KassHout, Jimeng Sun, Jiawei Han |  |
| 3220 |  |  [Brain Mapping with Dense Features: Grounding Cortical Semantic Selectivity in Natural Images With Vision Transformers](https://openreview.net/forum?id=yJ9QNbpMi2) |  | 0 |  | Andrew F. Luo, Jacob Yeung, Rushikesh Zawar, Shaurya Dewan, Margaret M. Henderson, Leila Wehbe, Michael J. Tarr |  |
| 3221 |  |  [X-Drive: Cross-modality Consistent Multi-Sensor Data Synthesis for Driving Scenarios](https://openreview.net/forum?id=IEMmEd5Jgm) |  | 0 |  | Yichen Xie, Chenfeng Xu, Chensheng Peng, Shuqi Zhao, Nhat Ho, Alexander T. Pham, Mingyu Ding, Masayoshi Tomizuka, Wei Zhan |  |
| 3222 |  |  [Selective Attention Improves Transformer](https://openreview.net/forum?id=v0FzmPCd1e) |  | 0 |  | Yaniv Leviathan, Matan Kalman, Yossi Matias |  |
| 3223 |  |  [STAR: Stability-Inducing Weight Perturbation for Continual Learning](https://openreview.net/forum?id=6N5OM5Duuj) |  | 0 |  | Masih Eskandar, Tooba Imtiaz, Davin Hill, Zifeng Wang, Jennifer G. Dy |  |
| 3224 |  |  [OMNI-EPIC: Open-endedness via Models of human Notions of Interestingness with Environments Programmed in Code](https://openreview.net/forum?id=Y1XkzMJpPd) |  | 0 |  | Maxence Faldor, Jenny Zhang, Antoine Cully, Jeff Clune |  |
| 3225 |  |  [CSA: Data-efficient Mapping of Unimodal Features to Multimodal Features](https://openreview.net/forum?id=6Mg7pjG7Sw) |  | 0 |  | Pohan Li, Sandeep P. Chinchali, Ufuk Topcu |  |
| 3226 |  |  [EC-Diffuser: Multi-Object Manipulation via Entity-Centric Behavior Generation](https://openreview.net/forum?id=o3pJU5QCtv) |  | 0 |  | Carl Qi, Dan Haramati, Tal Daniel, Aviv Tamar, Amy Zhang |  |
| 3227 |  |  [A Truncated Newton Method for Optimal Transport](https://openreview.net/forum?id=gWrWUaCbMa) |  | 0 |  | Mete Kemertas, Amirmassoud Farahmand, Allan Douglas Jepson |  |
| 3228 |  |  [Automated Proof Generation for Rust Code via Self-Evolution](https://openreview.net/forum?id=2NqssmiXLu) |  | 0 |  | Tianyu Chen, Shuai Lu, Shan Lu, Yeyun Gong, Chenyuan Yang, Xuheng Li, Md Rakib Hossain Misu, Hao Yu, Nan Duan, Peng Cheng, Fan Yang, Shuvendu K. Lahiri, Tao Xie, Lidong Zhou |  |
| 3229 |  |  [Cut the Crap: An Economical Communication Pipeline for LLM-based Multi-Agent Systems](https://openreview.net/forum?id=LkzuPorQ5L) |  | 0 |  | Guibin Zhang, Yanwei Yue, Zhixun Li, Sukwon Yun, Guancheng Wan, Kun Wang, Dawei Cheng, Jeffrey Xu Yu, Tianlong Chen |  |
| 3230 |  |  [Revealing and Reducing Gender Biases in Vision and Language Assistants (VLAs)](https://openreview.net/forum?id=oStNAMWELS) |  | 0 |  | Leander Girrbach, Stephan Alaniz, Yiran Huang, Trevor Darrell, Zeynep Akata |  |
| 3231 |  |  [Zero-Shot Whole-Body Humanoid Control via Behavioral Foundation Models](https://openreview.net/forum?id=9sOR0nYLtz) |  | 0 |  | Andrea Tirinzoni, Ahmed Touati, Jesse Farebrother, Mateusz Guzek, Anssi Kanervisto, Yingchen Xu, Alessandro Lazaric, Matteo Pirotta |  |
| 3232 |  |  [Outlier Synthesis via Hamiltonian Monte Carlo for Out-of-Distribution Detection](https://openreview.net/forum?id=N6ba2xsmds) |  | 0 |  | Hengzhuang Li, Teng Zhang |  |
| 3233 |  |  [Enhancing Cognition and Explainability of Multimodal Foundation Models with Self-Synthesized Data](https://openreview.net/forum?id=lHbLpwbEyt) |  | 0 |  | Yucheng Shi, Quanzheng Li, Jin Sun, Xiang Li, Ninghao Liu |  |
| 3234 |  |  [SMI-Editor: Edit-based SMILES Language Model with Fragment-level Supervision](https://openreview.net/forum?id=M29nUGozPa) |  | 0 |  | Kangjie Zheng, Siyue Liang, Junwei Yang, Bin Feng, Zequn Liu, Wei Ju, Zhiping Xiao, Ming Zhang |  |
| 3235 |  |  [MMRole: A Comprehensive Framework for Developing and Evaluating Multimodal Role-Playing Agents](https://openreview.net/forum?id=FGSgsefE0Y) |  | 0 |  | Yanqi Dai, Huanran Hu, Lei Wang, Shengjie Jin, Xu Chen, Zhiwu Lu |  |
| 3236 |  |  [REMEDY: Recipe Merging Dynamics in Large Vision-Language Models](https://openreview.net/forum?id=iX7eHHE5Tx) |  | 0 |  | Didi Zhu, Yibing Song, Tao Shen, Ziyu Zhao, Jinluan Yang, Min Zhang, Chao Wu |  |
| 3237 |  |  [TIPS: Text-Image Pretraining with Spatial awareness](https://openreview.net/forum?id=DaA0wAcTY7) |  | 0 |  | KevisKokitsi Maninis, Kaifeng Chen, Soham Ghosh, Arjun Karpur, Koert Chen, Ye Xia, Bingyi Cao, Daniel Salz, Guangxing Han, Jan Dlabal, Dan Gnanapragasam, Mojtaba Seyedhosseini, Howard Zhou, André Araújo |  |
| 3238 |  |  [From Promise to Practice: Realizing High-performance Decentralized Training](https://openreview.net/forum?id=lo3nlFHOft) |  | 0 |  | Zesen Wang, Jiaojiao Zhang, Xuyang Wu, Mikael Johansson |  |
| 3239 |  |  [mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models](https://openreview.net/forum?id=pr37sbuhVa) |  | 0 |  | Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, Jingren Zhou |  |
| 3240 |  |  [Long-Sequence Recommendation Models Need Decoupled Embeddings](https://openreview.net/forum?id=jkpGIxSsUD) |  | 0 |  | Ningya Feng, Junwei Pan, Jialong Wu, Baixu Chen, Ximei Wang, Qian Li, Xian Hu, Jie Jiang, Mingsheng Long |  |
| 3241 |  |  [MVTokenFlow: High-quality 4D Content Generation using Multiview Token Flow](https://openreview.net/forum?id=zu7cBTPsDb) |  | 0 |  | Hanzhuo Huang, Yuan Liu, Ge Zheng, Jiepeng Wang, Zhiyang Dou, Sibei Yang |  |
| 3242 |  |  [DenoiseVAE: Learning Molecule-Adaptive Noise Distributions for Denoising-based 3D Molecular Pre-training](https://openreview.net/forum?id=ym7pr83XQr) |  | 0 |  | Yurou Liu, Jiahao Chen, Rui Jiao, Jiangmeng Li, Wenbing Huang, Bing Su |  |
| 3243 |  |  [Halton Scheduler for Masked Generative Image Transformer](https://openreview.net/forum?id=RDVrlWAb7K) |  | 0 |  | Victor Besnier, Mickaël Chen, David Hurych, Eduardo Valle, Matthieu Cord |  |
| 3244 |  |  [GS-CPR: Efficient Camera Pose Refinement via 3D Gaussian Splatting](https://openreview.net/forum?id=mP7uV59iJM) |  | 0 |  | Changkun Liu, Shuai Chen, Yash Bhalgat, Siyan Hu, Ming Cheng, Zirui Wang, Victor Adrian Prisacariu, Tristan Braud |  |
| 3245 |  |  [Black Sheep in the Herd: Playing with Spuriously Correlated Attributes for Vision-Language Recognition](https://openreview.net/forum?id=g1fkhbhHjL) |  | 0 |  | Xinyu Tian, Shu Zou, Zhaoyuan Yang, Mengqi He, Jing Zhang |  |
| 3246 |  |  [Democratic Training Against Universal Adversarial Perturbations](https://openreview.net/forum?id=4M0BRyGMnJ) |  | 0 |  | Bing Sun, Jun Sun, Wei Zhao |  |
| 3247 |  |  [Framer: Interactive Frame Interpolation](https://openreview.net/forum?id=Lp40Z40N07) |  | 0 |  | Wen Wang, Qiuyu Wang, Kecheng Zheng, Hao Ouyang, Zhekai Chen, Biao Gong, Hao Chen, Yujun Shen, Chunhua Shen |  |
| 3248 |  |  [LucidPPN: Unambiguous Prototypical Parts Network for User-centric Interpretable Computer Vision](https://openreview.net/forum?id=BM9qfolt6p) |  | 0 |  | Mateusz Pach, Koryna Lewandowska, Jacek Tabor, Bartosz Michal Zielinski, Dawid Damian Rymarczyk |  |
| 3249 |  |  [Certifying Language Model Robustness with Fuzzed Randomized Smoothing: An Efficient Defense Against Backdoor Attacks](https://openreview.net/forum?id=USI3ZbuFaV) |  | 0 |  | Bowei He, Lihao Yin, HuiLing Zhen, Jianping Zhang, Lanqing Hong, Mingxuan Yuan, Chen Ma |  |
| 3250 |  |  [Kolmogorov-Arnold Transformer](https://openreview.net/forum?id=BCeock53nt) |  | 0 |  | Xingyi Yang, Xinchao Wang |  |
| 3251 |  |  [BrainUICL: An Unsupervised Individual Continual Learning Framework for EEG Applications](https://openreview.net/forum?id=6jjAYmppGQ) |  | 0 |  | Yangxuan Zhou, Sha Zhao, Jiquan Wang, Haiteng Jiang, Shijian Li, Tao Li, Gang Pan |  |
| 3252 |  |  [Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary Resolution](https://openreview.net/forum?id=ODiY6pbHZQ) |  | 0 |  | Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao |  |
| 3253 |  |  [Is In-Context Learning Sufficient for Instruction Following in LLMs?](https://openreview.net/forum?id=STEEDDv3zI) |  | 0 |  | Hao Zhao, Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion |  |
| 3254 |  |  [OVTR: End-to-End Open-Vocabulary Multiple Object Tracking with Transformer](https://openreview.net/forum?id=GDS5eN65QY) |  | 0 |  | Jinyang Li, En Yu, Sijia Chen, Wenbing Tao |  |
| 3255 |  |  [Learning Evolving Tools for Large Language Models](https://openreview.net/forum?id=wtrDLMFU9v) |  | 0 |  | Guoxin Chen, Zhong Zhang, Xin Cong, Fangda Guo, Yesai Wu, Yankai Lin, Wenzheng Feng, Yasheng Wang |  |
| 3256 |  |  [Towards Effective Evaluations and Comparisons for LLM Unlearning Methods](https://openreview.net/forum?id=wUtCieKuQU) |  | 0 |  | Qizhou Wang, Bo Han, Puning Yang, Jianing Zhu, Tongliang Liu, Masashi Sugiyama |  |
| 3257 |  |  [Distilling Reinforcement Learning Algorithms for In-Context Model-Based Planning](https://openreview.net/forum?id=BfUugGfBE5) |  | 0 |  | Jaehyeon Son, Soochan Lee, Gunhee Kim |  |
| 3258 |  |  [Chain-of-Focus Prompting: Leveraging Sequential Visual Cues to Prompt Large Autoregressive Vision Models](https://openreview.net/forum?id=noidywkBba) |  | 0 |  | Jiyang Zheng, Jialiang Shen, Yu Yao, Min Wang, Yang Yang, Dadong Wang, Tongliang Liu |  |
| 3259 |  |  [BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian RL](https://openreview.net/forum?id=UnCKU8pZVe) |  | 0 |  | YuHeng Hung, KaiJie Lin, YuHeng Lin, ChienYi Wang, Cheng Sun, PingChun Hsieh |  |
| 3260 |  |  [Deep Signature: Characterization of Large-Scale Molecular Dynamics](https://openreview.net/forum?id=xayT1nn8Mg) |  | 0 |  | Tiexin Qin, Mengxu Zhu, Chunyang Li, Terry Lyons, Hong Yan, Haoliang Li |  |
| 3261 |  |  [RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation](https://openreview.net/forum?id=yAzN4tz7oI) |  | 0 |  | Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, Jun Zhu |  |
| 3262 |  |  [VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation](https://openreview.net/forum?id=02haSpO453) |  | 0 |  | Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, Song Han, Yao Lu |  |
| 3263 |  |  [Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models](https://openreview.net/forum?id=1EnpStvBU8) |  | 0 |  | Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, Rongrong Ji |  |
| 3264 |  |  [PAL: Sample-Efficient Personalized Reward Modeling for Pluralistic Alignment](https://openreview.net/forum?id=1kFDrYCuSu) |  | 0 |  | Daiwei Chen, Yi Chen, Aniket Rege, Zhi Wang, Ramya Korlakai Vinayak |  |
| 3265 |  |  [Dynamic Diffusion Transformer](https://openreview.net/forum?id=taHwqSrbrb) |  | 0 |  | Wangbo Zhao, Yizeng Han, Jiasheng Tang, Kai Wang, Yibing Song, Gao Huang, Fan Wang, Yang You |  |
| 3266 |  |  [GS-LiDAR: Generating Realistic LiDAR Point Clouds with Panoramic Gaussian Splatting](https://openreview.net/forum?id=RMaRBE9s2H) |  | 0 |  | Junzhe Jiang, Chun Gu, Yurui Chen, Li Zhang |  |
| 3267 |  |  [LongVILA: Scaling Long-Context Visual Language Models for Long Videos](https://openreview.net/forum?id=wCXAlfvCy6) |  | 0 |  | Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Yihui He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, Song Han |  |
| 3268 |  |  [McEval: Massively Multilingual Code Evaluation](https://openreview.net/forum?id=UunCPtPOlZ) |  | 0 |  | Linzheng Chai, Shukai Liu, Jian Yang, Yuwei Yin, Ke Jin, Jiaheng Liu, Tao Sun, Ge Zhang, Changyu Ren, Hongcheng Guo, Noah Wang, Boyang Wang, Xianjie Wu, Bing Wang, Tongliang Li, Liqun Yang, Sufeng Duan, Zhaoxiang Zhang, Zhoujun Li |  |
| 3269 |  |  [Re-Evaluating the Impact of Unseen-Class Unlabeled Data on Semi-Supervised Learning Model](https://openreview.net/forum?id=WPsnH6875d) |  | 0 |  | Rundong He, Yicong Dong, Lanzhe Guo, Yilong Yin, Tailin Wu |  |
| 3270 |  |  [Cross-Attention Head Position Patterns Can Align with Human Visual Concepts in Text-to-Image Generative Models](https://openreview.net/forum?id=1vggIT5vvj) |  | 0 |  | Jungwon Park, Jungmin Ko, Dongnam Byun, Jangwon Suh, Wonjong Rhee |  |
| 3271 |  |  [FormalAlign: Automated Alignment Evaluation for Autoformalization](https://openreview.net/forum?id=B5RrIFMqbe) |  | 0 |  | Jianqiao Lu, Yingjia Wan, Yinya Huang, Jing Xiong, Zhengying Liu, Zhijiang Guo |  |
| 3272 |  |  [SEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection](https://openreview.net/forum?id=VHguhvcoM5) |  | 0 |  | Han Shen, PinYu Chen, Payel Das, Tianyi Chen |  |
| 3273 |  |  [SeRA: Self-Reviewing and Alignment of LLMs using Implicit Reward Margins](https://openreview.net/forum?id=uIGnuyDSB9) |  | 0 |  | Jongwoo Ko, Saket Dingliwal, Bhavana Ganesh, Sailik Sengupta, Sravan Babu Bodapati, Aram Galstyan |  |
| 3274 |  |  [Bidirectional Decoding: Improving Action Chunking via Guided Test-Time Sampling](https://openreview.net/forum?id=qZmn2hkuzw) |  | 0 |  | Yuejiang Liu, Jubayer Ibn Hamid, Annie Xie, Yoonho Lee, Max Du, Chelsea Finn |  |
| 3275 |  |  [6DGS: Enhanced Direction-Aware Gaussian Splatting for Volumetric Rendering](https://openreview.net/forum?id=sUvBTEYXGt) |  | 0 |  | Zhongpai Gao, Benjamin Planche, Meng Zheng, Anwesa Choudhuri, Terrence Chen, Ziyan Wu |  |
| 3276 |  |  [Credit-based self organizing maps: training deep topographic networks with minimal performance degradation](https://openreview.net/forum?id=wMgr7wBuUo) |  | 0 |  | Amirozhan Dehghani, Xinyu Qian, Asa Farahani, Pouya Bashivan |  |
| 3277 |  |  [An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels](https://openreview.net/forum?id=tjNf0L8QjR) |  | 0 |  | DuyKien Nguyen, Mido Assran, Unnat Jain, Martin R. Oswald, Cees G. M. Snoek, Xinlei Chen |  |
| 3278 |  |  [Model-Free Offline Reinforcement Learning with Enhanced Robustness](https://openreview.net/forum?id=QyVLJ7EnAC) |  | 0 |  | Chi Zhang, Zain Ulabedeen Farhat, George K. Atia, Yue Wang |  |
| 3279 |  |  [Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want](https://openreview.net/forum?id=bfa58H1nQ8) |  | 0 |  | Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, Hongsheng Li |  |
| 3280 |  |  [Let Your Features Tell The Differences: Understanding Graph Convolution By Feature Splitting](https://openreview.net/forum?id=I9omfcWfMp) |  | 0 |  | Yilun Zheng, Xiang Li, Sitao Luan, Xiaojiang Peng, Lihui Chen |  |
| 3281 |  |  [Learning to Generate Diverse Pedestrian Movements from Web Videos with Noisy Labels](https://openreview.net/forum?id=DydCqKa6AH) |  | 0 |  | Zhizheng Liu, Joe Lin, Wayne Wu, Bolei Zhou |  |
| 3282 |  |  [MIRAGE: Evaluating and Explaining Inductive Reasoning Process in Language Models](https://openreview.net/forum?id=tZCqSVncRf) |  | 0 |  | Jiachun Li, Pengfei Cao, Zhuoran Jin, Yubo Chen, Kang Liu, Jun Zhao |  |
| 3283 |  |  [One for all and all for one: Efficient computation of partial Wasserstein distances on the line](https://openreview.net/forum?id=kzEPsHbJDv) |  | 0 |  | Laetitia Chapel, Romain Tavenard |  |
| 3284 |  |  [Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse Representation Adjustment in Large Language Models](https://openreview.net/forum?id=s20W12XTF8) |  | 0 |  | Guobin Shen, Dongcheng Zhao, Yiting Dong, Xiang He, Yi Zeng |  |
| 3285 |  |  [Precise Parameter Localization for Textual Generation in Diffusion Models](https://openreview.net/forum?id=gdHtZlaaSo) |  | 0 |  | Lukasz Staniszewski, Bartosz Cywinski, Franziska Boenisch, Kamil Deja, Adam Dziedzic |  |
| 3286 |  |  [Lightning-Fast Image Inversion and Editing for Text-to-Image Diffusion Models](https://openreview.net/forum?id=t9l63huPRt) |  | 0 |  | Dvir Samuel, Barak Meiri, Haggai Maron, Yoad Tewel, Nir Darshan, Shai Avidan, Gal Chechik, Rami BenAri |  |
| 3287 |  |  [Revisiting Convolution Architecture in the Realm of DNA Foundation Models](https://openreview.net/forum?id=B07dLVWLyD) |  | 0 |  | Yu Bo, Weian Mao, Yanjun Shao, Weiqiang Bai, Peng Ye, Xinzhu Ma, Junbo Zhao, Hao Chen, Chunhua Shen |  |
| 3288 |  |  [Kronecker Mask and Interpretive Prompts are Language-Action Video Learners](https://openreview.net/forum?id=RUF7j1cJzK) |  | 0 |  | Jingyi Yang, Zitong Yu, Xiuming Ni, Jia He, Hui Li |  |
| 3289 |  |  [MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?](https://openreview.net/forum?id=k5VHHgsRbi) |  | 0 |  | Yifan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, Liang Wang, Rong Jin |  |
| 3290 |  |  [State Space Model Meets Transformer: A New Paradigm for 3D Object Detection](https://openreview.net/forum?id=Tisu1L0Jwt) |  | 0 |  | Chuxin Wang, Wenfei Yang, Xiang Liu, Tianzhu Zhang |  |
| 3291 |  |  [What to align in multimodal contrastive learning?](https://openreview.net/forum?id=Pe3AxLq6Wf) |  | 0 |  | Benoit Dufumier, Javiera Castillo Navarro, Devis Tuia, JeanPhilippe Thiran |  |
| 3292 |  |  [Smoothing the Shift: Towards Stable Test-Time Adaptation under Complex Multimodal Noises](https://openreview.net/forum?id=rObkvzJxTG) |  | 0 |  | Zirun Guo, Tao Jin |  |
| 3293 |  |  [Improving Language Model Distillation through Hidden State Matching](https://openreview.net/forum?id=IcVSKhVpKu) |  | 0 |  | Sayantan Dasgupta, Trevor Cohn |  |
| 3294 |  |  [Long-Context Linear System Identification](https://openreview.net/forum?id=2TuUXtLGhT) |  | 0 |  | Oguz Kaan Yüksel, Mathieu Even, Nicolas Flammarion |  |
| 3295 |  |  [Controllable Unlearning for Image-to-Image Generative Models via ϵ-Constrained Optimization](https://openreview.net/forum?id=9OJflnNu6C) |  | 0 |  | Xiaohua Feng, Yuyuan Li, Chaochao Chen, Li Zhang, Longfei Li, Jun Zhou, Xiaolin Zheng |  |
| 3296 |  |  [Random Is All You Need: Random Noise Injection on Feature Statistics for Generalizable Deep Image Denoising](https://openreview.net/forum?id=z8PcUSKXXN) |  | 0 |  | Zhengwei Yin, Hongjun Wang, Guixu Lin, Weihang Ran, Yinqiang Zheng |  |
| 3297 |  |  [Navigating Neural Space: Revisiting Concept Activation Vectors to Overcome Directional Divergence](https://openreview.net/forum?id=Q95MaWfF4e) |  | 0 |  | Frederik Pahde, Maximilian Dreyer, Moritz Weckbecker, Leander Weber, Christopher J. Anders, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin |  |
| 3298 |  |  [LLaVA-MoD: Making LLaVA Tiny via MoE-Knowledge Distillation](https://openreview.net/forum?id=uWtLOy35WD) |  | 0 |  | Fangxun Shu, Yue Liao, Lei Zhang, Le Zhuo, Chenning Xu, Guanghao Zhang, Haonan Shi, Long Chan, Tao Zhong, Zhelun Yu, Wanggui He, Siming Fu, Haoyuan Li, Si Liu, Hongsheng Li, Hao Jiang |  |
| 3299 |  |  [I2VControl-Camera: Precise Video Camera Control with Adjustable Motion Strength](https://openreview.net/forum?id=AcAD4VEgCX) |  | 0 |  | Wanquan Feng, Jiawei Liu, Pengqi Tu, Tianhao Qi, Mingzhen Sun, Tianxiang Ma, Songtao Zhao, SiYu Zhou, Qian He |  |
| 3300 |  |  [BinaryDM: Accurate Weight Binarization for Efficient Diffusion Models](https://openreview.net/forum?id=YaeZwhXJ4k) |  | 0 |  | Xingyu Zheng, Xianglong Liu, Haotong Qin, Xudong Ma, Mingyuan Zhang, Haojie Hao, Jiakai Wang, Zixiang Zhao, Jinyang Guo, Michele Magno |  |
| 3301 |  |  [LongWriter: Unleashing 10, 000+ Word Generation from Long Context LLMs](https://openreview.net/forum?id=kQ5s9Yh0WI) |  | 0 |  | Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li |  |
| 3302 |  |  [GROOT-2: Weakly Supervised Multimodal Instruction Following Agents](https://openreview.net/forum?id=S9GyQUXzee) |  | 0 |  | Shaofei Cai, Bowei Zhang, Zihao Wang, Haowei Lin, Xiaojian Ma, Anji Liu, Yitao Liang |  |
| 3303 |  |  [Medium-Difficulty Samples Constitute Smoothed Decision Boundary for Knowledge Distillation on Pruned Datasets](https://openreview.net/forum?id=Rz4UkJziFe) |  | 0 |  | Yudong Chen, Xuwei Xu, Frank de Hoog, Jiajun Liu, Sen Wang |  |
| 3304 |  |  [Mixture Compressor for Mixture-of-Experts LLMs Gains More](https://openreview.net/forum?id=hheFYjOsWO) |  | 0 |  | Wei Huang, Yue Liao, Jianhui Liu, Ruifei He, Haoru Tan, Shiming Zhang, Hongsheng Li, Si Liu, Xiaojuan Qi |  |
| 3305 |  |  [Methods for Convex (L0, L1)-Smooth Optimization: Clipping, Acceleration, and Adaptivity](https://openreview.net/forum?id=0wmfzWPAFu) |  | 0 |  | Eduard Gorbunov, Nazarii Tupitsa, Sayantan Choudhury, Alen Aliev, Peter Richtárik, Samuel Horváth, Martin Takác |  |
| 3306 |  |  [Minimax Optimal Reinforcement Learning with Quasi-Optimism](https://openreview.net/forum?id=i8LCUpKvAz) |  | 0 |  | Harin Lee, Minhwan Oh |  |
| 3307 |  |  [Weakly-Supervised Affordance Grounding Guided by Part-Level Semantic Priors](https://openreview.net/forum?id=0823rvTIhs) |  | 0 |  | Peiran Xu, Yadong Mu |  |
| 3308 |  |  [Safety Layers in Aligned Large Language Models: The Key to LLM Security](https://openreview.net/forum?id=kUH1yPMAn7) |  | 0 |  | Shen Li, Liuyi Yao, Lan Zhang, Yaliang Li |  |
| 3309 |  |  [Episodic Novelty Through Temporal Distance](https://openreview.net/forum?id=I7DeajDEx7) |  | 0 |  | Yuhua Jiang, Qihan Liu, Yiqin Yang, Xiaoteng Ma, Dianyu Zhong, Hao Hu, Jun Yang, Bin Liang, Bo Xu, Chongjie Zhang, Qianchuan Zhao |  |
| 3310 |  |  [ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation](https://openreview.net/forum?id=E1N1oxd63b) |  | 0 |  | Tianchen Zhao, Tongcheng Fang, Haofeng Huang, Rui Wan, Widyadewi Soedarmadji, Enshu Liu, Shiyao Li, Zinan Lin, Guohao Dai, Shengen Yan, Huazhong Yang, Xuefei Ning, Yu Wang |  |
| 3311 |  |  [Do You Keep an Eye on What I Ask? Mitigating Multimodal Hallucination via Attention-Guided Ensemble Decoding](https://openreview.net/forum?id=ziw5bzg2NO) |  | 0 |  | Yeongjae Cho, Keonwoo Kim, Taebaek Hwang, Sungzoon Cho |  |
| 3312 |  |  [Masked Temporal Interpolation Diffusion for Procedure Planning in Instructional Videos](https://openreview.net/forum?id=HnpDHiItd2) |  | 0 |  | Yufan Zhou, Zhaobo Qi, Lingshuai Lin, Junqi Jing, Tingting Chai, Beichen Zhang, Shuhui Wang, Weigang Zhang |  |
| 3313 |  |  [BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against Jailbreak Attacks](https://openreview.net/forum?id=wwVGZRnAYG) |  | 0 |  | Yunhan Zhao, Xiang Zheng, Lin Luo, Yige Li, Xingjun Ma, YuGang Jiang |  |
| 3314 |  |  [Active Learning for Continual Learning: Keeping the Past Alive in the Present](https://openreview.net/forum?id=mnLmmtW7HO) |  | 0 |  | Jaehyun Park, Dongmin Park, JaeGil Lee |  |
| 3315 |  |  [Group Downsampling with Equivariant Anti-aliasing](https://openreview.net/forum?id=sOte83GogU) |  | 0 |  | Md Ashiqur Rahman, Raymond A. Yeh |  |
| 3316 |  |  [Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models](https://openreview.net/forum?id=mQ55y4s5hj) |  | 0 |  | Donghoon Kim, Minji Bae, Kyuhong Shim, Byonghyo Shim |  |
| 3317 |  |  [BP-Modified Local Loss for Efficient Training of Deep Neural Networks](https://openreview.net/forum?id=MtW30ql5Oj) |  | 0 |  | Lianhai Ren, Qianxiao Li |  |
| 3318 |  |  [The Crucial Role of Samplers in Online Direct Preference Optimization](https://openreview.net/forum?id=F6z3utfcYw) |  | 0 |  | Ruizhe Shi, Runlong Zhou, Simon Shaolei Du |  |
| 3319 |  |  [Near-Optimal Policy Identification in Robust Constrained Markov Decision Processes via Epigraph Form](https://openreview.net/forum?id=G5sPv4KSjR) |  | 0 |  | Toshinori Kitamura, Tadashi Kozuno, Wataru Kumagai, Kenta Hoshino, Yohei Hosoe, Kazumi Kasaura, Masashi Hamaya, Paavo Parmas, Yutaka Matsuo |  |
| 3320 |  |  [Scaling Autonomous Agents via Automatic Reward Modeling And Planning](https://openreview.net/forum?id=womU9cEwcO) |  | 0 |  | Zhenfang Chen, Delin Chen, Rui Sun, Wenjun Liu, Chuang Gan |  |
| 3321 |  |  [Generalized Video Moment Retrieval](https://openreview.net/forum?id=qdOIkeZ5e4) |  | 0 |  | You Qin, Qilong Wu, Yicong Li, Wei Ji, Li Li, Pengcheng Cai, Lina Wei, Roger Zimmermann |  |
| 3322 |  |  [A Training-Free Sub-quadratic Cost Transformer Model Serving Framework with Hierarchically Pruned Attention](https://openreview.net/forum?id=PTcMzQgKmn) |  | 0 |  | Heejun Lee, Geon Park, Youngwan Lee, Jaduk Suh, Jina Kim, Wonyong Jeong, Bumsik Kim, Hyemin Lee, Myeongjae Jeon, Sung Ju Hwang |  |
| 3323 |  |  [GameGen-X: Interactive Open-world Game Video Generation](https://openreview.net/forum?id=8VG8tpPZhe) |  | 0 |  | Haoxuan Che, Xuanhua He, Quande Liu, Cheng Jin, Hao Chen |  |
| 3324 |  |  [StochSync: Stochastic Diffusion Synchronization for Image Generation in Arbitrary Spaces](https://openreview.net/forum?id=XPNprvlxuQ) |  | 0 |  | Kyeongmin Yeo, Jaihoon Kim, Minhyuk Sung |  |
| 3325 |  |  [FlickerFusion: Intra-trajectory Domain Generalizing Multi-agent Reinforcement Learning](https://openreview.net/forum?id=MRYyOaNxh3) |  | 0 |  | Woosung Koh, Wonbeen Oh, Siyeol Kim, Suhin Shin, Hyeongjin Kim, Jaein Jang, Junghyun Lee, SeYoung Yun |  |
| 3326 |  |  [DiTTo-TTS: Diffusion Transformers for Scalable Text-to-Speech without Domain-Specific Factors](https://openreview.net/forum?id=hQvX9MBowC) |  | 0 |  | Keon Lee, Dong Won Kim, Jaehyeon Kim, Seungjun Chung, Jaewoong Cho |  |
| 3327 |  |  [Intelligent Go-Explore: Standing on the Shoulders of Giant Foundation Models](https://openreview.net/forum?id=apErWGzCAA) |  | 0 |  | Cong Lu, Shengran Hu, Jeff Clune |  |
| 3328 |  |  [Online Reward-Weighted Fine-Tuning of Flow Matching with Wasserstein Regularization](https://openreview.net/forum?id=2IoFFexvuw) |  | 0 |  | Jiajun Fan, Shuaike Shen, Chaoran Cheng, Yuxin Chen, Chumeng Liang, Ge Liu |  |
| 3329 |  |  [GenEx: Generating an Explorable World](https://openreview.net/forum?id=8NlUL0Cv1L) |  | 0 |  | Taiming Lu, Tianmin Shu, Alan L. Yuille, Daniel Khashabi, Jieneng Chen |  |
| 3330 |  |  [Gradient-Free Generation for Hard-Constrained Systems](https://openreview.net/forum?id=teE4pl9ftK) |  | 0 |  | Chaoran Cheng, Boran Han, Danielle C. Maddix, Abdul Fatir Ansari, Andrew Stuart, Michael W. Mahoney, Bernie Wang |  |
| 3331 |  |  [UniDrive: Towards Universal Driving Perception Across Camera Configurations](https://openreview.net/forum?id=jVDPq9EdzT) |  | 0 |  | Ye Li, Wenzhao Zheng, Xiaonan Huang, Kurt Keutzer |  |
| 3332 |  |  [ImageFolder: Autoregressive Image Generation with Folded Tokens](https://openreview.net/forum?id=QE1LFzXQPL) |  | 0 |  | Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, Zhe Lin |  |
| 3333 |  |  [InstaSHAP: Interpretable Additive Models Explain Shapley Values Instantly](https://openreview.net/forum?id=ky7vVlBQBY) |  | 0 |  | James Enouen, Yan Liu |  |
| 3334 |  |  [Watermark Anything With Localized Messages](https://openreview.net/forum?id=IkZVDzdC8M) |  | 0 |  | Tom Sander, Pierre Fernandez, Alain Oliviero Durmus, Teddy Furon, Matthijs Douze |  |
| 3335 |  |  [M^3PC: Test-time Model Predictive Control using Pretrained Masked Trajectory Model](https://openreview.net/forum?id=inOwd7hZC1) |  | 0 |  | Kehan Wen, Yutong Hu, Yao Mu, Lei Ke |  |
| 3336 |  |  [TEASER: Token Enhanced Spatial Modeling for Expressions Reconstruction](https://openreview.net/forum?id=pTeOOKnjGM) |  | 0 |  | Yunfei Liu, Lei Zhu, Lijian Lin, Ye Zhu, Ailing Zhang, Yu Li |  |
| 3337 |  |  [SurFhead: Affine Rig Blending for Geometrically Accurate 2D Gaussian Surfel Head Avatars](https://openreview.net/forum?id=1x1gGg49jr) |  | 0 |  | Jaeseong Lee, Taewoong Kang, Marcel C. Bühler, MinJung Kim, Sungwon Hwang, Junha Hyung, Hyojin Jang, Jaegul Choo |  |
| 3338 |  |  [BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games](https://openreview.net/forum?id=fp6t3F669F) |  | 0 |  | Davide Paglieri, Bartlomiej Cupial, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan, Eduardo Pignatelli, Lukasz Kucinski, Lerrel Pinto, Rob Fergus, Jakob Nicolaus Foerster, Jack ParkerHolder, Tim Rocktäschel |  |
| 3339 |  |  [Ensembles of Low-Rank Expert Adapters](https://openreview.net/forum?id=l0gZS0sAlf) |  | 0 |  | Yinghao Li, Vianne R. Gao, Chao Zhang, MohamadAli Torkamani |  |
| 3340 |  |  [Does Refusal Training in LLMs Generalize to the Past Tense?](https://openreview.net/forum?id=aJUuere4fM) |  | 0 |  | Maksym Andriushchenko, Nicolas Flammarion |  |
| 3341 |  |  [Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks](https://openreview.net/forum?id=hXA8wqRdyV) |  | 0 |  | Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion |  |
| 3342 |  |  [Laplace Sample Information: Data Informativeness Through a Bayesian Lens](https://openreview.net/forum?id=qO6dk9KfIp) |  | 0 |  | Johannes Kaiser, Kristian Schwethelm, Daniel Rueckert, Georgios Kaissis |  |
| 3343 |  |  [Structural-Entropy-Based Sample Selection for Efficient and Effective Learning](https://openreview.net/forum?id=xUMI52rrW7) |  | 0 |  | Tianchi Xie, Jiangning Zhu, Guozu Ma, Minzhi Lin, Wei Chen, Weikai Yang, Shixia Liu |  |
| 3344 |  |  [Stable Segment Anything Model](https://openreview.net/forum?id=ooxj2Audlq) |  | 0 |  | Qi Fan, Xin Tao, Lei Ke, Mingqiao Ye, Di Zhang, Pengfei Wan, YuWing Tai, ChiKeung Tang |  |
| 3345 |  |  [ProtoSnap: Prototype Alignment For Cuneiform Signs](https://openreview.net/forum?id=XHTirKsQV6) |  | 0 |  | Rachel Mikulinsky, Morris Alper, Shai Gordin, Enrique Jiménez, Yoram Cohen, Hadar AverbuchElor |  |
| 3346 |  |  [Learning Fine-Grained Representations through Textual Token Disentanglement in Composed Video Retrieval](https://openreview.net/forum?id=wGa2plE8ka) |  | 0 |  | Yue Wu, Zhaobo Qi, Yiling Wu, Junshu Sun, Yaowei Wang, Shuhui Wang |  |
| 3347 |  |  [Salvage: Shapley-distribution Approximation Learning Via Attribution Guided Exploration for Explainable Image Classification](https://openreview.net/forum?id=WBUVagRgsd) |  | 0 |  | Mehdi Naouar, Hanne Raum, Jens Rahnfeld, Yannick Vogt, Joschka Boedecker, Gabriel Kalweit, Maria Kalweit |  |
| 3348 |  |  [Fine-tuning can Help Detect Pretraining Data from Large Language Models](https://openreview.net/forum?id=X8dzvdkQwO) |  | 0 |  | Hengxiang Zhang, Songxin Zhang, Bingyi Jing, Hongxin Wei |  |
| 3349 |  |  [Uncertainty and Influence aware Reward Model Refinement for Reinforcement Learning from Human Feedback](https://openreview.net/forum?id=iamWnRpMuQ) |  | 0 |  | Zexu Sun, Yiju Guo, Yankai Lin, Xu Chen, Qi Qi, Xing Tang, Xiuqiang He, JiRong Wen |  |
| 3350 |  |  [A Simple Framework for Open-Vocabulary Zero-Shot Segmentation](https://openreview.net/forum?id=QzPKSUUcud) |  | 0 |  | Thomas Stegmüller, Tim Lebailly, Nikola Dukic, Behzad Bozorgtabar, Tinne Tuytelaars, JeanPhilippe Thiran |  |
| 3351 |  |  [Understanding the Generalization of In-Context Learning in Transformers: An Empirical Study](https://openreview.net/forum?id=yOhNLIqTEF) |  | 0 |  | Xingxuan Zhang, Haoran Wang, Jiansheng Li, Yuan Xue, Shikai Guan, Renzhe Xu, Hao Zou, Han Yu, Peng Cui |  |
| 3352 |  |  [Progressive Mixed-Precision Decoding for Efficient LLM Inference](https://openreview.net/forum?id=OVxmpus9NA) |  | 0 |  | Hao Mark Chen, Fuwen Tan, Alexandros Kouris, Royson Lee, Hongxiang Fan, Stylianos I. Venieris |  |
| 3353 |  |  [Manifold Constraint Reduces Exposure Bias in Accelerated Diffusion Sampling](https://openreview.net/forum?id=5xmXUwDxep) |  | 0 |  | Yuzhe Yao, Jun Chen, Zeyi Huang, Haonan Lin, Mengmeng Wang, Guang Dai, Jingdong Wang |  |
| 3354 |  |  [Gaussian Head & Shoulders: High Fidelity Neural Upper Body Avatars with Anchor Gaussian Guided Texture Warping](https://openreview.net/forum?id=HtbqsbNw9c) |  | 0 |  | Tianhao (Walter) Wu, Jing Yang, Zhilin Guo, Jingyi Wan, Fangcheng Zhong, Cengiz Öztireli |  |
| 3355 |  |  [Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel](https://openreview.net/forum?id=OUuhwVsk9Z) |  | 0 |  | Zun Wang, Jialu Li, Yicong Hong, Songze Li, Kunchang Li, Shoubin Yu, Yi Wang, Yu Qiao, Yali Wang, Mohit Bansal, Limin Wang |  |
| 3356 |  |  [Hydra-SGG: Hybrid Relation Assignment for One-stage Scene Graph Generation](https://openreview.net/forum?id=tpD1rs25Uu) |  | 0 |  | Minghan Chen, Guikun Chen, Wenguan Wang, Yi Yang |  |
| 3357 |  |  [SANER: Annotation-free Societal Attribute Neutralizer for Debiasing CLIP](https://openreview.net/forum?id=x5hXkSMOd1) |  | 0 |  | Yusuke Hirota, MinHung Chen, ChienYi Wang, Yuta Nakashima, YuChiang Frank Wang, Ryo Hachiuma |  |
| 3358 |  |  [Dynamical Diffusion: Learning Temporal Dynamics with Diffusion Models](https://openreview.net/forum?id=c5JZEPyFUE) |  | 0 |  | Xingzhuo Guo, Yu Zhang, Baixu Chen, Haoran Xu, Jianmin Wang, Mingsheng Long |  |
| 3359 |  |  [A General Framework for Off-Policy Learning with Partially-Observed Reward](https://openreview.net/forum?id=mUbYof5MKp) |  | 0 |  | Rikiya Takehi, Masahiro Asami, Kosuke Kawakami, Yuta Saito |  |
| 3360 |  |  [Not-So-Optimal Transport Flows for 3D Point Cloud Generation](https://openreview.net/forum?id=62Ff8LDAJZ) |  | 0 |  | KaHei Hui, Chao Liu, Xiaohui Zeng, ChiWing Fu, Arash Vahdat |  |
| 3361 |  |  [Self-Boosting Large Language Models with Synthetic Preference Data](https://openreview.net/forum?id=7visV100Ms) |  | 0 |  | Qingxiu Dong, Li Dong, Xingxing Zhang, Zhifang Sui, Furu Wei |  |
| 3362 |  |  [Unifying Unsupervised Graph-Level Anomaly Detection and Out-of-Distribution Detection: A Benchmark](https://openreview.net/forum?id=g90RNzs8wX) |  | 0 |  | Yili Wang, Yixin Liu, Xu Shen, Chenyu Li, Rui Miao, Kaize Ding, Ying Wang, Shirui Pan, Xin Wang |  |
| 3363 |  |  [Building Interactable Replicas of Complex Articulated Objects via Gaussian Splatting](https://openreview.net/forum?id=ix2yRWarPn) |  | 0 |  | Yu Liu, Baoxiong Jia, Ruijie Lu, Junfeng Ni, SongChun Zhu, Siyuan Huang |  |
| 3364 |  |  [MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference Optimization](https://openreview.net/forum?id=x1Okv4kbVR) |  | 0 |  | Yougang Lyu, Lingyong Yan, Zihan Wang, Dawei Yin, Pengjie Ren, Maarten de Rijke, Zhaochun Ren |  |
| 3365 |  |  [EDiT: A Local-SGD-Based Efficient Distributed Training Method for Large Language Models](https://openreview.net/forum?id=xtlMtbVfWu) |  | 0 |  | Jialiang Cheng, Ning Gao, Yun Yue, Zhiling Ye, Jiadi Jiang, Jian Sha |  |
| 3366 |  |  [IDA-VLM: Towards Movie Understanding via ID-Aware Large Vision-Language Model](https://openreview.net/forum?id=N5YTixK4F1) |  | 0 |  | Yatai Ji, Shilong Zhang, Jie Wu, Peize Sun, Weifeng Chen, Xuefeng Xiao, Sidi Yang, Yujiu Yang, Ping Luo |  |
| 3367 |  |  [Gaussian-Based Instance-Adaptive Intensity Modeling for Point-Supervised Facial Expression Spotting](https://openreview.net/forum?id=daD6uGMeLs) |  | 0 |  | Yicheng Deng, Hideaki Hayashi, Hajime Nagahara |  |
| 3368 |  |  [Learning Spatial-Semantic Features for Robust Video Object Segmentation](https://openreview.net/forum?id=EM93t94zEi) |  | 0 |  | Xin Li, Deshui Miao, Zhenyu He, Yaowei Wang, Huchuan Lu, MingHsuan Yang |  |
| 3369 |  |  [Boosting the visual interpretability of CLIP via adversarial fine-tuning](https://openreview.net/forum?id=khuIvzxPRp) |  | 0 |  | Shizhan Gong, Haoyu Lei, Qi Dou, Farzan Farnia |  |
| 3370 |  |  [DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian Splat Generation](https://openreview.net/forum?id=eajZpoQkGK) |  | 0 |  | Chenguo Lin, Panwang Pan, Bangbang Yang, Zeming Li, Yadong Mu |  |
| 3371 |  |  [Scaling Offline Model-Based RL via Jointly-Optimized World-Action Model Pretraining](https://openreview.net/forum?id=T1OvCSFaum) |  | 0 |  | Jie Cheng, Ruixi Qiao, Yingwei Ma, Binhua Li, Gang Xiong, Qinghai Miao, Yongbin Li, Yisheng Lv |  |
| 3372 |  |  [Deep Incomplete Multi-view Learning via Cyclic Permutation of VAEs](https://openreview.net/forum?id=s4MwstmB8o) |  | 0 |  | Xin Gao, Jian Pu |  |
| 3373 |  |  [X-NeMo: Expressive Neural Motion Reenactment via Disentangled Latent Attention](https://openreview.net/forum?id=ML8FH4s5Ts) |  | 0 |  | Xiaochen Zhao, Hongyi Xu, Guoxian Song, You Xie, Chenxu Zhang, Xiu Li, Linjie Luo, Jinli Suo, Yebin Liu |  |
| 3374 |  |  [On Discriminative Probabilistic Modeling for Self-Supervised Representation Learning](https://openreview.net/forum?id=s15HrqCqbr) |  | 0 |  | Bokun Wang, Yunwen Lei, Yiming Ying, Tianbao Yang |  |
| 3375 |  |  [ThermalGaussian: Thermal 3D Gaussian Splatting](https://openreview.net/forum?id=ybFRoGxZjs) |  | 0 |  | Rongfeng Lu, Hangyu Chen, Zunjie Zhu, Yuhang Qin, Ming Lu, Le Zhang, Chenggang Yan, Anke Xue |  |
| 3376 |  |  [Zero-shot forecasting of chaotic systems](https://openreview.net/forum?id=TqYjhJrp9m) |  | 0 |  | Yuanzhao Zhang, William Gilpin |  |
| 3377 |  |  [MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models](https://openreview.net/forum?id=f7WBRSuf9l) |  | 0 |  | Ziyu Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Haodong Duan, Conghui He, Yuanjun Xiong, Dahua Lin, Jiaqi Wang |  |
| 3378 |  |  [Causal Information Prioritization for Efficient Reinforcement Learning](https://openreview.net/forum?id=nDj45w5wam) |  | 0 |  | Hongye Cao, Fan Feng, Tianpei Yang, Jing Huo, Yang Gao |  |
| 3379 |  |  [Towards Empowerment Gain through Causal Structure Learning in Model-Based Reinforcement Learning](https://openreview.net/forum?id=vgXI1Ws0ma) |  | 0 |  | Hongye Cao, Fan Feng, Meng Fang, Shaokang Dong, Tianpei Yang, Jing Huo, Yang Gao |  |
| 3380 |  |  [Toward Generalizing Visual Brain Decoding to Unseen Subjects](https://openreview.net/forum?id=At9JmGF3xy) |  | 0 |  | Xiangtao Kong, Kexin Huang, Ping Li, Lei Zhang |  |
| 3381 |  |  [The robustness of differentiable Causal Discovery in misspecified Scenarios](https://openreview.net/forum?id=iaP7yHRq1l) |  | 0 |  | Huiyang Yi, Yanyan He, Duxin Chen, Mingyu Kang, He Wang, Wenwu Yu |  |
| 3382 |  |  [A Sanity Check for AI-generated Image Detection](https://openreview.net/forum?id=ODRHZrkOQM) |  | 0 |  | Shilin Yan, Ouxiang Li, Jiayin Cai, Yanbin Hao, Xiaolong Jiang, Yao Hu, Weidi Xie |  |
| 3383 |  |  [Neural Phylogeny: Fine-Tuning Relationship Detection among Neural Networks](https://openreview.net/forum?id=jv2zHOalpL) |  | 0 |  | Runpeng Yu, Xinchao Wang |  |
| 3384 |  |  [Breaking the log⁡(1/Δ2) Barrier: Better Batched Best Arm Identification with Adaptive Grids](https://openreview.net/forum?id=buxFBI6GG4) |  | 0 |  | Tianyuan Jin, Qin Zhang, Dongruo Zhou |  |
| 3385 |  |  [Visual Agents as Fast and Slow Thinkers](https://openreview.net/forum?id=ncCuiD3KJQ) |  | 0 |  | Guangyan Sun, Mingyu Jin, Zhenting Wang, ChengLong Wang, Siqi Ma, Qifan Wang, Tong Geng, Ying Nian Wu, Yongfeng Zhang, Dongfang Liu |  |
| 3386 |  |  [Fourier Sliced-Wasserstein Embedding for Multisets and Measures](https://openreview.net/forum?id=BcYt84rcKq) |  | 0 |  | Tal Amir, Nadav Dym |  |
| 3387 |  |  [Autonomous Evaluation of LLMs for Truth Maintenance and Reasoning Tasks](https://openreview.net/forum?id=iv1TpRCJeK) |  | 0 |  | Rushang Karia, Daniel Bramblett, Daksh Dobhal, Siddharth Srivastava |  |
| 3388 |  |  [Efficient Action-Constrained Reinforcement Learning via Acceptance-Rejection Method and Augmented MDPs](https://openreview.net/forum?id=AgMpK7z4bz) |  | 0 |  | Wei Hung, ShaoHua Sun, PingChun Hsieh |  |
| 3389 |  |  [TeaserGen: Generating Teasers for Long Documentaries](https://openreview.net/forum?id=G1n50BMqzm) |  | 0 |  | Weihan Xu, Paul Pu Liang, Haven Kim, Julian J. McAuley, Taylor BergKirkpatrick, HaoWen Dong |  |
| 3390 |  |  [CameraCtrl: Enabling Camera Control for Video Diffusion Models](https://openreview.net/forum?id=Z4evOUYrk7) |  | 0 |  | Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, Ceyuan Yang |  |
| 3391 |  |  [SSOLE: Rethinking Orthogonal Low-rank Embedding for Self-Supervised Learning](https://openreview.net/forum?id=zBgiCWCxJB) |  | 0 |  | Lun Huang, Qiang Qiu, Guillermo Sapiro |  |
| 3392 |  |  [3DitScene: Editing Any Scene via Language-guided Disentangled Gaussian Splatting](https://openreview.net/forum?id=iKDbLpVgQc) |  | 0 |  | Qihang Zhang, Yinghao Xu, Chaoyang Wang, HsinYing Lee, Gordon Wetzstein, Bolei Zhou, Ceyuan Yang |  |
| 3393 |  |  [CoMotion: Concurrent Multi-person 3D Motion](https://openreview.net/forum?id=qKu6KWPgxt) |  | 0 |  | Alejandro Newell, Peiyun Hu, Lahav Lipson, Stephan R. Richter, Vladlen Koltun |  |
| 3394 |  |  [Scaling Optimal LR Across Token Horizons](https://openreview.net/forum?id=WYL4eFLcxG) |  | 0 |  | Johan Bjorck, Alon Benhaim, Vishrav Chaudhary, Furu Wei, Xia Song |  |
| 3395 |  |  [Order-aware Interactive Segmentation](https://openreview.net/forum?id=8ZLzw5pIrc) |  | 0 |  | Bin Wang, Anwesa Choudhuri, Meng Zheng, Zhongpai Gao, Benjamin Planche, Andong Deng, Qin Liu, Terrence Chen, Ulas Bagci, Ziyan Wu |  |
| 3396 |  |  [Re-Imagining Multimodal Instruction Tuning: A Representation View](https://openreview.net/forum?id=zxg6601zoc) |  | 0 |  | Yiyang Liu, James Chenhao Liang, Ruixiang Tang, Yugyung Lee, Majid Rabbani, Sohail A. Dianat, Raghuveer Rao, Lifu Huang, Dongfang Liu, Qifan Wang, Cheng Han |  |
| 3397 |  |  [Looped Transformers for Length Generalization](https://openreview.net/forum?id=2edigk8yoU) |  | 0 |  | Ying Fan, Yilun Du, Kannan Ramchandran, Kangwook Lee |  |
| 3398 |  |  [Exploring channel distinguishability in local neighborhoods of the model space in quantum neural networks](https://openreview.net/forum?id=gDcL7cgZBt) |  | 0 |  | Sabrina Herbst, Sandeep Suresh Cranganore, Vincenzo De Maio, Ivona Brandic |  |
| 3399 |  |  [Does Spatial Cognition Emerge in Frontier Models?](https://openreview.net/forum?id=WK6K1FMEQ1) |  | 0 |  | Santhosh Kumar Ramakrishnan, Erik Wijmans, Philipp Krähenbühl, Vladlen Koltun |  |
| 3400 |  |  [Adaptive teachers for amortized samplers](https://openreview.net/forum?id=BdmVgLMvaf) |  | 0 |  | Minsu Kim, Sanghyeok Choi, Taeyoung Yun, Emmanuel Bengio, Leo Feng, Jarrid RectorBrooks, Sungsoo Ahn, Jinkyoo Park, Nikolay Malkin, Yoshua Bengio |  |
| 3401 |  |  [MLPs Learn In-Context on Regression and Classification Tasks](https://openreview.net/forum?id=MbX0t1rUlp) |  | 0 |  | William Lingxiao Tong, Cengiz Pehlevan |  |
| 3402 |  |  [Restyling Unsupervised Concept Based Interpretable Networks with Generative Models](https://openreview.net/forum?id=CexatBp6rx) |  | 0 |  | Jayneel Parekh, Quentin Bouniot, Pavlo Mozharovskyi, Alasdair Newson, Florence d'AlchéBuc |  |
| 3403 |  |  [Active Learning for Neural PDE Solvers](https://openreview.net/forum?id=x4ZmQaumRg) |  | 0 |  | Daniel Musekamp, Marimuthu Kalimuthu, David Holzmüller, Makoto Takamoto, Mathias Niepert |  |
| 3404 |  |  [Contextual Self-paced Learning for Weakly Supervised Spatio-Temporal Video Grounding](https://openreview.net/forum?id=yHj6EunfVQ) |  | 0 |  | Akash Kumar, Zsolt Kira, Yogesh S. Rawat |  |
| 3405 |  |  [Multimodal Lego: Model Merging and Fine-Tuning Across Topologies and Modalities in Biomedicine](https://openreview.net/forum?id=pH543jrbe8) |  | 0 |  | Konstantin Hemker, Nikola Simidjievski, Mateja Jamnik |  |
| 3406 |  |  [Depth Pro: Sharp Monocular Metric Depth in Less Than a Second](https://openreview.net/forum?id=aueXfY0Clv) |  | 0 |  | Alexey Bochkovskiy, Amaël Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R. Richter, Vladlen Koltun |  |
| 3407 |  |  [Zero-Shot Natural Language Explanations](https://openreview.net/forum?id=X6VVK8pIzZ) |  | 0 |  | Fawaz Sammani, Nikos Deligiannis |  |
| 3408 |  |  [Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models](https://openreview.net/forum?id=ZeaTvXw080) |  | 0 |  | Yoad Tewel, Rinon Gal, Dvir Samuel, Yuval Atzmon, Lior Wolf, Gal Chechik |  |
| 3409 |  |  [RecDreamer: Consistent Text-to-3D Generation via Uniform Score Distillation](https://openreview.net/forum?id=aucMP9hGYv) |  | 0 |  | Chenxi Zheng, Yihong Lin, Bangzhen Liu, Xuemiao Xu, Yongwei Nie, Shengfeng He |  |
| 3410 |  |  [Aria-MIDI: A Dataset of Piano MIDI Files for Symbolic Music Modeling](https://openreview.net/forum?id=X5hrhgndxW) |  | 0 |  | Louis Bradshaw, Simon Colton |  |
| 3411 |  |  [Efficient and Context-Aware Label Propagation for Zero-/Few-Shot Training-Free Adaptation of Vision-Language Model](https://openreview.net/forum?id=D10yarGQNk) |  | 0 |  | Yushu Li, Yongyi Su, Adam Goodge, Kui Jia, Xun Xu |  |
| 3412 |  |  [Methods with Local Steps and Random Reshuffling for Generally Smooth Non-Convex Federated Optimization](https://openreview.net/forum?id=TrJ36UfD9P) |  | 0 |  | Yury Demidovich, Petr Ostroukhov, Grigory Malinovsky, Samuel Horváth, Martin Takác, Peter Richtárik, Eduard Gorbunov |  |
| 3413 |  |  [From Risk to Uncertainty: Generating Predictive Uncertainty Measures via Bayesian Estimation](https://openreview.net/forum?id=cWfpt2t37q) |  | 0 |  | Nikita Kotelevskii, Vladimir Kondratyev, Martin Takác, Eric Moulines, Maxim Panov |  |
| 3414 |  |  [Timer-XL: Long-Context Transformers for Unified Time Series Forecasting](https://openreview.net/forum?id=KMCJXjlDDr) |  | 0 |  | Yong Liu, Guo Qin, Xiangdong Huang, Jianmin Wang, Mingsheng Long |  |
| 3415 |  |  [Reflective Gaussian Splatting](https://openreview.net/forum?id=xPxHQHDH2u) |  | 0 |  | Yuxuan Yao, Zixuan Zeng, Chun Gu, Xiatian Zhu, Li Zhang |  |
| 3416 |  |  [Data Center Cooling System Optimization Using Offline Reinforcement Learning](https://openreview.net/forum?id=W8xukd70cU) |  | 0 |  | Xianyuan Zhan, Xiangyu Zhu, Peng Cheng, Xiao Hu, Ziteng He, Hanfei Geng, Jichao Leng, Huiwen Zheng, Chenhui Liu, Tianshun Hong, Yan Liang, Yunxin Liu, Feng Zhao |  |
| 3417 |  |  [Bias Mitigation in Graph Diffusion Models](https://openreview.net/forum?id=CSj72Rr2PB) |  | 0 |  | Meng Yu, Kun Zhang |  |
| 3418 |  |  [The Utility and Complexity of In- and Out-of-Distribution Machine Unlearning](https://openreview.net/forum?id=HVFMooKrHX) |  | 0 |  | Youssef Allouah, Joshua Kazdan, Rachid Guerraoui, Sanmi Koyejo |  |
| 3419 |  |  [SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints](https://openreview.net/forum?id=m8Rk3HLGFx) |  | 0 |  | Jianhong Bai, Menghan Xia, Xintao Wang, Ziyang Yuan, Zuozhu Liu, Haoji Hu, Pengfei Wan, Di Zhang |  |
| 3420 |  |  [CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models](https://openreview.net/forum?id=E77uvbOTtp) |  | 0 |  | Hyungjin Chung, Jeongsol Kim, Geon Yeong Park, Hyelin Nam, Jong Chul Ye |  |
| 3421 |  |  [TDDBench: A Benchmark for Training data detection](https://openreview.net/forum?id=hpeyWG1PP6) |  | 0 |  | Zhihao Zhu, Yi Yang, Defu Lian |  |
| 3422 |  |  [Warm Diffusion: Recipe for Blur-Noise Mixture Diffusion Models](https://openreview.net/forum?id=rdSVgnLHQB) |  | 0 |  | HaoChien Hsueh, WenHsiao Peng, ChingChun Huang |  |
| 3423 |  |  [ZeroDiff: Solidified Visual-semantic Correlation in Zero-Shot Learning](https://openreview.net/forum?id=wy9FRV8O5s) |  | 0 |  | Zihan Ye, Shreyank N. Gowda, Shiming Chen, Xiaowei Huang, Haotian Xu, Fahad Shahbaz Khan, Yaochu Jin, Kaizhu Huang, Xiaobo Jin |  |
| 3424 |  |  [Deep Networks Learn Features From Local Discontinuities in the Label Function](https://openreview.net/forum?id=52UtL8uA35) |  | 0 |  | Prithaj Banerjee, Harish Guruprasad Ramaswamy, Mahesh Lorik Yadav, Chandra Shekar Lakshminarayanan |  |
| 3425 |  |  [Data Pruning by Information Maximization](https://openreview.net/forum?id=93XT0lKOct) |  | 0 |  | Haoru Tan, Sitong Wu, Wei Huang, Shizhen Zhao, Xiaojuan Qi |  |
| 3426 |  |  [Mufu: Multilingual Fused Learning for Low-Resource Translation with LLM](https://openreview.net/forum?id=0eMsrRMmCw) |  | 0 |  | Zheng Wei Lim, Nitish Gupta, Honglin Yu, Trevor Cohn |  |
| 3427 |  |  [Rethinking LLM Unlearning Objectives: A Gradient Perspective and Go Beyond](https://openreview.net/forum?id=huo8MqVH6t) |  | 0 |  | Qizhou Wang, Jin Peng Zhou, Zhanke Zhou, Saebyeol Shin, Bo Han, Kilian Q. Weinberger |  |
| 3428 |  |  [HiSplat: Hierarchical 3D Gaussian Splatting for Generalizable Sparse-View Reconstruction](https://openreview.net/forum?id=SBzIbJojs8) |  | 0 |  | Shengji Tang, Weicai Ye, Peng Ye, Weihao Lin, Yang Zhou, Tao Chen, Wanli Ouyang |  |
| 3429 |  |  [OBI-Bench: Can LMMs Aid in Study of Ancient Script on Oracle Bones?](https://openreview.net/forum?id=hL5jone2Oh) |  | 0 |  | Zijian Chen, Tingzhu Chen, Wenjun Zhang, Guangtao Zhai |  |
| 3430 |  |  [Generalizable Human Gaussians from Single-View Image](https://openreview.net/forum?id=dQ2xiSIYzp) |  | 0 |  | Jinnan Chen, Chen Li, Jianfeng Zhang, Lingting Zhu, Buzhen Huang, Hanlin Chen, Gim Hee Lee |  |
| 3431 |  |  [Wavelet Diffusion Neural Operator](https://openreview.net/forum?id=FQhDIGuaJ4) |  | 0 |  | Peiyan Hu, Rui Wang, Xiang Zheng, Tao Zhang, Haodong Feng, Ruiqi Feng, Long Wei, Yue Wang, ZhiMing Ma, Tailin Wu |  |
| 3432 |  |  [MMSearch: Unveiling the Potential of Large Models as Multi-modal Search Engines](https://openreview.net/forum?id=J2Jyp1SZ0n) |  | 0 |  | Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Guanglu Song, Peng Gao, Yu Liu, Chunyuan Li, Hongsheng Li |  |
| 3433 |  |  [Coreset Selection via Reducible Loss in Continual Learning](https://openreview.net/forum?id=mAztx8QO3B) |  | 0 |  | Ruilin Tong, Yuhang Liu, Javen Qinfeng Shi, Dong Gong |  |
| 3434 |  |  [SegLLM: Multi-round Reasoning Segmentation with Large Language Models](https://openreview.net/forum?id=Pm1NXHgzyf) |  | 0 |  | Xudong Wang, Shaolun Zhang, Shufan Li, Kehan Li, Konstantinos Kallidromitis, Yusuke Kato, Kazuki Kozuka, Trevor Darrell |  |
| 3435 |  |  [Out-of-distribution Generalization for Total Variation based Invariant Risk Minimization](https://openreview.net/forum?id=c4wEKJOjY3) |  | 0 |  | Yuanchao Wang, ZhaoRong Lai, Tianqi Zhong |  |
| 3436 |  |  [Multi-Reward as Condition for Instruction-based Image Editing](https://openreview.net/forum?id=9RFocgIccP) |  | 0 |  | Xin Gu, Ming Li, Libo Zhang, Fan Chen, Longyin Wen, Tiejian Luo, Sijie Zhu |  |
| 3437 |  |  [Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations](https://openreview.net/forum?id=Hu0FSOSEyS) |  | 0 |  | Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, WenSheng Chu |  |
| 3438 |  |  [Vector-ICL: In-context Learning with Continuous Vector Representations](https://openreview.net/forum?id=xing7dDGh3) |  | 0 |  | Yufan Zhuang, Chandan Singh, Liyuan Liu, Jingbo Shang, Jianfeng Gao |  |
| 3439 |  |  [Understanding and Mitigating Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing](https://openreview.net/forum?id=pymXpl4qvi) |  | 0 |  | Peihao Wang, Ruisi Cai, Yuehao Wang, Jiajun Zhu, Pragya Srivastava, Zhangyang Wang, Pan Li |  |
| 3440 |  |  [LLaMaFlex: Many-in-one LLMs via Generalized Pruning and Weight Sharing](https://openreview.net/forum?id=AyC4uxx2HW) |  | 0 |  | Ruisi Cai, Saurav Muralidharan, Hongxu Yin, Zhangyang Wang, Jan Kautz, Pavlo Molchanov |  |
| 3441 |  |  [Generative Inbetweening: Adapting Image-to-Video Models for Keyframe Interpolation](https://openreview.net/forum?id=ykD8a9gJvy) |  | 0 |  | Xiaojuan Wang, Boyang Zhou, Brian Curless, Ira KemelmacherShlizerman, Aleksander Holynski, Steven M. Seitz |  |
| 3442 |  |  [Uncertainty-Aware Decoding with Minimum Bayes Risk](https://openreview.net/forum?id=hPpyUv1XyQ) |  | 0 |  | Nico Daheim, Clara Meister, Thomas Möllenhoff, Iryna Gurevych |  |
| 3443 |  |  [Improving Equivariant Networks with Probabilistic Symmetry Breaking](https://openreview.net/forum?id=ZE6lrLvATd) |  | 0 |  | Hannah Lawrence, Vasco Portilheiro, Yan Zhang, SékouOumar Kaba |  |
| 3444 |  |  [CraftRTL: High-quality Synthetic Data Generation for Verilog Code Models with Correct-by-Construction Non-Textual Representations and Targeted Code Repair](https://openreview.net/forum?id=8KQzoD5XAr) |  | 0 |  | Mingjie Liu, YunDa Tsai, Wenfei Zhou, Haoxing Ren |  |
| 3445 |  |  [FreSh: Frequency Shifting for Accelerated Neural Representation Learning](https://openreview.net/forum?id=zMjjzXxS64) |  | 0 |  | Adam Kania, Marko Mihajlovic, Sergey Prokudin, Jacek Tabor, Przemyslaw Spurek |  |
| 3446 |  |  [PointOBB-v2: Towards Simpler, Faster, and Stronger Single Point Supervised Oriented Object Detection](https://openreview.net/forum?id=R22JPTQYWV) |  | 0 |  | Botao Ren, Xue Yang, Yi Yu, Junwei Luo, Zhidong Deng |  |
| 3447 |  |  [A Unified Theory of Quantum Neural Network Loss Landscapes](https://openreview.net/forum?id=fv8TTt9srF) |  | 0 |  | Eric Ricardo Anschütz |  |
| 3448 |  |  [Enhancing Robust Fairness via Confusional Spectral Regularization](https://openreview.net/forum?id=lW0ZndAimF) |  | 0 |  | Gaojie Jin, Sihao Wu, Jiaxu Liu, Tianjin Huang, Ronghui Mu |  |
| 3449 |  |  [Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents](https://openreview.net/forum?id=U1T6sq12uj) |  | 0 |  | Haoyu Wang, Sunhao Dai, Haiyuan Zhao, Liang Pang, Xiao Zhang, Gang Wang, Zhenhua Dong, Jun Xu, JiRong Wen |  |
| 3450 |  |  [HART: Efficient Visual Generation with Hybrid Autoregressive Transformer](https://openreview.net/forum?id=q5sOv4xQe4) |  | 0 |  | Haotian Tang, Yecheng Wu, Shang Yang, Enze Xie, Junsong Chen, Junyu Chen, Zhuoyang Zhang, Han Cai, Yao Lu, Song Han |  |
| 3451 |  |  [ANaGRAM: A Natural Gradient Relative to Adapted Model for efficient PINNs learning](https://openreview.net/forum?id=o1IiiNIoaA) |  | 0 |  | Nilo Schwencke, Cyril Furtlehner |  |
| 3452 |  |  [EdgeRunner: Auto-regressive Auto-encoder for Artistic Mesh Generation](https://openreview.net/forum?id=81cta3WQVI) |  | 0 |  | Jiaxiang Tang, Zhaoshuo Li, Zekun Hao, Xian Liu, Gang Zeng, MingYu Liu, Qinsheng Zhang |  |
| 3453 |  |  [Beyond Random Augmentations: Pretraining with Hard Views](https://openreview.net/forum?id=AK1C55o4r7) |  | 0 |  | Fabio Ferreira, Ivo Rapant, Jörg K. H. Franke, Frank Hutter |  |
| 3454 |  |  [Accelerating Auto-regressive Text-to-Image Generation with Training-free Speculative Jacobi Decoding](https://openreview.net/forum?id=LZfjxvqw0N) |  | 0 |  | Yao Teng, Han Shi, Xian Liu, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, Xihui Liu |  |
| 3455 |  |  [The Breakdown of Gaussian Universality in Classification of High-dimensional Linear Factor Mixtures](https://openreview.net/forum?id=UrKbn51HjA) |  | 0 |  | Xiaoyi Mai, Zhenyu Liao |  |
| 3456 |  |  [MoDGS: Dynamic Gaussian Splatting from Casually-captured Monocular Videos with Depth Priors](https://openreview.net/forum?id=2prShxdLkX) |  | 0 |  | Qingming Liu, Yuan Liu, Jiepeng Wang, Xianqiang Lyu, Peng Wang, Wenping Wang, Junhui Hou |  |
| 3457 |  |  [HaDeMiF: Hallucination Detection and Mitigation in Large Language Models](https://openreview.net/forum?id=VwOYxPScxB) |  | 0 |  | Xiaoling Zhou, Mingjie Zhang, Zhemg Lee, Wei Ye, Shikun Zhang |  |
| 3458 |  |  [LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization](https://openreview.net/forum?id=qTrEq31Shm) |  | 0 |  | Guanzheng Chen, Xin Li, Michael Shieh, Lidong Bing |  |
| 3459 |  |  [See What You Are Told: Visual Attention Sink in Large Multimodal Models](https://openreview.net/forum?id=7uDI7w5RQA) |  | 0 |  | Seil Kang, Jinyeong Kim, Junhyeok Kim, Seong Jae Hwang |  |
| 3460 |  |  [Domain Guidance: A Simple Transfer Approach for a Pre-trained Diffusion Model](https://openreview.net/forum?id=PplM2kDrl3) |  | 0 |  | Jincheng Zhong, Xiangcheng Zhang, Jianmin Wang, Mingsheng Long |  |
| 3461 |  |  [MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation](https://openreview.net/forum?id=4z3IguA4Zg) |  | 0 |  | Chenxi Wang, Xiang Chen, Ningyu Zhang, Bozhong Tian, Haoming Xu, Shumin Deng, Huajun Chen |  |
| 3462 |  |  [GDrag: Towards General-Purpose Interactive Editing with Anti-ambiguity Point Diffusion](https://openreview.net/forum?id=8G3FyfHIko) |  | 0 |  | Xiaojian Lin, Hanhui Li, Yuhao Cheng, Yiqiang Yan, Xiaodan Liang |  |
| 3463 |  |  [AvatarGO: Zero-shot 4D Human-Object Interaction Generation and Animation](https://openreview.net/forum?id=Trf0R8eoGF) |  | 0 |  | Yukang Cao, Liang Pan, Kai Han, KwanYee K. Wong, Ziwei Liu |  |
| 3464 |  |  [Calibrating LLMs with Information-Theoretic Evidential Deep Learning](https://openreview.net/forum?id=YcML3rJl0N) |  | 0 |  | Yawei Li, David Rügamer, Bernd Bischl, Mina Rezaei |  |
| 3465 |  |  [TIGeR: Unifying Text-to-Image Generation and Retrieval with Large Multimodal Models](https://openreview.net/forum?id=mr2icR6dpD) |  | 0 |  | Leigang Qu, Haochuan Li, Tan Wang, Wenjie Wang, Yongqi Li, Liqiang Nie, TatSeng Chua |  |
| 3466 |  |  [Cross-Embodiment Dexterous Grasping with Reinforcement Learning](https://openreview.net/forum?id=twIPSx9qHn) |  | 0 |  | Haoqi Yuan, Bohan Zhou, Yuhui Fu, Zongqing Lu |  |
| 3467 |  |  [VEDIT: Latent Prediction Architecture For Procedural Video Representation Learning](https://openreview.net/forum?id=LDAj4UJ4aL) |  | 0 |  | Han Lin, Tushar Nagarajan, Nicolas Ballas, Mido Assran, Mojtaba Komeili, Mohit Bansal, Koustuv Sinha |  |
| 3468 |  |  [Refine-by-Align: Reference-Guided Artifacts Refinement through Semantic Alignment](https://openreview.net/forum?id=D9CRb1KZQc) |  | 0 |  | Yizhi Song, Liu He, Zhifei Zhang, Soo Ye Kim, He Zhang, Wei Xiong, Zhe Lin, Brian L. Price, Scott Cohen, Jianming Zhang, Daniel G. Aliaga |  |
| 3469 |  |  [FreCaS: Efficient Higher-Resolution Image Generation via Frequency-aware Cascaded Sampling](https://openreview.net/forum?id=TsBDfe8Ra5) |  | 0 |  | Zhengqiang Zhang, Ruihuang Li, Lei Zhang |  |
| 3470 |  |  [Transformers Learn Low Sensitivity Functions: Investigations and Implications](https://openreview.net/forum?id=4ikjWBs3tE) |  | 0 |  | Bhavya Vasudeva, Deqing Fu, Tianyi Zhou, Elliott Kau, Youqi Huang, Vatsal Sharan |  |
| 3471 |  |  [TLDR: Token-Level Detective Reward Model for Large Vision Language Models](https://openreview.net/forum?id=Zy2XgaGpDw) |  | 0 |  | Deqing Fu, Tong Xiao, Rui Wang, Wang Zhu, Pengchuan Zhang, Guan Pang, Robin Jia, Lawrence Chen |  |
| 3472 |  |  [Statistical Advantages of Perturbing Cosine Router in Mixture of Experts](https://openreview.net/forum?id=faDMOmnsjx) |  | 0 |  | Huy Nguyen, Pedram Akbarian, Huyen Trang Pham, Thien Trang Nguyen Vu, Shujian Zhang, Nhat Ho |  |
| 3473 |  |  [Interpretable Unsupervised Joint Denoising and Enhancement for Real-World low-light Scenarios](https://openreview.net/forum?id=PVHoELf5UN) |  | 0 |  | Huaqiu Li, Xiaowan Hu, Haoqian Wang |  |
| 3474 |  |  [Learning Clustering-based Prototypes for Compositional Zero-Shot Learning](https://openreview.net/forum?id=eE2PXlNydB) |  | 0 |  | Hongyu Qu, Jianan Wei, Xiangbo Shu, Wenguan Wang |  |
| 3475 |  |  [SAGEPhos: Sage Bio-Coupled and Augmented Fusion for Phosphorylation Site Detection](https://openreview.net/forum?id=hLwcNSFhC2) |  | 0 |  | Jingjie Zhang, Hanqun Cao, Zijun Gao, Xiaorui Wang, Chunbin Gu |  |
| 3476 |  |  [Depth Any Video with Scalable Synthetic Data](https://openreview.net/forum?id=gWqFbnKsqR) |  | 0 |  | Honghui Yang, Di Huang, Wei Yin, Chunhua Shen, Haifeng Liu, Xiaofei He, Binbin Lin, Wanli Ouyang, Tong He |  |
| 3477 |  |  [SPA: 3D Spatial-Awareness Enables Effective Embodied Representation](https://openreview.net/forum?id=6TLdqAZgzn) |  | 0 |  | Haoyi Zhu, Honghui Yang, Yating Wang, Jiange Yang, Limin Wang, Tong He |  |
| 3478 |  |  [Incremental Causal Effect for Time to Treatment Initialization](https://openreview.net/forum?id=0mtz0pet1z) |  | 0 |  | Andrew Ying, Zhichen Zhao, Ronghui Xu |  |
| 3479 |  |  [Leveraging Sub-Optimal Data for Human-in-the-Loop Reinforcement Learning](https://openreview.net/forum?id=DSyHRkpI7v) |  | 0 |  | Calarina Muslimani, Matthew E. Taylor |  |
| 3480 |  |  [Training-Free Dataset Pruning for Instance Segmentation](https://openreview.net/forum?id=rvxWEbTtRY) |  | 0 |  | Yalun Dai, Lingao Xiao, Ivor W. Tsang, Yang He |  |
| 3481 |  |  [Towards Understanding Text Hallucination of Diffusion Models via Local Generation Bias](https://openreview.net/forum?id=SKW10XJlAI) |  | 0 |  | Rui Lu, Runzhe Wang, Kaifeng Lyu, Xitai Jiang, Gao Huang, Mengdi Wang |  |
| 3482 |  |  [VideoShield: Regulating Diffusion-based Video Generation Models via Watermarking](https://openreview.net/forum?id=uzz3qAYy0D) |  | 0 |  | Runyi Hu, Jie Zhang, Yiming Li, Jiwei Li, Qing Guo, Han Qiu, Tianwei Zhang |  |
| 3483 |  |  [PARTNR: A Benchmark for Planning and Reasoning in Embodied Multi-agent Tasks](https://openreview.net/forum?id=T5QLRRHyL1) |  | 0 |  | Matthew Chang, Gunjan Chhablani, Alexander Clegg, Mikael Dallaire Cote, Ruta Desai, Michal Hlavac, Vladimir Karashchuk, Jacob Krantz, Roozbeh Mottaghi, Priyam Parashar, Siddharth Patki, Ishita Prasad, Xavier Puig, Akshara Rai, Ram Ramrakhya, Daniel Tran, Joanne Truong, John M. Turner, Eric Undersander, TsungYen Yang |  |
| 3484 |  |  [Causal Identification for Complex Functional Longitudinal Studies](https://openreview.net/forum?id=96beVMeHh9) |  | 0 |  | Andrew Ying |  |
| 3485 |  |  [Boost Self-Supervised Dataset Distillation via Parameterization, Predefined Augmentation, and Approximation](https://openreview.net/forum?id=2RfWRKwxYh) |  | 0 |  | ShengFeng Yu, JiaJiun Yao, WeiChen Chiu |  |
| 3486 |  |  [GALA: Geometry-Aware Local Adaptive Grids for Detailed 3D Generation](https://openreview.net/forum?id=KYOdZRR6nr) |  | 0 |  | Dingdong Yang, Yizhi Wang, Konrad Schindler, Ali Mahdavi Amiri, Hao Zhang |  |
| 3487 |  |  [MMEgo: Towards Building Egocentric Multimodal LLMs for Video QA](https://openreview.net/forum?id=67sSPPAZiG) |  | 0 |  | Hanrong Ye, Haotian Zhang, Erik A. Daxberger, Lin Chen, Zongyu Lin, Yanghao Li, Bowen Zhang, Haoxuan You, Dan Xu, Zhe Gan, Jiasen Lu, Yinfei Yang |  |
| 3488 |  |  [Beyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and LLM-as-a-judge](https://openreview.net/forum?id=E8gYIrbP00) |  | 0 |  | Aparna Elangovan, Lei Xu, Jongwoo Ko, Mahsa Elyasi, Ling Liu, Sravan Babu Bodapati, Dan Roth |  |
| 3489 |  |  [Federated Q-Learning with Reference-Advantage Decomposition: Almost Optimal Regret and Logarithmic Communication Cost](https://openreview.net/forum?id=FoUpv84hMw) |  | 0 |  | Zhong Zheng, Haochen Zhang, Lingzhou Xue |  |
| 3490 |  |  [Remove Symmetries to Control Model Expressivity and Improve Optimization](https://openreview.net/forum?id=Gv0TOAigIY) |  | 0 |  | Liu Ziyin, Yizhou Xu, Isaac L. Chuang |  |
| 3491 |  |  [ZIP: An Efficient Zeroth-order Prompt Tuning for Black-box Vision-Language Models](https://openreview.net/forum?id=2OegVbwvY2) |  | 0 |  | Seonghwan Park, Jaehyeon Jeong, Yongjun Kim, Jaeho Lee, Namhoon Lee |  |
| 3492 |  |  [Iformer: Integrating ConvNet and Transformer for Mobile Application](https://openreview.net/forum?id=4ytHislqDS) |  | 0 |  | Chuanyang Zheng |  |
| 3493 |  |  [kNN Attention Demystified: A Theoretical Exploration for Scalable Transformers](https://openreview.net/forum?id=49v8meXjHS) |  | 0 |  | Themistoklis Haris |  |
| 3494 |  |  [Diffusion Feedback Helps CLIP See Better](https://openreview.net/forum?id=tLFWU6izoA) |  | 0 |  | Wenxuan Wang, Quan Sun, Fan Zhang, Yepeng Tang, Jing Liu, Xinlong Wang |  |
| 3495 |  |  [Regulatory DNA Sequence Design with Reinforcement Learning](https://openreview.net/forum?id=F4IMiNhim1) |  | 0 |  | Zhao Yang, Bing Su, Chuan Cao, JiRong Wen |  |
| 3496 |  |  [In-Context Editing: Learning Knowledge from Self-Induced Distributions](https://openreview.net/forum?id=w6rHCuN3YG) |  | 0 |  | Siyuan Qi, Bangcheng Yang, Kailin Jiang, Xiaobo Wang, Jiaqi Li, Yifan Zhong, Yaodong Yang, Zilong Zheng |  |
| 3497 |  |  [CL-DiffPhyCon: Closed-loop Diffusion Control of Complex Physical Systems](https://openreview.net/forum?id=PiHGrTTnvb) |  | 0 |  | Long Wei, Haodong Feng, Yuchen Yang, Ruiqi Feng, Peiyan Hu, Xiang Zheng, Tao Zhang, Dixia Fan, Tailin Wu |  |
| 3498 |  |  [OSTQuant: Refining Large Language Model Quantization with Orthogonal and Scaling Transformations for Better Distribution Fitting](https://openreview.net/forum?id=rAcgDBdKnP) |  | 0 |  | Xing Hu, Yuan Cheng, Dawei Yang, Zhixuan Chen, Zukang Xu, Jiangyong Yu, Chen Xu, Zhihang Yuan, Zhe Jiang, Sifan Zhou |  |
| 3499 |  |  [Distribution Backtracking Builds A Faster Convergence Trajectory for Diffusion Distillation](https://openreview.net/forum?id=2ySt3cdGfJ) |  | 0 |  | Shengyuan Zhang, Ling Yang, Zejian Li, An Zhao, Chenye Meng, Changyuan Yang, Guang Yang, Zhiyuan Yang, Lingyun Sun |  |
| 3500 |  |  [Pursuing Better Decision Boundaries for Long-Tailed Object Detection via Category Information Amount](https://openreview.net/forum?id=LW55JrLYPg) |  | 0 |  | Yanbiao Ma, Wei Dai, Jiayi Chen |  |
| 3501 |  |  [ShortcutsBench: A Large-Scale Real-world Benchmark for API-based Agents](https://openreview.net/forum?id=kKILfPkhSz) |  | 0 |  | Haiyang Shen, Yue Li, Desong Meng, Dongqi Cai, Sheng Qi, Li Zhang, Mengwei Xu, Yun Ma |  |
| 3502 |  |  [Sitcom-Crafter: A Plot-Driven Human Motion Generation System in 3D Scenes](https://openreview.net/forum?id=FvIASa0tau) |  | 0 |  | Jianqi Chen, Panwen Hu, Xiaojun Chang, Zhenwei Shi, Michael Kampffmeyer, Xiaodan Liang |  |
| 3503 |  |  [Gap Preserving Distillation by Building Bidirectional Mappings with A Dynamic Teacher](https://openreview.net/forum?id=PnfghHD4Pi) |  | 0 |  | Yong Guo, Shulian Zhang, Haolin Pan, Jing Liu, Yulun Zhang, Jian Chen |  |
| 3504 |  |  [Bridging Information Asymmetry in Text-video Retrieval: A Data-centric Approach](https://openreview.net/forum?id=Tn6lrFbiP4) |  | 0 |  | Zechen Bai, Tianjun Xiao, Tong He, Pichao Wang, Zheng Zhang, Thomas Brox, Mike Zheng Shou |  |
| 3505 |  |  [Streaming Video Question-Answering with In-context Video KV-Cache Retrieval](https://openreview.net/forum?id=8g9fs6mdEG) |  | 0 |  | Shangzhe Di, Zhelun Yu, Guanghao Zhang, Haoyuan Li, Tao Zhong, Hao Cheng, Bolin Li, Wanggui He, Fangxun Shu, Hao Jiang |  |
| 3506 |  |  [PuzzleFusion++: Auto-agglomerative 3D Fracture Assembly by Denoise and Verify](https://openreview.net/forum?id=7E7v5mJnfl) |  | 0 |  | Zhengqing Wang, Jiacheng Chen, Yasutaka Furukawa |  |
| 3507 |  |  [Ready-to-React: Online Reaction Policy for Two-Character Interaction Generation](https://openreview.net/forum?id=mm0cqJ2O3f) |  | 0 |  | Zhi Cen, Huaijin Pi, Sida Peng, Qing Shuai, Yujun Shen, Hujun Bao, Xiaowei Zhou, Ruizhen Hu |  |
| 3508 |  |  [Tight Time Complexities in Parallel Stochastic Optimization with Arbitrary Computation Dynamics](https://openreview.net/forum?id=cUN8lJB4rD) |  | 0 |  | Alexander Tyurin |  |
| 3509 |  |  [MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks](https://openreview.net/forum?id=2rWbKbmOuM) |  | 0 |  | Jiacheng Chen, Tianhao Liang, Sherman Siu, Zhengqing Wang, Kai Wang, Yubo Wang, Yuansheng Ni, Ziyan Jiang, Wang Zhu, Bohan Lyu, Dongfu Jiang, Xuan He, Yuan Liu, Hexiang Hu, Xiang Yue, Wenhu Chen |  |
| 3510 |  |  [Rectified Diffusion: Straightness Is Not Your Need in Rectified Flow](https://openreview.net/forum?id=nEDToD1R8M) |  | 0 |  | FuYun Wang, Ling Yang, Zhaoyang Huang, Mengdi Wang, Hongsheng Li |  |
| 3511 |  |  [Diffusion-NPO: Negative Preference Optimization for Better Preference Aligned Generation of Diffusion Models](https://openreview.net/forum?id=iJi7nz5Cxc) |  | 0 |  | FuYun Wang, Yunhao Shui, Jingtan Piao, Keqiang Sun, Hongsheng Li |  |
| 3512 |  |  [Asymmetric Factorized Bilinear Operation for Vision Transformer](https://openreview.net/forum?id=MJyqwBVgMs) |  | 0 |  | Junjie Wu, Qilong Wang, Jiangtao Xie, Pengfei Zhu, Qinghua Hu |  |
| 3513 |  |  [Et-Seed: Efficient trajectory-Level SE(3) equivariant diffusion Policy](https://openreview.net/forum?id=OheAR2xrtb) |  | 0 |  | Chenrui Tie, Yue Chen, Ruihai Wu, Boxuan Dong, Zeyi Li, Chongkai Gao, Hao Dong |  |
| 3514 |  |  [An Optimal Discriminator Weighted Imitation Perspective for Reinforcement Learning](https://openreview.net/forum?id=9JtG4nN7ql) |  | 0 |  | Haoran Xu, Shuozhe Li, Harshit Sikchi, Scott Niekum, Amy Zhang |  |
| 3515 |  |  [DICE: End-to-end Deformation Capture of Hand-Face Interactions from a Single Image](https://openreview.net/forum?id=rfrtFwnF62) |  | 0 |  | Qingxuan Wu, Zhiyang Dou, Sirui Xu, Soshi Shimada, Chen Wang, Zhengming Yu, Yuan Liu, Cheng Lin, Zeyu Cao, Taku Komura, Vladislav Golyanik, Christian Theobalt, Wenping Wang, Lingjie Liu |  |
| 3516 |  |  [CatVTON: Concatenation Is All You Need for Virtual Try-On with Diffusion Models](https://openreview.net/forum?id=jt1h2dnmng) |  | 0 |  | Zheng Chong, Xiao Dong, Haoxiang Li, Shiyue Zhang, Wenqing Zhang, Hanqing Zhao, Xujie Zhang, Dongmei Jiang, Xiaodan Liang |  |
| 3517 |  |  [Hessian-Free Online Certified Unlearning](https://openreview.net/forum?id=C3TrHWanh5) |  | 0 |  | Xinbao Qiao, Meng Zhang, Ming Tang, Ermin Wei |  |
| 3518 |  |  [Video-STaR: Self-Training Enables Video Instruction Tuning with Any Supervision](https://openreview.net/forum?id=JYV2hrtFSv) |  | 0 |  | Orr Zohar, Xiaohan Wang, Yonatan Bitton, Idan Szpektor, Serena YeungLevy |  |
| 3519 |  |  [Field-DiT: Diffusion Transformer on Unified Video, 3D, and Game Field Generation](https://openreview.net/forum?id=w6YS9A78fq) |  | 0 |  | Kangfu Mei, Mo Zhou, Vishal M. Patel |  |
| 3520 |  |  [Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations](https://openreview.net/forum?id=94kQgWXojH) |  | 0 |  | Nick Jiang, Anish Kachinthaya, Suzanne Petryk, Yossi Gandelsman |  |
| 3521 |  |  [Robustness of Quantum Algorithms for Nonconvex Optimization](https://openreview.net/forum?id=JyQYYjtO88) |  | 0 |  | Weiyuan Gong, Chenyi Zhang, Tongyang Li |  |
| 3522 |  |  [Fourier Head: Helping Large Language Models Learn Complex Probability Distributions](https://openreview.net/forum?id=4hPwLg7zD3) |  | 0 |  | Nate Gillman, Daksh Aggarwal, Michael Freeman, Chen Sun |  |
| 3523 |  |  [Locality Sensitive Avatars From Video](https://openreview.net/forum?id=SVta2eQNt3) |  | 0 |  | Chunjin Song, Zhijie Wu, ShihYang Su, Bastian Wandt, Leonid Sigal, Helge Rhodin |  |
| 3524 |  |  [Unhackable Temporal Reward for Scalable Video MLLMs](https://openreview.net/forum?id=Gf1uBeuUJW) |  | 0 |  | En Yu, Kangheng Lin, Liang Zhao, Yana Wei, Zining Zhu, Haoran Wei, Jianjian Sun, Zheng Ge, Xiangyu Zhang, Jingyu Wang, Wenbing Tao |  |
| 3525 |  |  [Understanding Long Videos with Multimodal Language Models](https://openreview.net/forum?id=OxKi02I29I) |  | 0 |  | Kanchana Ranasinghe, Xiang Li, Kumara Kahatapitiya, Michael S. Ryoo |  |
| 3526 |  |  [Vec2Face: Scaling Face Dataset Generation with Loosely Constrained Vectors](https://openreview.net/forum?id=RoN6NnHjn4) |  | 0 |  | Haiyu Wu, Jaskirat Singh, Sicong Tian, Liang Zheng, Kevin W. Bowyer |  |
| 3527 |  |  [Discrete Codebook World Models for Continuous Control](https://openreview.net/forum?id=lfRYzd8ady) |  | 0 |  | Aidan Scannell, Mohammadreza Nakhaeinezhadfard, Kalle Kujanpää, Yi Zhao, Kevin Sebastian Luck, Arno Solin, Joni Pajarinen |  |
| 3528 |  |  [Connecting Federated ADMM to Bayes](https://openreview.net/forum?id=ipQrjRsl11) |  | 0 |  | Siddharth Swaroop, Mohammad Emtiyaz Khan, Finale DoshiVelez |  |
| 3529 |  |  [Robots Pre-train Robots: Manipulation-Centric Robotic Representation from Large-Scale Robot Datasets](https://openreview.net/forum?id=yTEwmr1TJb) |  | 0 |  | Guangqi Jiang, Yifei Sun, Tao Huang, Huanyu Li, Yongyuan Liang, Huazhe Xu |  |
| 3530 |  |  [TimeSuite: Improving MLLMs for Long Video Understanding via Grounded Tuning](https://openreview.net/forum?id=nAVejJURqZ) |  | 0 |  | Xiangyu Zeng, Kunchang Li, Chenting Wang, Xinhao Li, Tianxiang Jiang, Ziang Yan, Songze Li, Yansong Shi, Zhengrong Yue, Yi Wang, Yali Wang, Yu Qiao, Limin Wang |  |
| 3531 |  |  [CLIPure: Purification in Latent Space via CLIP for Adversarially Robust Zero-Shot Classification](https://openreview.net/forum?id=TQ2ZOy6miT) |  | 0 |  | Mingkun Zhang, Keping Bi, Wei Chen, Jiafeng Guo, Xueqi Cheng |  |
| 3532 |  |  [Solving Video Inverse Problems Using Image Diffusion Models](https://openreview.net/forum?id=TRWxFUzK9K) |  | 0 |  | Taesung Kwon, Jong Chul Ye |  |
| 3533 |  |  [Jailbreaking as a Reward Misspecification Problem](https://openreview.net/forum?id=uBnM3EFovQ) |  | 0 |  | Zhihui Xie, Jiahui Gao, Lei Li, Zhenguo Li, Qi Liu, Lingpeng Kong |  |
| 3534 |  |  [Streaming Video Understanding and Multi-round Interaction with Memory-enhanced Knowledge](https://openreview.net/forum?id=JbPb6RieNC) |  | 0 |  | Haomiao Xiong, Zongxin Yang, Jiazuo Yu, Yunzhi Zhuge, Lu Zhang, Jiawen Zhu, Huchuan Lu |  |
| 3535 |  |  [Beyond the convexity assumption: Realistic tabular data generation under quantifier-free real linear constraints](https://openreview.net/forum?id=rx0TCew0Lj) |  | 0 |  | Mihaela C. Stoian, Eleonora Giunchiglia |  |
| 3536 |  |  [Large Scale Knowledge Washing](https://openreview.net/forum?id=dXCpPgjTtd) |  | 0 |  | Yu Wang, Ruihan Wu, Zexue He, Xiusi Chen, Julian J. McAuley |  |
| 3537 |  |  [Self-Updatable Large Language Models by Integrating Context into Model Parameters](https://openreview.net/forum?id=aCPFCDL9QY) |  | 0 |  | Yu Wang, Xinshuang Liu, Xiusi Chen, Sean O'Brien, Junda Wu, Julian J. McAuley |  |
| 3538 |  |  [Process Reward Model with Q-value Rankings](https://openreview.net/forum?id=wQEdh2cgEk) |  | 0 |  | Wendi Li, Yixuan Li |  |
| 3539 |  |  [PostCast: Generalizable Postprocessing for Precipitation Nowcasting via Unsupervised Blurriness Modeling](https://openreview.net/forum?id=v2zcCDYMok) |  | 0 |  | Junchao Gong, Siwei Tu, Weidong Yang, Ben Fei, Kun Chen, Wenlong Zhang, Xiaokang Yang, Wanli Ouyang, Lei Bai |  |
| 3540 |  |  [ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation](https://openreview.net/forum?id=sGpCzsfd1K) |  | 0 |  | Cheng Yang, Chufan Shi, Yaxin Liu, Bo Shui, Junjie Wang, Mohan Jing, Linran Xu, Xinyu Zhu, Siheng Li, Yuxiang Zhang, Gongye Liu, Xiaomei Nie, Deng Cai, Yujiu Yang |  |
| 3541 |  |  [Does Training with Synthetic Data Truly Protect Privacy?](https://openreview.net/forum?id=C8niXBHjfO) |  | 0 |  | Yunpeng Zhao, Jie Zhang |  |
| 3542 |  |  [PT-T2I/V: An Efficient Proxy-Tokenized Diffusion Transformer for Text-to-Image/Video-Task](https://openreview.net/forum?id=lTrrnNdkOX) |  | 0 |  | Jing Wang, Ao Ma, Jiasong Feng, Dawei Leng, Yuhui Yin, Xiaodan Liang |  |
| 3543 |  |  [Fast Feedforward 3D Gaussian Splatting Compression](https://openreview.net/forum?id=DCandSZ2F1) |  | 0 |  | Yihang Chen, Qianyi Wu, Mengyao Li, Weiyao Lin, Mehrtash Harandi, Jianfei Cai |  |
| 3544 |  |  [Scalable Benchmarking and Robust Learning for Noise-Free Ego-Motion and 3D Reconstruction from Noisy Video](https://openreview.net/forum?id=Pz9zFea4MQ) |  | 0 |  | Xiaohao Xu, Tianyi Zhang, Shibo Zhao, Xiang Li, Sibo Wang, Yongqi Chen, Ye Li, Bhiksha Raj, Matthew JohnsonRoberson, Sebastian A. Scherer, Xiaonan Huang |  |
| 3545 |  |  [LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning](https://openreview.net/forum?id=LYawG8YkPa) |  | 0 |  | Zhe Li, Weihao Yuan, Yisheng He, Lingteng Qiu, Shenhao Zhu, Xiaodong Gu, Weichao Shen, Yuan Dong, Zilong Dong, Laurence Tianruo Yang |  |
| 3546 |  |  [Generating Physical Dynamics under Priors](https://openreview.net/forum?id=eNjXcP6C0H) |  | 0 |  | Zihan Zhou, Xiaoxue Wang, Tianshu Yu |  |
| 3547 |  |  [Denoising with a Joint-Embedding Predictive Architecture](https://openreview.net/forum?id=d4njmzM7jf) |  | 0 |  | Dengsheng Chen, Jie Hu, Xiaoming Wei, Enhua Wu |  |
| 3548 |  |  [Hyper-Connections](https://openreview.net/forum?id=9FqARW7dwB) |  | 0 |  | Defa Zhu, Hongzhi Huang, Zihao Huang, Yutao Zeng, Yunyao Mao, Banggu Wu, Qiyang Min, Xun Zhou |  |
| 3549 |  |  [Efficient Masked AutoEncoder for Video Object Counting and A Large-Scale Benchmark](https://openreview.net/forum?id=sY3anJ8C68) |  | 0 |  | Bing Cao, Quanhao Lu, Jiekang Feng, Qilong Wang, Pengfei Zhu, Qinghua Hu |  |
| 3550 |  |  [TS-LIF: A Temporal Segment Spiking Neuron Network for Time Series Forecasting](https://openreview.net/forum?id=rDe9yQQYKt) |  | 0 |  | Shibo Feng, Wanjin Feng, Xingyu Gao, Peilin Zhao, Zhiqi Shen |  |
| 3551 |  |  [ETA: Evaluating Then Aligning Safety of Vision Language Models at Inference Time](https://openreview.net/forum?id=QoDDNkx4fP) |  | 0 |  | Yi Ding, Bolian Li, Ruqi Zhang |  |
| 3552 |  |  [Ranking-aware adapter for text-driven image ordering with CLIP](https://openreview.net/forum?id=KbCh7zbw2K) |  | 0 |  | WeiHsiang Yu, YenYu Lin, MingHsuan Yang, YiHsuan Tsai |  |
| 3553 |  |  [ReMatching Dynamic Reconstruction Flow](https://openreview.net/forum?id=bwhI6bCGY1) |  | 0 |  | Sara Oblak, Despoina Paschalidou, Sanja Fidler, Matan Atzmon |  |
| 3554 |  |  [Analyzing and Boosting the Power of Fine-Grained Visual Recognition for Multi-modal Large Language Models](https://openreview.net/forum?id=p3NKpom1VL) |  | 0 |  | Hulingxiao He, Geng Li, Zijun Geng, Jinglin Xu, Yuxin Peng |  |
| 3555 |  |  [γ-MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models](https://openreview.net/forum?id=q44uq3tc2D) |  | 0 |  | Yaxin Luo, Gen Luo, Jiayi Ji, Yiyi Zhou, Xiaoshuai Sun, Zhiqiang Shen, Rongrong Ji |  |
| 3556 |  |  [Circuit Representation Learning with Masked Gate Modeling and Verilog-AIG Alignment](https://openreview.net/forum?id=US9k5TXVLZ) |  | 0 |  | Haoyuan Wu, Haisheng Zheng, Yuan Pu, Bei Yu |  |
| 3557 |  |  [AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark](https://openreview.net/forum?id=tTDUrseRRU) |  | 0 |  | Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer BarTal, JenqNeng Hwang, Saining Xie, Christopher D. Manning |  |
| 3558 |  |  [Diff-2-in-1: Bridging Generation and Dense Perception with Diffusion Models](https://openreview.net/forum?id=ZYd5wJSaMs) |  | 0 |  | Shuhong Zheng, Zhipeng Bao, Ruoyu Zhao, Martial Hebert, YuXiong Wang |  |
| 3559 |  |  [COME: Test-time Adaption by Conservatively Minimizing Entropy](https://openreview.net/forum?id=506BjJ1ziZ) |  | 0 |  | Qingyang Zhang, Yatao Bian, Xinke Kong, Peilin Zhao, Changqing Zhang |  |
| 3560 |  |  [Interpretable Vision-Language Survival Analysis with Ordinal Inductive Bias for Computational Pathology](https://openreview.net/forum?id=trj2Jq8riA) |  | 0 |  | Pei Liu, Luping Ji, Jiaxiang Gou, Bo Fu, Mao Ye |  |
| 3561 |  |  [SleepSMC: Ubiquitous Sleep Staging via Supervised Multimodal Coordination](https://openreview.net/forum?id=B5VEi5d3p2) |  | 0 |  | Shuo Ma, Yingwei Zhang, Yiqiang Chen, Hualei Wang, Yuan Jin, Wei Zhang, Ziyu Jia |  |
| 3562 |  |  [Towards Generalizable Reinforcement Learning via Causality-Guided Self-Adaptive Representations](https://openreview.net/forum?id=bMvqccRmKD) |  | 0 |  | Yupei Yang, Biwei Huang, Fan Feng, Xinyue Wang, Shikui Tu, Lei Xu |  |
| 3563 |  |  [From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data](https://openreview.net/forum?id=8m7p4k6Zeb) |  | 0 |  | Zheyang Xiong, Vasilis Papageorgiou, Kangwook Lee, Dimitris Papailiopoulos |  |
| 3564 |  |  [Enhancing Document Understanding with Group Position Embedding: A Novel Approach to Incorporate Layout Information](https://openreview.net/forum?id=Dj9a4zQsSl) |  | 0 |  | Yuke Zhu, Yue Zhang, Dongdong Liu, Chi Xie, Zihua Xiong, Bo Zheng, Sheng Guo |  |
| 3565 |  |  [Towards Realistic UAV Vision-Language Navigation: Platform, Benchmark, and Methodology](https://openreview.net/forum?id=rUvCIvI4eB) |  | 0 |  | Xiangyu Wang, Donglin Yang, Ziqin Wang, Hohin Kwan, Jinyu Chen, Wenjun Wu, Hongsheng Li, Yue Liao, Si Liu |  |
| 3566 |  |  [Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in Vision-Language Alignment](https://openreview.net/forum?id=cJQ1K2fjpD) |  | 0 |  | Chenhang Cui, An Zhang, Yiyang Zhou, Zhaorun Chen, Gelei Deng, Huaxiu Yao, TatSeng Chua |  |
| 3567 |  |  [Minimal Impact ControlNet: Advancing Multi-ControlNet Integration](https://openreview.net/forum?id=rzbSNDXgGD) |  | 0 |  | Shikun Sun, Min Zhou, Zixuan Wang, Xubin Li, Tiezheng Ge, Zijie Ye, Xiaoyu Qin, Junliang Xing, Bo Zheng, Jia Jia |  |
| 3568 |  |  [TASAR: Transfer-based Attack on Skeletal Action Recognition](https://openreview.net/forum?id=I393kV3bz4) |  | 0 |  | Yunfeng Diao, Baiqi Wu, Ruixuan Zhang, Ajian Liu, Xiaoshuai Hao, Xingxing Wei, Meng Wang, He Wang |  |
| 3569 |  |  [HiLo: A Learning Framework for Generalized Category Discovery Robust to Domain Shifts](https://openreview.net/forum?id=2eFq6S35iB) |  | 0 |  | Hongjun Wang, Sagar Vaze, Kai Han |  |
| 3570 |  |  [LoRanPAC: Low-rank Random Features and Pre-trained Models for Bridging Theory and Practice in Continual Learning](https://openreview.net/forum?id=bqv7M0wc4x) |  | 0 |  | Liangzu Peng, Juan Elenter, Joshua Agterberg, Alejandro Ribeiro, René Vidal |  |
| 3571 |  |  [I Can Hear You: Selective Robust Training for Deepfake Audio Detection](https://openreview.net/forum?id=2GcR9bO620) |  | 0 |  | Zirui Zhang, Wei Hao, Aroon Sankoh, William Lin, Emanuel MendiolaOrtiz, Junfeng Yang, Chengzhi Mao |  |
| 3572 |  |  [CoInD: Enabling Logical Compositions in Diffusion Models](https://openreview.net/forum?id=cCRlEvjrx4) |  | 0 |  | Sachit Gaudi, Gautam Sreekumar, Vishnu Boddeti |  |
| 3573 |  |  [HAMSTER: Hierarchical Action Models for Open-World Robot Manipulation](https://openreview.net/forum?id=h7aQxzKbq6) |  | 0 |  | Yi Li, Yuquan Deng, Jesse Zhang, Joel Jang, Marius Memmel, Caelan Reed Garrett, Fabio Ramos, Dieter Fox, Anqi Li, Abhishek Gupta, Ankit Goyal |  |
| 3574 |  |  [Mitigating Object Hallucination in MLLMs via Data-augmented Phrase-level Alignment](https://openreview.net/forum?id=yG1fW8igzP) |  | 0 |  | Pritam Sarkar, Sayna Ebrahimi, Ali Etemad, Ahmad Beirami, Sercan Ö. Arik, Tomas Pfister |  |
| 3575 |  |  [Breaking Neural Network Scaling Laws with Modularity](https://openreview.net/forum?id=5Qxx5KpFms) |  | 0 |  | Akhilan Boopathy, Sunshine Jiang, William Yue, Jaedong Hwang, Abhiram Iyer, Ila R. Fiete |  |
| 3576 |  |  [Automated Design of Agentic Systems](https://openreview.net/forum?id=t9U3LW7JVX) |  | 0 |  | Shengran Hu, Cong Lu, Jeff Clune |  |
| 3577 |  |  [STORM: Spatio-TempOral Reconstruction Model For Large-Scale Outdoor Scenes](https://openreview.net/forum?id=M2NFWRPMUd) |  | 0 |  | Jiawei Yang, Jiahui Huang, Boris Ivanovic, Yuxiao Chen, Yan Wang, Boyi Li, Yurong You, Apoorva Sharma, Maximilian Igl, Péter Karkus, Danfei Xu, Yue Wang, Marco Pavone |  |
| 3578 |  |  [Training Language Models on Synthetic Edit Sequences Improves Code Synthesis](https://openreview.net/forum?id=AqfUa08PCH) |  | 0 |  | Ulyana Piterbarg, Lerrel Pinto, Rob Fergus |  |
| 3579 |  |  [Learning under Temporal Label Noise](https://openreview.net/forum?id=5o0phqAhsP) |  | 0 |  | Sujay Nagaraj, Walter Gerych, Sana Tonekaboni, Anna Goldenberg, Berk Ustun, Thomas Hartvigsen |  |
| 3580 |  |  [Regretful Decisions under Label Noise](https://openreview.net/forum?id=7B9FCDoUzB) |  | 0 |  | Sujay Nagaraj, Yang Liu, Flávio P. Calmon, Berk Ustun |  |
| 3581 |  |  [LaGeM: A Large Geometry Model for 3D Representation Learning and Diffusion](https://openreview.net/forum?id=72OSO38a2z) |  | 0 |  | Biao Zhang, Peter Wonka |  |
| 3582 |  |  [InterLCM: Low-Quality Images as Intermediate States of Latent Consistency Models for Effective Blind Face Restoration](https://openreview.net/forum?id=rUxr9Ll5FQ) |  | 0 |  | Senmao Li, Kai Wang, Joost van de Weijer, Fahad Shahbaz Khan, ChunLe Guo, Shiqi Yang, Yaxing Wang, Jian Yang, MingMing Cheng |  |
| 3583 |  |  [Generalization Bounds for Canonicalization: A Comparative Study with Group Averaging](https://openreview.net/forum?id=n0lXaskyk5) |  | 0 |  | Behrooz Tahmasebi, Stefanie Jegelka |  |
| 3584 |  |  [VibeCheck: Discover and Quantify Qualitative Differences in Large Language Models](https://openreview.net/forum?id=acxHV6werE) |  | 0 |  | Lisa Dunlap, Krishna Mandal, Trevor Darrell, Jacob Steinhardt, Joseph E. Gonzalez |  |
| 3585 |  |  [3D-Spatial Multimodal Memory](https://openreview.net/forum?id=XYdstv3ySl) |  | 0 |  | Xueyan Zou, Yuchen Song, RiZhao Qiu, Xuanbin Peng, Jianglong Ye, Sifei Liu, Xiaolong Wang |  |
| 3586 |  |  [Identification of Intermittent Temporal Latent Process](https://openreview.net/forum?id=6Pz7afmsOp) |  | 0 |  | Yuke Li, Yujia Zheng, Guangyi Chen, Kun Zhang, Heng Huang |  |
| 3587 |  |  [Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis](https://openreview.net/forum?id=GJsuYHhAga) |  | 0 |  | Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, QingGuo Chen, Xiangtai Li, Zhen Dong, Lei Zhu, Shuicheng Yan |  |
| 3588 |  |  [AHA: A Vision-Language-Model for Detecting and Reasoning Over Failures in Robotic Manipulation](https://openreview.net/forum?id=JVkdSi7Ekg) |  | 0 |  | Jiafei Duan, Wilbert Pumacay, Nishanth Kumar, Yi Ru Wang, Shulin Tian, Wentao Yuan, Ranjay Krishna, Dieter Fox, Ajay Mandlekar, Yijie Guo |  |
| 3589 |  |  [Shapley-Guided Utility Learning for Effective Graph Inference Data Valuation](https://openreview.net/forum?id=8X74NZpARg) |  | 0 |  | Hongliang Chi, Qiong Wu, Zhengyi Zhou, Yao Ma |  |
| 3590 |  |  [DreamDistribution: Learning Prompt Distribution for Diverse In-distribution Generation](https://openreview.net/forum?id=oQoQ4u6MQC) |  | 0 |  | Brian Nlong Zhao, Yuhang Xiao, Jiashu Xu, Xinyang Jiang, Yifan Yang, Dongsheng Li, Laurent Itti, Vibhav Vineet, Yunhao Ge |  |
| 3591 |  |  [Personality Alignment of Large Language Models](https://openreview.net/forum?id=0DZEs8NpUH) |  | 0 |  | Minjun Zhu, Yixuan Weng, Linyi Yang, Yue Zhang |  |
| 3592 |  |  [CycleResearcher: Improving Automated Research via Automated Review](https://openreview.net/forum?id=bjcsVLoHYs) |  | 0 |  | Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, Linyi Yang |  |
| 3593 |  |  [Towards Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It](https://openreview.net/forum?id=6oWFn6fY4A) |  | 0 |  | Guoxuan Xia, Olivier Laurent, Gianni Franchi, ChristosSavvas Bouganis |  |
| 3594 |  |  [Towards Improving Exploration through Sibling Augmented GFlowNets](https://openreview.net/forum?id=HH4KWP8RP5) |  | 0 |  | Kanika Madan, Alex Lamb, Emmanuel Bengio, Glen Berseth, Yoshua Bengio |  |
| 3595 |  |  [Compositional 4D Dynamic Scenes Understanding with Physics Priors for Video Question Answering](https://openreview.net/forum?id=6Vx28LSR7f) |  | 0 |  | Xingrui Wang, Wufei Ma, Angtian Wang, Shuo Chen, Adam Kortylewski, Alan L. Yuille |  |
| 3596 |  |  [UGMathBench: A Diverse and Dynamic Benchmark for Undergraduate-Level Mathematical Reasoning with Large Language Models](https://openreview.net/forum?id=fovPyqPcKY) |  | 0 |  | Xin Xu, Jiaxin Zhang, Tianhao Chen, Zitong Chao, Jishan Hu, Can Yang |  |
| 3597 |  |  [Can LLMs Solve Longer Math Word Problems Better?](https://openreview.net/forum?id=C9ju8QQSCv) |  | 0 |  | Xin Xu, Tong Xiao, Zitong Chao, Zhenya Huang, Can Yang, Yang Wang |  |
| 3598 |  |  [Progressive Token Length Scaling in Transformer Encoders for Efficient Universal Segmentation](https://openreview.net/forum?id=dmzM5UdAq6) |  | 0 |  | Abhishek Aich, Yumin Suh, Samuel Schulter, Manmohan Chandraker |  |
| 3599 |  |  [Understanding Matrix Function Normalizations in Covariance Pooling through the Lens of Riemannian Geometry](https://openreview.net/forum?id=q1t0Lmvhty) |  | 0 |  | Ziheng Chen, Yue Song, Xiaojun Wu, Gaowen Liu, Nicu Sebe |  |
| 3600 |  |  [Gyrogroup Batch Normalization](https://openreview.net/forum?id=d1NWq4PjJW) |  | 0 |  | Ziheng Chen, Yue Song, Xiaojun Wu, Nicu Sebe |  |
| 3601 |  |  [T-JEPA: Augmentation-Free Self-Supervised Learning for Tabular Data](https://openreview.net/forum?id=gx3LMRB15C) |  | 0 |  | Hugo Thimonier, José Lucas De Melo Costa, Fabrice Popineau, Arpad Rimmel, BichLiên Doan |  |
| 3602 |  |  [Steering Masked Discrete Diffusion Models via Discrete Denoising Posterior Prediction](https://openreview.net/forum?id=Ombm8S40zN) |  | 0 |  | Jarrid RectorBrooks, Mohsin Hasan, Zhangzhi Peng, ChengHao Liu, Sarthak Mittal, Nouha Dziri, Michael M. Bronstein, Pranam Chatterjee, Alexander Tong, Joey Bose |  |
| 3603 |  |  [Learning to Adapt Frozen CLIP for Few-Shot Test-Time Domain Adaptation](https://openreview.net/forum?id=TD3SGJfBC7) |  | 0 |  | Zhixiang Chi, Li Gu, Huan Liu, Ziqiang Wang, Yanan Wu, Yang Wang, Konstantinos N. Plataniotis |  |
| 3604 |  |  [Tuning-Free Bilevel Optimization: New Algorithms and Convergence Analysis](https://openreview.net/forum?id=A4aG3XeIO7) |  | 0 |  | Yifan Yang, Hao Ban, Minhui Huang, Shiqian Ma, Kaiyi Ji |  |
| 3605 |  |  [ParetoFlow: Guided Flows in Multi-Objective Optimization](https://openreview.net/forum?id=mLyyB4le5u) |  | 0 |  | Ye Yuan, Can Chen, Christopher Pal, Xue Liu |  |
| 3606 |  |  [Quantized Spike-driven Transformer](https://openreview.net/forum?id=5J9B7Sb8rO) |  | 0 |  | Xuerui Qiu, Malu Zhang, Jieyuan Zhang, Wenjie Wei, Honglin Cao, Junsheng Guo, RuiJie Zhu, Yimeng Shan, Yang Yang, Haizhou Li |  |
| 3607 |  |  [Matérn Kernels for Tunable Implicit Surface Reconstruction](https://openreview.net/forum?id=Ox4AJ2Vurb) |  | 0 |  | Maximilian Weiherer, Bernhard Egger |  |
| 3608 |  |  [Hierarchically Encapsulated Representation for Protocol Design in Self-Driving Labs](https://openreview.net/forum?id=9nUBh4V6SA) |  | 0 |  | YuZhe Shi, Mingchen Liu, Fanxu Meng, Qiao Xu, Zhangqian Bi, Kun He, Lecheng Ruan, Qining Wang |  |
| 3609 |  |  [Mechanism and Emergence of Stacked Attention Heads in Multi-Layer Transformers](https://openreview.net/forum?id=rUC7tHecSQ) |  | 0 |  | Tiberiu Musat |  |
| 3610 |  |  [Progressive Parameter Efficient Transfer Learning for Semantic Segmentation](https://openreview.net/forum?id=YNbLUGDAX5) |  | 0 |  | Nan Zhou, Huiqun Wang, Yaoyan Zheng, Di Huang |  |
| 3611 |  |  [Dynamic Low-Rank Sparse Adaptation for Large Language Models](https://openreview.net/forum?id=oXh0939Zzq) |  | 0 |  | Weizhong Huang, Yuxin Zhang, Xiawu Zheng, Yang Liu, Jing Lin, Yiwu Yao, Rongrong Ji |  |
| 3612 |  |  [A Coefficient Makes SVRG Effective](https://openreview.net/forum?id=twtTLZnG0B) |  | 0 |  | Yida Yin, Zhiqiu Xu, Zhiyuan Li, Trevor Darrell, Zhuang Liu |  |
| 3613 |  |  [DiffPC: Diffusion-based High Perceptual Fidelity Image Compression with Semantic Refinement](https://openreview.net/forum?id=RL7PycCtAO) |  | 0 |  | Yichong Xia, Yimin Zhou, Jinpeng Wang, Baoyi An, Haoqian Wang, Yaowei Wang, Bin Chen |  |
| 3614 |  |  [Unleashing the Power of Task-Specific Directions in Parameter Efficient Fine-tuning](https://openreview.net/forum?id=RYrJqz44p4) |  | 0 |  | Chongjie Si, Zhiyi Shi, Shifan Zhang, Xiaokang Yang, Hanspeter Pfister, Wei Shen |  |
| 3615 |  |  [Learning Shape-Independent Transformation via Spherical Representations for Category-Level Object Pose Estimation](https://openreview.net/forum?id=D4xztKoz0Y) |  | 0 |  | Huan Ren, Wenfei Yang, Xiang Liu, Shifeng Zhang, Tianzhu Zhang |  |
| 3616 |  |  [SuperCorrect: Advancing Small LLM Reasoning with Thought Template Distillation and Self-Correction](https://openreview.net/forum?id=PyjZO7oSw2) |  | 0 |  | Ling Yang, Zhaochen Yu, Tianjun Zhang, Minkai Xu, Joseph E. Gonzalez, Bin Cui, Shuicheng Yan |  |
| 3617 |  |  [Beyond Sequence: Impact of Geometric Context for RNA Property Prediction](https://openreview.net/forum?id=9htTvHkUhh) |  | 0 |  | Junjie Xu, Artem Moskalev, Tommaso Mansi, Mangal Prakash, Rui Liao |  |
| 3618 |  |  [FaceShot: Bring Any Character into Life](https://openreview.net/forum?id=oJA1GUqRww) |  | 0 |  | Junyao Gao, Yanan Sun, Fei Shen, Xin Jiang, Zhening Xing, Kai Chen, Cairong Zhao |  |
| 3619 |  |  [IgGM: A Generative Model for Functional Antibody and Nanobody Design](https://openreview.net/forum?id=zmmfsJpYcq) |  | 0 |  | Rubo Wang, Fandi Wu, Xingyu Gao, Jiaxiang Wu, Peilin Zhao, Jianhua Yao |  |
| 3620 |  |  [Refining CLIP's Spatial Awareness: A Visual-Centric Perspective](https://openreview.net/forum?id=38No4B8sx6) |  | 0 |  | Congpei Qiu, Yanhao Wu, Wei Ke, Xiuxiu Bai, Tong Zhang |  |
| 3621 |  |  [Do Deep Neural Network Solutions Form a Star Domain?](https://openreview.net/forum?id=QjO0fUlVYK) |  | 0 |  | Ankit Sonthalia, Alexander Rubinstein, Ehsan Abbasnejad, Seong Joon Oh |  |
| 3622 |  |  [Information Theoretic Text-to-Image Alignment](https://openreview.net/forum?id=Ugs2W5XFFo) |  | 0 |  | Chao Wang, Giulio Franzese, Alessandro Finamore, Massimo Gallo, Pietro Michiardi |  |
| 3623 |  |  [Semantix: An Energy-guided Sampler for Semantic Style Transfer](https://openreview.net/forum?id=si37wk8U5D) |  | 0 |  | Huiang He, Minghui Hu, Chuanxia Zheng, Chaoyue Wang, TatJen Cham |  |
| 3624 |  |  [GaussianAnything: Interactive Point Cloud Flow Matching for 3D Generation](https://openreview.net/forum?id=P4DbTSDQFu) |  | 0 |  | Yushi Lan, Shangchen Zhou, Zhaoyang Lyu, Fangzhou Hong, Shuai Yang, Bo Dai, Xingang Pan, Chen Change Loy |  |
| 3625 |  |  [TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies](https://openreview.net/forum?id=b1CVu9l5GO) |  | 0 |  | Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daumé III, Andrey Kolobov, Furong Huang, Jianwei Yang |  |
| 3626 |  |  [Gnothi Seauton: Empowering Faithful Self-Interpretability in Black-Box Transformers](https://openreview.net/forum?id=UvMSKonce8) |  | 0 |  | Shaobo Wang, Hongxuan Tang, Mingyang Wang, Hongrui Zhang, Xuyang Liu, Weiya Li, Xuming Hu, Linfeng Zhang |  |
| 3627 |  |  [DenseGrounding: Improving Dense Language-Vision Semantics for Ego-centric 3D Visual Grounding](https://openreview.net/forum?id=iGafR0hSln) |  | 0 |  | Henry Zheng, Hao Shi, Qihang Peng, Yong Xien Chng, Rui Huang, Yepeng Weng, Zhongchao Shi, Gao Huang |  |
| 3628 |  |  [Bayesian Analysis of Combinatorial Gaussian Process Bandits](https://openreview.net/forum?id=50cmx4SrkM) |  | 0 |  | Jack Sandberg, Niklas Åkerblom, Morteza Haghir Chehreghani |  |
| 3629 |  |  [Test-time Adaptation for Image Compression with Distribution Regularization](https://openreview.net/forum?id=bsnRUkVn63) |  | 0 |  | Kecheng Chen, Pingping Zhang, Tiexin Qin, Shiqi Wang, Hong Yan, Haoliang Li |  |
| 3630 |  |  [Controllable Blur Data Augmentation Using 3D-Aware Motion Estimation](https://openreview.net/forum?id=Wvi8c0tgvt) |  | 0 |  | Insoo Kim, Hana Lee, HyongEuk Lee, Jinwoo Shin |  |
| 3631 |  |  [MaxCutPool: differentiable feature-aware Maxcut for pooling in graph neural networks](https://openreview.net/forum?id=xlbXRJ2XCP) |  | 0 |  | Carlo Abate, Filippo Maria Bianchi |  |
| 3632 |  |  [FakeShield: Explainable Image Forgery Detection and Localization via Multi-modal Large Language Models](https://openreview.net/forum?id=pAQzEY7M03) |  | 0 |  | Zhipei Xu, Xuanyu Zhang, Runyi Li, Zecheng Tang, Qing Huang, Jian Zhang |  |
| 3633 |  |  [SecureGS: Boosting the Security and Fidelity of 3D Gaussian Splatting Steganography](https://openreview.net/forum?id=H4FSx06FCZ) |  | 0 |  | Xuanyu Zhang, Jiarui Meng, Zhipei Xu, Shuzhou Yang, Yanmin Wu, Ronggang Wang, Jian Zhang |  |
| 3634 |  |  [GenDataAgent: On-the-fly Dataset Augmentation with Synthetic Data](https://openreview.net/forum?id=WoGnnggVCZ) |  | 0 |  | Zhiteng Li, Lele Chen, Jerone T. A. Andrews, Yunhao Ba, Yulun Zhang, Alice Xiang |  |
| 3635 |  |  [VideoGrain: Modulating Space-Time Attention for Multi-Grained Video Editing](https://openreview.net/forum?id=SSslAtcPB6) |  | 0 |  | Xiangpeng Yang, Linchao Zhu, Hehe Fan, Yi Yang |  |
| 3636 |  |  [Let the Code LLM Edit Itself When You Edit the Code](https://openreview.net/forum?id=zqzsZ5cXbB) |  | 0 |  | Zhenyu He, Jun Zhang, Shengjie Luo, Jingjing Xu, Zhi Zhang, Di He |  |
| 3637 |  |  [CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes](https://openreview.net/forum?id=a3ptUbuzbW) |  | 0 |  | Yang Liu, Chuanchen Luo, Zhongkai Mao, Junran Peng, Zhaoxiang Zhang |  |
| 3638 |  |  [Debiasing Mini-Batch Quadratics for Applications in Deep Learning](https://openreview.net/forum?id=Q0TEVKV2cp) |  | 0 |  | Lukas Tatzel, Bálint Mucsányi, Osane Hackel, Philipp Hennig |  |
| 3639 |  |  [Denoising as Adaptation: Noise-Space Domain Adaptation for Image Restoration](https://openreview.net/forum?id=jsBhmOCKYs) |  | 0 |  | Kang Liao, Zongsheng Yue, Zhouxia Wang, Chen Change Loy |  |
| 3640 |  |  [Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Dynamic Scenes](https://openreview.net/forum?id=LuGHbK8qTa) |  | 0 |  | Isabella Liu, Hao Su, Xiaolong Wang |  |
| 3641 |  |  [Robust Watermarking Using Generative Priors Against Image Editing: From Benchmarking to Advances](https://openreview.net/forum?id=16O8GCm8Wn) |  | 0 |  | Shilin Lu, Zihan Zhou, Jiayou Lu, Yuanzhi Zhu, Adams WaiKin Kong |  |
| 3642 |  |  [Revisiting In-context Learning Inference Circuit in Large Language Models](https://openreview.net/forum?id=xizpnYNvQq) |  | 0 |  | Hakaze Cho, Mariko Kato, Yoshihiro Sakai, Naoya Inoue |  |
| 3643 |  |  [FreeVS: Generative View Synthesis on Free Driving Trajectory](https://openreview.net/forum?id=dTGH9vUVdf) |  | 0 |  | Qitai Wang, Lue Fan, Yuqi Wang, Yuntao Chen, Zhaoxiang Zhang |  |
| 3644 |  |  [Enhancing End-to-End Autonomous Driving with Latent World Model](https://openreview.net/forum?id=fd2u60ryG0) |  | 0 |  | Yingyan Li, Lue Fan, Jiawei He, Yuqi Wang, Yuntao Chen, Zhaoxiang Zhang, Tieniu Tan |  |
| 3645 |  |  [Collapsed Language Models Promote Fairness](https://openreview.net/forum?id=kynD1UUk6q) |  | 0 |  | Jingxuan Xu, Wuyang Chen, Linyi Li, Yao Zhao, Yunchao Wei |  |
| 3646 |  |  [Long-horizon Visual Instruction Generation with Logic and Attribute Self-reflection](https://openreview.net/forum?id=EdMb9TqqDY) |  | 0 |  | Yucheng Suo, Fan Ma, Kaixin Shen, Linchao Zhu, Yi Yang |  |
| 3647 |  |  [Learning Harmonized Representations for Speculative Sampling](https://openreview.net/forum?id=T9u56s7mbk) |  | 0 |  | Lefan Zhang, Xiaodan Wang, Yanhua Huang, Ruiwen Xu |  |
| 3648 |  |  [MDSGen: Fast and Efficient Masked Diffusion Temporal-Aware Transformers for Open-Domain Sound Generation](https://openreview.net/forum?id=yFEqYwgttJ) |  | 0 |  | Trung X. Pham, Tri Ton, Chang D. Yoo |  |
| 3649 |  |  [SONICS: Synthetic Or Not - Identifying Counterfeit Songs](https://openreview.net/forum?id=PY7KSh29Z8) |  | 0 |  | Md Awsafur Rahman, Zaber Ibn Abdul Hakim, Najibul Haque Sarker, Bishmoy Paul, Shaikh Anowarul Fattah |  |
| 3650 |  |  [Self-Correcting Decoding with Generative Feedback for Mitigating Hallucinations in Large Vision-Language Models](https://openreview.net/forum?id=tTBXePRKSx) |  | 0 |  | Ce Zhang, Zifu Wan, Zhehan Kan, Martin Q. Ma, Simon Stepputtis, Deva Ramanan, Russ Salakhutdinov, LouisPhilippe Morency, Katia P. Sycara, Yaqi Xie |  |
| 3651 |  |  [Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation](https://openreview.net/forum?id=jxo70B9fQo) |  | 0 |  | Yiming Wang, Pei Zhang, Baosong Yang, Derek F. Wong, Rui Wang |  |
| 3652 |  |  [FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality](https://openreview.net/forum?id=W49UjcpGxx) |  | 0 |  | Zhengyao Lv, Chenyang Si, Junhao Song, Zhenyu Yang, Yu Qiao, Ziwei Liu, KwanYee K. Wong |  |
| 3653 |  |  [Correlation and Navigation in the Vocabulary Key Representation Space of Language Models](https://openreview.net/forum?id=VipcVxaTnG) |  | 0 |  | Letian Peng, Chenyang An, Jingbo Shang |  |
| 3654 |  |  [Reconstructive Visual Instruction Tuning](https://openreview.net/forum?id=8q9NOMzRDg) |  | 0 |  | Haochen Wang, Anlin Zheng, Yucheng Zhao, Tiancai Wang, Zheng Ge, Xiangyu Zhang, Zhaoxiang Zhang |  |
| 3655 |  |  [BLEND: Behavior-guided Neural Population Dynamics Modeling via Privileged Knowledge Distillation](https://openreview.net/forum?id=jE5ZbtMtcU) |  | 0 |  | Zhengrui Guo, Fangxu Zhou, Wei Wu, Qichen Sun, Lishuang Feng, Jinzhuo Wang, Hao Chen |  |
| 3656 |  |  [COMBO: Compositional World Models for Embodied Multi-Agent Cooperation](https://openreview.net/forum?id=YXRyYkb1im) |  | 0 |  | Hongxin Zhang, Zeyuan Wang, Qiushi Lyu, Zheyuan Zhang, Sunli Chen, Tianmin Shu, Behzad Dariush, Kwonjoon Lee, Yilun Du, Chuang Gan |  |
| 3657 |  |  [Image-level Memorization Detection via Inversion-based Inference Perturbation](https://openreview.net/forum?id=vwOq7twk7L) |  | 0 |  | Yue Jiang, Haokun Lin, Yang Bai, Bo Peng, Zhili Liu, Yueming Lyu, Yong Yang, Xing Zheng, Jing Dong |  |
| 3658 |  |  [SaRA: High-Efficient Diffusion Model Fine-tuning with Progressive Sparse Low-Rank Adaptation](https://openreview.net/forum?id=wGVOxplEbf) |  | 0 |  | Teng Hu, Jiangning Zhang, Ran Yi, Hongrui Huang, Yabiao Wang, Lizhuang Ma |  |
| 3659 |  |  [Microcanonical Langevin Ensembles: Advancing the Sampling of Bayesian Neural Networks](https://openreview.net/forum?id=QMtrW8Ej98) |  | 0 |  | Emanuel Sommer, Jakob Robnik, Giorgi Nozadze, Uros Seljak, David Rügamer |  |
| 3660 |  |  [Posterior-Mean Rectified Flow: Towards Minimum MSE Photo-Realistic Image Restoration](https://openreview.net/forum?id=hPOt3yUXii) |  | 0 |  | Guy Ohayon, Tomer Michaeli, Michael Elad |  |
| 3661 |  |  [Deep Weight Factorization: Sparse Learning Through the Lens of Artificial Symmetries](https://openreview.net/forum?id=vNdOHr7mn5) |  | 0 |  | Chris Kolb, Tobias Weber, Bernd Bischl, David Rügamer |  |
| 3662 |  |  [MotionClone: Training-Free Motion Cloning for Controllable Video Generation](https://openreview.net/forum?id=aY3L65HgHJ) |  | 0 |  | Pengyang Ling, Jiazi Bu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Tong Wu, Huaian Chen, Jiaqi Wang, Yi Jin |  |
| 3663 |  |  [Learning View-invariant World Models for Visual Robotic Manipulation](https://openreview.net/forum?id=vJwjWyt4Ed) |  | 0 |  | JingCheng Pang, Nan Tang, Kaiyuan Li, Yuting Tang, XinQiang Cai, ZhenYu Zhang, Gang Niu, Masashi Sugiyama, Yang Yu |  |
| 3664 |  |  [Recovery of Causal Graph Involving Latent Variables via Homologous Surrogates](https://openreview.net/forum?id=fGhr39bqZa) |  | 0 |  | XiuChuan Li, Jun Wang, Tongliang Liu |  |
| 3665 |  |  [Reconstruction-Guided Policy: Enhancing Decision-Making through Agent-Wise State Consistency](https://openreview.net/forum?id=Y8L5RB4GWb) |  | 0 |  | Qifan Liang, Yixiang Shan, Haipeng Liu, Zhengbang Zhu, Ting Long, Weinan Zhang, Yuan Tian |  |
| 3666 |  |  [Efficient and Trustworthy Causal Discovery with Latent Variables and Complex Relations](https://openreview.net/forum?id=BZYIEw4mcY) |  | 0 |  | XiuChuan Li, Tongliang Liu |  |
| 3667 |  |  [DeepGate4: Efficient and Effective Representation Learning for Circuit Design at Scale](https://openreview.net/forum?id=b10lRabU9W) |  | 0 |  | Ziyang Zheng, Shan Huang, Jianyuan Zhong, Zhengyuan Shi, Guohao Dai, Ningyi Xu, Qiang Xu |  |
| 3668 |  |  [TGB-Seq Benchmark: Challenging Temporal GNNs with Complex Sequential Dynamics](https://openreview.net/forum?id=8e2LirwiJT) |  | 0 |  | Lu Yi, Jie Peng, Yanping Zheng, Fengran Mo, Zhewei Wei, Yuhang Ye, Yue Zixuan, Zengfeng Huang |  |
| 3669 |  |  [ContraDiff: Planning Towards High Return States via Contrastive Learning](https://openreview.net/forum?id=XMOaOigOQo) |  | 0 |  | Yixiang Shan, Zhengbang Zhu, Ting Long, Qifan Liang, Yi Chang, Weinan Zhang, Liang Yin |  |
| 3670 |  |  [FreeCG: Free the Design Space of Clebsch-Gordan Transform for Machine Learning Force Fields](https://openreview.net/forum?id=sfi2j1Ot6j) |  | 0 |  | Shihao Shao, Haoran Geng, Zun Wang, Qinghua Cui |  |
| 3671 |  |  [Diffusion2: Dynamic 3D Content Generation via Score Composition of Video and Multi-view Diffusion Models](https://openreview.net/forum?id=fectsEG2GU) |  | 0 |  | Zeyu Yang, Zijie Pan, Chun Gu, Li Zhang |  |
| 3672 |  |  [Understanding and Mitigating Hallucination in Large Vision-Language Models via Modular Attribution and Intervention](https://openreview.net/forum?id=Bjq4W7P2Us) |  | 0 |  | Tianyun Yang, Ziniu Li, Juan Cao, Chang Xu |  |
| 3673 |  |  [Neuron Platonic Intrinsic Representation From Dynamics Using Contrastive Learning](https://openreview.net/forum?id=vFanHFE4Qv) |  | 0 |  | Wei Wu, Can Liao, Zizhen Deng, Zhengrui Guo, Jinzhuo Wang |  |
| 3674 |  |  [Where Am I and What Will I See: An Auto-Regressive Model for Spatial Localization and View Prediction](https://openreview.net/forum?id=NuHYh4YKNe) |  | 0 |  | Junyi Chen, Di Huang, Weicai Ye, Wanli Ouyang, Tong He |  |
| 3675 |  |  [Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization](https://openreview.net/forum?id=omrLHFzC37) |  | 0 |  | Zhe Li, Bicheng Ying, Zidong Liu, Chaosheng Dong, Haibo Yang |  |
| 3676 |  |  [MindSimulator: Exploring Brain Concept Localization via Synthetic fMRI](https://openreview.net/forum?id=vgt2rSf6al) |  | 0 |  | Guangyin Bao, Qi Zhang, Zixuan Gong, Zhuojia Wu, Duoqian Miao |  |
| 3677 |  |  [ImDy: Human Inverse Dynamics from Imitated Observations](https://openreview.net/forum?id=br8YB7KMug) |  | 0 |  | Xinpeng Liu, Junxuan Liang, Zili Lin, Haowen Hou, YongLu Li, Cewu Lu |  |
| 3678 |  |  [PIORF: Physics-Informed Ollivier-Ricci Flow for Long-Range Interactions in Mesh Graph Neural Networks](https://openreview.net/forum?id=qkBBHixPow) |  | 0 |  | YounYeol Yu, Jeongwhan Choi, Jaehyeon Park, Kookjin Lee, Noseong Park |  |
| 3679 |  |  [ControlAR: Controllable Image Generation with Autoregressive Models](https://openreview.net/forum?id=BWuBDdXVnH) |  | 0 |  | Zongming Li, Tianheng Cheng, Shoufa Chen, Peize Sun, Haocheng Shen, Longjin Ran, Xiaoxin Chen, Wenyu Liu, Xinggang Wang |  |
| 3680 |  |  [Linear Multistep Solver Distillation for Fast Sampling of Diffusion Models](https://openreview.net/forum?id=vkOFOUDLTn) |  | 0 |  | Yuchen Liang, Xiangzhong Fang, Hanting Chen, Yunhe Wang |  |
| 3681 |  |  [ImagineNav: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination](https://openreview.net/forum?id=vQFw9ryKyK) |  | 0 |  | Xinxin Zhao, Wenzhe Cai, Likun Tang, Teng Wang |  |
| 3682 |  |  [Navigation-Guided Sparse Scene Representation for End-to-End Autonomous Driving](https://openreview.net/forum?id=Vv76fCYffN) |  | 0 |  | Peidong Li, Dixiao Cui |  |
| 3683 |  |  [An Evolved Universal Transformer Memory](https://openreview.net/forum?id=s1kyHkdTmi) |  | 0 |  | Edoardo Cetin, Qi Sun, Tianyu Zhao, Yujin Tang |  |
| 3684 |  |  [Interpretable Bilingual Multimodal Large Language Model for Diverse Biomedical Tasks](https://openreview.net/forum?id=YuHQTo6G9S) |  | 0 |  | Lehan Wang, Haonan Wang, Honglong Yang, Jiaji Mao, Zehong Yang, Jun Shen, Xiaomeng Li |  |
| 3685 |  |  [MindSearch: Mimicking Human Minds Elicits Deep AI Searcher](https://openreview.net/forum?id=xgtXkyqw1f) |  | 0 |  | Zehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu, Wenwei Zhang, Kai Chen, Feng Zhao |  |
| 3686 |  |  [PseDet: Revisiting the Power of Pseudo Label in Incremental Object Detection](https://openreview.net/forum?id=Iu8FVcUmVp) |  | 0 |  | Qiuchen Wang, Zehui Chen, Chenhongyi Yang, Jiaming Liu, Zhenyu Li, Feng Zhao |  |
| 3687 |  |  [Weak-to-Strong Generalization Through the Data-Centric Lens](https://openreview.net/forum?id=uogG8BfLs2) |  | 0 |  | Changho Shin, John Cooper, Frederic Sala |  |
| 3688 |  |  [Distilling Structural Representations into Protein Sequence Models](https://openreview.net/forum?id=KXrgDM3mVD) |  | 0 |  | Jeffrey OuyangZhang, Chengyue Gong, Yue Zhao, Philipp Krähenbühl, Adam R. Klivans, Daniel Jesus Diaz |  |
| 3689 |  |  [Probabilistic Language-Image Pre-Training](https://openreview.net/forum?id=D5X6nPGFUY) |  | 0 |  | Sanghyuk Chun, Wonjae Kim, Song Park, Sangdoo Yun |  |
| 3690 |  |  [Find A Winning Sign: Sign Is All We Need to Win the Lottery](https://openreview.net/forum?id=cLtE4qoPlD) |  | 0 |  | Junghun Oh, Sungyong Baik, Kyoung Mu Lee |  |
| 3691 |  |  [IPDreamer: Appearance-Controllable 3D Object Generation with Complex Image Prompts](https://openreview.net/forum?id=3PguviI7Uf) |  | 0 |  | Bohan Zeng, Shanglin Li, Yutang Feng, Ling Yang, Juan Zhang, Hong Li, Jiaming Liu, Conghui He, Wentao Zhang, Jianzhuang Liu, Baochang Zhang, Shuicheng Yan |  |
| 3692 |  |  [Gated Delta Networks: Improving Mamba2 with Delta Rule](https://openreview.net/forum?id=r8H7xhYPwz) |  | 0 |  | Songlin Yang, Jan Kautz, Ali Hatamizadeh |  |
| 3693 |  |  [Rethinking Classifier Re-Training in Long-Tailed Recognition: Label Over-Smooth Can Balance](https://openreview.net/forum?id=OeKp3AdiVO) |  | 0 |  | Siyu Sun, Han Lu, Jiangtong Li, Yichen Xie, Tianjiao Li, Xiaokang Yang, Liqing Zhang, Junchi Yan |  |
| 3694 |  |  [STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs](https://openreview.net/forum?id=6XUSDvBFkV) |  | 0 |  | Peijie Dong, Lujun Li, Yuedong Zhong, Dayou Du, Ruibo Fan, Yuhan Chen, Zhenheng Tang, Qiang Wang, Wei Xue, Yike Guo, Xiaowen Chu |  |
| 3695 |  |  [3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation](https://openreview.net/forum?id=Gx04TnVjee) |  | 0 |  | Xiao Fu, Xian Liu, Xintao Wang, Sida Peng, Menghan Xia, Xiaoyu Shi, Ziyang Yuan, Pengfei Wan, Di Zhang, Dahua Lin |  |
| 3696 |  |  [Animate-X: Universal Character Image Animation with Enhanced Motion Representation](https://openreview.net/forum?id=1IuwdOI4Zb) |  | 0 |  | Shuai Tan, Biao Gong, Xiang Wang, Shiwei Zhang, Dandan Zheng, Ruobing Zheng, Kecheng Zheng, Jingdong Chen, Ming Yang |  |
| 3697 |  |  [Inner Information Analysis Algorithm for Deep Neural Network based on Community](https://openreview.net/forum?id=awz1JPyXNK) |  | 0 |  | Guipeng Lan, Shuai Xiao, Meng Xi, Jiabao Wen, Jiachen Yang |  |
| 3698 |  |  [AniSDF: Fused-Granularity Neural Surfaces with Anisotropic Encoding for High-Fidelity 3D Reconstruction](https://openreview.net/forum?id=v1f6c7wVBm) |  | 0 |  | Jingnan Gao, Zhuo Chen, Xiaokang Yang, Yichao Yan |  |
| 3699 |  |  [Scaling Large Language Model-based Multi-Agent Collaboration](https://openreview.net/forum?id=K3n5jPkrU6) |  | 0 |  | Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Kunlun Zhu, Hanchen Xia, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, Maosong Sun |  |
| 3700 |  |  [Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction](https://openreview.net/forum?id=stK7iOPH9Q) |  | 0 |  | Jing He, Haodong Li, Wei Yin, Yixun Liang, Leheng Li, Kaiqiang Zhou, Hongbo Zhang, Bingbing Liu, YingCong Chen |  |
| 3701 |  |  [ARB-LLM: Alternating Refined Binarizations for Large Language Models](https://openreview.net/forum?id=ZU8OdDLTts) |  | 0 |  | Zhiteng Li, Xianglong Yan, Tianao Zhang, Haotong Qin, Dong Xie, Jiang Tian, Zhongchao Shi, Linghe Kong, Yulun Zhang, Xiaokang Yang |  |
| 3702 |  |  [Understanding and Enhancing the Transferability of Jailbreaking Attacks](https://openreview.net/forum?id=asR9FVd4eL) |  | 0 |  | Runqi Lin, Bo Han, Fengwang Li, Tongliang Liu |  |
| 3703 |  |  [FLOPS: Forward Learning with OPtimal Sampling](https://openreview.net/forum?id=z1nSpA2dAW) |  | 0 |  | Tao Ren, Zishi Zhang, Jinyang Jiang, Guanghao Li, Zeliang Zhang, Mingqian Feng, Yijie Peng |  |
| 3704 |  |  [Storybooth: Training-Free Multi-Subject Consistency for Improved Visual Storytelling](https://openreview.net/forum?id=JZLon6cvx8) |  | 0 |  | Jaskirat Singh, Junshen K. Chen, Jonas Kohler, Michael F. Cohen |  |
