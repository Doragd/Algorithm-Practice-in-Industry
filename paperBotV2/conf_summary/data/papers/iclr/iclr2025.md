# ICLR2025

## 会议论文列表

本会议共有 3704 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [DarkBench: Benchmarking Dark Patterns in Large Language Models](https://openreview.net/forum?id=odjMSBSWRt) |  | 0 | We introduce DarkBench, a comprehensive benchmark for detecting dark design patterns—manipulative techniques that influence user behavior—in interactions with large language models (LLMs). Our benchmark comprises 660 prompts across six categories: brand bias, user retention, sycophancy,... | Akash Kundu, Esben Kran, Jinsuk Park, Jord Nguyen, Mateusz Maria Jurewicz, Sami Jawhar |  |
| 2 |  |  [RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style](https://openreview.net/forum?id=QEHrmQPBdd) |  | 0 | Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. Despite their importance, existing reward model benchmarks often evaluate models by asking them to... | Juanzi Li, Lei Hou, Rui Min, Yantao Liu, Yixin Cao, Zijun Yao |  |
| 3 |  |  [TopoLM: brain-like spatio-functional organization in a topographic language model](https://openreview.net/forum?id=aWXnKanInf) |  | 0 | Neurons in the brain are spatially organized such that neighbors on tissue often exhibit similar response profiles. In the human language system, experimental studies have observed clusters for syntactic and semantic categories, but the mechanisms underlying this functional organization remain... | Badr AlKhamissi, Johannes Mehrer, Martin Schrimpf, Neil Rathi, Nicholas M. Blauch, Taha Osama A Binhuraib |  |
| 4 |  |  [Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows](https://openreview.net/forum?id=XmProj9cPs) |  | 0 | Real-world enterprise text-to-SQL workflows often involve complex cloud or local data across various database systems, multiple SQL queries in various dialects, and diverse operations from data transformation to analytics. We introduce Spider 2.0, an evaluation framework comprising $632$ real-world... | Caiming Xiong, Dongchan Shin, Fangyu Lei, Hongcheng Gao, Hongjin Su, Jixuan Chen, Pengcheng Yin, Qian Liu, Ruisheng Cao, Ruoxi Sun, Sida Wang, Tao Yu, Victor Zhong, Wenjing Hu, Yuxiao Ye, Zhaoqing Suo |  |
| 5 |  |  [Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge Acquisition](https://openreview.net/forum?id=eHehzSDUFp) |  | 0 | In this work, we investigate how a model's tendency to broadly integrate its parametric knowledge evolves throughout pretraining, and how this behavior affects overall performance, particularly in terms of knowledge acquisition and forgetting. We introduce the concept of knowledge entropy, which... | Dohaeng Lee, Hyeonbin Hwang, Hyowon Cho, Hyunji Lee, Jiyeon Kim, Joel Jang, Minjoon Seo, Seungpil Won, Youbin Ahn |  |
| 6 |  |  [Diffusion-Based Planning for Autonomous Driving with Flexible Guidance](https://openreview.net/forum?id=wM2sfVgMDH) |  | 0 | Achieving human-like driving behaviors in complex open-world environments is a critical challenge in autonomous driving. Contemporary learning-based planning approaches such as imitation learning methods often struggle to balance competing objectives and lack of safety assurance,due to limited... | Jianxiong Li, Jingjing Liu, Jinliang Zheng, Kexin Zheng, Liyuan Mao, Rui Ai, Ruiming Liang, Shengbo Eben Li, Weihao Gu, Xianyuan Zhan, Yinan Zheng |  |
| 7 |  |  [Learning to Search from Demonstration Sequences](https://openreview.net/forum?id=v593OaNePQ) |  | 0 | Search and planning are essential for solving many real-world problems. However, in numerous learning scenarios, only action-observation sequences, such as demonstrations or instruction sequences, are available for learning. Relying solely on supervised learning with these sequences can lead to... | Dixant Mittal, Liwei Kang, Wee Sun Lee |  |
| 8 |  |  [Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse](https://openreview.net/forum?id=Iyrtb9EJBp) |  | 0 | LLMs are an integral component of retrieval-augmented generation (RAG) systems. While many studies focus on evaluating the overall quality of end-to-end RAG systems, there is a gap in understanding the appropriateness of LLMs for the RAG task. To address this, we introduce Trust-Score, a holistic... | Hai Leong Chieu, Maojia Song, Navonil Majumder, Rishabh Bhardwaj, Shang Hong Sim, Soujanya Poria |  |
| 9 |  |  [MAP: Multi-Human-Value Alignment Palette](https://openreview.net/forum?id=NN6QHwgRrQ) |  | 0 | Ensuring that generative AI systems align with human values is essential but challenging, especially when considering multiple human values and their potential trade-offs. Since human values can be personalized and dynamically change over time, the desirable levels of value alignment vary across... | Ali Anwar, Ammar Ahmed, Enmao Diao, Jie Ding, Nathalie Baracaldo, Qi Le, Xinran Wang, Yi Zhou |  |
| 10 |  |  [Can Neural Networks Achieve Optimal Computational-statistical Tradeoff? An Analysis on Single-Index Model](https://openreview.net/forum?id=is4nCVkSFA) |  | 0 | In this work, we tackle the following question: Can neural networks trained with gradient-based methods achieve the optimal statistical-computational tradeoff in learning Gaussian single-index models? Prior research has shown that any polynomial-time algorithm under the statistical query (SQ)... | Beining Wu, Miao Lu, Siyu Chen, Tianhao Wang, Zhuoran Yang |  |
| 11 |  |  [Consistency Checks for Language Model Forecasters](https://openreview.net/forum?id=r5IXBlTCGc) |  | 0 | Forecasting is a task that is difficult to evaluate: the ground truth can only be known in the future. Recent work showing LLM forecasters rapidly approaching human-level performance begs the question: how can we benchmark and evaluate these forecasters \*instantaneously\*? Following the... | Abhimanyu Pallavi Sudhir, Adam Shen, Alejandro Alvarez, Daniel Paleka, Evan Wang, Florian Tramèr, Vineeth Bhat |  |
| 12 |  |  [Spread Preference Annotation: Direct Preference Judgment for Efficient LLM Alignment](https://openreview.net/forum?id=BPgK5XW1Nb) |  | 0 | Aligning large language models (LLMs) with human preferences becomes a key component to obtaining state-of-the-art performance, but it yields a huge cost to construct a large human-annotated preference dataset. To tackle this problem, we propose a new framework, Spread Preference Annotation with... | Dongyoung Kim, Jaehyung Kim, Jinwoo Shin, Kimin Lee |  |
| 13 |  |  [Brain Bandit: A Biologically Grounded Neural Network for Efficient Control of Exploration](https://openreview.net/forum?id=RWJX5F5I9g) |  | 0 | How to balance between exploration and exploitation in an uncertain environment is a central challenge in reinforcement learning. In contrast, humans and animals have demonstrated superior exploration efficiency in novel environments. To understand how the brain’s neural network controls... | Chen Jiang, Jiahui An, Ni Ji, Yating Liu |  |
| 14 |  |  [MaestroMotif: Skill Design from Artificial Intelligence Feedback](https://openreview.net/forum?id=or8mMhmyRV) |  | 0 | Describing skills in natural language has the potential to provide an accessible way to inject human knowledge about decision-making into an AI system. We present MaestroMotif, a method for AI-assisted skill design, which yields high-performing and adaptable agents. MaestroMotif leverages the... | Amy Zhang, Doina Precup, Marlos C. Machado, Martin Klissarov, Mikael Henaff, Pascal Vincent, Pierluca D'Oro, PierreLuc Bacon, Roberta Raileanu, Shagun Sodhani |  |
| 15 |  |  [Learning to Discover Regulatory Elements for Gene Expression Prediction](https://openreview.net/forum?id=Mfnh1Sqdwf) |  | 0 | We consider the problem of predicting gene expressions from DNA sequences. A key challenge of this task is to find the regulatory elements that control gene expressions. Here, we introduce Seq2Exp, a Sequence to Expression network explicitly designed to discover and extract regulatory elements that... | Degui Zhi, Haiyang Yu, Shuiwang Ji, Xingyu Su |  |
| 16 |  |  [Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models](https://openreview.net/forum?id=tyEyYT267x) |  | 0 | Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models... | Aaron Gokaslan, Jiaqi Han, Justin T. Chiu, Marianne Arriola, Subham Sekhar Sahoo, Volodymyr Kuleshov, Zhihan Yang, Zhixuan Qi |  |
| 17 |  |  [Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo](https://openreview.net/forum?id=xoXn62FzD0) |  | 0 | A wide range of LM applications require generating text that conforms to syntactic or semantic constraints. Imposing such constraints can be naturally framed as _probabilistic conditioning_, but exact generation from the resulting distribution—which can differ substantially from the LM’s base... | Alexander K. Lew, Ben Lipkin, Benjamin LeBrun, Clemente Pasti, Gabriel Grand, Jason Eisner, João Loula, Li Du, Marjorie Freedman, Ryan Cotterell, Tianyu Liu, Tim Vieira, Timothy J. O'Donnell, Vikash Mansinghka, Yahya Emara |  |
| 18 |  |  [Scaling Laws for Precision](https://openreview.net/forum?id=wg1PCg3CUP) |  | 0 | Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. In this work, we devise "precision-aware" scaling laws for both training and inference. We propose that training in lower precision reduces the model's... | Aditi Raghunathan, Benjamin Frederick Spector, Blake Bordelon, Cengiz Pehlevan, Christopher Ré, Mansheej Paul, Niklas Muennighoff, Tanishq Kumar, Zachary Ankner |  |
| 19 |  |  [Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance](https://openreview.net/forum?id=SPS6HzVzyt) |  | 0 | Large Language Model's are instruction-finetuned to enhance their ability to follow user instructions and better comprehend input context. Still, they often struggle to follow the input context, especially when it contradicts model's parametric knowledge. This manifests as various failures, such as... | Aditi Raghunathan, Christina Baek, J. Zico Kolter, Sachin Goyal |  |
| 20 |  |  [Inference Scaling for Long-Context Retrieval Augmented Generation](https://openreview.net/forum?id=FSjIrOm1vz) |  | 0 | The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such... | Aijun Bai, Dong Wang, Hansi Zeng, Honglei Zhuang, Kai Hui, Michael Bendersky, Rolf Jagerman, Xuanhui Wang, Zhen Qin, Zhenrui Yue |  |
| 21 |  |  [Scaling LLM Test-Time Compute Optimally Can be More Effective than Scaling Parameters for Reasoning](https://openreview.net/forum?id=4FWAwZtd2n) |  | 0 | Enabling LLMs to improve their outputs by using more test-time compute is a critical step towards building self-improving agents that can operate on open-ended natural language. In this paper, we scale up inference-time computation in LLMs, with a focus on answering: if an LLM is allowed to use a... | Aviral Kumar, Charlie Victor Snell, Jaehoon Lee, Kelvin Xu |  |
| 22 |  |  [Capturing the Temporal Dependence of Training Data Influence](https://openreview.net/forum?id=uHLgDEgiS5) |  | 0 | Traditional data influence estimation methods, like influence function, assume that learning algorithms are permutation-invariant with respect to training data. However, modern training paradigms—especially for foundation models using stochastic algorithms and non-convergent, multi-stage... | Dawn Song, James Zou, Jiachen T. Wang, Prateek Mittal, Ruoxi Jia |  |
| 23 |  |  [Self-Improvement in Language Models: The Sharpening Mechanism](https://openreview.net/forum?id=WJaUkwci9o) |  | 0 | Recent work in language modeling has raised the possibility of “self-improvement,” where an LLM evaluates and refines its own generations to achieve higher performance without external feedback. It is impossible for this self-improvement to create information that is not already in the model, so... | Adam Block, Akshay Krishnamurthy, Audrey Huang, Cyril Zhang, Dhruv Rohatgi, Dylan J. Foster, Jordan T. Ash, Max Simchowitz |  |
| 24 |  |  [Data Shapley in One Training Run](https://openreview.net/forum?id=HD6bWcj87Y) |  | 0 | Data Shapley offers a principled framework for attributing the contribution of data within machine learning contexts. However, the traditional notion of Data Shapley requires re-training models on various data subsets, which becomes computationally infeasible for large-scale models. Additionally,... | Dawn Song, Jiachen T. Wang, Prateek Mittal, Ruoxi Jia |  |
| 25 |  |  [Computationally Efficient RL under Linear Bellman Completeness for Deterministic Dynamics](https://openreview.net/forum?id=hyfe5q5TD0) |  | 0 | We study computationally and statistically efficient Reinforcement Learning algorithms for the \*linear Bellman Complete\* setting. This setting uses linear function approximation to capture value functions and unifies existing models like linear Markov Decision Processes (MDP) and Linear Quadratic... | Akshay Krishnamurthy, Ayush Sekhari, Runzhe Wu, Wen Sun |  |
| 26 |  |  [Linear Representations of Political Perspective Emerge in Large Language Models](https://openreview.net/forum?id=rwqShzb9li) |  | 0 | Large language models (LLMs) have demonstrated the ability to generate text that realistically reflects a range of different subjective human perspectives. This paper studies how LLMs are seemingly able to reflect more liberal versus more conservative viewpoints among other political perspectives... | Aaron Schein, James Evans, Junsol Kim |  |
| 27 |  |  [Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs](https://openreview.net/forum?id=FBkpCyujtS) |  | 0 | Large Language Models (LLMs) generate text by sampling the next token from a probability distribution over the vocabulary at each decoding step. Popular sampling methods like top-p (nucleus sampling) often struggle to balance quality and diversity, especially at higher temperatures which lead to... | Allen G. Roush, Andreas Kirsch, Andrew Baker, Clement Neo, Nguyen Nhat Minh, Ravid ShwartzZiv |  |
| 28 |  |  [Joint Graph Rewiring and Feature Denoising via Spectral Resonance](https://openreview.net/forum?id=zBbZ2vdLzH) |  | 0 | When learning from graph data, the graph and the node features both give noisy information about the node labels. In this paper we propose an algorithm to \*\*j\*\*ointly \*\*d\*\*enoise the features and \*\*r\*\*ewire the graph (JDR), which improves the performance of downstream node... | Cheng Shi, Ivan Dokmanic, Jonas Linkerhägner |  |
| 29 |  |  [Iterative Nash Policy Optimization: Aligning LLMs with General Preferences via No-Regret Learning](https://openreview.net/forum?id=Pujt3ADZgI) |  | 0 | Reinforcement Learning with Human Feedback (RLHF) has achieved great success in aligning large language models (LLMs) with human preferences. Prevalent RLHF approaches are reward-based, following the Bradley-Terry (BT) model assumption, which may not fully capture the complexity of human... | Baolin Peng, Dian Yu, Dong Yu, Haitao Mi, Linfeng Song, Mingyue Huo, Nan Jiang, Ye Tian, Yuheng Zhang |  |
| 30 |  |  [Progressive Compression with Universally Quantized Diffusion Models](https://openreview.net/forum?id=CxXGvKRDnL) |  | 0 | Diffusion probabilistic models have achieved mainstream success in many generative modeling tasks, from image generation to inverse problem solving. A distinct feature of these models is that they correspond to deep hierarchical latent variable models optimizing a variational evidence lower bound... | Justus C. Will, Stephan Mandt, Yibo Yang |  |
| 31 |  |  [Accelerated training through iterative gradient propagation along the residual path](https://openreview.net/forum?id=JDm7oIcx4Y) |  | 0 | Despite being the cornerstone of deep learning, backpropagation is criticized for its inherent sequentiality, which can limit the scalability of very deep models. Such models faced convergence issues due to vanishing gradient, later resolved using residual connections. Variants of these are now... | Alexandre Allauzen, Blaise Delattre, Erwan Fagnou, Paul Caillon |  |
| 32 |  |  [Tight Lower Bounds under Asymmetric High-Order Hölder Smoothness and Uniform Convexity](https://openreview.net/forum?id=fMTPkDEhLQ) |  | 0 | In this paper, we provide tight lower bounds for the oracle complexity of minimizing high-order Hölder smooth and uniformly convex functions. Specifically, for a function whose $p^{th}$-order derivatives are Hölder continuous with degree $\nu$ and parameter $H$, and that is uniformly convex with... | Brian Bullins, Site Bai |  |
| 33 |  |  [ShEPhERD: Diffusing shape, electrostatics, and pharmacophores for bioisosteric drug design](https://openreview.net/forum?id=KSLkFYHlYg) |  | 0 | Engineering molecules to exhibit precise 3D intermolecular interactions with their environment forms the basis of chemical design. In ligand-based drug design, bioisosteric analogues of known bioactive hits are often identified by virtually screening chemical libraries with shape, electrostatic,... | Connor W. Coley, Jenna C. Fromer, Keir Adams, Kento Abeywardane |  |
| 34 |  |  [Restructuring Vector Quantization with the Rotation Trick](https://openreview.net/forum?id=GMwRl2e9Y1) |  | 0 | Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress a continuous input to a discrete latent space and reconstruct it with minimal distortion. They operate by maintaining a set of vectors---often referred to as the codebook---and quantizing each encoder output to the nearest... | Aniketh Iyengar, Christopher Fifty, Christopher Ré, Dennis Duan, Ehsan Amid, Jerry Weihong Liu, Ronald Guenther Junkins, Sebastian Thrun |  |
| 35 |  |  [Interpreting Emergent Planning in Model-Free Reinforcement Learning](https://openreview.net/forum?id=DzGe40glxs) |  | 0 | We present the first mechanistic evidence that model-free reinforcement learning agents can learn to plan. This is achieved by applying a methodology based on concept-based interpretability to a model-free agent in Sokoban -- a commonly used benchmark for studying planning. Specifically, we... | Adrià GarrigaAlonso, David Krueger, Stephen Chung, Thomas Bush, Usman Anwar |  |
| 36 |  |  [Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization](https://openreview.net/forum?id=kX8h23UG6v) |  | 0 | A long-standing belief holds that Bayesian Optimization (BO) with standard Gaussian processes (GP) --- referred to as standard BO --- underperforms in high-dimensional optimization problems. While this belief seems plausible, it lacks both robust empirical evidence and theoretical justification. To... | Haitao Wang, Jeff M. Phillips, Shandian Zhe, Zhitong Xu |  |
| 37 |  |  [Limits to scalable evaluation at the frontier: LLM as judge won't beat twice the data](https://openreview.net/forum?id=NO6Tv6QcDs) |  | 0 | High quality annotations are increasingly a bottleneck in the explosively growing machine learning ecosystem. Scalable evaluation methods that avoid costly annotation have therefore become an important research ambition. Many hope to use strong existing models in lieu of costly labels to provide... | Florian E. Dorner, Moritz Hardt, Vivian Yvonne Nastl |  |
| 38 |  |  [DEPT: Decoupled Embeddings for Pre-training Language Models](https://openreview.net/forum?id=vf5aUZT0Fz) |  | 0 | Language Model pre-training uses broad data mixtures to enhance performance across domains and languages. However, training on such heterogeneous text corpora requires extensive and expensive efforts. Since these data sources vary significantly in lexical, syntactic, and semantic aspects, they... | Alex Iacob, Dongqi Cai, Lorenzo Sani, Meghdad Kurmanji, Nicholas Donald Lane, William F. Shen, Xinchi Qiu, Yan Gao |  |
| 39 |  |  [Homomorphism Expressivity of Spectral Invariant Graph Neural Networks](https://openreview.net/forum?id=rdv6yeMFpn) |  | 0 | Graph spectra are an important class of structural features on graphs that have shown promising results in enhancing Graph Neural Networks (GNNs). Despite their widespread practical use, the theoretical understanding of the power of spectral invariants --- particularly their contribution to GNNs... | Bohang Zhang, Haggai Maron, Jingchu Gai, Liwei Wang, Yiheng Du |  |
| 40 |  |  [RB-Modulation: Training-Free Stylization using Reference-Based Modulation](https://openreview.net/forum?id=bnINPG5A32) |  | 0 | We propose Reference-Based Modulation (RB-Modulation), a new plug-and-play solution for training-free personalization of diffusion models. Existing training-free approaches exhibit difficulties in (a) style extraction from reference images in the absence of additional style or content text... | Abhishek Kumar, Constantine Caramanis, Litu Rout, Nataniel Ruiz, Sanjay Shakkottai, WenSheng Chu, Yujia Chen |  |
| 41 |  |  [Kinetix: Investigating the Training of General Agents through Open-Ended Physics-Based Control Tasks](https://openreview.net/forum?id=zCxGCdzreM) |  | 0 | While large models trained with self-supervised learning on offline datasets have shown remarkable capabilities in text and image domains, achieving the same generalisation for agents that act in sequential decision problems remains an open challenge. In this work, we take a step towards this goal... | Chris Lu, Jakob Nicolaus Foerster, Michael Beukman, Michael T. Matthews |  |
| 42 |  |  [OptionZero: Planning with Learned Options](https://openreview.net/forum?id=3IFRygQKGL) |  | 0 | Planning with options -- a sequence of primitive actions -- has been shown effective in reinforcement learning within complex environments. Previous studies have focused on planning with predefined options or learned options through expert demonstration data. Inspired by MuZero, which learns... | Hung Guei, PeiChiun Peng, PoWei Huang, TiRong Wu |  |
| 43 |  |  [Instant Policy: In-Context Imitation Learning via Graph Diffusion](https://openreview.net/forum?id=je3GZissZc) |  | 0 | Following the impressive capabilities of in-context learning with large transformers, In-Context Imitation Learning (ICIL) is a promising opportunity for robotics. We introduce Instant Policy, which learns new tasks instantly from just one or two demonstrations, achieving ICIL through two key... | Edward Johns, Vitalis Vosylius |  |
| 44 |  |  [What should a neuron aim for? Designing local objective functions based on information theory](https://openreview.net/forum?id=CLE09ESvul) |  | 0 | In modern deep neural networks, the learning dynamics of individual neurons are often obscure, as the networks are trained via global optimization. Conversely, biological systems build on self-organized, local learning, achieving robustness and efficiency with limited global information. Here, we... | Abdullah Makkeh, Alexander S. Ecker, Andreas Christian Schneider, David Alexander Ehrlich, Michael Wibral, Valentin Neuhaus, Viola Priesemann |  |
| 45 |  |  [Cross-Entropy Is All You Need To Invert the Data Generating Process](https://openreview.net/forum?id=hrqNOxpItr) |  | 0 | Supervised learning has become a cornerstone of modern machine learning, yet a comprehensive theory explaining its effectiveness remains elusive. Empirical phenomena, such as neural analogy-making and the linear representation hypothesis, suggest that supervised models can learn interpretable... | Alice Bizeul, Attila Juhos, David A. Klindt, Julia E. Vogt, Patrik Reizinger, Randall Balestriero, Wieland Brendel |  |
| 46 |  |  [Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues](https://openreview.net/forum?id=UvTo3tVBk2) |  | 0 | Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and DeltaNet have emerged as efficient alternatives to Transformers for long sequences. However, both Transformers and LRNNs struggle to perform state-tracking, which may impair performance in tasks such as code evaluation.... | Arber Zela, Frank Hutter, Julien Siems, Jörg K. H. Franke, Massimiliano Pontil, Riccardo Grazzi |  |
| 47 |  |  [Attention as a Hypernetwork](https://openreview.net/forum?id=V4K9h1qNxE) |  | 0 | Transformers can under some circumstances generalize to novel problem instances whose constituent parts might have been encountered during training, but whose compositions have not. What mechanisms underlie this ability for compositional generalization? By reformulating multi-head attention as a... | João Sacramento, Razvan Pascanu, Seijin Kobayashi, Simon Schug, Yassir Akram |  |
| 48 |  |  [Transformers Provably Solve Parity Efficiently with Chain of Thought](https://openreview.net/forum?id=n2NidsYDop) |  | 0 | This work provides the first theoretical analysis of training transformers to solve complex problems by recursively generating intermediate states, analogous to fine-tuning for chain-of-thought (CoT) reasoning. We consider training a one-layer transformer to solve the fundamental $k$-parity... | Juno Kim, Taiji Suzuki |  |
| 49 |  |  [Oscillatory State-Space Models](https://openreview.net/forum?id=GRMfXcAAFh) |  | 0 | We propose Linear Oscillatory State-Space models (LinOSS) for efficiently learning on long sequences. Inspired by cortical dynamics of biological neural networks, we base our proposed LinOSS model on a system of forced harmonic oscillators. A stable discretization, integrated over time using fast... | Daniela Rus, T. Konstantin Rusch |  |
| 50 |  |  [Latent Bayesian Optimization via Autoregressive Normalizing Flows](https://openreview.net/forum?id=ZCOwwRAaEl) |  | 0 | Bayesian Optimization (BO) has been recognized for its effectiveness in optimizing expensive and complex objective functions. Recent advancements in Latent Bayesian Optimization (LBO) have shown promise by integrating generative models such as variational autoencoders (VAEs) to manage the... | Hyunwoo J. Kim, Jaewon Chu, Jinyoung Park, Minseo Yoon, Seunghun Lee |  |
| 51 |  |  [Energy-based Backdoor Defense Against Federated Graph Learning](https://openreview.net/forum?id=5Jc7r5aqHJ) |  | 0 | Federated Graph Learning is rapidly evolving as a privacy-preserving collaborative approach. However, backdoor attacks are increasingly undermining federated systems by injecting carefully designed triggers that lead to the model making incorrect predictions. Trigger structures and injection... | Dacheng Tao, Guancheng Wan, Guibin Zhang, Mang Ye, Wenke Huang, Zitong Shi |  |
| 52 |  |  [Reasoning Elicitation in Language Models via Counterfactual Feedback](https://openreview.net/forum?id=VVixJ9QavY) |  | 0 | Despite the increasing effectiveness of language models, their reasoning capabilities remain underdeveloped. In particular, causal reasoning through counterfactual question answering is lacking. This work aims to bridge this gap. We first derive novel metrics that balance accuracy in factual and... | Aditya V. Nori, Alihan Hüyük, Jacqueline R. M. A. Maasch, Javier González, Xinnuo Xu |  |
| 53 |  |  [CAX: Cellular Automata Accelerated in JAX](https://openreview.net/forum?id=o2Igqm95SJ) |  | 0 | Cellular automata have become a cornerstone for investigating emergence and self-organization across diverse scientific disciplines. However, the absence of a hardware-accelerated cellular automata library limits the exploration of new research directions, hinders collaboration, and impedes... | Antoine Cully, Maxence Faldor |  |
| 54 |  |  [Proteina: Scaling Flow-based Protein Structure Generative Models](https://openreview.net/forum?id=TVQLu34bdw) |  | 0 | Recently, diffusion- and flow-based generative models of protein structures have emerged as a powerful tool for de novo protein design. Here, we develop \*Proteina\*, a new large-scale flow-based protein backbone generator that utilizes hierarchical fold class labels for conditioning and relies on... | Arash Vahdat, Christian Dallago, Danny Reidenbach, Emine Küçükbenli, Jason Yim, Karsten Kreis, Kieran Didi, Mario Geiger, Tomas Geffner, Zhonglin Cao, Zuobai Zhang |  |
| 55 |  |  [Residual Deep Gaussian Processes on Manifolds](https://openreview.net/forum?id=JWtrk7mprJ) |  | 0 | We propose practical deep Gaussian process models on Riemannian manifolds, similar in spirit to residual neural networks. With manifold-to-manifold hidden layers and an arbitrary last layer, they can model manifold- and scalar-valued functions, as well as vector fields. We target data inherently... | Andreas Krause, Kacper Wyrwal, Viacheslav Borovitskiy |  |
| 56 |  |  [Learning to Discretize Denoising Diffusion ODEs](https://openreview.net/forum?id=xDrFWUmCne) |  | 0 | Diffusion Probabilistic Models (DPMs) are generative models showing competitive performance in various domains, including image synthesis and 3D point cloud generation. Sampling from pre-trained DPMs involves multiple neural function evaluations (NFEs) to transform Gaussian noise samples into... | Anji Liu, DungTrung Hoang, Guy Van den Broeck, Mathias Niepert, Vinh Tong |  |
| 57 |  |  [Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment](https://openreview.net/forum?id=kGvXIlIVLM) |  | 0 | Classifier-Free Guidance (CFG) is a critical technique for enhancing the sample quality of visual generative models. However, in autoregressive (AR) multi-modal generation, CFG introduces design inconsistencies between language and visual content, contradicting the design philosophy of unifying... | Hang Su, Huayu Chen, Jun Zhu, Peize Sun |  |
| 58 |  |  [TimeMixer++: A General Time Series Pattern Machine for Universal Predictive Analysis](https://openreview.net/forum?id=1CLzLXSFNn) |  | 0 | Time series analysis plays a critical role in numerous applications, supporting tasks such as forecasting, classification, anomaly detection, and imputation. In this work, we present the time series pattern machine (TSPM), a model designed to excel in a broad range of time series tasks through... | Baichuan Mo, Jiawei Li, Ming Jin, Shengtong Ju, Shiyu Wang, Wenze Lin, Xiaoming Shi, Zhixuan Chu, Zhou Ye |  |
| 59 |  |  [RMP-SAM: Towards Real-Time Multi-Purpose Segment Anything](https://openreview.net/forum?id=1pXzC30ry5) |  | 0 | Recent segmentation methods, which adopt large-scale data training and transformer architecture, aim to create one foundation model that can perform multiple tasks. However, most of these methods rely on heavy encoder and decoder frameworks, hindering their performance in real-time scenarios. To... | Bernard Ghanem, Haobo Yuan, Jingbo Wang, Kai Chen, Lu Qi, MingHsuan Yang, Qingyu Shi, Shilin Xu, Xiangtai Li, Yibo Yang, Yining Li, Yunhai Tong |  |
| 60 |  |  [Steering Protein Family Design through Profile Bayesian Flow](https://openreview.net/forum?id=PSiijdQjNU) |  | 0 | Protein family design emerges as a promising alternative by combining the advantages of de novo protein design and mutation-based directed evolution.In this paper, we propose ProfileBFN, the Profile Bayesian Flow Networks, for specifically generative modeling of protein families. ProfileBFN extends... | Hao Zhou, Jingjing Gong, Shuyi Zhang, Siyu Long, WeiYing Ma, Wenhao Huang, Yu Pei, Yuxuan Song, Zhe Zhang, Ziyao Cao |  |
| 61 |  |  [GeSubNet: Gene Interaction Inference for Disease Subtype Network Generation](https://openreview.net/forum?id=ja4rpheN2n) |  | 0 | Retrieving gene functional networks from knowledge databases presents a challenge due to the mismatch between disease networks and subtype-specific variations. Current solutions, including statistical and deep learning methods, often fail to effectively integrate gene interaction knowledge from... | Jimeng Sun, Peng Chen, Rikuto Kotoge, Xin Liu, Yasuko Matsubara, Yasushi Sakurai, Zheng Chen, Ziwei Yang |  |
| 62 |  |  [Exploring The Loss Landscape Of Regularized Neural Networks Via Convex Duality](https://openreview.net/forum?id=4xWQS2z77v) |  | 0 | We discuss several aspects of the loss landscape of regularized neural networks: the structure of stationary points, connectivity of optimal solutions, path with non-increasing loss to arbitrary global optimum, and the nonuniqueness of optimal solutions, by casting the problem into an equivalent... | Aaron Mishkin, Mert Pilanci, Sungyoon Kim |  |
| 63 |  |  [Global Convergence in Neural ODEs: Impact of Activation Functions](https://openreview.net/forum?id=AoraWUmpLU) |  | 0 | Neural Ordinary Differential Equations (ODEs) have been successful in various applications due to their continuous nature and parameter-sharing efficiency. However, these unique characteristics also introduce challenges in training, particularly with respect to gradient computation accuracy and... | Hailiang Liu, Hongyang Gao, Siyuan Sun, Tianxiang Gao |  |
| 64 |  |  [MoDeGPT: Modular Decomposition for Large Language Model Compression](https://openreview.net/forum?id=8EfxjTCg2k) |  | 0 | Large Language Models (LLMs) have significantly advanced AI with their exceptional performance across a wide range of tasks. However, their extensive computational requirements restrict their use on devices with limited resources. While recent compression methods based on low-rank matrices show... | Abhishek Patel, ChiHeng Lin, Hongxia Jin, James Seale Smith, Shangqian Gao, Shikhar Tuli, YenChang Hsu, Yilin Shen |  |
| 65 |  |  [MIND over Body: Adaptive Thinking using Dynamic Computation](https://openreview.net/forum?id=EjJGND0m1x) |  | 0 | While the human brain efficiently handles various computations with a limited number of neurons, traditional deep learning networks require a significant increase in parameters to improve performance. Yet, these parameters are used inefficiently as the networks employ the same amount of computation... | Barak A. Pearlmutter, Mrinal Mathur, Sergey M. Plis |  |
| 66 |  |  [From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions](https://openreview.net/forum?id=QKBu1BOAwd) |  | 0 | Tool learning enables Large Language Models (LLMs) to interact with external environments by invoking tools, serving as an effective strategy to mitigate the limitations inherent in their pre-training data. In this process, tool documentation plays a crucial role by providing usage instructions for... | Changle Qu, Dawei Yin, Hengyi Cai, JiRong Wen, Jun Xu, Shuaiqiang Wang, Sunhao Dai, Xiaochi Wei |  |
| 67 |  |  [LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization](https://openreview.net/forum?id=VpWki1v2P8) |  | 0 | Low-rank adaption (LoRA) is a widely used parameter-efficient finetuning method for LLM that reduces memory requirements. However, current LoRA optimizers lack transformation invariance, meaning the updates depending on how the two LoRA factors are scaled or rotated. This deficiency leads to... | ChoJui Hsieh, Felix X. Yu, Inderjit S. Dhillon, JuiNan Yen, Sai Surya Duvvuri, Sanjiv Kumar, Si Si, Zhao Meng |  |
| 68 |  |  [Scaling and evaluating sparse autoencoders](https://openreview.net/forum?id=tcsZt9ZNKD) |  | 0 | Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features.... | Alec Radford, Gabriel Goh, Henk Tillman, Ilya Sutskever, Jan Leike, Jeffrey Wu, Leo Gao, Rajan Troll, Tom Dupré la Tour |  |
| 69 |  |  [ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement](https://openreview.net/forum?id=YUYJsHOf3c) |  | 0 | Post-training Large Language Models (LLMs) with explicit reasoning trajectories can enhance their reasoning abilities. However, acquiring such high-quality trajectory data typically demands meticulous supervision from humans or superior models, which can be either expensive or license-constrained.... | Caiming Xiong, Chen Xing, ChienSheng Wu, Congying Xia, Xiangyu Peng, Xinyi Yang |  |
| 70 |  |  [Feedback Favors the Generalization of Neural ODEs](https://openreview.net/forum?id=cmfyMV45XO) |  | 0 | The well-known generalization problem hinders the application of artificial neural networks in continuous-time prediction tasks with varying latent dynamics. In sharp contrast, biological systems can neatly adapt to evolving environments benefiting from real-time feedback mechanisms. Inspired by... | Jianfei Yang, Jindou Jia, Kexin Guo, Lei Guo, Meng Wang, Xiang Yu, Zihan Yang |  |
| 71 |  |  [Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs](https://openreview.net/forum?id=QWunLKbBGF) |  | 0 | Large Language Models (LLMs) are increasingly deployed as chatbots, yet their ability to personalize responses to user preferences remains limited. We introduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize and adhere to user preferences in long-context conversational... | Devamanyu Hazarika, Kaixiang Lin, Mingyi Hong, Siyan Zhao, Yang Liu |  |
| 72 |  |  [STAR: Synthesis of Tailored Architectures](https://openreview.net/forum?id=HsHxSN23rM) |  | 0 | Iterative improvement of model architectures is fundamental to deep learning: Transformers first enabled scaling, and recent advances in model hybridization have pushed the quality-efficiency frontier. However, optimizing architectures remains challenging and expensive, with a variety of automated... | Alexander Amini, Armin W. Thomas, Michael Poli, Rom N. Parnichkun, Stefano Massaroli |  |
| 73 |  |  [Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models](https://openreview.net/forum?id=WCRQFlji2q) |  | 0 | Hallucinations in large language models are a widespread problem, yet the mechanisms behind whether models will hallucinate are poorly understood, limiting our ability to solve this problem. Using sparse autoencoders as an interpretability tool, we discover that a key part of these mechanisms is... | Javier Ferrando, Neel Nanda, Oscar Balcells Obeso, Senthooran Rajamanoharan |  |
| 74 |  |  [Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces](https://openreview.net/forum?id=AP0ndQloqR) |  | 0 | Advances in reinforcement learning (RL) have led to its successful application in complex tasks with continuous state and action spaces. Despite these advances in practice, most theoretical work pertains to finite state and action spaces. We propose building a theoretical understanding of... | George Konidaris, Omer Gottesman, Saket Tiwari |  |
| 75 |  |  [When is Task Vector Provably Effective for Model Editing? A Generalization Analysis of Nonlinear Transformers](https://openreview.net/forum?id=vRvVVb0NAz) |  | 0 | Task arithmetic refers to editing the pre-trained model by adding a weighted sum of task vectors, each of which is the weight update from the pre-trained model to fine-tuned models for certain tasks. This approach recently gained attention as a computationally efficient inference method for model... | Hongkang Li, Meng Wang, PinYu Chen, Shuai Zhang, Sijia Liu, Yihua Zhang |  |
| 76 |  |  [Learning and aligning single-neuron invariance manifolds in visual cortex](https://openreview.net/forum?id=kbjJ9ZOakb) |  | 0 | Understanding how sensory neurons exhibit selectivity to certain features and invariance to others is central to uncovering the computational principles underlying robustness and generalization in visual perception. Most existing methods for characterizing selectivity and invariance identify single... | Fabian H. Sinz, Ján Antolík, Luca Baroni, Mohammad Bashiri |  |
| 77 |  |  [Feedback Schrödinger Bridge Matching](https://openreview.net/forum?id=k3tbMMW8rH) |  | 0 | Recent advancements in diffusion bridges for distribution transport problems have heavily relied on matching frameworks, yet existing methods often face a trade-off between scalability and access to optimal pairings during training. Fully unsupervised methods make minimal assumptions but incur high... | Evangelos A. Theodorou, GuanHorng Liu, Nikolaos Komianos, Panagiotis Theodoropoulos, Vincent Pacelli |  |
| 78 |  |  [TetSphere Splatting: Representing High-Quality Geometry with Lagrangian Volumetric Meshes](https://openreview.net/forum?id=8enWnd6Gp3) |  | 0 | We introduce TetSphere Splatting, a Lagrangian geometry representation designed for high-quality 3D shape modeling. TetSphere splatting leverages an underused yet powerful geometric primitive -- volumetric tetrahedral meshes. It represents 3D shapes by deforming a collection of tetrahedral spheres,... | Bohan Wang, Kaiming He, Minghao Guo, Wojciech Matusik |  |
| 79 |  |  [Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents](https://openreview.net/forum?id=kxnoqaisCT) |  | 0 | Multimodal large language models (MLLMs) are transforming the capabilities of graphical user interface (GUI) agents, facilitating their transition from controlled simulations to complex, real-world applications across various platforms. However, the effectiveness of these agents hinges on the... | Boyu Gou, Boyuan Zheng, Cheng Chang, Huan Sun, Ruohan Wang, Yanan Xie, Yiheng Shu, Yu Su |  |
| 80 |  |  [Progressive distillation induces an implicit curriculum](https://openreview.net/forum?id=wPMRwmytZe) |  | 0 | Knowledge distillation leverages a teacher model to improve the training of a student model. A persistent challenge is that a better teacher does not always yield a better student, to which a common mitigation is to use additional supervision from several “intermediate” teachers. One empirically... | Abhishek Panigrahi, Andrej Risteski, Bingbin Liu, Sadhika Malladi, Surbhi Goel |  |
| 81 |  |  [Rethinking Reward Modeling in Preference-based Large Language Model Alignment](https://openreview.net/forum?id=rfdblE10qm) |  | 0 | The Bradley-Terry (BT) model is a common and successful practice in reward modeling for Large Language Model (LLM) alignment. However, it remains unclear \*why\* this model --- originally developed for multi-player stochastic game matching --- can be adopted to convert pairwise response comparisons... | Hao Sun, JeanFrancois Ton, Yunyi Shen |  |
| 82 |  |  [Copyright-Protected Language Generation via Adaptive Model Fusion](https://openreview.net/forum?id=kRoWeLTpL4) |  | 0 | The risk of language models reproducing copyrighted material from their training data has led to the development of various protective measures. Among these, inference-time strategies that impose constraints via post-processing have shown promise in addressing the complexities of copyright... | Fanny Yang, Francesco Pinto, Javier Abad, Konstantin Donhauser |  |
| 83 |  |  [Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse Controls to Any Diffusion Model](https://openreview.net/forum?id=ny8T8OuNHe) |  | 0 | ControlNets are widely used for adding spatial control to text-to-image diffusion models. However, when it comes to controllable video generation, ControlNets cannot be directly integrated into new backbones due to feature space mismatches, and training ControlNets for new backbones can be a... | Abhay Zala, Han Lin, Jaemin Cho, Mohit Bansal |  |
| 84 |  |  [BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models](https://openreview.net/forum?id=fAAaT826Vv) |  | 0 | Predictive models often need to work with incomplete information in real-world tasks. Consequently, they must provide reliable probability or confidence estimation, especially in large-scale decision-making and planning tasks. Current large language models (LLMs) are insufficient for accurate... | Ben Zhou, Dan Roth, Weidong Lin, Yu Feng |  |
| 85 |  |  [LaMPlace: Learning to Optimize Cross-Stage Metrics in Macro Placement](https://openreview.net/forum?id=YLIsIzC74j) |  | 0 | Machine learning techniques have shown great potential in enhancing macro placement, a critical stage in modern chip design. However, existing methods primarily focus on \*online\* optimization of \*intermediate surrogate metrics\* that are available at the current placement stage, rather than... | Feng Wu, Jianye Hao, Jie Wang, Mingxuan Yuan, Shixiong Kai, Siyuan Xu, Zhentao Tang, Zijie Geng, Ziyan Liu |  |
| 86 |  |  [miniCTX: Neural Theorem Proving with (Long-)Contexts](https://openreview.net/forum?id=KIgaAqEFHW) |  | 0 | Real-world formal theorem proving often depends on a wealth of context, including definitions, lemmas, comments, file structure, and other information. We introduce $\texttt{miniCTX}$, which tests a model's ability to prove formal mathematical theorems that depend on new context that is not seen... | Jiewen Hu, Sean Welleck, Thomas Zhu |  |
| 87 |  |  [BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions](https://openreview.net/forum?id=YrycTjllL0) |  | 0 | Task automation has been greatly empowered by the recent advances in Large Language Models (LLMs) via Python code, where the tasks range from software engineering development to general-purpose reasoning. While current benchmarks have shown that LLMs can solve tasks using programs like human... | Armel Randy Zebaze, Chen Gong, Han Hu, Haolan Zhan, Imam Nur Bani Yusuf, Indraneil Paul, James Hoang, Jean Kaddour, Jenny Chim, Junda He, Ming Xu, Minh Chien Vu, Prateek Yadav, Ratnadira Widyasari, Simon Brunner, Terry Yue Zhuo, WenDing Li, Wenhao Yu, Xiaoheng Hong, Zhihan Zhang, et al. |  |
| 88 |  |  [Towards a Complete Logical Framework for GNN Expressiveness](https://openreview.net/forum?id=pqOjj90Vwp) |  | 0 | Designing expressive Graph neural networks (GNNs) is an important topic in graph machine learning fields. Traditionally, the Weisfeiler-Lehman (WL) test has been the primary measure for evaluating GNN expressiveness. However, high-order WL tests can be obscure, making it challenging to discern the... | Tuo Xu |  |
| 89 |  |  [Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think](https://openreview.net/forum?id=DJSZGGZYVi) |  | 0 | Recent studies have shown that the denoising process in (generative) diffusion models can induce meaningful (discriminative) representations inside the model, though the quality of these representations still lags behind those learned through recent self-supervised learning methods. We argue that... | Huiwon Jang, Jinwoo Shin, Jonathan Huang, Jongheon Jeong, Saining Xie, Sangkyung Kwak, Sihyun Yu |  |
| 90 |  |  [Classic but Everlasting: Traditional Gradient-Based Algorithms Converge Fast Even in Time-Varying Multi-Player Games](https://openreview.net/forum?id=t8FG4cJuL3) |  | 0 | Last-iterate convergence behaviours of well-known algorithms are intensively investigated in various games, such as two-player bilinear zero-sum games. However, most known last-iterate convergence properties rely on strict settings where the underlying games must have time-invariant payoffs.... | Jun Yu, Yanzheng Chen |  |
| 91 |  |  [DSPO: Direct Score Preference Optimization for Diffusion Model Alignment](https://openreview.net/forum?id=xyfb9HHvMe) |  | 0 | Diffusion-based Text-to-Image (T2I) models have achieved impressive success in generating high-quality images from textual prompts. While large language models (LLMs) effectively leverage Direct Preference Optimization (DPO) for fine-tuning on human preference data without the need for reward... | Huaisheng Zhu, Teng Xiao, Vasant G. Honavar |  |
| 92 |  |  [TANGO: Co-Speech Gesture Video Reenactment with Hierarchical Audio Motion Embedding and Diffusion Interpolation](https://openreview.net/forum?id=LbEWwJOufy) |  | 0 | We present TANGO, a framework for generating co-speech body-gesture videos. Given a few-minute, single-speaker reference video and target speech audio, TANGO produces high-fidelity videos with synchronized body gestures. TANGO builds on Gesture Video Reenactment (GVR), which splits and retrieves... | Haiyang Liu, Qiaoge Li, Shigeru Kuriyama, Takafumi Taketomi, Tomoya Akiyama, Xingchao Yang, Yuantian Huang |  |
| 93 |  |  [Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation](https://openreview.net/forum?id=meRCKuUpmc) |  | 0 | Current efforts to learn scalable policies in robotic manipulation primarily fall into two categories: one focuses on "action," which involves behavior cloning from extensive collections of robotic data, while the other emphasizes "vision," enhancing model generalization by pre-training... | Dahua Lin, Hao Dong, Jia Zeng, Jiangmiao Pang, Ping Wang, Sizhe Yang, Yang Tian |  |
| 94 |  |  [The Complexity of Two-Team Polymatrix Games with Independent Adversaries](https://openreview.net/forum?id=9VGTk2NYjF) |  | 0 | Adversarial multiplayer games are an important object of study in multiagent learning. In particular, polymatrix zero-sum games are a multiplayer setting where Nash equilibria are known to be efficiently computable. Towards understanding the limits of tractability in polymatrix games, we study the... | Alexandros Hollender, Gilbert Maystre, Sai Ganesh Nagarajan |  |
| 95 |  |  [MMQA: Evaluating LLMs with Multi-Table Multi-Hop Complex Questions](https://openreview.net/forum?id=GGlpykXDCa) |  | 0 | While large language models (LLMs) have made strides in understanding tabular data, current tabular evaluation benchmarks, such as WikiTableQuestions and WikiSQL, are focus on single-table scenarios, which cannot necessarily reflect the complexity of real-world applications. To bridge this gap, we... | Dongyuan Li, Jian Wu, Linyi Yang, Manabu Okumura, Yue Zhang, Yuliang Ji |  |
| 96 |  |  [On Scaling Up 3D Gaussian Splatting Training](https://openreview.net/forum?id=pQqeQpMkE7) |  | 0 | 3D Gaussian Splatting (3DGS) is increasingly popular for 3D reconstruction due to its superior visual quality and rendering speed. However, 3DGS training currently occurs on a single GPU, limiting its ability to handle high-resolution and large-scale 3D reconstruction tasks due to memory... | Ang Li, Aurojit Panda, Daohan Lu, Haoyang Weng, Hexu Zhao, Jinyang Li, Saining Xie |  |
| 97 |  |  [Emergence of meta-stable clustering in mean-field transformer models](https://openreview.net/forum?id=eBS3dQQ8GV) |  | 0 | We model the evolution of tokens within a deep stack of Transformer layers as a continuous-time flow on the unit sphere, governed by a mean-field interacting particle system, building on the framework introduced in Geshkovski et al. (2023). Studying the corresponding mean-field Partial Differential... | Andrea Agazzi, Federico Pasqualotto, Giuseppe Bruno |  |
| 98 |  |  [Wide Neural Networks Trained with Weight Decay Provably Exhibit Neural Collapse](https://openreview.net/forum?id=1HCN4pjTb4) |  | 0 | Deep neural networks (DNNs) at convergence consistently represent the training data in the last layer via a geometric structure referred to as neural collapse. This empirical evidence has spurred a line of theoretical research aimed at proving the emergence of neural collapse, mostly focusing on... | Arthur Jacot, Marco Mondelli, Peter Súkeník, Zihan Wang |  |
| 99 |  |  [ECD: A Machine Learning Benchmark for Predicting Enhanced-Precision Electronic Charge Density in Crystalline Inorganic Materials](https://openreview.net/forum?id=SBCMNc3Mq3) |  | 0 | Supervised machine learning techniques are increasingly being adopted to speed up electronic structure predictions, serving as alternatives to first-principles methods like Density Functional Theory (DFT). Although current DFT datasets mainly emphasize chemical properties and atomic forces, the... | Fengyang Xu, Hongjin Zhong, Pin Chen, Qing Mo, Yutong Lu, Zexin Xu |  |
| 100 |  |  [On the Benefits of Memory for Modeling Time-Dependent PDEs](https://openreview.net/forum?id=o9kqa5K3tB) |  | 0 | Data-driven techniques have emerged as a promising alternative to traditional numerical methods for solving PDEs. For time-dependent PDEs, many approaches are Markovian---the evolution of the trained system only depends on the current state, and not the past states. In this work, we investigate the... | Albert Gu, Andrej Risteski, Ricardo Buitrago Ruiz, Tanya Marwah |  |
| 101 |  |  [SD-LoRA: Scalable Decoupled Low-Rank Adaptation for Class Incremental Learning](https://openreview.net/forum?id=5U1rlpX68A) |  | 0 | Continual Learning (CL) with foundation models has recently emerged as a promising paradigm to exploit abundant knowledge acquired during pre-training for tackling sequential tasks. However, existing prompt-based and Low-Rank Adaptation-based (LoRA-based) methods often require expanding a... | Deyu Meng, Hanspeter Pfister, Hongming Piao, Kede Ma, LongKai Huang, Renzhen Wang, Wanhua Li, Yichen Wu, Ying Wei |  |
| 102 |  |  [Improving Probabilistic Diffusion Models With Optimal Diagonal Covariance Matching](https://openreview.net/forum?id=fV0t65OBUu) |  | 0 | The probabilistic diffusion model has become highly effective across various domains. Typically, sampling from a diffusion model involves using a denoising distribution characterized by a Gaussian with a learned mean and either fixed or learned covariances. In this paper, we leverage the recently... | Andi Zhang, David Barber, Mingtian Zhang, Tim Z. Xiao, Yingzhen Li, Zijing Ou |  |
| 103 |  |  [PathGen-1.6M: 1.6 Million Pathology Image-text Pairs Generation through Multi-agent Collaboration](https://openreview.net/forum?id=rFpZnn11gj) |  | 0 | Vision Language Models (VLMs) like CLIP have attracted substantial attention in pathology, serving as backbones for applications such as zero-shot image classification and Whole Slide Image (WSI) analysis. Additionally, they can function as vision encoders when combined with large language models... | Chenglu Zhu, Jingxiong Li, Kai Zhang, Lin Yang, Tao Lin, Xinheng Lyu, Xuan Gong, Yixuan Si, Yunlong Zhang, Yuxuan Sun, Zhongyi Shui |  |
| 104 |  |  [Training on the Test Task Confounds Evaluation and Emergence](https://openreview.net/forum?id=jOmk0uS1hl) |  | 0 | We study a fundamental problem in the evaluation of large language models that we call training on the test task. Unlike wrongful practices like training on the test data, leakage, or data contamination, training on the test task is not a malpractice. Rather, the term describes a growing set of... | Florian E. Dorner, Moritz Hardt, Ricardo DominguezOlmedo |  |
| 105 |  |  [Subgraph Federated Learning for Local Generalization](https://openreview.net/forum?id=cH65nS5sOz) |  | 0 | Federated Learning (FL) on graphs enables collaborative model training to enhance performance without compromising the privacy of each client. However, existing methods often overlook the mutable nature of graph data, which frequently introduces new nodes and leads to shifts in label distribution.... | Carl Yang, Chanyoung Park, Junseok Lee, Namkyeong Lee, Sein Kim, Sukwon Yun, Sungwon Kim, Yoonho Lee, Yunhak Oh |  |
| 106 |  |  [A Probabilistic Perspective on Unlearning and Alignment for Large Language Models](https://openreview.net/forum?id=51WraMid8K) |  | 0 | Comprehensive evaluation of Large Language Models (LLMs) is an open research problem. Existing evaluations rely on deterministic point estimates generated via greedy decoding. However, we find that deterministic evaluations fail to capture the whole output distribution of a model, yielding... | Leo Schwinn, Stephan Günnemann, Yan Scholten |  |
| 107 |  |  [MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering](https://openreview.net/forum?id=6s5uXNWGIh) |  | 0 | We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, we curate 75 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models,... | Aleksander Madry, Dane Sherburn, Evan Mays, Giulio Starace, James Aung, Jun Shern Chan, Kevin Liu, Leon Maksin, Lilian Weng, Neil Chowdhury, Oliver Jaffe, Tejal Patwardhan |  |
| 108 |  |  [Learning Randomized Algorithms with Transformers](https://openreview.net/forum?id=UV5p3JZMjC) |  | 0 | Randomization is a powerful tool that endows algorithms with remarkable properties. For instance, randomized algorithms excel in adversarial settings, often surpassing the worst-case performance of deterministic algorithms with large margins. Furthermore, their success probability can be amplified... | Angelika Steger, Johannes von Oswald, Seijin Kobayashi, Yassir Akram |  |
| 109 |  |  [Data Scaling Laws in Imitation Learning for Robotic Manipulation](https://openreview.net/forum?id=pISLZG7ktL) |  | 0 | Data scaling has revolutionized fields like natural language processing and computer vision, providing models with remarkable generalization capabilities. In this paper, we investigate whether similar data scaling laws exist in robotics, particularly in robotic manipulation, and whether appropriate... | Chuan Wen, Fanqi Lin, Jiacheng You, Pingyue Sheng, Yang Gao, Yingdong Hu |  |
| 110 |  |  [Amortized Control of Continuous State Space Feynman-Kac Model for Irregular Time Series](https://openreview.net/forum?id=8zJRon6k5v) |  | 0 | Many real-world datasets, such as healthcare, climate, and economics, are often collected as irregular time series, which poses challenges for accurate modeling. In this paper, we propose the Amortized Control of continuous State Space Model (ACSSM) for continuous dynamical modeling of time series... | Byoungwoo Park, Hyungi Lee, Juho Lee |  |
| 111 |  |  [Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates](https://openreview.net/forum?id=syThiTmWWm) |  | 0 | Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and MT-Bench, have become popular for evaluating language models due to their cost-effectiveness and scalability compared to human evaluation. Achieving high win rates on these benchmarks can significantly boost the promotional... | Chao Du, Jing Jiang, Min Lin, Qian Liu, Tianyu Pang, Xiaosen Zheng |  |
| 112 |  |  [On the Hölder Stability of Multiset and Graph Neural Networks](https://openreview.net/forum?id=P7KIGdgW8S) |  | 0 | Extensive research efforts have been put into characterizing and constructing maximally separating multiset and graph neural networks. However, recent empirical evidence suggests the notion of separation itself doesn't capture several interesting phenomena. On the one hand, the quality of this... | Nadav Dym, Yair Davidson |  |
| 113 |  |  [On Conformal Isometry of Grid Cells: Learning Distance-Preserving Position Embedding](https://openreview.net/forum?id=Xo0Q1N7CGk) |  | 0 | This paper investigates the conformal isometry hypothesis as a potential explanation for the hexagonal periodic patterns in grid cell response maps. We posit that grid cell activities form a high-dimensional vector in neural space, encoding the agent's position in 2D physical space. As the agent... | Dehong Xu, Ruiqi Gao, Wenhao Zhang, XueXin Wei, Ying Nian Wu |  |
| 114 |  |  [Combatting Dimensional Collapse in LLM Pre-Training Data via Submodular File Selection](https://openreview.net/forum?id=f4gF6AIHRy) |  | 0 | Selecting high-quality pre-training data for large language models (LLMs) is crucial for enhancing their overall performance under limited computation budget, improving both training and sample efficiency. Recent advancements in file selection primarily rely on using an existing or trained proxy... | Dacheng Tao, Li Shen, Pingjie Wang, Shengchao Hu, Siyuan Du, Ya Zhang, Yanfeng Wang, Ziqing Fan |  |
| 115 |  |  [Population Transformer: Learning Population-level Representations of Neural Activity](https://openreview.net/forum?id=FVuqJt3c4L) |  | 0 | We present a self-supervised framework that learns population-level codes for arbitrary ensembles of neural recordings at scale. We address key challenges in scaling models with neural time-series data, namely, sparse and variable electrode distribution across subjects and datasets. The Population... | Andrei Barbu, Boris Katz, Christopher Wang, Geeling Chau, Sabera J. Talukder, Saraswati Soedarmadji, Vighnesh Subramaniam, Yisong Yue |  |
| 116 |  |  [KAN: Kolmogorov-Arnold Networks](https://openreview.net/forum?id=Ozo7qJ5vZi) |  | 0 | Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes ("neurons''), KANs have learnable activation functions on edges ("weights''). KANs... | Fabian Ruehle, James Halverson, Marin Soljacic, Max Tegmark, Sachin Vaidya, Thomas Y. Hou, Yixuan Wang, Ziming Liu |  |
| 117 |  |  [Problem-Parameter-Free Federated Learning](https://openreview.net/forum?id=ZuazHmXTns) |  | 0 | Federated learning (FL) has garnered significant attention from academia and industry in recent years due to its advantages in data privacy, scalability, and communication efficiency. However, current FL algorithms face a critical limitation: their performance heavily depends on meticulously tuned... | Kai Zhang, Wenjing Yan, Xiaolu Wang, Xuanyu Cao |  |
| 118 |  |  [SymmetricDiffusers: Learning Discrete Diffusion on Finite Symmetric Groups](https://openreview.net/forum?id=EO8xpnW7aX) |  | 0 | The group of permutations $S_n$, also known as the finite symmetric groups, are essential in fields such as combinatorics, physics, and chemistry. However, learning a probability distribution over $S_n$ poses significant challenges due to its intractable size and discrete nature. In this paper, we... | Donglin Yang, Renjie Liao, Yongxing Zhang |  |
| 119 |  |  [Language Representations Can be What Recommenders Need: Findings and Potentials](https://openreview.net/forum?id=eIJfOIMN9z) |  | 0 | Recent studies empirically indicate that language models (LMs) encode rich world knowledge beyond mere semantics, attracting significant attention across various fields. However, in the recommendation domain, it remains uncertain whether LMs implicitly encode user preference information. Contrary... | An Zhang, Leheng Sheng, TatSeng Chua, Xiang Wang, Yi Zhang, Yuxin Chen |  |
| 120 |  |  [HiRA: Parameter-Efficient Hadamard High-Rank Adaptation for Large Language Models](https://openreview.net/forum?id=TwJrTz9cRS) |  | 0 | We propose Hadamard High-Rank Adaptation (HiRA), a parameter-efficient fine-tuning (PEFT) method that enhances the adaptability of Large Language Models (LLMs). While Low-rank Adaptation (LoRA) is widely used to reduce resource demands, its low-rank updates may limit its expressiveness for new... | Lilian Tang, Qiushi Huang, Tom Ko, Yu Zhang, Zhan Zhuang |  |
| 121 |  |  [A Theoretically-Principled Sparse, Connected, and Rigid Graph Representation of Molecules](https://openreview.net/forum?id=OIvg3MqWX2) |  | 0 | Graph neural networks (GNNs) -- learn graph representations by exploiting the graph's sparsity, connectivity, and symmetries -- have become indispensable for learning geometric data like molecules. However, the most used graphs (e.g., radial cutoff graphs) in molecular modeling lack theoretical... | Bao Wang, Justin M. Baker, Qi Tang, ShihHsin Wang, YuanEn Sun, Yuhao Huang |  |
| 122 |  |  [How much of my dataset did you use? Quantitative Data Usage Inference in Machine Learning](https://openreview.net/forum?id=EUSkm2sVJ6) |  | 0 | How much of my data was used to train a machine learning model? This is a critical question for data owners assessing the risk of unauthorized usage of their data to train models. However, previous work mistakenly treats this as a binary problem—inferring whether all-or-none or any-or-none of the... | Jiayuan Ye, Reza Shokri, Sajjad Zarifzadeh, Yao Tong |  |
| 123 |  |  [LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior](https://openreview.net/forum?id=Wr3UuEx72f) |  | 0 | We present LARP, a novel video tokenizer designed to overcome limitations in current video tokenization methods for autoregressive (AR) generative models. Unlike traditional patchwise tokenizers that directly encode local visual patches into discrete tokens, LARP introduces a holistic tokenization... | Abhinav Shrivastava, Hanyu Wang, Hao Chen, Saksham Suri, Yixuan Ren |  |
| 124 |  |  [MOS: Model Synergy for Test-Time Adaptation on LiDAR-Based 3D Object Detection](https://openreview.net/forum?id=Y6aHdDNQYD) |  | 0 | LiDAR-based 3D object detection is crucial for various applications but often experiences performance degradation in real-world deployments due to domain shifts. While most studies focus on cross-dataset shifts, such as changes in environments and object geometries, practical corruptions from... | Junjie Meng, Mahsa Baktashmotlagh, Yadan Luo, Yonggang Zhang, Zhuoxiao Chen, Zi Huang |  |
| 125 |  |  [Synthetic continued pretraining](https://openreview.net/forum?id=07yvxWDSla) |  | 0 | Pretraining on large-scale, unstructured internet text enables language models to acquire a significant amount of world knowledge. However, this knowledge acquisition is data-inefficient---to learn a fact, models must be trained on hundreds to thousands of diverse representations of it. This poses... | Emmanuel J. Candès, Neil Band, Shuangping Li, Tatsunori Hashimoto, Zitong Yang |  |
| 126 |  |  [EmbodiedSAM: Online Segment Any 3D Thing in Real Time](https://openreview.net/forum?id=XFYUwIyTxQ) |  | 0 | Embodied tasks require the agent to fully understand 3D scenes simultaneously with its exploration, so an online, real-time, fine-grained and highly-generalized 3D perception model is desperately needed. Since high-quality 3D data is limited, directly training such a model in 3D is infeasible.... | Huangxing Chen, Jie Zhou, Jiwen Lu, Linqing Zhao, Xiuwei Xu, Ziwei Wang |  |
| 127 |  |  [Tractable Multi-Agent Reinforcement Learning through Behavioral Economics](https://openreview.net/forum?id=stUKwWBuBm) |  | 0 | A significant roadblock to the development of principled multi-agent reinforcement learning (MARL) algorithms is the fact that desired solution concepts like Nash equilibria may be intractable to compute. We show how one can overcome this obstacle by introducing concepts from behavioral economics... | Eric Mazumdar, Kishan Panaganti, Laixi Shi |  |
| 128 |  |  [Improved Finite-Particle Convergence Rates for Stein Variational Gradient Descent](https://openreview.net/forum?id=sbG8qhMjkZ) |  | 0 | We provide finite-particle convergence rates for the Stein Variational Gradient Descent (SVGD) algorithm in the Kernelized Stein Discrepancy ($\KSD$) and Wasserstein-2 metrics. Our key insight is that the time derivative of the relative entropy between the joint density of $N$ particle locations... | Krishna Balasubramanian, Promit Ghosal, Sayan Banerjee |  |
| 129 |  |  [Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning](https://openreview.net/forum?id=gc8QAQfXv6) |  | 0 | Catastrophic forgetting (CF) poses a significant challenge in machine learning, where a model forgets previously learned information upon learning new tasks. Despite the advanced capabilities of Large Language Models (LLMs), they continue to face challenges with CF during continual learning. The... | Caigao Jiang, Defu Lian, Gangwei Jiang, Jun Zhou, Linqi Song, Siqiao Xue, Ying Wei, Zhaoyi Li |  |
| 130 |  |  [One Step Diffusion via Shortcut Models](https://openreview.net/forum?id=OlzB6LnXcS) |  | 0 | Diffusion models and flow matching models have enabled generating diverse and realistic images by learning to transfer noise to data. However, sampling from these models involves iterative denoising over many neural network passes, making generation slow and expensive. Previous approaches for... | Danijar Hafner, Kevin Frans, Pieter Abbeel, Sergey Levine |  |
| 131 |  |  [Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment](https://openreview.net/forum?id=mtSSFiqW6y) |  | 0 | The performance of large language models (LLMs) is closely linked to their underlying size, leading to ever-growing networks and hence slower inference. Speculative decoding has been proposed as a technique to accelerate autoregressive generation, leveraging a fast draft model to propose candidate... | Albert Pumarola, Ali K. Thabet, Artsiom Sanakoyeu, Edgar Schönfeld, Gregor Bachmann, Jonas Kohler, Markos Georgopoulos, Sotiris Anagnostidis, Yuming Du |  |
| 132 |  |  [Robustness Inspired Graph Backdoor Defense](https://openreview.net/forum?id=trKNi4IUiP) |  | 0 | Graph Neural Networks (GNNs) have achieved promising results in tasks such as node classification and graph classification. However, recent studies reveal that GNNs are vulnerable to backdoor attacks, posing a significant threat to their real-world adoption. Despite initial efforts to defend... | Enyan Dai, Junjie Xu, Minhua Lin, Suhang Wang, Zhiwei Zhang, Zongyu Wu |  |
| 133 |  |  [Proxy Denoising for Source-Free Domain Adaptation](https://openreview.net/forum?id=FIj9IEPCKr) |  | 0 | Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain with no access to the source data. Inspired by the success of large Vision-Language (ViL) models in many applications, the latest research has validated ViL's benefit for SFDA by using their... | Jianwei Zhang, Mao Ye, Song Tang, Wenxin Su, Xiatian Zhu, Yan Gan |  |
| 134 |  |  [Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models](https://openreview.net/forum?id=tc90LV0yRL) |  | 0 | Language Model (LM) agents for cybersecurity that are capable of autonomously identifying vulnerabilities and executing exploits have potential to cause real-world impact. Policymakers, model providers, and researchers in the AI and cybersecurity communities are interested in quantifying the... | Andy K. Zhang, Aolin Zhang, Ari Glenn, Celeste Menders, Daniel Zamoshchin, Derek Askaryar, Donovan Julian Jasper, Eliot Jones, Gashon Hussein, Haoxiang Yang, Joey Ji, Justin W. Lin, Leo Glikbarg, Nathan Tran, Neil Perry, Pura Peetathawatchai, Rishi Alluri, Riya Dulepet, Samantha Liu, Vikram Sivashankar, et al. |  |
| 135 |  |  [Open-YOLO 3D: Towards Fast and Accurate Open-Vocabulary 3D Instance Segmentation](https://openreview.net/forum?id=CRmiX0v16e) |  | 0 | Recent works on open-vocabulary 3D instance segmentation show strong promise but at the cost of slow inference speed and high computation requirements. This high computation cost is typically due to their heavy reliance on aggregated clip features from multi-view, which require computationally... | Angela Dai, Fahad Shahbaz Khan, Hisham Cholakkal, Jean Lahoud, Mohamed El Amine Boudjoghra, Rao Muhammad Anwer, Salman H. Khan |  |
| 136 |  |  [Safety Alignment Should be Made More Than Just a Few Tokens Deep](https://openreview.net/forum?id=6Mxhg9PtDE) |  | 0 | The safety alignment of current Large Language Models (LLMs) is vulnerable. Simple attacks, or even benign fine-tuning, can jailbreak aligned models. We note that many of these vulnerabilities are related to a shared underlying issue: safety alignment can take shortcuts, wherein the alignment... | Ahmad Beirami, Ashwinee Panda, Kaifeng Lyu, Peter Henderson, Prateek Mittal, Subhrajit Roy, Xiangyu Qi, Xiao Ma |  |
| 137 |  |  [On the Identification of Temporal Causal Representation with Instantaneous Dependence](https://openreview.net/forum?id=2efNHgYRvM) |  | 0 | Temporally causal representation learning aims to identify the latent causal process from time series observations, but most methods require the assumption that the latent causal processes do not have instantaneous relations. Although some recent methods achieve identifiability in the instantaneous... | Guangyi Chen, Kaitao Zheng, Kun Zhang, Mingming Gong, Ruichu Cai, Xiangchen Song, Yifan Shen, Zijian Li |  |
| 138 |  |  [WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://openreview.net/forum?id=mMPMHWOdOy) |  | 0 | Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization.... | Can Xu, Chongyang Tao, Dongmei Zhang, Haipeng Luo, JianGuang Lou, Pu Zhao, Qingfeng Sun, Qingwei Lin, Shifeng Chen, Xiubo Geng, Yansong Tang |  |
| 139 |  |  [Faster Cascades via Speculative Decoding](https://openreview.net/forum?id=vo9t20wsmd) |  | 0 | Cascades and speculative decoding are two common approaches to improving language models' inference efficiency. Both approaches interleave two models, but via fundamentally distinct mechanisms: deferral rule that invokes the larger model only for “hard” inputs, while speculative decoding uses... | Aditya Krishna Menon, Ankit Singh Rawat, Harikrishna Narasimhan, Neha Gupta, Sanjiv Kumar, Seungyeon Kim, Wittawat Jitkrittum |  |
| 140 |  |  [The Hidden Cost of Waiting for Accurate Predictions](https://openreview.net/forum?id=A3YUPeJTNR) |  | 0 | Algorithmic predictions are increasingly informing societal resource allocations by identifying individuals for targeting. Policymakers often build these systems with the assumption that by gathering more observations on individuals, they can improve predictive accuracy and, consequently,... | Ali Shirali, Ariel D. Procaccia, Rediet Abebe |  |
| 141 |  |  [Learning Dynamics of LLM Finetuning](https://openreview.net/forum?id=tPNHOoZFl9) |  | 0 | Learning dynamics, which describes how the learning of specific training examples influences the model's predictions on other examples, gives us a powerful tool for understanding the behavior of deep learning systems. We study the learning dynamics of large language models during different types of... | Danica J. Sutherland, Yi Ren |  |
| 142 |  |  [Root Cause Analysis of Anomalies in Multivariate Time Series through Granger Causal Discovery](https://openreview.net/forum?id=k38Th3x4d9) |  | 0 | Identifying the root causes of anomalies in multivariate time series is challenging due to the complex dependencies among the series. In this paper, we propose a comprehensive approach called AERCA that inherently integrates Granger causal discovery with root cause analysis. By defining anomalies... | Lu Zhang, Saima Absar, Shuhan Yuan, Xiao Han |  |
| 143 |  |  [ProtComposer: Compositional Protein Structure Generation with 3D Ellipsoids](https://openreview.net/forum?id=0ctvBgKFgc) |  | 0 | We develop ProtComposer to generate protein structures conditioned on spatial protein layouts that are specified via a set of 3D ellipsoids capturing substructure shapes and semantics. At inference time, we condition on ellipsoids that are hand-constructed, extracted from existing proteins, or from... | Arash Vahdat, Bowen Jing, Hannes Stärk, Jason Yim, Karsten Kreis, Tomas Geffner, Tommi S. Jaakkola |  |
| 144 |  |  [More RLHF, More Trust? On The Impact of Preference Alignment On Trustworthiness](https://openreview.net/forum?id=FpiCLJrSW8) |  | 0 | The trustworthiness of Large Language Models (LLMs) refers to the extent to which their outputs are reliable, safe, and ethically aligned, and it has become a crucial consideration alongside their cognitive performance. In practice, Reinforcement Learning From Human Feedback (RLHF) has been widely... | Aaron Jiaxun Li, Himabindu Lakkaraju, Satyapriya Krishna |  |
| 145 |  |  [Geometry-aware RL for Manipulation of Varying Shapes and Deformable Objects](https://openreview.net/forum?id=7BLXhmWvwF) |  | 0 | Manipulating objects with varying geometries and deformable objects is a major challenge in robotics. Tasks such as insertion with different objects or cloth hanging require precise control and effective modelling of complex dynamics. In this work, we frame this problem through the lens of a... | Gerhard Neumann, Huy Le, Ngo Anh Vien, Philipp Becker, Tai Hoang |  |
| 146 |  |  [Topological Blindspots: Understanding and Extending Topological Deep Learning Through the Lens of Expressivity](https://openreview.net/forum?id=EzjsoomYEb) |  | 0 | Topological deep learning (TDL) is a rapidly growing field that seeks to leverage topological structure in data and facilitate learning from data supported on topological objects, ranging from molecules to 3D shapes. Most TDL architectures can be unified under the framework of higher-order... | Fabrizio Frasca, Guy BarShalom, Haggai Maron, Michael M. Bronstein, Yam Eitan, Yoav Gelberg |  |
| 147 |  |  [Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency](https://openreview.net/forum?id=weM4YBicIP) |  | 0 | With the introduction of video diffusion model, audio-conditioned human video generation has recently achieved significant breakthroughs in both the naturalness of motion and the synthesis of portrait details. Due to the limited control of audio signals in driving human motion, existing methods... | Chao Liang, Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Tianyun Zhong, Yanbo Zheng |  |
| 148 |  |  [CyberHost: A One-stage Diffusion Framework for Audio-driven Talking Body Generation](https://openreview.net/forum?id=vaEPihQsAA) |  | 0 | Diffusion-based video generation technology has advanced significantly, catalyzing a proliferation of research in human animation. While breakthroughs have been made in driving human animation through various modalities for portraits, most of current solutions for human body animation still focus... | Chao Liang, Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Tianyun Zhong, Yanbo Zheng, Zerong Zheng |  |
| 149 |  |  [Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model](https://openreview.net/forum?id=SI2hI0frk6) |  | 0 | We introduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data. Transfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed-modality sequences. We pretrain multiple Transfusion models... | Arun Babu, Chunting Zhou, Jacob Kahn, Kushal Tirumala, Leonid Shamis, Lili Yu, Luke Zettlemoyer, Michihiro Yasunaga, Omer Levy, Xuezhe Ma |  |
| 150 |  |  [MoE++: Accelerating Mixture-of-Experts Methods with Zero-Computation Experts](https://openreview.net/forum?id=t7P5BUKcYv) |  | 0 | In this work, we aim to simultaneously enhance the effectiveness and efficiency of Mixture-of-Experts (MoE) methods. To achieve this, we propose MoE++, a general and heterogeneous MoE framework that integrates both Feed-Forward Network (FFN) and zero-computation experts. Specifically, we introduce... | Bo Zhu, Li Yuan, Peng Jin, Shuicheng Yan |  |
| 151 |  |  [Compositional Entailment Learning for Hyperbolic Vision-Language Models](https://openreview.net/forum?id=3i13Gev2hV) |  | 0 | Image-text representation learning forms a cornerstone in vision-language models, where pairs of images and textual descriptions are contrastively aligned in a shared embedding space. Since visual and textual concepts are naturally hierarchical, recent work has shown that hyperbolic space can serve... | Alessandro Flaborea, Avik Pal, Fabio Galasso, Guido Maria D'Amely di Melendugno, Max van Spengler, Pascal Mettes |  |
| 152 |  |  [Advantage Alignment Algorithms](https://openreview.net/forum?id=QFO1asgas2) |  | 0 | Artificially intelligent agents are increasingly being integrated into human decision-making: from large language model (LLM) assistants to autonomous vehicles. These systems often optimize their individual objective, leading to conflicts, particularly in general-sum games where naive reinforcement... | Aaron C. Courville, Gauthier Gidel, Juan Agustin Duque, Milad Aghajohari, Razvan Ciuca, Tianyu Zhang, Tim Cooijmans |  |
| 153 |  |  [Scaling In-the-Wild Training for Diffusion-based Illumination Harmonization and Editing by Imposing Consistent Light Transport](https://openreview.net/forum?id=u1cQYxRI1H) |  | 0 | Diffusion-based image generators are becoming unique methods for illumination harmonization and editing. The current bottleneck in scaling up the training of diffusion-based illumination editing models is mainly in the difficulty of preserving the underlying image details and maintaining intrinsic... | Anyi Rao, Lvmin Zhang, Maneesh Agrawala |  |
| 154 |  |  [AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models](https://openreview.net/forum?id=HvSytvg3Jh) |  | 0 | Large language models (LLMs) often exhibit hallucinations, producing incorrect or outdated knowledge. Hence, model editing methods have emerged to enable targeted knowledge updates. To achieve this, a prevailing paradigm is the locating-then-editing approach, which first locates influential... | Houcheng Jiang, Jie Shi, Junfeng Fang, Kun Wang, TatSeng Chua, Xiang Wang, Xiangnan He, Yunshan Ma |  |
| 155 |  |  [DeepLTL: Learning to Efficiently Satisfy Complex LTL Specifications for Multi-Task RL](https://openreview.net/forum?id=9pW2J49flQ) |  | 0 | Linear temporal logic (LTL) has recently been adopted as a powerful formalism for specifying complex, temporally extended tasks in multi-task reinforcement learning (RL). However, learning policies that efficiently satisfy arbitrary specifications not observed during training remains a challenging... | Alessandro Abate, Mathias Jackermeier |  |
| 156 |  |  [On the Role of Attention Heads in Large Language Model Safety](https://openreview.net/forum?id=h0Ak8A5yqw) |  | 0 | Large language models (LLMs) achieve state-of-the-art performance on multiple language tasks, yet their safety guardrails can be circumvented, leading to harmful generations. In light of this, recent research on safety mechanisms has emerged, revealing that when safety representations or component... | Fei Huang, Haiyang Yu, Junfeng Fang, Kun Wang, Rongwu Xu, Xinghua Zhang, Yang Liu, Yongbin Li, Zhenhong Zhou |  |
| 157 |  |  [Influence Functions for Scalable Data Attribution in Diffusion Models](https://openreview.net/forum?id=esYrEndGsr) |  | 0 | Diffusion models have led to significant advancements in generative modelling. Yet their widespread adoption poses challenges regarding data attribution and interpretability. In this paper, we aim to help address such challenges in diffusion models by extending influence functions. Influence... | Alexander Immer, Bruno Kacper Mlodozeniec, David Krueger, Juhan Bae, Richard E. Turner, Runa Eschenhagen |  |
| 158 |  |  [Second-Order Min-Max Optimization with Lazy Hessians](https://openreview.net/forum?id=ijbA5swmoK) |  | 0 | This paper studies second-order methods for convex-concave minimax optimization. Monteiro & Svaiter (2012) proposed a method to solve the problem with an optimal iteration complexity of $\mathcal{O}(\epsilon^{-3/2})$ to find an $\epsilon$-saddle point. However, it is unclear whether the... | Chengchang Liu, Jingzhao Zhang, Lesi Chen |  |
| 159 |  |  [Composing Unbalanced Flows for Flexible Docking and Relaxation](https://openreview.net/forum?id=gHLWTzKiZV) |  | 0 | Diffusion models have emerged as a successful approach for molecular docking, but they often cannot model protein flexibility or generate nonphysical poses. We argue that both these challenges can be tackled by framing the problem as a transport between distributions. Still, existing paradigms lack... | Andreas Krause, Gabriele Corso, Noah Getz, Regina Barzilay, Tommi S. Jaakkola, Vignesh Ram Somnath |  |
| 160 |  |  [Learning Distributions of Complex Fluid Simulations with Diffusion Graph Networks](https://openreview.net/forum?id=uKZdlihDDn) |  | 0 | Physical systems with complex unsteady dynamics, such as fluid flows, are often poorly represented by a single mean solution. For many practical applications, it is crucial to access the full distribution of possible states, from which relevant statistics (e.g., RMS and two-point correlations) can... | Mario Lino Valencia, Nils Thuerey, Tobias Pfaff |  |
| 161 |  |  [Training Language Models to Self-Correct via Reinforcement Learning](https://openreview.net/forum?id=CjwERcAU7w) |  | 0 | Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Current methods for training self-correction typically depend on either multiple models, a more advanced model, or additional forms of... | Aleksandra Faust, Avi Singh, Aviral Kumar, Colton Bishop, Cosmin Paduraru, Disha Shrivastava, Doina Precup, Feryal M. P. Behbahani, George Tucker, John D. CoReyes, Kate Baumli, Kay McKinney, Lei M. Zhang, Rebecca Roelofs, Rishabh Agarwal, Shariq Iqbal, Vincent Zhuang, Yi Su |  |
| 162 |  |  [AI as Humanity's Salieri: Quantifying Linguistic Creativity of Language Models via Systematic Attribution of Machine Text against Web Text](https://openreview.net/forum?id=ilOEOIqolQ) |  | 0 | Creativity has long been considered one of the most difficult aspect of human intelligence for AI to mimic. However, the rise of Large Language Models (LLMs), like ChatGPT, has raised questions about whether AI can match or even surpass human creativity. We present CREATIVITY INDEX as the first... | Allyson Ettinger, Jiacheng Liu, Khyathi Raghavi Chandu, Liwei Jiang, Melanie Sclar, Niloofar Mireshghallah, Nouha Dziri, Seungju Han, Skyler Hallinan, Ximing Lu, Yejin Choi |  |
| 163 |  |  [Comparing noisy neural population dynamics using optimal transport distances](https://openreview.net/forum?id=cNmu0hZ4CL) |  | 0 | Biological and artificial neural systems form high-dimensional neural representations that underpin their computational capabilities. Methods for quantifying geometric similarity in neural representations have become a popular tool for identifying computational principles that are potentially... | Alex H. Williams, Amin Nejatbakhsh, David Lipshutz, Victor Geadah |  |
| 164 |  |  [Learning stochastic dynamics from snapshots through regularized unbalanced optimal transport](https://openreview.net/forum?id=gQlxd3Mtru) |  | 0 | Reconstructing dynamics using samples from sparsely time-resolved snapshots is an important problem in both natural sciences and machine learning. Here, we introduce a new deep learning approach for solving regularized unbalanced optimal transport (RUOT) and inferring continuous unbalanced... | Peijie Zhou, Tiejun Li, Zhenyi Zhang |  |
| 165 |  |  [Prioritized Generative Replay](https://openreview.net/forum?id=5IkDAfabuo) |  | 0 | Sample-efficient online reinforcement learning often uses replay buffers to store experience for reuse when updating the value function. However, uniform replay is inefficient, since certain classes of transitions can be more relevant to learning. While prioritization of more useful samples is... | Alexei A. Efros, Kevin Frans, Pieter Abbeel, Renhao Wang, Sergey Levine |  |
| 166 |  |  [The Geometry of Categorical and Hierarchical Concepts in Large Language Models](https://openreview.net/forum?id=bVTM2QKYuA) |  | 0 | The linear representation hypothesis is the informal idea that semantic concepts are encoded as linear directions in the representation spaces of large language models (LLMs). Previous work has shown how to make this notion precise for representing binary concepts that have natural contrasts (e.g.,... | Kiho Park, Victor Veitch, Yibo Jiang, Yo Joong Choe |  |
| 167 |  |  [Generator Matching: Generative modeling with arbitrary Markov processes](https://openreview.net/forum?id=RuP17cJtZo) |  | 0 | We introduce Generator Matching, a modality-agnostic framework for generative modeling using arbitrary Markov processes. Generators characterize the infinitesimal evolution of a Markov process, which we leverage for generative modeling in a similar vein to flow matching: we construct conditional... | Brian Karrer, Itai Gat, Jason Yim, Marton Havasi, Neta Shaul, Peter Holderrieth, Ricky T. Q. Chen, Tommi S. Jaakkola, Yaron Lipman |  |
| 168 |  |  [No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images](https://openreview.net/forum?id=P4o9akekdf) |  | 0 | We introduce NoPoSplat, a feed-forward model capable of reconstructing 3D scenes parameterized by 3D Gaussians from unposed sparse multi-view images. Our model, trained exclusively with photometric loss, achieves real-time 3D Gaussian reconstruction during inference. To eliminate the need for... | Botao Ye, Haofei Xu, Marc Pollefeys, MingHsuan Yang, Sifei Liu, Songyou Peng, Xueting Li |  |
| 169 |  |  [Variational Diffusion Posterior Sampling with Midpoint Guidance](https://openreview.net/forum?id=6EUtjXAvmj) |  | 0 | Diffusion models have recently shown considerable potential in solving Bayesian inverse problems when used as priors. However, sampling from the resulting denoising posterior distributions remains a challenge as it involves intractable terms. To tackle this issue, state-of-the-art approaches... | Alain Oliviero Durmus, Badr Moufad, Eric Moulines, Jimmy Olsson, Lisa Bedin, Randal Douc, Yazid Janati |  |
| 170 |  |  [Towards Understanding Why FixMatch Generalizes Better Than Supervised Learning](https://openreview.net/forum?id=25kAzqzTrz) |  | 0 | Semi-supervised learning (SSL), exemplified by FixMatch (Sohn et al., 2020), has shown significant generalization advantages over supervised learning (SL), particularly in the context of deep neural networks (DNNs). However, it is still unclear, from a theoretical standpoint, why FixMatch-like SSL... | Jiachun Pan, Jingyang Li, KimChuan Toh, Pan Zhou, Vincent Y. F. Tan |  |
| 171 |  |  [NeuralPlane: Structured 3D Reconstruction in Planar Primitives with Neural Fields](https://openreview.net/forum?id=5UKrnKuspb) |  | 0 | 3D maps assembled from planar primitives are compact and expressive in representing man-made environments. In this paper, we present \*\*NeuralPlane\*\*, a novel approach that explores \*\*neural\*\* fields for multi-view 3D \*\*plane\*\* reconstruction. Our method is centered upon the core idea of... | Hanqiao Ye, Shuhan Shen, Yangdong Liu, Yuzhou Liu |  |
| 172 |  |  [Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models](https://openreview.net/forum?id=I4e82CIDxv) |  | 0 | We introduce methods for discovering and applying \*\*sparse feature circuits\*\*. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like... | Aaron Mueller, Can Rager, David Bau, Eric J. Michaud, Samuel Marks, Yonatan Belinkov |  |
| 173 |  |  [Retrieval Head Mechanistically Explains Long-Context Factuality](https://openreview.net/forum?id=EytBpUGB1Z) |  | 0 | Despite the recent progress in long-context language models, it remains elusive how transformer-based models exhibit the capability to retrieve relevant information from arbitrary locations within the long context. This paper aims to address this question. Our systematic investigation across a wide... | Guangxuan Xiao, Hao Peng, Wenhao Wu, Yao Fu, Yizhong Wang |  |
| 174 |  |  [High-Dynamic Radar Sequence Prediction for Weather Nowcasting Using Spatiotemporal Coherent Gaussian Representation](https://openreview.net/forum?id=Cjz9Xhm7sI) |  | 0 | Weather nowcasting is an essential task that involves predicting future radar echo sequences based on current observations, offering significant benefits for disaster management, transportation, and urban planning. Current prediction methods are limited by training and storage efficiency, mainly... | Lin Zeng, Ruimao Zhang, Yiran Qin, Ziye Wang |  |
| 175 |  |  [Differential Transformer](https://openreview.net/forum?id=OvoCm1gGhN) |  | 0 | Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two... | Furu Wei, Gao Huang, Li Dong, Tianzhu Ye, Yi Zhu, Yuqing Xia, Yutao Sun |  |
| 176 |  |  [Open-Vocabulary Customization from CLIP via Data-Free Knowledge Distillation](https://openreview.net/forum?id=1aF2D2CPHi) |  | 0 | Vision-language models such as CLIP have demonstrated strong zero-shot performance, but their considerable size and inefficient inference limit customizable deployment for users. While knowledge distillation is a solution, it still requires the original data, which is not always available due to... | Chun Yuan, Dacheng Tao, Li Shen, Yongxian Wei, Zhenyi Wang, Zixuan Hu |  |
| 177 |  |  [Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement](https://openreview.net/forum?id=UHPnqSTBPO) |  | 0 | We present a principled approach to provide LLM-based evaluation with a rigorous guarantee of human agreement. We first propose that a reliable evaluation method should not uncritically rely on model preferences for pairwise evaluation, but rather assess the confidence of judge models and... | Faeze Brahman, Jaehun Jung, Yejin Choi |  |
| 178 |  |  [Your Mixture-of-Experts LLM Is Secretly an Embedding Model for Free](https://openreview.net/forum?id=eFGQ97z5Cd) |  | 0 | While large language models (LLMs) excel on generation tasks, their decoder-only architecture often limits their potential as embedding models if no further representation finetuning is applied. Does this contradict their claim of generalists? To answer the question, we take a closer look at... | Tianyi Zhou, Ziyue Li |  |
| 179 |  |  [REEF: Representation Encoding Fingerprints for Large Language Models](https://openreview.net/forum?id=SnDmPkOJ0T) |  | 0 | Protecting the intellectual property of open-source Large Language Models (LLMs) is very important, because training LLMs costs extensive computational resources and data. Therefore, model owners and third parties need to identify whether a suspect model is a subsequent development of the victim... | Chen Qian, Dongrui Liu, Jie Zhang, Jing Shao, Linfeng Zhang, Yong Liu, Yu Qiao |  |
| 180 |  |  [Flat Reward in Policy Parameter Space Implies Robust Reinforcement Learning](https://openreview.net/forum?id=4OaO3GjP7k) |  | 0 | Investigating flat minima on loss surfaces in parameter space is well-documented in the supervised learning context, highlighting its advantages for model generalization. However, limited attention has been paid to the reinforcement learning (RL) context, where the impact of flatter reward... | HyunKyu Lee, Sung Whan Yoon |  |
| 181 |  |  [LLM-SR: Scientific Equation Discovery via Programming with Large Language Models](https://openreview.net/forum?id=m2nmp8P5in) |  | 0 | Mathematical equations have been unreasonably effective in describing complex natural phenomena across various scientific disciplines. However, discovering such insightful equations from data presents significant challenges due to the necessity of navigating extremely large combinatorial hypothesis... | Amir Barati Farimani, Chandan K. Reddy, Kazem Meidani, Parshin Shojaee, Shashank Gupta |  |
| 182 |  |  [Backtracking Improves Generation Safety](https://openreview.net/forum?id=Bo62NeU6VF) |  | 0 | Text generation has a fundamental limitation almost by definition: there is no taking back tokens that have been generated, even when they are clearly problematic. In the context of language model safety, when a partial unsafe generation is produced, language models by their nature tend to happily... | Daniel M. Bikel, Eric Michael Smith, Hailey Nguyen, Jason E. Weston, Jianfeng Chi, Kartikeya Upasani, Yiming Zhang |  |
| 183 |  |  [Rethinking the generalization of drug target affinity prediction algorithms via similarity aware evaluation](https://openreview.net/forum?id=j7cyANIAxV) |  | 0 | Drug-target binding affinity prediction is a fundamental task for drug discovery. It has been extensively explored in literature and promising results are reported. However, in this paper, we demonstrate that the results may be misleading and cannot be well generalized to real practice. The core... | Chenbin Zhang, Chuchu Jiang, Jie Xu, Shaoting Zhang, Wen Chen, Zhiqiang Hu |  |
| 184 |  |  [GridMix: Exploring Spatial Modulation for Neural Fields in PDE Modeling](https://openreview.net/forum?id=Fur0DtynPX) |  | 0 | Significant advancements have been achieved in PDE modeling using neural fields. Despite their effectiveness, existing methods rely on global modulation, limiting their ability to reconstruct local details. While spatial modulation with vanilla grid-based representations offers a promising... | Gao Huang, Honghui Wang, Shiji Song |  |
| 185 |  |  [Data Selection via Optimal Control for Language Models](https://openreview.net/forum?id=dhAL5fy8wS) |  | 0 | This work investigates the selection of high-quality pre-training data from massive corpora to enhance LMs' capabilities for downstream usage. We formulate data selection as a generalized Optimal Control problem, which can be solved theoretically by Pontryagin's Maximum Principle (PMP), yielding a... | Furu Wei, Hongning Wang, Li Dong, Minlie Huang, Qingxiu Dong, Yaru Hao, Yuxian Gu |  |
| 186 |  |  [Simplifying, Stabilizing and Scaling Continuous-time Consistency Models](https://openreview.net/forum?id=LyJi5ugyJx) |  | 0 | Consistency models (CMs) are a powerful class of diffusion-based generative models optimized for fast sampling. Most existing CMs are trained using discretized timesteps, which introduce additional hyperparameters and are prone to discretization errors. While continuous-time formulations can... | Cheng Lu, Yang Song |  |
| 187 |  |  [Dynamic Multimodal Evaluation with Flexible Complexity by Vision-Language Bootstrapping](https://openreview.net/forum?id=X1OfiRYCLn) |  | 0 | Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across multimodal tasks such as visual perception and reasoning, leading to good performance on various multimodal evaluation benchmarks. However, these benchmarks keep a static nature and overlap with the pre-training... | Kaipeng Zhang, Ping Luo, Shuibo Zhang, Wenqi Shao, Yi Bin, Yu Wang, Yue Yang |  |
| 188 |  |  [Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models](https://openreview.net/forum?id=mtJSMcF3ek) |  | 0 | Self-improvement is a mechanism in Large Language Model (LLM) pre-training, post-training and test-time inference. We explore a framework where the model verifies its own outputs, filters or reweights data based on this verification, and distills the filtered data. Despite several empirical... | Carson Eisenach, Dean P. Foster, Hanlin Zhang, Sham M. Kakade, Udaya Ghai, Yuda Song |  |
| 189 |  |  [SANA: Efficient High-Resolution Text-to-Image Synthesis with Linear Diffusion Transformers](https://openreview.net/forum?id=N8Oj1XhtYZ) |  | 0 | We introduce Sana, a text-to-image framework that can efficiently generate images up to 4096$\times$4096 resolution. Sana can synthesize high-resolution, high-quality images with strong text-image alignment at a remarkably fast speed, deployable on laptop GPU. Core designs include: (1) Deep... | Enze Xie, Han Cai, Haotian Tang, Junsong Chen, Junyu Chen, Ligeng Zhu, Muyang Li, Song Han, Yao Lu, Yujun Lin, Zhekai Zhang |  |
| 190 |  |  [Can a MISL Fly? Analysis and Ingredients for Mutual Information Skill Learning](https://openreview.net/forum?id=xoIeVdFO7U) |  | 0 | Self-supervised learning has the potential of lifting several of the key challenges in reinforcement learning today, such as exploration, representation learning, and reward design. Recent work (METRA) has effectively argued that moving away from mutual information and instead optimizing a certain... | Benjamin Eysenbach, Chongyi Zheng, Jens Tuyls, Joanne Peng |  |
| 191 |  |  [When Selection Meets Intervention: Additional Complexities in Causal Discovery](https://openreview.net/forum?id=xByvdb3DCm) |  | 0 | We address the common yet often-overlooked selection bias in interventional studies, where subjects are selectively enrolled into experiments. For instance, participants in a drug trial are usually patients of the relevant disease; A/B tests on mobile applications target existing users only, and... | Gongxu Luo, Haoyue Dai, Ignavier Ng, Jianle Sun, Kun Zhang, Peter Spirtes, Xinshuai Dong, Zeyu Tang |  |
| 192 |  |  [LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias](https://openreview.net/forum?id=QQBPWtvtcn) |  | 0 | We propose the Large View Synthesis Model (LVSM), a novel transformer-based approach for scalable and generalizable novel view synthesis from sparse-view inputs. We introduce two architectures: (1) an encoder-decoder LVSM, which encodes input image tokens into a fixed number of 1D latent tokens,... | Fujun Luan, Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Noah Snavely, Sai Bi, Tianyuan Zhang, Zexiang Xu |  |
| 193 |  |  [Flow Matching with General Discrete Paths: A Kinetic-Optimal Perspective](https://openreview.net/forum?id=tcvMzR2NrP) |  | 0 | The design space of discrete-space diffusion or flow generative models are significantly less well-understood than their continuous-space counterparts, with many works focusing only on a simple masked construction. In this work, we aim to take a holistic approach to the construction of discrete... | Anuroop Sriram, Brian Karrer, Daniel Severo, Itai Gat, Marton Havasi, Neta Shaul, Peter Holderrieth, Ricky T. Q. Chen, Yaron Lipman |  |
| 194 |  |  [Cut Your Losses in Large-Vocabulary Language Models](https://openreview.net/forum?id=E4Fk3YuG56) |  | 0 | As language models grow ever larger, so do their vocabularies. This has shifted the memory footprint of LLMs during training disproportionately to one single layer: the cross-entropy in the loss computation. Cross-entropy builds up a logit matrix with entries for each pair of input tokens and... | Alexander Hertzberg, Brody Huval, Erik Wijmans, Philipp Krähenbühl, Vladlen Koltun |  |
| 195 |  |  [AFlow: Automating Agentic Workflow Generation](https://openreview.net/forum?id=z5uVAKwmjf) |  | 0 | Large language models (LLMs) have demonstrated remarkable potential in solving complex tasks across diverse domains, typically by employing agentic workflows that follow detailed instructions and operational sequences. However, constructing these workflows requires significant human effort,... | Bang Liu, Bingnan Zheng, Chenglin Wu, Fengwei Teng, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Jinyu Xiang, Mingchen Zhuge, Sirui Hong, Xin Cheng, Xionghui Chen, Yuyu Luo, Zhaoyang Yu |  |
| 196 |  |  [Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Models](https://openreview.net/forum?id=uAFHCZRmXk) |  | 0 | Contrastive vision-language models (VLMs), like CLIP, have gained popularity for their versatile applicability to various downstream tasks. Despite their successes in some tasks, like zero-shot object recognition, they perform surprisingly poor on other tasks, like attribute recognition. Previous... | David T. Hoffmann, Max Argus, Simon Schrodi, Thomas Brox, Volker Fischer |  |
| 197 |  |  [FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference](https://openreview.net/forum?id=OfjIlbelrT) |  | 0 | Large language models (LLMs) encounter computational challenges during long-sequence inference, especially in the attention pre-filling phase, where the complexity grows quadratically with the prompt length. Previous efforts to mitigate these challenges have relied on fixed sparse attention... | Jianqiao Lu, Xun Zhou, Xunhao Lai, Yao Luo, Yiyuan Ma |  |
| 198 |  |  [REGENT: A Retrieval-Augmented Generalist Agent That Can Act In-Context in New Environments](https://openreview.net/forum?id=NxyfSW6mLK) |  | 0 | Building generalist agents that can rapidly adapt to new environments is a key challenge for deploying AI in the digital and real worlds. Is scaling current agent architectures the most effective way to build generalist agents? We propose a novel approach to pre-train relatively small policies on... | Dinesh Jayaraman, Insup Lee, Kaustubh Sridhar, Souradeep Dutta |  |
| 199 |  |  [MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models](https://openreview.net/forum?id=HnhNRrLPwm) |  | 0 | Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this capability remains insufficient. Existing benchmarks... | Chenhang Cui, Huaxiu Yao, Lijuan Wang, Linjie Li, Mingyu Ding, Peng Xia, Shi Qiu, Siwei Han, Wenhao Zheng, Yiyang Zhou, Zhaorun Chen, Zhaoyang Wang |  |
| 200 |  |  [Do as We Do, Not as You Think: the Conformity of Large Language Models](https://openreview.net/forum?id=st77ShxP1K) |  | 0 | Recent advancements in large language models (LLMs) revolutionize the field of intelligent agents, enabling collaborative multi-agent systems capable of tackling complex problems across various domains. However, the potential of conformity within these systems, analogous to phenomena like... | Guikun Chen, Wenguan Wang, Zhiyuan Weng |  |
| 201 |  |  [Artificial Kuramoto Oscillatory Neurons](https://openreview.net/forum?id=nwDRD4AMoN) |  | 0 | It has long been known in both neuroscience and AI that \`\`binding'' between neurons leads to a form of competitive learning where representations are compressed in order to represent more abstract concepts in deeper layers of the network. More recently, it was also hypothesized that dynamic... | Andreas Geiger, Max Welling, Sindy Löwe, Takeru Miyato |  |
| 202 |  |  [Booster: Tackling Harmful Fine-tuning for Large Language Models via Attenuating Harmful Perturbation](https://openreview.net/forum?id=tTPHgb0EtV) |  | 0 | Harmful fine-tuning attack poses serious safety concerns for large language models' fine-tuning-as-a-service. While existing defenses have been proposed to mitigate the issue, their performances are still far away from satisfactory, and the root cause of the problem has not been fully recovered. To... | Fatih Ilhan, Ling Liu, Selim Furkan Tekin, Sihao Hu, Tiansheng Huang |  |
| 203 |  |  [Unlearning-based Neural Interpretations](https://openreview.net/forum?id=PBjCTeDL6o) |  | 0 | Gradient-based interpretations often require an anchor point of comparison to avoid saturation in computing feature importance. We show that current baselines defined using static functions—constant mapping, averaging or blurring—inject harmful colour, texture or frequency assumptions that deviate... | Alexandre Duplessis, Ching Lam Choi, Serge J. Belongie |  |
| 204 |  |  [ChartMoE: Mixture of Diversely Aligned Expert Connector for Chart Understanding](https://openreview.net/forum?id=o5TsWTUSeF) |  | 0 | Automatic chart understanding is crucial for content comprehension and document parsing. Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in chart understanding through domain-specific alignment and fine-tuning. However, current MLLMs still struggle to provide... | Bowen Qu, Chengjin Xu, Chun Yuan, Jian Guo, Sinan Du, Yiyan Qi, Zhengzhuo Xu |  |
| 205 |  |  [Probabilistic Learning to Defer: Handling Missing Expert Annotations and Controlling Workload Distribution](https://openreview.net/forum?id=zl0HLZOJC9) |  | 0 | Recent progress in machine learning research is gradually shifting its focus towards \*human-AI cooperation\* due to the advantages of exploiting the reliability of human experts and the efficiency of AI models. One of the promising approaches in human-AI cooperation is \*learning to defer\* (L2D),... | Cuong C. Nguyen, Gustavo Carneiro, ThanhToan Do |  |
| 206 |  |  [A Decade's Battle on Dataset Bias: Are We There Yet?](https://openreview.net/forum?id=SctfBCLmWo) |  | 0 | We revisit the \`\`dataset classification'' experiment suggested by Torralba & Efros (2011) a decade ago, in the new era with large-scale, diverse, and hopefully less biased datasets as well as more capable neural network architectures. Surprisingly, we observe that modern neural networks can... | Kaiming He, Zhuang Liu |  |
| 207 |  |  [Knowing Your Target: Target-Aware Transformer Makes Better Spatio-Temporal Video Grounding](https://openreview.net/forum?id=WOzffPgVjF) |  | 0 | Transformer has attracted increasing interest in spatio-temporal video grounding, or STVG, owing to its end-to-end pipeline and promising result. Existing Transformer-based STVG approaches often leverage a set of object queries, which are initialized simply using zeros and then gradually learn... | Chenxi Luo, Heng Fan, Libo Zhang, Tiejian Luo, Xin Gu, Yan Huang, Yaojie Shen, Yuewei Lin |  |
| 208 |  |  [Open-World Reinforcement Learning over Long Short-Term Imagination](https://openreview.net/forum?id=vzItLaEoDa) |  | 0 | Training visual reinforcement learning agents in a high-dimensional open world presents significant challenges. While various model-based methods have improved sample efficiency by learning interactive world models, these agents tend to be “short-sighted”, as they are typically trained on short... | Jiajian Li, Qi Wang, Wenjun Zeng, Xiaokang Yang, Xin Jin, Yang Li, Yunbo Wang |  |
| 209 |  |  [OLMoE: Open Mixture-of-Experts Language Models](https://openreview.net/forum?id=xXTkbTBmqq) |  | 0 | We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all... | Akshita Bhagia, Alexander Wettig, Ali Farhadi, Binyuan Hui, David Wadden, Dirk Groeneveld, Douwe Kiela, Dustin Schwenk, Evan Pete Walsh, Jacob Morrison, Kyle Lo, Luca Soldaini, Nathan Lambert, Niklas Muennighoff, Oyvind Tafjord, Sewon Min, Shane Arora, Tim Dettmers, Weijia Shi, Yuling Gu, et al. |  |
| 210 |  |  [Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference under Ambiguities](https://openreview.net/forum?id=84pDoCD4lH) |  | 0 | Spatial expressions in situated communication can be ambiguous, as their meanings vary depending on the frames of reference (FoR) adopted by speakers and listeners. While spatial language understanding and reasoning by vision-language models (VLMs) have gained increasing attention, potential... | Fengyuan Hu, Freda Shi, Jayjun Lee, Joyce Chai, Parisa Kordjamshidi, Zheyuan Zhang, Ziqiao Ma |  |
| 211 |  |  [SAM 2: Segment Anything in Images and Videos](https://openreview.net/forum?id=Ha6RTeWMd0) |  | 0 | We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple... | Chaitanya Ryali, ChaoYuan Wu, Chloé Rolland, Christoph Feichtenhofer, Eric Mintun, Haitham Khedr, Junting Pan, Kalyan Vasudev Alwala, Laura Gustafson, Nicolas Carion, Nikhila Ravi, Piotr Dollár, Roman Rädle, Ronghang Hu, Ross B. Girshick, Tengyu Ma, Valentin Gabeur, YuanTing Hu |  |
| 212 |  |  [A Computational Framework for Modeling Emergence of Color Vision in the Human Brain](https://openreview.net/forum?id=g3xuCtrG6H) |  | 0 | It is a mystery how the brain decodes color vision purely from the optic nerve signals it receives, with a core inferential challenge being how it disentangles internal perception with the correct color dimensionality from the unknown encoding properties of the eye. In this paper, we introduce a... | Atsunobu Kotani, Ren Ng |  |
| 213 |  |  [PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding](https://openreview.net/forum?id=Q6a9W6kzv5) |  | 0 | Understanding the physical world is a fundamental challenge in embodied AI, critical for enabling agents to perform complex tasks and operate safely in real-world environments. While Vision-Language Models (VLMs) have shown great promise in reasoning and task planning for embodied agents, their... | Boyi Li, Daniel Seita, Jiageng Mao, Vitor Campagnolo Guizilini, Wei Chow, Yue Wang |  |
| 214 |  |  [How new data permeates LLM knowledge and how to dilute it](https://openreview.net/forum?id=NGKQoaqLpo) |  | 0 | Large language models continually learn through the accumulation of gradient-based updates, but how individual pieces of new information affect existing knowledge, leading to both beneficial generalization and problematic hallucination, remains poorly understood. We demonstrate that when learning... | Andrey Zhmoginov, Been Kim, Chen Sun, Mark Sandler, Max Vladymyrov, Nolan Andrew Miller, Renat Aksitov, Ulrich Rueckert |  |
| 215 |  |  [UniCBE: An Uniformity-driven Comparing Based Evaluation Framework with Unified Multi-Objective Optimization](https://openreview.net/forum?id=rpwGUtTeA5) |  | 0 | Human preference plays a significant role in measuring large language models and guiding them to align with human values. Unfortunately, current comparing-based evaluation (CBE) methods typically focus on a single optimization objective, failing to effectively utilize scarce yet valuable preference... | Boyuan Pan, Chuyi Tan, Jiayi Shi, Kan Li, Peiwen Yuan, Shaoxiong Feng, Xinglin Wang, Yao Hu, Yiwei Li, Yueqi Zhang |  |
| 216 |  |  [NetMoE: Accelerating MoE Training through Dynamic Sample Placement](https://openreview.net/forum?id=1qP3lsatCR) |  | 0 | Mixture of Experts (MoE) is a widely used technique to expand model sizes for better model quality while maintaining the computation cost constant. In a nutshell, an MoE model consists of multiple experts in each model layer and routes the training tokens to only a fixed number of experts rather... | Bin Cui, Fangcheng Fu, Shenhan Zhu, Xiaonan Nie, Xinyi Liu, Xupeng Miao, Yujie Wang |  |
| 217 |  |  [TabReD: Analyzing Pitfalls and Filling the Gaps in Tabular Deep Learning Benchmarks](https://openreview.net/forum?id=L14sqcrUC3) |  | 0 | Advances in machine learning research drive progress in real-world applications. To ensure this progress, it is important to understand the potential pitfalls on the way from a novel method's success on academic benchmarks to its practical deployment. In this work, we analyze existing tabular deep... | Artem Babenko, Ivan Rubachev, Nikolay Kartashev, Yury Gorishniy |  |
| 218 |  |  [Improving the Sparse Structure Learning of Spiking Neural Networks from the View of Compression Efficiency](https://openreview.net/forum?id=gcouwCx7dG) |  | 0 | The human brain utilizes spikes for information transmission and dynamically reorganizes its network structure to boost energy efficiency and cognitive capabilities throughout its lifespan. Drawing inspiration from this spike-based computation, Spiking Neural Networks (SNNs) have been developed to... | Badong Chen, Gang Pan, Jiangrong Shen, Qi Xu |  |
| 219 |  |  [JudgeLM: Fine-tuned Large Language Models are Scalable Judges](https://openreview.net/forum?id=xsELpEPn4A) |  | 0 | Evaluating Large Language Models (LLMs) in open-ended scenarios is challenging because existing benchmarks and metrics can not measure them comprehensively. To address this problem, we propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended... | Lianghui Zhu, Xinggang Wang, Xinlong Wang |  |
| 220 |  |  [Direct Post-Training Preference Alignment for Multi-Agent Motion Generation Model Using Implicit Feedback from Pre-training Demonstrations](https://openreview.net/forum?id=8UFG9D8xeU) |  | 0 | Recent advancements in Large Language Models (LLMs) have revolutionized motion generation models in embodied applications such as autonomous driving and robotic manipulation. While LLM-type auto-regressive motion generation models benefit from training scalability, there remains a discrepancy... | Kratarth Goel, Thomas Tian |  |
| 221 |  |  [Online Preference Alignment for Language Models via Count-based Exploration](https://openreview.net/forum?id=cfKZ5VrhXt) |  | 0 | Reinforcement Learning from Human Feedback (RLHF) has shown great potential in fine-tuning Large Language Models (LLMs) to align with human preferences. Existing methods perform preference alignment from a fixed dataset, which can be limited in data coverage and the resulting reward model is hard... | Chenjia Bai, Kang Xu, Qiaosheng Zhang, Shuang Qiu, Xuelong Li, Yang Zhang |  |
| 222 |  |  [Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence](https://openreview.net/forum?id=o1Et3MogPw) |  | 0 | The rapid advancement of large language models (LLMs) has paved the way for the development of highly capable autonomous agents. However, existing multi-agent frameworks often struggle with integrating diverse capable third-party agents due to reliance on agents defined within their own ecosystems.... | Chen Qian, Cheng Yang, Chenyang Zhao, Maosong Sun, Ran Li, Ruobing Xie, Weize Chen, Yitong Guan, Zhiyuan Liu, Ziming You |  |
| 223 |  |  [Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning](https://openreview.net/forum?id=A6Y7AqlzLW) |  | 0 | A promising approach for improving reasoning in large language models is to use process reward models (PRMs). PRMs provide feedback at each step of a multi-step reasoning trace, improving credit assignment over outcome reward models (ORMs) that only provide feedback at the final step. However,... | Adam Fisch, Alekh Agarwal, Amrith Setlur, Aviral Kumar, Chirag Nagpal, Jacob Eisenstein, Jonathan Berant, Rishabh Agarwal, Xinyang Geng |  |
| 224 |  |  [MQuAKE-Remastered: Multi-Hop Knowledge Editing Can Only Be Advanced with Reliable Evaluations](https://openreview.net/forum?id=m9wG6ai2Xk) |  | 0 | Large language models (LLMs) can give out erroneous answers to factually rooted questions either as a result of undesired training outcomes or simply because the world has moved on after a certain knowledge cutoff date. Under such scenarios, \*knowledge editing\* often comes to the rescue by... | Bhargav Bhushanam, Daochen Zha, KaiWei Chang, Kaixiong Zhou, Lize Shao, Louis Feng, Ninghao Liu, Shaochen (Henry) Zhong, Shuai Xu, Vipin Chaudhary, Xia Hu, Xiaocong Du, Yifan Lu, Yiwei Wang, Yixin Wan, Yucheng Shi |  |
| 225 |  |  [Competition Dynamics Shape Algorithmic Phases of In-Context Learning](https://openreview.net/forum?id=XgH1wfHSX8) |  | 0 | In-Context Learning (ICL) has significantly expanded the general-purpose nature of large language models, allowing them to adapt to novel tasks using merely the inputted context. This has motivated a series of papers that analyze tractable synthetic domains and postulate precise mechanisms that may... | Core Francisco Park, Ekdeep Singh Lubana, Hidenori Tanaka |  |
| 226 |  |  [In vivo cell-type and brain region classification via multimodal contrastive learning](https://openreview.net/forum?id=10JOlFIPjt) |  | 0 | Current electrophysiological approaches can track the activity of many neurons, yet it is usually unknown which cell-types or brain areas are being recorded without further molecular or histological analysis. Developing accurate and scalable algorithms for identifying the cell-type and brain region... | Andrew M. Shelton, Chandramouli Chandrasekaran, Charlie Windolf, Cole Lincoln Hurwitz, Eric Kenji Lee, Eva L. Dyer, Fan Yang, Han Yu, Hanrui Lyu, International Brain Laboratory, Liam Paninski, Nicholas A. Steinmetz, Olivier Winter, YiXun Xu |  |
| 227 |  |  [Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws](https://openreview.net/forum?id=FxNNiUgtfa) |  | 0 | Scaling laws describe the relationship between the size of language models and their capabilities. Unlike prior studies that evaluate a model's capability via loss or benchmarks, we estimate information-theoretically the number of knowledge \emph{bits} a model stores. We focus on factual knowledge... | Yuanzhi Li, Zeyuan AllenZhu |  |
| 228 |  |  [Scalable Decision-Making in Stochastic Environments through Learned Temporal Abstraction](https://openreview.net/forum?id=pQsllTesiE) |  | 0 | Sequential decision-making in high-dimensional continuous action spaces, particularly in stochastic environments, faces significant computational challenges. We explore this challenge in the traditional offline RL setting, where an agent must learn how to make decisions based on data collected... | Abhishek Dubey, Aron Laszka, Ava Pettet, Ayan Mukhopadhyay, Baiting Luo |  |
| 229 |  |  [Modeling Complex System Dynamics with Flow Matching Across Time and Conditions](https://openreview.net/forum?id=hwnObmOTrV) |  | 0 | Modeling the dynamics of complex real-world systems from temporal snapshot data is crucial for understanding phenomena such as gene regulation, climate change, and financial market fluctuations. Researchers have recently proposed a few methods based either on the Schroedinger Bridge or Flow... | Anne Biton, Aviv Regev, Charlotte Bunne, Edward De Brouwer, JanChristian Huetter, Kelvin Y. Chen, Martin Rohbeck, Romain Lopez |  |
| 230 |  |  [Joint Reward and Policy Learning with Demonstrations and Human Feedback Improves Alignment](https://openreview.net/forum?id=VCbqXtS5YY) |  | 0 | Aligning to human preferences and/or intentions is an important requirement for contemporary foundation models. To ensure alignment, popular approaches such as reinforcement learning with human feedback (RLHF) break down the task into three stages: (i) a model is computed with supervised... | Alfredo García, Chenliang Li, Dongyeop Kang, Jiaxiang Li, Mingyi Hong, Siliang Zeng, Zeyi Liao |  |
| 231 |  |  [Counterfactual Realizability](https://openreview.net/forum?id=uuriavczkL) |  | 0 | It is commonly believed that, in a real-world environment, samples can only be drawn from observational and interventional distributions, corresponding to Layers 1 and 2 of the \*Pearl Causal Hierarchy\*. Layer 3, representing counterfactual distributions, is believed to be inaccessible by... | Arvind Raghavan, Elias Bareinboim |  |
| 232 |  |  [Robustness Reprogramming for Representation Learning](https://openreview.net/forum?id=SuH5SdOXpe) |  | 0 | This work tackles an intriguing and fundamental open challenge in representation learning: Given a well-trained deep learning model, can it be reprogrammed to enhance its robustness against adversarial or noisy input perturbations without altering its parameters? To explore this, we revisit the... | Hamid Krim, MohamadAli Torkamani, Xiaorui Liu, Zhichao Hou |  |
| 233 |  |  [LayerDAG: A Layerwise Autoregressive Diffusion Model for Directed Acyclic Graph Generation](https://openreview.net/forum?id=kam84eEmub) |  | 0 | Directed acyclic graphs (DAGs) serve as crucial data representations in domains such as hardware synthesis and compiler/program optimization for computing systems. DAG generative models facilitate the creation of synthetic DAGs, which can be used for benchmarking computing systems while preserving... | Changhai Man, Eli Chien, Mufei Li, Pan Li, Srinivas, Tushar Krishna, Viraj Shitole, Ying Zhang, Zhaodong Wang |  |
| 234 |  |  [DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life](https://openreview.net/forum?id=PGhiPGBf47) |  | 0 | As users increasingly seek guidance from LLMs for decision-making in daily life, many of these decisions are not clear-cut and depend significantly on the personal values and ethical standards of people. We present DailyDilemmas, a dataset of 1,360 moral dilemmas encountered in everyday life. Each... | Liwei Jiang, Yejin Choi, Yu Ying Chiu |  |
| 235 |  |  [Learning-Augmented Frequent Directions](https://openreview.net/forum?id=WcZLG8XxhD) |  | 0 | An influential paper of Hsu et al. (ICLR'19) introduced the study of learning-augmented streaming algorithms in the context of frequency estimation. A fundamental problem in the streaming literature, the goal of frequency estimation is to approximate the number of occurrences of items appearing in... | Anders Aamand, Hao Wu, Justin Y. Chen, Sandeep Silwal, Siddharth Gollapudi |  |
| 236 |  |  [No Need to Talk: Asynchronous Mixture of Language Models](https://openreview.net/forum?id=pHOH8FVrTp) |  | 0 | We introduce SMALLTALK LM, an innovative method for training a mixture of language models in an almost asynchronous manner. Each model of the mixture specializes in distinct parts of the data distribution, without the need of high-bandwidth communication between the nodes training each model. At... | Anastasiia Filippova, Angelos Katharopoulos, David Grangier, Ronan Collobert |  |
| 237 |  |  [Estimating the Probabilities of Rare Outputs in Language Models](https://openreview.net/forum?id=DC8bsa9bzY) |  | 0 | We consider the problem of \*low probability estimation\*: given a machine learning model and a formally-specified input distribution, how can we estimate the probability of a binary property of the model's output, even when that probability is too small to estimate by random sampling? This problem... | Gabriel Wu, Jacob Hilton |  |
| 238 |  |  [Quality Measures for Dynamic Graph Generative Models](https://openreview.net/forum?id=8bjspmAMBk) |  | 0 | Deep generative models have recently achieved significant success in modeling graph data, including dynamic graphs, where topology and features evolve over time. However, unlike in vision and natural language domains, evaluating generative models for dynamic graphs is challenging due to the... | Filippo Simini, Henry Hoffmann, Rebecca Willett, Ryien Hosseini, Venkatram Vishwanath |  |
| 239 |  |  [Correlated Proxies: A New Definition and Improved Mitigation for Reward Hacking](https://openreview.net/forum?id=msEr27EejF) |  | 0 | Because it is difficult to precisely specify complex objectives, reinforcement learning policies are often optimized using proxy reward functions that only approximate the true goal. However, optimizing proxy rewards frequently leads to reward hacking: the optimized reward function ceases to be a... | Anca D. Dragan, Cassidy Laidlaw, Shivam Singhal |  |
| 240 |  |  [Holistically Evaluating the Environmental Impact of Creating Language Models](https://openreview.net/forum?id=04qx93Viwj) |  | 0 | As the performance of artificial intelligence systems has dramatically increased, so too has the environmental impact of creating these systems. While many model developers release estimates of the power consumption and carbon emissions from the final training runs for their latest models, there is... | Clara Na, Emma Strubell, Jacob Morrison, Jared Fernandez, Jesse Dodge, Tim Dettmers |  |
| 241 |  |  [Mixture-of-Agents Enhances Large Language Model Capabilities](https://openreview.net/forum?id=h0ZfDIrj7T) |  | 0 | Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal, we propose a new... | Ben Athiwaratkun, Ce Zhang, James Zou, Jue Wang, Junlin Wang |  |
| 242 |  |  [Answer, Assemble, Ace: Understanding How LMs Answer Multiple Choice Questions](https://openreview.net/forum?id=6NNA0MxhCH) |  | 0 | Multiple-choice question answering (MCQA) is a key competence of performant transformer language models that is tested by mainstream benchmarks. However, recent evidence shows that models can have quite a range of performance, particularly when the task format is diversified slightly (such as by... | Ashish Sabharwal, Hannaneh Hajishirzi, Oyvind Tafjord, Sarah Wiegreffe, Yonatan Belinkov |  |
| 243 |  |  [Provable Uncertainty Decomposition via Higher-Order Calibration](https://openreview.net/forum?id=TId1SHe8JG) |  | 0 | We give a principled method for decomposing the predictive uncertainty of a model into aleatoric and epistemic components with explicit semantics relating them to the real-world data distribution. While many works in the literature have proposed such decompositions, they lack the type of formal... | Aravind Gollakota, Charlotte Peale, Gustaf Ahdritz, Parikshit Gopalan, Udi Wieder |  |
| 244 |  |  [Sparse components distinguish visual pathways & their alignment to neural networks](https://openreview.net/forum?id=IqHeDe2lbl) |  | 0 | The ventral, dorsal, and lateral streams in high-level human visual cortex are implicated in distinct functional processes. Yet, deep neural networks (DNNs) trained on a single task model the entire visual system surprisingly well, hinting at common computational principles across these pathways.... | Ammar I Marvi, Meenakshi Khosla, Nancy Kanwisher |  |
| 245 |  |  [Reducing Hallucinations in Large Vision-Language Models via Latent Space Steering](https://openreview.net/forum?id=LBl7Hez0fF) |  | 0 | Hallucination poses a challenge to the deployment of large vision-language models (LVLMs) in applications. Unlike in large language models (LLMs), hallucination in LVLMs often arises from misalignments between visual inputs and textual outputs. This paper investigates the underlying mechanisms of... | Haotian Ye, James Zou, Sheng Liu |  |
| 246 |  |  [Mitigating Memorization in Language Models](https://openreview.net/forum?id=MGKDBuyv4p) |  | 0 | Language models (LMs) can “memorize” information, i.e., encode training data in their weights in such a way that inference-time queries can lead to verbatim regurgitation of that data. This ability to extract training data can be problematic, for example, when data are private or sensitive. In this... | Arham Mushtaq Khan, Aswathy Ajith, Caleb Geniesse, Ian T. Foster, Kyle Chard, Mansi Sakarvadia, Michael W. Mahoney, Nathaniel C. Hudson, Yaoqing Yang |  |
| 247 |  |  [Effective post-training embedding compression via temperature control in contrastive training](https://openreview.net/forum?id=szRmEM8Kx5) |  | 0 | Fixed-size learned representations (dense representations, or embeddings) are widely used in many machine learning applications across language, vision or speech modalities. This paper investigates the role of the temperature parameter in contrastive training for text embeddings. We shed light on... | Anna Currey, Corey D. Barrett, Georgiana Dinu, Miguel Romero Calvo, Xing Niu, Yi Xiang |  |
| 248 |  |  [TopoNets: High performing vision and language models with brain-like topography](https://openreview.net/forum?id=THqWPzL00e) |  | 0 | Neurons in the brain are organized such that nearby cells tend to share similar functions. AI models lack this organization, and past efforts to introduce topography have often led to trade-offs between topography and task performance. In this work, we present \*TopoLoss\*, a new loss function that... | Mainak Deb, Mayukh Deb, N. Apurva Ratan Murty |  |
| 249 |  |  [INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge](https://openreview.net/forum?id=k3gCieTXeY) |  | 0 | The performance differential of large language models (LLM) between languages hinders their effective deployment in many regions, inhibiting the potential economic and societal value of generative AI tools in many communities. However, the development of functional LLMs in many languages (i.e.,... | Abraham Diress, Aditya Kumar Dalmia, Alfonso Amayuelas, Angelika Romanou, Anna Sotnikova, Antoine Bosselut, Arshia Soltani Moakhar, Ayush Kumar Tarun, Azmine Toushik Wasi, Azril Hafizi Amirudin, Bardia Soltani Moakhar, Börje F. Karlsson, Christopher Klamm, Daniel Fernando Erazo Florez, Daniil Dzenhaliou, Danylo Boiko, Debjit Paul, Dominik Krzeminski, Drishti Sharma, Eldar Khalilov, Esther Ploeger, Fabian Farestam, Fajri Koto, Gabriel Adriano de Melo, Gal Cohen, Imanol Schlag, Jebish Purbey, Jekaterina Novikova, Jenny Chim, Joel Niklaus, Johan Samir ObandoCeron, Joseph Marvin Imperial, Maral Jabbarishiviari, Marjana Prifti Skenduli, Marzieh Fadaee, Michael Chang, Micol Altomare, Mike Zhang, Mohamed A. Haggag, Negar Foroutan, Perttu Isotalo, Ran Tamir, Rishabh Maheshwary, Roshan Santhosh, Sara Hooker, Sara Rydell, Selvan Sunitha Ravi, Serhan Yilmaz, Sharad Duwal, Shayekh Bin Islam, Shivalika Singh, Snegha A, Sree Harsha Nelaturu, Swati Rajwal, Syrielle Montariol, Thenuka Ovin Weerasinghe, Viraat Aryabumi, Yiyang Nan, Zeming Chen |  |
| 250 |  |  [Deep Learning Alternatives Of The Kolmogorov Superposition Theorem](https://openreview.net/forum?id=SyVPiehSbg) |  | 0 | This paper explores alternative formulations of the Kolmogorov Superposition Theorem (KST) as a foundation for neural network design. The original KST formulation, while mathematically elegant, presents practical challenges due to its limited insight into the structure of inner and outer functions... | Leonardo Ferreira Guilhoto, Paris Perdikaris |  |
| 251 |  |  [Vision Language Models are In-Context Value Learners](https://openreview.net/forum?id=friHAl5ofG) |  | 0 | Predicting temporal progress from visual trajectories is important for intelligent robots that can learn, adapt, and improve. However, learning such progress estimator, or temporal value function, across different tasks and domains requires both a large amount of diverse data and methods which can... | Chuyuan Fu, Danny Driess, Dhruv Shah, Dinesh Jayaraman, Dorsa Sadigh, Fei Xia, Jacky Liang, Joey Hejna, Osbert Bastani, Peng Xu, Sean Kirmani, Ted Xiao, Tingnan Zhang, Wenhao Yu, Yecheng Jason Ma, Zhuo Xu |  |
| 252 |  |  [Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding](https://openreview.net/forum?id=Tv36j85SqR) |  | 0 | Neural compression has brought tremendous progress in designing lossy compressors with good rate-distortion (RD) performance at low complexity. Thus far, neural compression design involves transforming the source to a latent vector, which is then rounded to integers and entropy coded. While this... | Eric Lei, Hamed Hassani, Shirin Saeedi Bidokhti |  |
| 253 |  |  [Wasserstein Distances, Neuronal Entanglement, and Sparsity](https://openreview.net/forum?id=cnKhHxN3xj) |  | 0 | Disentangling polysemantic neurons is at the core of many current approaches to interpretability of large language models. Here we attempt to study how disentanglement can be used to understand performance, particularly under weight sparsity, a leading post-training optimization technique. We... | Dan Alistarh, Ilia Markov, Linghao Kong, Nir Shavit, Shashata Sawmya |  |
| 254 |  |  [Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations](https://openreview.net/forum?id=4ub9gpx9xw) |  | 0 | Large language models (LLMs) are capable of generating \*plausible\* explanations of how they arrived at an answer to a question. However, these explanations can misrepresent the model's "reasoning" process, i.e., they can be \*unfaithful\*. This, in turn, can lead to over-trust and misuse. We... | Emre Kiciman, John V. Guttag, Katie Matton, Robert Osazuwa Ness |  |
| 255 |  |  [Implicit Bias of Mirror Flow for Shallow Neural Networks in Univariate Regression](https://openreview.net/forum?id=IF0Q9KY3p2) |  | 0 | We examine the implicit bias of mirror flow in least squares error regression with wide and shallow neural networks. For a broad class of potential functions, we show that mirror flow exhibits lazy training and has the same implicit bias as ordinary gradient flow when the network width tends to... | Guido Montúfar, Shuang Liang |  |
| 256 |  |  [WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild](https://openreview.net/forum?id=MKEHCx25xp) |  | 0 | We introduce WildBench, an automated evaluation framework designed to benchmark large language models (LLMs) using challenging, real-world user queries. WildBench consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs. For automated evaluation with... | Abhilasha Ravichander, Bill Yuchen Lin, Khyathi Raghavi Chandu, Nouha Dziri, Ronan Le Bras, Valentina Pyatkin, Yejin Choi, Yuntian Deng |  |
| 257 |  |  [Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators and Directionally Aligned Perturbations](https://openreview.net/forum?id=ywFOSIT9ik) |  | 0 | In this paper, we explore the two-point zeroth-order gradient estimator and identify the distribution of random perturbations that minimizes the estimator's asymptotic variance as the perturbation stepsize tends to zero. We formulate it as a constrained functional optimization problem over the... | Heng Huang, Shaocong Ma |  |
| 258 |  |  [Adaptive Batch Size for Privately Finding Second-Order Stationary Points](https://openreview.net/forum?id=ikkvC1UnnE) |  | 0 | There is a gap between finding a first-order stationary point (FOSP) and a second-order stationary point (SOSP) under differential privacy constraints, and it remains unclear whether privately finding an SOSP is more challenging than finding an FOSP. Specifically, Ganesh et al. (2023) claimed that... | Daogao Liu, Kunal Talwar |  |
| 259 |  |  [Adversarial Perturbations Cannot Reliably Protect Artists From Generative AI](https://openreview.net/forum?id=yfW1x7uBS5) |  | 0 | Artists are increasingly concerned about advancements in image generation models that can closely replicate their unique artistic styles. In response, several protection tools against style mimicry have been developed that incorporate small adversarial perturbations into artworks published online.... | Florian Tramèr, Javier Rando, Nicholas Carlini, Robert Hönig |  |
| 260 |  |  [Better autoregressive regression with LLMs via regression-aware fine-tuning](https://openreview.net/forum?id=xGs7Ch3Vyo) |  | 0 | Decoder-based large language models (LLMs) have proven highly versatile, with remarkable successes even on problems ostensibly removed from traditional language generation. One such example is solving regression problems, where the targets are real numbers rather than textual tokens. A common... | Aditya Krishna Menon, Felix Yu, Harikrishna Narasimhan, Michal Lukasik, Sanjiv Kumar, YinWen Chang, Zhao Meng |  |
| 261 |  |  [Analyzing Neural Scaling Laws in Two-Layer Networks with Power-Law Data Spectra](https://openreview.net/forum?id=wFD16gwpze) |  | 0 | Neural scaling laws describe how the performance of deep neural networks scales with key factors such as training data size, model complexity, and training time, often following power-law behaviors over multiple orders of magnitude. Despite their empirical observation, the theoretical understanding... | Bernd Rosenow, Roman Worschech |  |
| 262 |  |  [Differential learning kinetics govern the transition from memorization to generalization during in-context learning](https://openreview.net/forum?id=INyi7qUdjZ) |  | 0 | Transformers exhibit in-context learning (ICL): the ability to use novel information presented in the context without additional weight updates. Recent work shows that ICL emerges when models are trained on a sufficiently diverse set of tasks and the transition from memorization to generalization... | Alex Nguyen, Gautam Reddy |  |
| 263 |  |  [Multi-session, multi-task neural decoding from distinct cell-types and brain regions](https://openreview.net/forum?id=IuU0wcO0mo) |  | 0 | Recent work has shown that scale is important for improved brain decoding, with more data leading to greater decoding accuracy. However, large-scale decoding across many different datasets is challenging because neural circuits are heterogeneous---each brain region contains a unique mix of cellular... | Blake Aaron Richards, Eva L. Dyer, Ian Jarratt Knight, Krystal Xuejing Pan, Mehdi Azabou, Vinam Arora |  |
| 264 |  |  [Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models](https://openreview.net/forum?id=vQhn4wrQ6j) |  | 0 | Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. In this work, we present a model merging methodology that addresses the difficulty of fine-tuning Large Language Models (LLMs) for target tasks in... | Benjamin Muller, Bing Liu, Hongjiang Lv, Lucas Bandarkar, Nayan Singhal, Pritish Yuvraj, Rui Hou |  |
| 265 |  |  [Bayesian Optimization via Continual Variational Last Layer Training](https://openreview.net/forum?id=1jcnvghayD) |  | 0 | Gaussian Processes (GPs) are widely seen as the state-of-the-art surrogate models for Bayesian optimization (BO) due to their ability to model uncertainty and their performance on tasks where correlations are easily captured (such as those defined by Euclidean metrics) and their ability to be... | James Harrison, Jasper Snoek, John Willes, Mikkel Jordahn, Paul Brunzema, Sebastian Trimpe |  |
| 266 |  |  [Bayesian Optimization of Antibodies Informed by a Generative Model of Evolving Sequences](https://openreview.net/forum?id=E48QvQppIN) |  | 0 | To build effective therapeutics, biologists iteratively mutate antibody sequences to improve binding and stability. Proposed mutations can be informed by previous measurements or by learning from large antibody databases to predict only typical antibodies. Unfortunately, the space of typical... | Alan Nawzad Amin, Andrew Gordon Wilson, Aniruddh Raghu, Calvin McCarter, Hunter Elliott, Nate Gruver, Peyton Greenside, Yilun Kuang, Yucen Lily Li |  |
| 267 |  |  [Meta-Dynamical State Space Models for Integrative Neural Data Analysis](https://openreview.net/forum?id=SRpq5OBpED) |  | 0 | Learning shared structure across environments facilitates rapid learning and adaptive behavior in neural systems. This has been widely demonstrated and applied in machine learning to train models that are capable of generalizing to novel settings. However, there has been limited work exploiting the... | Ayesha Vermani, Hyungju Jeon, Il Memming Park, Josue Nassar, Matthew Dowling |  |
| 268 |  |  [AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs](https://openreview.net/forum?id=bhK7U37VW8) |  | 0 | Jailbreak attacks serve as essential red-teaming tools, proactively assessing whether LLMs can behave responsibly and safely in adversarial environments. Despite diverse strategies (e.g., cipher, low-resource language, persuasions, and so on) that have been proposed and shown success, these... | Bo Li, Chaowei Xiao, G. Edward Suh, Huan Sun, Patrick McDaniel, Peiran Li, Somesh Jha, Xiaogeng Liu, Yevgeniy Vorobeychik, Zhuoqing Mao |  |
| 269 |  |  [Towards Automated Knowledge Integration From Human-Interpretable Representations](https://openreview.net/forum?id=NTHMw8S1Ow) |  | 0 | A significant challenge in machine learning, particularly in noisy and low-data environments, lies in effectively incorporating inductive biases to enhance data efficiency and robustness. Despite the success of informed machine learning methods, designing algorithms with explicit inductive biases... | Kasia Kobalczyk, Mihaela van der Schaar |  |
| 270 |  |  [BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval](https://openreview.net/forum?id=ykuc5q381b) |  | 0 | Existing retrieval benchmarks primarily consist of information-seeking queries (e.g., aggregated questions from search engines) where keyword or semantic-based retrieval is usually sufficient. However, many complex real-world queries require in-depth reasoning to identify relevant documents that go... | Danqi Chen, Haisu Liu, Hanyu Wang, Hongjin Su, Howard Yen, Jinsung Yoon, Mengzhou Xia, Michael Tang, Niklas Muennighoff, Quan Shi, Ruoxi Sun, Sercan Ö. Arik, Tao Yu, Weijia Shi, Zachary S. Siegel |  |
| 271 |  |  [Reinforcement Learning for Control of Non-Markovian Cellular Population Dynamics](https://openreview.net/forum?id=dsHpulHpOK) |  | 0 | Many organisms and cell types, from bacteria to cancer cells, exhibit a remarkable ability to adapt to fluctuating environments. Additionally, cells can leverage memory of past environments to better survive previously-encountered stressors. From a control perspective, this adaptability poses... | Jacob Adamczyk, Josiah C. Kratz |  |
| 272 |  |  [Online Reinforcement Learning in Non-Stationary Context-Driven Environments](https://openreview.net/forum?id=l6QnSQizmN) |  | 0 | We study online reinforcement learning (RL) in non-stationary environments, where a time-varying exogenous context process affects the environment dynamics. Online RL is challenging in such environments due to "catastrophic forgetting" (CF). The agent tends to forget prior knowledge as it trains on... | Arash NasrEsfahany, Malte Schwarzkopf, Mohammad Alizadeh, Pouya Hamadanian, Siddhartha Sen |  |
| 273 |  |  [CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models](https://openreview.net/forum?id=IUmj2dw5se) |  | 0 | As Large Language Models (LLMs) are increasingly deployed to handle various natural language processing (NLP) tasks, concerns regarding the potential negative societal impacts of LLM-generated content have also arisen. To evaluate the biases exhibited by LLMs, researchers have recently proposed a... | Jundong Li, Peng Wang, Song Wang, Tong Zhou, Yushun Dong, Zhen Tan |  |
| 274 |  |  [TabWak: A Watermark for Tabular Diffusion Models](https://openreview.net/forum?id=71pur4y8gs) |  | 0 | Synthetic data offers alternatives for data augmentation and sharing. Till date, it remains unknown how to use watermarking techniques to trace and audit synthetic tables generated by tabular diffusion models to mitigate potential misuses. In this paper, we design TabWak, the first watermarking... | Chaoyi Zhu, Cornelis Bos, Jeroen M. Galjaard, Jiayi Tang, Lydia Y. Chen, PinYu Chen, Robert Birke |  |
| 275 |  |  [Active Task Disambiguation with LLMs](https://openreview.net/forum?id=JAMxRSXLFz) |  | 0 | Despite the impressive performance of large language models (LLMs) across various benchmarks, their ability to address ambiguously specified problems—frequent in real-world interactions—remains underexplored. To address this gap, we introduce a formal definition of task ambiguity and frame the... | Kasia Kobalczyk, Mihaela van der Schaar, Nicolás Astorga, Tennison Liu |  |
| 276 |  |  [Surprising Effectiveness of pretraining Ternary Language Model at Scale](https://openreview.net/forum?id=TJo6aQb7mK) |  | 0 | Rapid advancements in GPU computational power has outpaced memory capacity and bandwidth growth, creating bottlenecks in Large Language Model (LLM) inference. Post-training quantization is the leading method for addressing memory-related bottlenecks in LLM inference, but it suffers from significant... | Aaryan Bhagat, Arnab Kumar Mondal, Ayush Kaushal, Irina Rish, Tejas Pandey, Tejas Vaidhya |  |
| 277 |  |  [Generating Freeform Endoskeletal Robots](https://openreview.net/forum?id=awvJBtB2op) |  | 0 | The automatic design of embodied agents (e.g. robots) has existed for 31 years and is experiencing a renaissance of interest in the literature. To date however, the field has remained narrowly focused on two kinds of anatomically simple robots: (1) fully rigid, jointed bodies; and (2) fully soft,... | Lingji Kong, Muhan Li, Sam Kriegman |  |
| 278 |  |  [Understanding Factual Recall in Transformers via Associative Memories](https://openreview.net/forum?id=hwSmPOAmhk) |  | 0 | Large language models have demonstrated an impressive ability to perform factual recall. Prior work has found that transformers trained on factual recall tasks can store information at a rate proportional to their parameter count. In our work, we show that shallow transformers can use a combination... | Alberto Bietti, Eshaan Nichani, Jason D. Lee |  |
| 279 |  |  [Nesterov acceleration in benignly non-convex landscapes](https://openreview.net/forum?id=YwJkv2YqBq) |  | 0 | While momentum-based optimization algorithms are commonly used in the notoriously non-convex optimization problems of deep learning, their analysis has historically been restricted to the convex and strongly convex setting. In this article, we partially close this gap between theory and practice... | Kanan Gupta, Stephan Wojtowytsch |  |
| 280 |  |  [AnalogGenie: A Generative Engine for Automatic Discovery of Analog Circuit Topologies](https://openreview.net/forum?id=jCPak79Kev) |  | 0 | The massive and large-scale design of foundational semiconductor integrated circuits (ICs) is crucial to sustaining the advancement of many emerging and future technologies, such as generative AI, 5G/6G, and quantum computing. Excitingly, recent studies have shown the great capabilities of... | Jian Gao, Junyi Yang, Weidong Cao, Xuan Zhang |  |
| 281 |  |  [Probabilistic Geometric Principal Component Analysis with application to neural data](https://openreview.net/forum?id=mkDam1xIzW) |  | 0 | Dimensionality reduction is critical across various domains of science including neuroscience. Probabilistic Principal Component Analysis (PPCA) is a prominent dimensionality reduction method that provides a probabilistic approach unlike the deterministic approach of PCA and serves as a connection... | HanLin Hsieh, Maryam Shanechi |  |
| 282 |  |  [DataEnvGym: Data Generation Agents in Teacher Environments with Student Feedback](https://openreview.net/forum?id=00SnKBGTsz) |  | 0 | The process of creating training data to teach models is currently driven by humans, who manually analyze model weaknesses and plan how to create data that improves a student model. Recent approaches using large language models (LLMs) as annotators reduce human annotation effort, but still require... | Elias StengelEskin, Jaemin Cho, Mohit Bansal, Zaid Khan |  |
| 283 |  |  [Correcting the Mythos of KL-Regularization: Direct Alignment without Overoptimization via Chi-Squared Preference Optimization](https://openreview.net/forum?id=hXm0Wu2U9K) |  | 0 | Language model alignment methods such as reinforcement learning from human feedback (RLHF) have led to impressive advances in language model capabilities, but are limited by a widely observed phenomenon known as \*overoptimization\*, where the quality of the language model degrades over the course... | Akshay Krishnamurthy, Audrey Huang, Dylan J. Foster, Jason D. Lee, Tengyang Xie, Wen Sun, Wenhao Zhan |  |
| 284 |  |  [Exact Certification of (Graph) Neural Networks Against Label Poisoning](https://openreview.net/forum?id=d9aWa875kj) |  | 0 | Machine learning models are highly vulnerable to label flipping, i.e., the adversarial modification (poisoning) of training labels to compromise performance. Thus, deriving robustness certificates is important to guarantee that test predictions remain unaffected and to understand worst-case... | Debarghya Ghoshdastidar, Lukas Gosch, Mahalakshmi Sabanayagam, Stephan Günnemann |  |
| 285 |  |  [Better Instruction-Following Through Minimum Bayes Risk](https://openreview.net/forum?id=7xCSK9BLPy) |  | 0 | General-purpose LLM judges capable of human-level evaluation provide not only a scalable and accurate way of evaluating instruction-following LLMs but also new avenues for supervising and improving their performance. One promising way of leveraging LLM judges for supervision is through Minimum... | Amanda Bertsch, Graham Neubig, Ian Wu, Patrick Fernandes, Seungone Kim, Sina Khoshfetrat Pakazad |  |
| 286 |  |  [Union-over-Intersections: Object Detection beyond Winner-Takes-All](https://openreview.net/forum?id=HqLHY4TzGj) |  | 0 | This paper revisits the problem of predicting box locations in object detection architectures. Typically, each box proposal or box query aims to directly maximize the intersection-over-union score with the ground truth, followed by a winner-takes-all non-maximum suppression where only the highest... | Aritra Bhowmik, Cees G. M. Snoek, Martin R. Oswald, Pascal Mettes |  |
| 287 |  |  [MamKO: Mamba-based Koopman operator for modeling and predictive control](https://openreview.net/forum?id=hNjCVVm0EQ) |  | 0 | The Koopman theory, which enables the transformation of nonlinear systems into linear representations, is a powerful and efficient tool to model and control nonlinear systems. However, the ability of the Koopman operator to model complex systems, particularly time-varying systems, is limited by the... | Minghao Han, Xunyuan Yin, Zhaoyang Li |  |
| 288 |  |  [LoRA3D: Low-Rank Self-Calibration of 3D Geometric Foundation models](https://openreview.net/forum?id=LSp4KBhAom) |  | 0 | Emerging 3D geometric foundation models, such as DUSt3R, offer a promising approach for in-the-wild 3D vision tasks. However, due to the high-dimensional nature of the problem space and scarcity of high-quality 3D data, these pre-trained models still struggle to generalize to many challenging... | Boris Ivanovic, Boyi Li, Danfei Xu, Heng Yang, Marco Pavone, Yue Wang, Ziqi Lu |  |
| 289 |  |  [Weighted Point Set Embedding for Multimodal Contrastive Learning Toward Optimal Similarity Metric](https://openreview.net/forum?id=uSz2K30RRd) |  | 0 | In typical multimodal contrastive learning, such as CLIP, encoders produce one point in the latent representation space for each input. However, one-point representation has difficulty in capturing the relationship and the similarity structure of a huge amount of instances in the real world. For... | ChiehHsin Lai, Naoki Murata, Taiji Suzuki, Toshimitsu Uesaka, Yuhta Takida, Yuki Mitsufuji |  |
| 290 |  |  [Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar Book?](https://openreview.net/forum?id=aMBSY2ebPw) |  | 0 | Extremely low-resource (XLR) languages lack substantial corpora for training NLP models, motivating the use of all available resources such as dictionaries and grammar books. Machine Translation from One Book (Tanzer et al., 2024) suggests that prompting long-context LLMs with one grammar book... | Christof Monz, David Stap, Di Wu, Khalil Sima'an, Seth Aycock |  |
| 291 |  |  [Retri3D: 3D Neural Graphics Representation Retrieval](https://openreview.net/forum?id=q3EbOXb4y1) |  | 0 | Learnable 3D Neural Graphics Representations (3DNGR) have emerged as promising 3D representations for reconstructing 3D scenes from 2D images. Numerous works, including Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and their variants, have significantly enhanced the quality of these... | Daniel Kwan, Jean Sebastien Dandurand, Nandita Vijaykumar, Nilesh A. Ahuja, Nilesh Jain, Ruofan Liang, Selvakumar Panneer, Xi Yan, Yushi Guan, Yuxuan Zhang |  |
| 292 |  |  [Test-time Adaptation for Cross-modal Retrieval with Query Shift](https://openreview.net/forum?id=BmG88rONaU) |  | 0 | The success of most existing cross-modal retrieval methods heavily relies on the assumption that the given queries follow the same distribution of the source domain. However, such an assumption is easily violated in real-world scenarios due to the complexity and diversity of queries, thus leading... | Haobin Li, Mouxing Yang, Peng Hu, Qianjun Zhang, Xi Peng, XitingLiu |  |
| 293 |  |  [Universal generalization guarantees for Wasserstein distributionally robust models](https://openreview.net/forum?id=0h6v4SpLCY) |  | 0 | Distributionally robust optimization has emerged as an attractive way to train robust machine learning models, capturing data uncertainty and distribution shifts. Recent statistical analyses have proved that generalization guarantees of robust models based on the Wasserstein distance have... | Jérôme Malick, Tam Le |  |
| 294 |  |  [Conformal Prediction Sets Can Cause Disparate Impact](https://openreview.net/forum?id=fZK6AQXlUU) |  | 0 | Conformal prediction is a statistically rigorous method for quantifying uncertainty in models by having them output sets of predictions, with larger sets indicating more uncertainty. However, prediction sets are not inherently actionable; many applications require a single output to act on, not... | Bhargava Kumar, Jesse C. Cresswell, Mouloud Belbahri, Yi Sui |  |
| 295 |  |  [In Search of Forgotten Domain Generalization](https://openreview.net/forum?id=Fk3eod9aaD) |  | 0 | Out-of-Domain (OOD) generalization is the ability of a model trained on one or more domains to generalize to unseen domains. In the ImageNet era of computer vision, evaluation sets for measuring a model's OOD performance were designed to be strictly OOD with respect to style. However, the emergence... | Attila Juhos, Evgenia Rusak, Matthias Bethge, Prasanna Mayilvahanan, Roland S. Zimmermann, Thaddäus Wiedemer, Wieland Brendel |  |
| 296 |  |  [TOP-ERL: Transformer-based Off-Policy Episodic Reinforcement Learning](https://openreview.net/forum?id=N4NhVN30ph) |  | 0 | This work introduces Transformer-based Off-Policy Episodic Reinforcement Learning (TOP-ERL), a novel algorithm that enables off-policy updates in the ERL framework. In ERL, policies predict entire action trajectories over multiple time steps instead of single actions at every time step. These... | Dong Tian, Ge Li, Gerhard Neumann, Hongyi Zhou, Rudolf Lioutikov, Xinkai Jiang |  |
| 297 |  |  [On the Expressiveness of Rational ReLU Neural Networks With Bounded Depth](https://openreview.net/forum?id=uREg3OHjLL) |  | 0 | To confirm that the expressive power of ReLU neural networks grows with their depth, the function $F_n = \max (0,x_1,\ldots,x_n )$ has been considered in the literature. A conjecture by Hertrich, Basu, Di Summa, and Skutella [NeurIPS 2021] states that any ReLU network that exactly represents $F_n$... | Christopher Hojny, Gennadiy Averkov, Maximilian Merkert |  |
| 298 |  |  [On the Optimization and Generalization of Two-layer Transformers with Sign Gradient Descent](https://openreview.net/forum?id=97rOQDPmk2) |  | 0 | The Adam optimizer is widely used for transformer optimization in practice, which makes understanding the underlying optimization mechanisms an important problem. However, due to the Adam's complexity, theoretical analysis of how it optimizes transformers remains a challenging task. Fortunately,... | Andi Han, Bingrui Li, Jianfei Chen, Jun Zhu, Taiji Suzuki, Wei Huang, Zhanpeng Zhou |  |
| 299 |  |  [Emergent Orientation Maps - - Mechanisms, Coding Efficiency and Robustness](https://openreview.net/forum?id=rySLejeB1k) |  | 0 | Extensive experimental studies have shown that in lower mammals, neuronal orientation preference in the primary visual cortex is organized in disordered "salt-and-pepper" organizations. In contrast, higher-order mammals display a continuous variation in orientation preference, forming pinwheel-like... | Anna Wang Roe, Haixin Zhong, Haoyu Wang, Mingyi Huang, Rubin Wang, Wei P. Dai, Yuchao Huang, Yuguo Yu |  |
| 300 |  |  [How Much is Unseen Depends Chiefly on Information About the Seen](https://openreview.net/forum?id=uqWM9hBDAE) |  | 0 | The \*missing mass\* refers to the proportion of data points in an \*unknown\* population of classifier inputs that belong to classes \*not\* present in the classifier's training data, which is assumed to be a random sample from that unknown population. We find that \*in expectation\* the missing... | Marcel Boehme, Seongmin Lee |  |
| 301 |  |  [Harnessing Diversity for Important Data Selection in Pretraining Large Language Models](https://openreview.net/forum?id=bMC1t7eLRc) |  | 0 | Data selection is of great significance in pretraining large language models, given the variation in quality within the large-scale available training corpora. To achieve this, researchers are currently investigating the use of data influence to measure the importance of data instances, $i.e.,$ a... | Chengliang Chai, Chi Zhang, Conghui He, Guoren Wang, Huaping Zhong, Jiantao Qiu, Ju Fan, Kuan Zhang, Lei Cao, Rui Wang, Tianyi Bai, Xinlin Zhuang, Ye Yuan |  |
| 302 |  |  [Adaptive Gradient Clipping for Robust Federated Learning](https://openreview.net/forum?id=03OkC0LKDD) |  | 0 | Robust federated learning aims to maintain reliable performance despite the presence of adversarial or misbehaving workers. While state-of-the-art (SOTA) robust distributed gradient descent (Robust-DGD) methods were proven theoretically optimal, their empirical success has often relied on... | Ahmed Jellouli, Geovani Rizk, John Stephan, Nirupam Gupta, Rachid Guerraoui, Youssef Allouah |  |
| 303 |  |  [Imputation for prediction: beware of diminishing returns](https://openreview.net/forum?id=D1Y2XFgsPI) |  | 0 | Missing values are prevalent across various fields, posing challenges for training and deploying predictive models. In this context, imputation is a common practice, driven by the hope that accurate imputations will enhance predictions. However, recent theoretical and empirical studies indicate... | Gaël Varoquaux, Marine Le Morvan |  |
| 304 |  |  [Joint Gradient Balancing for Data Ordering in Finite-Sum Multi-Objective Optimization](https://openreview.net/forum?id=rdAbEn5DZt) |  | 0 | In finite-sum optimization problems, the sample orders for parameter updates can significantly influence the convergence rate of optimization algorithms. While numerous sample ordering techniques have been proposed in the context of single-objective optimization, the problem of sample ordering in... | Hansi Yang, James T. Kwok |  |
| 305 |  |  [Learning from negative feedback, or positive feedback or both](https://openreview.net/forum?id=4FVGowGzQb) |  | 0 | Existing preference optimization methods often assume scenarios where paired preference feedback (preferred/positive vs. dis-preferred/negative examples) is available. This requirement limits their applicability in scenarios where only unpaired feedback—for example, either positive or negative— is... | Abbas Abdolmaleki, Bilal Piot, Bobak Shahriari, Jonas Buchli, Jost Tobias Springenberg, Junhyuk Oh, Martin A. Riedmiller, Michael Bloesch, Nicolas Heess, Rishabh Joshi, Thomas Lampe, Tim Hertweck |  |
| 306 |  |  [CausalRivers - Scaling up benchmarking of causal discovery for real-world time-series](https://openreview.net/forum?id=wmV4cIbgl6) |  | 0 | Causal discovery, or identifying causal relationships from observational data, is a notoriously challenging task, with numerous methods proposed to tackle it. Despite this, in-the-wild evaluation of these methods is still lacking, as works frequently rely on synthetic data evaluation and sparse... | Gideon Stein, Jan Blunk, Joachim Denzler, Maha Shadaydeh, Niklas Penzel |  |
| 307 |  |  [Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes](https://openreview.net/forum?id=Nx4PMtJ1ER) |  | 0 | Inferring the causal structure underlying stochastic dynamical systems from observational data holds great promise in domains ranging from science and health to finance. Such processes can often be accurately modeled via stochastic differential equations (SDEs), which naturally imply causal... | Cecilia Casolo, Cristopher Salvi, Emilio Ferrucci, Georg Manten, Niki Kilbertus, Søren Wengel Mogensen |  |
| 308 |  |  [Provably Reliable Conformal Prediction Sets in the Presence of Data Poisoning](https://openreview.net/forum?id=ofuLWn8DFZ) |  | 0 | Conformal prediction provides model-agnostic and distribution-free uncertainty quantification through prediction sets that are guaranteed to include the ground truth with any user-specified probability. Yet, conformal prediction is not reliable under poisoning attacks where adversaries manipulate... | Stephan Günnemann, Yan Scholten |  |
| 309 |  |  [Weak-to-Strong Preference Optimization: Stealing Reward from Weak Aligned Model](https://openreview.net/forum?id=f7KxfUrRSb) |  | 0 | Aligning language models (LMs) with human preferences has become a key area of research, enabling these models to meet diverse user needs better. Inspired by weak-to-strong generalization, where a strong LM fine-tuned on labels generated by a weaker model can consistently outperform its weak... | Pengfei Liu, Rui Wang, Wenhong Zhu, Xiaofeng Wang, Zhiwei He |  |
| 310 |  |  [Benchmarking Predictive Coding Networks - Made Simple](https://openreview.net/forum?id=sahQq2sH5x) |  | 0 | In this work, we tackle the problems of efficiency and scalability for predictive coding networks (PCNs) in machine learning. To do so, we propose a library that focuses on performance and simplicity, and use it to implement a large set of standard benchmarks for the community to use for their... | Amine M'Charrak, Bayar Menzat, Chang Qi, Cornelius Emde, Gaspard Oliviers, Luca Pinchetti, Mufeng Tang, Oleh Lokshyn, Rafal Bogacz, Simon Frieder, Thomas Lukasiewicz, Tommaso Salvatori |  |
| 311 |  |  [LiFT: Learning to Fine-Tune via Bayesian Parameter Efficient Meta Fine-Tuning](https://openreview.net/forum?id=7nyJBVCTGQ) |  | 0 | We tackle the problem of parameter-efficient fine-tuning (PEFT) of a pre-trained large deep model on many different but related tasks. Instead of the simple but strong baseline strategy of task-wise independent fine-tuning, we aim to meta-learn the core shared information that can be used for... | Minyoung Kim, Timothy M. Hospedales |  |
| 312 |  |  [Lightweight Neural App Control](https://openreview.net/forum?id=BL4WBIfyrz) |  | 0 | This paper introduces a novel mobile phone control architecture, Lightweight Multi-modal App Control (LiMAC), for efficient interactions and control across various Android apps. LiMAC takes as input a textual goal and a sequence of past mobile observations, such as screenshots and corresponding UI... | Filippos Christianos, Georgios Papoudakis, Jianye Hao, Jun Wang, Kun Shao, Thomas Coste |  |
| 313 |  |  [Progressive Compositionality in Text-to-Image Generative Models](https://openreview.net/forum?id=S85PP4xjFD) |  | 0 | Despite the impressive text-to-image (T2I) synthesis capabilities of diffusion models, they often struggle to understand compositional relationships between objects and attributes, especially in complex settings. Existing approaches through building compositional architectures or generating... | Linghao Jin, Paul Pu Liang, Xiaofeng Liu, Xu Han |  |
| 314 |  |  [On Quantizing Neural Representation for Variable-Rate Video Coding](https://openreview.net/forum?id=44cMlQSreK) |  | 0 | This work introduces NeuroQuant, a novel post-training quantization (PTQ) approach tailored to non-generalized Implicit Neural Representations for variable-rate Video Coding (INR-VC). Unlike existing methods that require extensive weight retraining for each target bitrate, we hypothesize that... | Hanfei Li, Junqi Shi, Ming Lu, Qi Zhao, Tong Chen, Zhan Ma, Zhujia Chen |  |
| 315 |  |  [Can Watermarked LLMs be Identified by Users via Crafted Prompts?](https://openreview.net/forum?id=ujpAYpFDEA) |  | 0 | Text watermarking for Large Language Models (LLMs) has made significant progress in detecting LLM outputs and preventing misuse. Current watermarking techniques offer high detectability, minimal impact on text quality, and robustness to text editing. However, current researches lack investigation... | Aiwei Liu, Leyi Pan, Liancheng Fang, Lijie Wen, Philip S. Yu, Sheng Guan, Xuming Hu, Yifei Zhang, Yiming Liu |  |
| 316 |  |  [Temporal Heterogeneous Graph Generation with Privacy, Utility, and Efficiency](https://openreview.net/forum?id=tj5xJInWty) |  | 0 | Nowadays, temporal heterogeneous graphs attract much research and industrial attention for building the next-generation Relational Deep Learning models and applications, due to their informative structures and features. While providing timely and precise services like personalized recommendations... | Dongqi Fu, Hanghang Tong, Jingrui He, Ross Maciejewski, Xinyu He |  |
| 317 |  |  [Attention with Markov: A Curious Case of Single-layer Transformers](https://openreview.net/forum?id=SqZ0KY4qBD) |  | 0 | Attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages. To deepen our understanding of their sequential modeling capabilities, there is a growing interest in using Markov input processes to study them. A key finding is that when... | Adway Girish, Alliot Nagle, Ashok Vardhan Makkuva, Hyeji Kim, Marco Bondaschi, Martin Jaggi, Michael Gastpar |  |
| 318 |  |  [The Computational Complexity of Circuit Discovery for Inner Interpretability](https://openreview.net/forum?id=QogcGNXJVw) |  | 0 | Many proposed applications of neural networks in machine learning, cognitive/brain science, and society hinge on the feasibility of inner interpretability via circuit discovery. This calls for empirical and theoretical explorations of viable algorithmic options. Despite advances in the design and... | Federico Adolfi, Martina G. Vilas, Todd Wareham |  |
| 319 |  |  [Topological Schrödinger Bridge Matching](https://openreview.net/forum?id=WzCEiBILHu) |  | 0 | Given two boundary distributions, the \emph{Schrödinger Bridge} (SB) problem seeks the “most likely” random evolution between them with respect to a reference process. It has revealed rich connections to recent machine learning methods for generative modeling and distribution matching. While these... | Maosheng Yang |  |
| 320 |  |  [ThunderKittens: Simple, Fast, and Adorable Kernels](https://openreview.net/forum?id=0fJfVOSUra) |  | 0 | The challenge of mapping AI architectures to GPU hardware is creating a critical bottleneck in AI progress. Despite substantial efforts, hand-written custom kernels fail to meet their theoretical performance thresholds, even on well-established operations like linear attention. The diverse... | Aaryan Singhal, Arjun Parthasarathy, Benjamin Frederick Spector, Christopher Ré, Daniel Y. Fu, Simran Arora |  |
| 321 |  |  [gRNAde: Geometric Deep Learning for 3D RNA inverse design](https://openreview.net/forum?id=lvw3UgeVxS) |  | 0 | Computational RNA design tasks are often posed as inverse problems, where sequences are designed based on adopting a single desired secondary structure without considering 3D conformational diversity. We introduce gRNAde, a geometric RNA design pipeline operating on 3D RNA backbones to design... | Alex Morehead, Arian Rokkum Jamasb, Chaitanya K. Joshi, Charles Harris, Pietro Lio, Ramón Viñas Torné, Rishabh Anand, Simon V. Mathis |  |
| 322 |  |  [A Second-Order Perspective on Model Compositionality and Incremental Learning](https://openreview.net/forum?id=OZVTqoli2N) |  | 0 | The fine-tuning of deep pre-trained models has revealed compositional properties, with multiple specialized modules that can be arbitrarily composed into a single, multi-task model. However, identifying the conditions that promote compositionality remains an open issue, with recent efforts... | Angelo Porrello, Lorenzo Bonicelli, Monica Millunzi, Pietro Buzzega, Rita Cucchiara, Simone Calderara |  |
| 323 |  |  [Lean-STaR: Learning to Interleave Thinking and Proving](https://openreview.net/forum?id=SOWZ59UyNc) |  | 0 | Traditional language model-based theorem proving assumes that by training on a sufficient amount of formal proof data, a model will learn to prove theorems. Our key observation is that a wealth of informal information that is not present in formal proofs can be useful for learning to prove... | Haohan Lin, Sean Welleck, Yiming Yang, Zhiqing Sun |  |
| 324 |  |  [Spectral Compressive Imaging via Unmixing-driven Subspace Diffusion Refinement](https://openreview.net/forum?id=Q150eWkQ4I) |  | 0 | Spectral Compressive Imaging (SCI) reconstruction is inherently ill-posed because a single observation admits multiple plausible reconstructions. Traditional deterministic methods struggle to effectively recover high-frequency details. Although diffusion models offer promising solutions to this... | Benteng Sun, Haijin Zeng, Jingyong Su, Yong Xu, Yongyong Chen |  |
| 325 |  |  [CBQ: Cross-Block Quantization for Large Language Models](https://openreview.net/forum?id=eW4yh6HKz4) |  | 0 | Post-training quantization (PTQ) has played a pivotal role in compressing large language models (LLMs) at ultra-low costs. Although current PTQ methods have achieved promising results by addressing outliers and employing layer- or block-wise loss optimization techniques, they still suffer from... | Baoqun Yin, Hanting Chen, Jie Hu, Wei Li, Xiaoyu Liu, Xin Ding, Yehui Tang, Yun Zhang, Yunhe Wang, Zhijun Tu, Zhiwei Xiong |  |
| 326 |  |  [Iterative Label Refinement Matters More than Preference Optimization under Weak Supervision](https://openreview.net/forum?id=q5EZ7gKcnW) |  | 0 | Language model (LM) post-training relies on two stages of human supervision: task demonstrations for supervised finetuning (SFT), followed by preference comparisons for reinforcement learning from human feedback (RLHF). As LMs become more capable, the tasks they are given become harder to... | Cassidy Laidlaw, Jacob Steinhardt, Yaowen Ye |  |
| 327 |  |  [uniINF: Best-of-Both-Worlds Algorithm for Parameter-Free Heavy-Tailed MABs](https://openreview.net/forum?id=2pNLknCTvG) |  | 0 | In this paper, we present a novel algorithm, \`uniINF\`, for the Heavy-Tailed Multi-Armed Bandits (HTMAB) problem, demonstrating robustness and adaptability in both stochastic and adversarial environments. Unlike the stochastic MAB setting where loss distributions are stationary with time, our... | Jiatai Huang, Longbo Huang, Yan Dai, Yu Chen |  |
| 328 |  |  [Exploring Local Memorization in Diffusion Models via Bright Ending Attention](https://openreview.net/forum?id=p4cLtzk4oe) |  | 0 | Text-to-image diffusion models have achieved unprecedented proficiency in generating realistic images. However, their inherent tendency to memorize and replicate training data during inference raises significant concerns, including potential copyright infringement. In response, various methods have... | Chang Xu, Chen Chen, Daochang Liu, Mubarak Shah |  |
| 329 |  |  [EmbedLLM: Learning Compact Representations of Large Language Models](https://openreview.net/forum?id=Fs9EabmQrJ) |  | 0 | With hundreds of thousands of language models available on Huggingface today, efficiently evaluating and utilizing these models across various downstream tasks has become increasingly critical. Many existing methods repeatedly learn task-specific representations of Large Language Models (LLMs),... | Andrew Li, Jiantao Jiao, Kannan Ramchandran, Richard Zhuang, Tianhao Wu, Zhaojin Wen |  |
| 330 |  |  [InverseBench: Benchmarking Plug-and-Play Diffusion Priors for Inverse Problems in Physical Sciences](https://openreview.net/forum?id=U3PBITXNG6) |  | 0 | Plug-and-play diffusion priors (PnPDP) have emerged as a promising research direction for solving inverse problems. However, current studies primarily focus on natural image restoration, leaving the performance of these algorithms in scientific inverse problems largely unexplored. To address this... | Austin Wang, Berthy Feng, Bingliang Zhang, Caifeng Zou, Hongkai Zheng, Katherine L. Bouman, Nikola Borislavov Kovachki, Wenda Chu, Yisong Yue, Yu Sun, Zachary E. Ross, Zihui Wu |  |
| 331 |  |  [What Does It Mean to Be a Transformer? Insights from a Theoretical Hessian Analysis](https://openreview.net/forum?id=3ddi7Uss2A) |  | 0 | The Transformer architecture has inarguably revolutionized deep learning, overtaking classical architectures like multi-layer perceptions (MLPs) and convolutional neural networks (CNNs). At its core, the attention block differs in form and functionality from most other architectural components in... | Felix Dangel, Sidak Pal Singh, Weronika Ormaniec |  |
| 332 |  |  [Beyond Squared Error: Exploring Loss Design for Enhanced Training of Generative Flow Networks](https://openreview.net/forum?id=4NTrco82W0) |  | 0 | Generative Flow Networks (GFlowNets) are a novel class of generative models designed to sample from unnormalized distributions and have found applications in various important tasks, attracting great research interest in their training algorithms. In general, GFlowNets are trained by fitting the... | Longbo Huang, Rui Hu, Yifan Zhang, Zhuoran Li |  |
| 333 |  |  [Adam Exploits ℓ∞-geometry of Loss Landscape via Coordinate-wise Adaptivity](https://openreview.net/forum?id=PUnD86UEK5) |  | 0 | Adam outperforms SGD when training language models. Yet this advantage is not well-understood theoretically -- previous convergence analysis for Adam and SGD mainly focuses on the number of steps $T$ and is already minimax-optimal in non-convex cases, which are both $\widetilde{O}(T^{-1/4})$. In... | Mohamad Amin Mohamadi, Shuo Xie, Zhiyuan Li |  |
| 334 |  |  [CBGBench: Fill in the Blank of Protein-Molecule Complex Binding Graph](https://openreview.net/forum?id=mOpNrrV2zH) |  | 0 | Structure-based drug design (SBDD) aims to generate potential drugs that can bind to a target protein and is greatly expedited by the aid of AI techniques in generative models. However, a lack of systematic understanding persists due to the diverse settings, complex implementation, difficult... | Cheng Tan, Guojiang Zhao, Haitao Lin, Lirong Wu, Odin Zhang, Stan Z. Li, Yufei Huang, Zhifeng Gao, Zicheng Liu |  |
| 335 |  |  [Fair Clustering in the Sliding Window Model](https://openreview.net/forum?id=VGQugiuCQs) |  | 0 | We study streaming algorithms for proportionally fair clustering, a notion originally suggested by Chierichetti et al. (2017), in the sliding window model. We show that although there exist efficient streaming algorithms in the insertion-only model, surprisingly no algorithm can achieve finite... | Qiaoyuan Yang, Samson Zhou, Shaofeng H.C. Jiang, Vincent CohenAddad, Yubo Zhang |  |
| 336 |  |  [NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models](https://openreview.net/forum?id=lgsyLSsDRe) |  | 0 | Decoder-only large language model (LLM)-based embedding models are beginning to outperform BERT or T5-based embedding models in general-purpose text embedding tasks, including dense vector-based retrieval. In this work, we introduce the NV-Embed model, incorporating architectural designs, training... | Bryan Catanzaro, Chankyu Lee, Jonathan Raiman, Mengyao Xu, Mohammad Shoeybi, Rajarshi Roy, Wei Ping |  |
| 337 |  |  [Differentiation and Specialization of Attention Heads via the Refined Local Learning Coefficient](https://openreview.net/forum?id=SUc1UOWndp) |  | 0 | We introduce refined variants of the Local Learning Coefficient (LLC), a measure of model complexity grounded in singular learning theory, to study the development of internal structure in transformer language models during training. By applying these refined LLCs (rLLCs) to individual components... | Daniel Murfet, George Wang, Jesse Hoogland, Stan van Wingerden, Zach Furman |  |
| 338 |  |  [VLMaterial: Procedural Material Generation with Large Vision-Language Models](https://openreview.net/forum?id=wHebuIb6IH) |  | 0 | Procedural materials, represented as functional node graphs, are ubiquitous in computer graphics for photorealistic material appearance design. They allow users to perform intuitive and precise editing to achieve desired visual appearances. However, creating a procedural material given an input... | Armando SolarLezama, Beichen Li, Bernd Bickel, Changxi Zheng, Liang Shi, Rundi Wu, Wojciech Matusik |  |
| 339 |  |  [To Trust or Not to Trust? Enhancing Large Language Models' Situated Faithfulness to External Contexts](https://openreview.net/forum?id=K2jOacHUlO) |  | 0 | Large Language Models (LLMs) are often augmented with external contexts, such as those used in retrieval-augmented generation (RAG). However, these contexts can be inaccurate or intentionally misleading, leading to conflicts with the model’s internal knowledge. We argue that robust LLMs should... | Bhuwan Dhingra, Hongyi Cai, Sanxing Chen, Yukun Huang |  |
| 340 |  |  [Representative Guidance: Diffusion Model Sampling with Coherence](https://openreview.net/forum?id=gWgaypDBs8) |  | 0 | The diffusion sampling process faces a persistent challenge stemming from its incoherence, attributable to varying noise directions across different timesteps. Our Representative Guidance (RepG) offers a new perspective to address this issue by reformulating the sampling process with a coherent... | AnhDung Dinh, Chang Xu, Daochang Liu |  |
| 341 |  |  [AIR-BENCH 2024: A Safety Benchmark based on Regulation and Policies Specified Risk Categories](https://openreview.net/forum?id=UVnD9Ze6mF) |  | 0 | Foundation models (FMs) provide societal benefits but also amplify risks. Governments, companies, and researchers have proposed regulatory frameworks, acceptable use policies, and safety benchmarks in response. However, existing public benchmarks often define safety categories based on previous... | Andy Zhou, Bo Li, Dawn Song, Jeffrey Ziwei Tan, Kevin Klyman, Minzhou Pan, Percy Liang, Ruoxi Jia, Yi Zeng, Yifan Mai, Yu Yang, Yuheng Tu |  |
| 342 |  |  [DARE the Extreme: Revisiting Delta-Parameter Pruning For Fine-Tuned Models](https://openreview.net/forum?id=avSocG0oFA) |  | 0 | Storing open-source fine-tuned models separately introduces redundancy and increases response times in applications utilizing multiple models. Delta-parameter pruning (DPP), particularly the random drop and rescale (DARE) method proposed by Yu et al., addresses this by pruning the majority of delta... | Christos Thrampoulidis, Minghui Chen, Vala Vakilian, Wenlong Deng, Xiaoxiao Li, Yize Zhao |  |
| 343 |  |  [CREIMBO: Cross-Regional Ensemble Interactions in Multi-view Brain Observations](https://openreview.net/forum?id=28abpUEICJ) |  | 0 | Modern recordings of neural activity provide diverse observations of neurons across brain areas, behavioral conditions, and subjects; presenting an exciting opportunity to reveal the fundamentals of brain-wide dynamics. Current analysis methods, however, often fail to fully harness the richness of... | Adam Shabti Charles, Noga Mudrik, Oliver Rübel, Ryan Ly |  |
| 344 |  |  [Learning vector fields of differential equations on manifolds with geometrically constrained operator-valued kernels](https://openreview.net/forum?id=OwpLQrpdwE) |  | 0 | We address the problem of learning ordinary differential equations (ODEs) on manifolds. Existing machine learning methods, particularly those using neural networks, often struggle with high computational demands. To overcome this issue, we introduce a geometrically constrained operator-valued... | Daning Huang, Hanyang He, John Harlim, Yan Li |  |
| 345 |  |  [Budgeted Online Continual Learning by Adaptive Layer Freezing and Frequency-based Sampling](https://openreview.net/forum?id=dOAkHmsjRX) |  | 0 | The majority of online continual learning (CL) advocates single-epoch training and imposes restrictions on the size of replay memory. However, single-epoch training would incur a different amount of computations per CL algorithm, and the additional storage cost to store logit or model in addition... | Hyunseo Koh, Jonghyun Choi, Minhyuk Seo |  |
| 346 |  |  [Control-oriented Clustering of Visual Latent Representation](https://openreview.net/forum?id=pPQPQ7Yd58) |  | 0 | We initiate a study of the geometry of the visual representation space ---the information channel from the vision encoder to the action decoder--- in an image-based control pipeline learned from behavior cloning. Inspired by the phenomenon of \*neural collapse\* (NC) in image classification, we... | Han Qi, Haocheng Yin, Heng Yang |  |
| 347 |  |  [When do GFlowNets learn the right distribution?](https://openreview.net/forum?id=9GsgCUJtic) |  | 0 | Generative Flow Networks (GFlowNets) are an emerging class of sampling methods for distributions over discrete and compositional objects, e.g., graphs. In spite of their remarkable success in problems such as drug discovery and phylogenetic inference, the question of when and whether GFlowNets... | Amauri H. Souza, Diego Mesquita, Eliezer de Souza da Silva, Rodrigo Barreto Alves, Samuel Kaski, Tiago da Silva, Vikas Garg |  |
| 348 |  |  [R2-Guard: Robust Reasoning Enabled LLM Guardrail via Knowledge-Enhanced Logical Reasoning](https://openreview.net/forum?id=CkgKSqZbuC) |  | 0 | As large language models (LLMs) become increasingly prevalent across various applications, it is critical to establish safety guardrails to moderate input/output content of LLMs and ensure compliance with safety policies. Existing guardrail models, such as OpenAI Mod and LlamaGuard, treat various... | Bo Li, Mintong Kang |  |
| 349 |  |  [NetFormer: An interpretable model for recovering dynamical connectivity in neuronal population dynamics](https://openreview.net/forum?id=bcTjW5kS4W) |  | 0 | Neuronal dynamics are highly nonlinear and nonstationary. Traditional methods for extracting the underlying network structure from neuronal activity recordings mainly concentrate on modeling static connectivity, without accounting for key nonstationary aspects of biological neural systems, such as... | Eric Todd SheaBrown, Hao Wang, Lu Mi, Trung Le, Uygar Sümbül, Wuwei Zhang, Ziyu Lu |  |
| 350 |  |  [Revisiting Random Walks for Learning on Graphs](https://openreview.net/forum?id=SG1R2H3fa1) |  | 0 | We revisit a simple model class for machine learning on graphs, where a random walk on a graph produces a machine-readable record, and this record is processed by a deep neural network to directly make vertex-level or graph-level predictions. We call these stochastic machines random walk neural... | Ayhan Suleymanzade, Jinwoo Kim, Olga Zaghen, Seunghoon Hong, Youngmin Ryou |  |
| 351 |  |  [DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured LLM Inference](https://openreview.net/forum?id=2c7pfOqu9k) |  | 0 | Large language models (LLMs) are increasingly employed for complex tasks that process multiple generation calls in a tree structure with shared prefixes of tokens, including few-shot prompting, multi-step reasoning, speculative decoding, etc. However, existing inference systems for tree-based... | Binhang Yuan, Jiaxuan You, Jinwei Yao, Kaiqi Chen, Kexun Zhang, Tao Lin, Zeke Wang |  |
| 352 |  |  [How to Find the Exact Pareto Front for Multi-Objective MDPs?](https://openreview.net/forum?id=S4dItvpvAv) |  | 0 | Multi-Objective Markov Decision Processes (MO-MDPs) are receiving increasing attention, as real-world decision-making problems often involve conflicting objectives that cannot be addressed by a single-objective MDP. The Pareto front identifies the set of policies that cannot be dominated, providing... | Ness B. Shroff, Peizhong Ju, Yining Li |  |
| 353 |  |  [Enhancing Learning with Label Differential Privacy by Vector Approximation](https://openreview.net/forum?id=IwPXYk6BV9) |  | 0 | Label differential privacy (DP) is a framework that protects the privacy of labels in training datasets, while the feature vectors are public. Existing approaches protect the privacy of labels by flipping them randomly, and then train a model to make the output approximate the privatized label.... | Jiafei Wu, Le Sun, Li Shen, Puning Zhao, Qingming Li, Rongfei Fan, Zhe Liu, Zhikun Zhang |  |
| 354 |  |  [VisualPredicator: Learning Abstract World Models with Neuro-Symbolic Predicates for Robot Planning](https://openreview.net/forum?id=QOfswj7hij) |  | 0 | Broadly intelligent agents should form task-specific abstractions that selectively expose the essential elements of a task, while abstracting away the complexity of the raw sensorimotor space. In this work, we present Neuro-Symbolic Predicates, a first-order abstraction language that combines the... | Adrian Weller, Hao Tang, Joshua B. Tenenbaum, João F. Henriques, Kevin Ellis, Nishanth Kumar, Tom Silver, Yichao Liang |  |
| 355 |  |  [Multi-Draft Speculative Sampling: Canonical Decomposition and Theoretical Limits](https://openreview.net/forum?id=N1L5TgtkAw) |  | 0 | We consider multi-draft speculative sampling, where the proposal sequences are sampled independently from different draft models. At each step, a token-level draft selection scheme takes a list of valid tokens as input and produces an output token whose distribution matches that of the target... | Arash Behboodi, Ashish J. Khisti, Christos Louizos, Hassan Dbouk, MohammadReza Ebrahimi, Roland Memisevic |  |
| 356 |  |  [First-Person Fairness in Chatbots](https://openreview.net/forum?id=TlAdgeoDTo) |  | 0 | Evaluating chatbot fairness is crucial given their rapid proliferation, yet typical chatbot tasks (e.g., resume writing, entertainment) diverge from the institutional decision-making tasks (e.g., resume screening) which have traditionally been central to discussion of algorithmic fairness. The... | Adam Tauman Kalai, Alex Beutel, AnnaLuisa Brakman, David G. Robinson, Johannes Heidecke, Keren Gu, Lilian Weng, Meghan Shah, Pamela Mishkin, Tyna Eloundou |  |
| 357 |  |  [Can Large Language Models Understand Symbolic Graphics Programs?](https://openreview.net/forum?id=Yk87CwhBDx) |  | 0 | Against the backdrop of enthusiasm for large language models (LLMs), there is a growing need to scientifically assess their capabilities and shortcomings. This is nontrivial in part because it is difficult to find tasks which the models have not encountered during training. Utilizing symbolic... | Adrian Weller, Bernhard Schölkopf, Haiwen Feng, Joshua B. Tenenbaum, Katherine M. Collins, Michael J. Black, Tim Z. Xiao, Weiyang Liu, Zeju Qiu, Zhen Liu |  |
| 358 |  |  [Token Statistics Transformer: Linear-Time Attention via Variational Rate Reduction](https://openreview.net/forum?id=lXRDQsiP2v) |  | 0 | The attention operator is arguably the key distinguishing factor of transformer architectures, which have demonstrated state-of-the-art performance on a variety of tasks. However, transformer attention operators often impose a significant computational burden, with the computational complexity... | Benjamin David Haeffele, Druv Pai, Jingyuan Zhang, Tianjiao Ding, Weida Wang, Yaodong Yu, Yi Ma, Yifu Lu, Ziyang Wu |  |
| 359 |  |  [Nonlinear Sequence Embedding by Monotone Variational Inequality](https://openreview.net/forum?id=U834XHJuqk) |  | 0 | In the wild, we often encounter collections of sequential data such as electrocardiograms, motion capture, genomes, and natural language, and sequences may be multichannel or symbolic with nonlinear dynamics. We introduce a method to learn low-dimensional representations of nonlinear sequence and... | Jonathan Yuyang Zhou, Yao Xie |  |
| 360 |  |  [X-ALMA: Plug & Play Modules and Adaptive Rejection for Quality Translation at Scale](https://openreview.net/forum?id=csbf1p8xUq) |  | 0 | Large language models (LLMs) have achieved remarkable success across various NLP tasks with a focus on English due to English-centric pre-training and limited multilingual data. In this work, we focus on the problem of translation, and while some multilingual LLMs claim to support for hundreds of... | Akiko Eriguchi, Haoran Xu, Hieu Hoang, Huda Khayrallah, Kenton Murray, Philipp Koehn |  |
| 361 |  |  [Not All LLM-Generated Data Are Equal: Rethinking Data Weighting in Text Classification](https://openreview.net/forum?id=oI5tZaWkF9) |  | 0 | Synthetic data augmentation via Large Language Models (LLMs) allows researchers to leverage additional training data, thus enhancing the performance of downstream tasks, especially when real-world data is scarce. However, the generated data can deviate from the real-world data, and this... | HsunYu Kuo, PuJen Cheng, WeiYun Ma, YinHsiang Liao, YuChieh Chao |  |
| 362 |  |  [Systems with Switching Causal Relations: A Meta-Causal Perspective](https://openreview.net/forum?id=J9VogDTa1W) |  | 0 | Most work on causality in machine learning assumes that causal relationships are driven by a constant underlying process. However, the flexibility of agents' actions or tipping points in the environmental process can change the qualitative dynamics of the system. As a result, new causal... | Devendra Singh Dhami, Florian Peter Busch, Jonas Seng, Kristian Kersting, Moritz Willig, Tim Nelson Tobiasch |  |
| 363 |  |  [Multi-Robot Motion Planning with Diffusion Models](https://openreview.net/forum?id=AUCYptvAf3) |  | 0 | Diffusion models have recently been successfully applied to a wide range of robotics applications for learning complex multi-modal behaviors from data. However, prior works have mostly been confined to single-robot and small-scale environments due to the high sample complexity of learning... | Itamar Mishani, Jiaoyang Li, Maxim Likhachev, Shivam Vats, Yorai Shaoul |  |
| 364 |  |  [Graph Neural Networks Can (Often) Count Substructures](https://openreview.net/forum?id=sZQRUrvLn4) |  | 0 | Message passing graph neural networks (GNNs) are known to have limited expressive power in their ability to distinguish some non-isomorphic graphs. Because of this, it is well known that they are unable to detect or count arbitrary graph substructures (i.e., solving the subgraph isomorphism... | Karsten M. Borgwardt, Paolo Pellizzoni, Till Hendrik Schulz |  |
| 365 |  |  [Towards hyperparameter-free optimization with differential privacy](https://openreview.net/forum?id=2kGKsyhtvh) |  | 0 | Differential privacy (DP) is a privacy-preserving paradigm that protects the training data when training deep learning models. Critically, the performance of models is determined by the training hyperparameters, especially those of the learning rate schedule, thus requiring fine-grained... | Ruixuan Liu, Zhiqi Bu |  |
| 366 |  |  [AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials](https://openreview.net/forum?id=EEgYUccwsV) |  | 0 | Graphical User Interface (GUI) agents hold great potential for automating complex tasks across diverse digital environments, from web applications to desktop software. However, the development of such agents is hindered by the lack of high-quality, multi-step trajectory data required for effective... | Caiming Xiong, Dunjie Lu, Junli Wang, Tao Yu, Yiheng Xu, Yuchen Mao, Zekun Wang, Zhennan Shen |  |
| 367 |  |  [Bilinear MLPs enable weight-based mechanistic interpretability](https://openreview.net/forum?id=gI0kPklUKS) |  | 0 | A mechanistic understanding of how MLPs do computation in deep neural net- works remains elusive. Current interpretability work can extract features from hidden activations over an input dataset but generally cannot explain how MLP weights construct features. One challenge is that element-wise... | Alice Rigg, José Oramas, Lee Sharkey, Michael T. Pearce, Thomas Dooms |  |
| 368 |  |  [Towards a Unified and Verified Understanding of Group-Operation Networks](https://openreview.net/forum?id=8xxEBAtD7y) |  | 0 | A recent line of work in mechanistic interpretability has focused on reverse-engineering the computation performed by neural networks trained on the binary operation of finite groups. We investigate the internals of one-hidden-layer neural networks trained on this task, revealing previously... | Jacob Drori, Jason Gross, Louis Jaburi, Wilson Wu |  |
| 369 |  |  [Approximation algorithms for combinatorial optimization with predictions](https://openreview.net/forum?id=AEFVa6VMu1) |  | 0 | We initiate a systematic study of utilizing predictions to improve over approximation guarantees of classic algorithms, without increasing the running time. We propose a generic method for a wide class of optimization problems that ask to select a feasible subset of input items of minimal (or... | Adam Polak, Antonios Antoniadis, Marek Eliás, Moritz Venzin |  |
| 370 |  |  [Bayesian Experimental Design Via Contrastive Diffusions](https://openreview.net/forum?id=h8yg0hT96f) |  | 0 | Bayesian Optimal Experimental Design (BOED) is a powerful tool to reduce the cost of running a sequence of experiments. When based on the Expected Information Gain (EIG), design optimization corresponds to the maximization of some intractable expected \*contrast\* between prior and posterior... | Christophe Heinkelé, Florence Forbes, Jacopo Iollo, Pierre Alliez |  |
| 371 |  |  [MorphoDiff: Cellular Morphology Painting with Diffusion Models](https://openreview.net/forum?id=PstM8YfhvI) |  | 0 | Understanding cellular responses to external stimuli is critical for parsing biological mechanisms and advancing therapeutic development. High-content image-based assays provide a cost-effective approach to examine cellular phenotypes induced by diverse interventions, which offers valuable insights... | Anne E. Carpenter, Benjamin HaibeKains, Beth A. Cimini, Bo Wang, Esteban Miglietta, Jun Ma, Le Liu, Zeinab Navidi |  |
| 372 |  |  [Uncertainty Modeling in Graph Neural Networks via Stochastic Differential Equations](https://openreview.net/forum?id=TYSQYx9vwd) |  | 0 | We propose a novel Stochastic Differential Equation (SDE) framework to address the problem of learning uncertainty-aware representations for graph-structured data. While Graph Neural Ordinary Differential Equations (GNODEs) have shown promise in learning node representations, they lack the ability... | Felix L. Opolka, José Miguel HernándezLobato, Pietro Lio, Richard Bergna, Sergio CalvoOrdoñez |  |
| 373 |  |  [Simplifying Deep Temporal Difference Learning](https://openreview.net/forum?id=7IzeL0kflu) |  | 0 | $Q$-learning played a foundational role in the field reinforcement learning (RL). However, TD algorithms with off-policy data, such as $Q$-learning, or nonlinear function approximation like deep neural networks require several additional tricks to stabilise training, primarily a large replay buffer... | Bartomeu Pou, Benjamin Ellis, Ivan Masmitja, Jakob Nicolaus Foerster, Mario Martin, Matteo Gallici, Mattie Fellows |  |
| 374 |  |  [The Superposition of Diffusion Models Using the Itô Density Estimator](https://openreview.net/forum?id=2o58Mbqkd2) |  | 0 | The Cambrian explosion of easily accessible pre-trained diffusion models suggests a demand for methods that combine multiple different pre-trained diffusion models without incurring the significant computational burden of re-training a larger combined model. In this paper, we cast the problem of... | Alexander Tong, Joey Bose, Kirill Neklyudov, Lazar Atanackovic, Marta Skreta |  |
| 375 |  |  [Differentiable Integer Linear Programming](https://openreview.net/forum?id=FPfCUJTsCn) |  | 0 | Machine learning (ML) techniques have shown great potential in generating high-quality solutions for integer linear programs (ILPs). However, existing methods typically rely on a \*supervised learning\* paradigm, leading to (1) \*expensive training cost\* due to repeated invocations of traditional... | Bin Li, Fangzhou Zhu, Feng Wu, Jianye Hao, Jie Wang, Xijun Li, Zijie Geng |  |
| 376 |  |  [Streaming Algorithms For ℓp Flows and ℓp Regression](https://openreview.net/forum?id=Kpjvm2mB0K) |  | 0 | We initiate the study of one-pass streaming algorithms for underdetermined $\ell_p$ linear regression problems of the form $$ \min_{\mathbf A\mathbf x = \mathbf b} \lVert\mathbf x\rVert_p \,, \qquad \text{where } \mathbf A \in \mathbb R^{n \times d} \text{ with } n \ll d \,, $$ which generalizes... | Amit Chakrabarti, David P. Woodruff, Jeffrey Jiang, Taisuke Yasuda |  |
| 377 |  |  [Generalized Principal-Agent Problem with a Learning Agent](https://openreview.net/forum?id=LqTz13JS2P) |  | 0 | Generalized principal-agent problems, including Stackelberg games, contract design, and Bayesian persuasion, are a class of economic problems where an agent best responds to a principal's committed strategy. We study repeated generalized principal-agent problems under the assumption that the... | Tao Lin, Yiling Chen |  |
| 378 |  |  [Targeted Attack Improves Protection against Unauthorized Diffusion Customization](https://openreview.net/forum?id=agHddsQhsL) |  | 0 | Diffusion models build a new milestone for image generation yet raising public concerns, for they can be fine-tuned on unauthorized images for customization. Protection based on adversarial attacks rises to encounter this unauthorized diffusion customization, by adding protective watermarks to... | Boyang Zheng, Chumeng Liang, Xiaoyu Wu |  |
| 379 |  |  [High-dimensional Analysis of Knowledge Distillation: Weak-to-Strong Generalization and Scaling Laws](https://openreview.net/forum?id=1xzqz73hvL) |  | 0 | A growing number of machine learning scenarios rely on knowledge distillation where one uses the output of a surrogate model as labels to supervise the training of a target model. In this work, we provide a sharp characterization of this process for ridgeless, high-dimensional regression, under two... | Ege Onur Taga, Halil Alperen Gozeten, Marco Mondelli, Muhammed Emrullah Ildiz, Samet Oymak |  |
| 380 |  |  [BlendRL: A Framework for Merging Symbolic and Neural Policy Learning](https://openreview.net/forum?id=60i0ksMAhd) |  | 0 | Humans can leverage both symbolic reasoning and intuitive responses. In contrast, reinforcement learning policies are typically encoded in either opaque systems like neural networks or symbolic systems that rely on predefined symbols and rules. This disjointed approach severely limits the agents’... | Devendra Singh Dhami, Hikaru Shindo, Kristian Kersting, Quentin Delfosse |  |
| 381 |  |  [Uncovering Overfitting in Large Language Model Editing](https://openreview.net/forum?id=t8qcGXaepr) |  | 0 | Knowledge editing has been proposed as an effective method for updating and correcting the internal knowledge of Large Language Models (LLMs). However, existing editing methods often struggle with complex tasks, such as multi-hop reasoning. In this paper, we identify and investigate the phenomenon... | Mengqi Zhang, Pengjie Ren, Qiang Liu, Shu Wu, Xiaotian Ye, Zhumin Chen |  |
| 382 |  |  [Advantage-Guided Distillation for Preference Alignment in Small Language Models](https://openreview.net/forum?id=xsx3Fpo3UD) |  | 0 | Alignment techniques enable Large Language Models (LLMs) to generate outputs that align with human preferences and play a crucial role in their effectiveness. However, their impact often diminishes when applied to Small Language Models (SLMs), likely due to the limited capacity of these models.... | Fanqi Wan, Jiajian Guo, Qifan Wang, Shiping Gao, Xiaojun Quan |  |
| 383 |  |  [SVBench: A Benchmark with Temporal Multi-Turn Dialogues for Streaming Video Understanding](https://openreview.net/forum?id=Hz4BYVY8YM) |  | 0 | Despite the significant advancements of Large Vision-Language Models (LVLMs) on established benchmarks, there remains a notable gap in suitable evaluation regarding their applicability in the emerging domain of long-context streaming video understanding. Current benchmarks for video understanding... | Changsheng Xu, Dizhan Xue, Fan Yang, Jiahong Wu, Shengsheng Qian, Weiming Dong, Yuhang Hu, Zemin Du, Zhenyu Yang |  |
| 384 |  |  [Knowledge Distillation with Multi-granularity Mixture of Priors for Image Super-Resolution](https://openreview.net/forum?id=cWHonXThtM) |  | 0 | Knowledge distillation (KD) is a promising yet challenging model compression approach that transmits rich learning representations from robust but resource-demanding teacher models to efficient student models. Previous methods for image super-resolution (SR) are often tailored to specific... | Bingyi Jing, Hanting Chen, Jie Hu, Shaohui Lin, Simiao Li, Wei Li, Wenjia Wang, Yun Zhang |  |
| 385 |  |  [Controlling Language and Diffusion Models by Transporting Activations](https://openreview.net/forum?id=l2zFn6TIQi) |  | 0 | The increasing capabilities of large generative models and their ever more widespread deployment have raised concerns about their reliability, safety, and potential misuse. To address these issues, recent works have proposed to control model generation by steering model activations in order to... | Arno Blaas, Luca Zappella, Marco Cuturi, Michal Klein, Nicholas Apostoloff, Pau Rodríguez, Xavier Suau |  |
| 386 |  |  [Efficient and Accurate Explanation Estimation with Distribution Compression](https://openreview.net/forum?id=LiUfN9h0Lx) |  | 0 | We discover a theoretical connection between explanation estimation and distribution compression that significantly improves the approximation of feature attributions, importance, and effects. While the exact computation of various machine learning explanations requires numerous model inferences... | Bernd Bischl, Giuseppe Casalicchio, Hubert Baniecki, Przemyslaw Biecek |  |
| 387 |  |  [Interleaved Scene Graphs for Interleaved Text-and-Image Generation Assessment](https://openreview.net/forum?id=rDLgnYLM5b) |  | 0 | Many real-world user queries (e.g. \*"How do to make egg fried rice?"\*) could benefit from systems capable of generating responses with both textual steps with accompanying images, similar to a cookbook. Models designed to generate interleaved text and images face challenges in ensuring... | Benlin Liu, Caixi Chen, Dongping Chen, Pan Zhou, Ranjay Krishna, Ruoxi Chen, Shu Pu, Yanru Wu, Yao Wan, Yue Huang, Zhaoyi Liu |  |
| 388 |  |  [ODE-based Smoothing Neural Network for Reinforcement Learning Tasks](https://openreview.net/forum?id=S5Yo6w3n3f) |  | 0 | The smoothness of control actions is a significant challenge faced by deep reinforcement learning (RL) techniques in solving optimal control problems. Existing RL-trained policies tend to produce non-smooth actions due to high-frequency input noise and unconstrained Lipschitz constants in neural... | Jingliang Duan, Liangfa Chen, Likun Wang, Shengbo Eben Li, Tong Liu, Wenxuan Wang, Xujie Song, Yinuo Wang, Yuming Yin |  |
| 389 |  |  [Learning from End User Data with Shuffled Differential Privacy over Kernel Densities](https://openreview.net/forum?id=QjSOgxJ0hp) |  | 0 | We study a setting of collecting and learning from private data distributed across end users. In the shuffled model of differential privacy, the end users partially protect their data locally before sharing it, and their data is also anonymized during its collection to enhance privacy. This model... | Tal Wagner |  |
| 390 |  |  [Biologically Constrained Barrel Cortex Model Integrates Whisker Inputs and Replicates Key Brain Network Dynamics](https://openreview.net/forum?id=UvfI4grcM7) |  | 0 | The brain's ability to transform sensory inputs into motor functions is central to neuroscience and crucial for the development of embodied intelligence. Sensory-motor integration involves complex neural circuits, diverse neuronal types, and intricate intercellular connections. Bridging the gap... | Anan Li, Dongli Hu, Jiandong Zhou, Kai Du, Tianfang Zhu |  |
| 391 |  |  [FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in Conversational LLMs](https://openreview.net/forum?id=RSGoXnS9GH) |  | 0 | The increasing deployment of large language model (LLM)-based chatbots has raised concerns regarding fairness. Fairness issues in LLMs may result in serious consequences, such as bias amplification, discrimination, and harm to minority groups. Many efforts are dedicated to evaluating and mitigating... | Ruizhe Chen, Tianxiang Hu, Zhiting Fan, Zuozhu Liu |  |
| 392 |  |  [OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text](https://openreview.net/forum?id=kwqhn2VuG4) |  | 0 | Image-text interleaved data, consisting of multiple images and texts arranged in a natural document format, aligns with the presentation paradigm of internet data and closely resembles human reading habits. Recent studies have shown that such data aids multimodal in-context learning and maintains... | Bin Wang, Bo Zhang, Chao Xu, Erfei Cui, Guanzhou Chen, Hao Tian, Jiasheng Zhou, Jiashuo Yu, Pinlong Cai, Qingyun Li, Shenglong Ye, Wei Li, Weiyun Wang, Wenhai Wang, Wenjian Zhang, Xingjian Wei, Yinan He, Zhangwei Gao, Zhe Chen, Zhenjiang Jin, et al. |  |
| 393 |  |  [MAGNet: Motif-Agnostic Generation of Molecules from Scaffolds](https://openreview.net/forum?id=5FXKgOxmb2) |  | 0 | Recent advances in machine learning for molecules exhibit great potential for facilitating drug discovery from in silico predictions. Most models for molecule generation rely on the decomposition of molecules into frequently occurring substructures (motifs), from which they generate novel... | Bastian Rieck, Fabian J. Theis, Johanna Sommer, Leon Hetzel, Stephan Günnemann |  |
| 394 |  |  [Broaden your SCOPE! Efficient Multi-turn Conversation Planning for LLMs with Semantic Space](https://openreview.net/forum?id=3cgMU3TyyE) |  | 0 | Large language models (LLMs) are used in chatbots or AI assistants to hold conversations with a human user. In such applications, the quality (e.g., user engagement, safety) of a conversation is important and can only be exactly known at the end of the conversation. To maximize its expected... | Bryan Kian Hsiang Low, ChuanSheng Foo, Xinyuan Niu, Zhiliang Chen |  |
| 395 |  |  [Spa-Bench: a comprehensive Benchmark for Smartphone Agent Evaluation](https://openreview.net/forum?id=OZbFRNhpwr) |  | 0 | Smartphone agents are increasingly important for helping users control devices efficiently, with (Multimodal) Large Language Model (MLLM)-based approaches emerging as key contenders. Fairly comparing these agents is essential but challenging, requiring a varied task scope, the integration of agents... | Bin Xie, Derek Yuen, Gongwei Chen, Jianye Hao, Jingxuan Chen, Jun Wang, Kaiwen Zhou, Kun Shao, Li Yixing, Liqiang Nie, Rui Shao, Shuai Wang, Weiwen Liu, Xurui Zhou, Yasheng Wang, Yuhao Yang, Zhihao Wu |  |
| 396 |  |  [DeepRTL: Bridging Verilog Understanding and Generation with a Unified Representation Model](https://openreview.net/forum?id=2hcfoCHKoB) |  | 0 | Recent advancements in large language models (LLMs) have shown significant potential for automating hardware description language (HDL) code generation from high-level natural language instructions. While fine-tuning has improved LLMs' performance in hardware design tasks, prior efforts have... | Changran Xu, Qiang Xu, Yi Liu, Yunhao Zhou, Zeju Li |  |
| 397 |  |  [Robust Function-Calling for On-Device Language Model via Function Masking](https://openreview.net/forum?id=yVQcr4qjD6) |  | 0 | Large language models have demonstrated impressive value in performing as autonomous agents when equipped with external tools and API calls. Nonetheless, effectively harnessing their potential for executing complex tasks crucially relies on enhancements in their function-calling capabilities. This... | Cheng Cheng, Guanyu Nie, Jiamu Zhou, Jun Wang, Jun Wang, Junwei Liao, Muning Wen, Qiqiang Lin, Qiuying Peng, Weinan Zhang, Xiaoyun Mo, Yin Zhao |  |
| 398 |  |  [IGL-Bench: Establishing the Comprehensive Benchmark for Imbalanced Graph Learning](https://openreview.net/forum?id=uTqnyF0JNR) |  | 0 | Deep graph learning has gained grand popularity over the past years due to its versatility and success in representing graph data across a wide range of domains. However, the pervasive issue of imbalanced graph data distributions, where certain parts exhibit disproportionally abundant data while... | Hao Peng, Haonan Yuan, Jianxin Li, Jiaqi Yuan, Jiawen Qin, Lyujin Xu, Pengfeng Huang, Philip S. Yu, Qingyun Sun, Xingcheng Fu, Zhaonan Wang |  |
| 399 |  |  [Learning Equivariant Non-Local Electron Density Functionals](https://openreview.net/forum?id=FhBT596F1X) |  | 0 | The accuracy of density functional theory hinges on the approximation of non-local contributions to the exchange-correlation (XC) functional. To date, machine-learned and human-designed approximations suffer from insufficient accuracy, limited scalability, or dependence on costly reference data. To... | Eike Eberhard, Nicholas Gao, Stephan Günnemann |  |
| 400 |  |  [PaRa: Personalizing Text-to-Image Diffusion via Parameter Rank Reduction](https://openreview.net/forum?id=KZgo2YQbhc) |  | 0 | Personalizing a large-scale pretrained Text-to-Image (T2I) diffusion model is chal- lenging as it typically struggles to make an appropriate trade-off between its training data distribution and the target distribution, i.e., learning a novel concept with only a few target images to achieve... | Dinh Q. Phung, Jianfei Cai, Shangyu Chen, Zizheng Pan |  |
| 401 |  |  [Near-Optimal Online Learning for Multi-Agent Submodular Coordination: Tight Approximation and Communication Efficiency](https://openreview.net/forum?id=i8dYPGdB1C) |  | 0 | Coordinating multiple agents to collaboratively maximize submodular functions in unpredictable environments is a critical task with numerous applications in machine learning, robot planning and control. The existing approaches, such as the OSG algorithm, are often hindered by their poor... | Dacheng Tao, Li Shen, Qixin Zhang, Yu Yang, Zongqi Wan |  |
| 402 |  |  [PETRA: Parallel End-to-end Training with Reversible Architectures](https://openreview.net/forum?id=0fhzSFsGUT) |  | 0 | Reversible architectures have been shown to be capable of performing on par with their non-reversible architectures, being applied in deep learning for memory savings and generative modeling. In this work, we show how reversible architectures can solve challenges in parallelizing deep model... | Edouard Oyallon, Eugene Belilovsky, Louis Fournier, Michael Eickenberg, Stéphane Rivaud, Thomas Pumir |  |
| 403 |  |  [BirdSet: A Large-Scale Dataset for Audio Classification in Avian Bioacoustics](https://openreview.net/forum?id=dRXxFEY8ZE) |  | 0 | Deep learning (DL) has greatly advanced audio classification, yet the field is limited by the scarcity of large-scale benchmark datasets that have propelled progress in other domains. While AudioSet is a pivotal step to bridge this gap as a universal-domain dataset, its restricted accessibility and... | Bernhard Sick, Christoph Scholz, Denis Huseljic, Jonas Lange, Lukas Rauch, Marek Herde, Moritz Wirth, Raphael Schwinger, René Heinrich, Stefan Kahl, Sven Tomforde |  |
| 404 |  |  [SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity Reduction](https://openreview.net/forum?id=ixMBnOhFGd) |  | 0 | Large Language Models (LLMs) have demonstrated improved generation performance by incorporating externally retrieved knowledge, a process known as retrieval-augmented generation (RAG). Despite the potential of this approach, existing studies evaluate RAG effectiveness by 1) assessing retrieval and... | Hao Liu, Hui Xiong, Jinhui Ye, Lu Dai, Yijie Xu |  |
| 405 |  |  [Physics-aligned field reconstruction with diffusion bridge](https://openreview.net/forum?id=D042vFwJAM) |  | 0 | The reconstruction of physical fields from sparse measurements is pivotal in both scientific research and engineering applications. Traditional methods are increasingly supplemented by deep learning models due to their efficacy in extracting features from data. However, except for the low accuracy... | Hongkun Dou, Lijun Yang, Shen Fang, Wang Han, Yue Deng, Zeyu Li |  |
| 406 |  |  [RegMix: Data Mixture as Regression for Language Model Pre-training](https://openreview.net/forum?id=5BjQOUXq7i) |  | 0 | The data mixture for large language model pre-training significantly impacts performance, yet how to determine an effective mixture remains unclear. We propose RegMix to automatically identify a high-performing data mixture by formulating it as a regression task. RegMix trains many small models on... | Guangtao Zeng, Jing Jiang, Longxu Dou, Min Lin, Niklas Muennighoff, Qian Liu, Tianyu Pang, Xiaosen Zheng |  |
| 407 |  |  [When Attention Sink Emerges in Language Models: An Empirical View](https://openreview.net/forum?id=78Nn4QJTEN) |  | 0 | Auto-regressive language Models (LMs) assign significant attention to the first token, even if it is not semantically important, which is known as \*\*attention sink\*\*. This phenomenon has been widely adopted in applications such as streaming/long context generation, KV cache optimization,... | Chao Du, Cunxiao Du, Fengzhuo Zhang, Min Lin, Qian Liu, Tianyu Pang, Xiangming Gu, Ye Wang |  |
| 408 |  |  [PianoMotion10M: Dataset and Benchmark for Hand Motion Generation in Piano Performance](https://openreview.net/forum?id=rxVvRBgqmS) |  | 0 | Recently, artificial intelligence techniques for education have been received increasing attentions, while it still remains an open problem to design the effective music instrument instructing systems. Although key presses can be directly derived from sheet music, the transitional movements among... | Jianke Zhu, Qijun Gan, Shengtao Wu, Song Wang |  |
| 409 |  |  [The Power of LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions](https://openreview.net/forum?id=ws5phQki00) |  | 0 | Stance detection holds great potential to improve online political discussions through its deployment in discussion platforms for purposes such as content moderation, topic summarisation or to facilitate more balanced discussions. Typically, transformer-based models are employed directly for stance... | Maike Behrendt, Marc Ziegele, Stefan Harmeling, Stefan Sylvius Wagner |  |
| 410 |  |  [Tell me about yourself: LLMs are aware of their learned behaviors](https://openreview.net/forum?id=IjQ2Jtemzy) |  | 0 | We study \*behavioral self-awareness\*, which we define as an LLM's capability to articulate its behavioral policies without relying on in-context examples. We finetune LLMs on examples that exhibit particular behaviors, including (a) making risk-seeking / risk-averse economic decisions, and (b)... | Anna SztyberBetley, James Chua, Jan Betley, Martín Soto, Owain Evans, Xuchan Bao |  |
| 411 |  |  [COPER: Correlation-based Permutations for Multi-View Clustering](https://openreview.net/forum?id=5ZEbpBYGwH) |  | 0 | Combining data from different sources can improve data analysis tasks such as clustering. However, most of the current multi-view clustering methods are limited to specific domains or rely on a suboptimal and computationally intensive two-stage process of representation learning and clustering. We... | Jonathan Svirsky, Ofir Lindenbaum, Ran Eisenberg |  |
| 412 |  |  [Improving Convergence Guarantees of Random Subspace Second-order Algorithm for Nonconvex Optimization](https://openreview.net/forum?id=tuu4de7HL1) |  | 0 | In recent years, random subspace methods have been actively studied for large-dimensional nonconvex problems. Recent subspace methods have improved theoretical guarantees such as iteration complexity and local convergence rate while reducing computational costs by deriving descent directions in... | Akiko Takeda, PierreLouis Poirion, Rei Higuchi |  |
| 413 |  |  [Revisiting text-to-image evaluation with Gecko: on metrics, prompts, and human rating](https://openreview.net/forum?id=Im2neAMlre) |  | 0 | While text-to-image (T2I) generative models have become ubiquitous, they do not necessarily generate images that align with a given prompt. While many metrics and benchmarks have been proposed to evaluate T2I models and alignment metrics, the impact of the evaluation components (prompt sets, human... | Aida Nematzadeh, Anant Nawalgaria, Christopher Knutsen, Chuhan Zhang, Cyrus Rashtchian, Emanuele Bugliarello, Ira Ktena, Isabela Albuquerque, Ivana Kajic, Jordi PontTuset, Olivia Wiles, Pinelopi Papalampidi, Su Wang, Yasumasa Onoe |  |
| 414 |  |  [Diffusion Bridge AutoEncoders for Unsupervised Representation Learning](https://openreview.net/forum?id=hBGavkf61a) |  | 0 | Diffusion-based representation learning has achieved substantial attention due to its promising capabilities in latent representation and sample generation. Recent studies have employed an auxiliary encoder to identify a corresponding representation from data and to adjust the dimensionality of a... | Byeonghu Na, IlChul Moon, Kwanghyeon Lee, Minsang Park, Yeongmin Kim |  |
| 415 |  |  [Bundle Neural Network for message diffusion on graphs](https://openreview.net/forum?id=scI9307PLG) |  | 0 | The dominant paradigm for learning on graphs is message passing. Despite being a strong inductive bias, the local message passing mechanism faces challenges such as over-smoothing, over-squashing, and limited expressivity. To address these issues, we introduce Bundle Neural Networks (BuNNs), a... | Federico Barbero, Jacob Bamberger, Michael M. Bronstein, Xiaowen Dong |  |
| 416 |  |  [SynFlowNet: Design of Diverse and Novel Molecules with Synthesis Constraints](https://openreview.net/forum?id=uvHmnahyp1) |  | 0 | Generative models see increasing use in computer-aided drug design. However, while performing well at capturing distributions of molecular motifs, they often produce synthetically inaccessible molecules. To address this, we introduce SynFlowNet, a GFlowNet model whose action space uses chemical... | Arne Schneuing, Bruno E. Correia, Charles Harris, Emmanuel Bengio, Ilia Igashov, Julien Roy, Marwin H. S. Segler, Miruna T. Cretu, Pietro Lio |  |
| 417 |  |  [u-μP: The Unit-Scaled Maximal Update Parametrization](https://openreview.net/forum?id=P7KRIiLM8T) |  | 0 | The Maximal Update Parametrization ($\mu$P) aims to make the optimal hyperparameters (HPs) of a model independent of its size, allowing them to be swept using a cheap proxy model rather than the full-size target model. We present a new scheme, u-$\mu$P, which improves upon $\mu$P by combining it... | Andrés Felipe CruzSalinas, Björn Deiseroth, Carlo Luschi, Charlie Blake, Constantin Eichenberg, Douglas Orr, Josef Dean, Lukas Balles, Luke Yuri Prince, Samuel Weinbach |  |
| 418 |  |  [Improved Convergence Rate for Diffusion Probabilistic Models](https://openreview.net/forum?id=SOd07Qxkw4) |  | 0 | Score-based diffusion models have achieved remarkable empirical performance in the field of machine learning and artificial intelligence for their ability to generate high-quality new data instances from complex distributions. Improving our understanding of diffusion models, including mainly... | Gen Li, Yuchen Jiao |  |
| 419 |  |  [MagicPIG: LSH Sampling for Efficient LLM Generation](https://openreview.net/forum?id=ALzTQUgW8a) |  | 0 | Large language models (LLMs) with long context windows have gained significant attention. However, the KV cache, stored to avoid re-computation, becomes a bottleneck. Various dynamic sparse or TopK-based attention approximation methods have been proposed to leverage the common insight that... | Beidi Chen, Jianyu Zhang, Léon Bottou, Matthijs Douze, Niklas Nolte, Ranajoy Sadhukhan, Yang Zhou, Yuandong Tian, Zhihao Jia, Zhuoming Chen, Zihao Ye |  |
| 420 |  |  [Streamlining Redundant Layers to Compress Large Language Models](https://openreview.net/forum?id=IC5RJvRoMp) |  | 0 | This paper introduces LLM-Streamline, a pioneer work on layer pruning for large language models (LLMs). It is based on the observation that different layers have varying impacts on hidden states, enabling the identification of less important layers to be pruned. LLM-Streamline comprises two parts:... | Cuiping Li, Hong Chen, Jing Zhang, Xiaodong Chen, Yanling Wang, Yuxuan Hu |  |
| 421 |  |  [Rethinking and Improving Autoformalization: Towards a Faithful Metric and a Dependency Retrieval-based Approach](https://openreview.net/forum?id=hUb2At2DsQ) |  | 0 | As a central component in formal verification, statement autoformalization has been widely studied including the recent efforts from machine learning community, but still remains a widely-recognized difficult and open problem. In this paper, we delve into two critical yet under-explored gaps: 1)... | Junchi Yan, Qi Liu, Qinxiang Cao, Xinhao Zheng, Xudong Lu |  |
| 422 |  |  [Identifiable Exchangeable Mechanisms for Causal Structure and Representation Learning](https://openreview.net/forum?id=k03mB41vyM) |  | 0 | Identifying latent representations or causal structures is important for good generalization and downstream task performance. However, both fields developed rather independently. We observe that several structure and representation identifiability methods, particularly those that require multiple... | Bernhard Schölkopf, Ferenc Huszár, Patrik Reizinger, Siyuan Guo, Wieland Brendel |  |
| 423 |  |  [Learning Spatiotemporal Dynamical Systems from Point Process Observations](https://openreview.net/forum?id=37EXtKCOkn) |  | 0 | Spatiotemporal dynamics models are fundamental for various domains, from heat propagation in materials to oceanic and atmospheric flows. However, currently available neural network-based spatiotemporal modeling approaches fall short when faced with data that is collected randomly over time and... | Harri Lähdesmäki, Valerii Iakovlev |  |
| 424 |  |  [Probabilistic Neural Pruning via Sparsity Evolutionary Fokker-Planck-Kolmogorov Equation](https://openreview.net/forum?id=hJ1BaJ5ELp) |  | 0 | Neural pruning aims to compress and accelerate deep neural networks by identifying the optimal subnetwork within a specified sparsity budget. In this work, we study how to gradually sparsify the unpruned dense model to the target sparsity level with minimal performance drop. Specifically, we... | Haosen Shi, Sinno Jialin Pan, Zhanfeng Mo |  |
| 425 |  |  [Diffusion Attribution Score: Evaluating Training Data Influence in Diffusion Models](https://openreview.net/forum?id=kuutidLf6R) |  | 0 | As diffusion models become increasingly popular, the misuse of copyrighted and private images has emerged as a major concern. One promising solution to mitigate this issue is identifying the contribution of specific training samples in generative models, a process known as data attribution.... | Chang Xu, Jinxu Lin, Linwei Tao, Minjing Dong |  |
| 426 |  |  [Uncovering Gaps in How Humans and LLMs Interpret Subjective Language](https://openreview.net/forum?id=gye2U9uNXx) |  | 0 | Humans often rely on subjective natural language to direct language models (LLMs); for example, users might instruct the LLM to write an \*enthusiastic\* blogpost, while developers might train models to be \*helpful\* and \*harmless\* using LLM-based edits. The LLM’s \*operational semantics\* of... | Arjun Patrawala, Erik Jones, Jacob Steinhardt |  |
| 427 |  |  [Learning local equivariant representations for quantum operators](https://openreview.net/forum?id=kpq3IIjUD3) |  | 0 | Predicting quantum operator matrices such as Hamiltonian, overlap, and density matrices in the density functional theory (DFT) framework is crucial for material science. Current methods often focus on individual operators and struggle with efficiency and scalability for large systems. Here we... | Linfeng Zhang, Qiangqiang Gu, Shishir Kumar Pandey, Zhanghao Zhouyin, Zixi Gan |  |
| 428 |  |  [PhyMPGN: Physics-encoded Message Passing Graph Network for spatiotemporal PDE systems](https://openreview.net/forum?id=fU8H4lzkIm) |  | 0 | Solving partial differential equations (PDEs) serves as a cornerstone for modeling complex dynamical systems. Recent progresses have demonstrated grand benefits of data-driven neural-based models for predicting spatiotemporal dynamics (e.g., tremendous speedup gain compared with classical numerical... | Bocheng Zeng, Hao Sun, Hongsheng Liu, Mengtao Yan, Qi Wang, Ruizhi Chengze, Yang Liu, Yi Zhang, Zidong Wang |  |
| 429 |  |  [Demystifying the Token Dynamics of Deep Selective State Space Models](https://openreview.net/forum?id=qtTIP5Gjc5) |  | 0 | Selective state space models (SSM), such as Mamba, have gained prominence for their effectiveness in modeling sequential data. Despite their outstanding empirical performance, a comprehensive theoretical understanding of deep selective SSM remains elusive, hindering their further development and... | DuyTung Pham, Tan Minh Nguyen, Thieu N. Vo, Xin T. Tong |  |
| 430 |  |  [Let SSMs be ConvNets: State-space Modeling with Optimal Tensor Contractions](https://openreview.net/forum?id=PkpNRmBZ32) |  | 0 | We introduce Centaurus, a class of networks composed of generalized state-space model (SSM) blocks, where the SSM operations can be treated as tensor contractions during training. The optimal order of tensor contractions can then be systematically determined for every SSM block to maximize training... | Yan Ru Pei |  |
| 431 |  |  [MixEval-X: Any-to-any Evaluations from Real-world Data Mixture](https://openreview.net/forum?id=hpCfPEvBsr) |  | 0 | Perceiving and generating diverse modalities are crucial for AI models to effectively learn from and engage with real-world signals, necessitating reliable evaluations for their development. We identify two major issues in current evaluations: (1) inconsistent standards, shaped by different... | Bo Li, David Junhao Zhang, Deepanway Ghosal, Fuzhao Xue, Jinjie Ni, Kabir Jain, Kaichen Zhang, Mahir Shah, Michael Shieh, Xiang Yue, Yang You, Yifan Song, Yuntian Deng, Zian Zheng |  |
| 432 |  |  [Knowledge Localization: Mission Not Accomplished? Enter Query Localization!](https://openreview.net/forum?id=tfyHbvFZ0K) |  | 0 | Large language models (LLMs) store extensive factual knowledge, but the mechanisms behind how they store and express this knowledge remain unclear. The Knowledge Neuron (KN) thesis is a prominent theory for explaining these mechanisms. This theory is based on the \*\*Knowledge Localization (KL)\*\*... | Jun Zhao, Kang Liu, Pengfei Cao, Yubo Chen, Yuheng Chen |  |
| 433 |  |  [Graph Sparsification via Mixture of Graphs](https://openreview.net/forum?id=7ANDviElAo) |  | 0 | Graph Neural Networks (GNNs) have demonstrated superior performance across various graph learning tasks but face significant computational challenges when applied to large-scale graphs. One effective approach to mitigate these challenges is graph sparsification, which involves removing... | Chonghe Jiang, Guibin Zhang, Kun Wang, Shirui Pan, Tianlong Chen, Xiangguo Sun, Yanwei Yue |  |
| 434 |  |  [Realistic Evaluation of Deep Partial-Label Learning Algorithms](https://openreview.net/forum?id=FtX6oAW7Dd) |  | 0 | Partial-label learning (PLL) is a weakly supervised learning problem in which each example is associated with multiple candidate labels and only one is the true label. In recent years, many deep PLL algorithms have been developed to improve model performance. However, we find that some early... | DongDong Wu, Gang Niu, Jindong Wang, Masashi Sugiyama, MinLing Zhang, Wei Wang |  |
| 435 |  |  [BodyGen: Advancing Towards Efficient Embodiment Co-Design](https://openreview.net/forum?id=cTR17xl89h) |  | 0 | Embodiment co-design aims to optimize a robot's morphology and control policy simultaneously. While prior work has demonstrated its potential for generating environment-adaptive robots, this field still faces persistent challenges in optimization efficiency due to the (i) combinatorial nature of... | Haofei Lu, Jianshu Li, Junliang Xing, Ruoyu Li, Yuanchun Shi, Zhe Li, Zhe Wu |  |
| 436 |  |  [RAG-SR: Retrieval-Augmented Generation for Neural Symbolic Regression](https://openreview.net/forum?id=NdHka08uWn) |  | 0 | Symbolic regression is a key task in machine learning, aiming to discover mathematical expressions that best describe a dataset. While deep learning has increased interest in using neural networks for symbolic regression, many existing approaches rely on pre-trained models. These models require... | Bing Xue, Hengzhe Zhang, Mengjie Zhang, Qi Chen, Wolfgang Banzhaf |  |
| 437 |  |  [Theory on Mixture-of-Experts in Continual Learning](https://openreview.net/forum?id=7XgKAabsPp) |  | 0 | Continual learning (CL) has garnered significant attention because of its ability to adapt to new tasks that arrive over time. Catastrophic forgetting (of old tasks) has been identified as a major issue in CL, as the model adapts to new tasks. The Mixture-of-Experts (MoE) model has recently been... | Hongbo Li, Lingjie Duan, Ness B. Shroff, Sen Lin, Yingbin Liang |  |
| 438 |  |  [Decentralized Sporadic Federated Learning: A Unified Algorithmic Framework with Convergence Guarantees](https://openreview.net/forum?id=cznqgb4DNv) |  | 0 | Decentralized federated learning (DFL) captures FL settings where both (i) model updates and (ii) model aggregations are exclusively carried out by the clients without a central server. Existing DFL works have mostly focused on settings where clients conduct a fixed number of local updates between... | Christopher G. Brinton, DongJun Han, Rohit Parasnis, Seyyedali Hosseinalipour, Shahryar Zehtabi |  |
| 439 |  |  [MAD-TD: Model-Augmented Data stabilizes High Update Ratio RL](https://openreview.net/forum?id=6RtRsg8ZV1) |  | 0 | Building deep reinforcement learning (RL) agents that find a good policy with few samples has proven notoriously challenging. To achieve sample efficiency, recent work has explored updating neural networks with large numbers of gradient steps for every new sample. While such high update-to-data... | Amirmassoud Farahmand, Claas Voelcker, Eric Eaton, Igor Gilitschenski, Marcel Hussing |  |
| 440 |  |  [Linear Mode Connectivity in Differentiable Tree Ensembles](https://openreview.net/forum?id=UqYNPyotxL) |  | 0 | Linear Mode Connectivity (LMC) refers to the phenomenon that performance remains consistent for linearly interpolated models in the parameter space. For independently optimized model pairs from different random initializations, achieving LMC is considered crucial for understanding the stable... | Mahito Sugiyama, Ryuichi Kanoh |  |
| 441 |  |  [Overcoming False Illusions in Real-World Face Restoration with Multi-Modal Guided Diffusion Model](https://openreview.net/forum?id=m9RNBZewW2) |  | 0 | We introduce a novel Multi-modal Guided Real-World Face Restoration (MGFR) technique designed to improve the quality of facial image restoration from low-quality inputs. Leveraging a blend of attribute text prompts, high-quality reference images, and identity information, MGFR can mitigate the... | Jinjin Gu, Keda Tao, Nan Cheng, Xiucheng Wang, Yulun Zhang |  |
| 442 |  |  [DEEM: Diffusion models serve as the eyes of large language models for image perception](https://openreview.net/forum?id=qtWjSboqfe) |  | 0 | The development of large language models (LLMs) has significantly advanced the emergence of large multimodal models (LMMs). While LMMs have achieved tremendous success by promoting the synergy between multimodal comprehension and creation, they often face challenges when confronted with... | Binyuan Hui, Hamid Rokny, Lei Zhang, Longze Chen, Min Yang, Run Luo, TingEn Lin, Tongliang Liu, Wanwei He, Xiaobo Xia, Yunshui Li, Zikai Song, Ziqiang Liu |  |
| 443 |  |  [Stabilizing Reinforcement Learning in Differentiable Multiphysics Simulation](https://openreview.net/forum?id=DRiLWb8bJg) |  | 0 | Recent advances in GPU-based parallel simulation have enabled practitioners to collect large amounts of data and train complex control policies using deep reinforcement learning (RL), on commodity GPUs. However, such successes for RL in robotics have been limited to tasks sufficiently simulated by... | Eliot Xing, Jean Oh, Vernon Luk |  |
| 444 |  |  [Credal Wrapper of Model Averaging for Uncertainty Estimation in Classification](https://openreview.net/forum?id=cv2iMNWCsh) |  | 0 | This paper presents an innovative approach, called credal wrapper, to formulating a credal set representation of model averaging for Bayesian neural networks (BNNs) and deep ensembles (DEs), capable of improving uncertainty estimation in classification tasks. Given a finite collection of single... | David Moens, Fabio Cuzzolin, Hans Hallez, Kaizheng Wang, Keivan Shariatmadar |  |
| 445 |  |  [Discovering Temporally Compositional Neural Manifolds with Switching Infinite GPFA](https://openreview.net/forum?id=2iCIHgE8KG) |  | 0 | Gaussian Process Factor Analysis (GPFA) is a powerful latent variable model for extracting low-dimensional manifolds underlying population neural activities. However, one limitation of standard GPFA models is that the number of latent factors needs to be pre-specified or selected through... | Changmin Yu, Maneesh Sahani, Máté Lengyel |  |
| 446 |  |  [Presto! Distilling Steps and Layers for Accelerating Music Generation](https://openreview.net/forum?id=Gj5JTAwdoy) |  | 0 | Despite advances in diffusion-based text-to-music (TTM) methods, efficient, high-quality generation remains a challenge. We introduce Presto!, an approach to inference acceleration for score-based diffusion transformers via reducing both sampling steps and cost per step. To reduce steps, we develop... | Ge Zhu, Jonah Casebeer, Julian J. McAuley, Nicholas J. Bryan, Taylor BergKirkpatrick, Zachary Novack |  |
| 447 |  |  [Grounding Video Models to Actions through Goal Conditioned Exploration](https://openreview.net/forum?id=G6dMvRuhFr) |  | 0 | Large video models, pretrained on massive quantities of amount of Internet video, provide a rich source of physical knowledge about the dynamics and motions of objects and tasks. However, video models are not grounded in the embodiment of an agent, and do not describe how to actuate the world to... | Yilun Du, Yunhao Luo |  |
| 448 |  |  [RESuM: A Rare Event Surrogate Model for Physics Detector Design](https://openreview.net/forum?id=lqTILjL6lP) |  | 0 | The experimental discovery of neutrinoless double-beta decay (NLDBD) would answer one of the most important questions in physics: Why is there more matter than antimatter in our universe? To maximize the chances of discovery, NLDBD experiments must optimize their detector designs to minimize the... | A. W. P. Poon, AnnKathrin Schuetz, Aobo Li |  |
| 449 |  |  [Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in Language Models](https://openreview.net/forum?id=GjM61KRiTG) |  | 0 | Fine-tuning large language models (LLMs) on human preferences, typically through reinforcement learning from human feedback (RLHF), has proven successful in enhancing their capabilities. However, ensuring the safety of LLMs during fine-tuning remains a critical concern, and mitigating the potential... | Adel Bibi, Mohamed Elhoseiny, Philip Torr, Wenxuan Zhang |  |
| 450 |  |  [Enhancing the Scalability and Applicability of Kohn-Sham Hamiltonians for Molecular Systems](https://openreview.net/forum?id=twEvvkQqPS) |  | 0 | Density Functional Theory (DFT) is a pivotal method within quantum chemistry and materials science, with its core involving the construction and solution of the Kohn-Sham Hamiltonian. Despite its importance, the application of DFT is frequently limited by the substantial computational resources... | Bin Shao, Chang Liu, Erpai Luo, Han Yang, Jia Zhang, Lin Huang, Mark Gerstein, Samuel Harshe, Xinran Wei, Yunyang Li, Zaishuo Xia, Zun Wang |  |
| 451 |  |  [Dense Video Object Captioning from Disjoint Supervision](https://openreview.net/forum?id=auZZ2gN0ZN) |  | 0 | We propose a new task and model for dense video object captioning -- detecting, tracking and captioning trajectories of objects in a video. This task unifies spatial and temporal localization in video, whilst also requiring fine-grained visual understanding that is best described by natural... | Anurag Arnab, Chen Sun, Cordelia Schmid, Xingyi Zhou |  |
| 452 |  |  [SplatFormer: Point Transformer for Robust 3D Gaussian Splatting](https://openreview.net/forum?id=9NfHbWKqMF) |  | 0 | 3D Gaussian Splatting (3DGS) has recently transformed photorealistic reconstruction, achieving high visual fidelity and real-time performance. However, rendering quality significantly deteriorates when test views deviate from the camera angles used during training, posing a major challenge for... | Marko Mihajlovic, Sergey Prokudin, Siyu Tang, Xiyi Chen, Yiming Wang, Yutong Chen |  |
| 453 |  |  [DeLLMa: Decision Making Under Uncertainty with Large Language Models](https://openreview.net/forum?id=Acvo2RGSCy) |  | 0 | The potential of large language models (LLMs) as decision support tools is increasingly being explored in fields such as business, engineering, and medicine, which often face challenging tasks of \*decision-making under uncertainty\*. In this paper, we show that directly prompting LLMs on these... | Dani Yogatama, Deqing Fu, Ollie Liu, Willie Neiswanger |  |
| 454 |  |  [OmniRe: Omni Urban Scene Reconstruction](https://openreview.net/forum?id=11xgiMEI5o) |  | 0 | We introduce OmniRe, a comprehensive system for efficiently creating high-fidelity digital twins of dynamic real-world scenes from on-device logs. Recent methods using neural fields or Gaussian Splatting primarily focus on vehicles, hindering a holistic framework for all dynamic foregrounds... | Boris Ivanovic, Janick Martinez Esturo, Jiahui Huang, Jiawei Yang, Li Song, Marco Pavone, Or Litany, Riccardo de Lutio, Sanja Fidler, Yue Wang, Zan Gojcic, Ziyu Chen |  |
| 455 |  |  [ZAPBench: A Benchmark for Whole-Brain Activity Prediction in Zebrafish](https://openreview.net/forum?id=oCHsDpyawq) |  | 0 | Data-driven benchmarks have led to significant progress in key scientific modeling domains including weather and structural biology. Here, we introduce the Zebrafish Activity Prediction Benchmark (ZAPBench) to measure progress on the problem of predicting cellular-resolution neural activity... | Alex BoYuan Chen, Alexander Immer, Alyson Petruncio, Aparna Dev, Aubrey Weigel, Florian Engert, Gudrun Ihrke, JanMatthis Lueckmann, Jeff Lichtman, Luuk Willem Hesselink, Mariela D. Petkova, Michal Januszewski, Misha B. Ahrens, Nirmala A. Iyer, Peter H. Li, Viren Jain, Woohyun Park, Wyatt Korff |  |
| 456 |  |  [Lumina-T2X: Scalable Flow-based Large Diffusion Transformer for Flexible Resolution Generation](https://openreview.net/forum?id=EbWf36quzd) |  | 0 | Sora unveils the potential of scaling Diffusion Transformer (DiT) for generating photorealistic images and videos at arbitrary resolutions, aspect ratios, and durations, yet it still lacks sufficient implementation details. In this paper, we introduce the Lumina-T2X family -- a series of Flow-based... | Dongyang Liu, Hongsheng Li, Jingwen He, Junjun He, Junlin Xie, Le Zhuo, Longtian Qiu, Peng Gao, Renrui Zhang, Rongjie Huang, Ruoyi Du, Shijie Geng, Tianshuo Yang, Tong He, Weicai Ye, Wenqi Shao, Xu Luo, Yu Qiao, Yuhang Zhang, Zhengkai Jiang |  |
| 457 |  |  [A Periodic Bayesian Flow for Material Generation](https://openreview.net/forum?id=Lz0XW99tE0) |  | 0 | Generative modeling of crystal data distribution is an important yet challenging task due to the unique periodic physical symmetry of crystals. Diffusion-based methods have shown early promise in modeling crystal distribution. More recently, Bayesian Flow Networks were introduced to aggregate noisy... | Hanlin Wu, Hao Zhou, Jianbing Zhang, Jingjing Gong, Jingjing Liu, WeiYing Ma, Yawen Ouyang, Yuxuan Song, Ziyao Cao |  |
| 458 |  |  [DiffPuter: Empowering Diffusion Models for Missing Data Imputation](https://openreview.net/forum?id=3fl1SENSYO) |  | 0 | Generative models play an important role in missing data imputation in that they aim to learn the joint distribution of full data. However, applying advanced deep generative models (such as Diffusion models) to missing data imputation is challenging due to 1) the inherent incompleteness of the... | Hengrui Zhang, Liancheng Fang, Philip S. Yu, Qitian Wu |  |
| 459 |  |  [Towards Marginal Fairness Sliced Wasserstein Barycenter](https://openreview.net/forum?id=NQqJPPCesd) |  | 0 | The Sliced Wasserstein barycenter (SWB) is a widely acknowledged method for efficiently generalizing the averaging operation within probability measure spaces. However, achieving marginal fairness SWB, ensuring approximately equal distances from the barycenter to marginals, remains unexplored. The... | Hai Nguyen, Khai Nguyen, Nhat Ho |  |
| 460 |  |  [Continuous Exposure Learning for Low-light Image Enhancement using Neural ODEs](https://openreview.net/forum?id=Mn2qgIcIPS) |  | 0 | Low-light image enhancement poses a significant challenge due to the limited information captured by image sensors in low-light environments. Despite recent improvements in deep learning models, the lack of paired training datasets remains a significant obstacle. Therefore, unsupervised methods... | Daehyun Kim, Donggoo Jung, Tae Hyun Kim |  |
| 461 |  |  [Learning to Solve Differential Equation Constrained Optimization Problems](https://openreview.net/forum?id=VeMC6Bn0ZB) |  | 0 | Differential equations (DE) constrained optimization plays a critical role in numerous scientific and engineering fields, including energy systems, aerospace engineering, ecology, and finance, where optimal configurations or control strategies must be determined for systems governed by ordinary or... | Ferdinando Fioretto, Kyri Baker, Mostafa Mohammadian, Vincenzo Di Vito Francesco |  |
| 462 |  |  [Fast Uncovering of Protein Sequence Diversity from Structure](https://openreview.net/forum?id=1iuaxjssVp) |  | 0 | We present InvMSAFold, an inverse folding method for generating protein sequences optimized for diversity and speed. For a given structure, InvMSAFold generates the parameters of a pairwise probability distribution over the space of sequences, capturing the amino acid covariances observed in... | Barthélémy MeynardPiganeau, Carlo Lucibello, Christoph Feinauer, Luca Alessandro Silva |  |
| 463 |  |  [Boosting Ray Search Procedure of Hard-label Attacks with Transfer-based Priors](https://openreview.net/forum?id=tIBAOcAvn4) |  | 0 | One of the most practical and challenging types of black-box adversarial attacks is the hard-label attack, where only the top-1 predicted label is available. One effective approach is to search for the optimal ray direction from the benign image that minimizes the $\ell_p$ norm distance to the... | Chen Ma, Qi Xuan, Shuyu Cheng, Xinjie Xu |  |
| 464 |  |  [3DIS: Depth-Driven Decoupled Image Synthesis for Universal Multi-Instance Generation](https://openreview.net/forum?id=MagmwodCAB) |  | 0 | The increasing demand for controllable outputs in text-to-image generation has spurred advancements in multi-instance generation (MIG), allowing users to define both instance layouts and attributes. However, unlike image-conditional generation methods such as ControlNet, MIG techniques have not... | Dewei Zhou, Ji Xie, Yi Yang, Zongxin Yang |  |
| 465 |  |  [Strong Model Collapse](https://openreview.net/forum?id=et5l9qPUhm) |  | 0 | Within the scaling laws paradigm, which underpins the training of large neural networks like ChatGPT and Llama, we consider a supervised regression setting and establish a strong form of the model collapse phenomenon, a critical performance degradation due to synthetic data in the training corpus.... | Arjun Subramonian, Elvis Dohmatob, Julia Kempe, Yunzhen Feng |  |
| 466 |  |  [DRoP: Distributionally Robust Data Pruning](https://openreview.net/forum?id=fxv0FfmDAg) |  | 0 | In the era of exceptionally data-hungry models, careful selection of the training data is essential to mitigate the extensive costs of deep learning. Data pruning offers a solution by removing redundant or uninformative samples from the dataset, which yields faster convergence and improved neural... | Artem M. Vysogorets, Julia Kempe, Kartik Ahuja |  |
| 467 |  |  [Co3Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion](https://openreview.net/forum?id=VaowElpVzd) |  | 0 | Generating gestures from human speech has gained tremendous progress in animating virtual avatars. While the existing methods enable synthesizing gestures cooperated by people self-talking, they overlook the practicality of concurrent gesture modeling with two-person interactive conversations.... | Hengyuan Zhang, Jiahao Pan, Qifeng Liu, Shanghang Zhang, Wei Xue, Wenhan Luo, Xingqun Qi, Yatian Wang, Yike Guo |  |
| 468 |  |  [Following the Human Thread in Social Navigation](https://openreview.net/forum?id=M8OGl34Pmg) |  | 0 | The success of collaboration between humans and robots in shared environments relies on the robot's real-time adaptation to human motion. Specifically, in Social Navigation, the agent should be close enough to assist but ready to back up to let the human move freely, avoiding collisions. Human... | Alessio Sampieri, Fabio Galasso, Indro Spinelli, Lamberto Ballan, Luca Scofano, Tommaso Campari, Valentino Sacco |  |
| 469 |  |  [Don't flatten, tokenize! Unlocking the key to SoftMoE's efficacy in deep RL](https://openreview.net/forum?id=8oCrlOaYcc) |  | 0 | The use of deep neural networks in reinforcement learning (RL) often suffers from performance degradation as model size increases. While soft mixtures of experts (SoftMoEs) have recently shown promise in mitigating this issue for online RL, the reasons behind their effectiveness remain largely... | Aaron C. Courville, Ghada Sokar, Hugo Larochelle, Johan S. ObandoCeron, Pablo Samuel Castro |  |
| 470 |  |  [Boltzmann-Aligned Inverse Folding Model as a Predictor of Mutational Effects on Protein-Protein Interactions](https://openreview.net/forum?id=lzdFImKK8w) |  | 0 | Predicting the change in binding free energy ($\Delta \Delta G$) is crucial for understanding and modulating protein-protein interactions, which are critical in drug design. Due to the scarcity of experimental $\Delta\Delta G$ data, existing methods focus on pre-training, while neglecting the... | Chunhua Shen, Hao Chen, Peiyuan Yang, Weian Mao, Wengong Jin, Xiaoran Jiao |  |
| 471 |  |  [Improving Unsupervised Constituency Parsing via Maximizing Semantic Information](https://openreview.net/forum?id=qyU5s4fzLg) |  | 0 | Unsupervised constituency parsers organize phrases within a sentence into a tree-shaped syntactic constituent structure that reflects the organization of sentence semantics. However, the traditional objective of maximizing sentence log-likelihood (LL) does not explicitly account for the close... | Danushka Bollegala, Junjie Chen, Xiangheng He, Yusuke Miyao |  |
| 472 |  |  [Student-Informed Teacher Training](https://openreview.net/forum?id=Dzh0hQPpuf) |  | 0 | Imitation learning with a privileged teacher has proven effective for learning complex control behaviors from high-dimensional inputs, such as images. In this framework, a teacher is trained with privileged task information, while a student tries to predict the actions of the teacher with more... | Davide Scaramuzza, Elie Aljalbout, Jiaxu Xing, Nico Messikommer |  |
| 473 |  |  [OS-ATLAS: Foundation Action Model for Generalist GUI Agents](https://openreview.net/forum?id=n9PDaFNi8t) |  | 0 | Existing efforts in building GUI agents heavily rely on the availability of robust commercial Vision-Language Models (VLMs) such as GPT-4o and GeminiProVision. Practitioners are often reluctant to use open-source VLMs due to their significant performance lag compared to their closed-source... | Chengyou Jia, Fangzhi Xu, Kanzhi Cheng, Liheng Chen, Paul Pu Liang, Qiushi Sun, Yian Wang, Yu Qiao, Zhenyu Wu, Zhiyong Wu, Zichen Ding |  |
| 474 |  |  [Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models](https://openreview.net/forum?id=9WYMDgxDac) |  | 0 | Multimodal Large Language Models (MLLMs) exhibit promising advancements across various tasks, yet they still encounter significant trustworthiness issues. Prior studies apply Split Conformal Prediction (SCP) in language modeling to construct prediction sets with statistical guarantees. However,... | Bo Fu, Feng Zheng, Qingni Wang, Teng Wang, Tiantian Geng, Zhiyuan Wang |  |
| 475 |  |  [Exploring the Camera Bias of Person Re-identification](https://openreview.net/forum?id=SgymXhOEA5) |  | 0 | We empirically investigate the camera bias of person re-identification (ReID) models. Previously, camera-aware methods have been proposed to address this issue, but they are largely confined to training domains of the models. We measure the camera bias of ReID models on unseen domains and reveal... | JinWoo Park, JongSeok Lee, Myungseo Song |  |
| 476 |  |  [Preference Optimization for Reasoning with Pseudo Feedback](https://openreview.net/forum?id=jkUp3lybXf) |  | 0 | Preference optimization techniques, such as Direct Preference Optimization (DPO), are frequently employed to enhance the reasoning capabilities of large language models (LLMs) in domains like mathematical reasoning and coding, typically following supervised fine-tuning. These methods rely on... | Fangkai Jiao, Furu Wei, Geyang Guo, Nancy F. Chen, Shafiq Joty, Xingxing Zhang |  |
| 477 |  |  [Regularization by Texts for Latent Diffusion Inverse Solvers](https://openreview.net/forum?id=TtUh0TOlGX) |  | 0 | The recent development of diffusion models has led to significant progress in solving inverse problems by leveraging these models as powerful generative priors. However, challenges persist due to the ill-posed nature of such problems, often arising from ambiguities in measurements or intrinsic... | Geon Yeong Park, Hyungjin Chung, Jeongsol Kim, Jong Chul Ye |  |
| 478 |  |  [Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool Usage](https://openreview.net/forum?id=0bmGL4q7vJ) |  | 0 | The advancement of large language models (LLMs) prompts the development of multi-modal agents, which are used as a controller to call external tools, providing a feasible way to solve practical tasks. In this paper, we propose a multi-modal agent tuning method that automatically generates... | Bofei Zhang, Pengxiang Li, Qing Li, SongChun Zhu, Tao Yuan, Xiaojian Ma, Yue Fan, Yunde Jia, Yuwei Wu, Zhi Gao |  |
| 479 |  |  [Monitoring Latent World States in Language Models with Propositional Probes](https://openreview.net/forum?id=0yvZm2AjUr) |  | 0 | Language models (LMs) are susceptible to bias, sycophancy, backdoors, and other tendencies that lead to unfaithful responses to the input context. Interpreting internal states of LMs could help monitor and correct unfaithful behavior. We hypothesize that LMs faithfully represent their input... | Jacob Steinhardt, Jiahai Feng, Stuart Russell |  |
| 480 |  |  [TAID: Temporally Adaptive Interpolated Distillation for Efficient Knowledge Transfer in Language Models](https://openreview.net/forum?id=cqsw28DuMW) |  | 0 | Causal language models have demonstrated remarkable capabilities, but their size poses significant challenges for deployment in resource-constrained environments. Knowledge distillation, a widely-used technique for transferring knowledge from a large teacher model to a small student model, presents... | Han Bao, Kou Misaki, Makoto Shing, Sho Yokoi, Takuya Akiba |  |
| 481 |  |  [Learning Transformer-based World Models with Contrastive Predictive Coding](https://openreview.net/forum?id=YK9G4Htdew) |  | 0 | The DreamerV3 algorithm recently obtained remarkable performance across diverse environment domains by learning an accurate world model based on Recurrent Neural Networks (RNNs). Following the success of model-based reinforcement learning algorithms and the rapid adoption of the Transformer... | Maxime Burchi, Radu Timofte |  |
| 482 |  |  [Multimodality Helps Few-shot 3D Point Cloud Semantic Segmentation](https://openreview.net/forum?id=jXvwJ51vcK) |  | 0 | Few-shot 3D point cloud segmentation (FS-PCS) aims at generalizing models to segment novel categories with minimal annotated support samples. While existing FS-PCS methods have shown promise, they primarily focus on unimodal point cloud inputs, overlooking the potential benefits of leveraging... | Ender Konukoglu, Guolei Sun, Min Wu, MingMing Cheng, Runjia Li, Serge J. Belongie, Yun Liu, Zhaochong An |  |
| 483 |  |  [MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark](https://openreview.net/forum?id=TeVAZXr3yv) |  | 0 | The ability to comprehend audio—which includes speech, non-speech sounds, and music—is crucial for AI agents to interact effectively with the world. We present MMAU, a novel benchmark designed to evaluate multimodal audio understanding models on tasks requiring expert-level knowledge and complex... | Ashish Seth, Dinesh Manocha, Oriol Nieto, Ramaneswaran Selvakumar, Ramani Duraiswami, S. Sakshi, Sonal Kumar, Sreyan Ghosh, Utkarsh Tyagi |  |
| 484 |  |  [Higher-Order Graphon Neural Networks: Approximation and Cut Distance](https://openreview.net/forum?id=SjufxrSOYd) |  | 0 | Graph limit models, like \*graphons\* for limits of dense graphs, have recently been used to study size transferability of graph neural networks (GNNs). While most literature focuses on message passing GNNs (MPNNs), in this work we attend to the more powerful \*higher-order\* GNNs. First, we extend... | Daniel Herbst, Stefanie Jegelka |  |
| 485 |  |  [Towards General-Purpose Model-Free Reinforcement Learning](https://openreview.net/forum?id=R1hIXdST22) |  | 0 | Reinforcement learning (RL) promises a framework for near-universal problem-solving. In practice however, RL algorithms are often tailored to specific benchmarks, relying on carefully tuned hyperparameters and algorithmic choices. Recently, powerful model-based RL methods have shown impressive... | Amy Zhang, Michael Rabbat, Pierluca D'Oro, Scott Fujimoto, Yuandong Tian |  |
| 486 |  |  [Multi-Field Adaptive Retrieval](https://openreview.net/forum?id=3PDklqqqfN) |  | 0 | Document retrieval for tasks such as search and retrieval-augmented generation typically involves datasets that are _unstructured_: free-form text without explicit internal structure in each document. However, documents can have some structure, containing fields such as an article title, a message... | Benjamin Van Durme, Millicent Li, Patrick Xia, Tongfei Chen |  |
| 487 |  |  [Scaling FP8 training to trillion-token LLMs](https://openreview.net/forum?id=E1EHO0imOb) |  | 0 | We train, for the first time, large language models using FP8 precision on datasets up to 2 trillion tokens --- a 20-fold increase over previous limits. Through these extended training runs, we uncover critical instabilities in FP8 training that were not observable in earlier works with shorter... | Brian Chmiel, Daniel Soudry, Maxim Fishman, Ron Banner |  |
| 488 |  |  [Topograph: An Efficient Graph-Based Framework for Strictly Topology Preserving Image Segmentation](https://openreview.net/forum?id=Q0zmmNNePz) |  | 0 | Topological correctness plays a critical role in many image segmentation tasks, yet most networks are trained using pixel-wise loss functions, such as Dice, neglecting topological accuracy. Existing topology-aware methods often lack robust topological guarantees, are limited to specific use cases,... | Alexander H. Berger, Alexander Weers, Daniel Rueckert, Johannes C. Paetzold, Laurin Lux, Nico Stucki, Ulrich Bauer |  |
| 489 |  |  [On Disentangled Training for Nonlinear Transform in Learned Image Compression](https://openreview.net/forum?id=U67J0QNtzo) |  | 0 | Learned image compression (LIC) has demonstrated superior rate-distortion (R-D) performance compared to traditional codecs, but is challenged by training inefficiency that could incur more than two weeks to train a state-of-the-art model from scratch. Existing LIC methods overlook the slow... | Chenglin Li, Han Li, Hongkai Xiong, Junni Zou, Maida Cao, Nuowen Kan, Shaohui Li, Wenrui Dai |  |
| 490 |  |  [Decomposition Polyhedra of Piecewise Linear Functions](https://openreview.net/forum?id=vVCHWVBsLH) |  | 0 | In this paper we contribute to the frequently studied question of how to decompose a continuous piecewise linear (CPWL) function into a difference of two convex CPWL functions. Every CPWL function has infinitely many such decompositions, but for applications in optimization and neural network... | Christoph Hertrich, MarieCharlotte Brandenburg, Moritz Leo Grillo |  |
| 491 |  |  [Provably Accurate Shapley Value Estimation via Leverage Score Sampling](https://openreview.net/forum?id=wg3rBImn3O) |  | 0 | Originally introduced in game theory, Shapley values have emerged as a central tool in explainable machine learning, where they are used to attribute model predictions to specific input features. However, computing Shapley values exactly is expensive: for a model with $n$ features, $O(2^n)$ model... | Christopher Musco, R. Teal Witter |  |
| 492 |  |  [Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?](https://openreview.net/forum?id=Cnwz9jONi5) |  | 0 | Reward Models (RMs) are crucial for aligning language models with human preferences. Currently, the evaluation of RMs depends on measuring accuracy against a validation set of manually annotated preference data. Although this method is straightforward and widely adopted, the relationship between RM... | Ben He, Debing Zhang, Hongyu Lin, Jie Lou, Le Sun, Xianpei Han, XingYu, Xinyu Lu, Xueru Wen, Yaojie Lu |  |
| 493 |  |  [Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts](https://openreview.net/forum?id=e1wDDFmlVu) |  | 0 | Deep learning for time series forecasting has seen significant advancements over the past decades. However, despite the success of large-scale pre-training in language and vision domains, pre-trained time series models remain limited in scale and operate at a high cost, hindering the development of... | Dianqi Li, Ming Jin, Qingsong Wen, Shiyu Wang, Xiaoming Shi, Yuqi Nie, Zhou Ye |  |
| 494 |  |  [Generalization Guarantees for Representation Learning via Data-Dependent Gaussian Mixture Priors](https://openreview.net/forum?id=fGdF8Bq1FV) |  | 0 | We establish in-expectation and tail bounds on the generalization error of representation learning type algorithms. The bounds are in terms of the relative entropy between the distribution of the representations extracted from the training and "test'' datasets and a data-dependent symmetric prior,... | Abdellatif Zaidi, Milad Sefidgaran, Piotr Krasnowski |  |
| 495 |  |  [OSDA Agent: Leveraging Large Language Models for De Novo Design of Organic Structure Directing Agents](https://openreview.net/forum?id=9YNyiCJE3k) |  | 0 | Zeolites are crystalline porous materials that have been widely utilized in petrochemical industries as well as sustainable chemistry areas. Synthesis of zeolites often requires small molecules termed Organic Structure Directing Agents (OSDAs), which are critical in forming the porous structure.... | Hehe Fan, Weimin Yang, Xin Li, Yi Yang, Yixiao Zhou, Zhaolin Hu, Zhongan Wang |  |
| 496 |  |  [ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability](https://openreview.net/forum?id=ztzZDzgfrh) |  | 0 | Retrieval-Augmented Generation (RAG) models are designed to incorporate external knowledge, reducing hallucinations caused by insufficient parametric (internal) knowledge. However, even with accurate and relevant retrieved content, RAG models can still produce hallucinations by generating outputs... | Han Li, Jun Xu, Kai Zheng, Weijie Yu, Xiao Zhang, Xiaoxue Zang, Yang Song, Zhongxiang Sun |  |
| 497 |  |  [Accelerating Goal-Conditioned Reinforcement Learning Algorithms and Research](https://openreview.net/forum?id=4gaySj8kvX) |  | 0 | Self-supervision has the potential to transform reinforcement learning (RL), paralleling the breakthroughs it has enabled in other areas of machine learning. While self-supervised learning in other domains aims to find patterns in a fixed dataset, self-supervised goal-conditioned reinforcement... | Benjamin Eysenbach, Lukasz Kucinski, Michal Bortkiewicz, Tadeusz Dziarmaga, Tomasz Arczewski, Vivek Myers, Wladyslaw Palucki |  |
| 498 |  |  [Transformers Learn to Implement Multi-step Gradient Descent with Chain of Thought](https://openreview.net/forum?id=r3DF5sOo5B) |  | 0 | Chain of Thought (CoT) prompting has been shown to significantly improve the performance of large language models (LLMs), particularly in arithmetic and reasoning tasks, by instructing the model to produce intermediate reasoning steps. Despite the remarkable empirical success of CoT and its... | Jason D. Lee, Jianhao Huang, Zixuan Wang |  |
| 499 |  |  [SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning](https://openreview.net/forum?id=jXLiDKsuDo) |  | 0 | Recent advances in CV and NLP have been largely driven by scaling up the number of network parameters, despite traditional theories suggesting that larger networks are prone to overfitting. These large networks avoid overfitting by integrating components that induce a simplicity bias, guiding... | Donghu Kim, Dongyoon Hwang, Hojoon Lee, Hyunseung Kim, Jaegul Choo, Jun Jet Tai, Kaushik Subramanian, Peter R. Wurman, Peter Stone, Takuma Seno |  |
| 500 |  |  [Tuning Frequency Bias of State Space Models](https://openreview.net/forum?id=wkHcXDv7cv) |  | 0 | State space models (SSMs) leverage linear, time-invariant (LTI) systems to effectively learn sequences with long-range dependencies. By analyzing the transfer functions of LTI systems, we find that SSMs exhibit an implicit bias toward capturing low-frequency components more effectively than... | Annan Yu, Dongwei Lyu, Michael W. Mahoney, N. Benjamin Erichson, Soon Hoe Lim |  |
| 501 |  |  [Planning in Natural Language Improves LLM Search for Code Generation](https://openreview.net/forum?id=48WAZhwHHw) |  | 0 | While scaling training compute has led to remarkable improvements in large language models (LLMs), scaling inference compute only recently began to yield analogous gains. We hypothesize that a core missing component is a lack of diverse LLM outputs, leading to inefficient search due to models... | Catherine Wu, Evan Z. Wang, Federico Cassano, Hugh Zhang, Sean M. Hendryx, Summer Yue, Vaskar Nath, William Song, Yunfeng Bai, Ziwen Han |  |
| 502 |  |  [Recovering Manifold Structure Using Ollivier Ricci Curvature](https://openreview.net/forum?id=aX7X9z3vQS) |  | 0 | We introduce ORC-ManL, a new algorithm to prune spurious edges from nearest neighbor graphs using a criterion based on Ollivier-Ricci curvature and estimated metric distortion. Our motivation comes from manifold learning: we show that when the data generating the nearest-neighbor graph consists of... | Abigail Hickok, Andrew J. Blumberg, Tristan Luca Saidi |  |
| 503 |  |  [Improved Approximation Algorithms for k-Submodular Maximization via Multilinear Extension](https://openreview.net/forum?id=EPHsIa0Ytg) |  | 0 | We investigate a generalized form of submodular maximization, referred to as $k$-submodular maximization, with applications across the domains of social networks and machine learning. In this work, we propose the multilinear extension of $k$-submodular functions and unified Frank-Wolfe-type... | Baoxiang Wang, Huanjian Zhou, Lingxiao Huang |  |
| 504 |  |  [Nonlinear multiregion neural dynamics with parametric impulse response communication channels](https://openreview.net/forum?id=LbgIZpSUCe) |  | 0 | Cognition arises from the coordinated interaction of brain regions with distinct computational roles. Despite improvements in our ability to extract the dynamics underlying circuit computation from population activity recorded in individual areas, understanding how multiple areas jointly support... | Cristina Savin, Matthew Dowling |  |
| 505 |  |  [Diffusion On Syntax Trees For Program Synthesis](https://openreview.net/forum?id=wN3KaUXA5X) |  | 0 | Large language models generate code one token at a time. Their autoregressive generation process lacks the feedback of observing the program's output. Training LLMs to suggest edits directly can be challenging due to the scarcity of rich edit data. To address these problems, we propose neural... | Erik Jenner, Shreyas Kapur, Stuart Russell |  |
| 506 |  |  [Scaling up the Banded Matrix Factorization Mechanism for Large Scale Differentially Private ML](https://openreview.net/forum?id=69Fp4dcmJN) |  | 0 | Correlated noise mechanisms such as DP Matrix Factorization (DP-MF) have proven to be effective alternatives to DP-SGD in large-epsilon few-epoch training regimes. Significant work has been done to find the best correlated noise strategies, and the current state-of-the-art approach is DP-BandMF ,... | Ryan McKenna |  |
| 507 |  |  [Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models](https://openreview.net/forum?id=cRR0oDFEBC) |  | 0 | One core capability of large language models~(LLMs) is to follow natural language instructions. However, the issue of automatically constructing high-quality training data to enhance the complex instruction-following abilities of LLMs without manual annotation remains unresolved. In this paper, we... | Bowen Yu, Chang Zhou, Chengpeng Li, Guanting Dong, Jingren Zhou, Keming Lu, Tingyu Xia |  |
| 508 |  |  [How Feature Learning Can Improve Neural Scaling Laws](https://openreview.net/forum?id=dEypApI1MZ) |  | 0 | We develop a simple solvable model of neural scaling laws beyond the kernel limit. Theoretical analysis of this model predicts the performance scaling predictions with model size, training time and total amount of available data. From the scaling analysis we identify three relevant regimes: hard... | Alexander B. Atanasov, Blake Bordelon, Cengiz Pehlevan |  |
| 509 |  |  [A CLIP-Powered Framework for Robust and Generalizable Data Selection](https://openreview.net/forum?id=9bMZ29SPVx) |  | 0 | Large-scale datasets have been pivotal to the advancements of deep learning models in recent years, but training on such large datasets inevitably incurs substantial storage and computational overhead. Meanwhile, real-world datasets often contain redundant and noisy data, imposing a negative impact... | Dongzhan Zhou, Furao Shen, Peng Ye, Suorong Yang, Wanli Ouyang |  |
| 510 |  |  [PABBO: Preferential Amortized Black-Box Optimization](https://openreview.net/forum?id=YhfrKB3Ah7) |  | 0 | Preferential Bayesian Optimization (PBO) is a sample-efficient method to learn latent user utilities from preferential feedback over a pair of designs. It relies on a statistical surrogate model for the latent function, usually a Gaussian process, and an acquisition strategy to select the next... | Daolang Huang, Julien Martinelli, Samuel Kaski, Xinyu Zhang |  |
| 511 |  |  [Test-time Alignment of Diffusion Models without Reward Over-optimization](https://openreview.net/forum?id=vi3DjUhFVm) |  | 0 | Diffusion models excel in generative tasks, but aligning them with specific objectives while maintaining their versatility remains challenging. Existing fine-tuning methods often suffer from reward over-optimization, while approximate guidance approaches fail to optimize target rewards effectively.... | Dongmin Park, Minkyu Kim, Sunwoo Kim |  |
| 512 |  |  [Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures](https://openreview.net/forum?id=nGiGXLnKhl) |  | 0 | Transformers have revolutionized computer vision and natural language processing, but their high computational complexity limits their application in high-resolution image processing and long-context analysis. This paper introduces Vision-RWKV (VRWKV), a model that builds upon the RWKV architecture... | Hongsheng Li, Jifeng Dai, Lewei Lu, Tong Lu, Weiyun Wang, Wenhai Wang, Xizhou Zhu, Yu Qiao, Yuchen Duan, Zhe Chen |  |
| 513 |  |  [GOLD: Graph Out-of-Distribution Detection via Implicit Adversarial Latent Generation](https://openreview.net/forum?id=y5einmJ0Yx) |  | 0 | Despite graph neural networks' (GNNs) great success in modelling graph-structured data, out-of-distribution (OOD) test instances still pose a great challenge for current GNNs. One of the most effective techniques to detect OOD nodes is to expose the detector model with an additional OOD node-set,... | Danny Wang, Guangdong Bai, Ruihong Qiu, Zi Huang |  |
| 514 |  |  [Fine-tuning with Reserved Majority for Noise Reduction](https://openreview.net/forum?id=ZV7CLf0RHK) |  | 0 | Parameter-efficient fine-tuning (PEFT) has revolutionized supervised fine-tuning, where LoRA and its variants gain the most popularity due to their low training costs and zero inference latency. However, LoRA tuning not only injects knowledgeable features but also noisy hallucination during... | Shuyang Jiang, Ya Zhang, Yanfeng Wang, Yu Wang, Yusheng Liao |  |
| 515 |  |  [LLaVA-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models](https://openreview.net/forum?id=oSQiao9GqB) |  | 0 | Visual instruction tuning has made considerable strides in enhancing the capabilities of Large Multimodal Models (LMMs). However, existing open LMMs largely focus on single-image tasks, their applications to multi-image scenarios remains less explored. Additionally, prior LMM research separately... | Bo Li, Chunyuan Li, Feng Li, Hao Zhang, Renrui Zhang, Wei Li, Yuanhan Zhang, Zejun Ma |  |
| 516 |  |  [Formation of Representations in Neural Networks](https://openreview.net/forum?id=Njx1NjHIx4) |  | 0 | Understanding neural representations will help open the black box of neural networks and advance our scientific understanding of modern AI systems. However, how complex, structured, and transferable representations emerge in modern neural networks has remained a mystery. Building on previous... | Isaac L. Chuang, Liu Ziyin, Tomaso A. Poggio, Tomer Galanti |  |
| 517 |  |  [ImpScore: A Learnable Metric For Quantifying The Implicitness Level of Sentences](https://openreview.net/forum?id=gYWqxXE5RJ) |  | 0 | Handling implicit language is essential for natural language processing systems to achieve precise text understanding and facilitate natural interactions with users. Despite its importance, the absence of a metric for accurately measuring the implicitness of language significantly constrains the... | Saeed Hassanpour, Soroush Vosoughi, Weimin Lyu, Xiaomeng Zhu, Yuxin Wang |  |
| 518 |  |  [ADIFF: Explaining audio difference using natural language](https://openreview.net/forum?id=l4fMj4Vnly) |  | 0 | Understanding and explaining differences between audio recordings is crucial for fields like audio forensics, quality assessment, and audio generation. This involves identifying and describing audio events, acoustic scenes, signal characteristics, and their emotional impact on listeners. This paper... | Bhiksha Raj, Rita Singh, Shuo Han, Soham Deshmukh |  |
| 519 |  |  [Enhancing Compositional Text-to-Image Generation with Reliable Random Seeds](https://openreview.net/forum?id=5BSlakturs) |  | 0 | Text-to-image diffusion models have demonstrated remarkable capability in generating realistic images from arbitrary text prompts. However, they often produce inconsistent results for compositional prompts such as "two dogs" or "a penguin on the right of a bowl". Understanding these inconsistencies... | Hieu Le, Jingyi Xu, Mathieu Salzmann, Shuangqi Li |  |
| 520 |  |  [LoRA-Pro: Are Low-Rank Adapters Properly Optimized?](https://openreview.net/forum?id=gTwRMU3lJ5) |  | 0 | Low-rank adaptation, also known as LoRA, has emerged as a prominent method for parameter-efficient fine-tuning of foundation models. Despite its computational efficiency, LoRA still yields inferior performance compared to full fine-tuning. In this paper, we first uncover a fundamental connection... | Jian Liang, Ran He, Tieniu Tan, Zhengbo Wang, Zilei Wang |  |
| 521 |  |  [Large-scale and Fine-grained Vision-language Pre-training for Enhanced CT Image Understanding](https://openreview.net/forum?id=nYpPAT4L3D) |  | 0 | Artificial intelligence (AI) shows great potential in assisting radiologists to improve the efficiency and accuracy of medical image interpretation and diagnosis. However, a versatile AI model requires large-scale data and comprehensive annotations, which are often impractical in medical settings.... | Jianpeng Zhang, Le Lu, Lin Yang, Ling Zhang, Qi Zhang, Ruizhe Guo, Sinuo Wang, Tingbo Liang, Weiwei Cao, Xianghua Ye, Zhongyi Shui |  |
| 522 |  |  [AutoCGP: Closed-Loop Concept-Guided Policies from Unlabeled Demonstrations](https://openreview.net/forum?id=9ehJCZz4aM) |  | 0 | Training embodied agents to perform complex robotic tasks presents significant challenges due to the entangled factors of task compositionality, environmental diversity, and dynamic changes. In this work, we introduce a novel imitation learning framework to train closed-loop concept-guided policies... | Fan Wang, Pei Zhou, Qian Luo, Ruizhe Liu, Yanchao Yang, Yibing Song |  |
| 523 |  |  [Perm: A Parametric Representation for Multi-Style 3D Hair Modeling](https://openreview.net/forum?id=WKfb1xGXGx) |  | 0 | We present Perm, a learned parametric representation of human 3D hair designed to facilitate various hair-related applications. Unlike previous work that jointly models the global hair structure and local curl patterns, we propose to disentangle them using a PCA-based strand representation in the... | Chengan He, Dominik Ludewig Michels, Fujun Luan, Holly E. Rushmeier, Jorge Alejandro Amador Herrera, Meng Zhang, Sören Pirk, Tuanfeng Yang Wang, Xin Sun, Yi Zhou, Zhixin Shu |  |
| 524 |  |  [Easing Training Process of Rectified Flow Models Via Lengthening Inter-Path Distance](https://openreview.net/forum?id=RaR3ETzyKp) |  | 0 | Recent research pinpoints that different diffusion methods and architectures trained on the same dataset produce similar results for the same input noise. This property suggests that they have some preferable noises for a given sample. By visualizing the noise-sample pairs of rectified flow models... | Adams WaiKin Kong, Shifeng Xu, Yanzhu Liu |  |
| 525 |  |  [GETS: Ensemble Temperature Scaling for Calibration in Graph Neural Networks](https://openreview.net/forum?id=qgsXsqahMq) |  | 0 | Graph Neural Networks (GNNs) deliver strong classification results but often suffer from poor calibration performance, leading to overconfidence or underconfidence. This is particularly problematic in high-stakes applications where accurate uncertainty estimates are essential. Existing post-hoc... | Chonghe Jiang, Dingyi Zhuang, Jinhua Zhao, Shenhao Wang, Yunhan Zheng |  |
| 526 |  |  [Conditional Diffusion with Ordinal Regression: Longitudinal Data Generation for Neurodegenerative Disease Studies](https://openreview.net/forum?id=9UGfOJBuL8) |  | 0 | Modeling the progression of neurodegenerative diseases such as Alzheimer’s disease (AD) is crucial for early detection and prevention given their irreversible nature. However, the scarcity of longitudinal data and complex disease dynamics make the analysis highly challenging. Moreover, longitudinal... | Guorong Wu, Hyuna Cho, Seungjoo Lee, Tingting Dan, Won Hwa Kim, Ziquan Wei |  |
| 527 |  |  [SoftCVI: Contrastive variational inference with self-generated soft labels](https://openreview.net/forum?id=PiZtlzMWUj) |  | 0 | Estimating a distribution given access to its unnormalized density is pivotal in Bayesian inference, where the posterior is generally known only up to an unknown normalizing constant. Variational inference and Markov chain Monte Carlo methods are the predominant tools for this task; however, both... | Daniel Ward, Mark Beaumont, Matteo Fasiolo |  |
| 528 |  |  [MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion](https://openreview.net/forum?id=lJpqxFgWCM) |  | 0 | Estimating geometry from dynamic scenes, where objects move and deform over time, remains a core challenge in computer vision. Current approaches often rely on multi-stage pipelines or global optimizations that decompose the problem into subtasks, like depth and flow, leading to complex systems... | Charles Herrmann, Deqing Sun, Forrester Cole, Junhwa Hur, Junyi Zhang, MingHsuan Yang, Trevor Darrell, Varun Jampani |  |
| 529 |  |  [ND-SDF: Learning Normal Deflection Fields for High-Fidelity Indoor Reconstruction](https://openreview.net/forum?id=4HRRcqE9SU) |  | 0 | Neural implicit reconstruction via volume rendering has demonstrated its effectiveness in recovering dense 3D surfaces. However, it is non-trivial to simultaneously recover meticulous geometry and preserve smoothness across regions with differing characteristics. To address this issue, previous... | Di Huang, Guofeng Zhang, Hujun Bao, Tong He, Weicai Ye, Yifan Wang, Ziyu Tang |  |
| 530 |  |  [SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation](https://openreview.net/forum?id=UL8b54P96G) |  | 0 | Human beings are endowed with a complementary learning system, which bridges the slow learning of general world dynamics with fast storage of episodic memory from a new experience. Previous video generation models, however, primarily focus on slow learning by pre-training on vast amounts of data,... | Beide Liu, ChungChing Lin, Jianfeng Wang, KaiWei Chang, Kevin Lin, Lijuan Wang, Linjie Li, Maxine Wu, Ying Nian Wu, Yining Hong, Yuanhao Zhai, Zhengyuan Yang |  |
| 531 |  |  [Atlas Gaussians Diffusion for 3D Generation](https://openreview.net/forum?id=H2Gxil855b) |  | 0 | Using the latent diffusion model has proven effective in developing novel 3D generation techniques. To harness the latent diffusion model, a key challenge is designing a high-fidelity and efficient representation that links the latent space and the 3D space. In this paper, we introduce Atlas... | Dejia Xu, Georgios Pavlakos, Haitao Yang, Hanwen Jiang, Qixing Huang, Yuan Dong |  |
| 532 |  |  [Min-K%++: Improved Baseline for Pre-Training Data Detection from Large Language Models](https://openreview.net/forum?id=ZGkfoufDaU) |  | 0 | The problem of pre-training data detection for large language models (LLMs) has received growing attention due to its implications in critical issues like copyright violation and test data contamination. Despite improved performance, existing methods (including the state-of-the-art, Min-K%) are... | Eric C. Yeats, Hai Li, Hao (Frank) Yang, Jianyi Zhang, Jingwei Sun, Jingyang Zhang, Martin Kuo, Yang Ouyang |  |
| 533 |  |  [4K4DGen: Panoramic 4D Generation at 4K Resolution](https://openreview.net/forum?id=qxRoo7ULCo) |  | 0 | The blooming of virtual reality and augmented reality (VR/AR) technologies has driven an increasing demand for the creation of high-quality, immersive, and dynamic environments. However, existing generative techniques either focus solely on dynamic objects or perform outpainting from a single... | Achuta Kadambi, Bangbang Yang, Dejia Xu, Panwang Pan, Renjie Li, Shijie Zhou, Xuanyang Zhang, Zeming Li, Zhangyang Wang, Zhengzhong Tu, Zhiwen Fan |  |
| 534 |  |  [Samba: Synchronized Set-of-Sequences Modeling for Multiple Object Tracking](https://openreview.net/forum?id=OeBY9XqiTz) |  | 0 | Multiple object tracking in complex scenarios - such as coordinated dance performances, team sports, or dynamic animal groups - presents unique challenges. In these settings, objects frequently move in coordinated patterns, occlude each other, and exhibit long-term dependencies in their... | Bernt Schiele, Luc Van Gool, Luigi Piccinelli, Mattia Segù, Siyuan Li, YungHsu Yang |  |
| 535 |  |  [Programming Refusal with Conditional Activation Steering](https://openreview.net/forum?id=Oi47wc10sm) |  | 0 | LLMs have shown remarkable capabilities, but precisely controlling their response behavior remains challenging. Existing activation steering methods alter LLM behavior indiscriminately, limiting their practical applicability in settings where selective responses are essential, such as content... | Amit Dhurandhar, Bruce W. Lee, Erik Miehling, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Manish Nagireddy, Pierre L. Dognin |  |
| 536 |  |  [Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders](https://openreview.net/forum?id=Y2RW9EVwhT) |  | 0 | The ability to accurately interpret complex visual information is a crucial topic of multimodal large language models (MLLMs). Recent work indicates that enhanced visual perception significantly reduces hallucinations and improves performance on resolution-sensitive tasks, such as optical character... | Andrew Tao, Bryan Catanzaro, DeAn Huang, Fuxiao Liu, Guilin Liu, Hongxu Yin, Humphrey Shi, Jan Kautz, Karan Sapra, Min Shi, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, Yaser Yacoob, Yilin Zhao, Zhiding Yu |  |
| 537 |  |  [PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training](https://openreview.net/forum?id=j4LITBSUjs) |  | 0 | This paper aims to address the challenge of hallucinations in Multimodal Large Language Models (MLLMs) particularly for dense image captioning tasks. To tackle the challenge, we identify the current lack of a metric that finely measures the caption quality in concept level. We hereby introduce... | Bo Zhang, Chenchen Jing, Chunhua Shen, Cong Chen, Fengyun Rao, Hao Chen, Mingyu Liu, Yizhou Zhou |  |
| 538 |  |  [CLoSD: Closing the Loop between Simulation and Diffusion for multi-task character control](https://openreview.net/forum?id=pZISppZSTv) |  | 0 | Motion diffusion models and Reinforcement Learning (RL) based control for physics-based simulations have complementary strengths for human motion generation. The former is capable of generating a wide variety of motions, adhering to intuitive control such as text, while the latter offers physically... | Amit Haim Bermano, Daniele Reda, Guy Tevet, Michiel van de Panne, Setareh Cohan, Sigal Raab, Xue Bin Peng, Zhengyi Luo |  |
| 539 |  |  [Linear SCM Identification in the Presence of Confounders and Gaussian Noise](https://openreview.net/forum?id=bjxuqI4KwU) |  | 0 | Noisy linear structural causal models (SCMs) in the presence of confounding variables are known to be identifiable if all confounding and noise variables are non-Gaussian and unidentifiable if all are Gaussian. The identifiability when only some are Gaussian remains concealed. We show that, in the... | Pouria Ramazi, Vahideh Sanjaroonpouri |  |
| 540 |  |  [Geometric Inductive Biases of Deep Networks: The Role of Data and Architecture](https://openreview.net/forum?id=cmXWYolrlo) |  | 0 | In this paper, we propose the \*geometric invariance hypothesis (GIH)\*, which argues that the input space curvature of a neural network remains invariant under transformation in certain architecture-dependent directions during training. We investigate a simple, non-linear binary classification... | Antonio Orvieto, Sajad Movahedi, SeyedMohsen MoosaviDezfooli |  |
| 541 |  |  [MotionAura: Generating High-Quality and Motion Consistent Videos using Discrete Diffusion](https://openreview.net/forum?id=bW9fGYo44s) |  | 0 | The spatio-temporal complexity of video data presents significant challenges in tasks such as compression, generation, and inpainting. We present four key contributions to address the challenges of spatiotemporal video processing. First, we introduce the 3D Mobile Inverted Vector-Quantization... | Chirag Sehgal, Jishu Sen Gupta, Onkar Kishor Susladkar, Rekha Singhal, Sparsh Mittal |  |
| 542 |  |  [Rare-to-Frequent: Unlocking Compositional Generation Power of Diffusion Models on Rare Concepts with LLM Guidance](https://openreview.net/forum?id=BgxsmpVoOX) |  | 0 | State-of-the-art text-to-image (T2I) diffusion models often struggle to generate rare compositions of concepts, e.g., objects with unusual attributes. In this paper, we show that the compositional generation power of diffusion models on such rare concepts can be significantly enhanced by the Large... | Dongmin Park, Jaewoong Cho, Kangwook Lee, Minkyu Kim, Sebin Kim, Taehong Moon |  |
| 543 |  |  [POTEC: Off-Policy Contextual Bandits for Large Action Spaces via Policy Decomposition](https://openreview.net/forum?id=LXftdR11io) |  | 0 | We study off-policy learning (OPL) of contextual bandit policies in large discrete action spaces where existing methods -- most of which rely crucially on reward-regression models or importance-weighted policy gradients -- fail due to excessive bias or variance. To overcome these issues in OPL, we... | Jihan Yao, Thorsten Joachims, Yuta Saito |  |
| 544 |  |  [MetaUrban: An Embodied AI Simulation Platform for Urban Micromobility](https://openreview.net/forum?id=kFsWpSxkFz) |  | 0 | Public urban spaces such as streetscapes and plazas serve residents and accommodate social life in all its vibrant variations. Recent advances in robotics and embodied AI make public urban spaces no longer exclusive to humans. Food delivery bots and electric wheelchairs have started sharing... | Bolei Zhou, Chenda Duan, Honglin He, Jack He, Quanyi Li, Wayne Wu, Yiran Wang, Zhizheng Liu |  |
| 545 |  |  [D-FINE: Redefine Regression Task of DETRs as Fine-grained Distribution Refinement](https://openreview.net/forum?id=MFZjrTFE7h) |  | 0 | We introduce D-FINE, a powerful real-time object detector that achieves outstanding localization precision by redefining the bounding box regression task in DETR models. D-FINE comprises two key components: Fine-grained Distribution Refinement (FDR) and Global Optimal Localization Self-Distillation... | Feng Wu, Hebei Li, Peixi Wu, Xiaoyan Sun, Yansong Peng, Yueyi Zhang |  |
| 546 |  |  [DartControl: A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control](https://openreview.net/forum?id=XNA3Mnnbvb) |  | 0 | Text-conditioned human motion generation, which allows for user interaction through natural language, has become increasingly popular. Existing methods typically generate short, isolated motions based on a single input sentence. However, human motions are continuous and can extend over long... | Gen Li, Kaifeng Zhao, Siyu Tang |  |
| 547 |  |  [Mitigating Information Loss in Tree-Based Reinforcement Learning via Direct Optimization](https://openreview.net/forum?id=qpXctF2aLZ) |  | 0 | Reinforcement learning (RL) has seen significant success across various domains, but its adoption is often limited by the black-box nature of neural network policies, making them difficult to interpret. In contrast, symbolic policies allow representing decision-making strategies in a compact and... | Christian Bartelt, Florian Vogt, Heiner Stuckenschmidt, Sascha Marton, Stefan Lüdtke, Tim Grams |  |
| 548 |  |  [Sharpness-Aware Minimization Efficiently Selects Flatter Minima Late In Training](https://openreview.net/forum?id=aD2uwhLbnA) |  | 0 | Sharpness-Aware Minimization (SAM) has substantially improved the generalization of neural networks under various settings. Despite the success, its effectiveness remains poorly understood. In this work, we discover an intriguing phenomenon in the training dynamics of SAM, shedding light on... | Bingrui Li, Junchi Yan, Mingze Wang, Yuchen Mao, Zhanpeng Zhou |  |
| 549 |  |  [What Makes a Good Diffusion Planner for Decision Making?](https://openreview.net/forum?id=7BQkXXM8Fy) |  | 0 | Diffusion models have recently shown significant potential in solving decision-making problems, particularly in generating behavior plans -- also known as diffusion planning. While numerous studies have demonstrated the impressive performance of diffusion planning, the mechanisms behind the key... | Dongqi Han, Dongsheng Li, Haofei Lu, Yifei Shen |  |
| 550 |  |  [ThinK: Thinner Key Cache by Query-Driven Pruning](https://openreview.net/forum?id=n0OtGl6VGb) |  | 0 | Large Language Models (LLMs) have revolutionized the field of natural language processing, achieving unprecedented performance across a variety of applications. However, their increased computational and memory demands present significant challenges, especially when handling long sequences. This... | Amrita Saha, Aojun Zhou, Caiming Xiong, Doyen Sahoo, Hanze Dong, Lei Wang, Xudong Lu, Yuhui Xu, Zhanming Jie |  |
| 551 |  |  [Linear Spherical Sliced Optimal Transport: A Fast Metric for Comparing Spherical Data](https://openreview.net/forum?id=fgUFZAxywx) |  | 0 | Efficient comparison of spherical probability distributions becomes important in fields such as computer vision, geosciences, and medicine. Sliced optimal transport distances, such as spherical and stereographic spherical sliced Wasserstein distances, have recently been developed to address this... | Ashkan Shahbazi, Bennett Allan Landman, Catie Chang, Kaiwen Shi, Rocio Diaz Martin, Soheil Kolouri, Xinran Liu, Yikun Bai |  |
| 552 |  |  [RelitLRM: Generative Relightable Radiance for Large Reconstruction Models](https://openreview.net/forum?id=3Oli4u6q3p) |  | 0 | We propose RelitLRM, a Large Reconstruction Model (LRM) for generating high-quality Gaussian splatting representations of 3D objects under novel illuminations from sparse (4-8) posed images captured under unknown static lighting. Unlike prior inverse rendering methods requiring dense captures and... | Fujun Luan, Haian Jin, Hao Tan, He Zhang, Kai Zhang, Milos Hasan, Sai Bi, Tianyuan Zhang, William T. Freeman, Yiwei Hu, Zexiang Xu, Zhengfei Kuang |  |
| 553 |  |  [A Geometric Framework for Understanding Memorization in Generative Models](https://openreview.net/forum?id=aZ1gNJu8wO) |  | 0 | As deep generative models have progressed, recent work has shown them to be capable of memorizing and reproducing training datapoints when deployed. These findings call into question the usability of generative models, especially in light of the legal and privacy risks brought about by... | Brendan Leigh Ross, Gabriel LoaizaGanem, George Stein, Hamidreza Kamkari, Jesse C. Cresswell, Rasa Hosseinzadeh, Tongzi Wu, Zhaoyan Liu |  |
| 554 |  |  [Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control](https://openreview.net/forum?id=xQBRrtQM8u) |  | 0 | Dynamical generative models that produce samples through an iterative process, such as Flow Matching and denoising diffusion models, have seen widespread use, but there have not been many theoretically-sound methods for improving these models with reward fine-tuning. In this work, we cast reward... | Brian Karrer, Carles DomingoEnrich, Michal Drozdzal, Ricky T. Q. Chen |  |
| 555 |  |  [ConFIG: Towards Conflict-free Training of Physics Informed Neural Networks](https://openreview.net/forum?id=APojAzJQiq) |  | 0 | The loss functions of many learning problems contain multiple additive terms that can disagree and yield conflicting update directions. For Physics-Informed Neural Networks (PINNs), loss terms on initial/boundary conditions and physics equations are particularly interesting as they are... | Mengyu Chu, Nils Thuerey, Qiang Liu |  |
| 556 |  |  [DLEFT-MKC: Dynamic Late Fusion Multiple Kernel Clustering with Robust Tensor Learning via Min-Max Optimization](https://openreview.net/forum?id=HE5JmwniHm) |  | 0 | Recent advancements in multiple kernel clustering (MKC) have highlighted the effectiveness of late fusion strategies, particularly in enhancing computational efficiency to near-linear complexity while achieving promising clustering performance. However, existing methods encounter three significant... | En Zhu, Jiyuan Liu, Shengju Yu, Siwei Wang, Suyuan Liu, Xinwang Liu, Yi Zhang, Zhibin Dong |  |
| 557 |  |  [Hymba: A Hybrid-head Architecture for Small Language Models](https://openreview.net/forum?id=A1ztozypga) |  | 0 | We propose Hymba, a family of small language models featuring a hybrid-head parallel architecture that integrates attention mechanisms and state space models (SSMs) within the same layer, offering parallel and complementary processing of the same inputs. In this hybrid-head module, attention heads... | Ameya Sunil Mahabaleshwarkar, Jan Kautz, Matthijs Van Keirsbilck, MinHung Chen, Pavlo Molchanov, ShihYang Liu, Shizhe Diao, Wonmin Byeon, Xin Dong, Yingyan Celine Lin, Yonggan Fu, Yoshi Suhara, Zijia Chen |  |
| 558 |  |  [Severing Spurious Correlations with Data Pruning](https://openreview.net/forum?id=Bk13Qfu8Ru) |  | 0 | Deep neural networks have been shown to learn and rely on spurious correlations present in the data that they are trained on. Reliance on such correlations can cause these networks to malfunction when deployed in the real world, where these correlations may no longer hold. To overcome the learning... | JungEun Kim, Varun Mulchandani |  |
| 559 |  |  [CubeDiff: Repurposing Diffusion-Based Image Models for Panorama Generation](https://openreview.net/forum?id=M2SsqpxGtc) |  | 0 | We introduce a novel method for generating 360° panoramas from text prompts or images. Our approach leverages recent advances in 3D generation by employing multi-view diffusion models to jointly synthesize the six faces of a cubemap. Unlike previous methods that rely on processing equirectangular... | Fabian Manhardt, Federico Tombari, Konrad Schindler, Michael Oechsle, Nikolai Kalischek, Philipp Henzler |  |
| 560 |  |  [Language Model Alignment in Multilingual Trolley Problems](https://openreview.net/forum?id=VEqPDZIDAh) |  | 0 | We evaluate the moral alignment of large language models (LLMs) with human preferences in multilingual trolley problems. Building on the Moral Machine experiment, which captures over 40 million human judgments across 200+ countries, we develop a cross-lingual corpus of moral dilemma vignettes in... | András Strausz, Bernhard Schölkopf, Fernando Gonzalez Adauto, Francesco Ortu, Giorgio Piatti, Jiarui Liu, Max KleimanWeiner, Mrinmaya Sachan, Rada Mihalcea, Sydney Levine, Yejin Choi, Zhijing Jin |  |
| 561 |  |  [Century: A Framework and Dataset for Evaluating Historical Contextualisation of Sensitive Images](https://openreview.net/forum?id=1KLBvrYz3V) |  | 0 | How do multi-modal generative models describe images of recent historical events and figures, whose legacies may be nuanced, multifaceted, or contested? This task necessitates not only accurate visual recognition, but also socio-cultural knowledge and cross-modal reasoning. To address this... | Canfer Akbulut, Iason Gabriel, Isabela Albuquerque, Kevin Robinson, Laura Weidinger, Lisa Anne Hendricks, Maribeth Rauh, Nahema Marchal, Olivia Wiles, Verena Rieser, William Isaac, Yana Hasson |  |
| 562 |  |  [Determine-Then-Ensemble: Necessity of Top-k Union for Large Language Model Ensembling](https://openreview.net/forum?id=FDnZFpHmU4) |  | 0 | Large language models (LLMs) exhibit varying strengths and weaknesses across different tasks, prompting recent studies to explore the benefits of ensembling models to leverage their complementary advantages. However, existing LLM ensembling methods often overlook model compatibility and struggle... | Han Wu, Jie Liu, Linqi Song, Mingyang Liu, Sichun Luo, Xiongwei Han, Yuxuan Yao, Zhijiang Guo |  |
| 563 |  |  [MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code](https://openreview.net/forum?id=1Iuw1jcIrf) |  | 0 | Code has been shown to be effective in enhancing the mathematical reasoning abilities of large language models due to its precision and accuracy. Previous works involving continued mathematical pretraining often include code that utilizes math-related packages, which are primarily designed for... | Aojun Zhou, Hongsheng Li, Houxing Ren, Junting Pan, Ke Wang, Mingjie Zhan, Weikang Shi, Zimu Lu |  |
| 564 |  |  [Training-Free Activation Sparsity in Large Language Models](https://openreview.net/forum?id=dGVZwyq5tV) |  | 0 | Activation sparsity can enable practical inference speedups in large language models (LLMs) by reducing the compute and memory-movement required for matrix multiplications during the forward pass. However, existing methods face limitations that inhibit widespread adoption. Some approaches are... | Ben Athiwaratkun, Han Guo, James Liu, Pragaash Ponnusamy, Tianle Cai, Yoon Kim |  |
| 565 |  |  [Poison-splat: Computation Cost Attack on 3D Gaussian Splatting](https://openreview.net/forum?id=ExrEw8cVlU) |  | 0 | 3D Gaussian splatting (3DGS), known for its groundbreaking performance and efficiency, has become a dominant 3D representation and brought progress to many 3D vision tasks. However, in this work, we reveal a significant security vulnerability that has been largely overlooked in 3DGS: the... | Jiahao Lu, Qiuhong Shen, Shuicheng Yan, Xinchao Wang, Yifan Zhang |  |
| 566 |  |  [Gap-Dependent Bounds for Q-Learning using Reference-Advantage Decomposition](https://openreview.net/forum?id=6tyPSkshtF) |  | 0 | We study the gap-dependent bounds of two important algorithms for on-policy $Q$-learning for finite-horizon episodic tabular Markov Decision Processes (MDPs): UCB-Advantage (Zhang et al. 2020) and Q-EarlySettled-Advantage (Li et al. 2021). UCB-Advantage and Q-EarlySettled-Advantage improve upon the... | Haochen Zhang, Lingzhou Xue, Zhong Zheng |  |
| 567 |  |  [Effective Interplay between Sparsity and Quantization: From Theory to Practice](https://openreview.net/forum?id=wJv4AIt4sK) |  | 0 | The increasing size of deep neural networks (DNNs) necessitates effective model compression to reduce their computational and memory footprints. Sparsity and quantization are two prominent compression methods that have been shown to reduce DNNs' computational and memory footprints significantly... | Amir Yazdanbakhsh, Ayan Chakraborty, Babak Falsafi, Danila Mishin, Dongho Ha, Elizaveta Kostenok, Martin Jaggi, Ming Liu, Simla Burcu Harma, Suvinay Subramanian, Yunho Oh |  |
| 568 |  |  [One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation Using a Single Prompt](https://openreview.net/forum?id=cD1kl2QKv1) |  | 0 | Text-to-image generation models can create high-quality images from input prompts. However, they struggle to support the consistent generation of identity-preserving requirements for storytelling. Existing approaches to this problem typically require extensive training in large datasets or... | Fahad Shahbaz Khan, Jian Yang, Joost van de Weijer, Kai Wang, MingMing Cheng, Senmao Li, Shiqi Yang, Tao Liu, Yaxing Wang |  |
| 569 |  |  [GrabS: Generative Embodied Agent for 3D Object Segmentation without Scene Supervision](https://openreview.net/forum?id=wXSshrxlP4) |  | 0 | We study the hard problem of 3D object segmentation in complex point clouds without requiring human labels of 3D scenes for supervision. By relying on the similarity of pretrained 2D features or external signals such as motion to group 3D points as objects, existing unsupervised methods are usually... | Bo Yang, Hongtao Wen, Yafei Yang, Zihui Zhang |  |
| 570 |  |  [Scalable and Certifiable Graph Unlearning: Overcoming the Approximation Error Barrier](https://openreview.net/forum?id=pPyJyeLriR) |  | 0 | Graph unlearning has emerged as a pivotal research area for ensuring privacy protection, given the widespread adoption of Graph Neural Networks (GNNs) in applications involving sensitive user data. Among existing studies, certified graph unlearning is distinguished by providing robust privacy... | Lu Yi, Zhewei Wei |  |
| 571 |  |  [Enhancing Pre-trained Representation Classifiability can Boost its Interpretability](https://openreview.net/forum?id=GjfIZan5jN) |  | 0 | The visual representation of a pre-trained model prioritizes the classifiability on downstream tasks, while the widespread applications for pre-trained visual models have posed new requirements for representation interpretability. However, it remains unclear whether the pre-trained representations... | Junshu Sun, Qi Tian, Qingming Huang, Shufan Shen, Shuhui Wang, Zhaobo Qi |  |
| 572 |  |  [TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters](https://openreview.net/forum?id=oQ4igHyh3N) |  | 0 | Transformers have become the predominant architecture in foundation models due to their excellent performance across various domains. However, the substantial cost of scaling these models remains a significant concern. This problem arises primarily from their dependence on a fixed number of... | Bernt Schiele, Federico Tombari, Haiyang Wang, Jan Eric Lenssen, Liwei Wang, Muhammad Ferjad Naeem, Yongqin Xian, Yue Fan |  |
| 573 |  |  [Simple yet Effective Incomplete Multi-view Clustering: Similarity-level Imputation and Intra-view Hybrid-group Prototype Construction](https://openreview.net/forum?id=KijslFbfOL) |  | 0 | Most of incomplete multi-view clustering (IMVC) methods typically choose to ignore the missing samples and only utilize observed unpaired samples to construct bipartite similarity. Moreover, they employ a single quantity of prototypes to extract the information of $\textbf{all}$ views. To eliminate... | Naiyang Guan, Pei Zhang, Shengju Yu, Siwei Wang, Tiejun Li, Xinwang Liu, Yi Zhang, Yiuming Cheung, Zhibin Dong |  |
| 574 |  |  [Stem-OB: Generalizable Visual Imitation Learning with Stem-Like Convergent Observation through Diffusion Inversion](https://openreview.net/forum?id=xaYlO03tIk) |  | 0 | Visual imitation learning methods demonstrate strong performance, yet they lack generalization when faced with visual input perturbations like variations in lighting and textures. This limitation hampers their practical application in real-world settings. To address this, we propose... | Huazhe Xu, Kaizhe Hu, Pu Hua, Yao He, Yuyao Liu, Zihang Rui |  |
| 575 |  |  [LiveBench: A Challenging, Contamination-Limited LLM Benchmark](https://openreview.net/forum?id=sKYHBTAxVa) |  | 0 | Test set contamination, wherein test data from a benchmark ends up in a newer model's training set, is a well-documented obstacle for fair LLM evaluation and can quickly render benchmarks obsolete. To mitigate this, many recent benchmarks crowdsource new prompts and evaluations from human or LLM... | Arka Pal, Benjamin Feuer, Chinmay Hegde, Colin White, Khalid Saifullah, Manley Roberts, Micah Goldblum, Neel Jain, Ravid ShwartzZiv, Samuel Dooley, Sandeep Singh Sandha, ShubhAgrawal, Siddartha V. Naidu, Siddhartha Jain, Sreemanti Dey, Tom Goldstein, Willie Neiswanger, Yann LeCun |  |
| 576 |  |  [Anti-Exposure Bias in Diffusion Models](https://openreview.net/forum?id=MtDd7rWok1) |  | 0 | Diffusion models (DMs) have achieved record-breaking performance in image generation tasks. Nevertheless, in practice, the training-sampling discrepancy, caused by score estimation error and discretization error, limits the modeling ability of DMs, a phenomenon known as exposure bias. To alleviate... | Chang Xu, Daochang Liu, Eunbyung Park, Junyu Zhang, Shichao Zhang |  |
| 577 |  |  [DynamicCity: Large-Scale 4D Occupancy Generation from Dynamic Scenes](https://openreview.net/forum?id=M7KyLjuN0A) |  | 0 | Urban scene generation has been developing rapidly recently. However, existing methods primarily focus on generating static and single-frame scenes, overlooking the inherently dynamic nature of real-world driving environments. In this work, we introduce DynamicCity, a novel 4D occupancy generation... | Haozhe Xie, Hengwei Bian, Liang Pan, Lingdong Kong, Yu Qiao, Ziwei Liu |  |
| 578 |  |  [Computational Explorations of Total Variation Distance](https://openreview.net/forum?id=xak8c9l1nu) |  | 0 | We investigate some previously unexplored (or underexplored) computational aspects of total variation (TV) distance. First, we give a simple deterministic polynomial-time algorithm for checking equivalence between mixtures of product distributions, over arbitrary alphabets. This corresponds to a... | Aduri Pavan, Arnab Bhattacharyya, Dimitrios Myrisiotis, Kuldeep S. Meel, N. V. Vinodchandran, Sutanu Gayen |  |
| 579 |  |  [Reti-Diff: Illumination Degradation Image Restoration with Retinex-based Latent Diffusion Model](https://openreview.net/forum?id=kxFtMHItrf) |  | 0 | Illumination degradation image restoration (IDIR) techniques aim to improve the visibility of degraded images and mitigate the adverse effects of deteriorated illumination. Among these algorithms, diffusion-based models (DM) have shown promising performance but are often burdened by heavy... | Chengyu Fang, Chunming He, Jinfa Huang, Kai Li, Longxiang Tang, Sina Farsiu, Xiu Li, Yulun Zhang, Zhenhua Guo |  |
| 580 |  |  [MaRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers](https://openreview.net/forum?id=yVeNBxwL5W) |  | 0 | In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of... | Ao Li, Ge Yang, Hongbo Zhao, Le Lu, Minfeng Xu, Wei Fang |  |
| 581 |  |  [SRSA: Skill Retrieval and Adaptation for Robotic Assembly Tasks](https://openreview.net/forum?id=RInisw1yin) |  | 0 | Enabling robots to learn novel tasks in a data-efficient manner is a long-standing challenge. Common strategies involve carefully leveraging prior experiences, especially transition data collected on related tasks. Although much progress has been made for general pick-and-place manipulation, far... | Abhishek Gupta, Bingjie Tang, Dieter Fox, Iretiayo Akinola, Yashraj Narang, Yijie Guo |  |
| 582 |  |  [SVDQuant: Absorbing Outliers by Low-Rank Component for 4-Bit Diffusion Models](https://openreview.net/forum?id=vWR3KuiQur) |  | 0 | Diffusion models can effectively generate high-quality images. However, as they scale, rising memory demands and higher latency pose substantial deployment challenges. In this work, we aim to accelerate diffusion models by quantizing their weights and activations to 4 bits. At such an aggressive... | Chenlin Meng, Enze Xie, JunYan Zhu, Junxian Guo, Muyang Li, Song Han, Tianle Cai, Xiuyu Li, Yujun Lin, Zhekai Zhang |  |
| 583 |  |  [DenseMatcher: Learning 3D Semantic Correspondence for Category-Level Manipulation from a Single Demo](https://openreview.net/forum?id=8oFvUBvF1u) |  | 0 | Dense 3D correspondence can enhance robotic manipulation by enabling the generalization of spatial, functional, and dynamic information from one object to an unseen counterpart. Compared to shape correspondence, semantic correspondence is more effective in generalizing across different object... | Huazhe Xu, Junyi Zhang, Junzhe Zhu, Kaizhe Hu, Muhan Wang, Yuanchen Ju, Zhecheng Yuan |  |
| 584 |  |  [LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression](https://openreview.net/forum?id=PpYy0dR3Qw) |  | 0 | In $D$istributed optimization and $L$earning, and even more in the modern framework of federated learning, communication, which is slow and costly, is critical. We introduce LoCoDL, a communication-efficient algorithm that leverages the two popular and effective techniques of $Lo$cal training,... | Arto Maranjyan, Laurent Condat, Peter Richtárik |  |
| 585 |  |  [LeFusion: Controllable Pathology Synthesis via Lesion-Focused Diffusion Models](https://openreview.net/forum?id=3b9SKkRAKw) |  | 0 | Patient data from real-world clinical practice often suffers from data scarcity and long-tail imbalances, leading to biased outcomes or algorithmic unfairness. This study addresses these challenges by generating lesion-containing image-segmentation pairs from lesion-free images. Previous efforts in... | Hantao Zhang, Jiancheng Yang, Pascal Fua, Shouhong Wan, Wei Peng, Xinyuan Wang, Yuhe Liu |  |
| 586 |  |  [Beyond Next Token Prediction: Patch-Level Training for Large Language Models](https://openreview.net/forum?id=dDpB23VbVa) |  | 0 | The prohibitive training costs of Large Language Models (LLMs) have emerged as a significant bottleneck in the development of next-generation LLMs. In this paper, we show that it is possible to significantly reduce the training costs of LLMs without sacrificing their performance. Specifically, we... | Chenze Shao, Fandong Meng, Jie Zhou |  |
| 587 |  |  [LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models](https://openreview.net/forum?id=z8sxoCYgmd) |  | 0 | With the rapid development of AI-generated content, the future internet may be inundated with synthetic data, making the discrimination of authentic and credible multimodal data increasingly challenging. Synthetic data detection has thus garnered widespread attention, and the performance of large... | Baichuan Zhou, Conghui He, Dahua Lin, Hengrui Kang, Honglin Lin, Jun He, Junan Zhang, Junyan Ye, Tianyi Bai, Tong Wu, Weijia Li, Yiping Chen, Zhizheng Wu, Zihao Wang, Zilong Huang |  |
| 588 |  |  [Recognize Any Surgical Object: Unleashing the Power of Weakly-Supervised Data](https://openreview.net/forum?id=iuxaCU3DI7) |  | 0 | We present RASO, a foundation model designed to Recognize Any Surgical Object, offering robust open-set recognition capabilities across a broad range of surgical procedures and object classes, in both surgical images and videos. RASO leverages a novel weakly-supervised learning framework that... | Brian R. Quaranto, Chenhui Xu, Dancheng Liu, Ishan Mishra, Jiajie Li, Jinjun Xiong, Peter C. W. Kim, Ruiyang Qin |  |
| 589 |  |  [Both Ears Wide Open: Towards Language-Driven Spatial Audio Generation](https://openreview.net/forum?id=qPx3i9sMxv) |  | 0 | Recently, diffusion models have achieved great success in mono-channel audio generation. However, when it comes to stereo audio generation, the soundscapes often have a complex scene of multiple objects and directions. Controlling stereo audio with spatial contexts remains challenging due to high... | Honggang Zhang, Huadai Liu, Peiwen Sun, Sitong Cheng, Wei Xue, Xiangtai Li, Yike Guo, Zhen Ye |  |
| 590 |  |  [Moner: Motion Correction in Undersampled Radial MRI with Unsupervised Neural Representation](https://openreview.net/forum?id=OdnqG1fYpo) |  | 0 | Motion correction (MoCo) in radial MRI is a particularly challenging problem due to the unpredictability of subject movement. Current state-of-the-art (SOTA) MoCo algorithms often rely on extensive high-quality MR images to pre-train neural networks, which constrains the solution space and leads to... | Chenhe Du, Hongjiang Wei, Jingyi Yu, Qing Wu, Xuanyu Tian, Yuyao Zhang |  |
| 591 |  |  [UniMatch: Universal Matching from Atom to Task for Few-Shot Drug Discovery](https://openreview.net/forum?id=v9EjwMM55Y) |  | 0 | Drug discovery is crucial for identifying candidate drugs for various diseases. However, its low success rate often results in a scarcity of annotations, posing a few-shot learning problem. Existing methods primarily focus on single-scale features, overlooking the hierarchical molecular structures... | Hongyang Chen, Mingqian Li, Qiang Zhang, Ruifeng Li, Wei Liu, Xiangxin Zhou, Yuan Yao, Yuhua Zhou |  |
| 592 |  |  [OASIS Uncovers: High-Quality T2I Models, Same Old Stereotypes](https://openreview.net/forum?id=L6IgkJvcgV) |  | 0 | Images generated by text-to-image (T2I) models often exhibit visual biases and stereotypes of concepts such as culture and profession. Existing quantitative measures of stereotypes are based on statistical parity that does not align with the sociological definition of stereotypes and, therefore,... | Gautam Sreekumar, Sepehr Dehdashtian, Vishnu Boddeti |  |
| 593 |  |  [Instance-dependent Early Stopping](https://openreview.net/forum?id=P42DbV2nuV) |  | 0 | In machine learning practice, early stopping has been widely used to regularize models and can save computational costs by halting the training process when the model's performance on a validation set stops improving. However, conventional early stopping applies the same stopping criterion to all... | Bo Han, Lei Feng, Runqi Lin, Suqin Yuan, Tongliang Liu |  |
| 594 |  |  [Beyond Random Masking: When Dropout meets Graph Convolutional Networks](https://openreview.net/forum?id=PwxYoMvmvy) |  | 0 | Graph Convolutional Networks (GCNs) have emerged as powerful tools for learning on graph-structured data, yet the behavior of dropout in these models remains poorly understood. This paper presents a comprehensive theoretical analysis of dropout in GCNs, revealing that its primary role differs... | Hao Zhu, XiaoMing Wu, Yuankai Luo |  |
| 595 |  |  [Self-supervised contrastive learning performs non-linear system identification](https://openreview.net/forum?id=ONfWFluZBI) |  | 0 | Self-supervised learning (SSL) approaches have brought tremendous success across many tasks and domains. It has been argued that these successes can be attributed to a link between SSL and identifiable representation learning: Temporal structure and auxiliary variables ensure that latent... | Rodrigo González Laiz, Steffen Schneider, Tobias Schmidt |  |
| 596 |  |  [Sparse autoencoders reveal selective remapping of visual concepts during adaptation](https://openreview.net/forum?id=imT03YXlG2) |  | 0 | Adapting foundation models for specific purposes has become a standard approach to build machine learning systems for downstream applications. Yet, it is an open question which mechanisms take place during adaptation. Here we develop a new Sparse Autoencoder (SAE) for the CLIP vision transformer,... | Hyesu Lim, Jaegul Choo, Jinho Choi, Steffen Schneider |  |
| 597 |  |  [PIED: Physics-Informed Experimental Design for Inverse Problems](https://openreview.net/forum?id=w7P92BEsb2) |  | 0 | In many science and engineering settings, system dynamics are characterized by governing partial differential equations (PDEs), and a major challenge is to solve inverse problems (IPs) where unknown PDE parameters are inferred based on observational data gathered under limited budget. Due to the... | Apivich Hemachandra, Bryan Kian Hsiang Low, Gregory Kang Ruey Lau, SeeKiong Ng |  |
| 598 |  |  [AgentRefine: Enhancing Agent Generalization through Refinement Tuning](https://openreview.net/forum?id=FDimWzmcWn) |  | 0 | Large Language Model (LLM) based agents have proved their ability to perform complex tasks like humans. However, there is still a large gap between open-sourced LLMs and commercial models like the GPT series. In this paper, we focus on improving the agent generalization capabilities of LLMs via... | Dayuan Fu, Jingang Wang, Keqing He, Wei Wang, Weihao Zeng, Weiran Xu, Wentao Hong, Xunliang Cai, Yejie Wang, Zhuoma Gongque |  |
| 599 |  |  [TabM: Advancing tabular deep learning with parameter-efficient ensembling](https://openreview.net/forum?id=Sd4wYYOhmY) |  | 0 | Deep learning architectures for supervised learning on tabular data range from simple multilayer perceptrons (MLP) to sophisticated Transformers and retrieval-augmented methods. This study highlights a major, yet so far overlooked opportunity for substantially improving tabular MLPs; namely,... | Akim Kotelnikov, Artem Babenko, Yury Gorishniy |  |
| 600 |  |  [Multi-Label Test-Time Adaptation with Bound Entropy Minimization](https://openreview.net/forum?id=75PhjtbBdr) |  | 0 | Mainstream test-time adaptation (TTA) techniques endeavor to mitigate distribution shifts via entropy minimization for multi-class classification, inherently increasing the probability of the most confident class. However, when encountering multi-label instances, the primary challenge stems from... | Feng Yu, Jianfeng Lu, QingGuo Chen, Xiangyu Wu, Yang Yang |  |
| 601 |  |  [ToolGen: Unified Tool Retrieval and Calling via Generation](https://openreview.net/forum?id=XLMAMmowdY) |  | 0 | As large language models (LLMs) advance, their inability to autonomously execute tasks by directly interacting with external tools remains a critical limitation. Traditional methods rely on inputting tool descriptions as context, which is constrained by context length and requires separate, often... | Haonan Li, Lei Ji, Renxi Wang, Shu Wang, Timothy Baldwin, Xudong Han |  |
| 602 |  |  [Activation Gradient based Poisoned Sample Detection Against Backdoor Attacks](https://openreview.net/forum?id=VNMJfBBUd5) |  | 0 | This work studies the task of poisoned sample detection for defending against data poisoning based backdoor attacks. Its core challenge is finding a generalizable and discriminative metric to distinguish between clean and various types of poisoned samples (e.g., various triggers, various poisoning... | Baoyuan Wu, Danni Yuan, Li Liu, Mingda Zhang, Shaokui Wei |  |
| 603 |  |  [Causally Motivated Sycophancy Mitigation for Large Language Models](https://openreview.net/forum?id=yRKelogz5i) |  | 0 | Incorporating user preferences into large language models (LLMs) can enhance the personalization and reliability of model outputs and facilitate the application of LLMs to real-world scenarios. However, leveraging user preferences can be a double-edged sword. Recent studies have found that improper... | Haoxi Li, Jie Zhang, Peiran Dong, Sikai Bai, Song Guo, Xueyang Tang, Yue Yu |  |
| 604 |  |  [Compositional simulation-based inference for time series](https://openreview.net/forum?id=uClUUJk05H) |  | 0 | Amortized simulation-based inference (SBI) methods train neural networks on simulated data to perform Bayesian inference. While this strategy avoids the need for tractable likelihoods, it often requires a large number of simulations and has been challenging to scale to time series data. Scientific... | Jakob H. Macke, Kenji Fukumizu, Manuel Glöckler, Shoji Toyota |  |
| 605 |  |  [Bayesian Treatment of the Spectrum of the Empirical Kernel in (Sub)Linear-Width Neural Networks](https://openreview.net/forum?id=O6znYvxC1U) |  | 0 | We study Bayesian neural networks (BNNs) in the theoretical limits of infinitely increasing number of training examples, network width and input space dimension. Our findings establish new bridges between kernel-theoretic approaches and techniques derived from statistical mechanics through the... | Bernardo Cuenca Grau, Ouns El Harzli |  |
| 606 |  |  [When GNNs meet symmetry in ILPs: an orbit-based feature augmentation approach](https://openreview.net/forum?id=wVTJRnZ11Z) |  | 0 | A common characteristic in integer linear programs (ILPs) is symmetry, allowing variables to be permuted without altering the underlying problem structure. Recently, GNNs have emerged as a promising approach for solving ILPs. However, a significant challenge arises when applying GNNs to ILPs with... | Akang Wang, Jianghua Wu, Lei Li, Qian Chen, Qian Li, Qingjiang Shi, Ruoyu Sun, TsungHui Chang, Xiaodong Luo |  |
| 607 |  |  [Optimal Transport for Time Series Imputation](https://openreview.net/forum?id=xPTzjpIQNp) |  | 0 | Missing data imputation through distribution alignment has demonstrated advantages for non-temporal datasets but exhibits suboptimal performance in time-series applications. The primary obstacle is crafting a discrepancy measure that simultaneously (1) captures temporal patterns—accounting for... | BinChen, Hao Wang, Haoxuan Li, Mingming Gong, Xu Chen, Zhengnan Li, Zhichao Chen |  |
| 608 |  |  [Video Action Differencing](https://openreview.net/forum?id=3bcN6xlO6f) |  | 0 | How do two individuals differ when performing the same action? In this work, we introduce Video Action Differencing (VidDiff), the novel task of identifying subtle differences between videos of the same action, which has numerous applications, such as coaching and skill learning. To enable... | Alejandro Lozano, Anita Rau, James Burgess, Lisa Dunlap, Serena YeungLevy, Trevor Darrell, Xiaohan Wang, Yuhui Zhang |  |
| 609 |  |  [GANDALF: Generative AttentioN based Data Augmentation and predictive modeLing Framework for personalized cancer treatment](https://openreview.net/forum?id=WwmtcGr4lP) |  | 0 | Effective treatment of cancer is a major challenge faced by healthcare providers, due to the highly individualized nature of patient responses to treatment. This is caused by the heterogeneity seen in cancer-causing alterations (mutations) across patient genomes. Limited availability of response... | Aishwarya Jayagopal, Anand D. Jeyasekharan, Robert John Walsh, Tuan Zea Tan, Vaibhav Rajan, Yanrong Zhang |  |
| 610 |  |  [RaSA: Rank-Sharing Low-Rank Adaptation](https://openreview.net/forum?id=GdXI5zCoAt) |  | 0 | Low-rank adaptation (LoRA) has been prominently employed for parameter-efficient fine-tuning of large language models (LLMs). However, the limited expressive capacity of LoRA, stemming from the low-rank constraint, has been recognized as a bottleneck, particularly in rigorous tasks like code... | Jiahao Xu, Rui Wang, Tian Liang, Wenxiang Jiao, Xing Wang, Xingyu Chen, Zhaopeng Tu, Zhijie Wang, Zhiwei He, Zhuosheng Zhang |  |
| 611 |  |  [Scaling Speech-Text Pre-training with Synthetic Interleaved Data](https://openreview.net/forum?id=3tukjsVyrE) |  | 0 | Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised... | Aohan Zeng, Jie Tang, Lei Zhang, Mingdao Liu, Shengmin Jiang, Yuxiao Dong, Zhengxiao Du |  |
| 612 |  |  [Offline Model-Based Optimization by Learning to Rank](https://openreview.net/forum?id=sb1HgVDLjN) |  | 0 | Offline model-based optimization (MBO) aims to identify a design that maximizes a black-box function using only a fixed, pre-collected dataset of designs and their corresponding scores. This problem has garnered significant attention from both scientific and industrial domains. A common approach in... | Chao Qian, Haopu Shang, Ke Xue, RongXi Tan, ShenHuan Lyu, Sheng Fu, Yao Wang, Yaoyuan Wang |  |
| 613 |  |  [From Search to Sampling: Generative Models for Robust Algorithmic Recourse](https://openreview.net/forum?id=NtwFghsJne) |  | 0 | Algorithmic Recourse provides recommendations to individuals who are adversely impacted by automated model decisions, on how to alter their profiles to achieve a favorable outcome. Effective recourse methods must balance three conflicting goals: proximity to the original profile to minimize cost,... | Lokesh Nagalapatti, Prateek Garg, Sunita Sarawagi |  |
| 614 |  |  [Neural Wave Equation for Irregularly Sampled Sequence Data](https://openreview.net/forum?id=kbeX97jExm) |  | 0 | Sequence labeling problems arise in several real-world applications such as healthcare and robotics. In many such applications, sequence data are irregularly sampled and are of varying complexities. Recently, efforts have been made to develop neural ODE-based architectures to model the evolution of... | Arkaprava Majumdar, M. Anand Krishna, P. K. Srijith |  |
| 615 |  |  [ComaDICE: Offline Cooperative Multi-Agent Reinforcement Learning with Stationary Distribution Shift Regularization](https://openreview.net/forum?id=5o9JJJPPm6) |  | 0 | Offline reinforcement learning (RL) has garnered significant attention for its ability to learn effective policies from pre-collected datasets without the need for further environmental interactions. While promising results have been demonstrated in single-agent settings, offline multi-agent... | Thanh Hong Nguyen, The Viet Bui, Tien Mai |  |
| 616 |  |  [Probabilistic Conformal Prediction with Approximate Conditional Validity](https://openreview.net/forum?id=Nfd7z9d6Bb) |  | 0 | We develop a new method for generating prediction sets that combines the flexibility of conformal methods with an estimate of the conditional distribution $\textup{P}_{Y \mid X}$. Existing methods, such as conformalized quantile regression and probabilistic conformal prediction, usually provide... | Alexander Fishkov, Eric Moulines, Maxim Panov, Mohsen Guizani, Vincent Plassier |  |
| 617 |  |  [Rethinking Neural Multi-Objective Combinatorial Optimization via Neat Weight Embedding](https://openreview.net/forum?id=GM7cmQfk2F) |  | 0 | Recent decomposition-based neural multi-objective combinatorial optimization (MOCO) methods struggle to achieve desirable performance. Even equipped with complex learning techniques, they often suffer from significant optimality gaps in weight-specific subproblems. To address this challenge, we... | Hanzhang Qin, Jiahai Wang, Jinbiao Chen, Yaoxin Wu, YueJiao Gong, Zhiguang Cao, Zizhen Zhang |  |
| 618 |  |  [Robust Root Cause Diagnosis using In-Distribution Interventions](https://openreview.net/forum?id=l11DZY5Nxu) |  | 0 | Diagnosing the root cause of an anomaly in a complex interconnected system is a pressing problem in today’s cloud services and industrial operations. We propose In-Distribution Interventions (IDI), a novel algorithm that predicts root cause as nodes that meet two criteria: 1) Anomaly: root cause... | Amit Sharma, Ashutosh Srivastava, Lokesh Nagalapatti, Sunita Sarawagi |  |
| 619 |  |  [Boosting Neural Combinatorial Optimization for Large-Scale Vehicle Routing Problems](https://openreview.net/forum?id=TbTJJNjumY) |  | 0 | Neural Combinatorial Optimization (NCO) methods have exhibited promising performance in solving Vehicle Routing Problems (VRPs). However, most NCO methods rely on the conventional self-attention mechanism that induces excessive computational complexity, thereby struggling to contend with... | Fu Luo, Mingxuan Yuan, Qingfu Zhang, Xi Lin, Xialiang Tong, Yaoxin Wu, Zhenkun Wang |  |
| 620 |  |  [Sensitivity Verification for Additive Decision Tree Ensembles](https://openreview.net/forum?id=h0vC0fm1q7) |  | 0 | Tree ensemble models, such as Gradient Boosted Decision Trees (GBDTs) and random forests, are widely popular models for a variety of machine learning tasks. The power of these models comes from the ensemble of decision trees, which makes analysis of such models significantly harder than for single... | Arhaan Ahmad, Ashutosh Gupta, S. Akshay, Tanay Vineet Tayal |  |
| 621 |  |  [Monte Carlo Planning with Large Language Model for Text-Based Game Agents](https://openreview.net/forum?id=r1KcapkzCt) |  | 0 | Text-based games provide valuable environments for language-based autonomous agents. However, planning-then-learning paradigms, such as those combining Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably time-consuming due to extensive iterations. Additionally, these... | Ling Chen, Meng Fang, Zijing Shi |  |
| 622 |  |  [Actions Speak Louder Than Words: Rate-Reward Trade-off in Markov Decision Processes](https://openreview.net/forum?id=Za3M6OZuCU) |  | 0 | The impact of communication on decision-making systems has been extensively studied under the assumption of dedicated communication channels. We instead consider communicating through actions, where the message is embedded into the actions of an agent which interacts with the environment in a... | Deniz Gündüz, Gongpu Chen, Haotian Wu |  |
| 623 |  |  [A Statistical Framework for Ranking LLM-based Chatbots](https://openreview.net/forum?id=rAoEub6Nw2) |  | 0 | Large language models (LLMs) have transformed natural language processing, with frameworks like Chatbot Arena providing pioneering platforms for evaluating these models. By facilitating millions of pairwise comparisons based on human judgments, Chatbot Arena has become a cornerstone in LLM... | Ion Stoica, Michael W. Mahoney, Siavash Ameli, Siyuan Zhuang |  |
| 624 |  |  [Online epsilon Net & Piercing Set for Geometric Concepts](https://openreview.net/forum?id=nNiWRRj6r9) |  | 0 | VC-dimension (Vapnik & Chervonenkis (1971)) and $\varepsilon$-nets (Haussler & Welzl (1987)) are key concepts in Statistical Learning Theory. Intuitively, VC-dimension is a measure of the size of a class of sets. The famous $\varepsilon$-net theorem, a fundamental result in Discrete Geometry,... | Devdan Dey, Satyam Singh, Sujoy Bhore |  |
| 625 |  |  [SimulPL: Aligning Human Preferences in Simultaneous Machine Translation](https://openreview.net/forum?id=XBF63bHDZw) |  | 0 | Simultaneous Machine Translation (SiMT) generates translations while receiving streaming source inputs. This requires the SiMT model to learn a read/write policy, deciding when to translate and when to wait for more source input. Numerous linguistic studies indicate that audiences in SiMT scenarios... | Chengqing Zong, Donglei Yu, Jie Zhu, Yang Zhao, Yangyifan Xu, Yu Zhou |  |
| 626 |  |  [Neural Interactive Proofs](https://openreview.net/forum?id=R2834dhBlo) |  | 0 | We consider the problem of how a trusted, but computationally bounded agent (a 'verifier') can learn to interact with one or more powerful but untrusted agents ('provers') in order to solve a given task. More specifically, we study the case in which agents are represented using neural networks and... | Lewis Hammond, Sam AdamDay |  |
| 627 |  |  [Oracle efficient truncated statistics](https://openreview.net/forum?id=ZS7UEI3vG5) |  | 0 | We study the problem of learning from truncated samples: instead of observing samples from some underlying population $p^\ast$, we observe only the examples that fall in some survival set $S \subset \mathbb{R}^d$ whose probability mass (measured with respect to $p^\ast$) is at least $\alpha$.... | Christos Tzamos, Konstantinos Karatapanis, Vasilis Kontonis |  |
| 628 |  |  [Drop-Upcycling: Training Sparse Mixture of Experts with Partial Re-initialization](https://openreview.net/forum?id=gx1wHnf5Vp) |  | 0 | The Mixture of Experts (MoE) architecture reduces the training and inference cost significantly compared to a dense model of equivalent capacity. Upcycling is an approach that initializes and trains an MoE model using a pre-trained dense model. While upcycling leads to initial performance gains,... | Jun Suzuki, Kazuki Fujii, Rio Yokota, Taishi Nakamura, Takuya Akiba, Yusuke Oda |  |
| 629 |  |  [Black-Box Detection of Language Model Watermarks](https://openreview.net/forum?id=E4LAVLXAHW) |  | 0 | Watermarking has emerged as a promising way to detect LLM-generated text, by augmenting LLM generations with later detectable signals. Recent work has proposed multiple families of watermarking schemes, several of which focus on preserving the LLM distribution. This distribution-preservation... | Martin T. Vechev, Nikola Jovanovic, Robin Staab, Thibaud Gloaguen |  |
| 630 |  |  [ProAdvPrompter: A Two-Stage Journey to Effective Adversarial Prompting for LLMs](https://openreview.net/forum?id=tpHqsyZ3YX) |  | 0 | As large language models (LLMs) are increasingly being integrated into various real-world applications, the identification of their vulnerabilities to jailbreaking attacks becomes an essential component of ensuring the safety and reliability of LLMs. Previous studies have developed LLM assistants,... | Guang Dai, Haishan Ye, Hao Di, Ivor W. Tsang, Tong He, Xiangyu Chang, Yinghui Huang |  |
| 631 |  |  [Ward: Provable RAG Dataset Inference via LLM Watermarks](https://openreview.net/forum?id=kVrwHLAb20) |  | 0 | RAG enables LLMs to easily incorporate external data, raising concerns for data owners regarding unauthorized usage of their content. The challenge of detecting such unauthorized usage remains underexplored, with datasets and methods from adjacent fields being ill-suited for its study. We take... | Martin T. Vechev, Maximilian Baader, Nikola Jovanovic, Robin Staab |  |
| 632 |  |  [SCOPE: A Self-supervised Framework for Improving Faithfulness in Conditional Text Generation](https://openreview.net/forum?id=dTkqaCKLPp) |  | 0 | Large Language Models (LLMs), when used for conditional text generation, often produce hallucinations, i.e., information that is unfaithful or not grounded in the input context. This issue arises in typical conditional text generation tasks, such as text summarization and data-to-text generation,... | Alberto Lumbreras, Alexandre Allauzen, Florian Le Bronnec, Laure Soulier, Patrick Gallinari, Song Duong, Vincent Guigue |  |
| 633 |  |  [Clique Number Estimation via Differentiable Functions of Adjacency Matrix Permutations](https://openreview.net/forum?id=DFSb67ksVr) |  | 0 | Estimating the clique number in a graph is central to various applications, e.g., community detection, graph retrieval, etc. Existing estimators often rely on non-differentiable combinatorial components. Here, we propose a full differentiable estimator for clique number estimation, which can be... | Abir De, Eeshaan Jain, Indradyumna Roy, Soumen Chakrabarti |  |
| 634 |  |  [Reliable and Diverse Evaluation of LLM Medical Knowledge Mastery](https://openreview.net/forum?id=TXfzH933qV) |  | 0 | Mastering medical knowledge is crucial for medical-specific LLMs. However, despite the existence of medical benchmarks like MedQA, a unified framework that fully leverages existing knowledge bases to evaluate LLMs' mastery of medical knowledge is still lacking. We propose PretexEval, a novel... | Chen Ning, Ji Wu, Xiao Zhang, Xien Liu, Yuxuan Zhou |  |
| 635 |  |  [SWE-Search: Enhancing Software Agents with Monte Carlo Tree Search and Iterative Refinement](https://openreview.net/forum?id=G7sIFXugTX) |  | 0 | Software engineers operating in complex and dynamic environments must continuously adapt to evolving requirements, learn iteratively from experience, and reconsider their approaches based on new insights. However, current large language model (LLM)-based software agents often follow linear,... | Albert Örwall, Anirudh Goyal, Antonis Antoniades, Kexun Zhang, William Yang Wang, Yuxi Xie |  |
| 636 |  |  [Language Models are Advanced Anonymizers](https://openreview.net/forum?id=82p8VHRsaK) |  | 0 | Recent privacy research on large language models (LLMs) has shown that they achieve near-human-level performance at inferring personal data from online texts. With ever-increasing model capabilities, existing text anonymization methods are currently lacking behind regulatory requirements and... | Mark Vero, Martin T. Vechev, Mislav Balunovic, Robin Staab |  |
| 637 |  |  [ADAM: An Embodied Causal Agent in Open-World Environments](https://openreview.net/forum?id=Ouu3HnIVBc) |  | 0 | In open-world environments like Minecraft, existing agents face challenges in continuously learning structured knowledge, particularly causality. These challenges stem from the opacity inherent in black-box models and an excessive reliance on prior knowledge during training, which impair their... | Chaochao Lu, Shu Yu |  |
| 638 |  |  [Expected Return Symmetries](https://openreview.net/forum?id=wFg0shwoRe) |  | 0 | Symmetry is an important inductive bias that can improve model robustness and generalization across many deep learning domains. In multi-agent settings, a priori known symmetries have been shown to address a fundamental coordination failure mode known as mutually incompatible symmetry breaking;... | Darius Muglich, Elise van der Pol, Jakob Nicolaus Foerster, Johannes Forkel |  |
| 639 |  |  [Beware of Calibration Data for Pruning Large Language Models](https://openreview.net/forum?id=x83w6yGIWb) |  | 0 | As large language models (LLMs) are widely applied across various fields, model compression has become increasingly crucial for reducing costs and improving inference efficiency. Post-training pruning is a promising method that does not require resource-intensive iterative training and only needs a... | Juntao Li, Min Zhang, Ping Li, Qingrong Xia, Xinyu Duan, Yang Xiang, Yixin Ji, Zhefeng Wang |  |
| 640 |  |  [Herald: A Natural Language Annotated Lean 4 Dataset](https://openreview.net/forum?id=Se6MgCtRhz) |  | 0 | Verifiable formal languages like Lean have profoundly impacted mathematical reasoning, particularly through the use of large language models (LLMs) for automated reasoning. A significant challenge in training LLMs for these formal languages is the lack of parallel datasets that align natural... | Bin Dong, Guoxiong Gao, Jiedong Jiang, Qi Gao, Tianyi Xu, Yutong Wang, Zihan Qin |  |
| 641 |  |  [Efficient Residual Learning with Mixture-of-Experts for Universal Dexterous Grasping](https://openreview.net/forum?id=BUj9VSCoET) |  | 0 | Universal dexterous grasping across diverse objects presents a fundamental yet formidable challenge in robot learning. Existing approaches using reinforcement learning (RL) to develop policies on extensive object datasets face critical limitations, including complex curriculum design for multi-task... | Haoqi Yuan, Yuhui Fu, Ziye Huang, Zongqing Lu |  |
| 642 |  |  [DPLM-2: A Multimodal Diffusion Protein Language Model](https://openreview.net/forum?id=5z9GjHgerY) |  | 0 | Proteins are essential macromolecules defined by their amino acid sequences, which determine their three-dimensional structures and, consequently, their functions in all living organisms. Therefore, generative protein modeling necessitates a multimodal approach to simultaneously model, understand,... | Dongyu Xue, Fei Ye, Quanquan Gu, Shujian Huang, Xinyou Wang, Zaixiang Zheng |  |
| 643 |  |  [Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation](https://openreview.net/forum?id=moWiYJuSGF) |  | 0 | Large language models (LLMs) have recently gained much attention in building autonomous agents. However, performance of current LLM-based web agents in long-horizon tasks is far from optimal, often yielding errors such as repeatedly buying a non-refundable flight ticket. By contrast, humans can... | Dongha Lee, Gwanwoo Song, Hyungjoo Chae, Jihoon Kim, Jinyoung Yeo, Kai Tzuiunn Ong, Minju Gwak, Namyoung Kim, Sunghwan Kim |  |
| 644 |  |  [HyperFace: Generating Synthetic Face Recognition Datasets by Exploring Face Embedding Hypersphere](https://openreview.net/forum?id=4YzVF9isgD) |  | 0 | Face recognition datasets are often collected by crawling Internet and without individuals' consents, raising ethical and privacy concerns. Generating synthetic datasets for training face recognition models has emerged as a promising alternative. However, the generation of synthetic datasets... | Hatef OtroshiShahreza, Sébastien Marcel |  |
| 645 |  |  [Language Imbalance Driven Rewarding for Multilingual Self-improving](https://openreview.net/forum?id=Kak2ZH5Itp) |  | 0 | Large Language Models (LLMs) have achieved state-of-the-art performance across numerous tasks. However, these advancements have predominantly benefited "first-class" languages such as English and Chinese, leaving many other languages underrepresented. This imbalance, while limiting broader... | Chen Wang, Chengqing Zong, Jiajun Zhang, Junhong Wu, Wen Yang |  |
| 646 |  |  [Quantum-PEFT: Ultra parameter-efficient fine-tuning](https://openreview.net/forum?id=dgR6i4TSng) |  | 0 | This paper introduces Quantum-PEFT that leverages quantum computations for parameter-efficient fine-tuning (PEFT). Unlike other additive PEFT methods, such as low-rank adaptation (LoRA), Quantum-PEFT exploits an underlying full-rank yet surprisingly parameter efficient _quantum unitary... | Francesco Tonin, Frank Zhengqing Wu, Leyla Naz Candogan, Toshiaki KoikeAkino, Volkan Cevher, Yongtao Wu |  |
| 647 |  |  [Think Then React: Towards Unconstrained Action-to-Reaction Motion Generation](https://openreview.net/forum?id=UxzKcIZedp) |  | 0 | Modeling human-like action-to-reaction generation has significant real-world applications, like human-robot interaction and games. Despite recent advancements in single-person motion generation, it is still challenging to well handle action-to-reaction generation, due to the difficulty of directly... | Boyuan Li, Chuhao Jin, Ruihua Song, Wenbing Huang, Wenhui Tan, Xiting Wang |  |
| 648 |  |  [Rapid Selection and Ordering of In-Context Demonstrations via Prompt Embedding Clustering](https://openreview.net/forum?id=1Iu2Yte5N6) |  | 0 | While Large Language Models (LLMs) excel at in-context learning (ICL) using just a few demonstrations, their performances are sensitive to demonstration orders. The reasons behind this sensitivity remain poorly understood. In this paper, we investigate the prompt embedding space to bridge the gap... | Hung Le, Kha Pham, Man Ngo, Truyen Tran |  |
| 649 |  |  [Asymptotic Analysis of Two-Layer Neural Networks after One Gradient Step under Gaussian Mixtures Data with Structure](https://openreview.net/forum?id=tNn6Hskmti) |  | 0 | In this work, we study the training and generalization performance of two-layer neural networks (NNs) after one gradient descent step under structured data modeled by Gaussian mixtures. While previous research has extensively analyzed this model under isotropic data assumption, such simplifications... | Samet Demir, Zafer Dogan |  |
| 650 |  |  [OCEAN: Offline Chain-of-thought Evaluation and Alignment in Large Language Models](https://openreview.net/forum?id=rlgplAuN2p) |  | 0 | Offline evaluation of LLMs is crucial in understanding their capacities, though current methods remain underexplored in existing research. In this work, we focus on the offline evaluation of the chain-of-thought capabilities and show how to optimize LLMs based on the proposed evaluation method. To... | Branislav Kveton, Jianing Wang, Jingbo Shang, Julian J. McAuley, Junda Wu, Lina Yao, Ruoyu Wang, Tong Yu, Xiang Chen, Xintong Li, Yu Xia, Yuxin Xiong |  |
| 651 |  |  [Distribution-Free Data Uncertainty for Neural Network Regression](https://openreview.net/forum?id=pDDODPtpx9) |  | 0 | Quantifying uncertainty is an essential part of predictive modeling, especially in the context of high-stakes decision-making. While classification output includes data uncertainty by design in the form of class probabilities, the regression task generally aims only to predict the expected value of... | András A. Benczúr, Domokos M. Kelen, Péter Kersch, Ádám Jung |  |
| 652 |  |  [SOO-Bench: Benchmarks for Evaluating the Stability of Offline Black-Box Optimization](https://openreview.net/forum?id=bqf0aCF3Dd) |  | 0 | Black-box optimization aims to find the optima through building a model close to the black-box objective function based on function value evaluation. However, in many real-world tasks, such as the design of molecular formulas and mechanical structures, it is perilous, costly, or even infeasible to... | Aimin Zhou, Hong Qian, Huakang Lu, Ke Tang, Shuo Liu, Xiang Shu, Xin An, Yang Yu, Yaolin Wen, Yiyi Zhu |  |
| 653 |  |  [Understanding and Enhancing Safety Mechanisms of LLMs via Safety-Specific Neuron](https://openreview.net/forum?id=yR47RmND1m) |  | 0 | Safety alignment for large language models (LLMs) has become a critical issue due to their rapid progress. However, our understanding of effective safety mechanisms in LLMs remains limited, leading to safety alignment training that mainly focuses on improving optimization, data-level enhancement,... | Anirudh Goyal, Kenji Kawaguchi, Michael Shieh, Wenxuan Zhang, Yiran Zhao, Yuxi Xie |  |
| 654 |  |  [Long Context Compression with Activation Beacon](https://openreview.net/forum?id=1eQT9OzfNQ) |  | 0 | Long context compression is a critical research problem due to its significance in reducing the high computational and memory costs associated with LLMs. In this paper, we propose Activation Beacon, a plug-in module for transformer-based LLMs that targets effective, efficient, and flexible... | Ninglu Shao, Peitian Zhang, Qiwei Ye, Shitao Xiao, Zheng Liu, Zhicheng Dou |  |
| 655 |  |  [LASeR: Towards Diversified and Generalizable Robot Design with Large Language Models](https://openreview.net/forum?id=7mlvOHL6qJ) |  | 0 | Recent advances in Large Language Models (LLMs) have stimulated a significant paradigm shift in evolutionary optimization, where hand-crafted search heuristics are gradually replaced with LLMs serving as intelligent search operators. However, these studies still bear some notable limitations,... | Feifei Wang, Huan Xiao, Junru Song, Wei Peng, Wen Yao, Yang Yang |  |
| 656 |  |  [Be More Diverse than the Most Diverse: Optimal Mixtures of Generative Models via Mixture-UCB Bandit Algorithms](https://openreview.net/forum?id=2Chkk5Ye2s) |  | 0 | The availability of multiple training algorithms and architectures for generative models requires a selection mechanism to form a single model over a group of well-trained generation models. The selection task is commonly addressed by identifying the model that maximizes an evaluation score based... | Cheuk Ting Li, Farzan Farnia, Parham Rezaei |  |
| 657 |  |  [Arithmetic Transformers Can Length-Generalize in Both Operand Length and Count](https://openreview.net/forum?id=eIgGesYKLG) |  | 0 | Transformers often struggle with \*length generalization\*, meaning they fail to generalize to sequences longer than those encountered during training. While arithmetic tasks are commonly used to study length generalization, certain tasks are considered notoriously difficult, e.g., multi-operand... | Chulhee Yun, Hanseul Cho, Jaeyoung Cha, Srinadh Bhojanapalli |  |
| 658 |  |  [Stealthy Shield Defense: A Conditional Mutual Information-Based Approach against Black-Box Model Inversion Attacks](https://openreview.net/forum?id=p0DjhjPXl3) |  | 0 | Model inversion attacks (MIAs) aim to reconstruct the private training data by accessing the public model, raising concerns about privacy leakage. Black-box MIAs, where attackers can only query the model and obtain outputs, are closer to real-world scenarios. The latest black-box attacks have... | Bin Chen, Hao Fang, Hongyao Yu, ShuTao Xia, Tianqu Zhuang, Yixiang Qiu |  |
| 659 |  |  [NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens](https://openreview.net/forum?id=uMEsKEiB7J) |  | 0 | Recent advancements in Large Language Models (LLMs) have pushed the boundaries of natural language processing, especially in long-context understanding. However, the evaluation of these models' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this... | Boqi Pan, Cheng Deng, Cunxiang Wang, Guangsheng Bao, Qian Wang, Qipeng Guo, Ruoxi Ning, Tonghui Wu, Xiangkun Hu, Yue Zhang, Zheng Zhang |  |
| 660 |  |  [Look Before You Leap: Universal Emergent Mechanism for Retrieval in Language Models](https://openreview.net/forum?id=eIB1UZFcFg) |  | 0 | When solving challenging problems, language models (LMs) are able to identify relevant information from long and complicated contexts. To study how LMs solve retrieval tasks in diverse situations, we introduce ORION, a collection of structured retrieval tasks spanning six domains, from text... | Alexandre Variengien, Eric Winsor |  |
| 661 |  |  [A Multi-Power Law for Loss Curve Prediction Across Learning Rate Schedules](https://openreview.net/forum?id=KnoS9XxIlK) |  | 0 | Training large models is both resource-intensive and time-consuming, making it crucial to understand the quantitative relationship between model performance and hyperparameters. In this paper, we derive an empirical law that predicts pretraining loss for large language models for every intermediate... | Haodong Wen, Kaifeng Lyu, Kairong Luo, Maosong Sun, Shengding Hu, Wenguang Chen, Zhenbo Sun, Zhiyuan Liu |  |
| 662 |  |  [LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token](https://openreview.net/forum?id=UQJ7CDW8nb) |  | 0 | The advent of real-time large multimodal models (LMMs) like GPT-4o has sparked considerable interest in efficient LMMs. LMM frameworks typically encode visual inputs into vision tokens (continuous representations) and integrate them and textual instructions into the context of large language models... | Qingkai Fang, Shaolei Zhang, Yang Feng, Zhe Yang |  |
| 663 |  |  [URLOST: Unsupervised Representation Learning without Stationarity or Topology](https://openreview.net/forum?id=MBBRHDuiwM) |  | 0 | Unsupervised representation learning has seen tremendous progress. However, it is constrained by its reliance on domain specific stationarity and topology, a limitation not found in biological intelligence systems. For instance, unlike computer vision, human vision can process visual signals... | Juexiao Zhang, Yann LeCun, Yubei Chen, Zeyu Yun |  |
| 664 |  |  [One-for-All Few-Shot Anomaly Detection via Instance-Induced Prompt Learning](https://openreview.net/forum?id=Zzs3JwknAY) |  | 0 | Anomaly detection methods under the 'one-for-all' paradigm aim to develop a unified model capable of detecting anomalies across multiple classes. However, these approaches typically require a large number of normal samples for model training, which may not always be feasible in practice. Few-shot... | Qinliang Su, Wenchao Xu, Wenxi Lv |  |
| 665 |  |  [K-HALU: Multiple Answer Korean Hallucination Benchmark for Large Language Models](https://openreview.net/forum?id=VnLhUogHYE) |  | 0 | Recent researchers and companies have been developing large language models (LLMs) specifically designed for particular purposes and have achieved significant advancements in various natural language processing tasks. However, LLMs are still prone to generating hallucinations—results that are... | Heuiseok Lim, Jaehyung Seo |  |
| 666 |  |  [Charting the Design Space of Neural Graph Representations for Subgraph Matching](https://openreview.net/forum?id=5pd78GmXC6) |  | 0 | Subgraph matching is vital in knowledge graph (KG) question answering, molecule design, scene graph, code and circuit search, etc. Neural methods have shown promising results for subgraph matching. Our study of recent systems suggests refactoring them into a unified design space for graph matching... | Abir De, Ashwin Ramachandran, Indradyumna Roy, Soumen Chakrabarti, Vaibhav Raj |  |
| 667 |  |  [Convergence and Implicit Bias of Gradient Descent on Continual Linear Classification](https://openreview.net/forum?id=DTqx3iqjkz) |  | 0 | We study continual learning on multiple linear classification tasks by sequentially running gradient descent (GD) for a fixed budget of iterations per each given task. When all tasks are jointly linearly separable and are presented in a cyclic/random order, we show the directional convergence of... | Chulhee Yun, Hanseul Cho, Hyunji Jung |  |
| 668 |  |  [The Unreasonable Ineffectiveness of the Deeper Layers](https://openreview.net/forum?id=ngmEcEer8a) |  | 0 | How is knowledge stored in an LLM’s weights? We study this via layer pruning: if removing a certain layer does not affect model performance in common question-answering benchmarks, then the weights in that layer are not necessary for storing the knowledge needed to answer those questions. To find... | Andrey Gromov, Daniel A. Roberts, Hassan Shapourian, Kushal Tirumala, Paolo Glorioso |  |
| 669 |  |  [Distilling Dataset into Neural Field](https://openreview.net/forum?id=nCrJD7qPJN) |  | 0 | Utilizing a large-scale dataset is essential for training high-performance deep learning models, but it also comes with substantial computation and storage costs. To overcome these challenges, dataset distillation has emerged as a promising solution by compressing the large-scale dataset into a... | Donghyeok Shin, Gyuwon Sim, HeeSun Bae, IlChul Moon, Wanmo Kang |  |
| 670 |  |  [Relax and Merge: A Simple Yet Effective Framework for Solving Fair k-Means and k-sparse Wasserstein Barycenter Problems](https://openreview.net/forum?id=n8h1z588eu) |  | 0 | The fairness of clustering algorithms has gained widespread attention across various areas, including machine learning, In this paper, we study fair $k$-means clustering in Euclidean space. Given a dataset comprising several groups, the fairness constraint requires that each cluster should contain... | Guanlin Mo, Hu Ding, Shihong Song |  |
| 671 |  |  [Neural Dueling Bandits: Preference-Based Optimization with Human Feedback](https://openreview.net/forum?id=VELhv9BBfn) |  | 0 | Contextual dueling bandit is used to model the bandit problems, where a learner's goal is to find the best arm for a given context using observed noisy human preference feedback over the selected arms for the past contexts. However, existing algorithms assume the reward function is linear, which... | Arun Verma, Bryan Kian Hsiang Low, Patrick Jaillet, Xiaoqiang Lin, Zhongxiang Dai |  |
| 672 |  |  [SPORTU: A Comprehensive Sports Understanding Benchmark for Multimodal Large Language Models](https://openreview.net/forum?id=x1yOHtFfDh) |  | 0 | Multimodal Large Language Models (MLLMs) are advancing the ability to reason about complex sports scenarios by integrating textual and visual information. To comprehensively evaluate their capabilities, we introduce SPORTU, a benchmark designed to assess MLLMs across multi-level sports reasoning... | Chi Lu, Christopher Lai, Hanjie Chen, Haotian Xia, Junbo Zou, Rhys Tracy, Weining Shen, Xun Shao, Yanjun He, YuanFang Wang, Yuqing Wang, Zhengbang Yang, Zhuoqing Xie |  |
| 673 |  |  [SIM: Surface-based fMRI Analysis for Inter-Subject Multimodal Decoding from Movie-Watching Experiments](https://openreview.net/forum?id=OJsMGsO6yn) |  | 0 | Current AI frameworks for brain decoding and encoding, typically train and test models within the same datasets. This limits their utility for cognitive training (neurofeedback) for which it would be useful to pool experiences across individuals to better simulate stimuli not sampled during... | Daniel Rueckert, Emma Claire Robinson, Gabriel Bénédict, Logan Zane John Williams, Robert Leech, Simon Dahan, Yourong Guo |  |
| 674 |  |  [Why In-Context Learning Models are Good Few-Shot Learners?](https://openreview.net/forum?id=iLUcsecZJp) |  | 0 | We explore in-context learning (ICL) models from a learning-to-learn perspective. Unlike studies that identify specific learning algorithms in ICL models, we compare ICL models with typical meta-learners to understand their superior performance. We theoretically prove the expressiveness of ICL... | Quanming Yao, Shiguang Wu, Yaqing Wang |  |
| 675 |  |  [Evaluating Large Language Models through Role-Guide and Self-Reflection: A Comparative Study](https://openreview.net/forum?id=E36NHwe7Zc) |  | 0 | Large Language Models fine-tuned with Reinforcement Learning from Human Feedback (RLHF-LLMs) can over-rely on aligned preferences without truly gaining self-knowledge, leading to hallucination and biases. If an LLM can better access its knowledge and know what it knows, it can avoid making false or... | Lili Zhao, Mengyun Wang, Qi Liu, Shijin Wang, Wei Chen, Yang Wang, Zhichao Sheng |  |
| 676 |  |  [Local Loss Optimization in the Infinite Width: Stable Parameterization of Predictive Coding Networks and Target Propagation](https://openreview.net/forum?id=g6syfIrVuS) |  | 0 | Local learning, which trains a network through layer-wise local targets and losses, has been studied as an alternative to backpropagation (BP) in neural computation. However, its algorithms often become more complex or require additional hyperparameters due to the locality, making it challenging to... | Rio Yokota, Ryo Karakida, Satoki Ishikawa |  |
| 677 |  |  [Towards Faster Decentralized Stochastic Optimization with Communication Compression](https://openreview.net/forum?id=CMMpcs9prj) |  | 0 | Communication efficiency has garnered significant attention as it is considered the main bottleneck for large-scale decentralized Machine Learning applications in distributed and federated settings. In this regime, clients are restricted to transmitting small amounts of compressed information to... | Rustem Islamov, Sebastian U. Stich, Yuan Gao |  |
| 678 |  |  [Group-robust Sample Reweighting for Subpopulation Shifts via Influence Functions](https://openreview.net/forum?id=aQj9Ifxrl6) |  | 0 | Machine learning models often have uneven performance among subpopulations (a.k.a., groups) in the data distributions. This poses a significant challenge for the models to generalize when the proportions of the groups shift during deployment. To improve robustness to such shifts, existing... | Bryan Kian Hsiang Low, Jingtan Wang, Pang Wei Koh, Rui Qiao, Zhaoxuan Wu |  |
| 679 |  |  [Endless Jailbreaks with Bijection Learning](https://openreview.net/forum?id=xP1radUi32) |  | 0 | Despite extensive safety measures, LLMs are vulnerable to adversarial inputs, or jailbreaks, which can elicit unsafe behaviors. In this work, we introduce bijection learning, a powerful attack algorithm which automatically fuzzes LLMs for safety vulnerabilities using randomly-generated encodings... | Brian R. Y. Huang, Leonard Tang, Maximilian Li |  |
| 680 |  |  [GotenNet: Rethinking Efficient 3D Equivariant Graph Neural Networks](https://openreview.net/forum?id=5wxCQDtbMo) |  | 0 | Understanding complex three-dimensional (3D) structures of graphs is essential for accurately modeling various properties, yet many existing approaches struggle with fully capturing the intricate spatial relationships and symmetries inherent in such systems, especially in large-scale, dynamic... | Sarp Aykent, Tian Xia |  |
| 681 |  |  [Efficient Automated Circuit Discovery in Transformers using Contextual Decomposition](https://openreview.net/forum?id=41HlN8XYM5) |  | 0 | Automated mechanistic interpretation research has attracted great interest due to its potential to scale explanations of neural network internals to large models. Existing automated circuit discovery work relies on activation patching or its approximations to identify subgraphs in models for... | Aliyah R. Hsu, Anobel Y. Odisho, Bin Yu, Georgia Zhou, Peter R. Carroll, Yaxuan Huang, Yeshwanth Cherapanamjeri |  |
| 682 |  |  [How to Evaluate Reward Models for RLHF](https://openreview.net/forum?id=cbttLtO94Q) |  | 0 | We introduce a new benchmark for reward models that quantifies their ability to produce strong language models through RLHF (Reinforcement Learning from Human Feedback). The gold-standard approach is to run a full RLHF training pipeline and directly probe downstream LLM performance. However, this... | Anastasios Nikolas Angelopoulos, Banghua Zhu, Connor Chen, Evan Frick, Ion Stoica, Jiantao Jiao, Joseph E. Gonzalez, Tianle Li, WeiLin Chiang |  |
| 683 |  |  [An Efficient Framework for Crediting Data Contributors of Diffusion Models](https://openreview.net/forum?id=9EqQC2ct4H) |  | 0 | As diffusion models are deployed in real-world settings and their performance driven by training data, appraising the contribution of data contributors is crucial to creating incentives for sharing quality data and to implementing policies for data compensation. Depending on the use case, model... | Chanwoo Kim, Chris Lin, Mingyu Lu, SuIn Lee |  |
| 684 |  |  [Decentralized Optimization with Coupled Constraints](https://openreview.net/forum?id=AJM52ygi6Y) |  | 0 | We consider the decentralized minimization of a separable objective $\sum_{i=1}^{n} f_i(x_i)$, where the variables are coupled through an affine constraint $\sum_{i=1}^n\left(\mathbf{A}_i x_i - b_i\right) = 0$. We assume that the functions $f_i$, matrices $\mathbf{A}_i$, and vectors $b_i$ are... | Alexander Rogozin, Alexander V. Gasnikov, Daniil Dorin, Demyan Yarmoshik, Dmitry Kovalev, Nikita Kiselev |  |
| 685 |  |  [You Only Prune Once: Designing Calibration-Free Model Compression With Policy Learning](https://openreview.net/forum?id=5RZoYIT3u6) |  | 0 | The ever-increasing size of large language models (LLMs) presents significant challenges for deployment due to their heavy computational and memory requirements. Current model pruning techniques attempt to alleviate these issues by relying heavily on external calibration datasets to determine which... | Ayan Sengupta, Siddhant Chaudhary, Tanmoy Chakraborty |  |
| 686 |  |  [FreDF: Learning to Forecast in the Frequency Domain](https://openreview.net/forum?id=4A9IdSa1ul) |  | 0 | Time series modeling presents unique challenges due to autocorrelation in both historical data and future sequences. While current research predominantly addresses autocorrelation within historical data, the correlations among future labels are often overlooked. Specifically, modern forecasting... | Dacheng Tao, Degui Yang, Hao Wang, Haoxuan Li, Lichen Pan, Sen Zhang, Xinggao Liu, Yifei Yang, Yuan Shen, Zhichao Chen |  |
| 687 |  |  [SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration](https://openreview.net/forum?id=OL44KtasKc) |  | 0 | The transformer architecture predominates across various models. As the heart of the transformer, attention has a computational complexity of $O(N^2)$, compared to $O(N)$ for linear transformations. When handling large sequence lengths, attention becomes the primary time-consuming component.... | Jia wei, Jianfei Chen, Jintao Zhang, Jun Zhu, Pengle Zhang |  |
| 688 |  |  [Neural Multi-Objective Combinatorial Optimization via Graph-Image Multimodal Fusion](https://openreview.net/forum?id=4sJ2FYE65U) |  | 0 | Existing neural multi-objective combinatorial optimization (MOCO) methods still exhibit an optimality gap since they fail to fully exploit the intrinsic features of problem instances. A significant factor contributing to this shortfall is their reliance solely on graph-modal information. To... | Jiahai Wang, Jinbiao Chen, Yaoxin Wu, Zhiguang Cao |  |
| 689 |  |  [Have the VLMs Lost Confidence? A Study of Sycophancy in VLMs](https://openreview.net/forum?id=E2PFv7ad3p) |  | 0 | In the study of LLMs, sycophancy represents a prevalent hallucination that poses significant challenges to these models. Specifically, LLMs often fail to adhere to original correct responses, instead blindly agreeing with users' opinions, even when those opinions are incorrect or malicious.... | Leyi Yang, Linsheng Lu, Qi Zhang, Rui Zheng, Shuo Li, Tao Gui, Tao Ji, Xiaohui Zhao, Xiaoran Fan, Xuanjing Huang, Yuming Yang, Yuran Wang, Zhiheng Xi |  |
| 690 |  |  [Generative Adapter: Contextualizing Language Models in Parameters with A Single Forward Pass](https://openreview.net/forum?id=bc3sUsS6ck) |  | 0 | Large language models (LLMs) acquire substantial knowledge during pretraining but often need adaptation to new contexts, tasks, or domains, typically achieved through fine-tuning or prompting. However, fine-tuning incurs significant training costs, while prompting increases inference overhead.... | Benjamin Van Durme, Hao Cheng, Hao Fang, Jianfeng Gao, Luke Zettlemoyer, Patrick Xia, Tong Chen, Xiaodong Liu |  |
| 691 |  |  [ICLR: In-Context Learning of Representations](https://openreview.net/forum?id=pXlmOmlHJZ) |  | 0 | Recent work demonstrates that structured patterns in pretraining data influence how representations of different concepts are organized in a large language model’s (LLM) internals, with such representations then driving downstream abilities. Given the open-ended nature of LLMs, e.g., their ability... | Andrew Lee, Core Francisco Park, Ekdeep Singh Lubana, Hidenori Tanaka, Kento Nishi, Martin Wattenberg, Maya Okawa, Yongyi Yang |  |
| 692 |  |  [Ensembling Diffusion Models via Adaptive Feature Aggregation](https://openreview.net/forum?id=e32cI4r8Eo) |  | 0 | The success of the text-guided diffusion model has inspired the development and release of numerous powerful diffusion models within the open-source community. These models are typically fine-tuned on various expert datasets, showcasing diverse denoising capabilities. Leveraging multiple... | Cong Wang, Fei Shen, Jun Zhang, Kuan Tian, Qing Gu, Yonghang Guan, Zhiwei Jiang |  |
| 693 |  |  [Solving New Tasks by Adapting Internet Video Knowledge](https://openreview.net/forum?id=p01BR4njlY) |  | 0 | Video generative models demonstrate great promise in robotics by serving as visual planners or as policy supervisors. When pretrained on internet-scale data, such video models intimately understand alignment with natural language, and can thus facilitate generalization to novel downstream behavior... | Calvin Luo, Chen Sun, Yilun Du, Zilai Zeng |  |
| 694 |  |  [SoftMatcha: A Soft and Fast Pattern Matcher for Billion-Scale Corpus Searches](https://openreview.net/forum?id=Q6PAnqYVpo) |  | 0 | Researchers and practitioners in natural language processing and computational linguistics frequently observe and analyze the real language usage in large-scale corpora. For that purpose, they often employ off-the-shelf pattern-matching tools, such as grep, and keyword-in-context concordancers,... | Chihiro Taguchi, Go Kamoda, Hiroyuki Deguchi, Kohei Suenaga, Masaki Waga, Sho Yokoi, Yusuke Matsushita |  |
| 695 |  |  [Model merging with SVD to tie the Knots](https://openreview.net/forum?id=67X93aZHII) |  | 0 | Recent model merging methods demonstrate that the parameters of fully-finetuned models specializing in distinct tasks can be combined into one model capable of solving all tasks without retraining. Yet, this success does not transfer well when merging LoRA finetuned models. We study this phenomenon... | Boglarka Ecsedi, George Stoica, Judy Hoffman, Leshem Choshen, Pratik Ramesh |  |
| 696 |  |  [PolyhedronNet: Representation Learning for Polyhedra with Surface-attributed Graph](https://openreview.net/forum?id=BpyHIrpUOL) |  | 0 | Ubiquitous geometric objects can be precisely and efficiently represented as polyhedra. The transformation of a polyhedron into a vector, known as polyhedra representation learning, is crucial for manipulating these shapes with mathematical and statistical tools for tasks like classification,... | Dazhou Yu, Genpei Zhang, Liang Zhao |  |
| 697 |  |  [SafeDiffuser: Safe Planning with Diffusion Probabilistic Models](https://openreview.net/forum?id=ig2wk7kK9J) |  | 0 | Diffusion models have shown promise in data-driven planning. While these planners are commonly employed in applications where decisions are critical, they still lack established safety guarantees. In this paper, we address this limitation by introducing SafeDiffuser, a method to equip diffusion... | Chuang Gan, Daniela Rus, Mathias Lechner, Ramin M. Hasani, TsunHsuan Wang, Wei Xiao |  |
| 698 |  |  [BrainACTIV: Identifying visuo-semantic properties driving cortical selectivity using diffusion-based image manipulation](https://openreview.net/forum?id=CGON8Btleu) |  | 0 | The human brain efficiently represents visual inputs through specialized neural populations that selectively respond to specific categories. Advancements in generative modeling have enabled data-driven discovery of neural selectivity using brain-optimized image synthesis. However, current methods... | Christina Sartzetaki, Diego Garcia Cerdas, Gemma Roig, Iris I. A. Groen, Magnus Petersen, Pascal Mettes |  |
| 699 |  |  [Efficient Online Reinforcement Learning Fine-Tuning Need Not Retain Offline Data](https://openreview.net/forum?id=HN0CYZbAPw) |  | 0 | The modern paradigm in machine learning involves pre-training on diverse data, followed by task-specific fine-tuning. In reinforcement learning (RL), this translates to learning via offline RL on a diverse historical dataset, followed by rapid online RL fine-tuning using interaction data. Most RL... | Andy Peng, Aviral Kumar, Qiyang Li, Sergey Levine, Zhiyuan Zhou |  |
| 700 |  |  [Making Text Embedders Few-Shot Learners](https://openreview.net/forum?id=wfLuiDjQ0u) |  | 0 | Large language models (LLMs) with decoder-only architectures have demonstrated exceptional text-generation capabilities across a variety of tasks. Some researchers have also adapted these models for text representation tasks. However, in text representation tasks, these models often face... | Chaofan Li, Defu Lian, Jianlyu Chen, Kun Luo, Minghao Qin, Shitao Xiao, Yingxia Shao, Zheng Liu |  |
| 701 |  |  [Diverse Preference Learning for Capabilities and Alignment](https://openreview.net/forum?id=pOq9vDIYev) |  | 0 | As LLMs increasingly impact society, their ability to represent diverse perspectives is critical. However, recent studies reveal that alignment algorithms such as RLHF and DPO significantly reduce the diversity of LLM outputs. Not only do aligned LLMs generate text with repetitive structure and... | Asher ParkerSartori, Dylan HadfieldMenell, Stewart Slocum |  |
| 702 |  |  [Efficient Reinforcement Learning with Large Language Model Priors](https://openreview.net/forum?id=e2NRNQ0sZe) |  | 0 | In sequential decision-making (SDM) tasks, methods like reinforcement learning (RL) and heuristic search have made notable advances in specific cases. However, they often require extensive exploration and face challenges in generalizing across diverse environments due to their limited grasp of the... | Haifeng Zhang, Haitham BouAmmar, Jun Wang, Mengyue Yang, Xidong Feng, Xue Yan, Yan Song |  |
| 703 |  |  [Geometry-Aware Approaches for Balancing Performance and Theoretical Guarantees in Linear Bandits](https://openreview.net/forum?id=Oeb0I3JcVc) |  | 0 | This paper is motivated by recent research in the $d$-dimensional stochastic linear bandit literature, which has revealed an unsettling discrepancy: algorithms like Thompson sampling and Greedy demonstrate promising empirical performance, yet this contrasts with their pessimistic theoretical regret... | Mohsen Bayati, Yuwei Luo |  |
| 704 |  |  [Langevin Soft Actor-Critic: Efficient Exploration through Uncertainty-Driven Critic Learning](https://openreview.net/forum?id=FvQsk3la17) |  | 0 | Existing actor-critic algorithms, which are popular for continuous control reinforcement learning (RL) tasks, suffer from poor sample efficiency due to lack of principled exploration mechanism within them. Motivated by the success of Thompson sampling for efficient exploration in RL, we propose a... | Doina Precup, Guangyuan Wang, Haque Ishfaq, Sami Nur Islam |  |
| 705 |  |  [Searching for Optimal Solutions with LLMs via Bayesian Optimization](https://openreview.net/forum?id=aVfDrl7xDV) |  | 0 | Scaling test-time compute to search for optimal solutions is an important step towards building generally-capable language models that can reason. Recent work, however, shows that tasks of varying complexity require distinct search strategies to solve optimally, thus making it challenging to design... | Dhruv Agarwal, Manoj Ghuhan Arivazhagan, Rajarshi Das, Rashmi Gangadharaiah, Sandesh Swamy, Sopan Khosla |  |
| 706 |  |  [Context Steering: Controllable Personalization at Inference Time](https://openreview.net/forum?id=xQCXInDq0m) |  | 0 | To deliver high-quality, personalized responses, large language models (LLMs) must effectively incorporate context — personal, demographic, and cultural information specific to an end-user. For example, asking the model to explain Newton's second law with the context "I am a toddler'' should... | Anca D. Dragan, Jerry ZhiYang He, Mariah L. Schrum, Sashrika Pandey |  |
| 707 |  |  [Efficient Policy Evaluation with Safety Constraint for Reinforcement Learning](https://openreview.net/forum?id=Dem5LyVk8R) |  | 0 | In reinforcement learning, classic on-policy evaluation methods often suffer from high variance and require massive online data to attain the desired accuracy. Previous studies attempt to reduce evaluation variance by searching for or designing proper behavior policies to collect data. However,... | Claire Chen, Shangtong Zhang, Shuze Daniel Liu |  |
| 708 |  |  [Agent S: An Open Agentic Framework that Uses Computers Like a Human](https://openreview.net/forum?id=lIVRgt4nLv) |  | 0 | We present Agent S, an open agentic framework that enables autonomous interaction with computers through Graphical User Interface (GUI), aimed at transforming human-computer interaction by automating complex, multi-step tasks. Agent S addresses three key challenges in automating computer tasks:... | Ang Li, Jiachen Yang, Jiuzhou Han, Saaket Agashe, Shuyu Gan, Xin Eric Wang |  |
| 709 |  |  [Semantic Loss Guided Data Efficient Supervised Fine Tuning for Safe Responses in LLMs](https://openreview.net/forum?id=kO0DgO07hW) |  | 0 | Large Language Models (LLMs) generating unsafe responses to toxic prompts is a significant issue in their applications. While various efforts aim to address this safety concern, previous approaches often demand substantial human data collection or rely on the less dependable option of using another... | Arunesh Sinha, Pradeep Varakantham, Yuxiao Lu |  |
| 710 |  |  [Adversarial Generative Flow Network for Solving Vehicle Routing Problems](https://openreview.net/forum?id=tBom4xOW1H) |  | 0 | Recent research into solving vehicle routing problems (VRPs) has gained significant traction, particularly through the application of deep (reinforcement) learning for end-to-end solution construction. However, many current construction-based neural solvers predominantly utilize Transformer... | Jingfeng Yang, Ni Zhang, Xu Chi, Zhiguang Cao |  |
| 711 |  |  [Tracking the Copyright of Large Vision-Language Models through Parameter Learning Adversarial Images](https://openreview.net/forum?id=K7xpl3LZQp) |  | 0 | Large vision-language models (LVLMs) have demonstrated remarkable image understanding and dialogue capabilities, allowing them to handle a variety of visual question answering tasks. However, their widespread availability raises concerns about unauthorized usage and copyright infringement, where... | Chaohu Liu, Jianting Tang, Linli Xu, Yubo Wang |  |
| 712 |  |  [Self-Supervised Diffusion MRI Denoising via Iterative and Stable Refinement](https://openreview.net/forum?id=wxPnuFp8fZ) |  | 0 | Magnetic Resonance Imaging (MRI), including diffusion MRI (dMRI), serves as a \`\`microscope'' for anatomical structures and routinely mitigates the influence of low signal-to-noise ratio scans by compromising temporal or spatial resolution. However, these compromises fail to meet clinical demands... | Chenxu Wu, Qingpeng Kong, S. Kevin Zhou, Zihang Jiang |  |
| 713 |  |  [LongMamba: Enhancing Mamba's Long-Context Capabilities via Training-Free Receptive Field Enlargement](https://openreview.net/forum?id=fMbLszVO1H) |  | 0 | State space models (SSMs) have emerged as an efficient alternative to Transformer models for language modeling, offering linear computational complexity and constant memory usage as context length increases. However, despite their efficiency in handling long contexts, recent studies have shown that... | Jan Kautz, Jihoon Hong, Kejing Xia, Pavlo Molchanov, Shizhe Diao, Xiangchi Yuan, Xin Dong, Yingyan Celine Lin, Yonggan Fu, Zhifan Ye |  |
| 714 |  |  [Robust Weight Initialization for Tanh Neural Networks with Fixed Point Analysis](https://openreview.net/forum?id=Es4RPNDtmq) |  | 0 | As a neural network's depth increases, it can improve generalization performance. However, training deep networks is challenging due to gradient and signal propagation issues. To address these challenges, extensive theoretical research and various methods have been introduced. Despite these... | Hayoung Choi, Hyunju Kim, Hyunwoo Lee |  |
| 715 |  |  [Stiefel Flow Matching for Moment-Constrained Structure Elucidation](https://openreview.net/forum?id=84WmbzikPP) |  | 0 | Molecular structure elucidation is a fundamental step in understanding chemical phenomena, with applications in identifying molecules in natural products, lab syntheses, forensic samples, and the interstellar medium. We consider the task of predicting a molecule's all-atom 3D structure given only... | Alston Lo, Alán AspuruGuzik, Austin Henry Cheng, Kin Long Kelvin Lee, Santiago Miret |  |
| 716 |  |  [Lawma: The Power of Specialization for Legal Annotation](https://openreview.net/forum?id=7El7K1DoyX) |  | 0 | Annotation and classification of legal text are central components of empirical legal research. Traditionally, these tasks are often delegated to trained research assistants. Motivated by the advances in language modeling, empirical legal scholars are increasingly turning to commercial models,... | Christoph Engel, Jens Frankenreiter, Krishna P. Gummadi, Michael Livermore, Moritz Hardt, Rediet Abebe, Ricardo DominguezOlmedo, Stefan Bechtold, Vedant Nanda |  |
| 717 |  |  [OpenRCA: Can Large Language Models Locate the Root Cause of Software Failures?](https://openreview.net/forum?id=M4qNIzQYpd) |  | 0 | Large language models (LLMs) are driving substantial advancements in software engineering, with successful applications like Copilot and Cursor transforming real-world development practices. However, current research predominantly focuses on the early stages of development, such as code generation,... | Chaoyun Zhang, Dan Pei, Dongmei Zhang, Junjielong Xu, Pinjia He, Qi Zhang, Qinan Zhang, Qingwei Lin, Shilin He, Zhiqing Zhong |  |
| 718 |  |  [Wavelet-based Positional Representation for Long Context](https://openreview.net/forum?id=OhauMUNW8T) |  | 0 | In the realm of large-scale language models, a significant challenge arises when extrapolating sequences beyond the maximum allowable length. This is because the model's position embedding mechanisms are limited to positions encountered during training, thus preventing effective representation of... | Kuniko Saito, Kyosuke Nishida, Taku Hasegawa, Yui Oka |  |
| 719 |  |  [LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code](https://openreview.net/forum?id=chfJJYC3iL) |  | 0 | Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEvla, MBPP) are no longer sufficient for... | Alex Gu, Armando SolarLezama, Fanjia Yan, Ion Stoica, King Han, Koushik Sen, Naman Jain, Sida Wang, Tianjun Zhang, WenDing Li |  |
| 720 |  |  [ActionReasoningBench: Reasoning about Actions with and without Ramification Constraints](https://openreview.net/forum?id=NUD03NBDOE) |  | 0 | Reasoning about Actions and Change (RAC) has historically played a pivotal role in solving foundational AI problems, such as the frame problem. It has driven advancements in AI fields, such as non-monotonic and commonsense reasoning. RAC remains crucial for AI systems that operate in dynamic... | Chitta Baral, Divij Handa, Pavel Dolin, Shrinidhi Kumbhar, Tran Cao Son |  |
| 721 |  |  [No Preference Left Behind: Group Distributional Preference Optimization](https://openreview.net/forum?id=bgpNJBD6Va) |  | 0 | Preferences within a group of people are not uniform but follow a distribution. While existing alignment methods like Direct Preference Optimization (DPO) attempt to steer models to reflect human preferences, they struggle to capture the distributional pluralistic preferences within a group. These... | Binwei Yao, Diyi Yang, Junjie Hu, Ming Jiang, Shanglin Yang, YunShiuan Chuang, Zefan Cai |  |
| 722 |  |  [MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding](https://openreview.net/forum?id=CS2JWaziYr) |  | 0 | Large Language Models (LLMs) have become more prevalent in long-context applications such as interactive chatbots, document analysis, and agent workflows, but it is challenging to serve long-context requests with low latency and high throughput. Speculative decoding (SD) is a widely used technique... | Avner May, Beidi Chen, Ian EnHsu Yen, Jian Chen, Jinyuan Shi, Ranajoy Sadhukhan, Ruihang Lai, Tianqi Chen, Vashisth Tiwari, Zhuoming Chen |  |
| 723 |  |  [Parameter Expanded Stochastic Gradient Markov Chain Monte Carlo](https://openreview.net/forum?id=exgLs4snap) |  | 0 | Bayesian Neural Networks (BNNs) provide a promising framework for modeling predictive uncertainty and enhancing out-of-distribution robustness (OOD) by estimating the posterior distribution of network parameters. Stochastic Gradient Markov Chain Monte Carlo (SGMCMC) is one of the most powerful... | Chulhee Yun, Giung Nam, Hongseok Yang, Hyunsu Kim, Juho Lee |  |
| 724 |  |  [Hyperbolic Genome Embeddings](https://openreview.net/forum?id=NkGDNM8LB0) |  | 0 | Current approaches to genomic sequence modeling often struggle to align the inductive biases of machine learning models with the evolutionarily-informed structure of biological systems. To this end, we formulate a novel application of hyperbolic CNNs that exploits this structure, enabling more... | Itsik Pe'er, Philippe Chlenski, Raiyan R. Khan |  |
| 725 |  |  [Deep Distributed Optimization for Large-Scale Quadratic Programming](https://openreview.net/forum?id=hzuumhfYSO) |  | 0 | Quadratic programming (QP) forms a crucial foundation in optimization, appearing in a broad spectrum of domains and serving as the basis for more advanced algorithms. Consequently, as the scale and complexity of modern applications continue to grow, the development of efficient and reliable QP... | Alex Oshin, Arshiya Taj Abdul, Augustinos D. Saravanos, Evangelos A. Theodorou, Hunter Kuperman, Vincent Pacelli |  |
| 726 |  |  [Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining](https://openreview.net/forum?id=gU4ZgQNsOC) |  | 0 | Pretraining large language models (LLMs) on vast and heterogeneous datasets is crucial for achieving state-of-the-art performance across diverse downstream tasks. However, current training paradigms treat all samples equally, overlooking the importance or relevance of individual samples throughout... | Daouda Sow, HansArno Jacobsen, Herbert Woisetschläger, Saikiran Bulusu, Shiqiang Wang, Yingbin Liang |  |
| 727 |  |  [Frame-Voyager: Learning to Query Frames for Video Large Language Models](https://openreview.net/forum?id=LNL7zKvm7e) |  | 0 | Video Large Language Models (Video-LLMs) have made remarkable progress in video understanding tasks. However, they are constrained by the maximum length of input tokens, making it impractical to input entire videos. Existing frame selection approaches, such as uniform frame sampling and text-frame... | Bingni Zhang, Chengkai Jin, Hao Zhang, Huanyu Wang, Jiawei Wu, Qianru Sun, Sheng Jin, Sicheng Yu, Xiaolei Xu, Zhenbang Sun, Zhenghao Chen, Zhongrong Zuo |  |
| 728 |  |  [PN-GAIL: Leveraging Non-optimal Information from Imperfect Demonstrations](https://openreview.net/forum?id=0e2pcSxQJS) |  | 0 | Imitation learning aims at constructing an optimal policy by emulating expert demonstrations. However, the prevailing approaches in this domain typically presume that the demonstrations are optimal, an assumption that seldom holds true in the complexities of real-world applications. The data... | Chunlin Chen, Daoyi Dong, Huiqiao Fu, Kaiqiang Tang, Qiang Liu |  |
| 729 |  |  [Attributing Culture-Conditioned Generations to Pretraining Corpora](https://openreview.net/forum?id=XrsOu4KgDE) |  | 0 | In open-ended generative tasks like narrative writing or dialogue, large language models often exhibit cultural biases, showing limited knowledge and generating templated outputs for less prevalent cultures. Recent works show that these biases may stem from uneven cultural representation in... | Arnav Goel, Huihan Li, Keyu He, Xiang Ren |  |
| 730 |  |  [Semi-Parametric Retrieval via Binary Bag-of-Tokens Index](https://openreview.net/forum?id=l0fn10vSyM) |  | 0 | Information retrieval has transitioned from standalone systems into essential components across broader applications, with indexing efficiency, cost-effectiveness, and freshness becoming increasingly critical yet often overlooked. In this paper, we introduce SemI-parametric Disentangled Retrieval... | Furu Wei, Jiawei Zhou, Lei Chen, Li Dong |  |
| 731 |  |  [RTop-K: Ultra-Fast Row-Wise Top-K Selection for Neural Network Acceleration on GPUs](https://openreview.net/forum?id=PHg4rAXFVH) |  | 0 | Abstract Top-k selection algorithms are fundamental in a wide range of applications, including high-performance computing, information retrieval, big data processing, and neural network model training. In this paper, we present RTop-K, a highly efficient parallel row-wise top-k selection algorithm... | Caiwen Ding, Hongwu Peng, Xi Xie, Yuebo Luo |  |
| 732 |  |  [DelTA: An Online Document-Level Translation Agent Based on Multi-Level Memory](https://openreview.net/forum?id=hoYFLRNbhc) |  | 0 | Large language models (LLMs) have achieved reasonable quality improvements in machine translation (MT). However, most current research on MT-LLMs still faces significant challenges in maintaining translation consistency and accuracy when processing entire documents. In this paper, we introduce... | Derek F. Wong, Fandong Meng, Jiali Zeng, Jie Zhou, Min Zhang, Xuebo Liu, Yutong Wang |  |
| 733 |  |  [Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small LLMs](https://openreview.net/forum?id=eENHKMTOfW) |  | 0 | The rise of large language models (LLMs) has created a significant disparity: industrial research labs with their computational resources, expert teams, and advanced infrastructures, can effectively fine-tune LLMs, while individual developers and small organizations face barriers due to limited... | Abhishek Bhandwaldar, Akash Srivastava, Aldo Pareja, Guangxuan Xu, Hao Wang, Kai Xu, Krishnateja Killamsetty, Ligong Han, Luke Inglis, Nikhil Shivakumar Nayak, Seungwook Han, Shivchander Sudalairaj, Wenlong Zhao |  |
| 734 |  |  [ManiSkill-HAB: A Benchmark for Low-Level Manipulation in Home Rearrangement Tasks](https://openreview.net/forum?id=6bKEWevgSd) |  | 0 | High-quality benchmarks are the foundation for embodied AI research, enabling significant advancements in long-horizon navigation, manipulation and rearrangement tasks. However, as frontier tasks in robotics get more advanced, they require faster simulation speed, more intricate test environments,... | Arth Shukla, Hao Su, Stone Tao |  |
| 735 |  |  [How Gradient descent balances features: A dynamical analysis for two-layer neural networks](https://openreview.net/forum?id=25j2ZEgwTj) |  | 0 | This paper investigates the fundamental regression task of learning $k$ neurons (\emph{a.k.a.} teachers) from Gaussian input, using two-layer ReLU neural networks with width $m$ (\emph{a.k.a.} students) and $m, k= \mathcal{O}(1)$, trained via gradient descent under proper initialization and a small... | Fanghui Liu, Volkan Cevher, Zhenyu Zhu |  |
| 736 |  |  [A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement](https://openreview.net/forum?id=YaBiGjuDiC) |  | 0 | Reinforcement Learning from Human Feedback (RLHF) has become the predominant approach for aligning language models (LMs) to be more helpful and less harmful. At its core, RLHF uses a margin-based loss for preference optimization, which specifies the ideal LM behavior only in terms of the difference... | Huazheng Wang, Hui Yuan, Liu Leqi, Mengdi Wang, Yifan Zeng, Yue Wu |  |
| 737 |  |  [Model Editing as a Robust and Denoised variant of DPO: A Case Study on Toxicity](https://openreview.net/forum?id=lOi6FtIwR8) |  | 0 | Recent alignment algorithms such as direct preference optimization (DPO) have been developed to improve the safety of large language models (LLMs) by training these models to match human behaviors exemplified by preference data. However, these methods are both computationally intensive and lacking... | Apratim Dey, Junjie Hu, Rheeya Uppaal, Yiqiao Zhong, Yiting He |  |
| 738 |  |  [Should VLMs be Pre-trained with Image Data?](https://openreview.net/forum?id=Pj4Aid3XqL) |  | 0 | Pre-trained LLMs that are further trained with image data perform well on vision-language tasks. While adding images during a second training phase effectively unlocks this capability, it is unclear how much of a gain or loss this two-step pipeline gives over VLMs which integrate images earlier... | Achal Dave, Benjamin Burchfiel, Igor Vasiljevic, Jean Mercat, Kushal Arora, Ludwig Schmidt, Russ Tedrake, Samir Yitzhak Gadre, Sedrick Keh, Shuran Song, Thomas Kollar |  |
| 739 |  |  [DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models](https://openreview.net/forum?id=VOAMTA8jKu) |  | 0 | The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor modifications, we found that state-of-the-art VLMs like GPT-4o... | Bin Hu, Chengke Zou, Huan Zhang, Junyu Zhang, Rui Yang, Xingang Guo |  |
| 740 |  |  [MetaDesigner: Advancing Artistic Typography through AI-Driven, User-Centric, and Multilingual WordArt Synthesis](https://openreview.net/forum?id=Mv3GAYJGcW) |  | 0 | MetaDesigner introduces a transformative framework for artistic typography synthesis, powered by Large Language Models (LLMs) and grounded in a user-centric design paradigm. Its foundation is a multi-agent system comprising the Pipeline, Glyph, and Texture agents, which collectively orchestrate the... | Alexander G. Hauptmann, Bin Luo, Chenyang Li, Hanyuan Chen, JinPeng Lan, Jingdong Sun, JunYan He, Kang Zhu, Qi He, Wangmeng Xiang, Xianhui Lin, Xuansong Xie, Yifeng Geng, ZhiQi Cheng |  |
| 741 |  |  [Adaptive Rank Allocation: Speeding Up Modern Transformers with RaNA Adapters](https://openreview.net/forum?id=uAtDga3q0r) |  | 0 | Large Language Models (LLMs) are computationally intensive, particularly during inference. Neuron-adaptive techniques, which selectively activate neurons in Multi-Layer Perceptron (MLP) layers, offer some speedups but suffer from limitations in modern Transformers. These include reliance on sparse... | Daniel Sorvisto, Jerry Weihong Liu, Roberto Garcia, Sabri Eyuboglu |  |
| 742 |  |  [Efficient Jailbreak Attack sequences on Large Language Models via Multi-Armed Bandit-based Context switching](https://openreview.net/forum?id=jCDF7G3LpF) |  | 0 | Content warning: This paper contains examples of harmful language and content. Recent advances in large language models (LLMs) have made them increasingly vulnerable to jailbreaking attempts, where malicious users manipulate models into generating harmful content. While existing approaches rely on... | Aditya Ramesh, Aditya Saibewar, Manohar Kaul, Shivam Bhardwaj |  |
| 743 |  |  [A Little Goes a Long Way: Efficient Long Context Training and Inference with Partial Contexts](https://openreview.net/forum?id=TrKRpaOk8y) |  | 0 | Training and serving long-context large language models (LLMs) incurs substantial overhead. To address this, two critical steps are often required: a pretrained LLM typically undergoes a separate stage for context length extension by training on long-context data, followed by architectural... | Hao Peng, Jiawei Han, Suyu Ge, Xihui Lin, Yunan Zhang |  |
| 744 |  |  [What Secrets Do Your Manifolds Hold? Understanding the Local Geometry of Generative Models](https://openreview.net/forum?id=etif9j1CnG) |  | 0 | Deep Generative Models are frequently used to learn continuous representations of complex data distributions by training on a finite number of samples. For any generative model, including pre-trained foundation models with Diffusion or Transformer architectures, generation performance can... | Ahmed Imtiaz Humayun, Candice Schumann, Cristina Nader Vasconcelos, Deepak Ramachandran, Golnoosh Farnadi, Ibtihel Amara, Junfeng He, Katherine A. Heller, Mohammad Havaei, Negar Rostamzadeh |  |
| 745 |  |  [L-WISE: Boosting Human Visual Category Learning Through Model-Based Image Selection and Enhancement](https://openreview.net/forum?id=AoIKgHu9Si) |  | 0 | The currently leading artificial neural network models of the visual ventral stream - which are derived from a combination of performance optimization and robustification methods - have demonstrated a remarkable degree of behavioral alignment with humans on visual categorization tasks. We show that... | Gabriel Kreiman, Guy Gaziv, James J. DiCarlo, Morgan Bruce Talbot |  |
| 746 |  |  [Predictive Uncertainty Quantification for Bird's Eye View Segmentation: A Benchmark and Novel Loss Function](https://openreview.net/forum?id=k3y0oyK7sn) |  | 0 | The fusion of raw sensor data to create a Bird's Eye View (BEV) representation is critical for autonomous vehicle planning and control. Despite the growing interest in using deep learning models for BEV semantic segmentation, anticipating segmentation errors and enhancing the explainability of... | Bowen Yang, Feng Chen, Kangshuo Li, Linlin Yu, Tianhao Wang |  |
| 747 |  |  [DiscoveryBench: Towards Data-Driven Discovery with Large Language Models](https://openreview.net/forum?id=vyflgpwfJW) |  | 0 | Can the rapid advances in code generation, function calling, and data analysis using large language models (LLMs) help automate the search and verification of hypotheses purely from a set of provided datasets? To evaluate this question, we present DiscoveryBench, the first comprehensive benchmark... | Abhijeetsingh Meena, Aryan Prakhar, Ashish Sabharwal, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Dhruv Agarwal, Harshit Surana, Peter Clark, Tirth Vora, Tushar Khot |  |
| 748 |  |  [Optimizing (L0, L1)-Smooth Functions by Gradient Methods](https://openreview.net/forum?id=GQ1Tc3vHbt) |  | 0 | We study gradient methods for optimizing $(L_0, L_1)$-smooth functions, a class that generalizes Lipschitz-smooth functions and has gained attention for its relevance in machine learning. We provide new insights into the structure of this function class and develop a principled framework for... | Angelia Nedich, Anton Rodomanov, Daniil Vankov, Lalitha Sankar, Sebastian U. Stich |  |
| 749 |  |  [Compute-Optimal LLMs Provably Generalize Better with Scale](https://openreview.net/forum?id=MF7ljU8xcf) |  | 0 | Why do larger language models generalize better? To explore this question, we develop generalization bounds on the pretraining objective of large language models (LLMs) in the compute-optimal regime, as described by the Chinchilla scaling laws. We introduce a novel, fully empirical Freedman-type... | Andrew Gordon Wilson, Anming Gu, Christopher De Sa, Diego Granziol, J. Zico Kolter, Marc Anton Finzi, Sanyam Kapoor |  |
| 750 |  |  [Safety-Prioritizing Curricula for Constrained Reinforcement Learning](https://openreview.net/forum?id=f3QR9TEERH) |  | 0 | Curriculum learning aims to accelerate reinforcement learning (RL) by generating curricula, i.e., sequences of tasks of increasing difficulty. Although existing curriculum generation approaches provide benefits in sample efficiency, they overlook safety-critical settings where an RL agent must... | Cevahir Köprülü, Nils Jansen, Thiago D. Simão, Ufuk Topcu |  |
| 751 |  |  [Generalized Behavior Learning from Diverse Demonstrations](https://openreview.net/forum?id=Q7EjHroO1w) |  | 0 | Diverse behavior policies are valuable in domains requiring quick test-time adaptation or personalized human-robot interaction. Human demonstrations provide rich information regarding task objectives and factors that govern individual behavior variations, which can be used to characterize... | Letian Chen, Matthew C. Gombolay, Rohan R. Paleja, Sanne van Waveren, Varshith Sreeramdass |  |
| 752 |  |  [Neural Stochastic Differential Equations for Uncertainty-Aware Offline RL](https://openreview.net/forum?id=hxUMQ4fic3) |  | 0 | Offline model-based reinforcement learning (RL) offers a principled approach to using a learned dynamics model as a simulator to optimize a control policy. Despite the near-optimal performance of existing approaches on benchmarks with high-quality datasets, most struggle on datasets with low... | Cevahir Köprülü, Franck Djeumou, Ufuk Topcu |  |
| 753 |  |  [Towards Optimal Multi-draft Speculative Decoding](https://openreview.net/forum?id=9KxnxWOBA5) |  | 0 | Large Language Models (LLMs) have become an indispensable part of natural language processing tasks. However, autoregressive sampling has become an efficiency bottleneck. Multi-Draft Speculative Decoding (MDSD) is a recent approach where, when generating each token, a small draft model generates... | Dinesh Manocha, Heng Huang, Ryan A. Rossi, Tong Zheng, Vignesh Viswanathan, Yihan Wu, Zhengmian Hu, Ziyi Chen |  |
| 754 |  |  [Diffusion Generative Modeling for Spatially Resolved Gene Expression Inference from Histology Images](https://openreview.net/forum?id=FtjLUHyZAO) |  | 0 | Spatial Transcriptomics (ST) allows a high-resolution measurement of RNA sequence abundance by systematically connecting cell morphology depicted in Hematoxylin and eosin (H\&E) stained histology images to spatially resolved gene expressions. ST is a time-consuming, expensive yet powerful... | Molei Tao, Peng Qiu, Sichen Zhu, Yuchen Zhu |  |
| 755 |  |  [Diffusion State-Guided Projected Gradient for Inverse Problems](https://openreview.net/forum?id=kRBQwlkFSP) |  | 0 | Recent advancements in diffusion models have been effective in learning data priors for solving inverse problems. They leverage diffusion sampling steps for inducing a data prior while using a measurement guidance gradient at each step to impose data consistency. For general inverse problems,... | Anima Anandkumar, Bahareh Tolooshams, Rayhan Zirvi |  |
| 756 |  |  [MADGEN: Mass-Spec attends to De Novo Molecular generation](https://openreview.net/forum?id=78tc3EiUrN) |  | 0 | The annotation (assigning structural chemical identities) of MS/MS spectra remains a significant challenge due to the enormous molecular diversity in biological samples and the limited scope of reference databases. Currently, the vast majority of spectral measurements remain in the "dark chemical... | Liping Liu, Soha Hassoun, Xiaohui Chen, Yinkai Wang |  |
| 757 |  |  [Towards Federated RLHF with Aggregated Client Preference for LLMs](https://openreview.net/forum?id=mqNKiEB6pd) |  | 0 | Reinforcement learning with human feedback (RLHF) fine-tunes a pretrained large language model (LLM) using user preference data, enabling it to generate content aligned with human preferences. However, due to privacy concerns, users may be reluctant to share sensitive preference data. To address... | Feijie Wu, Haoyu Wang, Jing Gao, Lu Su, Xiaoze Liu, Xingchen Wang |  |
| 758 |  |  [Overcoming Slow Decision Frequencies in Continuous Control: Model-Based Sequence Reinforcement Learning for Model-Free Control](https://openreview.net/forum?id=w3iM4WLuvy) |  | 0 | Reinforcement learning (RL) is rapidly reaching and surpassing human-level control capabilities. However, state-of-the-art RL algorithms often require timesteps and reaction times significantly faster than human capabilities, which is impractical in real-world settings and typically necessitates... | Devdhar Patel, Hava T. Siegelmann |  |
| 759 |  |  [Physics of Language Models: Part 3.2, Knowledge Manipulation](https://openreview.net/forum?id=oDbiL9CLoS) |  | 0 | Language models can store vast factual knowledge, yet their ability to flexibly use this knowledge for downstream tasks (e.g., via instruction finetuning) remains questionable. This paper investigates four fundamental knowledge manipulation tasks: \textbf{retrieval} (e.g., "What is person A's... | Yuanzhi Li, Zeyuan AllenZhu |  |
| 760 |  |  [Logic-Logit: A Logic-Based Approach to Choice Modeling](https://openreview.net/forum?id=vJgJSrYPe1) |  | 0 | In this study, we propose a novel rule-based interpretable choice model, {\bf Logic-Logit}, designed to effectively learn and explain human choices. Choice models have been widely applied across various domains—such as commercial demand forecasting, recommendation systems, and consumer behavior... | Shuang Li, Shuhan Zhang, Wendi Ren |  |
| 761 |  |  [On Calibration of LLM-based Guard Models for Reliable Content Moderation](https://openreview.net/forum?id=wUbum0nd9N) |  | 0 | Large language models (LLMs) pose significant risks due to the potential for generating harmful content or users attempting to evade guardrails. Existing studies have developed LLM-based guard models designed to moderate the input and output of threat LLMs, ensuring adherence to safety policies by... | Hao Wang, Hengguan Huang, Hongfu Liu, Xiangming Gu, Ye Wang |  |
| 762 |  |  [Unified Convergence Analysis for Score-Based Diffusion Models with Deterministic Samplers](https://openreview.net/forum?id=HrdVqFSn1e) |  | 0 | Score-based diffusion models have emerged as powerful techniques for generating samples from high-dimensional data distributions. These models involve a two-phase process: first, injecting noise to transform the data distribution into a known prior distribution, and second, sampling to recover the... | Qiwei Di, Quanquan Gu, Runjia Li |  |
| 763 |  |  [Optimizing Neural Network Representations of Boolean Networks](https://openreview.net/forum?id=1H90Gb9rJ9) |  | 0 | Neural networks are known to be universal computers for Boolean functions. Recent advancements in hardware have significantly reduced matrix multiplication times, making neural network simulation both fast and efficient. Consequently, functions defined by complex Boolean networks are increasingly... | Devdhar Patel, Edward A. Rietman, Hava T. Siegelmann, Ignacio Gavier, Joshua Russell |  |
| 764 |  |  [Causal Order: The Key to Leveraging Imperfect Experts in Causal Inference](https://openreview.net/forum?id=9juyeCqL0u) |  | 0 | Large Language Models (LLMs) have recently been used as experts to infer causal graphs, often by repeatedly applying a pairwise prompt that asks about the causal relationship of each variable pair. However, such experts, including human domain experts, cannot distinguish between direct and indirect... | Abbavaram Gowtham Reddy, Abhinav Kumar, Amit Sharma, Aniket Vashishtha, Saketh Bachu, Vineeth N. Balasubramanian |  |
| 765 |  |  [Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School Math Problems](https://openreview.net/forum?id=zpDGwcmMV4) |  | 0 | Language models have demonstrated remarkable performance in solving reasoning tasks; however, even the strongest models still occasionally make reasoning mistakes. Recently, there has been active research aimed at improving reasoning accuracy, particularly by using pretrained language models to... | Tian Ye, Yuanzhi Li, Zeyuan AllenZhu, Zicheng Xu |  |
| 766 |  |  [Reasoning of Large Language Models over Knowledge Graphs with Super-Relations](https://openreview.net/forum?id=rTCJ29pkuA) |  | 0 | While large language models (LLMs) have made significant progress in processing and reasoning over knowledge graphs, current methods suffer from a high non-retrieval rate. This limitation reduces the accuracy of answering questions based on these graphs. Our analysis reveals that the combination of... | Julian Shun, Jundong Li, Junhong Lin, Song Wang, Xiaojie Guo, Yada Zhu |  |
| 767 |  |  [Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process](https://openreview.net/forum?id=Tn5B6Udq3E) |  | 0 | Recent advances in language models have demonstrated their capability to solve mathematical reasoning problems, achieving near-perfect accuracy on grade-school level math benchmarks like GSM8K. In this paper, we formally study how language models solve these problems. We design a series of... | Tian Ye, Yuanzhi Li, Zeyuan AllenZhu, Zicheng Xu |  |
| 768 |  |  [Agent-Oriented Planning in Multi-Agent Systems](https://openreview.net/forum?id=EqcLAU6gyU) |  | 0 | Through the collaboration of multiple LLM-empowered agents possessing diverse expertise and tools, multi-agent systems achieve impressive progress in solving real-world problems. Given the user queries, the meta-agents, serving as the brain within multi-agent systems, are required to decompose the... | Ao Li, Bolin Ding, Fugee Tsung, Songze Li, Yaliang Li, Yuexiang Xie |  |
| 769 |  |  [Context Clues: Evaluating Long Context Models for Clinical Prediction Tasks on EHR Data](https://openreview.net/forum?id=zg3ec1TdAP) |  | 0 | Foundation Models (FMs) trained on Electronic Health Records (EHRs) have achieved state-of-the-art results on numerous clinical prediction tasks. However, prior EHR FMs typically have context windows of $<$1k tokens, which prevents them from modeling full patient EHRs which can exceed 10k's of... | Christopher Ré, Ethan Steinberg, Jason Alan Fries, Michael Wornow, Miguel Angel Fuentes Hernandez, Nigam Shah, Sanmi Koyejo, Suhana Bedi |  |
| 770 |  |  [Plastic Learning with Deep Fourier Features](https://openreview.net/forum?id=NIkfix2eDQ) |  | 0 | Deep neural networks can struggle to learn continually in the face of non-stationarity, a phenomenon known as loss of plasticity. In this paper, we identify underlying principles that lead to plastic algorithms. We provide theoretical results showing that linear function approximation, as well as a... | Alex Lewandowski, Dale Schuurmans, Marlos C. Machado |  |
| 771 |  |  [STAFF: Speculative Coreset Selection for Task-Specific Fine-tuning](https://openreview.net/forum?id=FAfxvdv1Dy) |  | 0 | Task-specific fine-tuning is essential for the deployment of large language models (LLMs), but it requires significant computational resources and time. Existing solutions have proposed coreset selection methods to improve data efficiency and reduce model training overhead, but they still have... | Chao Shen, Juan Zhai, Shiqing Ma, Tianlin Li, Weipeng Jiang, Xiaoyu Zhang, Yang Liu |  |
| 772 |  |  [Time-to-Event Pretraining for 3D Medical Imaging](https://openreview.net/forum?id=zcTLpIfj9u) |  | 0 | With the rise of medical foundation models and the growing availability of imaging data, scalable pretraining techniques offer a promising way to identify imaging biomarkers predictive of future disease risk. While current self-supervised methods for 3D medical imaging models capture local... | Akshay S. Chaudhari, Alejandro Lozano, Curtis P. Langlotz, Ethan Steinberg, Jason Alan Fries, Jeya Maria Jose Valanarasu, Louis Blankemeier, Nigam Shah, Zepeng Frazier Huo |  |
| 773 |  |  [ExACT: Teaching AI Agents to Explore with Reflective-MCTS and Exploratory Learning](https://openreview.net/forum?id=GBIUbwW9D8) |  | 0 | Autonomous agents have demonstrated significant potential in automating complex multistep decision-making tasks. However, even state-of-the-art vision-language models (VLMs), such as GPT-4o, still fall short of human-level performance, particularly in intricate web environments and long-horizon... | Baolin Peng, Hao Cheng, Jianfeng Gao, Michel Galley, Vineeth Vajipey, Xiao Yu, Zhou Yu |  |
| 774 |  |  [TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models](https://openreview.net/forum?id=fCi4o83Mfs) |  | 0 | Existing benchmarks often highlight the remarkable performance achieved by state-of-the-art Multimodal Foundation Models (MFMs) in leveraging temporal context for video understanding. However, \*how well do the models truly perform visual temporal reasoning?\* Our study of existing benchmarks shows... | Arman Cohan, Chuhan Li, Tesca Fitzgerald, Yanan Zheng, Yilun Zhao, Yuxuan Ding, Ziyao Shangguan |  |
| 775 |  |  [Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers](https://openreview.net/forum?id=yzloNYH3QN) |  | 0 | Information retrieval (IR) systems have played a vital role in modern digital life and have cemented their continued usefulness in this new era of generative AI via retrieval-augmented generation. With strong language processing capabilities and remarkable versatility, large language models (LLMs)... | Bernal Jimenez Gutierrez, Shijie Chen, Yu Su |  |
| 776 |  |  [Inspection and Control of Self-Generated-Text Recognition Ability in Llama3-8b-Instruct](https://openreview.net/forum?id=wWnsoLhHwt) |  | 0 | It has been reported that LLMs can recognize their own writing. As this has potential implications for AI safety, yet is relatively understudied, we investigate the phenomenon, seeking to establish: whether it robustly occurs at the behavioral level, how the observed behavior is achieved, and... | Christopher Ackerman, Nina Panickssery |  |
| 777 |  |  [Towards Learning High-Precision Least Squares Algorithms with Sequence Models](https://openreview.net/forum?id=snocoXIQXz) |  | 0 | This paper investigates whether sequence models can learn to perform numerical algorithms, e.g. gradient descent, on the fundamental problem of least squares. Our goal is to inherit two properties of standard algorithms from numerical analysis: (1) machine precision, i.e. we want to obtain... | Ashish Rao, Atri Rudra, Christopher Ré, Jerry Weihong Liu, Jessica Grogan, Owen M. Dugan, Simran Arora |  |
| 778 |  |  [Learning Efficient Positional Encodings with Graph Neural Networks](https://openreview.net/forum?id=AWg2tkbydO) |  | 0 | Positional encodings (PEs) are essential for effective graph representation learning because they provide position awareness in inherently position-agnostic transformer architectures and increase the expressive capacity of Graph Neural Networks (GNNs). However, designing powerful and efficient PEs... | Alejandro Ribeiro, Charilaos I. Kanatsoulis, Evelyn Choi, Jure Leskovec, Stefanie Jegelka |  |
| 779 |  |  [Scale-Aware Contrastive Reverse Distillation for Unsupervised Medical Anomaly Detection](https://openreview.net/forum?id=HNOo4UNPBF) |  | 0 | Unsupervised anomaly detection using deep learning has garnered significant research attention due to its broad applicability, particularly in medical imaging where labeled anomalous data are scarce. While earlier approaches leverage generative models like autoencoders and generative adversarial... | Chunlei Li, Jingliang Hu, Lichao Mou, Xiao Xiang Zhu, Yilei Shi |  |
| 780 |  |  [Scalable Influence and Fact Tracing for Large Language Model Pretraining](https://openreview.net/forum?id=gLa96FlWwn) |  | 0 | Training data attribution (TDA) methods aim to attribute model outputs back to specific training examples, and the application of these methods to large language model (LLM) outputs could significantly advance model transparency and data curation. However, it has been challenging to date to apply... | Dheeraj Rajagopal, Ian Tenney, Lucas Dixon, Tolga Bolukbasi, Tyler A. Chang |  |
| 781 |  |  [AdvPaint: Protecting Images from Inpainting Manipulation via Adversarial Attention Disruption](https://openreview.net/forum?id=m73tETvFkX) |  | 0 | The outstanding capability of diffusion models in generating high-quality images poses significant threats when misused by adversaries. In particular, we assume malicious adversaries exploiting diffusion models for inpainting tasks, such as replacing a specific region with a celebrity. While... | Joonsung Jeon, Sooel Son, Suhyeon Ha, SungEui Yoon, Woo Jae Kim |  |
| 782 |  |  [Mastering Task Arithmetic: τJp as a Key Indicator for Weight Disentanglement](https://openreview.net/forum?id=1VwWi6zbxs) |  | 0 | Model-editing techniques using task arithmetic have rapidly gained attention. Through task arithmetic, simply through arithmetic operations on the weights of pre-trained and fine-tuned models create desired models, such as multi-task models, models in which specific tasks are unsolvable, or... | Hiroki Naganuma, Julian J. McAuley, Kotaro Yoshida, Ryosuke Yamaki, Ryotaro Shimizu, Takafumi Horie, Yuji Naraki, Yuki Saito |  |
| 783 |  |  [Generating CAD Code with Vision-Language Models for 3D Designs](https://openreview.net/forum?id=BLWaTeucYX) |  | 0 | Generative AI has transformed the fields of Design and Manufacturing by providing efficient and automated methods for generating and modifying 3D objects. One approach involves using Large Language Models (LLMs) to generate Computer- Aided Design (CAD) scripting code, which can then be executed to... | Kamel Alrashedy, Matthew C. Gombolay, Megan Langwasser, Pradyumna Tambwekar, Wei Xu, Zulfiqar Haider Zaidi |  |
| 784 |  |  [A Formal Framework for Understanding Length Generalization in Transformers](https://openreview.net/forum?id=U49N5V51rU) |  | 0 | A major challenge for transformers is generalizing to sequences longer than those observed during training. While previous works have empirically shown that transformers can either succeed or fail at length generalization depending on the task, theoretical understanding of this phenomenon remains... | Andreas Krebs, Andy Yang, Hattie Zhou, Michael Hahn, Preetum Nakkiran, Satwik Bhattamishra, Xinting Huang, Yash Raj Sarrof |  |
| 785 |  |  [Palmbench: a comprehensive Benchmark of Compressed Large Language Models on Mobile Platforms](https://openreview.net/forum?id=xzSUdw6s76) |  | 0 | Deploying large language models (LLMs) locally on mobile devices is advantageous in scenarios where transmitting data to remote cloud servers is either undesirable due to privacy concerns or impractical due to network connection. Recent advancements have facilitated the local deployment of LLMs.... | Hao Zhang, Jayaram Raghuram, Jingyu Liu, M. Badri Narayanan, Shuai Zhang, Suman Banerjee, Utkarsh Sharma, Yijing Zeng, Yilong Li |  |
| 786 |  |  [On Stochastic Contextual Bandits with Knapsacks in Small Budget Regime](https://openreview.net/forum?id=FCMpUOZkxi) |  | 0 | This paper studies stochastic contextual bandits with knapsack constraints (CBwK), where a learner observes a context, takes an action, receives a reward, and incurs a vector of costs at every round. The learner aims to maximize the cumulative rewards across $T$ rounds under the knapsack... | Hengquan Guo, Xin Liu |  |
| 787 |  |  [From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation](https://openreview.net/forum?id=JBXO05r4AV) |  | 0 | Recent advances in long-context large language models (LLMs) have led to the emerging paradigm of many-shot in-context learning (ICL), where it is observed that scaling many more demonstrating examples beyond the conventional few-shot setup in the context can lead to performance benefits. However,... | Han Zhou, Ruoxi Sun, Sercan Ö. Arik, Xingchen Wan |  |
| 788 |  |  [Learning Neural Networks with Distribution Shift: Efficiently Certifiable Guarantees](https://openreview.net/forum?id=ed7zI29lRF) |  | 0 | We give the first provably efficient algorithms for learning neural networks with respect to distribution shift. We work in the Testable Learning with Distribution Shift framework (TDS learning) of Klivans et al. (2024), where the learner receives labeled examples from a training distribution and... | Adam R. Klivans, Gautam Chandrasekaran, Konstantinos Stavropoulos, Lin Lin Lee |  |
| 789 |  |  [Forgetting Transformer: Softmax Attention with a Forget Gate](https://openreview.net/forum?id=q2Lnyegkr8) |  | 0 | An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name... | Aaron C. Courville, Evgenii Nikishin, Xu Owen He, Zhixuan Lin |  |
| 790 |  |  [MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization](https://openreview.net/forum?id=R4q3cY3kQf) |  | 0 | Reinforcement learning (RL) algorithms aim to balance exploiting the current best strategy with exploring new options that could lead to higher rewards. Most common RL algorithms use undirected exploration, i.e., select random sequences of actions. Exploration can also be directed using intrinsic... | Andreas Krause, Bhavya Sukhija, Carmelo Sferrazza, Pieter Abbeel, Stelian Coros |  |
| 791 |  |  [LancBiO: Dynamic Lanczos-aided Bilevel Optimization via Krylov Subspace](https://openreview.net/forum?id=wLmJIs1uqG) |  | 0 | Bilevel optimization, with broad applications in machine learning, has an intricate hierarchical structure. Gradient-based methods have emerged as a common approach to large-scale bilevel problems. However, the computation of the hyper-gradient, which involves a Hessian inverse vector product,... | Bin Gao, Yan Yang, Yaxiang Yuan |  |
| 792 |  |  [Toward Understanding In-context vs. In-weight Learning](https://openreview.net/forum?id=aKJr5NnN8U) |  | 0 | It has recently been demonstrated empirically that in-context learning emerges in transformers when certain distributional properties are present in the training data, but this ability can also diminish upon further training. We provide a new theoretical understanding of these phenomena by... | András György, Bryan Chan, Dale Schuurmans, Xinyi Chen |  |
| 793 |  |  [RouteLLM: Learning to Route LLMs from Preference Data](https://openreview.net/forum?id=8sSqNntaMr) |  | 0 | Large language models (LLMs) excel at a wide range of tasks, but choosing the right model often involves balancing performance and cost. Powerful models offer better results but are expensive, while smaller models are more cost-effective but less capable. To address this trade-off, we introduce a... | Amjad Almahairi, Ion Stoica, Isaac Ong, Joseph E. Gonzalez, M. Waleed Kadous, Tianhao Wu, Vincent Wu, WeiLin Chiang |  |
| 794 |  |  [Score-based Self-supervised MRI Denoising](https://openreview.net/forum?id=uNd289HjLi) |  | 0 | Magnetic resonance imaging (MRI) is a powerful noninvasive diagnostic imaging tool that provides unparalleled soft tissue contrast and anatomical detail. Noise contamination, especially in accelerated and/or low-field acquisitions, can significantly degrade image quality and diagnostic accuracy.... | Fan Lam, Jiachen Tu, Yaokun Shi |  |
| 795 |  |  [ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing](https://openreview.net/forum?id=4D0f16Vwc3) |  | 0 | Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to scale up model capacity without increasing the computation budget. However, vanilla TopK routers are trained in a discontinuous, non-differentiable way, limiting their performance and scalability. To address this issue, we... | Jianfei Chen, Jun Zhu, Ziteng Wang |  |
| 796 |  |  [HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks](https://openreview.net/forum?id=6fDjUoEQvm) |  | 0 | Mechanistic interpretability has made great strides in identifying neural network features (e.g., directions in hidden activation space) that mediate concepts (e.g., \*the birth year of a Nobel laureate\*) and enable predictable manipulation. Distributed alignment search (DAS) leverages supervision... | Atticus Geiger, Christopher Potts, Jing Huang, Jiuding Sun, Karel D'Oosterlinck, Michael Sklar, Sidharth Baskaran |  |
| 797 |  |  [GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models](https://openreview.net/forum?id=AjXkRZIvjB) |  | 0 | Recent advancements in Large Language Models (LLMs) have sparked interest in their mathematical reasoning capabilities. While performance on the widely popular GSM8K benchmark has improved, questions remain about whether reported evaluation metrics are reliable, and reasoning abilities of LLMs have... | Hooman Shahrokhi, Iman Mirzadeh, Keivan Alizadeh, Mehrdad Farajtabar, Oncel Tuzel, Samy Bengio |  |
| 798 |  |  [SEMDICE: Off-policy State Entropy Maximization via Stationary Distribution Correction Estimation](https://openreview.net/forum?id=rJ5g8ueQaI) |  | 0 | In the unsupervised pre-training for reinforcement learning, the agent aims to learn a prior policy for downstream tasks without relying on task-specific reward functions. We focus on state entropy maximization (SEM), where the goal is to learn a policy that maximizes the entropy of the state's... | Jongmin Lee, Meiqi Sun, Pieter Abbeel |  |
| 799 |  |  [Identifiability for Gaussian Processes with Holomorphic Kernels](https://openreview.net/forum?id=FUaDMRVrbS) |  | 0 | Gaussian processes (GPs) are widely recognized for their robustness and flexibility across various domains, including machine learning, time series, spatial statistics, and biomedicine. In addition to their common usage in regression tasks, GP kernel parameters are frequently interpreted in various... | Ameer Qaqish, Didong Li |  |
| 800 |  |  [Discovering Group Structures via Unitary Representation Learning](https://openreview.net/forum?id=Tz8Li6G2xU) |  | 0 | Discovering group structures within data poses a fundamental challenge across diverse scientific domains. A key obstacle is the non-differentiable nature of group axioms, hindering their integration into deep learning frameworks. To address this, we introduce a novel differentiable approach... | Dongsung Huh |  |
| 801 |  |  [Image Watermarks are Removable using Controllable Regeneration from Clean Noise](https://openreview.net/forum?id=mDKxlfraAn) |  | 0 | Image watermark techniques provide an effective way to assert ownership, deter misuse, and trace content sources, which has become increasingly essential in the era of large generative models. A critical attribute of watermark techniques is their robustness against various manipulations. In this... | Hai Ci, Haofan Wang, Mike Zheng Shou, Yepeng Liu, Yiren Song, Yu Zhang, Yuheng Bu |  |
| 802 |  |  [Point Cluster: A Compact Message Unit for Communication-Efficient Collaborative Perception](https://openreview.net/forum?id=54XlM8Clkg) |  | 0 | The objective of the collaborative perception task is to enhance the individual agent's perception capability through message communication among neighboring agents. A central challenge lies in optimizing the inherent trade-off between perception ability and communication cost. To tackle this... | Hongsheng Li, Hongyu Li, Jiahui Fu, Shifeng Zhang, Si Liu, Siheng Chen, Xu Zhou, Zihan Ding |  |
| 803 |  |  [A Theoretical Analysis of Self-Supervised Learning for Vision Transformers](https://openreview.net/forum?id=Antib6Uovh) |  | 0 | Self-supervised learning has become a cornerstone in computer vision, primarily divided into reconstruction-based methods like masked autoencoders (MAE) and discriminative methods such as contrastive learning (CL). Recent empirical observations reveal that MAE and CL capture different types of... | Yingbin Liang, Yu Huang, Yuejie Chi, Zixin Wen |  |
| 804 |  |  [ProteinBench: A Holistic Evaluation of Protein Foundation Models](https://openreview.net/forum?id=BksqWM8737) |  | 0 | Recent years have witnessed a surge in the development of protein foundation models, significantly improving performance in protein prediction and generative tasks ranging from 3D structure prediction and protein design to conformational dynamics. However, the capabilities and limitations... | Dongyu Xue, Fei Ye, Lihao Wang, Quanquan Gu, Xiangxin Zhou, Xinyou Wang, Yan Wang, Yiming Ma, Yuning Shen, Zaixiang Zheng |  |
| 805 |  |  [LeanQuant: Accurate and Scalable Large Language Model Quantization with Loss-error-aware Grid](https://openreview.net/forum?id=ISqx8giekS) |  | 0 | Large language models (LLMs) have shown immense potential across various domains, but their high memory requirements and inference costs remain critical challenges for deployment. Post-training quantization (PTQ) has emerged as a promising technique to reduce memory requirements and decoding... | Anshumali Shrivastava, Tianyi Zhang |  |
| 806 |  |  [Strategist: Self-improvement of LLM Decision Making via Bi-Level Tree Search](https://openreview.net/forum?id=gfI9v7AbFg) |  | 0 | Traditional reinforcement learning and planning require a lot of data and training to develop effective strategies. On the other hand, large language models (LLMs) can generalize well and perform tasks without prior training but struggle with complex planning and decision-making. We introduce... | Guanzhi Wang, Jonathan Light, Min Cai, Wei Cheng, Weiqin Chen, Xiusi Chen, Yisong Yue, Ziniu Hu |  |
| 807 |  |  [DataMan: Data Manager for Pre-training Large Language Models](https://openreview.net/forum?id=eNbA8Fqir4) |  | 0 | The performance emergence of large language models (LLMs) driven by data scaling laws makes the selection of pre-training data increasingly important. However, existing methods rely on limited heuristics and human intuition, lacking comprehensive and clear guidelines. To address this, we are... | Dayiheng Liu, Junbo Zhao, Junyang Lin, Kexin Yang, Ru Peng, Yawen Zeng |  |
| 808 |  |  [EditRoom: LLM-parameterized Graph Diffusion for Composable 3D Room Layout Editing](https://openreview.net/forum?id=Y2Dh8rWwlb) |  | 0 | Given the steep learning curve of professional 3D software and the time- consuming process of managing large 3D assets, language-guided 3D scene editing has significant potential in fields such as virtual reality, augmented reality, and gaming. However, recent approaches to language-guided 3D scene... | Jianfeng Wang, Jing Gu, Kaizhi Zheng, Kevin Lin, Lijuan Wang, Linjie Li, Xiaotong Chen, Xin Eric Wang, Xuehai He, Zhengyuan Yang |  |
| 809 |  |  [LocoVR: Multiuser Indoor Locomotion Dataset in Virtual Reality](https://openreview.net/forum?id=9mBodivRIo) |  | 0 | Understanding human locomotion is crucial for AI agents such as robots, particularly in complex indoor home environments. Modeling human trajectories in these spaces requires insight into how individuals maneuver around physical obstacles and manage social navigation dynamics. These dynamics... | Kojiro Takeyama, Misha Sra, Yimeng Liu |  |
| 810 |  |  [Prompting Fairness: Integrating Causality to Debias Large Language Models](https://openreview.net/forum?id=7GKbQ1WT1C) |  | 0 | Large language models (LLMs), despite their remarkable capabilities, are susceptible to generating biased and discriminatory responses. As LLMs increasingly influence high-stakes decision-making (e.g., hiring and healthcare), mitigating these biases becomes critical. In this work, we propose a... | Jingling Li, Kun Zhang, Liu Leqi, Peter Spirtes, Xiaoyu Liu, Yang Liu, Zeyu Tang |  |
| 811 |  |  [GReaTer: Gradients Over Reasoning Makes Smaller Language Models Strong Prompt Optimizers](https://openreview.net/forum?id=fWRBheSJth) |  | 0 | The effectiveness of large language models (LLMs) is closely tied to the design of prompts, making prompt optimization essential for enhancing their performance across a wide range of tasks. Although recent advancements have focused on automating prompt engineering, many existing approaches rely... | Bo Pang, Caiming Xiong, Rui Zhang, Ryo Kamoi, Sarkar Snigdha Sarathi Das, Yusen Zhang |  |
| 812 |  |  [Efficient Causal Decision Making with One-sided Feedback](https://openreview.net/forum?id=UWdPsY7agk) |  | 0 | We study a class of decision-making problems with one-sided feedback, where outcomes are only observable for specific actions. A typical example is bank loans, where the repayment status is known only if a loan is approved and remains undefined if rejected. In such scenarios, conventional... | Jianing Chu, Pulak Ghosh, Shu Yang, Wenbin Lu |  |
| 813 |  |  [LANTERN: Accelerating Visual Autoregressive Models with Relaxed Speculative Decoding](https://openreview.net/forum?id=98d7DLMGdt) |  | 0 | Auto-Regressive (AR) models have recently gained prominence in image generation, often matching or even surpassing the performance of diffusion models. However, one major limitation of AR models is their sequential nature, which processes tokens one at a time, slowing down generation compared to... | Doohyuk Jang, Eunho Yang, Jihun Yun, June Yong Yang, Sihwan Park, Souvik Kundu, Sungyub Kim, Yeonsung Jung |  |
| 814 |  |  [LICO: Large Language Models for In-Context Molecular Optimization](https://openreview.net/forum?id=yu1vqQqKkx) |  | 0 | Optimizing black-box functions is a fundamental problem in science and engineering. To solve this problem, many approaches learn a surrogate function that estimates the underlying objective from limited historical evaluations. Large Language Models (LLMs), with their strong pattern-matching... | Aditya Grover, Tung Nguyen |  |
| 815 |  |  [Forking Paths in Neural Text Generation](https://openreview.net/forum?id=8RCmNLeeXx) |  | 0 | Estimating uncertainty in Large Language Models (LLMs) is important for properly evaluating LLMs, and ensuring safety for users. However, prior approaches to uncertainty estimation focus on the final answer in generated text, ignoring intermediate steps that might dramatically impact the outcome.... | Ari Holtzman, Eric J. Bigelow, Hidenori Tanaka, Tomer David Ullman |  |
| 816 |  |  [BRAID: Input-driven Nonlinear Dynamical Modeling of Neural-Behavioral Data](https://openreview.net/forum?id=3usdM1AuI3) |  | 0 | Neural populations exhibit complex recurrent structures that drive behavior, while continuously receiving and integrating external inputs from sensory stimuli, upstream regions, and neurostimulation. However, neural populations are often modeled as autonomous dynamical systems, with little... | Maryam Shanechi, Omid G. Sani, Parsa Vahidi |  |
| 817 |  |  [LLM-based Typed Hyperresolution for Commonsense Reasoning with Knowledge Bases](https://openreview.net/forum?id=wNobG8bV5Q) |  | 0 | Large language models (LLM) are being increasingly applied to tasks requiring commonsense reasoning. Despite their outstanding potential, the reasoning process of LLMs is prone to errors and hallucinations that hinder their applicability, especially in high-stakes scenarios. Several works have... | Ali Pesaranghader, Armin Toroghi, Scott Sanner, Tanmana Sadhu |  |
| 818 |  |  [What's the Move? Hybrid Imitation Learning via Salient Points](https://openreview.net/forum?id=r0pLGGcuY6) |  | 0 | While imitation learning (IL) offers a promising framework for teaching robots various behaviors, learning complex tasks remains challenging. Existing IL policies struggle to generalize effectively across visual and spatial variations even for simple tasks. In this work, we introduce... | Dorsa Sadigh, Hengyuan Hu, Jeannette Bohg, Priya Sundaresan, Quan Vuong |  |
| 819 |  |  [PEARL: Towards Permutation-Resilient LLMs](https://openreview.net/forum?id=txoJvjfI9w) |  | 0 | The in-context learning (ICL) capability of large language models (LLMs) enables them to perform challenging tasks using provided demonstrations. However, ICL is highly sensitive to the ordering of demonstrations, leading to instability in predictions. This paper shows that this vulnerability can... | Bin Liang, KamFai Wong, Li Shen, Liang Chen, Xiaoyan Zhao, Yang Deng |  |
| 820 |  |  [SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domains?](https://openreview.net/forum?id=riTiq3i21b) |  | 0 | Autonomous systems for software engineering are now capable of fixing bugs and developing features. These systems are commonly evaluated on SWE-bench (Jimenez et al., 2024a), which assesses their ability to solve software issues from GitHub repositories. However, SWE-bench uses only Python... | Alex L. Zhang, Carlos E. Jimenez, Diyi Yang, Gabriel Synnaeve, John Yang, Joyce Yang, Karthik R. Narasimhan, Kilian Lieret, Niklas Muennighoff, Ofir Press, Ori Press, Sida Wang, Xindi Wu |  |
| 821 |  |  [Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems](https://openreview.net/forum?id=Y4aWwRh25b) |  | 0 | Retrieval-Augmented Generation (RAG) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation. We study the risk of datastore leakage in Retrieval-In-Context RAG Language Models (LMs). We show that an adversary can exploit LMs'... | Eric P. Xing, Hanlin Zhang, Himabindu Lakkaraju, Sham M. Kakade, Zhenting Qi |  |
| 822 |  |  [Collab: Controlled Decoding using Mixture of Agents for LLM Alignment](https://openreview.net/forum?id=7ohlQUbTpp) |  | 0 | Alignment of Large Language models (LLMs) is crucial for safe and trustworthy deployment in applications. Reinforcement learning from human feedback (RLHF) has emerged as an effective technique to align LLMs to human preferences, and broader utilities, but it requires updating billions of model... | Alec Koppel, Dinesh Manocha, Furong Huang, Jiahao Qiu, Mengdi Wang, Soumya Suvra Ghosal, Souradip Chakraborty, Sujay Bhatt, Sumitra Ganesh, Udari Madhushani Sehwag |  |
| 823 |  |  [Finally Rank-Breaking Conquers MNL Bandits: Optimal and Efficient Algorithms for MNL Assortment](https://openreview.net/forum?id=kx8i1yfkRX) |  | 0 | We address the problem of active online assortment optimization problem with preference feedback, which is a framework for modeling user choices and subsetwise utility maximization. The framework is useful in various real-world applications including ad placement, online retail, recommender... | Aadirupa Saha, Pierre Gaillard |  |
| 824 |  |  [Efficient Top-m Data Values Identification for Data Selection](https://openreview.net/forum?id=lOfuvmi2HT) |  | 0 | Data valuation has found many real-world applications, e.g., data pricing and data selection. However, the most adopted approach -- Shapley value (SV) -- is computationally expensive due to the large number of model trainings required. Fortunately, most applications (e.g., data selection) require... | Bryan Kian Hsiang Low, SeeKiong Ng, Xiaoqiang Lin, Xinyi Xu |  |
| 825 |  |  [ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery](https://openreview.net/forum?id=6z4YKr0GK6) |  | 0 | The advancements of language language models (LLMs) have piqued growing interest in developing LLM-based language agents to automate scientific discovery end-to-end, which has sparked both excitement and skepticism about the true capabilities of such agents. In this work, we argue that for an agent... | Benjamin Burns, Boshi Wang, Botao Yu, Chen Wei, Daniel AduAmpratwum, Frazier N. Baker, Huan Sun, Mingyi Xue, Qianheng Zhang, Shijie Chen, Song Gao, Vishal Dey, Xia Ning, Xuhui Huang, Yifei Li, Yu Su, Yuting Ning, Zeyi Liao, Ziru Chen, Zitong Lu |  |
| 826 |  |  [RazorAttention: Efficient KV Cache Compression Through Retrieval Heads](https://openreview.net/forum?id=tkiZQlL04w) |  | 0 | The memory and computational demands of Key-Value (KV) cache present significant challenges for deploying long-context language models. Previous approaches attempt to mitigate this issue by selectively dropping tokens, which irreversibly erases critical information that might be needed for future... | Danning Ke, Gongyi Wang, Hanlin Tang, Jing Lin, Qingsen Han, Shikuan Hong, Yang Lin, Yiwu Yao |  |
| 827 |  |  [Simple is Effective: The Roles of Graphs and Large Language Models in Knowledge-Graph-Based Retrieval-Augmented Generation](https://openreview.net/forum?id=JvkuZZ04O7) |  | 0 | Large Language Models (LLMs) demonstrate strong reasoning abilities but face limitations such as hallucinations and outdated knowledge. Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by grounding LLM outputs in structured external knowledge from KGs. However,... | Mufei Li, Pan Li, Siqi Miao |  |
| 828 |  |  [MrT5: Dynamic Token Merging for Efficient Byte-level Language Models](https://openreview.net/forum?id=VYWBMq1L7H) |  | 0 | Models that rely on subword tokenization have significant drawbacks, such as sensitivity to character-level noise like spelling errors and inconsistent compression rates across different languages and scripts. While character- or byte-level models like ByT5 attempt to address these concerns, they... | Christopher D. Manning, Christopher Potts, Julie Kallini, Róbert Csordás, Shikhar Murty |  |
| 829 |  |  [VVC-Gym: A Fixed-Wing UAV Reinforcement Learning Environment for Multi-Goal Long-Horizon Problems](https://openreview.net/forum?id=5xSRg3eYZz) |  | 0 | Multi-goal long-horizon problems are prevalent in real-world applications. The additional goal space introduced by multi-goal problems intensifies the spatial complexity of exploration; meanwhile, the long interaction sequences in long-horizon problems exacerbate the temporal complexity of... | Bo Ding, Dawei Feng, Huaimin Wang, Kele Xu, Si Zheng, Weijia Wang, Xing Zhou, Xudong Gong, Zhangjun Sun |  |
| 830 |  |  [Commit0: Library Generation from Scratch](https://openreview.net/forum?id=MMwaQEVsAg) |  | 0 | With the goal of benchmarking generative systems beyond expert software development ability, we introduce Commit0, a benchmark that challenges AI agents to write libraries from scratch. Agents are provided with a specification document outlining the library’s API as well as a suite of interactive... | Alexander M. Rush, Celine Lee, Claire Cardie, Justin T. Chiu, Matthias Gallé, Nan Jiang, Wenting Zhao |  |
| 831 |  |  [Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control](https://openreview.net/forum?id=1Njl73JKjB) |  | 0 | Disentangling model activations into human-interpretable features is a central problem in interpretability. Sparse autoencoders (SAEs) have recently attracted much attention as a scalable unsupervised approach to this problem. However, our imprecise understanding of ground-truth features in... | Aleksandar Makelov, Georg Lange, Neel Nanda |  |
| 832 |  |  [CL-MFAP: A Contrastive Learning-Based Multimodal Foundation Model for Molecular Property Prediction and Antibiotic Screening](https://openreview.net/forum?id=fv9XU7CyN2) |  | 0 | Due to the rise in antimicrobial resistance, identifying novel compounds with antibiotic potential is crucial for combatting this global health issue. However, traditional drug development methods are costly and inefficient. Recognizing the pressing need for more effective solutions, researchers... | Gen Zhou, Pingzhao Hu, Sugitha Janarthanan, Yutong Lu |  |
| 833 |  |  [SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators](https://openreview.net/forum?id=u3TL0qxLWf) |  | 0 | Large Language Models (LLMs) have transformed natural language processing, but face significant challenges in widespread deployment due to their high runtime cost. In this paper, we introduce SeedLM, a novel post-training compression method that uses seeds of a pseudo-random generator to encode and... | David Harrison, Houman Bedayat, Jeffrey Marker, Mahyar Najibi, Maxwell Horton, Mohammad Rastegari, Rasoul Shafipour, Sachin Mehta, Saman Naderiparizi |  |
| 834 |  |  [Dissecting Adversarial Robustness of Multimodal LM Agents](https://openreview.net/forum?id=YauQYh2k1g) |  | 0 | As language models (LMs) are used to build autonomous agents in real environments, ensuring their adversarial robustness becomes a critical challenge. Unlike chatbots, agents are compound systems with multiple components taking actions, which existing LMs safety evaluations do not adequately... | Aditi Raghunathan, Chen Henry Wu, Daniel Fried, Jing Yu Koh, Rishi Rajesh Shah, Russ Salakhutdinov |  |
| 835 |  |  [ACC-Collab: An Actor-Critic Approach to Multi-Agent LLM Collaboration](https://openreview.net/forum?id=nfKfAzkiez) |  | 0 | Large language models (LLMs) have demonstrated a remarkable ability to serve as general-purpose tools for various language-based tasks. Recent works have demonstrated that the efficacy of such models can be improved through iterative dialog between multiple models. While these paradigms show... | Andrew Estornell, JeanFrancois Ton, Yang Liu, Yuanshun Yao |  |
| 836 |  |  [Greener GRASS: Enhancing GNNs with Encoding, Rewiring, and Attention](https://openreview.net/forum?id=rEQqBZIz49) |  | 0 | Graph Neural Networks (GNNs) have become important tools for machine learning on graph-structured data. In this paper, we explore the synergistic combination of graph encoding, graph rewiring, and graph attention, by introducing Graph Attention with Stochastic Structures (GRASS), a novel GNN... | Barnabás Póczos, Tongzhou Liao |  |
| 837 |  |  [NeSyC: A Neuro-symbolic Continual Learner For Complex Embodied Tasks in Open Domains](https://openreview.net/forum?id=VoayJihXra) |  | 0 | We explore neuro-symbolic approaches to generalize actionable knowledge, enabling embodied agents to tackle complex tasks more effectively in open-domain environments. A key challenge for embodied agents is the generalization of knowledge across diverse environments and situations, as limited... | Daehee Lee, Honguk Woo, Jinwoo Park, Sanghyun Ahn, Wonje Choi |  |
| 838 |  |  [Conformal Language Model Reasoning with Coherent Factuality](https://openreview.net/forum?id=AJpUZd8Clb) |  | 0 | Language models are increasingly being used in important decision pipelines, so ensuring the correctness of their outputs is crucial. Recent work has proposed evaluating the “factuality” of claims decomposed from a language model generation and applying conformal prediction techniques to filter out... | Aaron Roth, Keshav Ramji, Maxon RubinToles, Maya Gambhir, Surbhi Goel |  |
| 839 |  |  [Constraint-Conditioned Actor-Critic for Offline Safe Reinforcement Learning](https://openreview.net/forum?id=nrRkAAAufl) |  | 0 | Offline safe reinforcement learning (OSRL) aims to learn policies with high rewards while satisfying safety constraints solely from data collected offline. However, the learned policies often struggle to handle states and actions that are not present or out-of-distribution (OOD) from the offline... | Shengao Wang, Weichao Zhou, Wenchao Li, Zijian Guo |  |
| 840 |  |  [Specialized Foundation Models Struggle to Beat Supervised Baselines](https://openreview.net/forum?id=JYTQ6ELUVO) |  | 0 | Following its success for vision and text, the "foundation model" (FM) paradigm&#151;pretraining large models on massive data, then fine-tuning on target tasks&#151;has rapidly expanded to domains in the sciences, engineering, healthcare, and beyond. Has this achieved what the original FMs... | Alexander Shen, Ameet Talwalkar, Junhong Shen, Mikhail Khodak, Ritvik Gupta, Wenduo Cheng, Zongzhe Xu |  |
| 841 |  |  [Efficient Biological Data Acquisition through Inference Set Design](https://openreview.net/forum?id=gVkX9QMBO3) |  | 0 | In drug discovery, highly automated high-throughput laboratories are used to screen a large number of compounds in search of effective drugs. These experiments are expensive, so one might hope to reduce their cost by only experimenting on a subset of the compounds, and predicting the outcomes of... | Emmanuel Bengio, Ihor Neporozhnii, Jason S. Hartford, Julien Roy |  |
| 842 |  |  [Uncertainty Herding: One Active Learning Method for All Label Budgets](https://openreview.net/forum?id=UgPoHhYQ2U) |  | 0 | Most active learning research has focused on methods which perform well when many labels are available, but can be dramatically worse than random selection when label budgets are small. Other methods have focused on the low-budget regime, but do poorly as label budgets increase. As the line between... | Danica J. Sutherland, Gabriel L. Oliveira, Wonho Bae |  |
| 843 |  |  [In-context Time Series Predictor](https://openreview.net/forum?id=dCcY2pyNIO) |  | 0 | Recent Transformer-based large language models (LLMs) demonstrate in-context learning ability to perform various functions based solely on the provided context, without updating model parameters. To fully utilize the in-context capabilities in time series forecasting (TSF) problems, unlike previous... | Jiecheng Lu, Shihao Yang, Yan Sun |  |
| 844 |  |  [More Experts Than Galaxies: Conditionally-Overlapping Experts with Biologically-Inspired Fixed Routing](https://openreview.net/forum?id=1qq1QJKM5q) |  | 0 | The evolution of biological neural systems has led to both modularity and sparse coding, which enables energy efficiency and robustness across the diversity of tasks in the lifespan. In contrast, standard neural networks rely on dense, non-specialized architectures, where all model parameters are... | Francisco Pereira, Katharina von der Wense, Lawrence Hunter, Matt Jones, Sagi Shaier |  |
| 845 |  |  [Can Watermarks be Used to Detect LLM IP Infringement For Free?](https://openreview.net/forum?id=KRMSH1GxUK) |  | 0 | The powerful capabilities of LLMs stem from their rich training data and high-quality labeled datasets, making the training of strong LLMs a resource-intensive process, which elevates the importance of IP protection for such LLMs. Compared to gathering high-quality labeled data, directly sampling... | Bo Li, Chaowei Xiao, Patrick McDaniel, Somesh Jha, Xiaogeng Liu, Zhengyue Zhao |  |
| 846 |  |  [Discovering Influential Neuron Path in Vision Transformers](https://openreview.net/forum?id=WQQyJbr5Lh) |  | 0 | Vision Transformer models exhibit immense power yet remain opaque to human understanding, posing challenges and risks for practical applications. While prior research has attempted to demystify these models through input attribution and neuron role analysis, there's been a notable gap in... | Anqi Pang, Changming Li, Jingyi Yu, Kan Ren, Sibei Yang, Yifan Wang, Yifei Liu, Yingdong Shi |  |
| 847 |  |  [When narrower is better: the narrow width limit of Bayesian parallel branching neural networks](https://openreview.net/forum?id=CkUHtnyhpY) |  | 0 | The infinite width limit of random neural networks is known to result in Neural Networks as Gaussian Process (NNGP) (Lee et al. (2018)), characterized by task-independent kernels. It is widely accepted that larger network widths contribute to improved generalization (Park et al. (2019)). However,... | Haim Sompolinsky, Zechen Zhang |  |
| 848 |  |  [Directional Gradient Projection for Robust Fine-Tuning of Foundation Models](https://openreview.net/forum?id=goBaGHLAdP) |  | 0 | Robust fine-tuning aims to adapt large foundation models to downstream tasks while preserving their robustness to distribution shifts. Existing methods primarily focus on constraining and projecting current model towards the pre-trained initialization based on the magnitudes between fine-tuned and... | Brisa Maneechotesuwan, Chengyue Huang, Junjiao Tian, Shivang Chopra, Zsolt Kira |  |
| 849 |  |  [Reward Learning from Multiple Feedback Types](https://openreview.net/forum?id=9Ieq8jQNAl) |  | 0 | Learning rewards from preference feedback has become an important tool in the alignment of agentic models. Preference-based feedback, often implemented as a binary comparison between multiple completions, is an established method to acquire large-scale human feedback. However, human feedback in... | András Geiszl, Mennatallah ElAssady, Raphaël Baur, Yannick Metz |  |
| 850 |  |  [Token-Supervised Value Models for Enhancing Mathematical Problem-Solving Capabilities of Large Language Models](https://openreview.net/forum?id=6HcnC3pPkp) |  | 0 | With the rapid advancement of test-time compute search strategies to improve the mathematical problem-solving capabilities of large language models (LLMs), the need for building robust verifiers has become increasingly important. However, all these inference strategies rely on existing verifiers... | Byeongho Heo, Dongyoon Han, Eunho Yang, June Yong Yang, Jung Hyun Lee, Kang Min Yoo, Kyungsu Kim |  |
| 851 |  |  [ImProver: Agent-Based Automated Proof Optimization](https://openreview.net/forum?id=dWsdJAXjQD) |  | 0 | Large language models (LLMs) have been used to generate formal proofs of mathematical theorems in proofs assistants such as Lean. However, we often want to optimize a formal proof with respect to various criteria, depending on its downstream use. For example, we may want a proof to adhere to a... | Jeremy Avigad, Prasad Tetali, Riyaz Ahuja, Sean Welleck |  |
| 852 |  |  [Learning-Guided Rolling Horizon Optimization for Long-Horizon Flexible Job-Shop Scheduling](https://openreview.net/forum?id=Aly68Y5Es0) |  | 0 | Long-horizon combinatorial optimization problems (COPs), such as the Flexible Job-Shop Scheduling Problem (FJSP), often involve complex, interdependent decisions over extended time frames, posing significant challenges for existing solvers. While Rolling Horizon Optimization (RHO) addresses this by... | Cathy Wu, Sirui Li, Wenbin Ouyang, Yining Ma |  |
| 853 |  |  [Humanizing the Machine: Proxy Attacks to Mislead LLM Detectors](https://openreview.net/forum?id=PIpGN5Ko3v) |  | 0 | The advent of large language models (LLMs) has revolutionized the field of text generation, producing outputs that closely mimic human-like writing. Although academic and industrial institutions have developed detectors to prevent the malicious usage of LLM-generated texts, other research has doubt... | Haifeng Chen, Tianchun Wang, Wei Cheng, Xiang Zhang, Yuanzhou Chen, Zhanwen Chen, Zichuan Liu |  |
| 854 |  |  [Probe Pruning: Accelerating LLMs through Dynamic Pruning via Model-Probing](https://openreview.net/forum?id=WOt1owGfuN) |  | 0 | We introduce Probe Pruning (PP), a novel framework for online, dynamic, structured pruning of Large Language Models (LLMs) applied in a batch-wise manner. PP leverages the insight that not all samples and tokens contribute equally to the model's output, and probing a small portion of each batch... | Ali Anwar, Enmao Diao, Jie Ding, Li Yang, Qi Le, Xinran Wang, Ziyan Wang |  |
| 855 |  |  [AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents](https://openreview.net/forum?id=oWdzUpOlkX) |  | 0 | Autonomy via agents based on large language models (LLMs) that can carry out personalized yet standardized tasks presents a significant opportunity to drive human efficiency. There is an emerging need and interest in automating web tasks (e.g., booking a hotel for a given date within a budget).... | George Karypis, Huzefa Rangwala, Ke Yang, Pratik Chaudhari, Rasool Fakoor, Sapana Chaudhary, Yao Liu |  |
| 856 |  |  [Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF](https://openreview.net/forum?id=SQnitDuow6) |  | 0 | Reinforcement learning from human feedback (RLHF) has demonstrated great promise in aligning large language models (LLMs) with human preference. Depending on the availability of preference data, both online and offline RLHF are active areas of investigation. A key bottleneck is understanding how to... | Bo Dai, Dale Schuurmans, Hanjun Dai, Jincheng Mei, Katayoon Goshvadi, Sherry Yang, Shicong Cen, Tong Yang, Yuejie Chi |  |
| 857 |  |  [Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration](https://openreview.net/forum?id=FviefuxmeW) |  | 0 | Imitation learning is a central problem in reinforcement learning where the goal is to learn a policy that mimics the expert's behavior. In practice, it is often challenging to learn the expert policy from a limited number of demonstrations accurately due to the complexity of the state space.... | David Mark Bossens, Heyang Zhao, Ivor W. Tsang, Quanquan Gu, Xingrui Yu |  |
| 858 |  |  [ORSO: Accelerating Reward Design via Online Reward Selection and Policy Optimization](https://openreview.net/forum?id=0uRc3CfJIQ) |  | 0 | Reward shaping is critical in reinforcement learning (RL), particularly for complex tasks where sparse rewards can hinder learning. However, choosing effective shaping rewards from a set of reward functions in a computationally efficient manner remains an open challenge. We propose Online Reward... | Aldo Pacchiano, Chen Bo Calvin Zhang, Pulkit Agrawal, ZhangWei Hong |  |
| 859 |  |  [Risk-Sensitive Variational Actor-Critic: A Model-Based Approach](https://openreview.net/forum?id=irrtPRFksw) |  | 0 | Risk-sensitive reinforcement learning (RL) with an entropic risk measure typically requires knowledge of the transition kernel or performs unstable updates w.r.t. exponential Bellman equations. As a consequence, algorithms that optimize this objective have been restricted to tabular or... | Alonso Granados Baca, Jason Pacheco, Reza Ebrahimi |  |
| 860 |  |  [L3Ms - Lagrange Large Language Models](https://openreview.net/forum?id=ULGbw2URE3) |  | 0 | Supervised fine-tuning (SFT) and alignment of large language models (LLMs) are key steps in providing a good user experience. However, the concept of an appropriate alignment is inherently application-dependent, and current methods often rely on heuristic choices to drive optimization. In this... | Alex Smola, Guneet S. Dhillon, Xingjian Shi, Yee Whye Teh |  |
| 861 |  |  [Locality Alignment Improves Vision-Language Models](https://openreview.net/forum?id=qssVptHTPN) |  | 0 | Vision language models (VLMs) have seen growing adoption in recent years, but many still struggle with basic spatial reasoning errors. We hypothesize that this is due to VLMs adopting pre-trained vision backbones, specifically vision transformers (ViTs) trained with image-level supervision and... | Ian Connick Covert, James Zou, Tatsunori Hashimoto, Tony Sun |  |
| 862 |  |  [Benign Overfitting in Out-of-Distribution Generalization of Linear Models](https://openreview.net/forum?id=6jxUsDAdAu) |  | 0 | Benign overfitting refers to the phenomenon where an over-parameterized model fits the training data perfectly, including noise in the data, but still generalizes well to the unseen test data. While prior work provides some theoretical understanding of this phenomenon under the in-distribution... | Chi Jin, Jianqing Fan, Jiayun Wu, Shange Tang |  |
| 863 |  |  [ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models](https://openreview.net/forum?id=goFpCuJalN) |  | 0 | The use of Large Language Models (LLMs) in climate science has recently gained significant attention. However, a critical issue remains: the lack of a comprehensive evaluation framework capable of assessing the quality and scientific validity of model outputs. To address this issue, we develop... | Duncan WatsonParris, Leon Bergen, Rose Yu, Spencer Ho, Srikar Eranky, Taylor BergKirkpatrick, Veeramakali Vignesh Manivannan, Yasaman Jafari, Yian Ma |  |
| 864 |  |  [Lossy Compression with Pretrained Diffusion Models](https://openreview.net/forum?id=raUnLe0Z04) |  | 0 | We apply Theis et al. (2022)'s DiffC algorithm to Stable Diffusion 1.5, 2.1, XL, and and Flux-dev, and demonstrate that these pretrained models are remarkably capable lossy image compressors. A principled algorithm for compression using pretrained diffusion models has been understood since at least... | Feng Liu, Jeremy Vonderfecht |  |
| 865 |  |  [Human-inspired Episodic Memory for Infinite Context LLMs](https://openreview.net/forum?id=BI2int5SAC) |  | 0 | Large language models (LLMs) have shown remarkable capabilities, but still struggle with processing extensive contexts, limiting their ability to maintain coherence and accuracy over long sequences. In contrast, the human brain excels at organising and retrieving episodic experiences across vast... | Adnan Oomerjee, Fenia Christopoulou, Gerasimos Lampouras, Haitham BouAmmar, Jun Wang, Martin Benfeghoul, Zafeirios Fountas |  |
| 866 |  |  [Global Convergence of Policy Gradient in Average Reward MDPs](https://openreview.net/forum?id=2PRpcmJecX) |  | 0 | We present the first comprehensive finite-time global convergence analysis of policy gradient for infinite horizon average reward Markov decision processes (MDPs). Specifically, we focus on ergodic tabular MDPs with finite state and action spaces. Our analysis shows that the policy gradient... | Itai Shufaro, Kfir Yehuda Levy, Navdeep Kumar, R. Srikant, Shie Mannor, Yashaswini Murthy |  |
| 867 |  |  [What Matters in Learning from Large-Scale Datasets for Robot Manipulation](https://openreview.net/forum?id=LqhorpRLIm) |  | 0 | Imitation learning from large multi-task demonstration datasets has emerged as a promising path for building generally-capable robots. As a result, 1000s of hours have been spent on building such large-scale datasets around the globe. Despite the continuous growth of such efforts, we still lack a... | Ajay Mandlekar, Danfei Xu, Kuancheng Wang, Matthew Bronars, Nadun Ranawaka Arachchige, Soroush Nasiriany, Vaibhav Saxena, WooChul Shin |  |
| 868 |  |  [Rapidly Adapting Policies to the Real-World via Simulation-Guided Fine-Tuning](https://openreview.net/forum?id=XwUrzurG94) |  | 0 | Robot learning requires a considerable amount of high-quality data to realize the promise of generalization. However, large data sets are costly to collect in the real world. Physics simulators can cheaply generate vast data sets with broad coverage over states, actions, and environments. However,... | Abhishek Gupta, Andrey Kolobov, ChingAn Cheng, Patrick Yin, Tyler Westenbroek |  |
| 869 |  |  [Empowering Users in Digital Privacy Management through Interactive LLM-Based Agents](https://openreview.net/forum?id=FEpAUnS7f7) |  | 0 | This paper presents a novel application of large language models (LLMs) to enhance user comprehension of privacy policies through an interactive dialogue agent. We demonstrate that LLMs significantly outperform traditional models in tasks like Data Practice Identification, Choice Identification,... | Bolun Sun, Haiyun Jiang, Yifan Zhou |  |
| 870 |  |  [Object-Centric Pretraining via Target Encoder Bootstrapping](https://openreview.net/forum?id=7d2JwGbxhA) |  | 0 | Object-centric representation learning has recently been successfully applied to real-world datasets. This success can be attributed to pretrained non-object-centric foundation models, whose features serve as reconstruction targets for slot attention. However, targets must remain frozen throughout... | Nikola Dukic, Tim Lebailly, Tinne Tuytelaars |  |
| 871 |  |  [Modality-Specialized Synergizers for Interleaved Vision-Language Generalists](https://openreview.net/forum?id=7UgQjFEadn) |  | 0 | Recent advancements in Vision-Language Models (VLMs) have led to the emergence of Vision-Language Generalists (VLGs) capable of understanding and generating both text and images. However, seamlessly generating an arbitrary sequence of text and images remains a challenging task for the current VLGs.... | Jiaxin Zhang, Joy Rimchala, Lifu Huang, Minqian Liu, Qifan Wang, Ying Shen, Yu Cheng, Zhiyang Xu |  |
| 872 |  |  [Self-Normalized Resets for Plasticity in Continual Learning](https://openreview.net/forum?id=G82uQztzxl) |  | 0 | Plasticity Loss is an increasingly important phenomenon that refers to the empirical observation that as a neural network is continually trained on a sequence of changing tasks, its ability to adapt to a new task diminishes over time. We introduce Self-Normalized Resets (SNR), a simple adaptive... | Adam Daniel Jozefiak, Vivek F. Farias |  |
| 873 |  |  [Homomorphism Counts as Structural Encodings for Graph Learning](https://openreview.net/forum?id=qFw2RFJS5g) |  | 0 | Graph Transformers are popular neural networks that extend the well-known Transformer architecture to the graph domain. These architectures operate by applying self-attention on graph nodes and incorporating graph structure through the use of positional encodings (e.g., Laplacian positional... | Emily Jin, Ismail Ilkan Ceylan, Linus Bao, Matthias Lanzinger, Michael M. Bronstein |  |
| 874 |  |  [Real2Code: Reconstruct Articulated Objects via Code Generation](https://openreview.net/forum?id=CAssIgPN4I) |  | 0 | We present Real2Code, a novel approach to reconstructing articulated objects via code generation. Given visual observations of an object, we first reconstruct its part geometry using image segmentation and shape completion. We represent these object parts with oriented bounding boxes, from which a... | Dominik Bauer, Shuran Song, Yijia Weng, Zhao Mandi |  |
| 875 |  |  [An Asynchronous Bundle Method for Distributed Learning Problems](https://openreview.net/forum?id=Kwo20MWWCb) |  | 0 | We propose a novel asynchronous bundle method to solve distributed learning problems. Compared to existing asynchronous methods, our algorithm computes the next iterate based on a more accurate approximation of the objective function and does not require any prior information about the maximal... | Daniel Cederberg, Mikael Johansson, Stephen P. Boyd, Xuyang Wu |  |
| 876 |  |  [On Linear Representations and Pretraining Data Frequency in Language Models](https://openreview.net/forum?id=EDoD3DgivF) |  | 0 | Pretraining data has a direct impact on the behaviors and quality of language models (LMs), but we only understand the most basic principles of this relationship. While most work focuses on pretraining data's effect on downstream task behavior, we investigate its relationship to LM representations.... | Jack Merullo, Noah A. Smith, Sarah Wiegreffe, Yanai Elazar |  |
| 877 |  |  [CoRNStack: High-Quality Contrastive Data for Better Code Retrieval and Reranking](https://openreview.net/forum?id=iyJOUELYir) |  | 0 | Effective code retrieval plays a crucial role in advancing code generation, bug fixing, and software maintenance, particularly as software systems increase in complexity. While current code embedding models have demonstrated promise in retrieving code snippets for small-scale, well-defined tasks,... | Andriy Mulyar, Brandon Duderstadt, Heng Ji, Revanth Gangi Reddy, Tarun Suresh, Yifei Xu, Zach Nussbaum |  |
| 878 |  |  [BingoGuard: LLM Content Moderation Tools with Risk Levels](https://openreview.net/forum?id=HPSAkIHRbb) |  | 0 | Malicious content generated by large language models (LLMs) can pose varying degrees of harm. Although existing LLM-based moderators can detect harmful content, they struggle to assess risk levels and may miss lower-risk outputs. Accurate risk assessment allows platforms with different safety... | Caiming Xiong, ChienSheng Wu, Divyansh Agarwal, Fan Yin, Linnea Ross, Philippe Laban, Vaibhav Vats, Xiangyu Peng, Yilun Zhou, Yixin Mao |  |
| 879 |  |  [Learn hybrid prototypes for multivariate time series anomaly detection](https://openreview.net/forum?id=8TBGdH3t6a) |  | 0 | In multivariate time series anomaly detection (MTSAD), reconstruction-based models reconstruct testing series with learned knowledge of only normal series and identify anomalies with higher reconstruction errors. In practice, over-generalization often occurs with unexpectedly well reconstruction of... | KeYuan Shen |  |
| 880 |  |  [Reasoning with Latent Thoughts: On the Power of Looped Transformers](https://openreview.net/forum?id=din0lGfZFd) |  | 0 | Large language models have shown remarkable reasoning abilities and scaling laws suggest that large parameter count, especially along the depth axis, is the primary driver. In this work, we make a stronger claim --- many reasoning problems require a large depth but not necessarily many parameters.... | Nikunj Saunshi, Nishanth Dikkala, Sanjiv Kumar, Sashank J. Reddi, Zhiyuan Li |  |
| 881 |  |  [Conservative Contextual Bandits: Beyond Linear Representations](https://openreview.net/forum?id=SThJXvucjQ) |  | 0 | Conservative Contextual Bandits (CCBs) address safety in sequential decision making by requiring that an agent's policy, along with minimizing regret, also satisfies a safety constraint: the performance is not worse than a baseline policy (e.g., the policy that the company has in production) by... | Arindam Banerjee, Mohammad Ghavamzadeh, Rohan Deb |  |
| 882 |  |  [Quantifying Generalization Complexity for Large Language Models](https://openreview.net/forum?id=jpSLXoRKnH) |  | 0 | While large language models (LLMs) have shown exceptional capabilities in understanding complex queries and performing sophisticated tasks, their generalization abilities are often deeply entangled with memorization, necessitating more precise evaluation. To address this challenge, we introduce... | Himabindu Lakkaraju, Hongyin Luo, James R. Glass, Xiangjun Fan, Xuliang Huang, Yibo Jiang, Zhenting Qi, Zhuokai Zhao |  |
| 883 |  |  [SqueezeAttention: 2D Management of KV-Cache in LLM Inference via Layer-wise Optimal Budget](https://openreview.net/forum?id=9HK2rHNAhd) |  | 0 | Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has been considered critical to saving the cost of inference. Most of the existing KV-cache compression algorithms attempted to sparsify the sequence of tokens by taking advantage of the different importance of tokens. However,... | Bin Cui, Shaoduo Gan, Zihao Wang |  |
| 884 |  |  [Gaussian Mixture Counterfactual Generator](https://openreview.net/forum?id=lBB3eSn6fY) |  | 0 | We address the individualized treatment effect (ITE) estimation problem, focusing on continuous, multidimensional, and time-dependent treatments for precision medicine. The central challenge lies in modeling these complex treatment scenarios while capturing dynamic patient responses and minimizing... | Akshay Vashist, JongHoon Ahn |  |
| 885 |  |  [Multi-domain Distribution Learning for De Novo Drug Design](https://openreview.net/forum?id=g3VCIM94ke) |  | 0 | We introduce DrugFlow, a generative model for structure-based drug design that integrates continuous flow matching with discrete Markov bridges, demonstrating state-of-the-art performance in learning chemical, geometric, and physical aspects of three-dimensional protein-ligand data. We endow... | Adrian W. Dobbelstein, Arne Schneuing, Bruno Correia, Ilia Igashov, Michael M. Bronstein, Thomas Castiglione |  |
| 886 |  |  [{τ}-bench: A Benchmark for \underline{T}ool-\underline{A}gent-\underline{U}ser Interaction in Real-World Domains](https://openreview.net/forum?id=roNSXZpUDN) |  | 0 | Existing benchmarks for language agents do not set them up to interact with human users or follow domain-specific rules, both of which are vital to safe and realistic deployment. We propose $\tau$-bench, a benchmark with two domains (retail and airline) emulating dynamic conversations between a... | Karthik R. Narasimhan, Noah Shinn, Pedram Razavi, Shunyu Yao |  |
| 887 |  |  [DEPfold: RNA Secondary Structure Prediction as Dependency Parsing](https://openreview.net/forum?id=DpLFmc09pC) |  | 0 | RNA secondary structure prediction is critical for understanding RNA function but remains challenging due to complex structural elements like pseudoknots and limited training data. We introduce DEPfold, a novel deep learning approach that re-frames RNA secondary structure prediction as a dependency... | Ke Wang, Shay B. Cohen |  |
| 888 |  |  [APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding](https://openreview.net/forum?id=yUC8pU508S) |  | 0 | Context-augmented generation (CAG) techniques, including RAG and ICL, require the efficient combination of multiple contexts to generate responses to user queries. Directly inputting these contexts as a sequence introduces a considerable computational burden by re-encoding the combined selection of... | Beidi Chen, Tianqi Chen, Xinyu Yang |  |
| 889 |  |  [SFS: Smarter Code Space Search improves LLM Inference Scaling](https://openreview.net/forum?id=MCHuGOkExF) |  | 0 | We frame code generation as a black-box optimization problem within the code space and demonstrate how optimization-inspired techniques can enhance inference scaling over text. Based on this perspective, we propose \*\*SCATTERED FOREST SEARCH (SFS)\*\*, a novel approach that improves solution... | Haifeng Chen, Jonathan Light, Wei Cheng, Wenchao Yu, Xujiang Zhao, Yanchi Liu, Yiyou Sun, Yue Wu, Ziniu Hu |  |
| 890 |  |  [Aioli: A Unified Optimization Framework for Language Model Data Mixing](https://openreview.net/forum?id=sZGZJhaNSe) |  | 0 | Language model performance depends on identifying the optimal mixture of data groups to train on (e.g., law, code, math). Prior work has proposed a diverse set of methods to efficiently learn mixture proportions, ranging from fitting regression models over training runs to dynamically updating... | Christopher Ré, Kyunghyun Cho, Mayee F. Chen, Michael Y. Hu, Nicholas Lourie |  |
| 891 |  |  [Differentiable Optimization of Similarity Scores Between Models and Brains](https://openreview.net/forum?id=vWRwdmA3wU) |  | 0 | How do we know if two systems - biological or artificial - process information in a similar way? Similarity measures such as linear regression, Centered Kernel Alignment (CKA), Normalized Bures Similarity (NBS), and angular Procrustes distance, are often used to quantify this similarity. However,... | Christopher J. Cueva, Earl K. Miller, Guangyu Robert Yang, Markus Siegel, Moufan Li, Nathan Cloos, Scott L. Brincat |  |
| 892 |  |  [Provable Convergence Bounds for Hybrid Dynamical Sampling and Optimization](https://openreview.net/forum?id=FJv8VMPxWi) |  | 0 | Analog dynamical accelerators (DXs) are a growing sub-field in computer architecture research, offering order-of-magnitude gains in power efficiency and latency over traditional digital methods in several machine learning, optimization, and sampling tasks. However, limited-capacity accelerators... | Matthew X. Burns, Michael C. Huang, Qingyuan Hou |  |
| 893 |  |  [CTSyn: A Foundation Model for Cross Tabular Data Generation](https://openreview.net/forum?id=Sh4FOyZRpv) |  | 0 | Generative Foundation Models (GFMs) have achieved remarkable success in producing high-quality synthetic data for images and text. However, their application to tabular data presents significant challenges due to the heterogeneous nature of table features. Current cross-table learning frameworks... | Chenheng Xu, Guang Cheng, Matthew Yang, Xiaofeng Lin |  |
| 894 |  |  [Solving hidden monotone variational inequalities with surrogate losses](https://openreview.net/forum?id=4ZX2a3OKEV) |  | 0 | Deep learning has proven to be effective in a wide variety of loss minimization problems. However, many applications of interest, like minimizing projected Bellman error and min-max optimization, cannot be modelled as minimizing a scalar loss function but instead correspond to solving a variational... | Danilo Vucetic, Gauthier Gidel, Ioannis Mitliagkas, Junhyung Lyle Kim, Ryan D'Orazio, Zichu Liu |  |
| 895 |  |  [TorchTitan: One-stop PyTorch native solution for production ready LLM pretraining](https://openreview.net/forum?id=SFN6Wm7YBI) |  | 0 | The development of large language models (LLMs) has been instrumental in advancing state-of-the-art natural language processing applications. Training LLMs with billions of parameters and trillions of tokens requires sophisticated distributed systems that enable composing and comparing several... | Andrew Gu, ChienChin Huang, Gokul Nadathur, Howard Huang, Iris Zhang, Junjie Wang, Less Wright, Sanket Purandare, Stratos Idreos, Tianyu Liu, Wanchao Liang, Wei Feng, Will Constable |  |
| 896 |  |  [Chunk-Distilled Language Modeling](https://openreview.net/forum?id=nrvoWOWcyg) |  | 0 | We introduce Chunk-Distilled Language Modeling (CD-LM), an approach to text generation that addresses two challenges in current large language models (LLMs): the inefficiency of token-level generation, and the difficulty of adapting to new data and knowledge. Our method combines deep network-based... | Jiawei Zhou, Karen Livescu, Yanhong Li |  |
| 897 |  |  [When does compositional structure yield compositional generalization? A kernel theory](https://openreview.net/forum?id=FPBce2P1er) |  | 0 | Compositional generalization (the ability to respond correctly to novel combinations of familiar components) is thought to be a cornerstone of intelligent behavior. Compositionally structured (e.g. disentangled) representations support this ability; however, the conditions under which they are... | Kim Stachenfeld, Samuel Lippl |  |
| 898 |  |  [BadJudge: Backdoor Vulnerabilities of LLM-As-A-Judge](https://openreview.net/forum?id=eC2a2IndIt) |  | 0 | This paper proposes a novel backdoor threat attacking the LLM-as-a-Judge evaluation regime, where the adversary controls both the candidate and evaluator model. The backdoored evaluator victimizes benign users by unfairly assigning inflated scores to adversary. A trivial single token backdoor... | Fei Wang, Muhao Chen, Terry Tong, Zhe Zhao |  |
| 899 |  |  [The Value of Sensory Information to a Robot](https://openreview.net/forum?id=ikr5XomWHS) |  | 0 | A decision-making agent, such as a robot, must observe and react to any new task-relevant information that becomes available from its environment. We seek to study a fundamental scientific question: what value does sensory information hold to an agent at various moments in time during the execution... | Arjun Krishna, Dinesh Jayaraman, Edward S. Hu |  |
| 900 |  |  [Sensor-Invariant Tactile Representation](https://openreview.net/forum?id=RnJY9WcpA3) |  | 0 | High-resolution tactile sensors have become critical for embodied perception and robotic manipulation. However, a key challenge in the field is the lack of transferability between sensors due to design and manufacturing variations, which result in significant differences in tactile signals. This... | Harsh Gupta, Shengmiao Jin, Wenzhen Yuan, Yuchen Mo |  |
| 901 |  |  [NutriBench: A Dataset for Evaluating Large Language Models in Nutrition Estimation from Meal Descriptions](https://openreview.net/forum?id=6LtdZCyuZR) |  | 0 | Accurate nutrition estimation helps people make informed dietary choices and is essential in the prevention of serious health complications. We present NutriBench, the first publicly available natural language meal description nutrition benchmark. NutriBench consists of 11,857 meal descriptions... | Andong Hua, Laya Pullela, Mehak Preet Dhaliwal, Ryan Burke, Yao Qin |  |
| 902 |  |  [GOttack: Universal Adversarial Attacks on Graph Neural Networks via Graph Orbits Learning](https://openreview.net/forum?id=YbURbViE7l) |  | 0 | Graph Neural Networks (GNNs) have demonstrated superior performance in node classification tasks across diverse applications. However, their vulnerability to adversarial attacks, where minor perturbations can mislead model predictions, poses significant challenges. This study introduces GOttack, a... | Cuneyt Gurcan Akcora, Murat Kantarcioglu, Tran Gia Bao Ngo, Zulfikar Alom |  |
| 903 |  |  [NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics](https://openreview.net/forum?id=hJVdwBpWjt) |  | 0 | Large language models (LLMs) prompted with text and audio have achieved state-of-the-art performance across various auditory tasks, including speech, music, and general audio, showing emergent abilities on unseen tasks. However, their potential has yet to be fully demonstrated in bioacoustics... | David Robinson, Marius Miron, Masato Hagiwara, Olivier Pietquin |  |
| 904 |  |  [Privacy-Preserving Personalized Federated Prompt Learning for Multimodal Large Language Models](https://openreview.net/forum?id=Equ277PBN0) |  | 0 | Multimodal Large Language Models (LLMs) are pivotal in revolutionizing customer support and operations by integrating multiple modalities such as text, images, and audio. Federated Prompt Learning (FPL) is a recently proposed approach that combines pre-trained multimodal LLMs such as... | Ana L. Milanova, Linh Tran, Stacy Patterson, Wei Sun |  |
| 905 |  |  [Optimizing 4D Gaussians for Dynamic Scene Video from Single Landscape Images](https://openreview.net/forum?id=IcYDRzcccP) |  | 0 | To achieve realistic immersion in landscape images, fluids such as water and clouds need to move within the image while revealing new scenes from various camera perspectives. Recently, a field called dynamic scene video has emerged, which combines single image animation with 3D photography. These... | Haesoo Choo, InHwan Jin, Junghwan Kim, Kyeongbo Kong, Ohjoon Kwon, Park Heemoon, SeongHun Jeong |  |
| 906 |  |  [Explore Theory of Mind: program-guided adversarial data generation for theory of mind reasoning](https://openreview.net/forum?id=246rHKUnnf) |  | 0 | Do large language models (LLMs) have theory of mind? A plethora of papers and benchmarks have been introduced to evaluate if current models have been able to develop this key ability of social intelligence. However, all rely on limited datasets with simple patterns that can potentially lead to... | Asli Celikyilmaz, Jane DwivediYu, Maryam FazelZarandi, Melanie Sclar, Yejin Choi, Yonatan Bisk, Yulia Tsvetkov |  |
| 907 |  |  [Controllable Context Sensitivity and the Knob Behind It](https://openreview.net/forum?id=Igm9bbkzHC) |  | 0 | When making predictions, a language model must trade off how much it relies on its context vs. its prior knowledge. Choosing how sensitive the model is to its context is a fundamental functionality, as it enables the model to excel at tasks like retrieval-augmented generation and... | Chris Wendler, Giovanni Monea, Julian Minder, Kevin Du, Niklas Stoehr, Robert West, Ryan Cotterell |  |
| 908 |  |  [The 3D-PC: a benchmark for visual perspective taking in humans and machines](https://openreview.net/forum?id=UIFAJZ22ZF) |  | 0 | Visual perspective taking (VPT) is the ability to perceive and reason about the perspectives of others. It is an essential feature of human intelligence, which develops over the first decade of life and requires an ability to process the 3D structure of visual scenes. A growing number of reports... | Akash Nagaraj, Alekh Karkada Ashok, Drew Linsley, Francis E. Lewis, Gaurav Gaonkar, Peisen Zhou, Thomas Serre, Zygmunt Pizlo |  |
| 909 |  |  [HelpSteer2-Preference: Complementing Ratings with Preferences](https://openreview.net/forum?id=MnfHxPP5gs) |  | 0 | Reward models are critical for aligning models to follow instructions, and are typically trained following one of two popular paradigms: Bradley-Terry style or Regression style. However, there is a lack of evidence that either approach is better than the other, when adequately matched for data.... | Alexander Bukharin, Daniel Egert, Gerald Shen, Jiaqi Zeng, Oleksii Kuchaiev, Olivier Delalleau, Yi Dong, Zhilin Wang |  |
| 910 |  |  [Shared-AE: Automatic Identification of Shared Subspaces in High-dimensional Neural and Behavioral Activity](https://openreview.net/forum?id=zXCnIyX9MG) |  | 0 | Understanding the relationship between behavior and neural activity is crucial for understanding brain function. An effective method is to learn embeddings for interconnected modalities. For simple behavioral tasks, neural features can be learned based on labels. However, complex behaviors, such as... | Anne Churchland, Daiyao Yi, Hao Dong, Michael James Higley, Shreya Saxena |  |
| 911 |  |  [Bounds on Lp Errors in Density Ratio Estimation via f-Divergence Loss Functions](https://openreview.net/forum?id=ttq44QjKda) |  | 0 | Density ratio estimation (DRE) is a core technique in machine learning used to capture relationships between two probability distributions. $f$-divergence loss functions, which are derived from variational representations of $f$-divergence, have become a standard choice in DRE for achieving... | Yoshiaki Kitazawa |  |
| 912 |  |  [Palu: KV-Cache Compression with Low-Rank Projection](https://openreview.net/forum?id=LWMS4pk2vK) |  | 0 | Post-training KV-Cache compression methods typically either sample a subset of effectual tokens or quantize the data into lower numerical bit width. However, these methods cannot exploit redundancy in the hidden dimension of the KV tenors. This paper presents a hidden dimension compression approach... | ChiChih Chang, ChienYu Lin, ChongYan Chen, KaiChiang Wu, Luis Ceze, Mohamed S. Abdelfattah, NingChi Huang, PeiShuo Wang, WeiCheng Lin, YuFang Hu |  |
| 913 |  |  [Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling](https://openreview.net/forum?id=bIlnpVM4bc) |  | 0 | Efficiently modeling sequences with infinite context length has long been a challenging problem. Previous approaches have either suffered from quadratic computational complexity or limited extrapolation ability in length generalization. In this work, we present Samba, a simple hybrid architecture... | Chen Liang, Liliang Ren, Weizhu Chen, Yadong Lu, Yang Liu, Yelong Shen |  |
| 914 |  |  [Nova: Generative Language Models for Assembly Code with Hierarchical Attention and Contrastive Learning](https://openreview.net/forum?id=4ytRL3HJrq) |  | 0 | Binary code analysis is the foundation of crucial tasks in the security domain; thus building effective binary analysis techniques is more important than ever. Large language models (LLMs) although have brought impressive improvement to source code tasks, do not directly generalize to assembly code... | Chengxiao Wang, Kevin Liu, Lin Tan, Nan Jiang, Petr Babkin, Xiangyu Zhang, Xiangzhe Xu |  |
| 915 |  |  [Efficient Model-Based Reinforcement Learning Through Optimistic Thompson Sampling](https://openreview.net/forum?id=Ian00SaFHg) |  | 0 | Learning complex robot behavior through interactions with the environment necessitates principled exploration. Effective strategies should prioritize exploring regions of the state-action space that maximize rewards, with optimistic exploration emerging as a promising direction aligned with this... | Amanda Prorok, Carl Henrik Ek, Jasmine Bayrooti |  |
| 916 |  |  [Latent Safety-Constrained Policy Approach for Safe Offline Reinforcement Learning](https://openreview.net/forum?id=bDt5qc7TfO) |  | 0 | In safe offline reinforcement learning, the objective is to develop a policy that maximizes cumulative rewards while strictly adhering to safety constraints, utilizing only offline data. Traditional methods often face difficulties in balancing these constraints, leading to either diminished... | Cody H. Fleming, Prajwal Koirala, Soumik Sarkar, Zhanhong Jiang |  |
| 917 |  |  [Large (Vision) Language Models are Unsupervised In-Context Learners](https://openreview.net/forum?id=ohJxgRLlLt) |  | 0 | Recent advances in large language and vision-language models have enabled zero-shot inference, allowing models to solve new tasks without task-specific training. Various adaptation techniques such as prompt engineering, In-Context Learning (ICL), and supervised fine-tuning can further enhance the... | Amir Zamir, Andrei Atanov, Artyom Gadetsky, Ghazal Hosseini Mighan, Maria Brbic, Yulun Jiang, Zhitong Gao |  |
| 918 |  |  [LLMs Can Plan Only If We Tell Them](https://openreview.net/forum?id=K3KrOsR6y9) |  | 0 | Large language models (LLMs) have demonstrated significant capabilities in natural language processing and reasoning, yet their effectiveness in autonomous planning has been under debate. While existing studies have utilized LLMs with external feedback mechanisms or in controlled environments for... | Bilgehan Sel, Ming Jin, Ruoxi Jia |  |
| 919 |  |  [Feature Responsiveness Scores: Model-Agnostic Explanations for Recourse](https://openreview.net/forum?id=wsWCVrH9dv) |  | 0 | Machine learning models routinely automate decisions in applications like lending and hiring. In such settings, consumer protection rules require companies that deploy models to explain predictions to decision subjects. These rules are motivated, in part, by the belief that explanations can promote... | Anneke Wernerfelt, Berk Ustun, Seung Hyun Cheon, Sorelle A. Friedler |  |
| 920 |  |  [InterMask: 3D Human Interaction Generation via Collaborative Masked Modeling](https://openreview.net/forum?id=ZAyuwJYN8N) |  | 0 | Generating realistic 3D human-human interactions from textual descriptions remains a challenging task. Existing approaches, typically based on diffusion models, often produce results lacking realism and fidelity. In this work, we introduce \*InterMask\*, a novel framework for generating human... | Chuan Guo, Li Cheng, Muhammad Gohar Javed, Xingyu Li |  |
| 921 |  |  [Denoising Autoregressive Transformers for Scalable Text-to-Image Generation](https://openreview.net/forum?id=amDkNPVWcn) |  | 0 | Diffusion models have become the dominant approach for visual generation. They are trained by denoising a Markovian process which gradually adds noise to the input. We argue that the Markovian property limits the model’s ability to fully utilize the generation trajectory, leading to inefficiencies... | Dinghuai Zhang, Jiatao Gu, Joshua M. Susskind, Navdeep Jaitly, Qihang Zhang, Shuangfei Zhai, Yizhe Zhang, Yuyang Wang |  |
| 922 |  |  [Many-Objective Multi-Solution Transport](https://openreview.net/forum?id=Neb17mimVH) |  | 0 | Optimizing the performance of many objectives (instantiated by tasks or clients) jointly with a few Pareto stationary solutions (models) is critical in machine learning. However, previous multi-objective optimization methods often focus on a few objectives and cannot scale to many objectives that... | Jeff Bilmes, Tian Li, Tianyi Zhou, Virginia Smith, Ziyue Li |  |
| 923 |  |  [An Auditing Test to Detect Behavioral Shift in Language Models](https://openreview.net/forum?id=h0jdAboh0o) |  | 0 | As language models (LMs) approach human-level performance, a comprehensive understanding of their behavior becomes crucial. This includes evaluating capabilities, biases, task performance, and alignment with societal values. Extensive initial evaluations, including red teaming and diverse... | Leo Richter, Matt J. Kusner, Pasquale Minervini, Xuanli He |  |
| 924 |  |  [Radar: Fast Long-Context Decoding for Any Transformer](https://openreview.net/forum?id=ZTpWOwMrzQ) |  | 0 | Transformer models have demonstrated exceptional performance across a wide range of applications. Though forming the foundation of Transformer models, the dot-product attention does not scale well to long-context data since its time requirement grows quadratically with context length. In this work,... | Frederick Tung, Hossein Hajimirsadeghi, Mengyao Zhai, Sepidehsadat Hosseini, Yongchang Hao |  |
| 925 |  |  [Can Knowledge Editing Really Correct Hallucinations?](https://openreview.net/forum?id=hmDt068MoZ) |  | 0 | Large Language Models (LLMs) suffer from hallucinations, referring to the non-factual information in generated content, despite their superior capacities across tasks. Meanwhile, knowledge editing has been developed as a new popular paradigm to correct erroneous factual knowledge encoded in LLMs... | Ali Payani, Baixiang Huang, Canyu Chen, Kai Shu, Xiongxiao Xu |  |
| 926 |  |  [SysCaps: Language Interfaces for Simulation Surrogates of Complex Systems](https://openreview.net/forum?id=1R5BcYS8EC) |  | 0 | Surrogate models are used to predict the behavior of complex energy systems that are too expensive to simulate with traditional numerical methods. Our work introduces the use of language descriptions, which we call "system captions" or SysCaps, to interface with such surrogates. We argue that... | Patrick Emami, Saumya Sinha, Truc Nguyen, Zhaonan Li |  |
| 927 |  |  [Generative Classifiers Avoid Shortcut Solutions](https://openreview.net/forum?id=oCUYc7BzXQ) |  | 0 | Discriminative approaches to classification often learn shortcuts that hold in-distribution but fail even under minor distribution shift. This failure mode stems from an overreliance on features that are spuriously correlated with the label. We show that generative classifiers, which use... | Alexander Cong Li, Ananya Kumar, Deepak Pathak |  |
| 928 |  |  [Compute-Constrained Data Selection](https://openreview.net/forum?id=4es2oO9tw1) |  | 0 | Data selection can reduce the amount of training data needed to finetune LLMs; however, the efficacy of data selection scales directly with its compute. Motivated by the practical challenge of compute-constrained finetuning, we consider the setting in which both the cost of selecting data and... | Alexander M. Rush, Junjie Oscar Yin |  |
| 929 |  |  [Generalization through variance: how noise shapes inductive biases in diffusion models](https://openreview.net/forum?id=7lUdo8Vuqa) |  | 0 | How diffusion models generalize beyond their training set is not known, and is somewhat mysterious given two facts: the optimum of the denoising score matching (DSM) objective usually used to train diffusion models is the score function of the training distribution; and the networks usually used to... | John J. Vastola |  |
| 930 |  |  [Learning to Explore and Exploit with GNNs for Unsupervised Combinatorial Optimization](https://openreview.net/forum?id=vaJ4FObpXN) |  | 0 | Combinatorial optimization (CO) problems are pervasive across various domains, but their NP-hard nature often necessitates problem-specific heuristic algorithms. Recent advancements in deep learning have led to the development of learning-based heuristics, yet these approaches often struggle with... | Aaron M. Ferber, Carla P. Gomes, Utku Umur Acikalin |  |
| 931 |  |  [Fitting Networks with a Cancellation Trick](https://openreview.net/forum?id=C06kww3Qky) |  | 0 | The degree-corrected block model (DCBM), latent space model (LSM), and $\beta$-model are all popular network models. We combine their modeling ideas and propose the logit-DCBM as a new model. Similar as the $\beta$-model and LSM, the logit-DCBM contains nonlinear factors, where fitting the... | Jiashun Jin, Jingming Wang |  |
| 932 |  |  [Do LLM Agents Have Regret? A Case Study in Online Learning and Games](https://openreview.net/forum?id=qn9tBYQHGi) |  | 0 | Large language models (LLMs) have been increasingly employed for (interactive) decision-making, via the development of LLM-based autonomous agents. Despite their emerging successes, the performance of LLM agents in decision-making has not been fully investigated through quantitative metrics,... | Asuman E. Ozdaglar, Chanwoo Park, Kaiqing Zhang, Xiangyu Liu |  |
| 933 |  |  [Seq-VCR: Preventing Collapse in Intermediate Transformer Representations for Enhanced Reasoning](https://openreview.net/forum?id=30oIfmrcFO) |  | 0 | Decoder-only Transformers often struggle with complex reasoning tasks, particularly arithmetic reasoning requiring multiple sequential operations. In this work, we identify representation collapse in the model’s intermediate layers as a key factor limiting their reasoning capabilities. To address... | Christopher Pal, Gopeshh Subbaraj, Irina Rish, Md Rifat Arefin, Nicolas Gontier, Ravid ShwartzZiv, Yann LeCun |  |
| 934 |  |  [Underdamped Diffusion Bridges with Applications to Sampling](https://openreview.net/forum?id=Q1QTxFm0Is) |  | 0 | We provide a general framework for learning diffusion bridges that transport prior to target distributions. It includes existing diffusion models for generative modeling, but also underdamped versions with degenerate diffusion matrices, where the noise only acts in certain dimensions. Extending... | Denis Blessing, Gerhard Neumann, Julius Berner, Lorenz Richter |  |
| 935 |  |  [BoneMet: An Open Large-Scale Multi-Modal Murine Dataset for Breast Cancer Bone Metastasis Diagnosis and Prognosis](https://openreview.net/forum?id=YH4M1Tbxfz) |  | 0 | Breast cancer bone metastasis (BCBM) affects women’s health globally, calling for the development of effective diagnosis and prognosis solutions. While deep learning has exhibited impressive capacities across various healthcare domains, its applicability in BCBM diseases is consistently hindered by... | Fudong Lin, Jason Jiang, Liyun Wang, Shubo Wang, Tiankuo Chu, Wiley JiaWei Gong, Xu Yuan |  |
| 936 |  |  [Time After Time: Deep-Q Effect Estimation for Interventions on When and What to do](https://openreview.net/forum?id=5yDS32hKJc) |  | 0 | Problems in fields such as healthcare, robotics, and finance requires reasoning about the value both of what decision or action to take and when to take it. The prevailing hope is that artificial intelligence will support such decisions by estimating the causal effect of policies such as how to... | Mark Goldstein, Rajesh Ranganath, Wouter A. C. van Amsterdam, Yoav Wald, Yonathan Efroni |  |
| 937 |  |  [GRAIN: Exact Graph Reconstruction from Gradients](https://openreview.net/forum?id=7bAjVh3CG3) |  | 0 | Federated learning claims to enable collaborative model training among multiple clients with data privacy by transmitting gradient updates instead of the actual client data. However, recent studies have shown the client privacy is still at risk due to the, so called, gradient inversion attacks... | Dimitar Iliev Dimitrov, Ivo Petrov, Maria Drencheva, Martin T. Vechev, Maximilian Baader |  |
| 938 |  |  [ALLaM: Large Language Models for Arabic and English](https://openreview.net/forum?id=MscdsFVZrN) |  | 0 | In this work, we present ALLaM: Arabic Large Language Model, a series of large language models to support the ecosystem of Arabic Language Technologies (ALT). ALLaM is carefully trained, considering the values of language alignment and transferability of knowledge at scale. The models are based on... | Abdulmohsen AlThubaity, Ali Alammari, Faisal Abdulrahman Mirza, Ghadah Alabduljabbar, Hassan A. Alahmed, Hisham Abdullah Alyahya, M. Saiful Bari, Majed Alrubaian, Maryam Al Mansour, Norah A. Alzahrani, Nouf M. Alotaibi, Raghad Alkhathran, Raneem Alnajim, Saad Amin Hassan, Salman Alsubaihi, Shaykhah Z. Alsubaie, Sultan Alrashed, Yazeed Alnumay, Yousef Almushayqih, Zaki Alawami, et al. |  |
| 939 |  |  [Execution-guided within-prompt search for programming-by-example](https://openreview.net/forum?id=PY56Wur7S0) |  | 0 | Large language models (LLMs) can generate code from examples without being limited to a DSL, but they lack search, as sampled programs are independent. In this paper, we use an LLM as a policy that generates lines of code and then join these lines of code to let the LLM implicitly estimate the... | Ashish Tiwari, Gust Verbruggen, Mukul Singh, Sumit Gulwani, Vu Le |  |
| 940 |  |  [Lie Algebra Canonicalization: Equivariant Neural Operators under arbitrary Lie Groups](https://openreview.net/forum?id=7PLpiVdnUC) |  | 0 | The quest for robust and generalizable machine learning models has driven recent interest in exploiting symmetries through equivariant neural networks. In the context of PDE solvers, recent works have shown that Lie point symmetries can be a useful inductive bias for Physics-Informed Neural... | CarolaBibiane Schönlieb, Ferdia Sherry, James Rowbottom, Melanie Weber, Peter Zaika, Zakhar Shumaylov |  |
| 941 |  |  [Extendable and Iterative Structure Learning Strategy for Bayesian Networks](https://openreview.net/forum?id=3n6DYH3cIP) |  | 0 | Learning the structure of Bayesian networks is a fundamental yet computationally intensive task, especially as the number of variables grows. Traditional algorithms require retraining from scratch when new variables are introduced, making them impractical for dynamic or large-scale applications. In... | Hamid Kalantari, Pouria Ramazi, Russell Greiner |  |
| 942 |  |  [Improving Graph Neural Networks by Learning Continuous Edge Directions](https://openreview.net/forum?id=iAmR7FfMmq) |  | 0 | Graph Neural Networks (GNNs) traditionally employ a message-passing mechanism that resembles diffusion over undirected graphs, which often leads to homogenization of node features and reduced discriminative power in tasks such as node classification. Our key insight for addressing this limitation... | Sahand Hormoz, Seong Ho Pahng |  |
| 943 |  |  [Accelerating Task Generalisation with Multi-Level Skill Hierarchies](https://openreview.net/forum?id=KfeRfxTemB) |  | 0 | Developing reinforcement learning agents that can generalise effectively to new tasks is one of the main challenges in AI research. This paper introduces Fracture Cluster Options (FraCOs), a multi-level hierarchical reinforcement learning method designed to improve generalisation performance.... | Thomas P. Cannon, Özgür Simsek |  |
| 944 |  |  [SiReRAG: Indexing Similar and Related Information for Multihop Reasoning](https://openreview.net/forum?id=yp95goUAT1) |  | 0 | Indexing is an important step towards strong performance in retrieval-augmented generation (RAG) systems. However, existing methods organize data based on either semantic similarity (similarity) or related information (relatedness), but do not cover both perspectives comprehensively. Our analysis... | Alexander R. Fabbri, Caiming Xiong, ChienSheng Wu, Gabriel BernadettShapiro, Nan Zhang, Prafulla Kumar Choubey, Prasenjit Mitra, Rui Zhang |  |
| 945 |  |  [Make Haste Slowly: A Theory of Emergent Structured Mixed Selectivity in Feature Learning ReLU Networks](https://openreview.net/forum?id=27SSnLl85x) |  | 0 | In spite of finite dimension ReLU neural networks being a consistent factor behind recent deep learning successes, a theory of feature learning in these models remains elusive. Currently, insightful theories still rely on assumptions including the linearity of the network computations, unstructured... | Andrew M. Saxe, Benjamin Rosman, Devon Jarvis, Richard Klein |  |
| 946 |  |  [Eliciting Human Preferences with Language Models](https://openreview.net/forum?id=LvDwwAgMEW) |  | 0 | Language models (LMs) can be directed to perform user- and context-dependent tasks by using labeled examples or natural language prompts. But selecting examples or writing prompts can be challenging---especially in tasks that require users to precisely articulate nebulous preferences or reason... | Alex Tamkin, Belinda Z. Li, Jacob Andreas, Noah D. Goodman |  |
| 947 |  |  [SymDiff: Equivariant Diffusion via Stochastic Symmetrisation](https://openreview.net/forum?id=i1NNCrRxdM) |  | 0 | We propose SymDiff, a method for constructing equivariant diffusion models using the framework of stochastic symmetrisation. SymDiff resembles a learned data augmentation that is deployed at sampling time, and is lightweight, computationally efficient, and easy to implement on top of arbitrary... | Kianoosh Ashouritaklimi, Leo Zhang, Rob Cornish, Yee Whye Teh |  |
| 948 |  |  [AdaWM: Adaptive World Model based Planning for Autonomous Driving](https://openreview.net/forum?id=NEu8wgPctU) |  | 0 | World model based reinforcement learning (RL) has emerged as a promising approach for autonomous driving, which learns a latent dynamics model and uses it to train a planning policy. To speed up the learning process, the pretrain-finetune paradigm is often used, where online RL is initialized by a... | Abhirup Mallik, Burhaneddin Yaman, Chenbin Pan, Feng Tao, Hang Wang, Junshan Zhang, Liu Ren, Xin Ye |  |
| 949 |  |  [A Meta-Learning Approach to Bayesian Causal Discovery](https://openreview.net/forum?id=eeJz7eDWKO) |  | 0 | Discovering a unique causal structure is difficult due to both inherent identifiability issues, and the consequences of finite data. As such, uncertainty over causal structures, such as those obtained from a Bayesian posterior, are often necessary for downstream tasks. Finding an accurate... | Anish Dhir, James Requeima, Mark van der Wilk, Matthew Ashman |  |
| 950 |  |  [Is Your Video Language Model a Reliable Judge?](https://openreview.net/forum?id=m8yby1JfbU) |  | 0 | As video language models (VLMs) gain more applications in various scenarios, the need for robust and scalable evaluation of their performance becomes increasingly critical. The traditional human expert-based evaluation of VLMs has limitations in consistency and scalability, which sparked interest... | Ming Liu, Wensheng Zhang |  |
| 951 |  |  [Boosting Multiple Views for pretrained-based Continual Learning](https://openreview.net/forum?id=AZR4R3lw7y) |  | 0 | Recent research has shown that Random Projection (RP) can effectively improve the performance of pre-trained models in Continual learning (CL). The authors hypothesized that using RP to map features onto a higher-dimensional space can make them more linearly separable. In this work, we... | Dinh Q. Phung, Khanh Doan, Khoat Than, Quyen Tran, Toan Tran, Trung Le, Tung Lam Tran |  |
| 952 |  |  [Matrix Product Sketching via Coordinated Sampling](https://openreview.net/forum?id=eHfq8Q3LeD) |  | 0 | We revisit the well-studied problem of approximating a matrix product, $\bv{A}^T\bv{B}$, based on small space sketches $\mathcal{S}(\bv{A})$ and $\mathcal{S}(\bv{B})$ of $\bv{A} \in \R^{n \times d}$ and $\bv{B}\in \R^{n \times m}$. We are interested in the setting where the sketches must be... | Christopher Musco, Danrong Li, Juliana Freire, Majid Daliri |  |
| 953 |  |  [DUET: Decentralized Bilevel Optimization without Lower-Level Strong Convexity](https://openreview.net/forum?id=jxMAPMqNr5) |  | 0 | Decentralized bilevel optimization (DBO) provides a powerful framework for multi-agent systems to solve local bilevel tasks in a decentralized fashion without the need for a central server. However, most existing DBO methods rely on lower-level strong convexity (LLSC) to guarantee unique solutions... | Jia Liu, Songtao Lu, Yingbin Liang, Zhen Qin, Zhuqing Liu |  |
| 954 |  |  [Preserving Deep Representations in One-Shot Pruning: A Hessian-Free Second-Order Optimization Framework](https://openreview.net/forum?id=eNQp79A5Oz) |  | 0 | We present SNOWS, a one-shot post-training pruning framework aimed at reducing the cost of vision network inference without retraining. Current leading one-shot pruning methods minimize layer-wise least squares reconstruction error which does not take into account deeper network representations. We... | Rahul Mazumder, Ryan Lucas |  |
| 955 |  |  [Chemistry-Inspired Diffusion with Non-Differentiable Guidance](https://openreview.net/forum?id=4dAgG8ma3B) |  | 0 | Recent advances in diffusion models have shown remarkable potential in the conditional generation of novel molecules. These models can be guided in two ways: (i) explicitly, through additional features representing the condition, or (ii) implicitly, using a property predictor. However, training... | Barnabás Póczos, Chenghui Zhou, Chenhao Zhang, Newell Washburn, Sijie Fu, Yuchen Shen |  |
| 956 |  |  [Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning](https://openreview.net/forum?id=nDmwloEl3N) |  | 0 | Diffusion Policies have become widely used in Imitation Learning, offering several appealing properties, such as generating multimodal and discontinuous behavior. As models are becoming larger to capture more complex capabilities, their computational demands increase, as shown by recent scaling... | Jyothish Pari, Moritz Reuss, Pulkit Agrawal, Rudolf Lioutikov |  |
| 957 |  |  [No Location Left Behind: Measuring and Improving the Fairness of Implicit Representations for Earth Data](https://openreview.net/forum?id=hSZaCIznB2) |  | 0 | Implicit neural representations (INRs) exhibit growing promise in addressing Earth representation challenges, ranging from emissions monitoring to climate modeling. However, existing methods disproportionately prioritize global average performance, whereas practitioners require fine-grained... | Daniel Cai, Randall Balestriero |  |
| 958 |  |  [InvestESG: A multi-agent reinforcement learning benchmark for studying climate investment as a social dilemma](https://openreview.net/forum?id=2TasVD7FXp) |  | 0 | \*\*InvestESG\*\* is a novel multi-agent reinforcement learning (MARL) benchmark designed to study the impact of Environmental, Social, and Governance (ESG) disclosure mandates on corporate climate investments. The benchmark models an intertemporal social dilemma where companies balance short-term... | Jiayi Yuan, Joel Z. Leibo, Natasha Jaques, Xiaoxuan Hou |  |
| 959 |  |  [Discrete GCBF Proximal Policy Optimization for Multi-agent Safe Optimal Control](https://openreview.net/forum?id=1X1R7P6yzt) |  | 0 | Control policies that can achieve high task performance and satisfy safety constraints are desirable for any system, including multi-agent systems (MAS). One promising technique for ensuring the safety of MAS is distributed control barrier functions (CBF). However, it is difficult to design... | Chuchu Fan, Mitchell Black, Oswin So, Songyuan Zhang |  |
| 960 |  |  [Aligning Language Models with Demonstrated Feedback](https://openreview.net/forum?id=1qGkuxI9UX) |  | 0 | Language models are aligned to emulate the collective voice of many, resulting in outputs that align with no one in particular. Steering LLMs away from generic output is possible through supervised finetuning or RLHF, but requires prohibitively large datasets for new ad-hoc tasks. We argue that it... | Diyi Yang, Hyundong Justin Cho, Joey Hejna, Michael S. Bernstein, Michelle S. Lam, Omar Shaikh, Yijia Shao |  |
| 961 |  |  [See It from My Perspective: How Language Affects Cultural Bias in Image Understanding](https://openreview.net/forum?id=Xbl6t6zxZs) |  | 0 | Vision-language models (VLMs) can respond to queries about images in many languages. However, beyond language, culture affects how we see things. For example, individuals from Western cultures focus more on the central figure in an image while individuals from East Asian cultures attend more to... | Amith Ananthram, Elias StengelEskin, Kathleen McKeown, Mohit Bansal |  |
| 962 |  |  [Do Mice Grok? Glimpses of Hidden Progress in Sensory Cortex](https://openreview.net/forum?id=oYemKnlIrO) |  | 0 | Does learning of task-relevant representations stop when behavior stops changing? Motivated by recent work in machine learning and the intuitive observation that human experts continue to learn after mastery, we hypothesize that task-specific representation learning in cortex can continue, even... | Blake Bordelon, Cengiz Pehlevan, Samuel J. Gershman, Tanishq Kumar, Venkatesh N. Murthy |  |
| 963 |  |  [SymmCD: Symmetry-Preserving Crystal Generation with Diffusion Models](https://openreview.net/forum?id=xnssGv9rpW) |  | 0 | Generating novel crystalline materials has potential to lead to advancements in fields such as electronics, energy storage, and catalysis. The defining characteristic of crystals is their symmetry, which plays a central role in determining their physical properties. However, existing crystal... | Daniel Levy, Kin Long Kelvin Lee, Mikhail Galkin, Qiang Zhu, Santiago Miret, Siamak Ravanbakhsh, Siba Smarak Panigrahi, SékouOumar Kaba |  |
| 964 |  |  [Benchmarking LLMs' Judgments with No Gold Standard](https://openreview.net/forum?id=uE84MGbKD7) |  | 0 | We introduce the GEM (Generative Estimator for Mutual Information), an evaluation metric for assessing language generation by large language models (LLMs), particularly in generating informative judgments, without the need for a gold standard reference. GEM broadens the scenarios where we can... | Grant Schoenebeck, Shengwei Xu, Yuqing Kong, Yuxuan Lu |  |
| 965 |  |  [The Pitfalls of Memorization: When Memorization Hurts Generalization](https://openreview.net/forum?id=vVhZh9ZpIM) |  | 0 | Neural networks often learn simple explanations that fit the majority of the data while memorizing exceptions that deviate from these explanations. This behavior leads to poor generalization when the learned explanations rely on spurious correlations. In this work, we formalize $\textit{the... | David LopezPaz, Elvis Dohmatob, Mohammad Pezeshki, Pascal Vincent, Reza Bayat |  |
| 966 |  |  [Graph Neural Networks Gone Hogwild](https://openreview.net/forum?id=WfxPVtYRlL) |  | 0 | Graph neural networks (GNNs) appear to be powerful tools to learn state representations for agents in distributed, decentralized multi-agent systems, but generate catastrophically incorrect predictions when nodes update asynchronously during inference. This failure under asynchrony effectively... | Deniz Oktay, Nick Richardson, Olga Solodova, Ryan P. Adams |  |
| 967 |  |  [Benchmarking Vision Language Model Unlearning via Fictitious Facial Identity Dataset](https://openreview.net/forum?id=0y3hGn1wOk) |  | 0 | Machine unlearning has emerged as an effective strategy for forgetting specific information in the training data. However, with the increasing integration of visual data, privacy concerns in Vision Language Models (VLMs) remain underexplored. To address this, we introduce Facial Identity Unlearning... | Bo Li, Chaowei Xiao, Fei Wang, Furong Huang, Jiazhao Li, Jinsheng Pan, Jiongxiao Wang, Lichao Sun, Muhao Chen, Siyuan Ma, Xiujun Li, Yejin Choi, Yingzi Ma |  |
| 968 |  |  [A Deep Generative Learning Approach for Two-stage Adaptive Robust Optimization](https://openreview.net/forum?id=CKXul9iX77) |  | 0 | Two-stage adaptive robust optimization (ARO) is a powerful approach for planning under uncertainty, balancing first-stage decisions with recourse decisions made after uncertainty is realized. To account for uncertainty, modelers typically define a simple uncertainty set over which potential... | Aron Brenner, Jennifer Z. Sun, Rahman Khorramfar, Saurabh Amin |  |
| 969 |  |  [Fundamental Limitations on Subquadratic Alternatives to Transformers](https://openreview.net/forum?id=T2d0geb6y0) |  | 0 | The Transformer architecture is widely deployed in many popular and impactful Large Language Models. At its core is the attention mechanism for calculating correlations between pairs of tokens. Performing an attention computation takes quadratic time in the input size, and had become the time... | Hantao Yu, Josh Alman |  |
| 970 |  |  [Cocoon: Robust Multi-Modal Perception with Uncertainty-Aware Sensor Fusion](https://openreview.net/forum?id=DKgAFfCs5F) |  | 0 | An important paradigm in 3D object detection is the use of multiple modalities to enhance accuracy in both normal and challenging conditions, particularly for long-tail scenarios. To address this, recent studies have explored two directions of adaptive approaches: MoE-based adaptive fusion, which... | Heng Yang, Jeong Joon Park, Jiachen Sun, Marco Pavone, Minkyoung Cho, Qingzhao Zhang, Yulong Cao, Zhuoqing Mao |  |
| 971 |  |  [Decision Tree Induction Through LLMs via Semantically-Aware Evolution](https://openreview.net/forum?id=UyhRtB4hjN) |  | 0 | Decision trees are a crucial class of models offering robust predictive performance and inherent interpretability across various domains, including healthcare, finance, and logistics. However, current tree induction methods often face limitations such as suboptimal solutions from greedy methods or... | Mihaela van der Schaar, Nicolas Huynh, Tennison Liu |  |
| 972 |  |  [Efficient Imitation under Misspecification](https://openreview.net/forum?id=fn36V5qsCw) |  | 0 | We consider the problem of imitation learning under misspecification: settings where the learner is fundamentally unable to replicate expert behavior everywhere. This is often true in practice due to differences in observation space and action space expressiveness (e.g. perceptual or morphological... | Gokul Swamy, Nicolas A. Espinosa Dice, Sanjiban Choudhury, Wen Sun |  |
| 973 |  |  [SV-RAG: LoRA-Contextualizing Adaptation of MLLMs for Long Document Understanding](https://openreview.net/forum?id=FDaHjwInXO) |  | 0 | Multimodal large language models (MLLMs) have recently shown great progress in text-rich image understanding, yet they still struggle with complex, multi-page visually-rich documents. Traditional methods using document parsers for retrieval-augmented generation suffer from performance and... | Changyou Chen, Franck Dernoncourt, Jian Chen, Jiuxiang Gu, Ruiyi Zhang, Ryan A. Rossi, Tong Sun, Tong Yu, Yufan Zhou |  |
| 974 |  |  [Intelligence at the Edge of Chaos](https://openreview.net/forum?id=IeRcpsdY7P) |  | 0 | We explore the emergence of intelligent behavior in artificial systems by investigating how the complexity of rule-based systems influences the capabilities of models trained to predict these rules. Our study focuses on elementary cellular automata (ECA), simple yet powerful one-dimensional systems... | Aakash Patel, Amin Karbasi, David van Dijk, Emanuele Zappala, Nianchen Liu, Shiyang Zhang, Sizhuang He, Syed Asad Rizvi |  |
| 975 |  |  [Towards a learning theory of representation alignment](https://openreview.net/forum?id=DShqJA1Z64) |  | 0 | It has recently been argued that AI models' representations are becoming aligned as their scale and performance increase. Empirical analyses have been designed to support this idea and conjecture the possible alignment of different representations toward a shared statistical model of reality. In... | Francesco Insulla, Lorenzo Rosasco, Shuo Huang |  |
| 976 |  |  [A Unified Framework for Forward and Inverse Problems in Subsurface Imaging using Latent Space Translations](https://openreview.net/forum?id=yIlyHJdYV3) |  | 0 | In subsurface imaging, learning the mapping from velocity maps to seismic waveforms (forward problem) and waveforms to velocity (inverse problem) is important for several applications. While traditional techniques for solving forward and inverse problems are computationally prohibitive, there is a... | Anuj Karpatne, Arka Daw, Medha Sawhney, Naveen Gupta, Youzuo Lin |  |
| 977 |  |  [Cauchy-Schwarz Regularizers](https://openreview.net/forum?id=KZu3xhPhke) |  | 0 | We introduce a novel class of regularization functions, called Cauchy–Schwarz (CS) regularizers, which can be designed to induce a wide range of properties in solution vectors of optimization problems. To demonstrate the versatility of CS regularizers, we derive regularization functions that... | Christoph Studer, Sueda Taner, Ziyi Wang |  |
| 978 |  |  [Adversarially Robust Anomaly Detection through Spurious Negative Pair Mitigation](https://openreview.net/forum?id=t8fu5m8R5m) |  | 0 | Despite significant progress in Anomaly Detection (AD), the robustness of existing detection methods against adversarial attacks remains a challenge, compromising their reliability in critical real-world applications such as autonomous driving. This issue primarily arises from the AD setup, which... | Hossein Mirzaei, Jafar Habibi, Mohammad Hossein Rohban, Mohammad Sabokrou, Mojtaba Nafez |  |
| 979 |  |  [Holographic Node Representations: Pre-training Task-Agnostic Node Embeddings](https://openreview.net/forum?id=tGYFikNONB) |  | 0 | Large general purpose pre-trained models have revolutionized computer vision and natural language understanding. However, the development of general purpose pre-trained Graph Neural Networks (GNNs) lags behind other domains due to the lack of suitable generalist node representations. Existing GNN... | Beatrice Bevilacqua, Bruno Ribeiro, Joshua Robinson, Jure Leskovec |  |
| 980 |  |  [The Ramanujan Library - Automated Discovery on the Hypergraph of Integer Relations](https://openreview.net/forum?id=EyaH1wzmao) |  | 0 | Fundamental mathematical constants appear in nearly every field of science, from physics to biology. Formulas that connect different constants often bring great insight by hinting at connections between previously disparate fields. Discoveries of such relations, however, have remained scarce... | Ido Kaminer, Itay Beit Halachmi |  |
| 981 |  |  [HELMET: How to Evaluate Long-context Models Effectively and Thoroughly](https://openreview.net/forum?id=293V3bJbmE) |  | 0 | Many benchmarks exist for evaluating long-context language models (LCLMs), yet developers often rely on synthetic tasks such as needle-in-a-haystack (NIAH) or an arbitrary subset of tasks. However, it remains unclear whether these benchmarks reflect the diverse downstream applications of LCLMs, and... | Daniel Fleischer, Danqi Chen, Howard Yen, Ke Ding, Minmin Hou, Moshe Wasserblat, Peter Izsak, Tianyu Gao |  |
| 982 |  |  [Uncovering Latent Memories in Large Language Models](https://openreview.net/forum?id=KSBx6FBZpE) |  | 0 | Frontier AI systems are making transformative impacts across society, but such benefits are not without costs: models trained on web-scale datasets containing personal and private data raise profound concerns about data privacy and security. Language models are trained on extensive corpora... | Abhiram Iyer, Ila R. Fiete, Mikail Khona, Rylan Schaeffer, Sunny Duan |  |
| 983 |  |  [Episodic Memories Generation and Evaluation Benchmark for Large Language Models](https://openreview.net/forum?id=6ycX677p2l) |  | 0 | Episodic memory -- the ability to recall specific events grounded in time and space -- is a cornerstone of human cognition, enabling not only coherent storytelling, but also planning and decision-making. Despite their remarkable capabilities, Large Language Models (LLMs) lack a robust mechanism for... | Alexis Huet, Dario Rossi, Zied BenHouidi |  |
| 984 |  |  [Continual Slow-and-Fast Adaptation of Latent Neural Dynamics (CoSFan): Meta-Learning What-How & When to Adapt](https://openreview.net/forum?id=Dl3MsjaIdp) |  | 0 | An increasing interest in learning to forecast for time-series of high-dimensional observations is the ability to adapt to systems with diverse underlying dynamics. Access to observations that define a stationary distribution of these systems is often unattainable, as the underlying dynamics may... | Linwei Wang, Ryan Missel |  |
| 985 |  |  [R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference](https://openreview.net/forum?id=9VMW4iXfKt) |  | 0 | Large Language Models (LLMs), while demonstrating remarkable capabilities across various applications, present significant challenges during inference due to their substantial model size, especially when deployed on edge devices. Activation sparsity offers a promising solution to reduce computation... | Harshit Khaitan, Steven Li, Yuandong Tian, Zechun Liu, Zhangyang Wang, Zhenyu Zhang |  |
| 986 |  |  [Query-based Knowledge Transfer for Heterogeneous Learning Environments](https://openreview.net/forum?id=XKv29sMyjF) |  | 0 | Decentralized collaborative learning under data heterogeneity and privacy constraints has rapidly advanced. However, existing solutions like federated learning, ensembles, and transfer learning, often fail to adequately serve the unique needs of clients, especially when local data representation is... | Ahmed M. Abdelmoniem, Marco Canini, Mohamed Elhoseiny, Norah Alballa, Wenxuan Zhang, Ziquan Liu |  |
| 987 |  |  [Language Models Are Implicitly Continuous](https://openreview.net/forum?id=SMK0f8JoKF) |  | 0 | Language is typically modelled with discrete sequences. However, the most successful approaches to language modelling, namely neural networks, are continuous and smooth function approximators. In this work, we show that Transformer-based language models implicitly learn to represent sentences as... | Davide Evangelista, Emanuele La Malfa, Michael J. Wooldridge, Michele Lombardi, Samuele Marro, X. Angelo Huang |  |
| 988 |  |  [Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape View](https://openreview.net/forum?id=m51BgoqvbP) |  | 0 | Training language models currently requires pre-determining a fixed compute budget because the typical cosine learning rate schedule depends on the total number of steps. In contrast, the Warmup-Stable-Decay (WSD) schedule uses a constant learning rate to produce a main branch of iterates that can... | David Leo Wright Hall, Jason S. Wang, Kaiyue Wen, Percy Liang, Tengyu Ma, Zhiyuan Li |  |
| 989 |  |  [ConceptPrune: Concept Editing in Diffusion Models via Skilled Neuron Pruning](https://openreview.net/forum?id=kSdWcw5mkp) |  | 0 | While large-scale text-to-image diffusion models have demonstrated impressive image-generation capabilities, there are significant concerns about their potential misuse for generating unsafe content, violating copyright, and perpetuating societal biases. Recently, the text-to-image generation... | Da Li, Ruchika Chavhan, Timothy M. Hospedales |  |
| 990 |  |  [Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning](https://openreview.net/forum?id=v4MTnPiYXY) |  | 0 | Value-based reinforcement learning (RL) can in principle learn effective policies for a wide range of multi-turn problems, from games to dialogue to robotic control, including via offline RL from static previously collected datasets. However, despite the widespread use of policy gradient methods to... | Anca D. Dragan, Joey Hong, Sergey Levine |  |
| 991 |  |  [Mm-Embed: Universal Multimodal Retrieval with Multimodal LLMS](https://openreview.net/forum?id=i45NQb2iKO) |  | 0 | State-of-the-art retrieval models typically address a straightforward search scenario, in which retrieval tasks are fixed (e.g., finding a passage to answer a specific question) and only a single modality is supported for both queries and retrieved results. This paper introduces techniques for... | Bryan Catanzaro, Chankyu Lee, Jimmy Lin, Mohammad Shoeybi, ShengChieh Lin, Wei Ping |  |
| 992 |  |  [RNNs are not Transformers (Yet): The Key Bottleneck on In-Context Retrieval](https://openreview.net/forum?id=h3wbI8Uk1Z) |  | 0 | This paper investigates the gap in representation powers of Transformers and Recurrent Neural Networks (RNNs), which are more memory efficient than Transformers. We aim to understand whether RNNs can match the performance of Transformers, particularly when enhanced with Chain-of-Thought (CoT)... | Kaifeng Lyu, Kaiyue Wen, Xingyu Dang |  |
| 993 |  |  [Regressing the Relative Future: Efficient Policy Optimization for Multi-turn RLHF](https://openreview.net/forum?id=cVyELMpMRS) |  | 0 | Large Language Models (LLMs) have achieved remarkable success at tasks like summarization that involve a single turn of interaction. However, they can still struggle with multi-turn tasks like dialogue that require long-term planning. Previous works on multi-turn dialogue extend single-turn... | Gokul Swamy, Jason D. Lee, Jonathan Daniel Chang, Kianté Brantley, Wen Sun, Wenhao Zhan, Zhaolin Gao |  |
| 994 |  |  [Diffusing States and Matching Scores: A New Framework for Imitation Learning](https://openreview.net/forum?id=kWRKNDU6uN) |  | 0 | Adversarial Imitation Learning is traditionally framed as a two-player zero-sum game between a learner and an adversarially chosen cost function, and can therefore be thought of as the sequential generalization of a Generative Adversarial Network (GAN). However, in recent years, diffusion models... | Gokul Swamy, Kianté Brantley, Runzhe Wu, Wen Sun, Yiding Chen |  |
| 995 |  |  [Behavioral Entropy-Guided Dataset Generation for Offline Reinforcement Learning](https://openreview.net/forum?id=LuT2CVrlpU) |  | 0 | Entropy-based objectives are widely used to perform state space exploration in reinforcement learning (RL) and dataset generation for offline RL. Behavioral entropy (BE), a rigorous generalization of classical entropies that incorporates cognitive and perceptual biases of agents, was recently... | Aamodh Suresh, Carlos NietoGranda, Wesley A. Suttle |  |
| 996 |  |  [Context-aware Dynamic Pruning for Speech Foundation Models](https://openreview.net/forum?id=u2QdCiOgwA) |  | 0 | Foundation models, such as large language models, have achieved remarkable success in natural language processing and are evolving into models capable of handling multiple modalities. Listening ability, in particular, is crucial for many applications, leading to research on building speech... | Athanasios Mouchtaris, Grant P. Strimel, Jing Liu, Markus Müller, Masao Someki, Shinji Watanabe, Siddhant Arora, Yifan Peng |  |
| 997 |  |  [Infinite-Resolution Integral Noise Warping for Diffusion Models](https://openreview.net/forum?id=Y6LPWBo2HP) |  | 0 | Adapting pretrained image-based diffusion models to generate temporally consistent videos has become an impactful generative modeling research direction. Training-free noise-space manipulation has proven to be an effective technique, where the challenge is to preserve the Gaussian white noise... | Dmitriy Smirnov, Lingxiao Li, Mohammad H. Taghavi, Ning Yu, Ryan D. Burgert, Vincent Dedun, Winnie Lin, Yitong Deng |  |
| 998 |  |  [Injective flows for star-like manifolds](https://openreview.net/forum?id=Jyh0DR4fFE) |  | 0 | Normalizing Flows (NFs) are powerful and efficient models for density estimation. When modeling densities on manifolds, NFs can be generalized to injective flows but the Jacobian determinant becomes computationally prohibitive. Current approaches either consider bounds on the log-likelihood or rely... | Jonathan Aellen, Marcello Massimo Negri, Volker Roth |  |
| 999 |  |  [Mixture of In-Context Prompters for Tabular PFNs](https://openreview.net/forum?id=2fojNANZSv) |  | 0 | Recent benchmarks find In-Context Learning (ICL) outperforms both deep learning and tree-based algorithms on small tabular datasets. However, on larger datasets, ICL for tabular learning suffers in both efficiency and effectiveness. In terms of efficiency, transformers incur linear space and... | Derek Qiang Xu, F. Olcay Cirit, Reza Asadi, Wei Wang, Yizhou Sun |  |
| 1000 |  |  [Policy Design in Long-run Welfare Dynamics](https://openreview.net/forum?id=d8hYXbxX71) |  | 0 | Improving social welfare is a complex challenge requiring policymakers to optimize objectives across multiple time horizons. Evaluating the impact of such policies presents a fundamental challenge, as those that appear suboptimal in the short run may yield significant long-term benefits. We tackle... | AnaAndreea Stoica, Jiduan Wu, Moritz Hardt, Rediet Abebe |  |
| 1001 |  |  [Dynamic Modeling of Patients, Modalities and Tasks via Multi-modal Multi-task Mixture of Experts](https://openreview.net/forum?id=NJxCpMt0sf) |  | 0 | Multi-modal multi-task learning holds significant promise in tackling complex diagnostic tasks and many significant medical imaging problems. It fulfills the needs in real-world diagnosis protocol to leverage information from different data sources and simultaneously perform mutually informative... | Chenwei Wu, Liyue Shen, Luning Wang, Zhengxu Tang, Zitao Shuai |  |
| 1002 |  |  [Procedural Synthesis of Synthesizable Molecules](https://openreview.net/forum?id=OGfyzExd69) |  | 0 | Designing synthetically accessible molecules and recommending analogs to unsynthesizable molecules are important problems for accelerating molecular discovery. We reconceptualize both problems using ideas from program synthesis. Drawing inspiration from syntax-guided synthesis approaches, we... | Alston Lo, Connor W. Coley, Jie Chen, Michael Sun, Minghao Guo, Wojciech Matusik |  |
| 1003 |  |  [Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning](https://openreview.net/forum?id=44z7HL4mfX) |  | 0 | We introduce INSTRUCT-SKILLMIX, an automated approach for creating diverse, high quality SFT data for instruction-following. The pipeline involves two stages, each leveraging an existing powerful LLM: (1) Skill extraction: uses the LLM to extract core “skills” for instruction-following by directly... | Anirudh Goyal, Sanjeev Arora, Simon Park, Simran Kaur |  |
| 1004 |  |  [P-Spikessm: Harnessing Probabilistic Spiking State Space Models for Long-Range Dependency Tasks](https://openreview.net/forum?id=Sf4ep9Udjf) |  | 0 | Spiking neural networks (SNNs) are posited as a computationally efficient and biologically plausible alternative to conventional neural architectures, with their core computational framework primarily using the leaky integrate-and-fire (LIF) neuron model. However, the limited hidden state... | Abhronil Sengupta, Malyaban Bal |  |
| 1005 |  |  [Trained Transformer Classifiers Generalize and Exhibit Benign Overfitting In-Context](https://openreview.net/forum?id=jwsPS8yRe4) |  | 0 | Transformers have the capacity to act as supervised learning algorithms: by properly encoding a set of labeled training (''in-context'') examples and an unlabeled test example into an input sequence of vectors of the same dimension, the forward pass of the transformer can produce predictions for... | Gal Vardi, Spencer Frei |  |
| 1006 |  |  [PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs](https://openreview.net/forum?id=bmrYu2Ekdz) |  | 0 | The stability of language model pre-training and its effects on downstream performance are still understudied. Prior work shows that the training process can yield significantly different results in response to slight variations in initial conditions, e.g., the random seed. Crucially, the research... | Hailey Schoelkopf, Max MüllerEberstein, Naomi Saphra, Oskar van der Wal, Pietro Lesci, Stella Biderman, Willem H. Zuidema |  |
| 1007 |  |  [Adapters for Altering LLM Vocabularies: What Languages Benefit the Most?](https://openreview.net/forum?id=KxQRHOre9D) |  | 0 | Vocabulary adaptation, which integrates new vocabulary into pre-trained language models, enables expansion to new languages and mitigates token over-fragmentation. However, existing approaches are limited by their reliance on heuristics or external embeddings. We propose VocADT, a novel method for... | Akiko Eriguchi, Haoran Xu, Hieu Hoang, Huda Khayrallah, HyoJung Han, Marine Carpuat |  |
| 1008 |  |  [DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories Search](https://openreview.net/forum?id=tn2mjzjSyR) |  | 0 | Enhancing the capability of large language models (LLMs) in reasoning has gained significant attention in recent years. Previous studies have demonstrated the effectiveness of various prompting strategies in aiding LLMs in reasoning (called "reasoning actions"), such as step-by-step thinking,... | Dian Yu, Dong Yu, Haitao Mi, Murong Yue, Wenlin Yao, Ziyu Yao |  |
| 1009 |  |  [Learning Diagrams: A Graphical Language for Compositional Training Regimes](https://openreview.net/forum?id=dqyuCsBvn9) |  | 0 | Motivated by deep learning regimes with multiple interacting yet distinct model components, we introduce learning diagrams, graphical depictions of training setups that capture parameterized learning as data rather than code. A learning diagram compiles to a unique loss function on which component... | Alexander Wilentz, Alina Zare, James P. Fairbanks, Mason Lary, Matthew Klawonn, Richard Samuelson |  |
| 1010 |  |  [Learning Continually by Spectral Regularization](https://openreview.net/forum?id=Hcb2cgPbMg) |  | 0 | Loss of plasticity is a phenomenon where neural networks can become more difficult to train over the course of learning. Continual learning algorithms seek to mitigate this effect by sustaining good performance while maintaining network trainability. We develop a new technique for improving... | Alex Lewandowski, András György, Dale Schuurmans, Marlos C. Machado, Mateusz Ostaszewski, Michal Bortkiewicz, Saurabh Kumar |  |
| 1011 |  |  [No Equations Needed: Learning System Dynamics Without Relying on Closed-Form ODEs](https://openreview.net/forum?id=kbm6tsICar) |  | 0 | Data-driven modeling of dynamical systems is a crucial area of machine learning. In many scenarios, a thorough understanding of the model’s behavior becomes essential for practical applications. For instance, understanding the behavior of a pharmacokinetic model, constructed as part of drug... | Krzysztof Kacprzyk, Mihaela van der Schaar |  |
| 1012 |  |  [An Undetectable Watermark for Generative Image Models](https://openreview.net/forum?id=jlhBFm7T2J) |  | 0 | We present the first undetectable watermarking scheme for generative image models. _Undetectability_ ensures that no efficient adversary can distinguish between watermarked and un-watermarked images, even after making many adaptive queries. In particular, an undetectable watermark does not degrade... | Dawn Song, Sam Gunn, Xuandong Zhao |  |
| 1013 |  |  [Teaching LLMs How to Learn with Contextual Fine-Tuning](https://openreview.net/forum?id=FS2nukC2jv) |  | 0 | Prompting Large Language Models (LLMs), or providing context on the expected model of operation, is an effective way to steer the outputs of such models to satisfy human desiderata after they have been trained. But in rapidly evolving domains, there is often need to fine-tune LLMs to improve either... | John Willes, Muhammad Adil Asif, Rahul G. Krishnan, Younwoo Choi, Ziwen Han |  |
| 1014 |  |  [Modeling dynamic social vision highlights gaps between deep learning and humans](https://openreview.net/forum?id=wAXsx2MYgV) |  | 0 | Deep learning models trained on computer vision tasks are widely considered the most successful models of human vision to date. The majority of work that supports this idea evaluates how accurately these models predict behavior and brain responses to static images of objects and scenes. Real-world... | Colin Conwell, Emalie McMahon, Kathy Garcia, Leyla Isik, Michael F. Bonner |  |
| 1015 |  |  [Statistical Tractability of Off-policy Evaluation of History-dependent Policies in POMDPs](https://openreview.net/forum?id=Qja5s0K3VX) |  | 0 | We investigate off-policy evaluation (OPE), a central and fundamental problem in reinforcement learning (RL), in the challenging setting of Partially Observable Markov Decision Processes (POMDPs) with large observation spaces. Recent works of Uehara et al. (2023a); Zhang & Jiang (2024) developed a... | Nan Jiang, Yuheng Zhang |  |
| 1016 |  |  [Large Language Models can Become Strong Self-Detoxifiers](https://openreview.net/forum?id=jY5oml9fe9) |  | 0 | Reducing the likelihood of generating harmful and toxic output is an essential task when aligning large language models (LLMs). Existing methods mainly rely on training an external reward model (i.e., another language model) or fine-tuning the LLM using self-generated data to influence the outcome.... | ChingYun Ko, Georgios Kollias, Luca Daniel, Payel Das, PinYu Chen, Soham Dan, Subhajit Chaudhury, Tejaswini Pedapati, Youssef Mroueh |  |
| 1017 |  |  [STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning](https://openreview.net/forum?id=4VHiptx7xe) |  | 0 | Robot learning is witnessing a significant increase in the size, diversity, and complexity of pre-collected datasets, mirroring trends in domains such as natural language processing and computer vision. Many robot learning methods treat such datasets as multi-task expert data and learn a... | Abhishek Gupta, Bingqing Chen, Jacob Berg, Jonathan Francis, Marius Memmel |  |
| 1018 |  |  [PortLLM: Personalizing Evolving Large Language Models with Training-Free and Portable Model Patches](https://openreview.net/forum?id=gyHoR6uFhU) |  | 0 | As large language models (LLMs) increasingly shape the AI landscape, fine-tuning pretrained models has become more popular than in the pre-LLM era for achieving optimal performance in domain-specific tasks. However, pretrained LLMs such as ChatGPT are periodically evolved (i.e., model parameters... | ChauWai Wong, Pingzhi Li, Rana Muhammad Shahroz, Shahriar Nirjon, Sukwon Yun, Tianlong Chen, Zhenyu Wang |  |
| 1019 |  |  [Robust Transfer of Safety-Constrained Reinforcement Learning Agents](https://openreview.net/forum?id=rvXdGL4pCJ) |  | 0 | Reinforcement learning (RL) often relies on trial and error, which may cause undesirable outcomes. As a result, standard RL is inappropriate for safety-critical applications. To address this issue, one may train a safe agent in a controlled environment (where safety violations are allowed) and then... | Markel Zubia, Nils Jansen, Thiago D. Simão |  |
| 1020 |  |  [Residual Stream Analysis with Multi-Layer SAEs](https://openreview.net/forum?id=XAjfjizaKs) |  | 0 | Sparse autoencoders (SAEs) are a promising approach to interpreting the internal representations of transformer language models. However, SAEs are usually trained separately on each transformer layer, making it difficult to use them to study how information flows across layers. To solve this... | Conor Houghton, Laurence Aitchison, Lucy Farnik, Tim Lawson |  |
| 1021 |  |  [The Same but Different: Structural Similarities and Differences in Multilingual Language Modeling](https://openreview.net/forum?id=NCrFA7dq8T) |  | 0 | We employ new tools from mechanistic interpretability to ask whether the internal structure of large language models (LLMs) shows correspondence to the linguistic structures which underlie the languages on which they are trained. In particular, we ask (1) when two languages employ the same... | Carsten Eickhoff, Ellie Pavlick, Matianyu Zang, Qinan Yu, Ruochen Zhang |  |
| 1022 |  |  [Disentangling Representations through Multi-task Learning](https://openreview.net/forum?id=yVGGtsOgc7) |  | 0 | Intelligent perception and interaction with the world hinges on internal representations that capture its underlying structure ("disentangled" or "abstract" representations). Disentangled representations serve as world models, isolating latent factors of variation in the world along approximately... | Aman Bhargava, Antonio Rangel, Pantelis Vafidis |  |
| 1023 |  |  [Training Free Guided Flow-Matching with Optimal Control](https://openreview.net/forum?id=61ss5RA1MM) |  | 0 | Controlled generation with pre-trained Diffusion and Flow Matching models has vast applications. One strategy for guiding ODE-based generative models is through optimizing a target loss $R(x_1)$ while staying close to the prior distribution. Along this line, some recent work showed the... | Chaoran Cheng, Ge Liu, Luran Wang, Yanru Qu, Yizhen Liao |  |
| 1024 |  |  [Efficient Sparse PCA via Block-Diagonalization](https://openreview.net/forum?id=FAYIlGDBa1) |  | 0 | Sparse Principal Component Analysis (Sparse PCA) is a pivotal tool in data analysis and dimensionality reduction. However, Sparse PCA is a challenging problem in both theory and practice: it is known to be NP-hard and current exact methods generally require exponential runtime. In this paper, we... | Alberto Del Pia, Dekun Zhou, Yinglun Zhu |  |
| 1025 |  |  [The Last Iterate Advantage: Empirical Auditing and Principled Heuristic Analysis of Differentially Private SGD](https://openreview.net/forum?id=DwqoBkj2Mw) |  | 0 | We propose a simple heuristic privacy analysis of noisy clipped stochastic gradient descent (DP-SGD) in the setting where only the last iterate is released and the intermediate iterates remain hidden. Namely, our heuristic assumes a linear structure for the model. We show experimentally that our... | Abhradeep Guha Thakurta, Adam Smith, Andreas Terzis, Arun Ganesh, Borja Balle, Christopher A. ChoquetteChoo, Jamie Hayes, Matthew Jagielski, Milad Nasr, Thomas Steinke |  |
| 1026 |  |  [Trivialized Momentum Facilitates Diffusion Generative Modeling on Lie Groups](https://openreview.net/forum?id=DTatjJTDl1) |  | 0 | The generative modeling of data on manifolds is an important task, for which diffusion models in flat spaces typically need nontrivial adaptations. This article demonstrates how a technique called \`trivialization' can transfer the effectiveness of diffusion models in Euclidean spaces to Lie... | Evangelos A. Theodorou, Lingkai Kong, Molei Tao, Tianrong Chen, Yuchen Zhu |  |
| 1027 |  |  [GeoLoRA: Geometric integration for parameter efficient fine-tuning](https://openreview.net/forum?id=bsFWJ0Kget) |  | 0 | Low-Rank Adaptation (LoRA) has become a widely used method for parameter-efficient fine-tuning of large-scale, pre-trained neural networks. However, LoRA and its extensions face several challenges, including the need for rank adaptivity, robustness, and computational efficiency during the... | Emanuele Zangrando, Francesco Tudisco, Gianluca Ceruti, Jonas Kusch, Steffen Schotthöfer |  |
| 1028 |  |  [Support is All You Need for Certified VAE Training](https://openreview.net/forum?id=oZkqkkvdND) |  | 0 | Variational Autoencoders (VAEs) have become increasingly popular and deployed in safety-critical applications. In such applications, we want to give certified probabilistic guarantees on performance under adversarial attacks. We propose a novel method, CIVET, for certified training of VAEs. CIVET... | Changming Xu, Debangshu Banerjee, Deepak Vasisht, Gagandeep Singh |  |
| 1029 |  |  [CONGO: Compressive Online Gradient Optimization](https://openreview.net/forum?id=4BFzTrIjPN) |  | 0 | We address the challenge of zeroth-order online convex optimization where the objective function's gradient exhibits sparsity, indicating that only a small number of dimensions possess non-zero gradients. Our aim is to leverage this sparsity to obtain useful estimates of the objective function's... | Aditya Akella, Dheeraj Narasimha, Divyanshu Saxena, Jeremy Carleton, Prathik Vijaykumar, Srinivas Shakkottai |  |
| 1030 |  |  [Scalable Extraction of Training Data from Aligned, Production Language Models](https://openreview.net/forum?id=vjel3nWP2a) |  | 0 | Large language models are prone to \*memorizing\* some of their training data. Memorized (and possibly sensitive) samples can then be extracted at generation time by adversarial or benign users. There is hope that \*model alignment\*---a standard training process that tunes a model to harmlessly... | A. Feder Cooper, Christopher A. ChoquetteChoo, Daphne Ippolito, Florian Tramèr, Javier Rando, Jonathan Hayase, Katherine Lee, Matthew Jagielski, Milad Nasr, Nicholas Carlini |  |
| 1031 |  |  [Measuring Non-Adversarial Reproduction of Training Data in Large Language Models](https://openreview.net/forum?id=590yfqz1LE) |  | 0 | Large language models memorize parts of their training data. Memorizing short snippets and facts is required to answer questions about the world and to be fluent in any language. But models have also been shown to reproduce long verbatim sequences of memorized text when prompted by a motivated... | Daphne Ippolito, Edoardo Debenedetti, Florian Tramèr, Javier Rando, Michael Aerni, Nicholas Carlini |  |
| 1032 |  |  [InsightBench: Evaluating Business Analytics Agents Through Multi-Step Insight Generation](https://openreview.net/forum?id=ZGqd0cbBvm) |  | 0 | Data analytics is essential for extracting valuable insights from data that can assist organizations in making effective decisions. We introduce InsightBench, a benchmark dataset with three key features. First, it consists of 100 datasets representing diverse business use cases such as finance and... | Abhay Puri, Alexandre Drouin, Alexandre Lacoste, Amirhossein Abaskohi, Christopher Pal, David Vázquez, Gaurav Sahu, Issam H. Laradji, Juan A. Rodríguez, Mohammad Chegini, Nicolas Chapados, Perouz Taslakian, Sai Rajeswar, Valentina Zantedeschi |  |
| 1033 |  |  [Optimizing Posterior Samples for Bayesian Optimization via Rootfinding](https://openreview.net/forum?id=I6UbnkUveF) |  | 0 | Bayesian optimization devolves the global optimization of a costly objective function to the global optimization of a sequence of acquisition functions. This inner-loop optimization can be catastrophically difficult if it involves posterior sample paths, especially in higher dimensions. We... | Bach Do, Ruda Zhang, Taiwo A. Adebiyi |  |
| 1034 |  |  [Model-Agnostic Knowledge Guided Correction for Improved Neural Surrogate Rollout](https://openreview.net/forum?id=3ep9ZYMZS3) |  | 0 | Modeling the evolution of physical systems is critical to many applications in science and engineering. As the evolution of these systems is governed by partial differential equations (PDEs), there are a number of computational simulations which resolve these systems with high accuracy. However, as... | Bharat Srikishan, Daniel O'Malley, Mohamed Mehana, Nicholas Lubbers, Nikhil Muralidhar |  |
| 1035 |  |  [Towards Fast, Specialized Machine Learning Force Fields: Distilling Foundation Models via Energy Hessians](https://openreview.net/forum?id=1durmugh3I) |  | 0 | The foundation model (FM) paradigm is transforming Machine Learning Force Fields (MLFFs), leveraging general-purpose representations and scalable training to perform a variety of computational chemistry tasks. Although MLFF FMs have begun to close the accuracy gap relative to first-principles... | Aditi S. Krishnapriyan, Ishan Amin, Sanjeev Raja |  |
| 1036 |  |  [Scaling Stick-Breaking Attention: An Efficient Implementation and In-depth Study](https://openreview.net/forum?id=r8J3DSD5kF) |  | 0 | The self-attention mechanism traditionally relies on the softmax operator, necessitating positional embeddings like RoPE, or position biases to account for token order. But current methods using still face length generalisation challenges. We investigate an alternative attention mechanism based on... | Aaron C. Courville, Rameswar Panda, Shawn Tan, Songlin Yang, Yikang Shen |  |
| 1037 |  |  [FlowDec: A flow-based full-band general audio codec with high perceptual quality](https://openreview.net/forum?id=uxDFlPGRLX) |  | 0 | We propose FlowDec, a neural full-band audio codec for general audio sampled at 48 kHz that combines non-adversarial codec training with a stochastic postfilter based on a novel conditional flow matching method. Compared to the prior work ScoreDec which is based on score matching, we generalize... | Alexander Richard, Matthew Le, Ricky T. Q. Chen, Simon Welker, Timo Gerkmann, WeiNing Hsu, YiChiao Wu |  |
| 1038 |  |  [End-to-end Learning of Gaussian Mixture Priors for Diffusion Sampler](https://openreview.net/forum?id=iXbUquaWbl) |  | 0 | Diffusion models optimized via variational inference (VI) have emerged as a promising tool for generating samples from unnormalized target densities. These models create samples by simulating a stochastic differential equation, starting from a simple, tractable prior, typically a Gaussian... | Denis Blessing, Gerhard Neumann, Xiaogang Jia |  |
| 1039 |  |  [IterGen: Iterative Semantic-aware Structured LLM Generation with Backtracking](https://openreview.net/forum?id=ac93gRzxxV) |  | 0 | Large Language Models (LLMs) are widely used for tasks such as natural language and code generation, but their outputs often suffer from issues like hallucination, toxicity, and incorrect results. Current libraries for structured LLM generation rely on left-to-right decoding without support for... | Gagandeep Singh, Rohan Gumaste, Sasa Misailovic, Shubham Ugare, Tarun Suresh |  |
| 1040 |  |  [OpenPRM: Building Open-domain Process-based Reward Models with Preference Trees](https://openreview.net/forum?id=fGIqGfmgkW) |  | 0 | Scaling inference-time computation is increasingly seen as the next frontier in scaling laws for large language models. Previous work in mathematics and coding has demonstrated the remarkable potential for inference-time scaling. During such scaling, fine-grained supervision through process-based... | Biqing Qi, Bowen Zhou, Ermo Hua, Haoxin Li, Jiayuan Zhang, Kaiyan Zhang, Ning Ding, Xingtai Lv, Xuekai Zhu |  |
| 1041 |  |  [SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training](https://openreview.net/forum?id=L9eBxTCpQG) |  | 0 | Large Language Models (LLMs) have demonstrated exceptional performance across diverse tasks, yet their training remains highly resource intensive and susceptible to critical challenges such as training instability. A predominant source of this instability stems from gradient and loss spikes, which... | Gaojie Jin, Lu Liu, Shiwei Liu, Tianjin Huang, Zhangyang Wang, Ziquan Zhu |  |
| 1042 |  |  [Probing the Latent Hierarchical Structure of Data via Diffusion Models](https://openreview.net/forum?id=0GzqVqCKns) |  | 0 | High-dimensional data must be highly structured to be learnable. Although the compositional and hierarchical nature of data is often put forward to explain learnability, quantitative measurements establishing these properties are scarce. Likewise, accessing the latent variables underlying such a... | Alessandro Favero, Antonio Sclocchi, Matthieu Wyart, Noam Itzhak Levi |  |
| 1043 |  |  [Dynamic Sparse Training versus Dense Training: The Unexpected Winner in Image Corruption Robustness](https://openreview.net/forum?id=daUQ7vmGap) |  | 0 | It is generally perceived that Dynamic Sparse Training opens the door to a new era of scalability and efficiency for artificial neural networks at, perhaps, some costs in accuracy performance for the classification task. At the same time, Dense Training is widely accepted as being the "de facto"... | Boqian Wu, Decebal Constantin Mocanu, Elena Mocanu, Maurice van Keulen, Mykola Pechenizkiy, Nicola Strisciuglio, Qiao Xiao, Shunxin Wang |  |
| 1044 |  |  [Unlocking Point Processes through Point Set Diffusion](https://openreview.net/forum?id=4anfpHj0wf) |  | 0 | Point processes model the distribution of random point sets in mathematical spaces, such as spatial and temporal domains, with applications in fields like seismology, neuroscience, and economics. Existing statistical and machine learning models for point processes are predominantly constrained by... | David Lüdke, Enric Rabasseda Raventós, Marcel Kollovieh, Stephan Günnemann |  |
| 1045 |  |  [Tracing Representation Progression: Analyzing and Enhancing Layer-Wise Similarity](https://openreview.net/forum?id=vVxeFSR4fU) |  | 0 | Analyzing the similarity of internal representations within and across different models has been an important technique for understanding the behavior of deep neural networks. Most existing methods for analyzing the similarity between representations of high dimensions, such as those based on... | Jiachen Jiang, Jinxin Zhou, Zhihui Zhu |  |
| 1046 |  |  [Language Agents Meet Causality - Bridging LLMs and Causal World Models](https://openreview.net/forum?id=y9A2TpaGsE) |  | 0 | Large Language Models (LLMs) have recently shown great promise in planning and reasoning applications. These tasks demand robust systems, which arguably require a causal understanding of the environment. While LLMs can acquire and reflect common sense causal knowledge from their pretraining data,... | Efstratios Gavves, Ivan Titov, John Gkountouras, Matthias Lindemann, Phillip Lippe |  |
| 1047 |  |  [From Models to Microtheories: Distilling a Model's Topical Knowledge for Grounded Question-Answering](https://openreview.net/forum?id=JV8zULNh24) |  | 0 | Recent reasoning methods (e.g., chain-of-thought) help users understand how language models (LMs) answer a single question, but they do little to reveal the LM’s overall understanding, or “theory,” about the question’s topic, making it still hard to trust the model. Our goal is to materialize such... | Alexander Sabol, Benjamin Van Durme, Bhavana Dalvi Mishra, Nathaniel Weir, Orion Weller, Oyvind Tafjord, Peter A. Jansen, Peter Clark, Sam Hornstein |  |
| 1048 |  |  [Enabling Realtime Reinforcement Learning at Scale with Staggered Asynchronous Inference](https://openreview.net/forum?id=fXb9BbuyAD) |  | 0 | Realtime environments change even as agents perform action inference and learning, thus requiring high interaction frequencies to effectively minimize regret. However, recent advances in machine learning involve larger neural networks with longer inference times, raising questions about their... | Glen Berseth, Gopeshh Subbaraj, Irina Rish, Matthew Riemer |  |
| 1049 |  |  [On the Transfer of Object-Centric Representation Learning](https://openreview.net/forum?id=bSq0XGS3kW) |  | 0 | The goal of object-centric representation learning is to decompose visual scenes into a structured representation that isolates the entities into individual vectors. Recent successes have shown that object-centric representation learning can be scaled to real-world scenes by utilizing features from... | Andrii Zadaianchuk, Aniket Rajiv Didolkar, Anirudh Goyal, Georg Martius, Maximilian Seitzer, Michael Curtis Mozer, Yoshua Bengio |  |
| 1050 |  |  [ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities](https://openreview.net/forum?id=lfPkGWXLLf) |  | 0 | Forecasts of future events are essential inputs into informed decision-making. Machine learning (ML) systems have the potential to deliver forecasts at scale, but there is no framework for evaluating the accuracy of ML systems on a standardized set of forecasting questions. To address this gap, we... | Chen YuehHan, Danny Halawi, Ezra Karger, Fred Zhang, Houtan Bastani, Philip Tetlock, Zachary Jacobs |  |
| 1051 |  |  [Score-based free-form architectures for high-dimensional Fokker-Planck equations](https://openreview.net/forum?id=5qg6JPSgCj) |  | 0 | Deep learning methods incorporate PDE residuals as the loss function for solving Fokker-Planck equations, and usually impose the proper normalization condition to avoid a trivial solution. However, soft constraints require careful balancing of multi-objective loss functions, and specific network... | Faguo Wu, Feng Liu, Xiao Zhang |  |
| 1052 |  |  [ReGen: Generative Robot Simulation via Inverse Design](https://openreview.net/forum?id=EbCUbPZjM1) |  | 0 | Simulation plays a key role in scaling robot learning and validating policies, but constructing simulations remains labor-intensive. In this paper, we introduce ReGen, a generative simulation framework that automates this process using inverse design. Given an agent's behavior (such as a motion... | Andrew Silva, Daniela Rus, Erfan Aasi, Guy Rosman, Phat Nguyen, Sertac Karaman, TsunHsuan Wang, ZhangWei Hong |  |
| 1053 |  |  [Flow-based Variational Mutual Information: Fast and Flexible Approximations](https://openreview.net/forum?id=spDUv05cEq) |  | 0 | Mutual Information (MI) is a fundamental measure of dependence between random variables, but its practical application is limited because it is difficult to calculate in many circumstances. Variational methods offer one approach by introducing an approximate distribution to create various bounds on... | Caleb Dahlke, Jason Pacheco |  |
| 1054 |  |  [Multi-agent cooperation through learning-aware policy gradients](https://openreview.net/forum?id=GkWA6NjePN) |  | 0 | Self-interested individuals often fail to cooperate, posing a fundamental challenge for multi-agent learning. How can we achieve cooperation among self-interested, independent learning agents? Promising recent work has shown that in certain tasks cooperation can be established between... | Alexander Meulemans, Blaise Agüera y Arcas, Blake Aaron Richards, Eric Elmoznino, Guillaume Lajoie, Johannes von Oswald, João Sacramento, Nino Scherrer, Seijin Kobayashi |  |
| 1055 |  |  [Few-Class Arena: A Benchmark for Efficient Selection of Vision Models and Dataset Difficulty Measurement](https://openreview.net/forum?id=2ET561DyPe) |  | 0 | We propose Few-Class Arena (FCA), as a unified benchmark with focus on testing efficient image classification models for few classes. A wide variety of benchmark datasets with many classes (80-1000) have been created to assist Computer Vision architectural evolution. An increasing number of vision... | Bryan Bo Cao, Lawrence O'Gorman, Michael Coss, Shubham Jain |  |
| 1056 |  |  [Transformers Struggle to Learn to Search](https://openreview.net/forum?id=9cQB1Hwrtw) |  | 0 | Search is an ability foundational in many important tasks, and recent studies have shown that large language models (LLMs) struggle to perform search robustly. It is unknown whether this inability is due to a lack of data, insufficient model parameters, or fundamental limitations of the transformer... | Abulhair Saparov, He He, Mehran Kazemi, Najoung Kim, Nitish Joshi, Richard Yuanzhe Pang, Shreyas Pimpalgaonkar, Srushti Ajay Pawar, Vishakh Padmakumar |  |
| 1057 |  |  [Is uniform expressivity too restrictive? Towards efficient expressivity of GNNs](https://openreview.net/forum?id=lsvGqR6OTf) |  | 0 | Uniform expressivity guarantees that a Graph Neural Network (GNN) can express a query without the parameters depending on the size of the input graphs. This property is desirable in applications in order to have number of trainable parameters that is independent of the size of the input graphs.... | Josué TonelliCueto, Sammy Khalife |  |
| 1058 |  |  [Training Neural Networks as Recognizers of Formal Languages](https://openreview.net/forum?id=aWLQTbfFgV) |  | 0 | Characterizing the computational power of neural network architectures in terms of formal language theory remains a crucial line of research, as it describes lower and upper bounds on the reasoning capabilities of modern AI. However, when empirically testing these bounds, existing work often leaves... | Alexandra Butoi, Anej Svete, Brian DuSell, Ghazal Khalighinejad, Josef Valvoda, Ryan Cotterell |  |
| 1059 |  |  [Nonconvex Stochastic Optimization under Heavy-Tailed Noises: Optimal Convergence without Gradient Clipping](https://openreview.net/forum?id=NKotdPUc3L) |  | 0 | Recently, the study of heavy-tailed noises in first-order nonconvex stochastic optimization has gotten a lot of attention since it was recognized as a more realistic condition as suggested by many empirical observations. Specifically, the stochastic noise (the difference between the stochastic and... | Zhengyuan Zhou, Zijian Liu |  |
| 1060 |  |  [Can We Trust Embodied Agents? Exploring Backdoor Attacks against Embodied LLM-Based Decision-Making Systems](https://openreview.net/forum?id=S1Bv3068Xt) |  | 0 | Large Language Models (LLMs) have shown significant promise in real-world decision-making tasks for embodied artificial intelligence, especially when fine-tuned to leverage their inherent common sense and reasoning abilities while being tailored to specific applications. However, this fine-tuning... | Justin Yue, Lixu Wang, Qi Alfred Chen, Qi Zhu, Ruochen Jiao, Shaoyuan Xie, Takami Sato, Yixuan Wang |  |
| 1061 |  |  [Self-Attention-Based Contextual Modulation Improves Neural System Identification](https://openreview.net/forum?id=JeLqFpFzwX) |  | 0 | Convolutional neural networks (CNNs) have been shown to be state-of-the-art models for visual cortical neurons. Cortical neurons in the primary visual cortex are sensitive to contextual information mediated by extensive horizontal and feedback connections. Standard CNNs integrate global contextual... | Isaac Lin, Shang Gao, Shiming Tang, Tai Sing Lee, Tianye Wang |  |
| 1062 |  |  [Dataset Distillation via Knowledge Distillation: Towards Efficient Self-Supervised Pre-training of Deep Networks](https://openreview.net/forum?id=c61unr33XA) |  | 0 | Dataset distillation (DD) generates small synthetic datasets that can efficiently train deep networks with a limited amount of memory and compute. Despite the success of DD methods for supervised learning, DD for self-supervised pre-training of deep models has remained unaddressed. Pre-training on... | Baharan Mirzasoleiman, Jiayi Ni, Siddharth Joshi |  |
| 1063 |  |  [Robotouille: An Asynchronous Planning Benchmark for LLM Agents](https://openreview.net/forum?id=OhUoTMxFIH) |  | 0 | Effective asynchronous planning, or the ability to efficiently reason and plan over states and actions that must happen in parallel or sequentially, is essential for agents that must account for time delays, reason over diverse long-horizon tasks, and collaborate with other agents. While large... | Gonzalo GonzalezPumariega, Leong Su Yean, Neha Sunkara, Sanjiban Choudhury |  |
| 1064 |  |  [Learn-by-interact: A Data-Centric Framework For Self-Adaptive Agents in Realistic Environments](https://openreview.net/forum?id=3UKOzGWCVY) |  | 0 | Autonomous agents powered by large language models (LLMs) have the potential to enhance human capabilities, assisting with digital tasks from sending emails to performing data analysis. The abilities of existing LLMs at such tasks are often hindered by the lack of high-quality agent data from the... | Hongjin Su, Jinsung Yoon, Pengcheng Yin, Ruoxi Sun, Sercan Ö. Arik, Tao Yu |  |
| 1065 |  |  [Causal Representation Learning from Multimodal Biomedical Observations](https://openreview.net/forum?id=hjROBHstZ3) |  | 0 | Prevalent in biomedical applications (e.g., human phenotype research), multimodal datasets can provide valuable insights into the underlying physiological mechanisms. However, current machine learning (ML) models designed to analyze these datasets often lack interpretability and identifiability... | Eran Segal, Eric P. Xing, Gongxu Luo, Guangyi Chen, Kun Zhang, Lingjing Kong, Loka Li, Mengyue Yang, Petar Stojanov, Yixuan Zhang, Yuewen Sun, Yujia Zheng, Zijian Li |  |
| 1066 |  |  [On the Learn-to-Optimize Capabilities of Transformers in In-Context Sparse Recovery](https://openreview.net/forum?id=NHhjczmJjo) |  | 0 | An intriguing property of the Transformer is its ability to perform in-context learning (ICL), where the Transformer can solve different inference tasks without parameter updating based on the contextual information provided by the corresponding input-output demonstration pairs. It has been... | Cong Shen, Jing Yang, Renpu Liu, Ruida Zhou |  |
| 1067 |  |  [OvercookedV2: Rethinking Overcooked for Zero-Shot Coordination](https://openreview.net/forum?id=hlvLM3GX8R) |  | 0 | AI agents hold the potential to transform everyday life by helping humans achieve their goals. To do this successfully, agents need to be able to coordinate with novel partners without prior interaction, a setting known as zero-shot coordination (ZSC). Overcooked has become one of the most popular... | Andrei Lupu, Ani Calinescu, Benjamin Ellis, Jakob Nicolaus Foerster, Tin Dizdarevic, Tobias Gessler |  |
| 1068 |  |  [Sketching for Convex and Nonconvex Regularized Least Squares with Sharp Guarantees](https://openreview.net/forum?id=7liN6uHAQZ) |  | 0 | Randomized algorithms play a crucial role in efficiently solving large-scale optimization problems. In this paper, we introduce Sketching for Regularized Optimization (SRO), a fast sketching algorithm designed for least squares problems with convex or nonconvex regularization. SRO operates by first... | Ping Li, Yingzhen Yang |  |
| 1069 |  |  [TULIP: Token-length Upgraded CLIP](https://openreview.net/forum?id=r9oqHOdoHf) |  | 0 | We address the challenge of representing long captions in vision-language models, such as CLIP. By design these models are limited by fixed, absolute positional encodings, restricting inputs to a maximum of 77 tokens and hindering performance on tasks requiring longer descriptions. Although recent... | Cees G. M. Snoek, Ivona Najdenkoska, Marcel Worring, Mohammad Mahdi Derakhshani, Nanne van Noord, Yuki M. Asano |  |
| 1070 |  |  [DELIFT: Data Efficient Language model Instruction Fine-Tuning](https://openreview.net/forum?id=Fty0wTcemV) |  | 0 | Fine-tuning large language models (LLMs) is crucial for task specialization but often becomes resource-intensive due to redundant or uninformative data. Existing data selection methods typically rely either on computationally expensive gradient-based metrics or static embeddings that fail to adapt... | Ishika Agarwal, Krishnateja Killamsetty, Lucian Popa, Marina Danilevsky |  |
| 1071 |  |  [U-shaped and Inverted-U Scaling behind Emergent Abilities of Large Language Models](https://openreview.net/forum?id=jjfve2gIXe) |  | 0 | Large language models (LLMs) have been shown to exhibit \*emergent abilities\* in some downstream tasks, where model performance stagnates at first and then improves sharply and unpredictably with scale beyond a threshold. In this work, we investigate the phenomenon by grouping questions based on... | Melody Lo, TungYu Wu |  |
| 1072 |  |  [Hierarchical Autoregressive Transformers: Combining Byte- and Word-Level Processing for Robust, Adaptable Language Models](https://openreview.net/forum?id=tU074jg2vS) |  | 0 | Tokenization is a fundamental step in natural language processing, breaking text into units that computational models can process. While learned subword tokenizers have become the de-facto standard, they present challenges such as large vocabularies, limited adaptability to new domains or... | Björn Deiseroth, Constantin Eichenberg, Lukas Balles, Pit Neitemeier |  |
| 1073 |  |  [Locally Connected Echo State Networks for Time Series Forecasting](https://openreview.net/forum?id=KeRwLLwZaw) |  | 0 | Echo State Networks (ESNs) are a class of recurrent neural networks in which only a small readout regression layer is trained, while the weights of the recurrent network, termed the reservoir, are randomly assigned and remain fixed. Our work introduces the Locally Connected ESN (LCESN), a novel ESN... | Filip Matzner, Frantisek Mráz |  |
| 1074 |  |  [Scaling Laws for Adversarial Attacks on Language Model Activations and Tokens](https://openreview.net/forum?id=YzxMu1asQi) |  | 0 | We explore a class of adversarial attacks targeting the activations of language models to derive upper-bound scaling laws on their attack susceptibility. By manipulating a relatively small subset of model activations, $a$, we demonstrate the ability to control the exact prediction of a significant... | Stanislav Fort |  |
| 1075 |  |  [BEEM: Boosting Performance of Early Exit DNNs using Multi-Exit Classifiers as Experts](https://openreview.net/forum?id=EzrZX9bd4G) |  | 0 | Early Exit (EE) techniques have emerged as a means to reduce inference latency in Deep Neural Networks (DNNs). The latency improvement and accuracy in these techniques crucially depend on the criteria used to make exit decisions. We propose a new decision criterion BEEM where exit classifiers are... | Divya Jyoti Bajpai, Manjesh Kumar Hanawal |  |
| 1076 |  |  [Real-time design of architectural structures with differentiable mechanics and neural networks](https://openreview.net/forum?id=Tpjq66xwTq) |  | 0 | Designing mechanically efficient geometry for architectural structures like shells, towers, and bridges, is an expensive iterative process. Existing techniques for solving such inverse problems rely on traditional optimization methods, which are slow and computationally expensive, limiting... | Eder Medina, Isabel M. de Oliveira, Rafael Pastrana, Ryan P. Adams, Sigrid Adriaenssens |  |
| 1077 |  |  [DAMO: Decoding by Accumulating Activations Momentum for Mitigating Hallucinations in Vision-Language Models](https://openreview.net/forum?id=JUr0YOMvZA) |  | 0 | Large Vision-Language Models (VLMs) exhibit significant potential in multimodal tasks but often struggle with hallucinations—responses that are plausible yet visually ungrounded. In this work, we investigate the layer-wise prediction tendencies of VLMs and conduct an in-depth analysis of their... | Hengrui Gu, Kaishen Wang, Kaixiong Zhou, Meijun Gao |  |
| 1078 |  |  [Talking Turns: Benchmarking Audio Foundation Models on Turn-Taking Dynamics](https://openreview.net/forum?id=2e4ECh0ikn) |  | 0 | The recent wave of audio foundation models (FMs) could provide new capabilities for conversational modeling. However, there have been limited efforts to evaluate these audio FMs comprehensively on their ability to have natural and interactive conversations. To engage in meaningful conversation with... | ChungCheng Chiu, Ruoming Pang, Shinji Watanabe, Siddhant Arora, Zhiyun Lu |  |
| 1079 |  |  [Mind the GAP: Glimpse-based Active Perception improves generalization and sample efficiency of visual reasoning](https://openreview.net/forum?id=iXCeQ2m6vT) |  | 0 | Human capabilities in understanding visual relations are far superior to those of AI systems, especially for previously unseen objects. For example, while AI systems struggle to determine whether two such objects are visually the same or different, humans can do so with ease. Active vision theories... | Angeliki Pantazi, Oleh Kolner, Stanislaw Wozniak, Thomas Ortner |  |
| 1080 |  |  [MMTEB: Massive Multilingual Text Embedding Benchmark](https://openreview.net/forum?id=zl3pfz4VCV) |  | 0 | Text embeddings are typically evaluated on a narrow set of tasks, limited in terms of languages, domains, and task types. To circumvent this limitation and to provide a more comprehensive evaluation, we introduce the Massive Multilingual Text Embedding Benchmark (MMTEB) -- a large-scale... | Akash Kundu, Ashwin Mathur, David Stap, Diganta Misra, Dominik Krzeminski, Genta Indra Winata, Imene Kerboua, Isaac Chung, Jay Gala, Jonathan Rystrøm, Kenneth C. Enevoldsen, Marion Schaeffer, Mathieu Ciancone, Márton Kardos, Roman Solomatin, Saba Sturua, Saiteja Utpala, Shreeya Dhakal, Wissam Siblini, et al., Ömer Veysel Çagatan |  |
| 1081 |  |  [TexTailor: Customized Text-aligned Texturing via Effective Resampling](https://openreview.net/forum?id=1NprT9Kz0d) |  | 0 | We present TexTailor, a novel method for generating consistent object textures from textual descriptions. Existing text-to-texture synthesis approaches utilize depth-aware diffusion models to progressively generate images and synthesize textures across predefined multiple viewpoints. However, these... | Daeshik Kim, Suin Lee |  |
| 1082 |  |  [Generalization and Distributed Learning of GFlowNets](https://openreview.net/forum?id=PJNhZoCjLh) |  | 0 | Conventional wisdom attributes the success of Generative Flow Networks (GFlowNets) to their ability to exploit the compositional structure of the sample space for learning generalizable flow functions (Bengio et al., 2021). Despite the abundance of empirical evidence, formalizing this belief with... | Amauri H. Souza, Diego Mesquita, Omar Rivasplata, Samuel Kaski, Tiago Silva, Vikas Garg |  |
| 1083 |  |  [MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions](https://openreview.net/forum?id=WWXjMYZxfH) |  | 0 | Reinforcement learning from human feedback (RLHF) has demonstrated effectiveness in aligning large language models (LLMs) with human preferences. However, token-level RLHF suffers from the credit assignment problem over long sequences, where delayed rewards make it challenging for the model to... | Haoran Sun, Hua Wu, Huang Fang, Shuohuan Wang, Yekun Chai, Yu Sun |  |
| 1084 |  |  [Bonsai: Gradient-free Graph Condensation for Node Classification](https://openreview.net/forum?id=5x88lQ2MsH) |  | 0 | Graph condensation has emerged as a promising avenue to enable scalable training of GNNs by compressing the training dataset while preserving essential graph characteristics. Our study uncovers significant shortcomings in current graph condensation techniques. First, the majority of the algorithms... | Hariprasad Kodamana, Mridul Gupta, Samyak Jain, Sayan Ranu, Vansh Ramani |  |
| 1085 |  |  [Logically Consistent Language Models via Neuro-Symbolic Integration](https://openreview.net/forum?id=7PGluppo4k) |  | 0 | Current large language models (LLMs) are far from reliable: they are prone to generate non-factual information and, more crucially, to contradict themselves when prompted to reason about relations between real entities of the world. These problems are currently addressed with large scale... | Antonio Vergari, Diego Calanzone, Stefano Teso |  |
| 1086 |  |  [Narrowing Information Bottleneck Theory for Multimodal Image-Text Representations Interpretability](https://openreview.net/forum?id=INqLJwqUmc) |  | 0 | The task of identifying multimodal image-text representations has garnered increasing attention, particularly with models such as CLIP (Contrastive Language-Image Pretraining), which demonstrate exceptional performance in learning complex associations between images and text. Despite these... | Fang Chen, Jiahao Huang, Jianlong Zhou, Jiayu Zhang, Nan Yang, Zhibo Jin, Zhiyu Zhu |  |
| 1087 |  |  [Efficient Source-Free Time-Series Adaptation via Parameter Subspace Disentanglement](https://openreview.net/forum?id=Q5Sawm0nqo) |  | 0 | In this paper, we propose a framework for efficient Source-Free Domain Adaptation (SFDA) in the context of time-series, focusing on enhancing both parameter efficiency and data-sample utilization. Our approach introduces an improved paradigm for source-model preparation and target-side adaptation,... | Ali Moin, Behrooz Mahasseni, Christopher Michael Sandino, Ellen L. Zippi, Erdrin Azemi, Gaurav Patel, Juri Minxha |  |
| 1088 |  |  [WardropNet: Traffic Flow Predictions via Equilibrium-Augmented Learning](https://openreview.net/forum?id=7FHSPd3SRE) |  | 0 | When optimizing transportation systems, anticipating traffic flows is a central element. Yet, computing such traffic equilibria remains computationally expensive. Against this background, we introduce a novel combinatorial optimization augmented neural network pipeline that allows for fast and... | Axel Parmentier, Dario Paccagnan, Kai Jungel, Maximilian Schiffer |  |
| 1089 |  |  [To Clip or not to Clip: the Dynamics of SGD with Gradient Clipping in High-Dimensions](https://openreview.net/forum?id=jmN1zXMq0O) |  | 0 | The success of modern machine learning is due in part to the adaptive optimization methods that have been developed to deal with the difficulties of training large models over complex datasets. One such method is gradient clipping: a practical procedure with limited theoretical underpinnings. In... | Atish Agarwala, Elliot Paquette, Ke Liang Xiao, Noah Marshall |  |
| 1090 |  |  [Discrete Diffusion Schrödinger Bridge Matching for Graph Transformation](https://openreview.net/forum?id=tQyh0gnfqW) |  | 0 | Transporting between arbitrary distributions is a fundamental goal in generative modeling. Recently proposed diffusion bridge models provide a potential solution, but they rely on a joint distribution that is difficult to obtain in practice. Furthermore, formulations based on continuous domains... | Hyeongwoo Kim, Jeheon Woo, Jun Hyeong Kim, Seokhyun Moon, Seonghwan Kim, Woo Youn Kim |  |
| 1091 |  |  [Breaking Mental Set to Improve Reasoning through Diverse Multi-Agent Debate](https://openreview.net/forum?id=t6QHYUOQL7) |  | 0 | Large Language Models (LLMs) have seen significant progress but continue to struggle with persistent reasoning mistakes. Previous methods of \*self-reflection\* have been proven limited due to the models’ inherent fixed thinking patterns. While Multi-Agent Debate (MAD) attempts to mitigate this by... | Jie Cao, Ran He, Tieniu Tan, Yexiang Liu, Zekun Li |  |
| 1092 |  |  [Resolution Attack: Exploiting Image Compression to Deceive Deep Neural Networks](https://openreview.net/forum?id=OFukl9Qg8P) |  | 0 | Model robustness is essential for ensuring the stability and reliability of machine learning systems. Despite extensive research on various aspects of model robustness, such as adversarial robustness and label noise robustness, the exploration of robustness towards different resolutions, remains... | Jizhong Han, Qiao Li, Wangjia Yu, Xiaodan Zhang, Xiaomeng Fu |  |
| 1093 |  |  [SLMRec: Distilling Large Language Models into Small for Sequential Recommendation](https://openreview.net/forum?id=G4wARwjF8M) |  | 0 | Sequential Recommendation (SR) task involves predicting the next item a user is likely to interact with, given their past interactions. The SR models examine the sequence of a user's actions to discern more complex behavioral patterns and temporal dynamics. Recent research demonstrates the great... | Jiaojiao Han, Qitian Wu, Wenfang Lin, Wujiang Xu, Xuying Ning, Yongfeng Zhang, Yunxiao Shi, Zujie Liang |  |
| 1094 |  |  [How Do Large Language Models Understand Graph Patterns? A Benchmark for Graph Pattern Comprehension](https://openreview.net/forum?id=CkKEuLmRnr) |  | 0 | Benchmarking the capabilities and limitations of large language models (LLMs) in graph-related tasks is becoming an increasingly popular and crucial area of research. Recent studies have shown that LLMs exhibit a preliminary ability to understand graph structures and node features. However, the... | Bohang Zhang, Caihua Shan, Dongsheng Li, Haohao Qu, Jiliang Tang, Qihao Wen, Wenqi Fan, Xinnan Dai, Yifei Shen |  |
| 1095 |  |  [Efficient Active Imitation Learning with Random Network Distillation](https://openreview.net/forum?id=GFgn2LprFR) |  | 0 | Developing agents for complex and underspecified tasks, where no clear objective exists, remains challenging but offers many opportunities. This is especially true in video games, where simulated players (bots) need to play realistically, and there is no clear reward to evaluate them. While... | Anthony Kobanda, Emilien Biré, Ludovic Denoyer, Rémy Portelas |  |
| 1096 |  |  [Model-based RL as a Minimalist Approach to Horizon-Free and Second-Order Bounds](https://openreview.net/forum?id=txD9llAYn9) |  | 0 | Learning a transition model via Maximum Likelihood Estimation (MLE) followed by planning inside the learned model is perhaps the most standard and simplest Model-based Reinforcement Learning (RL) framework. In this work, we show that such a simple Model-based RL scheme, when equipped with... | Dongruo Zhou, John C. S. Lui, Wen Sun, Zhiyong Wang |  |
| 1097 |  |  [What Do You See in Common? Learning Hierarchical Prototypes over Tree-of-Life to Discover Evolutionary Traits](https://openreview.net/forum?id=4sDicVEy6M) |  | 0 | A grand challenge in biology is to discover evolutionary traits---features of organisms common to a group of species with a shared ancestor in the tree of life (also referred to as phylogenetic tree). With the growing availability of image repositories in biology, there is a tremendous opportunity... | Anuj Karpatne, Arka Daw, Caleb Patrick Charpentier, Elizabeth G. Campolongo, Harish Babu Manogaran, Hilmar Lapp, Josef C. Uyeda, Kaiya L. Provost, Kazi Sajeed Mehrab, M. Maruf, Matthew J. Thompson, Paula M. Mabee, Tanya Y. BergerWolf, Wasila M. Dahdul, WeiLun Chao |  |
| 1098 |  |  [Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning](https://openreview.net/forum?id=9RCT0ngvZP) |  | 0 | Synthetic data has been widely used to train large language models, but their generative nature inevitably introduces noisy, non-informative, and misleading learning signals. In this paper, we propose Montessori-Instruct, a novel data synthesis framework that tailors the data synthesis ability of... | Chenyan Xiong, Xiaochuan Li, Zichun Yu |  |
| 1099 |  |  [Affine Steerable Equivariant Layer for Canonicalization of Neural Networks](https://openreview.net/forum?id=5i6ZZUjCA9) |  | 0 | In the field of equivariant networks, achieving affine equivariance, particularly for general group representations, has long been a challenge. In this paper, we propose the steerable EquivarLayer, a generalization of InvarLayer (Li et al., 2024), by building on the concept of equivariants beyond... | Yeqing Qiu, Yikang Li, Yuxuan Chen, Zhouchen Lin |  |
| 1100 |  |  [Has the Deep Neural Network learned the Stochastic Process? An Evaluation Viewpoint](https://openreview.net/forum?id=2U8owdruSQ) |  | 0 | This paper presents the first systematic study of evaluating Deep Neural Networks (DNNs) designed to forecast the evolution of stochastic complex systems. We show that traditional evaluation methods like threshold-based classification metrics and error-based scoring rules assess a DNN's ability to... | Beomseok Kang, Biswadeep Chakraborty, Harshit Kumar, Saibal Mukhopadhyay |  |
| 1101 |  |  [LICORICE: Label-Efficient Concept-Based Interpretable Reinforcement Learning](https://openreview.net/forum?id=Mjn53GtMxi) |  | 0 | Recent advances in reinforcement learning (RL) have predominantly leveraged neural network policies for decision-making, yet these models often lack interpretability, posing challenges for stakeholder comprehension and trust. Concept bottleneck models offer an interpretable alternative by... | Fei Fang, Geoffrey J. Gordon, Stephanie Milani, Zhuorui Ye |  |
| 1102 |  |  [MIND: Math Informed syNthetic Dialogues for Pretraining LLMs](https://openreview.net/forum?id=TuOTSAiHDn) |  | 0 | The utility of synthetic data to enhance pretraining data quality and hence to improve downstream task accuracy has been widely explored in recent large language models (LLMs). Yet, these approaches fall inadequate in complex, multi-hop and mathematical reasoning tasks as the synthetic data... | Bryan Catanzaro, Eric Nyberg, John Kamalu, Mohammad Shoeybi, Mostofa Patwary, Sanjeev Satheesh, Shrimai Prabhumoye, Syeda Nahida Akter |  |
| 1103 |  |  [SMT: Fine-Tuning Large Language Models with Sparse Matrices](https://openreview.net/forum?id=GbgCRJedQ7) |  | 0 | Various parameter-efficient fine-tuning (PEFT) methods, including LoRA and its variants, have gained popularity for reducing computational costs. However, there is often an accuracy gap between PEFT approaches and full fine-tuning (FT), and this discrepancy has not yet been systematically explored.... | Haoze He, Heather Miller, Juncheng B. Li, Xuan Jiang |  |
| 1104 |  |  [KGARevion: An AI Agent for Knowledge-Intensive Biomedical QA](https://openreview.net/forum?id=tnB94WQGrn) |  | 0 | Biomedical reasoning integrates structured, codified knowledge with tacit, experience-driven insights. Depending on the context, quantity, and nature of available evidence, researchers and clinicians use diverse strategies, including rule-based, prototype-based, and case-based reasoning. Effective... | DjorkArné Clevert, Marinka Zitnik, Shanghua Gao, Valentina Giunchiglia, Xiaolong Liu, Xiaorui Su, Yibo Wang |  |
| 1105 |  |  [Moral Alignment for LLM Agents](https://openreview.net/forum?id=MeGDmZjUXy) |  | 0 | Decision-making agents based on pre-trained Large Language Models (LLMs) are increasingly being deployed across various domains of human activity. While their applications are currently rather specialized, several research efforts are underway to develop more generalist agents. As LLM-based systems... | Elizaveta Tennant, Mirco Musolesi, Stephen Hailes |  |
| 1106 |  |  [ColPali: Efficient Document Retrieval with Vision Language Models](https://openreview.net/forum?id=ogjBpZ8uSi) |  | 0 | Documents are visually rich structures that convey information through text, but also figures, page layouts, tables, or even fonts. Since modern retrieval systems mainly rely on the textual information they extract from document pages to index documents -often through lengthy and brittle... | Bilel Omrani, Céline Hudelot, Gautier Viaud, Hugues Sibille, Manuel Faysse, Pierre Colombo, Tony Wu |  |
| 1107 |  |  [Content-Style Learning from Unaligned Domains: Identifiability under Unknown Latent Dimensions](https://openreview.net/forum?id=p60Y6o85Cj) |  | 0 | Understanding identifiability of latent content and style variables from unaligned multi-domain data is essential for tasks such as domain translation and data generation. Existing works on content-style identification were often developed under somewhat stringent conditions, e.g., that all latent... | Sagar Shrestha, Xiao Fu |  |
| 1108 |  |  [From an LLM Swarm to a PDDL-empowered Hive: Planning Self-executed Instructions in a Multi-modal Jungle](https://openreview.net/forum?id=QAAsnSRwgu) |  | 0 | In response to the call for agent-based solutions that leverage the ever-increasing capabilities of the deep models' ecosystem, we introduce a comprehensive solution for selecting appropriate models and subsequently planning a set of atomic actions to satisfy the end-users' instructions. Our... | Chenxin Diao, Damien Graux, Jeff Z. Pan, Kaustubh Vyas, Keshuang Li, Pavlos Vougiouklis, Ruofei Lai, Sébastien Montella, Wendi Zhou, Yang Ren, Yijun Yang |  |
| 1109 |  |  [ZooProbe: A Data Engine for Evaluating, Exploring, and Evolving Large-scale Training Data for Multimodal LLMs](https://openreview.net/forum?id=T4LtGj7us1) |  | 0 | Multimodal Large Language Models (MLLMs) are thriving through continuous fine-tuning by LLMs. Driven by the law that "scale is everything", MLLMs expand their training sets during version iterations. In this paper, we propose a large-scale training data engine built around an... | DeChuan Zhan, HanJia Ye, QingGuo Chen, Shiyin Lu, YiKai Zhang |  |
| 1110 |  |  [Wicked Oddities: Selectively Poisoning for Effective Clean-Label Backdoor Attacks](https://openreview.net/forum?id=1Z3C49JQVf) |  | 0 | Deep neural networks are vulnerable to backdoor attacks, a type of adversarial attack that poisons the training data to manipulate the behavior of models trained on such data. Clean-label backdoor is a more stealthy form of backdoor attacks that can perform the attack without changing the labels of... | Hoang ThanhTung, Khoa D. Doan, KokSeng Wong, NgocHieu Nguyen, Nguyen HungQuang, Thanh NguyenTang, TheAnh Ta |  |
| 1111 |  |  [Quantum (Inspired) D2-sampling with Applications](https://openreview.net/forum?id=tDIL7UXmSS) |  | 0 | $D^2$-sampling is a fundamental component of sampling-based clustering algorithms such as $k$-means++. Given a dataset $V \subset \mathbb{R}^d$ with $N$ points and a center set $C \subset \mathbb{R}^d$, $D^2$-sampling refers to picking a point from $V$ where the sampling probability of a point is... | Poojan Chetan Shah, Ragesh Jaiswal |  |
| 1112 |  |  [Non-Adversarial Inverse Reinforcement Learning via Successor Feature Matching](https://openreview.net/forum?id=LvRQgsvd5V) |  | 0 | In inverse reinforcement learning (IRL), an agent seeks to replicate expert demonstrations through interactions with the environment. Traditionally, IRL is treated as an adversarial game, where an adversary searches over reward models, and a learner optimizes the reward through repeated RL... | Arnav Kumar Jain, Glen Berseth, Harley Wiltzer, Irina Rish, Jesse Farebrother, Sanjiban Choudhury |  |
| 1113 |  |  [MatExpert: Decomposing Materials Discovery By Mimicking Human Experts](https://openreview.net/forum?id=AUBvo4sxVL) |  | 0 | Material discovery is a critical research area with profound implications for various industries. In this work, we introduce MatExpert, a novel framework that leverages Large Language Models (LLMs) and contrastive learning to accelerate the discovery and design of new solid-state materials.... | Bang Liu, Qianggang Ding, Santiago Miret |  |
| 1114 |  |  [SOAP: Improving and Stabilizing Shampoo using Adam for Language Modeling](https://openreview.net/forum?id=IDxZhXrpNf) |  | 0 | There is growing evidence of the effectiveness of Shampoo, a higher-order preconditioning method, over Adam in deep learning optimization tasks. However, Shampoo's drawbacks include additional hyperparameters and computational overhead when compared to Adam, which only updates running averages of... | David Brandfonbrener, Depen Morwani, Itai Shapira, Lucas Janson, Nikhil Vyas, Rosie Zhao, Sham M. Kakade |  |
| 1115 |  |  [Causal Graphical Models for Vision-Language Compositional Understanding](https://openreview.net/forum?id=haJHr4UsQX) |  | 0 | Recent work has empirically shown that Vision-Language Models (VLMs) struggle to fully understand the compositional properties of the human language, usually modeling an image caption as a “bag of words”. As a result, they perform poorly on compositional tasks, which require a deeper understanding... | Enver Sangineto, Fiorenzo Parascandolo, Lorenzo Baraldi, Nicholas Moratelli, Rita Cucchiara |  |
| 1116 |  |  [Mixture of Attentions For Speculative Decoding](https://openreview.net/forum?id=Rz0kozh3LE) |  | 0 | The growth in the number of parameters of Large Language Models (LLMs) has led to a significant surge in computational requirements, making them challenging and costly to deploy. Speculative decoding (SD) leverages smaller models to efficiently propose future tokens, which are then verified by the... | Gerasimos Lampouras, Haitham BouAmmar, Jun Wang, Matthieu Zimmer, Milan Gritta |  |
| 1117 |  |  [VL-Cache: Sparsity and Modality-Aware KV Cache Compression for Vision-Language Model Inference Acceleration](https://openreview.net/forum?id=HMrcv7Q4Ub) |  | 0 | Vision-Language Models (VLMs) have demonstrated impressive performance across a versatile set of tasks. A key challenge in accelerating VLMs is storing and accessing the large Key-Value (KV) cache that encodes long visual contexts, such as images or videos. While existing KV cache compression... | Danylo Vashchilenko, Dezhan Tu, Panpan Xu, Yuzhe Lu |  |
| 1118 |  |  [Connectome Mapping: Shape-Memory Network via Interpretation of Contextual Semantic Information](https://openreview.net/forum?id=PZYr22zFyE) |  | 0 | Contextual semantic information plays a pivotal role in the brain's visual interpretation of the surrounding environment. When processing visual information, electrical signals within synapses facilitate the dynamic activation and deactivation of synaptic connections, guided by the contextual... | Haeyun Lee, Jae Youn Hwang, Kyungsu Lee |  |
| 1119 |  |  [Learning Color Equivariant Representations](https://openreview.net/forum?id=IXyfbaGlps) |  | 0 | In this paper, we introduce group convolutional neural networks (GCNNs) equivariant to color variation. GCNNs have been designed for a variety of geometric transformations from 2D and 3D rotation groups, to semi-groups such as scale. Despite the improved interpretability, accuracy and... | Christine AllenBlanchette, Felix O'Mahony, Yulong Yang |  |
| 1120 |  |  [Omni-MATH: A Universal Olympiad Level Mathematic Benchmark for Large Language Models](https://openreview.net/forum?id=yaqPf0KAlN) |  | 0 | Recent advancements in large language models (LLMs) have led to significant breakthroughs in mathematical reasoning capabilities. However, existing benchmarks like GSM8K or MATH are now being solved with high accuracy (e.g., OpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for... | Baobao Chang, Benyou Wang, Bofei Gao, Chenghao Ma, Daoguang Zan, Feifan Song, Ge Zhang, Lei Li, Lei Sha, Liang Chen, Qingxiu Dong, Runxin Xu, Shanghaoran Quan, Tianyu Liu, Xuancheng Ren, Yibo Miao, Yichang Zhang, Zefan Cai, Zhe Yang, Zhengyang Tang |  |
| 1121 |  |  [Semantic Temporal Abstraction via Vision-Language Model Guidance for Efficient Reinforcement Learning](https://openreview.net/forum?id=zY37C8d6bS) |  | 0 | Extracting temporally extended skills can significantly improve the efficiency of reinforcement learning (RL) by breaking down complex decision-making problems with sparse rewards into simpler subtasks and enabling more effective credit assignment. However, existing abstraction methods either... | Lixuan Jin, Pengyuan Wang, Ruifeng Chen, TianShuo Liu, XuHui Liu, Yang Yu, Zhilong Zhang |  |
| 1122 |  |  [ScImage: How good are multimodal large language models at scientific text-to-image generation?](https://openreview.net/forum?id=ugyqNEOjoU) |  | 0 | Multimodal large language models (LLMs) have demonstrated impressive capabilities in generating high-quality images from textual instructions. However, their performance in generating scientific images—a critical application for accelerating scientific progress—remains underexplored. In this work,... | Fahimeh Moafian, Jonas Belouadi, Leixin Zhang, Steffen Eger, Weihe Zhai, Yinjie Cheng, Zhixue Zhao |  |
| 1123 |  |  [Utility-Directed Conformal Prediction: A Decision-Aware Framework for Actionable Uncertainty Quantification](https://openreview.net/forum?id=iOMnn1hSBO) |  | 0 | There is increasing interest in \`\`decision-focused" machine learning methods which train models to account for how their predictions are used in downstream optimization problems. Doing so can often improve performance on subsequent decision problems. However, current methods for uncertainty... | Bryan Wilder, Carlos Miguel Patiño, Eric Horvitz, Santiago CortesGomez, Steven Wu, Yewon Byun |  |
| 1124 |  |  [On Targeted Manipulation and Deception when Optimizing LLMs for User Feedback](https://openreview.net/forum?id=Wf2ndb8nhf) |  | 0 | As LLMs become more widely deployed, there is increasing interest in directly optimizing for feedback from end users (e.g. thumbs up) in addition to feedback from paid annotators. However, training to maximize human feedback creates a perverse incentive structure for the AI to resort to... | Adhyyan Narang, Anca D. Dragan, Brendan Murphy, Constantin Weisser, Marcus Williams, Micah Carroll |  |
| 1125 |  |  [Gaussian Differentially Private Human Faces Under a Face Radial Curve Representation](https://openreview.net/forum?id=K2Tqn8R9pu) |  | 0 | In this paper we consider the problem of releasing a Gaussian Differentially Private (GDP) 3D human face. The human face is a complex structure with many features and inherently tied to one's identity. Protecting this data, in a formally private way, is important yet challenging given the... | Aleksandra B. Slavkovic, Carlos J. Soto, Mark Shriver, Matthew Reimherr |  |
| 1126 |  |  [Diffusion Transformers for Tabular Data Time Series Generation](https://openreview.net/forum?id=bhOysNJvWm) |  | 0 | Tabular data generation has recently attracted a growing interest due to its different application scenarios. However, generating time series of tabular data, where each element of the series depends on the others, remains a largely unexplored domain. This gap is probably due to the difficulty of... | Enver Sangineto, Fabrizio Garuti, Lorenzo Forni, Rita Cucchiara, Simone Luetto |  |
| 1127 |  |  [Non-Equilibrium Dynamics of Hybrid Continuous-Discrete Ground-State Sampling](https://openreview.net/forum?id=BlSIKSPhfz) |  | 0 | We propose a general framework for a hybrid continuous-discrete algorithm that integrates continuous-time deterministic dynamics with Metropolis-Hastings (MH) steps to combine search dynamics that either preserve or break detailed balance. Our purpose is to study the non-equilibrium dynamics that... | Sam Reifenstein, Timothée G. Leleu |  |
| 1128 |  |  [The Belief State Transformer](https://openreview.net/forum?id=ThRMTCgpvo) |  | 0 | We introduce the "Belief State Transformer", a next-token predictor that takes both a prefix and suffix as inputs, with a novel objective of predicting both the next token for the prefix and the previous token for the suffix. The Belief State Transformer effectively learns to solve challenging... | Ada Langford, Alex Lamb, Dinesh Jayaraman, Edward S. Hu, Haoran Xu, John Langford, Kwangjun Ahn, Manan Tomar, Qinghua Liu |  |
| 1129 |  |  [Bridging the Data Provenance Gap Across Text, Speech, and Video](https://openreview.net/forum?id=G5DziesYxL) |  | 0 | Progress in AI is driven largely by the scale and quality of training data. Despite this, there is a deficit of empirical analysis examining the attributes of well-established datasets beyond text. In this work we conduct the largest and first-of-its-kind longitudinal audit across modalities ---... | Ahmad Mustafa Anis, An Dinh, Da Yin, Emad A. Alghamdi, Joanna Materzynska, Kun Qian, Kushagra Tiwary, Manan Dey, Manuel Cherep, Minnie Liang, Mohammed Hamdy, Naana ObengMarnu, Nayan Saxena, Nikhil Singh, Robert Mahari, Shayne Longpre, Shrestha Mohanty, Vu Minh Chien, William Brannon, Yizhi Li, et al. |  |
| 1130 |  |  [Free Hunch: Denoiser Covariance Estimation for Diffusion Models Without Extra Costs](https://openreview.net/forum?id=4JK2XMGUc8) |  | 0 | The covariance for clean data given a noisy observation is an important quantity in many training-free guided generation methods for diffusion models. Current methods require heavy test-time computation, altering the standard diffusion training process or denoiser architecture, or making heavy... | Arno Solin, Markus Heinonen, Severi Rissanen |  |
| 1131 |  |  [One Hundred Neural Networks and Brains Watching Videos: Lessons from Alignment](https://openreview.net/forum?id=LM4PYXBId5) |  | 0 | What can we learn from comparing video models to human brains, arguably the most efficient and effective video processing systems in existence? Our work takes a step towards answering this question by performing the first large-scale benchmarking of deep video models on representational alignment... | Cees G. M. Snoek, Christina Sartzetaki, Gemma Roig, Iris I. A. Groen |  |
| 1132 |  |  [Value-aligned Behavior Cloning for Offline Reinforcement Learning via Bi-level Optimization](https://openreview.net/forum?id=elTJBP7Fbv) |  | 0 | Offline reinforcement learning (RL) aims to optimize policies under pre-collected data, without requiring any further interactions with the environment. Derived from imitation learning, Behavior cloning (BC) is extensively utilized in offline RL for its simplicity and effectiveness. Although BC... | Hongkun Dou, Ning Gao, Xingyu Jiang, Xiuhui Zhang, Yue Deng |  |
| 1133 |  |  [Fugatto 1: Foundational Generative Audio Transformer Opus 1](https://openreview.net/forum?id=B2Fqu7Y2cd) |  | 0 | Fugatto is a versatile audio synthesis and transformation model capable of following free-form text instructions with optional audio inputs. While large language models (LLMs) trained with text on a simple next-token prediction objective can learn to infer instructions directly from the data,... | Alexander H. Liu, Arushi Goel, Aya Aljafari, Bryan Catanzaro, ChaoHan Huck Yang, João Felipe Santos, Kevin J. Shih, Rafael Valle, Rohan Badlani, Ryan Prenger, Sanggil Lee, Shuqi Dai, Siddharth Gururani, Sungwon Kim, Wei Ping, Zhifeng Kong |  |
| 1134 |  |  [Balancing Bias in Two-sided Markets for Fair Stable Matchings](https://openreview.net/forum?id=qykpnEWf2J) |  | 0 | The Balanced Stable Marriage (BSM) problem aims to find a stable matching in a two-sided market that minimizes the maximum dissatisfaction among two sides. The classical Deferred Acceptance algorithm merely produces an unfair stable marriage, providing optimal partners for one side while partially... | Leong Hou U, Panagiotis Karras, Siyuan Wu |  |
| 1135 |  |  [Addax: Utilizing Zeroth-Order Gradients to Improve Memory Efficiency and Performance of SGD for Fine-Tuning Language Models](https://openreview.net/forum?id=QhxjQOMdDF) |  | 0 | Fine-tuning language models (LMs) with the standard Adam optimizer often demands excessive memory, limiting accessibility. The \`\`in-place'' version of Stochastic Gradient Descent (IP-SGD) and Memory-Efficient Zeroth-order Optimizer (MeZO) have been proposed as solutions to improve memory... | Meisam Razaviyayn, Peilin Zhong, Vahab Mirrokni, Xinwei Zhang, Yuan Deng, Zeman Li |  |
| 1136 |  |  [Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for Large Language Models](https://openreview.net/forum?id=uZgK0tcPqd) |  | 0 | Advancements in Natural Language Processing (NLP), have led to the emergence of Large Language Models (LLMs) such as GPT, Llama, Claude, and Gemini, which excel across a range of tasks but require extensive fine-tuning to align their outputs with human expectations. A widely used method for... | Alexandros Karatzoglou, Carlos Segura, Ioannis Arapakis, Sergi Abadal, Ángela LópezCardona |  |
| 1137 |  |  [Revisiting Large-Scale Non-convex Distributionally Robust Optimization](https://openreview.net/forum?id=JYwVijuNA7) |  | 0 | Distributionally robust optimization (DRO) is a powerful technique to train robust machine learning models that perform well under distribution shifts. Compared with empirical risk minimization (ERM), DRO optimizes the expected loss under the worst-case distribution in an uncertainty set of... | Ashley PraterBennette, Lixin Shen, Qi Zhang, Shaofeng Zou, Simon Khan, Yi Zhou |  |
| 1138 |  |  [Beyond single neurons: population response geometry in digital twins of mouse visual cortex](https://openreview.net/forum?id=kSISSDUYFh) |  | 0 | Hierarchical visual processing is essential for cognitive functions like object recognition and spatial localization. Traditional studies of the neural basis of these computations have focused on single-neuron activity, but recent advances in large-scale neural recordings emphasize the growing need... | Alessandro Marin Vargas, Alessandro Sanzeni, Dario Liscai, Emanuele Luconi |  |
| 1139 |  |  [State Space Models are Provably Comparable to Transformers in Dynamic Token Selection](https://openreview.net/forum?id=QFgbJOYJSE) |  | 0 | Deep neural networks based on state space models (SSMs) are attracting significant attention in sequence modeling since their computational cost is much smaller than that of Transformers. While the capabilities of SSMs have been demonstrated through experiments in various tasks, theoretical... | Naoki Nishikawa, Taiji Suzuki |  |
| 1140 |  |  [Consistency Models Made Easy](https://openreview.net/forum?id=xQVxo9dSID) |  | 0 | Consistency models (CMs) offer faster sampling than traditional diffusion models, but their training is resource-intensive. For example, as of 2024, training a state-of-the-art CM on CIFAR-10 takes one week on 8 GPUs. In this work, we propose an effective scheme for training CMs that largely... | Ashwini Pokle, J. Zico Kolter, Justin Lin, Weijian Luo, Zhengyang Geng |  |
| 1141 |  |  [OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data](https://openreview.net/forum?id=mTCbq2QssD) |  | 0 | Mathematical reasoning continues to be a critical challenge in large language model (LLM) development with significant interest. However, most of the cutting-edge progress in mathematical reasoning with LLMs has become closed-source due to lack of access to training data. This lack of data access... | Alexan Ayrapetyan, Branislav Kisacanin, Igor Gitman, Ivan Moshkov, Shubham Toshniwal, Wei Du |  |
| 1142 |  |  [MAPS: Advancing Multi-Modal Reasoning in Expert-Level Physical Science](https://openreview.net/forum?id=GR0y0F3Ipd) |  | 0 | Pre-trained on extensive text and image corpora, current Multi-Modal Large Language Models (MLLM) have shown strong capabilities in general visual reasoning tasks. However, their performance is still lacking in physical domains that require understanding diagrams with complex physical structures... | Erle Zhu, Hongning Wang, Jin Zhou, Minlie Huang, Xinjie Yu, Xujun Li, Yadi Liu, Zhe Zhang |  |
| 1143 |  |  [MAESTRO: Masked Encoding Set Transformer with Self-Distillation](https://openreview.net/forum?id=FEZOLWexPb) |  | 0 | The interrogation of cellular states and interactions in immunology research is an ever-evolving task, requiring adaptation to the current levels of high dimensionality. Cytometry enables high-dimensional profiling of immune cells, but its analysis is hindered by the complexity and variability of... | Ajinkya Pattekar, Allison R. Greenplate, Amit BarOr, Benjamin A Fensterheim, Benjamin A. Abramoff, Damian Maseda, Divij Mathew, Irene Khavin, Jaesik Kim, Jonghyun Lee, Matei Ionita, Matthew Eric Lee, Michelle L. McKeague, Rennie L. Rhee, Shwetank, Sokratis Apostolidis, Victoria Fang, Yidi Huang, Yonghyun Nam, Zahabia Rangwala, et al. |  |
| 1144 |  |  [AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out Context Attribution](https://openreview.net/forum?id=9kJperA2a4) |  | 0 | The influence of contextual input on the behavior of large language models (LLMs) has prompted the development of context attribution methods that aim to quantify each context span's effect on an LLM's generations. The leave-one-out (LOO) error, which measures the change in the likelihood of the... | Colin Raffel, Fengyuan Liu, Nikhil Kandpal |  |
| 1145 |  |  [Provably Safeguarding a Classifier from OOD and Adversarial Samples](https://openreview.net/forum?id=kwCHcaeHrf) |  | 0 | This paper aims to transform a trained classifier into an abstaining classifier, such that the latter is provably protected from out-of-distribution and adversarial samples. The proposed Sample-efficient Probabilistic Detection using Extreme Value Theory (SPADE) approach relies on a Generalized... | Christophe Labreuche, Johanne Cohen, Michèle Sebag, Nicolas Atienza |  |
| 1146 |  |  [Learning Chaos In A Linear Way](https://openreview.net/forum?id=Llh6CinTiy) |  | 0 | Learning long-term behaviors in chaotic dynamical systems, such as turbulent flows and climate modelling, is challenging due to their inherent instability and unpredictability. These systems exhibit positive Lyapunov exponents, which significantly hinder accurate long-term forecasting. As a result,... | Daniel Giles, Sibo Cheng, Xiao Xue, Xiaohang Tang, Xiaoyuan Cheng, Yi He, Yiming Yang, Yukun Hu |  |
| 1147 |  |  [Data Taggants: Dataset Ownership Verification Via Harmless Targeted Data Poisoning](https://openreview.net/forum?id=6ldD8Y4gBQ) |  | 0 | Dataset ownership verification, the process of determining if a dataset is used in a model's training data, is necessary for detecting unauthorized data usage and data contamination. Existing approaches, such as backdoor watermarking, rely on inducing a detectable behavior into the trained model on... | ElMahdi ElMhamdi, Nicolas Usunier, Wassim Bouaziz |  |
| 1148 |  |  [Do Contemporary Causal Inference Models Capture Real-World Heterogeneity? Findings from a Large-Scale Benchmark](https://openreview.net/forum?id=Q2bJ2qgcP1) |  | 0 | We present unexpected findings from a large-scale benchmark study evaluating Conditional Average Treatment Effect (CATE) estimation algorithms. By running 16 modern CATE models across 43,200 datasets, we find that: (a) 62\% of CATE estimates have a higher Mean Squared Error (MSE) than a trivial... | Haining Yu, Yizhou Sun |  |
| 1149 |  |  [Harnessing Webpage UIs for Text-Rich Visual Understanding](https://openreview.net/forum?id=IIsTO4P3Ag) |  | 0 | Text-rich visual understanding—the ability to interpret both textual content and visual elements within a scene—is crucial for multimodal large language models (MLLMs) to effectively interact with structured environments. We propose leveraging webpage UIs as a naturally structured and diverse data... | Chenyan Xiong, Graham Neubig, Junpeng Liu, Tianyue Ou, Wai Lam, Wenhu Chen, Xiang Yue, Yifan Song, Yuxiao Qu |  |
| 1150 |  |  [Fast unsupervised ground metric learning with tree-Wasserstein distance](https://openreview.net/forum?id=FBhKUXK7od) |  | 0 | The performance of unsupervised methods such as clustering depends on the choice of distance metric between features, or ground metric. Commonly, ground metrics are decided with heuristics or learned via supervised algorithms. However, since many interesting datasets are unlabelled, unsupervised... | Kira Michaela Düsterwald, Makoto Yamada, Samo Hromadka |  |
| 1151 |  |  [World Model on Million-Length Video And Language With Blockwise RingAttention](https://openreview.net/forum?id=HN8V0flwJF) |  | 0 | Enabling long-context understanding remains a key challenge in scaling existing sequence models -- a crucial component in developing generally intelligent models that can process and operate over long temporal horizons that potentially consist of millions of tokens. In this paper, we aim to address... | Hao Liu, Matei Zaharia, Pieter Abbeel, Wilson Yan |  |
| 1152 |  |  [Quamba: A Post-Training Quantization Recipe for Selective State Space Models](https://openreview.net/forum?id=mnna9LUg7P) |  | 0 | State Space Models (SSMs) have emerged as an appealing alternative to Transformers for large language models, achieving state-of-the-art accuracy with constant memory complexity which allows for holding longer context lengths than attention-based networks. The superior computational efficiency of... | ChiChih Chang, Diana Marculescu, HungYueh Chiang, KaiChiang Wu, Natalia Frumkin |  |
| 1153 |  |  [Learning to engineer protein flexibility](https://openreview.net/forum?id=L238BAx0wP) |  | 0 | Generative machine learning models are increasingly being used to design novel proteins. However, their major limitation is the inability to account for protein flexibility, a property crucial for protein function. Learning to engineer flexibility is difficult because the relevant data is scarce,... | Jirí Damborský, Jirí Sedlár, Joan PlanasIglesias, Josef Sivic, Petr Kouba, Stanislav Mazurenko |  |
| 1154 |  |  [ElasticTok: Adaptive Tokenization for Image and Video](https://openreview.net/forum?id=tFV5GrWOGm) |  | 0 | Efficient video tokenization remains a key bottleneck in learning general purpose vision models that are capable of processing long video sequences. Prevailing approaches are restricted to encoding videos to a fixed number of tokens, where too few tokens will result in overly lossy encodings, and... | Aleksandra Faust, Hao Liu, Matei Zaharia, Pieter Abbeel, Volodymyr Mnih, Wilson Yan |  |
| 1155 |  |  [Trajectory-Class-Aware Multi-Agent Reinforcement Learning](https://openreview.net/forum?id=uqe5HkjbT9) |  | 0 | In the context of multi-agent reinforcement learning, \*generalization\* is a challenge to solve various tasks that may require different joint policies or coordination without relying on policies specialized for each task. We refer to this type of problem as a \*multi-task\*, and we train agents... | Hyungho Na, IlChul Moon, Kwanghyeon Lee, Sumin Lee |  |
| 1156 |  |  [Revisit the Open Nature of Open Vocabulary Semantic Segmentation](https://openreview.net/forum?id=2vHIHrJAcI) |  | 0 | In Open Vocabulary Semantic Segmentation (OVS), we observe a consistent drop in model performance as the query vocabulary set expands, especially when it includes semantically similar and ambiguous vocabularies, such as ‘sofa’ and ‘couch’. The previous OVS evaluation protocol, however, does not... | Han Hu, Jianbo Jiao, Qiming Huang |  |
| 1157 |  |  [Robust Barycenter Estimation using Semi-Unbalanced Neural Optimal Transport](https://openreview.net/forum?id=CI5Cj0vktS) |  | 0 | Aggregating data from multiple sources can be formalized as an \*Optimal Transport\* (OT) barycenter problem, which seeks to compute the average of probability distributions with respect to OT discrepancies. However, in real-world scenarios, the presence of outliers and noise in the data measures... | Alexander Kolesov, Alexander Korotin, Jaemoo Choi, Jaewoong Choi, Milena Gazdieva, Petr Mokrov |  |
| 1158 |  |  [Inverse decision-making using neural amortized Bayesian actors](https://openreview.net/forum?id=zxO4WuVGns) |  | 0 | Bayesian observer and actor models have provided normative explanations for many behavioral phenomena in perception, sensorimotor control, and other areas of cognitive science and neuroscience. They attribute behavioral variability and biases to interpretable entities such as perceptual and motor... | Constantin A. Rothkopf, Dominik Straub, Jan Peters, Tobias F. Niehues |  |
| 1159 |  |  [Reframing Structure-Based Drug Design Model Evaluation via Metrics Correlated to Practical Needs](https://openreview.net/forum?id=RyWypcIMiE) |  | 0 | Recent advances in structure-based drug design (SBDD) have produced surprising results, with models often generating molecules that achieve better Vina docking scores than actual ligands. However, these results are frequently overly optimistic due to the limitations of docking score accuracy and... | Bowen Gao, Haichuan Tan, Minsi Ren, WeiYing Ma, Xiao Huang, YaQin Zhang, Yanwen Huang, Yanyan Lan |  |
| 1160 |  |  [Valid Conformal Prediction for Dynamic GNNs](https://openreview.net/forum?id=i3T0wvQDKg) |  | 0 | Dynamic graphs provide a flexible data abstraction for modelling many sorts of real-world systems, such as transport, trade, and social networks. Graph neural networks (GNNs) are powerful tools allowing for different kinds of prediction and inference on these systems, but getting a handle on... | Daniel John Lawson, Ed Davis, Ian Gallagher, Patrick RubinDelanchy |  |
| 1161 |  |  [ContextGNN: Beyond Two-Tower Recommendation Systems](https://openreview.net/forum?id=nzOD1we8Z4) |  | 0 | Recommendation systems predominantly utilize two-tower architectures, which evaluate user-item rankings through the inner product of their respective embeddings. However, one key limitation of two-tower models is that they learn a pair-agnostic representation of users and items. In contrast,... | Akihiro Nitta, Blaz Stojanovic, Jan Eric Lenssen, Jure Leskovec, Manan Shah, Matthias Fey, Shenyang Huang, Weihua Hu, Xinwei He, Yiwen Yuan, Zecheng Zhang |  |
| 1162 |  |  [KOR-Bench: Benchmarking Language Models on Knowledge-Orthogonal Reasoning Tasks](https://openreview.net/forum?id=SVRRQ8goQo) |  | 0 | In this paper, we introduce Knowledge-Orthogonal Reasoning (KOR), a concept aimed at minimizing reliance on domain-specific knowledge, enabling more accurate evaluation of models' reasoning abilities in out-of-distribution settings. Based on this concept, we propose the Knowledge-Orthogonal... | Ge Zhang, Haoran Zhang, Jiaheng Liu, Jian Yang, Kaijing Ma, Minghao Liu, Wenhao Huang, Xeron Du, Xiang Yue, Xingwei Qu, Yunran Wang, Zhoufutu Wen |  |
| 1163 |  |  [Gaussian Splatting Lucas-Kanade](https://openreview.net/forum?id=dkrEoT68by) |  | 0 | Gaussian Splatting and its dynamic extensions are effective for reconstructing 3D scenes from 2D images when there is significant camera movement to facilitate motion parallax and when scene objects remain relatively static. However, in many real-world scenarios, these conditions are not met. As a... | Joel Julin, Koichiro Niinuma, Liuyue Xie, László Attila Jeni |  |
| 1164 |  |  [Beyond Content Relevance: Evaluating Instruction Following in Retrieval Models](https://openreview.net/forum?id=OlRjxSuSwl) |  | 0 | Instruction-following capabilities in large language models (LLMs) have progressed significantly, enabling more complex user interactions through detailed prompts. However, retrieval systems have not matched these advances, most of them still relies on traditional lexical and semantic matching... | Jianqun Zhou, Qianqian Zheng, Rui Meng, Wei Chen, Wei Zhang, Xiaoyu Shen, Yuanlei Zheng, Zeyuan Shang |  |
| 1165 |  |  [RobustKV: Defending Large Language Models against Jailbreak Attacks via KV Eviction](https://openreview.net/forum?id=L5godAOC2z) |  | 0 | Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful queries within adversarial prompts. While most existing defenses attempt to mitigate the effects of adversarial prompts, they often prove inadequate as adversarial prompts can take arbitrary, adaptive forms. This paper... | Changjiang Li, Jiacheng Liang, Tanqiu Jiang, Ting Wang, Yuhui Wang, Zian Wang |  |
| 1166 |  |  [From Probability to Counterfactuals: the Increasing Complexity of Satisfiability in Pearl's Causal Hierarchy](https://openreview.net/forum?id=rvvSSmGIFS) |  | 0 | The framework of Pearl's Causal Hierarchy (PCH) formalizes three types of reasoning: probabilistic (i.e. purely observational), interventional, and counterfactual, that reflect the progressive sophistication of human thought regarding causation. We investigate the computational complexity aspects... | Benito van der Zander, Julian Dörfler, Maciej Liskiewicz, Markus Bläser |  |
| 1167 |  |  [High-Dimensional Bayesian Optimisation with Gaussian Process Prior Variational Autoencoders](https://openreview.net/forum?id=SIuD7CySb4) |  | 0 | Bayesian optimisation (BO) using a Gaussian process (GP)-based surrogate model is a powerful tool for solving black-box optimisation problems but does not scale well to high-dimensional data. Previous works have proposed to use variational autoencoders (VAEs) to project high-dimensional data onto a... | Harri Lähdesmäki, Manuel Haussmann, Siddharth Ramchandran |  |
| 1168 |  |  [LIFe-GoM: Generalizable Human Rendering with Learned Iterative Feedback Over Multi-Resolution Gaussians-on-Mesh](https://openreview.net/forum?id=gY08Ou8EL7) |  | 0 | Generalizable rendering of an animatable human avatar from sparse inputs relies on data priors and inductive biases extracted from training on large data to avoid scene-specific optimization and to enable fast reconstruction. This raises two main challenges: First, unlike iterative gradient-based... | Alexander G. Schwing, Jing Wen, Shenlong Wang |  |
| 1169 |  |  [NNsight and NDIF: Democratizing Access to Open-Weight Foundation Model Internals](https://openreview.net/forum?id=MxbEiFRf39) |  | 0 | We introduce NNsight and NDIF, technologies that work in tandem to enable scientific study of the representations and computations learned by very large neural networks. NNsight is an open-source system that extends PyTorch to introduce deferred remote execution. The National Deep Inference Fabric... | Aaron Mueller, Adam Belfki, Alexander Russell Loftus, Arjun Guha, Arnab Sen Sharma, Byron C. Wallace, Caden Juang, Can Rager, Carla E. Brodley, David Bau, Dmitrii Troitskii, Eric Todd, Francesca Lucchetti, Jaden Fried FiottoKaufman, Jannik Brinkmann, Jonathan Bell, Koyena Pal, Michael Ripa, Nikhil Prakash, Samuel Marks |  |
| 1170 |  |  [Protecting against simultaneous data poisoning attacks](https://openreview.net/forum?id=rK0YJwL69S) |  | 0 | Current backdoor defense methods are evaluated against a single attack at a time. This is unrealistic, as powerful machine learning systems are trained on large datasets scraped from the internet, which may be attacked multiple times by one or more attackers. We demonstrate that multiple backdoors... | Amartya Sanyal, David Krueger, Neel Alex, Shoaib Ahmed Siddiqui |  |
| 1171 |  |  [Lift Your Molecules: Molecular Graph Generation in Latent Euclidean Space](https://openreview.net/forum?id=uNomADvF3s) |  | 0 | We introduce a new framework for 2D molecular graph generation using 3D molecule generative models. Our Synthetic Coordinate Embedding (SyCo) framework maps 2D molecular graphs to 3D Euclidean point clouds via synthetic coordinates and learns the inverse map using an E($n$)-Equivariant Graph Neural... | Johanna Sommer, Mohamed Amine Ketata, Nicholas Gao, Stephan Günnemann, Tom Wollschläger |  |
| 1172 |  |  [Neural Sampling from Boltzmann Densities: Fisher-Rao Curves in the Wasserstein Geometry](https://openreview.net/forum?id=TUvg5uwdeG) |  | 0 | We deal with the task of sampling from an unnormalized Boltzmann density $\rho_D$ by learning a Boltzmann curve given by energies $f_t$ starting in a simple density $\rho_Z$. First, we examine conditions under which Fisher-Rao flows are absolutely continuous in the Wasserstein geometry. Second, we... | Christian Wald, Gabriele Steidl, Jannis Chemseddine, Richard Duong |  |
| 1173 |  |  [GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation](https://openreview.net/forum?id=hPWWXpCaJ7) |  | 0 | With the rapid development of embodied artificial intelligence, significant progress has been made in vision-language-action (VLA) models for general robot decision-making. However, the majority of existing VLAs fail to account for the inevitable external perturbations encountered during... | Donglin Wang, Hongyin Zhang, Pengxiang Ding, Shangke Lyu, Ying Peng |  |
| 1174 |  |  [Revealing and Mitigating Over-Attention in Knowledge Editing](https://openreview.net/forum?id=4l3AH8Bhmt) |  | 0 | Large Language Models~(LLMs) have demonstrated superior performance across a wide range of tasks, but they still exhibit undesirable errors due to incorrect knowledge learned from the training data. To avoid this, knowledge editing methods emerged to precisely edit the specific model knowledge via... | Juntao Li, Keyan Zhou, Min Zhang, Pinzheng Wang, Qiaoming Zhu, Zecheng Tang |  |
| 1175 |  |  [A Generalist Hanabi Agent](https://openreview.net/forum?id=pCj2sLNoJq) |  | 0 | Traditional multi-agent reinforcement learning (MARL) systems can develop cooperative strategies through repeated interactions. However, these systems are unable to perform well on any other setting than the one they have been trained on, and struggle to successfully cooperate with unfamiliar... | Arjun Vaithilingam Sudhakar, Hadi Nekoei, Janarthanan Rajendran, Mathieu Reymond, Miao Liu, Sarath Chandar |  |
| 1176 |  |  [GraphEval: A Lightweight Graph-Based LLM Framework for Idea Evaluation](https://openreview.net/forum?id=5RUM1aIdok) |  | 0 | The powerful capabilities of Large Language Models (LLMs) have led to their growing use in evaluating human-generated content, particularly in evaluating research ideas within academic settings. Existing solutions primarily rely on prompt-based LLM methods or fine-tuned lightweight language models... | Jiaxuan You, Tao Feng, Yihang Sun |  |
| 1177 |  |  [Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying Questions](https://openreview.net/forum?id=cwuSAR7EKd) |  | 0 | Large language models (LLMs) must often respond to highly ambiguous user requests. In such cases, the LLM's best response may be to ask a clarifying question to elicit more information. Existing LLMs often respond by presupposing a single interpretation of such ambiguous requests, frustrating users... | Eunsol Choi, Michael Jq Zhang, W. Bradley Knox |  |
| 1178 |  |  [Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs](https://openreview.net/forum?id=1ExfUpmIW4) |  | 0 | Large Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, this poses risk of privacy and copyright violations, highlighting the need for efficient machine unlearning methods that remove sensitive data without... | Dasol Hwang, Moontae Lee, Sungjun Cho, Sungmin Cha |  |
| 1179 |  |  [What Makes Large Language Models Reason in (Multi-Turn) Code Generation?](https://openreview.net/forum?id=Zk9guOl9NS) |  | 0 | Prompting techniques such as chain-of-thought have established themselves as a popular vehicle for improving the outputs of large language models (LLMs). For code generation, however, their exact mechanics and efficacy are under-explored using unified metrics and benchmarks. We thus investigate the... | Benjamin NéXuanjinggrevergne, Gabriel Synnaeve, Jonas Gehring, Juliette Decugis, Kunhao Zheng, Taco Cohen |  |
| 1180 |  |  [RMB: Comprehensively benchmarking reward models in LLM alignment](https://openreview.net/forum?id=kmgrlG9TR0) |  | 0 | Reward models (RMs) guide the alignment of large language models (LLMs), steering them toward behaviors preferred by humans. Evaluating RMs is the key to better aligning LLMs. However, the current evaluation of RMs may not directly correspond to their alignment performance due to the limited... | Binghai Wang, Enyu Zhou, Guodong Zheng, Jessica Fan, Limao Xiong, Qi Zhang, Rong Bao, Rui Zheng, Shihan Dou, Tao Gui, Wei Shen, Xuanjing Huang, Yurong Mou, Zhiheng Xi |  |
| 1181 |  |  [FedTMOS: Efficient One-Shot Federated Learning with Tsetlin Machine](https://openreview.net/forum?id=44hcrfzydU) |  | 0 | One-Shot Federated Learning (OFL) is a promising approach that reduce communication to a single round, minimizing latency and resource consumption. However, existing OFL methods often rely on Knowledge Distillation, which introduce server-side training, increasing latency. While neuron matching and... | Geoff V. Merrett, Jagmohan Chauhan, Jonathon S. Hare, Shannon How Shi Qi |  |
| 1182 |  |  [From Sparse Dependence to Sparse Attention: Unveiling How Chain-of-Thought Enhances Transformer Sample Efficiency](https://openreview.net/forum?id=AmEgWDhmTr) |  | 0 | Chain-of-thought (CoT) significantly enhances the reasoning performance of large language models (LLM). While current theoretical studies often attribute this improvement to increased expressiveness and computational capacity, we argue that expressiveness is not the primary limitation in the LLM... | Hongzhou Lin, Huaqing Zhang, Jingzhao Zhang, Kaiyue Wen |  |
| 1183 |  |  [CS-Bench: A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery](https://openreview.net/forum?id=fjEZ2LPceZ) |  | 0 | Large language models (LLMs) have demonstrated significant potential in advancing various fields of research and society. However, the current community of LLMs overly focuses on benchmarks for analyzing specific foundational skills (e.g. mathematics and code generation), neglecting an all-round... | Bin Liang, Dayuan Fu, Guanting Dong, Huangxuan Wu, Jianing Yu, Muxi Diao, Qiuna Tan, Runqi Qiao, Weihao Zeng, Weiran Xu, Xiaoshuai Song, Yejie Wang, Yujia Fu, Zhengyang Wang, Zhexu Wang, Zhuoma Gongque |  |
| 1184 |  |  [A deep inverse-mapping model for a flapping robotic wing](https://openreview.net/forum?id=254NJe9JEw) |  | 0 | In systems control, the dynamics of a system are governed by modulating its inputs to achieve a desired outcome. For example, to control the thrust of a quad-copter propeller the controller modulates its rotation rate, relying on a straightforward mapping between the input rotation rate and the... | Hadar Sharvit, Raz Karl, Tsevi Beatus |  |
| 1185 |  |  [Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal Perspective](https://openreview.net/forum?id=NWb128pSCb) |  | 0 | Accurate interpretation and visualization of human instructions are crucial for text-to-image (T2I) synthesis. However, current models struggle to capture semantic variations from word order changes, and existing evaluations, relying on indirect metrics like text-image similarity, fail to reliably... | Bei Yang, Chengyu Wang, Jun Huang, Penglei Sun, Xiangru Zhu, Xiaoxiao Xu, Yanghua Xiao, Yaoxian Song, Zhixu Li |  |
| 1186 |  |  [GraphRouter: A Graph-based Router for LLM Selections](https://openreview.net/forum?id=eU39PDsZtT) |  | 0 | The rapidly growing number and variety of Large Language Models (LLMs) present significant challenges in efficiently selecting the appropriate LLM for a given query, especially considering the trade-offs between performance and computational cost. Current LLM selection methods often struggle to... | Jiaxuan You, Tao Feng, Yanzhen Shen |  |
| 1187 |  |  [CONDA: Adaptive Concept Bottleneck for Foundation Models Under Distribution Shifts](https://openreview.net/forum?id=8sfc8MwG5v) |  | 0 | Advancements in foundation models (FMs) have led to a paradigm shift in machine learning. The rich, expressive feature representations from these pre-trained, large- scale FMs are leveraged for multiple downstream tasks, usually via lightweight fine-tuning of a shallow fully-connected network... | Jayaram Raghuram, Jihye Choi, Somesh Jha, Yixuan Li |  |
| 1188 |  |  [Offline RL in Regular Decision Processes: Sample Efficiency via Language Metrics](https://openreview.net/forum?id=EW6bNEqalF) |  | 0 | This work studies offline Reinforcement Learning (RL) in a class of non-Markovian environments called Regular Decision Processes (RDPs). In RDPs, the unknown dependency of future observations and rewards from the past interactions can be captured by some hidden finite-state automaton. For this... | Ahana Deb, Alessandro Ronca, Anders Jonsson, Mohammad Sadegh Talebi, Roberto Cipollone |  |
| 1189 |  |  [Think while You Generate: Discrete Diffusion with Planned Denoising](https://openreview.net/forum?id=MJNywBdSDy) |  | 0 | Discrete diffusion has achieved state-of-the-art performance, outperforming or approaching autoregressive models on standard benchmarks. In this work, we introduce \*Discrete Diffusion with Planned Denoising\* (DDPD), a novel framework that separates the generation process into two models: a... | Andrew Campbell, Hannes Stärk, Juno Nam, Rafael GómezBombarelli, Sulin Liu, Tommi S. Jaakkola, Yilun Xu |  |
| 1190 |  |  [Complexity Lower Bounds of Adaptive Gradient Algorithms for Non-convex Stochastic Optimization under Relaxed Smoothness](https://openreview.net/forum?id=ZjOXuAfS6l) |  | 0 | Recent results in non-convex stochastic optimization demonstrate the convergence of popular adaptive algorithms (e.g., AdaGrad) under the $(L_0, L_1)$-smoothness condition, but the rate of convergence is a higher-order polynomial in terms of problem parameters like the smoothness constants. The... | Michael Crawshaw, Mingrui Liu |  |
| 1191 |  |  [Scaling Transformers for Low-Bitrate High-Quality Speech Coding](https://openreview.net/forum?id=4YpMrGfldX) |  | 0 | The tokenization of audio with neural audio codec models is a vital part of modern AI pipelines for the generation or understanding of speech, alone or in a multimodal context. Traditionally such tokenization models have concentrated on low parameter-count architectures using only components with... | Anton Smirnov, CJ Carr, Jordi Pons, Julian D. Parker, Xubo Liu, Zach Evans, Zack Zukowski |  |
| 1192 |  |  [Learning Geometric Reasoning Networks For Robot Task And Motion Planning](https://openreview.net/forum?id=ajxAJ8GUX4) |  | 0 | Task and Motion Planning (TAMP) is a computationally challenging robotics problem due to the tight coupling of discrete symbolic planning and continuous geometric planning of robot motions. In particular, planning manipulation tasks in complex 3D environments leads to a large number of costly... | Rachid Alami, Smail Ait Bouhsain, Thierry Siméon |  |
| 1193 |  |  [Differentially Private Steering for Large Language Model Alignment](https://openreview.net/forum?id=lLkgj7FEtZ) |  | 0 | Aligning Large Language Models (LLMs) with human values and away from undesirable behaviors (such as hallucination) has become increasingly important. Recently, steering LLMs towards a desired behavior via activation editing has emerged as an effective method to mitigate harmful generations at... | Amartya Sanyal, Anmol Goel, Iryna Gurevych, Yaxi Hu |  |
| 1194 |  |  [Mixture of Parrots: Experts improve memorization more than reasoning](https://openreview.net/forum?id=9XETcRsufZ) |  | 0 | The Mixture-of-Experts (MoE) architecture enables a significant increase in the total number of model parameters with minimal computational overhead. However, it is not clear what performance tradeoffs, if any, exist between MoEs and standard dense transformers. In this paper, we show that as we... | Alex Gu, Clara Mohri, David AlvarezMelis, David Brandfonbrener, Eran Malach, Nikhil Anand, Nikhil Vyas, Samy Jelassi, Sham M. Kakade, Yuanzhi Li |  |
| 1195 |  |  [Efficient Multi-agent Offline Coordination via Diffusion-based Trajectory Stitching](https://openreview.net/forum?id=EpnZEzYDUT) |  | 0 | Learning from offline data without interacting with the environment is a promising way to fully leverage the intelligent decision-making capabilities of multi-agent reinforcement learning (MARL). Previous approaches have primarily focused on developing learning techniques, such as conservative... | Cong Guan, Lei Yuan, Lihe Li, Yang Yu, Yuqi Bian, Ziqian Zhang |  |
| 1196 |  |  [AdvWave: Stealthy Adversarial Jailbreak Attack against Large Audio-Language Models](https://openreview.net/forum?id=0BujOfTqab) |  | 0 | Recent advancements in large audio-language models (LALMs) have enabled speech-based user interactions, significantly enhancing user experience and accelerating the deployment of LALMs in real-world applications. However, ensuring the safety of LALMs is crucial to prevent risky outputs that may... | Bo Li, Chejian Xu, Mintong Kang |  |
| 1197 |  |  [Scaling up Masked Diffusion Models on Text](https://openreview.net/forum?id=WNvvwK0tut) |  | 0 | Masked diffusion models (MDMs) have shown promise in language modeling, yet their scalability and effectiveness in core language tasks, such as text generation and language understanding, remain underexplored. This paper establishes the first scaling law for MDMs, demonstrating a scaling rate... | Chao Du, Chongxuan Li, Fengqi Zhu, Guangtao Zeng, Min Lin, Qian Liu, Shen Nie, Tianyu Pang |  |
| 1198 |  |  [LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations](https://openreview.net/forum?id=KRnsX5Em3W) |  | 0 | Large language models (LLMs) often produce errors, including factual inaccuracies, biases, and reasoning failures, collectively referred to as "hallucinations". Recent studies have demonstrated that LLMs' internal states encode information regarding the truthfulness of their outputs, and that this... | Hadas Kotek, Hadas Orgad, Idan Szpektor, Michael Toker, Roi Reichart, Yonatan Belinkov, Zorik Gekhman |  |
| 1199 |  |  [Selective Unlearning via Representation Erasure Using Domain Adversarial Training](https://openreview.net/forum?id=KzSGJy1PIf) |  | 0 | When deploying machine learning models in the real world, we often face the challenge of “unlearning” specific data points or subsets after training. Inspired by Domain-Adversarial Training of Neural Networks (DANN), we propose a novel algorithm,SURE, for targeted unlearning.SURE treats the process... | Daniel M. Roy, Doina Precup, Eleni Triantafillou, Gintare Karolina Dziugaite, Hugo Larochelle, James J. Clark, Nazanin Mohammadi Sepahvand |  |
| 1200 |  |  [Local Steps Speed Up Local GD for Heterogeneous Distributed Logistic Regression](https://openreview.net/forum?id=lydPkW4lfz) |  | 0 | We analyze two variants of Local Gradient Descent applied to distributed logistic regression with heterogeneous, separable data and show convergence at the rate $O(1/KR)$ for $K$ local steps and sufficiently large $R$ communication rounds. In contrast, all existing convergence guarantees for Local... | Blake Woodworth, Michael Crawshaw, Mingrui Liu |  |
| 1201 |  |  [Robustness Auditing for Linear Regression: To Singularity and Beyond](https://openreview.net/forum?id=V5ns6uvRZ9) |  | 0 | It has recently been discovered that the conclusions of many highly influential econometrics studies can be overturned by removing a very small fraction of their samples (often less than $0.5\%$). These conclusions are typically based on the results of one or more Ordinary Least Squares (OLS)... | Ittai Rubinstein, Samuel B. Hopkins |  |
| 1202 |  |  [Can Transformers Do Enumerative Geometry?](https://openreview.net/forum?id=4X9RpKH4Ls) |  | 0 | We introduce a Transformer-based approach to computational enumerative geometry, specifically targeting the computation of $\psi$-class intersection numbers on the moduli space of curves. Traditional methods for calculating these numbers suffer from factorial computational complexity, making them... | Alessandro Giacchetto, Baran Hashemi, Roderic Guigo Corominas |  |
| 1203 |  |  [Large Language Models Assume People are More Rational than We Really are](https://openreview.net/forum?id=dAeET8gxqg) |  | 0 | In order for AI systems to communicate effectively with people, they must understand how we make decisions. However, people's decisions are not always rational, so the implicit internal models of human decision-making in Large Language Models (LLMs) must account for this. Previous empirical... | Ilia Sucholutsky, Jiayi Geng, Joshua C. Peterson, Ryan Liu, Thomas L. Griffiths |  |
| 1204 |  |  [Small-to-Large Generalization: Training Data Influences Models Consistently Across Scale](https://openreview.net/forum?id=79ZkWgY2FI) |  | 0 | Choice of training data distribution greatly influences model behavior. Yet, in large-scale settings, precisely characterizing \*how\* changes in training data affects predictions is often difficult due to model training costs. Current practice is to instead extrapolate from scaled down,... | Alaa Khaddaj, Aleksander Madry, Logan Engstrom |  |
| 1205 |  |  [Scaling Instruction-tuned LLMs to Million-token Contexts via Hierarchical Synthetic Data Generation](https://openreview.net/forum?id=BkwCrIsTbR) |  | 0 | Large Language Models (LLMs) struggle with long-context reasoning, not only due to the quadratic scaling of computational complexity with sequence length but also because of the scarcity and expense of annotating long-context data. There has been barely any open-source work that systematically... | Ben Athiwaratkun, Ce Zhang, Jue Wang, Linda He, Maurice Weber, Shang Zhu |  |
| 1206 |  |  [EqNIO: Subequivariant Neural Inertial Odometry](https://openreview.net/forum?id=C8jXEugWkq) |  | 0 | Neural network-based odometry using accelerometer and gyroscope readings from a single IMU can achieve robust, and low-drift localization capabilities, through the use of _neural displacement priors (NDPs)_. These priors learn to produce denoised displacement measurements but need to ignore data... | Daniel Gehrig, Evangelos Chatzipantazis, Kostas Daniilidis, Royina Karegoudra Jayanth, Yinshuang Xu, Ziyun Wang |  |
| 1207 |  |  [MANTRA: The Manifold Triangulations Assemblage](https://openreview.net/forum?id=X6y5CC44HM) |  | 0 | The rising interest in leveraging higher-order interactions present in complex systems has led to a surge in more expressive models exploiting higher-order structures in the data, especially in topological deep learning (TDL), which designs neural networks on higher-order domains such as simplicial... | Bastian Rieck, Carles Casacuberta, Daniel Bin Schmid, Ernst Röell, Mathieu Alain, Rubén Ballester, Sergio Escalera |  |
| 1208 |  |  [Aligning Visual Contrastive learning models via Preference Optimization](https://openreview.net/forum?id=wgRQ2WAORJ) |  | 0 | Contrastive learning models have demonstrated impressive abilities to capture semantic similarities by aligning representations in the embedding space. However, their performance can be limited by the quality of the training data and its inherent biases. While Preference Optimization (PO) methods... | Ali Rasekh, Amirabbas Afzali, Borna Khodabandeh, Mahyar JafariNodeh, Sepehr Kazemi Ranjbar, Simon Gottschalk |  |
| 1209 |  |  [Exponential Topology-enabled Scalable Communication in Multi-agent Reinforcement Learning](https://openreview.net/forum?id=CL3U0GxFRD) |  | 0 | In cooperative multi-agent reinforcement learning (MARL), well-designed communication protocols can effectively facilitate consensus among agents, thereby enhancing task performance. Moreover, in large-scale multi-agent systems commonly found in real-world applications, effective communication... | Chenjia Bai, Jun Zhang, Xiaolu Wang, Xinran Li |  |
| 1210 |  |  [Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages](https://openreview.net/forum?id=a3g2l4yEys) |  | 0 | Despite recent advances in multimodal large language models (MLLMs), their development has predominantly focused on English- and western-centric datasets and tasks, leaving most of the world's languages and diverse cultural contexts underrepresented. This paper introduces PANGEA, a multilingual... | Akari Asai, Anjali Kantharuban, Graham Neubig, Jean de Dieu Nyandwi, Lintang Sutawika, Sathyanarayanan Ramamoorthy, Seungone Kim, Simran Khanuja, Xiang Yue, Yueqi Song |  |
| 1211 |  |  [Brain-inspired Lp-Convolution benefits large kernels and aligns better with visual cortex](https://openreview.net/forum?id=0LSAmFCc4p) |  | 0 | Convolutional Neural Networks (CNNs) have profoundly influenced the field of computer vision, drawing significant inspiration from the visual processing mechanisms inherent in the brain. Despite sharing fundamental structural and representational similarities with the biological visual system,... | C. Justin Lee, Jea Kwon, Kyungwoo Song, Sungjun Lim |  |
| 1212 |  |  [Deconstructing What Makes a Good Optimizer for Autoregressive Language Models](https://openreview.net/forum?id=zfeso8ceqr) |  | 0 | Training language models becomes increasingly expensive with scale, prompting numerous attempts to improve optimization efficiency. Despite these efforts, the Adam optimizer remains the most widely used, due to a prevailing view that it is the most effective approach. We aim to compare several... | David Brandfonbrener, Depen Morwani, Nikhil Vyas, Rosie Zhao, Sham M. Kakade |  |
| 1213 |  |  [dEBORA: Efficient Bilevel Optimization-based low-Rank Adaptation](https://openreview.net/forum?id=5M0ic2RxQZ) |  | 0 | Low-rank adaptation methods are a popular approach for parameter-efficient fine-tuning of large-scale neural networks. However, selecting the optimal rank for each layer remains a challenging problem that significantly affects both performance and efficiency. In this paper, we introduce a novel... | Emanuele Zangrando, Francesco Rinaldi, Francesco Tudisco, Sara Venturini |  |
| 1214 |  |  [Interaction Asymmetry: A General Principle for Learning Composable Abstractions](https://openreview.net/forum?id=cCl10IU836) |  | 0 | Learning disentangled representations of concepts and re-composing them in unseen ways is crucial for generalizing to out-of-domain situations. However, the underlying properties of concepts that enable such disentanglement and compositional generalization remain poorly understood. In this work, we... | Jack Brady, Julius von Kügelgen, Simon Buchholz, Sébastien Lachapelle, Thomas Kipf, Wieland Brendel |  |
| 1215 |  |  [Decoupling Angles and Strength in Low-rank Adaptation](https://openreview.net/forum?id=X1U74IwuxG) |  | 0 | Parameter-Efficient FineTuning (PEFT) methods have recently gained significant popularity thanks to the widespread availability of large-scale pretrained models. These methods allow for quick adaptation to downstream tasks with minimal computational cost. However, popular finetuning methods such as... | Leander Girrbach, Massimo Bini, Zeynep Akata |  |
| 1216 |  |  [AutoG: Towards automatic graph construction from tabular data](https://openreview.net/forum?id=hovDbX4Gh6) |  | 0 | Recent years have witnessed significant advancements in graph machine learning (GML), with its applications spanning numerous domains. However, the focus of GML has predominantly been on developing powerful models, often overlooking a crucial initial step: constructing suitable graphs from common... | George Karypis, Han Xie, Huzefa Rangwala, Jian Zhang, Jiliang Tang, Xiang Song, Zhikai Chen |  |
| 1217 |  |  [How Learnable Grids Recover Fine Detail in Low Dimensions: A Neural Tangent Kernel Analysis of Multigrid Parametric Encodings](https://openreview.net/forum?id=Ge7okBGZYi) |  | 0 | Neural networks that map between low dimensional spaces are ubiquitous in computer graphics and scientific computing; however, in their naive implementation, they are unable to learn high frequency information. We present a comprehensive analysis comparing the two most common techniques for... | Dinesh Manocha, Matthias Zwicker, Samuel Audia, Soheil Feizi |  |
| 1218 |  |  [AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents](https://openreview.net/forum?id=il5yUQsrjC) |  | 0 | Autonomous agents that execute human tasks by controlling computers can enhance human productivity and application accessibility. However, progress in this field will be driven by realistic and reproducible benchmarks. We present AndroidWorld, a fully functional Android environment that provides... | Alice Li, Christopher Rawles, Daniel Kenji Toyama, Divya Tyamagundlu, Folawiyo CampbellAjala, Gabrielle Lau, Jonathan Waltz, Marybeth Fair, Oriana Riva, Robert James Berry, Sarah Clinckemaillie, Timothy P. Lillicrap, Wei Li, William E. Bishop, Yifan Chang |  |
| 1219 |  |  [Safety Representations for Safer Policy Learning](https://openreview.net/forum?id=gJG4IPwg6l) |  | 0 | Reinforcement learning algorithms typically necessitate extensive exploration of the state space to find optimal policies. However, in safety-critical applications, the risks associated with such exploration can lead to catastrophic consequences. Existing safe exploration methods attempt to... | Annie S. Chen, Charlie Gauthier, Kaustubh Mani, Liam Paull, Samer B. Nashed, Vincent Mai |  |
| 1220 |  |  [Combining Induction and Transduction for Abstract Reasoning](https://openreview.net/forum?id=UmdotAAVDe) |  | 0 | When learning an input-output mapping from very few examples, is it better to first infer a latent function that explains the examples, or is it better to directly predict new test outputs, e.g. using a neural network? We study this question on ARC by training neural models for \emph{induction}... | Caleb Woo, Carter Larsen, Hao Tang, Kevin Ellis, Keya Hu, Simon Alford, Spencer M. Dunn, WeiLong Zheng, WenDing Li, Yewen Pu, Yuqing Wu |  |
| 1221 |  |  [CViT: Continuous Vision Transformer for Operator Learning](https://openreview.net/forum?id=cRnCcuLvyr) |  | 0 | Operator learning, which aims to approximate maps between infinite-dimensional function spaces, is an important area in scientific machine learning with applications across various physical domains. Here we introduce the Continuous Vision Transformer (CViT), a novel neural operator architecture... | George J. Pappas, Hanwen Wang, Jacob H. Seidman, Paris Perdikaris, Shyam Sankaran, Sifan Wang |  |
| 1222 |  |  [ReCogLab: a framework testing relational reasoning & cognitive hypotheses on LLMs](https://openreview.net/forum?id=yORSk4Ycsa) |  | 0 | A fundamental part of human cognition is the ability to not only recall previous memories, but also reason across them to draw conclusions. In cognitive science and psychology, this is termed relational reasoning and a number of effects and biases have been observed in human cognition. Designing... | Amir Zait, Andrew Liu, Danny Karmon, Gargi Balasubramaniam, Henry Prior, Ilia Labzovsky, Ishita Dasgupta, Kenneth Marino, Kim Stachenfeld, Rivka Moroshko |  |
| 1223 |  |  [MGCFNN: A Neural MultiGrid Solver with Novel Fourier Neural Network for High Wave Number Helmholtz Equations](https://openreview.net/forum?id=ThhQyIruEs) |  | 0 | Solving high wavenumber Helmholtz equations is notoriously challenging. Traditional solvers have yet to yield satisfactory results, and most neural network methods struggle to accurately solve cases with extremely high wavenumbers within heterogeneous media. This paper presents an advanced... | Chensong Zhang, Minrui Lv, Yan Xie |  |
| 1224 |  |  [CirT: Global Subseasonal-to-Seasonal Forecasting with Geometry-inspired Transformer](https://openreview.net/forum?id=YslOW2SO6S) |  | 0 | Accurate Subseasonal-to-Seasonal (S2S) climate forecasting is pivotal for decision-making including agriculture planning and disaster preparedness but is known to be challenging due to its chaotic nature. Although recent data-driven models have shown promising results, their performance is limited... | Deli Zhao, Fugee Tsung, Jia Li, Jiashun Cheng, Yang Liu, Yu Rong, Zinan Zheng |  |
| 1225 |  |  [Accelerating neural network training: An analysis of the AlgoPerf competition](https://openreview.net/forum?id=CtM5xjRSfm) |  | 0 | The goal of the AlgoPerf: Training Algorithms competition is to evaluate practical speed-ups in neural network training achieved solely by improving the underlying training algorithms. In the external tuning ruleset, submissions must provide workload-agnostic hyperparameter search spaces, while in... | Boyuan Feng, Chandramouli Shama Sastry, Edward Z. Yang, Frank Schneider, George E. Dahl, Juhan Bae, Less Wright, Mark Saroufim, Michael Rabbat, Philipp Hennig, Priya Kasimbeg, Runa Eschenhagen, Sourabh Medapati, Zachary Nado |  |
| 1226 |  |  [Injecting Universal Jailbreak Backdoors into LLMs in Minutes](https://openreview.net/forum?id=aSy2nYwiZ2) |  | 0 | Jailbreak backdoor attacks on LLMs have garnered attention for their effectiveness and stealth. However, existing methods rely on the crafting of poisoned datasets and the time-consuming process of fine-tuning. In this work, we propose JailbreakEdit, a novel jailbreak backdoor injection method that... | Qiannan Zhang, Shichao Pei, Zhuowei Chen |  |
| 1227 |  |  [Action abstractions for amortized sampling](https://openreview.net/forum?id=ispjankYab) |  | 0 | As trajectories sampled by policies used by reinforcement learning (RL) and generative flow networks (GFlowNets) grow longer, credit assignment and exploration become more challenging, and the long planning horizon hinders mode discovery and generalization. The challenge is particularly pronounced... | Emmanuel Bengio, Joseph D. Viviano, Léna Néhale Ezzine, Michal Koziarski, Moksh Jain, Nikolay Malkin, Oussama Boussif, Rim Assouel, Yoshua Bengio |  |
| 1228 |  |  [Provence: efficient and robust context pruning for retrieval-augmented generation](https://openreview.net/forum?id=TDy5Ih78b4) |  | 0 | Retrieval-Augmented Generation improves various aspects of large language models (LLMs) generation, but suffers from computational overhead caused by long contexts, and the propagation of irrelevant retrieved information into generated responses. Context pruning deals with both aspects, by removing... | Nadezhda Chirkova, Stéphane Clinchant, Thibault Formal, Vassilina Nikoulina |  |
| 1229 |  |  [Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning](https://openreview.net/forum?id=UyU8ETswPg) |  | 0 | Recent studies have identified one aggravating factor of LLM hallucinations as the knowledge inconsistency between pre-training and fine-tuning, where unfamiliar fine-tuning data mislead the LLM to fabricate plausible but wrong outputs. In this paper, we propose a novel fine-tuning strategy called... | Shiyu Chang, Tommi S. Jaakkola, Yang Zhang, Yujian Liu |  |
| 1230 |  |  [Second Order Bounds for Contextual Bandits with Function Approximation](https://openreview.net/forum?id=h6ktwCPYxE) |  | 0 | Many works have developed no-regret algorithms for contextual bandits with function approximation, where the mean rewards over context-action pairs belong to a function class $\mathcal{F}$. Although there are many approaches to this problem, algorithms based on the principle of optimism, such as... | Aldo Pacchiano |  |
| 1231 |  |  [Unveiling the Magic of Code Reasoning through Hypothesis Decomposition and Amendment](https://openreview.net/forum?id=kN25ggeq1J) |  | 0 | The reasoning abilities are one of the most enigmatic and captivating aspects of large language models (LLMs). Numerous studies are dedicated to exploring and expanding the boundaries of this reasoning capability. However, tasks that embody both reasoning and recall characteristics are often... | Enhong Chen, Kai Zhang, Qi Liu, Tianyun Ji, Wenjun Feng, Yixiao Ma, Yuze Zhao, Zhenya Huang, Zhiding Liu |  |
| 1232 |  |  [Mitigating Parameter Interference in Model Merging via Sharpness-Aware Fine-Tuning](https://openreview.net/forum?id=eaTqsptDPL) |  | 0 | Large-scale deep learning models with a pretraining-finetuning paradigm have led to a surge of numerous task-specific models fine-tuned from a common pre-trained model. Recently, several research efforts have been made on merging these large models into a single multi-task model, particularly with... | Jinwook Jung, Sungyong Baik, Yeoreum Lee |  |
| 1233 |  |  [Flow: Modularized Agentic Workflow Automation](https://openreview.net/forum?id=sLKDbuyq99) |  | 0 | Multi-agent frameworks powered by large language models (LLMs) have demonstrated great success in automated planning and task execution. However, the effective adjustment of agentic workflows during execution has not been well studied. An effective workflow adjustment is crucial in real-world... | Boye Niu, Kai Lian, Kun Zhang, Tongliang Liu, Yifan Shen, Yiliao Song, Yu Yao |  |
| 1234 |  |  [S4M: S4 for multivariate time series forecasting with Missing values](https://openreview.net/forum?id=BkftcwIVmR) |  | 0 | Multivariate time series data play a pivotal role in a wide range of real-world applications, such as finance, healthcare, and meteorology, where accurate forecasting is critical for informed decision-making and proactive interventions. However, the presence of block missing data introduces... | Meiqi Yang, Peng Jing, Qiong Zhang, Xiaoxiao Li |  |
| 1235 |  |  [An Exploration with Entropy Constrained 3D Gaussians for 2D Video Compression](https://openreview.net/forum?id=JbRM5QKRDd) |  | 0 | 3D Gaussian Splatting (3DGS) has witnessed its rapid development in novel view synthesis, which attains high quality reconstruction and real-time rendering. At the same time, there is still a gap before implicit neural representation (INR) can become a practical compressor due to the lack of stream... | Bin Chen, ShuTao Xia, Xiang Liu, Yaowei Wang, Zimo Liu |  |
| 1236 |  |  [ILLUSION: Unveiling Truth with a Comprehensive Multi-Modal, Multi-Lingual Deepfake Dataset](https://openreview.net/forum?id=qnlG3zPQUy) |  | 0 | The proliferation of deepfakes and AI-generated content has led to a surge in media forgeries and misinformation, necessitating robust detection systems. However, current datasets lack diversity across modalities, languages, and real-world scenarios. To address this gap, we present ILLUSION... | Akanksha Singh, Akshat Jain, Kartik Thakral, Mayank Vatsa, Richa Singh, Rishabh Ranjan |  |
| 1237 |  |  [Gramian Multimodal Representation Learning and Alignment](https://openreview.net/forum?id=ftGnpZrW7P) |  | 0 | Human perception integrates multiple modalities—such as vision, hearing, and language—into a unified understanding of the surrounding reality. While recent multimodal models have achieved significant progress by aligning pairs of modalities via contrastive learning, their solutions are unsuitable... | Danilo Comminiello, Eleonora Grassucci, Giordano Cicchetti, Luigi Sigillo |  |
| 1238 |  |  [Going Beyond Static: Understanding Shifts with Time-Series Attribution](https://openreview.net/forum?id=XQlccqJpCC) |  | 0 | Distribution shifts in time-series data are complex due to temporal dependencies, multivariable interactions, and trend changes. However, robust methods often rely on structural assumptions that lack thorough empirical validation, limiting their practical applicability. In order to support an... | Jiashuo Liu, Mihaela van der Schaar, Nabeel Seedat, Peng Cui |  |
| 1239 |  |  [Video In-context Learning: Autoregressive Transformers are Zero-Shot Video Imitators](https://openreview.net/forum?id=wkbx7BRAsM) |  | 0 | People interact with the real-world largely dependent on visual signal, which are ubiquitous and illustrate detailed demonstrations. In this paper, we explore utilizing visual signals as a new interface for models to interact with the environment. Specifically, we choose videos as a representative... | Jiang Bian, Junliang Guo, Li Zhao, Linli Xu, Tianyu He, Wentao Zhang |  |
| 1240 |  |  [HR-Extreme: A High-Resolution Dataset for Extreme Weather Forecasting](https://openreview.net/forum?id=5AtlfHYCPa) |  | 0 | The application of large deep learning models in weather forecasting has led to significant advancements in the field, including higher-resolution forecasting and extended prediction periods exemplified by models such as Pangu and Fuxi. Despite these successes, previous research has largely been... | Jianxin Lin, Nian Ran, Peng Xiao, Qi Meng, Richard Allmendinger, Wesley Shi, Yue Wang |  |
| 1241 |  |  [MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily Complex Proofs](https://openreview.net/forum?id=5ck9PIrTpH) |  | 0 | Large language models (LLMs) can solve arithmetic word problems with high accuracy, but little is known about how well they generalize to more complex problems. This is difficult to study, as (i) much of the available evaluation data has already been seen by the most capable models during training,... | Abulhair Saparov, Andreas Opedal, Bernhard Schölkopf, Haruki Shirakami, Mrinmaya Sachan |  |
| 1242 |  |  [Feature-Based Online Bilateral Trade](https://openreview.net/forum?id=xnF2U0ro7b) |  | 0 | Bilateral trade models the problem of facilitating trades between a seller and a buyer having private valuations for the item being sold. In the online version of the problem, the learner faces a new seller and buyer at each time step, and has to post a price for each of the two parties without any... | Andrea Celli, Martino Bernasconi, Matteo Castiglioni, Solenne Gaucher, Vianney Perchet |  |
| 1243 |  |  [Interpretable Causal Representation Learning for Biological Data in the Pathway Space](https://openreview.net/forum?id=3Fgylj4uqL) |  | 0 | Predicting the impact of genomic and drug perturbations in cellular function is crucial for understanding gene functions and drug effects, ultimately leading to improved therapies. To this end, Causal Representation Learning (CRL) constitutes one of the most promising approaches, as it aims to... | Carlos RuizArenas, David GomezCabrero, Idoia Ochoa, Irene MarínGoñi, Jan Voges, Jesper Tegnér, Jesus de la Fuente Cedeño, Mikel Hernaez, Robert Lehmann, Vincenzo Lagani, Xabier Martinez de Morentin |  |
| 1244 |  |  [Separation Power of Equivariant Neural Networks](https://openreview.net/forum?id=RAyRXQjsFl) |  | 0 | The separation power of a machine learning model refers to its ability to distinguish between different inputs and is often used as a proxy for its expressivity. Indeed, knowing the separation power of a family of models is a necessary condition to obtain fine-grained universality results. In this... | Bruno Lepri, Gabriele Santin, Marco Pacini, Xiaowen Dong |  |
| 1245 |  |  [HASARD: A Benchmark for Vision-Based Safe Reinforcement Learning in Embodied Agents](https://openreview.net/forum?id=5BRFddsAai) |  | 0 | Advancing safe autonomous systems through reinforcement learning (RL) requires robust benchmarks to evaluate performance, analyze methods, and assess agent competencies. Humans primarily rely on embodied visual perception to safely navigate and interact with their surroundings, making it a valuable... | Meng Fang, Mykola Pechenizkiy, Tristan Tomilin |  |
| 1246 |  |  [Associative memory and dead neurons](https://openreview.net/forum?id=mkNVPGpEPm) |  | 0 | In \`\`Large Associative Memory Problem in Neurobiology and Machine Learning,'' Dmitry Krotov and John Hopfield introduced a general technique for the systematic construction of neural ordinary differential equations with non-increasing energy or Lyapunov function. We study this energy function and... | Ivan V. Oseledets, Vladimir Fanaskov |  |
| 1247 |  |  [The OMG dataset: An Open MetaGenomic corpus for mixed-modality genomic language modeling](https://openreview.net/forum?id=jlzNb1iWs3) |  | 0 | Biological language model performance depends heavily on pretraining data quality, diversity, and size. While metagenomic datasets feature enormous biological diversity, their utilization as pretraining data has been limited due to challenges in data accessibility, quality filtering and... | Andre Cornman, Antonio Pedro Camargo, Jacob WestRoberts, Martin Beracochea, Milot Mirdita, Sergey Ovchinnikov, Simon Roux, Yunha Hwang |  |
| 1248 |  |  [Physics-informed Temporal Difference Metric Learning for Robot Motion Planning](https://openreview.net/forum?id=TOiageVNru) |  | 0 | The motion planning problem involves finding a collision-free path from a robot's starting to its target configuration. Recently, self-supervised learning methods have emerged to tackle motion planning problems without requiring expensive expert demonstrations. They solve the Eikonal equation for... | Ahmed H. Qureshi, Ruiqi Ni, Zherong Pan |  |
| 1249 |  |  [Towards Scalable Topological Regularizers](https://openreview.net/forum?id=FjZcwQJX8D) |  | 0 | Latent space matching, which consists of matching distributions of features in latent space, is a crucial component for tasks such as adversarial attacks and defenses, domain adaptation, and generative modelling. Metrics for probability measures, such as Wasserstein and maximum mean discrepancy,... | Darrick Lee, HiuTung Wong, Hong Yan |  |
| 1250 |  |  [KaSA: Knowledge-Aware Singular-Value Adaptation of Large Language Models](https://openreview.net/forum?id=OQqNieeivq) |  | 0 | The increasing sizes of large language models (LLMs) result in significant computational overhead and memory usage when adapting these models to specific tasks or domains. Various parameter-efficient fine-tuning (PEFT) methods have been devised to mitigate these challenges by training a small set... | Chansung Park, Fan Wang, Jing Tang, Juyong Jiang, Sunghun Kim |  |
| 1251 |  |  [The Foundations of Tokenization: Statistical and Computational Concerns](https://openreview.net/forum?id=B5iOSxM2I0) |  | 0 | Tokenization — the practice of converting strings of characters from an alphabet into sequences of tokens over a vocabulary — is a critical step in the NLP pipeline. The use of token representations is widely credited with increased model performance but is also the source of many undesirable... | Brian DuSell, John Terilla, Juan Luis Gastaldi, Luca Malagutti, Ryan Cotterell, Tim Vieira |  |
| 1252 |  |  [Can Video LLMs Refuse to Answer? Alignment for Answerability in Video Large Language Models](https://openreview.net/forum?id=P9VdRQOyqu) |  | 0 | In the broader context of deep learning, Multimodal Large Language Models have achieved significant breakthroughs by leveraging powerful Large Language Models as a backbone to align different modalities into the language space. A prime exemplification is the development of Video Large Language... | Chang D. Yoo, Eunseop Yoon, Hee Suk Yoon, Mark A. HasegawaJohnson |  |
| 1253 |  |  [Intrinsic Dimension Correlation: uncovering nonlinear connections in multimodal representations](https://openreview.net/forum?id=Qj1KwBZaEI) |  | 0 | To gain insight into the mechanisms behind machine learning methods, it is crucial to establish connections among the features describing data points. However, these correlations often exhibit a high-dimensional and strongly nonlinear nature, which makes them challenging to detect using standard... | Alex Rodriguez, Fabio Anselmi, Lorenzo Basile, Luca Bortolussi, Santiago Acevedo |  |
| 1254 |  |  [Regret-Optimal List Replicable Bandit Learning: Matching Upper and Lower Bounds](https://openreview.net/forum?id=0T49QbSOho) |  | 0 | This paper investigates \*list replicability\* [Dixon et al., 2023] in the context of multi-armed (also linear) bandits (MAB). We define an algorithm $A$ for MAB to be $(\ell,\delta)$-list replicable if with probability at least $1-\delta$, $A$ has at most $\ell$ traces in independent executions... | Aduri Pavan, Lin Yang, Michael Chen, N. V. Vinodchandran, Ruosong Wang |  |
| 1255 |  |  [Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models](https://openreview.net/forum?id=odvSjn416y) |  | 0 | Instruction-tuned language models (LM) are able to respond to imperative commands, providing a more natural user interface compared to their base counterparts. In this work, we present Promptriever, the first retrieval model able to be prompted like an LM. To train Promptriever, we curate and... | Ashwin Paranjape, Benjamin Van Durme, Dawn J. Lawrie, Jack Hessel, Orion Weller, Yuhao Zhang |  |
| 1256 |  |  [Everything, Everywhere, All at Once: Is Mechanistic Interpretability Identifiable?](https://openreview.net/forum?id=5IWJBStfU7) |  | 0 | As AI systems are increasingly deployed in high-stakes applications, ensuring their interpretability is essential. Mechanistic Interpretability (MI) aims to reverse-engineer neural networks by extracting human-understandable algorithms embedded within their structures to explain their behavior.... | François Portet, Maxime Méloux, Maxime Peyrard, Silviu Maniu |  |
| 1257 |  |  [Language Models Need Inductive Biases to Count Inductively](https://openreview.net/forum?id=s3IBHTTDYl) |  | 0 | Counting constitutes a core skill underlying a wide range of tasks, such as formal language recognition, multi-hop reasoning and simulating algorithms. Generaliz- ing counting inductively is central to task success on out-of-distribution (OOD) instances where testing inputs are longer than those... | Yingshan Chang, Yonatan Bisk |  |
| 1258 |  |  [General Scene Adaptation for Vision-and-Language Navigation](https://openreview.net/forum?id=2oKkQTyfz7) |  | 0 | Vision-and-Language Navigation (VLN) tasks mainly evaluate agents based on one-time execution of individual instructions across multiple environments, aiming to develop agents capable of functioning in any environment in a zero-shot manner. However, real-world navigation robots often operate in... | Haodong Hong, Jiajun Liu, Qi Wu, Sen Wang, Yanyuan Qiao |  |
| 1259 |  |  [Bridging Context Gaps: Leveraging Coreference Resolution for Long Contextual Understanding](https://openreview.net/forum?id=cPozlf9OaF) |  | 0 | Large language models (LLMs) have shown remarkable capabilities in natural language processing; however, they still face difficulties when tasked with understanding lengthy contexts and executing effective question answering. These challenges often arise due to the complexity and ambiguity present... | Jiannan Cao, Jianwei Yin, Sheng Cheng, Shi Bo, Tianyu Du, Xinyue Peng, Xuhong Zhang, Xun Wang, Yanming Liu, Yanxin Shen |  |
| 1260 |  |  [Scalable Mechanistic Neural Networks](https://openreview.net/forum?id=Oazgf8A24z) |  | 0 | We propose Scalable Mechanistic Neural Network (S-MNN), an enhanced neural network framework designed for scientific machine learning applications involving long temporal sequences. By reformulating the original Mechanistic Neural Network (MNN) (Pervez et al., 2024), we reduce the computational... | Adeel Pervez, Dan Alistarh, Dingling Yao, Francesco Locatello, Jiale Chen |  |
| 1261 |  |  [Linear Partial Gromov-Wasserstein Embedding](https://openreview.net/forum?id=BA1eG7vCNb) |  | 0 | The Gromov–Wasserstein (GW) problem, a variant of the classical optimal transport (OT) problem, has attracted growing interest in the machine learning and data science communities due to its ability to quantify similarity between measures in different metric spaces. However, like the classical OT... | Abihith Kothapalli, Hengrong Du, Rocio Diaz Martin, Soheil Kolouri, Yikun Bai |  |
| 1262 |  |  [As Simple as Fine-tuning: LLM Alignment via Bidirectional Negative Feedback Loss](https://openreview.net/forum?id=fsX9nFwMNj) |  | 0 | Direct Preference Optimization (DPO) has emerged as a more computationally efficient alternative to Reinforcement Learning from Human Feedback (RLHF) with Proximal Policy Optimization (PPO), eliminating the need for reward models and online sampling. Despite these benefits, DPO and its variants... | Anh Tuan Luu, FengLin Li, Huimin Xu, Wang Chen, Wei Zhang, Xin Mao, Ziqi Jin |  |
| 1263 |  |  [E(3)-equivariant models cannot learn chirality: Field-based molecular generation](https://openreview.net/forum?id=mXHTifc1Fn) |  | 0 | Obtaining the desired effect of drugs is highly dependent on their molecular geometries. Thus, the current prevailing paradigm focuses on 3D point-cloud atom representations, utilizing graph neural network (GNN) parametrizations, with rotational symmetries baked in via E(3) invariant layers. We... | Alexandru Dumitrescu, Dani Korpela, Harri Lähdesmäki, Markus Heinonen, Valerii Iakovlev, Vikas Garg, Yogesh Verma |  |
| 1264 |  |  [One Model Transfer to All: On Robust Jailbreak Prompts Generation against LLMs](https://openreview.net/forum?id=sULAwlAWc1) |  | 0 | Safety alignment in large language models (LLMs) is increasingly compromised by jailbreak attacks, which can manipulate these models to generate harmful or unintended content. Investigating these attacks is crucial for uncovering model vulnerabilities. However, many existing jailbreak strategies... | Daojing He, Linbao Li, Yannan Liu, Yu Li |  |
| 1265 |  |  [PaPaGei: Open Foundation Models for Optical Physiological Signals](https://openreview.net/forum?id=kYwTmlq6Vn) |  | 0 | Photoplethysmography (PPG) is the leading non-invasive technique for monitoring biosignals and cardiovascular health, with widespread adoption in both clinical settings and consumer wearable devices. While machine learning models trained on PPG signals have shown promise, they tend to be... | Arvind Pillai, Dimitris Spathis, Fahim Kawsar, Mohammad Malekzadeh |  |
| 1266 |  |  [Sensitivity-Constrained Fourier Neural Operators for Forward and Inverse Problems in Parametric Differential Equations](https://openreview.net/forum?id=DPzQ5n3mNm) |  | 0 | Parametric differential equations of the form $\frac{\partial u}{\partial t} = f(u, x, t, p)$ are fundamental in science and engineering. While deep learning frameworks like the Fourier Neural Operator (FNO) efficiently approximate differential equation solutions, they struggle with inverse... | Abdolmehdi Behroozi, Chaopeng Shen, Daniel Kifer |  |
| 1267 |  |  [Captured by Captions: On Memorization and its Mitigation in CLIP Models](https://openreview.net/forum?id=5V0f8igznO) |  | 0 | Multi-modal models, such as CLIP, have demonstrated strong performance in aligning visual and textual representations, excelling in tasks like image retrieval and zero-shot classification. Despite this success, the mechanisms by which these models utilize training data, particularly the role of... | Adam Dziedzic, Franziska Boenisch, Grace C. Kim, Michael Backes, Wenhao Wang |  |
| 1268 |  |  [CHASE-SQL: Multi-Path Reasoning and Preference Optimized Candidate Selection in Text-to-SQL](https://openreview.net/forum?id=CvGqMD5OtX) |  | 0 | We present CHASE-SQL, a novel framework addressing large language model (LLM) performance challenges for Text-to-SQL tasks by leveraging multi-agent modeling and test-time compute for improved candidate generation and selection. CHASE-SQL uses LLMs to generate diverse SQL candidates with: (1) a... | Amin Saberi, Fatma Ozcan, Gaurav Tarlok Kakkar, Hailong Li, Mohammadreza Pourreza, Ruoxi Sun, Sercan Ö. Arik, Shayan Talaei, Yeounoh Chung, Yu Gan |  |
| 1269 |  |  [JPEG Inspired Deep Learning](https://openreview.net/forum?id=te2IdORabL) |  | 0 | Although it is traditionally believed that lossy image compression, such as JPEG compression, has a negative impact on the performance of deep neural networks (DNNs), it is shown by recent works that well-crafted JPEG compression can actually improve the performance of deep learning (DL). Inspired... | Ahmed H. Salamah, EnHui Yang, Kaixiang Zheng, Yiwen Liu |  |
| 1270 |  |  [MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents](https://openreview.net/forum?id=K5yeB4dTtS) |  | 0 | MLLM agents demonstrate potential for complex embodied tasks by retrieving multimodal task-relevant trajectory data. However, current retrieval methods primarily focus on surface-level similarities of textual or visual cues in trajectories, neglecting their effectiveness for the specific task at... | Börje F. Karlsson, Junpeng Yue, Xinrun Xu, Zongqing Lu |  |
| 1271 |  |  [Divergence-Regularized Discounted Aggregation: Equilibrium Finding in Multiplayer Partially Observable Stochastic Games](https://openreview.net/forum?id=KD5nJUgeW4) |  | 0 | This paper presents Divergence-Regularized Discounted Aggregation (DRDA), a multi-round learning system for solving partially observable stochastic games (POSGs). DRDA is based on action values and applicable to multiplayer POSGs, which can unify normal-form games (NFGs), extensive-form games... | Dongbin Zhao, Runyu Lu, Yuanheng Zhu |  |
| 1272 |  |  [The Case for Cleaner Biosignals: High-fidelity Neural Compressor Enables Transfer from Cleaner iEEG to Noisier EEG](https://openreview.net/forum?id=b57IG6N20B) |  | 0 | All data modalities are not created equal, even when the signal they measure comes from the same source. In the case of the brain, two of the most important data modalities are the scalp electroencephalogram (EEG), and the intracranial electroencephalogram (iEEG). iEEG benefits from a higher... | Abbas Rahimi, Francesco S. Carzaniga, Gary Tom Hoppeler, Kaspar Schindler, Michael Hersche |  |
| 1273 |  |  [Preference Diffusion for Recommendation](https://openreview.net/forum?id=6GATHdOi1x) |  | 0 | Recommender systems aim to predict personalized item rankings by modeling user preference distributions derived from historical behavior data. While diffusion models (DMs) have recently gained attention for their ability to model complex distributions, current DM-based recommenders typically rely... | An Zhang, Guoqing Hu, Hong Qian, Shuo Liu, TatSeng Chua |  |
| 1274 |  |  [Rethinking Evaluation of Sparse Autoencoders through the Representation of Polysemous Words](https://openreview.net/forum?id=HpUs2EXjOl) |  | 0 | Sparse autoencoders (SAEs) have gained a lot of attention as a promising tool to improve the interpretability of large language models (LLMs) by mapping the complex superposition of \*polysemantic\* neurons into \*monosemantic\* features and composing a sparse dictionary of words. However,... | Gouki Minegishi, Hiroki Furuta, Yusuke Iwasawa, Yutaka Matsuo |  |
| 1275 |  |  [Tool-Planner: Task Planning with Clusters across Multiple Tools](https://openreview.net/forum?id=dRz3cizftU) |  | 0 | Large language models (LLMs) have demonstrated exceptional reasoning capabilities, enabling them to solve various complex problems. Recently, this ability has been applied to the paradigm of tool learning. Tool learning involves providing examples of tool usage and their corresponding functions,... | Jiannan Cao, Jianwei Yin, Sheng Cheng, Shi Bo, Tianyu Du, Xinyue Peng, Xuhong Zhang, Xun Wang, Yanming Liu, Yuwei Zhang |  |
| 1276 |  |  [On a Connection Between Imitation Learning and RLHF](https://openreview.net/forum?id=2QdsjiNXgj) |  | 0 | This work studies the alignment of large language models with preference data from an imitation learning perspective. We establish a close theoretical connection between reinforcement learning from human feedback RLHF and imitation learning (IL), revealing that RLHF implicitly performs imitation... | Mingxiao Li, Teng Xiao, Vasant G. Honavar, Yige Yuan, Zhengyu Chen |  |
| 1277 |  |  [GIFT: Unlocking Full Potential of Labels in Distilled Dataset at Near-zero Cost](https://openreview.net/forum?id=FoF5RaA3ug) |  | 0 | Recent advancements in dataset distillation have demonstrated the significant benefits of employing soft labels generated by pre-trained teacher models. In this paper, we introduce a novel perspective by emphasizing the full utilization of labels. We first conduct a comprehensive comparison of... | Peng Sun, Tao Lin, Xinyi Shang |  |
| 1278 |  |  [Beyond Mere Token Analysis: A Hypergraph Metric Space Framework for Defending Against Socially Engineered LLM Attacks](https://openreview.net/forum?id=rnJxelIZrq) |  | 0 | Recent jailbreak attempts on Large Language Models (LLMs) have shifted from algorithm-focused to human-like social engineering attacks, with persuasion-based techniques emerging as a particularly effective subset. These attacks evolve rapidly, demonstrate high creativity, and boast superior attack... | Aditya Saibewar, Manohar Kaul, Sadbhavana Babar |  |
| 1279 |  |  [Controlling Space and Time with Diffusion Models](https://openreview.net/forum?id=d2UrCGtntF) |  | 0 | We present 4DiM, a cascaded diffusion model for 4D novel view synthesis (NVS), supporting generation with arbitrary camera trajectories and timestamps, in natural scenes, conditioned on one or more images. With a novel architecture and sampling procedure, we enable training on a mixture of 3D (with... | Andrea Tagliasacchi, Daniel Watson, David J. Fleet, Lala Li, Saurabh Saxena |  |
| 1280 |  |  [Geometry Image Diffusion: Fast and Data-Efficient Text-to-3D with Image-Based Surface Representation](https://openreview.net/forum?id=Glm7Kj47nN) |  | 0 | Generating high-quality 3D objects from textual descriptions remains a challenging problem due to high computational costs, the scarcity of 3D data, and the complexity of 3D representations. We introduce Geometry Image Diffusion (GIMDiffusion), a novel Text-to-3D model that utilizes geometry images... | Ciara Rowles, Simon Donné, Slava Elizarov |  |
| 1281 |  |  [Persistent Pre-training Poisoning of LLMs](https://openreview.net/forum?id=eiqrnVaeIw) |  | 0 | Large language models are pre-trained on uncurated text datasets consisting of trillions of tokens scraped from the Web. Prior work has shown that: (1) web-scraped pre-training datasets can be practically poisoned by malicious actors; and (2) adversaries can compromise language models after... | Daphne Ippolito, Eric Michael Smith, Florian Tramèr, Ivan Evtimov, Javier Rando, Jianfeng Chi, Nicholas Carlini, Yiming Zhang |  |
| 1282 |  |  [CipherPrune: Efficient and Scalable Private Transformer Inference](https://openreview.net/forum?id=mUMvr33FTu) |  | 0 | Private Transformer inference using cryptographic protocols offers promising solutions for privacy-preserving machine learning; however, it still faces significant runtime overhead (efficiency issues) and challenges in handling long-token inputs (scalability issues). We observe that the... | Jiaqi Xue, Lei Jiang, Mengxin Zheng, Mimi Xie, Mingzhe Zhang, Qian Lou, Yancheng Zhang |  |
| 1283 |  |  [Approximating Full Conformal Prediction for Neural Network Regression with Gauss-Newton Influence](https://openreview.net/forum?id=vcX0k4rGTt) |  | 0 | Uncertainty quantification is an important prerequisite for the deployment of deep learning models in safety-critical areas. Yet, this hinges on the uncertainty estimates being useful to the extent the prediction intervals are well-calibrated and sharp. In the absence of inherent uncertainty... | Alvaro H. C. Correia, Christos Louizos, Dharmesh Tailor, Eric T. Nalisnick |  |
| 1284 |  |  [Adversarial Policy Optimization for Offline Preference-based Reinforcement Learning](https://openreview.net/forum?id=5Y9NT6lW21) |  | 0 | In this paper, we study offline preference-based reinforcement learning (PbRL), where learning is based on pre-collected preference feedback over pairs of trajectories. While offline PbRL has demonstrated remarkable empirical success, existing theoretical approaches face challenges in ensuring... | Hyungkyu Kang, Minhwan Oh |  |
| 1285 |  |  [Optimal Strong Regret and Violation in Constrained MDPs via Policy Optimization](https://openreview.net/forum?id=8eNLKk5by4) |  | 0 | We study online learning in constrained MDPs (CMDPs), focusing on the goal of attaining sublinear strong regret and strong cumulative constraint violation. Differently from their standard (weak) counterparts, these metrics do not allow negative terms to compensate positive ones, raising... | Alberto Marchesi, Francesco Emanuele Stradi, Matteo Castiglioni, Nicola Gatti |  |
| 1286 |  |  [Endowing Visual Reprogramming with Adversarial Robustness](https://openreview.net/forum?id=OuLgaHEmzi) |  | 0 | Visual reprogramming (VR) leverages well-developed pre-trained models (e.g., a pre-trained classifier on ImageNet) to tackle target tasks (e.g., a traffic sign recognition task), without the need for training from scratch. Despite the effectiveness of previous VR methods, all of them did not... | Feng Liu, Haiyang Xu, Lei Feng, Ming Yan, Shengjie Zhou, Tao Xiang, Xin Cheng |  |
| 1287 |  |  [Systematic Relational Reasoning With Epistemic Graph Neural Networks](https://openreview.net/forum?id=qNp86ByQlN) |  | 0 | Developing models that can learn to reason is a notoriously challenging problem. We focus on reasoning in relational domains, where the use of Graph Neural Networks (GNNs) seems like a natural choice. However, previous work has shown that regular GNNs lack the ability to systematically generalize... | Irtaza Khalid, Steven Schockaert |  |
| 1288 |  |  [RetroInText: A Multimodal Large Language Model Enhanced Framework for Retrosynthetic Planning via In-Context Representation Learning](https://openreview.net/forum?id=J6e4hurEKd) |  | 0 | Development of robust and effective strategies for retrosynthetic planning requires a deep understanding of the synthesis process. A critical step in achieving this goal is accurately identifying synthetic intermediates. Current machine learning-based methods often overlook the valuable context... | Chenglong Kang, Fei Guo, Xiaoyi Liu |  |
| 1289 |  |  [XAIguiFormer: explainable artificial intelligence guided transformer for brain disorder identification](https://openreview.net/forum?id=AD5yx2xq8R) |  | 0 | EEG-based connectomes offer a low-cost and portable method to identify brain disorders using deep learning. With the growing interest in model interpretability and transparency, explainable artificial intelligence (XAI) is widely applied to understand the decision of deep learning models. However,... | Abigail Morrison, Farah Abdellatif, Hanning Guo, Jürgen Dammers, N. Jon Shah, Yu Fu |  |
| 1290 |  |  [Simple, Good, Fast: Self-Supervised World Models Free of Baggage](https://openreview.net/forum?id=yFGR36PLDJ) |  | 0 | What are the essential components of world models? How far do we get with world models that are not employing RNNs, transformers, discrete representations, and image reconstructions? This paper introduces SGF, a Simple, Good, and Fast world model that uses self-supervised representation learning,... | Jan Robine, Marc Höftmann, Stefan Harmeling |  |
| 1291 |  |  [A new framework for evaluating model out-of-distribution generalisation for the biochemical domain](https://openreview.net/forum?id=qFZnAC4GHR) |  | 0 | Quantifying model generalization to out-of-distribution data has been a longstanding challenge in machine learning. Addressing this issue is crucial for leveraging machine learning in scientific discovery, where models must generalize to new molecules or materials. Current methods typically split... | Denis C. Shields, Hoang Thanh Lam, Raúl FernandezDiaz, Vanessa López |  |
| 1292 |  |  [Long-Short Decision Transformer: Bridging Global and Local Dependencies for Generalized Decision-Making](https://openreview.net/forum?id=NHMuM84tRT) |  | 0 | Decision Transformers (DTs) effectively capture long-range dependencies using self-attention but struggle with fine-grained local relationships, especially the Markovian properties in many offline-RL datasets. Conversely, Decision Convformer (DC) utilizes convolutional filters for capturing local... | Diego Martínez Plasencia, Dimitrios Kanoulas, Elia Gatti, Jincheng Wang, Pengyuan Wei, Penny Karanasou |  |
| 1293 |  |  [Audio Large Language Models Can Be Descriptive Speech Quality Evaluators](https://openreview.net/forum?id=U42TkrEDzb) |  | 0 | An ideal multimodal agent should be aware of the quality of its input modalities. Recent advances have enabled large language models (LLMs) to incorporate auditory systems for handling various speech-related tasks. However, most audio LLMs remain unaware of the quality of the speech they process.... | Chao Zhang, ChaoHan Huck Yang, Chen Chen, Eng Siong Chng, Helin Wang, Siyin Wang, Yuchen Hu, Zhehuai Chen |  |
| 1294 |  |  [Learn Your Reference Model for Real Good Alignment](https://openreview.net/forum?id=H0qIWXXLUR) |  | 0 | Despite the fact that offline methods for Large Language Models (LLMs) alignment do not require a direct reward model, they remain susceptible to overoptimization. This issue arises when the trained model deviates excessively from the reference policy, leading to a decrease in sample quality. We... | Alexey Gorbatovski, Alexey Malakhov, Boris Shaposhnikov, Daniil Gavrilov, Ian Maksimov, Nikita Balagansky, Nikita Surnachev, Yaroslav Aksenov |  |
| 1295 |  |  [Noise-conditioned Energy-based Annealed Rewards (NEAR): A Generative Framework for Imitation Learning from Observation](https://openreview.net/forum?id=DL9txImSzm) |  | 0 | This paper introduces a new imitation learning framework based on energy-based generative models capable of learning complex, physics-dependent, robot motion policies through state-only expert motion trajectories. Our algorithm, called Noise-conditioned Energy-based Annealed Rewards (NEAR),... | Anish Abhijit Diwan, Jan Peters, Jens Kober, Julen Urain |  |
| 1296 |  |  [EcoFace: Audio-Visual Emotional Co-Disentanglement Speech-Driven 3D Talking Face Generation](https://openreview.net/forum?id=iDcWYtYUwX) |  | 0 | Speech-driven 3D facial animation has attracted significant attention due to its wide range of applications in animation production and virtual reality. Recent research has explored speech-emotion disentanglement to enhance facial expressions rather than manually assigning emotions. However, this... | Chengfei Lv, Fei Wu, Jiajian Xie, Mengze Li, Shengyu Zhang, Zhou Zhao |  |
| 1297 |  |  [Elliptic Loss Regularization](https://openreview.net/forum?id=YwzxpZW3p7) |  | 0 | Regularizing neural networks is important for anticipating model behavior in regions of the data space that are not well represented. In this work, we propose a regularization technique for enforcing a level of smoothness in the mapping between the input space and the loss. We specify the level of... | Ali Hasan, Haoming Yang, Vahid Tarokh, Yuting Ng |  |
| 1298 |  |  [Optimizing Backward Policies in GFlowNets via Trajectory Likelihood Maximization](https://openreview.net/forum?id=Xj66fkrlTk) |  | 0 | Generative Flow Networks (GFlowNets) are a family of generative models that learn to sample objects with probabilities proportional to a given reward function. The key concept behind GFlowNets is the use of two stochastic policies: a forward policy, which incrementally constructs compositional... | Daniil Tiapkin, Nikita Morozov, Sergey Samsonov, Timofei Gritsaev |  |
| 1299 |  |  [Improving Reasoning Performance in Large Language Models via Representation Engineering](https://openreview.net/forum?id=IssPhpUsKt) |  | 0 | Recent advancements in large language models (LLMs) have resulted in increasingly anthropomorphic language concerning the ability of LLMs to reason. Whether \textit{reasoning} in LLMs should be understood to be inherently different is, however, widely debated. We propose utilizing a representation... | Bertram Højer, Oliver Simon Jarvis, Stefan Heinrich |  |
| 1300 |  |  [Improving Uncertainty Estimation through Semantically Diverse Language Generation](https://openreview.net/forum?id=HSi4VetQLj) |  | 0 | Large language models (LLMs) can suffer from hallucinations when generating text. These hallucinations impede various applications in society and industry by making LLMs untrustworthy. Current LLMs generate text in an autoregressive fashion by predicting and appending text tokens. When an LLM is... | Kajetan Schweighofer, Lukas Aichberger, Mykyta Ielanskyi, Sepp Hochreiter |  |
| 1301 |  |  [Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization](https://openreview.net/forum?id=CbfsKHiWEn) |  | 0 | This study addresses the challenge of noise in training datasets for Direct Preference Optimization (DPO), a method for aligning Large Language Models (LLMs) with human preferences. We categorize noise into pointwise noise, which includes low-quality data points, and pairwise noise, which... | Bolin Ding, Jiancan Wu, Jiawei Chen, Jinyang Gao, Junkang Wu, Xiang Wang, Xiangnan He, Yuexiang Xie, Zhengyi Yang |  |
| 1302 |  |  [PFGuard: A Generative Framework with Privacy and Fairness Safeguards](https://openreview.net/forum?id=8rbkePAapb) |  | 0 | Generative models must ensure both privacy and fairness for Trustworthy AI. While these goals have been pursued separately, recent studies propose to combine existing privacy and fairness techniques to achieve both goals. However, naively combining these techniques can be insufficient due to... | Geon Heo, Soyeon Kim, Steven Euijong Whang, Yuji Roh |  |
| 1303 |  |  [SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models](https://openreview.net/forum?id=9chRqsPOGL) |  | 0 | Instruction-following is a fundamental capability of language models, requiring the model to recognize even the most subtle requirements in the instructions and accurately reflect them in its output. Such an ability is well-suited for and often optimized by preference learning. However, existing... | Cunxiang Wang, Dan Zhang, Hongning Wang, Jiale Cheng, Jie Tang, Minlie Huang, Xiao Liu, Xiaotao Gu, Yida Lu, Yuxiao Dong |  |
| 1304 |  |  [EMMA: Empowering Multi-modal Mamba with Structural and Hierarchical Alignment](https://openreview.net/forum?id=Ev4iw23gdI) |  | 0 | Mamba-based architectures have shown to be a promising new direction for deep learning models owing to their competitive performance and sub-quadratic deployment speed. However, current Mamba multi-modal large language models (MLLM) are insufficient in extracting visual features, leading to... | Dongmei Jiang, Qingfang Zheng, Ruiping Wang, Wenjun Huang, Xiangyuan Lan, Yaowei Wang, Yifei Xing |  |
| 1305 |  |  [RAPID: Retrieval Augmented Training of Differentially Private Diffusion Models](https://openreview.net/forum?id=txZVQRc2ab) |  | 0 | Differentially private diffusion models (DPDMs) harness the remarkable generative capabilities of diffusion models while enforcing differential privacy (DP) for sensitive data. However, existing DPDM training approaches often suffer from significant utility loss, large memory footprint, and... | Changjiang Li, Fenglong Ma, Tanqiu Jiang, Ting Wang |  |
| 1306 |  |  [Exact Computation of Any-Order Shapley Interactions for Graph Neural Networks](https://openreview.net/forum?id=9tKC0YM8sX) |  | 0 | Albeit the ubiquitous use of Graph Neural Networks (GNNs) in machine learning (ML) prediction tasks involving graph-structured data, their interpretability remains challenging. In explainable artificial intelligence (XAI), the Shapley Value (SV) is the predominant method to quantify contributions... | Alessandro Sperduti, Barbara Hammer, Eyke Hüllermeier, Fabian Fumagalli, Janine Strotherm, Luca Hermes, Maximilian Muschalik, Paolo Frazzetto |  |
| 1307 |  |  [Physiome-ODE: A Benchmark for Irregularly Sampled Multivariate Time-Series Forecasting Based on Biological ODEs](https://openreview.net/forum?id=6ouZaBzeNO) |  | 0 | State-of-the-art methods for forecasting irregularly sampled time series with missing values predominantly rely on just four datasets and a few small toy examples for evaluation. While ordinary differential equations (ODE) are the prevalent models in science and engineering, a baseline model that... | Christian Klötergens, Lars SchmidtThieme, Maximilian Stubbemann, Randolf Scholz, Stefan Born, Vijaya Krishna Yalavarthi |  |
| 1308 |  |  [Neuron-based Multifractal Analysis of Neuron Interaction Dynamics in Large Models](https://openreview.net/forum?id=nt8gBX58Kh) |  | 0 | In recent years, there has been increasing attention on the capabilities of large-scale models, particularly in handling complex tasks that small-scale models are unable to perform. Notably, large language models (LLMs) have demonstrated \`\`intelligent'' abilities such as complex reasoning and... | Chenyu Zhou, Defu Cao, Heng Ping, Nikos Kanakaris, Paul Bogdan, Shixuan Li, Xiongye Xiao, Yaxing Li, Yizhuo Zhou |  |
| 1309 |  |  [Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts](https://openreview.net/forum?id=JSB171dSUU) |  | 0 | Adapting medical Large Language Models to local languages can reduce barriers to accessing healthcare services, but data scarcity remains a significant challenge, particularly for low-resource languages. To address this, we first construct a high-quality medical dataset and conduct analysis to... | Benyou Wang, Guorui Zheng, Juhao Liang, Nuo Chen, Xidong Wang, Yuping Zheng |  |
| 1310 |  |  [Multi-Resolution Decomposable Diffusion Model for Non-Stationary Time Series Anomaly Detection](https://openreview.net/forum?id=eWocmTQn7H) |  | 0 | Recently, generative models have shown considerable promise in unsupervised time series anomaly detection. Nonetheless, the task of effectively capturing complex temporal patterns and minimizing false alarms becomes increasingly challenging when dealing with non-stationary time series,... | Guojin Zhong, Jin Yuan, Long Chen, Pan Wang, Zhiyong Li |  |
| 1311 |  |  [Gumbel Counterfactual Generation From Language Models](https://openreview.net/forum?id=TUC0ZT2zIQ) |  | 0 | Understanding and manipulating the causal generation mechanisms in language models is essential for controlling their behavior. Previous work has primarily relied on techniques such as representation surgery---e.g., model ablations or manipulation of linear subspaces tied to specific concepts---to... | Anej Svete, Ryan Cotterell, Shauli Ravfogel, Vésteinn Snæbjarnarson |  |
| 1312 |  |  [Decoupled Subgraph Federated Learning](https://openreview.net/forum?id=v1rFkElnIn) |  | 0 | We address the challenge of federated learning on graph-structured data distributed across multiple clients. Specifically, we focus on the prevalent scenario of interconnected subgraphs, where inter-connections between different clients play a critical role. We present a novel framework for this... | Alexandre Graell i Amat, Javad Aliakbari, Johan Östman |  |
| 1313 |  |  [Decoupled Finetuning for Domain Generalizable Semantic Segmentation](https://openreview.net/forum?id=qZEdmyqCHF) |  | 0 | Joint finetuning of a pretrained encoder and a randomly initialized decoder has been the de facto standard in semantic segmentation, but the vulnerability of this approach to domain shift has not been studied. We investigate the vulnerability issue of joint finetuning, and propose a novel... | Donghyeon Kwon, Jaehyun Pahk, Seong Joon Oh, Suha Kwak |  |
| 1314 |  |  [Magnetic Preference Optimization: Achieving Last-iterate Convergence for Language Model Alignment](https://openreview.net/forum?id=PDnEDS244P) |  | 0 | Self-play methods have demonstrated remarkable success in enhancing model capabilities across various domains. In the context of Reinforcement Learning from Human Feedback (RLHF), self-play not only boosts Large Language Model (LLM) performance but also overcomes the limitations of traditional... | Chengdong Ma, Jiancong Xiao, Jing Huo, Linjian Meng, Mingzhi Wang, Qizhi Chen, Weijie J. Su, Yang Han, Yaodong Yang, Zhaowei Zhang |  |
| 1315 |  |  [Cached Multi-Lora Composition for Multi-Concept Image Generation](https://openreview.net/forum?id=4iFSBgxvIO) |  | 0 | Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in text-to-image models, enabling precise rendering of multiple distinct elements, such as characters and styles, in multi-concept image generation. However, current approaches face significant challenges when composing these... | ChristosSavvas Bouganis, Mingzhu Shen, Xiandong Zou, Yiren Zhao |  |
| 1316 |  |  [The Effectiveness of Curvature-Based Rewiring and the Role of Hyperparameters in GNNs Revisited](https://openreview.net/forum?id=EcrdmRT99M) |  | 0 | Message passing is the dominant paradigm in Graph Neural Networks (GNNs). The efficiency of message passing, however, can be limited by the topology of the graph. This happens when information is lost during propagation due to being oversquashed when travelling through bottlenecks. To remedy this,... | Floriano Tori, Vincent Ginis, Vincent Holst |  |
| 1317 |  |  [Utilitarian Algorithm Configuration for Infinite Parameter Spaces](https://openreview.net/forum?id=CA06Nqa7CG) |  | 0 | Utilitarian algorithm configuration is a general-purpose technique for automatically searching the parameter space of a given algorithm to optimize its performance, as measured by a given utility function, on a given set of inputs. Recently introduced utilitarian configuration procedures offer... | Devon R. Graham, Kevin LeytonBrown |  |
| 1318 |  |  [Triples as the Key: Structuring Makes Decomposition and Verification Easier in LLM-based TableQA](https://openreview.net/forum?id=UwcZEoNP19) |  | 0 | As the mainstream approach, LLMs have been widely applied and researched in TableQA tasks. Currently, the core of LLM-based TableQA methods typically include three phases: question decomposition, sub-question TableQA reasoning, and answer verification. However, several challenges remain in this... | Jie Chen, Minghan Zhang, Shu Zhao, Wei Du, Zhen Duan, Zhen Yang, Ziwei Du |  |
| 1319 |  |  [Words in Motion: Extracting Interpretable Control Vectors for Motion Transformers](https://openreview.net/forum?id=J9eKm7j6KD) |  | 0 | Transformer-based models generate hidden states that are difficult to interpret. In this work, we analyze hidden states and modify them at inference, with a focus on motion forecasting. We use linear probing to analyze whether interpretable features are embedded in hidden states. Our experiments... | Royden Wagner, Ömer Sahin Tas |  |
| 1320 |  |  [ProtPainter: Draw or Drag Protein via Topology-guided Diffusion](https://openreview.net/forum?id=Nq7yKYL0Bp) |  | 0 | Recent advances in protein backbone generation have achieved promising results under structural, functional, or physical constraints. However, existing methods lack the flexibility for precise topology control, limiting navigation of the backbone space. We present $\textbf{ProtPainter}$, a... | Min Zhang, Shizhuo Cheng, Tintin Jiang, Yan Zhang, Zhengxi Lu |  |
| 1321 |  |  [KinPFN: Bayesian Approximation of RNA Folding Kinetics using Prior-Data Fitted Networks](https://openreview.net/forum?id=E1m5yGMOiV) |  | 0 | RNA is a dynamic biomolecule crucial for cellular regulation, with its function largely determined by its folding into complex structures, while misfolding can lead to multifaceted biological sequelae. During the folding process, RNA traverses through a series of intermediate structural states,... | Christoph Flamm, Dominik Scheuer, Frank Hutter, Frederic Runge, Jörg K. H. Franke, Michael T. Wolfinger |  |
| 1322 |  |  [Sketch2Diagram: Generating Vector Diagrams from Hand-Drawn Sketches](https://openreview.net/forum?id=KvaDHPhhir) |  | 0 | We address the challenge of automatically generating high-quality vector diagrams from hand-drawn sketches. Vector diagrams are essential for communicating complex ideas across various fields, offering flexibility and scalability. While recent research has progressed in generating diagrams from... | Haruto Yoshida, Itsumi Saito, Keisuke Sakaguchi |  |
| 1323 |  |  [Agents' Room: Narrative Generation through Multi-step Collaboration](https://openreview.net/forum?id=HfWcFs7XLR) |  | 0 | Writing compelling fiction is a multifaceted process combining elements such as crafting a plot, developing interesting characters, and using evocative language. While large language models (LLMs) show promise for story writing, they currently rely heavily on intricate prompting, which limits their... | Alice Shoshana Jakobovits, Elizabeth Clark, Fantine Huot, Jennimaria Palomaki, Mirella Lapata, Reinald Kim Amplayo |  |
| 1324 |  |  [How Does Vision-Language Adaptation Impact the Safety of Vision Language Models?](https://openreview.net/forum?id=eXB5TCrAu9) |  | 0 | Vision-Language adaptation (VL adaptation) transforms Large Language Models (LLMs) into Large Vision-Language Models (LVLMs) for multimodal tasks, but this process often compromises the inherent safety capabilities embedded in the original LLMs. Despite potential harmfulness due to weakened safety... | Geewook Kim, Hoyeon Chang, Hyunji Lee, Jiyeon Kim, Minjoon Seo, Seongyun Lee, Sue Hyun Park |  |
| 1325 |  |  [CR-CTC: Consistency regularization on CTC for improved speech recognition](https://openreview.net/forum?id=CIs9x2ZRgh) |  | 0 | Connectionist Temporal Classification (CTC) is a widely used method for automatic speech recognition (ASR), renowned for its simplicity and computational efficiency. However, it often falls short in recognition performance. In this work, we propose the Consistency-Regularized CTC (CR-CTC), which... | Daniel Povey, Fangjun Kuang, Han Zhu, Liyong Guo, Long Lin, Wei Kang, Xiaoyu Yang, Zengrui Jin, Zengwei Yao, Zhaoqing Li |  |
| 1326 |  |  [ChroKnowledge: Unveiling Chronological Knowledge of Language Models in Multiple Domains](https://openreview.net/forum?id=whaO3482bs) |  | 0 | Large language models (LLMs) have brought significant changes to many aspects of our lives. However, assessing and ensuring their chronological knowledge remains challenging. Existing approaches fall short in addressing the temporal adaptability of knowledge, often relying on a fixed time-point... | Chanwoong Yoon, Donghyeon Lee, Jaewoo Kang, Jungwoo Park, Minbyul Jeong, Yein Park |  |
| 1327 |  |  [Equivariant Denoisers Cannot Copy Graphs: Align Your Graph Diffusion Models](https://openreview.net/forum?id=onIro14tHv) |  | 0 | Graph diffusion models, dominant in graph generative modeling, remain underexplored for graph-to-graph translation tasks like chemical reaction prediction. We demonstrate that standard permutation equivariant denoisers face fundamental limitations in these tasks due to their inability to break... | Arno Solin, Markus Heinonen, Najwa Laabid, Severi Rissanen, Vikas Garg |  |
| 1328 |  |  [Influence-Guided Diffusion for Dataset Distillation](https://openreview.net/forum?id=0whx8MhysK) |  | 0 | Dataset distillation aims to streamline the training process by creating a compact yet effective dataset for a much larger original dataset. However, existing methods often struggle with distilling large, high-resolution datasets due to prohibitive resource costs and limited performance, primarily... | Bo Huang, Jiawei Du, Mingyang Chen, Wei Wang, Xiaobo Zhang, Yi Wang |  |
| 1329 |  |  [ELICIT: LLM Augmentation Via External In-context Capability](https://openreview.net/forum?id=CI4sCBMXjP) |  | 0 | Enhancing the adaptive capabilities of large language models is a critical pursuit in both research and application. Traditional fine-tuning methods require substantial data, computational resources, and specific capabilities, while in-context learning is limited by the need for appropriate... | Futing Wang, Jianhao Yan, Tao Lin, Yue Zhang |  |
| 1330 |  |  [RecFlow: An Industrial Full Flow Recommendation Dataset](https://openreview.net/forum?id=vVHc8bGRns) |  | 0 | Industrial recommendation systems (RS) rely on the multi-stage pipeline to balance effectiveness and efficiency when delivering items from a vast corpus to users. Existing RS benchmark datasets primarily focus on the exposure space, where novel RS algorithms are trained and evaluated. However, when... | Bing Han, Defu Lian, Guorui Zhou, Han Li, Hongning Wang, Kai Zheng, Kun Gai, Kuo Cai, Na Mou, Qi Liu, Rui Huang, Wentian Bao, Wuchao Li, Yanan Niu, Yang Song, Yiqun Hui, Yuan Chai, Yunen Yu |  |
| 1331 |  |  [Offline Hierarchical Reinforcement Learning via Inverse Optimization](https://openreview.net/forum?id=dTPz4rEDok) |  | 0 | Hierarchical policies enable strong performance in many sequential decision-making problems, such as those with high-dimensional action spaces, those requiring long-horizon planning, and settings with sparse rewards. However, learning hierarchical policies from static offline datasets presents a... | Carolin Schmidt, Daniele Gammelli, Filipe Rodrigues, James Harrison, Marco Pavone |  |
| 1332 |  |  [QA-Calibration of Language Model Confidence Scores](https://openreview.net/forum?id=D2hhkU5O48) |  | 0 | To use generative question-and-answering (QA) systems for decision-making and in any critical application, these systems need to provide well-calibrated confidence scores that reflect the correctness of their answers. Existing calibration methods aim to ensure that the confidence score is, \*on... | Aaditya Ramdas, AtalantiAnastasia Mastakouri, Elke Kirschbaum, Putra Manggala, Shiva Prasad Kasiviswanathan |  |
| 1333 |  |  [ARLON: Boosting Diffusion Transformers with Autoregressive Models for Long Video Generation](https://openreview.net/forum?id=8pusxkLEQO) |  | 0 | Text-to-video (T2V) models have recently undergone rapid and substantial advancements. Nevertheless, due to limitations in data and computational resources, achieving efficient generation of long videos with rich motion dynamics remains a significant challenge. To generate high-quality, dynamic,... | Furu Wei, Hefei Ling, Jeongsoo Choi, Jinyu Li, Lingwei Meng, Long Zhou, Shujie Hu, Shujie Liu, Xun Guo, Zongyi Li |  |
| 1334 |  |  [Rethinking Shapley Value for Negative Interactions in Non-convex Games](https://openreview.net/forum?id=b24n2LS2BJ) |  | 0 | We study causal interactions for payoff allocation in cooperative game theory, including quantifying feature attribution for deep learning models. Most feature attribution methods mainly stem from the criteria of the Shapley value, which assigns fair payoffs to players based on their expected... | Jaesik Choi, Myeongjin Lee, Wonjoon Chang |  |
| 1335 |  |  [CO-MOT: Boosting End-to-end Transformer-based Multi-Object Tracking via Coopetition Label Assignment and Shadow Sets](https://openreview.net/forum?id=0ov0dMQ3mN) |  | 0 | Existing end-to-end Multi-Object Tracking (e2e-MOT) methods have not surpassed non-end-to-end tracking-by-detection methods. One possible reason lies in the training label assignment strategy that consistently binds the tracked objects with tracking queries and assigns few newborns to detection... | Feng Yan, Lin Ma, Weixin Luo, Yiyang Gan, Yujie Zhong |  |
| 1336 |  |  [What's New in My Data? Novelty Exploration via Contrastive Generation](https://openreview.net/forum?id=IZDiRbVSVN) |  | 0 | Fine-tuning is widely used to adapt language models for specific goals, often leveraging real-world data such as patient records, customer-service interactions, or web content in languages not covered in pre-training. These datasets are typically massive, noisy, and often confidential, making their... | Ivan Titov, Masaru Isonuma |  |
| 1337 |  |  [Durable Quantization Conditioned Misalignment Attack on Large Language Models](https://openreview.net/forum?id=41uZB8bDFh) |  | 0 | As large language models (LLMs) are increasingly deployed on resource-constrained edge devices, quantization techniques have been widely adopted to reduce model size and computational requirements. However, this process can expose models to new vulnerabilities. In this work, we introduce the... | Haowei Li, Peiran Dong, Song Guo |  |
| 1338 |  |  [Node Identifiers: Compact, Discrete Representations for Efficient Graph Learning](https://openreview.net/forum?id=t9lS1lX9FQ) |  | 0 | We present a novel end-to-end framework that generates highly compact (typically 6-15 dimensions), discrete (int4 type), and interpretable node representations—termed node identifiers (node IDs)—to tackle inference challenges on large-scale graphs. By employing vector quantization, we compress... | Hongkang Li, Lei Shi, Qijiong Liu, XiaoMing Wu, Yuankai Luo |  |
| 1339 |  |  [UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation](https://openreview.net/forum?id=eLLBILFRsA) |  | 0 | We present UniDetox, a universally applicable method designed to mitigate toxicity across various large language models (LLMs). Previous detoxification methods are typically model-specific, addressing only individual models or model families, and require careful hyperparameter tuning due to the... | Huimin Lu, Ichiro Sakata, Junichiro Mori, Masaru Isonuma |  |
| 1340 |  |  [MrSteve: Instruction-Following Agents in Minecraft with What-Where-When Memory](https://openreview.net/forum?id=CjXaMI2kUH) |  | 0 | Significant advances have been made in developing general-purpose embodied AI in environments like Minecraft through the adoption of LLM-augmented hierarchical approaches. While these approaches, which combine high-level planners with low-level controllers, show promise, low-level controllers... | Junmo Cho, Junyeong Park, Sungjin Ahn |  |
| 1341 |  |  [Optimality and Adaptivity of Deep Neural Features for Instrumental Variable Regression](https://openreview.net/forum?id=ReItdfwMcg) |  | 0 | We provide a convergence analysis of \emph{deep feature instrumental variable} (DFIV) regression (Xu et al., 2021), a nonparametric approach to IV regression using data-adaptive features learned by deep neural networks in two stages. We prove that the DFIV algorithm achieves the minimax optimal... | Arthur Gretton, Dimitri Meunier, Juno Kim, Taiji Suzuki, Zhu Li |  |
| 1342 |  |  [Addressing Label Shift in Distributed Learning via Entropy Regularization​](https://openreview.net/forum?id=kuYxecnlv2) |  | 0 | We address the challenge of minimizing "true risk" in multi-node distributed learning.\footnote{We use the term node to refer to a client, FPGA, APU, CPU, GPU, or worker.} These systems are frequently exposed to both inter-node and intra-node "label shifts", which present a critical obstacle to... | Ali RamezaniKebrya, Changkyu Choi, Volkan Cevher, Xiangcheng Cao, Zhiyuan Wu |  |
| 1343 |  |  [Dreamweaver: Learning Compositional World Models from Pixels](https://openreview.net/forum?id=e5mTvjXG9u) |  | 0 | Humans have an innate ability to decompose their perceptions of the world into objects and their attributes, such as colors, shapes, and movement patterns. This cognitive process enables us to imagine novel futures by recombining familiar concepts. However, replicating this ability in artificial... | Gautam Singh, Junyeob Baek, Sungjin Ahn, YiFu Wu |  |
| 1344 |  |  [Rare event modeling with self-regularized normalizing flows: what can we learn from a single failure?](https://openreview.net/forum?id=gQoBw7sGAu) |  | 0 | Increased deployment of autonomous systems in fields like transportation and robotics have seen a corresponding increase in safety-critical failures. These failures can be difficult to model and debug due to the relative lack of data: compared to tens of thousands of examples from normal... | Charles Dawson, Chuchu Fan, Max Z. Li, Van Tran |  |
| 1345 |  |  [Projection Head is Secretly an Information Bottleneck](https://openreview.net/forum?id=L0evcuybH5) |  | 0 | Recently, contrastive learning has risen to be a promising paradigm for extracting meaningful data representations. Among various special designs, adding a projection head on top of the encoder during training and removing it for downstream tasks has proven to significantly enhance the performance... | Kaiwen Hu, Qi Zhang, Yifei Wang, Yisen Wang, Zhuo Ouyang |  |
| 1346 |  |  [Schur's Positive-Definite Network: Deep Learning in the SPD cone with structure](https://openreview.net/forum?id=v1B4aet9ct) |  | 0 | Estimating matrices in the symmetric positive-definite (SPD) cone is of interest for many applications ranging from computer vision to graph learning. While there exist various convex optimization-based estimators, they remain limited in expressivity due to their model-based approach. The success... | Can Pouliquen, Mathurin Massias, Titouan Vayer |  |
| 1347 |  |  [SFESS: Score Function Estimators for k-Subset Sampling](https://openreview.net/forum?id=q87GUkdQBm) |  | 0 | Are score function estimators a viable approach to learning with $k$-subset sampling? Sampling $k$-subsets is a fundamental operation that is not amenable to differentiable parametrization, impeding gradient-based optimization. Previous work has favored approximate pathwise gradients or relaxed... | Hossein Azizpour, Klas Wijk, Ricardo Vinuesa |  |
| 1348 |  |  [ConcreTizer: Model Inversion Attack via Occupancy Classification and Dispersion Control for 3D Point Cloud Restoration](https://openreview.net/forum?id=I4iZmsV4HM) |  | 0 | The growing use of 3D point cloud data in autonomous vehicles (AVs) has raised serious privacy concerns, particularly due to the sensitive information that can be extracted from 3D data. While model inversion attacks have been widely studied in the context of 2D data, their application to 3D point... | HyungSin Kim, Saewoong Bahk, Sunwook Hwang, Youngseok Kim |  |
| 1349 |  |  [Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models](https://openreview.net/forum?id=CbpWPbYHuv) |  | 0 | Transformers have found extensive applications across various domains due to their powerful fitting capabilities. This success can be partially attributed to their inherent nonlinearity. Thus, in addition to the ReLU function employed in the original transformer architecture, researchers have... | Jinwen Ma, Xiaoqing Li, Xun Zhou, Ya Wang, Yutao Zeng, Zhijian Zhuo |  |
| 1350 |  |  [ParamΔ for Direct Mixing: Post-Train Large Language Model At Zero Cost](https://openreview.net/forum?id=vqbd2OQnGp) |  | 0 | The post-training phase of large language models is essential for enhancing capabilities such as instruction-following, reasoning, and alignment with human preferences. However, it demands extensive high-quality data and poses risks like overfitting, alongside significant computational costs due to... | Karthik Prasad, Mingrui Wu, Sheng Cao, Yuandong Tian, Zechun Liu |  |
| 1351 |  |  [A Theory of Initialisation's Impact on Specialisation](https://openreview.net/forum?id=RQz7szbVDs) |  | 0 | Prior work has demonstrated a consistent tendency in neural networks engaged in continual learning tasks, wherein intermediate task similarity results in the highest levels of catastrophic interference. This phenomenon is attributed to the network's tendency to reuse learned features across tasks.... | Andrew M. Saxe, Clémentine Carla Juliette Dominé, Devon Jarvis, Sebastian Lee, Stefano Sarao Mannelli |  |
| 1352 |  |  [Manifolds, Random Matrices and Spectral Gaps: The geometric phases of generative diffusion](https://openreview.net/forum?id=KlN00vQEY2) |  | 0 | In this paper, we investigate the latent geometry of generative diffusion models under the manifold hypothesis. For this purpose, we analyze the spectrum of eigenvalues (and singular values) of the Jacobian of the score function, whose discontinuities (gaps) reveal the presence and dimensionality... | Beatrice Achilli, Carlo Lucibello, Enrico Ventura, Gianluigi Silvestri, Luca Ambrogioni |  |
| 1353 |  |  [AgentSquare: Automatic LLM Agent Search in Modular Design Space](https://openreview.net/forum?id=mPdmDYIQ7f) |  | 0 | Recent advancements in Large Language Models (LLMs) have led to a rapid growth of agentic systems capable of handling a wide range of complex tasks. However, current research largely relies on manual, task-specific design, limiting their adaptability to novel tasks. In this paper, we introduce a... | Fengli Xu, Jiahe Liu, Keyu Zhao, Likai Ma, Yong Li, Yu Li, Yu Shang |  |
| 1354 |  |  [Multi-Task Corrupted Prediction for Learning Robust Audio-Visual Speech Representation](https://openreview.net/forum?id=WEQL5ksDnB) |  | 0 | Audio-visual speech recognition (AVSR) incorporates auditory and visual modalities to improve recognition accuracy, particularly in noisy environments where audio-only speech systems are insufficient. While previous research has largely addressed audio disruptions, few studies have dealt with... | Kangwook Jang, Sangmin Bae, SeYoung Yun, Sungnyun Kim, Sungwoo Cho |  |
| 1355 |  |  [Size-Generalizable RNA Structure Evaluation by Exploring Hierarchical Geometries](https://openreview.net/forum?id=QaTBHSqmH9) |  | 0 | Understanding the 3D structure of RNA is essential for deciphering its function and developing RNA-based therapeutics. Geometric Graph Neural Networks (GeoGNNs) that conform to the $\mathrm{E}(3)$-symmetry have advanced RNA structure evaluation, a crucial step toward RNA structure prediction.... | Jiacheng Cen, Le Song, Taifeng Wang, Wenbing Huang, Zongzhao Li |  |
| 1356 |  |  [Semi-Supervised Vision-Centric 3D Occupancy World Model for Autonomous Driving](https://openreview.net/forum?id=rCX9l4OTCT) |  | 0 | Understanding world dynamics is crucial for planning in autonomous driving. Recent methods attempt to achieve this by learning a 3D occupancy world model that forecasts future surrounding scenes based on current observation. However, 3D occupancy labels are still required to produce promising... | Pengfei Li, Wei Sun, Xiang Li, Yan Wang, Yilun Chen, Yupeng Zheng |  |
| 1357 |  |  [Efficient Interpolation between Extragradient and Proximal Methods for Weak MVIs](https://openreview.net/forum?id=Y7slJZPGCy) |  | 0 | We study nonmonotone games satisfying the weak Minty variational inequality (MVI) with parameter $\rho \in (-\tfrac{1}{L}, \infty)$, where $L$ is the Lipschitz constant of the gradient operator. An error corrected version of the inexact proximal point algorithm is proposed, with which we establish... | Ioannis Mavrothalassitis, Thomas Pethick, Volkan Cevher |  |
| 1358 |  |  [Generative Flows on Synthetic Pathway for Drug Design](https://openreview.net/forum?id=pB1XSj2y4X) |  | 0 | Generative models in drug discovery have recently gained attention as efficient alternatives to brute-force virtual screening. However, most existing models do not account for synthesizability, limiting their practical use in real-world scenarios. In this paper, we propose RxnFlow, which... | Jinkyoo Park, Martin Ester, Minsu Kim, Seonghwan Seo, Sungsoo Ahn, Tony Shen, Woo Youn Kim |  |
| 1359 |  |  [Integrating Protein Dynamics into Structure-Based Drug Design via Full-Atom Stochastic Flows](https://openreview.net/forum?id=9qS3HzSDNv) |  | 0 | The dynamic nature of proteins, influenced by ligand interactions, is essential for comprehending protein function and progressing drug discovery. Traditional structure-based drug design (SBDD) approaches typically target binding sites with rigid structures, limiting their practical application in... | Feng Zhou, Haowei Lin, Jianzhu Ma, Jiaqi Guan, Liang Wang, Qiang Liu, Xiangxin Zhou, Xinheng He, Yang Wang, Yi Xiao |  |
| 1360 |  |  [InstantSplamp: Fast and Generalizable Stenography Framework for Generative Gaussian Splatting](https://openreview.net/forum?id=xvhV3LvYTc) |  | 0 | With the rapid development of large generative models for 3D, especially the evolution from NeRF representations to more efficient Gaussian Splatting, the synthesis of 3D assets has become increasingly fast and efficient, enabling the large-scale publication and sharing of generated 3D objects.... | Chenxin Li, Hengyu Liu, Panwang Pan, Wuyang Li, Yifan Liu, Yixuan Yuan, Zhiwen Fan |  |
| 1361 |  |  [Semantics-Adaptive Activation Intervention for LLMs via Dynamic Steering Vectors](https://openreview.net/forum?id=8WQ7VTfPTl) |  | 0 | Large language models (LLMs) have achieved remarkable performance across many tasks, yet aligning them with desired behaviors remains challenging. Activation intervention has emerged as an effective and economical method to modify the behavior of LLMs. Despite considerable interest in this area,... | Jingyuan Yang, Wei Peng, Weixuan Wang |  |
| 1362 |  |  [ADAM Optimization with Adaptive Batch Selection](https://openreview.net/forum?id=BZrSCv2SBq) |  | 0 | Adam is a widely used optimizer in neural network training due to its adaptive learning rate. However, because different data samples influence model updates to varying degrees, treating them equally can lead to inefficient convergence. To address this, a prior work proposed adapting the sampling... | GyuYeol Kim, Minhwan Oh |  |
| 1363 |  |  [Tailoring Mixup to Data for Calibration](https://openreview.net/forum?id=3ygfMPLv0P) |  | 0 | Among all data augmentation techniques proposed so far, linear interpolation of training samples, also called Mixup, has found to be effective for a large panel of applications. Along with improved predictive performance, Mixup is also a good technique for improving calibration. However, mixing... | Florence d'AlchéBuc, Pavlo Mozharovskyi, Quentin Bouniot |  |
| 1364 |  |  [MIRACLE 3D: Memory-efficient Integrated Robust Approach for Continual Learning on 3D Point Clouds via Shape Model Construction](https://openreview.net/forum?id=ANBuEJesgx) |  | 0 | In this paper, we introduce a novel framework for memory-efficient and privacy-preserving continual learning in 3D object classification. Unlike conventional memory-based approaches in continual learning that require storing numerous exemplars, our method constructs a compact shape model for each... | Behrooz Nasihatkon, Hossein Resani |  |
| 1365 |  |  [Facilitating Multi-turn Function Calling for LLMs via Compositional Instruction Tuning](https://openreview.net/forum?id=owP2mymrTD) |  | 0 | Large Language Models (LLMs) have exhibited significant potential in performing diverse tasks, including the ability to call functions or use external tools to enhance their performance. While current research on function calling by LLMs primarily focuses on single-turn interactions, this paper... | Bin Cui, Fan Yang, Hao Liang, Haoze Sun, Keer Lu, Mingyang Chen, Tianpeng Li, Weipeng Chen, Wentao Zhang, Zenan Zhou |  |
| 1366 |  |  [SafeWatch: An Efficient Safety-Policy Following Video Guardrail Model with Transparent Explanations](https://openreview.net/forum?id=xjKz6IxgCX) |  | 0 | With the rise of generative AI and rapid growth of high-quality video generation, video guardrails have become more crucial than ever to ensure safety and security across platforms. Current video guardrails, however, are either overly simplistic, relying on pure classification models trained on... | Bo Li, Francesco Pinto, Minzhou Pan, Zhaorun Chen |  |
| 1367 |  |  [TRENDy: Temporal Regression of Effective Nonlinear Dynamics](https://openreview.net/forum?id=NvDRvtrGLo) |  | 0 | Spatiotemporal dynamics pervade the natural sciences, from the morphogen dynamics underlying patterning in animal pigmentation to the protein waves controlling cell division. A central challenge lies in understanding how controllable parameters induce qualitative changes in system behavior called... | Guy Pelc, Matthew Ricci, Mor Nitzan, Noa Moriel, Zoe Piran |  |
| 1368 |  |  [On Bits and Bandits: Quantifying the Regret-Information Trade-off](https://openreview.net/forum?id=0oWGVvC6oq) |  | 0 | In many sequential decision problems, an agent performs a repeated task. He then suffers regret and obtains information that he may use in the following rounds. However, sometimes the agent may also obtain information and avoid suffering regret by querying external sources. We study the trade-off... | Itai Shufaro, Nadav Merlis, Nir Weinberger, Shie Mannor |  |
| 1369 |  |  [SEBRA : Debiasing through Self-Guided Bias Ranking](https://openreview.net/forum?id=MyVC4X5B2X) |  | 0 | Ranking samples by fine-grained estimates of spuriosity (the degree to which spurious cues are present) has recently been shown to significantly benefit bias mitigation, over the traditional binary biased-vs-unbiased partitioning of train sets. However, this spuriousity ranking comes with the... | Abhra Chaudhuri, Adarsh Kappiyath, Ajay Kumar Jaiswal, Lu Yin, Xiatian Zhu, Yunpeng Li, Ziquan Liu |  |
| 1370 |  |  [Lambda-Skip Connections: the architectural component that prevents Rank Collapse](https://openreview.net/forum?id=1yJP5TVWih) |  | 0 | Rank collapse, a phenomenon where embedding vectors in sequence models rapidly converge to a uniform token or equilibrium state, has recently gained at- tention in the deep learning literature. This phenomenon leads to reduced expres- sivity and potential training instabilities due to vanishing... | Carmen Amo Alonso, Federico Arangath Joseph, Jerome Sieber, Melanie Nicole Zeilinger |  |
| 1371 |  |  [Foundation Models Secretly Understand Neural Network Weights: Enhancing Hypernetwork Architectures with Foundation Models](https://openreview.net/forum?id=cADpvQgnqg) |  | 0 | Large pre-trained models, or foundation models, have shown impressive performance when adapted to a variety of downstream tasks, often out-performing specialized models. Hypernetworks, neural networks that generate some or all of the parameters of another neural network, have become an increasingly... | Jeffrey Gu, Serena YeungLevy |  |
| 1372 |  |  [Mini-Monkey: Alleviating the Semantic Sawtooth Effect for Lightweight MLLMs via Complementary Image Pyramid](https://openreview.net/forum?id=71XtUhazG0) |  | 0 | Recently, scaling images to high resolution has received much attention in multimodal large language models (MLLMs). Most existing practices adopt a sliding-window-style cropping strategy to adapt to resolution increase. Such a cropping strategy, however, can easily cut off objects and connected... | Dingkang Liang, Lianwen Jin, Mingxin Huang, Xiang Bai, Yuliang Liu |  |
| 1373 |  |  [Multi-objective antibody design with constrained preference optimization](https://openreview.net/forum?id=4ktJJBvvUd) |  | 0 | Antibody design is crucial for developing therapies against diseases such as cancer and viral infections. Recent deep generative models have significantly advanced computational antibody design, particularly in enhancing binding affinity to target antigens. However, beyond binding affinity,... | Haicang Zhang, Milong Ren, ZaiKai He |  |
| 1374 |  |  [NL-Eye: Abductive NLI For Images](https://openreview.net/forum?id=2zmO1GVT0Y) |  | 0 | Will a Visual Language Model (VLM)-based bot warn us about slipping if it detects a wet floor? Recent VLMs have demonstrated impressive capabilities, yet their ability to infer outcomes and causes remains underexplored. To address this, we introduce NL-Eye, a benchmark designed to assess VLMs'... | Michael Toker, Mor Ventura, Nitay Calderon, Roi Reichart, Yonatan Bitton, Zorik Gekhman |  |
| 1375 |  |  [COFlowNet: Conservative Constraints on Flows Enable High-Quality Candidate Generation](https://openreview.net/forum?id=tXUkT709OJ) |  | 0 | Generative flow networks (GFlowNets) have been considered as powerful tools for generating candidates with desired properties. Given that evaluating the property of candidates can be complex and time-consuming, existing GFlowNets train proxy models for efficient online evaluation. However, the... | Chen Zhang, Pengkun Wang, Xu Wang, Xuan Yu, Yang Wang, Yudong Zhang, Zhaoyang Sun |  |
| 1376 |  |  [Curriculum-aware Training for Discriminating Molecular Property Prediction Models](https://openreview.net/forum?id=6DHIkLv5i3) |  | 0 | Despite their wide application across various fields, current molecular property prediction models struggle with the challenge of activity cliff, which refers to the situation where molecules with similar chemical structures display remarkable different properties. This phenomenon hinders existing... | Hansi Yang, James Kwok, Quanming Yao |  |
| 1377 |  |  [Does Safety Training of LLMs Generalize to Semantically Related Natural Prompts?](https://openreview.net/forum?id=LO4MEPoqrG) |  | 0 | Large Language Models (LLMs) are known to be susceptible to crafted adversarial attacks or jailbreaks that lead to the generation of objectionable content despite being aligned to human preferences using safety fine-tuning methods. While the large dimensionality of input token space makes it... | Arun Sai Suggala, Karthikeyan Shanmugam, Prateek Jain, Sravanti Addepalli, Yerram Varun |  |
| 1378 |  |  [Neural Context Flows for Meta-Learning of Dynamical Systems](https://openreview.net/forum?id=8vzMLo8LDN) |  | 0 | Neural Ordinary Differential Equations (NODEs) often struggle to adapt to new dynamic behaviors caused by parameter changes in the underlying physical system, even when these dynamics are similar to previously observed behaviors. This problem becomes more challenging when the changing parameters... | David A. W. Barton, Roussel Desmond Nzoyem, Tom Deakin |  |
| 1379 |  |  [Density estimation with LLMs: a geometric investigation of in-context learning trajectories](https://openreview.net/forum?id=semTHoVGsJ) |  | 0 | Large language models (LLMs) demonstrate remarkable emergent abilities to perform in-context learning across various tasks, including time series forecasting. This work investigates LLMs' ability to estimate probability density functions (PDFs) from data observed in-context; such density estimation... | Christopher J. Earls, Nicolas Boullé, Raphaël Sarfati, Toni J. B. Liu |  |
| 1380 |  |  [Multimodal Quantitative Language for Generative Recommendation](https://openreview.net/forum?id=v7YrIjpkTF) |  | 0 | Generative recommendation has emerged as a promising paradigm aiming at directly generating the identifiers of the target candidates. Most existing methods attempt to leverage prior knowledge embedded in Pre-trained Language Models (PLMs) to improve the recommendation performance. However, they... | ChangDong Wang, Feidiao Yang, Hui Li, Jianyang Zhai, Xiawu Zheng, Yonghong Tian, ZiFeng Mai |  |
| 1381 |  |  [Coreset Spectral Clustering](https://openreview.net/forum?id=1qgZXeMTTU) |  | 0 | Coresets have become an invaluable tool for solving $k$-means and kernel $k$-means clustering problems on large datasets with small numbers of clusters. On the other hand, spectral clustering works well on sparse graphs and has recently been extended to scale efficiently to large numbers of... | Ben Jourdan, Gregory Schwartzman, He Sun, Peter Macgregor |  |
| 1382 |  |  [Disentangled Representation Learning with the Gromov-Monge Gap](https://openreview.net/forum?id=ehr4oTe6XI) |  | 0 | Learning disentangled representations from unlabelled data is a fundamental challenge in machine learning. Solving it may unlock other problems, such as generalization, interpretability, or fairness. Although remarkably challenging to solve in theory, disentanglement is often achieved in practice... | Fabian J. Theis, Karsten Roth, Luca Eyring, Marco Cuturi, Théo Uscidda, Zeynep Akata |  |
| 1383 |  |  [Divergence-enhanced Knowledge-guided Context Optimization for Visual-Language Prompt Tuning](https://openreview.net/forum?id=6wOmHdwCC4) |  | 0 | Prompt tuning vision-language models like CLIP has shown great potential in learning transferable representations for various downstream tasks. The main issue is how to mitigate the over-fitting problem on downstream tasks with limited training samples. While knowledge-guided context optimization... | Miaomiao Cheng, Wei Song, Xu Han, Yilun Li |  |
| 1384 |  |  [CheapNet: Cross-attention on Hierarchical representations for Efficient protein-ligand binding Affinity Prediction](https://openreview.net/forum?id=A1HhtITVEi) |  | 0 | Accurately predicting protein-ligand binding affinity is a critical challenge in drug discovery, crucial for understanding drug efficacy. While existing models typically rely on atom-level interactions, they often fail to capture the complex, higher-order interactions, resulting in noise and... | Hyukjun Lim, Sangseon Lee, Sun Kim |  |
| 1385 |  |  [A Riemannian Framework for Learning Reduced-order Lagrangian Dynamics](https://openreview.net/forum?id=RoN6M3i7gJ) |  | 0 | By incorporating physical consistency as inductive bias, deep neural networks display increased generalization capabilities and data efficiency in learning nonlinear dynamic models. However, the complexity of these models generally increases with the system dimensionality, requiring larger... | Danica Kragic, Jens Lundell, Katharina Friedl, Noémie Jaquier, Tamim Asfour |  |
| 1386 |  |  [CARTS: Advancing Neural Theorem Proving with Diversified Tactic Calibration and Bias-Resistant Tree Search](https://openreview.net/forum?id=VQwI055flA) |  | 0 | Recent advancements in neural theorem proving integrate large language models with tree search algorithms like Monte Carlo Tree Search (MCTS), where the language model suggests tactics and the tree search finds the complete proof path. However, many tactics proposed by the language model converge... | Aoxue Li, Haiming Wang, Hui Jin, WenDa Wei, XiaoWen Yang, YuFeng Li, Zhenguo Li, Zhi Zhou |  |
| 1387 |  |  [Simple ReFlow: Improved Techniques for Fast Flow Models](https://openreview.net/forum?id=fpvgSDKXGY) |  | 0 | Diffusion and flow-matching models achieve remarkable generative performance but at the cost of many neural function evaluations (NFE), which slows inference and limits applicability to time-critical tasks. The ReFlow procedure can accelerate sampling by straightening generation trajectories. But... | Bahjat Kawar, Beomsu Kim, James Thornton, Jong Chul Ye, Marco Cuturi, Michal Klein, YuGuan Hsieh |  |
| 1388 |  |  [A Theoretical Perspective: How to Prevent Model Collapse in Self-consuming Training Loops](https://openreview.net/forum?id=WttfQGwpES) |  | 0 | High-quality data is essential for training large generative models, yet the vast reservoir of real data available online has become nearly depleted. Consequently, models increasingly generate their own data for further training, forming Self-consuming Training Loops (STLs). However, the empirical... | Dacheng Tao, Shi Fu, Xinmei Tian, Yingjie Wang, Yuzhu Chen |  |
| 1389 |  |  [Pareto Low-Rank Adapters: Efficient Multi-Task Learning with Preferences](https://openreview.net/forum?id=icDoYdUhRa) |  | 0 | Multi-task trade-offs in machine learning can be addressed via Pareto Front Learning (PFL) methods that parameterize the Pareto Front (PF) with a single model. PFL permits to select the desired operational point during inference, contrary to traditional Multi-Task Learning (MTL) that optimizes for... | François Fleuret, Nikolaos Dimitriadis, Pascal Frossard |  |
| 1390 |  |  [CryoGEN: Generative Energy-based Models for Cryogenic Electron Tomography Reconstruction](https://openreview.net/forum?id=uOb7rij7sR) |  | 0 | Cryogenic electron tomography (Cryo-ET) is a powerful technique for visualizing subcellular structures in their native states. Nonetheless, its effectiveness is compromised by anisotropic resolution artifacts caused by the missing-wedge effect. To address this, IsoNet, a deep learning-based method,... | Kai Chen, Qiwei Ye, Xi Chen, Yunfei Teng, Yuxuan Ren, Zhaoming Chen |  |
| 1391 |  |  [Data Unlearning in Diffusion Models](https://openreview.net/forum?id=SuHScQv5gP) |  | 0 | Recent work has shown that diffusion models memorize and reproduce training data examples. At the same time, large copyright lawsuits and legislation such as GDPR have highlighted the need for erasing datapoints from diffusion models. However, retraining from scratch is often too expensive. This... | Kenan Hasanaliyev, Manav Shah, Silas Alberti, Stefano Ermon |  |
| 1392 |  |  [Measuring And Improving Engagement of Text-to-Image Generation Models](https://openreview.net/forum?id=TmCcNuo03f) |  | 0 | Recent advances in text-to-image generation have achieved impressive aesthetic quality, making these models usable for both personal and commercial purposes. However, in the fields of marketing and advertising, images are often created to be more engaging, as reflected in user behaviors such as... | Balaji Krishnamurthy, Changyou Chen, Jayakumar Subramanian, Rajiv Ratn Shah, Varun Khurana, Yaman Kumar Singla, Zhiqiang Xu |  |
| 1393 |  |  [Convergence of Score-Based Discrete Diffusion Models: A Discrete-Time Analysis](https://openreview.net/forum?id=pq1WUegkza) |  | 0 | Diffusion models have achieved great success in generating high-dimensional samples across various applications. While the theoretical guarantees for continuous-state diffusion models have been extensively studied, the convergence analysis of the discrete-state counterparts remains under-explored.... | Quanquan Gu, Zikun Zhang, Zixiang Chen |  |
| 1394 |  |  [VLAS: Vision-Language-Action Model with Speech Instructions for Customized Robot Manipulation](https://openreview.net/forum?id=K4FAFNRpko) |  | 0 | Vision-language-action models (VLAs) have recently become highly prevalent in robot manipulation due to its end-to-end architecture and impressive performance. However, current VLAs are limited to processing human instructions in textual form, neglecting the more natural speech modality for human... | Donglin Wang, Han Zhao, Min Zhang, Pengxiang Ding, Shuanghao Bai, Wei Zhao, Zhefei Gong |  |
| 1395 |  |  [Teaching Human Behavior Improves Content Understanding Abilities Of VLMs](https://openreview.net/forum?id=ff2V3UR9sC) |  | 0 | Communication is defined as "\*Who\* says \*what\* to \*whom\* with \*what\* effect." A message from a communicator generates downstream receiver effects, also known as behavior. Receiver behavior, being a downstream effect of the message, carries rich signals about it. Even after carrying signals... | Balaji Krishnamurthy, Changyou Chen, Harini S. I, Rajiv Ratn Shah, Somesh Kumar Singh, Veeky Baths, Yaman Kumar Singla |  |
| 1396 |  |  [Atomas: Hierarchical Adaptive Alignment on Molecule-Text for Unified Molecule Understanding and Generation](https://openreview.net/forum?id=mun3bGqdDM) |  | 0 | Molecule-and-text cross-modal representation learning has emerged as a promising direction for enhancing the quality of molecular representation, thereby improving performance in various scientific fields. However, most approaches employ a global alignment approach to learn the knowledge from... | Bo Han, Chaohao Yuan, Geyan Ye, Jianhua Yao, LongKai Huang, Wei Liu, Yikun Zhang, Yu Rong |  |
| 1397 |  |  [LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory](https://openreview.net/forum?id=pZiyCaVuti) |  | 0 | Recent large language model (LLM)-driven chat assistant systems have integrated memory components to track user-assistant chat histories, enabling more accurate and personalized responses. However, their long-term memory capabilities in sustained interactions remain underexplored. We introduce... | Di Wu, Dong Yu, Hongwei Wang, KaiWei Chang, Wenhao Yu, Yuwei Zhang |  |
| 1398 |  |  [Reinforcement Learning from Imperfect Corrective Actions and Proxy Rewards](https://openreview.net/forum?id=JTji0Jfh5a) |  | 0 | In practice, reinforcement learning (RL) agents are often trained with a possibly imperfect proxy reward function, which may lead to a human-agent alignment issue (i.e., the learned policy either converges to non-optimal performance with low cumulative rewards, or achieves high cumulative rewards... | Changjie Fan, Paul Weng, Tangjie Lv, Tianze Zhou, Xuening Feng, Yan Song, Yifei Zhu, Yujing Hu, Zhaohui Jiang |  |
| 1399 |  |  [Improved Sampling Algorithms for Lévy-Itô Diffusion Models](https://openreview.net/forum?id=XxCgeWSTNp) |  | 0 | Lévy-Itô denoising diffusion models relying on isotropic α-stable noise instead of Gaussian distribution have recently been shown to improve performance of conventional diffusion models in image generation on imbalanced datasets while performing comparably in the standard settings. However, the... | Artem Khrapov, Assel Yermekova, Mikhail Sergeevich Kudinov, Tasnima Sadekova, Vadim Popov |  |
| 1400 |  |  [Measuring And Improving Persuasiveness Of Large Language Models](https://openreview.net/forum?id=NfCEVihkdC) |  | 0 | Large Language Models (LLMs) are increasingly being used in workflows involving generating content to be consumed by humans (\*e.g.,\* marketing) and also in directly interacting with humans (\*e.g.,\* through chatbots). The development of such systems that are capable of generating verifiably... | Balaji Krishnamurthy, Harini S. I, Somesh Kumar Singh, Yaman Kumar Singla |  |
| 1401 |  |  [Accelerating 3D Molecule Generation via Jointly Geometric Optimal Transport](https://openreview.net/forum?id=VGURexnlUL) |  | 0 | This paper proposes a new 3D molecule generation framework, called GOAT, for fast and effective 3D molecule generation based on the flow-matching optimal transport objective. Specifically, we formulate a geometric transport formula for measuring the cost of mapping multi-modal features (e.g.,... | Haokai Hong, Kc Tan, Wanyu Lin |  |
| 1402 |  |  [Learning LLM-as-a-Judge for Preference Alignment](https://openreview.net/forum?id=HZVIQE1MsJ) |  | 0 | Learning from preference feedback is a common practice for aligning large language models (LLMs) with human value. Conventionally, preference data is learned and encoded into a scalar reward model that connects a value head with an LLM to produce a scalar score as preference. However, scalar models... | Dong Yan, Qingyao Ai, Qiuchi Li, Wei Shen, Xiangsheng Li, Yiqun Liu, Yujia Zhou, Ziyi Ye |  |
| 1403 |  |  [SCBench: A KV Cache-Centric Analysis of Long-Context Methods](https://openreview.net/forum?id=gkUyYcY1W9) |  | 0 | Long-context Large Language Models (LLMs) have enabled numerous downstream applications but also introduced significant challenges related to computational and memory efficiency. To address these challenges, optimizations for long-context inference have been developed, centered around the KV cache.... | Amir H. Abdi, Chengruidong Zhang, Dongsheng Li, Huiqiang Jiang, Jianfeng Gao, Lili Qiu, Qianhui Wu, Surin Ahn, Xufang Luo, Yucheng Li, Yuqing Yang |  |
| 1404 |  |  [Monet: Mixture of Monosemantic Experts for Transformers](https://openreview.net/forum?id=1Ogw1SHY3p) |  | 0 | Understanding the internal computations of large language models (LLMs) is crucial for aligning them with human values and preventing undesirable behaviors like toxic content generation. However, mechanistic interpretability is hindered by \*polysemanticity\*—where individual neurons respond to... | Ahn Young Jin, Jaewoo Kang, Jungwoo Park, KeeEung Kim |  |
| 1405 |  |  [GraphBridge: Towards Arbitrary Transfer Learning in GNNs](https://openreview.net/forum?id=gjRhw5S3A4) |  | 0 | Graph neural networks (GNNs) are conventionally trained on a per-domain, per-task basis. It creates a significant barrier in transferring the acquired knowledge to different, heterogeneous data setups. This paper introduces \*\*GraphBridge\*\*, a novel framework to enable knowledge transfer across... | Li Ju, Qi Li, Xinchao Wang, Xingyi Yang |  |
| 1406 |  |  [The KoLMogorov Test: Compression by Code Generation](https://openreview.net/forum?id=C45YqeBDUM) |  | 0 | Compression is at the heart of intelligence. A theoretically optimal way to compress any sequence of data is to find the shortest program that outputs that sequence and then halts. However, such Kolmogorov compression is uncomputable, and code generating LLMs struggle to approximate this... | Fabian Gloeckle, Gabriel Synnaeve, Jonas Gehring, Kunhao Zheng, Ori Yoran, Taco Cohen |  |
| 1407 |  |  [Towards Explaining the Power of Constant-depth Graph Neural Networks for Structured Linear Programming](https://openreview.net/forum?id=INow59Vurm) |  | 0 | Graph neural networks (GNNs) have recently emerged as powerful tools for solving complex optimization problems, often being employed to approximate solution mappings. Empirical evidence shows that even shallow GNNs (with fewer than ten layers) can achieve strong performance in predicting optimal... | Minghui Ouyang, Qian Li, Qingjiang Shi, Ruoyu Sun, Tian Ding, Yuyi Wang |  |
| 1408 |  |  [Hybrid Regularization Improves Diffusion-based Inverse Problem Solving](https://openreview.net/forum?id=d7pr2doXn3) |  | 0 | Diffusion models, recognized for their effectiveness as generative priors, have become essential tools for addressing a wide range of visual challenges. Recently, there has been a surge of interest in leveraging Denoising processes for Regularization (DR) to solve inverse problems. However,... | Hongkun Dou, Jinyang Du, Lijun Yang, Wen Yao, Yue Deng, Zeyu Li |  |
| 1409 |  |  [RelCon: Relative Contrastive Learning for a Motion Foundation Model for Wearable Data](https://openreview.net/forum?id=k2uUeLCrQq) |  | 0 | We present RelCon, a novel self-supervised Relative Contrastive learning approach for training a motion foundation model from wearable accelerometry sensors. First, a learnable distance measure is trained to capture motif similarity and domain-specific semantic information such as rotation... | Darren Forde, Gregory Darnell, Haraldur Tómas Hallgrímsson, Hyewon Jeong, James Matthew Rehg, Jaya Narain, Karthik Jayaraman Raghuram, Maxwell A. Xu, Richard Andres Fineman, Shirley You Ren |  |
| 1410 |  |  [RevisEval: Improving LLM-as-a-Judge via Response-Adapted References](https://openreview.net/forum?id=1tBvzOYTLF) |  | 0 | With significant efforts in recent studies, LLM-as-a-Judge has become a cost-effective alternative to human evaluation for assessing text generation quality in a wide range of tasks. However, there still remains a reliability gap between LLM-as-a-Judge and human evaluation. One important reason is... | Chen Ma, Chuhan Wu, Fuyuan Lyu, Liangyou Li, Lifeng Shang, Qiyuan Zhang, Ruiming Tang, Tiezheng Yu, Xin Jiang, Yasheng Wang, Yufei Wang, Yuxin Jiang |  |
| 1411 |  |  [InCoDe: Interpretable Compressed Descriptions For Image Generation](https://openreview.net/forum?id=aXwukBD6M6) |  | 0 | Generative models have been successfully applied in diverse domains, from natural language processing to image synthesis. However, despite this success, a key challenge that remains is the ability to control the semantic content of the scene being generated. We argue that adequate control of the... | Aditya Chattopadhyay, Armand Comas Massague, Changyu Liu, Feliu Formosa, Octavia I. Camps, René Vidal |  |
| 1412 |  |  [Neuroplastic Expansion in Deep Reinforcement Learning](https://openreview.net/forum?id=20qZK2T7fa) |  | 0 | The loss of plasticity in learning agents, analogous to the solidification of neural pathways in biological brains, significantly impedes learning and adaptation in reinforcement learning due to its non-stationary nature. To address this fundamental challenge, we propose a novel approach,... | Aaron C. Courville, Jiashun Liu, Johan S. ObandoCeron, Ling Pan |  |
| 1413 |  |  [Efficient and Robust Neural Combinatorial Optimization via Wasserstein-Based Coresets](https://openreview.net/forum?id=F57HPKZ6KD) |  | 0 | Combinatorial optimization (CO) is a fundamental tool in many fields. Many neural combinatorial optimization (NCO) methods have been proposed to solve CO problems. However, existing NCO methods typically require significant computational and storage resources, and face challenges in maintaining... | Fuyou Miao, Wenjie Liu, Xu Wang, Yan Xiong |  |
| 1414 |  |  [Bio-xLSTM: Generative modeling, representation and in-context learning of biological and chemical sequences](https://openreview.net/forum?id=IjbXZdugdj) |  | 0 | Language models for biological and chemical sequences enable crucial applications such as drug discovery, protein engineering, and precision medicine. Currently, these language models are predominantly based on Transformer architectures. While Transformers have yielded impressive results, their... | Andreas Mayr, Günter Klambauer, Johannes Brandstetter, Johannes Schimunek, Lisa Schneckenreiter, Niklas Schmidinger, Philipp Seidl, PieterJan Hoedt, Sepp Hochreiter, Sohvi Luukkonen |  |
| 1415 |  |  [Uni2Det: Unified and Universal Framework for Prompt-Guided Multi-dataset 3D Detection](https://openreview.net/forum?id=AcVpLS86RT) |  | 0 | We present Uni$^2$Det, a brand new framework for unified and universal multi-dataset training on 3D detection, enabling robust performance across diverse domains and generalization to unseen domains. Due to substantial disparities in data distribution and variations in taxonomy across diverse... | Cairong Zhao, Errui Ding, Xiao Tan, Xiaoqing Ye, Yubin Wang, Zhikang Zou |  |
| 1416 |  |  [Neural networks on Symmetric Spaces of Noncompact Type](https://openreview.net/forum?id=bwOndfohRK) |  | 0 | Recent works have demonstrated promising performances of neural networks on hyperbolic spaces and symmetric positive definite (SPD) manifolds. These spaces belong to a family of Riemannian manifolds referred to as symmetric spaces of noncompact type. In this paper, we propose a novel approach for... | Aymeric Histace, Shuo Yang, Xuan Son Nguyen |  |
| 1417 |  |  [Graph-based Document Structure Analysis](https://openreview.net/forum?id=Fu0aggezN9) |  | 0 | When reading a document, glancing at the spatial layout of a document is an initial step to understand it roughly. Traditional document layout analysis (DLA) methods, however, offer only a superficial parsing of documents, focusing on basic instance detection and often failing to capture the... | Di Wen, Jiaming Zhang, Junwei Zheng, Kunyu Peng, Rainer Stiefelhagen, Ruiping Liu, Yufan Chen |  |
| 1418 |  |  [Rethinking Invariance in In-context Learning](https://openreview.net/forum?id=q1UyoY3MgJ) |  | 0 | In-Context Learning (ICL) has emerged as a pivotal capability of auto-regressive large language models, yet it is hindered by a notable sensitivity to the ordering of context examples regardless of their mutual independence. To address this issue, recent studies have introduced several variant... | Khashayar Gatmiry, Lei Fang, Lizhe Fang, Yifei Wang, Yisen Wang |  |
| 1419 |  |  [FlashMask: Efficient and Rich Mask Extension of FlashAttention](https://openreview.net/forum?id=wUtXB43Chi) |  | 0 | The computational and memory demands of vanilla attention scale quadratically with the sequence length $N$, posing significant challenges for processing long sequences in Transformer models. FlashAttention alleviates these challenges by eliminating the $\mathcal{O}(N^2)$ memory dependency and... | Dianhai Yu, Guoxia Wang, Haifeng Wang, Jiabin Yang, Jiang Bian, Jinle Zeng, Lujing Zheng, Siming Wu, Xiyuan Xiao, Zeyu Chen |  |
| 1420 |  |  [You Only Sample Once: Taming One-Step Text-to-Image Synthesis by Self-Cooperative Diffusion GANs](https://openreview.net/forum?id=T7bmHkwzS6) |  | 0 | Recently, some works have tried to combine diffusion and Generative Adversarial Networks (GANs) to alleviate the computational cost of the iterative denoising inference in Diffusion Models (DMs). However, existing works in this line suffer from either training instability and mode collapse or... | Jing Tang, Tianyang Hu, Xiaolong Chen, Xinghua Qu, Yihong Luo |  |
| 1421 |  |  [Generalizing Weisfeiler-Lehman Kernels to Subgraphs](https://openreview.net/forum?id=HZgZrtIreg) |  | 0 | Subgraph representation learning has been effective in solving various real-world problems. However, current graph neural networks (GNNs) produce suboptimal results for subgraph-level tasks due to their inability to capture complex interactions within and between subgraphs. To provide a more... | Alice Oh, Dongkwan Kim |  |
| 1422 |  |  [Automated Filtering of Human Feedback Data for Aligning Text-to-Image Diffusion Models](https://openreview.net/forum?id=8jvVNPHtVJ) |  | 0 | Fine-tuning text-to-image diffusion models with human feedback is an effective method for aligning model behavior with human intentions. However, this alignment process often suffers from slow convergence due to the large size and noise present in human feedback datasets. In this work, we propose... | Hojung Jung, Kimin Lee, SangMook Kim, Sangmin Bae, SeYoung Yun, Sihyeon Kim, Yongjin Yang |  |
| 1423 |  |  [What is Wrong with Perplexity for Long-context Language Modeling?](https://openreview.net/forum?id=fL4qWkSmtM) |  | 0 | Handling long-context inputs is crucial for large language models (LLMs) in tasks such as extended conversations, document summarization, and many-shot in-context learning. While recent approaches have extended the context windows of LLMs and employed perplexity (PPL) as a standard evaluation... | Bolin Ding, Chenheng Zhang, Jinyang Gao, Lizhe Fang, Stefanie Jegelka, Yifei Wang, Yisen Wang, Zhaoyang Liu |  |
| 1424 |  |  [KooNPro: A Variance-Aware Koopman Probabilistic Model Enhanced by Neural Process for Time Series Forecasting](https://openreview.net/forum?id=5oSUgTzs8Y) |  | 0 | The probabilistic forecasting of time series is a well-recognized challenge, particularly in disentangling correlations among interacting time series and addressing the complexities of distribution modeling. By treating time series as temporal dynamics, we introduce \*\*KooNPro\*\*, a novel... | Hanru Bai, Ronghua Zheng, Weiyang Ding |  |
| 1425 |  |  [TC-MoE: Augmenting Mixture of Experts with Ternary Expert Choice](https://openreview.net/forum?id=dsP91M4hDL) |  | 0 | The Mixture of Experts (MoE) architecture has emerged as a promising solution to reduce computational overhead by selectively activating subsets of model parameters. The effectiveness of MoE models depends primarily on their routing mechanisms, with the widely adopted Top-K routing scheme used for... | Shen Yan, Sijun Zhang, Xingyan Bin, Yisen Wang, Zhouchen Lin |  |
| 1426 |  |  [Bayesian WeakS-to-Strong from Text Classification to Generation](https://openreview.net/forum?id=pHe4P1IVnb) |  | 0 | Advances in large language models raise the question of how alignment techniques will adapt as models become increasingly complex and humans will only be able to supervise them weakly. Weak-to-Strong mimics such a scenario where weak model supervision attempts to harness the full capabilities of a... | Chao Zhang, Guangzhi Sun, Wen Wu, Ziyang Zhang, Ziyun Cui |  |
| 1427 |  |  [Polyrating: A Cost-Effective and Bias-Aware Rating System for LLM Evaluation](https://openreview.net/forum?id=URPwT55i6O) |  | 0 | Rating-based human evaluation has become an essential tool to accurately evaluate the impressive performance of large language models (LLMs). However, current rating systems suffer from several important limitations: first, they fail to account for biases that significantly influence evaluation... | Jasper Dekoninck, Martin T. Vechev, Maximilian Baader |  |
| 1428 |  |  [The impact of allocation strategies in subset learning on the expressive power of neural networks](https://openreview.net/forum?id=upoxXRRTQ2) |  | 0 | In traditional machine learning, models are defined by a set of parameters, which are optimized to perform specific tasks. In neural networks, these parameters correspond to the synaptic weights. However, in reality, it is often infeasible to control or update all weights. This challenge is not... | Ofir Schlisselberg, Ran Darshan |  |
| 1429 |  |  [WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning](https://openreview.net/forum?id=oVKEAFjEqv) |  | 0 | Large language models (LLMs) have shown remarkable potential as autonomous agents, particularly in web-based tasks. However, existing LLM web agents face significant limitations: high-performing agents rely on expensive proprietary LLM APIs, while open LLMs lack the necessary decision-making... | Hanyu Lai, Iat Long Iong, Jiadai Sun, Jie Tang, Shuntian Yao, Wei Xu, Xiao Liu, Xinyue Yang, Xueqiao Sun, Yu Yang, Yuxiao Dong, Zehan Qi |  |
| 1430 |  |  [Surgical, Cheap, and Flexible: Mitigating False Refusal in Language Models via Single Vector Ablation](https://openreview.net/forum?id=SCBn8MCLwc) |  | 0 | Training a language model to be both helpful and harmless requires careful calibration of refusal behaviours: Models should refuse to follow malicious instructions or give harmful advice (e.g."how do I kill someone?"), but they should not refuse safe requests, even if they superficially resemble... | Barbara Plank, Chengzhi Hu, Paul Röttger, Xinpeng Wang |  |
| 1431 |  |  [Adaptive Methods through the Lens of SDEs: Theoretical Insights on the Role of Noise](https://openreview.net/forum?id=ww3CLRhF1v) |  | 0 | Despite the vast empirical evidence supporting the efficacy of adaptive optimization methods in deep learning, their theoretical understanding is far from complete. This work introduces novel SDEs for commonly used adaptive optimizers: SignSGD, RMSprop(W), and Adam(W). These SDEs offer a... | Antonio Orvieto, Aurélien Lucchi, Enea Monzio Compagnoni, Frank Norbert Proske, Rustem Islamov, Tianlin Liu |  |
| 1432 |  |  [Glauber Generative Model: Discrete Diffusion Models via Binary Classification](https://openreview.net/forum?id=HyjIEf90Tn) |  | 0 | We introduce the Glauber Generative Model (GGM), a new class of discrete diffusion models, to obtain new samples from a distribution given samples from a discrete space. GGM deploys a discrete Markov chain called the heat bath dynamics (or the Glauber dynamics) to denoise a sequence of noisy tokens... | Dheeraj Mysore Nagaraj, Harshit Varma, Karthikeyan Shanmugam |  |
| 1433 |  |  [AutoBencher: Towards Declarative Benchmark Construction](https://openreview.net/forum?id=ymt4crbbXh) |  | 0 | We present AutoBencher, a declarative framework for automatic benchmark construction, and use it to scalably discover novel insights and vulnerabilities of existing language models. Concretely, given a few desiderata of benchmarks (e.g., question difficulty, topic salience), we operationalize each... | Evan Zheran Liu, Farzaan Kaiyom, Percy Liang, Tatsunori Hashimoto, Xiang Lisa Li, Yifan Mai |  |
| 1434 |  |  [Self-Evolving Multi-Agent Collaboration Networks for Software Development](https://openreview.net/forum?id=4R71pdPBZp) |  | 0 | LLM-driven multi-agent collaboration (MAC) systems have demonstrated impressive capabilities in automatic software development at the function level. However, their heavy reliance on human design limits their adaptability to the diverse demands of real-world software development. To address this... | Shuo Tang, Siheng Chen, Xiangrui Liu, Xinyu Zhu, Yaxin Du, Yuchen Hou, Yue Hu, Yuzhu Cai, Zijie Yu |  |
| 1435 |  |  [BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation Experiments](https://openreview.net/forum?id=HAwZGLcye3) |  | 0 | Agents based on large language models have shown great potential in accelerating scientific discovery by leveraging their rich background knowledge and reasoning capabilities. In this paper, we introduce BioDiscoveryAgent, an agent that designs new experiments, reasons about their outcomes, and... | Alexander Marson, Andrew H. Lee, Jian Vora, Jure Leskovec, Kexin Huang, Percy Liang, Qian Huang, Yusuf H. Roohani, Zachary Steinhart |  |
| 1436 |  |  [Privacy-Aware Lifelong Learning](https://openreview.net/forum?id=UstOpZCESc) |  | 0 | Lifelong learning algorithms enable models to incrementally acquire new knowledge without forgetting previously learned information. Contrarily, the field of machine unlearning focuses on explicitly forgetting certain previous knowledge from pretrained models when requested, in order to comply with... | Elmar Rueckert, Ozan Özdenizci, Robert Legenstein |  |
| 1437 |  |  [PhyloLM: Inferring the Phylogeny of Large Language Models and Predicting their Performances in Benchmarks](https://openreview.net/forum?id=rTQNGQxm4K) |  | 0 | This paper introduces PhyloLM, a method adapting phylogenetic algorithms to Large Language Models (LLMs) to explore whether and how they relate to each other and to predict their performance characteristics. Our method calculates a phylogenetic distance metric based on the similarity of LLMs'... | Nicolas Yax, PierreYves Oudeyer, Stefano Palminteri |  |
| 1438 |  |  [Stabilized Neural Prediction of Potential Outcomes in Continuous Time](https://openreview.net/forum?id=aN57tSd5Us) |  | 0 | Patient trajectories from electronic health records are widely used to estimate conditional average potential outcomes (CAPOs) of treatments over time, which then allows to personalize care. Yet, existing neural methods for this purpose have a key limitation: while some adjust for time-varying... | Konstantin Hess, Stefan Feuerriegel |  |
| 1439 |  |  [UIFace: Unleashing Inherent Model Capabilities to Enhance Intra-Class Diversity in Synthetic Face Recognition](https://openreview.net/forum?id=riieAeQBJm) |  | 0 | Face recognition (FR) stands as one of the most crucial applications in computer vision. The accuracy of FR models has significantly improved in recent years due to the availability of large-scale human face datasets. However, directly using these datasets can inevitably lead to privacy and legal... | Jianqing Xu, Shouhong Ding, Shuigeng Zhou, Xiao Lin, Yuge Huang, Yuxi Mi |  |
| 1440 |  |  [FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware](https://openreview.net/forum?id=l0ZzTvPfTw) |  | 0 | While Transformers and other sequence-parallelizable neural network architectures seem like the current state of the art in sequence modeling, they specifically lack state-tracking capabilities. These are important for time-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,... | Korbinian Pöppel, Maximilian Beck, Sepp Hochreiter |  |
| 1441 |  |  [Dream to Manipulate: Compositional World Models Empowering Robot Imitation Learning with Imagination](https://openreview.net/forum?id=3RSLW9YSgk) |  | 0 | A world model provides an agent with a representation of its environment, enabling it to predict the causal consequences of its actions. Current world models typically cannot directly and explicitly imitate the actual environment in front of a robot, often resulting in unrealistic behaviors and... | Andrii Zadaianchuk, Davide Allegro, Efstratios Gavves, Leonardo Barcellona, Samuele Papa, Stefano Ghidoni |  |
| 1442 |  |  [Inverse Rendering using Multi-Bounce Path Tracing and Reservoir Sampling](https://openreview.net/forum?id=KEXoZxTwbr) |  | 0 | We introduce MIRReS, a novel two-stage inverse rendering framework that jointly reconstructs and optimizes explicit geometry, materials, and lighting from multi-view images. Unlike previous methods that rely on implicit irradiance fields or oversimplified ray tracing, our method begins with an... | Chen Qian, Dianbing Xi, Jingsen Zhu, Qi Wang, Ying He, Yuchi Huo, Yuxin Dai |  |
| 1443 |  |  [GeoILP: A Synthetic Dataset to Guide Large-Scale Rule Induction](https://openreview.net/forum?id=cfGpIcOIa5) |  | 0 | Inductive logic programming (ILP) is a machine learning approach aiming to learn explanatory rules from data. While existing ILP systems can successfully solve small-scale tasks, large-scale applications with various language biases are rarely explored. Besides, it is crucial for a large majority... | Richong Zhang, Si Chen, Xu Zhang |  |
| 1444 |  |  [Generating Graphs via Spectral Diffusion](https://openreview.net/forum?id=AAXBfJNHDt) |  | 0 | In this paper, we present GGSD, a novel graph generative model based on 1) the spectral decomposition of the graph Laplacian matrix and 2) a diffusion process. Specifically, we propose to use a denoising model to sample eigenvectors and eigenvalues from which we can reconstruct the graph Laplacian... | Alessandro Bicciato, Andrea Torsello, Giorgia Minello, Luca Cosmo, Luca Rossi |  |
| 1445 |  |  [Grounding Continuous Representations in Geometry: Equivariant Neural Fields](https://openreview.net/forum?id=A4eCzSohhx) |  | 0 | Conditional Neural Fields (CNFs) are increasingly being leveraged as continuous signal representations, by associating each data-sample with a latent variable that conditions a shared backbone Neural Field (NeF) to reconstruct the sample. However, existing CNF architectures face limitations when... | David M. Knigge, David R. Wessels, Efstratios Gavves, Erik J. Bekkers, Riccardo Valperga, Samuele Papa, Sharvaree P. Vadgama |  |
| 1446 |  |  [FOSP: Fine-tuning Offline Safe Policy through World Models](https://openreview.net/forum?id=dbuFJg7eaw) |  | 0 | Offline Safe Reinforcement Learning (RL) seeks to address safety constraints by learning from static datasets and restricting exploration. However, these approaches heavily rely on the dataset and struggle to generalize to unseen scenarios safely. In this paper, we aim to improve safety during the... | Chenyang Cao, Junbo Tan, Longxiang He, Silang Wu, Xueqian Wang, Yucheng Xin, Zichen Yan |  |
| 1447 |  |  [Autocorrelation Matters: Understanding the Role of Initialization Schemes for State Space Models](https://openreview.net/forum?id=sZJNkorXMk) |  | 0 | Current methods for initializing state space model (SSM) parameters primarily rely on the HiPPO framework \citep{gu2023how}, which is based on online function approximation with the SSM kernel basis. However, the HiPPO framework does not explicitly account for the effects of the temporal structures... | Fusheng Liu, Qianxiao Li |  |
| 1448 |  |  [SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents](https://openreview.net/forum?id=xKDZAW0He3) |  | 0 | To deliver coherent and personalized experiences in long-term conversations, existing approaches typically perform retrieval augmented response generation by constructing memory banks from conversation history at either the turn-level, session-level, or through summarization techniques. In this... | ChinYew Lin, Dongsheng Li, H. Vicky Zhao, Hao Cheng, Huiqiang Jiang, Jianfeng Gao, Lili Qiu, Qianhui Wu, Xufang Luo, Yuqing Yang, Zhuoshi Pan |  |
| 1449 |  |  [Self-supervised Monocular Depth Estimation Robust to Reflective Surface Leveraged by Triplet Mining](https://openreview.net/forum?id=XdRIno98gG) |  | 0 | Self-supervised monocular depth estimation (SSMDE) aims to predict the dense depth map of a monocular image, by learning depth from RGB image sequences, eliminating the need for ground-truth depth labels. Although this approach simplifies data acquisition compared to supervised methods, it... | Kyumin Hwang, Minwoo Choi, Sunghoon Im, Wei Peng, Wonhyeok Choi |  |
| 1450 |  |  [An Effective Theory of Bias Amplification](https://openreview.net/forum?id=VoI4d6uhdr) |  | 0 | Machine learning models can capture and amplify biases present in data, leading to disparate test performance across social groups. To better understand, evaluate, and mitigate these biases, a deeper theoretical understanding of how model design choices and data distribution properties contribute... | Arjun Subramonian, Elvis Dohmatob, Levent Sagun, Samuel J. Bell |  |
| 1451 |  |  [Representational Similarity via Interpretable Visual Concepts](https://openreview.net/forum?id=ih3BJmIZbC) |  | 0 | How do two deep neural networks differ in how they arrive at a decision? Measuring the similarity of deep networks has been a long-standing open question. Most existing methods provide a single number to measure the similarity of two networks at a given layer, but give no insight into what makes... | Neehar Kondapaneni, Oisin Mac Aodha, Pietro Perona |  |
| 1452 |  |  [DRoC: Elevating Large Language Models for Complex Vehicle Routing via Decomposed Retrieval of Constraints](https://openreview.net/forum?id=s9zoyICZ4k) |  | 0 | This paper proposes Decomposed Retrieval of Constraints (DRoC), a novel framework aimed at enhancing large language models (LLMs) in exploiting solvers to tackle vehicle routing problems (VRPs) with intricate constraints. While LLMs have shown promise in solving simple VRPs, their potential in... | Chenhao Zhang, Xia Jiang, Yaoxin Wu, Yingqian Zhang |  |
| 1453 |  |  [On Minimizing Adversarial Counterfactual Error in Adversarial Reinforcement Learning](https://openreview.net/forum?id=eUEMjwh5wK) |  | 0 | Deep Reinforcement Learning (DRL) policies are highly susceptible to adversarial noise in observations, which poses significant risks in safety-critical scenarios. The challenge inherent to adversarial perturbations is that by altering the information observed by the agent, the state becomes only... | Arunesh Sinha, Pradeep Varakantham, Roman Belaire |  |
| 1454 |  |  [ToolDial: Multi-turn Dialogue Generation Method for Tool-Augmented Language Models](https://openreview.net/forum?id=J1J5eGJsKZ) |  | 0 | Tool-Augmented Language Models (TALMs) leverage external APIs to answer user queries across various domains. However, existing benchmark datasets for TALM research often feature simplistic dialogues that do not reflect real-world scenarios, such as the need for models to ask clarifying questions or... | Cheongsu Lim, Gyuhyeon Seo, Jeonghoon Shim, Yohan Jo |  |
| 1455 |  |  [3D-MolT5: Leveraging Discrete Structural Information for Molecule-Text Modeling](https://openreview.net/forum?id=eGqQyTAbXC) |  | 0 | The integration of molecular and natural language representations has emerged as a focal point in molecular science, with recent advancements in Language Models (LMs) demonstrating significant potential for comprehensive modeling of both domains. However, existing approaches face notable... | Jinhua Zhu, Kaiyuan Gao, Lijun Wu, Qizhi Pei, Rui Yan |  |
| 1456 |  |  [ParFam - (Neural Guided) Symbolic Regression via Continuous Global Optimization](https://openreview.net/forum?id=8y5Uf6oEiB) |  | 0 | The problem of symbolic regression (SR) arises in many different applications, such as identifying physical laws or deriving mathematical equations describing the behavior of financial markets from given data. Various methods exist to address the problem of SR, often based on genetic programming.... | Gitta Kutyniok, Hillary Hauger, Katharina Bieker, Philipp Scholl |  |
| 1457 |  |  [Taming Transformer Without Using Learning Rate Warmup](https://openreview.net/forum?id=GeUK3zGreN) |  | 0 | Scaling Transformer to a large scale without using some technical tricks such as learning rate warump and an obviously lower learning rate, is an extremely challenging task, and is increasingly gaining more attention. In this paper, we provide a theoretical analysis for the process of training... | Bojia Zi, ChunGuang Li, Jiaquan Ye, Qin Zou, Rong Xiao, Xianbiao Qi, Xili Dai, Yelin He |  |
| 1458 |  |  [Redefining the task of Bioactivity Prediction](https://openreview.net/forum?id=S8gbnkCgxZ) |  | 0 | Small molecules are vital to modern medicine, and accurately predicting their bioactivity against protein targets is crucial for therapeutic discovery and development. However, current machine learning models often rely on spurious features, leading to biased outcomes. Notably, a simple pocket-only... | Bowen Gao, Hongbo Ma, WeiYing Ma, YaQin Zhang, Yanwen Huang, Yanyan Lan, Yinjun Jia |  |
| 1459 |  |  [Boltzmann priors for Implicit Transfer Operators](https://openreview.net/forum?id=pRCOZllZdT) |  | 0 | Accurate prediction of thermodynamic properties is essential in drug discovery and materials science. Molecular dynamics (MD) simulations provide a principled approach to this task, yet they typically rely on prohibitively long sequential simulations. Implicit Transfer Operator (ITO) Learning... | Juan Viguera Diez, Mathias Jacob Schreiner, Ola Engkvist, Simon Olsson |  |
| 1460 |  |  [Can In-context Learning Really Generalize to Out-of-distribution Tasks?](https://openreview.net/forum?id=INe4otjryz) |  | 0 | In this work, we explore the mechanism of in-context learning (ICL) on out-of-distribution (OOD) tasks that were not encountered during training. To achieve this, we conduct synthetic experiments where the objective is to learn OOD mathematical functions through ICL using a GPT-2 model. We reveal... | Qixun Wang, Xianghua Ying, Yifei Wang, Yisen Wang |  |
| 1461 |  |  [CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences](https://openreview.net/forum?id=EQgEMAD4kv) |  | 0 | Large language models (LLMs) excel at processing long sequences, boosting demand for key-value (KV) caching. While recent efforts to evict KV cache have alleviated the inference burden, they often fail to allocate resources rationally across layers with different attention patterns. In this paper,... | Jianguo Li, Ke Cheng, Mingbao Lin, Shixuan Fan, Weiyao Lin, Wen Hu, Yuchen Cao, Ziran Qin |  |
| 1462 |  |  [Revisiting Nearest Neighbor for Tabular Data: A Deep Tabular Baseline Two Decades Later](https://openreview.net/forum?id=JytL2MrlLT) |  | 0 | The widespread enthusiasm for deep learning has recently expanded into the domain of tabular data. Recognizing that the advancement in deep tabular methods is often inspired by classical methods, e.g., integration of nearest neighbors into neural networks, we investigate whether these classical... | DeChuan Zhan, HanJia Ye, HuaiHong Yin, WeiLun Chao |  |
| 1463 |  |  [LoR-VP: Low-Rank Visual Prompting for Efficient Vision Model Adaptation](https://openreview.net/forum?id=5btFIv2PNb) |  | 0 | Visual prompting has gained popularity as a method for adapting pre-trained models to specific tasks, particularly in the realm of parameter-efficient tuning. However, existing visual prompting techniques often pad the prompt parameters around the image, limiting the interaction between the visual... | Can Jin, Dimitris N. Metaxas, Ligong Han, Mingyu Zhao, Shiyu Zhao, Tong Che, Xiaoxiao He, Ying Li, Zhenting Wang |  |
| 1464 |  |  [Robust Simulation-Based Inference under Missing Data via Neural Processes](https://openreview.net/forum?id=GsR3zRCRX5) |  | 0 | Simulation-based inference (SBI) methods typically require fully observed data to infer parameters of models with intractable likelihood functions. However, datasets often contain missing values due to incomplete observations, data corruptions (common in astrophysics), or instrument limitations... | Ayush Bharti, Vikas Garg, Yogesh Verma |  |
| 1465 |  |  [Start Smart: Leveraging Gradients For Enhancing Mask-based XAI Methods](https://openreview.net/forum?id=Iht4NNVqk0) |  | 0 | Mask-based explanation methods offer a powerful framework for interpreting deep learning model predictions across diverse data modalities, such as images and time series, in which the central idea is to identify an instance-dependent mask that minimizes the performance drop from the resulting... | Buelent Uendes, Mark Hoogendoorn, Shujian Yu |  |
| 1466 |  |  [DRESSing Up LLM: Efficient Stylized Question-Answering via Style Subspace Editing](https://openreview.net/forum?id=mNVR9jJYqK) |  | 0 | We introduce DRESS, a novel approach for generating stylized large language model (LLM) responses through representation editing. Existing methods like prompting and fine-tuning are either insufficient for complex style adaptation or computationally expensive, particularly in tasks like NPC... | Junfeng Zhao, Tianlong Wang, Xin Gao, Xinyu Ma, Xu Chu, Yang Lin, Yasha Wang, Yifeng Xu |  |
| 1467 |  |  [LLMOPT: Learning to Define and Solve General Optimization Problems from Scratch](https://openreview.net/forum?id=9OMvtboTJg) |  | 0 | Optimization problems are prevalent across various scenarios. Formulating and then solving optimization problems described by natural language often requires highly specialized human expertise, which could block the widespread application of optimization-based decision making. To automate problem... | Aimin Zhou, Caigao Jiang, Hong Qian, Jun Zhou, Xiang Shu, Xingyu Lu, Yang Yu |  |
| 1468 |  |  [T2V2: A Unified Non-Autoregressive Model for Speech Recognition and Synthesis via Multitask Learning](https://openreview.net/forum?id=TtKN1TpvUu) |  | 0 | We introduce T2V2 (\*\*T\*\*ext to \*\*V\*\*oice and \*\*V\*\*oice to \*\*T\*\*ext), a unified non-autoregressive model capable of performing both automatic speech recognition (ASR) and text-to-speech (TTS) synthesis within the same framework. T2V2 uses a shared Conformer backbone with rotary... | Hanqin Wang, Nabarun Goswami, Tatsuya Harada |  |
| 1469 |  |  [Physics-Informed Diffusion Models](https://openreview.net/forum?id=tpYeermigp) |  | 0 | Generative models such as denoising diffusion models are quickly advancing their ability to approximate highly complex data distributions. They are also increasingly leveraged in scientific machine learning, where samples from the implied data distribution are expected to adhere to specific... | Dennis M. Kochmann, JanHendrik Bastek, WaiChing Sun |  |
| 1470 |  |  [A Simple yet Effective ΔΔG Predictor is An Unsupervised Antibody Optimizer and Explainer](https://openreview.net/forum?id=IxmWIkcKs5) |  | 0 | The proteins that exist today have been optimized over billions of years of natural evolution, during which nature creates random mutations and selects them. The discovery of functionally promising mutations is challenged by the limited evolutionary accessible regions, i.e., only a small region on... | Guojiang Zhao, Haitao Lin, Lirong Wu, Stan Z. Li, Yufei Huang, Yunfan Liu, Zhifeng Gao |  |
| 1471 |  |  [Identifying latent state transitions in non-linear dynamical systems](https://openreview.net/forum?id=d16mJDyQN6) |  | 0 | This work aims to recover the underlying states and their time evolution in a latent dynamical system from high-dimensional sensory measurements. Previous works on identifiable representation learning in dynamical systems focused on identifying the latent states, often with linear transition... | Matthias Bethge, Pekka Marttinen, S. T. John, Çagatay Yildiz, Çaglar Hizli |  |
| 1472 |  |  [Neuron based Personality Trait Induction in Large Language Models](https://openreview.net/forum?id=LYHEY783Np) |  | 0 | Large language models (LLMs) have become increasingly proficient at simulating various personality traits, an important capability for supporting related applications (e.g., role-playing). To further improve this capacity, in this paper, we present a neuron based approach for personality trait... | JiRong Wen, Jia Deng, Tianyi Tang, Wenhao Yang, Xin Zhao, Yanbin Yin |  |
| 1473 |  |  [Diffusion-based Neural Network Weights Generation](https://openreview.net/forum?id=j8WHjM9aMm) |  | 0 | Transfer learning is a cornerstone of modern deep learning, yet it remains constrained by challenges in model selection and the overhead of extensive model storage. In this work, we present Diffusion-based Neural Network Weights Generation, D2NWG, a novel framework that leverages diffusion... | Bedionita Soro, Bruno Andreis, Frank Hutter, Hayeon Lee, Song Chong, Sung Ju Hwang, Wonyong Jeong |  |
| 1474 |  |  [Improving Neural Network Accuracy by Concurrently Training with a Twin Network](https://openreview.net/forum?id=TEmE9PSC65) |  | 0 | Recently within Spiking Neural Networks, a method called Twin Network Augmentation (TNA) has been introduced. This technique claims to improve the validation accuracy of a Spiking Neural Network simply by training two networks in conjunction and matching the logits via the Mean Squared Error loss.... | Benjamin Vandersmissen, José Oramas M., Lucas Deckers |  |
| 1475 |  |  [Sylber: Syllabic Embedding Representation of Speech from Raw Audio](https://openreview.net/forum?id=FyMjfDQ9RO) |  | 0 | Syllables are compositional units of spoken language that efficiently structure human speech perception and production. However, current neural speech representations lack such structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model,... | Akshat Gupta, Alan W. Black, Cheol Jun Cho, Dhruv Agarwal, Ethan Chen, Gopala Anumanchipalli, Nicholas Lee |  |
| 1476 |  |  [How to Verify Any (Reasonable) Distribution Property: Computationally Sound Argument Systems for Distributions](https://openreview.net/forum?id=GfXMTAJaxZ) |  | 0 | As statistical analyses become more central to science, industry and society, there is a growing need to ensure correctness of their results. Approximate correctness can be verified by replicating the entire analysis, but can we verify without replication? We focus on distribution testing problems:... | Guy N. Rothblum, Tal Herman |  |
| 1477 |  |  [Synthesizing Programmatic Reinforcement Learning Policies with Large Language Model Guided Search](https://openreview.net/forum?id=8DBTq09LgN) |  | 0 | Programmatic reinforcement learning (PRL) has been explored for representing policies through programs as a means to achieve interpretability and generalization. Despite promising outcomes, current state-of-the-art PRL methods are hindered by sample inefficiency, necessitating tens of millions of... | ChanHung Yu, ChengWei Hung, Max Liu, ShaoHua Sun, WeiHsu Lee, YenChun Chen |  |
| 1478 |  |  [DynFrs: An Efficient Framework for Machine Unlearning in Random Forest](https://openreview.net/forum?id=nsCOeCLR8e) |  | 0 | Random Forests are widely recognized for establishing efficacy in classification and regression tasks, standing out in various domains such as medical diagnosis, finance, and personalized recommendations. These domains, however, are inherently sensitive to privacy concerns, as personal and... | Meng Zhang, Shurong Wang, Tongning Zhang, Xinbao Qiao, Zhuoyang Shen |  |
| 1479 |  |  [GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling](https://openreview.net/forum?id=1p6xFLBU4J) |  | 0 | Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in... | Chen Chen, Eng Siong Chng, Hexin Liu, Jixun Yao, Lei Xie, Yuchen Hu |  |
| 1480 |  |  [Emerging Safety Attack and Defense in Federated Instruction Tuning of Large Language Models](https://openreview.net/forum?id=sYNWqQYJhz) |  | 0 | Federated learning (FL) enables multiple parties to collaboratively fine-tune an large language model (LLM) without the need of direct data sharing. Ideally, by training on decentralized data that is aligned with human preferences and safety principles, federated instruction tuning (FedIT) can... | Jingyi Chai, Rui Ye, Siheng Chen, Xiangrui Liu, Yanfeng Wang, Yaodong Yang |  |
| 1481 |  |  [Learned Reference-based Diffusion Sampler for multi-modal distributions](https://openreview.net/forum?id=fmJUYgmMbL) |  | 0 | Over the past few years, several approaches utilizing score-based diffusion have been proposed to sample from probability distributions, that is without having access to exact samples and relying solely on evaluations of unnormalized densities. The resulting samplers approximate the time-reversal... | Alain Oliviero Durmus, Louis Grenioux, Marylou Gabrié, Maxence Noble |  |
| 1482 |  |  [Looking Backward: Retrospective Backward Synthesis for Goal-Conditioned GFlowNets](https://openreview.net/forum?id=fNMKqyvuZT) |  | 0 | Generative Flow Networks (GFlowNets), a new family of probabilistic samplers, have demonstrated remarkable capabilities to generate diverse sets of high-reward candidates, in contrast to standard return maximization approaches (e.g., reinforcement learning) which often converge to a single optimal... | Can Chang, Haoran He, Huazhe Xu, Ling Pan |  |
| 1483 |  |  [Tree-Wasserstein Distance for High Dimensional Data with a Latent Feature Hierarchy](https://openreview.net/forum?id=nYjAzwor9R) |  | 0 | Finding meaningful distances between high-dimensional data samples is an important scientific task. To this end, we propose a new tree-Wasserstein distance (TWD) for high-dimensional data with two key aspects. First, our TWD is specifically designed for data with a latent feature hierarchy, i.e.,... | Gal Mishne, Ronald R. Coifman, Ronen Talmon, YaWei Eileen Lin |  |
| 1484 |  |  [StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via Inference-time Hybrid Information Structurization](https://openreview.net/forum?id=GhexuBLxbO) |  | 0 | Retrieval-augmented generation (RAG) is a key means to effectively enhance large language models (LLMs) in many knowledge-based tasks. However, existing RAG methods struggle with knowledge-intensive reasoning tasks, because useful information required to these tasks are badly scattered. This... | Fei Huang, Haiyang Yu, Hongyu Lin, Le Sun, Qiaoyu Tang, Xianpei Han, Xuanang Chen, Yaojie Lu, Yongbin Li, Zhuoqun Li |  |
| 1485 |  |  [Infilling Score: A Pretraining Data Detection Algorithm for Large Language Models](https://openreview.net/forum?id=9QPH1YQCMn) |  | 0 | In pretraining data detection, the goal is to detect whether a given sentence is in the dataset used for training a Large Language Model LLM). Recent methods (such as Min-K % and Min-K%++) reveal that most training corpora are likely contaminated with both sensitive content and evaluation... | Alex Dimakis, Constantine Caramanis, Giannis Daras, Litu Rout, Negin Raoof, Sanjay Shakkottai, Sujay Sanghavi |  |
| 1486 |  |  [Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model](https://openreview.net/forum?id=VxvnV6slP0) |  | 0 | The Mixture-of-Experts (MoE) has gained increasing attention in studying Large Vision-Language Models (LVLMs). It uses a sparse model to replace the dense model, achieving comparable performance while activating fewer parameters during inference, thus significantly reducing the inference cost.... | Chaoxiang Cai, Di Zhang, Dong Shen, Fan Yang, Longrong Yang, Tingting Gao, Xi Li |  |
| 1487 |  |  [ConvCodeWorld: Benchmarking Conversational Code Generation in Reproducible Feedback Environments](https://openreview.net/forum?id=rpouyo09V0) |  | 0 | Large language models (LLMs) have proven invaluable for code generation, particularly in interactive settings. However, existing code generation benchmarks fail to capture the diverse feedback encountered in multi-turn interactions, limiting our ability to evaluate LLMs in these contexts. To... | Hojae Han, Rajhans Samdani, Seungwon Hwang, Yuxiong He |  |
| 1488 |  |  [Neural Causal Graph for Interpretable and Intervenable Classification](https://openreview.net/forum?id=nmvmPIi185) |  | 0 | Advancements in neural networks have significantly enhanced the performance of classification models, achieving remarkable accuracy across diverse datasets. However, these models often lack transparency and do not support interactive reasoning with human users, which are essential attributes for... | Da Cao, Dongyu Wang, Jiawei Wang, Shaofei Lu, TatSeng Chua, Yuquan Le, Zhe Quan |  |
| 1489 |  |  [Loss Landscape of Shallow ReLU-like Neural Networks: Stationary Points, Saddle Escape, and Network Embedding](https://openreview.net/forum?id=ogKE7LcvW6) |  | 0 | In this paper, we study the loss landscape of one-hidden-layer neural networks with ReLU-like activation functions trained with the empirical squared loss using gradient descent (GD). We identify the stationary points of such networks, which significantly slow down loss decrease during training. To... | Berfin Simsek, François Gaston Ged, Zhengqing Wu |  |
| 1490 |  |  [TPO: Aligning Large Language Models with Multi-branch & Multi-step Preference Trees](https://openreview.net/forum?id=O0sQ9CPzai) |  | 0 | In the domain of complex reasoning tasks, such as mathematical reasoning, recent advancements have proposed the use of Direct Preference Optimization (DPO) to suppress output of dispreferred responses, thereby enhancing the long-chain reasoning capabilities of large language models (LLMs). To this... | Weibin Liao, Xu Chu, Yasha Wang |  |
| 1491 |  |  [VideoWebArena: Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks](https://openreview.net/forum?id=unDQOUah0F) |  | 0 | Videos are often used to learn or extract the necessary information to complete tasks in ways different than what text or static imagery can provide. However, many existing agent benchmarks neglect long-context video understanding, instead focus- ing on text or static image inputs. To bridge this... | Charles Ding, Dan Zhao, Justin Lin, Kazuhito Koishida, Lawrence Keunho Jang, Paul Pu Liang, Rogerio Bonatti, Yinheng Li |  |
| 1492 |  |  [A Skewness-Based Criterion for Addressing Heteroscedastic Noise in Causal Discovery](https://openreview.net/forum?id=zGzs5SIwT8) |  | 0 | Real-world data often violates the equal-variance assumption (homoscedasticity), making it essential to account for heteroscedastic noise in causal discovery. In this work, we explore heteroscedastic symmetric noise models (HSNMs), where the effect $Y$ is modeled as $Y = f(X) + \sigma(X)N$, with... | Biwei Huang, Haoran Deng, Ignavier Ng, Kun Zhang, Mingming Gong, Wenqin Liu, Yian Ma, Yingyu Lin, Yuxing Huang |  |
| 1493 |  |  [Shallow diffusion networks provably learn hidden low-dimensional structure](https://openreview.net/forum?id=KlxK4ncqWZ) |  | 0 | Diffusion-based generative models provide a powerful framework for learning to sample from a complex target distribution. The remarkable empirical success of these models applied to high-dimensional signals, including images and video, stands in stark contrast to classical results highlighting the... | Arthur Jacot, Ingvar M. Ziemann, Nicholas Matthew Boffi, Stephen Tu |  |
| 1494 |  |  [Exploring the Design Space of Visual Context Representation in Video MLLMs](https://openreview.net/forum?id=UN6Ik6OCx8) |  | 0 | Video Multimodal Large Language Models~(MLLMs) have shown remarkable capability of understanding the video semantics on various downstream tasks. Despite the advancements, there is still a lack of systematic research on visual context representation, which refers to the scheme to select frames from... | Bingning Wang, Han Huang, Haoyu Lu, JiRong Wen, Kun Zhou, Weipeng Chen, Xin Zhao, Yifan Du, Yuqi Huo, Zijia Zhao |  |
| 1495 |  |  [Adversarial Score identity Distillation: Rapidly Surpassing the Teacher in One Step](https://openreview.net/forum?id=lS2SGfWizd) |  | 0 | Score identity Distillation (SiD) is a data-free method that has achieved state-of-the-art performance in image generation by leveraging only a pretrained diffusion model, without requiring any training data. However, the ultimate performance of SiD is constrained by the accuracy with which the... | Hai Huang, Huangjie Zheng, Mingyuan Zhou, Yi Gu, Zhendong Wang |  |
| 1496 |  |  [Q-Adapter: Customizing Pre-trained LLMs to New Preferences with Forgetting Mitigation](https://openreview.net/forum?id=WLSrq1254E) |  | 0 | Large Language Models (LLMs), trained on a large amount of corpus, have demonstrated remarkable abilities. However, it may not be sufficient to directly apply open-source LLMs like Llama to certain real-world scenarios, since most of them are trained for \emph{general} purposes. Thus, the demands... | Bo An, Chengxing Jia, Fuxiang Zhang, Lei Yuan, Wenjie Qiu, Yang Yu, YiChen Li, Zongzhang Zhang |  |
| 1497 |  |  [Bridging the Gap between Variational Inference and Stochastic Gradient MCMC in Function Space](https://openreview.net/forum?id=bNVbOS3lrl) |  | 0 | Traditional parameter-space posterior inference for Bayesian neural networks faces several challenges, such as the difficulty in specifying meaningful prior, the potential pathologies in deep models and the intractability for multi-modal posterior. To address these issues, functional variational... | Jie Lu, Junyu Xuan, Mengjing Wu |  |
| 1498 |  |  [Flash Inference: Near Linear Time Inference for Long Convolution Sequence Models and Beyond](https://openreview.net/forum?id=cZWCjan02B) |  | 0 | While transformers have been at the core of most recent advancements in sequence generative models, their computational cost remains quadratic in sequence length. Several subquadratic architectures have been proposed to address this computational issue. Some of them, including long convolution... | CostinAndrei Oncescu, Sanket Purandare, Sham M. Kakade, Stratos Idreos |  |
| 1499 |  |  [σ-zero: Gradient-based Optimization of ℓ0-norm Adversarial Examples](https://openreview.net/forum?id=JMPOqoe4tl) |  | 0 | Evaluating the adversarial robustness of deep networks to gradient-based attacks is challenging. While most attacks consider $\ell_2$- and $\ell_\infty$-norm constraints to craft input perturbations, only a few investigate sparse $\ell_1$- and $\ell_0$-norm attacks. In particular, $\ell_0$-norm... | Antonio Emanuele Cinà, Battista Biggio, Francesco Villani, Lea Schönherr, Marcello Pelillo, Maura Pintor |  |
| 1500 |  |  [On the Adversarial Vulnerability of Label-Free Test-Time Adaptation](https://openreview.net/forum?id=N0ETIi580T) |  | 0 | Despite the success of Test-time adaptation (TTA), recent work has shown that adding relatively small adversarial perturbations to a limited number of samples leads to significant performance degradation. Therefore, it is crucial to rigorously evaluate existing TTA algorithms against relevant... | Ananthram Swami, Francesco Restuccia, Jonathan D. Ashdown, Michael J. De Lucia, Shahriar Rifat |  |
| 1501 |  |  [Strength Estimation and Human-Like Strength Adjustment in Games](https://openreview.net/forum?id=CvjXlsBLCX) |  | 0 | Strength estimation and adjustment are crucial in designing human-AI interactions, particularly in games where AI surpasses human players. This paper introduces a novel strength system, including a \*strength estimator\* (SE) and an SE-based Monte Carlo tree search, denoted as \*SE-MCTS\*, which... | Chun Jung Chen, ChungChin Shih, TiRong Wu |  |
| 1502 |  |  [Discrete Copula Diffusion](https://openreview.net/forum?id=FXw0okNcOb) |  | 0 | Discrete diffusion models have recently shown significant progress in modeling complex data, such as natural languages and DNA sequences. However, unlike diffusion models for continuous data, which can generate high-quality samples in just a few denoising steps, modern discrete diffusion models... | Anji Liu, Guy Van den Broeck, Mathias Niepert, Oliver Broadrick |  |
| 1503 |  |  [Adaptive Energy Alignment for Accelerating Test-Time Adaptation](https://openreview.net/forum?id=sEMJ1PLSZR) |  | 0 | In response to the increasing demand for tackling out-of-domain (OOD) scenarios, test-time adaptation (TTA) has garnered significant research attention in recent years. To adapt a source pre-trained model to target samples without getting access to their labels, existing approaches have typically... | DoYeon Kim, DongJun Han, Jaekyun Moon, Jungmoon Lee, Jungwuk Park, Wonjeong Choi, Younghyun Park |  |
| 1504 |  |  [Unsupervised Disentanglement of Content and Style via Variance-Invariance Constraints](https://openreview.net/forum?id=Lut5t3qElA) |  | 0 | We contribute an unsupervised method that effectively learns disentangled content and style representations from sequences of observations. Unlike most disentanglement algorithms that rely on domain-specific labels or knowledge, our method is based on the insight of domain-general statistical... | Bhiksha Raj, Gus Xia, Yuxuan Wu, Ziyu Wang |  |
| 1505 |  |  [InversionGNN: A Dual Path Network for Multi-Property Molecular Optimization](https://openreview.net/forum?id=nYPuSzGE3X) |  | 0 | Exploring chemical space to find novel molecules that simultaneously satisfy multiple properties is crucial in drug discovery. However, existing methods often struggle with trading off multiple properties due to the conflicting or correlated nature of chemical properties. To tackle this issue, we... | Jia Li, Junzhou Huang, Tingyang Xu, Yang Liu, Yatao Bian, Yifan Niu, Yu Rong, Ziqi Gao |  |
| 1506 |  |  [Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN](https://openreview.net/forum?id=BChpQU64RG) |  | 0 | Large Language Models (LLMs) have achieved remarkable success, yet recent findings reveal that their deeper layers often contribute minimally and can be pruned without affecting overall performance. While some view this as an opportunity for model compression, we identify it as a training shortfall... | Lu Yin, Pengxiang Li, Shiwei Liu |  |
| 1507 |  |  [Large Language Models Often Say One Thing and Do Another](https://openreview.net/forum?id=RTHbao4Mib) |  | 0 | As large language models (LLMs) increasingly become central to various applications and interact with diverse user populations, ensuring their reliable and consistent performance is becoming more important. This paper explores a critical issue in assessing the reliability of LLMs: the consistency... | Hongyu Lin, Jia Zheng, Le Sun, Ruoxi Xu, Weixiang Zhou, Xianpei Han, Yingfei Sun |  |
| 1508 |  |  [Learning Task Belief Similarity with Latent Dynamics for Meta-Reinforcement Learning](https://openreview.net/forum?id=5YbuOTUFQ4) |  | 0 | Meta-reinforcement learning requires utilizing prior task distribution information obtained during exploration to rapidly adapt to unknown tasks. The efficiency of an agent's exploration hinges on accurately identifying the current task. Recent Bayes-Adaptive Deep RL approaches often rely on... | Fuyuan Qian, Menglong Zhang, Quanying Liu |  |
| 1509 |  |  [Unearthing Skill-level Insights for Understanding Trade-offs of Foundation Models](https://openreview.net/forum?id=kNHVViEPWK) |  | 0 | With models getting stronger, evaluations have grown more complex, testing multiple skills in one benchmark and even in the same instance at once. However, skill-wise performance is obscured when inspecting aggregate accuracy, under-utilizing the rich signal modern benchmarks contain. We propose an... | Besmira Nushi, Mazda Moayeri, Neel Joshi, Safoora Yousefi, Soheil Feizi, Thomas Fel, Varun Chandrasekaran, Vibhav Vineet, Vidhisha Balachandran |  |
| 1510 |  |  [Decoupling Layout from Glyph in Online Chinese Handwriting Generation](https://openreview.net/forum?id=DhHIw9Nbl1) |  | 0 | Text plays a crucial role in the transmission of human civilization, and teaching machines to generate online handwritten text in various styles presents an interesting and significant challenge. However, most prior work has concentrated on generating individual Chinese fonts, leaving complete text... | Minsi Ren, YanMing Zhang, Yi Chen |  |
| 1511 |  |  [Handling Delay in Real-Time Reinforcement Learning](https://openreview.net/forum?id=YOc5t8PHf2) |  | 0 | Real-time reinforcement learning (RL) introduces several challenges. First, policies are constrained to a fixed number of actions per second due to hardware limitations. Second, the environment may change while the network is still computing an action, leading to observational delay. The first... | Irina Rish, Ivan Anokhin, Matthew Riemer, Rishav Rishav, Samira Ebrahimi Kahou, Stephen Chung |  |
| 1512 |  |  [Which Tasks Should Be Compressed Together? A Causal Discovery Approach for Efficient Multi-Task Representation Compression](https://openreview.net/forum?id=x33vSZUg0A) |  | 0 | Conventional image compression methods are inadequate for intelligent analysis, as they overemphasize pixel-level precision while neglecting semantic significance and the interaction among multiple tasks. This paper introduces a Taskonomy-Aware Multi-Task Compression framework comprising (1)... | Jing Chen, Lingyu Duan, Sha Guo, Wenhan Yang, Xing Jiang, Yu Lin, Zhuo Chen, Zixuan Hu |  |
| 1513 |  |  [CrossMPT: Cross-attention Message-passing Transformer for Error Correcting Codes](https://openreview.net/forum?id=gFvRRCnQvX) |  | 0 | Error correcting codes (ECCs) are indispensable for reliable transmission in communication systems. Recent advancements in deep learning have catalyzed the exploration of ECC decoders based on neural networks. Among these, transformer-based neural decoders have achieved state-of-the-art decoding... | Heeyoul Kwak, JongSeon No, SangHyo Kim, SeongJoon Park, Yongjune Kim |  |
| 1514 |  |  [Diversity Empowers Intelligence: Integrating Expertise of Software Engineering Agents](https://openreview.net/forum?id=cKlzKs3Nnb) |  | 0 | Large language model (LLM) agents have shown great potential in solving real-world software engineering (SWE) problems. The most advanced open-source SWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite. However, these sophisticated agent frameworks exhibit varying strengths,... | Bo Pang, Caiming Xiong, Huan Wang, Jiacheng Xu, Kexun Zhang, Lei Li, Renze Lou, Rithesh R. N., Shelby Heinecke, Silvio Savarese, Tian Lan, Weiran Yao, Yihao Feng, Yingbo Zhou, Zhiwei Liu, Zuxin Liu |  |
| 1515 |  |  [Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon](https://openreview.net/forum?id=3E8YNv1HjU) |  | 0 | Memorization in language models is typically treated as a homogenous phenomenon, neglecting the specifics of the memorized data. We instead model memorization as the effect of a set of complex factors that describe each sample and relate it to the model and corpus. To build intuition around these... | Alvin Deng, Christopher A. ChoquetteChoo, Jacob Ray Fuehne, Jaydeep Borkar, Jyothir S. V, Katherine Lee, Kyle O'Brien, Mohammad Aflah Khan, Naomi Saphra, Stella Biderman, Tracy Ke, USVSN Sai Prashanth |  |
| 1516 |  |  [Selective Label Enhancement Learning for Test-Time Adaptation](https://openreview.net/forum?id=3Z2flzXzBY) |  | 0 | Test-time adaptation (TTA) aims to adapt a pre-trained model to the target domain using only unlabeled test samples. Most existing TTA approaches rely on definite pseudo-labels, inevitably introducing false labels and failing to capture uncertainty for each test sample. This prevents pseudo-labels... | Congyu Qiao, Ning Xu, Xin Geng, Yihao Hu |  |
| 1517 |  |  [MambaExtend: A Training-Free Approach to Improve Long Context Extension of Mamba](https://openreview.net/forum?id=LgzRo1RpLS) |  | 0 | The inherent quadratic complexity of the attention mechanism in transformer models has driven the research community to explore alternative architectures with sub-quadratic complexity, such as state-space models. Mamba has established itself as a leading model within this emerging paradigm,... | Massoud Pedram, Mohammad Erfan Sadeghi, Seyedarmin Azizi, Souvik Kundu |  |
| 1518 |  |  [CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer](https://openreview.net/forum?id=LQzN6TRFg9) |  | 0 | We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos that align seamlessly with text prompts, with a frame rate of 16 fps and resolution of 768 x 1360 pixels. Previous video generation models often... | Bin Xu, Da Yin, Guanyu Feng, Jiayan Teng, Jiazheng Xu, Jie Tang, Ming Ding, Shiyu Huang, Weihan Wang, Wendi Zheng, Wenyi Hong, Xiaohan Zhang, Xiaotao Gu, Yean Cheng, Yuanming Yang, Yuxiao Dong, Yuxuan Zhang, Zhuoyi Yang |  |
| 1519 |  |  [PeriodWave: Multi-Period Flow Matching for High-Fidelity Waveform Generation](https://openreview.net/forum?id=tQ1PmLfPBL) |  | 0 | Recently, universal waveform generation tasks have been investigated conditioned on various out-of-distribution scenarios. Although one-step GAN-based methods have shown their strength in fast waveform generation, they are vulnerable to train-inference mismatch scenarios such as two-stage... | HaYeong Choi, SangHoon Lee, SeongWhan Lee |  |
| 1520 |  |  [ADBM: Adversarial Diffusion Bridge Model for Reliable Adversarial Purification](https://openreview.net/forum?id=g0rnZeBguq) |  | 0 | Recently Diffusion-based Purification (DiffPure) has been recognized as an effective defense method against adversarial examples. However, we find DiffPure which directly employs the original pre-trained diffusion models for adversarial purification, to be suboptimal. This is due to an inherent... | Huanran Chen, Jie Shi, Qiongxiu Li, Wenxuan Sun, Xiao Li, Xiaolin Hu, Yingzhe He |  |
| 1521 |  |  [A Non-Contrastive Learning Framework for Sequential Recommendation with Preference-Preserving Profile Generation](https://openreview.net/forum?id=Ke2BEL4csm) |  | 0 | Contrastive Learning (CL) proves to be effective for learning generalizable user representations in Sequential Recommendation (SR), but it suffers from high computational costs due to its reliance on negative samples. To overcome this limitation, we propose the first Non-Contrastive Learning (NCL)... | Anoop Jain, Dong Wang, Huimin Zeng, Xiaojie Wang, Zhicheng Dou |  |
| 1522 |  |  [Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models](https://openreview.net/forum?id=1GTARJhxtq) |  | 0 | In this work, we investigate whether small language models can determine high-quality subsets of large-scale text datasets that improve the performance of larger language models. While existing work has shown that pruning based on the perplexity of a larger model can yield high-quality data, we... | Cody Blakeney, Kartik Sreenivasan, Mansheej Paul, Matthew L. Leavitt, Max Marion, Zachary Ankner |  |
| 1523 |  |  [Discriminator-Guided Embodied Planning for LLM Agent](https://openreview.net/forum?id=TjP1d8PP8l) |  | 0 | Large Language Models (LLMs) have showcased remarkable reasoning capabilities in various domains, yet face challenges in complex embodied tasks due to the need for a coherent long-term policy and context-sensitive environmental understanding. Previous work performed LLM refinement relying on... | Chenjia Bai, Fei Wu, Haofu Qian, Jiatao Zhang, Wei Song, Xuelong Li |  |
| 1524 |  |  [MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning](https://openreview.net/forum?id=HVtu26XDAA) |  | 0 | We present MM1.5, a new family of multimodal large language models (MLLMs) designed to enhance capabilities in text-rich image understanding, visual referring and grounding, and multi-image reasoning. Building upon the MM1 architecture, MM1.5 adopts a data-centric approach to model training,... | Aleksei Timofeev, Bowen Zhang, Dhruti Shah, Forrest Huang, Haotian Zhang, Haoxuan You, HongYou Chen, JeanPhilippe Fauconnier, Keen You, Mingfei Gao, Mingze Xu, Nina Wenzel, Philipp Dufter, Sam Dodge, Xianzhi Du, Yanghao Li, Zhe Gan, Zhen Yang, Zhengfeng Lai, Zirui Wang, et al. |  |
| 1525 |  |  [Flow matching achieves almost minimax optimal convergence](https://openreview.net/forum?id=2OMyAFjiJJ) |  | 0 | Flow matching (FM) has gained significant attention as a simulation-free generative model. Unlike diffusion models, which are based on stochastic differential equations, FM employs a simpler approach by solving an ordinary differential equation with an initial condition from a normal distribution,... | Kazusato Oko, Kenji Fukumizu, Masanori Koyama, Noboru Isobe, Taiji Suzuki |  |
| 1526 |  |  [Pursuing Feature Separation based on Neural Collapse for Out-of-Distribution Detection](https://openreview.net/forum?id=mUXdysoxEP) |  | 0 | In the open world, detecting out-of-distribution (OOD) data, whose labels are disjoint with those of in-distribution (ID) samples, is important for reliable deep neural networks (DNNs). To achieve better detection performance, one type of approach proposes to fine-tune the model with auxiliary OOD... | Ruiji Yu, Xiaolin Huang, Xinwen Cheng, Yingwen Wu, Zhengbao He |  |
| 1527 |  |  [MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models](https://openreview.net/forum?id=qIbbBSzH6n) |  | 0 | Multimodal foundation models (MMFMs) play a crucial role in various applications, including autonomous driving, healthcare, and virtual assistants. However, several studies have revealed vulnerabilities in these models, such as generating unsafe content by text-to-image models. Existing benchmarks... | Alexander Xiong, Andy Zhou, Chejian Xu, Chengquan Guo, Chenhui Zhang, Chulin Xie, Francesco Pinto, Jeffrey Ziwei Tan, Jiawei Zhang, Lingzhi Yuan, Mintong Kang, Peiyang Xu, Xuandong Zhao, Yi Zeng, Yujin Potter, Zhaorun Chen, Zhen Xiang, Zhun Wang, Zhuowen Yuan, Zidi Xiong, et al. |  |
| 1528 |  |  [Ferret-UI 2: Mastering Universal User Interface Understanding Across Platforms](https://openreview.net/forum?id=GBfYgjOfSe) |  | 0 | Building a generalist model for user interface (UI) understanding is challenging due to various foundational issues, such as platform diversity, resolution variation, and data limitation. In this paper, we introduce Ferret-UI 2, a multimodal large language model (MLLM) designed for universal UI... | Di Feng, Haotian Zhang, Harsh Agrawal, Jeffrey Nichols, Keen You, Mohana Prasad Sathya Moorthy, Xiujun Li, Yinfei Yang, Zhangheng Li, Zhe Gan |  |
| 1529 |  |  [MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models](https://openreview.net/forum?id=Usklli4gMc) |  | 0 | Existing multimodal retrieval benchmarks primarily focus on evaluating whether models can retrieve and utilize external textual knowledge for question answering. However, there are scenarios where retrieving visual information is either more beneficial or easier to access than textual data. In this... | JiaChen Gu, KaiWei Chang, Mohsen Fayyaz, Nanyun Peng, Pan Lu, Wenbo Hu, ZiYi Dou |  |
| 1530 |  |  [Conflict-Averse Gradient Aggregation for Constrained Multi-Objective Reinforcement Learning](https://openreview.net/forum?id=ogXkmugNZw) |  | 0 | In real-world applications, a reinforcement learning (RL) agent should consider multiple objectives and adhere to safety guidelines. To address these considerations, we propose a constrained multi-objective RL algorithm named constrained multi-objective gradient aggregator (CoMOGA). In the field of... | Dohyeong Kim, Jeongho Park, Mineui Hong, Songhwai Oh |  |
| 1531 |  |  [MotionDreamer: One-to-Many Motion Synthesis with Localized Generative Masked Transformer](https://openreview.net/forum?id=d23EVDRJ6g) |  | 0 | Generative masked transformer have demonstrated remarkable success across various content generation tasks, primarily due to their ability to effectively model large-scale dataset distributions with high consistency. However, in the animation domain, large datasets are not always available.... | Chuan Guo, Hai Jiang, Juwei Lu, Li Cheng, Muhammad Gohar Javed, Xinxin Zuo, Yilin Wang, Yuxuan Mu |  |
| 1532 |  |  [DynAlign: Unsupervised Dynamic Taxonomy Alignment for Cross-Domain Segmentation](https://openreview.net/forum?id=IdAyXxBud7) |  | 0 | Current unsupervised domain adaptation (UDA) methods for semantic segmentation typically assume identical class labels between the source and target domains. This assumption ignores the label-level domain gap, which is common in real-world scenarios, and limits their ability to identify... | Han Sun, Ismail Nejjar, Olga Fink, Rui Gong |  |
| 1533 |  |  [Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning](https://openreview.net/forum?id=09FiNmvNMw) |  | 0 | Complex logical reasoning tasks require a long sequence of reasoning, which a large language model (LLM) with chain-of-thought prompting still falls short. To alleviate this issue, neurosymbolic approaches incorporate a symbolic solver. Specifically, an LLM only translates a natural language... | Eunho Yang, Gyeongman Kim, Hyemin S. Lee, Hyun Ryu |  |
| 1534 |  |  [To Tackle Adversarial Transferability: A Novel Ensemble Training Method with Fourier Transformation](https://openreview.net/forum?id=KW8yzAOIZr) |  | 0 | Ensemble methods are commonly used for enhancing robustness in machine learning. However, due to the ''transferability'' of adversarial examples, the performance of an ensemble model can be seriously affected even it contains a set of independently trained sub-models. To address this issue, we... | Hu Ding, Ruomin Huang, Shihong Song, Wanlin Zhang, Weichen Lin |  |
| 1535 |  |  [Manifold Induced Biases for Zero-shot and Few-shot Detection of Generated Images](https://openreview.net/forum?id=7gGl6HB5Zd) |  | 0 | Distinguishing between real and AI-generated images, commonly referred to as 'image detection', presents a timely and significant challenge. Despite extensive research in the (semi-)supervised regime, zero-shot and few-shot solutions have only recently emerged as promising alternatives. Their main... | Amit Giloni, Guy Gilboa, Hisashi Kojima, Jonathan Brokman, Omer Hofman, Roman Vainshtein |  |
| 1536 |  |  [Federated Domain Generalization with Data-free On-server Matching Gradient](https://openreview.net/forum?id=8TERgu1Lb2) |  | 0 | Domain Generalization (DG) aims to learn from multiple known source domains a model that can generalize well to unknown target domains. One of the key approaches in DG is training an encoder which generates domain-invariant representations. However, this approach is not applicable in Federated... | Duong Minh Nguyen, Jinsun Park, TrongBinh Nguyen, Viet Quoc Pham, WonJoo Hwang |  |
| 1537 |  |  [The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws](https://openreview.net/forum?id=ud8FtE1N4N) |  | 0 | Pruning eliminates unnecessary parameters in neural networks; it offers a promising solution to the growing computational demands of large language models (LLMs). While many focus on post-training pruning, sparse pre-training--which combines pruning and pre-training into a single phase--provides a... | Ahmed Imtiaz Humayun, Amir Yazdanbakhsh, Dan Alistarh, Gintare Karolina Dziugaite, Suvinay Subramanian, Tian Jin, Utku Evci |  |
| 1538 |  |  [Wayward Concepts In Multimodal Models](https://openreview.net/forum?id=74vnDs1R97) |  | 0 | Large multimodal models such as Stable Diffusion can generate, detect, and classify new visual concepts after optimizing just the prompt. How are prompt embeddings for visual concepts found by prompt tuning methods different from typical discrete prompts? We conduct a large-scale analysis on three... | Brandon Trabucco, Kyle Doherty, Max Gurinas, Russ Salakhutdinov |  |
| 1539 |  |  [MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer](https://openreview.net/forum?id=ExuBFYtCQU) |  | 0 | The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems. The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability. Non-autoregressive systems require... | Haotian Guo, Haoyue Zhan, Jiachen Zheng, Liwei Liu, Qiang Zhang, Ruihong Zeng, Shunsi Zhang, Xueyao Zhang, Yuancheng Wang, Zhizheng Wu |  |
| 1540 |  |  [FedLWS: Federated Learning with Adaptive Layer-wise Weight Shrinking](https://openreview.net/forum?id=6RjQ54M1rM) |  | 0 | In Federated Learning (FL), weighted aggregation of local models is conducted to generate a new global model, and the aggregation weights are typically normalized to 1. A recent study identifies the global weight shrinking effect in FL, indicating an enhancement in the global model’s generalization... | Changlong Shi, Dandan Guo, He Zhao, Jinmeng Li, Yi Chang |  |
| 1541 |  |  [Sail into the Headwind: Alignment via Robust Rewards and Dynamic Labels against Reward Hacking](https://openreview.net/forum?id=I8af9JdQTy) |  | 0 | Aligning AI systems with human preferences typically suffers from the infamous \*reward hacking\* problem, where optimization of an imperfect reward model leads to undesired behaviors. In this paper, we investigate reward hacking in offline preference optimization, which aims to improve an initial... | Paria Rashidinejad, Yuandong Tian |  |
| 1542 |  |  [MA2E: Addressing Partial Observability in Multi-Agent Reinforcement Learning with Masked Auto-Encoder](https://openreview.net/forum?id=klpdEThT8q) |  | 0 | Centralized Training and Decentralized Execution (CTDE) is a widely adopted paradigm to solve cooperative multi-agent reinforcement learning (MARL) problems. Despite the successes achieved with CTDE, partial observability still limits cooperation among agents. While previous studies have attempted... | Gahee Kim, SeYoung Yun, Sehyeok Kang, Song Chong, Yongsik Lee |  |
| 1543 |  |  [Training Free Exponential Context Extension via Cascading KV Cache](https://openreview.net/forum?id=dSneEp59yX) |  | 0 | The transformer's context window is vital for tasks such as few-shot learning and conditional generation as it preserves previous tokens for active memory. However, as the context lengths increase, the computational costs grow quadratically, hindering the deployment of large language models (LLMs)... | Heejun Lee, Jeffrey Willette, Myeongjae Jeon, Sung Ju Hwang, Youngwan Lee |  |
| 1544 |  |  [Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens](https://openreview.net/forum?id=jQP5o1VAVc) |  | 0 | Scaling up autoregressive models in vision has not proven as beneficial as in large language models. In this work, we investigate this scaling problem in the context of text-to-image generation, focusing on two critical factors: whether models use discrete or continuous tokens, and whether tokens... | Chen Sun, Deqing Sun, Kaiming He, Lijie Fan, Michael Rubinstein, Siyang Qin, Tianhong Li, Yonglong Tian, Yuanzhen Li |  |
| 1545 |  |  [The AdEMAMix Optimizer: Better, Faster, Older](https://openreview.net/forum?id=jj7b3p5kLY) |  | 0 | Momentum based optimizers are central to a wide range of machine learning applications. These typically rely on an Exponential Moving Average (EMA) of gradients, which decays exponentially the present contribution of older gradients. This accounts for gradients being local linear approximations... | David Grangier, Matteo Pagliardini, Pierre Ablin |  |
| 1546 |  |  [FIG: Flow with Interpolant Guidance for Linear Inverse Problems](https://openreview.net/forum?id=fs2Z2z3GRx) |  | 0 | Diffusion and flow matching models have recently been used to solve various linear inverse problems in image restoration, such as super-resolution and inpainting. Using a pre-trained diffusion or flow-matching model as a prior, most existing methods modify the reverse-time sampling process by... | Xiangming Meng, Yichi Zhang, Yici Yan, Zhizhen Zhao |  |
| 1547 |  |  [Scaling Long Context Training Data by Long-Distance Referrals](https://openreview.net/forum?id=tePFpDgyqg) |  | 0 | Training large language models for long context understanding faces the challenge of data shortage. Previous data engineering approaches mechanically concatenate short documents, which may create many pseudo long documents but raise concerns about data quality. In this paper, we study the core... | Eric P. Xing, Hao Zhang, Lanxiang Hu, Longfei Yun, Souvik Kundu, Yonghao Zhuang, Zhengzhong Liu |  |
| 1548 |  |  [Exploring Learning Complexity for Efficient Downstream Dataset Pruning](https://openreview.net/forum?id=FN7n7JRjsk) |  | 0 | The ever-increasing fine-tuning cost of large-scale pre-trained models gives rise to the importance of dataset pruning, which aims to reduce dataset size while maintaining task performance. However, existing dataset pruning methods require training on the entire dataset, which is impractical for... | Bingyi Jing, Hongxin Wei, Songxin Zhang, Wenyu Jiang, Zejian Xie, Zhenlong Liu |  |
| 1549 |  |  [Event-Driven Online Vertical Federated Learning](https://openreview.net/forum?id=FCBbh0HCrF) |  | 0 | Online learning is more adaptable to real-world scenarios in Vertical Federated Learning (VFL) compared to offline learning. However, integrating online learning into VFL presents challenges due to the unique nature of VFL, where clients possess non-intersecting feature sets for the same sample. In... | Bin Gu, Boyu Wang, Charles Ling, Ganyu Wang |  |
| 1550 |  |  [DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References](https://openreview.net/forum?id=ajSmXqgS24) |  | 0 | We address the challenge of developing a generalizable neural tracking controller for dexterous manipulation from human references. This controller aims to manage a dexterous robot hand to manipulate diverse objects for various purposes defined by kinematic human-object interactions. Developing... | Jianibieke Adalibieke, Li Yi, Qianwei Han, Xueyi Liu, Yuzhe Qin |  |
| 1551 |  |  [MELODI: Exploring Memory Compression for Long Contexts](https://openreview.net/forum?id=TvGPP8i18S) |  | 0 | We present MELODI, a novel memory architecture designed to efficiently process long documents using short context windows. The key principle behind MELODI is to represent short-term and long-term memory as a hierarchical compression scheme across both transformer layers and context windows.... | Andrey Zhmoginov, Aren Jansen, David Racz, DeLesley Hutchins, Jesper Sparre Andersen, Yinpeng Chen |  |
| 1552 |  |  [CryoFM: A Flow-based Foundation Model for Cryo-EM Densities](https://openreview.net/forum?id=T4sMzjy7fO) |  | 0 | Cryo-electron microscopy (cryo-EM) is a powerful technique in structural biology and drug discovery, enabling the study of biomolecules at high resolution. Significant advancements by structural biologists using cryo-EM have led to the production of around 40k protein density maps at various... | Jing Yuan, Quanquan Gu, Yi Zhou, Yilai Li |  |
| 1553 |  |  [Empowering LLM Agents with Zero-Shot Optimal Decision-Making through Q-learning](https://openreview.net/forum?id=JsVIGVntnQ) |  | 0 | Large language models (LLMs) are trained on extensive text data to gain general comprehension capability. Current LLM agents leverage this ability to make zero- or few-shot decisions without reinforcement learning (RL) but fail in making optimal decisions, as LLMs inherently perform next-token... | Dongbin Zhao, Jiajun Chai, Sicheng Li, Yuanheng Zhu, Yuqian Fu |  |
| 1554 |  |  [Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA](https://openreview.net/forum?id=WwpYSOkkCt) |  | 0 | Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. In this work, we revisit "layer tying" as form of parameter sharing in Transformers, and introduce novel... | Adam Fisch, Hrayr Harutyunyan, Sangmin Bae, Seungyeon Kim, Tal Schuster, Ziwei Ji |  |
| 1555 |  |  [Controllable Generation via Locally Constrained Resampling](https://openreview.net/forum?id=8g4XgC8HPF) |  | 0 | Autoregressive models have demonstrated an unprecedented ability at modeling the intricacies of natural language. However, they continue to struggle with generating complex outputs that adhere to logical constraints. Sampling from a fully-independent distribution subject to a constraint is hard.... | Guy Van den Broeck, KaiWei Chang, Kareem Ahmed |  |
| 1556 |  |  [Hot-pluggable Federated Learning: Bridging General and Personalized FL via Dynamic Selection](https://openreview.net/forum?id=B8akWa62Da) |  | 0 | Personalized federated learning (PFL) achieves high performance by assuming clients only meet test data locally, which does not meet many generic federated learning (GFL) scenarios. In this work, we theoretically show that PMs can be used to enhance GFL with a new learning problem named Selective... | Bo Han, Lei Shen, Lijun Wu, Tao Qin, Xiaowen Chu, Yonggang Zhang, Zhenheng Tang |  |
| 1557 |  |  [Stable Hadamard Memory: Revitalizing Memory-Augmented Agents for Reinforcement Learning](https://openreview.net/forum?id=We5z3UEnUY) |  | 0 | Effective decision-making in partially observable environments demands robust memory management. Despite their success in supervised learning, current deep-learning memory models struggle in reinforcement learning environments that are partially observable and long-term. They fail to efficiently... | Dung Nguyen, Hung Le, Kien Do, Sunil Gupta, Svetha Venkatesh |  |
| 1558 |  |  [Lipschitz Bandits in Optimal Space](https://openreview.net/forum?id=i7k2sXSW1b) |  | 0 | This paper considers the Lipschitz bandit problem, where the set of arms is continuous and the expected reward is a Lipschitz function over the arm space. This problem has been extensively studied. Prior algorithms need to store the reward information of all visited arms, leading to significant... | Xiaoyi Zhu, Zengfeng Huang |  |
| 1559 |  |  [Fast and Slow Streams for Online Time Series Forecasting Without Information Leakage](https://openreview.net/forum?id=I0n3EyogMi) |  | 0 | Current research in online time series forecasting (OTSF) faces two significant issues. The first is information leakage, where models make predictions and are then evaluated on historical time steps that have already been used in backpropagation for parameter updates. The second is practicality:... | DitYan Yeung, Yingyee Ava Lau, Zhiwen Shao |  |
| 1560 |  |  [Unlearning or Obfuscating? Jogging the Memory of Unlearned LLMs via Benign Relearning](https://openreview.net/forum?id=fMNRYBvcQN) |  | 0 | Machine unlearning is a promising approach to mitigate undesirable memorization of training data in ML models. However, in this work we show that existing approaches for unlearning in LLMs are surprisingly susceptible to a simple set of benign relearning attacks. With access to only a small and... | Shengyuan Hu, Steven Z. Wu, Virginia Smith, Yiwei Fu |  |
| 1561 |  |  [ToVE: Efficient Vision-Language Learning via Knowledge Transfer from Vision Experts](https://openreview.net/forum?id=EMMnAd3apQ) |  | 0 | Vision-language (VL) learning requires extensive visual perception capabilities, such as fine-grained object recognition and spatial perception. Recent works typically rely on training huge models on massive datasets to develop these capabilities. As a more efficient alternative, this paper... | Junlong Du, Ke Yan, Shouhong Ding, Xiaoqiang Li, Yuanchen Wu |  |
| 1562 |  |  [SaMer: A Scenario-aware Multi-dimensional Evaluator for Large Language Models](https://openreview.net/forum?id=aBnVU5DL3I) |  | 0 | Evaluating the response quality of large language models (LLMs) for open-ended questions poses a significant challenge, especially given the subjectivity and multi-dimensionality of "quality" in natural language generation. Existing LLM evaluators often neglect that different scenarios require... | Chengfei Lv, Gang Yu, Huajun Chen, Jing Yu, Kehua Feng, Keyan Ding, Qiang Zhang, Yiwen Qu, Zhiwen Chen |  |
| 1563 |  |  [Towards Hierarchical Rectified Flow](https://openreview.net/forum?id=6F6qwdycgJ) |  | 0 | We formulate a hierarchical rectified flow to model data distributions. It hierarchically couples multiple ordinary differential equations (ODEs) and defines a time-differentiable stochastic process that generates a data distribution from a known source distribution. Each ODE resembles the ODE that... | Alexander G. Schwing, Yichi Zhang, Yici Yan, Zhizhen Zhao |  |
| 1564 |  |  [A Theoretical Framework for Partially-Observed Reward States in RLHF](https://openreview.net/forum?id=OjAU0LLDbe) |  | 0 | The growing deployment of reinforcement learning from human feedback (RLHF) calls for a deeper theoretical investigation of its underlying models. The prevalent models of RLHF do not account for neuroscience-backed, partially-observed "internal states'' that can affect human feedback, nor do they... | Aldo Pacchiano, Ambuj Tewari, Chinmaya Kausik, Mirco Mutti |  |
| 1565 |  |  [CONTRA: Conformal Prediction Region via Normalizing Flow Transformation](https://openreview.net/forum?id=pOO9cqLq7Q) |  | 0 | Density estimation and reliable prediction regions for outputs are crucial in supervised and unsupervised learning. While conformal prediction effectively generates coverage-guaranteed regions, it struggles with multi-dimensional outputs due to reliance on one-dimensional nonconformity scores. To... | Aixin Tan, Jian Huang, Zhenhan Fang |  |
| 1566 |  |  [ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities](https://openreview.net/forum?id=cPD2hU35x3) |  | 0 | In this work, we introduce ChatQA 2, an Llama 3.0-based model with a 128K context window, designed to bridge the gap between open-source LLMs and leading proprietary models (e.g., GPT-4-Turbo-2024-04-09) in long context un- derstanding and retrieval-augmented generation (RAG) capabilities. These... | Bryan Catanzaro, Chejian Xu, Mohammad Shoeybi, Peng Xu, Wei Ping, Xianchao Wu, Zihan Liu |  |
| 1567 |  |  [Rethinking Artistic Copyright Infringements In the Era Of Text-to-Image Generative Models](https://openreview.net/forum?id=0OTVNEm9N4) |  | 0 | The advent of text-to-image generative models has led artists to worry that their individual styles may be copied, creating a pressing need to reconsider the lack of protection for artistic styles under copyright law. This requires answering challenging questions, like what defines style and what... | Atoosa Malemir Chegini, Mazda Moayeri, Priyatham Kattakinda, Robert Brauneis, Samyadeep Basu, Soheil Feizi, Sriram Balasubramanian |  |
| 1568 |  |  [Dysca: A Dynamic and Scalable Benchmark for Evaluating Perception Ability of LVLMs](https://openreview.net/forum?id=bU1JOvdXXK) |  | 0 | Currently many benchmarks have been proposed to evaluate the perception ability of the Large Vision-Language Models (LVLMs). However, most benchmarks conduct questions by selecting images from existing datasets, resulting in the potential data leakage. Besides, these benchmarks merely focus on... | Bei Yan, Jie Zhang, Mengqi Lei, Shiguang Shan, Xilin Chen, Zheng Yuan, Zhongqi Wang |  |
| 1569 |  |  [Heavy-Tailed Diffusion Models](https://openreview.net/forum?id=tozlOEN4qp) |  | 0 | Diffusion models achieve state-of-the-art generation quality across many applications, but their ability to capture rare or extreme events in heavy-tailed distributions remains unclear. In this work, we show that traditional diffusion and flow-matching models with standard Gaussian priors fail to... | Arash Vahdat, Jaideep Pathak, Kushagra Pandey, Michael S. Pritchard, Morteza Mardani, Stephan Mandt, Yilun Xu |  |
| 1570 |  |  [Efficient Inference for Large Language Model-based Generative Recommendation](https://openreview.net/forum?id=ACSNlt77hq) |  | 0 | Large Language Model (LLM)-based generative recommendation has achieved notable success, yet its practical deployment is costly particularly due to excessive inference latency caused by autoregressive decoding. For lossless LLM decoding acceleration, Speculative Decoding (SD) has emerged as a... | Chaoqun Yang, Cunxiao Du, Fuli Feng, SeeKiong Ng, TatSeng Chua, Wenjie Wang, Xinyu Lin, Yongqi Li |  |
| 1571 |  |  [Refine Knowledge of Large Language Models via Adaptive Contrastive Learning](https://openreview.net/forum?id=HqjRlT65WX) |  | 0 | How to alleviate the hallucinations of Large Language Models (LLMs) has always been the fundamental goal pursued by the LLMs research community. Looking through numerous hallucination-related studies, a mainstream category of methods is to reduce hallucinations by optimizing the knowledge... | Chao Qu, HaiTao Zheng, Haojing Huang, Jiayi Kuang, Philip S. Yu, ShuYu Guo, Xiaoyu Tan, Yangning Li, Ying Shen, Yinghui Li |  |
| 1572 |  |  [Dynamic Contrastive Skill Learning with State-Transition Based Skill Clustering and Dynamic Length Adjustment](https://openreview.net/forum?id=8egnwady4b) |  | 0 | Reinforcement learning (RL) has made significant progress in various domains, but scaling it to long-horizon tasks with complex decision-making remains challenging. Skill learning attempts to address this by abstracting actions into higher-level behaviors. However, current approaches often fail to... | Jinwoo Choi, SeungWoo Seo |  |
| 1573 |  |  [MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal Projection](https://openreview.net/forum?id=BQwsRy1h3U) |  | 0 | KV cache has become a \*de facto\* technique for the inference of large language models (LLMs), where tensors of shape (layer number, head number, sequence length, feature dimension) are introduced to cache historical information for self-attention. As the size of the model and data grows, the KV... | Bokai Lin, Hao Zhang, Siqi Kou, TianQi Hou, Xiaofeng Gao, Zhijie Deng, Zihao Zeng, Zipeng Xiao |  |
| 1574 |  |  [Do Stochastic, Feel Noiseless: Stable Stochastic Optimization via a Double Momentum Mechanism](https://openreview.net/forum?id=zCZnEXF3bN) |  | 0 | Optimization methods are crucial to the success of machine learning, with Stochastic Gradient Descent (SGD) serving as a foundational algorithm for training models. However, SGD is often sensitive to the choice of the learning rate, which necessitates extensive hyperparameter tuning. In this work,... | Kfir Yehuda Levy, Tehila Dahan |  |
| 1575 |  |  [Beyond Surface Structure: A Causal Assessment of LLMs' Comprehension ability](https://openreview.net/forum?id=gsShHPxkUW) |  | 0 | Large language models (LLMs) have shown remarkable capability in natural language tasks, yet debate persists on whether they truly comprehend deep structure (i.e., core semantics) or merely rely on surface structure (e.g., presentation format). Prior studies observe that LLMs' performance declines... | Chaochao Lu, Difan Zou, Lei Xu, Sirui Chen, Yujin Han |  |
| 1576 |  |  [Efficiently Parameterized Neural Metriplectic Systems](https://openreview.net/forum?id=uL1H29dM0c) |  | 0 | Metriplectic systems are learned from data in a way that scales quadratically in both the size of the state and the rank of the metriplectic operators. In addition to being provably energy-conserving and entropy-stable, the proposed neural metriplectic systems (NMS) approach includes approximation... | Anthony Gruber, Haksoo Lim, Kookjin Lee, Nathaniel Trask, Noseong Park |  |
| 1577 |  |  [Transformer Learns Optimal Variable Selection in Group-Sparse Classification](https://openreview.net/forum?id=fuoM5YDBX4) |  | 0 | Transformers have demonstrated remarkable success across various applications. However, the success of transformers have not been understood in theory. In this work, we give a case study of how transformers can be trained to learn a classic statistical model with "group sparsity", where the input... | Chenyang Zhang, Xuran Meng, Yuan Cao |  |
| 1578 |  |  [PaLD: Detection of Text Partially Written by Large Language Models](https://openreview.net/forum?id=rWjZWHYPcz) |  | 0 | Advances in large language models (LLM) have produced text that appears increasingly human-like and difficult to detect with the human eye. In order to mitigate the impact of misusing LLM-generated texts, e.g., copyright infringement, fair student assessment, fraud, and other societally harmful LLM... | ChunFu Chen, Eric Lei, Hsiang Hsu |  |
| 1579 |  |  [How Discrete and Continuous Diffusion Meet: Comprehensive Analysis of Discrete Diffusion Models via a Stochastic Integral Framework](https://openreview.net/forum?id=6awxwQEI82) |  | 0 | Discrete diffusion models have gained increasing attention for their ability to model complex distributions with tractable sampling and inference. However, the error analysis for discrete diffusion models remains less well-understood. In this work, we propose a comprehensive framework for the error... | Grant M. Rotskoff, Haoxuan Chen, Lexing Ying, Yinuo Ren |  |
| 1580 |  |  [Balanced Ranking with Relative Centrality: A multi-core periphery perspective](https://openreview.net/forum?id=21rSeWJHPF) |  | 0 | Ranking of vertices in a graph for different objectives is one of the most fundamental tasks in computer science. It is known that traditional ranking algorithms can generate unbalanced ranking when the graph has underlying communities, resulting in loss of information, polarised opinions, and... | Chandra Sekhar Mukherjee, Jiapeng Zhang |  |
| 1581 |  |  [Can Reinforcement Learning Solve Asymmetric Combinatorial-Continuous Zero-Sum Games?](https://openreview.net/forum?id=7YKV7zkNpX) |  | 0 | There have been extensive studies on learning in zero-sum games, focusing on the analysis of the existence and algorithmic convergence of Nash equilibrium (NE). Existing studies mainly focus on symmetric games where the strategy spaces of the players are of the same type and size. For the few... | Haipeng Chen, Panpan Wang, Yuheng Li |  |
| 1582 |  |  [Learning Robust Representations with Long-Term Information for Generalization in Visual Reinforcement Learning](https://openreview.net/forum?id=PDtMrogheZ) |  | 0 | Generalization in visual reinforcement learning (VRL) aims to learn agents that can adapt to test environments with unseen visual distractions. Despite advances in robust representations learning, many methods do not take into account the essential downstream task of sequential decision-making.... | Bin Li, Guoping Wu, Jie Wang, Qijie Peng, Rui Yang, Ruibo Guo |  |
| 1583 |  |  [Zeroth-Order Fine-Tuning of LLMs with Transferable Static Sparsity](https://openreview.net/forum?id=myYzr50xBh) |  | 0 | Zeroth-order optimization (ZO) is a memory-efficient strategy for fine-tuning Large Language Models using only forward passes. However, applying ZO fine-tuning in memory-constrained settings such as mobile phones and laptops remains challenging since these settings often involve weight... | Beidi Chen, Christopher De Sa, Jacob R. Gardner, Jikai Long, Osbert Bastani, Wentao Guo, Xiaodong Yu, Xinyu Yang, Yide Ran, Yimeng Zeng, Zhaozhuo Xu, Zirui Liu |  |
| 1584 |  |  [Bootstrapped Model Predictive Control](https://openreview.net/forum?id=i7jAYFYDcM) |  | 0 | Model Predictive Control (MPC) has been demonstrated to be effective in continuous control tasks. When a world model and a value function are available, planning a sequence of actions ahead of time leads to a better policy. Existing methods typically obtain the value function and the corresponding... | Hanwei Guo, Long Qian, Sizhe Wang, Xuguang Lan, Yuhang Wang |  |
| 1585 |  |  [Group Ligands Docking to Protein Pockets](https://openreview.net/forum?id=zDC3iCBxJb) |  | 0 | Molecular docking is a key task in computational biology that has attracted increasing interest from the machine learning community. While existing methods have achieved success, they generally treat each protein-ligand pair in isolation. Inspired by the biochemical observation that ligands binding... | Jiahan Li, Jian Peng, Jianzhu Ma, Jiaqi Guan, Sheng Wang, Xiangxin Zhou, Xingang Peng, Yunan Luo |  |
| 1586 |  |  [Revisiting Mode Connectivity in Neural Networks with Bezier Surface](https://openreview.net/forum?id=1NevL7zdHS) |  | 0 | Understanding the loss landscapes of neural networks (NNs) is critical for optimizing model performance. Previous research has identified the phenomenon of mode connectivity on curves, where two well-trained NNs can be connected by a continuous path in parameter space where the path maintains... | Jie Ren, PinYu Chen, Ren Wang |  |
| 1587 |  |  [Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent](https://openreview.net/forum?id=VvDEuyVXkG) |  | 0 | Multimodal Retrieval Augmented Generation (mRAG) plays an important role in mitigating the “hallucination” issue inherent in multimodal large language models (MLLMs). Although promising, existing heuristic mRAGs typically predefined fixed retrieval processes, which causes two issues: (1)... | Fei Huang, HaiTao Zheng, Hui Wang, Jingren Zhou, Philip S. Yu, Xinran Zheng, Xinyu Wang, Yangning Li, Yinghui Li, Yong Jiang, Zhen Zhang |  |
| 1588 |  |  [AdaGrad under Anisotropic Smoothness](https://openreview.net/forum?id=4GT9uTsAJE) |  | 0 | Adaptive gradient methods have been widely adopted in training large-scale deep neural networks, especially large foundation models. Despite the huge success in practice, their theoretical advantages over classical gradient methods with uniform step sizes across all coordinates (e.g. SGD) have not... | Rui Pan, Tong Zhang, Yuxing Liu |  |
| 1589 |  |  [Geometry of Long-Tailed Representation Learning: Rebalancing Features for Skewed Distributions](https://openreview.net/forum?id=GySIAKEwtZ) |  | 0 | Deep learning has achieved significant success by training on balanced datasets. However, real-world data often exhibit long-tailed distributions. Empirical studies have revealed that long-tailed data skew data representations, where head classes dominate the feature space. Many methods have been... | Chao Chen, Haibin Ling, Jiachen Yao, Lingjie Yi, Raphael Douady, Weimin Lyu |  |
| 1590 |  |  [IMDPrompter: Adapting SAM to Image Manipulation Detection by Cross-View Automated Prompt Learning](https://openreview.net/forum?id=vl7kf0YHwj) |  | 0 | Using extensive training data from SA-1B, the Segment Anything Model (SAM) has demonstrated exceptional generalization and zero-shot capabilities, attracting widespread attention in areas such as medical image segmentation and remote sensing image segmentation. However, its performance in the field... | Chun Yuan, Jinwei Fang, Ke Zhang, Quan Zhang, Xi Lin, Xi Tang, Yuxin Qi |  |
| 1591 |  |  [Is Your Multimodal Language Model Oversensitive to Safe Queries?](https://openreview.net/forum?id=QsA3YzNUxA) |  | 0 | Humans are prone to cognitive distortions — biased thinking patterns that lead to exaggerated responses to specific stimuli, albeit in very different contexts. This paper demonstrates that advanced Multimodal Large Language Models (MLLMs) exhibit similar tendencies. While these models are designed... | ChoJui Hsieh, Hengguang Zhou, Minhao Cheng, Ruochen Wang, Tianyi Zhou, Xirui Li |  |
| 1592 |  |  [Grounding by Trying: LLMs with Reinforcement Learning-Enhanced Retrieval](https://openreview.net/forum?id=BPAZ6yW3K7) |  | 0 | The hallucinations of large language models (LLMs) are increasingly mitigated by allowing LLMs to search for information and to ground their answers in real sources. Unfortunately, LLMs often struggle with posing the right search queries, especially when dealing with complex or otherwise indirect... | Archit Sharma, Chelsea Finn, Omar Khattab, Sheryl Hsu |  |
| 1593 |  |  [MMR: A Large-scale Benchmark Dataset for Multi-target and Multi-granularity Reasoning Segmentation](https://openreview.net/forum?id=mzL19kKE3r) |  | 0 | The fusion of Large Language Models (LLMs) with vision models is pioneering new possibilities in user-interactive vision-language tasks. A notable application is reasoning segmentation, where models generate pixel-level segmentation masks by comprehending implicit meanings in human instructions.... | Daeshik Kim, Donggon Jang, Suin Lee, Taehyeon Kim, Yucheol Cho |  |
| 1594 |  |  [Meta-Continual Learning of Neural Fields](https://openreview.net/forum?id=OCpxDSn0G4) |  | 0 | Neural Fields (NF) have gained prominence as a versatile framework for complex data representation. This work unveils a new problem setting termed Meta-Continual Learning of Neural Fields (MCL-NF) and introduces a novel strategy that employs a modular architecture combined with optimization-based... | Gunhee Kim, Junhyeog Yun, Seungyoon Woo |  |
| 1595 |  |  [Forewarned is Forearmed: Harnessing LLMs for Data Synthesis via Failure-induced Exploration](https://openreview.net/forum?id=yitH9xAHQs) |  | 0 | Large language models (LLMs) have significantly benefited from training on diverse, high-quality task-specific data, leading to impressive performance across a range of downstream applications. Current methods often rely on human-annotated data or predefined task templates to direct powerful LLMs... | Chuan Wu, Jiahui Gao, Lingpeng Kong, Qintong Li, Renjie Pi, Sheng Wang, Xin Jiang, Xueliang Zhao, Zhenguo Li |  |
| 1596 |  |  [Needle In A Video Haystack: A Scalable Synthetic Evaluator for Video MLLMs](https://openreview.net/forum?id=ZJo6Radbqq) |  | 0 | Video understanding is a crucial next step for multimodal large language models (MLLMs). Various benchmarks are introduced for better evaluating the MLLMs. Nevertheless, current video benchmarks are still inefficient for evaluating video models during iterative development due to the high cost of... | Bingning Wang, Haoyu Lu, Jing Liu, Longteng Guo, Tongtian Yue, Weipeng Chen, Yifan Du, Yuqi Huo, Zijia Zhao |  |
| 1597 |  |  [Do Egocentric Video-Language Models Truly Understand Hand-Object Interactions?](https://openreview.net/forum?id=M8gXSFGkn2) |  | 0 | Egocentric video-language pretraining is a crucial step in advancing the understanding of hand-object interactions in first-person scenarios. Despite successes on existing testbeds, we find that current EgoVLMs can be easily misled by simple modifications, such as changing the verbs or nouns in... | Boshen Xu, Qin Jin, Sipeng Zheng, Yang Du, Zhinan Song, Ziheng Wang |  |
| 1598 |  |  [Boosting Methods for Interval-censored Data with Regression and Classification](https://openreview.net/forum?id=DzbUL4AJPP) |  | 0 | Boosting has garnered significant interest across both machine learning and statistical communities. Traditional boosting algorithms, designed for fully observed random samples, often struggle with real-world problems, particularly with interval-censored data. This type of data is common in... | Grace Y. Yi, Wenqing He, Yuan Bian |  |
| 1599 |  |  [ZETA: Leveraging Z-order Curves for Efficient Top-k Attention](https://openreview.net/forum?id=j9VVzueEbG) |  | 0 | Over recent years, the Transformer has become a fundamental building block for sequence modeling architectures. Yet at its core is the use of self-attention, whose memory and computational cost grow quadratically with the sequence length $N$, rendering it prohibitively expensive for long sequences.... | Boxing Chen, Boyu Wang, Charles Ling, Gezheng Xu, Jerry Huang, Peng Lu, Qiuhao Zeng |  |
| 1600 |  |  [When Prompt Engineering Meets Software Engineering: CNL-P as Natural and Robust "APIs" for Human-AI Interaction](https://openreview.net/forum?id=SqoL14HDm0) |  | 0 | With the growing capabilities of large language models (LLMs), they are increasingly applied in areas like intelligent customer service, code generation, and knowledge management. Natural language (NL) prompts act as the \`\`APIs'' for human-LLM interaction. To improve prompt quality, best... | Chenhua Liu, Daniel Sun, Dehai Zhao, Qing Huang, Yang Liu, Zhenchang Xing, Zhuo Cheng |  |
| 1601 |  |  [DreamBench++: A Human-Aligned Benchmark for Personalized Image Generation](https://openreview.net/forum?id=4GSOESJrk6) |  | 0 | Personalized image generation holds great promise in assisting humans in everyday work and life due to its impressive function in creatively generating personalized content. However, current evaluations either are automated but misalign with humans or require human evaluations that are... | Chunrui Han, Haomiao Tang, Jing Bai, Runpei Dong, ShuTao Xia, Xiangyu Zhang, Yuang Peng, Yuxin Cui, Zekun Qi, Zheng Ge |  |
| 1602 |  |  [Data-centric Prediction Explanation via Kernelized Stein Discrepancy](https://openreview.net/forum?id=KlV5CkNQkl) |  | 0 | Existing example-based prediction explanation methods often bridge test and training data points through the model’s parameters or latent representations. While these methods offer clues to the causes of model predictions, they often exhibit innate shortcomings, such as incurring significant... | Ga Wu, Hassan Sajjad, Mahtab Sarvmaili |  |
| 1603 |  |  [Accurate and Scalable Graph Neural Networks via Message Invariance](https://openreview.net/forum?id=UqrFPhcmFp) |  | 0 | Message passing-based graph neural networks (GNNs) have achieved great success in many real-world applications. For a sampled mini-batch of target nodes, the message passing process is divided into two parts: message passing between nodes within the batch (MP-IB) and message passing from nodes... | Bin Li, Feng Wu, Jie Wang, Xize Liang, Zhihao Shi, Zhiwei Zhuang |  |
| 1604 |  |  [SimXRD-4M: Big Simulated X-ray Diffraction Data and Crystal Symmetry Classification Benchmark](https://openreview.net/forum?id=mkuB677eMM) |  | 0 | Powder X-ray diffraction (XRD) patterns are highly effective for crystal identification and play a pivotal role in materials discovery. While machine learning (ML) has advanced the analysis of powder XRD patterns, progress has been constrained by the limited availability of training data and... | Bin Cao, Jia Li, Ruifeng Tan, TongYi Zhang, Yang Liu, Zinan Zheng |  |
| 1605 |  |  [Human Simulacra: Benchmarking the Personification of Large Language Models](https://openreview.net/forum?id=BCP5nAHXqs) |  | 0 | Large Language Models (LLMs) are recognized as systems that closely mimic aspects of human intelligence. This capability has attracted the attention of the social science community, who see the potential in leveraging LLMs to replace human participants in experiments, thereby reducing research... | Liang He, Linyi Yang, Qiming Feng, Qingqiu Li, Qiujie Xie, Rui Feng, Shang Gao, Tianqi Zhang, Yue Zhang, Yuejie Zhang |  |
| 1606 |  |  [Automatic Curriculum Expert Iteration for Reliable LLM Reasoning](https://openreview.net/forum?id=3ogIALgghF) |  | 0 | Hallucinations (i.e., generating plausible but inaccurate content) and laziness (i.e. excessive refusals or defaulting to "I don't know") persist as major challenges in LLM reasoning. Current efforts to reduce hallucinations primarily focus on factual errors in knowledge-grounded tasks, often... | Amrita Saha, Caiming Xiong, Doyen Sahoo, Hanze Dong, Zirui Zhao |  |
| 1607 |  |  [Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning](https://openreview.net/forum?id=FiyS0ecSm0) |  | 0 | Large language models (LLMs) can prove mathematical theorems formally by generating proof steps (\textit{a.k.a.} tactics) within a proof system. However, the space of possible tactics is vast and complex, while the available training data for formal proofs is limited, posing a significant challenge... | Fan Yang, Kaiyu Yang, Wen Tang, Xian Zhang, Xiaoxing Ma, Xujie Si, Yuan Yao, Zenan Li, Zhaoyu Li |  |
| 1608 |  |  [DUALFormer: Dual Graph Transformer](https://openreview.net/forum?id=4v4RcAODj9) |  | 0 | Graph Transformers (GTs), adept at capturing the locality and globality of graphs, have shown promising potential in node classification tasks. Most state-of-the-art GTs succeed through integrating local Graph Neural Networks (GNNs) with their global Self-Attention (SA) modules to enhance... | Chuan Wang, Jiaming Zhuo, Kun Fu, Liang Yang, Xiaochun Cao, Yintong Lu, Yuanfang Guo, Yuwei Liu, Zhen Wang, Ziyi Ma |  |
| 1609 |  |  [Progress or Regress? Self-Improvement Reversal in Post-training](https://openreview.net/forum?id=RFqeoVfLHa) |  | 0 | Self-improvement through post-training methods such as iterative preference learning has been acclaimed for enhancing the problem-solving capabilities (e.g., mathematical reasoning) of Large Language Models (LLMs) without human intervention. However, as our exploration deepens, it is crucial to... | Pengfei Liu, Ting Wu, Xuefeng Li |  |
| 1610 |  |  [Reflexive Guidance: Improving OoDD in Vision-Language Models via Self-Guided Image-Adaptive Concept Generation](https://openreview.net/forum?id=R4h5PXzUuU) |  | 0 | With the recent emergence of foundation models trained on internet-scale data and demonstrating remarkable generalization capabilities, such foundation models have become more widely adopted, leading to an expanding range of application domains. Despite this rapid proliferation, the trustworthiness... | Jihyo Kim, Sangheum Hwang, Seulbi Lee |  |
| 1611 |  |  [MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models](https://openreview.net/forum?id=6guG2OlXsr) |  | 0 | Large Language Models (LLMs) have displayed massive improvements in reason- ing and decision-making skills and can hold natural conversations with users. Recently, many tool-use benchmark datasets have been proposed. However, existing datasets have the following limitations: (1). Insufficient... | Bo Zheng, Chenchen Zhang, Ge Zhang, Hangyu Guo, Jiaheng Liu, Jiakai Wang, Junran Peng, Ken Deng, Noah Wang, Pei Wang, Wenbo Su, Xiaoshuai Song, Yanan Wu, Z. Y. Peng, Zhaoxiang Zhang |  |
| 1612 |  |  [The Crystal Ball Hypothesis in diffusion models: Anticipating object positions from initial noise](https://openreview.net/forum?id=GpdO9r73xT) |  | 0 | Diffusion models have achieved remarkable success in text-to-image generation tasks, yet the influence of initial noise remains largely unexplored. In this study, we identify specific regions within the initial noise image, termed trigger patches, that play a key role in inducing object generation... | Boqing Gong, ChoJui Hsieh, Minhao Cheng, Ruochen Wang, Tianyi Zhou, Yuanhao Ban |  |
| 1613 |  |  [Pacmann: Efficient Private Approximate Nearest Neighbor Search](https://openreview.net/forum?id=yQcFniousM) |  | 0 | We propose a new private Approximate Nearest Neighbor (ANN) search scheme named Pacmann that allows a client to perform ANN search in a vector database without revealing the query vector to the server. Unlike prior constructions that run encrypted search on the server side, Pacmann carefully... | Elaine Shi, Giulia Fanti, Mingxun Zhou |  |
| 1614 |  |  [h4rm3l: A Language for Composable Jailbreak Attack Synthesis](https://openreview.net/forum?id=zZ8fgXHkXi) |  | 0 | Despite their demonstrated valuable capabilities, state-of-the-art (SOTA) widely deployed large language models (LLMs) still have the potential to cause harm to society due to the ineffectiveness of their safety filters, which can be bypassed by prompt transformations called jailbreak attacks.... | Ananjan Nandi, Anna Goldie, Christopher D. Manning, Dan Jurafsky, Davide Ghilardi, Federico Bianchi, Gabriel Poesia, Moussa Koulako Bala Doumbouya |  |
| 1615 |  |  [Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models](https://openreview.net/forum?id=otW0TJOUYF) |  | 0 | Multi-agent reinforcement learning (MARL) methods struggle with the non-stationarity of multi-agent systems and fail to adaptively learn online when tested with novel agents. Here, we leverage large language models (LLMs) to create an autonomous agent that can handle these challenges. Our agent,... | Agam Bhatia, Daniel L. K. Yamins, Logan Matthew Cross, Nick Haber, Violet Xiang |  |
| 1616 |  |  [On Evaluating the Durability of Safeguards for Open-Weight LLMs](https://openreview.net/forum?id=fXJCqdUSVG) |  | 0 | Many stakeholders---from model developers to policymakers---seek to minimize the risks of large language models (LLMs). Key to this goal is whether technical safeguards can impede the misuse of LLMs, even when models are customizable via fine-tuning or when model weights are openly available.... | Boyi Wei, Luxi He, Matthew Jagielski, Milad Nasr, Nicholas Carlini, Peter Henderson, Prateek Mittal, Tinghao Xie, Xiangyu Qi, Yangsibo Huang |  |
| 1617 |  |  [The "Law" of the Unconscious Contrastive Learner: Probabilistic Alignment of Unpaired Modalities](https://openreview.net/forum?id=DsIOUoZkVk) |  | 0 | While internet-scale data often come in pairs (e.g., audio+image, image+text), we often want to perform inferences over modalities unseen together in the training data (e.g., audio+text). Prior work has addressed this issue by learning multiple contrastive embedding spaces between existing modality... | Benjamin Eysenbach, Yongwei Che |  |
| 1618 |  |  [Graph Neural Preconditioners for Iterative Solutions of Sparse Linear Systems](https://openreview.net/forum?id=Tkkrm3pA35) |  | 0 | Preconditioning is at the heart of iterative solutions of large, sparse linear systems of equations in scientific disciplines. Several algebraic approaches, which access no information beyond the matrix itself, are widely studied and used, but ill-conditioned matrices remain very challenging. We... | Jie Chen |  |
| 1619 |  |  [A Robust Method to Discover Causal or Anticausal Relation](https://openreview.net/forum?id=Q0s6kgrUMr) |  | 0 | Understanding whether the data generative process follows causal or anticausal relations is important for many applications. Existing causal discovery methods struggle with high-dimensional perceptual data such as images. Moreover, they require well-labeled data, which may not be feasible due to... | Bo Han, Kun Zhang, Mingming Gong, Tongliang Liu, Yang Zhou, Yu Yao |  |
| 1620 |  |  [An Empirical Analysis of Uncertainty in Large Language Model Evaluations](https://openreview.net/forum?id=J4xLuCt2kg) |  | 0 | As LLM-as-a-Judge emerges as a new paradigm for assessing large language models (LLMs), concerns have been raised regarding the alignment, bias, and stability of LLM evaluators. While substantial work has focused on alignment and bias, little research has concentrated on the stability of LLM... | Linyi Yang, Qingqiu Li, Qiujie Xie, Yue Zhang, Yuejie Zhang, Zhuohao Yu |  |
| 1621 |  |  [On Large Language Model Continual Unlearning](https://openreview.net/forum?id=Essg9kb4yx) |  | 0 | While large language models have demonstrated impressive performance across various domains and tasks, their security issues have become increasingly severe. Machine unlearning has emerged as a representative approach for model safety and security by removing the influence of undesired data on the... | Chenkai Weng, Chongyang Gao, Kaize Ding, Lixu Wang, Qi Zhu, Xiao Wang |  |
| 1622 |  |  [Reward Dimension Reduction for Scalable Multi-Objective Reinforcement Learning](https://openreview.net/forum?id=ssRdQimeUI) |  | 0 | In this paper, we introduce a simple yet effective reward dimension reduction method to tackle the scalability challenges of multi-objective reinforcement learning algorithms. While most existing approaches focus on optimizing two to four objectives, their abilities to scale to environments with... | Giseung Park, Youngchul Sung |  |
| 1623 |  |  [MuseGNN: Forming Scalable, Convergent GNN Layers that Minimize a Sampling-Based Energy](https://openreview.net/forum?id=Gq7RDMeZi4) |  | 0 | Among the many variants of graph neural network (GNN) architectures capable of modeling data with cross-instance relations, an important subclass involves layers designed such that the forward pass iteratively reduces a graph-regularized energy function of interest. In this way, node embeddings... | David Wipf, Haitian Jiang, Minjie Wang, Renjie Liu, Xiao Yan, Yichuan Wang, Zengfeng Huang, Zhenkun Cai |  |
| 1624 |  |  [Once-for-All: Controllable Generative Image Compression with Dynamic Granularity Adaptation](https://openreview.net/forum?id=z0hUsPhwUN) |  | 0 | Although recent generative image compression methods have demonstrated impressive potential in optimizing the rate-distortion-perception trade-off, they still face the critical challenge of flexible rate adaptation to diverse compression necessities and scenarios. To overcome this challenge, this... | Anqi Li, Feng Li, Huihui Bai, Runmin Cong, Yao Zhao, Yuxi Liu |  |
| 1625 |  |  [System 1.x: Learning to Balance Fast and Slow Planning with Language Models](https://openreview.net/forum?id=zd0iX5xBhA) |  | 0 | Language models can be used to solve long-horizon planning problems in two distinct modes. In a fast 'System-1' mode, models directly generate plans without any explicit search or backtracking, and in a slow 'System-2' mode, they plan step-by-step by explicitly searching over possible actions.... | Archiki Prasad, Elias StengelEskin, Justin ChihYao Chen, Mohit Bansal, Peter Hase, Swarnadeep Saha |  |
| 1626 |  |  [Stochastic Semi-Gradient Descent for Learning Mean Field Games with Population-Aware Function Approximation](https://openreview.net/forum?id=tfO07iz0b9) |  | 0 | Mean field games (MFGs) model interactions in large-population multi-agent systems through population distributions. Traditional learning methods for MFGs are based on fixed-point iteration (FPI), where policy updates and induced population distributions are computed separately and sequentially.... | Chenyu Zhang, Xu Chen, Xuan Di |  |
| 1627 |  |  [Visually Consistent Hierarchical Image Classification](https://openreview.net/forum?id=7HEMpBTb3R) |  | 0 | Hierarchical classification predicts labels across multiple levels of a taxonomy, e.g., from coarse-level \textit{Bird} to mid-level \textit{Hummingbird} to fine-level \textit{Green hermit}, allowing flexible recognition under varying visual conditions. It is commonly framed as multiple... | Jonathan Huang, Sara Beery, Seulki Park, Stella X. Yu, Youren Zhang |  |
| 1628 |  |  [Second-Order Fine-Tuning without Pain for LLMs: A Hessian Informed Zeroth-Order Optimizer](https://openreview.net/forum?id=bEqI61iBue) |  | 0 | Fine-tuning large language models (LLMs) is necessary for specific downstream tasks, but classic first-order optimizer entails prohibitive GPU memory because of the back propagation. Recent works such as MeZO have turned to zeroth-order optimizers for fine-tuning, which reduce substantial memory by... | Guang Dai, Haishan Ye, Ivor W. Tsang, Sizhe Dang, Yanjun Zhao, Yi Qian |  |
| 1629 |  |  [Diverse Policies Recovering via Pointwise Mutual Information Weighted Imitation Learning](https://openreview.net/forum?id=6Ai8SuDsh3) |  | 0 | Recovering a spectrum of diverse policies from a set of expert trajectories is an important research topic in imitation learning. After determining a latent style for a trajectory, previous diverse polices recovering methods usually employ a vanilla behavioral cloning learning objective conditioned... | Chao Yu, Hanlin Yang, Hanmin Qin, Hansheng Kong, Haobo Fu, Hongwu Chen, Jian Yao, Jiechao Xiong, Juchao Zhuo, Junliang Xing, Kai Li, Kirk Tang, Qiang Fu, Qing Wang, Weiming Liu, Yang Wei |  |
| 1630 |  |  [An Effective Manifold-based Optimization Method for Distributionally Robust Classification](https://openreview.net/forum?id=nzjSvVZBIp) |  | 0 | How to promote the robustness of existing deep learning models is a challenging problem for many practical classification tasks. Recently, Distributionally Robust Optimization (DRO) methods have shown promising potential to tackle this problem. These methods aim to construct reliable models by... | Hu Ding, Jiawei Huang |  |
| 1631 |  |  [Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment](https://openreview.net/forum?id=h1XoHOd19I) |  | 0 | Adapting large language models (LLMs) to specialized domains typically requires domain-specific corpora for continual pre-training to facilitate knowledge memorization and related instructions for fine-tuning to apply this knowledge. However, this method may lead to inefficient knowledge... | JiRong Wen, Jinhao Jiang, Junyi Li, Tao Zhang, Xin Zhao, Yang Song |  |
| 1632 |  |  [ACE: All-round Creator and Editor Following Instructions via Diffusion Transformer](https://openreview.net/forum?id=Bpn8q40n1n) |  | 0 | Diffusion models have emerged as a powerful generative technology and have been found to be applicable in various scenarios. Most existing foundational diffusion models are primarily designed for text-guided visual generation and do not support multi-modal conditions, which are essential for many... | Chaojie Mao, ChenWei Xie, Jingfeng Zhang, Jingren Zhou, Yu Liu, Yulin Pan, Zeyinzi Jiang, Zhen Han |  |
| 1633 |  |  [Sequential Controlled Langevin Diffusions](https://openreview.net/forum?id=dImD2sgy86) |  | 0 | An effective approach for sampling from unnormalized densities is based on the idea of gradually transporting samples from an easy prior to the complicated target distribution. Two popular methods are (1) Sequential Monte Carlo (SMC), where the transport is performed through successive annealed... | Anima Anandkumar, Denis Blessing, Gerhard Neumann, Julius Berner, Junhua Chen, Lorenz Richter |  |
| 1634 |  |  [HOPE for a Robust Parameterization of Long-memory State Space Models](https://openreview.net/forum?id=RZwtbg3qYD) |  | 0 | State-space models (SSMs) that utilize linear, time-invariant (LTI) systems are known for their effectiveness in learning long sequences. To achieve state-of-the-art performance, an SSM often needs a specifically designed initialization, and the training of state matrices is on a logarithmic scale... | Annan Yu, Michael W. Mahoney, N. Benjamin Erichson |  |
| 1635 |  |  [Enhancing Uncertainty Estimation and Interpretability with Bayesian Non-negative Decision Layer](https://openreview.net/forum?id=xJXq6FkqEw) |  | 0 | Although deep neural networks have demonstrated significant success due to their powerful expressiveness, most models struggle to meet practical requirements for uncertainty estimation. Concurrently, the entangled nature of deep neural net- works leads to a multifaceted problem, where various... | Bo Chen, Mingyuan Zhou, Xinyue Hu, Zhibin Duan |  |
| 1636 |  |  [Decision Information Meets Large Language Models: The Future of Explainable Operations Research](https://openreview.net/forum?id=W2dR6rypBQ) |  | 0 | Operations Research (OR) is vital for decision-making in many industries. While recent OR methods have seen significant improvements in automation and efficiency through integrating Large Language Models (LLMs), they still struggle to produce meaningful explanations. This lack of clarity raises... | Chen Ma, Hailei Gong, Qingcan Kang, Tao Zhong, Wing Yin Yu, Xiaojin Fu, Xiongwei Han, Yansen Zhang |  |
| 1637 |  |  [Skill Expansion and Composition in Parameter Space](https://openreview.net/forum?id=GLWf2fq0bX) |  | 0 | Humans excel at reusing prior knowledge to address new challenges and developing skills while solving problems. This paradigm becomes increasingly popular in the development of autonomous agents, as it develops systems that can self-evolve in response to new challenges like human beings. However,... | Haoyi Niu, Jianxiong Li, Tenglong Liu, Xianyuan Zhan, Xin Xu, Yinan Zheng, Yixing Lan |  |
| 1638 |  |  [OMG: Opacity Matters in Material Modeling with Gaussian Splatting](https://openreview.net/forum?id=oeP6OL7ouB) |  | 0 | Decomposing geometry, materials and lighting from a set of images, namely inverse rendering, has been a long-standing problem in computer vision and graphics. Recent advances in neural rendering enable photo-realistic and plausible inverse rendering results. The emergence of 3D Gaussian Splatting... | Bernhard Kerbl, Katia P. Sycara, Silong Yong, Simon Stepputtis, Venkata Nagarjun Pudureddiyur Manivannan, Yaqi Xie, Zifu Wan |  |
| 1639 |  |  [Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing](https://openreview.net/forum?id=Pnk7vMbznK) |  | 0 | High-quality instruction data is critical for aligning large language models (LLMs). Although some models, such as Llama-3-Instruct, have open weights, their alignment data remain private, which hinders the democratization of AI. High human labor costs and a limited, predefined scope for prompting... | Bill Yuchen Lin, Fengqing Jiang, Luyao Niu, Radha Poovendran, Yejin Choi, Yuntian Deng, Zhangchen Xu |  |
| 1640 |  |  [Expected Sliced Transport Plans](https://openreview.net/forum?id=P7O1Vt1BdU) |  | 0 | The optimal transport (OT) problem has gained significant traction in modern machine learning for its ability to: (1) provide versatile metrics, such as Wasserstein distances and their variants, and (2) determine optimal couplings between probability measures. To reduce the computational complexity... | Akram Aldroubi, Ashkan Shahbazi, Matthew Thorpe, Rocio Diaz Martin, Soheil Kolouri, Xinran Liu, Yikun Bai |  |
| 1641 |  |  [CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding & Reasoning Capabilities of CodeLLMs](https://openreview.net/forum?id=CahIEKCu5Q) |  | 0 | Recent advances in Code Large Language Models (CodeLLMs) have primarily focused on open-ended code generation, often overlooking the crucial aspect of code understanding & reasoning. To bridge this gap, we introduce CodeMMLU, a comprehensive multiple-choice benchmark designed to evaluate the depth... | Dung Manh Nguyen, Nam Le Hai, Nam V. Nguyen, Nghi D. Q. Bui, Quang Pham, Thang Chau Phan, TienThong Doan |  |
| 1642 |  |  [MAGE: Model-Level Graph Neural Networks Explanations via Motif-based Graph Generation](https://openreview.net/forum?id=vue9P1Ypk6) |  | 0 | Graph Neural Networks (GNNs) have shown remarkable success in molecular tasks, yet their interpretability remains challenging. Traditional model-level explanation methods like XGNN and GNNInterpreter often fail to identify valid substructures like rings, leading to questionable interpretability.... | Hongyang Gao, Zhaoning Yu |  |
| 1643 |  |  [Prevalence of Negative Transfer in Continual Reinforcement Learning: Analyses and a Simple Baseline](https://openreview.net/forum?id=KAIqwkB3dT) |  | 0 | We argue that the negative transfer problem occurring when the new task to learn arrives is an important problem that needs not be overlooked when developing effective Continual Reinforcement Learning (CRL) algorithms. Through comprehensive experimental validation, we demonstrate that such issue... | Bosun Hwang, Hongjoon Ahn, Jinu Hyeon, Taesup Moon, Youngmin Oh |  |
| 1644 |  |  [Shedding Light on Time Series Classification using Interpretability Gated Networks](https://openreview.net/forum?id=n34taxF0TC) |  | 0 | In time-series classification, interpretable models can bring additional insights but be outperformed by deep models since human-understandable features have limited expressivity and flexibility. In this work, we present InterpGN, a framework that integrates an interpretable model and a deep neural... | Achille Fokoue, Anak Agung Julius, Debarun Bhattacharjya, Ronny Luss, Tengfei Ma, Yunshi Wen |  |
| 1645 |  |  [Towards Neural Scaling Laws for Time Series Foundation Models](https://openreview.net/forum?id=uCqxDfLYrB) |  | 0 | Scaling laws offer valuable insights into the design of time series foundation models (TSFMs). However, previous research has largely focused on the scaling laws of TSFMs for in-distribution (ID) data, leaving their out-of-distribution (OOD) scaling behavior and the influence of model architectures... | ChaoHan Huck Yang, Ming Jin, Qingren Yao, Renhe Jiang, Shirui Pan, Yuxuan Liang |  |
| 1646 |  |  [Leveraging Variable Sparsity to Refine Pareto Stationarity in Multi-Objective Optimization](https://openreview.net/forum?id=Bl3e8HV9xW) |  | 0 | Gradient-based multi-objective optimization (MOO) is essential in modern machine learning, with applications in e.g., multi-task learning, federated learning, algorithmic fairness and reinforcement learning. In this work, we first reveal some limitations of Pareto stationarity, a widely accepted... | Yaoliang Yu, Zeou Hu |  |
| 1647 |  |  [OptiBench Meets ReSocratic: Measure and Improve LLMs for Optimization Modeling](https://openreview.net/forum?id=fsDZwS49uY) |  | 0 | Large language models (LLMs) have exhibited their problem-solving abilities in mathematical reasoning. Solving realistic optimization (OPT) problems in application scenarios requires advanced and applied mathematics ability. However, current OPT benchmarks that merely solve linear programming are... | Jing Tang, Liang Feng, Linqi Song, Wei Shi, Xiaodan Liang, Xiongwei Han, Yinya Huang, Yiwei Wang, Zhicheng Yang, Zhijiang Guo |  |
| 1648 |  |  [Test-time Adaptation for Regression by Subspace Alignment](https://openreview.net/forum?id=SXtl7NRyE5) |  | 0 | This paper investigates test-time adaptation (TTA) for regression, where a regression model pre-trained in a source domain is adapted to an unknown target distribution with unlabeled target data. Although regression is one of the fundamental tasks in machine learning, most of the existing TTA... | Atsutoshi Kumagai, Kazuki Adachi, Shin'ya Yamaguchi, Tomoki Hamagami |  |
| 1649 |  |  [Prediction Risk and Estimation Risk of the Ridgeless Least Squares Estimator under General Assumptions on Regression Errors](https://openreview.net/forum?id=AsAy7CROLs) |  | 0 | In recent years, there has been a significant growth in research focusing on minimum $\ell_2$ norm (ridgeless) interpolation least squares estimators. However, the majority of these analyses have been limited to an unrealistic regression error structure, assuming independent and identically... | Sokbae Lee, Sungyoon Lee |  |
| 1650 |  |  [Advancing LLM Reasoning Generalists with Preference Trees](https://openreview.net/forum?id=2ea5TNVR0c) |  | 0 | We introduce EURUS, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B, Llama-3-8B, and Mixtral-8x22B, EURUS models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical... | Boji Shan, Bowen Zhou, Ganqu Cui, Hanbin Wang, Hao Peng, Huimin Chen, Jia Deng, Lifan Yuan, Maosong Sun, Ning Ding, Ruobing Xie, Xingyao Wang, Yankai Lin, Zeyuan Liu, Zhenghao Liu, Zhiyuan Liu |  |
| 1651 |  |  [Pedestrian Motion Reconstruction: A Large-scale Benchmark via Mixed Reality Rendering with Multiple Perspectives and Modalities](https://openreview.net/forum?id=YOpa6dTrpt) |  | 0 | Reconstructing pedestrian motion from dynamic sensors, with a focus on pedestrian intention, is crucial for advancing autonomous driving safety. However, this task is challenging due to data limitations arising from technical complexities, safety, and cost concerns. We introduce the Pedestrian... | Jianfu Zhang, Junchi Yan, Li Niu, Liqing Zhang, Pai Peng, Tao He, Wenlong Liao, Xinhao Hu, Yasushi Makihara, Yasushi Yagi, Yichen Wang, Yiyi Zhang |  |
| 1652 |  |  [Generalizing Reasoning Problems to Longer Lengths](https://openreview.net/forum?id=zpENPcQSj1) |  | 0 | Length generalization (LG) is a challenging problem in learning to reason. It refers to the phenomenon that when trained on reasoning problems of smaller lengths/sizes, the model struggles with problems of larger sizes or lengths. Although it has been proven that reasoning can be learned if the... | Bing Liu, Changnan Xiao |  |
| 1653 |  |  [DeeperForward: Enhanced Forward-Forward Training for Deeper and Better Performance](https://openreview.net/forum?id=kOYnXVQCtA) |  | 0 | While backpropagation effectively trains models, it presents challenges related to bio-plausibility, resulting in high memory demands and limited parallelism. Recently, Hinton (2022) proposed the Forward-Forward (FF) algorithm for high-parallel local updates. FF leverages squared sums as the local... | Jiajun Wen, Liang Sun, Linlin Shen, Weicheng Xie, Weizhao He, Yang Zhang |  |
| 1654 |  |  [The Directionality of Optimization Trajectories in Neural Networks](https://openreview.net/forum?id=JY6P45sFDS) |  | 0 | The regularity or implicit bias in neural network optimization has been typically studied via the parameter norms or the landscape curvature, often overlooking the trajectory leading to these parameters. However, properties of the trajectory --- particularly its directionality --- capture critical... | Bernhard Schölkopf, Bobby He, Sidak Pal Singh, Thomas Hofmann |  |
| 1655 |  |  [T-Stitch: Accelerating Sampling in Pre-Trained Diffusion Models with Trajectory Stitching](https://openreview.net/forum?id=2mqb8bPHeb) |  | 0 | Sampling from diffusion probabilistic models (DPMs) is often expensive for high-quality image generation and typically requires many steps with a large model. In this paper, we introduce sampling Trajectory Stitching (T-Stitch), a simple yet efficient technique to improve the sampling efficiency... | Anima Anandkumar, Bohan Zhuang, Chaowei Xiao, DeAn Huang, Jianfei Cai, Weili Nie, Zhiding Yu, Zizheng Pan |  |
| 1656 |  |  [ReNovo: Retrieval-Based \emph{De Novo} Mass Spectrometry Peptide Sequencing](https://openreview.net/forum?id=uQnvYP7yX9) |  | 0 | Proteomics is the large-scale study of proteins. Tandem mass spectrometry, as the only high-throughput technique for protein sequence identification, plays a pivotal role in proteomics research. One of the long-standing challenges in this field is peptide identification, which entails determining... | Bozhen Hu, Cheng Tan, Jingbo Zhou, Jun Xia, Lecheng Zhang, Shaorong Chen, Stan Z. Li, Wenjie Du, Zhangyang Gao |  |
| 1657 |  |  [Hotspot-Driven Peptide Design via Multi-Fragment Autoregressive Extension](https://openreview.net/forum?id=jqmptcSNVG) |  | 0 | Peptides, short chains of amino acids, interact with target proteins, making them a unique class of protein-based therapeutics for treating human diseases. Recently, deep generative models have shown great promise in peptide generation. However, several challenges remain in designing effective... | Chaoran Cheng, Ge Liu, Jiahan Li, Jian Peng, Jianzhu Ma, Jiaqi Guan, Ruihan Guo, Sheng Wang, Shitong Luo, Tong Chen |  |
| 1658 |  |  [Learning How Hard to Think: Input-Adaptive Allocation of LM Computation](https://openreview.net/forum?id=6qUUgw9bAZ) |  | 0 | Computationally intensive decoding procedures---including search, reranking, and self-critique---can improve the quality of language model (LM) outputs in problems spanning code generation, numerical reasoning, and dialog. Existing work typically applies the same decoding procedure for every input... | Andi Peng, Andreea Bobu, Idan Shenfeld, Jacob Andreas, Mehul Damani |  |
| 1659 |  |  [Examining Alignment of Large Language Models through Representative Heuristics: the case of political stereotypes](https://openreview.net/forum?id=7LGmXXZXtP) |  | 0 | Examining the alignment of large language models (LLMs) has become increasingly important, e.g., when LLMs fail to operate as intended. This study examines the alignment of LLMs with human values for the domain of politics. Prior research has shown that LLM-generated outputs can include political... | Haohan Wang, Jana Diesner, Sullam Jeoung, Yubin Ge |  |
| 1660 |  |  [Learning-Augmented Search Data Structures](https://openreview.net/forum?id=N4rYbQowE3) |  | 0 | We study the integration of machine learning advice to improve upon traditional data structure designed for efficient search queries. Although there has been recent effort in improving the performance of binary search trees using machine learning advice, e.g., Lin et. al. (ICML 2022), the resulting... | Brandon G. Nguyen, Chunkai Fu, Jung Hoon Seo, Ryan S. Zesch, Samson Zhou |  |
| 1661 |  |  [Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models](https://openreview.net/forum?id=TWnUgSAWNw) |  | 0 | Recent advancements in multimodal models highlight the value of rewritten captions for improving performance, yet key challenges remain. For example, while synthetic captions often provide superior quality and image-text alignment, it is not clear whether they can fully replace AltTexts: the role... | Bowen Zhang, Chen Chen, Haotian Zhang, HongYou Chen, Juan Lao Tebar, Meng Cao, Peter Grasch, Vasileios Saveris, Wenze Hu, Yinfei Yang, Zhe Gan, Zhengfeng Lai |  |
| 1662 |  |  [Advancing Graph Generation through Beta Diffusion](https://openreview.net/forum?id=x1An5a3U9I) |  | 0 | Diffusion models have excelled in generating natural images and are now being adapted to a variety of data types, including graphs. However, conventional models often rely on Gaussian or categorical diffusion processes, which can struggle to accommodate the mixed discrete and continuous components... | Bo Chen, Mingyuan Zhou, Xinyang Liu, Yilin He |  |
| 1663 |  |  [Accelerated Over-Relaxation Heavy-Ball Method: Achieving Global Accelerated Convergence with Broad Generalization](https://openreview.net/forum?id=SWEqzy7IQB) |  | 0 | The heavy-ball momentum method accelerates gradient descent with a momentum term but lacks accelerated convergence for general smooth strongly convex problems. This work introduces the Accelerated Over-Relaxation Heavy-Ball (AOR-HB) method, the first variant with provable global and accelerated... | Jingrong Wei, Long Chen |  |
| 1664 |  |  [Investigating the Pre-Training Dynamics of In-Context Learning: Task Recognition vs. Task Learning](https://openreview.net/forum?id=htDczodFN5) |  | 0 | The emergence of in-context learning (ICL) is potentially attributed to two major abilities: task recognition (TR) for recognizing the task from demonstrations and utilizing pre-trained priors, and task learning (TL) for learning from demonstrations. However, relationships between the two abilities... | JiRong Wen, Junyi Li, Xiaolei Wang, Xin Zhao, Xinyu Tang |  |
| 1665 |  |  [UniWav: Towards Unified Pre-training for Speech Representation Learning and Generation](https://openreview.net/forum?id=yj9lLwMjnE) |  | 0 | Pre-training and representation learning have been playing an increasingly important role in modern speech processing. Nevertheless, different applications have been relying on different foundation models, since predominant pre-training techniques are either designed for discriminative tasks or... | Alexander H. Liu, Bryan Catanzaro, ChaoHan Huck Yang, James R. Glass, Rafael Valle, Sanggil Lee, YuChiang Frank Wang, Yuan Gong |  |
| 1666 |  |  [Weighted Multi-Prompt Learning with Description-free Large Language Model Distillation](https://openreview.net/forum?id=NDLmZZWATc) |  | 0 | Recent advances in pre-trained Vision Language Models (VLM) have shown promising potential for effectively adapting to downstream tasks through _prompt learning_, without the need for additional annotated paired datasets. To supplement the text information in VLM trained on correlations with vision... | Jung Ho Park, Kyubum Shin, Sua Lee |  |
| 1667 |  |  [Multiple Heads are Better than One: Mixture of Modality Knowledge Experts for Entity Representation Learning](https://openreview.net/forum?id=ue1Tt3h1VC) |  | 0 | Learning high-quality multi-modal entity representations is an important goal of multi-modal knowledge graph (MMKG) representation learning, which can en- hance reasoning tasks within the MMKGs, such as MMKG completion (MMKGC). The main challenge is to collaboratively model the structural... | Binbin Hu, Huajun Chen, Lingbing Guo, Wen Zhang, Yajing Xu, Yichi Zhang, Zhuo Chen, Ziqi Liu |  |
| 1668 |  |  [Contrastive Learning from Synthetic Audio Doppelgängers](https://openreview.net/forum?id=XRtyVELwr6) |  | 0 | Learning robust audio representations currently demands extensive datasets of real-world sound recordings. By applying artificial transformations to these recordings, models can learn to recognize similarities despite subtle variations through techniques like contrastive learning. However, these... | Manuel Cherep, Nikhil Singh |  |
| 1669 |  |  [MetaMetrics: Calibrating Metrics for Generation Tasks Using Human Preferences](https://openreview.net/forum?id=slO3xTt4CG) |  | 0 | Understanding the quality of a performance evaluation metric is crucial for ensuring that model outputs align with human preferences. However, it remains unclear how well each metric captures the diverse aspects of these preferences, as metrics often excel in one particular area but not across all... | David Anugraha, Derry Tanti Wijaya, Garry Kuwanto, Genta Indra Winata, Lucky Susanto |  |
| 1670 |  |  [Confidence Elicitation: A New Attack Vector for Large Language Models](https://openreview.net/forum?id=aTYexOYlLb) |  | 0 | A fundamental issue in deep learning has been adversarial robustness. As these systems have scaled, such issues have persisted. Currently, large language models (LLMs) with billions of parameters suffer from adversarial attacks just like their earlier, smaller counterparts. However, the threat... | Brian Formento, ChuanSheng Foo, SeeKiong Ng |  |
| 1671 |  |  [AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large Language Models](https://openreview.net/forum?id=jTEKTdI3K9) |  | 0 | Following the success of Large Language Models (LLMs), expanding their boundaries to new modalities represents a significant paradigm shift in multimodal understanding. Human perception is inherently multimodal, relying not only on text but also on auditory and visual cues for a complete... | Arda Senocak, Joon Son Chung, JungMok Lee, Kim SungBin, Oh HyunBin, TaeHyun Oh |  |
| 1672 |  |  [Looking Inward: Language Models Can Learn About Themselves by Introspection](https://openreview.net/forum?id=eb5pkwIB5i) |  | 0 | Humans acquire knowledge by observing the external world, but also by introspection. Introspection gives a person privileged access to their current state of mind (e.g. thoughts and feelings) that are not accessible to external observers. Do LLMs have this introspective capability of privileged... | Ethan Perez, Felix Jedidja Binder, Henry Sleight, James Chua, John Hughes, Miles Turpin, Owain Evans, Robert Long, Tomek Korbak |  |
| 1673 |  |  [Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts](https://openreview.net/forum?id=IDJUscOjM3) |  | 0 | We present Self-MoE, an approach that transforms a monolithic LLM into a compositional, modular system of self-specialized experts, named MiXSE (MiXture of Self-specialized Experts). Our approach leverages self-specialization, which constructs expert modules using self-generated synthetic data,... | Alan Ritter, David Daniel Cox, Hongyin Luo, Jacob A. Hansen, James R. Glass, Junmo Kang, Leonid Karlinsky, Rameswar Panda, Rogério Feris, Zhen Wang |  |
| 1674 |  |  [AssembleFlow: Rigid Flow Matching with Inertial Frames for Molecular Assembly](https://openreview.net/forum?id=jckKNzYYA6) |  | 0 | Molecular assembly, where a cluster of rigid molecules aggregated into strongly correlated forms, is fundamental to determining the properties of materials. However, traditional numerical methods for simulating this process are computationally expensive, and existing generative models on material... | Hongyu Guo, Shengchao Liu, Yoshua Bengio |  |
| 1675 |  |  [The Semantic Hub Hypothesis: Language Models Share Semantic Representations Across Languages and Modalities](https://openreview.net/forum?id=FrFQpAgnGE) |  | 0 | Modern language models can process inputs across diverse languages and modalities. We hypothesize that models acquire this capability through learning a _shared representation space_ across heterogeneous data types (e.g., different languages and modalities), which places semantically similar inputs... | Dani Yogatama, Jiasen Lu, Xinyan Velocity Yu, Yoon Kim, Zhaofeng Wu |  |
| 1676 |  |  [PWM: Policy Learning with Multi-Task World Models](https://openreview.net/forum?id=hOELrZfg0J) |  | 0 | Reinforcement Learning (RL) has made significant strides in complex tasks but struggles in multi-task settings with different embodiments. World model methods offer scalability by learning a simulation of the environment but often rely on inefficient gradient-free optimization methods for policy... | Animesh Garg, Ignat Georgiev, Nicklas Hansen, Varun Giridhar |  |
| 1677 |  |  [Improved Diffusion-based Generative Model with Better Adversarial Robustness](https://openreview.net/forum?id=1DVgysiIt7) |  | 0 | Diffusion Probabilistic Models (DPMs) have achieved significant success in generative tasks. However, their training and sampling processes suffer from the issue of distribution mismatch. During the denoising process, the input data distributions differ between the training and inference stages,... | Bing Qin, Ming Liu, Mingyang Yi, Shuchen Xue, Zekun Wang, Zhenguo Li, Zhiming Ma |  |
| 1678 |  |  [Self-Improving Robust Preference Optimization](https://openreview.net/forum?id=ZSdubdbOoi) |  | 0 | Online and offline $\mathtt{RLHF}$ methods, such as $\mathtt{PPO}$ and $\mathtt{DPO}$, have been highly successful in aligning AI with human preferences. Despite their success, however, these methods suffer from fundamental limitations: $\mathbf{(a)}$ Models trained with $\mathtt{RLHF}$ can learn... | Arash Ahmadian, Eugene Choi, Matthieu Geist, Mohammad Gheshlaghi Azar, Olivier Pietquin |  |
| 1679 |  |  [EC-DIT: Scaling Diffusion Transformers with Adaptive Expert-Choice Routing](https://openreview.net/forum?id=PxlfzEePC0) |  | 0 | Diffusion transformers have been widely adopted for text-to-image synthesis. While scaling these models up to billions of parameters shows promise, the effectiveness of scaling beyond current sizes remains underexplored and challenging. By explicitly exploiting the computational heterogeneity of... | Bo Dai, Bowen Zhang, Haoshuo Huang, Haotian Sun, Nan Du, Ruoming Pang, Tao Lei, Yanghao Li |  |
| 1680 |  |  [Fine-Tuning Discrete Diffusion Models via Reward Optimization with Applications to DNA and Protein Design](https://openreview.net/forum?id=G328D1xt4W) |  | 0 | Recent studies have demonstrated the strong empirical performance of diffusion models on discrete sequences (i.e., discrete diffusion models) across domains such as natural language and biological sequence generation. For example, in the protein inverse folding task, where the goal is to generate a... | Amy Wang, Avantika Lal, Aviv Regev, Chenyu Wang, Hanchen Wang, Masatoshi Uehara, Sergey Levine, Tommaso Biancalani, Tommi S. Jaakkola, Yichun He |  |
| 1681 |  |  [BAMDP Shaping: a Unified Framework for Intrinsic Motivation and Reward Shaping](https://openreview.net/forum?id=tijmpS9Vy2) |  | 0 | Intrinsic motivation and reward shaping guide reinforcement learning (RL) agents by adding pseudo-rewards, which can lead to useful emergent behaviors. However, they can also encourage counterproductive exploits, e.g., fixation with noisy TV screens. Here we provide a theoretical model which... | Aly Lidayan, Michael D. Dennis, Stuart Russell |  |
| 1682 |  |  [Kernel-based Optimally Weighted Conformal Time-Series Prediction](https://openreview.net/forum?id=oP7arLOWix) |  | 0 | Conformal prediction has been a popular distribution-free framework for uncertainty quantification. In this work, we present a novel conformal prediction method for time-series, which we call Kernel-based Optimally Weighted Conformal Prediction Intervals ($\texttt{KOWCPI}$). Specifically,... | Chen Xu, Jonghyeok Lee, Yao Xie |  |
| 1683 |  |  [Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed GFlowNets](https://openreview.net/forum?id=Aye5wL6TCn) |  | 0 | While one commonly trains large diffusion models by collecting datasets on target downstream tasks, it is often desired to align and finetune pretrained diffusion models with some reward functions that are either designed by experts or learned from small-scale datasets. Existing post-training... | Dinghuai Zhang, Tim Z. Xiao, Weiyang Liu, Yoshua Bengio, Zhen Liu |  |
| 1684 |  |  [Block Verification Accelerates Speculative Decoding](https://openreview.net/forum?id=frsg32u0rO) |  | 0 | Speculative decoding is an effective method for lossless acceleration of large language models during inference. It uses a fast model to draft a block of tokens which are then verified in parallel by the target model, and provides a guarantee that the output is distributed identically to a sample... | Ahmad Beirami, Ananda Theertha Suresh, Asaf Aharoni, Jae Hun Ro, Uri Mendlovic, Yaniv Leviathan, Ziteng Sun |  |
| 1685 |  |  [Revisiting Source-Free Domain Adaptation: a New Perspective via Uncertainty Control](https://openreview.net/forum?id=nx9Z5Kva96) |  | 0 | Source-Free Domain Adaptation (SFDA) seeks to adapt a pre-trained source model to the target domain using only unlabeled target data, without access to the original source data. While current state-of-the-art (SOTA) methods rely on leveraging weak supervision from the source model to extract... | Boyu Wang, Charles Ling, Gezheng Xu, Grace Yi, Hui Guo, Li Yi |  |
| 1686 |  |  [Generalizable Motion Planning via Operator Learning](https://openreview.net/forum?id=UYcUpiULmT) |  | 0 | In this work, we introduce a planning neural operator (PNO) for predicting the value function of a motion planning problem. We recast value function approximation as learning a single operator from the cost function space to the value function space, which is defined by an Eikonal partial... | Luke Bhan, Nikolay Atanasov, Sharath Matada, Yuanyuan Shi |  |
| 1687 |  |  [Modeling Unseen Environments with Language-guided Composable Causal Components in Reinforcement Learning](https://openreview.net/forum?id=XMgpnZ2ET7) |  | 0 | Generalization in reinforcement learning (RL) remains a significant challenge, especially when agents encounter novel environments with unseen dynamics. Drawing inspiration from human compositional reasoning—where known components are reconfigured to handle new situations—we introduce World... | Biwei Huang, Xinyue Wang |  |
| 1688 |  |  [Doubly robust identification of treatment effects from multiple environments](https://openreview.net/forum?id=9vTAkJ9Tik) |  | 0 | Practical and ethical constraints often require the use of observational data for causal inference, particularly in medicine and social sciences. Yet, observational datasets are prone to confounding, potentially compromising the validity of causal conclusions. While it is possible to correct for... | Fanny Yang, Javier Abad, Julia Kostin, Piersilvio De Bartolomeis, Yixin Wang |  |
| 1689 |  |  [Variational Search Distributions](https://openreview.net/forum?id=1vrpdV9U3i) |  | 0 | We develop VSD, a method for conditioning a generative model of discrete, combinatorial designs on a rare desired class by efficiently evaluating a black-box (e.g. experiment, simulation) in a batch sequential manner. We call this task active generation; we formalize active generation's... | Cheng Soon Ong, Daniel M. Steinberg, Edwin V. Bonilla, Rafael Oliveira |  |
| 1690 |  |  [Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language Models](https://openreview.net/forum?id=77gQUdQhE7) |  | 0 | Recent studies indicate that effectively utilizing inference-time compute is crucial for attaining good performance from large language models (LLMs). Specifically, the Best-of-N (BoN) inference strategy, where an LLM generates multiple responses and a verifier selects the best, has shown strong... | Aleksandra Faust, Aviral Kumar, Bo Dai, Craig Boutilier, Guy Tennenholtz, Izzeddin Gur, Rishabh Agarwal, Sridhar Thiagarajan, Vincent Zhuang, Yinlam Chow |  |
| 1691 |  |  [Neural Spacetimes for DAG Representation Learning](https://openreview.net/forum?id=skGSOcrIj7) |  | 0 | We propose a class of trainable deep learning-based geometries called Neural SpaceTimes (NSTs), which can universally represent nodes in weighted Directed Acyclic Graphs (DAGs) as events in a spacetime manifold. While most works in the literature focus on undirected graph representation learning or... | Anastasis Kratsios, Haitz Sáez de Ocáriz Borde, Marc T. Law, Michael M. Bronstein, Xiaowen Dong |  |
| 1692 |  |  [Data-adaptive Differentially Private Prompt Synthesis for In-Context Learning](https://openreview.net/forum?id=sVNfWhtaJC) |  | 0 | Large Language Models (LLMs) rely on the contextual information embedded in examples/demonstrations to perform in-context learning (ICL). To mitigate the risk of LLMs potentially leaking private information contained in examples in the prompt, we introduce a novel data-adaptive differentially... | Cong Shen, Fengyu Gao, Jing Yang, Ruida Zhou, Tianhao Wang |  |
| 1693 |  |  [Towards Bridging Generalization and Expressivity of Graph Neural Networks](https://openreview.net/forum?id=BOQpRtI4F5) |  | 0 | Expressivity and generalization are two critical aspects of graph neural networks (GNNs). While significant progress has been made in studying the expressivity of GNNs, much less is known about their generalization capabilities, particularly when dealing with the inherent complexity of... | Dongwoo Kim, Floris Geerts, Qing Wang, Shouheng Li |  |
| 1694 |  |  [Intermediate Layer Classifiers for OOD generalization](https://openreview.net/forum?id=ByCV9xWfNK) |  | 0 | Deep classifiers are known to be sensitive to data distribution shifts, primarily due to their reliance on spurious correlations in training data. It has been suggested that these classifiers can still find useful features in the network's last layer that hold up under such shifts. In this work, we... | Arnas Uselis, Seong Joon Oh |  |
| 1695 |  |  [Lightweight Predictive 3D Gaussian Splats](https://openreview.net/forum?id=PbheqxnO1e) |  | 0 | Recent approaches representing 3D objects and scenes using Gaussian splats show increased rendering speed across a variety of platforms and devices. While rendering such representations is indeed extremely efficient, storing and transmitting them is often prohibitively expensive. To represent... | Anil Kag, Chaoyang Wang, Chenfanfu Jiang, Jian Ren, Ju Hu, Junli Cao, Sergei Korolev, Sergey Tulyakov, Vidit Goel |  |
| 1696 |  |  [IntersectionZoo: Eco-driving for Benchmarking Multi-Agent Contextual Reinforcement Learning](https://openreview.net/forum?id=XoulHHQGFi) |  | 0 | Despite the popularity of multi-agent reinforcement learning (RL) in simulated and two-player applications, its success in messy real-world applications has been limited. A key challenge lies in its generalizability across problem variations, a common necessity for many real-world problems.... | Ao Qu, Baptiste Freydt, Cameron Hickert, Cathy Wu, Vindula Jayawardana, Zhongxia Yan |  |
| 1697 |  |  [SPDIM: Source-Free Unsupervised Conditional and Label Shift Adaptation in EEG](https://openreview.net/forum?id=CoQw1dXtGb) |  | 0 | The non-stationary nature of electroencephalography (EEG) introduces distribution shifts across domains (e.g., days and subjects), posing a significant challenge to EEG-based neurotechnology generalization. Without labeled calibration data for target domains, the problem is a source-free... | Motoaki Kawanabe, Reinmar J. Kobler, Shanglin Li |  |
| 1698 |  |  [CoMRes: Semi-Supervised Time Series Forecasting Utilizing Consensus Promotion of Multi-Resolution](https://openreview.net/forum?id=bRa4JLPzii) |  | 0 | Long-term time series forecasting poses significant challenges due to the complex dynamics and temporal variations, particularly when dealing with unseen patterns and data scarcity. Traditional supervised learning approaches, which rely on cleaned and labeled data, struggle to capture these unseen... | JayYoon Lee, Yunju Cho |  |
| 1699 |  |  [Transformer Block Coupling and its Correlation with Generalization in LLMs](https://openreview.net/forum?id=kvLenbZZgg) |  | 0 | Large Language Models (LLMs) have made significant strides in natural language processing, and a precise understanding of the internal mechanisms driving their success is essential. In this work, we analyze the trajectories of token embeddings as they pass through transformer blocks, linearizing... | Anton Sugolov, Haoming Meng, Murdock Aubry, Vardan Papyan |  |
| 1700 |  |  [Reconciling Model Multiplicity for Downstream Decision Making](https://openreview.net/forum?id=uy4EavBEwl) |  | 0 | We consider the problem of model multiplicity in downstream decision-making, a setting where two predictive models of equivalent accuracy cannot agree on what action to take for a downstream decision-making problem. Prior work attempts to address model multiplicity by resolving prediction... | Ally Yalei Du, Dung Daniel T. Ngo, Zhiwei Steven Wu |  |
| 1701 |  |  [Robust System Identification: Finite-sample Guarantees and Connection to Regularization](https://openreview.net/forum?id=ZNnmcddaB3) |  | 0 | We consider the problem of learning nonlinear dynamical systems from a single sample trajectory. While the least squares estimate (LSE) is commonly used for this task, it suffers from poor identification errors when the sample size is small or the model fails to capture the system's true dynamics.... | Grani A. Hanasusanto, Hyuk Park, Yingying Li |  |
| 1702 |  |  [eQMARL: Entangled Quantum Multi-Agent Reinforcement Learning for Distributed Cooperation over Quantum Channels](https://openreview.net/forum?id=cR5GTis5II) |  | 0 | Collaboration is a key challenge in distributed multi-agent reinforcement learning (MARL) environments. Learning frameworks for these decentralized systems must weigh the benefits of explicit player coordination against the communication overhead and computational cost of sharing local observations... | Alexander C. DeRieux, Walid Saad |  |
| 1703 |  |  [E(n) Equivariant Topological Neural Networks](https://openreview.net/forum?id=Ax3uliEBVR) |  | 0 | Graph neural networks excel at modeling pairwise interactions, but they cannot flexibly accommodate higher-order interactions and features. Topological deep learning (TDL) has emerged recently as a promising tool for addressing this issue. TDL enables the principled modeling of arbitrary multi-way,... | Claudio Battiloro, Ege Karaismailoglu, Francesca Dominici, George Dasoulas, Mauricio Tec, Michelle Audirac |  |
| 1704 |  |  [Tamper-Resistant Safeguards for Open-Weight LLMs](https://openreview.net/forum?id=4FIjRodbW6) |  | 0 | Rapid advances in the capabilities of large language models (LLMs) have raised widespread concerns regarding their potential for malicious use. Open-weight LLMs present unique challenges, as existing safeguards lack robustness to tampering attacks that modify model weights. For example, recent... | Alice Gatti, Andy Zhou, Andy Zou, Bhrugu Bharathi, Bo Li, Dan Hendrycks, Dawn Song, Justin Wang, Long Phan, Mantas Mazeika, Maxwell Lin, Rishub Tamirisa, Ron Arel, Rowan Wang, Tarun Suresh |  |
| 1705 |  |  [Beyond Worst-Case Dimensionality Reduction for Sparse Vectors](https://openreview.net/forum?id=e8qXTxMgPg) |  | 0 | We study beyond worst-case dimensionality reduction for $s$-sparse vectors (vectors with at most $s$ non-zero coordinates). Our work is divided into two parts, each focusing on a different facet of beyond worst-case analysis: \noindent (a) We first consider average-case guarantees for embedding... | David P. Woodruff, Qiuyi Zhang, Sandeep Silwal |  |
| 1706 |  |  [Steering Large Language Models between Code Execution and Textual Reasoning](https://openreview.net/forum?id=5X5Z7Ffrjb) |  | 0 | While a lot of recent research focuses on enhancing the textual reasoning capabilities of Large Language Models (LLMs) by optimizing the multi-agent framework or reasoning chains, several benchmark tasks can be solved with 100\% success through direct coding, which is more scalable and avoids the... | Chi Wang, Chuchu Fan, Harsh Jhamtani, Srinagesh Sharma, Yongchao Chen |  |
| 1707 |  |  [Misspecified Q-Learning with Sparse Linear Function Approximation: Tight Bounds on Approximation Error](https://openreview.net/forum?id=nIEjY4a2Lf) |  | 0 | The recent work by Dong and Yang (2023) showed for misspecified sparse linear bandits, one can obtain an $O(\epsilon)$-optimal policy using a polynomial number of samples when the sparsity is a constant, where $\epsilon$ is the misspecification error. This result is in sharp contrast to... | Ally Yalei Du, Lin Yang, Ruosong Wang |  |
| 1708 |  |  [RankSHAP: Shapley Value Based Feature Attributions for Learning to Rank](https://openreview.net/forum?id=4011PUI9vm) |  | 0 | Numerous works propose post-hoc, model-agnostic explanations for learning to rank, focusing on ordering entities by their relevance to a query through feature attribution methods. However, these attributions often weakly correlate or contradict each other, confusing end users. We adopt an axiomatic... | James Allan, Tanya Chowdhury, Yair Zick |  |
| 1709 |  |  [Machine Unlearning Fails to Remove Data Poisoning Attacks](https://openreview.net/forum?id=HaX48yksVL) |  | 0 | We revisit the efficacy of several practical methods for approximate machine unlearning developed for large-scale deep learning. In addition to complying with data deletion requests, one often-cited potential application for unlearning methods is to remove the effects of poisoned data. We... | Ayush Sekhari, Gautam Kamath, Jimmy Z. Di, Martin Pawelczyk, Seth Neel, Yiwei Lu |  |
| 1710 |  |  [Residual-MPPI: Online Policy Customization for Continuous Control](https://openreview.net/forum?id=gVnJFY8nCM) |  | 0 | Policies developed through Reinforcement Learning (RL) and Imitation Learning (IL) have shown great potential in continuous control tasks, but real-world applications often require adapting trained policies to unforeseen requirements. While fine-tuning can address such needs, it typically requires... | Catherine Weaver, Chen Tang, Chenran Li, Kenta Kawamoto, Masayoshi Tomizuka, Pengcheng Wang, Wei Zhan |  |
| 1711 |  |  [MCNC: Manifold-Constrained Reparameterization for Neural Compression](https://openreview.net/forum?id=VMV8gefvq8) |  | 0 | The outstanding performance of large foundational models across diverse tasks, from computer vision to speech and natural language processing, has significantly increased their demand. However, storing and transmitting these models poses significant challenges due to their massive size (e.g., 750GB... | Ali Abbasi, Chayne Thrash, Hamed Pirsiavash, Parsa Nooralinejad, Reed Andreas, Soheil Kolouri, Soroush Abbasi Koohpayegani |  |
| 1712 |  |  [HQ-Edit: A High-Quality Dataset for Instruction-based Image Editing](https://openreview.net/forum?id=mZptYYttFj) |  | 0 | This study introduces HQ-Edit, a high-quality instruction-based image editing dataset with around 200,000 edits. Unlike prior approaches relying on attribute guidance or human feedback on building datasets, we devise a scalable data collection pipeline leveraging advanced foundation models, namely... | Bingchen Zhao, Cihang Xie, Heng Wang, Mude Hui, Peng Wang, Siwei Yang, Yichun Shi, Yuyin Zhou |  |
| 1713 |  |  [Last-Iterate Convergence Properties of Regret-Matching Algorithms in Games](https://openreview.net/forum?id=LWeVVPuIx0) |  | 0 | We study last-iterate convergence properties of algorithms for solving two-player zero-sum games based on Regret Matching$^+$ (RM$^+$). Despite their widespread use for solving real games, virtually nothing is known about their last-iterate convergence. A major obstacle to analyzing RM-type... | Christian Kroer, ChungWei Lee, Gabriele Farina, Haipeng Luo, Julien GrandClément, Weiqiang Zheng, Yang Cai |  |
| 1714 |  |  [HARDMath: A Benchmark Dataset for Challenging Problems in Applied Mathematics](https://openreview.net/forum?id=nDTvP6tBMd) |  | 0 | Advanced applied mathematics problems are underrepresented in existing Large Language Model (LLM) benchmark datasets. To address this, we introduce $\textbf{HARDMath}$, a dataset inspired by a graduate course on asymptotic methods, featuring challenging applied mathematics problems that require... | Corey Wang, Danxian Liu, Erik Y. Wang, Jingxuan Fan, Jonah Brenner, Kaylie Hausknecht, Michael P. Brenner, Nianli Peng, Sarah Martinson |  |
| 1715 |  |  [Provable weak-to-strong generalization via benign overfitting](https://openreview.net/forum?id=4vzGQcVUG8) |  | 0 | The classic teacher-student model in machine learning posits that a strong teacher supervises a weak student to improve the student's capabilities. We instead consider the inverted situation, where a weak teacher supervises a strong student with imperfect pseudolabels. This paradigm was recently... | Anant Sahai, David Xing Wu |  |
| 1716 |  |  [Toward Efficient Multi-Agent Exploration With Trajectory Entropy Maximization](https://openreview.net/forum?id=YvKJGYL4j7) |  | 0 | Recent works have increasingly focused on learning decentralized policies for agents as a solution to the scalability challenges in Multi-Agent Reinforcement Learning (MARL), where agents typically share the parameters of a policy network to make action decisions. However, this parameter sharing... | Kun Zhu, Tianxu Li |  |
| 1717 |  |  [Law of the Weakest Link: Cross Capabilities of Large Language Models](https://openreview.net/forum?id=TljGdvzFq2) |  | 0 | The development and evaluation of Large Language Models (LLMs) have largely focused on individual capabilities. However, this overlooks the intersection of multiple abilities across different types of expertise that are often required for real-world tasks, which we term \*\*cross capabilities\*\*.... | Aston Zhang, Chenguang Zhu, Chloe Bi, Dhruv Mahajan, Jiawei Han, Laurens van der Maaten, Liang Tan, Melanie Kambadur, Mike Lewis, Ming Zhong, Rui Hou, Sergey Edunov, Sharan Narang, Sravya Popuri, Wenhan Xiong, Xuewei Wang, Zhengxing Chen |  |
| 1718 |  |  [BenTo: Benchmark Reduction with In-Context Transferability](https://openreview.net/forum?id=ki7b0qD11r) |  | 0 | Evaluating large language models (LLMs) is costly: it requires the generation and examination of LLM outputs on a large-scale benchmark of various tasks. This paper investigates how to efficiently reduce the tasks used to benchmark LLMs without affecting the evaluation quality. Our study reveals... | Hongyu Zhao, Lichao Sun, Ming Li, Tianyi Zhou |  |
| 1719 |  |  [Round and Round We Go! What makes Rotary Positional Encodings useful?](https://openreview.net/forum?id=GtvuNrk58a) |  | 0 | Positional Encodings (PEs) are a critical component of Transformer-based Large Language Models (LLMs), providing the attention mechanism with important sequence-position information. One of the most popular types of encoding used today in LLMs are Rotary Positional Encodings (RoPE), that rotate the... | Alex Vitvitskyi, Christos Perivolaropoulos, Federico Barbero, Petar Velickovic, Razvan Pascanu |  |
| 1720 |  |  [On the self-verification limitations of large language models on reasoning and planning tasks](https://openreview.net/forum?id=4O0v4s3IzY) |  | 0 | There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples--ranging from multiplication to simple planning--there... | Karthik Valmeekam, Kaya Stechly, Subbarao Kambhampati |  |
| 1721 |  |  [Broadening Target Distributions for Accelerated Diffusion Models via a Novel Analysis Approach](https://openreview.net/forum?id=reZKq6hjOZ) |  | 0 | Accelerated diffusion models hold the potential to significantly enhance the efficiency of standard diffusion processes. Theoretically, these models have been shown to achieve faster convergence rates than the standard $\mathcal O(1/\epsilon^2)$ rate of vanilla diffusion models, where $\epsilon$... | Ness B. Shroff, Peizhong Ju, Yingbin Liang, Yuchen Liang |  |
| 1722 |  |  [Single-agent Poisoning Attacks Suffice to Ruin Multi-Agent Learning](https://openreview.net/forum?id=46xYl55hdc) |  | 0 | We investigate the robustness of multi-agent learning in strongly monotone games with bandit feedback. While previous research has developed learning algorithms that achieve last-iterate convergence to the unique Nash equilibrium (NE) at a polynomial rate, we demonstrate that all such algorithms... | Ermin Wei, Fan Yao, Haifeng Xu, Yuwei Cheng |  |
| 1723 |  |  [Improving Pretraining Data Using Perplexity Correlations](https://openreview.net/forum?id=huuKoVQnB0) |  | 0 | Quality pretraining data is often seen as the key to high-performance language models. However, progress in understanding pretraining data has been slow due to the costly pretraining runs required for data selection experiments. We present a framework that avoids these costs and selects... | Christopher Potts, Tatsunori Hashimoto, Tristan Thrush |  |
| 1724 |  |  [Self-Play Preference Optimization for Language Model Alignment](https://openreview.net/forum?id=a3PmRgAB5T) |  | 0 | Standard reinforcement learning from human feedback (RLHF) approaches relying on parametric models like the Bradley-Terry model fall short in capturing the intransitivity and irrationality in human preferences. Recent advancements suggest that directly working with preference probabilities can... | Huizhuo Yuan, Kaixuan Ji, Quanquan Gu, Yiming Yang, Yue Wu, Zhiqing Sun |  |
| 1725 |  |  [Protein Language Model Fitness is a Matter of Preference](https://openreview.net/forum?id=UvPdpa4LuV) |  | 0 | Leveraging billions of years of evolution, scientists have trained protein language models (pLMs) to understand the sequence and structure space of proteins aiding in the design of more functional proteins. Although they have shown ability to improve efficiency in engineering, it remains unclear... | Amy X. Lu, Cade W. Gordon, Pieter Abbeel |  |
| 1726 |  |  [LoLCATs: On Low-Rank Linearizing of Large Language Models](https://openreview.net/forum?id=8VtGeyJyx9) |  | 0 | Recent works show we can linearize large language models (LLMs)—swapping the quadratic attentions of popular Transformer-based LLMs with subquadratic analogs, such as linear attention—avoiding the expensive pretraining costs. However, linearizing LLMs often significantly degrades model quality,... | Aaryan Singhal, Alan Wu, Benjamin Frederick Spector, Christopher Ré, Krithik Ramesh, Michael Zhang, Rahul Chalamala, Simran Arora |  |
| 1727 |  |  [Flow Matching with Gaussian Process Priors for Probabilistic Time Series Forecasting](https://openreview.net/forum?id=uxVBbSlKQ4) |  | 0 | Recent advancements in generative modeling, particularly diffusion models, have opened new directions for time series modeling, achieving state-of-the-art performance in forecasting and synthesis. However, the reliance of diffusion-based models on a simple, fixed prior complicates the generative... | David Lüdke, Leo Schwinn, Marcel Kollovieh, Marten Lienen, Stephan Günnemann |  |
| 1728 |  |  [Robust Feature Learning for Multi-Index Models in High Dimensions](https://openreview.net/forum?id=aKkDY1Wca0) |  | 0 | Recently, there have been numerous studies on feature learning with neural networks, specifically on learning single- and multi-index models where the target is a function of a low-dimensional projection of the input. Prior works have shown that in high dimensions, the majority of the compute and... | Adel Javanmard, Alireza Mousavi Hosseini, Murat A. Erdogdu |  |
| 1729 |  |  [AdaIR: Adaptive All-in-One Image Restoration via Frequency Mining and Modulation](https://openreview.net/forum?id=M5t0WvjfCg) |  | 0 | In the image acquisition process, various forms of degradation, including noise, blur, haze, and rain, are frequently introduced. These degradations typically arise from the inherent limitations of cameras or unfavorable ambient conditions. To recover clean images from their degraded versions,... | Alois Knoll, Fahad Shahbaz Khan, Mubarak Shah, Salman H. Khan, Syed Waqas Zamir, Yuning Cui |  |
| 1730 |  |  [Discretization-invariance? On the Discretization Mismatch Errors in Neural Operators](https://openreview.net/forum?id=J9FgrqOOni) |  | 0 | In recent years, neural operators have emerged as a prominent approach for learning mappings between function spaces, such as the solution operators of parametric PDEs. A notable example is the Fourier Neural Operator (FNO), which models the integral kernel as a convolution operator and uses the... | Ruichen Xu, Wenhan Gao, Yi Liu, Yuefan Deng |  |
| 1731 |  |  [(Mis)Fitting Scaling Laws: A Survey of Scaling Law Fitting Techniques in Deep Learning](https://openreview.net/forum?id=xI71dsS3o4) |  | 0 | Modern foundation models rely heavily on using scaling laws to guide crucial training decisions. Researchers often extrapolate the optimal architecture and hyper parameters settings from smaller training runs by describing the relationship between, loss, or task performance, and scale. All... | Luke Zettlemoyer, Margaret Li, Sneha Kudugunta |  |
| 1732 |  |  [Semantic Aware Representation Learning for Lifelong Learning](https://openreview.net/forum?id=WwwJfkGq0G) |  | 0 | The human brain excels at lifelong learning by not only encoding information in sparse activation codes but also leveraging rich semantic structures and relationships between newly encountered and previously learned objects. This ability to utilize semantic similarities is crucial for efficient... | Bahram Zonooz, Elahe Arani, Fahad Sarfraz |  |
| 1733 |  |  [Singular Subspace Perturbation Bounds via Rectangular Random Matrix Diffusions](https://openreview.net/forum?id=G8U2nGP3Vi) |  | 0 | Given a matrix $A \in \mathbb{R}^{m\times d}$ with singular values $\sigma_1\geq \cdots \geq \sigma_d$, and a random matrix $G \in \mathbb{R}^{m\times d}$ with iid $N(0,T)$ entries for some $T>0$, we derive new bounds on the Frobenius distance between subspaces spanned by the top-$k$ (right)... | Oren Mangoubi, Peiyao Lai |  |
| 1734 |  |  [LeanAgent: Lifelong Learning for Formal Theorem Proving](https://openreview.net/forum?id=Uo4EHT4ZZ8) |  | 0 | Large Language Models (LLMs) have been successful in mathematical reasoning tasks such as formal theorem proving when integrated with interactive proof assistants like Lean. Existing approaches involve training or fine-tuning an LLM on a specific dataset to perform well on particular domains, such... | Adarsh Kumarappan, Anima Anandkumar, Chaowei Xiao, Mo Tiwari, Peiyang Song, Robert Joseph George |  |
| 1735 |  |  [Multilevel Generative Samplers for Investigating Critical Phenomena](https://openreview.net/forum?id=YcUV5apdlq) |  | 0 | Investigating critical phenomena or phase transitions is of high interest in physics and chemistry, for which Monte Carlo (MC) simulations, a crucial tool for numerically analyzing macroscopic properties of given systems, are often hindered by an emerging divergence of correlation length---known as... | Ankur Singha, Elia Cellini, Karl Jansen, Kim Andrea Nicoli, Shinichi Nakajima, Stefan Kühn |  |
| 1736 |  |  [Certifying Counterfactual Bias in LLMs](https://openreview.net/forum?id=HQHnhVQznF) |  | 0 | Large Language Models (LLMs) can produce biased responses that can cause representational harms. However, conventional studies are insufficient to thoroughly evaluate biases across LLM responses for different demographic groups (a.k.a. counterfactual bias), as they do not scale to large number of... | Gagandeep Singh, Isha Chaudhary, Manoj Kumar, Morteza Ziyadi, Qian Hu, Rahul Gupta |  |
| 1737 |  |  [Not All Heads Matter: A Head-Level KV Cache Compression Method with Integrated Retrieval and Reasoning](https://openreview.net/forum?id=FJFVmeXusW) |  | 0 | Key-Value (KV) caching is a common technique to enhance the computational efficiency of Large Language Models (LLMs), but its memory overhead grows rapidly with input length. Prior work has shown that not all tokens are equally important for text generation, proposing layer-level KV cache... | Abedelkadir Asi, Wayne Xiong, Wen Xiao, Yu Fu, Yue Dong, Zefan Cai |  |
| 1738 |  |  [Preble: Efficient Distributed Prompt Scheduling for LLM Serving](https://openreview.net/forum?id=meKEKDhdnx) |  | 0 | Prompts to large language models (LLMs) have evolved beyond simple user questions. For LLMs to solve complex problems, today’s practices are to include domain-specific instructions, illustration of tool usages, and/or long context such as textbook chapters in prompts. As such, many parts of prompts... | Dongming Li, Reyna Abhyankar, Vikranth Srivatsa, Yiying Zhang, Zijian He |  |
| 1739 |  |  [Sufficient Context: A New Lens on Retrieval Augmented Generation Systems](https://openreview.net/forum?id=Jjr2Odj8DJ) |  | 0 | Augmenting LLMs with context leads to improved performance across many applications. Despite much research on Retrieval Augmented Generation (RAG) systems, an open question is whether errors arise because LLMs fail to utilize the context from retrieval or the context itself is insufficient to... | Ankur Taly, ChunSung Ferng, Cyrus Rashtchian, DaCheng Juan, Hailey Joren, Jianyi Zhang |  |
| 1740 |  |  [Exploratory Preference Optimization: Harnessing Implicit Q\*-Approximation for Sample-Efficient RLHF](https://openreview.net/forum?id=QYigQ6gXNw) |  | 0 | This paper investigates a basic question in reinforcement learning from human feedback (RLHF) from a theoretical perspective: how to efficiently explore in an online manner under preference feedback and general function approximation. We take the initial step towards a theoretical understanding of... | Ahmed Hassan Awadallah, Akshay Krishnamurthy, Alexander Rakhlin, Corby Rosset, Dylan J. Foster, Tengyang Xie |  |
| 1741 |  |  [MoLEx: Mixture of Layer Experts for Fine-tuning with Sparse Upcycling](https://openreview.net/forum?id=rWui9vLhOc) |  | 0 | Large-scale pre-training of deep models, followed by fine-tuning them to adapt to downstream tasks, has become the cornerstone of natural language processing (NLP). The prevalence of vast corpses of data coupled with computational resources has led to large models with a considerable number of... | Rachel S. Y. Teo, Tan Minh Nguyen |  |
| 1742 |  |  [Faster Diffusion Sampling with Randomized Midpoints: Sequential and Parallel](https://openreview.net/forum?id=MT3aOfXIbY) |  | 0 | Sampling algorithms play an important role in controlling the quality and runtime of diffusion model inference. In recent years, a number of works (Chen et al., 2023c;b; Benton et al., 2023; Lee et al., 2022) have analyzed algorithms for diffusion sampling with provable guarantees; these works show... | Linda Cai, Shivam Gupta, Sitan Chen |  |
| 1743 |  |  [nGPT: Normalized Transformer with Representation Learning on the Hypersphere](https://openreview.net/forum?id=se4vjm7h4E) |  | 0 | We propose a novel neural network architecture, the normalized Transformer (nGPT) with representation learning on the hypersphere. In nGPT, all vectors forming the embeddings, MLP, attention matrices and hidden states are unit norm normalized. The input stream of tokens travels on the surface of a... | Boris Ginsburg, ChengPing Hsieh, Ilya Loshchilov, Simeng Sun |  |
| 1744 |  |  [Pushing the Limits of All-Atom Geometric Graph Neural Networks: Pre-Training, Scaling, and Zero-Shot Transfer](https://openreview.net/forum?id=4S2L519nIX) |  | 0 | The ability to construct transferable descriptors for molecular and biological systems has broad applications in drug discovery, molecular dynamics, and protein analysis. Geometric graph neural networks (Geom-GNNs) utilizing all-atom information have revolutionized atomistic simulations by enabling... | Huzefa Rangwala, Marcus D. Collins, Zhengyuan Shen, Zichen Wang, Zihan Pengmei |  |
| 1745 |  |  [MuPT: A Generative Symbolic Music Pretrained Transformer](https://openreview.net/forum?id=iAK9oHp4Zz) |  | 0 | In this paper, we explore the application of Large Language Models (LLMs) to the pre-training of music. While the prevalent use of MIDI in music modeling is well-established, our findings suggest that LLMs are inherently more compatible with ABC Notation, which aligns more closely with their design... | Fengze Han, Jiaheng Liu, Junting Zhou, Ka Man Lo, Lejun Min, Ruibin Yuan, Shangda Wu, Shuyue Guo, Tianyu Zhang, Tianyu Zheng, Wei Xue, Xeron Du, Xingwei Qu, Xueling Liu, Yiming Liang, Yinghao Ma, Yizhi Li, Yuelin Bai, Ziya Zhou, Ziyang Ma, et al. |  |
| 1746 |  |  [Improving Instruction-Following in Language Models through Activation Steering](https://openreview.net/forum?id=wozhdnRCtw) |  | 0 | The ability to follow instructions is crucial for numerous real-world applications of language models. In pursuit of deeper insights and more powerful capabilities, we derive instruction-specific vector representations from language models and use them to steer models accordingly. These vectors are... | Alessandro Stolfo, Besmira Nushi, Eric Horvitz, Safoora Yousefi, Vidhisha Balachandran |  |
| 1747 |  |  [Functional Homotopy: Smoothing Discrete Optimization via Continuous Parameters for LLM Jailbreak Attacks](https://openreview.net/forum?id=uhaLuZcCjH) |  | 0 | Optimization methods are widely employed in deep learning to address and mitigate undesired model responses. While gradient-based techniques have proven effective for image models, their application to language models is hindered by the discrete nature of the input space. This study introduces a... | Ashish Hooda, Divyam Anshumaan, Somesh Jha, Yudong Chen, Zi Wang |  |
| 1748 |  |  [Forte : Finding Outliers with Representation Typicality Estimation](https://openreview.net/forum?id=7XNgVPxCiA) |  | 0 | Generative models can now produce photorealistic synthetic data which is virtually indistinguishable from the real data used to train it. This is a significant evolution over previous models which could produce reasonable facsimiles of the training data, but ones which could be visually... | Andrew Seohwan Yu, Debargha Ganguly, Vipin Chaudhary, Warren Richard Morningstar |  |
| 1749 |  |  [NarrativeBridge: Enhancing Video Captioning with Causal-Temporal Narrative](https://openreview.net/forum?id=bBoetBIN2R) |  | 0 | Existing video captioning benchmarks and models lack causal-temporal narrative, which is sequences of events linked through cause and effect, unfolding over time and driven by characters or agents. This lack of narrative restricts models’ ability to generate text descriptions that capture the... | Adrian Hilton, Armin Mustafa, Asmar Nadeem, Faegheh Sardari, Robert Dawes, Syed Sameed Husain |  |
| 1750 |  |  [Provable unlearning in topic modeling and downstream tasks](https://openreview.net/forum?id=dh78yRFVK9) |  | 0 | Machine unlearning algorithms are increasingly important as legal concerns arise around the provenance of training data, but verifying the success of unlearning is often difficult. Provable guarantees for unlearning are often limited to supervised learning settings. In this paper, we provide the... | Amartya Sanyal, Sadhika Malladi, Sanjeev Arora, Stanley Wei |  |
| 1751 |  |  [Point-based Instance Completion with Scene Constraints](https://openreview.net/forum?id=llSiIJosDj) |  | 0 | Recent point-based object completion methods have demonstrated the ability to accurately recover the missing geometry of partially observed objects. However, these approaches are not well-suited for completing objects within a scene, as they do not consider known scene constraints (e.g., other... | Fuxin Li, Wesley Khademi |  |
| 1752 |  |  [FACTS: A Factored State-Space Framework for World Modelling](https://openreview.net/forum?id=dmCGjPFVhF) |  | 0 | World modelling is essential for understanding and predicting the dynamics of complex systems by learning both spatial and temporal dependencies. However, current frameworks, such as Transformers and selective state-space models like Mambas, exhibit limitations in efficiently encoding spatial and... | Firas Laakom, Jürgen Schmidhuber, Nanbo Li, Wenyi Wang, Yucheng Xu |  |
| 1753 |  |  [6D Object Pose Tracking in Internet Videos for Robotic Manipulation](https://openreview.net/forum?id=1CIUkpoata) |  | 0 | We seek to extract a temporally consistent 6D pose trajectory of a manipulated object from an Internet instructional video. This is a challenging set-up for current 6D pose estimation methods due to uncontrolled capturing conditions, subtle but dynamic object motions, and the fact that the exact... | Georgy Ponimatkin, Josef Sivic, Martin Cífka, Médéric Fourmy, Tomás Soucek, Vladimír Petrík, Yann Labbé |  |
| 1754 |  |  [Towards Foundation Models for Mixed Integer Linear Programming](https://openreview.net/forum?id=6yENDA7J4G) |  | 0 | Mixed Integer Linear Programming (MILP) is essential for modeling complex decision-making problems but faces challenges in computational tractability and interpretability. Current deep learning approaches for MILP focus on specific problem classes and do not generalize to unseen classes. To address... | Beibin Li, Cathy Wu, Ishai Menache, Janardhan Kulkarni, Sirui Li |  |
| 1755 |  |  [A Generic Framework for Conformal Fairness](https://openreview.net/forum?id=xiQNfYl33p) |  | 0 | Conformal Prediction (CP) is a popular method for uncertainty quantification with machine learning models. While conformal prediction provides probabilistic guarantees regarding the coverage of the true label, these guarantees are agnostic to the presence of sensitive attributes within the dataset.... | Aditya T. Vadlamani, Ali Payani, Anutam Srinivasan, Pranav Maneriker, Srinivasan Parthasarathy |  |
| 1756 |  |  [Concept Bottleneck Large Language Models](https://openreview.net/forum?id=RC5FPYVQaH) |  | 0 | We introduce Concept Bottleneck Large Language Models (CB-LLMs), a novel framework for building inherently interpretable Large Language Models (LLMs). In contrast to traditional black-box LLMs that rely on limited post-hoc interpretations, CB-LLMs integrate intrinsic interpretability directly into... | Berk Ustun, ChungEn Sun, TsuiWei Weng, Tuomas P. Oikarinen |  |
| 1757 |  |  [Dual Process Learning: Controlling Use of In-Context vs. In-Weights Strategies with Weight Forgetting](https://openreview.net/forum?id=jDsmB4o5S0) |  | 0 | Language models have the ability to perform in-context learning (ICL), allowing them to flexibly adapt their behavior based on context. This contrasts with in-weights learning (IWL), where memorized information is encoded in model parameters after iterated observations of data. An ideal model... | Ellie Pavlick, Jack Merullo, Michael A. Lepori, Suraj Anand |  |
| 1758 |  |  [Near-Exact Privacy Amplification for Matrix Mechanisms](https://openreview.net/forum?id=txV4dNeusx) |  | 0 | We study the problem of computing the privacy parameters for DP machine learning when using privacy amplification via random batching and noise correlated across rounds via a correlation matrix $\textbf{C}$ (i.e., the matrix mechanism). Past work on this problem either only applied to banded... | Abhradeep Guha Thakurta, Arun Ganesh, Christopher A. ChoquetteChoo, Saminul Haque, Thomas Steinke |  |
| 1759 |  |  [Improving Semantic Understanding in Speech Language Models via Brain-tuning](https://openreview.net/forum?id=KL8Sm4xRn7) |  | 0 | Speech language models align with human brain responses to natural language to an impressive degree. However, current models rely heavily on low-level speech features, indicating they lack brain-relevant semantics which limits their utility as model organisms of semantic processing in the brain. In... | Dietrich Klakow, Mariya Toneva, Omer Moussa |  |
| 1760 |  |  [BitStack: Any-Size Compression of Large Language Models in Variable Memory Environments](https://openreview.net/forum?id=lBntjGbyv0) |  | 0 | Large language models (LLMs) have revolutionized numerous applications, yet their deployment remains challenged by memory constraints on local devices. While scaling laws have enhanced LLM capabilities, the primary bottleneck has shifted from $\textit{capability}$ to $\textit{availability}$,... | Bo Wang, Dong Zhang, Pengyu Wang, Xinghao Wang, Xipeng Qiu, Yunhua Zhou |  |
| 1761 |  |  [ACES: Automatic Cohort Extraction System for Event-Stream Datasets](https://openreview.net/forum?id=P4XmKjXTrM) |  | 0 | Reproducibility remains a significant challenge in machine learning (ML) for healthcare. Datasets, model pipelines, and even task or cohort definitions are often private in this field, leading to a significant barrier in sharing, iterating, and understanding ML results on electronic health record... | Alistair E. W. Johnson, Jack Gallifant, Justin Xu, Matthew B. A. McDermott |  |
| 1762 |  |  [Almost Optimal Batch-Regret Tradeoff for Batch Linear Contextual Bandits](https://openreview.net/forum?id=rakhNY32vw) |  | 0 | We study the optimal batch-regret tradeoff for batch linear contextual bandits. For this problem, we design batch learning algorithms and prove that they achieve the optimal regret bounds (up to logarithmic factors) for any batch number $M$, number of actions $K$, time horizon $T$, and dimension... | Xiangyang Ji, Yuan Zhou, Zihan Zhang |  |
| 1763 |  |  [Can We Talk Models Into Seeing the World Differently?](https://openreview.net/forum?id=iVMcYxTiVM) |  | 0 | Unlike traditional vision-only models, vision language models (VLMs) offer an intuitive way to access visual content through language prompting by combining a large language model (LLM) with a vision encoder. However, both the LLM and the vision encoder come with their own set of biases, cue... | Janis Keuper, Jovita Lukasik, Margret Keuper, Muhammad Jehanzeb Mirza, Paul Gavrikov, Robert Geirhos, Steffen Jung |  |
| 1764 |  |  [Vision CNNs trained to estimate spatial latents learned similar ventral-stream-aligned representations](https://openreview.net/forum?id=emMMa4q0qw) |  | 0 | Studies of the functional role of the primate ventral visual stream have traditionally focused on object categorization, often ignoring -- despite much prior evidence -- its role in estimating "spatial" latents such as object position and pose. Most leading ventral stream models are derived by... | Esther Alter, James J. DiCarlo, Jeremy Schwartz, Joshua B. Tenenbaum, Weichen Huang, Yudi Xie |  |
| 1765 |  |  [HAINAN: Fast and Accurate Transducer for Hybrid-Autoregressive ASR](https://openreview.net/forum?id=LrmPGtnros) |  | 0 | We present Hybrid-Autoregressive INference TrANsducers (HAINAN), a novel architecture for speech recognition that extends the Token-and-Duration Transducer (TDT) model. Trained with randomly masked predictor network outputs, HAINAN supports both autoregressive inference with all network components... | Boris Ginsburg, Hainan Xu, Travis M. Bartley, Vladimir Bataev |  |
| 1766 |  |  [Inference Optimal VLMs Need Fewer Visual Tokens and More Parameters](https://openreview.net/forum?id=6VhDQP7WGX) |  | 0 | Vision Language Models (VLMs) have demonstrated strong capabilities across various visual understanding and reasoning tasks, driven by incorporating image representations into the token inputs of Large Language Models (LLMs). However, their real-world deployment is often constrained by high latency... | J. Zico Kolter, João D. Semedo, Kevin Y. Li, Sachin Goyal |  |
| 1767 |  |  [TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention](https://openreview.net/forum?id=EkfLaCJ7bk) |  | 0 | Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints,... | Lijie Yang, Zhihao Jia, Zhihao Zhang, Zhuofu Chen, Zikun Li |  |
| 1768 |  |  [MediConfusion: Can you trust your AI radiologist? Probing the reliability of multimodal medical foundation models](https://openreview.net/forum?id=H9UnNgdq0g) |  | 0 | Multimodal Large Language Models (MLLMs) have tremendous potential to improve the accuracy, availability, and cost-effectiveness of healthcare by providing automated solutions or serving as aids to medical professionals. Despite promising first steps in developing medical MLLMs in the past few... | Mahdi Soltanolkotabi, Maryam Soltanolkotabi, Mohammad Shahab Sepehri, Zalan Fabian |  |
| 1769 |  |  [Mutual Effort for Efficiency: A Similarity-based Token Pruning for Vision Transformers in Self-Supervised Learning](https://openreview.net/forum?id=GTcEe5fayC) |  | 0 | Self-supervised learning (SSL) offers a compelling solution to the challenge of extensive labeled data requirements in traditional supervised learning. With the proven success of Vision Transformers (ViTs) in supervised tasks, there is increasing interest in adapting them for SSL frameworks.... | Ao Li, Geng Yuan, Jun Liu, Ninghao Liu, Qitao Tan, Sheng Li, Tianyu Wang, Xulong Tang, Yue Dai, Yufei Ding, Zhenglun Kong |  |
| 1770 |  |  [Towards counterfactual fairness through auxiliary variables](https://openreview.net/forum?id=GpUv1FvZi1) |  | 0 | The challenge of balancing fairness and predictive accuracy in machine learning models, especially when sensitive attributes such as race, gender, or age are considered, has motivated substantial research in recent years. Counterfactual fairness ensures that predictions remain consistent across... | Ang Li, Bowei Tian, Guoheng Sun, Shwai He, Wanghao Ye, Yongkai Wu, Yucong Dai, Ziyao Wang |  |
| 1771 |  |  [LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language Models for Referring Expression Comprehension](https://openreview.net/forum?id=PgXpOOqtyd) |  | 0 | Vision Language Models (VLMs) have demonstrated remarkable capabilities in various open-vocabulary tasks, yet their zero-shot performance lags behind task-specific fine-tuned models, particularly in complex tasks like Referring Expression Comprehension (REC). Fine-tuning usually requires... | Amaia Cardiel, Elias Ramzi, Eloi Zablocki, Matthieu Cord, Oriane Siméoni |  |
| 1772 |  |  [Conformalized Interactive Imitation Learning: Handling Expert Shift and Intermittent Feedback](https://openreview.net/forum?id=Ym2RNPX6la) |  | 0 | In interactive imitation learning (IL), uncertainty quantification offers a way for the learner (i.e. robot) to contend with distribution shifts encountered during deployment by actively seeking additional feedback from an expert (i.e. human) online. Prior works use mechanisms like ensemble... | Aaditya Ramdas, Andrea Bajcsy, Henny Admoni, Michelle D. Zhao, Reid G. Simmons |  |
| 1773 |  |  [Bayesian Regularization of Latent Representation](https://openreview.net/forum?id=VOoJEQlLW5) |  | 0 | The effectiveness of statistical and machine learning methods depends on how well data features are characterized. Developing informative and interpretable latent representations with controlled complexity is essential for visualizing data structure and for facilitating efficient model building... | Chukwudi Paul Obite, Keyan Wu, Shiwei Lan, Zhi Chang |  |
| 1774 |  |  [AnoLLM: Large Language Models for Tabular Anomaly Detection](https://openreview.net/forum?id=7VkHffT5X2) |  | 0 | We introduce AnoLLM, a novel framework that leverages large language models (LLMs) for unsupervised tabular anomaly detection. By converting tabular data into a standardized text format, we further adapt a pre-trained LLM with this serialized data, and assign anomaly scores based on the negative... | ChePing Tsai, Ganyu Teng, Phillip Wallis, Wei Ding |  |
| 1775 |  |  [GPS: A Probabilistic Distributional Similarity with Gumbel Priors for Set-to-Set Matching](https://openreview.net/forum?id=U0SijGsCHJ) |  | 0 | Set-to-set matching aims to identify correspondences between two sets of unordered items by minimizing a distance metric or maximizing a similarity measure. Traditional metrics, such as Chamfer Distance (CD) and Earth Mover’s Distance (EMD), are widely used for this purpose but often suffer from... | Fangzhou Lin, Haichong Zhang, Haotian Liu, Jose Morales, Kazunori D. Yamada, Venkatesh Saligrama, Vijaya B. Kolachalama, Ziming Zhang |  |
| 1776 |  |  [Learning Graph Quantized Tokenizers](https://openreview.net/forum?id=oYSsbY3G4o) |  | 0 | Transformers serve as the backbone architectures of Foundational Models, where domain-specific tokenizers allow them to adapt to various domains. Graph Transformers (GTs) have recently emerged as leading models in geometric deep learning, outperforming Graph Neural Networks (GNNs) in various graph... | Baichuan Yuan, Bo Long, Dongqi Fu, Hao Wu, Kaveh Hassani, Limei Wang, Ning Yao, Si Zhang, Weilin Cong, Zhigang Hua |  |
| 1777 |  |  [Bridging the Gap Between f-divergences and Bayes Hilbert Spaces](https://openreview.net/forum?id=m5qpn0KTMZ) |  | 0 | We introduce a novel framework that generalizes $f$-divergences by incorporating locally non-convex divergence-generating functions. Using this extension, we define a new class of pseudo $f$-divergences, encompassing a wider range of distances between distributions that traditional $f$-divergences... | Alexander Willi Fottner, Linus Lach, Yarema Okhrin |  |
| 1778 |  |  [SBSC: Step-by-Step Coding for Improving Mathematical Olympiad Performance](https://openreview.net/forum?id=wSkvf2WyYz) |  | 0 | We propose Step-by-Step Coding (SBSC): a multi-turn math reasoning framework that enables Large Language Models (LLMs) to generate sequence of programs for solving Olympiad level math problems. After each turn/step, by leveraging the code execution outputs and programs of previous steps, the model... | Ankan Biswas, Kunal Singh, Pradeep Moturi, Sayandeep Bhowmick, Siva Kishore Gollapalli |  |
| 1779 |  |  [Transformers Can Learn Temporal Difference Methods for In-Context Reinforcement Learning](https://openreview.net/forum?id=Pj06mxCXPl) |  | 0 | Traditionally, reinforcement learning (RL) agents learn to solve new tasks by updating their neural network parameters through interactions with the task environment. However, recent works demonstrate that some RL agents, after certain pretraining procedures, can learn to solve unseen new tasks... | Ethan Blaser, Hadi Daneshmand, Jiuqi Wang, Shangtong Zhang |  |
| 1780 |  |  [Balancing Act: Diversity and Consistency in Large Language Model Ensembles](https://openreview.net/forum?id=Dl6nkKKvlX) |  | 0 | Ensembling strategies for Large Language Models (LLMs) have demonstrated significant potential in improving performance across various tasks by combining the strengths of individual models. However, identifying the most effective ensembling method remains an open challenge, as neither maximizing... | Ahmed Abdulaal, Amrutha Saseendran, Aryo Pradipta Gema, Chen Jin, Daniel C. Alexander, Daniel C. Castro, Dino Oglic, Nina Montaña Brown, Philip Alexander Teare, Tom Diethe |  |
| 1781 |  |  [Improved Algorithms for Kernel Matrix-Vector Multiplication Under Sparsity Assumptions](https://openreview.net/forum?id=wLnls9LS3x) |  | 0 | Motivated by the problem of fast processing of attention matrices, we study fast algorithms for computing matrix-vector products for asymmetric Gaussian Kernel matrices $K\in \mathbb{R}^{n\times n}$. $K$'s columns are indexed by a set of $n$ keys $k_1,k_2\ldots, k_n\in \mathbb{R}^d$, rows by a set... | Kshiteej Sheth, Michael Kapralov, Piotr Indyk, Tal Wagner |  |
| 1782 |  |  [NextBestPath: Efficient 3D Mapping of Unseen Environments](https://openreview.net/forum?id=7WaRh4gCXp) |  | 0 | This work addresses the problem of active 3D mapping, where an agent must find an efficient trajectory to exhaustively reconstruct a new scene. Previous approaches mainly predict the next best view near the agent's location, which is prone to getting stuck in local areas. Additionally, existing... | Antoine Guédon, Clémentin Boittiaux, Shiyao Li, Shizhe Chen, Vincent Lepetit |  |
| 1783 |  |  [Is Large-scale Pretraining the Secret to Good Domain Generalization?](https://openreview.net/forum?id=wCOJpXm0Me) |  | 0 | Multi-Source Domain Generalization (DG) is the task of training on multiple source domains and achieving high classification performance on unseen target domains. Recent methods combine robust features from web-scale pretrained backbones with new features learned from source data, and this has... | Bryan A. Plummer, Kate Saenko, Kuniaki Saito, Piotr Teterwak, Theodoros Tsiligkaridis |  |
| 1784 |  |  [Grokking at the Edge of Numerical Stability](https://openreview.net/forum?id=TvfkSyHZRA) |  | 0 | Grokking, or sudden generalization that occurs after prolonged overfitting, is a surprising phenomenon that has challenged our understanding of deep learning. While a lot of progress has been made in understanding grokking, it is still not clear why generalization is delayed and why grokking often... | Lucas Prieto, Melih Barsbey, Pedro A. M. Mediano, Tolga Birdal |  |
| 1785 |  |  [Learning from Imperfect Human Feedback: A Tale from Corruption-Robust Dueling](https://openreview.net/forum?id=ptjrpEGrGg) |  | 0 | This paper studies Learning from Imperfect Human Feedback (LIHF), addressing the potential irrationality or imperfect perception when learning from comparative human feedback. Building on evidences that human's imperfection decays over time (i.e., humans learn to improve), we cast this problem as a... | Fan Yao, Haifeng Xu, Xuefeng Liu, Yuwei Cheng |  |
| 1786 |  |  [Linear Combination of Saved Checkpoints Makes Consistency and Diffusion Models Better](https://openreview.net/forum?id=QowsEic1sc) |  | 0 | Diffusion Models (DM) and Consistency Models (CM) are two types of popular generative models with good generation quality on various tasks. When training DM and CM, intermediate weight checkpoints are not fully utilized and only the last converged checkpoint is used. In this work, we find proper... | Enshu Liu, Guohao Dai, Huazhong Yang, Junyi Zhu, Matthew B. Blaschko, Sergey Yekhanin, Shengen Yan, Shuaiqi Wang, Xuefei Ning, Yu Wang, Zinan Lin |  |
| 1787 |  |  [Scalable Bayesian Learning with posteriors](https://openreview.net/forum?id=fifXzmzeGy) |  | 0 | Although theoretically compelling, Bayesian learning with modern machine learning models is computationally challenging since it requires approximating a high dimensional posterior distribution. In this work, we (i) introduce \*\*_posteriors_\*\*, an easily extensible PyTorch library hosting... | Daniel Simpson, Johnathan Chiu, Kaelan Donatella, Phoebe Klett, Samuel Duffield |  |
| 1788 |  |  [On the Convergence of No-Regret Dynamics in Information Retrieval Games with Proportional Ranking Functions](https://openreview.net/forum?id=jJXZvPe5z0) |  | 0 | Publishers who publish their content on the web act strategically, in a behavior that can be modeled within the online learning framework. Regret, a central concept in machine learning, serves as a canonical measure for assessing the performance of learning agents within this framework. We prove... | Idan Pipano, Itamar Reinman, Moshe Tennenholtz, Omer Madmon |  |
| 1789 |  |  [MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos](https://openreview.net/forum?id=tRNKe2Vgqt) |  | 0 | Multimodal Language Language Models (MLLMs) demonstrate the emerging abilities of "world models"---interpreting and reasoning about complex real-world dynamics. To assess these abilities, we posit videos are the ideal medium, as they encapsulate rich representations of real-world dynamics and... | Jiachen Li, Jianfeng Wang, Kaizhi Zheng, Kevin Lin, Lijuan Wang, Linjie Li, Wanrong Zhu, Weixi Feng, William Yang Wang, Xin Eric Wang, Xuehai He, Yue Fan, Yujie Lu, Zhengyuan Yang |  |
| 1790 |  |  [PolyNet: Learning Diverse Solution Strategies for Neural Combinatorial Optimization](https://openreview.net/forum?id=TKuYWeFE6S) |  | 0 | Reinforcement learning-based methods for constructing solutions to combinatorial optimization problems are rapidly approaching the performance of human-designed algorithms. To further narrow the gap, learning-based approaches must efficiently explore the solution space during the search process.... | André Hottung, Kevin Tierney, Mridul Mahajan |  |
| 1791 |  |  [MetaOOD: Automatic Selection of OOD Detection Models](https://openreview.net/forum?id=9qpdDiDQ2H) |  | 0 | How can we automatically select an out-of-distribution (OOD) detection model for various underlying tasks? This is crucial for maintaining the reliability of open-world applications by identifying data distribution shifts, particularly in critical domains such as online transactions, autonomous... | Xueying Ding, Yi Nian, Yichi Zhang, Yue Zhao, Yuehan Qin |  |
| 1792 |  |  [MGDA Converges under Generalized Smoothness, Provably](https://openreview.net/forum?id=wgDB1QuxIA) |  | 0 | Multi-objective optimization (MOO) is receiving more attention in various fields such as multi-task learning. Recent works provide some effective algorithms with theoretical analysis but they are limited by the standard $L$-smooth or bounded-gradient assumptions, which typically do not hold for... | Kaiyi Ji, Peiyao Xiao, Qi Zhang, Shaofeng Zou |  |
| 1793 |  |  [Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models](https://openreview.net/forum?id=FhTAG591Ve) |  | 0 | The dominant paradigm for RLHF is \*online\* and \*on-policy\* RL: synchronously generating from the large language model (LLM) policy, labelling with a reward model, and learning using feedback on the LLM's own outputs. While performant, this paradigm is computationally inefficient. Inspired by... | Aaron C. Courville, Arian Hosseini, Michael Noukhovitch, Rishabh Agarwal, Shengyi Huang, Sophie Xhonneux |  |
| 1794 |  |  [RepoGraph: Enhancing AI Software Engineering with Repository-level Code Graph](https://openreview.net/forum?id=dw9VUsSHGB) |  | 0 | Large Language Models (LLMs) excel in code generation yet struggle with modern AI software engineering tasks. Unlike traditional function-level or file-level coding tasks, AI software engineering requires not only basic coding proficiency but also advanced skills in managing and interacting with... | Dong Yu, Hongming Zhang, Jiawei Han, Kaixin Ma, Mengzhao Jia, Siru Ouyang, Wenhao Yu, Zhihan Zhang, Zilin Xiao |  |
| 1795 |  |  [On the Price of Differential Privacy for Hierarchical Clustering](https://openreview.net/forum?id=yLhJYvkKA0) |  | 0 | Hierarchical clustering is a fundamental unsupervised machine learning task with the aim of organizing data into a hierarchy of clusters. Many applications of hierarchical clustering involve sensitive user information, therefore motivating recent studies on differentially private hierarchical... | Chen Wang, Chengyuan Deng, Jalaj Upadhyay, Jie Gao, Samson Zhou |  |
| 1796 |  |  [GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-Time Alignment](https://openreview.net/forum?id=J0qTpmbSbh) |  | 0 | Large Language Models (LLMs) exhibit impressive capabilities but require careful alignment with human preferences. Traditional training-time methods finetune LLMs using human preference datasets but incur significant training costs and require repeated training to handle diverse user preferences.... | Alec Koppel, Bang An, Furong Huang, Sicheng Zhu, Sumitra Ganesh, Udari Madhushani Sehwag, Yuancheng Xu |  |
| 1797 |  |  [Unlocking Guidance for Discrete State-Space Diffusion and Flow Models](https://openreview.net/forum?id=XsgHl54yO7) |  | 0 | Generative models on discrete state-spaces have a wide range of potential applications, particularly in the domain of natural sciences. In continuous state-spaces, controllable and flexible generation of samples with desired properties has been realized using guidance on diffusion and flow models.... | Hunter Nisonoff, Jennifer Listgarten, Junhao Xiong, Stephan Allenspach |  |
| 1798 |  |  [NeurFlow: Interpreting Neural Networks through Neuron Groups and Functional Interactions](https://openreview.net/forum?id=GdbQyFOUlJ) |  | 0 | Understanding the inner workings of neural networks is essential for enhancing model performance and interpretability. Current research predominantly focuses on examining the connection between individual neurons and the model's final predictions, which suffers from challenges in interpreting the... | Hieu H. Pham, My T. Thai, Nhat HoangXuan, Phi Le Nguyen, Tue Minh Cao |  |
| 1799 |  |  [Beyond FVD: An Enhanced Evaluation Metrics for Video Generation Distribution Quality](https://openreview.net/forum?id=cC3LxGZasH) |  | 0 | The Fréchet Video Distance (FVD) is a widely adopted metric for evaluating video generation distribution quality. However, its effectiveness relies on critical assumptions. Our analysis reveals three significant limitations: (1) the non-Gaussianity of the Inflated 3D Convnet (I3D) feature space;... | Alexia JolicoeurMartineau, Christopher Pal, Ge Ya Luo, Gian Mario Favero, Zhi Hao Luo |  |
| 1800 |  |  [MAVIS: Mathematical Visual Instruction Tuning with an Automatic Data Engine](https://openreview.net/forum?id=MnJzJ2gvuf) |  | 0 | Multi-modal Large Language Models (MLLMs) have recently showcased superior proficiency in general visual scenarios. However, we identify their mathematical capabilities remain under-explored with three areas to be improved: visual encoding of math diagrams, diagram-language alignment, and... | Aojun Zhou, Chengzhuo Tong, Dongzhi Jiang, Hongsheng Li, Jiaming Liu, Peng Gao, Renrui Zhang, Shanghang Zhang, Xinyu Wei, Yichi Zhang, Ziyu Guo |  |
| 1801 |  |  [LLaRA: Supercharging Robot Learning Data for Vision-Language Policy](https://openreview.net/forum?id=iVxxgZlXh6) |  | 0 | Vision Language Models (VLMs) have recently been leveraged to generate robotic actions, forming Vision-Language-Action (VLA) models. However, directly adapting a pretrained VLM for robotic control remains challenging, particularly when constrained by a limited number of robot demonstrations. In... | Cristina Mata, Jinghuan Shang, Jongwoo Park, Kanchana Ranasinghe, Kumara Kahatapitiya, Michael S. Ryoo, Mu Cai, Ryan D. Burgert, Xiang Li, Yong Jae Lee, Yoo Sung Jang |  |
| 1802 |  |  [Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning](https://openreview.net/forum?id=44CoQe6VCq) |  | 0 | Large language models (LLMs) have showcased remarkable reasoning capabilities, yet they remain susceptible to errors, particularly in temporal reasoning tasks involving complex temporal logic. Existing research has explored LLM performance on temporal reasoning using diverse datasets and... | Anton Tsitsulin, Bahare Fatemi, Bryan Perozzi, Jinyeong Yim, John Palowitch, Jonathan Halcrow, Karishma Malkan, Mehran Kazemi, Sungyong Seo |  |
| 1803 |  |  [Efficient stagewise pretraining via progressive subnetworks](https://openreview.net/forum?id=Y5LjYI4N6P) |  | 0 | Recent developments in large language models have sparked interest in efficient pretraining methods. Stagewise training approaches to improve efficiency, like gradual stacking and layer dropping (Reddi et al., 2023; Zhang & He, 2020), have recently garnered attention. The prevailing view suggests... | Abhishek Panigrahi, Kaifeng Lyu, Nikunj Saunshi, Sanjiv Kumar, Sashank J. Reddi, Satyen Kale, Sobhan Miryoosefi |  |
| 1804 |  |  [Better than Your Teacher: LLM Agents that learn from Privileged AI Feedback](https://openreview.net/forum?id=st7XqFgbAH) |  | 0 | While large language models (LLMs) show impressive decision-making abilities, current methods lack a mechanism for automatic self-improvement from errors during task execution. We propose LEAP, an iterative fine-tuning framework that continually improves LLM agents using feedback from AI expert... | Paloma Sodhi, Sanjiban Choudhury |  |
| 1805 |  |  [MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding](https://openreview.net/forum?id=TrVYEZtSQH) |  | 0 | We introduce MuirBench, a comprehensive benchmark that focuses on robust multi-image understanding capabilities of multimodal LLMs. MuirBench consists of 12 diverse multi-image tasks (e.g., scene understanding, ordering) that involve 10 categories of multi-image relations (e.g., multiview, temporal... | Chaowei Xiao, Chunyuan Li, Dan Roth, Fei Wang, Hoifung Poon, HsiangHui Liu, James Y. Huang, Kai Zhang, KaiWei Chang, Mingyu Derek Ma, Muhao Chen, Nan Xu, Pan Lu, Qin Liu, Sheng Zhang, Tianyi Lorena Yan, Wenjie Jacky Mo, Wenxuan Zhou, Xiaogeng Liu, Xingyu Fu, Zekun Li |  |
| 1806 |  |  [Online Clustering with Nearly Optimal Consistency](https://openreview.net/forum?id=NA2vUMaMOm) |  | 0 | We give online algorithms for $k$-Means(more generally, $(k, z)$-Clustering) with nearly optimal consistency (a notion suggested by Lattanzi & Vassilvitskii (2017)). Our result turns any $\alpha$-approximate offline algorithm for clustering into an $(1+\epsilon)\alpha^2$-competitive online... | Mengshi Zhao, Shaofeng H.C. Jiang, T.H. Hubert Chan, Tianyi Wu |  |
| 1807 |  |  [Graph Transformers Dream of Electric Flow](https://openreview.net/forum?id=rWQDzq3O5c) |  | 0 | We show theoretically and empirically that the linear Transformer, when applied to graph data, can implement algorithms that solve canonical problems such as electric flow and eigenvector decomposition. The Transformer has access to information on the input graph only via the graph's incidence... | Lawrence Carin, Suvrit Sra, Xiang Cheng |  |
| 1808 |  |  [Doubly Optimal Policy Evaluation for Reinforcement Learning](https://openreview.net/forum?id=60GeEoG5kD) |  | 0 | Policy evaluation estimates the performance of a policy by (1) collecting data from the environment and (2) processing raw data into a meaningful estimate. Due to the sequential nature of reinforcement learning, any improper data-collecting policy or data-processing method substantially... | Claire Chen, Shangtong Zhang, Shuze Daniel Liu |  |
| 1809 |  |  [SoundCTM: Unifying Score-based and Consistency Models for Full-band Text-to-Sound Generation](https://openreview.net/forum?id=KrK6zXbjfO) |  | 0 | Sound content creation, essential for multimedia works such as video games and films, often involves extensive trial-and-error, enabling creators to semantically reflect their artistic ideas and inspirations, which evolve throughout the creation process, into the sound. Recent high-quality... | ChiehHsin Lai, Dongjun Kim, Koichi Saito, Takashi Shibuya, Yuhta Takida, Yuki Mitsufuji, Zhi Zhong |  |
| 1810 |  |  [Breaking the Reclustering Barrier in Centroid-based Deep Clustering](https://openreview.net/forum?id=r01fcKhzT5) |  | 0 | This work investigates an important phenomenon in centroid-based deep clustering (DC) algorithms: Performance quickly saturates after a period of rapid early gains. Practitioners commonly address early saturation with periodic reclustering, which we demonstrate to be insufficient to address... | Andrii Shkabrii, Claudia Plant, Collin Leiber, Kevin Sidak, Lukas Miklautz, Sebastian Tschiatschek, Thomas Lang, Timo Klein |  |
| 1811 |  |  [The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for Open-Ended Text Generation](https://openreview.net/forum?id=Ij9ilPh36h) |  | 0 | This paper introduces the counter-intuitive generalization results of overfitting pre-trained large language models (LLMs) on very small datasets. In the setting of open-ended text generation, it is well-documented that LLMs tend to generate repetitive and dull sequences, a phenomenon that is... | Daniel Ward, Fangyu Liu, Fredrik Carlsson, Joakim Nivre, Murathan Kurfali |  |
| 1812 |  |  [Learning a Fast Mixing Exogenous Block MDP using a Single Trajectory](https://openreview.net/forum?id=41WIgfdd5o) |  | 0 | In order to train agents that can quickly adapt to new objectives or reward functions, efficient unsupervised representation learning in sequential decision-making environments can be important. Frameworks such as the Exogenous Block Markov Decision Process (Ex-BMDP) have been proposed to formalize... | Alexander Levine, Amy Zhang, Peter Stone |  |
| 1813 |  |  [DataGen: Unified Synthetic Dataset Generation via Large Language Models](https://openreview.net/forum?id=F5R0lG74Tu) |  | 0 | Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly impacted various fields by enabling high-quality synthetic data generation and reducing dependence on expensive human-generated datasets. Despite this, challenges remain in the areas of generalization, controllability,... | Chaowei Xiao, Chujie Gao, Dongping Chen, Jianfeng Gao, Lichao Sun, Qihui Zhang, Siyuan Wu, Tianyi Zhou, Xiangliang Zhang, Yao Wan, Yue Huang |  |
| 1814 |  |  [Language-Image Models with 3D Understanding](https://openreview.net/forum?id=yaQbTAD2JJ) |  | 0 | Multi-modal large language models (MLLMs) have shown incredible capabilities in a variety of 2D vision and language tasks. We extend MLLMs’ perceptual capabilities to ground and reason about images in 3-dimensional space. To that end, we first develop a large-scale pretraining dataset for 2D and 3D... | Boris Ivanovic, Boyi Li, Edward Schmerling, Jang Hyun Cho, Marco Pavone, Philipp Krähenbühl, Xinshuo Weng, Yan Wang, Yue Wang, Yulong Cao, Yurong You |  |
| 1815 |  |  [Differentially Private Federated Learning with Time-Adaptive Privacy Spending](https://openreview.net/forum?id=W0nydevOlG) |  | 0 | Federated learning (FL) with differential privacy (DP) provides a framework for collaborative machine learning, enabling clients to train a shared model while adhering to strict privacy constraints. The framework allows each client to have an individual privacy guarantee, e.g., by adding different... | Adam Dziedzic, Franziska Boenisch, Nupur Kulkarni, Shahrzad Kiani, Stark C. Draper |  |
| 1816 |  |  [Expressivity of Neural Networks with Random Weights and Learned Biases](https://openreview.net/forum?id=5xwx1Myosu) |  | 0 | Landmark universal function approximation results for neural networks with trained weights and biases provided the impetus for the ubiquitous use of neural networks as learning models in neuroscience and Artificial Intelligence (AI). Recent work has extended these results to networks in which a... | Alexandre Payeur, Avery HeeWoon Ryoo, Ezekiel Williams, Guillaume Lajoie, Luca Mazzucato, Matthew G. Perich, Thomas Jiralerspong |  |
| 1817 |  |  [PADRe: A Unifying Polynomial Attention Drop-in Replacement for Efficient Vision Transformer](https://openreview.net/forum?id=YFxfcQMLWX) |  | 0 | We present Polynomial Attention Drop-in Replacement (PADRe), a novel and unifying framework designed to replace the conventional self-attention mechanism in transformer models. Notably, several recent alternative attention mechanisms, including Hyena, Mamba, SimA, Conv2Former, and Castling-ViT, can... | Dalton Jones, Fatih Porikli, Hong Cai, HsinPai Cheng, Manish Kumar Singh, Matthew Harper Langston, PierreDavid Letourneau, Shizhong Han, Yunxiao Shi |  |
| 1818 |  |  [Frequency-Guided Masking for Enhanced Vision Self-Supervised Learning](https://openreview.net/forum?id=VmJdqhuTCh) |  | 0 | We present a novel frequency-based Self-Supervised Learning (SSL) approach that significantly enhances its efficacy for pre-training. Prior work in this direction masks out pre-defined frequencies in the input image and employs a reconstruction loss to pre-train the model. While achieving promising... | Amin Karimi Monsefi, Mengxi Zhou, Nastaran Karimi Monsefi, Rajiv Ramnath, SerNam Lim, WeiLun Chao |  |
| 1819 |  |  [Generalization v.s. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data](https://openreview.net/forum?id=IQxBDLmVpT) |  | 0 | The impressive capabilities of large language models (LLMs) have sparked debate over whether these models genuinely generalize to unseen tasks or predominantly rely on memorizing vast amounts of pretraining data. To explore this issue, we introduce an extended concept of memorization,... | Alfonso Amayuelas, Alon Albalak, Antonis Antoniades, Kexun Zhang, William Yang Wang, Xinyi Wang, Yanai Elazar |  |
| 1820 |  |  [Transformers Handle Endogeneity in In-Context Linear Regression](https://openreview.net/forum?id=QfhU3ZC2g1) |  | 0 | We explore the capability of transformers to address endogeneity in in-context linear regression. Our main finding is that transformers inherently possess a mechanism to handle endogeneity effectively using instrumental variables (IV). First, we demonstrate that the transformer architecture can... | Haodong Liang, Krishna Balasubramanian, Lifeng Lai |  |
| 1821 |  |  [How Does Critical Batch Size Scale in Pre-training?](https://openreview.net/forum?id=JCiF03qnmi) |  | 0 | Training large-scale models under given resources requires careful design of parallelism strategies. In particular, the efficiency notion of critical batch size (CBS), concerning the compromise between time and compute, marks the threshold beyond which greater data parallelism leads to diminishing... | Dean P. Foster, Depen Morwani, Difan Zou, Hanlin Zhang, Jingfeng Wu, Nikhil Vyas, Sham M. Kakade, Udaya Ghai |  |
| 1822 |  |  [Comparing Targeting Strategies for Maximizing Social Welfare with Limited Resources](https://openreview.net/forum?id=0iscEAo2xB) |  | 0 | Machine learning is increasingly used to select which individuals receive limited-resource interventions in domains such as human services, education, development, and more. However, it is often not apparent what the right quantity is for models to predict. In particular, policymakers rarely have... | Bryan Wilder, Vibhhu Sharma |  |
| 1823 |  |  [Federated Granger Causality Learning For Interdependent Clients With State Space Representation](https://openreview.net/forum?id=KTgQGXz5xj) |  | 0 | Advanced sensors and IoT devices have improved the monitoring and control of complex industrial enterprises. They have also created an interdependent fabric of geographically distributed process operations (clients) across these enterprises. Granger causality is an effective approach to detect and... | Ayush Mohanty, Nagi Gebraeel, Nazal Mohamed, Paritosh Ramanan |  |
| 1824 |  |  [Convex Formulations for Training Two-Layer ReLU Neural Networks](https://openreview.net/forum?id=e0X9l4kecx) |  | 0 | Solving non-convex, NP-hard optimization problems is crucial for training machine learning models, including neural networks. However, non-convexity often leads to black-box machine learning models with unclear inner workings. While convex formulations have been used for verifying neural network... | Alp Yurtsever, Karthik Prakhya, Tolga Birdal |  |
| 1825 |  |  [Beyond Linear Approximations: A Novel Pruning Approach for Attention Matrix](https://openreview.net/forum?id=sgbI8Pxwie) |  | 0 | Large Language Models (LLMs) have shown immense potential in enhancing various aspects of our daily lives, from conversational AI to search and AI assistants. However, their growing capabilities come at the cost of extremely large model sizes, making deployment on edge devices challenging due to... | Jiangxuan Long, Yingyu Liang, Yufa Zhou, Zhao Song, Zhenmei Shi |  |
| 1826 |  |  [Closed-Form Merging of Parameter-Efficient Modules for Federated Continual Learning](https://openreview.net/forum?id=ROpY0qRUXL) |  | 0 | Model merging has emerged as a crucial technique in Deep Learning, enabling the integration of multiple models into a unified system while preserving performance and scalability. In this respect, the compositional properties of low-rank adaptation techniques (e.g., LoRA) have proven beneficial, as... | Jacopo Bonato, Luigi Sabetta, Matteo Mosconi, Pietro Buzzega, Riccardo Salami, Simone Calderara |  |
| 1827 |  |  [Eliminating Position Bias of Language Models: A Mechanistic Approach](https://openreview.net/forum?id=fvkElsJOsN) |  | 0 | Position bias has proven to be a prevalent issue of modern language models (LMs), where the models prioritize content based on its position within the given context. This bias often leads to unexpected model failures and hurts performance, robustness, and reliability across various applications. A... | Chi Han, Hanlin Zhang, Hao Peng, Heng Ji, KuanHao Huang, Sham M. Kakade, Shuiwang Ji, Xiner Li, Ziqi Wang |  |
| 1828 |  |  [From Tokens to Lattices: Emergent Lattice Structures in Language Models](https://openreview.net/forum?id=md9qolJwLl) |  | 0 | Pretrained masked language models (MLMs) have demonstrated an impressive capability to comprehend and encode conceptual knowledge, revealing a lattice structure among concepts. This raises a critical question: how does this conceptualization emerge from MLM pretraining? In this paper, we explore... | Bo Xiong, Steffen Staab |  |
| 1829 |  |  [Demystifying Topological Message-Passing with Relational Structures: A Case Study on Oversquashing in Simplicial Message-Passing](https://openreview.net/forum?id=QC2qE1tcmd) |  | 0 | Topological deep learning (TDL) has emerged as a powerful tool for modeling higher-order interactions in relational data. However, phenomena such as oversquashing in topological message-passing remain understudied and lack theoretical analysis. We propose a unifying axiomatic framework that bridges... | Diaaeldin Taha, Guido Montúfar, James Chapman, Karel Devriendt, Marzieh Eidi |  |
| 1830 |  |  [Generalized Consistency Trajectory Models for Image Manipulation](https://openreview.net/forum?id=Zjv38dg1Hb) |  | 0 | Diffusion-based generative models excel in unconditional generation, as well as on applied tasks such as image editing and restoration. The success of diffusion models lies in the iterative nature of diffusion: diffusion breaks down the complex process of mapping noise to data into a sequence of... | Beomsu Kim, Jaemin Kim, Jeongsol Kim, Jong Chul Ye |  |
| 1831 |  |  [Fair Submodular Cover](https://openreview.net/forum?id=ULorFBST6X) |  | 0 | Machine learning algorithms are becoming increasing prevalent in the modern world, and as a result there has been significant recent study into algorithmic fairness in order to minimize the possibility of unintentional bias or discrimination in these algorithms. Submodular optimization problems... | Samson Zhou, Shuo Xing, Victoria G. Crawford, Wenjing Chen |  |
| 1832 |  |  [Revealing the 3D Cosmic Web through Gravitationally Constrained Neural Fields](https://openreview.net/forum?id=Ax0i933gtp) |  | 0 | Weak gravitational lensing is the slight distortion of galaxy shapes caused primarily by the gravitational effects of dark matter in the universe. In our work, we seek to invert the weak lensing signal from 2D telescope images to reconstruct a 3D map of the universe's dark matter field. While... | Aviad Levis, Brandon Zhao, Katherine L. Bouman, Liam Connor, Pratul P. Srinivasan |  |
| 1833 |  |  [Swing-by Dynamics in Concept Learning and Compositional Generalization](https://openreview.net/forum?id=s1zO0YBEF8) |  | 0 | Prior work has shown that text-conditioned diffusion models can learn to identify and manipulate primitive concepts underlying a compositional data-generating process, enabling generalization to entirely novel, out-of-distribution compositions. Beyond performance evaluations, these studies develop... | Core Francisco Park, Ekdeep Singh Lubana, Hidenori Tanaka, Maya Okawa, Wei Hu, Yongyi Yang |  |
| 1834 |  |  [MotherNet: Fast Training and Inference via Hyper-Network Transformers](https://openreview.net/forum?id=6H4jRWKFc3) |  | 0 | Foundation models are transforming machine learning across many modalities, with in-context learning replacing classical model training. Recent work on tabular data hints at a similar opportunity to build foundation models for classification for numerical data. However, existing meta-learning... | Andreas C. Mueller, Carlo Curino, Raghu Ramakrishnan |  |
| 1835 |  |  [AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents](https://openreview.net/forum?id=AC5n7xHuR1) |  | 0 | The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Meanwhile, LLM agents---which use external tools and can execute multi-stage tasks---may pose a greater... | Alexandra Souly, Andy Zou, Dan Hendrycks, Derek Duenas, J. Zico Kolter, Justin Wang, Maksym Andriushchenko, Mateusz Dziemian, Matt Fredrikson, Maxwell Lin, Xander Davies, Yarin Gal |  |
| 1836 |  |  [GTR: Improving Large 3D Reconstruction Models through Geometry and Texture Refinement](https://openreview.net/forum?id=Oxpkn0YLG1) |  | 0 | We propose a novel approach for 3D mesh reconstruction from multi-view images. We improve upon the large reconstruction model LRM that use a transformer-based triplane generator and a Neural Radiance Field (NeRF) model trained on multi-view images. We introduce three key components to significantly... | Aliaksandr Siarohin, Chaoyang Wang, HsinYing Lee, Jiaxu Zou, Michael Vasilkovsky, Peiye Zhuang, Sergei Korolev, Sergey Tulyakov, Songfang Han, Vladislav Shakhrai |  |
| 1837 |  |  [InstantPortrait: One-Step Portrait Editing via Diffusion Multi-Objective Distillation](https://openreview.net/forum?id=ZkFMe3OPfw) |  | 0 | Real-time instruction-based portrait image editing is crucial in various applications, including filters, augmented reality, and video communications, etc. However, real-time portrait editing presents three significant challenges: identity preservation, fidelity to editing instructions, and fast... | Dhritiman Sagar, Erli Ding, FuYun Wang, Keqiang Sun, Zhixin Lai |  |
| 1838 |  |  [GPUDrive: Data-driven, multi-agent driving simulation at 1 million FPS](https://openreview.net/forum?id=ERv8ptegFi) |  | 0 | Multi-agent learning algorithms have been successful at generating superhuman planning in various games but have had limited impact on the design of deployed multi-agent planners. A key bottleneck in applying these techniques to multi-agent planning is that they require billions of steps of... | Aarav Pandya, Brennan Shacklett, Daphne Cornelisse, Eugene Vinitsky, Saman Kazemkhani |  |
| 1839 |  |  [Hierarchical Uncertainty Estimation for Learning-based Registration in Neuroimaging](https://openreview.net/forum?id=w8LMtFY97b) |  | 0 | Over recent years, deep learning based image registration has achieved impressive accuracy in many domains, including medical imaging and, specifically, human neuroimaging with magnetic resonance imaging (MRI). However, the uncertainty estimation associated with these methods has been largely... | Juan Eugenio Iglesias, Karthik Gopinath, Koen Van Leemput, Malte Hoffmann, Oula Puonti, Peirong Liu, Xiaoling Hu |  |
| 1840 |  |  [When LLMs Play the Telephone Game: Cultural Attractors as Conceptual Tools to Evaluate LLMs in Multi-turn Settings](https://openreview.net/forum?id=fN8yLc3eA7) |  | 0 | As large language models (LLMs) start interacting with each other and generating an increasing amount of text online, it becomes crucial to better understand how information is transformed as it passes from one LLM to the next. While significant research has examined individual LLM behaviors,... | Clément MoulinFrier, Corentin Léger, Cédric Colas, Gaia Molinaro, Grgur Kovac, Jérémy Perez, Maxime Derex, PierreYves Oudeyer |  |
| 1841 |  |  [Advancing Out-of-Distribution Detection via Local Neuroplasticity](https://openreview.net/forum?id=1F8xTfv6ah) |  | 0 | In the domain of machine learning, the assumption that training and test data share the same distribution is often violated in real-world scenarios, requiring effective out-of-distribution (OOD) detection. This paper presents a novel OOD detection method that leverages the unique local... | Alessandro Canevaro, Georg Martius, Hang Yu, Julian Jordan, Julian Schmidt, Mohammad Sajad Marvi |  |
| 1842 |  |  [TimeInf: Time Series Data Contribution via Influence Functions](https://openreview.net/forum?id=Vz0CWFMPUe) |  | 0 | Evaluating the contribution of individual data points to a model's prediction is critical for interpreting model predictions and improving model performance. Existing data contribution methods have been applied to various data types, including tabular data, images, and text; however, their primary... | Jingyan Shen, Xiaoxue Xiong, Yizi Zhang, Yongchan Kwon |  |
| 1843 |  |  [DistRL: An Asynchronous Distributed Reinforcement Learning Framework for On-Device Control Agent](https://openreview.net/forum?id=LPG8pPSfQD) |  | 0 | On-device control agents, especially on mobile devices, are responsible for operating mobile devices to fulfill users' requests, enabling seamless and intuitive interactions. Integrating Multimodal Large Language Models (MLLMs) into these agents enhances their ability to understand and execute... | Jianheng Liu, Jianye Hao, Jun Wang, Kun Shao, Taiyi Wang, Zhihao Wu |  |
| 1844 |  |  [Learning mirror maps in policy mirror descent](https://openreview.net/forum?id=n4wcdct43X) |  | 0 | Policy Mirror Descent (PMD) is a popular framework in reinforcement learning, serving as a unifying perspective that encompasses numerous algorithms. These algorithms are derived through the selection of a mirror map and enjoy finite-time convergence guarantees. Despite its popularity, the... | Carlo Alfano, Chris Lu, Patrick Rebeschini, Sebastian Rene Towers, Silvia Sapora |  |
| 1845 |  |  [Understanding Optimization in Deep Learning with Central Flows](https://openreview.net/forum?id=sIE2rI3ZPs) |  | 0 | Optimization in deep learning remains poorly understood. A key difficulty is that optimizers exhibit complex oscillatory dynamics, referred to as "edge of stability," which cannot be captured by traditional optimization theory. In this paper, we show that the path taken by an oscillatory optimizer... | Alex Damian, Ameet Talwalkar, J. Zico Kolter, Jason D. Lee, Jeremy Cohen |  |
| 1846 |  |  [Mini-batch Coresets for Memory-efficient Language Model Training on Data Mixtures](https://openreview.net/forum?id=bAFVlpFQvT) |  | 0 | Training with larger mini-batches improves the convergence rate and can yield superior performance. However, training with large mini-batches becomes prohibitive for Large Language Models (LLMs), due to the large GPU memory requirement. To address this problem, an effective approach is finding... | Baharan Mirzasoleiman, Dang Nguyen, Rathul Anand, Wenhan Yang, Yu Yang |  |
| 1847 |  |  [Spherical Tree-Sliced Wasserstein Distance](https://openreview.net/forum?id=FPQzXME9NK) |  | 0 | Sliced Optimal Transport (OT) simplifies the OT problem in high-dimensional spaces by projecting supports of input measures onto one-dimensional lines, then exploiting the closed-form expression of the univariate OT to reduce the computational burden of OT. Recently, the Tree-Sliced method has been... | Hoang V. Tran, Huyen Trang Pham, MinhKhoi NguyenNhat, Tam Le, Tan Minh Nguyen, Thanh T. Chu |  |
| 1848 |  |  [Repetition Improves Language Model Embeddings](https://openreview.net/forum?id=Ahlrf2HGJR) |  | 0 | Bidirectional models are considered essential for strong text embeddings. Recent approaches to adapt autoregressive language models (LMs) into strong text embedding models have largely had the requirement to modify the LM architecture to be bidirectional. We challenge this premise by introducing... | Aditi Raghunathan, Daniel Fried, Graham Neubig, Jacob Mitchell Springer, Suhas Kotha |  |
| 1849 |  |  [TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking](https://openreview.net/forum?id=VIUisLx8lQ) |  | 0 | Large Language Models (LLMs) have demonstrated strong reasoning capabilities in solving complex problems. However, current approaches primarily enhance reasoning through the elaboration of thoughts while neglecting the diversity of reasoning types. LLMs typically employ deductive reasoning,... | Danqing Wang, Fei Fang, Jianxin Ma, Lei Li |  |
| 1850 |  |  [Positive-Unlabeled Diffusion Models for Preventing Sensitive Data Generation](https://openreview.net/forum?id=jKcZ4hF4s5) |  | 0 | Diffusion models are powerful generative models but often generate sensitive data that are unwanted by users, mainly because the unlabeled training data frequently contain such sensitive data. Since labeling all sensitive data in the large-scale unlabeled training data is impractical, we address... | Atsutoshi Kumagai, Hiroshi Takahashi, Tomoharu Iwata, Tomoya Yamashita, Yuuki Yamanaka |  |
| 1851 |  |  [API Pack: A Massive Multi-Programming Language Dataset for API Call Generation](https://openreview.net/forum?id=f7O3hITh5s) |  | 0 | We introduce API Pack, a massive multi-programming language dataset containing over one million instruction-API calls for improving the API call generation capabilities of large language models. Our evaluation highlights three key findings: First, fine-tuning on API Pack enables open-source models... | Adriana Meza Soria, Rameswar Panda, Wei Sun, Yikang Shen, Zhen Guo |  |
| 1852 |  |  [GNNs Getting ComFy: Community and Feature Similarity Guided Rewiring](https://openreview.net/forum?id=g6v09VxgFw) |  | 0 | Maximizing the spectral gap through graph rewiring has been proposed to enhance the performance of message-passing graph neural networks (GNNs) by addressing over-squashing. However, as we show, minimizing the spectral gap can also improve generalization. To explain this, we analyze how rewiring... | Adarsh Jamadandi, Celia RubioMadrigal, Rebekka Burkholz |  |
| 1853 |  |  [ToddlerDiffusion: Interactive Structured Image Generation with Cascaded Schrödinger Bridge](https://openreview.net/forum?id=Jszf4et48m) |  | 0 | Diffusion models break down the challenging task of generating data from high-dimensional distributions into a series of easier denoising steps. Inspired by this paradigm, we propose a novel approach that extends the diffusion framework into modality space, decomposing the complex task of RGB image... | Eslam Mohamed Bakr, Liangbing Zhao, Matthieu Cord, Mohamed Elhoseiny, Patrick Pérez, Vincent Tao Hu |  |
| 1854 |  |  [MIA-Bench: Towards Better Instruction Following Evaluation of Multimodal LLMs](https://openreview.net/forum?id=7EhS3YBxjY) |  | 0 | Effective evaluation of Multimodal Large Language Models (MLLMs) is essential for understanding their capabilities and limitations. In this paper, we introduce MIA-Bench, a benchmark designed to assess MLLMs’ ability to strictly adhere to complex instructions. Our benchmark comprises a diverse set... | Hanrong Ye, JeanPhilippe Fauconnier, Peter Grasch, Yinfei Yang, Yusu Qian, Zhe Gan |  |
| 1855 |  |  [SGD with memory: fundamental properties and stochastic acceleration](https://openreview.net/forum?id=Qzd4BloAjQ) |  | 0 | An important open problem is the theoretically feasible acceleration of mini-batch SGD-type algorithms on quadratic problems with power-law spectrum. In the non-stochastic setting, the optimal exponent $\xi$ in the loss convergence $L_t\sim C_Lt^{-\xi}$ is double that in plain GD and is achievable... | Dmitry Yarotsky, Maksim Velikanov |  |
| 1856 |  |  [Distance-Based Tree-Sliced Wasserstein Distance](https://openreview.net/forum?id=OiQttMHwce) |  | 0 | To overcome computational challenges of Optimal Transport (OT), several variants of Sliced Wasserstein (SW) has been developed in the literature. These approaches exploit the closed-form expression of the univariate OT by projecting measures onto one-dimensional lines. However, projecting measures... | Hoang V. Tran, Huyen Trang Pham, MinhKhoi NguyenNhat, Tam Le, Tan Minh Nguyen, Thanh T. Chu |  |
| 1857 |  |  [Provable Benefit of Annealed Langevin Monte Carlo for Non-log-concave Sampling](https://openreview.net/forum?id=P6IVIoGRRg) |  | 0 | We consider the outstanding problem of sampling from an unnormalized density that may be non-log-concave and multimodal. To enhance the performance of simple Markov chain Monte Carlo (MCMC) methods, techniques of annealing type have been widely used. However, quantitative theoretical guarantees of... | Molei Tao, Wei Guo, Yongxin Chen |  |
| 1858 |  |  [On Speeding Up Language Model Evaluation](https://openreview.net/forum?id=3cvwO5DBZn) |  | 0 | Developing prompt-based methods with Large Language Models (LLMs) requires making numerous decisions, which give rise to a combinatorial search problem over hyper-parameters. This exhaustive evaluation can be time-consuming and costly. In this paper, we propose an \textit{adaptive} approach to... | Carla P. Gomes, Christian K. Belardi, Jin Peng Zhou, Kilian Q. Weinberger, Ruihan Wu, Travis Zhang, Wen Sun |  |
| 1859 |  |  [Shifting the Paradigm: A Diffeomorphism Between Time Series Data Manifolds for Achieving Shift-Invariancy in Deep Learning](https://openreview.net/forum?id=nibeaHUEJx) |  | 0 | Deep learning models lack shift invariance, making them sensitive to input shifts that cause changes in output. While recent techniques seek to address this for images, our findings show that these approaches fail to provide shift-invariance in time series, where the data generation mechanism is... | Berken Utku Demirel, Christian Holz |  |
| 1860 |  |  [Human-Aligned Chess With a Bit of Search](https://openreview.net/forum?id=bc2H72hGxB) |  | 0 | Chess has long been a testbed for AI's quest to match human intelligence, and in recent years, chess AI systems have surpassed the strongest humans at the game. However, these systems are \*not human-aligned\*; they are unable to match the skill levels of all human partners or model human-like... | Athul Paul Jacob, Daniel Fried, Daphne Ippolito, Vivian Lai, Yiming Zhang |  |
| 1861 |  |  [Dualformer: Controllable Fast and Slow Thinking by Learning with Randomized Reasoning Traces](https://openreview.net/forum?id=bmbRCRiNDu) |  | 0 | In cognition theory, human thinking is governed by two systems: the fast and intuitive System 1 and the slower but more deliberative System 2. Analogously, Large Language Models (LLMs) can operate in two reasoning modes: outputting only the solutions (\emph{fast mode}) or both the reasoning chain... | DiJia Su, Michael Rabbat, Qinqing Zheng, Sainbayar Sukhbaatar, Yuandong Tian |  |
| 1862 |  |  [Autoregressive Pretraining with Mamba in Vision](https://openreview.net/forum?id=PQpvhUrA1C) |  | 0 | The vision community has started to build with the recently developed state space model, Mamba, as the new backbone for a range of tasks. This paper shows that Mamba's visual capability can be significantly enhanced through autoregressive pretraining, a direction not previously explored.... | Alan L. Yuille, Cihang Xie, Fangxun Shu, Feng Wang, Haoqin Tu, Heng Wang, Jieru Mei, Lei Zhang, Linjie Yang, Peng Wang, Sucheng Ren, Xianhang Li |  |
| 1863 |  |  [Task-Adaptive Pretrained Language Models via Clustered-Importance Sampling](https://openreview.net/forum?id=p6ncr0eTKE) |  | 0 | Specialist language models (LMs) focus on a specific task or domain on which they often outperform generalist LMs of the same size. However, the specialist data needed to pretrain these models is only available in limited amount for most tasks. In this work, we build specialist models from large... | David Grangier, Pierre Ablin, Simin Fan, Skyler Seto |  |
| 1864 |  |  [Sparse Autoencoders Do Not Find Canonical Units of Analysis](https://openreview.net/forum?id=9ca9eHNrdH) |  | 0 | A common goal of mechanistic interpretability is to decompose the activations of neural networks into features: interpretable properties of the input computed by the model. Sparse autoencoders (SAEs) are a popular method for finding these features in LLMs, and it has been postulated that they can... | Bart Bussmann, Curt Tigges, Joseph Isaac Bloom, Lee Sharkey, Michael T. Pearce, Neel Nanda, Noura Al Moubayed, Patrick Leask |  |
| 1865 |  |  [BiGR: Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities](https://openreview.net/forum?id=1Z6PSw7OL8) |  | 0 | We introduce BiGR, a novel conditional image generation model using compact binary latent codes for generative training, focusing on enhancing both generation and representation capabilities. BiGR is the first conditional generative model that unifies generation and discrimination within the same... | Bojia Zi, Kai Han, KwanYee K. Wong, Rong Xiao, Shaozhe Hao, Shihao Zhao, Xianbiao Qi, Xuantong Liu |  |
| 1866 |  |  [Training Robust Ensembles Requires Rethinking Lipschitz Continuity](https://openreview.net/forum?id=WKW5TG8ItY) |  | 0 | Transferability of adversarial examples is a well-known property that endangers all classification models, even those that are only accessible through black-box queries. Prior work has shown that an ensemble of models is more resilient to transferability: the probability that an adversarial example... | Ali Ebrahimpour Boroojeny, Hari Sundaram, Varun Chandrasekaran |  |
| 1867 |  |  [Generative Monoculture in Large Language Models](https://openreview.net/forum?id=yZ7sn9pyqb) |  | 0 | We introduce {\em generative monoculture}, a behavior observed in large language models (LLMs) characterized by a significant narrowing of model output diversity relative to available training data for a given task: for example, generating only positive book reviews for books with a mixed... | Emily Black, Fan Wu, Varun Chandrasekaran |  |
| 1868 |  |  [Composable Interventions for Language Models](https://openreview.net/forum?id=tu3qwNjrtw) |  | 0 | Test-time interventions for language models can enhance factual accuracy, mitigate harmful outputs, and improve model efficiency without costly retraining. But despite a flood of new methods, different types of interventions are largely developing independently. In practice, multiple interventions... | Anurag Jayant Vaidya, Arinbjörn Kolbeinsson, Faisal Mahmood, Jonathan Richard Schwarz, Kyle O'Brien, Marinka Zitnik, Shanghua Gao, Shiwei Liu, Thomas Hartvigsen, Tianjin Huang, Tianlong Chen |  |
| 1869 |  |  [MUSE: Machine Unlearning Six-Way Evaluation for Language Models](https://openreview.net/forum?id=TArmA033BU) |  | 0 | Language models (LMs) are trained on vast amounts of text data, which may include private and copyrighted content. Data owners may request the removal of their data from a trained model due to privacy or copyright concerns. However, exactly unlearning only these datapoints (i.e., retraining with... | Ari Holtzman, Chiyuan Zhang, Daogao Liu, Jaechan Lee, Jieyu Zhao, Luke Zettlemoyer, Noah A. Smith, Sadhika Malladi, Weijia Shi, Yangsibo Huang |  |
| 1870 |  |  [Air Quality Prediction with Physics-Guided Dual Neural ODEs in Open Systems](https://openreview.net/forum?id=kOJf7Dklyv) |  | 0 | Air pollution significantly threatens human health and ecosystems, necessitating effective air quality prediction to inform public policy. Traditional approaches are generally categorized into physics-based and data-driven models. Physics-based models usually struggle with high computational... | Aoying Zhou, Bin Yang, Chenjuan Guo, Jindong Tian, Lujia Pan, Peng Chen, Ronghui Xu, Yuxuan Liang, Zhongwen Rao |  |
| 1871 |  |  [A Transfer Attack to Image Watermarks](https://openreview.net/forum?id=UchRjcf4z7) |  | 0 | Watermark has been widely deployed by industry to detect AI-generated images. The robustness of such watermark-based detector against evasion attacks in the white-box and black-box settings is well understood in the literature. However, the robustness in the no-box setting is much less understood.... | Moyang Guo, Neil Zhenqiang Gong, Yuepeng Hu, Zhengyuan Jiang |  |
| 1872 |  |  [Port-Hamiltonian Architectural Bias for Long-Range Propagation in Deep Graph Networks](https://openreview.net/forum?id=03EkqSCKuO) |  | 0 | The dynamics of information diffusion within graphs is a critical open issue that heavily influences graph representation learning, especially when considering long-range propagation. This calls for principled approaches that control and regulate the degree of propagation and dissipation of... | Alessandro Trenta, Alessio Gravina, Claudio Gallicchio, Davide Bacciu, Simon Heilig |  |
| 1873 |  |  [LevAttention: Time, Space and Streaming Efficient Algorithm for Heavy Attentions](https://openreview.net/forum?id=90DC0IvlSs) |  | 0 | A central problem related to transformers can be stated as follows: given two $n \times d$ matrices $Q$ and $K$, and a non-negative function $f$, define the matrix $A$ as follows: (1) apply the function $f$ to each entry of the $n \times n$ matrix $Q K^T$, and then (2) normalize each of the row... | Chiranjib Bhattacharyya, David P. Woodruff, Praneeth Kacham, Ravindran Kannan |  |
| 1874 |  |  [ActSafe: Active Exploration with Safety Constraints for Reinforcement Learning](https://openreview.net/forum?id=aKRADWBJ1I) |  | 0 | Reinforcement learning (RL) is ubiquitous in the development of modern AI systems. However, state-of-the-art RL agents require extensive, and potentially unsafe, interactions with their environments to learn effectively. These limitations confine RL agents to simulated environments, hindering their... | Andreas Krause, Bhavya Sukhija, Carmelo Sferrazza, Lenart Treven, Stelian Coros, Yarden As |  |
| 1875 |  |  [Straight to Zero: Why Linearly Decaying the Learning Rate to Zero Works Best for LLMs](https://openreview.net/forum?id=hrOlBgHsMI) |  | 0 | LLMs are commonly trained with a learning rate (LR) warmup, followed by cosine decay to 10% of the maximum (10x decay). In a large-scale empirical study, we show that under an optimal peak LR, a simple linear decay-to-zero (D2Z) schedule consistently outperforms other schedules when training at... | Daria Soboleva, Gavia Gray, Gurpreet Gosal, Joel Hestness, Nolan Simran Dey, Shane Bergsma |  |
| 1876 |  |  [Adaptive Length Image Tokenization via Recurrent Allocation](https://openreview.net/forum?id=mb2ryuZ3wz) |  | 0 | Current vision systems typically assign fixed-length representations to images, regardless of the information content. This contrasts with human intelligence —and even large language models—which allocate varying representational capacities based on entropy, context and familiarity. Inspired by... | Antonio Torralba, Phillip Isola, Shivam Duggal, William T. Freeman |  |
| 1877 |  |  [Scaling Diffusion Language Models via Adaptation from Autoregressive Models](https://openreview.net/forum?id=j1tSLYKwg8) |  | 0 | Diffusion Language Models (DLMs) have emerged as a promising new paradigm for text generative modeling, potentially addressing limitations of autoregressive (AR) models. However, current DLMs have been studied at a smaller scale compared to their AR counterparts and lack fair comparison on language... | Chenxin An, Hao Peng, Jiacheng Ye, Jiawei Han, Lin Zheng, Lingpeng Kong, Mukai Li, Peilin Zhao, Shansan Gong, Shivam Agarwal, Wei Bi, Yizhe Zhang |  |
| 1878 |  |  [Mixture of Experts Made Personalized: Federated Prompt Learning for Vision-Language Models](https://openreview.net/forum?id=xiDJaTim3P) |  | 0 | Federated prompt learning benefits federated learning with CLIP-like Vision-Language Model's (VLM's) robust representation learning ability through prompt learning. However, current federated prompt learning methods are habitually restricted to the traditional FL paradigm, where the participating... | Chen Chen, Jun Luo, Shandong Wu |  |
| 1879 |  |  [FaithEval: Can Your Language Model Stay Faithful to Context, Even If "The Moon is Made of Marshmallows"](https://openreview.net/forum?id=UeVx6L59fg) |  | 0 | Ensuring faithfulness to context in large language models (LLMs) and retrieval-augmented generation (RAG) systems is crucial for reliable deployment in real-world applications, as incorrect or unsupported information can erode user trust. Despite advancements on standard benchmarks, faithfulness... | Caiming Xiong, Senthil Purushwalkam, Shafiq Joty, Shrey Pandit, XuanPhi Nguyen, Yifei Ming, Zixuan Ke |  |
| 1880 |  |  [GenVP: Generating Visual Puzzles with Contrastive Hierarchical VAEs](https://openreview.net/forum?id=Pd7IOswRUZ) |  | 0 | Raven’s Progressive Matrices (RPMs) is an established benchmark to examine the ability to perform high-level abstract visual reasoning (AVR). Despite the current success of algorithms that solve this task, humans can generalize beyond a given puzzle and create new puzzles given a set of rules,... | Hao Wang, Kalliopi Basioti, Pritish Sahu, Tony Qingze Liu, Vladimir Pavlovic, Zihao Xu |  |
| 1881 |  |  [Computing Circuits Optimization via Model-Based Circuit Genetic Evolution](https://openreview.net/forum?id=KWH4UIoQKS) |  | 0 | Optimizing computing circuits such as multipliers and adders is a fundamental challenge in modern integrated circuit design. Recent efforts propose formulating this optimization problem as a reinforcement learning (RL) proxy task, offering a promising approach to search high-speed and... | Dongsheng Zuo, Feng Wu, Jianye Hao, Jie Wang, Lei Chen, Mingxuan Yuan, Xilin Xia, Yuzhe Ma, Zhihai Wang |  |
| 1882 |  |  [VideoPhy: Evaluating Physical Commonsense for Video Generation](https://openreview.net/forum?id=9D2QvO1uWj) |  | 0 | Recent advances in internet-scale video data pretraining have led to the development of text-to-video generative models that can create high-quality videos across a broad range of visual concepts, synthesize realistic motions and render complex objects. Hence, these generative models have the... | Aditya Grover, Chenfanfu Jiang, Hritik Bansal, KaiWei Chang, Michal Yarom, Tianyi Xie, Yizhou Sun, Yonatan Bitton, Zeshun Zong, Zongyu Lin |  |
| 1883 |  |  [On the Completeness of Invariant Geometric Deep Learning Models](https://openreview.net/forum?id=52x04chyQs) |  | 0 | Invariant models, one important class of geometric deep learning models, are capable of generating meaningful geometric representations by leveraging informative geometric features in point clouds. These models are characterized by their simplicity, good experimental results and computational... | Muhan Zhang, Shijia Kang, Xiyuan Wang, Zian Li |  |
| 1884 |  |  [Multi-Modal and Multi-Attribute Generation of Single Cells with CFGen](https://openreview.net/forum?id=3MnMGLctKb) |  | 0 | Generative modeling of single-cell RNA-seq data is crucial for tasks like trajectory inference, batch effect removal, and simulation of realistic cellular data. However, recent deep generative models simulating synthetic single cells from noise operate on pre-processed continuous gene expression... | Alessandro Palma, Alexander Tong, Andrea Dittadi, Fabian J. Theis, Hanyi Zhang, Manuel Lubetzki, Till Richter |  |
| 1885 |  |  [Reassessing How to Compare and Improve the Calibration of Machine Learning Models](https://openreview.net/forum?id=X0epAjg0hd) |  | 0 | A machine learning model is calibrated if its predicted probability for an outcome matches the observed frequency for that outcome conditional on the model prediction. This property has become increasingly important as the impact of machine learning models has continued to spread to various... | Muthu Chidambaram, Rong Ge |  |
| 1886 |  |  [SSLAM: Enhancing Self-Supervised Models with Audio Mixtures for Polyphonic Soundscapes](https://openreview.net/forum?id=odU59TxdiB) |  | 0 | Self-supervised pre-trained audio networks have seen widespread adoption in real-world systems, particularly in multi-modal large language models. These networks are often employed in a frozen state, under the assumption that the self-supervised pre-training has sufficiently equipped them to handle... | Armin Mustafa, Muhammad Awais, Philip J. B. Jackson, Sara Atito, Tony Alex |  |
| 1887 |  |  [Sharper Guarantees for Learning Neural Network Classifiers with Gradient Methods](https://openreview.net/forum?id=h7GAgbLSmC) |  | 0 | In this paper, we study the data-dependent convergence and generalization behavior of gradient methods for neural networks with smooth activation. Our first result is a novel bound on the excess risk of deep networks trained by the logistic loss via an alogirthmic stability analysis. Compared to... | Arya Mazumdar, Christos Thrampoulidis, Hossein Taheri |  |
| 1888 |  |  [Diversity-Rewarded CFG Distillation](https://openreview.net/forum?id=lJ66m0ibQL) |  | 0 | Generative models are transforming creative domains such as music generation, with inference-time strategies like Classifier-Free Guidance (CFG) playing a crucial role. However, CFG doubles inference cost while limiting originality and diversity across generated contents. In this paper, we... | Alexandre Ramé, Andrea Agostinelli, Geoffrey Cideron, Johan Ferret, Olivier Bachem, Romuald Elie, Sarah Perrin, Sertan Girgin |  |
| 1889 |  |  [Language-Assisted Feature Transformation for Anomaly Detection](https://openreview.net/forum?id=2p03KljxE9) |  | 0 | This paper introduces LAFT, a novel feature transformation method designed to incorporate user knowledge and preferences into anomaly detection using natural language. Accurately modeling the boundary of normality is crucial for distinguishing abnormal data, but this is often challenging due to... | Bryan Dongik Lee, EungGu Yun, Heonjin Ha, Yeongwoo Nam |  |
| 1890 |  |  [LoRA-X: Bridging Foundation Models with Training-Free Cross-Model Adaptation](https://openreview.net/forum?id=6cQ6cBqzV3) |  | 0 | The rising popularity of large foundation models has led to a heightened demand for parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), which offer performance comparable to full model fine-tuning while requiring only a few additional parameters tailored to the specific... | Debasmit Das, Farzad Farhadzadeh, Fatih Porikli, Shubhankar Borse |  |
| 1891 |  |  [Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection](https://openreview.net/forum?id=HE6pJoNnFp) |  | 0 | Large language models (LLMs) augmented with retrieval exhibit robust performance and extensive versatility by incorporating external contexts. However, the input length grows linearly in the number of retrieved documents, causing a dramatic increase in latency. In this paper, we propose a novel... | Bang Liu, Caitlin Sikora, ChuCheng Lin, Ho Ko, JiaChen Gu, Jindong Chen, Lei Meng, Lei Shu, Liangchen Luo, Yinxiao Liu, Yun Zhu |  |
| 1892 |  |  [How efficient is LLM-generated code? A rigorous & high-standard benchmark](https://openreview.net/forum?id=suz4utPr9Y) |  | 0 | The emergence of large language models (LLMs) has significantly pushed the frontiers of program synthesis. Advancement of LLM-based program synthesis calls for a thorough evaluation of LLM-generated code. Most evaluation frameworks focus on the (functional) correctness of generated code;... | Christopher Lott, Hanghang Tong, James Ezick, Ruizhong Qiu, Weiliang Will Zeng |  |
| 1893 |  |  [Topological Zigzag Spaghetti for Diffusion-based Generation and Prediction on Graphs](https://openreview.net/forum?id=mYgoNEsUDi) |  | 0 | Diffusion models have recently emerged as a new powerful machinery for generative artificial intelligence on graphs, with applications ranging from drug design to knowledge discovery. However, despite their high potential, most, if not all, existing graph diffusion models are limited in their... | Yulia R. Gel, Yuzhou Chen |  |
| 1894 |  |  [Model Equality Testing: Which Model is this API Serving?](https://openreview.net/forum?id=QCDdI7X3f9) |  | 0 | Users often interact with large language models through black-box inference APIs, both for closed- and open-weight models (e.g., Llama models are popularly accessed via Amazon Bedrock and Azure AI Studio). In order to cut costs or add functionality, API providers may quantize, watermark, or... | Carlos Guestrin, Irena Gao, Percy Liang |  |
| 1895 |  |  [Learning Structured Representations by Embedding Class Hierarchy with Fast Optimal Transport](https://openreview.net/forum?id=AnL6BuWzxa) |  | 0 | To embed structured knowledge within labels into feature representations, prior work (Zeng et al., 2022) proposed to use the Cophenetic Correlation Coefficient (CPCC) as a regularizer during supervised learning. This regularizer calculates pairwise Euclidean distances of class means and aligns them... | Han Zhao, Makoto Yamada, Siqi Zeng, Sixian Du |  |
| 1896 |  |  [DICE: Data Influence Cascade in Decentralized Learning](https://openreview.net/forum?id=2TIYkqieKw) |  | 0 | Decentralized learning offers a promising approach to crowdsource data consumptions and computational workloads across geographically distributed compute interconnected through peer-to-peer networks, accommodating the exponentially increasing demands. However, proper incentives are still in... | Can Wang, Fengxiang He, Tongtian Zhu, Wenhao Li |  |
| 1897 |  |  [SpikeLLM: Scaling up Spiking Neural Network to Large Language Models via Saliency-based Spiking](https://openreview.net/forum?id=ZadnlOHsHv) |  | 0 | Recent advancements in large language models (LLMs) with billions of parameters have improved performance in various applications, but their inference processes demand significant energy and computational resources. In contrast, the human brain, with approximately 86 billion neurons, is much more... | Boyan Gao, David A. Clifton, Guoqi Li, Jiajun Zhang, Li Du, Shitao Xiao, Wanpeng Zhang, Xingrun Xing, Zheng Liu, Zheng Zhang |  |
| 1898 |  |  [UNSURE: self-supervised learning with Unknown Noise level and Stein's Unbiased Risk Estimate](https://openreview.net/forum?id=ScVnYBaSEw) |  | 0 | Recently, many self-supervised learning methods for image reconstruction have been proposed that can learn from noisy data alone, bypassing the need for ground-truth references. Most existing methods cluster around two classes: i) Stein's Unbiased Risk Estimate (SURE) and similar approaches that... | Julián Tachella, Laurent Jacques, Mike E. Davies |  |
| 1899 |  |  [ALBAR: Adversarial Learning approach to mitigate Biases in Action Recognition](https://openreview.net/forum?id=9KiE3t6CsL) |  | 0 | Bias in machine learning models can lead to unfair decision making, and while it has been well-studied in the image and text domains, it remains underexplored in action recognition. Action recognition models often suffer from background bias (i.e., inferring actions based on background cues) and... | Ishan Rajendrakumar Dave, Joseph Fioresi, Mubarak Shah |  |
| 1900 |  |  [Efficient Exploration and Discriminative World Model Learning with an Object-Centric Abstraction](https://openreview.net/forum?id=hgwGi81ndj) |  | 0 | In the face of difficult exploration problems in reinforcement learning, we study whether giving an agent an object-centric mapping (describing a set of items and their attributes) allow for more efficient learning. We found this problem is best solved hierarchically by modelling items at a higher... | Anthony GXChen, Kenneth Marino, Rob Fergus |  |
| 1901 |  |  [Drama: Mamba-Enabled Model-Based Reinforcement Learning Is Sample and Parameter Efficient](https://openreview.net/forum?id=7XIkRgYjK3) |  | 0 | Model-based reinforcement learning (RL) offers a solution to the data inefficiency that plagues most model-free RL algorithms. However, learning a robust world model often requires complex and deep architectures, which are computationally expensive and challenging to train. Within the world model,... | Ivana Dusparic, Ke Zhang, Vinny Cahill, Wenlong Wang, Yucheng Shi |  |
| 1902 |  |  [PooDLe🐩: Pooled and dense self-supervised learning from naturalistic videos](https://openreview.net/forum?id=dEg5SdGaiq) |  | 0 | Self-supervised learning has driven significant progress in learning from single-subject, _iconic_ images. However, there are still unanswered questions about the use of minimally-curated, naturalistic video data, which contain _dense_ scenes with many independent objects, imbalanced class... | Alex N. Wang, Christopher Hoang, Mengye Ren, Yann LeCun, Yuwen Xiong |  |
| 1903 |  |  [Apollo-MILP: An Alternating Prediction-Correction Neural Solving Framework for Mixed-Integer Linear Programming](https://openreview.net/forum?id=mFY0tPDWK8) |  | 0 | Leveraging machine learning (ML) to predict an initial solution for mixed-integer linear programming (MILP) has gained considerable popularity in recent years. These methods predict a solution and fix a subset of variables to reduce the problem dimension. Then, they solve the reduced problem to... | Fangzhou Zhu, Feng Wu, Haoyang Liu, Jianye Hao, Jie Wang, Xijun Li, Yuxuan Zong, Zijie Geng |  |
| 1904 |  |  [Theory, Analysis, and Best Practices for Sigmoid Self-Attention](https://openreview.net/forum?id=Zhdhg6n2OG) |  | 0 | Attention is a key part of the transformer architecture. It is a sequence-to-sequence mapping that transforms each sequence element into a weighted sum of values. The weights are typically obtained as the softmax of dot products between keys and queries. Recent work has explored alternatives to... | Amitis Shidani, Dan Busbridge, Eeshan Gunesh Dhekane, Federico Danieli, Floris Weers, Jagrit Digani, Jason Ramapuram, Pierre Ablin, Russell Webb, Tatiana Likhomanenko, Zijin Gu |  |
| 1905 |  |  [Small Models are LLM Knowledge Triggers for Medical Tabular Prediction](https://openreview.net/forum?id=WoPovNkM5h) |  | 0 | Recent development in large language models (LLMs) has demonstrated impressive domain proficiency on unstructured textual or multi-modal tasks. However, despite with intrinsic world knowledge, their application on structured tabular data prediction still lags behind, primarily due to the numerical... | Bo Zheng, Chaowen Hu, Jiahuan Yan, Jian Wu, Jimeng Sun, Jintai Chen, Yaojun Hu |  |
| 1906 |  |  [Efficient Model Editing with Task-Localized Sparse Fine-tuning](https://openreview.net/forum?id=TDyE2iuvyc) |  | 0 | Task arithmetic has emerged as a promising approach for editing models by representing task-specific knowledge as composable task vectors. However, existing methods rely on network linearization to derive task vectors, leading to computational bottlenecks during training and inference. Moreover,... | Leonardo Iurada, Marco Ciccone, Tatiana Tommasi |  |
| 1907 |  |  [Shot2Story: A New Benchmark for Comprehensive Understanding of Multi-shot Videos](https://openreview.net/forum?id=FZv3kPHTtB) |  | 0 | A short clip of video may contain progression of multiple events and an interesting story line. A human need to capture both the event in every shot and associate them together to understand the story behind it. In this work, we present a new multi-shot video understanding benchmark \dataset with... | Heng Wang, Lina Yao, Linjie Yang, Mingfei Han, Xiaojun Chang |  |
| 1908 |  |  [HGM³: Hierarchical Generative Masked Motion Modeling with Hard Token Mining](https://openreview.net/forum?id=IEul1M5pyk) |  | 0 | Text-to-motion generation has significant potential in a wide range of applications including animation, robotics, and AR/VR. While recent works on masked motion models are promising, the task remains challenging due to the inherent ambiguity in text and the complexity of human motion dynamics. To... | Jaejin Lee, Minjae Jeong, Sungyoon Jung, Won Hwa Kim, Yechan Hwang |  |
| 1909 |  |  [Improving Large Language Model Planning with Action Sequence Similarity](https://openreview.net/forum?id=tpGkEgxMJT) |  | 0 | Planning is essential for artificial intelligence systems to look ahead and proactively determine a course of actions to reach objectives in the virtual and real world. Recent work on large language models (LLMs) sheds light on their planning capability in various tasks. However, it remains unclear... | Azade Nova, Bernd Bohnet, Dale Schuurmans, Hanie Sedghi, Xinran Zhao |  |
| 1910 |  |  [Tackling Data Corruption in Offline Reinforcement Learning via Sequence Modeling](https://openreview.net/forum?id=phAlw3JPms) |  | 0 | Learning policy from offline datasets through offline reinforcement learning (RL) holds promise for scaling data-driven decision-making while avoiding unsafe and costly online interactions. However, real-world data collected from sensors or humans often contains noise and errors, posing a... | Baoxiang Wang, Feng Luo, Jiawei Xu, Lei Han, Meng Fang, Rui Yang, Shuang Qiu |  |
| 1911 |  |  [VoxDialogue: Can Spoken Dialogue Systems Understand Information Beyond Words?](https://openreview.net/forum?id=vbmSSIhKAM) |  | 0 | With the rapid advancement of large models, voice assistants are gradually acquiring the ability to engage in open-ended daily conversations with humans. However, current spoken dialogue systems often overlook multi-modal information in audio beyond text, such as speech rate, volume, emphasis, and... | Boyang Zhang, Dongjie Fu, Jingyu Lu, Rongjie Huang, Ruofan Hu, Shengpeng Ji, Tao Jin, Xiaoda Yang, Xize Cheng, Zehan Wang, Zhou Zhao |  |
| 1912 |  |  [PQMass: Probabilistic Assessment of the Quality of Generative Models using Probability Mass Estimation](https://openreview.net/forum?id=n7qGCmluZr) |  | 0 | We propose a likelihood-free method for comparing two distributions given samples from each, with the goal of assessing the quality of generative models. The proposed approach, PQMass, provides a statistically rigorous method for assessing the performance of a single generative model or the... | Connor Stone, Laurence Perreault Levasseur, Nikolay Malkin, Pablo Lemos, Salma Salhi, Sammy Nasser Sharief, Yashar Hezaveh |  |
| 1913 |  |  [OpenHands: An Open Platform for AI Software Developers as Generalist Agents](https://openreview.net/forum?id=OJd3ayDDoF) |  | 0 | Software is one of the most powerful tools that we humans have at our disposal; it allows a skilled programmer to interact with the world in complex and profound ways. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that... | Bill Qian, Binyuan Hui, Bowen Li, Boxuan Li, Frank F. Xu, Fuqiang Li, Hoang H. Tran, Jaskirat Singh, Jiayi Pan, Junyang Lin, Mingchen Zhuge, Mingzhang Zheng, Niklas Muennighoff, Ren Ma, Xiangru Tang, Xingyao Wang, Yanjun Shao, Yizhe Zhang, Yueqi Song, Yufan Song, et al. |  |
| 1914 |  |  [Lines of Thought in Large Language Models](https://openreview.net/forum?id=zjAEa4s3sH) |  | 0 | Large Language Models achieve next-token prediction by transporting a vectorized piece of text (prompt) across an accompanying embedding space under the action of successive transformer layers. The resulting high-dimensional trajectories realize different contextualization, or 'thinking', steps,... | Christopher J. Earls, Nicolas Boullé, Raphaël Sarfati, Toni J. B. Liu |  |
| 1915 |  |  [Knowledge Graph Finetuning Enhances Knowledge Manipulation in Large Language Models](https://openreview.net/forum?id=oMFOKjwaRS) |  | 0 | Despite the impressive performance of general large language models(LLMs), many of their applications in specific domains (e.g., low-data and knowledge-intensive) still confront significant challenges. Supervised fine-tuning (SFT)---where a general LLM is further trained on a small labeled dataset... | Feng Wu, Hanzhu Chen, Jie Wang, Jieping Ye, Junjie He, Qitan Lv, Rong Wu, Xu Shen, Zehao Wang |  |
| 1916 |  |  [Vevo: Controllable Zero-Shot Voice Imitation with Self-Supervised Disentanglement](https://openreview.net/forum?id=anQDiQZhDP) |  | 0 | The imitation of voice, targeted on specific speech attributes such as timbre and speaking style, is crucial in speech generation. However, existing methods rely heavily on annotated data, and struggle with effectively disentangling timbre and style, leading to challenges in achieving controllable... | Dangna Li, Jeff Hwang, Julian Chan, Kainan Peng, Mingbo Ma, Vimal Manohar, Xiaohui Zhang, Xueyao Zhang, Yingru Liu, Yuan Huang, Yuhao Wang, Zhenyu Tang, Zhizheng Wu |  |
| 1917 |  |  [Designing Mechanical Meta-Materials by Learning Equivariant Flows](https://openreview.net/forum?id=VMurwgAFWP) |  | 0 | Mechanical meta-materials are solids whose geometric structure results in exotic nonlinear behaviors that are not typically achievable via homogeneous materials. We show how to drastically expand the design space of a class of mechanical meta-materials known as $\textit{cellular solids}$, by... | Anne S. Meeussen, Katia Bertoldi, Mehran Mirramezani, Peter Orbanz, Ryan P. Adams |  |
| 1918 |  |  [Simulating Training Dynamics to Reconstruct Training Data from Deep Neural Networks](https://openreview.net/forum?id=ZJftXKy12x) |  | 0 | Whether deep neural networks (DNNs) memorize the training data is a fundamental open question in understanding deep learning. A direct way to verify the memorization of DNNs is to reconstruct training data from DNNs’ parameters. Since parameters are gradually determined by data throughout training,... | Hanling Tian, Mingzhen He, Ruikai Yang, Xiaolin Huang, Yuhang Liu, Zhehao Huang, Zhengbao He |  |
| 1919 |  |  [XLand-100B: A Large-Scale Multi-Task Dataset for In-Context Reinforcement Learning](https://openreview.net/forum?id=p9OsTj0nMP) |  | 0 | Following the success of the in-context learning paradigm in large-scale language and computer vision models, the recently emerging field of in-context reinforcement learning is experiencing a rapid growth. However, its development has been held back by the lack of challenging benchmarks, as all... | Alexander Nikulin, Alexey Zemtsov, Ilya Zisman, Vladislav Kurenkov |  |
| 1920 |  |  [How Far Are We from True Unlearnability?](https://openreview.net/forum?id=I4Lq2RJ0eJ) |  | 0 | High-quality data plays an indispensable role in the era of large models, but the use of unauthorized data for model training greatly damages the interests of data owners. To overcome this threat, several unlearnable methods have been proposed, which generate unlearnable examples (UEs) by... | Chenxiong Qian, Kai Ye, Liangcai Su |  |
| 1921 |  |  [Bridging the Gap between Database Search and De Novo Peptide Sequencing with SearchNovo](https://openreview.net/forum?id=SjMtxqdQ73) |  | 0 | Accurate protein identification from mass spectrometry (MS) data is fundamental to unraveling the complex roles of proteins in biological systems, with peptide sequencing being a pivotal step in this process. The two main paradigms for peptide sequencing are database search, which matches... | Hongxin Xiang, Jingbo Zhou, Jun Xia, Shaorong Chen, Sizhe Liu, Stan Z. Li, Yue Liu, Zicheng Liu |  |
| 1922 |  |  [Rethinking Invariance Regularization in Adversarial Training to Improve Robustness-Accuracy Trade-off](https://openreview.net/forum?id=M9SKazbVkJ) |  | 0 | Adversarial training often suffers from a robustness-accuracy trade-off, where achieving high robustness comes at the cost of accuracy. One approach to mitigate this trade-off is leveraging invariance regularization, which encourages model invariance under adversarial perturbations; however, it... | ChingChun Chang, Futa Kai Waseda, Isao Echizen |  |
| 1923 |  |  [BANGS: Game-theoretic Node Selection for Graph Self-Training](https://openreview.net/forum?id=h51mpl8Tyx) |  | 0 | Graph self-training is a semi-supervised learning method that iteratively selects a set of unlabeled data to retrain the underlying graph neural network (GNN) model and improve its prediction performance. While selecting highly confident nodes has proven effective for self-training, this... | Fangxin Wang, Kay Liu, Philip S. Yu, Sourav Medya |  |
| 1924 |  |  [Holistic Reasoning with Long-Context LMs: A Benchmark for Database Operations on Massive Textual Data](https://openreview.net/forum?id=5LXcoDtNyq) |  | 0 | The rapid increase in textual information means we need more efficient methods to sift through, organize, and understand it all. While retrieval-augmented generation (RAG) models excel in accessing information from large document collections, they struggle with complex tasks that require... | Hayate Iso, Nikita Bhutani, Seiji Maekawa |  |
| 1925 |  |  [Multi-Dimensional Conformal Prediction](https://openreview.net/forum?id=loDppyW7e2) |  | 0 | Conformal prediction has attracted significant attention as a distribution-free method for uncertainty quantification in black-box models, providing prediction sets with guaranteed coverage. However, its practical utility is often limited when these prediction sets become excessively large,... | Bracha LauferGoldshtein, Yam Tawachi |  |
| 1926 |  |  [Generating Likely Counterfactuals Using Sum-Product Networks](https://openreview.net/forum?id=rGyi8NNqB0) |  | 0 | The need to explain decisions made by AI systems is driven by both recent regulation and user demand. The decisions are often explainable only post hoc. In counterfactual explanations, one may ask what constitutes the best counterfactual explanation. Clearly, multiple criteria must be taken into... | Jakub Marecek, Jiri Nemecek, Tomás Pevný |  |
| 1927 |  |  [Overcoming Lower-Level Constraints in Bilevel Optimization: A Novel Approach with Regularized Gap Functions](https://openreview.net/forum?id=cyPMEXdqQ2) |  | 0 | Constrained bilevel optimization tackles nested structures present in constrained learning tasks like constrained meta-learning, adversarial learning, and distributed bilevel optimization. However, existing bilevel optimization methods mostly are typically restricted to specific constraint... | Haian Yin, Jin Zhang, Shangzhi Zeng, Wei Yao |  |
| 1928 |  |  [Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization](https://openreview.net/forum?id=uaMSBJDnRv) |  | 0 | Direct Preference Optimization (DPO) and its variants are increasingly used for aligning language models with human preferences. Although these methods are designed to teach a model to generate preferred responses more frequently relative to dispreferred responses, prior work has observed that the... | Adithya Bhaskar, Boris Hanin, Danqi Chen, Noam Razin, Sadhika Malladi, Sanjeev Arora |  |
| 1929 |  |  [Newton Meets Marchenko-Pastur: Massively Parallel Second-Order Optimization with Hessian Sketching and Debiasing](https://openreview.net/forum?id=Ty6TCjKNSF) |  | 0 | Motivated by recent advances in serverless cloud computing, in particular the \`\`function as a service'' (FaaS) model, we consider the problem of minimizing a convex function in a massively parallel fashion, where communication between workers is limited. Focusing on the case of a... | Elad Romanov, Fangzhao Zhang, Mert Pilanci |  |
| 1930 |  |  [SIMPL: Scalable and hassle-free optimisation of neural representations from behaviour](https://openreview.net/forum?id=9kFaNwX6rv) |  | 0 | Neural activity in the brain is known to encode low-dimensional, time-evolving, behaviour-related variables. A long-standing goal of neural data analysis has been to identify these variables and their mapping to neural activity. A productive and canonical approach has been to simply visualise... | Caswell Barry, Claudia Clopath, Kim Stachenfeld, Pierre Glaser, Tom M. George |  |
| 1931 |  |  [Diffusion-based Decoupled Deterministic and Uncertain Framework for Probabilistic Multivariate Time Series Forecasting](https://openreview.net/forum?id=HdUkF1Qk7g) |  | 0 | Diffusion-based denoising models have demonstrated impressive performance in probabilistic forecasting for multivariate time series (MTS). Nonetheless, existing approaches often model the entire data distribution, neglecting the variability in uncertainty across different components of the time... | Lei Yao, Qi Li, Tianyi Zhong, Yong Zhang, Zhaoxia Li, Zhenyu Zhang |  |
| 1932 |  |  [AugKD: Ingenious Augmentations Empower Knowledge Distillation for Image Super-Resolution](https://openreview.net/forum?id=AC3713Fmhx) |  | 0 | Knowledge distillation (KD) compresses deep neural networks by transferring task-related knowledge from cumbersome pre-trained teacher models to more compact student models. However, vanilla KD for image super-resolution (SR) networks yields only limited improvements due to the inherent nature of... | Bingyi Jing, Hanting Chen, Jie Hu, Shaohui Lin, Simiao Li, Wei Li, Wenjia Wang, Yun Zhang, Zhijun Tu |  |
| 1933 |  |  [qNBO: quasi-Newton Meets Bilevel Optimization](https://openreview.net/forum?id=BTOdzCzSRg) |  | 0 | Bilevel optimization, which addresses challenges in hierarchical learning tasks, has gained significant interest in machine learning. Implementing gradient descent for bilevel optimization presents computational hurdles, notably the need to compute the exact lower-level solution and the inverse... | Chengming Yu, Jin Zhang, Sheng Fang, Wei Yao, Yongjin Liu |  |
| 1934 |  |  [Deconstructing Denoising Diffusion Models for Self-Supervised Learning](https://openreview.net/forum?id=9oMB6wnFYM) |  | 0 | In this study, we examine the representation learning abilities of Denoising Diffusion Models (DDM) that were originally purposed for image generation. Our philosophy is to deconstruct a DDM, gradually transforming it into a classical Denoising Autoencoder (DAE). This deconstructive process allows... | Kaiming He, Saining Xie, Xinlei Chen, Zhuang Liu |  |
| 1935 |  |  [W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models](https://openreview.net/forum?id=YkmbJSHjj7) |  | 0 | The demand for efficient natural language processing (NLP) systems has led to the development of lightweight language models. Previous work in this area has primarily focused on manual design or training-based neural architecture search (NAS) methods. Recently, zero-shot NAS methods have been... | Shang Wang |  |
| 1936 |  |  [Graph Neural Networks for Edge Signals: Orientation Equivariance and Invariance](https://openreview.net/forum?id=XWBE90OYlH) |  | 0 | Many applications in traffic, civil engineering, or electrical engineering revolve around edge-level signals. Such signals can be categorized as inherently directed, for example, the water flow in a pipe network, and undirected, like the diameter of a pipe. Topological methods model edge signals... | Dominik Fuchsgruber, Simon Geisler, Stephan Günnemann, Tim Postuvan |  |
| 1937 |  |  [Neural Functions for Learning Periodic Signal](https://openreview.net/forum?id=GCH5leffZp) |  | 0 | As function approximators, deep neural networks have served as an effective tool to represent various signal types. Recent approaches utilize multi-layer perceptrons (MLPs) to learn a nonlinear mapping from a coordinate to its corresponding signal, facilitating the learning of continuous neural... | Kookjin Lee, Minju Jo, Noseong Park, Woojin Cho |  |
| 1938 |  |  [Language Guided Skill Discovery](https://openreview.net/forum?id=i3e92uSZCp) |  | 0 | Skill discovery methods enable agents to learn diverse emergent behaviors without explicit rewards. To make learned skills useful for downstream tasks, obtaining a semantically diverse repertoire of skills is crucial. While some approaches use discriminators to acquire distinguishable skills and... | Laura Smith, Sehoon Ha, Sergey Levine, Seungeun Rho, Tianyu Li, Xue Bin Peng |  |
| 1939 |  |  [A Differentiable Rank-Based Objective for Better Feature Learning](https://openreview.net/forum?id=KiN7g8mf9N) |  | 0 | In this paper, we leverage existing statistical methods to better understand feature learning from data. We tackle this by modifying the model-free variable selection method, Feature Ordering by Conditional Independence (FOCI), which is introduced in Azadkia & Chatterjee (2021). While FOCI is based... | Giulio Biroli, Krunoslav Lehman Pavasovic, Levent Sagun |  |
| 1940 |  |  [RESfM: Robust Deep Equivariant Structure from Motion](https://openreview.net/forum?id=wldwEhQ7cl) |  | 0 | Multiview Structure from Motion is a fundamental and challenging computer vision problem. A recent deep-based approach utilized matrix equivariant architectures for simultaneous recovery of camera pose and 3D scene structure from large image collections. That work, however, made the unrealistic... | Dror Moran, Fadi Khatib, Meirav Galun, Ronen Basri, Yoni Kasten |  |
| 1941 |  |  [TweedieMix: Improving Multi-Concept Fusion for Diffusion-based Image/Video Generation](https://openreview.net/forum?id=ee2c4MEx9l) |  | 0 | Despite significant advancements in customizing text-to-image and video generation models, generating images and videos that effectively integrate multiple personalized concepts remains challenging. To address this, we present TweedieMix, a novel method for composing customized diffusion models... | Gihyun Kwon, Jong Chul Ye |  |
| 1942 |  |  [Filtered not Mixed: Filtering-Based Online Gating for Mixture of Large Language Models](https://openreview.net/forum?id=ecIvumCyAj) |  | 0 | We propose MoE-F — a formalized mechanism for combining N pre-trained expert Large Language Models (LLMs) in online time-series prediction tasks by adaptively forecasting the best weighting of LLM predictions at every time step. Our mechanism leverages the conditional information in each expert's... | Anastasis Kratsios, Blanka Horvath, Florian Krach, Frank Rudzicz, Raeid Saqur, Yannick Limmer |  |
| 1943 |  |  [LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances Model Merging](https://openreview.net/forum?id=J5sUOvlLbQ) |  | 0 | Fine-tuning pre-trained models has become the standard approach to endow them with specialized knowledge, but it poses fundamental challenges. In particular, (i) fine-tuning often leads to catastrophic forgetting, where improvements on a target domain degrade generalization on other tasks, and (ii)... | Alessandro Favero, François Fleuret, Guillermo OrtizJiménez, Ke Wang, Nikolaos Dimitriadis, Pascal Frossard |  |
| 1944 |  |  [Training-Free Message Passing for Learning on Hypergraphs](https://openreview.net/forum?id=4AuyYxt7A2) |  | 0 | Hypergraphs are crucial for modelling higher-order interactions in real-world data. Hypergraph neural networks (HNNs) effectively utilise these structures by message passing to generate informative node features for various downstream tasks like node classification. However, the message passing... | Bohan Tang, Keyue Jiang, Siheng Chen, Xiaowen Dong, Zexi Liu |  |
| 1945 |  |  [POGEMA: A Benchmark Platform for Cooperative Multi-Agent Pathfinding](https://openreview.net/forum?id=6VgwE2tCRm) |  | 0 | Multi-agent reinforcement learning (MARL) has recently excelled in solving challenging cooperative and competitive multi-agent problems in various environments, typically involving a small number of agents and full observability. Moreover, a range of crucial robotics-related tasks, such as... | Aleksandr Panov, Alexander Chernyavskiy, Alexey Skrynnik, Anatolii Borzilov, Anton Andreychuk, Konstantin S. Yakovlev |  |
| 1946 |  |  [TD-Paint: Faster Diffusion Inpainting Through Time-Aware Pixel Conditioning](https://openreview.net/forum?id=erWwBoR59l) |  | 0 | Diffusion models have emerged as highly effective techniques for inpainting, however, they remain constrained by slow sampling rates. While recent advances have enhanced generation quality, they have also increased sampling time, thereby limiting scalability in real-world applications. We... | Clément Chatelain, Eric Granger, Pourya Shamsolmoali, Romain Hérault, Simon Bernard, Tsiry Mayet |  |
| 1947 |  |  [Class Distribution-induced Attention Map for Open-vocabulary Semantic Segmentations](https://openreview.net/forum?id=CMqOfvD3tO) |  | 0 | Open-vocabulary semantic segmentation is a challenging task that assigns seen or unseen class labels to individual pixels. While recent works with vision-language models (VLMs) have shown promising results in zero-shot semantic segmentation, they still struggle to accurately localize class-related... | Dong Un Kang, Hayeon Kim, Se Young Chun |  |
| 1948 |  |  [Weak to Strong Generalization for Large Language Models with Multi-capabilities](https://openreview.net/forum?id=N1vYivuSKq) |  | 0 | As large language models (LLMs) grow in sophistication, some of their capabilities surpass human abilities, making it essential to ensure their alignment with human values and intentions, i.e., Superalignment. This superalignment challenge is particularly critical for complex tasks, as annotations... | Jianbing Shen, Yu Cheng, Yucheng Zhou |  |
| 1949 |  |  [CHiP: Cross-modal Hierarchical Direct Preference Optimization for Multimodal LLMs](https://openreview.net/forum?id=7lpDn2MhM2) |  | 0 | Multimodal Large Language Models (MLLMs) still struggle with hallucinations despite their impressive capabilities. Recent studies have attempted to mitigate this by applying Direct Preference Optimization (DPO) to multimodal scenarios using preference pairs from text-based responses. However, our... | Bryan Hooi, Hao Fei, Jinlan Fu, SeeKiong Ng, Shenzhen Huangfu, Xiaoyu Shen, Xipeng Qiu |  |
| 1950 |  |  [Concept Pinpoint Eraser for Text-to-image Diffusion Models via Residual Attention Gate](https://openreview.net/forum?id=ZRDhBwKs7l) |  | 0 | Remarkable progress in text-to-image diffusion models has brought a major concern about potentially generating images on inappropriate or trademarked concepts. Concept erasing has been investigated with the goals of deleting target concepts in diffusion models while preserving other concepts with... | Byung Hyun Lee, Dong Un Kang, Se Young Chun, Seunggyu Lee, Sungjin Lim |  |
| 1951 |  |  [Graph-Guided Scene Reconstruction from Images with 3D Gaussian Splatting](https://openreview.net/forum?id=56vHbnk35S) |  | 0 | This paper investigates an open research challenge of reconstructing high-quality, large-scale 3D open scenes from images. It is observed existing methods have various limitations, such as requiring precise camera poses for input and dense viewpoints for supervision. To perform effective and... | Chong Cheng, Gangjian Zhang, Gaochao Song, Hao Wang, Qinzheng Zhou, Yiyang Yao |  |
| 1952 |  |  [Residual Kernel Policy Network: Enhancing Stability and Robustness in RKHS-Based Reinforcement Learning](https://openreview.net/forum?id=2vgcDW2blS) |  | 0 | Achieving optimal performance in reinforcement learning requires robust policies supported by training processes that ensure both sample efficiency and stability. Modeling the policy in reproducing kernel Hilbert space (RKHS) enables efficient exploration of local optimal solutions. However, the... | Huaze Tang, Huijing Lin, Wenbo Ding, Yixian Zhang |  |
| 1953 |  |  [Speech Robust Bench: A Robustness Benchmark For Speech Recognition](https://openreview.net/forum?id=D0LuQNZfEl) |  | 0 | As Automatic Speech Recognition (ASR) models become ever more pervasive, it is important to ensure that they make reliable predictions under corruptions present in the physical and digital world. We propose Speech Robust Bench (SRB), a comprehensive benchmark for evaluating the robustness of ASR... | Bhiksha Raj, David Solans Noguero, Mikko A. Heikkilä, Muhammad A. Shah, Nicolas Kourtellis |  |
| 1954 |  |  [Learning High-Degree Parities: The Crucial Role of the Initialization](https://openreview.net/forum?id=OuNIWgGGif) |  | 0 | Parities have become a standard benchmark for evaluating learning algorithms. Recent works show that regular neural networks trained by gradient descent can efficiently learn degree $k$ parities on uniform inputs for constant $k$, but fail to do so when $k$ and $d-k$ grow with $d$ (here $d$ is the... | Donald KougangYombi, Elisabetta Cornacchia, Emmanuel Abbe, Jan Hazla |  |
| 1955 |  |  [Debiasing Federated Learning with Correlated Client Participation](https://openreview.net/forum?id=9h45qxXEx0) |  | 0 | In cross-device federated learning (FL) with millions of mobile clients, only a small subset of clients participate in training in every communication round, and Federated Averaging (FedAvg) is the most popular algorithm in practice. Existing analyses of FedAvg usually assume the participating... | Ermin Wei, Gauri Joshi, Pranay Sharma, Zheng Xu, Zhenyu Sun, Ziyang Zhang |  |
| 1956 |  |  [Residual Connections and Normalization Can Provably Prevent Oversmoothing in GNNs](https://openreview.net/forum?id=i8vPRlsrYu) |  | 0 | Residual connections and normalization layers have become standard design choices for graph neural networks (GNNs), and were proposed as solutions to the mitigate the oversmoothing problem in GNNs. However, how exactly these methods help alleviate the oversmoothing problem from a theoretical... | Ali Jadbabaie, Michael Scholkemper, Michael T. Schaub, Xinyi Wu |  |
| 1957 |  |  [SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs](https://openreview.net/forum?id=lqHv6dxBkj) |  | 0 | We propose SLoPe, a Double-Pruned \*\*S\*\*parse Plus \*\*L\*\*azy L\*\*o\*\*w-rank Adapter \*\*P\*\*r\*\*e\*\*training method for LLMs that improves the accuracy of sparse LLMs while accelerating their pretraining and inference and reducing their memory footprint. Sparse pretraining of LLMs... | Amir Yazdanbakhsh, Maryam Mehri Dehnavi, Mohammad Mozaffari, Zhao Zhang |  |
| 1958 |  |  [DSBench: How Far Are Data Science Agents from Becoming Data Science Experts?](https://openreview.net/forum?id=DSsSPr0RZJ) |  | 0 | Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have demonstrated impressive language/vision reasoning abilities, igniting the recent trend of building agents for targeted applications such as shopping assistants or AI software engineers. Recently, many data science benchmarks... | Dong Yu, Hongming Zhang, Kaixin Ma, Liqiang Jing, Wenhao Yu, Wenlin Yao, Xiaoyang Wang, Xinya Du, Zhehui Huang |  |
| 1959 |  |  [Bringing NeRFs to the Latent Space: Inverse Graphics Autoencoder](https://openreview.net/forum?id=LTDtjrv02Y) |  | 0 | While pre-trained image autoencoders are increasingly utilized in computer vision, the application of inverse graphics in 2D latent spaces has been under-explored. Yet, besides reducing the training and rendering complexity, applying inverse graphics in the latent space enables a valuable... | Andrew I. Comport, Antoine Schnepf, Flavian Vasile, JeanYves Franceschi, Jérémie Mary, Karim Kassab, Laurent Caraffa, Valérie GouetBrunet |  |
| 1960 |  |  [What Are Good Positional Encodings for Directed Graphs?](https://openreview.net/forum?id=s4Wm71LFK4) |  | 0 | Positional encodings (PEs) are essential for building powerful and expressive graph neural networks and graph transformers, as they effectively capture the relative spatial relationships between nodes. Although extensive research has been devoted to PEs in undirected graphs, PEs for directed graphs... | Haoyu Peter Wang, Pan Li, Yinan Huang |  |
| 1961 |  |  [ASTrA: Adversarial Self-supervised Training with Adaptive-Attacks](https://openreview.net/forum?id=ZbkqhKbggH) |  | 0 | Existing self-supervised adversarial training (self-AT) methods rely on hand-crafted adversarial attack strategies for PGD attacks, which fail to adapt to the evolving learning dynamics of the model and do not account for instance-specific characteristics of images. This results in sub-optimal... | Gautam Vashishtha, Marcus Liwicki, Mubarak Shah, Prakash Chandra Chhipa, Rajkumar Saini, Settur Jithamanyu |  |
| 1962 |  |  [GUI-World: A Video Benchmark and Dataset for Multimodal GUI-oriented Understanding](https://openreview.net/forum?id=QarKTT5brZ) |  | 0 | Recently, Multimodal Large Language Models (MLLMs) have been used as agents to control keyboard and mouse inputs by directly perceiving the Graphical User Interface (GUI) and generating corresponding commands. However, current agents primarily demonstrate strong understanding capabilities in static... | Chenlong Wang, Chujie Gao, Dongping Chen, Huichi Zhou, Jianfeng Gao, Jingyu Tang, Lichao Sun, Liuyi Chen, Pan Zhou, Qihui Zhang, Siyuan Wu, Tianshuo Zhou, Yao Wan, Yi Gui, Yilin Bai, Yiqiang Li, Yue Huang, Yue Yu, Zhen Li, Zhigang He |  |
| 1963 |  |  [Effective and Efficient Time-Varying Counterfactual Prediction with State-Space Models](https://openreview.net/forum?id=yheQRc5xWB) |  | 0 | Time-varying counterfactual prediction (TCP) from observational data supports the answer of when and how to assign multiple sequential treatments, yielding importance in various applications. Despite the progress achieved by recent advances, e.g., LSTM or Transformer based causal approaches, their... | Hao Zou, Haoang Chi, Haotian Wang, Haoxuan Li, Long Lan, Wanrong Huang, Wenjing Yang |  |
| 1964 |  |  [Number Cookbook: Number Understanding of Language Models and How to Improve It](https://openreview.net/forum?id=BWS5gVjgeY) |  | 0 | Large language models (LLMs) can solve an increasing number of complex reasoning tasks while making surprising mistakes in basic numerical understanding and processing (such as $9.11 > 9.9$). The latter ability is essential for tackling complex arithmetic and mathematical problems and serves as a... | Haotong Yang, Muhan Zhang, Shijia Kang, Yi Hu, Zhouchen Lin |  |
| 1965 |  |  [Spectral-Refiner: Accurate Fine-Tuning of Spatiotemporal Fourier Neural Operator for Turbulent Flows](https://openreview.net/forum?id=MKP1g8wU0P) |  | 0 | Recent advancements in operator-type neural networks have shown promising results in approximating the solutions of spatiotemporal Partial Differential Equations (PDEs). However, these neural networks often entail considerable training expenses, and may not always achieve the desired accuracy... | Francesco Brarda, Ruipeng Li, Shuhao Cao, Yuanzhe Xi |  |
| 1966 |  |  [Adaptive backtracking line search](https://openreview.net/forum?id=SrGP0RQbYH) |  | 0 | Backtracking line search is foundational in numerical optimization. The basic idea is to adjust the step size of an algorithm by a {\em constant} factor until some chosen criterion (e.g. Armijo, Descent Lemma) is satisfied. We propose a novel way to adjust step sizes, replacing the constant factor... | Ashia C. Wilson, Joao V. Cavalcanti, Laurent Lessard |  |
| 1967 |  |  [Breach By A Thousand Leaks: Unsafe Information Leakage in 'Safe' AI Responses](https://openreview.net/forum?id=8Rov0fjpOL) |  | 0 | Vulnerability of Frontier language models to misuse has prompted the development of safety measures like filters and alignment training seeking to ensure safety through robustness to adversarially crafted prompts. We assert that robustness is fundamentally insufficient for ensuring safety goals due... | David Glukhov, Ilia Shumailov, Nicolas Papernot, Vardan Papyan, Ziwen Han |  |
| 1968 |  |  [Efficient Off-Policy Learning for High-Dimensional Action Spaces](https://openreview.net/forum?id=JDzTI9rKls) |  | 0 | Existing off-policy reinforcement learning algorithms often rely on an explicit state-action-value function representation, which can be problematic in high-dimensional action spaces due to the curse of dimensionality. This reliance results in data inefficiency as maintaining a state-action-value... | Fabian Otto, Gerhard Neumann, Ngo Anh Vien, Philipp Becker |  |
| 1969 |  |  [Causal Concept Graph Models: Beyond Causal Opacity in Deep Learning](https://openreview.net/forum?id=lmKJ1b6PaL) |  | 0 | Causal opacity denotes the difficulty in understanding the "hidden" causal structure underlying the decisions of deep neural network (DNN) models. This leads to the inability to rely on and verify state-of-the-art DNN-based systems, especially in high-stakes scenarios. For this reason,... | Alberto Termine, Gabriele Dominici, Giuseppe Marra, Marc Langheinrich, Martin Gjoreski, Mateo Espinosa Zarlenga, Pietro Barbiero |  |
| 1970 |  |  [Meta Flow Matching: Integrating Vector Fields on the Wasserstein Manifold](https://openreview.net/forum?id=9SYczU3Qgm) |  | 0 | Numerous biological and physical processes can be modeled as systems of interacting entities evolving continuously over time, e.g. the dynamics of communicating cells or physical particles. Learning the dynamics of such systems is essential for predicting the temporal evolution of populations... | Alexander Tong, Brandon Amos, Kirill Neklyudov, Lazar Atanackovic, Leo J. Lee, Mathieu Blanchette, Xi Zhang, Yoshua Bengio |  |
| 1971 |  |  [JetFormer: An autoregressive generative model of raw images and text](https://openreview.net/forum?id=sgAp2qG86e) |  | 0 | Removing modeling constraints and unifying architectures across domains has been a key driver of the recent progress in training large multimodal models. However, most of these models still rely on many separately trained components such as modality-specific encoders and decoders. In this work, we... | Alexander Kolesnikov, André Susano Pinto, Michael Tschannen |  |
| 1972 |  |  [Counterfactual Concept Bottleneck Models](https://openreview.net/forum?id=w7pMjyjsKN) |  | 0 | Current deep learning models are not designed to simultaneously address three fundamental questions: predict class labels to solve a given classification task (the "What?"), simulate changes in the situation to evaluate how this impacts class predictions (the "How?"), and imagine how the scenario... | Francesco Giannini, Gabriele Dominici, Giuseppe Marra, Marc Langheinrich, Martin Gjoreski, Pietro Barbiero |  |
| 1973 |  |  [Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models](https://openreview.net/forum?id=EbxYDBhE3S) |  | 0 | Backdoor unalignment attacks against Large Language Models (LLMs) enable the stealthy compromise of safety alignment using a hidden trigger while evading normal safety auditing. These attacks pose significant threats to the applications of LLMs in the real-world Large Language Model as a Service... | Biao Yi, Sishuo Chen, Tiansheng Huang, Tong Li, Yiming Li, Zheli Liu, Zhixuan Chu |  |
| 1974 |  |  [SigDiffusions: Score-Based Diffusion Models for Time Series via Log-Signature Embeddings](https://openreview.net/forum?id=Y8KK9kjgIK) |  | 0 | Score-based diffusion models have recently emerged as state-of-the-art generative models for a variety of data modalities. Nonetheless, it remains unclear how to adapt these models to generate long multivariate time series. Viewing a time series as the discretisation of an underlying continuous... | Barbora Barancikova, Cristopher Salvi, Zhuoyue Huang |  |
| 1975 |  |  [Graph Neural Ricci Flow: Evolving Feature from a Curvature Perspective](https://openreview.net/forum?id=7b2JrzdLhA) |  | 0 | Differential equations provide a dynamical perspective for understanding and designing graph neural networks (GNNs). By generalizing the discrete Ricci flow (DRF) to attributed graphs, we can leverage a new paradigm for the evolution of node features with the help of curvature. We show that in the... | Bowen Deng, Chuan Chen, Jialong Chen, Zhen Wang, Zibin Zheng |  |
| 1976 |  |  [Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents](https://openreview.net/forum?id=V4y0CpX4hK) |  | 0 | Although LLM-based agents, powered by Large Language Models (LLMs), can use external tools and memory mechanisms to solve complex real-world tasks, they may also introduce critical security vulnerabilities. However, the existing literature does not comprehensively evaluate attacks and defenses... | Chenlu Zhan, Hanrong Zhang, Hongwei Wang, Jingyuan Huang, Kai Mei, Yifei Yao, Yongfeng Zhang, Zhenting Wang |  |
| 1977 |  |  [CofCA: A STEP-WISE Counterfactual Multi-hop QA benchmark](https://openreview.net/forum?id=q2DmkZ1wVe) |  | 0 | While Large Language Models (LLMs) excel in question-answering (QA) tasks, their real reasoning abilities on multiple evidence retrieval and integration on Multi-hop QA tasks remain less explored. Firstly, LLMs sometimes generate answers that rely on internal memory rather than retrieving evidence... | Jian Wu, Linyi Yang, Manabu Okumura, Yue Zhang, Zhen Wang |  |
| 1978 |  |  [Differentiable and Learnable Wireless Simulation with Geometric Transformers](https://openreview.net/forum?id=9TClCDZXeh) |  | 0 | Modelling the propagation of electromagnetic wireless signals is critical for designing modern communication systems. Wireless ray tracing simulators model signal propagation based on the 3D geometry and other scene parameters, but their accuracy is fundamentally limited by underlying modelling... | Arash Behboodi, Johann Brehmer, Markus Peschl, Thomas Hehn, Tribhuvanesh Orekondy |  |
| 1979 |  |  [Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning](https://openreview.net/forum?id=QOfWubPhdS) |  | 0 | Reward shaping is a reinforcement learning technique that addresses the sparse-reward problem by providing frequent, informative feedback. We propose an efficient self-adaptive reward-shaping mechanism that uses success rates derived from historical experiences as shaped rewards. The success rates... | Haozhe Ma, Kuankuan Sima, Thanh Vinh Vo, TzeYun Leong, Zhengding Luo |  |
| 1980 |  |  [Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements](https://openreview.net/forum?id=ERce2rgMQC) |  | 0 | The current paradigm for safety alignment of large language models (LLMs) follows a _one-size-fits-all_ approach: the model refuses to interact with any content deemed unsafe by the model provider. This approach lacks flexibility in the face of varying social norms across cultures and regions. In... | Ahmed Elgohary, Ahmed Magooda, Benjamin Van Durme, Daniel Khashabi, Jingyu Zhang |  |
| 1981 |  |  [YouTube-SL-25: A Large-Scale, Open-Domain Multilingual Sign Language Parallel Corpus](https://openreview.net/forum?id=nFVsK3QLgs) |  | 0 | Even for better-studied sign languages like American Sign Language (ASL), data is the bottleneck for machine learning research. The situation is worse yet for the many other sign languages used by Deaf/Hard of Hearing communities around the world. In this paper, we present YouTube-SL-25, a... | Biao Zhang, Garrett Tanzer |  |
| 1982 |  |  [TFG-Flow: Training-free Guidance in Multimodal Generative Flow](https://openreview.net/forum?id=GK5ni7tIHp) |  | 0 | Given an unconditional generative model and a predictor for a target property (e.g., a classifier), the goal of training-free guidance is to generate samples with desirable target properties without additional training. As a highly efficient technique for steering generative models toward flexible... | Haotian Ye, Haowei Lin, Jianzhu Ma, Shanda Li, Stefano Ermon, Yiming Yang, Yitao Liang |  |
| 1983 |  |  [PhiNets: Brain-inspired Non-contrastive Learning Based on Temporal Prediction Hypothesis](https://openreview.net/forum?id=5tjdRyqnSn) |  | 0 | Predictive coding has been established as a promising neuroscientific theory to describe the mechanism of information processing in the retina or cortex. This theory hypothesises that cortex predicts sensory inputs at various levels of abstraction to minimise prediction errors. Inspired by... | Han Bao, Makoto Yamada, Satoki Ishikawa, Yuki Takezawa |  |
| 1984 |  |  [UniCon: Unidirectional Information Flow for Effective Control of Large-Scale Diffusion Models](https://openreview.net/forum?id=uJqKf24HGN) |  | 0 | We introduce UniCon, a novel architecture designed to enhance control and efficiency in training adapters for large-scale diffusion models. Unlike existing methods that rely on bidirectional interaction between the diffusion model and control adapter, UniCon implements a unidirectional flow from... | Chao Dong, Fanghua Yu, Jinfan Hu, Jinjin Gu, Zheyuan Li |  |
| 1985 |  |  [No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models](https://openreview.net/forum?id=b3CzCCCILJ) |  | 0 | Classifier-free guidance (CFG) has become the standard method for enhancing the quality of conditional diffusion models. However, employing CFG requires either training an unconditional model alongside the main diffusion model or modifying the training procedure by periodically inserting a null... | Manuel Kansy, Otmar Hilliges, Romann M. Weber, Seyedmorteza Sadat |  |
| 1986 |  |  [CarbonSense: A Multimodal Dataset and Baseline for Carbon Flux Modelling](https://openreview.net/forum?id=l8zRnvD95l) |  | 0 | Terrestrial carbon fluxes provide vital information about our biosphere's health and its capacity to absorb anthropogenic CO$_2$ emissions. The importance of predicting carbon fluxes has led to the emerging field of data-driven carbon flux modelling (DDCFM), which uses statistical techniques to... | Christopher Pal, Mats Leon Richter, Matthew Fortier, Oliver Sonnentag |  |
| 1987 |  |  [MuHBoost: Multi-Label Boosting For Practical Longitudinal Human Behavior Modeling](https://openreview.net/forum?id=BAelAyADqn) |  | 0 | Longitudinal human behavior modeling has received increasing attention over the years due to its widespread applications to patient monitoring, dietary and lifestyle recommendations, and just-in-time intervention for at-risk individuals (e.g., problematic drug users and struggling students), to... | Alex Mason, Anika R. Eisenbraun, Bilal Khan, Hau Chan, Kimberly Tyler, Nguyen T. Thach, Patrick Habecker |  |
| 1988 |  |  [Unlocking the Potential of Model Calibration in Federated Learning](https://openreview.net/forum?id=Osr0KZJeTX) |  | 0 | Over the past several years, various federated learning (FL) methodologies have been developed to improve model accuracy, a primary performance metric in machine learning. However, to utilize FL in practical decision-making scenarios, beyond considering accuracy, the trained model must also have a... | Christopher Brinton, DongJun Han, Seyyedali Hosseinalipour, YunWei Chu |  |
| 1989 |  |  [AIMS.au: A Dataset for the Analysis of Modern Slavery Countermeasures in Corporate Statements](https://openreview.net/forum?id=ybfmpJiKXX) |  | 0 | Despite over a decade of legislative efforts to address modern slavery in the supply chains of large corporations, the effectiveness of government oversight remains hampered by the challenge of scrutinizing thousands of statements annually. While Large Language Models (LLMs) can be considered a... | Adriana Eufrosina Bora, Arsène Fansi Tchango, Bruno Rousseau, Kerrie L. Mengersen, Mirko Bronzi, PierreLuc StCharles |  |
| 1990 |  |  [Multi-Scale Fusion for Object Representation](https://openreview.net/forum?id=nobDw4d1k7) |  | 0 | Representing images or videos as object-level feature vectors, rather than pixel-level feature maps, facilitates advanced visual tasks. Object-Centric Learning (OCL) primarily achieves this by reconstructing the input under the guidance of Variational Autoencoder (VAE) intermediate representation... | Joni Pajarinen, Juho Kannala, Rongzhen Zhao, Vivienne Huiling Wang |  |
| 1991 |  |  [Factor Graph-based Interpretable Neural Networks](https://openreview.net/forum?id=10DtLPsdro) |  | 0 | Comprehensible neural network explanations are foundations for a better understanding of decisions, especially when the input data are infused with malicious perturbations. Existing solutions generally mitigate the impact of perturbations through adversarial training, yet they fail to generate... | Feng Xia, Kuanjiu Zhou, Qiang Zhang, Renqiang Luo, Shuo Yu, Xiaodong Li, Yicong Li |  |
| 1992 |  |  [A Graph Enhanced Symbolic Discovery Framework For Efficient Logic Optimization](https://openreview.net/forum?id=EG9nDN3eGB) |  | 0 | The efficiency of Logic Optimization (LO) has become one of the key bottlenecks in chip design. To prompt efficient LO, previous studies propose using a key scoring function to predict and prune a large number of ineffective nodes of the LO heuristics. However, the existing scoring functions... | Feng Wu, Jianye Hao, Jie Wang, Lei Chen, Mingxuan Yuan, Yinqi Bai, Yufei Kuang, Zhihai Wang |  |
| 1993 |  |  [Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning with Knowledge-guided Retrieval Augmented Generation](https://openreview.net/forum?id=oFBu7qaZpS) |  | 0 | Retrieval-augmented generation (RAG) has improved large language models (LLMs) by using knowledge retrieval to overcome knowledge deficiencies. However, current RAG methods often fall short of ensuring the depth and completeness of retrieved information, which is necessary for complex reasoning... | Cehao Yang, Chengjin Xu, Huaren Qu, Jian Guo, Jiaxin Mao, Muzhi Li, Shengjie Ma, Xuhui Jiang |  |
| 1994 |  |  [Generation and Comprehension Hand-in-Hand: Vision-guided Expression Diffusion for Boosting Referring Expression Generation and Comprehension](https://openreview.net/forum?id=1qbZekXGrp) |  | 0 | Referring expression generation (REG) and comprehension (REC) are vital and complementary in joint visual and textual reasoning. Existing REC datasets typically contain insufficient image-expression pairs for training, hindering the generalization of REC models to unseen referring expressions.... | ChiaWen Lin, IHong Jhuo, Jingcheng Ke, JunCheng Chen, YenYu Lin |  |
| 1995 |  |  [Optimality of Matrix Mechanism on ℓpp-metric](https://openreview.net/forum?id=fbqOEOqurU) |  | 0 | In this paper, we introduce the $\ell_p^p$-error metric (for $p \geq 2$) when answering linear queries under the constraint of differential privacy. We characterize such an error under $(\epsilon,\delta)$-differential privacy in the natural add/remove model. Before this paper, tight... | Jalaj Upadhyay, Jingcheng Liu, Zongrui Zou |  |
| 1996 |  |  [CLIPDrag: Combining Text-based and Drag-based Instructions for Image Editing](https://openreview.net/forum?id=2HjRezQ1nj) |  | 0 | Precise and flexible image editing remains a fundamental challenge in computer vision. Based on the modified areas, most editing methods can be divided into two main types: global editing and local editing. In this paper, we choose the two most common editing approaches (\ie text-based editing and... | Long Chen, Zhen Wang, Ziqi Jiang |  |
| 1997 |  |  [Interactive Adjustment for Human Trajectory Prediction with Individual Feedback](https://openreview.net/forum?id=DCpukR83sw) |  | 0 | Human trajectory prediction is fundamental for autonomous driving and service robot. The research community has studied various important aspects of this task and made remarkable progress recently. However, there is an essential perspective which is not well exploited in previous research all... | Cewu Lu, Jianhua Sun, Liang Chai, Yuxuan Li |  |
| 1998 |  |  [Transformers Provably Learn Two-Mixture of Linear Classification via Gradient Flow](https://openreview.net/forum?id=AuAj4vRPkv) |  | 0 | Understanding how transformers learn and utilize hidden connections between tokens is crucial to understand the behavior of large language models. To understand this mechanism, we consider the task of two-mixture of linear classification which possesses a hidden correspondence structure among... | Hongru Yang, Jason D. Lee, Yingbin Liang, Zhangyang Wang |  |
| 1999 |  |  [Alchemy: Amplifying Theorem-Proving Capability Through Symbolic Mutation](https://openreview.net/forum?id=7NL74jUiMg) |  | 0 | Formal proofs are challenging to write even for experienced experts. Recent progress in Neural Theorem Proving (NTP) shows promise in expediting this process. However, the formal corpora available on the Internet are limited compared to the general text, posing a significant data scarcity challenge... | Nan Duan, Ping Wei, Shaonan Wu, Shuai Lu, Yeyun Gong |  |
| 2000 |  |  [NExUME: Adaptive Training and Inference for DNNs under Intermittent Power Environments](https://openreview.net/forum?id=SFNqrHQTEP) |  | 0 | The deployment of Deep Neural Networks (DNNs) in energy-constrained environments, such as Energy Harvesting Wireless Sensor Networks (EH-WSNs), introduces significant challenges due to the intermittent nature of power availability. This study introduces NExUME, a novel training methodology designed... | Chita R. Das, Cyan Subhra Mishra, Deeksha Chaudhary, Jack Sampson, Mahmut T. Kandemir |  |
| 2001 |  |  [Agent Skill Acquisition for Large Language Models via CycleQD](https://openreview.net/forum?id=Kvdh12wGC0) |  | 0 | Training large language models to acquire specific skills remains a challenging endeavor. Conventional training approaches often struggle with data distribution imbalances and inadequacies in objective functions that do not align well with task-specific performance. To address these challenges, we... | So Kuroki, Taishi Nakamura, Takuya Akiba, Yujin Tang |  |
| 2002 |  |  [Learning Graph Invariance by Harnessing Spuriosity](https://openreview.net/forum?id=UsVJlgD1F7) |  | 0 | Recently, graph invariant learning has become the _de facto_ approach to tackle the Out-of-Distribution (OOD) generalization failure in graph representation learning. They generically follow the framework of invariant risk minimization to capture the invariance of graph data from different... | Kai Hu, Kun Zhang, Tianjun Yao, Tongliang Liu, Yongqiang Chen, Zhiqiang Shen |  |
| 2003 |  |  [Spiking Vision Transformer with Saccadic Attention](https://openreview.net/forum?id=qzZsz6MuEq) |  | 0 | The combination of Spiking Neural Networks (SNNs) and Vision Transformers (ViTs) holds potential for achieving both energy efficiency and high performance, particularly suitable for edge vision applications. However, a significant performance gap still exists between SNN-based ViTs and their ANN... | Ammar Belatreche, Dehao Zhang, Enqi Zhang, Malu Zhang, Qian Sun, Shuai Wang, Yang Yang, Yichen Xiao, Yimeng Shan, Yu Liang |  |
| 2004 |  |  [Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models](https://openreview.net/forum?id=1hQKHHUsMx) |  | 0 | The capabilities and limitations of Large Language Models (LLMs) have been sketched out in great detail in recent years, providing an intriguing yet conflicting picture. On the one hand, LLMs demonstrate a general ability to solve problems. On the other hand, they show surprising reasoning gaps... | Acyr Locatelli, Dwaraknath Gnaneshwar, Edward Grefenstette, Juhan Bae, Laura Ruis, Max Bartolo, Maximilian Mozes, Robert Kirk, Siddhartha Rao Kamalakara, Tim Rocktäschel |  |
| 2005 |  |  [From Attention to Activation: Unraveling the Enigmas of Large Language Models](https://openreview.net/forum?id=IjduZQK8gM) |  | 0 | We study two strange phenomena in auto-regressive Transformers: (1) the dominance of the ﬁrst token in attention heads; (2) the occurrence of large outlier activations in the hidden states. We ﬁnd that popular large language models, such as Llama attend maximally to the first token in 98% of... | Chengcheng Ma, Ismail Elezi, Jiankang Deng, Prannay Kaul |  |
| 2006 |  |  [On the Linear Speedup of Personalized Federated Reinforcement Learning with Shared Representations](https://openreview.net/forum?id=BfUDZGqCAu) |  | 0 | Federated reinforcement learning (FedRL) enables multiple agents to collaboratively learn a policy without needing to share the local trajectories collected during agent-environment interactions. However, in practice, the environments faced by different agents are often heterogeneous, but since... | Daniel Jiang, Guojun Xiong, Jian Li, Shufan Wang |  |
| 2007 |  |  [Global Identifiability of Overcomplete Dictionary Learning via L1 and Volume Minimization](https://openreview.net/forum?id=4nrcn0YoDG) |  | 0 | We propose a novel formulation for dictionary learning with an overcomplete dictionary, i.e., when the number of atoms is larger than the dimension of the dictionary. The proposed formulation consists of a weighted sum of $\ell_1$ norms of the rows of the sparse coefficient matrix plus the log of... | Kejun Huang, Yuchen Sun |  |
| 2008 |  |  [KBLaM: Knowledge Base augmented Language Model](https://openreview.net/forum?id=aLsMzkTej9) |  | 0 | In this paper, we propose Knowledge Base augmented Language Model (KBLAM), a new method for augmenting Large Language Models (LLMs) with external knowledge. KBLAM works with a knowledge base (KB) constructed from a corpus of documents, transforming each piece of knowledge in the KB into continuous... | James Hensman, Liana Mikaelyan, Taketomo Isazawa, Xi Wang |  |
| 2009 |  |  [Minimalistic Predictions for Online Class Constraint Scheduling](https://openreview.net/forum?id=j8lqABLgub) |  | 0 | We consider online scheduling with class constraints. That is, we are given $m$ machines, each with $k$ class slots. Upon receiving a job $j$ with class $c_j$, an algorithm needs to allocate $j$ on some machine $i$. The goal is to minimize the makespan while not assigning more than $k$ different... | Alexandra Anna Lassota, Dorian Guyot |  |
| 2010 |  |  [Do Large Language Models Truly Understand Geometric Structures?](https://openreview.net/forum?id=FjQOXenaXK) |  | 0 | Geometric ability is a significant challenge for large language models (LLMs) due to the need for advanced spatial comprehension and abstract thinking. Existing datasets primarily evaluate LLMs on their final answers, but they cannot truly measure their true understanding of geometric structures,... | Rui Wang, Wenhong Zhu, Xiaofeng Wang, Yiming Wang |  |
| 2011 |  |  [DeepTAGE: Deep Temporal-Aligned Gradient Enhancement for Optimizing Spiking Neural Networks](https://openreview.net/forum?id=drPDukdY3t) |  | 0 | Spiking Neural Networks (SNNs), with their biologically inspired spatio-temporal dynamics and spike-driven processing, are emerging as a promising low-power alternative to traditional Artificial Neural Networks (ANNs). However, the complex neuronal dynamics and non-differentiable spike... | Bing Li, Jin Gao, Li Yang, Mingxuan Zhao, Shuxun Wang, Wei Liu, Weiming Hu, Wenjuan Li |  |
| 2012 |  |  [Mitigating Reward Over-Optimization in RLHF via Behavior-Supported Regularization](https://openreview.net/forum?id=PNMv4r7s1i) |  | 0 | Reinforcement learning from human feedback (RLHF) is an effective method for aligning large language models (LLMs) with human values. However, reward over-optimization remains an open challenge leading to discrepancies between the performance of LLMs under the reward model and the true human... | Gang Pan, Juntao Dai, Qian Zheng, Taiye Chen, Yaodong Yang |  |
| 2013 |  |  [New Algorithms for the Learning-Augmented k-means Problem](https://openreview.net/forum?id=Xuyp1dGAbi) |  | 0 | In this paper, we study the clustering problems in the learning-augmented setting, where predicted labels for a d-dimensional dataset with size m are given by an oracle to serve as auxiliary information to improve the clustering performance. Following the prior work, the given oracle is... | Jianxin Wang, Jinhui Xu, Junyu Huang, Qilong Feng, Zhen Zhang, Ziyun Huang |  |
| 2014 |  |  [Bridging the Semantic Gap Between Text and Table: A Case Study on NL2SQL](https://openreview.net/forum?id=qmsX2R19p9) |  | 0 | The rise of Large Language Models (LLMs) has revolutionized numerous domains, yet these models still exhibit weakness in understanding structured tabular data. Although the growing context window promises to accommodate a larger volume of table contents, it does not inherently improve the model's... | Gang Chen, Haobo Wang, Junbo Zhao, Lin Long, Sai Wu, Wentao Ye, Xijun Gu, Xinjie Sun |  |
| 2015 |  |  [Ask, and it shall be given: On the Turing completeness of prompting](https://openreview.net/forum?id=AS8SPTyBgw) |  | 0 | Since the success of GPT, large language models (LLMs) have revolutionized machine learning and have initiated the so-called \*LLM prompting\* paradigm. In the era of LLMs, people train a single general-purpose LLM and provide the LLM with different \*prompts\* to perform different tasks. However,... | Hanghang Tong, Ruizhong Qiu, Wenxuan Bao, Zhe Xu |  |
| 2016 |  |  [Noisy Test-Time Adaptation in Vision-Language Models](https://openreview.net/forum?id=iylpeTI0Ql) |  | 0 | Test-time adaptation (TTA) aims to address distribution shifts between source and target data by relying solely on target data during testing. In open-world scenarios, models often encounter noisy samples, i.e., samples outside the in-distribution (ID) label space. Leveraging the zero-shot... | Bo Han, Chentao Cao, Kun Zhang, Tongliang Liu, Yang Liu, Zhanke Zhou, Zhun Zhong |  |
| 2017 |  |  [MOFFlow: Flow Matching for Structure Prediction of Metal-Organic Frameworks](https://openreview.net/forum?id=dNT3abOsLo) |  | 0 | Metal-organic frameworks (MOFs) are a class of crystalline materials with promising applications in many areas such as carbon capture and drug delivery. In this work, we introduce MOFFlow, the first deep generative model tailored for MOF structure prediction. Existing approaches, including ab... | Jinkyoo Park, Minsu Kim, Nayoung Kim, Seongsu Kim, Sungsoo Ahn |  |
| 2018 |  |  [QP-SNN: Quantized and Pruned Spiking Neural Networks](https://openreview.net/forum?id=MiPyle6Jef) |  | 0 | Brain-inspired Spiking Neural Networks (SNNs) leverage sparse spikes to encode information and operate in an asynchronous event-driven manner, offering a highly energy-efficient paradigm for machine intelligence. However, the current SNN community focuses primarily on performance improvement by... | Ammar Belatreche, Honglin Cao, Jieyuan Zhang, Malu Zhang, Wenjie Wei, Yang Yang, Yimeng Shan, Yu Liang, Zijian Zhou |  |
| 2019 |  |  [Fantastic Copyrighted Beasts and How (Not) to Generate Them](https://openreview.net/forum?id=ftHNJmogT1) |  | 0 | Recent studies show that image and video generation models can be prompted to reproduce copyrighted content from their training data, raising serious legal con- cerns about copyright infringement. Copyrighted characters (e.g., Mario, Batman) present a significant challenge: at least one lawsuit has... | Chiyuan Zhang, Danqi Chen, Haotian Liu, Luke Zettlemoyer, Luxi He, Peter Henderson, Tinghao Xie, Weijia Shi, Yangsibo Huang, Yue Wang |  |
| 2020 |  |  [Verifying Properties of Binary Neural Networks Using Sparse Polynomial Optimization](https://openreview.net/forum?id=9c96mGtQVR) |  | 0 | This paper explores methods for verifying the properties of Binary Neural Networks (BNNs), focusing on robustness against adversarial attacks. Despite their lower computational and memory needs, BNNs, like their full-precision counterparts, are also sensitive to input perturbations. Established... | Jean B. Lasserre, Jianting Yang, Jun Zhao, Srecko Ðurasinovic, Victor Magron |  |
| 2021 |  |  [ReAttention: Training-Free Infinite Context with Finite Attention Scope](https://openreview.net/forum?id=KDGP8yAz5b) |  | 0 | The long-context capability of the Large Language Models (LLM) has made significant breakthroughs, but \textit{the maximum supported context length in length extrapolation} remains a critical bottleneck limiting their practical applications. The constraint of context length in LLMs arises from the... | Hang Yan, Kai Lv, Linlin Li, Qipeng Guo, Qun Liu, Ruixiao Li, Xiaoran Liu, Xipeng Qiu, Yuerong Song, Zhigeng Liu |  |
| 2022 |  |  [Mitigating the Backdoor Effect for Multi-Task Model Merging via Safety-Aware Subspace](https://openreview.net/forum?id=dqMqAaw7Sq) |  | 0 | Model merging has gained significant attention as a cost-effective approach to integrate multiple single-task fine-tuned models into a unified one that can perform well on multiple tasks. However, existing model merging techniques primarily focus on resolving conflicts between task-specific models,... | Anke Tang, Didi Zhu, Fei Wu, Jinluan Yang, Li Shen, Zhengyu Chen |  |
| 2023 |  |  [An Information Criterion for Controlled Disentanglement of Multimodal Data](https://openreview.net/forum?id=3n4RY25UWP) |  | 0 | Multimodal representation learning seeks to relate and decompose information inherent in multiple modalities. By disentangling modality-specific information from information that is shared across modalities, we can improve interpretability and robustness and enable downstream tasks such as the... | Caroline Uhler, Chenyu Wang, Sana Tonekaboni, Sharut Gupta, Stefanie Jegelka, Tommi S. Jaakkola, Xinyi Zhang |  |
| 2024 |  |  [Fréchet Wavelet Distance: A Domain-Agnostic Metric for Image Generation](https://openreview.net/forum?id=QinkNNKZ3b) |  | 0 | Modern metrics for generative learning like Fréchet Inception Distance (FID) and DINOv2-Fréchet Distance (FD-DINOv2) demonstrate impressive performance. However, they suffer from various shortcomings, like a bias towards specific generators and datasets. To address this problem, we propose the... | Hilde Kuehne, Juergen Gall, Lokesh Veeramacheneni, Moritz Wolter |  |
| 2025 |  |  [RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction](https://openreview.net/forum?id=gRmWtOnTLK) |  | 0 | Recent advancements in generative modeling have significantly enhanced the reconstruction of audio waveforms from various representations. While diffusion models are adept at this task, they are hindered by latency issues due to their operation at the individual sample point level and the need for... | Dongyang Dai, Peng Liu, Zhiyong Wu |  |
| 2026 |  |  [SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal](https://openreview.net/forum?id=YfKNaRktan) |  | 0 | Evaluating aligned large language models' (LLMs) ability to recognize and reject unsafe user requests is crucial for safe, policy-compliant deployments. Existing evaluation efforts, however, face three limitations that we address with \*\*SORRY-Bench\*\*, our proposed benchmark. \*\*First\*\*,... | Bo Li, Boyi Wei, Dacheng Li, Danqi Chen, Kai Li, Kaixuan Huang, Luxi He, Peter Henderson, Prateek Mittal, Ruoxi Jia, Tinghao Xie, Udari Madhushani Sehwag, Xiangyu Qi, Yangsibo Huang, Yi Zeng, Ying Sheng |  |
| 2027 |  |  [Towards Auto-Regressive Next-Token Prediction: In-context Learning Emerges from Generalization](https://openreview.net/forum?id=gK1rl98VRp) |  | 0 | Large language models (LLMs) have demonstrated remarkable in-context learning (ICL) abilities. However, existing theoretical analysis of ICL primarily exhibits two limitations: \textbf{(a) Limited \textit{i.i.d.} Setting.} Most studies focus on supervised function learning tasks where prompts are... | Huayi Tang, Xiaolin Hu, Yong Liu, Zixuan Gong |  |
| 2028 |  |  [SimPER: A Minimalist Approach to Preference Alignment without Hyperparameters](https://openreview.net/forum?id=jfwe9qNqRi) |  | 0 | Existing preference optimization objectives for language model alignment require additional hyperparameters that must be extensively tuned to achieve optimal performance, increasing both the complexity and time required for fine-tuning large language models. In this paper, we propose a simple yet... | Mingxiao Li, Shangsong Liang, Teng Xiao, Vasant G. Honavar, Yige Yuan, Zhaochun Ren, Zhengyu Chen |  |
| 2029 |  |  [Zero-cost Proxy for Adversarial Robustness Evaluation](https://openreview.net/forum?id=zHf7hOfeer) |  | 0 | Deep neural networks (DNNs) easily cause security issues due to the lack of adversarial robustness. An emerging research topic for this problem is to design adversarially robust architectures via neural architecture search (NAS), i.e., robust NAS. However, robust NAS needs to train numerous DNNs... | Jiahao Fan, Yanan Sun, Yuqi Feng, Yuwei Ou |  |
| 2030 |  |  [NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation](https://openreview.net/forum?id=p66a00KLWN) |  | 0 | 3D molecule generation is crucial for drug discovery and material design. While prior efforts focus on 3D diffusion models for their benefits in modeling continuous 3D conformers, they overlook the advantages of 1D SELFIES-based Language Models (LMs), which can generate 100\% valid molecules and... | Enzhi Zhang, Han Huang, Junfeng Fang, Kenji Kawaguchi, Sihang Li, TatSeng Chua, Xiang Wang, Yanchen Luo, Yaorui Shi, Zhiyuan Liu |  |
| 2031 |  |  [PnP-Flow: Plug-and-Play Image Restoration with Flow Matching](https://openreview.net/forum?id=5AtHrq3B5R) |  | 0 | In this paper, we introduce Plug-and-Play (PnP) Flow Matching, an algorithm for solving imaging inverse problems. PnP methods leverage the strength of pre-trained denoisers, often deep neural networks, by integrating them in optimization schemes. While they achieve state-of-the-art performance on... | Anne Gagneux, Gabriele Steidl, Paul Hagemann, Ségolène Tiffany Martin |  |
| 2032 |  |  [Logicbreaks: A Framework for Understanding Subversion of Rule-based Inference](https://openreview.net/forum?id=pljYMCYDWJ) |  | 0 | We study how to subvert large language models (LLMs) from following prompt-specified rules. We first formalize rule-following as inference in propositional Horn logic, a mathematical system in which rules have the form "if $P$ and $Q$, then $R$" for some propositions $P$, $Q$, and $R$. Next, we... | Anton Xue, Avishree Khare, Eric Wong, Rajeev Alur, Surbhi Goel |  |
| 2033 |  |  [Periodic Materials Generation using Text-Guided Joint Diffusion Model](https://openreview.net/forum?id=AkBrb7yQ0G) |  | 0 | Equivariant diffusion models have emerged as the prevailing approach for generat- ing novel crystal materials due to their ability to leverage the physical symmetries of periodic material structures. However, current models do not effectively learn the joint distribution of atom types, fractional... | Kishalay Das, Niloy Ganguly, Pawan Goyal, Satadeep Bhattacharjee, SeungCheol Lee, Subhojyoti Khastagir |  |
| 2034 |  |  [Understanding Virtual Nodes: Oversquashing and Node Heterogeneity](https://openreview.net/forum?id=NmcOAwRyH5) |  | 0 | While message passing neural networks (MPNNs) have convincing success in a range of applications, they exhibit limitations such as the oversquashing problem and their inability to capture long-range interactions. Augmenting MPNNs with a virtual node (VN) removes the locality constraint of the layer... | Francesco Di Giovanni, Johannes F. Lutzeyer, Joshua Southern, Michael M. Bronstein |  |
| 2035 |  |  [TODO: Enhancing LLM Alignment with Ternary Preferences](https://openreview.net/forum?id=utkGLDSNOk) |  | 0 | Aligning large language models (LLMs) with human intent is critical for enhancing their performance across a variety of tasks. Standard alignment techniques, such as Direct Preference Optimization (DPO), often rely on the binary Bradley-Terry (BT) model, which can struggle to capture the... | Bo Jiang, Jiaqi Zhang, Lu Yin, Yuxiang Guo |  |
| 2036 |  |  [Occlusion-aware Non-Rigid Point Cloud Registration via Unsupervised Neural Deformation Correntropy](https://openreview.net/forum?id=cjJqU40nYS) |  | 0 | Non-rigid alignment of point clouds is crucial for scene understanding, reconstruction, and various computer vision and robotics tasks. Recent advancements in implicit deformation networks for non-rigid registration have significantly reduced the reliance on large amounts of annotated training... | DongMing Yan, Gaofeng Meng, Mingyang Zhao |  |
| 2037 |  |  [Operator Deep Smoothing for Implied Volatility](https://openreview.net/forum?id=DPlUWG4WMw) |  | 0 | We devise a novel method for nowcasting implied volatility based on neural operators. Better known as implied volatility smoothing in the financial industry, nowcasting of implied volatility means constructing a smooth surface that is consistent with the prices presently observed on a given option... | Antoine Jacquier, Lukas Gonon, Ruben Wiedemann |  |
| 2038 |  |  [Re-evaluating Open-ended Evaluation of Large Language Models](https://openreview.net/forum?id=kbOAIXKWgx) |  | 0 | Evaluation has traditionally focused on ranking candidates for a specific skill. Modern generalist models, such as Large Language Models (LLMs), decidedly outpace this paradigm. Open-ended evaluation systems, where candidate models are compared on user-submitted prompts, have emerged as a popular... | Georgios Piliouras, Ian Gemp, Luke Marris, Marc Lanctot, Nicolas Heess, Siqi Liu |  |
| 2039 |  |  [An Intelligent Agentic System for Complex Image Restoration Problems](https://openreview.net/forum?id=3RLxccFPHz) |  | 0 | Real-world image restoration (IR) is inherently complex and often requires combining multiple specialized models to address diverse degradations. Inspired by human problem-solving, we propose AgenticIR, an agentic system that mimics the human approach to image processing by following five key... | Chao Dong, Jinjin Gu, Kaiwen Zhu, Yu Qiao, Zhiyuan You |  |
| 2040 |  |  [Open-Set Graph Anomaly Detection via Normal Structure Regularisation](https://openreview.net/forum?id=kSvoX0xdlO) |  | 0 | This paper considers an important Graph Anomaly Detection (GAD) task, namely open-set GAD, which aims to train a detection model using a small number of normal and anomaly nodes (referred to as \*seen anomalies\*) to detect both seen anomalies and \*unseen anomalies\* (\*i.e\*., anomalies that... | Christopher Leckie, Guansong Pang, Mahsa Salehi, Qizhou Wang, Xiaokun Xia |  |
| 2041 |  |  [QERA: an Analytical Framework for Quantization Error Reconstruction](https://openreview.net/forum?id=LB5cKhgOTu) |  | 0 | The growing number of parameters and computational demands of large language models (LLMs) present significant challenges for their efficient deployment. Recently, there is an increasing interest in quantizing weights to extremely low precision while offsetting the resulting error with low-rank,... | Can Xiao, Cheng Zhang, George Anthony Constantinides, Jeffrey T. H. Wong, Yiren Zhao |  |
| 2042 |  |  [Advancing Prompt-Based Methods for Replay-Independent General Continual Learning](https://openreview.net/forum?id=V6uxd8MEqw) |  | 0 | General continual learning (GCL) is a broad concept to describe real-world continual learning (CL) problems, which are often characterized by online data streams without distinct transitions between tasks, i.e., blurry task boundaries. Such requirements result in poor initial performance, limited... | Karteek Alahari, Liyuan Wang, Xingxing Zhang, Zhiqi Kang |  |
| 2043 |  |  [Is Your Model Really A Good Math Reasoner? Evaluating Mathematical Reasoning with Checklist](https://openreview.net/forum?id=nDvgHIBRxQ) |  | 0 | Exceptional mathematical reasoning ability is one of the key features that demonstrate the power of large language models (LLMs). How to comprehensively define and evaluate the mathematical abilities of LLMs, and even reflect the user experience in real-world scenarios, has emerged as a critical... | Derek F. Wong, Jindong Wang, Kaizhu Huang, Maizhen Ning, Qiufeng Wang, Shudong Liu, Wei Liu, Xiaowei Huang, Zihao Zhou |  |
| 2044 |  |  [Beyond Circuit Connections: A Non-Message Passing Graph Transformer Approach for Quantum Error Mitigation](https://openreview.net/forum?id=XnVttczoAV) |  | 0 | Despite the progress in quantum computing, one major bottleneck against the practical utility is its susceptibility to noise, which frequently occurs in current quantum systems. Existing quantum error mitigation (QEM) methods either lack generality to noise and circuit types or fail to capture the... | Chang Liu, Hang Ruan, Junchi Yan, Tianyi Bao, Wenjie Wu, Xinyu Ye |  |
| 2045 |  |  [ViSAGe: Video-to-Spatial Audio Generation](https://openreview.net/forum?id=8bF1Vaj9tm) |  | 0 | Spatial audio is essential for enhancing the immersiveness of audio-visual experiences, yet its production typically demands complex recording systems and specialized expertise. In this work, we address a novel problem of generating first-order ambisonics, a widely used spatial audio format,... | Gunhee Kim, Heeseung Yun, Jaeyeon Kim |  |
| 2046 |  |  [Learning Long Range Dependencies on Graphs via Random Walks](https://openreview.net/forum?id=kJ5H7oGT2M) |  | 0 | Message-passing graph neural networks (GNNs) excel at capturing local relationships but struggle with long-range dependencies in graphs. In contrast, graph transformers (GTs) enable global information exchange but often oversimplify the graph structure by representing graphs as sets of fixed-length... | Dexiong Chen, Karsten M. Borgwardt, Till Hendrik Schulz |  |
| 2047 |  |  [ReSi: A Comprehensive Benchmark for Representational Similarity Measures](https://openreview.net/forum?id=PRvdO3nfFi) |  | 0 | Measuring the similarity of different representations of neural architectures is a fundamental task and an open research challenge for the machine learning community. This paper presents the first comprehensive benchmark for evaluating representational similarity measures based on well-defined... | Florian Lemmerich, Klaus H. MaierHein, Markus Strohmaier, Max Klabunde, Tassilo Wald, Tobias Schumacher |  |
| 2048 |  |  [EvA: Erasing Spurious Correlations with Activations](https://openreview.net/forum?id=zKvrOOBouT) |  | 0 | Spurious correlations often arise when models associate features strongly correlated with, but not causally related to, the label e.g. an image classifier associates bodies of water with ducks. To mitigate spurious correlations, existing methods focus on learning unbiased representation or... | Angela Yao, Kai Xu, Qiyuan He |  |
| 2049 |  |  [Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models](https://openreview.net/forum?id=e2ONKX6qzJ) |  | 0 | Classifier-free guidance (CFG) is crucial for improving both generation quality and alignment between the input condition and final output in diffusion models. While a high guidance scale is generally required to enhance these aspects, it also causes oversaturation and unrealistic artifacts. In... | Otmar Hilliges, Romann M. Weber, Seyedmorteza Sadat |  |
| 2050 |  |  [An Online Learning Theory of Trading-Volume Maximization](https://openreview.net/forum?id=OvU9u6wS2J) |  | 0 | We explore brokerage between traders in an online learning framework. At any round $t$, two traders meet to exchange an asset, provided the exchange is mutually beneficial. The broker proposes a trading price, and each trader tries to sell their asset or buy the asset from the other party,... | Roberto Colomboni, Tommaso Cesari |  |
| 2051 |  |  [Fast and Accurate Blind Flexible Docking](https://openreview.net/forum?id=iezDdA9oeB) |  | 0 | Molecular docking that predicts the bound structures of small molecules (ligands) to their protein targets, plays a vital role in drug discovery. However, existing docking methods often face limitations: they either overlook crucial structural changes by assuming protein rigidity or suffer from low... | Bo Han, Jiangchao Yao, Kaiyuan Gao, Lijun Wu, Tao Qin, Zizhuo Zhang |  |
| 2052 |  |  [Efficient Discovery of Pareto Front for Multi-Objective Reinforcement Learning](https://openreview.net/forum?id=fDGPIuCdGi) |  | 0 | Multi-objective reinforcement learning (MORL) excels at handling rapidly changing preferences in tasks that involve multiple criteria, even for unseen preferences. However, previous dominating MORL methods typically generate a fixed policy set or preference-conditioned policy through multiple... | Jiang Bian, Lei Song, Linjie Xu, Pengcheng You, Ruohong Liu, Yize Chen, Yuxin Pan |  |
| 2053 |  |  [CAMEx: Curvature-aware Merging of Experts](https://openreview.net/forum?id=nT2u0M0nf8) |  | 0 | Existing methods for merging experts during model training and fine-tuning predominantly rely on Euclidean geometry, which assumes a flat parameter space. This assumption can limit the model's generalization ability, especially during the pre-training phase, where the parameter manifold might... | Linh Duy Tran, Luc Q. Nguyen, Minh Nguyen Hoang, Rachel S. Y. Teo, Tan Minh Nguyen, Viet Dung Nguyen |  |
| 2054 |  |  [Efficient Online Pruning and Abstraction for Imperfect Information Extensive-Form Games](https://openreview.net/forum?id=MTcgsz1SHr) |  | 0 | Efficiently computing approximate equilibrium strategies in large Imperfect Information Extensive-Form Games (IIEFGs) poses significant challenges due to the game tree's exponential growth. While pruning and abstraction techniques are essential for complexity reduction, existing methods face two... | Boning Li, Longbo Huang |  |
| 2055 |  |  [metabench - A Sparse Benchmark of Reasoning and Knowledge in Large Language Models](https://openreview.net/forum?id=4T33izzFpK) |  | 0 | Large Language Models (LLMs) vary in their abilities on a range of tasks. Initiatives such as the Open LLM Leaderboard aim to quantify these differences with several large benchmarks (sets of test items to which an LLM can respond either correctly or incorrectly). However, high correlations within... | Alexander Kipnis, Eric Schulz, Konstantinos Voudouris, Luca M. Schulze Buschoff |  |
| 2056 |  |  [CAT-3DGS: A Context-Adaptive Triplane Approach to Rate-Distortion-Optimized 3DGS Compression](https://openreview.net/forum?id=m3KuuE2ozw) |  | 0 | 3D Gaussian Splatting (3DGS) has recently emerged as a promising 3D representation. Much research has been focused on reducing its storage requirements and memory footprint. However, the needs to compress and transmit the 3DGS representation to the remote side are overlooked. This new application... | ChengYuan Ho, Hebi Yang, JuiChiu Chiang, WenHsiao Peng, YiHsin Chen, YuLun Liu, YuTing Zhan |  |
| 2057 |  |  [On the Benefits of Attribute-Driven Graph Domain Adaptation](https://openreview.net/forum?id=t2TUw5nJsW) |  | 0 | Graph Domain Adaptation (GDA) addresses a pressing challenge in cross-network learning, particularly pertinent due to the absence of labeled data in real-world graph datasets. Recent studies attempted to learn domain invariant representations by eliminating structural shifts between graphs. In this... | Bingheng Li, Boyu Wang, Charles Ling, Nima Hosseini Dashtbayaz, Qiuhao Zeng, Ruiyi Fang, Ruizhi Pu, Zhao Kang |  |
| 2058 |  |  [EgoSim: Egocentric Exploration in Virtual Worlds with Multi-modal Conditioning](https://openreview.net/forum?id=zAyS5aRKV8) |  | 0 | Recent advancements in video diffusion models have established a strong foundation for developing world models with practical applications. The next challenge lies in exploring how an agent can leverage these foundation models to understand, interact with, and plan within observed environments.... | Animesh Garg, Songheng Yin, Steve Easterbrook, Wei Yu |  |
| 2059 |  |  [Capability Localization: Capabilities Can be Localized rather than Individual Knowledge](https://openreview.net/forum?id=f6r1mYwM1g) |  | 0 | Large scale language models have achieved superior performance in tasks related to natural language processing, however, it is still unclear how model parameters affect performance improvement. Previous studies assumed that individual knowledge is stored in local parameters, and the storage form of... | Jiaxiang Liu, Jun Zhao, Kang Liu, Xiusheng Huang, Yequan Wang |  |
| 2060 |  |  [Semi-Supervised CLIP Adaptation by Enforcing Semantic and Trapezoidal Consistency](https://openreview.net/forum?id=97D725GJtQ) |  | 0 | Vision-language pre-training models, such as CLIP, have demonstrated strong capability in rapidly adapting to downstream tasks through fine-tuning, and have been widely applied across various tasks. However, when the downstream tasks are constrained by limited image-text paired data, CLIP struggles... | Bo Ye, Kai Gan, MinLing Zhang, Tong Wei |  |
| 2061 |  |  [Learning Successor Features with Distributed Hebbian Temporal Memory](https://openreview.net/forum?id=wYJII5BRYU) |  | 0 | This paper presents a novel approach to address the challenge of online sequence learning for decision making under uncertainty in non-stationary, partially observable environments. The proposed algorithm, Distributed Hebbian Temporal Memory (DHTM), is based on the factor graph formalism and a... | Aleksandr Panov, Evgenii Aleksandrovich Dzhivelikian, Petr Kuderov |  |
| 2062 |  |  [Divergence of Neural Tangent Kernel in Classification Problems](https://openreview.net/forum?id=VEJzjAvaIy) |  | 0 | This paper primarily investigates the convergence of the Neural Tangent Kernel (NTK) in classification problems. This study firstly show the strictly positive definiteness of NTK of multi-layer fully connected neural networks and residual neural networks. Then, through a contradiction argument, it... | Guhan Chen, Songtao Tian, Zixiong Yu |  |
| 2063 |  |  [Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model](https://openreview.net/forum?id=sAYnDWaGd5) |  | 0 | Recent advancements in large language models (LLMs) have highlighted the importance of extending context lengths for handling complex tasks. While traditional methods for training on long contexts often use filtered long documents, these approaches lead to domain imbalances, limiting model... | Chaochen Gao, Qi Fu, Songlin Hu, Xing Wu |  |
| 2064 |  |  [Robust LLM safeguarding via refusal feature adversarial training](https://openreview.net/forum?id=s5orchdb33) |  | 0 | Large language models (LLMs) are vulnerable to adversarial attacks that can elicit harmful responses. Defending against such attacks remains challenging due to the opacity of jailbreaking mechanisms and the high computational cost of training LLMs robustly. We demonstrate that adversarial attacks... | Karen Hambardzumyan, Lei Yu, Nicola Cancedda, Virginie Do |  |
| 2065 |  |  [The Rise and Down of Babel Tower: Investigating the Evolution Process of Multilingual Code Large Language Model](https://openreview.net/forum?id=eznTVIM3bs) |  | 0 | Large language models (LLMs) have shown significant multilingual capabilities. However, the mechanisms underlying the development of these capabilities during pre-training are not well understood. In this paper, we use code LLMs as an experimental platform to explore the evolution of multilingual... | Hongyu Lin, Jiawei Chen, Jing Su, Jingjing Xu, Le Sun, Mengjie Ren, Wentao Chen, Xianpei Han, Yaojie Lu |  |
| 2066 |  |  [Unsupervised Zero-Shot Reinforcement Learning via Dual-Value Forward-Backward Representation](https://openreview.net/forum?id=0QnKnt411O) |  | 0 | Online unsupervised reinforcement learning (URL) can discover diverse skills via reward-free pre-training and exhibits impressive downstream task adaptation abilities through further fine-tuning. However, online URL methods face challenges in achieving zero-shot generalization, i.e., directly... | Dongbin Zhao, Haoran Li, Jingbo Sun, Ke Chen, Qichao Zhang, Songjun Tu, Xin Liu, Yaran Chen |  |
| 2067 |  |  [To Code or Not To Code? Exploring Impact of Code in Pre-training](https://openreview.net/forum?id=zSfeN1uAcx) |  | 0 | Including code in the pre-training data mixture, even for models not specifically designed for code, has become a common practice in LLMs pre-training. While there has been anecdotal consensus among practitioners that code data plays a vital role in general LLMs' performance, there is only limited... | Acyr Locatelli, Adrien Morisot, Ahmet Üstün, Ivan Zhang, Marzieh Fadaee, Raymond Ma, Sara Hooker, Viraat Aryabumi, Yixuan Su |  |
| 2068 |  |  [Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data](https://openreview.net/forum?id=sMyXP8Tanm) |  | 0 | Discrete diffusion models with absorbing processes have shown promise in language modeling. The key quantities to be estimated are the ratios between the marginal probabilities of two transitive states at all timesteps, called the concrete score. In this paper, we reveal that the concrete score in... | Chongxuan Li, Fengqi Zhu, Jiacheng Sun, Jingyang Ou, Kaiwen Xue, Shen Nie, Zhenguo Li |  |
| 2069 |  |  [TempMe: Video Temporal Token Merging for Efficient Text-Video Retrieval](https://openreview.net/forum?id=lVp97zZ5i8) |  | 0 | Most text-video retrieval methods utilize the text-image pre-trained models like CLIP as a backbone. These methods process each sampled frame independently by the image encoder, resulting in high computational overhead and limiting practical deployment. Addressing this, we focus on efficient... | Guiguang Ding, Leqi Shen, Pengzhang Liu, Sicheng Zhao, Tao He, Tianxiang Hao, Yifeng Zhang, Yongjun Bao |  |
| 2070 |  |  [Predicting the Energy Landscape of Stochastic Dynamical System via Physics-informed Self-supervised Learning](https://openreview.net/forum?id=PxRATSTDlS) |  | 0 | Energy landscapes play a crucial role in shaping dynamics of many real-world complex systems. System evolution is often modeled as particles moving on a landscape under the combined effect of energy-driven drift and noise-induced diffusion, where the energy governs the long-term motion of the... | Huandong Wang, Qingmin Liao, Ruikun Li, Yong Li |  |
| 2071 |  |  [Dimension Agnostic Neural Processes](https://openreview.net/forum?id=uGJxl2odR0) |  | 0 | Meta-learning aims to train models that can generalize to new tasks with limited labeled data by extracting shared features across diverse task datasets. Additionally, it accounts for prediction uncertainty during both training and evaluation, a concept known as uncertainty-aware meta-learning.... | Chaeyun Jang, Dongbok Lee, Hyungi Lee, Juho Lee |  |
| 2072 |  |  [Variational Bayesian Pseudo-Coreset](https://openreview.net/forum?id=0NAVeUm7sk) |  | 0 | The success of deep learning requires large datasets and extensive training, which can create significant computational challenges. To address these challenges, pseudo-coresets, small learnable datasets that mimic the entire data, have been proposed. Bayesian Neural Networks, which offer predictive... | Hyungi Lee, Juho Lee, Seungyoo Lee |  |
| 2073 |  |  [Biologically Plausible Brain Graph Transformer](https://openreview.net/forum?id=rQyg6MnsDb) |  | 0 | State-of-the-art brain graph analysis methods fail to fully encode the small-world architecture of brain graphs (accompanied by the presence of hubs and functional modules), and therefore lack biological plausibility to some extent. This limitation hinders their ability to accurately represent the... | Chengqi Zhang, Ciyuan Peng, Feng Xia, Qichao Dong, Shuo Yu, Yaochu Jin, Yuelong Huang |  |
| 2074 |  |  [Building, Reusing, and Generalizing Abstract Representations from Concrete Sequences](https://openreview.net/forum?id=xIUUnzrUtD) |  | 0 | Humans excel at learning abstract patterns across different sequences, filtering out irrelevant details, and transferring these generalized concepts to new sequences. In contrast, many sequence learning models lack the ability to abstract, which leads to memory inefficiency and poor transfer. We... | Eric Schulz, Mirko Thalmann, Peter Dayan, Shuchen Wu, Zeynep Akata |  |
| 2075 |  |  [HD-Painter: High-Resolution and Prompt-Faithful Text-Guided Image Inpainting with Diffusion Models](https://openreview.net/forum?id=6lB5qtdYAg) |  | 0 | Recent progress in text-guided image inpainting, based on the unprecedented success of text-to-image diffusion models, has led to exceptionally realistic and visually plausible results. However, there is still significant potential for improvement in current text-to-image inpainting models,... | Andranik Sargsyan, Barsegh Atanyan, Hayk Manukyan, Humphrey Shi, Shant Navasardyan, Zhangyang Wang |  |
| 2076 |  |  [ViBiDSampler: Enhancing Video Interpolation Using Bidirectional Diffusion Sampler](https://openreview.net/forum?id=nNYA7tcJSE) |  | 0 | Recent progress in large-scale text-to-video (T2V) and image-to-video (I2V) diffusion models has greatly enhanced video generation, especially in terms of keyframe interpolation. However, current image-to-video diffusion models, while powerful in generating videos from a single conditioning frame,... | Jong Chul Ye, Serin Yang, Taesung Kwon |  |
| 2077 |  |  [Stochastic Bandits Robust to Adversarial Attacks](https://openreview.net/forum?id=vOFx8HDcvF) |  | 0 | This paper investigates stochastic multi-armed bandit algorithms that are robust to adversarial attacks, where an attacker can first observe the learner's action and \*then\* alter their reward observation. We study two cases of this model, with or without the knowledge of an attack budget $C$,... | Jinhang Zuo, John C. S. Lui, Maoli Liu, Mohammad Hajiesmaili, Xuchuang Wang, Xutong Liu |  |
| 2078 |  |  [UV-Attack: Physical-World Adversarial Attacks on Person Detection via Dynamic-NeRF-based UV Mapping](https://openreview.net/forum?id=pqeWzZTrZY) |  | 0 | Recent works have attacked person detectors using adversarial patches or static-3D-model-based texture modifications. However, these methods suffer from low attack success rates when faced with significant human movements. The primary challenge stems from the highly non-rigid nature of the human... | Bin Xiao, Kaisheng Liang, Yanjie Li |  |
| 2079 |  |  [TestGenEval: A Real World Unit Test Generation and Test Completion Benchmark](https://openreview.net/forum?id=7o6SG5gVev) |  | 0 | Code generation models can help improve many common software tasks ranging from code completion to defect prediction. Most of the existing benchmarks for code generation LLMs focus on code authoring or code completion. Surprisingly, there has been far less effort dedicated to benchmarking software... | Baptiste Rozière, Gabriel Synnaeve, Kush Jain |  |
| 2080 |  |  [Efficient Distribution Matching of Representations via Noise-Injected Deep InfoMax](https://openreview.net/forum?id=mAmCdASmJ5) |  | 0 | Deep InfoMax (DIM) is a well-established method for self-supervised representation learning (SSRL) based on maximization of the mutual information between the input and the output of a deep neural network encoder. Despite the DIM and contrastive SSRL in general being well-explored, the task of... | Alexander Semenenko, Alexander Tolmachev, Alexey A. Frolov, Andrey Gladkov, Ivan Butakov, Marina Munkhoeva |  |
| 2081 |  |  [Model-agnostic meta-learners for estimating heterogeneous treatment effects over time](https://openreview.net/forum?id=QGGNvKaoIU) |  | 0 | Estimating heterogeneous treatment effects (HTEs) over time is crucial in many disciplines such as personalized medicine. Existing works for this task have mostly focused on \*model-based\* learners that adapt specific machine-learning models and adjustment mechanisms. In contrast, model-agnostic... | Dennis Frauen, Konstantin Hess, Stefan Feuerriegel |  |
| 2082 |  |  [F3Set: Towards Analyzing Fast, Frequent, and Fine-grained Events from Videos](https://openreview.net/forum?id=vlg5WRKHxh) |  | 0 | Analyzing Fast, Frequent, and Fine-grained ($F^3$) events presents a significant challenge in video analytics and multi-modal LLMs. Current methods struggle to identify events that satisfy all the $F^3$ criteria with high accuracy due to challenges such as motion blur and subtle visual... | Jin Song Dong, Kan Jiang, Murong Ma, Yun Lin, Zhaoyu Liu, Zhe Hou |  |
| 2083 |  |  [Training Large Language Models for Retrieval-Augmented Question Answering through Backtracking Correction](https://openreview.net/forum?id=IOg47mg74i) |  | 0 | Despite recent progress in Retrieval-Augmented Generation (RAG) achieved by large language models (LLMs), retrievers often recall uncorrelated documents, regarded as "noise" during subsequent text generation. To address this, some methods train LLMs to distinguish between relevant and irrelevant... | Huawen Feng, Junhao Zheng, Qianli Ma, Zekun Yao |  |
| 2084 |  |  [INS: Interaction-aware Synthesis to Enhance Offline Multi-agent Reinforcement Learning](https://openreview.net/forum?id=kxD2LlPr40) |  | 0 | Data scarcity in offline multi-agent reinforcement learning (MARL) is a key challenge for real-world applications. Recent advances in offline single-agent reinforcement learning (RL) demonstrate the potential of data synthesis to mitigate this issue. However, in multi-agent systems, interactions... | Dongbin Zhao, Jiajun Chai, Jian Zhao, Yuanheng Zhu, Yuqian Fu |  |
| 2085 |  |  [ϕ-Update: A Class of Policy Update Methods with Policy Convergence Guarantee](https://openreview.net/forum?id=fh7GYa7cjO) |  | 0 | Inspired by the similar update pattern of softmax natural policy gradient and Hadamard policy gradient, we propose to study a general policy update rule called $\phi$-update, where $\phi$ refers to a scaling function on advantage functions. Under very mild conditions on $\phi$, the global... | Jiacai Liu, Ke Wei, Wenye Li |  |
| 2086 |  |  [SAVA: Scalable Learning-Agnostic Data Valuation](https://openreview.net/forum?id=0UCoWxPhQ4) |  | 0 | Selecting data for training machine learning models is crucial since large, web-scraped, real datasets contain noisy artifacts that affect the quality and relevance of individual data points. These noisy artifacts will impact model performance. We formulate this problem as a data valuation task,... | Samuel Kessler, Tam Le, Vu Nguyen |  |
| 2087 |  |  [Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving](https://openreview.net/forum?id=VNckp7JEHn) |  | 0 | While the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored. We study inference scaling laws (aka test-time scaling laws) and compute-optimal inference, focusing on the trade-offs between model sizes... | Sean Welleck, Shanda Li, Yangzhen Wu, Yiming Yang, Zhiqing Sun |  |
| 2088 |  |  [Can Textual Gradient Work in Federated Learning?](https://openreview.net/forum?id=Cy5IKvYbR3) |  | 0 | Recent studies highlight the promise of LLM-based prompt optimization, especially with TextGrad, which automates \`\`differentiation'' via texts and backpropagates textual feedback provided by LLMs. This approach facilitates training in various real-world applications that do not support numerical... | Han Yu, Minghui Chen, Ruinan Jin, Wenlong Deng, Xiaoxiao Li, Yuanyuan Chen, Zhi Huang |  |
| 2089 |  |  [How Low Can You Go? Searching for the Intrinsic Dimensionality of Complex Networks using Metric Node Embeddings](https://openreview.net/forum?id=V71ITh2w40) |  | 0 | Low-dimensional embeddings are essential for machine learning tasks involving graphs, such as node classification, link prediction, community detection, network visualization, and network compression. Although recent studies have identified exact low-dimensional embeddings, the limits of the... | Andreas Lyhne Fiehn, Morten Mørup, Niels Raunkjær Holm, Nikolaos Nakis |  |
| 2090 |  |  [Synthesizing Realistic fMRI: A Physiological Dynamics-Driven Hierarchical Diffusion Model for Efficient fMRI Acquisition](https://openreview.net/forum?id=zZ6TT254Np) |  | 0 | Functional magnetic resonance imaging (fMRI) is essential for mapping brain activity but faces challenges like lengthy acquisition time and sensitivity to patient movement, limiting its clinical and machine learning applications. While generative models such as diffusion models can synthesize fMRI... | Wuyang Li, Yixuan Yuan, Yu Jiang, Yufan Hu |  |
| 2091 |  |  [MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation](https://openreview.net/forum?id=br4H61LOoI) |  | 0 | In this work, we introduce a novel evaluation paradigm for Large Language Models (LLMs) that compels them to transition from a traditional question-answering role, akin to a student, to a solution-scoring role, akin to a teacher. This paradigm, focusing on "reasoning about reasoning," termed... | Haiyun Jiang, Jiaya Jia, Pengguang Chen, Shu Liu, Zhongshen Zeng |  |
| 2092 |  |  [Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?](https://openreview.net/forum?id=8EtSBX41mt) |  | 0 | Large Language Models (LLMs) show impressive results in numerous practical applications, but they lack essential safety features that are common in other areas of computer science, particularly an explicit separation of instructions and data. This makes them vulnerable to manipulations such as... | Christoph H. Lampert, Egor Zverev, Mario Fritz, Sahar Abdelnabi, Soroush Tabesh |  |
| 2093 |  |  [Risk-Sensitive Diffusion: Robustly Optimizing Diffusion Models with Noisy Samples](https://openreview.net/forum?id=b0WpXBABdu) |  | 0 | Diffusion models are mainly studied on image data. However, non-image data (e.g., tabular data) are also prevalent in real applications and tend to be noisy due to some inevitable factors in the stage of data collection, degrading the generation quality of diffusion models. In this paper, we... | Max Ruiz Luyten, Mihaela van der Schaar, Yangming Li |  |
| 2094 |  |  [Adversarial Mixup Unlearning](https://openreview.net/forum?id=GcbhbZsgiu) |  | 0 | Machine unlearning is a critical area of research aimed at safeguarding data privacy by enabling the removal of sensitive information from machine learning models. One unique challenge in this field is catastrophic unlearning, where erasing specific data from a well-trained model unintentionally... | Yi Yang, Yixuan Tang, Zhuoyi Peng |  |
| 2095 |  |  [Learning on One Mode: Addressing Multi-modality in Offline Reinforcement Learning](https://openreview.net/forum?id=upkxzurnLC) |  | 0 | Offline reinforcement learning (RL) seeks to learn optimal policies from static datasets without interacting with the environment. A common challenge is handling multi-modal action distributions, where multiple behaviours are represented in the data. Existing methods often assume unimodal behaviour... | Giovanni Montana, Mianchu Wang, Yue Jin |  |
| 2096 |  |  [GraphArena: Evaluating and Exploring Large Language Models on Graph Computation](https://openreview.net/forum?id=Y1r9yCMzeA) |  | 0 | The \`\`arms race'' of Large Language Models (LLMs) demands new benchmarks to examine their progresses. In this paper, we introduce GraphArena, a benchmarking tool designed to evaluate LLMs on real-world graph computational problems. It offers a suite of four polynomial-time tasks (e.g., Shortest... | Jia Li, Jianheng Tang, Nuo Chen, Qifan Zhang, Yuhan Li |  |
| 2097 |  |  [Continuous Ensemble Weather Forecasting with Diffusion models](https://openreview.net/forum?id=ePEZvQNFDW) |  | 0 | Weather forecasting has seen a shift in methods from numerical simulations to data-driven systems. While initial research in the area focused on deterministic forecasting, recent works have used diffusion models to produce skillful ensemble forecasts. These models are trained on a single... | Fredrik Lindsten, Joel Oskarsson, Martin Andrae, Tomas Landelius |  |
| 2098 |  |  [VTDexManip: A Dataset and Benchmark for Visual-tactile Pretraining and Dexterous Manipulation with Reinforcement Learning](https://openreview.net/forum?id=jf7C7EGw21) |  | 0 | Vision and touch are the most commonly used senses in human manipulation. While leveraging human manipulation videos for robotic task pretraining has shown promise in prior works, it is limited to image and language modalities and deployment to simple parallel grippers. In this paper, aiming to... | Gaofeng Li, Jiming Chen, Qi Ye, Qingtao Liu, Yu Cui, Zhengnan Sun |  |
| 2099 |  |  [Breaking Free from MMI: A New Frontier in Rationalization by Probing Input Utilization](https://openreview.net/forum?id=WZ0s2smcKP) |  | 0 | Extracting a small subset of crucial rationales from the full input is a key problem in explainability research. The most widely used fundamental criterion for rationale extraction is the maximum mutual information (MMI) criterion. In this paper, we first demonstrate that MMI suffers from... | Haozhao Wang, Jun Wang, Ruixuan Li, Wei Liu, Zhigang Zeng, Zhiying Deng, Zhongyu Niu |  |
| 2100 |  |  [HiBug2: Efficient and Interpretable Error Slice Discovery for Comprehensive Model Debugging](https://openreview.net/forum?id=l30moNjSY9) |  | 0 | Despite the significant success of deep learning models in computer vision, they often exhibit systematic failures on specific data subsets, known as error slices. Identifying and mitigating these error slices is crucial to enhancing model robustness and reliability in real-world scenarios. In this... | Chenchen Zhao, Muxi Chen, Qiang Xu |  |
| 2101 |  |  [LiveXiv - A Multi-Modal live benchmark based on Arxiv papers content](https://openreview.net/forum?id=SulRfnEVK4) |  | 0 | The large-scale training of multi-modal models on data scraped from the web has shown outstanding utility in infusing these models with the required world knowledge to perform effectively on multiple downstream tasks. However, one downside of scraping data from the web can be the potential... | Assaf Arbelle, Felipe Maia Polo, Leonid Karlinsky, Leshem Choshen, Mikhail Yurochkin, Muhammad Jehanzeb Mirza, Nimrod Shabtay, Raja Giryes, Sivan Doveh, Wei Lin, Yuekai Sun |  |
| 2102 |  |  [SEPARATE: A Simple Low-rank Projection for Gradient Compression in Modern Large-scale Model Training Process](https://openreview.net/forum?id=8HuLgtjqOD) |  | 0 | Training Large Language Models (LLMs) presents a significant communication bottleneck, predominantly due to the growing scale of the gradient to communicate across multi-device clusters. However, how to mitigate communication overhead in practice remains a formidable challenge due to the weakness... | Cong Fang, Hanzhen Zhao, Xingyu Xie, Zhouchen Lin |  |
| 2103 |  |  [FIRING-Net: A filtered feature recycling network for speech enhancement](https://openreview.net/forum?id=TJp3LnQgSX) |  | 0 | Current deep neural networks for speech enhancement (SE) aim to minimize the distance between the output signal and the clean target by filtering out noise features from input features. However, when noise and speech components are highly similar, SE models struggle to learn effective... | Jizhen Li, Weiping Tu, Xinmeng Xu, Yiqun Zhang, Yong Luo, Yuhong Yang |  |
| 2104 |  |  [Conformal Generative Modeling with Improved Sample Efficiency through Sequential Greedy Filtering](https://openreview.net/forum?id=1i6lkavJ94) |  | 0 | Generative models lack rigorous statistical guarantees with respect to their predictions. In this work, we propose Sequential Conformal Prediction for Generative Models (SCOPE-Gen), a sequential conformal prediction method producing prediction sets that satisfy a rigorous statistical guarantee... | Bernhard Schölkopf, KlausRudolf Kladny, Michael Muehlebach |  |
| 2105 |  |  [Federated Continual Learning Goes Online: Uncertainty-Aware Memory Management for Vision Tasks and Beyond](https://openreview.net/forum?id=f65RuQgVlp) |  | 0 | Given the ability to model more realistic and dynamic problems, Federated Continual Learning (FCL) has been increasingly investigated recently. A well-known problem encountered in this setting is the so-called catastrophic forgetting, for which the learning model is inclined to focus on more recent... | Florian Buettner, Giuseppe Serra |  |
| 2106 |  |  [GaussianBlock: Building Part-Aware Compositional and Editable 3D Scene by Primitives and Gaussians](https://openreview.net/forum?id=wrXCIsysqB) |  | 0 | Recently, with the development of Neural Radiance Fields and Gaussian Splatting, 3D reconstruction techniques have achieved remarkably high fidelity. However, the latent representations learnt by these methods are highly entangled and lack interpretability. In this paper, we propose a novel... | De Wen Soh, Hossein Rahmani, Jun Liu, Na Zhao, Qihao Zhao, Shuyi Jiang |  |
| 2107 |  |  [Equivariant Masked Position Prediction for Efficient Molecular Representation](https://openreview.net/forum?id=Nue5iMj8n6) |  | 0 | Graph neural networks (GNNs) have shown considerable promise in computational chemistry. However, the limited availability of molecular data raises concerns regarding GNNs' ability to effectively capture the fundamental principles of physics and chemistry, which constrains their generalization... | Chao Qu, Fenglei Cao, Junyi An, Qianwei Tang, Xinhao Liu, Yuan Qi, Yunfei Shi |  |
| 2108 |  |  [Towards Understanding the Universality of Transformers for Next-Token Prediction](https://openreview.net/forum?id=yWoV4Ca6ji) |  | 0 | Causal Transformers are trained to predict the next token for a given context. While it is widely accepted that self-attention is crucial for encoding the causal structure of sequences, the precise underlying mechanism behind this in-context autoregressive learning ability remains unclear. In this... | Gabriel Peyré, Michael Eli Sander |  |
| 2109 |  |  [A-Bench: Are LMMs Masters at Evaluating AI-generated Images?](https://openreview.net/forum?id=4muXQ5r8Ol) |  | 0 | How to accurately and efficiently assess AI-generated images (AIGIs) remains a critical challenge for generative models. Given the high costs and extensive time commitments required for user studies, many researchers have turned towards employing large multi-modal models (LMMs) as AIGI evaluators,... | Chunyi Li, Guangtao Zhai, Haoning Wu, Wei Sun, Weisi Lin, Xiaohong Liu, Xiongkuo Min, Yingjie Zhou, Zicheng Zhang, Zijian Chen |  |
| 2110 |  |  [IDInit: A Universal and Stable Initialization Method for Neural Network Training](https://openreview.net/forum?id=LFiaoYnP6T) |  | 0 | Deep neural networks have achieved remarkable accomplishments in practice. The success of these networks hinges on effective initialization methods, which are vital for ensuring stable and rapid convergence during training. Recently, initialization methods that maintain identity transition within... | Chaozheng Wang, Min Zhang, Qifan Wang, Yu Pan, Zekai Wu, Zenglin Xu |  |
| 2111 |  |  [Tighter Privacy Auditing of DP-SGD in the Hidden State Threat Model](https://openreview.net/forum?id=xzKFnsJIXL) |  | 0 | Machine learning models can be trained with formal privacy guarantees via differentially private optimizers such as DP-SGD. In this work, we focus on a threat model where the adversary has access only to the final model, with no visibility into intermediate updates. In the literature, this... | Aurélien Bellet, Nicolas Papernot, Tudor Ioan Cebere |  |
| 2112 |  |  [Improving Complex Reasoning with Dynamic Prompt Corruption: A Soft Prompt Optimization Approach](https://openreview.net/forum?id=h7Qz1ulnvF) |  | 0 | Prompt Tuning (PT) has emerged as a promising Parameter-Efficient Fine-Tuning (PEFT) approach by appending trainable continuous prompt vectors to the input, maintaining competitive performance with significantly fewer trainable parameters. While PT has shown effectiveness in enhancing task... | Chen Shen, Chenxi Huang, Ge Teng, Jieping Ye, Liang Xie, Sinan Fan, Wenxiao Wang, Xiaofei He, Xiaofeng Zhang, Xiaosong Yuan |  |
| 2113 |  |  [DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework for Talking head Video Generation](https://openreview.net/forum?id=vjHySpxDsv) |  | 0 | Talking head generation intends to produce vivid and realistic talking head videos from a single portrait and speech audio clip. Although significant progress has been made in diffusion-based talking head generation, almost all methods rely on autoregressive strategies, which suffer from limited... | Chenyu Liu, Hanbo Cheng, Jia Pan, Jiefeng Ma, Jun Du, Limin Lin, Pengcheng Xia, Pengfei Hu |  |
| 2114 |  |  [Precedence-Constrained Winter Value for Effective Graph Data Valuation](https://openreview.net/forum?id=tVRVE0OAyb) |  | 0 | Data valuation is essential for quantifying data’s worth, aiding in assessing data quality and determining fair compensation. While existing data valuation methods have proven effective in evaluating the value of Euclidean data, they face limitations when applied to the increasingly popular... | Charu C. Aggarwal, Hongliang Chi, Wei Jin, Yao Ma |  |
| 2115 |  |  [Federated Few-Shot Class-Incremental Learning](https://openreview.net/forum?id=ZiPoAlKf9Y) |  | 0 | This study proposes a challenging yet practical Federated Few-Shot Class-Incremental Learning (FFSCIL) problem, where clients only hold very few samples for new classes. We develop a novel Unified Optimized Prototype Prompt (UOPP) model to simultaneously handle catastrophic forgetting,... | Habibullah, Lin Liu, Mahardhika Pratama, Muhammad Anwar Ma'sum, Ryszard Kowalczyk |  |
| 2116 |  |  [GlycanML: A Multi-Task and Multi-Structure Benchmark for Glycan Machine Learning](https://openreview.net/forum?id=owEQ0FTfVj) |  | 0 | Glycans are basic biomolecules and perform essential functions within living organisms. The rapid increase of functional glycan data provides a good opportunity for machine learning solutions to glycan understanding. However, there still lacks a standard machine learning benchmark for glycan... | Jian Tang, Ling Yang, Minghao Xu, Wentao Zhang, Yihang Zhang, Yunteng Geng |  |
| 2117 |  |  [Optimistic Games for Combinatorial Bayesian Optimization with Application to Protein Design](https://openreview.net/forum?id=xiyzCfXTS6) |  | 0 | Bayesian optimization (BO) is a powerful framework to optimize black-box expensive-to-evaluate functions via sequential interactions. In several important problems (e.g. drug discovery, circuit design, neural architecture search, etc.), though, such functions are defined over large... | Andreas Krause, Melis Ilayda Bal, Mojmir Mutny, Pier Giuseppe Sessa |  |
| 2118 |  |  [Iterative Substructure Extraction for Molecular Relational Learning with Interactive Graph Information Bottleneck](https://openreview.net/forum?id=3kiZ5S5WkY) |  | 0 | Molecular relational learning (MRL) seeks to understand the interaction behaviors between molecules, a pivotal task in domains such as drug discovery and materials science. Recently, extracting core substructures and modeling their interactions have emerged as mainstream approaches within machine... | Alan Xia, Hongxin Xiang, Junfeng Fang, Shuai Zhang, Wenjie Du, Xuqiang Li, Yang Wang, Ye Wei |  |
| 2119 |  |  [Local-Prompt: Extensible Local Prompts for Few-Shot Out-of-Distribution Detection](https://openreview.net/forum?id=Ew3VifXaxZ) |  | 0 | Out-of-Distribution (OOD) detection, aiming to distinguish outliers from known categories, has gained prominence in practical scenarios. Recently, the advent of vision-language models (VLM) has heightened interest in enhancing OOD detection for VLM through few-shot tuning. However, existing methods... | Fanhu Zeng, Fei Zhu, Hongxin Wei, XuYao Zhang, Zhen Cheng |  |
| 2120 |  |  [QuaDiM: A Conditional Diffusion Model For Quantum State Property Estimation](https://openreview.net/forum?id=P7f55HQtV8) |  | 0 | Quantum state property estimation (QPE) is a fundamental challenge in quantum many-body problems in physics and chemistry, involving the prediction of characteristics such as correlation and entanglement entropy through statistical analysis of quantum measurement data. Recent advances in deep... | Junchi Yan, Mabiao Long, Yehui Tang |  |
| 2121 |  |  [Aligning Human Motion Generation with Human Perceptions](https://openreview.net/forum?id=QOHgjY5KDp) |  | 0 | Human motion generation is a critical task with a wide spectrum of applications. Achieving high realism in generated motions requires naturalness, smoothness, and plausibility. However, current evaluation metrics often rely on simple heuristics or distribution distances and do not align well with... | Feng Gao, Haoru Wang, Luyi Miao, Qi Tian, Wentao Zhu, Yishu Xu, Yizhou Wang |  |
| 2122 |  |  [Centrality-guided Pre-training for Graph](https://openreview.net/forum?id=X8E65IxA73) |  | 0 | Self-supervised learning (SSL) has shown great potential in learning generalizable representations for graph-structured data. However, existing SSL-based graph pre-training methods largely focus on improving graph representations by learning the structure information based on disturbing or... | Bin Liang, Hui Wang, KamFai Wong, Lin Gui, Ruifeng Xu, Shiwei Chen, Yue Yu |  |
| 2123 |  |  [Vision and Language Synergy for Rehearsal Free Continual Learning](https://openreview.net/forum?id=9aZ2ixiYGd) |  | 0 | The prompt-based approach has demonstrated its success for continual learning problems. However, it still suffers from catastrophic forgetting due to inter-task vector similarity and unfitted new components of previously learned tasks. On the other hand, the language-guided approach falls short of... | Habibullah, Lin Liu, Mahardhika Pratama, Muhammad Anwar Ma'sum, Ryszard Kowalczyk, Savitha Ramasamy |  |
| 2124 |  |  [Towards a General Time Series Anomaly Detector with Adaptive Bottlenecks and Dual Adversarial Decoders](https://openreview.net/forum?id=aKcd7ImG5e) |  | 0 | Time series anomaly detection plays a vital role in a wide range of applications. Existing methods require training one specific model for each dataset, which exhibits limited generalization capability across different target datasets, hindering anomaly detection performance in various scenarios... | Beibu Li, Bin Yang, Chenjuan Guo, Kai Zhao, Lujia Pan, Qichao Shentu, Yang Shu, Zhongwen Rao |  |
| 2125 |  |  [Finding Shared Decodable Concepts and their Negations in the Brain](https://openreview.net/forum?id=L07zWidgdW) |  | 0 | Prior work has offered evidence for functional localization in the brain; different anatomical regions preferentially activate for certain types of visual input. For example, the fusiform face area preferentially activates for visual stimuli that include a face. However, the spectrum of visual... | Alex Murphy, Alona Fyshe, Cory Daniel Efird, Joel Zylberberg |  |
| 2126 |  |  [A Conditional Independence Test in the Presence of Discretization](https://openreview.net/forum?id=gqbbL7k8BF) |  | 0 | Testing conditional independence (CI) has many important applications, such as Bayesian network learning and causal discovery. Although several approaches have been developed for learning CI structures for observed variables, those existing methods generally fail to work when the variables of... | Boyang Sun, GuangYuan Hao, Kun Zhang, Yu Yao, Yumou Qiu |  |
| 2127 |  |  [Innovative Thinking, Infinite Humor: Humor Research of Large Language Models through Structured Thought Leaps](https://openreview.net/forum?id=CGhgB8Kz8i) |  | 0 | Humor is previously regarded as a gift exclusive to humans for the following reasons. Humor is a culturally nuanced aspect of human language, presenting challenges for its understanding and generation. Humor generation necessitates a multi-hop reasoning process, with each hop founded on proper... | Dian Li, Han Wang, Hui Wang, Sinbadliu, Xiaohan Wang, Xuguang Lan, Yilin Zhao |  |
| 2128 |  |  [Fantastic Targets for Concept Erasure in Diffusion Models and Where To Find Them](https://openreview.net/forum?id=tZdqL5FH7w) |  | 0 | Concept erasure has emerged as a promising technique for mitigating the risk of harmful content generation in diffusion models by selectively unlearning undesirable concepts. The common principle of previous works to remove a specific concept is to map it to a fixed generic concept, such as a... | Anh Tuan Bui, Dinh Phung, Junae Kim, Long Tung Vuong, Paul Montague, Tamas Abraham, ThuyTrang Vu, Trung Le |  |
| 2129 |  |  [PhyloVAE: Unsupervised Learning of Phylogenetic Trees via Variational Autoencoders](https://openreview.net/forum?id=Z8TglKXDWm) |  | 0 | Learning informative representations of phylogenetic tree structures is essential for analyzing evolutionary relationships. Classical distance-based methods have been widely used to project phylogenetic trees into Euclidean space, but they are often sensitive to the choice of distance metric and... | Cheng Zhang, Frederick A. Matsen IV, Harry Richman, Jiansi Gao, Tianyu Xie |  |
| 2130 |  |  [Self-Evolved Reward Learning for LLMS](https://openreview.net/forum?id=Zonhl0c9I0) |  | 0 | Reinforcement Learning from Human Feedback (RLHF) is a crucial technique for aligning language models with human preferences and is a key factor in the success of modern conversational models like GPT-4, ChatGPT, and Llama 2. A significant challenge in employing RLHF lies in training a reliable RM,... | Chenghua Huang, Dongmei Zhang, Fangkai Yang, Lu Wang, Pu Zhao, Qi Zhang, Qingwei Lin, Saravan Rajmohan, Zeqi Lin, Zhizhen Fan |  |
| 2131 |  |  [Long-time asymptotics of noisy SVGD outside the population limit](https://openreview.net/forum?id=X7eAhXcps1) |  | 0 | Stein Variational Gradient Descent (SVGD) is a widely used sampling algorithm that has been successfully applied in several areas of Machine Learning. SVGD operates by iteratively moving a set of $n$ interacting particles (which represent the samples) to approximate the target distribution. Despite... | Adil Salim, Pascal Bianchi, Victor Priser |  |
| 2132 |  |  [The Computational Complexity of Positive Non-Clashing Teaching in Graphs](https://openreview.net/forum?id=Jd3Vd7GCyq) |  | 0 | We study the classical and parameterized complexity of computing the positive non-clashing teaching dimension of a set of concepts, that is, the smallest number of examples per concept required to successfully teach an intelligent learner under the considered, previously established model. For any... | Fionn Mc Inerney, Liana Khazaliya, Mathis Rocton, Robert Ganian |  |
| 2133 |  |  [VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents](https://openreview.net/forum?id=2snKOc7TVp) |  | 0 | Large Multimodal Models (LMMs) have ushered in a new era in artificial intelligence, merging capabilities in both language and vision to form highly capable \textbf{Visual Foundation Agents} that are postulated to excel across a myriad of tasks. However, existing benchmarks fail to sufficiently... | Aohan Zeng, Chan Hee Song, Hanchen Zhang, Hanyu Lai, Hao Yu, Iat Long Iong, Jiadai Sun, Jie Tang, Lihang Pan, Ming Ding, Qinkai Zheng, Shudan Zhang, Shuntian Yao, Siyi Cheng, Tianjie Zhang, Wenyi Hong, Xiao Liu, Xiaotao Gu, Xinyue Yang, Xixuan Song, Xueqiao Sun, Yifan Xu, Yu Gu, Yu Su, Yu Yang, Yuxiao Dong, Zehan Qi, Zhengxiao Du |  |
| 2134 |  |  [Rethinking Multiple-Instance Learning From Feature Space to Probability Space](https://openreview.net/forum?id=torbeUlslS) |  | 0 | Multiple-instance learning (MIL) was initially proposed to identify key instances within a set (bag) of instances when only one bag-level label is provided. Current deep MIL models mostly solve multi-instance problem in feature space. Nevertheless, with the increasing complexity of data, we found... | Jing Gu, Licheng Jiao, Mengnan Qi, Shasha Mao, Xuequan Lu, Yimeng Zhang, Zhaolong Du |  |
| 2135 |  |  [Attribute-based Visual Reprogramming for Vision-Language Models](https://openreview.net/forum?id=j964C6y92q) |  | 0 | \*Visual reprogramming\* (VR) reuses pre-trained vision models for downstream image classification tasks by adding trainable noise patterns to inputs. When applied to vision-language models (e.g., CLIP), existing VR approaches follow the same pipeline used in vision models (e.g., ResNet, ViT),... | Chengyi Cai, Feng Liu, Jianzhong Qi, Lei Feng, Zesheng Ye |  |
| 2136 |  |  [Inverse Constitutional AI: Compressing Preferences into Principles](https://openreview.net/forum?id=9FRwkPw3Cn) |  | 0 | Feedback data is widely used for fine-tuning and evaluating state-of-the-art AI models. Pairwise text preferences, where human or AI annotators select the “better” of two options, are particularly common. Such preferences are used to train (reward) models or to rank models with aggregate... | Arduin Findeis, Eyke Hüllermeier, Robert D. Mullins, Samuel Albanie, Timo Kaufmann |  |
| 2137 |  |  [Spreading Out-of-Distribution Detection on Graphs](https://openreview.net/forum?id=p1TBYyqy8v) |  | 0 | Node-level out-of-distribution (OOD) detection on graphs has received significant attention from the machine learning community. However, previous approaches are evaluated using unrealistic benchmarks that consider only randomly selected OOD nodes, failing to reflect the interactions among nodes.... | Daeho Um, Jongin Lim, Sunoh Kim, Yoonho Jung, Yuneil Yeo |  |
| 2138 |  |  [Why Does the Effective Context Length of LLMs Fall Short?](https://openreview.net/forum?id=eoln5WgrPx) |  | 0 | Advancements in distributed training and efficient attention mechanisms have significantly expanded the context window sizes of large language models (LLMs). However, recent work reveals that the effective context lengths of open-source LLMs often fall short, typically not exceeding half of their... | Chenxin An, Jingjing Xu, Jun Zhang, Lei Li, Lingpeng Kong, Ming Zhong, Shansan Gong, Yao Luo |  |
| 2139 |  |  [Don't Take Things Out of Context: Attention Intervention for Enhancing Chain-of-Thought Reasoning in Large Language Models](https://openreview.net/forum?id=W6yIKliMot) |  | 0 | Few-shot Chain-of-Thought (CoT) significantly enhances the reasoning capabilities of large language models (LLMs), functioning as a whole to guide these models in generating reasoning steps toward final answers. However, we observe that isolated segments, words, or tokens within CoT demonstrations... | Chen Shen, Jieping Ye, Junjie Liu, Liang Xie, Shaotian Yan, Wenxiao Wang |  |
| 2140 |  |  [Towards Self-Supervised Covariance Estimation in Deep Heteroscedastic Regression](https://openreview.net/forum?id=Q1kPHLUbhi) |  | 0 | Deep heteroscedastic regression models the mean and covariance of the target distribution through neural networks. The challenge arises from heteroscedasticity, which implies that the covariance is sample dependent and is often unknown. Consequently, recent methods learn the covariance through... | Alexandre Alahi, Aziz Shameem, Mathieu Salzmann, Megh Shukla |  |
| 2141 |  |  [G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model](https://openreview.net/forum?id=px1674Wp3C) |  | 0 | Large language models (LLMs) have shown remarkable proficiency in human-level reasoning and generation capabilities, which encourages extensive research on their application in mathematical problem solving. However, current work has been largely focused on text-based mathematical problems, with... | Hang Xu, Jiacheng Ye, Jiahui Gao, Jianhua Han, Jipeng Zhang, Lanqing Hong, Lingpeng Kong, Renjie Pi, Wanjun Zhong, Yufei Wang, Zhenguo Li |  |
| 2142 |  |  [Complementary Label Learning with Positive Label Guessing and Negative Label Enhancement](https://openreview.net/forum?id=LPRxGZ7Oax) |  | 0 | Complementary label learning (CLL) is a weakly supervised learning paradigm that constructs a multi-class classifier only with complementary labels, specifying classes that the instance does not belong to. We reformulate CLL as an inverse problem that infers the full label information from the... | Yuhang Li, Yuheng Jia, Zhuying Li |  |
| 2143 |  |  [Emergence of a High-Dimensional Abstraction Phase in Language Transformers](https://openreview.net/forum?id=0fD3iIBhlV) |  | 0 | A language model (LM) is a mapping from a linguistic context to an output token. However, much remains to be known about this mapping, including how its geometric properties relate to its function. We take a high-level geometric approach to its analysis, observing, across five pre-trained... | Alessandro Laio, Corentin Kervadec, Diego Doimo, Emily Cheng, Iuri Macocco, Lei Yu, Marco Baroni |  |
| 2144 |  |  [Collaborative Discrete-Continuous Black-Box Prompt Learning for Language Models](https://openreview.net/forum?id=sdLGY9Dj5r) |  | 0 | Large Scale Pre-Trained Language Models (PTMs) have demonstrated unprecedented capabilities across diverse natural language processing tasks. Adapting such models to downstream tasks is computationally intensive and time-consuming, particularly in black-box scenarios common in... | Bin Gu, Haozhen Zhang, Hualin Zhang, Yi Chang, Zhekai Liu |  |
| 2145 |  |  [Relation-Aware Diffusion for Heterogeneous Graphs with Partially Observed Features](https://openreview.net/forum?id=TPYwwqF0bv) |  | 0 | Diffusion-based imputation methods, which impute missing features through the iterative propagation of observed features, have shown impressive performance in homogeneous graphs. However, these methods are not directly applicable to heterogeneous graphs, which have multiple types of nodes and... | Daeho Um, Jiwoong Park, SeongJin Ahn, Seulki Park, Yoonji Lee, Yuneil Yeo |  |
| 2146 |  |  [Rotated Runtime Smooth: Training-Free Activation Smoother for accurate INT4 inference](https://openreview.net/forum?id=WG7GzGx3G9) |  | 0 | Large language models have demonstrated promising capabilities upon scaling up parameters. However, serving large language models incurs substantial computation and memory movement costs due to their large scale. Quantization methods have been employed to reduce service costs and latency.... | Chengyuan Li, Jianwei Zhang, Jingren Zhou, Junyang Lin, Ke Yi, Tong Zhang, Zengke Liu |  |
| 2147 |  |  [Conditional Diffusion Models are Minimax-Optimal and Manifold-Adaptive for Conditional Distribution Estimation](https://openreview.net/forum?id=NltQraRnbW) |  | 0 | We consider a class of conditional forward-backward diffusion models for conditional generative modeling, that is, generating new data given a covariate (or control variable). To formally study the theoretical properties of these conditional generative models, we adopt a statistical framework of... | Lizhen Lin, Rong Tang, Yun Yang |  |
| 2148 |  |  [Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance](https://openreview.net/forum?id=jjCB27TMK3) |  | 0 | Pretraining data of large language models composes multiple domains (e.g., web texts, academic papers, codes), whose mixture proportions crucially impact the competence of outcome models. While existing endeavors rely on heuristics or qualitative strategies to tune the proportions, we discover the... | Jiasheng Ye, Jun Zhan, Peiju Liu, Tianxiang Sun, Xipeng Qiu, Yunhua Zhou |  |
| 2149 |  |  [econSG: Efficient and Multi-view Consistent Open-Vocabulary 3D Semantic Gaussians](https://openreview.net/forum?id=qSEEQPNbu4) |  | 0 | The primary focus of most recent works on open-vocabulary neural fields is extracting precise semantic features from the VLMs and then consolidating them efficiently into a multi-view consistent 3D neural fields representation. However, most existing works over-trusted SAM to regularize image-level... | Can Zhang, Gim Hee Lee |  |
| 2150 |  |  [AgentStudio: A Toolkit for Building General Virtual Agents](https://openreview.net/forum?id=axUf8BOjnH) |  | 0 | General virtual agents need to handle multimodal observations, master complex action spaces, and self-improve in dynamic, open-domain environments. However, existing environments are often domain-specific and require complex setups, which limits agent development and evaluation in real-world... | Bo An, Longtao Zheng, Shuicheng Yan, Xinrun Wang, Zhenghai Xue, Zhiyuan Huang |  |
| 2151 |  |  [EVA: Geometric Inverse Design for Fast Protein Motif-Scaffolding with Coupled Flow](https://openreview.net/forum?id=KHkBpvmYVI) |  | 0 | Motif-scaffolding is a fundamental component of protein design, which aims to construct the scaffold structure that stabilizes motifs conferring desired functions. Recent advances in generative models are promising for designing scaffolds, with two main approaches: training-based and sampling-based... | Cheng Tan, Haitao Lin, Lirong Wu, Odin Zhang, Siyuan Li, Stan Z. Li, Tailin Wu, Yufei Huang, Yunfan Liu, Yunshu Liu, Zhangyang Gao, Zicheng Liu |  |
| 2152 |  |  [Multi-Perspective Data Augmentation for Few-shot Object Detection](https://openreview.net/forum?id=qG0WCAhZE0) |  | 0 | Recent few-shot object detection (FSOD) methods have focused on augmenting synthetic samples for novel classes, show promising results to the rise of diffusion models. However, the diversity of such datasets is often limited in representativeness because they lack awareness of typical and hard... | AnhKhoa Nguyen Vu, QuocTruong Truong, Tam V. Nguyen, Thanh Duc Ngo, ThanhToan Do, VinhTiep Nguyen |  |
| 2153 |  |  [Intrinsic User-Centric Interpretability through Global Mixture of Experts](https://openreview.net/forum?id=wDcunIOAOk) |  | 0 | In human-centric settings like education or healthcare, model accuracy and model explainability are key factors for user adoption. Towards these two goals, intrinsically interpretable deep learning models have gained popularity, focusing on accurate predictions alongside faithful explanations.... | Jibril Frej, Julian Blackwell, Martin Jaggi, Syrielle Montariol, Tanja Käser, Vinitra Swamy |  |
| 2154 |  |  [VCR: A Task for Pixel-Level Complex Reasoning in Vision Language Models via Restoring Occluded Text](https://openreview.net/forum?id=s0Z4csHOoE) |  | 0 | We introduce Visual Caption Restoration (VCR), a novel vision-language task that challenges models to accurately restore partially obscured texts using pixel-level hints within images through complex reasoning. This task stems from the observation that text embedded in images intrinsically differs... | Bang Liu, Ge Zhang, Jie Fu, Lu Li, Perouz Taslakian, Sai Rajeswar, Suyuchen Wang, Tianyu Zhang, Yoshua Bengio |  |
| 2155 |  |  [Mitigate the Gap: Improving Cross-Modal Alignment in CLIP](https://openreview.net/forum?id=aPTGvFqile) |  | 0 | Contrastive Language--Image Pre-training (CLIP) has manifested remarkable improvements in zero-shot classification and cross-modal vision-language tasks. Yet, from a geometrical point of view, the CLIP embedding space has been found to have a pronounced modality gap. This gap renders the embedding... | Gerard de Melo, Sedigheh Eslami |  |
| 2156 |  |  [A Tight Convergence Analysis of Inexact Stochastic Proximal Point Algorithm for Stochastic Composite Optimization Problems](https://openreview.net/forum?id=n3TkrH7fEr) |  | 0 | The \textbf{i}nexact \textbf{s}tochastic \textbf{p}roximal \textbf{p}oint \textbf{a}lgorithm (isPPA) is popular for solving stochastic composite optimization problems with many applications in machine learning. While the convergence theory of the (inexact) PPA has been well established, the known... | Chenglong Bao, Defeng Sun, Shulan Zhu, Yancheng Yuan |  |
| 2157 |  |  [SparsyFed: Sparse Adaptive Federated Learning](https://openreview.net/forum?id=OBUQNASaWw) |  | 0 | Sparse training is often adopted in cross-device federated learning (FL) environments where constrained devices collaboratively train a machine learning model on private data by exchanging pseudo-gradients across heterogeneous networks. Although sparse training methods can reduce communication... | Adriano Guastella, Alessio Mora, Alex Iacob, Lorenzo Sani, Nicholas Donald Lane, Paolo Bellavista |  |
| 2158 |  |  [MambaPEFT: Exploring Parameter-Efficient Fine-Tuning for Mamba](https://openreview.net/forum?id=UAKnJMIBwf) |  | 0 | An ecosystem of Transformer-based models has been established by building large models with extensive data. Parameter-efficient fine-tuning (PEFT) is a crucial technology for deploying these models to downstream tasks with minimal cost while achieving effective performance. Recently, Mamba, a State... | Masakazu Yoshimura, Teruaki Hayashi, Yota Maeda |  |
| 2159 |  |  [DiffGAD: A Diffusion-based Unsupervised Graph Anomaly Detector](https://openreview.net/forum?id=AhcYq4CnfF) |  | 0 | Graph Anomaly Detection (GAD) is crucial for identifying abnormal entities within networks, garnering significant attention across various fields. Traditional unsupervised methods, which decode encoded latent representations of unlabeled data with a reconstruction focus, often fail to capture... | Congcong Wen, Hui Lin, Jinda Lu, Jinghan Li, Junfeng Fang, Xiang Wang, Yuan Gao |  |
| 2160 |  |  [On the Relation between Trainability and Dequantization of Variational Quantum Learning Models](https://openreview.net/forum?id=TdqaZbQvdi) |  | 0 | Quantum machine learning (QML) explores the potential advantages of quantum computers for machine learning tasks, with variational QML among the main current approaches. While quantum computers promise to solve problems that are classically intractable, it has been recently shown that a particular... | Adrián PérezSalinas, Casper Gyurik, Elies GilFuster, Vedran Dunjko |  |
| 2161 |  |  [Regularizing Energy among Training Samples for Out-of-Distribution Generalization](https://openreview.net/forum?id=Lbx9zdURxe) |  | 0 | The energy-based model provides a unified framework for various learning models where an energy value is assigned to each configuration of random variables based on probability. Recently, different methods have been proposed to derive an energy value out of the logits of a classifier for... | Junchi Yan, Qitian Wu, Yiting Chen |  |
| 2162 |  |  [Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks](https://openreview.net/forum?id=s7lzZpAW7T) |  | 0 | Multimodal foundation models, such as Gemini and ChatGPT, have revolutionized human-machine interactions by seamlessly integrating various forms of data. Developing a universal spoken language model that comprehends a wide range of natural language instructions is critical for bridging... | Andy T. Liu, Anuj Diwan, ChenAn Li, ChiYuan Hsiao, Chienyu Huang, ChihKai Yang, ChunYi Kuan, Fabian Alejandro Ritter Gutierrez, Jiatong Shi, KaiWei Chang, KeHan Lu, Puyuan Peng, ShihHeng Wang, ShuWen Yang, WeiCheng Tseng, WeiChih Chen, William Chen, Xuanjun Chen, YiJen Shih, YuXiang Lin, et al. |  |
| 2163 |  |  [MMFakeBench: A Mixed-Source Multimodal Misinformation Detection Benchmark for LVLMs](https://openreview.net/forum?id=D6zn6ozJs7) |  | 0 | Current multimodal misinformation detection (MMD) methods often assume a single source and type of forgery for each sample, which is insufficient for real-world scenarios where multiple forgery sources coexist. The lack of a benchmark for mixed-source misinformation has hindered progress in this... | Huaibo Huang, Linzhi Huang, PeiPei Li, Shuhan Xia, Weihong Deng, Xing Cui, Xuannan Liu, Zekun Li, Zhaofeng He |  |
| 2164 |  |  [DoF: A Diffusion Factorization Framework for Offline Multi-Agent Reinforcement Learning](https://openreview.net/forum?id=OTFKVkxSlL) |  | 0 | Diffusion models have been widely adopted in image and language generation and are now being applied to reinforcement learning. However, the application of diffusion models in offline cooperative Multi-Agent Reinforcement Learning (MARL) remains limited. Although existing studies explore this... | Chao Li, Cheng Wang, Chenglu Wen, Chenxing Lin, Siqi Shen, Weiquan Liu, Wenqi Chen, Yongquan Fu, Ziwei Deng |  |
| 2165 |  |  [Beyond Autoregression: Fast LLMs via Self-Distillation Through Time](https://openreview.net/forum?id=uZ5K4HeNwd) |  | 0 | Autoregressive (AR) Large Language Models (LLMs) have demonstrated significant success across numerous tasks. However, the AR modeling paradigm presents certain limitations; for instance, contemporary autoregressive LLMs are trained to generate one token at a time, which can result in noticeable... | Caglar Gulcehre, Justin Deschenaux |  |
| 2166 |  |  [AI Sandbagging: Language Models can Strategically Underperform on Evaluations](https://openreview.net/forum?id=7Qa2SpjxIS) |  | 0 | Trustworthy capability evaluations are crucial for ensuring the safety of AI systems, and are becoming a key component of AI regulation. However, the developers of an AI system, or the AI system itself, may have incentives for evaluations to understate the AI's actual capability. These conflicting... | Felix Hofstätter, Francis Rhys Ward, Oliver Jaffe, Samuel F. Brown, Teun van der Weij |  |
| 2167 |  |  [Dataset Ownership Verification in Contrastive Pre-trained Models](https://openreview.net/forum?id=zeAOzn80VQ) |  | 0 | High-quality open-source datasets, which necessitate substantial efforts for curation, has become the primary catalyst for the swift progress of deep learning. Concurrently, protecting these datasets is paramount for the well-being of the data owner. Dataset ownership verification emerges as a... | Bingde Hu, Genlang Chen, Haofei Zhang, Jie Song, Mengqi Xue, Mingli Song, Xingen Wang, Yuechen Xie |  |
| 2168 |  |  [Transformer Encoder Satisfiability: Complexity and Impact on Formal Reasoning](https://openreview.net/forum?id=VVO3ApdMUE) |  | 0 | We analyse the complexity of the satisfiability problem, or similarly feasibility problem, (trSAT) for transformer encoders (TE), which naturally occurs in formal verification or interpretation, collectively referred to as formal reasoning. We find that trSAT is undecidable when considering TE as... | Eric Alsmann, Marco Sälzer, Martin Lange |  |
| 2169 |  |  [Diffusion Transformer Captures Spatial-Temporal Dependencies: A Theory for Gaussian Process Data](https://openreview.net/forum?id=MbM1BqGpZu) |  | 0 | Diffusion Transformer, the backbone of Sora for video generation, successfully scales the capacity of diffusion models, pioneering new avenues for high-fidelity sequential data generation. Unlike static data such as images, sequential data consists of consecutive data frames indexed by time,... | Hengyu Fu, Jiawei Guo, Mengdi Wang, Minshuo Chen, Zehao Dou |  |
| 2170 |  |  [Fast training and sampling of Restricted Boltzmann Machines](https://openreview.net/forum?id=3fGtV4Zfgq) |  | 0 | Restricted Boltzmann Machines (RBMs) are powerful tools for modeling complex systems and extracting insights from data, but their training is hindered by the slow mixing of Markov Chain Monte Carlo (MCMC) processes, especially with highly structured datasets. In this study, we build on recent... | Aurélien Decelle, Beatriz Seoane, Cyril Furtlehner, Lorenzo Rosset, Nicolas Béreux |  |
| 2171 |  |  [Training-free LLM-generated Text Detection by Mining Token Probability Sequences](https://openreview.net/forum?id=vo4AHjowKi) |  | 0 | Large language models (LLMs) have demonstrated remarkable capabilities in generating high-quality texts across diverse domains. However, the potential misuse of LLMs has raised significant concerns, underscoring the urgent need for reliable detection of LLM-generated texts. Conventional... | Fei Wu, Huangsen Cao, Yifei Bi, Yihuai Xu, Yongwei Wang, Yu Zhao, Zhouhan Lin |  |
| 2172 |  |  [ELBOing Stein: Variational Bayes with Stein Mixture Inference](https://openreview.net/forum?id=2rBLbNJwBm) |  | 0 | Stein variational gradient descent (SVGD) (Liu & Wang, 2016) performs approximate Bayesian inference by representing the posterior with a set of particles. However, SVGD suffers from variance collapse, i.e. poor predictions due to underestimating uncertainty (Ba et al., 2021), even for... | Christophe Ley, Eric T. Nalisnick, Ola Rønning, Padhraic Smyth, Thomas Hamelryck |  |
| 2173 |  |  [Learning Hierarchical Polynomials of Multiple Nonlinear Features](https://openreview.net/forum?id=UZ893n8FXr) |  | 0 | In deep learning theory, a critical question is to understand how neural networks learn hierarchical features. In this work, we study the learning of hierarchical polynomials of multiple nonlinear features using three-layer neural networks. We examine a broad class of functions of the form... | Eshaan Nichani, Hengyu Fu, Jason D. Lee, Zihao Wang |  |
| 2174 |  |  [Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model](https://openreview.net/forum?id=DugT77rRhW) |  | 0 | Room layout estimation from multiple-perspective images is poorly investigated due to the complexities that emerge from multi-view geometry, which requires muti-step solutions such as camera intrinsic and extrinsic estimation, image matching, and triangulation. However, in 3D reconstruction, the... | Jianan Wang, Xianbiao Qi, Xiangyu Yue, Xili Dai, Yaxuan Huang, Yixing Yuan |  |
| 2175 |  |  [Standardizing Structural Causal Models](https://openreview.net/forum?id=aXuWowhIYt) |  | 0 | Synthetic datasets generated by structural causal models (SCMs) are commonly used for benchmarking causal structure learning algorithms. However, the variances and pairwise correlations in SCM data tend to increase along the causal ordering. Several popular algorithms exploit these artifacts,... | Andreas Krause, Bernhard Schölkopf, Lars Lorch, Scott Sussex, Weronika Ormaniec |  |
| 2176 |  |  [OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation](https://openreview.net/forum?id=j7kdXSrISM) |  | 0 | Text-to-video (T2V) generation has recently garnered significant attention thanks to the large multi-modality model Sora. However, T2V generation still faces two important challenges: 1) Lacking a precise open sourced high-quality dataset. The previously popular video datasets, e.g.WebVid-10M and... | Jian Yang, Kepan Nan, Penghao Zhou, Rui Xie, Tiehan Fan, Xiang Li, Ying Tai, Zhenheng Yang, Zhijie Chen |  |
| 2177 |  |  [Model Risk-sensitive Offline Reinforcement Learning](https://openreview.net/forum?id=h6k4809xVV) |  | 0 | Offline reinforcement learning (RL) is becoming critical in risk-sensitive areas such as finance and autonomous driving, where incorrect decisions can lead to substantial financial loss or compromised safety. However, traditional risk-sensitive offline RL methods often struggle with accurately... | Gwangpyo Yoo, Honguk Woo |  |
| 2178 |  |  [VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents](https://openreview.net/forum?id=zG459X3Xge) |  | 0 | Retrieval-augmented generation (RAG) is an effective technique that enables large language models (LLMs) to utilize external knowledge sources for generation. However, current RAG systems are solely based on text, rendering it impossible to utilize vision information like layout and images that... | Bokai Xu, Chaoyue Tang, Junbo Cui, Junhao Ran, Maosong Sun, Shi Yu, Shuo Wang, Xu Han, Yukun Yan, Zhenghao Liu, Zhiyuan Liu |  |
| 2179 |  |  [Learning system dynamics without forgetting](https://openreview.net/forum?id=rjuZyMfLSd) |  | 0 | Observation-based trajectory prediction for systems with unknown dynamics is essential in fields such as physics and biology. Most existing approaches are limited to learning within a single system with fixed dynamics patterns. However, many real-world applications require learning across systems... | Dacheng Tao, Dongjin Song, Xikun Zhang, Yixin Chen, Yushan Jiang |  |
| 2180 |  |  [Bootstrapping Language Models with DPO Implicit Rewards](https://openreview.net/forum?id=dliIIodM6b) |  | 0 | Human alignment in large language models (LLMs) is an active area of research. A recent groundbreaking work, direct preference optimization (DPO), has greatly simplified the process from past work in reinforcement learning from human feedback (RLHF) by bypassing the reward learning stage in RLHF.... | Arunesh Sinha, Changyu Chen, Chao Du, Min Lin, Pradeep Varakantham, Qian Liu, Tianyu Pang, Zichen Liu |  |
| 2181 |  |  [Reconsidering Faithfulness in Regular, Self-Explainable and Domain Invariant GNNs](https://openreview.net/forum?id=kiOxNsrpQy) |  | 0 | As Graph Neural Networks (GNNs) become more pervasive, it becomes paramount to build reliable tools for explaining their predictions. A core desideratum is that explanations are \*faithful\*, i.e., that they portray an accurate picture of the GNN's reasoning process. However, a number of different... | Andrea Passerini, Antonio Longa, Stefano Teso, Steve Azzolin |  |
| 2182 |  |  [SC-OmniGS: Self-Calibrating Omnidirectional Gaussian Splatting](https://openreview.net/forum?id=7idCpuEAiR) |  | 0 | 360-degree cameras streamline data collection for radiance field 3D reconstruction by capturing comprehensive scene data. However, traditional radiance field methods do not address the specific challenges inherent to 360-degree images. We present SC-OmniGS, a novel self-calibrating omnidirectional... | Huajian Huang, Hui Cheng, Longwei Li, SaiKit Yeung, Tristan Braud, Yajie Zhao, Yingshu Chen |  |
| 2183 |  |  [SWIFT: On-the-Fly Self-Speculative Decoding for LLM Inference Acceleration](https://openreview.net/forum?id=EKJhH5D5wA) |  | 0 | Speculative decoding (SD) has emerged as a widely used paradigm to accelerate LLM inference without compromising quality. It works by first employing a compact model to draft multiple tokens efficiently and then using the target LLM to verify them in parallel. While this technique has achieved... | Cunxiao Du, Heming Xia, Jun Zhang, Wenjie Li, Yongqi Li |  |
| 2184 |  |  [Fast Direct: Query-Efficient Online Black-box Guidance for Diffusion-model Target Generation](https://openreview.net/forum?id=OmpTdjl7RV) |  | 0 | Guided diffusion-model generation is a promising direction for customizing the generation process of a pre-trained diffusion model to address specific downstream tasks. Existing guided diffusion models either rely on training the guidance model with pre-collected datasets or require the objective... | Ivor W. Tsang, Kim Yong Tan, YewSoon Ong, Yueming Lyu |  |
| 2185 |  |  [Adaptive Transformer Programs: Bridging the Gap Between Performance and Interpretability in Transformers](https://openreview.net/forum?id=W8K8slZ73R) |  | 0 | Balancing high performance with interpretability in increasingly powerful Transformer-based models remains a challenge. While mechanistic interpretability aims to specify neural network computations in explicit, pseudocode-like formats, existing methods often involve laborious manual analysis or... | QuocVinh LaiDang, Seungah Son, Taemin Kang |  |
| 2186 |  |  [SysBench: Can LLMs Follow System Message?](https://openreview.net/forum?id=KZWaxtzIRx) |  | 0 | Large Language Models (LLMs) have become instrumental across various applications, with the customization of these models to specific scenarios becoming increasingly critical. System message, a fundamental component of LLMs, is consist of carefully crafted instructions that guide the behavior of... | Bin Cui, Haoze Sun, Tao Zhang, Tao Zhang, Weipeng Chen, Wenjing Luo, Wentao Zhang, Yan Zhang, Yanjun Shen, Yanzhao Qin, Yujing Qiao, Zenan Zhou |  |
| 2187 |  |  [DECO: Unleashing the Potential of ConvNets for Query-based Detection and Segmentation](https://openreview.net/forum?id=TWRhLAN5rz) |  | 0 | Transformer and its variants have shown great potential for various vision tasks in recent years, including image classification, object detection and segmentation. Meanwhile, recent studies also reveal that with proper architecture design, convolutional networks (ConvNets) also achieve competitive... | Siwei Li, Xinghao Chen, Yijing Yang, Yunhe Wang |  |
| 2188 |  |  [Erasing Concept Combination from Text-to-Image Diffusion Model](https://openreview.net/forum?id=OBjF5I4PWg) |  | 0 | Advancements in the text-to-image diffusion model have raised security concerns due to their potential to generate images with inappropriate themes such as societal biases and copyright infringements. Current studies have made notable progress in preventing the model from generating images... | Hongyi Nie, Quanming Yao, Yang Liu, Yatao Bian, Zhen Wang |  |
| 2189 |  |  [Bridging and Modeling Correlations in Pairwise Data for Direct Preference Optimization](https://openreview.net/forum?id=hRwxZmcvW9) |  | 0 | Direct preference optimization (DPO), a widely adopted offline preference optimization algorithm, aims to align large language models (LLMs) with human-desired behaviors using pairwise preference data. However, the generation of the winning response and the losing response within pairwise data are... | Bo Huang, Liangyou Li, Lifeng Shang, Ruiming Tang, Wei Wang, Xin Jiang, Xingshan Zeng, Yasheng Wang, Yufei Wang, Yuxin Jiang |  |
| 2190 |  |  [Deriving Causal Order from Single-Variable Interventions: Guarantees & Algorithm](https://openreview.net/forum?id=u63OVngeSp) |  | 0 | Targeted and uniform interventions to a system are crucial for unveiling causal relationships. While several methods have been developed to leverage interventional data for causal structure learning, their practical application in real-world scenarios often remains challenging. Recent benchmark... | Arash Mehrjou, Mathieu Chevalley, Patrick Schwab |  |
| 2191 |  |  [Making Transformer Decoders Better Differentiable Indexers](https://openreview.net/forum?id=bePaRx0otZ) |  | 0 | Retrieval aims to find the top-k items most relevant to a query/user from a large dataset. Traditional retrieval models represent queries/users and items as embedding vectors and use Approximate Nearest Neighbor (ANN) search for retrieval. Recently, researchers have proposed a generative-based... | Defu Lian, Han Li, Kai Zheng, Kun Gai, Qi Liu, Wentian Bao, Wuchao Li, Yang Song, Yunen Yu |  |
| 2192 |  |  [Sharpness-Aware Black-Box Optimization](https://openreview.net/forum?id=h7EwIfjxgn) |  | 0 | Black-box optimization algorithms have been widely used in various machine learning problems, including reinforcement learning and prompt fine-tuning. However, directly optimizing the training loss value, as commonly done in existing black-box optimization methods, could lead to suboptimal model... | Feiyang Ye, Ivor W. Tsang, Masashi Sugiyama, Xuehao Wang, Yu Zhang, Yueming Lyu |  |
| 2193 |  |  [OmniBind: Large-scale Omni Multimodal Representation via Binding Spaces](https://openreview.net/forum?id=l2izo0z7gu) |  | 0 | Recently, human-computer interaction with various modalities has shown promising applications, like GPT-4o and Gemini. Meanwhile, multimodal representation models have emerged as the foundation for these versatile multimodal understanding and generation pipeline. Models like CLIP, CLAP and... | Hang Zhang, Hengshuang Zhao, Luping Liu, Minjie Hong, Rongjie Huang, Shengpeng Ji, Tao Jin, Xize Cheng, Zehan Wang, Zhou Zhao, Ziang Zhang |  |
| 2194 |  |  [From Tokens to Words: On the Inner Lexicon of LLMs](https://openreview.net/forum?id=328vch6tRs) |  | 0 | Natural language is composed of words, but modern large language models (LLMs) process sub-words as input. A natural question raised by this discrepancy is whether LLMs encode words internally, and if so how. We present evidence that LLMs engage in an intrinsic detokenization process, where subword... | Guy Kaplan, Matanel Oren, Roy Schwartz, Yuval Reif |  |
| 2195 |  |  [GLoRa: A Benchmark to Evaluate the Ability to Learn Long-Range Dependencies in Graphs](https://openreview.net/forum?id=2jf5x5XoYk) |  | 0 | Learning on graphs is one of the most active research topics in machine learning (ML). Among the key challenges in this field, effectively learning long-range dependencies in graphs has been particularly difficult. It has been observed that, in practice, the performance of many ML approaches,... | Dongzhuoran Zhou, Egor V. Kostylev, Evgeny Kharlamov |  |
| 2196 |  |  [B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners](https://openreview.net/forum?id=P6dwZJpJ4m) |  | 0 | In the absence of extensive human-annotated data for complex reasoning tasks, self-improvement -- where models are trained on their own outputs -- has emerged as a primary method for enhancing performance. Recently, the approach to self-improvement has shifted toward a more dynamic, online fashion... | Junxian He, Lulu Zhao, Weihao Zeng, Yijun Wang, Yuzhen Huang, Zifei Shan |  |
| 2197 |  |  [AdaRankGrad: Adaptive Gradient Rank and Moments for Memory-Efficient LLMs Training and Fine-Tuning](https://openreview.net/forum?id=LvNROciCne) |  | 0 | Training and fine-tuning large language models (LLMs) come with challenges related to memory and computational requirements due to the increasing size of the model weights and the optimizer states. To tackle these challenges, various techniques have been developed, such as low-rank adaptation... | Boris Shustin, Jonathan Svirsky, Ofir Lindenbaum, Wasim Huleihel, Yehonathan Refael |  |
| 2198 |  |  [ThinkBot: Embodied Instruction Following with Thought Chain Reasoning](https://openreview.net/forum?id=tFDTHA3odg) |  | 0 | Embodied Instruction Following (EIF) requires agents to complete human instruction by interacting objects in complicated surrounding environments. Conventional methods directly consider the sparse human instruction to generate action plans for agents, which usually fail to achieve human goals... | Changliu Liu, Guanxing Lu, Jiwen Lu, Yansong Tang, Ziwei Wang |  |
| 2199 |  |  [Understanding Constraint Inference in Safety-Critical Inverse Reinforcement Learning](https://openreview.net/forum?id=B2RXwASSpy) |  | 0 | In practical applications, the underlying constraint knowledge is often unknown and difficult to specify. To address this issue, recent advances in Inverse Constrained Reinforcement Learning (ICRL) have focused on inferring these constraints from expert demonstrations. However, the ICRL approach... | Ashish Gaurav, Bo Yue, Guiliang Liu, Jian Li, Pascal Poupart, Shufan Wang |  |
| 2200 |  |  [Few for Many: Tchebycheff Set Scalarization for Many-Objective Optimization](https://openreview.net/forum?id=O4N9kWwV6R) |  | 0 | Multi-objective optimization can be found in many real-world applications where some conflicting objectives can not be optimized by a single solution. Existing optimization methods often focus on finding a set of Pareto solutions with different optimal trade-offs among the objectives. However, the... | Fei Liu, Qingfu Zhang, Xi Lin, Xiaoyuan Zhang, Yilu Liu, Zhenkun Wang |  |
| 2201 |  |  [Forget the Data and Fine-Tuning! Just Fold the Network to Compress](https://openreview.net/forum?id=W2Wkp9MQsF) |  | 0 | We introduce model folding, a novel data-free model compression technique that merges structurally similar neurons across layers, significantly reducing the model size without the need for fine-tuning or access to training data. Unlike existing methods, model folding preserves data statistics... | Dong Wang, Haris Sikic, Lothar Thiele, Olga Saukh |  |
| 2202 |  |  [HeadMap: Locating and Enhancing Knowledge Circuits in LLMs](https://openreview.net/forum?id=jUsrbOuQ5e) |  | 0 | Large language models (LLMs), through pretraining on extensive corpora, encompass rich semantic knowledge and exhibit the potential for efficient adaptation to diverse downstream tasks. However, the intrinsic mechanisms underlying LLMs remain unexplored, limiting the efficacy of applying these... | Binghuai Lin, Liyuan Wang, Xuehao Wang, Yu Zhang |  |
| 2203 |  |  [Optimization by Parallel Quasi-Quantum Annealing with Gradient-Based Sampling](https://openreview.net/forum?id=9EfBeXaXf0) |  | 0 | Learning-based methods have gained attention as general-purpose solvers due to their ability to automatically learn problem-specific heuristics, reducing the need for manually crafted heuristics. However, these methods often face scalability challenges. To address these issues, the improved... | Yamato Arai, Yuma Ichikawa |  |
| 2204 |  |  [Guaranteed Generation from Large Language Models](https://openreview.net/forum?id=8roRgrjbjv) |  | 0 | As large language models (LLMs) are increasingly used across various applications, there is a growing need to control text generation to satisfy specific constraints or requirements. This raises a crucial question: Is it possible to guarantee strict constraint satisfaction in generated outputs... | Hwaran Lee, Jos Rozen, Kyomin Jung, Marc Dymetman, Minbeom Kim, Thibaut Thonet |  |
| 2205 |  |  [On Rollouts in Model-Based Reinforcement Learning](https://openreview.net/forum?id=Uh5GRmLlvt) |  | 0 | Model-based reinforcement learning (MBRL) seeks to enhance data efficiency by learning a model of the environment and generating synthetic rollouts from it. However, accumulated model errors during these rollouts can distort the data distribution, negatively impacting policy learning and hindering... | Bernd Frauenknecht, Devdutt Subhasish, Friedrich Solowjow, Sebastian Trimpe |  |
| 2206 |  |  [MTSAM: Multi-Task Fine-Tuning for Segment Anything Model](https://openreview.net/forum?id=6N4QMbeVaO) |  | 0 | The Segment Anything Model (SAM), with its remarkable zero-shot capability, has the potential to be a foundation model for multi-task learning. However, adopting SAM to multi-task learning faces two challenges: (a) SAM has difficulty generating task-specific outputs with different channel numbers,... | Feiyang Ye, Xuehao Wang, Yu Zhang, Zhan Zhuang |  |
| 2207 |  |  [On Generalization Across Environments In Multi-Objective Reinforcement Learning](https://openreview.net/forum?id=tuEP424UQ5) |  | 0 | Real-world sequential decision-making tasks often require balancing trade-offs between multiple conflicting objectives, making Multi-Objective Reinforcement Learning (MORL) an increasingly prominent field of research. Despite recent advances, existing MORL literature has narrowly focused on... | Jayden Teoh, Peter Vamplew, Pradeep Varakantham |  |
| 2208 |  |  [Ultra-Sparse Memory Network](https://openreview.net/forum?id=zjeHLSiNv1) |  | 0 | It is widely acknowledged that the performance of Transformer models is logarithmically related to their number of parameters and computational complexity. While approaches like Mixture of Experts (MoE) decouple parameter count from computational complexity, they still face challenges in inference... | Defa Zhu, Hongzhi Huang, Qiyang Min, Ran Guo, Xun Zhou, Yutao Zeng, Zihao Huang |  |
| 2209 |  |  [VICtoR: Learning Hierarchical Vision-Instruction Correlation Rewards for Long-horizon Manipulation](https://openreview.net/forum?id=UpQLu9bzAR) |  | 0 | We study reward models for long-horizon manipulation by learning from action-free videos and language instructions, which we term the visual-instruction correlation (VIC) problem. Existing VIC methods face challenges in learning rewards for long-horizon tasks due to their lack of sub-stage... | HanYuan Hsu, JiaFong Yeh, KuoHan Hung, PangChi Lo, Winston H. Hsu, YiTing Chen |  |
| 2210 |  |  [Arithmetic Without Algorithms: Language Models Solve Math with a Bag of Heuristics](https://openreview.net/forum?id=O9YTt26r2P) |  | 0 | Do large language models (LLMs) solve reasoning tasks by learning robust generalizable algorithms, or do they memorize training data? To investigate this question, we use arithmetic reasoning as a representative task. Using causal analysis, we identify a subset of the model (a circuit) that... | Aaron Mueller, Anja Reusch, Yaniv Nikankin, Yonatan Belinkov |  |
| 2211 |  |  [Think Thrice Before You Act: Progressive Thought Refinement in Large Language Models](https://openreview.net/forum?id=pUbbLHjCPM) |  | 0 | Recent advancements in large language models (LLMs) have demonstrated that progressive refinement, rather than providing a single answer, results in more accurate and thoughtful outputs. However, existing methods often rely heavily on supervision signals to evaluate previous responses, making it... | Aili Chen, Chengyu Du, Haokun Zhao, Haoran Guo, Jiaqing Liang, Jinyi Han, Liangyue Li, Qianyu He, Sirui Xia, Yanghua Xiao, Yizhou Ying, Zulong Chen |  |
| 2212 |  |  [Noise Separation guided Candidate Label Reconstruction for Noisy Partial Label Learning](https://openreview.net/forum?id=TOahfjA3sP) |  | 0 | Partial label learning is a weakly supervised learning problem in which an instance is annotated with a set of candidate labels, among which only one is the correct label. However, in practice the correct label is not always in the candidate label set, leading to the noisy partial label learning... | Fuchao Yang, MinLing Zhang, Ran Wang, Xiaorui Peng, Yuheng Jia |  |
| 2213 |  |  [Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation](https://openreview.net/forum?id=rkzabmWl5k) |  | 0 | Recent advances in latent diffusion-based generative models for portrait image animation, such as Hallo, have achieved impressive results in short-duration video synthesis. In this paper, we present updates to Hallo, introducing several design enhancements to extend its capabilities.First, we... | Hang Zhou, Hanlin Shang, Hao Zhu, Hui Li, Jiahao Cui, Jingdong Wang, Kaihui Cheng, Siyu Zhu, Yao Yao |  |
| 2214 |  |  [Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank Structures](https://openreview.net/forum?id=9BiVepgmWW) |  | 0 | Parameter-efficient fine-tuning (PEFT) significantly reduces memory costs when adapting large language models (LLMs) for downstream applications. However, traditional first-order (FO) fine-tuning algorithms incur substantial memory overhead due to the need to store activation values for... | Kun Yuan, Liyuan Cao, Yiming Chen, Yuan Zhang, Zaiwen Wen |  |
| 2215 |  |  [Efficient Learning with Sine-Activated Low-Rank Matrices](https://openreview.net/forum?id=cWGCkd7mCp) |  | 0 | Low-rank decomposition has emerged as a vital tool for enhancing parameter efficiency in neural network architectures, gaining traction across diverse applications in machine learning. These techniques significantly lower the number of parameters, striking a balance between compactness and... | Cameron Gordon, Hemanth Saratchandran, Simon Lucey, Yiping Ji, Zeyu Zhang |  |
| 2216 |  |  [Earlier Tokens Contribute More: Learning Direct Preference Optimization From Temporal Decay Perspective](https://openreview.net/forum?id=OspqtLVUN5) |  | 0 | Direct Preference Optimization (DPO) has gained attention as an efficient alternative to reinforcement learning from human feedback (RLHF) for aligning large language models (LLMs) with human preferences. Despite its advantages, DPO suffers from a length bias, generating responses longer than those... | Bei Li, Gangao Liu, Jingang Wang, Peng Li, Ruichen Shao, Xunliang Cai, Yang Chen, ZhouXiang |  |
| 2217 |  |  [Lasso Bandit with Compatibility Condition on Optimal Arm](https://openreview.net/forum?id=f3jySJpEFT) |  | 0 | We consider a stochastic sparse linear bandit problem where only a sparse subset of context features affects the expected reward function, i.e., the unknown reward parameter has a sparse structure. In the existing Lasso bandit literature, the compatibility conditions, together with additional... | Harin Lee, Minhwan Oh, Taehyun Hwang |  |
| 2218 |  |  [Equivariant Neural Functional Networks for Transformers](https://openreview.net/forum?id=uBai0ukstY) |  | 0 | This paper systematically explores neural functional networks (NFN) for transformer architectures. NFN are specialized neural networks that treat the weights, gradients, or sparsity patterns of a deep neural network (DNN) as input data and have proven valuable for tasks such as learnable... | An Nguyen The, DuyTung Pham, Hoang V. Tran, MinhKhoi NguyenNhat, Tan Minh Nguyen, Thanh Tran, Thieu Vo, Tho Tran Huu |  |
| 2219 |  |  [Improving Deep Regression with Tightness](https://openreview.net/forum?id=dkoiAGjZV9) |  | 0 | For deep regression, preserving the ordinality of the targets with respect to the feature representation improves performance across various tasks. However, a theoretical explanation for the benefits of ordinality is still lacking. This work reveals that preserving ordinality reduces the... | Angela Yao, Shihao Zhang, Yuguang Yan |  |
| 2220 |  |  [Looking Backward: Streaming Video-to-Video Translation with Feature Banks](https://openreview.net/forum?id=AMkf7h7HER) |  | 0 | This paper introduces StreamV2V, a diffusion model that achieves real-time streaming video-to-video (V2V) translation with user prompts. Unlike prior V2V methods using batches to process limited frames, we opt to process frames in a streaming fashion, to support unlimited frames. At the heart of... | Akio Kodaira, Chenfeng Xu, Diana Marculescu, Feng Liang, Kurt Keutzer, Masayoshi Tomizuka |  |
| 2221 |  |  [Basis Sharing: Cross-Layer Parameter Sharing for Large Language Model Compression](https://openreview.net/forum?id=gp32jvUquq) |  | 0 | Large Language Models (LLMs) have achieved remarkable breakthroughs. However, the huge number of parameters in LLMs require significant amount of memory storage in inference, which prevents their practical deployment in many applications. To reduce memory storage of LLMs, singular value... | Bing Li, Grace Li Zhang, IngChao Lin, Jingcun Wang, YuGuang Chen |  |
| 2222 |  |  [DyCAST: Learning Dynamic Causal Structure from Time Series](https://openreview.net/forum?id=WjDjem8mWE) |  | 0 | Understanding the dynamics of causal structures is crucial for uncovering the underlying processes in time series data. Previous approaches rely on static assumptions, where contemporaneous and time-lagged dependencies are assumed to have invariant topological structures. However, these models fail... | Bochen Lyu, Weiwei Xing, Yue Cheng, Zhanxing Zhu |  |
| 2223 |  |  [Towards Calibrated Deep Clustering Network](https://openreview.net/forum?id=JvH4jDDcG3) |  | 0 | Deep clustering has exhibited remarkable performance; however, the over confidence problem, i.e., the estimated confidence for a sample belonging to a particular cluster greatly exceeds its actual prediction accuracy, has been over looked in prior research. To tackle this critical issue, we pioneer... | Hui Liu, Jianhong Cheng, Junhui Hou, Yuheng Jia |  |
| 2224 |  |  [DisPose: Disentangling Pose Guidance for Controllable Human Image Animation](https://openreview.net/forum?id=AumOa10MKG) |  | 0 | Controllable human image animation aims to generate videos from reference images using driving videos. Due to the limited control signals provided by sparse guidance (e.g., skeleton pose), recent works have attempted to introduce additional dense conditions (e.g., depth map) to ensure motion... | Hongxiang Li, Junjie Cao, Long Chen, Xuxin Cheng, Yaowei Li, Yuhang Yang, Zhihong Zhu |  |
| 2225 |  |  [Online-to-Offline RL for Agent Alignment](https://openreview.net/forum?id=ruv3HdK6he) |  | 0 | Reinforcement learning (RL) has shown remarkable success in training agents to achieve high-performing policies, particularly in domains like Game AI where simulation environments enable efficient interactions. However, despite their success in maximizing these returns, such online-trained policies... | Haobo Fu, Qiang Fu, Shuai Li, Stefano V. Albrecht, Xu Liu |  |
| 2226 |  |  [SynQ: Accurate Zero-shot Quantization by Synthesis-aware Fine-tuning](https://openreview.net/forum?id=2rnOgyFQgb) |  | 0 | How can we accurately quantize a pre-trained model without any data? Quantization algorithms are widely used for deploying neural networks on resource-constrained edge devices. Zero-shot Quantization (ZSQ) addresses the crucial and practical scenario where training data are inaccessible for privacy... | Jongjin Kim, Minjun Kim, U Kang |  |
| 2227 |  |  [CollabEdit: Towards Non-destructive Collaborative Knowledge Editing](https://openreview.net/forum?id=2PzozgigiA) |  | 0 | Collaborative learning of large language models (LLMs) has emerged as a new paradigm for utilizing private data from different parties to guarantee efﬁciency and privacy. Meanwhile, Knowledge Editing (KE) for LLMs has also garnered increased attention due to its ability to manipulate the behaviors... | Jiamu Zheng, Jianwei Yin, Jinghuai Zhang, Tao Lin, Tianyu Du, Xuhong Zhang |  |
| 2228 |  |  [RuAG: Learned-rule-augmented Generation for Large Language Models](https://openreview.net/forum?id=BpIbnXWfhL) |  | 0 | In-context learning (ICL) and Retrieval-Augmented Generation (RAG) have gained attention for their ability to enhance LLMs' reasoning by incorporating external knowledge but suffer from limited contextual window size, leading to insufficient information injection. To this end, we propose a novel... | Chaoyun Zhang, Dongmei Zhang, Lu Wang, Meng Fang, Mykola Pechenizkiy, Pei Xiao, Qi Zhang, Qingwei Lin, Randolph Yao, Saravan Rajmohan, Si Qin, Yali Du, Yevgeniy Puzyrev, Yudi Zhang |  |
| 2229 |  |  [Reveal Object in Lensless Photography via Region Gaze and Amplification](https://openreview.net/forum?id=EV7FMBZxnx) |  | 0 | Detecting concealed objects, such as in vivo lesions or camouflage, requires customized imaging systems. Lensless cameras, being compact and flexible, offer a promising alternative to bulky lens systems. However, the absence of lenses leads to measurements lacking visual semantics, posing... | Huihui Yue, Xiangjun Yin |  |
| 2230 |  |  [Learning Video-Conditioned Policy on Unlabelled Data with Joint Embedding Predictive Transformer](https://openreview.net/forum?id=TqM0hifngW) |  | 0 | The video-conditioned policy takes prompt videos of the desired tasks as a condition and is regarded for its prospective generalizability. Despite its promise, training a video-conditioned policy is non-trivial due to the need for abundant demonstrations. In some tasks, the expert rollouts are... | Hao Luo, Zongqing Lu |  |
| 2231 |  |  [Enhancing Graph Of Thought: Enhancing Prompts with LLM Rationales and Dynamic Temperature Control](https://openreview.net/forum?id=l32IrJtpOP) |  | 0 | We introduce Enhancing Graph of Thoughts (EGoT), a method designed to enhance the performance of large language models (LLMs) on complex reasoning tasks. EGoT automates the process of generating accurate responses using given data and a base prompt. The process consists of several steps: It obtains... | Sunguk Shin, Youngjoon Kim |  |
| 2232 |  |  [SplineGS: Learning Smooth Trajectories in Gaussian Splatting for Dynamic Scene Reconstruction](https://openreview.net/forum?id=tMG6btjBfd) |  | 0 | Reconstructing complex scenes with deforming objects for novel view synthesis is a challenging task. Recent works have addressed this with 3D Gaussian Splatting, which effectively reconstructs static scenes with high quality in short training time, by adding specialized modules for the deformations... | Jaeseok Oh, Jihwan Yoon, Minsik Lee, Sangbeom Han |  |
| 2233 |  |  [A Closer Look at Machine Unlearning for Large Language Models](https://openreview.net/forum?id=Q1MHvGmhyT) |  | 0 | Large language models (LLMs) may memorize sensitive or copyrighted content, raising privacy and legal concerns. Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this... | Chao Du, Kejiang Chen, Min Lin, Tianyu Pang, Weiming Zhang, Xiaojian Yuan |  |
| 2234 |  |  [Weighted-Reward Preference Optimization for Implicit Model Fusion](https://openreview.net/forum?id=fq24pEb8SL) |  | 0 | While fusing heterogeneous open-source LLMs with varying architectures and sizes can potentially integrate the strengths of different models, existing fusion methods face significant challenges, such as vocabulary alignment and merging distribution matrices. These procedures are not only complex... | Fanqi Wan, Longguang Zhong, Tianyuan Shi, Xiaojun Quan, Ziyi Yang |  |
| 2235 |  |  [Unified Parameter-Efficient Unlearning for LLMs](https://openreview.net/forum?id=zONMuIVCAT) |  | 0 | The advent of Large Language Models (LLMs) has revolutionized natural language processing, enabling advanced understanding and reasoning capabilities across a variety of tasks. Fine-tuning these models for specific domains, particularly through Parameter-Efficient Fine-Tuning (PEFT) strategies like... | Alex Su, Chenlu Ding, Jiancan Wu, Jinda Lu, Kai Zhang, Xiang Wang, Xiangnan He, Yancheng Yuan |  |
| 2236 |  |  [HyPoGen: Optimization-Biased Hypernetworks for Generalizable Policy Generation](https://openreview.net/forum?id=CJWMXqAnAy) |  | 0 | Policy learning through behavior cloning poses significant challenges, particularly when demonstration data is limited. In this work, we present HyPoGen, a novel optimization-biased hypernetwork for policy generation. The proposed hypernetwork learns to synthesize optimal policy parameters solely... | Difan Zou, Hanxiang Ren, Li Sun, Pei Zhou, Siyan Dong, Xulong Wang, Yanchao Yang, Youyi Zheng, Zewen Wu |  |
| 2237 |  |  [Conformalized Survival Analysis for General Right-Censored Data](https://openreview.net/forum?id=JQtuCumAFD) |  | 0 | We develop a framework to quantify predictive uncertainty in survival analysis, providing a reliable lower predictive bound (LPB) for the true, unknown patient survival time. Recently, conformal prediction has been used to construct such valid LPBs for \*type-I right-censored data\*, with the... | Gil Shamai, Hen Davidov, Ron Kimmel, Shai Feldman, Yaniv Romano |  |
| 2238 |  |  [A Stochastic Approach to the Subset Selection Problem via Mirror Descent](https://openreview.net/forum?id=5K0fmGnFqP) |  | 0 | The subset selection problem is fundamental in machine learning and other fields of computer science. We introduce a stochastic formulation for the minimum cost subset selection problem in a black box setting, in which only the subset metric value is available. Subsequently, we can handle two-stage... | Alex Shtoff, Dan Greenstein, Elazar Gershuni, Fiana Raiber, Ilan BenBassat, Moshe Ran, Nadav Hallak, Oren Somekh, Yaroslav Fyodorov |  |
| 2239 |  |  [Everything is Editable: Extend Knowledge Editing to Unstructured Data in Large Language Models](https://openreview.net/forum?id=X5rO5VyTgB) |  | 0 | Recent knowledge editing methods have primarily focused on modifying structured knowledge in large language models. However, this task setting overlooks the fact that a significant portion of real-world knowledge is stored in an unstructured format, characterized by long-form content, noise, and a... | Hanxing Ding, Huawei Shen, Jingcheng Deng, Liang Pang, Xueqi Cheng, Zihao Wei |  |
| 2240 |  |  [Locality-aware Gaussian Compression for Fast and High-quality Rendering](https://openreview.net/forum?id=dHYwfV2KeP) |  | 0 | We present LocoGS, a locality-aware 3D Gaussian Splatting (3DGS) framework that exploits the spatial coherence of 3D Gaussians for compact modeling of volumetric scenes. To this end, we first analyze the local coherence of 3D Gaussian attributes, and propose a novel locality-aware 3D Gaussian... | Jaesik Park, Seungjoo Shin, Sunghyun Cho |  |
| 2241 |  |  [Diffusion Actor-Critic: Formulating Constrained Policy Iteration as Diffusion Noise Regression for Offline Reinforcement Learning](https://openreview.net/forum?id=ldVkAO09Km) |  | 0 | In offline reinforcement learning, it is necessary to manage out-of-distribution actions to prevent overestimation of value functions. One class of methods, the policy-regularized method, addresses this problem by constraining the target policy to stay close to the behavior policy. Although several... | Bingyi Jing, Jing Zhang, Linjiajie Fang, Ruoxue Liu, Wenjia Wang |  |
| 2242 |  |  [IRIS: LLM-Assisted Static Analysis for Detecting Security Vulnerabilities](https://openreview.net/forum?id=9LdJDU7E91) |  | 0 | Software is prone to security vulnerabilities. Program analysis tools to detect them have limited effectiveness in practice due to their reliance on human labeled specifications. Large language models (or LLMs) have shown impressive code generation capabilities but they cannot do complex reasoning... | Mayur Naik, Saikat Dutta, Ziyang Li |  |
| 2243 |  |  [Improving Long-Text Alignment for Text-to-Image Diffusion Models](https://openreview.net/forum?id=2ZK8zyIt7o) |  | 0 | The rapid advancement of text-to-image (T2I) diffusion models has enabled them to generate unprecedented results from given texts. However, as text inputs become longer, existing encoding methods like CLIP face limitations, and aligning the generated images with long texts becomes challenging. To... | Chao Du, Chongxuan Li, Dong Xu, Luping Liu, Tianyu Pang, Zehan Wang |  |
| 2244 |  |  [A Solvable Attention for Neural Scaling Laws](https://openreview.net/forum?id=wYxOMEzpkl) |  | 0 | Transformers and many other deep learning models are empirically shown to predictably enhance their performance as a power law in training time, model size, or the number of training data points, which is termed as the neural scaling law. This paper studies this intriguing phenomenon particularly... | Bochen Lyu, Di Wang, Zhanxing Zhu |  |
| 2245 |  |  [GPromptShield: Elevating Resilience in Graph Prompt Tuning Against Adversarial Attacks](https://openreview.net/forum?id=yCN4yI6zhH) |  | 0 | The paradigm of \`\`pre-training and prompt-tuning", with its effectiveness and lightweight characteristics, has rapidly spread from the language field to the graph field. Several pioneering studies have designed specialized prompt functions for diverse downstream graph tasks based on various graph... | Huawei Cao, Maolei Huang, Ming Dun, Ping Li, Shuhan Song, Xiaochun Ye |  |
| 2246 |  |  [Unify ML4TSP: Drawing Methodological Principles for TSP and Beyond from Streamlined Design Space of Learning and Search](https://openreview.net/forum?id=grU1VKEOLi) |  | 0 | Despite the rich works on machine learning (ML) for combinatorial optimization (CO), a unified, principled framework remains lacking. This study utilizes the Travelling Salesman Problem (TSP) as a major case study, with adaptations demonstrated for other CO problems, dissecting established... | Haoyu Geng, Jiale Ma, Junchi Yan, Nianzu Yang, Runzhong Wang, Wenzheng Pan, Yang Li |  |
| 2247 |  |  [Bridging Compressed Image Latents and Multimodal Large Language Models](https://openreview.net/forum?id=GSUNPIw7Ad) |  | 0 | This paper presents the first-ever study of adapting compressed image latents to suit the needs of downstream vision tasks that adopt Multimodal Large Language Models (MLLMs). MLLMs have extended the success of large language models to modalities (e.g. images) beyond text, but their billion scale... | Alessandro Gnutti, Cheng Chien, ChiaHao Kao, Riccardo Leonardi, ShaoYuan Lo, WenHsiao Peng, YiHsin Chen, YuJen Tseng |  |
| 2248 |  |  [Learning Partial Graph Matching via Optimal Partial Transport](https://openreview.net/forum?id=uDXFOurrHM) |  | 0 | Partial graph matching extends traditional graph matching by allowing some nodes to remain unmatched, enabling applications in more complex scenarios. However, this flexibility introduces additional complexity, as both the subset of nodes to match and the optimal mapping must be determined. While... | Gathika Ratnayaka, James Nichols, Qing Wang |  |
| 2249 |  |  [Offline RL with Smooth OOD Generalization in Convex Hull and its Neighborhood](https://openreview.net/forum?id=eY5JNJE56i) |  | 0 | Offline Reinforcement Learning (RL) struggles with distributional shifts, leading to the $Q$-value overestimation for out-of-distribution (OOD) actions. Existing methods address this issue by imposing constraints; however, they often become overly conservative when evaluating OOD regions, which... | Faguo Wu, Jianxiang Liu, Qingmao Yao, Tianyuan Chen, Xiao Zhang, Xuefan Chen, Zhichao Lei, Ziyue Yuan |  |
| 2250 |  |  [Adversarial Search Engine Optimization for Large Language Models](https://openreview.net/forum?id=hkdqxN3c7t) |  | 0 | Large Language Models (LLMs) are increasingly used in applications where the model selects from competing third-party content, such as in LLM-powered search engines or chatbot plugins. In this paper, we introduce _Preference Manipulation Attacks_, a new class of attacks that manipulate an LLM's... | Edoardo Debenedetti, Florian Tramèr, Fredrik Nestaas |  |
| 2251 |  |  [Near, far: Patch-ordering enhances vision foundation models' scene understanding](https://openreview.net/forum?id=Qro97zWC29) |  | 0 | We introduce NeCo: Patch Neighbor Consistency, a novel self-supervised training loss that enforces patch-level nearest neighbor consistency across a student and teacher model. Compared to contrastive approaches that only yield binary learning signals, i.e. "attract" and "repel", this approach... | Francesco Locatello, Gertjan J. Burghouts, Mohammadreza Salehi, Valentinos Pariza, Yuki M. Asano |  |
| 2252 |  |  [NeRAF: 3D Scene Infused Neural Radiance and Acoustic Fields](https://openreview.net/forum?id=njvSBvtiwp) |  | 0 | Sound plays a major role in human perception. Along with vision, it provides essential information for understanding our surroundings. Despite advances in neural implicit representations, learning acoustics that align with visual scenes remains a challenge. We propose NeRAF, a method that jointly... | Amandine Brunetto, Fabien Moutarde, Sascha Hornauer |  |
| 2253 |  |  [Flow Distillation Sampling: Regularizing 3D Gaussians with Pre-trained Matching Priors](https://openreview.net/forum?id=BzsjHiBfLk) |  | 0 | 3D Gaussian Splatting (3DGS) has achieved excellent rendering quality with fast training and rendering speed. However, its optimization process lacks explicit geometric constraints, leading to suboptimal geometric reconstruction in regions with sparse or no observational input views. In this work,... | Kangjie Liu, LinZhuo Chen, Siyu Zhu, Xun Cao, Yao Yao, Youtian Lin, Zhihao Li |  |
| 2254 |  |  [Test-Time Adaptation for Combating Missing Modalities in Egocentric Videos](https://openreview.net/forum?id=1L52bHEL5d) |  | 0 | Understanding videos that contain multiple modalities is crucial, especially in egocentric videos, where combining various sensory inputs significantly improves tasks like action recognition and moment localization. However, real-world applications often face challenges with incomplete modalities... | Alejandro Pardo, Bernard Ghanem, Merey Ramazanova, Motasem Alfarra |  |
| 2255 |  |  [Model-based Offline Reinforcement Learning with Lower Expectile Q-Learning](https://openreview.net/forum?id=OATPSB5JK1) |  | 0 | Model-based offline reinforcement learning (RL) is a compelling approach that addresses the challenge of learning from limited, static data by generating imaginary trajectories using learned models. However, these approaches often struggle with inaccurate value estimation from model rollouts. In... | Kwanyoung Park, Youngwoon Lee |  |
| 2256 |  |  [A Large-scale Dataset and Benchmark for Commuting Origin-Destination Flow Generation](https://openreview.net/forum?id=WeJEidTzff) |  | 0 | Commuting Origin-Destination~(OD) flows are critical inputs for urban planning and transportation, providing crucial information about the population residing in one region and working in another within an interested area. Due to the high cost of data collection, researchers have developed physical... | Can Rong, Jingtao Ding, Yan Liu, Yong Li |  |
| 2257 |  |  [Cross-Domain Off-Policy Evaluation and Learning for Contextual Bandits](https://openreview.net/forum?id=Z8dr422vtr) |  | 0 | Off-Policy Evaluation and Learning (OPE/L) in contextual bandits is rapidly gaining popularity in real systems because new policies can be evaluated and learned securely using only historical logged data. However, existing methods in OPE/L cannot handle many challenging but prevalent scenarios such... | Masataka Ushiku, Yuta Natsubori, Yuta Saito |  |
| 2258 |  |  [MAST: model-agnostic sparsified training](https://openreview.net/forum?id=sPuLtU32av) |  | 0 | We introduce a novel optimization problem formulation that departs from the conventional way of minimizing machine learning model loss as a black-box function. Unlike traditional formulations, the proposed approach explicitly incorporates an initially pre-trained model and random sketch operators,... | Egor Shulgin, Grigory Malinovsky, Peter Richtárik, Yury Demidovich |  |
| 2259 |  |  [LOIRE: LifelOng learning on Incremental data via pre-trained language model gRowth Efficiently](https://openreview.net/forum?id=F5PlYMC5ik) |  | 0 | Large-scale pre-trained language models (PLMs) require significant computational resources to train from scratch on large volumes of data. But in the real world, emerging data from diverse sources may not be initially available for pre-training. Recent studies on lifelong learning have tried to... | Chao Deng, Junlan Feng, Qian Hu, Wenchun Gao, Xue Han, Yitong Wang |  |
| 2260 |  |  [Continuous Diffusion for Mixed-Type Tabular Data](https://openreview.net/forum?id=QPtoBPn4lZ) |  | 0 | Score-based generative models, commonly referred to as diffusion models, have proven to be successful at generating text and image data. However, their adaptation to mixed-type tabular data remains underexplored. In this work, we propose CDTD, a Continuous Diffusion model for mixed-type Tabular... | Dennis Fok, Kathrin Gruber, Markus Mueller |  |
| 2261 |  |  [Single Teacher, Multiple Perspectives: Teacher Knowledge Augmentation for Enhanced Knowledge Distillation](https://openreview.net/forum?id=DmEHmZ89iB) |  | 0 | Do diverse perspectives help students learn better? Multi-teacher knowledge distillation, which is a more effective technique than traditional single-teacher methods, supervises the student from different perspectives (i.e., teacher). While effective, multi-teacher, teacher ensemble, or teaching... | Choong Seon Hong, EuiNam Huh, Md. Imtiaz Hossain, Sharmen Akhter |  |
| 2262 |  |  [SpaceGNN: Multi-Space Graph Neural Network for Node Anomaly Detection with Extremely Limited Labels](https://openreview.net/forum?id=Syt4fWwVm1) |  | 0 | Node Anomaly Detection (NAD) has gained significant attention in the deep learning community due to its diverse applications in real-world scenarios. Existing NAD methods primarily embed graphs within a single Euclidean space, while overlooking the potential of non-Euclidean spaces. Besides, to... | Lei Chen, Mingxuan Yuan, Sibo Wang, Xiangyu Dong, Xingyi Zhang |  |
| 2263 |  |  [Toward Exploratory Inverse Constraint Inference with Generative Diffusion Verifiers](https://openreview.net/forum?id=0UvlnHgaii) |  | 0 | An important prerequisite for safe control is aligning the policy with the underlying constraints in the environment. In many real-world applications, due to the difficulty of manually specifying these constraints, existing works have proposed recovering constraints from expert demonstrations by... | Bo Yue, Guiliang Liu, Runyi Zhao, Sheng Xu |  |
| 2264 |  |  [Exploring the Effectiveness of Object-Centric Representations in Visual Question Answering: Comparative Insights with Foundation Models](https://openreview.net/forum?id=DD11okKg13) |  | 0 | Object-centric (OC) representations, which model visual scenes as compositions of discrete objects, have the potential to be used in various downstream tasks to achieve systematic compositional generalization and facilitate reasoning. However, these claims have yet to be thoroughly validated... | Amir Mohammad KarimiMamaghan, Andrea Dittadi, Karl Henrik Johansson, Samuele Papa, Stefan Bauer |  |
| 2265 |  |  [ADMM for Nonconvex Optimization under Minimal Continuity Assumption](https://openreview.net/forum?id=GKAQ92ua3A) |  | 0 | This paper introduces a novel approach to solving multi-block nonconvex composite optimization problems through a proximal linearized Alternating Direction Method of Multipliers (ADMM). This method incorporates an Increasing Penalization and Decreasing Smoothing (IPDS) strategy. Distinguishing... | Ganzhao Yuan |  |
| 2266 |  |  [DOCS: Quantifying Weight Similarity for Deeper Insights into Large Language Models](https://openreview.net/forum?id=XBHoaHlGQM) |  | 0 | We introduce a novel index, the Distribution of Cosine Similarity (DOCS), for quantitatively assessing the similarity between weight matrices in Large Language Models (LLMs), aiming to facilitate the analysis of their complex architectures. Leveraging DOCS, our analysis uncovers intriguing patterns... | Xinshang Wang, Zeping Min |  |
| 2267 |  |  [Can LLMs Understand Time Series Anomalies?](https://openreview.net/forum?id=LGafQ1g2D2) |  | 0 | Large Language Models (LLMs) have gained popularity in time series forecasting, but their potential for anomaly detection remains largely unexplored. Our study investigates whether LLMs can understand and detect anomalies in time series data, focusing on zero-shot and few-shot scenarios. Inspired... | Rose Yu, Zihao Zhou |  |
| 2268 |  |  [Node-Time Conditional Prompt Learning in Dynamic Graphs](https://openreview.net/forum?id=kVlfYvIqaK) |  | 0 | Dynamic graphs capture evolving interactions between entities, such as in social networks, online learning platforms, and crowdsourcing projects. For dynamic graph modeling, dynamic graph neural networks (DGNNs) have emerged as a mainstream technique. However, they are generally pre-trained on the... | Xingtong Yu, Xinming Zhang, Yuan Fang, Zhenghao Liu |  |
| 2269 |  |  [Physics-Informed Deep Inverse Operator Networks for Solving PDE Inverse Problems](https://openreview.net/forum?id=0FxnSZJPmh) |  | 0 | Inverse problems involving partial differential equations (PDEs) can be seen as discovering a mapping from measurement data to unknown quantities, often framed within an operator learning approach. However, existing methods typically rely on large amounts of labeled training data, which is... | Hwijae Son, Sung Woong Cho |  |
| 2270 |  |  [X-Fi: A Modality-Invariant Foundation Model for Multimodal Human Sensing](https://openreview.net/forum?id=b42wmsdwmB) |  | 0 | Human sensing, which employs various sensors and advanced deep learning technologies to accurately capture and interpret human body information, has significantly impacted fields like public security and robotics. However, current human sensing primarily depends on modalities such as cameras and... | Jianfei Yang, Xinyan Chen |  |
| 2271 |  |  [Entropy-based Activation Function Optimization: A Method on Searching Better Activation Functions](https://openreview.net/forum?id=7TZYM6Hm9p) |  | 0 | The success of artificial neural networks (ANNs) hinges greatly on the judicious selection of an activation function, introducing non-linearity into network and enabling them to model sophisticated relationships in data. However, the search of activation functions has largely relied on empirical... | Bo Xia, Haoyuan Sun, Pu Chang, Xueqian Wang, Yifu Yuan, Yongzhe Chang, Zibin Dong, Zihao Wu |  |
| 2272 |  |  [Towards Synergistic Path-based Explanations for Knowledge Graph Completion: Exploration and Evaluation](https://openreview.net/forum?id=WQvkqarwXi) |  | 0 | Knowledge graph completion (KGC) aims to alleviate the inherent incompleteness of knowledge graphs (KGs), a crucial task for numerous applications such as recommendation systems and drug repurposing. The success of knowledge graph embedding (KGE) models provokes the question about the... | Bosheng Song, Jiani Zhang, Mufei Li, Tengfei Ma, Wen Tao, Xiang Song, Xiangxiang Zeng, Xiaoqin Pan, Yijun Wang |  |
| 2273 |  |  [InstaRevive: One-Step Image Enhancement via Dynamic Score Matching](https://openreview.net/forum?id=G1CN7R5qwE) |  | 0 | Image enhancement finds wide-ranging applications in real-world scenarios due to complex environments and the inherent limitations of imaging devices. Recent diffusion-based methods yield promising outcomes but necessitate prolonged and computationally intensive iterative sampling. In response, we... | Ao Li, Haolin Wang, Jie Zhou, Jingxuan Niu, Jiwen Lu, Lei Chen, Wenliang Zhao, Yansong Tang, Yixuan Zhu |  |
| 2274 |  |  [LLM Unlearning via Loss Adjustment with Only Forget Data](https://openreview.net/forum?id=6ESRicalFE) |  | 0 | Unlearning in Large Language Models (LLMs) is essential for ensuring ethical and responsible AI use, especially in addressing privacy leak, bias, safety, and evolving regulations. Existing approaches to LLM unlearning often rely on retain data or a reference LLM, yet they struggle to adequately... | Ankit Shah, Chris Yuhao Liu, Jiaheng Wei, Jinlong Pang, Quan Liu, Wei Wei, Yang Liu, Yaxuan Wang, Yujia Bao |  |
| 2275 |  |  [AI2TALE: An Innovative Information Theory-based Approach for Learning to Localize Phishing Attacks](https://openreview.net/forum?id=3xpTXF5ALZ) |  | 0 | Phishing attacks remain a significant challenge for detection, explanation, and defense, despite over a decade of research on both technical and non-technical solutions. AI-based phishing detection methods are among the most effective approaches for defeating phishing attacks, providing predictions... | Carsten Rudolph, Marthie Grobler, Surya Nepal, Tingmin Wu, Van Nguyen, Xingliang Yuan |  |
| 2276 |  |  [A Distributional Approach to Uncertainty-Aware Preference Alignment Using Offline Demonstrations](https://openreview.net/forum?id=RKOAU5ti1y) |  | 0 | Designing reward functions in Reinforcement Learning (RL) often demands significant task-specific expertise. Offline Preference-based Reinforcement Learning (PbRL) provides an effective alternative to address the complexity of reward design by learning policies from offline datasets that contain... | Bo Yue, Guiliang Liu, Hongyuan Zha, Sheng Xu |  |
| 2277 |  |  [Bridging Jensen Gap for Max-Min Group Fairness Optimization in Recommendation](https://openreview.net/forum?id=1PDz4Ny1N2) |  | 0 | Group max-min fairness (MMF) is commonly used in fairness-aware recommender systems (RS) as an optimization objective, as it aims to protect marginalized item groups and ensures a fair competition platform. However, our theoretical analysis indicates that integrating MMF constraint violates the... | Chen Xu, Jun Xu, Liang Pang, TatSeng Chua, Wenjie Wang, Yuxin Li |  |
| 2278 |  |  [DaWin: Training-free Dynamic Weight Interpolation for Robust Adaptation](https://openreview.net/forum?id=L8e7tBf4pP) |  | 0 | Adapting a pre-trained foundation model on downstream tasks should ensure robustness against distribution shifts without the need to retrain the whole model. Although existing weight interpolation methods are simple yet effective, we argue their static nature limits downstream performance while... | Changdae Oh, Dongyoon Han, Kyungwoo Song, Sangdoo Yun, Yixuan Li |  |
| 2279 |  |  [Competitive Fair Scheduling with Predictions](https://openreview.net/forum?id=jBYQAtzp5Z) |  | 0 | Beyond the worst-case analysis of algorithms, the learning-augmented framework considers that an algorithm can leverage possibly imperfect predictions about the unknown variables to have guarantees tied to the prediction quality. We consider online non-clairvoyant scheduling to minimize the... | Albert Y. Zomaya, Chunhao Li, Chunqiu Xia, Tianming Zhao, Wei Li, Xiaomin Chang |  |
| 2280 |  |  [Demystifying Online Clustering of Bandits: Enhanced Exploration Under Stochastic and Smoothed Adversarial Contexts](https://openreview.net/forum?id=421D67DY3i) |  | 0 | The contextual multi-armed bandit (MAB) problem is crucial in sequential decision-making. A line of research, known as online clustering of bandits, extends contextual MAB by grouping similar users into clusters, utilizing shared features to improve learning efficiency. However, existing... | John C. S. Lui, Maoli Liu, Xiangxiang Dai, Zhuohua Li |  |
| 2281 |  |  [High-Precision Dichotomous Image Segmentation via Probing Diffusion Capacity](https://openreview.net/forum?id=vh1e2WJfZp) |  | 0 | In the realm of high-resolution (HR), fine-grained image segmentation, the primary challenge is balancing broad contextual awareness with the precision required for detailed object delineation, capturing intricate details and the finest edges of objects. Diffusion models, trained on vast datasets... | Bo Li, Hao Zhang, Huchuan Lu, Jinwei Chen, Lihe Zhang, PengTao Jiang, Qian Yu |  |
| 2282 |  |  [Delta: Dense Efficient Long-Range 3D tracking for any video](https://openreview.net/forum?id=d9iHI1eimo) |  | 0 | Tracking dense 3D motion from monocular videos remains challenging, particularly when aiming for pixel-level precision over long sequences. We introduce DELTA, a novel method that efficiently tracks every pixel in 3D space, enabling accurate motion estimation across entire videos. Our approach... | Chaoyang Wang, Chuang Gan, Evangelos Kalogerakis, HsinYing Lee, Peiye Zhuang, Sergey Tulyakov, Tuan Duc Ngo |  |
| 2283 |  |  [Unsupervised Multiple Kernel Learning for Graphs via Ordinality Preservation](https://openreview.net/forum?id=6nb2J90XJD) |  | 0 | Learning effective graph similarities is crucial for tasks like clustering, yet selecting the optimal kernel to evaluate such similarities in unsupervised settings remains a major challenge. Despite the development of various graph kernels, determining the most appropriate one for a specific task... | Stanley Kok, Yan Sun |  |
| 2284 |  |  [Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge](https://openreview.net/forum?id=3GTtZFiajM) |  | 0 | LLM-as-a-Judge has been widely utilized as an evaluation method in various benchmarks and served as supervised rewards in model training. However, despite their excellence in many domains, potential issues are under-explored, undermining their reliability and the scope of their utility. Therefore,... | Chao Huang, Dongping Chen, Jiayi Ye, Nitesh V. Chawla, Nuno Moniz, PinYu Chen, Qihui Zhang, Tian Gao, Werner Geyer, Xiangliang Zhang, Yanbo Wang, Yue Huang |  |
| 2285 |  |  [OmnixR: Evaluating Omni-modality Language Models on Reasoning across Modalities](https://openreview.net/forum?id=jki6EFsZLw) |  | 0 | We introduce \textbf{OmnixR}, an evaluation suite designed to benchmark state-of-the-art Omni-modality Language Models (OLMs), such as GPT-4o and Gemini. Evaluating OLMs, which integrate multiple modalities such as text, vision, and audio, presents unique challenges. Particularly, the user message... | Boqing Gong, Heng Huang, Hexiang Hu, Lichang Chen, MingHsuan Yang, Mingda Zhang, Pranav Shyam, Tianyi Zhou, Yandong Li, Yiwen Chen, Zifeng Wang |  |
| 2286 |  |  [Planning Anything with Rigor: General-Purpose Zero-Shot Planning with LLM-based Formalized Programming](https://openreview.net/forum?id=0K1OaL6XuK) |  | 0 | While large language models (LLMs) have recently demonstrated strong potential in solving planning problems, there is a trade-off between flexibility and complexity. LLMs, as zero-shot planners themselves, are still not capable of directly generating valid plans for complex planning problems such... | Chuchu Fan, Yang Zhang, Yilun Hao |  |
| 2287 |  |  [MolSpectra: Pre-training 3D Molecular Representation with Multi-modal Energy Spectra](https://openreview.net/forum?id=xJDxVDG3x2) |  | 0 | Establishing the relationship between 3D structures and the energy states of molecular systems has proven to be a promising approach for learning 3D molecular representations. However, existing methods are limited to modeling the molecular energy states from classical mechanics. This limitation... | Deli Zhao, Liang Wang, Liang Wang, Qiang Liu, Shaozhen Liu, Shu Wu, Yu Rong |  |
| 2288 |  |  [Enhancing Clustered Federated Learning: Integration of Strategies and Improved Methodologies](https://openreview.net/forum?id=zPDpdk3V8L) |  | 0 | Federated Learning (FL) is an evolving distributed machine learning approach that safeguards client privacy by keeping data on edge devices. However, the variation in data among clients poses challenges in training models that excel across all local distributions. Recent studies suggest clustering... | Tao Lin, Xiaoying Tang, Yongxin Guo |  |
| 2289 |  |  [INFER: A Neural-symbolic Model For Extrapolation Reasoning on Temporal Knowledge Graph](https://openreview.net/forum?id=ExHUtB2vnz) |  | 0 | Temporal Knowledge Graph(TKG) serves as an efficacious way to store dynamic facts in real-world. Extrapolation reasoning on TKGs, which aims at predicting possible future events, has attracted consistent research interest. Recently, some rule-based methods have been proposed, which are considered... | Haihong E, Haoran Luo, Meina Song, Ningyuan Li, Tianyi Hu, Tianyu Yao, Yifan Zhu, Yuhan Li |  |
| 2290 |  |  [Supervised and Semi-Supervised Diffusion Maps with Label-Driven Diffusion](https://openreview.net/forum?id=G3B5ReApDw) |  | 0 | In this paper, we introduce Supervised Diffusion Maps (SDM) and Semi-Supervised Diffusion Maps (SSDM), which transform the well-known unsupervised dimensionality reduction algorithm, Diffusion Maps, into supervised and semi-supervised learning tools. The proposed methods, SDM and SSDM, are based on... | Harel Mendelman, Ronen Talmon |  |
| 2291 |  |  [Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models with Flow Matching](https://openreview.net/forum?id=zKlFXV87Pp) |  | 0 | Autoregressive (AR) models have recently achieved state-of-the-art performance in text and image generation. However, their primary limitation is slow generation speed due to the token-by-token process. We ask an ambitious question: can a pre-trained AR model be adapted to generate outputs in just... | Enshu Liu, Xuefei Ning, Yu Wang, Zinan Lin |  |
| 2292 |  |  [Policy Decorator: Model-Agnostic Online Refinement for Large Policy Model](https://openreview.net/forum?id=e5jGTEiJMT) |  | 0 | Recent advancements in robot learning have used imitation learning with large models and extensive demonstrations to develop effective policies. However, these models are often limited by the quantity quality, and diversity of demonstrations. This paper explores improving offline-trained imitation... | Hao Su, Mengke Zhang, Stone Tao, Tongzhou Mu, Xiu Yuan, Yunhao Fang |  |
| 2293 |  |  [Precise Localization of Memories: A Fine-grained Neuron-level Knowledge Editing Technique for LLMs](https://openreview.net/forum?id=5xP1HDvpXI) |  | 0 | Knowledge editing aims to update outdated information in Large Language Models (LLMs). A representative line of study is locate-then-edit methods, which typically employ causal tracing to identify the modules responsible for recalling factual knowledge about entities. However, we find these methods... | Haowen Pan, Juanzi Li, Meng Wang, Xiaozhi Wang, Xun Yang, Yixin Cao, Zenglin Shi |  |
| 2294 |  |  [Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solver](https://openreview.net/forum?id=6aHUmotXaw) |  | 0 | This paper introduces rStar, a self-play mutual reasoning approach that significantly improves reasoning capabilities of small language models (SLMs) without fine-tuning or superior models. rStar decouples reasoning into a self-play mutual generation-discrimination process. First, a target SLM... | Fan Yang, Jiahang Xu, Li Lyna Zhang, Mao Yang, Mingyuan Ma, Zhenting Qi |  |
| 2295 |  |  [On the Importance of Language-driven Representation Learning for Heterogeneous Federated Learning](https://openreview.net/forum?id=7pDI74iOyu) |  | 0 | Non-Independent and Identically Distributed (Non-IID) training data significantly challenge federated learning (FL), impairing the performance of the global model in distributed frameworks. Inspired by the superior performance and generalizability of language-driven representation learning in... | ChunMei Feng, Lei Zhu, Salman H. Khan, Wangmeng Zuo, Yong Liu, Yunlu Yan |  |
| 2296 |  |  [Is Factuality Enhancement a Free Lunch For LLMs? Better Factuality Can Lead to Worse Context-Faithfulness](https://openreview.net/forum?id=asGQQc7gNo) |  | 0 | As the modern tools of choice for text understanding and generation, large language models (LLMs) are expected to accurately output answers by leveraging the input context. This requires LLMs to possess both context-faithfulness and factual accuracy. While extensive efforts aim to reduce... | Baolong Bi, Hongcheng Gao, Junfeng Fang, Lingrui Mei, Shenghua Liu, Shiyu Ni, Xueqi Cheng, Yiwei Wang |  |
| 2297 |  |  [TopoGaussian: Inferring Internal Topology Structures from Visual Clues](https://openreview.net/forum?id=B5PbOsJqt3) |  | 0 | We present TopoGaussian, a holistic, particle-based pipeline for inferring the interior structure of an opaque object from easily accessible photos and videos as input. Traditional mesh-based approaches require tedious and error-prone mesh filling and fixing process, while typically output rough... | Changyu Hu, Chuang Gan, Chunru Lin, Pingchuan Ma, Tao Du, Xiaoyu Xiong |  |
| 2298 |  |  [Tree of Attributes Prompt Learning for Vision-Language Models](https://openreview.net/forum?id=wFs2E5wCw6) |  | 0 | Prompt learning has proven effective in adapting vision language models for downstream tasks. However, existing methods usually append learnable prompt tokens solely with the category names to obtain textual features, which fails to fully leverage the rich context indicated in the category name. To... | Hanspeter Pfister, Tong Ding, Wanhua Li, Zhongqi Miao |  |
| 2299 |  |  [DiffusionGuard: A Robust Defense Against Malicious Diffusion-based Image Editing](https://openreview.net/forum?id=9OfKxKoYNw) |  | 0 | Recent advances in diffusion models have introduced a new era of text-guided image manipulation, enabling users to create realistic edited images with simple textual prompts. However, there is significant concern about the potential misuse of these methods, especially in creating misleading or... | Jinwoo Shin, Jongheon Jeong, June Suk Choi, Kimin Lee, Kyungmin Lee, Saining Xie |  |
| 2300 |  |  [On the Performance Analysis of Momentum Method: A Frequency Domain Perspective](https://openreview.net/forum?id=tznvtmSEiN) |  | 0 | Momentum-based optimizers are widely adopted for training neural networks. However, the optimal selection of momentum coefficients remains elusive. This uncertainty impedes a clear understanding of the role of momentum in stochastic gradient methods. In this paper, we present a frequency domain... | Hanxiao Wang, Jun Luo, Li Luo, Lingkun Wen, Linlong Wu, Sheng Xu, Xianliang Li, Zhiwei Zheng |  |
| 2301 |  |  [3DMolFormer: A Dual-channel Framework for Structure-based Drug Discovery](https://openreview.net/forum?id=RgE1qiO2ek) |  | 0 | Structure-based drug discovery, encompassing the tasks of protein-ligand docking and pocket-aware 3D drug design, represents a core challenge in drug discovery. However, no existing work can deal with both tasks to effectively leverage the duality between them, and current methods for each task are... | Can Chen, Guoqing Liu, Hao Zhang, Xiuyuan Hu, Xue Liu, Yang Zhao |  |
| 2302 |  |  [Evidential Learning-based Certainty Estimation for Robust Dense Feature Matching](https://openreview.net/forum?id=4NWtrQciRH) |  | 0 | Dense feature matching methods aim to estimate a dense correspondence field between images. Inaccurate correspondence can occur due to the presence of unmatchable region, necessitating the need for certainty measurement. This is typically addressed by training a binary classifier to decide whether... | ChuanSheng Foo, Jun Cheng, Lile Cai, Xulei Yang, Xun Xu, Zaiwang Gu |  |
| 2303 |  |  [Language models scale reliably with over-training and on downstream tasks](https://openreview.net/forum?id=iZeQBqJamf) |  | 0 | Scaling laws are useful guides for derisking expensive training runs, as they predict performance of large models using cheaper, small-scale experiments. However, there remain gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is... | Alex Dimakis, Alex Fang, Gabriel Ilharco, Georgios Smyrnis, Igor Vasiljevic, Jean Mercat, Jeffrey Li, Jenia Jitsev, Luca Soldaini, Marianna Nezhurina, Mitchell Wortsman, Pang Wei Koh, Rui Xin, Rulin Shao, Samir Yitzhak Gadre, Sedrick Keh, Shuran Song, Suchin Gururangan, Thomas Kollar, Vaishaal Shankar, et al. |  |
| 2304 |  |  [ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL](https://openreview.net/forum?id=BAglD6NGy0) |  | 0 | Despite the significant advancements in Text-to-SQL (Text2SQL) facilitated by large language models (LLMs), the latest state-of-the-art techniques are still trapped in the in-context learning of closed-source LLMs (e.g., GPT-4), which limits their applicability in open scenarios. To address this... | Chao Chen, Dezhong Peng, Jieping Ye, Peng Hu, Yang Qin, Ze Chen, Zhihang Fu |  |
| 2305 |  |  [Towards a Theoretical Understanding of Synthetic Data in LLM Post-Training: A Reverse-Bottleneck Perspective](https://openreview.net/forum?id=UxkznlcnHf) |  | 0 | Synthetic data has become a pivotal resource in post-training tasks for large language models (LLMs) due to the scarcity of high-quality, specific data. While various methods have been developed to generate synthetic data, there remains a discernible gap between the practical effects of synthetic... | Yong Liu, Zeyu Gan |  |
| 2306 |  |  [I2AM: Interpreting Image-to-Image Latent Diffusion Models via Bi-Attribution Maps](https://openreview.net/forum?id=bBNUiErs26) |  | 0 | Large-scale diffusion models have made significant advances in image generation, particularly through cross-attention mechanisms. While cross-attention has been well-studied in text-to-image tasks, their interpretability in image-to-image (I2I) diffusion models remains underexplored. This paper... | Hyeryung Jang, Junseo Park |  |
| 2307 |  |  [3D StreetUnveiler with Semantic-aware 2DGS - a simple baseline](https://openreview.net/forum?id=G6aJyS0ZV0) |  | 0 | Unveiling an empty street from crowded observations captured by in-car cameras is crucial for autonomous driving. However, removing all temporarily static objects, such as stopped vehicles and standing pedestrians, presents a significant challenge. Unlike object-centric 3D inpainting, which relies... | Jingwei Xu, Shenghua Gao, Yanwei Fu, Yikai Wang, Yiqun Zhao |  |
| 2308 |  |  [Prompt as Knowledge Bank: Boost Vision-language model via Structural Representation for zero-shot medical detection](https://openreview.net/forum?id=l0t2rumAvR) |  | 0 | Zero-shot medical detection can further improve detection performance without relying on annotated medical images even upon the fine-tuned model, showing great clinical value. Recent studies leverage grounded vision-language models (GLIP) to achieve this by using detailed disease descriptions as... | Baochang Zhang, Chunyu Xie, Dawei Leng, Haoyu Huang, Linlin Yang, Tongfei Chen, Xianbin Cao, Yuguang Yang |  |
| 2309 |  |  [Adam-mini: Use Fewer Learning Rates To Gain More](https://openreview.net/forum?id=iBExhaU3Lc) |  | 0 | We propose Adam-mini, an optimizer that achieves on-par or better performance than AdamW with $50$% less memory footprint. Adam-mini reduces memory by cutting down the learning rate resources in Adam (i.e., $1/\sqrt{v}$). By delving into the Hessian structure of neural nets, we find Adam’s $v$... | Chenwei Wu, Congliang Chen, Diederik P. Kingma, Ruoyu Sun, Tian Ding, Yinyu Ye, Yushun Zhang, ZhiQuan Luo, Ziniu Li |  |
| 2310 |  |  [An Engorgio Prompt Makes Large Language Model Babble on](https://openreview.net/forum?id=m4eXBo0VNc) |  | 0 | Auto-regressive large language models (LLMs) have yielded impressive performance in many real-world tasks. However, the new paradigm of these LLMs also exposes novel threats. In this paper, we explore their vulnerability to inference cost attacks, where a malicious user crafts Engorgio prompts to... | Chao Zhang, Han Qiu, Hao Wang, Hewu Li, Jianshuo Dong, Ke Xu, Qi Li, Qingjie Zhang, Tianwei Zhang, Ziyuan Zhang |  |
| 2311 |  |  [Learning to Contextualize Web Pages for Enhanced Decision Making by LLM Agents](https://openreview.net/forum?id=3Gzz7ZQLiz) |  | 0 | Recent advances in large language models (LLMs) have led to a growing interest in developing LLM-based agents for automating web tasks. However, these agents often struggle with even simple tasks on real-world websites due to their limited capability to understand and process complex web page... | Dongjun Lee, Jihoon Tack, Jinwoo Shin, Juyong Lee, Kimin Lee, Kyuyoung Kim, Yee Whye Teh |  |
| 2312 |  |  [Energy-Based Diffusion Language Models for Text Generation](https://openreview.net/forum?id=sL2F9YCMXf) |  | 0 | Despite remarkable progress in autoregressive language models, alternative generative paradigms beyond left-to-right generation are still being actively explored. Discrete diffusion models, with the capacity for parallel generation, have recently emerged as a promising alternative. Unfortunately,... | Arash Vahdat, Jure Leskovec, Karsten Kreis, Minkai Xu, Stefano Ermon, Tomas Geffner, Weili Nie, Yilun Xu |  |
| 2313 |  |  [From GNNs to Trees: Multi-Granular Interpretability for Graph Neural Networks](https://openreview.net/forum?id=KEUPk0wXXe) |  | 0 | Interpretable Graph Neural Networks (GNNs) aim to reveal the underlying reasoning behind model predictions, attributing their decisions to specific subgraphs that are informative. However, existing subgraph-based interpretable methods suffer from an overemphasis on local structure, potentially... | Ji Cao, Jie Yang, Kaixuan Chen, Mingli Song, Shunyu Liu, Tongya Zheng, Yihe Zhou, Yuwen Wang, Zhenbang Xiao |  |
| 2314 |  |  [Multi-objective Differentiable Neural Architecture Search](https://openreview.net/forum?id=9mjZ800m7Y) |  | 0 | Pareto front profiling in multi-objective optimization (MOO), i.e., finding a diverse set of Pareto optimal solutions, is challenging, especially with expensive objectives that require training a neural network. Typically, in MOO for neural architecture search (NAS), we aim to balance performance... | Arber Zela, Benedikt Staffler, Frank Hutter, Josif Grabocka, Rhea Sanjay Sukthanker, Samuel Dooley |  |
| 2315 |  |  [Generative Representational Instruction Tuning](https://openreview.net/forum?id=BC4lIvfSzv) |  | 0 | All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by... | Amanpreet Singh, Douwe Kiela, Furu Wei, Hongjin Su, Liang Wang, Nan Yang, Niklas Muennighoff, Tao Yu |  |
| 2316 |  |  [CogCoM: A Visual Language Model with Chain-of-Manipulations Reasoning](https://openreview.net/forum?id=Fg0eo2AkST) |  | 0 | Vision-Language Models (VLMs) have shown broad effectiveness due to extensive training that aligns visual inputs with corresponding language responses. However, this conclusive alignment training causes models to overlook essential visual reasoning, leading to failures in handling detailed visual... | Bin Xu, Ji Qi, Jie Tang, Juanzi Li, Lei Hou, Ming Ding, Qingsong Lv, Weihan Wang, Wenyi Hong, Yushi Bai, Yuxiao Dong |  |
| 2317 |  |  [TabDiff: a Mixed-type Diffusion Model for Tabular Data Generation](https://openreview.net/forum?id=swvURjrt8z) |  | 0 | Synthesizing high-quality tabular data is an important topic in many data science tasks, ranging from dataset augmentation to privacy protection. However, developing expressive generative models for tabular data is challenging due to its inherent heterogeneous data types, complex... | Harper Hua, Hengrui Zhang, Juntong Shi, Jure Leskovec, Minkai Xu, Stefano Ermon |  |
| 2318 |  |  [Jamba: Hybrid Transformer-Mamba Language Models](https://openreview.net/forum?id=JFPaD7lpBD) |  | 0 | We present Jamba, a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage... | Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Lenz, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, Daniel Gissin, Daniel Jannai, Dor Muhlgay, Dor Zimberg, Edden M. Gerber, Elad Dolev, Eran Krakovsky, Erez Safahi, Erez Schwartz, Gal Cohen, Opher Lieber, et al. |  |
| 2319 |  |  [Rodimus\*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions](https://openreview.net/forum?id=IIVYiJ1ggK) |  | 0 | Recent advancements in Transformer-based large language models (LLMs) have set new standards in natural language processing. However, the classical softmax attention incurs significant computational costs, leading to a $O(T)$ complexity for per-token generation, where $T$ represents the context... | Hang Yu, Jianguo Li, Shizhan Liu, Weiyao Lin, Zhihao He, Zi Gong |  |
| 2320 |  |  [KinFormer: Generalizable Dynamical Symbolic Regression for Catalytic Organic Reaction Kinetics](https://openreview.net/forum?id=nhrXqy5d5q) |  | 0 | Modeling kinetic equations is essential for understanding the mechanisms of chemical reactions, yet a complex and time-consuming task. Kinetic equation prediction is formulated as a problem of dynamical symbolic regression (DSR) subject to physical chemistry constraints. Deep learning (DL) holds... | ChenXinWei, Jidong Tian, Jindou Chen, Liang Wu, Xiaokang Yang, Yanyan Xu, Yaohui Jin |  |
| 2321 |  |  [IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and Illuminations](https://openreview.net/forum?id=uuef1HP6X7) |  | 0 | Capturing geometric and material information from images remains a fundamental challenge in computer vision and graphics. Traditional optimization-based methods often require hours of computational time to reconstruct geometry, material properties, and environmental lighting from dense multi-view... | Dahua Lin, Jiaqi Wang, Jing Tan, Mengchen Zhang, Tong Wu, Zhibing Li |  |
| 2322 |  |  [Learning 3D Perception from Others' Predictions](https://openreview.net/forum?id=Ylk98vWQuQ) |  | 0 | Accurate 3D object detection in real-world environments requires a huge amount of annotated data with high quality. Acquiring such data is tedious and expensive, and often needs repeated effort when a new sensor is adopted or when the detector is deployed in a new environment. We investigate a new... | Bharath Hariharan, Cheng Perng Phoo, Jinsu Yoo, Kilian Q. Weinberger, Mark E. Campbell, TaiYu Pan, WeiLun Chao, Xiangyu Chen, Yihong Sun, Zhenyang Feng |  |
| 2323 |  |  [SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding](https://openreview.net/forum?id=8dzKkeWUUb) |  | 0 | Scientific literature understanding is crucial for extracting targeted information and garnering insights, thereby significantly advancing scientific discovery. Despite the remarkable success of Large Language Models (LLMs), they face challenges in scientific literature understanding, primarily due... | Guolin Ke, Hengxing Cai, Jiaxi Zhuang, Jin Huang, Linfeng Zhang, Mingjun Xu, Sihang Li, Xiang Wang, Xiaochen Cai, Yaorui Shi |  |
| 2324 |  |  [VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks](https://openreview.net/forum?id=TE0KOzWYAF) |  | 0 | Embedding models play a crucial role in a variety of downstream tasks, including semantic similarity, information retrieval, and clustering. While there has been a surge of interest in developing universal text embedding models that generalize across tasks (e.g., MTEB), progress in learning... | Rui Meng, Semih Yavuz, Wenhu Chen, Xinyi Yang, Yingbo Zhou, Ziyan Jiang |  |
| 2325 |  |  [MMDisCo: Multi-Modal Discriminator-Guided Cooperative Diffusion for Joint Audio and Video Generation](https://openreview.net/forum?id=agbiPPuSeQ) |  | 0 | This study aims to construct an audio-video generative model with minimal computational cost by leveraging pre-trained single-modal generative models for audio and video. To achieve this, we propose a novel method that guides single-modal models to cooperatively generate well-aligned samples across... | Akio Hayakawa, Masato Ishii, Takashi Shibuya, Yuki Mitsufuji |  |
| 2326 |  |  [Enhanced Diffusion Sampling via Extrapolation with Multiple ODE Solutions](https://openreview.net/forum?id=rCGleSgNBK) |  | 0 | Diffusion probabilistic models (DPMs), while effective in generating high-quality samples, often suffer from high computational costs due to their iterative sampling process. To address this, we propose an enhanced ODE-based sampling method for DPMs inspired by Richardson extrapolation, which... | Bohyung Han, Jinyoung Choi, Junoh Kang |  |
| 2327 |  |  [MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers](https://openreview.net/forum?id=KGZAs8VcOM) |  | 0 | Recently, 3D assets created via reconstruction and generation have matched the quality of manually crafted assets, highlighting their potential for replacement. However, this potential is largely unrealized because these assets always need to be converted to meshes for 3D industry applications, and... | Chi Zhang, Di Huang, Gang Yu, Guosheng Lin, Jiaxiang Tang, Lei Yang, Sijin Chen, Tong He, Weicai Ye, Yiwen Chen, Zhongang Cai |  |
| 2328 |  |  [Anyprefer: An Agentic Framework for Preference Data Synthesis](https://openreview.net/forum?id=WpZyPk79Fu) |  | 0 | High-quality preference data is essential for aligning foundation models with human values through preference learning. However, manual annotation of such data is often time-consuming and costly. Recent methods often adopt a self-rewarding approach, where the target model generates and annotates... | Bo Li, Chetan Bansal, Huaxiu Yao, Kaiyuan Zheng, Mohit Bansal, Peng Xia, Shangyu Xing, Tianle Wang, Weitong Zhang, Wenhao Zheng, Xuchao Zhang, Ying Wei, Yiyang Zhou, Zhaorun Chen, Zhaoyang Wang, Zijian Zhang |  |
| 2329 |  |  [Personalized Representation from Personalized Generation](https://openreview.net/forum?id=jw7P4MHLWw) |  | 0 | Modern vision models excel at general purpose downstream tasks. It is unclear, however, how they may be used for personalized vision tasks, which are both fine-grained and data-scarce. Recent works have successfully applied synthetic data to general-purpose representation learning, while advances... | Julia Chae, Phillip Isola, Sara Beery, Shobhita Sundaram, Yonglong Tian |  |
| 2330 |  |  [Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View](https://openreview.net/forum?id=u8VOQVzduP) |  | 0 | Large language models (LLMs) have been shown to face hallucination issues due to the data they trained on often containing human bias; whether this is reflected in the decision-making process of LLM agents remains under-explored. As LLM Agents are increasingly employed in intricate social... | Chengxu Yang, Haoyang Shang, Jie Zhang, Quanyan Zhu, Song Guo, Xuan Liu |  |
| 2331 |  |  [SVG: 3D Stereoscopic Video Generation via Denoising Frame Matrix](https://openreview.net/forum?id=sx2jXZuhIx) |  | 0 | Video generation models have demonstrated great capability of producing impressive monocular videos, however, the generation of 3D stereoscopic video remains under-explored. We propose a pose-free and training-free approach for generating 3D stereoscopic videos using an off-the-shelf monocular... | David Futschik, Feitong Tan, Peng Dai, Qiangeng Xu, Ruofei Du, Sean Fanello, Xiaojuan Qi, Yinda Zhang |  |
| 2332 |  |  [Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws](https://openreview.net/forum?id=aqok1UX7Z1) |  | 0 | The composition of pretraining data is a key determinant of foundation models' performance, but there is no standard guideline for allocating a limited computational budget across different data sources. Most current approaches either rely on extensive experiments with smaller models or dynamic... | Allan Zhou, J. Zico Kolter, Sadhika Malladi, Yiding Jiang, Zhili Feng |  |
| 2333 |  |  [Perturbation-Restrained Sequential Model Editing](https://openreview.net/forum?id=bfI8cp8qmk) |  | 0 | Model editing is an emerging field that focuses on updating the knowledge embedded within large language models (LLMs) without extensive retraining. However, current model editing methods significantly compromise the general abilities of LLMs as the number of edits increases, and this trade-off... | HaoXiang Xu, Hong Wang, JiaChen Gu, JunYu Ma, ZhenHua Ling |  |
| 2334 |  |  [Incorporating Visual Correspondence into Diffusion Model for Virtual Try-On](https://openreview.net/forum?id=XXzOzJRyOZ) |  | 0 | Diffusion models have shown preliminary success in virtual try-on (VTON) task. The typical dual-branch architecture comprises two UNets for implicit garment deformation and synthesized image generation respectively, and has emerged as the recipe for VTON task. Nevertheless, the problem remains... | Jingwen Chen, Siqi Wan, Tao Mei, Ting Yao, Yingwei Pan |  |
| 2335 |  |  [The adaptive complexity of parallelized log-concave sampling](https://openreview.net/forum?id=EeqlkPpaV8) |  | 0 | In large-data applications, such as the inference process of diffusion models, it is desirable to design sampling algorithms with a high degree of parallelization. In this work, we study the adaptive complexity of sampling, which is the minimum number of sequential rounds required to achieve... | Baoxiang Wang, Huanjian Zhou, Masashi Sugiyama |  |
| 2336 |  |  [MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine](https://openreview.net/forum?id=IwgmgidYPS) |  | 0 | This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal dataset for medicine, covering over 25 million images across 10 modalities with multigranular annotations for more than 65 diseases. These multigranular annotations encompass both global information, such as modality and... | Ce Zhou, Cihang Xie, HongYu Zhou, James Zou, Juncheng Wu, Lang Gao, Lei Xing, Sheng Liu, Xianhang Li, Yunfei Xie, Yuyin Zhou |  |
| 2337 |  |  [CBraMod: A Criss-Cross Brain Foundation Model for EEG Decoding](https://openreview.net/forum?id=NPNUHgHF2w) |  | 0 | Electroencephalography (EEG) is a non-invasive technique to measure and record brain electrical activity, widely used in various BCI and healthcare applications. Early EEG decoding methods rely on supervised learning, limited by specific tasks and datasets, hindering model performance and... | Gang Pan, Haiteng Jiang, Jiquan Wang, Sha Zhao, Shijian Li, Tao Li, Yangxuan Zhou, Zhiling Luo |  |
| 2338 |  |  [PFDiff: Training-Free Acceleration of Diffusion Models Combining Past and Future Scores](https://openreview.net/forum?id=wmmDvZGFK7) |  | 0 | Diffusion Probabilistic Models (DPMs) have shown remarkable potential in image generation, but their sampling efficiency is hindered by the need for numerous denoising steps. Most existing solutions accelerate the sampling process by proposing fast ODE solvers. However, the inevitable... | Guangyi Wang, Lijiang Li, SongZhi Su, Wei Peng, Yuren Cai |  |
| 2339 |  |  [Consistent Flow Distillation for Text-to-3D Generation](https://openreview.net/forum?id=A51NEXIq1J) |  | 0 | Score Distillation Sampling (SDS) has made significant strides in distilling image-generative models for 3D generation. However, its maximum-likelihood-seeking behavior often leads to degraded visual quality and diversity, limiting its effectiveness in 3D applications. In this work, we propose... | Runjie Yan, Xiaolong Wang, Yinbo Chen |  |
| 2340 |  |  [Robust-PIFu: Robust Pixel-aligned Implicit Function for 3D Human Digitalization from a Single Image](https://openreview.net/forum?id=ftdJEiFudy) |  | 0 | Existing methods for 3D clothed human digitalization perform well when the input image is captured in ideal conditions that assume the lack of any occlusion. However, in reality, images may often have occlusion problems such as incomplete observation of the human subject's full body, self-occlusion... | ChuanSheng Foo, Fayao Liu, Guosheng Lin, Kennard Yanting Chan, Weisi Lin |  |
| 2341 |  |  [Descent with Misaligned Gradients and Applications to Hidden Convexity](https://openreview.net/forum?id=2L4PTJO8VQ) |  | 0 | We consider the problem of minimizing a convex objective given access to an oracle that outputs "misaligned" stochastic gradients, where the expected value of the output is guaranteed to be correlated with, but not necessarily equal to the true gradient of the objective. In the case where the... | Aditya Bhaskara, Ashok Cutkosky, Manish Purohit, Ravi Kumar |  |
| 2342 |  |  [On Statistical Rates of Conditional Diffusion Transformers: Approximation, Estimation and Minimax Optimality](https://openreview.net/forum?id=c54apoozCS) |  | 0 | We investigate the approximation and estimation rates of conditional diffusion transformers (DiTs) with classifier-free guidance. We present a comprehensive analysis for “in-context” conditional DiTs under various common assumptions: generic and strong Hölder, linear latent (subspace), and... | Han Liu, Jerry YaoChieh Hu, Minshuo Chen, Weimin Wu, YiChen Lee, YuChao Huang |  |
| 2343 |  |  [Universal Image Restoration Pre-training via Degradation Classification](https://openreview.net/forum?id=PacBhLzeGO) |  | 0 | This paper proposes the Degradation Classification Pre-Training (DCPT), which enables models to learn how to classify the degradation type of input images for universal image restoration pre-training. Unlike the existing self-supervised pre-training methods, DCPT utilizes the degradation type of... | Jiakui Hu, Lujia Jin, Yanye Lu, Zhengjian Yao |  |
| 2344 |  |  [On-the-fly Preference Alignment via Principle-Guided Decoding](https://openreview.net/forum?id=cfn2O1qvxp) |  | 0 | With the rapidly expanding landscape of large language models, aligning model generations with human values and preferences is becoming increasingly important. Popular alignment methods, such as Reinforcement Learning from Human Feedback, have shown significant success in guiding models with... | Junbo Guo, Lei Zhang, Mingye Zhu, Yi Liu, Zhendong Mao |  |
| 2345 |  |  [Analysis of Linear Mode Connectivity via Permutation-Based Weight Matching: With Insights into Other Permutation Search Methods](https://openreview.net/forum?id=lYRkGZZi9D) |  | 0 | Recently, Ainsworth et al. (2023) showed that using weight matching (WM) to minimize the $L^2$ distance in a permutation search of model parameters effectively identifies permutations that satisfy linear mode connectivity (LMC), where the loss along a linear path between two independently trained... | Akira Ito, Atsutoshi Kumagai, Masanori Yamada |  |
| 2346 |  |  [ConMix: Contrastive Mixup at Representation Level for Long-tailed Deep Clustering](https://openreview.net/forum?id=3lH8WT0fhu) |  | 0 | Deep clustering has made remarkable progress in recent years. However, most existing deep clustering methods assume that distributions of different clusters are balanced or roughly balanced, which are not consistent with the common long-tailed distributions in reality. In nature, the datasets often... | Yuheng Jia, Zhixin Li |  |
| 2347 |  |  [ADePT: Adaptive Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning](https://openreview.net/forum?id=fswihJIYbd) |  | 0 | Prompt Tuning (PT) enables the adaptation of Pre-trained Large Language Models (PLMs) to downstream tasks by optimizing a small amount of soft virtual tokens, which are prepended to the input token embeddings. Recently, Decomposed Prompt Tuning (DePT) has demonstrated superior adaptation... | Pengwei Tang, Xiaolin Hu, Yong Liu |  |
| 2348 |  |  [Dobi-SVD: Differentiable SVD for LLM Compression and Some New Perspectives](https://openreview.net/forum?id=kws76i5XB8) |  | 0 | Large language models (LLMs) have sparked a new wave of AI applications; however, their substantial computational costs and memory demands pose significant challenges to democratizing access to LLMs for a broader audience. Singular Value Decomposition (SVD), a technique studied for decades, offers... | Chenfeng Xu, Jinghan Ke, Kurt Keutzer, Masayoshi Tomizuka, Qinsi Wang |  |
| 2349 |  |  [Transition Path Sampling with Improved Off-Policy Training of Diffusion Path Samplers](https://openreview.net/forum?id=WQV9kB1qSU) |  | 0 | Understanding transition pathways between two meta-stable states of a molecular system is crucial to advance drug discovery and material design. However, unbiased molecular dynamics (MD) simulations are computationally infeasible because of the high energy barriers that separate these states.... | Kiyoung Seong, Seonghwan Kim, Seonghyun Park, Sungsoo Ahn, Woo Youn Kim |  |
| 2350 |  |  [Taming Overconfidence in LLMs: Reward Calibration in RLHF](https://openreview.net/forum?id=l0tg0jzsdL) |  | 0 | Language model calibration refers to the alignment between the confidence of the model and the actual performance of its responses. While previous studies point out the overconfidence phenomenon in Large Language Models (LLMs) and show that LLMs trained with Reinforcement Learning from Human... | Banghua Zhu, Chengsong Huang, Jiaxin Huang, Jixuan Leng |  |
| 2351 |  |  [Designing Concise ConvNets with Columnar Stages](https://openreview.net/forum?id=zvaiz3FjA9) |  | 0 | In the era of vision Transformers, the recent success of VanillaNet shows the huge potential of simple and concise convolutional neural networks (ConvNets). Where such models mainly focus on runtime, it is also crucial to simultaneously focus on other aspects, e.g., FLOPs, parameters, etc, to... | Ashish Kumar, Jaesik Park |  |
| 2352 |  |  [Denoising Task Difficulty-based Curriculum for Training Diffusion Models](https://openreview.net/forum?id=96GMFXsbJE) |  | 0 | Diffusion-based generative models have emerged as powerful tools in the realm of generative modeling. Despite extensive research on denoising across various timesteps and noise levels, a conflict persists regarding the relative difficulties of the denoising tasks. While various studies argue that... | Hyojun Go, HyunGyoon Kim, JinYoung Kim, Soonwoo Kwon |  |
| 2353 |  |  [Circuit Transformer: A Transformer That Preserves Logical Equivalence](https://openreview.net/forum?id=kpnW12Lm9p) |  | 0 | Implementing Boolean functions with circuits consisting of logic gates is fundamental in digital computer design. However, the implemented circuit must be exactly equivalent, which hinders generative neural approaches on this task due to their occasionally wrong predictions. In this study, we... | Jun Wang, Lei Chen, Mingxuan Yuan, Xihan Li, Xing Li, Xing Zhang |  |
| 2354 |  |  [Learning Splitting Heuristics in Divide-and-Conquer SAT Solvers with Reinforcement Learning](https://openreview.net/forum?id=uUsL07BsMA) |  | 0 | We propose RDC-SAT, a novel approach to optimize splitting heuristics in Divide-and-Conquer SAT solvers using deep reinforcement learning. Our method dynamically extracts features from the current solving state whenever a split is required. These features, such as learned clauses, variable activity... | Ning Ge, Shumao Zhai |  |
| 2355 |  |  [Provably Robust Explainable Graph Neural Networks against Graph Perturbation Attacks](https://openreview.net/forum?id=iFK0xoceR0) |  | 0 | Explaining Graph Neural Network (XGNN) has gained growing attention to facilitate the trust of using GNNs, which is the mainstream method to learn graph data. Despite their growing attention, Existing XGNNs focus on improving the explanation performance, and its robustness under attacks is largely... | Binghui Wang, Jiate Li, Jinyuan Jia, Meng Pang, Yun Dong |  |
| 2356 |  |  [StringLLM: Understanding the String Processing Capability of Large Language Models](https://openreview.net/forum?id=kTXChtaaNO) |  | 0 | String processing, which mainly involves the analysis and manipulation of strings, is a fundamental component of modern computing. Despite the significant advancements of large language models (LLMs) in various natural language processing (NLP) tasks, their capability in string processing remains... | Hao Fu, Jindong Wang, Neil Zhenqiang Gong, Xilong Wang |  |
| 2357 |  |  [Counterfactual Generative Modeling with Variational Causal Inference](https://openreview.net/forum?id=oeDcgVC7Xh) |  | 0 | Estimating an individual's potential outcomes under counterfactual treatments is a challenging task for traditional causal inference and supervised learning approaches when the outcome is high-dimensional (e.g. gene expressions, facial images) and covariates are relatively limited. In this case, to... | Claudia Iriondo, Louie McConnell, Yulun Wu |  |
| 2358 |  |  [Efficient Evolutionary Search Over Chemical Space with Large Language Models](https://openreview.net/forum?id=awWiNvQwf3) |  | 0 | Molecular discovery, when formulated as an optimization problem, presents significant computational challenges because optimization objectives can be non-differentiable. Evolutionary Algorithms (EAs), often used to optimize black-box objectives in molecular discovery, traverse chemical space by... | Alán AspuruGuzik, Chao Zhang, Chenru Duan, Cher Tian Ser, Felix StriethKalthoff, Haorui Wang, Kirill Neklyudov, Lingkai Kong, Marta Skreta, Wenhao Gao, Yanqiao Zhu, Yuanqi Du, Yuchen Zhuang, Yue Yu |  |
| 2359 |  |  [Preserving Diversity in Supervised Fine-Tuning of Large Language Models](https://openreview.net/forum?id=NQEe7B7bSw) |  | 0 | Large Language Models (LLMs) typically rely on Supervised Fine-Tuning (SFT) to specialize in downstream tasks, with the Cross Entropy (CE) loss being the de facto choice. However, CE maximizes the likelihood of observed data without accounting for alternative possibilities. As such, CE usually... | Congliang Chen, Jiancong Xiao, Ruoyu Sun, Tian Xu, Zeyu Qin, ZhiQuan Luo, Ziniu Li |  |
| 2360 |  |  [GSBAK: top-K Geometric Score-based Black-box Attack](https://openreview.net/forum?id=htX7AoHyln) |  | 0 | Existing score-based adversarial attacks mainly focus on crafting $top$-1 adversarial examples against classifiers with single-label classification. Their attack success rate and query efficiency are often less than satisfactory, particularly under small perturbation requirements; moreover, the... | Huaiyu Dai, Md Farhamdur Reza, Richeng Jin, Tianfu Wu |  |
| 2361 |  |  [Adversarial Machine Unlearning](https://openreview.net/forum?id=swWF948IiC) |  | 0 | This paper focuses on the challenge of machine unlearning, aiming to remove the influence of specific training data on machine learning models. Traditionally, the development of unlearning algorithms runs parallel with that of membership inference attacks (MIA), a type of privacy threat to... | Sixie Yu, Yang Liu, Yevgeniy Vorobeychik, Zonglin Di |  |
| 2362 |  |  [Concept Bottleneck Language Models For Protein Design](https://openreview.net/forum?id=Yt9CFhOOFe) |  | 0 | We introduce Concept Bottleneck Protein Language Models (CB-pLM), a generative masked language model with a layer where each neuron corresponds to an interpretable concept. Our architecture offers three key benefits: i) Control: We can intervene on concept values to precisely control the properties... | Amy Wang, Aya Abdelsalam Ismail, Héctor Corrada Bravo, Julius Adebayo, Kyunghyun Cho, Nathan C. Frey, Samuel Don Stanton, Tuomas P. Oikarinen |  |
| 2363 |  |  [Difference-of-submodular Bregman Divergence](https://openreview.net/forum?id=vr1QdCNJmN) |  | 0 | The Bregman divergence, which is generated from a convex function, is commonly used as a pseudo-distance for comparing vectors or functions in continuous spaces. In contrast, defining an analog of the Bregman divergence for discrete spaces is nontrivial. Iyer & Bilmes (2012b) considered Bregman... | Hideitsu Hino, Masanari Kimura, Takahiro Kawashima, Tasuku Soma |  |
| 2364 |  |  [Are Large Vision Language Models Good Game Players?](https://openreview.net/forum?id=c4OGMNyzPT) |  | 0 | Large Vision Language Models (LVLMs) have demonstrated remarkable abilities in understanding and reasoning about both visual and textual information. However, existing evaluation methods for LVLMs, primarily based on benchmarks like Visual Question Answering and image captioning, often fail to... | Bohan Zhuang, Qi Wu, Xinyu Wang |  |
| 2365 |  |  [GoodDrag: Towards Good Practices for Drag Editing with Diffusion Models](https://openreview.net/forum?id=VtYfbvwpWp) |  | 0 | In this paper, we introduce GoodDrag, a novel approach to improve the stability and image quality of drag editing. Unlike existing methods that struggle with accumulated perturbations and often result in distortions, GoodDrag introduces an AlDD framework that alternates between drag and denoising... | Huan Liu, Jun Chen, Xiangyu Xu, Zewei Zhang |  |
| 2366 |  |  [DisEnvisioner: Disentangled and Enriched Visual Prompt for Customized Image Generation](https://openreview.net/forum?id=vQxqcVGrhR) |  | 0 | In the realm of image generation, creating customized images from visual prompt with additional textual instruction emerges as a promising endeavor. However, existing methods, both tuning-based and tuning-free, struggle with interpreting the subject-essential attributes from the visual prompt. This... | Guibao Shen, Haodong Li, Jing He, Weichao Qiu, YingCong Chen, Yingjie Cai, Yongzhe Hu |  |
| 2367 |  |  [A Benchmark for Semantic Sensitive Information in LLMs Outputs](https://openreview.net/forum?id=p3mxzKmuZy) |  | 0 | Large language models (LLMs) can output sensitive information, which has emerged as a novel safety concern. Previous works focus on structured sensitive information (e.g. personal identifiable information). However, we notice that sensitive information can also be at semantic level, i.e. semantic... | Chao Zhang, Di Wang, Haiqin Weng, Han Qiu, Liu Yan, Qingjie Zhang, Tianwei Zhang, Wenyu Zhu, Yiming Li |  |
| 2368 |  |  [FlexCAD: Unified and Versatile Controllable CAD Generation with Fine-tuned Large Language Models](https://openreview.net/forum?id=Z0eiiV3Yyh) |  | 0 | Recently, there is a growing interest in creating computer-aided design (CAD) models based on user intent, known as controllable CAD generation. Existing work offers limited controllability and needs separate models for different types of control, reducing efficiency and practicality. To achieve... | Deng Cai, Jiang Bian, Shizhao Sun, Wenxiao Wang, Zhanwei Zhang |  |
| 2369 |  |  [Efficient Neuron Segmentation in Electron Microscopy by Affinity-Guided Queries](https://openreview.net/forum?id=Y0QqruhqIa) |  | 0 | Accurate segmentation of neurons in electron microscopy (EM) images plays a crucial role in understanding the intricate wiring patterns of the brain. Existing automatic neuron segmentation methods rely on traditional clustering algorithms, where affinities are predicted first, and then watershed... | Chufeng Tang, Hang Chen, Xiao Li, Xiaolin Hu |  |
| 2370 |  |  [Continuous Autoregressive Modeling with Stochastic Monotonic Alignment for Speech Synthesis](https://openreview.net/forum?id=cuFzE8Jlvb) |  | 0 | We propose a novel autoregressive modeling approach for speech synthesis, combining a variational autoencoder (VAE) with a multi-modal latent space and an autoregressive model that uses Gaussian Mixture Models (GMM) as the conditional probability distribution. Unlike previous methods that rely on... | Chenhang He, Weiwei Lin |  |
| 2371 |  |  [PIG: Physics-Informed Gaussians as Adaptive Parametric Mesh Representations](https://openreview.net/forum?id=y5B0ca4mjt) |  | 0 | The numerical approximation of partial differential equations (PDEs) using neural networks has seen significant advancements through Physics-Informed Neural Networks (PINNs). Despite their straightforward optimization framework and flexibility in implementing various PDEs, PINNs often suffer from... | Eunbyung Park, Jaemin Oh, Namgyu Kang, Youngjoon Hong |  |
| 2372 |  |  [xFinder: Large Language Models as Automated Evaluators for Reliable Evaluation](https://openreview.net/forum?id=7UqQJUKaLM) |  | 0 | The continuous advancement of large language models (LLMs) has brought increasing attention to the critical issue of developing fair and reliable methods for evaluating their performance. Particularly, the emergence of cheating phenomena, such as test set leakage and prompt format overfitting,... | Bo Tang, Ding Chen, Feiyu Xiong, Qingchen Yu, Shichao Song, Zhiyu Li, Zifan Zheng |  |
| 2373 |  |  [PiCO: Peer Review in LLMs based on Consistency Optimization](https://openreview.net/forum?id=sfQ6XpApfS) |  | 0 | Existing large language models (LLMs) evaluation methods typically focus on testing the performance on some closed-environment and domain-specific benchmarks with human annotations. In this paper, we explore a novel unsupervised evaluation direction, utilizing peer-review mechanisms to measure LLMs... | JiaYu Yao, KunPeng Ning, Li Yuan, Shuo Yang, Yibing Song, Yonghong Tian, Yuyang Liu, ZhenHui Liu |  |
| 2374 |  |  [Boosting Perturbed Gradient Ascent for Last-Iterate Convergence in Games](https://openreview.net/forum?id=Jrt9iWalFy) |  | 0 | This paper presents a payoff perturbation technique, introducing a strong convexity to players' payoff functions in games. This technique is specifically designed for first-order methods to achieve last-iterate convergence in games where the gradient of the payoff functions is monotone in the... | Atsushi Iwasaki, Kaito Ariu, Kenshi Abe, Mitsuki Sakamoto |  |
| 2375 |  |  [Retrieval Augmented Diffusion Model for Structure-informed Antibody Design and Optimization](https://openreview.net/forum?id=a6U41REOa5) |  | 0 | Antibodies are essential proteins responsible for immune responses in organisms, capable of specifically recognizing antigen molecules of pathogens. Recent advances in generative models have significantly enhanced rational antibody design. However, existing methods mainly create antibodies from... | Jianing Tian, Shuangjia Zheng, Yaokun Ji, Zichen Wang |  |
| 2376 |  |  [Improving Neural Optimal Transport via Displacement Interpolation](https://openreview.net/forum?id=CfZPzH7ftt) |  | 0 | Optimal Transport (OT) theory investigates the cost-minimizing transport map that moves a source distribution to a target distribution. Recently, several approaches have emerged for learning the optimal transport map for a given cost function using neural networks. We refer to these approaches as... | Jaemoo Choi, Jaewoong Choi, Yongxin Chen |  |
| 2377 |  |  [Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models](https://openreview.net/forum?id=s7DkcgpRxL) |  | 0 | Large Language Models (LLMs) have significantly advanced natural language processing with exceptional task generalization capabilities. Low-Rank Adaption (LoRA) offers a cost-effective fine-tuning solution, freezing the original model parameters and training only lightweight, low-rank adapter... | Guiming Xie, Huan Li, Jue Wang, Jun Zhang, Ke Chen, Kunlong Zhou, Lidan Shou, Xuejian Gong, Yang You |  |
| 2378 |  |  [UniCoTT: A Unified Framework for Structural Chain-of-Thought Distillation](https://openreview.net/forum?id=3baOKeI2EU) |  | 0 | Chains of thought (CoTs) have achieved success in enhancing the reasoning capabilities of large language models (LLMs), while their effectiveness is predominantly observed in LLMs. Existing solutions methods adopt distillation to inject chain-of-thought capabilities into small models (SLMs).... | Xianwei Zhuang, Xuxin Cheng, Yuexian Zou, Zhichang Wang, Zhihong Zhu |  |
| 2379 |  |  [Neural Fluid Simulation on Geometric Surfaces](https://openreview.net/forum?id=58lbAsXCoZ) |  | 0 | Incompressible fluid on the surface is an interesting research area in the fluid simulation, which is the fundamental building block in visual effects, design of liquid crystal films, scientific analyses of atmospheric and oceanic phenomena, etc. The task brings two key challenges: the extension of... | Haoxiang Wang, Hui Qiao, Qionghai Dai, Tao Yu |  |
| 2380 |  |  [RandLoRA: Full rank parameter-efficient fine-tuning of large models](https://openreview.net/forum?id=Hn5eoTunHN) |  | 0 | Low-Rank Adaptation (LoRA) and its variants have shown impressive results in reducing the number of trainable parameters and memory requirements of large transformer networks while maintaining fine-tuning performance. The low-rank nature of the weight update inherently limits the representation... | Anton van den Hengel, Cristian Rodriguez Opazo, Ehsan Abbasnejad, Frederic Z. Zhang, Hemanth Saratchandran, Paul Albert |  |
| 2381 |  |  [VOILA: Evaluation of MLLMs For Perceptual Understanding and Analogical Reasoning](https://openreview.net/forum?id=q5MUMlHxpd) |  | 0 | Multimodal Large Language Models (MLLMs) have become a powerful tool for integrating visual and textual information. Despite their exceptional performance on visual understanding benchmarks, measuring their ability to reason abstractly across multiple images remains a significant challenge. To... | Chitta Baral, Maitreya Patel, Nilay Yilmaz, Suren Jayasuriya, Tejas Gokhale, Yezhou Yang, Yiran Lawrence Luo |  |
| 2382 |  |  [MamBEV: Enabling State Space Models to Learn Birds-Eye-View Representations](https://openreview.net/forum?id=MvEkN2ejZ1) |  | 0 | 3D visual perception tasks, such as 3D detection from multi-camera images, are essential components of autonomous driving and assistance systems. However, designing computationally efficient methods remains a significant challenge. In this paper, we propose a Mamba-based framework called MamBEV,... | Haoxin Wang, Hongyu Ke, Jack Morris, Kentaro Oguchi, Xiaofei Cao, Yi Ding, Yongkang Liu |  |
| 2383 |  |  [RainbowPO: A Unified Framework for Combining Improvements in Preference Optimization](https://openreview.net/forum?id=trKee5pIFv) |  | 0 | Recently, numerous preference optimization algorithms have been introduced as extensions to the Direct Preference Optimization (DPO) family. While these methods have successfully aligned models with human preferences, there is a lack of understanding regarding the contributions of their additional... | Anirban Das, David D. Yao, Genta Indra Winata, Hanyang Zhao, Sambit Sahu, ShiXiong Zhang, Wenpin Tang |  |
| 2384 |  |  [Integral Performance Approximation for Continuous-Time Reinforcement Learning Control](https://openreview.net/forum?id=z21DkDDdgq) |  | 0 | We introduce integral performance approximation (IPA), a new continuous-time reinforcement learning (CT-RL) control method. It leverages an affine nonlinear dynamic model, which partially captures the dynamics of the physical environment, alongside state-action trajectory data to enable optimal... | Brent A. Wallace, Jennie Si |  |
| 2385 |  |  [Any-step Dynamics Model Improves Future Predictions for Online and Offline Reinforcement Learning](https://openreview.net/forum?id=JZCxlrwjZ8) |  | 0 | Model-based methods in reinforcement learning offer a promising approach to enhance data efficiency by facilitating policy exploration within a dynamics model. However, accurately predicting sequential steps in the dynamics model remains a challenge due to the bootstrapping prediction, which... | Chengxing Jia, Haoxin Lin, Jiaji Zhang, Junyin Ye, Yang Yu, YiChen Li, Yihao Sun, YuYan Xu, Zhilong Zhang |  |
| 2386 |  |  [Learning Interleaved Image-Text Comprehension in Vision-Language Large Models](https://openreview.net/forum?id=jZsN9zo8Qi) |  | 0 | The swift progress of Multi-modal Large Models (MLLMs) has showcased their impressive ability to tackle tasks blending vision and language. Yet, most current models and benchmarks cater to scenarios with a narrow scope of visual and textual contexts. These models often fall short when faced with... | Chaoyou Fu, Chenyu Zhou, Mengdan Zhang, Peixian Chen, Rongrong Ji, Xiawu Zheng, Xing Sun, Yunhang Shen |  |
| 2387 |  |  [Rethinking the role of frames for SE(3)-invariant crystal structure modeling](https://openreview.net/forum?id=gzxDjnvBDa) |  | 0 | Crystal structure modeling with graph neural networks is essential for various applications in materials informatics, and capturing SE(3)-invariant geometric features is a fundamental requirement for these networks. A straightforward approach is to model with orientation-standardized structures... | Kanta Ono, Ryo Igarashi, Tatsunori Taniai, Yoshitaka Ushiku, Yusei Ito |  |
| 2388 |  |  [A transfer learning framework for weak to strong generalization](https://openreview.net/forum?id=PeLLMw3wLX) |  | 0 | Modern large language model (LLM) alignment techniques rely on human feedback, but it is unclear whether the techniques fundamentally limit the capabilities of aligned LLMs. In particular, it is unclear whether it is possible to align (stronger) LLMs with superhuman capabilities with (weaker) human... | Felipe Maia Polo, Mikhail Yurochkin, Moulinath Banerjee, Seamus Somerstep, Yaacov Ritov, Yuekai Sun |  |
| 2389 |  |  [UniGS: Unified Language-Image-3D Pretraining with Gaussian Splatting](https://openreview.net/forum?id=6U2KI1dpfl) |  | 0 | Recent advancements in multi-modal 3D pre-training methods have shown promising efficacy in learning joint representations of text, images, and point clouds. However, adopting point clouds as 3D representation fails to fully capture the intricacies of the 3D world and exhibits a noticeable gap... | Hang Xu, Haoyuan Li, Jifei Song, Michael Kampffmeyer, Tao Tang, Xiaodan Liang, Yanpeng Zhou, Yihan Zeng |  |
| 2390 |  |  [Multi-Task Dense Predictions via Unleashing the Power of Diffusion](https://openreview.net/forum?id=TzdTRC85SQ) |  | 0 | Diffusion models have exhibited extraordinary performance in dense prediction tasks. However, there are few works exploring the diffusion pipeline for multi-task dense predictions. In this paper, we unlock the potential of diffusion models in solving multi-task dense predictions and propose a novel... | Bo Li, Hao Zhang, Jinwei Chen, PengTao Jiang, Qibin Hou, Yuqi Yang |  |
| 2391 |  |  [Fewer May Be Better: Enhancing Offline Reinforcement Learning with Reduced Dataset](https://openreview.net/forum?id=zqtql1YmlS) |  | 0 | Research in offline reinforcement learning (RL) marks a paradigm shift in RL. However, a critical yet under-investigated aspect of offline RL is determining the subset of the offline dataset, which is used to improve algorithm performance while accelerating algorithm training. Moreover, the size of... | Bo Xu, Chenghao Li, Chengjie Wu, Chongjie Zhang, Dianyu Zhong, Hao Hu, Qianchuan Zhao, Quanwei Wang, Yiqin Yang, Yuhua Jiang, Ziyou Zhang |  |
| 2392 |  |  [From Commands to Prompts: LLM-based Semantic File System for AIOS](https://openreview.net/forum?id=2G021ZqUEZ) |  | 0 | Large language models (LLMs) have demonstrated significant potential in the development of intelligent LLM-based agents. However, when users use these agent applications to perform file operations, their interaction with the file system still remains the traditional paradigm: reliant on manual... | Chaoji Zuo, Dong Deng, Kai Mei, Mengnan Du, Mingyu Jin, Wenyue Hua, Wujiang Xu, Yongfeng Zhang, Yongye Su, Yujie Ren, Zeru Shi, Zirui Liu |  |
| 2393 |  |  [ComLoRA: A Competitive Learning Approach for Enhancing LoRA](https://openreview.net/forum?id=jFcNXJGPGh) |  | 0 | We propose a Competitive Low-Rank Adaptation (ComLoRA) framework to address the limitations of the LoRA method, which either lacks capacity with a single rank-$r$ LoRA or risks inefficiency and overfitting with a larger rank-$Kr$ LoRA, where $K$ is an integer larger than 1. The proposed ComLoRA... | Lilian Tang, Qiushi Huang, Tom Ko, Yu Zhang |  |
| 2394 |  |  [Adapting Multi-modal Large Language Model to Concept Drift From Pre-training Onwards](https://openreview.net/forum?id=b20VK2GnSs) |  | 0 | Multi-modal Large Language Models (MLLMs) frequently face challenges from concept drift when dealing with real-world streaming data, wherein distributions change unpredictably. This mainly includes gradual drift due to long-tailed data and sudden drift from Out-Of-Distribution (OOD) data, both of... | En Yu, Jie Lu, Xiaoyu Yang |  |
| 2395 |  |  [RECAST: Reparameterized, Compact weight Adaptation for Sequential Tasks](https://openreview.net/forum?id=J3H8Az3YlB) |  | 0 | Incremental learning aims to adapt to new sets of categories over time with minimal computational overhead. Prior work often addresses this task by training efficient task-specific adaptors that modify frozen layer weights or features to capture relevant information without affecting predictions on... | Bryan A. Plummer, Nazia Tasnim |  |
| 2396 |  |  [PivotMesh: Generic 3D Mesh Generation via Pivot Vertices Guidance](https://openreview.net/forum?id=WAC8LmlKYf) |  | 0 | Generating compact and sharply detailed 3D meshes poses a significant challenge for current 3D generative models. Different from extracting dense meshes from neural representation, some recent works try to model the native mesh distribution (i.e., a set of triangles), which generates more compact... | C. L. Philip Chen, Haohan Weng, Jun Zhu, Tong Zhang, Yikai Wang |  |
| 2397 |  |  [Faster Algorithms for Structured Linear and Kernel Support Vector Machines](https://openreview.net/forum?id=DDNFTaVQdU) |  | 0 | Quadratic programming is a ubiquitous prototype in convex programming. Many machine learning problems can be formulated as quadratic programming, including the famous Support Vector Machines (SVMs). Linear and kernel SVMs have been among the most popular models in machine learning over the past... | Lichen Zhang, Yuzhou Gu, Zhao Song |  |
| 2398 |  |  [Uni-Sign: Toward Unified Sign Language Understanding at Scale](https://openreview.net/forum?id=0Xt7uT04cQ) |  | 0 | Sign language pre-training has gained increasing attention for its ability to enhance performance across various sign language understanding (SLU) tasks. However, existing methods often suffer from a gap between pre-training and fine-tuning, leading to suboptimal results. To address this, we... | Hezhen Hu, Houqiang Li, Kepeng Wu, Weichao Zhao, Wengang Zhou, Zecheng Li |  |
| 2399 |  |  [Ada-K Routing: Boosting the Efficiency of MoE-based LLMs](https://openreview.net/forum?id=9CqkpQExe2) |  | 0 | In the era of Large Language Models (LLMs), Mixture-of-Experts (MoE) architectures offer a promising approach to managing computational costs while scaling up model parameters. Conventional MoE-based LLMs typically employ static Top-K routing, which activates a fixed and equal number of experts for... | Hua Huang, Jie Cheng, Jing Liu, Longteng Guo, Tongtian Yue, Xuange Gao |  |
| 2400 |  |  [SonicSim: A customizable simulation platform for speech processing in moving sound source scenarios](https://openreview.net/forum?id=Hx2ADQLi8M) |  | 0 | Systematic evaluation of speech separation and enhancement models under moving sound source conditions requires extensive and diverse data. However, real-world datasets often lack sufficient data for training and evaluation, and synthetic datasets, while larger, lack acoustic realism. Consequently,... | Chang Zeng, Guo Chen, Kai Li, Runxuan Yang, Wendi Sang, Xiaolin Hu |  |
| 2401 |  |  [TIGER: Time-frequency Interleaved Gain Extraction and Reconstruction for Efficient Speech Separation](https://openreview.net/forum?id=rzx3vcvlzj) |  | 0 | In recent years, much speech separation research has focused primarily on improving model performance. However, for low-latency speech processing systems, high efficiency is equally important. Therefore, we propose a speech separation model with significantly reduced parameters and computational... | Guo Chen, Kai Li, Mohan Xu, Xiaolin Hu |  |
| 2402 |  |  [Less is More: Masking Elements in Image Condition Features Avoids Content Leakages in Style Transfer Diffusion Models](https://openreview.net/forum?id=88JJjsLtqr) |  | 0 | Given a style-reference image as the additional image condition, text-to-image diffusion models have demonstrated impressive capabilities in generating images that possess the content of text prompts while adopting the visual style of the reference image. However, current state-of-the-art methods... | Chenghu Zhou, Lin Zhu, Nanyang Ye, Qinying Gu, Xinbing Wang |  |
| 2403 |  |  [Neural Exploratory Landscape Analysis for Meta-Black-Box-Optimization](https://openreview.net/forum?id=EEI5R89Cmv) |  | 0 | Recent research in Meta-Black-Box-Optimization (MetaBBO) have shown that meta-trained neural networks can effectively guide the design of black-box optimizers, significantly reducing the need for expert tuning and delivering robust performance across complex problem distributions. Despite their... | Hongshu Guo, Jiacheng Chen, YueJiao Gong, Zeyuan Ma |  |
| 2404 |  |  [ACTIVE: Offline Reinforcement Learning via Adaptive Imitation and In-sample V-Ensemble](https://openreview.net/forum?id=qiluFujVc8) |  | 0 | Offline reinforcement learning (RL) aims to learn from static datasets and thus faces the challenge of value estimation errors for out-of-distribution actions. The in-sample learning scheme addresses this issue by performing implicit TD backups that does not query the values of unseen actions.... | Faguo Wu, Ronglong Cai, Tianyuan Chen, Xiao Zhang |  |
| 2405 |  |  [DRL: Decomposed Representation Learning for Tabular Anomaly Detection](https://openreview.net/forum?id=CJnceDksRd) |  | 0 | Anomaly detection, indicating to identify the anomalies that significantly deviate from the majority normal instances of data, has been an important role in machine learning and related applications. Despite the significant success achieved in anomaly detection on image and text data, the accurate... | Dandan Guo, Hangting Ye, He Zhao, Mingyuan Zhou, Wei Fan, Yi Chang |  |
| 2406 |  |  [RA-TTA: Retrieval-Augmented Test-Time Adaptation for Vision-Language Models](https://openreview.net/forum?id=V3zobHnS61) |  | 0 | Vision-language models (VLMs) are known to be susceptible to distribution shifts between pre-training data and test data, and test-time adaptation (TTA) methods for VLMs have been proposed to mitigate the detrimental impact of the distribution shifts. However, the existing methods solely rely on... | Doyoung Kim, Hwanjun Song, JaeGil Lee, Jihwan Bang, Junhyeok Kang, Youngjun Lee |  |
| 2407 |  |  [Training-free Camera Control for Video Generation](https://openreview.net/forum?id=KI1zldOFz9) |  | 0 | We propose a training-free and robust solution to offer camera movement control for off-the-shelf video diffusion models. Unlike previous work, our method does not require any supervised finetuning on camera-annotated datasets or self-supervised training via data augmentation. Instead, it is... | Chen Hou, Zhibo Chen |  |
| 2408 |  |  [CFD: Learning Generalized Molecular Representation via Concept-Enhanced Feedback Disentanglement](https://openreview.net/forum?id=CsOIYMOZaV) |  | 0 | To accelerate biochemical research, e.g., drug and protein discovery, molecular representation learning (MRL) has attracted much attention. However, most existing methods follow the closed-set assumption that training and testing data share identical distribution, which limits their generalization... | Aming Wu, Cheng Deng |  |
| 2409 |  |  [Towards Unbiased Learning in Semi-Supervised Semantic Segmentation](https://openreview.net/forum?id=85G2t3yklD) |  | 0 | Semi-supervised semantic segmentation aims to learn from a limited amount of labeled data and a large volume of unlabeled data, which has witnessed impressive progress with the recent advancement of deep neural networks. However, existing methods tend to neglect the fact of class imbalance issues,... | Huayu Mai, Rui Sun, Tianzhu Zhang, Wangkai Li |  |
| 2410 |  |  [Causal Effect Estimation with Mixed Latent Confounders and Post-treatment Variables](https://openreview.net/forum?id=qe1CsfnN1W) |  | 0 | Causal inference from observational data has attracted considerable attention among researchers. One main obstacle is the handling of confounders. As direct measurement of confounders may not be feasible, recent methods seek to address the confounding bias via proxy variables, i.e., covariates... | Jing Ma, Jundong Li, Liang Wu, Liangjie Hong, Qi Guo, Yaochen Zhu |  |
| 2411 |  |  [NVS-Solver: Video Diffusion Model as Zero-Shot Novel View Synthesizer](https://openreview.net/forum?id=zDJf7fvdid) |  | 0 | By harnessing the potent generative capabilities of pre-trained large video diffusion models, we propose a new novel view synthesis paradigm that operates without the need for training. The proposed method adaptively modulates the diffusion sampling process with the given views to enable the... | Hui Liu, Junhui Hou, Meng You, Zhiyu Zhu |  |
| 2412 |  |  [q-exponential family for policy optimization](https://openreview.net/forum?id=OyyE1FDdrQ) |  | 0 | Policy optimization methods benefit from a simple and tractable policy parametrization, usually the Gaussian for continuous action spaces. In this paper, we consider a broader policy family that remains tractable: the $q$-exponential family. This family of policies is flexible, allowing the... | Han Wang, Haseeb Shah, Lingwei Zhu, Martha White, Yukie Nagai |  |
| 2413 |  |  [Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models](https://openreview.net/forum?id=1BdPHbuimc) |  | 0 | We present a Chain-of-Action (CoA) framework for multimodal and retrieval-augmented Question-Answering (QA). Compared to the literature, CoA overcomes two major challenges of current QA applications: (i) unfaithful hallucination that is inconsistent with real-time or domain facts and (ii) weak... | Han Liu, Haozheng Luo, Manling Li, Zhenyu Pan |  |
| 2414 |  |  [Efficient Alternating Minimization with Applications to Weighted Low Rank Approximation](https://openreview.net/forum?id=rvhu4V7yrX) |  | 0 | Weighted low rank approximation is a fundamental problem in numerical linear algebra, and it has many applications in machine learning. Given a matrix $M \in \mathbb{R}^{n \times n}$, a non-negative weight matrix $W \in \mathbb{R}_{\geq 0}^{n \times n}$, a parameter $k$, the goal is to output two... | Junze Yin, Lichen Zhang, Mingquan Ye, Zhao Song |  |
| 2415 |  |  [MAI: A Multi-turn Aggregation-Iteration Model for Composed Image Retrieval](https://openreview.net/forum?id=gXyWbl71n1) |  | 0 | Multi-Turn Composed Image Retrieval (MTCIR) addresses a real-world scenario where users iteratively refine retrieval results by providing additional information until a target meeting all their requirements is found. Existing methods primarily achieve MTCIR through a "multiple single-turn"... | Jinglin Xu, Yanzhe Chen, Yuxin Peng, Zhiwen Yang |  |
| 2416 |  |  [Fat-to-Thin Policy Optimization: Offline Reinforcement Learning with Sparse Policies](https://openreview.net/forum?id=SRjzerUpB2) |  | 0 | Sparse continuous policies are distributions that can choose some actions at random yet keep strictly zero probability for the other actions, which are radically different from the Gaussian. They have important real-world implications, e.g. in modeling safety-critical tasks like medicine. The... | Han Wang, Lingwei Zhu, Yukie Nagai |  |
| 2417 |  |  [Analytic DAG Constraints for Differentiable DAG Learning](https://openreview.net/forum?id=oCdIo9757e) |  | 0 | Recovering the underlying Directed Acyclic Graph (DAG) structures from observational data presents a formidable challenge, partly due to the combinatorial nature of the DAG-constrained optimization problem. Recently, researchers have identified gradient vanishing as one of the primary obstacles in... | Anton van den Hengel, Biwei Huang, Dong Gong, Ignavier Ng, Javen Qinfeng Shi, Kun Zhang, Mingming Gong, Yuhang Liu, Zhen Zhang |  |
| 2418 |  |  [Decoupled Graph Energy-based Model for Node Out-of-Distribution Detection on Heterophilic Graphs](https://openreview.net/forum?id=NuVBI4wPMm) |  | 0 | Despite extensive research efforts focused on Out-of-Distribution (OOD) detection on images, OOD detection on nodes in graph learning remains underexplored. The dependence among graph nodes hinders the trivial adaptation of existing approaches on images that assume inputs to be i.i.d. sampled,... | Jing Tang, Pengwen Dai, Xiaochun Cao, Yifan Song, Yihong Luo, Yuhan Chen |  |
| 2419 |  |  [Learning Causal Alignment for Reliable Disease Diagnosis](https://openreview.net/forum?id=ozZG5FXuTV) |  | 0 | Aligning the decision-making process of machine learning algorithms with that of experienced radiologists is crucial for reliable diagnosis. While existing methods have attempted to align their prediction behaviors to those of radiologists reflected in the training data, this alignment is primarily... | ChingWen Lee, Mingzhou Liu, Xinwei Sun, Xueqing Yu, Yizhou Wang, Yu Qiao |  |
| 2420 |  |  [CLDyB: Towards Dynamic Benchmarking for Continual Learning with Pre-trained Models](https://openreview.net/forum?id=RnxwxGXxex) |  | 0 | The emergence of the foundation model era has sparked immense research interest in utilizing pre-trained representations for continual learning~(CL), yielding a series of strong CL methods with outstanding performance on standard evaluation benchmarks. Nonetheless, there are growing concerns... | Kede Ma, Shengzhuang Chen, Xiaoxiao Sun, Yikai Liao, Ying Wei |  |
| 2421 |  |  [MGMapNet: Multi-Granularity Representation Learning for End-to-End Vectorized HD Map Construction](https://openreview.net/forum?id=E8S5Upr6oO) |  | 0 | The construction of vectorized high-definition map typically requires capturing both category and geometry information of map elements. Current state-of-the-art methods often adopt solely either point-level or instance-level representation, overlooking the strong intrinsic relationship between... | Errui Ding, Hanli Wang, Jing Yang, Jingdong Wang, Minyue Jiang, Sen Yang, Xiao Tan, Yingying Li |  |
| 2422 |  |  [Rethinking Spiking Neural Networks from an Ensemble Learning Perspective](https://openreview.net/forum?id=ZyknpOQwkT) |  | 0 | Spiking neural networks (SNNs) exhibit superior energy efficiency but suffer from limited performance. In this paper, we consider SNNs as ensembles of temporal subnetworks that share architectures and weights, and highlight a crucial issue that affects their performance: excessive differences in... | Hanpu Deng, Lin Zuo, Mengmeng Jing, Pei He, Yongqi Ding |  |
| 2423 |  |  [Near-optimal Active Regression of Single-Index Models](https://openreview.net/forum?id=iF06WjHnNj) |  | 0 | The active regression problem of the single-index model is to solve $\min_x \lVert f(Ax)-b\rVert_p$, where $A$ is fully accessible and $b$ can only be accessed via entry queries, with the goal of minimizing the number of queries to the entries of $b$. When $f$ is Lipschitz, previous results only... | Wai Ming Tai, Yi Li |  |
| 2424 |  |  [Accelerating Neural ODEs: A Variational Formulation-based Approach](https://openreview.net/forum?id=trV41CpAK4) |  | 0 | Neural Ordinary Differential Equations (Neural ODEs or NODEs) excel at modeling continuous dynamical systems from observational data, especially when the data is irregularly sampled. However, existing training methods predominantly rely on numerical ODE solvers, which are time-consuming and prone... | Hairong Qi, Han Zhao, Hongjue Zhao, Huajie Shao, Lui Sha, Yuchen Wang, Zijie Huang |  |
| 2425 |  |  [DiSK: Differentially Private Optimizer with Simplified Kalman Filter for Noise Reduction](https://openreview.net/forum?id=Lfy9q7Icp9) |  | 0 | Differential privacy (DP) offers a robust framework for safeguarding individual data privacy. To utilize DP in training modern machine learning models, differentially private optimizers have been widely used in recent years. A popular approach to privatize an optimizer is to clip the individual... | Borja Balle, Meisam Razaviyayn, Mingyi Hong, Vahab Mirrokni, Xinwei Zhang, Zhiqi Bu |  |
| 2426 |  |  [A Theory for Token-Level Harmonization in Retrieval-Augmented Generation](https://openreview.net/forum?id=tbx3u2oZAu) |  | 0 | Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance large language models (LLMs). Studies show that while RAG provides valuable external information (benefit), it may also mislead LLMs (detriment) with noisy or incorrect retrieved texts. Although many existing methods attempt... | Huawei Shen, Liang Pang, Shicheng Xu, Xueqi Cheng |  |
| 2427 |  |  [Deep Compression Autoencoder for Efficient High-Resolution Diffusion Models](https://openreview.net/forum?id=wH8XXUOUZU) |  | 0 | We present Deep Compression Autoencoder (DC-AE), a new family of autoencoders for accelerating high-resolution diffusion models. Existing autoencodes have demonstrated impressive results at a moderate spatial compression ratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy... | Enze Xie, Han Cai, Haotian Tang, Junsong Chen, Junyu Chen, Muyang Li, Shang Yang, Song Han |  |
| 2428 |  |  [PRISM: Privacy-Preserving Improved Stochastic Masking for Federated Generative Models](https://openreview.net/forum?id=B9kUJuWrYC) |  | 0 | Despite recent advancements in federated learning (FL), the integration of generative models into FL has been limited due to challenges such as high communication costs and unstable training in heterogeneous data environments. To address these issues, we propose PRISM, a FL framework tailored for... | DongJun Han, Jaejun Yoo, Kyeongkook Seo |  |
| 2429 |  |  [Re-Aligning Language to Visual Objects with an Agentic Workflow](https://openreview.net/forum?id=MPJ4SMnScw) |  | 0 | Language-based object detection (LOD) aims to align visual objects with language expressions. A large amount of paired data is utilized to improve LOD model generalizations. During the training process, recent studies leverage vision-language models (VLMs) to automatically generate human-like... | Feng Zhu, Haodong Zhang, Jiangyan Feng, Lijun Gong, MingMing Cheng, Qibin Hou, Rui Zhao, Yibing Song, Yuming Chen |  |
| 2430 |  |  [Step-by-Step Reasoning for Math Problems via Twisted Sequential Monte Carlo](https://openreview.net/forum?id=Ze4aPP0tIn) |  | 0 | Augmenting the multi-step reasoning abilities of Large Language Models (LLMs) has been a persistent challenge. Recently, verification has shown promise in improving solution consistency by evaluating generated outputs. However, current verification approaches suffer from sampling inefficiencies,... | Aonan Zhang, Chong Wang, Dong Yin, Ruoming Pang, Shengyu Feng, Shuang Ma, Xiang Kong, Yiming Yang |  |
| 2431 |  |  [EG4D: Explicit Generation of 4D Object without Score Distillation](https://openreview.net/forum?id=uq9TLFT7tF) |  | 0 | In recent years, the increasing demand for dynamic 3D assets in design and gaming applications has given rise to powerful generative pipelines capable of synthesizing high-quality 4D objects. Previous methods generally rely on score distillation sampling (SDS) algorithm to infer the unseen views... | Houqiang Li, Jing Liao, Jing Nathan Yan, Qi Sun, Shengming Yin, Wengang Zhou, Zhiyang Guo, Ziyu Wan |  |
| 2432 |  |  [Large Language Models are Interpretable Learners](https://openreview.net/forum?id=hTphfqtafO) |  | 0 | The trade-off between expressiveness and interpretability remains a core challenge when building human-centric models for classification and decision-making. While symbolic rules offer interpretability, they often lack expressiveness, whereas neural networks excel in performance but are known for... | ChoJui Hsieh, Dorothea Wiesmann Rothuizen, Felix X. Yu, Inderjit S. Dhillon, Ruochen Wang, Si Si |  |
| 2433 |  |  [On the expressiveness and spectral bias of KANs](https://openreview.net/forum?id=ydlDRUuGm9) |  | 0 | Kolmogorov-Arnold Networks (KAN) \cite{liu2024kan} were very recently proposed as a potential alternative to the prevalent architectural backbone of many deep learning models, the multi-layer perceptron (MLP). KANs have seen success in various tasks of AI for science, with their empirical... | Jonathan W. Siegel, Thomas Y. Hou, Yixuan Wang, Ziming Liu |  |
| 2434 |  |  [Does SGD really happen in tiny subspaces?](https://openreview.net/forum?id=v6iLQBoIJw) |  | 0 | Understanding the training dynamics of deep neural networks is challenging due to their high-dimensional nature and intricate loss landscapes. Recent studies have revealed that, along the training trajectory, the gradient approximately aligns with a low-rank top eigenspace of the training loss... | Chulhee Yun, Kwangjun Ahn, Minhak Song |  |
| 2435 |  |  [BaB-ND: Long-Horizon Motion Planning with Branch-and-Bound and Neural Dynamics](https://openreview.net/forum?id=JXKFPJe0NU) |  | 0 | Neural-network-based dynamics models learned from observational data have shown strong predictive capabilities for scene dynamics in robotic manipulation tasks. However, their inherent non-linearity presents significant challenges for effective planning. Current planning methods, often dependent on... | Huan Zhang, Jiangwei Yu, Jose A. Barreiros, Keyi Shen, Yunzhu Li |  |
| 2436 |  |  [D2O: Dynamic Discriminative Operations for Efficient Long-Context Inference of Large Language Models](https://openreview.net/forum?id=HzBfoUdjHt) |  | 0 | Efficient generative inference in Large Language Models (LLMs) is impeded by the growing memory demands of Key-Value (KV) cache, especially for longer sequences. Traditional KV Cache eviction strategies, which discard less critical KV-pairs based on attention scores, often degrade generation... | Chaofan Tao, Jing Xiong, Longyue Wang, Mi Zhang, Siqi Luo, Xin Wang, Xinjian Wu, Yi Xin, Yu Zhang, Zhihong Zhu, Zhongwei Wan |  |
| 2437 |  |  [Scaling Laws for Downstream Task Performance in Machine Translation](https://openreview.net/forum?id=vPOMTkmSiu) |  | 0 | Scaling laws provide important insights that can guide the design of large language models (LLMs). Existing work has primarily focused on studying scaling laws for pretraining (upstream) loss. However, in transfer learning settings, in which LLMs are pretrained on an unsupervised dataset and then... | Berivan Isik, Dimitris Paparas, Hussein Hazimeh, Natalia Ponomareva, Sanmi Koyejo, Sergei Vassilvitskii |  |
| 2438 |  |  [Routing Experts: Learning to Route Dynamic Experts in Existing Multi-modal Large Language Models](https://openreview.net/forum?id=vtT09dYPGI) |  | 0 | Recently, mixture of experts (MoE) has become a popular paradigm for achieving the trade-off between modal capacity and efficiency of multimodal large language models (MLLMs). Different from previous efforts, we are dedicated to exploring the dynamic experts in existing MLLMs and showing that a... | Qiong Wu, Rongrong Ji, Xiaoshuai Sun, Yiyi Zhou, Zhaoxi Ke |  |
| 2439 |  |  [Regret Bounds for Episodic Risk-Sensitive Linear Quadratic Regulator](https://openreview.net/forum?id=VD4PFpecG2) |  | 0 | Risk-sensitive linear quadratic regulator is one of the most fundamental problems in risk-sensitive optimal control. In this paper, we study online adaptive control of risk-sensitive linear quadratic regulator in the finite horizon episodic setting. We propose a simple least-squares greedy... | Wenhao Xu, Xuedong He, Xuefeng Gao |  |
| 2440 |  |  [Ambient Diffusion Posterior Sampling: Solving Inverse Problems with Diffusion Models Trained on Corrupted Data](https://openreview.net/forum?id=qeXcMutEZY) |  | 0 | We provide a framework for solving inverse problems with diffusion models learned from linearly corrupted data. Firstly, we extend the Ambient Diffusion framework to enable training directly from measurements corrupted in the Fourier domain. Subsequently, we train diffusion models for MRI with... | Alex Dimakis, Asad Aali, Brett Levac, Giannis Daras, Jonathan I. Tamir, Sidharth Kumar |  |
| 2441 |  |  [HERO: Human-Feedback Efficient Reinforcement Learning for Online Diffusion Model Finetuning](https://openreview.net/forum?id=yMHe9SRvxk) |  | 0 | Controllable generation through Stable Diffusion (SD) fine-tuning aims to improve fidelity, safety, and alignment with human guidance. Existing reinforcement learning from human feedback methods usually rely on predefined heuristic reward functions or pretrained reward models built on large-scale... | Ayano Hiranaka, ChiehHsin Lai, Dongjun Kim, Naoki Murata, ShangFu Chen, ShaoHua Sun, Takashi Shibuya, WeiHsiang Liao, Yuki Mitsufuji |  |
| 2442 |  |  [Rational Decision-Making Agent with Learning Internal Utility Judgment](https://openreview.net/forum?id=GEBkyKZOc4) |  | 0 | With remarkable advancements, large language models (LLMs) have attracted significant efforts to develop LLM-based agents capable of executing intricate multi-step decision-making tasks. Existing approaches predominantly build upon the external performance measure to guide the decision-making... | Chong Liu, Maosong Sun, Shizuo Tian, Xin Cong, Yankai Lin, Yining Ye, Yujia Qin, Zhiyuan Liu |  |
| 2443 |  |  [Pareto Prompt Optimization](https://openreview.net/forum?id=HGCk5aaSvE) |  | 0 | Natural language prompt optimization, or prompt engineering, has emerged as a powerful technique to unlock the potential of Large Language Models (LLMs) for various tasks. While existing methods primarily focus on maximizing a single task-specific performance metric for LLM outputs, real-world... | ByungJun Yoon, Gilchan Park, Guang Zhao, Shantenu Jha, Shinjae Yoo, Xiaoning Qian |  |
| 2444 |  |  [BigDocs: An Open Dataset for Training Multimodal Models on Document and Code Tasks](https://openreview.net/forum?id=b1ivBPLb1n) |  | 0 | Multimodal AI has the potential to significantly enhance document-understanding tasks, such as processing receipts, understanding workflows, extracting data from documents, and summarizing reports. Code generation tasks that require long-structured outputs can also be enhanced by multimodality.... | Aarash Feizi, Abhay Puri, Ahmed Masry, Akshay Kalkunte Suresh, Amirhossein Abaskohi, François Savard, Juan A. Rodríguez, Mahsa Massoud, Mats Leon Richter, PierreAndré Noël, Rabiul Awal, Sanket Biswas, Saverio Vadacchino, Shravan Nayak, Shubham Agarwal, Siba Smarak Panigrahi, Suyuchen Wang, Tianyu Zhang, Xiangru Jian, Zichao Li, et al. |  |
| 2445 |  |  [3D-Properties: Identifying Challenges in DPO and Charting a Path Forward](https://openreview.net/forum?id=9Hxdixed7p) |  | 0 | Aligning large language models (LLMs) with human preferences has gained significant attention, with Proximal Policy Optimization (PPO) as a standard yet computationally expensive method and Direct Preference Optimization (DPO) as a more efficient alternative. While DPO offers simplicity, it remains... | Dong Yan, Jialian Li, Jian Xie, Yibo Miao, Yipin Zhang, Yuzi Yan, Zhijie Deng |  |
| 2446 |  |  [MAP: Low-compute Model Merging with Amortized Pareto Fronts via Quadratic Approximation](https://openreview.net/forum?id=1v7SRWsYve) |  | 0 | Model merging has emerged as an effective approach to combining multiple single-task models into a multitask model. This process typically involves computing a weighted average of the model parameters without additional training. Existing model-merging methods focus on improving average task... | Huan He, Jiang Bian, Jie Fu, Lu Li, Suyuchen Wang, Tianyu Zhang, Yong Chen, Yonghui Wu, Yoshua Bengio, Zhiqi Bu |  |
| 2447 |  |  [Long-tailed Adversarial Training with Self-Distillation](https://openreview.net/forum?id=vM94dZiqx4) |  | 0 | Adversarial training significantly enhances adversarial robustness, yet superior performance is predominantly achieved on balanced datasets. Addressing adversarial robustness in the context of unbalanced or long-tailed distributions is considerably more challenging, mainly due to the scarcity of... | Changick Kim, Hongsin Lee, Seungju Cho |  |
| 2448 |  |  [JudgeBench: A Benchmark for Evaluating LLM-Based Judges](https://openreview.net/forum?id=G0dksFayVq) |  | 0 | LLM-based judges have emerged as a scalable alternative to human evaluation and are increasingly used to assess, compare, and improve models. However, the reliability of LLM-based judges themselves is rarely scrutinized. As LLMs become more advanced, their responses grow more sophisticated,... | Alejandro Cuadron, Chenguang Wang, Ion Stoica, Kyle Montgomery, Raluca A. Popa, Sijun Tan, Siyuan Zhuang, William Yuan Tang |  |
| 2449 |  |  [Indirect Gradient Matching for Adversarial Robust Distillation](https://openreview.net/forum?id=juKVq5dWTR) |  | 0 | Adversarial training significantly improves adversarial robustness, but superior performance is primarily attained with large models. This substantial performance gap for smaller models has spurred active research into adversarial distillation (AD) to mitigate the difference. Existing AD methods... | Changick Kim, Hongsin Lee, Seungju Cho |  |
| 2450 |  |  [Towards Generalization Bounds of GCNs for Adversarially Robust Node Classification](https://openreview.net/forum?id=cp3aW7C5tD) |  | 0 | Adversarially robust generalization of Graph Convolutional Networks (GCNs) has garnered significant attention in various security-sensitive application areas, driven by intrinsic adversarial vulnerability. Albeit remarkable empirical advancement, theoretical understanding of the generalization... | Han Li, Hong Chen, Tieliang Gong, Wen Wen |  |
| 2451 |  |  [REvolve: Reward Evolution with Large Language Models using Human Feedback](https://openreview.net/forum?id=cJPUpL8mOw) |  | 0 | Designing effective reward functions is crucial to training reinforcement learning (RL) algorithms. However, this design is non-trivial, even for domain experts, due to the subjective nature of certain tasks that are hard to quantify explicitly. In recent works, large language models (LLMs) have... | Alkis Sygkounas, Amy Loutfi, Andreas Persson, Pedro Zuidberg Dos Martires, Rishi Hazra |  |
| 2452 |  |  [Task Descriptors Help Transformers Learn Linear Models In-Context](https://openreview.net/forum?id=lZNb1CVm5O) |  | 0 | Large language models (LLMs) exhibit strong in-context learning (ICL) ability, which allows the model to make predictions on new examples based on the given prompt. Recently, a line of research (Von Oswald et al., 2023; Aky¨urek et al., 2023; Ahn et al., 2023; Mahankali et al., 2023; Zhang et al.,... | Rong Ge, Ruomin Huang |  |
| 2453 |  |  [MallowsPO: Fine-Tune Your LLM with Preference Dispersions](https://openreview.net/forum?id=d8cnezVcaW) |  | 0 | Direct Preference Optimization (DPO) has recently emerged as a popular approach to improve reinforcement learning from human feedback (RLHF), leading to better techniques to fine-tune large language models (LLM). A weakness of DPO, however, lies in its lack of capability to characterize the... | David D. Yao, Hanyang Zhao, Haoxian Chen, Henry Lam, Wenpin Tang |  |
| 2454 |  |  [Distribution-Specific Agnostic Conditional Classification With Halfspaces](https://openreview.net/forum?id=KZEqbwJfTl) |  | 0 | We study "selective" or "conditional" classification problems under an agnostic setting. Classification tasks commonly focus on modeling the relationship between features and categories that captures the vast majority of data. In contrast to common machine learning frameworks, conditional... | Brendan Juba, Jizhou Huang |  |
| 2455 |  |  [Temporal Difference Learning: Why It Can Be Fast and How It Will Be Faster](https://openreview.net/forum?id=j3bKnEidtT) |  | 0 | Temporal difference (TD) learning represents a fascinating paradox: It is the prime example of a divergent algorithm that has not vanished after its instability was proven. On the contrary, TD continues to thrive in reinforcement learning (RL), suggesting that it provides significant compensatory... | Luca Guastoni, Nils Thuerey, Patrick Schnell |  |
| 2456 |  |  [OCCAM: Towards Cost-Efficient and Accuracy-Aware Classification Inference](https://openreview.net/forum?id=CUABD2qIB4) |  | 0 | Classification tasks play a fundamental role in various applications, spanning domains such as healthcare, natural language processing and computer vision. With the growing popularity and capacity of machine learning models, people can easily access trained classifiers as a service online or... | Bicheng Xu, Dujian Ding, Laks V. S. Lakshmanan |  |
| 2457 |  |  [Machine Unlearning via Simulated Oracle Matching](https://openreview.net/forum?id=3vXpZpOn29) |  | 0 | Machine unlearning---efficiently removing the effect of a small "forget set" of training data on a pre-trained machine learning model---has recently attracted significant research interest. Despite this interest, however, recent work shows that existing machine unlearning techniques do not hold up... | Aleksander Madry, Andrew Ilyas, Kristian Georgiev, Roy Rinberg, Seth Neel, Shivam Garg, Sung Min Park |  |
| 2458 |  |  [Discovering Clone Negatives via Adaptive Contrastive Learning for Image-Text Matching](https://openreview.net/forum?id=My9MBsO41H) |  | 0 | In this paper, we identify a common yet challenging issue in image-text matching, i.e., clone negatives: negative image-text pairs that semantically resemble positive pairs, leading to ambiguous and sub-optimal matching outcomes. To tackle this issue, we propose Adaptive Contrastive Learning... | Hua Yang, Jihao Dong, Renjie Pan |  |
| 2459 |  |  [Conformal Structured Prediction](https://openreview.net/forum?id=2ATD8a8P3C) |  | 0 | Conformal prediction has recently emerged as a promising strategy for quantifying the uncertainty of a predictive model; these algorithms modify the model to output sets of labels that are guaranteed to contain the true label with high probability. However, existing conformal prediction algorithms... | Botong Zhang, Osbert Bastani, Shuo Li |  |
| 2460 |  |  [Global Well-posedness and Convergence Analysis of Score-based Generative Models via Sharp Lipschitz Estimates](https://openreview.net/forum?id=r3cWq6KKbt) |  | 0 | We establish global well-posedness and convergence of the score-based generative models (SGM) under minimal general assumptions of initial data for score estimation. For the smooth case, we start from a Lipschitz bound of the score function with optimal time length. The optimality is validated by... | Connor Mooney, Jack Xin, Yifeng Yu, Zhongjian Wang |  |
| 2461 |  |  [To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning](https://openreview.net/forum?id=w6nlcS8Kkn) |  | 0 | Chain-of-thought (CoT) via prompting is the de facto method for eliciting reasoning capabilities from large language models (LLMs). But for what kinds of tasks is this extra "thinking" really helpful? To analyze this, we conducted a quantitative meta-analysis covering over 100 papers using CoT and... | Dongwei Jiang, Fangcong Yin, Greg Durrett, Juan Diego Rodriguez, Kyle Mahowald, Manya Wadhwa, Prasann Singhal, Xi Ye, Xinyu Zhao, Zayne Rea Sprague |  |
| 2462 |  |  [Interference Among First-Price Pacing Equilibria: A Bias and Variance Analysis](https://openreview.net/forum?id=6bDJ3CIm5w) |  | 0 | A/B testing is widely used in the internet industry. For online marketplaces (such as advertising markets), standard approaches to A/B testing may lead to biased results when buyers have budget constraints, as budget consumption in one arm of the experiment impacts performance of the other arm.... | Christian Kroer, Congshan Zhang, Liang Shi, Luofeng Liao, Nicolás Stier Moses, Okke Schrijvers, Sergei Leonenkov |  |
| 2463 |  |  [Exploiting Structure in Offline Multi-Agent RL: The Benefits of Low Interaction Rank](https://openreview.net/forum?id=AOlm45AUVS) |  | 0 | We study the problem of learning an approximate equilibrium in the offline multi-agent reinforcement learning (MARL) setting. We introduce a structural assumption---the interaction rank---and establish that functions with low interaction rank are significantly more robust to distribution shift... | Daniel Jiang, Jason D. Lee, Scott Fujimoto, Wenhao Zhan, Yonathan Efroni, Zheqing Zhu |  |
| 2464 |  |  [Fully-inductive Node Classification on Arbitrary Graphs](https://openreview.net/forum?id=1Qpt43cqhg) |  | 0 | One fundamental challenge in graph machine learning is generalizing to new graphs. Many existing methods following the inductive setup can generalize to test graphs with new structures, but assuming the feature and label spaces remain the same as the training ones. This paper introduces a... | Hesham Mostafa, Jian Tang, Jianan Zhao, Michael M. Bronstein, Mikhail Galkin, Zhaocheng Zhu |  |
| 2465 |  |  [Post-hoc Reward Calibration: A Case Study on Length Bias](https://openreview.net/forum?id=Iu8RytBaji) |  | 0 | Reinforcement Learning from Human Feedback aligns the outputs of Large Language Models with human values and preferences. Central to this process is the reward model (RM), which translates human feedback into training signals for optimising LLM behaviour. However, RMs can develop biases by... | Edoardo M. Ponti, Ivan Titov, Zeyu Huang, Zihan Qiu, Zili Wang |  |
| 2466 |  |  [ObscuraCoder: Powering Efficient Code LM Pre-Training Via Obfuscation Grounding](https://openreview.net/forum?id=VYvxrD7aS0) |  | 0 | Language models (LMs) have become a staple of the code-writing toolbox. Their pre-training recipe has, however, remained stagnant over recent years, barring the occasional changes in data sourcing and filtering strategies. In particular, research exploring modifications to Code-LMs' pre-training... | Goran Glavas, Haoyi Yang, Indraneil Paul, Iryna Gurevych, Kristian Kersting |  |
| 2467 |  |  [Differentiable Causal Discovery for Latent Hierarchical Causal Models](https://openreview.net/forum?id=Bp0HBaMNRl) |  | 0 | Discovering causal structures with latent variables from observational data is a fundamental challenge in causal discovery. Existing methods often rely on constraint-based, iterative discrete searches, limiting their scalability for large numbers of variables. Moreover, these methods frequently... | Biwei Huang, Ignavier Ng, Kun Zhang, Parjanya Prajakta Prashant |  |
| 2468 |  |  [SyllableLM: Learning Coarse Semantic Units for Speech Language Models](https://openreview.net/forum?id=dGSOn7sdWg) |  | 0 | Language models require tokenized inputs. However, tokenization strategies for continuous data like audio and vision are often based on simple heuristics such as fixed sized convolutions or discrete clustering, which do not necessarily align with the semantic structure of the data. For speech in... | Alan Baade, David Harwath, Puyuan Peng |  |
| 2469 |  |  [Exact Community Recovery under Side Information: Optimality of Spectral Algorithms](https://openreview.net/forum?id=zhFyKgqxlz) |  | 0 | We study the problem of exact community recovery in general, two-community block models, in the presence of node-attributed \*side information\*. We allow for a very general side information channel for node attributes, and for pairwise (edge) observations, consider both Bernoulli and Gaussian... | Julia Gaudio, Nirmit Joshi |  |
| 2470 |  |  [Repulsive Latent Score Distillation for Solving Inverse Problems](https://openreview.net/forum?id=bwJxUB0y46) |  | 0 | Score Distillation Sampling (SDS) has been pivotal for leveraging pre-trained diffusion models in downstream tasks such as inverse problems, but it faces two major challenges: $(i)$ mode collapse and $(ii)$ latent space inversion, which become more pronounced in high-dimensional data. To address... | Morteza Mardani, Nicolas Zilberstein, Santiago Segarra |  |
| 2471 |  |  [Calibrating Expressions of Certainty](https://openreview.net/forum?id=dNunnVB4W6) |  | 0 | We present a novel approach to calibrating linguistic expressions of certainty, e.g., "Maybe" and "Likely". Unlike prior work that assigns a single score to each certainty phrase, we model uncertainty as distributions over the simplex to capture their semantics more accurately. To accommodate this... | Ameneh AsgariTarghi, Barbara D. Lam, Peiqi Wang, Polina Golland, Rameswar Panda, Tina Kapur, William M. Wells III, Yingcheng Liu |  |
| 2472 |  |  [Leveraging Driver Field-of-View for Multimodal Ego-Trajectory Prediction](https://openreview.net/forum?id=LLWj8on4Rv) |  | 0 | Understanding drivers’ decision-making is crucial for road safety. Although predicting the ego-vehicle’s path is valuable for driver-assistance systems, existing methods mainly focus on external factors like other vehicles’ motions, often neglecting the driver’s attention and intent. To address... | Christian Vater, Danda Pani Paudel, Luc Van Gool, M. Eren Akbiyik, Nedko Savov, Nikola Popovic, Otmar Hilliges, Xi Wang |  |
| 2473 |  |  [Learning General-purpose Biomedical Volume Representations using Randomized Synthesis](https://openreview.net/forum?id=xOmC5LiVuN) |  | 0 | Current volumetric biomedical foundation models struggle to generalize as public 3D datasets are small and do not cover the broad diversity of medical procedures, conditions, anatomical regions, and imaging protocols. We address this by creating a representation learning method that instead... | Adrian V. Dalca, Benjamin Billot, Clinton J. Wang, Ellen Grant, Hallee E. Wong, Mengwei Ren, Neel Dey, Polina Golland |  |
| 2474 |  |  [Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning](https://openreview.net/forum?id=rQ7fz9NO7f) |  | 0 | While large language models (LLMs) have integrated images, adapting them to graphs remains challenging, limiting their applications in materials and drug design. This difficulty stems from the need for coherent autoregressive generation across texts and graphs. To address this, we introduce... | Gang Liu, Jie Chen, Meng Jiang, Michael Sun, Wojciech Matusik |  |
| 2475 |  |  [Test-Time Ensemble via Linear Mode Connectivity: A Path to Better Adaptation](https://openreview.net/forum?id=4wk2eOKGvh) |  | 0 | Test-time adaptation updates pretrained models on the fly to handle distribution shifts in test data. While existing research has focused on stable optimization during adaptation, less attention has been given to enhancing model representations for adaptation capability. To address this gap, we... | Byungjai Kim, Chanho Ahn, Eunho Yang, Huijin Lee, Kikyung Kim, Saehyun Ahn, Seungju Han, Sungjoo Suh, Wissam J. Baddar |  |
| 2476 |  |  [Faster Inference of Flow-Based Generative Models via Improved Data-Noise Coupling](https://openreview.net/forum?id=rsGPrJDIhh) |  | 0 | Conditional Flow Matching (CFM), a simulation-free method for training continuous normalizing flows, provides an efficient alternative to diffusion models for key tasks like image and video generation. The performance of CFM in solving these tasks depends on the way data is coupled with noise. A... | Aram Davtyan, Leello Tadesse Dadi, Paolo Favaro, Volkan Cevher |  |
| 2477 |  |  [Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional, Black-box Systems](https://openreview.net/forum?id=PLskiLUBDW) |  | 0 | Efficient inference in high-dimensional models is a central challenge in machine learning. We introduce the Gaussian Ensemble Belief Propagation (GEnBP) algorithm, which combines the strengths of the Ensemble Kalman Filter (EnKF) and Gaussian Belief Propagation (GaBP) to address this challenge.... | Daniel Edward Pagendam, Daniel MacKinlay, Petra Kuhnert, Russell Tsuchida |  |
| 2478 |  |  [Learning Multi-Index Models with Neural Networks via Mean-Field Langevin Dynamics](https://openreview.net/forum?id=WHhZv8X5zF) |  | 0 | We study the problem of learning multi-index models in high-dimensions using a two-layer neural network trained with the mean-field Langevin algorithm. Under mild distributional assumptions on the data, we characterize the effective dimension $d_{\mathrm{eff}}$ that controls both sample and... | Alireza Mousavi Hosseini, Denny Wu, Murat A. Erdogdu |  |
| 2479 |  |  [Adversarial Latent Feature Augmentation for Fairness](https://openreview.net/forum?id=cNaHOdvh9J) |  | 0 | Achieving fairness in machine learning remains a critical challenge, especially due to the opaque effects of data augmentation on input spaces within nonlinear neural networks. Nevertheless, current approaches that emphasize augmenting latent features, rather than input spaces, offer limited... | Hoin Jung, Junyi Chai, Xiaoqian Wang |  |
| 2480 |  |  [Symbolic regression via MDLformer-guided search: from minimizing prediction error to minimizing description length](https://openreview.net/forum?id=ljAS7cPAU0) |  | 0 | Symbolic regression, a task discovering the formula best fitting the given data, is typically based on the heuristical search. These methods usually update candidate formulas to obtain new ones with lower prediction errors iteratively. However, since formulas with similar function shapes may have... | Depeng Jin, Jingtao Ding, Yong Li, Zihan Yu |  |
| 2481 |  |  [Robust Gymnasium: A Unified Modular Benchmark for Robust Reinforcement Learning](https://openreview.net/forum?id=2uQBSa2X4R) |  | 0 | Driven by inherent uncertainty and the sim-to-real gap, robust reinforcement learning (RL) seeks to improve resilience against the complexity and variability in agent-environment sequential interactions. Despite the existence of a large number of RL benchmarks, there is a lack of standardized... | Adam Wierman, Costas J. Spanos, Eric Mazumdar, Laixi Shi, Ming Jin, Muning Wen, Shangding Gu, Yuejie Chi |  |
| 2482 |  |  [Multimodal Unsupervised Domain Generalization by Retrieving Across the Modality Gap](https://openreview.net/forum?id=bqoHdVMIbt) |  | 0 | Domain generalization (DG) is an important problem that learns a model which generalizes to unseen test domains leveraging one or more source domains, under the assumption of shared label spaces. However, most DG methods assume access to abundant source data in the target label space, a requirement... | Brian Kulis, Christian So, Christopher Liao, Theodoros Tsiligkaridis |  |
| 2483 |  |  [Dynamic Neural Fortresses: An Adaptive Shield for Model Extraction Defense](https://openreview.net/forum?id=029hDSVoXK) |  | 0 | Model extraction aims to acquire a pre-trained black-box model concealed behind a black-box API. Existing defense strategies against model extraction primarily concentrate on preventing the unauthorized extraction of API functionality. However, two significant challenges still need to be solved:... | Chao Wu, Dacheng Tao, Li Shen, Siyu Luan, Zhenyi Wang, Zonghua Gu |  |
| 2484 |  |  [How many samples are needed to train a deep neural network?](https://openreview.net/forum?id=q6zrZbth1F) |  | 0 | Even though neural networks have become standard tools in many areas, many important statistical questions remain open. This paper studies the question of how much data are needed to train a ReLU feed-forward neural network. Our theoretical and empirical results suggest that the generalization... | Johannes Lederer, Mahsa Taheri, Pegah Golestaneh |  |
| 2485 |  |  [Guided Score identity Distillation for Data-Free One-Step Text-to-Image Generation](https://openreview.net/forum?id=HMVDiaWMwM) |  | 0 | Diffusion-based text-to-image generation models trained on extensive text-image pairs have demonstrated the ability to produce photorealistic images aligned with textual descriptions. However, a significant limitation of these models is their slow sample generation process, which requires iterative... | Hai Huang, Huangjie Zheng, Mingyuan Zhou, Zhendong Wang |  |
| 2486 |  |  [Partial Gromov-Wasserstein Metric](https://openreview.net/forum?id=sCew1tR6No) |  | 0 | The Gromov-Wasserstein (GW) distance has gained increasing interest in the machine learning community in recent years, as it allows for the comparison of measures in different metric spaces. To overcome the limitations imposed by the equal mass requirements of the classical GW problem, researchers... | Abihith Kothapalli, Hengrong Du, Rocio Diaz Martin, Soheil Kolouri, Xinran Liu, Yikun Bai |  |
| 2487 |  |  [Score Forgetting Distillation: A Swift, Data-Free Method for Machine Unlearning in Diffusion Models](https://openreview.net/forum?id=gjwhDHeAsz) |  | 0 | The machine learning community is increasingly recognizing the importance of fostering trust and safety in modern generative AI (GenAI) models. We posit machine unlearning (MU) as a crucial foundation for developing safe, secure, and trustworthy GenAI models. Traditional MU methods often rely on... | Mingyuan Zhou, Shujian Zhang, Tianqi Chen |  |
| 2488 |  |  [Deep MMD Gradient Flow without adversarial training](https://openreview.net/forum?id=Pf85K2wtz8) |  | 0 | We propose a gradient flow procedure for generative modeling by transporting particles from an initial source distribution to a target distribution, where the gradient field on the particles is given by a noise-adaptive Wasserstein Gradient of the Maximum Mean Discrepancy (MMD). The noise adaptive... | Alexandre Galashov, Arthur Gretton, Valentin De Bortoli |  |
| 2489 |  |  [Fragment and Geometry Aware Tokenization of Molecules for Structure-Based Drug Design Using Language Models](https://openreview.net/forum?id=mMhZS7qt0U) |  | 0 | Structure-based drug design (SBDD) is crucial for developing specific and effective therapeutics against protein targets but remains challenging due to complex protein-ligand interactions and vast chemical space. Although language models (LMs) have excelled in natural language processing, their... | Blake Olson, Cong Fu, Heng Ji, Shuiwang Ji, Xiner Li |  |
| 2490 |  |  [Catastrophic Failure of LLM Unlearning via Quantization](https://openreview.net/forum?id=lHSeDYamnz) |  | 0 | Large language models (LLMs) have shown remarkable proficiency in generating text, benefiting from extensive training on vast textual corpora. However, LLMs may also acquire unwanted behaviors from the diverse and sensitive nature of their training data, which can include copyrighted and private... | Fali Wang, Hui Liu, Qi He, Suhang Wang, Wenpeng Yin, Xianfeng Tang, Xiaomin Li, Zhiwei Zhang, Zongyu Wu |  |
| 2491 |  |  [Learning the Optimal Stopping for Early Classification within Finite Horizons via Sequential Probability Ratio Test](https://openreview.net/forum?id=SRghq20nGU) |  | 0 | Time-sensitive machine learning benefits from Sequential Probability Ratio Test (SPRT), which provides an optimal stopping time for early classification of time series. However, in \*finite horizon\* scenarios, where input lengths are finite, determining the optimal stopping rule becomes... | Akinori F. Ebihara, Hitoshi Imaoka, Kazuyuki Sakurai, Taiki Miyagawa |  |
| 2492 |  |  [CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale](https://openreview.net/forum?id=d5HUnyByAI) |  | 0 | Measuring biodiversity is crucial for understanding ecosystem health. While prior works have developed machine learning models for taxonomic classification of photographic images and DNA separately, in this work, we introduce a multi-modal approach combining both, using CLIP-style contrastive... | Angel X. Chang, Austin T. Wang, Graham W. Taylor, Joakim Bruslund Haurum, Scott C. Lowe, Xiaoliang Huo, ZeMing Gong |  |
| 2493 |  |  [How DNNs break the Curse of Dimensionality: Compositionality and Symmetry Learning](https://openreview.net/forum?id=UvpuGrd6ey) |  | 0 | We show that deep neural networks (DNNs) can efficiently learn any composition of functions with bounded $F_{1}$-norm, which allows DNNs to break the curse of dimensionality in ways that shallow networks cannot. More specifically, we derive a generalization bound that combines a covering number... | Arthur Jacot, Seok Hoan Choi, Yuxiao Wen |  |
| 2494 |  |  [Detecting Backdoor Samples in Contrastive Language Image Pretraining](https://openreview.net/forum?id=KmQEsIfhr9) |  | 0 | Contrastive language-image pretraining (CLIP) has been found to be vulnerable to poisoning backdoor attacks where the adversary can achieve an almost perfect attack success rate on CLIP models by poisoning only 0.01\% of the training dataset. This raises security concerns on the current practice of... | Hanxun Huang, James Bailey, Sarah Monazam Erfani, Xingjun Ma, Yige Li |  |
| 2495 |  |  [Contractive Dynamical Imitation Policies for Efficient Out-of-Sample Recovery](https://openreview.net/forum?id=lILEtkWOXD) |  | 0 | Imitation learning is a data-driven approach to learning policies from expert behavior, but it is prone to unreliable outcomes in out-of-sample (OOS) regions. While previous research relying on stable dynamical systems guarantees convergence to a desired state, it often overlooks transient... | Amin Abyaneh, Giancarlo FerrariTrecate, HsiuChin Lin, Mahrokh Ghoddousi Boroujeni |  |
| 2496 |  |  [Duoduo CLIP: Efficient 3D Understanding with Multi-View Images](https://openreview.net/forum?id=iGbuc9ekKK) |  | 0 | We introduce Duoduo CLIP, a model for 3D representation learning that learns shape encodings from multi-view images instead of point-clouds. The choice of multi-view images allows us to leverage 2D priors from off-the-shelf CLIP models to facilitate fine-tuning with 3D data. Our approach not only... | Angel X. Chang, HanHung Lee, Yiming Zhang |  |
| 2497 |  |  [OmniSep: Unified Omni-Modality Sound Separation with Query-Mixup](https://openreview.net/forum?id=DkzZ1ooc7q) |  | 0 | Query-based sound separation (QSS) effectively isolate sound signals that match the content of a given query, enhancing the understanding of audio data. However, most existing QSS methods rely on a single modality for separation, lacking the ability to fully leverage homologous but heterogeneous... | Jialong Zuo, Minghui Fang, Rongjie Huang, Shengpeng Ji, Siqi Zheng, Tao Jin, Xize Cheng, Zehan Wang, Zhou Zhao, Ziang Zhang |  |
| 2498 |  |  [Optimal Learning of Kernel Logistic Regression for Complex Classification Scenarios](https://openreview.net/forum?id=WlhVRh2rQ0) |  | 0 | Complex classification scenarios, including long-tailed learning, domain adaptation, and transfer learning, present substantial challenges for traditional algorithms. Conditional class probability (CCP) predictions have recently become critical components of many state-of-the-art algorithms... | Annika Betken, Hanyuan Hang, Hongwei Wen |  |
| 2499 |  |  [Aligning Generative Denoising with Discriminative Objectives Unleashes Diffusion for Visual Perception](https://openreview.net/forum?id=rMOhA1JNPo) |  | 0 | With success in image generation, generative diffusion models are increasingly adopted for discriminative scenarios because generating pixels is a unified and natural perception interface. Although directly re-purposing their generative denoising process has established promising progress in... | Xin Xu, YuXiong Wang, Ziqi Pang |  |
| 2500 |  |  [Varying Shades of Wrong: Aligning LLMs with Wrong Answers Only](https://openreview.net/forum?id=p74CpDzw1Y) |  | 0 | In the absence of abundant reliable annotations for challenging tasks and contexts, how can we expand the frontier of LLM capabilities with potentially wrong answers? We focus on two research questions: (1) Can LLMs generate reliable preferences among wrong options? And if so, (2) Would alignment... | Jihan Yao, Lucy Lu Wang, Shangbin Feng, Wenxuan Ding, Yulia Tsvetkov |  |
| 2501 |  |  [Boltzmann Semantic Score: A Semantic Metric for Evaluating Large Vision Models Using Large Language Models](https://openreview.net/forum?id=9yJKTosUex) |  | 0 | Do Large Vision Models (LVMs) extract medically and semantically relevant features similar to those identified by human experts? Currently, only biased, qualitative approaches with limited, small-scale expert evaluations are available to answer this question. In this study, we propose the Boltzmann... | Ali Bashashati, Ali Khajegili Mirabadi, Hossein Farahani, Katherine Rich |  |
| 2502 |  |  [Training Nonlinear Transformers for Chain-of-Thought Inference: A Theoretical Generalization Analysis](https://openreview.net/forum?id=n7n8McETXw) |  | 0 | Chain-of-Thought (CoT) is an efficient prompting method that enables the reasoning ability of large language models by augmenting the query using multiple examples with multiple intermediate steps. Despite the empirical success, the theoretical understanding of how to train a Transformer to achieve... | Hongkang Li, Meng Wang, PinYu Chen, Songtao Lu, Xiaodong Cui |  |
| 2503 |  |  [Competing Large Language Models in Multi-Agent Gaming Environments](https://openreview.net/forum?id=DI4gW8viB6) |  | 0 | Decision-making is a complex process requiring diverse abilities, making it an excellent framework for evaluating Large Language Models (LLMs). Researchers have examined LLMs' decision-making through the lens of Game Theory. However, existing evaluation mainly focus on two-player scenarios where an... | Eric John Li, Jentse Huang, Man Ho Lam, Michael R. Lyu, Tian Liang, Wenxiang Jiao, Wenxuan Wang, Xing Wang, Youliang Yuan, Zhaopeng Tu |  |
| 2504 |  |  [A Causal Lens for Learning Long-term Fair Policies](https://openreview.net/forum?id=rPkCVSsoM4) |  | 0 | Fairness-aware learning studies the development of algorithms that avoid discriminatory decision outcomes despite biased training data. While most studies have concentrated on immediate bias in static contexts, this paper highlights the importance of investigating long-term fairness in dynamic... | Jacob Lear, Lu Zhang |  |
| 2505 |  |  [Building Math Agents with Multi-Turn Iterative Preference Learning](https://openreview.net/forum?id=WjKea8bGFF) |  | 0 | Recent studies have shown that large language models' (LLMs) mathematical problem-solving capabilities can be enhanced by integrating external tools, such as code interpreters, and employing multi-turn Chain-of-Thought (CoT) reasoning. While current methods focus on synthetic data generation and... | Aviv Rosenberg, Bilal Piot, Chengshuai Shi, Chi Jin, Daniele Calandriello, Jiaming Shen, Misha Khalman, Mohammad Saleh, Rishabh Joshi, Tianqi Liu, Tong Zhang, Wei Xiong, Zhen Qin |  |
| 2506 |  |  [Looking into User's Long-term Interests through the Lens of Conservative Evidential Learning](https://openreview.net/forum?id=o99Yn1wN9J) |  | 0 | Reinforcement learning (RL) provides an effective means to capture users' evolving preferences, leading to improved recommendation performance over time. However, existing RL approaches primarily rely on standard exploration strategies, which are less effective for a large item space with sparse... | Dingrong Wang, Ervine Zheng, Krishna Prasad Neupane, Qi Yu |  |
| 2507 |  |  [Sharpness-Aware Minimization: General Analysis and Improved Rates](https://openreview.net/forum?id=8rvqpiTTFv) |  | 0 | Sharpness-Aware Minimization (SAM) has emerged as a powerful method for improving generalization in machine learning models by minimizing the sharpness of the loss landscape. However, despite its success, several important questions regarding the convergence properties of SAM in non-convex settings... | Dimitris Oikonomou, Nicolas Loizou |  |
| 2508 |  |  [Learning to Steer Markovian Agents under Model Uncertainty](https://openreview.net/forum?id=IzYczpPqKq) |  | 0 | Designing incentives for an adapting population is a ubiquitous problem in a wide array of economic applications and beyond. In this work, we study how to design additional rewards to steer multi-agent systems towards desired policies \emph{without} prior knowledge of the agents' underlying... | Heinrich H. Nax, Jiawei Huang, Niao He, Vinzenz Thoma, Zebang Shen |  |
| 2509 |  |  [Diff-PIC: Revolutionizing Particle-In-Cell Nuclear Fusion Simulation with Diffusion Models](https://openreview.net/forum?id=c9z65sDx6M) |  | 0 | The rapid development of AI highlights the pressing need for sustainable energy, a critical global challenge for decades. Nuclear fusion, generally seen as a promising solution, has been the focus of intensive research for nearly a century, with investments reaching hundreds of billions of dollars.... | Ang Li, Chuan Liu, Chuang Ren, Chunshu Wu, Dongfang Liu, James Chenhao Liang, Michael Huang, Mingkai Chen, Shihui Cao, Tong Geng, Ying Nian Wu |  |
| 2510 |  |  [NRGBoost: Energy-Based Generative Boosted Trees](https://openreview.net/forum?id=wQHyjIZ1SH) |  | 0 | Despite the rise to dominance of deep learning in unstructured data domains, tree-based methods such as Random Forests (RF) and Gradient Boosted Decision Trees (GBDT) are still the workhorses for handling discriminative tasks on tabular data. We explore generative extensions of these popular... | João Bravo |  |
| 2511 |  |  [Scalable Universal T-Cell Receptor Embeddings from Adaptive Immune Repertoires](https://openreview.net/forum?id=wyF5vNIsO7) |  | 0 | T cells are a key component of the adaptive immune system, targeting infections, cancers, and allergens with specificity encoded by their T cell receptors (TCRs), and retaining a memory of their targets. High-throughput TCR repertoire sequencing captures a cross-section of TCRs that encode the... | Elon Portugaly, H. Jabran Zahid, Ilker Demirel, Javier Zazo, Julia Greissl, Lorenzo Pisani, Paidamoyo Chapfuwa |  |
| 2512 |  |  [Stochastic Polyak Step-sizes and Momentum: Convergence Guarantees and Practical Performance](https://openreview.net/forum?id=nuX2yPejiL) |  | 0 | Stochastic gradient descent with momentum, also known as Stochastic Heavy Ball method (SHB), is one of the most popular algorithms for solving large-scale stochastic optimization problems in various machine learning tasks. In practical scenarios, tuning the step-size and momentum parameters of the... | Dimitris Oikonomou, Nicolas Loizou |  |
| 2513 |  |  [InstaTrain: Adaptive Training via Ultra-Fast Natural Annealing within Dynamical Systems](https://openreview.net/forum?id=QhhShUQIpJ) |  | 0 | Time-series modeling is broadly adopted to capture underlying patterns present in historical data, allowing prediction of future values. However, one crucial aspect of such modeling is often overlooked: in highly dynamic environments, data distributions can shift drastically within a second or... | Chuan Liu, Chunshu Wu, Pouya Haghi, Ruibing Song, Tong Geng |  |
| 2514 |  |  [Agree to Disagree: Demystifying Homogeneous Deep Ensembles through Distributional Equivalence](https://openreview.net/forum?id=XYRPm8rAGM) |  | 0 | Deep ensembles improve the performance of the models by taking the average predictions of a group of ensemble members. However, the origin of these capabilities remains a mystery and deep ensembles are used as a reliable “black box” to improve the performance. Existing studies typically attribute... | Xiaoqian Wang, Yipei Wang |  |
| 2515 |  |  [Lr0.Fm: low-Resolution Zero-Shot Classification Benchmark for Foundation Models](https://openreview.net/forum?id=AsFxRSLtqR) |  | 0 | Visual-language foundation Models (FMs) exhibit remarkable zero-shot generalization across diverse tasks, largely attributed to extensive pre-training on largescale datasets. However, their robustness on low-resolution/pixelated (LR) images, a common challenge in real-world scenarios, remains... | Priyank Pathak, Shruti Vyas, Shyam Marjit, Yogesh S. Rawat |  |
| 2516 |  |  [Reinforcement learning with combinatorial actions for coupled restless bandits](https://openreview.net/forum?id=DhH3LbA6F6) |  | 0 | Reinforcement learning (RL) has increasingly been applied to solve real-world planning problems, with progress in handling large state spaces and time horizons. However, a key bottleneck in many domains is that RL methods cannot accommodate large, combinatorially structured action spaces. In such... | Bryan Wilder, Elias Boutros Khalil, Lily Xu, Milind Tambe |  |
| 2517 |  |  [Scale-Free Graph-Language Models](https://openreview.net/forum?id=nFcgay1Yo9) |  | 0 | Graph-language models (GLMs) have demonstrated great potential in graph-based semi-supervised learning. A typical GLM consists of two key stages: graph generation and text embedding, which are usually implemented by inferring a latent graph and finetuning a language model (LM), respectively.... | Jianglin Lu, Yitian Zhang, Yixuan Liu, Yun Fu |  |
| 2518 |  |  [DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads](https://openreview.net/forum?id=cFu7ze7xUm) |  | 0 | Deploying long-context large language models (LLMs) is essential but poses significant computational and memory challenges. Caching all Key and Value (KV) states across all attention heads consumes substantial memory. Existing KV cache pruning methods either damage the long-context capabilities of... | Guangxuan Xiao, Haotian Tang, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Song Han, Yao Fu |  |
| 2519 |  |  [Generalization in VAE and Diffusion Models: A Unified Information-Theoretic Analysis](https://openreview.net/forum?id=NGB6YNnO5o) |  | 0 | Despite the empirical success of Diffusion Models (DMs) and Variational Autoencoders (VAEs), their generalization performance remains theoretically underexplored, especially lacking a full consideration of the shared encoder-generator structure. Leveraging recent information-theoretic tools, we... | Florian Shkurti, Jierui Zhu, Qi Chen |  |
| 2520 |  |  [OGBench: Benchmarking Offline Goal-Conditioned RL](https://openreview.net/forum?id=M992mjgKzI) |  | 0 | Offline goal-conditioned reinforcement learning (GCRL) is a major problem in reinforcement learning (RL) because it provides a simple, unsupervised, and domain-agnostic way to acquire diverse behaviors and representations from unlabeled data without rewards. Despite the importance of this setting,... | Benjamin Eysenbach, Kevin Frans, Seohong Park, Sergey Levine |  |
| 2521 |  |  [DOPL: Direct Online Preference Learning for Restless Bandits with Preference Feedback](https://openreview.net/forum?id=2iYVBqRHK4) |  | 0 | Restless multi-armed bandits (RMAB) has been widely used to model constrained sequential decision making problems, where the state of each restless arm evolves according to a Markov chain and each state transition generates a scalar reward. However, the success of RMAB crucially relies on the... | Debajoy Mukherjee, Guojun Xiong, Jian Li, Srinivas Shakkottai, Ujwal Dinesha |  |
| 2522 |  |  [For Better or For Worse? Learning Minimum Variance Features With Label Augmentation](https://openreview.net/forum?id=LCL8SMGxDY) |  | 0 | Data augmentation has been pivotal in successfully training deep learning models on classification tasks over the past decade. An important subclass of data augmentation techniques - which includes both label smoothing and Mixup - involves modifying not only the input data but also the input label... | Muthu Chidambaram, Rong Ge |  |
| 2523 |  |  [Permute-and-Flip: An optimally stable and watermarkable decoder for LLMs](https://openreview.net/forum?id=YyVVicZ32M) |  | 0 | In this paper, we propose a new decoding method called Permute-and-Flip (PF) decoder. It enjoys stability properties similar to the standard sampling decoder, but is provably up to 2x better in its quality-stability tradeoff than sampling and never worse than any other decoder. We also design a... | Lei Li, Xuandong Zhao, YuXiang Wang |  |
| 2524 |  |  [Distributional Associations vs In-Context Reasoning: A Study of Feed-forward and Attention Layers](https://openreview.net/forum?id=WCVMqRHWW5) |  | 0 | Large language models have been successful at tasks involving basic forms of in-context reasoning, such as generating coherent language, as well as storing vast amounts of knowledge. At the core of the Transformer architecture behind such models are feed-forward and attention layers, which are... | Alberto Bietti, Joan Bruna, Lei Chen |  |
| 2525 |  |  [Last Iterate Convergence of Incremental Methods as a Model of Forgetting](https://openreview.net/forum?id=mSGcDhQPwm) |  | 0 | Incremental gradient and incremental proximal methods are a fundamental class of optimization algorithms used for solving finite sum problems, broadly studied in the literature. Yet, without strong convexity, their convergence guarantees have primarily been established for the ergodic (average)... | Jelena Diakonikolas, Xufeng Cai |  |
| 2526 |  |  [PEAR: Primitive Enabled Adaptive Relabeling for Boosting Hierarchical Reinforcement Learning](https://openreview.net/forum?id=0nJEgNpb4l) |  | 0 | Hierarchical reinforcement learning (HRL) has the potential to solve complex long horizon tasks using temporal abstraction and increased exploration. However, hierarchical agents are difficult to train due to inherent non-stationarity. We present primitive enabled adaptive relabeling (PEAR), a... | Utsav Singh, Vinay P. Namboodiri |  |
| 2527 |  |  [Image and Video Tokenization with Binary Spherical Quantization](https://openreview.net/forum?id=yGnsH3gQ6U) |  | 0 | We propose a new transformer-based image and video tokenizer with Binary Spherical Quantization (BSQ). BSQ projects the high-dimensional visual embedding to a lower-dimensional hypersphere and then applies binary quantization. BSQ is (1) parameter-efficient without an explicit codebook, (2)... | Philipp Krähenbühl, Yuanjun Xiong, Yue Zhao |  |
| 2528 |  |  [DeciMamba: Exploring the Length Extrapolation Potential of Mamba](https://openreview.net/forum?id=iWSl5Zyjjw) |  | 0 | Long-range sequence processing poses a significant challenge for Transformers due to their quadratic complexity in input length. A promising alternative is Mamba, which demonstrates high performance and achieves Transformer-level capabilities while requiring substantially fewer computational... | Amir Globerson, Assaf BenKish, Itamar Zimerman, Lior Wolf, Nadav Cohen, Raja Giryes, Shady AbuHussein |  |
| 2529 |  |  [Quality over Quantity in Attention Layers: When Adding More Heads Hurts](https://openreview.net/forum?id=y9Xp9NozPR) |  | 0 | Attention-based mechanisms are widely used in machine learning, most prominently in transformers. However, hyperparameters such as the number of attention heads and the attention rank (i.e., the query/key dimension) are set nearly the same way in all realizations of this architecture, without... | Gilad Yehudai, Joan Bruna, Noah Amsel |  |
| 2530 |  |  [Energy-Weighted Flow Matching for Offline Reinforcement Learning](https://openreview.net/forum?id=HA0oLUvuGI) |  | 0 | This paper investigates energy guidance in generative modeling, where the target distribution is defined as $q(\mathbf x) \propto p(\mathbf x)\exp(-\beta \mathcal E(\mathbf x))$, with $p(\mathbf x)$ being the data distribution and $\mathcal E(\mathbf x)$ as the energy function. To comply with... | Quanquan Gu, Shiyuan Zhang, Weitong Zhang |  |
| 2531 |  |  [X-Sample Contrastive Loss: Improving Contrastive Learning with Sample Similarity Graphs](https://openreview.net/forum?id=c1Ng0f8ivn) |  | 0 | Learning good representations involves capturing the diverse ways in which data samples relate. Contrastive loss—an objective matching related samples—underlies methods from self-supervised to multimodal learning. Contrastive losses, however, can be viewed more broadly as modifying a similarity... | Diane Bouchacourt, Kyunghyun Cho, Mark Ibrahim, Pietro Astolfi, Randall Balestriero, Vivien Cabannes, Vlad Sobal, Yann LeCun |  |
| 2532 |  |  [Metamizer: A Versatile Neural Optimizer for Fast and Accurate Physics Simulations](https://openreview.net/forum?id=60TXv9Xif5) |  | 0 | Efficient physics simulations are essential for numerous applications, ranging from realistic cloth animations in video games, to analyzing pollutant dispersion in environmental sciences, to calculating vehicle drag coefficients in engineering applications. Unfortunately, analytical solutions to... | Nils Wandel, Reinhard Klein, Stefan Schulz |  |
| 2533 |  |  [Satisficing Regret Minimization in Bandits](https://openreview.net/forum?id=5WPQIVgWCg) |  | 0 | Motivated by the concept of satisficing in decision-making, we consider the problem of satisficing exploration in bandit optimization. In this setting, the learner aims at finding a satisficing arm whose mean reward exceeds a certain threshold. The performance is measured by satisficing regret,... | Qing Feng, Ruihao Zhu, Tianyi Ma |  |
| 2534 |  |  [VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control](https://openreview.net/forum?id=0n4bS0R5MM) |  | 0 | Modern text-to-video synthesis models demonstrate coherent, photorealistic generation of complex videos from a text description. However, most existing models lack fine-grained control over camera movement, which is critical for downstream applications related to content creation, visual effects,... | Aliaksandr Siarohin, Andrea Tagliasacchi, Chaoyang Wang, David B. Lindell, Guocheng Qian, HsinYing Lee, Ivan Skorokhodov, Jiaxu Zou, Michael Vasilkovsky, Sergey Tulyakov, Sherwin Bahmani, Willi Menapace |  |
| 2535 |  |  [Structure Language Models for Protein Conformation Generation](https://openreview.net/forum?id=OzUNDnpQyd) |  | 0 | Proteins adopt multiple structural conformations to perform their diverse biological functions, and understanding these conformations is crucial for advancing drug discovery. Traditional physics-based simulation methods often struggle with sampling equilibrium conformations and are computationally... | Chence Shi, Hongyu Guo, Jian Tang, Jiarui Lu, Stephen Zhewen Lu, Xiaoyin Chen, Yoshua Bengio |  |
| 2536 |  |  [Three Mechanisms of Feature Learning in a Linear Network](https://openreview.net/forum?id=Wh4SE2S7Mo) |  | 0 | Understanding the dynamics of neural networks in different width regimes is crucial for improving their training and performance. We present an exact solution for the learning dynamics of a one-hidden-layer linear network, with one-dimensional data, across any finite width, uniquely exhibiting both... | Yizhou Xu, Ziyin Liu |  |
| 2537 |  |  [Matryoshka Multimodal Models](https://openreview.net/forum?id=Uhj5OxAz7I) |  | 0 | Large Multimodal Models (LMMs) such as LLaVA have shown strong performance in visual-linguistic reasoning. These models first embed images into a fixed large number of visual tokens and then feed them into a Large Language Model (LLM). However, this design causes an excessive number of tokens for... | Jianfeng Gao, Jianwei Yang, Mu Cai, Yong Jae Lee |  |
| 2538 |  |  [CREMA: Generalizable and Efficient Video-Language Reasoning via Multimodal Modular Fusion](https://openreview.net/forum?id=3UaOlzDEt2) |  | 0 | Despite impressive advancements in recent multimodal reasoning approaches, they are still limited in flexibility and efficiency, as these models typically process only a few fixed modality inputs and require updates to numerous parameters. This paper tackles these critical challenges and proposes... | Jaehong Yoon, Mohit Bansal, Shoubin Yu |  |
| 2539 |  |  [GI-GS: Global Illumination Decomposition on Gaussian Splatting for Inverse Rendering](https://openreview.net/forum?id=hJIEtJlvhL) |  | 0 | We present GI-GS, a novel inverse rendering framework that leverages 3D Gaussian Splatting (3DGS) and deferred shading to achieve photo-realistic novel view synthesis and relighting. In inverse rendering, accurately modeling the shading processes of objects is essential for achieving high-fidelity... | Hongze Chen, Jun Zhang, Zehong Lin |  |
| 2540 |  |  [TEOChat: A Large Vision-Language Assistant for Temporal Earth Observation Data](https://openreview.net/forum?id=pZz0nOroGv) |  | 0 | Large vision and language assistants have enabled new capabilities for interpreting natural images. These approaches have recently been adapted to earth observation data, but they are only able to handle single image inputs, limiting their use for many real-world tasks. In this work, we develop a... | Emily Ruoyu Liu, Ines Dormoy, Jeremy Andrew Irvin, Jinyoung Kim, Joyce Chuyi Chen, Samar Khanna, Stefano Ermon, Zhuo Zheng |  |
| 2541 |  |  [Concept-ROT: Poisoning Concepts in Large Language Models with Model Editing](https://openreview.net/forum?id=RzUvkI3p1D) |  | 0 | Model editing methods modify specific behaviors of Large Language Models by altering a small, targeted set of network weights and require very little data and compute. These methods can be used for malicious applications such as inserting misinformation or simple trojans that result in... | David Shriver, Keltin Grimes, Marco Christiani, Marissa Catherine Connor |  |
| 2542 |  |  [SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And Video Generation](https://openreview.net/forum?id=hgTFotBRKl) |  | 0 | Recent advances in diffusion models have significantly enhanced their ability to generate high-quality images and videos, but they have also increased the risk of producing unsafe content. Existing unlearning/editing-based methods for safe generation remove harmful concepts from models but face... | Huaxiu Yao, Jaehong Yoon, Mohit Bansal, Shoubin Yu, Vaidehi Patil |  |
| 2543 |  |  [Aligned LLMs Are Not Aligned Browser Agents](https://openreview.net/forum?id=NsFZZU9gvk) |  | 0 | For safety reasons, large language models (LLMs) are trained to refuse harmful user instructions, such as assisting dangerous activities. We study an open question in this work: does the desired safety refusal, typically enforced in chat contexts, generalize to non-chat and agentic use cases?... | Elaine Lau, Elaine T. Chang, Matt Fredrikson, Priyanshu Kumar, Saranya Vijayakumar, Sean M. Hendryx, Shuyan Zhou, Summer Yue, Tu Trinh, Vaughn Robinson, Zifan Wang |  |
| 2544 |  |  [Revisiting Prefix-tuning: Statistical Benefits of Reparameterization among Prompts](https://openreview.net/forum?id=QjTSaFXg25) |  | 0 | Prompt-based techniques, such as prompt-tuning and prefix-tuning, have gained prominence for their efficiency in fine-tuning large pre-trained models. Despite their widespread adoption, the theoretical foundations of these methods remain limited. For instance, in prefix-tuning, we observe that a... | Chau Nguyen, Huy Nguyen, Minh Le, Nhat Ho, Quyen Tran, Trung Le |  |
| 2545 |  |  [Adapt-∞: Scalable Continual Multimodal Instruction Tuning via Dynamic Data Selection](https://openreview.net/forum?id=EwFJaXVePU) |  | 0 | Visual instruction datasets from various distributors are released at different times and often contain a significant number of semantically redundant text-image pairs, depending on their task compositions (i.e., skills) or reference sources. This redundancy greatly limits the efficient deployment... | Adyasha Maharana, Jaehong Yoon, Mohit Bansal, Tianlong Chen |  |
| 2546 |  |  [Matcha: Mitigating Graph Structure Shifts with Test-Time Adaptation](https://openreview.net/forum?id=EpgoFFUM2q) |  | 0 | Powerful as they are, graph neural networks (GNNs) are known to be vulnerable to distribution shifts. Recently, test-time adaptation (TTA) has attracted attention due to its ability to adapt a pre-trained model to a target domain, without re-accessing the source domain. However, existing TTA... | Hanghang Tong, Jingrui He, Wenxuan Bao, Zhichen Zeng, Zhining Liu |  |
| 2547 |  |  [Adaptive Shrinkage Estimation for Personalized Deep Kernel Regression in Modeling Brain Trajectories](https://openreview.net/forum?id=peX9zpWgg4) |  | 0 | Longitudinal biomedical studies monitor individuals over time to capture dynamics in brain development, disease progression, and treatment effects. However, estimating trajectories of brain biomarkers is challenging due to biological variability, inconsistencies in measurement protocols (e.g.,... | Christos Davatzikos, Haochang Shou, Vasiliki Tassopoulou |  |
| 2548 |  |  [DS-LLM: Leveraging Dynamical Systems to Enhance Both Training and Inference of Large Language Models](https://openreview.net/forum?id=OPSpdc25IZ) |  | 0 | The training of large language models (LLMs) faces significant computational cost challenges, limiting their scalability toward artificial general intelligence (AGI) and broader adoption. With model sizes doubling approximately every 3.4 months and training costs escalating from 64 million USD for... | Ang Li, Chuan Liu, Chunshu Wu, Dongfang Liu, Ruibing Song, Tong Geng, Ying Nian Wu |  |
| 2549 |  |  [AdaFisher: Adaptive Second Order Optimization via Fisher Information](https://openreview.net/forum?id=puTxuiK2qO) |  | 0 | First-order optimization methods are currently the mainstream in training deep neural networks (DNNs). Optimizers like Adam incorporate limited curvature information by employing the diagonal matrix preconditioning of the stochastic gradient during the training. Despite their widespread,... | Damien Martins Gomes, Eugene Belilovsky, Guy Wolf, Mahdi S. Hosseini, Yanlei Zhang |  |
| 2550 |  |  [DPaI: Differentiable Pruning at Initialization with Node-Path Balance Principle](https://openreview.net/forum?id=hvLBTpiDt3) |  | 0 | Pruning at Initialization (PaI) is a technique in neural network optimization characterized by the proactive elimination of weights before the network's training on designated tasks. This innovative strategy potentially reduces the costs for training and inference, significantly advancing... | Hoang Pham, Hongkai Wen, Khoat Than, LanCuong Nguyen, Lichuan Xiang, Long TranThanh, Quan NguyenTri |  |
| 2551 |  |  [Learning Molecular Representation in a Cell](https://openreview.net/forum?id=BbZy8nI1si) |  | 0 | Predicting drug efficacy and safety in vivo requires information on biological responses (e.g., cell morphology and gene expression) to small molecule perturbations. However, current molecular representation learning methods do not provide a comprehensive view of cell states under these... | Anne E. Carpenter, Gang Liu, John Arevalo, Meng Jiang, Shantanu Singh, Srijit Seal, Zhenwen Liang |  |
| 2552 |  |  [Gradient descent with generalized Newton's method](https://openreview.net/forum?id=bI3fcTsKW4) |  | 0 | We propose the generalized Newton's method (GeN) --- a Hessian-informed approach that applies to any optimizer such as SGD and Adam, and covers the Newton-Raphson method as a sub-case. Our method automatically and dynamically selects the learning rate that accelerates the convergence, without the... | Shiyun Xu, Zhiqi Bu |  |
| 2553 |  |  [Certified Robustness Under Bounded Levenshtein Distance](https://openreview.net/forum?id=cd79pbXi4N) |  | 0 | Text classifiers suffer from small perturbations, that if chosen adversarially, can dramatically change the output of the model. Verification methods can provide robustness certificates against such adversarial perturbations, by computing a sound lower bound on the robust accuracy. Nevertheless,... | Elías AbadRocamora, Grigorios Chrysos, Volkan Cevher |  |
| 2554 |  |  [Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice](https://openreview.net/forum?id=Tn8EQIFIMQ) |  | 0 | The observed similarities in the behavior of humans and Large Language Models (LLMs) have prompted researchers to consider the potential of using LLMs as models of human cognition. However, several significant challenges must be addressed before LLMs can be legitimately regarded as cognitive... | Haijiang Yan, JianQiao Zhu, Thomas L. Griffiths |  |
| 2555 |  |  [Memory Mosaics](https://openreview.net/forum?id=IiagjrJNwF) |  | 0 | Memory Mosaics are networks of associative memories working in concert to achieve a prediction task of interest. Like transformers, memory mosaics possess compositional capabilities and in-context learning capabilities. Unlike transformers, memory mosaics achieve these capabilities in comparatively... | Beidi Chen, Jianyu Zhang, Léon Bottou, Niklas Nolte, Ranajoy Sadhukhan |  |
| 2556 |  |  [Hierarchical World Models as Visual Whole-Body Humanoid Controllers](https://openreview.net/forum?id=7wuJMvK639) |  | 0 | Whole-body control for humanoids is challenging due to the high-dimensional nature of the problem, coupled with the inherent instability of a bipedal morphology. Learning from visual observations further exacerbates this difficulty. In this work, we explore highly data-driven approaches to visual... | Hao Su, Jyothir S. V, Nicklas Hansen, Vlad Sobal, Xiaolong Wang, Yann LeCun |  |
| 2557 |  |  [How to Probe: Simple Yet Effective Techniques for Improving Post-hoc Explanations](https://openreview.net/forum?id=57NfyYxh5f) |  | 0 | Post-hoc importance attribution methods are a popular tool for “explaining” Deep Neural Networks (DNNs) and are inherently based on the assumption that the explanations can be applied independently of how the models were trained. Contrarily, in this work we bring forward empirical evidence that... | Bernt Schiele, Francesco Locatello, Moritz Böhle, Siddhartha Gairola |  |
| 2558 |  |  [Random-Set Neural Networks](https://openreview.net/forum?id=pdjkikvCch) |  | 0 | Machine learning is increasingly deployed in safety-critical domains where erroneous predictions may lead to potentially catastrophic consequences, highlighting the need for learning systems to be aware of how confident they are in their own predictions: in other words, 'to know when they do not... | Fabio Cuzzolin, Kaizheng Wang, Keivan Shariatmadar, Muhammad Mubashar, Shireen Kudukkil Manchingal |  |
| 2559 |  |  [Mitigating Spurious Correlations in Zero-Shot Multimodal Models](https://openreview.net/forum?id=UsRKFYR4lM) |  | 0 | Multimodal models or Vision Language Models (VLMs) have reshaped the paradigm in machine learning, offering zero-shot capabilities that require no additional training when adapted to new classification tasks. However, despite their advancements, spurious correlations still exist in VLMs. Existing... | Junyi Chai, Shenyu Lu, Xiaoqian Wang |  |
| 2560 |  |  [Efficient Cross-Episode Meta-RL](https://openreview.net/forum?id=UENQuayzr1) |  | 0 | We introduce Efficient Cross-Episodic Transformers (ECET), a new algorithm for online Meta-Reinforcement Learning that addresses the challenge of enabling reinforcement learning agents to perform effectively in previously unseen tasks. We demonstrate how past episodes serve as a rich source of... | André Biedenkapp, Florian Walter, Gresa Shala, Josif Grabocka, Pierre Krack |  |
| 2561 |  |  [Exposure Bracketing Is All You Need For A High-Quality Image](https://openreview.net/forum?id=rDIf6NA5mj) |  | 0 | It is highly desired but challenging to acquire high-quality photos with clear content in low-light environments. Although multi-image processing methods (using burst, dual-exposure, or multi-exposure images) have made significant progress in addressing this issue, they typically focus on specific... | Renlong Wu, Shuohao Zhang, Wangmeng Zuo, Zhilu Zhang, Zifei Yan |  |
| 2562 |  |  [Towards Realistic Data Generation for Real-World Super-Resolution](https://openreview.net/forum?id=JkCJBoNUcU) |  | 0 | Existing image super-resolution (SR) techniques often fail to generalize effectively in complex real-world settings due to the significant divergence between training data and practical scenarios. To address this challenge, previous efforts have either manually simulated intricate physical-based... | Jiaqi Xu, Jingjing Ren, Long Peng, Renjing Pei, Wenbo Li, Yang Cao, Yang Wang, ZhengJun Zha |  |
| 2563 |  |  [Visual Description Grounding Reduces Hallucinations and Boosts Reasoning in LVLMs](https://openreview.net/forum?id=3PRvlT8b1R) |  | 0 | Large Vision-Language Models (LVLMs) often produce responses that misalign with factual information, a phenomenon known as hallucinations. While hallucinations are well-studied, the exact causes behind them remain underexplored. In this paper, we first investigate the root causes of hallucinations... | Chandra Kiran Reddy Evuru, Dinesh Manocha, Oriol Nieto, Sonal Kumar, Sreyan Ghosh, Utkarsh Tyagi, Zeyu Jin |  |
| 2564 |  |  [MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs](https://openreview.net/forum?id=DgaY5mDdmT) |  | 0 | Multimodal Large Language Models (MLLMs) have experienced rapid progress in visual recognition tasks in recent years. Given their potential integration into many critical applications, it is important to understand the limitations of their visual perception. In this work, we study whether MLLMs can... | Filip Ilievski, Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara |  |
| 2565 |  |  [Partially Observed Trajectory Inference using Optimal Transport and a Dynamics Prior](https://openreview.net/forum?id=H8hO3T3DYe) |  | 0 | Trajectory inference seeks to recover the temporal dynamics of a population from snapshots of its (uncoupled) temporal marginals, i.e. where observed particles are \emph{not} tracked over time. Prior works addressed this challenging problem under a stochastic differential equation (SDE) model with... | Anming Gu, Edward Chien, Kristjan H. Greenewald |  |
| 2566 |  |  [Universal Sharpness Dynamics in Neural Network Training: Fixed Point Analysis, Edge of Stability, and Route to Chaos](https://openreview.net/forum?id=VZN0irKnl0) |  | 0 | In gradient descent dynamics of neural networks, the top eigenvalue of the Hessian of the loss (sharpness) displays a variety of robust phenomena throughout training. This includes early time regimes where the sharpness may decrease during early periods of training (sharpness reduction), and later... | Dayal Singh Kalra, Maissam Barkeshli, Tianyu He |  |
| 2567 |  |  [CHAMP: Conformalized 3D Human Multi-Hypothesis Pose Estimators](https://openreview.net/forum?id=kPC83HK4br) |  | 0 | We introduce CHAMP, a novel method for learning sequence-to-sequence, multi-hypothesis 3D human poses from 2D keypoints by leveraging a conditional distribution with a diffusion model. To predict a single output 3D pose sequence, we generate and aggregate multiple 3D pose hypotheses. For better... | Harry Zhang, Luca Carlone |  |
| 2568 |  |  [ELFS: Label-Free Coreset Selection with Proxy Training Dynamics](https://openreview.net/forum?id=yklJpvB7Dq) |  | 0 | High-quality human-annotated data is crucial for modern deep learning pipelines, yet the human annotation process is both costly and time-consuming. Given a constrained human labeling budget, selecting an informative and representative data subset for labeling can significantly reduce human... | Atul Prakash, Bhavya Kailkhura, Brian R. Bartoldson, Elisa Tsai, Haizhong Zheng, Jiachen Sun, Yifu Lu |  |
| 2569 |  |  [Inverse Attention Agents for Multi-Agent Systems](https://openreview.net/forum?id=OaoDVZntGe) |  | 0 | A major challenge for Multi-Agent Systems (MAS) is enabling agents to adapt dynamically to diverse environments in which opponents and teammates may continually change. Agents trained using conventional methods tend to excel only within the confines of their training cohorts; their performance... | Demetri Terzopoulos, Minglu Zhao, Qian Long, Ruoyan Li, Tao Gao |  |
| 2570 |  |  [R2Det: Exploring Relaxed Rotation Equivariance in 2D Object Detection](https://openreview.net/forum?id=EUeNr3e8AV) |  | 0 | Group Equivariant Convolution (GConv) empowers models to explore underlying symmetry in data, improving performance. However, real-world scenarios often deviate from ideal symmetric systems caused by physical permutation, characterized by non-trivial actions of a symmetry group, resulting in... | Bo Jin, Hanlin Dong, Jian Yang, Mingsong Chen, Xian Wei, Xuan Tang, Yingjie Liu, Zhiqiang Wu |  |
| 2571 |  |  [Computational Limits of Low-Rank Adaptation (LoRA) Fine-Tuning for Transformer Models](https://openreview.net/forum?id=Lf5znhZmFu) |  | 0 | We study the computational limits of Low-Rank Adaptation (LoRA) for finetuning transformer-based models using fine-grained complexity theory. Our key observation is that the existence of low-rank decompositions within the gradient computation of LoRA adaptation leads to possible algorithmic... | EnJui Kuo, Han Liu, Jerry YaoChieh Hu, Maojiang Su, Zhao Song |  |
| 2572 |  |  [A Simple Approach to Unifying Diffusion-based Conditional Generation](https://openreview.net/forum?id=tAGmxz1TUi) |  | 0 | Recent progress in image generation has sparked research into controlling these models through condition signals, with various methods addressing specific challenges in conditional generation. Instead of proposing another specialized technique, we introduce a simple, unified framework to handle... | Chao Ma, Charles Herrmann, Deqing Sun, Kelvin C. K. Chan, MingHsuan Yang, Xirui Li, Yinxiao Li |  |
| 2573 |  |  [Do LLMs "know" internally when they follow instructions?](https://openreview.net/forum?id=qIN5VDdEOr) |  | 0 | Instruction-following is crucial for building AI agents with large language models (LLMs), as these models must adhere strictly to user-provided constraints and guidelines. However, LLMs often fail to follow even simple and clear instructions. To improve instruction-following behavior and prevent... | Andrew C. Miller, Christina HeinzeDeml, Jaya Narain, Juyeon Heo, Kwan Ho Ryan Chan, Oussama Elachqar, Shirley You Ren, Udhyakumar Nallasamy |  |
| 2574 |  |  [Fundamental Limits of Prompt Tuning Transformers: Universality, Capacity and Efficiency](https://openreview.net/forum?id=jDpdQPMosW) |  | 0 | We investigate the statistical and computational limits of prompt tuning for transformer-based foundation models. Our key contributions are that prompt tuning on \*single-head\* transformers with only a \*single\* self-attention layer: (i) is universal, and (ii) supports efficient (even... | Ammar Gilani, Chenyang Li, Han Liu, Jerry YaoChieh Hu, WeiPo Wang, Zhao Song |  |
| 2575 |  |  [Disentangling 3D Animal Pose Dynamics with Scrubbed Conditional Latent Variables](https://openreview.net/forum?id=i7drKWhFCo) |  | 0 | Methods for tracking lab animal movements in unconstrained environments have become increasingly common and powerful tools for neuroscience. The prevailing hypothesis is that animal behavior in these environments comprises sequences of discrete stereotyped body movements ("motifs" or "actions").... | Alex H. Williams, Anshuman Sabath, Hari Koneru, James Michael Roach, James Russell Ravenel, Joshua Huang Wu, Michael R. Tadross, Shaun SzeXian Lim, Timothy W. Dunn |  |
| 2576 |  |  [Provable Convergence and Limitations of Geometric Tempering for Langevin Dynamics](https://openreview.net/forum?id=DZcmz9wU0i) |  | 0 | Geometric tempering is a popular approach to sampling from challenging multi-modal probability distributions by instead sampling from a sequence of distributions which interpolate, using the geometric mean, between an easier proposal distribution and the target distribution. In this paper, we... | Adrien Vacher, Anna Korba, Austin J. Stromme, Omar Chehab |  |
| 2577 |  |  [Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training](https://openreview.net/forum?id=SIE6VFps9x) |  | 0 | Large language models (LLMs), optimized through human feedback, have rapidly emerged as a leading paradigm for developing intelligent conversational assistants. However, despite their strong performance across many benchmarks, LLM-based agents might still lack conversational skills such as... | Maximillian Chen, Ruoxi Sun, Sercan Ö. Arik, Tomas Pfister |  |
| 2578 |  |  [Can a Large Language Model be a Gaslighter?](https://openreview.net/forum?id=RQPSPGpBOP) |  | 0 | Large language models (LLMs) have gained human trust due to their capabilities and helpfulness. However, this in turn may allow LLMs to affect users' mindsets by manipulating language. It is termed as gaslighting, a psychological effect. In this work, we aim to investigate the vulnerability of LLMs... | Luyao Zhu, Rui Mao, Ruixi Lin, Wei Li, Yang Song, Yang You |  |
| 2579 |  |  [Synergy Between Sufficient Changes and Sparse Mixing Procedure for Disentangled Representation Learning](https://openreview.net/forum?id=G1r2rBkUdu) |  | 0 | Disentangled representation learning aims to uncover the latent variables underlying observed data, yet identifying these variables under mild assumptions remains challenging. Some methods rely on sufficient changes in the distribution of latent variables indicated by auxiliary variables, such as... | Guangyi Chen, Ignavier Ng, Kun Zhang, Ruichu Cai, Shaoan Xie, Shunxing Fan, Xinshuai Dong, Yujia Zheng, Zijian Li |  |
| 2580 |  |  [Edge Prompt Tuning for Graph Neural Networks](https://openreview.net/forum?id=92vMaHotTM) |  | 0 | Pre-training powerful Graph Neural Networks (GNNs) with unlabeled graph data in a self-supervised manner has emerged as a prominent technique in recent years. However, inevitable objective gaps often exist between pre-training and downstream tasks. To bridge this gap, graph prompt tuning techniques... | Jundong Li, Xingbo Fu, Yinhan He |  |
| 2581 |  |  [Sparse Learning for State Space Models on Mobile](https://openreview.net/forum?id=t8KLjiFNwn) |  | 0 | Transformer models have been widely investigated in different domains by providing long-range dependency handling and global contextual awareness, driving the development of popular AI applications such as ChatGPT, Gemini, and Alexa. State Space Models (SSMs) have emerged as strong contenders in... | Changdi Yang, Hangyu Zheng, Pu Zhao, Wei Niu, Xuan Shen, Xue Lin, Yanzhi Wang, Yifan Gong, Yushu Wu, Zheng Zhan, Zhenglun Kong |  |
| 2582 |  |  [Optimized Multi-Token Joint Decoding With Auxiliary Model for LLM Inference](https://openreview.net/forum?id=ZHhBawo3k5) |  | 0 | Large language models (LLMs) have achieved remarkable success across diverse tasks, yet their inference processes are hindered by substantial time and energy demands due to single-token generation at each decoding step. While previous methods such as speculative decoding mitigate these... | Jason Cong, Neha Prakriya, Yizhou Sun, Zifan He, Ziniu Hu, Zongyue Qin |  |
| 2583 |  |  [Unlocking Global Optimality in Bilevel Optimization: A Pilot Study](https://openreview.net/forum?id=2xvisNIfdw) |  | 0 | Bilevel optimization has witnessed a resurgence of interest, driven by its critical role in trustworthy and efficient AI applications. Recent focus has been on finding efficient methods with provable convergence guarantees. However, while many prior works have established convergence to stationary... | Quan Xiao, Tianyi Chen |  |
| 2584 |  |  [Selective Task Group Updates for Multi-Task Optimization](https://openreview.net/forum?id=EdNSQHaaMR) |  | 0 | Multi-task learning enables the acquisition of task-generic knowledge by training multiple tasks within a unified architecture. However, training all tasks together in a single architecture can lead to performance degradation, known as negative transfer, which is a main concern in multi-task... | KukJin Yoon, Wooseong Jeong |  |
| 2585 |  |  [Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting](https://openreview.net/forum?id=xgQfWbV6Ey) |  | 0 | Retrieval augmented generation (RAG) combines the generative abilities of large language models (LLMs) with external knowledge sources to provide more accurate and up-to-date responses. Recent RAG advancements focus on improving retrieval outcomes through iterative LLM refinement or self-critique... | Ankur Taly, Anush Mattapalli, ChenYu Lee, Huaixiu Steven Zheng, Jingbo Shang, Long T. Le, Swaroop Mishra, Tomas Pfister, Vincent Perot, Yuwei Zhang, Zifeng Wang, Zilong Wang |  |
| 2586 |  |  [Algorithmic Stability Based Generalization Bounds for Adversarial Training](https://openreview.net/forum?id=2GwMazl9ND) |  | 0 | In this paper, we present a novel stability analysis of adversarial training and prove generalization upper bounds in terms of an expansiveness property of adversarial perturbations used during training and used for evaluation. These expansiveness parameters appear not only govern the vanishing... | Runzhi Tian, Yongyi Mao |  |
| 2587 |  |  [Tracking objects that change in appearance with phase synchrony](https://openreview.net/forum?id=m2gVfgWYDO) |  | 0 | Objects we encounter often change appearance as we interact with them. Changes in illumination (shadows), object pose, or the movement of non-rigid objects can drastically alter available image features. How do biological visual systems track objects as they change? One plausible mechanism involves... | Alekh Karkada Ashok, Drew Linsley, Ennio Mingolla, Girik Malik, Rufin VanRullen, Sabine Muzellec, Thomas Serre |  |
| 2588 |  |  [GSE: Group-wise Sparse and Explainable Adversarial Attacks](https://openreview.net/forum?id=d54fIsAbff) |  | 0 | Sparse adversarial attacks fool deep neural networks (DNNs) through minimal pixel perturbations, often regularized by the $\ell_0$ norm. Recent efforts have replaced this norm with a structural sparsity regularizer, such as the nuclear group norm, to craft group-wise sparse adversarial attacks. The... | Moritz Wagner, Sebastian Pokutta, Shpresim Sadiku |  |
| 2589 |  |  [CoTFormer: A Chain of Thought Driven Architecture with Budget-Adaptive Computation Cost at Inference](https://openreview.net/forum?id=7igPXQFupX) |  | 0 | Scaling language models to larger and deeper sizes has led to significant boosts in performance. Even though the size of these models limits their application in compute-constrained environments, the race to continually develop ever larger and deeper foundational models is underway. At the same... | Amirkeivan Mohtashami, Martin Jaggi, Matteo Pagliardini |  |
| 2590 |  |  [Adaptive Pruning of Pretrained Transformer via Differential Inclusions](https://openreview.net/forum?id=WA84oMWHaH) |  | 0 | Large transformers have demonstrated remarkable success, making it necessary to compress these models to reduce inference costs while preserving their performance. Current compression algorithms prune transformers at fixed compression ratios, requiring a unique pruning process for each ratio, which... | Ke Fan, Xinwei Sun, Yanwei Fu, Yikai Wang, Yizhuo Ding |  |
| 2591 |  |  [Convergence of Distributed Adaptive Optimization with Local Updates](https://openreview.net/forum?id=VNg7srnvD9) |  | 0 | We study distributed adaptive algorithms with local updates (intermittent communication). Despite the great empirical success of adaptive methods in distributed training of modern machine learning models, the theoretical benefits of local updates within adaptive methods, particularly in terms of... | Margalit Glasgow, Ziheng Cheng |  |
| 2592 |  |  [Learning a Neural Solver for Parametric PDEs to Enhance Physics-Informed Methods](https://openreview.net/forum?id=jqVj8vCQsT) |  | 0 | Physics-informed deep learning often faces optimization challenges due to the complexity of solving partial differential equations (PDEs), which involve exploring large solution spaces, require numerous iterations, and can lead to unstable training. These challenges arise particularly from the... | Emmanuel de Bézenac, Lise Le Boudec, Louis Serrano, Patrick Gallinari, Ramon Daniel RegueiroEspino, Yuan Yin |  |
| 2593 |  |  [Theory on Score-Mismatched Diffusion Models and Zero-Shot Conditional Samplers](https://openreview.net/forum?id=nWT6LxbuGi) |  | 0 | The denoising diffusion model has recently emerged as a powerful generative technique, capable of transforming noise into meaningful data. While theoretical convergence guarantees for diffusion models are well established when the target distribution aligns with the training distribution, practical... | Ness B. Shroff, Peizhong Ju, Yingbin Liang, Yuchen Liang |  |
| 2594 |  |  [Towards Interpreting Visual Information Processing in Vision-Language Models](https://openreview.net/forum?id=chanJGoa7f) |  | 0 | Vision-Language Models (VLMs) are powerful tools for processing and understanding text and images. We study the processing of visual tokens in the language model component of LLaVA, a prominent VLM. Our approach focuses on analyzing the localization of object information, the evolution of visual... | Clement Neo, David Krueger, Fazl Barez, Luke Ong, Mor Geva, Philip Torr |  |
| 2595 |  |  [Adaptive Q-Network: On-the-fly Target Selection for Deep Reinforcement Learning](https://openreview.net/forum?id=leACdxBEgv) |  | 0 | Deep Reinforcement Learning (RL) is well known for being highly sensitive to hyperparameters, requiring practitioners substantial efforts to optimize them for the problem at hand. This also limits the applicability of RL in real-world scenarios. In recent years, the field of automated Reinforcement... | Boris Belousov, Carlo D'Eramo, Fabian Wahren, Jan Peters, Théo Vincent |  |
| 2596 |  |  [Adversarial Attacks on Data Attribution](https://openreview.net/forum?id=oJgIRwkIUB) |  | 0 | Data attribution aims to quantify the contribution of individual training data points to the outputs of an AI model, which has been used to measure the value of training data and compensate data providers. Given the impact on financial decisions and compensation mechanisms, a critical question... | Jiaqi W. Ma, Junwei Deng, Pingbang Hu, Xinhe Wang |  |
| 2597 |  |  [Not All Prompts Are Made Equal: Prompt-based Pruning of Text-to-Image Diffusion Models](https://openreview.net/forum?id=3BhZCfJ73Y) |  | 0 | Text-to-image (T2I) diffusion models have demonstrated impressive image generation capabilities. Still, their computational intensity prohibits resource-constrained organizations from deploying T2I models after fine-tuning them on their internal \*target\* data. While pruning techniques offer a... | Alireza Ganjdanesh, Heng Huang, Reza Shirkavand, Shangqian Gao |  |
| 2598 |  |  [F-Fidelity: A Robust Framework for Faithfulness Evaluation of Explainable AI](https://openreview.net/forum?id=X0r4BN50Dv) |  | 0 | Recent research has developed a number of eXplainable AI (XAI) techniques, such as gradient-based approaches, input perturbation-base methods, and black-box explanation methods. While these XAI techniques can extract meaningful insights from deep learning models, how to properly evaluate them... | Chaohao Lin, Dongsheng Luo, Farhad Shirani, Wei Cheng, Wenbo Guo, Xu Zheng, Zhuomin Chen |  |
| 2599 |  |  [Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with Gaussian Distribution](https://openreview.net/forum?id=CvttyK4XzV) |  | 0 | Probing learned concepts in large language models (LLMs) is crucial for understanding how semantic knowledge is encoded internally. Training linear classifiers on probing tasks is a principle approach to denote the vector of a certain concept in the representation space. However, the single vector... | Ali Payani, Bo Shen, Fan Yang, Haiyan Zhao, Heng Zhao, Mengnan Du |  |
| 2600 |  |  [InstructRAG: Instructing Retrieval-Augmented Generation via Self-Synthesized Rationales](https://openreview.net/forum?id=P1qhkp8gQT) |  | 0 | Retrieval-augmented generation (RAG) has shown promising potential to enhance the accuracy and factuality of language models (LMs). However, imperfect retrievers or noisy corpora can introduce misleading or even erroneous information to the retrieved contents, posing a significant challenge to the... | WeiLin Chen, Yu Meng, Zhepei Wei |  |
| 2601 |  |  [MixMax: Distributional Robustness in Function Space via Optimal Data Mixtures](https://openreview.net/forum?id=dIkpHooa2D) |  | 0 | Machine learning models are often required to perform well across several pre-defined settings, such as a set of user groups. Worst-case performance is a common metric to capture this requirement, and is the objective of group distributionally robust optimization (group DRO). Unfortunately, these... | Anvith Thudi, Chris J. Maddison |  |
| 2602 |  |  [Digi-Q: Learning VLM Q-Value Functions for Training Device-Control Agents](https://openreview.net/forum?id=CjfQssZtAb) |  | 0 | While a number of existing approaches for building foundation model agents rely on prompting or fine-tuning with human demonstrations, it is not sufficient in dynamic environments (e.g., mobile device control). On-policy reinforcement learning (RL) should address these limitations, but collecting... | Aviral Kumar, Hao Bai, Li Erran Li, Sergey Levine, Yifei Zhou |  |
| 2603 |  |  [Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains](https://openreview.net/forum?id=JtGPIZpOrz) |  | 0 | Large language models (LLMs) have achieved remarkable performance in recent years but are fundamentally limited by the underlying training data. To improve models beyond the training data, recent works have explored how LLMs can be used to generate synthetic data for autonomous self-improvement.... | Antonio Torralba, Igor Mordatch, Joshua B. Tenenbaum, Shuang Li, Vighnesh Subramaniam, Yilun Du |  |
| 2604 |  |  [MeToken: Uniform Micro-environment Token Boosts Post-Translational Modification Prediction](https://openreview.net/forum?id=noUF58SMra) |  | 0 | Post-translational modifications (PTMs) profoundly expand the complexity and functionality of the proteome, regulating protein attributes and interactions that are crucial for biological processes. Accurately predicting PTM sites and their specific types is therefore essential for elucidating... | Bozhen Hu, Cheng Tan, Jun Xia, Lirong Wu, Siyuan Li, Stan Z. Li, Yufei Huang, Zhangyang Gao, Zhenxiao Cao |  |
| 2605 |  |  [GMValuator: Similarity-based Data Valuation for Generative Models](https://openreview.net/forum?id=WncnpvJk83) |  | 0 | Data valuation plays a crucial role in machine learning. Existing data valuation methods, mainly focused on discriminative models, overlook generative models that have gained attention recently. In generative models, data valuation measures the impact of training data on generated datasets. Very... | Benlin Liu, James Zou, Jiaxi Yang, Wenlong Deng, Xiaoxiao Li, Yangsibo Huang |  |
| 2606 |  |  [Implicit In-context Learning](https://openreview.net/forum?id=G7u4ue6ncT) |  | 0 | In-context Learning (ICL) empowers large language models (LLMs) to swiftly adapt to unseen tasks at inference-time by prefixing a few demonstration examples before queries. Despite its versatility, ICL incurs substantial computational and memory overheads compared to zero-shot learning and is... | Di Liu, Dimitris N. Metaxas, Hao Wang, Ligong Han, Song Wen, Yunhe Gao, Zhuowei Li, Zihao Xu |  |
| 2607 |  |  [UTILITY: Utilizing Explainable Reinforcement Learning to Improve Reinforcement Learning](https://openreview.net/forum?id=Tk1VQDadfL) |  | 0 | Reinforcement learning (RL) faces two challenges: (1) The RL agent lacks explainability. (2) The trained RL agent is, in many cases, non-optimal and even far from optimal. To address the first challenge, explainable reinforcement learning (XRL) is proposed to explain the decision-making of the RL... | Minghui Zhu, Shicheng Liu |  |
| 2608 |  |  [MS-Diffusion: Multi-subject Zero-shot Image Personalization with Layout Guidance](https://openreview.net/forum?id=PJqP0wyQek) |  | 0 | Recent advancements in text-to-image generation models have dramatically enhanced the generation of photorealistic images from textual prompts, leading to an increased interest in personalized text-to-image applications, particularly in multi-subject scenarios. However, these advances are hindered... | Hao Jiang, Qihan Huang, Siming Fu, Wanggui He, Xierui Wang |  |
| 2609 |  |  [Generalization Bounds and Model Complexity for Kolmogorov-Arnold Networks](https://openreview.net/forum?id=q5zMyAUhGx) |  | 0 | Kolmogorov–Arnold Network (KAN) is a network structure recently proposed in Liu et al. (2024) that offers improved interpretability and a more parsimonious design in many science-oriented tasks compared to multi-layer perceptrons. This work provides a rigorous theoretical analysis of KAN by... | Huijuan Zhou, Xianyang Zhang |  |
| 2610 |  |  [Backdooring Vision-Language Models with Out-Of-Distribution Data](https://openreview.net/forum?id=tZozeR3VV7) |  | 0 | The emergence of Vision-Language Models (VLMs) represents a significant advancement in integrating computer vision with Large Language Models (LLMs) to generate detailed text descriptions from visual inputs. Despite their growing importance, the security of VLMs, particularly against backdoor... | Chao Chen, Haibin Ling, Jiachen Yao, Lijie Hu, Lingjie Yi, Lu Pang, Saumya Gupta, Tao Sun, Weimin Lyu |  |
| 2611 |  |  [Towards Homogeneous Lexical Tone Decoding from Heterogeneous Intracranial Recordings](https://openreview.net/forum?id=cWEfRkYj46) |  | 0 | Recent advancements in brain-computer interfaces (BCIs) and deep learning have made decoding lexical tones from intracranial recordings possible, providing the potential to restore the communication ability of speech-impaired tonal language speakers. However, data heterogeneity induced by both... | Chen Feng, Di Wu, Jie Yang, Lu Cao, Mohamad Sawan, Siyuan Li, Yue Zhang |  |
| 2612 |  |  [Minimal Variance Model Aggregation: A principled, non-intrusive, and versatile integration of black box models](https://openreview.net/forum?id=grM2Yv49cI) |  | 0 | Whether deterministic or stochastic, models can be viewed as functions designed to approximate a specific quantity of interest. We introduce Minimal Empirical Variance Aggregation (MEVA), a data-driven framework that integrates predictions from various models, enhancing overall accuracy by... | Houman Owhadi, Théo Bourdais |  |
| 2613 |  |  [Towards Continuous Reuse of Graph Models via Holistic Memory Diversification](https://openreview.net/forum?id=Pbz4i7B0B4) |  | 0 | This paper addresses the challenge of incremental learning in growing graphs with increasingly complex tasks. The goal is to continuously train a graph model to handle new tasks while retaining proficiency in previous tasks via memory replay. Existing methods usually overlook the importance of... | Hui Xiong, Junren Xiao, Meng Xiao, Qingqiang Sun, Xiao Luo, Ziyue Qiao |  |
| 2614 |  |  [Multi-modal brain encoding models for multi-modal stimuli](https://openreview.net/forum?id=0dELcFHig2) |  | 0 | Despite participants engaging in unimodal stimuli, such as watching images or silent videos, recent work has demonstrated that multi-modal Transformer models can predict visual brain activity impressively well, even with incongruent modality representations. This raises the question of how... | Bapi Raju Surampudi, Khushbu Pahwa, Maneesh Kumar Singh, Manish Gupta, Mounika Marreddy, Subba Reddy Oota |  |
| 2615 |  |  [Unlocking Efficient, Scalable, and Continual Knowledge Editing with Basis-Level Representation Fine-Tuning](https://openreview.net/forum?id=PITFO1ddeh) |  | 0 | Large language models (LLMs) have achieved remarkable performance on vari- ous natural language tasks. However, they are trained on static corpora and their knowledge can become outdated quickly in the fast-changing world. This moti- vates the development of knowledge editing methods designed to... | Haoyu Wang, Hui Liu, Jing Gao, Jun Huan, Monica Xiao Cheng, Qingyu Yin, Ruirui Li, Tianci Liu, Tianqi Zheng, Xianfeng Tang, Yunzhe Qi |  |
| 2616 |  |  [Contextual Document Embeddings](https://openreview.net/forum?id=Wqsk3FbD6D) |  | 0 | Dense document embeddings are central to neural retrieval. The dominant paradigm is to train and construct embeddings by running encoders directly on individual documents. In this work, we argue that these embeddings, while effective, are implicitly out-of-context for targeted use cases of... | Alexander M. Rush, John Xavier Morris |  |
| 2617 |  |  [Do LLMs have Consistent Values?](https://openreview.net/forum?id=8zxGruuzr9) |  | 0 | Large Language Models (LLM) technology is rapidly advancing towards human- like dialogue. Values are fundamental drivers of human behavior, yet research on the values expressed in LLM-generated text remains limited. While prior work has begun to explore value ranking in LLMs, the crucial aspect of... | Amir Globerson, Ella Daniel, Gal Elidan, Liat Bezalel, Naama Rozen |  |
| 2618 |  |  [AdaManip: Adaptive Articulated Object Manipulation Environments and Policy Learning](https://openreview.net/forum?id=Luss2sa0vc) |  | 0 | Articulated object manipulation is a critical capability for robots to perform various tasks in real-world scenarios. Composed of multiple parts connected by joints, articulated objects are endowed with diverse functional mechanisms through complex relative motions. For example, a safe consists of... | Hao Dong, Mingdong Wu, Ruihai Wu, Xiaojie Zhang, Yan Shen, Yizhou Wang, Yu Li, Yuanfei Wang, Zhaofeng He |  |
| 2619 |  |  [Range, not Independence, Drives Modularity in Biologically Inspired Representations](https://openreview.net/forum?id=BxQkDog4ti) |  | 0 | Why do biological and artificial neurons sometimes modularise, each encoding a single meaningful variable, and sometimes entangle their representation of many variables? In this work, we develop a theory of when biologically inspired networks---those that are nonnegative and energy... | Chelsea Finn, James C. R. Whittington, Jiajun Wu, Jin Hwa Lee, Kyle Hsu, Luke Hollingsworth, Peter E. Latham, Timothy Edward John Behrens, Will Dorrell |  |
| 2620 |  |  [Measuring memorization in RLHF for code completion](https://openreview.net/forum?id=Tg8RLxpMDu) |  | 0 | Reinforcement learning with human feedback (RLHF) has become the dominant method to align large models to user preferences. Unlike fine-tuning, for which there are many studies regarding training data memorization, it is not clear how memorization is affected by or introduced in the RLHF alignment... | Aneesh Pappu, Ilia Shumailov, Jamie Hayes, William P. Porter |  |
| 2621 |  |  [Towards Certification of Uncertainty Calibration under Adversarial Attacks](https://openreview.net/forum?id=uuPkll6i7m) |  | 0 | Since neural classifiers are known to be sensitive to adversarial perturbations that alter their accuracy, certification methods have been developed to provide provable guarantees on the insensitivity of their predictions to such perturbations. On the other hand, in safety-critical applications,... | Adel Bibi, Cornelius Emde, Francesco Pinto, Philip Torr, Thomas Lukasiewicz |  |
| 2622 |  |  [A Policy-Gradient Approach to Solving Imperfect-Information Games with Best-Iterate Convergence](https://openreview.net/forum?id=ZW4MRZrmSA) |  | 0 | Policy gradient methods have become a staple of any single-agent reinforcement learning toolbox, due to their combination of desirable properties: iterate convergence, efficient use of stochastic trajectory feedback, and theoretically-sound avoidance of importance sampling corrections. In... | Asuman E. Ozdaglar, Gabriele Farina, Mingyang Liu |  |
| 2623 |  |  [Improved Techniques for Optimization-Based Jailbreaking on Large Language Models](https://openreview.net/forum?id=e9yfCY7Q3U) |  | 0 | Large language models (LLMs) are being rapidly developed, and a key component of their widespread deployment is their safety-related alignment. Many red-teaming efforts aim to jailbreak LLMs, where among these efforts, the Greedy Coordinate Gradient (GCG) attack's success has led to a growing... | Chao Du, Jindong Gu, Min Lin, Tianyu Pang, Xiaochun Cao, Xiaojun Jia, Yang Liu, Yihao Huang |  |
| 2624 |  |  [Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient Transformer Models](https://openreview.net/forum?id=T26f9z2rEe) |  | 0 | The Sparse Mixture of Experts (SMoE) has been widely employed to enhance the efficiency of training and inference for Transformer-based foundational models, yielding promising results. However, the performance of SMoE heavily depends on the choice of hyper-parameters, such as the number of experts... | Tao Lin, Xiaoying Tang, Yongxin Guo, Zhaopeng Tu, Zhenglin Cheng |  |
| 2625 |  |  [Agent-to-Sim: Learning Interactive Behavior Models from Casual Longitudinal Videos](https://openreview.net/forum?id=y80D4IojuY) |  | 0 | We present Agent-to-Sim (ATS), a framework for learning interactive behavior models of 3D agents from casual longitudinal video collections. Different from prior works that rely on marker-based tracking and multiview cameras, ATS learns natural behaviors of animal agents non-invasively through... | Andrea Bajcsy, Angjoo Kanazawa, Gengshan Yang, Shunsuke Saito |  |
| 2626 |  |  [Explaining Modern Gated-Linear RNNs via a Unified Implicit Attention Formulation](https://openreview.net/forum?id=wnT8bfJCDx) |  | 0 | Recent advances in efficient sequence modeling have led to attention-free layers, such as Mamba, RWKV, and various gated RNNs, all featuring sub-quadratic complexity in sequence length and excellent scaling properties, enabling the construction of a new type of foundation models. In this paper, we... | Ameen Ali, Itamar Zimerman, Lior Wolf |  |
| 2627 |  |  [Zero-shot Model-based Reinforcement Learning using Large Language Models](https://openreview.net/forum?id=uZFXpPrwSh) |  | 0 | The emerging zero-shot capabilities of Large Language Models (LLMs) have led to their applications in areas extending well beyond natural language processing tasks. In reinforcement learning, while LLMs have been extensively used in text-based environments, their integration with continuous state... | Abdelhakim Benechehab, Albert Thomas, Ambroise Odonnat, Balázs Kégl, Giuseppe Paolo, Ievgen Redko, Maurizio Filippone, Oussama Zekri, Youssef Attia El Hili |  |
| 2628 |  |  [High-quality Text-to-3D Character Generation with SparseCubes and Sparse Transformers](https://openreview.net/forum?id=rfeksadZox) |  | 0 | Current state-of-the-art text-to-3D generation methods struggle to produce 3D models with fine details and delicate structures due to limitations in differentiable mesh representation techniques. This limitation is particularly pronounced in anime character generation, where intricate features such... | Feihu Zhang, Hongye Yang, Jiachen Qian, Jingxi Xu, Shuang Wu |  |
| 2629 |  |  [Implicit Neural Surface Deformation with Explicit Velocity Fields](https://openreview.net/forum?id=sYAFiHP6qr) |  | 0 | In this work, we introduce the first unsupervised method that simultaneously predicts time-varying neural implicit surfaces and deformations between pairs of point clouds. We propose to model the point movement using an explicit velocity field and directly deform a time-varying implicit field using... | Daniel Cremers, Dongliang Cao, Florian Bernard, Lu Sang, Zehranaz Canfes |  |
| 2630 |  |  [Multi-Label Node Classification with Label Influence Propagation](https://openreview.net/forum?id=3X3LuwzZrl) |  | 0 | Graphs are a complex and versatile data structure used across various domains, with possibly multi-label nodes playing a particularly crucial role. Examples include proteins in PPI networks with multiple functions and users in social or e-commerce networks exhibiting diverse interests. Tackling... | Bingsheng He, Bryan Hooi, Jia Chen, Rizal Fathony, Yang Yang, Yifei Sun, Zemin Liu |  |
| 2631 |  |  [Interpreting the Second-Order Effects of Neurons in CLIP](https://openreview.net/forum?id=GPDcvoFGOL) |  | 0 | We interpret the function of individual neurons in CLIP by automatically describing them using text. Analyzing the direct effects (i.e. the flow from a neuron through the residual stream to the output) or the indirect effects (overall contribution) fails to capture the neurons' function in CLIP.... | Alexei A. Efros, Jacob Steinhardt, Yossi Gandelsman |  |
| 2632 |  |  [ClawMachine: Learning to Fetch Visual Tokens for Referential Comprehension](https://openreview.net/forum?id=TOtk9dTYGG) |  | 0 | Aligning vision and language concepts at a finer level remains an essential topic of multimodal large language models (MLLMs), particularly for tasks such as referring and grounding. Existing methods, such as \*proxy encoding\* and \*geometry encoding\* genres, incorporate additional syntax to... | Boyu Yang, Lingxi Xie, Qixiang Ye, Tianren Ma, Yunjie Tian |  |
| 2633 |  |  [Diff3DS: Generating View-Consistent 3D Sketch via Differentiable Curve Rendering](https://openreview.net/forum?id=aIMi2lOKIn) |  | 0 | 3D sketches are widely used for visually representing the 3D shape and structure of objects or scenes. However, the creation of 3D sketch often requires users to possess professional artistic skills. Existing research efforts primarily focus on enhancing the ability of interactive sketch generation... | Changqing Zou, Lihong Wang, Rui Ma, Tieru Wu, Yibo Zhang |  |
| 2634 |  |  [InfoGS: Efficient Structure-Aware 3D Gaussians via Lightweight Information Shaping](https://openreview.net/forum?id=Pj2qEVzufH) |  | 0 | 3D Gaussians, as an explicit scene representation, typically involve thousands to millions of elements per scene. This makes it challenging to control the scene in ways that reflect the underlying semantics, where the number of independent entities is typically much smaller. Especially, if one... | Guandao Yang, Leonidas J. Guibas, Yanchao Yang, Yunchao Zhang |  |
| 2635 |  |  [Bayesian Image Regression with Soft-thresholded Conditional Autoregressive Prior](https://openreview.net/forum?id=rnL3OafDdw) |  | 0 | In the analysis of brain functional MRI (fMRI) data using regression models, Bayesian methods are highly valued for their flexibility and ability to quantify uncertainty. However, these methods face computational challenges in high-dimensional settings typical of brain imaging, and the often... | Jian Kang, Yuliang Xu |  |
| 2636 |  |  [TimeKAN: KAN-based Frequency Decomposition Learning Architecture for Long-term Time Series Forecasting](https://openreview.net/forum?id=wTLc79YNbh) |  | 0 | Real-world time series often have multiple frequency components that are intertwined with each other, making accurate time series forecasting challenging. Decomposing the mixed frequency components into multiple single frequency components is a natural choice. However, the information density of... | Can Li, Lei Bai, Songtao Huang, Zhen Zhao |  |
| 2637 |  |  [Preference Elicitation for Offline Reinforcement Learning](https://openreview.net/forum?id=2pJpFtdVNe) |  | 0 | Applying reinforcement learning (RL) to real-world problems is often made challenging by the inability to interact with the environment and the difficulty of designing reward functions. Offline RL addresses the first challenge by considering access to an offline dataset of environment interactions... | Alizée Pace, Bernhard Schölkopf, Giorgia Ramponi, Gunnar Rätsch |  |
| 2638 |  |  [Gaussian-Det: Learning Closed-Surface Gaussians for 3D Object Detection](https://openreview.net/forum?id=DtFCIfvAFc) |  | 0 | Skins wrapping around our bodies, leathers covering over the sofa, sheet metal coating the car – it suggests that objects are enclosed by a series of continuous surfaces, which provides us with informative geometry prior for objectness deduction. In this paper, we propose Gaussian-Det which... | Hongru Yan, Yu Zheng, Yueqi Duan |  |
| 2639 |  |  [RFMamba: Frequency-Aware State Space Model for RF-Based Human-Centric Perception](https://openreview.net/forum?id=lG9fjBLb6d) |  | 0 | Human-centric perception with radio frequency (RF) signals has recently entered a new era of end-to-end processing with Transformers. Considering the long-sequence nature of RF signals, the State Space Model (SSM) has emerged as a superior alternative due to its effective long-sequence modeling and... | Dongheng Zhang, Hanqin Gong, Rui Zhang, Ruixu Geng, Ruiyuan Song, Yadong Li, Yan Chen, Yang Hu |  |
| 2640 |  |  [Feature Averaging: An Implicit Bias of Gradient Descent Leading to Non-Robustness in Neural Networks](https://openreview.net/forum?id=zPHra4V5Mc) |  | 0 | In this work, we investigate a particular implicit bias in gradient descent training, which we term “Feature Averaging,” and argue that it is one of the principal factors contributing to the non-robustness of deep neural networks. We show that, even when multiple discriminative features are present... | Binghui Li, Jian Li, Kaifeng Lyu, Zhixuan Pan |  |
| 2641 |  |  [GOAL: A Generalist Combinatorial Optimization Agent Learner](https://openreview.net/forum?id=z2z9suDRjw) |  | 0 | Machine Learning-based heuristics have recently shown impressive performance in solving a variety of hard combinatorial optimization problems (COPs). However they generally rely on a separate neural model, specialized and trained for each single problem. Any variation of a problem requires... | Darko Drakulic, JeanMarc Andreoli, Sofia Michel |  |
| 2642 |  |  [Optimizing importance weighting in the presence of sub-population shifts](https://openreview.net/forum?id=j4gzziSUr0) |  | 0 | A distribution shift between the training and test data can severely harm performance of machine learning models. Importance weighting addresses this issue by assigning different weights to data points during training. We argue that existing heuristics for determining the weights are suboptimal, as... | Bram Wouters, Cees Diks, Floris Holstege, Noud P. A. van Giersbergen |  |
| 2643 |  |  [Auto-GDA: Automatic Domain Adaptation for Efficient Grounding Verification in Retrieval-Augmented Generation](https://openreview.net/forum?id=w5ZtXOzMeJ) |  | 0 | While retrieval-augmented generation (RAG) has been shown to enhance factuality of large language model (LLM) outputs, LLMs still suffer from hallucination, generating incorrect or irrelevant information. A common detection strategy involves prompting the LLM again to assess whether its response is... | Aaron Roth, Dionysis Manousakas, Giuseppe Vietri, Periklis Petridis, Sergül Aydöre, Tobias Leemann |  |
| 2644 |  |  [Cross the Gap: Exposing the Intra-modal Misalignment in CLIP via Modality Inversion](https://openreview.net/forum?id=VVVfuIcmKR) |  | 0 | Pre-trained multi-modal Vision-Language Models like CLIP are widely used off-the-shelf for a variety of applications. In this paper, we show that the common practice of individually exploiting the text or image encoders of these powerful multi-modal models is highly suboptimal for intra-modal tasks... | Alberto Baldrati, Andrew D. Bagdanov, Lorenzo Agnolucci, Marco Bertini, Marco Mistretta |  |
| 2645 |  |  [Cross-Domain Offline Policy Adaptation with Optimal Transport and Dataset Constraint](https://openreview.net/forum?id=LRrbD8EZJl) |  | 0 | We explore cross-domain offline reinforcement learning (RL) where offline datasets from another domain can be accessed to facilitate policy learning. However, the underlying environments of the two datasets may have dynamics mismatches, incurring inferior performance when simply merging the data of... | Deheng Ye, Jiafei Lyu, Jingwen Yang, Mengbei Yan, Runze Liu, Xiaoteng Ma, Xiu Li, Zhongjian Qiao, Zongqing Lu |  |
| 2646 |  |  [BadRobot: Jailbreaking Embodied LLM Agents in the Physical World](https://openreview.net/forum?id=ei3qCntB66) |  | 0 | Embodied AI represents systems where AI is integrated into physical entities. Multimodal Large Language Model (LLM), which exhibits powerful language understanding abilities, has been extensively employed in embodied AI by facilitating sophisticated task planning. However, a critical safety issue... | Aishan Liu, Changgan Yin, Chenyu Zhu, Hangtao Zhang, Leo Yu Zhang, Lulu Xue, Minghui Li, Peijin Guo, Shengshan Hu, Xianlong Wang, Yichen Wang, Ziqi Zhou |  |
| 2647 |  |  [Input Space Mode Connectivity in Deep Neural Networks](https://openreview.net/forum?id=3qeOy7HwUT) |  | 0 | We extend the concept of loss landscape mode connectivity to the input space of deep neural networks. Mode connectivity was originally studied within parameter space, where it describes the existence of low-loss paths between different solutions (loss minimizers) obtained through gradient descent.... | David Krueger, Jakub Vrábel, Ori ShemUr, Yaron Oz |  |
| 2648 |  |  [CircuitFusion: Multimodal Circuit Representation Learning for Agile Chip Design](https://openreview.net/forum?id=rbnf7oe6JQ) |  | 0 | The rapid advancements of AI rely on the support of integrated circuits (ICs). However, the growing complexity of digital ICs makes the traditional IC design process costly and time-consuming. In recent years, AI-assisted IC design methods have demonstrated great potential, but most methods are... | Jing Wang, Shang Liu, Wenji Fang, Zhiyao Xie |  |
| 2649 |  |  [Accessing Vision Foundation Models via ImageNet-1K](https://openreview.net/forum?id=LC6ZtQV6u2) |  | 0 | Vision foundation models are renowned for the generalization ability due to massive training data. Nevertheless, they demand tremendous training resources, and the training data is often inaccessible, e.g., CLIP, DINOv2, posing great challenges to developing derivatives that could facilitate the... | Huan Wang, Xu Ma, Yitian Zhang, Yue Bai, Yun Fu |  |
| 2650 |  |  [Binary Losses for Density Ratio Estimation](https://openreview.net/forum?id=562B7aLi5X) |  | 0 | Estimating the ratio of two probability densities from a finite number of observations is a central machine learning problem. A common approach is to construct estimators using binary classifiers that distinguish observations from the two densities. However, the accuracy of these estimators depends... | Werner Zellinger |  |
| 2651 |  |  [Sports-Traj: A Unified Trajectory Generation Model for Multi-Agent Movement in Sports](https://openreview.net/forum?id=9aTZf71uiD) |  | 0 | Understanding multi-agent movement is critical across various fields. The conventional approaches typically focus on separate tasks such as trajectory prediction, imputation, or spatial-temporal recovery. Considering the unique formulation and constraint of each task, most existing methods are... | Yi Xu, Yun Fu |  |
| 2652 |  |  [SWEb: A Large Web Dataset for the Scandinavian Languages](https://openreview.net/forum?id=vhPE3PtTgC) |  | 0 | This paper presents the hitherto largest pretraining dataset for the Scandinavian languages: the Scandinavian WEb (SWEb), comprising over one trillion tokens. The paper details the collection and processing pipeline, and introduces a novel model-based text extractor that significantly reduces... | Amaru Cuba Gyllensten, Ariel Ekgren, Danila Petrelli, Magnus Sahlgren, Paul Gabriel dos Santos, Tim Isbister, Tobias Norlund |  |
| 2653 |  |  [RTDiff: Reverse Trajectory Synthesis via Diffusion for Offline Reinforcement Learning](https://openreview.net/forum?id=0FK6tzqV76) |  | 0 | In offline reinforcement learning (RL), managing the distribution shift between the learned policy and the static offline dataset is a persistent challenge that can result in overestimated values and suboptimal policies. Traditional offline RL methods address this by introducing conservative biases... | Qianlan Yang, YuXiong Wang |  |
| 2654 |  |  [Generalization, Expressivity, and Universality of Graph Neural Networks on Attributed Graphs](https://openreview.net/forum?id=qKgd7RaAem) |  | 0 | We analyze the universality and generalization of graph neural networks (GNNs) on attributed graphs, i.e., with node attributes. To this end, we propose pseudometrics over the space of all attributed graphs that describe the fine-grained expressivity of GNNs. Namely, GNNs are both Lipschitz... | Levi Rauchwerger, Ron Levie, Stefanie Jegelka |  |
| 2655 |  |  [I-Con: A Unifying Framework for Representation Learning](https://openreview.net/forum?id=WfaQrKCr4X) |  | 0 | As the field of representation learning grows, there has been a proliferation of different loss functions to solve different classes of problems. We introduce a single information-theoretic equation that generalizes a large collection of mod- ern loss functions in machine learning. In particular,... | Axel Feldmann, John R. Hershey, Mark Hamilton, Shaden Naif Alshammari, William T. Freeman |  |
| 2656 |  |  [Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG](https://openreview.net/forum?id=oU3tpaR8fm) |  | 0 | Retrieval-augmented generation (RAG) empowers large language models (LLMs) to utilize external knowledge sources. The increasing capacity of LLMs to process longer input sequences opens up avenues for providing more retrieved information, to potentially enhance the quality of generated outputs.... | Bowen Jin, Jiawei Han, Jinsung Yoon, Sercan Ö. Arik |  |
| 2657 |  |  [Local convergence of simultaneous min-max algorithms to differential equilibrium on Riemannian manifold](https://openreview.net/forum?id=ROYSNn3vvB) |  | 0 | We study min-max algorithms to solve zero-sum differential games on Riemannian manifold. Based on the notions of differential Stackelberg equilibrium and differential Nash equilibrium on Riemannian manifold, we analyze the local convergence of two representative deterministic simultaneous... | Sixin Zhang |  |
| 2658 |  |  [Explanations of GNN on Evolving Graphs via Axiomatic Layer edges](https://openreview.net/forum?id=pXN8T5RwNN) |  | 0 | Graphs are ubiquitous in social networks, chemical molecules, and financial data, where Graph Neural Networks (GNNs) achieve superior predictive accuracy. Graphs can be evolving, while understanding how GNN predictions respond to the evolution provides significant insight and trust. We explore the... | Sihong Xie, Yazheng Liu |  |
| 2659 |  |  [Deep Random Features for Scalable Interpolation of Spatiotemporal Data](https://openreview.net/forum?id=OD1MV7vf41) |  | 0 | The rapid growth of earth observation systems calls for a scalable approach to interpolate remote-sensing observations. These methods in principle, should acquire more information about the observed field as data grows. Gaussian processes (GPs) are candidate model choices for interpolation.... | Azhir Mahmood, Michel Tsamados, So Takao, Weibin Chen |  |
| 2660 |  |  [MarS: a Financial Market Simulation Engine Powered by Generative Foundation Model](https://openreview.net/forum?id=Yqk7EyT52H) |  | 0 | Generative models aim to simulate realistic effects of various actions across different contexts, from text generation to visual effects. Despite significant efforts to build real-world simulators, the application of generative models to virtual worlds, like financial markets, remains... | Chang Xu, Jiang Bian, Junjie Li, Lewen Wang, Shikai Fang, Weiqing Liu, Yang Liu |  |
| 2661 |  |  [Reading Your Heart: Learning ECG Words and Sentences via Pre-training ECG Language Model](https://openreview.net/forum?id=6Hz1Ko087B) |  | 0 | Electrocardiogram (ECG) is essential for the clinical diagnosis of arrhythmias and other heart diseases, but deep learning methods based on ECG often face limitations due to the need for high-quality annotations. Although previous ECG self-supervised learning (eSSL) methods have made significant... | Haoyu Wang, Hongyan Li, Jiahui Pan, Jiarui Jin, Jun Li, Shenda Hong |  |
| 2662 |  |  [A New Perspective on Shampoo's Preconditioner](https://openreview.net/forum?id=c6zI3Cp8c6) |  | 0 | Shampoo, a second-order optimization algorithm that uses a Kronecker product preconditioner, has recently received increasing attention from the machine learning community. Despite the increasing popularity of Shampoo, the theoretical foundations of its effectiveness are not well understood. The... | Depen Morwani, Eran Malach, Itai Shapira, Lucas Janson, Nikhil Vyas, Sham M. Kakade |  |
| 2663 |  |  [Text2PDE: Latent Diffusion Models for Accessible Physics Simulation](https://openreview.net/forum?id=Nb3a8aUGfj) |  | 0 | Recent advances in deep learning have inspired numerous works on data-driven solutions to partial differential equation (PDE) problems. These neural PDE solvers can often be much faster than their numerical counterparts; however, each presents its unique limitations and generally balances training... | Amir Barati Farimani, Anthony Y. Zhou, John R. Buchanan Jr., Michael Schneier, Zijie Li |  |
| 2664 |  |  [Adversarial Training Can Provably Improve Robustness: Theoretical Analysis of Feature Learning Process Under Structured Data](https://openreview.net/forum?id=inLUnCpDIB) |  | 0 | Adversarial training is a widely-applied approach to training deep neural networks to be robust against adversarial perturbation. However, although adversarial training has achieved empirical success in practice, it still remains unclear why adversarial examples exist and how adversarial training... | Binghui Li, Yuanzhi Li |  |
| 2665 |  |  [Unleashing the Potential of Vision-Language Pre-Training for 3D Zero-Shot Lesion Segmentation via Mask-Attribute Alignment](https://openreview.net/forum?id=QG31By6S6w) |  | 0 | Recent advancements in medical vision-language pre-training models have driven significant progress in zero-shot disease recognition. However, transferring image-level knowledge to pixel-level tasks, such as lesion segmentation in 3D CT scans, remains a critical challenge. Due to the complexity and... | Shaoting Zhang, Wenhui Lei, Xiaofan Zhang, Yankai Jiang |  |
| 2666 |  |  [KLay: Accelerating Arithmetic Circuits for Neurosymbolic AI](https://openreview.net/forum?id=Zes7Wyif8G) |  | 0 | A popular approach to neurosymbolic AI involves mapping logic formulas to arithmetic circuits (computation graphs consisting of sums and products) and passing the outputs of a neural network through these circuits. This approach enforces symbolic constraints onto a neural network in a principled... | Jaron Maene, Pedro Zuidberg Dos Martires, Vincent Derkinderen |  |
| 2667 |  |  [Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs](https://openreview.net/forum?id=NS1G1Uhny3) |  | 0 | Recent efforts in fine-tuning language models often rely on automatic data selection, commonly using Nearest Neighbors retrieval from large datasets. However, we theoretically show that this approach tends to select redundant data, limiting its effectiveness or even hurting performance. To address... | Andreas Krause, Ido Hakimi, Jonas Hübotter, Sascha Bongni |  |
| 2668 |  |  [Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification](https://openreview.net/forum?id=hzVpZDrW73) |  | 0 | Multimodal Large Language Models (MLLMs) have achieved remarkable success in vision understanding, reasoning, and interaction. However, the inference computation and memory increase progressively with the generation of output tokens during decoding, directly affecting the efficacy of MLLMs.... | Fei Zhao, Shaohui Lin, Shaosheng Cao, Wenxuan Huang, Xiangfeng Xu, Yunhang Shen, Zheyu Ye, Zijie Zhai |  |
| 2669 |  |  [REBIND: Enhancing Ground-state Molecular Conformation Prediction via Force-Based Graph Rewiring](https://openreview.net/forum?id=WNIEr5kydF) |  | 0 | Predicting the ground-state 3D molecular conformations from 2D molecular graphs is critical in computational chemistry due to its profound impact on molecular properties. Deep learning (DL) approaches have recently emerged as promising alternatives to computationally-heavy classical methods such as... | Eunho Yang, Hyunjin Seo, Sungsoo Ahn, Taewon Kim |  |
| 2670 |  |  [Improved Sampling Of Diffusion Models In Fluid Dynamics With Tweedie's Formula](https://openreview.net/forum?id=0FbzC7B9xI) |  | 0 | State-of-the-art Denoising Diffusion Probabilistic Models (DDPMs) rely on an expensive sampling process with a large Number of Function Evaluations (NFEs) to provide high-fidelity predictions. This computational bottleneck renders diffusion models less appealing as surrogates for the... | Benjamin J. Holzschuh, Nils Thuerey, Youssef Shehata |  |
| 2671 |  |  [CATCH: Channel-Aware Multivariate Time Series Anomaly Detection via Frequency Patching](https://openreview.net/forum?id=m08aK3xxdJ) |  | 0 | Anomaly detection in multivariate time series is challenging as heterogeneous subsequence anomalies may occur. Reconstruction-based methods, which focus on learning normal patterns in the frequency domain to detect diverse abnormal subsequences, achieve promising results, while still falling short... | Bin Yang, Chenjuan Guo, Hui Xiong, Jilin Hu, Xiangfei Qiu, Xingjian Wu, Yihang Wang, Zhengyu Li |  |
| 2672 |  |  [Swift Hydra: Self-Reinforcing Generative Framework for Anomaly Detection with Multiple Mamba Models](https://openreview.net/forum?id=P7t2niLbvw) |  | 0 | Despite a plethora of anomaly detection models developed over the years, their ability to generalize to unseen anomalies remains an issue, particularly in critical systems. This paper aims to address this challenge by introducing Swift Hydra, a new framework for training an anomaly detection method... | Jung Taek Seo, Malik Hassanaly, My T. Thai, Nguyen Hoang Khoi Do, Raed Alharbi, Truc Nguyen |  |
| 2673 |  |  [Select before Act: Spatially Decoupled Action Repetition for Continuous Control](https://openreview.net/forum?id=PDgZ3rvqHn) |  | 0 | Reinforcement Learning (RL) has achieved remarkable success in various continuous control tasks, such as robot manipulation and locomotion. Different to mainstream RL which makes decisions at individual steps, recent studies have incorporated action repetition into RL, achieving enhanced action... | Buqing Nie, Yangqing Fu, Yue Gao |  |
| 2674 |  |  [Adversaries With Incentives: A Strategic Alternative to Adversarial Robustness](https://openreview.net/forum?id=U9j40EohfY) |  | 0 | Adversarial training aims to defend against \*adversaries\*: malicious opponents whose sole aim is to harm predictive performance in any way possible. This presents a rather harsh perspective, which we assert results in unnecessarily conservative training. As an alternative, we propose to model... | Maayan Ehrenberg, Nir Rosenfeld, Roy Ganz |  |
| 2675 |  |  [BOND: Aligning LLMs with Best-of-N Distillation](https://openreview.net/forum?id=0tAXMiSufG) |  | 0 | Reinforcement learning from human feedback (RLHF) is a key driver of quality and safety in state-of-the-art large language models. Yet, a surprisingly simple and strong inference-time strategy is Best-of-N sampling that selects the best generation among N candidates. In this paper, we propose... | Abram L. Friesen, Alexandre Ramé, Aliaksei Severyn, Amélie Héliou, Andrea Michi, Bobak Shahriari, Danila Sinopalnikov, Geoffrey Cideron, Johan Ferret, Léonard Hussenot, Matthew Hoffman, Nikola Momchev, Nino Vieillard, Olivier Bachem, Pier Giuseppe Sessa, Piotr Stanczyk, Robert DadashiTazehozi, Sabela Ramos Garea, Sarah Perrin, Sertan Girgin |  |
| 2676 |  |  [Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures](https://openreview.net/forum?id=2J18i8T0oI) |  | 0 | The hypothesis of \textit{Universality} in interpretability suggests that different neural networks may converge to implement similar algorithms on similar tasks. In this work, we investigate two mainstream architectures for language modeling, namely Transformers and Mambas, to explore the extent... | Junxuan Wang, Qiong Tang, Wentao Shu, Xipeng Qiu, Xuyang Ge, Yunhua Zhou, Zhengfu He |  |
| 2677 |  |  [Grammar Reinforcement Learning: path and cycle counting in graphs with a Context-Free Grammar and Transformer approach](https://openreview.net/forum?id=yEox25xAED) |  | 0 | This paper presents Grammar Reinforcement Learning (GRL), a reinforcement learning algorithm that uses Monte Carlo Tree Search (MCTS) and a transformer architecture that models a Pushdown Automaton (PDA) within a context-free grammar (CFG) framework. Taking as use case the problem of efficiently... | Jason Piquenot, JeanYves Ramel, Maxime Berar, Pierre Héroux, Romain Raveaux, Sébastien Adam |  |
| 2678 |  |  [Dist Loss: Enhancing Regression in Few-Shot Region through Distribution Distance Constraint](https://openreview.net/forum?id=YeSxbRrDRl) |  | 0 | Imbalanced data distributions are prevalent in real-world scenarios, presenting significant challenges in both classification and regression tasks. This imbalance often causes deep learning models to overfit in regions with abundant data (manyshot regions) while underperforming in regions with... | Gongzheng Tang, Guangkun Nie, Shenda Hong |  |
| 2679 |  |  [Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs](https://openreview.net/forum?id=f9w89OY2cp) |  | 0 | How to align large language models (LLMs) with user preferences from a static general dataset has been frequently studied. However, user preferences are usually personalized, changing, and diverse. This leads to the problem that the actual user preferences often do not coincide with those trained... | Chengdong Ma, Fengshuo Bai, Haoran Sun, Mingzhi Wang, Qizhi Chen, Yaodong Yang, Zhaowei Zhang, Zilong Zheng |  |
| 2680 |  |  [Revisiting a Design Choice in Gradient Temporal Difference Learning](https://openreview.net/forum?id=38BBWrXUhP) |  | 0 | Off-policy learning enables a reinforcement learning (RL) agent to reason counterfactually about policies that are not executed and is one of the most important ideas in RL. It, however, can lead to instability when combined with function approximation and bootstrapping, two arguably indispensable... | Shangtong Zhang, Xiaochi Qian |  |
| 2681 |  |  [Pairwise Elimination with Instance-Dependent Guarantees for Bandits with Cost Subsidy](https://openreview.net/forum?id=eB7T1bqthA) |  | 0 | Multi-armed bandits (MAB) are commonly used in sequential online decision-making when the reward of each decision is an unknown random variable. In practice however, the typical goal of maximizing total reward may be less important than minimizing the total cost of the decisions taken, subject to a... | Carlee JoeWong, Ishank Juneja, Osman Yagan |  |
| 2682 |  |  [A Quantum Circuit-Based Compression Perspective for Parameter-Efficient Learning](https://openreview.net/forum?id=bB0OKNpznp) |  | 0 | Quantum-centric supercomputing presents a compelling framework for large-scale hybrid quantum-classical tasks. Although quantum machine learning (QML) offers theoretical benefits in various applications, challenges such as large-size data encoding in the input stage and the reliance on quantum... | ChaoHan Huck Yang, ChenYu Liu, HsiSheng Goan, MinHsiu Hsieh |  |
| 2683 |  |  [Leave-One-Out Stable Conformal Prediction](https://openreview.net/forum?id=Bt1vnCnAVS) |  | 0 | Conformal prediction (CP) is an important tool for distribution-free predictive uncertainty quantification. Yet, a major challenge is to balance computational efficiency and prediction accuracy, particularly for multiple predictions. We propose \*\*L\*\*eave-\*\*O\*\*ne-\*\*O\*\*ut \*\*Stab\*\*le... | Kiljae Lee, Yuan Zhang |  |
| 2684 |  |  [Encryption-Friendly LLM Architecture](https://openreview.net/forum?id=pbre0HKsfE) |  | 0 | Large language models (LLMs) offer personalized responses based on user interactions, but this use case raises serious privacy concerns. Homomorphic encryption (HE) is a cryptographic protocol supporting arithmetic computations in encrypted states and provides a potential solution for... | Donghwan Rho, Ernest K. Ryu, Hyunsik Chae, Jung Hee Cheon, Jung Woo Kim, Minje Park, Taeseong Kim |  |
| 2685 |  |  [Improved Training Technique for Latent Consistency Models](https://openreview.net/forum?id=PQjZes6vFV) |  | 0 | Consistency models are a new family of generative models capable of producing high-quality samples in either a single step or multiple steps. Recently, consistency models have demonstrated impressive performance, achieving results on par with diffusion models in the pixel space. However, the... | Di Liu, Dimitris N. Metaxas, Khanh Doan, Quan Dao, Trung Le |  |
| 2686 |  |  [On the Crucial Role of Initialization for Matrix Factorization](https://openreview.net/forum?id=YTEwJaBdh0) |  | 0 | This work revisits the classical low-rank matrix factorization problem and unveils the critical role of initialization in shaping convergence rates for such nonconvex and nonsmooth optimization. We introduce Nystrom initialization, which significantly improves the global convergence of Scaled... | Aryan Mokhtari, Bingcong Li, Liang Zhang, Niao He |  |
| 2687 |  |  [Boosting Latent Diffusion with Perceptual Objectives](https://openreview.net/forum?id=y4DtzADzd1) |  | 0 | Latent diffusion models (LDMs) power state-of-the-art high-resolution generative image models. LDMs learn the data distribution in the latent space of an autoencoder (AE) and produce images by mapping the generated latents into RGB image space using the AE decoder. While this approach allows for... | Adriana RomeroSoriano, Jakob Verbeek, Karteek Alahari, Marton Havasi, Melissa Hall, Michal Drozdzal, Pietro Astolfi, Tariq Berrada, Yohann Benchetrit |  |
| 2688 |  |  [Dynamic Negative Guidance of Diffusion Models](https://openreview.net/forum?id=6p74UyAdLa) |  | 0 | Negative Prompting (NP) is widely utilized in diffusion models, particularly in text-to-image applications, to prevent the generation of undesired features. In this paper, we show that conventional NP is limited by the assumption of a constant guidance scale, which may lead to highly suboptimal... | Felix Koulischer, Gabriel Raya, Johannes Deleu, Luca Ambrogioni, Thomas Demeester |  |
| 2689 |  |  [Solving Differential Equations with Constrained Learning](https://openreview.net/forum?id=5KqveQdXiZ) |  | 0 | (Partial) differential equations (PDEs) are fundamental tools for describing natural phenomena, making their solution crucial in science and engineering. While traditional methods, such as the finite element method, provide reliable solutions, their accuracy is often tied to the use of... | Luiz F. O. Chamon, Viggo Moro |  |
| 2690 |  |  [ClassDiffusion: More Aligned Personalization Tuning with Explicit Class Guidance](https://openreview.net/forum?id=iTm4H6N4aG) |  | 0 | Recent text-to-image customization works have proven successful in generating images of given concepts by fine-tuning diffusion models on a few examples. However, tuning-based methods inherently tend to overfit the concepts, resulting in failure to create the concept under multiple conditions... | Hanshu Yan, Humphrey Shi, Jiannan Huang, Jun Hao Liew, Yao Zhao, Yunchao Wei, Yuyang Yin |  |
| 2691 |  |  [AtomSurf: Surface Representation for Learning on Protein Structures](https://openreview.net/forum?id=ARQIJXFcTH) |  | 0 | While there has been significant progress in evaluating and comparing different representations for learning on protein data, the role of surface-based learning approaches remains not well-understood. In particular, there is a lack of direct and fair benchmark comparison between the best available... | Bruno Correia, Maks Ovsjanikov, Souhaib Attaiki, Vincent Mallet, Yangyang Miao |  |
| 2692 |  |  [Adversarial Training for Defense Against Label Poisoning Attacks](https://openreview.net/forum?id=UlpkHciYQP) |  | 0 | As machine learning models grow in complexity and increasingly rely on publicly sourced data, such as the human-annotated labels used in training large language models, they become more vulnerable to label poisoning attacks. These attacks, in which adversaries subtly alter the labels within a... | Melis Ilayda Bal, Michael Muehlebach, Volkan Cevher |  |
| 2693 |  |  [PersonalLLM: Tailoring LLMs to Individual Preferences](https://openreview.net/forum?id=2R7498e2Tx) |  | 0 | As LLMs become capable of complex tasks, there is growing potential for personalized interactions tailored to the subtle and idiosyncratic preferences of the user. We present a public benchmark, PersonalLLM, focusing on adapting LLMs to provide maximal benefits for a particular user. Departing from... | Andrew Wei Tung Siah, Ang Li, Hongseok Namkoong, Naimeng Ye, Thomas P. Zollo |  |
| 2694 |  |  [Streamlining Prediction in Bayesian Deep Learning](https://openreview.net/forum?id=pW387D5OUN) |  | 0 | The rising interest in Bayesian deep learning (BDL) has led to a plethora of methods for estimating the posterior distribution. However, efficient computation of inferences, such as predictions, has been largely overlooked with Monte Carlo integration remaining the standard. In this work we examine... | Arno Solin, Marcus Klasson, Martin Trapp, Rui Li |  |
| 2695 |  |  [UniCO: On Unified Combinatorial Optimization via Problem Reduction to Matrix-Encoded General TSP](https://openreview.net/forum?id=yEwakMNIex) |  | 0 | Various neural solvers have been devised for combinatorial optimization (CO), which are often tailored for specific problem types, e.g., TSP, CVRP and SAT, etc. Yet, it remains an open question how to achieve universality regarding problem representing and learning with a general framework. This... | Hao Xiong, Jiale Ma, Junchi Yan, Wentao Zhao, Wenzheng Pan, Yang Li |  |
| 2696 |  |  [Learning to Communicate Through Implicit Communication Channels](https://openreview.net/forum?id=wm5wwAdiEt) |  | 0 | Effective communication is an essential component in collaborative multi-agent systems. Situations where explicit messaging is not feasible have been common in human society throughout history, which motivate the study of implicit communication. Previous works on learning implicit communication... | Baoxiang Wang, Binbin Chen, Han Wang, Tieying Zhang |  |
| 2697 |  |  [Mask in the Mirror: Implicit Sparsification](https://openreview.net/forum?id=U47ymTS3ut) |  | 0 | Continuous sparsification strategies are among the most effective methods for reducing the inference costs and memory demands of large-scale neural networks. A key factor in their success is the implicit $L_1$ regularization induced by jointly learning both mask and weight variables, which has been... | Rebekka Burkholz, Tom Jacobs |  |
| 2698 |  |  [Shh, don't say that! Domain Certification in LLMs](https://openreview.net/forum?id=F64wTvQBum) |  | 0 | Large language models (LLMs) are often deployed to do constrained tasks, with narrow domains. For example, customer support bots can be built on top of LLMs, relying on their broad language understanding and capabilities to enhance performance. However, these LLMs are adversarially susceptible,... | Adel Bibi, Alasdair Paren, Cornelius Emde, Maxime Guillaume Kayser, Philip Torr, Preetham Arvind, Thomas Lukasiewicz, Tom Rainforth |  |
| 2699 |  |  [Systematic Outliers in Large Language Models](https://openreview.net/forum?id=rLX7Vyyzus) |  | 0 | Outliers have been widely observed in Large Language Models (LLMs), significantly impacting model performance and posing challenges for model compression. Understanding the functionality and formation mechanisms of these outliers is critically important. Existing works, however, largely focus on... | Jinqiao Wang, Ming Tang, Tao Yu, Xu Zhao, Yongqi An |  |
| 2700 |  |  [Implicit Search via Discrete Diffusion: A Study on Chess](https://openreview.net/forum?id=A9y3LFX4ds) |  | 0 | In the post-AlphaGo era, there has been a renewed interest in search techniques such as Monte Carlo Tree Search (MCTS), particularly in their application to Large Language Models (LLMs). This renewed attention is driven by the recognition that current next-token prediction models often lack the... | Jiacheng Ye, Jiahui Gao, Lingpeng Kong, Xin Jiang, Zhenguo Li, Zhenyu Wu, Zhiyong Wu |  |
| 2701 |  |  [Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning](https://openreview.net/forum?id=NRYgUzSPZz) |  | 0 | Autoregressive language models, despite their impressive capabilities, struggle with complex reasoning and long-term planning tasks. We introduce discrete diffusion models as a novel solution to these challenges. Through the lens of subgoal imbalance, we demonstrate how diffusion models effectively... | Jiacheng Ye, Jiahui Gao, Lin Zheng, Lingpeng Kong, Shansan Gong, Xin Jiang, Zhenguo Li |  |
| 2702 |  |  [RGB-Event ISP: The Dataset and Benchmark](https://openreview.net/forum?id=BqtoARyz7Y) |  | 0 | Event-guided imaging has received significant attention due to its potential to revolutionize instant imaging systems. However, the prior methods primarily focus on enhancing RGB images in a post-processing manner, neglecting the challenges of image signal processor (ISP) dealing with event sensor... | Hui Xiong, Junren Xiao, Liming Chen, Yanlin Qian, Yunfan Lu, Ziyang Rao |  |
| 2703 |  |  [Diff-Prompt: Diffusion-Driven Prompt Generator with Mask Supervision](https://openreview.net/forum?id=LfghnrSJNg) |  | 0 | Prompt learning has demonstrated promising results in fine-tuning pre-trained multimodal models. However, the performance improvement is limited when applied to more complex and fine-grained tasks. The reason is that most existing methods directly optimize the parameters involved in the prompt... | Fangming Feng, Tao Jin, Wang Lin, Weicai Yan, Xiaoda Yang, Ye Wang, Zehan Wang, Zirun Guo |  |
| 2704 |  |  [Sparse Autoencoders Reveal Temporal Difference Learning in Large Language Models](https://openreview.net/forum?id=2tIyA5cri8) |  | 0 | In-context learning, the ability to adapt based on a few examples in the input prompt, is a ubiquitous feature of large language models (LLMs). However, as LLMs' in-context learning abilities continue to improve, understanding this phenomenon mechanistically becomes increasingly important. In... | Akshay Kumar Jagadish, Can Demircan, Eric Schulz, Marcel Binz, Tankred Saanum |  |
| 2705 |  |  [FairDen: Fair Density-Based Clustering](https://openreview.net/forum?id=aPHHhnZktB) |  | 0 | Fairness in data mining tasks like clustering has recently become an increasingly important aspect. However, few clustering algorithms exist that focus on fair groupings of data with sensitive attributes. Including fairness in the clustering objective is especially hard for density-based... | Anna Beer, Anneka Myrup Thiesson, Ira Assent, Lena Krieger, Pernille Matthews |  |
| 2706 |  |  [Trajectory attention for fine-grained video motion control](https://openreview.net/forum?id=2z1HT5lw5M) |  | 0 | Recent advancements in video generation have been greatly driven by video diffusion models, with camera motion control emerging as a crucial challenge in creating view-customized visual content. This paper introduces trajectory attention, a novel approach that performs attention along available... | Jianlou Si, Lei Yang, Shuai Yang, Wenqi Ouyang, Xingang Pan, Yifan Zhou, Zeqi Xiao |  |
| 2707 |  |  [Revolutionizing EMCCD Denoising through a Novel Physics-Based Learning Framework for Noise Modeling](https://openreview.net/forum?id=vmulbBDCan) |  | 0 | Electron-multiplying charge-coupled device (EMCCD) has been instrumental in sensitive observations under low-light situations including astronomy, material science, and biology. Despite its ingenious designs to enhance target signals overcoming read-out circuit noises, produced images are not... | Haiyang Jiang, Imari Sato, Takeharu Nagai, Tetsuichi Wazawa, Yinqiang Zheng |  |
| 2708 |  |  [SAM-CP: Marrying SAM with Composable Prompts for Versatile Segmentation](https://openreview.net/forum?id=UiEjzBRYeI) |  | 0 | The Segment Anything model (SAM) has shown a generalized ability to group image pixels into patches, but applying it to semantic-aware segmentation still faces major challenges. This paper presents SAM-CP, a simple approach that establishes two types of composable prompts beyond SAM and composes... | Lingxi Xie, Pengfei Chen, Qi Tian, Xiaopeng Zhang, Xinyue Huo, Xuehui Yu, Yingfei Sun, Zhenjun Han |  |
| 2709 |  |  [Proactive Agent: Shifting LLM Agents from Reactive Responses to Active Assistance](https://openreview.net/forum?id=sRIU6k2TcU) |  | 0 | Agents powered by large language models have shown remarkable abilities in solving complex tasks. However, most agent systems remain reactive, limiting their effectiveness in scenarios requiring foresight and autonomous decision-making. In this paper, we tackle the challenge of developing proactive... | Cheng Qian, Fangming Liu, Guirong Chen, Huadong Wang, Maosong Sun, Qinyu Luo, Shenzhi Yang, Weiwen Liu, Xin Cong, Yankai Lin, Yasheng Wang, Yaxi Lu, Yesai Wu, Zhiyuan Liu, Zhong Zhang |  |
| 2710 |  |  [Can Generative AI Solve Your In-Context Learning Problem? A Martingale Perspective](https://openreview.net/forum?id=bcynT7s2du) |  | 0 | This work is about estimating when a conditional generative model (CGM) can solve an in-context learning (ICL) problem. An in-context learning (ICL) problem comprises a CGM, a dataset, and a prediction task. The CGM could be a multi-modal foundation model; the dataset, a collection of patient... | Andrew Jesson, David M. Blei, Nicolas BeltranVelez |  |
| 2711 |  |  [Conditional Testing based on Localized Conformal p-values](https://openreview.net/forum?id=Ip6UwB35uT) |  | 0 | In this paper, we address conditional testing problems through the conformal inference framework. We define the localized conformal $p$-values by inverting prediction intervals and prove their theoretical properties. These defined $p$-values are then applied to several conditional testing problems... | Changliang Zou, Lin Lu, Xiaoyang Wu, Zhaojun Wang |  |
| 2712 |  |  [Text4Seg: Reimagining Image Segmentation as Text Generation](https://openreview.net/forum?id=vkakKdznFS) |  | 0 | Multimodal Large Language Models (MLLMs) have shown exceptional capabilities in vision-language tasks; however, effectively integrating image segmentation into these models remains a significant challenge. In this paper, we introduce Text4Seg, a novel text-as-mask paradigm that casts image... | Chaofeng Chen, Jiaxing Xu, Litong Feng, Mengcheng Lan, Wayne Zhang, Xinjiang Wang, Yiping Ke, Yue Zhou |  |
| 2713 |  |  [Diffusing to the Top: Boost Graph Neural Networks with Minimal Hyperparameter Tuning](https://openreview.net/forum?id=D756s2YQ6b) |  | 0 | Graph Neural Networks (GNNs) are proficient in graph representation learning and achieve promising performance on versatile tasks such as node classification and link prediction. Usually, a comprehensive hyperparameter tuning is essential for fully unlocking GNN's top performance, especially for... | Andi Han, Dai Shi, Junbin Gao, Lequan Lin, Zhiyong Wang |  |
| 2714 |  |  [Towards Robust Multimodal Open-set Test-time Adaptation via Adaptive Entropy-aware Optimization](https://openreview.net/forum?id=hj323oR3rw) |  | 0 | Test-time adaptation (TTA) has demonstrated significant potential in addressing distribution shifts between training and testing data. Open-set test-time adaptation (OSTTA) aims to adapt a source pre-trained model online to an unlabeled target domain that contains unknown classes. This task becomes... | Eleni N. Chatzi, Hao Dong, Olga Fink |  |
| 2715 |  |  [On the Feature Learning in Diffusion Models](https://openreview.net/forum?id=JjdU6ysnCr) |  | 0 | The predominant success of diffusion models in generative modeling has spurred significant interest in understanding their theoretical foundations. In this work, we propose a feature learning framework aimed at analyzing and comparing the training dynamics of diffusion models with those of... | Andi Han, Difan Zou, Wei Huang, Yuan Cao |  |
| 2716 |  |  [Deep Kernel Relative Test for Machine-generated Text Detection](https://openreview.net/forum?id=z9j7wctoGV) |  | 0 | Recent studies demonstrate that two-sample test can effectively detect machine-generated texts (MGTs) with excellent adaptation ability to texts generated by newer LLMs. However, two-sample test-based detection relies on the assumption that human-written texts (HWTs) must follow the distribution of... | Feng Liu, Jun Yu, Shuhai Zhang, Yiliao Song, Zhen Fang, Zhenqiao Yuan |  |
| 2717 |  |  [LDAdam: Adaptive Optimization from Low-Dimensional Gradient Statistics](https://openreview.net/forum?id=Zkp1GuHerF) |  | 0 | We introduce LDAdam, a memory-efficient optimizer for training large models, that performs adaptive optimization steps within lower dimensional subspaces, while consistently exploring the full parameter space during training. This strategy keeps the optimizer's memory footprint to a fraction of the... | Dan Alistarh, IonutVlad Modoranu, Mher Safaryan, Thomas Robert |  |
| 2718 |  |  [Layerwise Recurrent Router for Mixture-of-Experts](https://openreview.net/forum?id=eWNEqdH0vk) |  | 0 | The scaling of large language models (LLMs) has revolutionized their capabilities in various tasks, yet this growth must be matched with efficient computational strategies. The Mixture-of-Experts (MoE) architecture stands out for its ability to scale model size without significantly increasing... | Ivan Titov, Jie Fu, Shuang Cheng, Yizhi Zhou, Zeyu Huang, Zihan Qiu, Zili Wang |  |
| 2719 |  |  [AutoUAD: Hyper-parameter Optimization for Unsupervised Anomaly Detection](https://openreview.net/forum?id=ErQPdaD5wJ) |  | 0 | Unsupervised anomaly detection (UAD) has important applications in diverse fields such as manufacturing industry and medical diagnosis. In the past decades, although numerous insightful and effective UAD methods have been proposed, it remains a huge challenge to tune the hyper-parameters of each... | Jicong Fan, Wei Dai |  |
| 2720 |  |  [Advancing Mathematical Reasoning in Language Models: The Impact of Problem-Solving Data, Data Synthesis Methods, and Training Stages](https://openreview.net/forum?id=GtpubstM1D) |  | 0 | Mathematical reasoning remains a challenging area for large language models (LLMs), prompting the development of math-specific LLMs such as LLEMMA, DeepSeekMath, and Qwen2-Math, among others. These models typically follow a two-stage training paradigm: pre-training with math-related corpora and... | Mi Tian, Tianqiao Liu, Tongqing, Weiqi Luo, Zitao Liu, Zui Chen |  |
| 2721 |  |  [DocMIA: Document-Level Membership Inference Attacks against DocVQA Models](https://openreview.net/forum?id=gNxvs5pUdu) |  | 0 | Document Visual Question Answering (DocVQA) has introduced a new paradigm for end-to-end document understanding, and quickly became one of the standard benchmarks for multimodal LLMs. Automating document processing workflows, driven by DocVQA models, presents significant potential for many business... | Dimosthenis Karatzas, Khanh Nguyen, Mario Fritz, Raouf Kerkouche |  |
| 2722 |  |  [Transformer-Squared: Self-adaptive LLMs](https://openreview.net/forum?id=dh4t9qmcvK) |  | 0 | Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce Transformer-Squared, a novel self-adaptation framework that adapts LLMs for... | Edoardo Cetin, Qi Sun, Yujin Tang |  |
| 2723 |  |  [Maintaining Structural Integrity in Parameter Spaces for Parameter Efficient Fine-tuning](https://openreview.net/forum?id=OALIb8oNfl) |  | 0 | Adapting pre-trained foundation models for various downstream tasks has been prevalent in artificial intelligence. Due to the vast number of tasks and high costs, adjusting all parameters becomes unfeasible. To mitigate this, several fine-tuning techniques have been developed to update the... | Chongjie Si, Jifeng Dai, Qingyun Li, Wei Shen, Xiaokang Yang, Xue Yang, Xuehui Wang, Yu Qiao, Zhengqin Xu |  |
| 2724 |  |  [Mechanistic Permutability: Match Features Across Layers](https://openreview.net/forum?id=MDvecs7EvO) |  | 0 | Understanding how features evolve across layers in deep neural networks is a fundamental challenge in mechanistic interpretability, particularly due to polysemanticity and feature superposition. While Sparse Autoencoders (SAEs) have been used to extract interpretable features from individual... | Daniil Gavrilov, Ian Maksimov, Nikita Balagansky |  |
| 2725 |  |  [HALL-E: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis](https://openreview.net/forum?id=868masI331) |  | 0 | Recently, Text-to-speech (TTS) models based on large language models (LLMs) that translate natural language text into sequences of discrete audio tokens have gained great research attention, with advances in neural audio codec (NAC) mod- els using residual vector quantization (RVQ). However,... | Hideki Nakayama, Masanari Ohi, Nakamasa Inoue, Takumi Hirose, Yuto Nishimura |  |
| 2726 |  |  [Zeroth-Order Policy Gradient for Reinforcement Learning from Human Feedback without Reward Inference](https://openreview.net/forum?id=cmYScmfu4Q) |  | 0 | Reward inference (learning a reward model from human preferences) is a critical intermediate step in the Reinforcement Learning from Human Feedback (RLHF) pipeline for fine-tuning Large Language Models (LLMs). In practice, RLHF faces fundamental challenges such as distribution shift, reward model... | Lei Ying, Qining Zhang |  |
| 2727 |  |  [Zigzag Diffusion Sampling: Diffusion Models Can Self-Improve via Self-Reflection](https://openreview.net/forum?id=MKvQH1ekeY) |  | 0 | Diffusion models, the most popular generative paradigm so far, can inject conditional information into the generation path to guide the latent towards desired directions. However, existing text-to-image diffusion models often fail to maintain high image quality and high prompt-image alignment for... | Haoyi Xiong, Lichen Bai, Shitong Shao, Zeke Xie, Zhiqiang Xu, Zikai Zhou, Zipeng Qi |  |
| 2728 |  |  [Discrete Latent Plans via Semantic Skill Abstractions](https://openreview.net/forum?id=L66G39JrM4) |  | 0 | Skill learning from language instructions is a critical challenge in developing intelligent agents that can generalize across diverse tasks and follow complex human instructions. Hierarchical methods address this by decomposing the learning problem into multiple levels, where the high-level and... | Haobin Jiang, Jiangxing Wang, Zongqing Lu |  |
| 2729 |  |  [Neuralized Markov Random Field for Interaction-Aware Stochastic Human Trajectory Prediction](https://openreview.net/forum?id=r3cEOVj7Ze) |  | 0 | Interactive human motions and the continuously changing nature of intentions pose significant challenges for human trajectory prediction. In this paper, we present a neuralized Markov random field (MRF)-based motion evolution method for probabilistic interaction-aware human trajectory prediction.... | David Hsu, Gim Hee Lee, Zilin Fang |  |
| 2730 |  |  [A General Framework for Producing Interpretable Semantic Text Embeddings](https://openreview.net/forum?id=23uY3FpQxc) |  | 0 | Semantic text embedding is essential to many tasks in Natural Language Processing (NLP). While black-box models are capable of generating high-quality embeddings, their lack of interpretability limits their use in tasks that demand transparency. Recent approaches have improved interpretability by... | Anthony Kum Hoe Tung, Jun Yu, Qiang Huang, Yiqun Sun, Yixuan Tang |  |
| 2731 |  |  [Unifying Causal Representation Learning with the Invariance Principle](https://openreview.net/forum?id=lk2Qk5xjeu) |  | 0 | Causal representation learning (CRL) aims at recovering latent causal variables from high-dimensional observations to solve causal downstream tasks, such as predicting the effect of new interventions or more robust classification. A plethora of methods have been developed, each tackling carefully... | Dario Rancati, Dingling Yao, Francesco Locatello, Marco Fumero, Riccardo Cadei |  |
| 2732 |  |  [A Multiscale Frequency Domain Causal Framework for Enhanced Pathological Analysis](https://openreview.net/forum?id=6xrDPHhwD3) |  | 0 | Multiple Instance Learning (MIL) in digital pathology Whole Slide Image (WSI) analysis has shown significant progress. However, due to data bias and unobservable confounders, this paradigm still faces challenges in terms of performance and interpretability. Existing MIL methods might identify... | Jiandong Su, Weixing Chen, Xiaoyu Cui |  |
| 2733 |  |  [WeatherGFM: Learning a Weather Generalist Foundation Model via In-context Learning](https://openreview.net/forum?id=izjNI5bcOV) |  | 0 | The Earth's weather system involves intricate weather data modalities and diverse weather understanding tasks, which hold significant value to human life. Existing data-driven models focus on single weather understanding tasks (e.g., weather forecasting). While these models have achieved promising... | Ben Fei, Hao Chen, Junchao Gong, Lei Bai, Shiqi Chen, Wanli Ouyang, Wenlong Zhang, Xiangyu Chen, Xiangyu Zhao, XiaoMing Wu, Yihao Liu, Zhiwang Zhou |  |
| 2734 |  |  [Provable Robust Overfitting Mitigation in Wasserstein Distributionally Robust Optimization](https://openreview.net/forum?id=sq5LLWk5SN) |  | 0 | Wasserstein distributionally robust optimization (WDRO) optimizes against worst-case distributional shifts within a specified uncertainty set, leading to enhanced generalization on unseen adversarial examples, compared to standard adversarial training which focuses on pointwise adversarial... | Shuang Liu, XiaoShan Gao, Yibo Miao, Yifan Zhu, Yihan Wang |  |
| 2735 |  |  [Direct Distributional Optimization for Provable Alignment of Diffusion Models](https://openreview.net/forum?id=Nvw2szDdmI) |  | 0 | We introduce a novel alignment method for diffusion models from distribution optimization perspectives while providing rigorous convergence guarantees. We first formulate the problem as a generic regularized loss minimization over probability distributions and directly optimize the distribution... | Atsushi Nitanda, Kazusato Oko, Ryotaro Kawata, Taiji Suzuki |  |
| 2736 |  |  [MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses](https://openreview.net/forum?id=X9OfMNNepI) |  | 0 | Scientific discovery contributes largely to the prosperity of human society, and recent progress shows that LLMs could potentially catalyst the process. However, it is still unclear whether LLMs can discover novel and valid hypotheses in chemistry. In this work, we investigate this main research... | Ben Gao, Dongzhan Zhou, Erik Cambria, Soujanya Poria, Tong Xie, Wanhao Liu, Wanli Ouyang, Yuqiang Li, Zonglin Yang |  |
| 2737 |  |  [AnyTouch: Learning Unified Static-Dynamic Representation across Multiple Visuo-tactile Sensors](https://openreview.net/forum?id=XToAemis1h) |  | 0 | Visuo-tactile sensors aim to emulate human tactile perception, enabling robots to precisely understand and manipulate objects. Over time, numerous meticulously designed visuo-tactile sensors have been integrated into robotic systems, aiding in completing various tasks. However, the distinct data... | Ao Shen, Bin Fang, Di Hu, Jiangyu Hu, Ruoxuan Feng, Tianci Gao, Wenke Xia, Yuhao Sun |  |
| 2738 |  |  [Optimal Flow Transport and its Entropic Regularization: a GPU-friendly Matrix Iterative Algorithm for Flow Balance Satisfaction](https://openreview.net/forum?id=NtSlKEJ2DS) |  | 0 | The Sinkhorn algorithm, based on Entropic Regularized Optimal Transport (OT), has garnered significant attention due to its computational efficiency enabled by GPU-friendly matrix-vector multiplications. However, vanilla OT primarily deals with computations between the source and target nodes in a... | Junchi Yan, Kaipeng Zeng, Liangliang Shi, Yihui Tu, Yufeng Li |  |
| 2739 |  |  [Exploring The Forgetting in Adversarial Training: A Novel Method for Enhancing Robustness](https://openreview.net/forum?id=fjPOt8QlqQ) |  | 0 | In recent years, there has been an explosion of research into developing robust deep neural networks against adversarial examples. As one of the most successful methods, Adversarial Training (AT) has been widely studied before, but there is still a gap to achieve promising clean and robust accuracy... | Hu Ding, Xianglu Wang |  |
| 2740 |  |  [From Decoupling to Adaptive Transformation: a Wider Optimization Space for PTQ](https://openreview.net/forum?id=JElN0LJMKB) |  | 0 | Post-Training low-bit Quantization (PTQ) is useful to accelerate DNNs due to its high efficiency, the current SOTAs of which mostly adopt feature reconstruction with self-distillation finetuning. However, when bitwidth goes to be extremely low, we find the current reconstruction optimization space... | Di Xie, Jiang Zhu, Qiulin Zhang, Rudan Chen, Xichao Yang, Yuan Zhang, Zhaojing Wen |  |
| 2741 |  |  [SaLoRA: Safety-Alignment Preserved Low-Rank Adaptation](https://openreview.net/forum?id=GOoVzE9nSj) |  | 0 | As advancements in large language models (LLMs) continue and the demand for personalized models increases, parameter-efficient fine-tuning (PEFT) methods (e.g., LoRA) become essential due to their efficiency in reducing computation costs. However, recent studies have raised alarming concerns that... | Michael Backes, Mingjie Li, Wai Man Si, Yang Zhang, Yisen Wang |  |
| 2742 |  |  [Unsupervised Meta-Learning via In-Context Learning](https://openreview.net/forum?id=Jprs1v2wPA) |  | 0 | Unsupervised meta-learning aims to learn feature representations from unsupervised datasets that can transfer to downstream tasks with limited labeled data. In this paper, we propose a novel approach to unsupervised meta-learning that leverages the generalization abilities of in-context learning... | Anna Vettoruzzo, Joaquin Vanschoren, Lorenzo Braccaioli, Marlena Nowaczyk |  |
| 2743 |  |  [Enhancing Prediction Performance through Influence Measure](https://openreview.net/forum?id=KjBG4JNOc2) |  | 0 | In the field of machine learning, the pursuit of accurate models is ongoing. A key aspect of improving prediction performance lies in identifying which data points in the training set should be excluded and which high-quality, potentially unlabeled data points outside the training set should be... | Fan Zhou, Hongtu Zhu, Shuguang Yu, Wenqian Xu, Xinyi Zhou, Xuechun Wang |  |
| 2744 |  |  [Poisson-Dirac Neural Networks for Modeling Coupled Dynamical Systems across Domains](https://openreview.net/forum?id=U1DjXQeJRx) |  | 0 | Deep learning has achieved great success in modeling dynamical systems, providing data-driven simulators to predict complex phenomena, even without known governing equations. However, existing models have two major limitations: their narrow focus on mechanical systems and their tendency to treat... | Hiroaki Yoshimura, Razmik Arman Khosrovian, Takaharu Yaguchi, Takashi Matsubara |  |
| 2745 |  |  [Show-o: One Single Transformer to Unify Multimodal Understanding and Generation](https://openreview.net/forum?id=o6Ynz6OIQ6) |  | 0 | We present a unified transformer, i.e., Show-o, that unifies multimodal understanding and generation. Unlike fully autoregressive models, Show-o unifies autoregressive and (discrete) diffusion modeling to adaptively handle inputs and outputs of various and mixed modalities. The unified model... | David Junhao Zhang, Jinheng Xie, Kevin Qinghong Lin, Mike Zheng Shou, Weihao Wang, Weijia Mao, Yuchao Gu, Zechen Bai, Zhenheng Yang, Zhijie Chen |  |
| 2746 |  |  [Wasserstein-Regularized Conformal Prediction under General Distribution Shift](https://openreview.net/forum?id=aJ3tiX1Tu4) |  | 0 | Conformal prediction yields a prediction set with guaranteed $1-\alpha$ coverage of the true target under the i.i.d. assumption, which can fail and lead to a gap between $1-\alpha$ and the actual coverage. Prior studies bound the gap using total variation distance, which cannot identify the gap... | Chao Chen, Parvathinathan Venkitasubramaniam, Rui Xu, Sihong Xie, Yue Sun |  |
| 2747 |  |  [Motion Control of High-Dimensional Musculoskeletal Systems with Hierarchical Model-Based Planning](https://openreview.net/forum?id=MWHIIWrWWu) |  | 0 | Controlling high-dimensional nonlinear systems, such as those found in biological and robotic applications, is challenging due to large state and action spaces. While deep reinforcement learning has achieved a number of successes in these domains, it is computationally intensive and time consuming,... | Shanning Zhuang, Vincent Zhuang, Yanan Sui, Yunyue Wei |  |
| 2748 |  |  [Multimodal Situational Safety](https://openreview.net/forum?id=I9bEi6LNgt) |  | 0 | Multimodal Large Language Models (MLLMs) are rapidly evolving, demonstrating impressive capabilities as multimodal assistants that interact with both humans and their environments. However, this increased sophistication introduces significant safety concerns. In this paper, we present the first... | Anderson Compalas, Chengzhi Liu, Dawn Song, Kaiwen Zhou, Xin Eric Wang, Xuandong Zhao |  |
| 2749 |  |  [PaCA: Partial Connection Adaptation for Efficient Fine-Tuning](https://openreview.net/forum?id=iYkhxre0In) |  | 0 | Prior parameter-efficient fine-tuning (PEFT) algorithms reduce memory usage and computational costs of fine-tuning large neural network models by training only a few additional adapter parameters, rather than the entire model. However, the reduction in computational costs due to PEFT does not... | Beomseok Kim, Dongsuk Jeon, Inho Jeong, Sol Namkung, Sunghyeon Woo, Sunwoo Lee |  |
| 2750 |  |  [Text-to-Image Rectified Flow as Plug-and-Play Priors](https://openreview.net/forum?id=SzPZK856iI) |  | 0 | Large-scale diffusion models have achieved remarkable performance in generative tasks. Beyond their initial training applications, these models have proven their ability to function as versatile plug-and-play priors. For instance, 2D diffusion models can serve as loss functions to optimize 3D... | Cheng Chen, Fayao Liu, Guosheng Lin, Xiaofeng Yang, Xulei Yang |  |
| 2751 |  |  [Graph Assisted Offline-Online Deep Reinforcement Learning for Dynamic Workflow Scheduling](https://openreview.net/forum?id=4PlbIfmX9o) |  | 0 | Dynamic workflow scheduling (DWS) in cloud computing presents substantial challenges due to heterogeneous machine configurations, unpredictable workflow arrivals/patterns, and constantly evolving environments. However, existing research often assumes homogeneous setups and static conditions,... | Cong Zhang, Gang Chen, Hui Ma, Mengjie Zhang, Yifan Yang, Zhiguang Cao |  |
| 2752 |  |  [MP-Mat: A 3D-and-Instance-Aware Human Matting and Editing Framework with Multiplane Representation](https://openreview.net/forum?id=Xw0fCEMFss) |  | 0 | Human instance matting aims to estimate an alpha matte for each human instance in an image, which is challenging as it easily fails in complex cases requiring disentangling mingled pixels belonging to multiple instances along hairy and thin boundary structures. In this work, we address this by... | Changxin Gao, Huayu Zhang, Mike Zheng Shou, Nong Sang, Siyi Jiao, Wenzheng Zeng, Yerong Li |  |
| 2753 |  |  [Visual-O1: Understanding Ambiguous Instructions via Multi-modal Multi-turn Chain-of-thoughts Reasoning](https://openreview.net/forum?id=v9CDpLpjiE) |  | 0 | As large-scale models evolve, language instructions are increasingly utilized in multi-modal tasks. Due to human language habits, these instructions often contain ambiguities in real-world scenarios, necessitating the integration of visual context or common sense for accurate interpretation.... | Lei Zhang, Minheng Ni, Wangmeng Zuo, Yutao Fan |  |
| 2754 |  |  [TTVD: Towards a Geometric Framework for Test-Time Adaptation Based on Voronoi Diagram](https://openreview.net/forum?id=5sU32OCxgZ) |  | 0 | Deep learning models often struggle with generalization when deploying on real-world data, due to the common distributional shift to the training data. Test-time adaptation (TTA) is an emerging scheme used at inference time to address this issue. In TTA, models are adapted online at the same time... | Chunwei Ma, Jinhui Xu, Meng Ding, Mingxi Lei, Yufan Zhou, Ziyun Huang |  |
| 2755 |  |  [Spurious Forgetting in Continual Learning of Language Models](https://openreview.net/forum?id=ScI7IlKGdI) |  | 0 | Recent advancements in large language models (LLMs) reveal a perplexing phenomenon in continual learning: despite extensive training, models experience significant performance declines, raising questions about task alignment and underlying knowledge retention. This study first explores the concept... | Junhao Zheng, Qianli Ma, Shengjie Qiu, Xidi Cai |  |
| 2756 |  |  [Optimal Non-Asymptotic Rates of Value Iteration for Average-Reward Markov Decision Processes](https://openreview.net/forum?id=WuTczPV8WC) |  | 0 | While there is an extensive body of research on the analysis of Value Iteration (VI) for discounted cumulative-reward MDPs, prior work on analyzing VI for (undiscounted) average-reward MDPs has been limited, and most prior results focus on asymptotic rates in terms of Bellman error. In this work,... | Ernest K. Ryu, Jongmin Lee |  |
| 2757 |  |  [When Graph Neural Networks Meet Dynamic Mode Decomposition](https://openreview.net/forum?id=duGygkA3QR) |  | 0 | Graph Neural Networks (GNNs) have emerged as fundamental tools for a wide range of prediction tasks on graph-structured data. Recent studies have drawn analogies between GNN feature propagation and diffusion processes, which can be interpreted as dynamical systems. In this paper, we delve deeper... | Andi Han, Dai Shi, Junbin Gao, Lequan Lin, Yi Guo, Zhiyong Wang |  |
| 2758 |  |  [HQGS: High-Quality Novel View Synthesis with Gaussian Splatting in Degraded Scenes](https://openreview.net/forum?id=25Zlvl7JxW) |  | 0 | 3D Gaussian Splatting (3DGS) has shown promising results for Novel View Synthesis. However, while it is quite effective when based on high-quality images, its performance declines as image quality degrades, due to lack of resolution, motion blur, noise, compression artifacts, or other factors... | Chao Ren, Lu Qi, MingHsuan Yang, Nuno Vasconcelos, Shi Luo, Xiaojun Shan, Xiaoyu Zhou, Xin Lin |  |
| 2759 |  |  [Cyclic Contrastive Knowledge Transfer for Open-Vocabulary Object Detection](https://openreview.net/forum?id=JU9oHs7ivN) |  | 0 | In pursuit of detecting unstinted objects that extend beyond predefined categories, prior arts of open-vocabulary object detection (OVD) typically resort to pretrained vision-language models (VLMs) for base-to-novel category generalization. However, to mitigate the misalignment between upstream... | Chaoyang Zhu, Chuhan Zhang, Dong Zhang, Long Chen, Pingcheng Dong |  |
| 2760 |  |  [Transformer Meets Twicing: Harnessing Unattended Residual Information](https://openreview.net/forum?id=16kG5aNleS) |  | 0 | Transformer-based deep learning models have achieved state-of-the-art performance across numerous language and vision tasks. While the self-attention mechanism, a core component of transformers, has proven capable of handling complex data patterns, it has been observed that the representational... | Laziz U. Abdullaev, Tan Minh Nguyen |  |
| 2761 |  |  [SmartPretrain: Model-Agnostic and Dataset-Agnostic Representation Learning for Motion Prediction](https://openreview.net/forum?id=Bmzv2Gch9v) |  | 0 | Predicting the future motion of surrounding agents is essential for autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed environments. However, the scarcity of large-scale driving datasets has hindered the development of robust and generalizable motion prediction models,... | Hao Shao, Hongsheng Li, Letian Wang, Steven L. Waslander, Yang Zhou, Yu Liu |  |
| 2762 |  |  [Jump Your Steps: Optimizing Sampling Schedule of Discrete Diffusion Models](https://openreview.net/forum?id=pD6TiCpyDR) |  | 0 | Diffusion models have seen notable success in continuous domains, leading to the development of discrete diffusion models (DDMs) for discrete variables. Despite recent advances, DDMs face the challenge of slow sampling speeds. While parallel sampling methods like $\tau$-leaping accelerate this... | ChiehHsin Lai, Satoshi Hayakawa, YongHyun Park, Yuhta Takida, Yuki Mitsufuji |  |
| 2763 |  |  [Trusted Multi-View Classification via Evolutionary Multi-View Fusion](https://openreview.net/forum?id=M3kBtqpys5) |  | 0 | Multi-view classification based on the Dempster-Shafer theory is widely recognized for its reliability in safety-critical domains with multi-view data. However, the adoption of a late fusion strategy constrains information interaction among views, thereby leading to suboptimal utilization of... | Guoqing Liu, Pinhan Fu, Qian Guo, Xinyan Liang, Yuhua Qian |  |
| 2764 |  |  [3D Vision-Language Gaussian Splatting](https://openreview.net/forum?id=SSE9myD9SG) |  | 0 | Recent advancements in 3D reconstruction methods and vision-language models have propelled the development of multi-modal 3D scene understanding, which has vital applications in robotics, autonomous driving, and virtual/augmented reality. However, current multi-modal scene understanding approaches... | Anwesa Choudhuri, Benjamin Planche, Chen Chen, Meng Zheng, Qucheng Peng, Terrence Chen, Zhongpai Gao, Ziyan Wu |  |
| 2765 |  |  [REFINE: Inversion-Free Backdoor Defense via Model Reprogramming](https://openreview.net/forum?id=4IYdCws9fc) |  | 0 | Backdoor attacks on deep neural networks (DNNs) have emerged as a significant security threat, allowing adversaries to implant hidden malicious behaviors during the model training phase. Pre-processing-based defense, which is one of the most important defense paradigms, typically focuses on input... | Enhao Huang, Kui Ren, PinYu Chen, Shuo Shao, Yiming Li, Yukun Chen, Zhan Qin |  |
| 2766 |  |  [Mind Control through Causal Inference: Predicting Clean Images from Poisoned Data](https://openreview.net/forum?id=ho4mNiwr2n) |  | 0 | Anti-backdoor learning, aiming to train clean models directly from poisoned datasets, serves as an important defense method for backdoor attack. However, existing methods usually fail to recover backdoored samples to their original, correct labels and suffer from poor generalization to large... | Anil Kumar S. Vullikanti, Jielu Zhang, Junfeng Guo, Mengxuan Hu, Ruoxi Jia, Sheng Li, Yi Zeng, Zhongliang Zhou, Zihan Guan |  |
| 2767 |  |  [Segment Any 3D Object with Language](https://openreview.net/forum?id=ENv1CeTwxc) |  | 0 | In this paper, we investigate Open-Vocabulary 3D Instance Segmentation (OV-3DIS) with free-form language instructions. Earlier works mainly rely on annotated base categories for training which leads to limited generalization to unseen novel categories. To mitigate the poor generalizability to novel... | Gim Hee Lee, Seungjun Lee, Yuyang Zhao |  |
| 2768 |  |  [Adaptive Camera Sensor for Vision Models](https://openreview.net/forum?id=He2FGdmsas) |  | 0 | Domain shift remains a persistent challenge in deep-learning-based computer vision, often requiring extensive model modifications or large labeled datasets to address. Inspired by human visual perception, which adjusts input quality through corrective lenses rather than over-training the brain, we... | Eunsu Baek, HyungSin Kim, Sunghwan Han, Taesik Gong |  |
| 2769 |  |  [Swift4D: Adaptive divide-and-conquer Gaussian Splatting for compact and efficient reconstruction of dynamic scene](https://openreview.net/forum?id=c1RhJVTPwT) |  | 0 | Novel view synthesis has long been a practical but challenging task, although the introduction of numerous methods to solve this problem, even combining advanced representations like 3D Gaussian Splatting, they still struggle to recover high-quality results and often consume too much storage memory... | Jiahao Wu, Jinbo Yan, Kaiqiang Xiong, Lu Xiao, Luyang Tang, Ronggang Wang, Rui Peng, Zhiyan Wang |  |
| 2770 |  |  [Rethinking Audio-Visual Adversarial Vulnerability from Temporal and Modality Perspectives](https://openreview.net/forum?id=ePJrZLIqpV) |  | 0 | While audio-visual learning equips models with a richer understanding of the real world by leveraging multiple sensory modalities, this integration also introduces new vulnerabilities to adversarial attacks. In this paper, we present a comprehensive study of the adversarial robustness of... | Chenliang Xu, Daiki Shimada, Susan Liang, Zeliang Zhang |  |
| 2771 |  |  [Large Convolutional Model Tuning via Filter Subspace](https://openreview.net/forum?id=E5YmIBvOqV) |  | 0 | Efficient fine-tuning methods are critical to address the high computational and parameter complexity while adapting large pre-trained models to downstream tasks. Our study is inspired by prior research that represents each convolution filter as a linear combination of a small set of filter... | Qiang Qiu, Wei Chen, Zichen Miao |  |
| 2772 |  |  [TVNet: A Novel Time Series Analysis Method Based on Dynamic Convolution and 3D-Variation](https://openreview.net/forum?id=MZDdTzN6Cy) |  | 0 | With the recent development and advancement of Transformer and MLP architectures, significant strides have been made in time series analysis. Conversely, the performance of Convolutional Neural Networks (CNNs) in time series analysis has fallen short of expectations, diminishing their potential for... | Chenghan Li, Mingchen Li, Ruisheng Diao |  |
| 2773 |  |  [Articulate-Anything: Automatic Modeling of Articulated Objects via a Vision-Language Foundation Model](https://openreview.net/forum?id=s3FTX4Ay55) |  | 0 | Interactive 3D simulated objects are crucial in AR/VR, animations, and robotics, driving immersive experiences and advanced automation. However, creating these articulated objects requires extensive human effort and expertise, limiting their broader applications. To overcome this challenge, we... | Arjun Krishna, Dinesh Jayaraman, Eric Eaton, HungJu Wang, Jason Xie, Kyle Vedder, Long Le, William Liang, Yecheng Jason Ma, Yue Yang |  |
| 2774 |  |  [Investigating Pattern Neurons in Urban Time Series Forecasting](https://openreview.net/forum?id=a9vey6B54y) |  | 0 | Urban time series forecasting is crucial for smart city development and is key to sustainable urban management. Although urban time series models (UTSMs) are effective in general forecasting, they often overlook low-frequency events, such as holidays and extreme weather, leading to degraded... | Chengxin Wang, Gary Tan, Shaofeng Cai, Yiran Zhao |  |
| 2775 |  |  [Tight Clusters Make Specialized Experts](https://openreview.net/forum?id=Pu3c0209cx) |  | 0 | Sparse Mixture-of-Experts (MoE) architectures have emerged as a promising approach to decoupling model capacity from computational cost. At the core of the MoE model is the router, which learns the underlying clustering structure of the input distribution in order to send input tokens to... | Laziz U. Abdullaev, Rachel S. Y. Teo, Stefan K. Nielsen, Tan Minh Nguyen |  |
| 2776 |  |  [Trajectory-LLM: A Language-based Data Generator for Trajectory Prediction in Autonomous Driving](https://openreview.net/forum?id=UapxTvxB3N) |  | 0 | Vehicle trajectory prediction is a crucial aspect of autonomous driving, which requires extensive trajectory data to train prediction models to understand the complex, varied, and unpredictable patterns of vehicular interactions. However, acquiring real-world data is expensive, so we advocate using... | Di Lin, Die Zuo, Gengjie Lin, Haotian Dong, Jibin Peng, Junchi Yan, Kairui Yang, Qing Guo, Xiaosong Jia, Xin Wang, Yipeng Wu, Zhao Huang, Zihao Guo, Ziyuan Zhong |  |
| 2777 |  |  [LASER: A Neuro-Symbolic Framework for Learning Spatio-Temporal Scene Graphs with Weak Supervision](https://openreview.net/forum?id=HEXtydywnE) |  | 0 | Supervised approaches for learning spatio-temporal scene graphs (STSG) from video are greatly hindered due to their reliance on STSG-annotated videos, which are labor-intensive to construct at scale. Is it feasible to instead use readily available video captions as weak supervision? To address this... | Jiani Huang, Mayur Naik, SerNam Lim, Ziyang Li |  |
| 2778 |  |  [Real-Time Video Generation with Pyramid Attention Broadcast](https://openreview.net/forum?id=hDBrQ4DApF) |  | 0 | We present Pyramid Attention Broadcast (PAB), a real-time, high quality and training-free approach for DiT-based video generation. Our method is founded on the observation that attention difference in the diffusion process exhibits a U-shaped pattern, indicating significant redundancy. We mitigate... | Kai Wang, Xiaolong Jin, Xuanlei Zhao, Yang You |  |
| 2779 |  |  [QMP: Q-switch Mixture of Policies for Multi-Task Behavior Sharing](https://openreview.net/forum?id=aUZEeb2yvK) |  | 0 | Multi-task reinforcement learning (MTRL) aims to learn several tasks simultaneously for better sample efficiency than learning them separately. Traditional methods achieve this by sharing parameters or relabeling data between tasks. In this work, we introduce a new framework for sharing behavioral... | Ayush Jain, Grace Zhang, Injune Hwang, Joseph J. Lim, ShaoHua Sun |  |
| 2780 |  |  [Rationalizing and Augmenting Dynamic Graph Neural Networks](https://openreview.net/forum?id=thV5KRQFgQ) |  | 0 | Graph data augmentation (GDA) has shown significant promise in enhancing the performance, generalization, and robustness of graph neural networks (GNNs). However, contemporary methodologies are often limited to static graphs, whose applicability on dynamic graphs—more prevalent in real-world... | Dawei Cheng, Guibin Zhang, Jian Guo, Yanwei Yue, Yiyan Qi, Ziyang Cheng |  |
| 2781 |  |  [Decoding Game: On Minimax Optimality of Heuristic Text Generation Strategies](https://openreview.net/forum?id=Wfw4ypsgRZ) |  | 0 | Decoding strategies play a pivotal role in text generation for modern language models, yet a puzzling gap divides theory and practice. Surprisingly, strategies that should intuitively be optimal, such as Maximum a Posteriori (MAP), often perform poorly in practice. Meanwhile, popular heuristic... | Jason Matthew Klusowski, Omar Hagrass, Sijin Chen |  |
| 2782 |  |  [DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search](https://openreview.net/forum?id=I4YAIwrsXa) |  | 0 | Lean is an advanced proof assistant designed to facilitate formal theorem proving by providing a variety of interactive feedback. In this paper, we explore methodologies to leverage proof assistant feedback to augment the capabilities of large language models in constructing formal proofs. First,... | Bo Liu, Chong Ruan, Dejian Yang, Fuli Luo, Haocheng Wang, Haowei Zhang, Huajian Xin, Junxiao Song, Liyue Zhang, Qihao Zhu, Qiushi Du, Wanjia Zhao, Wenjun Gao, Xuan Lu, Z. F. Wu, Z. Z. Ren, Zhibin Gou, Zhihong Shao |  |
| 2783 |  |  [Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs](https://openreview.net/forum?id=d2H1oTNITn) |  | 0 | Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or nonsensical information) when serving as AI assistants in various domains. Since hallucinations always come with truthful content in the LLM responses, previous factuality alignment methods that conduct response-level... | Chengqi Lyu, Dahua Lin, Kai Chen, Wenwei Zhang, Yuzhe Gu |  |
| 2784 |  |  [Fine-Tuning Attention Modules Only: Enhancing Weight Disentanglement in Task Arithmetic](https://openreview.net/forum?id=dj0TktJcVI) |  | 0 | In recent years, \*task arithmetic\* has garnered increasing attention. This approach edits pre-trained models directly in weight space by combining the fine-tuned weights of various tasks into a \*unified model\*. Its efficiency and cost-effectiveness stem from its training-free combination,... | Bojian Hou, Jiancong Xiao, Li Shen, Ruochen Jin, Weijie J. Su |  |
| 2785 |  |  [Rethinking Self-Distillation: Label Averaging and Enhanced Soft Label Refinement with Partial Labels](https://openreview.net/forum?id=EJfLvrzh2Q) |  | 0 | We investigate the mechanisms of self-distillation in multi-class classification, particularly in the context of linear probing with fixed feature extractors where traditional feature learning explanations do not apply. Our theoretical analysis reveals that multi-round self-distillation effectively... | Hye Won Chung, Hyeonsu Jeong |  |
| 2786 |  |  [Natural Language Inference Improves Compositionality in Vision-Language Models](https://openreview.net/forum?id=G3aXjVAJjU) |  | 0 | Compositional reasoning in Vision-Language Models (VLMs) remains challenging as these models often struggle to relate objects, attributes, and spatial relationships. Recent methods aim to address these limitations by relying on the semantics of the textual description, using Large Language Models... | Hal Daumé III, Paola CascanteBonilla, Rachel Rudinger, Yang Trista Cao, Yu Hou |  |
| 2787 |  |  [KAA: Kolmogorov-Arnold Attention for Enhancing Attentive Graph Neural Networks](https://openreview.net/forum?id=atXCzVSXTJ) |  | 0 | Graph neural networks (GNNs) with attention mechanisms, often referred to as attentive GNNs, have emerged as a prominent paradigm in advanced GNN models in recent years. However, our understanding of the critical process of scoring neighbor nodes remains limited, leading to the underperformance of... | Chunping Wang, Lei Chen, Taoran Fang, Tianhong Gao, Wei Chow, Yang Yang, Yihao Shang |  |
| 2788 |  |  [Isometric Regularization for Manifolds of Functional Data](https://openreview.net/forum?id=xBuURiCChw) |  | 0 | While conventional data are represented as discrete vectors, Implicit Neural Representations (INRs) utilize neural networks to represent data points as continuous functions. By incorporating a shared network that maps latent vectors to individual functions, one can model the distribution of... | Hyeongjun Heo, Jae Yong Lee, Seonghun Oh, Yonghyeon Lee, Young Min Kim |  |
| 2789 |  |  [PostEdit: Posterior Sampling for Efficient Zero-Shot Image Editing](https://openreview.net/forum?id=J8YWCBPgx7) |  | 0 | In the field of image editing, three core challenges persist: controllability, background preservation, and efficiency. Inversion-based methods rely on time-consuming optimization to preserve the features of the initial images, which results in low efficiency due to the requirement for extensive... | Feng Tian, Shanyan Guan, Xiaokang Yang, Yanhao Ge, Yichao Yan, Yixuan Li |  |
| 2790 |  |  [Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models](https://openreview.net/forum?id=45rvZkJbuX) |  | 0 | Vision-language alignment in Large Vision-Language Models (LVLMs) successfully enables LLMs to understand visual input. However, we find that existing vision-language alignment methods fail to transfer the existing safety mechanism for text in LLMs to vision, which leads to vulnerabilities in toxic... | Huawei Shen, Liang Pang, Shicheng Xu, Xueqi Cheng, Yunchang Zhu |  |
| 2791 |  |  [RefactorBench: Evaluating Stateful Reasoning in Language Agents Through Code](https://openreview.net/forum?id=NiNIthntx7) |  | 0 | Recent advances in language model (LM) agents and function calling have enabled autonomous, feedback-driven systems to solve problems across various digital domains. To better understand the unique limitations of LM agents, we introduce RefactorBench, a benchmark consisting of 100 large handcrafted... | Dhruv Gautam, Jinu Jang, Neel Sundaresan, Roshanak Zilouchian Moghaddam, Spandan Garg |  |
| 2792 |  |  [Efficient Training of Neural Stochastic Differential Equations by Matching Finite Dimensional Distributions](https://openreview.net/forum?id=d4qMoUSMLT) |  | 0 | Neural Stochastic Differential Equations (Neural SDEs) have emerged as powerful mesh-free generative models for continuous stochastic processes, with critical applications in fields such as finance, physics, and biology. Previous state-of-the-art methods have relied on adversarial training, such as... | Doosan Jung, Emily Pitler, Jianxin Zhang, Josh Viktorov |  |
| 2793 |  |  [Learning Diverse Attacks on Large Language Models for Robust Red-Teaming and Safety Tuning](https://openreview.net/forum?id=1mXufFuv95) |  | 0 | Red-teaming, or identifying prompts that elicit harmful responses, is a critical step in ensuring the safe and responsible deployment of large language models (LLMs). Developing effective protection against many modes of attack prompts requires discovering diverse attacks. Automated red-teaming... | David Dobre, Gauthier Gidel, Juho Lee, Kenji Kawaguchi, Lynn Cherif, Minsu Kim, Moksh Jain, Nikolay Malkin, Seanie Lee, Sung Ju Hwang, Yoshua Bengio |  |
| 2794 |  |  [TIS-DPO: Token-level Importance Sampling for Direct Preference Optimization With Estimated Weights](https://openreview.net/forum?id=oF6e2WwxX0) |  | 0 | Direct Preference Optimization (DPO) has been widely adopted for preference alignment of Large Language Models (LLMs) due to its simplicity and effectiveness. However, DPO is derived as a bandit problem in which the whole response is treated as a single arm, ignoring the importance differences... | Aiwei Liu, Albin Madappally Jose, Haoping Bai, Jiulong Shan, Lijie Wen, Meng Cao, Philip S. Yu, Xiang Kong, Xiaojiang Liu, Xiaoming Simon Wang, Yanchao Sun, Zhiyun Lu |  |
| 2795 |  |  [Spectro-Riemannian Graph Neural Networks](https://openreview.net/forum?id=2MLvV7fvAz) |  | 0 | Can integrating spectral and curvature signals unlock new potential in graph representation learning? Non-Euclidean geometries, particularly Riemannian manifolds such as hyperbolic (negative curvature) and spherical (positive curvature), offer powerful inductive biases for embedding complex graph... | Christos Faloutsos, Haiyang Yu, Han Xie, Karish Grover, Qi Zhu, Vassilis N. Ioannidis, Xiang Song |  |
| 2796 |  |  [CBMA: Improving Conformal Prediction through Bayesian Model Averaging](https://openreview.net/forum?id=BKSeNw2HIr) |  | 0 | Conformal prediction has emerged as a popular technique for facilitating valid predictive inference across a spectrum of machine learning models, under minimal assumption of exchangeability. Recently, Hoff (2023) showed that full conformal Bayes provides the most efficient prediction sets (smallest... | Bei Jiang, Linglong Kong, Pankaj Bhagwat |  |
| 2797 |  |  [Point-SAM: Promptable 3D Segmentation Model for Point Clouds](https://openreview.net/forum?id=yXCTDhZDh6) |  | 0 | The development of 2D foundation models for image segmentation has been significantly advanced by the Segment Anything Model (SAM). However, achieving similar success in 3D models remains a challenge due to issues such as non-unified data formats, poor model scalability, and the scarcity of labeled... | Fanbo Xiang, Hao Su, Jiayuan Gu, Tung Yen Chiang, Yuchen Zhou |  |
| 2798 |  |  [Causal Discovery via Bayesian Optimization](https://openreview.net/forum?id=8muemqlnG3) |  | 0 | Existing score-based methods for directed acyclic graph (DAG) learning from observational data struggle to recover the causal graph accurately and sample-efficiently. To overcome this, in this study, we propose DrBO (DAG recovery via Bayesian Optimization)—a novel DAG learning framework leveraging... | Bao Duong, Sunil Gupta, Thin Nguyen |  |
| 2799 |  |  [Towards Out-of-Modal Generalization without Instance-level Modal Correspondence](https://openreview.net/forum?id=LuVulfPgZN) |  | 0 | The world is understood from various modalities, such as appearance, sound, language, etc. Since each modality only partially represents objects in a certain physical meaning, leveraging additional ones is beneficial in both theory and practice. However, exploiting novel modalities normally... | Bo Han, Gang Niu, Masashi Sugiyama, Tongliang Liu, Zhuo Huang |  |
| 2800 |  |  [Data Distillation for extrapolative protein design through exact preference optimization](https://openreview.net/forum?id=ua5MHdsbck) |  | 0 | The goal of protein design typically involves increasing fitness (extrapolating) beyond what is seen during training (e.g., towards higher stability, stronger binding affinity, etc.). State-of-the-art methods assume that one can safely steer proteins towards such extrapolated regions by learning... | Bella Dubrov, Mostafa Karimi, Ron Benson, Shang Shang, Sharmi Banerjee, Tommi Jaakkola |  |
| 2801 |  |  [Let Me Grok for You: Accelerating Grokking via Embedding Transfer from a Weaker Model](https://openreview.net/forum?id=4rEI2JdHH6) |  | 0 | ''Grokking'' is a phenomenon where a neural network first memorizes training data and generalizes poorly, but then suddenly transitions to near-perfect generalization after prolonged training. While intriguing, this delayed generalization phenomenon compromises predictability and efficiency.... | Wei Hu, Yixin Wang, Zhiwei Xu, Zhiyu Ni |  |
| 2802 |  |  [Do LLMs estimate uncertainty well in instruction-following?](https://openreview.net/forum?id=IHp3vOVQO2) |  | 0 | Large language models (LLMs) could be valuable personal AI agents across various domains, provided they can precisely follow user instructions. However, recent studies have shown significant limitations in LLMs' instruction-following capabilities, raising concerns about their reliability in... | Christina HeinzeDeml, Jaya Narain, Juyeon Heo, Miao Xiong |  |
| 2803 |  |  [TopoDiffusionNet: A Topology-aware Diffusion Model](https://openreview.net/forum?id=ZK1LoTo10R) |  | 0 | Diffusion models excel at creating visually impressive images but often struggle to generate images with a specified topology. The Betti number, which represents the number of structures in an image, is a fundamental measure in topology. Yet, diffusion models fail to satisfy even this basic... | Chao Chen, Dimitris Samaras, Saumya Gupta |  |
| 2804 |  |  [DistillHGNN: A Knowledge Distillation Approach for High-Speed Hypergraph Neural Networks](https://openreview.net/forum?id=vzrs42hgb0) |  | 0 | In this paper, we propose a novel framework to significantly enhance the inference speed and memory efficiency of Hypergraph Neural Networks (HGNNs) while preserving their high accuracy. Our approach utilizes an advanced teacher-student knowledge distillation strategy. The teacher model, consisting... | Mahdi Jalili, Parham Moradi, Saman Forouzandeh |  |
| 2805 |  |  [Logical Consistency of Large Language Models in Fact-Checking](https://openreview.net/forum?id=SimlDuN0YT) |  | 0 | In recent years, large language models (LLMs) have demonstrated significant success in performing varied natural language tasks such as language translation, question-answering, summarizing, fact-checking, etc. Despite LLMs’ impressive ability to generate human-like texts, LLMs are infamous for... | Arijit Khan, Bishwamittra Ghosh, Naheed Anjum Arafat, Sarah Hasan |  |
| 2806 |  |  [PIN: Prolate Spheroidal Wave Function-based Implicit Neural Representations](https://openreview.net/forum?id=Eh1QM3OK51) |  | 0 | Implicit Neural Representations (INRs) provide a continuous mapping between the coordinates of a signal and the corresponding values. As the performance of INRs heavily depends on the choice of nonlinear-activation functions, there has been a significant focus on encoding explicit signals within... | Demetrio Labate, Dhananjaya Jayasundara, Heng Zhao, Vishal M. Patel |  |
| 2807 |  |  [Sort-free Gaussian Splatting via Weighted Sum Rendering](https://openreview.net/forum?id=y8uPsxR8PN) |  | 0 | Recently, 3D Gaussian Splatting (3DGS) has emerged as a significant advancement in 3D scene reconstruction, attracting considerable attention due to its ability to recover high-fidelity details while maintaining low complexity. Despite the promising results achieved by 3DGS, its rendering... | Alexei Bourd, Amir Said, Farzad Farhadzadeh, Fatih Porikli, Hoang Le, Qiqi Hou, Randall Rauwendaal, Zifeng Li |  |
| 2808 |  |  [Subtask-Aware Visual Reward Learning from Segmented Demonstrations](https://openreview.net/forum?id=mqKVe6F3Up) |  | 0 | Reinforcement Learning (RL) agents have demonstrated their potential across various robotic tasks. However, they still heavily rely on human-engineered reward functions, requiring extensive trial-and-error and access to target behavior information, often unavailable in real-world settings. This... | Changyeon Kim, Doohyun Lee, Honglak Lee, Jinwoo Shin, Joseph J. Lim, Kimin Lee, Minho Heo |  |
| 2809 |  |  [Not All Language Model Features Are One-Dimensionally Linear](https://openreview.net/forum?id=d63a4AM4hb) |  | 0 | Recent work has proposed that language models perform computation by manipulating one-dimensional representations of concepts ("features") in activation space. In contrast, we explore whether some language model representations may be inherently multi-dimensional. We begin by developing a rigorous... | Eric J. Michaud, Isaac Liao, Joshua Engels, Max Tegmark, Wes Gurnee |  |
| 2810 |  |  [SelectFormer in Data Markets: Privacy-Preserving and Efficient Data Selection for Transformers with Multi-Party Computation](https://openreview.net/forum?id=2cF3f9t31y) |  | 0 | Critical to a free data market is $ \textit{private data selection}$, i.e. the model owner selects and then appraises training data from the data owner before both parties commit to a transaction. To keep the data and model private, this process shall evaluate the target model to be trained over... | Felix Xiaozhu Lin, Xu Ouyang, Yangfeng Ji |  |
| 2811 |  |  [Enhancing Language Model Agents using Diversity of Thoughts](https://openreview.net/forum?id=ZsP3YbYeE9) |  | 0 | A popular approach to building agents using Language Models (LMs) involves iteratively prompting the LM, reflecting on its outputs, and updating the input prompts until the desired task is achieved. However, our analysis reveals two key shortcomings in the existing methods: $(i)$ limited... | Anoop Deoras, Behrooz OmidvarTehrani, Gaurav Gupta, Jun Huan, Linbo Liu, Sayan Ghosh, Sujay Sanghavi, Vijay Lingam |  |
| 2812 |  |  [Instructional Segment Embedding: Improving LLM Safety with Instruction Hierarchy](https://openreview.net/forum?id=sjWG7B8dvt) |  | 0 | Large Language Models (LLMs) are susceptible to security and safety threats, such as prompt injection, prompt extraction, and harmful requests. One major cause of these vulnerabilities is the lack of an instruction hierarchy. Modern LLM architectures treat all inputs equally, failing to distinguish... | Chong Xiang, Kaiqiang Song, Prateek Mittal, Ravi Agrawal, Sanqiang Zhao, Sathish Reddy Indurthi, Shujian Zhang, Silei Xu, Tong Wu, Wenxuan Zhou |  |
| 2813 |  |  [Eia: Environmental Injection Attack on Generalist Web Agents for Privacy Leakage](https://openreview.net/forum?id=xMOLUzo2Lk) |  | 0 | Recently, generalist web agents have demonstrated remarkable potential in autonomously completing a wide range of tasks on real websites, significantly boosting human productivity. However, web tasks, such as booking flights, usually involve users' personally identifiable information (PII), which... | Bo Li, Chaowei Xiao, Chejian Xu, Huan Sun, Jiawei Zhang, Lingbo Mo, Mintong Kang, Yuan Tian, Zeyi Liao |  |
| 2814 |  |  [Don't stop me Now: Embedding based Scheduling for LLMS](https://openreview.net/forum?id=7JhGdZvW4T) |  | 0 | Efficient scheduling is crucial for interactive Large Language Model (LLM) applications, where low request completion time directly impacts user engagement. Size-based scheduling algorithms like Shortest Remaining Process Time (SRPT) aim to reduce average request completion time by leveraging known... | Chunwei Liu, Eran Malach, Michael Mitzenmacher, Minlan Yu, Rana Shahout, Weifan Jiang |  |
| 2815 |  |  [cryoSPHERE: Single-Particle HEterogeneous REconstruction from cryo EM](https://openreview.net/forum?id=n8O0trhost) |  | 0 | The three-dimensional structure of proteins plays a crucial role in determining their function. Protein structure prediction methods, like AlphaFold, offer rapid access to a protein’s structure. However, large protein complexes cannot be reliably predicted, and proteins are dynamic, making it... | Fredrik Lindsten, Gabriel Ducrocq, Lukas Grunewald, Sebastian Westenhoff |  |
| 2816 |  |  [Contextualizing biological perturbation experiments through language](https://openreview.net/forum?id=5WEpbilssv) |  | 0 | High-content perturbation experiments allow scientists to probe biomolecular systems at unprecedented resolution, but experimental and analysis costs pose significant barriers to widespread adoption. Machine learning has the potential to guide efficient exploration of the perturbation space and... | David Richmond, Jacob Levine, JanChristian Huetter, Lin Qiu, Menghua Wu, Russell Littman, Tommaso Biancalani |  |
| 2817 |  |  [Rethinking Fair Representation Learning for Performance-Sensitive Tasks](https://openreview.net/forum?id=pBZntPrdrI) |  | 0 | We investigate the prominent class of fair representation learning methods for bias mitigation. Using causal reasoning to define and formalise different sources of dataset bias, we reveal important implicit assumptions inherent to these methods. We prove fundamental limitations on fair... | Ben Glocker, Charles Jones, Daniel C. Castro, Fabio De Sousa Ribeiro, Mélanie Roschewitz |  |
| 2818 |  |  [Privacy Auditing of Large Language Models](https://openreview.net/forum?id=60Vd7QOXlM) |  | 0 | Current techniques for privacy auditing of large language models (LLMs) have limited efficacy---they rely on basic approaches to generate canaries which leads to weak membership inference attacks that in turn give loose lower bounds on the empirical privacy leakage. We develop canaries that are far... | Ashwinee Panda, Christopher A. ChoquetteChoo, Milad Nasr, Prateek Mittal, Xinyu Tang |  |
| 2819 |  |  [Variance-Reducing Couplings for Random Features](https://openreview.net/forum?id=oJLpXraSLb) |  | 0 | Random features (RFs) are a popular technique to scale up kernel methods in machine learning, replacing exact kernel evaluations with stochastic Monte Carlo estimates. They underpin models as diverse as efficient transformers (by approximating attention) to sparse spectrum Gaussian processes (by... | Adrian Weller, Isaac Reid, Krzysztof Marcin Choromanski, Richard E. Turner, Stratis Markou |  |
| 2820 |  |  [Linear Transformer Topological Masking with Graph Random Features](https://openreview.net/forum?id=6MBqQLp17E) |  | 0 | When training transformers on graph-structured data, incorporating information about the underlying topology is crucial for good performance. Topological masking, a type of relative position encoding, achieves this by upweighting or downweighting attention depending on the relationship between the... | Adrian Weller, Alex Bewley, Amr Ahmed, Aranyak Mehta, Connor Schenck, David Rendleman, Deepali Jain, Isaac Reid, Joshua Ainslie, Krzysztof Marcin Choromanski, Kumar Avinava Dubey, Mithun George Jacob, René Wagner, Richard E. Turner, William F. Whitney |  |
| 2821 |  |  [Unlearn and Burn: Adversarial Machine Unlearning Requests Destroy Model Accuracy](https://openreview.net/forum?id=5xxGP9x5dZ) |  | 0 | Machine unlearning algorithms, designed for selective removal of training data from models, have emerged as a promising approach to growing privacy concerns. In this work, we expose a critical yet underexplored vulnerability in the deployment of unlearning systems: the assumption that the data... | Amer Sinha, Badih Ghazi, Chiyuan Zhang, Daogao Liu, Lynn Chua, Milad Nasr, Pasin Manurangsi, Pritish Kamath, Ravi Kumar, Yangsibo Huang |  |
| 2822 |  |  [A3D: Does Diffusion Dream about 3D Alignment?](https://openreview.net/forum?id=QQCIfkhGIq) |  | 0 | We tackle the problem of text-driven 3D generation from a geometry alignment perspective. Given a set of text prompts, we aim to generate a collection of objects with semantically corresponding parts aligned across them. Recent methods based on Score Distillation have succeeded in distilling the... | Alexander Filippov, Alexey Artemov, Anton Konushin, Daniil Selikhanovych, Dmitry Senushkin, Evgeny Burnaev, Ilya Olkov, Nikolay Patakin, Nina Konovalova, Oleg Voynov, Peter Wonka, Savva Victorovich Ignatyev |  |
| 2823 |  |  [Non-myopic Generation of Language Models for Reasoning and Planning](https://openreview.net/forum?id=OoNazl6T7D) |  | 0 | Large Language Models (LLMs) have demonstrated remarkable abilities in reasoning and planning. Despite their success in various domains, such as mathematical problem-solving and coding, LLMs face challenges in ensuring reliable and optimal planning due to the inherent myopic nature of... | Chang Ma, Haiteng Zhao, Junlei Zhang, Junxian He, Lingpeng Kong |  |
| 2824 |  |  [3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing](https://openreview.net/forum?id=7JUrBLDjCq) |  | 0 | The transformative potential of 3D content creation has been progressively unlocked through advancements in generative models. Recently, intuitive drag editing with geometric changes has attracted significant attention in 2D editing yet remains challenging for 3D scenes. In this paper, we introduce... | Jiahua Dong, YuXiong Wang |  |
| 2825 |  |  [Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling](https://openreview.net/forum?id=3OyaXFQuDl) |  | 0 | Training on high-quality synthetic data from strong language models (LMs) is a common strategy to improve the reasoning performance of LMs. In this work, we revisit whether this strategy is compute-optimal under a fixed inference budget (e.g., FLOPs). To do so, we investigate the trade-offs between... | Arian Hosseini, Hritik Bansal, Mehran Kazemi, Rishabh Agarwal, Vinh Q. Tran |  |
| 2826 |  |  [Distributed Speculative Inference (DSI): Speculation Parallelism for Provably Faster Lossless Language Model Inference](https://openreview.net/forum?id=cJd1BgZ9CS) |  | 0 | This paper introduces \*distributed speculative inference (DSI)\*, a novel inference algorithm that is provably faster than speculative inference (SI) [leviathan2023, chen2023, miao2024, sun2025, timor2025] and standard autoregressive inference (non-SI). Like other SI algorithms, DSI operates on... | Daniel Korat, David Harel, Jonathan Mamou, Michal GordonKiwkowitz, Moshe Berchansky, Moshe Wasserblat, Nadav Timor, Oren Pereg, Tomer Galanti |  |
| 2827 |  |  [Improving Generalization and Robustness in SNNs Through Signed Rate Encoding and Sparse Encoding Attacks](https://openreview.net/forum?id=qLh6Ufvnuc) |  | 0 | Rate-encoded spiking neural networks (SNNs) are known to offer superior adversarial robustness compared to direct-encoded SNNs but have relatively poor generalization on clean input. While the latter offers good generalization on clean input it suffers poor adversarial robustness under standard... | Bhaskar Mukhoty, Bin Gu, Hilal AlQuabeh |  |
| 2828 |  |  [Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers](https://openreview.net/forum?id=M23dTGWCZy) |  | 0 | Recent advancements in large language models (LLMs) have sparked optimism about their potential to accelerate scientific discovery, with a growing number of works proposing research agents that autonomously generate and validate new ideas. Despite this, no evaluations have shown that LLM systems... | Chenglei Si, Diyi Yang, Tatsunori Hashimoto |  |
| 2829 |  |  [CertainlyUncertain: A Benchmark and Metric for Multimodal Epistemic and Aleatoric Awareness](https://openreview.net/forum?id=cQ25MQQSNI) |  | 0 | The ability to acknowledge the inevitable uncertainty in their knowledge and reasoning is a prerequisite for AI systems to be truly truthful and reliable. In this paper, we present a taxonomy of uncertainty specific to vision-language AI systems, distinguishing between epistemic uncertainty... | Anas Awadalla, Jack Hessel, Jae Sung Park, Khyathi Raghavi Chandu, Lijuan Wang, Linjie Li, Ximing Lu, Yejin Choi |  |
| 2830 |  |  [Generative Verifiers: Reward Modeling as Next-Token Prediction](https://openreview.net/forum?id=Ccwp4tFEtE) |  | 0 | Verifiers or reward models are often used to enhance the reasoning performance of large language models (LLMs). A common approach is the Best-of-N method, where N candidate solutions generated by the LLM are ranked by a verifier, and the best one is selected. While LLM-based verifiers are typically... | Arian Hosseini, Aviral Kumar, Hritik Bansal, Lunjun Zhang, Mehran Kazemi, Rishabh Agarwal |  |
| 2831 |  |  [Towards Understanding the Robustness of Diffusion-Based Purification: A Stochastic Perspective](https://openreview.net/forum?id=shqjOIK3SA) |  | 0 | Diffusion-Based Purification (DBP) has emerged as an effective defense mechanism against adversarial attacks. The success of DBP is often attributed to the forward diffusion process, which reduces the distribution gap between clean and adversarial images by adding Gaussian noise. Although this... | Kezhao Liu, Liang Lin, Pengxu Wei, Xiaogang Xu, Yao Xiao, Yiming Liu, Ziyi Dong |  |
| 2832 |  |  [COAT: Compressing Optimizer states and Activations for Memory-Efficient FP8 Training](https://openreview.net/forum?id=XfKSDgqIRj) |  | 0 | FP8 training has emerged as a promising method for improving training efficiency. Existing frameworks accelerate training by applying FP8 computation to linear layers while leaving optimizer states and activations in higher precision, which fails to fully optimize memory usage. This paper... | Han Cai, Haocheng Xi, Jianfei Chen, Kurt Keutzer, Ligeng Zhu, Song Han, Yao Lu |  |
| 2833 |  |  [Error-quantified Conformal Inference for Time Series](https://openreview.net/forum?id=RD9q5vEe1Q) |  | 0 | Uncertainty quantification in time series prediction is challenging due to the temporal dependence and distribution shift on sequential data. Conformal prediction provides a pivotal and flexible instrument for assessing the uncertainty of machine learning models through prediction sets. Recently, a... | Changliang Zou, Dongjian Hu, Junxi Wu, ShuTao Xia, Yajie Bao |  |
| 2834 |  |  [Fengbo: a Clifford Neural Operator pipeline for 3D PDEs in Computational Fluid Dynamics](https://openreview.net/forum?id=VsxbWTDHjh) |  | 0 | We introduce Fengbo, a pipeline entirely in Clifford Algebra to solve 3D partial differential equations (PDEs) specifically for computational fluid dynamics (CFD). Fengbo is an architecture composed of only 3D convolutional and Fourier Neural Operator (FNO) layers, all working in 3D Clifford... | Alberto Pepe, Joan Lasenby, Mattia Montanari |  |
| 2835 |  |  [Needle Threading: Can LLMs Follow Threads Through Near-Million-Scale Haystacks?](https://openreview.net/forum?id=wHLMsM1SrP) |  | 0 | As the context limits of Large Language Models (LLMs) increase, the range of possible applications and downstream functions broadens. In many real-world tasks, decisions depend on details scattered across collections of often disparate documents containing mostly irrelevant information.... | Jonathan Roberts, Kai Han, Samuel Albanie |  |
| 2836 |  |  [SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Model](https://openreview.net/forum?id=FGMkSL8NR0) |  | 0 | Integrating the 3D world into large language models (3D-based LLMs) has been a promising research direction for 3D scene understanding. However, current 3D-based LLMs fall short in situated understanding due to two key limitations: 1) existing 3D datasets are constructed from a global perspective... | Lifu Huang, Parisa Kordjamshidi, Ying Shen, Yue Zhang, Zhiyang Xu |  |
| 2837 |  |  [Optimal Protocols for Continual Learning via Statistical Physics and Control Theory](https://openreview.net/forum?id=rhhQjGj09A) |  | 0 | Artificial neural networks often struggle with _catastrophic forgetting_ when learning multiple tasks sequentially, as training on new tasks degrades the performance on previously learned tasks. Recent theoretical work has addressed this issue by analysing learning curves in synthetic frameworks... | Francesca Mignacco, Francesco Mori, Stefano Sarao Mannelli |  |
| 2838 |  |  [Learning Generalizable Skills from Offline Multi-Task Data for Multi-Agent Cooperation](https://openreview.net/forum?id=HR1ujVR0ig) |  | 0 | Learning cooperative multi-agent policy from offline multi-task data that can generalize to unseen tasks with varying numbers of agents and targets is an attractive problem in many scenarios. Although aggregating general behavior patterns among multiple tasks as skills to improve policy transfer is... | Bin Yang, Chenjuan Guo, Sicong Liu, Yang Shu |  |
| 2839 |  |  [Visual Haystacks: A Vision-Centric Needle-In-A-Haystack Benchmark](https://openreview.net/forum?id=9JCNPFL1f9) |  | 0 | Large Multimodal Models (LMMs) have made significant strides in visual question-answering for single images. Recent advancements like long-context LMMs have allowed them to ingest larger, or even multiple, images. However, the ability to process a large number of visual tokens does not guarantee... | David M. Chan, Giscard Biamby, Jerome Quenum, Joseph E. Gonzalez, Ritwik Gupta, Trevor Darrell, TsungHan Wu |  |
| 2840 |  |  [Null Counterfactual Factor Interactions for Goal-Conditioned Reinforcement Learning](https://openreview.net/forum?id=2uPZ4aX1VV) |  | 0 | Hindsight relabeling is a powerful tool for overcoming sparsity in goal-conditioned reinforcement learning (GCRL), especially in certain domains such as navigation and locomotion. However, hindsight relabeling can struggle in object-centric domains. For example, suppose that the goal space consists... | Amy Zhang, Caleb Chuck, Carl Qi, Chang Shi, Fan Feng, Scott Niekum, Siddhant Agarwal |  |
| 2841 |  |  [PAD: Personalized Alignment of LLMs at Decoding-time](https://openreview.net/forum?id=e7AUJpP8bV) |  | 0 | Aligning with personalized preferences, which vary significantly across cultural, educational, and political differences, poses a significant challenge due to the computational costs and data demands of traditional alignment methods. In response, this paper presents Personalized Alignment at... | Meng Luo, Ruizhe Chen, Wenhao Chai, Xiaotian Zhang, Zuozhu Liu |  |
| 2842 |  |  [From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities](https://openreview.net/forum?id=3TnLGGHhNx) |  | 0 | Multimodal Large Language Models have made significant strides in integrating visual and textual information, yet they often struggle with effectively aligning these modalities. We introduce a novel image tokenizer that bridges this gap by applying the principle of Byte-Pair Encoding (BPE) to... | Sipeng Zheng, Wanpeng Zhang, Xingrun Xing, Yicheng Feng, Yijiang Li, Zilong Xie, Zongqing Lu |  |
| 2843 |  |  [Diffusion Models as Cartoonists: The Curious Case of High Density Regions](https://openreview.net/forum?id=RiS2cxpENN) |  | 0 | We investigate what kind of images lie in the high-density regions of diffusion models. We introduce a theoretical mode-tracking process capable of pinpointing the exact mode of the denoising distribution, and we propose a practical high-density sampler that consistently generates images of higher... | Markus Heinonen, Rafal Karczewski, Vikas Garg |  |
| 2844 |  |  [Differentiable Rule Induction from Raw Sequence Inputs](https://openreview.net/forum?id=zDjHOsSQxd) |  | 0 | Rule learning-based models are widely used in highly interpretable scenarios due to their transparent structures. Inductive logic programming (ILP), a form of machine learning, induces rules from facts while maintaining interpretability. Differentiable ILP models enhance this process by leveraging... | Hanpin Wang, Katsumi Inoue, Kun Gao, Yang Feng, Yongzhi Cao |  |
| 2845 |  |  [On the Modeling Capabilities of Large Language Models for Sequential Decision Making](https://openreview.net/forum?id=vodsIF3o7N) |  | 0 | Large pretrained models are showing increasingly better performance in reasoning and planning tasks across different modalities, opening the possibility to leverage them for complex sequential decision making problems. In this paper, we investigate the capabilities of Large Language Models (LLMs)... | Alexander T. Toshev, Bogdan Mazoure, Martin Klissarov, R. Devon Hjelm |  |
| 2846 |  |  [Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks](https://openreview.net/forum?id=BEpaPHDl9r) |  | 0 | We study the implicit bias of the family of steepest descent algorithms with infinitesimal learning rate, including gradient descent, sign gradient descent and coordinate descent, in deep homogeneous neural networks. We prove that an algorithm-dependent geometric margin increases during training... | Gal Vardi, Julia Kempe, Nikolaos Tsilivis |  |
| 2847 |  |  [Exploiting Distribution Constraints for Scalable and Efficient Image Retrieval](https://openreview.net/forum?id=d0tlL0ZWlu) |  | 0 | Image retrieval is crucial in robotics and computer vision, with downstream applications in robot place recognition and vision-based product recommendations. Modern retrieval systems face two key challenges: scalability and efficiency. State-of-the-art image retrieval systems train specific neural... | Mohammad Omama, Pohan Li, Sandeep P. Chinchali |  |
| 2848 |  |  [Discrete Distribution Networks](https://openreview.net/forum?id=xNsIfzlefG) |  | 0 | We introduce a novel generative model, the Discrete Distribution Networks (DDN), that approximates data distribution using hierarchical discrete distributions. We posit that since the features within a network inherently capture distributional information, enabling the network to generate multiple... | Lei Yang |  |
| 2849 |  |  [Chain-of-Thought Provably Enables Learning the (Otherwise) Unlearnable](https://openreview.net/forum?id=N6pbLYLeej) |  | 0 | Modern language models have demonstrated remarkable reasoning capabilities by using chain-of-thought (CoT). One hypothesis about the inner workings of CoT is that it breaks down originally complex tasks into smaller subtasks that are more amenable to learning. We formalize this notion by showing... | Chenxiao Yang, David Wipf, Zhiyuan Li |  |
| 2850 |  |  [Glad: A Streaming Scene Generator for Autonomous Driving](https://openreview.net/forum?id=ZFxpclrCCf) |  | 0 | The generation and simulation of diverse real-world scenes have significant application value in the field of autonomous driving, especially for the corner cases. Recently, researchers have explored employing neural radiance fields or diffusion models to generate novel views or synthetic data under... | Bin Xie, Jiale Cao, Tiancai Wang, Xiangyu Zhang, Yingfei Liu |  |
| 2851 |  |  [Diffusion Models are Evolutionary Algorithms](https://openreview.net/forum?id=xVefsBbG2O) |  | 0 | In a convergence of machine learning and biology, we reveal that diffusion models are evolutionary algorithms. By considering evolution as a denoising process and reversed evolution as diffusion, we mathematically demonstrate that diffusion models inherently perform evolutionary algorithms,... | Benedikt Hartl, Hananel Hazan, Michael Levin, Yanbo Zhang |  |
| 2852 |  |  [Beyond Interpretability: The Gains of Feature Monosemanticity on Model Robustness](https://openreview.net/forum?id=g6Qc3p7JH5) |  | 0 | Deep learning models often suffer from a lack of interpretability due to \emph{polysemanticity}, where individual neurons are activated by multiple unrelated semantics, resulting in unclear attributions of model behavior. Recent advances in \emph{monosemanticity}, where neurons correspond to... | Jingyi Cui, Qi Lei, Qi Zhang, Stefanie Jegelka, Xiang Pan, Yifei Wang, Yisen Wang |  |
| 2853 |  |  [Exploring a Principled Framework for Deep Subspace Clustering](https://openreview.net/forum?id=7psWohxvxp) |  | 0 | Subspace clustering is a classical unsupervised learning task, built on a basic assumption that high-dimensional data can be approximated by a union of subspaces (UoS). Nevertheless, the real-world data are often deviating from the UoS assumption. To address this challenge, state-of-the-art deep... | ChunGuang Li, Rong Xiao, Wei He, Xianbiao Qi, Xianghan Meng, Zhiyuan Huang |  |
| 2854 |  |  [HELM: Hierarchical Encoding for mRNA Language Modeling](https://openreview.net/forum?id=MMHqnUOnl0) |  | 0 | Messenger RNA (mRNA) plays a crucial role in protein synthesis, with its codon structure directly impacting biological properties. While Language Models (LMs) have shown promise in analyzing biological sequences, existing approaches fail to account for the hierarchical nature of mRNA's codon... | Artem Moskalev, Mangal Prakash, Mehdi YazdaniJahromi, Rui Liao, Tommaso Mansi |  |
| 2855 |  |  [Stochastic variance-reduced Gaussian variational inference on the Bures-Wasserstein manifold](https://openreview.net/forum?id=iMJpmcYucq) |  | 0 | Optimization in the Bures-Wasserstein space has been gaining popularity in the machine learning community since it draws connections between variational inference and Wasserstein gradient flows. The variational inference objective function of Kullback–Leibler divergence can be written as the sum of... | Arto Klami, Bernardo Williams, Hanlin Yu, Hoang Phuc Hau Luu, Marcelo Hartmann |  |
| 2856 |  |  [SelKD: Selective Knowledge Distillation via Optimal Transport Perspective](https://openreview.net/forum?id=H4iVLvRusn) |  | 0 | Knowledge Distillation (KD) has been a popular paradigm for training a (smaller) student model from its teacher model. However, little research has been done on the practical scenario where only a subset of the teacher's knowledge needs to be distilled, which we term selective KD (SelKD). This... | Junchi Yan, Liangliang Shi, Zhengyan Shi |  |
| 2857 |  |  [MeshMask: Physics-Based Simulations with Masked Graph Neural Networks](https://openreview.net/forum?id=bFHR8hNk4I) |  | 0 | We introduce a novel masked pre-training technique for graph neural networks (GNNs) applied to computational fluid dynamics (CFD) problems. By randomly masking up to 40\% of input mesh nodes during pre-training, we force the model to learn robust representations of complex fluid dynamics. We pair... | Elie Hachem, Jonathan Viquerat, Paul Garnier, Vincent Lannelongue |  |
| 2858 |  |  [EMOS: Embodiment-aware Heterogeneous Multi-robot Operating System with LLM Agents](https://openreview.net/forum?id=Ey8KcabBpB) |  | 0 | Heterogeneous multi-robot systems (HMRS) have emerged as a powerful ap- proach for tackling complex tasks that single robots cannot manage alone. Current large-language-model-based multi-agent systems (LLM-based MAS) have shown success in areas like software development and operating systems, but... | Checheng Yu, Guohao Li, Junting Chen, Lin Shao, Mengkang Hu, Tianqi Xu, Wenqi Shao, Xunzhe Zhou, Yao Mu, Yikai Wang |  |
| 2859 |  |  [Watch Less, Do More: Implicit Skill Discovery for Video-Conditioned Policy](https://openreview.net/forum?id=hgvERMkXOx) |  | 0 | In this paper, we study the problem of video-conditioned policy learning. While previous works mostly focus on learning policies that perform a single skill specified by the given video, we take a step further and aim to learn a policy that can perform multiple skills according to the given video,... | Jiangxing Wang, Zongqing Lu |  |
| 2860 |  |  [MiniPLM: Knowledge Distillation for Pre-training Language Models](https://openreview.net/forum?id=tJHDw8XfeC) |  | 0 | Knowledge distillation (KD) is widely used to train small, high-performing student language models (LMs) using large teacher LMs. While effective in fine-tuning, KD during pre-training faces efficiency, flexibility, and effectiveness issues. Existing methods either incur high computational costs... | Fandong Meng, Hao Zhou, Jie Zhou, Minlie Huang, Yuxian Gu |  |
| 2861 |  |  [Scalable Decentralized Learning with Teleportation](https://openreview.net/forum?id=AvmBgiQxxp) |  | 0 | Decentralized SGD can run with low communication costs, but its sparse communication characteristics deteriorate the convergence rate, especially when the number of nodes is large. In decentralized learning settings, communication is assumed to occur on only a given topology, while in many... | Sebastian U. Stich, Yuki Takezawa |  |
| 2862 |  |  [What Matters When Repurposing Diffusion Models for General Dense Perception Tasks?](https://openreview.net/forum?id=BgYbk6ZmeX) |  | 0 | Extensive pre-training with large data is indispensable for downstream geometry and semantic visual perception tasks. Thanks to large-scale text-to-image (T2I) pretraining, recent works show promising results by simply fine-tuning T2I diffusion models for a few dense perception tasks. However,... | Chengxiang Fan, Chunhua Shen, Guangkai Xu, Hao Chen, Kangyang Xie, Mingyu Liu, Yongtao Ge, Zhiyue Zhao |  |
| 2863 |  |  [Scalable Discrete Diffusion Samplers: Combinatorial Optimization and Statistical Physics](https://openreview.net/forum?id=peNgxpbdxB) |  | 0 | Learning to sample from complex unnormalized distributions over discrete domains emerged as a promising research direction with applications in statistical physics, variational inference, and combinatorial optimization. Recent work has demonstrated the potential of diffusion models in this domain.... | Haoyu Peter Wang, Martin Ennemoser, Sebastian Lehner, Sebastian Sanokowski, Sepp Hochreiter, Wilhelm Franz Berghammer |  |
| 2864 |  |  [Towards Multiple Character Image Animation Through Enhancing Implicit Decoupling](https://openreview.net/forum?id=aqlzXgXwWa) |  | 0 | Controllable character image animation has a wide range of applications. Although existing studies have consistently improved performance, challenges persist in the field of character image animation, particularly concerning stability in complex backgrounds and tasks involving multiple characters.... | Andong Wang, HeungYeung Shum, Hongfa Wang, Jingyun Xue, Kaihao Zhang, Mengyang Liu, Qi Tian, Shaobo Min, Wei Liu, Wenhan Luo, Wenzhe Zhao, Yue Ma, Zhiyuan Zhao |  |
| 2865 |  |  [On the Byzantine-Resilience of Distillation-Based Federated Learning](https://openreview.net/forum?id=of6EuHT7de) |  | 0 | Federated Learning (FL) algorithms using Knowledge Distillation (KD) have received increasing attention due to their favorable properties with respect to privacy, non-i.i.d. data and communication cost. These methods depart from transmitting model parameters and instead communicate information... | Christophe Roux, Max Zimmer, Sebastian Pokutta |  |
| 2866 |  |  [QPM: Discrete Optimization for Globally Interpretable Image Classification](https://openreview.net/forum?id=GlAeL0I8LX) |  | 0 | Understanding the classifications of deep neural networks, e.g. used in safety-critical situations, is becoming increasingly important. While recent models can locally explain a single decision, to provide a faithful global explanation about an accurate model’s general behavior is a more... | Bodo Rosenhahn, Ramesh Manuvinakurike, Sovan Biswas, Thomas Norrenbrock, Timo Kaiser |  |
| 2867 |  |  [DynaPrompt: Dynamic Test-Time Prompt Tuning](https://openreview.net/forum?id=EFZEdHB3Mp) |  | 0 | Test-time prompt tuning enhances zero-shot generalization of vision-language models but tends to ignore the relatedness among test samples during inference. Online test-time prompt tuning provides a simple way to leverage the information in previous test samples, albeit with the risk of prompt... | Cees G. M. Snoek, Cheems Wang, Jack Hong, Jiayi Shen, Jiayin Cai, Shilin Yan, Xiaolong Jiang, Yao Hu, Zehao Xiao |  |
| 2868 |  |  [EffoVPR: Effective Foundation Model Utilization for Visual Place Recognition](https://openreview.net/forum?id=NSpe8QgsCB) |  | 0 | The task of Visual Place Recognition (VPR) is to predict the location of a query image from a database of geo-tagged images. Recent studies in VPR have highlighted the significant advantage of employing pre-trained foundation models like DINOv2 for the VPR task. However, these models are often... | Boaz Lerner, Dvir Samuel, Gavriel Habib, Issar Tzachor, Matan Levy, Michael Green, Nir Darshan, Noam Korngut Zailer, Or Shimshi, Rami BenAri, Tal Berkovitz Shalev |  |
| 2869 |  |  [Linear combinations of latents in generative models: subspaces and beyond](https://openreview.net/forum?id=n5PrId7pk5) |  | 0 | Sampling from generative models has become a crucial tool for applications like data synthesis and augmentation. Diffusion, Flow Matching and Continuous Normalising Flows have shown effectiveness across various modalities, and rely on latent variables for generation. For experimental design or... | Alexandru I. Stere, Carl Henrik Ek, Dragos D. Margineantu, Erik Bodin, Henry Moss |  |
| 2870 |  |  [SOREL: A Stochastic Algorithm for Spectral Risks Minimization](https://openreview.net/forum?id=pdF86dyoS6) |  | 0 | The spectral risk has wide applications in machine learning, especially in real-world decision-making, where people are concerned with more than just average model performance. By assigning different weights to the losses of different sample points, rather than the same weights as in the empirical... | Rujun Jiang, Yuze Ge |  |
| 2871 |  |  [Proactive Privacy Amnesia for Large Language Models: Safeguarding PII with Negligible Impact on Model Utility](https://openreview.net/forum?id=io8uRPYktn) |  | 0 | With the rise of large language models (LLMs), increasing research has recognized their risk of leaking personally identifiable information (PII) under malicious attacks. Although efforts have been made to protect PII in LLMs, existing methods struggle to balance privacy protection with maintaining... | Amin Hass, Aolin Ding, Hai Li, Jianyi Zhang, Jingwei Sun, Jingyang Zhang, Louis DiValentin, Martin Kuo, Minxue Tang, Tianlong Chen, William Chen, Yiran Chen |  |
| 2872 |  |  [Variational Best-of-N Alignment](https://openreview.net/forum?id=W9FZEQj3vv) |  | 0 | Best-of-N (BoN) is a popular and effective algorithm for aligning language models to human preferences. The algorithm works as follows: at inference time, N samples are drawn from the language model, and the sample with the highest reward, as judged by a reward model, is returned as the output.... | Afra Amini, Elliott Ash, Ryan Cotterell, Tim Vieira |  |
| 2873 |  |  [HMoRA: Making LLMs More Effective with Hierarchical Mixture of LoRA Experts](https://openreview.net/forum?id=lTkHiXeuDl) |  | 0 | Recent studies have combined Mixture of Experts (MoE) and Parameter-Efficient Fine-tuning (PEFT) to fine-tune large language models (LLMs), holding excellent performance in multi-task scenarios while remaining resource-efficient. However, existing MoE approaches still exhibit the following... | Huaiyu Wan, Junfeng Shen, Mengqi Liao, Shengnan Guo, Wei Chen |  |
| 2874 |  |  [Neural ODE Transformers: Analyzing Internal Dynamics and Adaptive Fine-tuning](https://openreview.net/forum?id=XnDyddPcBT) |  | 0 | Recent advancements in large language models (LLMs) based on transformer architectures have sparked significant interest in understanding their inner workings. In this paper, we introduce a novel approach to modeling transformer architectures using highly flexible non-autonomous neural ordinary... | Anh Tong, Cheongwoong Kang, David Leo Wright Hall, Dongeun Lee, Duc Nguyen, Jaesik Choi, Thanh NguyenTang, Toan M. Tran |  |
| 2875 |  |  [Adversarially Robust Out-of-Distribution Detection Using Lyapunov-Stabilized Embeddings](https://openreview.net/forum?id=GrDne4055L) |  | 0 | Despite significant advancements in out-of-distribution (OOD) detection, existing methods still struggle to maintain robustness against adversarial attacks, compromising their reliability in critical real-world applications. Previous studies have attempted to address this challenge by exposing... | Hossein Mirzaei, Mackenzie W. Mathis |  |
| 2876 |  |  [Shape as Line Segments: Accurate and Flexible Implicit Surface Representation](https://openreview.net/forum?id=RavSZTIe2s) |  | 0 | Distance field-based implicit representations like signed/unsigned distance fields have recently gained prominence in geometry modeling and analysis. However, these distance fields are reliant on the closest distance of points to the surface, introducing inaccuracies when interpolating along cube... | Junhui Hou, Siyu Ren |  |
| 2877 |  |  [MoS: Unleashing Parameter Efficiency of Low-Rank Adaptation with Mixture of Shards](https://openreview.net/forum?id=1uLW9eYNJB) |  | 0 | The rapid scaling of large language models necessitates more lightweight finetuning methods to reduce the explosive GPU memory overhead when numerous customized models are served simultaneously. Targeting more parameter-efficient low-rank adaptation (LoRA), parameter sharing presents a promising... | Boyang Xue, Chuan Wu, Jingwei Dong, Jiyue Jiang, Liheng Chen, Lingpeng Kong, Pengan Chen, Sheng Wang |  |
| 2878 |  |  [Simulating Human-like Daily Activities with Desire-driven Autonomy](https://openreview.net/forum?id=3ms8EQY7f8) |  | 0 | Desires motivate humans to interact autonomously with the complex world. In contrast, current AI agents require explicit task specifications, such as instructions or reward functions, which constrain their autonomy and behavioral diversity. In this paper, we introduce a Desire-driven Autonomous... | Fangwei Zhong, Long Ma, Yiding Wang, Yizhou Wang, Yuxuan Chen |  |
| 2879 |  |  [Ctrl-U: Robust Conditional Image Generation via Uncertainty-aware Reward Modeling](https://openreview.net/forum?id=eC2ICbECNM) |  | 0 | In this paper, we focus on the task of conditional image generation, where an image is synthesized according to user instructions. The critical challenge underpinning this task is ensuring both the fidelity of the generated images and their semantic alignment with the provided conditions. To tackle... | Guiyu Zhang, Hao Zhao, Huanang Gao, Zhedong Zheng, Zijian Jiang |  |
| 2880 |  |  [Gradient correlation is a key ingredient to accelerate SGD with momentum](https://openreview.net/forum?id=2Q8gTck8Uq) |  | 0 | Empirically, it has been observed that adding momentum to Stochastic Gradient Descent (SGD) accelerates the convergence of the algorithm. However, the literature has been rather pessimistic, even in the case of convex functions, about the possibility of theoretically proving this observation. We... | Aude Rondepierre, Charles Dossal, JeanFrançois Aujol, Julien Hermant, Marien Renaud |  |
| 2881 |  |  [Multiplicative Logit Adjustment Approximates Neural-Collapse-Aware Decision Boundary Adjustment](https://openreview.net/forum?id=II81zQUS1x) |  | 0 | Real-world data distributions are often highly skewed. This has spurred a growing body of research on long-tailed recognition, aimed at addressing the imbalance in training classification models. Among the methods studied, multiplicative logit adjustment (MLA) stands out as a simple and effective... | Issei Sato, Naoya Hasegawa |  |
| 2882 |  |  [Pyramidal Flow Matching for Efficient Video Generative Modeling](https://openreview.net/forum?id=66NzcRQuOq) |  | 0 | Video generation requires modeling a vast spatiotemporal space, which demands significant computational resources and data usage. To reduce the complexity, the prevailing approaches employ a cascaded architecture to avoid direct training with full resolution latent. Despite reducing computational... | Hao Jiang, Kun Xu, Nan Zhuang, Ningyuan Li, Quzhe Huang, Yadong Mu, Yang Jin, Yang Song, Zhicheng Sun, Zhouchen Lin |  |
| 2883 |  |  [CtrLoRA: An Extensible and Efficient Framework for Controllable Image Generation](https://openreview.net/forum?id=3Gga05Jdmj) |  | 0 | Recently, large-scale diffusion models have made impressive progress in text-to-image (T2I) generation. To further equip these T2I models with fine-grained spatial control, approaches like ControlNet introduce an extra network that learns to follow a condition image. However, for every single... | Shiguang Shan, Xilin Chen, Yifeng Xu, Zhenliang He |  |
| 2884 |  |  [Fast Summation of Radial Kernels via QMC Slicing](https://openreview.net/forum?id=iNmVX9lx9l) |  | 0 | The fast computation of large kernel sums is a challenging task, which arises as a subproblem in any kernel method. We approach the problem by slicing, which relies on random projections to one-dimensional subspaces and fast Fourier summation. We prove bounds for the slicing error and propose a... | Johannes Hertrich, Michael Quellmalz, Tim Jahn |  |
| 2885 |  |  [Enhancing Federated Domain Adaptation with Multi-Domain Prototype-Based Federated Fine-Tuning](https://openreview.net/forum?id=3wEGdrV5Cb) |  | 0 | Federated Domain Adaptation (FDA) is a Federated Learning (FL) scenario where models are trained across multiple clients with unique data domains but a shared category space, without transmitting private data. The primary challenge in FDA is data heterogeneity, which causes significant divergences... | Jingyuan Zhang, Shuaicheng Niu, Wei Yang Bryan Lim, Yang Cao, Yiyang Duan |  |
| 2886 |  |  [NEAR: A Training-Free Pre-Estimator of Machine Learning Model Performance](https://openreview.net/forum?id=Z8RZrvngm5) |  | 0 | Artificial neural networks have been shown to be state-of-the-art machine learning models in a wide variety of applications, including natural language processing and image recognition. However, building a performant neural network is a laborious task and requires substantial computing power.... | Marco Eckhoff, Markus Reiher, Raphael T. Husistein |  |
| 2887 |  |  [Do as I do (Safely): Mitigating Task-Specific Fine-tuning Risks in Large Language Models](https://openreview.net/forum?id=lXE5lB6ppV) |  | 0 | Recent research shows that fine-tuning on benign instruction-following data can inadvertently undo the safety alignment process and increase a model's propensity to comply with harmful queries. While instruction-following fine-tuning is important, task-specific fine-tuning-where models are trained... | Adel Bibi, Aleksandar Petrov, Francisco Eiras, M. Pawan Kumar, Philip Torr |  |
| 2888 |  |  [Epistemic Monte Carlo Tree Search](https://openreview.net/forum?id=Tb8RiXOc3N) |  | 0 | The AlphaZero/MuZero (A/MZ) family of algorithms has achieved remarkable success across various challenging domains by integrating Monte Carlo Tree Search (MCTS) with learned models. Learned models introduce epistemic uncertainty, which is caused by learning from limited data and is useful for... | Matthijs T. J. Spaan, Viliam Vadocz, Wendelin Boehmer, Yaniv Oren |  |
| 2889 |  |  [ParaSolver: A Hierarchical Parallel Integral Solver for Diffusion Models](https://openreview.net/forum?id=2JihLwirxO) |  | 0 | This paper explores the challenge of accelerating the sequential inference process of Diffusion Probabilistic Models (DPMs). We tackle this critical issue from a dynamic systems perspective, in which the inherent sequential nature is transformed into a parallel sampling process. Specifically, we... | Jianrong Lu, Junhui Hou, Zhiyu Zhu |  |
| 2890 |  |  [Enhance Multi-View Classification Through Multi-Scale Alignment and Expanded Boundary](https://openreview.net/forum?id=t1J2CnDFwj) |  | 0 | Multi-view classification aims at unifying the data from multiple views to complementarily enhance the classification performance. Unfortunately, two major problems in multi-view data are damaging model performance. The first is feature heterogeneity, which makes it hard to fuse features from... | Gengyu Lyu, Haichun Cai, Haobo Wang, Huibin Lin, Yiyuan Wang, Yongjian Deng, Yuena Lin, Zhen Yang |  |
| 2891 |  |  [LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs](https://openreview.net/forum?id=3A71qNKWAS) |  | 0 | Current benchmarks like \`\`$\textit{Needle-in-a-Haystack}$'' ($\textit{NIAH}$), $\textit{Ruler}$, and $\textit{Needlebench}$ focus on models' ability to understand long-context input sequences but fail to capture a critical dimension: the generation of high-quality long-form text. Applications... | Ming Shan Hee, Roy KaWei Lee, Yuhao Wu, Zhiqiang Hu |  |
| 2892 |  |  [Temporal Reasoning Transfer from Text to Video](https://openreview.net/forum?id=sHAvMp5J4R) |  | 0 | Video Large Language Models (Video LLMs) have shown promising capabilities in video comprehension, yet they struggle with tracking temporal changes and reasoning about temporal relationships. While previous research attributed this limitation to the ineffective temporal encoding of visual inputs,... | Chenxin An, Lean Wang, Lei Li, Lingpeng Kong, Linli Yao, Peiyuan Zhang, Qi Liu, Xu Sun, Yuanxin Liu |  |
| 2893 |  |  [Interpreting Language Reward Models via Contrastive Explanations](https://openreview.net/forum?id=i8IwcQBi74) |  | 0 | Reward models (RMs) are a crucial component in the alignment of large language models’ (LLMs) outputs with human values. RMs approximate human preferences over possible LLM responses to the same prompt by predicting and comparing reward scores. However, as they are typically modified versions of... | Freddy Lécué, Junqi Jiang, Manuela Veloso, Saumitra Mishra, Tom Bewley |  |
| 2894 |  |  [Studying the Interplay Between the Actor and Critic Representations in Reinforcement Learning](https://openreview.net/forum?id=tErHYBGlWc) |  | 0 | Extracting relevant information from a stream of high-dimensional observations is a central challenge for deep reinforcement learning agents. Actor-critic algorithms add further complexity to this challenge, as it is often unclear whether the same information will be relevant to both the actor and... | Christopher G. Lucas, David Abel, Pablo Samuel Castro, Prakash Panangaden, Samuel Garcin, Stefano V. Albrecht, Trevor McInroe |  |
| 2895 |  |  [IV-mixed Sampler: Leveraging Image Diffusion Models for Enhanced Video Synthesis](https://openreview.net/forum?id=ImpeMDJfVL) |  | 0 | Exploring suitable solutions to improve performance by increasing the computational cost of inference in visual diffusion models is a highly promising direction. Sufficient prior studies have demonstrated that correctly scaling up computation in the sampling process can successfully lead to... | Bai Lichen, Haoyi Xiong, Shitong Shao, Zeke Xie, Zikai Zhou |  |
| 2896 |  |  [GLOMA: Global Video Text Spotting with Morphological Association](https://openreview.net/forum?id=tMKibc9Uxi) |  | 0 | Video Text Spotting (VTS) is a fundamental visual task that aims to predict the trajectories and content of texts in a video. Previous works usually conduct local associations and apply IoU-based distance and complex post-processing procedures to boost performance, ignoring the abundant temporal... | Can Huang, Han Wang, Yang Li, Yanjie Wang |  |
| 2897 |  |  [Robust Representation Consistency Model via Contrastive Denoising](https://openreview.net/forum?id=armbJRJdrH) |  | 0 | Robustness is essential for deep neural networks, especially in security-sensitive applications. To this end, randomized smoothing provides theoretical guarantees for certifying robustness against adversarial perturbations. Recently, diffusion models have been successfully employed for randomized... | Anima Anandkumar, Chaowei Xiao, Jiachen Lei, Jiongxiao Wang, Julius Berner, Jun Zhu, Kui Ren, Zhongjie Ba, Zhongzhu Chen |  |
| 2898 |  |  [CtD: Composition through Decomposition in Emergent Communication](https://openreview.net/forum?id=KlalQu2423) |  | 0 | Compositionality is a cognitive mechanism that allows humans to systematically combine known concepts in novel ways. This study demonstrates how artificial neural agents acquire and utilize compositional generalization to describe previously unseen images. Our method, termed \\`\\`Composition... | Boaz Carmeli, Ron Meir, Yonatan Belinkov |  |
| 2899 |  |  [Geometry of Lightning Self-Attention: Identifiability and Dimension](https://openreview.net/forum?id=XtY3xYQWcW) |  | 0 | We consider function spaces defined by self-attention networks without normalization, and theoretically analyze their geometry. Since these networks are polynomial, we rely on tools from algebraic geometry. In particular, we study the identifiability of deep attention by providing a description of... | Giovanni Luca Marchetti, Kathlén Kohn, Nathan W. Henry |  |
| 2900 |  |  [PharmacoMatch: Efficient 3D Pharmacophore Screening via Neural Subgraph Matching](https://openreview.net/forum?id=27Qk18IZum) |  | 0 | The increasing size of screening libraries poses a significant challenge for the development of virtual screening methods for drug discovery, necessitating a re-evaluation of traditional approaches in the era of big data. Although 3D pharmacophore screening remains a prevalent technique, its... | Daniel Rose, Oliver Wieder, Thierry Langer, Thomas Seidel |  |
| 2901 |  |  [Animate Your Thoughts: Reconstruction of Dynamic Natural Vision from Human Brain Activity](https://openreview.net/forum?id=BpfsxFqhGa) |  | 0 | Reconstructing human dynamic vision from brain activity is a challenging task with great scientific significance. Although prior video reconstruction methods have made substantial progress, they still suffer from several limitations, including: (1) difficulty in simultaneously reconciling semantic... | Changde Du, Chong Wang, Huiguang He, Liuyun Jiang, Xuanliu Zhu, Xujin Li, Yizhuo Lu |  |
| 2902 |  |  [Mining your own secrets: Diffusion Classifier Scores for Continual Personalization of Text-to-Image Diffusion Models](https://openreview.net/forum?id=hUdLs6TqZL) |  | 0 | Personalized text-to-image diffusion models have grown popular for their ability to efficiently acquire a new concept from user-defined text descriptions and a few images. However, in the real world, a user may wish to personalize a model on multiple concepts but one at a time, with no access to... | Christian Simon, Dong Gong, Lina Yao, Masato Ishii, Mengjie Zhao, Muhammad Jehanzeb Mirza, Saurav Jha, Shiqi Yang, Shusuke Takahashi, Yuki Mitsufuji |  |
| 2903 |  |  [On the Optimal Memorization Capacity of Transformers](https://openreview.net/forum?id=UGVYezlLcZ) |  | 0 | Recent research in the field of machine learning has increasingly focused on the memorization capacity of Transformers, but how efficient they are is not yet well understood. We demonstrate that Transformers can memorize labels with $\tilde{O}(\sqrt{N})$ parameters in a next-token prediction... | Issei Sato, Tokio Kajitsuka |  |
| 2904 |  |  [Edge-aware Image Smoothing with Relative Wavelet Domain Representation](https://openreview.net/forum?id=0UO1mH3Iwv) |  | 0 | Image smoothing is a fundamental technique in image processing, designed to eliminate perturbations and textures while preserving dominant structures. It plays a pivotal role in numerous high-level computer vision tasks. More recently, both traditional and deep learning-based smoothing methods have... | Fang Li, Huiqing Qi, Tingting Li, Xiaoliu Luo |  |
| 2905 |  |  [Action Sequence Augmentation for Action Anticipation](https://openreview.net/forum?id=f3CdjpPkSq) |  | 0 | Action anticipation models require an understanding of temporal action patterns and dependencies to predict future actions from previous events. The key challenges arise from the vast number of possible action sequences, given the flexibility in action ordering and the interleaving of multiple... | Deepu Rajan, Yihui Qiu |  |
| 2906 |  |  [Optimal Brain Apoptosis](https://openreview.net/forum?id=88rjm6AXoC) |  | 0 | The increasing complexity and parameter count of Convolutional Neural Networks (CNNs) and Transformers pose challenges in terms of computational efficiency and resource demands. Pruning has been identified as an effective strategy to address these challenges by removing redundant elements such as... | Chenming Hu, Delei Kong, Jiaxu Wang, Junjie Jiang, Mingyuan Sun, Renjing Xu, Yuetong Fang, Zheng Fang |  |
| 2907 |  |  [RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards](https://openreview.net/forum?id=Pnktu2PBXD) |  | 0 | Retrieval-Augmented Generation (RAG) has proven its effectiveness in mitigating hallucinations in Large Language Models (LLMs) by retrieving knowledge from external resources. To adapt LLMs for the RAG systems, current approaches use instruction tuning to optimize LLMs, improving their ability to... | Chenyan Xiong, Ge Yu, Hao Chen, Maosong Sun, Sen Mei, Shi Yu, Shuo Wang, Xinze Li, Yukun Yan, Zhenghao Liu, Zheni Zeng, Zhiyuan Liu |  |
| 2908 |  |  [Selective Aggregation for Low-Rank Adaptation in Federated Learning](https://openreview.net/forum?id=iX3uESGdsO) |  | 0 | We investigate LoRA in federated learning through the lens of the asymmetry analysis of the learned $A$ and $B$ matrices. In doing so, we uncover that $A$ matrices are responsible for learning general knowledge, while $B$ matrices focus on capturing client-specific knowledge. Based on this finding,... | Feifei Wang, Huijie Fan, Liangqiong Qu, Pengxin Guo, Shuang Zeng, Yanran Wang |  |
| 2909 |  |  [O(d/T) Convergence Theory for Diffusion Probabilistic Models under Minimal Assumptions](https://openreview.net/forum?id=4EjdYiNRzE) |  | 0 | Score-based diffusion models, which generate new data by learning to reverse a diffusion process that perturbs data from the target distribution into noise, have achieved remarkable success across various generative tasks. Despite their superior empirical performance, existing theoretical... | Gen Li, Yuling Yan |  |
| 2910 |  |  [DebGCD: Debiased Learning with Distribution Guidance for Generalized Category Discovery](https://openreview.net/forum?id=9B8o9AxSyb) |  | 0 | In this paper, we tackle the problem of Generalized Category Discovery (GCD). Given a dataset containing both labelled and unlabelled images, the objective is to categorize all images in the unlabelled subset, irrespective of whether they are from known or unknown classes. In GCD, an inherent label... | Kai Han, Yuanpei Liu |  |
| 2911 |  |  [GeoX: Geometric Problem Solving Through Unified Formalized Vision-Language Pre-training](https://openreview.net/forum?id=6RiBl5sCDF) |  | 0 | Despite their proficiency in general tasks, Multi-modal Large Language Models (MLLMs) struggle with automatic Geometry Problem Solving (GPS), which demands understanding diagrams, interpreting symbols, and performing complex reasoning. This limitation arises from their pre-training on natural... | Bin Wang, Bo Zhang, Botian Shi, Conghui He, Hancheng Ye, Hongbin Zhou, Jiakang Yuan, Junchi Yan, Mingsheng Li, Renqiu Xia, Tao Chen, Tianshuo Peng, Wenjie Wu, Xiangchao Yan, Xinyu Cai |  |
| 2912 |  |  [3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary Affordance Detection in 3D Worlds](https://openreview.net/forum?id=GThTiuXgDC) |  | 0 | 3D Affordance detection is a challenging problem with broad applications on various robotic tasks. Existing methods typically formulate the detection paradigm as a label-based semantic segmentation task. This paradigm relies on predefined labels and lacks the ability to comprehend complex natural... | Hengshuo Chu, Jianye Hao, Liqiang Nie, Qi Lv, Xiang Deng, Xiaoyang Chen, Yinchuan Li |  |
| 2913 |  |  [OmniKV: Dynamic Context Selection for Efficient Long-Context LLMs](https://openreview.net/forum?id=ulCAPXYXfa) |  | 0 | During the inference phase of Large Language Models (LLMs) with long context, a substantial portion of GPU memory is allocated to the KV cache, with memory usage increasing as the sequence length grows. To mitigate the GPU memory footprint associate with KV cache, some previous studies have... | Bo Zheng, Jitai Hao, Jun Yu, Sheng Guo, Tian Wang, Xin Xin, Yuke Zhu, Zhaochun Ren |  |
| 2914 |  |  [Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement Learning](https://openreview.net/forum?id=X2x2DuGIbx) |  | 0 | Similar to other machine learning frameworks, Offline Reinforcement Learning (RL) is shown to be vulnerable to poisoning attacks, due to its reliance on externally sourced datasets, a vulnerability that is exacerbated by its sequential nature. To mitigate the risks posed by RL poisoning, we extend... | Andrew Craig Cullen, Benjamin I. P. Rubinstein, Paul Montague, Sarah Monazam Erfani, Shijie Liu |  |
| 2915 |  |  [Two Sparse Matrices are Better than One: Sparsifying Neural Networks with Double Sparse Factorization](https://openreview.net/forum?id=DwiwOcK1B7) |  | 0 | Neural networks are often challenging to work with due to their large size and complexity. To address this, various methods aim to reduce model size by sparsifying or decomposing weight matrices, such as magnitude pruning and low-rank or block-diagonal factorization. In this work, we present Double... | Vladimír Boza, Vladimír Macko |  |
| 2916 |  |  [Tuning Timestep-Distilled Diffusion Model Using Pairwise Sample Optimization](https://openreview.net/forum?id=fXnE4gB64o) |  | 0 | Recent advancements in timestep-distilled diffusion models have enabled high-quality image generation that rivals non-distilled multi-step models, but with significantly fewer inference steps. While such models are attractive for applications due to the low inference cost and latency, fine-tuning... | Kevin Lin, Lijuan Wang, Qiang Qiu, Ze Wang, Zhengyuan Yang, Zichen Miao, Zicheng Liu |  |
| 2917 |  |  [LoCA: Location-Aware Cosine Adaptation for Parameter-Efficient Fine-Tuning](https://openreview.net/forum?id=4NRjdISWby) |  | 0 | Low-rank adaptation (LoRA) has become a prevalent method for adapting pre-trained large language models to downstream tasks. However, the simple low-rank decomposition form may constrain the optimization flexibility. To address this limitation, we introduce Location-aware Cosine Adaptation (LoCA),... | Changliang Zou, Jingjing Li, Ke Lu, Liuhua Peng, Mingming Gong, Tingjin Chu, Yinjie Min, Zhekai Du |  |
| 2918 |  |  [Build-A-Scene: Interactive 3D Layout Control for Diffusion-Based Image Generation](https://openreview.net/forum?id=gg6dPtdC1C) |  | 0 | We propose a diffusion-based approach for Text-to-Image (T2I) generation with interactive 3D layout control. Layout control has been widely studied to alleviate the shortcomings of T2I diffusion models in understanding objects' placement and relationships from text descriptions. Nevertheless,... | Abdelrahman Eldesokey, Peter Wonka |  |
| 2919 |  |  [Learning Dynamics of Deep Matrix Factorization Beyond the Edge of Stability](https://openreview.net/forum?id=J4Dvxv7WnG) |  | 0 | Deep neural networks trained using gradient descent with a fixed learning rate $\eta$ often operate in the regime of \`\`edge of stability'' (EOS), where the largest eigenvalue of the Hessian equilibrates about the stability threshold $2/\eta$. In this work, we present a fine-grained analysis of... | Avrajit Ghosh, Qing Qu, Rongrong Wang, Saiprasad Ravishankar, Soo Min Kwon |  |
| 2920 |  |  [Intent3D: 3D Object Detection in RGB-D Scans Based on Human Intention](https://openreview.net/forum?id=5GgjiRzYp3) |  | 0 | In real-life scenarios, humans seek out objects in the 3D world to fulfill their daily needs or intentions. This inspires us to introduce 3D intention grounding, a new task in 3D object detection employing RGB-D, based on human intention, such as "I want something to support my back." Closely... | Jyoti Kini, Mengxue Qu, Mubarak Shah, Weitai Kang, Yan Yan, Yunchao Wei |  |
| 2921 |  |  [On the Adversarial Risk of Test Time Adaptation: An Investigation into Realistic Test-Time Data Poisoning](https://openreview.net/forum?id=7893vsQenk) |  | 0 | Test-time adaptation (TTA) updates the model weights during the inference stage using testing data to enhance generalization. However, this practice exposes TTA to adversarial risks. Existing studies have shown that when TTA is updated with crafted adversarial test samples, also known as test-time... | ChuanSheng Foo, Kui Jia, Nanqing Liu, Xulei Yang, Xun Xu, Yongyi Su, Yushu Li |  |
| 2922 |  |  [GenXD: Generating Any 3D and 4D Scenes](https://openreview.net/forum?id=1ThYY28HXg) |  | 0 | Recent developments in 2D visual generation have been remarkably successful. However, 3D and 4D generation remain challenging in real-world applications due to the lack of large-scale 4D data and effective model design. In this paper, we propose to jointly investigate general 3D and 4D generation... | ChungChing Lin, Gim Hee Lee, Jianfeng Wang, Kevin Lin, Lijuan Wang, Linjie Li, Yuyang Zhao, Zhengyuan Yang, Zhiwen Yan |  |
| 2923 |  |  [Latent Action Pretraining from Videos](https://openreview.net/forum?id=VYOe2eBQeh) |  | 0 | We introduce Latent Action Pretraining for general Action models (LAPA), the first unsupervised method for pretraining Vision-Language-Action (VLA) models without ground-truth robot action labels. Existing Vision-Language-Action models require action labels typically collected by human... | Ajay Mandlekar, Baolin Peng, Bill Yuchen Lin, Byeongguk Jeon, Dieter Fox, Jianfeng Gao, Jianwei Yang, Joel Jang, Kimin Lee, Lars Liden, Luke Zettlemoyer, Minjoon Seo, Reuben Tan, Se June Joo, Seonghyeon Ye, YuWei Chao |  |
| 2924 |  |  [Open-CK: A Large Multi-Physics Fields Coupling benchmarks in Combustion Kinetics](https://openreview.net/forum?id=A23C57icJt) |  | 0 | In this paper, we use the Fire Dynamics Simulator (FDS) combined with the {\fontfamily{lmtt}\selectfont \textit{supercomputer}} support to create a \textbf{C}ombustion \textbf{K}inetics (CK) dataset for machine learning and scientific research. This dataset captures the development of fires in... | Fan Xu, Hao Wu, Junyuan Mao, Kun Wang, Qingsong Wen, Yang Wang, Yuxuan Liang, Zaige Fei |  |
| 2925 |  |  [Motion-Agent: A Conversational Framework for Human Motion Generation with LLMs](https://openreview.net/forum?id=AvOhBgsE5R) |  | 0 | While previous approaches to 3D human motion generation have achieved notable success, they often rely on extensive training and are limited to specific tasks. To address these challenges, we introduce \*\*Motion-Agent\*\*, an efficient conversational framework designed for general human motion... | ChiKeung Tang, Qi Wu, Xinhang Liu, Yifan Wang, YuWing Tai, Yubo Zhao |  |
| 2926 |  |  [Morphing Tokens Draw Strong Masked Image Models](https://openreview.net/forum?id=d7q9IGj2p0) |  | 0 | Masked image modeling (MIM) has emerged as a promising approach for pre-training Vision Transformers (ViTs). MIMs predict masked tokens token-wise to recover target signals that are tokenized from images or generated by pre-trained models like vision-language models. While using tokenizers or... | Byeongho Heo, Dongyoon Han, Taekyung Kim |  |
| 2927 |  |  [Longhorn: State Space Models are Amortized Online Learners](https://openreview.net/forum?id=8jOqCcLzeO) |  | 0 | The most fundamental capability of modern AI methods such as Large Language Models (LLMs) is the ability to predict the next token in a long sequence of tokens, known as “sequence modeling.” Although the Transformers model is the current dominant approach to sequence modeling, its quadratic... | Bo Liu, Lemeng Wu, Peter Stone, Qiang Liu, Rui Wang, Yihao Feng |  |
| 2928 |  |  [CaPo: Cooperative Plan Optimization for Efficient Embodied Multi-Agent Cooperation](https://openreview.net/forum?id=KRv9NubipP) |  | 0 | In this work, we address the cooperation problem among large language model (LLM) based embodied agents, where agents must cooperate to achieve a common goal. Previous methods often execute actions extemporaneously and incoherently, without long-term strategic and cooperative planning, leading to... | AhHwee Tan, Cees G. M. Snoek, Efstratios Gavves, JanJakob Sonke, Jie Liu, Pan Zhou, Yingjun Du |  |
| 2929 |  |  [SMITE: Segment Me In TimE](https://openreview.net/forum?id=KW6B6s1X82) |  | 0 | Segmenting an object in a video presents significant challenges. Each pixel must be accurately labeled, and these labels must remain consistent across frames. The difficulty increases when the segmentation is with arbitrary granularity, meaning the number of segments can vary arbitrarily, and masks... | Ali MahdaviAmiri, Amirhossein Alimohammadi, Andrea Tagliasacchi, Ghassan Hamarneh, Saeid Asgari Taghanaki, Sauradip Nag |  |
| 2930 |  |  [Interactive Speculative Planning: Enhance Agent Efficiency through Co-design of System and User Interface](https://openreview.net/forum?id=BwR8t91yqh) |  | 0 | Agents, as user-centric tools, are increasingly deployed for human task delegation, assisting with a broad spectrum of requests by generating thoughts, engaging with user proxies, and producing action plans. However, agents based on large language models often face substantial planning latency due... | Chi Wang, Jagannath Shashank Subramanya Sai Vadrevu, Mengting Wan, Ryan Nadel, Wenyue Hua, Yongfeng Zhang |  |
| 2931 |  |  [MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models](https://openreview.net/forum?id=s5epFPdIW6) |  | 0 | Artificial Intelligence (AI) has demonstrated significant potential in healthcare, particularly in disease diagnosis and treatment planning. Recent progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new possibilities for interactive diagnostic tools. However, these models... | Haoran Li, Huaxiu Yao, James Zou, Kangyu Zhu, Linjun Zhang, Peng Xia, Sheng Wang, Tianze Wang, Weijia Shi |  |
| 2932 |  |  [Controllable Satellite-to-Street-View Synthesis with Precise Pose Alignment and Zero-Shot Environmental Control](https://openreview.net/forum?id=f92M45YRfh) |  | 0 | Generating street-view images from satellite imagery is a challenging task, particularly in maintaining accurate pose alignment and incorporating diverse environmental conditions. While diffusion models have shown promise in generative tasks, their ability to maintain strict pose alignment... | Jianfeng Lu, Qiwei Wang, Xianghui Ze, Yujiao Shi, Zhenbo Song |  |
| 2933 |  |  [NoVo: Norm Voting off Hallucinations with Attention Heads in Large Language Models](https://openreview.net/forum?id=yaOe2xBcLC) |  | 0 | Hallucinations in Large Language Models (LLMs) remain a major obstacle, particularly in high-stakes applications where factual accuracy is critical. While representation editing and reading methods have made strides in reducing hallucinations, their heavy reliance on specialised tools and training... | Dacheng Tao, Sen Zhang, Siyuan Liang, Yibing Zhan, Zheng Yi Ho |  |
| 2934 |  |  [CPSample: Classifier Protected Sampling for Guarding Training Data During Diffusion](https://openreview.net/forum?id=LIBLIlk5M9) |  | 0 | Diffusion models have a tendency to exactly replicate their training data, especially when trained on small datasets. Most prior work has sought to mitigate this problem by imposing differential privacy constraints or masking parts of the training data, resulting in a notable substantial decrease... | Felix Petersen, Frederick Vu, Hao Sun, Jiaqi Han, Joshua Kazdan, Stefano Ermon |  |
| 2935 |  |  [Leveraging Flatness to Improve Information-Theoretic Generalization Bounds for SGD](https://openreview.net/forum?id=pSdE7PIA64) |  | 0 | Information-theoretic (IT) generalization bounds have been used to study the generalization of learning algorithms. These bounds are intrinsically data- and algorithm-dependent so that one can exploit the properties of data and algorithm to derive tighter bounds. However, we observe that although... | Jian Zhang, Lei Qi, Yang Gao, Yinghuan Shi, Yisen Wang, Ze Peng |  |
| 2936 |  |  [BTBS-LNS: Binarized-Tightening, Branch and Search on Learning LNS Policies for MIP](https://openreview.net/forum?id=siHHqDDzvS) |  | 0 | Learning to solve large-scale Mixed Integer Program (MIP) problems is an emerging research topic, and policy learning-based Large Neighborhood Search (LNS) has been a popular paradigm. However, the explored space of LNS policy is often limited even in the training phase, making the learned policy... | Changwen Zhang, Hao Yuan, Junchi Yan, Liming Gong, Wenli Ouyang, Yong Sun |  |
| 2937 |  |  [Extending Mercer's expansion to indefinite and asymmetric kernels](https://openreview.net/forum?id=jZwwMxG8PO) |  | 0 | Mercer's expansion and Mercer's theorem are cornerstone results in kernel theory. While the classical Mercer's theorem only considers continuous symmetric positive definite kernels, analogous expansions are effective in practice for indefinite and asymmetric kernels. In this paper we extend... | Alex Townsend, Sungwoo Jeong |  |
| 2938 |  |  [A Statistical Approach for Controlled Training Data Detection](https://openreview.net/forum?id=XAN8G0rvoB) |  | 0 | Detecting training data for large language models (LLMs) is receiving growing attention, especially in applications requiring high reliability. While numerous efforts have been made to address this issue, they typically focus on accuracy without ensuring controllable results. To fill this gap, we... | Dacheng Tao, Hong Chen, Yingjie Wang, Zheng Zhang, Zirui Hu |  |
| 2939 |  |  [Discriminating image representations with principal distortions](https://openreview.net/forum?id=ugXGFCS6HK) |  | 0 | Image representations (artificial or biological) are often compared in terms of their global geometric structure; however, representations with similar global structure can have strikingly different local geometries. Here, we propose a framework for comparing a set of image representations in terms... | Alex H. Williams, David Lipshutz, Eero P. Simoncelli, Jenelle Feather, Sarah E. Harvey |  |
| 2940 |  |  [PixWizard: Versatile Image-to-Image Visual Assistant with Open-Language Instructions](https://openreview.net/forum?id=xuQSp75HmP) |  | 0 | This paper presents a versatile image-to-image visual assistant, PixWizard, designed for image generation, manipulation, and translation based on free-from language instructions. To this end, we tackle a variety of vision tasks into a unified image-text-to-image generation framework and curate an... | Hongsheng Li, Junlin Xie, Le Zhuo, Peng Gao, Renrui Zhang, Shitian Zhao, Siyuan Huang, Weifeng Lin, Xinyu Wei |  |
| 2941 |  |  [SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation](https://openreview.net/forum?id=uQjySppU9x) |  | 0 | Methods for image-to-video generation have achieved impressive, photo-realistic quality. However, adjusting specific elements in generated videos, such as object motion or camera movement, is often a tedious process of trial and error, e.g., involving re-generating videos with different random... | David B. Lindell, Igor Gilitschenski, Koichi Namekata, Sherwin Bahmani, Yash Kant, Ziyi Wu |  |
| 2942 |  |  [PvNeXt: Rethinking Network Design and Temporal Motion for Point Cloud Video Recognition](https://openreview.net/forum?id=ZsU52Zkzjr) |  | 0 | Point cloud video perception has become an essential task for the realm of 3D vision. Current 4D representation learning techniques typically engage in iterative processing coupled with dense query operations. Although effective in capturing temporal features, this approach leads to substantial... | Jianan Li, Jie Wang, Lihe Ding, Long Bai, Tingfa Xu, Xinjie Zhang |  |
| 2943 |  |  [Beyond Graphs: Can Large Language Models Comprehend Hypergraphs?](https://openreview.net/forum?id=28qOQwjuma) |  | 0 | Existing benchmarks like NLGraph and GraphQA evaluate LLMs on graphs by focusing mainly on pairwise relationships, overlooking the high-order correlations found in real-world data. Hypergraphs, which can model complex beyond-pairwise relationships, offer a more robust framework but are still... | Chengwu Yang, Shaoyi Du, Shihui Ying, Xingliang Hou, Yifan Feng, Yue Gao, Zongze Wu |  |
| 2944 |  |  [How Much is a Noisy Image Worth? Data Scaling Laws for Ambient Diffusion](https://openreview.net/forum?id=qZwtPEw2qN) |  | 0 | The quality of generative models depends on the quality of the data they are trained on. Creating large-scale, high-quality datasets is often expensive and sometimes impossible, e.g.~in certain scientific applications where there is no access to clean data due to physical or instrumentation... | Constantinos Daskalakis, Giannis Daras, Yeshwanth Cherapanamjeri |  |
| 2945 |  |  [High-dimension Prototype is a Better Incremental Object Detection Learner](https://openreview.net/forum?id=6T8czSBWce) |  | 0 | Incremental object detection (IOD), surpassing simple classification, requires the simultaneous overcoming of catastrophic forgetting in both recognition and localization tasks, primarily due to the significantly higher feature space complexity. Integrating Knowledge Distillation (KD) would... | Guodong Wang, Jiahuan Zhou, Liqun Chen, Luxin Yan, Sheng Zhong, Tao Zhang, Tianming Zhao, Xu Zou, Yanjie Wang |  |
| 2946 |  |  [HG-Adapter: Improving Pre-Trained Heterogeneous Graph Neural Networks with Dual Adapters](https://openreview.net/forum?id=AEglX9CHFN) |  | 0 | The "pre-train, prompt-tuning'' paradigm has demonstrated impressive performance for tuning pre-trained heterogeneous graph neural networks (HGNNs) by mitigating the gap between pre-trained models and downstream tasks. However, most prompt-tuning-based works may face at least two limitations: (i)... | Runpeng Yu, Xiaofeng Zhu, Xinchao Wang, Yujie Mo |  |
| 2947 |  |  [HyperPLR: Hypergraph Generation through Projection, Learning, and Reconstruction](https://openreview.net/forum?id=TYnne6Pa35) |  | 0 | Hypergraphs are essential in modeling higher-order complex networks, excelling in representing group interactions within real-world contexts. This is particularly evident in collaboration networks, where they facilitate the capture of groupwise polyadic patterns, extending beyond traditional... | Tianshu Yu, Weihuang Wen |  |
| 2948 |  |  [Continuity-Preserving Convolutional Autoencoders for Learning Continuous Latent Dynamical Models from Images](https://openreview.net/forum?id=MxALfOAnXv) |  | 0 | Continuous dynamical systems are cornerstones of many scientific and engineering disciplines. While machine learning offers powerful tools to model these systems from trajectory data, challenges arise when these trajectories are captured as images, resulting in pixel-level observations that are... | Aiqing Zhu, Qianxiao Li, Yuting Pan |  |
| 2949 |  |  [SINGAPO: Single Image Controlled Generation of Articulated Parts in Objects](https://openreview.net/forum?id=OdMqKszKSd) |  | 0 | We address the challenge of creating 3D assets for household articulated objects from a single image. Prior work on articulated object creation either requires multi-view multi-state input, or only allows coarse control over the generation process. These limitations hinder the scalability and... | Ali Mahdavi Amiri, Angel X. Chang, Denys Iliash, Jiayi Liu, Manolis Savva |  |
| 2950 |  |  [Learning to Select Nodes in Branch and Bound with Sufficient Tree Representation](https://openreview.net/forum?id=gyvYKLEm8t) |  | 0 | Branch-and-bound methods are pivotal in solving Mixed Integer Linear Programming (MILP), where the challenge of node selection arises, necessitating the prioritization of different regions of the space for subsequent exploration. While machine learning techniques have been proposed to address this,... | Feng Wu, Shaoang Li, Shuli Zeng, Sijia Zhang, Xiangyang Li |  |
| 2951 |  |  [Weakly Supervised Video Scene Graph Generation via Natural Language Supervision](https://openreview.net/forum?id=GQgPj1H4pO) |  | 0 | Existing Video Scene Graph Generation (VidSGG) studies are trained in a fully supervised manner, which requires all frames in a video to be annotated, thereby incurring high annotation cost compared to Image Scene Graph Generation (ImgSGG). Although the annotation cost of VidSGG can be alleviated... | Chanyoung Park, Donghyun Kim, Jaehyeong Jeon, Jinyoung Moon, Kanghoon Yoon, Kibum Kim, Yeonjun In |  |
| 2952 |  |  [GameArena: Evaluating LLM Reasoning through Live Computer Games](https://openreview.net/forum?id=SeQ8l8xo1r) |  | 0 | Evaluating the reasoning abilities of large language models (LLMs) is challenging. Existing benchmarks often depend on static datasets, which are vulnerable to data contamination and may get saturated over time, or on binary live human feedback that conflates reasoning with other abilities. As the... | Anze Xie, Hao Zhang, Haojian Jin, Ion Stoica, Lanxiang Hu, Nan Jiang, Qiyu Li |  |
| 2953 |  |  [T2V-Turbo-v2: Enhancing Video Model Post-Training through Data, Reward, and Conditional Guidance Design](https://openreview.net/forum?id=BZwXMqu4zG) |  | 0 | In this paper, we focus on enhancing a diffusion-based text-to-video (T2V) model during the post-training phase by distilling a highly capable consistency model from a pretrained T2V model. Our proposed method, T2V-Turbo-v2, introduces a significant advancement by integrating various supervision... | Jiachen Li, Jian Zheng, Qian Long, Robinson Piramuthu, Wenhu Chen, William Yang Wang, Xiaofeng Gao |  |
| 2954 |  |  [Diffusion Policy Policy Optimization](https://openreview.net/forum?id=mEpqHvbD2h) |  | 0 | We introduce Diffusion Policy Policy Optimization, DPPO, an algorithmic framework including best practices for fine-tuning diffusion-based policies (e.g. Diffusion Policy) in continuous control and robot learning tasks using the policy gradient (PG) method from reinforcement learning (RL). PG... | Allen Z. Ren, Anirudha Majumdar, Anthony Simeonov, Benjamin Burchfiel, Hongkai Dai, Justin Lidard, Lars Lien Ankile, Max Simchowitz, Pulkit Agrawal |  |
| 2955 |  |  [Horizon Generalization in Reinforcement Learning](https://openreview.net/forum?id=BH8Nrt2dPf) |  | 0 | We study goal-conditioned RL through the lens of generalization, but not in the traditional sense of random augmentations and domain randomization. Rather, we aim to learn goal-directed policies that generalize with respect to the horizon: after training to reach nearby goals (which are easy to... | Benjamin Eysenbach, Catherine Ji, Vivek Myers |  |
| 2956 |  |  [OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision](https://openreview.net/forum?id=Hlm0cga0sv) |  | 0 | Instruction-guided image editing methods have demonstrated significant potential by training diffusion models on automatically synthesized or manually annotated image editing pairs. However, these methods remain far from practical, real-life applications. We identify three primary challenges... | Cong Wei, Ge Zhang, Weiming Ren, Wenhu Chen, Xeron Du, Zheyang Xiong |  |
| 2957 |  |  [From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory Representation for LLMs](https://openreview.net/forum?id=moXtEmCleY) |  | 0 | Recent advancements in large language models have significantly improved their context windows, yet challenges in effective long-term memory management remain. We introduce MemTree, an algorithm that leverages a dynamic, tree-structured memory representation to optimize the organization, retrieval,... | Alireza Rezazadeh, Wei Wei, Yujia Bao, Zichao Li |  |
| 2958 |  |  [AstroCompress: A benchmark dataset for multi-purpose compression of astronomical data](https://openreview.net/forum?id=kQCHCkNk7s) |  | 0 | The site conditions that make astronomical observatories in space and on the ground so desirable---cold and dark---demand a physical remoteness that leads to limited data transmission capabilities. Such transmission limitations directly bottleneck the amount of data acquired and in an era of costly... | Joshua S. Bloom, Peter Xiangyuan Ma, Rithwik Sudharsan, Ruihan Yang, Stephan Mandt, Tuan Truong, Yibo Yang |  |
| 2959 |  |  [CURIE: Evaluating LLMs on Multitask Scientific Long-Context Understanding and Reasoning](https://openreview.net/forum?id=jw2fC6REUB) |  | 0 | Scientific problem-solving involves synthesizing information while applying expert knowledge. We introduce CURIE, a scientific long-Context Understanding, Reasoning, and Information Extraction benchmark to measure the potential of Large Language Models (LLMs) in scientific problem-solving and... | Brian Rohr, Dan Morris, Drew Purves, Elise Kleeman, Gowoon Cheon, Haining Pan, Hao Cui, Maria Tikhanovskaya, Martyna Beata Plomecka, Michael J. Statt, Nayantara Mudur, Paul Raccuglia, Peter Christian Norgaard, Philippe Faist, Pranesh Srinivasan, Shutong Li, Victor V. Albert, Xuejian Ma, Yasaman Bahri, Zahra Shamsi, et al. |  |
| 2960 |  |  [Truncated Consistency Models](https://openreview.net/forum?id=ZYDEJEvCbv) |  | 0 | Consistency models have recently been introduced to accelerate the generation speed of diffusion models by directly predicting the solution (data) of the probability flow ODE (PF ODE) from initial noise. However, the training of consistency models requires learning to map all intermediate points... | Arash Vahdat, Giulia Fanti, Karsten Kreis, Sangyun Lee, Tomas Geffner, Weili Nie, Yilun Xu |  |
| 2961 |  |  [Elucidating the Preconditioning in Consistency Distillation](https://openreview.net/forum?id=55pCDKiS8B) |  | 0 | Consistency distillation is a prevalent way for accelerating diffusion models adopted in consistency (trajectory) models, in which a student model is trained to traverse backward on the probability flow (PF) ordinary differential equation (ODE) trajectory determined by the teacher model.... | Fan Bao, Guande He, Jianfei Chen, Jun Zhu, Kaiwen Zheng |  |
| 2962 |  |  [Diffusion Bridge Implicit Models](https://openreview.net/forum?id=eghAocvqBk) |  | 0 | Denoising diffusion bridge models (DDBMs) are a powerful variant of diffusion models for interpolating between two arbitrary paired distributions given as endpoints. Despite their promising performance in tasks like image translation, DDBMs require a computationally intensive sampling process that... | Fan Bao, Guande He, Jianfei Chen, Jun Zhu, Kaiwen Zheng |  |
| 2963 |  |  [Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling](https://openreview.net/forum?id=CTC7CmirNr) |  | 0 | Masked diffusion models (MDMs) have emerged as a popular research topic for generative modeling of discrete data, thanks to their superior performance over other discrete diffusion models, and are rivaling the auto-regressive models (ARMs) for language modeling tasks. The recent effort in... | Hanzi Mao, Jun Zhu, Kaiwen Zheng, MingYu Liu, Qinsheng Zhang, Yongxin Chen |  |
| 2964 |  |  [Synthio: Augmenting Small-Scale Audio Classification Datasets with Synthetic Data](https://openreview.net/forum?id=bR1J7SpzrD) |  | 0 | We present Synthio, a novel approach for augmenting small-scale audio classification datasets with synthetic data. Our goal is to improve audio classification accuracy with limited labeled data. Traditional data augmentation techniques, which apply artificial transformations (e.g., adding random... | Bryan Catanzaro, Dinesh Manocha, Rafael Valle, Sonal Kumar, Sreyan Ghosh, Zhifeng Kong |  |
| 2965 |  |  [CREAM: Consistency Regularized Self-Rewarding Language Models](https://openreview.net/forum?id=Vf6RDObyEF) |  | 0 | Recent self-rewarding large language models (LLM) have successfully applied LLM-as-a-Judge to iteratively improve the alignment performance without the need of human annotations for preference data. These methods commonly utilize the same LLM to act as both the policy model (which generates... | Chetan Bansal, Huaxiu Yao, Weilei He, Weitong Zhang, Xuchao Zhang, Ying Wei, Zhaoyang Wang, Zhiyuan Liang |  |
| 2966 |  |  [PICASO: Permutation-Invariant Context Composition with State Space Models](https://openreview.net/forum?id=88TC1AWV27) |  | 0 | Providing Large Language Models with relevant contextual knowledge at inference time has been shown to greatly improve the quality of their generations. This is often achieved by prepending informative passages of text, or 'contexts', retrieved from external knowledge bases to their input. However,... | Aditya Golatkar, Alessandro Achille, Luca Zancato, Matthew Trager, Stefano Soatto, Tian Yu Liu |  |
| 2967 |  |  [Swiss Army Knife: Synergizing Biases in Knowledge from Vision Foundation Models for Multi-Task Learning](https://openreview.net/forum?id=eePww5u7J3) |  | 0 | Vision Foundation Models (VFMs) have demonstrated outstanding performance on numerous downstream tasks. However, due to their inherent representation biases originating from different training paradigms, VFMs exhibit advantages and disadvantages across distinct vision tasks. Although amalgamating... | Shengcao Cao, YuXiong Wang, Yuxiang Lu |  |
| 2968 |  |  [Neural Eulerian Scene Flow Fields](https://openreview.net/forum?id=0CieWy9ONY) |  | 0 | We reframe scene flow as the task of estimating a continuous space-time ordinary differential equation (ODE) that describes motion for an entire observation sequence, represented with a neural prior. Our method, EulerFlow, optimizes this neural prior estimate against several multi-observation... | Deva Ramanan, Eric Eaton, Ishan Khatri, Joachim Pehserl, Kyle Vedder, Mehmet Kemal Kocamaz, Neehar Peri, Siyi Li, Yue Wang, Zhiding Yu |  |
| 2969 |  |  [DCT-CryptoNets: Scaling Private Inference in the Frequency Domain](https://openreview.net/forum?id=lPJUQsSIxm) |  | 0 | The convergence of fully homomorphic encryption (FHE) and machine learning offers unprecedented opportunities for private inference of sensitive data. FHE enables computation directly on encrypted data, safeguarding the entire machine learning pipeline, including data and model confidentiality.... | Arjun Roy, Kaushik Roy |  |
| 2970 |  |  [A Large-scale Training Paradigm for Graph Generative Models](https://openreview.net/forum?id=c01YB8pF0s) |  | 0 | Large Generative Models (LGMs) such as GPT, Stable Diffusion, Sora, and Suno are trained on a huge amount of texts, images, videos, and audio that are extremely diverse from numerous domains. This large-scale training paradigm on diverse well-curated data enhances the creativity and diversity of... | Danai Koutra, Franck Dernoncourt, Huiyuan Chen, Namyong Park, Nesreen K. Ahmed, Puja Trivedi, Ryan A. Rossi, Tyler Derr, Yu Wang |  |
| 2971 |  |  [Beyond Model Collapse: Scaling Up with Synthesized Data Requires Verification](https://openreview.net/forum?id=MQXrTMonT1) |  | 0 | Large Language Models (LLM) are increasingly trained on data generated by other LLMs, either because generated text and images become part of the pre-training corpus, or because synthetized data is used as a replacement for expensive human-annotation. This raises concerns about \*model collapse\*,... | Elvis Dohmatob, François Charton, Julia Kempe, Pu Yang, Yunzhen Feng |  |
| 2972 |  |  [OmniPhysGS: 3D Constitutive Gaussians for General Physics-Based Dynamics Generation](https://openreview.net/forum?id=9HZtP6I5lv) |  | 0 | Recently, significant advancements have been made in the reconstruction and generation of 3D assets, including static cases and those with physical interactions. To recover the physical properties of 3D assets, existing methods typically assume that all materials belong to a specific predefined... | Chenguo Lin, Jianjin Xu, Yadong Mu, Yuchen Lin |  |
| 2973 |  |  [Graph Neural Networks Are More Than Filters: Revisiting and Benchmarking from A Spectral Perspective](https://openreview.net/forum?id=nWdQX5hOL9) |  | 0 | Graph Neural Networks (GNNs) have achieved remarkable success in various graph-based learning tasks. While their performance is often attributed to the powerful neighborhood aggregation mechanism, recent studies suggest that other components such as non-linear layers may also significantly... | Jundong Li, Patrick Soga, Song Wang, Yinhan He, Yushun Dong |  |
| 2974 |  |  [Deep Kernel Posterior Learning under Infinite Variance Prior Weights](https://openreview.net/forum?id=usFdPd4Ghs) |  | 0 | Neal (1996) proved that infinitely wide shallow Bayesian neural networks (BNN) converge to Gaussian processes (GP), when the network weights have bounded prior variance. Cho & Saul (2009) provided a useful recursive formula for deep kernel processes for relating the covariance kernel of each layer... | Anindya Bhadra, Jorge Loría |  |
| 2975 |  |  [Differentially private optimization for non-decomposable objective functions](https://openreview.net/forum?id=F52tAK5Gbg) |  | 0 | Unsupervised pre-training is a common step in developing computer vision models and large language models. In this setting, the absence of labels requires the use of similarity-based loss functions, such as the contrastive loss, that favor minimizing the distance between similar inputs and... | Andrés Muñoz Medina, Mónica Ribero, Weiwei Kong |  |
| 2976 |  |  [Differentially private learners for heterogeneous treatment effects](https://openreview.net/forum?id=1z3SOCwst9) |  | 0 | Patient data is widely used to estimate heterogeneous treatment effects and understand the effectiveness and safety of drugs. Yet, patient data includes highly sensitive information that must be kept private. In this work, we aim to estimate the conditional average treatment effect (CATE) from... | Maresa Schröder, Stefan Feuerriegel, Valentyn Melnychuk |  |
| 2977 |  |  [Predicate Hierarchies Improve Few-Shot State Classification](https://openreview.net/forum?id=lxu8Vz6cLs) |  | 0 | State classification of objects and their relations is core to many long-horizon tasks, particularly in robot planning and manipulation. However, the combinatorial explosion of possible object-predicate combinations, coupled with the need to adapt to novel real-world environments, makes it a... | Emily Jin, Jiajun Wu, Joy Hsu |  |
| 2978 |  |  [A Watermark for Order-Agnostic Language Models](https://openreview.net/forum?id=Nlm3Xf0W9S) |  | 0 | Statistical watermarking techniques are well-established for sequentially decoded language models (LMs). However, these techniques cannot be directly applied to order-agnostic LMs, as the tokens in order-agnostic LMs are not generated sequentially. In this work, we introduce PATTERN-MARK, a... | Chenxi Liu, Heng Huang, Junfeng Guo, Ruibo Chen, Yanshuo Chen, Yihan Wu |  |
| 2979 |  |  [What Makes a Maze Look Like a Maze?](https://openreview.net/forum?id=Iz75SDbRmm) |  | 0 | A unique aspect of human visual understanding is the ability to flexibly interpret abstract concepts: acquiring lifted rules explaining what they symbolize, grounding them across familiar and unfamiliar contexts, and making predictions or reasoning about them. While off-the-shelf vision-language... | Jiajun Wu, Jiayuan Mao, Joshua B. Tenenbaum, Joy Hsu, Noah D. Goodman |  |
| 2980 |  |  [SV4D: Dynamic 3D Content Generation with Multi-Frame and Multi-View Consistency](https://openreview.net/forum?id=tJoS2d0Onf) |  | 0 | We present Stable Video 4D (SV4D) — a latent video diffusion model for multi-frame and multi-view consistent dynamic 3D content generation. Unlike previous methods that rely on separately trained generative models for video generation and novel view synthesis, we design a unified diffusion model to... | ChunHan Yao, Huaizu Jiang, Varun Jampani, Vikram Voleti, Yiming Xie |  |
| 2981 |  |  [No Free Lunch: Fundamental Limits of Learning Non-Hallucinating Generative Models](https://openreview.net/forum?id=OwNoTs2r8e) |  | 0 | Generative models have shown impressive capabilities in synthesizing high-quality outputs across various domains. However, a persistent challenge is the occurrence of "hallucinations," where the model produces outputs that are not grounded in the underlying facts. While empirical strategies have... | Ananth Grama, Changlong Wu, Wojciech Szpankowski |  |
| 2982 |  |  [Accelerating Training with Neuron Interaction and Nowcasting Networks](https://openreview.net/forum?id=cUFIil6hEG) |  | 0 | Neural network training can be accelerated when a learnable update rule is used in lieu of classic adaptive optimizers (e.g. Adam). However, learnable update rules can be costly and unstable to train and use. Recently, Jang et al. (2023) proposed a simpler approach to accelerate training based on... | Abhinav Moudgil, Boris Knyazev, Eugene Belilovsky, Guillaume Lajoie, Simon LacosteJulien |  |
| 2983 |  |  [Unbounded: A Generative Infinite Game of Character Life Simulation](https://openreview.net/forum?id=uy31tqVuNo) |  | 0 | We introduce the concept of a generative infinite game, a video game that transcends the traditional boundaries of finite, hard-coded systems by using generative models. Inspired by James P. Carse's distinction between finite and infinite games, we leverage recent advances in generative AI to... | David E. Jacobs, Jialu Li, Michael Rubinstein, Mohit Bansal, Nataniel Ruiz, Neal Wadhwa, Yael Pritch, Yuanzhen Li |  |
| 2984 |  |  [Privately Counting Partially Ordered Data](https://openreview.net/forum?id=hVTaXJ0I5M) |  | 0 | We consider differentially private counting when each data point consists of $d$ bits satisfying a partial order. Our main technical contribution is a problem-specific $K$-norm mechanism that runs in time $O(d^2)$. Experiments show that, depending on the partial order in question, our solution... | Alexander Yu, Matthew Joseph, Mónica Ribero |  |
| 2985 |  |  [Convergent Privacy Loss of Noisy-SGD without Convexity and Smoothness](https://openreview.net/forum?id=kjmLabjSE2) |  | 0 | We study the Differential Privacy (DP) guarantee of hidden-state Noisy-SGD algorithms over a bounded domain. Standard privacy analysis for Noisy-SGD assumes all internal states are revealed, which leads to a divergent R\'enyi DP bound with respect to the number of iterations. Ye & Shokri (2022) and... | Eli Chien, Pan Li |  |
| 2986 |  |  [Semialgebraic Neural Networks: From roots to representations](https://openreview.net/forum?id=zboCXnuNv7) |  | 0 | Many numerical algorithms in scientific computing—particularly in areas like numerical linear algebra, PDE simulation, and inverse problems—produce outputs that can be represented by semialgebraic functions; that is, the graph of the computed function can be described by finitely many polynomial... | Maarten V. de Hoop, Matti Lassas, S. David Mis |  |
| 2987 |  |  [OPTAMI: Global Superlinear Convergence of High-order Methods](https://openreview.net/forum?id=Cpr6Wv2tfr) |  | 0 | Second-order methods for convex optimization outperform first-order methods in terms of theoretical iteration convergence, achieving rates up to $O(k^{-5})$ for highly-smooth functions. However, their practical performance and applications are limited due to their multi-level structure and... | Alexander V. Gasnikov, Artem Agafonov, Dmitry Kamzolov, Dmitry Pasechnyuk, Martin Takác |  |
| 2988 |  |  [On the Almost Sure Convergence of the Stochastic Three Points Algorithm](https://openreview.net/forum?id=N8tJmhCw25) |  | 0 | The stochastic three points (STP) algorithm is a derivative-free optimization technique designed for unconstrained optimization problems in $\mathbb{R}^d$. In this paper, we analyze this algorithm for three classes of functions: smooth functions that may lack convexity, smooth convex functions, and... | Omar Saadi, Taha el Bakkali el Kadi |  |
| 2989 |  |  [Metric-Driven Attributions for Vision Transformers](https://openreview.net/forum?id=rGP2jbWt0l) |  | 0 | Attribution algorithms explain computer vision models by attributing the model response to pixels within the input. Existing attribution methods generate explanations by combining transformations of internal model representations such as class activation maps, gradients, attention, or relevance... | Chase Walker, Rickard Ewetz, Sumit Kumar Jha |  |
| 2990 |  |  [Maximizing the Potential of Synthetic Data: Insights from Random Matrix Theory](https://openreview.net/forum?id=I9Dsq0cVo9) |  | 0 | Synthetic data has gained attention for training large language models, but poor-quality data can harm performance (see, e.g., Shumailov et al. (2023); Seddik et al. (2024)). A potential solution is data pruning, which retains only high-quality data based on a score function (human or machine... | Ahmed Alzubaidi, Aymane El Firdoussi, Hakim Hacid, Mohamed El Amine Seddik, Réda Alami, Soufiane Hayou |  |
| 2991 |  |  [Simple Guidance Mechanisms for Discrete Diffusion Models](https://openreview.net/forum?id=i5MrJ6g5G1) |  | 0 | Diffusion models for continuous data gained widespread adoption owing to their high quality generation and control mechanisms. However, controllable diffusion on discrete data faces challenges given that continuous guidance methods do not directly apply to discrete diffusion. Here, we provide a... | Alexander M. Rush, Bernardo P. de Almeida, Guanghan Wang, Hao Phung, Hugo Dallatorre, Sam Boshar, Subham Sekhar Sahoo, Thomas Pierrot, Volodymyr Kuleshov, Yair Schiff |  |
| 2992 |  |  [STAMP: Scalable Task- And Model-agnostic Collaborative Perception](https://openreview.net/forum?id=8NdNniulYE) |  | 0 | Perception is a crucial component of autonomous driving systems. However, single-agent setups often face limitations due to sensor constraints, especially under challenging conditions like severe occlusion, adverse weather, and long-range object detection. Multi-agent collaborative perception (CP)... | Jiachen Li, Runsheng Xu, Xiangbo Gao, Zhengzhong Tu, Zhiwen Fan, Ziran Wang |  |
| 2993 |  |  [Constructing Confidence Intervals for Average Treatment Effects from Multiple Datasets](https://openreview.net/forum?id=BHFs80Jf5V) |  | 0 | Constructing confidence intervals (CIs) for the average treatment effect (ATE) from patient records is crucial to assess the effectiveness and safety of drugs. However, patient records typically come from different hospitals, thus raising the question of how multiple observational/experimental... | Dennis Frauen, Jonas Schweisthal, Konstantin Hess, Maresa Schröder, Stefan Feuerriegel, Yuxin Wang |  |
| 2994 |  |  [Learning to Help in Multi-Class Settings](https://openreview.net/forum?id=NCgTbt2j1F) |  | 0 | Deploying complex machine learning models on resource-constrained devices is challenging due to limited computational power, memory, and model retrainability. To address these limitations, a hybrid system can be established by augmenting the local model with a server-side model, where samples are... | Anand D. Sarwate, Nitya Sathyavageeswaran, Yansong Li, Yu Wu, Zeyu Dong |  |
| 2995 |  |  [Metalic: Meta-Learning In-Context with Protein Language Models](https://openreview.net/forum?id=TUKt7ag0qq) |  | 0 | Predicting the biophysical and functional properties of proteins is essential for in silico protein design. Machine learning has emerged as a promising technique for such prediction tasks. However, the relative scarcity of in vitro annotations means that these models often have little, or no,... | Jacob Beck, Juan Jose GarauLuis, Manus McAuliffe, Oliver Bent, Paul Duckworth, Shikha Surana, Thomas D. Barrett |  |
| 2996 |  |  [Can We Ignore Labels in Out of Distribution Detection?](https://openreview.net/forum?id=falBlwUsIH) |  | 0 | Out-of-distribution (OOD) detection methods have recently become more prominent, serving as a core element in safety-critical autonomous systems. One major purpose of OOD detection is to reject invalid inputs that could lead to unpredictable errors and compromise safety. Due to the cost of labeled... | Hong Yang, Qi Yu, Travis Desell |  |
| 2997 |  |  [CodePlan: Unlocking Reasoning Potential in Large Language Models by Scaling Code-form Planning](https://openreview.net/forum?id=dCPF1wlqj8) |  | 0 | Despite the remarkable success of large language models (LLMs) on traditional natural language processing tasks, their planning ability remains a critical bottleneck in tackling complex multi-step reasoning tasks. Existing approaches mainly rely on prompting or task-specific fine-tuning, often... | Hongning Wang, Jian Guan, Jiaxin Wen, Minlie Huang, Wei Wu |  |
| 2998 |  |  [OATS: Outlier-Aware Pruning Through Sparse and Low Rank Decomposition](https://openreview.net/forum?id=DLDuVbxORA) |  | 0 | The recent paradigm shift to large-scale foundation models has brought about a new era for deep learning that, while has found great success in practice, has also been plagued by prohibitively expensive costs in terms of high memory consumption and compute. To mitigate these issues, there has been... | Stephen Zhang, Vardan Papyan |  |
| 2999 |  |  [Diffusion Models Are Real-Time Game Engines](https://openreview.net/forum?id=P8pqeEkn1H) |  | 0 | We present GameNGen, the first game engine powered entirely by a neural model that also enables real-time interaction with a complex environment over long trajectories at high quality. When trained on the classic game DOOM, GameNGen extracts gameplay and uses it to generate a playable environment... | Dani Valevski, Moab Arar, Shlomi Fruchter, Yaniv Leviathan |  |
| 3000 |  |  [Efficient Perplexity Bound and Ratio Matching in Discrete Diffusion Language Models](https://openreview.net/forum?id=Mri9WIfxSm) |  | 0 | While continuous diffusion models excel in modeling continuous distributions, their application to categorical data has been less effective. Recent work has shown that ratio-matching through \*score-entropy\* within a continuous-time discrete Markov chain (CTMC) framework serves as a competitive... | Eli Waxman, Etrit Haxholli, Ogul Can, Yeti Ziya Gurbuz |  |
| 3001 |  |  [Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats](https://openreview.net/forum?id=keu6sxrPWn) |  | 0 | As large language models (LLMs) grow more powerful, they also become more difficult to trust. They could be either aligned with human intentions, or exhibit "subversive misalignment" -- introducing subtle errors that bypass safety checks. Although individual errors may not immediately cause harm,... | Akbir Khan, Ansh Radhakrishnan, Aryan Bhatt, Buck Shlegeris, Caleb Larson, Ethan Perez, He He, Henry Sleight, Jiaxin Wen, Mrinank Sharma, Shi Feng, Vivek Hebbar |  |
| 3002 |  |  [Fast Training of Sinusoidal Neural Fields via Scaling Initialization](https://openreview.net/forum?id=Sr5XaZzirA) |  | 0 | Neural fields are an emerging paradigm that represent data as continuous functions parameterized by neural networks. Despite many advantages, neural fields often have a high training cost, which prevents a broader adoption. In this paper, we focus on a popular family of neural fields, called... | Jaeho Lee, Sangyoon Lee, Taesun Yeom |  |
| 3003 |  |  [Language Models Learn to Mislead Humans via RLHF](https://openreview.net/forum?id=xJljiPE6dg) |  | 0 | Language models (LMs) can produce errors that are hard to detect for humans, especially when the task is complex. RLHF, the most popular post-training method, may exacerbate this problem: to achieve higher rewards, LMs might get better at convincing humans that they are right even when they are... | Akbir Khan, Ethan Perez, He He, Jacob Steinhardt, Jiaxin Wen, Minlie Huang, Ruiqi Zhong, Samuel R. Bowman, Shi Feng |  |
| 3004 |  |  [Towards Semantic Equivalence of Tokenization in Multimodal LLM](https://openreview.net/forum?id=n64NYyc6rQ) |  | 0 | Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in processing vision-language tasks. One of the crux of MLLMs lies in vision tokenization, which involves efficiently transforming input visual signals into feature representations that are most beneficial for LLMs.... | Hanwang Zhang, Hao Fei, Jiayi Ji, Shengqiong Wu, Shuicheng Yan, TatSeng Chua, Xiangtai Li |  |
| 3005 |  |  [ChemAgent: Self-updating Memories in Large Language Models Improves Chemical Reasoning](https://openreview.net/forum?id=kuhIqeVg0e) |  | 0 | Chemical reasoning usually involves complex, multi-step processes that demand precise calculations, where even minor errors can lead to cascading failures. Furthermore, large language models (LLMs) encounter difficulties handling domain-specific formulas, executing reasoning steps accurately, and... | Arman Cohan, Mark Gerstein, Muyang Ye, Pan Lu, Siru Ouyang, Tianyu Hu, Wangchunshu Zhou, Xiangru Tang, Xunjian Yin, Yanjun Shao, Yilun Zhao, Zhuosheng Zhang |  |
| 3006 |  |  [Node Similarities under Random Projections: Limits and Pathological Cases](https://openreview.net/forum?id=Frok9AItud) |  | 0 | Random Projections have been widely used to generate embeddings for various graph learning tasks due to their computational efficiency. The majority of applications have been justified through the Johnson-Lindenstrauss Lemma. In this paper, we take a step further and investigate how well dot... | Cassiano O. Becker, Jennifer Neville, Tvrtko Tadic |  |
| 3007 |  |  [Hadamrnn: Binary and Sparse Ternary orthogonal RNNs](https://openreview.net/forum?id=amOpepqmSl) |  | 0 | Binary and sparse ternary weights in neural networks enable faster computations and lighter representations, facilitating their use on edge devices with limited computational power. Meanwhile, vanilla RNNs are highly sensitive to changes in their recurrent weights, making the binarization and... | Armand Foucault, Franck Mamalet, François Malgouyres |  |
| 3008 |  |  [Style Outweighs Substance: Failure Modes of LLM Judges in Alignment Benchmarking](https://openreview.net/forum?id=MzHNftnAM1) |  | 0 | The release of ChatGPT in November 2022 sparked an explosion of interest in post-training and an avalanche of new preference optimization (PO) methods. These methods claim superior alignment by virtue of better correspondence with human pairwise preferences, often measured by LLM-judges. In this... | Benjamin Feuer, John P. Dickerson, Max Cembalest, Micah Goldblum, Raz Besaleli, Samuel Dooley, Sanjana Nambiar, Teresa Datta |  |
| 3009 |  |  [Learning Interpretable Hierarchical Dynamical Systems Models from Time Series Data](https://openreview.net/forum?id=Vp2OAxMs2s) |  | 0 | In science, we are often interested in obtaining a generative model of the underlying system dynamics from observed time series. While powerful methods for dynamical systems reconstruction (DSR) exist when data come from a single domain, how to best integrate data from multiple dynamical regimes... | Daniel Durstewitz, Elias Weber, Georgia Koppe, Manuel Brenner |  |
| 3010 |  |  [Nonasymptotic Analysis of Stochastic Gradient Descent with the Richardson-Romberg Extrapolation](https://openreview.net/forum?id=Odtr1rzQMq) |  | 0 | We address the problem of solving strongly convex and smooth minimization problems using stochastic gradient descent (SGD) algorithm with a constant step size. Previous works suggested to combine the Polyak-Ruppert averaging procedure with the Richardson-Romberg extrapolation to reduce the... | Alain Oliviero Durmus, Alexey Naumov, Denis Belomestny, Eric Moulines, Marina Sheshukova, Sergey Samsonov |  |
| 3011 |  |  [SimpleTM: A Simple Baseline for Multivariate Time Series Forecasting](https://openreview.net/forum?id=oANkBaVci5) |  | 0 | The versatility of large Transformer-based models has led to many efforts focused on adaptations to other modalities, including time-series data. For instance, one could start from a pre-trained checkpoint of a large language model and attach adapters to recast the new modality (e.g., time-series)... | Hui Chen, Lopamudra Mukherjee, Viet Luong, Vikas Singh |  |
| 3012 |  |  [Expand and Compress: Exploring Tuning Principles for Continual Spatio-Temporal Graph Forecasting](https://openreview.net/forum?id=FRzCIlkM7I) |  | 0 | The widespread deployment of sensing devices leads to a surge in data for spatio-temporal forecasting applications such as traffic flow, air quality, and wind energy. Although spatio-temporal graph neural networks (STGNNs) have achieved success in modeling various static spatio-temporal forecasting... | Wei Chen, Yuxuan Liang |  |
| 3013 |  |  [Aligned Better, Listen Better for Audio-Visual Large Language Models](https://openreview.net/forum?id=1SYUKPeM12) |  | 0 | Audio is essential for multimodal video understanding. On the one hand, video inherently contains audio, which supplies complementary information to vision. Besides, video large language models (Video-LLMs) can encounter many audio-centric settings. However, existing Video-LLMs and Audio-Visual... | ChenWei Xie, Kecheng Zheng, Shijie Ma, Shuailei Ma, Siyang Sun, Tingyu Weng, Wei Zou, Xiaoyi Bao, Yun Zheng, Yuxin Guo |  |
| 3014 |  |  [OSCAR: Operating System Control via State-Aware Reasoning and Re-Planning](https://openreview.net/forum?id=VuTrZzrPfn) |  | 0 | Large language models (LLMs) and large multimodal models (LMMs) have shown great potential in automating complex tasks like web browsing and gaming. However, their ability to generalize across diverse applications remains limited, hindering broader utility. To address this challenge, we present... | Bang Liu, Xiaoqiang Wang |  |
| 3015 |  |  [Learning the Complexity of Weakly Noisy Quantum States](https://openreview.net/forum?id=tmSWFGpBb8) |  | 0 | Quantifying the complexity of quantum states is a longstanding key problem in various subfields of science, ranging from quantum computing to the black-hole theory. The lower bound on quantum pure state complexity has been shown to grow linearly with system size [J. Haferkamp et al., 2022, \*Nat.... | Bujiao Wu, Jingbo Wang, Xiao Yuan, Yanqi Song, Yusen Wu |  |
| 3016 |  |  [Efficient Low-Bit Quantization with Adaptive Scales for Multi-Task Co-Training](https://openreview.net/forum?id=wA2RMD2AFq) |  | 0 | Co-training can achieve parameter-efficient multi-task models but remains unexplored for quantization-aware training. Our investigation shows that directly introducing co-training into existing quantization-aware training (QAT) methods results in significant performance degradation. Our... | Baochang Zhang, Boyu Liu, Guodong Guo, Haoyu Huang, Linlin Yang, Xianbin Cao, Yanjing Li |  |
| 3017 |  |  [Qinco2: Vector Compression and Search with Improved Implicit Neural Codebooks](https://openreview.net/forum?id=2zMHHZ569S) |  | 0 | Vector quantization is a fundamental technique for compression and large-scale nearest neighbor search. For high-accuracy operating points, multi-codebook quantization associates data vectors with one element from each of multiple codebooks. An example is residual quantization (RQ), which... | Jakob Verbeek, Matthew J. Muckley, Matthijs Douze, Théophane Vallaeys |  |
| 3018 |  |  [Balanced Neural ODEs: nonlinear model order reduction and Koopman operator approximations](https://openreview.net/forum?id=nA464tCGR5) |  | 0 | Variational Autoencoders (VAEs) are a powerful framework for learning latent representations of reduced dimensionality, while Neural ODEs excel in learning transient system dynamics. This work combines the strengths of both to generate fast surrogate models with adjustable complexity reacting on... | Arne Speerforck, Johannes Brunnemann, Julius Aka, Jörg Eiden, Lars Mikelsons |  |
| 3019 |  |  [Attention layers provably solve single-location regression](https://openreview.net/forum?id=DVlPp7Jd7P) |  | 0 | Attention-based models, such as Transformer, excel across various tasks but lack a comprehensive theoretical understanding, especially regarding token-wise sparsity and internal linear representations. To address this gap, we introduce the single-location regression task, where only one token in a... | Claire Boyer, Gérard Biau, Pierre Marion, Raphaël Berthier |  |
| 3020 |  |  [Training One-Dimensional Graph Neural Networks is NP-Hard](https://openreview.net/forum?id=7BESdFZ7YA) |  | 0 | We initiate the study of the computational complexity of training graph neural networks (GNNs). We consider the classical node classification setting; there, the intractability of training multidimensonal GNNs immediately follows from known lower bounds for training classical neural networks (and... | Mathis Rocton, Robert Ganian, Simon Wietheger |  |
| 3021 |  |  [Grounding Multimodal Large Language Model in GUI World](https://openreview.net/forum?id=M9iky9Ruhx) |  | 0 | Recent advancements in Multimodal Large Language Models (MLLMs) have accelerated the development of Graphical User Interface (GUI) agents capable of automating complex tasks across digital platforms. However, precise GUI element grounding remains a key challenge for accurate interaction and... | Difei Gao, Mike Zheng Shou, Weixian Lei |  |
| 3022 |  |  [Zero-shot Imputation with Foundation Inference Models for Dynamical Systems](https://openreview.net/forum?id=NPSZ7V1CCY) |  | 0 | Dynamical systems governed by ordinary differential equations (ODEs) serve as models for a vast number of natural and social phenomena. In this work, we offer a fresh perspective on the classical problem of imputing missing time series data, whose underlying dynamics are assumed to be determined by... | Antonia Körner, Kostadin Cvejoski, Patrick Seifner, Ramsés J. Sánchez |  |
| 3023 |  |  [PINP: Physics-Informed Neural Predictor with latent estimation of fluid flows](https://openreview.net/forum?id=vAuodZOQEZ) |  | 0 | Accurately predicting fluid dynamics and evolution has been a long-standing challenge in physical sciences. Conventional deep learning methods often rely on the nonlinear modeling capabilities of neural networks to establish mappings between past and future states, overlooking the fluid dynamics,... | Hao Sun, Huaguan Chen, Yang Liu |  |
| 3024 |  |  [GravMAD: Grounded Spatial Value Maps Guided Action Diffusion for Generalized 3D Manipulation](https://openreview.net/forum?id=qPzYF2EpXb) |  | 0 | Robots' ability to follow language instructions and execute diverse 3D manipulation tasks is vital in robot learning. Traditional imitation learning-based methods perform well on seen tasks but struggle with novel, unseen ones due to variability. Recent approaches leverage large foundation models... | Jieqi Shi, Jing Huo, Junhui Yin, Pinzhuo Tian, Yang Gao, Yangtao Chen, Zixuan Chen |  |
| 3025 |  |  [InstantSwap: Fast Customized Concept Swapping across Sharp Shape Differences](https://openreview.net/forum?id=UFrHWzZENz) |  | 0 | Recent advances in Customized Concept Swapping (CCS) enable a text-to-image model to swap a concept in the source image with a customized target concept. However, the existing methods still face the challenges of $\textit{\textbf{inconsistency}}$ and $\textit{\textbf{inefficiency}}$. They struggle... | Chengyu Fang, Chenyang Zhu, Chubin Chen, Kai Li, Longxiang Tang, Qifeng Chen, Xiu Li, Yue Ma |  |
| 3026 |  |  [Explain Yourself, Briefly! Self-Explaining Neural Networks with Concise Sufficient Reasons](https://openreview.net/forum?id=8nuzsfiQfS) |  | 0 | \*Minimal sufficient reasons\* represent a prevalent form of explanation - the smallest subset of input features which, when held constant at their corresponding values, ensure that the prediction remains unchanged. Previous \*post-hoc\* methods attempt to obtain such explanations but face two main... | Ron Eliav, Shahaf Bassan, Shlomit Gur |  |
| 3027 |  |  [Leveraging Submodule Linearity Enhances Task Arithmetic Performance in LLMs](https://openreview.net/forum?id=irPcM6X5FV) |  | 0 | Task arithmetic is a straightforward yet highly effective strategy for model merging, enabling the resultant model to exhibit multi-task capabilities. Recent research indicates that models demonstrating linearity enhance the performance of task arithmetic. In contrast to existing methods that rely... | Jieping Ye, Rui Dai, Sile Hu, Xinmei Tian, Xu Shen, Yonggang Zhang |  |
| 3028 |  |  [HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous Environment](https://openreview.net/forum?id=Cs6MrbFuMq) |  | 0 | Disaggregating the prefill and decoding phases represents an effective new paradigm for generative inference of large language models (LLM). This approach offers some significant system advantages, such as eliminating prefill-decoding interference and optimizing resource allocation. However, it is... | Binhang Yuan, Ran Yan, Youhe Jiang |  |
| 3029 |  |  [FreqPrior: Improving Video Diffusion Models with Frequency Filtering Gaussian Noise](https://openreview.net/forum?id=8x0SGbCpzs) |  | 0 | Text-driven video generation has advanced significantly due to developments in diffusion models. Beyond the training and sampling phases, recent studies have investigated noise priors of diffusion models, as improved noise priors yield better generation results. One recent approach employs the... | Chunwei Wang, Hang Xu, Li Zhang, Wei Zhang, Yuanfan Guo, Yunlong Yuan |  |
| 3030 |  |  [PRDP: Progressively Refined Differentiable Physics](https://openreview.net/forum?id=9Fh0z1JmPU) |  | 0 | The physics solvers employed for neural network training are primarily iterative, and hence, differentiating through them introduces a severe computational burden as iterations grow large. Inspired by works in bilevel optimization, we show that full accuracy of the network is achievable through... | Felix Koehler, Kanishk Bhatia, Nils Thuerey |  |
| 3031 |  |  [Going Beyond Feature Similarity: Effective Dataset distillation based on Class-aware Conditional Mutual Information](https://openreview.net/forum?id=0no1Wp2R2j) |  | 0 | Dataset distillation (DD) aims to minimize the time and memory consumption needed for training deep neural networks on large datasets, by creating a smaller synthetic dataset that has similar performance to that of the full real dataset. However, current dataset distillation methods often result in... | Bin Chen, EnHui Yang, Hao Fang, ShuTao Xia, Xinhao Zhong, Xulin Gu |  |
| 3032 |  |  [YOLO-RD: Introducing Relevant and Compact Explicit Knowledge to YOLO by Retriever-Dictionary](https://openreview.net/forum?id=KXDOmD7DM7) |  | 0 | Identifying and localizing objects within images is a fundamental challenge, and numerous efforts have been made to enhance model accuracy by experimenting with diverse architectures and refining training strategies. Nevertheless, a prevalent limitation in existing models is overemphasizing the... | ChienYao Wang, HaoTang Tsui, HongYuan Mark Liao |  |
| 3033 |  |  [Block-Attention for Efficient Prefilling](https://openreview.net/forum?id=7zNYY1E2fq) |  | 0 | We introduce Block-attention, an attention mechanism designed to address the increased inference latency and cost in Retrieval-Augmented Generation (RAG) scenarios. Traditional approaches often encode the entire context in an auto-regressive manner. Instead, Block-attention divides retrieved... | Dongyang Ma, Tian Lan, Yan Wang |  |
| 3034 |  |  [Uncertainty modeling for fine-tuned implicit functions](https://openreview.net/forum?id=iZl0VqEdxa) |  | 0 | Implicit functions such as Neural Radiance Fields (NeRFs), occupancy networks, and signed distance functions (SDFs) have become pivotal in computer vision for reconstructing detailed object shapes from sparse views. Achieving optimal performance with these models can be challenging due to the... | Anna Susmelj, Ender Konukoglu, JeanPhilippe Thiran, Mael Macuglia, Natasa Tagasovska, Reto Sutter, Sebastiano Caprara |  |
| 3035 |  |  [Learning Mask Invariant Mutual Information for Masked Image Modeling](https://openreview.net/forum?id=NoiaAT0eec) |  | 0 | Masked autoencoders (MAEs) represent a prominent self-supervised learning paradigm in computer vision. Despite their empirical success, the underlying mechanisms of MAEs remain insufficiently understood. Recent studies have attempted to elucidate the functioning of MAEs through contrastive learning... | Chang Xu, Shan You, Tao Huang, Yanxiang Ma |  |
| 3036 |  |  [UniRestore3D: A Scalable Framework For General Shape Restoration](https://openreview.net/forum?id=xPO6fwvldG) |  | 0 | Shape restoration aims to recover intact 3D shapes from defective ones, such as those that are incomplete, noisy, and low-resolution. Previous works have achieved impressive results in shape restoration subtasks thanks to advanced generative models. While effective for specific shape defects, they... | Haoyu Guo, Hujun Bao, Sida Peng, Xiaowei Zhou, Xingyi He, Yuang Wang, Yujian Zhang, Yujun Shen |  |
| 3037 |  |  [Painting with Words: Elevating Detailed Image Captioning with Benchmark and Alignment Learning](https://openreview.net/forum?id=636M0nNbPs) |  | 0 | Image captioning has long been a pivotal task in visual understanding, with recent advancements in vision-language models (VLMs) significantly enhancing the ability to generate detailed image captions. However, the evaluation of detailed image captioning remains underexplored due to outdated... | Chunyuan Li, Fu Li, Haoqi Fan, Qinghao Ye, Xianhan Zeng |  |
| 3038 |  |  [LLaMA-Omni: Seamless Speech Interaction with Large Language Models](https://openreview.net/forum?id=PYmrUQmMEw) |  | 0 | Models like GPT-4o enable real-time interaction with large language models (LLMs) through speech, significantly enhancing user experience compared to traditional text-based interaction. However, there is still a lack of exploration on how to build speech interaction models based on open-source... | Qingkai Fang, Shaolei Zhang, Shoutao Guo, Yan Zhou, Yang Feng, Zhengrui Ma |  |
| 3039 |  |  [TaskGalaxy: Scaling Multi-modal Instruction Fine-tuning with Tens of Thousands Vision Task Types](https://openreview.net/forum?id=JXgnnUC0PH) |  | 0 | Multimodal visual language models are gaining prominence in open-world applications, driven by advancements in model architectures, training techniques, and high-quality data. However, their performance is often limited by insufficient task-specific data, leading to poor generalization and biased... | Bin Wen, Changyi Liu, Cheng Feng, Di Zhang, Fan Yang, Haojie Ding, Huihui Xiao, Jiankang Chen, Tianke Zhang, Tingting Gao, Yaya Shi |  |
| 3040 |  |  [MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Masked Image Modeling Representations](https://openreview.net/forum?id=0PxLpVURTl) |  | 0 | We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning boost for pre-trained MIM models. MIM-Refiner is motivated by the insight that strong representations within MIM models generally reside in intermediate layers. Accordingly, MIM-Refiner leverages multiple instance... | Benedikt Alkin, Johannes Brandstetter, Lukas Miklautz, Sepp Hochreiter |  |
| 3041 |  |  [Exploiting Hidden Symmetry to Improve Objective Perturbation for DP Linear Learners with a Nonsmooth L1-Norm](https://openreview.net/forum?id=J863DxU7Sx) |  | 0 | Objective Perturbation (OP) is a classic approach to differentially private (DP) convex optimization with smooth loss functions but is less understood for nonsmooth cases. In this work, we study how to apply OP to DP linear learners under loss functions with an implicit $\ell_1$-norm structure,... | Du Chen, Geoffrey A. Chua |  |
| 3042 |  |  [ESE: Espresso Sentence Embeddings](https://openreview.net/forum?id=plgLA2YBLH) |  | 0 | High-quality sentence embeddings are fundamental in many natural language processing (NLP) tasks, such as semantic textual similarity (STS) and retrieval-augmented generation (RAG). However, most existing methods leverage fixed-length sentence embeddings from full-layer language models, which lack... | Haoran Xie, Jing Li, Qing Li, Xianming Li, Zongxi Li |  |
| 3043 |  |  [Vision-LSTM: xLSTM as Generic Vision Backbone](https://openreview.net/forum?id=SiH7DwNKZZ) |  | 0 | Transformers are widely used as generic backbones in computer vision, despite initially introduced for natural language processing. Recently, the Long Short-Term Memory (LSTM) has been extended to a scalable and performant architecture - the xLSTM - which overcomes long-standing LSTM limitations... | Benedikt Alkin, Johannes Brandstetter, Korbinian Pöppel, Maximilian Beck, Sepp Hochreiter |  |
| 3044 |  |  [On the Fourier analysis in the SO(3) space : the EquiLoPO Network](https://openreview.net/forum?id=LvTSvdiSwG) |  | 0 | Analyzing volumetric data with rotational invariance or equivariance is currently an active research topic. Existing deep-learning approaches utilize either group convolutional networks limited to discrete rotations or steerable convolutional networks with constrained filter structures. This work... | Dmitrii Zhemchuzhnikov, Sergei Grudinin |  |
| 3045 |  |  [Breaking Class Barriers: Efficient Dataset Distillation via Inter-Class Feature Compensator](https://openreview.net/forum?id=X0CxfByJog) |  | 0 | Dataset distillation has emerged as a technique aiming to condense informative features from large, natural datasets into a compact and synthetic form. While recent advancements have refined this technique, its performance is bottlenecked by the prevailing class-specific synthesis paradigm. Under... | Jiawei Du, Joey Tianyi Zhou, Ping Liu, Xin Zhang |  |
| 3046 |  |  [Robust Conformal Prediction with a Single Binary Certificate](https://openreview.net/forum?id=ltrxRX5t0H) |  | 0 | Conformal prediction (CP) converts any model's output to prediction sets with a guarantee to cover the true label with (adjustable) high probability. Robust CP extends this guarantee to worst-case (adversarial) inputs. Existing baselines achieve robustness by bounding randomly smoothed conformity... | Aleksandar Bojchevski, Soroush H. Zargarbashi |  |
| 3047 |  |  [Selective induction Heads: How Transformers Select Causal Structures in Context](https://openreview.net/forum?id=bnJgzAQjWf) |  | 0 | Transformers have exhibited exceptional capabilities in sequence modelling tasks, leveraging self-attention and in-context learning. Critical to this success are induction heads, attention circuits that enable copying tokens based on their previous occurrences. In this work, we introduce a novel... | Francesco Croce, Francesco D'Angelo, Nicolas Flammarion |  |
| 3048 |  |  [PEARL: Parallel Speculative Decoding with Adaptive Draft Length](https://openreview.net/forum?id=QOXrVMiHGK) |  | 0 | Speculative decoding (SD), where an extra draft model is employed to provide multiple \*\*draft\*\* tokens first and then the original target model verifies these tokens in parallel, has shown great power for LLM inference acceleration. However, existing SD methods suffer from the mutual waiting... | Jianchen Zhu, Kai Liu, Qitan Lv, Tianyu Liu, Winston Hu, Xiao Sun, Yun Li |  |
| 3049 |  |  [CityAnchor: City-scale 3D Visual Grounding with Multi-modality LLMs](https://openreview.net/forum?id=7nOl5W6xU4) |  | 0 | In this paper, we present a 3D visual grounding method called CityAnchor for localizing an urban object in a city-scale point cloud. Recent developments in multiview reconstruction enable us to reconstruct city-scale point clouds but how to conduct visual grounding on such a large-scale urban point... | Bisheng Yang, Haiping Wang, Jiabin Chen, Jinpeng Li, Sibei Yang, Wenping Wang, Yuan Li, Yuan Liu, Yuexin Ma, Zhen Dong, Zhiyang Dou |  |
| 3050 |  |  [DriveTransformer: Unified Transformer for Scalable End-to-End Autonomous Driving](https://openreview.net/forum?id=M42KR4W9P5) |  | 0 | End-to-end autonomous driving (E2E-AD) has emerged as a trend in the field of autonomous driving, promising a data-driven, scalable approach to system design. However, existing E2E-AD methods usually adopt the sequential paradigm of perception-prediction-planning, which leads to cumulative errors... | Junchi Yan, Junqi You, Xiaosong Jia, Zhiyuan Zhang |  |
| 3051 |  |  [TRACE: Temporal Grounding Video LLM via Causal Event Modeling](https://openreview.net/forum?id=14fFV0chUS) |  | 0 | Video Temporal Grounding (VTG) is a crucial capability for video understanding models and plays a vital role in downstream tasks such as video browsing and editing. To effectively handle various tasks simultaneously and enable zero-shot prediction, there is a growing trend in employing video LLMs... | Jingyu Liu, Mingda Li, Qingbin Liu, Xi Chen, Xiaoying Tang, Yongxin Guo |  |
| 3052 |  |  [RobuRCDet: Enhancing Robustness of Radar-Camera Fusion in Bird's Eye View for 3D Object Detection](https://openreview.net/forum?id=9xHlhKLu1h) |  | 0 | While recent low-cost radar-camera approaches have shown promising results in multi-modal 3D object detection, both sensors face challenges from environmen- tal and intrinsic disturbances. Poor lighting or adverse weather conditions de- grade camera performance, while radar suffers from noise and... | Jingtong Yue, Lu Qi, MingHsuan Yang, Xiangtai Li, Xiaoyu Zhou, Xin Lin, Yongtao Wang, Zhiwei Lin |  |
| 3053 |  |  [Transformers are Universal In-context Learners](https://openreview.net/forum?id=6S4WQD1LZR) |  | 0 | Transformers are deep architectures that define \`\`in-context mappings'' which enable predicting new tokens based on a given set of tokens (such as a prompt in NLP applications or a set of patches for a vision transformer). In this work, we study in particular the ability of these architectures to... | Gabriel Peyré, Maarten V. de Hoop, Takashi Furuya |  |
| 3054 |  |  [Autoregressive Video Generation without Vector Quantization](https://openreview.net/forum?id=JE9tCwe3lp) |  | 0 | This paper presents a novel approach that enables autoregressive video generation with high efficiency. We propose to reformulate the video generation problem as a non-quantized autoregressive modeling of temporal frame-by-frame prediction and spatial set-by-set prediction. Unlike raster-scan... | Haiwen Diao, Haoge Deng, Huchuan Lu, Shiguang Shan, Ting Pan, Xinlong Wang, Yonggang Qi, Yufeng Cui, Zhengxiong Luo |  |
| 3055 |  |  [Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models](https://openreview.net/forum?id=rsZwwjYHuD) |  | 0 | Hallucination remains a significant challenge in Large Vision-Language Models (LVLMs). To alleviate this issue, some methods, known as contrastive decoding, induce hallucinations by manually disturbing the raw vision or instruction inputs and then mitigate them by contrasting the outputs of the... | Fushuo Huo, Haozhao Wang, Peilin Zhao, Wenchao Xu, Zhicheng Chen, Zhong Zhang |  |
| 3056 |  |  [Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models](https://openreview.net/forum?id=N5fVv6PZGz) |  | 0 | Large Language Models (LLMs) with the Mixture-of-Experts (MoE) architectures have shown promising performance on various tasks. However, due to the huge model sizes, running them in resource-constrained environments where the GPU memory is not abundant is challenging. Some existing systems propose... | Baris Kasikci, Kan Zhu, Keisuke Kamahori, Tian Tang, Yile Gu |  |
| 3057 |  |  [Quantitative Approximation for Neural Operators in Nonlinear Parabolic Equations](https://openreview.net/forum?id=yUefexs79U) |  | 0 | Neural operators serve as universal approximators for general continuous operators. In this paper, we derive the approximation rate of solution operators for the nonlinear parabolic partial differential equations (PDEs), contributing to the quantitative approximation theorem for solution operators... | Koichi Taniguchi, Satoshi Okuda, Takashi Furuya |  |
| 3058 |  |  [A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation](https://openreview.net/forum?id=wryFCrWB0A) |  | 0 | This work tackles the information loss bottleneck of vector-quantization (VQ) autoregressive image generation by introducing a novel model architecture called the 2-Dimensional Autoregression (DnD) Transformer. The DnD-Transformer predicts more codes for an image by introducing a new direction,... | Baobao Chang, Haozhe Zhao, Jinze Bai, Junyang Lin, Liang Chen, Sinan Tan, Tianyu Liu, Weichu Xie, Yichi Zhang, Zefan Cai |  |
| 3059 |  |  [SiMHand: Mining Similar Hands for Large-Scale 3D Hand Pose Pre-training](https://openreview.net/forum?id=96jZFqM5E0) |  | 0 | We present a framework for pre-training of 3D hand pose estimation from in-the-wild hand images sharing with similar hand characteristics, dubbed SiMHand. Pre-training with large-scale images achieves promising results in various tasks, but prior methods for 3D hand pose pre-training have not fully... | Ming Li, Mingfang Zhang, Minjie Cai, Nie Lin, Ryosuke Furuta, Takehiko Ohkawa, Yifei Huang, Yoichi Sato |  |
| 3060 |  |  [Learning Structured Universe Graph with Outlier OOD Detection for Partial Matching](https://openreview.net/forum?id=dmjQLHufev) |  | 0 | Partial matching is a kind of graph matching where only part of two graphs can be aligned. This problem is particularly important in computer vision applications, where challenges like point occlusion or annotation errors often occur when labeling key points. Previous work has often conflated point... | Haizhao Fan, Jiaxin Lu, Junchi Yan, Tianzhe Wang, Zetian Jiang |  |
| 3061 |  |  [Layout-your-3D: Controllable and Precise 3D Generation with 2D Blueprint](https://openreview.net/forum?id=myolhJPuRI) |  | 0 | We present Layout-Your-3D, a framework that allows controllable and compositional 3D generation from text prompts. Existing text-to-3D methods often struggle to generate assets with plausible object interactions or require tedious optimization processes. To address these challenges, our approach... | Junwei Zhou, Lu Qi, MingHsuan Yang, Xueting Li |  |
| 3062 |  |  [Youku Dense Caption: A Large-scale Chinese Video Dense Caption Dataset and Benchmarks](https://openreview.net/forum?id=vvi5OjPhbu) |  | 0 | With the explosive growth of video content, video captions have emerged as a crucial tool for video comprehension, significantly enhancing the ability to understand and retrieve information from videos. However, most publicly available dense video captioning datasets are in English, resulting in a... | Guangwei Xu, HaiTao Zheng, LinHai, Ruijie Guo, Wenkai Zhang, Xuan Wu, Yuan Miao, Zixuan Xiong |  |
| 3063 |  |  [ADMM for Structured Fractional Minimization](https://openreview.net/forum?id=DcZpQhVpp9) |  | 0 | This paper considers a class of structured fractional minimization problems. The numerator consists of a differentiable function, a simple nonconvex nonsmooth function, a concave nonsmooth function, and a convex nonsmooth function composed with a linear operator. The denominator is a continuous... | Ganzhao Yuan |  |
| 3064 |  |  [TAU-106K: A New Dataset for Comprehensive Understanding of Traffic Accident](https://openreview.net/forum?id=Fb0q2uI4Ha) |  | 0 | Multimodal Large Language Models (MLLMs) have demonstrated impressive performance in general visual understanding tasks. However, their potential for high-level, fine-grained comprehension, such as anomaly understanding, remains unexplored. Focusing on traffic accidents, a critical and practical... | Bing Deng, Heng Tao Shen, Long Bai, Sijia Cai, Xing Xu, Yixuan Zhou |  |
| 3065 |  |  [SAMRefiner: Taming Segment Anything Model for Universal Mask Refinement](https://openreview.net/forum?id=JlDx2xp01W) |  | 0 | In this paper, we explore a principal way to enhance the quality of widely pre-existing coarse masks, enabling them to serve as reliable training data for segmentation models to reduce the annotation cost. In contrast to prior refinement techniques that are tailored to specific models or tasks in a... | Hengjia Li, Jun Zhao, Kaipeng Zhang, Ping Luo, Wenqi Shao, Xiaofei He, Yuqi Lin, Zheng Yang |  |
| 3066 |  |  [Rethinking Diffusion Posterior Sampling: From Conditional Score Estimator to Maximizing a Posterior](https://openreview.net/forum?id=GcvLoqOoXL) |  | 0 | Recent advancements in diffusion models have been leveraged to address inverse problems without additional training, and Diffusion Posterior Sampling (DPS) (Chung et al., 2022a) is among the most popular approaches. Previous analyses suggest that DPS accomplishes posterior sampling by approximating... | Dailan He, Jian Li, Jingjing Liu, Ming Sun, Tongda Xu, Xingtong Ge, Xinjie Zhang, Xiyan Cai, YaQin Zhang, Yan Wang |  |
| 3067 |  |  [Benchmarking Agentic Workflow Generation](https://openreview.net/forum?id=vunPXOFmoi) |  | 0 | Large Language Models (LLMs), with their exceptional ability to handle a wide range of tasks, have driven significant advancements in tackling reasoning and planning tasks, wherein decomposing complex problems into executable workflows is a crucial step in this process. Existing workflow evaluation... | Fei Huang, Huajun Chen, Ningyu Zhang, Pengjun Xie, Runnan Fang, Shuofei Qiao, Xiaobin Wang, Yong Jiang, Zhisong Qiu |  |
| 3068 |  |  [DreamCatalyst: Fast and High-Quality 3D Editing via Controlling Editability and Identity Preservation](https://openreview.net/forum?id=FA5ZAJlv96) |  | 0 | Score distillation sampling (SDS) has emerged as an effective framework in text-driven 3D editing tasks, leveraging diffusion models for 3D-consistent editing. However, existing SDS-based 3D editing methods suffer from long training times and produce low-quality results. We identify that the root... | Hyunjung Shim, Jaeyo Shin, Jiho Choi, Jiwook Kim, Seonho Lee |  |
| 3069 |  |  [ADAPT: Attentive Self-Distillation and Dual-Decoder Prediction Fusion for Continual Panoptic Segmentation](https://openreview.net/forum?id=HF1UmIVv6a) |  | 0 | Panoptic segmentation, which unifies semantic and instance segmentation into a single task, has witnessed considerable success on predefined tasks. However, traditional methods tend to struggle with catastrophic forgetting and poor generalization when learning from a continuous stream of new tasks.... | Guosheng Lin, Nan Song, Ruibo Li, Shichao Dong, Ze Yang |  |
| 3070 |  |  [DGQ: Distribution-Aware Group Quantization for Text-to-Image Diffusion Models](https://openreview.net/forum?id=ZyNEr7Xw5L) |  | 0 | Despite the widespread use of text-to-image diffusion models across various tasks, their computational and memory demands limit practical applications. To mitigate this issue, quantization of diffusion models has been explored. It reduces memory usage and computational costs by compressing weights... | Hyogon Ryu, Hyunjung Shim, NaHyeon Park |  |
| 3071 |  |  [NeuroLM: A Universal Multi-task Foundation Model for Bridging the Gap between Language and EEG Signals](https://openreview.net/forum?id=Io9yFt7XH7) |  | 0 | Recent advancements for large-scale pre-training with neural signals such as electroencephalogram (EEG) have shown promising results, significantly boosting the development of brain-computer interfaces (BCIs) and healthcare. However, these pre-trained models often require full fine-tuning on each... | BaoLiang Lu, Dongsheng Li, Weibang Jiang, Yansen Wang |  |
| 3072 |  |  [N-ForGOT: Towards Not-forgetting and Generalization of Open Temporal Graph Learning](https://openreview.net/forum?id=rLlDt2FQvz) |  | 0 | Temporal Graph Neural Networks (TGNNs) lay emphasis on capturing node interactions over time but often overlook evolution in node classes and dynamic data distributions triggered by the continuous emergence of new class labels, known as the open-set problem. This problem poses challenges for... | Chen Zhang, Jingshu Peng, Lei Chen, Liping Wang, Xujia Li, Yan Zhou, Yue Wang |  |
| 3073 |  |  [Cafe-Talk: Generating 3D Talking Face Animation with Multimodal Coarse- and Fine-grained Control](https://openreview.net/forum?id=S7cWJkWqOi) |  | 0 | Speech-driven 3D talking face method should offer both accurate lip synchronization and controllable expressions. Previous methods solely adopt discrete emotion labels to globally control expressions throughout sequences while limiting flexible fine-grained facial control within the spatiotemporal... | Di Zhang, Haoxian Zhang, Hejia Chen, Pengfei Wan, Shoulong Zhang, Shuai Li, Sisi Zhuang, Xiaoqiang Liu, Yuan Zhang |  |
| 3074 |  |  [MovieDreamer: Hierarchical Generation for Coherent Long Visual Sequences](https://openreview.net/forum?id=QHj2LL958o) |  | 0 | Recent advancements in video generation have primarily leveraged diffusion models for short-duration content. However, these approaches often fall short in modeling complex narratives and maintaining character consistency over extended periods, which is essential for long-form video production like... | Bo Zhang, Canyu Zhao, Chunhua Shen, Fan Wang, Hao Chen, Mingyu Liu, Weihua Chen, Wen Wang |  |
| 3075 |  |  [The Optimization Landscape of SGD Across the Feature Learning Strength](https://openreview.net/forum?id=iEfdvDTcZg) |  | 0 | We consider neural networks (NNs) where the final layer is down-scaled by a fixed hyperparameter $\gamma$. Recent work has identified $\gamma$ as controlling the strength of feature learning. As $\gamma$ increases, network evolution changes from "lazy" kernel dynamics to "rich" feature-learning... | Alexander B. Atanasov, Alexandru Meterez, Cengiz Pehlevan, James B. Simon |  |
| 3076 |  |  [UNIP: Rethinking Pre-trained Attention Patterns for Infrared Semantic Segmentation](https://openreview.net/forum?id=Xq7gwsnhPT) |  | 0 | Pre-training techniques significantly enhance the performance of semantic segmentation tasks with limited training data. However, the efficacy under a large domain gap between pre-training (e.g. RGB) and fine-tuning (e.g. infrared) remains underexplored. In this study, we first benchmark the... | Chunhong Pan, Jinyong Wen, Kun Ding, Shiming Xiang, Tao Zhang, Zhen Chen |  |
| 3077 |  |  [Latent-EnSF: A Latent Ensemble Score Filter for High-Dimensional Data Assimilation with Sparse Observation Data](https://openreview.net/forum?id=urcEYsZOBz) |  | 0 | Accurate modeling and prediction of complex physical systems often rely on data assimilation techniques to correct errors inherent in model simulations. Traditional methods like the Ensemble Kalman Filter (EnKF) and its variants as well as the recently developed Ensemble Score Filters (EnSF) face... | Peng Chen, Phillip Si |  |
| 3078 |  |  [Modeling Fine-Grained Hand-Object Dynamics for Egocentric Video Representation Learning](https://openreview.net/forum?id=P6G1Z6jkf3) |  | 0 | In egocentric video understanding, the motion of hands and objects as well as their interactions play a significant role by nature. However, existing egocentric video representation learning methods mainly focus on aligning video representation with high-level narrations, overlooking the intricate... | Baoqi Pei, Fei Wu, Guo Chen, Jilan Xu, Lijin Yang, Limin Wang, Weidi Xie, Yali Wang, Yifei Huang, Yu Qiao, Yuping He |  |
| 3079 |  |  [Rethinking Light Decoder-based Solvers for Vehicle Routing Problems](https://openreview.net/forum?id=4pRwkYpa2u) |  | 0 | Light decoder-based solvers have gained popularity for solving vehicle routing problems (VRPs) due to their efficiency and ease of integration with reinforcement learning algorithms. However, they often struggle with generalization to larger problem instances or different VRP variants. This paper... | Jianan Zhou, Yixin Xu, Zhiguang Cao, Ziwei Huang |  |
| 3080 |  |  [Scrutinize What We Ignore: Reining In Task Representation Shift Of Context-Based Offline Meta Reinforcement Learning](https://openreview.net/forum?id=Cr1XlGBGVm) |  | 0 | Offline meta reinforcement learning (OMRL) has emerged as a promising approach for interaction avoidance and strong generalization performance by leveraging pre-collected data and meta-learning techniques. Previous context-based approaches predominantly rely on the intuition that alternating... | Anqi Guo, Boyuan Zheng, Hai Zhang, Jinhang Liu, Junqiao Zhao, Lanqing Li, Tianying Ji |  |
| 3081 |  |  [Federated Residual Low-Rank Adaptation of Large Language Models](https://openreview.net/forum?id=e0rQRMUhs7) |  | 0 | Low-Rank Adaptation (LoRA) presents an effective solution for federated fine-tuning of Large Language Models (LLMs), as it substantially reduces communication overhead. However, a straightforward combination of FedAvg and LoRA results in suboptimal performance, especially under data heterogeneity.... | ChunMei Feng, Lei Zhu, Rick Siow Mong Goh, Wangmeng Zuo, Yong Liu, Yunlu Yan |  |
| 3082 |  |  [Strong Preferences Affect the Robustness of Preference Models and Value Alignment](https://openreview.net/forum?id=Upoxh7wvmJ) |  | 0 | Value alignment, which aims to ensure that large language models (LLMs) and other AI agents behave in accordance with human values, is critical for ensuring safety and trustworthiness of these systems. A key component of value alignment is the modeling of human preferences as a representation of... | Mohan S. Kankanhalli, Ziwei Xu |  |
| 3083 |  |  [HarmAug: Effective Data Augmentation for Knowledge Distillation of Safety Guard Models](https://openreview.net/forum?id=y3zswp3gek) |  | 0 | Safety guard models that detect malicious queries aimed at large language models (LLMs) are essential for ensuring the secure and responsible deployment of LLMs in real-world applications. However, deploying existing safety guard models with billions of parameters alongside LLMs on mobile devices... | Dominik Wagner, Dong Bok Lee, Haebin Seong, Juho Lee, Minki Kang, Seanie Lee, Sung Ju Hwang, Xiaoyin Chen, Yoshua Bengio |  |
| 3084 |  |  [MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge](https://openreview.net/forum?id=v8qABSeeKO) |  | 0 | Knowledge editing techniques have emerged as essential tools for updating the factual knowledge of large language models (LLMs) and multimodal models (LMMs), allowing them to correct outdated or inaccurate information without retraining from scratch. However, existing benchmarks for multimodal... | Chenrui Shi, Kailin Jiang, Qing Li, Siyuan Qi, Yuntao Du, Zhi Gao, Zilong Zheng |  |
| 3085 |  |  [Synergy and Diversity in CLIP: Enhancing Performance Through Adaptive Backbone Ensembling](https://openreview.net/forum?id=Zkq4fsyjfp) |  | 0 | Contrastive Language-Image Pretraining (CLIP) stands out as a prominent method for image representation learning. Various architectures, from vision transformers~(ViTs) to convolutional networks (ResNets) have been trained with CLIP to serve as general solutions to diverse vision tasks. This paper... | Anton van den Hengel, Cristian Rodriguez Opazo, Damien Teney, Edison MarreseTaylor, Ehsan Abbasnejad, Hamed Damirchi |  |
| 3086 |  |  [A Black Swan Hypothesis: The Role of Human Irrationality in AI Safety](https://openreview.net/forum?id=7k4HVhUS9k) |  | 0 | Black swan events are statistically rare occurrences that carry extremely high risks. A typical view of defining black swan events is heavily assumed to originate from an unpredictable time-varying environments; however, the community lacks a comprehensive definition of black swan events. To this... | Chanwoo Park, David Abel, Hyunin Lee, Ming Jin |  |
| 3087 |  |  [Local Patterns Generalize Better for Novel Anomalies](https://openreview.net/forum?id=4ua4wyAQLm) |  | 0 | Video anomaly detection (VAD) aims to identify novel actions or events which are unseen during training. Existing mainstream VAD techniques typically focus on the global patterns with redundant details and struggle to generalize to unseen samples. In this paper, we propose a framework that... | Yalong Jiang |  |
| 3088 |  |  [Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving](https://openreview.net/forum?id=KmmNb7631I) |  | 0 | In the field of large language model (LLM) post-training, the effectiveness of utilizing synthetic data generated by the LLM itself has been well-presented. However, a key question remains unaddressed: what essential information should such self-generated data encapsulate? Existing approaches only... | Chongjie Zhang, Flood Sung, Jin Zhang, Yang Gao, Zhilin Yang |  |
| 3089 |  |  [Minimax Optimal Two-Stage Algorithm For Moment Estimation Under Covariate Shift](https://openreview.net/forum?id=oc4yw7zX9T) |  | 0 | Covariate shift occurs when the distribution of input features differs between the training and testing phases. In covariate shift, estimating an unknown function's moment is a classical problem that remains under-explored, despite its common occurrence in real-world scenarios. In this paper, we... | Jiaye Teng, Shaoli Wang, Xin Liu, Zhen Zhang |  |
| 3090 |  |  [The Labyrinth of Links: Navigating the Associative Maze of Multi-modal LLMs](https://openreview.net/forum?id=vJ0axKTh7t) |  | 0 | Multi-modal Large Language Models (MLLMs) have exhibited impressive capability. However, recently many deficiencies of MLLMs have been found compared to human intelligence, $\textit{e.g.}$, hallucination. To drive the MLLMs study, the community dedicated efforts to building larger benchmarks with... | Cewu Lu, Hong Li, Jianbin Zhu, Nanxi Li, Qinlu Guo, YongLu Li, Yuanjie Chen |  |
| 3091 |  |  [Failures to Find Transferable Image Jailbreaks Between Vision-Language Models](https://openreview.net/forum?id=wvFnqVVUhN) |  | 0 | The integration of new modalities into frontier AI systems offers exciting capabilities, but also increases the possibility such systems can be adversarially manipulated in undesirable ways. In this work, we focus on a popular class of vision-language models (VLMs) that generate text outputs... | Brando Miranda, Cristóbal Eyzaguirre, Dan Valentine, Ethan Perez, Henry Sleight, James Chua, Joe Benton, John Hughes, Luke Bailey, Mrinank Sharma, Rajashree Agrawal, Rylan Schaeffer, Sanmi Koyejo, Scott Emmons, Tony Tong Wang, Zane Durante |  |
| 3092 |  |  [A Percolation Model of Emergence: Analyzing Transformers Trained on a Formal Language](https://openreview.net/forum?id=0pLCDJVVRD) |  | 0 | Increase in data, size, or compute can lead to sudden learning of specific capabilities by a neural network---a phenomenon often called "emergence". Beyond scientific understanding, establishing the causal factors underlying such emergent capabilities is crucial to enable risk regulation frameworks... | Ekdeep Singh Lubana, Hidenori Tanaka, Kyogo Kawaguchi, Robert P. Dick |  |
| 3093 |  |  [Sequential Stochastic Combinatorial Optimization Using Hierarchal Reinforcement Learning](https://openreview.net/forum?id=AloCXPpq54) |  | 0 | Reinforcement learning (RL) has emerged as a promising tool for combinatorial optimization (CO) problems due to its ability to learn fast, effective, and generalizable solutions. Nonetheless, existing works mostly focus on one-shot deterministic CO, while sequential stochastic CO (SSCO) has rarely... | Haipeng Chen, Xinsong Feng, Yanhai Xiong, Zihan Yu |  |
| 3094 |  |  [CR2PQ: Continuous Relative Rotary Positional Query for Dense Visual Representation Learning](https://openreview.net/forum?id=3l6PwssLNY) |  | 0 | Dense visual contrastive learning (DRL) shows promise for learning localized information in dense prediction tasks, but struggles with establishing pixel/patch correspondence across different views (cross-contrasting). Existing methods primarily rely on self-contrasting the same view with... | Haoru Tan, Jinfa Huang, Junchi Yan, Qiang Zhou, Shaofeng Zhang, Sitong Wu, Zhibin Wang |  |
| 3095 |  |  [Rethinking Visual Counterfactual Explanations Through Region Constraint](https://openreview.net/forum?id=gqeXXrIMr0) |  | 0 | Visual counterfactual explanations (VCEs) have recently gained immense popularity as a tool for clarifying the decision-making process of image classifiers. This trend is largely motivated by what these explanations promise to deliver -- indicate semantically meaningful factors that change the... | Bartlomiej Sadlej, Bartlomiej Sobieski, Jakub Grzywaczewski, Matthew Tivnan, Przemyslaw Biecek |  |
| 3096 |  |  [Hidden in the Noise: Two-Stage Robust Watermarking for Images](https://openreview.net/forum?id=ll2nz6qwRG) |  | 0 | As the quality of image generators continues to improve, deepfakes become a topic of considerable societal debate. Image watermarking allows responsible model owners to detect and label their AI-generated content, which can mitigate the harm. Yet, current state-of-the-art methods in image... | Benjamin Feuer, Chinmay Hegde, Kasra Arabi, Niv Cohen, R. Teal Witter |  |
| 3097 |  |  [From Lazy to Rich: Exact Learning Dynamics in Deep Linear Networks](https://openreview.net/forum?id=ZXaocmXc6d) |  | 0 | Biological and artificial neural networks develop internal representations that enable them to perform complex tasks. In artificial networks, the effectiveness of these models relies on their ability to build task specific representation, a process influenced by interactions among datasets,... | Alexandra M. Proca, Andrew M. Saxe, Clémentine Carla Juliette Dominé, Daniel Kunin, Lukas Braun, Nicolas Anguita, Pedro A. M. Mediano |  |
| 3098 |  |  [Dynamic Assortment Selection and Pricing with Censored Preference Feedback](https://openreview.net/forum?id=DOXnqYLCcd) |  | 0 | In this study, we investigate the problem of dynamic multi-product selection and pricing by introducing a novel framework based on a \*censored multinomial logit\* (C-MNL) choice model. In this model, sellers present a set of products with prices, and buyers filter out products priced above their... | Junghun Kim, Minhwan Oh |  |
| 3099 |  |  [Release the Powers of Prompt Tuning: Cross-Modality Prompt Transfer](https://openreview.net/forum?id=SYnIf4LxAG) |  | 0 | Prompt Tuning adapts frozen models to new tasks by prepending a few learnable embeddings to the input. However, it struggles with tasks that suffer from data scarcity. To address this, we explore Cross-Modality Prompt Transfer, leveraging prompts pretrained on a data-rich modality to improve... | Guangquan Zhang, Jie Lu, Keqiuyin Li, Ningyuan Zhang, Zhen Fang |  |
| 3100 |  |  [Temporal Flexibility in Spiking Neural Networks: Towards Generalization Across Time Steps and Deployment Friendliness](https://openreview.net/forum?id=9HsfTgflT7) |  | 0 | Spiking Neural Networks (SNNs), models inspired by neural mechanisms in the brain, allow for energy-efficient implementation on neuromorphic hardware. However, SNNs trained with current direct training approaches are constrained to a specific time step. This "temporal inflexibility" 1) hinders... | Kangrui Du, Shi Gu, Shikuang Deng, Yuhang Wu |  |
| 3101 |  |  [GOFA: A Generative One-For-All Model for Joint Graph Language Modeling](https://openreview.net/forum?id=mIjblC9hfm) |  | 0 | Foundation models, such as Large Language Models (LLMs) or Large Vision Models (LVMs), have emerged as one of the most powerful tools in the respective fields. However, unlike text and image data, graph data do not have a definitive structure, posing great challenges to developing a Graph... | Chengsong Huang, Hao Liu, Jiarui Feng, Jiaxin Huang, Lecheng Kong, Muhan Zhang, Yixin Chen |  |
| 3102 |  |  [Group Distributionally Robust Dataset Distillation with Risk Minimization](https://openreview.net/forum?id=3JsU5QXNru) |  | 0 | Dataset distillation (DD) has emerged as a widely adopted technique for crafting a synthetic dataset that captures the essential information of a training dataset, facilitating the training of accurate neural models. Its applications span various domains, including transfer learning, federated... | Jianyang Gu, Mingyu Wang, Saeed Vahidian, Vyacheslav Kungurtsev, Wei Jiang, Yiran Chen |  |
| 3103 |  |  [CapeX: Category-Agnostic Pose Estimation from Textual Point Explanation](https://openreview.net/forum?id=scKAXgonmq) |  | 0 | Conventional 2D pose estimation models are constrained by their design to specific object categories. This limits their applicability to predefined objects. To overcome these limitations, category-agnostic pose estimation (CAPE) emerged as a solution. CAPE aims to facilitate keypoint localization... | Matan Rusanovsky, Or Hirschorn, Shai Avidan |  |
| 3104 |  |  [Slot-Guided Adaptation of Pre-trained Diffusion Models for Object-Centric Learning and Compositional Generation](https://openreview.net/forum?id=kZvor5aaz7) |  | 0 | We present SlotAdapt, an object-centric learning method that combines slot attention with pretrained diffusion models by introducing adapters for slot-based conditioning. Our method preserves the generative power of pretrained diffusion models, while avoiding their text-centric conditioning bias.... | Adil Kaan Akan, Yucel Yemez |  |
| 3105 |  |  [Strategic Classification With Externalities](https://openreview.net/forum?id=o6CXkEEttn) |  | 0 | We propose a new variant of the strategic classification problem: a principal reveals a classifier, and $n$ agents report their (possibly manipulated) features to be classified. Motivated by real-world applications, our model crucially allows the manipulation of one agent to affect another; that... | Ariel D. Procaccia, Evi Micha, Safwan Hossain, Yiling Chen |  |
| 3106 |  |  [C-CLIP: Multimodal Continual Learning for Vision-Language Model](https://openreview.net/forum?id=sb7qHFYwBc) |  | 0 | Multimodal pre-trained models like CLIP need large image-text pairs for training but often struggle with domain-specific tasks. Since retraining with specialized and historical data incurs significant memory and time costs, it is important to continually learn new domains in the open world while... | Fei Zhu, Longhui Wei, Qi Tian, Wenzhuo Liu |  |
| 3107 |  |  [SpinQuant: LLM Quantization with Learned Rotations](https://openreview.net/forum?id=ogO6DGE6FZ) |  | 0 | Post-training quantization (PTQ) techniques applied to weights, activations, and the KV cache greatly reduce memory usage, latency, and power consumption of Large Language Models (LLMs), but may lead to large quantization errors when outliers are present. Rotating activation or weight matrices... | Bilge Soran, Changsheng Zhao, Dhruv Choudhary, Igor Fedorov, Raghuraman Krishnamoorthi, Tijmen Blankevoort, Vikas Chandra, Yuandong Tian, Zechun Liu |  |
| 3108 |  |  [Revisit Micro-batch Clipping: Adaptive Data Pruning via Gradient Manipulation](https://openreview.net/forum?id=pAkQhhn4vB) |  | 0 | Micro-batch clipping, a gradient clipping method, has recently shown potential in enhancing auto-speech recognition (ASR) model performance. However, the underlying mechanism behind this improvement remains mysterious, particularly the observation that only certain micro-batch sizes are beneficial.... | Lun Wang |  |
| 3109 |  |  [Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?](https://openreview.net/forum?id=1Xg4JPPxJ0) |  | 0 | Humans exhibit remarkable compositional reasoning by integrating knowledge from various sources. For example, if someone learns ( B = f(A) ) from one source and ( C = g(B) ) from another, they can deduce ( C=g(B)=g(f(A)) ) even without encountering ( ABC ) together, showcasing the generalization... | Yutong Yin, Zhaoran Wang |  |
| 3110 |  |  [Adding Conditional Control to Diffusion Models with Reinforcement Learning](https://openreview.net/forum?id=svp1EBA6hA) |  | 0 | Diffusion models are powerful generative models that allow for precise control over the characteristics of the generated samples. While these diffusion models trained on large datasets have achieved success, there is often a need to introduce additional controls in downstream fine-tuning processes,... | Ehsan Hajiramezanali, Gabriele Scalia, Masatoshi Uehara, Sergey Levine, SunYuan Kung, Tommaso Biancalani, Yulai Zhao |  |
| 3111 |  |  [Spatial-Mamba: Effective Visual State Space Models via Structure-Aware State Fusion](https://openreview.net/forum?id=iDe1mtxqK5) |  | 0 | Selective state space models (SSMs), such as Mamba, highly excel at capturing long-range dependencies in 1D sequential data, while their applications to 2D vision tasks still face challenges. Current visual SSMs often convert images into 1D sequences and employ various scanning patterns to... | Chaodong Xiao, Deyu Meng, Lei Zhang, Minghan Li, Zhengqiang Zhang |  |
| 3112 |  |  [On the Optimization Landscape of Low Rank Adaptation Methods for Large Language Models](https://openreview.net/forum?id=pxclAomHat) |  | 0 | Training Large Language Models (LLMs) poses significant memory challenges, making low-rank adaptation methods an attractive solution. Previously, Low-Rank Adaptation (LoRA) addressed this by adding a trainable low-rank matrix to the frozen pre-trained weights in each layer, reducing the number of... | Jun Wang, XuHui Liu, Yali Du, Yang Yu |  |
| 3113 |  |  [Federated Class-Incremental Learning: A Hybrid Approach Using Latent Exemplars and Data-Free Techniques to Address Local and Global Forgetting](https://openreview.net/forum?id=ydREOIttdC) |  | 0 | Federated Class-Incremental Learning (FCIL) refers to a scenario where a dynamically changing number of clients collaboratively learn an ever-increasing number of incoming tasks. FCIL is known to suffer from local forgetting due to class imbalance at each client and global forgetting due to class... | Guanghui Wang, IlMin Kim, Milad Khademi Nori |  |
| 3114 |  |  [VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis](https://openreview.net/forum?id=Kb9PnkWYNT) |  | 0 | Despite tremendous progress in the field of text-to-video (T2V) synthesis, open-sourced T2V diffusion models struggle to generate longer videos with dynamically varying and evolving content. They tend to synthesize quasi-static videos, ignoring the necessary visual change-over-time implied in the... | Anna Khoreva, Dan Zhang, Margret Keuper, William H. Beluch, Yumeng Li |  |
| 3115 |  |  [Towards Scalable Exact Machine Unlearning Using Parameter-Efficient Fine-Tuning](https://openreview.net/forum?id=oe51Q5Uo37) |  | 0 | Machine unlearning is the process of efficiently removing the influence of a training data instance from a trained machine learning model without retraining it from scratch. A popular subclass of unlearning approaches is exact machine unlearning, which focuses on techniques that explicitly... | Arijit Sehanobish, Krzysztof Marcin Choromanski, Kumar Avinava Dubey, Snigdha Chaturvedi, Somnath Basu Roy Chowdhury |  |
| 3116 |  |  [Understanding the Stability-based Generalization of Personalized Federated Learning](https://openreview.net/forum?id=znhZbonEoe) |  | 0 | Despite great achievements in algorithm design for Personalized Federated Learning (PFL), research on the theoretical analysis of generalization is still in its early stages. Some theoretical results have investigated the generalization performance of personalized models under the problem setting... | Jie Tang, Li Shen, Qinglun Li, Xiaochun Cao, Yifan Shi, Yingqi Liu |  |
| 3117 |  |  [Adaptive Retention & Correction: Test-Time Training for Continual Learning](https://openreview.net/forum?id=9bLdbp46Q1) |  | 0 | Continual learning, also known as lifelong learning or incremental learning, refers to the process by which a model learns from a stream of incoming data over time. A common problem in continual learning is the classification layer’s bias towards the most recent task. Traditionally, methods have... | Haoran Chen, Micah Goldblum, YuGang Jiang, Zuxuan Wu |  |
| 3118 |  |  [Unsupervised Model Tree Heritage Recovery](https://openreview.net/forum?id=QVj3kUvdvl) |  | 0 | The number of models shared online has recently skyrocketed, with over one million public models available on Hugging Face. Sharing models allows other users to build on existing models, using them as initialization for fine-tuning, improving accuracy, and saving compute and energy. However, it... | Asaf Shul, Eliahu Horwitz, Yedid Hoshen |  |
| 3119 |  |  [Deep Linear Probe Generators for Weight Space Learning](https://openreview.net/forum?id=XoYdD3m0mv) |  | 0 | Weight space learning aims to extract information about a neural network, such as its training dataset or generalization error. Recent approaches learn directly from model weights, but this presents many challenges as weights are high-dimensional and include permutation symmetries between neurons.... | Eliahu Horwitz, Imri Shuval, Jonathan Kahana, Yedid Hoshen |  |
| 3120 |  |  [Do WGANs succeed because they minimize the Wasserstein Distance? Lessons from Discrete Generators](https://openreview.net/forum?id=7YXaOvunqo) |  | 0 | Since WGANs were first introduced, there has been considerable debate whether their success in generating realistic images can be attributed to minimizing the Wasserstein distance between the distribution of generated images and the training distribution. In this paper we present theoretical and... | Ariel Elnekave, Yair Weiss |  |
| 3121 |  |  [Your Weak LLM is Secretly a Strong Teacher for Alignment](https://openreview.net/forum?id=sGqd1tF8P8) |  | 0 | The burgeoning capabilities of large language models (LLMs) have underscored the need for alignment to ensure these models act in accordance with human values and intentions. Existing alignment frameworks present constraints either in the form of expensive human effort or high computational costs.... | Leitian Tao, Yixuan Li |  |
| 3122 |  |  [Beyond Canonicalization: How Tensorial Messages Improve Equivariant Message Passing](https://openreview.net/forum?id=vDp6StrKIq) |  | 0 | In numerous applications of geometric deep learning, the studied systems exhibit spatial symmetries and it is desirable to enforce these. For the symmetry of global rotations and reflections, this means that the model should be equivariant with respect to the transformations that form the group of... | Fred A. Hamprecht, Gerrit Gerhartz, Peter Lippmann, Roman Remme |  |
| 3123 |  |  [Accelerating Diffusion Transformers with Token-wise Feature Caching](https://openreview.net/forum?id=yYZbZGo4ei) |  | 0 | Diffusion transformers have shown significant effectiveness in both image and video synthesis at the expense of huge computation costs. To address this problem, feature caching methods have been introduced to accelerate diffusion transformers by caching the features in previous timesteps and... | Chang Zou, Linfeng Zhang, Siteng Huang, Ting Liu, Xuyang Liu |  |
| 3124 |  |  [Towards Unified Human Motion-Language Understanding via Sparse Interpretable Characterization](https://openreview.net/forum?id=Oh8MuCacJW) |  | 0 | Recently, the comprehensive understanding of human motion has been a prominent area of research due to its critical importance in many fields. However, existing methods often prioritize specific downstream tasks and roughly align text and motion features within a CLIP-like framework. This results... | Cheng Deng, Chenghao Xu, Guangtao Lyu, Jiexi Yan, Muli Yang |  |
| 3125 |  |  [WorkflowLLM: Enhancing Workflow Orchestration Capability of Large Language Models](https://openreview.net/forum?id=3Hy00Wvabi) |  | 0 | Recent advancements in large language models (LLMs) have driven a revolutionary paradigm shift in process automation from Robotic Process Automation to Agentic Process Automation by automating the workflow orchestration procedure based on LLMs. However, existing LLMs (even the advanced OpenAI... | Maosong Sun, Shengda Fan, Shuyan Zhang, Xin Cong, Yankai Lin, Yesai Wu, Yuanwei Liu, Yuepeng Fu, Zhiyuan Liu, Zhong Zhang |  |
| 3126 |  |  [Intervening Anchor Token: Decoding Strategy in Alleviating Hallucinations for MLLMs](https://openreview.net/forum?id=zGb4WgCW5i) |  | 0 | Multimodal large language models (MLLMs) offer a powerful mechanism for interpreting visual information. However, they often suffer from hallucinations, which impede the real-world usage of these models. Existing methods attempt to alleviate this issue by designing special decoding strategies that... | Chengzhi Liu, Feilong Tang, Harry Yang, Qiang Sun, SerNam Lim, Zile Huang |  |
| 3127 |  |  [Estimation of single-cell and tissue perturbation effect in spatial transcriptomics via Spatial Causal Disentanglement](https://openreview.net/forum?id=Tqdsruwyac) |  | 0 | Models of Virtual Cells and Virtual Tissues at single-cell resolution would allow us to test perturbations in silico and accelerate progress in tissue and cell engineering. However, most such models are not rooted in causal inference and as a result, could mistake correlation for causation. We... | CarolaBibiane Schönlieb, Daniel G. Chen, Krzysztof Polanski, Moshe Eliasof, Sarah A. Teichmann, Stathis Megas |  |
| 3128 |  |  [ComPC: Completing a 3D Point Cloud with 2D Diffusion Priors](https://openreview.net/forum?id=SoUwcVplq4) |  | 0 | 3D point clouds directly collected from objects through sensors are often incomplete due to self-occlusion. Conventional methods for completing these partial point clouds rely on manually organized training sets and are usually limited to object categories seen during training. In this work, we... | Gim Hee Lee, Tianxin Huang, Yuyang Zhao, Zhiwen Yan |  |
| 3129 |  |  [SINGER: Stochastic Network Graph Evolving Operator for High Dimensional PDEs](https://openreview.net/forum?id=wVADj7yKee) |  | 0 | We present a novel framework, StochastIc Network Graph Evolving operatoR (SINGER), for learning the evolution operator of high-dimensional partial differential equations (PDEs). The framework uses a sub-network to approximate the solution at the initial time step and stochastically evolves the... | Junchi Yan, Mingquan Feng, Weixin Liao, Yixin Huang, Yizhou Liu, Yuhong Liu |  |
| 3130 |  |  [PhysPDE: Rethinking PDE Discovery and a Physical Hypothesis Selection Benchmark](https://openreview.net/forum?id=G3CpBCQwNh) |  | 0 | Despite extensive research, recovering PDE expressions from experimental observations often involves symbolic regression. This method generally lacks the incorporation of meaningful physical insights, resulting in outcomes lacking clear physical interpretations. Recognizing that the primary... | Bofang Jiang, Junchi Yan, Mingquan Feng, Yixin Huang, Yizhou Liu |  |
| 3131 |  |  [Track-On: Transformer-based Online Point Tracking with Memory](https://openreview.net/forum?id=oRlANEuqG5) |  | 0 | In this paper, we consider the problem of long-term point tracking, which requires consistent identification of points across multiple frames in a video, despite changes in appearance, lighting, perspective, and occlusions. We target online tracking on a frame-by-frame basis, making it suitable for... | Fatma Güney, Görkay Aydemir, Weidi Xie, Xiongyi Cai |  |
| 3132 |  |  [Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation](https://openreview.net/forum?id=C25SgeXWjE) |  | 0 | First-order logic (FOL) reasoning, which involves sequential deduction, is pivotal for intelligent systems and serves as a valuable task for evaluating reasoning capabilities, particularly in chain-of-thought (CoT) contexts. Existing benchmarks often rely on extensive human annotation or... | Binyuan Hui, Bowen Li, Chengwen Qi, Conghui He, He Du, Jinwang Wu, Ren Ma, Yuanjun Laili |  |
| 3133 |  |  [Revisiting Multi-Permutation Equivariance through the Lens of irreducible Representations](https://openreview.net/forum?id=4v4nmYWzBa) |  | 0 | This paper explores the characterization of equivariant linear layers for representations of permutations and related groups. Unlike traditional approaches, which address these problems using parameter-sharing, we consider an alternative methodology based on irreducible representations and Schur’s... | Ido Springer, Nadav Dym, Yonatan Sverdlov |  |
| 3134 |  |  [On the Expressive Power of Sparse Geometric MPNNs](https://openreview.net/forum?id=NY7aEek0mi) |  | 0 | Motivated by applications in chemistry and other sciences, we study the expressive power of message-passing neural networks for geometric graphs, whose node features correspond to 3-dimensional positions. Recent work has shown that such models can separate generic pairs of non-isomorphic geometric... | Nadav Dym, Yonatan Sverdlov |  |
| 3135 |  |  [Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)](https://openreview.net/forum?id=xkgfLXZ4e0) |  | 0 | Transformer-based language models, though not explicitly trained to mimic brain recordings, have demonstrated surprising alignment with brain activity. Progress in these models—through increased size, instruction-tuning, and multimodality—has led to better representational alignment with neural... | Akshett Rai Jindal, Bapi Raju Surampudi, Ishani Mondal, Khushbu Pahwa, Maneesh Kumar Singh, Manish Gupta, Manish Shrivastava, Satya Sai Srinath Namburi GNVV, Subba Reddy Oota |  |
| 3136 |  |  [Immunogenicity Prediction with Dual Attention Enables Vaccine Target Selection](https://openreview.net/forum?id=hWmwL9gizZ) |  | 0 | Immunogenicity prediction is a central topic in reverse vaccinology for finding candidate vaccines that can trigger protective immune responses. Existing approaches typically rely on highly compressed features and simple model architectures, leading to limited prediction accuracy and poor... | Bingxin Zhou, Liang Hong, Song Ke, Song Li, Yang Tan |  |
| 3137 |  |  [Bad-PFL: Exploiting Backdoor Attacks against Personalized Federated Learning](https://openreview.net/forum?id=79nO2DPjVX) |  | 0 | Data heterogeneity and backdoor attacks rank among the most significant challenges facing federated learning (FL). For data heterogeneity, personalized federated learning (PFL) enables each client to maintain a private personalized model to cater to client-specific knowledge. Meanwhile, vanilla FL... | Cen Chen, Fuyi Wang, Mingyuan Fan, Zhanyi Hu |  |
| 3138 |  |  [BrainOOD: Out-of-distribution Generalizable Brain Network Analysis](https://openreview.net/forum?id=3xqqYOKILp) |  | 0 | In neuroscience, identifying distinct patterns linked to neurological disorders, such as Alzheimer's and Autism, is critical for early diagnosis and effective intervention. Graph Neural Networks (GNNs) have shown promising in analyzing brain networks, but there are two major challenges in using... | James Cheng, Jiaxing Xu, Mengcheng Lan, Qingtian Bian, Tiancheng Huang, Xia Dong, Yiping Ke, Yongqiang Chen |  |
| 3139 |  |  [WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio Language Modeling](https://openreview.net/forum?id=yBlVlS2Fd9) |  | 0 | Language models have been effectively applied to modeling natural signals, such as images, video, speech, and audio. A crucial component of these models is the codec tokenizer, which compresses high-dimensional natural signals into lower-dimensional discrete tokens. In this paper, we introduce... | Jialong Zuo, Minghui Fang, Qian Chen, Qian Yang, Rongjie Huang, Ruiqi Li, Shengpeng Ji, Siqi Zheng, Wen Wang, Xiaoda Yang, Xize Cheng, Yidi Jiang, Yifu Chen, Zehan Wang, Zhou Zhao, Ziang Zhang, Ziyue Jiang |  |
| 3140 |  |  [Integrative Decoding: Improving Factuality via Implicit Self-consistency](https://openreview.net/forum?id=gGWYecsK1U) |  | 0 | Self-consistency-based approaches, which involve repeatedly sampling multiple outputs and selecting the most consistent one as the final response, prove to be remarkably effective in improving the factual accuracy of large language models. Nonetheless, existing methods usually have strict... | Jian Jiao, Kaishuai Xu, Peng Cheng, Qi Chen, Song Wang, Wayne Xiong, Wen Xiao, Wenge Liu, Wenjie Li, Wenjun Hou, Xiao Liang, Yeyun Gong, Yi Cheng, Yuji Zhang |  |
| 3141 |  |  [Structuring Benchmark into Knowledge Graphs to Assist Large Language Models in Retrieving and Designing Models](https://openreview.net/forum?id=49fIu0yDJ4) |  | 0 | In recent years, the design and transfer of neural network models have been widely studied due to their exceptional performance and capabilities. However, the complex nature of datasets and the vast architecture space pose significant challenges for both manual and automated algorithms in creating... | Hanmo Liu, Jiachuan Wang, Jialiang Wang, Lei Chen, Shimin Di, Xiaofang Zhou, Zhili Wang |  |
| 3142 |  |  [Learnable Expansion of Graph Operators for Multi-Modal Feature Fusion](https://openreview.net/forum?id=SMZqIOSdlN) |  | 0 | In computer vision tasks, features often come from diverse representations, domains (e.g., indoor and outdoor), and modalities (e.g., text, images, and videos). Effectively fusing these features is essential for robust performance, especially with the availability of powerful pre-trained models... | Dexuan Ding, Lei Wang, Liyun Zhu, Piotr Koniusz, Tom Gedeon |  |
| 3143 |  |  [OccProphet: Pushing the Efficiency Frontier of Camera-Only 4D Occupancy Forecasting with an Observer-Forecaster-Refiner Framework](https://openreview.net/forum?id=vC7AlY1ytz) |  | 0 | Predicting variations in complex traffic environments is crucial for the safety of autonomous driving. Recent advancements in occupancy forecasting have enabled forecasting future 3D occupied status in driving environments by observing historical 2D images. However, high computational demands make... | Huaiyuan Xu, Junliang Chen, LapPui Chau, Yi Wang |  |
| 3144 |  |  [Glimpse: Enabling White-Box Methods to Use Proprietary Models for Zero-Shot LLM-Generated Text Detection](https://openreview.net/forum?id=an3fugFA23) |  | 0 | Advanced large language models (LLMs) can generate text almost indistinguishable from human-written text, highlighting the importance of LLM-generated text detection. However, current zero-shot techniques face challenges as white-box methods are restricted to use weaker open-source LLMs, and... | Guangsheng Bao, Juncai He, Yanbin Zhao, Yue Zhang |  |
| 3145 |  |  [Exact Byte-Level Probabilities from Tokenized Language Models for FIM-Tasks and Model Ensembles](https://openreview.net/forum?id=zGej22CBnS) |  | 0 | Tokenization is associated with many poorly understood shortcomings in language models (LMs), yet remains an important component for long sequence scaling purposes. This work studies how tokenization impacts model performance by analyzing and comparing the stochastic behavior of tokenized models... | Brandon Amos, Buu Phan, Itai Gat, Karen Ullrich, Marton Havasi, Matthew J. Muckley |  |
| 3146 |  |  [Context-Alignment: Activating and Enhancing LLMs Capabilities in Time Series](https://openreview.net/forum?id=syC2764fPc) |  | 0 | Recently, leveraging pre-trained Large Language Models (LLMs) for time series (TS) tasks has gained increasing attention, which involves activating and enhancing LLMs' capabilities. Many methods aim to activate LLMs' capabilities based on token-level alignment, but overlook LLMs' inherent strength... | Dongxiao Zhang, Jinyue Yan, Qian Li, Yuntian Chen, Yuxiao Hu |  |
| 3147 |  |  [Prototype antithesis for biological few-shot class-incremental learning](https://openreview.net/forum?id=bRqaHn3J5I) |  | 0 | Deep learning has become essential in the biological species recognition task. However, a significant challenge is the ability to continuously learn new or mutated species with limited annotated samples. Since species within the same family typically share similar traits, distinguishing between new... | Binghao Liu, Fang Wan, Fei Gu, Han Yang |  |
| 3148 |  |  [Growth Inhibitors for Suppressing Inappropriate Image Concepts in Diffusion Models](https://openreview.net/forum?id=w4C4z80w59) |  | 0 | Despite their remarkable image generation capabilities, text-to-image diffusion models inadvertently learn inappropriate concepts from vast and unfiltered training data, which leads to various ethical and business risks. Specifically, model-generated images may exhibit not safe for work (NSFW)... | Cen Chen, Die Chen, Mingyuan Fan, Wenmeng Zhou, Yaliang Li, Yanhao Wang, Zhiwen Li |  |
| 3149 |  |  [Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality](https://openreview.net/forum?id=AV7OXVlAyi) |  | 0 | Multimodal Large Language Models (MLLMs) have emerged as a central focus in both industry and academia, but often suffer from biases introduced by visual and language priors, which can lead to multimodal hallucination. These biases arise from the visual encoder and the Large Language Model (LLM)... | Aiwei Liu, Guanyu Zhou, Kun Wang, Xin Zou, Xuming Hu, Yibo Yan |  |
| 3150 |  |  [Super(ficial)-alignment: Strong Models May Deceive Weak Models in Weak-to-Strong Generalization](https://openreview.net/forum?id=HxKSzulSD1) |  | 0 | Superalignment, where humans act as weak supervisors for superhuman models, has become a crucial problem with the rapid development of Large Language Models (LLMs). Recent work has preliminarily studied this problem by using weak models to supervise strong models, and discovered that weakly... | Gong Zhi, Guangyao Shen, JiRong Wen, Shiqi Shen, Wei Yao, Wenkai Yang, Yankai Lin, Yong Liu |  |
| 3151 |  |  [ECHOPulse: ECG Controlled Echocardio-gram Video Generation](https://openreview.net/forum?id=i2r7LDjba3) |  | 0 | Echocardiography (ECHO) is essential for cardiac assessments, but its video quality and interpretation heavily relies on manual expertise, leading to inconsistent results from clinical and portable devices. ECHO video generation offers a solution by improving automated monitoring through synthetic... | Hanqi Jiang, Pengfei Jin, Quanzheng Li, Sekeun Kim, Sifan Song, Tianming Liu, Tianze Yang, Xiang Li, Xiaowei Yu, Yi Pan, Yiwei Li, Yucheng Shi, Zihao Wu |  |
| 3152 |  |  [Training-Free Diffusion Model Alignment with Sampling Demons](https://openreview.net/forum?id=tfemquulED) |  | 0 | Aligning diffusion models with user preferences has been a key challenge. Existing methods for aligning diffusion models either require retraining or are limited to differentiable reward functions. To address these limitations, we propose a stochastic optimization approach, dubbed \*Demon\*, to... | JunCheng Chen, KuangHuei Lee, PoHung Yeh |  |
| 3153 |  |  [MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models](https://openreview.net/forum?id=yOOJwR15xg) |  | 0 | The pretrain+fine-tune paradigm is foundational for deploying large language models (LLMs) across various downstream applications. Within this framework, Low-Rank Adaptation (LoRA) stands out for its parameter-efficient fine-tuning (PEFT), producing numerous reusable task-specific LoRA adapters.... | Jingwei Xu, Junyu Lai, Yunpeng Huang |  |
| 3154 |  |  [ToolACE: Winning the Points of LLM Function Calling](https://openreview.net/forum?id=8EB8k6DdCU) |  | 0 | Function calling significantly extends the application boundary of large language models (LLMs), where high-quality and diverse training data is critical for unlocking this capability. However, collecting and annotating real function-calling data is challenging, while synthetic data from existing... | Bin Wang, Chuhan Wu, Dandan Tu, Defu Lian, Dexun Li, Duyu Tang, Enhong Chen, Lifeng Shang, Qun Liu, Ruiming Tang, Shuai Wang, Shuai Yu, Weinan Gan, Weiwen Liu, Wu Ning, Xin Jiang, Xingshan Zeng, Xinlong Hao, Xinzhi Wang, Xu Huang, Yasheng Wang, Yong Liu, Yuanqing Yu, Yutai Hou, Yuxian Wang, Zezhong Wang, Zhengying Liu |  |
| 3155 |  |  [Improving Data Efficiency via Curating LLM-Driven Rating Systems](https://openreview.net/forum?id=DKkQtRMowq) |  | 0 | Instruction tuning is critical for adapting large language models (LLMs) to downstream tasks, and recent studies have demonstrated that small amounts of human-curated data can outperform larger datasets, challenging traditional data scaling laws. While LLM-based data quality rating systems offer a... | Ankit Shah, Chen Qian, Jiaheng Wei, Jinlong Pang, Wei Wei, Yang Liu, Yaxuan Wang, Yujia Bao, Zhaowei Zhu |  |
| 3156 |  |  [VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning](https://openreview.net/forum?id=cpGPPLLYYx) |  | 0 | Large language models (LLMs) famously exhibit emergent in-context learning (ICL) - the ability to rapidly adapt to new tasks using few-shot examples provided as a prompt, without updating the model's weights. Built on top of LLMs, vision large language models (VLLMs) have advanced significantly in... | Ondrej Bohdal, Timothy M. Hospedales, Yongshuo Zong |  |
| 3157 |  |  [U-Nets as Belief Propagation: Efficient Classification, Denoising, and Diffusion in Generative Hierarchical Models](https://openreview.net/forum?id=sy1lbQxj9J) |  | 0 | U-Nets are among the most widely used architectures in computer vision, renowned for their exceptional performance in applications such as image segmentation, denoising, and diffusion modeling. However, a theoretical explanation of the U-Net architecture design has not yet been fully established.... | Song Mei |  |
| 3158 |  |  [Controlled LLM Decoding via Discrete Auto-regressive Biasing](https://openreview.net/forum?id=Duuerhutvq) |  | 0 | Controlled text generation allows for enforcing user-defined constraints on large language model outputs, an increasingly important field as LLMs become more prevalent in everyday life. One common approach uses energy-based decoding, which defines a target distribution through an energy function... | Patrick Pynadath, Ruqi Zhang |  |
| 3159 |  |  [A Single Goal is All You Need: Skills and Exploration Emerge from Contrastive RL without Rewards, Demonstrations, or Subgoals](https://openreview.net/forum?id=xCkgX4Xfu0) |  | 0 | In this paper, we present empirical evidence of skills and directed exploration emerging from a simple RL algorithm long before any successful trials are observed. For example, in a manipulation task, the agent is given a single observation of the goal state (see Fig. 1) and learns skills, first... | Benjamin Eysenbach, Grace Liu, Michael Tang |  |
| 3160 |  |  [Efficient Dictionary Learning with Switch Sparse Autoencoders](https://openreview.net/forum?id=k2ZVAzVeMP) |  | 0 | Sparse autoencoders (SAEs) are a recent technique for decomposing neural network activations into human-interpretable features. However, in order for SAEs to identify all features represented in frontier models, it will be necessary to scale them up to very high width, posing a computational... | Anish Mudide, Christian Schröder de Witt, Eric J. Michaud, Joshua Engels, Max Tegmark |  |
| 3161 |  |  [Scaling Wearable Foundation Models](https://openreview.net/forum?id=yb4QE6b22f) |  | 0 | Wearable sensors have become ubiquitous thanks to a variety of health tracking features. The resulting continuous and longitudinal measurements from everyday life generate large volumes of data. However, making sense of these observations for scientific and actionable insights is non-trivial.... | Daniel McDuff, Girish Narayanswamy, Jacob E. Sunshine, Jake Garrison, Jiening Zhan, Kumar Ayush, Mark Malhotra, Pushmeet Kohli, Samy AbdelGhaffar, Shrikanth Narayanan, Shun Liao, Shwetak N. Patel, Shyam A. Tailor, Tim Althoff, Xin Liu, Xuhai Xu, Yun Liu, Yuzhe Yang |  |
| 3162 |  |  [Asynchronous Federated Reinforcement Learning with Policy Gradient Updates: Algorithm Design and Convergence Analysis](https://openreview.net/forum?id=5DUekOKWcS) |  | 0 | To improve the efficiency of reinforcement learning (RL), we propose a novel asynchronous federated reinforcement learning (FedRL) framework termed AFedPG, which constructs a global model through collaboration among $N$ agents using policy gradient (PG) updates. To address the challenge of lagged... | Abolfazl Hashemi, Christopher Brinton, DongJun Han, Guangchen Lan, Vaneet Aggarwal |  |
| 3163 |  |  [Hummingbird: High Fidelity Image Generation via Multimodal Context Alignment](https://openreview.net/forum?id=6kPBThI6ZJ) |  | 0 | While diffusion models are powerful in generating high-quality, diverse synthetic data for object-centric tasks, existing methods struggle with scene-aware tasks such as Visual Question Answering (VQA) and Human-Object Interaction (HOI) Reasoning, where it is critical to preserve scene attributes... | A S. M. Iftekhar, Barun Patra, Dimitris Samaras, Gaurav Mittal, Mei Chen, MinhQuan Le, Tianjian Meng, Vishwas Suryanarayanan |  |
| 3164 |  |  [On Designing General and Expressive Quantum Graph Neural Networks with Applications to MILP Instance Representation](https://openreview.net/forum?id=IQi8JOqLuv) |  | 0 | Graph-structured data is ubiquitous, and graph learning models have recently been extended to address complex problems like mixed-integer linear programming (MILP). However, studies have shown that the vanilla message-passing based graph neural networks (GNNs) suffer inherent limitations in... | Hao Xiong, Jia Wang, Jianhao Huang, Junchi Yan, Xinyu Ye, Ziang Chen |  |
| 3165 |  |  [Personalized Visual Instruction Tuning](https://openreview.net/forum?id=sAxdIJ4l6z) |  | 0 | Recent advancements in multimodal large language models (MLLMs) have demonstrated significant progress; however, these models exhibit a notable limitation, which we refer to as "face blindness." Specifically, they can engage in general conversations but fail to conduct personalized dialogues... | Jianshu Zhang, Jipeng Zhang, Renjie Pi, Rui Pan, Tianyang Han, Tong Zhang |  |
| 3166 |  |  [Neural Approximate Mirror Maps for Constrained Diffusion Models](https://openreview.net/forum?id=vgZDcUetWS) |  | 0 | Diffusion models excel at creating visually-convincing images, but they often struggle to meet subtle constraints inherent in the training data. Such constraints could be physics-based (e.g., satisfying a PDE), geometric (e.g., respecting symmetry), or semantic (e.g., including a particular number... | Berthy Feng, Katherine L. Bouman, Ricardo Baptista |  |
| 3167 |  |  [Chain-of-region: Visual Language Models Need Details for Diagram Analysis](https://openreview.net/forum?id=M6fYrICcQs) |  | 0 | Visual Language Models (VLMs) like GPT-4V have broadened the scope of LLM applications, yet they face significant challenges in accurately processing visual details, particularly in scientific diagrams. This paper explores the necessity of meticulous visual detail collection and region... | Haifeng Chen, Wei Cheng, Xue Li, Yinglun Zhu, Yiyou Sun |  |
| 3168 |  |  [pMoE: Prompting Diverse Experts Together Wins More in Visual Adaptation](https://openreview.net/forum?id=scozdyKzET) |  | 0 | Parameter-efficient fine-tuning has demonstrated promising results across various visual adaptation tasks, such as classification and segmentation. Typically, prompt tuning techniques have harnessed knowledge from a single pre-trained model, whether from a general or a specialized medical domain.... | Dongsheng Li, Shentong Mo, Xufang Luo |  |
| 3169 |  |  [MMAD: A Comprehensive Benchmark for Multimodal Large Language Models in Industrial Anomaly Detection](https://openreview.net/forum?id=JDiER86r8v) |  | 0 | In the field of industrial inspection, Multimodal Large Language Models (MLLMs) have a high potential to renew the paradigms in practical applications due to their robust language capabilities and generalization abilities. However, despite their impressive problem-solving skills in many domains,... | BinBin Gao, Chengjie Wang, Feng Zheng, Hanqiu Deng, Jialin Li, Jian Li, Xi Jiang, Yifeng Zhou, Yong Liu |  |
| 3170 |  |  [TSC-Net: Prediction of Pedestrian Trajectories by Trajectory-Scene-Cell Classification](https://openreview.net/forum?id=Xmh5gdMfRJ) |  | 0 | To predict future trajectories of pedestrians, scene is as important as the history trajectory since i) scene reflects the position of possible goals of the pedestrian ii) trajectories are affected by the semantic information of the scene. It requires the model to capture scene information and... | Bo Hu, TatJen Cham |  |
| 3171 |  |  [RRM: Robust Reward Model Training Mitigates Reward Hacking](https://openreview.net/forum?id=88AS5MQnmC) |  | 0 | Reward models (RMs) play a pivotal role in aligning large language models (LLMs) with human preferences. However, traditional RM training, which relies on response pairs tied to specific prompts, struggles to disentangle prompt-driven preferences from prompt-independent artifacts, such as response... | Abe Ittycheriah, Anastasia Makarova, Aviral Kumar, Bilal Piot, Daniel Sohn, Jeremiah Zhe Liu, Jiaming Shen, Jie Ren, Junru Wu, Lichang Chen, Mohammad Saleh, Rishabh Joshi, Tianhe Yu, Tianqi Liu, Wei Xiong, Yang Gao, Yuan Liu, Zhen Qin |  |
| 3172 |  |  [Vertical Federated Learning with Missing Features During Training and Inference](https://openreview.net/forum?id=OXi1FmHGzz) |  | 0 | Vertical federated learning trains models from feature-partitioned datasets across multiple clients, who collaborate without sharing their local data. Standard approaches assume that all feature partitions are available during both training and inference. Yet, in practice, this assumption rarely... | Pedro Valdeira, Shiqiang Wang, Yuejie Chi |  |
| 3173 |  |  [High-Quality Joint Image and Video Tokenization with Causal VAE](https://openreview.net/forum?id=aRD1NqcXTC) |  | 0 | Generative modeling has seen significant advancements in image and video synthesis. However, the curse of dimensionality remains a significant obstacle, especially for video generation, given its inherently complex and high-dimensional nature. Many existing works rely on low-dimensional latent... | Dawit Mureja Argaw, Fitsum Reda, Joon Son Chung, MingYu Liu, Qinsheng Zhang, Xian Liu |  |
| 3174 |  |  [Joint Fine-tuning and Conversion of Pretrained Speech and Language Models towards Linear Complexity](https://openreview.net/forum?id=90Db4RUBc7) |  | 0 | Architectures such as Linformer and Mamba have recently emerged as competitive linear time replacements for transformers. However, corresponding large pretrained models are often unavailable, especially in non-text domains. To remedy this, we present a Cross-Architecture Layerwise Distillation... | Mutian He, Philip N. Garner |  |
| 3175 |  |  [Proximal Mapping Loss: Understanding Loss Functions in Crowd Counting & Localization](https://openreview.net/forum?id=7p8CcxP1Xc) |  | 0 | Crowd counting and localization involve extracting the number and distribution of crowds from images or videos using computer vision techniques. Most counting methods are based on density regression and are based on an \`\`intersection'' hypothesis, \*i.e.\*, one pixel is influenced by multiple... | Antoni B. Chan, Jia Wan, Wei Lin |  |
| 3176 |  |  [Bandit Learning in Matching Markets with Indifference](https://openreview.net/forum?id=7ENakslm9J) |  | 0 | A rich line of recent works studies how participants in matching markets learn their unknown preferences through iterative interactions with each other. The two sides of participants in the market can be respectively formulated as players and arms in the bandit problem. To ensure market stability,... | Fang Kong, Jingqi Tang, John C. S. Lui, Mingzhu Li, Pinyan Lu, Shuai Li |  |
| 3177 |  |  [Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering](https://openreview.net/forum?id=j6fsbpAllN) |  | 0 | Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning large language models (LLMs) to various domains due to its modular design and widespread availability on platforms like Huggingface. This modularity has sparked interest in combining multiple LoRAs to significantly... | Didi Zhu, Fei Wu, Jing Su, Tao Shen, Xuwu Wang, Zexi Li, Ziyu Zhao |  |
| 3178 |  |  [EgoExo-Gen: Ego-centric Video Prediction by Watching Exo-centric Videos](https://openreview.net/forum?id=8J2DrrWDKE) |  | 0 | Generating videos in the first-person perspective has broad application prospects in the field of augmented reality and embodied intelligence. In this work, we explore the cross-view video prediction task, where given an exo-centric video, the first frame of the corresponding ego-centric video, and... | Baoqi Pei, Guo Chen, Jilan Xu, Junlin Hou, Qingqiu Li, Rui Feng, Weidi Xie, Yifei Huang, Yuejie Zhang |  |
| 3179 |  |  [UniGEM: A Unified Approach to Generation and Property Prediction for Molecules](https://openreview.net/forum?id=Lb91pXwZMR) |  | 0 | Molecular generation and molecular property prediction are both crucial for drug discovery, but they are often developed independently. Inspired by recent studies, which demonstrate that diffusion model, a prominent generative approach, can learn meaningful data representations that enhance... | Shikun Feng, WeiYing Ma, Yan Lu, Yanyan Lan, Yuyan Ni, ZhiMing Ma |  |
| 3180 |  |  [SmartRAG: Jointly Learn RAG-Related Tasks From the Environment Feedback](https://openreview.net/forum?id=OCd3cffulp) |  | 0 | RAG systems consist of multiple modules to work together. However, these modules are usually separately trained. We argue that a system like RAG that incorporates multiple modules should be jointly optimized to achieve optimal performance. To demonstrate this, we design a specific pipeline called... | Bin Dai, Jingsheng Gao, Ke Ji, Linxu Li, Weiyuan Li, Yixin Lian, Yuzhuo Fu |  |
| 3181 |  |  [Self-Supervised Diffusion Models for Electron-Aware Molecular Representation Learning](https://openreview.net/forum?id=UQ0RqfhgCk) |  | 0 | Physical properties derived from electronic distributions are essential information that determines molecular properties. However, the electron-level information is not accessible in most real-world complex molecules due to the extensive computational costs of determining uncertain electronic... | Chanyoung Park, Gyoung S. Na |  |
| 3182 |  |  [HShare: Fast LLM Decoding by Hierarchical Key-Value Sharing](https://openreview.net/forum?id=Tb5PY5vwp6) |  | 0 | The frequent retrieval of Key-Value (KV) cache data has emerged as a significant factor contributing to the inefficiency of the inference process in large language models. Previous research has demonstrated that a small subset of critical KV cache tokens largely influences attention outcomes,... | Hantao Huang, Huaijin Wu, Jihang Zhang, Junchi Yan, Lianqiang Li, Minghui Yu, Tu Yi |  |
| 3183 |  |  [Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations?](https://openreview.net/forum?id=lCasyP21Bf) |  | 0 | Vision and language model (VLM) decoders are currently the best-performing architectures on multimodal tasks. Next to answers, they are able to produce natural language explanations, either in post-hoc or CoT settings. However, it is not clear to what extent they are using the input vision and text... | Anette Frank, Letitia Parcalabescu |  |
| 3184 |  |  [Generalizability of Neural Networks Minimizing Empirical Risk Based on Expressive Power](https://openreview.net/forum?id=8wAL9ywQNB) |  | 0 | The primary objective of learning methods is generalization. Classic generalization bounds, based on VC-dimension or Rademacher complexity, are uniformly applicable to all networks in the hypothesis space. On the other hand, algorithm-dependent generalization bounds, like stability bounds, address... | Lijia Yu, Lijun Zhang, XiaoShan Gao, Yibo Miao, Yifan Zhu |  |
| 3185 |  |  [Limits of Deep Learning: Sequence Modeling through the Lens of Complexity Theory](https://openreview.net/forum?id=DhdqML3FdM) |  | 0 | Despite their successes, deep learning models struggle with tasks requiring complex reasoning and function composition. We present a theoretical and empirical investigation into the limitations of Structured State Space Models (SSMs) and Transformers in such tasks. We prove that one-layer SSMs... | Aurelio L. Sulser, Davide Scaramuzza, Federico Soldà, Nikola Zubic |  |
| 3186 |  |  [It Helps to Take a Second Opinion: Teaching Smaller LLMs To Deliberate Mutually via Selective Rationale Optimisation](https://openreview.net/forum?id=NHxwxc3ql6) |  | 0 | Very large language models (LLMs) such as GPT-4 have shown the ability to handle complex tasks by generating and self-refining step-by-step rationales. Smaller language models (SLMs), typically with < 13B parameters, have been improved by using the data generated from very-large LMs through... | Balaji Krishnamurthy, Milan Aggarwal, Sohan Patnaik, Sumit Bhatia |  |
| 3187 |  |  [Heavy-Tailed Diffusion with Denoising Levy Probabilistic Models](https://openreview.net/forum?id=SYmUS6qRub) |  | 0 | Investigating noise distributions beyond Gaussian in diffusion generative models remains an open challenge. The Gaussian case has been a large success experimentally and theoretically, admitting a unified stochastic differential equation (SDE) framework, encompassing score-based and denoising... | Alain Oliviero Durmus, Dario Shariatian, Umut Simsekli |  |
| 3188 |  |  [MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models](https://openreview.net/forum?id=WsgEWL8i0K) |  | 0 | The capability to process multiple images is crucial for Large Vision-Language Models (LVLMs) to develop a more thorough and nuanced understanding of a scene. Recent multi-image LVLMs have begun to address this need. However, their evaluation has not kept pace with their development. To fill this... | Chuanhao Li, Fanqing Meng, Hao Tian, Jiaqi Liao, Jifeng Dai, Jin Wang, Kaipeng Zhang, Ping Luo, Quanfeng Lu, Tianshuo Yang, Wenqi Shao, Xizhou Zhu, Yu Qiao |  |
| 3189 |  |  [From Layers to States: A State Space Model Perspective to Deep Neural Network Layer Dynamics](https://openreview.net/forum?id=msD4DHZzFg) |  | 0 | The depth of neural networks is a critical factor for their capability, with deeper models often demonstrating superior performance. Motivated by this, significant efforts have been made to enhance layer aggregation - reusing information from previous layers to better extract features at the... | Guodong Li, Lequan Yu, Qinshuo Liu, Wei Huang, Weiqin Zhao, Yanwen Fang |  |
| 3190 |  |  [VAE-Var: Variational Autoencoder-Enhanced Variational Methods for Data Assimilation in Meteorology](https://openreview.net/forum?id=utz99dx2RN) |  | 0 | Data assimilation (DA) is an essential statistical technique for generating accurate estimates of a physical system's states by combining prior model predictions with observational data, especially in the realm of weather forecasting. Effectively modeling the prior distribution while adapting to... | Kun Chen, Lei Bai, Qilong Jia, Wei Xue, Yi Xiao |  |
| 3191 |  |  [Towards Domain Adaptive Neural Contextual Bandits](https://openreview.net/forum?id=LNkMWCEssX) |  | 0 | Contextual bandit algorithms are essential for solving real-world decision making problems. In practice, collecting a contextual bandit's feedback from different domains may involve different costs. For example, measuring drug reaction from mice (as a source domain) and humans (as a target domain).... | Hao Wang, Xiaoming Huo, Ziyan Wang |  |
| 3192 |  |  [Phidias: A Generative Model for Creating 3D Content from Text, Image, and 3D Conditions with Reference-Augmented Diffusion](https://openreview.net/forum?id=TEkoMEjf7E) |  | 0 | Generative 3D modeling has made significant advances recently, but it remains constrained by its inherently ill-posed nature, leading to challenges in quality and controllability. Inspired by the real-world workflow that designers typically refer to existing 3D models when creating new ones, we... | Gerhard Petrus Hancke, Rynson W. H. Lau, Tengfei Wang, Zexin He, Zhenwei Wang, Ziwei Liu |  |
| 3193 |  |  [FLIP: Flow-Centric Generative Planning as General-Purpose Manipulation World Model](https://openreview.net/forum?id=B2N0nCVC91) |  | 0 | We aim to develop a model-based planning framework for world models that can be scaled with increasing model and data budgets for general-purpose manipulation tasks with only language and vision inputs. To this end, we present FLow-CentrIc generative Planning (FLIP), a model-based planning... | Chongkai Gao, Haozhuo Zhang, Lin Shao, Zhehao Cai, Zhixuan Xu |  |
| 3194 |  |  [Causal Graph Transformer for Treatment Effect Estimation Under Unknown Interference](https://openreview.net/forum?id=foQ4AeEGG7) |  | 0 | Networked interference, also known as the peer effect in social science and spillover effect in economics, has drawn increasing interest across various domains. This phenomenon arises when a unit’s treatment and outcome are influenced by the actions of its peers, posing significant challenges to... | Anpeng Wu, Fei Wu, Haiyi Qiu, Kun Zhang, Ruoxuan Xiong, Zhengming Chen, Zijian Li |  |
| 3195 |  |  [Can One Modality Model Synergize Training of Other Modality Models?](https://openreview.net/forum?id=5BXWhVbHAK) |  | 0 | Learning with multiple modalities has recently demonstrated significant gains in many domains by maximizing the shared information across modalities. However, the current approaches strongly rely on high-quality paired datasets, which allow co-training from the paired labels from different... | JaeJun Lee, Sung Whan Yoon |  |
| 3196 |  |  [Aligned Datasets Improve Detection of Latent Diffusion-Generated Images](https://openreview.net/forum?id=doBkiqESYq) |  | 0 | As latent diffusion models (LDMs) democratize image generation capabilities, there is a growing need to detect fake images. A good detector should focus on the generative model’s fingerprints while ignoring image properties such as semantic content, resolution, file format, etc. Fake image... | Anirudh Sundara Rajan, Jedidiah Schloesser, Utkarsh Ojha, Yong Jae Lee |  |
| 3197 |  |  [RocketEval: Efficient automated LLM evaluation via grading checklist](https://openreview.net/forum?id=zJjzNj6QUe) |  | 0 | Evaluating large language models (LLMs) in diverse and challenging scenarios is essential to align them with human preferences. To mitigate the prohibitive costs associated with human evaluations, utilizing a powerful LLM as a judge has emerged as a favored approach. Nevertheless, this methodology... | Jianghong Ma, Ruizhi Qiao, Tianjun Wei, Wei Wen, Xing Sun |  |
| 3198 |  |  [PolaFormer: Polarity-aware Linear Attention for Vision Transformers](https://openreview.net/forum?id=kN6MFmKUSK) |  | 0 | Linear attention has emerged as a promising alternative to softmax-based attention, leveraging kernelized feature maps to reduce complexity from quadratic to linear in sequence length. However, the non-negative constraint on feature maps and the relaxed exponential function used in approximation... | Dongmei Jiang, Weikang Meng, Xin Li, Yadan Luo, Zheng Zhang |  |
| 3199 |  |  [Learning Gain Map for Inverse Tone Mapping](https://openreview.net/forum?id=GtHRhpgpzB) |  | 0 | For a more compatible and consistent high dynamic range (HDR) viewing experience, a new image format with a double-layer structure has been developed recently, which incorporates an auxiliary Gain Map (GM) within a standard dynamic range (SDR) image for adaptive HDR display. This new format... | Jiacheng Li, Ruikang Xu, Shida Sun, Yinuo Liao, Yuanshen Guan, Zhiwei Xiong |  |
| 3200 |  |  [SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression](https://openreview.net/forum?id=LNYIUouhdt) |  | 0 | The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitates LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM... | Mi Zhang, Xin Wang, Yu Zheng, Zhongwei Wan |  |
| 3201 |  |  [CG-Bench: Clue-grounded Question Answering Benchmark for Long Video Understanding](https://openreview.net/forum?id=le4IoZZHy1) |  | 0 | The existing video understanding benchmarks for multimodal large language models (MLLMs) mainly focus on short videos. The few benchmarks for long video understanding often rely on multiple-choice questions (MCQs). Due to the limitations of MCQ evaluations and the advanced reasoning abilities of... | Baoqi Pei, Guo Chen, Jilan Xu, Limin Wang, Tong Lu, Yali Wang, Yicheng Liu, Yifei Huang, Yuping He |  |
| 3202 |  |  [Parameter and Memory Efficient Pretraining via Low-rank Riemannian Optimization](https://openreview.net/forum?id=i0zzO7Hslk) |  | 0 | Pretraining large language models often requires significant computational resources and memory due to their vast parameter amount. An effective approach to enhance parameter efficiency in both training and inference is to parameterize each full-size weight as the product of two trainable low-rank... | LongKai Huang, Sinno Jialin Pan, Zhanfeng Mo |  |
| 3203 |  |  [Latent Radiance Fields with 3D-aware 2D Representations](https://openreview.net/forum?id=vL9t9tpKli) |  | 0 | Latent 3D reconstruction has shown great promise in empowering 3D semantic understanding and 3D generation by distilling 2D features into the 3D space. However, existing approaches struggle with the domain gap between 2D feature space and 3D representations, resulting in degraded rendering... | Chaoyi Zhou, Feng Luo, Siyu Huang, Xi Liu |  |
| 3204 |  |  [Memory Efficient Transformer Adapter for Dense Predictions](https://openreview.net/forum?id=vJkktqyU8B) |  | 0 | While current Vision Transformer (ViT) adapter methods have shown promising accuracy, their inference speed is implicitly hindered by inefficient memory access operations, e.g., standard normalization and frequent reshaping. In this work, we propose META, a simple and fast ViT adapter that can... | Dong Zhang, KwangTing Cheng, Pingcheng Dong, Rui Yan |  |
| 3205 |  |  [Policy Optimization under Imperfect Human Interactions with Agent-Gated Shared Autonomy](https://openreview.net/forum?id=LfekK1E0QE) |  | 0 | We introduce AGSA, an Agent-Gated Shared Autonomy framework that learns from high-level human feedback to tackle the challenges of reward-free training, safe exploration, and imperfect low-level human control. Recent human-in-the loop learning methods enable human participants to intervene a... | Bo An, Shuicheng Yan, Zhenghai Xue |  |
| 3206 |  |  [Multiview Equivariance Improves 3D Correspondence Understanding with Minimal Feature Finetuning](https://openreview.net/forum?id=CNO4rbSV6v) |  | 0 | Vision foundation models, particularly the ViT family, have revolutionized image understanding by providing rich semantic features. However, despite their success in 2D comprehension, their abilities on grasping 3D spatial relationships are still unclear. In this work, we evaluate and enhance the... | Congyue Deng, Leonidas J. Guibas, Yang You, Yixin Li, Yue Wang |  |
| 3207 |  |  [Scale-aware Recognition in Satellite Images under Resource Constraints](https://openreview.net/forum?id=QIxFo9mFwR) |  | 0 | Recognition of features in satellite imagery (forests, swimming pools, etc.) depends strongly on the spatial scale of the concept and therefore the resolution of the images. This poses two challenges: Which resolution is best suited for recognizing a given concept, and where and when should the... | Bharath Hariharan, Cheng Perng Phoo, Kavita Bala, Shreelekha Revankar, Utkarsh Mall |  |
| 3208 |  |  [IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation](https://openreview.net/forum?id=4w99NAikOE) |  | 0 | Advanced diffusion models like Stable Diffusion 3, Omost, and FLUX have made notable strides in compositional text-to-image generation. However, these methods typically exhibit distinct strengths for compositional generation, with some excelling in handling attribute binding and others in spatial... | Bin Cui, Guohao Li, Jiake Xie, Ling Yang, Mengdi Wang, Xinchen Zhang, Yaqi Cai, Yong Tang, Yujiu Yang |  |
| 3209 |  |  [MambaQuant: Quantizing the Mamba Family with Variance Aligned Rotation Methods](https://openreview.net/forum?id=KI45uDnmzv) |  | 0 | Mamba is an efficient sequence model that rivals Transformers and demonstrates significant potential as a foundational architecture for various tasks. Quantization is commonly used in neural networks to reduce model size and computational latency. However, applying quantization to Mamba remains... | Chen Xu, Dawei Yang, Jiangyong Yu, Sifan Zhou, Xing Hu, Yuxuan Yue, Zhihang Yuan, Zhixuan Chen, Zixu Jiang, Zukang Xu |  |
| 3210 |  |  [ST-GCond: Self-supervised and Transferable Graph Dataset Condensation](https://openreview.net/forum?id=wYWJFLQov9) |  | 0 | The increasing scale of graph datasets significantly enhances deep learning models but also presents substantial training challenges. Graph dataset condensation has emerged to condense large datasets into smaller yet informative ones that maintain similar test performance. However, these methods... | Beining Yang, Cheng Ji, Jianxin Li, Qingyun Sun, Xingcheng Fu |  |
| 3211 |  |  [PPT: Patch Order Do Matters In Time Series Pretext Task](https://openreview.net/forum?id=7zwIEbSTDy) |  | 0 | Recently, patch-based models have been widely discussed in time series analysis. However, existing pretext tasks for patch-based learning, such as masking, may not capture essential time and channel-wise patch interdependencies in time series data, presumed to result in subpar model performance. In... | Jaeho Kim, Kwangryeol Park, Seulki Lee, Sukmin Yun |  |
| 3212 |  |  [Rethinking Graph Neural Networks From A Geometric Perspective Of Node Features](https://openreview.net/forum?id=lBMRmw59Lk) |  | 0 | Many works on graph neural networks (GNNs) focus on graph topologies and analyze graph-related operations to enhance performance on tasks such as node classification. In this paper, we propose to understand GNNs based on a feature-centric approach. Our main idea is to treat the features of nodes... | Feng Ji, Hanyang Meng, Jielong Yang, Kai Zhao, Wee Peng Tay, Yanan Zhao |  |
| 3213 |  |  [KiVA: Kid-inspired Visual Analogies for Testing Large Multimodal Models](https://openreview.net/forum?id=vNATZfmY6R) |  | 0 | This paper investigates visual analogical reasoning in large multimodal models (LMMs) compared to human adults and children. A “visual analogy” is an abstract rule inferred from one image and applied to another. While benchmarks exist for testing visual reasoning in LMMs, they require advanced... | Alison Gopnik, Anisa Noor Majhi, Charlie Wong, Eunice Yiu, Kate Saenko, Maan Qraitem, Shiry Ginosar, Yutong Bai |  |
| 3214 |  |  [Learning from weak labelers as constraints](https://openreview.net/forum?id=2BtFKEeMGo) |  | 0 | We study programmatic weak supervision, where in contrast to labeled data, we have access to \emph{weak labelers}, each of which either abstains or provides noisy labels corresponding to any input. Most previous approaches typically employ latent generative models that model the joint distribution... | MariaFlorina Balcan, Pradeep Kumar Ravikumar, Rattana Pukdee, Vishwajeet Agrawal |  |
| 3215 |  |  [Bisimulation Metric for Model Predictive Control](https://openreview.net/forum?id=F07ic7huE3) |  | 0 | Model-based reinforcement learning (MBRL) has shown promise for improving sample efficiency and decision-making in complex environments. However, existing methods face challenges in training stability, robustness to noise, and computational efficiency. In this paper, we propose Bisimulation Metric... | Masayoshi Tomizuka, Yutaka Shimizu |  |
| 3216 |  |  [Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling](https://openreview.net/forum?id=EgJhwYR2tB) |  | 0 | Recent advances in knowledge distillation (KD) have enabled smaller student models to approach the performance of larger teacher models. However, popular methods such as supervised KD and on-policy KD, are adversely impacted by the knowledge gaps between teacher-student in practical scenarios.... | ChenYu Lee, Dhruv Madeka, Lei Li, Long T. Le, Rishabh Agarwal, Rujun Han, Tomas Pfister, Wenda Xu, William Yang Wang, Zifeng Wang |  |
| 3217 |  |  [NUDGE: Lightweight Non-Parametric Fine-Tuning of Embeddings for Retrieval](https://openreview.net/forum?id=MYw74B77KQ) |  | 0 | $k$-Nearest Neighbor search on dense vector embeddings ($k$-NN retrieval) from pre-trained embedding models is the predominant retrieval method for text and images, as well as Retrieval-Augmented Generation (RAG) pipelines. In practice, application developers often fine-tune the embeddings to... | Aditya G. Parameswaran, Sepanta Zeighami, Zac Wellmer |  |
| 3218 |  |  [Private Mechanism Design via Quantile Estimation](https://openreview.net/forum?id=JQQDePbfxh) |  | 0 | We investigate the problem of designing differentially private (DP), revenue-maximizing single item auction. Specifically, we consider broadly applicable settings in mechanism design where agents' valuation distributions are \*\*independent\*\*, \*\*non-identical\*\*, and can be either... | Bhuvesh Kumar, Jamie H. Morgenstern, Tao Xiao, Yuanyuan Yang |  |
| 3219 |  |  [Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community Retrieval](https://openreview.net/forum?id=8fLgt7PQza) |  | 0 | Large language models (LLMs) have demonstrated significant potential in clinical decision support. Yet LLMs still suffer from hallucinations and lack fine-grained contextual medical knowledge, limiting their high-stake healthcare applications such as clinical diagnosis. Traditional... | Cao Xiao, Jiawei Han, Jimeng Sun, Minhao Jiang, Parminder Bhatia, Pengcheng Jiang, Taha A. KassHout |  |
| 3220 |  |  [Brain Mapping with Dense Features: Grounding Cortical Semantic Selectivity in Natural Images With Vision Transformers](https://openreview.net/forum?id=yJ9QNbpMi2) |  | 0 | We introduce BrainSAIL (Semantic Attribution and Image Localization), a method for linking neural selectivity with spatially distributed semantic visual concepts in natural scenes. BrainSAIL leverages recent advances in large-scale artificial neural networks, using them to provide insights into the... | Andrew F. Luo, Jacob Yeung, Leila Wehbe, Margaret M. Henderson, Michael J. Tarr, Rushikesh Zawar, Shaurya Dewan |  |
| 3221 |  |  [X-Drive: Cross-modality Consistent Multi-Sensor Data Synthesis for Driving Scenarios](https://openreview.net/forum?id=IEMmEd5Jgm) |  | 0 | Recent advancements have exploited diffusion models for the synthesis of either LiDAR point clouds or camera image data in driving scenarios. Despite their success in modeling single-modality data marginal distribution, there is an under- exploration in the mutual reliance between different... | Alexander T. Pham, Chenfeng Xu, Chensheng Peng, Masayoshi Tomizuka, Mingyu Ding, Nhat Ho, Shuqi Zhao, Wei Zhan, Yichen Xie |  |
| 3222 |  |  [Selective Attention Improves Transformer](https://openreview.net/forum?id=v0FzmPCd1e) |  | 0 | Unneeded elements in the attention’s context degrade performance. We introduce Selective Attention, a simple parameter-free change to the standard attention mechanism which reduces attention to unneeded elements. Selective attention consistently improves language modeling and downstream task... | Matan Kalman, Yaniv Leviathan, Yossi Matias |  |
| 3223 |  |  [STAR: Stability-Inducing Weight Perturbation for Continual Learning](https://openreview.net/forum?id=6N5OM5Duuj) |  | 0 | Humans can naturally learn new and varying tasks in a sequential manner. Continual learning is a class of learning algorithms that updates its learned model as it sees new data (on potentially new tasks) in a sequence. A key challenge in continual learning is that as the model is updated to learn... | Davin Hill, Jennifer G. Dy, Masih Eskandar, Tooba Imtiaz, Zifeng Wang |  |
| 3224 |  |  [OMNI-EPIC: Open-endedness via Models of human Notions of Interestingness with Environments Programmed in Code](https://openreview.net/forum?id=Y1XkzMJpPd) |  | 0 | Open-ended and AI-generating algorithms aim to continuously generate and solve increasingly complex tasks indefinitely, offering a promising path toward more general intelligence. To accomplish this grand vision, learning must occur within a vast array of potential tasks. Existing approaches to... | Antoine Cully, Jeff Clune, Jenny Zhang, Maxence Faldor |  |
| 3225 |  |  [CSA: Data-efficient Mapping of Unimodal Features to Multimodal Features](https://openreview.net/forum?id=6Mg7pjG7Sw) |  | 0 | Multimodal encoders like CLIP excel in tasks such as zero-shot image classification and cross-modal retrieval. However, they require excessive training data. We propose canonical similarity analysis (CSA), which uses two unimodal encoders to replicate multimodal encoders using limited data. CSA... | Pohan Li, Sandeep P. Chinchali, Ufuk Topcu |  |
| 3226 |  |  [EC-Diffuser: Multi-Object Manipulation via Entity-Centric Behavior Generation](https://openreview.net/forum?id=o3pJU5QCtv) |  | 0 | Object manipulation is a common component of everyday tasks, but learning to manipulate objects from high-dimensional observations presents significant challenges. These challenges are heightened in multi-object environments due to the combinatorial complexity of the state space as well as of the... | Amy Zhang, Aviv Tamar, Carl Qi, Dan Haramati, Tal Daniel |  |
| 3227 |  |  [A Truncated Newton Method for Optimal Transport](https://openreview.net/forum?id=gWrWUaCbMa) |  | 0 | Developing a contemporary optimal transport (OT) solver requires navigating trade-offs among several critical requirements: GPU parallelization, scalability to high-dimensional problems, theoretical convergence guarantees, empirical performance in terms of precision versus runtime, and numerical... | Allan Douglas Jepson, Amirmassoud Farahmand, Mete Kemertas |  |
| 3228 |  |  [Automated Proof Generation for Rust Code via Self-Evolution](https://openreview.net/forum?id=2NqssmiXLu) |  | 0 | Ensuring correctness is crucial for code generation. Formal verification offers a definitive assurance of correctness, but demands substantial human effort in proof construction and hence raises a pressing need for automation. The primary obsta- cle lies in the severe lack of data—there is much... | Chenyuan Yang, Fan Yang, Hao Yu, Lidong Zhou, Md Rakib Hossain Misu, Nan Duan, Peng Cheng, Shan Lu, Shuai Lu, Shuvendu K. Lahiri, Tao Xie, Tianyu Chen, Xuheng Li, Yeyun Gong |  |
| 3229 |  |  [Cut the Crap: An Economical Communication Pipeline for LLM-based Multi-Agent Systems](https://openreview.net/forum?id=LkzuPorQ5L) |  | 0 | Recent advancements in large language model (LLM)-powered agents have shown that collective intelligence can significantly outperform individual capabilities, largely attributed to the meticulously designed inter-agent communication topologies. Though impressive in performance, existing multi-agent... | Dawei Cheng, Guancheng Wan, Guibin Zhang, Jeffrey Xu Yu, Kun Wang, Sukwon Yun, Tianlong Chen, Yanwei Yue, Zhixun Li |  |
| 3230 |  |  [Revealing and Reducing Gender Biases in Vision and Language Assistants (VLAs)](https://openreview.net/forum?id=oStNAMWELS) |  | 0 | Pre-trained large language models (LLMs) have been reliably integrated with visual input for multimodal tasks. The widespread adoption of instruction-tuned image-to-text vision-language assistants (VLAs) like LLaVA and InternVL necessitates evaluating gender biases. We study gender bias in 22... | Leander Girrbach, Stephan Alaniz, Trevor Darrell, Yiran Huang, Zeynep Akata |  |
| 3231 |  |  [Zero-Shot Whole-Body Humanoid Control via Behavioral Foundation Models](https://openreview.net/forum?id=9sOR0nYLtz) |  | 0 | Unsupervised reinforcement learning (RL) aims at pre-training models that can solve a wide range of downstream tasks in complex environments. Despite recent advancements, existing approaches suffer from several limitations: they may require running an RL process on each task to achieve a... | Ahmed Touati, Alessandro Lazaric, Andrea Tirinzoni, Anssi Kanervisto, Jesse Farebrother, Mateusz Guzek, Matteo Pirotta, Yingchen Xu |  |
| 3232 |  |  [Outlier Synthesis via Hamiltonian Monte Carlo for Out-of-Distribution Detection](https://openreview.net/forum?id=N6ba2xsmds) |  | 0 | Out-of-distribution (OOD) detection is crucial for developing trustworthy and reliable machine learning systems. Recent advances in training with auxiliary OOD data demonstrate efficacy in enhancing detection capabilities. Nonetheless, these methods heavily rely on acquiring a large pool of... | Hengzhuang Li, Teng Zhang |  |
| 3233 |  |  [Enhancing Cognition and Explainability of Multimodal Foundation Models with Self-Synthesized Data](https://openreview.net/forum?id=lHbLpwbEyt) |  | 0 | Large Multimodal Models (LMMs), or Vision-Language Models (VLMs), have shown impressive capabilities in a wide range of visual tasks. However, they often struggle with fine-grained visual reasoning, failing to identify domain-specific objectives and provide justifiable explanations for their... | Jin Sun, Ninghao Liu, Quanzheng Li, Xiang Li, Yucheng Shi |  |
| 3234 |  |  [SMI-Editor: Edit-based SMILES Language Model with Fragment-level Supervision](https://openreview.net/forum?id=M29nUGozPa) |  | 0 | SMILES, a crucial textual representation of molecular structures, has garnered significant attention as a foundation for pre-trained language models (LMs). However, most existing pre-trained SMILES LMs focus solely on the single-token level supervision during pre-training, failing to fully leverage... | Bin Feng, Junwei Yang, Kangjie Zheng, Ming Zhang, Siyue Liang, Wei Ju, Zequn Liu, Zhiping Xiao |  |
| 3235 |  |  [MMRole: A Comprehensive Framework for Developing and Evaluating Multimodal Role-Playing Agents](https://openreview.net/forum?id=FGSgsefE0Y) |  | 0 | Recently, Role-Playing Agents (RPAs) have garnered increasing attention for their potential to deliver emotional value and facilitate sociological research. However, existing studies are primarily confined to the textual modality, unable to simulate humans' multimodal perceptual capabilities. To... | Huanran Hu, Lei Wang, Shengjie Jin, Xu Chen, Yanqi Dai, Zhiwu Lu |  |
| 3236 |  |  [REMEDY: Recipe Merging Dynamics in Large Vision-Language Models](https://openreview.net/forum?id=iX7eHHE5Tx) |  | 0 | Model merging has emerged as a powerful technique for combining task-specific vision models into a unified and multi-functional model. Previous methods represented by task arithmetic, have demonstrated effectiveness and scalability in this domain. When large vision-language models (LVLMs) arise... | Chao Wu, Didi Zhu, Jinluan Yang, Min Zhang, Tao Shen, Yibing Song, Ziyu Zhao |  |
| 3237 |  |  [TIPS: Text-Image Pretraining with Spatial awareness](https://openreview.net/forum?id=DaA0wAcTY7) |  | 0 | While image-text representation learning has become very popular in recent years, existing models tend to lack spatial awareness and have limited direct applicability for dense understanding tasks. For this reason, self-supervised image-only pretraining is still the go-to method for many dense... | André Araújo, Arjun Karpur, Bingyi Cao, Dan Gnanapragasam, Daniel Salz, Guangxing Han, Howard Zhou, Jan Dlabal, Kaifeng Chen, KevisKokitsi Maninis, Koert Chen, Mojtaba Seyedhosseini, Soham Ghosh, Ye Xia |  |
| 3238 |  |  [From Promise to Practice: Realizing High-performance Decentralized Training](https://openreview.net/forum?id=lo3nlFHOft) |  | 0 | Decentralized training of deep neural networks has attracted significant attention for its theoretically superior scalability compared to synchronous data-parallel methods like All-Reduce. However, realizing this potential in multi-node training is challenging due to the complex design space that... | Jiaojiao Zhang, Mikael Johansson, Xuyang Wu, Zesen Wang |  |
| 3239 |  |  [mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models](https://openreview.net/forum?id=pr37sbuhVa) |  | 0 | Multi-modal Large Language Models have demonstrated remarkable capabilities in executing instructions for a variety of single-image tasks. Despite this progress, significant challenges remain in modeling long image sequences. In this work, we introduce the versatile multi-modal large language... | Anwen Hu, Fei Huang, Haiyang Xu, Haowei Liu, Ji Zhang, Jiabo Ye, Jingren Zhou, Ming Yan, Qi Qian |  |
| 3240 |  |  [Long-Sequence Recommendation Models Need Decoupled Embeddings](https://openreview.net/forum?id=jkpGIxSsUD) |  | 0 | Lifelong user behavior sequences are crucial for capturing user interests and predicting user responses in modern recommendation systems. A two-stage paradigm is typically adopted to handle these long sequences: a subset of relevant behaviors is first searched from the original long sequences via... | Baixu Chen, Jialong Wu, Jie Jiang, Junwei Pan, Mingsheng Long, Ningya Feng, Qian Li, Xian Hu, Ximei Wang |  |
| 3241 |  |  [MVTokenFlow: High-quality 4D Content Generation using Multiview Token Flow](https://openreview.net/forum?id=zu7cBTPsDb) |  | 0 | In this paper, we present MVTokenFlow for high-quality 4D content creation from monocular videos. Recent advancements in generative models such as video diffusion models and multiview diffusion models enable us to create videos or 3D models. However, extending these generative models for dynamic 4D... | Ge Zheng, Hanzhuo Huang, Jiepeng Wang, Sibei Yang, Yuan Liu, Zhiyang Dou |  |
| 3242 |  |  [DenoiseVAE: Learning Molecule-Adaptive Noise Distributions for Denoising-based 3D Molecular Pre-training](https://openreview.net/forum?id=ym7pr83XQr) |  | 0 | Denoising learning of 3D molecules learns molecular representations by imposing noises into the equilibrium conformation and predicting the added noises to recover the equilibrium conformation, which essentially captures the information of molecular force fields. Due to the specificity of Potential... | Bing Su, Jiahao Chen, Jiangmeng Li, Rui Jiao, Wenbing Huang, Yurou Liu |  |
| 3243 |  |  [Halton Scheduler for Masked Generative Image Transformer](https://openreview.net/forum?id=RDVrlWAb7K) |  | 0 | Masked Generative Image Transformers (MaskGIT) have emerged as a scalable and efficient image generation framework, able to deliver high-quality visuals with low inference costs. However, MaskGIT’s token unmasking scheduler, an essential component of the framework, has not received the attention it... | David Hurych, Eduardo Valle, Matthieu Cord, Mickaël Chen, Victor Besnier |  |
| 3244 |  |  [GS-CPR: Efficient Camera Pose Refinement via 3D Gaussian Splatting](https://openreview.net/forum?id=mP7uV59iJM) |  | 0 | We leverage 3D Gaussian Splatting (3DGS) as a scene representation and propose a novel test-time camera pose refinement (CPR) framework, GS-CPR. This framework enhances the localization accuracy of state-of-the-art absolute pose regression and scene coordinate regression methods. The 3DGS model... | Changkun Liu, Ming Cheng, Shuai Chen, Siyan Hu, Tristan Braud, Victor Adrian Prisacariu, Yash Bhalgat, Zirui Wang |  |
| 3245 |  |  [Black Sheep in the Herd: Playing with Spuriously Correlated Attributes for Vision-Language Recognition](https://openreview.net/forum?id=g1fkhbhHjL) |  | 0 | Few-shot adaptation for Vision-Language Models (VLMs) presents a dilemma: balancing in-distribution accuracy with out-of-distribution generalization. Recent research has utilized low-level concepts such as visual attributes to enhance generalization. However, this study reveals that VLMs overly... | Jing Zhang, Mengqi He, Shu Zou, Xinyu Tian, Zhaoyuan Yang |  |
| 3246 |  |  [Democratic Training Against Universal Adversarial Perturbations](https://openreview.net/forum?id=4M0BRyGMnJ) |  | 0 | Despite their advances and success, real-world deep neural networks are known to be vulnerable to adversarial attacks. Universal adversarial perturbation, an input-agnostic attack, poses a serious threat for them to be deployed in security-sensitive systems. In this case, a single universal... | Bing Sun, Jun Sun, Wei Zhao |  |
| 3247 |  |  [Framer: Interactive Frame Interpolation](https://openreview.net/forum?id=Lp40Z40N07) |  | 0 | We propose Framer for interactive frame interpolation, which targets producing smoothly transitioning frames between two images as per user creativity. Concretely, besides taking the start and end frames as inputs, our approach supports customizing the transition process by tailoring the trajectory... | Biao Gong, Chunhua Shen, Hao Chen, Hao Ouyang, Kecheng Zheng, Qiuyu Wang, Wen Wang, Yujun Shen, Zhekai Chen |  |
| 3248 |  |  [LucidPPN: Unambiguous Prototypical Parts Network for User-centric Interpretable Computer Vision](https://openreview.net/forum?id=BM9qfolt6p) |  | 0 | Prototypical parts networks combine the power of deep learning with the explainability of case-based reasoning to make accurate, interpretable decisions. They follow the this looks like that reasoning, representing each prototypical part with patches from training images. However, a single image... | Bartosz Michal Zielinski, Dawid Damian Rymarczyk, Jacek Tabor, Koryna Lewandowska, Mateusz Pach |  |
| 3249 |  |  [Certifying Language Model Robustness with Fuzzed Randomized Smoothing: An Efficient Defense Against Backdoor Attacks](https://openreview.net/forum?id=USI3ZbuFaV) |  | 0 | The widespread deployment of pre-trained language models (PLMs) has exposed them to textual backdoor attacks, particularly those planted during the pre-training stage. These attacks pose significant risks to high-reliability applications, as they can stealthily affect multiple downstream tasks.... | Bowei He, Chen Ma, HuiLing Zhen, Jianping Zhang, Lanqing Hong, Lihao Yin, Mingxuan Yuan |  |
| 3250 |  |  [Kolmogorov-Arnold Transformer](https://openreview.net/forum?id=BCeock53nt) |  | 0 | Transformers stand as the cornerstone of mordern deep learning. Traditionally, these models rely on multi-layer perceptron (MLP) layers to mix the information between channels. In this paper, we introduce the Kolmogorov–Arnold Transformer (KAT), a novel architecture that replaces MLP layers with... | Xinchao Wang, Xingyi Yang |  |
| 3251 |  |  [BrainUICL: An Unsupervised Individual Continual Learning Framework for EEG Applications](https://openreview.net/forum?id=6jjAYmppGQ) |  | 0 | Electroencephalography (EEG) is a non-invasive brain-computer interface technology used for recording brain electrical activity. It plays an important role in human life and has been widely uesd in real life, including sleep staging, emotion recognition, and motor imagery. However, existing... | Gang Pan, Haiteng Jiang, Jiquan Wang, Sha Zhao, Shijian Li, Tao Li, Yangxuan Zhou |  |
| 3252 |  |  [Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary Resolution](https://openreview.net/forum?id=ODiY6pbHZQ) |  | 0 | Visual data comes in various forms, ranging from small icons of just a few pixels to long videos spanning hours. Existing multi-modal LLMs usually standardize these diverse visual inputs to fixed-resolution images or patches for visual encoders and yield similar numbers of tokens for LLMs. This... | Jiwen Lu, Winston Hu, Yongming Rao, Yuhao Dong, Ziwei Liu, Zuyan Liu |  |
| 3253 |  |  [Is In-Context Learning Sufficient for Instruction Following in LLMs?](https://openreview.net/forum?id=STEEDDv3zI) |  | 0 | In-context learning (ICL) allows LLMs to learn from examples without changing their weights: this is a particularly promising capability for long-context LLMs that can potentially learn from many examples. Recently, Lin et al. (2024) proposed URIAL, a method using only three in-context examples to... | Francesco Croce, Hao Zhao, Maksym Andriushchenko, Nicolas Flammarion |  |
| 3254 |  |  [OVTR: End-to-End Open-Vocabulary Multiple Object Tracking with Transformer](https://openreview.net/forum?id=GDS5eN65QY) |  | 0 | Open-vocabulary multiple object tracking aims to generalize trackers to unseen categories during training, enabling their application across a variety of real-world scenarios. However, the existing open-vocabulary tracker is constrained by its framework structure, isolated frame-level perception,... | En Yu, Jinyang Li, Sijia Chen, Wenbing Tao |  |
| 3255 |  |  [Learning Evolving Tools for Large Language Models](https://openreview.net/forum?id=wtrDLMFU9v) |  | 0 | Tool learning enables large language models (LLMs) to interact with external tools and APIs, greatly expanding the application scope of LLMs. However, due to the dynamic nature of external environments, these tools and APIs may become outdated over time, preventing LLMs from correctly invoking... | Fangda Guo, Guoxin Chen, Wenzheng Feng, Xin Cong, Yankai Lin, Yasheng Wang, Yesai Wu, Zhong Zhang |  |
| 3256 |  |  [Towards Effective Evaluations and Comparisons for LLM Unlearning Methods](https://openreview.net/forum?id=wUtCieKuQU) |  | 0 | The imperative to eliminate undesirable data memorization underscores the significance of machine unlearning for large language models (LLMs). Recent research has introduced a series of promising unlearning methods, notably boosting the practical significance of the field. Nevertheless, adopting a... | Bo Han, Jianing Zhu, Masashi Sugiyama, Puning Yang, Qizhou Wang, Tongliang Liu |  |
| 3257 |  |  [Distilling Reinforcement Learning Algorithms for In-Context Model-Based Planning](https://openreview.net/forum?id=BfUugGfBE5) |  | 0 | Recent studies have shown that Transformers can perform in-context reinforcement learning (RL) by imitating existing RL algorithms, enabling sample-efficient adaptation to unseen tasks without parameter updates. However, these models also inherit the suboptimal behaviors of the RL algorithms they... | Gunhee Kim, Jaehyeon Son, Soochan Lee |  |
| 3258 |  |  [Chain-of-Focus Prompting: Leveraging Sequential Visual Cues to Prompt Large Autoregressive Vision Models](https://openreview.net/forum?id=noidywkBba) |  | 0 | In-context learning (ICL) has revolutionized natural language processing by enabling models to adapt to diverse tasks with only a few illustrative examples. However, the exploration of ICL within the field of computer vision remains limited. Inspired by Chain-of-Thought (CoT) prompting in the... | Dadong Wang, Jialiang Shen, Jiyang Zheng, Min Wang, Tongliang Liu, Yang Yang, Yu Yao |  |
| 3259 |  |  [BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian RL](https://openreview.net/forum?id=UnCKU8pZVe) |  | 0 | Bayesian optimization (BO) offers an efficient pipeline for optimizing black-box functions with the help of a Gaussian process prior and an acquisition function (AF). Recently, in the context of single-objective BO, learning-based AFs witnessed promising empirical results given its favorable... | Cheng Sun, ChienYi Wang, KaiJie Lin, PingChun Hsieh, YuHeng Hung, YuHeng Lin |  |
| 3260 |  |  [Deep Signature: Characterization of Large-Scale Molecular Dynamics](https://openreview.net/forum?id=xayT1nn8Mg) |  | 0 | Understanding protein dynamics are essential for deciphering protein functional mechanisms and developing molecular therapies. However, the complex high-dimensional dynamics and interatomic interactions of biological processes pose significant challenge for existing computational techniques. In... | Chunyang Li, Haoliang Li, Hong Yan, Mengxu Zhu, Terry Lyons, Tiexin Qin |  |
| 3261 |  |  [RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation](https://openreview.net/forum?id=yAzN4tz7oI) |  | 0 | Bimanual manipulation is essential in robotics, yet developing foundation models is extremely challenging due to the inherent complexity of coordinating two robot arms (leading to multi-modal action distributions) and the scarcity of training data. In this paper, we present the Robotics Diffusion... | Bangguo Li, Hang Su, Hengkai Tan, Huayu Chen, Jun Zhu, Ke Xu, Lingxuan Wu, Songming Liu, Zhengyi Wang |  |
| 3262 |  |  [VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation](https://openreview.net/forum?id=02haSpO453) |  | 0 | VILA-U is a Unified foundation model that integrates Video, Image, Language understanding and generation. Traditional visual language models (VLMs) use separate modules for understanding and generating visual content, which can lead to misalignment and increased complexity. In contrast, VILA-U... | Dacheng Li, Enze Xie, Haotian Tang, Hongxu Yin, Junyu Chen, Li Yi, Ligeng Zhu, Song Han, Yao Lu, Yecheng Wu, Yunhao Fang, Zhuoyang Zhang |  |
| 3263 |  |  [Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models](https://openreview.net/forum?id=1EnpStvBU8) |  | 0 | In existing multimodal large language models (MLLMs), image resolution plays a significant role for granular visual recognition. However, directly increasing image resolution leads to expensive computational cost for MLLMs. In this paper, we reveal that a combination of low- and high-resolution... | Gen Luo, Rongrong Ji, Xiaoshuai Sun, Xiawu Zheng, Yiyi Zhou, Yuxin Zhang |  |
| 3264 |  |  [PAL: Sample-Efficient Personalized Reward Modeling for Pluralistic Alignment](https://openreview.net/forum?id=1kFDrYCuSu) |  | 0 | Foundation models trained on internet-scale data benefit from extensive alignment to human preferences before deployment. However, existing methods typically assume a homogeneous preference shared by all individuals, overlooking the diversity inherent in human values. In this work, we propose a... | Aniket Rege, Daiwei Chen, Ramya Korlakai Vinayak, Yi Chen, Zhi Wang |  |
| 3265 |  |  [Dynamic Diffusion Transformer](https://openreview.net/forum?id=taHwqSrbrb) |  | 0 | Diffusion Transformer (DiT), an emerging diffusion model for image generation, has demonstrated superior performance but suffers from substantial computational costs. Our investigations reveal that these costs stem from the static inference paradigm, which inevitably introduces redundant... | Fan Wang, Gao Huang, Jiasheng Tang, Kai Wang, Wangbo Zhao, Yang You, Yibing Song, Yizeng Han |  |
| 3266 |  |  [GS-LiDAR: Generating Realistic LiDAR Point Clouds with Panoramic Gaussian Splatting](https://openreview.net/forum?id=RMaRBE9s2H) |  | 0 | LiDAR novel view synthesis (NVS) has emerged as a novel task within LiDAR simulation, offering valuable simulated point cloud data from novel viewpoints to aid in autonomous driving systems. However, existing LiDAR NVS methods typically rely on neural radiance fields (NeRF) as their 3D... | Chun Gu, Junzhe Jiang, Li Zhang, Yurui Chen |  |
| 3267 |  |  [LongVILA: Scaling Long-Context Visual Language Models for Long Videos](https://openreview.net/forum?id=wCXAlfvCy6) |  | 0 | Long-context capability is critical for multi-modal foundation models, especially for long video understanding. We introduce LongVILA, a full-stack solution for long-context visual-language models by co-designing the algorithm and system. For model training, we upgrade existing VLMs to support long... | Dacheng Li, Fuzhao Xue, Haotian Tang, Hongxu Yin, Jan Kautz, Ligeng Zhu, Linxi Fan, Pavlo Molchanov, Qinghao Hu, Shang Yang, Song Han, Xiuyu Li, Yao Lu, Yihui He, Yukang Chen, Yuke Zhu, Yunhao Fang, Zhijian Liu |  |
| 3268 |  |  [McEval: Massively Multilingual Code Evaluation](https://openreview.net/forum?id=UunCPtPOlZ) |  | 0 | Code large language models (LLMs) have shown remarkable advances in code understanding, completion, and generation tasks. Programming benchmarks, comprised of a selection of code challenges and corresponding test cases, serve as a standard to evaluate the capability of different LLMs in such tasks.... | Bing Wang, Boyang Wang, Changyu Ren, Ge Zhang, Hongcheng Guo, Jiaheng Liu, Jian Yang, Ke Jin, Linzheng Chai, Liqun Yang, Noah Wang, Shukai Liu, Sufeng Duan, Tao Sun, Tongliang Li, Xianjie Wu, Yuwei Yin, Zhaoxiang Zhang, Zhoujun Li |  |
| 3269 |  |  [Re-Evaluating the Impact of Unseen-Class Unlabeled Data on Semi-Supervised Learning Model](https://openreview.net/forum?id=WPsnH6875d) |  | 0 | Semi-supervised learning (SSL) effectively leverages unlabeled data and has been proven successful across various fields. Current safe SSL methods believe that unseen classes in unlabeled data harm the performance of SSL models. However, previous methods for assessing the impact of unseen classes... | Lanzhe Guo, Rundong He, Tailin Wu, Yicong Dong, Yilong Yin |  |
| 3270 |  |  [Cross-Attention Head Position Patterns Can Align with Human Visual Concepts in Text-to-Image Generative Models](https://openreview.net/forum?id=1vggIT5vvj) |  | 0 | Recent text-to-image diffusion models leverage cross-attention layers, which have been effectively utilized to enhance a range of visual generative tasks. However, our understanding of cross-attention layers remains somewhat limited. In this study, we introduce a mechanistic interpretability... | Dongnam Byun, Jangwon Suh, Jungmin Ko, Jungwon Park, Wonjong Rhee |  |
| 3271 |  |  [FormalAlign: Automated Alignment Evaluation for Autoformalization](https://openreview.net/forum?id=B5RrIFMqbe) |  | 0 | Autoformalization aims to convert informal mathematical proofs into machine-verifiable formats, bridging the gap between natural and formal languages. However, ensuring semantic alignment between the informal and formalized statements remains challenging. Existing approaches heavily rely on manual... | Jianqiao Lu, Jing Xiong, Yingjia Wan, Yinya Huang, Zhengying Liu, Zhijiang Guo |  |
| 3272 |  |  [SEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection](https://openreview.net/forum?id=VHguhvcoM5) |  | 0 | Fine-tuning on task-specific data to boost downstream performance is a crucial step for leveraging Large Language Models (LLMs). However, though fine-tuning enhances the model performance for specialized applications, previous studies have demonstrated that fine-tuning the models on several... | Han Shen, Payel Das, PinYu Chen, Tianyi Chen |  |
| 3273 |  |  [SeRA: Self-Reviewing and Alignment of LLMs using Implicit Reward Margins](https://openreview.net/forum?id=uIGnuyDSB9) |  | 0 | Direct alignment algorithms (DAAs), such as direct preference optimization (DPO), have become popular alternatives to Reinforcement Learning from Human Feedback (RLHF) due to their simplicity, efficiency, and stability. However, the preferences used by DAAs are usually collected before alignment... | Aram Galstyan, Bhavana Ganesh, Jongwoo Ko, Sailik Sengupta, Saket Dingliwal, Sravan Babu Bodapati |  |
| 3274 |  |  [Bidirectional Decoding: Improving Action Chunking via Guided Test-Time Sampling](https://openreview.net/forum?id=qZmn2hkuzw) |  | 0 | Predicting and executing a sequence of actions without intermediate replanning, known as action chunking, is increasingly used in robot learning from human demonstrations. Yet, its effects on the learned policy remain inconsistent: some studies find it crucial for achieving strong results, while... | Annie Xie, Chelsea Finn, Jubayer Ibn Hamid, Max Du, Yoonho Lee, Yuejiang Liu |  |
| 3275 |  |  [6DGS: Enhanced Direction-Aware Gaussian Splatting for Volumetric Rendering](https://openreview.net/forum?id=sUvBTEYXGt) |  | 0 | Novel view synthesis has advanced significantly with the development of neural radiance fields (NeRF) and 3D Gaussian splatting (3DGS). However, achieving high quality without compromising real-time rendering remains challenging, particularly for physically-based rendering using ray/path tracing... | Anwesa Choudhuri, Benjamin Planche, Meng Zheng, Terrence Chen, Zhongpai Gao, Ziyan Wu |  |
| 3276 |  |  [Credit-based self organizing maps: training deep topographic networks with minimal performance degradation](https://openreview.net/forum?id=wMgr7wBuUo) |  | 0 | In the primate neocortex, neurons with similar function are often found to be spatially close. Kohonen's self-organizing map (SOM) has been one of the most influential approaches for simulating brain-like topographical organization in artificial neural network models. However, integrating these... | Amirozhan Dehghani, Asa Farahani, Pouya Bashivan, Xinyu Qian |  |
| 3277 |  |  [An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels](https://openreview.net/forum?id=tjNf0L8QjR) |  | 0 | This work does not introduce a new method. Instead, we present an interesting finding that questions the necessity of the inductive bias of locality in modern computer vision architectures. Concretely, we find that vanilla Transformers can operate by directly treating each individual pixel as a... | Cees G. M. Snoek, DuyKien Nguyen, Martin R. Oswald, Mido Assran, Unnat Jain, Xinlei Chen |  |
| 3278 |  |  [Model-Free Offline Reinforcement Learning with Enhanced Robustness](https://openreview.net/forum?id=QyVLJ7EnAC) |  | 0 | Offline reinforcement learning (RL) has gained considerable attention for its ability to learn policies from pre-collected data without real-time interaction, which makes it particularly useful for high-risk applications. However, due to its reliance on offline datasets, existing works inevitably... | Chi Zhang, George K. Atia, Yue Wang, Zain Ulabedeen Farhat |  |
| 3279 |  |  [Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want](https://openreview.net/forum?id=bfa58H1nQ8) |  | 0 | In this paper, we present the Draw-and-Understand framework, exploring how to integrate visual prompting understanding capabilities into Multimodal Large Language Models (MLLMs). Visual prompts allow users to interact through multi-modal instructions, enhancing the models' interactivity and... | Bocheng Zou, Hongsheng Li, Peng Gao, Ruichuan An, Shanghang Zhang, Siyuan Huang, Weifeng Lin, Xinyu Wei, Yulin Luo |  |
| 3280 |  |  [Let Your Features Tell The Differences: Understanding Graph Convolution By Feature Splitting](https://openreview.net/forum?id=I9omfcWfMp) |  | 0 | Graph Neural Networks (GNNs) have demonstrated strong capabilities in processing structured data. While traditional GNNs typically treat each feature dimension equally important during graph convolution, we raise an important question: \*\*Is the graph convolution operation equally beneficial for... | Lihui Chen, Sitao Luan, Xiang Li, Xiaojiang Peng, Yilun Zheng |  |
| 3281 |  |  [Learning to Generate Diverse Pedestrian Movements from Web Videos with Noisy Labels](https://openreview.net/forum?id=DydCqKa6AH) |  | 0 | Understanding and modeling pedestrian movements in the real world is crucial for applications like motion forecasting and scene simulation. Many factors influence pedestrian movements, such as scene context, individual characteristics, and goals, which are often ignored by the existing human... | Bolei Zhou, Joe Lin, Wayne Wu, Zhizheng Liu |  |
| 3282 |  |  [MIRAGE: Evaluating and Explaining Inductive Reasoning Process in Language Models](https://openreview.net/forum?id=tZCqSVncRf) |  | 0 | Inductive reasoning is an essential capability for large language models (LLMs) to achieve higher intelligence, which requires the model to generalize rules from observed facts and then apply them to unseen examples. We present {\scshape Mirage}, a synthetic dataset that addresses the limitations... | Jiachun Li, Jun Zhao, Kang Liu, Pengfei Cao, Yubo Chen, Zhuoran Jin |  |
| 3283 |  |  [One for all and all for one: Efficient computation of partial Wasserstein distances on the line](https://openreview.net/forum?id=kzEPsHbJDv) |  | 0 | Partial Wasserstein helps overcoming some of the limitations of Optimal Transport when the distributions at stake differ in mass, contain noise or outliers or exhibit mass mismatches across distribution modes. We introduce PAWL, a novel algorithm designed to efficiently compute exact PArtial... | Laetitia Chapel, Romain Tavenard |  |
| 3284 |  |  [Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse Representation Adjustment in Large Language Models](https://openreview.net/forum?id=s20W12XTF8) |  | 0 | As large language models (LLMs) become integral to various applications, ensuring both their safety and utility is paramount. Jailbreak attacks, which manipulate LLMs into generating harmful content, pose significant challenges to this balance. Existing defenses, such as prompt engineering and... | Dongcheng Zhao, Guobin Shen, Xiang He, Yi Zeng, Yiting Dong |  |
| 3285 |  |  [Precise Parameter Localization for Textual Generation in Diffusion Models](https://openreview.net/forum?id=gdHtZlaaSo) |  | 0 | Novel diffusion models can synthesize photo-realistic images with integrated high-quality text. Surprisingly, we demonstrate through attention activation patching that only less than $1$\% of diffusion models' parameters, all contained in attention layers, influence the generation of textual... | Adam Dziedzic, Bartosz Cywinski, Franziska Boenisch, Kamil Deja, Lukasz Staniszewski |  |
| 3286 |  |  [Lightning-Fast Image Inversion and Editing for Text-to-Image Diffusion Models](https://openreview.net/forum?id=t9l63huPRt) |  | 0 | Diffusion inversion is the problem of taking an image and a text prompt that describes it and finding a noise latent that would generate the exact same image. Most current deterministic inversion techniques operate by approximately solving an implicit equation and may converge slowly or yield poor... | Barak Meiri, Dvir Samuel, Gal Chechik, Haggai Maron, Nir Darshan, Rami BenAri, Shai Avidan, Yoad Tewel |  |
| 3287 |  |  [Revisiting Convolution Architecture in the Realm of DNA Foundation Models](https://openreview.net/forum?id=B07dLVWLyD) |  | 0 | In recent years, A variety of methods based on Transformer and state space model (SSM) architectures have been proposed, advancing foundational DNA language models. However, there is a lack of comparison between these recent approaches and the classical architecture—convolutional networks (CNNs)—on... | Chunhua Shen, Hao Chen, Junbo Zhao, Peng Ye, Weian Mao, Weiqiang Bai, Xinzhu Ma, Yanjun Shao, Yu Bo |  |
| 3288 |  |  [Kronecker Mask and Interpretive Prompts are Language-Action Video Learners](https://openreview.net/forum?id=RUF7j1cJzK) |  | 0 | Contrastive language-image pretraining (CLIP) has significantly advanced image-based vision learning. A pressing topic subsequently arises: how can we effectively adapt CLIP to the video domain? Recent studies have focused on adjusting either the textual or visual branch of CLIP for action... | Hui Li, Jia He, Jingyi Yang, Xiuming Ni, Zitong Yu |  |
| 3289 |  |  [MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?](https://openreview.net/forum?id=k5VHHgsRbi) |  | 0 | Comprehensive evaluation of Multimodal Large Language Models (MLLMs) has recently garnered widespread attention in the research community. However, we observe that existing benchmarks present several common barriers that make it difficult to measure the significant challenges that models face in... | Chaoyou Fu, Feng Li, Haochen Tian, Huanyu Zhang, Junfei Wu, Kun Wang, Liang Wang, Qingsong Wen, Rong Jin, Shuangqing Zhang, Yifan Zhang, Zhang Zhang |  |
| 3290 |  |  [State Space Model Meets Transformer: A New Paradigm for 3D Object Detection](https://openreview.net/forum?id=Tisu1L0Jwt) |  | 0 | DETR-based methods, which use multi-layer transformer decoders to refine object queries iteratively, have shown promising performance in 3D indoor object detection. However, the scene point features in the transformer decoder remain fixed, leading to minimal contributions from later decoder layers,... | Chuxin Wang, Tianzhu Zhang, Wenfei Yang, Xiang Liu |  |
| 3291 |  |  [What to align in multimodal contrastive learning?](https://openreview.net/forum?id=Pe3AxLq6Wf) |  | 0 | Humans perceive the world through multisensory integration, blending the information of different modalities to adapt their behavior. Contrastive learning offers an appealing solution for multimodal self-supervised learning. Indeed, by considering each modality as a different view of the same... | Benoit Dufumier, Devis Tuia, Javiera Castillo Navarro, JeanPhilippe Thiran |  |
| 3292 |  |  [Smoothing the Shift: Towards Stable Test-Time Adaptation under Complex Multimodal Noises](https://openreview.net/forum?id=rObkvzJxTG) |  | 0 | Test-Time Adaptation (TTA) aims to tackle distribution shifts using unlabeled test data without access to the source data. In the context of multimodal data, there are more complex noise patterns than unimodal data such as simultaneous corruptions for multiple modalities and missing modalities.... | Tao Jin, Zirun Guo |  |
| 3293 |  |  [Improving Language Model Distillation through Hidden State Matching](https://openreview.net/forum?id=IcVSKhVpKu) |  | 0 | Hidden State Matching is shown to improve knowledge distillation of language models by encouraging similarity between a student and its teacher's hidden states, as demonstrated by DistilBERT and its successors. This typically uses a cosine loss, which restricts the dimensionality of the student to... | Sayantan Dasgupta, Trevor Cohn |  |
| 3294 |  |  [Long-Context Linear System Identification](https://openreview.net/forum?id=2TuUXtLGhT) |  | 0 | This paper addresses the problem of long-context linear system identification, where the state $x_t$ of the system at time $t$ depends linearly on previous states $x_s$ over a fixed context window of length $p$. We establish a sample complexity bound that matches the _i.i.d._ parametric rate, up to... | Mathieu Even, Nicolas Flammarion, Oguz Kaan Yüksel |  |
| 3295 |  |  [Controllable Unlearning for Image-to-Image Generative Models via ϵ-Constrained Optimization](https://openreview.net/forum?id=9OJflnNu6C) |  | 0 | While generative models have made significant advancements in recent years, they also raise concerns such as privacy breaches and biases. Machine unlearning has emerged as a viable solution, aiming to remove specific training data, e.g., containing private information and bias, from models. In this... | Chaochao Chen, Jun Zhou, Li Zhang, Longfei Li, Xiaohua Feng, Xiaolin Zheng, Yuyuan Li |  |
| 3296 |  |  [Random Is All You Need: Random Noise Injection on Feature Statistics for Generalizable Deep Image Denoising](https://openreview.net/forum?id=z8PcUSKXXN) |  | 0 | Recent advancements in generalizable deep image denoising have catalyzed the development of robust noise-handling models. The current state-of-the-art, Masked Training (MT), constructs a masked swinir model which is trained exclusively on Gaussian noise ($\sigma$=15) but can achieve commendable... | Guixu Lin, Hongjun Wang, Weihang Ran, Yinqiang Zheng, Zhengwei Yin |  |
| 3297 |  |  [Navigating Neural Space: Revisiting Concept Activation Vectors to Overcome Directional Divergence](https://openreview.net/forum?id=Q95MaWfF4e) |  | 0 | With a growing interest in understanding neural network prediction strategies, Concept Activation Vectors (CAVs) have emerged as a popular tool for modeling human-understandable concepts in the latent space. Commonly, CAVs are computed by leveraging linear classifiers optimizing the... | Christopher J. Anders, Frederik Pahde, Leander Weber, Maximilian Dreyer, Moritz Weckbecker, Sebastian Lapuschkin, Thomas Wiegand, Wojciech Samek |  |
| 3298 |  |  [LLaVA-MoD: Making LLaVA Tiny via MoE-Knowledge Distillation](https://openreview.net/forum?id=uWtLOy35WD) |  | 0 | We introduce LLaVA-MoD, a novel framework designed to enable the efficient training of small-scale Multimodal Language Models ($s$-MLLM) distilling knowledge from large-scale MLLM ($l$-MLLM). Our approach tackles two fundamental challenges in MLLM distillation. First, we optimize the network... | Chenning Xu, Fangxun Shu, Guanghao Zhang, Hao Jiang, Haonan Shi, Haoyuan Li, Hongsheng Li, Le Zhuo, Lei Zhang, Long Chan, Si Liu, Siming Fu, Tao Zhong, Wanggui He, Yue Liao, Zhelun Yu |  |
| 3299 |  |  [I2VControl-Camera: Precise Video Camera Control with Adjustable Motion Strength](https://openreview.net/forum?id=AcAD4VEgCX) |  | 0 | Video generation technologies are developing rapidly and have broad potential applications. Among these technologies, camera control is crucial for generating professional-quality videos that accurately meet user expectations. However, existing camera control methods still suffer from several... | Jiawei Liu, Mingzhen Sun, Pengqi Tu, Qian He, SiYu Zhou, Songtao Zhao, Tianhao Qi, Tianxiang Ma, Wanquan Feng |  |
| 3300 |  |  [BinaryDM: Accurate Weight Binarization for Efficient Diffusion Models](https://openreview.net/forum?id=YaeZwhXJ4k) |  | 0 | With the advancement of diffusion models (DMs) and the substantially increased computational requirements, quantization emerges as a practical solution to obtain compact and efficient low-bit DMs. However, the highly discrete representation leads to severe accuracy degradation, hindering the... | Haojie Hao, Haotong Qin, Jiakai Wang, Jinyang Guo, Michele Magno, Mingyuan Zhang, Xianglong Liu, Xingyu Zheng, Xudong Ma, Zixiang Zhao |  |
| 3301 |  |  [LongWriter: Unleashing 10, 000+ Word Generation from Long Context LLMs](https://openreview.net/forum?id=kQ5s9Yh0WI) |  | 0 | Current long context large language models (LLMs) can process inputs up to 100,000 tokens, yet struggle to generate outputs exceeding even a modest length of 2,000 words. Through controlled experiments, we find that the model's effective generation length is inherently bounded by the sample it has... | Jiajie Zhang, Jie Tang, Juanzi Li, Lei Hou, Linzhi Zheng, Siqi Zhu, Xin Lv, Yushi Bai, Yuxiao Dong |  |
| 3302 |  |  [GROOT-2: Weakly Supervised Multimodal Instruction Following Agents](https://openreview.net/forum?id=S9GyQUXzee) |  | 0 | Developing agents that can follow multimodal instructions remains a fundamental challenge in robotics and AI. Although large-scale pre-training on unlabeled datasets has enabled agents to learn diverse behaviors, these agents often struggle with following instructions. While augmenting the dataset... | Anji Liu, Bowei Zhang, Haowei Lin, Shaofei Cai, Xiaojian Ma, Yitao Liang, Zihao Wang |  |
| 3303 |  |  [Medium-Difficulty Samples Constitute Smoothed Decision Boundary for Knowledge Distillation on Pruned Datasets](https://openreview.net/forum?id=Rz4UkJziFe) |  | 0 | This paper tackles a new problem of dataset pruning for Knowledge Distillation (KD), from a fresh perspective of Decision Boundary (DB) preservation and drifts. Existing dataset pruning methods generally assume that the post-pruning DB formed by the selected samples can be well-captured by future... | Frank de Hoog, Jiajun Liu, Sen Wang, Xuwei Xu, Yudong Chen |  |
| 3304 |  |  [Mixture Compressor for Mixture-of-Experts LLMs Gains More](https://openreview.net/forum?id=hheFYjOsWO) |  | 0 | Mixture-of-Experts large language models (MoE-LLMs) marks a significant step forward of language models, however, they encounter two critical challenges in practice: 1) expert parameters lead to considerable memory consumption and loading latency; and 2) the current activated experts are redundant,... | Haoru Tan, Hongsheng Li, Jianhui Liu, Ruifei He, Shiming Zhang, Si Liu, Wei Huang, Xiaojuan Qi, Yue Liao |  |
| 3305 |  |  [Methods for Convex (L0, L1)-Smooth Optimization: Clipping, Acceleration, and Adaptivity](https://openreview.net/forum?id=0wmfzWPAFu) |  | 0 | Due to the non-smoothness of optimization problems in Machine Learning, generalized smoothness assumptions have been gaining a lot of attention in recent years. One of the most popular assumptions of this type is $(L_0,L_1)$-smoothness (Zhang et al., 2020). In this paper, we focus on the class of... | Alen Aliev, Eduard Gorbunov, Martin Takác, Nazarii Tupitsa, Peter Richtárik, Samuel Horváth, Sayantan Choudhury |  |
| 3306 |  |  [Minimax Optimal Reinforcement Learning with Quasi-Optimism](https://openreview.net/forum?id=i8LCUpKvAz) |  | 0 | In our quest for a reinforcement learning (RL) algorithm that is both practical and provably optimal, we introduce EQO (Exploration via Quasi-Optimism). Unlike existing minimax optimal approaches, EQO avoids reliance on empirical variances and employs a simple bonus term proportional to the inverse... | Harin Lee, Minhwan Oh |  |
| 3307 |  |  [Weakly-Supervised Affordance Grounding Guided by Part-Level Semantic Priors](https://openreview.net/forum?id=0823rvTIhs) |  | 0 | In this work, we focus on the task of weakly supervised affordance grounding, where a model is trained to identify affordance regions on objects using human-object interaction images and egocentric object images without dense labels. Previous works are mostly built upon class activation maps, which... | Peiran Xu, Yadong Mu |  |
| 3308 |  |  [Safety Layers in Aligned Large Language Models: The Key to LLM Security](https://openreview.net/forum?id=kUH1yPMAn7) |  | 0 | Aligned LLMs are secure, capable of recognizing and refusing to answer malicious questions. However, the role of internal parameters in maintaining such security is not well understood yet, further these models can be vulnerable to security degradation when subjected to fine-tuning attacks. To... | Lan Zhang, Liuyi Yao, Shen Li, Yaliang Li |  |
| 3309 |  |  [Episodic Novelty Through Temporal Distance](https://openreview.net/forum?id=I7DeajDEx7) |  | 0 | Exploration in sparse reward environments remains a significant challenge in reinforcement learning, particularly in Contextual Markov Decision Processes (CMDPs), where environments differ across episodes. Existing episodic intrinsic motivation methods for CMDPs primarily rely on count-based... | Bin Liang, Bo Xu, Chongjie Zhang, Dianyu Zhong, Hao Hu, Jun Yang, Qianchuan Zhao, Qihan Liu, Xiaoteng Ma, Yiqin Yang, Yuhua Jiang |  |
| 3310 |  |  [ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation](https://openreview.net/forum?id=E1N1oxd63b) |  | 0 | Diffusion transformers have demonstrated remarkable performance in visual generation tasks, such as generating realistic images or videos based on textual instructions. However, larger model sizes and multi-frame processing for video generation lead to increased computational and memory costs,... | Enshu Liu, Guohao Dai, Haofeng Huang, Huazhong Yang, Rui Wan, Shengen Yan, Shiyao Li, Tianchen Zhao, Tongcheng Fang, Widyadewi Soedarmadji, Xuefei Ning, Yu Wang, Zinan Lin |  |
| 3311 |  |  [Do You Keep an Eye on What I Ask? Mitigating Multimodal Hallucination via Attention-Guided Ensemble Decoding](https://openreview.net/forum?id=ziw5bzg2NO) |  | 0 | Recent advancements in Large Vision-Language Models (LVLMs) have significantly expanded their utility in tasks like image captioning and visual question answering. However, they still struggle with object hallucination, where models generate descriptions that inaccurately reflect the visual content... | Keonwoo Kim, Sungzoon Cho, Taebaek Hwang, Yeongjae Cho |  |
| 3312 |  |  [Masked Temporal Interpolation Diffusion for Procedure Planning in Instructional Videos](https://openreview.net/forum?id=HnpDHiItd2) |  | 0 | In this paper, we address the challenge of procedure planning in instructional videos, aiming to generate coherent and task-aligned action sequences from start and end visual observations. Previous work has mainly relied on text-level supervision to bridge the gap between observed states and... | Beichen Zhang, Junqi Jing, Lingshuai Lin, Shuhui Wang, Tingting Chai, Weigang Zhang, Yufan Zhou, Zhaobo Qi |  |
| 3313 |  |  [BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against Jailbreak Attacks](https://openreview.net/forum?id=wwVGZRnAYG) |  | 0 | In this paper, we focus on black-box defense for VLMs against jailbreak attacks. Existing black-box defense methods are either unimodal or bimodal. Unimodal methods enhance either the vision or language module of the VLM, while bimodal methods robustify the model through text-image representation... | Lin Luo, Xiang Zheng, Xingjun Ma, Yige Li, YuGang Jiang, Yunhan Zhao |  |
| 3314 |  |  [Active Learning for Continual Learning: Keeping the Past Alive in the Present](https://openreview.net/forum?id=mnLmmtW7HO) |  | 0 | \*Continual learning (CL)\* enables deep neural networks to adapt to ever-changing data distributions. In practice, there may be scenarios where annotation is costly, leading to \*active continual learning (ACL)\*, which performs \*active learning (AL)\* for the CL scenarios when reducing the... | Dongmin Park, JaeGil Lee, Jaehyun Park |  |
| 3315 |  |  [Group Downsampling with Equivariant Anti-aliasing](https://openreview.net/forum?id=sOte83GogU) |  | 0 | Downsampling layers are crucial building blocks in CNN architectures, which help to increase the receptive field for learning high-level features and reduce the amount of memory/computation in the model. In this work, we study the generalization of the uniform downsampling layer for group... | Md Ashiqur Rahman, Raymond A. Yeh |  |
| 3316 |  |  [Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models](https://openreview.net/forum?id=mQ55y4s5hj) |  | 0 | Text-to-image generative models like DALL-E and Stable Diffusion have revolutionized visual content creation across various applications, including advertising, personalized media, and design prototyping. However, crafting effective textual prompts to guide these models remains challenging, often... | Byonghyo Shim, Donghoon Kim, Kyuhong Shim, Minji Bae |  |
| 3317 |  |  [BP-Modified Local Loss for Efficient Training of Deep Neural Networks](https://openreview.net/forum?id=MtW30ql5Oj) |  | 0 | The training of large models is memory-constrained, one direction to relieve this is training using local loss, like GIM, LoCo, and Forward-Forward algorithms. However, the local loss methods often face the issue of slow or non-convergence. In this paper, we propose a novel BP-modified local loss... | Lianhai Ren, Qianxiao Li |  |
| 3318 |  |  [The Crucial Role of Samplers in Online Direct Preference Optimization](https://openreview.net/forum?id=F6z3utfcYw) |  | 0 | Direct Preference Optimization (DPO) has emerged as a stable, scalable, and efficient solution for language model alignment. Despite its empirical success, the optimization properties, particularly the impact of samplers on its convergence rates, remain under-explored. In this paper, we provide a... | Ruizhe Shi, Runlong Zhou, Simon Shaolei Du |  |
| 3319 |  |  [Near-Optimal Policy Identification in Robust Constrained Markov Decision Processes via Epigraph Form](https://openreview.net/forum?id=G5sPv4KSjR) |  | 0 | Designing a safe policy for uncertain environments is crucial in real-world control systems. However, this challenge remains inadequately addressed within the Markov decision process (MDP) framework. This paper presents the first algorithm guaranteed to identify a near-optimal policy in a robust... | Kazumi Kasaura, Kenta Hoshino, Masashi Hamaya, Paavo Parmas, Tadashi Kozuno, Toshinori Kitamura, Wataru Kumagai, Yohei Hosoe, Yutaka Matsuo |  |
| 3320 |  |  [Scaling Autonomous Agents via Automatic Reward Modeling And Planning](https://openreview.net/forum?id=womU9cEwcO) |  | 0 | Large language models (LLMs) have demonstrated remarkable capabilities across a range of text-generation tasks. However, LLMs still struggle with problems requiring multi-step decision-making and environmental feedback, such as online shopping, scientific reasoning, and mathematical... | Chuang Gan, Delin Chen, Rui Sun, Wenjun Liu, Zhenfang Chen |  |
| 3321 |  |  [Generalized Video Moment Retrieval](https://openreview.net/forum?id=qdOIkeZ5e4) |  | 0 | In this paper, we introduce the Generalized Video Moment Retrieval (GVMR) framework, which extends traditional Video Moment Retrieval (VMR) to handle a wider range of query types. Unlike conventional VMR systems, which are often limited to simple, single-target queries, GVMR accommodates both... | Li Li, Lina Wei, Pengcheng Cai, Qilong Wu, Roger Zimmermann, Wei Ji, Yicong Li, You Qin |  |
| 3322 |  |  [A Training-Free Sub-quadratic Cost Transformer Model Serving Framework with Hierarchically Pruned Attention](https://openreview.net/forum?id=PTcMzQgKmn) |  | 0 | In modern large language models (LLMs), increasing the context length is crucial for improving comprehension and coherence in long-context, multi-modal, and retrieval-augmented language generation. While many recent transformer models attempt to extend their context length over a million tokens,... | Bumsik Kim, Geon Park, Heejun Lee, Hyemin Lee, Jaduk Suh, Jina Kim, Myeongjae Jeon, Sung Ju Hwang, Wonyong Jeong, Youngwan Lee |  |
| 3323 |  |  [GameGen-X: Interactive Open-world Game Video Generation](https://openreview.net/forum?id=8VG8tpPZhe) |  | 0 | We introduce GameGen-$\mathbb{X}$, the first diffusion transformer model specifically designed for both generating and interactively controlling open-world game videos. This model facilitates high-quality, open-domain generation by approximating various game elements, such as innovative characters,... | Cheng Jin, Hao Chen, Haoxuan Che, Quande Liu, Xuanhua He |  |
| 3324 |  |  [StochSync: Stochastic Diffusion Synchronization for Image Generation in Arbitrary Spaces](https://openreview.net/forum?id=XPNprvlxuQ) |  | 0 | We propose a zero-shot method for generating images in arbitrary spaces (e.g., a sphere for 360◦ panoramas and a mesh surface for texture) using a pretrained image diffusion model. The zero-shot generation of various visual content using a pretrained image diffusion model has been explored mainly... | Jaihoon Kim, Kyeongmin Yeo, Minhyuk Sung |  |
| 3325 |  |  [FlickerFusion: Intra-trajectory Domain Generalizing Multi-agent Reinforcement Learning](https://openreview.net/forum?id=MRYyOaNxh3) |  | 0 | Multi-agent reinforcement learning has demonstrated significant potential in addressing complex cooperative tasks across various real-world applications. However, existing MARL approaches often rely on the restrictive assumption that the number of entities (e.g., agents, obstacles) remains constant... | Hyeongjin Kim, Jaein Jang, Junghyun Lee, SeYoung Yun, Siyeol Kim, Suhin Shin, Wonbeen Oh, Woosung Koh |  |
| 3326 |  |  [DiTTo-TTS: Diffusion Transformers for Scalable Text-to-Speech without Domain-Specific Factors](https://openreview.net/forum?id=hQvX9MBowC) |  | 0 | Large-scale latent diffusion models (LDMs) excel in content generation across various modalities, but their reliance on phonemes and durations in text-to-speech (TTS) limits scalability and access from other fields. While recent studies show potential in removing these domain-specific factors,... | Dong Won Kim, Jaehyeon Kim, Jaewoong Cho, Keon Lee, Seungjun Chung |  |
| 3327 |  |  [Intelligent Go-Explore: Standing on the Shoulders of Giant Foundation Models](https://openreview.net/forum?id=apErWGzCAA) |  | 0 | Go-Explore is a powerful family of algorithms designed to solve hard-exploration problems built on the principle of archiving discovered states, and iteratively returning to and exploring from the most promising states. This approach has led to superhuman performance across a wide variety of... | Cong Lu, Jeff Clune, Shengran Hu |  |
| 3328 |  |  [Online Reward-Weighted Fine-Tuning of Flow Matching with Wasserstein Regularization](https://openreview.net/forum?id=2IoFFexvuw) |  | 0 | Recent advancements in reinforcement learning (RL) have achieved great success in fine-tuning diffusion-based generative models. However, fine-tuning continuous flow-based generative models to align with arbitrary user-defined reward functions remains challenging, particularly due to issues such as... | Chaoran Cheng, Chumeng Liang, Ge Liu, Jiajun Fan, Shuaike Shen, Yuxin Chen |  |
| 3329 |  |  [GenEx: Generating an Explorable World](https://openreview.net/forum?id=8NlUL0Cv1L) |  | 0 | Understanding, navigating, and exploring the 3D physical real world has long been a central challenge in the development of artificial intelligence. In this work, we take a step toward this goal by introducing \*GenEx\*, a system capable of planning complex embodied world exploration, guided by its... | Alan L. Yuille, Daniel Khashabi, Jieneng Chen, Taiming Lu, Tianmin Shu |  |
| 3330 |  |  [Gradient-Free Generation for Hard-Constrained Systems](https://openreview.net/forum?id=teE4pl9ftK) |  | 0 | Generative models that satisfy hard constraints are critical in many scientific and engineering applications, where physical laws or system requirements must be strictly respected. Many existing constrained generative models, especially those developed for computer vision, rely heavily on gradient... | Abdul Fatir Ansari, Andrew Stuart, Bernie Wang, Boran Han, Chaoran Cheng, Danielle C. Maddix, Michael W. Mahoney |  |
| 3331 |  |  [UniDrive: Towards Universal Driving Perception Across Camera Configurations](https://openreview.net/forum?id=jVDPq9EdzT) |  | 0 | Vision-centric autonomous driving has demonstrated excellent performance with economical sensors. As the fundamental step, 3D perception aims to infer 3D information from 2D images based on 3D-2D projection. This makes driving perception models susceptible to sensor configuration (e.g., camera... | Kurt Keutzer, Wenzhao Zheng, Xiaonan Huang, Ye Li |  |
| 3332 |  |  [ImageFolder: Autoregressive Image Generation with Folded Tokens](https://openreview.net/forum?id=QE1LFzXQPL) |  | 0 | Image tokenizers are crucial for visual generative models, \eg, diffusion models (DMs) and autoregressive (AR) models, as they construct the latent representation for modeling. Increasing token length is a common approach to improve image reconstruction quality. However, tokenizers with longer... | Bhiksha Raj, Hao Chen, Jason Kuen, Jiuxiang Gu, Kai Qiu, Xiang Li, Zhe Lin |  |
| 3333 |  |  [InstaSHAP: Interpretable Additive Models Explain Shapley Values Instantly](https://openreview.net/forum?id=ky7vVlBQBY) |  | 0 | In recent years, the Shapley value and SHAP explanations have emerged as one of the most dominant paradigms for providing post-hoc explanations of blackbox models. Despite their well-founded theoretical properties, many recent works have focused on the limitations in both their computational... | James Enouen, Yan Liu |  |
| 3334 |  |  [Watermark Anything With Localized Messages](https://openreview.net/forum?id=IkZVDzdC8M) |  | 0 | Image watermarking methods are not tailored to handle small watermarked areas. This restricts applications in real-world scenarios where parts of the image may come from different sources or have been edited. We introduce a deep-learning model for localized image watermarking, dubbed the Watermark... | Alain Oliviero Durmus, Matthijs Douze, Pierre Fernandez, Teddy Furon, Tom Sander |  |
| 3335 |  |  [M^3PC: Test-time Model Predictive Control using Pretrained Masked Trajectory Model](https://openreview.net/forum?id=inOwd7hZC1) |  | 0 | Recent work in Offline Reinforcement Learning (RL) has shown that a unified transformer trained under a masked auto-encoding objective can effectively capture the relationships between different modalities (e.g., states, actions, rewards) within given trajectory datasets. However, this information... | Kehan Wen, Lei Ke, Yao Mu, Yutong Hu |  |
| 3336 |  |  [TEASER: Token Enhanced Spatial Modeling for Expressions Reconstruction](https://openreview.net/forum?id=pTeOOKnjGM) |  | 0 | 3D facial reconstruction from a single in-the-wild image is a crucial task in human-centered computer vision tasks. While existing methods can recover accurate facial shapes, there remains significant space for improvement in fine-grained expression capture. Current approaches struggle with... | Ailing Zhang, Lei Zhu, Lijian Lin, Ye Zhu, Yu Li, Yunfei Liu |  |
| 3337 |  |  [SurFhead: Affine Rig Blending for Geometrically Accurate 2D Gaussian Surfel Head Avatars](https://openreview.net/forum?id=1x1gGg49jr) |  | 0 | Recent advancements in head avatar rendering using Gaussian primitives have achieved significantly high-fidelity results. Although precise head geometry is crucial for applications like mesh reconstruction and relighting, current methods struggle to capture intricate geometric details and render... | Hyojin Jang, Jaegul Choo, Jaeseong Lee, Junha Hyung, Marcel C. Bühler, MinJung Kim, Sungwon Hwang, Taewoong Kang |  |
| 3338 |  |  [BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games](https://openreview.net/forum?id=fp6t3F669F) |  | 0 | Large Language Models (LLMs) and Vision Language Models (VLMs) possess extensive knowledge and exhibit promising reasoning abilities, however, they still struggle to perform well in complex, dynamic environments. Real-world tasks require handling intricate interactions, advanced spatial reasoning,... | Akbir Khan, Bartlomiej Cupial, Davide Paglieri, Eduardo Pignatelli, Jack ParkerHolder, Jakob Nicolaus Foerster, Lerrel Pinto, Lukasz Kucinski, Maciej Wolczyk, Rob Fergus, Samuel Coward, Tim Rocktäschel, Ulyana Piterbarg |  |
| 3339 |  |  [Ensembles of Low-Rank Expert Adapters](https://openreview.net/forum?id=l0gZS0sAlf) |  | 0 | The training and fine-tuning of large language models (LLMs) often involve diverse textual data from multiple sources, which poses challenges due to conflicting gradient directions, hindering optimization and specialization. These challenges can undermine model generalization across tasks,... | Chao Zhang, MohamadAli Torkamani, Vianne R. Gao, Yinghao Li |  |
| 3340 |  |  [Does Refusal Training in LLMs Generalize to the Past Tense?](https://openreview.net/forum?id=aJUuere4fM) |  | 0 | Refusal training is widely used to prevent LLMs from generating harmful, undesirable, or illegal outputs. We reveal a curious generalization gap in the current refusal training approaches: simply reformulating a harmful request in the past tense (e.g., \*"How to make a Molotov cocktail?"\* to... | Maksym Andriushchenko, Nicolas Flammarion |  |
| 3341 |  |  [Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks](https://openreview.net/forum?id=hXA8wqRdyV) |  | 0 | We show that even the most recent safety-aligned LLMs are not robust to simple \*adaptive\* jailbreaking attacks. First, we demonstrate how to successfully leverage access to \*logprobs\* for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and... | Francesco Croce, Maksym Andriushchenko, Nicolas Flammarion |  |
| 3342 |  |  [Laplace Sample Information: Data Informativeness Through a Bayesian Lens](https://openreview.net/forum?id=qO6dk9KfIp) |  | 0 | Accurately estimating the informativeness of individual samples in a dataset is an important objective in deep learning, as it can guide sample selection, which can improve model efficiency and accuracy by removing redundant or potentially harmful samples. We propose $\text{\textit{Laplace Sample... | Daniel Rueckert, Georgios Kaissis, Johannes Kaiser, Kristian Schwethelm |  |
| 3343 |  |  [Structural-Entropy-Based Sample Selection for Efficient and Effective Learning](https://openreview.net/forum?id=xUMI52rrW7) |  | 0 | Sample selection improves the efficiency and effectiveness of machine learning models by providing informative and representative samples. Typically, samples can be modeled as a sample graph, where nodes are samples and edges represent their similarities. Most existing methods are based on local... | Guozu Ma, Jiangning Zhu, Minzhi Lin, Shixia Liu, Tianchi Xie, Wei Chen, Weikai Yang |  |
| 3344 |  |  [Stable Segment Anything Model](https://openreview.net/forum?id=ooxj2Audlq) |  | 0 | The Segment Anything Model (SAM) achieves remarkable promptable segmentation given high-quality prompts which, however, often require good skills to specify. To make SAM robust to casual prompts, this paper presents the first comprehensive analysis on SAM’s segmentation stability across a diverse... | ChiKeung Tang, Di Zhang, Lei Ke, Mingqiao Ye, Pengfei Wan, Qi Fan, Xin Tao, YuWing Tai |  |
| 3345 |  |  [ProtoSnap: Prototype Alignment For Cuneiform Signs](https://openreview.net/forum?id=XHTirKsQV6) |  | 0 | The cuneiform writing system served as the medium for transmitting knowledge in the ancient Near East for a period of over three thousand years. Cuneiform signs have a complex internal structure which is the subject of expert paleographic analysis, as variations in sign shapes bear witness to... | Enrique Jiménez, Hadar AverbuchElor, Morris Alper, Rachel Mikulinsky, Shai Gordin, Yoram Cohen |  |
| 3346 |  |  [Learning Fine-Grained Representations through Textual Token Disentanglement in Composed Video Retrieval](https://openreview.net/forum?id=wGa2plE8ka) |  | 0 | With the explosive growth of video data, finding videos that meet detailed requirements in large datasets has become a challenge. To address this, the composed video retrieval task has been introduced, enabling users to retrieve videos using complex queries that involve both visual and textual... | Junshu Sun, Shuhui Wang, Yaowei Wang, Yiling Wu, Yue Wu, Zhaobo Qi |  |
| 3347 |  |  [Salvage: Shapley-distribution Approximation Learning Via Attribution Guided Exploration for Explainable Image Classification](https://openreview.net/forum?id=WBUVagRgsd) |  | 0 | The integration of deep learning into critical vision application areas has given rise to a necessity for techniques that can explain the rationale behind predictions. In this paper, we address this need by introducing Salvage, a novel removal-based explainability method for image classification.... | Gabriel Kalweit, Hanne Raum, Jens Rahnfeld, Joschka Boedecker, Maria Kalweit, Mehdi Naouar, Yannick Vogt |  |
| 3348 |  |  [Fine-tuning can Help Detect Pretraining Data from Large Language Models](https://openreview.net/forum?id=X8dzvdkQwO) |  | 0 | In the era of large language models (LLMs), detecting pretraining data has been increasingly important due to concerns about fair evaluation and ethical risks. Current methods differentiate members and non-members by designing scoring functions, like Perplexity and Min-k%. However, the diversity... | Bingyi Jing, Hengxiang Zhang, Hongxin Wei, Songxin Zhang |  |
| 3349 |  |  [Uncertainty and Influence aware Reward Model Refinement for Reinforcement Learning from Human Feedback](https://openreview.net/forum?id=iamWnRpMuQ) |  | 0 | Reinforcement Learning from Human Feedback (RLHF) has emerged as a standard and effective approach for training large language models (LLMs) with human preferences. In this framework, a learned reward model approximates human preferences and guides policy optimization, making it crucial to develop... | JiRong Wen, Qi Qi, Xing Tang, Xiuqiang He, Xu Chen, Yankai Lin, Yiju Guo, Zexu Sun |  |
| 3350 |  |  [A Simple Framework for Open-Vocabulary Zero-Shot Segmentation](https://openreview.net/forum?id=QzPKSUUcud) |  | 0 | Zero-shot classification capabilities naturally arise in models trained within a vision-language contrastive framework. Despite their classification prowess, these models struggle in dense tasks like zero-shot open-vocabulary segmentation. This deficiency is often attributed to the absence of... | Behzad Bozorgtabar, JeanPhilippe Thiran, Nikola Dukic, Thomas Stegmüller, Tim Lebailly, Tinne Tuytelaars |  |
| 3351 |  |  [Understanding the Generalization of In-Context Learning in Transformers: An Empirical Study](https://openreview.net/forum?id=yOhNLIqTEF) |  | 0 | Large language models (LLMs) like GPT-4 and LLaMA-3 utilize the powerful in-context learning (ICL) capability of Transformer architecture to learn on the fly from limited examples. While ICL underpins many LLM applications, its full potential remains hindered by a limited understanding of its... | Han Yu, Hao Zou, Haoran Wang, Jiansheng Li, Peng Cui, Renzhe Xu, Shikai Guan, Xingxuan Zhang, Yuan Xue |  |
| 3352 |  |  [Progressive Mixed-Precision Decoding for Efficient LLM Inference](https://openreview.net/forum?id=OVxmpus9NA) |  | 0 | In spite of the great potential of large language models (LLMs) across various tasks, their deployment on resource-constrained devices remains challenging due to their excessive computational and memory demands. Quantization has emerged as an effective solution by storing weights in reduced... | Alexandros Kouris, Fuwen Tan, Hao Mark Chen, Hongxiang Fan, Royson Lee, Stylianos I. Venieris |  |
| 3353 |  |  [Manifold Constraint Reduces Exposure Bias in Accelerated Diffusion Sampling](https://openreview.net/forum?id=5xmXUwDxep) |  | 0 | Diffusion models have demonstrated significant potential for generating high-quality images, audio, and videos. However, their iterative inference process entails substantial computational costs, limiting practical applications. Recently, researchers have introduced accelerated sampling methods... | Guang Dai, Haonan Lin, Jingdong Wang, Jun Chen, Mengmeng Wang, Yuzhe Yao, Zeyi Huang |  |
| 3354 |  |  [Gaussian Head & Shoulders: High Fidelity Neural Upper Body Avatars with Anchor Gaussian Guided Texture Warping](https://openreview.net/forum?id=HtbqsbNw9c) |  | 0 | The ability to reconstruct realistic and controllable upper body avatars from casual monocular videos is critical for various applications in communication and entertainment. By equipping the most recent 3D Gaussian Splatting representation with head 3D morphable models (3DMM), existing methods... | Cengiz Öztireli, Fangcheng Zhong, Jing Yang, Jingyi Wan, Tianhao (Walter) Wu, Zhilin Guo |  |
| 3355 |  |  [Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel](https://openreview.net/forum?id=OUuhwVsk9Z) |  | 0 | Creating high-quality data for training robust language-instructed agents is a long-lasting challenge in embodied AI. In this paper, we introduce a Self-Refining Data Flywheel (SRDF) that generates high-quality and large-scale navigational instruction-trajectory pairs by iteratively refining the... | Jialu Li, Kunchang Li, Limin Wang, Mohit Bansal, Shoubin Yu, Songze Li, Yali Wang, Yi Wang, Yicong Hong, Yu Qiao, Zun Wang |  |
| 3356 |  |  [Hydra-SGG: Hybrid Relation Assignment for One-stage Scene Graph Generation](https://openreview.net/forum?id=tpD1rs25Uu) |  | 0 | DETR introduces a simplified one-stage framework for scene graph generation (SGG) but faces challenges of sparse supervision and false negative samples. The former occurs because each image typically contains fewer than 10 relation annotations, while DETR-based SGG models employ over 100 relation... | Guikun Chen, Minghan Chen, Wenguan Wang, Yi Yang |  |
| 3357 |  |  [SANER: Annotation-free Societal Attribute Neutralizer for Debiasing CLIP](https://openreview.net/forum?id=x5hXkSMOd1) |  | 0 | Large-scale vision-language models, such as CLIP, are known to contain societal bias regarding protected attributes (e.g., gender, age). This paper aims to address the problems of societal bias in CLIP. Although previous studies have proposed to debias societal bias through adversarial learning or... | ChienYi Wang, MinHung Chen, Ryo Hachiuma, YuChiang Frank Wang, Yusuke Hirota, Yuta Nakashima |  |
| 3358 |  |  [Dynamical Diffusion: Learning Temporal Dynamics with Diffusion Models](https://openreview.net/forum?id=c5JZEPyFUE) |  | 0 | Diffusion models have emerged as powerful generative frameworks by progressively adding noise to data through a forward process and then reversing this process to generate realistic samples. While these models have achieved strong performance across various tasks and modalities, their application... | Baixu Chen, Haoran Xu, Jianmin Wang, Mingsheng Long, Xingzhuo Guo, Yu Zhang |  |
| 3359 |  |  [A General Framework for Off-Policy Learning with Partially-Observed Reward](https://openreview.net/forum?id=mUbYof5MKp) |  | 0 | Off-policy learning (OPL) in contextual bandits aims to learn a decision-making policy that maximizes the target rewards by using only historical interaction data collected under previously developed policies. Unfortunately, when rewards are only partially observed, the effectiveness of OPL... | Kosuke Kawakami, Masahiro Asami, Rikiya Takehi, Yuta Saito |  |
| 3360 |  |  [Not-So-Optimal Transport Flows for 3D Point Cloud Generation](https://openreview.net/forum?id=62Ff8LDAJZ) |  | 0 | Learning generative models of 3D point clouds is one of the fundamental problems in 3D generative learning. One of the key properties of point clouds is their permutation invariance, i.e., changing the order of points in a point cloud does not change the shape they represent. In this paper, we... | Arash Vahdat, Chao Liu, ChiWing Fu, KaHei Hui, Xiaohui Zeng |  |
| 3361 |  |  [Self-Boosting Large Language Models with Synthetic Preference Data](https://openreview.net/forum?id=7visV100Ms) |  | 0 | Through alignment with human preferences, Large Language Models (LLMs) have advanced significantly in generating honest, harmless, and helpful responses. However, collecting high-quality preference data is a resource-intensive and creativity-demanding process, especially for the continual... | Furu Wei, Li Dong, Qingxiu Dong, Xingxing Zhang, Zhifang Sui |  |
| 3362 |  |  [Unifying Unsupervised Graph-Level Anomaly Detection and Out-of-Distribution Detection: A Benchmark](https://openreview.net/forum?id=g90RNzs8wX) |  | 0 | To build safe and reliable graph machine learning systems, unsupervised graph-level anomaly detection (GLAD) and unsupervised graph-level out-of-distribution (OOD) detection (GLOD) have received significant attention in recent years. Though these two lines of research share the same objective, they... | Chenyu Li, Kaize Ding, Rui Miao, Shirui Pan, Xin Wang, Xu Shen, Yili Wang, Ying Wang, Yixin Liu |  |
| 3363 |  |  [Building Interactable Replicas of Complex Articulated Objects via Gaussian Splatting](https://openreview.net/forum?id=ix2yRWarPn) |  | 0 | Building interactable replicas of articulated objects is a key challenge in computer vision. Existing methods often fail to effectively integrate information across different object states, limiting the accuracy of part-mesh reconstruction and part dynamics modeling, particularly for complex... | Baoxiong Jia, Junfeng Ni, Ruijie Lu, Siyuan Huang, SongChun Zhu, Yu Liu |  |
| 3364 |  |  [MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference Optimization](https://openreview.net/forum?id=x1Okv4kbVR) |  | 0 | As large language models (LLMs) are rapidly advancing and achieving near-human capabilities on specific tasks, aligning them with human values is becoming more urgent. In scenarios where LLMs outperform humans, we face a weak-to-strong alignment problem where we need to effectively align strong... | Dawei Yin, Lingyong Yan, Maarten de Rijke, Pengjie Ren, Yougang Lyu, Zhaochun Ren, Zihan Wang |  |
| 3365 |  |  [EDiT: A Local-SGD-Based Efficient Distributed Training Method for Large Language Models](https://openreview.net/forum?id=xtlMtbVfWu) |  | 0 | Distributed training methods are crucial for large language models (LLMs). However, existing distributed training methods often suffer from communication bottlenecks, stragglers, and limited elasticity, particularly in heterogeneous or large-scale environments. Local SGD methods have been proposed... | Jiadi Jiang, Jialiang Cheng, Jian Sha, Ning Gao, Yun Yue, Zhiling Ye |  |
| 3366 |  |  [IDA-VLM: Towards Movie Understanding via ID-Aware Large Vision-Language Model](https://openreview.net/forum?id=N5YTixK4F1) |  | 0 | The rapid advancement of Large Vision-Language models (LVLMs) has demonstrated a spectrum of emergent capabilities. Nevertheless, current models only focus on the visual content of a single scenario, while their ability to associate instances across different scenes has not yet been explored, which... | Jie Wu, Peize Sun, Ping Luo, Shilong Zhang, Sidi Yang, Weifeng Chen, Xuefeng Xiao, Yatai Ji, Yujiu Yang |  |
| 3367 |  |  [Gaussian-Based Instance-Adaptive Intensity Modeling for Point-Supervised Facial Expression Spotting](https://openreview.net/forum?id=daD6uGMeLs) |  | 0 | Point-supervised facial expression spotting (P-FES) aims to localize facial expression instances in untrimmed videos, requiring only a single timestamp label for each instance during training. To address label sparsity, hard pseudo-labeling is often employed to propagate point labels to unlabeled... | Hajime Nagahara, Hideaki Hayashi, Yicheng Deng |  |
| 3368 |  |  [Learning Spatial-Semantic Features for Robust Video Object Segmentation](https://openreview.net/forum?id=EM93t94zEi) |  | 0 | Tracking and segmenting multiple similar objects with distinct or complex parts in long-term videos is particularly challenging due to the ambiguity in identifying target components and the confusion caused by occlusion, background clutter, and changes in appearance or environment over time. In... | Deshui Miao, Huchuan Lu, MingHsuan Yang, Xin Li, Yaowei Wang, Zhenyu He |  |
| 3369 |  |  [Boosting the visual interpretability of CLIP via adversarial fine-tuning](https://openreview.net/forum?id=khuIvzxPRp) |  | 0 | CLIP has achieved great success in visual representation learning and is becoming an important plug-in component for many large multi-modal models like LLaVA and DALL-E. However, the lack of interpretability caused by the intricate image encoder architecture and training process restricts its wider... | Farzan Farnia, Haoyu Lei, Qi Dou, Shizhan Gong |  |
| 3370 |  |  [DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian Splat Generation](https://openreview.net/forum?id=eajZpoQkGK) |  | 0 | Recent advancements in 3D content generation from text or a single image struggle with limited high-quality 3D datasets and inconsistency from 2D multi-view generation. We introduce DiffSplat, a novel 3D generative framework that natively generates 3D Gaussian splats by taming large-scale... | Bangbang Yang, Chenguo Lin, Panwang Pan, Yadong Mu, Zeming Li |  |
| 3371 |  |  [Scaling Offline Model-Based RL via Jointly-Optimized World-Action Model Pretraining](https://openreview.net/forum?id=T1OvCSFaum) |  | 0 | A significant aspiration of offline reinforcement learning (RL) is to develop a generalist agent with high capabilities from large and heterogeneous datasets. However, prior approaches that scale offline RL either rely heavily on expert trajectories or struggle to generalize to diverse unseen... | Binhua Li, Gang Xiong, Jie Cheng, Qinghai Miao, Ruixi Qiao, Yingwei Ma, Yisheng Lv, Yongbin Li |  |
| 3372 |  |  [Deep Incomplete Multi-view Learning via Cyclic Permutation of VAEs](https://openreview.net/forum?id=s4MwstmB8o) |  | 0 | Multi-View Representation Learning (MVRL) aims to derive a unified representation from multi-view data by leveraging shared and complementary information across views. However, when views are irregularly missing, the incomplete data can lead to representations that lack sufficiency and consistency.... | Jian Pu, Xin Gao |  |
| 3373 |  |  [X-NeMo: Expressive Neural Motion Reenactment via Disentangled Latent Attention](https://openreview.net/forum?id=ML8FH4s5Ts) |  | 0 | We propose X-NeMo, a novel zero-shot diffusion-based portrait animation pipeline that animates a static portrait using facial movements from a driving video of a different individual. Our work first identifies the root causes of the limitations in prior approaches, such as identity leakage and... | Chenxu Zhang, Guoxian Song, Hongyi Xu, Jinli Suo, Linjie Luo, Xiaochen Zhao, Xiu Li, Yebin Liu, You Xie |  |
| 3374 |  |  [On Discriminative Probabilistic Modeling for Self-Supervised Representation Learning](https://openreview.net/forum?id=s15HrqCqbr) |  | 0 | We study the discriminative probabilistic modeling on a continuous domain for the data prediction task of (multimodal) self-supervised representation learning. To address the challenge of computing the integral in the partition function for each anchor data, we leverage the multiple importance... | Bokun Wang, Tianbao Yang, Yiming Ying, Yunwen Lei |  |
| 3375 |  |  [ThermalGaussian: Thermal 3D Gaussian Splatting](https://openreview.net/forum?id=ybFRoGxZjs) |  | 0 | Thermography is especially valuable for the military and other users of surveillance cameras. Some recent methods based on Neural Radiance Fields (NeRF) are proposed to reconstruct the thermal scenes in 3D from a set of thermal and RGB images. However, unlike NeRF, 3D Gaussian splatting (3DGS)... | Anke Xue, Chenggang Yan, Hangyu Chen, Le Zhang, Ming Lu, Rongfeng Lu, Yuhang Qin, Zunjie Zhu |  |
| 3376 |  |  [Zero-shot forecasting of chaotic systems](https://openreview.net/forum?id=TqYjhJrp9m) |  | 0 | Time-series forecasting is a challenging problem that traditionally requires specialized models custom-trained for the specific task at hand. Recently, inspired by the success of large language models, foundation models pre-trained on vast amounts of time-series data from diverse domains have... | William Gilpin, Yuanzhao Zhang |  |
| 3377 |  |  [MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models](https://openreview.net/forum?id=f7WBRSuf9l) |  | 0 | Visual preference alignment involves training Large Vision-Language Models (LVLMs) to predict human preferences between visual inputs. This is typically achieved by using labeled datasets of chosen/rejected pairs and employing optimization algorithms like direct preference optimization (DPO).... | Conghui He, Dahua Lin, Haodong Duan, Jiaqi Wang, Pan Zhang, Xiaoyi Dong, Yuanjun Xiong, Yuhang Cao, Yuhang Zang, Ziyu Liu |  |
| 3378 |  |  [Causal Information Prioritization for Efficient Reinforcement Learning](https://openreview.net/forum?id=nDj45w5wam) |  | 0 | Current Reinforcement Learning (RL) methods often suffer from sample-inefficiency, resulting from blind exploration strategies that neglect causal relationships among states, actions, and rewards. Although recent causal approaches aim to address this problem, they lack grounded modeling of... | Fan Feng, Hongye Cao, Jing Huo, Tianpei Yang, Yang Gao |  |
| 3379 |  |  [Towards Empowerment Gain through Causal Structure Learning in Model-Based Reinforcement Learning](https://openreview.net/forum?id=vgXI1Ws0ma) |  | 0 | In Model-Based Reinforcement Learning (MBRL), incorporating causal structures into dynamics models provides agents with a structured understanding of the environments, enabling efficient decision. Empowerment as an intrinsic motivation enhances the ability of agents to actively control their... | Fan Feng, Hongye Cao, Jing Huo, Meng Fang, Shaokang Dong, Tianpei Yang, Yang Gao |  |
| 3380 |  |  [Toward Generalizing Visual Brain Decoding to Unseen Subjects](https://openreview.net/forum?id=At9JmGF3xy) |  | 0 | Visual brain decoding aims to decode visual information from human brain activities. Despite the great progress, one critical limitation of current brain decoding research lies in the lack of generalization capability to unseen subjects. Prior work typically focuses on decoding brain activity of... | Kexin Huang, Lei Zhang, Ping Li, Xiangtao Kong |  |
| 3381 |  |  [The robustness of differentiable Causal Discovery in misspecified Scenarios](https://openreview.net/forum?id=iaP7yHRq1l) |  | 0 | Causal discovery aims to learn causal relationships between variables from targeted data, making it a fundamental task in machine learning. However, causal discovery algorithms often rely on unverifiable causal assumptions, which are usually difficult to satisfy in real-world data, thereby limiting... | Duxin Chen, He Wang, Huiyang Yi, Mingyu Kang, Wenwu Yu, Yanyan He |  |
| 3382 |  |  [A Sanity Check for AI-generated Image Detection](https://openreview.net/forum?id=ODRHZrkOQM) |  | 0 | With the rapid development of generative models, discerning AI-generated content has evoked increasing attention from both industry and academia. In this paper, we conduct a sanity check on whether the task of AI-generated image detection has been solved. To start with, we present Chameleon... | Jiayin Cai, Ouxiang Li, Shilin Yan, Weidi Xie, Xiaolong Jiang, Yanbin Hao, Yao Hu |  |
| 3383 |  |  [Neural Phylogeny: Fine-Tuning Relationship Detection among Neural Networks](https://openreview.net/forum?id=jv2zHOalpL) |  | 0 | Given a collection of neural networks, can we determine which are parent models and which are child models fine-tuned from the parents? In this work, we strive to answer this question via introducing a new task termed as neural phylogeny detection, aimed at identifying the existence and direction... | Runpeng Yu, Xinchao Wang |  |
| 3384 |  |  [Breaking the log⁡(1/Δ2) Barrier: Better Batched Best Arm Identification with Adaptive Grids](https://openreview.net/forum?id=buxFBI6GG4) |  | 0 | We investigate the problem of batched best arm identification in multi-armed bandits, where we want to find the best arm from a set of $n$ arms while minimizing both the number of samples and batches. We introduce an algorithm that achieves near-optimal sample complexity and features an... | Dongruo Zhou, Qin Zhang, Tianyuan Jin |  |
| 3385 |  |  [Visual Agents as Fast and Slow Thinkers](https://openreview.net/forum?id=ncCuiD3KJQ) |  | 0 | Achieving human-level intelligence requires refining cognitive distinctions between \textit{System 1} and \textit{System 2} thinking. While contemporary AI, driven by large language models, demonstrates human-like traits, it falls short of genuine cognition. Transitioning from structured benchmarks... | ChengLong Wang, Dongfang Liu, Guangyan Sun, Mingyu Jin, Qifan Wang, Siqi Ma, Tong Geng, Ying Nian Wu, Yongfeng Zhang, Zhenting Wang |  |
| 3386 |  |  [Fourier Sliced-Wasserstein Embedding for Multisets and Measures](https://openreview.net/forum?id=BcYt84rcKq) |  | 0 | We present the _Fourier Sliced-Wasserstein (FSW) embedding_—a novel method to embed multisets and measures over $\mathbb{R}^d$ into Euclidean space. Our proposed embedding approximately preserves the sliced Wasserstein distance on distributions, thereby yielding geometrically meaningful... | Nadav Dym, Tal Amir |  |
| 3387 |  |  [Autonomous Evaluation of LLMs for Truth Maintenance and Reasoning Tasks](https://openreview.net/forum?id=iv1TpRCJeK) |  | 0 | This paper presents AutoEval, a novel benchmark for scaling Large Language Model (LLM) assessment in formal tasks with clear notions of correctness, such as truth maintenance in translation and logical reasoning. AutoEval is the first benchmarking paradigm that offers several key advantages... | Daksh Dobhal, Daniel Bramblett, Rushang Karia, Siddharth Srivastava |  |
| 3388 |  |  [Efficient Action-Constrained Reinforcement Learning via Acceptance-Rejection Method and Augmented MDPs](https://openreview.net/forum?id=AgMpK7z4bz) |  | 0 | Action-constrained reinforcement learning (ACRL) is a generic framework for learning control policies with zero action constraint violation, which is required by various safety-critical and resource-constrained applications. The existing ACRL methods can typically achieve favorable constraint... | PingChun Hsieh, ShaoHua Sun, Wei Hung |  |
| 3389 |  |  [TeaserGen: Generating Teasers for Long Documentaries](https://openreview.net/forum?id=G1n50BMqzm) |  | 0 | Teasers are an effective tool for promoting content in entertainment, commercial and educational fields. However, creating an effective teaser for long videos is challenging for it requires long-range multimodal modeling capability for the input videos, while necessitating maintaining audiovisual... | HaoWen Dong, Haven Kim, Julian J. McAuley, Paul Pu Liang, Taylor BergKirkpatrick, Weihan Xu |  |
| 3390 |  |  [CameraCtrl: Enabling Camera Control for Video Diffusion Models](https://openreview.net/forum?id=Z4evOUYrk7) |  | 0 | Controllability plays a crucial role in video generation, as it allows users to create and edit content more precisely. Existing models, however, lack control of camera pose that serves as a cinematic language to express deeper narrative nuances. To alleviate this issue, we introduce \method,... | Bo Dai, Ceyuan Yang, Gordon Wetzstein, Hao He, Hongsheng Li, Yinghao Xu, Yuwei Guo |  |
| 3391 |  |  [SSOLE: Rethinking Orthogonal Low-rank Embedding for Self-Supervised Learning](https://openreview.net/forum?id=zBgiCWCxJB) |  | 0 | Self-supervised learning (SSL) aims to learn meaningful representations from unlabeled data. Orthogonal Low-rank Embedding (OLE) shows promise for SSL by enhancing intra-class similarity in a low-rank subspace and promoting inter-class dissimilarity in a high-rank subspace, making it particularly... | Guillermo Sapiro, Lun Huang, Qiang Qiu |  |
| 3392 |  |  [3DitScene: Editing Any Scene via Language-guided Disentangled Gaussian Splatting](https://openreview.net/forum?id=iKDbLpVgQc) |  | 0 | Scene image editing is crucial for entertainment, photography, and advertising design. Existing methods solely focus on either 2D individual object or 3D global scene editing. This results in a lack of a unified approach to effectively control and manipulate scenes at the 3D level with different... | Bolei Zhou, Ceyuan Yang, Chaoyang Wang, Gordon Wetzstein, HsinYing Lee, Qihang Zhang, Yinghao Xu |  |
| 3393 |  |  [CoMotion: Concurrent Multi-person 3D Motion](https://openreview.net/forum?id=qKu6KWPgxt) |  | 0 | We introduce an approach for detecting and tracking detailed 3D poses of multiple people from a single monocular camera stream. Our system maintains temporally coherent predictions in crowded scenes filled with difficult poses and occlusions. Our model performs both strong per-frame detection and a... | Alejandro Newell, Lahav Lipson, Peiyun Hu, Stephan R. Richter, Vladlen Koltun |  |
| 3394 |  |  [Scaling Optimal LR Across Token Horizons](https://openreview.net/forum?id=WYL4eFLcxG) |  | 0 | State-of-the-art LLMs are powered by scaling -- scaling model size, training tokens, and cluster size. It is economically infeasible to extensively tune hyperparameters for the largest runs. Instead, approximately optimal hyperparameters must be inferred or transferred from smaller experiments.... | Alon Benhaim, Furu Wei, Johan Bjorck, Vishrav Chaudhary, Xia Song |  |
| 3395 |  |  [Order-aware Interactive Segmentation](https://openreview.net/forum?id=8ZLzw5pIrc) |  | 0 | Interactive segmentation aims to accurately segment target objects with minimal user interactions. However, current methods often fail to accurately separate target objects from the background, due to a limited understanding of order, the relative depth between objects in a scene. To address this... | Andong Deng, Anwesa Choudhuri, Benjamin Planche, Bin Wang, Meng Zheng, Qin Liu, Terrence Chen, Ulas Bagci, Zhongpai Gao, Ziyan Wu |  |
| 3396 |  |  [Re-Imagining Multimodal Instruction Tuning: A Representation View](https://openreview.net/forum?id=zxg6601zoc) |  | 0 | Multimodal instruction tuning has proven to be an effective strategy for achieving zero-shot generalization by fine-tuning pre-trained Large Multimodal Models (LMMs) with instruction-following data. However, as the scale of LMMs continues to grow, fully fine-tuning these models has become highly... | Cheng Han, Dongfang Liu, James Chenhao Liang, Lifu Huang, Majid Rabbani, Qifan Wang, Raghuveer Rao, Ruixiang Tang, Sohail A. Dianat, Yiyang Liu, Yugyung Lee |  |
| 3397 |  |  [Looped Transformers for Length Generalization](https://openreview.net/forum?id=2edigk8yoU) |  | 0 | Recent work has shown that Transformers trained from scratch can successfully solve various arithmetic and algorithmic tasks, such as adding numbers and computing parity. While these Transformers generalize well on unseen inputs of the same length, they struggle with length generalization, i.e.,... | Kangwook Lee, Kannan Ramchandran, Yilun Du, Ying Fan |  |
| 3398 |  |  [Exploring channel distinguishability in local neighborhoods of the model space in quantum neural networks](https://openreview.net/forum?id=gDcL7cgZBt) |  | 0 | With the increasing interest in Quantum Machine Learning, Quantum Neural Networks (QNNs) have emerged and gained significant attention. These models have, however, been shown to be notoriously difficult to train, which we hypothesize is partially due to the architectures, called ansatzes, that are... | Ivona Brandic, Sabrina Herbst, Sandeep Suresh Cranganore, Vincenzo De Maio |  |
| 3399 |  |  [Does Spatial Cognition Emerge in Frontier Models?](https://openreview.net/forum?id=WK6K1FMEQ1) |  | 0 | Not yet. We present SPACE, a benchmark that systematically evaluates spatial cognition in frontier models. Our benchmark builds on decades of research in cognitive science. It evaluates large-scale mapping abilities that are brought to bear when an organism traverses physical environments,... | Erik Wijmans, Philipp Krähenbühl, Santhosh Kumar Ramakrishnan, Vladlen Koltun |  |
| 3400 |  |  [Adaptive teachers for amortized samplers](https://openreview.net/forum?id=BdmVgLMvaf) |  | 0 | Amortized inference is the task of training a parametric model, such as a neural network, to approximate a distribution with a given unnormalized density where exact sampling is intractable. When sampling is modeled as a sequential decision-making process, reinforcement learning (RL) methods, such... | Emmanuel Bengio, Jarrid RectorBrooks, Jinkyoo Park, Leo Feng, Minsu Kim, Nikolay Malkin, Sanghyeok Choi, Sungsoo Ahn, Taeyoung Yun, Yoshua Bengio |  |
| 3401 |  |  [MLPs Learn In-Context on Regression and Classification Tasks](https://openreview.net/forum?id=MbX0t1rUlp) |  | 0 | In-context learning (ICL), the remarkable ability to solve a task from only input exemplars, is often assumed to be a unique hallmark of Transformer models. By examining commonly employed synthetic ICL tasks, we demonstrate that multi-layer perceptrons (MLPs) can also learn in-context. Moreover,... | Cengiz Pehlevan, William Lingxiao Tong |  |
| 3402 |  |  [Restyling Unsupervised Concept Based Interpretable Networks with Generative Models](https://openreview.net/forum?id=CexatBp6rx) |  | 0 | Developing inherently interpretable models for prediction has gained prominence in recent years. A subclass of these models, wherein the interpretable network relies on learning high-level concepts, are valued because of closeness of concept representations to human communication. However, the... | Alasdair Newson, Florence d'AlchéBuc, Jayneel Parekh, Pavlo Mozharovskyi, Quentin Bouniot |  |
| 3403 |  |  [Active Learning for Neural PDE Solvers](https://openreview.net/forum?id=x4ZmQaumRg) |  | 0 | Solving partial differential equations (PDEs) is a fundamental problem in engineering and science. While neural PDE solvers can be more efficient than established numerical solvers, they often require large amounts of training data that is costly to obtain. Active learning (AL) could help surrogate... | Daniel Musekamp, David Holzmüller, Makoto Takamoto, Marimuthu Kalimuthu, Mathias Niepert |  |
| 3404 |  |  [Contextual Self-paced Learning for Weakly Supervised Spatio-Temporal Video Grounding](https://openreview.net/forum?id=yHj6EunfVQ) |  | 0 | In this work, we focus on Weakly Supervised Spatio-Temporal Video Grounding (WSTVG). It is a multimodal task aimed at localizing specific subjects spatio-temporally based on textual queries without bounding box supervision. Motivated by recent advancements in multi-modal foundation models for... | Akash Kumar, Yogesh S. Rawat, Zsolt Kira |  |
| 3405 |  |  [Multimodal Lego: Model Merging and Fine-Tuning Across Topologies and Modalities in Biomedicine](https://openreview.net/forum?id=pH543jrbe8) |  | 0 | Learning holistic computational representations in physical, chemical or biological systems requires the ability to process information from different distributions and modalities within the same model. Thus, the demand for multimodal machine learning models has sharply risen for modalities that go... | Konstantin Hemker, Mateja Jamnik, Nikola Simidjievski |  |
| 3406 |  |  [Depth Pro: Sharp Monocular Metric Depth in Less Than a Second](https://openreview.net/forum?id=aueXfY0Clv) |  | 0 | We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such... | Alexey Bochkovskiy, Amaël Delaunoy, Hugo Germain, Marcel Santos, Stephan R. Richter, Vladlen Koltun, Yichao Zhou |  |
| 3407 |  |  [Zero-Shot Natural Language Explanations](https://openreview.net/forum?id=X6VVK8pIzZ) |  | 0 | Natural Language Explanations (NLEs) interpret the decision-making process of a given model through textual sentences. Current NLEs suffer from a severe limitation; they are unfaithful to the model’s actual reasoning process, as a separate textual decoder is explicitly trained to generate those... | Fawaz Sammani, Nikos Deligiannis |  |
| 3408 |  |  [Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models](https://openreview.net/forum?id=ZeaTvXw080) |  | 0 | Adding Object into images based on text instructions is a challenging task in semantic image editing, requiring a balance between preserving the original scene and seamlessly integrating the new object in a fitting location. Despite extensive efforts, existing models often struggle with this... | Dvir Samuel, Gal Chechik, Lior Wolf, Rinon Gal, Yoad Tewel, Yuval Atzmon |  |
| 3409 |  |  [RecDreamer: Consistent Text-to-3D Generation via Uniform Score Distillation](https://openreview.net/forum?id=aucMP9hGYv) |  | 0 | Current text-to-3D generation methods based on score distillation often suffer from geometric inconsistencies, leading to repeated patterns across different poses of 3D assets. This issue, known as the Multi-Face Janus problem, arises because existing methods struggle to maintain consistency across... | Bangzhen Liu, Chenxi Zheng, Shengfeng He, Xuemiao Xu, Yihong Lin, Yongwei Nie |  |
| 3410 |  |  [Aria-MIDI: A Dataset of Piano MIDI Files for Symbolic Music Modeling](https://openreview.net/forum?id=X5hrhgndxW) |  | 0 | We introduce an extensive new dataset of MIDI files, created by transcribing audio recordings of piano performances into their constituent notes. The data pipeline we use is multi-stage, employing a language model to autonomously crawl and score audio recordings from the internet based on their... | Louis Bradshaw, Simon Colton |  |
| 3411 |  |  [Efficient and Context-Aware Label Propagation for Zero-/Few-Shot Training-Free Adaptation of Vision-Language Model](https://openreview.net/forum?id=D10yarGQNk) |  | 0 | Vision-language models (VLMs) have revolutionized machine learning by leveraging large pre-trained models to tackle various downstream tasks. Although label, training, and data efficiency have improved, many state-of-the-art VLMs still require task-specific hyperparameter tuning and fail to fully... | Adam Goodge, Kui Jia, Xun Xu, Yongyi Su, Yushu Li |  |
| 3412 |  |  [Methods with Local Steps and Random Reshuffling for Generally Smooth Non-Convex Federated Optimization](https://openreview.net/forum?id=TrJ36UfD9P) |  | 0 | Non-convex Machine Learning problems typically do not adhere to the standard smoothness assumption. Based on empirical findings, Zhang et al. (2020b) proposed a more realistic generalized $(L_0,L_1)$-smoothness assumption, though it remains largely unexplored. Many existing algorithms designed for... | Eduard Gorbunov, Grigory Malinovsky, Martin Takác, Peter Richtárik, Petr Ostroukhov, Samuel Horváth, Yury Demidovich |  |
| 3413 |  |  [From Risk to Uncertainty: Generating Predictive Uncertainty Measures via Bayesian Estimation](https://openreview.net/forum?id=cWfpt2t37q) |  | 0 | There are various measures of predictive uncertainty in the literature, but their relationships to each other remain unclear. This paper uses a decomposition of statistical pointwise risk into components associated with different sources of predictive uncertainty: namely, aleatoric uncertainty... | Eric Moulines, Martin Takác, Maxim Panov, Nikita Kotelevskii, Vladimir Kondratyev |  |
| 3414 |  |  [Timer-XL: Long-Context Transformers for Unified Time Series Forecasting](https://openreview.net/forum?id=KMCJXjlDDr) |  | 0 | We present Timer-XL, a causal Transformer for unified time series forecasting. To uniformly predict multidimensional time series, we generalize next token prediction, predominantly adopted for 1D token sequences, to multivariate next token prediction. The paradigm formulates various forecasting... | Guo Qin, Jianmin Wang, Mingsheng Long, Xiangdong Huang, Yong Liu |  |
| 3415 |  |  [Reflective Gaussian Splatting](https://openreview.net/forum?id=xPxHQHDH2u) |  | 0 | Novel view synthesis has experienced significant advancements owing to increasingly capable NeRF- and 3DGS-based methods. However, reflective object reconstruction remains challenging, lacking a proper solution to achieve real-time, high-quality rendering while accommodating inter-reflection. To... | Chun Gu, Li Zhang, Xiatian Zhu, Yuxuan Yao, Zixuan Zeng |  |
| 3416 |  |  [Data Center Cooling System Optimization Using Offline Reinforcement Learning](https://openreview.net/forum?id=W8xukd70cU) |  | 0 | The recent advances in information technology and artificial intelligence have fueled a rapid expansion of the data center (DC) industry worldwide, accompanied by an immense appetite for electricity to power the DCs. In a typical DC, around 30-40% of the energy is spent on the cooling system rather... | Chenhui Liu, Feng Zhao, Hanfei Geng, Huiwen Zheng, Jichao Leng, Peng Cheng, Tianshun Hong, Xiangyu Zhu, Xianyuan Zhan, Xiao Hu, Yan Liang, Yunxin Liu, Ziteng He |  |
| 3417 |  |  [Bias Mitigation in Graph Diffusion Models](https://openreview.net/forum?id=CSj72Rr2PB) |  | 0 | Most existing graph diffusion models have significant bias problems. We observe that the forward diffusion’s maximum perturbation distribution in most models deviates from the standard Gaussian distribution, while reverse sampling consistently starts from a standard Gaussian distribution, which... | Kun Zhang, Meng Yu |  |
| 3418 |  |  [The Utility and Complexity of In- and Out-of-Distribution Machine Unlearning](https://openreview.net/forum?id=HVFMooKrHX) |  | 0 | Machine unlearning, the process of selectively removing data from trained models, is increasingly crucial for addressing privacy concerns and knowledge gaps post-deployment. Despite this importance, existing approaches are often heuristic and lack formal guarantees. In this paper, we analyze the... | Joshua Kazdan, Rachid Guerraoui, Sanmi Koyejo, Youssef Allouah |  |
| 3419 |  |  [SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints](https://openreview.net/forum?id=m8Rk3HLGFx) |  | 0 | Recent advancements in video diffusion models demonstrate remarkable capabilities in simulating real-world dynamics and 3D consistency. This progress motivates us to explore the potential of these models to maintain dynamic consistency across diverse viewpoints, a feature highly sought after in... | Di Zhang, Haoji Hu, Jianhong Bai, Menghan Xia, Pengfei Wan, Xintao Wang, Ziyang Yuan, Zuozhu Liu |  |
| 3420 |  |  [CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models](https://openreview.net/forum?id=E77uvbOTtp) |  | 0 | Classifier-free guidance (CFG) is a fundamental tool in modern diffusion models for text-guided generation. Although effective, CFG has notable drawbacks. For instance, DDIM with CFG lacks invertibility, complicating image editing; furthermore, high guidance scales, essential for high-quality... | Geon Yeong Park, Hyelin Nam, Hyungjin Chung, Jeongsol Kim, Jong Chul Ye |  |
| 3421 |  |  [TDDBench: A Benchmark for Training data detection](https://openreview.net/forum?id=hpeyWG1PP6) |  | 0 | Training Data Detection (TDD) is a task aimed at determining whether a specific data instance is used to train a machine learning model. In the computer security literature, TDD is also referred to as Membership Inference Attack (MIA). Given its potential to assess the risks of training data... | Defu Lian, Yi Yang, Zhihao Zhu |  |
| 3422 |  |  [Warm Diffusion: Recipe for Blur-Noise Mixture Diffusion Models](https://openreview.net/forum?id=rdSVgnLHQB) |  | 0 | Diffusion probabilistic models have achieved remarkable success in generative tasks across diverse data types. While recent studies have explored alternative degradation processes beyond Gaussian noise, this paper bridges two key diffusion paradigms: hot diffusion, which relies entirely on noise,... | ChingChun Huang, HaoChien Hsueh, WenHsiao Peng |  |
| 3423 |  |  [ZeroDiff: Solidified Visual-semantic Correlation in Zero-Shot Learning](https://openreview.net/forum?id=wy9FRV8O5s) |  | 0 | Zero-shot Learning (ZSL) aims to enable classifiers to identify unseen classes. This is typically achieved by generating visual features for unseen classes based on learned visual-semantic correlations from seen classes. However, most current generative approaches heavily rely on having a... | Fahad Shahbaz Khan, Haotian Xu, Kaizhu Huang, Shiming Chen, Shreyank N. Gowda, Xiaobo Jin, Xiaowei Huang, Yaochu Jin, Zihan Ye |  |
| 3424 |  |  [Deep Networks Learn Features From Local Discontinuities in the Label Function](https://openreview.net/forum?id=52UtL8uA35) |  | 0 | Deep neural networks outperform kernel machines on several datasets due to feature learning that happens during gradient descent training. In this paper, we analyze the mechanism through which feature learning happens and use a notion of features that corresponds to discontinuities in the true... | Chandra Shekar Lakshminarayanan, Harish Guruprasad Ramaswamy, Mahesh Lorik Yadav, Prithaj Banerjee |  |
| 3425 |  |  [Data Pruning by Information Maximization](https://openreview.net/forum?id=93XT0lKOct) |  | 0 | In this paper, we present InfoMax, a novel data pruning method, also known as coreset selection, designed to maximize the information content of selected samples while minimizing redundancy. By doing so, InfoMax enhances the overall informativeness of the coreset. The information of individual... | Haoru Tan, Shizhen Zhao, Sitong Wu, Wei Huang, Xiaojuan Qi |  |
| 3426 |  |  [Mufu: Multilingual Fused Learning for Low-Resource Translation with LLM](https://openreview.net/forum?id=0eMsrRMmCw) |  | 0 | Multilingual large language models (LLMs) are great translators, but this is largely limited to high-resource languages. For many LLMs, translating in and out of low-resource languages remains a challenging task. To maximize data efficiency in this low-resource setting, we introduce Mufu, which... | Honglin Yu, Nitish Gupta, Trevor Cohn, Zheng Wei Lim |  |
| 3427 |  |  [Rethinking LLM Unlearning Objectives: A Gradient Perspective and Go Beyond](https://openreview.net/forum?id=huo8MqVH6t) |  | 0 | Large language models (LLMs) should undergo rigorous audits to identify potential risks, such as copyright and privacy infringements. Once these risks emerge, timely updates are crucial to remove undesirable responses, ensuring legal and safe model usage. It has spurred recent research into LLM... | Bo Han, Jin Peng Zhou, Kilian Q. Weinberger, Qizhou Wang, Saebyeol Shin, Zhanke Zhou |  |
| 3428 |  |  [HiSplat: Hierarchical 3D Gaussian Splatting for Generalizable Sparse-View Reconstruction](https://openreview.net/forum?id=SBzIbJojs8) |  | 0 | Reconstructing 3D scenes from multiple viewpoints is a fundamental task in stereo vision. Recently, advances in generalizable 3D Gaussian Splatting have enabled high-quality novel view synthesis for unseen scenes from sparse input views by feed-forward predicting per-pixel Gaussian parameters... | Peng Ye, Shengji Tang, Tao Chen, Wanli Ouyang, Weicai Ye, Weihao Lin, Yang Zhou |  |
| 3429 |  |  [OBI-Bench: Can LMMs Aid in Study of Ancient Script on Oracle Bones?](https://openreview.net/forum?id=hL5jone2Oh) |  | 0 | We introduce OBI-Bench, a holistic benchmark crafted to systematically evaluate large multi-modal models (LMMs) on whole-process oracle bone inscriptions (OBI) processing tasks demanding expert-level domain knowledge and deliberate cognition. OBI-Bench includes 5,523 meticulously collected... | Guangtao Zhai, Tingzhu Chen, Wenjun Zhang, Zijian Chen |  |
| 3430 |  |  [Generalizable Human Gaussians from Single-View Image](https://openreview.net/forum?id=dQ2xiSIYzp) |  | 0 | In this work, we tackle the task of learning 3D human Gaussians from a single image, focusing on recovering detailed appearance and geometry including unobserved regions. We introduce a single-view generalizable Human Gaussian Model (HGM), which employs a novel generate-then-refine pipeline with... | Buzhen Huang, Chen Li, Gim Hee Lee, Hanlin Chen, Jianfeng Zhang, Jinnan Chen, Lingting Zhu |  |
| 3431 |  |  [Wavelet Diffusion Neural Operator](https://openreview.net/forum?id=FQhDIGuaJ4) |  | 0 | Simulating and controlling physical systems described by partial differential equations (PDEs) are crucial tasks across science and engineering. Recently, diffusion generative models have emerged as a competitive class of methods for these tasks due to their ability to capture long-term... | Haodong Feng, Long Wei, Peiyan Hu, Rui Wang, Ruiqi Feng, Tailin Wu, Tao Zhang, Xiang Zheng, Yue Wang, ZhiMing Ma |  |
| 3432 |  |  [MMSearch: Unveiling the Potential of Large Models as Multi-modal Search Engines](https://openreview.net/forum?id=J2Jyp1SZ0n) |  | 0 | The advent of Large Language Models (LLMs) has paved the way for AI search engines, e.g., SearchGPT, showcasing a new paradigm in human-internet interaction. However, most current AI search engines are limited to text-only settings, neglecting the multimodal user queries and the text-image... | Chunyuan Li, Dongzhi Jiang, Guanglu Song, Hongsheng Li, Jiayi Lei, Pan Lu, Peng Gao, Pengshuo Qiu, Renrui Zhang, Yanmin Wu, Yu Liu, Zehui Chen, Ziyu Guo |  |
| 3433 |  |  [Coreset Selection via Reducible Loss in Continual Learning](https://openreview.net/forum?id=mAztx8QO3B) |  | 0 | Rehearsal-based continual learning (CL) aims to mitigate catastrophic forgetting by maintaining a subset of samples from previous tasks and replaying them. The rehearsal memory can be naturally constructed as a coreset, designed to form a compact subset that enables training with performance... | Dong Gong, Javen Qinfeng Shi, Ruilin Tong, Yuhang Liu |  |
| 3434 |  |  [SegLLM: Multi-round Reasoning Segmentation with Large Language Models](https://openreview.net/forum?id=Pm1NXHgzyf) |  | 0 | We present SegLLM, a novel multi-round interactive reasoning segmentation model that enhances LLM-based segmentation by exploiting conversational memory of both visual and textual outputs. By leveraging a mask-aware multimodal LLM, SegLLM re-integrates previous segmentation results into its input... | Kazuki Kozuka, Kehan Li, Konstantinos Kallidromitis, Shaolun Zhang, Shufan Li, Trevor Darrell, Xudong Wang, Yusuke Kato |  |
| 3435 |  |  [Out-of-distribution Generalization for Total Variation based Invariant Risk Minimization](https://openreview.net/forum?id=c4wEKJOjY3) |  | 0 | Invariant risk minimization is an important general machine learning framework that has recently been interpreted as a total variation model (IRM-TV). However, how to improve out-of-distribution (OOD) generalization in the IRM-TV setting remains unsolved. In this paper, we extend IRM-TV to a... | Tianqi Zhong, Yuanchao Wang, ZhaoRong Lai |  |
| 3436 |  |  [Multi-Reward as Condition for Instruction-based Image Editing](https://openreview.net/forum?id=9RFocgIccP) |  | 0 | High-quality training triplets (instruction, original image, edited image) are essential for instruction-based image editing. Predominant training datasets (e.g., InsPix2Pix) are created using text-to-image generative models (e.g., Stable Diffusion, DALL-E) which are not trained for image editing.... | Fan Chen, Libo Zhang, Longyin Wen, Ming Li, Sijie Zhu, Tiejian Luo, Xin Gu |  |
| 3437 |  |  [Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations](https://openreview.net/forum?id=Hu0FSOSEyS) |  | 0 | Generative models transform random noise into images, while their inversion aims to reconstruct structured noise for recovery and editing. This paper addresses two key tasks: (i) \*inversion\* and (ii) \*editing\* of real images using stochastic equivalents of rectified flow models (e.g., Flux).... | Constantine Caramanis, Litu Rout, Nataniel Ruiz, Sanjay Shakkottai, WenSheng Chu, Yujia Chen |  |
| 3438 |  |  [Vector-ICL: In-context Learning with Continuous Vector Representations](https://openreview.net/forum?id=xing7dDGh3) |  | 0 | Large language models (LLMs) have shown remarkable in-context learning (ICL) capabilities on textual data. We explore whether these capabilities can be extended to continuous vectors from diverse domains, obtained from black-box pretrained encoders. By aligning input data with an LLM's embedding... | Chandan Singh, Jianfeng Gao, Jingbo Shang, Liyuan Liu, Yufan Zhuang |  |
| 3439 |  |  [Understanding and Mitigating Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing](https://openreview.net/forum?id=pymXpl4qvi) |  | 0 | Structured State Space Models (SSMs) have emerged as alternatives to transformers. While SSMs are often regarded as effective in capturing long-sequence dependencies, we rigorously demonstrate that they are inherently limited by strong recency bias. Our empirical studies also reveal that this bias... | Jiajun Zhu, Pan Li, Peihao Wang, Pragya Srivastava, Ruisi Cai, Yuehao Wang, Zhangyang Wang |  |
| 3440 |  |  [LLaMaFlex: Many-in-one LLMs via Generalized Pruning and Weight Sharing](https://openreview.net/forum?id=AyC4uxx2HW) |  | 0 | Large Language Model (LLM) providers typically train a family of models, each of a different size targeting a specific deployment scenario. Models in the family are all trained from scratch, making the process extremely resource intensive. Recent work has successfully reduced the cost of training... | Hongxu Yin, Jan Kautz, Pavlo Molchanov, Ruisi Cai, Saurav Muralidharan, Zhangyang Wang |  |
| 3441 |  |  [Generative Inbetweening: Adapting Image-to-Video Models for Keyframe Interpolation](https://openreview.net/forum?id=ykD8a9gJvy) |  | 0 | We present a method for generating video sequences with coherent motion between a pair of input keyframes. We adapt a pretrained large-scale image-to-video diffusion model (originally trained to generate videos moving forward in time from a single input image) for keyframe interpolation, i.e., to... | Aleksander Holynski, Boyang Zhou, Brian Curless, Ira KemelmacherShlizerman, Steven M. Seitz, Xiaojuan Wang |  |
| 3442 |  |  [Uncertainty-Aware Decoding with Minimum Bayes Risk](https://openreview.net/forum?id=hPpyUv1XyQ) |  | 0 | Despite their outstanding performance in the majority of scenarios, contemporary language models still occasionally generate undesirable outputs, for example, hallucinated text. While such behaviors have previously been linked to uncertainty, there is a notable lack of methods that actively... | Clara Meister, Iryna Gurevych, Nico Daheim, Thomas Möllenhoff |  |
| 3443 |  |  [Improving Equivariant Networks with Probabilistic Symmetry Breaking](https://openreview.net/forum?id=ZE6lrLvATd) |  | 0 | Equivariance encodes known symmetries into neural networks, often enhancing generalization. However, equivariant networks cannot \*break\* symmetries: the output of an equivariant network must, by definition, have at least the same self-symmetries as its input. This poses an important problem, both... | Hannah Lawrence, SékouOumar Kaba, Vasco Portilheiro, Yan Zhang |  |
| 3444 |  |  [CraftRTL: High-quality Synthetic Data Generation for Verilog Code Models with Correct-by-Construction Non-Textual Representations and Targeted Code Repair](https://openreview.net/forum?id=8KQzoD5XAr) |  | 0 | Despite the significant progress made in code generation with large language models, challenges persist, especially with hardware description languages such as Verilog. This paper first presents an analysis of fine-tuned LLMs on Verilog coding, with synthetic data from prior methods. We identify... | Haoxing Ren, Mingjie Liu, Wenfei Zhou, YunDa Tsai |  |
| 3445 |  |  [FreSh: Frequency Shifting for Accelerated Neural Representation Learning](https://openreview.net/forum?id=zMjjzXxS64) |  | 0 | Implicit Neural Representations (INRs) have recently gained attention as a powerful approach for continuously representing signals such as images, videos, and 3D shapes using multilayer perceptrons (MLPs). However, MLPs are known to exhibit a low-frequency bias, limiting their ability to capture... | Adam Kania, Jacek Tabor, Marko Mihajlovic, Przemyslaw Spurek, Sergey Prokudin |  |
| 3446 |  |  [PointOBB-v2: Towards Simpler, Faster, and Stronger Single Point Supervised Oriented Object Detection](https://openreview.net/forum?id=R22JPTQYWV) |  | 0 | Single point supervised oriented object detection has gained attention and made initial progress within the community. Diverse from those approaches relying on one-shot samples or powerful pretrained models (e.g. SAM), PointOBB has shown promise due to its prior-free feature. In this paper, we... | Botao Ren, Junwei Luo, Xue Yang, Yi Yu, Zhidong Deng |  |
| 3447 |  |  [A Unified Theory of Quantum Neural Network Loss Landscapes](https://openreview.net/forum?id=fv8TTt9srF) |  | 0 | Classical neural networks with random initialization famously behave as Gaussian processes in the limit of many neurons, which allows one to completely characterize their training and generalization behavior. No such general understanding exists for quantum neural networks (QNNs), which—outside of... | Eric Ricardo Anschütz |  |
| 3448 |  |  [Enhancing Robust Fairness via Confusional Spectral Regularization](https://openreview.net/forum?id=lW0ZndAimF) |  | 0 | Recent research has highlighted a critical issue known as \`\`robust fairness", where robust accuracy varies significantly across different classes, undermining the reliability of deep neural networks (DNNs). A common approach to address this has been to dynamically reweight classes during... | Gaojie Jin, Jiaxu Liu, Ronghui Mu, Sihao Wu, Tianjin Huang |  |
| 3449 |  |  [Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents](https://openreview.net/forum?id=U1T6sq12uj) |  | 0 | Previous studies have found that PLM-based retrieval models exhibit a preference for LLM-generated content, assigning higher relevance scores to these documents even when their semantic quality is comparable to human-written ones. This phenomenon, known as source bias, threatens the sustainable... | Gang Wang, Haiyuan Zhao, Haoyu Wang, JiRong Wen, Jun Xu, Liang Pang, Sunhao Dai, Xiao Zhang, Zhenhua Dong |  |
| 3450 |  |  [HART: Efficient Visual Generation with Hybrid Autoregressive Transformer](https://openreview.net/forum?id=q5sOv4xQe4) |  | 0 | We introduce Hybrid Autoregressive Transformer (HART), the first autoregressive (AR) visual generation model capable of directly generating 1024x1024 images, rivaling diffusion models in image generation quality. Existing AR models face limitations due to the poor image reconstruction quality of... | Enze Xie, Han Cai, Haotian Tang, Junsong Chen, Junyu Chen, Shang Yang, Song Han, Yao Lu, Yecheng Wu, Zhuoyang Zhang |  |
| 3451 |  |  [ANaGRAM: A Natural Gradient Relative to Adapted Model for efficient PINNs learning](https://openreview.net/forum?id=o1IiiNIoaA) |  | 0 | In the recent years, Physics Informed Neural Networks (PINNs) have received strong interest as a method to solve PDE driven systems, in particular for data assimilation purpose. This method is still in its infancy, with many shortcomings and failures that remain not properly understood. In this... | Cyril Furtlehner, Nilo Schwencke |  |
| 3452 |  |  [EdgeRunner: Auto-regressive Auto-encoder for Artistic Mesh Generation](https://openreview.net/forum?id=81cta3WQVI) |  | 0 | Current auto-regressive mesh generation methods suffer from issues such as incompleteness, insufficient detail, and poor generalization. In this paper, we propose an Auto-regressive Auto-encoder (ArAE) model capable of generating high-quality 3D meshes with up to 4,000 faces at a spatial resolution... | Gang Zeng, Jiaxiang Tang, MingYu Liu, Qinsheng Zhang, Xian Liu, Zekun Hao, Zhaoshuo Li |  |
| 3453 |  |  [Beyond Random Augmentations: Pretraining with Hard Views](https://openreview.net/forum?id=AK1C55o4r7) |  | 0 | Self-Supervised Learning (SSL) methods typically rely on random image augmentations, or views, to make models invariant to different transformations. We hypothesize that the efficacy of pretraining pipelines based on conventional random view sampling can be enhanced by explicitly selecting views... | Fabio Ferreira, Frank Hutter, Ivo Rapant, Jörg K. H. Franke |  |
| 3454 |  |  [Accelerating Auto-regressive Text-to-Image Generation with Training-free Speculative Jacobi Decoding](https://openreview.net/forum?id=LZfjxvqw0N) |  | 0 | The current large auto-regressive models can generate high-quality, high-resolution images, but these models require hundreds or even thousands of steps of next-token prediction during inference, resulting in substantial time consumption. In existing studies, Jacobi decoding, an iterative parallel... | Guohao Dai, Han Shi, Xian Liu, Xihui Liu, Xuefei Ning, Yao Teng, Yu Wang, Zhenguo Li |  |
| 3455 |  |  [The Breakdown of Gaussian Universality in Classification of High-dimensional Linear Factor Mixtures](https://openreview.net/forum?id=UrKbn51HjA) |  | 0 | The assumption of Gaussian or Gaussian mixture data has been extensively exploited in a long series of precise performance analyses of machine learning (ML) methods, on large datasets having comparably numerous samples and features. To relax this restrictive assumption, subsequent efforts have been... | Xiaoyi Mai, Zhenyu Liao |  |
| 3456 |  |  [MoDGS: Dynamic Gaussian Splatting from Casually-captured Monocular Videos with Depth Priors](https://openreview.net/forum?id=2prShxdLkX) |  | 0 | In this paper, we propose MoDGS, a new pipeline to render novel-view images in dynamic scenes using only casually captured monocular videos. Previous monocular dynamic NeRF or Gaussian Splatting methods strongly rely on the rapid movement of input cameras to construct multiview consistency but fail... | Jiepeng Wang, Junhui Hou, Peng Wang, Qingming Liu, Wenping Wang, Xianqiang Lyu, Yuan Liu |  |
| 3457 |  |  [HaDeMiF: Hallucination Detection and Mitigation in Large Language Models](https://openreview.net/forum?id=VwOYxPScxB) |  | 0 | The phenomenon of knowledge hallucinations has raised substantial concerns about the security and reliability of deployed large language models (LLMs). Current methods for detecting hallucinations primarily depend on manually designed individual metrics, such as prediction uncertainty and... | Mingjie Zhang, Shikun Zhang, Wei Ye, Xiaoling Zhou, Zhemg Lee |  |
| 3458 |  |  [LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization](https://openreview.net/forum?id=qTrEq31Shm) |  | 0 | Large Language Models (LLMs) have demonstrated remarkable capabilities through pretraining and alignment. However, superior short-context LLMs may underperform in long-context scenarios due to insufficient long-context alignment. This alignment process remains challenging due to the impracticality... | Guanzheng Chen, Lidong Bing, Michael Shieh, Xin Li |  |
| 3459 |  |  [See What You Are Told: Visual Attention Sink in Large Multimodal Models](https://openreview.net/forum?id=7uDI7w5RQA) |  | 0 | Large multimodal models (LMMs) "see" images by leveraging the attention mechanism between text and visual tokens in the transformer decoder. Ideally, these models should focus on key visual information relevant to the text token. However, recent findings indicate that LMMs have an extraordinary... | Jinyeong Kim, Junhyeok Kim, Seil Kang, Seong Jae Hwang |  |
| 3460 |  |  [Domain Guidance: A Simple Transfer Approach for a Pre-trained Diffusion Model](https://openreview.net/forum?id=PplM2kDrl3) |  | 0 | Recent advancements in diffusion models have revolutionized generative modeling. However, the impressive and vivid outputs they produce often come at the cost of significant model scaling and increased computational demands. Consequently, building personalized diffusion models based on... | Jianmin Wang, Jincheng Zhong, Mingsheng Long, Xiangcheng Zhang |  |
| 3461 |  |  [MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation](https://openreview.net/forum?id=4z3IguA4Zg) |  | 0 | Multimodal Large Language Models (MLLMs) frequently exhibit hallucination phenomena, but the underlying reasons remain poorly understood. In this paper, we present an empirical analysis and find that, although MLLMs incorrectly generate the objects in the final output, they are actually able to... | Bozhong Tian, Chenxi Wang, Haoming Xu, Huajun Chen, Ningyu Zhang, Shumin Deng, Xiang Chen |  |
| 3462 |  |  [GDrag: Towards General-Purpose Interactive Editing with Anti-ambiguity Point Diffusion](https://openreview.net/forum?id=8G3FyfHIko) |  | 0 | Recent interactive point-based image manipulation methods have gained considerable attention for being user-friendly. However, these methods still face two types of ambiguity issues that can lead to unsatisfactory outcomes, namely, intention ambiguity which misinterprets the purposes of users, and... | Hanhui Li, Xiaodan Liang, Xiaojian Lin, Yiqiang Yan, Yuhao Cheng |  |
| 3463 |  |  [AvatarGO: Zero-shot 4D Human-Object Interaction Generation and Animation](https://openreview.net/forum?id=Trf0R8eoGF) |  | 0 | Recent advancements in diffusion models have led to significant improvements in the generation and animation of 4D full-body human-object interactions (HOI). Nevertheless, existing methods primarily focus on SMPL-based motion generation, which is limited by the scarcity of realistic large-scale... | Kai Han, KwanYee K. Wong, Liang Pan, Yukang Cao, Ziwei Liu |  |
| 3464 |  |  [Calibrating LLMs with Information-Theoretic Evidential Deep Learning](https://openreview.net/forum?id=YcML3rJl0N) |  | 0 | Fine-tuned large language models (LLMs) often exhibit overconfidence, particularly when trained on small datasets, resulting in poor calibration and inaccurate uncertainty estimates. Evidential Deep Learning (EDL), an uncertainty-aware approach, enables uncertainty estimation in a single forward... | Bernd Bischl, David Rügamer, Mina Rezaei, Yawei Li |  |
| 3465 |  |  [TIGeR: Unifying Text-to-Image Generation and Retrieval with Large Multimodal Models](https://openreview.net/forum?id=mr2icR6dpD) |  | 0 | How humans can effectively and efficiently acquire images has always been a perennial question. A classic solution is \*text-to-image retrieval\* from an existing database; however, the limited database typically lacks creativity. By contrast, recent breakthroughs in \*text-to-image generation\*... | Haochuan Li, Leigang Qu, Liqiang Nie, Tan Wang, TatSeng Chua, Wenjie Wang, Yongqi Li |  |
| 3466 |  |  [Cross-Embodiment Dexterous Grasping with Reinforcement Learning](https://openreview.net/forum?id=twIPSx9qHn) |  | 0 | Dexterous hands exhibit significant potential for complex real-world grasping tasks. While recent studies have primarily focused on learning policies for specific robotic hands, the development of a universal policy that controls diverse dexterous hands remains largely unexplored. In this work, we... | Bohan Zhou, Haoqi Yuan, Yuhui Fu, Zongqing Lu |  |
| 3467 |  |  [VEDIT: Latent Prediction Architecture For Procedural Video Representation Learning](https://openreview.net/forum?id=LDAj4UJ4aL) |  | 0 | Procedural video representation learning is an active research area where the objective is to learn an agent which can anticipate and forecast the future given the present video input, typically in conjunction with textual annotations. Prior works often rely on large-scale pretraining of visual... | Han Lin, Koustuv Sinha, Mido Assran, Mohit Bansal, Mojtaba Komeili, Nicolas Ballas, Tushar Nagarajan |  |
| 3468 |  |  [Refine-by-Align: Reference-Guided Artifacts Refinement through Semantic Alignment](https://openreview.net/forum?id=D9CRb1KZQc) |  | 0 | Personalized image generation has emerged from the recent advancements in generative models. However, these generated personalized images often suffer from localized artifacts such as incorrect logos, reducing fidelity and fine-grained identity details of the generated results. Furthermore, there... | Brian L. Price, Daniel G. Aliaga, He Zhang, Jianming Zhang, Liu He, Scott Cohen, Soo Ye Kim, Wei Xiong, Yizhi Song, Zhe Lin, Zhifei Zhang |  |
| 3469 |  |  [FreCaS: Efficient Higher-Resolution Image Generation via Frequency-aware Cascaded Sampling](https://openreview.net/forum?id=TsBDfe8Ra5) |  | 0 | While image generation with diffusion models has achieved a great success, generating images of higher resolution than the training size remains a challenging task due to the high computational cost. Current methods typically perform the entire sampling process at full resolution and process all... | Lei Zhang, Ruihuang Li, Zhengqiang Zhang |  |
| 3470 |  |  [Transformers Learn Low Sensitivity Functions: Investigations and Implications](https://openreview.net/forum?id=4ikjWBs3tE) |  | 0 | Transformers achieve state-of-the-art accuracy and robustness across many tasks, but an understanding of their inductive biases and how those biases differ from other neural network architectures remains elusive. In this work, we identify the sensitivity of the model to token-wise random... | Bhavya Vasudeva, Deqing Fu, Elliott Kau, Tianyi Zhou, Vatsal Sharan, Youqi Huang |  |
| 3471 |  |  [TLDR: Token-Level Detective Reward Model for Large Vision Language Models](https://openreview.net/forum?id=Zy2XgaGpDw) |  | 0 | Although reward models have been successful in improving multimodal large language models, the reward models themselves remain brutal and contain minimal information. Notably, existing reward models only mimic human annotations by assigning only one feedback to any text, no matter how long the text... | Deqing Fu, Guan Pang, Lawrence Chen, Pengchuan Zhang, Robin Jia, Rui Wang, Tong Xiao, Wang Zhu |  |
| 3472 |  |  [Statistical Advantages of Perturbing Cosine Router in Mixture of Experts](https://openreview.net/forum?id=faDMOmnsjx) |  | 0 | The cosine router in Mixture of Experts (MoE) has recently emerged as an attractive alternative to the conventional linear router. Indeed, the cosine router demonstrates favorable performance in image and language tasks and exhibits better ability to mitigate the representation collapse issue,... | Huy Nguyen, Huyen Trang Pham, Nhat Ho, Pedram Akbarian, Shujian Zhang, Thien Trang Nguyen Vu |  |
| 3473 |  |  [Interpretable Unsupervised Joint Denoising and Enhancement for Real-World low-light Scenarios](https://openreview.net/forum?id=PVHoELf5UN) |  | 0 | Real-world low-light images often suffer from complex degradations such as local overexposure, low brightness, noise, and uneven illumination. Supervised methods tend to overfit to specific scenarios, while unsupervised methods, though better at generalization, struggle to model these degradations... | Haoqian Wang, Huaqiu Li, Xiaowan Hu |  |
| 3474 |  |  [Learning Clustering-based Prototypes for Compositional Zero-Shot Learning](https://openreview.net/forum?id=eE2PXlNydB) |  | 0 | Learning primitive (i.e., attribute and object) concepts from seen compositions is the primary challenge of Compositional Zero-Shot Learning (CZSL). Existing CZSL solutions typically rely on oversimplified data assumptions, e.g., modeling each primitive with a single centroid primitive... | Hongyu Qu, Jianan Wei, Wenguan Wang, Xiangbo Shu |  |
| 3475 |  |  [SAGEPhos: Sage Bio-Coupled and Augmented Fusion for Phosphorylation Site Detection](https://openreview.net/forum?id=hLwcNSFhC2) |  | 0 | Phosphorylation site prediction based on kinase-substrate interaction plays a vital role in understanding cellular signaling pathways and disease mechanisms. Computational methods for this task can be categorized into kinase-family-focused and individual kinase-targeted approaches. Individual... | Chunbin Gu, Hanqun Cao, Jingjie Zhang, Xiaorui Wang, Zijun Gao |  |
| 3476 |  |  [Depth Any Video with Scalable Synthetic Data](https://openreview.net/forum?id=gWqFbnKsqR) |  | 0 | Video depth estimation has long been hindered by the scarcity of consistent and scalable ground truth data, leading to inconsistent and unreliable results. In this paper, we introduce Depth Any Video, a model that tackles the challenge through two key innovations. First, we develop a scalable... | Binbin Lin, Chunhua Shen, Di Huang, Haifeng Liu, Honghui Yang, Tong He, Wanli Ouyang, Wei Yin, Xiaofei He |  |
| 3477 |  |  [SPA: 3D Spatial-Awareness Enables Effective Embodied Representation](https://openreview.net/forum?id=6TLdqAZgzn) |  | 0 | In this paper, we introduce SPA, a novel representation learning framework that emphasizes the importance of 3D spatial awareness in embodied AI. Our approach leverages differentiable neural rendering on multi-view images to endow a vanilla Vision Transformer (ViT) with intrinsic spatial... | Haoyi Zhu, Honghui Yang, Jiange Yang, Limin Wang, Tong He, Yating Wang |  |
| 3478 |  |  [Incremental Causal Effect for Time to Treatment Initialization](https://openreview.net/forum?id=0mtz0pet1z) |  | 0 | We consider time to treatment initialization. This can commonly occur in preventive medicine, such as disease screening and vaccination; it can also occur with non-fatal health conditions such as HIV infection without the onset of AIDS. While traditional causal inference focused on ‘when to treat’... | Andrew Ying, Ronghui Xu, Zhichen Zhao |  |
| 3479 |  |  [Leveraging Sub-Optimal Data for Human-in-the-Loop Reinforcement Learning](https://openreview.net/forum?id=DSyHRkpI7v) |  | 0 | To create useful reinforcement learning (RL) agents, step zero is to design a suitable reward function that captures the nuances of the task. However, reward engineering can be a difficult and time-consuming process. Instead, human-in-the-loop RL methods hold the promise of learning reward... | Calarina Muslimani, Matthew E. Taylor |  |
| 3480 |  |  [Training-Free Dataset Pruning for Instance Segmentation](https://openreview.net/forum?id=rvxWEbTtRY) |  | 0 | Existing dataset pruning techniques primarily focus on classification tasks, limiting their applicability to more complex and practical tasks like instance segmentation. Instance segmentation presents three key challenges: pixel-level annotations, instance area variations, and class imbalances,... | Ivor W. Tsang, Lingao Xiao, Yalun Dai, Yang He |  |
| 3481 |  |  [Towards Understanding Text Hallucination of Diffusion Models via Local Generation Bias](https://openreview.net/forum?id=SKW10XJlAI) |  | 0 | Score-based diffusion models have achieved incredible performance in generating realistic images, audio, and video data. While these models produce high-quality samples with impressive details, they often introduce unrealistic artifacts, such as distorted fingers or hallucinated texts with no... | Gao Huang, Kaifeng Lyu, Mengdi Wang, Rui Lu, Runzhe Wang, Xitai Jiang |  |
| 3482 |  |  [VideoShield: Regulating Diffusion-based Video Generation Models via Watermarking](https://openreview.net/forum?id=uzz3qAYy0D) |  | 0 | Artificial Intelligence Generated Content (AIGC) has advanced significantly, particularly with the development of video generation models such as text-to-video (T2V) models and image-to-video (I2V) models. However, like other AIGC types, video generation requires robust content control. A common... | Han Qiu, Jie Zhang, Jiwei Li, Qing Guo, Runyi Hu, Tianwei Zhang, Yiming Li |  |
| 3483 |  |  [PARTNR: A Benchmark for Planning and Reasoning in Embodied Multi-agent Tasks](https://openreview.net/forum?id=T5QLRRHyL1) |  | 0 | We present a benchmark for Planning And Reasoning Tasks in humaN-Robot collaboration (PARTNR) designed to study human-robot coordination in household activities. PARTNR tasks exhibit characteristics of everyday tasks, such as spatial, temporal, and heterogeneous agent capability constraints. We... | Akshara Rai, Alexander Clegg, Daniel Tran, Eric Undersander, Gunjan Chhablani, Ishita Prasad, Jacob Krantz, Joanne Truong, John M. Turner, Matthew Chang, Michal Hlavac, Mikael Dallaire Cote, Priyam Parashar, Ram Ramrakhya, Roozbeh Mottaghi, Ruta Desai, Siddharth Patki, TsungYen Yang, Vladimir Karashchuk, Xavier Puig |  |
| 3484 |  |  [Causal Identification for Complex Functional Longitudinal Studies](https://openreview.net/forum?id=96beVMeHh9) |  | 0 | Real-time monitoring in modern medical research introduces functional longitudinal data, characterized by continuous-time measurements of outcomes, treatments, and confounders. This complexity leads to uncountably infinite treatment-confounder feedbacks, which traditional causal inference... | Andrew Ying |  |
| 3485 |  |  [Boost Self-Supervised Dataset Distillation via Parameterization, Predefined Augmentation, and Approximation](https://openreview.net/forum?id=2RfWRKwxYh) |  | 0 | Although larger datasets are crucial for training large deep models, the rapid growth of dataset size has brought a significant challenge in terms of considerable training costs, which even results in prohibitive computational expenses. Dataset Distillation becomes a popular technique recently to... | JiaJiun Yao, ShengFeng Yu, WeiChen Chiu |  |
| 3486 |  |  [GALA: Geometry-Aware Local Adaptive Grids for Detailed 3D Generation](https://openreview.net/forum?id=KYOdZRR6nr) |  | 0 | We propose GALA, a novel representation of 3D shapes that (i) excels at capturing and reproducing complex geometry and surface details, (ii) is computationally efficient, and (iii) lends itself to 3D generative modelling with modern, diffusion-based schemes. The key idea of GALA is to exploit both... | Ali Mahdavi Amiri, Dingdong Yang, Hao Zhang, Konrad Schindler, Yizhi Wang |  |
| 3487 |  |  [MMEgo: Towards Building Egocentric Multimodal LLMs for Video QA](https://openreview.net/forum?id=67sSPPAZiG) |  | 0 | This research aims to comprehensively explore building a multimodal foundation model for egocentric video understanding. To achieve this goal, we work on three fronts. First, as there is a lack of QA data for egocentric video understanding, we automatically generate 7M high-quality QA samples for... | Bowen Zhang, Dan Xu, Erik A. Daxberger, Hanrong Ye, Haotian Zhang, Haoxuan You, Jiasen Lu, Lin Chen, Yanghao Li, Yinfei Yang, Zhe Gan, Zongyu Lin |  |
| 3488 |  |  [Beyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and LLM-as-a-judge](https://openreview.net/forum?id=E8gYIrbP00) |  | 0 | The effectiveness of automatic evaluation of generative models is typically measured by comparing the labels generated via automation with human labels using correlation metrics. However, metrics like Krippendorff's $\alpha$ and Randolph's $\kappa$ were originally designed to measure the... | Aparna Elangovan, Dan Roth, Jongwoo Ko, Lei Xu, Ling Liu, Mahsa Elyasi, Sravan Babu Bodapati |  |
| 3489 |  |  [Federated Q-Learning with Reference-Advantage Decomposition: Almost Optimal Regret and Logarithmic Communication Cost](https://openreview.net/forum?id=FoUpv84hMw) |  | 0 | In this paper, we consider model-free federated reinforcement learning for tabular episodic Markov decision processes. Under the coordination of a central server, multiple agents collaboratively explore the environment and learn an optimal policy without sharing their raw data. Despite recent... | Haochen Zhang, Lingzhou Xue, Zhong Zheng |  |
| 3490 |  |  [Remove Symmetries to Control Model Expressivity and Improve Optimization](https://openreview.net/forum?id=Gv0TOAigIY) |  | 0 | When symmetry is present in the loss function, the model is likely to be trapped in a low-capacity state that is sometimes known as a \`\`collapse." Being trapped in these low-capacity states can be a major obstacle to training across many scenarios where deep learning technology is applied. We... | Isaac L. Chuang, Liu Ziyin, Yizhou Xu |  |
| 3491 |  |  [ZIP: An Efficient Zeroth-order Prompt Tuning for Black-box Vision-Language Models](https://openreview.net/forum?id=2OegVbwvY2) |  | 0 | Recent studies have introduced various approaches for prompt-tuning black-box vision-language models, referred to as black-box prompt-tuning (BBPT). While BBPT has demonstrated considerable potential, it is often found that many existing methods require an excessive number of queries (i.e.,... | Jaeho Lee, Jaehyeon Jeong, Namhoon Lee, Seonghwan Park, Yongjun Kim |  |
| 3492 |  |  [Iformer: Integrating ConvNet and Transformer for Mobile Application](https://openreview.net/forum?id=4ytHislqDS) |  | 0 | We present a new family of mobile hybrid vision networks, called iFormer, with a focus on optimizing latency and accuracy on mobile applications. iFormer effectively integrates the fast local representation capacity of convolution with the efficient global modeling ability of self-attention. The... | Chuanyang Zheng |  |
| 3493 |  |  [kNN Attention Demystified: A Theoretical Exploration for Scalable Transformers](https://openreview.net/forum?id=49v8meXjHS) |  | 0 | Despite their power, Transformers face challenges with long sequences due to the quadratic complexity of self-attention. To address this limitation, methods like $k$-Nearest-Neighbor ($k$NN) attention have been introduced [Roy et al., 2017], enabling each token to attend to only its $k$ closest... | Themistoklis Haris |  |
| 3494 |  |  [Diffusion Feedback Helps CLIP See Better](https://openreview.net/forum?id=tLFWU6izoA) |  | 0 | Contrastive Language-Image Pre-training (CLIP), which excels at abstracting open-world representations across domains and modalities, has become a foundation for a variety of vision and multimodal tasks. However, recent studies reveal that CLIP has severe visual shortcomings, such as which can... | Fan Zhang, Jing Liu, Quan Sun, Wenxuan Wang, Xinlong Wang, Yepeng Tang |  |
| 3495 |  |  [Regulatory DNA Sequence Design with Reinforcement Learning](https://openreview.net/forum?id=F4IMiNhim1) |  | 0 | $\textit{Cis}$-regulatory elements (CREs), such as promoters and enhancers, are relatively short DNA sequences that directly regulate gene expression. The fitness of CREs, measured by their ability to modulate gene expression, highly depends on the nucleotide sequences, especially specific motifs... | Bing Su, Chuan Cao, JiRong Wen, Zhao Yang |  |
| 3496 |  |  [In-Context Editing: Learning Knowledge from Self-Induced Distributions](https://openreview.net/forum?id=w6rHCuN3YG) |  | 0 | In scenarios where language models must incorporate new information efficiently without extensive retraining, traditional fine-tuning methods are prone to overfitting, degraded generalization, and unnatural language generation. To address these limitations, we introduce Consistent In-Context... | Bangcheng Yang, Jiaqi Li, Kailin Jiang, Siyuan Qi, Xiaobo Wang, Yaodong Yang, Yifan Zhong, Zilong Zheng |  |
| 3497 |  |  [CL-DiffPhyCon: Closed-loop Diffusion Control of Complex Physical Systems](https://openreview.net/forum?id=PiHGrTTnvb) |  | 0 | The control problems of complex physical systems have broad applications in science and engineering. Previous studies have shown that generative control methods based on diffusion models offer significant advantages for solving these problems. However, existing generative control approaches face... | Dixia Fan, Haodong Feng, Long Wei, Peiyan Hu, Ruiqi Feng, Tailin Wu, Tao Zhang, Xiang Zheng, Yuchen Yang |  |
| 3498 |  |  [OSTQuant: Refining Large Language Model Quantization with Orthogonal and Scaling Transformations for Better Distribution Fitting](https://openreview.net/forum?id=rAcgDBdKnP) |  | 0 | Post-training quantization (PTQ) has emerged as a widely adopted technique for compressing and accelerating Large Language Models (LLMs). The major challenge in LLM quantization is that uneven and heavy-tailed data distributions can expand the quantization range, thereby reducing bit precision for... | Chen Xu, Dawei Yang, Jiangyong Yu, Sifan Zhou, Xing Hu, Yuan Cheng, Zhe Jiang, Zhihang Yuan, Zhixuan Chen, Zukang Xu |  |
| 3499 |  |  [Distribution Backtracking Builds A Faster Convergence Trajectory for Diffusion Distillation](https://openreview.net/forum?id=2ySt3cdGfJ) |  | 0 | Accelerating the sampling speed of diffusion models remains a significant challenge. Recent score distillation methods distill a heavy teacher model into a student generator to achieve one-step generation, which is optimized by calculating the difference between two score functions on the samples... | An Zhao, Changyuan Yang, Chenye Meng, Guang Yang, Ling Yang, Lingyun Sun, Shengyuan Zhang, Zejian Li, Zhiyuan Yang |  |
| 3500 |  |  [Pursuing Better Decision Boundaries for Long-Tailed Object Detection via Category Information Amount](https://openreview.net/forum?id=LW55JrLYPg) |  | 0 | In object detection, the number of instances is commonly used to determine whether a dataset follows a long-tailed distribution, implicitly assuming that the model will perform poorly on categories with fewer instances. This assumption has led to extensive research on category bias in datasets with... | Jiayi Chen, Wei Dai, Yanbiao Ma |  |
| 3501 |  |  [ShortcutsBench: A Large-Scale Real-world Benchmark for API-based Agents](https://openreview.net/forum?id=kKILfPkhSz) |  | 0 | Recent advancements in integrating large language models (LLMs) with application programming interfaces (APIs) have gained significant interest in both academia and industry. Recent work demonstrates that these API-based agents exhibit relatively strong autonomy and planning capabilities. However,... | Desong Meng, Dongqi Cai, Haiyang Shen, Li Zhang, Mengwei Xu, Sheng Qi, Yue Li, Yun Ma |  |
| 3502 |  |  [Sitcom-Crafter: A Plot-Driven Human Motion Generation System in 3D Scenes](https://openreview.net/forum?id=FvIASa0tau) |  | 0 | Recent advancements in human motion synthesis have focused on specific types of motions, such as human-scene interaction, locomotion or human-human interaction, however, there is a lack of a unified system capable of generating a diverse combination of motion types. In response, we introduce... | Jianqi Chen, Michael Kampffmeyer, Panwen Hu, Xiaodan Liang, Xiaojun Chang, Zhenwei Shi |  |
| 3503 |  |  [Gap Preserving Distillation by Building Bidirectional Mappings with A Dynamic Teacher](https://openreview.net/forum?id=PnfghHD4Pi) |  | 0 | Knowledge distillation aims to transfer knowledge from a large teacher model to a compact student counterpart, often coming with a significant performance gap between them. Interestingly, we find that a too-large performance gap can hamper the training process. To alleviate this, we propose a... | Haolin Pan, Jian Chen, Jing Liu, Shulian Zhang, Yong Guo, Yulun Zhang |  |
| 3504 |  |  [Bridging Information Asymmetry in Text-video Retrieval: A Data-centric Approach](https://openreview.net/forum?id=Tn6lrFbiP4) |  | 0 | As online video content rapidly grows, the task of text-video retrieval (TVR) becomes increasingly important. A key challenge in TVR is the information asymmetry between video and text: videos are inherently richer in information, while their textual descriptions often capture only fragments of... | Mike Zheng Shou, Pichao Wang, Thomas Brox, Tianjun Xiao, Tong He, Zechen Bai, Zheng Zhang |  |
| 3505 |  |  [Streaming Video Question-Answering with In-context Video KV-Cache Retrieval](https://openreview.net/forum?id=8g9fs6mdEG) |  | 0 | We propose ReKV, a novel training-free approach that enables efficient streaming video question-answering (StreamingVQA), by seamlessly integrating with existing Video Large Language Models (Video-LLMs). Traditional VideoQA systems struggle with long videos, as they must process entire videos... | Bolin Li, Fangxun Shu, Guanghao Zhang, Hao Cheng, Hao Jiang, Haoyuan Li, Shangzhe Di, Tao Zhong, Wanggui He, Zhelun Yu |  |
| 3506 |  |  [PuzzleFusion++: Auto-agglomerative 3D Fracture Assembly by Denoise and Verify](https://openreview.net/forum?id=7E7v5mJnfl) |  | 0 | This paper proposes a novel “auto-agglomerative” 3D fracture assembly method, PuzzleFusion++, resembling how humans solve challenging spatial puzzles. Starting from individual fragments, the approach 1) aligns and merges fragments into larger groups akin to agglomerative clustering and 2) repeats... | Jiacheng Chen, Yasutaka Furukawa, Zhengqing Wang |  |
| 3507 |  |  [Ready-to-React: Online Reaction Policy for Two-Character Interaction Generation](https://openreview.net/forum?id=mm0cqJ2O3f) |  | 0 | This paper addresses the task of generating two-character online interactions. Previously, two main settings existed for two-character interaction generation: (1) generating one's motions based on the counterpart's complete motion sequence, and (2) jointly generating two-character motions based on... | Huaijin Pi, Hujun Bao, Qing Shuai, Ruizhen Hu, Sida Peng, Xiaowei Zhou, Yujun Shen, Zhi Cen |  |
| 3508 |  |  [Tight Time Complexities in Parallel Stochastic Optimization with Arbitrary Computation Dynamics](https://openreview.net/forum?id=cUN8lJB4rD) |  | 0 | In distributed stochastic optimization, where parallel and asynchronous methods are employed, we establish optimal time complexities under virtually any computation behavior of workers/devices/CPUs/GPUs, capturing potential disconnections due to hardware and network delays, time-varying computation... | Alexander Tyurin |  |
| 3509 |  |  [MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks](https://openreview.net/forum?id=2rWbKbmOuM) |  | 0 | We present MEGA-Bench, an evaluation suite that scales multimodal evaluation to over 500 real-world tasks, to address the highly heterogeneous daily use cases of end users. Our objective is to optimize for a set of high-quality data samples that cover a highly diverse and rich set of multimodal... | Bohan Lyu, Dongfu Jiang, Hexiang Hu, Jiacheng Chen, Kai Wang, Sherman Siu, Tianhao Liang, Wang Zhu, Wenhu Chen, Xiang Yue, Xuan He, Yuan Liu, Yuansheng Ni, Yubo Wang, Zhengqing Wang, Ziyan Jiang |  |
| 3510 |  |  [Rectified Diffusion: Straightness Is Not Your Need in Rectified Flow](https://openreview.net/forum?id=nEDToD1R8M) |  | 0 | Diffusion models have greatly improved visual generation but are hindered by slow generation speed due to the computationally intensive nature of solving generative ODEs. Rectified flow, a widely recognized solution, improves generation speed by straightening the ODE path. Its key components... | FuYun Wang, Hongsheng Li, Ling Yang, Mengdi Wang, Zhaoyang Huang |  |
| 3511 |  |  [Diffusion-NPO: Negative Preference Optimization for Better Preference Aligned Generation of Diffusion Models](https://openreview.net/forum?id=iJi7nz5Cxc) |  | 0 | Diffusion models have made substantial advances in image generation, yet models trained on large, unfiltered datasets often yield outputs misaligned with human preferences. Numerous methods have already been proposed to fine-tune pre-trained diffusion models, achieving notable improvements in... | FuYun Wang, Hongsheng Li, Jingtan Piao, Keqiang Sun, Yunhao Shui |  |
| 3512 |  |  [Asymmetric Factorized Bilinear Operation for Vision Transformer](https://openreview.net/forum?id=MJyqwBVgMs) |  | 0 | As a core component of Transformer-like deep architectures, a feed-forward network (FFN) for channel mixing is responsible for learning features of each token. Recent works show channel mixing can be enhanced by increasing computational burden or can be slimmed at the sacrifice of performance.... | Jiangtao Xie, Junjie Wu, Pengfei Zhu, Qilong Wang, Qinghua Hu |  |
| 3513 |  |  [Et-Seed: Efficient trajectory-Level SE(3) equivariant diffusion Policy](https://openreview.net/forum?id=OheAR2xrtb) |  | 0 | Imitation learning, e.g., diffusion policy, has been proven effective in various robotic manipulation tasks. However, extensive demonstrations are required for policy robustness and generalization. To reduce the demonstration reliance, we leverage spatial symmetry and propose ET-SEED, an efficient... | Boxuan Dong, Chenrui Tie, Chongkai Gao, Hao Dong, Ruihai Wu, Yue Chen, Zeyi Li |  |
| 3514 |  |  [An Optimal Discriminator Weighted Imitation Perspective for Reinforcement Learning](https://openreview.net/forum?id=9JtG4nN7ql) |  | 0 | We introduce Iterative Dual Reinforcement Learning (IDRL), a new method that takes an optimal discriminator-weighted imitation view of solving RL. Our method is motivated by a simple experiment in which we find training a discriminator using the offline dataset plus an additional expert dataset and... | Amy Zhang, Haoran Xu, Harshit Sikchi, Scott Niekum, Shuozhe Li |  |
| 3515 |  |  [DICE: End-to-end Deformation Capture of Hand-Face Interactions from a Single Image](https://openreview.net/forum?id=rfrtFwnF62) |  | 0 | Reconstructing 3D hand-face interactions with deformations from a single image is a challenging yet crucial task with broad applications in AR, VR, and gaming. The challenges stem from self-occlusions during single-view hand-face interactions, diverse spatial relationships between hands and face,... | Chen Wang, Cheng Lin, Christian Theobalt, Lingjie Liu, Qingxuan Wu, Sirui Xu, Soshi Shimada, Taku Komura, Vladislav Golyanik, Wenping Wang, Yuan Liu, Zeyu Cao, Zhengming Yu, Zhiyang Dou |  |
| 3516 |  |  [CatVTON: Concatenation Is All You Need for Virtual Try-On with Diffusion Models](https://openreview.net/forum?id=jt1h2dnmng) |  | 0 | Virtual try-on methods based on diffusion models achieve realistic effects but often require additional encoding modules, a large number of training parameters, and complex preprocessing, which increases the burden on training and inference. In this work, we re-evaluate the necessity of additional... | Dongmei Jiang, Hanqing Zhao, Haoxiang Li, Shiyue Zhang, Wenqing Zhang, Xiao Dong, Xiaodan Liang, Xujie Zhang, Zheng Chong |  |
| 3517 |  |  [Hessian-Free Online Certified Unlearning](https://openreview.net/forum?id=C3TrHWanh5) |  | 0 | Machine unlearning strives to uphold the data owners' right to be forgotten by enabling models to selectively forget specific data. Recent advances suggest pre-computing and storing statistics extracted from second-order information and implementing unlearning through Newton-style updates. However,... | Ermin Wei, Meng Zhang, Ming Tang, Xinbao Qiao |  |
| 3518 |  |  [Video-STaR: Self-Training Enables Video Instruction Tuning with Any Supervision](https://openreview.net/forum?id=JYV2hrtFSv) |  | 0 | The performance and reasoning capabilities of Large Multi-modal Models (LMMs) is dependent on the size and quality of their training datasets. However, collecting datasets that support chain-of-thought instruction tuning is highly challenging. Existing video instruction tuning datasets are often... | Idan Szpektor, Orr Zohar, Serena YeungLevy, Xiaohan Wang, Yonatan Bitton |  |
| 3519 |  |  [Field-DiT: Diffusion Transformer on Unified Video, 3D, and Game Field Generation](https://openreview.net/forum?id=w6YS9A78fq) |  | 0 | The probabilistic field models the distribution of continuous functions defined over metric spaces. While these models hold great potential for unifying data generation across various modalities, including images, videos, and 3D geometry, they still struggle with long-context generation beyond... | Kangfu Mei, Mo Zhou, Vishal M. Patel |  |
| 3520 |  |  [Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations](https://openreview.net/forum?id=94kQgWXojH) |  | 0 | We investigate the internal representations of vision-language models (VLMs) to address hallucinations, a persistent challenge despite advances in model size and training. We project VLMs’ internal image representations to their language vocabulary and observe more confident output probabilities on... | Anish Kachinthaya, Nick Jiang, Suzanne Petryk, Yossi Gandelsman |  |
| 3521 |  |  [Robustness of Quantum Algorithms for Nonconvex Optimization](https://openreview.net/forum?id=JyQYYjtO88) |  | 0 | In this paper, we systematically study quantum algorithms for finding an $\epsilon$-approximate second-order stationary point ($\epsilon$-SOSP) of a $d$-dimensional nonconvex function, a fundamental problem in nonconvex optimization, with noisy zeroth- or first-order oracles as inputs. We first... | Chenyi Zhang, Tongyang Li, Weiyuan Gong |  |
| 3522 |  |  [Fourier Head: Helping Large Language Models Learn Complex Probability Distributions](https://openreview.net/forum?id=4hPwLg7zD3) |  | 0 | As the quality of large language models has improved, there has been increased interest in using them to model non-linguistic tokens. For example, the Decision Transformer recasts agentic decision making as a sequence modeling problem, using a decoder-only LLM to model the distribution over the... | Chen Sun, Daksh Aggarwal, Michael Freeman, Nate Gillman |  |
| 3523 |  |  [Locality Sensitive Avatars From Video](https://openreview.net/forum?id=SVta2eQNt3) |  | 0 | We present locality-sensitive avatar, a neural radiance field (NeRF) based network to learn human motions from monocular videos. To this end, we estimate a canonical representation between different frames of a video with a non-linear mapping from observation to canonical space, which we decompose... | Bastian Wandt, Chunjin Song, Helge Rhodin, Leonid Sigal, ShihYang Su, Zhijie Wu |  |
| 3524 |  |  [Unhackable Temporal Reward for Scalable Video MLLMs](https://openreview.net/forum?id=Gf1uBeuUJW) |  | 0 | In the pursuit of superior video-processing MLLMs, we have encountered a perplexing paradox: the “anti-scaling law”, where more data and larger models lead to worse performance. This study unmasks the culprit: “temporal hacking”, a phenomenon where models shortcut by fixating on select frames,... | En Yu, Haoran Wei, Jianjian Sun, Jingyu Wang, Kangheng Lin, Liang Zhao, Wenbing Tao, Xiangyu Zhang, Yana Wei, Zheng Ge, Zining Zhu |  |
| 3525 |  |  [Understanding Long Videos with Multimodal Language Models](https://openreview.net/forum?id=OxKi02I29I) |  | 0 | Large Language Models (LLMs) have allowed recent LLM-based approaches to achieve excellent performance on long-video understanding benchmarks. We investigate how extensive world knowledge and strong reasoning skills of underlying LLMs influence this strong performance. Surprisingly, we discover... | Kanchana Ranasinghe, Kumara Kahatapitiya, Michael S. Ryoo, Xiang Li |  |
| 3526 |  |  [Vec2Face: Scaling Face Dataset Generation with Loosely Constrained Vectors](https://openreview.net/forum?id=RoN6NnHjn4) |  | 0 | This paper studies how to synthesize face images of non-existent persons, to create a dataset that allows effective training of face recognition (FR) models. Besides generating realistic face images, two other important goals are: 1) the ability to generate a large number of distinct identities... | Haiyu Wu, Jaskirat Singh, Kevin W. Bowyer, Liang Zheng, Sicong Tian |  |
| 3527 |  |  [Discrete Codebook World Models for Continuous Control](https://openreview.net/forum?id=lfRYzd8ady) |  | 0 | In reinforcement learning (RL), world models serve as internal simulators, enabling agents to predict environment dynamics and future outcomes in order to make informed decisions. While previous approaches leveraging discrete latent spaces, such as DreamerV3, have demonstrated strong performance in... | Aidan Scannell, Arno Solin, Joni Pajarinen, Kalle Kujanpää, Kevin Sebastian Luck, Mohammadreza Nakhaeinezhadfard, Yi Zhao |  |
| 3528 |  |  [Connecting Federated ADMM to Bayes](https://openreview.net/forum?id=ipQrjRsl11) |  | 0 | We provide new connections between two distinct federated learning approaches based on (i) ADMM and (ii) Variational Bayes (VB), and propose new variants by combining their complementary strengths. Specifically, we show that the dual variables in ADMM naturally emerge through the "site" parameters... | Finale DoshiVelez, Mohammad Emtiyaz Khan, Siddharth Swaroop |  |
| 3529 |  |  [Robots Pre-train Robots: Manipulation-Centric Robotic Representation from Large-Scale Robot Datasets](https://openreview.net/forum?id=yTEwmr1TJb) |  | 0 | The pre-training of visual representations has enhanced the efficiency of robot learning. Due to the lack of large-scale in-domain robotic datasets, prior works utilize in-the-wild human videos to pre-train robotic visual representation. Despite their promising results, representations from human... | Guangqi Jiang, Huanyu Li, Huazhe Xu, Tao Huang, Yifei Sun, Yongyuan Liang |  |
| 3530 |  |  [TimeSuite: Improving MLLMs for Long Video Understanding via Grounded Tuning](https://openreview.net/forum?id=nAVejJURqZ) |  | 0 | Multimodal Large Language Models (MLLMs) have demonstrated impressive performance in short video understanding. However, understanding long-form videos still remains challenging for MLLMs. This paper proposes TimeSuite, a collection of new designs to adapt the existing short-form video MLLMs for... | Chenting Wang, Kunchang Li, Limin Wang, Songze Li, Tianxiang Jiang, Xiangyu Zeng, Xinhao Li, Yali Wang, Yansong Shi, Yi Wang, Yu Qiao, Zhengrong Yue, Ziang Yan |  |
| 3531 |  |  [CLIPure: Purification in Latent Space via CLIP for Adversarially Robust Zero-Shot Classification](https://openreview.net/forum?id=TQ2ZOy6miT) |  | 0 | In this paper, we aim to build an adversarially robust zero-shot image classifier that can accurately and efficiently classify unseen examples while defending against unforeseen adversarial attacks, addressing critical challenges in real-world safety-sensitive scenarios. To achieve this, we focus... | Jiafeng Guo, Keping Bi, Mingkun Zhang, Wei Chen, Xueqi Cheng |  |
| 3532 |  |  [Solving Video Inverse Problems Using Image Diffusion Models](https://openreview.net/forum?id=TRWxFUzK9K) |  | 0 | Recently, diffusion model-based inverse problem solvers (DIS) have emerged as state-of-the-art approaches for addressing inverse problems, including image super-resolution, deblurring, inpainting, etc. However, their application to video inverse problems arising from spatio-temporal degradation... | Jong Chul Ye, Taesung Kwon |  |
| 3533 |  |  [Jailbreaking as a Reward Misspecification Problem](https://openreview.net/forum?id=uBnM3EFovQ) |  | 0 | The widespread adoption of large language models (LLMs) has raised concerns about their safety and reliability, particularly regarding their vulnerability to adversarial attacks. In this paper, we propose a new perspective that attributes this vulnerability to reward misspecification during the... | Jiahui Gao, Lei Li, Lingpeng Kong, Qi Liu, Zhenguo Li, Zhihui Xie |  |
| 3534 |  |  [Streaming Video Understanding and Multi-round Interaction with Memory-enhanced Knowledge](https://openreview.net/forum?id=JbPb6RieNC) |  | 0 | Recent advances in Large Language Models (LLMs) have enabled the development of Video-LLMs, advancing multimodal learning by bridging video data with language tasks. However, current video understanding models struggle with processing long video sequences, supporting multi-turn dialogues, and... | Haomiao Xiong, Huchuan Lu, Jiawen Zhu, Jiazuo Yu, Lu Zhang, Yunzhi Zhuge, Zongxin Yang |  |
| 3535 |  |  [Beyond the convexity assumption: Realistic tabular data generation under quantifier-free real linear constraints](https://openreview.net/forum?id=rx0TCew0Lj) |  | 0 | Synthetic tabular data generation has traditionally been a challenging problem due to the high complexity of the underlying distributions that characterise this type of data. Despite recent advances in deep generative models (DGMs), existing methods often fail to produce realistic datapoints that... | Eleonora Giunchiglia, Mihaela C. Stoian |  |
| 3536 |  |  [Large Scale Knowledge Washing](https://openreview.net/forum?id=dXCpPgjTtd) |  | 0 | Large language models show impressive abilities in memorizing world knowledge, which leads to concerns regarding memorization of private information, toxic or sensitive knowledge, and copyrighted content. We introduce the problem of Large Scale Knowledge Washing, focusing on unlearning an extensive... | Julian J. McAuley, Ruihan Wu, Xiusi Chen, Yu Wang, Zexue He |  |
| 3537 |  |  [Self-Updatable Large Language Models by Integrating Context into Model Parameters](https://openreview.net/forum?id=aCPFCDL9QY) |  | 0 | Despite significant advancements in large language models (LLMs), the rapid and frequent integration of small-scale experiences, such as interactions with sur- rounding objects, remains a substantial challenge. Two critical factors in assimilating these experiences are (1) \*\*Efficacy\*\*: the... | Julian J. McAuley, Junda Wu, Sean O'Brien, Xinshuang Liu, Xiusi Chen, Yu Wang |  |
| 3538 |  |  [Process Reward Model with Q-value Rankings](https://openreview.net/forum?id=wQEdh2cgEk) |  | 0 | Process Reward Modeling (PRM) is critical for complex reasoning and decision-making tasks where the accuracy of intermediate steps significantly influences the overall outcome. Existing PRM approaches, primarily framed as classification problems, employ cross-entropy loss to independently evaluate... | Wendi Li, Yixuan Li |  |
| 3539 |  |  [PostCast: Generalizable Postprocessing for Precipitation Nowcasting via Unsupervised Blurriness Modeling](https://openreview.net/forum?id=v2zcCDYMok) |  | 0 | Precipitation nowcasting plays a pivotal role in socioeconomic sectors, especially in severe convective weather warnings. Although notable progress has been achieved by approaches mining the spatiotemporal correlations with deep learning, these methods still suffer severe blurriness as the lead... | Ben Fei, Junchao Gong, Kun Chen, Lei Bai, Siwei Tu, Wanli Ouyang, Weidong Yang, Wenlong Zhang, Xiaokang Yang |  |
| 3540 |  |  [ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation](https://openreview.net/forum?id=sGpCzsfd1K) |  | 0 | We introduce a new benchmark, ChartMimic, aimed at assessing the visually-grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart... | Bo Shui, Cheng Yang, Chufan Shi, Deng Cai, Gongye Liu, Junjie Wang, Linran Xu, Mohan Jing, Siheng Li, Xiaomei Nie, Xinyu Zhu, Yaxin Liu, Yujiu Yang, Yuxiang Zhang |  |
| 3541 |  |  [Does Training with Synthetic Data Truly Protect Privacy?](https://openreview.net/forum?id=C8niXBHjfO) |  | 0 | As synthetic data becomes increasingly popular in machine learning tasks, numerous methods---without formal differential privacy guarantees---use synthetic data for training. These methods often claim, either explicitly or implicitly, to protect the privacy of the original training data. In this... | Jie Zhang, Yunpeng Zhao |  |
| 3542 |  |  [PT-T2I/V: An Efficient Proxy-Tokenized Diffusion Transformer for Text-to-Image/Video-Task](https://openreview.net/forum?id=lTrrnNdkOX) |  | 0 | The global self-attention mechanism in diffusion transformers involves redundant computation due to the sparse and redundant nature of visual information, and the attention map of tokens within a spatial window shows significant similarity. To address this redundancy, we propose the Proxy-Tokenized... | Ao Ma, Dawei Leng, Jiasong Feng, Jing Wang, Xiaodan Liang, Yuhui Yin |  |
| 3543 |  |  [Fast Feedforward 3D Gaussian Splatting Compression](https://openreview.net/forum?id=DCandSZ2F1) |  | 0 | With 3D Gaussian Splatting (3DGS) advancing real-time and high-fidelity rendering for novel view synthesis, storage requirements pose challenges for their widespread adoption. Although various compression techniques have been proposed, previous art suffers from a common limitation: for any existing... | Jianfei Cai, Mehrtash Harandi, Mengyao Li, Qianyi Wu, Weiyao Lin, Yihang Chen |  |
| 3544 |  |  [Scalable Benchmarking and Robust Learning for Noise-Free Ego-Motion and 3D Reconstruction from Noisy Video](https://openreview.net/forum?id=Pz9zFea4MQ) |  | 0 | We aim to redefine robust ego-motion estimation and photorealistic 3D reconstruction by addressing a critical limitation: the reliance on noise-free data in existing models. While such sanitized conditions simplify evaluation, they fail to capture the unpredictable, noisy complexities of real-world... | Bhiksha Raj, Matthew JohnsonRoberson, Sebastian A. Scherer, Shibo Zhao, Sibo Wang, Tianyi Zhang, Xiang Li, Xiaohao Xu, Xiaonan Huang, Ye Li, Yongqi Chen |  |
| 3545 |  |  [LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning](https://openreview.net/forum?id=LYawG8YkPa) |  | 0 | Language plays a vital role in the realm of human motion. Existing methods have largely depended on CLIP text embeddings for motion generation, yet they fall short in effectively aligning language and motion due to CLIP’s pretraining on static image-text pairs. This work introduces LaMP, a novel... | Laurence Tianruo Yang, Lingteng Qiu, Shenhao Zhu, Weichao Shen, Weihao Yuan, Xiaodong Gu, Yisheng He, Yuan Dong, Zhe Li, Zilong Dong |  |
| 3546 |  |  [Generating Physical Dynamics under Priors](https://openreview.net/forum?id=eNjXcP6C0H) |  | 0 | Generating physically feasible dynamics in a data-driven context is challenging, especially when adhering to physical priors expressed in specific equations or formulas. Existing methodologies often overlook the integration of ''physical priors'', resulting in violation of basic physical laws and... | Tianshu Yu, Xiaoxue Wang, Zihan Zhou |  |
| 3547 |  |  [Denoising with a Joint-Embedding Predictive Architecture](https://openreview.net/forum?id=d4njmzM7jf) |  | 0 | Joint-embedding predictive architectures (JEPAs) have shown substantial promise in self-supervised representation learning, yet their application in generative modeling remains underexplored. Conversely, diffusion models have demonstrated significant efficacy in modeling arbitrary probability... | Dengsheng Chen, Enhua Wu, Jie Hu, Xiaoming Wei |  |
| 3548 |  |  [Hyper-Connections](https://openreview.net/forum?id=9FqARW7dwB) |  | 0 | We present hyper-connections, a simple yet effective method that can serve as an alternative to residual connections. This approach specifically addresses common drawbacks observed in residual connection variants, such as the seesaw effect between gradient vanishing and representation collapse.... | Banggu Wu, Defa Zhu, Hongzhi Huang, Qiyang Min, Xun Zhou, Yunyao Mao, Yutao Zeng, Zihao Huang |  |
| 3549 |  |  [Efficient Masked AutoEncoder for Video Object Counting and A Large-Scale Benchmark](https://openreview.net/forum?id=sY3anJ8C68) |  | 0 | The dynamic imbalance of the fore-background is a major challenge in video object counting, which is usually caused by the sparsity of target objects. This remains understudied in existing works and often leads to severe under-/over-prediction errors. To tackle this issue in video object counting,... | Bing Cao, Jiekang Feng, Pengfei Zhu, Qilong Wang, Qinghua Hu, Quanhao Lu |  |
| 3550 |  |  [TS-LIF: A Temporal Segment Spiking Neuron Network for Time Series Forecasting](https://openreview.net/forum?id=rDe9yQQYKt) |  | 0 | Spiking Neural Networks (SNNs) offer a promising, biologically inspired approach for processing spatiotemporal data, particularly for time series forecasting. However, conventional neuron models like the Leaky Integrate-and-Fire (LIF) struggle to capture long-term dependencies and effectively... | Peilin Zhao, Shibo Feng, Wanjin Feng, Xingyu Gao, Zhiqi Shen |  |
| 3551 |  |  [ETA: Evaluating Then Aligning Safety of Vision Language Models at Inference Time](https://openreview.net/forum?id=QoDDNkx4fP) |  | 0 | Vision Language Models (VLMs) have become essential backbones for multi-modal intelligence, yet significant safety challenges limit their real-world application. While textual inputs can often be effectively safeguarded, adversarial visual inputs can often easily bypass VLM defense mechanisms.... | Bolian Li, Ruqi Zhang, Yi Ding |  |
| 3552 |  |  [Ranking-aware adapter for text-driven image ordering with CLIP](https://openreview.net/forum?id=KbCh7zbw2K) |  | 0 | Recent advances in vision-language models (VLMs) have made significant progress in downstream tasks that require quantitative concepts such as facial age estimation and image quality assessment, enabling VLMs to explore applications like image ranking and retrieval. However, existing studies... | MingHsuan Yang, WeiHsiang Yu, YenYu Lin, YiHsuan Tsai |  |
| 3553 |  |  [ReMatching Dynamic Reconstruction Flow](https://openreview.net/forum?id=bwhI6bCGY1) |  | 0 | Reconstructing a dynamic scene from image inputs is a fundamental computer vision task with many downstream applications. Despite recent advancements, existing approaches still struggle to achieve high-quality reconstructions from unseen viewpoints and timestamps. This work introduces the... | Despoina Paschalidou, Matan Atzmon, Sanja Fidler, Sara Oblak |  |
| 3554 |  |  [Analyzing and Boosting the Power of Fine-Grained Visual Recognition for Multi-modal Large Language Models](https://openreview.net/forum?id=p3NKpom1VL) |  | 0 | Multi-modal large language models (MLLMs) have shown remarkable abilities in various visual understanding tasks. However, MLLMs still struggle with fine-grained visual recognition (FGVR), which aims to identify subordinate-level categories from images. This can negatively impact more advanced... | Geng Li, Hulingxiao He, Jinglin Xu, Yuxin Peng, Zijun Geng |  |
| 3555 |  |  [γ-MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models](https://openreview.net/forum?id=q44uq3tc2D) |  | 0 | Despite the significant progress in multimodal large language models (MLLMs), their high computational cost remains a barrier to real-world deployment. Inspired by the mixture of depths (MoDs) in natural language processing, we aim to address this limitation from the perspective of \`\`activated... | Gen Luo, Jiayi Ji, Rongrong Ji, Xiaoshuai Sun, Yaxin Luo, Yiyi Zhou, Zhiqiang Shen |  |
| 3556 |  |  [Circuit Representation Learning with Masked Gate Modeling and Verilog-AIG Alignment](https://openreview.net/forum?id=US9k5TXVLZ) |  | 0 | Understanding the structure and function of circuits is crucial for electronic design automation (EDA). Circuits can be formulated as And-Inverter graphs (AIGs), enabling efficient implementation of representation learning through graph neural networks (GNNs). Masked modeling paradigms have been... | Bei Yu, Haisheng Zheng, Haoyuan Wu, Yuan Pu |  |
| 3557 |  |  [AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark](https://openreview.net/forum?id=tTDUrseRRU) |  | 0 | Video detailed captioning is a key task which aims to generate comprehensive and coherent textual descriptions of video content, benefiting both video understanding and generation. In this paper, we propose AuroraCap, a video captioner based on a large multimodal model. We follow the simplest... | Chenlin Meng, Christopher D. Manning, Enxin Song, JenqNeng Hwang, Omer BarTal, Saining Xie, Vashisht Madhavan, Wenhao Chai, Yilun Du |  |
| 3558 |  |  [Diff-2-in-1: Bridging Generation and Dense Perception with Diffusion Models](https://openreview.net/forum?id=ZYd5wJSaMs) |  | 0 | Beyond high-fidelity image synthesis, diffusion models have recently exhibited promising results in dense visual perception tasks. However, most existing work treats diffusion models as a standalone component for perception tasks, employing them either solely for off-the-shelf data augmentation or... | Martial Hebert, Ruoyu Zhao, Shuhong Zheng, YuXiong Wang, Zhipeng Bao |  |
| 3559 |  |  [COME: Test-time Adaption by Conservatively Minimizing Entropy](https://openreview.net/forum?id=506BjJ1ziZ) |  | 0 | Machine learning models must continuously self-adjust themselves for novel data distribution in the open world. As the predominant principle, entropy minimization (EM) has been proven to be a simple yet effective cornerstone in existing test-time adaption (TTA) methods. While unfortunately its... | Changqing Zhang, Peilin Zhao, Qingyang Zhang, Xinke Kong, Yatao Bian |  |
| 3560 |  |  [Interpretable Vision-Language Survival Analysis with Ordinal Inductive Bias for Computational Pathology](https://openreview.net/forum?id=trj2Jq8riA) |  | 0 | Histopathology Whole-Slide Images (WSIs) provide an important tool to assess cancer prognosis in computational pathology (CPATH). While existing survival analysis (SA) approaches have made exciting progress, they are generally limited to adopting highly-expressive network architectures and only... | Bo Fu, Jiaxiang Gou, Luping Ji, Mao Ye, Pei Liu |  |
| 3561 |  |  [SleepSMC: Ubiquitous Sleep Staging via Supervised Multimodal Coordination](https://openreview.net/forum?id=B5VEi5d3p2) |  | 0 | Sleep staging is critical for assessing sleep quality and tracking health. Polysomnography (PSG) provides comprehensive multimodal sleep-related information, but its complexity and impracticality limit its practical use in daily and ubiquitous monitoring. Conversely, unimodal devices offer more... | Hualei Wang, Shuo Ma, Wei Zhang, Yingwei Zhang, Yiqiang Chen, Yuan Jin, Ziyu Jia |  |
| 3562 |  |  [Towards Generalizable Reinforcement Learning via Causality-Guided Self-Adaptive Representations](https://openreview.net/forum?id=bMvqccRmKD) |  | 0 | General intelligence requires quick adaptation across tasks. While existing reinforcement learning (RL) methods have made progress in generalization, they typically assume only distribution changes between source and target domains. In this paper, we explore a wider range of scenarios where not... | Biwei Huang, Fan Feng, Lei Xu, Shikui Tu, Xinyue Wang, Yupei Yang |  |
| 3563 |  |  [From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data](https://openreview.net/forum?id=8m7p4k6Zeb) |  | 0 | Recent studies have shown that Large Language Models (LLMs) struggle to accurately retrieve information and maintain reasoning capabilities when processing long-context inputs. To address these limitations, we propose a finetuning approach utilizing a carefully designed synthetic dataset comprising... | Dimitris Papailiopoulos, Kangwook Lee, Vasilis Papageorgiou, Zheyang Xiong |  |
| 3564 |  |  [Enhancing Document Understanding with Group Position Embedding: A Novel Approach to Incorporate Layout Information](https://openreview.net/forum?id=Dj9a4zQsSl) |  | 0 | Recent advancements in document understanding have been dominated by leveraging large language models (LLMs) and multimodal large models. However, enabling LLMs to comprehend complex document layouts and structural information often necessitates intricate network modifications or costly... | Bo Zheng, Chi Xie, Dongdong Liu, Sheng Guo, Yue Zhang, Yuke Zhu, Zihua Xiong |  |
| 3565 |  |  [Towards Realistic UAV Vision-Language Navigation: Platform, Benchmark, and Methodology](https://openreview.net/forum?id=rUvCIvI4eB) |  | 0 | Developing agents capable of navigating to a target location based on language instructions and visual information, known as vision-language navigation (VLN), has attracted widespread interest. Most research has focused on ground-based agents, while UAV-based VLN remains relatively underexplored.... | Donglin Yang, Hohin Kwan, Hongsheng Li, Jinyu Chen, Si Liu, Wenjun Wu, Xiangyu Wang, Yue Liao, Ziqin Wang |  |
| 3566 |  |  [Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in Vision-Language Alignment](https://openreview.net/forum?id=cJQ1K2fjpD) |  | 0 | The recent advancements in large language models (LLMs) and pre-trained vision models have accelerated the development of vision-language large models (VLLMs), enhancing the interaction between visual and linguistic modalities. Despite their notable success across various domains, VLLMs face... | An Zhang, Chenhang Cui, Gelei Deng, Huaxiu Yao, TatSeng Chua, Yiyang Zhou, Zhaorun Chen |  |
| 3567 |  |  [Minimal Impact ControlNet: Advancing Multi-ControlNet Integration](https://openreview.net/forum?id=rzbSNDXgGD) |  | 0 | With the advancement of diffusion models, there is a growing demand for high-quality, controllable image generation, particularly through methods that utilize one or multiple control signals based on ControlNet. However, in current ControlNet training, each control is designed to influence all... | Bo Zheng, Jia Jia, Junliang Xing, Min Zhou, Shikun Sun, Tiezheng Ge, Xiaoyu Qin, Xubin Li, Zijie Ye, Zixuan Wang |  |
| 3568 |  |  [TASAR: Transfer-based Attack on Skeletal Action Recognition](https://openreview.net/forum?id=I393kV3bz4) |  | 0 | Skeletal sequence data, as a widely employed representation of human actions, are crucial in Human Activity Recognition (HAR). Recently, adversarial attacks have been proposed in this area, which exposes potential security concerns, and more importantly provides a good tool for model robustness... | Ajian Liu, Baiqi Wu, He Wang, Meng Wang, Ruixuan Zhang, Xiaoshuai Hao, Xingxing Wei, Yunfeng Diao |  |
| 3569 |  |  [HiLo: A Learning Framework for Generalized Category Discovery Robust to Domain Shifts](https://openreview.net/forum?id=2eFq6S35iB) |  | 0 | Generalized Category Discovery (GCD) is a challenging task in which, given a partially labelled dataset, models must categorize all unlabelled instances, regardless of whether they come from labelled categories or from new ones. In this paper, we challenge a remaining assumption in this task: that... | Hongjun Wang, Kai Han, Sagar Vaze |  |
| 3570 |  |  [LoRanPAC: Low-rank Random Features and Pre-trained Models for Bridging Theory and Practice in Continual Learning](https://openreview.net/forum?id=bqv7M0wc4x) |  | 0 | The goal of continual learning (CL) is to train a model that can solve multiple tasks presented sequentially. Recent CL approaches have achieved strong performance by leveraging large pre-trained models that generalize well to downstream tasks. However, such methods lack theoretical guarantees,... | Alejandro Ribeiro, Joshua Agterberg, Juan Elenter, Liangzu Peng, René Vidal |  |
| 3571 |  |  [I Can Hear You: Selective Robust Training for Deepfake Audio Detection](https://openreview.net/forum?id=2GcR9bO620) |  | 0 | Recent advances in AI-generated voices have intensified the challenge of detecting deepfake audio, posing risks for scams and the spread of disinformation. To tackle this issue, we establish the largest public voice dataset to date, named DeepFakeVox-HQ, comprising 1.3 million samples, including... | Aroon Sankoh, Chengzhi Mao, Emanuel MendiolaOrtiz, Junfeng Yang, Wei Hao, William Lin, Zirui Zhang |  |
| 3572 |  |  [CoInD: Enabling Logical Compositions in Diffusion Models](https://openreview.net/forum?id=cCRlEvjrx4) |  | 0 | How can we learn generative models to sample data with arbitrary logical compositions of statistically independent attributes? The prevailing solution is to sample from distributions expressed as a composition of attributes' conditional marginal distributions under the assumption that they are... | Gautam Sreekumar, Sachit Gaudi, Vishnu Boddeti |  |
| 3573 |  |  [HAMSTER: Hierarchical Action Models for Open-World Robot Manipulation](https://openreview.net/forum?id=h7aQxzKbq6) |  | 0 | Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is the lack of robotic data, which are typically obtained through expensive on-robot... | Abhishek Gupta, Ankit Goyal, Anqi Li, Caelan Reed Garrett, Dieter Fox, Fabio Ramos, Jesse Zhang, Joel Jang, Marius Memmel, Yi Li, Yuquan Deng |  |
| 3574 |  |  [Mitigating Object Hallucination in MLLMs via Data-augmented Phrase-level Alignment](https://openreview.net/forum?id=yG1fW8igzP) |  | 0 | Despite their significant advancements, Multimodal Large Language Models (MLLMs) often generate factually inaccurate information, referred to as hallucination. In this work, we address object hallucinations in MLLMs, where information is generated about an object not present in the input image. We... | Ahmad Beirami, Ali Etemad, Pritam Sarkar, Sayna Ebrahimi, Sercan Ö. Arik, Tomas Pfister |  |
| 3575 |  |  [Breaking Neural Network Scaling Laws with Modularity](https://openreview.net/forum?id=5Qxx5KpFms) |  | 0 | Modular neural networks outperform nonmodular neural networks on tasks ranging from visual question answering to robotics. These performance improvements are thought to be due to modular networks' superior ability to model the compositional and combinatorial structure of real-world problems.... | Abhiram Iyer, Akhilan Boopathy, Ila R. Fiete, Jaedong Hwang, Sunshine Jiang, William Yue |  |
| 3576 |  |  [Automated Design of Agentic Systems](https://openreview.net/forum?id=t9U3LW7JVX) |  | 0 | Researchers are investing substantial effort in developing powerful general-purpose agents, wherein Foundation Models are used as modules within agentic systems (e.g. Chain-of-Thought, Self-Reflection, Toolformer). However, the history of machine learning teaches us that hand-designed solutions are... | Cong Lu, Jeff Clune, Shengran Hu |  |
| 3577 |  |  [STORM: Spatio-TempOral Reconstruction Model For Large-Scale Outdoor Scenes](https://openreview.net/forum?id=M2NFWRPMUd) |  | 0 | We present STORM, a spatio-temporal reconstruction model designed for reconstructing dynamic outdoor scenes from sparse observations. Existing dynamic reconstruction methods often rely on per-scene optimization, dense observations across space and time, and strong motion supervision, resulting in... | Apoorva Sharma, Boris Ivanovic, Boyi Li, Danfei Xu, Jiahui Huang, Jiawei Yang, Marco Pavone, Maximilian Igl, Péter Karkus, Yan Wang, Yue Wang, Yurong You, Yuxiao Chen |  |
| 3578 |  |  [Training Language Models on Synthetic Edit Sequences Improves Code Synthesis](https://openreview.net/forum?id=AqfUa08PCH) |  | 0 | Software engineers mainly write code by editing existing programs. In contrast, language models (LMs) autoregressively synthesize programs in a single pass. One explanation for this is the scarcity of sequential edit data. While high-quality instruction data for code synthesis is scarce, edit data... | Lerrel Pinto, Rob Fergus, Ulyana Piterbarg |  |
| 3579 |  |  [Learning under Temporal Label Noise](https://openreview.net/forum?id=5o0phqAhsP) |  | 0 | Many time series classification tasks, where labels vary over time, are affected by label noise that also varies over time. Such noise can cause label quality to improve, worsen, or periodically change over time. We first propose and formalize temporal label noise, an unstudied problem for... | Anna Goldenberg, Berk Ustun, Sana Tonekaboni, Sujay Nagaraj, Thomas Hartvigsen, Walter Gerych |  |
| 3580 |  |  [Regretful Decisions under Label Noise](https://openreview.net/forum?id=7B9FCDoUzB) |  | 0 | Machine learning models are routinely used to support decisions that affect individuals -- be it to screen a patient for a serious illness or to gauge their response to treatment. In these tasks, we are limited to learning models from datasets with noisy labels. In this paper, we study the... | Berk Ustun, Flávio P. Calmon, Sujay Nagaraj, Yang Liu |  |
| 3581 |  |  [LaGeM: A Large Geometry Model for 3D Representation Learning and Diffusion](https://openreview.net/forum?id=72OSO38a2z) |  | 0 | This paper introduces a novel hierarchical autoencoder that maps 3D models into a highly compressed latent space. The hierarchical autoencoder is specifically designed to tackle the challenges arising from large-scale datasets and generative modeling using diffusion. Different from previous... | Biao Zhang, Peter Wonka |  |
| 3582 |  |  [InterLCM: Low-Quality Images as Intermediate States of Latent Consistency Models for Effective Blind Face Restoration](https://openreview.net/forum?id=rUxr9Ll5FQ) |  | 0 | Diffusion priors have been used for blind face restoration (BFR) by fine-tuning diffusion models (DMs) on restoration datasets to recover low-quality images. However, the naive application of DMs presents several key limitations. (i) The diffusion prior has inferior semantic consistency (e.g., ID,... | ChunLe Guo, Fahad Shahbaz Khan, Jian Yang, Joost van de Weijer, Kai Wang, MingMing Cheng, Senmao Li, Shiqi Yang, Yaxing Wang |  |
| 3583 |  |  [Generalization Bounds for Canonicalization: A Comparative Study with Group Averaging](https://openreview.net/forum?id=n0lXaskyk5) |  | 0 | Canonicalization, a popular method for generating invariant or equivariant function classes from arbitrary function sets, involves initial data projection onto a reduced input space subset, followed by applying any learning method to the projected dataset. Despite recent research on the expressive... | Behrooz Tahmasebi, Stefanie Jegelka |  |
| 3584 |  |  [VibeCheck: Discover and Quantify Qualitative Differences in Large Language Models](https://openreview.net/forum?id=acxHV6werE) |  | 0 | Large language models (LLMs) often exhibit subtle yet distinctive characteristics in their outputs that users intuitively recognize, but struggle to quantify. These "vibes" -- such as tone, formatting, or writing style -- influence user preferences, yet traditional evaluations focus primarily on... | Jacob Steinhardt, Joseph E. Gonzalez, Krishna Mandal, Lisa Dunlap, Trevor Darrell |  |
| 3585 |  |  [3D-Spatial Multimodal Memory](https://openreview.net/forum?id=XYdstv3ySl) |  | 0 | We present 3D Spatial MultiModal Memory (M3), a multimodal memory system designed to retain information about medium-sized static scenes through video sources for visual perception. By integrating 3D Gaussian Splatting techniques with foundation models, M3 builds a multimodal memory capable of... | Jianglong Ye, RiZhao Qiu, Sifei Liu, Xiaolong Wang, Xuanbin Peng, Xueyan Zou, Yuchen Song |  |
| 3586 |  |  [Identification of Intermittent Temporal Latent Process](https://openreview.net/forum?id=6Pz7afmsOp) |  | 0 | Identifying time-delayed latent causal process is crucial for understanding temporal dynamics and enabling downstream reasoning. While recent methods have made progress in identifying latent time-delayed causal processes, they cannot address the dynamics in which the influence of some latent... | Guangyi Chen, Heng Huang, Kun Zhang, Yujia Zheng, Yuke Li |  |
| 3587 |  |  [Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis](https://openreview.net/forum?id=GJsuYHhAga) |  | 0 | We present Meissonic, which elevates non-autoregressive text-to-image Masked Image Modeling (MIM) to a level comparable with state-of-the-art diffusion models like SDXL. By incorporating a comprehensive suite of architectural innovations, advanced positional encoding strategies, and optimized... | Enxin Song, Jinbin Bai, Lei Zhu, QingGuo Chen, Shuicheng Yan, Tian Ye, Wei Chow, Xiangtai Li, Zhen Dong |  |
| 3588 |  |  [AHA: A Vision-Language-Model for Detecting and Reasoning Over Failures in Robotic Manipulation](https://openreview.net/forum?id=JVkdSi7Ekg) |  | 0 | Robotic manipulation in open-world settings requires not only task execution but also the ability to detect and learn from failures. While recent advances in vision-language models (VLMs) and large language models (LLMs) have improved robots' spatial reasoning and problem-solving abilities, they... | Ajay Mandlekar, Dieter Fox, Jiafei Duan, Nishanth Kumar, Ranjay Krishna, Shulin Tian, Wentao Yuan, Wilbert Pumacay, Yi Ru Wang, Yijie Guo |  |
| 3589 |  |  [Shapley-Guided Utility Learning for Effective Graph Inference Data Valuation](https://openreview.net/forum?id=8X74NZpARg) |  | 0 | Graph Neural Networks (GNNs) have demonstrated remarkable performance in various graph-based machine learning tasks, yet evaluating the importance of neighbors of testing nodes remains largely unexplored due to the challenge of assessing data importance without test labels. To address this gap, we... | Hongliang Chi, Qiong Wu, Yao Ma, Zhengyi Zhou |  |
| 3590 |  |  [DreamDistribution: Learning Prompt Distribution for Diverse In-distribution Generation](https://openreview.net/forum?id=oQoQ4u6MQC) |  | 0 | The popularization of Text-to-Image (T2I) diffusion models enables the generation of high-quality images from text descriptions. However, generating diverse customized images with reference visual attributes remains challenging. This work focuses on personalizing T2I diffusion models at a more... | Brian Nlong Zhao, Dongsheng Li, Jiashu Xu, Laurent Itti, Vibhav Vineet, Xinyang Jiang, Yifan Yang, Yuhang Xiao, Yunhao Ge |  |
| 3591 |  |  [Personality Alignment of Large Language Models](https://openreview.net/forum?id=0DZEs8NpUH) |  | 0 | Aligning large language models (LLMs) typically aim to reflect general human values and behaviors, but they often fail to capture the unique characteristics and preferences of individual users. To address this gap, we introduce the concept of Personality Alignment. This approach tailors LLMs'... | Linyi Yang, Minjun Zhu, Yixuan Weng, Yue Zhang |  |
| 3592 |  |  [CycleResearcher: Improving Automated Research via Automated Review](https://openreview.net/forum?id=bjcsVLoHYs) |  | 0 | The automation of scientific discovery has been a long-standing goal within the research community, driven by the potential to accelerate knowledge creation. While significant progress has been made using commercial large language models (LLMs) as research assistants or idea generators, the... | Guangsheng Bao, Hongbo Zhang, Jindong Wang, Linyi Yang, Minjun Zhu, Yixuan Weng, Yue Zhang |  |
| 3593 |  |  [Towards Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It](https://openreview.net/forum?id=6oWFn6fY4A) |  | 0 | Label smoothing (LS) is a popular regularisation method for training neural networks as it is effective in improving test accuracy and is simple to implement. ''Hard'' one-hot labels are ''smoothed'' by uniformly distributing probability mass to other classes, reducing overfitting. Prior work has... | ChristosSavvas Bouganis, Gianni Franchi, Guoxuan Xia, Olivier Laurent |  |
| 3594 |  |  [Towards Improving Exploration through Sibling Augmented GFlowNets](https://openreview.net/forum?id=HH4KWP8RP5) |  | 0 | Exploration is a key factor for the success of an active learning agent, especially when dealing with sparse extrinsic terminal rewards and long trajectories. We introduce Sibling Augmented Generative Flow Networks (SA-GFN), a novel framework designed to enhance exploration and training efficiency... | Alex Lamb, Emmanuel Bengio, Glen Berseth, Kanika Madan, Yoshua Bengio |  |
| 3595 |  |  [Compositional 4D Dynamic Scenes Understanding with Physics Priors for Video Question Answering](https://openreview.net/forum?id=6Vx28LSR7f) |  | 0 | For vision-language models (VLMs), understanding the dynamic properties of objects and their interactions in 3D scenes from videos is crucial for effective reasoning about high-level temporal and action semantics. Although humans are adept at understanding these properties by constructing 3D and... | Adam Kortylewski, Alan L. Yuille, Angtian Wang, Shuo Chen, Wufei Ma, Xingrui Wang |  |
| 3596 |  |  [UGMathBench: A Diverse and Dynamic Benchmark for Undergraduate-Level Mathematical Reasoning with Large Language Models](https://openreview.net/forum?id=fovPyqPcKY) |  | 0 | Large Language Models (LLMs) have made significant strides in mathematical reasoning, underscoring the need for a comprehensive and fair evaluation of their capabilities. However, existing benchmarks often fall short, either lacking extensive coverage of undergraduate-level mathematical problems or... | Can Yang, Jiaxin Zhang, Jishan Hu, Tianhao Chen, Xin Xu, Zitong Chao |  |
| 3597 |  |  [Can LLMs Solve Longer Math Word Problems Better?](https://openreview.net/forum?id=C9ju8QQSCv) |  | 0 | Math Word Problems (MWPs) play a vital role in assessing the capabilities of Large Language Models (LLMs), yet current research primarily focuses on questions with concise contexts. The impact of longer contexts on mathematical reasoning remains under-explored. This study pioneers the investigation... | Can Yang, Tong Xiao, Xin Xu, Yang Wang, Zhenya Huang, Zitong Chao |  |
| 3598 |  |  [Progressive Token Length Scaling in Transformer Encoders for Efficient Universal Segmentation](https://openreview.net/forum?id=dmzM5UdAq6) |  | 0 | A powerful architecture for universal segmentation relies on transformers that encode multi-scale image features and decode object queries into mask predictions. With efficiency being a high priority for scaling such models, we observed that the state-of-the-art method Mask2Former uses \~50% of its... | Abhishek Aich, Manmohan Chandraker, Samuel Schulter, Yumin Suh |  |
| 3599 |  |  [Understanding Matrix Function Normalizations in Covariance Pooling through the Lens of Riemannian Geometry](https://openreview.net/forum?id=q1t0Lmvhty) |  | 0 | Global Covariance Pooling (GCP) has been demonstrated to improve the performance of Deep Neural Networks (DNNs) by exploiting second-order statistics of high-level representations. GCP typically performs classification of the covariance matrices by applying matrix function normalization, such as... | Gaowen Liu, Nicu Sebe, Xiaojun Wu, Yue Song, Ziheng Chen |  |
| 3600 |  |  [Gyrogroup Batch Normalization](https://openreview.net/forum?id=d1NWq4PjJW) |  | 0 | Several Riemannian manifolds in machine learning, such as Symmetric Positive Definite (SPD), Grassmann, spherical, and hyperbolic manifolds, have been proven to admit gyro structures, thus enabling a principled and effective extension of Euclidean Deep Neural Networks (DNNs) to manifolds. Inspired... | Nicu Sebe, Xiaojun Wu, Yue Song, Ziheng Chen |  |
| 3601 |  |  [T-JEPA: Augmentation-Free Self-Supervised Learning for Tabular Data](https://openreview.net/forum?id=gx3LMRB15C) |  | 0 | Self-supervision is often used for pre-training to foster performance on a downstream task by constructing meaningful representations of samples. Self-supervised learning (SSL) generally involves generating different views of the same sample and thus requires data augmentations that are challenging... | Arpad Rimmel, BichLiên Doan, Fabrice Popineau, Hugo Thimonier, José Lucas De Melo Costa |  |
| 3602 |  |  [Steering Masked Discrete Diffusion Models via Discrete Denoising Posterior Prediction](https://openreview.net/forum?id=Ombm8S40zN) |  | 0 | Generative modeling of discrete data underlies important applications spanning text-based agents like ChatGPT to the design of the very building blocks of life in protein sequences. However, application domains need to exert control over the generated data by steering the generative... | Alexander Tong, ChengHao Liu, Jarrid RectorBrooks, Joey Bose, Michael M. Bronstein, Mohsin Hasan, Nouha Dziri, Pranam Chatterjee, Sarthak Mittal, Zhangzhi Peng |  |
| 3603 |  |  [Learning to Adapt Frozen CLIP for Few-Shot Test-Time Domain Adaptation](https://openreview.net/forum?id=TD3SGJfBC7) |  | 0 | Few-shot Test-Time Domain Adaptation focuses on adapting a model at test time to a specific domain using only a few unlabeled examples, addressing domain shift. Prior methods leverage CLIP's strong out-of-distribution (OOD) abilities by generating domain-specific prompts to guide its generalized,... | Huan Liu, Konstantinos N. Plataniotis, Li Gu, Yanan Wu, Yang Wang, Zhixiang Chi, Ziqiang Wang |  |
| 3604 |  |  [Tuning-Free Bilevel Optimization: New Algorithms and Convergence Analysis](https://openreview.net/forum?id=A4aG3XeIO7) |  | 0 | Bilevel optimization has recently attracted considerable attention due to its abundant applications in machine learning problems. However, existing methods rely on prior knowledge of problem parameters to determine stepsizes, resulting in significant effort in tuning stepsizes when these parameters... | Hao Ban, Kaiyi Ji, Minhui Huang, Shiqian Ma, Yifan Yang |  |
| 3605 |  |  [ParetoFlow: Guided Flows in Multi-Objective Optimization](https://openreview.net/forum?id=mLyyB4le5u) |  | 0 | In offline multi-objective optimization (MOO), we leverage an offline dataset of designs and their associated labels to simultaneously minimize multiple objectives. This setting more closely mirrors complex real-world problems compared to single-objective optimization. Recent works mainly employ... | Can Chen, Christopher Pal, Xue Liu, Ye Yuan |  |
| 3606 |  |  [Quantized Spike-driven Transformer](https://openreview.net/forum?id=5J9B7Sb8rO) |  | 0 | Spiking neural networks (SNNs) are emerging as a promising energy-efficient alternative to traditional artificial neural networks (ANNs) due to their spike-driven paradigm. However, recent research in the SNN domain has mainly focused on enhancing accuracy by designing large-scale Transformer... | Haizhou Li, Honglin Cao, Jieyuan Zhang, Junsheng Guo, Malu Zhang, RuiJie Zhu, Wenjie Wei, Xuerui Qiu, Yang Yang, Yimeng Shan |  |
| 3607 |  |  [Matérn Kernels for Tunable Implicit Surface Reconstruction](https://openreview.net/forum?id=Ox4AJ2Vurb) |  | 0 | We propose to use the family of Matérn kernels for implicit surface reconstruction, building upon the recent success of kernel methods for 3D reconstruction of oriented point clouds. As we show from a theoretical and practical perspective, Matérn kernels have some appealing properties which make... | Bernhard Egger, Maximilian Weiherer |  |
| 3608 |  |  [Hierarchically Encapsulated Representation for Protocol Design in Self-Driving Labs](https://openreview.net/forum?id=9nUBh4V6SA) |  | 0 | Self-driving laboratories have begun to replace human experimenters in performing single experimental skills or predetermined experimental protocols. However, as the pace of idea iteration in scientific research has been intensified by Artificial Intelligence, the demand for rapid design of new... | Fanxu Meng, Kun He, Lecheng Ruan, Mingchen Liu, Qiao Xu, Qining Wang, YuZhe Shi, Zhangqian Bi |  |
| 3609 |  |  [Mechanism and Emergence of Stacked Attention Heads in Multi-Layer Transformers](https://openreview.net/forum?id=rUC7tHecSQ) |  | 0 | In this paper, I introduce the retrieval problem, a simple yet common reasoning task that can be solved only by transformers with a minimum number of layers, which grows logarithmically with the input size. I empirically show that large language models can solve the task under different prompting... | Tiberiu Musat |  |
| 3610 |  |  [Progressive Parameter Efficient Transfer Learning for Semantic Segmentation](https://openreview.net/forum?id=YNbLUGDAX5) |  | 0 | Parameter Efficient Transfer Learning (PETL) excels in downstream classification fine-tuning with minimal computational overhead, demonstrating its potential within the pre-train and fine-tune paradigm. However, recent PETL methods consistently struggle when fine-tuning for semantic segmentation... | Di Huang, Huiqun Wang, Nan Zhou, Yaoyan Zheng |  |
| 3611 |  |  [Dynamic Low-Rank Sparse Adaptation for Large Language Models](https://openreview.net/forum?id=oXh0939Zzq) |  | 0 | Despite the efficacy of network sparsity in alleviating the deployment strain of Large Language Models (LLMs), it endures significant performance degradation. Applying Low-Rank Adaptation (LoRA) to fine-tune the sparse LLMs offers an intuitive approach to counter this predicament, while it holds... | Jing Lin, Rongrong Ji, Weizhong Huang, Xiawu Zheng, Yang Liu, Yiwu Yao, Yuxin Zhang |  |
| 3612 |  |  [A Coefficient Makes SVRG Effective](https://openreview.net/forum?id=twtTLZnG0B) |  | 0 | Stochastic Variance Reduced Gradient (SVRG), introduced by Johnson & Zhang (2013), is a theoretically compelling optimization method. However, as Defazio & Bottou (2019) highlight, its effectiveness in deep learning is yet to be proven. In this work, we demonstrate the potential of SVRG in... | Trevor Darrell, Yida Yin, Zhiqiu Xu, Zhiyuan Li, Zhuang Liu |  |
| 3613 |  |  [DiffPC: Diffusion-based High Perceptual Fidelity Image Compression with Semantic Refinement](https://openreview.net/forum?id=RL7PycCtAO) |  | 0 | Reconstructing high-quality images under low bitrates conditions presents a challenge, and previous methods have made this task feasible by leveraging the priors of diffusion models. However, the effective exploration of pre-trained latent diffusion models and semantic information integration in... | Baoyi An, Bin Chen, Haoqian Wang, Jinpeng Wang, Yaowei Wang, Yichong Xia, Yimin Zhou |  |
| 3614 |  |  [Unleashing the Power of Task-Specific Directions in Parameter Efficient Fine-tuning](https://openreview.net/forum?id=RYrJqz44p4) |  | 0 | Large language models demonstrate impressive performance on downstream tasks, yet requiring extensive resource consumption when fully fine-tuning all parameters. To mitigate this, Parameter Efficient Fine-Tuning (PEFT) strategies, such as LoRA, have been developed. In this paper, we delve into the... | Chongjie Si, Hanspeter Pfister, Shifan Zhang, Wei Shen, Xiaokang Yang, Zhiyi Shi |  |
| 3615 |  |  [Learning Shape-Independent Transformation via Spherical Representations for Category-Level Object Pose Estimation](https://openreview.net/forum?id=D4xztKoz0Y) |  | 0 | Category-level object pose estimation aims to determine the pose and size of novel objects in specific categories. Existing correspondence-based approaches typically adopt point-based representations to establish the correspondences between primitive observed points and normalized object... | Huan Ren, Shifeng Zhang, Tianzhu Zhang, Wenfei Yang, Xiang Liu |  |
| 3616 |  |  [SuperCorrect: Advancing Small LLM Reasoning with Thought Template Distillation and Self-Correction](https://openreview.net/forum?id=PyjZO7oSw2) |  | 0 | Large language models (LLMs) like GPT-4, DeepSeek-R1, and ReasonFlux have shown significant improvements in various reasoning tasks. However, smaller LLMs still struggle with complex mathematical reasoning because they fail to effectively identify and correct reasoning errors. Recent... | Bin Cui, Joseph E. Gonzalez, Ling Yang, Minkai Xu, Shuicheng Yan, Tianjun Zhang, Zhaochen Yu |  |
| 3617 |  |  [Beyond Sequence: Impact of Geometric Context for RNA Property Prediction](https://openreview.net/forum?id=9htTvHkUhh) |  | 0 | Accurate prediction of RNA properties, such as stability and interactions, is crucial for advancing our understanding of biological processes and developing RNA-based therapeutics. RNA structures can be represented as 1D sequences, 2D topological graphs, or 3D all-atom models, each offering... | Artem Moskalev, Junjie Xu, Mangal Prakash, Rui Liao, Tommaso Mansi |  |
| 3618 |  |  [FaceShot: Bring Any Character into Life](https://openreview.net/forum?id=oJA1GUqRww) |  | 0 | In this paper, we present \*\*\*FaceShot\*\*\*, a novel training-free portrait animation framework designed to bring any character into life from any driven video without fine-tuning or retraining. We achieve this by offering precise and robust reposed landmark sequences from an appearance-guided... | Cairong Zhao, Fei Shen, Junyao Gao, Kai Chen, Xin Jiang, Yanan Sun, Zhening Xing |  |
| 3619 |  |  [IgGM: A Generative Model for Functional Antibody and Nanobody Design](https://openreview.net/forum?id=zmmfsJpYcq) |  | 0 | Immunoglobulins are crucial proteins produced by the immune system to identify and bind to foreign substances, playing an essential role in shielding organisms from infections and diseases. Designing specific antibodies opens new pathways for disease treatment. With the rise of deep learning,... | Fandi Wu, Jianhua Yao, Jiaxiang Wu, Peilin Zhao, Rubo Wang, Xingyu Gao |  |
| 3620 |  |  [Refining CLIP's Spatial Awareness: A Visual-Centric Perspective](https://openreview.net/forum?id=38No4B8sx6) |  | 0 | Contrastive Language-Image Pre-training (CLIP) excels in global alignment with language but exhibits limited sensitivity to spatial information, leading to strong performance in zero-shot classification tasks but underperformance in tasks requiring precise spatial understanding. Recent approaches... | Congpei Qiu, Tong Zhang, Wei Ke, Xiuxiu Bai, Yanhao Wu |  |
| 3621 |  |  [Do Deep Neural Network Solutions Form a Star Domain?](https://openreview.net/forum?id=QjO0fUlVYK) |  | 0 | It has recently been conjectured that neural network solution sets reachable via stochastic gradient descent (SGD) are convex, considering permutation invariances. This means that a linear path can connect two independent solutions with low loss, given the weights of one of the models are... | Alexander Rubinstein, Ankit Sonthalia, Ehsan Abbasnejad, Seong Joon Oh |  |
| 3622 |  |  [Information Theoretic Text-to-Image Alignment](https://openreview.net/forum?id=Ugs2W5XFFo) |  | 0 | Diffusion models for Text-to-Image (T2I) conditional generation have recently achieved tremendous success. Yet, aligning these models with user’s intentions still involves a laborious trial-and-error process, and this challenging alignment problem has attracted considerable attention from the... | Alessandro Finamore, Chao Wang, Giulio Franzese, Massimo Gallo, Pietro Michiardi |  |
| 3623 |  |  [Semantix: An Energy-guided Sampler for Semantic Style Transfer](https://openreview.net/forum?id=si37wk8U5D) |  | 0 | Recent advances in style and appearance transfer are impressive, but most methods isolate global style and local appearance transfer, neglecting semantic correspondence. Additionally, image and video tasks are typically handled in isolation, with little focus on integrating them for video transfer.... | Chaoyue Wang, Chuanxia Zheng, Huiang He, Minghui Hu, TatJen Cham |  |
| 3624 |  |  [GaussianAnything: Interactive Point Cloud Flow Matching for 3D Generation](https://openreview.net/forum?id=P4DbTSDQFu) |  | 0 | Recent advancements in diffusion models and large-scale datasets have revolutionized image and video generation, with increasing focus on 3D content generation. While existing methods show promise, they face challenges in input formats, latent space structures, and output representations. This... | Bo Dai, Chen Change Loy, Fangzhou Hong, Shangchen Zhou, Shuai Yang, Xingang Pan, Yushi Lan, Zhaoyang Lyu |  |
| 3625 |  |  [TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies](https://openreview.net/forum?id=b1CVu9l5GO) |  | 0 | Although large vision-language-action (VLA) models pretrained on extensive robot datasets offer promising generalist policies for robotic learning, they still struggle with spatial-temporal dynamics in interactive robotics, making them less effective in handling complex tasks, such as manipulation.... | Andrey Kolobov, Furong Huang, Hal Daumé III, Jianfeng Gao, Jianwei Yang, Ruijie Zheng, Shuaiyi Huang, Yongyuan Liang |  |
| 3626 |  |  [Gnothi Seauton: Empowering Faithful Self-Interpretability in Black-Box Transformers](https://openreview.net/forum?id=UvMSKonce8) |  | 0 | The debate between self-interpretable models and post-hoc explanations for black-box models is central to Explainable AI (XAI). Self-interpretable models, such as concept-based networks, offer insights by connecting decisions to human-understandable concepts but often struggle with performance and... | Hongrui Zhang, Hongxuan Tang, Linfeng Zhang, Mingyang Wang, Shaobo Wang, Weiya Li, Xuming Hu, Xuyang Liu |  |
| 3627 |  |  [DenseGrounding: Improving Dense Language-Vision Semantics for Ego-centric 3D Visual Grounding](https://openreview.net/forum?id=iGafR0hSln) |  | 0 | Enabling intelligent agents to comprehend and interact with 3D environments through natural language is crucial for advancing robotics and human-computer interaction. A fundamental task in this field is ego-centric 3D visual grounding, where agents locate target objects in real-world 3D spaces... | Gao Huang, Hao Shi, Henry Zheng, Qihang Peng, Rui Huang, Yepeng Weng, Yong Xien Chng, Zhongchao Shi |  |
| 3628 |  |  [Bayesian Analysis of Combinatorial Gaussian Process Bandits](https://openreview.net/forum?id=50cmx4SrkM) |  | 0 | We consider the combinatorial volatile Gaussian process (GP) semi-bandit problem. Each round, an agent is provided a set of available base arms and must select a subset of them to maximize the long-term cumulative reward. We study the Bayesian setting and provide novel Bayesian cumulative regret... | Jack Sandberg, Morteza Haghir Chehreghani, Niklas Åkerblom |  |
| 3629 |  |  [Test-time Adaptation for Image Compression with Distribution Regularization](https://openreview.net/forum?id=bsnRUkVn63) |  | 0 | Current test- or compression-time adaptation image compression (TTA-IC) approaches, which leverage both latent and decoder refinements as a two-step adaptation scheme, have potentially enhanced the rate-distortion (R-D) performance of learned image compression models on cross-domain compression... | Haoliang Li, Hong Yan, Kecheng Chen, Pingping Zhang, Shiqi Wang, Tiexin Qin |  |
| 3630 |  |  [Controllable Blur Data Augmentation Using 3D-Aware Motion Estimation](https://openreview.net/forum?id=Wvi8c0tgvt) |  | 0 | Existing realistic blur datasets provide insufficient variety in scenes and blur patterns to be trained, while expanding data diversity demands considerable time and effort due to complex dual-camera systems. To address the challenge, data augmentation can be an effective way to artificially... | Hana Lee, HyongEuk Lee, Insoo Kim, Jinwoo Shin |  |
| 3631 |  |  [MaxCutPool: differentiable feature-aware Maxcut for pooling in graph neural networks](https://openreview.net/forum?id=xlbXRJ2XCP) |  | 0 | We propose a novel approach to compute the MAXCUT in attributed graphs, i.e., graphs with features associated with nodes and edges. Our approach works well on any kind of graph topology and can find solutions that jointly optimize the MAXCUT along with other objectives. Based on the obtained MAXCUT... | Carlo Abate, Filippo Maria Bianchi |  |
| 3632 |  |  [FakeShield: Explainable Image Forgery Detection and Localization via Multi-modal Large Language Models](https://openreview.net/forum?id=pAQzEY7M03) |  | 0 | The rapid development of generative AI is a double-edged sword, which not only facilitates content creation but also makes image manipulation easier and more difficult to detect. Although current image forgery detection and localization (IFDL) methods are generally effective, they tend to face two... | Jian Zhang, Qing Huang, Runyi Li, Xuanyu Zhang, Zecheng Tang, Zhipei Xu |  |
| 3633 |  |  [SecureGS: Boosting the Security and Fidelity of 3D Gaussian Splatting Steganography](https://openreview.net/forum?id=H4FSx06FCZ) |  | 0 | 3D Gaussian Splatting (3DGS) has emerged as a premier method for 3D representation due to its real-time rendering and high-quality outputs, underscoring the critical need to protect the privacy of 3D assets. Traditional NeRF steganography methods fail to address the explicit nature of 3DGS since... | Jian Zhang, Jiarui Meng, Ronggang Wang, Shuzhou Yang, Xuanyu Zhang, Yanmin Wu, Zhipei Xu |  |
| 3634 |  |  [GenDataAgent: On-the-fly Dataset Augmentation with Synthetic Data](https://openreview.net/forum?id=WoGnnggVCZ) |  | 0 | We propose a generative agent that augments training datasets with synthetic data for model fine-tuning. Unlike prior work, which uniformly samples synthetic data, our agent iteratively generates relevant samples on-the-fly, aligning with the target distribution. It prioritizes synthetic data that... | Alice Xiang, Jerone T. A. Andrews, Lele Chen, Yulun Zhang, Yunhao Ba, Zhiteng Li |  |
| 3635 |  |  [VideoGrain: Modulating Space-Time Attention for Multi-Grained Video Editing](https://openreview.net/forum?id=SSslAtcPB6) |  | 0 | Recent advancements in diffusion models have significantly improved video generation and editing capabilities. However, multi-grained video editing, which encompasses class-level, instance-level, and part-level modifications, remains a formidable challenge. The major difficulties in multi-grained... | Hehe Fan, Linchao Zhu, Xiangpeng Yang, Yi Yang |  |
| 3636 |  |  [Let the Code LLM Edit Itself When You Edit the Code](https://openreview.net/forum?id=zqzsZ5cXbB) |  | 0 | In this work, we investigate a typical scenario in code generation where a developer edits existing code in real time and requests a code assistant, e.g., a large language model, to re-predict the next token or next line on the fly. Naively, the LLM needs to re-encode the entire KV cache to provide... | Di He, Jingjing Xu, Jun Zhang, Shengjie Luo, Zhenyu He, Zhi Zhang |  |
| 3637 |  |  [CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes](https://openreview.net/forum?id=a3ptUbuzbW) |  | 0 | Recently, 3D Gaussian Splatting (3DGS) has revolutionized radiance field reconstruction, manifesting efficient and high-fidelity novel view synthesis. However, accurately representing surfaces, especially in large and complex scenarios, remains a significant challenge due to the unstructured nature... | Chuanchen Luo, Junran Peng, Yang Liu, Zhaoxiang Zhang, Zhongkai Mao |  |
| 3638 |  |  [Debiasing Mini-Batch Quadratics for Applications in Deep Learning](https://openreview.net/forum?id=Q0TEVKV2cp) |  | 0 | Quadratic approximations form a fundamental building block of machine learning methods. E.g., second-order optimizers try to find the Newton step into the minimum of a local quadratic proxy to the objective function; and the second-order approximation of a network's loss function can be used to... | Bálint Mucsányi, Lukas Tatzel, Osane Hackel, Philipp Hennig |  |
| 3639 |  |  [Denoising as Adaptation: Noise-Space Domain Adaptation for Image Restoration](https://openreview.net/forum?id=jsBhmOCKYs) |  | 0 | Although learning-based image restoration methods have made significant progress, they still struggle with limited generalization to real-world scenarios due to the substantial domain gap caused by training on synthetic data. Existing methods address this issue by improving data synthesis... | Chen Change Loy, Kang Liao, Zhouxia Wang, Zongsheng Yue |  |
| 3640 |  |  [Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Dynamic Scenes](https://openreview.net/forum?id=LuGHbK8qTa) |  | 0 | Modern 3D engines and graphics pipelines require mesh as a memory-efficient representation, which allows efficient rendering, geometry processing, texture editing, and many other downstream operations. However, it is still highly difficult to obtain high-quality mesh in terms of detailed structure... | Hao Su, Isabella Liu, Xiaolong Wang |  |
| 3641 |  |  [Robust Watermarking Using Generative Priors Against Image Editing: From Benchmarking to Advances](https://openreview.net/forum?id=16O8GCm8Wn) |  | 0 | Current image watermarking methods are vulnerable to advanced image editing techniques enabled by large-scale text-to-image models. These models can distort embedded watermarks during editing, posing significant challenges to copyright protection. In this work, we introduce W-Bench, the first... | Adams WaiKin Kong, Jiayou Lu, Shilin Lu, Yuanzhi Zhu, Zihan Zhou |  |
| 3642 |  |  [Revisiting In-context Learning Inference Circuit in Large Language Models](https://openreview.net/forum?id=xizpnYNvQq) |  | 0 | In-context Learning (ICL) is an emerging few-shot learning paradigm on Language Models (LMs) with inner mechanisms un-explored. There are already existing works describing the inner processing of ICL, while they struggle to capture all the inference phenomena in large language models. Therefore,... | Hakaze Cho, Mariko Kato, Naoya Inoue, Yoshihiro Sakai |  |
| 3643 |  |  [FreeVS: Generative View Synthesis on Free Driving Trajectory](https://openreview.net/forum?id=dTGH9vUVdf) |  | 0 | Existing reconstruction-based novel view synthesis methods for driving scenes focus on synthesizing camera views along the recorded trajectory of the ego vehicle. Their image rendering performance will severely degrade on viewpoints falling out of the recorded trajectory, where camera rays are... | Lue Fan, Qitai Wang, Yuntao Chen, Yuqi Wang, Zhaoxiang Zhang |  |
| 3644 |  |  [Enhancing End-to-End Autonomous Driving with Latent World Model](https://openreview.net/forum?id=fd2u60ryG0) |  | 0 | In autonomous driving, end-to-end planners directly utilize raw sensor data, enabling them to extract richer scene features and reduce information loss compared to traditional planners. This raises a crucial research question: how can we develop better scene feature representations to fully... | Jiawei He, Lue Fan, Tieniu Tan, Yingyan Li, Yuntao Chen, Yuqi Wang, Zhaoxiang Zhang |  |
| 3645 |  |  [Collapsed Language Models Promote Fairness](https://openreview.net/forum?id=kynD1UUk6q) |  | 0 | To mitigate societal biases implicitly encoded in recent successful pretrained language models, a diverse array of approaches have been proposed to encourage model fairness, focusing on prompting, data augmentation, regularized fine-tuning, and more. Despite the development, it is nontrivial to... | Jingxuan Xu, Linyi Li, Wuyang Chen, Yao Zhao, Yunchao Wei |  |
| 3646 |  |  [Long-horizon Visual Instruction Generation with Logic and Attribute Self-reflection](https://openreview.net/forum?id=EdMb9TqqDY) |  | 0 | Visual instructions for long-horizon tasks are crucial as they intuitively clarify complex concepts and enhance retention across extended steps. Directly generating a series of images using text-to-image models without considering the context of previous steps results in inconsistent images,... | Fan Ma, Kaixin Shen, Linchao Zhu, Yi Yang, Yucheng Suo |  |
| 3647 |  |  [Learning Harmonized Representations for Speculative Sampling](https://openreview.net/forum?id=T9u56s7mbk) |  | 0 | Speculative sampling is a promising approach to accelerate the decoding stage for Large Language Models (LLMs). Recent advancements that leverage target LLM's contextual information, such as hidden states and KV cache, have shown significant practical improvements. However, these approaches suffer... | Lefan Zhang, Ruiwen Xu, Xiaodan Wang, Yanhua Huang |  |
| 3648 |  |  [MDSGen: Fast and Efficient Masked Diffusion Temporal-Aware Transformers for Open-Domain Sound Generation](https://openreview.net/forum?id=yFEqYwgttJ) |  | 0 | We introduce MDSGen, a novel framework for vision-guided open-domain sound generation optimized for model parameter size, memory consumption, and inference speed. This framework incorporates two key innovations: (1) a redundant video feature removal module that filters out unnecessary visual... | Chang D. Yoo, Tri Ton, Trung X. Pham |  |
| 3649 |  |  [SONICS: Synthetic Or Not - Identifying Counterfeit Songs](https://openreview.net/forum?id=PY7KSh29Z8) |  | 0 | The recent surge in AI-generated songs presents exciting possibilities and challenges. These innovations necessitate the ability to distinguish between human-composed and synthetic songs to safeguard artistic integrity and protect human musical artistry. Existing research and datasets in fake song... | Bishmoy Paul, Md Awsafur Rahman, Najibul Haque Sarker, Shaikh Anowarul Fattah, Zaber Ibn Abdul Hakim |  |
| 3650 |  |  [Self-Correcting Decoding with Generative Feedback for Mitigating Hallucinations in Large Vision-Language Models](https://openreview.net/forum?id=tTBXePRKSx) |  | 0 | While recent Large Vision-Language Models (LVLMs) have shown remarkable performance in multi-modal tasks, they are prone to generating hallucinatory text responses that do not align with the given visual input, which restricts their practical applicability in real-world scenarios. In this work,... | Ce Zhang, Deva Ramanan, Katia P. Sycara, LouisPhilippe Morency, Martin Q. Ma, Russ Salakhutdinov, Simon Stepputtis, Yaqi Xie, Zhehan Kan, Zifu Wan |  |
| 3651 |  |  [Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation](https://openreview.net/forum?id=jxo70B9fQo) |  | 0 | LLM self-evaluation relies on the LLM's own ability to estimate response correctness, which can greatly improve its deployment reliability. In this research track, we propose the Chain-of-Embedding (CoE) in the latent space to enable LLMs to perform output-free self-evaluation. CoE consists of all... | Baosong Yang, Derek F. Wong, Pei Zhang, Rui Wang, Yiming Wang |  |
| 3652 |  |  [FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality](https://openreview.net/forum?id=W49UjcpGxx) |  | 0 | In this paper, we present \textbf{\textit{FasterCache}}, a novel training-free strategy designed to accelerate the inference of video diffusion models with high-quality generation. By analyzing existing cache-based methods, we observe that \textit{directly reusing adjacent-step features degrades... | Chenyang Si, Junhao Song, KwanYee K. Wong, Yu Qiao, Zhengyao Lv, Zhenyu Yang, Ziwei Liu |  |
| 3653 |  |  [Correlation and Navigation in the Vocabulary Key Representation Space of Language Models](https://openreview.net/forum?id=VipcVxaTnG) |  | 0 | Language model (LM) decoding is based on the next-token prediction (NTP) probability distribution. For neural LMs (e.g., Transformer-based), NTP distribution is essentially a softmax-regularized dot product between an encoded input context (query) and fixed vocabulary representations (keys). In... | Chenyang An, Jingbo Shang, Letian Peng |  |
| 3654 |  |  [Reconstructive Visual Instruction Tuning](https://openreview.net/forum?id=8q9NOMzRDg) |  | 0 | This paper introduces reconstructive visual instruction tuning (ROSS), a family of Large Multimodal Models (LMMs) that exploit vision-centric supervision signals. In contrast to conventional visual instruction tuning approaches that exclusively supervise text outputs, ROSS prompts LMMs to supervise... | Anlin Zheng, Haochen Wang, Tiancai Wang, Xiangyu Zhang, Yucheng Zhao, Zhaoxiang Zhang, Zheng Ge |  |
| 3655 |  |  [BLEND: Behavior-guided Neural Population Dynamics Modeling via Privileged Knowledge Distillation](https://openreview.net/forum?id=jE5ZbtMtcU) |  | 0 | Modeling the nonlinear dynamics of neuronal populations represents a key pursuit in computational neuroscience. Recent research has increasingly focused on jointly modeling neural activity and behavior to unravel their interconnections. Despite significant efforts, these approaches often... | Fangxu Zhou, Hao Chen, Jinzhuo Wang, Lishuang Feng, Qichen Sun, Wei Wu, Zhengrui Guo |  |
| 3656 |  |  [COMBO: Compositional World Models for Embodied Multi-Agent Cooperation](https://openreview.net/forum?id=YXRyYkb1im) |  | 0 | In this paper, we investigate the problem of embodied multi-agent cooperation, where decentralized agents must cooperate given only egocentric views of the world. To effectively plan in this setting, in contrast to learning world dynamics in a single-agent scenario, we must simulate world dynamics... | Behzad Dariush, Chuang Gan, Hongxin Zhang, Kwonjoon Lee, Qiushi Lyu, Sunli Chen, Tianmin Shu, Yilun Du, Zeyuan Wang, Zheyuan Zhang |  |
| 3657 |  |  [Image-level Memorization Detection via Inversion-based Inference Perturbation](https://openreview.net/forum?id=vwOq7twk7L) |  | 0 | Recent studies have discovered that widely used text-to-image diffusion models can replicate training samples during image generation, a phenomenon known as memorization. Existing detection methods primarily focus on identifying memorized prompts. However, in real-world scenarios, image owners may... | Bo Peng, Haokun Lin, Jing Dong, Xing Zheng, Yang Bai, Yong Yang, Yue Jiang, Yueming Lyu, Zhili Liu |  |
| 3658 |  |  [SaRA: High-Efficient Diffusion Model Fine-tuning with Progressive Sparse Low-Rank Adaptation](https://openreview.net/forum?id=wGVOxplEbf) |  | 0 | The development of diffusion models has led to significant progress in image and video generation tasks, with pre-trained models like the Stable Diffusion series playing a crucial role. However, a key challenge remains in downstream task applications: how to effectively and efficiently adapt... | Hongrui Huang, Jiangning Zhang, Lizhuang Ma, Ran Yi, Teng Hu, Yabiao Wang |  |
| 3659 |  |  [Microcanonical Langevin Ensembles: Advancing the Sampling of Bayesian Neural Networks](https://openreview.net/forum?id=QMtrW8Ej98) |  | 0 | Despite recent advances, sampling-based inference for Bayesian Neural Networks (BNNs) remains a significant challenge in probabilistic deep learning. While sampling-based approaches do not require a variational distribution assumption, current state-of-the-art samplers still struggle to navigate... | David Rügamer, Emanuel Sommer, Giorgi Nozadze, Jakob Robnik, Uros Seljak |  |
| 3660 |  |  [Posterior-Mean Rectified Flow: Towards Minimum MSE Photo-Realistic Image Restoration](https://openreview.net/forum?id=hPOt3yUXii) |  | 0 | Photo-realistic image restoration algorithms are typically evaluated by distortion measures (e.g., PSNR, SSIM) and by perceptual quality measures (e.g., FID, NIQE), where the desire is to attain the lowest possible distortion without compromising on perceptual quality. To achieve this goal, current... | Guy Ohayon, Michael Elad, Tomer Michaeli |  |
| 3661 |  |  [Deep Weight Factorization: Sparse Learning Through the Lens of Artificial Symmetries](https://openreview.net/forum?id=vNdOHr7mn5) |  | 0 | Sparse regularization techniques are well-established in machine learning, yet their application in neural networks remains challenging due to the non-differentiability of penalties like the $L_1$ norm, which is incompatible with stochastic gradient descent. A promising alternative is shallow... | Bernd Bischl, Chris Kolb, David Rügamer, Tobias Weber |  |
| 3662 |  |  [MotionClone: Training-Free Motion Cloning for Controllable Video Generation](https://openreview.net/forum?id=aY3L65HgHJ) |  | 0 | Motion-based controllable video generation offers the potential for creating captivating visual content. Existing methods typically necessitate model training to encode particular motion cues or incorporate fine-tuning to inject certain motion patterns, resulting in limited flexibility and... | Huaian Chen, Jiaqi Wang, Jiazi Bu, Pan Zhang, Pengyang Ling, Tong Wu, Xiaoyi Dong, Yi Jin, Yuhang Zang |  |
| 3663 |  |  [Learning View-invariant World Models for Visual Robotic Manipulation](https://openreview.net/forum?id=vJwjWyt4Ed) |  | 0 | Robotic manipulation tasks often rely on visual inputs from cameras to perceive the environment. However, previous approaches still suffer from performance degradation when the camera’s viewpoint changes during manipulation. In this paper, we propose ReViWo (Representation learning for... | Gang Niu, JingCheng Pang, Kaiyuan Li, Masashi Sugiyama, Nan Tang, XinQiang Cai, Yang Yu, Yuting Tang, ZhenYu Zhang |  |
| 3664 |  |  [Recovery of Causal Graph Involving Latent Variables via Homologous Surrogates](https://openreview.net/forum?id=fGhr39bqZa) |  | 0 | Causal discovery with latent variables is an important and challenging problem. To identify latent variables and infer their causal relations, most existing works rely on the assumption that latent variables have pure children. Considering that this assumption is potentially restrictive in practice... | Jun Wang, Tongliang Liu, XiuChuan Li |  |
| 3665 |  |  [Reconstruction-Guided Policy: Enhancing Decision-Making through Agent-Wise State Consistency](https://openreview.net/forum?id=Y8L5RB4GWb) |  | 0 | An important challenge in multi-agent reinforcement learning is partial observability, where agents cannot access the global state of the environment during execution and can only receive observations within their field of view. To address this issue, previous works typically use the... | Haipeng Liu, Qifan Liang, Ting Long, Weinan Zhang, Yixiang Shan, Yuan Tian, Zhengbang Zhu |  |
| 3666 |  |  [Efficient and Trustworthy Causal Discovery with Latent Variables and Complex Relations](https://openreview.net/forum?id=BZYIEw4mcY) |  | 0 | Most traditional causal discovery methods assume that all task-relevant variables are observed, an assumption often violated in practice. Although some recent works allow the presence of latent variables, they typically assume the absence of certain special causal relations to ensure a degree of... | Tongliang Liu, XiuChuan Li |  |
| 3667 |  |  [DeepGate4: Efficient and Effective Representation Learning for Circuit Design at Scale](https://openreview.net/forum?id=b10lRabU9W) |  | 0 | Circuit representation learning has become pivotal in electronic design automation, enabling critical tasks such as testability analysis, logic reasoning, power estimation, and SAT solving. However, existing models face significant challenges in scaling to large circuits due to limitations like... | Guohao Dai, Jianyuan Zhong, Ningyi Xu, Qiang Xu, Shan Huang, Zhengyuan Shi, Ziyang Zheng |  |
| 3668 |  |  [TGB-Seq Benchmark: Challenging Temporal GNNs with Complex Sequential Dynamics](https://openreview.net/forum?id=8e2LirwiJT) |  | 0 | Future link prediction is a fundamental challenge in various real-world dynamic systems. To address this, numerous temporal graph neural networks (temporal GNNs) and benchmark datasets have been developed. However, these datasets often feature excessive repeated edges and lack complex sequential... | Fengran Mo, Jie Peng, Lu Yi, Yanping Zheng, Yue Zixuan, Yuhang Ye, Zengfeng Huang, Zhewei Wei |  |
| 3669 |  |  [ContraDiff: Planning Towards High Return States via Contrastive Learning](https://openreview.net/forum?id=XMOaOigOQo) |  | 0 | The performance of offline reinforcement learning (RL) is sensitive to the proportion of high-return trajectories in the offline dataset. However, in many simulation environments and real-world scenarios, there are large ratios of low-return trajectories rather than high-return trajectories, which... | Liang Yin, Qifan Liang, Ting Long, Weinan Zhang, Yi Chang, Yixiang Shan, Zhengbang Zhu |  |
| 3670 |  |  [FreeCG: Free the Design Space of Clebsch-Gordan Transform for Machine Learning Force Fields](https://openreview.net/forum?id=sfi2j1Ot6j) |  | 0 | Machine Learning Force Fields (MLFFs) are of great importance for chemistry, physics, materials science, and many other related fields. The Clebsch–Gordan transform (CG transform) effectively encodes many-body interactions and is thus an important building block for many models of MLFFs. However,... | Haoran Geng, Qinghua Cui, Shihao Shao, Zun Wang |  |
| 3671 |  |  [Diffusion2: Dynamic 3D Content Generation via Score Composition of Video and Multi-view Diffusion Models](https://openreview.net/forum?id=fectsEG2GU) |  | 0 | Recent advancements in 3D generation are predominantly propelled by improvements in 3D-aware image diffusion models. These models are pretrained on Internet-scale image data and fine-tuned on massive 3D data, offering the capability of producing highly consistent multi-view images. However, due to... | Chun Gu, Li Zhang, Zeyu Yang, Zijie Pan |  |
| 3672 |  |  [Understanding and Mitigating Hallucination in Large Vision-Language Models via Modular Attribution and Intervention](https://openreview.net/forum?id=Bjq4W7P2Us) |  | 0 | Large Vision-Language Models (LVLMs) exhibit impressive capabilities in complex visual tasks but are prone to hallucination, especially in open-ended generation tasks. This paper explores why LVLMs tend to hallucinate and how to mitigate it. First, we conduct causal mediation analysis through... | Chang Xu, Juan Cao, Tianyun Yang, Ziniu Li |  |
| 3673 |  |  [Neuron Platonic Intrinsic Representation From Dynamics Using Contrastive Learning](https://openreview.net/forum?id=vFanHFE4Qv) |  | 0 | The Platonic Representation Hypothesis posits that behind different modalities of data (what we sense or detect), there exists a universal, modality-independent representation of reality. Inspired by this, we treat each neuron as a system, where we can detect the neuron’s multi-segment activity... | Can Liao, Jinzhuo Wang, Wei Wu, Zhengrui Guo, Zizhen Deng |  |
| 3674 |  |  [Where Am I and What Will I See: An Auto-Regressive Model for Spatial Localization and View Prediction](https://openreview.net/forum?id=NuHYh4YKNe) |  | 0 | Spatial intelligence is the ability of a machine to perceive, reason, and act in three dimensions within space and time. Recent advancements in large-scale auto-regressive models have demonstrated remarkable capabilities across various reasoning tasks. However, these models often struggle with... | Di Huang, Junyi Chen, Tong He, Wanli Ouyang, Weicai Ye |  |
| 3675 |  |  [Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization](https://openreview.net/forum?id=omrLHFzC37) |  | 0 | Federated Learning (FL) offers a promising framework for collaborative and privacy-preserving machine learning across distributed data sources. However, the substantial communication costs associated with FL significantly challenge its efficiency. Specifically, in each communication round, the... | Bicheng Ying, Chaosheng Dong, Haibo Yang, Zhe Li, Zidong Liu |  |
| 3676 |  |  [MindSimulator: Exploring Brain Concept Localization via Synthetic fMRI](https://openreview.net/forum?id=vgt2rSf6al) |  | 0 | Concept-selective regions within the human cerebral cortex exhibit significant activation in response to specific visual stimuli associated with particular concepts. Precisely localizing these regions stands as a crucial long-term goal in neuroscience to grasp essential brain functions and... | Duoqian Miao, Guangyin Bao, Qi Zhang, Zhuojia Wu, Zixuan Gong |  |
| 3677 |  |  [ImDy: Human Inverse Dynamics from Imitated Observations](https://openreview.net/forum?id=br8YB7KMug) |  | 0 | Inverse dynamics (ID), which aims at reproducing the driven torques from human kinematic observations, has been a critical tool for gait analysis. However, it is hindered from wider application to general motion due to its limited scalability. Conventional optimization-based ID requires expensive... | Cewu Lu, Haowen Hou, Junxuan Liang, Xinpeng Liu, YongLu Li, Zili Lin |  |
| 3678 |  |  [PIORF: Physics-Informed Ollivier-Ricci Flow for Long-Range Interactions in Mesh Graph Neural Networks](https://openreview.net/forum?id=qkBBHixPow) |  | 0 | Recently, data-driven simulators based on graph neural networks have gained attention in modeling physical systems on unstructured meshes. However, they struggle with long-range dependencies in fluid flows, particularly in refined mesh regions. This challenge, known as the 'over-squashing' problem,... | Jaehyeon Park, Jeongwhan Choi, Kookjin Lee, Noseong Park, YounYeol Yu |  |
| 3679 |  |  [ControlAR: Controllable Image Generation with Autoregressive Models](https://openreview.net/forum?id=BWuBDdXVnH) |  | 0 | Autoregressive (AR) models have reformulated image generation as next-token prediction, demonstrating remarkable potential and emerging as strong competitors to diffusion models. However, control-to-image generation, akin to ControlNet, remains largely unexplored within AR models. Although a... | Haocheng Shen, Longjin Ran, Peize Sun, Shoufa Chen, Tianheng Cheng, Wenyu Liu, Xiaoxin Chen, Xinggang Wang, Zongming Li |  |
| 3680 |  |  [Linear Multistep Solver Distillation for Fast Sampling of Diffusion Models](https://openreview.net/forum?id=vkOFOUDLTn) |  | 0 | Sampling from diffusion models can be seen as solving the corresponding probability flow ordinary differential equation (ODE). The solving process requires a significant number of function evaluations (NFE), making it time-consuming. Recently, several solver search frameworks have attempted to find... | Hanting Chen, Xiangzhong Fang, Yuchen Liang, Yunhe Wang |  |
| 3681 |  |  [ImagineNav: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination](https://openreview.net/forum?id=vQFw9ryKyK) |  | 0 | Visual navigation is an essential skill for home-assistance robots, providing the object-searching ability to accomplish long-horizon daily tasks. Many recent approaches use Large Language Models (LLMs) for commonsense inference to improve exploration efficiency. However, the planning process of... | Likun Tang, Teng Wang, Wenzhe Cai, Xinxin Zhao |  |
| 3682 |  |  [Navigation-Guided Sparse Scene Representation for End-to-End Autonomous Driving](https://openreview.net/forum?id=Vv76fCYffN) |  | 0 | End-to-End Autonomous Driving (E2EAD) methods typically rely on supervised perception tasks to extract explicit scene information (e.g., objects, maps). This reliance necessitates expensive annotations and constrains deployment and data scalability in real-time applications. In this paper, we... | Dixiao Cui, Peidong Li |  |
| 3683 |  |  [An Evolved Universal Transformer Memory](https://openreview.net/forum?id=s1kyHkdTmi) |  | 0 | Prior methods propose to offset the escalating costs of modern foundation models by dropping specific parts of their contexts with hand-designed rules, while attempting to preserve their original performance. We overcome this trade-off with Neural Attention Memory Models (NAMMs), introducing a... | Edoardo Cetin, Qi Sun, Tianyu Zhao, Yujin Tang |  |
| 3684 |  |  [Interpretable Bilingual Multimodal Large Language Model for Diverse Biomedical Tasks](https://openreview.net/forum?id=YuHQTo6G9S) |  | 0 | Several medical Multimodal Large Languange Models (MLLMs) have been developed to address tasks involving visual images with textual instructions across various medical modalities, achieving impressive results. Most current medical generalist models are region-agnostic, treating the entire image as... | Haonan Wang, Honglong Yang, Jiaji Mao, Jun Shen, Lehan Wang, Xiaomeng Li, Zehong Yang |  |
| 3685 |  |  [MindSearch: Mimicking Human Minds Elicits Deep AI Searcher](https://openreview.net/forum?id=xgtXkyqw1f) |  | 0 | Information seeking and integration is a complex cognitive task that consumes enormous time and effort. Inspired by the remarkable progress of Large Language Models, recent works attempt to solve this task by combining LLMs and search engines. However, these methods still obtain unsatisfying... | Feng Zhao, Jiangning Liu, Kai Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Zehui Chen |  |
| 3686 |  |  [PseDet: Revisiting the Power of Pseudo Label in Incremental Object Detection](https://openreview.net/forum?id=Iu8FVcUmVp) |  | 0 | Incremental Objection Detection (IOD) facilitates the expansion of the usage scope of object detectors without forgetting previously acquired knowledge. Current approaches mostly adopt response-level knowledge distillation to overcome forgetting issues, by conducting implicit memory replay from the... | Chenhongyi Yang, Feng Zhao, Jiaming Liu, Qiuchen Wang, Zehui Chen, Zhenyu Li |  |
| 3687 |  |  [Weak-to-Strong Generalization Through the Data-Centric Lens](https://openreview.net/forum?id=uogG8BfLs2) |  | 0 | The weak-to-strong generalization phenomenon is the driver for important machine learning applications including highly data-efficient learning and, most recently, performing superalignment. While decades of research have resulted in numerous algorithms that produce strong empirical performance,... | Changho Shin, Frederic Sala, John Cooper |  |
| 3688 |  |  [Distilling Structural Representations into Protein Sequence Models](https://openreview.net/forum?id=KXrgDM3mVD) |  | 0 |  | Adam R. Klivans, Chengyue Gong, Daniel Jesus Diaz, Jeffrey OuyangZhang, Philipp Krähenbühl, Yue Zhao |  |
| 3689 |  |  [Probabilistic Language-Image Pre-Training](https://openreview.net/forum?id=D5X6nPGFUY) |  | 0 |  | Sangdoo Yun, Sanghyuk Chun, Song Park, Wonjae Kim |  |
| 3690 |  |  [Find A Winning Sign: Sign Is All We Need to Win the Lottery](https://openreview.net/forum?id=cLtE4qoPlD) |  | 0 |  | Junghun Oh, Kyoung Mu Lee, Sungyong Baik |  |
| 3691 |  |  [IPDreamer: Appearance-Controllable 3D Object Generation with Complex Image Prompts](https://openreview.net/forum?id=3PguviI7Uf) |  | 0 |  | Baochang Zhang, Bohan Zeng, Conghui He, Hong Li, Jiaming Liu, Jianzhuang Liu, Juan Zhang, Ling Yang, Shanglin Li, Shuicheng Yan, Wentao Zhang, Yutang Feng |  |
| 3692 |  |  [Gated Delta Networks: Improving Mamba2 with Delta Rule](https://openreview.net/forum?id=r8H7xhYPwz) |  | 0 |  | Ali Hatamizadeh, Jan Kautz, Songlin Yang |  |
| 3693 |  |  [Rethinking Classifier Re-Training in Long-Tailed Recognition: Label Over-Smooth Can Balance](https://openreview.net/forum?id=OeKp3AdiVO) |  | 0 |  | Han Lu, Jiangtong Li, Junchi Yan, Liqing Zhang, Siyu Sun, Tianjiao Li, Xiaokang Yang, Yichen Xie |  |
| 3694 |  |  [STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs](https://openreview.net/forum?id=6XUSDvBFkV) |  | 0 |  | Dayou Du, Lujun Li, Peijie Dong, Qiang Wang, Ruibo Fan, Wei Xue, Xiaowen Chu, Yike Guo, Yuedong Zhong, Yuhan Chen, Zhenheng Tang |  |
| 3695 |  |  [3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation](https://openreview.net/forum?id=Gx04TnVjee) |  | 0 |  | Dahua Lin, Di Zhang, Menghan Xia, Pengfei Wan, Sida Peng, Xian Liu, Xiao Fu, Xiaoyu Shi, Xintao Wang, Ziyang Yuan |  |
| 3696 |  |  [Animate-X: Universal Character Image Animation with Enhanced Motion Representation](https://openreview.net/forum?id=1IuwdOI4Zb) |  | 0 |  | Biao Gong, Dandan Zheng, Jingdong Chen, Kecheng Zheng, Ming Yang, Ruobing Zheng, Shiwei Zhang, Shuai Tan, Xiang Wang |  |
| 3697 |  |  [Inner Information Analysis Algorithm for Deep Neural Network based on Community](https://openreview.net/forum?id=awz1JPyXNK) |  | 0 |  | Guipeng Lan, Jiabao Wen, Jiachen Yang, Meng Xi, Shuai Xiao |  |
| 3698 |  |  [AniSDF: Fused-Granularity Neural Surfaces with Anisotropic Encoding for High-Fidelity 3D Reconstruction](https://openreview.net/forum?id=v1f6c7wVBm) |  | 0 |  | Jingnan Gao, Xiaokang Yang, Yichao Yan, Zhuo Chen |  |
| 3699 |  |  [Scaling Large Language Model-based Multi-Agent Collaboration](https://openreview.net/forum?id=K3n5jPkrU6) |  | 0 |  | Chen Qian, Cheng Yang, Hanchen Xia, Kunlun Zhu, Maosong Sun, Wei Liu, Weize Chen, Yifei Wang, Yufan Dang, Zhiyuan Liu, Zhuoyun Du, Zihao Xie |  |
| 3700 |  |  [Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction](https://openreview.net/forum?id=stK7iOPH9Q) |  | 0 |  | Bingbing Liu, Haodong Li, Hongbo Zhang, Jing He, Kaiqiang Zhou, Leheng Li, Wei Yin, YingCong Chen, Yixun Liang |  |
| 3701 |  |  [ARB-LLM: Alternating Refined Binarizations for Large Language Models](https://openreview.net/forum?id=ZU8OdDLTts) |  | 0 |  | Dong Xie, Haotong Qin, Jiang Tian, Linghe Kong, Tianao Zhang, Xianglong Yan, Xiaokang Yang, Yulun Zhang, Zhiteng Li, Zhongchao Shi |  |
| 3702 |  |  [Understanding and Enhancing the Transferability of Jailbreaking Attacks](https://openreview.net/forum?id=asR9FVd4eL) |  | 0 |  | Bo Han, Fengwang Li, Runqi Lin, Tongliang Liu |  |
| 3703 |  |  [FLOPS: Forward Learning with OPtimal Sampling](https://openreview.net/forum?id=z1nSpA2dAW) |  | 0 |  | Guanghao Li, Jinyang Jiang, Mingqian Feng, Tao Ren, Yijie Peng, Zeliang Zhang, Zishi Zhang |  |
| 3704 |  |  [Storybooth: Training-Free Multi-Subject Consistency for Improved Visual Storytelling](https://openreview.net/forum?id=JZLon6cvx8) |  | 0 |  | Jaskirat Singh, Jonas Kohler, Junshen K. Chen, Michael F. Cohen |  |
