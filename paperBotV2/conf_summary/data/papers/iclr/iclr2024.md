# ICLR2024

## 会议论文列表

本会议共有 2455 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [The Second Tiny Papers Track at ICLR 2024, Tiny Papers @ ICLR 2024, Vienna, Austria, May 11, 2024](https://openreview.net/group?id=ICLR.cc/2024/TinyPapers) |  | 0 |  |  |  |
| 2 |  |  [Chemical Language Models Have Problems with Chemistry: A Case Study on Molecule Captioning Task](https://openreview.net/forum?id=JoO6mtCLHD) |  | 0 | Drug discovery has been greatly enhanced through the recent fusion of molecular sciences and natural language processing, leading these research fields to significant advancements. Considering the crucial role of molecule representation in chemical understanding within these models, we introduce novel probing tests designed to evaluate chemical knowledge of molecular structure in state-of-the-art language models (LMs), specifically MolT5 and Text+Chem T5. These probing tests are conducted on a molecule captioning task to gather evidence and insights into the language models' comprehension of chemical information. By applying rules to transform molecular SMILES into equivalent variants, we have observed significant differences in the natural language descriptions generated by the LM for a given molecule depending on the exact transformation used. | Veronika Ganeeva, Kuzma Khrabrov, Artur Kadurin, Andrey V. Savchenko, Elena Tutubalina |  |
| 3 |  |  [Towards Fairness constrained Restless Multi-Armed Bandits: a Case Study of Maternal and child Care Domain](https://openreview.net/forum?id=IfjStJ6GoX) |  | 0 | Restless multi-armed bandits (RMABs) are widely used for resource allocation in dynamic environments, but they typically do not consider fairness implications. This paper introduces a fairness-aware approach for offline RMABs. We propose a Kullback-Leibler (KL) divergence-based fairness metric to quantify the discrepancy between the selected and the overall population. This is incorporated as a regularizer into the soft whittle index optimization. We evaluate our fairness-aware algorithm on a real-world RMAB dataset where initial results suggest that our approach can potentially improve fairness while preserving solution quality. | Gargi Singh, Milind Tambe, Aparna Taneja |  |
| 4 |  |  [Utilizing Cross-Version Consistency for Domain Adaptation: A Case Study on Music Audio](https://openreview.net/forum?id=ZNg3YQQKWT) |  | 0 | Deep learning models are commonly trained on large annotated corpora, often in a specific domain. Generalization to another domain without annotated data is usually challenging. In this paper, we address such unsupervised domain adaptation based on the teacher--student learning paradigm. For improved efficacy in the target domain, we propose to exploit cross-version scenarios, i.e., corresponding data pairs assumed to obtain the same yet unknown labels. More specifically, our idea is to compare teacher annotations across versions and use only consistent annotations as labels to train the student model. Examples of cross-version data include the same text by different speakers (in speech recognition) or the same character by different writers (in handwritten text recognition). In our case study on music audio, versions are different recorded performances of the same composition, aligned with music synchronization techniques. Taking pitch estimation (a multi-label classification task) as an example task, we show that enforcing consistency across versions in student training helps to improve the transfer from a source domain (piano) to unseen and more complex target domains (singing/orchestra). | Lele Liu, Christof Weiss |  |
| 5 |  |  [Aligners: Decoupling LLMs and Alignment](https://openreview.net/forum?id=E6WukV41He) |  | 0 | Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion. We propose to decouple LLMs and alignment by training aligner models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance. Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria. We illustrate our method by training an \`\`ethical'' aligner and verify its efficacy empirically. | Lilian Ngweta, Mayank Agarwal, Subha Maity, Alex Gittens, Yuekai Sun, Mikhail Yurochkin |  |
| 6 |  |  [When is RL better than DPO in RLHF? A Representation and Optimization Perspective](https://openreview.net/forum?id=lNEFatlsQb) |  | 0 | Aligning large language models with human preferences is important, and there are two kinds of alignment methods. The first class of algorithms is based on reinforcement learning (RL), which involves learning a reward function from a human preference dataset and improving performance via online reward maximization. Another class is characterized by direct preference optimization, exemplified by DPO (Rafailov et al., 2023), which learns an implicit reward and improves performance directly using a static offline dataset. Which algorithm performs well? We investigate this question using contextual bandits, which serve as mathematical models for alignment. We have two findings: First, we show that DPO may suffer from a reward quality issue when the feature representation is misspecified. Second, we present the error bounds for RL algorithms and show that they achieve the best improvement when the online updates are sufficient. The code to reproduce our results is available at https://github.com/liziniu/policy_optimization. | Ziniu Li, Tian Xu, Yang Yu |  |
| 7 |  |  [Is Watermarking LLM-Generated Code Robust?](https://openreview.net/forum?id=8PhI1PzSYY) |  | 0 | We present the first study of the robustness of existing watermarking techniques on Python code generated by large language models. Although existing works showed that watermarking can be robust for natural language text, we show that it is easy to remove these watermarks on code by simple semantic-preserving transformations. | Tarun Suresh, Shubham Ugare, Gagandeep Singh, Sasa Misailovic |  |
| 8 |  |  [Lost in Translation: GANs' Inability to Generate Simple Probability Distributions](https://openreview.net/forum?id=MUOmyZMEd4) |  | 0 | Since its inception, Generative Adversarial Networks (GAN) have marked a triumph in generative modeling. Its impeccable capacity to mimic observations from unknown probability distributions has positioned it as a widely used simulation tool. In typical applications, GANs find themselves simulating data rich in semantic information such as images or text out of random noise. As such, it is reasonable to expect that large parametric models such as GANs must be able to estimate standard theoretical probability densities with ease. In this paper, based on a series of disillusioning experimental findings, we show that GANs often fail to induce the simplest of statistical transformations between distributions. For example, starting with a standard Gaussian noise, GANs with 2-deep generators are unable to perform a positional translation. Supporting theoretical tests on generated data further corroborates our rather unsettling conclusions. | Debanjan Dutta, Anish Chakrabarty, Swagatam Das |  |
| 9 |  |  [CMFPN: Context Modeling Meets Feature Pyramid Network](https://openreview.net/forum?id=Qtu2Od1ggw) |  | 0 | Feature fusion is a powerful technique that enables predictors to access a semantically rich representation of an image. Feature Pyramid Networks (FPNs) are the most widely used models for fusing features. However, the context within the FPN layers is inconsistent, leading to false predictions. This article addresses the context inconsistency in FPN and proposes CMFPN, a new design that improves feature fusion by decoupling feature aggregation from context modeling. Experimental results, based on the COCO dataset, show that CMFPN effectively resolves the context issues and enhances the Average Precision (AP) results for both object detection and instance segmentation by $2.30\%$ and $1.7\%$, respectively. | Faroq AlTam, Muhammad AlQurishi, Thariq Khalid Kadavil, Riad Souissi |  |
| 10 |  |  [Enhancing Drug-Drug Interaction Prediction with Context-Aware Architecture](https://openreview.net/forum?id=e2Bkf1Bzh4) |  | 0 | In the field of disease treatment, the simultaneous use of multiple medications can lead to unforeseen adverse reactions, compromising patient safety and therapeutic efficacy. Consequently, predicting drug-drug interactions (DDIs) has emerged as a pivotal research focus on improving disease treatment. While recent advancements have been made in deep learning models for predicting drug pair relations, the nuanced consideration of individual or cellular conditions as influential contextual factors in DDIs is notably lacking. In this study, leveraging existing models, we introduce a methodology to predict DDIs through a context-aware architecture. The evident performance improvement compared to established methodologies underscores the crucial role of the context-aware mechanism in addressing context-conditional DDIs. Furthermore, we perform a systematic ablation analysis to assess the impact of model elements. Simultaneously, we also investigate the potential of incorporating pre-trained molecular representation learning models in this domain. | Yijingxiu Lu, Yinhua Piao, Sun Kim |  |
| 11 |  |  [Can LLMs Learn a New Language on the Fly? A Case Study on Zhuang](https://openreview.net/forum?id=GTHD2UnDIb) |  | 0 | Existing large language models still fail to support many low-resource languages. Especially for the extremely low-resource ones, there is hardly any training data to effectively update the model parameters. We thus investigate whether LLMs can learn a new language on the fly through in-context learning prompting. To study this question, we collect a tiny parallel corpus for Zhuang, a language supported by no LLMs currently. We study the performance of various LLMs on the Zhuang-Chinese translation task and find out the great potential of this learning paradigm. | Chen Zhang, Mingxu Tao, Quzhe Huang, Zhibin Chen, Yansong Feng |  |
| 12 |  |  [Weighted Branch Aggregation Based Deep Learning Model for Track Detection in Autonomous Racing](https://openreview.net/forum?id=K3ilD3QhX6) |  | 0 | Intelligent track detection is a vital component of autonomous racing cars. We develop a novel Weighted Branch Aggregation based Convolutional Neural Network (WeBACNN) model that can accurately detect the track while being robust against image blurring due to high speed, and can work independently of lane markings. The code and dataset for this work is available at (anonymous). | Shreya Ghosh, YiHuan Chen, ChingHsiang Huang, Abu Shafin Mohammad Mahdee Jameel, Aly El Gamal, Samuel Labi |  |
| 13 |  |  [Visual prompting Methods for GPT-4V based Zero-Shot Graphic Layout Design Generation](https://openreview.net/forum?id=DI4gETm7aa) |  | 0 | Graphic layout design generation is a challenging problem in computer vision. The key aspect of the challenge is ensuring coherent placement of textual elements on the background image to ensure aesthetic appeal and avoiding occlusion of key visual elements. Although prior methods have made attempts to solve this multi-modal problem, they couldn't perfect it. Owing to the complexity required in understanding the relationship between visual and text elements in the aforementioned task, we investigate GPT-4-Vision(GPT-4V), a large multimodal models(LMMs), to do zero-shot graphic layout design generation in a versatile manner. Our approach explores various off-the-shelf segmentation/superpixel methods to identify and mark the key regions to visually augment the image to enhance GPT-4V's spatial reasoning capability . The results of our comprehensive experiments on a self-curated dataset demonstrates the efficacy of our proposed visual prompting methods, showing improvement over standard GPT-4V prompting method and also performing at par and even better, for some techniques, than state-of-the-art specialist model.The code and data is available at https://anonymous.4open.science/r/VISUAL-PROMPTING-TECHNIQUES-FOR-GPT-4V-BASED-ZERO-SHOT-GRAPHIC-LAYOUT-DESIGN-GENERATION-5A6E | Kunal Singh, Mukund Khanna, Ankan Biswas, Pradeep Moturi, Shivam |  |
| 14 |  |  [Exploring the Limits of Semantic Image Compression at Micro-bits per Pixel](https://openreview.net/forum?id=sfwtoH5GdD) |  | 0 | Traditional methods, such as JPEG, perform image compression by operating on structural information, such as pixel values or frequency content. These methods are effective to bitrates around one bit per pixel (bpp) and higher at standard image sizes. However, to compress further text-based semantic compression directly stores concepts and their relationships using natural language, which has evolved with humans to efficiently represent these salient concepts. These methods can operate at extremely low bitrates by disregarding structural information like location, size, and orientation. In this work, we use GPT-4V and DALL-E3 from OpenAI to explore the quality-compression frontier for image compression and identify the limitations with current technology. We push semantic compression as low as 100 μbpp (up to 10,000× smaller than JPEG) by introducing an iterative reflection process to improve the decoded image. We further hypothesize this 100 μbpp level represents a soft limit on semantic compression at standard image resolutions. | Jordan Dotzel, Bahaa Kotb, James Dotzel, Mohamed S. Abdelfattah, Zhiru Zhang |  |
| 15 |  |  [Evaluating Groups of Features via Consistency, Contiguity, and Stability](https://openreview.net/forum?id=IP2etbIEuC) |  | 0 | Feature attributions explain model predictions by assigning importance scores to input features. In high-dimensional data such as images, these scores are often assigned to groups of features. There are various strategies for creating these groups, ranging from simple patches to deep-learning-based algorithms. Which group should be used for explanation? We formally define three key criteria for interpretable groups of features: consistency, contiguity, and stability. We find that patch-based groups outperform groups created via modern segmentation tools. | Chaehyeon Kim, Weiqiu You, Shreya Havaldar, Eric Wong |  |
| 16 |  |  [What Does a Visual Formal Analysis of the World's 500 Most Famous Paintings Tell Us About Multimodal LLMs?](https://openreview.net/forum?id=dINMwL186O) |  | 0 | This work introduces ArtQA, a new benchmark for multimodal LLMs through the lens of formal analysis of paintings. We focus on key elements such as line, shape, space, color, form, value, and texture—collectively referred to as the elements of art in visual formal analysis. ArtQA contains questions spanning 4 metrics, further divided into 16 fine-grained categories. We leverage the power of LLMs to generate VQA questions based on formal analysis of 500 renowned paintings. These questions undergo a rigorous filtering process by both model annotation and human experts, ensuring ArtQA's quality and reliability. | Muzi Tao, Saining Xie |  |
| 17 |  |  [Revamp: Automated Simulations of Adversarial Attacks on Arbitrary Objects in Realistic Scenes](https://openreview.net/forum?id=XCLrySEUBe) |  | 0 | Deep learning models, such as those used in autonomous vehicles are vulnerable to adversarial attacks where attackers could place adversarial objects in the environment to induce incorrect detections. While generating such adversarial objects in the digital realm is well-studied, successfully transferring these attacks to the physical realm remains challenging, especially when accounting for real-world environmental factors. We address these challenges with REVAMP, a first-of-its-kind Python library for creating attack scenarios with arbitrary objects in scenes with realistic environmental factors, lighting, reflection, and refraction. REVAMP empowers researchers and practitioners to swiftly explore diverse scenarios, offering a wide range of configurable options for experiment design and using differentiable rendering to replicate physically-plausible adversarial objects. REVAMP is open-source and available at https://github.com/poloclub/revamp and a demo video is available at https://youtu.be/NA0XR0XkS1E. | Matthew Hull, Zijie J. Wang, Duen Horng Chau |  |
| 18 |  |  [Hard ASH: Sparsity and the right optimizer make a continual learner](https://openreview.net/forum?id=WwQKl1OrMX) |  | 0 | In class incremental learning, neural networks typically suffer from catastrophic forgetting. We show that an MLP featuring a sparse activation function and an adaptive learning rate optimizer can compete with established regularization techniques in the Split-MNIST task. We highlight the effectiveness of the Adaptive SwisH (ASH) activation function in this context and introduce a novel variant, Hard Adaptive SwisH (Hard ASH) to further enhance the learning retention. | Santtu Keskinen |  |
| 19 |  |  [Analog In-Memory Computing with Uncertainty Quantification for Efficient Edge-based Medical Imaging Segmentation](https://openreview.net/forum?id=hvp5I4dDya) |  | 0 | This work investigates the role of the emerging Analog In-memory computing (AIMC) paradigm in enabling Medical AI analysis and improving the certainty of these models at the edge. It contrasts AIMC's efficiency with traditional digital computing's limitations in power, speed, and scalability. Our comprehensive evaluation focuses on brain tumor analysis, spleen segmentation, and nuclei detection. The study highlights the superior robustness of isotropic architectures, which exhibit a minimal accuracy drop (0.04) in analog-aware training, compared to significant drops (up to 0.15) in pyramidal structures. Additionally, the paper emphasizes IMC's effective data pipelining, reducing latency and increasing throughput as well as the exploitation of inherent noise within AIMC, strategically harnessed to augment model certainty. | Imane Hamzaoui, Hadjer Benmeziane, Zayneb Cherif, Kaoutar El Maghraoui |  |
| 20 |  |  [VoltaVision: A Transfer Learning model for electronic component classification](https://openreview.net/forum?id=JHTqFvmVYz) |  | 0 | In this paper, we analyze the effectiveness of transfer learning on classifying electronic components. Transfer learning reuses pre-trained models to save time and resources in building a robust classifier rather than learning from scratch. Our work introduces a lightweight CNN, coined as VoltaVision, and compares its performance against more complex models. We test the hypothesis that transferring knowledge from a similar task to our target domain yields better results than state-of-the-art models trained on general datasets. Our dataset and code for this work are available at https://github.com/AnasIshfaque/VoltaVision. | Anas Mohammad Ishfaqul Muktadir Osmani, Taimur Rahman, Salekul Islam |  |
| 21 |  |  [Training Mixture-of-Experts: A Focus on Expert-Token Matching](https://openreview.net/forum?id=UJgSQjBWZS) |  | 0 | Recent advancements in sparse Mixture-of-Experts (MoE) models, particularly in the Vision MoE (VMoE) framework, have demonstrated promising results in enhancing vision task performance. However, a key challenge persists in optimally routing tokens (such as image patches) to the right experts, without incurring excessive computational costs. Addressing this, we apply the regularized optimal transport, which relies on the Sinkhorn algorithm to the Vision MoE (VMoE) framework, aiming at improving the token-expert matching process. The resulting model, Sinkhorn-VMoE (SVMoE), represents a meaningful step in optimizing efficiency and effectiveness of sparsely-gated MoE models. | Fateme Vesaghati, Masoumeh Zareapoor |  |
| 22 |  |  [Learning Disentangled Audio Representations through Controlled Synthesis](https://openreview.net/forum?id=Fn9ORH8PLl) |  | 0 | This paper tackles the scarcity of benchmarking data in disentangled auditory representation learning. We introduce \*\*SynTone\*\*, a synthetic dataset with explicit ground truth explanatory factors for evaluating disentanglement techniques. Benchmarking state-of-the-art methods on SynTone highlights its utility for method evaluation. Our results underscore strengths and limitations in audio disentanglement, motivating future research. | Yusuf Brima, Ulf Krumnack, Simone Pika, Gunther Heidemann |  |
| 23 |  |  [Collapse of Self-trained Language Models](https://openreview.net/forum?id=DeuBfEWyR4) |  | 0 | In various fields of knowledge creation, including science, new ideas often build on pre-existing information. In this work, we explore this concept within the context of language models. Specifically, we explore the potential of self-training models on their own outputs, akin to how humans learn and build on their previous thoughts and actions. While this approach is intuitively appealing, our research reveals its practical limitations. We find that extended self-training of the GPT-2 model leads to a significant degradation in performance, resulting in repetitive and collapsed token output. | David Herel, Tomás Mikolov |  |
| 24 |  |  [KFC: Knowledge Reconstruction and Feedback Consolidation Enable Efficient and Effective Continual Generative Learning](https://openreview.net/forum?id=pVTcR8ig3R) |  | 0 | To address the issues of catastrophic forgetting in Continual Generative Learning (CGL), dominant methods leverage the generative replay strategy. However, they often suffer from high time complexity and inferior generative sample quality. In this work, we develop an efficient and effective CGL method via \*K\*nowledge reconstruction and \*F\*eedback \*C\*onsolidation (\*KFC\*). KFC extends the inherent data reconstruction properties of the variational autoencoder framework to historical knowledge reconstruction and re-encodes the current task's reconstructed data to the same posterior distribution as the original data. Experiments showcase that KFC achieves state-of-the-art performances in time complexity, sample quality, and accuracy on various CGL tasks. Code is in github.com/libo-huang/KFC. | Libo Huang, Zhulin An, Yan Zeng, Xiang Zhi, Yongjun Xu |  |
| 25 |  |  [Rescaling Intermediate Features Makes Trained Consistency Models Perform Better](https://openreview.net/forum?id=1o3LnwflAl) |  | 0 | In the domain of deep generative models, diffusion models are renowned for their high-quality image generation but are constrained by intensive computational demands. To mitigate this, consistency models have been proposed as a computationally efficient alternative. Our research reveals that post-training rescaling of internal features can enhance the one-step sample quality of these models without incurring detectable computational overhead. This optimization is evidenced by an obvious improvement in Fr&eacute;chet Inception Distance (FID). For example, with our rescaled consistency distillation (CD) model, FID on the ImageNet dataset reduces from 6.2 to 5.2, on the LSUN-cat dataset from 10.9 to 9.5. Closer inspection of the generated images reveals that this enhancement may originate from improved visual details and clarity. | Junyi Zhu, Zinan Lin, Enshu Liu, Xuefei Ning, Matthew B. Blaschko |  |
| 26 |  |  [DFWLayer: Differentiable Frank-Wolfe Optimization Layer](https://openreview.net/forum?id=XoPQqUctS1) |  | 0 | Differentiable optimization has received a significant amount of attention due to its foundational role in the domain of machine learning based on neural networks. This paper proposes a differentiable layer, named Differentiable Frank-Wolfe Layer (DFWLayer), by rolling out the Frank-Wolfe method, a well-known optimization algorithm which can solve constrained optimization problems without projections and Hessian matrix computations, thus leading to an efficient way of dealing with large-scale convex optimization problems with norm constraints. Experimental results demonstrate that the DFWLayer not only attains competitive accuracy in solutions and gradients but also consistently adheres to constraints. | Zixuan Liu, Liu Liu, Xueqian Wang, Peilin Zhao |  |
| 27 |  |  [3D Shape Completion via Sparse Irregular Representation](https://openreview.net/forum?id=mBQYpnq3Pj) |  | 0 | The task of 3D shape completion involves completing missing regions of an object from partial observation. The current methods accomplish this task by modeling latent completion distributions based on an autoregressive model. However, this approach often struggles with geometric details, as it represents 3D shapes with variable latent sequences, leading to gaps (local missing) in the completed shape. In this paper, we introduce a multiple 3D shape completion method using a transformer-based autoregressive model and a fixed-length sparse irregular latent sequence. Experiments demonstrate that our method outperforms state-of-the-art methods in terms of both quality and fidelity. | Jiahui Li, Pourya Shamsolmoali |  |
| 28 |  |  [Dissecting Zero-Shot Visual Reasoning Capabilities in Vision and Language Models](https://openreview.net/forum?id=jM1nZDBIto) |  | 0 | Vision-language models (VLMs) have shown impressive zero- and few-shot performance on real-world visual question answering (VQA) benchmarks, alluding to their capabilities as visual reasoning engines. However, existing works (typically) use benchmarks that conflate “pure” visual reasoning with world knowledge, and also have questions that involve a limited number of reasoning steps. Thus, it remains unclear whether a VLM’s apparent visual reasoning performance is due to its world knowledge, or due to actual visual reasoning capabilities. To clarify this ambiguity, we systematically benchmark and dissect the zero-shot visual reasoning capabilities of VLMs through synthetic datasets that require minimal world knowledge, and allow for analysis over a broad range of reasoning steps. We specifically focus on evaluating the impact of conveying scene information as either visual embeddings or purely textual scene descriptions to the underlying large language model (LLM) of the VLM. We notably find that the underlying LLMs, when provided textual scene descriptions, consistently perform significantly better compared to being provided visual embeddings. Our work comprehensively identifies limitations of VLMs for compositional visual reasoning, and highlights the important role that LLMs can play in scene understanding and visual reasoning. | Aishik Nagar, Shantanu Jaiswal, Cheston Tan |  |
| 29 |  |  [MosquitoFusion: A Multiclass Dataset for Real-Time Detection of Mosquitoes, Swarms, and Breeding Sites Using Deep Learning](https://openreview.net/forum?id=3s4hFx8pYs) |  | 0 | In this paper, we present an integrated approach to real-time mosquito detection using our multiclass dataset (MosquitoFusion) containing 1204 diverse images and leverage cutting-edge technologies, specifically computer vision, to automate the identification of Mosquitoes, Swarms, and Breeding Sites. The pre-trained YOLOv8 model, trained on this dataset, achieved a mean Average Precision (mAP@50) of 57.1%, with precision at 73.4% and recall at 50.5%. The integration of Geographic Information Systems (GIS) further enriches the depth of our analysis, providing valuable insights into spatial patterns. The dataset and code are available at https://github.com/faiyazabdullah/MosquitoFusion. | Md. Faiyaz Abdullah Sayeedi, Fahim Hafiz, Md Ashiqur Rahman |  |
| 30 |  |  [Nonlinear model reduction for operator learning](https://openreview.net/forum?id=Jw6TUpB7Rw) |  | 0 | Operator learning provides methods to approximate mappings between infinite-dimensional function spaces. Deep operator networks (DeepONets) are a notable architecture in this field. Recently, an extension of DeepONet based on model reduction and neural networks, proper orthogonal decomposition (POD)-DeepONet, has been able to outperform other architectures in terms of accuracy for several benchmark tests. We extend this idea towards nonlinear model order reduction by proposing an efficient framework that combines neural networks with kernel principal component analysis (KPCA) for operator learning. Our results demonstrate the superior performance of KPCA-DeepONet over POD-DeepONet. | Hamidreza Eivazi, Stefan H. A. Wittek, Andreas Rausch |  |
| 31 |  |  [Logic-guided Deep Reinforcement Learning for Stock Trading](https://openreview.net/forum?id=SPDNi5Ys8R) |  | 0 | Previous state-of-the-art trading strategy proposes using ensemble reinforcement learning to combine the advantages of different subpolicies. Despite its improved performance, we observe that this policy is still quite sensitive to market volatility. In this work, we propose a novel framework called SYENS (Program Synthesis-based Ensemble Strategy) which aims to improve the trading strategy's robustness via the program synthesis by sketching paradigm. SYENS is a hierarchical strategy that uses a program sketch as the high-level strategy. The program sketch embeds human expert knowledge of market trends. And based on the program sketch, we adopt the program synthesis by sketching paradigm to synthesize the detailed ensemble strategy. Experimental results demonstrate that SYENS achieves the highest return while retaining low drawdown. | Zhiming Li, Junzhe Jiang, Yushi Cao, Aixin Cui, Bozhi Wu, Bo Li, Yang Liu |  |
| 32 |  |  [A Shared Encoder for Multi-Source Hyperspectral Images](https://openreview.net/forum?id=dIThDxLwA1) |  | 0 | Multi-source hyperspectral images(HSIs) which captured from diverse sensors commonly possess varying bands. When employing deep learning techniques for their processing, individual models are necessitated for each source due to the disparate dimensions. To tackle this problem, we propose a shared encoder to project all HSIs into a unified feature space. It establishes a general framework for the representation of multi-source HSIs, providing foundational conditions for the development of a universal HSI analysis model. | Weili Kong, Baisen Liu, Xiaojun Bi, Jiaming Pei |  |
| 33 |  |  [Transfer Learning for Global Feature Importance Measurements](https://openreview.net/forum?id=EExn3iNKs3) |  | 0 | Understanding feature importance is crucial for conducting interpretable clinical decision-making. However, the reliability of such analyses can be heavily impacted by the available sample size, placing sites with lower data quality and smaller sample sizes at inherent disadvantages. To address the challenge, we propose a model-agnostic transfer learning-based approach for feature importance measurement and evaluate its effectiveness using real-world heterogeneous electronic health records. | Xin Li, Siqi Li, Qiming Wu, Kunyu Yu |  |
| 34 |  |  [Enhancing Spiking Transformers with Binary Attention Mechanisms](https://openreview.net/forum?id=6X3TNqLb5t) |  | 0 | Spiking Neural Networks (SNNs) are increasingly recognized as an efficient alternative to traditional artificial neural networks. Recent advancements, particularly the integration of SNNs with Transformer structures to create 'SpikFormer', have significantly enhanced the performance of SNNs. However, the current non-spiking form of attention in SpikFormer poses risks of attention value explosion and still results in high computational costs for SNNs. To address this issue, we propose a novel binary attention mechanism. By introducing an attention shift mechanism and adaptive thresholds for neurons, we have successfully binarized the attention matrices in SpikFormer, leading to more efficient and sparser spiking neural networks. Experiments on image and neuromorphic datasets demonstrate that our approach maintains comparable performance to the original SpikFormer while reducing computational costs. | Guobin Shen, Dongcheng Zhao, Sicheng Shen, Yi Zeng |  |
| 35 |  |  [Key Patch Proposer: Key Patches Contain Rich Information](https://openreview.net/forum?id=NHm3OB6bIG) |  | 0 | In this paper, we introduce a novel algorithm named Key Patch Proposer (KPP) designed to select key patches in an image without additional training. Our experiments showcase KPP's robust capacity to capture semantic information by both reconstruction and classification tasks. The efficacy of KPP suggests its potential application in active learning for semantic segmentation. Our source code is publicly available at https://github.com/CA-TT-AC/key-patch-proposer. | Jing Xu, Beiwen Tian, Hao Zhao |  |
| 36 |  |  [Hallucination Benchmark in Medical Visual Question Answering](https://openreview.net/forum?id=vxlXqOj4zv) |  | 0 | The recent success of large language and vision models (LLVMs) on vision question answering (VQA), particularly their applications in medicine (Med-VQA), has shown a great potential of realizing effective visual assistants for healthcare. However, these models are not extensively tested on the hallucination phenomenon in clinical settings. Here, we created a hallucination benchmark of medical images paired with question-answer sets and conducted a comprehensive evaluation of the state-of-the-art models. The study provides an in-depth analysis of current models' limitations and reveals the effectiveness of various prompting strategies. | Jinge Wu, Yunsoo Kim, Honghan Wu |  |
| 37 |  |  [Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian Perspective](https://openreview.net/forum?id=AfVtVrCH9U) |  | 0 | This paper proposes an interpretation of RLAIF as Bayesian inference by introducing distilled Self-Critique (dSC), which refines the outputs of a LLM through a Gibbs sampler that is later distilled into a fine-tuned model. Only requiring synthetic data, dSC is exercised in experiments regarding safety, sentiment, and privacy control, showing it can be a viable and cheap alternative to align LLMs. Code released at https://github.com/vicgalle/distilled-self-critique. | Víctor Gallego |  |
| 38 |  |  [Parameter and Data Efficient Spectral Style-DCGAN](https://openreview.net/forum?id=JQlfTmlHNz) |  | 0 | We present a simple, highly parameter, and data-efficient adversarial network for unconditional face generation. Our method: Spectral Style-DCGAN or SSD utilizes only 6.574 million parameters and 4739 dog faces from the Animal Faces HQ (AFHQ) dataset as training samples while preserving fidelity at low resolutions up to 64x64. Code available at Anonymous-repo. | Aryan Garg |  |
| 39 |  |  [G-PECNet: Towards a Generalizable Pedestrian Trajectory Prediction System](https://openreview.net/forum?id=wSWk1sNK0m) |  | 0 | Navigating dynamic physical environments without obstructing or damaging human assets is of quintessential importance for social robots. In this work, we solve autonomous drone navigation's sub-problem of predicting out-of-domain human and agent trajectories using a deep generative model. Our method: General-PECNet or G-PECNet observes an improvement of $9.5$% on the Final Displacement Error (FDE) on 2020's benchmark: PECNet through a combination of architectural improvements inspired by periodic activation functions and synthetic trajectory or data augmentations using hidden markov modeling and reinforcement learning based agents. Additionally, we propose a simple geometry-inspired loss and evaluation metric for trajectory non-linearity analysis. Code available at [Anonymous-repository](https://github.com/ANonyMouxe/GPECNet) | Aryan Garg, Renu Rameshan |  |
| 40 |  |  [Colorful Cutout: Enhancing Image Data Augmentation with Curriculum Learning](https://openreview.net/forum?id=mTF4OcDnSP) |  | 0 | Data augmentation is one of the regularization strategies for the training of deep learning models, which enhances generalizability and prevents overfitting, leading to performance improvement. Although researchers have proposed various data augmentation techniques, they often lack consideration for the difficulty of augmented data. Recently, another line of research suggests incorporating the concept of curriculum learning with data augmentation in the field of natural language processing. In this study, we adopt curriculum data augmentation for image data augmentation and propose \*\*colorful cutout\*\*, which gradually increases the noise and difficulty introduced in the augmented image. Our experimental results highlight the possibility of curriculum data augmentation for image data. We publicly released our source code to improve the reproducibility of our study. | Juhwan Choi, YoungBin Kim |  |
| 41 |  |  [On Difficulties of Attention Factorization through Shared Memory](https://openreview.net/forum?id=pexcddsXGY) |  | 0 | Transformers have revolutionized deep learning in numerous fields, including natural language processing, computer vision, and audio processing. Their strength lies in their attention mechanism, which allows for the discovering of complex input relationships. However, this mechanism's quadratic time and memory complexity pose challenges for larger inputs. Researchers are now investigating models like Linear Unified Nested Attention (Luna) or Memory Augmented Transformer, which leverage external learnable memory to either reduce the attention computation complexity down to linear, or to propagate information between chunks in chunk-wise processing. Our findings challenge the conventional thinking on these models, revealing that interfacing with the memory directly through an attention operation is suboptimal, and that the performance may be considerably improved by filtering the input signal before communicating with memory. | Uladzislau Yorsh, Martin Holena, Ondrej Bojar, David Herel |  |
| 42 |  |  [Bad Predictive Coding Activation Functions](https://openreview.net/forum?id=5Vc1ACZ8eC) |  | 0 | We investigate predictive coding networks (PCNs) by analyzing their performance under different activation function choices. We expand a previous theoretical discussion of a simple toy example of PCN in the training stage. Compared to classic gradient-based empirical risk minimization, we observe differences for the ReLU activation function. This leads us to carry out an empirical evaluation of classification tasks on FashionMNIST, CIFAR-10. We show that while ReLU might be a good baseline for classic machine learning, for predictive coding, it performs worse than other activation functions while also leading to the largest drop in performance compared to gradient-based empirical risk minimization. | Simon Frieder, Luca Pinchetti, Thomas Lukasiewicz |  |
| 43 |  |  [An Evaluation Benchmark for Autoformalization in Lean4](https://openreview.net/forum?id=22ITxc8y5p) |  | 0 | In the advancing field of computational mathematics, Large Language Models (LLMs) hold the potential to revolutionize autoformalization, a process crucial across various disciplines. The introduction of Lean4, a mathematical programming language, presents an unprecedented opportunity to rigorously assess the autoformalization capabilities of LLMs. This paper introduces a novel evaluation benchmark designed for Lean4, applying it to test the abilities of state-of-the-art LLMs, including GPT-3.5, GPT-4, and Gemini Pro. Our comprehensive analysis reveals that, despite recent advancements, these LLMs still exhibit limitations in autoformalization, particularly in more complex areas of mathematics. These findings underscore the need for further development in LLMs to fully harness their potential in scientific research and development. This study not only benchmarks current LLM capabilities but also sets the stage for future enhancements in the field of autoformalization. | Jasdeep Sidhu, Shubhra Mishra, Aryan Gulati, Devanshu Ladsaria, Brando Miranda |  |
| 44 |  |  [Software 1.0 Strengths for Interpretability and Data Efficiency](https://openreview.net/forum?id=gyl8r8ANcd) |  | 0 | Machine learning has demonstrated remarkable capabilities across various tasks, yet it confronts significant challenges such as limited interpretability, reliance on extensive data, and difficulties in incorporating human intuition. In contrast, traditional software development avoids these pitfalls, offering full interpretability, less data dependency, and easy integration of intuitive decision-making. To have the strengths of both approaches, we introduce the BasedOn library. This tool focuses on code written by programmers while providing very simple interfaces to let programmers use machine learning. The BasedOn library, leveraging policy gradient methods, offers "learnable" if statements. | Maral Jabbarishiviari, Arshia Soltani Moakhar |  |
| 45 |  |  [Sailing Through Spectra: Unveiling the Potential of Multi-Spectral Information in Marine Debris Segmentation](https://openreview.net/forum?id=tJPLJS97X4) |  | 0 | Plastic debris in ocean waters poses ecological and economic challenges. Addressing this issue begins with estimating plastic distribution in oceans for effective policy and awareness efforts. Traditional monitoring methods are costly and labour-intensive, with limited coverage. Deep learning models using multispectral remote sensing data show promise in overcoming these limitations. However, accurately distinguishing floating plastic from other sea surface features remains challenging. In our work, we use the multi-spectral Sentinel-2 MARIDA dataset to explore the impact of various spectral feature combinations on the performance of deep learning models for segmenting marine plastic in the presence of other sea surface features. This innovative approach improves accuracy and serves as an open benchmark for multi-spectral marine debris segmentation. | Dyutit Mohanty, Aditya Kasliwal, Bharath Udupa, Pratinav Seth |  |
| 46 |  |  [Network Inversion of Binarised Neural Nets](https://openreview.net/forum?id=zKcB0vb7qd) |  | 0 | While the deployment of neural networks, yielding impressive results, becomes more prevalent in various applications, their interpretability and understanding remain a critical challenge. Network inversion, a technique that aims to reconstruct the input space from the model’s learned internal representations, plays a pivotal role in unraveling the black-box nature of input to output mappings in neural networks. In safety-critical scenarios, where model outputs may influence pivotal decisions, the integrity of the corresponding input space is paramount, necessitating the elimination of any extraneous ”garbage” to ensure the trustworthiness of the network. Binarised Neural Networks (BNNs), characterized by binary weights and activations, offer computational efficiency and reduced memory requirements, making them suitable for resource-constrained environments. This paper introduces a novel approach to invert a trained BNN by encoding it into a CNF formula that captures the network’s structure, allowing for both inference and inversion. | Pirzada Suhail |  |
| 47 |  |  [Uncovering Bias: Exploring Gender Dynamics in Distance-Aware Mixup Techniques](https://openreview.net/forum?id=n8Yv4R6132) |  | 0 | Bias is a pervasive issue in machine learning and has implications in multiple AI applications, encompassing dimensions like gender, age, demographics, and social aspects. Complex models, including deep neural networks, transformers etc., often inherit biases and stereotypes during training, attributable to selection bias within training data and algorithmic creation processes. Augmentation techniques like Mixup exhibit promising potential as debiasing frameworks, leveraging specialized sampling strategies and spatial information for bias mitigation. In this study, we evaluate gender bias within the distance-aware mixing frameworks, while exploring diverse sampling strategies for mixup. Using the Trustpilot corpus, we conduct experiments quantitatively analyzing bias as error disparity, investigating the impact of distance thresholds and various gender-based criteria on mixup operations. Our quantitative analysis indicates that employing a cross-gender mixup strategy yields the most effective bias reduction. We also release the code for our work. | Samyak Jain, Parth Chhabra, Ramit Sawhney |  |
| 48 |  |  [Empirical Study on Updating Key-Value Memories in Transformer Feed-forward Layers](https://openreview.net/forum?id=WSl84nwG7i) |  | 0 | The feed-forward networks (FFNs) in transformers are recognized as a group of key-value neural memories to restore abstract high-level knowledge. In this work, we conduct an empirical ablation study on updating keys (the 1st layer in the FFNs layer) or values (the 2nd layer in the FFNs layer). We compare those two methods in various knowledge editing and fine-tuning tasks of large language models to draw insights to understand FFNs further. Code is available at \href{https://github.com/qiuzh20/Tuning-keys-v.s.-values}{this repo}. | Zihan Qiu, Zeyu Huang, Youcheng Huang, Jie Fu |  |
| 49 |  |  [When SMILES have Language: Drug Classification using Text Classification Methods on Drug SMILES Strings](https://openreview.net/forum?id=VUYCyH8fCw) |  | 0 | Complex chemical structures, like drugs, are usually defined by SMILES strings as a sequence of molecules and bonds. These SMILES strings are used in different complex machine learning-based drug-related research and representation works. Escaping from complex representation, in this work, we pose a single question: What if we treat drug SMILES as conventional sentences and engage in text classification for drug classification? Our experiments affirm the possibility with very competitive scores. The study explores the notion of viewing each atom and bond as sentence components, employing basic NLP methods to categorize drug types, proving that complex problems can also be solved with simpler perspectives. The data and code are available here: https://github.com/azminewasi/Drug-Classification-NLP. | Azmine Toushik Wasi, Karlo Serbetar, Raima Islam, Taki Hasan Rafi, DongKyu Chae |  |
| 50 |  |  [Multilingual Prosody Transfer: Comparing Supervised & Transfer Learning](https://openreview.net/forum?id=DKF7YCwCmd) |  | 0 | The field of prosody transfer in speech synthesis systems is rapidly advancing. This research is focused on evaluating learning methods for adapting pre-trained monolingual text-to-speech (TTS) models to multilingual conditions, i.e., Supervised Fine-Tuning (SFT) and Transfer Learning (TL). This comparison utilizes three distinct metrics: Mean Opinion Score (MOS), Recognition Accuracy (RA), and Mel Cepstral Distortion (MCD). Results demonstrate that, in comparison to SFT, TL leads to significantly enhanced performance, with an average MOS higher by 1.53 points, a 37.5\% increase in RA, and approximately a 7.8-point improvement in MCD. These findings are instrumental in helping build TTS models for low-resource languages. | Arnav Goel, Medha Hira, Anubha Gupta |  |
| 51 |  |  [Region Mixup](https://openreview.net/forum?id=2qPBw8OuU7) |  | 0 | This paper introduces a simple extension of mixup data augmentation to enhance generalization in visual recognition tasks. Unlike the vanilla mixup method, which blends entire images, our approach focuses on combining regions from multiple images. | Saptarshi Saha, Utpal Garain |  |
| 52 |  |  [Design of a molecular exchange-based robust perceptron for biomolecular neural network](https://openreview.net/forum?id=AXwGBliKOV) |  | 0 | A molecular perceptron is of immense interest due to its computing and classification ability in biophysical and aqueous environments. Because such a perceptron relies on biochemical interactions, it must adapt to perturbations and be resilient against stochastic fluctuations to maintain faithful in vivo classification. In this paper, we design a molecular exchange mechanism (MEM)-based perceptron following a set of evolutionarily preserved in vivo signaling steps, including negative feedback known for noise regulation. The efficacy study of the MEM-perceptron demonstrates an improved adaptation against perturbations and noise. | Moshiur Rahman, Muhtasim Ishmum Khan, Md. Shahriar Karim |  |
| 53 |  |  [The IMO Small Challenge: Not-Too-Hard Olympiad Math Datasets for LLMs](https://openreview.net/forum?id=HYuKXhgzjm) |  | 0 | We introduce the IMO Small Challenge (IMOSC), as opposed to the IMO Grand Challenge: A text-only, natural-language dataset consisting of mathematical problems from various mathematical competitions. The IMOSC dataset exceeds the difficulty level of current datasets that are widely used for LLM evaluation, such as the MATH dataset, while not being too challenging for the current generation of LLMs. The IMOSC currently contains a carefully curated collection of the easiest possible problems from difficult competitions, such as the International Mathematical Olympiad (IMO). Problem hardness is measured by applying a mixture of (objective and subjective) difficulty filters to the original problems. We release the full dataset under the link below to encourage transparent evaluation of LLMs and theorem provers toward their mathematical proof-generating abilities: www.imo-small-challenge.io | Simon Frieder, Mirek Olsák, Julius Berner, Thomas Lukasiewicz |  |
| 54 |  |  [Dynamic Activations for Neural Net Training](https://openreview.net/forum?id=F7rBDSxIMs) |  | 0 | Recent advancements in deep learning have seen breakthroughs in training algorithms, benefiting speech, text, image, and video processing. While deeper architectures like ResNet have made strides, shallow Convolutional Neural Networks (CNNs) remain underexplored. Activation functions, pivotal for introducing non-linearity, drive significant progress. This paper investigates complex piece-wise linear hidden layer activations. Our experiments highlight their superiority over traditional Rectified Linear Units (ReLUs) across architectures. We introduce AdAct, an Adaptive Activation algorithm showing promising performance boosts in diverse CNN and multilayer perceptron setups, advocating for its adoption. | Chinmay Rane, Kanishka Tyagi, Tushar Chugh, Nirmala Murali |  |
| 55 |  |  [Non Parametric Aleatoric Uncertainty Quantification with Neural Networks](https://openreview.net/forum?id=o5q2VsCMXo) |  | 0 | Classic methods for aleatoric uncertainty quantification in regression settings make assumptions about the distribution of noise in the dependent variable. Incorrect assumptions can lead to poor model performance and unreliable uncertainty estimates. In this paper, we introduce a simple method for non-parametric aleatoric uncertainty quantification. In particular, we train a neural network model for binary classification. The inputs to our binary classifier are the independent variables and a sample from the marginal distribution of the dependent variable. This binary classifier is trained to predict whether the sample from the marginal distribution of the dependent variable is greater than the dependent variable corresponding to independent variables in the input. Our method can be used for not only quantifying aleatoric uncertainty but also estimating the conditional distribution of the dependent variable. | Kshitij Kapoor, Debayan Gupta |  |
| 56 |  |  [U2NeRF: Unsupervised Underwater Image Restoration and Neural Radiance Fields](https://openreview.net/forum?id=d8nXcePud3) |  | 0 | Underwater images suffer from colour shifts, low contrast, and haziness due to light absorption, refraction, scattering and restoring these images has warranted much attention. In this work, we present $\textbf{U2NeRF}$, a transformer-based architecture that learns to render and restore novel views conditioned on multi-view geometry simultaneously. We attempt to implicitly bake restoring capabilities onto the NeRF pipeline and disentangle the predicted color into several components and when combined reconstruct the underwater image in a self-supervised manner. In addition, we release an Underwater View Synthesis $\textbf{UVS}$ dataset consisting of 8 real underwater scenes. Our experiments demonstrate that when optimized on a single scene, U2NeRF outperforms several baselines and showcases improved rendering and restoration capabilities. | Vinayak Gupta, Manoj S., Mukund Varma T., Kaushik Mitra |  |
| 57 |  |  [Beyond Uniform Scaling: Exploring Depth Heterogeneity in Neural Architectures](https://openreview.net/forum?id=mURVIdmojf) |  | 0 | Conventional scaling of neural networks typically involves designing a base network and growing different dimensions like width, depth, etc. of the same by some predefined scaling factors. We introduce an automated scaling approach leveraging second-order loss landscape information. Our method is flexible towards skip connections a mainstay in modern vision transformers. Our training-aware method jointly scales and trains transformers without additional training iterations. Motivated by the hypothesis that not all neurons need uniform depth complexity, our approach embraces depth heterogeneity. Extensive evaluations on DeiT-S with ImageNet100 show a 2.5% accuracy gain and 10% parameter efficiency improvement over conventional scaling. Scaled networks demonstrate superior performance upon training small scale datasets from scratch. We introduce the first intact scaling mechanism for vision transformers, a step towards efficient model scaling. | Akash Guna R. T., Arnav Chavan, Deepak K. Gupta |  |
| 58 |  |  [A Bi-Objective $\epsilon $-Constrained Framework for Quality-Cost Optimization in Language Model Ensembles](https://openreview.net/forum?id=P7LrBatrOR) |  | 0 | We propose an ensembling framework that uses diverse open-sourced Large Language Models (LLMs) to achieve high response quality while maintaining cost efficiency. We formulate a bi-objective optimization problem to represent the quality-cost tradeoff and then introduce an additional budget constraint that reduces the problem to a straightforward 0/1 knapsack problem. We empirically demonstrate that our framework outperforms the existing ensembling approaches in response quality while significantly reducing costs. | Aditya Singh, Aditi Singla, Kanishk Kukreja |  |
| 59 |  |  [Zero-shot generalization across architectures for visual classification](https://openreview.net/forum?id=orYMrUv7eu) |  | 0 | Generalization to unseen data is a key desideratum for deep networks, but its relation to classification accuracy is unclear. Using a minimalist vision dataset and a measure of generalizability, we show that popular networks, from deep convolutional networks (CNNs) to transformers, vary in their power to extrapolate to unseen classes both across layers and across architectures. Accuracy is not a good predictor of generalizability, and generalization varies non-monotonically with layer depth. | Evan Gerritz, Luciano Dyballa, Steven W. Zucker |  |
| 60 |  |  [The Duality of Hope: A Critical Examination of Controversial Annotations in HopeEDI](https://openreview.net/forum?id=r6QZ8YKSBd) |  | 0 | This study investigates the HopeEDI hope speech dataset, revealing a significant number of potentially controversial annotations, notably tied to the 'All Lives Matter' movement. We have also identified instances where hateful/toxic/implicitly controversial content was wrongly marked as hopeful. The implications for deploying models trained on this dataset are profound, risking biases and stigmatization. We advocate for thoroughly examining the HopeEDI dataset, cautioning against biased models. We reannotate the hope speech and non-english labelled text, introducing a new class, 'Potentially Controversial', providing reasons for why the label was changed. This updated dataset aims to promote balance and mitigate ethical concerns in real-world applications. | Mohammad Aflah Khan, Neemesh Yadav, Diksha Sethi, Raghav Sahni |  |
| 61 |  |  [DynamicPoseNet: Advanced Human Motion Generation with Dual-Pathway CNNs and LoRA-Enhanced LLaMA](https://openreview.net/forum?id=u6gVK59f4d) |  | 0 | This study introduces DynamicPoseNet, a novel convolutional neural network architecture leveraging depthwise separable convolutions and dual-pathway feature extraction for advanced human motion generation. Using large-scale datasets such as HumanML3D and KIT-ML, DynamicPoseNet efficiently synthesizes realistic human motions, controlled by multiple inputs including textual descriptions and keyframe poses. The model, fine-tuned from a pre-trained 13B LLaMA with LoRA and contrastive learning adaptation, demonstrates superior performance in terms of quality and diversity of generated motions, outperforming state-of-the-art methods with significantly reduced training time and computational resources. Our results indicate a promising direction for future research in diverse and realistic motion generation using advanced deep learning techniques. | Zundong Wu, Jin Xu |  |
| 62 |  |  [CrossVoice: Crosslingual Prosody Preserving Cascade-S2ST using Transfer Learning](https://openreview.net/forum?id=zEdBzTxXHl) |  | 0 | This paper presents CrossVoice, a novel cascade-based Speech-to-Speech Translation (S2ST) system employing advanced ASR, MT, and TTS technologies with cross-lingual prosody preservation through transfer learning. We conducted comprehensive experiments comparing CrossVoice with direct-S2ST systems, showing improved BLEU scores on tasks such as Fisher Es-En, VoxPopuli Fr-En, and prosody preservation on benchmark datasets CVSS-T and IndicTTS. With an average mean opinion score of 3.75 out of 4, speech synthesized by CrossVoice closely rivals human speech on the benchmark, highlighting the efficacy of cascade-based systems and transfer learning in multilingual S2ST with prosody transfer. | Medha Hira, Arnav Goel, Anubha Gupta |  |
| 63 |  |  [Beyond Words: A Topological Exploration of Coherence in Text Documents](https://openreview.net/forum?id=QJxVhljAyS) |  | 0 | Coherence serves as a pivotal metric in evaluating the quality of a text. It quantifies how well the sentences within the text are connected and how well the text is structured and organized. It plays a vital role in various downstream Natural Language Processing tasks such as text summarization, question answering and machine translation among others. In this work, we explore the use of topological data analysis (TDA) techniques on attention graphs of text documents to model coherence. TDA techniques are known to capture structural information and patterns in data, making it suitable for modeling the $\textit{structure}$ and $\textit{flow}$ of a document, i.e. coherence. We validate our approach with experiments on the GCDC dataset, achieving state-of-the-art results with a simple MLP. | Samyak Jain, Rishi Singhal, Sriram Krishna, Yaman Kumar Singla, Rajiv Ratn Shah |  |
| 64 |  |  [Exploring Loss Design Techniques For Decision Tree Robustness To Label Noise](https://openreview.net/forum?id=B8H7pqlj3N) |  | 0 | In the real world, data is often noisy, affecting not only the quality of features but also the accuracy of labels. Current research on mitigating label errors stems primarily from advances in deep learning, and a gap exists in exploring interpretable models, particularly those rooted in decision trees. In this study, we investigate whether ideas from deep learning loss design can be applied to improve the robustness of decision trees. In particular, we show that loss correction and symmetric losses, both standard approaches, are not effective. We argue that other directions need to be explored to improve the robustness of decision trees in dealing with label noise. | Lukasz Sztukiewicz, Jack Henry Good, Artur Dubrawski |  |
| 65 |  |  [Performance Analysis of a quantum-Classical Hybrid Reinforcement Learning Approach](https://openreview.net/forum?id=1POy0o0Z8j) |  | 0 | Quantum Machine Learning (QML) is a nascent field of technology that is yet to be fully explored. While previous QML implementations have demonstrated performance efficiency gains over classical benchmarks, it has not been studied in detail whether shallow unentangled quantum circuits can provide the same benefits to reinforcement learning algorithms. Towards this goal, we present a shallow Deep Q-Network (DQN) hybrid quantum-classical Variational Quantum Circuit (VQC) model in the Cartpole-v0 environment that provides an increase in training stability and average reward for any given training run with a simpler unentangled quantum circuit than what is proposed in prior literature. | Evan Mitchell, Biswajit Basu, Pabitra Mitra |  |
| 66 |  |  [Bypassing the Safety Training of Open-Source LLMs with Priming Attacks](https://openreview.net/forum?id=nz8Byp7ep6) |  | 0 | With the recent surge in popularity of LLMs has come an ever-increasing need for LLM safety training. In this paper, we investigate the fragility of SOTA open-source LLMs under simple, optimization-free attacks we refer to as \*priming attacks\*, which are easy to execute and effectively bypass alignment from safety training. Our proposed attack improves the Attack Success Rate on Harmful Behaviors, as measured by Llama Guard, by up to $3.3\times$ compared to baselines. Source code and data are available at https://github.com/uiuc-focal-lab/llm-priming-attacks. | Jason Vega, Isha Chaudhary, Changming Xu, Gagandeep Singh |  |
| 67 |  |  [No More BLAH-BLAH: Embracing Real Text in the Image synthesis World](https://openreview.net/forum?id=qjrvRK24S0) |  | 0 | The integration of text onto objects within an image frequently results in an unnatural appearance, where the text appears overlaid rather than seamlessly embedded. Existing text-to-image models often encounter challenges in accurately incorporating text within an image context. This paper introduces a novel conditional inpainting technique aimed at overcoming these limitations. Our proposed model showcases exceptional effectiveness in addressing this issue, achieving compelling results by integrating text into images, thus significantly enhancing the naturalness and coherence of the final compositions. | Aref Tabatabaei, Zahra Dehghanian, Negar Movaghatian, Maryam Amirmazlaghani |  |
| 68 |  |  [Borderline Sample Extraction from a Trained Classifier](https://openreview.net/forum?id=xCCNAy8mQK) |  | 0 | Extracting pseudo samples from a trained classifier helps understand classifier decisions, and extracted samples also can assist downstream tasks like knowledge distillation, continual learning, etc. Existing works mostly focus on extracting exemplary samples, i.e., samples that carry salient features of a class; however, seldom effort has been put into extracting borderline samples that reflect minor differences between two classes. In this paper, we propose a Perturbation Minimization method to extract borderline samples from a trained classifier. Through experiments, we show PM can extract borderline samples, and these samples improve the accuracy in class-incremental learning. | Borui Cai, Zihao Johnson Zheng, Longxiang Gao, Yong Xiang |  |
| 69 |  |  [Self-Teaching Prompting for Multi-Intent Learning with Limited Supervision](https://openreview.net/forum?id=DeoamI1BFh) |  | 0 | Multi-intent learning with limited supervision involves predicting multiple intentions of utterances using only a few annotated samples. The primary motivation for this task stems from the high costs and cumbersome processes associated with annotating large datasets. To mitigate this, we propose utilising Large Language Models (LLMs) for annotation assistance. Although LLMs show promise, they struggle with response randomness, and their previous prompts is static and do not learn from their outputs. To address this, we propose \`self-teaching prompting' (STP), a method that enables Large Language Models (LLMs) to iteratively learn from their consistent samples and refine their predictions over time. Our experiments with multi-intention datasets demonstrate that STP significantly enhances response accuracy. | Cheng Chen, Ivor W. Tsang |  |
| 70 |  |  [Knowledge Distillation Through Time For Future Event Prediction](https://openreview.net/forum?id=JBSMl0nAFa) |  | 0 | Is it possible to learn from the future? Here, we introduce knowledge distillation through time (KDTT). In traditional knowledge distillation (KD), a reliable teacher model is used to train an error-prone student model. The difference between the teacher and student is typically model capacity; the teacher is larger in architecture. In KDTT, the teacher and student models differ in their assigned tasks. The teacher model is tasked with detecting events in sequential data, a simple task compared to the student model, which is challenged with forecasting said events in the future. Through KDTT, the student can use the ’future’ logits from a teacher model to extract temporal uncertainty. We show the efficacy of KDTT on seizure prediction, where the student forecaster achieves a 20.0% average increase in the area under the curve of the receiver operating characteristic (AUC-ROC) | Skye Gunasekaran, Jason Eshraghian, Ruomin Zhu, Zdenka Kuncic |  |
| 71 |  |  [Can Perplexity Reflect Large Language Model's Ability in Long Text Understanding?](https://openreview.net/forum?id=Cjp6YKVeAa) |  | 0 | Recent studies have shown that Large Language Models (LLMs) have the potential to process extremely long text with evidence that LLMs could perform well in the language modeling task with even 1 million input tokens. When the input context length increases, the perplexity (PPL) of the model is observed to maintain at a low level or even decrease. However, in our study, we find that the PPL may only reflect the model's ability to model local information instead of catching long-range dependency, and thus only using PPL to prove the model could process very long context is not appropriate. The local focus feature of perplexity could also explain some existing phenomena, such as the great extrapolation ability of the position method ALiBi. When evaluating a model's ability in long text, we might pay more attention to the limitation of PPL and avoid overly reliance on it. | Yutong Hu, Quzhe Huang, Mingxu Tao, Chen Zhang, Yansong Feng |  |
| 72 |  |  [Session-Aware Product filter Ranking in E- Commerce Search](https://openreview.net/forum?id=r4LMF2IJ6R) |  | 0 | Product filters are commonly used by e-commerce websites to refine search results based on attribute values such as price, brand, size, etc. However, existing filter recommendation approaches typically generate filters independently of the user's search query or browsing history. This can lead to suboptimal recommendations that do not account for what the user has already viewed or selected in their current browsing session. In this paper, we propose a session-aware product filter recommendation framework that leverages user's past actions to provide filter recommendations. An offline evaluation demonstrates that our model achieved significant improvement over non-contextual baseline models. | Hanqing Lu, Xianfeng Tang, Chen Luo, Limeng Cui, Zhenwei Dai, Rahul Goutam, Haiyang Zhang, Monica Xiao Cheng |  |
| 73 |  |  [Can Speculative Sampling Accelerate ReAct Without Compromising Reasoning Quality?](https://openreview.net/forum?id=42b9hJrIpX) |  | 0 | Large language models (LLMs) are increasingly used as agents for interaction with external environments. These interplays are commonly facilitated through various prompting paradigms. However, such paradigms require extended interaction traces between the LLMs and the environment, resulting in low task-solving efficiency. In this work, we integrate speculative sampling (SpS) into the novel ReAct paradigm. In particular, we investigate speculative sampling’s impact on the efficiency of ReAct and the quality of reasoning tasks. Our evaluations using HotPotQA and FEVER datasets demonstrate that implementing speculative sampling alongside ReAct results in a 2.18x-2.62x acceleration compared to using ReAct alone, while only introducing a negligible impact on the reasoning abilities. | Han Xu, Jingyang Ye, Yutong Li, Haipeng Chen |  |
| 74 |  |  [Rethinking Compression: Reduced order modelling of Latent Features in Large Language Models](https://openreview.net/forum?id=BfVccaZiEv) |  | 0 | Due to the substantial scale of Large Language Models (LLMs), the direct application of conventional compression methodologies proves impractical. The computational demands associated with even minimal gradient updates present challenges, particularly on consumer-grade hardware. This paper introduces an innovative approach for the parametric and practical compression of LLMs based on reduced order modelling, which entails low-rank decomposition within the feature space and re-parameterization in the weight space. Notably, this compression technique operates in a layer-wise manner, obviating the need for a GPU device and enabling the compression of billion-scale models within stringent constraints of both memory and time. Our method represents a significant advancement in model compression by leveraging matrix decomposition, demonstrating superior efficacy compared to the prevailing state-of-the-art structured pruning method. | Arnav Chavan, Nahush Lele, Deepak K. Gupta |  |
| 75 |  |  [Geometric Implications of Classification on Reducing Open Space Risk](https://openreview.net/forum?id=vYdLXrToTG) |  | 0 | To reduce open space risk of hypotheses, we reexamine the 'simplest' hypothesis class, binary linear classifiers, geometrically. Generalizing linear classification, we establish a surprising fact: linear classifiers can have arbitrarily high VC dimension, stemming from increasing the number of partitions in input space. Hence, linear classifiers with multiple margins are more expressive than single-margin classifiers. Despite a higher VC dimension, such classifiers have less open space risk than halfspace separators. These geometric insights are useful to detect unseen classes, while probabilistic modeling of risk minimization helps with seen classes. In supervised anomaly detection, we show that a classifier that combines a probabilistic and geometric lens can detect both seen and unseen anomalies well. | Matthew Lau, Leyan Pan, Stefan Davidov, Athanasios P. Meliopoulos, Wenke Lee |  |
| 76 |  |  [Role of Over-Parameterization in Generalization of 3-layer ReLU Networks](https://openreview.net/forum?id=EarRzo8ixe) |  | 0 | Over-parameterized neural networks defy conventional wisdom by generalizing effectively; however, standard complexity metrics like norms and margins fail to account for this. A recent work introduced a novel measure considering unit-wise capacities and provided a better explanation and tighter generalization bounds but was confined to two-layer networks. This paper extends that framework to three-layer ReLU networks. We empirically confirm the applicability of these measures and introduce a corresponding theoretical Rademacher complexity bound. | Simranjit Singh, Aditya Golatkar, Avijit Verma |  |
| 77 |  |  [Density-Preserving Heterogeneous Graph Sparsification for Representation Learning](https://openreview.net/forum?id=sISO5ubRGP) |  | 0 | Graph sparsification is the task of compressing a graph with fewer edges or nodes while preserving its essential structural characteristics. It has been used in machine learning to significantly improve the computational efficiency over homogeneous graphs. In heterogeneous graphs with diverse types of nodes and edges, however, sparsification has not been extensively explored. This work develops sparsification methods that can preserve edge density across different edge types and/or edge importance in terms of eigenvector centrality, improving over existing methods. The methods have been tested on real-world networks, and the results indicate great improvements in the computational efficiency and memory cost. | Srilekha Geda, Chunjiang Zhu |  |
| 78 |  |  [Toward Computationally Efficient Inverse Reinforcement Learning via Reward Shaping](https://openreview.net/forum?id=hhsOkmgDEz) |  | 0 | Inverse reinforcement learning (IRL) is computationally challenging, with common approaches requiring the solution of multiple reinforcement learning (RL) sub-problems. This work motivates the use of potential-based reward shaping to reduce the computational burden of each RL sub-problem. This work serves as a proof-of-concept and we hope will inspire future developments towards computationally efficient IRL. | Lauren H. Cooke, Harvey Klyne, Edwin Zhang, Cassidy Laidlaw, Milind Tambe, Finale DoshiVelez |  |
| 79 |  |  [Jensen-Shannon Divergence in Safe Multi- Agent RL](https://openreview.net/forum?id=PRBspmgNkY) |  | 0 | Reinforcement Learning (RL) has achieved significant milestones, however its safety remains a concern for real-world applications. Safe RL solutions focus on maximizing environment rewards while minimizing cost. In this work, we extend the Multi-Agent Constrained Policy Optimisation (MACPO) approach that maintains policy consistency using Kullback-Leibler (KL) divergence. We find that Jensen-Shannon (JS) Divergence, a symmetric measure, serves as a better alternative to KL divergence; its symmetric nature is more forgiving of extreme differences in policies. Our results demonstrate that JS divergence improves rewards and reduces costs, enhancing safety and performance in multi-agent systems. | Rushikesh Zawar, Prabhdeep Singh Sethi, Roshan Roy |  |
| 80 |  |  [Exploiting Time Channel Vulnerability of Learned Bloom Filters](https://openreview.net/forum?id=jHRWVA1H0f) |  | 0 | Neural network for computer systems—such as operating systems, databases, and network systems—attract much attention. However, using neural networks in systems introduces new attacking surfaces. This paper makes the first attempt to study the security factor of learned bloom filters, a promising neural network based data structure in systems. We design and implement an attack that can efficiently recover system owners’ data via a timing side channel and a new recovering algorithm. | Harman Singh Farwah, Gagandeep Singh, Cheng Tan |  |
| 81 |  |  [Small Transformers, Big Results: Efficient Diffusion with Parameter Sharing](https://openreview.net/forum?id=owfuhF0bT9) |  | 0 | The interplay between model depth, computational complexity, and parameter count remains an intricate aspect of neural network design. We propose a novel block sharing mechanism for denoising diffusion generative models, enabling us to maintain or even improve model quality while reducing parameter count. Our approach leverages the architectural homogeneity of Vision Transformers and demonstrates enhanced performance with less computational overhead on various datasets. We provide our code and pre-trained models to facilitate further research. | Mohamed Osman, Daniel Z. Kaplan |  |
| 82 |  |  [KLCE: Regularized Imbalance Node-classification Via KL-divergence and Cross-Entropy](https://openreview.net/forum?id=JuqfAXWHfr) |  | 0 | This paper introduces a novel regularization based on KL-divergence and cross-entropy for imbalance node classification via Graph neural networks. We evaluate the performance of our approach on several benchmark datasets and compare it with state-of-the-art methods. The experimental results demonstrate the effectiveness of our proposed method in addressing imbalance node classification tasks. | Mohammad Taha Teimuri Jervakani, Zahra Dehghanian, Gholamali Aminian, Hamid R. Rabiee |  |
| 83 |  |  [Affinity-based Homophily: Can we measure homophily of a graph without using node labels?](https://openreview.net/forum?id=IsdDOrAowN) |  | 0 | The homophily (heterophily) ratio in a graph represents the proportion of edges connecting nodes with similar (dissimilar) class labels. Existing methods for estimating the homophily ratio typically rely on knowing the class labels of each node in the graph. While several algorithms address both homophilic and heterophilic graphs, they necessitate prior knowledge of the homophily ratio to choose the appropriate one. To address this limitation, we propose a novel metric for measuring homophily ratio without information about node labels. In our approach, we define learnable affinity vectors for each node, characterizing the expected feature relationships with its neighbors. Our method, Affinity-based Homophily, derives the homophily ratio using these affinity vectors, eliminating the need for prior node label information. We conducted experiments on various benchmark homophilic and heterophilic graphs, demonstrating the commendable performance of our homophily measure. | Indranil Ojha, Kushal Bose, Swagatam Das |  |
| 84 |  |  [Can Decoupling Embedded Text from Images Improve Multimodal Learning?](https://openreview.net/forum?id=uF7ZWntl3m) |  | 0 | Multimodal models have widely been used to process text-embedded images on social media. However, the effect of embedded text on the image encoding process remains unexplored. In this work, we eliminated the text in text-embedded images and compared the intervention's effect on the performance of unimodal and multimodal models. We find that the image encoders of multimodal models utilize linguistic information in the pixel space to a considerable degree. Further, we observe that disentangling text and images can improve multimodal learning under certain circumstances. | Siddhant Bikram Shah |  |
| 85 |  |  [Cognitive resilience: Unraveling the proficiency of image-captioning models to interpret masked visual content](https://openreview.net/forum?id=RQ0wmIBcTB) |  | 0 | This study explores the ability of Image Captioning (IC) models to decode masked visual content sourced from diverse datasets. Our findings reveal the IC model's capability to generate captions from masked images, closely resembling the original content. Notably, even in the presence of masks, the model adeptly crafts descriptive textual information that goes beyond what is observable in the original image-generated captions. While the decoding performance of the IC model experiences a decline with an increase in the masked region's area, the model still performs well when important regions of the image are not masked at high coverage. | Zhicheng Du, Zhaotian Xie, Huazhang Ying, Likun Zhang, Peiwu Qin |  |
| 86 |  |  [Revelio: Interpretable Long-Form Question Answering](https://openreview.net/forum?id=fyvEJXsaQf) |  | 0 | The black-box architecture of pretrained language models (PLMs) hinders the interpretability of lengthy responses in long-form question answering (LFQA). Prior studies use knowledge graphs (KGs) to enhance output transparency, but mostly focus on non-generative or short-form QA. We present Revelio, a new layer that maps PLM's inner working onto a KG walk. Tests on two LFQA datasets show that Revelio supports PLM-generated answers with reasoning paths presented as rationales while retaining performance and time akin to their vanilla counterparts. | Gianluca Moro, Luca Ragazzi, Lorenzo Valgimigli, Fabian Vincenzi, Davide Freddi |  |
| 87 |  |  [Loss-Free Machine Unlearning](https://openreview.net/forum?id=bCPz7uqmmh) |  | 0 | We present a machine unlearning approach that is both retraining- and label-free. Most existing machine unlearning approaches require a model to be fine-tuned to remove information while preserving performance. This is computationally expensive and necessitates the storage of the whole dataset for the lifetime of the model. Retraining-free approaches often utilise Fisher information, which is derived from the loss and requires labelled data which may not be available. Thus, we present an extension to the Selective Synaptic Dampening algorithm, substituting the diagonal of the Fisher information matrix for the gradient of the $l_2$ norm of the model output to approximate sensitivity. We evaluate our method in a range of experiments using ResNet18 and Vision Transformer. Results show our label-free method is competitive with existing state-of-the-art approaches. | Jack Foster, Stefan Schoepf, Alexandra Brintrup |  |
| 88 |  |  [How Many OptiFaces? A New Evaluation Metric For 3D Face Reconstruction](https://openreview.net/forum?id=KzNM3S3IDT) |  | 0 | Three dimensional face reconstruction is a challenging problem, so much so that the mean face is highly competitive with recent learning-based approaches for 3D face reconstruction from 2D images. No other universal baselines for this task exist. We propose a novel baseline that selects a subset of face meshes, called OptiFaces, that minimise overall 3D reconstruction error. This is a universal approach to calculate dataset-specific metrics for 3D face reconstruction, offering intuitive new baselines for the interpretation of 3D reconstruction error. | Will Rowan, Patrik Huber, Nick E. Pears, Andrew Keeling |  |
| 89 |  |  [Sequence Mixup for Zero-Shot Cross-Lingual Part-of-speech Tagging](https://openreview.net/forum?id=H6gQ9iOS1X) |  | 0 | There have been efforts in cross-lingual transfer learning for various tasks. We present an approach utilizing an interpolative data augmentation method, Mixup, to improve the generalizability of models for part-of-speech tagging trained on a source language, improving its performance on unseen target languages. Through experiments on ten languages with diverse structures and language roots, we put forward its applicability for downstream zero-shot cross-lingual tasks. | Ramit Sawhney, Megh Thakkar |  |
| 90 |  |  [A Novel Window-Interaction Module Based on W-MSA](https://openreview.net/forum?id=ki4R0z0C4K) |  | 0 | W-MSA proposed by Swin Transformer has limitations in facilitating information interaction between windows. To address this, we introduce a module that utilizes convolution to achieve inter-window information interaction across different regions. Experiments demonstrate that our proposed module, when combined with W-MSA in a dual-branch structure, outperforms the simple W-MSA. In the deraining task conducted on the Uformer, we observe a 0.14dB improvement in performance. Our code: https://github.com/421zuoduan/WIM-code. | Ruochen Cui, Yongding Tao, Mingjun Ni |  |
| 91 |  |  [Mask2tasks: Leveraging Segmentation to Enhance Classification Performance in histopathological colorectal Images](https://openreview.net/forum?id=WMxXuVTzFm) |  | 0 | In this study, we explore the enhancement of colorectal image classification accuracy with the aid of a segmentation task. We introduce Mask2Tasks, a deep neural network for joint colorectal image classification and segmentation which is trained using a novel two-stage training approach. Numerical results have demonstrated its effectiveness in both classification and multi-task learning scenarios. | Hieu Le Xuan, Minh Hoang Le, Viet V. Truong, Huy Phan Quang, Hoang Vu Huy |  |
| 92 |  |  [A Framework for Policy Evaluation Enhancement by Diffusion Models](https://openreview.net/forum?id=QTE2kMP4DJ) |  | 0 | Reinforcement learning plays an important role in various fields, and has fast development in policy evaluation and learning methods, enjoying the advantages of large data size. However, when data are limited, directly applying evaluation methods does not necessarily result in a good policy evaluation. In this work, we provide a framework to generate synthetic data with diffusion models, to enhance data-efficient policy evaluation, which is supported by experiments. | Tao Ma, Xuzhi Yang |  |
| 93 |  |  [FeedFace: Efficient Inference-based Face Personalization via Diffusion Models](https://openreview.net/forum?id=PqPKBcamy3) |  | 0 | We introduce FeedFace, a novel inference-based method designed to augment text-to-image diffusion models with face-based conditional generation. Trained on a thoroughly curated and annotated dataset of diverse human faces, FeedFace operates without additional training for new facial conditions during generation. Our method can create images that are not only true to the textual descriptions but also exhibit remarkable facial faithfulness in seconds. Our model supports using multiple faces as input conditions, leveraging extra facial information to improve facial consistency. A key strength of our method lies in its efficiency. Through our experiments, we demonstrate that FeedFace can produce face-conditioned samples with comparable quality to leading industry methods, using only 0.4% of their data volume and fewer than 5% of the samples seen by these methods during training. | Chendong Xiang, Armando Teles Fortes, Khang Hui Chua, Hang Su, Jun Zhu |  |
| 94 |  |  [Knowledge Graph Unlearning to Defend Language Model Against Jailbreak Attack](https://openreview.net/forum?id=gqTUIesy4H) |  | 0 | Large language models (LLMs) are vulnerable to jailbreak attacks that bypass safety measures and induce LLMs to generate harmful content. There is a notable dearth of research on defense mechanisms against jailbreak attack, especially attacks that leverage fine-tuning techniques on open-access LLMs. To bridge this gap, this paper proposes the Knowledge Graph Unlearning (KGUnL) framework to remove harmful content from LLMs. The empirical study demonstrate the effectiveness of our framework on defending LLM against fine-tuning attacks. | Peihua Mai, Hao Jiang, Ran Yan, Youjia Yang, Zhe Huang, Yan Pang |  |
| 95 |  |  [Partial Rankings of Optimizers](https://openreview.net/forum?id=mhwNoQcEjQ) |  | 0 | We introduce a framework for benchmarking optimizers according to multiple criteria over a collection of test functions. Based on a recently introduced union-free generic depth function for partial orders/rankings, it fully exploits the ordinal information and allows for incomparability. Our method describes the distribution of all partial orders/rankings, avoiding the notorious shortcomings of aggregation. This permits to identify test functions that produce central or outlying rankings of optimizers and to assess the quality of benchmarking suites. | Julian Rodemann, Hannah Blocher |  |
| 96 |  |  [LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked](https://openreview.net/forum?id=YoqgcIA19o) |  | 0 | Large language models (LLMs) are popular for high-quality text generation but can also produce harmful responses as adversarial prompts can bypass their safety measures. We propose LLM Self Defense, a simple approach to defend against these attacks by having an LLM screen the induced responses, thus not requiring any fine-tuning, input preprocessing, or iterative output generation. Instead, we incorporate the generated content into a pre-defined prompt and employ another instance of an LLM to analyze the text and predict whether it is harmful. Notably, LLM Self Defense succeeds in reducing the attack success rate to virtually 0 against various types of attacks on GPT 3.5 and Llama 2. The code is publicly available at https://github.com/poloclub/llm-self-defense | Mansi Phute, Alec Helbling, Matthew Hull, Shengyun Peng, Sebastian Szyller, Cory Cornelius, Duen Horng Chau |  |
| 97 |  |  [Cognitive Reframing via Large Language Models for Enhanced Linguistic Attributes](https://openreview.net/forum?id=IIus8CSwsU) |  | 0 | Cognitive Reframing, a core technique in Cognitive Behavioral Therapy (CBT), seeks to enhance mental well-being. While previous research has highlighted the efficacy of Large Language Models (LLMs) for cognitive reframing, there has been limited focus on enhancing reframing quality across multiple linguistic attributes in the final output. This paper fills this gap by employing LLMs to generate and iteratively refine reframed thoughts. The results of our study outperform in helpfulness, empathy and rationality in GPT-4 evaluation. | Xiaomeng Wang, Dharmendra Sharma, Dinesh Kumar |  |
| 98 |  |  [DSF-GAN: Downstream Feedback Generative Adversarial Network](https://openreview.net/forum?id=Vfp8jhwcCc) |  | 0 | Utility and privacy are two crucial measurements of synthetic tabular data. While privacy measures have been dramatically improved with the use of Generative Adversarial Networks (GANs), generating high-utility synthetic samples remains challenging. To increase the samples' utility, we propose a novel architecture called DownStream Feedback Generative Adversarial Network (DSF-GAN). This approach uses feedback from a downstream prediction model mid-training, to add valuable information to the generator’s loss function. Hence, DSF-GAN harnesses a downstream prediction task to increase the utility of the synthetic samples. To properly evaluate our method, we tested it using two popular data sets. Our experiments show better model performance when training on DSF-GAN-generated synthetic samples compared to synthetic data generated using the same GAN architecture without feedback when evaluated on the same validation set comprised of real samples. All code and datasets used in this research are openly available for ease of reproduction. | Oriel Perets, Nadav Rappoport |  |
| 99 |  |  [Improving Automated Speech Recognition Using Retrieval-Based Voice Conversion](https://openreview.net/forum?id=OMBFB6pU6c) |  | 0 | This study examines the efficacy of voice conversion techniques in enhancing Automatic Speech Recognition (ASR) accuracy for non-native English speakers. Utilizing the OpenAI Whisper models, we analyzed transcription accuracy across various accents and countries. Significant reductions in Word Error Rates (WER) were observed, with the Whisper Large-v2 model showing the most pronounced improvements. Our findings indicate that advanced voice conversion can mitigate accent bias, promoting inclusivity and broadening the applicability of ASR technology to a more diverse user base. | Anas Mohammed Alhumud, Muhammad AlQurishi, Yasser Omar Alomar, Ali Alzahrani, Riad Souissi |  |
| 100 |  |  [Doc2Command: Furthering Language Guided Document Editing](https://openreview.net/forum?id=inQ9bW5AQz) |  | 0 | Language guided document editing is a novel task that includes generating a machine parsable command and a bounding box from an open vocabulary user request. This paper introduces Doc2Command, a multi-task, multimodal model that unifies the document and user request into a singular visual modality and utilises a transformer base image encoder-text decoder to generate the command text. Additionally, it reconceptualises bounding box detection as a segmentation task and employs a mask transformer operating on the image encoder. Doc2Command surpasses baseline models in command text generation, demonstrating significant performance improvements ranging from 2-33\% for exact matched commands. It also improves on the bounding box detection task on existing baselines by a margin of 12.19-31.65\%. | Manan Suri, Puneet Mathur, Ramit Sawhney, Preslav Nakov, Dinesh Manocha |  |
| 101 |  |  [Neural Control System for Continuous Glucose Monitoring and Maintenance](https://openreview.net/forum?id=Te4P3Cn54g) |  | 0 | Precise glucose level monitoring is critical for people with diabetes to avoid serious complications. While there are several methods for continuous glucose level monitoring, research on maintenance devices is limited. To mitigate the gap, we provide a novel neural control system for continuous glucose monitoring and management that uses differential predictive control, \*NeuralCGMM\*. Our approach, led by a sophisticated neural policy and differentiable modeling, constantly adjusts insulin supply in real-time, thereby improving glucose level optimization in the body. This end-to-end method maximizes efficiency, providing personalized care and improved health outcomes, as confirmed by empirical evidence. Code and data are available at: https://github.com/azminewasi/NeuralCGMM. | Azmine Toushik Wasi |  |
| 102 |  |  [Enhancing Language Models for Financial Relation Extraction with Named Entities and Part-of-Speech](https://openreview.net/forum?id=BAR6OE80OW) |  | 0 | The Financial Relation Extraction (FinRE) task involves identifying the entities and their relation, given a piece of financial statement/text. To solve this FinRE problem, we propose a simple but effective strategy that improves the performance of pre-trained language models by augmenting them with Named Entity Recognition (NER) and Part-Of-Speech (POS), as well as different approaches to combine these information. Experiments on a financial relations dataset show promising results and highlights the benefits of incorporating NER and POS in existing models. Our dataset and codes are available at https://github.com/kwanhui/FinRelExtract. | Menglin Li, Kwan Hui Lim |  |
| 103 |  |  [Adaptive Brain Network Augmentation Based on Group-aware Graph Learning](https://openreview.net/forum?id=29N5YY0OuO) |  | 0 | Brain network analysis significantly improves artificial intelligence techniques in the realm of digital health. Most existing methods uniformly construct brain networks for different groups (e.g., male and female groups, healthy and sick people groups), facing the interference of group-irrelevant noises and failing to capture group-specific features to enhance brain networks. To address this issue, this paper proposes an adaptive brain network augmentation method based on group-aware graph learning. We construct group-aware brain networks, which can adapt to distinct groups, reducing the interference of noises, and improving model robustness across various tasks and subject groups. | Ciyuan Peng, Mujie Liu, Chenxuan Meng, Shuo Yu, Feng Xia |  |
| 104 |  |  [Generating Counterfactual Explanations Using Cardinality Constraints](https://openreview.net/forum?id=4hmAp3Ca3o) |  | 0 | Providing explanations about how machine learning algorithms work and/or make particular predictions is one of the main tools that can be used to improve their trusworthiness, fairness and robustness. Among the most intuitive type of explanations are counterfactuals, which are examples that differ from a given point only in the prediction target and some set of features, presenting which features need to be changed in the original example to flip the prediction for that example. However, such counterfactuals can have many different features than the original example, making their interpretation difficult. In this paper, we propose to explicitly add a cardinality constraint to counterfactual generation limiting how many features can be different from the original example, thus providing more interpretable and easily understantable counterfactuals. | Rubén RuizTorrubiano |  |
| 105 |  |  [Explicitly Stating Assumptions Reduces Hallucinations in Natural Language Inference](https://openreview.net/forum?id=eJI9pfNwBS) |  | 0 | A natural language inference (NLI) model might hold a hallucination that a 'premise' infers a 'hypothesis'. This hallucination is possibly not due to insufficient training on the data but rather is inherent in the data themselves. A training label might suggest that 'a premise infers a hypothesis', but this inferential link could be overstated. This overstating might arise from the mismatch in intensity or context. We propose that reinforcing the premise could mitigate this problem. To address this, we introduce a method that employs predicate logic and natural deduction to explicitly articulate assumptions that validate the inferential link. This method allows for a transparent inference process, drawing logically plausible conclusions based on explicit premises. The NLI model may thus achieve higher reliability in understanding and processing natural language. | Wenchuan Mu, Kwan Hui Lim |  |
| 106 |  |  [On the robustness of Chatgpt under input perturbations for Named Entity Recognition Task](https://openreview.net/forum?id=cyN5Ck1RFT) |  | 0 | We present a systematic evaluation of the robustness of ChatGPT (in both zeroand few-shot settings) under input perturbations for Named Entity Recognition (NER) task. Our findings suggest: (1) ChatGPT is more brittle on Drug or Disease entity perturbations (rare entities) as compared to those on widely known Person or Location entities, and (2) the quality of explanations (localness and globalness) for the same entity considerably differ under various Entity-specific and Contextspecific perturbations; the quality significantly improves using in-context learning. | Ishani Mondal, Abhilasha Sancheti |  |
| 107 |  |  [Neural Controlled Differential Equations with Quantum Hidden Evolutions](https://openreview.net/forum?id=vQUq4DVeu6) |  | 0 | We introduce a class of neural controlled differential equation inspired by quantum mechanics. Neural quantum controlled differential equations (NQDEs) model the dynamics by analogue of the Schrodinger equation. Specifically, the hidden state represents the wave function, and its collapse leads to an interpretation of the classification probability. We implement and compare the results of four variants of NQDEs on a toy spiral classification problem. | Lingyi Yang, Zhen Shao |  |
| 108 |  |  [GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural Networks](https://openreview.net/forum?id=2yuAxTs0QV) |  | 0 | The successful graph neural networks (GNNs) and particularly message passing neural networks critically depend on the functions employed for message aggregation and graph-level readout. Using signal propagation theory, we propose a variance-preserving aggregation function, which maintains the expressivity of GNNs while improving learning dynamics. Our results could pave the way towards normalizer-free or self-normalizing GNNs. | Lisa Schneckenreiter, Richard Freinschlag, Florian Sestak, Johannes Brandstetter, Günter Klambauer, Andreas Mayr |  |
| 109 |  |  [Using spiking neural networks to assist fine art and philology study: to classify styles of Chinese calligraphy with minimal computing power](https://openreview.net/forum?id=1715SJkbj7) |  | 0 | Spiking Neural Networks have drawn much attention for their potential deployment in low computing power scenarios and interdisciplinary research. This paper focuses on a novel task of classifying Chinese Calligraphy styles properly and introduces a cutting-edge network called CaStySNN. Compared to same-structured traditional artificial neural networks, CaStySNN requires significantly less computing power, while demonstrating superior performances across different datasets. In the future, this innovative approach can be applied to neuromorphic devices, offering solutions to a wide range of challenges in the realms of fine arts and philology. | Zheng Luan, Xiangqi Kong, Shuimu Zeng, Yuke Yao, Yaxuan Zhang, Xuerui Qiu |  |
| 110 |  |  [Common Sense Initialization of Mixture Density Networks for Motion Planning with Overestimated Number of Components](https://openreview.net/forum?id=C6EHiLaiBT) |  | 0 | Mixture density networks (MDNs) are a natural choice to model multi-modal predictions for trajectory prediction or motion planning. However, MDNs are often difficult to train due to mode collapse and a need for careful initialization, which becomes even more problematic when the number of mixture components are strongly overestimated. To address this issue in motion planning problems, we propose a pre-training scheme for MDNs called common sense initialization (CSI). Pre-training with CSI allows variety-encouraging optimization such as Winner-Takes-All (WTA) to exploit the initialized weights during training so that the MDN can converge when the number of components are overestimated. This paper presents empirical evidence for the effectiveness of CSI when applied to motion planning of pedestrian agents in urban environments. | Thomas Kreutz, Max Mühlhäuser, Alejandro Sánchez Guinea |  |
| 111 |  |  [Improving Image Editing Models with Generative Data Refinement](https://openreview.net/forum?id=q5UrA58oyY) |  | 0 | Instruction-based generative image editing models allow an image to be modified based on a text prompt and have the potential to significantly improve the accessibility of image processing software. Like other generative models, they are highly dependent on the quality of their training dataset, and generating good editing datasets is an expensive task. In this paper, we show that a simple refinement of the original InstructPix2Pix (Brooks et al., 2023) dataset using SDXL (Podell et al., 2023) leads to consistent improvements in downstream models. We finetune SDXL on our refined dataset and observe competitive performance to much more cost-intensive methods. We will make the dataset and models publicly available. | Frederic Boesel, Robin Rombach |  |
| 112 |  |  [Learning the Uncertainty Set in Robust Markov Decision Process](https://openreview.net/forum?id=viHuAGGjqK) |  | 0 | In robust Markov Decision Processes (MDPs), the uncertainty set is often assumed to be fixed and given. However, the size of the uncertainty set is crucial due to the inherent trade-off between robustness and conservatives: a larger uncertainty set fosters a more robust solution but tends towards increased conservativeness, while a smaller set may sacrifice robustness for higher performance. In this work, we introduce a novel method to learn the size of reward uncertainty set from data. Such a data-driven approach ensures that the learned uncertainty set is large enough to cover the underlying models implied by the data while being compact to minimize conservativeness. | Navdeep Kumar, Kaixin Wang, Uri Gadot, Kfir Yehuda Levy, Shie Mannor |  |
| 113 |  |  [Exploring GPT-4 Vision for Text-to-Image Synthesis Evaluation](https://openreview.net/forum?id=xmQoodG82a) |  | 0 | This paper addresses the critical need for more accurate evaluation methods in text-to-image synthesis. While the standard CLIPScore metric can reflect text-image alignment to some extent, it often falls short in consistency with human perception. We propose the use of GPT-4 Vision as a novel evaluative standard, capable of interpreting text and image nuances akin to human cognition. Our study focuses on the pivotal role of prompt design in maximizing GPT-4 Vision's effectiveness, presenting a systematic discussion for prompt construction. Empirical evaluations demonstrate that GPT-4 Vision, augmented by our prompt-design strategy, aligns more closely with human judgment. | Xiao Cui, Qi Sun, Wengang Zhou, Houqiang Li |  |
| 114 |  |  [Language Model Knowledge Distillation for Efficient Question Answering in Spanish](https://openreview.net/forum?id=iTgKbVwQAF) |  | 0 | Recent advances in the development of pre-trained Spanish language models has led to significant progress in many Natural Language Processing (NLP) tasks, such as question answering. However, the lack of efficient models imposes a barrier for the adoption of such models in resource-constrained environments. Therefore, smaller distilled models for the Spanish language could be proven to be highly scalable and facilitate their further adoption on a variety of tasks and scenarios. In this work, we take one step in this direction by developing SpanishTinyRoBERTa, a compressed language model based on RoBERTa for efficient question answering in Spanish. To achieve this, we employ knowledge distillation from a large model onto a lighter model that allows for a wider implementation, even in areas with limited computational resources, whilst attaining negligible performance sacrifice. Our experiments show that the dense distilled model can still preserve the performance of its larger counterpart, while significantly increasing inference speedup. This work serves as a starting point for further research and investigation of model compression efforts for Spanish language models across various NLP tasks. | Adrián Bazaga, Pietro Lio, Gos Micklem |  |
| 115 |  |  [AdAct: Learning to Optimize Activation Function Choice through Adaptive Activation Modules](https://openreview.net/forum?id=M7d5k4AxCE) |  | 0 | This paper presents an innovative approach to enhancing neural network performance through the development and implementation of an adaptive activation function, termed adaptive activation (AdAct). AdAct amalgamates various well-established and novel activation functions into a single, learnable framework, allowing dynamic adaptation to specific network layers' needs. We explore the effectiveness of ReLU and its variants, including ELU, LReLU, PReLU, RReLU, and more recent functions like Swish and Mish, integrating them into the AdAct function. Employing ConvNet variants across FMNIST, CIFAR10, SVHN and FER datasets, our study empirically assesses each function's contribution and demonstrates AdAct's potential in optimizing neural networks, especially in selecting optimal activation functions for diverse tasks. | Ritabrata Maiti |  |
| 116 |  |  [A Novel Two-stage Model with Cross-Level Contrastive Learning for Text-VQA](https://openreview.net/forum?id=0Nm98D6mEh) |  | 0 | Text-based Visual Question Answering (Text-VQA) task requires the model to learn effective representations in a joint semantic space. Previous methods lack the explicit alignment between object-level and scene text-level in visual-linguistic modalities. To address this issue, we propose a novel two-stage model with cross-level contrastive learning. In the first pre-training stage, we encourage the model to enhance the proximity of cross-level cross-modal representations within the same image in semantic space, while also distancing representations from different images. Then we fine-tune the model to generate the answer to the question. Experimental results on a widely used benchmark dataset demonstrate the effectiveness of our proposed model compared to existing methods. | Yuejy, Xiaojun Bi, Zheng Chen |  |
| 117 |  |  [Heredity-aware Child Face Image Generation with Latent Space Disentanglement](https://openreview.net/forum?id=WBVfUjA80m) |  | 0 | In this paper, we propose ChildGAN to generate a child's face image according to the images of parents with heredity prior. The main idea is to disentangle the latent space of a pre-trained generation model and precisely control the face attributes of child images with clear semantics. We use distances between face landmarks as pseudo labels so as to avoid using external labels. By calculating the gradient of latent vectors to pseudo labels, we figure out the most influential semantic vectors of the corresponding face attributes. Then we disentangle the semantic vectors in three aspects: adding a weight factor in the calculating process, working on the proper resolution layers, and using Schmidt orthogonalization to orthogonalize these vectors. Finally, we fuse the latent vectors of the parents by leveraging the disentangled semantic vectors under the guidance of biological genetic laws. | Xiao Cui, Wengang Zhou, Houqiang Li |  |
| 118 |  |  [Explorations in Texture Learning](https://openreview.net/forum?id=gJua7kBOHz) |  | 0 | In this work, we investigate \*texture learning\*: the identification of textures learned by object classification models, and the extent to which they rely on these textures. We build texture-object associations that uncover new insights about the relationships between texture and object classes in CNNs and find three classes of results: associations that are strong and expected, strong and not expected, and expected but not present. Our analysis demonstrates that investigations in texture learning enable new methods for interpretability and have the potential to uncover unexpected biases. | Blaine Hoak, Patrick D. McDaniel |  |
| 119 |  |  [Backtracking Mathematical Reasoning of Language Models to the Pretraining Data](https://openreview.net/forum?id=otHhLO7GZj) |  | 0 | In this study, we identify subsets of model pretraining data that contribute to the math reasoning ability of language models and evaluate it on several mathematical tasks (e.g., addition, multiplication). We find that training on math-only data improves simple arithmetic but doesn't fully account for complex reasoning abilities, such as chain-of-thought reasoning. We also find that code data contributes to chain-of-thought reasoning while reducing arithmetic performance. | Yasaman Razeghi, Hamish Ivison, Sameer Singh, Yanai Elazar |  |
| 120 |  |  [Reward Bound for Behavioral Guarantee of Model-based Planning Agents](https://openreview.net/forum?id=n3ip7H2ioh) |  | 0 | Recent years have seen an emerging interest in the Verification and Validation (V\&V) of machine learning-based agents in the wild, especially in robotics, to provide safety assurance for the industry. Obtaining behavioral guarantees for these agents remains an important problem. In this work, we focus on guaranteeing a model-based planning agent reaches a goal state within a specific future time step. We show that there exists a lower bound for the reward at the goal state, such that if the said reward is below that bound, it is impossible to obtain such a guarantee. By extension, we show how to enforce preferences over multiple goals. | Zhiyu An, Xianzhong Ding, Wan Du |  |
| 121 |  |  [Can Graph Neural Networks learn node-level structural features?](https://openreview.net/forum?id=HRxVPPdyDh) |  | 0 | Graph Neural Networks (GNNs) have become one of the most widely adopted solutions for graph machine learning (GML) tasks. They perform feature learning on graphs using message passing on the network structure, avoiding the feature engineering step required for traditional tabular approaches for GML tasks. However, it is unclear which structural features GNNs can or cannot easily learn from data, especially for node- and edge-level properties. In this work, we propose a methodology to investigate which structural features GNNs can reconstruct from graph data. We conducted a first experimental analysis on one of the most used benchmarks for GML, considering some of the most well-known node-level features, such as centrality and transitivity measures. The results show that GNNs can easily reconstruct PageRank and in/out-degree centralities. But, surprisingly, GNNs can also learn centrality measures based on shortest path distances. Moreover, they reach quite good performance in learning the local clustering coefficient. | Manuel Dileo, Matteo Zignani |  |
| 122 |  |  [L-Tuning: Synchronized Label Tuning for Prompt and Prefix in LLMS](https://openreview.net/forum?id=zv3xpxqjjB) |  | 0 | Efficiently fine-tuning Large Language Models (LLMs) for specific tasks presents a considerable challenge in natural language processing. Traditional methods, like prompt or prefix tuning, typically rely on arbitrary tokens for training, leading to prolonged training times and generalized token use across various class labels. To address these issues, this paper introduces L-Tuning, an efficient fine-tuning approach designed for classification tasks within the Natural Language Inference (NLI) framework. Diverging from conventional methods, L-Tuning focuses on the fine-tuning of label tokens processed through a pre-trained LLM, thereby harnessing its pre-existing semantic knowledge. This technique not only improves the fine-tuning accuracy and efficiency but also facilitates the generation of distinct label embeddings for each class, enhancing the model's training nuance. Our experimental results indicate a significant improvement in training efficiency and classification accuracy with L-Tuning compared to traditional approaches, marking a promising advancement in fine-tuning LLMs for complex language tasks. Code is available at: https://github.com/Kowsher/L-Tuning | Md. Kowsher, Md. Shohanur Islam Sobuj, Asif Mahmud, Nusrat Jahan Prottasha, Prakash Bhat |  |
| 123 |  |  [Tracing Footprints: Neural Networks Meet Non-integer Order Differential Equations For Modelling Systems with Memory](https://openreview.net/forum?id=8518dcW4hc) |  | 0 | Neural Ordinary Differential Equations (Neural ODEs) have gained popularity for modelling real-world systems, thanks to their ability to fit ODEs to data. However, numerous systems in science and engineering often exhibit intricate memory behaviours, being classical ODEs inadequate for such tasks due to their inability to handle strong and complex memory effects. In this work, we introduce the Neural Fractional Differential Equation (Neural FDE), a Neural Network (NN) architecture to fit a FDE to data. With this we leverage the capabilities of FDEs allowing the architecture to take into account all past states and their influence on a system's current and future behaviours. Neural FDE inherently exhibits memory, providing a more accurate representation of complex phenomena in systems with long-term dependencies. Numerical experiments show Neural FDE generalises better and has faster convergence than Neural ODEs. | C. Coelho, M. Fernanda P. Costa, Luís L. Ferrás |  |
| 124 |  |  [A Generalized Semiconductor Wafer Defect Classifier](https://openreview.net/forum?id=VAJVN44B5b) |  | 0 | Silicon-based integrated circuits (ICs) and electronic devices are used in every possible electronic device, including high-performance computers fabricated from silicon wafers. Hence, ensuring the quality and reliability of silicon components is of utmost importance. This work focuses on developing and implementing computer vision and deep learning algorithms to detect defects in semiconductor manufacturing ICs, contributing to higher yields and reduced production costs. | Priyanshu Kumar Rai, Pratik Pal, Akshay Agarwal |  |
| 125 |  |  [Using the Polyak Step Size in training Convolutional Neural Networks](https://openreview.net/forum?id=QmRqSqpV0y) |  | 0 | The Polyak Step Size (PSS) is an adaptive learning rate that has yet to see much prominence in Deep Learning (DL). This paper investigates using the PSS in training Convolutional Neural Networks (CNN) for Image Classification (IC). We show that by introducing two upper bounds for the PSS, we can train accurate CNNs without the need for calculating a learning rate apriori. Additionally, we compare the upper-bounded PSS rates against other adaptive learning rate methods (AdaGrad and AdaDelta), showing that they achieve competitive performance. | Daragh King |  |
| 126 |  |  [Averaging Rate Scheduler for Decentralized Learning on Heterogeneous Data](https://openreview.net/forum?id=w9ZzNmWmjA) |  | 0 | Presently, state-of-the-art decentralized learning algorithms typically require the data distribution to be Independent and Identically Distributed (IID). However, in practical scenarios, the data distribution across the agents can have significant heterogeneity. In this work, we propose averaging rate scheduling as a simple yet effective way to reduce the impact of heterogeneity in decentralized learning. Our experiments illustrate the superiority of the proposed method ($\sim 3\%$ improvement in test accuracy) compared to the conventional approach of employing a constant averaging rate. | Sai Aparna Aketi, Sakshi Choudhary, Kaushik Roy |  |
| 127 |  |  [Audio vs. Text: Identify a Powerful Modality for Effective Hate Speech Detection](https://openreview.net/forum?id=dD2e3aCEcO) |  | 0 | The boom in social media platforms has witnessed a significant jump in offensive, hate, and toxic languages. These toxic contents leave a significant effect on one's personality which can even lead to depression and can be in various forms including audio and text. \textbf{However, the primary concern is that the benchmark datasets such as Toxigen for hate speech detection, are only text-based}. \textit{Therefore, in this research, for the first time, we have collected the audio-based hate speech dataset for unified security}. Utilizing both these modalities (text and audio), we have performed the benchmark study for audio and text hate detection and proposed a multimodal hate detection algorithm. | Kirtilekha Bhesra, Shivam Ashok Shukla, Akshay Agarwal |  |
| 128 |  |  [When Does Second-Order Optimization Speed Up Training?](https://openreview.net/forum?id=NLrfEsSZNb) |  | 0 | While numerous second-order optimization methods have been proposed to accelerate training in deep learning, they are seldom used in practice. This is partly due to a limited understanding of the conditions under which second-order optimization outperforms first-order optimization. This study aims to identify these conditions, particularly in terms of batch size and dataset size. We find empirically that second-order optimization outperforms first-order optimization when the batch size is large and the data set size is not too large. | Satoki Ishikawa, Rio Yokota |  |
| 129 |  |  [Discrete Natural Evolution Strategies](https://openreview.net/forum?id=yDdmC6Tq8s) |  | 0 | Natural evolution strategies are a class of approximate-gradient black-box optimizers that have been successfully used for continuous parameter spaces. In this paper, we derive NES algorithms for discrete parameter spaces and demonstrate their effectiveness in tasks involving discrete parameters. | Ahmad Ayaz Amin |  |
| 130 |  |  [Lost or Liberated? A Dive into Bidirectional Transformer LMs Without Positional Encoding](https://openreview.net/forum?id=Jr3XQRtn3B) |  | 0 | Recent studies have shown that Autoregressive Transformer Language Models (LMs) can generate text sequences without relying on positional encodings (PEs). This capability is attributed to the causal masks in these models, which prevent tokens from accessing information from future tokens, allowing implicit learning of token positions. On the other hand, Bidirectional LMs, such as BERT, tend to under-perform on masked language modeling tasks when PEs are omitted. This performance dip arises because transformer layers are inherently permutation equivariant; without PEs, they cannot differentiate token positions, making bidirectional processing difficult. In this study, we examine a variant of bidirectional Transformer LM that operates without PEs but incorporates causal masks in its initial layers. Our findings reveal that this configuration yields masked language modeling losses comparable to traditional transformers that use PEs. However, when tested on the GLUE language understanding benchmark, the model without PEs exhibits diminished performance. These results highlight the importance of positional encodings in bidirectional LMs and indicate that pretraining loss might not always correlate with performance on downstream tasks. | Urchade Zaratiana |  |
| 131 |  |  [Policy Gradient with Tree Search (PGTS) in Reinforcement Learning Evades Local Maxima](https://openreview.net/forum?id=61g5xxWOI6) |  | 0 | The policy gradient (PG) methods are being extensively used in practice. However, their theoretical convergence guarantees require strict regularity conditions. Such conditions are unnatural and generally not satisfied in practice, causing such techniques to get stuck in a sub-optimal local maximum (rewards). Tree search (TS) methods, have been recently shown to enjoy strong empirical performance in related planning tasks. In this work, we attempt at first theoretical analysis of Tree search-based policy gradient and its convergence properties. Specifically, we show that for a large tree length, the number of local maxima decreases, and therefore in the limiting case, PG converges to a global optimal solution. | Navdeep Kumar, Priyank Agrawal, Kfir Yehuda Levy, Shie Mannor |  |
| 132 |  |  [Emoji Kitchen with Controlled Fusion](https://openreview.net/forum?id=CmaquCIlPU) |  | 0 | The image fusion method is widely used in many different fields. The fusion processes both need models to extract semantic information and contain details. Traditional image processing techniques used for this issue have limited ability to extract semantic features from images, and advanced deep learning techniques often lose the details. In this work, we propose the Controlled Fusion Network (CFN) that adopts a multi-step progressive generation method and injects control elements at every step. We test the model in the emoji fusion task which accepts various emojis and combines them. We find that the generated emojis sufficiently retain and reasonably combine the semantic information of the input images, while the result images also conform to human intuitive perception. Our source code is released at: https://github.com/ChengAoShen/Emoji_fusion. | Chengao Shen, Siyuan Mu, Ge Diao |  |
| 133 |  |  [Learning to Reason with Autoregressive In-Context Distillation](https://openreview.net/forum?id=auvDeqEKrk) |  | 0 |  | Yuxuan Liu |  |
| 134 |  |  [ONLS: Optimal Noise Level Search in Diffusion Autoencoders Without Fine-Tuning](https://openreview.net/forum?id=Q8diCUHTZd) |  | 0 |  | Zihan Wang |  |
| 135 |  |  [SD-NAE: Generating Natural Adversarial Examples with Stable Diffusion](https://openreview.net/forum?id=D87rimdkGd) |  | 0 |  | Yueqian Lin, Jingyang Zhang, Yiran Chen, Hai Li |  |
| 136 |  |  [Adverb Is the Key: Simple Text Data Augmentation with Adverb Deletion](https://openreview.net/forum?id=bNxB3Yd3Pd) |  | 0 |  | Juhwan Choi, YoungBin Kim |  |
| 137 |  |  [Frustratingly Simple Prompting-based Text Denoising](https://openreview.net/forum?id=XlJRjhIkNi) |  | 0 |  | Jungyeul Park, Mengyang Qiu |  |
| 138 |  |  [Towards Faster Global Convergence of Robust Policy Gradient Methods](https://openreview.net/forum?id=JWviHsYSKn) |  | 0 |  | Navdeep Kumar, Ilnura Usmanova, Kfir Yehuda Levy, Shie Mannor |  |
| 139 |  |  [Policy Gradient for Reinforcement Learning with General Utilities](https://openreview.net/forum?id=XsWA2y2TyI) |  | 0 |  | Navdeep Kumar, Kaixin Wang, Utkarsh Pratiush, Kfir Yehuda Levy, Shie Mannor |  |
| 140 |  |  [Investigating Representations for Vision And Touch in Contact Rich Robot Scooping Tasks](https://openreview.net/forum?id=p8GJzu52XL) |  | 0 |  | JiEun Jung |  |
| 141 |  |  [Semantic Patch Embedding for Security Detection: A Fine-to-Coarse Grained Approach](https://openreview.net/forum?id=zAst1ulgP2) |  | 0 |  | Xunzhu Tang, Yewei Song, Haoye Tian, Zhenghan Chen, Jacques Klein, Tegawendé F. Bissyandé |  |
| 142 |  |  [A Systematic Study of the Role of Data Quality and Alignment for Fine-tuning LLMs for Enhanced Autoformalization](https://openreview.net/forum?id=9LkPbnRwXu) |  | 0 |  | Krrish Chawla, Mario DePavia, Aryan Sahai, Brando Miranda |  |
| 143 |  |  [Toward Learning Latent-Variable Representations of Microstructures by Optimizing in Spatial Statistics Space](https://openreview.net/forum?id=XAXGOq1isw) |  | 0 |  | Sayed Sajad Hashemi, Michael Guerzhoy, Noah H. Paulson |  |
| 144 |  |  [PhoWhisper: Automatic Speech Recognition for Vietnamese](https://openreview.net/forum?id=x3c3MkJfpG) |  | 0 |  | ThanhThien Le, Linh The Nguyen, Dat Quoc Nguyen |  |
| 145 |  |  [NIRo: A Metric to capture non-iid robustness for Federated Learning Algorithms](https://openreview.net/forum?id=Bq9SgpL0dX) |  | 0 |  | Anupam Gupta, Pabitra Mitra |  |
| 146 |  |  [Pre-Tokenization of Numbers for Large Language Models](https://openreview.net/forum?id=bv8n5aYP1l) |  | 0 |  | Zhenglong Wu, Qi Qi, Zirui Zhuang, Haifeng Sun, Jingyu Wang |  |
| 147 |  |  [Back to the Future: predicting causal relationships influencing oil prices](https://openreview.net/forum?id=TqQlYLd52W) |  | 0 |  | Joze M. Rozanec, Beno Sircelj, Michael Cochez, Gregor Leban |  |
| 148 |  |  [Balancing performance and complexity with adaptive graph coarsening](https://openreview.net/forum?id=DrHwIzz93C) |  | 0 |  | Marek Dedic, Lukás Bajer, Pavel Procházka, Martin Holena |  |
| 149 |  |  [Probing the Hidden Layers of a Music Generating Language Model](https://openreview.net/forum?id=2Dlx6YgS3n) |  | 0 |  | Rafik Hachana |  |
| 150 |  |  [A Study on Polarity distributions for Network Learning](https://openreview.net/forum?id=0Ddq3BtHsH) |  | 0 |  | Aoife Igoe, Arindam Biswas, Biswajit Basu |  |
| 151 |  |  [Layered insights into Pyramid feature fusion architecture for SSL](https://openreview.net/forum?id=E1kT452SdL) |  | 0 |  | Ashrya Agrawal, Priyanshi Shah, Sourabh Prakash |  |
| 152 |  |  [Advancing Image Classification through Parameter-Efficient Fine-Tuning: A Study on LoRA with Plant Disease Detection Datasets](https://openreview.net/forum?id=lPnNhcqLSi) |  | 0 |  | Deeksha Aggarwal, Yash Mittal, Uttam Kumar |  |
| 153 |  |  [Exploring Dimensional Collapse in Self-Supervised Video Representation Learning](https://openreview.net/forum?id=iDeRIyYTop) |  | 0 |  | Paul Kapust, Monika Kwiatkowski, Olaf Hellwich, Patrik Reiske |  |
| 154 |  |  [Graph Expansion in Pruned Recurrent Neural Network Layers Preserves Performance](https://openreview.net/forum?id=hG5eu7ikDy) |  | 0 |  | Suryam Arnav Kalra, Pabitra Mitra, Arindam Biswas, Biswajit Basu |  |
| 155 |  |  [Bayesian-Driven Learning of A New Weighted Tensor Norm for Tensor Recovery](https://openreview.net/forum?id=ciEbMa2xuC) |  | 0 |  | Jingjing Zheng, Yankai Cao |  |
| 156 |  |  [Racial and Gender Stereotypes Encoded Into CLIP Representations](https://openreview.net/forum?id=hQb6ts30wv) |  | 0 |  | Vatsal Baherwani, Joseph James Vincent |  |
| 157 |  |  [Native machine learning for noisy microscopic data processing](https://openreview.net/forum?id=5sWY5ENwtl) |  | 0 |  | Danil Afonchikov, Elena Kornaeva, Irina Makovik, Alexey Kornaev |  |
| 158 |  |  [Learning from Graphs Beyond Message Passing Neural Networks](https://openreview.net/forum?id=m6pvyXIQcu) |  | 0 |  | Tong Zhao, Neil Shah, Elham Ghazizadeh |  |
| 159 |  |  [Fusing Vision and Language Models to Generate Sequence of Recipe Images from Steps](https://openreview.net/forum?id=McFvoo7s6V) |  | 0 |  | Hshmat Sahak |  |
| 160 |  |  [Autonomous Generation of Innovative Content by Multi-Agent System](https://openreview.net/forum?id=Ed5vlTvI0u) |  | 0 |  | Rui Hao, Weikai Xie |  |
| 161 |  |  [Semi-supervised Learning under Self-training via $f$-Divergence](https://openreview.net/forum?id=3cZuCyJSe7) |  | 0 |  | Gholamali Aminian, Amirhossein Bagheri, Radmehr Karimian, Mahyar JafariNodeh, Mohammad Hossein Yassaee |  |
| 162 |  |  [Ai Gen ASSISTment: Analyzing the Effectiveness of Generative AI Data Amplification for Dkt](https://openreview.net/forum?id=ZZB8z9hIjR) |  | 0 |  | NohMyongSung, Cho Ung Hui |  |
| 163 |  |  [LAMPER: LanguAge Model and Prompt EngineeRing for zero-shot time series classification](https://openreview.net/forum?id=eUQiESSvmF) |  | 0 |  | Zhicheng Du, Zhaotian Xie, Yan Tong, Peiwu Qin |  |
| 164 |  |  [Synthetic Labeling: A Novel Approach to Advancing Few-Shot Learning](https://openreview.net/forum?id=x7azREBRyw) |  | 0 |  | Zhaoyan Lyu, Gholamali Aminian, Miguel R. D. Rodrigues |  |
| 165 |  |  [Generation of a random self-similarity curve](https://openreview.net/forum?id=FVR89igilr) |  | 0 |  | Ilya Pershin, Dmitrii Tumakov |  |
| 166 |  |  [User Modeling Challenges in Interactive AI Assistant Systems](https://openreview.net/forum?id=YEjIcZpqed) |  | 0 |  | Megan Su, Yuwei Bao |  |
| 167 |  |  [Discerning Self-supervised Learning and Weakly Supervised Learning](https://openreview.net/forum?id=VQWuHYeTL6) |  | 0 |  | Chandan Kumar, Ali Jannesari, Matthew J. Darr |  |
| 168 |  |  [A Pre-Search Evaluation Framework for Assessing Search Space Complexity](https://openreview.net/forum?id=M8V7qBhDVU) |  | 0 |  | Hadjer Benmeziane |  |
| 169 |  |  [Large Language Models for Wearable Data Analysis and Interpretation](https://openreview.net/forum?id=GoWD6logcd) |  | 0 |  | Simon Böhi, Shkurta Gashi |  |
| 170 |  |  [Empirical Evaluations of Personalized Federated Learning on Heterogeneous Electronic Health Records](https://openreview.net/forum?id=iaTN5lQsSu) |  | 0 |  | Yuqing Shang, Qiming Wu, Siqi Li, Di Miao |  |
| 171 |  |  [Beyond Time: Accurately Estimating the Fair Value of Stocks with Machine Learning and Fundamental Data](https://openreview.net/forum?id=ujHSaZdpFb) |  | 0 |  | Daniel Netzl |  |
| 172 |  |  [Observations on Building RAG Systems for Technical Documents](https://openreview.net/forum?id=RFujq4HoV4) |  | 0 |  | Sumit Soman, Sujoy Roychowdhury |  |
| 173 |  |  [Sound Classification in Indian Cities Using Multi-Label Data and Transfer Learning](https://openreview.net/forum?id=UWszefaphF) |  | 0 |  | Rishi Gupta, Vaibhav Kumar |  |
| 174 |  |  [Causal Covariate Shift Correction using Fisher information penalty](https://openreview.net/forum?id=mhIL5JQr3d) |  | 0 |  | Behraj Khan, Behroz Mirza, Tahir Syed |  |
| 175 |  |  [Q-Learning as a montone scheme](https://openreview.net/forum?id=xEx5Uyc0c4) |  | 0 |  | Lingyi Yang |  |
| 176 |  |  [RA-NER: Retrieval augmented NER for knowledge intensive named entity recognition](https://openreview.net/forum?id=HyKNjew2Ad) |  | 0 |  | Zhenwei Dai, Chen Luo, Zhen Li, Xianfeng Tang, Hanqing Lu, Rahul Goutam, Haiyang Zhang |  |
| 177 |  |  [On the Robustness of Drug Abuse Face Classification](https://openreview.net/forum?id=Jdk8o63wyP) |  | 0 |  | Hruturaj Dhake, Akshay Agarwal |  |
| 178 |  |  [Federated Learning on Small Batch Sizes via Batch Renormalization](https://openreview.net/forum?id=TrvOETLeYO) |  | 0 |  | Rubing Xue, Jiaming Pei, Lukun Wang |  |
| 179 |  |  [t-Divergence: A New Divergence Measure with Application to Robust Statistics & Clustering](https://openreview.net/forum?id=xusEi6Fx3y) |  | 0 |  | Debolina Paul, Saptarshi Chakraborty, Swagatam Das |  |
| 180 |  |  [Stacking/Ensemble Model for Smartphone-Based Human Activity Recognition using Feature Engineering](https://openreview.net/forum?id=hbSJSFe2mM) |  | 0 |  | Pratik Pal, Priyanshu Kumar Rai |  |
| 181 |  |  [Visualizing Information Conservation and Decomposition via the Information Matrix](https://openreview.net/forum?id=GNlfjLepnP) |  | 0 |  | Dor Tsur, Haim H. Permuter |  |
| 182 |  |  [Evaluating the Efficacy of Federated Scoring Systems with Heterogeneous Electronic Health Records](https://openreview.net/forum?id=c4GVRbEx1g) |  | 0 |  | Qiming Wu, Siqi Li, Di Miao, Yuqing Shang, Xin Li, Nan Liu |  |
| 183 |  |  [A Novel Sector-Based Algorithm for an Optimized Star-Galaxy Classification](https://openreview.net/forum?id=HzEefCle2c) |  | 0 |  | Anumanchi Agastya Sai Ram Likhit, Divyansh Tripathi, Akshay Agarwal |  |
| 184 |  |  [Paraphrase Loss for Abstractive Summarization](https://openreview.net/forum?id=7Zzg5rpwm0) |  | 0 |  | Daniele Rege Cambrin, Paolo Garza |  |
| 185 |  |  [Exo-Spacetimeformer: Time Series Prediction with External Forecast Integration](https://openreview.net/forum?id=uyfz7fAcvu) |  | 0 |  | Boris Kraychev, Ensiye Kiyamousavi |  |
| 186 |  |  [WiFi CSI-based Long-Range Person Localization Using Directional Antennas](https://openreview.net/forum?id=AOJFcEh5Eb) |  | 0 |  | Julian Strohmayer, Martin Kampel |  |
| 187 |  |  [DDA: A dual-domain attention plug-and-play prior for pansharpening](https://openreview.net/forum?id=SACKV1UWc6) |  | 0 |  | Wenjie Shu, Zien Zhang |  |
| 188 |  |  [SDSRA: A Skill-Driven Skill-Recombination Algorithm for Efficient Policy Learning](https://openreview.net/forum?id=0aOh1KRECK) |  | 0 |  | Eric Hanchen Jiang, Andrew Lizarraga |  |
| 189 |  |  [Enhancing Fairness in In-Context Learning: Prioritizing Minority Samples in Demonstrations](https://openreview.net/forum?id=LrPSJTNfmt) |  | 0 |  | Jingyu Hu, Mengnan Du |  |
| 190 |  |  [Entropy-aided prompt Federated learning](https://openreview.net/forum?id=P7MVXYI3Xx) |  | 0 |  | Jiaming Pei, Haotian Wu |  |
| 191 |  |  [Distributionally Robust Federated Learning with Wasserstein Barycenter](https://openreview.net/forum?id=k2DMAGoide) |  | 0 |  | Wenqian Li, Shuran Fu, Yan Pang |  |
| 192 |  |  [Solar Panel Segmentation: Self-Supervised Learning Solutions for Imperfect Datasets](https://openreview.net/forum?id=V4kjFcopPS) |  | 0 |  | Sankarshanaa Sagaram, Laven Srivastava, Krish Didwania, Aditya Kasliwal, Pallavi Kailas, Ujjwal Verma |  |
| 193 |  |  [Combinatorial CNNs for words](https://openreview.net/forum?id=Htn3pRs2v7) |  | 0 |  | Karen Sargsyan |  |
| 194 |  |  [Generalize Neural Network Through Smooth Hypothesis Function](https://openreview.net/forum?id=RpiiKMGaK3) |  | 0 |  | Yupu Yao |  |
| 195 |  |  [Even a single simple augmentation with Self-Supervised Learning can be helpful for the downstream tasks](https://openreview.net/forum?id=FfkW423EmB) |  | 0 |  | Evgenii Pishchik |  |
| 196 |  |  [Proving Test Set Contamination in Black-Box Language Models](https://openreview.net/forum?id=KS8mIvetg2) |  | 0 |  | Yonatan Oren, Nicole Meister, Niladri S. Chatterji, Faisal Ladhak, Tatsunori Hashimoto |  |
| 197 |  |  [BooookScore: A systematic exploration of book-length summarization in the era of LLMs](https://openreview.net/forum?id=7Ttk3RzDeu) |  | 0 |  | Yapei Chang, Kyle Lo, Tanya Goyal, Mohit Iyyer |  |
| 198 |  |  [Generalization in diffusion models arises from geometry-adaptive harmonic representations](https://openreview.net/forum?id=ANvmVS2Yr0) |  | 0 |  | Zahra Kadkhodaie, Florentin Guth, Eero P. Simoncelli, Stéphane Mallat |  |
| 199 |  |  [Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions](https://openreview.net/forum?id=ekeyCgeRfC) |  | 0 |  | Satwik Bhattamishra, Arkil Patel, Phil Blunsom, Varun Kanade |  |
| 200 |  |  [The mechanistic basis of data dependence and abrupt learning in an in-context classification task](https://openreview.net/forum?id=aN4Jf6Cx69) |  | 0 |  | Gautam Reddy |  |
| 201 |  |  [Improved Techniques for Training Consistency Models](https://openreview.net/forum?id=WNzy9bRDvG) |  | 0 |  | Yang Song, Prafulla Dhariwal |  |
| 202 |  |  [Provable Compositional Generalization for Object-Centric Learning](https://openreview.net/forum?id=7VPTUWkiDQ) |  | 0 |  | Thaddäus Wiedemer, Jack Brady, Alexander Panfilov, Attila Juhos, Matthias Bethge, Wieland Brendel |  |
| 203 |  |  [Predictive auxiliary objectives in deep RL mimic learning in the brain](https://openreview.net/forum?id=agPpmEgf8C) |  | 0 |  | Ching Fang, Kim Stachenfeld |  |
| 204 |  |  [Pre-Training Goal-based Models for Sample-Efficient Reinforcement Learning](https://openreview.net/forum?id=o2IEmeLL9r) |  | 0 |  | Haoqi Yuan, Zhancun Mu, Feiyang Xie, Zongqing Lu |  |
| 205 |  |  [Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!](https://openreview.net/forum?id=hTEGyKf0dZ) |  | 0 |  | Xiangyu Qi, Yi Zeng, Tinghao Xie, PinYu Chen, Ruoxi Jia, Prateek Mittal, Peter Henderson |  |
| 206 |  |  [Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors](https://openreview.net/forum?id=PdaPky8MUn) |  | 0 |  | Ido Amos, Jonathan Berant, Ankit Gupta |  |
| 207 |  |  [LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models](https://openreview.net/forum?id=LzPWWPAdY4) |  | 0 |  | Yixiao Li, Yifan Yu, Chen Liang, Nikos Karampatziakis, Pengcheng He, Weizhu Chen, Tuo Zhao |  |
| 208 |  |  [Graph Neural Networks for Learning Equivariant Representations of Neural Networks](https://openreview.net/forum?id=oO6FsMyDBt) |  | 0 |  | Miltiadis Kofinas, Boris Knyazev, Yan Zhang, Yunlu Chen, Gertjan J. Burghouts, Efstratios Gavves, Cees G. M. Snoek, David W. Zhang |  |
| 209 |  |  [GNNCert: Deterministic Certification of Graph Neural Networks against Adversarial Perturbations](https://openreview.net/forum?id=IGzaH538fz) |  | 0 |  | Zaishuo Xia, Han Yang, Binghui Wang, Jinyuan Jia |  |
| 210 |  |  [Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning](https://openreview.net/forum?id=LjivA1SLZ6) |  | 0 |  | Hyungho Na, Yunkyeong Seo, IlChul Moon |  |
| 211 |  |  [ClimODE: Climate and Weather Forecasting with Physics-informed Neural ODEs](https://openreview.net/forum?id=xuY33XhEGR) |  | 0 |  | Yogesh Verma, Markus Heinonen, Vikas Garg |  |
| 212 |  |  [Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space](https://openreview.net/forum?id=4Ay23yeuz0) |  | 0 |  | Hengrui Zhang, Jiani Zhang, Zhengyuan Shen, Balasubramaniam Srinivasan, Xiao Qin, Christos Faloutsos, Huzefa Rangwala, George Karypis |  |
| 213 |  |  [Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement](https://openreview.net/forum?id=bNt7oajl2a) |  | 0 |  | Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, Xiang Ren |  |
| 214 |  |  [Beyond Weisfeiler-Lehman: A Quantitative Framework for GNN Expressiveness](https://openreview.net/forum?id=HSKaGOi7Ar) |  | 0 |  | Bohang Zhang, Jingchu Gai, Yiheng Du, Qiwei Ye, Di He, Liwei Wang |  |
| 215 |  |  [MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts](https://openreview.net/forum?id=KUNzEQMWU7) |  | 0 |  | Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, Jianfeng Gao |  |
| 216 |  |  [Protein Discovery with Discrete Walk-Jump Sampling](https://openreview.net/forum?id=zMPHKOmQNb) |  | 0 |  | Nathan C. Frey, Daniel Berenberg, Karina Zadorozhny, Joseph Kleinhenz, Julien LafranceVanasse, Isidro Hötzel, Yan Wu, Stephen Ra, Richard Bonneau, Kyunghyun Cho, Andreas Loukas, Vladimir Gligorijevic, Saeed Saremi |  |
| 217 |  |  [ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis](https://openreview.net/forum?id=oTRwljRgiv) |  | 0 |  | Kensen Shi, Joey Hong, Yinlin Deng, Pengcheng Yin, Manzil Zaheer, Charles Sutton |  |
| 218 |  |  [Batched Low-Rank Adaptation of Foundation Models](https://openreview.net/forum?id=w4abltTZ2f) |  | 0 |  | Yeming Wen, Swarat Chaudhuri |  |
| 219 |  |  [Improved Active Learning via Dependent Leverage Score Sampling](https://openreview.net/forum?id=IYxDy2jDFL) |  | 0 |  | Atsushi Shimizu, Xiaoou Cheng, Christopher Musco, Jonathan Weare |  |
| 220 |  |  [Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs](https://openreview.net/forum?id=uNrFpDPMyo) |  | 0 |  | Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, Jianfeng Gao |  |
| 221 |  |  [One-shot Empirical Privacy Estimation for Federated Learning](https://openreview.net/forum?id=0BqyZSWfzo) |  | 0 |  | Galen Andrew, Peter Kairouz, Sewoong Oh, Alina Oprea, Hugh Brendan McMahan, Vinith Menon Suriyakumar |  |
| 222 |  |  [SWE-bench: Can Language Models Resolve Real-world Github Issues?](https://openreview.net/forum?id=VTF8yNQM66) |  | 0 |  | Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik R. Narasimhan |  |
| 223 |  |  [ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models](https://openreview.net/forum?id=osoWxY8q2E) |  | 0 |  | Iman Mirzadeh, Keivan AlizadehVahid, Sachin Mehta, Oncel Tuzel, Golnoosh Samei, Mohammad Rastegari, Mehrdad Farajtabar |  |
| 224 |  |  [On the Joint Interaction of Models, Data, and Features](https://openreview.net/forum?id=ze7DOLi394) |  | 0 |  | Yiding Jiang, Christina Baek, J. Zico Kolter |  |
| 225 |  |  [Topological data analysis on noisy quantum computers](https://openreview.net/forum?id=dLrhRIMVmB) |  | 0 |  | Ismail Yunus Akhalwaya, Shashanka Ubaru, Kenneth L. Clarkson, Mark S. Squillante, Vishnu Jejjala, YangHui He, Kugendran Naidoo, Vasileios Kalantzis, Lior Horesh |  |
| 226 |  |  [Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection](https://openreview.net/forum?id=hSyW5go0v8) |  | 0 |  | Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi |  |
| 227 |  |  ["What Data Benefits My Classifier?" Enhancing Model Performance and Interpretability through Influence-Based Data Selection](https://openreview.net/forum?id=HE9eUQlAvo) |  | 0 |  | Anshuman Chhabra, Peizhao Li, Prasant Mohapatra, Hongfu Liu |  |
| 228 |  |  [Generative Modeling with Phase Stochastic Bridge](https://openreview.net/forum?id=tUtGjQEDd4) |  | 0 |  | Tianrong Chen, Jiatao Gu, Laurent Dinh, Evangelos A. Theodorou, Joshua M. Susskind, Shuangfei Zhai |  |
| 229 |  |  [Zipformer: A faster and better encoder for automatic speech recognition](https://openreview.net/forum?id=9WD9KwssyT) |  | 0 |  | Zengwei Yao, Liyong Guo, Xiaoyu Yang, Wei Kang, Fangjun Kuang, Yifan Yang, Zengrui Jin, Long Lin, Daniel Povey |  |
| 230 |  |  [MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework](https://openreview.net/forum?id=VtmBAGCN7o) |  | 0 |  | Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, Jürgen Schmidhuber |  |
| 231 |  |  [ValUES: A Framework for Systematic Validation of Uncertainty Estimation in Semantic Segmentation](https://openreview.net/forum?id=yV6fD7LYkF) |  | 0 |  | KimCeline Kahl, Carsten T. Lüth, Maximilian Zenk, Klaus H. MaierHein, Paul F. Jaeger |  |
| 232 |  |  [Finetuning Text-to-Image Diffusion Models for Fairness](https://openreview.net/forum?id=hnrB5YHoYu) |  | 0 |  | Xudong Shen, Chao Du, Tianyu Pang, Min Lin, Yongkang Wong, Mohan S. Kankanhalli |  |
| 233 |  |  [Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models](https://openreview.net/forum?id=WbWtOYIzIK) |  | 0 |  | Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, Yulia Tsvetkov |  |
| 234 |  |  [METRA: Scalable Unsupervised RL with Metric-Aware Abstraction](https://openreview.net/forum?id=c5pwL0Soay) |  | 0 |  | Seohong Park, Oleh Rybkin, Sergey Levine |  |
| 235 |  |  [Meta Continual Learning Revisited: Implicitly Enhancing Online Hessian Approximation via Variance Reduction](https://openreview.net/forum?id=TpD2aG1h0D) |  | 0 |  | Yichen Wu, LongKai Huang, Renzhen Wang, Deyu Meng, Ying Wei |  |
| 236 |  |  [Improving Convergence and Generalization Using Parameter Symmetries](https://openreview.net/forum?id=L0r0GphlIL) |  | 0 |  | Bo Zhao, Robert M. Gower, Robin Walters, Rose Yu |  |
| 237 |  |  [Flow Matching on General Geometries](https://openreview.net/forum?id=g7ohDlTITL) |  | 0 |  | Ricky T. Q. Chen, Yaron Lipman |  |
| 238 |  |  [Ghost on the Shell: An Expressive Representation of General 3D Shapes](https://openreview.net/forum?id=Ad87VjRqUw) |  | 0 |  | Zhen Liu, Yao Feng, Yuliang Xiu, Weiyang Liu, Liam Paull, Michael J. Black, Bernhard Schölkopf |  |
| 239 |  |  [Würstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models](https://openreview.net/forum?id=gU58d5QeGv) |  | 0 |  | Pablo Pernias, Dominic Rampas, Mats L. Richter, Christopher Pal, Marc Aubreville |  |
| 240 |  |  [Unified Generative Modeling of 3D Molecules with Bayesian Flow Networks](https://openreview.net/forum?id=NSVtmmzeRB) |  | 0 |  | Yuxuan Song, Jingjing Gong, Hao Zhou, Mingyue Zheng, Jingjing Liu, WeiYing Ma |  |
| 241 |  |  [Small-scale proxies for large-scale Transformer training instabilities](https://openreview.net/forum?id=d8w0pmvXbZ) |  | 0 |  | Mitchell Wortsman, Peter J. Liu, Lechao Xiao, Katie E. Everett, Alexander A. Alemi, Ben Adlam, John D. CoReyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Pennington, Jascha SohlDickstein, Kelvin Xu, Jaehoon Lee, Justin Gilmer, Simon Kornblith |  |
| 242 |  |  [How I Warped Your Noise: a Temporally-Correlated Noise Prior for Diffusion Models](https://openreview.net/forum?id=pzElnMrgSD) |  | 0 |  | Pascal Chang, Jingwei Tang, Markus Gross, Vinicius C. Azevedo |  |
| 243 |  |  [Vision Transformers Need Registers](https://openreview.net/forum?id=2dnO3LLiJ1) |  | 0 |  | Timothée Darcet, Maxime Oquab, Julien Mairal, Piotr Bojanowski |  |
| 244 |  |  [An Analytical Solution to Gauss-Newton Loss for Direct Image Alignment](https://openreview.net/forum?id=mE52zURNGc) |  | 0 |  | Sergei Solonets, Daniil Sinitsyn, Lukas von Stumberg, Nikita Araslanov, Daniel Cremers |  |
| 245 |  |  [Learning Energy Decompositions for Partial Inference in GFlowNets](https://openreview.net/forum?id=P15CHILQlg) |  | 0 |  | Hyosoon Jang, Minsu Kim, Sungsoo Ahn |  |
| 246 |  |  [Approximating Nash Equilibria in Normal-Form Games via Stochastic Optimization](https://openreview.net/forum?id=cc8h3I3V4E) |  | 0 |  | Ian Gemp, Luke Marris, Georgios Piliouras |  |
| 247 |  |  [Multi-Source Diffusion Models for Simultaneous Music Generation and Separation](https://openreview.net/forum?id=h922Qhkmx1) |  | 0 |  | Giorgio Mariani, Irene Tallini, Emilian Postolache, Michele Mancusi, Luca Cosmo, Emanuele Rodolà |  |
| 248 |  |  [LEGO-Prover: Neural Theorem Proving with Growing Libraries](https://openreview.net/forum?id=3f5PALef5B) |  | 0 |  | Haiming Wang, Huajian Xin, Chuanyang Zheng, Zhengying Liu, Qingxing Cao, Yinya Huang, Jing Xiong, Han Shi, Enze Xie, Jian Yin, Zhenguo Li, Xiaodan Liang |  |
| 249 |  |  [ASID: Active Exploration for System Identification in Robotic Manipulation](https://openreview.net/forum?id=jNR6s6OSBT) |  | 0 |  | Marius Memmel, Andrew Wagenmaker, Chuning Zhu, Dieter Fox, Abhishek Gupta |  |
| 250 |  |  [Towards a statistical theory of data selection under weak supervision](https://openreview.net/forum?id=HhfcNgQn6p) |  | 0 |  | Germain Kolossov, Andrea Montanari, Pulkit Tandon |  |
| 251 |  |  [Mastering Memory Tasks with World Models](https://openreview.net/forum?id=1vDArHJ68h) |  | 0 |  | Mohammad Reza Samsami, Artem Zholus, Janarthanan Rajendran, Sarath Chandar |  |
| 252 |  |  [Monte Carlo guided Denoising Diffusion models for Bayesian linear inverse problems](https://openreview.net/forum?id=nHESwXvxWK) |  | 0 |  | Gabriel Cardoso, Yazid Janati El Idrissi, Sylvain Le Corff, Eric Moulines |  |
| 253 |  |  [Self-Alignment with Instruction Backtranslation](https://openreview.net/forum?id=1oijHJBRsT) |  | 0 |  | Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason Weston, Mike Lewis |  |
| 254 |  |  [Learning Interactive Real-World Simulators](https://openreview.net/forum?id=sFyTZEqmUY) |  | 0 |  | Sherry Yang, Yilun Du, Seyed Kamyar Seyed Ghasemipour, Jonathan Tompson, Leslie Pack Kaelbling, Dale Schuurmans, Pieter Abbeel |  |
| 255 |  |  [Candidate Label Set Pruning: A Data-centric Perspective for Deep Partial-label Learning](https://openreview.net/forum?id=Fk5IzauJ7F) |  | 0 |  | Shuo He, Chaojie Wang, Guowu Yang, Lei Feng |  |
| 256 |  |  [Robust agents learn causal world models](https://openreview.net/forum?id=pOoKI3ouv1) |  | 0 |  | Jonathan Richens, Tom Everitt |  |
| 257 |  |  [On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs](https://openreview.net/forum?id=H3UayAQWoE) |  | 0 |  | Jentse Huang, Wenxuan Wang, Eric John Li, Man Ho Lam, Shujie Ren, Youliang Yuan, Wenxiang Jiao, Zhaopeng Tu, Michael R. Lyu |  |
| 258 |  |  [Diffusion Model for Dense Matching](https://openreview.net/forum?id=Zsfiqpft6K) |  | 0 |  | Jisu Nam, Gyuseong Lee, Sunwoo Kim, Hyeonsu Kim, Hyoungwon Cho, Seyeon Kim, Seungryong Kim |  |
| 259 |  |  [Is ImageNet worth 1 video? Learning strong image encoders from 1 long unlabelled video](https://openreview.net/forum?id=Yen1lGns2o) |  | 0 |  | Shashanka Venkataramanan, Mamshad Nayeem Rizve, João Carreira, Yuki M. Asano, Yannis Avrithis |  |
| 260 |  |  [Neural Fine-Tuning Search for Few-Shot Learning](https://openreview.net/forum?id=T7YV5UZKBc) |  | 0 |  | Panagiotis Eustratiadis, Lukasz Dudziak, Da Li, Timothy M. Hospedales |  |
| 261 |  |  [Latent Trajectory Learning for Limited Timestamps under Distribution Shift over Time](https://openreview.net/forum?id=bTMMNT7IdW) |  | 0 |  | Qiuhao Zeng, Changjian Shui, LongKai Huang, Peng Liu, Xi Chen, Charles Ling, Boyu Wang |  |
| 262 |  |  [Less is More: Fewer Interpretable Region via Submodular Subset Selection](https://openreview.net/forum?id=jKTUlxo5zy) |  | 0 |  | Ruoyu Chen, Hua Zhang, Siyuan Liang, Jingzhi Li, Xiaochun Cao |  |
| 263 |  |  [Cameras as Rays: Pose Estimation via Ray Diffusion](https://openreview.net/forum?id=EanCFCwAjM) |  | 0 |  | Jason Y. Zhang, Amy Lin, Moneish Kumar, TzuHsuan Yang, Deva Ramanan, Shubham Tulsiani |  |
| 264 |  |  [Accelerating Distributed Stochastic Optimization via Self-Repellent Random Walks](https://openreview.net/forum?id=BV1PHbTJzd) |  | 0 |  | Jie Hu, Vishwaraj Doshi, Do Young Eun |  |
| 265 |  |  [Detecting, Explaining, and Mitigating Memorization in Diffusion Models](https://openreview.net/forum?id=84n3UwkH7b) |  | 0 |  | Yuxin Wen, Yuchen Liu, Chen Chen, Lingjuan Lyu |  |
| 266 |  |  [Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How](https://openreview.net/forum?id=tqh1zdXIra) |  | 0 |  | Sebastian PinedaArango, Fabio Ferreira, Arlind Kadra, Frank Hutter, Josif Grabocka |  |
| 267 |  |  [LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models](https://openreview.net/forum?id=6PmJoRfdaK) |  | 0 |  | Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, Jiaya Jia |  |
| 268 |  |  [Amortizing intractable inference in large language models](https://openreview.net/forum?id=Ouj6p4ca60) |  | 0 |  | Edward J. Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, Nikolay Malkin |  |
| 269 |  |  [LLMCarbon: Modeling the End-to-End Carbon Footprint of Large Language Models](https://openreview.net/forum?id=aIok3ZD9to) |  | 0 |  | Ahmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Chukwunyere Osi, Prateek Sharma, Fan Chen, Lei Jiang |  |
| 270 |  |  [A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis](https://openreview.net/forum?id=9JQtrumvg8) |  | 0 |  | Izzeddin Gur, Hiroki Furuta, Austin V. Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, Aleksandra Faust |  |
| 271 |  |  [Lipschitz Singularities in Diffusion Models](https://openreview.net/forum?id=WNkW0cOwiz) |  | 0 |  | Zhantao Yang, Ruili Feng, Han Zhang, Yujun Shen, Kai Zhu, Lianghua Huang, Yifei Zhang, Yu Liu, Deli Zhao, Jingren Zhou, Fan Cheng |  |
| 272 |  |  [Interpreting CLIP's Image Representation via Text-Based Decomposition](https://openreview.net/forum?id=5Ca9sSzuDp) |  | 0 |  | Yossi Gandelsman, Alexei A. Efros, Jacob Steinhardt |  |
| 273 |  |  [Multisize Dataset Condensation](https://openreview.net/forum?id=FVhmnvqnsI) |  | 0 |  | Yang He, Lingao Xiao, Joey Tianyi Zhou, Ivor W. Tsang |  |
| 274 |  |  [DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation](https://openreview.net/forum?id=UyNXMqnN3c) |  | 0 |  | Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, Gang Zeng |  |
| 275 |  |  [LRM: Large Reconstruction Model for Single Image to 3D](https://openreview.net/forum?id=sllU8vvsFF) |  | 0 |  | Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, Hao Tan |  |
| 276 |  |  [How Well Do Supervised 3D Models Transfer to Medical Imaging Tasks?](https://openreview.net/forum?id=AhizIPytk4) |  | 0 |  | Wenxuan Li, Alan L. Yuille, Zongwei Zhou |  |
| 277 |  |  [Gene Regulatory Network Inference in the Presence of Dropouts: a Causal View](https://openreview.net/forum?id=gFR4QwK53h) |  | 0 |  | Haoyue Dai, Ignavier Ng, Gongxu Luo, Peter Spirtes, Petar Stojanov, Kun Zhang |  |
| 278 |  |  [Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming](https://openreview.net/forum?id=v7ZPwoHU1j) |  | 0 |  | Yubo Zhuang, Xiaohui Chen, Yun Yang, Richard Y. Zhang |  |
| 279 |  |  [Unprocessing Seven Years of Algorithmic Fairness](https://openreview.net/forum?id=jr03SfWsBS) |  | 0 |  | André F. Cruz, Moritz Hardt |  |
| 280 |  |  [InfoBatch: Lossless Training Speed Up by Unbiased Dynamic Data Pruning](https://openreview.net/forum?id=C61sk5LsK6) |  | 0 |  | Ziheng Qin, Kai Wang, Zangwei Zheng, Jianyang Gu, Xiangyu Peng, Zhaopan Xu, Daquan Zhou, Lei Shang, Baigui Sun, Xuansong Xie, Yang You |  |
| 281 |  |  [Multi-granularity Correspondence Learning from Long-term Noisy Videos](https://openreview.net/forum?id=9Cu8MRmhq2) |  | 0 |  | Yijie Lin, Jie Zhang, Zhenyu Huang, Jia Liu, Zujie Wen, Xi Peng |  |
| 282 |  |  [SaNN: Simple Yet Powerful Simplicial-aware Neural Networks](https://openreview.net/forum?id=eUgS9Ig8JG) |  | 0 |  | Sravanthi Gurugubelli, Sundeep Prabhakar Chepuri |  |
| 283 |  |  [Beyond Memorization: Violating Privacy via Inference with Large Language Models](https://openreview.net/forum?id=kmn0BhQk7p) |  | 0 |  | Robin Staab, Mark Vero, Mislav Balunovic, Martin T. Vechev |  |
| 284 |  |  [Controlled Text Generation via Language Model Arithmetic](https://openreview.net/forum?id=SLw9fp4yI6) |  | 0 |  | Jasper Dekoninck, Marc Fischer, Luca BeurerKellner, Martin T. Vechev |  |
| 285 |  |  [Consistency Training with Learnable Data Augmentation for Graph Anomaly Detection with Limited Supervision](https://openreview.net/forum?id=elMKXvhhQ9) |  | 0 |  | Nan Chen, Zemin Liu, Bryan Hooi, Bingsheng He, Rizal Fathony, Jun Hu, Jia Chen |  |
| 286 |  |  [Symmetric Mean-field Langevin Dynamics for Distributional Minimax Problems](https://openreview.net/forum?id=YItWKZci78) |  | 0 |  | Juno Kim, Kakei Yamamoto, Kazusato Oko, Zhuoran Yang, Taiji Suzuki |  |
| 287 |  |  [Generalized Policy Iteration using Tensor Approximation for Hybrid Control](https://openreview.net/forum?id=csukJcpYDe) |  | 0 |  | Suhan Shetty, Teng Xue, Sylvain Calinon |  |
| 288 |  |  [Generalization error of spectral algorithms](https://openreview.net/forum?id=3SJE1WLB4M) |  | 0 |  | Maksim Velikanov, Maxim Panov, Dmitry Yarotsky |  |
| 289 |  |  [Debiased Collaborative Filtering with Kernel-Based Causal Balancing](https://openreview.net/forum?id=Ffjc8ApSbt) |  | 0 |  | Haoxuan Li, Chunyuan Zheng, Yanghao Xiao, Peng Wu, Zhi Geng, Xu Chen, Peng Cui |  |
| 290 |  |  [The Effective Horizon Explains Deep RL Performance in Stochastic Environments](https://openreview.net/forum?id=5ES5Hdlbxw) |  | 0 |  | Cassidy Laidlaw, Banghua Zhu, Stuart Russell, Anca D. Dragan |  |
| 291 |  |  [Selective Visual Representations Improve Convergence and Generalization for Embodied AI](https://openreview.net/forum?id=kC5nZDU5zf) |  | 0 |  | Ainaz Eftekhar, KuoHao Zeng, Jiafei Duan, Ali Farhadi, Aniruddha Kembhavi, Ranjay Krishna |  |
| 292 |  |  [Improving Generalization of Alignment with Human Preferences through Group Invariant Learning](https://openreview.net/forum?id=fwCoLe3TAX) |  | 0 |  | Rui Zheng, Wei Shen, Yuan Hua, Wenbin Lai, Shihan Dou, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Haoran Huang, Tao Gui, Qi Zhang, Xuanjing Huang |  |
| 293 |  |  [PINNACLE: PINN Adaptive ColLocation and Experimental points selection](https://openreview.net/forum?id=GzNaCp6Vcg) |  | 0 |  | Gregory Kang Ruey Lau, Apivich Hemachandra, SeeKiong Ng, Bryan Kian Hsiang Low |  |
| 294 |  |  [Learning to Relax: Setting Solver Parameters Across a Sequence of Linear System Instances](https://openreview.net/forum?id=5t57omGVMw) |  | 0 |  | Mikhail Khodak, Edmond Chow, MariaFlorina Balcan, Ameet Talwalkar |  |
| 295 |  |  [Rotation Has Two Sides: Evaluating Data Augmentation for Deep One-class Classification](https://openreview.net/forum?id=Ad81awoBVS) |  | 0 |  | Guodong Wang, Yunhong Wang, Xiuguo Bao, Di Huang |  |
| 296 |  |  [Realistic Evaluation of Semi-supervised Learning Algorithms in Open Environments](https://openreview.net/forum?id=RvUVMjfp8i) |  | 0 |  | LinHan Jia, LanZhe Guo, Zhi Zhou, Yufeng Li |  |
| 297 |  |  [Efficient Inverse Multiagent Learning](https://openreview.net/forum?id=JzvIWvC9MG) |  | 0 |  | Denizalp Goktas, Amy Greenwald, Sadie Zhao, Alec Koppel, Sumitra Ganesh |  |
| 298 |  |  [On the Role of Discrete Tokenization in Visual Representation Learning](https://openreview.net/forum?id=WNLAkjUm19) |  | 0 |  | Tianqi Du, Yifei Wang, Yisen Wang |  |
| 299 |  |  [The Consensus Game: Language Model Generation via Equilibrium Search](https://openreview.net/forum?id=n9xeGcI4Yg) |  | 0 |  | Athul Paul Jacob, Yikang Shen, Gabriele Farina, Jacob Andreas |  |
| 300 |  |  [AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents](https://openreview.net/forum?id=M6XWoEdmwf) |  | 0 |  | Jake Grigsby, Linxi Fan, Yuke Zhu |  |
| 301 |  |  [PILOT: An $\mathcal{O}(1/K)$-Convergent Approach for Policy Evaluation with Nonlinear Function Approximation](https://openreview.net/forum?id=OkHHJcMroY) |  | 0 |  | Zhuqing Liu, Xin Zhang, Jia Liu, Zhengyuan Zhu, Songtao Lu |  |
| 302 |  |  [Confronting Reward Model Overoptimization with Constrained RLHF](https://openreview.net/forum?id=gkfUvn0fLU) |  | 0 |  | Ted Moskovitz, Aaditya K. Singh, DJ Strouse, Tuomas Sandholm, Ruslan Salakhutdinov, Anca D. Dragan, Stephen Marcus McAleer |  |
| 303 |  |  [LiDAR: Sensing Linear Probing Performance in Joint Embedding SSL Architectures](https://openreview.net/forum?id=f3g5XpL9Kb) |  | 0 |  | Vimal Thilak, Chen Huang, Omid Saremi, Laurent Dinh, Hanlin Goh, Preetum Nakkiran, Joshua M. Susskind, Etai Littwin |  |
| 304 |  |  [Improved Efficiency Based on Learned Saccade and Continuous Scene Reconstruction From Foveated Visual Sampling](https://openreview.net/forum?id=lOwkOIUJtx) |  | 0 |  | Jiayang Liu, Yiming Bu, Daniel Tso, Qinru Qiu |  |
| 305 |  |  [Overthinking the Truth: Understanding how Language Models Process False Demonstrations](https://openreview.net/forum?id=Tigr1kMDZy) |  | 0 |  | Danny Halawi, JeanStanislas Denain, Jacob Steinhardt |  |
| 306 |  |  [MT-Ranker: Reference-free machine translation evaluation by inter-system ranking](https://openreview.net/forum?id=Rry1SeSOQL) |  | 0 |  | Ibraheem Muhammad Moosa, Rui Zhang, Wenpeng Yin |  |
| 307 |  |  [MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning](https://openreview.net/forum?id=jenyYQzue1) |  | 0 |  | Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, Greg Durrett |  |
| 308 |  |  [Harnessing Density Ratios for Online Reinforcement Learning](https://openreview.net/forum?id=THJEa8adBn) |  | 0 |  | Philip Amortila, Dylan J. Foster, Nan Jiang, Ayush Sekhari, Tengyang Xie |  |
| 309 |  |  [Predictive, scalable and interpretable knowledge tracing on structured domains](https://openreview.net/forum?id=NgaLU2fP5D) |  | 0 |  | Hanqi Zhou, Robert Bamler, Charley M. Wu, Álvaro TejeroCantero |  |
| 310 |  |  [From Bricks to Bridges: Product of Invariances to Enhance Latent Space Communication](https://openreview.net/forum?id=vngVydDWft) |  | 0 |  | Irene Cannistraci, Luca Moschella, Marco Fumero, Valentino Maiorca, Emanuele Rodolà |  |
| 311 |  |  [Proximal Policy Gradient Arborescence for Quality Diversity Reinforcement Learning](https://openreview.net/forum?id=TFKIfhvdmZ) |  | 0 |  | Sumeet Batra, Bryon Tjanaka, Matthew Christopher Fontaine, Aleksei Petrenko, Stefanos Nikolaidis, Gaurav S. Sukhatme |  |
| 312 |  |  [Memorization Capacity of Multi-Head Attention in Transformers](https://openreview.net/forum?id=MrR3rMxqqv) |  | 0 |  | Sadegh Mahdavi, Renjie Liao, Christos Thrampoulidis |  |
| 313 |  |  [Circuit Component Reuse Across Tasks in Transformer Language Models](https://openreview.net/forum?id=fpoAYV6Wsk) |  | 0 |  | Jack Merullo, Carsten Eickhoff, Ellie Pavlick |  |
| 314 |  |  [Likelihood Training of Cascaded Diffusion Models via Hierarchical Volume-preserving Maps](https://openreview.net/forum?id=sojpn00o8z) |  | 0 |  | Henry Li, Ronen Basri, Yuval Kluger |  |
| 315 |  |  [Empirical Analysis of Model Selection for Heterogeneous Causal Effect Estimation](https://openreview.net/forum?id=yuy6cGt3KL) |  | 0 |  | Divyat Mahajan, Ioannis Mitliagkas, Brady Neal, Vasilis Syrgkanis |  |
| 316 |  |  [Confidential-DPproof: Confidential Proof of Differentially Private Training](https://openreview.net/forum?id=PQY2v6VtGe) |  | 0 |  | Ali Shahin Shamsabadi, Gefei Tan, Tudor Cebere, Aurélien Bellet, Hamed Haddadi, Nicolas Papernot, Xiao Wang, Adrian Weller |  |
| 317 |  |  [In-Context Pretraining: Language Modeling Beyond Document Boundaries](https://openreview.net/forum?id=LXVswInHOo) |  | 0 |  | Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Xi Victoria Lin, Noah A. Smith, Luke Zettlemoyer, Wentau Yih, Mike Lewis |  |
| 318 |  |  [What's In My Big Data?](https://openreview.net/forum?id=RvfPnOkPV4) |  | 0 |  | Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Evan Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hannaneh Hajishirzi, Noah A. Smith, Jesse Dodge |  |
| 319 |  |  [On Diffusion Modeling for Anomaly Detection](https://openreview.net/forum?id=lR3rk7ysXz) |  | 0 |  | Victor Livernoche, Vineet Jain, Yashar Hezaveh, Siamak Ravanbakhsh |  |
| 320 |  |  [Social Reward: Evaluating and Enhancing Generative AI through Million-User Feedback from an Online Creative Community](https://openreview.net/forum?id=tjn2YZSHUv) |  | 0 |  | Arman Isajanyan, Artur Shatveryan, David Kocharian, Zhangyang Wang, Humphrey Shi |  |
| 321 |  |  [Role of Locality and Weight Sharing in Image-Based Tasks: A Sample Complexity Separation between CNNs, LCNs, and FCNs](https://openreview.net/forum?id=AfnsTnYphT) |  | 0 |  | Aakash Lahoti, Stefani Karp, Ezra Winston, Aarti Singh, Yuanzhi Li |  |
| 322 |  |  [Lion Secretly Solves a Constrained Optimization: As Lyapunov Predicts](https://openreview.net/forum?id=e4xS9ZarDr) |  | 0 |  | Lizhang Chen, Bo Liu, Kaizhao Liang, Qiang Liu |  |
| 323 |  |  [Distributionally Robust Optimization with Bias and Variance Reduction](https://openreview.net/forum?id=TTrzgEZt9s) |  | 0 |  | Ronak Mehta, Vincent Roulet, Krishna Pillutla, Zaïd Harchaoui |  |
| 324 |  |  [A Benchmark for Learning to Translate a New Language from One Grammar Book](https://openreview.net/forum?id=tbVWug9f2h) |  | 0 |  | Garrett Tanzer, Mirac Suzgun, Eline Visser, Dan Jurafsky, Luke MelasKyriazi |  |
| 325 |  |  [Improving Offline RL by Blending Heuristics](https://openreview.net/forum?id=MCl0TLboP1) |  | 0 |  | Sinong Geng, Aldo Pacchiano, Andrey Kolobov, ChingAn Cheng |  |
| 326 |  |  [Decision ConvFormer: Local Filtering in MetaFormer is Sufficient for Decision Making](https://openreview.net/forum?id=af2c8EaKl8) |  | 0 |  | Jeonghye Kim, Suyoung Lee, Woojun Kim, Youngchul Sung |  |
| 327 |  |  [How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?](https://openreview.net/forum?id=vSh5ePa0ph) |  | 0 |  | Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, Peter L. Bartlett |  |
| 328 |  |  [Tool-Augmented Reward Modeling](https://openreview.net/forum?id=d94x0gWTUX) |  | 0 |  | Lei Li, Yekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Ningyu Zhang, Hua Wu |  |
| 329 |  |  [Reward-Consistent Dynamics Models are Strongly Generalizable for Offline Reinforcement Learning](https://openreview.net/forum?id=GSBHKiw19c) |  | 0 |  | FanMing Luo, Tian Xu, Xingchen Cao, Yang Yu |  |
| 330 |  |  [Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback](https://openreview.net/forum?id=6yv8UHVJn4) |  | 0 |  | Haolin Liu, ChenYu Wei, Julian Zimmert |  |
| 331 |  |  [Dual RL: Unification and New Methods for Reinforcement and Imitation Learning](https://openreview.net/forum?id=xt9Bu66rqv) |  | 0 |  | Harshit Sikchi, Qinqing Zheng, Amy Zhang, Scott Niekum |  |
| 332 |  |  [Out-Of-Domain Unlabeled Data Improves Generalization](https://openreview.net/forum?id=Bo6GpQ3B9a) |  | 0 |  | Seyed Amir Hossein Saberi, Amir Najafi, Alireza Heidari, Mohammad Hosein Movasaghinia, Abolfazl S. Motahari, Babak H. Khalaj |  |
| 333 |  |  [Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation](https://openreview.net/forum?id=r42tSSCHPh) |  | 0 |  | Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, Danqi Chen |  |
| 334 |  |  [PolyGCL: GRAPH CONTRASTIVE LEARNING via Learnable Spectral Polynomial Filters](https://openreview.net/forum?id=y21ZO6M86t) |  | 0 |  | Jingyu Chen, Runlin Lei, Zhewei Wei |  |
| 335 |  |  [Solving Homogeneous and Heterogeneous Cooperative Tasks with Greedy Sequential Execution](https://openreview.net/forum?id=hB2hXtxIPH) |  | 0 |  | Shanqi Liu, Dong Xing, Pengjie Gu, Xinrun Wang, Bo An, Yong Liu |  |
| 336 |  |  [Stabilizing Contrastive RL: Techniques for Robotic Goal Reaching from Offline Data](https://openreview.net/forum?id=Xkf2EBj4w3) |  | 0 |  | Chongyi Zheng, Benjamin Eysenbach, Homer Rich Walke, Patrick Yin, Kuan Fang, Ruslan Salakhutdinov, Sergey Levine |  |
| 337 |  |  [Multi-View Causal Representation Learning with Partial Observability](https://openreview.net/forum?id=OGtnhKQJms) |  | 0 |  | Dingling Yao, Danru Xu, Sébastien Lachapelle, Sara Magliacane, Perouz Taslakian, Georg Martius, Julius von Kügelgen, Francesco Locatello |  |
| 338 |  |  [CABINET: Content Relevance-based Noise Reduction for Table Question Answering](https://openreview.net/forum?id=SQrHpTllXa) |  | 0 |  | Sohan Patnaik, Heril Changwal, Milan Aggarwal, Sumit Bhatia, Yaman Kumar, Balaji Krishnamurthy |  |
| 339 |  |  [Safe RLHF: Safe Reinforcement Learning from Human Feedback](https://openreview.net/forum?id=TyFrPOKYXw) |  | 0 |  | Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, Yaodong Yang |  |
| 340 |  |  [Benchmarking Algorithms for Federated Domain Generalization](https://openreview.net/forum?id=wprSv7ichW) |  | 0 |  | Ruqi Bai, Saurabh Bagchi, David I. Inouye |  |
| 341 |  |  [CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity](https://openreview.net/forum?id=PczQtTsTIX) |  | 0 |  | Aditya Bhatt, Daniel Palenicek, Boris Belousov, Max Argus, Artemij Amiranashvili, Thomas Brox, Jan Peters |  |
| 342 |  |  [Blending Imitation and Reinforcement Learning for Robust Policy Improvement](https://openreview.net/forum?id=eJ0dzPJq1F) |  | 0 |  | Xuefeng Liu, Takuma Yoneda, Rick Stevens, Matthew R. Walter, Yuxin Chen |  |
| 343 |  |  [H-GAP: Humanoid Control with a Generalist Planner](https://openreview.net/forum?id=LYG6tBlEX0) |  | 0 |  | Zhengyao Jiang, Yingchen Xu, Nolan Wagener, Yicheng Luo, Michael Janner, Edward Grefenstette, Tim Rocktäschel, Yuandong Tian |  |
| 344 |  |  [Unlocking the Power of Representations in Long-term Novelty-based Exploration](https://openreview.net/forum?id=OwtMhMSybu) |  | 0 |  | Alaa Saade, Steven Kapturowski, Daniele Calandriello, Charles Blundell, Pablo Sprechmann, Leopoldo Sarra, Oliver Groth, Michal Valko, Bilal Piot |  |
| 345 |  |  [Accelerating Data Generation for Neural Operators via Krylov Subspace Recycling](https://openreview.net/forum?id=UpgRVWexaD) |  | 0 |  | Hong Wang, Zhongkai Hao, Jie Wang, Zijie Geng, Zhen Wang, Bin Li, Feng Wu |  |
| 346 |  |  [Deep Orthogonal Hypersphere Compression for Anomaly Detection](https://openreview.net/forum?id=cJs4oE4m9Q) |  | 0 |  | Yunhe Zhang, Yan Sun, Jinyu Cai, Jicong Fan |  |
| 347 |  |  [On the Role of General Function Approximation in Offline Reinforcement Learning](https://openreview.net/forum?id=JSS9rKHySk) |  | 0 |  | Chenjie Mao, Qiaosheng Zhang, Zhen Wang, Xuelong Li |  |
| 348 |  |  [Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Maps](https://openreview.net/forum?id=mYWsyTuiRp) |  | 0 |  | Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, Kentaro Inui |  |
| 349 |  |  [Asymptotically Free Sketched Ridge Ensembles: Risks, Cross-Validation, and Tuning](https://openreview.net/forum?id=i9Vs5NGDpk) |  | 0 |  | Pratik Patil, Daniel LeJeune |  |
| 350 |  |  [Nemesis: Normalizing the Soft-prompt Vectors of Vision-Language Models](https://openreview.net/forum?id=zmJDzPh1Dm) |  | 0 |  | Shuai Fu, Xiequn Wang, Qiushi Huang, Yu Zhang |  |
| 351 |  |  [Towards Understanding Factual Knowledge of Large Language Models](https://openreview.net/forum?id=9OevMUdods) |  | 0 |  | Xuming Hu, Junzhe Chen, Xiaochuan Li, Yufei Guo, Lijie Wen, Philip S. Yu, Zhijiang Guo |  |
| 352 |  |  [CAS: A Probability-Based Approach for Universal Condition Alignment Score](https://openreview.net/forum?id=E78OaH2s3f) |  | 0 |  | Chunsan Hong, Byunghee Cha, TaeHyun Oh |  |
| 353 |  |  [Demystifying CLIP Data](https://openreview.net/forum?id=5BCFlnfE1g) |  | 0 |  | Hu Xu, Saining Xie, Xiaoqing Ellen Tan, PoYao Huang, Russell Howes, Vasu Sharma, ShangWen Li, Gargi Ghosh, Luke Zettlemoyer, Christoph Feichtenhofer |  |
| 354 |  |  [Adversarial AutoMixup](https://openreview.net/forum?id=o8tjamaJ80) |  | 0 |  | Huafeng Qin, Xin Jin, Yun Jiang, Mounîm A. ElYacoubi, Xinbo Gao |  |
| 355 |  |  [Spatially-Aware Transformers for Embodied Agents](https://openreview.net/forum?id=Ts95eXsPBc) |  | 0 |  | Junmo Cho, Jaesik Yoon, Sungjin Ahn |  |
| 356 |  |  [Grounding Language Plans in Demonstrations Through Counterfactual Perturbations](https://openreview.net/forum?id=qoHeuRAcSl) |  | 0 |  | Yanwei Wang, TsunHsuan Wang, Jiayuan Mao, Michael Hagenow, Julie Shah |  |
| 357 |  |  [Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies](https://openreview.net/forum?id=DFTHW0MyiW) |  | 0 |  | Xiangyu Liu, Chenghao Deng, Yanchao Sun, Yongyuan Liang, Furong Huang |  |
| 358 |  |  [Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy](https://openreview.net/forum?id=eFWG9Cy3WK) |  | 0 |  | Pingzhi Li, Zhenyu Zhang, Prateek Yadav, YiLin Sung, Yu Cheng, Mohit Bansal, Tianlong Chen |  |
| 359 |  |  [On Bias-Variance Alignment in Deep Models](https://openreview.net/forum?id=i2Phucne30) |  | 0 |  | Lin Chen, Michal Lukasik, Wittawat Jitkrittum, Chong You, Sanjiv Kumar |  |
| 360 |  |  [SEGNO: Generalizing Equivariant Graph Neural Networks with Physical Inductive Biases](https://openreview.net/forum?id=3oTPsORaDH) |  | 0 |  | Yang Liu, Jiashun Cheng, Haihong Zhao, Tingyang Xu, Peilin Zhao, Fugee Tsung, Jia Li, Yu Rong |  |
| 361 |  |  [Spectrally Transformed Kernel Regression](https://openreview.net/forum?id=OeQE9zsztS) |  | 0 |  | Runtian Zhai, Rattana Pukdee, Roger Jin, MariaFlorina Balcan, Pradeep Kumar Ravikumar |  |
| 362 |  |  [Online GNN Evaluation Under Test-time Graph Distribution Shifts](https://openreview.net/forum?id=KbetDM33YG) |  | 0 |  | Xin Zheng, Dongjin Song, Qingsong Wen, Bo Du, Shirui Pan |  |
| 363 |  |  [Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks](https://openreview.net/forum?id=TjhUtloBZU) |  | 0 |  | Hao Chen, Jindong Wang, Ankit Shah, Ran Tao, Hongxin Wei, Xing Xie, Masashi Sugiyama, Bhiksha Raj |  |
| 364 |  |  [WildChat: 1M ChatGPT Interaction Logs in the Wild](https://openreview.net/forum?id=Bl8u7ZRlbM) |  | 0 |  | Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, Yuntian Deng |  |
| 365 |  |  [Learning Hierarchical Image Segmentation For Recognition and By Recognition](https://openreview.net/forum?id=IRcv4yFX6z) |  | 0 |  | TsungWei Ke, Sangwoo Mo, Stella X. Yu |  |
| 366 |  |  [Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models](https://openreview.net/forum?id=plmBsXHxgR) |  | 0 |  | Erfan Shayegani, Yue Dong, Nael B. AbuGhazaleh |  |
| 367 |  |  [DreamFlow: High-quality text-to-3D generation by Approximating Probability Flow](https://openreview.net/forum?id=GURqUuTebY) |  | 0 |  | Kyungmin Lee, Kihyuk Sohn, Jinwoo Shin |  |
| 368 |  |  [Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns](https://openreview.net/forum?id=XVhm3X8Fum) |  | 0 |  | Brian DuSell, David Chiang |  |
| 369 |  |  [SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents](https://openreview.net/forum?id=mM7VurbA4r) |  | 0 |  | Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, LouisPhilippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap |  |
| 370 |  |  [Privileged Sensing Scaffolds Reinforcement Learning](https://openreview.net/forum?id=EpVe8jAjdx) |  | 0 |  | Edward S. Hu, James Springer, Oleh Rybkin, Dinesh Jayaraman |  |
| 371 |  |  [Learning to Act without Actions](https://openreview.net/forum?id=rvUq3cxpDF) |  | 0 |  | Dominik Schmidt, Minqi Jiang |  |
| 372 |  |  [Error Norm Truncation: Robust Training in the Presence of Data Noise for Text Generation Models](https://openreview.net/forum?id=zMvMwNvs4R) |  | 0 |  | Tianjian Li, Haoran Xu, Philipp Koehn, Daniel Khashabi, Kenton Murray |  |
| 373 |  |  [Massively Scalable Inverse Reinforcement Learning in Google Maps](https://openreview.net/forum?id=z3L59iGALM) |  | 0 |  | Matt Barnes, Matthew Abueg, Oliver F. Lange, Matt Deeds, Jason Trader, Denali Molitor, Markus Wulfmeier, Shawn O'Banion |  |
| 374 |  |  [Thin-Shell Object Manipulations With Differentiable Physics Simulations](https://openreview.net/forum?id=KsUh8MMFKQ) |  | 0 |  | Yian Wang, Juntian Zheng, Zhehuan Chen, Zhou Xian, Gu Zhang, Chao Liu, Chuang Gan |  |
| 375 |  |  [Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks](https://openreview.net/forum?id=7erlRDoaV8) |  | 0 |  | Vaidehi Patil, Peter Hase, Mohit Bansal |  |
| 376 |  |  [Learning to Reject Meets Long-tail Learning](https://openreview.net/forum?id=ta26LtNq2r) |  | 0 |  | Harikrishna Narasimhan, Aditya Krishna Menon, Wittawat Jitkrittum, Neha Gupta, Sanjiv Kumar |  |
| 377 |  |  [On the Foundations of Shortcut Learning](https://openreview.net/forum?id=Tj3xLVuE9f) |  | 0 |  | Katherine L. Hermann, Hossein Mobahi, Thomas Fel, Michael Curtis Mozer |  |
| 378 |  |  [Synaptic Weight Distributions Depend on the Geometry of Plasticity](https://openreview.net/forum?id=x5txICnnjC) |  | 0 |  | Roman Pogodin, Jonathan Cornford, Arna Ghosh, Gauthier Gidel, Guillaume Lajoie, Blake Aaron Richards |  |
| 379 |  |  [Graph Metanetworks for Processing Diverse Neural Architectures](https://openreview.net/forum?id=ijK5hyxs0n) |  | 0 |  | Derek Lim, Haggai Maron, Marc T. Law, Jonathan Lorraine, James Lucas |  |
| 380 |  |  [Dropout Enhanced Bilevel Training](https://openreview.net/forum?id=06lrITXVAx) |  | 0 |  | Peiran Yu, Junyi Li, Heng Huang |  |
| 381 |  |  [Privacy Amplification for Matrix Mechanisms](https://openreview.net/forum?id=xUzWmFdglP) |  | 0 |  | Christopher A. ChoquetteChoo, Arun Ganesh, Thomas Steinke, Abhradeep Guha Thakurta |  |
| 382 |  |  [Bandits Meet Mechanism Design to Combat Clickbait in Online Recommendation](https://openreview.net/forum?id=lsxeNvYqCj) |  | 0 |  | Thomas Kleine Buening, Aadirupa Saha, Christos Dimitrakakis, Haifeng Xu |  |
| 383 |  |  [Towards Principled Representation Learning from Videos for Reinforcement Learning](https://openreview.net/forum?id=3mnWvUZIXt) |  | 0 |  | Dipendra Misra, Akanksha Saran, Tengyang Xie, Alex Lamb, John Langford |  |
| 384 |  |  [Optimal Sample Complexity of Contrastive Learning](https://openreview.net/forum?id=NU9AYHJvYe) |  | 0 |  | Noga Alon, Dmitrii Avdiukhin, Dor Elboim, Orr Fischer, Grigory Yaroslavtsev |  |
| 385 |  |  [Post-hoc bias scoring is optimal for fair classification](https://openreview.net/forum?id=FM5xfcaR2Y) |  | 0 |  | Wenlong Chen, Yegor Klochkov, Yang Liu |  |
| 386 |  |  [Sharpness-Aware Data Poisoning Attack](https://openreview.net/forum?id=bxITGFPVWh) |  | 0 |  | Pengfei He, Han Xu, Jie Ren, Yingqian Cui, Shenglai Zeng, Hui Liu, Charu C. Aggarwal, Jiliang Tang |  |
| 387 |  |  [Pre-training with Random Orthogonal Projection Image Modeling](https://openreview.net/forum?id=z4Hcegjzph) |  | 0 |  | Maryam Haghighat, Peyman Moghadam, Shaheer Mohamed, Piotr Koniusz |  |
| 388 |  |  [Lagrangian Flow Networks for Conservation Laws](https://openreview.net/forum?id=Nshk5YpdWE) |  | 0 |  | Fabricio Arend Torres, Marcello Massimo Negri, Marco Inversi, Jonathan Aellen, Volker Roth |  |
| 389 |  |  [Linearity of Relation Decoding in Transformer Language Models](https://openreview.net/forum?id=w7LU2s14kE) |  | 0 |  | Evan Hernandez, Arnab Sen Sharma, Tal Haklay, Kevin Meng, Martin Wattenberg, Jacob Andreas, Yonatan Belinkov, David Bau |  |
| 390 |  |  [Subtractive Mixture Models via Squaring: Representation and Learning](https://openreview.net/forum?id=xIHi5nxu9P) |  | 0 |  | Lorenzo Loconte, Aleksanteri M. Sladek, Stefan Mengel, Martin Trapp, Arno Solin, Nicolas Gillis, Antonio Vergari |  |
| 391 |  |  [On the Provable Advantage of Unsupervised Pretraining](https://openreview.net/forum?id=rmXXKxQpOR) |  | 0 |  | Jiawei Ge, Shange Tang, Jianqing Fan, Chi Jin |  |
| 392 |  |  [TorchRL: A data-driven decision-making library for PyTorch](https://openreview.net/forum?id=QxItoEAVMb) |  | 0 |  | Albert Bou, Matteo Bettini, Sebastian Dittert, Vikash Kumar, Shagun Sodhani, Xiaomeng Yang, Gianni De Fabritiis, Vincent Moens |  |
| 393 |  |  [Towards Robust Offline Reinforcement Learning under Diverse Data Corruption](https://openreview.net/forum?id=5hAMmCU0bK) |  | 0 | Offline reinforcement learning (RL) presents a promising approach for learning reinforced policies from offline datasets without the need for costly or unsafe interactions with the environment. However, datasets collected by humans in real-world environments are often noisy and may even be maliciously corrupted, which can significantly degrade the performance of offline RL. In this work, we first investigate the performance of current offline RL algorithms under comprehensive data corruption, including states, actions, rewards, and dynamics. Our extensive experiments reveal that implicit Q-learning (IQL) demonstrates remarkable resilience to data corruption among various offline RL algorithms. Furthermore, we conduct both empirical and theoretical analyses to understand IQL's robust performance, identifying its supervised policy learning scheme as the key factor. Despite its relative robustness, IQL still suffers from heavy-tail targets of Q functions under dynamics corruption. To tackle this challenge, we draw inspiration from robust statistics to employ the Huber loss to handle the heavy-tailedness and utilize quantile estimators to balance penalization for corrupted data and learning stability. By incorporating these simple yet effective modifications into IQL, we propose a more robust offline RL approach named Robust IQL (RIQL). Extensive experiments demonstrate that RIQL exhibits highly robust performance when subjected to diverse data corruption scenarios. | Rui Yang, Han Zhong, Jiawei Xu, Amy Zhang, Chongjie Zhang, Lei Han, Tong Zhang |  |
| 394 |  |  [Variational Bayesian Last Layers](https://openreview.net/forum?id=Sx7BIiPzys) |  | 0 |  | James Harrison, John Willes, Jasper Snoek |  |
| 395 |  |  [EQA-MX: Embodied Question Answering using Multimodal Expression](https://openreview.net/forum?id=7gUrYE50Rb) |  | 0 | Humans predominantly use verbal utterances and nonverbal gestures (e.g., eye gaze and pointing gestures) in their natural interactions. For instance, pointing gestures and verbal information is often required to comprehend questions such as "what object is that?" Thus, this question-answering (QA) task involves complex reasoning of multimodal expressions (verbal utterances and nonverbal gestures). However, prior works have explored QA tasks in non-embodied settings, where questions solely contain verbal utterances from a single verbal and visual perspective. In this paper, we have introduced 8 novel embodied question answering (EQA) tasks to develop learning models to comprehend embodied questions with multimodal expressions. We have developed a novel large-scale dataset, EQA-MX, with over 8 million diverse embodied QA data samples involving multimodal expressions from multiple visual and verbal perspectives. To learn salient multimodal representations from discrete verbal embeddings and continuous wrapping of multiview visual representations, we propose a vector-quantization (VQ) based multimodal representation learning model, VQ-Fusion, for the EQA tasks. Our extensive experimental results suggest that VQ-Fusion can improve the performance of existing state-of-the-art visual-language models up to 13% across EQA tasks. | Md Mofijul Islam, Alexi Gladstone, Riashat Islam, Tariq Iqbal |  |
| 396 |  |  [Retrieval-based Disentangled Representation Learning with Natural Language Supervision](https://openreview.net/forum?id=ZlQRiFmq7Y) |  | 0 | Disentangled representation learning remains challenging as the underlying factors of variation in the data do not naturally exist. The inherent complexity of real-world data makes it unfeasible to exhaustively enumerate and encapsulate all its variations within a finite set of factors. However, it is worth noting that most real-world data have linguistic equivalents, typically in the form of textual descriptions. These linguistic counterparts can represent the data and effortlessly decomposed into distinct tokens. In light of this, we present Vocabulary Disentangled Retrieval (VDR), a retrieval-based framework that harnesses natural language as proxies of the underlying data variation to drive disentangled representation learning. Our approach employ a bi-encoder model to represent both data and natural language in a vocabulary space, enabling the model to distinguish dimensions that capture intrinsic characteristics within data through its natural language counterpart, thus facilitating disentanglement. We extensively assess the performance of VDR across 15 retrieval benchmark datasets, covering text-to-text and cross-modal retrieval scenarios, as well as human evaluation. Our experimental results compellingly demonstrate the superiority of VDR over previous bi-encoder retrievers with comparable model size and training costs, achieving an impressive 8.7% improvement in NDCG@10 on the BEIR benchmark, a 5.3\% increase on MS COCO, and a 6.0% increase on Flickr30k in terms of mean recall in the zero-shot setting. Moreover, The results from human evaluation indicate that interpretability of our method is on par with SOTA captioning models. | Jiawei Zhou, Xiaoguang Li, Lifeng Shang, Xin Jiang, Qun Liu, Lei Chen |  |
| 397 |  |  [On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods](https://openreview.net/forum?id=Kn7tWhuetn) |  | 0 | Neural algorithmic reasoning is an emerging research direction that endows neural networks with the ability to mimic algorithmic executions step-by-step. A common paradigm in existing designs involves the use of historical embeddings in predicting the results of future execution steps. Our observation in this work is that such historical dependence intrinsically contradicts the Markov nature of algorithmic reasoning tasks. Based on this motivation, we present our ForgetNet, which does not use historical embeddings and thus is consistent with the Markov nature of the tasks. To address challenges in training ForgetNet at early stages, we further introduce G-ForgetNet, which uses a gating mechanism to allow for the selective integration of historical embeddings. Such an enhanced capability provides valuable computational pathways during the model's early training phase. Our extensive experiments, based on the CLRS-30 algorithmic reasoning benchmark, demonstrate that both ForgetNet and G-ForgetNet achieve better generalization capability than existing methods. Furthermore, we investigate the behavior of the gating mechanism, highlighting its degree of alignment with our intuitions and its effectiveness for robust performance. Our code is publicly available at https://github.com/divelab/ForgetNet. | Montgomery Bohde, Meng Liu, Alexandra Saxton, Shuiwang Ji |  |
| 398 |  |  [TRAM: Bridging Trust Regions and Sharpness Aware Minimization](https://openreview.net/forum?id=kxebDHZ7b7) |  | 0 | Sharpness-aware minimization (SAM) reports improving domain generalization by reducing the loss surface curvature in the parameter space. However, generalization during _fine-tuning_ is often more dependent on the transferability of _representations_ in the function space. Trust-region methods (TR) target this goal by regularizing representation curvature to reduce catastrophic forgetting of pre-trained task-agnostic information while adopting task-specific skills. We consider unifying these strategies for low curvature in both parameter space and function space to improve out-of-domain (OOD) generalization. We propose \*\*Trust Region Aware Minimization\*\* (TRAM), a SAM algorithm fine-tuning for low parameter sharpness and smooth, informative representations preserving pre-trained structure. TRAM uses a trust region bound to inform the SAM adversarial neighborhood, introducing an awareness of function curvature within optimization for flatter minima. We empirically validate TRAM in vision (cross-dataset adaptation) and text (OOD language modeling, zero-shot cross-lingual transfer) tasks where robust domain transfer and representation generality are critical. TRAM outperforms SAM- and TR-based optimization across all tasks, notably surpassing competing methods for hard transfer between _anticorrelated_ domains. TRAM establishes a novel standard in fine-tuning for domain-generalizable models with minimal additional computation over previous sharpness-aware methods. | Tom Sherborne, Naomi Saphra, Pradeep Dasigi, Hao Peng |  |
| 399 |  |  [CAMIL: Context-Aware Multiple Instance Learning for Cancer Detection and Subtyping in Whole Slide Images](https://openreview.net/forum?id=rzBskAEmoc) |  | 0 | The visual examination of tissue biopsy sections is fundamental for cancer diagnosis, with pathologists analyzing sections at multiple magnifications to discern tumor cells and their subtypes. However, existing attention-based multiple instance learning (MIL) models used for analyzing Whole Slide Images (WSIs) in cancer diagnostics often overlook the contextual information of tumor and neighboring tiles, leading to misclassifications. To address this, we propose the Context-Aware Multiple Instance Learning (CAMIL) architecture. CAMIL incorporates neighbor-constrained attention to consider dependencies among tiles within a WSI and integrates contextual constraints as prior knowledge into the MIL model. We evaluated CAMIL on subtyping non-small cell lung cancer (TCGA-NSCLC) and detecting lymph node (CAMELYON16 and CAMELYON17) metastasis, achieving test AUCs of 97.5\%, 95.9\%, and 88.1\%, respectively, outperforming other state-of-the-art methods. Additionally, CAMIL enhances model interpretability by identifying regions of high diagnostic value. Our code is available at https://github.com/olgarithmics/ICLR_CAMIL. | Olga Fourkioti, Matt De Vries, Chris Bakal |  |
| 400 |  |  [DyST: Towards Dynamic Neural Scene Representations on Real-World Videos](https://openreview.net/forum?id=MnMWa94t12) |  | 0 | Visual understanding of the world goes beyond the semantics and flat structure of individual images. In this work, we aim to capture both the 3D structure and dynamics of real-world scenes from monocular real-world videos. Our Dynamic Scene Transformer (DyST) model leverages recent work in neural scene representation to learn a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose. This separation is achieved through a novel co-training scheme on monocular videos and our new synthetic dataset DySO. DyST learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene. | Maximilian Seitzer, Sjoerd van Steenkiste, Thomas Kipf, Klaus Greff, Mehdi S. M. Sajjadi |  |
| 401 |  |  [Bilevel Optimization under Unbounded Smoothness: A New Algorithm and Convergence Analysis](https://openreview.net/forum?id=LqRGsGWOTX) |  | 0 | Bilevel optimization is an important formulation for many machine learning problems, such as meta-learning and hyperparameter optimization. Current bilevel optimization algorithms assume that the gradient of the upper-level function is Lipschitz (i.e., the upper-level function has a bounded smoothness parameter). However, recent studies reveal that certain neural networks such as recurrent neural networks (RNNs) and long-short-term memory networks (LSTMs) exhibit potential unbounded smoothness, rendering conventional bilevel optimization algorithms unsuitable for these neural networks. In this paper, we design a new bilevel optimization algorithm, namely BO-REP, to address this challenge. This algorithm updates the upper-level variable using normalized momentum and incorporates two novel techniques for updating the lower-level variable: \textit{initialization refinement} and \textit{periodic updates}. Specifically, once the upper-level variable is initialized, a subroutine is invoked to obtain a refined estimate of the corresponding optimal lower-level variable, and the lower-level variable is updated only after every specific period instead of each iteration. When the upper-level problem is nonconvex and unbounded smooth, and the lower-level problem is strongly convex, we prove that our algorithm requires $\widetilde{O}(1/\epsilon^4)$ \footnote{Here $\widetilde{O}(\cdot)$ compresses logarithmic factors of $1/\epsilon$ and $1/\delta$, where $\delta\in(0,1)$ denotes the failure probability.} iterations to find an $\epsilon$-stationary point in the stochastic setting, where each iteration involves calling a stochastic gradient or Hessian-vector product oracle. Notably, this result matches the state-of-the-art complexity results under the bounded smoothness setting and without mean-squared smoothness of the stochastic gradient, up to logarithmic factors. Our proof relies on novel technical lemmas for the periodically updated lower-level variable, which are of independent interest. Our experiments on hyper-representation learning, hyperparameter optimization, and data hyper-cleaning for text classification tasks demonstrate the effectiveness of our proposed algorithm. The code is available at [https://github.com/MingruiLiu-ML-Lab/Bilevel-Optimization-under-Unbounded-Smoothness](https://github.com/MingruiLiu-ML-Lab/Bilevel-Optimization-under-Unbounded-Smoothness). | Jie Hao, Xiaochuan Gong, Mingrui Liu |  |
| 402 |  |  [Bounds on Representation-Induced Confounding Bias for Treatment Effect Estimation](https://openreview.net/forum?id=d3xKPQVjSc) |  | 0 | State-of-the-art methods for conditional average treatment effect (CATE) estimation make widespread use of representation learning. Here, the idea is to reduce the variance of the low-sample CATE estimation by a (potentially constrained) low-dimensional representation. However, low-dimensional representations can lose information about the observed confounders and thus lead to bias, because of which the validity of representation learning for CATE estimation is typically violated. In this paper, we propose a new, representation-agnostic refutation framework for estimating bounds on the representation-induced confounding bias that comes from dimensionality reduction (or other constraints on the representations) in CATE estimation. First, we establish theoretically under which conditions CATE is non-identifiable given low-dimensional (constrained) representations. Second, as our remedy, we propose a neural refutation framework which performs partial identification of CATE or, equivalently, aims at estimating lower and upper bounds of the representation-induced confounding bias. We demonstrate the effectiveness of our bounds in a series of experiments. In sum, our refutation framework is of direct relevance in practice where the validity of CATE estimation is of importance. | Valentyn Melnychuk, Dennis Frauen, Stefan Feuerriegel |  |
| 403 |  |  [DSPy: Compiling Declarative Language Model Calls into State-of-the-Art Pipelines](https://openreview.net/forum?id=sY5N0zY5Od) |  | 0 | The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded “prompt templates”, i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, or imperative computational graphs where LMs are invoked through declarative modules. DSPy modules are parameterized, meaning they can learn how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric, by creating and collecting demonstrations. We conduct two case studies, showing that succinct DSPy programs can express and optimize pipelines that reason about math word problems, tackle multi-hop retrieval, answer complex questions, and control agent loops. Within minutes of compiling, DSPy can automatically produce pipelines that outperform out-of-the-box few-shot prompting as well as expert-created demonstrations for GPT-3.5 and Llama2-13b-chat. On top of that, DSPy programs compiled for relatively small LMs like 770M parameter T5 and Llama2-13b-chat are competitive with many approaches that rely on large and proprietary LMs like GPT-3.5 and on expert-written prompt chains. DSPy is available at https://github.com/stanfordnlp/dspy | Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, Christopher Potts |  |
| 404 |  |  [Impact of Computation in Integral Reinforcement Learning for Continuous-Time Control](https://openreview.net/forum?id=xJEd8PkdNz) |  | 0 | Integral reinforcement learning (IntRL) demands the precise computation of the utility function's integral at its policy evaluation (PEV) stage. This is achieved through quadrature rules, which are weighted sums of utility functions evaluated from state samples obtained in discrete time. Our research reveals a critical yet underexplored phenomenon: the choice of the computational method -- in this case, the quadrature rule -- can significantly impact control performance. This impact is traced back to the fact that computational errors introduced in the PEV stage can affect the policy iteration's convergence behavior, which in turn affects the learned controller. To elucidate how computation impacts control, we draw a parallel between IntRL's policy iteration and Newton's method applied to the Hamilton-Jacobi-Bellman equation. In this light, computational error in PEV manifests as an extra error term in each iteration of Newton's method, with its upper bound proportional to the computational error. Further, we demonstrate that when the utility function resides in a reproducing kernel Hilbert space (RKHS), the optimal quadrature is achievable by employing Bayesian quadrature with the RKHS-inducing kernel function. We prove that the local convergence rates for IntRL using the trapezoidal rule and Bayesian quadrature with a Matérn kernel to be $O(N^{-2})$ and $O(N^{-b})$, where $N$ is the number of evenly-spaced samples and $b$ is the Matérn kernel's smoothness parameter. These theoretical findings are finally validated by two canonical control tasks. | Wenhan Cao, Wei Pan |  |
| 405 |  |  [Masks, Signs, And Learning Rate Rewinding](https://openreview.net/forum?id=qODvxQ8TXW) |  | 0 | Learning Rate Rewinding (LRR) has been established as a strong variant of Iterative Magnitude Pruning (IMP) to find lottery tickets in deep overparameterized neural networks. While both iterative pruning schemes couple structure and parameter learning, understanding how LRR excels in both aspects can bring us closer to the design of more flexible deep learning algorithms that can optimize diverse sets of sparse architectures. To this end, we conduct experiments that disentangle the effect of mask learning and parameter optimization and how both benefit from overparameterization. The ability of LRR to flip parameter signs early and stay robust to sign perturbations seems to make it not only more effective in mask identification but also in optimizing diverse sets of masks, including random ones. In support of this hypothesis, we prove in a simplified single hidden neuron setting that LRR succeeds in more cases than IMP, as it can escape initially problematic sign configurations. | Advait Harshal Gadhikar, Rebekka Burkholz |  |
| 406 |  |  [Gradual Domain Adaptation via Gradient Flow](https://openreview.net/forum?id=iTTZFKrlGV) |  | 0 | Domain shift degrades classification models on new data distributions. Conventional unsupervised domain adaptation (UDA) aims to learn features that bridge labeled source and unlabeled target domains. In contrast to feature learning, gradual domain adaptation (GDA) leverages extra continuous intermediate domains with pseudo-labels to boost the source classifier. However, real intermediate domains are sometimes unavailable or ineffective. In this paper, we propose $\textbf{G}$radual Domain Adaptation via $\textbf{G}$radient $\textbf{F}$low (GGF) to generate intermediate domains with preserving labels, thereby enabling us a fine-tuning method for GDA. We employ the Wasserstein gradient flow in Kullback–Leibler divergence to transport samples from the source to the target domain. To simulate the dynamics, we utilize the Langevin algorithm. Since the Langevin algorithm disregards label information and introduces diffusion noise, we introduce classifier-based and sample-based potentials to avoid label switching and dramatic deviations in the sampling process. For the proposed GGF model, we analyze its generalization bound. Experiments on several benchmark datasets demonstrate the superiority of the proposed GGF method compared to state-of-the-art baselines. | Zhan Zhuang, Yu Zhang, Ying Wei |  |
| 407 |  |  [Maximum Entropy Heterogeneous-Agent Reinforcement Learning](https://openreview.net/forum?id=tmqOhBC4a5) |  | 0 | \*Multi-agent reinforcement learning\* (MARL) has been shown effective for cooperative games in recent years. However, existing state-of-the-art methods face challenges related to sample complexity, training instability, and the risk of converging to a suboptimal Nash Equilibrium. In this paper, we propose a unified framework for learning \emph{stochastic} policies to resolve these issues. We embed cooperative MARL problems into probabilistic graphical models, from which we derive the maximum entropy (MaxEnt) objective for MARL. Based on the MaxEnt framework, we propose \*Heterogeneous-Agent Soft Actor-Critic\* (HASAC) algorithm. Theoretically, we prove the monotonic improvement and convergence to \*quantal response equilibrium\* (QRE) properties of HASAC. Furthermore, we generalize a unified template for MaxEnt algorithmic design named \*Maximum Entropy Heterogeneous-Agent Mirror Learning\* (MEHAML), which provides any induced method with the same guarantees as HASAC. We evaluate HASAC on six benchmarks: Bi-DexHands, Multi-Agent MuJoCo, StarCraft Multi-Agent Challenge, Google Research Football, Multi-Agent Particle Environment, and Light Aircraft Game. Results show that HASAC consistently outperforms strong baselines, exhibiting better sample efficiency, robustness, and sufficient exploration. | Jiarong Liu, Yifan Zhong, Siyi Hu, Haobo Fu, Qiang Fu, Xiaojun Chang, Yaodong Yang |  |
| 408 |  |  [Hybrid Directional Graph Neural Network for Molecules](https://openreview.net/forum?id=BBD6KXIGJL) |  | 0 | Equivariant message passing neural networks have emerged as the prevailing approach for predicting chemical properties of molecules due to their ability to leverage translation and rotation symmetries, resulting in a strong inductive bias. However, the equivariant operations in each layer can impose excessive constraints on the function form and network flexibility. To address these challenges, we introduce a novel network called the Hybrid Directional Graph Neural Network (HDGNN), which effectively combines strictly equivariant operations with learnable modules. We evaluate the performance of HDGNN on the QM9 dataset and the IS2RE dataset of OC20, demonstrating its state-of-the-art performance on several tasks and competitive performance on others. Our code is anonymously released on https://github.com/ajy112/HDGNN. | Junyi An, Chao Qu, Zhipeng Zhou, Fenglei Cao, Yinghui Xu, Yuan Qi, Furao Shen |  |
| 409 |  |  [Unbiased Watermark for Large Language Models](https://openreview.net/forum?id=uWVC5FVidc) |  | 0 | The recent advancements in large language models (LLMs) have sparked a growing apprehension regarding the potential misuse. One approach to mitigating this risk is to incorporate watermarking techniques into LLMs, allowing for the tracking and attribution of model outputs. This study examines a crucial aspect of watermarking: how significantly watermarks impact the quality of model-generated outputs. Previous studies have suggested a trade-off between watermark strength and output quality. However, our research demonstrates that it is possible to integrate watermarks without affecting the output probability distribution with appropriate implementation. We refer to this type of watermark as an unbiased watermark. This has significant implications for the use of LLMs, as it becomes impossible for users to discern whether a service provider has incorporated watermarks or not. Furthermore, the presence of watermarks does not compromise the performance of the model in downstream tasks, ensuring that the overall utility of the language model is preserved. Our findings contribute to the ongoing discussion around responsible AI development, suggesting that unbiased watermarks can serve as an effective means of tracking and attributing model outputs without sacrificing output quality. | Zhengmian Hu, Lichang Chen, Xidong Wu, Yihan Wu, Hongyang Zhang, Heng Huang |  |
| 410 |  |  [Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop Control](https://openreview.net/forum?id=EriR6Ec69a) |  | 0 | Developing autonomous agents that can interact with changing environments is an open challenge in machine learning. Robustness is particularly important in these settings as agents are often fit offline on expert demonstrations but deployed online where they must generalize to the closed feedback loop within the environment. In this work, we explore the application of recurrent neural networks to tasks of this nature and understand how a parameterization of their recurrent connectivity influences robustness in closed-loop settings. Specifically, we represent the recurrent connectivity as a function of rank and sparsity and show both theoretically and empirically that modulating these two variables has desirable effects on network dynamics. The proposed low-rank, sparse connectivity induces an interpretable prior on the network that proves to be most amenable for a class of models known as closed-form continuous-time neural networks (CfCs). We find that CfCs with fewer parameters can outperform their full-rank, fully-connected counterparts in the online setting under distribution shift. This yields memory-efficient and robust agents while opening a new perspective on how we can modulate network dynamics through connectivity. | Neehal Tumma, Mathias Lechner, Noel Loo, Ramin M. Hasani, Daniela Rus |  |
| 411 |  |  [CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction](https://openreview.net/forum?id=DjzvJCRsVf) |  | 0 | Open-vocabulary dense prediction tasks including object detection and image segmentation have been advanced by the success of Contrastive Language-Image Pre-training (CLIP). CLIP models, particularly those incorporating vision transformers (ViTs), have exhibited remarkable generalization ability in zero-shot image classification. However, when transferring the vision-language alignment of CLIP from global image representation to local region representation for the open-vocabulary dense prediction tasks, CLIP ViTs suffer from the domain shift from full images to local image regions. In this paper, we embark on an in-depth analysis of the region-language alignment in CLIP models, which is essential for downstream open-vocabulary dense prediction tasks. Subsequently, we propose an approach named CLIPSelf, which adapts the image-level recognition ability of CLIP ViT to local image regions without needing any region-text pairs. CLIPSelf empowers ViTs to distill itself by aligning a region representation extracted from its dense feature map with the image-level representation of the corresponding image crop. With the enhanced CLIP ViTs, we achieve new state-of-the-art performance on open-vocabulary object detection, semantic segmentation, and panoptic segmentation across various benchmarks. Models and code are released at https://github.com/wusize/CLIPSelf. | Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Xiangtai Li, Wentao Liu, Chen Change Loy |  |
| 412 |  |  [Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI](https://openreview.net/forum?id=QzTpTRVtrP) |  | 0 | The current electroencephalogram (EEG) based deep learning models are typically designed for specific datasets and applications in brain-computer interaction (BCI), limiting the scale of the models and thus diminishing their perceptual capabilities and generalizability. Recently, Large Language Models (LLMs) have achieved unprecedented success in text processing, prompting us to explore the capabilities of Large EEG Models (LEMs). We hope that LEMs can break through the limitations of different task types of EEG datasets, and obtain universal perceptual capabilities of EEG signals through unsupervised pre-training. Then the models can be fine-tuned for different downstream tasks. However, compared to text data, the volume of EEG datasets is generally small and the format varies widely. For example, there can be mismatched numbers of electrodes, unequal length data samples, varied task designs, and low signal-to-noise ratio. To overcome these challenges, we propose a unified foundation model for EEG called Large Brain Model (LaBraM). LaBraM enables cross-dataset learning by segmenting the EEG signals into EEG channel patches. Vector-quantized neural spectrum prediction is used to train a semantically rich neural tokenizer that encodes continuous raw EEG channel patches into compact neural codes. We then pre-train neural Transformers by predicting the original neural codes for the masked EEG channel patches. The LaBraMs were pre-trained on about 2,500 hours of various types of EEG signals from around 20 datasets and validated on multiple different types of downstream tasks. Experiments on abnormal detection, event type classification, emotion recognition, and gait prediction show that our LaBraM outperforms all compared SOTA methods in their respective fields. Our code is available at https://github.com/935963004/LaBraM. | WeiBang Jiang, LiMing Zhao, BaoLiang Lu |  |
| 413 |  |  [Towards LLM4QPE: Unsupervised Pretraining of Quantum Property Estimation and A Benchmark](https://openreview.net/forum?id=vrBVFXwAmi) |  | 0 | Estimating the properties of quantum systems such as quantum phase has been critical in addressing the essential quantum many-body problems in physics and chemistry. Deep learning models have been recently introduced to property estimation, surpassing conventional statistical approaches. However, these methods are tailored to the specific task and quantum data at hand. It remains an open and attractive question for devising a more universal task-agnostic pretraining model for quantum property estimation. In this paper, we propose LLM4QPE, a large language model style quantum task-agnostic pretraining and finetuning paradigm that 1) performs unsupervised pretraining on diverse quantum systems with different physical conditions; 2) uses the pretrained model for supervised finetuning and delivers high performance with limited training data, on downstream tasks. It mitigates the cost for quantum data collection and speeds up convergence. Extensive experiments show the promising efficacy of LLM4QPE in various tasks including classifying quantum phases of matter on Rydberg atom model and predicting two-body correlation function on anisotropic Heisenberg model. | Yehui Tang, Hao Xiong, Nianzu Yang, Tailong Xiao, Junchi Yan |  |
| 414 |  |  [GTMGC: Using Graph Transformer to Predict Molecule's Ground-State Conformation](https://openreview.net/forum?id=F7QnIKlC1N) |  | 0 | The ground-state conformation of a molecule is often decisive for its properties. However, experimental or computational methods, such as density functional theory (DFT), are time-consuming and labor-intensive for obtaining this conformation. Deep learning (DL) based molecular representation learning (MRL) has made significant advancements in molecular modeling and has achieved remarkable results in various tasks. Consequently, it has emerged as a promising approach for directly predicting the ground-state conformation of molecules. In this regard, we introduce GTMGC, a novel network based on Graph-Transformer (GT) that seamlessly predicts the spatial configuration of molecules in a 3D space from their 2D topological architecture in an end-to-end manner. Moreover, we propose a novel self-attention mechanism called Molecule Structural Residual Self-Attention (MSRSA) for molecular structure modeling. This mechanism not only guarantees high model performance and easy implementation but also lends itself well to other molecular modeling tasks. Our method has been evaluated on the Molecule3D benchmark dataset and the QM9 dataset. Experimental results demonstrate that our approach achieves remarkable performance and outperforms current state-of-the-art methods as well as the widely used open-source software RDkit. | Guikun Xu, Yongquan Jiang, PengChuan Lei, Yan Yang, Jim Chen |  |
| 415 |  |  [Generalization of Scaled Deep ResNets in the Mean-Field Regime](https://openreview.net/forum?id=tMzPZTvz2H) |  | 0 | Despite the widespread empirical success of ResNet, the generalization properties of deep ResNet are rarely explored beyond the lazy training regime. In this work, we investigate scaled ResNet in the limit of infinitely deep and wide neural networks, of which the gradient flow is described by a partial differential equation in the large-neural network limit, i.e., the mean-field regime. To derive the generalization bounds under this setting, our analysis necessitates a shift from the conventional time-invariant Gram matrix employed in the lazy training regime to a time-variant, distribution-dependent version. To this end, we provide a global lower bound on the minimum eigenvalue of the Gram matrix under the mean-field regime. Besides, for the traceability of the dynamic of Kullback-Leibler (KL) divergence, we establish the linear convergence of the empirical error and estimate the upper bound of the KL divergence over parameters distribution. Finally, we build the uniform convergence for generalization bound via Rademacher complexity. Our results offer new insights into the generalization ability of deep ResNet beyond the lazy training regime and contribute to advancing the understanding of the fundamental properties of deep neural networks. | Yihang Chen, Fanghui Liu, Yiping Lu, Grigorios Chrysos, Volkan Cevher |  |
| 416 |  |  [ODE Discovery for Longitudinal Heterogeneous Treatment Effects Inference](https://openreview.net/forum?id=pxI5IPeWgW) |  | 0 | Inferring unbiased treatment effects has received widespread attention in the machine learning community. In recent years, our community has proposed numerous solutions in standard settings, high-dimensional treatment settings, and even longitudinal settings. While very diverse, the solution has mostly relied on neural networks for inference and simultaneous correction of assignment bias. New approaches typically build on top of previous approaches by proposing new (or refined) architectures and learning algorithms. However, the end result—a neural-network-based inference machine—remains unchallenged. In this paper, we introduce a different type of solution in the longitudinal setting: a closed-form ordinary differential equation (ODE). While we still rely on continuous optimization to learn an ODE, the resulting inference machine is no longer a neural network. Doing so yields several advantages such as interpretability, irregular sampling, and a different set of identification assumptions. Above all, we consider the introduction of a completely new type of solution to be our most important contribution as it may spark entirely new innovations in treatment effects in general. We facilitate this by formulating our contribution as a framework that can transform any ODE discovery method into a treatment effects method. | Krzysztof Kacprzyk, Samuel Holt, Jeroen Berrevoets, Zhaozhi Qian, Mihaela van der Schaar |  |
| 417 |  |  [Learning Hierarchical World Models with Adaptive Temporal Abstractions from Discrete Latent Dynamics](https://openreview.net/forum?id=TjCDNssXKU) |  | 0 | Hierarchical world models can significantly improve model-based reinforcement learning (MBRL) and planning by enabling reasoning across multiple time scales. Nonetheless, the majority of state-of-the-art MBRL methods employ flat, non-hierarchical models. We propose Temporal Hierarchies from Invariant Context Kernels (THICK), an algorithm that learns a world model hierarchy via discrete latent dynamics. The lower level of THICK updates parts of its latent state sparsely in time, forming invariant contexts. The higher level exclusively predicts situations involving context changes. Our experiments demonstrate that THICK learns categorical, interpretable, temporal abstractions on the high level, while maintaining precise low-level predictions. Furthermore, we show that the emergent hierarchical predictive model seamlessly enhances the abilities of MBRL or planning methods. We believe that THICK contributes to the further development of hierarchical agents capable of more sophisticated planning and reasoning abilities. | Christian Gumbsch, Noor Sajid, Georg Martius, Martin V. Butz |  |
| 418 |  |  [Prediction without Preclusion: Recourse Verification with Reachable Sets](https://openreview.net/forum?id=SCQfYpdoGE) |  | 0 | Machine learning models are often used to decide who receives a loan, a job interview, or a public benefit. Models in such settings use features without considering their \*actionability\*. As a result, they can assign predictions that are \emph{fixed} -- meaning that individuals who are denied loans and interviews are, in fact, \*precluded from access\* to credit and employment. In this work, we introduce a procedure called \*recourse verification\* to test if a model assigns fixed predictions to its decision subjects. We propose a model-agnostic approach for verification with \*reachable sets\* -- i.e., the set of all points that a person can reach through their actions in feature space. We develop methods to construct reachable sets for discrete feature spaces, which can certify the responsiveness of \*any model\* by simply querying its predictions. We conduct a comprehensive empirical study on the infeasibility of recourse on datasets from consumer finance. Our results highlight how models can inadvertently preclude access by assigning fixed predictions and underscore the need to account for actionability in model development. | Avni Kothari, Bogdan Kulynych, TsuiWei Weng, Berk Ustun |  |
| 419 |  |  [ODICE: Revealing the Mystery of Distribution Correction Estimation via Orthogonal-gradient Update](https://openreview.net/forum?id=L8UNn7Llt4) |  | 0 | In this study, we investigate the DIstribution Correction Estimation (DICE) methods, an important line of work in offline reinforcement learning (RL) and imitation learning (IL). DICE-based methods impose state-action-level behavior constraint, which is an ideal choice for offline learning. However, they typically perform much worse than current state-of-the-art (SOTA) methods that solely use action-level behavior constraint. After revisiting DICE-based methods, we find there exist two gradient terms when learning the value function using true-gradient update: forward gradient (taken on the current state) and backward gradient (taken on the next state). Using forward gradient bears a large similarity to many offline RL methods, and thus can be regarded as applying action-level constraint. However, directly adding the backward gradient may degenerate or cancel out its effect if these two gradients have conflicting directions. To resolve this issue, we propose a simple yet effective modification that projects the backward gradient onto the normal plane of the forward gradient, resulting in an orthogonal-gradient update, a new learning rule for DICE-based methods. We conduct thorough theoretical analyses and find that the projected backward gradient brings state-level behavior regularization, which reveals the mystery of DICE-based methods: the value learning objective does try to impose state-action-level constraint, but needs to be used in a corrected way. Through toy examples and extensive experiments on complex offline RL and IL tasks, we demonstrate that DICE-based methods using orthogonal-gradient updates achieve SOTA performance and great robustness. | Liyuan Mao, Haoran Xu, Weinan Zhang, Xianyuan Zhan |  |
| 420 |  |  [Improving Non-Transferable Representation Learning by Harnessing Content and Style](https://openreview.net/forum?id=FYKVPOHCpE) |  | 0 | Non-transferable learning (NTL) aims to restrict the generalization of models toward the target domain(s). To this end, existing works learn non-transferable representations by reducing statistical dependence between the source and target domain. However, such statistical methods essentially neglect to distinguish between \*styles\* and \*contents\*, leading them to inadvertently fit (i) spurious correlation between \*styles\* and \*labels\*, and (ii) fake independence between \*contents\* and \*labels\*. Consequently, their performance will be limited when natural distribution shifts occur or malicious intervention is imposed. In this paper, we propose a novel method (dubbed as H-NTL) to understand and advance the NTL problem by introducing a causal model to separately model \*content\* and \*style\* as two latent factors, based on which we disentangle and harness them as guidances for learning non-transferable representations with intrinsically causal relationships. Speciﬁcally, to avoid fitting spurious correlation and fake independence, we propose a variational inference framework to disentangle the naturally mixed \*content factors\* and \*style factors\* under our causal model. Subsequently, based on dual-path knowledge distillation, we harness the disentangled two \*factors\* as guidances for non-transferable representation learning: (i) we constraint the source domain representations to fit \*content factors\* (which are the intrinsic cause of \*labels\*), and (ii) we enforce that the target domain representations fit \*style factors\* which barely can predict labels. As a result, the learned feature representations follow optimal untransferability toward the target domain and minimal negative influence on the source domain, thus enabling better NTL performance. Empirically, the proposed H-NTL signiﬁcantly outperforms competing methods by a large margin. | Ziming Hong, Zhenyi Wang, Li Shen, Yu Yao, Zhuo Huang, Shiming Chen, Chuanwu Yang, Mingming Gong, Tongliang Liu |  |
| 421 |  |  [ModernTCN: A Modern Pure Convolution Structure for General Time Series Analysis](https://openreview.net/forum?id=vpJMJerXHU) |  | 0 | Recently, Transformer-based and MLP-based models have emerged rapidly and won dominance in time series analysis. In contrast, convolution is losing steam in time series tasks nowadays for inferior performance. This paper studies the open question of how to better use convolution in time series analysis and makes efforts to bring convolution back to the arena of time series analysis. To this end, we modernize the traditional TCN and conduct time series related modifications to make it more suitable for time series tasks. As the outcome, we propose ModernTCN and successfully solve this open question through a seldom-explored way in time series community. As a pure convolution structure, ModernTCN still achieves the consistent state-of-the-art performance on five mainstream time series analysis tasks while maintaining the efficiency advantage of convolution-based models, therefore providing a better balance of efficiency and performance than state-of-the-art Transformer-based and MLP-based models. Our study further reveals that, compared with previous convolution-based models, our ModernTCN has much larger effective receptive fields (ERFs), therefore can better unleash the potential of convolution in time series analysis. Code is available at this repository: https://github.com/luodhhh/ModernTCN. | Donghao Luo, Xue Wang |  |
| 422 |  |  [Towards Robust Out-of-Distribution Generalization Bounds via Sharpness](https://openreview.net/forum?id=tPEwSYPtAC) |  | 0 | Generalizing to out-of-distribution (OOD) data or unseen domain, termed OOD generalization, still lacks appropriate theoretical guarantees. Canonical OOD bounds focus on different distance measurements between source and target domains but fail to consider the optimization property of the learned model. As empirically shown in recent work, sharpness of learned minimum influences OOD generalization. To bridge this gap between optimization and OOD generalization, we study the effect of sharpness on how a model tolerates data change in domain shift which is usually captured by "robustness" in generalization. In this paper, we give a rigorous connection between sharpness and robustness, which gives better OOD guarantees for robust algorithms. It also provides a theoretical backing for "flat minima leads to better OOD generalization". Overall, we propose a sharpness-based OOD generalization bound by taking robustness into consideration, resulting in a tighter bound than non-robust guarantees. Our findings are supported by the experiments on a ridge regression model, as well as the experiments on deep learning classification tasks. | Yingtian Zou, Kenji Kawaguchi, Yingnan Liu, Jiashuo Liu, MongLi Lee, Wynne Hsu |  |
| 423 |  |  [MAPE-PPI: Towards Effective and Efficient Protein-Protein Interaction Prediction via Microenvironment-Aware Protein Embedding](https://openreview.net/forum?id=itGkF993gz) |  | 0 | Protein-Protein Interactions (PPIs) are fundamental in various biological processes and play a key role in life activities. The growing demand and cost of experimental PPI assays require computational methods for efficient PPI prediction. While existing methods rely heavily on protein sequence for PPI prediction, it is the protein structure that is the key to determine the interactions. To take both protein modalities into account, we define the microenvironment of an amino acid residue by its sequence and structural contexts, which describe the surrounding chemical properties and geometric features. In addition, microenvironments defined in previous work are largely based on experimentally assayed physicochemical properties, for which the "vocabulary" is usually extremely small. This makes it difficult to cover the diversity and complexity of microenvironments. In this paper, we propose Microenvironment-Aware Protein Embedding for PPI prediction (MPAE-PPI), which encodes microenvironments into chemically meaningful discrete codes via a sufficiently large microenvironment "vocabulary" (i.e., codebook). Moreover, we propose a novel pre-training strategy, namely Masked Codebook Modeling (MCM), to capture the dependencies between different microenvironments by randomly masking the codebook and reconstructing the input. With the learned microenvironment codebook, we can reuse it as an off-the-shelf tool to efficiently and effectively encode proteins of different sizes and functions for large-scale PPI prediction. Extensive experiments show that MAPE-PPI can scale to PPI prediction with millions of PPIs with superior trade-offs between effectiveness and computational efficiency than the state-of-the-art competitors. | Lirong Wu, Yijun Tian, Yufei Huang, Siyuan Li, Haitao Lin, Nitesh V. Chawla, Stan Z. Li |  |
| 424 |  |  [Negative Label Guided OOD Detection with Pretrained Vision-Language Models](https://openreview.net/forum?id=xUO1HXz4an) |  | 0 | Out-of-distribution (OOD) detection aims at identifying samples from unknown classes, playing a crucial role in trustworthy models against errors on unexpected inputs. Extensive research has been dedicated to exploring OOD detection in the vision modality. {Vision-language models (VLMs) can leverage both textual and visual information for various multi-modal applications, whereas few OOD detection methods take into account information from the text modality. In this paper, we propose a novel post hoc OOD detection method, called NegLabel, which takes a vast number of negative labels from extensive corpus databases. We design a novel scheme for the OOD score collaborated with negative labels. Theoretical analysis helps to understand the mechanism of negative labels. Extensive experiments demonstrate that our method NegLabel achieves state-of-the-art performance on various OOD detection benchmarks and generalizes well on multiple VLM architectures. Furthermore, our method NegLabel exhibits remarkable robustness against diverse domain shifts. The codes are available at https://github.com/tmlr-group/NegLabel. | Xue Jiang, Feng Liu, Zhen Fang, Hong Chen, Tongliang Liu, Feng Zheng, Bo Han |  |
| 425 |  |  [Optimal robust Memorization with ReLU Neural Networks](https://openreview.net/forum?id=47hDbAMLbc) |  | 0 | Memorization with neural networks is to study the expressive power of neural networks to interpolate a finite classification data set, which is closely related to the generalizability of deep learning. However, the important problem of robust memorization has not been thoroughly studied. In this paper, several basic problems about robust memorization are solved. First, we prove that it is NP-hard to compute neural networks with certain simple structures, which are robust memorization. A network hypothesis space is called optimal robust memorization for a data set if it can achieve robust memorization for any budget less than half the separation bound of the data set. Second, we explicitly construct neural networks with O(N n) parameters for optimal robust memorization of any data set with dimension n and size N . We also give a lower bound for the width of networks to achieve optimal robust memorization. Finally, we explicitly construct neural networks with O(N n log n) parameters for optimal robust memorization of any binary classification data set by controlling the Lipschitz constant of the network. | Lijia Yu, XiaoShan Gao, Lijun Zhang |  |
| 426 |  |  [Neural Contractive Dynamical Systems](https://openreview.net/forum?id=iAYIRHOYy8) |  | 0 | Stability guarantees are crucial when ensuring that a fully autonomous robot does not take undesirable or potentially harmful actions. Unfortunately, global stability guarantees are hard to provide in dynamical systems learned from data, especially when the learned dynamics are governed by neural networks. We propose a novel methodology to learn \emph{neural contractive dynamical systems}, where our neural architecture ensures contraction, and hence, global stability. To efficiently scale the method to high-dimensional dynamical systems, we develop a variant of the variational autoencoder that learns dynamics in a low-dimensional latent representation space while retaining contractive stability after decoding. We further extend our approach to learning contractive systems on the Lie group of rotations to account for full-pose end-effector dynamic motions. The result is the first highly flexible learning architecture that provides contractive stability guarantees with capability to perform obstacle avoidance. Empirically, we demonstrate that our approach encodes the desired dynamics more accurately than the current state-of-the-art, which provides less strong stability guarantees. | Hadi BeikMohammadi, Søren Hauberg, Georgios Arvanitidis, Nadia Figueroa, Gerhard Neumann, Leonel Rozo |  |
| 427 |  |  [Scaling Laws for Associative Memories](https://openreview.net/forum?id=Tzh6xAJSll) |  | 0 | Learning arguably involves the discovery and memorization of abstract rules. The aim of this paper is to study associative memory mechanisms. Our model is based on high-dimensional matrices consisting of outer products of embeddings, which relates to the inner layers of transformer language models. We derive precise scaling laws with respect to sample size and parameter size, and discuss the statistical efficiency of different estimators, including optimization-based algorithms. We provide extensive numerical experiments to validate and interpret theoretical results, including fine-grained visualizations of the stored memory associations. | Vivien Cabannes, Elvis Dohmatob, Alberto Bietti |  |
| 428 |  |  [Text2Reward: Reward Shaping with Language Models for Reinforcement Learning](https://openreview.net/forum?id=tUM39YTRxH) |  | 0 | Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation and shaping of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates shaped dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes or unshaped dense rewards with a constant function across timesteps, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better task success rates and convergence speed than expert-written reward codes. For locomotion tasks, our method learns six novel locomotion behaviors with a success rate exceeding 94%. Furthermore, we show that the policies trained in the simulator with our method can be deployed in the real world. Finally, Text2Reward further improves the policies by refining their reward functions with human feedback. Video results are available at https://text-to-reward.github.io/ | Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, Tao Yu |  |
| 429 |  |  [Towards Meta-Pruning via Optimal Transport](https://openreview.net/forum?id=sMoifbuxjB) |  | 0 | Structural pruning of neural networks conventionally relies on identifying and discarding less important neurons, a practice often resulting in significant accuracy loss that necessitates subsequent fine-tuning efforts. This paper introduces a novel approach named Intra-Fusion, challenging this prevailing pruning paradigm. Unlike existing methods that focus on designing meaningful neuron importance metrics, Intra-Fusion redefines the overlying pruning procedure. Through utilizing the concepts of model fusion and Optimal Transport, we leverage an agnostically given importance metric to arrive at a more effective sparse model representation. Notably, our approach achieves substantial accuracy recovery without the need for resource-intensive fine-tuning, making it an efficient and promising tool for neural network compression. Additionally, we explore how fusion can be added to the pruning process to significantly decrease the training time while maintaining competitive performance. We benchmark our results for various networks on commonly used datasets such as CIFAR-10, CIFAR-100, and ImageNet. More broadly, we hope that the proposed Intra-Fusion approach invigorates exploration into a fresh alternative to the predominant compression approaches. Our code is available [here](https://github.com/alexandertheus/Intra-Fusion). | Alexander Theus, Olin Geimer, Friedrich Wicke, Thomas Hofmann, Sotiris Anagnostidis, Sidak Pal Singh |  |
| 430 |  |  [InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation](https://openreview.net/forum?id=MLBdiWu4Fw) |  | 0 | This paper introduces InternVid, a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation. InternVid contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words. Our core contribution is to develop a scalable approach to autonomously build a high-quality video-text dataset with large language models (LLM), thereby showcasing its efficacy in learning video-language representation at scale. Specifically, we utilize a multi-scale approach to generate video-related descriptions. Furthermore, we introduce ViCLIP, a video-text representation learning model based on ViT-L. Learned on InternVid via contrastive learning, this model demonstrates leading zero-shot action recognition and competitive video retrieval performance. Beyond basic video understanding tasks like recognition and retrieval, our dataset and model have broad applications. They are particularly beneficial for generating interleaved video-text data for learning a video-centric dialogue system, advancing video-to-text and text-to-video generation research. These proposed resources provide a tool for researchers and practitioners interested in multimodal video understanding and generation. | Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, Yu Qiao |  |
| 431 |  |  [Dictionary Contrastive Learning for Efficient Local Supervision without Auxiliary Networks](https://openreview.net/forum?id=Gg7cXo3S8l) |  | 0 | While backpropagation (BP) has achieved widespread success in deep learning, it faces two prominent challenges: computational inefficiency and biological implausibility. In response to these challenges, local supervision, encompassing Local Learning (LL) and Forward Learning (FL), has emerged as a promising research direction. LL employs module-wise BP to achieve competitive results yet relies on module-wise auxiliary networks, which increase memory and parameter demands. Conversely, FL updates layer weights without BP and auxiliary networks but falls short of BP’s performance. This paper proposes a simple yet effective objective within a contrastive learning framework for local supervision without auxiliary networks. Given the insight that the existing contrastive learning framework for local supervision is susceptible to task-irrelevant information without auxiliary networks, we present DICTIONARY CONTRASTIVE LEARNING (DCL) that optimizes the similarity between local features and label embeddings. Our method using static label embeddings yields substantial performance improvements in the FL scenario, outperforming state-of-the-art FL approaches. Moreover, our method using adaptive label embeddings closely approaches the performance achieved by LL while achieving superior memory and parameter efficiency. | Suhwan Choi, Myeongho Jeon, Yeonjung Hwang, Jeonglyul Oh, Sungjun Lim, Joonseok Lee, Myungjoo Kang |  |
| 432 |  |  [Bounding Box Stability against Feature Dropout Reflects Detector Generalization across Environments](https://openreview.net/forum?id=lmM4Ecm4HJ) |  | 0 | Bounding boxes uniquely characterize object detection, where a good detector gives accurate bounding boxes of categories of interest. However, in the real-world where test ground truths are not provided, it is non-trivial to find out whether bounding boxes are accurate, thus preventing us from assessing the detector generalization ability. In this work, we find under feature map dropout, good detectors tend to output bounding boxes whose locations do not change much, while bounding boxes of poor detectors will undergo noticeable position changes. We compute the box stability score (BS score) to reflect this stability. Specifically, given an image, we compute a normal set of bounding boxes and a second set after feature map dropout. To obtain BS score, we use bipartite matching to find the corresponding boxes between the two sets and compute the average Intersection over Union (IoU) across the entire test set. We contribute to finding that BS score has a strong, positive correlation with detection accuracy measured by mean average precision (mAP) under various test environments. This relationship allows us to predict the accuracy of detectors on various real-world test sets without accessing test ground truths, verified on canonical detection tasks such as vehicle detection and pedestrian detection. | Yang Yang, Wenhai Wang, Zhe Chen, Jifeng Dai, Liang Zheng |  |
| 433 |  |  [Deep Geodesic Canonical Correlation Analysis for Covariance-Based Neuroimaging Data](https://openreview.net/forum?id=PnR1MNen7u) |  | 0 | In human neuroimaging, multi-modal imaging techniques are frequently combined to enhance our comprehension of whole-brain dynamics and improve diagnosis in clinical practice. Modalities like electroencephalography and functional magnetic resonance imaging provide distinct views to the brain dynamics due to diametral spatiotemporal sensitivities and underlying neurophysiological coupling mechanisms. These distinct views pose a considerable challenge to learning a shared representation space, especially when dealing with covariance-based data characterized by their geometric structure. To capitalize on the geometric structure, we introduce a measure called geodesic correlation which expands traditional correlation consistency to covariance-based data on the symmetric positive definite (SPD) manifold. This measure is derived from classical canonical correlation analysis and serves to evaluate the consistency of latent representations obtained from paired views. For multi-view, self-supervised learning where one or both latent views are SPD we propose an innovative geometric deep learning framework termed DeepGeoCCA. Its primary objective is to enhance the geodesic correlation of unlabeled, paired data, thereby generating novel representations while retaining the geometric structures. In simulations and experiments with multi-view and multi-modal human neuroimaging data, we find that DeepGeoCCA learns latent representations with high geodesic correlation for unseen data while retaining relevant information for downstream tasks. | Ce Ju, Reinmar J. Kobler, Liyao Tang, Cuntai Guan, Motoaki Kawanabe |  |
| 434 |  |  [SWAP-NAS: Sample-Wise Activation Patterns for Ultra-fast NAS](https://openreview.net/forum?id=tveiUXU2aa) |  | 0 | Training-free metrics (a.k.a. zero-cost proxies) are widely used to avoid resource-intensive neural network training, especially in Neural Architecture Search (NAS). Recent studies show that existing training-free metrics have several limitations, such as limited correlation and poor generalisation across different search spaces and tasks. Hence, we propose Sample-Wise Activation Patterns and its derivative, SWAP-Score, a novel high-performance training-free metric. It measures the expressivity of networks over a batch of input samples. The SWAP-Score is strongly correlated with ground-truth performance across various search spaces and tasks, outperforming 15 existing training-free metrics on NAS-Bench-101/201/301 and TransNAS-Bench-101. The SWAP-Score can be further enhanced by regularisation, which leads to even higher correlations in cell-based search space and enables model size control during the search. For example, Spearman’s rank correlation coefficient between regularised SWAP-Score and CIFAR-100 validation accuracies on NAS-Bench-201 networks is 0.90, significantly higher than 0.80 from the second-best metric, NWOT. When integrated with an evolutionary algorithm for NAS, our SWAP-NAS achieves competitive performance on CIFAR-10 and ImageNet in approximately 6 minutes and 9 minutes of GPU time respectively. | Yameng Peng, Andy Song, Haytham M. Fayek, Vic Ciesielski, Xiaojun Chang |  |
| 435 |  |  [RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches](https://openreview.net/forum?id=F1TKzG8LJO) |  | 0 | Generalization remains one of the most important desiderata for robust robot learning systems. While recently proposed approaches show promise in generalization to novel objects, semantic concepts, or visual distribution shifts, generalization to new tasks remains challenging. For example, a language-conditioned policy trained on pick-and-place tasks will not be able to generalize to a folding task, even if the arm trajectory of folding is similar to pick-and-place. Our key insight is that this kind of generalization becomes feasible if we represent the task through rough trajectory sketches. We propose a policy conditioning method using such rough trajectory sketches, which we call RT-Trajectory, that is practical, easy to specify, and allows the policy to effectively perform new tasks that would otherwise be challenging to perform. We find that trajectory sketches strike a balance between being detailed enough to express low-level motion-centric guidance while being coarse enough to allow the learned policy to interpret the trajectory sketch in the context of situational visual observations. In addition, we show how trajectory sketches can provide a useful interface to communicate with robotic policies -- they can be specified through simple human inputs like drawings or videos, or through automated methods such as modern image-generating or waypoint-generating methods. We evaluate RT-Trajectory at scale on a variety of real-world robotic tasks, and find that RT-Trajectory is able to perform a wider range of tasks compared to language-conditioned and goal-conditioned policies, when provided the same training data. | Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu, Montserrat Gonzalez Arenas, Kanishka Rao, Wenhao Yu, Chuyuan Fu, Keerthana Gopalakrishnan, Zhuo Xu, Priya Sundaresan, Peng Xu, Hao Su, Karol Hausman, Chelsea Finn, Quan Vuong, Ted Xiao |  |
| 436 |  |  [NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers](https://openreview.net/forum?id=Rc7dAwVL3v) |  | 0 | Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild datasets is important to capture the diversity in human speech such as speaker identities, prosodies, and styles (e.g., singing). Current large TTS systems usually quantize speech into discrete tokens and use language models to generate these tokens one by one, which suffer from unstable prosody, word skipping/repeating issue, and poor voice quality. In this paper, we develop NaturalSpeech 2, a TTS system that leverages a neural audio codec with residual vector quantizers to get the quantized latent vectors and uses a diffusion model to generate these latent vectors conditioned on text input. To enhance the zero-shot capability that is important to achieve diverse speech synthesis, we design a speech prompting mechanism to facilitate in-context learning in the diffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to large-scale datasets with 44K hours of speech and singing data and evaluate its voice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS systems by a large margin in terms of prosody/timbre similarity, robustness, and voice quality in a zero-shot setting, and performs novel zero-shot singing synthesis with only a speech prompt. Audio samples are available at https://naturalspeech2.github.io/. | Kai Shen, Zeqian Ju, Xu Tan, Eric Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, Jiang Bian |  |
| 437 |  |  [Submodular Reinforcement Learning](https://openreview.net/forum?id=loYSzjSaAK) |  | 0 | In reinforcement learning (RL), rewards of states are typically considered additive, and following the Markov assumption, they are independent of states visited previously. In many important applications, such as coverage control, experiment design and informative path planning, rewards naturally have diminishing returns, i.e., their value decreases in light of similar states visited previously. To tackle this, we propose Submodular RL (subRL), a paradigm which seeks to optimize more general, non-additive (and history-dependent) rewards modelled via submodular set functions, which capture diminishing returns. Unfortunately, in general, even in tabular settings, we show that the resulting optimization problem is hard to approximate. On the other hand, motivated by the success of greedy algorithms in classical submodular optimization, we propose subPO, a simple policy gradient-based algorithm for subRL that handles non-additive rewards by greedily maximizing marginal gains. Indeed, under some assumptions on the underlying Markov Decision Process (MDP), subPO recovers optimal constant factor approximations of submodular bandits. Moreover, we derive a natural policy gradient approach for locally optimizing subRL instances even in large state- and action- spaces. We showcase the versatility of our approach by applying subPO to several applications, such as biodiversity monitoring, Bayesian experiment design, informative path planning, and coverage maximization. Our results demonstrate sample efficiency, as well as scalability to high-dimensional state-action spaces. | Manish Prajapat, Mojmir Mutny, Melanie N. Zeilinger, Andreas Krause |  |
| 438 |  |  [Making Pre-trained Language Models Great on Tabular Prediction](https://openreview.net/forum?id=anzIzGZuLi) |  | 0 | The transferability of deep neural networks (DNNs) has made significant progress in image and language processing. However, due to the heterogeneity among tables, such DNN bonus is still far from being well exploited on tabular data prediction (e.g., regression or classification tasks). Condensing knowledge from diverse domains, language models (LMs) possess the capability to comprehend feature names from various tables, potentially serving as versatile learners in transferring knowledge across distinct tables and diverse prediction tasks, but their discrete text representation space is inherently incompatible with numerical feature values in tables. In this paper, we present TP-BERTa, a specifically pre-trained LM for tabular data prediction. Concretely, a novel relative magnitude tokenization converts scalar numerical feature values to finely discrete, high-dimensional tokens, and an intra-feature attention approach integrates feature values with the corresponding feature names. Comprehensive experiments demonstrate that our pre-trained TP-BERTa leads the performance among tabular DNNs and is competitive with Gradient Boosted Decision Tree models in typical tabular data regime. | Jiahuan Yan, Bo Zheng, Hongxia Xu, Yiheng Zhu, Danny Z. Chen, Jimeng Sun, Jian Wu, Jintai Chen |  |
| 439 |  |  [Solving Inverse Problems with Latent Diffusion Models via Hard Data Consistency](https://openreview.net/forum?id=j8hdRqOUhN) |  | 0 | Latent diffusion models have been demonstrated to generate high-quality images, while offering efficiency in model training compared to diffusion models operating in the pixel space. However, incorporating latent diffusion models to solve inverse problems remains a challenging problem due to the nonlinearity of the encoder and decoder. To address these issues, we propose ReSample, an algorithm that can solve general inverse problems with pre-trained latent diffusion models. Our algorithm incorporates data consistency by solving an optimization problem during the reverse sampling process, a concept that we term as hard data consistency. Upon solving this optimization problem, we propose a novel resampling scheme to map the measurement-consistent sample back onto the noisy data manifold and theoretically demonstrate its benefits. Lastly, we apply our algorithm to solve a wide range of linear and nonlinear inverse problems in both natural and medical images, demonstrating that our approach outperforms existing state-of-the-art approaches, including those based on pixel-space diffusion models. | Bowen Song, Soo Min Kwon, Zecheng Zhang, Xinyu Hu, Qing Qu, Liyue Shen |  |
| 440 |  |  [The False Promise of Imitating Proprietary Language Models](https://openreview.net/forum?id=Kz3yckpCN5) |  | 0 | An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct, and others). In this work, we critically analyze this approach of imitating language models. We first finetune a series of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the models using crowd raters and canonical NLP benchmarks. Initially, we were surprised by the output quality of our imitation models---they appear far better at following instructions, and crowd workers rate their outputs as competitive with ChatGPT. However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data. We show that these performance discrepancies may slip past human raters because imitation models are adept at mimicking ChatGPT’s style but not its factuality. Overall, we conclude that while model imitation can be useful for training models to follow instructions and avoid toxic outputs, it falls short its full promise in many ways. In particular, there exists a substantial capabilities gap between open and closed LMs that we find cannot be bridged merely by adding more imitation data. Instead, we find that fine-tuning more capable base LMs has a significantly more substantial effect on closing this gap. In turn, we argue that the higher leverage action for improving open-source models is to tackle the difficult challenge of developing better base LMs, rather than taking the shortcut of imitating proprietary systems. | Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, Dawn Song |  |
| 441 |  |  [Sample-Efficient Linear Representation Learning from Non-IID Non-Isotropic Data](https://openreview.net/forum?id=Tr3fZocrI6) |  | 0 | A powerful concept behind much of the recent progress in machine learning is the extraction of common features across data from heterogeneous sources or tasks. Intuitively, using all of one's data to learn a common representation function benefits both computational effort and statistical generalization by leaving a smaller number of parameters to fine-tune on a given task. Toward theoretically grounding these merits, we propose a general setting of recovering linear operators $M$ from noisy vector measurements $y = Mx + w$, where the covariates $x$ may be both non-i.i.d. and non-isotropic. We demonstrate that existing isotropy-agnostic meta-learning approaches incur biases on the representation update, which causes the scaling of the noise terms to lose favorable dependence on the number of source tasks. This in turn can cause the sample complexity of representation learning to be bottlenecked by the single-task data size. We introduce an adaptation, $\texttt{De-bias}$ & $\texttt{Feature-Whiten}$ ($\texttt{DFW}$), of the popular alternating minimization-descent (AMD) scheme proposed in Collins et al., (2021), and establish linear convergence to the optimal representation with noise level scaling down with the $\textit{total}$ source data size. This leads to generalization bounds on the same order as an oracle empirical risk minimizer. We verify the vital importance of $\texttt{DFW}$ on various numerical simulations. In particular, we show that vanilla alternating-minimization descent fails catastrophically even for iid, but mildly non-isotropic data. Our analysis unifies and generalizes prior work, and provides a flexible framework for a wider range of applications, such as in controls and dynamical systems. | Thomas T. C. K. Zhang, Leonardo Felipe Toso, James Anderson, Nikolai Matni |  |
| 442 |  |  [Information Retention via Learning Supplemental Features](https://openreview.net/forum?id=o83eu4H9Mb) |  | 0 | The information bottleneck principle provides an information-theoretic method for learning a good representation as a trade-off between conciseness and predictive ability, which can reduce information redundancy, eliminate irrelevant and superfluous features, and thus enhance the in-domain generalizability. However, in low-resource or out-of-domain scenarios where the assumption of i.i.d does not necessarily hold true, superfluous (or redundant) relevant features may be supplemental to the mainline features of the model, and be beneficial in making prediction for test dataset with distribution shift. Therefore, instead of squeezing the input information by information bottleneck, we propose to keep as much relevant information as possible in use for making predictions. A three-stage supervised learning framework is designed and implemented to jointly learn the mainline and supplemental features, relieving supplemental features from the suppression of mainline features. Extensive experiments have shown that the learned representations of our method have good in-domain and out-of-domain generalization abilities, especially in low-resource cases. | Zhipeng Xie, Yahe Li |  |
| 443 |  |  [Mayfly: a Neural Data Structure for Graph Stream Summarization](https://openreview.net/forum?id=n7Sr8SW4bn) |  | 0 | A graph is a structure made up of vertices and edges used to represent complex relationships between entities, while a graph stream is a continuous flow of graph updates that convey evolving relationships between entities. The massive volume and high dynamism of graph streams promote research on data structures of graph summarization, which provides a concise and approximate view of graph streams with sub-linear space and linear construction time, enabling real-time graph analytics in various domains, such as social networking, financing, and cybersecurity. In this work, we propose the Mayfly, the first neural data structure for summarizing graph streams. The Mayfly replaces handcrafted data structures with better accuracy and adaptivity. To cater to practical applications, Mayfly incorporates two offline training phases. During the larval phase, the Mayfly learns basic summarization abilities from automatically and synthetically constituted meta-tasks, and in the metamorphosis phase, it rapidly adapts to real graph streams via meta-tasks. With specific configurations of information pathways, the Mayfly enables flexible support for miscellaneous graph queries, including edge, node, and connectivity queries. Extensive empirical studies show that the Mayfly significantly outperforms its handcrafted competitors. | Yuan Feng, Yukun Cao, Hairu Wang, Xike Xie, S. Kevin Zhou |  |
| 444 |  |  [Exploring the Common Appearance-Boundary Adaptation for Nighttime Optical Flow](https://openreview.net/forum?id=776lhoaulC) |  | 0 | We investigate a challenging task of nighttime optical flow, which suffers from weakened texture and amplified noise. These degradations weaken discriminative visual features, thus causing invalid motion feature matching. Typically, existing methods employ domain adaptation to transfer knowledge from auxiliary domain to nighttime domain in either input visual space or output motion space. However, this direct adaptation is ineffective, since there exists a large domain gap due to the intrinsic heterogeneous nature of the feature representations between auxiliary and nighttime domains. To overcome this issue, we explore a common-latent space as the intermediate bridge to reinforce the feature alignment between auxiliary and nighttime domains. In this work, we exploit two auxiliary daytime and event domains, and propose a novel common appearance-boundary adaptation framework for nighttime optical flow. In appearance adaptation, we employ the intrinsic image decomposition to embed the auxiliary daytime image and the nighttime image into a reflectance-aligned common space. We discover that motion distributions of the two reflectance maps are very similar, benefiting us to consistently transfer motion appearance knowledge from daytime to nighttime domain. In boundary adaptation, we theoretically derive the motion correlation formula between nighttime image and accumulated events within a spatiotemporal gradient-aligned common space. We figure out that the correlation of the two spatiotemporal gradient maps shares significant discrepancy, benefitting us to contrastively transfer boundary knowledge from event to nighttime domain. Moreover, appearance adaptation and boundary adaptation are complementary to each other, since they could jointly transfer global motion and local boundary knowledge to the nighttime domain. Extensive experiments have been performed to verify the superiority of the proposed method. | Hanyu Zhou, Yi Chang, Haoyue Liu, Wending Yan, Yuxing Duan, Zhiwei Shi, Luxin Yan |  |
| 445 |  |  [Graphical Multioutput Gaussian Process with Attention](https://openreview.net/forum?id=6N8TW504aa) |  | 0 | Integrating information while recognizing dependence from multiple data sources and enhancing the predictive performance of the multi-output regression are challenging tasks. Multioutput Gaussian Process (MOGP) methods offer outstanding solutions with tractable predictions and uncertainty quantification. However, their practical applications are hindered by high computational complexity and storage demand. Additionally, there exist model mismatches in existing MOGP models when dealing with non-Gaussian data. To improve the model representation ability in terms of flexibility, optimality, and scalability, this paper introduces a novel multi-output regression framework, termed Graphical MOGP (GMOGP), which is empowered by: (i) Generating flexible Gaussian process priors consolidated from dentified parents, (ii) providing dependent processes with attention-based graphical representations, and (iii) achieving Pareto optimal solutions of kernel hyperparameters via a distributed learning framework. Numerical results confirm that the proposed GMOGP significantly outperforms state-of-the-art MOGP alternatives in predictive performance, as well as in time and memory efficiency, across various synthetic and real datasets. | Yijue Dai, Wenzhong Yan, Feng Yin |  |
| 446 |  |  [Soft Contrastive Learning for Time Series](https://openreview.net/forum?id=pAsQSWlDUf) |  | 0 | Contrastive learning has shown to be effective to learn representations from time series in a self-supervised way. However, contrasting similar time series instances or values from adjacent timestamps within a time series leads to ignore their inherent correlations, which results in deteriorating the quality of learned representations. To address this issue, we propose \textit{SoftCLT}, a simple yet effective soft contrastive learning strategy for time series. This is achieved by introducing instance-wise and temporal contrastive loss with soft assignments ranging from zero to one. Specifically, we define soft assignments for 1) instance-wise contrastive loss by distance between time series on the data space, warping and 2) temporal contrastive loss by the difference of timestamps. SoftCLT is a plug-and-play method for time series contrastive learning that improves the quality of learned representations without bells and whistles. In experiments, we demonstrate that SoftCLT consistently improves the performance in various downstream tasks including classification, semi-supervised learning, transfer learning, and anomaly detection, showing state-of-the-art performance. Code is available at this repository: https://github.com/seunghan96/softclt. | Seunghan Lee, Taeyoung Park, Kibok Lee |  |
| 447 |  |  [Enhancing Group Fairness in Online Settings Using Oblique Decision Forests](https://openreview.net/forum?id=E1NxN5QMOE) |  | 0 | Fairness, especially group fairness, is an important consideration in the context of machine learning systems. The most commonly adopted group fairness-enhancing techniques are in-processing methods that rely on a mixture of a fairness objective (e.g., demographic parity) and a task-specific objective (e.g., cross-entropy) during the training process. However, when data arrives in an online fashion – one instance at a time – optimizing such fairness objectives poses several challenges. In particular, group fairness objectives are defined using expectations of predictions across different demographic groups. In the online setting, where the algorithm has access to a single instance at a time, estimating the group fairness objective requires additional storage and significantly more computation (e.g., forward/backward passes) than the task-specific objective at every time step. In this paper, we propose Aranyani, an ensemble of oblique decision trees, to make fair decisions in online settings. The hierarchical tree structure of Aranyani enables parameter isolation and allows us to efficiently compute the fairness gradients using aggregate statistics of previous decisions, eliminating the need for additional storage and forward/backward passes. We also present an efficient framework to train Aranyani and theoretically analyze several of its properties. We conduct empirical evaluations on 5 publicly available benchmarks (including vision and language datasets) to show that Aranyani achieves a better accuracy-fairness trade-off compared to baseline approaches. | Somnath Basu Roy Chowdhury, Nicholas Monath, Ahmad Beirami, Rahul Kidambi, Kumar Avinava Dubey, Amr Ahmed, Snigdha Chaturvedi |  |
| 448 |  |  [Generative Learning for Financial Time Series with Irregular and Scale-Invariant Patterns](https://openreview.net/forum?id=CdjnzWsQax) |  | 0 | Limited data availability poses a major obstacle in training deep learning models for financial applications. Synthesizing financial time series to augment real-world data is challenging due to the irregular and scale-invariant patterns uniquely associated with financial time series - temporal dynamics that repeat with varying duration and magnitude. Such dynamics cannot be captured by existing approaches, which often assume regularity and uniformity in the underlying data. We develop a novel generative framework called FTS-Diffusion to model irregular and scale-invariant patterns that consists of three modules. First, we develop a scale-invariant pattern recognition algorithm to extract recurring patterns that vary in duration and magnitude. Second, we construct a diffusion-based generative network to synthesize segments of patterns. Third, we model the temporal transition of patterns in order to aggregate the generated segments. Extensive experiments show that FTS-Diffusion generates synthetic financial time series highly resembling observed data, outperforming state-of-the-art alternatives. Two downstream experiments demonstrate that augmenting real-world data with synthetic data generated by FTS-Diffusion reduces the error of stock market prediction by up to 17.9%. To the best of our knowledge, this is the first work on generating intricate time series with irregular and scale-invariant patterns, addressing data limitation issues in finance. | Hongbin Huang, Minghua Chen, Xiao Qiao |  |
| 449 |  |  [Multiscale Positive-Unlabeled Detection of AI-Generated Texts](https://openreview.net/forum?id=5Lp6qU9hzV) |  | 0 | Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are astonishing at generating human-like texts, but they may impact the authenticity of texts. Previous works proposed methods to detect these AI-generated texts, including simple ML classifiers, pretrained-model-based zero-shot methods, and finetuned language classification models. However, mainstream detectors always fail on short texts, like SMSes, Tweets, and reviews. In this paper, a Multiscale Positive-Unlabeled (MPU) training framework is proposed to address the difficulty of short-text detection without sacrificing long-texts. Firstly, we acknowledge the human-resemblance property of short machine texts, and rephrase AI text detection as a partial Positive-Unlabeled (PU) problem by regarding these short machine texts as partially "unlabeled". Then in this PU context, we propose the length-sensitive Multiscale PU Loss, where a recurrent model in abstraction is used to estimate positive priors of scale-variant corpora. Additionally, we introduce a Text Multiscaling module to enrich training corpora. Experiments show that our MPU method augments detection performance on long AI-generated texts, and significantly improves short-text detection of language model detectors. Language Models trained with MPU could outcompete existing detectors on various short-text and long-text detection benchmarks. The codes are available at https://github.com/mindspore-lab/mindone/tree/master/examples/detect_chatgpt and https://github.com/YuchuanTian/AIGC_text_detector. | Yuchuan Tian, Hanting Chen, Xutao Wang, Zheyuan Bai, Qinghua Zhang, Ruifeng Li, Chao Xu, Yunhe Wang |  |
| 450 |  |  [A Lightweight Method for Tackling Unknown Participation Statistics in Federated Averaging](https://openreview.net/forum?id=ZKEuFKfCKA) |  | 0 | In federated learning (FL), clients usually have diverse participation statistics that are unknown a priori, which can significantly harm the performance of FL if not handled properly. Existing works aiming at addressing this problem are usually based on global variance reduction, which requires a substantial amount of additional memory in a multiplicative factor equal to the total number of clients. An important open problem is to find a lightweight method for FL in the presence of clients with unknown participation rates. In this paper, we address this problem by adapting the aggregation weights in federated averaging (FedAvg) based on the participation history of each client. We first show that, with heterogeneous participation statistics, FedAvg with non-optimal aggregation weights can diverge from the optimal solution of the original FL objective, indicating the need of finding optimal aggregation weights. However, it is difficult to compute the optimal weights when the participation statistics are unknown. To address this problem, we present a new algorithm called FedAU, which improves FedAvg by adaptively weighting the client updates based on online estimates of the optimal weights without knowing the statistics of client participation. We provide a theoretical convergence analysis of FedAU using a novel methodology to connect the estimation error and convergence. Our theoretical results reveal important and interesting insights, while showing that FedAU converges to an optimal solution of the original objective and has desirable properties such as linear speedup. Our experimental results also verify the advantage of FedAU over baseline methods with various participation patterns. | Shiqiang Wang, Mingyue Ji |  |
| 451 |  |  [Identifying the Risks of LM Agents with an LM-Emulated Sandbox](https://openreview.net/forum?id=GEcwtMk1uA) |  | 0 | Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks—such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, setting up the environment for each test scenario manually, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tail risks. To address these challenges, we introduce ToolEmu: a framework that uses an LM to emulate tool execution and enables scalable testing of LM agents against a diverse range of tools and scenarios. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 68.8% of failures identified with ToolEmu would be valid real-world agent failures. Using our curated initial benchmark consisting of 36 high-stakes toolkits and 144 test cases, we provide a quantitative risk analysis of current LM agents and identify numerous failures with potentially severe outcomes. Notably, even the safest LM agent exhibits such failures 23.9% of the time according to our evaluator, underscoring the need to develop safer LM agents for real-world deployment. | Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris J. Maddison, Tatsunori Hashimoto |  |
| 452 |  |  [Coeditor: Leveraging Repo-level Diffs for Code Auto-editing](https://openreview.net/forum?id=ALVwQjZRS8) |  | 0 | Developers often dedicate significant time to maintaining and refactoring existing code. However, most prior work on generative models for code focuses solely on creating new code, overlooking the distinctive needs of editing existing code. In this work, we explore a multi-round code auto-editing setting, aiming to predict edits to a code region based on recent changes within the same codebase. Our model, Coeditor, is a fine-tuned language model specifically designed for code editing tasks. We represent code changes using a line diff format and employ static analysis to form large customized model contexts, ensuring the availability of appropriate information for prediction. We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation. In a simplified single-round, single-edit task, Coeditor significantly outperforms GPT-3.5 and SOTA open-source code completion models (bringing exact-match accuracy from 34.7 up to 60.4), demonstrating the benefits of incorporating editing history for code completion. In a multi-round, multi-edit setting, we observe substantial gains by iteratively conditioning on additional user edits. We have open-sourced our code, data, and model weights to encourage future research and have released a VSCode extension powered by our model for interactive IDE usage. | Jiayi Wei, Greg Durrett, Isil Dillig |  |
| 453 |  |  [FITS: Modeling Time Series with 10k Parameters](https://openreview.net/forum?id=bWcnvZ3qMb) |  | 0 | In this paper, we introduce FITS, a lightweight yet powerful model for time series analysis. Unlike existing models that directly process raw time-domain data, FITS operates on the principle that time series can be manipulated through interpolation in the complex frequency domain, achieving performance comparable to state-of-the-art models for time series forecasting and anomaly detection tasks. Notably, FITS accomplishes this with a svelte profile of just about $10k$ parameters, making it ideally suited for edge devices and paving the way for a wide range of applications. The code is available for review at: \url{https://anonymous.4open.science/r/FITS}. | Zhijian Xu, Ailing Zeng, Qiang Xu |  |
| 454 |  |  [MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models](https://openreview.net/forum?id=N8N0hgNDRt) |  | 0 |  | Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu |  |
| 455 |  |  [Query-Policy Misalignment in Preference-Based Reinforcement Learning](https://openreview.net/forum?id=UoBymIwPJR) |  | 0 |  | Xiao Hu, Jianxiong Li, Xianyuan Zhan, QingShan Jia, YaQin Zhang |  |
| 456 |  |  [Feature-aligned N-BEATS with Sinkhorn divergence](https://openreview.net/forum?id=TS8HoIWAPQ) |  | 0 |  | Joonhun Lee, Myeongho Jeon, Myungjoo Kang, Kyunghyun Park |  |
| 457 |  |  [Instructive Decoding: Instruction-Tuned Large Language Models are Self-Refiner from Noisy Instructions](https://openreview.net/forum?id=LebzzClHYw) |  | 0 |  | Taehyeon Kim, Joonkee Kim, Gihun Lee, SeYoung Yun |  |
| 458 |  |  [Consistent Multi-Class Classification from Multiple Unlabeled Datasets](https://openreview.net/forum?id=fW7DOHDQvF) |  | 0 |  | Zixi Wei, Senlin Shu, Yuzhou Cao, Hongxin Wei, Bo An, Lei Feng |  |
| 459 |  |  [SpikePoint: An Efficient Point-based Spiking Neural Network for Event Cameras Action Recognition](https://openreview.net/forum?id=7etoNfU9uF) |  | 0 |  | Hongwei Ren, Yue Zhou, Xiaopeng Lin, Yulong Huang, Haotian Fu, Jie Song, Bojun Cheng |  |
| 460 |  |  [Inverse Approximation Theory for Nonlinear Recurrent Neural Networks](https://openreview.net/forum?id=yC2waD70Vj) |  | 0 |  | Shida Wang, Zhong Li, Qianxiao Li |  |
| 461 |  |  [Kernel Metric Learning for In-Sample Off-Policy Evaluation of Deterministic RL Policies](https://openreview.net/forum?id=plebgsdiiV) |  | 0 |  | Haanvid Lee, Tri Wahyu Guntara, Jongmin Lee, YungKyun Noh, KeeEung Kim |  |
| 462 |  |  [Large Language Models are Efficient Learners of Noise-Robust Speech Recognition](https://openreview.net/forum?id=ceATjGPTUD) |  | 0 |  | Yuchen Hu, Chen Chen, ChaoHan Huck Yang, Ruizhe Li, Chao Zhang, PinYu Chen, Engsiong Chng |  |
| 463 |  |  [H2O-SDF: Two-phase Learning for 3D Indoor Reconstruction using Object Surface Fields](https://openreview.net/forum?id=P1ANzoGg3W) |  | 0 |  | Minyoung Park, Mirae Do, YeonJae Shin, Jaeseok Yoo, Jongkwang Hong, Joongrock Kim, Chul Lee |  |
| 464 |  |  [Sample-Efficient Quality-Diversity by Cooperative Coevolution](https://openreview.net/forum?id=JDud6zbpFv) |  | 0 |  | Ke Xue, RenJian Wang, Pengyi Li, Dong Li, Jianye Hao, Chao Qian |  |
| 465 |  |  [SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore](https://openreview.net/forum?id=ruk0nyQPec) |  | 0 |  | Sewon Min, Suchin Gururangan, Eric Wallace, Weijia Shi, Hannaneh Hajishirzi, Noah A. Smith, Luke Zettlemoyer |  |
| 466 |  |  [Dynamic Discounted Counterfactual Regret Minimization](https://openreview.net/forum?id=6PbvbLyqT6) |  | 0 |  | Hang Xu, Kai Li, Haobo Fu, Qiang Fu, Junliang Xing, Jian Cheng |  |
| 467 |  |  [GIO: Gradient Information Optimization for Training Dataset Selection](https://openreview.net/forum?id=3NnfJnbJT2) |  | 0 |  | Dante Everaert, Christopher Potts |  |
| 468 |  |  [SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training](https://openreview.net/forum?id=KZSEgJGPxu) |  | 0 |  | Kazem Meidani, Parshin Shojaee, Chandan K. Reddy, Amir Barati Farimani |  |
| 469 |  |  [Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model](https://openreview.net/forum?id=m50eKHCttz) |  | 0 |  | Karsten Roth, Lukas Thede, A. Sophia Koepke, Oriol Vinyals, Olivier J. Hénaff, Zeynep Akata |  |
| 470 |  |  [Robustifying State-space Models for Long Sequences via Approximate Diagonalization](https://openreview.net/forum?id=DjeQ39QoLQ) |  | 0 |  | Annan Yu, Arnur Nigmetov, Dmitriy Morozov, Michael W. Mahoney, N. Benjamin Erichson |  |
| 471 |  |  [Provable Offline Preference-Based Reinforcement Learning](https://openreview.net/forum?id=tVMPfEGT2w) |  | 0 |  | Wenhao Zhan, Masatoshi Uehara, Nathan Kallus, Jason D. Lee, Wen Sun |  |
| 472 |  |  [Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory](https://openreview.net/forum?id=gmg7t8b4s0) |  | 0 |  | Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, Yejin Choi |  |
| 473 |  |  [Provable Reward-Agnostic Preference-Based Reinforcement Learning](https://openreview.net/forum?id=yTBXeXdbMf) |  | 0 |  | Wenhao Zhan, Masatoshi Uehara, Wen Sun, Jason D. Lee |  |
| 474 |  |  [Unleashing the Potential of Fractional Calculus in Graph Neural Networks with FROND](https://openreview.net/forum?id=wcka3bd7P4) |  | 0 |  | Qiyu Kang, Kai Zhao, Qinxu Ding, Feng Ji, Xuhao Li, Wenfei Liang, Yang Song, Wee Peng Tay |  |
| 475 |  |  [MetaPhysiCa: Improving OOD Robustness in Physics-informed Machine Learning](https://openreview.net/forum?id=KrWuDiW4Qm) |  | 0 |  | S. Chandra Mouli, Muhammad Ashraful Alam, Bruno Ribeiro |  |
| 476 |  |  [Views Can Be Deceiving: Improved SSL Through Feature Space Augmentation](https://openreview.net/forum?id=mutJBk3ILg) |  | 0 |  | Kimia Hamidieh, Haoran Zhang, Swami Sankaranarayanan, Marzyeh Ghassemi |  |
| 477 |  |  [Project and Probe: Sample-Efficient Adaptation by Interpolating Orthogonal Features](https://openreview.net/forum?id=f6CBQYxXvr) |  | 0 |  | Annie S. Chen, Yoonho Lee, Amrith Setlur, Sergey Levine, Chelsea Finn |  |
| 478 |  |  [Implicit bias of SGD in L2-regularized linear DNNs: One-way jumps from high to low rank](https://openreview.net/forum?id=P1aobHnjjj) |  | 0 |  | Zihan Wang, Arthur Jacot |  |
| 479 |  |  [Whole-Song Hierarchical Generation of Symbolic Music Using Cascaded Diffusion Models](https://openreview.net/forum?id=sn7CYWyavh) |  | 0 |  | Ziyu Wang, Lejun Min, Gus Xia |  |
| 480 |  |  [Evaluating the Zero-shot Robustness of Instruction-tuned Language Models](https://openreview.net/forum?id=g9diuvxN6D) |  | 0 |  | Jiuding Sun, Chantal Shaib, Byron C. Wallace |  |
| 481 |  |  [Critical Learning Periods Emerge Even in Deep Linear Networks](https://openreview.net/forum?id=Aq35gl2c1k) |  | 0 |  | Michael Kleinman, Alessandro Achille, Stefano Soatto |  |
| 482 |  |  [MOTOR: A Time-to-Event Foundation Model For Structured Medical Records](https://openreview.net/forum?id=NialiwI2V6) |  | 0 |  | Ethan Steinberg, Jason Alan Fries, Yizhe Xu, Nigam Shah |  |
| 483 |  |  [GenSim: Generating Robotic Simulation Tasks via Large Language Models](https://openreview.net/forum?id=OI3RoHoWAN) |  | 0 |  | Lirui Wang, Yiyang Ling, Zhecheng Yuan, Mohit Shridhar, Chen Bao, Yuzhe Qin, Bailin Wang, Huazhe Xu, Xiaolong Wang |  |
| 484 |  |  [Understanding Augmentation-based Self-Supervised Representation Learning via RKHS Approximation and Regression](https://openreview.net/forum?id=Ax2yRhCQr1) |  | 0 |  | Runtian Zhai, Bingbin Liu, Andrej Risteski, J. Zico Kolter, Pradeep Kumar Ravikumar |  |
| 485 |  |  [Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs](https://openreview.net/forum?id=MO5PiKHELW) |  | 0 |  | Angelica Chen, Ravid ShwartzZiv, Kyunghyun Cho, Matthew L. Leavitt, Naomi Saphra |  |
| 486 |  |  [SE(3)-Stochastic Flow Matching for Protein Backbone Generation](https://openreview.net/forum?id=kJFIH23hXb) |  | 0 |  | Avishek Joey Bose, Tara AkhoundSadegh, Guillaume Huguet, Kilian Fatras, Jarrid RectorBrooks, ChengHao Liu, Andrei Cristian Nica, Maksym Korablyov, Michael M. Bronstein, Alexander Tong |  |
| 487 |  |  [DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer](https://openreview.net/forum?id=Ifz3IgsEPX) |  | 0 |  | Junyuan Hong, Jiachen T. Wang, Chenhui Zhang, Zhangheng Li, Bo Li, Zhangyang Wang |  |
| 488 |  |  [Geographic Location Encoding with Spherical Harmonics and Sinusoidal Representation Networks](https://openreview.net/forum?id=PudduufFLa) |  | 0 |  | Marc Rußwurm, Konstantin Klemmer, Esther Rolf, Robin Zbinden, Devis Tuia |  |
| 489 |  |  [A General Framework for User-Guided Bayesian Optimization](https://openreview.net/forum?id=NjU0jtXcYn) |  | 0 |  | Carl Hvarfner, Frank Hutter, Luigi Nardi |  |
| 490 |  |  [Lemur: Harmonizing Natural Language and Code for Language Agents](https://openreview.net/forum?id=hNhwSmtXRh) |  | 0 |  | Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, Zhoujun Cheng, Siheng Zhao, Lingpeng Kong, Bailin Wang, Caiming Xiong, Tao Yu |  |
| 491 |  |  [A path-norm toolkit for modern networks: consequences, promises and challenges](https://openreview.net/forum?id=hiHZVUIYik) |  | 0 |  | Antoine Gonon, Nicolas Brisebarre, Elisa Riccietti, Rémi Gribonval |  |
| 492 |  |  [Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages](https://openreview.net/forum?id=Kuh5qgCGCp) |  | 0 |  | Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue, Dahai Li, Zhiyuan Liu, Maosong Sun |  |
| 493 |  |  [From Sparse to Soft Mixtures of Experts](https://openreview.net/forum?id=jxpsAj7ltE) |  | 0 |  | Joan Puigcerver, Carlos Riquelme Ruiz, Basil Mustafa, Neil Houlsby |  |
| 494 |  |  [Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives](https://openreview.net/forum?id=rxVBKhyfSo) |  | 0 |  | Shrinivas Ramasubramanian, Harsh Rangwani, Sho Takemori, Kunal Samanta, Yuhei Umeda, Venkatesh Babu Radhakrishnan |  |
| 495 |  |  [NoiseDiffusion: Correcting Noise for Image Interpolation with Diffusion Models beyond Spherical Linear Interpolation](https://openreview.net/forum?id=6O3Q6AFUTu) |  | 0 |  | Pengfei Zheng, Yonggang Zhang, Zhen Fang, Tongliang Liu, Defu Lian, Bo Han |  |
| 496 |  |  [SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis](https://openreview.net/forum?id=di52zR8xgf) |  | 0 |  | Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, Robin Rombach |  |
| 497 |  |  [Entity-Centric Reinforcement Learning for Object Manipulation from Pixels](https://openreview.net/forum?id=uDxeSZ1wdI) |  | 0 |  | Dan Haramati, Tal Daniel, Aviv Tamar |  |
| 498 |  |  [Constrained Bi-Level Optimization: Proximal Lagrangian Value Function Approach and Hessian-free Algorithm](https://openreview.net/forum?id=xJ5N8qrEPl) |  | 0 |  | Wei Yao, Chengming Yu, Shangzhi Zeng, Jin Zhang |  |
| 499 |  |  [Inherently Interpretable Time Series Classification via Multiple Instance Learning](https://openreview.net/forum?id=xriGRsoAza) |  | 0 |  | Joseph Early, Gavin K. C. Cheung, Kurt Cutajar, Hanting Xie, Jas Kandola, Niall Twomey |  |
| 500 |  |  [A Mutual Information Perspective on Federated Contrastive Learning](https://openreview.net/forum?id=JrmPG9ufKg) |  | 0 |  | Christos Louizos, Matthias Reisser, Denis Korzhenkov |  |
| 501 |  |  [MMD Graph Kernel: Effective Metric Learning for Graphs via Maximum Mean Discrepancy](https://openreview.net/forum?id=GZ6AcZwA8r) |  | 0 |  | Yan Sun, Jicong Fan |  |
| 502 |  |  [SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem](https://openreview.net/forum?id=HgOJlxzB16) |  | 0 |  | Margalit Glasgow |  |
| 503 |  |  [DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks](https://openreview.net/forum?id=gjfOL9z5Xr) |  | 0 |  | Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, Xing Xie |  |
| 504 |  |  [Illusory Attacks: Information-theoretic detectability matters in adversarial attacks](https://openreview.net/forum?id=F5dhGCdyYh) |  | 0 |  | Tim Franzmeyer, Stephen Marcus McAleer, João F. Henriques, Jakob Nicolaus Foerster, Philip Torr, Adel Bibi, Christian Schröder de Witt |  |
| 505 |  |  [Addressing Signal Delay in Deep Reinforcement Learning](https://openreview.net/forum?id=Z8UfDs4J46) |  | 0 |  | William Wei Wang, Dongqi Han, Xufang Luo, Dongsheng Li |  |
| 506 |  |  [Relay Diffusion: Unifying diffusion process across resolutions for image synthesis](https://openreview.net/forum?id=qTlcbLSm4p) |  | 0 |  | Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, Jie Tang |  |
| 507 |  |  [ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with Diffusion Models](https://openreview.net/forum?id=u48tHG5f66) |  | 0 |  | Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng Chen, Ying Shan |  |
| 508 |  |  [DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization](https://openreview.net/forum?id=MSe8YFbhUE) |  | 0 |  | Guowei Xu, Ruijie Zheng, Yongyuan Liang, Xiyao Wang, Zhecheng Yuan, Tianying Ji, Yu Luo, Xiaoyu Liu, Jiaxin Yuan, Pu Hua, Shuzhen Li, Yanjie Ze, Hal Daumé III, Furong Huang, Huazhe Xu |  |
| 509 |  |  [How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing: The Curses of Symmetry and Initialization](https://openreview.net/forum?id=xGvPKAiOhq) |  | 0 |  | Nuoya Xiong, Lijun Ding, Simon Shaolei Du |  |
| 510 |  |  [AnyText: Multilingual Visual Text Generation and Editing](https://openreview.net/forum?id=ezBH9WE9s2) |  | 0 |  | Yuxiang Tuo, Wangmeng Xiang, JunYan He, Yifeng Geng, Xuansong Xie |  |
| 511 |  |  [At Which Training Stage Does Code Data Help LLMs Reasoning?](https://openreview.net/forum?id=KIPJKST4gw) |  | 0 |  | Yingwei Ma, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, Shanshan Li |  |
| 512 |  |  [Coordinate-Aware Modulation for Neural Fields](https://openreview.net/forum?id=4UiLqimGm5) |  | 0 |  | Joo Chan Lee, Daniel Rho, Seungtae Nam, Jong Hwan Ko, Eunbyung Park |  |
| 513 |  |  [Efficient ConvBN Blocks for Transfer Learning and Beyond](https://openreview.net/forum?id=lHZm9vNm5H) |  | 0 |  | Kaichao You, Guo Qin, Anchang Bao, Meng Cao, Ping Huang, Jiulong Shan, Mingsheng Long |  |
| 514 |  |  [Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior](https://openreview.net/forum?id=TrKq4Wlwcz) |  | 0 |  | Ashmit Khandelwal, Aditya Agrawal, Aanisha Bhattacharyya, Yaman Kumar, Somesh Singh, Uttaran Bhattacharya, Ishita Dasgupta, Stefano Petrangeli, Rajiv Ratn Shah, Changyou Chen, Balaji Krishnamurthy |  |
| 515 |  |  [Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints](https://openreview.net/forum?id=2cRzmWXK9N) |  | 0 |  | Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, Yuxin Chen |  |
| 516 |  |  [FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets](https://openreview.net/forum?id=CYmF38ysDa) |  | 0 |  | Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, Minjoon Seo |  |
| 517 |  |  [LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset](https://openreview.net/forum?id=BOfDKxfwt0) |  | 0 |  | Lianmin Zheng, WeiLin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica, Hao Zhang |  |
| 518 |  |  [EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models](https://openreview.net/forum?id=UmMa3UNDAz) |  | 0 |  | Yefei He, Jing Liu, Weijia Wu, Hong Zhou, Bohan Zhuang |  |
| 519 |  |  [BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models](https://openreview.net/forum?id=3TO3TtnOFl) |  | 0 |  | Qingqing Cao, Sewon Min, Yizhong Wang, Hannaneh Hajishirzi |  |
| 520 |  |  [Frozen Transformers in Language Models Are Effective Visual Encoder Layers](https://openreview.net/forum?id=t0FI3Q66K5) |  | 0 |  | Ziqi Pang, Ziyang Xie, Yunze Man, YuXiong Wang |  |
| 521 |  |  [SocioDojo: Building Lifelong Analytical Agents with Real-world Text and Time Series](https://openreview.net/forum?id=s9z0HzWJJp) |  | 0 |  | Junyan Cheng, Peter Chin |  |
| 522 |  |  [Learning Performance-Improving Code Edits](https://openreview.net/forum?id=ix7rLVHXyY) |  | 0 |  | Alexander Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob R. Gardner, Yiming Yang, Milad Hashemi, Graham Neubig, Parthasarathy Ranganathan, Osbert Bastani, Amir Yazdanbakhsh |  |
| 523 |  |  [Quasi-Monte Carlo for 3D Sliced Wasserstein](https://openreview.net/forum?id=Wd47f7HEXg) |  | 0 |  | Khai Nguyen, Nicola Bariletto, Nhat Ho |  |
| 524 |  |  [A Poincaré Inequality and Consistency Results for Signal Sampling on Large Graphs](https://openreview.net/forum?id=l3qtSNsPvC) |  | 0 |  | Thien Le, Luana Ruiz, Stefanie Jegelka |  |
| 525 |  |  [Cascading Reinforcement Learning](https://openreview.net/forum?id=KjOAHlKMF5) |  | 0 |  | Yihan Du, R. Srikant, Wei Chen |  |
| 526 |  |  [Complex priors and flexible inference in recurrent circuits with dendritic nonlinearities](https://openreview.net/forum?id=S5aUhpuyap) |  | 0 |  | Benjamin Lyo, Cristina Savin |  |
| 527 |  |  [On the hardness of learning under symmetries](https://openreview.net/forum?id=ARPrtuzAnQ) |  | 0 |  | Bobak T. Kiani, Thien Le, Hannah Lawrence, Stefanie Jegelka, Melanie Weber |  |
| 528 |  |  [An Image Is Worth 1000 Lies: Transferability of Adversarial Images across Prompts on Vision-Language Models](https://openreview.net/forum?id=nc5GgFAvtk) |  | 0 |  | Haochen Luo, Jindong Gu, Fengyuan Liu, Philip Torr |  |
| 529 |  |  [One For All: Towards Training One Graph Model For All Classification Tasks](https://openreview.net/forum?id=4IT2pgc9v6) |  | 0 |  | Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen, Muhan Zhang |  |
| 530 |  |  [NAISR: A 3D Neural Additive Model for Interpretable Shape Representation](https://openreview.net/forum?id=wg8NPfeMF9) |  | 0 |  | Yining Jiao, Carlton J. Zdanski, Julia S. Kimbell, Andrew Prince, Cameron Worden, Samuel Kirse, Christopher Rutter, Benjamin Shields, William Dunn, Jisan Mahmud, Marc Niethammer |  |
| 531 |  |  [Feature emergence via margin maximization: case studies in algebraic tasks](https://openreview.net/forum?id=i9wDX850jR) |  | 0 |  | Depen Morwani, Benjamin L. Edelman, CostinAndrei Oncescu, Rosie Zhao, Sham M. Kakade |  |
| 532 |  |  [On the Stability of Iterative Retraining of Generative Models on their own Data](https://openreview.net/forum?id=JORAfH2xFd) |  | 0 |  | Quentin Bertrand, Avishek Joey Bose, Alexandre Duplessis, Marco Jiralerspong, Gauthier Gidel |  |
| 533 |  |  [Intriguing Properties of Generative Classifiers](https://openreview.net/forum?id=rmg0qMKYRQ) |  | 0 |  | Priyank Jaini, Kevin Clark, Robert Geirhos |  |
| 534 |  |  [Fast Imitation via Behavior Foundation Models](https://openreview.net/forum?id=qnWtw3l0jb) |  | 0 |  | Matteo Pirotta, Andrea Tirinzoni, Ahmed Touati, Alessandro Lazaric, Yann Ollivier |  |
| 535 |  |  [Task Adaptation from Skills: Information Geometry, Disentanglement, and New Objectives for Unsupervised Reinforcement Learning](https://openreview.net/forum?id=zSxpnKh1yS) |  | 0 |  | Yucheng Yang, Tianyi Zhou, Qiang He, Lei Han, Mykola Pechenizkiy, Meng Fang |  |
| 536 |  |  [NuwaDynamics: Discovering and Updating in Causal Spatio-Temporal Modeling](https://openreview.net/forum?id=sLdVl0q68X) |  | 0 |  | Kun Wang, Hao Wu, Yifan Duan, Guibin Zhang, Kai Wang, Xiaojiang Peng, Yu Zheng, Yuxuan Liang, Yang Wang |  |
| 537 |  |  [Pre-Training and Fine-Tuning Generative Flow Networks](https://openreview.net/forum?id=ylhiMfpqkm) |  | 0 |  | Ling Pan, Moksh Jain, Kanika Madan, Yoshua Bengio |  |
| 538 |  |  [CO2: Efficient Distributed Training with Full Communication-Computation Overlap](https://openreview.net/forum?id=ZO5cn4IfaN) |  | 0 |  | Weigao Sun, Zhen Qin, Weixuan Sun, Shidi Li, Dong Li, Xuyang Shen, Yu Qiao, Yiran Zhong |  |
| 539 |  |  [CADS: Unleashing the Diversity of Diffusion Models through Condition-Annealed Sampling](https://openreview.net/forum?id=zMoNrajk2X) |  | 0 |  | Seyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, Romann M. Weber |  |
| 540 |  |  [Image Inpainting via Iteratively Decoupled Probabilistic Modeling](https://openreview.net/forum?id=rUf9G9k2im) |  | 0 |  | Wenbo Li, Xin Yu, Kun Zhou, Yibing Song, Zhe Lin |  |
| 541 |  |  [Image2Sentence based Asymmetrical Zero-shot Composed Image Retrieval](https://openreview.net/forum?id=5BXAXOpaWu) |  | 0 |  | Yongchao Du, Min Wang, Wengang Zhou, Shuping Hui, Houqiang Li |  |
| 542 |  |  [Bespoke Solvers for Generative Flow Models](https://openreview.net/forum?id=1PXEY7ofFX) |  | 0 |  | Neta Shaul, Juan C. Pérez, Ricky T. Q. Chen, Ali K. Thabet, Albert Pumarola, Yaron Lipman |  |
| 543 |  |  [Leveraging augmented-Lagrangian techniques for differentiating over infeasible quadratic programs in machine learning](https://openreview.net/forum?id=YCPDFfmkFr) |  | 0 |  | Antoine Bambade, Fabian Schramm, Adrien B. Taylor, Justin Carpentier |  |
| 544 |  |  [ODEFormer: Symbolic Regression of Dynamical Systems with Transformers](https://openreview.net/forum?id=TzoHLiGVMo) |  | 0 |  | Stéphane d'Ascoli, Sören Becker, Philippe Schwaller, Alexander Mathis, Niki Kilbertus |  |
| 545 |  |  [Convergence of Bayesian Bilevel Optimization](https://openreview.net/forum?id=fLXpXa7iiz) |  | 0 |  | Shi Fu, Fengxiang He, Xinmei Tian, Dacheng Tao |  |
| 546 |  |  [MovingParts: Motion-based 3D Part Discovery in Dynamic Radiance Field](https://openreview.net/forum?id=QQ6RgKYiQq) |  | 0 |  | Kaizhi Yang, Xiaoshuai Zhang, Zhiao Huang, Xuejin Chen, Zexiang Xu, Hao Su |  |
| 547 |  |  [Equivariant Matrix Function Neural Networks](https://openreview.net/forum?id=yrgQdA5NkI) |  | 0 |  | Ilyes Batatia, Lars L. Schaaf, Gábor Csányi, Christoph Ortner, Felix A. Faber |  |
| 548 |  |  [Multi-resolution HuBERT: Multi-resolution Speech Self-Supervised Learning with Masked Unit Prediction](https://openreview.net/forum?id=kUuKFW7DIF) |  | 0 |  | Jiatong Shi, Hirofumi Inaguma, Xutai Ma, Ilia Kulikov, Anna Y. Sun |  |
| 549 |  |  [Input-gradient space particle inference for neural network ensembles](https://openreview.net/forum?id=nLWiR5P3wr) |  | 0 |  | Trung Q. Trinh, Markus Heinonen, Luigi Acerbi, Samuel Kaski |  |
| 550 |  |  [Prototypical Information Bottlenecking and Disentangling for Multimodal Cancer Survival Prediction](https://openreview.net/forum?id=otHZ8JAIgh) |  | 0 |  | Yilan Zhang, Yingxue Xu, Jianqi Chen, Fengying Xie, Hao Chen |  |
| 551 |  |  [MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data](https://openreview.net/forum?id=8xliOUg9EW) |  | 0 |  | Yinya Huang, Xiaohan Lin, Zhengying Liu, Qingxing Cao, Huajian Xin, Haiming Wang, Zhenguo Li, Linqi Song, Xiaodan Liang |  |
| 552 |  |  [FLD: Fourier Latent Dynamics for Structured Motion Representation and Learning](https://openreview.net/forum?id=xsd2llWYSA) |  | 0 |  | Chenhao Li, Elijah StangerJones, Steve Heim, Sangbae Kim |  |
| 553 |  |  [Towards Reliable and Efficient Backdoor Trigger Inversion via Decoupling Benign Features](https://openreview.net/forum?id=Tw9wemV6cb) |  | 0 |  | Xiong Xu, Kunzhe Huang, Yiming Li, Zhan Qin, Kui Ren |  |
| 554 |  |  [Long-Term Typhoon Trajectory Prediction: A Physics-Conditioned Approach Without Reanalysis Data](https://openreview.net/forum?id=ziDFH8TPPK) |  | 0 |  | YoungJae Park, Minseok Seo, Doyi Kim, Hyeri Kim, Sanghoon Choi, Beomkyu Choi, Jeongwon Ryu, Sohee Son, HaeGon Jeon, Yeji Choi |  |
| 555 |  |  [Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery Detection](https://openreview.net/forum?id=8iTpB4RNvP) |  | 0 |  | Jiawei Liang, Siyuan Liang, Aishan Liu, Xiaojun Jia, Junhao Kuang, Xiaochun Cao |  |
| 556 |  |  [Unified Human-Scene Interaction via Prompted Chain-of-Contacts](https://openreview.net/forum?id=1vCnDyQkjg) |  | 0 |  | Zeqi Xiao, Tai Wang, Jingbo Wang, Jinkun Cao, Wenwei Zhang, Bo Dai, Dahua Lin, Jiangmiao Pang |  |
| 557 |  |  [PTaRL: Prototype-based Tabular Representation Learning via Space Calibration](https://openreview.net/forum?id=G32oY4Vnm8) |  | 0 |  | Hangting Ye, Wei Fan, Xiaozhuang Song, Shun Zheng, He Zhao, Dandan Guo, Yi Chang |  |
| 558 |  |  [Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data](https://openreview.net/forum?id=4VIgNuQ1pY) |  | 0 |  | YongKyung Oh, Dongyoung Lim, Sungil Kim |  |
| 559 |  |  [What does the Knowledge Neuron Thesis Have to do with Knowledge?](https://openreview.net/forum?id=2HJRwwbV3G) |  | 0 |  | Jingcheng Niu, Andrew Liu, Zining Zhu, Gerald Penn |  |
| 560 |  |  [Point2SSM: Learning Morphological Variations of Anatomies from Point Clouds](https://openreview.net/forum?id=DqziS8DG4M) |  | 0 |  | Jadie Adams, Shireen Y. Elhabian |  |
| 561 |  |  [Improving Domain Generalization with Domain Relations](https://openreview.net/forum?id=Dc4rXq3HIA) |  | 0 |  | Huaxiu Yao, Xinyu Yang, Xinyi Pan, Shengchao Liu, Pang Wei Koh, Chelsea Finn |  |
| 562 |  |  [Generating Images with 3D Annotations Using Diffusion Models](https://openreview.net/forum?id=XlkN11Xj6J) |  | 0 |  | Wufei Ma, Qihao Liu, Jiahao Wang, Angtian Wang, Xiaoding Yuan, Yi Zhang, Zihao Xiao, Guofeng Zhang, Beijia Lu, Ruxiao Duan, Yongrui Qi, Adam Kortylewski, Yaoyao Liu, Alan L. Yuille |  |
| 563 |  |  [High-dimensional SGD aligns with emerging outlier eigenspaces](https://openreview.net/forum?id=MHjigVnI04) |  | 0 |  | Gérard Ben Arous, Reza Gheissari, Jiaoyang Huang, Aukosh Jagannath |  |
| 564 |  |  [B-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis](https://openreview.net/forum?id=fLf589bx1f) |  | 0 |  | Zishun Yu, Yunzhe Tao, Liyu Chen, Tao Sun, Hongxia Yang |  |
| 565 |  |  [Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts](https://openreview.net/forum?id=auKAUJZMO6) |  | 0 |  | Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, Yu Su |  |
| 566 |  |  [A Hierarchical Bayesian Model for Few-Shot Meta Learning](https://openreview.net/forum?id=mQ72XRfYRZ) |  | 0 |  | Minyoung Kim, Timothy M. Hospedales |  |
| 567 |  |  [Faithful and Efficient Explanations for Neural Networks via Neural Tangent Kernel Surrogate Models](https://openreview.net/forum?id=yKksu38BpM) |  | 0 |  | Andrew Engel, Zhichao Wang, Natalie Frank, Ioana Dumitriu, Sutanay Choudhury, Anand D. Sarwate, Tony Chiang |  |
| 568 |  |  [Conformal Risk Control](https://openreview.net/forum?id=33XGfHLtZg) |  | 0 |  | Anastasios Nikolas Angelopoulos, Stephen Bates, Adam Fisch, Lihua Lei, Tal Schuster |  |
| 569 |  |  [RetroBridge: Modeling Retrosynthesis with Markov Bridges](https://openreview.net/forum?id=770DetV8He) |  | 0 |  | Ilia Igashov, Arne Schneuing, Marwin H. S. Segler, Michael M. Bronstein, Bruno E. Correia |  |
| 570 |  |  [InstructScene: Instruction-Driven 3D Indoor Scene Synthesis with Semantic Graph Prior](https://openreview.net/forum?id=LtuRgL03pI) |  | 0 |  | Chenguo Lin, Yadong Mu |  |
| 571 |  |  [Single Motion Diffusion](https://openreview.net/forum?id=DrhZneqz4n) |  | 0 |  | Sigal Raab, Inbal Leibovitch, Guy Tevet, Moab Arar, Amit Haim Bermano, Daniel CohenOr |  |
| 572 |  |  [Physics-Regulated Deep Reinforcement Learning: Invariant Embeddings](https://openreview.net/forum?id=5Dwqu5urzs) |  | 0 |  | Hongpeng Cao, Yanbing Mao, Lui Sha, Marco Caccamo |  |
| 573 |  |  [BatteryML: An Open-source Platform for Machine Learning on Battery Degradation](https://openreview.net/forum?id=sxGugrYhP9) |  | 0 |  | Han Zhang, Xiaofan Gui, Shun Zheng, Ziheng Lu, Yuqi Li, Jiang Bian |  |
| 574 |  |  [SaProt: Protein Language Modeling with Structure-aware Vocabulary](https://openreview.net/forum?id=6MRm3G4NiU) |  | 0 |  | Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan |  |
| 575 |  |  [PixArt-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis](https://openreview.net/forum?id=eAKmQPe3m1) |  | 0 |  | Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Zhongdao Wang, James T. Kwok, Ping Luo, Huchuan Lu, Zhenguo Li |  |
| 576 |  |  [Sentence-level Prompts Benefit Composed Image Retrieval](https://openreview.net/forum?id=m3ch3kJL7q) |  | 0 |  | Yang Bai, Xinxing Xu, Yong Liu, Salman Khan, Fahad Khan, Wangmeng Zuo, Rick Siow Mong Goh, ChunMei Feng |  |
| 577 |  |  [Compositional Generative Inverse Design](https://openreview.net/forum?id=wmX0CqFSd7) |  | 0 |  | Tailin Wu, Takashi Maruyama, Long Wei, Tao Zhang, Yilun Du, Gianluca Iaccarino, Jure Leskovec |  |
| 578 |  |  [What does automatic differentiation compute for neural networks?](https://openreview.net/forum?id=8vKknbgXxf) |  | 0 |  | Sejun Park, Sanghyuk Chun, Wonyeol Lee |  |
| 579 |  |  [OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models](https://openreview.net/forum?id=8Wuvhh0LYW) |  | 0 |  | Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, Ping Luo |  |
| 580 |  |  [Ferret: Refer and Ground Anything Anywhere at Any Granularity](https://openreview.net/forum?id=2msbbX3ydD) |  | 0 |  | Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, ShihFu Chang, Yinfei Yang |  |
| 581 |  |  [SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation](https://openreview.net/forum?id=gn0mIhQGNM) |  | 0 |  | Chongyu Fan, Jiancheng Liu, Yihua Zhang, Eric Wong, Dennis Wei, Sijia Liu |  |
| 582 |  |  [Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization](https://openreview.net/forum?id=KOZu91CzbK) |  | 0 |  | Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh R. N., Zeyuan Chen, Jianguo Zhang, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, Silvio Savarese |  |
| 583 |  |  [BECLR: Batch Enhanced Contrastive Few-Shot Learning](https://openreview.net/forum?id=k9SVcrmXL8) |  | 0 |  | Stylianos PoulakakisDaktylidis, Hadi Jamali Rad |  |
| 584 |  |  [How to Capture Higher-order Correlations? Generalizing Matrix Softmax Attention to Kronecker Computation](https://openreview.net/forum?id=v0zNCwwkaV) |  | 0 |  | Josh Alman, Zhao Song |  |
| 585 |  |  [DreamLLM: Synergistic Multimodal Comprehension and Creation](https://openreview.net/forum?id=y01KGvd9Bw) |  | 0 |  | Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, Li Yi |  |
| 586 |  |  [Learning to Act from Actionless Videos through Dense Correspondences](https://openreview.net/forum?id=Mhb5fpA1T0) |  | 0 |  | PoChen Ko, Jiayuan Mao, Yilun Du, ShaoHua Sun, Joshua B. Tenenbaum |  |
| 587 |  |  [On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation](https://openreview.net/forum?id=CvYBvgEUK9) |  | 0 |  | Jeongyeol Kwon, Dohyun Kwon, Stephen Wright, Robert D. Nowak |  |
| 588 |  |  [Scaling Laws for Sparsely-Connected Foundation Models](https://openreview.net/forum?id=i9K2ZWkYIP) |  | 0 |  | Elias Frantar, Carlos Riquelme Ruiz, Neil Houlsby, Dan Alistarh, Utku Evci |  |
| 589 |  |  [Nearly d-Linear Convergence Bounds for Diffusion Models via Stochastic Localization](https://openreview.net/forum?id=r5njV3BsuD) |  | 0 |  | Joe Benton, Valentin De Bortoli, Arnaud Doucet, George Deligiannidis |  |
| 590 |  |  [DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models](https://openreview.net/forum?id=OEL4FJMg1b) |  | 0 |  | Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, Jian Zhang |  |
| 591 |  |  [Uni3D: Exploring Unified 3D Representation at Scale](https://openreview.net/forum?id=wcaE4Dfgt8) |  | 0 |  | Junsheng Zhou, Jinsheng Wang, Baorui Ma, YuShen Liu, Tiejun Huang, Xinlong Wang |  |
| 592 |  |  [CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents](https://openreview.net/forum?id=UBVNwD3hPN) |  | 0 |  | Siyuan Qi, Shuo Chen, Yexin Li, Xiangyu Kong, Junqi Wang, Bangcheng Yang, Pring Wong, Yifan Zhong, Xiaoyuan Zhang, Zhaowei Zhang, Nian Liu, Yaodong Yang, SongChun Zhu |  |
| 593 |  |  [Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy](https://openreview.net/forum?id=EXitynZhYn) |  | 0 |  | Simon Ging, María Alejandra Bravo, Thomas Brox |  |
| 594 |  |  [GIM: Learning Generalizable Image Matcher From Internet Videos](https://openreview.net/forum?id=NYN1b8GRGS) |  | 0 |  | Xuelun Shen, Zhipeng Cai, Wei Yin, Matthias Müller, Zijun Li, Kaixuan Wang, Xiaozhi Chen, Cheng Wang |  |
| 595 |  |  [SyncDreamer: Generating Multiview-consistent Images from a Single-view Image](https://openreview.net/forum?id=MN3yH2ovHb) |  | 0 |  | Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, Wenping Wang |  |
| 596 |  |  [Finite-State Autoregressive Entropy Coding for Efficient Learned Lossless Compression](https://openreview.net/forum?id=D5mJSNtUtv) |  | 0 |  | Yufeng Zhang, Hang Yu, Jianguo Li, Weiyao Lin |  |
| 597 |  |  [ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs](https://openreview.net/forum?id=dHng2O0Jjr) |  | 0 |  | Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, Maosong Sun |  |
| 598 |  |  [Enhanced Face Recognition using Intra-class Incoherence Constraint](https://openreview.net/forum?id=uELjxVbrqG) |  | 0 |  | Yuanqing Huang, Yinggui Wang, Le Yang, Lei Wang |  |
| 599 |  |  [Entropy is not Enough for Test-Time Adaptation: From the Perspective of Disentangled Factors](https://openreview.net/forum?id=9w3iw8wDuE) |  | 0 |  | Jonghyun Lee, Dahuin Jung, Saehyung Lee, Junsung Park, Juhyeon Shin, Uiwon Hwang, Sungroh Yoon |  |
| 600 |  |  [SEAL: A Framework for Systematic Evaluation of Real-World Super-Resolution](https://openreview.net/forum?id=CGlczSBBSj) |  | 0 |  | Wenlong Zhang, Xiaohui Li, Xiangyu Chen, Xiaoyun Zhang, Yu Qiao, XiaoMing Wu, Chao Dong |  |
| 601 |  |  [Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood](https://openreview.net/forum?id=AyzkDpuqcl) |  | 0 |  | Yaxuan Zhu, Jianwen Xie, Ying Nian Wu, Ruiqi Gao |  |
| 602 |  |  [MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning](https://openreview.net/forum?id=yLClGs770I) |  | 0 |  | Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen |  |
| 603 |  |  [Time Travel in LLMs: Tracing Data Contamination in Large Language Models](https://openreview.net/forum?id=2Rwq6c3tvr) |  | 0 |  | Shahriar Golchin, Mihai Surdeanu |  |
| 604 |  |  [Variational Inference for SDEs Driven by Fractional Noise](https://openreview.net/forum?id=rtx8B94JMS) |  | 0 |  | Rembert Daems, Manfred Opper, Guillaume Crevecoeur, Tolga Birdal |  |
| 605 |  |  [Implicit regularization of deep residual networks towards neural ODEs](https://openreview.net/forum?id=AbXGwqb5Ht) |  | 0 |  | Pierre Marion, YuHan Wu, Michael Eli Sander, Gérard Biau |  |
| 606 |  |  [NetInfoF Framework: Measuring and Exploiting Network Usable Information](https://openreview.net/forum?id=KY8ZNcljVU) |  | 0 |  | MengChieh Lee, Haiyang Yu, Jian Zhang, Vassilis N. Ioannidis, Xiang Song, Soji Adeshina, Da Zheng, Christos Faloutsos |  |
| 607 |  |  [BarLeRIa: An Efficient Tuning Framework for Referring Image Segmentation](https://openreview.net/forum?id=wHLDHRkmEu) |  | 0 |  | Yaoming Wang, Jin Li, Xiaopeng Zhang, Bowen Shi, Chenglin Li, Wenrui Dai, Hongkai Xiong, Qi Tian |  |
| 608 |  |  [Local Search GFlowNets](https://openreview.net/forum?id=6cFcw1Rxww) |  | 0 |  | Minsu Kim, Taeyoung Yun, Emmanuel Bengio, Dinghuai Zhang, Yoshua Bengio, Sungsoo Ahn, Jinkyoo Park |  |
| 609 |  |  [Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt Tensor Products](https://openreview.net/forum?id=mhyQXJ6JsK) |  | 0 |  | Shengjie Luo, Tianlang Chen, Aditi S. Krishnapriyan |  |
| 610 |  |  [Idempotence and Perceptual Image Compression](https://openreview.net/forum?id=Cy5v64DqEF) |  | 0 |  | Tongda Xu, Ziran Zhu, Dailan He, Yanghao Li, Lina Guo, Yuanyuan Wang, Zhe Wang, Hongwei Qin, Yan Wang, Jingjing Liu, YaQin Zhang |  |
| 611 |  |  [Forward χ2 Divergence Based Variational Importance Sampling](https://openreview.net/forum?id=HD5Y7M8Xdk) |  | 0 |  | Chengrui Li, Yule Wang, Weihan Li, Anqi Wu |  |
| 612 |  |  [Noisy Interpolation Learning with Shallow Univariate ReLU Networks](https://openreview.net/forum?id=GTUoTJXPBf) |  | 0 |  | Nirmit Joshi, Gal Vardi, Nathan Srebro |  |
| 613 |  |  [Initializing Models with Larger Ones](https://openreview.net/forum?id=dyrGMhicMw) |  | 0 |  | Zhiqiu Xu, Yanjie Chen, Kirill Vishniakov, Yida Yin, Zhiqiang Shen, Trevor Darrell, Lingjie Liu, Zhuang Liu |  |
| 614 |  |  [DMV3D: Denoising Multi-view Diffusion Using 3D Large Reconstruction Model](https://openreview.net/forum?id=H4yQefeXhp) |  | 0 |  | Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, Kai Zhang |  |
| 615 |  |  [Influencer Backdoor Attack on Semantic Segmentation](https://openreview.net/forum?id=VmGRoNDQgJ) |  | 0 |  | Haoheng Lan, Jindong Gu, Philip Torr, Hengshuang Zhao |  |
| 616 |  |  [PF-LRM: Pose-Free Large Reconstruction Model for Joint Pose and Shape Prediction](https://openreview.net/forum?id=noe76eRcPC) |  | 0 |  | Peng Wang, Hao Tan, Sai Bi, Yinghao Xu, Fujun Luan, Kalyan Sunkavalli, Wenping Wang, Zexiang Xu, Kai Zhang |  |
| 617 |  |  [Procedural Fairness Through Decoupling Objectionable Data Generating Components](https://openreview.net/forum?id=cxfPefbu1s) |  | 0 |  | Zeyu Tang, Jialu Wang, Yang Liu, Peter Spirtes, Kun Zhang |  |
| 618 |  |  [Vision-Language Foundation Models as Effective Robot Imitators](https://openreview.net/forum?id=lFYj0oibGR) |  | 0 |  | Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, Hang Li, Tao Kong |  |
| 619 |  |  [OctoPack: Instruction Tuning Code Large Language Models](https://openreview.net/forum?id=mw1PWNSWZP) |  | 0 |  | Niklas Muennighoff, Qian Liu, Armel Randy Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, Shayne Longpre |  |
| 620 |  |  [Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision](https://openreview.net/forum?id=0V5TVt9bk0) |  | 0 |  | Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, Weisi Lin |  |
| 621 |  |  [iTransformer: Inverted Transformers Are Effective for Time Series Forecasting](https://openreview.net/forum?id=JePfAI8fah) |  | 0 |  | Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, Mingsheng Long |  |
| 622 |  |  [De novo Protein Design Using Geometric Vector Field Networks](https://openreview.net/forum?id=9UIGyJJpay) |  | 0 |  | Weian Mao, Muzhi Zhu, Zheng Sun, Shuaike Shen, Lin Yuanbo Wu, Hao Chen, Chunhua Shen |  |
| 623 |  |  [Prompt Gradient Projection for Continual Learning](https://openreview.net/forum?id=EH2O3h7sBI) |  | 0 |  | Jingyang Qiao, Zhizhong Zhang, Xin Tan, Chengwei Chen, Yanyun Qu, Yong Peng, Yuan Xie |  |
| 624 |  |  [R-EDL: Relaxing Nonessential Settings of Evidential Deep Learning](https://openreview.net/forum?id=Si3YFA641c) |  | 0 |  | Mengyuan Chen, Junyu Gao, Changsheng Xu |  |
| 625 |  |  [Neuron Activation Coverage: Rethinking Out-of-distribution Detection and Generalization](https://openreview.net/forum?id=SNGXbZtK6Q) |  | 0 |  | Yibing Liu, Chris Xing Tian, Haoliang Li, Lei Ma, Shiqi Wang |  |
| 626 |  |  [ResFields: Residual Neural Fields for Spatiotemporal Signals](https://openreview.net/forum?id=EHrvRNs2Y0) |  | 0 |  | Marko Mihajlovic, Sergey Prokudin, Marc Pollefeys, Siyu Tang |  |
| 627 |  |  [TD-MPC2: Scalable, Robust World Models for Continuous Control](https://openreview.net/forum?id=Oxh5CstDJU) |  | 0 |  | Nicklas Hansen, Hao Su, Xiaolong Wang |  |
| 628 |  |  [Stochastic Controlled Averaging for Federated Learning with Communication Compression](https://openreview.net/forum?id=jj5ZjZsWJe) |  | 0 |  | Xinmeng Huang, Ping Li, Xiaoyun Li |  |
| 629 |  |  [AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning](https://openreview.net/forum?id=Fx2SbBgcte) |  | 0 |  | Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, Bo Dai |  |
| 630 |  |  [Guiding Instruction-based Image Editing via Multimodal Large Language Models](https://openreview.net/forum?id=S1RKWSyZ2Y) |  | 0 |  | TsuJui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, Zhe Gan |  |
| 631 |  |  [Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning](https://openreview.net/forum?id=YR3ETaElNK) |  | 0 |  | Bingchen Zhao, Haoqin Tu, Chen Wei, Jieru Mei, Cihang Xie |  |
| 632 |  |  [Universal Humanoid Motion Representations for Physics-Based Control](https://openreview.net/forum?id=OrOd8PxOO2) |  | 0 |  | Zhengyi Luo, Jinkun Cao, Josh Merel, Alexander Winkler, Jing Huang, Kris M. Kitani, Weipeng Xu |  |
| 633 |  |  [Adaptive Rational Activations to Boost Deep Reinforcement Learning](https://openreview.net/forum?id=g90ysX1sVs) |  | 0 |  | Quentin Delfosse, Patrick Schramowski, Martin Mundt, Alejandro Molina, Kristian Kersting |  |
| 634 |  |  [Learning No-Regret Sparse Generalized Linear Models with Varying Observation(s)](https://openreview.net/forum?id=wISvONp3Kq) |  | 0 |  | Diyang Li, Charles Ling, Zhiqiang Xu, Huan Xiong, Bin Gu |  |
| 635 |  |  [Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game](https://openreview.net/forum?id=fsW7wJGLBd) |  | 0 |  | Sam Toyer, Olivia Watkins, Ethan Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, Alan Ritter, Stuart Russell |  |
| 636 |  |  [Leveraging Unpaired Data for Vision-Language Generative Models via Cycle Consistency](https://openreview.net/forum?id=kNjrhD67LP) |  | 0 |  | Tianhong Li, Sangnie Bhardwaj, Yonglong Tian, Han Zhang, Jarred Barber, Dina Katabi, Guillaume Lajoie, Huiwen Chang, Dilip Krishnan |  |
| 637 |  |  [Learning the greatest common divisor: explaining transformer predictions](https://openreview.net/forum?id=cmcD05NPKa) |  | 0 |  | François Charton |  |
| 638 |  |  [Space and time continuous physics simulation from partial observations](https://openreview.net/forum?id=4yaFQ7181M) |  | 0 |  | Steeven Janny, Madiha Nadri, Julie Digne, Christian Wolf |  |
| 639 |  |  [GROOT: Learning to Follow Instructions by Watching Gameplay Videos](https://openreview.net/forum?id=uleDLeiaT3) |  | 0 |  | Shaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji Liu, Yitao Liang |  |
| 640 |  |  [Mask-Based Modeling for Neural Radiance Fields](https://openreview.net/forum?id=SEiuSzlD1d) |  | 0 |  | Ganlin Yang, Guoqiang Wei, Zhizheng Zhang, Yan Lu, Dong Liu |  |
| 641 |  |  [Large Language Models Are Not Robust Multiple Choice Selectors](https://openreview.net/forum?id=shr9PXz7T0) |  | 0 |  | Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, Minlie Huang |  |
| 642 |  |  [Robust Adversarial Reinforcement Learning via Bounded Rationality Curricula](https://openreview.net/forum?id=pFOoOdaiue) |  | 0 |  | Aryaman Reddi, Maximilian Tölle, Jan Peters, Georgia Chalvatzaki, Carlo D'Eramo |  |
| 643 |  |  [Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions](https://openreview.net/forum?id=BXY6fe7q31) |  | 0 |  | Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Wei Ji, Wenqiao Zhang, TatSeng Chua, Siliang Tang, Hanwang Zhang, Yueting Zhuang |  |
| 644 |  |  [CLAP: Collaborative Adaptation for Patchwork Learning](https://openreview.net/forum?id=8EyRkd3Qj2) |  | 0 |  | Sen Cui, Abudukelimu Wuerkaixi, Weishen Pan, Jian Liang, Lei Fang, Changshui Zhang, Fei Wang |  |
| 645 |  |  [Towards Energy Efficient Spiking Neural Networks: An Unstructured Pruning Framework](https://openreview.net/forum?id=eoSeaK4QJo) |  | 0 |  | Xinyu Shi, Jianhao Ding, Zecheng Hao, Zhaofei Yu |  |
| 646 |  |  [Online Stabilization of Spiking Neural Networks](https://openreview.net/forum?id=CIj1CVbkpr) |  | 0 |  | Yaoyu Zhu, Jianhao Ding, Tiejun Huang, Xiaodong Xie, Zhaofei Yu |  |
| 647 |  |  [CrIBo: Self-Supervised Learning via Cross-Image Object-Level Bootstrapping](https://openreview.net/forum?id=3M0GXoUEzP) |  | 0 |  | Tim Lebailly, Thomas Stegmüller, Behzad Bozorgtabar, JeanPhilippe Thiran, Tinne Tuytelaars |  |
| 648 |  |  [Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis](https://openreview.net/forum?id=7ERQPyR2eb) |  | 0 |  | Zhenhui Ye, Tianyun Zhong, Yi Ren, Jiaqi Yang, Weichuang Li, Jiawei Huang, Ziyue Jiang, Jinzheng He, Rongjie Huang, Jinglin Liu, Chen Zhang, Xiang Yin, Zejun Ma, Zhou Zhao |  |
| 649 |  |  [TabR: Tabular Deep Learning Meets Nearest Neighbors](https://openreview.net/forum?id=rhgIgTSSxW) |  | 0 |  | Yury Gorishniy, Ivan Rubachev, Nikolay Kartashev, Daniil Shlenskii, Akim Kotelnikov, Artem Babenko |  |
| 650 |  |  [Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models](https://openreview.net/forum?id=qBL04XXex6) |  | 0 |  | Sijia Chen, Baochun Li, Di Niu |  |
| 651 |  |  [Locality Sensitive Sparse Encoding for Learning World Models Online](https://openreview.net/forum?id=i8PjQT3Uig) |  | 0 |  | Zichen Liu, Chao Du, Wee Sun Lee, Min Lin |  |
| 652 |  |  [Enhancing Neural Subset Selection: Integrating Background Information into Set Representations](https://openreview.net/forum?id=eepoE7iLpL) |  | 0 |  | Binghui Xie, Yatao Bian, Kaiwen Zhou, Yongqiang Chen, Peilin Zhao, Bo Han, Wei Meng, James Cheng |  |
| 653 |  |  [Bridging Vision and Language Spaces with Assignment Prediction](https://openreview.net/forum?id=lK2V2E2MNv) |  | 0 |  | Jungin Park, Jiyoung Lee, Kwanghoon Sohn |  |
| 654 |  |  [Generative Judge for Evaluating Alignment](https://openreview.net/forum?id=gtkFw6sZGS) |  | 0 |  | Junlong Li, Shichao Sun, Weizhe Yuan, RunZe Fan, Hai Zhao, Pengfei Liu |  |
| 655 |  |  [Rethinking and Extending the Probabilistic Inference Capacity of GNNs](https://openreview.net/forum?id=7vVWiCrFnd) |  | 0 |  | Tuo Xu, Lei Zou |  |
| 656 |  |  [Learning model uncertainty as variance-minimizing instance weights](https://openreview.net/forum?id=bDWXhzZT40) |  | 0 |  | Nishant Jain, Karthikeyan Shanmugam, Pradeep Shenoy |  |
| 657 |  |  [Forward Learning with Top-Down Feedback: Empirical and Analytical Characterization](https://openreview.net/forum?id=My7lkRNnL9) |  | 0 |  | Ravi Francesco Srinivasan, Francesca Mignacco, Martino Sorbaro, Maria Refinetti, Avi Cooper, Gabriel Kreiman, Giorgia Dellaferrera |  |
| 658 |  |  [Towards Cross Domain Generalization of Hamiltonian Representation via Meta Learning](https://openreview.net/forum?id=AZGIwqCyYY) |  | 0 |  | Yeongwoo Song, Hawoong Jeong |  |
| 659 |  |  [What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning](https://openreview.net/forum?id=BTKAeLqLMw) |  | 0 |  | Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, Junxian He |  |
| 660 |  |  [Causality-Inspired Spatial-Temporal Explanations for Dynamic Graph Neural Networks](https://openreview.net/forum?id=AJBkfwXh3u) |  | 0 |  | Kesen Zhao, Liang Zhang |  |
| 661 |  |  [Dissecting learning and forgetting in language model finetuning](https://openreview.net/forum?id=tmsqb6WpLz) |  | 0 |  | Xiao Zhang, Ji Wu |  |
| 662 |  |  [Test-time Adaptation against Multi-modal Reliability Bias](https://openreview.net/forum?id=TPZRq4FALB) |  | 0 |  | Mouxing Yang, Yunfan Li, Changqing Zhang, Peng Hu, Xi Peng |  |
| 663 |  |  [Mirage: Model-agnostic Graph Distillation for Graph Classification](https://openreview.net/forum?id=78iGZdqxYY) |  | 0 |  | Mridul Gupta, Sahil Manchanda, Hariprasad Kodamana, Sayan Ranu |  |
| 664 |  |  [On the Learnability of Watermarks for Language Models](https://openreview.net/forum?id=9k0krNzvlV) |  | 0 |  | Chenchen Gu, Xiang Lisa Li, Percy Liang, Tatsunori Hashimoto |  |
| 665 |  |  [Bellman Optimal Stepsize Straightening of Flow-Matching Models](https://openreview.net/forum?id=Iyve2ycvGZ) |  | 0 |  | Bao Nguyen, Binh Nguyen, Viet Anh Nguyen |  |
| 666 |  |  [Enhancing Tail Performance in Extreme Classifiers by Label Variance Reduction](https://openreview.net/forum?id=6ARlSgun7J) |  | 0 |  | Anirudh Buvanesh, Rahul Chand, Jatin Prakash, Bhawna Paliwal, Mudit Dhawan, Neelabh Madan, Deepesh Hada, Vidit Jain, Sonu Mehta, Yashoteja Prabhu, Manish Gupta, Ramachandran Ramjee, Manik Varma |  |
| 667 |  |  [Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching](https://openreview.net/forum?id=Ebt7JgMHv1) |  | 0 |  | Aleksandar Makelov, Georg Lange, Atticus Geiger, Neel Nanda |  |
| 668 |  |  [Demonstration-Regularized RL](https://openreview.net/forum?id=lF2aip4Scn) |  | 0 |  | Daniil Tiapkin, Denis Belomestny, Daniele Calandriello, Eric Moulines, Alexey Naumov, Pierre Perrault, Michal Valko, Pierre Ménard |  |
| 669 |  |  [Multilingual Jailbreak Challenges in Large Language Models](https://openreview.net/forum?id=vESNKdEMGp) |  | 0 |  | Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing |  |
| 670 |  |  [$t^3$-Variational Autoencoder: Learning Heavy-tailed Data with Student's t and Power Divergence](https://openreview.net/forum?id=RzNlECeoOB) |  | 0 |  | Juno Kim, Jaehyuk Kwon, Mincheol Cho, Hyunjong Lee, JoongHo Won |  |
| 671 |  |  [Rethinking the Power of Graph Canonization in Graph Representation Learning with Stability](https://openreview.net/forum?id=nTwb2vBLOV) |  | 0 |  | Zehao Dong, Muhan Zhang, Philip R. O. Payne, Michael A. Province, Carlos Cruchaga, Tianyu Zhao, Fuhai Li, Yixin Chen |  |
| 672 |  |  [Gradual Optimization Learning for Conformational Energy Minimization](https://openreview.net/forum?id=FMMF1a9ifL) |  | 0 |  | Artem Tsypin, Leonid Ugadiarov, Kuzma Khrabrov, Alexander Telepov, Egor Rumiantsev, Alexey Skrynnik, Aleksandr Panov, Dmitry P. Vetrov, Elena Tutubalina, Artur Kadurin |  |
| 673 |  |  [AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection](https://openreview.net/forum?id=buC4E91xZE) |  | 0 |  | Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, Jiming Chen |  |
| 674 |  |  [Towards Faithful Explanations: Boosting Rationalization with Shortcuts Discovery](https://openreview.net/forum?id=uGtfk2OphU) |  | 0 |  | Linan Yue, Qi Liu, Yichao Du, Li Wang, Weibo Gao, Yanqing An |  |
| 675 |  |  [CORN: Contact-based Object Representation for Nonprehensile Manipulation of General Unseen Objects](https://openreview.net/forum?id=KTtEICH4TO) |  | 0 |  | Yoonyoung Cho, Junhyek Han, Yoontae Cho, Beomjoon Kim |  |
| 676 |  |  [An Intuitive Multi-Frequency Feature Representation for SO(3)-Equivariant Networks](https://openreview.net/forum?id=5JWAOLBxwp) |  | 0 |  | Dongwon Son, Jaehyung Kim, Sanghyeon Son, Beomjoon Kim |  |
| 677 |  |  [KoLA: Carefully Benchmarking World Knowledge of Large Language Models](https://openreview.net/forum?id=AqN23oqraW) |  | 0 |  | Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel ZhangLi, Xin Lv, Hao Peng, Zijun Yao, Xiaohan Zhang, Hanming Li, Chunyang Li, Zheyuan Zhang, Yushi Bai, Yantao Liu, Amy Xin, Kaifeng Yun, Linlu Gong, Nianyi Lin, Jianhui Chen, Zhili Wu, Yunjia Qi, Weikai Li, Yong Guan, Kaisheng Zeng, Ji Qi, Hailong Jin, Jinxin Liu, Yu Gu, Yuan Yao, Ning Ding, Lei Hou, Zhiyuan Liu, Bin Xu, Jie Tang, Juanzi Li |  |
| 678 |  |  [Graph Parsing Networks](https://openreview.net/forum?id=hv3SklibkL) |  | 0 |  | Yunchong Song, Siyuan Huang, Xinbing Wang, Chenghu Zhou, Zhouhan Lin |  |
| 679 |  |  [TESTAM: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of Experts](https://openreview.net/forum?id=N0nTk5BSvO) |  | 0 |  | Hyunwook Lee, Sungahn Ko |  |
| 680 |  |  [Learning From Simplicial Data Based on Random Walks and 1D Convolutions](https://openreview.net/forum?id=OsGUnYOzii) |  | 0 |  | Florian Frantzen, Michael T. Schaub |  |
| 681 |  |  [LUM-ViT: Learnable Under-sampling Mask Vision Transformer for Bandwidth Limited Optical Signal Acquisition](https://openreview.net/forum?id=wkbeqr5XhC) |  | 0 |  | Lingfeng Liu, Dong Ni, Hangjie Yuan |  |
| 682 |  |  [Social-Transmotion: Promptable Human Trajectory Prediction](https://openreview.net/forum?id=SQpnEfv9WH) |  | 0 |  | Saeed Saadatnejad, Yang Gao, Kaouther Messaoud, Alexandre Alahi |  |
| 683 |  |  [Robust Classification via Regression for Learning with Noisy Labels](https://openreview.net/forum?id=wfgZc3IMqo) |  | 0 |  | Erik Englesson, Hossein Azizpour |  |
| 684 |  |  [Learning to Reject with a Fixed Predictor: Application to Decontextualization](https://openreview.net/forum?id=dCHbFDsCZz) |  | 0 |  | Christopher Mohri, Daniel Andor, Eunsol Choi, Michael Collins, Anqi Mao, Yutao Zhong |  |
| 685 |  |  [Dynamics-Informed Protein Design with Structure Conditioning](https://openreview.net/forum?id=jZPqf2G9Sw) |  | 0 |  | Urszula Julia Komorowska, Simon V. Mathis, Kieran Didi, Francisco Vargas, Pietro Lio, Mateja Jamnik |  |
| 686 |  |  [Partitioning Message Passing for Graph Fraud Detection](https://openreview.net/forum?id=tEgrUrUuwA) |  | 0 |  | Wei Zhuo, Zemin Liu, Bryan Hooi, Bingsheng He, Guang Tan, Rizal Fathony, Jia Chen |  |
| 687 |  |  [Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation](https://openreview.net/forum?id=EmQSOi1X2f) |  | 0 |  | Niels Mündler, Jingxuan He, Slobodan Jenko, Martin T. Vechev |  |
| 688 |  |  [Symmetric Neural-Collapse Representations with Supervised Contrastive Loss: The Impact of ReLU and Batching](https://openreview.net/forum?id=AyXIDfvYg8) |  | 0 |  | Ganesh Ramachandra Kini, Vala Vakilian, Tina Behnia, Jaidev Gill, Christos Thrampoulidis |  |
| 689 |  |  [Manipulating dropout reveals an optimal balance of efficiency and robustness in biological and machine visual systems](https://openreview.net/forum?id=ADDCErFzev) |  | 0 |  | Jacob S. Prince, Gabriel Fajardo, George A. Alvarez, Talia Konkle |  |
| 690 |  |  [DDMI: Domain-agnostic Latent Diffusion Models for Synthesizing High-Quality Implicit Neural Representations](https://openreview.net/forum?id=327tbF3S65) |  | 0 |  | Dogyun Park, Sihyeon Kim, Sojin Lee, Hyunwoo J. Kim |  |
| 691 |  |  [Bayesian Coreset Optimization for Personalized Federated Learning](https://openreview.net/forum?id=uz7d2N2zul) |  | 0 |  | Prateek Chanda, Shrey Modi, Ganesh Ramakrishnan |  |
| 692 |  |  [In-context Autoencoder for Context Compression in a Large Language Model](https://openreview.net/forum?id=uREj4ZuGJE) |  | 0 |  | Tao Ge, Jing Hu, Lei Wang, Xun Wang, SiQing Chen, Furu Wei |  |
| 693 |  |  [Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning](https://openreview.net/forum?id=J44HfH4JCg) |  | 0 |  | Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, Lijuan Wang |  |
| 694 |  |  [Multimarginal Generative Modeling with Stochastic Interpolants](https://openreview.net/forum?id=FHqAzWl2wE) |  | 0 |  | Michael S. Albergo, Nicholas Matthew Boffi, Michael Lindsey, Eric VandenEijnden |  |
| 695 |  |  [Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators](https://openreview.net/forum?id=JiTVtCUOpS) |  | 0 |  | Lifan Zhao, Yanyan Shen |  |
| 696 |  |  [RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval](https://openreview.net/forum?id=GN921JHCRw) |  | 0 |  | Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning |  |
| 697 |  |  [Fair and Efficient Contribution Valuation for Vertical Federated Learning](https://openreview.net/forum?id=sLQb8q0sUi) |  | 0 |  | Zhenan Fan, Huang Fang, Xinglu Wang, Zirui Zhou, Jian Pei, Michael P. Friedlander, Yong Zhang |  |
| 698 |  |  [In-Context Learning through the Bayesian Prism](https://openreview.net/forum?id=HX5ujdsSon) |  | 0 |  | Madhur Panwar, Kabir Ahuja, Navin Goyal |  |
| 699 |  |  [RingAttention with Blockwise Transformers for Near-Infinite Context](https://openreview.net/forum?id=WsRHpHH4s0) |  | 0 |  | Hao Liu, Matei Zaharia, Pieter Abbeel |  |
| 700 |  |  [Chain of Hindsight aligns Language Models with Feedback](https://openreview.net/forum?id=6xfe4IVcOu) |  | 0 |  | Hao Liu, Carmelo Sferrazza, Pieter Abbeel |  |
| 701 |  |  [GraphChef: Decision-Tree Recipes to Explain Graph Neural Networks](https://openreview.net/forum?id=IjMUGuUmBI) |  | 0 |  | Peter Müller, Lukas Faber, Karolis Martinkus, Roger Wattenhofer |  |
| 702 |  |  [Safe Collaborative Filtering](https://openreview.net/forum?id=yarUvgEXq3) |  | 0 |  | Riku Togashi, Tatsushi Oka, Naoto Ohsaka, Tetsuro Morimura |  |
| 703 |  |  [On Representation Complexity of Model-based and Model-free Reinforcement Learning](https://openreview.net/forum?id=3K3s9qxSn7) |  | 0 |  | Hanlin Zhu, Baihe Huang, Stuart Russell |  |
| 704 |  |  [Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning](https://openreview.net/forum?id=sKPzAXoylB) |  | 0 |  | Mohamed Elsayed, A. Rupam Mahmood |  |
| 705 |  |  [A Good Learner can Teach Better: Teacher-Student Collaborative Knowledge Distillation](https://openreview.net/forum?id=Ixi4j6LtdX) |  | 0 |  | Ayan Sengupta, Shantanu Dixit, Md. Shad Akhtar, Tanmoy Chakraborty |  |
| 706 |  |  [Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making](https://openreview.net/forum?id=k581sTMyPt) |  | 0 |  | Aliyah R. Hsu, Yeshwanth Cherapanamjeri, Briton Park, Tristan Naumann, Anobel Y. Odisho, Bin Yu |  |
| 707 |  |  [Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks](https://openreview.net/forum?id=A0HKeKl4Nl) |  | 0 |  | Samyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka, Tim Rocktäschel, Edward Grefenstette, David Scott Krueger |  |
| 708 |  |  [RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems](https://openreview.net/forum?id=pPjZIOuQuF) |  | 0 |  | Tianyang Liu, Canwen Xu, Julian J. McAuley |  |
| 709 |  |  [Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective](https://openreview.net/forum?id=mIEHIcHGOo) |  | 0 |  | Ming Zhong, Chenxin An, Weizhu Chen, Jiawei Han, Pengcheng He |  |
| 710 |  |  [Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning](https://openreview.net/forum?id=RXFVcynVe1) |  | 0 |  | Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann LeCun, Bryan Hooi |  |
| 711 |  |  [SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs](https://openreview.net/forum?id=w4DW6qkRmt) |  | 0 |  | Jaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin Park, SangWoo Lee, Minjoon Seo, JungWoo Ha, Jinwoo Shin |  |
| 712 |  |  [Retrieval meets Long Context Large Language Models](https://openreview.net/forum?id=xw5nxFWMlo) |  | 0 |  | Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, Bryan Catanzaro |  |
| 713 |  |  [Neural Spectral Methods: Self-supervised learning in the spectral domain](https://openreview.net/forum?id=2DbVeuoa6a) |  | 0 |  | Yiheng Du, Nithin Chalapathi, Aditi S. Krishnapriyan |  |
| 714 |  |  [Kosmos-G: Generating Images in Context with Multimodal Large Language Models](https://openreview.net/forum?id=he6mX9LTyE) |  | 0 |  | Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, Furu Wei |  |
| 715 |  |  [Assessing Uncertainty in Similarity Scoring: Performance & Fairness in Face Recognition](https://openreview.net/forum?id=lAhQCHuANV) |  | 0 |  | JeanRémy Conti, Stéphan Clémençon |  |
| 716 |  |  [LitCab: Lightweight Language Model Calibration over Short- and Long-form Responses](https://openreview.net/forum?id=jH67LHVOIO) |  | 0 |  | Xin Liu, Muhammad Khalifa, Lu Wang |  |
| 717 |  |  [Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources](https://openreview.net/forum?id=cPgh4gWZlz) |  | 0 |  | Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, Lidong Bing |  |
| 718 |  |  [Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning](https://openreview.net/forum?id=m3xVPaZp6Z) |  | 0 |  | Chengxing Jia, Chenxiao Gao, Hao Yin, Fuxiang Zhang, XiongHui Chen, Tian Xu, Lei Yuan, Zongzhang Zhang, ZhiHua Zhou, Yang Yu |  |
| 719 |  |  [Energy-based Automated Model Evaluation](https://openreview.net/forum?id=CHGcP6lVWd) |  | 0 |  | Ru Peng, Heming Zou, Haobo Wang, Yawen Zeng, Zenan Huang, Junbo Zhao |  |
| 720 |  |  [Deceptive Fairness Attacks on Graphs via Meta Learning](https://openreview.net/forum?id=iS5ADHNg2A) |  | 0 |  | Jian Kang, Yinglong Xia, Ross Maciejewski, Jiebo Luo, Hanghang Tong |  |
| 721 |  |  [What Matters to You? Towards Visual Representation Alignment for Robot Learning](https://openreview.net/forum?id=CTlUHIKF71) |  | 0 |  | Thomas Tian, Chenfeng Xu, Masayoshi Tomizuka, Jitendra Malik, Andrea Bajcsy |  |
| 722 |  |  [FedDA: Faster Adaptive Gradient Methods for Federated Constrained Optimization](https://openreview.net/forum?id=kjn99xFUF3) |  | 0 |  | Junyi Li, Feihu Huang, Heng Huang |  |
| 723 |  |  [Extending Power of Nature from Binary to Real-Valued Graph Learning in Real World](https://openreview.net/forum?id=qT7DXUmX7j) |  | 0 |  | Chunshu Wu, Ruibing Song, Chuan Liu, Yunan Yang, Ang Li, Michael C. Huang, Tong Geng |  |
| 724 |  |  [Meta-VBO: Utilizing Prior Tasks in Optimizing Risk Measures with Gaussian Processes](https://openreview.net/forum?id=ElykcDu5YK) |  | 0 |  | Quoc Phong Nguyen, Bryan Kian Hsiang Low, Patrick Jaillet |  |
| 725 |  |  [Data Debugging with Shapley Importance over Machine Learning Pipelines](https://openreview.net/forum?id=qxGXjWxabq) |  | 0 |  | Bojan Karlas, David Dao, Matteo Interlandi, Sebastian Schelter, Wentao Wu, Ce Zhang |  |
| 726 |  |  [Pessimistic Nonlinear Least-Squares Value Iteration for Offline Reinforcement Learning](https://openreview.net/forum?id=4kLVvIh8cp) |  | 0 |  | Qiwei Di, Heyang Zhao, Jiafan He, Quanquan Gu |  |
| 727 |  |  [SKILL-MIX: a Flexible and Expandable Family of Evaluations for AI Models](https://openreview.net/forum?id=Jf5gplvglq) |  | 0 |  | Dingli Yu, Simran Kaur, Arushi Gupta, Jonah BrownCohen, Anirudh Goyal, Sanjeev Arora |  |
| 728 |  |  [A Quadratic Synchronization Rule for Distributed Deep Learning](https://openreview.net/forum?id=yroyhkhWS6) |  | 0 |  | Xinran Gu, Kaifeng Lyu, Sanjeev Arora, Jingzhao Zhang, Longbo Huang |  |
| 729 |  |  [ArchLock: Locking DNN Transferability at the Architecture Level with a Zero-Cost Binary Predictor](https://openreview.net/forum?id=e2YOVTenU9) |  | 0 |  | Tong Zhou, Shaolei Ren, Xiaolin Xu |  |
| 730 |  |  [Leftover Lunch: Advantage-based Offline Reinforcement Learning for Language Models](https://openreview.net/forum?id=ZDGKPbF0VQ) |  | 0 |  | Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap, Mark O. Riedl |  |
| 731 |  |  [RECOMP: Improving Retrieval-Augmented LMs with Context Compression and Selective Augmentation](https://openreview.net/forum?id=mlJLVigNHp) |  | 0 |  | Fangyuan Xu, Weijia Shi, Eunsol Choi |  |
| 732 |  |  [Gen-Z: Generative Zero-Shot Text Classification with Contextualized Label Descriptions](https://openreview.net/forum?id=rkplYfqUr0) |  | 0 |  | Sachin Kumar, Chan Young Park, Yulia Tsvetkov |  |
| 733 |  |  [In-Context Learning Dynamics with Random Binary Sequences](https://openreview.net/forum?id=62K7mALO2q) |  | 0 |  | Eric J. Bigelow, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka, Tomer D. Ullman |  |
| 734 |  |  [Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking](https://openreview.net/forum?id=XsHqr9dEGH) |  | 0 |  | Kaifeng Lyu, Jikai Jin, Zhiyuan Li, Simon Shaolei Du, Jason D. Lee, Wei Hu |  |
| 735 |  |  [Be Aware of the Neighborhood Effect: Modeling Selection Bias under Interference](https://openreview.net/forum?id=52fz5sUAy2) |  | 0 |  | Haoxuan Li, Chunyuan Zheng, Sihao Ding, Peng Wu, Zhi Geng, Fuli Feng, Xiangnan He |  |
| 736 |  |  [PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization](https://openreview.net/forum?id=22pyNMuIoa) |  | 0 |  | Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P. Xing, Zhiting Hu |  |
| 737 |  |  [Repeated Random Sampling for Minimizing the Time-to-Accuracy of Learning](https://openreview.net/forum?id=JnRStoIuTe) |  | 0 |  | Patrik Okanovic, Roger Waleffe, Vasilis Mageirakos, Konstantinos E. Nikolakakis, Amin Karbasi, Dionysios S. Kalogerias, Nezihe Merve Gürel, Theodoros Rekatsinas |  |
| 738 |  |  [Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs](https://openreview.net/forum?id=kGteeZ18Ir) |  | 0 |  | Shashank Gupta, Vaishnavi Shrivastava, Ameet Deshpande, Ashwin Kalyan, Peter Clark, Ashish Sabharwal, Tushar Khot |  |
| 739 |  |  [Enhancing Instance-Level Image Classification with Set-Level Labels](https://openreview.net/forum?id=AZW3qlCGTe) |  | 0 |  | Renyu Zhang, Aly A. Khan, Yuxin Chen, Robert L. Grossman |  |
| 740 |  |  [Pushing Boundaries: Mixup's Influence on Neural Collapse](https://openreview.net/forum?id=jTSKkcbEsj) |  | 0 |  | Quinn LeBlanc Fisher, Haoming Meng, Vardan Papyan |  |
| 741 |  |  [sRGB Real Noise Modeling via Noise-Aware Sampling with Normalizing Flows](https://openreview.net/forum?id=2XBBumBGeP) |  | 0 |  | Dongjin Kim, Donggoo Jung, Sungyong Baik, Tae Hyun Kim |  |
| 742 |  |  [Uncertainty-aware Graph-based Hyperspectral Image Classification](https://openreview.net/forum?id=8dN7gApKm3) |  | 0 |  | Linlin Yu, Yifei Lou, Feng Chen |  |
| 743 |  |  [Generative Adversarial Equilibrium Solvers](https://openreview.net/forum?id=TlyiaPXaVN) |  | 0 |  | Denizalp Goktas, David C. Parkes, Ian Gemp, Luke Marris, Georgios Piliouras, Romuald Elie, Guy Lever, Andrea Tacchetti |  |
| 744 |  |  [Graph Transformers on EHRs: Better Representation Improves Downstream Performance](https://openreview.net/forum?id=pe0Vdv7rsL) |  | 0 |  | Raphael Poulain, Rahmatollah Beheshti |  |
| 745 |  |  [On the Scalability and Memory Efficiency of Semidefinite Programs for Lipschitz Constant Estimation of Neural Networks](https://openreview.net/forum?id=dwzLn78jq7) |  | 0 |  | Zi Wang, Bin Hu, Aaron J. Havens, Alexandre Araujo, Yang Zheng, Yudong Chen, Somesh Jha |  |
| 746 |  |  [Large Language Models as Automated Aligners for benchmarking Vision-Language Models](https://openreview.net/forum?id=kZEXgtMNNo) |  | 0 |  | Yuanfeng Ji, Chongjian Ge, Weikai Kong, Enze Xie, Zhengying Liu, Zhenguo Li, Ping Luo |  |
| 747 |  |  [CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech](https://openreview.net/forum?id=ofzeypWosV) |  | 0 |  | Jaehyeon Kim, Keon Lee, Seungjun Chung, Jaewoong Cho |  |
| 748 |  |  [Unveiling the Unseen: Identifiable Clusters in Trained Depthwise Convolutional Kernels](https://openreview.net/forum?id=4VgBjsOC8k) |  | 0 |  | Zahra Babaiee, Peyman M. Kiasari, Daniela Rus, Radu Grosu |  |
| 749 |  |  [UNR-Explainer: Counterfactual Explanations for Unsupervised Node Representation Learning Models](https://openreview.net/forum?id=0j9ZDzMPqr) |  | 0 |  | Hyunju Kang, Geonhee Han, Hogun Park |  |
| 750 |  |  [Are Bert Family Good Instruction Followers? A Study on Their Potential And Limitations](https://openreview.net/forum?id=x8VNtpCu1I) |  | 0 |  | Yisheng Xiao, Juntao Li, Zechen Sun, Zechang Li, Qingrong Xia, Xinyu Duan, Zhefeng Wang, Min Zhang |  |
| 751 |  |  [Exploring the Promise and Limits of Real-Time Recurrent Learning](https://openreview.net/forum?id=V2cBKtdC3a) |  | 0 |  | Kazuki Irie, Anand Gopalakrishnan, Jürgen Schmidhuber |  |
| 752 |  |  [TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting](https://openreview.net/forum?id=YH5w12OUuU) |  | 0 |  | Defu Cao, Furong Jia, Sercan Ö. Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, Yan Liu |  |
| 753 |  |  [Scaling physics-informed hard constraints with mixture-of-experts](https://openreview.net/forum?id=u3dX2CEIZb) |  | 0 |  | Nithin Chalapathi, Yiheng Du, Aditi S. Krishnapriyan |  |
| 754 |  |  [Structural Fairness-aware Active Learning for Graph Neural Networks](https://openreview.net/forum?id=bvjcMvMn7B) |  | 0 |  | Haoyu Han, Xiaorui Liu, Li Ma, MohamadAli Torkamani, Hui Liu, Jiliang Tang, Makoto Yamada |  |
| 755 |  |  [Neural-Symbolic Recursive Machine for Systematic Generalization](https://openreview.net/forum?id=FWJAmwE0xH) |  | 0 |  | Qing Li, Yixin Zhu, Yitao Liang, Ying Nian Wu, SongChun Zhu, Siyuan Huang |  |
| 756 |  |  [Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-to-Image Generation](https://openreview.net/forum?id=ITq4ZRUT4a) |  | 0 |  | Jaemin Cho, Yushi Hu, Jason M. Baldridge, Roopal Garg, Peter Anderson, Ranjay Krishna, Mohit Bansal, Jordi PontTuset, Su Wang |  |
| 757 |  |  [Chain of Thought Empowers Transformers to Solve Inherently Serial Problems](https://openreview.net/forum?id=3EWTEy9MTM) |  | 0 |  | Zhiyuan Liu, Hong Liu, Denny Zhou, Tengyu Ma |  |
| 758 |  |  [Tractable MCMC for Private Learning with Pure and Gaussian Differential Privacy](https://openreview.net/forum?id=pmweVpJ229) |  | 0 |  | Yingyu Lin, Yian Ma, YuXiang Wang, Rachel Redberg, Zhiqi Bu |  |
| 759 |  |  [Optimal Sketching for Residual Error Estimation for Matrix and Vector Norms](https://openreview.net/forum?id=RsJwmWvE6Q) |  | 0 |  | Yi Li, Honghao Lin, David P. Woodruff |  |
| 760 |  |  [Reverse Diffusion Monte Carlo](https://openreview.net/forum?id=kIPEyMSdFV) |  | 0 |  | Xunpeng Huang, Hanze Dong, Yifan Hao, Yian Ma, Tong Zhang |  |
| 761 |  |  [Counting Graph Substructures with Graph Neural Networks](https://openreview.net/forum?id=qaJxPhkYtD) |  | 0 |  | Charilaos I. Kanatsoulis, Alejandro Ribeiro |  |
| 762 |  |  [Are Models Biased on Text without Gender-related Language?](https://openreview.net/forum?id=w1JanwReU6) |  | 0 |  | Catarina G. Belém, Preethi Seshadri, Yasaman Razeghi, Sameer Singh |  |
| 763 |  |  [PlaSma: Procedural Knowledge Models for Language-based Planning and Re-Planning](https://openreview.net/forum?id=dFcXJgnrGB) |  | 0 |  | Faeze Brahman, Chandra Bhagavatula, Valentina Pyatkin, Jena D. Hwang, Xiang Lorraine Li, Hirona Jacqueline Arai, Soumya Sanyal, Keisuke Sakaguchi, Xiang Ren, Yejin Choi |  |
| 764 |  |  [From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction](https://openreview.net/forum?id=PfPnugdxup) |  | 0 |  | Nima Shoghi, Adeesh Kolluru, John R. Kitchin, Zachary W. Ulissi, C. Lawrence Zitnick, Brandon M. Wood |  |
| 765 |  |  [Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets](https://openreview.net/forum?id=Zc2aIcucwc) |  | 0 |  | Dominique Beaini, Shenyang Huang, Joao Alex Cunha, Zhiyi Li, Gabriela MoisescuPareja, Oleksandr Dymov, Samuel MaddrellMander, Callum McLean, Frederik Wenkel, Luis Müller, Jama Hussein Mohamud, Ali Parviz, Michael Craig, Michal Koziarski, Jiarui Lu, Zhaocheng Zhu, Cristian Gabellini, Kerstin Klaser, Josef Dean, Cas Wognum, Maciej Sypetkowski, Guillaume Rabusseau, Reihaneh Rabbany, Jian Tang, Christopher Morris, Mirco Ravanelli, Guy Wolf, Prudencio Tossou, Hadrien Mary, Therence Bois, Andrew W. Fitzgibbon, Blazej Banaszewski, Chad Martin, Dominic Masters |  |
| 766 |  |  [Independent-Set Design of Experiments for Estimating Treatment and Spillover Effects under Network Interference](https://openreview.net/forum?id=w50MQ9Vfty) |  | 0 |  | Chencheng Cai, Xu Zhang, Edoardo M. Airoldi |  |
| 767 |  |  [FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores](https://openreview.net/forum?id=gPKTTAfYBp) |  | 0 |  | Daniel Y. Fu, Hermann Kumbong, Eric Nguyen, Christopher Ré |  |
| 768 |  |  [Transformer-VQ: Linear-Time Transformers via Vector Quantization](https://openreview.net/forum?id=oDdzXQzP2F) |  | 0 |  | Lucas D. Lingle |  |
| 769 |  |  [The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry](https://openreview.net/forum?id=4g02l2N2Nx) |  | 0 |  | Michael Zhang, Kush Bhatia, Hermann Kumbong, Christopher Ré |  |
| 770 |  |  [Abstractors and relational cross-attention: An inductive bias for explicit relational reasoning in Transformers](https://openreview.net/forum?id=XNa6r6ZjoB) |  | 0 |  | Awni Altabaa, Taylor Whittington Webb, Jonathan D. Cohen, John Lafferty |  |
| 771 |  |  [Doubly Robust Instance-Reweighted Adversarial Training](https://openreview.net/forum?id=OF5x1dzWSS) |  | 0 |  | Daouda Sow, Sen Lin, Zhangyang Wang, Yingbin Liang |  |
| 772 |  |  [Training Diffusion Models with Reinforcement Learning](https://openreview.net/forum?id=YCWjhGrJFD) |  | 0 |  | Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, Sergey Levine |  |
| 773 |  |  [Finite-Time Analysis of On-Policy Heterogeneous Federated Reinforcement Learning](https://openreview.net/forum?id=D2eOVqPX9g) |  | 0 |  | Chenyu Zhang, Han Wang, Aritra Mitra, James Anderson |  |
| 774 |  |  [Federated Q-Learning: Linear Regret Speedup with Low Communication Cost](https://openreview.net/forum?id=fe6ANBxcKM) |  | 0 |  | Zhong Zheng, Fengyu Gao, Lingzhou Xue, Jing Yang |  |
| 775 |  |  [The Trickle-down Impact of Reward Inconsistency on RLHF](https://openreview.net/forum?id=MeHmwCDifc) |  | 0 |  | Lingfeng Shen, Sihao Chen, Linfeng Song, Lifeng Jin, Baolin Peng, Haitao Mi, Daniel Khashabi, Dong Yu |  |
| 776 |  |  [Efficient Modulation for Vision Networks](https://openreview.net/forum?id=ip5LHJs6QX) |  | 0 |  | Xu Ma, Xiyang Dai, Jianwei Yang, Bin Xiao, Yinpeng Chen, Yun Fu, Lu Yuan |  |
| 777 |  |  [Pre-training LiDAR-based 3D Object Detectors through Colorization](https://openreview.net/forum?id=fB1iiH9xo7) |  | 0 |  | TaiYu Pan, Chenyang Ma, Tianle Chen, Cheng Perng Phoo, Katie Z. Luo, Yurong You, Mark Campbell, Kilian Q. Weinberger, Bharath Hariharan, WeiLun Chao |  |
| 778 |  |  [An Emulator for Fine-tuning Large Language Models using Small Language Models](https://openreview.net/forum?id=Eo7kv0sllr) |  | 0 |  | Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, Christopher D. Manning |  |
| 779 |  |  [Toward Student-oriented Teacher Network Training for Knowledge Distillation](https://openreview.net/forum?id=wsWGcw6qKD) |  | 0 |  | Chengyu Dong, Liyuan Liu, Jingbo Shang |  |
| 780 |  |  [Language Models Represent Space and Time](https://openreview.net/forum?id=jE8xbmvFin) |  | 0 |  | Wes Gurnee, Max Tegmark |  |
| 781 |  |  [Causal Modelling Agents: Causal Graph Discovery through Synergising Metadata- and Data-driven Reasoning](https://openreview.net/forum?id=pAoqRlTBtY) |  | 0 |  | Ahmed Abdulaal, Adamos Hadjivasiliou, Nina Montaña Brown, Tiantian He, Ayodeji Ijishakin, Ivana Drobnjak, Daniel C. Castro, Daniel C. Alexander |  |
| 782 |  |  [Fast-ELECTRA for Efficient Pre-training](https://openreview.net/forum?id=8OBuqbLb8h) |  | 0 |  | Chengyu Dong, Liyuan Liu, Hao Cheng, Jingbo Shang, Jianfeng Gao, Xiaodong Liu |  |
| 783 |  |  [Maximum Entropy Model Correction in Reinforcement Learning](https://openreview.net/forum?id=kNpSUN0uCc) |  | 0 |  | Amin Rakhsha, Mete Kemertas, Mohammad Ghavamzadeh, Amirmassoud Farahmand |  |
| 784 |  |  [SpaCE: The Spatial Confounding Environment](https://openreview.net/forum?id=D9rJdtmIG6) |  | 0 |  | Mauricio Tec, Ana Trisovic, Michelle Audirac, Sophie Woodward, Jie Kate Hu, Naeem Khoshnevis, Francesca Dominici |  |
| 785 |  |  [Language Model Detectors Are Easily Optimized Against](https://openreview.net/forum?id=4eJDMjYZZG) |  | 0 |  | Charlotte Nicks, Eric Mitchell, Rafael Rafailov, Archit Sharma, Christopher D. Manning, Chelsea Finn, Stefano Ermon |  |
| 786 |  |  [Zero-Shot Robotic Manipulation with Pre-Trained Image-Editing Diffusion Models](https://openreview.net/forum?id=c0chJTSbci) |  | 0 |  | Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Rich Walke, Chelsea Finn, Aviral Kumar, Sergey Levine |  |
| 787 |  |  [Simple Hierarchical Planning with Diffusion](https://openreview.net/forum?id=kXHEBK9uAY) |  | 0 |  | Chang Chen, Fei Deng, Kenji Kawaguchi, Caglar Gulcehre, Sungjin Ahn |  |
| 788 |  |  [Stochastic Gradient Descent for Gaussian Processes Done Right](https://openreview.net/forum?id=fj2E5OcLFn) |  | 0 |  | Jihao Andreas Lin, Shreyas Padhy, Javier Antorán, Austin Tripp, Alexander Terenin, Csaba Szepesvári, José Miguel HernándezLobato, David Janz |  |
| 789 |  |  [GAFormer: Enhancing Timeseries Transformers Through Group-Aware Embeddings](https://openreview.net/forum?id=c56TWtYp0W) |  | 0 |  | Jingyun Xiao, Ran Liu, Eva L. Dyer |  |
| 790 |  |  [Why is SAM Robust to Label Noise?](https://openreview.net/forum?id=3aZCPl3ZvR) |  | 0 |  | Christina Baek, J. Zico Kolter, Aditi Raghunathan |  |
| 791 |  |  [Revisiting the Last-Iterate Convergence of Stochastic Gradient Methods](https://openreview.net/forum?id=xxaEhwC1I4) |  | 0 |  | Zijian Liu, Zhengyuan Zhou |  |
| 792 |  |  [CNN Kernels Can Be the Best Shapelets](https://openreview.net/forum?id=O8ouVV8PjF) |  | 0 |  | Eric Qu, Yansen Wang, Xufang Luo, Wenqiang He, Kan Ren, Dongsheng Li |  |
| 793 |  |  [Fine-Tuning Language Models for Factuality](https://openreview.net/forum?id=WPZ2yPag4K) |  | 0 |  | Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D. Manning, Chelsea Finn |  |
| 794 |  |  [Soft Robust MDPs and Risk-Sensitive MDPs: Equivalence, Policy Gradient, and Sample Complexity](https://openreview.net/forum?id=dEz3ge8QSo) |  | 0 |  | Runyu Zhang, Yang Hu, Na Li |  |
| 795 |  |  [Tensor Programs VI: Feature Learning in Infinite Depth Neural Networks](https://openreview.net/forum?id=17pVDnpwwl) |  | 0 |  | Greg Yang, Dingli Yu, Chen Zhu, Soufiane Hayou |  |
| 796 |  |  [Demystifying Poisoning Backdoor Attacks from a Statistical Perspective](https://openreview.net/forum?id=BPHcEpGvF8) |  | 0 |  | Ganghua Wang, Xun Xian, Ashish Kundu, Jayanth Srinivasa, Xuan Bi, Mingyi Hong, Jie Ding |  |
| 797 |  |  [Learning to Make Adherence-aware Advice](https://openreview.net/forum?id=RgELE1dQXx) |  | 0 |  | Guanting Chen, Xiaocheng Li, Chunlin Sun, Hanzhao Wang |  |
| 798 |  |  [Beyond Spatio-Temporal Representations: Evolving Fourier Transform for Temporal Graphs](https://openreview.net/forum?id=uvFhCUPjtI) |  | 0 |  | Anson Bastos, Kuldeep Singh, Abhishek Nadgeri, Manish Singh, Toyotaro Suzumura |  |
| 799 |  |  [Cycle Consistency Driven Object Discovery](https://openreview.net/forum?id=f1xnBr4WD6) |  | 0 |  | Aniket Rajiv Didolkar, Anirudh Goyal, Yoshua Bengio |  |
| 800 |  |  [Sufficient conditions for offline reactivation in recurrent neural networks](https://openreview.net/forum?id=RVrINT6MT7) |  | 0 |  | Nanda H. Krishna, Colin Bredenberg, Daniel Levenstein, Blake Aaron Richards, Guillaume Lajoie |  |
| 801 |  |  [Forward Learning of Graph Neural Networks](https://openreview.net/forum?id=Abr7dU98ME) |  | 0 |  | Namyong Park, Xing Wang, Antoine Simoulin, Shuai Yang, Grey Yang, Ryan A. Rossi, Puja Trivedi, Nesreen K. Ahmed |  |
| 802 |  |  [Curriculum reinforcement learning for quantum architecture search under hardware errors](https://openreview.net/forum?id=rINBD8jPoP) |  | 0 |  | Yash J. Patel, Akash Kundu, Mateusz Ostaszewski, Xavier BonetMonroig, Vedran Dunjko, Onur Danaci |  |
| 803 |  |  [Does CLIP's generalization performance mainly stem from high train-test similarity?](https://openreview.net/forum?id=tnBaiidobu) |  | 0 |  | Prasanna Mayilvahanan, Thaddäus Wiedemer, Evgenia Rusak, Matthias Bethge, Wieland Brendel |  |
| 804 |  |  [Unified Projection-Free Algorithms for Adversarial DR-Submodular Optimization](https://openreview.net/forum?id=H4A9e8HvIn) |  | 0 |  | Mohammad Pedramfar, Yididiya Y. Nadew, Christopher John Quinn, Vaneet Aggarwal |  |
| 805 |  |  [When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method](https://openreview.net/forum?id=5HCnKDeTws) |  | 0 |  | Biao Zhang, Zhongtao Liu, Colin Cherry, Orhan Firat |  |
| 806 |  |  [Learning to design protein-protein interactions with enhanced generalization](https://openreview.net/forum?id=xcMmebCT7s) |  | 0 |  | Anton Bushuiev, Roman Bushuiev, Petr Kouba, Anatolii Filkin, Marketa Gabrielova, Michal Gabriel, Jirí Sedlár, Tomás Pluskal, Jirí Damborský, Stanislav Mazurenko, Josef Sivic |  |
| 807 |  |  [L2MAC: Large Language Model Automatic Computer for Extensive Code Generation](https://openreview.net/forum?id=EhrzQwsV4K) |  | 0 |  | Samuel Holt, Max Ruiz Luyten, Mihaela van der Schaar |  |
| 808 |  |  [BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models](https://openreview.net/forum?id=c93SBwz1Ma) |  | 0 |  | Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, Bo Li |  |
| 809 |  |  [NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks](https://openreview.net/forum?id=samyfu6G93) |  | 0 |  | Wenxi Wang, Yang Hu, Mohit Tiwari, Sarfraz Khurshid, Kenneth L. McMillan, Risto Miikkulainen |  |
| 810 |  |  [Group Preference Optimization: Few-Shot Alignment of Large Language Models](https://openreview.net/forum?id=DpFeMH4l8Q) |  | 0 |  | Siyan Zhao, John Dang, Aditya Grover |  |
| 811 |  |  [MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training](https://openreview.net/forum?id=w3YZ9MSlBu) |  | 0 |  | Yizhi Li, Ruibin Yuan, Ge Zhang, Yinghao Ma, Xingran Chen, Hanzhi Yin, Chenghao Xiao, Chenghua Lin, Anton Ragni, Emmanouil Benetos, Norbert Gyenge, Roger B. Dannenberg, Ruibo Liu, Wenhu Chen, Gus Xia, Yemin Shi, Wenhao Huang, Zili Wang, Yike Guo, Jie Fu |  |
| 812 |  |  [Variance-aware Regret Bounds for Stochastic Contextual Dueling Bandits](https://openreview.net/forum?id=rDH7dIFn20) |  | 0 |  | Qiwei Di, Tao Jin, Yue Wu, Heyang Zhao, Farzad Farnoud, Quanquan Gu |  |
| 813 |  |  [A Discretization Framework for Robust Contextual Stochastic Optimization](https://openreview.net/forum?id=ueTdErd5Ib) |  | 0 |  | Rares Cristian, Georgia Perakis |  |
| 814 |  |  [Risk Bounds of Accelerated SGD for Overparameterized Linear Regression](https://openreview.net/forum?id=AcoXPIPh4A) |  | 0 |  | Xuheng Li, Yihe Deng, Jingfeng Wu, Dongruo Zhou, Quanquan Gu |  |
| 815 |  |  [Task structure and nonlinearity jointly determine learned representational geometry](https://openreview.net/forum?id=k9t8dQ30kU) |  | 0 |  | Matteo Alleman, Jack W. Lindsey, Stefano Fusi |  |
| 816 |  |  [Llemma: An Open Language Model for Mathematics](https://openreview.net/forum?id=4WnqRR915j) |  | 0 |  | Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, Sean Welleck |  |
| 817 |  |  [Directly Fine-Tuning Diffusion Models on Differentiable Rewards](https://openreview.net/forum?id=1vmSEVL19f) |  | 0 |  | Kevin Clark, Paul Vicol, Kevin Swersky, David J. Fleet |  |
| 818 |  |  [Maximum Likelihood Estimation is All You Need for Well-Specified Covariate Shift](https://openreview.net/forum?id=eoTCKKOgIs) |  | 0 |  | Jiawei Ge, Shange Tang, Jianqing Fan, Cong Ma, Chi Jin |  |
| 819 |  |  [Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks](https://openreview.net/forum?id=pAVJKp3Dvn) |  | 0 |  | Changwoo Lee, HunSeok Kim |  |
| 820 |  |  [A Flexible Generative Model for Heterogeneous Tabular EHR with Missing Modality](https://openreview.net/forum?id=W2tCmRrj7H) |  | 0 |  | Huan He, William Hao, Yuanzhe Xi, Yong Chen, Bradley A. Malin, Joyce C. Ho |  |
| 821 |  |  [Designing Skill-Compatible AI: Methodologies and Frameworks in Chess](https://openreview.net/forum?id=79rfgv3jw4) |  | 0 |  | Karim Hamade, Reid McIlroyYoung, Siddhartha Sen, Jon M. Kleinberg, Ashton Anderson |  |
| 822 |  |  [Tree Search-Based Policy Optimization under Stochastic Execution Delay](https://openreview.net/forum?id=RaqZX9LSGA) |  | 0 |  | David Valensi, Esther Derman, Shie Mannor, Gal Dalal |  |
| 823 |  |  [Context-Aware Meta-Learning](https://openreview.net/forum?id=lJYAkDVnRU) |  | 0 |  | Christopher Fifty, Dennis Duan, Ronald G. Junkins, Ehsan Amid, Jure Leskovec, Christopher Ré, Sebastian Thrun |  |
| 824 |  |  [Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain](https://openreview.net/forum?id=caW7LdAALh) |  | 0 |  | Marcus J. Min, Yangruibo Ding, Luca Buratti, Saurabh Pujar, Gail E. Kaiser, Suman Jana, Baishakhi Ray |  |
| 825 |  |  [Ito Diffusion Approximation of Universal Ito Chains for Sampling, Optimization and Boosting](https://openreview.net/forum?id=fjpfCOV4ru) |  | 0 |  | Aleksei Ustimenko, Aleksandr Beznosikov |  |
| 826 |  |  [Modeling Boundedly Rational Agents with Latent Inference Budgets](https://openreview.net/forum?id=W3VsHuga3j) |  | 0 |  | Athul Paul Jacob, Abhishek Gupta, Jacob Andreas |  |
| 827 |  |  [The Effectiveness of Random Forgetting for Robust Generalization](https://openreview.net/forum?id=MEGQGNUfPx) |  | 0 |  | Vijaya Raghavan T. Ramkumar, Bahram Zonooz, Elahe Arani |  |
| 828 |  |  [Diffeomorphic Mesh Deformation via Efficient Optimal Transport for Cortical Surface Reconstruction](https://openreview.net/forum?id=gxhRR8vUQb) |  | 0 |  | ThanhTung Le, Khai Nguyen, Shanlin Sun, Kun Han, Nhat Ho, Xiaohui Xie |  |
| 829 |  |  [Lie Group Decompositions for Equivariant Neural Networks](https://openreview.net/forum?id=p34fRKp8qA) |  | 0 |  | Mircea Mironenco, Patrick Forré |  |
| 830 |  |  [Efficient Heterogeneous Meta-Learning via Channel Shuffling Modulation](https://openreview.net/forum?id=QiJuMJl0QS) |  | 0 |  | Minh Hoang, Carl Kingsford |  |
| 831 |  |  [To Grok or not to Grok: Disentangling Generalization and Memorization on Corrupted Algorithmic Datasets](https://openreview.net/forum?id=UHjE5v5MB7) |  | 0 |  | Darshil Doshi, Aritra Das, Tianyu He, Andrey Gromov |  |
| 832 |  |  [VCR-Graphormer: A Mini-batch Graph Transformer via Virtual Connections](https://openreview.net/forum?id=SUUrkC3STJ) |  | 0 |  | Dongqi Fu, Zhigang Hua, Yan Xie, Jin Fang, Si Zhang, Kaan Sancak, Hao Wu, Andrey Malevich, Jingrui He, Bo Long |  |
| 833 |  |  [Optimistic Bayesian Optimization with Unknown Constraints](https://openreview.net/forum?id=D4NJFfrqoq) |  | 0 |  | Quoc Phong Nguyen, Wan Theng Ruth Chew, Le Song, Bryan Kian Hsiang Low, Patrick Jaillet |  |
| 834 |  |  [DRSM: De-Randomized Smoothing on Malware Classifier Providing Certified Robustness](https://openreview.net/forum?id=m7aPLHwsLr) |  | 0 |  | Shoumik Saha, Wenxiao Wang, Yigitcan Kaya, Soheil Feizi, Tudor Dumitras |  |
| 835 |  |  [On the Variance of Neural Network Training with respect to Test Sets and Distributions](https://openreview.net/forum?id=pEGSdJu52I) |  | 0 |  | Keller Jordan |  |
| 836 |  |  [Large Language Models to Enhance Bayesian Optimization](https://openreview.net/forum?id=OOxotBmGol) |  | 0 |  | Tennison Liu, Nicolás Astorga, Nabeel Seedat, Mihaela van der Schaar |  |
| 837 |  |  [Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach](https://openreview.net/forum?id=55uj7mU7Cv) |  | 0 |  | Sagar Shrestha, Xiao Fu |  |
| 838 |  |  [SineNet: Learning Temporal Dynamics in Time-Dependent Partial Differential Equations](https://openreview.net/forum?id=LSYhE2hLWG) |  | 0 |  | Xuan Zhang, Jacob Helwig, Yuchao Lin, Yaochen Xie, Cong Fu, Stephan Wojtowytsch, Shuiwang Ji |  |
| 839 |  |  [GNNBoundary: Towards Explaining Graph Neural Networks through the Lens of Decision Boundaries](https://openreview.net/forum?id=WIzzXCVYiH) |  | 0 |  | Xiaoqi Wang, HanWei Shen |  |
| 840 |  |  [Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales for Pruning Recurrent SNN](https://openreview.net/forum?id=0jsfesDZDq) |  | 0 |  | Biswadeep Chakraborty, Beomseok Kang, Harshit Kumar, Saibal Mukhopadhyay |  |
| 841 |  |  [Investigating the Benefits of Projection Head for Representation Learning](https://openreview.net/forum?id=GgEAdqYPNA) |  | 0 |  | Yihao Xue, Eric Gan, Jiayi Ni, Siddharth Joshi, Baharan Mirzasoleiman |  |
| 842 |  |  [A Variational Perspective on Solving Inverse Problems with Diffusion Models](https://openreview.net/forum?id=1YO4EE3SPB) |  | 0 |  | Morteza Mardani, Jiaming Song, Jan Kautz, Arash Vahdat |  |
| 843 |  |  [Can Large Language Models Infer Causation from Correlation?](https://openreview.net/forum?id=vqIH0ObdqL) |  | 0 |  | Zhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, Mona T. Diab, Bernhard Schölkopf |  |
| 844 |  |  [Improved statistical and computational complexity of the mean-field Langevin dynamics under structured data](https://openreview.net/forum?id=Of2nEDc4s7) |  | 0 |  | Atsushi Nitanda, Kazusato Oko, Taiji Suzuki, Denny Wu |  |
| 845 |  |  [Jointly-Learned Exit and Inference for a Dynamic Neural Network](https://openreview.net/forum?id=jX2DT7qDam) |  | 0 |  | Florence Regol, Joud Chataoui, Mark Coates |  |
| 846 |  |  [Understanding the Robustness of Multi-modal Contrastive Learning to Distribution Shift](https://openreview.net/forum?id=rtl4XnJYBh) |  | 0 |  | Yihao Xue, Siddharth Joshi, Dang Nguyen, Baharan Mirzasoleiman |  |
| 847 |  |  [SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking](https://openreview.net/forum?id=FJWT0692hw) |  | 0 |  | Chris Cundy, Stefano Ermon |  |
| 848 |  |  [Layer-wise linear mode connectivity](https://openreview.net/forum?id=LfmZh91tDI) |  | 0 |  | Linara Adilova, Maksym Andriushchenko, Michael Kamp, Asja Fischer, Martin Jaggi |  |
| 849 |  |  [Understanding Certified Training with Interval Bound Propagation](https://openreview.net/forum?id=h05eQniJsQ) |  | 0 |  | Yuhao Mao, Mark Niklas Müller, Marc Fischer, Martin T. Vechev |  |
| 850 |  |  [Offline RL with Observation Histories: Analyzing and Improving Sample Complexity](https://openreview.net/forum?id=GnOLWS4Llt) |  | 0 |  | Joey Hong, Anca D. Dragan, Sergey Levine |  |
| 851 |  |  [Graph-based Virtual Sensing from Sparse and Partial Multivariate Observations](https://openreview.net/forum?id=CAqdG2dy5s) |  | 0 |  | Giovanni de Felice, Andrea Cini, Daniele Zambon, Vladimir V. Gusev, Cesare Alippi |  |
| 852 |  |  [NEFTune: Noisy Embeddings Improve Instruction Finetuning](https://openreview.net/forum?id=0bMmZ3fkCk) |  | 0 |  | Neel Jain, Pingyeh Chiang, Yuxin Wen, John Kirchenbauer, HongMin Chu, Gowthami Somepalli, Brian R. Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein |  |
| 853 |  |  [An operator preconditioning perspective on training in physics-informed machine learning](https://openreview.net/forum?id=WWlxFtR5sV) |  | 0 |  | Tim De Ryck, Florent Bonnet, Siddhartha Mishra, Emmanuel de Bézenac |  |
| 854 |  |  [Two-stage LLM Fine-tuning with Less Specialization and More Generalization](https://openreview.net/forum?id=pCEgna6Qco) |  | 0 |  | Yihan Wang, Si Si, Daliang Li, Michal Lukasik, Felix X. Yu, ChoJui Hsieh, Inderjit S. Dhillon, Sanjiv Kumar |  |
| 855 |  |  [Expressive Losses for Verified Robustness via Convex Combinations](https://openreview.net/forum?id=mzyZ4wzKlM) |  | 0 |  | Alessandro De Palma, Rudy Bunel, Krishnamurthy (Dj) Dvijotham, M. Pawan Kumar, Robert Stanforth, Alessio Lomuscio |  |
| 856 |  |  [Learning Adaptive Multiresolution Transforms via Meta-Framelet-based Graph Convolutional Network](https://openreview.net/forum?id=5RielfrDkP) |  | 0 |  | Tianze Luo, Zhanfeng Mo, Sinno Jialin Pan |  |
| 857 |  |  [REFACTOR: Learning to Extract Theorems from Proofs](https://openreview.net/forum?id=fgKjiVrm6u) |  | 0 |  | Jin Peng Zhou, Yuhuai Wu, Qiyang Li, Roger Baker Grosse |  |
| 858 |  |  [Let's do the time-warp-attend: Learning topological invariants of dynamical systems](https://openreview.net/forum?id=Fj7Fzm5lWL) |  | 0 |  | Noa Moriel, Matthew Ricci, Mor Nitzan |  |
| 859 |  |  [Sparse MoE with Language Guided Routing for Multilingual Machine Translation](https://openreview.net/forum?id=ySS7hH1smL) |  | 0 |  | Xinyu Zhao, Xuxi Chen, Yu Cheng, Tianlong Chen |  |
| 860 |  |  [Detecting Pretraining Data from Large Language Models](https://openreview.net/forum?id=zWqr3MQuNs) |  | 0 |  | Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, Luke Zettlemoyer |  |
| 861 |  |  [Don't Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization](https://openreview.net/forum?id=V5tdi14ple) |  | 0 |  | Jin Peng Zhou, Charles Staats, Wenda Li, Christian Szegedy, Kilian Q. Weinberger, Yuhuai Wu |  |
| 862 |  |  [PubDef: Defending Against Transfer Attacks From Public Models](https://openreview.net/forum?id=Tvwf4Vsi5F) |  | 0 |  | Chawin Sitawarin, Jaewon Chang, David Huang, Wesson Altoyan, David A. Wagner |  |
| 863 |  |  [AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ](https://openreview.net/forum?id=v3K5TVP8kZ) |  | 0 |  | Jonas Belouadi, Anne Lauscher, Steffen Eger |  |
| 864 |  |  [Sharpness-Aware Minimization Enhances Feature Quality via Balanced Learning](https://openreview.net/forum?id=3xDaj4pRna) |  | 0 |  | Jacob Mitchell Springer, Vaishnavh Nagarajan, Aditi Raghunathan |  |
| 865 |  |  [Can LLM-Generated Misinformation Be Detected?](https://openreview.net/forum?id=ccxD4mtkTU) |  | 0 |  | Canyu Chen, Kai Shu |  |
| 866 |  |  [A Simple Interpretable Transformer for Fine-Grained Image Classification and Analysis](https://openreview.net/forum?id=bkdWThqE6q) |  | 0 |  | Dipanjyoti Paul, Arpita Chowdhury, Xinqi Xiong, FengJu Chang, David Edward Carlyn, Samuel Stevens, Kaiya Provost, Anuj Karpatne, Bryan Carstens, Daniel I. Rubenstein, Charles V. Stewart, Tanya Y. BergerWolf, Yu Su, WeiLun Chao |  |
| 867 |  |  [One-shot Active Learning Based on Lewis Weight Sampling for Multiple Deep Models](https://openreview.net/forum?id=EDXkkUAIFW) |  | 0 |  | ShengJun Huang, Yi Li, Yiming Sun, YingPeng Tang |  |
| 868 |  |  [Disentangling Time Series Representations via Contrastive Independence-of-Support on l-Variational Inference](https://openreview.net/forum?id=iI7hZSczxE) |  | 0 |  | Khalid Oublal, Saïd Ladjal, David Benhaiem, Emmanuel Leborgne, François Roueff |  |
| 869 |  |  [Improved algorithm and bounds for successive projection](https://openreview.net/forum?id=GlpawHh80l) |  | 0 |  | Jiashun Jin, Zheng Tracy Ke, Gabriel Moryoussef, Jiajun Tang, Jingming Wang |  |
| 870 |  |  [Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF](https://openreview.net/forum?id=0tWTxYYPnW) |  | 0 |  | Anand Siththaranjan, Cassidy Laidlaw, Dylan HadfieldMenell |  |
| 871 |  |  [Estimating Shape Distances on Neural Representations with Limited Samples](https://openreview.net/forum?id=kvByNnMERu) |  | 0 |  | Dean A. Pospisil, Brett W. Larsen, Sarah E. Harvey, Alex H. Williams |  |
| 872 |  |  [Learning semilinear neural operators: A unified recursive framework for prediction and data assimilation](https://openreview.net/forum?id=ZMv6zKYYUs) |  | 0 |  | Ashutosh Singh, Ricardo Augusto Borsoi, Deniz Erdogmus, Tales Imbiriba |  |
| 873 |  |  [Eureka: Human-Level Reward Design via Coding Large Language Models](https://openreview.net/forum?id=IEduRUO55F) |  | 0 |  | Yecheng Jason Ma, William Liang, Guanzhi Wang, DeAn Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, Anima Anandkumar |  |
| 874 |  |  [f-FERM: A Scalable Framework for Robust Fair Empirical Risk Minimization](https://openreview.net/forum?id=s90VIdza2K) |  | 0 |  | Sina Baharlouei, Shivam Patel, Meisam Razaviyayn |  |
| 875 |  |  [Source-Free and Image-Only Unsupervised Domain Adaptation for Category Level Object Pose Estimation](https://openreview.net/forum?id=UPvufoBAIs) |  | 0 |  | Prakhar Kaushik, Aayush Mishra, Adam Kortylewski, Alan L. Yuille |  |
| 876 |  |  [Closing the Curious Case of Neural Text Degeneration](https://openreview.net/forum?id=dONpC9GL1o) |  | 0 |  | Matthew Finlayson, John Hewitt, Alexander Koller, Swabha Swayamdipta, Ashish Sabharwal |  |
| 877 |  |  [Mediator Interpretation and Faster Learning Algorithms for Linear Correlated Equilibria in General Sequential Games](https://openreview.net/forum?id=bsKMPAFHO7) |  | 0 |  | Brian Hu Zhang, Gabriele Farina, Tuomas Sandholm |  |
| 878 |  |  [3D Feature Prediction for Masked-AutoEncoder-Based Point Cloud Pretraining](https://openreview.net/forum?id=LokR2TTFMs) |  | 0 |  | Siming Yan, Yuqi Yang, YuXiao Guo, Hao Pan, PengShuai Wang, Xin Tong, Yang Liu, Qixing Huang |  |
| 879 |  |  [Understanding Catastrophic Forgetting in Language Models via Implicit Inference](https://openreview.net/forum?id=VrHiF2hsrm) |  | 0 |  | Suhas Kotha, Jacob Mitchell Springer, Aditi Raghunathan |  |
| 880 |  |  [Efficient Subgraph GNNs by Learning Effective Selection Policies](https://openreview.net/forum?id=gppLqZLQeY) |  | 0 |  | Beatrice Bevilacqua, Moshe Eliasof, Eli A. Meirom, Bruno Ribeiro, Haggai Maron |  |
| 881 |  |  [Parsing neural dynamics with infinite recurrent switching linear dynamical systems](https://openreview.net/forum?id=YIls9HEa52) |  | 0 |  | Victor Geadah, International Brain Laboratory, Jonathan W. Pillow |  |
| 882 |  |  [Active Retrosynthetic Planning Aware of Route Quality](https://openreview.net/forum?id=h7DGnWGeos) |  | 0 |  | Luotian Yuan, Yemin Yu, Ying Wei, Yongwei Wang, Zhihua Wang, Fei Wu |  |
| 883 |  |  [How do Language Models Bind Entities in Context?](https://openreview.net/forum?id=zb3b6oKO77) |  | 0 |  | Jiahai Feng, Jacob Steinhardt |  |
| 884 |  |  [Novel Quadratic Constraints for Extending LipSDP beyond Slope-Restricted Activations](https://openreview.net/forum?id=HfXDrAzFvG) |  | 0 |  | Patricia Pauli, Aaron J. Havens, Alexandre Araujo, Siddharth Garg, Farshad Khorrami, Frank Allgöwer, Bin Hu |  |
| 885 |  |  [Unbalancedness in Neural Monge Maps Improves Unpaired Domain Translation](https://openreview.net/forum?id=2UnCj3jeao) |  | 0 |  | Luca Eyring, Dominik Klein, Théo Uscidda, Giovanni Palla, Niki Kilbertus, Zeynep Akata, Fabian J. Theis |  |
| 886 |  |  [Implicit Neural Representations and the Algebra of Complex Wavelets](https://openreview.net/forum?id=uZfjFyPAvn) |  | 0 |  | T. Mitchell Roddenberry, Vishwanath Saragadam, Maarten V. de Hoop, Richard G. Baraniuk |  |
| 887 |  |  [Fiber Monte Carlo](https://openreview.net/forum?id=sP1tCl2QBk) |  | 0 |  | Nick Richardson, Deniz Oktay, Yaniv Ovadia, James C. Bowden, Ryan P. Adams |  |
| 888 |  |  [Learning from Label Proportions: Bootstrapping Supervised Learners via Belief Propagation](https://openreview.net/forum?id=KQe9tHd0k8) |  | 0 |  | Shreyas Havaldar, Navodita Sharma, Shubhi Sareen, Karthikeyan Shanmugam, Aravindan Raghuveer |  |
| 889 |  |  [Modeling state-dependent communication between brain regions with switching nonlinear dynamical systems](https://openreview.net/forum?id=WQwV7Y8qwa) |  | 0 |  | Orren KarniolTambour, David M. Zoltowski, E. Mika Diamanti, Lucas Pinto, Carlos D. Brody, David W. Tank, Jonathan W. Pillow |  |
| 890 |  |  [Neural Language of Thought Models](https://openreview.net/forum?id=HYyRwm367m) |  | 0 |  | YiFu Wu, Minseung Lee, Sungjin Ahn |  |
| 891 |  |  [Retrieval-Guided Reinforcement Learning for Boolean Circuit Minimization](https://openreview.net/forum?id=0t1O8ziRZp) |  | 0 |  | Animesh Basak Chowdhury, Marco Romanelli, Benjamin Tan, Ramesh Karri, Siddharth Garg |  |
| 892 |  |  [Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting](https://openreview.net/forum?id=ztpy1gsUpT) |  | 0 |  | Xinlu Zhang, Shiyang Li, Xianjun Yang, Chenxin Tian, Yao Qin, Linda Ruth Petzold |  |
| 893 |  |  [What Makes a Good Prune? Maximal Unstructured Pruning for Maximal Cosine Similarity](https://openreview.net/forum?id=jsvvPVVzwf) |  | 0 |  | Gabryel MasonWilliams, Fredrik Dahlqvist |  |
| 894 |  |  [Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain](https://openreview.net/forum?id=BqEvdOS1Hs) |  | 0 |  | Yiming Gao, Feiyu Liu, Liang Wang, Dehua Zheng, Zhenjie Lian, Weixuan Wang, Wenjin Yang, Siqin Li, Xianliang Wang, Wenhui Chen, Jing Dai, Qiang Fu, Wei Yang, Lanxiao Huang, Wei Liu |  |
| 895 |  |  [Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis](https://openreview.net/forum?id=vY9nzQmQBw) |  | 0 |  | Hubert Siuzdak |  |
| 896 |  |  [An Agnostic View on the Cost of Overfitting in (Kernel) Ridge Regression](https://openreview.net/forum?id=YrTI2Zu0dd) |  | 0 |  | Lijia Zhou, James B. Simon, Gal Vardi, Nathan Srebro |  |
| 897 |  |  [Explaining Kernel Clustering via Decision Trees](https://openreview.net/forum?id=FAGtjl7HOw) |  | 0 |  | Maximilian Fleissner, Leena Chennuru Vankadara, Debarghya Ghoshdastidar |  |
| 898 |  |  [Provable Benefits of Multi-task RL under Non-Markovian Decision Making Processes](https://openreview.net/forum?id=U6Qulbv2qT) |  | 0 |  | Ruiquan Huang, Yuan Cheng, Jing Yang, Vincent Tan, Yingbin Liang |  |
| 899 |  |  [Learning Delays in Spiking Neural Networks using Dilated Convolutions with Learnable Spacings](https://openreview.net/forum?id=4r2ybzJnmN) |  | 0 |  | Ilyass Hammouamri, Ismail Khalfaoui Hassani, Timothée Masquelier |  |
| 900 |  |  [The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"](https://openreview.net/forum?id=GPKTIktA0k) |  | 0 |  | Lukas Berglund, Meg Tong, Maximilian Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, Owain Evans |  |
| 901 |  |  [AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models](https://openreview.net/forum?id=7Jwpw4qKkb) |  | 0 |  | Xiaogeng Liu, Nan Xu, Muhao Chen, Chaowei Xiao |  |
| 902 |  |  [MixSATGEN: Learning Graph Mixing for SAT Instance Generation](https://openreview.net/forum?id=PXXuLvIH5r) |  | 0 |  | Xinyan Chen, Yang Li, Runzhong Wang, Junchi Yan |  |
| 903 |  |  [A Stable, Fast, and Fully Automatic Learning Algorithm for Predictive Coding Networks](https://openreview.net/forum?id=RyUvzda8GH) |  | 0 |  | Tommaso Salvatori, Yuhang Song, Yordan Yordanov, Beren Millidge, Lei Sha, Cornelius Emde, Zhenghua Xu, Rafal Bogacz, Thomas Lukasiewicz |  |
| 904 |  |  [Scalable Monotonic Neural Networks](https://openreview.net/forum?id=DjIsNDEOYX) |  | 0 |  | Hyunho Kim, JongSeok Lee |  |
| 905 |  |  [Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models](https://openreview.net/forum?id=s2NjWfaYdZ) |  | 0 |  | Seungcheol Park, Hojun Choi, U Kang |  |
| 906 |  |  [PROGRAM: PROtotype GRAph Model based Pseudo-Label Learning for Test-Time Adaptation](https://openreview.net/forum?id=x5LvBK43wg) |  | 0 |  | Haopeng Sun, Lumin Xu, Sheng Jin, Ping Luo, Chen Qian, Wentao Liu |  |
| 907 |  |  [Navigating Dataset Documentations in AI: A Large-Scale Analysis of Dataset Cards on HuggingFace](https://openreview.net/forum?id=xC8xh2RSs2) |  | 0 |  | Xinyu Yang, Weixin Liang, James Zou |  |
| 908 |  |  [A Differentially Private Clustering Algorithm for Well-Clustered Graphs](https://openreview.net/forum?id=hkSjjs4o5d) |  | 0 |  | Weiqiang He, Hendrik Fichtenberger, Pan Peng |  |
| 909 |  |  [Minimax optimality of convolutional neural networks for infinite dimensional input-output problems and separation from kernel methods](https://openreview.net/forum?id=EW8ZExRZkJ) |  | 0 |  | Yuto Nishimura, Taiji Suzuki |  |
| 910 |  |  [Consistent algorithms for multi-label classification with macro-at-k metrics](https://openreview.net/forum?id=XOnya9gSdF) |  | 0 |  | Erik Schultheis, Wojciech Kotlowski, Marek Wydmuch, Rohit Babbar, Strom Borman, Krzysztof Dembczynski |  |
| 911 |  |  [Dynamic Layer Tying for Parameter-Efficient Transformers](https://openreview.net/forum?id=d4uL2MSe0z) |  | 0 |  | Tamir David Hay, Lior Wolf |  |
| 912 |  |  [Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion](https://openreview.net/forum?id=ymjI8feDTD) |  | 0 |  | Dongjun Kim, ChiehHsin Lai, WeiHsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, Stefano Ermon |  |
| 913 |  |  [Dissecting Sample Hardness: A Fine-Grained Analysis of Hardness Characterization Methods for Data-Centric AI](https://openreview.net/forum?id=icTZCUbtD6) |  | 0 |  | Nabeel Seedat, Fergus Imrie, Mihaela van der Schaar |  |
| 914 |  |  [Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency](https://openreview.net/forum?id=1OfAO2mes1) |  | 0 |  | Soumyadeep Pal, Yuguang Yao, Ren Wang, Bingquan Shen, Sijia Liu |  |
| 915 |  |  [Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness Constants](https://openreview.net/forum?id=Ch7WqGcGmb) |  | 0 |  | Peter Richtárik, Elnur Gasanov, Konstantin Burlachenko |  |
| 916 |  |  [Deep SE(3)-Equivariant Geometric Reasoning for Precise Placement Tasks](https://openreview.net/forum?id=2inBuwTyL2) |  | 0 |  | Ben Eisner, Yi Yang, Todor Davchev, Mel Vecerík, Jonathan Scholz, David Held |  |
| 917 |  |  [Lemur: Integrating Large Language Models in Automated Program Verification](https://openreview.net/forum?id=Q3YaCghZNt) |  | 0 |  | Haoze Wu, Clark W. Barrett, Nina Narodytska |  |
| 918 |  |  [ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models](https://openreview.net/forum?id=liuqDwmbQJ) |  | 0 |  | Ilker Kesen, Andrea Pedrotti, Mustafa Dogan, Michele Cafagna, Emre Can Acikgoz, Letitia Parcalabescu, Iacer Calixto, Anette Frank, Albert Gatt, Aykut Erdem, Erkut Erdem |  |
| 919 |  |  [A Precise Characterization of SGD Stability Using Loss Surface Geometry](https://openreview.net/forum?id=UMOlFJzLfL) |  | 0 |  | Gregory Dexter, Borja Ocejo, S. Sathiya Keerthi, Aman Gupta, Ayan Acharya, Rajiv Khanna |  |
| 920 |  |  [CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models](https://openreview.net/forum?id=86NGO8qeWs) |  | 0 |  | Sreyan Ghosh, Ashish Seth, Sonal Kumar, Utkarsh Tyagi, Chandra Kiran Reddy Evuru, Ramaneswaran S., Sakshi Singh, Oriol Nieto, Ramani Duraiswami, Dinesh Manocha |  |
| 921 |  |  [Hierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMs](https://openreview.net/forum?id=ulaUJFd96G) |  | 0 |  | Woomin Song, Seunghyuk Oh, Sangwoo Mo, Jaehyung Kim, Sukmin Yun, JungWoo Ha, Jinwoo Shin |  |
| 922 |  |  [Diffusion in Diffusion: Cyclic One-Way Diffusion for Text-Vision-Conditioned Generation](https://openreview.net/forum?id=ePOjNlOjLC) |  | 0 |  | Ruoyu Wang, Yongqi Yang, Zhihao Qian, Ye Zhu, Yu Wu |  |
| 923 |  |  [Bounding the Expected Robustness of Graph Neural Networks Subject to Node Feature Attacks](https://openreview.net/forum?id=DfPtC8uSot) |  | 0 |  | Yassine Abbahaddou, Sofiane Ennadir, Johannes F. Lutzeyer, Michalis Vazirgiannis, Henrik Boström |  |
| 924 |  |  [Score Models for Offline Goal-Conditioned Reinforcement Learning](https://openreview.net/forum?id=oXjnwQLcTA) |  | 0 |  | Harshit Sikchi, Rohan Chitnis, Ahmed Touati, Alborz Geramifard, Amy Zhang, Scott Niekum |  |
| 925 |  |  [Hiding in Plain Sight: Disguising Data Stealing Attacks in Federated Learning](https://openreview.net/forum?id=krx55l2A6G) |  | 0 |  | Kostadin Garov, Dimitar Iliev Dimitrov, Nikola Jovanovic, Martin T. Vechev |  |
| 926 |  |  [USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields](https://openreview.net/forum?id=igfDXfMvm5) |  | 0 |  | Moyang Li, Peng Wang, Lingzhe Zhao, Bangyan Liao, Peidong Liu |  |
| 927 |  |  [Supervised Knowledge Makes Large Language Models Better In-context Learners](https://openreview.net/forum?id=bAMPOUF227) |  | 0 |  | Linyi Yang, Shuibai Zhang, Zhuohao Yu, Guangsheng Bao, Yidong Wang, Jindong Wang, Ruochen Xu, Wei Ye, Xing Xie, Weizhu Chen, Yue Zhang |  |
| 928 |  |  [COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via Probabilistic Circuits](https://openreview.net/forum?id=XN6ZPINdSg) |  | 0 |  | Mintong Kang, Nezihe Merve Gürel, Linyi Li, Bo Li |  |
| 929 |  |  [Contrastive Difference Predictive Coding](https://openreview.net/forum?id=0akLDTFR9x) |  | 0 |  | Chongyi Zheng, Ruslan Salakhutdinov, Benjamin Eysenbach |  |
| 930 |  |  [Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment](https://openreview.net/forum?id=LNLjU5C5dK) |  | 0 |  | Geyang Guo, Ranchi Zhao, Tianyi Tang, Xin Zhao, JiRong Wen |  |
| 931 |  |  [Effective Data Augmentation With Diffusion Models](https://openreview.net/forum?id=ZWzUA9zeAg) |  | 0 |  | Brandon Trabucco, Kyle Doherty, Max Gurinas, Ruslan Salakhutdinov |  |
| 932 |  |  [Towards Transparent Time Series Forecasting](https://openreview.net/forum?id=TYXtXLYHpR) |  | 0 |  | Krzysztof Kacprzyk, Tennison Liu, Mihaela van der Schaar |  |
| 933 |  |  [A Fast and Provable Algorithm for Sparse Phase Retrieval](https://openreview.net/forum?id=BlkxbI6vzl) |  | 0 |  | JianFeng Cai, Yu Long, Ruixue Wen, Jiaxi Ying |  |
| 934 |  |  [MCM: Masked Cell Modeling for Anomaly Detection in Tabular Data](https://openreview.net/forum?id=lNZJyEDxy4) |  | 0 |  | Jiaxin Yin, Yuanyuan Qiao, Zitang Zhou, Xiangchao Wang, Jie Yang |  |
| 935 |  |  [HiGen: Hierarchical Graph Generative Networks](https://openreview.net/forum?id=KNvubydSB5) |  | 0 |  | Mahdi Karami |  |
| 936 |  |  [A Policy Gradient Method for Confounded POMDPs](https://openreview.net/forum?id=8BAkNCqpGW) |  | 0 |  | Mao Hong, Zhengling Qi, Yanxun Xu |  |
| 937 |  |  [Achieving Fairness in Multi-Agent MDP Using Reinforcement Learning](https://openreview.net/forum?id=yoVq2BGQdP) |  | 0 |  | Peizhong Ju, Arnob Ghosh, Ness B. Shroff |  |
| 938 |  |  [Escape Sky-high Cost: Early-stopping Self-Consistency for Multi-step Reasoning](https://openreview.net/forum?id=ndR8Ytrzhh) |  | 0 |  | Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Xinglin Wang, Bin Sun, Heda Wang, Kan Li |  |
| 939 |  |  [REBAR: Retrieval-Based Reconstruction for Time-series Contrastive Learning](https://openreview.net/forum?id=3zQo5oUvia) |  | 0 |  | Maxwell A. Xu, Alexander Moreno, Hui Wei, Benjamin M. Marlin, James Matthew Rehg |  |
| 940 |  |  [CoLiDE: Concomitant Linear DAG Estimation](https://openreview.net/forum?id=fGAIgO75dG) |  | 0 |  | Seyed Saman Saboksayr, Gonzalo Mateos, Mariano Tepper |  |
| 941 |  |  [Scaling Convex Neural Networks with Burer-Monteiro Factorization](https://openreview.net/forum?id=ikmuHqugN7) |  | 0 |  | Arda Sahiner, Tolga Ergen, Batu Ozturkler, John M. Pauly, Morteza Mardani, Mert Pilanci |  |
| 942 |  |  [UniTabE: A Universal Pretraining Protocol for Tabular Foundation Model in Data Science](https://openreview.net/forum?id=6LLho5X6xV) |  | 0 |  | Yazheng Yang, Yuqi Wang, Guang Liu, Ledell Wu, Qi Liu |  |
| 943 |  |  [PolyVoice: Language Models for Speech to Speech Translation](https://openreview.net/forum?id=hCrFG9cyuC) |  | 0 |  | Qianqian Dong, Zhiying Huang, Qi Tian, Chen Xu, Tom Ko, Yunlong Zhao, Siyuan Feng, Tang Li, Kexin Wang, Xuxin Cheng, Fengpeng Yue, Ye Bai, Xi Chen, Lu Lu, Zejun Ma, Yuping Wang, Mingxuan Wang, Yuxuan Wang |  |
| 944 |  |  [Adversarial Feature Map Pruning for Backdoor](https://openreview.net/forum?id=IOEEDkla96) |  | 0 |  | Dong Huang, Qingwen Bu |  |
| 945 |  |  [Expressivity of ReLU-Networks under Convex Relaxations](https://openreview.net/forum?id=awHTL3Hpto) |  | 0 |  | Maximilian Baader, Mark Niklas Müller, Yuhao Mao, Martin T. Vechev |  |
| 946 |  |  [EmerDiff: Emerging Pixel-level Semantic Knowledge in Diffusion Models](https://openreview.net/forum?id=YqyTXmF8Y2) |  | 0 |  | Koichi Namekata, Amirmojtaba Sabour, Sanja Fidler, Seung Wook Kim |  |
| 947 |  |  [CLEX: Continuous Length Extrapolation for Large Language Models](https://openreview.net/forum?id=wXpSidPpc5) |  | 0 |  | Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, Lidong Bing |  |
| 948 |  |  [Implicit Gaussian process representation of vector fields over arbitrary latent manifolds](https://openreview.net/forum?id=YEPlTU5mZC) |  | 0 |  | Robert L. Peach, Matteo VinaoCarl, Nir Grossman, Michael David, Emma Mallas, David J. Sharp, Paresh A. Malhotra, Pierre Vandergheynst, Adam Gosztolai |  |
| 949 |  |  [Logical Languages Accepted by Transformer Encoders with Hard Attention](https://openreview.net/forum?id=gbrHZq07mq) |  | 0 |  | Pablo Barceló, Alexander Kozachinskiy, Anthony Widjaja Lin, Vladimir V. Podolskii |  |
| 950 |  |  [FairSeg: A Large-Scale Medical Image Segmentation Dataset for Fairness Learning Using Segment Anything Model with Fair Error-Bound Scaling](https://openreview.net/forum?id=qNrJJZAKI3) |  | 0 |  | Yu Tian, Min Shi, Yan Luo, Ava Kouhana, Tobias Elze, Mengyu Wang |  |
| 951 |  |  [Customizable Combination of Parameter-Efficient Modules for Multi-Task Learning](https://openreview.net/forum?id=G1Hlubz1fR) |  | 0 |  | Haowen Wang, Tao Sun, Congyun Jin, Yingbo Wang, Yibo Fan, Yunqi Xu, Yuliang Du, Cong Fan |  |
| 952 |  |  [InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists](https://openreview.net/forum?id=Nu9mOSq7eH) |  | 0 |  | Yulu Gan, Sungwoo Park, Alexander Schubert, Anthony Philippakis, Ahmed M. Alaa |  |
| 953 |  |  [Mitigating Emergent Robustness Degradation while Scaling Graph Learning](https://openreview.net/forum?id=Koh0i2u8qX) |  | 0 |  | Xiangchi Yuan, Chunhui Zhang, Yijun Tian, Yanfang Ye, Chuxu Zhang |  |
| 954 |  |  [Traveling Waves Encode The Recent Past and Enhance Sequence Learning](https://openreview.net/forum?id=p4S5Z6Sah4) |  | 0 |  | T. Anderson Keller, Lyle Muller, Terrence J. Sejnowski, Max Welling |  |
| 955 |  |  [Conserve-Update-Revise to Cure Generalization and Robustness Trade-off in Adversarial Training](https://openreview.net/forum?id=6IjN7oxjXt) |  | 0 |  | Shruthi Gowda, Bahram Zonooz, Elahe Arani |  |
| 956 |  |  [Hindsight PRIORs for Reward Learning from Human Preferences](https://openreview.net/forum?id=NLevOah0CJ) |  | 0 |  | Mudit Verma, Katherine Metcalf |  |
| 957 |  |  [Navigating the Design Space of Equivariant Diffusion-Based Generative Models for De Novo 3D Molecule Generation](https://openreview.net/forum?id=kzGuiRXZrQ) |  | 0 |  | Tuan Le, Julian Cremer, Frank Noé, DjorkArné Clevert, Kristof T. Schütt |  |
| 958 |  |  [LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation](https://openreview.net/forum?id=BqHaLnans2) |  | 0 |  | Suhyeon Lee, Won Jun Kim, Jinho Chang, Jong Chul Ye |  |
| 959 |  |  [Can We Evaluate Domain Adaptation Models Without Target-Domain Labels?](https://openreview.net/forum?id=fszrlQ2DuP) |  | 0 |  | Jianfei Yang, Hanjie Qian, Yuecong Xu, Kai Wang, Lihua Xie |  |
| 960 |  |  [Global Optimality for Non-linear Constrained Restoration Problems via Invexity](https://openreview.net/forum?id=fyTPWfXtcc) |  | 0 |  | Samuel Pinilla, Jeyan Thiyagalingam |  |
| 961 |  |  [DOS: Diverse Outlier Sampling for Out-of-Distribution Detection](https://openreview.net/forum?id=iriEqxFB4y) |  | 0 |  | Wenyu Jiang, Hao Cheng, Mingcai Chen, Chongjun Wang, Hongxin Wei |  |
| 962 |  |  [Denoising Task Routing for Diffusion Models](https://openreview.net/forum?id=MY0qlcFcUg) |  | 0 |  | Byeongjun Park, Sangmin Woo, Hyojun Go, JinYoung Kim, Changick Kim |  |
| 963 |  |  [Reward Model Ensembles Help Mitigate Overoptimization](https://openreview.net/forum?id=dcjtMYkpXx) |  | 0 |  | Thomas Coste, Usman Anwar, Robert Kirk, David Krueger |  |
| 964 |  |  [Frequency-Aware Transformer for Learned Image Compression](https://openreview.net/forum?id=HKGQDDTuvZ) |  | 0 |  | Han Li, Shaohui Li, Wenrui Dai, Chenglin Li, Junni Zou, Hongkai Xiong |  |
| 965 |  |  [CircuitNet 2.0: An Advanced Dataset for Promoting Machine Learning Innovations in Realistic Chip Design Environment](https://openreview.net/forum?id=nMFSUjxMIl) |  | 0 |  | Xun Jiang, Zhuomin Chai, Yuxiang Zhao, Yibo Lin, Runsheng Wang, Ru Huang |  |
| 966 |  |  [Leveraging Hyperbolic Embeddings for Coarse-to-Fine Robot Design](https://openreview.net/forum?id=q9jQPA6zPK) |  | 0 |  | Heng Dong, Junyu Zhang, Chongjie Zhang |  |
| 967 |  |  [GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction](https://openreview.net/forum?id=Y3wpuxd7u9) |  | 0 |  | Oscar Sainz, Iker GarcíaFerrero, Rodrigo Agerri, Oier Lopez de Lacalle, German Rigau, Eneko Agirre |  |
| 968 |  |  [Understanding the Robustness of Randomized Feature Defense Against Query-Based Adversarial Attacks](https://openreview.net/forum?id=vZ6r9GMT1n) |  | 0 |  | Nguyen HungQuang, Yingjie Lao, Tung Pham, KokSeng Wong, Khoa D. Doan |  |
| 969 |  |  [Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing](https://openreview.net/forum?id=02f3mUtqnM) |  | 0 |  | Dujian Ding, Ankur Mallick, Chi Wang, Robert Sim, Subhabrata Mukherjee, Victor Rühle, Laks V. S. Lakshmanan, Ahmed Hassan Awadallah |  |
| 970 |  |  [Identifying Representations for Intervention Extrapolation](https://openreview.net/forum?id=3cuJwmPxXj) |  | 0 |  | Sorawit Saengkyongam, Elan Rosenfeld, Pradeep Kumar Ravikumar, Niklas Pfister, Jonas Peters |  |
| 971 |  |  [Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model](https://openreview.net/forum?id=j5JvZCaDM0) |  | 0 |  | Yinan Zheng, Jianxiong Li, Dongjie Yu, Yujie Yang, Shengbo Eben Li, Xianyuan Zhan, Jingjing Liu |  |
| 972 |  |  [Do Generated Data Always Help Contrastive Learning?](https://openreview.net/forum?id=S5EqslEHnz) |  | 0 |  | Yifei Wang, Jizhe Zhang, Yisen Wang |  |
| 973 |  |  [ASMR: Activation-Sharing Multi-Resolution Coordinate Networks for Efficient Inference](https://openreview.net/forum?id=kMp8zCsXNb) |  | 0 |  | Jason Chun Lok Li, Steven Tin Sui Luo, Le Xu, Ngai Wong |  |
| 974 |  |  [Exploring Weight Balancing on Long-Tailed Recognition Problem](https://openreview.net/forum?id=JsnR0YO4Fq) |  | 0 |  | Naoya Hasegawa, Issei Sato |  |
| 975 |  |  [Zero Bubble (Almost) Pipeline Parallelism](https://openreview.net/forum?id=tuzTN0eIO5) |  | 0 |  | Penghui Qi, Xinyi Wan, Guangxing Huang, Min Lin |  |
| 976 |  |  [CoRe-GD: A Hierarchical Framework for Scalable Graph Visualization with GNNs](https://openreview.net/forum?id=vtyasLn4RM) |  | 0 |  | Florian Grötschla, Joël Mathys, Robert Veres, Roger Wattenhofer |  |
| 977 |  |  [Compositional Conservatism: A Transductive Approach in Offline Reinforcement Learning](https://openreview.net/forum?id=HRkyLbBRHI) |  | 0 |  | Yeda Song, Dongwook Lee, Gunhee Kim |  |
| 978 |  |  [Flow to Better: Offline Preference-based Reinforcement Learning via Preferred Trajectory Generation](https://openreview.net/forum?id=EG68RSznLT) |  | 0 |  | Zhilong Zhang, Yihao Sun, Junyin Ye, TianShuo Liu, Jiaji Zhang, Yang Yu |  |
| 979 |  |  [Kernelised Normalising Flows](https://openreview.net/forum?id=iTFdNLHE7k) |  | 0 |  | Eshant English, Matthias Kirchler, Christoph Lippert |  |
| 980 |  |  [Topic Modeling as Multi-Objective Contrastive Optimization](https://openreview.net/forum?id=HdAoLSBYXj) |  | 0 |  | Thong Thanh Nguyen, Xiaobao Wu, Xinshuai Dong, CongDuy T. Nguyen, SeeKiong Ng, Anh Tuan Luu |  |
| 981 |  |  [ED-NeRF: Efficient Text-Guided Editing of 3D Scene With Latent Space NeRF](https://openreview.net/forum?id=9DvDRTTdlu) |  | 0 |  | Jangho Park, Gihyun Kwon, Jong Chul Ye |  |
| 982 |  |  [Learning Personalized Causally Invariant Representations for Heterogeneous Federated Clients](https://openreview.net/forum?id=8FHWkY0SwF) |  | 0 |  | Xueyang Tang, Song Guo, Jie Zhang, Jingcai Guo |  |
| 983 |  |  [PAE: Reinforcement Learning from External Knowledge for Efficient Exploration](https://openreview.net/forum?id=R7rZUSGOPD) |  | 0 |  | Zhe Wu, Haofei Lu, Junliang Xing, You Wu, Renye Yan, Yaozhong Gan, Yuanchun Shi |  |
| 984 |  |  [Advancing Pose-Guided Image Synthesis with Progressive Conditional Diffusion Models](https://openreview.net/forum?id=rHzapPnCgT) |  | 0 |  | Fei Shen, Hu Ye, Jun Zhang, Cong Wang, Xiao Han, Yang Wei |  |
| 985 |  |  [One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention](https://openreview.net/forum?id=8p3fu56lKc) |  | 0 |  | Arvind V. Mahankali, Tatsunori Hashimoto, Tengyu Ma |  |
| 986 |  |  [Prometheus: Inducing Fine-Grained Evaluation Capability in Language Models](https://openreview.net/forum?id=8euJaTveKw) |  | 0 |  | Seungone Kim, Jamin Shin, Yejin Choi, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, Minjoon Seo |  |
| 987 |  |  [Querying Easily Flip-flopped Samples for Deep Active Learning](https://openreview.net/forum?id=THUBTfSAS2) |  | 0 |  | Seong Jin Cho, Gwangsu Kim, Junghyun Lee, Jinwoo Shin, Chang D. Yoo |  |
| 988 |  |  [Attention-based Iterative Decomposition for Tensor Product Representation](https://openreview.net/forum?id=FDb2JQZsFH) |  | 0 |  | Taewon Park, Inchul Choi, Minho Lee |  |
| 989 |  |  [Efficient Continual Finite-Sum Minimization](https://openreview.net/forum?id=RR70yWYenC) |  | 0 |  | Ioannis Mavrothalassitis, Stratis Skoulakis, Leello Tadesse Dadi, Volkan Cevher |  |
| 990 |  |  [BroGNet: Momentum-Conserving Graph Neural Stochastic Differential Equation for Learning Brownian Dynamics](https://openreview.net/forum?id=2iGiSHmeAN) |  | 0 |  | Suresh Bishnoi, Jayadeva, Sayan Ranu, N. M. Anoop Krishnan |  |
| 991 |  |  [Some Fundamental Aspects about Lipschitz Continuity of Neural Networks](https://openreview.net/forum?id=5jWsW08zUh) |  | 0 |  | Grigory Khromov, Sidak Pal Singh |  |
| 992 |  |  [Evaluating Language Model Agency Through Negotiations](https://openreview.net/forum?id=3ZqKxMHcAg) |  | 0 |  | Tim R. Davidson, Veniamin Veselovsky, Michal Kosinski, Robert West |  |
| 993 |  |  [Making Retrieval-Augmented Language Models Robust to Irrelevant Context](https://openreview.net/forum?id=ZS4m74kZpH) |  | 0 |  | Ori Yoran, Tomer Wolfson, Ori Ram, Jonathan Berant |  |
| 994 |  |  [VersVideo: Leveraging Enhanced Temporal Diffusion Models for Versatile Video Generation](https://openreview.net/forum?id=K9sVJ17zvB) |  | 0 |  | Jinxi Xiang, Ricong Huang, Jun Zhang, Guanbin Li, Xiao Han, Yang Wei |  |
| 995 |  |  [Label-Agnostic Forgetting: A Supervision-Free Unlearning in Deep Models](https://openreview.net/forum?id=SIZWiya7FE) |  | 0 |  | Shaofei Shen, Chenhao Zhang, Yawen Zhao, Alina Bialkowski, Weitong Chen, Miao Xu |  |
| 996 |  |  [Controlling Vision-Language Models for Multi-Task Image Restoration](https://openreview.net/forum?id=t3vnnLeajU) |  | 0 |  | Ziwei Luo, Fredrik K. Gustafsson, Zheng Zhao, Jens Sjölund, Thomas B. Schön |  |
| 997 |  |  [Yet Another ICU Benchmark: A Flexible Multi-Center Framework for Clinical ML](https://openreview.net/forum?id=ox2ATRM90I) |  | 0 |  | Robin Van De Water, Hendrik Schmidt, Paul W. G. Elbers, Patrick Thoral, Bert Arnrich, Patrick Rockenschaub |  |
| 998 |  |  [Identifying Policy Gradient Subspaces](https://openreview.net/forum?id=iPWxqnt2ke) |  | 0 |  | Jan Schneider, Pierre Schumacher, Simon Guist, Le Chen, Daniel F. B. Haeufle, Bernhard Schölkopf, Dieter Büchler |  |
| 999 |  |  [Training Graph Transformers via Curriculum-Enhanced Attention Distillation](https://openreview.net/forum?id=j4VMrwgn1M) |  | 0 |  | Yisong Huang, Jin Li, Xinlong Chen, YangGeng Fu |  |
| 1000 |  |  [Adaptive Stochastic Gradient Algorithm for Black-box Multi-Objective Learning](https://openreview.net/forum?id=bm1JVsVZVu) |  | 0 |  | Feiyang Ye, Yueming Lyu, Xuehao Wang, Yu Zhang, Ivor W. Tsang |  |
| 1001 |  |  [AgentBench: Evaluating LLMs as Agents](https://openreview.net/forum?id=zAdUB0aCTQ) |  | 0 |  | Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, Jie Tang |  |
| 1002 |  |  [Image Background Serves as Good Proxy for Out-of-distribution Data](https://openreview.net/forum?id=ym0ubZrsmm) |  | 0 |  | Sen Pei |  |
| 1003 |  |  [Differentially Private Synthetic Data via Foundation Model APIs 1: Images](https://openreview.net/forum?id=YEhQs8POIo) |  | 0 |  | Zinan Lin, Sivakanth Gopi, Janardhan Kulkarni, Harsha Nori, Sergey Yekhanin |  |
| 1004 |  |  [Harnessing Joint Rain-/Detail-aware Representations to Eliminate Intricate Rains](https://openreview.net/forum?id=pdJXYfJjz9) |  | 0 |  | Wu Ran, Peirong Ma, Zhiquan He, Hao Ren, Hong Lu |  |
| 1005 |  |  [Beyond IID weights: sparse and low-rank deep Neural Networks are also Gaussian Processes](https://openreview.net/forum?id=1Wi0Ys33Nm) |  | 0 |  | Thiziri Nait Saada, Alireza Naderi, Jared Tanner |  |
| 1006 |  |  [Understanding Addition in Transformers](https://openreview.net/forum?id=rIx1YXVWZb) |  | 0 |  | Philip Quirke, Fazl Barez |  |
| 1007 |  |  [Beating Price of Anarchy and Gradient Descent without Regret in Potential Games](https://openreview.net/forum?id=36L7W3ri4U) |  | 0 |  | Iosif Sakos, Stefanos Leonardos, Stelios Andrew Stavroulakis, Will Overman, Ioannis Panageas, Georgios Piliouras |  |
| 1008 |  |  [In defense of parameter sharing for model-compression](https://openreview.net/forum?id=ypAT2ixD4X) |  | 0 |  | Aditya Desai, Anshumali Shrivastava |  |
| 1009 |  |  [Kill Two Birds with One Stone: Rethinking Data Augmentation for Deep Long-tailed Learning](https://openreview.net/forum?id=RzY9qQHUXy) |  | 0 |  | Binwu Wang, Pengkun Wang, Wei Xu, Xu Wang, Yudong Zhang, Kun Wang, Yang Wang |  |
| 1010 |  |  [Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents](https://openreview.net/forum?id=MCNqgUFTHI) |  | 0 |  | Yang Deng, Wenxuan Zhang, Wai Lam, SeeKiong Ng, TatSeng Chua |  |
| 1011 |  |  [Bootstrapping Variational Information Pursuit with Large Language and Vision Models for Interpretable Image Classification](https://openreview.net/forum?id=9bmTbVaA2A) |  | 0 |  | Aditya Chattopadhyay, Kwan Ho Ryan Chan, René Vidal |  |
| 1012 |  |  [Contextual Bandits with Online Neural Regression](https://openreview.net/forum?id=5ep85sakT3) |  | 0 |  | Rohan Deb, Yikun Ban, Shiliang Zuo, Jingrui He, Arindam Banerjee |  |
| 1013 |  |  [Evaluating Large Language Models at Evaluating Instruction Following](https://openreview.net/forum?id=tr0KidwPLc) |  | 0 |  | Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, Danqi Chen |  |
| 1014 |  |  [How to Fine-Tune Vision Models with SGD](https://openreview.net/forum?id=ZTssMmhC2X) |  | 0 |  | Ananya Kumar, Ruoqi Shen, Sébastien Bubeck, Suriya Gunasekar |  |
| 1015 |  |  [Backdoor Contrastive Learning via Bi-level Trigger Optimization](https://openreview.net/forum?id=oxjeePpgSP) |  | 0 |  | Weiyu Sun, Xinyu Zhang, Hao Lu, YingCong Chen, Ting Wang, Jinghui Chen, Lu Lin |  |
| 1016 |  |  [PARL: A Unified Framework for Policy Alignment in Reinforcement Learning from Human Feedback](https://openreview.net/forum?id=ByR3NdDSZB) |  | 0 |  | Souradip Chakraborty, Amrit Singh Bedi, Alec Koppel, Huazheng Wang, Dinesh Manocha, Mengdi Wang, Furong Huang |  |
| 1017 |  |  [SafeDreamer: Safe Reinforcement Learning with World Models](https://openreview.net/forum?id=tsE5HLYtYg) |  | 0 |  | Weidong Huang, Jiaming Ji, Chunhe Xia, Borong Zhang, Yaodong Yang |  |
| 1018 |  |  [MetaCoCo: A New Few-Shot Classification Benchmark with Spurious Correlation](https://openreview.net/forum?id=DiWRG9JTWZ) |  | 0 |  | Min Zhang, Haoxuan Li, Fei Wu, Kun Kuang |  |
| 1019 |  |  [Whittle Index with Multiple Actions and State Constraint for Inventory Management](https://openreview.net/forum?id=5sixirvG0I) |  | 0 |  | Chuheng Zhang, Xiangsen Wang, Wei Jiang, Xianliang Yang, Siwei Wang, Lei Song, Jiang Bian |  |
| 1020 |  |  [Looped Transformers are Better at Learning Learning Algorithms](https://openreview.net/forum?id=HHbRxoDTxE) |  | 0 |  | Liu Yang, Kangwook Lee, Robert D. Nowak, Dimitris Papailiopoulos |  |
| 1021 |  |  [Sum-Product-Set Networks: Deep Tractable Models for Tree-Structured Graphs](https://openreview.net/forum?id=mF3cTns4pe) |  | 0 |  | Milan Papez, Martin Rektoris, Václav Smídl, Tomás Pevný |  |
| 1022 |  |  [Learning Thresholds with Latent Values and Censored Feedback](https://openreview.net/forum?id=qaKRfobbTg) |  | 0 |  | Jiahao Zhang, Tao Lin, Weiqiang Zheng, Zhe Feng, Yifeng Teng, Xiaotie Deng |  |
| 1023 |  |  [Is attention required for ICL? Exploring the Relationship Between Model Architecture and In-Context Learning Ability](https://openreview.net/forum?id=Qwq4cpLtoX) |  | 0 |  | Ivan Lee, Nan Jiang, Taylor BergKirkpatrick |  |
| 1024 |  |  [Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?](https://openreview.net/forum?id=nJnky5K944) |  | 0 |  | Tokio Kajitsuka, Issei Sato |  |
| 1025 |  |  [Language-Interfaced Tabular Oversampling via Progressive Imputation and Self-Authentication](https://openreview.net/forum?id=8F6bws5JBy) |  | 0 |  | June Yong Yang, Geondo Park, Joowon Kim, Hyeongwon Jang, Eunho Yang |  |
| 1026 |  |  [Certified Adversarial Robustness for Rate Encoded Spiking Neural Networks](https://openreview.net/forum?id=5bNYf0CqxY) |  | 0 |  | Bhaskar Mukhoty, Hilal AlQuabeh, Giulia De Masi, Huan Xiong, Bin Gu |  |
| 1027 |  |  [Fake It Till Make It: Federated Learning with Consensus-Oriented Generation](https://openreview.net/forum?id=NY3wMJuaLf) |  | 0 |  | Rui Ye, Yaxin Du, Zhenyang Ni, Yanfeng Wang, Siheng Chen |  |
| 1028 |  |  [SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction](https://openreview.net/forum?id=FNq3nIvP4F) |  | 0 |  | Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, Ziwei Liu |  |
| 1029 |  |  [Explaining Time Series via Contrastive and Locally Sparse Perturbations](https://openreview.net/forum?id=qDdSRaOiyb) |  | 0 |  | Zichuan Liu, Yingying Zhang, Tianchun Wang, Zefan Wang, Dongsheng Luo, Mengnan Du, Min Wu, Yi Wang, Chunlin Chen, Lunting Fan, Qingsong Wen |  |
| 1030 |  |  [GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking](https://openreview.net/forum?id=VJvbOSXRUq) |  | 0 |  | Mert Kosan, Samidha Verma, Burouj Armgaan, Khushbu Pahwa, Ambuj K. Singh, Sourav Medya, Sayan Ranu |  |
| 1031 |  |  [Grounded Object-Centric Learning](https://openreview.net/forum?id=pBxeZ6pVUD) |  | 0 |  | Avinash Kori, Francesco Locatello, Fabio De Sousa Ribeiro, Francesca Toni, Ben Glocker |  |
| 1032 |  |  [On the Stability of Expressive Positional Encodings for Graphs](https://openreview.net/forum?id=xAqcJ9XoTf) |  | 0 |  | Yinan Huang, William Lu, Joshua Robinson, Yu Yang, Muhan Zhang, Stefanie Jegelka, Pan Li |  |
| 1033 |  |  [Dynamic Neural Response Tuning](https://openreview.net/forum?id=HiTg16qhxp) |  | 0 |  | Tian Qiu, Wenxiang Xu, Lin Chen, Linyun Zhou, Zunlei Feng, Mingli Song |  |
| 1034 |  |  [Confidence-aware Reward Optimization for Fine-tuning Text-to-Image Models](https://openreview.net/forum?id=Let8OMe20n) |  | 0 |  | Kyuyoung Kim, Jongheon Jeong, Minyong An, Mohammad Ghavamzadeh, Krishnamurthy Dj Dvijotham, Jinwoo Shin, Kimin Lee |  |
| 1035 |  |  [Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts](https://openreview.net/forum?id=jvtmdK69KQ) |  | 0 |  | Huy Nguyen, Pedram Akbarian, Fanqi Yan, Nhat Ho |  |
| 1036 |  |  [The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Language Models](https://openreview.net/forum?id=SQGUDc9tC8) |  | 0 |  | Yan Liu, Yu Liu, Xiaokang Chen, PinYu Chen, Daoguang Zan, MinYen Kan, TsungYi Ho |  |
| 1037 |  |  [Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation and Human Feedback](https://openreview.net/forum?id=vW1SkPl4kp) |  | 0 |  | Yu Chen, Yihan Du, Pihe Hu, Siwei Wang, Desheng Wu, Longbo Huang |  |
| 1038 |  |  [Enhancing Transfer Learning with Flexible Nonparametric Posterior Sampling](https://openreview.net/forum?id=vSwu81S33z) |  | 0 |  | Hyungi Lee, Giung Nam, Edwin Fong, Juho Lee |  |
| 1039 |  |  [ToolChain\*: Efficient Action Space Navigation in Large Language Models with A\* Search](https://openreview.net/forum?id=B6pQxqUcT8) |  | 0 |  | Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor S. Bursztyn, Ryan A. Rossi, Somdeb Sarkhel, Chao Zhang |  |
| 1040 |  |  [Ensemble Distillation for Unsupervised Constituency Parsing](https://openreview.net/forum?id=RR8y0WKrFv) |  | 0 |  | Behzad Shayegh, Yanshuai Cao, Xiaodan Zhu, Jackie C. K. Cheung, Lili Mou |  |
| 1041 |  |  [What Algorithms can Transformers Learn? A Study in Length Generalization](https://openreview.net/forum?id=AssIuHnmHX) |  | 0 |  | Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Joshua M. Susskind, Samy Bengio, Preetum Nakkiran |  |
| 1042 |  |  [Beyond Vanilla Variational Autoencoders: Detecting Posterior Collapse in Conditional and Hierarchical Variational Autoencoders](https://openreview.net/forum?id=4zZFGliCl9) |  | 0 |  | Hien Dang, Tho Tran Huu, Tan Minh Nguyen, Nhat Ho |  |
| 1043 |  |  [Training-free Multi-objective Diffusion Model for 3D Molecule Generation](https://openreview.net/forum?id=X41c4uB4k0) |  | 0 |  | Xu Han, Caihua Shan, Yifei Shen, Can Xu, Han Yang, Xiang Li, Dongsheng Li |  |
| 1044 |  |  [Neuro-Inspired Information-Theoretic Hierarchical Perception for Multimodal Learning](https://openreview.net/forum?id=Z9AZsU1Tju) |  | 0 |  | Xiongye Xiao, Gengshuo Liu, Gaurav Gupta, Defu Cao, Shixuan Li, Yaxing Li, Tianqing Fang, Mingxi Cheng, Paul Bogdan |  |
| 1045 |  |  [Flag Aggregator: Scalable Distributed Training under Failures and Augmented Losses using Convex Optimization](https://openreview.net/forum?id=7avlrpzWqo) |  | 0 |  | Hamidreza Almasi, Harsh Mishra, Balajee Vamanan, Sathya N. Ravi |  |
| 1046 |  |  [DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models](https://openreview.net/forum?id=9m02ib92Wz) |  | 0 |  | Yongchan Kwon, Eric Wu, Kevin Wu, James Zou |  |
| 1047 |  |  [Understanding Domain Generalization: A Noise Robustness Perspective](https://openreview.net/forum?id=I2mIxuXA72) |  | 0 |  | Rui Qiao, Bryan Kian Hsiang Low |  |
| 1048 |  |  [Non-negative Contrastive Learning](https://openreview.net/forum?id=lNCnZwcH5Z) |  | 0 |  | Yifei Wang, Qi Zhang, Yaoyu Guo, Yisen Wang |  |
| 1049 |  |  [Image Clustering Conditioned on Text Criteria](https://openreview.net/forum?id=G2cG3mQqop) |  | 0 |  | Sehyun Kwon, Jaeseung Park, Minkyu Kim, Jaewoong Cho, Ernest K. Ryu, Kangwook Lee |  |
| 1050 |  |  [Correlated Noise Provably Beats Independent Noise for Differentially Private Learning](https://openreview.net/forum?id=xHmCdSArUC) |  | 0 |  | Christopher A. ChoquetteChoo, Krishnamurthy Dj Dvijotham, Krishna Pillutla, Arun Ganesh, Thomas Steinke, Abhradeep Guha Thakurta |  |
| 1051 |  |  [Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models](https://openreview.net/forum?id=6bcAD6g688) |  | 0 |  | Zhaowei Zhu, Jialu Wang, Hao Cheng, Yang Liu |  |
| 1052 |  |  [Understanding Expressivity of GNN in Rule Learning](https://openreview.net/forum?id=43cYe4oogi) |  | 0 |  | Haiquan Qiu, Yongqi Zhang, Yong Li, Quanming Yao |  |
| 1053 |  |  [COLLIE: Systematic Construction of Constrained Text Generation Tasks](https://openreview.net/forum?id=kxgSlyirUZ) |  | 0 |  | Shunyu Yao, Howard Chen, Austin W. Hanjie, Runzhe Yang, Karthik R. Narasimhan |  |
| 1054 |  |  [GENOME: Generative Neuro-Symbolic Visual Reasoning by Growing and Reusing Modules](https://openreview.net/forum?id=MNShbDSxKH) |  | 0 |  | Zhenfang Chen, Rui Sun, Wenjun Liu, Yining Hong, Chuang Gan |  |
| 1055 |  |  [Vanishing Gradients in Reinforcement Finetuning of Language Models](https://openreview.net/forum?id=IcVNBR7qZi) |  | 0 |  | Noam Razin, Hattie Zhou, Omid Saremi, Vimal Thilak, Arwen Bradley, Preetum Nakkiran, Joshua M. Susskind, Etai Littwin |  |
| 1056 |  |  [Hyper Evidential Deep Learning to Quantify Composite Classification Uncertainty](https://openreview.net/forum?id=A7t7z6g6tM) |  | 0 |  | Changbin Li, Kangshuo Li, Yuzhe Ou, Lance M. Kaplan, Audun Jøsang, JinHee Cho, Dong Hyun Jeong, Feng Chen |  |
| 1057 |  |  [Goodhart's Law in Reinforcement Learning](https://openreview.net/forum?id=5o9G4XF1LI) |  | 0 |  | Jacek Karwowski, Oliver Hayman, Xingjian Bai, Klaus Kiendlhofer, Charlie Griffin, Joar Max Viktor Skalse |  |
| 1058 |  |  [Score Regularized Policy Optimization through Diffusion Behavior](https://openreview.net/forum?id=xCRr9DrolJ) |  | 0 |  | Huayu Chen, Cheng Lu, Zhengyi Wang, Hang Su, Jun Zhu |  |
| 1059 |  |  [Robustifying and Boosting Training-Free Neural Architecture Search](https://openreview.net/forum?id=qPloNoDJZn) |  | 0 |  | Zhenfeng He, Yao Shu, Zhongxiang Dai, Bryan Kian Hsiang Low |  |
| 1060 |  |  [Concept Bottleneck Generative Models](https://openreview.net/forum?id=L9U5MJJleF) |  | 0 |  | Aya Abdelsalam Ismail, Julius Adebayo, Héctor Corrada Bravo, Stephen Ra, Kyunghyun Cho |  |
| 1061 |  |  [MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction Following](https://openreview.net/forum?id=1vrS1zwekw) |  | 0 |  | Renze Lou, Kai Zhang, Jian Xie, Yuxuan Sun, Janice Ahn, Hanzi Xu, Yu Su, Wenpeng Yin |  |
| 1062 |  |  [Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text Guidance](https://openreview.net/forum?id=2JF8mJRJ7M) |  | 0 |  | Giung Nam, Byeongho Heo, Juho Lee |  |
| 1063 |  |  [Generative Modeling of Regular and Irregular Time Series Data via Koopman VAEs](https://openreview.net/forum?id=eY7sLb0dVF) |  | 0 |  | Ilan Naiman, N. Benjamin Erichson, Pu Ren, Michael W. Mahoney, Omri Azencot |  |
| 1064 |  |  [Generating Pragmatic Examples to Train Neural Program Synthesizers](https://openreview.net/forum?id=yxKZGQLzOP) |  | 0 |  | Saujas Vaduguru, Daniel Fried, Yewen Pu |  |
| 1065 |  |  [Making RL with Preference-based Feedback Efficient via Randomization](https://openreview.net/forum?id=Pe2lo3QOvo) |  | 0 |  | Runzhe Wu, Wen Sun |  |
| 1066 |  |  [Adaptive Regret for Bandits Made Possible: Two Queries Suffice](https://openreview.net/forum?id=AY9KyTGcnk) |  | 0 |  | Zhou Lu, Qiuyi Zhang, Xinyi Chen, Fred Zhang, David P. Woodruff, Elad Hazan |  |
| 1067 |  |  [Negatively Correlated Ensemble Reinforcement Learning for Online Diverse Game Level Generation](https://openreview.net/forum?id=iAW2EQXfwb) |  | 0 |  | Ziqi Wang, Chengpeng Hu, Jialin Liu, Xin Yao |  |
| 1068 |  |  [Achieving Sample and Computational Efficient Reinforcement Learning by Action Space Reduction via Grouping](https://openreview.net/forum?id=MOmqfJovQ6) |  | 0 |  | Yining Li, Peizhong Ju, Ness B. Shroff |  |
| 1069 |  |  [Learning Hierarchical Polynomials with Three-Layer Neural Networks](https://openreview.net/forum?id=QgwAYFrh9t) |  | 0 |  | Zihao Wang, Eshaan Nichani, Jason D. Lee |  |
| 1070 |  |  [Learning Grounded Action Abstractions from Language](https://openreview.net/forum?id=qJ0Cfj4Ex9) |  | 0 |  | Lionel Wong, Jiayuan Mao, Pratyusha Sharma, Zachary S. Siegel, Jiahai Feng, Noa Korneev, Joshua B. Tenenbaum, Jacob Andreas |  |
| 1071 |  |  [Pre-training Sequence, Structure, and Surface Features for Comprehensive Protein Representation Learning](https://openreview.net/forum?id=BEH4mGo7zP) |  | 0 |  | Youhan Lee, Hasun Yu, Jaemyung Lee, Jaehoon Kim |  |
| 1072 |  |  [A unique M-pattern for micro-expression spotting in long videos](https://openreview.net/forum?id=H396R79GiQ) |  | 0 |  | Jinxuan Wang, Shiting Xu, Tong Zhang |  |
| 1073 |  |  [BayesDiff: Estimating Pixel-wise Uncertainty in Diffusion via Bayesian Inference](https://openreview.net/forum?id=YcM6ofShwY) |  | 0 |  | Siqi Kou, Lei Gan, Dequan Wang, Chongxuan Li, Zhijie Deng |  |
| 1074 |  |  [D2 Pruning: Message Passing for Balancing Diversity & Difficulty in Data Pruning](https://openreview.net/forum?id=thbtoAkCe9) |  | 0 |  | Adyasha Maharana, Prateek Yadav, Mohit Bansal |  |
| 1075 |  |  [GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs](https://openreview.net/forum?id=tVTN7Zs0ml) |  | 0 |  | Pengcheng Jiang, Cao Xiao, Adam Cross, Jimeng Sun |  |
| 1076 |  |  [The Human-AI Substitution game: active learning from a strategic labeler](https://openreview.net/forum?id=s5hSp7EdL3) |  | 0 |  | Tom Yan, Chicheng Zhang |  |
| 1077 |  |  [Deep Confident Steps to New Pockets: Strategies for Docking Generalization](https://openreview.net/forum?id=UfBIxpTK10) |  | 0 |  | Gabriele Corso, Arthur Deng, Nicholas Polizzi, Regina Barzilay, Tommi S. Jaakkola |  |
| 1078 |  |  [Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation](https://openreview.net/forum?id=mqVgBbNCm9) |  | 0 |  | Xuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang, Huazhong Yang, Yu Wang |  |
| 1079 |  |  [Language Model Beats Diffusion - Tokenizer is key to visual generation](https://openreview.net/forum?id=gzqrANCF4g) |  | 0 |  | Lijun Yu, José Lezama, Nitesh Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing Gong, MingHsuan Yang, Irfan Essa, David A. Ross, Lu Jiang |  |
| 1080 |  |  [A Sublinear Adversarial Training Algorithm](https://openreview.net/forum?id=N2WchST43h) |  | 0 |  | Yeqi Gao, Lianke Qin, Zhao Song, Yitan Wang |  |
| 1081 |  |  [Proper Laplacian Representation Learning](https://openreview.net/forum?id=7gLfQT52Nn) |  | 0 |  | Diego Gomez, Michael Bowling, Marlos C. Machado |  |
| 1082 |  |  [LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning](https://openreview.net/forum?id=xw29VvOMmU) |  | 0 |  | Han Guo, Philip Greengard, Eric P. Xing, Yoon Kim |  |
| 1083 |  |  [Deep Temporal Graph Clustering](https://openreview.net/forum?id=ViNe1fjGME) |  | 0 |  | Meng Liu, Yue Liu, Ke Liang, Wenxuan Tu, Siwei Wang, Sihang Zhou, Xinwang Liu |  |
| 1084 |  |  [CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding](https://openreview.net/forum?id=PHGxChm1l5) |  | 0 |  | Junyan Li, Delin Chen, Yining Hong, Zhenfang Chen, Peihao Chen, Yikang Shen, Chuang Gan |  |
| 1085 |  |  [Treatment Effects Estimation By Uniform Transformer](https://openreview.net/forum?id=oOGqJ6Z1sA) |  | 0 |  | Ruoqi Yu, Shulei Wang |  |
| 1086 |  |  [Demystifying Linear MDPs and Novel Dynamics Aggregation Framework](https://openreview.net/forum?id=RDSj6S8WJe) |  | 0 |  | Joongkyu Lee, Minhwan Oh |  |
| 1087 |  |  [Towards Aligned Layout Generation via Diffusion Model with Aesthetic Constraints](https://openreview.net/forum?id=kJ0qp9Xdsh) |  | 0 |  | Jian Chen, Ruiyi Zhang, Yufan Zhou, Changyou Chen |  |
| 1088 |  |  [DREAM: Dual Structured Exploration with Mixup for Open-set Graph Domain Adaption](https://openreview.net/forum?id=4olqbTBt1Y) |  | 0 |  | Nan Yin, Mengzhu Wang, Zhenghan Chen, Li Shen, Huan Xiong, Bin Gu, Xiao Luo |  |
| 1089 |  |  [Communication-Efficient Federated Non-Linear Bandit Optimization](https://openreview.net/forum?id=nFI3wFM9yN) |  | 0 |  | Chuanhao Li, Chong Liu, YuXiang Wang |  |
| 1090 |  |  [PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization](https://openreview.net/forum?id=5Nn2BLV7SB) |  | 0 |  | Yidong Wang, Zhuohao Yu, Wenjin Yao, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, Yue Zhang |  |
| 1091 |  |  [Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds](https://openreview.net/forum?id=NltzxpG0nz) |  | 0 |  | Sipeng Zheng, Jiazheng Liu, Yicheng Feng, Zongqing Lu |  |
| 1092 |  |  [Tackling the Data Heterogeneity in Asynchronous Federated Learning with Cached Update Calibration](https://openreview.net/forum?id=4aywmeb97I) |  | 0 |  | Yujia Wang, Yuanpu Cao, Jingcheng Wu, Ruoyu Chen, Jinghui Chen |  |
| 1093 |  |  [Waxing-and-Waning: a Generic Similarity-based Framework for Efficient Self-Supervised Learning](https://openreview.net/forum?id=TilcG5C8bN) |  | 0 |  | Sheng Li, Chao Wu, Ao Li, Yanzhi Wang, Xulong Tang, Geng Yuan |  |
| 1094 |  |  [WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions](https://openreview.net/forum?id=CfXh93NDgH) |  | 0 |  | Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, Daxin Jiang |  |
| 1095 |  |  [LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors](https://openreview.net/forum?id=usrChqw6yK) |  | 0 |  | Sheng Jin, Xueying Jiang, Jiaxing Huang, Lewei Lu, Shijian Lu |  |
| 1096 |  |  [CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic Decoding](https://openreview.net/forum?id=lKxL5zkssv) |  | 0 |  | Qiongyi Zhou, Changde Du, Shengpei Wang, Huiguang He |  |
| 1097 |  |  [Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning](https://openreview.net/forum?id=v8jdwkUNXb) |  | 0 |  | Zihan Ding, Chi Jin |  |
| 1098 |  |  [Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning](https://openreview.net/forum?id=iX1RjVQODj) |  | 0 |  | Joey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, W. Bradley Knox, Dorsa Sadigh |  |
| 1099 |  |  [Neural Network-Based Score Estimation in Diffusion Models: Optimization and Generalization](https://openreview.net/forum?id=h8GeqOxtd4) |  | 0 |  | Yinbin Han, Meisam Razaviyayn, Renyuan Xu |  |
| 1100 |  |  [Time-Varying Propensity Score to Bridge the Gap between the Past and Present](https://openreview.net/forum?id=m0x0rv6Iwm) |  | 0 |  | Rasool Fakoor, Jonas Mueller, Zachary Chase Lipton, Pratik Chaudhari, Alex Smola |  |
| 1101 |  |  [Debiasing Attention Mechanism in Transformer without Demographics](https://openreview.net/forum?id=jLIUfrAcMQ) |  | 0 |  | Shenyu Lu, Yipei Wang, Xiaoqian Wang |  |
| 1102 |  |  [RA-DIT: Retrieval-Augmented Dual Instruction Tuning](https://openreview.net/forum?id=22OTbutug9) |  | 0 |  | Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, Wentau Yih |  |
| 1103 |  |  [Like Oil and Water: Group Robustness Methods and Poisoning Defenses May Be at Odds](https://openreview.net/forum?id=rM9VJPB20F) |  | 0 |  | MichaelAndrei PanaitescuLiess, Yigitcan Kaya, Sicheng Zhu, Furong Huang, Tudor Dumitras |  |
| 1104 |  |  [Visual Data-Type Understanding does not emerge from scaling Vision-Language Models](https://openreview.net/forum?id=WyEdX2R4er) |  | 0 |  | Vishaal Udandarao, Max F. Burg, Samuel Albanie, Matthias Bethge |  |
| 1105 |  |  [CoT3DRef: Chain-of-Thoughts Data-Efficient 3D Visual Grounding](https://openreview.net/forum?id=ORUiqcLpV6) |  | 0 |  | Eslam Mohamed Bakr, Mohamed Ayman, Mahmoud Ahmed, Habib Slim, Mohamed Elhoseiny |  |
| 1106 |  |  [An Efficient Tester-Learner for Halfspaces](https://openreview.net/forum?id=z6n1fKMMC1) |  | 0 |  | Aravind Gollakota, Adam R. Klivans, Konstantinos Stavropoulos, Arsen Vasilyan |  |
| 1107 |  |  [Reclaiming the Source of Programmatic Policies: Programmatic versus Latent Spaces](https://openreview.net/forum?id=NGVljI6HkR) |  | 0 |  | Tales Henrique Carvalho, Kenneth Tjhia, Levi Lelis |  |
| 1108 |  |  [Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment](https://openreview.net/forum?id=w9tc699w3Z) |  | 0 |  | Utkarsh Mall, Cheng Perng Phoo, Meilin Kelsey Liu, Carl Vondrick, Bharath Hariharan, Kavita Bala |  |
| 1109 |  |  [The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing](https://openreview.net/forum?id=C36v8541Ns) |  | 0 |  | Blaise Delattre, Alexandre Araujo, Quentin Barthélemy, Alexandre Allauzen |  |
| 1110 |  |  [On the Fairness ROAD: Robust Optimization for Adversarial Debiasing](https://openreview.net/forum?id=xnhvVtZtLD) |  | 0 |  | Vincent Grari, Thibault Laugel, Tatsunori Hashimoto, Sylvain Lamprier, Marcin Detyniecki |  |
| 1111 |  |  [Learning Planning Abstractions from Language](https://openreview.net/forum?id=3UWuFoksGb) |  | 0 |  | Weiyu Liu, Geng Chen, Joy Hsu, Jiayuan Mao, Jiajun Wu |  |
| 1112 |  |  [Tailoring Self-Rationalizers with Multi-Reward Distillation](https://openreview.net/forum?id=t8eO0CiZJV) |  | 0 |  | Sahana Ramnath, Brihi Joshi, Skyler Hallinan, Ximing Lu, Liunian Harold Li, Aaron Chan, Jack Hessel, Yejin Choi, Xiang Ren |  |
| 1113 |  |  [Building Cooperative Embodied Agents Modularly with Large Language Models](https://openreview.net/forum?id=EnXJfQqy0K) |  | 0 |  | Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B. Tenenbaum, Tianmin Shu, Chuang Gan |  |
| 1114 |  |  [Fast Hyperboloid Decision Tree Algorithms](https://openreview.net/forum?id=TTonmgTT9X) |  | 0 |  | Philippe Chlenski, Ethan Turok, Antonio Khalil Moretti, Itsik Pe'er |  |
| 1115 |  |  [Private Zeroth-Order Nonsmooth Nonconvex Optimization](https://openreview.net/forum?id=IzqZbNMZ0M) |  | 0 |  | Qinzi Zhang, Hoang Tran, Ashok Cutkosky |  |
| 1116 |  |  [Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training](https://openreview.net/forum?id=3xHDeA8Noi) |  | 0 |  | Hong Liu, Zhiyuan Li, David Leo Wright Hall, Percy Liang, Tengyu Ma |  |
| 1117 |  |  [Conversational Drug Editing Using Retrieval and Domain Feedback](https://openreview.net/forum?id=yRrPfKyJQ2) |  | 0 |  | Shengchao Liu, Jiongxiao Wang, Yijin Yang, Chengpeng Wang, Ling Liu, Hongyu Guo, Chaowei Xiao |  |
| 1118 |  |  [Poly-View Contrastive Learning](https://openreview.net/forum?id=iHcTLIor0m) |  | 0 |  | Amitis Shidani, R. Devon Hjelm, Jason Ramapuram, Russell Webb, Eeshan Gunesh Dhekane, Dan Busbridge |  |
| 1119 |  |  [A Probabilistic Framework for Modular Continual Learning](https://openreview.net/forum?id=MVe2dnWPCu) |  | 0 |  | Lazar Valkov, Akash Srivastava, Swarat Chaudhuri, Charles Sutton |  |
| 1120 |  |  [Boundary Denoising for Video Activity Localization](https://openreview.net/forum?id=bLpUtGyf9g) |  | 0 |  | Mengmeng Xu, Mattia Soldan, Jialin Gao, Shuming Liu, JuanManuel PérezRúa, Bernard Ghanem |  |
| 1121 |  |  [Few-Shot Detection of Machine-Generated Text using Style Representations](https://openreview.net/forum?id=cWiEN1plhJ) |  | 0 |  | Rafael A. Rivera Soto, Kailin Koch, Aleem Khan, Barry Y. Chen, Marcus Bishop, Nicholas Andrews |  |
| 1122 |  |  [Safe and Robust Watermark Injection with a Single OoD Image](https://openreview.net/forum?id=PCm1oT8pZI) |  | 0 |  | Shuyang Yu, Junyuan Hong, Haobo Zhang, Haotao Wang, Zhangyang Wang, Jiayu Zhou |  |
| 1123 |  |  [Massive Editing for Large Language Models via Meta Learning](https://openreview.net/forum?id=L6L1CJQ2PE) |  | 0 |  | Chenmien Tan, Ge Zhang, Jie Fu |  |
| 1124 |  |  [BrainLM: A foundation model for brain activity recordings](https://openreview.net/forum?id=RwI7ZEfR27) |  | 0 |  | Josue Ortega Caro, Antonio Henrique de Oliveira Fonseca, Syed Asad Rizvi, Matteo Rosati, Christopher L. Averill, James L. Cross, Prateek Mittal, Emanuele Zappala, Rahul Madhav Dhodapkar, Chadi Abdallah, David van Dijk |  |
| 1125 |  |  [Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality](https://openreview.net/forum?id=1NHgmKqOzZ) |  | 0 |  | Xuxi Chen, Yu Yang, Zhangyang Wang, Baharan Mirzasoleiman |  |
| 1126 |  |  [Object centric architectures enable efficient causal representation learning](https://openreview.net/forum?id=r9FsiXZxZt) |  | 0 |  | Amin Mansouri, Jason S. Hartford, Yan Zhang, Yoshua Bengio |  |
| 1127 |  |  [Efficient Score Matching with Deep Equilibrium Layers](https://openreview.net/forum?id=J1djqLAa6N) |  | 0 |  | Yuhao Huang, Qingsong Wang, Akwum Onwunta, Bao Wang |  |
| 1128 |  |  [Alt-Text with Context: Improving Accessibility for Images on Twitter](https://openreview.net/forum?id=97Dl82avFs) |  | 0 |  | Nikita Srivatsan, Sofía Samaniego, Omar Florez, Taylor BergKirkpatrick |  |
| 1129 |  |  [Defining Expertise: Applications to Treatment Effect Estimation](https://openreview.net/forum?id=1YPfmglNRU) |  | 0 |  | Alihan Hüyük, Qiyao Wei, Alicia Curth, Mihaela van der Schaar |  |
| 1130 |  |  [Alleviating Exposure Bias in Diffusion Models through Sampling with Shifted Time Steps](https://openreview.net/forum?id=ZSD3MloKe6) |  | 0 |  | Mingxiao Li, Tingyu Qu, Ruicong Yao, Wei Sun, MarieFrancine Moens |  |
| 1131 |  |  [Rethinking the Benefits of Steerable Features in 3D Equivariant Graph Neural Networks](https://openreview.net/forum?id=mGHJAyR8w0) |  | 0 |  | ShihHsin Wang, YungChang Hsu, Justin M. Baker, Andrea L. Bertozzi, Jack Xin, Bao Wang |  |
| 1132 |  |  [Neural SDF Flow for 3D Reconstruction of Dynamic Scenes](https://openreview.net/forum?id=rzF0R6GOd4) |  | 0 |  | Wei Mao, Richard Hartley, Mathieu Salzmann, Miaomiao Liu |  |
| 1133 |  |  [Class Probability Matching with Calibrated Networks for Label Shift Adaption](https://openreview.net/forum?id=mliQ2huFrZ) |  | 0 |  | Hongwei Wen, Annika Betken, Hanyuan Hang |  |
| 1134 |  |  [DIFFTACTILE: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation](https://openreview.net/forum?id=eJHnSg783t) |  | 0 |  | Zilin Si, Gu Zhang, Qingwei Ben, Branden Romero, Zhou Xian, Chao Liu, Chuang Gan |  |
| 1135 |  |  [Tangent Transformers for Composition, Privacy and Removal](https://openreview.net/forum?id=VLFhbOCz5D) |  | 0 |  | Tian Yu Liu, Aditya Golatkar, Stefano Soatto |  |
| 1136 |  |  [Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information](https://openreview.net/forum?id=yV6wwEbtkR) |  | 0 |  | Linfeng Ye, Shayan Mohajer Hamidi, Renhao Tan, EnHui Yang |  |
| 1137 |  |  [Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting](https://openreview.net/forum?id=RIu5lyNXjT) |  | 0 |  | Melanie Sclar, Yejin Choi, Yulia Tsvetkov, Alane Suhr |  |
| 1138 |  |  [Universal Guidance for Diffusion Models](https://openreview.net/forum?id=pzpWBbnwiJ) |  | 0 |  | Arpit Bansal, HongMin Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, Tom Goldstein |  |
| 1139 |  |  [Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models](https://openreview.net/forum?id=L4nOxziGf9) |  | 0 |  | Archiki Prasad, Elias StengelEskin, Mohit Bansal |  |
| 1140 |  |  [A Black-box Approach for Non-stationary Multi-agent Reinforcement Learning](https://openreview.net/forum?id=LWuYsSD94h) |  | 0 |  | Haozhe Jiang, Qiwen Cui, Zhihan Xiong, Maryam Fazel, Simon Shaolei Du |  |
| 1141 |  |  [Prompt Risk Control: A Rigorous Framework for Responsible Deployment of Large Language Models](https://openreview.net/forum?id=5tGGWOijvq) |  | 0 |  | Thomas P. Zollo, Todd Morrill, Zhun Deng, Jake Snell, Toniann Pitassi, Richard S. Zemel |  |
| 1142 |  |  [Energy-conserving equivariant GNN for elasticity of lattice architected metamaterials](https://openreview.net/forum?id=smy4DsUbBo) |  | 0 |  | Ivan Grega, Ilyes Batatia, Gábor Csányi, Sri Karlapati, Vikram S. Deshpande |  |
| 1143 |  |  [Adaptive Federated Learning with Auto-Tuned Clients](https://openreview.net/forum?id=g0mlwqs8pi) |  | 0 |  | Junhyung Lyle Kim, Mohammad Taha Toghani, César A. Uribe, Anastasios Kyrillidis |  |
| 1144 |  |  [CrossLoco: Human Motion Driven Control of Legged Robots via Guided Unsupervised Reinforcement Learning](https://openreview.net/forum?id=UCfz492fM8) |  | 0 |  | Tianyu Li, Hyunyoung Jung, Matthew C. Gombolay, Yong Kwon Cho, Sehoon Ha |  |
| 1145 |  |  [Zoology: Measuring and Improving Recall in Efficient Language Models](https://openreview.net/forum?id=LY3ukUANko) |  | 0 |  | Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, Christopher Ré |  |
| 1146 |  |  [Let Models Speak Ciphers: Multiagent Debate through Embeddings](https://openreview.net/forum?id=sehRvaIPQQ) |  | 0 |  | Chau Pham, Boyi Liu, Yingxiang Yang, Zhengyu Chen, Tianyi Liu, Jianbo Yuan, Bryan A. Plummer, Zhaoran Wang, Hongxia Yang |  |
| 1147 |  |  [Towards Training Without Depth Limits: Batch Normalization Without Gradient Explosion](https://openreview.net/forum?id=xhCZD9hiiA) |  | 0 |  | Alexandru Meterez, Amir Joudaki, Francesco Orabona, Alexander Immer, Gunnar Rätsch, Hadi Daneshmand |  |
| 1148 |  |  [Plugin estimators for selective classification with out-of-distribution detection](https://openreview.net/forum?id=DASh78rJ7g) |  | 0 |  | Harikrishna Narasimhan, Aditya Krishna Menon, Wittawat Jitkrittum, Sanjiv Kumar |  |
| 1149 |  |  [Dynamic Sparse Training with Structured Sparsity](https://openreview.net/forum?id=kOBkxFRKTA) |  | 0 |  | Mike Lasby, Anna Golubeva, Utku Evci, Mihai Nica, Yani Ioannou |  |
| 1150 |  |  [Efficient Dynamics Modeling in Interactive Environments with Koopman Theory](https://openreview.net/forum?id=fkrYDQaHOJ) |  | 0 |  | Arnab Kumar Mondal, Siba Smarak Panigrahi, Sai Rajeswar, Kaleem Siddiqi, Siamak Ravanbakhsh |  |
| 1151 |  |  [Learning interpretable control inputs and dynamics underlying animal locomotion](https://openreview.net/forum?id=MFCjgEOLJT) |  | 0 |  | Thomas Soares Mullen, Marine Schimel, Guillaume Hennequin, Christian K. Machens, Michael B. Orger, Adrien Jouary |  |
| 1152 |  |  [Tight Rates in Supervised Outlier Transfer Learning](https://openreview.net/forum?id=nUBLhhVM1l) |  | 0 |  | Mohammadreza M. Kalan, Samory Kpotufe |  |
| 1153 |  |  [Sampling Multimodal Distributions with the Vanilla Score: Benefits of Data-Based Initialization](https://openreview.net/forum?id=oAMArMMQxb) |  | 0 |  | Frederic Koehler, ThuyDuong Vuong |  |
| 1154 |  |  [Curiosity-driven Red-teaming for Large Language Models](https://openreview.net/forum?id=4KqkizXgXU) |  | 0 |  | ZhangWei Hong, Idan Shenfeld, TsunHsuan Wang, YungSung Chuang, Aldo Pareja, James R. Glass, Akash Srivastava, Pulkit Agrawal |  |
| 1155 |  |  [Understanding prompt engineering may not require rethinking generalization](https://openreview.net/forum?id=a745RnSFLT) |  | 0 |  | Victor Akinwande, Yiding Jiang, Dylan Sam, J. Zico Kolter |  |
| 1156 |  |  [Adversarial Imitation Learning via Boosting](https://openreview.net/forum?id=DuQkqSe9en) |  | 0 |  | Jonathan D. Chang, Dhruv Sreenivas, Yingbing Huang, Kianté Brantley, Wen Sun |  |
| 1157 |  |  [Unsupervised Pretraining for Fact Verification by Language Model Distillation](https://openreview.net/forum?id=1mjsP8RYAw) |  | 0 |  | Adrián Bazaga, Pietro Lio, Gos Micklem |  |
| 1158 |  |  [LipSim: A Provably Robust Perceptual Similarity Metric](https://openreview.net/forum?id=0w42S2Gp70) |  | 0 |  | Sara Ghazanfari, Alexandre Araujo, Prashanth Krishnamurthy, Farshad Khorrami, Siddharth Garg |  |
| 1159 |  |  [Does Progress On Object Recognition Benchmarks Improve Generalization on Crowdsourced, Global Data?](https://openreview.net/forum?id=rhaQbS3K3R) |  | 0 |  | Megan Richards, Polina Kirichenko, Diane Bouchacourt, Mark Ibrahim |  |
| 1160 |  |  [TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate Time Series](https://openreview.net/forum?id=xtOydkE1Ku) |  | 0 |  | Arjun Ashok, Étienne Marcotte, Valentina Zantedeschi, Nicolas Chapados, Alexandre Drouin |  |
| 1161 |  |  [A robust differential Neural ODE Optimizer](https://openreview.net/forum?id=zbOSJ3CATY) |  | 0 |  | Panagiotis Theodoropoulos, GuanHorng Liu, Tianrong Chen, Augustinos D. Saravanos, Evangelos A. Theodorou |  |
| 1162 |  |  [A Primal-Dual Approach to Solving Variational Inequalities with General Constraints](https://openreview.net/forum?id=RsztjXcvUf) |  | 0 |  | Tatjana Chavdarova, Tong Yang, Matteo Pagliardini, Michael I. Jordan |  |
| 1163 |  |  [TiC-CLIP: Continual Training of CLIP Models](https://openreview.net/forum?id=TLADT8Wrhn) |  | 0 |  | Saurabh Garg, Mehrdad Farajtabar, Hadi Pouransari, Raviteja Vemulapalli, Sachin Mehta, Oncel Tuzel, Vaishaal Shankar, Fartash Faghri |  |
| 1164 |  |  [Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks](https://openreview.net/forum?id=dLoAdIKENc) |  | 0 |  | Mehrdad Saberi, Vinu Sankar Sadasivan, Keivan Rezaei, Aounon Kumar, Atoosa Malemir Chegini, Wenxiao Wang, Soheil Feizi |  |
| 1165 |  |  [Constrained Decoding for Cross-lingual Label Projection](https://openreview.net/forum?id=DayPQKXaQk) |  | 0 |  | Duong Minh Le, Yang Chen, Alan Ritter, Wei Xu |  |
| 1166 |  |  [Channel Vision Transformers: An Image Is Worth 1 x 16 x 16 Words](https://openreview.net/forum?id=CK5Hfb5hBG) |  | 0 |  | Yujia Bao, Srinivasan Sivanandan, Theofanis Karaletsos |  |
| 1167 |  |  [Understanding Reconstruction Attacks with the Neural Tangent Kernel and Dataset Distillation](https://openreview.net/forum?id=VoLDkQ6yR3) |  | 0 |  | Noel Loo, Ramin M. Hasani, Mathias Lechner, Alexander Amini, Daniela Rus |  |
| 1168 |  |  [ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models](https://openreview.net/forum?id=iIT02bAKzv) |  | 0 |  | YiLin Sung, Jaehong Yoon, Mohit Bansal |  |
| 1169 |  |  [Behaviour Distillation](https://openreview.net/forum?id=qup9xD8mW4) |  | 0 |  | Andrei Lupu, Chris Lu, Jarek Liesen, Robert Tjarko Lange, Jakob Nicolaus Foerster |  |
| 1170 |  |  [Improving LoRA in Privacy-preserving Federated Learning](https://openreview.net/forum?id=NLPzL6HWNl) |  | 0 |  | Youbang Sun, Zitao Li, Yaliang Li, Bolin Ding |  |
| 1171 |  |  [PhyloGFN: Phylogenetic inference with generative flow networks](https://openreview.net/forum?id=hB7SlfEmze) |  | 0 |  | MingYang Zhou, Zichao Yan, Elliot Layne, Nikolay Malkin, Dinghuai Zhang, Moksh Jain, Mathieu Blanchette, Yoshua Bengio |  |
| 1172 |  |  [Training Bayesian Neural Networks with Sparse Subspace Variational Inference](https://openreview.net/forum?id=TskzCtpMEO) |  | 0 |  | Junbo Li, Zichen Miao, Qiang Qiu, Ruqi Zhang |  |
| 1173 |  |  [Locality-Aware Graph Rewiring in GNNs](https://openreview.net/forum?id=4Ua4hKiAJX) |  | 0 |  | Federico Barbero, Ameya Velingker, Amin Saberi, Michael M. Bronstein, Francesco Di Giovanni |  |
| 1174 |  |  [Adapting to Distribution Shift by Visual Domain Prompt Generation](https://openreview.net/forum?id=sSaN4gxuEf) |  | 0 |  | Zhixiang Chi, Li Gu, Tao Zhong, Huan Liu, Yuanhao Yu, Konstantinos N. Plataniotis, Yang Wang |  |
| 1175 |  |  [Multimodal Patient Representation Learning with Missing Modalities and Labels](https://openreview.net/forum?id=Je5SHCKpPa) |  | 0 |  | Zhenbang Wu, Anant Dadu, Nicholas J. Tustison, Brian B. Avants, Mike A. Nalls, Jimeng Sun, Faraz Faghri |  |
| 1176 |  |  [Improved sampling via learned diffusions](https://openreview.net/forum?id=h4pNROsO06) |  | 0 |  | Lorenz Richter, Julius Berner |  |
| 1177 |  |  [MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models](https://openreview.net/forum?id=1tZbq88f27) |  | 0 |  | Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny |  |
| 1178 |  |  [Towards Establishing Guaranteed Error for Learned Database Operations](https://openreview.net/forum?id=6tqgL8VluV) |  | 0 |  | Sepanta Zeighami, Cyrus Shahabi |  |
| 1179 |  |  [Quantifying the Plausibility of Context Reliance in Neural Machine Translation](https://openreview.net/forum?id=XTHfNGI3zT) |  | 0 |  | Gabriele Sarti, Grzegorz Chrupala, Malvina Nissim, Arianna Bisazza |  |
| 1180 |  |  [Bayesian Optimization through Gaussian Cox Process Models for Spatio-temporal Data](https://openreview.net/forum?id=9j1RD9LlWH) |  | 0 |  | Yongsheng Mei, Mahdi Imani, Tian Lan |  |
| 1181 |  |  [Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX](https://openreview.net/forum?id=C4CxQmp9wc) |  | 0 |  | Clément Bonnet, Daniel Luo, Donal Byrne, Shikha Surana, Sasha Abramowitz, Paul Duckworth, Vincent Coyette, Laurence Illing Midgley, Elshadai Tegegn, Tristan Kalloniatis, Omayma Mahjoub, Matthew Macfarlane, Andries P. Smit, Nathan Grinsztajn, Raphaël Boige, Cemlyn N. Waters, Mohamed A. Mimouni, Ulrich A. Mbou Sob, Ruan de Kock, Siddarth Singh, Daniel FurelosBlanco, Victor Le, Arnu Pretorius, Alexandre Laterre |  |
| 1182 |  |  [Searching for High-Value Molecules Using Reinforcement Learning and Transformers](https://openreview.net/forum?id=nqlymMx42E) |  | 0 |  | Raj Ghugare, Santiago Miret, Adriana Hugessen, Mariano Phielipp, Glen Berseth |  |
| 1183 |  |  [Differentiable Euler Characteristic Transforms for Shape Classification](https://openreview.net/forum?id=MO632iPq3I) |  | 0 |  | Ernst Röell, Bastian Rieck |  |
| 1184 |  |  [Causally Aligned Curriculum Learning](https://openreview.net/forum?id=hp4yOjhwTs) |  | 0 |  | Mingxuan Li, Junzhe Zhang, Elias Bareinboim |  |
| 1185 |  |  [Faster Sampling from Log-Concave Densities over Polytopes via Efficient Linear Solvers](https://openreview.net/forum?id=v63GWletn8) |  | 0 |  | Oren Mangoubi, Nisheeth K. Vishnoi |  |
| 1186 |  |  [Towards Characterizing Domain Counterfactuals for Invertible Latent Causal Models](https://openreview.net/forum?id=v1VvCWJAL8) |  | 0 |  | Zeyu Zhou, Ruqi Bai, Sean Kulinski, Murat Kocaoglu, David I. Inouye |  |
| 1187 |  |  [Grokking as the transition from lazy to rich training dynamics](https://openreview.net/forum?id=vt5mnLVIVo) |  | 0 |  | Tanishq Kumar, Blake Bordelon, Samuel J. Gershman, Cengiz Pehlevan |  |
| 1188 |  |  [Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement Learning](https://openreview.net/forum?id=M0xK8nPGvt) |  | 0 |  | Mirco Mutti, Riccardo De Santi, Marcello Restelli, Alexander Marx, Giorgia Ramponi |  |
| 1189 |  |  [Language Model Cascades: Token-Level Uncertainty And Beyond](https://openreview.net/forum?id=KgaBScZ4VI) |  | 0 |  | Neha Gupta, Harikrishna Narasimhan, Wittawat Jitkrittum, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar |  |
| 1190 |  |  [The Marginal Value of Momentum for Small Learning Rate SGD](https://openreview.net/forum?id=3JjJezzVkT) |  | 0 |  | Runzhe Wang, Sadhika Malladi, Tianhao Wang, Kaifeng Lyu, Zhiyuan Li |  |
| 1191 |  |  [Learning Polynomial Problems with SL(2, R)-Equivariance](https://openreview.net/forum?id=gyfXuRfxW2) |  | 0 |  | Hannah Lawrence, Mitchell Tong Harris |  |
| 1192 |  |  [Mixture of Weak and Strong Experts on Graphs](https://openreview.net/forum?id=wYvuY60SdD) |  | 0 |  | Hanqing Zeng, Hanjia Lyu, Diyi Hu, Yinglong Xia, Jiebo Luo |  |
| 1193 |  |  [Transformers can optimally learn regression mixture models](https://openreview.net/forum?id=sLkj91HIZU) |  | 0 |  | Reese Pathak, Rajat Sen, Weihao Kong, Abhimanyu Das |  |
| 1194 |  |  [Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective](https://openreview.net/forum?id=iCNOK45Csv) |  | 0 |  | MingYu Chung, ShengYen Chou, ChiaMu Yu, PinYu Chen, SyYen Kuo, TsungYi Ho |  |
| 1195 |  |  [Towards Diverse Behaviors: A Benchmark for Imitation Learning with Human Demonstrations](https://openreview.net/forum?id=6pPYRXKPpw) |  | 0 |  | Xiaogang Jia, Denis Blessing, Xinkai Jiang, Moritz Reuss, Atalay Donat, Rudolf Lioutikov, Gerhard Neumann |  |
| 1196 |  |  [Reconciling Spatial and Temporal Abstractions for Goal Representation](https://openreview.net/forum?id=odY3PkI5VB) |  | 0 |  | Mehdi Zadem, Sergio Mover, Sao Mai Nguyen |  |
| 1197 |  |  [Estimating Conditional Mutual Information for Dynamic Feature Selection](https://openreview.net/forum?id=Oju2Qu9jvn) |  | 0 |  | Soham Gadgil, Ian Connick Covert, SuIn Lee |  |
| 1198 |  |  [LLM Augmented LLMs: Expanding Capabilities through Composition](https://openreview.net/forum?id=jjA4O1vJRz) |  | 0 |  | Rachit Bansal, Bidisha Samanta, Siddharth Dalmia, Nitish Gupta, Sriram Ganapathy, Abhishek Bapna, Prateek Jain, Partha Talukdar |  |
| 1199 |  |  [Quadratic models for understanding catapult dynamics of neural networks](https://openreview.net/forum?id=PvJnX3dwsD) |  | 0 |  | Libin Zhu, Chaoyue Liu, Adityanarayanan Radhakrishnan, Mikhail Belkin |  |
| 1200 |  |  [Evaluating Representation Learning on the Protein Structure Universe](https://openreview.net/forum?id=sTYuRVrdK3) |  | 0 |  | Arian Rokkum Jamasb, Alex Morehead, Chaitanya K. Joshi, Zuobai Zhang, Kieran Didi, Simon V. Mathis, Charles Harris, Jian Tang, Jianlin Cheng, Pietro Lio, Tom L. Blundell |  |
| 1201 |  |  [T-MARS: Improving Visual Representations by Circumventing Text Feature Learning](https://openreview.net/forum?id=ViPtjIVzUw) |  | 0 |  | Pratyush Maini, Sachin Goyal, Zachary Chase Lipton, J. Zico Kolter, Aditi Raghunathan |  |
| 1202 |  |  [Nougat: Neural Optical Understanding for Academic Documents](https://openreview.net/forum?id=fUtxNAKpdV) |  | 0 |  | Lukas Blecher, Guillem Cucurull, Thomas Scialom, Robert Stojnic |  |
| 1203 |  |  [When can transformers reason with abstract symbols?](https://openreview.net/forum?id=STUGfUz8ob) |  | 0 |  | Enric BoixAdserà, Omid Saremi, Emmanuel Abbe, Samy Bengio, Etai Littwin, Joshua M. Susskind |  |
| 1204 |  |  [Towards Generative Abstract Reasoning: Completing Raven's Progressive Matrix via Rule Abstraction and Selection](https://openreview.net/forum?id=IcR1OOFzxm) |  | 0 |  | Fan Shi, Bin Li, Xiangyang Xue |  |
| 1205 |  |  [A Characterization Theorem for Equivariant Networks with Point-wise Activations](https://openreview.net/forum?id=79FVDdfoSR) |  | 0 |  | Marco Pacini, Xiaowen Dong, Bruno Lepri, Gabriele Santin |  |
| 1206 |  |  [Think before you speak: Training Language Models With Pause Tokens](https://openreview.net/forum?id=ph04CRkPdC) |  | 0 |  | Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, Vaishnavh Nagarajan |  |
| 1207 |  |  [Talk like a Graph: Encoding Graphs for Large Language Models](https://openreview.net/forum?id=IuXR1CCrSi) |  | 0 |  | Bahare Fatemi, Jonathan Halcrow, Bryan Perozzi |  |
| 1208 |  |  [Privately Aligning Language Models with Reinforcement Learning](https://openreview.net/forum?id=3d0OmYTNui) |  | 0 |  | Fan Wu, Huseyin A. Inan, Arturs Backurs, Varun Chandrasekaran, Janardhan Kulkarni, Robert Sim |  |
| 1209 |  |  [YaRN: Efficient Context Window Extension of Large Language Models](https://openreview.net/forum?id=wHBfxhZu1u) |  | 0 |  | Bowen Peng, Jeffrey Quesnelle, Honglu Fan, Enrico Shippole |  |
| 1210 |  |  [Accelerating Sinkhorn algorithm with sparse Newton iterations](https://openreview.net/forum?id=Kuj5gVp5GQ) |  | 0 |  | Xun Tang, Michael Shavlovsky, Holakou Rahmanian, Elisa Tardini, Kiran Koshy Thekumparampil, Tesi Xiao, Lexing Ying |  |
| 1211 |  |  [Functional Interpolation for Relative Positions improves Long Context Transformers](https://openreview.net/forum?id=rR03qFesqk) |  | 0 |  | Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontañón, Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, Srinadh Bhojanapalli |  |
| 1212 |  |  [FeatUp: A Model-Agnostic Framework for Features at Any Resolution](https://openreview.net/forum?id=GkJiNn2QDF) |  | 0 |  | Stephanie Fu, Mark Hamilton, Laura E. Brandt, Axel Feldmann, Zhoutong Zhang, William T. Freeman |  |
| 1213 |  |  [Masked Autoencoders with Multi-Window Local-Global Attention Are Better Audio Learners](https://openreview.net/forum?id=Q53QLftNkA) |  | 0 |  | Sarthak Yadav, Sergios Theodoridis, Lars Kai Hansen, ZhengHua Tan |  |
| 1214 |  |  [Toward Optimal Policy Population Growth in Two-Player Zero-Sum Games](https://openreview.net/forum?id=J2TZgj3Tac) |  | 0 |  | Stephen Marcus McAleer, JB Lanier, Kevin A. Wang, Pierre Baldi, Tuomas Sandholm, Roy Fox |  |
| 1215 |  |  [Sparse Autoencoders Find Highly Interpretable Features in Language Models](https://openreview.net/forum?id=F76bwRSLeK) |  | 0 |  | Robert Huben, Hoagy Cunningham, Logan Riggs Smith, Aidan Ewart, Lee Sharkey |  |
| 1216 |  |  [OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning](https://openreview.net/forum?id=FbuyDzZTPt) |  | 0 |  | WeiCheng Huang, ChunFu Richard Chen, Hsiang Hsu |  |
| 1217 |  |  [Chain of Log-Concave Markov Chains](https://openreview.net/forum?id=yiMB2DOjsR) |  | 0 |  | Saeed Saremi, Ji Won Park, Francis R. Bach |  |
| 1218 |  |  [Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data](https://openreview.net/forum?id=BxHgpC6FNv) |  | 0 |  | Zhiwei Xu, Yutong Wang, Spencer Frei, Gal Vardi, Wei Hu |  |
| 1219 |  |  [VBH-GNN: Variational Bayesian Heterogeneous Graph Neural Networks for Cross-subject Emotion Recognition](https://openreview.net/forum?id=EArTDUmILF) |  | 0 |  | Chenyu Liu, Xinliang Zhou, Zhengri Zhu, Liming Zhai, Ziyu Jia, Yang Liu |  |
| 1220 |  |  [AUGCAL: Improving Sim2Real Adaptation by Uncertainty Calibration on Augmented Synthetic Images](https://openreview.net/forum?id=WNQjN5HzXt) |  | 0 |  | Prithvijit Chattopadhyay, Bharat Goyal, Boglarka Ecsedi, Viraj Prabhu, Judy Hoffman |  |
| 1221 |  |  [Quality-Diversity through AI Feedback](https://openreview.net/forum?id=owokKCrGYr) |  | 0 |  | Herbie Bradley, Andrew Dai, Hannah Benita Teufel, Jenny Zhang, Koen Oostermeijer, Marco Bellagente, Jeff Clune, Kenneth O. Stanley, Grégory Schott, Joel Lehman |  |
| 1222 |  |  [Learning from Sparse Offline Datasets via Conservative Density Estimation](https://openreview.net/forum?id=4WM0OogPTx) |  | 0 |  | Zhepeng Cen, Zuxin Liu, Zitong Wang, Yihang Yao, Henry Lam, Ding Zhao |  |
| 1223 |  |  [On the Expressivity of Objective-Specification Formalisms in Reinforcement Learning](https://openreview.net/forum?id=qr4ECbGcSj) |  | 0 |  | Rohan Subramani, Marcus Williams, Max Heitmann, Halfdan Holm, Charlie Griffin, Joar Max Viktor Skalse |  |
| 1224 |  |  [Implicit Maximum a Posteriori Filtering via Adaptive Optimization](https://openreview.net/forum?id=auUngos7eR) |  | 0 |  | Gianluca M. Bencomo, Jake Snell, Thomas L. Griffiths |  |
| 1225 |  |  [Hybrid Internal Model: Learning Agile Legged Locomotion with Simulated Robot Response](https://openreview.net/forum?id=93LoCyww8o) |  | 0 |  | Junfeng Long, Zirui Wang, Quanyi Li, Liu Cao, Jiawei Gao, Jiangmiao Pang |  |
| 1226 |  |  [S2AC: Energy-Based Reinforcement Learning with Stein Soft Actor Critic](https://openreview.net/forum?id=rAHcTCMaLc) |  | 0 |  | Safa Messaoud, Billel Mokeddem, Zhenghai Xue, Linsey Pang, Bo An, Haipeng Chen, Sanjay Chawla |  |
| 1227 |  |  [A Newborn Embodied Turing Test for Comparing Object Segmentation Across Animals and Machines](https://openreview.net/forum?id=qhkEOCcVX9) |  | 0 |  | Manju Garimella, Denizhan Pak, Justin N. Wood, Samantha Marie Waters Wood |  |
| 1228 |  |  [Robust Model-Based Optimization for Challenging Fitness Landscapes](https://openreview.net/forum?id=xhEN0kJh4q) |  | 0 |  | Saba Ghaffari, Ehsan Saleh, Alexander G. Schwing, YuXiong Wang, Martin D. Burke, Saurabh Sinha |  |
| 1229 |  |  [Solving High Frequency and Multi-Scale PDEs with Gaussian Processes](https://openreview.net/forum?id=q4AEBLHuA6) |  | 0 |  | Shikai Fang, Madison Cooley, Da Long, Shibo Li, Mike Kirby, Shandian Zhe |  |
| 1230 |  |  [OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text](https://openreview.net/forum?id=jKHmjlpViu) |  | 0 |  | Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, Jimmy Ba |  |
| 1231 |  |  [Replay across Experiments: A Natural Extension of Off-Policy RL](https://openreview.net/forum?id=Nf4Lm6fXN8) |  | 0 |  | Dhruva Tirumala, Thomas Lampe, José Enrique Chen, Tuomas Haarnoja, Sandy H. Huang, Guy Lever, Ben Moran, Tim Hertweck, Leonard Hasenclever, Martin A. Riedmiller, Nicolas Heess, Markus Wulfmeier |  |
| 1232 |  |  [Understanding Transferable Representation Learning and Zero-shot Transfer in CLIP](https://openreview.net/forum?id=S5yOuNfSA0) |  | 0 |  | Zixiang Chen, Yihe Deng, Yuanzhi Li, Quanquan Gu |  |
| 1233 |  |  [Conditional Variational Diffusion Models](https://openreview.net/forum?id=YOKnEkIuoi) |  | 0 |  | Gabriel della Maggiora, Luis Alberto Croquevielle, Nikita Deshpande, Harry Horsley, Thomas Heinis, Artur Yakimovich |  |
| 1234 |  |  [Better Neural PDE Solvers Through Data-Free Mesh Movers](https://openreview.net/forum?id=hj9ZuNimRl) |  | 0 |  | Peiyan Hu, Yue Wang, ZhiMing Ma |  |
| 1235 |  |  [From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module](https://openreview.net/forum?id=0JsRZEGZ7L) |  | 0 |  | Claudio Battiloro, Indro Spinelli, Lev Telyatnikov, Michael M. Bronstein, Simone Scardapane, Paolo Di Lorenzo |  |
| 1236 |  |  [BEND: Benchmarking DNA Language Models on Biologically Meaningful Tasks](https://openreview.net/forum?id=uKB4cFNQFg) |  | 0 |  | Frederikke Isa Marin, Felix Teufel, Marc Horlacher, Dennis Madsen, Dennis Pultz, Ole Winther, Wouter Boomsma |  |
| 1237 |  |  [Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks](https://openreview.net/forum?id=up6hr4hIQH) |  | 0 |  | Xu Zheng, Farhad Shirani, Tianchun Wang, Wei Cheng, Zhuomin Chen, Haifeng Chen, Hua Wei, Dongsheng Luo |  |
| 1238 |  |  [Piecewise Linear Parametrization of Policies: Towards Interpretable Deep Reinforcement Learning](https://openreview.net/forum?id=hOMVq57Ce0) |  | 0 |  | Maxime Wabartha, Joelle Pineau |  |
| 1239 |  |  [Optimal criterion for feature learning of two-layer linear neural network in high dimensional interpolation regime](https://openreview.net/forum?id=Jc0FssXh2R) |  | 0 |  | Keita Suzuki, Taiji Suzuki |  |
| 1240 |  |  [Neural Optimal Transport with General Cost Functionals](https://openreview.net/forum?id=gIiz7tBtYZ) |  | 0 |  | Arip Asadulaev, Alexander Korotin, Vage Egiazarian, Petr Mokrov, Evgeny Burnaev |  |
| 1241 |  |  [A Topological Perspective on Demystifying GNN-Based Link Prediction Performance](https://openreview.net/forum?id=apA6SSXx2e) |  | 0 |  | Yu Wang, Tong Zhao, Yuying Zhao, Yunchao Liu, Xueqi Cheng, Neil Shah, Tyler Derr |  |
| 1242 |  |  [Time-Efficient Reinforcement Learning with Stochastic Stateful Policies](https://openreview.net/forum?id=5liV2xUdJL) |  | 0 |  | Firas AlHafez, Guoping Zhao, Jan Peters, Davide Tateo |  |
| 1243 |  |  [Open the Black Box: Step-based Policy Updates for Temporally-Correlated Episodic Reinforcement Learning](https://openreview.net/forum?id=mnipav175N) |  | 0 |  | Ge Li, Hongyi Zhou, Dominik Roth, Serge Thilges, Fabian Otto, Rudolf Lioutikov, Gerhard Neumann |  |
| 1244 |  |  [Emergent Communication with Conversational Repair](https://openreview.net/forum?id=Sy8upuD6Bw) |  | 0 |  | Mitja Nikolaus |  |
| 1245 |  |  [Can we get the best of both Binary Neural Networks and Spiking Neural Networks for Efficient Computer Vision?](https://openreview.net/forum?id=lGUyAuuTYZ) |  | 0 |  | Gourav Datta, Zeyu Liu, Peter Anthony Beerel |  |
| 1246 |  |  [Node2ket: Efficient High-Dimensional Network Embedding in Quantum Hilbert Space](https://openreview.net/forum?id=lROh08eK6n) |  | 0 |  | Hao Xiong, Yehui Tang, Yunlin He, Wei Tan, Junchi Yan |  |
| 1247 |  |  [Latent Representation and Simulation of Markov Processes via Time-Lagged Information Bottleneck](https://openreview.net/forum?id=bH6T0Jjw5y) |  | 0 |  | Marco Federici, Patrick Forré, Ryota Tomioka, Bastiaan S. Veeling |  |
| 1248 |  |  [Non-Exchangeable Conformal Risk Control](https://openreview.net/forum?id=j511LaqEeP) |  | 0 |  | António Farinhas, Chrysoula Zerva, Dennis Ulmer, André F. T. Martins |  |
| 1249 |  |  [Provably Efficient UCB-type Algorithms For Learning Predictive State Representations](https://openreview.net/forum?id=jId5PXbBbX) |  | 0 |  | Ruiquan Huang, Yingbin Liang, Jing Yang |  |
| 1250 |  |  [Lifting Architectural Constraints of Injective Flows](https://openreview.net/forum?id=kBNIx4Biq4) |  | 0 |  | Peter Sorrenson, Felix Draxler, Armand Rousselot, Sander Hummerich, Lea Zimmermann, Ullrich Köthe |  |
| 1251 |  |  [Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models](https://openreview.net/forum?id=ptCIlV24YZ) |  | 0 |  | Tianzhe Chu, Shengbang Tong, Tianjiao Ding, Xili Dai, Benjamin David Haeffele, René Vidal, Yi Ma |  |
| 1252 |  |  [Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond](https://openreview.net/forum?id=khAE1sTMdX) |  | 0 |  | Tianxin Wei, Bowen Jin, Ruirui Li, Hansi Zeng, Zhengyang Wang, Jianhui Sun, Qingyu Yin, Hanqing Lu, Suhang Wang, Jingrui He, Xianfeng Tang |  |
| 1253 |  |  [Orbit-Equivariant Graph Neural Networks](https://openreview.net/forum?id=GkJOCga62u) |  | 0 |  | Matthew Morris, Bernardo Cuenca Grau, Ian Horrocks |  |
| 1254 |  |  [Object-Centric Learning with Slot Mixture Module](https://openreview.net/forum?id=aBUidW4Nkd) |  | 0 |  | Daniil E. Kirilenko, Vitaliy Vorobyov, Alexey K. Kovalev, Aleksandr Panov |  |
| 1255 |  |  [Ring-A-Bell! How Reliable are Concept Removal Methods For Diffusion Models?](https://openreview.net/forum?id=lm7MRcsFiS) |  | 0 |  | YuLin Tsai, ChiaYi Hsu, Chulin Xie, ChihHsun Lin, JiaYou Chen, Bo Li, PinYu Chen, ChiaMu Yu, ChunYing Huang |  |
| 1256 |  |  [PerceptionCLIP: Visual Classification by Inferring and Conditioning on Contexts](https://openreview.net/forum?id=2Oiee202rd) |  | 0 |  | Bang An, Sicheng Zhu, MichaelAndrei PanaitescuLiess, Chaithanya Kumar Mummadi, Furong Huang |  |
| 1257 |  |  [VeRA: Vector-based Random Matrix Adaptation](https://openreview.net/forum?id=NjNfLdxr3A) |  | 0 |  | Dawid Jan Kopiczko, Tijmen Blankevoort, Yuki M. Asano |  |
| 1258 |  |  [AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?](https://openreview.net/forum?id=Bb21JPnhhr) |  | 0 |  | Qi Zhao, Shijie Wang, Ce Zhang, Changcheng Fu, Minh Quan Do, Nakul Agarwal, Kwonjoon Lee, Chen Sun |  |
| 1259 |  |  [A Plug-and-Play Image Registration Network](https://openreview.net/forum?id=DGez4B2a6Y) |  | 0 |  | Junhao Hu, Weijie Gan, Zhixin Sun, Hongyu An, Ulugbek Kamilov |  |
| 1260 |  |  [BENO: Boundary-embedded Neural Operators for Elliptic PDEs](https://openreview.net/forum?id=ZZTkLDRmkg) |  | 0 |  | Haixin Wang, Jiaxin Li, Anubhav Dwivedi, Kentaro Hara, Tailin Wu |  |
| 1261 |  |  [Bayesian Neural Controlled Differential Equations for Treatment Effect Estimation](https://openreview.net/forum?id=uwO71a8wET) |  | 0 |  | Konstantin Hess, Valentyn Melnychuk, Dennis Frauen, Stefan Feuerriegel |  |
| 1262 |  |  [Training Socially Aligned Language Models on Simulated Social Interactions](https://openreview.net/forum?id=NddKiWtdUm) |  | 0 |  | Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Diyi Yang, Soroush Vosoughi |  |
| 1263 |  |  [The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks](https://openreview.net/forum?id=vE1e1mLJ0U) |  | 0 |  | Aaron Spieler, Nasim Rahaman, Georg Martius, Bernhard Schölkopf, Anna Levina |  |
| 1264 |  |  [Incentivized Truthful Communication for Federated Bandits](https://openreview.net/forum?id=ykEixGIJYb) |  | 0 |  | Zhepei Wei, Chuanhao Li, Tianze Ren, Haifeng Xu, Hongning Wang |  |
| 1265 |  |  [Large-scale Training of Foundation Models for Wearable Biosignals](https://openreview.net/forum?id=pC3WJHf51j) |  | 0 |  | Salar Abbaspourazad, Oussama Elachqar, Andrew C. Miller, Saba Emrani, Udhyakumar Nallasamy, Ian Shapiro |  |
| 1266 |  |  [Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization](https://openreview.net/forum?id=OIsahq1UYC) |  | 0 |  | Dinghuai Zhang, Ricky T. Q. Chen, ChengHao Liu, Aaron C. Courville, Yoshua Bengio |  |
| 1267 |  |  [On Trajectory Augmentations for Off-Policy Evaluation](https://openreview.net/forum?id=eMNN0wIyVw) |  | 0 |  | Ge Gao, Qitong Gao, Xi Yang, Song Ju, Miroslav Pajic, Min Chi |  |
| 1268 |  |  [Federated Wasserstein Distance](https://openreview.net/forum?id=rsg1mvUahT) |  | 0 |  | Alain Rakotomamonjy, Kimia Nadjahi, Liva Ralaivola |  |
| 1269 |  |  [Fourier Transporter: Bi-Equivariant Robotic Manipulation in 3D](https://openreview.net/forum?id=UulwvAU1W0) |  | 0 |  | Haojie Huang, Owen Howell, Dian Wang, Xupeng Zhu, Robert Platt, Robin Walters |  |
| 1270 |  |  [Clifford Group Equivariant Simplicial Message Passing Networks](https://openreview.net/forum?id=Zz594UBNOH) |  | 0 |  | Cong Liu, David Ruhe, Floor Eijkelboom, Patrick Forré |  |
| 1271 |  |  [Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation](https://openreview.net/forum?id=NxoFmGgWC9) |  | 0 |  | Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, Tao Kong |  |
| 1272 |  |  [Vision-by-Language for Training-Free Compositional Image Retrieval](https://openreview.net/forum?id=EDPxCjXzSb) |  | 0 |  | Shyamgopal Karthik, Karsten Roth, Massimiliano Mancini, Zeynep Akata |  |
| 1273 |  |  [Benign Oscillation of Stochastic Gradient Descent with Large Learning Rate](https://openreview.net/forum?id=wYmvN3sQpG) |  | 0 |  | Miao Lu, Beining Wu, Xiaodong Yang, Difan Zou |  |
| 1274 |  |  [RobustTSF: Towards Theory and Design of Robust Time Series Forecasting with Anomalies](https://openreview.net/forum?id=ltZ9ianMth) |  | 0 |  | Hao Cheng, Qingsong Wen, Yang Liu, Liang Sun |  |
| 1275 |  |  [Understanding the Effects of RLHF on LLM Generalisation and Diversity](https://openreview.net/forum?id=PXD3FAVHJT) |  | 0 |  | Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette, Roberta Raileanu |  |
| 1276 |  |  [GAIA: Zero-shot Talking Avatar Generation](https://openreview.net/forum?id=ATEawsFUj4) |  | 0 |  | Tianyu He, Junliang Guo, Runyi Yu, Yuchi Wang, Jialiang Zhu, Kaikai An, Leyi Li, Xu Tan, Chunyu Wang, Han Hu, HsiangTao Wu, Sheng Zhao, Jiang Bian |  |
| 1277 |  |  [Probabilistic Self-supervised Representation Learning via Scoring Rules Minimization](https://openreview.net/forum?id=skcTCdJz0f) |  | 0 |  | Amirhossein Vahidi, Simon Schoßer, Lisa Wimmer, Yawei Li, Bernd Bischl, Eyke Hüllermeier, Mina Rezaei |  |
| 1278 |  |  [DORSal: Diffusion for Object-centric Representations of Scenes et al](https://openreview.net/forum?id=3zvB14IF6D) |  | 0 |  | Allan Jabri, Sjoerd van Steenkiste, Emiel Hoogeboom, Mehdi S. M. Sajjadi, Thomas Kipf |  |
| 1279 |  |  [T-Rep: Representation Learning for Time Series using Time-Embeddings](https://openreview.net/forum?id=3y2TfP966N) |  | 0 |  | Archibald Fraikin, Adrien Bennetot, Stéphanie Allassonnière |  |
| 1280 |  |  [Beyond Stationarity: Convergence Analysis of Stochastic Softmax Policy Gradient Methods](https://openreview.net/forum?id=1VeQ6VBbev) |  | 0 |  | Sara Klein, Simon Weissmann, Leif Döring |  |
| 1281 |  |  [SliceGPT: Compress Large Language Models by Deleting Rows and Columns](https://openreview.net/forum?id=vXxardq6db) |  | 0 |  | Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari Do Nascimento, Torsten Hoefler, James Hensman |  |
| 1282 |  |  [Experimental Design for Multi-Channel Imaging via Task-Driven Feature Selection](https://openreview.net/forum?id=MloaGA6WwX) |  | 0 |  | Stefano B. Blumberg, Paddy J. Slator, Daniel C. Alexander |  |
| 1283 |  |  [Convolutional Deep Kernel Machines](https://openreview.net/forum?id=1oqedRt6Z7) |  | 0 |  | Edward Milsom, Ben Anson, Laurence Aitchison |  |
| 1284 |  |  [Weakly Supervised Virus Capsid Detection with Image-Level Annotations in Electron Microscopy Images](https://openreview.net/forum?id=RJDjSXNuAZ) |  | 0 |  | Hannah Kniesel, Leon Sick, Tristan Payer, Tim Bergner, Kavitha Shaga Devan, Clarissa Read, Paul Walther, Timo Ropinski, Pedro Hermosilla |  |
| 1285 |  |  [Chain-of-Experts: When LLMs Meet Complex Operations Research Problems](https://openreview.net/forum?id=HobyL1B9CZ) |  | 0 |  | Ziyang Xiao, Dongxiang Zhang, Yangjun Wu, Lilin Xu, Yuan Jessica Wang, Xiongwei Han, Xiaojin Fu, Tao Zhong, Jia Zeng, Mingli Song, Gang Chen |  |
| 1286 |  |  [On the Reliability of Watermarks for Large Language Models](https://openreview.net/forum?id=DEJIDCmWOz) |  | 0 |  | John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, Tom Goldstein |  |
| 1287 |  |  [Near-Optimal Solutions of Constrained Learning Problems](https://openreview.net/forum?id=fDaLmkdSKU) |  | 0 |  | Juan Elenter, Luiz F. O. Chamon, Alejandro Ribeiro |  |
| 1288 |  |  [Delphic Offline Reinforcement Learning under Nonidentifiable Hidden Confounding](https://openreview.net/forum?id=lUYY2qsRTI) |  | 0 |  | Alizée Pace, Hugo Yèche, Bernhard Schölkopf, Gunnar Rätsch, Guy Tennenholtz |  |
| 1289 |  |  [Leave-one-out Distinguishability in Machine Learning](https://openreview.net/forum?id=9RNfX0ah0K) |  | 0 |  | Jiayuan Ye, Anastasia Borovykh, Soufiane Hayou, Reza Shokri |  |
| 1290 |  |  [Increasing Model Capacity for Free: A Simple Strategy for Parameter Efficient Fine-tuning](https://openreview.net/forum?id=H3IUunLy8s) |  | 0 |  | Haobo Song, Hao Zhao, Soumajit Majumder, Tao Lin |  |
| 1291 |  |  [Brain decoding: toward real-time reconstruction of visual perception](https://openreview.net/forum?id=3y1K6buO8c) |  | 0 |  | Yohann Benchetrit, Hubert J. Banville, JeanRemi King |  |
| 1292 |  |  [Linear Log-Normal Attention with Unbiased Concentration](https://openreview.net/forum?id=5nM2AHzqUj) |  | 0 |  | Yury Nahshan, Joseph Kampeas, Emir Haleva |  |
| 1293 |  |  [Prioritized Soft Q-Decomposition for Lexicographic Reinforcement Learning](https://openreview.net/forum?id=c0MyyXyGfn) |  | 0 |  | Finn Rietz, Erik Schaffernicht, Stefan Heinrich, Johannes A. Stork |  |
| 1294 |  |  [Energy-guided Entropic Neural Optimal Transport](https://openreview.net/forum?id=d6tUsZeVs7) |  | 0 |  | Petr Mokrov, Alexander Korotin, Alexander Kolesov, Nikita Gushchin, Evgeny Burnaev |  |
| 1295 |  |  [Learning Semantic Proxies from Visual Prompts for Parameter-Efficient Fine-Tuning in Deep Metric Learning](https://openreview.net/forum?id=TWVMVPx2wO) |  | 0 |  | Li Ren, Chen Chen, Liqiang Wang, Kien A. Hua |  |
| 1296 |  |  [Fine-Tuned Language Models Generate Stable Inorganic Materials as Text](https://openreview.net/forum?id=vN9fpfqoP1) |  | 0 |  | Nate Gruver, Anuroop Sriram, Andrea Madotto, Andrew Gordon Wilson, C. Lawrence Zitnick, Zachary W. Ulissi |  |
| 1297 |  |  [TextField3D: Towards Enhancing Open-Vocabulary 3D Generation with Noisy Text Fields](https://openreview.net/forum?id=WOiOzHG2zD) |  | 0 |  | Tianyu Huang, Yihan Zeng, Bowen Dong, Hang Xu, Songcen Xu, Rynson W. H. Lau, Wangmeng Zuo |  |
| 1298 |  |  [GlucoBench: Curated List of Continuous Glucose Monitoring Datasets with Prediction Benchmarks](https://openreview.net/forum?id=cUSNs8nGaV) |  | 0 |  | Renat Sergazinov, Elizabeth Chun, Valeriya Rogovchenko, Nathaniel J. Fernandes, Nicholas Kasman, Irina Gaynanova |  |
| 1299 |  |  [Quantifying and Enhancing Multi-modal Robustness with Modality Preference](https://openreview.net/forum?id=XyrB1Ay44j) |  | 0 |  | Zequn Yang, Yake Wei, Ce Liang, Di Hu |  |
| 1300 |  |  [Advancing the Lower Bounds: an Accelerated, Stochastic, Second-order Method with Optimal Adaptation to Inexactness](https://openreview.net/forum?id=otU31x3fus) |  | 0 |  | Artem Agafonov, Dmitry Kamzolov, Alexander V. Gasnikov, Ali Kavis, Kimon Antonakopoulos, Volkan Cevher, Martin Takác |  |
| 1301 |  |  [Distinguished In Uniform: Self-Attention Vs. Virtual Nodes](https://openreview.net/forum?id=AcSChDWL6V) |  | 0 |  | Eran Rosenbluth, Jan Tönshoff, Martin Ritzert, Berke Kisin, Martin Grohe |  |
| 1302 |  |  [Faster Approximation of Probabilistic and Distributional Values via Least Squares](https://openreview.net/forum?id=lvSMIsztka) |  | 0 |  | Weida Li, Yaoliang Yu |  |
| 1303 |  |  [Rigid Protein-Protein Docking via Equivariant Elliptic-Paraboloid Interface Prediction](https://openreview.net/forum?id=zgQ0PHeGnL) |  | 0 |  | Ziyang Yu, Wenbing Huang, Yang Liu |  |
| 1304 |  |  [Incentive-Aware Federated Learning with Training-Time Model Rewards](https://openreview.net/forum?id=FlY7WQ2hWS) |  | 0 |  | Zhaoxuan Wu, Mohammad Mohammadi Amiri, Ramesh Raskar, Bryan Kian Hsiang Low |  |
| 1305 |  |  [Removing Biases from Molecular Representations via Information Maximization](https://openreview.net/forum?id=7TOs9gjAg1) |  | 0 |  | Chenyu Wang, Sharut Gupta, Caroline Uhler, Tommi S. Jaakkola |  |
| 1306 |  |  [Neural Architecture Retrieval](https://openreview.net/forum?id=1JtTPYBKqt) |  | 0 |  | Xiaohuan Pei, Yanxi Li, Minjing Dong, Chang Xu |  |
| 1307 |  |  [Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization](https://openreview.net/forum?id=QibPzdVrRu) |  | 0 |  | Hancheng Min, Enrique Mallada, René Vidal |  |
| 1308 |  |  [Rethinking the Uniformity Metric in Self-Supervised Learning](https://openreview.net/forum?id=3pf2hEdu8B) |  | 0 |  | Xianghong Fang, Jian Li, Qiang Sun, Benyou Wang |  |
| 1309 |  |  [Learning Decentralized Partially Observable Mean Field Control for Artificial Collective Behavior](https://openreview.net/forum?id=99tKiMVJhY) |  | 0 |  | Kai Cui, Sascha Hauck, Christian Fabian, Heinz Koeppl |  |
| 1310 |  |  [TAB: Temporal Accumulated Batch Normalization in Spiking Neural Networks](https://openreview.net/forum?id=k1wlmtPGLq) |  | 0 |  | Haiyan Jiang, Vincent Zoonekynd, Giulia De Masi, Bin Gu, Huan Xiong |  |
| 1311 |  |  [StructComp: Substituting propagation with Structural Compression in Training Graph Contrastive Learning](https://openreview.net/forum?id=a4DBEeGfQq) |  | 0 |  | Shengzhong Zhang, Wenjie Yang, Xinyuan Cao, Hongwei Zhang, Zengfeng Huang |  |
| 1312 |  |  [Particle Guidance: non-I.I.D. Diverse Sampling with Diffusion Models](https://openreview.net/forum?id=KqbCvIFBY7) |  | 0 |  | Gabriele Corso, Yilun Xu, Valentin De Bortoli, Regina Barzilay, Tommi S. Jaakkola |  |
| 1313 |  |  [On Adversarial Training without Perturbing all Examples](https://openreview.net/forum?id=pE6gWrASQm) |  | 0 |  | Max Maria Losch, Mohamed Omran, David Stutz, Mario Fritz, Bernt Schiele |  |
| 1314 |  |  [Diving Segmentation Model into Pixels](https://openreview.net/forum?id=KBo7Z5aTV0) |  | 0 |  | Chen Gan, Zihao Yin, Kelei He, Yang Gao, Junfeng Zhang |  |
| 1315 |  |  [General Stability Analysis for Zeroth-Order Optimization Algorithms](https://openreview.net/forum?id=AfhNyr73Ma) |  | 0 |  | Xinyue Liu, Hualin Zhang, Bin Gu, Hong Chen |  |
| 1316 |  |  [Hybrid Sharing for Multi-Label Image Classification](https://openreview.net/forum?id=yVJd8lKyVX) |  | 0 |  | Zihao Yin, Chen Gan, Kelei He, Yang Gao, Junfeng Zhang |  |
| 1317 |  |  [Symmetric Single Index Learning](https://openreview.net/forum?id=e1vqloonRy) |  | 0 |  | Aaron Zweig, Joan Bruna |  |
| 1318 |  |  [An improved analysis of per-sample and per-update clipping in federated learning](https://openreview.net/forum?id=BdPvGRvoBC) |  | 0 |  | Bo Li, Xiaowen Jiang, Mikkel N. Schmidt, Tommy Sonne Alstrøm, Sebastian U. Stich |  |
| 1319 |  |  [Sample-Efficiency in Multi-Batch Reinforcement Learning: The Need for Dimension-Dependent Adaptivity](https://openreview.net/forum?id=ey3GhWXQ97) |  | 0 |  | Emmeran Johnson, Ciara PikeBurke, Patrick Rebeschini |  |
| 1320 |  |  [On the Power of the Weisfeiler-Leman Test for Graph Motif Parameters](https://openreview.net/forum?id=FddFxi08J3) |  | 0 |  | Matthias Lanzinger, Pablo Barceló |  |
| 1321 |  |  [PAC Prediction Sets Under Label Shift](https://openreview.net/forum?id=4vPVBh3fhz) |  | 0 |  | Wenwen Si, Sangdon Park, Insup Lee, Edgar Dobriban, Osbert Bastani |  |
| 1322 |  |  [Memorization in Self-Supervised Learning Improves Downstream Generalization](https://openreview.net/forum?id=KSjPaXtxP8) |  | 0 |  | Wenhao Wang, Muhammad Ahmad Kaleem, Adam Dziedzic, Michael Backes, Nicolas Papernot, Franziska Boenisch |  |
| 1323 |  |  [The Curse of Diversity in Ensemble-Based Exploration](https://openreview.net/forum?id=M3QXCOTTk4) |  | 0 |  | Zhixuan Lin, Pierluca D'Oro, Evgenii Nikishin, Aaron C. Courville |  |
| 1324 |  |  [Multilinear Operator Networks](https://openreview.net/forum?id=bbCL5aRjUx) |  | 0 |  | Yixin Cheng, Grigorios Chrysos, Markos Georgopoulos, Volkan Cevher |  |
| 1325 |  |  [Leveraging Generative Models for Unsupervised Alignment of Neural Time Series Data](https://openreview.net/forum?id=9zhHVyLY4K) |  | 0 |  | Ayesha Vermani, Il Memming Park, Josue Nassar |  |
| 1326 |  |  [UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition](https://openreview.net/forum?id=r65xfUb76p) |  | 0 |  | Wenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, Hoifung Poon |  |
| 1327 |  |  [Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data](https://openreview.net/forum?id=W8S8SxS9Ng) |  | 0 |  | Antonis Antoniades, Yiyi Yu, Joseph Canzano, William Yang Wang, Spencer L. Smith |  |
| 1328 |  |  [Off-Policy Primal-Dual Safe Reinforcement Learning](https://openreview.net/forum?id=vy42bYs1Wo) |  | 0 |  | Zifan Wu, Bo Tang, Qian Lin, Chao Yu, Shangqin Mao, Qianlong Xie, Xingxing Wang, Dong Wang |  |
| 1329 |  |  [An Extensible Framework for Open Heterogeneous Collaborative Perception](https://openreview.net/forum?id=KkrDUGIASk) |  | 0 |  | Yifan Lu, Yue Hu, Yiqi Zhong, Dequan Wang, Yanfeng Wang, Siheng Chen |  |
| 1330 |  |  [Neural structure learning with stochastic differential equations](https://openreview.net/forum?id=V1GM9xDvIY) |  | 0 |  | Benjie Wang, Joel Jennings, Wenbo Gong |  |
| 1331 |  |  [STARC: A General Framework For Quantifying Differences Between Reward Functions](https://openreview.net/forum?id=wPhbtwlCDa) |  | 0 |  | Joar Max Viktor Skalse, Lucy Farnik, Sumeet Ramesh Motwani, Erik Jenner, Adam Gleave, Alessandro Abate |  |
| 1332 |  |  [GAIA: a benchmark for General AI Assistants](https://openreview.net/forum?id=fibxvahvs3) |  | 0 |  | Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, Thomas Scialom |  |
| 1333 |  |  [A differentiable brain simulator bridging brain simulation and brain-inspired computing](https://openreview.net/forum?id=AU2gS9ut61) |  | 0 |  | Chaoming Wang, Tianqiu Zhang, Sichao He, Hongyaoxing Gu, Shangyang Li, Si Wu |  |
| 1334 |  |  [FOSI: Hybrid First and Second Order Optimization](https://openreview.net/forum?id=NvbeD9Ttkx) |  | 0 |  | Hadar Sivan, Moshe Gabel, Assaf Schuster |  |
| 1335 |  |  [Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach](https://openreview.net/forum?id=zwU9scoU4A) |  | 0 |  | Christian Fabian, Kai Cui, Heinz Koeppl |  |
| 1336 |  |  [Unraveling the Key Components of OOD Generalization via Diversification](https://openreview.net/forum?id=Lvf7GnaLru) |  | 0 |  | Harold Benoit, Liangze Jiang, Andrei Atanov, Oguzhan Fatih Kar, Mattia Rigotti, Amir Zamir |  |
| 1337 |  |  [Deep Generative Clustering with Multimodal Diffusion Variational Autoencoders](https://openreview.net/forum?id=k5THrhXDV3) |  | 0 |  | Emanuele Palumbo, Laura Manduchi, Sonia Laguna, Daphné Chopard, Julia E. Vogt |  |
| 1338 |  |  [Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning](https://openreview.net/forum?id=AY6aM13gGF) |  | 0 |  | Ruizhe Shi, Yuyao Liu, Yanjie Ze, Simon Shaolei Du, Huazhe Xu |  |
| 1339 |  |  [Equivariant Scalar Fields for Molecular Docking with Fast Fourier Transforms](https://openreview.net/forum?id=BIveOmD1Nh) |  | 0 |  | Bowen Jing, Tommi S. Jaakkola, Bonnie Berger |  |
| 1340 |  |  [The Alignment Problem from a Deep Learning Perspective](https://openreview.net/forum?id=fh8EYKFKns) |  | 0 |  | Richard Ngo, Lawrence Chan, Sören Mindermann |  |
| 1341 |  |  [Discovering Temporally-Aware Reinforcement Learning Algorithms](https://openreview.net/forum?id=MJJcs3zbmi) |  | 0 |  | Matthew Thomas Jackson, Chris Lu, Louis Kirsch, Robert Tjarko Lange, Shimon Whiteson, Jakob Nicolaus Foerster |  |
| 1342 |  |  [Guiding Masked Representation Learning to Capture Spatio-Temporal Relationship of Electrocardiogram](https://openreview.net/forum?id=WcOohbsF4H) |  | 0 |  | Yeongyeon Na, Minje Park, Yunwon Tae, Sunghoon Joo |  |
| 1343 |  |  [How Realistic Is Your Synthetic Data? Constraining Deep Generative Models for Tabular Data](https://openreview.net/forum?id=tBROYsEz9G) |  | 0 |  | Mihaela C. Stoian, Salijona Dyrmishi, Maxime Cordy, Thomas Lukasiewicz, Eleonora Giunchiglia |  |
| 1344 |  |  [Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks](https://openreview.net/forum?id=s8cMuxI5gu) |  | 0 |  | Yanbo Wang, Jian Liang, Ran He |  |
| 1345 |  |  [From Zero to Turbulence: Generative Modeling for 3D Flow Simulation](https://openreview.net/forum?id=ZhlwoC1XaN) |  | 0 |  | Marten Lienen, David Lüdke, Jan HansenPalmus, Stephan Günnemann |  |
| 1346 |  |  [INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection](https://openreview.net/forum?id=Zj12nzlQbz) |  | 0 |  | Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, Jieping Ye |  |
| 1347 |  |  [DisenBooth: Identity-Preserving Disentangled Tuning for Subject-Driven Text-to-Image Generation](https://openreview.net/forum?id=FlhjUkC7vH) |  | 0 |  | Hong Chen, Yipeng Zhang, Simin Wu, Xin Wang, Xuguang Duan, Yuwei Zhou, Wenwu Zhu |  |
| 1348 |  |  [Improving equilibrium propagation without weight symmetry through Jacobian homeostasis](https://openreview.net/forum?id=kUveo5k1GF) |  | 0 |  | Axel Laborieux, Friedemann Zenke |  |
| 1349 |  |  [Revisiting Data Augmentation in Deep Reinforcement Learning](https://openreview.net/forum?id=EGQBpkIEuu) |  | 0 |  | Jianshu Hu, Yunpeng Jiang, Paul Weng |  |
| 1350 |  |  [Structural Inference with Dynamics Encoding and Partial Correlation Coefficients](https://openreview.net/forum?id=TKnzPdyeJu) |  | 0 |  | Aoran Wang, Jun Pang |  |
| 1351 |  |  [Simplicial Representation Learning with Neural k-Forms](https://openreview.net/forum?id=Djw0XhjHZb) |  | 0 |  | Kelly Maggs, Celia Hacker, Bastian Rieck |  |
| 1352 |  |  [Improved Analysis of Sparse Linear Regression in Local Differential Privacy Model](https://openreview.net/forum?id=cVUOnF7iVp) |  | 0 |  | Liyang Zhu, Meng Ding, Vaneet Aggarwal, Jinhui Xu, Di Wang |  |
| 1353 |  |  [Toward effective protection against diffusion-based mimicry through score distillation](https://openreview.net/forum?id=NzxCMe88HX) |  | 0 |  | Haotian Xue, Chumeng Liang, Xiaoyu Wu, Yongxin Chen |  |
| 1354 |  |  [Sign2GPT: Leveraging Large Language Models for Gloss-Free Sign Language Translation](https://openreview.net/forum?id=LqaEEs3UxU) |  | 0 |  | Ryan Wong, Necati Cihan Camgöz, Richard Bowden |  |
| 1355 |  |  [Cauchy-Schwarz Divergence Information Bottleneck for Regression](https://openreview.net/forum?id=7wY67ZDQTE) |  | 0 |  | Shujian Yu, Xi Yu, Sigurd Løkse, Robert Jenssen, José C. Príncipe |  |
| 1356 |  |  [SALMONN: Towards Generic Hearing Abilities for Large Language Models](https://openreview.net/forum?id=14rn7HpKVk) |  | 0 |  | Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Chao Zhang |  |
| 1357 |  |  [Kalman Filter for Online Classification of Non-Stationary Data](https://openreview.net/forum?id=ZzmKEpze8e) |  | 0 |  | Michalis K. Titsias, Alexandre Galashov, Amal RannenTriki, Razvan Pascanu, Yee Whye Teh, Jörg Bornschein |  |
| 1358 |  |  [Magnushammer: A Transformer-Based Approach to Premise Selection](https://openreview.net/forum?id=oYjPk8mqAV) |  | 0 |  | Maciej Mikula, Szymon Tworkowski, Szymon Antoniak, Bartosz Piotrowski, Albert Q. Jiang, Jin Peng Zhou, Christian Szegedy, Lukasz Kucinski, Piotr Milos, Yuhuai Wu |  |
| 1359 |  |  [Learning to Compose: Improving Object Centric Learning by Injecting Compositionality](https://openreview.net/forum?id=HT2dAhh4uV) |  | 0 |  | Whie Jung, Jaehoon Yoo, Sungjin Ahn, Seunghoon Hong |  |
| 1360 |  |  [Light Schrödinger Bridge](https://openreview.net/forum?id=WhZoCLRWYJ) |  | 0 |  | Alexander Korotin, Nikita Gushchin, Evgeny Burnaev |  |
| 1361 |  |  [Reward-Free Curricula for Training Robust World Models](https://openreview.net/forum?id=eCGpNGDeNu) |  | 0 |  | Marc Rigter, Minqi Jiang, Ingmar Posner |  |
| 1362 |  |  [Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design](https://openreview.net/forum?id=7UhxsmbdaQ) |  | 0 |  | Jeff Guo, Philippe Schwaller |  |
| 1363 |  |  [Leveraging Uncertainty Estimates To Improve Classifier Performance](https://openreview.net/forum?id=nsNyDvNQTc) |  | 0 |  | Gundeep Arora, Srujana Merugu, Anoop Saladi, Rajeev Rastogi |  |
| 1364 |  |  [C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion](https://openreview.net/forum?id=jzzEHTBFOT) |  | 0 |  | Hee Suk Yoon, Eunseop Yoon, Joshua Tian Jin Tee, Mark A. HasegawaJohnson, Yingzhen Li, Chang D. Yoo |  |
| 1365 |  |  [Retrieval-Enhanced Contrastive Vision-Text Models](https://openreview.net/forum?id=b2UlHeyyC0) |  | 0 |  | Ahmet Iscen, Mathilde Caron, Alireza Fathi, Cordelia Schmid |  |
| 1366 |  |  [Deep Neural Network Initialization with Sparsity Inducing activations](https://openreview.net/forum?id=uvXK8Xk9Jk) |  | 0 |  | Ilan Price, Nicholas Daultry Ball, Adam C. Jones, Samuel C. H. Lam, Jared Tanner |  |
| 1367 |  |  [PeFLL: Personalized Federated Learning by Learning to Learn](https://openreview.net/forum?id=MrYiwlDRQO) |  | 0 |  | Jonathan Scott, Hossein Zakerinia, Christoph H. Lampert |  |
| 1368 |  |  [Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging](https://openreview.net/forum?id=xx0ITyHp3u) |  | 0 |  | Max Zimmer, Christoph Spiegel, Sebastian Pokutta |  |
| 1369 |  |  [DP-SGD Without Clipping: The Lipschitz Neural Network Way](https://openreview.net/forum?id=BEyEziZ4R6) |  | 0 |  | Louis Béthune, Thomas Massena, Thibaut Boissin, Aurélien Bellet, Franck Mamalet, Yannick Prudent, Corentin Friedrich, Mathieu Serrurier, David Vigouroux |  |
| 1370 |  |  [Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo](https://openreview.net/forum?id=nfIAEJFiBZ) |  | 0 |  | Haque Ishfaq, Qingfeng Lan, Pan Xu, A. Rupam Mahmood, Doina Precup, Anima Anandkumar, Kamyar Azizzadenesheli |  |
| 1371 |  |  [Human Feedback is not Gold Standard](https://openreview.net/forum?id=7W3GLNImfS) |  | 0 |  | Tom Hosking, Phil Blunsom, Max Bartolo |  |
| 1372 |  |  [Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering](https://openreview.net/forum?id=L3FHMoKZcS) |  | 0 |  | Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine A. Heller, Subhrajit Roy |  |
| 1373 |  |  [Posterior Sampling Based on Gradient Flows of the MMD with Negative Distance Kernel](https://openreview.net/forum?id=YrXHEb2qMb) |  | 0 |  | Paul Hagemann, Johannes Hertrich, Fabian Altekrüger, Robert Beinert, Jannis Chemseddine, Gabriele Steidl |  |
| 1374 |  |  [First-order ANIL provably learns representations despite overparametrisation](https://openreview.net/forum?id=if2vRbS8Ew) |  | 0 |  | Oguz Kaan Yüksel, Etienne Boursier, Nicolas Flammarion |  |
| 1375 |  |  [Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning](https://openreview.net/forum?id=ZGNWW7xZ6Q) |  | 0 |  | Linhao Luo, YuanFang Li, Gholamreza Haffari, Shirui Pan |  |
| 1376 |  |  [How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions](https://openreview.net/forum?id=567BjxgaTp) |  | 0 |  | Lorenzo Pacchiardi, Alex James Chan, Sören Mindermann, Ilan Moscovitz, Alexa Y. Pan, Yarin Gal, Owain Evans, Jan Markus Brauner |  |
| 1377 |  |  [CPPO: Continual Learning for Reinforcement Learning with Human Feedback](https://openreview.net/forum?id=86zAUE80pP) |  | 0 |  | Han Zhang, Yu Lei, Lin Gui, Min Yang, Yulan He, Hui Wang, Ruifeng Xu |  |
| 1378 |  |  [Learning Optimal Contracts: How to Exploit Small Action Spaces](https://openreview.net/forum?id=WKuimaBj4I) |  | 0 |  | Francesco Bacchiocchi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti |  |
| 1379 |  |  [AGILE3D: Attention Guided Interactive Multi-object 3D Segmentation](https://openreview.net/forum?id=9cQtXpRshE) |  | 0 |  | Yuanwen Yue, Sabarinath Mahadevan, Jonas Schult, Francis Engelmann, Bastian Leibe, Konrad Schindler, Theodora Kontogianni |  |
| 1380 |  |  [A Multi-Level Framework for Accelerating Training Transformer Models](https://openreview.net/forum?id=BI1N3lTWtn) |  | 0 |  | Longwei Zou, Han Zhang, Yangdong Deng |  |
| 1381 |  |  [Online Information Acquisition: Hiring Multiple Agents](https://openreview.net/forum?id=oQKKlzxV1o) |  | 0 |  | Federico Cacciamani, Matteo Castiglioni, Nicola Gatti |  |
| 1382 |  |  [Learning Multi-Faceted Prototypical User Interests](https://openreview.net/forum?id=MzjiMxlWab) |  | 0 |  | NhuThuat Tran, Hady W. Lauw |  |
| 1383 |  |  [Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations](https://openreview.net/forum?id=wZWTHU7AsQ) |  | 0 |  | Yongyuan Liang, Yanchao Sun, Ruijie Zheng, Xiangyu Liu, Benjamin Eysenbach, Tuomas Sandholm, Furong Huang, Stephen Marcus McAleer |  |
| 1384 |  |  [Bandits with Replenishable Knapsacks: the Best of both Worlds](https://openreview.net/forum?id=yBIJRIYTqa) |  | 0 |  | Martino Bernasconi, Matteo Castiglioni, Andrea Celli, Federico Fusco |  |
| 1385 |  |  [Out-of-Variable Generalisation for Discriminative Models](https://openreview.net/forum?id=zwMfg9PfPs) |  | 0 |  | Siyuan Guo, Jonas Bernhard Wildberger, Bernhard Schölkopf |  |
| 1386 |  |  [Training Unbiased Diffusion Models From Biased Dataset](https://openreview.net/forum?id=39cPKijBed) |  | 0 |  | Yeongmin Kim, Byeonghu Na, Minsang Park, JoonHo Jang, Dongjun Kim, Wanmo Kang, IlChul Moon |  |
| 1387 |  |  [The optimality of kernel classifiers in Sobolev space](https://openreview.net/forum?id=JfqN3gu0i7) |  | 0 |  | Jianfa Lai, Zhifan Li, Dongming Huang, Qian Lin |  |
| 1388 |  |  [Neural Fourier Transform: A General Approach to Equivariant Representation Learning](https://openreview.net/forum?id=eOCvA8iwXH) |  | 0 |  | Masanori Koyama, Kenji Fukumizu, Kohei Hayashi, Takeru Miyato |  |
| 1389 |  |  [On Harmonizing Implicit Subpopulations](https://openreview.net/forum?id=3GurO0kRue) |  | 0 |  | Feng Hong, Jiangchao Yao, Yueming Lyu, Zhihan Zhou, Ivor W. Tsang, Ya Zhang, Yanfeng Wang |  |
| 1390 |  |  [Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators](https://openreview.net/forum?id=oOwDQl8haC) |  | 0 |  | Yaniv Blumenfeld, Itay Hubara, Daniel Soudry |  |
| 1391 |  |  [Continual Learning in the Presence of Spurious Correlations: Analyses and a Simple Baseline](https://openreview.net/forum?id=3Y7r6xueJJ) |  | 0 |  | Donggyu Lee, Sangwon Jung, Taesup Moon |  |
| 1392 |  |  [Rethinking Branching on Exact Combinatorial Optimization Solver: The First Deep Symbolic Discovery Framework](https://openreview.net/forum?id=jKhNBulNMh) |  | 0 |  | Yufei Kuang, Jie Wang, Haoyang Liu, Fangzhou Zhu, Xijun Li, Jia Zeng, Jianye Hao, Bin Li, Feng Wu |  |
| 1393 |  |  [Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors](https://openreview.net/forum?id=0jHkUDyEO9) |  | 0 |  | Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, HsinYing Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, Bernard Ghanem |  |
| 1394 |  |  [Adversarial Adaptive Sampling: Unify PINN and Optimal Transport for the Approximation of PDEs](https://openreview.net/forum?id=7QI7tVrh2c) |  | 0 |  | Kejun Tang, Jiayu Zhai, Xiaoliang Wan, Chao Yang |  |
| 1395 |  |  [Probabilistically Rewired Message-Passing Neural Networks](https://openreview.net/forum?id=Tj6Wcx7gVk) |  | 0 |  | Chendi Qian, Andrei Manolache, Kareem Ahmed, Zhe Zeng, Guy Van den Broeck, Mathias Niepert, Christopher Morris |  |
| 1396 |  |  [BadEdit: Backdooring Large Language Models by Model Editing](https://openreview.net/forum?id=duZANm2ABX) |  | 0 |  | Yanzhou Li, Tianlin Li, Kangjie Chen, Jian Zhang, Shangqing Liu, Wenhan Wang, Tianwei Zhang, Yang Liu |  |
| 1397 |  |  [Robust Training of Federated Models with Extremely Label Deficiency](https://openreview.net/forum?id=qxLVaYbsSI) |  | 0 |  | Yonggang Zhang, Zhiqin Yang, Xinmei Tian, Nannan Wang, Tongliang Liu, Bo Han |  |
| 1398 |  |  [On Accelerating Diffusion-Based Sampling Processes via Improved Integration Approximation](https://openreview.net/forum?id=ktJAF3lxbi) |  | 0 |  | Guoqiang Zhang, Kenta Niwa, W. Bastiaan Kleijn |  |
| 1399 |  |  [Unconstrained Stochastic CCA: Unifying Multiview and Self-Supervised Learning](https://openreview.net/forum?id=PHLVmV88Zy) |  | 0 |  | James Chapman, Lennie Wells, Ana Lawry Aguila |  |
| 1400 |  |  [The Generalization Gap in Offline Reinforcement Learning](https://openreview.net/forum?id=3w6xuXDOdY) |  | 0 |  | Ishita Mediratta, Qingfei You, Minqi Jiang, Roberta Raileanu |  |
| 1401 |  |  [Emergent mechanisms for long timescales depend on training curriculum and affect performance in memory tasks](https://openreview.net/forum?id=xwKt6bUkXj) |  | 0 |  | Sina Khajehabdollahi, Roxana Zeraati, Emmanouil Giannakakis, Tim Jakob Schäfer, Georg Martius, Anna Levina |  |
| 1402 |  |  [Sparsistency for inverse optimal transport](https://openreview.net/forum?id=wpXGPCBOTX) |  | 0 |  | Francisco Andrade, Gabriel Peyré, Clarice Poon |  |
| 1403 |  |  [Scalable Modular Network: A Framework for Adaptive Learning via Agreement Routing](https://openreview.net/forum?id=pEKJl5sflp) |  | 0 |  | Minyang Hu, Hong Chang, Bingpeng Ma, Shiguang Shan, Xilin Chen |  |
| 1404 |  |  [Noise-free Score Distillation](https://openreview.net/forum?id=dlIMcmlAdk) |  | 0 |  | Oren Katzir, Or Patashnik, Daniel CohenOr, Dani Lischinski |  |
| 1405 |  |  [Near-Optimal Quantum Algorithm for Minimizing the Maximal Loss](https://openreview.net/forum?id=pB1FeRSQxh) |  | 0 |  | Hao Wang, Chenyi Zhang, Tongyang Li |  |
| 1406 |  |  [Consistent4D: Consistent 360° Dynamic Object Generation from Monocular Video](https://openreview.net/forum?id=sPUrdFGepF) |  | 0 |  | Yanqin Jiang, Li Zhang, Jin Gao, Weiming Hu, Yao Yao |  |
| 1407 |  |  [Diverse Projection Ensembles for Distributional Reinforcement Learning](https://openreview.net/forum?id=qe49ybvvPs) |  | 0 |  | Moritz Akiya Zanger, Wendelin Boehmer, Matthijs T. J. Spaan |  |
| 1408 |  |  [MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations](https://openreview.net/forum?id=nY9nITZQjc) |  | 0 |  | Hanlei Zhang, Xin Wang, Hua Xu, Qianrui Zhou, Kai Gao, Jianhua Su, Jinyue Zhao, Wenrui Li, Yanting Chen |  |
| 1409 |  |  [Intriguing Properties of Data Attribution on Diffusion Models](https://openreview.net/forum?id=vKViCoKGcB) |  | 0 |  | Xiaosen Zheng, Tianyu Pang, Chao Du, Jing Jiang, Min Lin |  |
| 1410 |  |  [Fully Hyperbolic Convolutional Neural Networks for Computer Vision](https://openreview.net/forum?id=ekz1hN5QNh) |  | 0 |  | Ahmad Bdeir, Kristian Schwethelm, Niels Landwehr |  |
| 1411 |  |  [Enhancing Transferable Adversarial Attacks on Vision Transformers through Gradient Normalization Scaling and High-Frequency Adaptation](https://openreview.net/forum?id=1BuWv9poWz) |  | 0 |  | Zhiyu Zhu, Xinyi Wang, Zhibo Jin, Jiayu Zhang, Huaming Chen |  |
| 1412 |  |  [An interpretable error correction method for enhancing code-to-code translation](https://openreview.net/forum?id=fVxIEHGnVT) |  | 0 |  | Min Xue, Artur Andrzejak, Marla Leuther |  |
| 1413 |  |  [RLIF: Interactive Imitation Learning as Reinforcement Learning](https://openreview.net/forum?id=oLLZhbBSOU) |  | 0 |  | Jianlan Luo, Perry Dong, Yuexiang Zhai, Yi Ma, Sergey Levine |  |
| 1414 |  |  [The Need for Speed: Pruning Transformers with One Recipe](https://openreview.net/forum?id=MVmT6uQ3cQ) |  | 0 |  | Samir Khaki, Konstantinos N. Plataniotis |  |
| 1415 |  |  [Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World](https://openreview.net/forum?id=hWS4MueyzC) |  | 0 |  | Rujie Wu, Xiaojian Ma, Zhenliang Zhang, Wei Wang, Qing Li, SongChun Zhu, Yizhou Wang |  |
| 1416 |  |  [Towards 3D Molecule-Text Interpretation in Language Models](https://openreview.net/forum?id=xI4yNlkaqh) |  | 0 |  | Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji Kawaguchi, TatSeng Chua, Qi Tian |  |
| 1417 |  |  [Effective pruning of web-scale datasets based on complexity of concept clusters](https://openreview.net/forum?id=CtOA9aN8fr) |  | 0 |  | Amro Abbas, Evgenia Rusak, Kushal Tirumala, Wieland Brendel, Kamalika Chaudhuri, Ari S. Morcos |  |
| 1418 |  |  [AttEXplore: Attribution for Explanation with model parameters eXploration](https://openreview.net/forum?id=FsVxd9CIlb) |  | 0 |  | Zhiyu Zhu, Huaming Chen, Jiayu Zhang, Xinyi Wang, Zhibo Jin, Jason Xue, Flora D. Salim |  |
| 1419 |  |  [Brusleattack: a Query-Efficient Score- based Black-Box Sparse Adversarial Attack](https://openreview.net/forum?id=PAfnMGXief) |  | 0 |  | Viet Quoc Vo, Ehsan Abbasnejad, Damith Ranasinghe |  |
| 1420 |  |  [Win-Win: Training High-Resolution Vision Transformers from Two Windows](https://openreview.net/forum?id=N23A4ybMJr) |  | 0 |  | Vincent Leroy, Jérôme Revaud, Thomas Lucas, Philippe Weinzaepfel |  |
| 1421 |  |  [COSA: Concatenated Sample Pretrained Vision-Language Foundation Model](https://openreview.net/forum?id=bDkisS75zy) |  | 0 |  | Sihan Chen, Xingjian He, Handong Li, Xiaojie Jin, Jiashi Feng, Jing Liu |  |
| 1422 |  |  [SOInter: A Novel Deep Energy-Based Interpretation Method for Explaining Structured Output Models](https://openreview.net/forum?id=Fn655mJ4bv) |  | 0 |  | Seyyede Fatemeh Seyyedsalehi, Mahdieh Soleymani Baghshah, Hamid R. Rabiee |  |
| 1423 |  |  [An Unforgeable Publicly Verifiable Watermark for Large Language Models](https://openreview.net/forum?id=gMLQwKDY3N) |  | 0 |  | Aiwei Liu, Leyi Pan, Xuming Hu, Shuang Li, Lijie Wen, Irwin King, Philip S. Yu |  |
| 1424 |  |  [Constructing Adversarial Examples for Vertical Federated Learning: Optimal Client Corruption through Multi-Armed Bandit](https://openreview.net/forum?id=m52uU0dVbH) |  | 0 |  | Duanyi Yao, Songze Li, Ye Xue, Jin Liu |  |
| 1425 |  |  [Generative Learning for Solving Non-Convex Problem with Multi-Valued Input-Solution Mapping](https://openreview.net/forum?id=3tM1l5tSbv) |  | 0 |  | Enming Liang, Minghua Chen |  |
| 1426 |  |  [Interpretable Diffusion via Information Decomposition](https://openreview.net/forum?id=X6tNkN6ate) |  | 0 |  | Xianghao Kong, Ollie Liu, Han Li, Dani Yogatama, Greg Ver Steeg |  |
| 1427 |  |  [Algorithms for Caching and MTS with reduced number of predictions](https://openreview.net/forum?id=QuIiLSktO4) |  | 0 |  | Karim Abdel Sadek, Marek Eliás |  |
| 1428 |  |  [Two-timescale Extragradient for Finding Local Minimax Points](https://openreview.net/forum?id=6CIGhcJYJH) |  | 0 |  | Jiseok Chae, Kyuwon Kim, Donghwan Kim |  |
| 1429 |  |  [Uncertainty-aware Constraint Inference in Inverse Constrained Reinforcement Learning](https://openreview.net/forum?id=ILYjDvUM6U) |  | 0 |  | Sheng Xu, Guiliang Liu |  |
| 1430 |  |  [AutoChunk: Automated Activation Chunk for Memory-Efficient Deep Learning Inference](https://openreview.net/forum?id=GQGNLEHmdl) |  | 0 |  | Xuanlei Zhao, Shenggan Cheng, Guangyang Lu, Haotian Zhou, Bin Jia, Yang You |  |
| 1431 |  |  [Scalable and Effective Implicit Graph Neural Networks on Large Graphs](https://openreview.net/forum?id=QcMdPYBwTu) |  | 0 |  | Juncheng Liu, Bryan Hooi, Kenji Kawaguchi, Yiwei Wang, Chaosheng Dong, Xiaokui Xiao |  |
| 1432 |  |  [Retrieval is Accurate Generation](https://openreview.net/forum?id=oXYZJXDdo7) |  | 0 |  | Bowen Cao, Deng Cai, Leyang Cui, Xuxin Cheng, Wei Bi, Yuexian Zou, Shuming Shi |  |
| 1433 |  |  [Towards Imitation Learning to Branch for MIP: A Hybrid Reinforcement Learning based Sample Augmentation Approach](https://openreview.net/forum?id=NdcQQ82mfy) |  | 0 |  | Changwen Zhang, Wenli Ouyang, Hao Yuan, Liming Gong, Yong Sun, Ziao Guo, Zhichen Dong, Junchi Yan |  |
| 1434 |  |  [Decoupling Weighing and Selecting for Integrating Multiple Graph Pre-training Tasks](https://openreview.net/forum?id=c85tdYOOju) |  | 0 |  | Tianyu Fan, Lirong Wu, Yufei Huang, Haitao Lin, Cheng Tan, Zhangyang Gao, Stan Z. Li |  |
| 1435 |  |  [FedLoGe: Joint Local and Generic Federated Learning under Long-tailed Data](https://openreview.net/forum?id=V3j5d0GQgH) |  | 0 |  | Zikai Xiao, Zihan Chen, Liyinglan Liu, Yang Feng, Joey Tianyi Zhou, Jian Wu, Wanlu Liu, Howard Hao Yang, Zuozhu Liu |  |
| 1436 |  |  [MAP IT to Visualize Representations](https://openreview.net/forum?id=OKf6JtXtoy) |  | 0 |  | Robert Jenssen |  |
| 1437 |  |  [Measuring Vision-Language STEM Skills of Neural Models](https://openreview.net/forum?id=spvaV5LELF) |  | 0 |  | Jianhao Shen, Ye Yuan, Srbuhi Mirzoyan, Ming Zhang, Chenguang Wang |  |
| 1438 |  |  [On Double Descent in Reinforcement Learning with LSTD and Random Features](https://openreview.net/forum?id=9RIbNmx984) |  | 0 |  | David Brellmann, Eloïse Berthier, David Filliat, Goran Frehse |  |
| 1439 |  |  [The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models](https://openreview.net/forum?id=KrtGfTGaGe) |  | 0 |  | Raphaël Avalos, Florent Delgrange, Ann Nowé, Guillermo A. Pérez, Diederik M. Roijers |  |
| 1440 |  |  [Modulated Phase Diffusor: Content-Oriented Feature Synthesis for Detecting Unknown Objects](https://openreview.net/forum?id=gHAr7ZA1OL) |  | 0 |  | Aming Wu, Cheng Deng |  |
| 1441 |  |  [Towards Assessing and Benchmarking Risk-Return Tradeoff of Off-Policy Evaluation](https://openreview.net/forum?id=ycF7mKfVGO) |  | 0 |  | Haruka Kiyohara, Ren Kishimoto, Kosuke Kawakami, Ken Kobayashi, Kazuhide Nakata, Yuta Saito |  |
| 1442 |  |  [Provable Memory Efficient Self-Play Algorithm for Model-free Reinforcement Learning](https://openreview.net/forum?id=vNiI3aGcE6) |  | 0 |  | Na Li, Yuchen Jiao, Hangguan Shan, Shefeng Yan |  |
| 1443 |  |  [Transformer Fusion with Optimal Transport](https://openreview.net/forum?id=LjeqMvQpen) |  | 0 |  | Moritz Imfeld, Jacopo Graldi, Marco Giordano, Thomas Hofmann, Sotiris Anagnostidis, Sidak Pal Singh |  |
| 1444 |  |  [Mixture of LoRA Experts](https://openreview.net/forum?id=uWvKBCYh4S) |  | 0 |  | Xun Wu, Shaohan Huang, Furu Wei |  |
| 1445 |  |  [On the Posterior Distribution in Denoising: Application to Uncertainty Quantification](https://openreview.net/forum?id=adSGeugiuj) |  | 0 |  | Hila Manor, Tomer Michaeli |  |
| 1446 |  |  [LogicMP: A Neuro-symbolic Approach for Encoding First-order Logic Constraints](https://openreview.net/forum?id=BLGQ3oqldb) |  | 0 |  | Weidi Xu, Jingwei Wang, Lele Xie, Jianshan He, Hongting Zhou, Taifeng Wang, Xiaopei Wan, Jingdong Chen, Chao Qu, Wei Chu |  |
| 1447 |  |  [Turning large language models into cognitive models](https://openreview.net/forum?id=eiC4BKypf1) |  | 0 |  | Marcel Binz, Eric Schulz |  |
| 1448 |  |  [Skip-Attention: Improving Vision Transformers by Paying Less Attention](https://openreview.net/forum?id=vI95kcLAoU) |  | 0 |  | Shashanka Venkataramanan, Amir Ghodrati, Yuki M. Asano, Fatih Porikli, Amirhossein Habibian |  |
| 1449 |  |  [Benchmarking and Improving Generator-Validator Consistency of Language Models](https://openreview.net/forum?id=phBS6YpTzC) |  | 0 |  | Xiang Lisa Li, Vaishnavi Shrivastava, Siyan Li, Tatsunori Hashimoto, Percy Liang |  |
| 1450 |  |  [Adversarial Training on Purification (AToP): Advancing Both Robustness and Generalization](https://openreview.net/forum?id=u7559ZMvwY) |  | 0 |  | Guang Lin, Chao Li, Jianhai Zhang, Toshihisa Tanaka, Qibin Zhao |  |
| 1451 |  |  [Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization](https://openreview.net/forum?id=tbFBh3LMKi) |  | 0 |  | Kun Lei, Zhengmao He, Chenhao Lu, Kaizhe Hu, Yang Gao, Huazhe Xu |  |
| 1452 |  |  [LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts](https://openreview.net/forum?id=mNYF0IHbRy) |  | 0 |  | Hanan Gani, Shariq Farooq Bhat, Muzammal Naseer, Salman Khan, Peter Wonka |  |
| 1453 |  |  [FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods](https://openreview.net/forum?id=TzAJbTClAz) |  | 0 |  | Xiaotian Han, Jianfeng Chi, Yu Chen, Qifan Wang, Han Zhao, Na Zou, Xia Hu |  |
| 1454 |  |  [Invariance-based Learning of Latent Dynamics](https://openreview.net/forum?id=EWTFMkTdkT) |  | 0 |  | Kai Lagemann, Christian Lagemann, Sach Mukherjee |  |
| 1455 |  |  [Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh Transformer](https://openreview.net/forum?id=90yw2uM6J5) |  | 0 |  | YounYeol Yu, Jeongwhan Choi, Woojin Cho, Kookjin Lee, Nayong Kim, Kiseok Chang, ChangSeung Woo, Ilho Kim, SeokWoo Lee, JoonYoung Yang, Sooyoung Yoon, Noseong Park |  |
| 1456 |  |  [Masked Structural Growth for 2x Faster Language Model Pre-training](https://openreview.net/forum?id=rL7xsg1aRn) |  | 0 |  | Yiqun Yao, Zheng Zhang, Jing Li, Yequan Wang |  |
| 1457 |  |  [MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning](https://openreview.net/forum?id=5KojubHBr8) |  | 0 |  | Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, Baobao Chang |  |
| 1458 |  |  [Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt Editing](https://openreview.net/forum?id=OXv0zQ1umU) |  | 0 |  | Xinyu Hu, Pengfei Tang, Simiao Zuo, Zihan Wang, Bowen Song, Qiang Lou, Jian Jiao, Denis Charles |  |
| 1459 |  |  [Label-Noise Robust Diffusion Models](https://openreview.net/forum?id=HXWTXXtHNl) |  | 0 |  | Byeonghu Na, Yeongmin Kim, HeeSun Bae, Jung Hyun Lee, Se Jung Kwon, Wanmo Kang, IlChul Moon |  |
| 1460 |  |  [EasyTPP: Towards Open Benchmarking Temporal Point Processes](https://openreview.net/forum?id=PJwAkg0z7h) |  | 0 |  | Siqiao Xue, Xiaoming Shi, Zhixuan Chu, Yan Wang, Hongyan Hao, Fan Zhou, Caigao Jiang, Chen Pan, James Y. Zhang, Qingsong Wen, Jun Zhou, Hongyuan Mei |  |
| 1461 |  |  [ConjNorm: Tractable Density Estimation for Out-of-Distribution Detection](https://openreview.net/forum?id=1pSL2cXWoz) |  | 0 |  | Bo Peng, Yadan Luo, Yonggang Zhang, Yixuan Li, Zhen Fang |  |
| 1462 |  |  [Adaptive Window Pruning for Efficient Local Motion Deblurring](https://openreview.net/forum?id=hI18CDyadM) |  | 0 |  | Haoying Li, Jixin Zhao, Shangchen Zhou, Huajun Feng, Chongyi Li, Chen Change Loy |  |
| 1463 |  |  [A Unified Sampling Framework for Solver Searching of Diffusion Probabilistic Models](https://openreview.net/forum?id=W2d3LZbhhI) |  | 0 |  | Enshu Liu, Xuefei Ning, Huazhong Yang, Yu Wang |  |
| 1464 |  |  [A representation-learning game for classes of prediction tasks](https://openreview.net/forum?id=Uw8xvFqVAE) |  | 0 |  | Neria Uzan, Nir Weinberger |  |
| 1465 |  |  [Neuron-Enhanced AutoEncoder Matrix Completion and Collaborative Filtering: Theory and Practice](https://openreview.net/forum?id=kPrxk6tUcg) |  | 0 |  | Jicong Fan, Rui Chen, Zhao Zhang, Chris Ding |  |
| 1466 |  |  [∞-Diff: Infinite Resolution Diffusion with Subsampled Mollified States](https://openreview.net/forum?id=OUeIBFhyem) |  | 0 |  | Sam BondTaylor, Chris G. Willcocks |  |
| 1467 |  |  [Stochastic Modified Equations and Dynamics of Dropout Algorithm](https://openreview.net/forum?id=Bpkhu2ExxU) |  | 0 |  | Zhongwang Zhang, Yuqing Li, Tao Luo, ZhiQin John Xu |  |
| 1468 |  |  [Foundation Model-oriented Robustness: Robust Image Model Evaluation with Pretrained Models](https://openreview.net/forum?id=jd5GokdySz) |  | 0 |  | Peiyan Zhang, Haoyang Liu, Chaozhuo Li, Xing Xie, Sunghun Kim, Haohan Wang |  |
| 1469 |  |  [Transferring Learning Trajectories of Neural Networks](https://openreview.net/forum?id=bWNJFD1l8M) |  | 0 |  | Daiki Chijiwa |  |
| 1470 |  |  [VQ-TR: Vector Quantized Attention for Time Series Forecasting](https://openreview.net/forum?id=IxpTsFS7mh) |  | 0 |  | Kashif Rasul, Andrew Bennett, Pablo Vicente, Umang Gupta, Hena Ghonia, Anderson Schneider, Yuriy Nevmyvaka |  |
| 1471 |  |  [Decoupled Marked Temporal Point Process using Neural Ordinary Differential Equations](https://openreview.net/forum?id=BuFNoKBiMs) |  | 0 |  | Yujee Song, Donghyun Lee, Rui Meng, Won Hwa Kim |  |
| 1472 |  |  [Unsupervised Order Learning](https://openreview.net/forum?id=1CK45cqkEh) |  | 0 |  | SeonHo Lee, NyeongHo Shin, ChangSu Kim |  |
| 1473 |  |  [Fixed Non-negative Orthogonal Classifier: Inducing Zero-mean Neural Collapse with Feature Dimension Separation](https://openreview.net/forum?id=F4bmOrmUwc) |  | 0 |  | Hoyong Kim, Kangil Kim |  |
| 1474 |  |  [MaGIC: Multi-modality Guided Image Completion](https://openreview.net/forum?id=o7x0XVlCpX) |  | 0 |  | Hao Wang, Yongsheng Yu, Tiejian Luo, Heng Fan, Libo Zhang |  |
| 1475 |  |  [AmortizedPeriod: Attention-based Amortized Inference for Periodicity Identification](https://openreview.net/forum?id=psEswR8Jz4) |  | 0 |  | Hang Yu, Cong Liao, Ruolan Liu, Jianguo Li, Yun Hu, Xinzhe Wang |  |
| 1476 |  |  [How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations](https://openreview.net/forum?id=ikwEDva1JZ) |  | 0 |  | Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, Yu Bai |  |
| 1477 |  |  [EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision](https://openreview.net/forum?id=ycv2z8TYur) |  | 0 |  | Jiawei Yang, Boris Ivanovic, Or Litany, Xinshuo Weng, Seung Wook Kim, Boyi Li, Tong Che, Danfei Xu, Sanja Fidler, Marco Pavone, Yue Wang |  |
| 1478 |  |  [Combinatorial Bandits for Maximum Value Reward Function under Value-Index Feedback](https://openreview.net/forum?id=eMHn77ZKOp) |  | 0 |  | Yiliu Wang, Wei Chen, Milan Vojnovic |  |
| 1479 |  |  [Alice Benchmarks: Connecting Real World Re-Identification with the Synthetic](https://openreview.net/forum?id=vkkHqoerLV) |  | 0 |  | Xiaoxiao Sun, Yue Yao, Shengjin Wang, Hongdong Li, Liang Zheng |  |
| 1480 |  |  [Large Language Models as Generalizable Policies for Embodied Tasks](https://openreview.net/forum?id=u6imHU4Ebu) |  | 0 |  | Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Rin Metcalf, Walter Talbott, Natalie Mackraz, R. Devon Hjelm, Alexander T. Toshev |  |
| 1481 |  |  [Video Language Planning](https://openreview.net/forum?id=9pKtcJcMP3) |  | 0 |  | Yilun Du, Sherry Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Brian Ichter, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua B. Tenenbaum, Leslie Pack Kaelbling, Andy Zeng, Jonathan Tompson |  |
| 1482 |  |  [LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment](https://openreview.net/forum?id=QmZKc7UZCy) |  | 0 |  | Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, Hongfa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, Caiwan Zhang, Zhifeng Li, Wei Liu, Li Yuan |  |
| 1483 |  |  [An Efficient Membership Inference Attack for the Diffusion Model by Proximal Initialization](https://openreview.net/forum?id=rpH9FcCEV6) |  | 0 |  | Fei Kong, Jinhao Duan, Ruipeng Ma, Heng Tao Shen, Xiaoshuang Shi, Xiaofeng Zhu, Kaidi Xu |  |
| 1484 |  |  [Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models](https://openreview.net/forum?id=Tlsdsb6l9n) |  | 0 |  | Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, Huajun Chen |  |
| 1485 |  |  [Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution](https://openreview.net/forum?id=BtT6o5tfHu) |  | 0 |  | Yiyang Ma, Huan Yang, Wenhan Yang, Jianlong Fu, Jiaying Liu |  |
| 1486 |  |  [Don't Play Favorites: Minority Guidance for Diffusion Models](https://openreview.net/forum?id=3NmO9lY4Jn) |  | 0 |  | Soobin Um, Suhyeon Lee, Jong Chul Ye |  |
| 1487 |  |  [Learning from Aggregate responses: Instance Level versus Bag Level Loss Functions](https://openreview.net/forum?id=E60SIDItyT) |  | 0 |  | Adel Javanmard, Lin Chen, Vahab Mirrokni, Ashwinkumar Badanidiyuru, Gang Fu |  |
| 1488 |  |  [HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments](https://openreview.net/forum?id=n6mLhaBahJ) |  | 0 |  | Qinhong Zhou, Sunli Chen, Yisong Wang, Haozhe Xu, Weihua Du, Hongxin Zhang, Yilun Du, Joshua B. Tenenbaum, Chuang Gan |  |
| 1489 |  |  [Temporal Generalization Estimation in Evolving Graphs](https://openreview.net/forum?id=HFtrXBfNru) |  | 0 |  | Bin Lu, Tingyan Ma, Xiaoying Gan, Xinbing Wang, Yunqiang Zhu, Chenghu Zhou, Shiyu Liang |  |
| 1490 |  |  [Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining](https://openreview.net/forum?id=yN4Wv17ss3) |  | 0 |  | Licong Lin, Yu Bai, Song Mei |  |
| 1491 |  |  [On the Hardness of Online Nonconvex Optimization with Single Oracle Feedback](https://openreview.net/forum?id=iZgECfyHXF) |  | 0 |  | Ziwei Guan, Yi Zhou, Yingbin Liang |  |
| 1492 |  |  [On the Vulnerability of Adversarially Trained Models Against Two-faced Attacks](https://openreview.net/forum?id=mXpNp8MMr5) |  | 0 |  | Shengjie Zhou, Lue Tao, Yuzhou Cao, Tao Xiang, Bo An, Lei Feng |  |
| 1493 |  |  [MOFI: Learning Image Representations from Noisy Entity Annotated Images](https://openreview.net/forum?id=QQYpgReSRk) |  | 0 |  | Wentao Wu, Aleksei Timofeev, Chen Chen, Bowen Zhang, Kun Duan, Shuangning Liu, Yantao Zheng, Xianzhi Du, Yinfei Yang |  |
| 1494 |  |  [Efficient Integrators for Diffusion Generative Models](https://openreview.net/forum?id=qA4foxO5Gf) |  | 0 |  | Kushagra Pandey, Maja Rudolph, Stephan Mandt |  |
| 1495 |  |  [Learning Over Molecular Conformer Ensembles: Datasets and Benchmarks](https://openreview.net/forum?id=NSDszJ2uIV) |  | 0 |  | Yanqiao Zhu, Jeehyun Hwang, Keir Adams, Zhen Liu, Bozhao Nan, Brock Stenfors, Yuanqi Du, Jatin Chauhan, Olaf Wiest, Olexandr Isayev, Connor W. Coley, Yizhou Sun, Wei Wang |  |
| 1496 |  |  [Provably Robust Conformal Prediction with Improved Efficiency](https://openreview.net/forum?id=BWAhEjXjeG) |  | 0 |  | Ge Yan, Yaniv Romano, TsuiWei Weng |  |
| 1497 |  |  [DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text](https://openreview.net/forum?id=Xlayxj2fWp) |  | 0 |  | Xianjun Yang, Wei Cheng, Yue Wu, Linda Ruth Petzold, William Yang Wang, Haifeng Chen |  |
| 1498 |  |  [FedImpro: Measuring and Improving Client Update in Federated Learning](https://openreview.net/forum?id=giU9fYGTND) |  | 0 |  | Zhenheng Tang, Yonggang Zhang, Shaohuai Shi, Xinmei Tian, Tongliang Liu, Bo Han, Xiaowen Chu |  |
| 1499 |  |  [Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis](https://openreview.net/forum?id=p4eG8rCa0b) |  | 0 |  | Jonghyun Lee, Hansam Cho, Young Joon Yoo, Seoung Bum Kim, Yonghyun Jeong |  |
| 1500 |  |  [A Unified Framework for Bayesian Optimization under Contextual Uncertainty](https://openreview.net/forum?id=oMNkj4ER7V) |  | 0 |  | Sebastian Shenghong Tay, ChuanSheng Foo, Daisuke Urano, Richalynn Leong, Bryan Kian Hsiang Low |  |
| 1501 |  |  [Free from Bellman Completeness: Trajectory Stitching via Model-based Return-conditioned Supervised Learning](https://openreview.net/forum?id=7zY781bMDO) |  | 0 |  | Zhaoyi Zhou, Chuning Zhu, Runlong Zhou, Qiwen Cui, Abhishek Gupta, Simon Shaolei Du |  |
| 1502 |  |  [Diffusion Models for Multi-Task Generative Modeling](https://openreview.net/forum?id=cbv0sBIZh9) |  | 0 |  | Changyou Chen, Han Ding, Bunyamin Sisman, Yi Xu, Ouye Xie, Benjamin Z. Yao, Son Dinh Tran, Belinda Zeng |  |
| 1503 |  |  [WebArena: A Realistic Web Environment for Building Autonomous Agents](https://openreview.net/forum?id=oKn9c6ytLx) |  | 0 |  | Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig |  |
| 1504 |  |  [Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers](https://openreview.net/forum?id=ZG3RaNIsO8) |  | 0 |  | Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, Yujiu Yang |  |
| 1505 |  |  [Sparse Weight Averaging with Multiple Particles for Iterative Magnitude Pruning](https://openreview.net/forum?id=Y9t7MqZtCR) |  | 0 |  | Moonseok Choi, Hyungi Lee, Giung Nam, Juho Lee |  |
| 1506 |  |  [WildFusion: Learning 3D-Aware Latent Diffusion Models in View Space](https://openreview.net/forum?id=VdwVOREDZM) |  | 0 |  | Katja Schwarz, Seung Wook Kim, Jun Gao, Sanja Fidler, Andreas Geiger, Karsten Kreis |  |
| 1507 |  |  [Causal Inference with Conditional Front-Door Adjustment and Identifiable Variational Autoencoder](https://openreview.net/forum?id=wFf9m4v7oC) |  | 0 |  | Ziqi Xu, Debo Cheng, Jiuyong Li, Jixue Liu, Lin Liu, Kui Yu |  |
| 1508 |  |  [Diffusion Sampling with Momentum for Mitigating Divergence Artifacts](https://openreview.net/forum?id=HXc5aXeoc8) |  | 0 |  | Suttisak Wizadwongsa, Worameth Chinchuthakun, Pramook Khungurn, Amit Raj, Supasorn Suwajanakorn |  |
| 1509 |  |  [Active Test-Time Adaptation: Theoretical Analyses and An Algorithm](https://openreview.net/forum?id=YHUGlwTzFB) |  | 0 |  | Shurui Gui, Xiner Li, Shuiwang Ji |  |
| 1510 |  |  [AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable Diffusion Model](https://openreview.net/forum?id=bxfKIYfHyx) |  | 0 |  | Zibin Dong, Yifu Yuan, Jianye Hao, Fei Ni, Yao Mu, Yan Zheng, Yujing Hu, Tangjie Lv, Changjie Fan, Zhipeng Hu |  |
| 1511 |  |  [Doubly Robust Proximal Causal Learning for Continuous Treatments](https://openreview.net/forum?id=TjGJFkU3xL) |  | 0 |  | Yong Wu, Yanwei Fu, Shouyan Wang, Xinwei Sun |  |
| 1512 |  |  [One-hot Generalized Linear Model for Switching Brain State Discovery](https://openreview.net/forum?id=MREQ0k6qvD) |  | 0 |  | Chengrui Li, Soon Ho Kim, Chris Rodgers, Hannah Choi, Anqi Wu |  |
| 1513 |  |  [Neural Auto-designer for Enhanced Quantum Kernels](https://openreview.net/forum?id=8htNAnMSyP) |  | 0 |  | Cong Lei, Yuxuan Du, Peng Mi, Jun Yu, Tongliang Liu |  |
| 1514 |  |  [On the Parameterization of Second-Order Optimization Effective towards the Infinite Width](https://openreview.net/forum?id=g8sGBSQjYk) |  | 0 |  | Satoki Ishikawa, Ryo Karakida |  |
| 1515 |  |  [The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing](https://openreview.net/forum?id=DesYwmUG00) |  | 0 |  | Shen Nie, Hanzhong Allan Guo, Cheng Lu, Yuhao Zhou, Chenyu Zheng, Chongxuan Li |  |
| 1516 |  |  [CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules](https://openreview.net/forum?id=vYhglxSj8j) |  | 0 |  | Hung Le, Hailin Chen, Amrita Saha, Akash Gokul, Doyen Sahoo, Shafiq Joty |  |
| 1517 |  |  [Towards Robust Multi-Modal Reasoning via Model Selection](https://openreview.net/forum?id=KTf4DGAzus) |  | 0 |  | Xiangyan Liu, Rongxue Li, Wei Ji, Tao Lin |  |
| 1518 |  |  [DistillSpec: Improving Speculative Decoding via Knowledge Distillation](https://openreview.net/forum?id=rsY6J3ZaTF) |  | 0 |  | Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, JeanFrançois Kagy, Rishabh Agarwal |  |
| 1519 |  |  [Boosting the Adversarial Robustness of Graph Neural Networks: An OOD Perspective](https://openreview.net/forum?id=DCDT918ZkI) |  | 0 |  | Kuan Li, Yiwen Chen, Yang Liu, Jin Wang, Qing He, Minhao Cheng, Xiang Ao |  |
| 1520 |  |  [DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning](https://openreview.net/forum?id=qAoxvePSlq) |  | 0 |  | Jing Xiong, Zixuan Li, Chuanyang Zheng, Zhijiang Guo, Yichun Yin, Enze Xie, Zhicheng Yang, Qingxing Cao, Haiming Wang, Xiongwei Han, Jing Tang, Chengming Li, Xiaodan Liang |  |
| 1521 |  |  [LRR: Language-Driven Resamplable Continuous Representation against Adversarial Tracking Attacks](https://openreview.net/forum?id=3qo1pJHabg) |  | 0 |  | Jianlang Chen, Xuhong Ren, Qing Guo, Felix JuefeiXu, Di Lin, Wei Feng, Lei Ma, Jianjun Zhao |  |
| 1522 |  |  [SPTNet: An Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning](https://openreview.net/forum?id=3QLkwU40EE) |  | 0 |  | Hongjun Wang, Sagar Vaze, Kai Han |  |
| 1523 |  |  [Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning](https://openreview.net/forum?id=09iOdaeOzp) |  | 0 |  | Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, Danqi Chen |  |
| 1524 |  |  [Rethinking Complex Queries on Knowledge Graphs with Neural Link Predictors](https://openreview.net/forum?id=1BmveEMNbG) |  | 0 |  | Hang Yin, Zihao Wang, Yangqiu Song |  |
| 1525 |  |  [Shadow Cones: A Generalized Framework for Partial Order Embeddings](https://openreview.net/forum?id=zbKcFZ6Dbp) |  | 0 |  | Tao Yu, Toni J. B. Liu, Albert Tseng, Christopher De Sa |  |
| 1526 |  |  [Neural Active Learning Beyond Bandits](https://openreview.net/forum?id=g1S72T3FGc) |  | 0 |  | Yikun Ban, Ishika Agarwal, Ziwei Wu, Yada Zhu, Kommy Weldemariam, Hanghang Tong, Jingrui He |  |
| 1527 |  |  [From Graphs to Hypergraphs: Hypergraph Projection and its Reconstruction](https://openreview.net/forum?id=qwYKE3VB2h) |  | 0 |  | Yanbang Wang, Jon M. Kleinberg |  |
| 1528 |  |  [Synergistic Patch Pruning for Vision Transformer: Unifying Intra- & Inter-Layer Patch Importance](https://openreview.net/forum?id=COO51g41Q4) |  | 0 |  | Yuyao Zhang, Lan Wei, Nikolaos M. Freris |  |
| 1529 |  |  [On the Effect of Batch Size in Byzantine-Robust Distributed Learning](https://openreview.net/forum?id=wriKDQqiOQ) |  | 0 |  | YiRui Yang, ChangWei Shi, WuJun Li |  |
| 1530 |  |  [Large Language Model Cascades with Mixture of Thought Representations for Cost-Efficient Reasoning](https://openreview.net/forum?id=6okaSfANzh) |  | 0 |  | Murong Yue, Jie Zhao, Min Zhang, Liang Du, Ziyu Yao |  |
| 1531 |  |  [Unpaired Image-to-Image Translation via Neural Schrödinger Bridge](https://openreview.net/forum?id=uQBW7ELXfO) |  | 0 |  | Beomsu Kim, Gihyun Kwon, Kwanyoung Kim, Jong Chul Ye |  |
| 1532 |  |  [Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages](https://openreview.net/forum?id=0aR1s9YxoL) |  | 0 |  | Guozheng Ma, Lu Li, Sen Zhang, Zixuan Liu, Zhen Wang, Yixin Chen, Li Shen, Xueqian Wang, Dacheng Tao |  |
| 1533 |  |  [A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models](https://openreview.net/forum?id=farT6XXntP) |  | 0 |  | Haoran Xu, Young Jin Kim, Amr Sharaf, Hany Hassan Awadalla |  |
| 1534 |  |  [Ins-DetCLIP: Aligning Detection Model to Follow Human-Language Instruction](https://openreview.net/forum?id=M0MF4t3hE9) |  | 0 |  | Renjie Pi, Lewei Yao, Jianhua Han, Xiaodan Liang, Wei Zhang, Hang Xu |  |
| 1535 |  |  [Scaling Supervised Local Learning with Augmented Auxiliary Networks](https://openreview.net/forum?id=Qbf1hy8b7m) |  | 0 |  | Chenxiang Ma, Jibin Wu, Chenyang Si, Kay Chen Tan |  |
| 1536 |  |  [Elucidating the design space of classifier-guided diffusion generation](https://openreview.net/forum?id=9DXXMXnIGm) |  | 0 |  | Jiajun Ma, Tianyang Hu, Wenjia Wang, Jiacheng Sun |  |
| 1537 |  |  [Teach LLMs to Phish: Stealing Private Information from Language Models](https://openreview.net/forum?id=qo21ZlfNu6) |  | 0 |  | Ashwinee Panda, Christopher A. ChoquetteChoo, Zhengming Zhang, Yaoqing Yang, Prateek Mittal |  |
| 1538 |  |  [Robot Fleet Learning via Policy Merging](https://openreview.net/forum?id=IL71c1z7et) |  | 0 |  | Lirui Wang, Kaiqing Zhang, Allan Zhou, Max Simchowitz, Russ Tedrake |  |
| 1539 |  |  [InfoCon: Concept Discovery with Generative and Discriminative Informativeness](https://openreview.net/forum?id=g6eCbercEc) |  | 0 |  | Ruizhe Liu, Qian Luo, Yanchao Yang |  |
| 1540 |  |  [Sample-efficient Learning of Infinite-horizon Average-reward MDPs with General Function Approximation](https://openreview.net/forum?id=fq1wNrC2ai) |  | 0 |  | Jianliang He, Han Zhong, Zhuoran Yang |  |
| 1541 |  |  [Revisit and Outstrip Entity Alignment: A Perspective of Generative Models](https://openreview.net/forum?id=z3dfuRcGAK) |  | 0 |  | Lingbing Guo, Zhuo Chen, Jiaoyan Chen, Yin Fang, Wen Zhang, Huajun Chen |  |
| 1542 |  |  [ImplicitSLIM and How it Improves Embedding-based Collaborative Filtering](https://openreview.net/forum?id=6vF0ZJGor4) |  | 0 |  | Ilya Shenbin, Sergey I. Nikolenko |  |
| 1543 |  |  [Prompt Learning with Quaternion Networks](https://openreview.net/forum?id=dKlxDx2SoS) |  | 0 |  | Boya Shi, Zhengqin Xu, Shuai Jia, Chao Ma |  |
| 1544 |  |  [Meta-Learning Priors Using Unrolled Proximal Networks](https://openreview.net/forum?id=b3Cu426njo) |  | 0 |  | Yilang Zhang, Georgios B. Giannakis |  |
| 1545 |  |  [Bidirectional Temporal Diffusion Model for Temporally Consistent Human Animation](https://openreview.net/forum?id=yQDFsuG9HP) |  | 0 |  | Tserendorj Adiya, Jae Shin Yoon, Jungeun Lee, Sanghun Kim, Hwasup Lim |  |
| 1546 |  |  [Uncertainty Quantification via Stable Distribution Propagation](https://openreview.net/forum?id=cZttUMTiPL) |  | 0 |  | Felix Petersen, Aashwin Ananda Mishra, Hilde Kuehne, Christian Borgelt, Oliver Deussen, Mikhail Yurochkin |  |
| 1547 |  |  [MOFDiff: Coarse-grained Diffusion for Metal-Organic Framework Design](https://openreview.net/forum?id=0VBsoluxR2) |  | 0 |  | Xiang Fu, Tian Xie, Andrew S. Rosen, Tommi S. Jaakkola, Jake Smith |  |
| 1548 |  |  [FedWon: Triumphing Multi-domain Federated Learning Without Normalization](https://openreview.net/forum?id=hAYHmV1gM8) |  | 0 |  | Weiming Zhuang, Lingjuan Lyu |  |
| 1549 |  |  [TAIL: Task-specific Adapters for Imitation Learning with Large Pretrained Models](https://openreview.net/forum?id=RRayv1ZPN3) |  | 0 |  | Zuxin Liu, Jesse Zhang, Kavosh Asadi, Yao Liu, Ding Zhao, Shoham Sabach, Rasool Fakoor |  |
| 1550 |  |  [L2P-MIP: Learning to Presolve for Mixed Integer Programming](https://openreview.net/forum?id=McfYbKnpT8) |  | 0 |  | Chang Liu, Zhichen Dong, Haobo Ma, Weilin Luo, Xijun Li, Bowen Pang, Jia Zeng, Junchi Yan |  |
| 1551 |  |  [Exploring the cloud of feature interaction scores in a Rashomon set](https://openreview.net/forum?id=EPNEazJoAg) |  | 0 |  | Sichao Li, Rong Wang, Quanling Deng, Amanda S. Barnard |  |
| 1552 |  |  [Long-Short-Range Message-Passing: A Physics-Informed Framework to Capture Non-Local Interaction for Scalable Molecular Dynamics Simulation](https://openreview.net/forum?id=rvDQtdMnOl) |  | 0 |  | Yunyang Li, Yusong Wang, Lin Huang, Han Yang, Xinran Wei, Jia Zhang, Tong Wang, Zun Wang, Bin Shao, TieYan Liu |  |
| 1553 |  |  [A Study of Bayesian Neural Network Surrogates for Bayesian Optimization](https://openreview.net/forum?id=SA19ijj44B) |  | 0 |  | Yucen Lily Li, Tim G. J. Rudner, Andrew Gordon Wilson |  |
| 1554 |  |  [Large Language Models as Analogical Reasoners](https://openreview.net/forum?id=AgDICX1h50) |  | 0 |  | Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed H. Chi, Denny Zhou |  |
| 1555 |  |  [Multi-modal Gaussian Process Variational Autoencoders for Neural and Behavioral Data](https://openreview.net/forum?id=aGH43rjoe4) |  | 0 |  | Rabia Gondur, Usama Bin Sikandar, Evan Schaffer, Mikio Christian Aoi, Stephen L. Keeley |  |
| 1556 |  |  [UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling](https://openreview.net/forum?id=f5H8WGLQm5) |  | 0 |  | Haoyu Lu, Yuqi Huo, Guoxing Yang, Zhiwu Lu, Wei Zhan, Masayoshi Tomizuka, Mingyu Ding |  |
| 1557 |  |  [Annealing Self-Distillation Rectification Improves Adversarial Training](https://openreview.net/forum?id=eT6oLkm1cm) |  | 0 |  | YuYu Wu, HungJui Wang, ShangTse Chen |  |
| 1558 |  |  [Don't Judge by the Look: Towards Motion Coherent Video Representation](https://openreview.net/forum?id=RIcYTbpO38) |  | 0 |  | Yitian Zhang, Yue Bai, Huan Wang, Yizhou Wang, Yun Fu |  |
| 1559 |  |  [Time-LLM: Time Series Forecasting by Reprogramming Large Language Models](https://openreview.net/forum?id=Unb5CVPtae) |  | 0 |  | Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, PinYu Chen, Yuxuan Liang, YuanFang Li, Shirui Pan, Qingsong Wen |  |
| 1560 |  |  [BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction](https://openreview.net/forum?id=DmD1wboID9) |  | 0 |  | Jiangmeng Li, Fei Song, Yifan Jin, Wenwen Qiang, Changwen Zheng, Fuchun Sun, Hui Xiong |  |
| 1561 |  |  [Offline Data Enhanced On-Policy Policy Gradient with Provable Guarantees](https://openreview.net/forum?id=RMgqvQGTwH) |  | 0 |  | Yifei Zhou, Ayush Sekhari, Yuda Song, Wen Sun |  |
| 1562 |  |  [Embarrassingly Simple Dataset Distillation](https://openreview.net/forum?id=PLoWVP7Mjc) |  | 0 |  | Yunzhen Feng, Shanmukha Ramakrishna Vedantam, Julia Kempe |  |
| 1563 |  |  [RDesign: Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design](https://openreview.net/forum?id=RemfXx7ebP) |  | 0 |  | Cheng Tan, Yijie Zhang, Zhangyang Gao, Bozhen Hu, Siyuan Li, Zicheng Liu, Stan Z. Li |  |
| 1564 |  |  [Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation](https://openreview.net/forum?id=sGVmr7KHfn) |  | 0 |  | Yuxiang Lai, Yi Zhou, Xinghong Liu, Tao Zhou |  |
| 1565 |  |  [Large Language Models as Tool Makers](https://openreview.net/forum?id=qV83K9d5WB) |  | 0 |  | Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, Denny Zhou |  |
| 1566 |  |  [CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing](https://openreview.net/forum?id=Sx038qxjek) |  | 0 |  | Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen |  |
| 1567 |  |  [Geometrically Aligned Transfer Encoder for Inductive Transfer in Regression Tasks](https://openreview.net/forum?id=3z60EWfh1p) |  | 0 |  | Sung Moon Ko, Sumin Lee, DaeWoong Jeong, Woohyung Lim, Sehui Han |  |
| 1568 |  |  [Zero and Few-shot Semantic Parsing with Ambiguous Inputs](https://openreview.net/forum?id=qL9gogRepu) |  | 0 |  | Elias StengelEskin, Kyle Rawlins, Benjamin Van Durme |  |
| 1569 |  |  [Graph Generation with K2-trees](https://openreview.net/forum?id=RIEW6M9YoV) |  | 0 |  | Yunhui Jang, Dongwoo Kim, Sungsoo Ahn |  |
| 1570 |  |  [Manifold Preserving Guided Diffusion](https://openreview.net/forum?id=o3BxOLoxm1) |  | 0 |  | Yutong He, Naoki Murata, ChiehHsin Lai, Yuhta Takida, Toshimitsu Uesaka, Dongjun Kim, WeiHsiang Liao, Yuki Mitsufuji, J. Zico Kolter, Ruslan Salakhutdinov, Stefano Ermon |  |
| 1571 |  |  [Neural Common Neighbor with Completion for Link Prediction](https://openreview.net/forum?id=sNFLN3itAd) |  | 0 |  | Xiyuan Wang, Haotong Yang, Muhan Zhang |  |
| 1572 |  |  [Latent Intuitive Physics: Learning to Transfer Hidden Physics from A 3D Video](https://openreview.net/forum?id=WZu4gUGN13) |  | 0 |  | Xiangming Zhu, Huayu Deng, Haochen Yuan, Yunbo Wang, Xiaokang Yang |  |
| 1573 |  |  [Privacy-Preserving In-Context Learning for Large Language Models](https://openreview.net/forum?id=x4OPJ7lHVU) |  | 0 |  | Tong Wu, Ashwinee Panda, Jiachen T. Wang, Prateek Mittal |  |
| 1574 |  |  [Masked Distillation Advances Self-Supervised Transformer Architecture Search](https://openreview.net/forum?id=LUpC8KTvdV) |  | 0 |  | Caixia Yan, Xiaojun Chang, Zhihui Li, Lina Yao, Minnan Luo, Qinghua Zheng |  |
| 1575 |  |  [Adaptive Self-training Framework for Fine-grained Scene Graph Generation](https://openreview.net/forum?id=WipsLtH77t) |  | 0 |  | Kibum Kim, Kanghoon Yoon, Yeonjun In, Jinyoung Moon, Donghyun Kim, Chanyoung Park |  |
| 1576 |  |  [Exploring Effective Stimulus Encoding via Vision System Modeling for Visual Prostheses](https://openreview.net/forum?id=cKAUvMePUN) |  | 0 |  | Chuanqing Wang, Di Wu, Chaoming Fang, Jie Yang, Mohamad Sawan |  |
| 1577 |  |  [Towards Foundation Models for Knowledge Graph Reasoning](https://openreview.net/forum?id=jVEoydFOl9) |  | 0 |  | Mikhail Galkin, Xinyu Yuan, Hesham Mostafa, Jian Tang, Zhaocheng Zhu |  |
| 1578 |  |  [COCO-Periph: Bridging the Gap Between Human and Machine Perception in the Periphery](https://openreview.net/forum?id=MiRPBbQNHv) |  | 0 |  | Anne Harrington, Vasha DuTell, Mark Hamilton, Ayush Tewari, Simon Stent, William T. Freeman, Ruth Rosenholtz |  |
| 1579 |  |  [Towards Non-Asymptotic Convergence for Diffusion-Based Generative Models](https://openreview.net/forum?id=4VGEeER6W9) |  | 0 |  | Gen Li, Yuting Wei, Yuxin Chen, Yuejie Chi |  |
| 1580 |  |  [More is Better: when Infinite Overparameterization is Optimal and Overfitting is Obligatory](https://openreview.net/forum?id=OdpIjS0vkO) |  | 0 |  | James B. Simon, Dhruva Karkada, Nikhil Ghosh, Mikhail Belkin |  |
| 1581 |  |  [SALMON: Self-Alignment with Instructable Reward Models](https://openreview.net/forum?id=xJbsmB8UMx) |  | 0 |  | Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Daniel Cox, Yiming Yang, Chuang Gan |  |
| 1582 |  |  [ControlVideo: Training-free Controllable Text-to-video Generation](https://openreview.net/forum?id=5a79AqFr0c) |  | 0 |  | Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, Qi Tian |  |
| 1583 |  |  [RETSim: Resilient and Efficient Text Similarity](https://openreview.net/forum?id=23b9KSNQTX) |  | 0 |  | Marina Zhang, Owen S. Vallis, Aysegul Bumin, Tanay Vakharia, Elie Bursztein |  |
| 1584 |  |  [KW-Design: Pushing the Limit of Protein Design via Knowledge Refinement](https://openreview.net/forum?id=mpqMVWgqjn) |  | 0 |  | Zhangyang Gao, Cheng Tan, Xingran Chen, Yijie Zhang, Jun Xia, Siyuan Li, Stan Z. Li |  |
| 1585 |  |  [Fusing Models with Complementary Expertise](https://openreview.net/forum?id=PhMrGCMIRL) |  | 0 |  | Hongyi Wang, Felipe Maia Polo, Yuekai Sun, Souvik Kundu, Eric P. Xing, Mikhail Yurochkin |  |
| 1586 |  |  [Magnitude Invariant Parametrizations Improve Hypernetwork Learning](https://openreview.net/forum?id=fJNnerz6iH) |  | 0 |  | Jose Javier Gonzalez Ortiz, John V. Guttag, Adrian V. Dalca |  |
| 1587 |  |  [A Versatile Causal Discovery Framework to Allow Causally-Related Hidden Variables](https://openreview.net/forum?id=FhQSGhBlqv) |  | 0 |  | Xinshuai Dong, Biwei Huang, Ignavier Ng, Xiangchen Song, Yujia Zheng, Songyao Jin, Roberto Legaspi, Peter Spirtes, Kun Zhang |  |
| 1588 |  |  [Teaching Large Language Models to Self-Debug](https://openreview.net/forum?id=KuPixIqPiq) |  | 0 |  | Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou |  |
| 1589 |  |  [Achieving Human Parity in Content-Grounded Datasets Generation](https://openreview.net/forum?id=RjYKTQ0L0W) |  | 0 |  | Asaf Yehudai, Boaz Carmeli, Yosi Mass, Ofir Arviv, Nathaniel Mills, Eyal Shnarch, Leshem Choshen |  |
| 1590 |  |  [Federated Recommendation with Additive Personalization](https://openreview.net/forum?id=xkXdE81mOK) |  | 0 |  | Zhiwei Li, Guodong Long, Tianyi Zhou |  |
| 1591 |  |  [Identifiable Latent Polynomial Causal Models through the Lens of Change](https://openreview.net/forum?id=ia9fKO1Vjq) |  | 0 |  | Yuhang Liu, Zhen Zhang, Dong Gong, Mingming Gong, Biwei Huang, Anton van den Hengel, Kun Zhang, Javen Qinfeng Shi |  |
| 1592 |  |  [Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models](https://openreview.net/forum?id=kIP0duasBb) |  | 0 |  | Shuai Zhao, Xiaohan Wang, Linchao Zhu, Yi Yang |  |
| 1593 |  |  [Future Language Modeling from Temporal Document History](https://openreview.net/forum?id=bRLed9prWC) |  | 0 |  | Changmao Li, Jeffrey Flanigan |  |
| 1594 |  |  [SemiReward: A General Reward Model for Semi-supervised Learning](https://openreview.net/forum?id=dnqPvUjyRI) |  | 0 |  | Siyuan Li, Weiyang Jin, Zedong Wang, Fang Wu, Zicheng Liu, Cheng Tan, Stan Z. Li |  |
| 1595 |  |  [DATS: Difficulty-Aware Task Sampler for Meta-Learning Physics-Informed Neural Networks](https://openreview.net/forum?id=EvyYFSxdgB) |  | 0 |  | Maryam Toloubidokhti, Yubo Ye, Ryan Missel, Xiajun Jiang, Nilesh Kumar, Ruby Shrestha, Linwei Wang |  |
| 1596 |  |  [Modulate Your Spectrum in Self-Supervised Learning](https://openreview.net/forum?id=TKqMmKlmA7) |  | 0 |  | Xi Weng, Yunhao Ni, Tengwei Song, Jie Luo, Rao Muhammad Anwer, Salman Khan, Fahad Khan, Lei Huang |  |
| 1597 |  |  [Improving Intrinsic Exploration by Creating Stationary Objectives](https://openreview.net/forum?id=YbZxT0SON4) |  | 0 |  | Roger Creus Castanyer, Joshua Romoff, Glen Berseth |  |
| 1598 |  |  [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://openreview.net/forum?id=mZn2Xyh9Ec) |  | 0 |  | Tri Dao |  |
| 1599 |  |  [LDReg: Local Dimensionality Regularized Self-Supervised Learning](https://openreview.net/forum?id=oZyAqjAjJW) |  | 0 |  | Hanxun Huang, Ricardo J. G. B. Campello, Sarah Monazam Erfani, Xingjun Ma, Michael E. Houle, James Bailey |  |
| 1600 |  |  [Empirical Likelihood for Fair Classification](https://openreview.net/forum?id=GACjMj1MS1) |  | 0 |  | Pangpang Liu, Yichuan Zhao |  |
| 1601 |  |  [Smooth ECE: Principled Reliability Diagrams via Kernel Smoothing](https://openreview.net/forum?id=XwiA1nDahv) |  | 0 |  | Jaroslaw Blasiok, Preetum Nakkiran |  |
| 1602 |  |  [Probabilistic Adaptation of Black-Box Text-to-Video Models](https://openreview.net/forum?id=pjtIEgscE3) |  | 0 |  | Sherry Yang, Yilun Du, Bo Dai, Dale Schuurmans, Joshua B. Tenenbaum, Pieter Abbeel |  |
| 1603 |  |  [Horizon-Free Regret for Linear Markov Decision Processes](https://openreview.net/forum?id=SdBApv9iT4) |  | 0 |  | Zihan Zhang, Jason D. Lee, Yuxin Chen, Simon Shaolei Du |  |
| 1604 |  |  [Faithful Rule Extraction for Differentiable Rule Learning Models](https://openreview.net/forum?id=kBTzlxM2J1) |  | 0 |  | Xiaxia Wang, David Jaime Tena Cucala, Bernardo Cuenca Grau, Ian Horrocks |  |
| 1605 |  |  [Score-based generative models break the curse of dimensionality in learning a family of sub-Gaussian distributions](https://openreview.net/forum?id=wG12xUSqrI) |  | 0 |  | Frank Cole, Yulong Lu |  |
| 1606 |  |  [Scalable Diffusion for Materials Generation](https://openreview.net/forum?id=wm4WlHoXpC) |  | 0 |  | Sherry Yang, KwangHwan Cho, Amil Merchant, Pieter Abbeel, Dale Schuurmans, Igor Mordatch, Ekin Dogus Cubuk |  |
| 1607 |  |  [Towards Optimal Feature-Shaping Methods for Out-of-Distribution Detection](https://openreview.net/forum?id=dm8e7gsH0d) |  | 0 |  | Qinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng, Stephen Gould |  |
| 1608 |  |  [Guess & Sketch: Language Model Guided Transpilation](https://openreview.net/forum?id=qPFsIbF3V6) |  | 0 |  | Celine Lee, Abdulrahman Mahmoud, Michal Kurek, Simone Campanoni, David Brooks, Stephen Chong, GuYeon Wei, Alexander M. Rush |  |
| 1609 |  |  [Motif: Intrinsic Motivation from Artificial Intelligence Feedback](https://openreview.net/forum?id=tmBKIecDE9) |  | 0 |  | Martin Klissarov, Pierluca D'Oro, Shagun Sodhani, Roberta Raileanu, PierreLuc Bacon, Pascal Vincent, Amy Zhang, Mikael Henaff |  |
| 1610 |  |  [On Differentially Private Federated Linear Contextual Bandits](https://openreview.net/forum?id=cuAxSHcsSX) |  | 0 |  | Xingyu Zhou, Sayak Ray Chowdhury |  |
| 1611 |  |  [Generative Human Motion Stylization in Latent Space](https://openreview.net/forum?id=daEqXJ0yZo) |  | 0 |  | Chuan Guo, Yuxuan Mu, Xinxin Zuo, Peng Dai, Youliang Yan, Juwei Lu, Li Cheng |  |
| 1612 |  |  [Neural Neighborhood Search for Multi-agent Path Finding](https://openreview.net/forum?id=2NpAw2QJBY) |  | 0 |  | Zhongxia Yan, Cathy Wu |  |
| 1613 |  |  [Teaching Language Models to Hallucinate Less with Synthetic Tasks](https://openreview.net/forum?id=xpw7V0P136) |  | 0 |  | Erik Jones, Hamid Palangi, Clarisse Simões, Varun Chandrasekaran, Subhabrata Mukherjee, Arindam Mitra, Ahmed Hassan Awadallah, Ece Kamar |  |
| 1614 |  |  [Enabling Lanuguage Models to Implicitly Learn Self-Improvement](https://openreview.net/forum?id=2tVHNRZuCs) |  | 0 |  | Ziqi Wang, Le Hou, Tianjian Lu, Yuexin Wu, Yunxuan Li, Hongkun Yu, Heng Ji |  |
| 1615 |  |  [ReLoRA: High-Rank Training Through Low-Rank Updates](https://openreview.net/forum?id=DLJznSp6X3) |  | 0 |  | Vladislav Lialin, Sherin Muckatira, Namrata Shivagunde, Anna Rumshisky |  |
| 1616 |  |  [Learning with Language-Guided State Abstractions](https://openreview.net/forum?id=qi5Xa2cOZg) |  | 0 |  | Andi Peng, Ilia Sucholutsky, Belinda Z. Li, Theodore R. Sumers, Thomas L. Griffiths, Jacob Andreas, Julie Shah |  |
| 1617 |  |  [Multimodal Molecular Pretraining via Modality Blending](https://openreview.net/forum?id=oM7Jbxdk6Z) |  | 0 |  | Qiying Yu, Yudi Zhang, Yuyan Ni, Shikun Feng, Yanyan Lan, Hao Zhou, Jingjing Liu |  |
| 1618 |  |  [Understanding when Dynamics-Invariant Data Augmentations Benefit Model-free Reinforcement Learning Updates](https://openreview.net/forum?id=sVEu295o70) |  | 0 |  | Nicholas Corrado, Josiah P. Hanna |  |
| 1619 |  |  [JoMA: Demystifying Multilayer Transformers via Joint Dynamics of MLP and Attention](https://openreview.net/forum?id=LbJqRGNYCf) |  | 0 |  | Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, Simon Shaolei Du |  |
| 1620 |  |  [Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks](https://openreview.net/forum?id=YZrg56G0JV) |  | 0 |  | Ziping Xu, Zifan Xu, Runxuan Jiang, Peter Stone, Ambuj Tewari |  |
| 1621 |  |  [Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications](https://openreview.net/forum?id=BrjLHbqiYs) |  | 0 |  | Paul Pu Liang, Chun Kai Ling, Yun Cheng, Alexander Obolenskiy, Yudong Liu, Rohan Pandey, Alex Wilf, LouisPhilippe Morency, Russ Salakhutdinov |  |
| 1622 |  |  [Adaptive Retrieval and Scalable Indexing for k-NN Search with Cross-Encoders](https://openreview.net/forum?id=1CPta0bfN2) |  | 0 |  | Nishant Yadav, Nicholas Monath, Manzil Zaheer, Rob Fergus, Andrew McCallum |  |
| 1623 |  |  [The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction](https://openreview.net/forum?id=ozX92bu8VA) |  | 0 |  | Pratyusha Sharma, Jordan T. Ash, Dipendra Misra |  |
| 1624 |  |  [DeepZero: Scaling Up Zeroth-Order Optimization for Deep Model Training](https://openreview.net/forum?id=qBWhjsNPEY) |  | 0 |  | Aochuan Chen, Yimeng Zhang, Jinghan Jia, James Diffenderfer, Konstantinos Parasyris, Jiancheng Liu, Yihua Zhang, Zheng Zhang, Bhavya Kailkhura, Sijia Liu |  |
| 1625 |  |  [Multi-Resolution Diffusion Models for Time Series Forecasting](https://openreview.net/forum?id=mmjnr0G8ZY) |  | 0 |  | Lifeng Shen, Weiyu Chen, James T. Kwok |  |
| 1626 |  |  [CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?](https://openreview.net/forum?id=FIGXAxr9E4) |  | 0 |  | Ibrahim Alabdulmohsin, Xiao Wang, Andreas Peter Steiner, Priya Goyal, Alexander D'Amour, Xiaohua Zhai |  |
| 1627 |  |  [On Error Propagation of Diffusion Models](https://openreview.net/forum?id=RtAct1E2zS) |  | 0 |  | Yangming Li, Mihaela van der Schaar |  |
| 1628 |  |  [The Update-Equivalence Framework for Decision-Time Planning](https://openreview.net/forum?id=JXGph215fL) |  | 0 |  | Samuel Sokota, Gabriele Farina, David J. Wu, Hengyuan Hu, Kevin A. Wang, J. Zico Kolter, Noam Brown |  |
| 1629 |  |  [Soft Mixture Denoising: Beyond the Expressive Bottleneck of Diffusion Models](https://openreview.net/forum?id=aaBnFAyW9O) |  | 0 |  | Yangming Li, Boris van Breugel, Mihaela van der Schaar |  |
| 1630 |  |  [LabelDP-Pro: Learning with Label Differential Privacy via Projections](https://openreview.net/forum?id=JnYaF3vv3G) |  | 0 |  | Badih Ghazi, Yangsibo Huang, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Chiyuan Zhang |  |
| 1631 |  |  [AUC-CL: A Batchsize-Robust Framework for Self-Supervised Contrastive Representation Learning](https://openreview.net/forum?id=YgMdDQB09U) |  | 0 |  | Rohan Sharma, Kaiyi Ji, Zhiqiang Xu, Changyou Chen |  |
| 1632 |  |  [Teaching Arithmetic to Small Transformers](https://openreview.net/forum?id=dsUB4bst9S) |  | 0 |  | Nayoung Lee, Kartik Sreenivasan, Jason D. Lee, Kangwook Lee, Dimitris Papailiopoulos |  |
| 1633 |  |  [ReMasker: Imputing Tabular Data with Masked Autoencoding](https://openreview.net/forum?id=KI9NqjLVDT) |  | 0 |  | Tianyu Du, Luca Melis, Ting Wang |  |
| 1634 |  |  [Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions](https://openreview.net/forum?id=0i6Z9N5MLY) |  | 0 |  | Xufeng Cai, Ahmet Alacaoglu, Jelena Diakonikolas |  |
| 1635 |  |  [A Dynamical View of the Question of Why](https://openreview.net/forum?id=lrQlLqQase) |  | 0 |  | Mehdi Fatemi, Sindhu C. M. Gowda |  |
| 1636 |  |  [DrS: Learning Reusable Dense Rewards for Multi-Stage Tasks](https://openreview.net/forum?id=6CZ50WgfCG) |  | 0 |  | Tongzhou Mu, Minghua Liu, Hao Su |  |
| 1637 |  |  [Fair Classifiers that Abstain without Harm](https://openreview.net/forum?id=jvveGAbkVx) |  | 0 |  | Tongxin Yin, JeanFrancois Ton, Ruocheng Guo, Yuanshun Yao, Mingyan Liu, Yang Liu |  |
| 1638 |  |  [Aligning Relational Learning with Lipschitz Fairness](https://openreview.net/forum?id=ODSgo2m8aE) |  | 0 |  | Yaning Jia, Chunhui Zhang, Soroush Vosoughi |  |
| 1639 |  |  [Listen, Think, and Understand](https://openreview.net/forum?id=nBZBPXdJlC) |  | 0 |  | Yuan Gong, Hongyin Luo, Alexander H. Liu, Leonid Karlinsky, James R. Glass |  |
| 1640 |  |  [GOAt: Explaining Graph Neural Networks via Graph Output Attribution](https://openreview.net/forum?id=2Q8TZWAHv4) |  | 0 |  | Shengyao Lu, Keith G. Mills, Jiao He, Bang Liu, Di Niu |  |
| 1641 |  |  [Time Fairness in Online Knapsack Problems](https://openreview.net/forum?id=9kG7TwgLYu) |  | 0 |  | Adam Lechowicz, Rik Sengupta, Bo Sun, Shahin Kamali, Mohammad Hajiesmaili |  |
| 1642 |  |  [A Lie Group Approach to Riemannian Batch Normalization](https://openreview.net/forum?id=okYdj8Ysru) |  | 0 |  | Ziheng Chen, Yue Song, Yunmei Liu, Nicu Sebe |  |
| 1643 |  |  [Federated Text-driven Prompt Generation for Vision-Language Models](https://openreview.net/forum?id=NW31gAylIm) |  | 0 |  | Chen Qiu, Xingyu Li, Chaithanya Kumar Mummadi, Madan Ravi Ganesh, Zhenzhen Li, Lu Peng, WanYi Lin |  |
| 1644 |  |  [On the Hardness of Constrained Cooperative Multi-Agent Reinforcement Learning](https://openreview.net/forum?id=wFWuX1Fhtj) |  | 0 |  | Ziyi Chen, Yi Zhou, Heng Huang |  |
| 1645 |  |  [Det-CGD: Compressed Gradient Descent with Matrix Stepsizes for Non-Convex Optimization](https://openreview.net/forum?id=ZEZ0CPmoSI) |  | 0 |  | Hanmin Li, Avetik G. Karagulyan, Peter Richtárik |  |
| 1646 |  |  [Causal-StoNet: Causal Inference for High-Dimensional Complex Data](https://openreview.net/forum?id=BtZ7vCt5QY) |  | 0 |  | Yaxin Fang, Faming Liang |  |
| 1647 |  |  [Conditional Information Bottleneck Approach for Time Series Imputation](https://openreview.net/forum?id=K1mcPiDdOJ) |  | 0 |  | MinGyu Choi, Changhee Lee |  |
| 1648 |  |  [Deep Neural Networks Tend To Extrapolate Predictably](https://openreview.net/forum?id=ljwoQ3cvQh) |  | 0 |  | Katie Kang, Amrith Setlur, Claire J. Tomlin, Sergey Levine |  |
| 1649 |  |  [RLCD: Reinforcement Learning from Contrastive Distillation for LM Alignment](https://openreview.net/forum?id=v3XXtxWKi6) |  | 0 |  | Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, Yuandong Tian |  |
| 1650 |  |  [Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles](https://openreview.net/forum?id=TVDUVpgu9s) |  | 0 |  | Zhiwei Tang, Dmitry Rybin, TsungHui Chang |  |
| 1651 |  |  [Entropy Coding of Unordered Data Structures](https://openreview.net/forum?id=afQuNt3Ruh) |  | 0 |  | Julius Kunze, Daniel Severo, Giulio Zani, JanWillem van de Meent, James Townsend |  |
| 1652 |  |  [Neurosymbolic Grounding for Compositional World Models](https://openreview.net/forum?id=4KZpDGD4Nh) |  | 0 |  | Atharva Sehgal, Arya Grayeli, Jennifer J. Sun, Swarat Chaudhuri |  |
| 1653 |  |  [OpenTab: Advancing Large Language Models as Open-domain Table Reasoners](https://openreview.net/forum?id=Qa0ULgosc9) |  | 0 |  | Kezhi Kong, Jiani Zhang, Zhengyuan Shen, Balasubramaniam Srinivasan, Chuan Lei, Christos Faloutsos, Huzefa Rangwala, George Karypis |  |
| 1654 |  |  [Adaptive Sharpness-Aware Pruning for Robust Sparse Networks](https://openreview.net/forum?id=QFYVVwiAM8) |  | 0 |  | Anna Bair, Hongxu Yin, Maying Shen, Pavlo Molchanov, José M. Álvarez |  |
| 1655 |  |  [MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods](https://openreview.net/forum?id=bkNx3O0sND) |  | 0 |  | Mara Finkelstein, Markus Freitag |  |
| 1656 |  |  [Federated Orthogonal Training: Mitigating Global Catastrophic Forgetting in Continual Federated Learning](https://openreview.net/forum?id=nAs4LdaP9Y) |  | 0 |  | Yavuz Faruk Bakman, Duygu Nur Yaldiz, Yahya H. Ezzeldin, Salman Avestimehr |  |
| 1657 |  |  [Self-Consuming Generative Models Go MAD](https://openreview.net/forum?id=ShjMHfmPs0) |  | 0 |  | Sina Alemohammad, Josue CascoRodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, Richard G. Baraniuk |  |
| 1658 |  |  [The Hidden Language of Diffusion Models](https://openreview.net/forum?id=awWpHnEJDw) |  | 0 |  | Hila Chefer, Oran Lang, Mor Geva, Volodymyr Polosukhin, Assaf Shocher, Michal Irani, Inbar Mosseri, Lior Wolf |  |
| 1659 |  |  [Reasoning with Latent Diffusion in Offline Reinforcement Learning](https://openreview.net/forum?id=tGQirjzddO) |  | 0 |  | Siddarth Venkatraman, Shivesh Khaitan, Ravi Tej Akella, John M. Dolan, Jeff Schneider, Glen Berseth |  |
| 1660 |  |  [Low Rank Matrix Completion via Robust Alternating Minimization in Nearly Linear Time](https://openreview.net/forum?id=N0gT4A0jNV) |  | 0 |  | Yuzhou Gu, Zhao Song, Junze Yin, Lichen Zhang |  |
| 1661 |  |  [DiffusionNAG: Predictor-guided Neural Architecture Generation with Diffusion Models](https://openreview.net/forum?id=dyG2oLJYyX) |  | 0 |  | Sohyun An, Hayeon Lee, Jaehyeong Jo, Seanie Lee, Sung Ju Hwang |  |
| 1662 |  |  [Federated Causal Discovery from Heterogeneous Data](https://openreview.net/forum?id=m7tJxajC3G) |  | 0 |  | Loka Li, Ignavier Ng, Gongxu Luo, Biwei Huang, Guangyi Chen, Tongliang Liu, Bin Gu, Kun Zhang |  |
| 1663 |  |  [CellPLM: Pre-training of Cell Language Model Beyond Single Cells](https://openreview.net/forum?id=BKXvPDekud) |  | 0 |  | Hongzhi Wen, Wenzhuo Tang, Xinnan Dai, Jiayuan Ding, Wei Jin, Yuying Xie, Jiliang Tang |  |
| 1664 |  |  [Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks](https://openreview.net/forum?id=0H6DFoZZXZ) |  | 0 |  | Edwin Zhang, Yujie Lu, Shinda Huang, William Yang Wang, Amy Zhang |  |
| 1665 |  |  [CALICO: Self-Supervised Camera-LiDAR Contrastive Pre-training for BEV Perception](https://openreview.net/forum?id=U7iiF79kI3) |  | 0 |  | Jiachen Sun, Haizhong Zheng, Qingzhao Zhang, Atul Prakash, Zhuoqing Mao, Chaowei Xiao |  |
| 1666 |  |  [Principled Architecture-aware Scaling of Hyperparameters](https://openreview.net/forum?id=HZndRcfyNI) |  | 0 |  | Wuyang Chen, Junru Wu, Zhangyang Wang, Boris Hanin |  |
| 1667 |  |  [Provable Robust Watermarking for AI-Generated Text](https://openreview.net/forum?id=SsmT8aO45L) |  | 0 |  | Xuandong Zhao, Prabhanjan Vijendra Ananth, Lei Li, YuXiang Wang |  |
| 1668 |  |  [Str2Str: A Score-based Framework for Zero-shot Protein Conformation Sampling](https://openreview.net/forum?id=C4BikKsgmK) |  | 0 |  | Jiarui Lu, Bozitao Zhong, Zuobai Zhang, Jian Tang |  |
| 1669 |  |  [CCIL: Continuity-Based Data Augmentation for Corrective Imitation Learning](https://openreview.net/forum?id=LQ6LQ8f4y8) |  | 0 |  | Liyiming Ke, Yunchu Zhang, Abhay Deshpande, Siddhartha S. Srinivasa, Abhishek Gupta |  |
| 1670 |  |  [Memory-Consistent Neural Networks for Imitation Learning](https://openreview.net/forum?id=R3Tf7LDdX4) |  | 0 |  | Kaustubh Sridhar, Souradeep Dutta, Dinesh Jayaraman, James Weimer, Insup Lee |  |
| 1671 |  |  [Learning dynamic representations of the functional connectome in neurobiological networks](https://openreview.net/forum?id=ZwhHSOHMTM) |  | 0 |  | Luciano Dyballa, Samuel Lang, Alexandra HaslundGourley, Eviatar Yemini, Steven W. Zucker |  |
| 1672 |  |  [Skill or Luck? Return Decomposition via Advantage Functions](https://openreview.net/forum?id=ZFMiHfZwIf) |  | 0 |  | HsiaoRu Pan, Bernhard Schölkopf |  |
| 1673 |  |  [Belief-Enriched Pessimistic Q-Learning against Adversarial State Perturbations](https://openreview.net/forum?id=7gDENzTzw1) |  | 0 |  | Xiaolin Sun, Zizhan Zheng |  |
| 1674 |  |  [SmartPlay : A Benchmark for LLMs as Intelligent Agents](https://openreview.net/forum?id=S2oTVrlcp3) |  | 0 |  | Yue Wu, Xuan Tang, Tom M. Mitchell, Yuanzhi Li |  |
| 1675 |  |  [Conditional Instrumental Variable Regression with Representation Learning for Causal Inference](https://openreview.net/forum?id=qDhq1icpO8) |  | 0 |  | Debo Cheng, Ziqi Xu, Jiuyong Li, Lin Liu, Jixue Liu, Thuc Duy Le |  |
| 1676 |  |  [Look, Remember and Reason: Grounded Reasoning in Videos with Language Models](https://openreview.net/forum?id=jhPvuc7kxB) |  | 0 |  | Apratim Bhattacharyya, Sunny Panchal, Reza Pourreza, Mingu Lee, Pulkit Madan, Roland Memisevic |  |
| 1677 |  |  [SOHES: Self-supervised Open-world Hierarchical Entity Segmentation](https://openreview.net/forum?id=PXNrncg2DF) |  | 0 |  | Shengcao Cao, Jiuxiang Gu, Jason Kuen, Hao Tan, Ruiyi Zhang, Handong Zhao, Ani Nenkova, Liangyan Gui, Tong Sun, YuXiong Wang |  |
| 1678 |  |  [EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations](https://openreview.net/forum?id=mCOBKZmrzD) |  | 0 |  | YiLun Liao, Brandon M. Wood, Abhishek Das, Tess E. Smidt |  |
| 1679 |  |  [Chameleon: Increasing Label-Only Membership Leakage with Adaptive Poisoning](https://openreview.net/forum?id=4DoSULcfG6) |  | 0 |  | Harsh Chaudhari, Giorgio Severi, Alina Oprea, Jonathan R. Ullman |  |
| 1680 |  |  [Robust NAS under adversarial training: benchmark, theory, and beyond](https://openreview.net/forum?id=cdUpf6t6LZ) |  | 0 |  | Yongtao Wu, Fanghui Liu, CarlJohann SimonGabriel, Grigorios Chrysos, Volkan Cevher |  |
| 1681 |  |  [Conformal Language Modeling](https://openreview.net/forum?id=pzUhfQ74c5) |  | 0 |  | Victor Quach, Adam Fisch, Tal Schuster, Adam Yala, Jae Ho Sohn, Tommi S. Jaakkola, Regina Barzilay |  |
| 1682 |  |  [Representation Deficiency in Masked Language Modeling](https://openreview.net/forum?id=b3l0piOrGU) |  | 0 |  | Yu Meng, Jitin Krishnan, Sinong Wang, Qifan Wang, Yuning Mao, Han Fang, Marjan Ghazvininejad, Jiawei Han, Luke Zettlemoyer |  |
| 1683 |  |  [Polynomial Width is Sufficient for Set Representation with High-dimensional Features](https://openreview.net/forum?id=34STseLBrQ) |  | 0 |  | Peihao Wang, Shenghao Yang, Shu Li, Zhangyang Wang, Pan Li |  |
| 1684 |  |  [Parametric Augmentation for Time Series Contrastive Learning](https://openreview.net/forum?id=EIPLdFy3vp) |  | 0 |  | Xu Zheng, Tianchun Wang, Wei Cheng, Aitian Ma, Haifeng Chen, Mo Sha, Dongsheng Luo |  |
| 1685 |  |  [Prediction Error-based Classification for Class-Incremental Learning](https://openreview.net/forum?id=DJZDgMOLXQ) |  | 0 |  | Michal Zajac, Tinne Tuytelaars, Gido M. van de Ven |  |
| 1686 |  |  [Test-Time Training on Nearest Neighbors for Large Language Models](https://openreview.net/forum?id=CNL2bku4ra) |  | 0 |  | Moritz Hardt, Yu Sun |  |
| 1687 |  |  [Improved Probabilistic Image-Text Representations](https://openreview.net/forum?id=ft1mr3WlGM) |  | 0 |  | Sanghyuk Chun |  |
| 1688 |  |  [SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos](https://openreview.net/forum?id=abL5LJNZ49) |  | 0 |  | Yulei Niu, Wenliang Guo, Long Chen, Xudong Lin, ShihFu Chang |  |
| 1689 |  |  [BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection](https://openreview.net/forum?id=s56xikpD92) |  | 0 |  | Tinghao Xie, Xiangyu Qi, Ping He, Yiming Li, Jiachen T. Wang, Prateek Mittal |  |
| 1690 |  |  [Is Self-Repair a Silver Bullet for Code Generation?](https://openreview.net/forum?id=y0GJXRungR) |  | 0 |  | Theo X. Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, Armando SolarLezama |  |
| 1691 |  |  [MgNO: Efficient Parameterization of Linear Operators via Multigrid](https://openreview.net/forum?id=8OxL034uEr) |  | 0 |  | Juncai He, Xinliang Liu, Jinchao Xu |  |
| 1692 |  |  [Interpreting Robustness Proofs of Deep Neural Networks](https://openreview.net/forum?id=Ev10F9TWML) |  | 0 |  | Debangshu Banerjee, Avaljot Singh, Gagandeep Singh |  |
| 1693 |  |  [Differentially Private SGD Without Clipping Bias: An Error-Feedback Approach](https://openreview.net/forum?id=uFbWHyTlPn) |  | 0 |  | Xinwei Zhang, Zhiqi Bu, Steven Wu, Mingyi Hong |  |
| 1694 |  |  [A Linear Algebraic Framework for Counterfactual Generation](https://openreview.net/forum?id=PoDkdFQIu3) |  | 0 |  | JongHoon Ahn, Akshay Vashist |  |
| 1695 |  |  [DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genomes](https://openreview.net/forum?id=oMLQB4EZE1) |  | 0 |  | Zhihan Zhou, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana V. Davuluri, Han Liu |  |
| 1696 |  |  [Learning Stackable and Skippable LEGO Bricks for Efficient, Reconfigurable, and Variable-Resolution Diffusion Modeling](https://openreview.net/forum?id=qmXedvwrT1) |  | 0 |  | Huangjie Zheng, Zhendong Wang, Jianbo Yuan, Guanghan Ning, Pengcheng He, Quanzeng You, Hongxia Yang, Mingyuan Zhou |  |
| 1697 |  |  [Latent 3D Graph Diffusion](https://openreview.net/forum?id=cXbnGtO0NZ) |  | 0 |  | Yuning You, Ruida Zhou, Jiwoong Park, Haotian Xu, Chao Tian, Zhangyang Wang, Yang Shen |  |
| 1698 |  |  [Reward Design for Justifiable Sequential Decision-Making](https://openreview.net/forum?id=OUkZXbbwQr) |  | 0 |  | Aleksa Sukovic, Goran Radanovic |  |
| 1699 |  |  [SAFLEX: Self-Adaptive Augmentation via Feature Label Extrapolation](https://openreview.net/forum?id=qL6brrBDk2) |  | 0 |  | Mucong Ding, Bang An, Yuancheng Xu, Anirudh Satheesh, Furong Huang |  |
| 1700 |  |  [LILO: Learning Interpretable Libraries by Compressing and Documenting Code](https://openreview.net/forum?id=TqYbAWKMIe) |  | 0 |  | Gabriel Grand, Lionel Wong, Matthew Bowers, Theo X. Olausson, Muxin Liu, Joshua B. Tenenbaum, Jacob Andreas |  |
| 1701 |  |  [Matryoshka Diffusion Models](https://openreview.net/forum?id=tOzCcDdH9O) |  | 0 |  | Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Joshua M. Susskind, Navdeep Jaitly |  |
| 1702 |  |  [Scalable Neural Network Kernels](https://openreview.net/forum?id=4iPw1klFWa) |  | 0 |  | Arijit Sehanobish, Krzysztof Marcin Choromanski, Yunfan Zhao, Kumar Avinava Dubey, Valerii Likhosherstov |  |
| 1703 |  |  [Model Merging by Uncertainty-Based Gradient Matching](https://openreview.net/forum?id=D7KJmfEDQP) |  | 0 |  | Nico Daheim, Thomas Möllenhoff, Edoardo M. Ponti, Iryna Gurevych, Mohammad Emtiyaz Khan |  |
| 1704 |  |  [αTC-VAE: On the relationship between Disentanglement and Diversity](https://openreview.net/forum?id=ptXo0epLQo) |  | 0 |  | Cristian Meo, Louis Mahon, Anirudh Goyal, Justin Dauwels |  |
| 1705 |  |  [A Restoration Network as an Implicit Prior](https://openreview.net/forum?id=x7d1qXEn1e) |  | 0 |  | Yuyang Hu, Mauricio Delbracio, Peyman Milanfar, Ulugbek Kamilov |  |
| 1706 |  |  [The Reasonableness Behind Unreasonable Translation Capability of Large Language Model](https://openreview.net/forum?id=3KDbIWT26J) |  | 0 |  | Tingchen Fu, Lemao Liu, Deng Cai, Guoping Huang, Shuming Shi, Rui Yan |  |
| 1707 |  |  [Fast Value Tracking for Deep Reinforcement Learning](https://openreview.net/forum?id=LZIOBA2oDU) |  | 0 |  | Frank Shih, Faming Liang |  |
| 1708 |  |  [Oracle Efficient Algorithms for Groupwise Regret](https://openreview.net/forum?id=HrRKc9ei7h) |  | 0 |  | Krishna Acharya, Eshwar Ram Arunachaleswaran, Sampath Kannan, Aaron Roth, Juba Ziani |  |
| 1709 |  |  [Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models](https://openreview.net/forum?id=26XphugOcS) |  | 0 |  | Zijun Wu, Yongkang Wu, Lili Mou |  |
| 1710 |  |  [FairerCLIP: Debiasing CLIP's Zero-Shot Predictions using Functions in RKHSs](https://openreview.net/forum?id=HXoq9EqR9e) |  | 0 |  | Sepehr Dehdashtian, Lan Wang, Vishnu Boddeti |  |
| 1711 |  |  [Generalized Schrödinger Bridge Matching](https://openreview.net/forum?id=SoismgeX7z) |  | 0 |  | GuanHorng Liu, Yaron Lipman, Maximilian Nickel, Brian Karrer, Evangelos A. Theodorou, Ricky T. Q. Chen |  |
| 1712 |  |  [Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition](https://openreview.net/forum?id=dQVtTdsvZH) |  | 0 |  | Sihyun Yu, Weili Nie, DeAn Huang, Boyi Li, Jinwoo Shin, Anima Anandkumar |  |
| 1713 |  |  [MAMBA: an Effective World Model Approach for Meta-Reinforcement Learning](https://openreview.net/forum?id=1RE0H6mU7M) |  | 0 |  | Zohar Rimon, Tom Jurgenson, Orr Krupnik, Gilad Adler, Aviv Tamar |  |
| 1714 |  |  [Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit](https://openreview.net/forum?id=KZJehvRKGD) |  | 0 |  | Blake Bordelon, Lorenzo Noci, Mufan Bill Li, Boris Hanin, Cengiz Pehlevan |  |
| 1715 |  |  [Efficient-3Dim: Learning a Generalizable Single-image Novel-view Synthesizer in One Day](https://openreview.net/forum?id=3eFMnZ3N4J) |  | 0 |  | Yifan Jiang, Hao Tang, JenHao Rick Chang, Liangchen Song, Zhangyang Wang, Liangliang Cao |  |
| 1716 |  |  [Code Representation Learning at Scale](https://openreview.net/forum?id=vfzRRjumpX) |  | 0 |  | Dejiao Zhang, Wasi Uddin Ahmad, Ming Tan, Hantian Ding, Ramesh Nallapati, Dan Roth, Xiaofei Ma, Bing Xiang |  |
| 1717 |  |  [Relaxing the Additivity Constraints in Decentralized No-Regret High-Dimensional Bayesian Optimization](https://openreview.net/forum?id=de1218PoEl) |  | 0 |  | Anthony Bardou, Patrick Thiran, Thomas Begin |  |
| 1718 |  |  [Compressing LLMs: The Truth is Rarely Pure and Never Simple](https://openreview.net/forum?id=B9klVS7Ddk) |  | 0 |  | Ajay Kumar Jaiswal, Zhe Gan, Xianzhi Du, Bowen Zhang, Zhangyang Wang, Yinfei Yang |  |
| 1719 |  |  [An Investigation of Representation and Allocation Harms in Contrastive Learning](https://openreview.net/forum?id=q4SiDyYQbo) |  | 0 |  | Subha Maity, Mayank Agarwal, Mikhail Yurochkin, Yuekai Sun |  |
| 1720 |  |  [SEPT: Towards Efficient Scene Representation Learning for Motion Prediction](https://openreview.net/forum?id=efeBC1sQj9) |  | 0 |  | Zhiqian Lan, Yuxuan Jiang, Yao Mu, Chen Chen, Shengbo Eben Li |  |
| 1721 |  |  [Efficient and Scalable Graph Generation through Iterative Local Expansion](https://openreview.net/forum?id=2XkTz7gdpc) |  | 0 |  | Andreas Bergmeister, Karolis Martinkus, Nathanaël Perraudin, Roger Wattenhofer |  |
| 1722 |  |  [Discovering modular solutions that generalize compositionally](https://openreview.net/forum?id=H98CVcX1eh) |  | 0 |  | Simon Schug, Seijin Kobayashi, Yassir Akram, Maciej Wolczyk, Alexandra Proca, Johannes von Oswald, Razvan Pascanu, João Sacramento, Angelika Steger |  |
| 1723 |  |  [Neural Atoms: Propagating Long-range Interaction in Molecular Graphs through Efficient Communication Channel](https://openreview.net/forum?id=CUfSCwcgqm) |  | 0 |  | Xuan Li, Zhanke Zhou, Jiangchao Yao, Yu Rong, Lu Zhang, Bo Han |  |
| 1724 |  |  [FairTune: Optimizing Parameter Efficient Fine Tuning for Fairness in Medical Image Analysis](https://openreview.net/forum?id=ArpwmicoYW) |  | 0 |  | Raman Dutt, Ondrej Bohdal, Sotirios A. Tsaftaris, Timothy M. Hospedales |  |
| 1725 |  |  [Tree-Planner: Efficient Close-loop Task Planning with Large Language Models](https://openreview.net/forum?id=Glcsog6zOe) |  | 0 |  | Mengkang Hu, Yao Mu, Xinmiao Yu, Mingyu Ding, Shiguang Wu, Wenqi Shao, Qiguang Chen, Bin Wang, Yu Qiao, Ping Luo |  |
| 1726 |  |  [It's Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition](https://openreview.net/forum?id=QqjFHyQwtF) |  | 0 |  | Chen Chen, Ruizhe Li, Yuchen Hu, Sabato Marco Siniscalchi, PinYu Chen, Engsiong Chng, ChaoHan Huck Yang |  |
| 1727 |  |  [LOQA: Learning with Opponent Q-Learning Awareness](https://openreview.net/forum?id=FDQF6A1s6M) |  | 0 |  | Milad Aghajohari, Juan Agustin Duque, Tim Cooijmans, Aaron C. Courville |  |
| 1728 |  |  [A 2-Dimensional State Space Layer for Spatial Inductive Bias](https://openreview.net/forum?id=BGkqypmGvm) |  | 0 |  | Ethan Baron, Itamar Zimerman, Lior Wolf |  |
| 1729 |  |  [Learning 3D Particle-based Simulators from RGB-D Videos](https://openreview.net/forum?id=4rBEgZCubP) |  | 0 |  | William F. Whitney, Tatiana LopezGuevara, Tobias Pfaff, Yulia Rubanova, Thomas Kipf, Kim Stachenfeld, Kelsey R. Allen |  |
| 1730 |  |  [CAMBranch: Contrastive Learning with Augmented MILPs for Branching](https://openreview.net/forum?id=K6kt50zAiG) |  | 0 |  | Jiacheng Lin, Meng Xu, Zhihua Xiong, Huangang Wang |  |
| 1731 |  |  [ALAM: Averaged Low-Precision Activation for Memory-Efficient Training of Transformer Models](https://openreview.net/forum?id=OfXqQ5TRwp) |  | 0 |  | Sunghyeon Woo, Sunwoo Lee, Dongsuk Jeon |  |
| 1732 |  |  [Causal Fairness under Unobserved Confounding: A Neural Sensitivity Framework](https://openreview.net/forum?id=DqD59dQP37) |  | 0 |  | Maresa Schröder, Dennis Frauen, Stefan Feuerriegel |  |
| 1733 |  |  [One Forward is Enough for Neural Network Training via Likelihood Ratio Method](https://openreview.net/forum?id=ALGFFPXWSi) |  | 0 |  | Jinyang Jiang, Zeliang Zhang, Chenliang Xu, Zhaofei Yu, Yijie Peng |  |
| 1734 |  |  [A Framework for Inference Inspired by Human Memory Mechanisms](https://openreview.net/forum?id=vBo7544jZx) |  | 0 |  | Xiangyu Zeng, Jie Lin, Piao Hu, Ruizheng Huang, Zhicheng Zhang |  |
| 1735 |  |  [Counterfactual Density Estimation using Kernel Stein Discrepancies](https://openreview.net/forum?id=wZXlEFO3tZ) |  | 0 |  | Diego MartinezTaboada, Edward Kennedy |  |
| 1736 |  |  [Does Writing with Language Models Reduce Content Diversity?](https://openreview.net/forum?id=Feiz5HtCD0) |  | 0 |  | Vishakh Padmakumar, He He |  |
| 1737 |  |  [Conformal Inductive Graph Neural Networks](https://openreview.net/forum?id=homn1jOKI5) |  | 0 |  | Soroush H. Zargarbashi, Aleksandar Bojchevski |  |
| 1738 |  |  [PB-LLM: Partially Binarized Large Language Models](https://openreview.net/forum?id=BifeBRhikU) |  | 0 |  | Zhihang Yuan, Yuzhang Shang, Zhen Dong |  |
| 1739 |  |  [Random Sparse Lifts: Construction, Analysis and Convergence of finite sparse networks](https://openreview.net/forum?id=rBH7x87VfJ) |  | 0 |  | David A. R. Robin, Kevin Scaman, Marc Lelarge |  |
| 1740 |  |  [Course Correcting Koopman Representations](https://openreview.net/forum?id=A18gWgc5mi) |  | 0 |  | Mahan Fathi, Clement Gehring, Jonathan Pilault, David Kanaa, PierreLuc Bacon, Ross Goroshin |  |
| 1741 |  |  [Out-of-Distribution Detection by Leveraging Between-Layer Transformation Smoothness](https://openreview.net/forum?id=AcRfzLS6se) |  | 0 |  | Fran Jelenic, Josip Jukic, Martin Tutek, Mate Puljiz, Jan Snajder |  |
| 1742 |  |  [Intelligent Switching for Reset-Free RL](https://openreview.net/forum?id=Nq45xeghcL) |  | 0 |  | Darshan Patil, Janarthanan Rajendran, Glen Berseth, Sarath Chandar |  |
| 1743 |  |  [Adversarial Supervision Makes Layout-to-Image Diffusion Models Thrive](https://openreview.net/forum?id=EJPIzl7mgc) |  | 0 |  | Yumeng Li, Margret Keuper, Dan Zhang, Anna Khoreva |  |
| 1744 |  |  [Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning](https://openreview.net/forum?id=EvDeiLv7qc) |  | 0 |  | Ted Zadouri, Ahmet Üstün, Arash Ahmadian, Beyza Ermis, Acyr Locatelli, Sara Hooker |  |
| 1745 |  |  [Less is More: One-shot Subgraph Reasoning on Large-scale Knowledge Graphs](https://openreview.net/forum?id=QHROe7Mfcb) |  | 0 |  | Zhanke Zhou, Yongqi Zhang, Jiangchao Yao, Quanming Yao, Bo Han |  |
| 1746 |  |  [Fixed-Budget Differentially Private Best Arm Identification](https://openreview.net/forum?id=vrE2fqAInO) |  | 0 |  | Zhirui Chen, P. N. Karthik, Yeow Meng Chee, Vincent Y. F. Tan |  |
| 1747 |  |  [Separate and Diffuse: Using a Pretrained Diffusion Model for Better Source Separation](https://openreview.net/forum?id=UXALv0lJZS) |  | 0 |  | Shahar Lutati, Eliya Nachmani, Lior Wolf |  |
| 1748 |  |  [On the Limitations of Temperature Scaling for Distributions with Overlaps](https://openreview.net/forum?id=zavLQJ1XjB) |  | 0 |  | Muthu Chidambaram, Rong Ge |  |
| 1749 |  |  [Unknown Domain Inconsistency Minimization for Domain Generalization](https://openreview.net/forum?id=eNoiRal5xi) |  | 0 |  | Seungjae Shin, HeeSun Bae, Byeonghu Na, YoonYeong Kim, IlChul Moon |  |
| 1750 |  |  [Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs](https://openreview.net/forum?id=gjeQKFxFpZ) |  | 0 |  | Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi |  |
| 1751 |  |  [Enhancing Neural Training via a Correlated Dynamics Model](https://openreview.net/forum?id=c9xsaASm9L) |  | 0 |  | Jonathan Brokman, Roy Betser, Rotem Turjeman, Tom Berkov, Ido Cohen, Guy Gilboa |  |
| 1752 |  |  [Flat Minima in Linear Estimation and an Extended Gauss Markov Theorem](https://openreview.net/forum?id=nxnbPPVvOG) |  | 0 |  | Simon N. Segert |  |
| 1753 |  |  [A Simple and Scalable Representation for Graph Generation](https://openreview.net/forum?id=nO344avRib) |  | 0 |  | Yunhui Jang, Seul Lee, Sungsoo Ahn |  |
| 1754 |  |  [True Knowledge Comes from Practice: Aligning Large Language Models with Embodied Environments via Reinforcement Learning](https://openreview.net/forum?id=hILVmJ4Uvu) |  | 0 |  | Weihao Tan, Wentao Zhang, Shanqi Liu, Longtao Zheng, Xinrun Wang, Bo An |  |
| 1755 |  |  [AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation](https://openreview.net/forum?id=fcqWJ8JgMR) |  | 0 |  | Zihao Tang, Zheqi Lv, Shengyu Zhang, Yifan Zhou, Xinyu Duan, Fei Wu, Kun Kuang |  |
| 1756 |  |  [The Cost of Scaling Down Large Language Models: Reducing Model Size Affects Memory before In-context Learning](https://openreview.net/forum?id=ldJXXxPE0L) |  | 0 |  | Tian Jin, Nolan Clement, Xin Dong, Vaishnavh Nagarajan, Michael Carbin, Jonathan RaganKelley, Gintare Karolina Dziugaite |  |
| 1757 |  |  [FedInverse: Evaluating Privacy Leakage in Federated Learning](https://openreview.net/forum?id=nTNgkEIfeb) |  | 0 |  | Di Wu, Jun Bai, Yiliao Song, Junjun Chen, Wei Zhou, Yong Xiang, Atul Sajjanhar |  |
| 1758 |  |  [TOSS: High-quality Text-guided Novel View Synthesis from a Single Image](https://openreview.net/forum?id=9ZUYJpvIys) |  | 0 |  | Yukai Shi, Jianan Wang, He Cao, Boshi Tang, Xianbiao Qi, Tianyu Yang, Yukun Huang, Shilong Liu, Lei Zhang, HeungYeung Shum |  |
| 1759 |  |  [Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation](https://openreview.net/forum?id=oZtt0pRnOl) |  | 0 |  | Xinyu Tang, Richard Shin, Huseyin A. Inan, Andre Manoel, Fatemehsadat Mireshghallah, Zinan Lin, Sivakanth Gopi, Janardhan Kulkarni, Robert Sim |  |
| 1760 |  |  [Grokking as a First Order Phase Transition in Two Layer Networks](https://openreview.net/forum?id=3ROGsTX3IR) |  | 0 |  | Noa Rubin, Inbar Seroussi, Zohar Ringel |  |
| 1761 |  |  [Elucidating the Exposure Bias in Diffusion Models](https://openreview.net/forum?id=xEJMoj1SpX) |  | 0 |  | Mang Ning, Mingxiao Li, Jianlin Su, Albert Ali Salah, Itir Önal Ertugrul |  |
| 1762 |  |  [Accelerated Convergence of Stochastic Heavy Ball Method under Anisotropic Gradient Noise](https://openreview.net/forum?id=CIqjp9yTDq) |  | 0 |  | Rui Pan, Yuxing Liu, Xiaoyu Wang, Tong Zhang |  |
| 1763 |  |  [Fast, Expressive SE(n) Equivariant Networks through Weight-Sharing in Position-Orientation Space](https://openreview.net/forum?id=dPHLbUqGbr) |  | 0 |  | Erik J. Bekkers, Sharvaree P. Vadgama, Rob Hesselink, Putri A. van der Linden, David W. Romero |  |
| 1764 |  |  [Learning Large DAGs is Harder than you Think: Many Losses are Minimal for the Wrong DAG](https://openreview.net/forum?id=gwbQ2YwLhD) |  | 0 |  | Jonas Seng, Matej Zecevic, Devendra Singh Dhami, Kristian Kersting |  |
| 1765 |  |  [JointNet: Extending Text-to-Image Diffusion for Dense Distribution Modeling](https://openreview.net/forum?id=kv5xE1p3jz) |  | 0 |  | Jingyang Zhang, Shiwei Li, Yuanxun Lu, Tian Fang, David McKinnon, Yanghai Tsin, Long Quan, Yao Yao |  |
| 1766 |  |  [Successor Heads: Recurring, Interpretable Attention Heads In The Wild](https://openreview.net/forum?id=kvcbV8KQsi) |  | 0 |  | Rhys Gould, Euan Ong, George Ogden, Arthur Conmy |  |
| 1767 |  |  [RAPPER: Reinforced Rationale-Prompted Paradigm for Natural Language Explanation in Visual Question Answering](https://openreview.net/forum?id=bshfchPM9H) |  | 0 |  | KaiPo Chang, ChiPin Huang, WeiYuan Cheng, FuEn Yang, ChienYi Wang, YungHsuan Lai, YuChiang Frank Wang |  |
| 1768 |  |  [Information Bottleneck Analysis of Deep Neural Networks via Lossy Compression](https://openreview.net/forum?id=huGECz8dPp) |  | 0 |  | Ivan Butakov, Aleksander Tolmachev, Sofia Malanchuk, Anna Neopryatnaya, Alexey A. Frolov, Kirill V. Andreev |  |
| 1769 |  |  [Knowledge Fusion of Large Language Models](https://openreview.net/forum?id=jiDsk12qcz) |  | 0 |  | Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, Shuming Shi |  |
| 1770 |  |  [FedTrans: Client-Transparent Utility Estimation for Robust Federated Learning](https://openreview.net/forum?id=DRu8PMHgCh) |  | 0 |  | Mingkun Yang, Ran Zhu, Qing Wang, Jie Yang |  |
| 1771 |  |  [From Posterior Sampling to Meaningful Diversity in Image Restoration](https://openreview.net/forum?id=ff2g30cZxj) |  | 0 |  | Noa Cohen, Hila Manor, Yuval Bahat, Tomer Michaeli |  |
| 1772 |  |  [A Neural Framework for Generalized Causal Sensitivity Analysis](https://openreview.net/forum?id=ikX6D1oM1c) |  | 0 |  | Dennis Frauen, Fergus Imrie, Alicia Curth, Valentyn Melnychuk, Stefan Feuerriegel, Mihaela van der Schaar |  |
| 1773 |  |  [Skill Machines: Temporal Logic Skill Composition in Reinforcement Learning](https://openreview.net/forum?id=qiduMcw3CU) |  | 0 |  | Geraud Nangue Tasse, Devon Jarvis, Steven James, Benjamin Rosman |  |
| 1774 |  |  [Unveiling and Manipulating Prompt Influence in Large Language Models](https://openreview.net/forum?id=ap1ByuwQrX) |  | 0 |  | Zijian Feng, Hanzhang Zhou, Zixiao Zhu, Junlang Qian, Kezhi Mao |  |
| 1775 |  |  [Learning to solve Class-Constrained Bin Packing Problems via Encoder-Decoder Model](https://openreview.net/forum?id=6hvtSLkKeZ) |  | 0 |  | Hanni Cheng, Ya Cong, Weihao Jiang, Shiliang Pu |  |
| 1776 |  |  [Idempotent Generative Network](https://openreview.net/forum?id=XIaS66XkNA) |  | 0 |  | Assaf Shocher, Amil Dravid, Yossi Gandelsman, Inbar Mosseri, Michael Rubinstein, Alexei A. Efros |  |
| 1777 |  |  [DeepSPF: Spherical SO(3)-Equivariant Patches for Scan-to-CAD Estimation](https://openreview.net/forum?id=Dnc3paMqDE) |  | 0 |  | Driton Salihu, Adam Misik, Yuankai Wu, Constantin Patsch, Fabián Seguel, Eckehard G. Steinbach |  |
| 1778 |  |  [SparseDFF: Sparse-View Feature Distillation for One-Shot Dexterous Manipulation](https://openreview.net/forum?id=HHWlwxDeRn) |  | 0 |  | Qianxu Wang, Haotong Zhang, Congyue Deng, Yang You, Hao Dong, Yixin Zhu, Leonidas J. Guibas |  |
| 1779 |  |  [ReFusion: Improving Natural Language Understanding with Computation-Efficient Retrieval Representation Fusion](https://openreview.net/forum?id=JtKGkz9fAe) |  | 0 |  | Shangyu Wu, Ying Xiong, Yufei Cui, Xue Liu, Buzhou Tang, TeiWei Kuo, Chun Jason Xue |  |
| 1780 |  |  [FedCDA: Federated Learning with Cross-rounds Divergence-aware Aggregation](https://openreview.net/forum?id=nbPGqeH3lt) |  | 0 |  | Haozhao Wang, Haoran Xu, Yichen Li, Yuan Xu, Ruixuan Li, Tianwei Zhang |  |
| 1781 |  |  [Select to Perfect: Imitating desired behavior from large multi-agent data](https://openreview.net/forum?id=L6crLU7MIE) |  | 0 |  | Tim Franzmeyer, Edith Elkind, Philip Torr, Jakob Nicolaus Foerster, João F. Henriques |  |
| 1782 |  |  [Respect the model: Fine-grained and Robust Explanation with Sharing Ratio Decomposition](https://openreview.net/forum?id=U7VW3KBm34) |  | 0 |  | Sangyu Han, Yearim Kim, Nojun Kwak |  |
| 1783 |  |  [Exploring Diffusion Time-steps for Unsupervised Representation Learning](https://openreview.net/forum?id=bWzxhtl1HP) |  | 0 |  | Zhongqi Yue, Jiankun Wang, Qianru Sun, Lei Ji, Eric IChao Chang, Hanwang Zhang |  |
| 1784 |  |  [SweetDreamer: Aligning Geometric Priors in 2D diffusion for Consistent Text-to-3D](https://openreview.net/forum?id=extpNXo6hB) |  | 0 |  | Weiyu Li, Rui Chen, Xuelin Chen, Ping Tan |  |
| 1785 |  |  [Hypergraph Dynamic System](https://openreview.net/forum?id=NLbRvr840Q) |  | 0 |  | Jielong Yan, Yifan Feng, Shihui Ying, Yue Gao |  |
| 1786 |  |  [Augmented Bayesian Policy Search](https://openreview.net/forum?id=OvlcyABNQT) |  | 0 |  | Mahdi Kallel, Debabrota Basu, Riad Akrour, Carlo D'Eramo |  |
| 1787 |  |  [Emu: Generative Pretraining in Multimodality](https://openreview.net/forum?id=mL8Q9OOamV) |  | 0 |  | Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, Xinlong Wang |  |
| 1788 |  |  [Dirichlet-based Per-Sample Weighting by Transition Matrix for Noisy Label Learning](https://openreview.net/forum?id=A4mJuFRMN8) |  | 0 |  | HeeSun Bae, Seungjae Shin, Byeonghu Na, IlChul Moon |  |
| 1789 |  |  [HoloNets: Spectral Convolutions do extend to Directed Graphs](https://openreview.net/forum?id=EhmEwfavOW) |  | 0 |  | Christian Koke, Daniel Cremers |  |
| 1790 |  |  [Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback](https://openreview.net/forum?id=WesY0H9ghM) |  | 0 |  | Yifu Yuan, Jianye Hao, Yi Ma, Zibin Dong, Hebin Liang, Jinyi Liu, Zhixin Feng, Kai Zhao, Yan Zheng |  |
| 1791 |  |  [Improving the Convergence of Dynamic NeRFs via Optimal Transport](https://openreview.net/forum?id=KiespDPaRH) |  | 0 |  | Sameera Ramasinghe, Violetta Shevchenko, Gil Avraham, Hisham Husain, Anton van den Hengel |  |
| 1792 |  |  [Fast Updating Truncated SVD for Representation Learning with Sparse Matrices](https://openreview.net/forum?id=CX2RgsS29V) |  | 0 |  | Haoran Deng, Yang Yang, Jiahe Li, Cheng Chen, Weihao Jiang, Shiliang Pu |  |
| 1793 |  |  [Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning](https://openreview.net/forum?id=N0I2RtD8je) |  | 0 |  | Juan Rocamonde, Victoriano Montesinos, Elvis Nava, Ethan Perez, David Lindner |  |
| 1794 |  |  [TokenFlow: Consistent Diffusion Features for Consistent Video Editing](https://openreview.net/forum?id=lKK50q2MtV) |  | 0 |  | Michal Geyer, Omer BarTal, Shai Bagon, Tali Dekel |  |
| 1795 |  |  [DreamTime: An Improved Optimization Strategy for Diffusion-Guided 3D Generation](https://openreview.net/forum?id=1bAUywYJTU) |  | 0 |  | Yukun Huang, Jianan Wang, Yukai Shi, Boshi Tang, Xianbiao Qi, Lei Zhang |  |
| 1796 |  |  [CARD: Channel Aligned Robust Blend Transformer for Time Series Forecasting](https://openreview.net/forum?id=MJksrOhurE) |  | 0 |  | Xue Wang, Tian Zhou, Qingsong Wen, Jinyang Gao, Bolin Ding, Rong Jin |  |
| 1797 |  |  [Reinforcement Symbolic Regression Machine](https://openreview.net/forum?id=PJVUWpPnZC) |  | 0 |  | Yilong Xu, Yang Liu, Hao Sun |  |
| 1798 |  |  [Towards Faithful XAI Evaluation via Generalization-Limited Backdoor Watermark](https://openreview.net/forum?id=cObFETcoeW) |  | 0 |  | Mengxi Ya, Yiming Li, Tao Dai, Bin Wang, Yong Jiang, ShuTao Xia |  |
| 1799 |  |  [Space Group Constrained Crystal Generation](https://openreview.net/forum?id=jkvZ7v4OmP) |  | 0 |  | Rui Jiao, Wenbing Huang, Yu Liu, Deli Zhao, Yang Liu |  |
| 1800 |  |  [Learning Multi-Agent Communication from Graph Modeling Perspective](https://openreview.net/forum?id=Qox9rO0kN0) |  | 0 |  | Shengchao Hu, Li Shen, Ya Zhang, Dacheng Tao |  |
| 1801 |  |  [Efficient Multi-agent Reinforcement Learning by Planning](https://openreview.net/forum?id=CpnKq3UJwp) |  | 0 |  | Qihan Liu, Jianing Ye, Xiaoteng Ma, Jun Yang, Bin Liang, Chongjie Zhang |  |
| 1802 |  |  [EventRPG: Event Data Augmentation with Relevance Propagation Guidance](https://openreview.net/forum?id=i7LCsDMcZ4) |  | 0 |  | Mingyuan Sun, Donghao Zhang, Zongyuan Ge, Jiaxu Wang, Jia Li, Zheng Fang, Renjing Xu |  |
| 1803 |  |  [NeRM: Learning Neural Representations for High-Framerate Human Motion Synthesis](https://openreview.net/forum?id=sOJriBlOFd) |  | 0 |  | Dong Wei, Huaijiang Sun, Bin Li, Xiaoning Sun, Shengxiang Hu, Weiqing Li, Jianfeng Lu |  |
| 1804 |  |  [Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization](https://openreview.net/forum?id=FlvtjAB0gl) |  | 0 |  | Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Quzhe Huang, Bin Chen, Chengru Song, Dai Meng, Di Zhang, Wenwu Ou, Kun Gai, Yadong Mu |  |
| 1805 |  |  [Efficient Backpropagation with Variance Controlled Adaptive Sampling](https://openreview.net/forum?id=gEwKAZZmSw) |  | 0 |  | Ziteng Wang, Jianfei Chen, Jun Zhu |  |
| 1806 |  |  [Dynamic Neighborhood Construction for Structured Large Discrete Action Spaces](https://openreview.net/forum?id=80wh3jjCZf) |  | 0 |  | Fabian Akkerman, Julius Luy, Wouter van Heeswijk, Maximilian Schiffer |  |
| 1807 |  |  [Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models](https://openreview.net/forum?id=6mLjDwYte5) |  | 0 |  | Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret Zoph, William Fedus, Xinyun Chen, Tu Vu, Yuexin Wu, Wuyang Chen, Albert Webson, Yunxuan Li, Vincent Y. Zhao, Hongkun Yu, Kurt Keutzer, Trevor Darrell, Denny Zhou |  |
| 1808 |  |  [DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models](https://openreview.net/forum?id=OqTMUPuLuC) |  | 0 |  | Licheng Wen, Daocheng Fu, Xin Li, Xinyu Cai, Tao Ma, Pinlong Cai, Min Dou, Botian Shi, Liang He, Yu Qiao |  |
| 1809 |  |  [PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images](https://openreview.net/forum?id=Cf4FJGmHRQ) |  | 0 |  | Jinsung Jeon, Hyundong Jin, Jonghyun Choi, Sanghyun Hong, Dongeun Lee, Kookjin Lee, Noseong Park |  |
| 1810 |  |  [CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets](https://openreview.net/forum?id=G0vdDSt9XM) |  | 0 |  | Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi Fung, Hao Peng, Heng Ji |  |
| 1811 |  |  [A Variational Framework for Estimating Continuous Treatment Effects with Measurement Error](https://openreview.net/forum?id=S46Knicu56) |  | 0 |  | Erdun Gao, Howard D. Bondell, Wei Huang, Mingming Gong |  |
| 1812 |  |  [InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation](https://openreview.net/forum?id=1k4yZbbDqX) |  | 0 |  | Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, Qiang Liu |  |
| 1813 |  |  [Object-Aware Inversion and Reassembly for Image Editing](https://openreview.net/forum?id=dpcVXiMlcv) |  | 0 |  | Zhen Yang, Ganggui Ding, Wen Wang, Hao Chen, Bohan Zhuang, Chunhua Shen |  |
| 1814 |  |  [SAN: Inducing Metrizability of GAN with Discriminative Normalized Linear Layer](https://openreview.net/forum?id=eiF7TU1E8E) |  | 0 |  | Yuhta Takida, Masaaki Imaizumi, Takashi Shibuya, ChiehHsin Lai, Toshimitsu Uesaka, Naoki Murata, Yuki Mitsufuji |  |
| 1815 |  |  [Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models](https://openreview.net/forum?id=dKl6lMwbCy) |  | 0 |  | Hritik Bansal, John Dang, Aditya Grover |  |
| 1816 |  |  [Sample-Efficient Multi-Agent RL: An Optimization Perspective](https://openreview.net/forum?id=o7qhUMylLU) |  | 0 |  | Nuoya Xiong, Zhihan Liu, Zhaoran Wang, Zhuoran Yang |  |
| 1817 |  |  [Minimum width for universal approximation using ReLU networks on compact domain](https://openreview.net/forum?id=dpDw5U04SU) |  | 0 |  | Namjun Kim, Chanho Min, Sejun Park |  |
| 1818 |  |  [Self-Supervised Dataset Distillation for Transfer Learning](https://openreview.net/forum?id=h57gkDO2Yg) |  | 0 |  | Dong Bok Lee, Seanie Lee, Joonho Ko, Kenji Kawaguchi, Juho Lee, Sung Ju Hwang |  |
| 1819 |  |  [Rethinking the symmetry-preserving circuits for constrained variational quantum algorithms](https://openreview.net/forum?id=SL7djdVpde) |  | 0 |  | Ge Yan, Hongxu Chen, Kaisen Pan, Junchi Yan |  |
| 1820 |  |  [Towards Codable Watermarking for Injecting Multi-Bits Information to LLMs](https://openreview.net/forum?id=JYu5Flqm9D) |  | 0 |  | Lean Wang, Wenkai Yang, Deli Chen, Hao Zhou, Yankai Lin, Fandong Meng, Jie Zhou, Xu Sun |  |
| 1821 |  |  [Hypothesis Search: Inductive Reasoning with Language Models](https://openreview.net/forum?id=G7UtIGQmjm) |  | 0 |  | Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, Noah D. Goodman |  |
| 1822 |  |  [Language Model Decoding as Direct Metrics Optimization](https://openreview.net/forum?id=488A64eOf6) |  | 0 |  | Haozhe Ji, Pei Ke, Hongning Wang, Minlie Huang |  |
| 1823 |  |  [Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment](https://openreview.net/forum?id=uMAujpVi9m) |  | 0 |  | Bowen Gao, Yinjun Jia, Yuanle Mo, Yuyan Ni, WeiYing Ma, ZhiMing Ma, Yanyan Lan |  |
| 1824 |  |  [MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning](https://openreview.net/forum?id=z8TW0ttBPp) |  | 0 |  | Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, Hongsheng Li |  |
| 1825 |  |  [Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification](https://openreview.net/forum?id=c8McWs4Av0) |  | 0 |  | Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, Hongsheng Li |  |
| 1826 |  |  [Dual Associated Encoder for Face Restoration](https://openreview.net/forum?id=gwDuW7Ok5f) |  | 0 |  | YuJu Tsai, YuLun Liu, Lu Qi, Kelvin C. K. Chan, MingHsuan Yang |  |
| 1827 |  |  [DiffusionSat: A Generative Foundation Model for Satellite Imagery](https://openreview.net/forum?id=I5webNFDgQ) |  | 0 |  | Samar Khanna, Patrick Liu, Linqi Zhou, Chenlin Meng, Robin Rombach, Marshall Burke, David B. Lobell, Stefano Ermon |  |
| 1828 |  |  [DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior](https://openreview.net/forum?id=DDX1u29Gqr) |  | 0 |  | Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen Liu, Zhenda Xie, Yebin Liu |  |
| 1829 |  |  [Pseudo-Generalized Dynamic View Synthesis from a Video](https://openreview.net/forum?id=QuVlUn4T2G) |  | 0 |  | Xiaoming Zhao, Alex Colburn, Fangchang Ma, Miguel Ángel Bautista, Joshua M. Susskind, Alexander G. Schwing |  |
| 1830 |  |  [Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification](https://openreview.net/forum?id=g6rZtxaXRm) |  | 0 |  | Reza Esfandiarpoor, Stephen H. Bach |  |
| 1831 |  |  [Interpretable Sparse System Identification: Beyond Recent Deep Learning Techniques on Time-Series Prediction](https://openreview.net/forum?id=aFWUY3E7ws) |  | 0 |  | Xiaoyi Liu, Duxin Chen, Wenjia Wei, Xia Zhu, Wenwu Yu |  |
| 1832 |  |  [Functional Bayesian Tucker Decomposition for Continuous-indexed Tensor Data](https://openreview.net/forum?id=ZWyZeqE928) |  | 0 |  | Shikai Fang, Xin Yu, Zheng Wang, Shibo Li, Mike Kirby, Shandian Zhe |  |
| 1833 |  |  [Generative Pre-training for Speech with Flow Matching](https://openreview.net/forum?id=KpoQSgxbKH) |  | 0 |  | Alexander H. Liu, Matthew Le, Apoorv Vyas, Bowen Shi, Andros Tjandra, WeiNing Hsu |  |
| 1834 |  |  [EBMDock: Neural Probabilistic Protein-Protein Docking via a Differentiable Energy Model](https://openreview.net/forum?id=qg2boc2AwU) |  | 0 |  | Huaijin Wu, Wei Liu, Yatao Bian, Jiaxiang Wu, Nianzu Yang, Junchi Yan |  |
| 1835 |  |  [CausalTime: Realistically Generated Time-series for Benchmarking of Causal Discovery](https://openreview.net/forum?id=iad1yyyGme) |  | 0 |  | Yuxiao Cheng, Ziqian Wang, Tingxiong Xiao, Qin Zhong, Jinli Suo, Kunlun He |  |
| 1836 |  |  [Protein-ligand binding representation learning from fine-grained interactions](https://openreview.net/forum?id=AXbN2qMNiW) |  | 0 |  | Shikun Feng, Minghao Li, Yinjun Jia, WeiYing Ma, Yanyan Lan |  |
| 1837 |  |  [SLiMe: Segment Like Me](https://openreview.net/forum?id=7FeIRqCedv) |  | 0 |  | Aliasghar Khani, Saeid Asgari Taghanaki, Aditya Sanghi, Ali MahdaviAmiri, Ghassan Hamarneh |  |
| 1838 |  |  [Adversarial Attacks on Fairness of Graph Neural Networks](https://openreview.net/forum?id=q3KNrmW6Ql) |  | 0 |  | Binchi Zhang, Yushun Dong, Chen Chen, Yada Zhu, Minnan Luo, Jundong Li |  |
| 1839 |  |  [Faithful Vision-Language Interpretation via Concept Bottleneck Models](https://openreview.net/forum?id=rp0EdI8X4e) |  | 0 |  | Songning Lai, Lijie Hu, Junxiao Wang, Laure BertiÉquille, Di Wang |  |
| 1840 |  |  [Efficiently Computing Similarities to Private Datasets](https://openreview.net/forum?id=HMe5CJv9dQ) |  | 0 |  | Arturs Backurs, Zinan Lin, Sepideh Mahabadi, Sandeep Silwal, Jakub Tarnawski |  |
| 1841 |  |  [Sliced Denoising: A Physics-Informed Molecular Pre-Training Method](https://openreview.net/forum?id=liKkG1zcWq) |  | 0 |  | Yuyan Ni, Shikun Feng, WeiYing Ma, ZhiMing Ma, Yanyan Lan |  |
| 1842 |  |  [Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection](https://openreview.net/forum?id=4UIBysXjVq) |  | 0 |  | Xiangyu Dong, Xingyi Zhang, Sibo Wang |  |
| 1843 |  |  [P2OT: Progressive Partial Optimal Transport for Deep Imbalanced Clustering](https://openreview.net/forum?id=hD3sGVqPsr) |  | 0 |  | Chuyu Zhang, Hui Ren, Xuming He |  |
| 1844 |  |  [SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores](https://openreview.net/forum?id=lajn1iROCu) |  | 0 |  | Zhiyu Mei, Wei Fu, Jiaxuan Gao, Guangju Wang, Huanchen Zhang, Yi Wu |  |
| 1845 |  |  [A Unified and General Framework for Continual Learning](https://openreview.net/forum?id=BE5aK0ETbp) |  | 0 |  | Zhenyi Wang, Yan Li, Li Shen, Heng Huang |  |
| 1846 |  |  [MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process](https://openreview.net/forum?id=CZiY6OLktd) |  | 0 |  | Xinyao Fan, Yueying Wu, Chang Xu, Yuhao Huang, Weiqing Liu, Jiang Bian |  |
| 1847 |  |  [Heterogeneous Personalized Federated Learning by Local-Global Updates Mixing via Convergence Rate](https://openreview.net/forum?id=7pWRLDBAtc) |  | 0 |  | Meirui Jiang, Anjie Le, Xiaoxiao Li, Qi Dou |  |
| 1848 |  |  [SetCSE: Set Operations using Contrastive Learning of Sentence Embeddings](https://openreview.net/forum?id=zEHGSN8Hy8) |  | 0 |  | Kang Liu |  |
| 1849 |  |  [Polynormer: Polynomial-Expressive Graph Transformer in Linear Time](https://openreview.net/forum?id=hmv1LpNfXa) |  | 0 |  | Chenhui Deng, Zichao Yue, Zhiru Zhang |  |
| 1850 |  |  [Where We Have Arrived in Proving the Emergence of Sparse Interaction Primitives in DNNs](https://openreview.net/forum?id=3pWSL8My6B) |  | 0 |  | Qihan Ren, Jiayang Gao, Wen Shen, Quanshi Zhang |  |
| 1851 |  |  [DV-3DLane: End-to-end Multi-modal 3D Lane Detection with Dual-view Representation](https://openreview.net/forum?id=l1U6sEgYkb) |  | 0 |  | Yueru Luo, Shuguang Cui, Zhen Li |  |
| 1852 |  |  [Continuous Field Reconstruction from Sparse Observations with Implicit Neural Networks](https://openreview.net/forum?id=kuTZMZdCPZ) |  | 0 |  | Xihaier Luo, Wei Xu, Balu Nadiga, Yihui Ren, Shinjae Yoo |  |
| 1853 |  |  [Weaker MVI Condition: Extragradient Methods with Multi-Step Exploration](https://openreview.net/forum?id=RNGUbTYSjk) |  | 0 |  | Yifeng Fan, Yongqiang Li, Bo Chen |  |
| 1854 |  |  [Demystifying Embedding Spaces using Large Language Models](https://openreview.net/forum?id=qoYogklIPz) |  | 0 |  | Guy Tennenholtz, Yinlam Chow, ChihWei Hsu, Jihwan Jeong, Lior Shani, Azamat Tulepbergenov, Deepak Ramachandran, Martin Mladenov, Craig Boutilier |  |
| 1855 |  |  [The Expressive Power of Low-Rank Adaptation](https://openreview.net/forum?id=likXVjmh3E) |  | 0 |  | Yuchen Zeng, Kangwook Lee |  |
| 1856 |  |  [Towards Category Unification of 3D Single Object Tracking on Point Clouds](https://openreview.net/forum?id=QlqdXrzzD1) |  | 0 |  | Jiahao Nie, Zhiwei He, Xudong Lv, Xueyi Zhou, DongKyu Chae, Fei Xie |  |
| 1857 |  |  [You Only Query Once: An Efficient Label-Only Membership Inference Attack](https://openreview.net/forum?id=7WsivwyHrS) |  | 0 |  | Yutong Wu, Han Qiu, Shangwei Guo, Jiwei Li, Tianwei Zhang |  |
| 1858 |  |  [Mathematical Justification of Hard Negative Mining via Isometric Approximation Theorem](https://openreview.net/forum?id=udO3k28bEw) |  | 0 |  | Albert Xu, JhihYi Hsieh, Bhaskar Vundurthy, Nithya Kemp, Eliana Cohen, Lu Li, Howie Choset |  |
| 1859 |  |  [Bridging Neural and Symbolic Representations with Transitional Dictionary Learning](https://openreview.net/forum?id=uqxBTcWRnj) |  | 0 |  | Junyan Cheng, Peter Chin |  |
| 1860 |  |  [AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality Prediction](https://openreview.net/forum?id=JW3jTjaaAB) |  | 0 |  | Kethmi Hirushini Hettige, Jiahao Ji, Shili Xiang, Cheng Long, Gao Cong, Jingyuan Wang |  |
| 1861 |  |  [DittoGym: Learning to Control Soft Shape-Shifting Robots](https://openreview.net/forum?id=MpyFAhH9CK) |  | 0 |  | Suning Huang, Boyuan Chen, Huazhe Xu, Vincent Sitzmann |  |
| 1862 |  |  [PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks](https://openreview.net/forum?id=gjXor87Xfy) |  | 0 |  | Junwei Su, Difan Zou, Chuan Wu |  |
| 1863 |  |  [TEDDY: Trimming Edges with Degree-based Discrimination Strategy](https://openreview.net/forum?id=5RUf9nEdyC) |  | 0 |  | Hyunjin Seo, Jihun Yun, Eunho Yang |  |
| 1864 |  |  [Learning to Jointly Understand Visual and Tactile Signals](https://openreview.net/forum?id=NtQqIcSbqv) |  | 0 |  | Yichen Li, Yilun Du, Chao Liu, Chao Liu, Francis Williams, Michael Foshey, Benjamin Eckart, Jan Kautz, Joshua B. Tenenbaum, Antonio Torralba, Wojciech Matusik |  |
| 1865 |  |  [NeurRev: Train Better Sparse Neural Network Practically via Neuron Revitalization](https://openreview.net/forum?id=60lNoatp7u) |  | 0 |  | Gen Li, Lu Yin, Jie Ji, Wei Niu, Minghai Qin, Bin Ren, Linke Guo, Shiwei Liu, Xiaolong Ma |  |
| 1866 |  |  [SPDER: Semiperiodic Damping-Enabled Object Representation](https://openreview.net/forum?id=92btneN9Wm) |  | 0 |  | Kathan Shah, Chawin Sitawarin |  |
| 1867 |  |  [The Unreasonable Effectiveness of Linear Prediction as a Perceptual Metric](https://openreview.net/forum?id=e4FG5PJ9uC) |  | 0 |  | Daniel Severo, Lucas Theis, Johannes Ballé |  |
| 1868 |  |  [Crystalformer: Infinitely Connected Attention for Periodic Structure Encoding](https://openreview.net/forum?id=fxQiecl9HB) |  | 0 |  | Tatsunori Taniai, Ryo Igarashi, Yuta Suzuki, Naoya Chiba, Kotaro Saito, Yoshitaka Ushiku, Kanta Ono |  |
| 1869 |  |  [Fast Ensembling with Diffusion Schrödinger Bridge](https://openreview.net/forum?id=Mgq6kxl115) |  | 0 |  | Hyunsu Kim, Jongmin Yoon, Juho Lee |  |
| 1870 |  |  [Decoupling regularization from the action space](https://openreview.net/forum?id=UaMgmoKEBj) |  | 0 |  | Sobhan Mohammadpour, Emma Frejinger, PierreLuc Bacon |  |
| 1871 |  |  [Robust Similarity Learning with Difference Alignment Regularization](https://openreview.net/forum?id=K9V7ugVuUz) |  | 0 |  | Shuo Chen, Gang Niu, Chen Gong, Okan Koc, Jian Yang, Masashi Sugiyama |  |
| 1872 |  |  [Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization](https://openreview.net/forum?id=PKICZXVY9M) |  | 0 |  | Yuhang Zang, Hanlin Goh, Joshua M. Susskind, Chen Huang |  |
| 1873 |  |  [ConR: Contrastive Regularizer for Deep Imbalanced Regression](https://openreview.net/forum?id=RIuevDSK5V) |  | 0 |  | Mahsa Keramati, Lili Meng, R. David Evans |  |
| 1874 |  |  [A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks](https://openreview.net/forum?id=LnLySuf1vp) |  | 0 |  | Jintang Li, Huizhe Zhang, Ruofan Wu, Zulun Zhu, Baokun Wang, Changhua Meng, Zibin Zheng, Liang Chen |  |
| 1875 |  |  [Adversarial Training Should Be Cast as a Non-Zero-Sum Game](https://openreview.net/forum?id=XJ9vjEAqbx) |  | 0 |  | Alexander Robey, Fabian Latorre, George J. Pappas, Hamed Hassani, Volkan Cevher |  |
| 1876 |  |  [Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks](https://openreview.net/forum?id=hQVCCxQrYN) |  | 0 |  | Murtaza Dalal, Tarun Chiruvolu, Devendra Singh Chaplot, Ruslan Salakhutdinov |  |
| 1877 |  |  [Diffusion-TS: Interpretable Diffusion for General Time Series Generation](https://openreview.net/forum?id=4h1apFjO99) |  | 0 |  | Xinyu Yuan, Yan Qiao |  |
| 1878 |  |  [ARM: Refining Multivariate Forecasting with Adaptive Temporal-Contextual Learning](https://openreview.net/forum?id=JWpwDdVbaM) |  | 0 |  | Jiecheng Lu, Xu Han, Shihao Yang |  |
| 1879 |  |  [Combining Axes Preconditioners through Kronecker Approximation for Deep Learning](https://openreview.net/forum?id=8j9hz8DVi8) |  | 0 |  | Sai Surya Duvvuri, Devvrit, Rohan Anil, ChoJui Hsieh, Inderjit S. Dhillon |  |
| 1880 |  |  [Dropout-Based Rashomon Set Exploration for Efficient Predictive Multiplicity Estimation](https://openreview.net/forum?id=Sf2A2PUXO3) |  | 0 |  | Hsiang Hsu, Guihong Li, Shaohan Hu, ChunFu Chen |  |
| 1881 |  |  [Copula Conformal prediction for multi-step time series prediction](https://openreview.net/forum?id=ojIJZDNIBj) |  | 0 |  | Sophia Huiwen Sun, Rose Yu |  |
| 1882 |  |  [Branch-GAN: Improving Text Generation with (not so) Large Language Models](https://openreview.net/forum?id=sHEJJmzBIN) |  | 0 |  | Fredrik Carlsson, Johan Broberg, Erik Hillbom, Magnus Sahlgren, Joakim Nivre |  |
| 1883 |  |  [Effective Structural Encodings via Local Curvature Profiles](https://openreview.net/forum?id=GIUjLsDP4Z) |  | 0 |  | Lukas Fesser, Melanie Weber |  |
| 1884 |  |  [Domain Randomization via Entropy Maximization](https://openreview.net/forum?id=GXtmuiVrOM) |  | 0 |  | Gabriele Tiboni, Pascal Klink, Jan Peters, Tatiana Tommasi, Carlo D'Eramo, Georgia Chalvatzaki |  |
| 1885 |  |  [OMNI: Open-endedness via Models of human Notions of Interestingness](https://openreview.net/forum?id=AgM3MzT99c) |  | 0 |  | Jenny Zhang, Joel Lehman, Kenneth O. Stanley, Jeff Clune |  |
| 1886 |  |  [Machine Unlearning for Image-to-Image Generative Models](https://openreview.net/forum?id=9hjVoPWPnh) |  | 0 |  | Guihong Li, Hsiang Hsu, ChunFu Chen, Radu Marculescu |  |
| 1887 |  |  [Classification with Conceptual Safeguards](https://openreview.net/forum?id=t8cBsT9mcg) |  | 0 |  | Hailey Joren, Charles T. Marx, Berk Ustun |  |
| 1888 |  |  [Linear attention is (maybe) all you need (to understand Transformer optimization)](https://openreview.net/forum?id=0uI5415ry7) |  | 0 |  | Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, Suvrit Sra |  |
| 1889 |  |  [MVDream: Multi-view Diffusion for 3D Generation](https://openreview.net/forum?id=FUgrjq2pbB) |  | 0 |  | Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, Xiao Yang |  |
| 1890 |  |  [Robust Model Based Reinforcement Learning Using L1 Adaptive Control](https://openreview.net/forum?id=GaLCLvJaoF) |  | 0 |  | Minjun Sung, Sambhu H. Karumanchi, Aditya Gahlawat, Naira Hovakimyan |  |
| 1891 |  |  [Headless Language Models: Learning without Predicting with Contrastive Weight Tying](https://openreview.net/forum?id=ONPECq0Rk7) |  | 0 |  | Nathan Godey, Éric Villemonte de la Clergerie, Benoît Sagot |  |
| 1892 |  |  [Leveraging Optimization for Adaptive Attacks on Image Watermarks](https://openreview.net/forum?id=O9PArxKLe1) |  | 0 |  | Nils Lukas, Abdulrahman Diaa, Lucas Fenaux, Florian Kerschbaum |  |
| 1893 |  |  [ZeRO++: Extremely Efficient Collective Communication for Large Model Training](https://openreview.net/forum?id=gx2BT0a9MQ) |  | 0 |  | Guanhua Wang, Heyang Qin, Sam Ade Jacobs, Xiaoxia Wu, Connor Holmes, Zhewei Yao, Samyam Rajbhandari, Olatunji Ruwase, Feng Yan, Lei Yang, Yuxiong He |  |
| 1894 |  |  [Large Language Models Cannot Self-Correct Reasoning Yet](https://openreview.net/forum?id=IkmD3fKBPQ) |  | 0 |  | Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, Denny Zhou |  |
| 1895 |  |  [Copilot4D: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion](https://openreview.net/forum?id=Psl75UCoZM) |  | 0 |  | Lunjun Zhang, Yuwen Xiong, Ze Yang, Sergio Casas, Rui Hu, Raquel Urtasun |  |
| 1896 |  |  [Universal Backdoor Attacks](https://openreview.net/forum?id=3QkzYBSWqL) |  | 0 |  | Benjamin Schneider, Nils Lukas, Florian Kerschbaum |  |
| 1897 |  |  [The Joint Effect of Task Similarity and Overparameterization on Catastrophic Forgetting - An Analytical Model](https://openreview.net/forum?id=u3dHl287oB) |  | 0 |  | Daniel Goldfarb, Itay Evron, Nir Weinberger, Daniel Soudry, Paul Hand |  |
| 1898 |  |  [GenCorres: Consistent Shape Matching via Coupled Implicit-Explicit Shape Generative Models](https://openreview.net/forum?id=dGH4kHFKFj) |  | 0 |  | Haitao Yang, Xiangru Huang, Bo Sun, Chandrajit L. Bajaj, Qixing Huang |  |
| 1899 |  |  [MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback](https://openreview.net/forum?id=jp3gWrMuIZ) |  | 0 |  | Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, Heng Ji |  |
| 1900 |  |  [Masked Audio Generation using a Single Non-Autoregressive Transformer](https://openreview.net/forum?id=Ny8NiVfi95) |  | 0 |  | Alon Ziv, Itai Gat, Gaël Le Lan, Tal Remez, Felix Kreuk, Jade Copet, Alexandre Défossez, Gabriel Synnaeve, Yossi Adi |  |
| 1901 |  |  [Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions](https://openreview.net/forum?id=gT5hALch9z) |  | 0 |  | Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Röttger, Dan Jurafsky, Tatsunori Hashimoto, James Zou |  |
| 1902 |  |  [Learning to Solve Bilevel Programs with Binary Tender](https://openreview.net/forum?id=PsDFgTosqb) |  | 0 |  | Bo Zhou, Ruiwei Jiang, Siqian Shen |  |
| 1903 |  |  [Manifold Diffusion Fields](https://openreview.net/forum?id=BZtEthuXRF) |  | 0 |  | Ahmed A. A. Elhag, Yuyang Wang, Joshua M. Susskind, Miguel Ángel Bautista |  |
| 1904 |  |  [Neur2RO: Neural Two-Stage Robust Optimization](https://openreview.net/forum?id=T5Xb0iGCCv) |  | 0 |  | Justin Dumouchelle, Esther Julien, Jannis Kurtz, Elias Boutros Khalil |  |
| 1905 |  |  [Efficient local linearity regularization to overcome catastrophic overfitting](https://openreview.net/forum?id=SZzQz8ikwg) |  | 0 |  | Elías AbadRocamora, Fanghui Liu, Grigorios Chrysos, Pablo M. Olmos, Volkan Cevher |  |
| 1906 |  |  [Compressing Latent Space via Least Volume](https://openreview.net/forum?id=jFJPd9kIiF) |  | 0 |  | Qiuyi Chen, Mark D. Fuge |  |
| 1907 |  |  [Parameter-Efficient Multi-Task Model Fusion with Partial Linearization](https://openreview.net/forum?id=iynRvVVAmH) |  | 0 |  | Anke Tang, Li Shen, Yong Luo, Yibing Zhan, Han Hu, Bo Du, Yixin Chen, Dacheng Tao |  |
| 1908 |  |  [Habitat 3.0: A Co-Habitat for Humans, Avatars, and Robots](https://openreview.net/forum?id=4znwzG92CE) |  | 0 |  | Xavier Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, TsungYen Yang, Ruslan Partsey, Ruta Desai, Alexander Clegg, Michal Hlavac, So Yeon Min, Vladimir Vondrus, Théophile Gervet, VincentPierre Berges, John M. Turner, Oleksandr Maksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra Malik, Devendra Singh Chaplot, Unnat Jain, Dhruv Batra, Akshara Rai, Roozbeh Mottaghi |  |
| 1909 |  |  [Let's Verify Step by Step](https://openreview.net/forum?id=v8L0pN6EOi) |  | 0 |  | Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, Karl Cobbe |  |
| 1910 |  |  [Masked Completion via Structured Diffusion with White-Box Transformers](https://openreview.net/forum?id=PvyOYleymy) |  | 0 |  | Druv Pai, Sam Buchanan, Ziyang Wu, Yaodong Yu, Yi Ma |  |
| 1911 |  |  [A Recipe for Improved Certifiable Robustness](https://openreview.net/forum?id=qz3mcn99cu) |  | 0 |  | Kai Hu, Klas Leino, Zifan Wang, Matt Fredrikson |  |
| 1912 |  |  [MEND: Meta Demonstration Distillation for Efficient and Effective In-Context Learning](https://openreview.net/forum?id=2Y5kBPtU0o) |  | 0 |  | Yichuan Li, Xiyao Ma, Sixing Lu, Kyumin Lee, Xiaohu Liu, Chenlei Guo |  |
| 1913 |  |  [The LLM Surgeon](https://openreview.net/forum?id=DYIIRgwg2i) |  | 0 |  | Tycho F. A. van der Ouderaa, Markus Nagel, Mart van Baalen, Tijmen Blankevoort |  |
| 1914 |  |  [Perceptual Scales Predicted by Fisher Information Metrics](https://openreview.net/forum?id=z7K2faBrDG) |  | 0 |  | Jonathan Vacher, Pascal Mamassian |  |
| 1915 |  |  [Language Model Inversion](https://openreview.net/forum?id=t9dWHpGkPj) |  | 0 |  | John X. Morris, Wenting Zhao, Justin T. Chiu, Vitaly Shmatikov, Alexander M. Rush |  |
| 1916 |  |  [Decomposed Diffusion Sampler for Accelerating Large-Scale Inverse Problems](https://openreview.net/forum?id=DsEhqQtfAG) |  | 0 |  | Hyungjin Chung, Suhyeon Lee, Jong Chul Ye |  |
| 1917 |  |  [Democratizing Fine-grained Visual Recognition with Large Language Models](https://openreview.net/forum?id=c7DND1iIgb) |  | 0 |  | Mingxuan Liu, Subhankar Roy, Wenjing Li, Zhun Zhong, Nicu Sebe, Elisa Ricci |  |
| 1918 |  |  [AlpaGasus: Training a Better Alpaca with Fewer Data](https://openreview.net/forum?id=FdVXgSJhvz) |  | 0 |  | Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, Hongxia Jin |  |
| 1919 |  |  [General Graph Random Features](https://openreview.net/forum?id=viftsX50Rt) |  | 0 |  | Isaac Reid, Krzysztof Marcin Choromanski, Eli Berger, Adrian Weller |  |
| 1920 |  |  [HyperAttention: Long-context Attention in Near-Linear Time](https://openreview.net/forum?id=Eh0Od2BJIM) |  | 0 |  | Insu Han, Rajesh Jayaram, Amin Karbasi, Vahab Mirrokni, David P. Woodruff, Amir Zandieh |  |
| 1921 |  |  [Repelling Random Walks](https://openreview.net/forum?id=31IOmrnoP4) |  | 0 |  | Isaac Reid, Eli Berger, Krzysztof Marcin Choromanski, Adrian Weller |  |
| 1922 |  |  [Stabilizing Backpropagation Through Time to Learn Complex Physics](https://openreview.net/forum?id=bozbTTWcaw) |  | 0 |  | Patrick Schnell, Nils Thuerey |  |
| 1923 |  |  [Learning in reverse causal strategic environments with ramifications on two sided markets](https://openreview.net/forum?id=vEfmVS5ywF) |  | 0 |  | Seamus Somerstep, Yuekai Sun, Yaacov Ritov |  |
| 1924 |  |  [Horizon-free Reinforcement Learning in Adversarial Linear Mixture MDPs](https://openreview.net/forum?id=aPNwsJgnZJ) |  | 0 |  | Kaixuan Ji, Qingyue Zhao, Jiafan He, Weitong Zhang, Quanquan Gu |  |
| 1925 |  |  [Backdoor Federated Learning by Poisoning Backdoor-Critical Layers](https://openreview.net/forum?id=AJBGSVSTT2) |  | 0 |  | Haomin Zhuang, Mingxian Yu, Hao Wang, Yang Hua, Jian Li, Xu Yuan |  |
| 1926 |  |  [RECOMBINER: Robust and Enhanced Compression with Bayesian Implicit Neural Representations](https://openreview.net/forum?id=VkWbxFrCC8) |  | 0 |  | Jiajun He, Gergely Flamich, Zongyu Guo, José Miguel HernándezLobato |  |
| 1927 |  |  [Light-MILPopt: Solving Large-scale Mixed Integer Linear Programs with Lightweight Optimizer and Small-scale Training Dataset](https://openreview.net/forum?id=2oWRumm67L) |  | 0 |  | Huigen Ye, Hua Xu, Hongyan Wang |  |
| 1928 |  |  [SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression](https://openreview.net/forum?id=Q1u25ahSuy) |  | 0 |  | Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, Dan Alistarh |  |
| 1929 |  |  [Towards Understanding Sycophancy in Language Models](https://openreview.net/forum?id=tvhaxkMKAn) |  | 0 |  | Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Esin Durmus, Zac HatfieldDodds, Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, Ethan Perez |  |
| 1930 |  |  [Network Memory Footprint Compression Through Jointly Learnable Codebooks and Mappings](https://openreview.net/forum?id=1RrOtCmuKr) |  | 0 |  | Edouard Yvinec, Arnaud Dapogny, Kevin Bailly |  |
| 1931 |  |  [Towards image compression with perfect realism at ultra-low bitrates](https://openreview.net/forum?id=ktdETU9JBg) |  | 0 |  | Marlène Careil, Matthew J. Muckley, Jakob Verbeek, Stéphane Lathuilière |  |
| 1932 |  |  [Scaling Laws of RoPE-based Extrapolation](https://openreview.net/forum?id=JO7k0SJ5V6) |  | 0 |  | Xiaoran Liu, Hang Yan, Chenxin An, Xipeng Qiu, Dahua Lin |  |
| 1933 |  |  [V-DETR: DETR with Vertex Relative Position Encoding for 3D Object Detection](https://openreview.net/forum?id=NDkpxG94sF) |  | 0 |  | Yichao Shen, Zigang Geng, Yuhui Yuan, Yutong Lin, Ze Liu, Chunyu Wang, Han Hu, Nanning Zheng, Baining Guo |  |
| 1934 |  |  [A Branching Decoder for Set Generation](https://openreview.net/forum?id=riNuqYiD66) |  | 0 |  | Zixian Huang, Gengyang Xiao, Yu Gu, Gong Cheng |  |
| 1935 |  |  [Data Filtering Networks](https://openreview.net/forum?id=KAk6ngZ09F) |  | 0 |  | Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander T. Toshev, Vaishaal Shankar |  |
| 1936 |  |  [Multi-task Learning with 3D-Aware Regularization](https://openreview.net/forum?id=TwBY17Hgiy) |  | 0 |  | WeiHong Li, Steven McDonagh, Ales Leonardis, Hakan Bilen |  |
| 1937 |  |  [Efficient Streaming Language Models with Attention Sinks](https://openreview.net/forum?id=NG7sS51zVF) |  | 0 |  | Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis |  |
| 1938 |  |  [OWL: A Large Language Model for IT Operations](https://openreview.net/forum?id=SZOQ9RKYJu) |  | 0 |  | Hongcheng Guo, Jian Yang, Jiaheng Liu, Liqun Yang, Linzheng Chai, Jiaqi Bai, Junran Peng, Xiaorong Hu, Chao Chen, Dongfeng Zhang, Xu Shi, Tieqiao Zheng, Liangfan Zheng, Bo Zhang, Ke Xu, Zhoujun Li |  |
| 1939 |  |  [DMBP: Diffusion model-based predictor for robust offline reinforcement learning against state observation perturbations](https://openreview.net/forum?id=ZULjcYLWKe) |  | 0 |  | Zhihe Yang, Yunjian Xu |  |
| 1940 |  |  [Generative Sliced MMD Flows with Riesz Kernels](https://openreview.net/forum?id=VdkGRV1vcf) |  | 0 |  | Johannes Hertrich, Christian Wald, Fabian Altekrüger, Paul Hagemann |  |
| 1941 |  |  [ARGS: Alignment as Reward-Guided Search](https://openreview.net/forum?id=shgx0eqdw6) |  | 0 |  | Maxim Khanov, Jirayu Burapacheep, Yixuan Li |  |
| 1942 |  |  [Compositional Preference Models for Aligning LMs](https://openreview.net/forum?id=tiiAzqi6Ol) |  | 0 |  | Dongyoung Go, Tomasz Korbak, Germán Kruszewski, Jos Rozen, Marc Dymetman |  |
| 1943 |  |  [Label-free Node Classification on Graphs with Large Language Models (LLMs)](https://openreview.net/forum?id=hESD2NJFg8) |  | 0 |  | Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, Jiliang Tang |  |
| 1944 |  |  [Sliced Wasserstein Estimation with Control Variates](https://openreview.net/forum?id=StYc4hQAEi) |  | 0 |  | Khai Nguyen, Nhat Ho |  |
| 1945 |  |  [Neural Rate Control for Learned Video Compression](https://openreview.net/forum?id=42lcaojZug) |  | 0 |  | Yiwei Zhang, Guo Lu, Yunuo Chen, Shen Wang, Yibo Shi, Jing Wang, Li Song |  |
| 1946 |  |  [Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts](https://openreview.net/forum?id=aZH1dM3GOX) |  | 0 |  | Ahmed Hendawy, Jan Peters, Carlo D'Eramo |  |
| 1947 |  |  [Local Composite Saddle Point Optimization](https://openreview.net/forum?id=kklwv4c4dI) |  | 0 |  | Site Bai, Brian Bullins |  |
| 1948 |  |  [Towards Poisoning Fair Representations](https://openreview.net/forum?id=YLJs4mKJCF) |  | 0 |  | Tianci Liu, Haoyu Wang, Feijie Wu, Hengtong Zhang, Pan Li, Lu Su, Jing Gao |  |
| 1949 |  |  [Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing](https://openreview.net/forum?id=nFMS6wF2xq) |  | 0 |  | Ling Yang, Zhilong Zhang, Zhaochen Yu, Jingwei Liu, Minkai Xu, Stefano Ermon, Bin Cui |  |
| 1950 |  |  [Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK Approach](https://openreview.net/forum?id=1op5YGZu8X) |  | 0 |  | Shaopeng Fu, Di Wang |  |
| 1951 |  |  [What's in a Prior? Learned Proximal Networks for Inverse Problems](https://openreview.net/forum?id=kNPcOaqC5r) |  | 0 |  | Zhenghan Fang, Sam Buchanan, Jeremias Sulam |  |
| 1952 |  |  [Fantastic Generalization Measures are Nowhere to be Found](https://openreview.net/forum?id=NkmJotfL42) |  | 0 |  | Michael Gastpar, Ido Nachum, Jonathan Shafer, Thomas Weinberger |  |
| 1953 |  |  [Towards Enhancing Time Series Contrastive Learning: A Dynamic Bad Pair Mining Approach](https://openreview.net/forum?id=K2c04ulKXn) |  | 0 |  | Xiang Lan, Hanshu Yan, Shenda Hong, Mengling Feng |  |
| 1954 |  |  [Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework](https://openreview.net/forum?id=fUGhVYPVRM) |  | 0 |  | Eliya Segev, Maya Alroy, Ronen Katsir, Noam Wies, Ayana Shenhav, Yael BenOren, David Zar, Oren Tadmor, Jacob Bitterman, Amnon Shashua, Tal Rosenwein |  |
| 1955 |  |  [Decoding Natural Images from EEG for Object Recognition](https://openreview.net/forum?id=dhLIno8FmH) |  | 0 |  | Yonghao Song, Bingchuan Liu, Xiang Li, Nanlin Shi, Yijun Wang, Xiaorong Gao |  |
| 1956 |  |  [LCOT: Linear Circular Optimal Transport](https://openreview.net/forum?id=49z97Y9lMq) |  | 0 |  | Rocio Diaz Martin, Ivan Vladimir Medri, Yikun Bai, Xinran Liu, Kangbai Yan, Gustavo K. Rohde, Soheil Kolouri |  |
| 1957 |  |  [Unveiling the Pitfalls of Knowledge Editing for Large Language Models](https://openreview.net/forum?id=fNktD3ib16) |  | 0 |  | Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen |  |
| 1958 |  |  [InstructPix2NeRF: Instructed 3D Portrait Editing from a Single Image](https://openreview.net/forum?id=XIxhINXtQk) |  | 0 |  | Jianhui Li, Shilong Liu, Zidong Liu, Yikai Wang, Kaiwen Zheng, Jinghui Xu, Jianmin Li, Jun Zhu |  |
| 1959 |  |  [Decongestion by Representation: Learning to Improve Economic Welfare in Marketplaces](https://openreview.net/forum?id=coIaBY8EVF) |  | 0 |  | Omer Nahum, Gali Noti, David C. Parkes, Nir Rosenfeld |  |
| 1960 |  |  [DiffEnc: Variational Diffusion with a Learned Encoder](https://openreview.net/forum?id=8nxy1bQWTG) |  | 0 |  | Beatrix Miranda Ginn Nielsen, Anders Christensen, Andrea Dittadi, Ole Winther |  |
| 1961 |  |  [Amortized Network Intervention to Steer the Excitatory Point Processes](https://openreview.net/forum?id=8g26Yv1EOu) |  | 0 |  | Zitao Song, Wendi Ren, Shuang Li |  |
| 1962 |  |  [Fast and unified path gradient estimators for normalizing flows](https://openreview.net/forum?id=zlkXLb3wpF) |  | 0 |  | Lorenz Vaitl, Ludwig Winkler, Lorenz Richter, Pan Kessel |  |
| 1963 |  |  [BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation](https://openreview.net/forum?id=gC6JTEU3jl) |  | 0 |  | Peng Xu, Wenqi Shao, Mengzhao Chen, Shitao Tang, Kaipeng Zhang, Peng Gao, Fengwei An, Yu Qiao, Ping Luo |  |
| 1964 |  |  [Learning invariant representations of time-homogeneous stochastic dynamical systems](https://openreview.net/forum?id=twSnZwiOIm) |  | 0 |  | Vladimir R. Kostic, Pietro Novelli, Riccardo Grazzi, Karim Lounici, Massimiliano Pontil |  |
| 1965 |  |  [Learning Nash Equilibria in Rank-1 Games](https://openreview.net/forum?id=8utTlmhw8v) |  | 0 |  | Nikolas Patris, Ioannis Panageas |  |
| 1966 |  |  [EControl: Fast Distributed Optimization with Compression and Error Control](https://openreview.net/forum?id=lsvlvWB9vz) |  | 0 |  | Yuan Gao, Rustem Islamov, Sebastian U. Stich |  |
| 1967 |  |  [State Representation Learning Using an Unbalanced Atlas](https://openreview.net/forum?id=cWdAYDLmPa) |  | 0 |  | Li Meng, Morten Goodwin, Anis Yazidi, Paal E. Engelstad |  |
| 1968 |  |  [Retro-fallback: retrosynthetic planning in an uncertain world](https://openreview.net/forum?id=dl0u4ODCuW) |  | 0 |  | Austin Tripp, Krzysztof Maziarz, Sarah Lewis, Marwin H. S. Segler, José Miguel HernándezLobato |  |
| 1969 |  |  [ACRF: Compressing Explicit Neural Radiance Fields via Attribute Compression](https://openreview.net/forum?id=POFrdKvpea) |  | 0 |  | Guangchi Fang, Qingyong Hu, Longguang Wang, Yulan Guo |  |
| 1970 |  |  [LiDAR-PTQ: Post-Training Quantization for Point Cloud 3D Object Detection](https://openreview.net/forum?id=0d1gQI114C) |  | 0 |  | Sifan Zhou, Liang Li, Xinyu Zhang, Bo Zhang, Shipeng Bai, Miao Sun, Ziyu Zhao, Xiaobo Lu, Xiangxiang Chu |  |
| 1971 |  |  [Optimal transport based adversarial patch to leverage large scale attack transferability](https://openreview.net/forum?id=nZP10evtkV) |  | 0 |  | Pol Labarbarie, Adrien ChanHonTong, Stéphane Herbin, Milad LeyliAbadi |  |
| 1972 |  |  [Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance Accompaniment](https://openreview.net/forum?id=GW4j4n2cjH) |  | 0 |  | Li Siyao, Tianpei Gu, Zhitao Yang, Zhengyu Lin, Ziwei Liu, Henghui Ding, Lei Yang, Chen Change Loy |  |
| 1973 |  |  [STREAM: Spatio-TempoRal Evaluation and Analysis Metric for Video Generative Models](https://openreview.net/forum?id=7JfKCZQPxJ) |  | 0 |  | Pum Jun Kim, Seojun Kim, Jaejun Yoo |  |
| 1974 |  |  [Accelerated Sampling with Stacked Restricted Boltzmann Machines](https://openreview.net/forum?id=kXNJ48Hvw1) |  | 0 |  | Jorge FernandezdeCossíoDiaz, Clément Roussel, Simona Cocco, Rémi Monasson |  |
| 1975 |  |  [Image Inpainting via Tractable Steering of Diffusion Models](https://openreview.net/forum?id=NSIVHTbZBR) |  | 0 |  | Anji Liu, Mathias Niepert, Guy Van den Broeck |  |
| 1976 |  |  [QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models](https://openreview.net/forum?id=WvFoJccpo8) |  | 0 |  | Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhengsu Chen, Xiaopeng Zhang, Qi Tian |  |
| 1977 |  |  [R-MAE: Regions Meet Masked Autoencoders](https://openreview.net/forum?id=ba84RDHFnz) |  | 0 |  | DuyKien Nguyen, Yanghao Li, Vaibhav Aggarwal, Martin R. Oswald, Alexander Kirillov, Cees G. M. Snoek, Xinlei Chen |  |
| 1978 |  |  [Transformer-Modulated Diffusion Models for Probabilistic Multivariate Time Series Forecasting](https://openreview.net/forum?id=qae04YACHs) |  | 0 |  | Yuxin Li, Wenchao Chen, Xinyue Hu, Bo Chen, Baolin Sun, Mingyuan Zhou |  |
| 1979 |  |  [OpenChat: Advancing Open-source Language Models with Mixed-Quality Data](https://openreview.net/forum?id=AOJyfhWYHf) |  | 0 |  | Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, Yang Liu |  |
| 1980 |  |  [In-Context Learning Learns Label Relationships but Is Not Conventional Learning](https://openreview.net/forum?id=YPIA7bgd5y) |  | 0 |  | Jannik Kossen, Yarin Gal, Tom Rainforth |  |
| 1981 |  |  [Human Motion Diffusion as a Generative Prior](https://openreview.net/forum?id=dTpbEdN9kr) |  | 0 |  | Yoni Shafir, Guy Tevet, Roy Kapon, Amit Haim Bermano |  |
| 1982 |  |  [New Insight of Variance reduce in Zero-Order Hard-Thresholding: Mitigating Gradient Error and Expansivity Contradictions](https://openreview.net/forum?id=fjf3YenThE) |  | 0 |  | Xinzhe Yuan, William de Vazelhes, Bin Gu, Huan Xiong |  |
| 1983 |  |  [AdaMerging: Adaptive Model Merging for Multi-Task Learning](https://openreview.net/forum?id=nZP6NgD3QY) |  | 0 |  | Enneng Yang, Zhenyi Wang, Li Shen, Shiwei Liu, Guibing Guo, Xingwei Wang, Dacheng Tao |  |
| 1984 |  |  [Self-Supervised High Dynamic Range Imaging with Multi-Exposure Images in Dynamic Scenes](https://openreview.net/forum?id=jjiOHEcS2c) |  | 0 |  | Zhilu Zhang, Haoyu Wang, Shuai Liu, Xiaotao Wang, Lei Lei, Wangmeng Zuo |  |
| 1985 |  |  [MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use](https://openreview.net/forum?id=R0c2qtalgG) |  | 0 |  | Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, Lichao Sun |  |
| 1986 |  |  [Composed Image Retrieval with Text Feedback via Multi-grained Uncertainty Regularization](https://openreview.net/forum?id=Yb5KvPkKQg) |  | 0 |  | Yiyang Chen, Zhedong Zheng, Wei Ji, Leigang Qu, TatSeng Chua |  |
| 1987 |  |  [Mean Field Theory in Deep Metric Learning](https://openreview.net/forum?id=ZPdZLlNXSm) |  | 0 |  | Takuya Furusawa |  |
| 1988 |  |  [Implicit Neural Representation Inference for Low-Dimensional Bayesian Deep Learning](https://openreview.net/forum?id=5KUiMKRebi) |  | 0 |  | Panagiotis Dimitrakopoulos, Giorgos Sfikas, Christophoros Nikou |  |
| 1989 |  |  [Going Beyond Neural Network Feature Similarity: The Network Feature Complexity and Its Interpretation Using Category Theory](https://openreview.net/forum?id=4bSQ3lsfEV) |  | 0 |  | Yiting Chen, Zhanpeng Zhou, Junchi Yan |  |
| 1990 |  |  [M3C: A Framework towards Convergent, Flexible, and Unsupervised Learning of Mixture Graph Matching and Clustering](https://openreview.net/forum?id=AXC9KydyZq) |  | 0 |  | Jiaxin Lu, Zetian Jiang, Tianzhe Wang, Junchi Yan |  |
| 1991 |  |  [AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models](https://openreview.net/forum?id=y33lDRBgWI) |  | 0 |  | Jiachun Pan, Jun Hao Liew, Vincent Y. F. Tan, Jiashi Feng, Hanshu Yan |  |
| 1992 |  |  [LLaMA-Adapter: Efficient Fine-tuning of Large Language Models with Zero-initialized Attention](https://openreview.net/forum?id=d4UiXAHN2W) |  | 0 |  | Renrui Zhang, Jiaming Han, Chris Liu, Aojun Zhou, Pan Lu, Yu Qiao, Hongsheng Li, Peng Gao |  |
| 1993 |  |  [Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation](https://openreview.net/forum?id=UbxWjq0UO2) |  | 0 |  | Junyoung Seo, Wooseok Jang, Minseop Kwak, Inès Hyeonsu Kim, Jaehoon Ko, Junho Kim, JinHwa Kim, Jiyoung Lee, Seungryong Kim |  |
| 1994 |  |  [SparseFormer: Sparse Visual Recognition via Limited Latent Tokens](https://openreview.net/forum?id=2pvECsmld3) |  | 0 |  | Ziteng Gao, Zhan Tong, Limin Wang, Mike Zheng Shou |  |
| 1995 |  |  [Demystifying Local & Global Fairness Trade-offs in Federated Learning Using Partial Information Decomposition](https://openreview.net/forum?id=SBj2Qdhgew) |  | 0 |  | Faisal Hamman, Sanghamitra Dutta |  |
| 1996 |  |  [Protein-Ligand Interaction Prior for Binding-aware 3D Molecule Diffusion Models](https://openreview.net/forum?id=qH9nrMNTIW) |  | 0 |  | Zhilin Huang, Ling Yang, Xiangxin Zhou, Zhilong Zhang, Wentao Zhang, Xiawu Zheng, Jie Chen, Yu Wang, Bin Cui, Wenming Yang |  |
| 1997 |  |  [Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph](https://openreview.net/forum?id=nnVO1PvbTv) |  | 0 |  | Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Lionel M. Ni, HeungYeung Shum, Jian Guo |  |
| 1998 |  |  [Self-Supervised Heterogeneous Graph Learning: a Homophily and Heterogeneity View](https://openreview.net/forum?id=3FJOKjooIj) |  | 0 |  | Yujie Mo, Feiping Nie, Ping Hu, Heng Tao Shen, Zheng Zhang, Xinchao Wang, Xiaofeng Zhu |  |
| 1999 |  |  [CoBIT: A Contrastive Bi-directional Image-Text Generation Model](https://openreview.net/forum?id=8ISRqgtjPc) |  | 0 |  | Haoxuan You, Mandy Guo, Zhecan Wang, KaiWei Chang, Jason M. Baldridge, Jiahui Yu |  |
| 2000 |  |  [Protein Multimer Structure Prediction via Prompt Learning](https://openreview.net/forum?id=OHpvivXrQr) |  | 0 |  | Ziqi Gao, Xiangguo Sun, Zijing Liu, Yu Li, Hong Cheng, Jia Li |  |
| 2001 |  |  [Domain-Agnostic Molecular Generation with Chemical Feedback](https://openreview.net/forum?id=9rPyHyjfwP) |  | 0 |  | Yin Fang, Ningyu Zhang, Zhuo Chen, Lingbing Guo, Xiaohui Fan, Huajun Chen |  |
| 2002 |  |  [LLM-grounded Video Diffusion Models](https://openreview.net/forum?id=exKHibougU) |  | 0 |  | Long Lian, Baifeng Shi, Adam Yala, Trevor Darrell, Boyi Li |  |
| 2003 |  |  [Periodicity Decoupling Framework for Long-term Series Forecasting](https://openreview.net/forum?id=dp27P5HBBt) |  | 0 |  | Tao Dai, Beiliang Wu, Peiyuan Liu, Naiqi Li, Jigang Bao, Yong Jiang, ShuTao Xia |  |
| 2004 |  |  [Imitation Learning from Observation with Automatic Discount Scheduling](https://openreview.net/forum?id=pPJTQYOpNI) |  | 0 |  | Yuyang Liu, Weijun Dong, Yingdong Hu, Chuan Wen, ZhaoHeng Yin, Chongjie Zhang, Yang Gao |  |
| 2005 |  |  [iGraphMix: Input Graph Mixup Method for Node Classification](https://openreview.net/forum?id=a2ljjXeDcE) |  | 0 |  | Jongwon Jeong, Hoyeop Lee, Hyui Geon Yoon, Beomyoung Lee, Junhee Heo, Geonsoo Kim, Kim Jin Seon |  |
| 2006 |  |  [Noise Map Guidance: Inversion with Spatial Context for Real Image Editing](https://openreview.net/forum?id=mhgm0IXtHw) |  | 0 |  | Hansam Cho, Jonghyun Lee, Seoung Bum Kim, TaeHyun Oh, Yonghyun Jeong |  |
| 2007 |  |  [Label-Focused Inductive Bias over Latent Object Features in Visual Classification](https://openreview.net/forum?id=cH3oufN8Pl) |  | 0 |  | Ilmin Kang, HyounYoung Bae, Kangil Kim |  |
| 2008 |  |  [Simple Minimax Optimal Byzantine Robust Algorithm for Nonconvex Objectives with Uniform Gradient Heterogeneity](https://openreview.net/forum?id=1ii8idH4tH) |  | 0 |  | Tomoya Murata, Kenta Niwa, Takumi Fukami, Iifan Tyou |  |
| 2009 |  |  [TopoMLP: A Simple yet Strong Pipeline for Driving Topology Reasoning](https://openreview.net/forum?id=0gTW5JUFTW) |  | 0 |  | Dongming Wu, Jiahao Chang, Fan Jia, Yingfei Liu, Tiancai Wang, Jianbing Shen |  |
| 2010 |  |  [Personalize Segment Anything Model with One Shot](https://openreview.net/forum?id=6Gzkhoc6YS) |  | 0 |  | Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junting Pan, Hao Dong, Yu Qiao, Peng Gao, Hongsheng Li |  |
| 2011 |  |  [Integrating Planning and Deep Reinforcement Learning via Automatic Induction of Task Substructures](https://openreview.net/forum?id=PR6RMsxuW7) |  | 0 |  | JungChun Liu, ChiHsien Chang, ShaoHua Sun, TianLi Yu |  |
| 2012 |  |  [PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training](https://openreview.net/forum?id=3Z1gxuAQrA) |  | 0 |  | Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, Sujian Li |  |
| 2013 |  |  [LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents](https://openreview.net/forum?id=ADSxCpCu9s) |  | 0 |  | Jaewoo Choi, Youngwoo Yoon, Hyobin Ong, Jaehong Kim, Minsu Jang |  |
| 2014 |  |  [Progressive3D: Progressively Local Editing for Text-to-3D Content Creation with Complex Semantic Prompts](https://openreview.net/forum?id=O072Rc8uUy) |  | 0 |  | Xinhua Cheng, Tianyu Yang, Jianan Wang, Yu Li, Lei Zhang, Jian Zhang, Li Yuan |  |
| 2015 |  |  [The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning](https://openreview.net/forum?id=wxJ0eXwwda) |  | 0 |  | Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Raghavi Chandu, Chandra Bhagavatula, Yejin Choi |  |
| 2016 |  |  [Towards Best Practices of Activation Patching in Language Models: Metrics and Methods](https://openreview.net/forum?id=Hf17y6u9BC) |  | 0 |  | Fred Zhang, Neel Nanda |  |
| 2017 |  |  [On the Analysis of GAN-based Image-to-Image Translation with Gaussian Noise Injection](https://openreview.net/forum?id=sLregLuXpn) |  | 0 |  | Chaohua Shi, Kexin Huang, Lu Gan, Hongqing Liu, Mingrui Zhu, Nannan Wang, Xinbo Gao |  |
| 2018 |  |  [FedHyper: A Universal and Robust Learning Rate Scheduler for Federated Learning with Hypergradient Descent](https://openreview.net/forum?id=Kl9CqKf7h6) |  | 0 |  | Ziyao Wang, Jianyu Wang, Ang Li |  |
| 2019 |  |  [FreeReg: Image-to-Point Cloud Registration Leveraging Pretrained Diffusion Models and Monocular Depth Estimators](https://openreview.net/forum?id=BPb5AhT2Vf) |  | 0 |  | Haiping Wang, Yuan Liu, Bing Wang, Yujing Sun, Zhen Dong, Wenping Wang, Bisheng Yang |  |
| 2020 |  |  [Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models](https://openreview.net/forum?id=3bq3jsvcQ1) |  | 0 |  | Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, HengTze Cheng, Ed H. Chi, Quoc V. Le, Denny Zhou |  |
| 2021 |  |  [ImagenHub: Standardizing the evaluation of conditional image generation models](https://openreview.net/forum?id=OuV9ZrkQlc) |  | 0 |  | Max Ku, Tianle Li, Kai Zhang, Yujie Lu, Xingyu Fu, Wenwen Zhuang, Wenhu Chen |  |
| 2022 |  |  [UC-NERF: Neural Radiance Field for Under-Calibrated Multi-View Cameras in Autonomous Driving](https://openreview.net/forum?id=bLKcCe7hYh) |  | 0 |  | Kai Cheng, Xiaoxiao Long, Wei Yin, Jin Wang, Zhiqiang Wu, Yuexin Ma, Kaixuan Wang, Xiaozhi Chen, Xuejin Chen |  |
| 2023 |  |  [Adapting Large Language Models via Reading Comprehension](https://openreview.net/forum?id=y886UXPEZ0) |  | 0 |  | Daixuan Cheng, Shaohan Huang, Furu Wei |  |
| 2024 |  |  [DIAGNOSIS: Detecting Unauthorized Data Usages in Text-to-image Diffusion Models](https://openreview.net/forum?id=f8S3aLm0Vp) |  | 0 |  | Zhenting Wang, Chen Chen, Lingjuan Lyu, Dimitris N. Metaxas, Shiqing Ma |  |
| 2025 |  |  [LEMON: Lossless model expansion](https://openreview.net/forum?id=3Vw7DQqq7U) |  | 0 |  | Yite Wang, Jiahao Su, Hanlin Lu, Cong Xie, Tianyi Liu, Jianbo Yuan, Haibin Lin, Ruoyu Sun, Hongxia Yang |  |
| 2026 |  |  [A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation](https://openreview.net/forum?id=Js5PJPHDyY) |  | 0 |  | Zhengbo Wang, Jian Liang, Lijun Sheng, Ran He, Zilei Wang, Tieniu Tan |  |
| 2027 |  |  [MiniLLM: Knowledge Distillation of Large Language Models](https://openreview.net/forum?id=5h0qf7IBZZ) |  | 0 |  | Yuxian Gu, Li Dong, Furu Wei, Minlie Huang |  |
| 2028 |  |  [Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation](https://openreview.net/forum?id=Vja3ecieXY) |  | 0 |  | Kai Huang, Hanyun Yin, Heng Huang, Wei Gao |  |
| 2029 |  |  [The importance of feature preprocessing for differentially private linear optimization](https://openreview.net/forum?id=XlTDBZFXWp) |  | 0 |  | Ziteng Sun, Ananda Theertha Suresh, Aditya Krishna Menon |  |
| 2030 |  |  [Pathformer: Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting](https://openreview.net/forum?id=lJkOCMP2aW) |  | 0 |  | Peng Chen, Yingying Zhang, Yunyao Cheng, Yang Shu, Yihang Wang, Qingsong Wen, Bin Yang, Chenjuan Guo |  |
| 2031 |  |  [Tree Cross Attention](https://openreview.net/forum?id=Vw24wtSddM) |  | 0 |  | Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Yoshua Bengio, Mohamed Osama Ahmed |  |
| 2032 |  |  [LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models](https://openreview.net/forum?id=gLARhFLE0F) |  | 0 |  | Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon Kim, Beomseok Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, Dongsoo Lee |  |
| 2033 |  |  [Outliers with Opposing Signals Have an Outsized Effect on Neural Network Optimization](https://openreview.net/forum?id=kIZ3S3tel6) |  | 0 |  | Elan Rosenfeld, Andrej Risteski |  |
| 2034 |  |  [Stable Anisotropic Regularization](https://openreview.net/forum?id=dbQH9AOVd5) |  | 0 |  | William Rudman, Carsten Eickhoff |  |
| 2035 |  |  [Threshold-Consistent Margin Loss for Open-World Deep Metric Learning](https://openreview.net/forum?id=vE5MyzpP92) |  | 0 |  | Qin Zhang, Linghan Xu, Jun Fang, Qingming Tang, Ying Nian Wu, Joseph Tighe, Yifan Xing |  |
| 2036 |  |  [Jointly Training Large Autoregressive Multimodal Models](https://openreview.net/forum?id=5jcav5RcKw) |  | 0 |  | Emanuele Aiello, Lili Yu, Yixin Nie, Armen Aghajanyan, Barlas Oguz |  |
| 2037 |  |  [Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in RL](https://openreview.net/forum?id=pDCublKPmG) |  | 0 |  | Xiangyu Liu, Souradip Chakraborty, Yanchao Sun, Furong Huang |  |
| 2038 |  |  [On the Over-Memorization During Natural, Robust and Catastrophic Overfitting](https://openreview.net/forum?id=2V1Z0Jdmss) |  | 0 |  | Runqi Lin, Chaojian Yu, Bo Han, Tongliang Liu |  |
| 2039 |  |  [The Generative AI Paradox: "What It Can Create, It May Not Understand"](https://openreview.net/forum?id=CF8H8MS5P8) |  | 0 |  | Peter West, Ximing Lu, Nouha Dziri, Faeze Brahman, Linjie Li, Jena D. Hwang, Liwei Jiang, Jillian Fisher, Abhilasha Ravichander, Khyathi Raghavi Chandu, Benjamin Newman, Pang Wei Koh, Allyson Ettinger, Yejin Choi |  |
| 2040 |  |  [Semantic Flow: Learning Semantic Fields of Dynamic Scenes from Monocular Videos](https://openreview.net/forum?id=A2mRcRyGdl) |  | 0 |  | Fengrui Tian, Yueqi Duan, Angtian Wang, Jianfei Guo, Shaoyi Du |  |
| 2041 |  |  [Revisiting Link Prediction: a data perspective](https://openreview.net/forum?id=8Ur2xmuw7w) |  | 0 |  | Haitao Mao, Juanhui Li, Harry Shomer, Bingheng Li, Wenqi Fan, Yao Ma, Tong Zhao, Neil Shah, Jiliang Tang |  |
| 2042 |  |  [Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding](https://openreview.net/forum?id=4L0xnS4GQM) |  | 0 |  | Zilong Wang, Hao Zhang, ChunLiang Li, Julian Martin Eisenschlos, Vincent Perot, Zifeng Wang, Lesly Miculicich, Yasuhisa Fujii, Jingbo Shang, ChenYu Lee, Tomas Pfister |  |
| 2043 |  |  [Denoising Diffusion Bridge Models](https://openreview.net/forum?id=FKksTayvGo) |  | 0 |  | Linqi Zhou, Aaron Lou, Samar Khanna, Stefano Ermon |  |
| 2044 |  |  [Incremental Randomized Smoothing Certification](https://openreview.net/forum?id=SdeAPV1irk) |  | 0 |  | Shubham Ugare, Tarun Suresh, Debangshu Banerjee, Gagandeep Singh, Sasa Misailovic |  |
| 2045 |  |  [Local Graph Clustering with Noisy Labels](https://openreview.net/forum?id=89A5c6enfc) |  | 0 |  | Artur Back de Luca, Kimon Fountoulakis, Shenghao Yang |  |
| 2046 |  |  [Principled Federated Domain Adaptation: Gradient Projection and Auto-Weighting](https://openreview.net/forum?id=6J3ehSUrMU) |  | 0 |  | Enyi Jiang, Yibo Jacky Zhang, Sanmi Koyejo |  |
| 2047 |  |  [GraphPulse: Topological representations for temporal graph property prediction](https://openreview.net/forum?id=DZqic2sPTY) |  | 0 |  | Kiarash Shamsi, Farimah Poursafaei, Shenyang Huang, Tran Gia Bao Ngo, Baris Coskunuzer, Cuneyt Gurcan Akcora |  |
| 2048 |  |  [Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs](https://openreview.net/forum?id=xZDWO0oejD) |  | 0 |  | Qingru Zhang, Chandan Singh, Liyuan Liu, Xiaodong Liu, Bin Yu, Jianfeng Gao, Tuo Zhao |  |
| 2049 |  |  [PRIME: Prioritizing Interpretability in Failure Mode Extraction](https://openreview.net/forum?id=QrEHs9w5UF) |  | 0 |  | Keivan Rezaei, Mehrdad Saberi, Mazda Moayeri, Soheil Feizi |  |
| 2050 |  |  [On gauge freedom, conservativity and intrinsic dimensionality estimation in diffusion models](https://openreview.net/forum?id=92KV9xAMhF) |  | 0 |  | Christian Horvat, JeanPascal Pfister |  |
| 2051 |  |  [Domain constraints improve risk prediction when outcome data is missing](https://openreview.net/forum?id=1mNFsbvo2P) |  | 0 |  | Sidhika Balachandar, Nikhil Garg, Emma Pierson |  |
| 2052 |  |  [Learning Multi-Agent Communication with Contrastive Learning](https://openreview.net/forum?id=vZZ4hhniJU) |  | 0 |  | Yat Long Lo, Biswa Sengupta, Jakob Nicolaus Foerster, Michael Noukhovitch |  |
| 2053 |  |  [Closing the Gap between TD Learning and Supervised Learning - A Generalisation Point of View](https://openreview.net/forum?id=qg5JENs0N4) |  | 0 |  | Raj Ghugare, Matthieu Geist, Glen Berseth, Benjamin Eysenbach |  |
| 2054 |  |  [lpNTK: Better Generalisation with Less Data via Sample Interaction During Learning](https://openreview.net/forum?id=8Ju0VmvMCW) |  | 0 |  | Shangmin Guo, Yi Ren, Stefano V. Albrecht, Kenny Smith |  |
| 2055 |  |  [Continual Learning on a Diet: Learning from Sparsely Labeled Streams Under Constrained Computation](https://openreview.net/forum?id=Xvfz8NHmCj) |  | 0 |  | Wenxuan Zhang, Youssef Mohamed, Bernard Ghanem, Philip Torr, Adel Bibi, Mohamed Elhoseiny |  |
| 2056 |  |  [Video Decomposition Prior: Editing Videos Layer by Layer](https://openreview.net/forum?id=nfMyERXNru) |  | 0 |  | Gaurav Shrivastava, SerNam Lim, Abhinav Shrivastava |  |
| 2057 |  |  [Beyond task performance: evaluating and reducing the flaws of large multimodal models with in-context-learning](https://openreview.net/forum?id=mMaQvkMzDi) |  | 0 |  | Mustafa Shukor, Alexandre Ramé, Corentin Dancette, Matthieu Cord |  |
| 2058 |  |  [Butterfly Effects of SGD Noise: Error Amplification in Behavior Cloning and Autoregression](https://openreview.net/forum?id=CgPs04l9TO) |  | 0 |  | Adam Block, Dylan J. Foster, Akshay Krishnamurthy, Max Simchowitz, Cyril Zhang |  |
| 2059 |  |  [On Stationary Point Convergence of PPO-Clip](https://openreview.net/forum?id=uznKlCpWjV) |  | 0 |  | Ruinan Jin, Shuai Li, Baoxiang Wang |  |
| 2060 |  |  [Automatic Functional Differentiation in JAX](https://openreview.net/forum?id=gzT61ziSCu) |  | 0 |  | Min Lin |  |
| 2061 |  |  [FedCompass: Efficient Cross-Silo Federated Learning on Heterogeneous Client Devices Using a Computing Power-Aware Scheduler](https://openreview.net/forum?id=msXxrttLOi) |  | 0 |  | Zilinghan Li, Pranshu Chaturvedi, Shilan He, Han Chen, Gagandeep Singh, Volodymyr V. Kindratenko, Eliu A. Huerta, Kibaek Kim, Ravi K. Madduri |  |
| 2062 |  |  [ADOPD: A Large-Scale Document Page Decomposition Dataset](https://openreview.net/forum?id=x1ptaXpOYa) |  | 0 |  | Jiuxiang Gu, Xiangxi Shi, Jason Kuen, Lu Qi, Ruiyi Zhang, Anqi Liu, Ani Nenkova, Tong Sun |  |
| 2063 |  |  [Provably Efficient CVaR RL in Low-rank MDPs](https://openreview.net/forum?id=9x6yrFAPnx) |  | 0 |  | Yulai Zhao, Wenhao Zhan, Xiaoyan Hu, Hofung Leung, Farzan Farnia, Wen Sun, Jason D. Lee |  |
| 2064 |  |  [COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically for Model-Based RL](https://openreview.net/forum?id=jnFcKjtUPN) |  | 0 |  | Xiyao Wang, Ruijie Zheng, Yanchao Sun, Ruonan Jia, Wichayaporn Wongkamjan, Huazhe Xu, Furong Huang |  |
| 2065 |  |  [Can Transformers Capture Spatial Relations between Objects?](https://openreview.net/forum?id=HgZUcwFhjr) |  | 0 |  | Chuan Wen, Dinesh Jayaraman, Yang Gao |  |
| 2066 |  |  [Plug-and-Play Posterior Sampling under Mismatched Measurement and Prior Models](https://openreview.net/forum?id=66arKkGiFy) |  | 0 |  | Marien Renaud, Jiaming Liu, Valentin De Bortoli, Andrés Almansa, Ulugbek Kamilov |  |
| 2067 |  |  [Learning Robust Generalizable Radiance Field with Visibility and Feature Augmented Point Representation](https://openreview.net/forum?id=o4CLLlIaaH) |  | 0 |  | Jiaxu Wang, Ziyi Zhang, Renjing Xu |  |
| 2068 |  |  [Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation](https://openreview.net/forum?id=wfzXa8e783) |  | 0 |  | ShihYing Yeh, YuGuan Hsieh, Zhidong Gao, Bernard B. W. Yang, Giyeong Oh, Yanmin Gong |  |
| 2069 |  |  [Finite Scalar Quantization: VQ-VAE Made Simple](https://openreview.net/forum?id=8ishA3LxN8) |  | 0 |  | Fabian Mentzer, David Minnen, Eirikur Agustsson, Michael Tschannen |  |
| 2070 |  |  [Interpretable Meta-Learning of Physical Systems](https://openreview.net/forum?id=nnicaG5xiH) |  | 0 |  | Matthieu Blanke, Marc Lelarge |  |
| 2071 |  |  [Grokking in Linear Estimators - A Solvable Model that Groks without Understanding](https://openreview.net/forum?id=GH2LYb9XV0) |  | 0 |  | Noam Levi, Alon Beck, Yohai BarSinai |  |
| 2072 |  |  [Discovering Failure Modes of Text-guided Diffusion Models via Adversarial Search](https://openreview.net/forum?id=TOWdQQgMJY) |  | 0 |  | Qihao Liu, Adam Kortylewski, Yutong Bai, Song Bai, Alan L. Yuille |  |
| 2073 |  |  [DiffAR: Denoising Diffusion Autoregressive Model for Raw Speech Waveform Generation](https://openreview.net/forum?id=GTk0AdOYLq) |  | 0 |  | Roi Benita, Michael Elad, Joseph Keshet |  |
| 2074 |  |  [Statistical Rejection Sampling Improves Preference Optimization](https://openreview.net/forum?id=xbjSwwrQOe) |  | 0 |  | Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J. Liu, Jialu Liu |  |
| 2075 |  |  [On the generalization capacity of neural networks during generic multimodal reasoning](https://openreview.net/forum?id=zyBJodMrn5) |  | 0 |  | Takuya Ito, Soham Dan, Mattia Rigotti, James R. Kozloski, Murray Campbell |  |
| 2076 |  |  [The Devil is in the Object Boundary: Towards Annotation-free Instance Segmentation using Foundation Models](https://openreview.net/forum?id=4JbrdrHxYy) |  | 0 |  | Cheng Shi, Sibei Yang |  |
| 2077 |  |  [Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages](https://openreview.net/forum?id=zzqn5G9fjn) |  | 0 |  | Wanru Zhao, Yihong Chen, Royson Lee, Xinchi Qiu, Yan Gao, Hongxiang Fan, Nicholas Donald Lane |  |
| 2078 |  |  [A Data-Driven Measure of Relative Uncertainty for Misclassification Detection](https://openreview.net/forum?id=ruGY8v10mK) |  | 0 |  | Eduardo Dadalto Câmara Gomes, Marco Romanelli, Georg Pichler, Pablo Piantanida |  |
| 2079 |  |  [Most discriminative stimuli for functional cell type clustering](https://openreview.net/forum?id=9W6KaAcYlr) |  | 0 |  | Max F. Burg, Thomas Zenkel, Michaela Vystrcilová, Jonathan Oesterle, Larissa Höfling, Konstantin F. Willeke, Jan Lause, Sarah Müller, Paul G. Fahey, Zhiwei Ding, Kelli Restivo, Shashwat Sridhar, Tim Gollisch, Philipp Berens, Andreas S. Tolias, Thomas Euler, Matthias Bethge, Alexander S. Ecker |  |
| 2080 |  |  [Biased Temporal Convolution Graph Network for Time Series Forecasting with Missing Values](https://openreview.net/forum?id=O9nZCwdGcG) |  | 0 |  | Xiaodan Chen, Xiucheng Li, Bo Liu, Zhijun Li |  |
| 2081 |  |  [Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models](https://openreview.net/forum?id=JzG7kSpjJk) |  | 0 |  | Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, Dongsoo Lee |  |
| 2082 |  |  [DecompOpt: Controllable and Decomposed Diffusion Models for Structure-based Molecular Optimization](https://openreview.net/forum?id=Y3BbxvAQS9) |  | 0 |  | Xiangxin Zhou, Xiwei Cheng, Yuwei Yang, Yu Bao, Liang Wang, Quanquan Gu |  |
| 2083 |  |  [Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals](https://openreview.net/forum?id=UMfcdRIotC) |  | 0 |  | Yair Ori Gat, Nitay Calderon, Amir Feder, Alexander Chapanin, Amit Sharma, Roi Reichart |  |
| 2084 |  |  [Separating common from salient patterns with Contrastive Representation Learning](https://openreview.net/forum?id=30N3bNAiw3) |  | 0 |  | Robin Louiset, Edouard Duchesnay, Antoine Grigis, Pietro Gori |  |
| 2085 |  |  [Self-Supervised Contrastive Learning for Long-term Forecasting](https://openreview.net/forum?id=nBCuRzjqK7) |  | 0 |  | Junwoo Park, Daehoon Gwak, Jaegul Choo, Edward Choi |  |
| 2086 |  |  [A Semantic Invariant Robust Watermark for Large Language Models](https://openreview.net/forum?id=6p8lpe4MNf) |  | 0 |  | Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, Lijie Wen |  |
| 2087 |  |  [Fast Equilibrium of SGD in Generic Situations](https://openreview.net/forum?id=qgWJkDiI5p) |  | 0 |  | Zhiyuan Liu, Yi Wang, Zhiren Wang |  |
| 2088 |  |  [Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM](https://openreview.net/forum?id=izrOLJov5y) |  | 0 |  | Eliya Nachmani, Alon Levkovitch, Roy Hirsch, Julian Salazar, Chulayuth Asawaroengchai, Soroosh Mariooryad, Ehud Rivlin, R. J. SkerryRyan, Michelle Tadmor Ramanovich |  |
| 2089 |  |  [Transport meets Variational Inference: Controlled Monte Carlo Diffusions](https://openreview.net/forum?id=PP1rudnxiW) |  | 0 |  | Francisco Vargas, Shreyas Padhy, Denis Blessing, Nikolas Nüsken |  |
| 2090 |  |  [DAFA: Distance-Aware Fair Adversarial Training](https://openreview.net/forum?id=BRdEBlwUW6) |  | 0 |  | Hyungyu Lee, Saehyung Lee, Hyemi Jang, Junsung Park, Ho Bae, Sungroh Yoon |  |
| 2091 |  |  [AffineQuant: Affine Transformation Quantization for Large Language Models](https://openreview.net/forum?id=of2rhALq8l) |  | 0 |  | Yuexiao Ma, Huixia Li, Xiawu Zheng, Feng Ling, Xuefeng Xiao, Rui Wang, Shilei Wen, Fei Chao, Rongrong Ji |  |
| 2092 |  |  [Encoding Unitig-level Assembly Graphs with Heterophilous Constraints for Metagenomic Contigs Binning](https://openreview.net/forum?id=vBw8JGBJWj) |  | 0 |  | Hansheng Xue, Vijini Mallawaarachchi, Lexing Xie, Vaibhav Rajan |  |
| 2093 |  |  [SF(DA)2: Source-free Domain Adaptation Through the Lens of Data Augmentation](https://openreview.net/forum?id=kUCgHbmO11) |  | 0 |  | Uiwon Hwang, Jonghyun Lee, Juhyeon Shin, Sungroh Yoon |  |
| 2094 |  |  [Mitigating the Curse of Dimensionality for Certified Robustness via Dual Randomized Smoothing](https://openreview.net/forum?id=C1sQBG6Sqp) |  | 0 |  | Song Xia, Yi Yu, Xudong Jiang, Henghui Ding |  |
| 2095 |  |  [In-context Exploration-Exploitation for Reinforcement Learning](https://openreview.net/forum?id=uIKZSStON3) |  | 0 |  | Zhenwen Dai, Federico Tomasi, Sina Ghiassian |  |
| 2096 |  |  [Out-of-Distribution Detection with Negative Prompts](https://openreview.net/forum?id=nanyAujl6e) |  | 0 |  | Jun Nie, Yonggang Zhang, Zhen Fang, Tongliang Liu, Bo Han, Xinmei Tian |  |
| 2097 |  |  [π2vec: Policy Representation with Successor Features](https://openreview.net/forum?id=o5Bqa4o5Mi) |  | 0 |  | Gianluca Scarpellini, Ksenia Konyushkova, Claudio Fantacci, Thomas Paine, Yutian Chen, Misha Denil |  |
| 2098 |  |  [Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition](https://openreview.net/forum?id=TVg6hlfsKa) |  | 0 |  | Feng Lu, Lijun Zhang, Xiangyuan Lan, Shuting Dong, Yaowei Wang, Chun Yuan |  |
| 2099 |  |  [FROSTER: Frozen CLIP is A Strong Teacher for Open-Vocabulary Action Recognition](https://openreview.net/forum?id=zYXFMeHRtO) |  | 0 |  | Xiaohu Huang, Hao Zhou, Kun Yao, Kai Han |  |
| 2100 |  |  [Interventional Fairness on Partially Known Causal Graphs: A Constrained Optimization Approach](https://openreview.net/forum?id=SKulT2VX9p) |  | 0 |  | Aoqi Zuo, Yiqing Li, Susan Wei, Mingming Gong |  |
| 2101 |  |  [The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World](https://openreview.net/forum?id=c2R7ajodcI) |  | 0 |  | Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhenhang Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu, Zhiguo Cao, Yushi Chen, Tong Lu, Jifeng Dai, Yu Qiao |  |
| 2102 |  |  [CIFAR-10-Warehouse: Broad and More Realistic Testbeds in Model Generalization Analysis](https://openreview.net/forum?id=pw2ssoOTpo) |  | 0 |  | Xiaoxiao Sun, Xingjian Leng, Zijian Wang, Yang Yang, Zi Huang, Liang Zheng |  |
| 2103 |  |  [Task Planning for Visual Room Rearrangement under Partial Observability](https://openreview.net/forum?id=jJvXNpvOdM) |  | 0 |  | Karan Mirakhor, Sourav Ghosh, Dipanjan Das, Brojeshwar Bhowmick |  |
| 2104 |  |  [Parallelizing non-linear sequential models over the sequence length](https://openreview.net/forum?id=E34AlVLN0v) |  | 0 |  | Yi Heng Lim, Qi Zhu, Joshua Selfridge, Muhammad Firmansyah Kasim |  |
| 2105 |  |  [Long-tailed Diffusion Models with Oriented Calibration](https://openreview.net/forum?id=NW2s5XXwXU) |  | 0 |  | Tianjiao Zhang, Huangjie Zheng, Jiangchao Yao, Xiangfeng Wang, Mingyuan Zhou, Ya Zhang, Yanfeng Wang |  |
| 2106 |  |  [A Simple Romance Between Multi-Exit Vision Transformer and Token Reduction](https://openreview.net/forum?id=gJeYtRuguR) |  | 0 |  | Dongyang Liu, Meina Kan, Shiguang Shan, Xilin Chen |  |
| 2107 |  |  [Optimal Sample Complexity for Average Reward Markov Decision Processes](https://openreview.net/forum?id=jOm5p3q7c7) |  | 0 |  | Shengbo Wang, José H. Blanchet, Peter W. Glynn |  |
| 2108 |  |  [Spatio-Temporal Approximation: A Training-Free SNN Conversion for Transformers](https://openreview.net/forum?id=XrunSYwoLr) |  | 0 |  | Yizhou Jiang, Kunlin Hu, Tianren Zhang, Haichuan Gao, Yuqian Liu, Ying Fang, Feng Chen |  |
| 2109 |  |  [NfgTransformer: Equivariant Representation Learning for Normal-form Games](https://openreview.net/forum?id=4YESQqIys7) |  | 0 |  | Siqi Liu, Luke Marris, Georgios Piliouras, Ian Gemp, Nicolas Heess |  |
| 2110 |  |  [#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models](https://openreview.net/forum?id=pszewhybU9) |  | 0 |  | Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, Jingren Zhou |  |
| 2111 |  |  [When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations](https://openreview.net/forum?id=JewzobRhay) |  | 0 |  | Aleksandar Petrov, Philip Torr, Adel Bibi |  |
| 2112 |  |  [Understanding In-Context Learning from Repetitions](https://openreview.net/forum?id=bGGYcvw8mp) |  | 0 |  | Jianhao Yan, Jin Xu, Chiyu Song, Chenming Wu, Yafu Li, Yue Zhang |  |
| 2113 |  |  [Analysis of Learning a Flow-based Generative Model from Limited Sample Complexity](https://openreview.net/forum?id=ndCJeysCPe) |  | 0 |  | Hugo Cui, Florent Krzakala, Eric VandenEijnden, Lenka Zdeborová |  |
| 2114 |  |  [Few-shot Hybrid Domain Adaptation of Image Generator](https://openreview.net/forum?id=FE2e8664Sl) |  | 0 |  | Hengjia Li, Yang Liu, Linxuan Xia, Yuqi Lin, Wenxiao Wang, Tu Zheng, Zheng Yang, Xiaohui Zhong, Xiaobo Ren, Xiaofei He |  |
| 2115 |  |  [Rethinking Information-theoretic Generalization: Loss Entropy Induced PAC Bounds](https://openreview.net/forum?id=GWSIo2MzuH) |  | 0 |  | Yuxin Dong, Tieliang Gong, Hong Chen, Shujian Yu, Chen Li |  |
| 2116 |  |  [Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via Selective Entropy Distillation](https://openreview.net/forum?id=vePdNU3u6n) |  | 0 |  | Yaofo Chen, Shuaicheng Niu, Yaowei Wang, Shoukai Xu, Hengjie Song, Mingkui Tan |  |
| 2117 |  |  [KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval](https://openreview.net/forum?id=b3kDP3IytM) |  | 0 |  | Marah I Abdin, Suriya Gunasekar, Varun Chandrasekaran, Jerry Li, Mert Yüksekgönül, Rahee Ghosh Peshawaria, Ranjita Naik, Besmira Nushi |  |
| 2118 |  |  [Boosting Graph Anomaly Detection with Adaptive Message Passing](https://openreview.net/forum?id=CanomFZssu) |  | 0 |  | Jingyan Chen, Guanghui Zhu, Chunfeng Yuan, Yihua Huang |  |
| 2119 |  |  [MINDE: Mutual Information Neural Diffusion Estimation](https://openreview.net/forum?id=0kWd8SJq8d) |  | 0 |  | Giulio Franzese, Mustapha Bounoua, Pietro Michiardi |  |
| 2120 |  |  [Continual Momentum Filtering on Parameter Space for Online Test-time Adaptation](https://openreview.net/forum?id=BllUWdpIOA) |  | 0 |  | JaeHong Lee, JoonHyuk Chang |  |
| 2121 |  |  [Deep Reinforcement Learning for Modelling Protein Complexes](https://openreview.net/forum?id=4MsfQ2H0lP) |  | 0 |  | Ziqi Gao, Tao Feng, Jiaxuan You, Chenyi Zi, Yan Zhou, Chen Zhang, Jia Li |  |
| 2122 |  |  [fairret: a Framework for Differentiable Fairness Regularization Terms](https://openreview.net/forum?id=NnyD0Rjx2B) |  | 0 |  | Maarten Buyl, MaryBeth Defrance, Tijl De Bie |  |
| 2123 |  |  [Debiasing Algorithm through Model Adaptation](https://openreview.net/forum?id=XIZEFyVGC9) |  | 0 |  | Tomasz Limisiewicz, David Marecek, Tomás Musil |  |
| 2124 |  |  [A Foundation Model for Error Correction Codes](https://openreview.net/forum?id=7KDuQPrAF3) |  | 0 |  | Yoni Choukroun, Lior Wolf |  |
| 2125 |  |  [Seer: Language Instructed Video Prediction with Latent Diffusion Models](https://openreview.net/forum?id=qHGgNyQk31) |  | 0 |  | Xianfan Gu, Chuan Wen, Weirui Ye, Jiaming Song, Yang Gao |  |
| 2126 |  |  [Matrix Manifold Neural Networks++](https://openreview.net/forum?id=30aSE3FB3L) |  | 0 |  | Xuan Son Nguyen, Shuo Yang, Aymeric Histace |  |
| 2127 |  |  [Emo: Earth Mover Distance Optimization for Auto-Regressive Language Modeling](https://openreview.net/forum?id=4bLXfRd0CX) |  | 0 |  | Siyu Ren, Zhiyong Wu, Kenny Q. Zhu |  |
| 2128 |  |  [Are Human-generated Demonstrations Necessary for In-context Learning?](https://openreview.net/forum?id=frRDT6EOhg) |  | 0 |  | Rui Li, Guoyin Wang, Jiwei Li |  |
| 2129 |  |  [LLM-Assisted Code Cleaning For Training Accurate Code Generators](https://openreview.net/forum?id=maRYffiUpI) |  | 0 |  | Naman Jain, Tianjun Zhang, WeiLin Chiang, Joseph E. Gonzalez, Koushik Sen, Ion Stoica |  |
| 2130 |  |  [HYPO: Hyperspherical Out-Of-Distribution Generalization](https://openreview.net/forum?id=VXak3CZZGC) |  | 0 |  | Haoyue Bai, Yifei Ming, Julian KatzSamuels, Yixuan Li |  |
| 2131 |  |  [Analyzing and Improving Optimal-Transport-based Adversarial Networks](https://openreview.net/forum?id=jODehvtTDx) |  | 0 |  | Jaemoo Choi, Jaewoong Choi, Myungjoo Kang |  |
| 2132 |  |  [SEABO: A Simple Search-Based Method for Offline Imitation Learning](https://openreview.net/forum?id=MNyOI3C7YB) |  | 0 |  | Jiafei Lyu, Xiaoteng Ma, Le Wan, Runze Liu, Xiu Li, Zongqing Lu |  |
| 2133 |  |  [Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs](https://openreview.net/forum?id=1ndDmZdT4g) |  | 0 |  | Yuxin Zhang, Lirui Zhao, Mingbao Lin, Yunyun Sun, Yiwu Yao, Xingjia Han, Jared Tanner, Shiwei Liu, Rongrong Ji |  |
| 2134 |  |  [Simplifying Transformer Blocks](https://openreview.net/forum?id=RtDok9eS3s) |  | 0 |  | Bobby He, Thomas Hofmann |  |
| 2135 |  |  [Aux-NAS: Exploiting Auxiliary Labels with Negligibly Extra Inference Cost](https://openreview.net/forum?id=cINwAhrgLf) |  | 0 |  | Yuan Gao, Weizhong Zhang, Wenhan Luo, Lin Ma, JinGang Yu, GuiSong Xia, Jiayi Ma |  |
| 2136 |  |  [EX-Graph: A Pioneering Dataset Bridging Ethereum and X](https://openreview.net/forum?id=juE0rWGCJW) |  | 0 |  | Qian Wang, Zhen Zhang, Zemin Liu, Shengliang Lu, Bingqiao Luo, Bingsheng He |  |
| 2137 |  |  [Enhancing One-Shot Federated Learning Through Data and Ensemble Co-Boosting](https://openreview.net/forum?id=tm8s3696Ox) |  | 0 |  | Rong Dai, Yonggang Zhang, Ang Li, Tongliang Liu, Xun Yang, Bo Han |  |
| 2138 |  |  [Symbol as Points: Panoptic Symbol Spotting via Point-based Representation](https://openreview.net/forum?id=aOnUe8ah7j) |  | 0 |  | Wenlong Liu, Tianyu Yang, Yuhan Wang, Qizhi Yu, Lei Zhang |  |
| 2139 |  |  [HypeBoy: Generative Self-Supervised Representation Learning on Hypergraphs](https://openreview.net/forum?id=DZUzOKE6og) |  | 0 |  | Sunwoo Kim, Shinhwan Kang, Fanchen Bu, Soo Yong Lee, Jaemin Yoo, Kijung Shin |  |
| 2140 |  |  [Zero-Shot Robustification of Zero-Shot Models](https://openreview.net/forum?id=fCeUoDr9Tq) |  | 0 |  | Dyah Adila, Changho Shin, Linrong Cai, Frederic Sala |  |
| 2141 |  |  [Thought Propagation: an Analogical Approach to Complex Reasoning with Large Language Models](https://openreview.net/forum?id=SBoRhRCzM3) |  | 0 |  | Junchi Yu, Ran He, Zhitao Ying |  |
| 2142 |  |  [FreeDyG: Frequency Enhanced Continuous-Time Dynamic Graph Model for Link Prediction](https://openreview.net/forum?id=82Mc5ilInM) |  | 0 |  | Yuxing Tian, Yiyan Qi, Fan Guo |  |
| 2143 |  |  [DreamSmooth: Improving Model-based Reinforcement Learning via Reward Smoothing](https://openreview.net/forum?id=GruDNzQ4ux) |  | 0 |  | Vint Lee, Pieter Abbeel, Youngwoon Lee |  |
| 2144 |  |  [VONet: Unsupervised Video Object Learning With Parallel U-Net Attention and Object-wise Sequential VAE](https://openreview.net/forum?id=qCyhvr0GG8) |  | 0 |  | Haonan Yu, Wei Xu |  |
| 2145 |  |  [ReSimAD: Zero-Shot 3D Domain Transfer for Autonomous Driving with Source Reconstruction and Target Simulation](https://openreview.net/forum?id=1d2cLKeNgY) |  | 0 |  | Bo Zhang, Xinyu Cai, Jiakang Yuan, Donglin Yang, Jianfei Guo, Xiangchao Yan, Renqiu Xia, Botian Shi, Min Dou, Tao Chen, Si Liu, Junchi Yan, Yu Qiao |  |
| 2146 |  |  [Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE](https://openreview.net/forum?id=rTDyN8yajn) |  | 0 |  | Zeren Chen, Ziqin Wang, Zhen Wang, Huayang Liu, Zhenfei Yin, Si Liu, Lu Sheng, Wanli Ouyang, Jing Shao |  |
| 2147 |  |  [Motion Guidance: Diffusion-Based Image Editing with Differentiable Motion Estimators](https://openreview.net/forum?id=WIAO4vbnNV) |  | 0 |  | Daniel Geng, Andrew Owens |  |
| 2148 |  |  [Balancing Act: Constraining Disparate Impact in Sparse Models](https://openreview.net/forum?id=Xz13DtbOVW) |  | 0 |  | Meraj Hashemizadeh, Juan Ramirez, Rohan Sukumaran, Golnoosh Farnadi, Simon LacosteJulien, Jose GallegoPosada |  |
| 2149 |  |  [NECO: NEural Collapse Based Out-of-distribution detection](https://openreview.net/forum?id=9ROuKblmi7) |  | 0 |  | Mouïn Ben Ammar, Nacim Belkhir, Sebastian Popescu, Antoine Manzanera, Gianni Franchi |  |
| 2150 |  |  [LightHGNN: Distilling Hypergraph Neural Networks into MLPs for 100x Faster Inference](https://openreview.net/forum?id=lHasEfGsXL) |  | 0 |  | Yifan Feng, Yihe Luo, Shihui Ying, Yue Gao |  |
| 2151 |  |  [Lewis's Signaling Game as beta-VAE For Natural Word Lengths and Segments](https://openreview.net/forum?id=HC0msxE3sf) |  | 0 |  | Ryo Ueda, Tadahiro Taniguchi |  |
| 2152 |  |  [Domain-Inspired Sharpness-Aware Minimization Under Domain Shifts](https://openreview.net/forum?id=I4wB3HA3dJ) |  | 0 |  | Ruipeng Zhang, Ziqing Fan, Jiangchao Yao, Ya Zhang, Yanfeng Wang |  |
| 2153 |  |  [Making LLaMA SEE and Draw with SEED Tokenizer](https://openreview.net/forum?id=0Nui91LBQS) |  | 0 |  | Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, Ying Shan |  |
| 2154 |  |  [A Cognitive Model for Learning Abstract Relational Structures from Memory-based Decision-Making Tasks](https://openreview.net/forum?id=KC58bVmxyN) |  | 0 |  | Haruo Hosoya |  |
| 2155 |  |  [DynaVol: Unsupervised Learning for Dynamic Scenes through Object-Centric Voxelization](https://openreview.net/forum?id=koYsgfEwCQ) |  | 0 |  | Yanpeng Zhao, Siyu Gao, Yunbo Wang, Xiaokang Yang |  |
| 2156 |  |  [TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series](https://openreview.net/forum?id=Tuh4nZVb0g) |  | 0 |  | Chenxi Sun, Hongyan Li, Yaliang Li, Shenda Hong |  |
| 2157 |  |  [Sin3DM: Learning a Diffusion Model from a Single 3D Textured Shape](https://openreview.net/forum?id=U0IOMStUQ8) |  | 0 |  | Rundi Wu, Ruoshi Liu, Carl Vondrick, Changxi Zheng |  |
| 2158 |  |  [Pooling Image Datasets with Multiple Covariate Shift and Imbalance](https://openreview.net/forum?id=2Mo7v69otj) |  | 0 |  | Sotirios Panagiotis Chytas, Vishnu Suresh Lokhande, Vikas Singh |  |
| 2159 |  |  [On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes](https://openreview.net/forum?id=3zKtaqxLhW) |  | 0 |  | Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos Garea, Matthieu Geist, Olivier Bachem |  |
| 2160 |  |  [Adaptive Regularization of Representation Rank as an Implicit Constraint of Bellman Equation](https://openreview.net/forum?id=apXtolxDaJ) |  | 0 |  | Qiang He, Tianyi Zhou, Meng Fang, Setareh Maghsudi |  |
| 2161 |  |  [Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks](https://openreview.net/forum?id=ZL6yd6N1S2) |  | 0 |  | Puja Trivedi, Mark Heimann, Rushil Anirudh, Danai Koutra, Jayaraman J. Thiagarajan |  |
| 2162 |  |  [Symphony: Symmetry-Equivariant Point-Centered Spherical Harmonics for 3D Molecule Generation](https://openreview.net/forum?id=MIEnYtlGyv) |  | 0 |  | Ameya Daigavane, Song Kim, Mario Geiger, Tess E. Smidt |  |
| 2163 |  |  [Set Learning for Accurate and Calibrated Models](https://openreview.net/forum?id=HZ3S17EI0o) |  | 0 |  | Lukas Muttenthaler, Robert A. Vandermeulen, Qiuyi Zhang, Thomas Unterthiner, KlausRobert Müller |  |
| 2164 |  |  [INViTE: INterpret and Control Vision-Language Models with Text Explanations](https://openreview.net/forum?id=5iENGLEJKG) |  | 0 |  | Haozhe Chen, Junfeng Yang, Carl Vondrick, Chengzhi Mao |  |
| 2165 |  |  [Trajeglish: Traffic Modeling as Next-Token Prediction](https://openreview.net/forum?id=Z59Rb5bPPP) |  | 0 |  | Jonah Philion, Xue Bin Peng, Sanja Fidler |  |
| 2166 |  |  [Meaning Representations from Trajectories in Autoregressive Models](https://openreview.net/forum?id=UyGWafcopT) |  | 0 |  | Tian Yu Liu, Matthew Trager, Alessandro Achille, Pramuditha Perera, Luca Zancato, Stefano Soatto |  |
| 2167 |  |  [SWAP: Sparse Entropic Wasserstein Regression for Robust Network Pruning](https://openreview.net/forum?id=LJWizuuBUy) |  | 0 |  | Lei You, Hei Victor Cheng |  |
| 2168 |  |  [Circumventing Concept Erasure Methods For Text-To-Image Generative Models](https://openreview.net/forum?id=ag3o2T51Ht) |  | 0 |  | Minh Pham, Kelly O. Marshall, Niv Cohen, Govind Mittal, Chinmay Hegde |  |
| 2169 |  |  [Pose Modulated Avatars from Video](https://openreview.net/forum?id=5t44vPlv9x) |  | 0 |  | Chunjin Song, Bastian Wandt, Helge Rhodin |  |
| 2170 |  |  [The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images](https://openreview.net/forum?id=ixP76Y33y1) |  | 0 |  | Nicholas Konz, Maciej A. Mazurowski |  |
| 2171 |  |  [Complete and Efficient Graph Transformers for Crystal Material Property Prediction](https://openreview.net/forum?id=BnQY9XiRAS) |  | 0 |  | Keqiang Yan, Cong Fu, Xiaofeng Qian, Xiaoning Qian, Shuiwang Ji |  |
| 2172 |  |  [Patched Denoising Diffusion Models For High-Resolution Image Synthesis](https://openreview.net/forum?id=TgSRPRz8cI) |  | 0 |  | Zheng Ding, Mengqi Zhang, Jiajun Wu, Zhuowen Tu |  |
| 2173 |  |  [NOLA: Compressing LoRA using Linear Combination of Random Basis](https://openreview.net/forum?id=TjfXcDgvzk) |  | 0 |  | Soroush Abbasi Koohpayegani, Navaneet K. L., Parsa Nooralinejad, Soheil Kolouri, Hamed Pirsiavash |  |
| 2174 |  |  [Unveiling Options with Neural Network Decomposition](https://openreview.net/forum?id=a8VETFwcVR) |  | 0 |  | Mahdi Alikhasi, Levi Lelis |  |
| 2175 |  |  [HIFA: High-fidelity Text-to-3D Generation with Advanced Diffusion Guidance](https://openreview.net/forum?id=IZMPWmcS3H) |  | 0 |  | Junzhe Zhu, Peiye Zhuang, Sanmi Koyejo |  |
| 2176 |  |  [FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing](https://openreview.net/forum?id=JgqftqZQZ7) |  | 0 |  | Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, JuanManuel PérezRúa, Bodo Rosenhahn, Tao Xiang, Sen He |  |
| 2177 |  |  [How Does Unlabeled Data Provably Help Out-of-Distribution Detection?](https://openreview.net/forum?id=jlEjB8MVGa) |  | 0 |  | Xuefeng Du, Zhen Fang, Ilias Diakonikolas, Yixuan Li |  |
| 2178 |  |  [Delta-AI: Local objectives for amortized inference in sparse graphical models](https://openreview.net/forum?id=LemSSn8htt) |  | 0 |  | JeanPierre R. Falet, Hae Beom Lee, Nikolay Malkin, Chen Sun, Dragos Secrieru, Dinghuai Zhang, Guillaume Lajoie, Yoshua Bengio |  |
| 2179 |  |  [Learning Implicit Representation for Reconstructing Articulated Objects](https://openreview.net/forum?id=KQ2i6jazVK) |  | 0 |  | Hao Zhang, Fang Li, Samyak Rawlekar, Narendra Ahuja |  |
| 2180 |  |  [Improving protein optimization with smoothed fitness landscapes](https://openreview.net/forum?id=rxlF2Zv8x0) |  | 0 |  | Andrew Kirjner, Jason Yim, Raman Samusevich, Shahar Bracha, Tommi S. Jaakkola, Regina Barzilay, Ila R. Fiete |  |
| 2181 |  |  [Rethinking Label Poisoning for GNNs: Pitfalls and Attacks](https://openreview.net/forum?id=J7ioefqDPw) |  | 0 |  | Vijay Lingam, Mohammad Sadegh Akhondzadeh, Aleksandar Bojchevski |  |
| 2182 |  |  [Structural Estimation of Partially Observed Linear Non-Gaussian Acyclic Model: A Practical Approach with Identifiability](https://openreview.net/forum?id=nHkMm0ywWm) |  | 0 |  | Songyao Jin, Feng Xie, Guangyi Chen, Biwei Huang, Zhengming Chen, Xinshuai Dong, Kun Zhang |  |
| 2183 |  |  [Meta Inverse Constrained Reinforcement Learning: Convergence Guarantee and Generalization Analysis](https://openreview.net/forum?id=bJ3gFiwRgi) |  | 0 |  | Shicheng Liu, Minghui Zhu |  |
| 2184 |  |  [Cleanba: A Reproducible and Efficient Distributed Reinforcement Learning Platform](https://openreview.net/forum?id=Diq6urt3lS) |  | 0 |  | Shengyi Huang, Jiayi Weng, Rujikorn Charakorn, Min Lin, Zhongwen Xu, Santiago Ontañón |  |
| 2185 |  |  [Consciousness-Inspired Spatio-Temporal Abstractions for Better Generalization in Reinforcement Learning](https://openreview.net/forum?id=eo9dHwtTFt) |  | 0 |  | Harry Zhao, Safa Alver, Harm van Seijen, Romain Laroche, Doina Precup, Yoshua Bengio |  |
| 2186 |  |  [Bayesian Low-rank Adaptation for Large Language Models](https://openreview.net/forum?id=FJiUyzOF1m) |  | 0 |  | Adam X. Yang, Maxime Robeyns, Xi Wang, Laurence Aitchison |  |
| 2187 |  |  [Function-space Parameterization of Neural Networks for Sequential Learning](https://openreview.net/forum?id=2dhxxIKhqz) |  | 0 |  | Aidan Scannell, Riccardo Mereu, Paul Edmund Chang, Ella Tamir, Joni Pajarinen, Arno Solin |  |
| 2188 |  |  [Denevil: towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning](https://openreview.net/forum?id=m3RRWWFaVe) |  | 0 |  | Shitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, Ning Gu |  |
| 2189 |  |  [The Expressive Power of Transformers with Chain of Thought](https://openreview.net/forum?id=NjNGlPh8Wh) |  | 0 |  | William Merrill, Ashish Sabharwal |  |
| 2190 |  |  [When should we prefer Decision Transformers for Offline Reinforcement Learning?](https://openreview.net/forum?id=vpV7fOFQy4) |  | 0 |  | Prajjwal Bhargava, Rohan Chitnis, Alborz Geramifard, Shagun Sodhani, Amy Zhang |  |
| 2191 |  |  [ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate](https://openreview.net/forum?id=FQepisCUWu) |  | 0 |  | ChiMin Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu |  |
| 2192 |  |  [Bridging State and History Representations: Understanding Self-Predictive RL](https://openreview.net/forum?id=ms0VgzSGF2) |  | 0 |  | Tianwei Ni, Benjamin Eysenbach, Erfan Seyedsalehi, Michel Ma, Clement Gehring, Aditya Mahajan, PierreLuc Bacon |  |
| 2193 |  |  [TapMo: Shape-aware Motion Generation of Skeleton-free Characters](https://openreview.net/forum?id=OeH6Fdhv7q) |  | 0 |  | Jiaxu Zhang, Shaoli Huang, Zhigang Tu, Xin Chen, Xiaohang Zhan, Gang Yu, Ying Shan |  |
| 2194 |  |  [InstructDET: Diversifying Referring Object Detection with Generalized Instructions](https://openreview.net/forum?id=hss35aoQ1Y) |  | 0 |  | Ronghao Dang, Jiangyan Feng, Haodong Zhang, Chongjian Ge, Lin Song, Lijun Gong, Chengju Liu, Qijun Chen, Feng Zhu, Rui Zhao, Yibing Song |  |
| 2195 |  |  [RAIN: Your Language Models Can Align Themselves without Finetuning](https://openreview.net/forum?id=pETSfWMUzy) |  | 0 |  | Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, Hongyang Zhang |  |
| 2196 |  |  [PBADet: A One-Stage Anchor-Free Approach for Part-Body Association](https://openreview.net/forum?id=pPh9p8anUi) |  | 0 |  | Zhongpai Gao, Huayi Zhou, Abhishek Sharma, Meng Zheng, Benjamin Planche, Terrence Chen, Ziyan Wu |  |
| 2197 |  |  [Performance Gaps in Multi-view Clustering under the Nested Matrix-Tensor Model](https://openreview.net/forum?id=ILqA09Oeq2) |  | 0 |  | Hugo Lebeau, Mohamed El Amine Seddik, José Henrique de Morais Goulart |  |
| 2198 |  |  [Deep Reinforcement Learning Guided Improvement Heuristic for Job Shop Scheduling](https://openreview.net/forum?id=jsWCmrsHHs) |  | 0 |  | Cong Zhang, Zhiguang Cao, Wen Song, Yaoxin Wu, Jie Zhang |  |
| 2199 |  |  [Communication-Efficient Gradient Descent-Accent Methods for Distributed Variational Inequalities: Unified Analysis and Local Updates](https://openreview.net/forum?id=hORCalGn3Z) |  | 0 |  | Siqi Zhang, Sayantan Choudhury, Sebastian U. Stich, Nicolas Loizou |  |
| 2200 |  |  [Batch normalization is sufficient for universal function approximation in CNNs](https://openreview.net/forum?id=wOSYMHfENq) |  | 0 |  | Rebekka Burkholz |  |
| 2201 |  |  [Predicting Emergent Abilities with Infinite Resolution Evaluation](https://openreview.net/forum?id=lDbjooxLkD) |  | 0 |  | Shengding Hu, Xin Liu, Xu Han, Xinrong Zhang, Chaoqun He, Weilin Zhao, Yankai Lin, Ning Ding, Zebin Ou, Guoyang Zeng, Zhiyuan Liu, Maosong Sun |  |
| 2202 |  |  [Graph-constrained diffusion for End-to-End Path Planning](https://openreview.net/forum?id=vuK8MhVtuu) |  | 0 |  | Dingyuan Shi, Yongxin Tong, Zimu Zhou, Ke Xu, Zheng Wang, Jieping Ye |  |
| 2203 |  |  [Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control](https://openreview.net/forum?id=Pc8AU1aF5e) |  | 0 |  | Longtao Zheng, Rundong Wang, Xinrun Wang, Bo An |  |
| 2204 |  |  [Elastic Feature Consolidation For Cold Start Exemplar-Free Incremental Learning](https://openreview.net/forum?id=7D9X2cFnt1) |  | 0 |  | Simone Magistri, Tomaso Trinci, Albin SoutifCormerais, Joost van de Weijer, Andrew D. Bagdanov |  |
| 2205 |  |  [Adversarial Causal Bayesian Optimization](https://openreview.net/forum?id=YcW8i9VCf5) |  | 0 |  | Scott Sussex, Pier Giuseppe Sessa, Anastasia Makarova, Andreas Krause |  |
| 2206 |  |  [Text-to-3D with Classifier Score Distillation](https://openreview.net/forum?id=ktG8Tun1Cy) |  | 0 |  | Xin Yu, YuanChen Guo, Yangguang Li, Ding Liang, SongHai Zhang, Xiaojuan Qi |  |
| 2207 |  |  [Accurate Forgetting for Heterogeneous Federated Continual Learning](https://openreview.net/forum?id=ShQrnAsbPI) |  | 0 |  | Abudukelimu Wuerkaixi, Sen Cui, Jingfeng Zhang, Kunda Yan, Bo Han, Gang Niu, Lei Fang, Changshui Zhang, Masashi Sugiyama |  |
| 2208 |  |  [GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with Noisy Polarization Priors](https://openreview.net/forum?id=pTN8dV2pL8) |  | 0 |  | Li Yang, Ruizheng Wu, Jiyong Li, YingCong Chen |  |
| 2209 |  |  [Porf: Pose residual field for accurate Neural surface Reconstruction](https://openreview.net/forum?id=eBeECjacpw) |  | 0 |  | JiaWang Bian, Wenjing Bian, Victor Adrian Prisacariu, Philip Torr |  |
| 2210 |  |  [Modelling complex vector drawings with stroke-clouds](https://openreview.net/forum?id=O2jyuo89CK) |  | 0 |  | Alexander Ashcroft, Ayan Das, Yulia Gryaditskaya, Zhiyu Qu, YiZhe Song |  |
| 2211 |  |  [Spurious Feature Diversification Improves Out-of-distribution Generalization](https://openreview.net/forum?id=d6H4RBi7RH) |  | 0 |  | Yong Lin, Lu Tan, Yifan Hao, Honam Wong, Hanze Dong, Weizhong Zhang, Yujiu Yang, Tong Zhang |  |
| 2212 |  |  [Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image Diffusion Models](https://openreview.net/forum?id=28L2FCtMWq) |  | 0 |  | Hyeonho Jeong, Jong Chul Ye |  |
| 2213 |  |  [Scalable Language Model with Generalized Continual Learning](https://openreview.net/forum?id=mz8owj4DXu) |  | 0 |  | Bohao Peng, Zhuotao Tian, Shu Liu, MingChang Yang, Jiaya Jia |  |
| 2214 |  |  [Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios](https://openreview.net/forum?id=vRyp2dhEQp) |  | 0 |  | Ziqiang Li, Hong Sun, Pengfei Xia, Heng Li, Beihao Xia, Yi Wu, Bin Li |  |
| 2215 |  |  [Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics](https://openreview.net/forum?id=HKgRwNhI9R) |  | 0 |  | Rene Winchenbach, Nils Thuerey |  |
| 2216 |  |  [G2N2 : Weisfeiler and Lehman go grammatical](https://openreview.net/forum?id=eZneJ55mRO) |  | 0 |  | Jason Piquenot, Aldo Moscatelli, Maxime Berar, Pierre Héroux, Romain Raveaux, JeanYves Ramel, Sébastien Adam |  |
| 2217 |  |  [VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks](https://openreview.net/forum?id=glwwbaeKm2) |  | 0 |  | Zhaomin Wu, Junyi Hou, Bingsheng He |  |
| 2218 |  |  [Multimodal Web Navigation with Instruction-Finetuned Foundation Models](https://openreview.net/forum?id=efFmBWioSc) |  | 0 |  | Hiroki Furuta, KuangHuei Lee, Ofir Nachum, Yutaka Matsuo, Aleksandra Faust, Shixiang Shane Gu, Izzeddin Gur |  |
| 2219 |  |  [Real-Fake: Effective Training Data Synthesis Through Distribution Matching](https://openreview.net/forum?id=svIdLLZpsA) |  | 0 |  | Jianhao Yuan, Jie Zhang, Shuyang Sun, Philip Torr, Bo Zhao |  |
| 2220 |  |  [Learning Conditional Invariances through Non-Commutativity](https://openreview.net/forum?id=tUVG9nGzgE) |  | 0 |  | Abhra Chaudhuri, Serban Georgescu, Anjan Dutta |  |
| 2221 |  |  [GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher](https://openreview.net/forum?id=MbfAK4s61A) |  | 0 |  | Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jentse Huang, Pinjia He, Shuming Shi, Zhaopeng Tu |  |
| 2222 |  |  [Towards the Fundamental Limits of Knowledge Transfer over Finite Domains](https://openreview.net/forum?id=Zh2iqiOtMt) |  | 0 |  | Qingyue Zhao, Banghua Zhu |  |
| 2223 |  |  [Self-Supervised Speech Quality Estimation and Enhancement Using Only Clean Speech](https://openreview.net/forum?id=ale56Ya59q) |  | 0 |  | SzuWei Fu, KuoHsuan Hung, Yu Tsao, YuChiang Frank Wang |  |
| 2224 |  |  [Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning](https://openreview.net/forum?id=Cc0qk6r4Nd) |  | 0 |  | YunHin Chan, Rui Zhou, Running Zhao, Zhihan Jiang, Edith C. H. Ngai |  |
| 2225 |  |  [Contrastive Learning is Spectral Clustering on Similarity Graph](https://openreview.net/forum?id=hLZQTFGToA) |  | 0 |  | Zhiquan Tan, Yifan Zhang, Jingqin Yang, Yang Yuan |  |
| 2226 |  |  [On the Generalization and Approximation Capacities of Neural Controlled Differential Equations](https://openreview.net/forum?id=kILAd8RdzA) |  | 0 |  | Linus Bleistein, Agathe Guilloux |  |
| 2227 |  |  [Epitopological learning and Cannistraci-Hebb network shape intelligence brain-inspired theory for ultra-sparse advantage in deep learning](https://openreview.net/forum?id=iayEcORsGd) |  | 0 |  | Yingtao Zhang, Jialin Zhao, Wenjing Wu, Alessandro Muscoloni, Carlo Vittorio Cannistraci |  |
| 2228 |  |  [Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models](https://openreview.net/forum?id=Tr0lPx9woF) |  | 0 |  | Yingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou, Carlo Vittorio Cannistraci |  |
| 2229 |  |  [Universal Jailbreak Backdoors from Poisoned Human Feedback](https://openreview.net/forum?id=GxCGsxiAaK) |  | 0 |  | Javier Rando, Florian Tramèr |  |
| 2230 |  |  [Neural Field Classifiers via Target Encoding and Classification Loss](https://openreview.net/forum?id=9NqC72m31m) |  | 0 |  | Xindi Yang, Zeke Xie, Xiong Zhou, Boyu Liu, Buhua Liu, Yi Liu, Haoran Wang, Yunfeng Cai, Mingming Sun |  |
| 2231 |  |  [Fusion Is Not Enough: Single Modal Attacks on Fusion Models for 3D Object Detection](https://openreview.net/forum?id=3VD4PNEt5q) |  | 0 |  | Zhiyuan Cheng, Hongjun Choi, Shiwei Feng, James Chenhao Liang, Guanhong Tao, Dongfang Liu, Michael Zuzak, Xiangyu Zhang |  |
| 2232 |  |  [LipVoicer: Generating Speech from Silent Videos Guided by Lip Reading](https://openreview.net/forum?id=ZZCPSC5OgD) |  | 0 |  | Yochai Yemini, Aviv Shamsian, Lior Bracha, Sharon Gannot, Ethan Fetaya |  |
| 2233 |  |  [Window Attention is Bugged: How not to Interpolate Position Embeddings](https://openreview.net/forum?id=IPhm01y9a9) |  | 0 |  | Daniel Bolya, Chaitanya Ryali, Judy Hoffman, Christoph Feichtenhofer |  |
| 2234 |  |  [Embodied Active Defense: Leveraging Recurrent Feedback to Counter Adversarial Patches](https://openreview.net/forum?id=uXjfOmTiDt) |  | 0 |  | Lingxuan Wu, Xiao Yang, Yinpeng Dong, Liuwei Xie, Hang Su, Jun Zhu |  |
| 2235 |  |  [Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks](https://openreview.net/forum?id=1SbkubNdbW) |  | 0 |  | Lukas Struppek, Dominik Hintersdorf, Kristian Kersting |  |
| 2236 |  |  [Continuous Invariance Learning](https://openreview.net/forum?id=70IgE3tRbu) |  | 0 |  | Lin Yong, Fan Zhou, Lu Tan, Lintao Ma, Jianmeng Liu, Yansu He, Yuan Yuan, Yu Liu, James Y. Zhang, Yujiu Yang, Hao Wang |  |
| 2237 |  |  [ZipIt! Merging Models from Different Tasks without Training](https://openreview.net/forum?id=LEYUkvdUhq) |  | 0 |  | George Stoica, Daniel Bolya, Jakob Bjorner, Pratik Ramesh, Taylor Hearn, Judy Hoffman |  |
| 2238 |  |  [Hybrid Distillation: Connecting Masked Autoencoders with Contrastive Learners](https://openreview.net/forum?id=jUWktnsplU) |  | 0 |  | Bowen Shi, Xiaopeng Zhang, Yaoming Wang, Jin Li, Wenrui Dai, Junni Zou, Hongkai Xiong, Qi Tian |  |
| 2239 |  |  [Facing the Elephant in the Room: Visual Prompt Tuning or Full finetuning?](https://openreview.net/forum?id=bJx4iOIOxn) |  | 0 |  | Cheng Han, Qifan Wang, Yiming Cui, Wenguan Wang, Lifu Huang, Siyuan Qi, Dongfang Liu |  |
| 2240 |  |  [Grounding Multimodal Large Language Models to the World](https://openreview.net/forum?id=lLmqxkfSIw) |  | 0 |  | Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Qixiang Ye, Furu Wei |  |
| 2241 |  |  [VFLAIR: A Research Library and Benchmark for Vertical Federated Learning](https://openreview.net/forum?id=sqRgz88TM3) |  | 0 |  | Tianyuan Zou, Zixuan Gu, Yu He, Hideaki Takahashi, Yang Liu, YaQin Zhang |  |
| 2242 |  |  [IRAD: Implicit Representation-driven Image Resampling against Adversarial Attacks](https://openreview.net/forum?id=jFa5KESW65) |  | 0 |  | Yue Cao, Tianlin Li, Xiaofeng Cao, Ivor W. Tsang, Yang Liu, Qing Guo |  |
| 2243 |  |  [Stylized Offline Reinforcement Learning: Extracting Diverse High-Quality Behaviors from Heterogeneous Datasets](https://openreview.net/forum?id=rnHNDihrIT) |  | 0 |  | Yihuan Mao, Chengjie Wu, Xi Chen, Hao Hu, Ji Jiang, Tianze Zhou, Tangjie Lv, Changjie Fan, Zhipeng Hu, Yi Wu, Yujing Hu, Chongjie Zhang |  |
| 2244 |  |  [Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight](https://openreview.net/forum?id=1hsVvgW0rU) |  | 0 |  | Jiacheng Guo, Minshuo Chen, Huan Wang, Caiming Xiong, Mengdi Wang, Yu Bai |  |
| 2245 |  |  [Pre-training with Synthetic Data Helps Offline Reinforcement Learning](https://openreview.net/forum?id=PcxQgtHGj2) |  | 0 |  | Zecheng Wang, Che Wang, Zixuan Dong, Keith W. Ross |  |
| 2246 |  |  [AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors](https://openreview.net/forum?id=EHg5GDnyq1) |  | 0 |  | Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, ChiMin Chan, Heyang Yu, Yaxi Lu, YiHsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong Sun, Jie Zhou |  |
| 2247 |  |  [IceFormer: Accelerated Inference with Long-Sequence Transformers on CPUs](https://openreview.net/forum?id=6RR3wU4mSZ) |  | 0 |  | Yuzhen Mao, Martin Ester, Ke Li |  |
| 2248 |  |  [Efficient Planning with Latent Diffusion](https://openreview.net/forum?id=btpgDo4u4j) |  | 0 |  | Wenhao Li |  |
| 2249 |  |  [Conformal Prediction via Regression-as-Classification](https://openreview.net/forum?id=rulxyXjf46) |  | 0 |  | Etash Kumar Guha, Shlok Natarajan, Thomas Möllenhoff, Mohammad Emtiyaz Khan, Eugène Ndiaye |  |
| 2250 |  |  [Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model](https://openreview.net/forum?id=ezscMer8L0) |  | 0 |  | Zihan Zhong, Zhiqiang Tang, Tong He, Haoyang Fang, Chun Yuan |  |
| 2251 |  |  [Tag2Text: Guiding Vision-Language Model via Image Tagging](https://openreview.net/forum?id=x6u2BQ7xcq) |  | 0 |  | Xinyu Huang, Youcai Zhang, Jinyu Ma, Weiwei Tian, Rui Feng, Yuejie Zhang, Yaqian Li, Yandong Guo, Lei Zhang |  |
| 2252 |  |  [Class Incremental Learning via Likelihood Ratio Based Task Prediction](https://openreview.net/forum?id=8QfK9Dq4q0) |  | 0 |  | Haowei Lin, Yijia Shao, Weinan Qian, Ningxin Pan, Yiduo Guo, Bing Liu |  |
| 2253 |  |  [Decentralized Riemannian Conjugate Gradient Method on the Stiefel Manifold](https://openreview.net/forum?id=PQbFUMKLFp) |  | 0 |  | Jun Chen, Haishan Ye, Mengmeng Wang, Tianxin Huang, Guang Dai, Ivor W. Tsang, Yong Liu |  |
| 2254 |  |  [Improved Regret Bounds for Non-Convex Online-Within-Online Meta Learning](https://openreview.net/forum?id=pA8Q5WiEMg) |  | 0 |  | Jiechao Guan, Hui Xiong |  |
| 2255 |  |  [Momentum Benefits Non-iid Federated Learning Simply and Provably](https://openreview.net/forum?id=TdhkAcXkRi) |  | 0 |  | Ziheng Cheng, Xinmeng Huang, Pengfei Wu, Kun Yuan |  |
| 2256 |  |  [Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy](https://openreview.net/forum?id=3fEKavFsnv) |  | 0 |  | Shuhai Zhang, Yiliao Song, Jiahao Yang, Yuanqing Li, Bo Han, Mingkui Tan |  |
| 2257 |  |  [Only Pay for What Is Uncertain: Variance-Adaptive Thompson Sampling](https://openreview.net/forum?id=p8ujRTjEf3) |  | 0 |  | Aadirupa Saha, Branislav Kveton |  |
| 2258 |  |  [PnP Inversion: Boosting Diffusion-based Editing with 3 Lines of Code](https://openreview.net/forum?id=FoMZ4ljhVw) |  | 0 |  | Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, Qiang Xu |  |
| 2259 |  |  [A Benchmark Study on Calibration](https://openreview.net/forum?id=GzNhzX9kVa) |  | 0 |  | Linwei Tao, Younan Zhu, Haolan Guo, Minjing Dong, Chang Xu |  |
| 2260 |  |  [Enhancing Human-AI Collaboration Through Logic-Guided Reasoning](https://openreview.net/forum?id=TWC4gLoAxY) |  | 0 |  | Chengzhi Cao, Yinghao Fu, Sheng Xu, Ruimao Zhang, Shuang Li |  |
| 2261 |  |  [Learning with Mixture of Prototypes for Out-of-Distribution Detection](https://openreview.net/forum?id=uNkKaD3MCs) |  | 0 |  | Haodong Lu, Dong Gong, Shuo Wang, Jason Xue, Lina Yao, Kristen Moore |  |
| 2262 |  |  [PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks](https://openreview.net/forum?id=DO2WFXU1Be) |  | 0 |  | Leo Zhiyuan Zhao, Xueying Ding, B. Aditya Prakash |  |
| 2263 |  |  [Attention-Guided Contrastive Role Representations for Multi-agent Reinforcement Learning](https://openreview.net/forum?id=LWmuPfEYhH) |  | 0 |  | Zican Hu, Zongzhang Zhang, Huaxiong Li, Chunlin Chen, Hongyu Ding, Zhi Wang |  |
| 2264 |  |  [A Statistical Analysis of Wasserstein Autoencoders for Intrinsically Low-dimensional Data](https://openreview.net/forum?id=WjRPZsfeBO) |  | 0 |  | Saptarshi Chakraborty, Peter L. Bartlett |  |
| 2265 |  |  [Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models](https://openreview.net/forum?id=gfFVATffPd) |  | 0 |  | Mert Yüksekgönül, Varun Chandrasekaran, Erik Jones, Suriya Gunasekar, Ranjita Naik, Hamid Palangi, Ece Kamar, Besmira Nushi |  |
| 2266 |  |  [Structuring Representation Geometry with Rotationally Equivariant Contrastive Learning](https://openreview.net/forum?id=lgaFMvZHSJ) |  | 0 |  | Sharut Gupta, Joshua Robinson, Derek Lim, Soledad Villar, Stefanie Jegelka |  |
| 2267 |  |  [Decodable and Sample Invariant Continuous Object Encoder](https://openreview.net/forum?id=QLoepRnoue) |  | 0 |  | Dehao Yuan, Furong Huang, Cornelia Fermüller, Yiannis Aloimonos |  |
| 2268 |  |  [Neural Snowflakes: Universal Latent Graph Inference via Trainable Latent Geometries](https://openreview.net/forum?id=djM3WzpOmK) |  | 0 |  | Haitz Sáez de Ocáriz Borde, Anastasis Kratsios |  |
| 2269 |  |  [Context is Environment](https://openreview.net/forum?id=8VPWfqtQMX) |  | 0 |  | Sharut Gupta, Stefanie Jegelka, David LopezPaz, Kartik Ahuja |  |
| 2270 |  |  [Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions](https://openreview.net/forum?id=RLSWbk9kPw) |  | 0 |  | Jungtaek Kim, Jeongbeen Yoon, Minsu Cho |  |
| 2271 |  |  [IMPUS: Image Morphing with Perceptually-Uniform Sampling Using Diffusion Models](https://openreview.net/forum?id=gG38EBe2S8) |  | 0 |  | Zhaoyuan Yang, Zhengyang Yu, Zhiwei Xu, Jaskirat Singh, Jing Zhang, Dylan Campbell, Peter H. Tu, Richard Hartley |  |
| 2272 |  |  [Adaptive Instrument Design for Indirect Experiments](https://openreview.net/forum?id=4Zz5UELkIt) |  | 0 |  | Yash Chandak, Shiv Shankar, Vasilis Syrgkanis, Emma Brunskill |  |
| 2273 |  |  [Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning](https://openreview.net/forum?id=HiYMiZYwkw) |  | 0 |  | Johnathan Xie, Yoonho Lee, Annie S. Chen, Chelsea Finn |  |
| 2274 |  |  [Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL](https://openreview.net/forum?id=N6o0ZtPzTg) |  | 0 |  | Hao Sun, Alihan Hüyük, Mihaela van der Schaar |  |
| 2275 |  |  [BioBridge: Bridging Biomedical Foundation Models via Knowledge Graphs](https://openreview.net/forum?id=jJCeMiwHdH) |  | 0 |  | Zifeng Wang, Zichen Wang, Balasubramaniam Srinivasan, Vassilis N. Ioannidis, Huzefa Rangwala, Rishita Anubhai |  |
| 2276 |  |  [Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning](https://openreview.net/forum?id=1jbh2e0b2K) |  | 0 |  | Zhuoyan Xu, Zhenmei Shi, Junyi Wei, Fangzhou Mu, Yin Li, Yingyu Liang |  |
| 2277 |  |  [ReTaSA: A Nonparametric Functional Estimation Approach for Addressing Continuous Target Shift](https://openreview.net/forum?id=KdVvOA00Or) |  | 0 |  | Hwanwoo Kim, Xin Zhang, Jiwei Zhao, Qinglong Tian |  |
| 2278 |  |  [GPAvatar: Generalizable and Precise Head Avatar from Image(s)](https://openreview.net/forum?id=hgehGq2bDv) |  | 0 |  | Xuangeng Chu, Yu Li, Ailing Zeng, Tianyu Yang, Lijian Lin, Yunfei Liu, Tatsuya Harada |  |
| 2279 |  |  [Mind Your Augmentation: The Key to Decoupling Dense Self-Supervised Learning](https://openreview.net/forum?id=WQYHbr36Fo) |  | 0 |  | Congpei Qiu, Tong Zhang, Yanhao Wu, Wei Ke, Mathieu Salzmann, Sabine Süsstrunk |  |
| 2280 |  |  [Entropy-MCMC: Sampling from Flat Basins with Ease](https://openreview.net/forum?id=oGNdBvymod) |  | 0 |  | Bolian Li, Ruqi Zhang |  |
| 2281 |  |  [Xformer: Hybrid X-Shaped Transformer for Image Denoising](https://openreview.net/forum?id=vXrIQLzIKY) |  | 0 |  | Jiale Zhang, Yulun Zhang, Jinjin Gu, Jiahua Dong, Linghe Kong, Xiaokang Yang |  |
| 2282 |  |  [Learning to Embed Time Series Patches Independently](https://openreview.net/forum?id=WS7GuBDFa2) |  | 0 |  | Seunghan Lee, Taeyoung Park, Kibok Lee |  |
| 2283 |  |  [Understanding Convergence and Generalization in Federated Learning through Feature Learning Theory](https://openreview.net/forum?id=EcetCr4trp) |  | 0 |  | Wei Huang, Ye Shi, Zhongyi Cai, Taiji Suzuki |  |
| 2284 |  |  [Quantifying the Sensitivity of Inverse Reinforcement Learning to Misspecification](https://openreview.net/forum?id=pz2E1Q9Wni) |  | 0 |  | Joar Max Viktor Skalse, Alessandro Abate |  |
| 2285 |  |  [Rethinking CNN's Generalization to Backdoor Attack from Frequency Domain](https://openreview.net/forum?id=mYhH0CDFFa) |  | 0 |  | Quanrui Rao, Lin Wang, Wuying Liu |  |
| 2286 |  |  [LLCP: Learning Latent Causal Processes for Reasoning-based Video Question Answer](https://openreview.net/forum?id=Cu5wJa5LGO) |  | 0 |  | Guangyi Chen, Yuke Li, Xiao Liu, Zijian Li, Eman Al Suradi, Donglai Wei, Kun Zhang |  |
| 2287 |  |  [PromptTTS 2: Describing and Generating Voices with Text Prompt](https://openreview.net/forum?id=NsCXDyv2Bn) |  | 0 |  | Yichong Leng, Zhifang Guo, Kai Shen, Zeqian Ju, Xu Tan, Eric Liu, Yufei Liu, Dongchao Yang, Leying Zhang, Kaitao Song, Lei He, Xiangyang Li, Sheng Zhao, Tao Qin, Jiang Bian |  |
| 2288 |  |  [RTFS-Net: Recurrent Time-Frequency Modelling for Efficient Audio-Visual Speech Separation](https://openreview.net/forum?id=PEuDO2EiDr) |  | 0 |  | Samuel Pegg, Kai Li, Xiaolin Hu |  |
| 2289 |  |  [Consistent Video-to-Video Transfer Using Synthetic Dataset](https://openreview.net/forum?id=IoKRezZMxF) |  | 0 |  | Jiaxin Cheng, Tianjun Xiao, Tong He |  |
| 2290 |  |  [Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a Bayesian Game](https://openreview.net/forum?id=z6KS9D1dxt) |  | 0 |  | Simin Li, Jun Guo, Jingqiao Xiu, Ruixiao Xu, Xin Yu, Jiakai Wang, Aishan Liu, Yaodong Yang, Xianglong Liu |  |
| 2291 |  |  [Scale-Adaptive Diffusion Model for Complex Sketch Synthesis](https://openreview.net/forum?id=5xadJmgwix) |  | 0 |  | Jijin Hu, Ke Li, Yonggang Qi, YiZhe Song |  |
| 2292 |  |  [Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature](https://openreview.net/forum?id=Bpcgcr8E8Z) |  | 0 |  | Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, Yue Zhang |  |
| 2293 |  |  [Defining and extracting generalizable interaction primitives from DNNs](https://openreview.net/forum?id=OCqyFVFNeF) |  | 0 |  | Lu Chen, Siyu Lou, Benhao Huang, Quanshi Zhang |  |
| 2294 |  |  [Revisiting Deep Audio-Text Retrieval Through the Lens of Transportation](https://openreview.net/forum?id=l60EM8md3t) |  | 0 |  | Manh Luong, Khai Nguyen, Nhat Ho, Gholamreza Haffari, Dinh Phung, Lizhen Qu |  |
| 2295 |  |  [Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching](https://openreview.net/forum?id=yzRXdhk2he) |  | 0 |  | Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, Chunhua Shen |  |
| 2296 |  |  [LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language Models](https://openreview.net/forum?id=qCUWVT0Ayy) |  | 0 |  | Zecheng Tang, Chenfei Wu, Juntao Li, Nan Duan |  |
| 2297 |  |  [Analyzing and Mitigating Object Hallucination in Large Vision-Language Models](https://openreview.net/forum?id=oZDJKTlOUe) |  | 0 |  | Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, Huaxiu Yao |  |
| 2298 |  |  [Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks](https://openreview.net/forum?id=MeB86edZ1P) |  | 0 |  | Mingqing Xiao, Qingyan Meng, Zongpeng Zhang, Di He, Zhouchen Lin |  |
| 2299 |  |  [DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models](https://openreview.net/forum?id=Th6NyL07na) |  | 0 |  | YungSung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James R. Glass, Pengcheng He |  |
| 2300 |  |  [PanoDiffusion: 360-degree Panorama Outpainting via Diffusion](https://openreview.net/forum?id=ZNzDXDFZ0B) |  | 0 |  | Tianhao Wu, Chuanxia Zheng, TatJen Cham |  |
| 2301 |  |  [ViDA: Homeostatic Visual Domain Adapter for Continual Test Time Adaptation](https://openreview.net/forum?id=sJ88Wg5Bp5) |  | 0 |  | Jiaming Liu, Senqiao Yang, Peidong Jia, Renrui Zhang, Ming Lu, Yandong Guo, Wei Xue, Shanghang Zhang |  |
| 2302 |  |  [IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models](https://openreview.net/forum?id=Spp2i1hKwV) |  | 0 |  | Shaokun Zhang, Xiaobo Xia, Zhaoqing Wang, LingHao Chen, Jiale Liu, Qingyun Wu, Tongliang Liu |  |
| 2303 |  |  [Connect, Collapse, Corrupt: Learning Cross-Modal Tasks with Uni-Modal Data](https://openreview.net/forum?id=ttXg3SKAg5) |  | 0 |  | Yuhui Zhang, Elaine Sui, Serena Yeung |  |
| 2304 |  |  [Unifying Feature and Cost Aggregation with Transformers for Semantic and Visual Correspondence](https://openreview.net/forum?id=fQHb1uZzl7) |  | 0 |  | Sunghwan Hong, Seokju Cho, Seungryong Kim, Stephen Lin |  |
| 2305 |  |  [Data-independent Module-aware Pruning for Hierarchical Vision Transformers](https://openreview.net/forum?id=7Ol6foUi1G) |  | 0 |  | Yang He, Joey Tianyi Zhou |  |
| 2306 |  |  [Scaling for Training Time and Post-hoc Out-of-distribution Detection Enhancement](https://openreview.net/forum?id=RDSTjtnqCg) |  | 0 |  | Kai Xu, Rongyu Chen, Gianni Franchi, Angela Yao |  |
| 2307 |  |  [A Simple and Effective Pruning Approach for Large Language Models](https://openreview.net/forum?id=PxoFut3dWW) |  | 0 |  | Mingjie Sun, Zhuang Liu, Anna Bair, J. Zico Kolter |  |
| 2308 |  |  [GeoLLM: Extracting Geospatial Knowledge from Large Language Models](https://openreview.net/forum?id=TqL2xBwXP3) |  | 0 |  | Rohin Manvi, Samar Khanna, Gengchen Mai, Marshall Burke, David B. Lobell, Stefano Ermon |  |
| 2309 |  |  [Instant3D: Fast Text-to-3D with Sparse-view Generation and Large Reconstruction Model](https://openreview.net/forum?id=2lDQLiH1W4) |  | 0 |  | Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, Sai Bi |  |
| 2310 |  |  [Effective and Efficient Federated Tree Learning on Hybrid Data](https://openreview.net/forum?id=py4ZV2qYQI) |  | 0 |  | Qinbin Li, Chulin Xie, Xiaojun Xu, Xiaoyuan Liu, Ce Zhang, Bo Li, Bingsheng He, Dawn Song |  |
| 2311 |  |  [Knowledge Distillation Based on Transformed Teacher Matching](https://openreview.net/forum?id=MJ3K7uDGGl) |  | 0 |  | Kaixiang Zheng, EnHui Yang |  |
| 2312 |  |  [Image Translation as Diffusion Visual Programmers](https://openreview.net/forum?id=yozwqhIHXj) |  | 0 |  | Cheng Han, James Chenhao Liang, Qifan Wang, Majid Rabbani, Sohail A. Dianat, Raghuveer Rao, Ying Nian Wu, Dongfang Liu |  |
| 2313 |  |  [Raidar: geneRative AI Detection viA Rewriting](https://openreview.net/forum?id=bQWE2UqXmf) |  | 0 |  | Chengzhi Mao, Carl Vondrick, Hao Wang, Junfeng Yang |  |
| 2314 |  |  [DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning](https://openreview.net/forum?id=KjegfPGRde) |  | 0 |  | Zhengxiang Shi, Aldo Lipani |  |
| 2315 |  |  [Multi-View Representation is What You Need for Point-Cloud Pre-Training](https://openreview.net/forum?id=imZcqOrbig) |  | 0 |  | Siming Yan, Chen Song, Youkang Kong, Qixing Huang |  |
| 2316 |  |  [VDT: General-purpose Video Diffusion Transformers via Mask Modeling](https://openreview.net/forum?id=Un0rgm9f04) |  | 0 |  | Haoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu, Ping Luo, Mingyu Ding |  |
| 2317 |  |  [InsertNeRF: Instilling Generalizability into NeRF with HyperNet Modules](https://openreview.net/forum?id=aHmNpLlUlb) |  | 0 |  | Yanqi Bao, Tianyu Ding, Jing Huo, Wenbin Li, Yuxin Li, Yang Gao |  |
| 2318 |  |  [Augmenting Transformers with Recursively Composed Multi-grained Representations](https://openreview.net/forum?id=u859gX7ADC) |  | 0 |  | Xiang Hu, Qingyang Zhu, Kewei Tu, Wei Wu |  |
| 2319 |  |  [P2Seg: Pointly-supervised Segmentation via Mutual Distillation](https://openreview.net/forum?id=B4vzu2aokv) |  | 0 |  | Zipeng Wang, Xuehui Yu, Xumeng Han, Wenwen Yu, Zhixun Huang, Jianbin Jiao, Zhenjun Han |  |
| 2320 |  |  [TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting](https://openreview.net/forum?id=7oLshfEIC2) |  | 0 |  | Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James Y. Zhang, Jun Zhou |  |
| 2321 |  |  [Continuous-Multiple Image Outpainting in One-Step via Positional Query and A Diffusion-based Approach](https://openreview.net/forum?id=7hxoYxKDTV) |  | 0 |  | Shaofeng Zhang, Jinfa Huang, Qiang Zhou, Zhibin Wang, Fan Wang, Jiebo Luo, Junchi Yan |  |
| 2322 |  |  [When Semantic Segmentation Meets Frequency Aliasing](https://openreview.net/forum?id=SYBdkHcXXK) |  | 0 |  | Linwei Chen, Lin Gu, Ying Fu |  |
| 2323 |  |  [Efficient Sharpness-Aware Minimization for Molecular Graph Transformer Models](https://openreview.net/forum?id=Od39h4XQ3Y) |  | 0 |  | Yili Wang, Kaixiong Zhou, Ninghao Liu, Ying Wang, Xin Wang |  |
| 2324 |  |  [MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo](https://openreview.net/forum?id=wXWfvSpYHh) |  | 0 |  | Chenjie Cao, Xinlin Ren, Yanwei Fu |  |
| 2325 |  |  [Spatio-Temporal Few-Shot Learning via Diffusive Neural Network Generation](https://openreview.net/forum?id=QyFm3D3Tzi) |  | 0 |  | Yuan Yuan, Chenyang Shao, Jingtao Ding, Depeng Jin, Yong Li |  |
| 2326 |  |  [Theoretical Understanding of Learning from Adversarial Perturbations](https://openreview.net/forum?id=Ww9rWUAcdo) |  | 0 |  | Soichiro Kumano, Hiroshi Kera, Toshihiko Yamasaki |  |
| 2327 |  |  [Graph Lottery Ticket Automated](https://openreview.net/forum?id=nmBjBZoySX) |  | 0 |  | Guibin Zhang, Kun Wang, Wei Huang, Yanwei Yue, Yang Wang, Roger Zimmermann, Aojun Zhou, Dawei Cheng, Jin Zeng, Yuxuan Liang |  |
| 2328 |  |  [FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling](https://openreview.net/forum?id=ijoqFqSC7p) |  | 0 |  | Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, Ziwei Liu |  |
| 2329 |  |  [Inner Classifier-Free Guidance and Its Taylor Expansion for Diffusion Models](https://openreview.net/forum?id=0QAzIMq32X) |  | 0 |  | Shikun Sun, Longhui Wei, Zhicai Wang, Zixuan Wang, Junliang Xing, Jia Jia, Qi Tian |  |
| 2330 |  |  [ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process](https://openreview.net/forum?id=cMPm8YFXZe) |  | 0 |  | Changyao Tian, Chenxin Tao, Jifeng Dai, Hao Li, Ziheng Li, Lewei Lu, Xiaogang Wang, Hongsheng Li, Gao Huang, Xizhou Zhu |  |
| 2331 |  |  [Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks](https://openreview.net/forum?id=9nsNyN0vox) |  | 0 |  | Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, Jun Zhao |  |
| 2332 |  |  [LMUFormer: Low Complexity Yet Powerful Spiking Model With Legendre Memory Units](https://openreview.net/forum?id=oEF7qExD9F) |  | 0 |  | Zeyu Liu, Gourav Datta, Anni Li, Peter Anthony Beerel |  |
| 2333 |  |  [InterpGNN: Understand and Improve Generalization Ability of Transdutive GNNs through the Lens of Interplay between Train and Test Nodes](https://openreview.net/forum?id=pwW807WJ9G) |  | 0 |  | Jiawei Sun, Kailai Li, Ruoxin Chen, Jie Li, Chentao Wu, Yue Ding, Junchi Yan |  |
| 2334 |  |  [STanHop: Sparse Tandem Hopfield Model for Memory-Enhanced Time Series Prediction](https://openreview.net/forum?id=6iwg437CZs) |  | 0 |  | Dennis Wu, Jerry YaoChieh Hu, Weijian Li, BoYu Chen, Han Liu |  |
| 2335 |  |  [Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images](https://openreview.net/forum?id=BteuUysuXX) |  | 0 |  | Kuofeng Gao, Yang Bai, Jindong Gu, ShuTao Xia, Philip Torr, Zhifeng Li, Wei Liu |  |
| 2336 |  |  [Progressive Fourier Neural Representation for Sequential Video Compilation](https://openreview.net/forum?id=rGFrRMBbOq) |  | 0 |  | Haeyong Kang, Jaehong Yoon, Dahyun Kim, Sung Ju Hwang, Chang D. Yoo |  |
| 2337 |  |  [Adaptive deep spiking neural network with global-local learning via balanced excitatory and inhibitory mechanism](https://openreview.net/forum?id=wpnlc2ONu0) |  | 0 |  | Tingting Jiang, Qi Xu, Xuming Ran, Jiangrong Shen, Pan Lv, Qiang Zhang, Gang Pan |  |
| 2338 |  |  [Diffusion Posterior Sampling for Linear Inverse Problem Solving: A Filtering Perspective](https://openreview.net/forum?id=tplXNcHZs1) |  | 0 |  | Zehao Dou, Yang Song |  |
| 2339 |  |  [How connectivity structure shapes rich and lazy learning in neural circuits](https://openreview.net/forum?id=slSmYGc8ee) |  | 0 |  | Yuhan Helena Liu, Aristide Baratin, Jonathan Cornford, Stefan Mihalas, Eric SheaBrown, Guillaume Lajoie |  |
| 2340 |  |  [An LLM can Fool Itself: A Prompt-Based Adversarial Attack](https://openreview.net/forum?id=VVgGbB9TNV) |  | 0 |  | Xilie Xu, Keyi Kong, Ning Liu, Lizhen Cui, Di Wang, Jingfeng Zhang, Mohan S. Kankanhalli |  |
| 2341 |  |  [AutoLoRa: An Automated Robust Fine-Tuning Framework](https://openreview.net/forum?id=09xFexjhqE) |  | 0 |  | Xilie Xu, Jingfeng Zhang, Mohan S. Kankanhalli |  |
| 2342 |  |  [Denoising Diffusion Step-aware Models](https://openreview.net/forum?id=c43FGk8Pcg) |  | 0 |  | Shuai Yang, Yukang Chen, Luozhou Wang, Shu Liu, YingCong Chen |  |
| 2343 |  |  [Get more for less: Principled Data Selection for Warming Up Fine-Tuning in LLMs](https://openreview.net/forum?id=QmYNBVukex) |  | 0 |  | Feiyang Kang, Hoang Anh Just, Yifan Sun, Himanshu Jahagirdar, Yuanzhi Zhang, Rongxing Du, Anit Kumar Sahu, Ruoxi Jia |  |
| 2344 |  |  [3D Reconstruction with Generalizable Neural Fields using Scene Priors](https://openreview.net/forum?id=Nu7dDaVF5a) |  | 0 |  | Yang Fu, Shalini De Mello, Xueting Li, Amey Kulkarni, Jan Kautz, Xiaolong Wang, Sifei Liu |  |
| 2345 |  |  [Causal Structure Recovery with Latent Variables under Milder Distributional and Graphical Assumptions](https://openreview.net/forum?id=MukGKGtgnr) |  | 0 |  | XiuChuan Li, Kun Zhang, Tongliang Liu |  |
| 2346 |  |  [AutoVP: An Automated Visual Prompting Framework and Benchmark](https://openreview.net/forum?id=wR9qVlPh0P) |  | 0 |  | HsiAi Tsao, Lei Hsiung, PinYu Chen, Si Liu, TsungYi Ho |  |
| 2347 |  |  [Structured Video-Language Modeling with Temporal Grouping and Spatial Grounding](https://openreview.net/forum?id=5dlfiJIXoh) |  | 0 |  | Yuanhao Xiong, Long Zhao, Boqing Gong, MingHsuan Yang, Florian Schroff, Ting Liu, ChoJui Hsieh, Liangzhe Yuan |  |
| 2348 |  |  [WizardCoder: Empowering Code Large Language Models with Evol-Instruct](https://openreview.net/forum?id=UnUwSIgK5W) |  | 0 |  | Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, Daxin Jiang |  |
| 2349 |  |  [Order-Preserving GFlowNets](https://openreview.net/forum?id=VXDPXuq4oG) |  | 0 |  | Yihang Chen, Lukas Mauch |  |
| 2350 |  |  [VQGraph: Rethinking Graph Representation Space for Bridging GNNs and MLPs](https://openreview.net/forum?id=h6Tz85BqRI) |  | 0 |  | Ling Yang, Ye Tian, Minkai Xu, Zhongyi Liu, Shenda Hong, Wei Qu, Wentao Zhang, Bin Cui, Muhan Zhang, Jure Leskovec |  |
| 2351 |  |  [Dual-Encoders for Extreme Multi-label Classification](https://openreview.net/forum?id=dNe1T0Ahby) |  | 0 |  | Nilesh Gupta, Devvrit, Ankit Singh Rawat, Srinadh Bhojanapalli, Prateek Jain, Inderjit S. Dhillon |  |
| 2352 |  |  [FasterViT: Fast Vision Transformers with Hierarchical Attention](https://openreview.net/forum?id=kB4yBiNmXX) |  | 0 |  | Ali Hatamizadeh, Greg Heinrich, Hongxu Yin, Andrew Tao, José M. Álvarez, Jan Kautz, Pavlo Molchanov |  |
| 2353 |  |  [AutoCast++: Enhancing World Event Prediction with Zero-shot Ranking-based Context Retrieval](https://openreview.net/forum?id=COYDmKkQH4) |  | 0 |  | Qi Yan, Raihan Seraj, Jiawei He, Lili Meng, Tristan Sylvain |  |
| 2354 |  |  [Feature Collapse](https://openreview.net/forum?id=gctmyMiPHH) |  | 0 |  | Thomas Laurent, James von Brecht, Xavier Bresson |  |
| 2355 |  |  [Function Vectors in Large Language Models](https://openreview.net/forum?id=AwyxtyMwaG) |  | 0 |  | Eric Todd, Millicent L. Li, Arnab Sen Sharma, Aaron Mueller, Byron C. Wallace, David Bau |  |
| 2356 |  |  [Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking](https://openreview.net/forum?id=8sKcAWOf2D) |  | 0 |  | Nikhil Prakash, Tamar Rott Shaham, Tal Haklay, Yonatan Belinkov, David Bau |  |
| 2357 |  |  [Perceptual Group Tokenizer: Building Perception with Iterative Grouping](https://openreview.net/forum?id=NnYaYVODyV) |  | 0 |  | Zhiwei Deng, Ting Chen, Yang Li |  |
| 2358 |  |  [ImageNet-OOD: Deciphering Modern Out-of-Distribution Detection Algorithms](https://openreview.net/forum?id=VTYg5ykEGS) |  | 0 |  | William Yang, Byron Zhang, Olga Russakovsky |  |
| 2359 |  |  [Self-supervised Representation Learning from Random Data Projectors](https://openreview.net/forum?id=EpYnZpDpsQ) |  | 0 |  | Yi Sui, Tongzi Wu, Jesse C. Cresswell, Ga Wu, George Stein, Xiao Shi Huang, Xiaochen Zhang, Maksims Volkovs |  |
| 2360 |  |  [Approximately Piecewise E(3) Equivariant Point Networks](https://openreview.net/forum?id=aKJEHWmBEf) |  | 0 |  | Matan Atzmon, Jiahui Huang, Francis Williams, Or Litany |  |
| 2361 |  |  [DAM: Towards a Foundation Model for Forecasting](https://openreview.net/forum?id=4NhMhElWqP) |  | 0 |  | Luke Nicholas Darlow, Qiwen Deng, Ahmed Hassan, Martin Asenov, Rajkarn Singh, Artjom Joosen, Adam Barker, Amos J. Storkey |  |
| 2362 |  |  [Weakly-supervised Audio Separation via Bi-modal Semantic Similarity](https://openreview.net/forum?id=4N97bz1sP6) |  | 0 |  | Tanvir Mahmud, Saeed Amizadeh, Kazuhito Koishida, Diana Marculescu |  |
| 2363 |  |  [Expected flow networks in stochastic environments and two-player zero-sum games](https://openreview.net/forum?id=uH0FGECSEI) |  | 0 |  | Marco Jiralerspong, Bilun Sun, Danilo Vucetic, Tianyu Zhang, Yoshua Bengio, Gauthier Gidel, Nikolay Malkin |  |
| 2364 |  |  [Neural Polynomial Gabor Fields for Macro Motion Analysis](https://openreview.net/forum?id=dTlKCQuuxP) |  | 0 |  | Chen Geng, HongXing Yu, Sida Peng, Xiaowei Zhou, Jiajun Wu |  |
| 2365 |  |  [Denoising Diffusion via Image-Based Rendering](https://openreview.net/forum?id=1JbsdayvhO) |  | 0 |  | Titas Anciukevicius, Fabian Manhardt, Federico Tombari, Paul Henderson |  |
| 2366 |  |  [LEAP: Liberate Sparse-View 3D Modeling from Camera Poses](https://openreview.net/forum?id=KPmajBxEaF) |  | 0 |  | Hanwen Jiang, Zhenyu Jiang, Yue Zhao, Qixing Huang |  |
| 2367 |  |  [Language Modeling Is Compression](https://openreview.net/forum?id=jznbgiynus) |  | 0 |  | Grégoire Delétang, Anian Ruoss, PaulAmbroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi GrauMoya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, Marcus Hutter, Joel Veness |  |
| 2368 |  |  [OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views](https://openreview.net/forum?id=SgjAojPKb3) |  | 0 |  | Francis Engelmann, Fabian Manhardt, Michael Niemeyer, Keisuke Tateno, Federico Tombari |  |
| 2369 |  |  [Learning with a Mole: Transferable latent spatial representations for navigation without reconstruction](https://openreview.net/forum?id=8HCARN2hhw) |  | 0 |  | Guillaume Bono, Leonid Antsfeld, Assem Sadek, Gianluca Monaci, Christian Wolf |  |
| 2370 |  |  [Reverse Forward Curriculum Learning for Extreme Sample and Demo Efficiency](https://openreview.net/forum?id=w4rODxXsmM) |  | 0 |  | Stone Tao, Arth Shukla, Tsekai Chan, Hao Su |  |
| 2371 |  |  [BatchPrompt: Accomplish more with less](https://openreview.net/forum?id=Agyicd577r) |  | 0 |  | Jianzhe Lin, Maurice Diesendruck, Liang Du, Robin Abraham |  |
| 2372 |  |  [Large Language Models as Optimizers](https://openreview.net/forum?id=Bb4VGOWELI) |  | 0 |  | Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, Xinyun Chen |  |
| 2373 |  |  [ContextRef: Evaluating Referenceless Metrics for Image Description Generation](https://openreview.net/forum?id=j0ZvKSNZiP) |  | 0 |  | Elisa Kreiss, Eric Zelikman, Christopher Potts, Nick Haber |  |
| 2374 |  |  [Tractable Probabilistic Graph Representation Learning with Graph-Induced Sum-Product Networks](https://openreview.net/forum?id=h7nOCxFsPg) |  | 0 |  | Federico Errica, Mathias Niepert |  |
| 2375 |  |  [HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion](https://openreview.net/forum?id=duyA42HlCK) |  | 0 |  | Xian Liu, Jian Ren, Aliaksandr Siarohin, Ivan Skorokhodov, Yanyu Li, Dahua Lin, Xihui Liu, Ziwei Liu, Sergey Tulyakov |  |
| 2376 |  |  [ZeroFlow: Scalable Scene Flow via Distillation](https://openreview.net/forum?id=FRCHDhbxZF) |  | 0 |  | Kyle Vedder, Neehar Peri, Nathaniel Chodosh, Ishan Khatri, Eric Eaton, Dinesh Jayaraman, Yang Liu, Deva Ramanan, James Hays |  |
| 2377 |  |  [R&B: Region and Boundary Aware Zero-shot Grounded Text-to-image Generation](https://openreview.net/forum?id=8Q4uVOJ5bX) |  | 0 |  | Jiayu Xiao, Henglei Lv, Liang Li, Shuhui Wang, Qingming Huang |  |
| 2378 |  |  [Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Probabilistic Interpretations](https://openreview.net/forum?id=I1quoTXZzc) |  | 0 |  | Xinyue Xu, Yi Qin, Lu Mi, Hao Wang, Xiaomeng Li |  |
| 2379 |  |  [SpeechTokenizer: Unified Speech Tokenizer for Speech Language Models](https://openreview.net/forum?id=AF9Q8Vip84) |  | 0 |  | Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, Xipeng Qiu |  |
| 2380 |  |  [Enhancing High-Resolution 3D Generation through Pixel-wise Gradient Clipping](https://openreview.net/forum?id=ukidfml68f) |  | 0 |  | Zijie Pan, Jiachen Lu, Xiatian Zhu, Li Zhang |  |
| 2381 |  |  [Transferring Labels to Solve Annotation Mismatches Across Object Detection Datasets](https://openreview.net/forum?id=ChHx5ORqF0) |  | 0 |  | YuanHong Liao, David Acuna, Rafid Mahmood, James Lucas, Viraj Prabhu, Sanja Fidler |  |
| 2382 |  |  [Weatherproofing Retrieval for Localization with Generative AI and Geometric Consistency](https://openreview.net/forum?id=5EniAcsO7f) |  | 0 |  | Yannis Kalantidis, Mert Bülent Sariyildiz, Rafael S. Rezende, Philippe Weinzaepfel, Diane Larlus, Gabriela Csurka |  |
| 2383 |  |  [DreamClean: Restoring Clean Image Using Deep Diffusion Prior](https://openreview.net/forum?id=6ALuy19mPa) |  | 0 |  | Jie Xiao, Ruili Feng, Han Zhang, Zhiheng Liu, Zhantao Yang, Yurui Zhu, Xueyang Fu, Kai Zhu, Yu Liu, ZhengJun Zha |  |
| 2384 |  |  [CausalLM is not optimal for in-context learning](https://openreview.net/forum?id=guRNebwZBb) |  | 0 |  | Nan Ding, Tomer Levinboim, Jialin Wu, Sebastian Goodman, Radu Soricut |  |
| 2385 |  |  [End-to-End (Instance)-Image Goal Navigation through Correspondence as an Emergent Phenomenon](https://openreview.net/forum?id=cphhnHjCvC) |  | 0 |  | Guillaume Bono, Leonid Antsfeld, Boris Chidlovskii, Philippe Weinzaepfel, Christian Wolf |  |
| 2386 |  |  [Strategic Preys Make Acute Predators: Enhancing Camouflaged Object Detectors by Generating Camouflaged Objects](https://openreview.net/forum?id=hywpSoHwgX) |  | 0 |  | Chunming He, Kai Li, Yachao Zhang, Yulun Zhang, Chenyu You, Zhenhua Guo, Xiu Li, Martin Danelljan, Fisher Yu |  |
| 2387 |  |  [Consistency-guided Prompt Learning for Vision-Language Models](https://openreview.net/forum?id=wsRXwlwx4w) |  | 0 |  | Shuvendu Roy, Ali Etemad |  |
| 2388 |  |  [Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting](https://openreview.net/forum?id=WhgB5sispV) |  | 0 |  | Zeyu Yang, Hongye Yang, Zijie Pan, Li Zhang |  |
| 2389 |  |  [Language-Informed Visual Concept Learning](https://openreview.net/forum?id=juuyW8B8ig) |  | 0 |  | Sharon Lee, Yunzhi Zhang, Shangzhe Wu, Jiajun Wu |  |
| 2390 |  |  [Online Continual Learning for Interactive Instruction Following Agents](https://openreview.net/forum?id=7M0EzjugaN) |  | 0 |  | Byeonghwi Kim, Minhyuk Seo, Jonghyun Choi |  |
| 2391 |  |  [Localizing and Editing Knowledge In Text-to-Image Generative Models](https://openreview.net/forum?id=Qmw9ne6SOQ) |  | 0 |  | Samyadeep Basu, Nanxuan Zhao, Vlad I. Morariu, Soheil Feizi, Varun Manjunatha |  |
| 2392 |  |  [Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization](https://openreview.net/forum?id=7NzgkEdGyr) |  | 0 |  | Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen Liu, Juyeon Heo, Songyou Peng, Yandong Wen, Michael J. Black, Adrian Weller, Bernhard Schölkopf |  |
| 2393 |  |  [Towards domain-invariant Self-Supervised Learning with Batch Styles Standardization](https://openreview.net/forum?id=qtE9K23ISq) |  | 0 |  | Marin Scalbert, Maria Vakalopoulou, Florent CouzinieDevy |  |
| 2394 |  |  [LaneSegNet: Map Learning with Lane Segment Perception for Autonomous Driving](https://openreview.net/forum?id=LsURkIPYR5) |  | 0 |  | Tianyu Li, Peijin Jia, Bangjun Wang, Li Chen, Kun Jiang, Junchi Yan, Hongyang Li |  |
| 2395 |  |  [Spike-driven Transformer V2: Meta Spiking Neural Network Architecture Inspiring the Design of Next-generation Neuromorphic Chips](https://openreview.net/forum?id=1SIBN5Xyw7) |  | 0 |  | Man Yao, Jiakui Hu, Tianxiang Hu, Yifan Xu, Zhaokun Zhou, Yonghong Tian, Bo Xu, Guoqi Li |  |
| 2396 |  |  [BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity](https://openreview.net/forum?id=mQYHXUUTkU) |  | 0 |  | Andrew F. Luo, Margaret M. Henderson, Michael J. Tarr, Leila Wehbe |  |
| 2397 |  |  [GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion](https://openreview.net/forum?id=FvK2noilxT) |  | 0 |  | Xueyi Liu, Li Yi |  |
| 2398 |  |  [Get What You Want, Not What You Don't: Image Content Suppression for Text-to-Image Diffusion Models](https://openreview.net/forum?id=zpVPhvVKXk) |  | 0 |  | Senmao Li, Joost van de Weijer, Taihang Hu, Fahad Shahbaz Khan, Qibin Hou, Yaxing Wang, Jian Yang |  |
| 2399 |  |  [Less or More From Teacher: Exploiting Trilateral Geometry For Knowledge Distillation](https://openreview.net/forum?id=OZitfSXpdT) |  | 0 |  | Chengming Hu, Haolun Wu, Xuan Li, Chen Ma, Xi Chen, Boyu Wang, Jun Yan, Xue Liu |  |
| 2400 |  |  [Unraveling the Enigma of Double Descent: An In-depth Analysis through the Lens of Learned Feature Space](https://openreview.net/forum?id=CEkIyshNbC) |  | 0 |  | Yufei Gu, Xiaoqing Zheng, Tomaso Aste |  |
| 2401 |  |  [Meta-Evolve: Continuous Robot Evolution for One-to-many Policy Transfer](https://openreview.net/forum?id=RthOl4jHw5) |  | 0 |  | Xingyu Liu, Deepak Pathak, Ding Zhao |  |
| 2402 |  |  [DFormer: Rethinking RGBD Representation Learning for Semantic Segmentation](https://openreview.net/forum?id=h1sFUGlI09) |  | 0 |  | Bowen Yin, Xuying Zhang, ZhongYu Li, Li Liu, MingMing Cheng, Qibin Hou |  |
| 2403 |  |  [ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving](https://openreview.net/forum?id=Ep0TtjVoap) |  | 0 |  | Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen |  |
| 2404 |  |  [Bayesian Bi-clustering of Neural Spiking Activity with Latent Structures](https://openreview.net/forum?id=ZYm1Ql6udy) |  | 0 |  | Ganchao Wei |  |
| 2405 |  |  [GRANDE: Gradient-Based Decision Tree Ensembles for Tabular Data](https://openreview.net/forum?id=XEFWBxi075) |  | 0 |  | Sascha Marton, Stefan Lüdtke, Christian Bartelt, Heiner Stuckenschmidt |  |
| 2406 |  |  [GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers](https://openreview.net/forum?id=uJVHygNeSZ) |  | 0 |  | Takeru Miyato, Bernhard Jaeger, Max Welling, Andreas Geiger |  |
| 2407 |  |  [VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models](https://openreview.net/forum?id=ygxTuVz9eU) |  | 0 |  | Zihao Zhu, Mingda Zhang, Shaokui Wei, Bingzhe Wu, Baoyuan Wu |  |
| 2408 |  |  [Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching](https://openreview.net/forum?id=rTBL8OhdhH) |  | 0 |  | Ziyao Guo, Kai Wang, George Cazenavette, Hui Li, Kaipeng Zhang, Yang You |  |
| 2409 |  |  [SYMBOL: Generating Flexible Black-Box Optimizers through Symbolic Equation Learning](https://openreview.net/forum?id=vLJcd43U7a) |  | 0 |  | Jiacheng Chen, Zeyuan Ma, Hongshu Guo, Yining Ma, Jie Zhang, YueJiao Gong |  |
| 2410 |  |  [SEA: Sparse Linear Attention with Estimated Attention Mask](https://openreview.net/forum?id=JbcwfmYrob) |  | 0 |  | Heejun Lee, Jina Kim, Jeffrey Willette, Sung Ju Hwang |  |
| 2411 |  |  [Zero-Mean Regularized Spectral Contrastive Learning: Implicitly Mitigating Wrong Connections in Positive-Pair Graphs](https://openreview.net/forum?id=RZBy8oHTz4) |  | 0 |  | Xiong Zhou, Xianming Liu, Feilong Zhang, Gang Wu, Deming Zhai, Junjun Jiang, Xiangyang Ji |  |
| 2412 |  |  [Variance-enlarged Poisson Learning for Graph-based Semi-Supervised Learning with Extremely Sparse Labeled Data](https://openreview.net/forum?id=yeeVBMDAwy) |  | 0 |  | Xiong Zhou, Xianming Liu, Hao Yu, Jialiang Wang, Zeke Xie, Junjun Jiang, Xiangyang Ji |  |
| 2413 |  |  [Enhancing Contrastive Learning for Ordinal Regression via Ordinal Content Preserved Data Augmentation](https://openreview.net/forum?id=kx2XZlmgB1) |  | 0 |  | Jiyang Zheng, Yu Yao, Bo Han, Dadong Wang, Tongliang Liu |  |
| 2414 |  |  [SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning](https://openreview.net/forum?id=pTHfApDakA) |  | 0 |  | Ning Miao, Yee Whye Teh, Tom Rainforth |  |
| 2415 |  |  [OmniControl: Control Any Joint at Any Time for Human Motion Generation](https://openreview.net/forum?id=gd0lAEtWso) |  | 0 |  | Yiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, Huaizu Jiang |  |
| 2416 |  |  [Guaranteed Approximation Bounds for Mixed-Precision Neural Operators](https://openreview.net/forum?id=QJGj07PD9C) |  | 0 |  | Renbo Tu, Colin White, Jean Kossaifi, Boris Bonev, Gennady Pekhimenko, Kamyar Azizzadenesheli, Anima Anandkumar |  |
| 2417 |  |  [Geometry-Aware Projective Mapping for Unbounded Neural Radiance Fields](https://openreview.net/forum?id=w7BwaDHppp) |  | 0 |  | Junoh Lee, Hyunjun Jung, JinHwi Park, Inhwan Bae, HaeGon Jeon |  |
| 2418 |  |  [REValueD: Regularised Ensemble Value-Decomposition for Factorisable Markov Decision Processes](https://openreview.net/forum?id=Gf15GsnfTy) |  | 0 |  | David Ireland, Giovanni Montana |  |
| 2419 |  |  [Path Choice Matters for Clear Attributions in Path Methods](https://openreview.net/forum?id=gzYgsZgwXa) |  | 0 |  | Borui Zhang, Wenzhao Zheng, Jie Zhou, Jiwen Lu |  |
| 2420 |  |  [Exploring Target Representations for Masked Autoencoders](https://openreview.net/forum?id=xmQMz9OPF5) |  | 0 |  | Xingbin Liu, Jinghao Zhou, Tao Kong, Xianming Lin, Rongrong Ji |  |
| 2421 |  |  [Koopman-based generalization bound: New aspect for full-rank weights](https://openreview.net/forum?id=JN7TcCm9LF) |  | 0 |  | Yuka Hashimoto, Sho Sonoda, Isao Ishikawa, Atsushi Nitanda, Taiji Suzuki |  |
| 2422 |  |  [Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis](https://openreview.net/forum?id=aA33A70IO6) |  | 0 |  | Kai Chen, Chunwei Wang, Kuo Yang, Jianhua Han, Lanqing Hong, Fei Mi, Hang Xu, Zhengying Liu, Wenyong Huang, Zhenguo Li, DitYan Yeung, Lifeng Shang, Xin Jiang, Qun Liu |  |
| 2423 |  |  [MagicDrive: Street View Generation with Diverse 3D Geometry Control](https://openreview.net/forum?id=sBQwvucduK) |  | 0 |  | Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, DitYan Yeung, Qiang Xu |  |
| 2424 |  |  [MogaNet: Multi-order Gated Aggregation Network](https://openreview.net/forum?id=XhYWgjqCrV) |  | 0 |  | Siyuan Li, Zedong Wang, Zicheng Liu, Cheng Tan, Haitao Lin, Di Wu, Zhiyuan Chen, Jiangbin Zheng, Stan Z. Li |  |
| 2425 |  |  [GeoDiffusion: Text-Prompted Geometric Control for Object Detection Data Generation](https://openreview.net/forum?id=xBfQZWeDRH) |  | 0 |  | Kai Chen, Enze Xie, Zhe Chen, Yibo Wang, Lanqing Hong, Zhenguo Li, DitYan Yeung |  |
| 2426 |  |  [Un-Mixing Test-Time Normalization Statistics: Combatting Label Temporal Correlation](https://openreview.net/forum?id=xyxU99Nutg) |  | 0 |  | Devavrat Tomar, Guillaume Vray, JeanPhilippe Thiran, Behzad Bozorgtabar |  |
| 2427 |  |  [Constraint-Free Structure Learning with Smooth Acyclic Orientations](https://openreview.net/forum?id=KWO8LSUC5W) |  | 0 |  | Riccardo Massidda, Francesco Landolfi, Martina Cinquini, Davide Bacciu |  |
| 2428 |  |  [Pareto Deep Long-Tailed Recognition: A Conflict-Averse Solution](https://openreview.net/forum?id=b66P1u0k15) |  | 0 |  | Zhipeng Zhou, Liu Liu, Peilin Zhao, Wei Gong |  |
| 2429 |  |  [MixSup: Mixed-grained Supervision for Label-efficient LiDAR-based 3D Object Detection](https://openreview.net/forum?id=Q1vkAhdI6j) |  | 0 |  | Yuxue Yang, Lue Fan, Zhaoxiang Zhang |  |
| 2430 |  |  [Boosting Vanilla Lightweight Vision Transformers via Re-parameterization](https://openreview.net/forum?id=3rmpixOjPS) |  | 0 |  | Zhentao Tan, Xiaodan Li, Yue Wu, Qi Chu, Le Lu, Nenghai Yu, Jieping Ye |  |
| 2431 |  |  [Robust Angular Synchronization via Directed Graph Neural Networks](https://openreview.net/forum?id=5sjxMwWmk8) |  | 0 |  | Yixuan He, Gesine Reinert, David Wipf, Mihai Cucuringu |  |
| 2432 |  |  [Multi-Scale Representations by Varying Window Attention for Semantic Segmentation](https://openreview.net/forum?id=lAhWGOkpSR) |  | 0 |  | Haotian Yan, Ming Wu, Chuang Zhang |  |
| 2433 |  |  [FedP3: Federated Personalized and Privacy-friendly Network Pruning under Model Heterogeneity](https://openreview.net/forum?id=hbHwZYqk9T) |  | 0 |  | Kai Yi, Nidham Gazagnadou, Peter Richtárik, Lingjuan Lyu |  |
| 2434 |  |  [Compressed Context Memory for Online Language Model Interaction](https://openreview.net/forum?id=64kSvC4iPg) |  | 0 |  | JangHyun Kim, Junyoung Yeom, Sangdoo Yun, Hyun Oh Song |  |
| 2435 |  |  [TUVF: Learning Generalizable Texture UV Radiance Fields](https://openreview.net/forum?id=dN4vpVTvWX) |  | 0 |  | AnChieh Cheng, Xueting Li, Sifei Liu, Xiaolong Wang |  |
| 2436 |  |  [Neural Processing of Tri-Plane Hybrid Neural Fields](https://openreview.net/forum?id=zRkM6UcA22) |  | 0 |  | Adriano Cardace, Pierluigi Zama Ramirez, Francesco Ballerini, Allan Zhou, Samuele Salti, Luigi Di Stefano |  |
| 2437 |  |  [Large-Vocabulary 3D Diffusion Model with Transformer](https://openreview.net/forum?id=q57JLSE2j5) |  | 0 |  | Ziang Cao, Fangzhou Hong, Tong Wu, Liang Pan, Ziwei Liu |  |
| 2438 |  |  [SAS: Structured Activation Sparsification](https://openreview.net/forum?id=vZfi5to2Xl) |  | 0 |  | Yusuke Sekikawa, Shingo Yashima |  |
| 2439 |  |  [A Progressive Training Framework for Spiking Neural Networks with Learnable Multi-hierarchical Model](https://openreview.net/forum?id=g52tgL8jy6) |  | 0 |  | Zecheng Hao, Xinyu Shi, Zihan Huang, Tong Bu, Zhaofei Yu, Tiejun Huang |  |
| 2440 |  |  [Mega-TTS 2: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis](https://openreview.net/forum?id=mvMI3N4AvD) |  | 0 |  | Ziyue Jiang, Jinglin Liu, Yi Ren, Jinzheng He, Zhenhui Ye, Shengpeng Ji, Qian Yang, Chen Zhang, Pengfei Wei, Chunfeng Wang, Xiang Yin, Zejun Ma, Zhou Zhao |  |
| 2441 |  |  [A Symmetry-Aware Exploration of Bayesian Neural Network Posteriors](https://openreview.net/forum?id=FOSBQuXgAq) |  | 0 |  | Olivier Laurent, Emanuel Aldea, Gianni Franchi |  |
| 2442 |  |  [Threaten Spiking Neural Networks through Combining Rate and Temporal Information](https://openreview.net/forum?id=xv8iGxENyI) |  | 0 |  | Zecheng Hao, Tong Bu, Xinyu Shi, Zihan Huang, Zhaofei Yu, Tiejun Huang |  |
| 2443 |  |  [QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models](https://openreview.net/forum?id=FIplmUWdm3) |  | 0 |  | Jing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei Cai, Bohan Zhuang |  |
| 2444 |  |  [3D-Aware Hypothesis & Verification for Generalizable Relative Object Pose Estimation](https://openreview.net/forum?id=U6hEOZlDf5) |  | 0 |  | Chen Zhao, Tong Zhang, Mathieu Salzmann |  |
| 2445 |  |  [Language Model Self-improvement by Reinforcement Learning Contemplation](https://openreview.net/forum?id=38E4yUbrgr) |  | 0 |  | JingCheng Pang, Pengyuan Wang, Kaiyuan Li, XiongHui Chen, Jiacheng Xu, Zongzhang Zhang, Yang Yu |  |
| 2446 |  |  [Divide and not forget: Ensemble of selectively trained experts in Continual Learning](https://openreview.net/forum?id=sSyytcewxe) |  | 0 |  | Grzegorz Rypesc, Sebastian Cygert, Valeriya Khan, Tomasz Trzcinski, Bartosz Zielinski, Bartlomiej Twardowski |  |
| 2447 |  |  [Towards Offline Opponent Modeling with In-context Learning](https://openreview.net/forum?id=2SwHngthig) |  | 0 |  | Yuheng Jing, Kai Li, Bingyun Liu, Yifan Zang, Haobo Fu, Qiang Fu, Junliang Xing, Jian Cheng |  |
| 2448 |  |  [Early Stopping Against Label Noise Without Validation Data](https://openreview.net/forum?id=CMzF2aOfqp) |  | 0 |  | Suqin Yuan, Lei Feng, Tongliang Liu |  |
| 2449 |  |  [Recursive Generalization Transformer for Image Super-Resolution](https://openreview.net/forum?id=owziuM1nsR) |  | 0 |  | Zheng Chen, Yulun Zhang, Jinjin Gu, Linghe Kong, Xiaokang Yang |  |
| 2450 |  |  [Rethinking Model Ensemble in Transfer-based Adversarial Attacks](https://openreview.net/forum?id=AcJrSoArlh) |  | 0 |  | Huanran Chen, Yichi Zhang, Yinpeng Dong, Xiao Yang, Hang Su, Jun Zhu |  |
| 2451 |  |  [Langevin Monte Carlo for strongly log-concave distributions: Randomized midpoint revisited](https://openreview.net/forum?id=hOxgrGM63n) |  | 0 |  | Lu Yu, Avetik G. Karagulyan, Arnak S. Dalalyan |  |
| 2452 |  |  [MuSc: Zero-Shot Industrial Anomaly Classification and Segmentation with Mutual Scoring of the Unlabeled Images](https://openreview.net/forum?id=AHgc5SMdtd) |  | 0 |  | Xurui Li, Ziming Huang, Feng Xue, Yu Zhou |  |
| 2453 |  |  [To the Cutoff... and Beyond? A Longitudinal Perspective on LLM Data Contamination](https://openreview.net/forum?id=m2NVG4Htxs) |  | 0 |  | Manley Roberts, Himanshu Thakur, Christine Herlihy, Colin White, Samuel Dooley |  |
| 2454 |  |  [I-PHYRE: Interactive Physical Reasoning](https://openreview.net/forum?id=1bbPQShCT2) |  | 0 |  | Shiqian Li, Kewen Wu, Chi Zhang, Yixin Zhu |  |
| 2455 |  |  [Exposing Text-Image Inconsistency Using Diffusion Models](https://openreview.net/forum?id=Ny150AblPu) |  | 0 |  | Mingzhen Huang, Shan Jia, Zhou Zhou, Yan Ju, Jialing Cai, Siwei Lyu |  |
