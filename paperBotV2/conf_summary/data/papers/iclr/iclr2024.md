# ICLR2024

## 会议论文列表

本会议共有 2455 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [The Second Tiny Papers Track at ICLR 2024, Tiny Papers @ ICLR 2024, Vienna, Austria, May 11, 2024](https://openreview.net/group?id=ICLR.cc/2024/TinyPapers) |  | 0 |  |  |  |
| 2 |  |  [Chemical Language Models Have Problems with Chemistry: A Case Study on Molecule Captioning Task](https://openreview.net/forum?id=JoO6mtCLHD) |  | 0 | Drug discovery has been greatly enhanced through the recent fusion of molecular sciences and natural language processing, leading these research fields to significant advancements. Considering the crucial role of molecule representation in chemical understanding within these models, we introduce... | Andrey V. Savchenko, Artur Kadurin, Elena Tutubalina, Kuzma Khrabrov, Veronika Ganeeva |  |
| 3 |  |  [Towards Fairness constrained Restless Multi-Armed Bandits: a Case Study of Maternal and child Care Domain](https://openreview.net/forum?id=IfjStJ6GoX) |  | 0 | Restless multi-armed bandits (RMABs) are widely used for resource allocation in dynamic environments, but they typically do not consider fairness implications. This paper introduces a fairness-aware approach for offline RMABs. We propose a Kullback-Leibler (KL) divergence-based fairness metric to... | Aparna Taneja, Gargi Singh, Milind Tambe |  |
| 4 |  |  [Utilizing Cross-Version Consistency for Domain Adaptation: A Case Study on Music Audio](https://openreview.net/forum?id=ZNg3YQQKWT) |  | 0 | Deep learning models are commonly trained on large annotated corpora, often in a specific domain. Generalization to another domain without annotated data is usually challenging. In this paper, we address such unsupervised domain adaptation based on the teacher--student learning paradigm. For... | Christof Weiss, Lele Liu |  |
| 5 |  |  [Aligners: Decoupling LLMs and Alignment](https://openreview.net/forum?id=E6WukV41He) |  | 0 | Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion. We propose to decouple LLMs and alignment by training aligner models... | Alex Gittens, Lilian Ngweta, Mayank Agarwal, Mikhail Yurochkin, Subha Maity, Yuekai Sun |  |
| 6 |  |  [When is RL better than DPO in RLHF? A Representation and Optimization Perspective](https://openreview.net/forum?id=lNEFatlsQb) |  | 0 | Aligning large language models with human preferences is important, and there are two kinds of alignment methods. The first class of algorithms is based on reinforcement learning (RL), which involves learning a reward function from a human preference dataset and improving performance via online... | Tian Xu, Yang Yu, Ziniu Li |  |
| 7 |  |  [Is Watermarking LLM-Generated Code Robust?](https://openreview.net/forum?id=8PhI1PzSYY) |  | 0 | We present the first study of the robustness of existing watermarking techniques on Python code generated by large language models. Although existing works showed that watermarking can be robust for natural language text, we show that it is easy to remove these watermarks on code by simple... | Gagandeep Singh, Sasa Misailovic, Shubham Ugare, Tarun Suresh |  |
| 8 |  |  [Lost in Translation: GANs' Inability to Generate Simple Probability Distributions](https://openreview.net/forum?id=MUOmyZMEd4) |  | 0 | Since its inception, Generative Adversarial Networks (GAN) have marked a triumph in generative modeling. Its impeccable capacity to mimic observations from unknown probability distributions has positioned it as a widely used simulation tool. In typical applications, GANs find themselves simulating... | Anish Chakrabarty, Debanjan Dutta, Swagatam Das |  |
| 9 |  |  [CMFPN: Context Modeling Meets Feature Pyramid Network](https://openreview.net/forum?id=Qtu2Od1ggw) |  | 0 | Feature fusion is a powerful technique that enables predictors to access a semantically rich representation of an image. Feature Pyramid Networks (FPNs) are the most widely used models for fusing features. However, the context within the FPN layers is inconsistent, leading to false predictions.... | Faroq AlTam, Muhammad AlQurishi, Riad Souissi, Thariq Khalid Kadavil |  |
| 10 |  |  [Enhancing Drug-Drug Interaction Prediction with Context-Aware Architecture](https://openreview.net/forum?id=e2Bkf1Bzh4) |  | 0 | In the field of disease treatment, the simultaneous use of multiple medications can lead to unforeseen adverse reactions, compromising patient safety and therapeutic efficacy. Consequently, predicting drug-drug interactions (DDIs) has emerged as a pivotal research focus on improving disease... | Sun Kim, Yijingxiu Lu, Yinhua Piao |  |
| 11 |  |  [Can LLMs Learn a New Language on the Fly? A Case Study on Zhuang](https://openreview.net/forum?id=GTHD2UnDIb) |  | 0 | Existing large language models still fail to support many low-resource languages. Especially for the extremely low-resource ones, there is hardly any training data to effectively update the model parameters. We thus investigate whether LLMs can learn a new language on the fly through in-context... | Chen Zhang, Mingxu Tao, Quzhe Huang, Yansong Feng, Zhibin Chen |  |
| 12 |  |  [Weighted Branch Aggregation Based Deep Learning Model for Track Detection in Autonomous Racing](https://openreview.net/forum?id=K3ilD3QhX6) |  | 0 | Intelligent track detection is a vital component of autonomous racing cars. We develop a novel Weighted Branch Aggregation based Convolutional Neural Network (WeBACNN) model that can accurately detect the track while being robust against image blurring due to high speed, and can work independently... | Abu Shafin Mohammad Mahdee Jameel, Aly El Gamal, ChingHsiang Huang, Samuel Labi, Shreya Ghosh, YiHuan Chen |  |
| 13 |  |  [Visual prompting Methods for GPT-4V based Zero-Shot Graphic Layout Design Generation](https://openreview.net/forum?id=DI4gETm7aa) |  | 0 | Graphic layout design generation is a challenging problem in computer vision. The key aspect of the challenge is ensuring coherent placement of textual elements on the background image to ensure aesthetic appeal and avoiding occlusion of key visual elements. Although prior methods have made... | Ankan Biswas, Kunal Singh, Mukund Khanna, Pradeep Moturi, Shivam |  |
| 14 |  |  [Exploring the Limits of Semantic Image Compression at Micro-bits per Pixel](https://openreview.net/forum?id=sfwtoH5GdD) |  | 0 | Traditional methods, such as JPEG, perform image compression by operating on structural information, such as pixel values or frequency content. These methods are effective to bitrates around one bit per pixel (bpp) and higher at standard image sizes. However, to compress further text-based semantic... | Bahaa Kotb, James Dotzel, Jordan Dotzel, Mohamed S. Abdelfattah, Zhiru Zhang |  |
| 15 |  |  [Evaluating Groups of Features via Consistency, Contiguity, and Stability](https://openreview.net/forum?id=IP2etbIEuC) |  | 0 | Feature attributions explain model predictions by assigning importance scores to input features. In high-dimensional data such as images, these scores are often assigned to groups of features. There are various strategies for creating these groups, ranging from simple patches to deep-learning-based... | Chaehyeon Kim, Eric Wong, Shreya Havaldar, Weiqiu You |  |
| 16 |  |  [What Does a Visual Formal Analysis of the World's 500 Most Famous Paintings Tell Us About Multimodal LLMs?](https://openreview.net/forum?id=dINMwL186O) |  | 0 | This work introduces ArtQA, a new benchmark for multimodal LLMs through the lens of formal analysis of paintings. We focus on key elements such as line, shape, space, color, form, value, and texture—collectively referred to as the elements of art in visual formal analysis. ArtQA contains questions... | Muzi Tao, Saining Xie |  |
| 17 |  |  [Revamp: Automated Simulations of Adversarial Attacks on Arbitrary Objects in Realistic Scenes](https://openreview.net/forum?id=XCLrySEUBe) |  | 0 | Deep learning models, such as those used in autonomous vehicles are vulnerable to adversarial attacks where attackers could place adversarial objects in the environment to induce incorrect detections. While generating such adversarial objects in the digital realm is well-studied, successfully... | Duen Horng Chau, Matthew Hull, Zijie J. Wang |  |
| 18 |  |  [Hard ASH: Sparsity and the right optimizer make a continual learner](https://openreview.net/forum?id=WwQKl1OrMX) |  | 0 | In class incremental learning, neural networks typically suffer from catastrophic forgetting. We show that an MLP featuring a sparse activation function and an adaptive learning rate optimizer can compete with established regularization techniques in the Split-MNIST task. We highlight the... | Santtu Keskinen |  |
| 19 |  |  [Analog In-Memory Computing with Uncertainty Quantification for Efficient Edge-based Medical Imaging Segmentation](https://openreview.net/forum?id=hvp5I4dDya) |  | 0 | This work investigates the role of the emerging Analog In-memory computing (AIMC) paradigm in enabling Medical AI analysis and improving the certainty of these models at the edge. It contrasts AIMC's efficiency with traditional digital computing's limitations in power, speed, and scalability. Our... | Hadjer Benmeziane, Imane Hamzaoui, Kaoutar El Maghraoui, Zayneb Cherif |  |
| 20 |  |  [VoltaVision: A Transfer Learning model for electronic component classification](https://openreview.net/forum?id=JHTqFvmVYz) |  | 0 | In this paper, we analyze the effectiveness of transfer learning on classifying electronic components. Transfer learning reuses pre-trained models to save time and resources in building a robust classifier rather than learning from scratch. Our work introduces a lightweight CNN, coined as... | Anas Mohammad Ishfaqul Muktadir Osmani, Salekul Islam, Taimur Rahman |  |
| 21 |  |  [Training Mixture-of-Experts: A Focus on Expert-Token Matching](https://openreview.net/forum?id=UJgSQjBWZS) |  | 0 | Recent advancements in sparse Mixture-of-Experts (MoE) models, particularly in the Vision MoE (VMoE) framework, have demonstrated promising results in enhancing vision task performance. However, a key challenge persists in optimally routing tokens (such as image patches) to the right experts,... | Fateme Vesaghati, Masoumeh Zareapoor |  |
| 22 |  |  [Learning Disentangled Audio Representations through Controlled Synthesis](https://openreview.net/forum?id=Fn9ORH8PLl) |  | 0 | This paper tackles the scarcity of benchmarking data in disentangled auditory representation learning. We introduce \*\*SynTone\*\*, a synthetic dataset with explicit ground truth explanatory factors for evaluating disentanglement techniques. Benchmarking state-of-the-art methods on SynTone... | Gunther Heidemann, Simone Pika, Ulf Krumnack, Yusuf Brima |  |
| 23 |  |  [Collapse of Self-trained Language Models](https://openreview.net/forum?id=DeuBfEWyR4) |  | 0 | In various fields of knowledge creation, including science, new ideas often build on pre-existing information. In this work, we explore this concept within the context of language models. Specifically, we explore the potential of self-training models on their own outputs, akin to how humans learn... | David Herel, Tomás Mikolov |  |
| 24 |  |  [KFC: Knowledge Reconstruction and Feedback Consolidation Enable Efficient and Effective Continual Generative Learning](https://openreview.net/forum?id=pVTcR8ig3R) |  | 0 | To address the issues of catastrophic forgetting in Continual Generative Learning (CGL), dominant methods leverage the generative replay strategy. However, they often suffer from high time complexity and inferior generative sample quality. In this work, we develop an efficient and effective CGL... | Libo Huang, Xiang Zhi, Yan Zeng, Yongjun Xu, Zhulin An |  |
| 25 |  |  [Rescaling Intermediate Features Makes Trained Consistency Models Perform Better](https://openreview.net/forum?id=1o3LnwflAl) |  | 0 | In the domain of deep generative models, diffusion models are renowned for their high-quality image generation but are constrained by intensive computational demands. To mitigate this, consistency models have been proposed as a computationally efficient alternative. Our research reveals that... | Enshu Liu, Junyi Zhu, Matthew B. Blaschko, Xuefei Ning, Zinan Lin |  |
| 26 |  |  [DFWLayer: Differentiable Frank-Wolfe Optimization Layer](https://openreview.net/forum?id=XoPQqUctS1) |  | 0 | Differentiable optimization has received a significant amount of attention due to its foundational role in the domain of machine learning based on neural networks. This paper proposes a differentiable layer, named Differentiable Frank-Wolfe Layer (DFWLayer), by rolling out the Frank-Wolfe method, a... | Liu Liu, Peilin Zhao, Xueqian Wang, Zixuan Liu |  |
| 27 |  |  [3D Shape Completion via Sparse Irregular Representation](https://openreview.net/forum?id=mBQYpnq3Pj) |  | 0 | The task of 3D shape completion involves completing missing regions of an object from partial observation. The current methods accomplish this task by modeling latent completion distributions based on an autoregressive model. However, this approach often struggles with geometric details, as it... | Jiahui Li, Pourya Shamsolmoali |  |
| 28 |  |  [Dissecting Zero-Shot Visual Reasoning Capabilities in Vision and Language Models](https://openreview.net/forum?id=jM1nZDBIto) |  | 0 | Vision-language models (VLMs) have shown impressive zero- and few-shot performance on real-world visual question answering (VQA) benchmarks, alluding to their capabilities as visual reasoning engines. However, existing works (typically) use benchmarks that conflate “pure” visual reasoning with... | Aishik Nagar, Cheston Tan, Shantanu Jaiswal |  |
| 29 |  |  [MosquitoFusion: A Multiclass Dataset for Real-Time Detection of Mosquitoes, Swarms, and Breeding Sites Using Deep Learning](https://openreview.net/forum?id=3s4hFx8pYs) |  | 0 | In this paper, we present an integrated approach to real-time mosquito detection using our multiclass dataset (MosquitoFusion) containing 1204 diverse images and leverage cutting-edge technologies, specifically computer vision, to automate the identification of Mosquitoes, Swarms, and Breeding... | Fahim Hafiz, Md Ashiqur Rahman, Md. Faiyaz Abdullah Sayeedi |  |
| 30 |  |  [Nonlinear model reduction for operator learning](https://openreview.net/forum?id=Jw6TUpB7Rw) |  | 0 | Operator learning provides methods to approximate mappings between infinite-dimensional function spaces. Deep operator networks (DeepONets) are a notable architecture in this field. Recently, an extension of DeepONet based on model reduction and neural networks, proper orthogonal decomposition... | Andreas Rausch, Hamidreza Eivazi, Stefan H. A. Wittek |  |
| 31 |  |  [Logic-guided Deep Reinforcement Learning for Stock Trading](https://openreview.net/forum?id=SPDNi5Ys8R) |  | 0 | Previous state-of-the-art trading strategy proposes using ensemble reinforcement learning to combine the advantages of different subpolicies. Despite its improved performance, we observe that this policy is still quite sensitive to market volatility. In this work, we propose a novel framework... | Aixin Cui, Bo Li, Bozhi Wu, Junzhe Jiang, Yang Liu, Yushi Cao, Zhiming Li |  |
| 32 |  |  [A Shared Encoder for Multi-Source Hyperspectral Images](https://openreview.net/forum?id=dIThDxLwA1) |  | 0 | Multi-source hyperspectral images(HSIs) which captured from diverse sensors commonly possess varying bands. When employing deep learning techniques for their processing, individual models are necessitated for each source due to the disparate dimensions. To tackle this problem, we propose a shared... | Baisen Liu, Jiaming Pei, Weili Kong, Xiaojun Bi |  |
| 33 |  |  [Transfer Learning for Global Feature Importance Measurements](https://openreview.net/forum?id=EExn3iNKs3) |  | 0 | Understanding feature importance is crucial for conducting interpretable clinical decision-making. However, the reliability of such analyses can be heavily impacted by the available sample size, placing sites with lower data quality and smaller sample sizes at inherent disadvantages. To address the... | Kunyu Yu, Qiming Wu, Siqi Li, Xin Li |  |
| 34 |  |  [Enhancing Spiking Transformers with Binary Attention Mechanisms](https://openreview.net/forum?id=6X3TNqLb5t) |  | 0 | Spiking Neural Networks (SNNs) are increasingly recognized as an efficient alternative to traditional artificial neural networks. Recent advancements, particularly the integration of SNNs with Transformer structures to create 'SpikFormer', have significantly enhanced the performance of SNNs.... | Dongcheng Zhao, Guobin Shen, Sicheng Shen, Yi Zeng |  |
| 35 |  |  [Key Patch Proposer: Key Patches Contain Rich Information](https://openreview.net/forum?id=NHm3OB6bIG) |  | 0 | In this paper, we introduce a novel algorithm named Key Patch Proposer (KPP) designed to select key patches in an image without additional training. Our experiments showcase KPP's robust capacity to capture semantic information by both reconstruction and classification tasks. The efficacy of KPP... | Beiwen Tian, Hao Zhao, Jing Xu |  |
| 36 |  |  [Hallucination Benchmark in Medical Visual Question Answering](https://openreview.net/forum?id=vxlXqOj4zv) |  | 0 | The recent success of large language and vision models (LLVMs) on vision question answering (VQA), particularly their applications in medicine (Med-VQA), has shown a great potential of realizing effective visual assistants for healthcare. However, these models are not extensively tested on the... | Honghan Wu, Jinge Wu, Yunsoo Kim |  |
| 37 |  |  [Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian Perspective](https://openreview.net/forum?id=AfVtVrCH9U) |  | 0 | This paper proposes an interpretation of RLAIF as Bayesian inference by introducing distilled Self-Critique (dSC), which refines the outputs of a LLM through a Gibbs sampler that is later distilled into a fine-tuned model. Only requiring synthetic data, dSC is exercised in experiments regarding... | Víctor Gallego |  |
| 38 |  |  [Parameter and Data Efficient Spectral Style-DCGAN](https://openreview.net/forum?id=JQlfTmlHNz) |  | 0 | We present a simple, highly parameter, and data-efficient adversarial network for unconditional face generation. Our method: Spectral Style-DCGAN or SSD utilizes only 6.574 million parameters and 4739 dog faces from the Animal Faces HQ (AFHQ) dataset as training samples while preserving fidelity at... | Aryan Garg |  |
| 39 |  |  [G-PECNet: Towards a Generalizable Pedestrian Trajectory Prediction System](https://openreview.net/forum?id=wSWk1sNK0m) |  | 0 | Navigating dynamic physical environments without obstructing or damaging human assets is of quintessential importance for social robots. In this work, we solve autonomous drone navigation's sub-problem of predicting out-of-domain human and agent trajectories using a deep generative model. Our... | Aryan Garg, Renu Rameshan |  |
| 40 |  |  [Colorful Cutout: Enhancing Image Data Augmentation with Curriculum Learning](https://openreview.net/forum?id=mTF4OcDnSP) |  | 0 | Data augmentation is one of the regularization strategies for the training of deep learning models, which enhances generalizability and prevents overfitting, leading to performance improvement. Although researchers have proposed various data augmentation techniques, they often lack consideration... | Juhwan Choi, YoungBin Kim |  |
| 41 |  |  [On Difficulties of Attention Factorization through Shared Memory](https://openreview.net/forum?id=pexcddsXGY) |  | 0 | Transformers have revolutionized deep learning in numerous fields, including natural language processing, computer vision, and audio processing. Their strength lies in their attention mechanism, which allows for the discovering of complex input relationships. However, this mechanism's quadratic... | David Herel, Martin Holena, Ondrej Bojar, Uladzislau Yorsh |  |
| 42 |  |  [Bad Predictive Coding Activation Functions](https://openreview.net/forum?id=5Vc1ACZ8eC) |  | 0 | We investigate predictive coding networks (PCNs) by analyzing their performance under different activation function choices. We expand a previous theoretical discussion of a simple toy example of PCN in the training stage. Compared to classic gradient-based empirical risk minimization, we observe... | Luca Pinchetti, Simon Frieder, Thomas Lukasiewicz |  |
| 43 |  |  [An Evaluation Benchmark for Autoformalization in Lean4](https://openreview.net/forum?id=22ITxc8y5p) |  | 0 | In the advancing field of computational mathematics, Large Language Models (LLMs) hold the potential to revolutionize autoformalization, a process crucial across various disciplines. The introduction of Lean4, a mathematical programming language, presents an unprecedented opportunity to rigorously... | Aryan Gulati, Brando Miranda, Devanshu Ladsaria, Jasdeep Sidhu, Shubhra Mishra |  |
| 44 |  |  [Software 1.0 Strengths for Interpretability and Data Efficiency](https://openreview.net/forum?id=gyl8r8ANcd) |  | 0 | Machine learning has demonstrated remarkable capabilities across various tasks, yet it confronts significant challenges such as limited interpretability, reliance on extensive data, and difficulties in incorporating human intuition. In contrast, traditional software development avoids these... | Arshia Soltani Moakhar, Maral Jabbarishiviari |  |
| 45 |  |  [Sailing Through Spectra: Unveiling the Potential of Multi-Spectral Information in Marine Debris Segmentation](https://openreview.net/forum?id=tJPLJS97X4) |  | 0 | Plastic debris in ocean waters poses ecological and economic challenges. Addressing this issue begins with estimating plastic distribution in oceans for effective policy and awareness efforts. Traditional monitoring methods are costly and labour-intensive, with limited coverage. Deep learning... | Aditya Kasliwal, Bharath Udupa, Dyutit Mohanty, Pratinav Seth |  |
| 46 |  |  [Network Inversion of Binarised Neural Nets](https://openreview.net/forum?id=zKcB0vb7qd) |  | 0 | While the deployment of neural networks, yielding impressive results, becomes more prevalent in various applications, their interpretability and understanding remain a critical challenge. Network inversion, a technique that aims to reconstruct the input space from the model’s learned internal... | Pirzada Suhail |  |
| 47 |  |  [Uncovering Bias: Exploring Gender Dynamics in Distance-Aware Mixup Techniques](https://openreview.net/forum?id=n8Yv4R6132) |  | 0 | Bias is a pervasive issue in machine learning and has implications in multiple AI applications, encompassing dimensions like gender, age, demographics, and social aspects. Complex models, including deep neural networks, transformers etc., often inherit biases and stereotypes during training,... | Parth Chhabra, Ramit Sawhney, Samyak Jain |  |
| 48 |  |  [Empirical Study on Updating Key-Value Memories in Transformer Feed-forward Layers](https://openreview.net/forum?id=WSl84nwG7i) |  | 0 | The feed-forward networks (FFNs) in transformers are recognized as a group of key-value neural memories to restore abstract high-level knowledge. In this work, we conduct an empirical ablation study on updating keys (the 1st layer in the FFNs layer) or values (the 2nd layer in the FFNs layer). We... | Jie Fu, Youcheng Huang, Zeyu Huang, Zihan Qiu |  |
| 49 |  |  [When SMILES have Language: Drug Classification using Text Classification Methods on Drug SMILES Strings](https://openreview.net/forum?id=VUYCyH8fCw) |  | 0 | Complex chemical structures, like drugs, are usually defined by SMILES strings as a sequence of molecules and bonds. These SMILES strings are used in different complex machine learning-based drug-related research and representation works. Escaping from complex representation, in this work, we pose... | Azmine Toushik Wasi, DongKyu Chae, Karlo Serbetar, Raima Islam, Taki Hasan Rafi |  |
| 50 |  |  [Multilingual Prosody Transfer: Comparing Supervised & Transfer Learning](https://openreview.net/forum?id=DKF7YCwCmd) |  | 0 | The field of prosody transfer in speech synthesis systems is rapidly advancing. This research is focused on evaluating learning methods for adapting pre-trained monolingual text-to-speech (TTS) models to multilingual conditions, i.e., Supervised Fine-Tuning (SFT) and Transfer Learning (TL). This... | Anubha Gupta, Arnav Goel, Medha Hira |  |
| 51 |  |  [Region Mixup](https://openreview.net/forum?id=2qPBw8OuU7) |  | 0 | This paper introduces a simple extension of mixup data augmentation to enhance generalization in visual recognition tasks. Unlike the vanilla mixup method, which blends entire images, our approach focuses on combining regions from multiple images. | Saptarshi Saha, Utpal Garain |  |
| 52 |  |  [Design of a molecular exchange-based robust perceptron for biomolecular neural network](https://openreview.net/forum?id=AXwGBliKOV) |  | 0 | A molecular perceptron is of immense interest due to its computing and classification ability in biophysical and aqueous environments. Because such a perceptron relies on biochemical interactions, it must adapt to perturbations and be resilient against stochastic fluctuations to maintain faithful... | Md. Shahriar Karim, Moshiur Rahman, Muhtasim Ishmum Khan |  |
| 53 |  |  [The IMO Small Challenge: Not-Too-Hard Olympiad Math Datasets for LLMs](https://openreview.net/forum?id=HYuKXhgzjm) |  | 0 | We introduce the IMO Small Challenge (IMOSC), as opposed to the IMO Grand Challenge: A text-only, natural-language dataset consisting of mathematical problems from various mathematical competitions. The IMOSC dataset exceeds the difficulty level of current datasets that are widely used for LLM... | Julius Berner, Mirek Olsák, Simon Frieder, Thomas Lukasiewicz |  |
| 54 |  |  [Dynamic Activations for Neural Net Training](https://openreview.net/forum?id=F7rBDSxIMs) |  | 0 | Recent advancements in deep learning have seen breakthroughs in training algorithms, benefiting speech, text, image, and video processing. While deeper architectures like ResNet have made strides, shallow Convolutional Neural Networks (CNNs) remain underexplored. Activation functions, pivotal for... | Chinmay Rane, Kanishka Tyagi, Nirmala Murali, Tushar Chugh |  |
| 55 |  |  [Non Parametric Aleatoric Uncertainty Quantification with Neural Networks](https://openreview.net/forum?id=o5q2VsCMXo) |  | 0 | Classic methods for aleatoric uncertainty quantification in regression settings make assumptions about the distribution of noise in the dependent variable. Incorrect assumptions can lead to poor model performance and unreliable uncertainty estimates. In this paper, we introduce a simple method for... | Debayan Gupta, Kshitij Kapoor |  |
| 56 |  |  [U2NeRF: Unsupervised Underwater Image Restoration and Neural Radiance Fields](https://openreview.net/forum?id=d8nXcePud3) |  | 0 | Underwater images suffer from colour shifts, low contrast, and haziness due to light absorption, refraction, scattering and restoring these images has warranted much attention. In this work, we present $\textbf{U2NeRF}$, a transformer-based architecture that learns to render and restore novel views... | Kaushik Mitra, Manoj S., Mukund Varma T., Vinayak Gupta |  |
| 57 |  |  [Beyond Uniform Scaling: Exploring Depth Heterogeneity in Neural Architectures](https://openreview.net/forum?id=mURVIdmojf) |  | 0 | Conventional scaling of neural networks typically involves designing a base network and growing different dimensions like width, depth, etc. of the same by some predefined scaling factors. We introduce an automated scaling approach leveraging second-order loss landscape information. Our method is... | Akash Guna R. T., Arnav Chavan, Deepak K. Gupta |  |
| 58 |  |  [A Bi-Objective $\epsilon $-Constrained Framework for Quality-Cost Optimization in Language Model Ensembles](https://openreview.net/forum?id=P7LrBatrOR) |  | 0 | We propose an ensembling framework that uses diverse open-sourced Large Language Models (LLMs) to achieve high response quality while maintaining cost efficiency. We formulate a bi-objective optimization problem to represent the quality-cost tradeoff and then introduce an additional budget... | Aditi Singla, Aditya Singh, Kanishk Kukreja |  |
| 59 |  |  [Zero-shot generalization across architectures for visual classification](https://openreview.net/forum?id=orYMrUv7eu) |  | 0 | Generalization to unseen data is a key desideratum for deep networks, but its relation to classification accuracy is unclear. Using a minimalist vision dataset and a measure of generalizability, we show that popular networks, from deep convolutional networks (CNNs) to transformers, vary in their... | Evan Gerritz, Luciano Dyballa, Steven W. Zucker |  |
| 60 |  |  [The Duality of Hope: A Critical Examination of Controversial Annotations in HopeEDI](https://openreview.net/forum?id=r6QZ8YKSBd) |  | 0 | This study investigates the HopeEDI hope speech dataset, revealing a significant number of potentially controversial annotations, notably tied to the 'All Lives Matter' movement. We have also identified instances where hateful/toxic/implicitly controversial content was wrongly marked as hopeful.... | Diksha Sethi, Mohammad Aflah Khan, Neemesh Yadav, Raghav Sahni |  |
| 61 |  |  [DynamicPoseNet: Advanced Human Motion Generation with Dual-Pathway CNNs and LoRA-Enhanced LLaMA](https://openreview.net/forum?id=u6gVK59f4d) |  | 0 | This study introduces DynamicPoseNet, a novel convolutional neural network architecture leveraging depthwise separable convolutions and dual-pathway feature extraction for advanced human motion generation. Using large-scale datasets such as HumanML3D and KIT-ML, DynamicPoseNet efficiently... | Jin Xu, Zundong Wu |  |
| 62 |  |  [CrossVoice: Crosslingual Prosody Preserving Cascade-S2ST using Transfer Learning](https://openreview.net/forum?id=zEdBzTxXHl) |  | 0 | This paper presents CrossVoice, a novel cascade-based Speech-to-Speech Translation (S2ST) system employing advanced ASR, MT, and TTS technologies with cross-lingual prosody preservation through transfer learning. We conducted comprehensive experiments comparing CrossVoice with direct-S2ST systems,... | Anubha Gupta, Arnav Goel, Medha Hira |  |
| 63 |  |  [Beyond Words: A Topological Exploration of Coherence in Text Documents](https://openreview.net/forum?id=QJxVhljAyS) |  | 0 | Coherence serves as a pivotal metric in evaluating the quality of a text. It quantifies how well the sentences within the text are connected and how well the text is structured and organized. It plays a vital role in various downstream Natural Language Processing tasks such as text summarization,... | Rajiv Ratn Shah, Rishi Singhal, Samyak Jain, Sriram Krishna, Yaman Kumar Singla |  |
| 64 |  |  [Exploring Loss Design Techniques For Decision Tree Robustness To Label Noise](https://openreview.net/forum?id=B8H7pqlj3N) |  | 0 | In the real world, data is often noisy, affecting not only the quality of features but also the accuracy of labels. Current research on mitigating label errors stems primarily from advances in deep learning, and a gap exists in exploring interpretable models, particularly those rooted in decision... | Artur Dubrawski, Jack Henry Good, Lukasz Sztukiewicz |  |
| 65 |  |  [Performance Analysis of a quantum-Classical Hybrid Reinforcement Learning Approach](https://openreview.net/forum?id=1POy0o0Z8j) |  | 0 | Quantum Machine Learning (QML) is a nascent field of technology that is yet to be fully explored. While previous QML implementations have demonstrated performance efficiency gains over classical benchmarks, it has not been studied in detail whether shallow unentangled quantum circuits can provide... | Biswajit Basu, Evan Mitchell, Pabitra Mitra |  |
| 66 |  |  [Bypassing the Safety Training of Open-Source LLMs with Priming Attacks](https://openreview.net/forum?id=nz8Byp7ep6) |  | 0 | With the recent surge in popularity of LLMs has come an ever-increasing need for LLM safety training. In this paper, we investigate the fragility of SOTA open-source LLMs under simple, optimization-free attacks we refer to as \*priming attacks\*, which are easy to execute and effectively bypass... | Changming Xu, Gagandeep Singh, Isha Chaudhary, Jason Vega |  |
| 67 |  |  [No More BLAH-BLAH: Embracing Real Text in the Image synthesis World](https://openreview.net/forum?id=qjrvRK24S0) |  | 0 | The integration of text onto objects within an image frequently results in an unnatural appearance, where the text appears overlaid rather than seamlessly embedded. Existing text-to-image models often encounter challenges in accurately incorporating text within an image context. This paper... | Aref Tabatabaei, Maryam Amirmazlaghani, Negar Movaghatian, Zahra Dehghanian |  |
| 68 |  |  [Borderline Sample Extraction from a Trained Classifier](https://openreview.net/forum?id=xCCNAy8mQK) |  | 0 | Extracting pseudo samples from a trained classifier helps understand classifier decisions, and extracted samples also can assist downstream tasks like knowledge distillation, continual learning, etc. Existing works mostly focus on extracting exemplary samples, i.e., samples that carry salient... | Borui Cai, Longxiang Gao, Yong Xiang, Zihao Johnson Zheng |  |
| 69 |  |  [Self-Teaching Prompting for Multi-Intent Learning with Limited Supervision](https://openreview.net/forum?id=DeoamI1BFh) |  | 0 | Multi-intent learning with limited supervision involves predicting multiple intentions of utterances using only a few annotated samples. The primary motivation for this task stems from the high costs and cumbersome processes associated with annotating large datasets. To mitigate this, we propose... | Cheng Chen, Ivor W. Tsang |  |
| 70 |  |  [Knowledge Distillation Through Time For Future Event Prediction](https://openreview.net/forum?id=JBSMl0nAFa) |  | 0 | Is it possible to learn from the future? Here, we introduce knowledge distillation through time (KDTT). In traditional knowledge distillation (KD), a reliable teacher model is used to train an error-prone student model. The difference between the teacher and student is typically model capacity; the... | Jason Eshraghian, Ruomin Zhu, Skye Gunasekaran, Zdenka Kuncic |  |
| 71 |  |  [Can Perplexity Reflect Large Language Model's Ability in Long Text Understanding?](https://openreview.net/forum?id=Cjp6YKVeAa) |  | 0 | Recent studies have shown that Large Language Models (LLMs) have the potential to process extremely long text with evidence that LLMs could perform well in the language modeling task with even 1 million input tokens. When the input context length increases, the perplexity (PPL) of the model is... | Chen Zhang, Mingxu Tao, Quzhe Huang, Yansong Feng, Yutong Hu |  |
| 72 |  |  [Session-Aware Product filter Ranking in E- Commerce Search](https://openreview.net/forum?id=r4LMF2IJ6R) |  | 0 | Product filters are commonly used by e-commerce websites to refine search results based on attribute values such as price, brand, size, etc. However, existing filter recommendation approaches typically generate filters independently of the user's search query or browsing history. This can lead to... | Chen Luo, Haiyang Zhang, Hanqing Lu, Limeng Cui, Monica Xiao Cheng, Rahul Goutam, Xianfeng Tang, Zhenwei Dai |  |
| 73 |  |  [Can Speculative Sampling Accelerate ReAct Without Compromising Reasoning Quality?](https://openreview.net/forum?id=42b9hJrIpX) |  | 0 | Large language models (LLMs) are increasingly used as agents for interaction with external environments. These interplays are commonly facilitated through various prompting paradigms. However, such paradigms require extended interaction traces between the LLMs and the environment, resulting in low... | Haipeng Chen, Han Xu, Jingyang Ye, Yutong Li |  |
| 74 |  |  [Rethinking Compression: Reduced order modelling of Latent Features in Large Language Models](https://openreview.net/forum?id=BfVccaZiEv) |  | 0 | Due to the substantial scale of Large Language Models (LLMs), the direct application of conventional compression methodologies proves impractical. The computational demands associated with even minimal gradient updates present challenges, particularly on consumer-grade hardware. This paper... | Arnav Chavan, Deepak K. Gupta, Nahush Lele |  |
| 75 |  |  [Geometric Implications of Classification on Reducing Open Space Risk](https://openreview.net/forum?id=vYdLXrToTG) |  | 0 | To reduce open space risk of hypotheses, we reexamine the 'simplest' hypothesis class, binary linear classifiers, geometrically. Generalizing linear classification, we establish a surprising fact: linear classifiers can have arbitrarily high VC dimension, stemming from increasing the number of... | Athanasios P. Meliopoulos, Leyan Pan, Matthew Lau, Stefan Davidov, Wenke Lee |  |
| 76 |  |  [Role of Over-Parameterization in Generalization of 3-layer ReLU Networks](https://openreview.net/forum?id=EarRzo8ixe) |  | 0 | Over-parameterized neural networks defy conventional wisdom by generalizing effectively; however, standard complexity metrics like norms and margins fail to account for this. A recent work introduced a novel measure considering unit-wise capacities and provided a better explanation and tighter... | Aditya Golatkar, Avijit Verma, Simranjit Singh |  |
| 77 |  |  [Density-Preserving Heterogeneous Graph Sparsification for Representation Learning](https://openreview.net/forum?id=sISO5ubRGP) |  | 0 | Graph sparsification is the task of compressing a graph with fewer edges or nodes while preserving its essential structural characteristics. It has been used in machine learning to significantly improve the computational efficiency over homogeneous graphs. In heterogeneous graphs with diverse types... | Chunjiang Zhu, Srilekha Geda |  |
| 78 |  |  [Toward Computationally Efficient Inverse Reinforcement Learning via Reward Shaping](https://openreview.net/forum?id=hhsOkmgDEz) |  | 0 | Inverse reinforcement learning (IRL) is computationally challenging, with common approaches requiring the solution of multiple reinforcement learning (RL) sub-problems. This work motivates the use of potential-based reward shaping to reduce the computational burden of each RL sub-problem. This work... | Cassidy Laidlaw, Edwin Zhang, Finale DoshiVelez, Harvey Klyne, Lauren H. Cooke, Milind Tambe |  |
| 79 |  |  [Jensen-Shannon Divergence in Safe Multi- Agent RL](https://openreview.net/forum?id=PRBspmgNkY) |  | 0 | Reinforcement Learning (RL) has achieved significant milestones, however its safety remains a concern for real-world applications. Safe RL solutions focus on maximizing environment rewards while minimizing cost. In this work, we extend the Multi-Agent Constrained Policy Optimisation (MACPO)... | Prabhdeep Singh Sethi, Roshan Roy, Rushikesh Zawar |  |
| 80 |  |  [Exploiting Time Channel Vulnerability of Learned Bloom Filters](https://openreview.net/forum?id=jHRWVA1H0f) |  | 0 | Neural network for computer systems—such as operating systems, databases, and network systems—attract much attention. However, using neural networks in systems introduces new attacking surfaces. This paper makes the first attempt to study the security factor of learned bloom filters, a promising... | Cheng Tan, Gagandeep Singh, Harman Singh Farwah |  |
| 81 |  |  [Small Transformers, Big Results: Efficient Diffusion with Parameter Sharing](https://openreview.net/forum?id=owfuhF0bT9) |  | 0 | The interplay between model depth, computational complexity, and parameter count remains an intricate aspect of neural network design. We propose a novel block sharing mechanism for denoising diffusion generative models, enabling us to maintain or even improve model quality while reducing parameter... | Daniel Z. Kaplan, Mohamed Osman |  |
| 82 |  |  [KLCE: Regularized Imbalance Node-classification Via KL-divergence and Cross-Entropy](https://openreview.net/forum?id=JuqfAXWHfr) |  | 0 | This paper introduces a novel regularization based on KL-divergence and cross-entropy for imbalance node classification via Graph neural networks. We evaluate the performance of our approach on several benchmark datasets and compare it with state-of-the-art methods. The experimental results... | Gholamali Aminian, Hamid R. Rabiee, Mohammad Taha Teimuri Jervakani, Zahra Dehghanian |  |
| 83 |  |  [Affinity-based Homophily: Can we measure homophily of a graph without using node labels?](https://openreview.net/forum?id=IsdDOrAowN) |  | 0 | The homophily (heterophily) ratio in a graph represents the proportion of edges connecting nodes with similar (dissimilar) class labels. Existing methods for estimating the homophily ratio typically rely on knowing the class labels of each node in the graph. While several algorithms address both... | Indranil Ojha, Kushal Bose, Swagatam Das |  |
| 84 |  |  [Can Decoupling Embedded Text from Images Improve Multimodal Learning?](https://openreview.net/forum?id=uF7ZWntl3m) |  | 0 | Multimodal models have widely been used to process text-embedded images on social media. However, the effect of embedded text on the image encoding process remains unexplored. In this work, we eliminated the text in text-embedded images and compared the intervention's effect on the performance of... | Siddhant Bikram Shah |  |
| 85 |  |  [Cognitive resilience: Unraveling the proficiency of image-captioning models to interpret masked visual content](https://openreview.net/forum?id=RQ0wmIBcTB) |  | 0 | This study explores the ability of Image Captioning (IC) models to decode masked visual content sourced from diverse datasets. Our findings reveal the IC model's capability to generate captions from masked images, closely resembling the original content. Notably, even in the presence of masks, the... | Huazhang Ying, Likun Zhang, Peiwu Qin, Zhaotian Xie, Zhicheng Du |  |
| 86 |  |  [Revelio: Interpretable Long-Form Question Answering](https://openreview.net/forum?id=fyvEJXsaQf) |  | 0 | The black-box architecture of pretrained language models (PLMs) hinders the interpretability of lengthy responses in long-form question answering (LFQA). Prior studies use knowledge graphs (KGs) to enhance output transparency, but mostly focus on non-generative or short-form QA. We present Revelio,... | Davide Freddi, Fabian Vincenzi, Gianluca Moro, Lorenzo Valgimigli, Luca Ragazzi |  |
| 87 |  |  [Loss-Free Machine Unlearning](https://openreview.net/forum?id=bCPz7uqmmh) |  | 0 | We present a machine unlearning approach that is both retraining- and label-free. Most existing machine unlearning approaches require a model to be fine-tuned to remove information while preserving performance. This is computationally expensive and necessitates the storage of the whole dataset for... | Alexandra Brintrup, Jack Foster, Stefan Schoepf |  |
| 88 |  |  [How Many OptiFaces? A New Evaluation Metric For 3D Face Reconstruction](https://openreview.net/forum?id=KzNM3S3IDT) |  | 0 | Three dimensional face reconstruction is a challenging problem, so much so that the mean face is highly competitive with recent learning-based approaches for 3D face reconstruction from 2D images. No other universal baselines for this task exist. We propose a novel baseline that selects a subset of... | Andrew Keeling, Nick E. Pears, Patrik Huber, Will Rowan |  |
| 89 |  |  [Sequence Mixup for Zero-Shot Cross-Lingual Part-of-speech Tagging](https://openreview.net/forum?id=H6gQ9iOS1X) |  | 0 | There have been efforts in cross-lingual transfer learning for various tasks. We present an approach utilizing an interpolative data augmentation method, Mixup, to improve the generalizability of models for part-of-speech tagging trained on a source language, improving its performance on unseen... | Megh Thakkar, Ramit Sawhney |  |
| 90 |  |  [A Novel Window-Interaction Module Based on W-MSA](https://openreview.net/forum?id=ki4R0z0C4K) |  | 0 | W-MSA proposed by Swin Transformer has limitations in facilitating information interaction between windows. To address this, we introduce a module that utilizes convolution to achieve inter-window information interaction across different regions. Experiments demonstrate that our proposed module,... | Mingjun Ni, Ruochen Cui, Yongding Tao |  |
| 91 |  |  [Mask2tasks: Leveraging Segmentation to Enhance Classification Performance in histopathological colorectal Images](https://openreview.net/forum?id=WMxXuVTzFm) |  | 0 | In this study, we explore the enhancement of colorectal image classification accuracy with the aid of a segmentation task. We introduce Mask2Tasks, a deep neural network for joint colorectal image classification and segmentation which is trained using a novel two-stage training approach. Numerical... | Hieu Le Xuan, Hoang Vu Huy, Huy Phan Quang, Minh Hoang Le, Viet V. Truong |  |
| 92 |  |  [A Framework for Policy Evaluation Enhancement by Diffusion Models](https://openreview.net/forum?id=QTE2kMP4DJ) |  | 0 | Reinforcement learning plays an important role in various fields, and has fast development in policy evaluation and learning methods, enjoying the advantages of large data size. However, when data are limited, directly applying evaluation methods does not necessarily result in a good policy... | Tao Ma, Xuzhi Yang |  |
| 93 |  |  [FeedFace: Efficient Inference-based Face Personalization via Diffusion Models](https://openreview.net/forum?id=PqPKBcamy3) |  | 0 | We introduce FeedFace, a novel inference-based method designed to augment text-to-image diffusion models with face-based conditional generation. Trained on a thoroughly curated and annotated dataset of diverse human faces, FeedFace operates without additional training for new facial conditions... | Armando Teles Fortes, Chendong Xiang, Hang Su, Jun Zhu, Khang Hui Chua |  |
| 94 |  |  [Knowledge Graph Unlearning to Defend Language Model Against Jailbreak Attack](https://openreview.net/forum?id=gqTUIesy4H) |  | 0 | Large language models (LLMs) are vulnerable to jailbreak attacks that bypass safety measures and induce LLMs to generate harmful content. There is a notable dearth of research on defense mechanisms against jailbreak attack, especially attacks that leverage fine-tuning techniques on open-access... | Hao Jiang, Peihua Mai, Ran Yan, Yan Pang, Youjia Yang, Zhe Huang |  |
| 95 |  |  [Partial Rankings of Optimizers](https://openreview.net/forum?id=mhwNoQcEjQ) |  | 0 | We introduce a framework for benchmarking optimizers according to multiple criteria over a collection of test functions. Based on a recently introduced union-free generic depth function for partial orders/rankings, it fully exploits the ordinal information and allows for incomparability. Our method... | Hannah Blocher, Julian Rodemann |  |
| 96 |  |  [LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked](https://openreview.net/forum?id=YoqgcIA19o) |  | 0 | Large language models (LLMs) are popular for high-quality text generation but can also produce harmful responses as adversarial prompts can bypass their safety measures. We propose LLM Self Defense, a simple approach to defend against these attacks by having an LLM screen the induced responses,... | Alec Helbling, Cory Cornelius, Duen Horng Chau, Mansi Phute, Matthew Hull, Sebastian Szyller, Shengyun Peng |  |
| 97 |  |  [Cognitive Reframing via Large Language Models for Enhanced Linguistic Attributes](https://openreview.net/forum?id=IIus8CSwsU) |  | 0 | Cognitive Reframing, a core technique in Cognitive Behavioral Therapy (CBT), seeks to enhance mental well-being. While previous research has highlighted the efficacy of Large Language Models (LLMs) for cognitive reframing, there has been limited focus on enhancing reframing quality across multiple... | Dharmendra Sharma, Dinesh Kumar, Xiaomeng Wang |  |
| 98 |  |  [DSF-GAN: Downstream Feedback Generative Adversarial Network](https://openreview.net/forum?id=Vfp8jhwcCc) |  | 0 | Utility and privacy are two crucial measurements of synthetic tabular data. While privacy measures have been dramatically improved with the use of Generative Adversarial Networks (GANs), generating high-utility synthetic samples remains challenging. To increase the samples' utility, we propose a... | Nadav Rappoport, Oriel Perets |  |
| 99 |  |  [Improving Automated Speech Recognition Using Retrieval-Based Voice Conversion](https://openreview.net/forum?id=OMBFB6pU6c) |  | 0 | This study examines the efficacy of voice conversion techniques in enhancing Automatic Speech Recognition (ASR) accuracy for non-native English speakers. Utilizing the OpenAI Whisper models, we analyzed transcription accuracy across various accents and countries. Significant reductions in Word... | Ali Alzahrani, Anas Mohammed Alhumud, Muhammad AlQurishi, Riad Souissi, Yasser Omar Alomar |  |
| 100 |  |  [Doc2Command: Furthering Language Guided Document Editing](https://openreview.net/forum?id=inQ9bW5AQz) |  | 0 | Language guided document editing is a novel task that includes generating a machine parsable command and a bounding box from an open vocabulary user request. This paper introduces Doc2Command, a multi-task, multimodal model that unifies the document and user request into a singular visual modality... | Dinesh Manocha, Manan Suri, Preslav Nakov, Puneet Mathur, Ramit Sawhney |  |
| 101 |  |  [Neural Control System for Continuous Glucose Monitoring and Maintenance](https://openreview.net/forum?id=Te4P3Cn54g) |  | 0 | Precise glucose level monitoring is critical for people with diabetes to avoid serious complications. While there are several methods for continuous glucose level monitoring, research on maintenance devices is limited. To mitigate the gap, we provide a novel neural control system for continuous... | Azmine Toushik Wasi |  |
| 102 |  |  [Enhancing Language Models for Financial Relation Extraction with Named Entities and Part-of-Speech](https://openreview.net/forum?id=BAR6OE80OW) |  | 0 | The Financial Relation Extraction (FinRE) task involves identifying the entities and their relation, given a piece of financial statement/text. To solve this FinRE problem, we propose a simple but effective strategy that improves the performance of pre-trained language models by augmenting them... | Kwan Hui Lim, Menglin Li |  |
| 103 |  |  [Adaptive Brain Network Augmentation Based on Group-aware Graph Learning](https://openreview.net/forum?id=29N5YY0OuO) |  | 0 | Brain network analysis significantly improves artificial intelligence techniques in the realm of digital health. Most existing methods uniformly construct brain networks for different groups (e.g., male and female groups, healthy and sick people groups), facing the interference of group-irrelevant... | Chenxuan Meng, Ciyuan Peng, Feng Xia, Mujie Liu, Shuo Yu |  |
| 104 |  |  [Generating Counterfactual Explanations Using Cardinality Constraints](https://openreview.net/forum?id=4hmAp3Ca3o) |  | 0 | Providing explanations about how machine learning algorithms work and/or make particular predictions is one of the main tools that can be used to improve their trusworthiness, fairness and robustness. Among the most intuitive type of explanations are counterfactuals, which are examples that differ... | Rubén RuizTorrubiano |  |
| 105 |  |  [Explicitly Stating Assumptions Reduces Hallucinations in Natural Language Inference](https://openreview.net/forum?id=eJI9pfNwBS) |  | 0 | A natural language inference (NLI) model might hold a hallucination that a 'premise' infers a 'hypothesis'. This hallucination is possibly not due to insufficient training on the data but rather is inherent in the data themselves. A training label might suggest that 'a premise infers a hypothesis',... | Kwan Hui Lim, Wenchuan Mu |  |
| 106 |  |  [On the robustness of Chatgpt under input perturbations for Named Entity Recognition Task](https://openreview.net/forum?id=cyN5Ck1RFT) |  | 0 | We present a systematic evaluation of the robustness of ChatGPT (in both zeroand few-shot settings) under input perturbations for Named Entity Recognition (NER) task. Our findings suggest: (1) ChatGPT is more brittle on Drug or Disease entity perturbations (rare entities) as compared to those on... | Abhilasha Sancheti, Ishani Mondal |  |
| 107 |  |  [Neural Controlled Differential Equations with Quantum Hidden Evolutions](https://openreview.net/forum?id=vQUq4DVeu6) |  | 0 | We introduce a class of neural controlled differential equation inspired by quantum mechanics. Neural quantum controlled differential equations (NQDEs) model the dynamics by analogue of the Schrodinger equation. Specifically, the hidden state represents the wave function, and its collapse leads to... | Lingyi Yang, Zhen Shao |  |
| 108 |  |  [GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural Networks](https://openreview.net/forum?id=2yuAxTs0QV) |  | 0 | The successful graph neural networks (GNNs) and particularly message passing neural networks critically depend on the functions employed for message aggregation and graph-level readout. Using signal propagation theory, we propose a variance-preserving aggregation function, which maintains the... | Andreas Mayr, Florian Sestak, Günter Klambauer, Johannes Brandstetter, Lisa Schneckenreiter, Richard Freinschlag |  |
| 109 |  |  [Using spiking neural networks to assist fine art and philology study: to classify styles of Chinese calligraphy with minimal computing power](https://openreview.net/forum?id=1715SJkbj7) |  | 0 | Spiking Neural Networks have drawn much attention for their potential deployment in low computing power scenarios and interdisciplinary research. This paper focuses on a novel task of classifying Chinese Calligraphy styles properly and introduces a cutting-edge network called CaStySNN. Compared to... | Shuimu Zeng, Xiangqi Kong, Xuerui Qiu, Yaxuan Zhang, Yuke Yao, Zheng Luan |  |
| 110 |  |  [Common Sense Initialization of Mixture Density Networks for Motion Planning with Overestimated Number of Components](https://openreview.net/forum?id=C6EHiLaiBT) |  | 0 | Mixture density networks (MDNs) are a natural choice to model multi-modal predictions for trajectory prediction or motion planning. However, MDNs are often difficult to train due to mode collapse and a need for careful initialization, which becomes even more problematic when the number of mixture... | Alejandro Sánchez Guinea, Max Mühlhäuser, Thomas Kreutz |  |
| 111 |  |  [Improving Image Editing Models with Generative Data Refinement](https://openreview.net/forum?id=q5UrA58oyY) |  | 0 | Instruction-based generative image editing models allow an image to be modified based on a text prompt and have the potential to significantly improve the accessibility of image processing software. Like other generative models, they are highly dependent on the quality of their training dataset,... | Frederic Boesel, Robin Rombach |  |
| 112 |  |  [Learning the Uncertainty Set in Robust Markov Decision Process](https://openreview.net/forum?id=viHuAGGjqK) |  | 0 | In robust Markov Decision Processes (MDPs), the uncertainty set is often assumed to be fixed and given. However, the size of the uncertainty set is crucial due to the inherent trade-off between robustness and conservatives: a larger uncertainty set fosters a more robust solution but tends towards... | Kaixin Wang, Kfir Yehuda Levy, Navdeep Kumar, Shie Mannor, Uri Gadot |  |
| 113 |  |  [Exploring GPT-4 Vision for Text-to-Image Synthesis Evaluation](https://openreview.net/forum?id=xmQoodG82a) |  | 0 | This paper addresses the critical need for more accurate evaluation methods in text-to-image synthesis. While the standard CLIPScore metric can reflect text-image alignment to some extent, it often falls short in consistency with human perception. We propose the use of GPT-4 Vision as a novel... | Houqiang Li, Qi Sun, Wengang Zhou, Xiao Cui |  |
| 114 |  |  [Language Model Knowledge Distillation for Efficient Question Answering in Spanish](https://openreview.net/forum?id=iTgKbVwQAF) |  | 0 | Recent advances in the development of pre-trained Spanish language models has led to significant progress in many Natural Language Processing (NLP) tasks, such as question answering. However, the lack of efficient models imposes a barrier for the adoption of such models in resource-constrained... | Adrián Bazaga, Gos Micklem, Pietro Lio |  |
| 115 |  |  [AdAct: Learning to Optimize Activation Function Choice through Adaptive Activation Modules](https://openreview.net/forum?id=M7d5k4AxCE) |  | 0 | This paper presents an innovative approach to enhancing neural network performance through the development and implementation of an adaptive activation function, termed adaptive activation (AdAct). AdAct amalgamates various well-established and novel activation functions into a single, learnable... | Ritabrata Maiti |  |
| 116 |  |  [A Novel Two-stage Model with Cross-Level Contrastive Learning for Text-VQA](https://openreview.net/forum?id=0Nm98D6mEh) |  | 0 | Text-based Visual Question Answering (Text-VQA) task requires the model to learn effective representations in a joint semantic space. Previous methods lack the explicit alignment between object-level and scene text-level in visual-linguistic modalities. To address this issue, we propose a novel... | Xiaojun Bi, Yuejy, Zheng Chen |  |
| 117 |  |  [Heredity-aware Child Face Image Generation with Latent Space Disentanglement](https://openreview.net/forum?id=WBVfUjA80m) |  | 0 | In this paper, we propose ChildGAN to generate a child's face image according to the images of parents with heredity prior. The main idea is to disentangle the latent space of a pre-trained generation model and precisely control the face attributes of child images with clear semantics. We use... | Houqiang Li, Wengang Zhou, Xiao Cui |  |
| 118 |  |  [Explorations in Texture Learning](https://openreview.net/forum?id=gJua7kBOHz) |  | 0 | In this work, we investigate \*texture learning\*: the identification of textures learned by object classification models, and the extent to which they rely on these textures. We build texture-object associations that uncover new insights about the relationships between texture and object classes... | Blaine Hoak, Patrick D. McDaniel |  |
| 119 |  |  [Backtracking Mathematical Reasoning of Language Models to the Pretraining Data](https://openreview.net/forum?id=otHhLO7GZj) |  | 0 | In this study, we identify subsets of model pretraining data that contribute to the math reasoning ability of language models and evaluate it on several mathematical tasks (e.g., addition, multiplication). We find that training on math-only data improves simple arithmetic but doesn't fully account... | Hamish Ivison, Sameer Singh, Yanai Elazar, Yasaman Razeghi |  |
| 120 |  |  [Reward Bound for Behavioral Guarantee of Model-based Planning Agents](https://openreview.net/forum?id=n3ip7H2ioh) |  | 0 | Recent years have seen an emerging interest in the Verification and Validation (V\&V) of machine learning-based agents in the wild, especially in robotics, to provide safety assurance for the industry. Obtaining behavioral guarantees for these agents remains an important problem. In this work, we... | Wan Du, Xianzhong Ding, Zhiyu An |  |
| 121 |  |  [Can Graph Neural Networks learn node-level structural features?](https://openreview.net/forum?id=HRxVPPdyDh) |  | 0 | Graph Neural Networks (GNNs) have become one of the most widely adopted solutions for graph machine learning (GML) tasks. They perform feature learning on graphs using message passing on the network structure, avoiding the feature engineering step required for traditional tabular approaches for GML... | Manuel Dileo, Matteo Zignani |  |
| 122 |  |  [L-Tuning: Synchronized Label Tuning for Prompt and Prefix in LLMS](https://openreview.net/forum?id=zv3xpxqjjB) |  | 0 | Efficiently fine-tuning Large Language Models (LLMs) for specific tasks presents a considerable challenge in natural language processing. Traditional methods, like prompt or prefix tuning, typically rely on arbitrary tokens for training, leading to prolonged training times and generalized token use... | Asif Mahmud, Md. Kowsher, Md. Shohanur Islam Sobuj, Nusrat Jahan Prottasha, Prakash Bhat |  |
| 123 |  |  [Tracing Footprints: Neural Networks Meet Non-integer Order Differential Equations For Modelling Systems with Memory](https://openreview.net/forum?id=8518dcW4hc) |  | 0 | Neural Ordinary Differential Equations (Neural ODEs) have gained popularity for modelling real-world systems, thanks to their ability to fit ODEs to data. However, numerous systems in science and engineering often exhibit intricate memory behaviours, being classical ODEs inadequate for such tasks... | C. Coelho, Luís L. Ferrás, M. Fernanda P. Costa |  |
| 124 |  |  [A Generalized Semiconductor Wafer Defect Classifier](https://openreview.net/forum?id=VAJVN44B5b) |  | 0 | Silicon-based integrated circuits (ICs) and electronic devices are used in every possible electronic device, including high-performance computers fabricated from silicon wafers. Hence, ensuring the quality and reliability of silicon components is of utmost importance. This work focuses on... | Akshay Agarwal, Pratik Pal, Priyanshu Kumar Rai |  |
| 125 |  |  [Using the Polyak Step Size in training Convolutional Neural Networks](https://openreview.net/forum?id=QmRqSqpV0y) |  | 0 | The Polyak Step Size (PSS) is an adaptive learning rate that has yet to see much prominence in Deep Learning (DL). This paper investigates using the PSS in training Convolutional Neural Networks (CNN) for Image Classification (IC). We show that by introducing two upper bounds for the PSS, we can... | Daragh King |  |
| 126 |  |  [Averaging Rate Scheduler for Decentralized Learning on Heterogeneous Data](https://openreview.net/forum?id=w9ZzNmWmjA) |  | 0 | Presently, state-of-the-art decentralized learning algorithms typically require the data distribution to be Independent and Identically Distributed (IID). However, in practical scenarios, the data distribution across the agents can have significant heterogeneity. In this work, we propose averaging... | Kaushik Roy, Sai Aparna Aketi, Sakshi Choudhary |  |
| 127 |  |  [Audio vs. Text: Identify a Powerful Modality for Effective Hate Speech Detection](https://openreview.net/forum?id=dD2e3aCEcO) |  | 0 | The boom in social media platforms has witnessed a significant jump in offensive, hate, and toxic languages. These toxic contents leave a significant effect on one's personality which can even lead to depression and can be in various forms including audio and text. \textbf{However, the primary... | Akshay Agarwal, Kirtilekha Bhesra, Shivam Ashok Shukla |  |
| 128 |  |  [When Does Second-Order Optimization Speed Up Training?](https://openreview.net/forum?id=NLrfEsSZNb) |  | 0 | While numerous second-order optimization methods have been proposed to accelerate training in deep learning, they are seldom used in practice. This is partly due to a limited understanding of the conditions under which second-order optimization outperforms first-order optimization. This study aims... | Rio Yokota, Satoki Ishikawa |  |
| 129 |  |  [Discrete Natural Evolution Strategies](https://openreview.net/forum?id=yDdmC6Tq8s) |  | 0 | Natural evolution strategies are a class of approximate-gradient black-box optimizers that have been successfully used for continuous parameter spaces. In this paper, we derive NES algorithms for discrete parameter spaces and demonstrate their effectiveness in tasks involving discrete parameters. | Ahmad Ayaz Amin |  |
| 130 |  |  [Lost or Liberated? A Dive into Bidirectional Transformer LMs Without Positional Encoding](https://openreview.net/forum?id=Jr3XQRtn3B) |  | 0 | Recent studies have shown that Autoregressive Transformer Language Models (LMs) can generate text sequences without relying on positional encodings (PEs). This capability is attributed to the causal masks in these models, which prevent tokens from accessing information from future tokens, allowing... | Urchade Zaratiana |  |
| 131 |  |  [Policy Gradient with Tree Search (PGTS) in Reinforcement Learning Evades Local Maxima](https://openreview.net/forum?id=61g5xxWOI6) |  | 0 | The policy gradient (PG) methods are being extensively used in practice. However, their theoretical convergence guarantees require strict regularity conditions. Such conditions are unnatural and generally not satisfied in practice, causing such techniques to get stuck in a sub-optimal local maximum... | Kfir Yehuda Levy, Navdeep Kumar, Priyank Agrawal, Shie Mannor |  |
| 132 |  |  [Emoji Kitchen with Controlled Fusion](https://openreview.net/forum?id=CmaquCIlPU) |  | 0 | The image fusion method is widely used in many different fields. The fusion processes both need models to extract semantic information and contain details. Traditional image processing techniques used for this issue have limited ability to extract semantic features from images, and advanced deep... | Chengao Shen, Ge Diao, Siyuan Mu |  |
| 133 |  |  [Learning to Reason with Autoregressive In-Context Distillation](https://openreview.net/forum?id=auvDeqEKrk) |  | 0 | We investigate the joint distillation of in-context learning and reasoning from advanced large language models (LLMs) to their smaller counterparts. We introduce Autoregressive In-Context Distillation (AICD), a simple yet effective paradigm for this purpose. AICD employs meta-teacher forcing on... | Yuxuan Liu |  |
| 134 |  |  [ONLS: Optimal Noise Level Search in Diffusion Autoencoders Without Fine-Tuning](https://openreview.net/forum?id=Q8diCUHTZd) |  | 0 | An ideal counterfactual estimation should achieve balance of precise intervention and identity preservation. Recently, Classifier-Guided Diffusion Model is proven effective to produce realistic and minimal counterfactuals. However, perfect intervention is often challenging to find and requires... | Zihan Wang |  |
| 135 |  |  [SD-NAE: Generating Natural Adversarial Examples with Stable Diffusion](https://openreview.net/forum?id=D87rimdkGd) |  | 0 | Natural Adversarial Examples (NAEs), images arising naturally from the environment and capable of deceiving classifiers, are instrumental in robustly evaluating and identifying vulnerabilities in trained models. In this work, unlike prior works that passively collect NAEs from real images, we... | Hai Li, Jingyang Zhang, Yiran Chen, Yueqian Lin |  |
| 136 |  |  [Adverb Is the Key: Simple Text Data Augmentation with Adverb Deletion](https://openreview.net/forum?id=bNxB3Yd3Pd) |  | 0 | In the field of text data augmentation, rule-based methods are widely adopted for real-world applications owing to their cost-efficiency. However, conventional rule-based approaches suffer from the possibility of losing the original semantics of the given text. We propose a novel text data... | Juhwan Choi, YoungBin Kim |  |
| 137 |  |  [Frustratingly Simple Prompting-based Text Denoising](https://openreview.net/forum?id=XlJRjhIkNi) |  | 0 | This paper introduces a novel perspective on the automated essay scoring (AES) task, challenging the conventional view of the ASAP dataset as a static entity. Employing simple text denoising techniques using prompting, we explore the dynamic potential within the dataset. While acknowledging the... | Jungyeul Park, Mengyang Qiu |  |
| 138 |  |  [Towards Faster Global Convergence of Robust Policy Gradient Methods](https://openreview.net/forum?id=JWviHsYSKn) |  | 0 | We establish the global convergence of the policy gradient method for robust Markov Decision Processes (MDPs) under the assumption that the robust return is smooth with respect to a policy. Despite restrictive, such smoothness assumption is satisfied in many interesting settings such as... | Ilnura Usmanova, Kfir Yehuda Levy, Navdeep Kumar, Shie Mannor |  |
| 139 |  |  [Policy Gradient for Reinforcement Learning with General Utilities](https://openreview.net/forum?id=XsWA2y2TyI) |  | 0 | We derive policy gradient theorem for reinforcement learning (RL) with the objective which is a general (non-linear and non-convex) function of the occupancy measure of the policy. This setting incorporates many problems in literature such as apprenticeship learning, pure exploration and... | Kaixin Wang, Kfir Yehuda Levy, Navdeep Kumar, Shie Mannor, Utkarsh Pratiush |  |
| 140 |  |  [Investigating Representations for Vision And Touch in Contact Rich Robot Scooping Tasks](https://openreview.net/forum?id=p8GJzu52XL) |  | 0 | Contact-rich robotic manipulation in unstructured environments remains an open challenge in robotics with no established universal architectures or representations to handle the involved modalities. This paper analyzes different approaches for combining vision and touch to improve robotic scooping,... | JiEun Jung |  |
| 141 |  |  [Semantic Patch Embedding for Security Detection: A Fine-to-Coarse Grained Approach](https://openreview.net/forum?id=zAst1ulgP2) |  | 0 | The surge in open-source software usage has heightened the risk of concealed vulnerabilities impacting downstream applications. Compounded by vendors silently releasing security patches without explicit notifications, users remain unaware of potential threats, providing attackers with... | Haoye Tian, Jacques Klein, Tegawendé F. Bissyandé, Xunzhu Tang, Yewei Song, Zhenghan Chen |  |
| 142 |  |  [A Systematic Study of the Role of Data Quality and Alignment for Fine-tuning LLMs for Enhanced Autoformalization](https://openreview.net/forum?id=9LkPbnRwXu) |  | 0 | This study explores the role of data quality, particularly alignment, in fine-tuning Large Language Models (LLMs) for the task of autoformalization. Contrary to the conventional emphasis on dataset size, our research highlights the importance of data alignment - the similarity between training data... | Aryan Sahai, Brando Miranda, Krrish Chawla, Mario DePavia |  |
| 143 |  |  [Toward Learning Latent-Variable Representations of Microstructures by Optimizing in Spatial Statistics Space](https://openreview.net/forum?id=XAXGOq1isw) |  | 0 | In Materials Science, material development involves evaluating and optimizing the internal structures of the material, generically referred to as microstructures. Microstructures structure is stochastic, analogously to image textures. A particular microstructure can be well characterized by its... | Michael Guerzhoy, Noah H. Paulson, Sayed Sajad Hashemi |  |
| 144 |  |  [PhoWhisper: Automatic Speech Recognition for Vietnamese](https://openreview.net/forum?id=x3c3MkJfpG) |  | 0 | We introduce PhoWhisper in five versions for Vietnamese automatic speech recognition. PhoWhisper's robustness is achieved through fine-tuning the Whisper model on an 844-hour dataset that encompasses diverse Vietnamese accents. Our experimental study demonstrates state-of-the-art performances of... | Dat Quoc Nguyen, Linh The Nguyen, ThanhThien Le |  |
| 145 |  |  [NIRo: A Metric to capture non-iid robustness for Federated Learning Algorithms](https://openreview.net/forum?id=Bq9SgpL0dX) |  | 0 | Federated Learning (FL) is a collaborative machine learning framework for decentralized nodes with non-IID(non- independent, identically, distributed) distributed private data, to create a globally un-biased, high performing central model. A majority of the proposed FL protocols report performances... | Anupam Gupta, Pabitra Mitra |  |
| 146 |  |  [Pre-Tokenization of Numbers for Large Language Models](https://openreview.net/forum?id=bv8n5aYP1l) |  | 0 | There have been many recent advances in LLM for zero-shot tasks. While these models have shown great promise, pre-tokenization process methods for numbers are mostly empirical and lack experimental justification. In this paper, we analyze tokenization of numbers in through time series forecasting.... | Haifeng Sun, Jingyu Wang, Qi Qi, Zhenglong Wu, Zirui Zhuang |  |
| 147 |  |  [Back to the Future: predicting causal relationships influencing oil prices](https://openreview.net/forum?id=TqQlYLd52W) |  | 0 | Strategic foresight has been identified as a key tool to enhance policymaking and guide decision-making related to environmental protection. Artificial intelligence (AI) has been scarcely applied to this domain. We present a novel approach to predict causal relationships between entities leveraging... | Beno Sircelj, Gregor Leban, Joze M. Rozanec, Michael Cochez |  |
| 148 |  |  [Balancing performance and complexity with adaptive graph coarsening](https://openreview.net/forum?id=DrHwIzz93C) |  | 0 | We present a method for graph node classification that allows a user to precisely select the resolution at which the graph in question should be simplified and through this provides a way of choosing a suitable point in the performance-complexity trade-off. The method is based on refining a reduced... | Lukás Bajer, Marek Dedic, Martin Holena, Pavel Procházka |  |
| 149 |  |  [Probing the Hidden Layers of a Music Generating Language Model](https://openreview.net/forum?id=2Dlx6YgS3n) |  | 0 | Our study delves into FIGARO, a Transformer-based music generation model, revealing its uneven handling of musical elements like chords and velocity. By running a probe experiment on the model's encoder, we uncover a significant bias in processing certain features, directly impacting music... | Rafik Hachana |  |
| 150 |  |  [A Study on Polarity distributions for Network Learning](https://openreview.net/forum?id=0Ddq3BtHsH) |  | 0 | Transfer learning is one of the most important techniques in modern deep learning. The knowledge gained from transferring weights helps networks to learn fast achieving high accuracy. Recent work has shown that transferring the polarity of the weights plays a fundamental role in transfer learning.... | Aoife Igoe, Arindam Biswas, Biswajit Basu |  |
| 151 |  |  [Layered insights into Pyramid feature fusion architecture for SSL](https://openreview.net/forum?id=E1kT452SdL) |  | 0 | In this paper, we introduce a novel self-supervised learning(SSL) approach leveraging pyramid layers to extract essential visual features. Employing image inpainting as the pretext task, we empirically demonstrate the effectiveness of this methodology by rigorously evaluating the trained network's... | Ashrya Agrawal, Priyanshi Shah, Sourabh Prakash |  |
| 152 |  |  [Advancing Image Classification through Parameter-Efficient Fine-Tuning: A Study on LoRA with Plant Disease Detection Datasets](https://openreview.net/forum?id=lPnNhcqLSi) |  | 0 | Low-Rank Approximation (LoRA) has demonstrated remarkable efficiency in Large Language Models (LLMs), enabling the attainment of state-of-the-art results across various Natural Language Processing (NLP) tasks. This study shifts the focus to the application of LoRA, a low-rank approximation... | Deeksha Aggarwal, Uttam Kumar, Yash Mittal |  |
| 153 |  |  [Exploring Dimensional Collapse in Self-Supervised Video Representation Learning](https://openreview.net/forum?id=iDeRIyYTop) |  | 0 | In the field of joint embedding methods, the complete collapse to a constant feature vector is a clear indication of an immediate deficiency in the approach. Another critical concern, known as dimensional collapse, describes the utilization of a feature space only to a lower-dimensional subspace.... | Monika Kwiatkowski, Olaf Hellwich, Patrik Reiske, Paul Kapust |  |
| 154 |  |  [Graph Expansion in Pruned Recurrent Neural Network Layers Preserves Performance](https://openreview.net/forum?id=hG5eu7ikDy) |  | 0 | Expansion property of a graph refers to its strong connectivity as well as sparseness. It has been reported that deep neural networks can be pruned to a high degree of sparsity while maintaining their performance. We prune recurrent networks such as RNNs and LSTMs, maintaining a large spectral gap... | Arindam Biswas, Biswajit Basu, Pabitra Mitra, Suryam Arnav Kalra |  |
| 155 |  |  [Bayesian-Driven Learning of A New Weighted Tensor Norm for Tensor Recovery](https://openreview.net/forum?id=ciEbMa2xuC) |  | 0 | This study addresses the performance limitations of t-SVD-based tensor recovery caused by non-smooth changes and imbalanced low-rankness in tensor data. We introduce a novel bilevel tensor completion model, integrating the learning of a data-dependent weighted tensor norm within the tensor... | Jingjing Zheng, Yankai Cao |  |
| 156 |  |  [Racial and Gender Stereotypes Encoded Into CLIP Representations](https://openreview.net/forum?id=hQb6ts30wv) |  | 0 | OpenAI’s CLIP is a vision language model widely used in current state-of-the-art architectures. This paper analyzes racial and gender biases present in CLIP’s representation of human images. We evaluate images from the FairFace dataset grouped by race and gender on a series of traits describing... | Joseph James Vincent, Vatsal Baherwani |  |
| 157 |  |  [Native machine learning for noisy microscopic data processing](https://openreview.net/forum?id=5sWY5ENwtl) |  | 0 | Cells count become a challenging problem when the cells move in a continuous stream, and their boundaries are difficult for visual detection. To resolve this problem we modified the training and decision making processes using curriculum learning and multi-view predictions techniques, respectively. | Alexey Kornaev, Danil Afonchikov, Elena Kornaeva, Irina Makovik |  |
| 158 |  |  [Learning from Graphs Beyond Message Passing Neural Networks](https://openreview.net/forum?id=m6pvyXIQcu) |  | 0 | Graph, as a potent data structure, models complex relational data that are ubiquitous in real-world applications like social networks and recommendation systems. In the past few years, message passing-based Graph Neural Networks (GNNs) have emerged as standard tools for direct learning from graph... | Elham Ghazizadeh, Neil Shah, Tong Zhao |  |
| 159 |  |  [Fusing Vision and Language Models to Generate Sequence of Recipe Images from Steps](https://openreview.net/forum?id=McFvoo7s6V) |  | 0 | There has been a lot of work on using generative models for generating text descriptions given an image, demonstrating the power of pretrained large language models. There has also been several work on generating a sequence of text from a sequence of images, highlighting the effectiveness of fusing... | Hshmat Sahak |  |
| 160 |  |  [Autonomous Generation of Innovative Content by Multi-Agent System](https://openreview.net/forum?id=Ed5vlTvI0u) |  | 0 | The ability to extrapolate from existing concepts and generate richer ideas is a crucial human capability. Although large models like ChatGPT have found widespread applications and enhanced productivity to some extent, tasks requiring more profound and extrapolative thinking still demand human... | Rui Hao, Weikai Xie |  |
| 161 |  |  [Semi-supervised Learning under Self-training via $f$-Divergence](https://openreview.net/forum?id=3cZuCyJSe7) |  | 0 | This paper investigates a range of empirical risk functions and regularization methods suitable for self-training methods in semi-supervised learning. These approaches draw inspiration from $f$-divergences. In the pseudo-labeling and entropy minimization techniques as self-training methods for... | Amirhossein Bagheri, Gholamali Aminian, Mahyar JafariNodeh, Mohammad Hossein Yassaee, Radmehr Karimian |  |
| 162 |  |  [Ai Gen ASSISTment: Analyzing the Effectiveness of Generative AI Data Amplification for Dkt](https://openreview.net/forum?id=ZZB8z9hIjR) |  | 0 | In edtech, it is one of the important factors to predict the learning level of learners and whether they will solve problems in the future by utilizing a learner track- ing system. However, the scarcity of initial data limits our ability to make accu- rate predictions from scratch and to evaluate... | Cho Ung Hui, NohMyongSung |  |
| 163 |  |  [LAMPER: LanguAge Model and Prompt EngineeRing for zero-shot time series classification](https://openreview.net/forum?id=eUQiESSvmF) |  | 0 | This study constructs the LanguAge Model with Prompt EngineeRing (LAMPER) framework, designed to systematically evaluate the adaptability of pre-trained language models (PLMs) in accommodating diverse prompts and their integration in zero-shot time series (TS) classification. We deploy LAMPER in... | Peiwu Qin, Yan Tong, Zhaotian Xie, Zhicheng Du |  |
| 164 |  |  [Synthetic Labeling: A Novel Approach to Advancing Few-Shot Learning](https://openreview.net/forum?id=x7azREBRyw) |  | 0 | In the field of few-shot learning, the scarcity of labeled data significantly hinders progress. This paper introduces an innovative regularization algorithm designed to enhance generalization performance in classification tasks by leveraging synthetically labeled data. The approach utilizes a... | Gholamali Aminian, Miguel R. D. Rodrigues, Zhaoyan Lyu |  |
| 165 |  |  [Generation of a random self-similarity curve](https://openreview.net/forum?id=FVR89igilr) |  | 0 | This work presents a novelty method for generating a random self-similarity curve by fractal dimension. Fractal dimension is the main feature of self-similarity objects, which can be used for for efficient representation of complex structures. Experiments show that the proposed method can be used... | Dmitrii Tumakov, Ilya Pershin |  |
| 166 |  |  [User Modeling Challenges in Interactive AI Assistant Systems](https://openreview.net/forum?id=YEjIcZpqed) |  | 0 | Interactive Artificial Intelligent (AI) assistant systems are designed to offer timely guidance to help human users to complete a variety tasks. One of the remaining challenges is to understand user's mental states during the task for more personalized guidance. In this work, we analyze users'... | Megan Su, Yuwei Bao |  |
| 167 |  |  [Discerning Self-supervised Learning and Weakly Supervised Learning](https://openreview.net/forum?id=VQWuHYeTL6) |  | 0 | The AI community has been a very rapidly growing community producing a vast amount of research in a very short span of time. These researches generate a lot of new methods and terminologies. With this scale of developments, it is very difficult to keep track of terminologies, and under such... | Ali Jannesari, Chandan Kumar, Matthew J. Darr |  |
| 168 |  |  [A Pre-Search Evaluation Framework for Assessing Search Space Complexity](https://openreview.net/forum?id=M8V7qBhDVU) |  | 0 | The search space represents the most important component of Neural Architecture Search. It defines the ranges of performance, encapsulating the potential for discovering optimal architectures. This paper presents a framework for evaluating these spaces based on size, performance diversity,... | Hadjer Benmeziane |  |
| 169 |  |  [Large Language Models for Wearable Data Analysis and Interpretation](https://openreview.net/forum?id=GoWD6logcd) |  | 0 | In this paper, we investigate the application of large language models (LLMs) to zero-shot and few-shot prediction and classification of multimodal wearable sensor data. Using data from the large-scale Homekit2020 dataset, we explore health tasks including cardiac activity monitoring, metabolic... | Shkurta Gashi, Simon Böhi |  |
| 170 |  |  [Empirical Evaluations of Personalized Federated Learning on Heterogeneous Electronic Health Records](https://openreview.net/forum?id=iaTN5lQsSu) |  | 0 | Beyond mobile health devices, federated learning (FL) in healthcare often occurs in cross-silo scenarios, revealing an underexplored area — the comparison of FL, including personalized FL (PFL) models with pre-existing local models. The fact that the majority of existing FL and PFL algorithms were... | Di Miao, Qiming Wu, Siqi Li, Yuqing Shang |  |
| 171 |  |  [Beyond Time: Accurately Estimating the Fair Value of Stocks with Machine Learning and Fundamental Data](https://openreview.net/forum?id=ujHSaZdpFb) |  | 0 | This paper presents a novel machine learning methodology to assess the intrinsic value of firms, based on both qualitative and quantitative data, and deliberately excluding time series analysis. The employed models demonstrate proficiency in identifying undervalued stocks, highlighting the... | Daniel Netzl |  |
| 172 |  |  [Observations on Building RAG Systems for Technical Documents](https://openreview.net/forum?id=RFujq4HoV4) |  | 0 | Retrieval augmented generation (RAG) for technical documents creates challenges as embeddings do not often capture domain information. We review prior art for important factors affecting RAG and perform experiments to highlight best practices and potential challenges to build RAG systems for... | Sujoy Roychowdhury, Sumit Soman |  |
| 173 |  |  [Sound Classification in Indian Cities Using Multi-Label Data and Transfer Learning](https://openreview.net/forum?id=UWszefaphF) |  | 0 | The existing research primarily focuses on single-label classifications of complex urban sounds, ignoring crucial factors like sound mixtures and duration. This work proposes a novel transfer learning approach for multi-label sound classification. We fine-tuned a pre-trained VGGish model using a... | Rishi Gupta, Vaibhav Kumar |  |
| 174 |  |  [Causal Covariate Shift Correction using Fisher information penalty](https://openreview.net/forum?id=mhIL5JQr3d) |  | 0 | Evolving feature densities across batches of training data bias cross-validation, making model selection and assessment unreliable (Sugiyama & Kawanabe (2012)). This work takes a distributed density estimation angle to the training setting where data are temporally distributed. Causal Covariate... | Behraj Khan, Behroz Mirza, Tahir Syed |  |
| 175 |  |  [Q-Learning as a montone scheme](https://openreview.net/forum?id=xEx5Uyc0c4) |  | 0 | Stability issues with reinforcement learning methods persist. To better understand some of these stability and convergence issues involving deep reinforcement learning methods, we examine a simple linear quadratic example. We interpret the convergence criterion of exact Q-learning in the sense of a... | Lingyi Yang |  |
| 176 |  |  [RA-NER: Retrieval augmented NER for knowledge intensive named entity recognition](https://openreview.net/forum?id=HyKNjew2Ad) |  | 0 | NER (named entity recognition) model aims to recognize the named entities in the keywords. However, when the entities are extremely knowledge intensive, traditional NER model cannot encode all the knowledge in its parameters, thus fails to recognize those entities with high accuracy. In this paper,... | Chen Luo, Haiyang Zhang, Hanqing Lu, Rahul Goutam, Xianfeng Tang, Zhen Li, Zhenwei Dai |  |
| 177 |  |  [On the Robustness of Drug Abuse Face Classification](https://openreview.net/forum?id=Jdk8o63wyP) |  | 0 | Face recognition is one of the secure mediums to access various security-restricted areas such as border control and mobile unlocking. However, face recognition can be severely impacted due to several factors including illicit drug abuse on the facial regions. These abuses drastically alter the... | Akshay Agarwal, Hruturaj Dhake |  |
| 178 |  |  [Federated Learning on Small Batch Sizes via Batch Renormalization](https://openreview.net/forum?id=TrvOETLeYO) |  | 0 | When batch size is too small in personalized federated learning, the parameters of the model change significantly due to data variability and outliers, leading to highly stochastic gradients between the layers of the model. This situation ultimately extends the convergence time and poses a... | Jiaming Pei, Lukun Wang, Rubing Xue |  |
| 179 |  |  [t-Divergence: A New Divergence Measure with Application to Robust Statistics & Clustering](https://openreview.net/forum?id=xusEi6Fx3y) |  | 0 | This paper introduces the $t$-divergence, a novel divergence measure associated with the inverse tangent function. We investigate its intriguing consistent and outlier-robust features, particularly its quasi-metric properties and role in establishing weak convergence. Additionally, we showcase the... | Debolina Paul, Saptarshi Chakraborty, Swagatam Das |  |
| 180 |  |  [Stacking/Ensemble Model for Smartphone-Based Human Activity Recognition using Feature Engineering](https://openreview.net/forum?id=hbSJSFe2mM) |  | 0 | Human activity recognition, commonly known as HAR, involves identifying numerous physical activities individuals conduct across diverse situations. Examples of such actions include walking, jogging, running, ascending stairs, descending stairs, and more. Our work attempts to construct a model that... | Pratik Pal, Priyanshu Kumar Rai |  |
| 181 |  |  [Visualizing Information Conservation and Decomposition via the Information Matrix](https://openreview.net/forum?id=GNlfjLepnP) |  | 0 | We introduce a novel framework for visualizing information conservation, decomposition and transfer in time-series data, termed the Information Matrix ($\bm{I}^{XY}$). Our approach, grounded in information theory, focuses on mutual information (MI), directed information (DI), and transfer entropy... | Dor Tsur, Haim H. Permuter |  |
| 182 |  |  [Evaluating the Efficacy of Federated Scoring Systems with Heterogeneous Electronic Health Records](https://openreview.net/forum?id=c4GVRbEx1g) |  | 0 | Federated learning in healthcare research has primarily focused on black-box models, leaving a notable gap in interpretability crucial for clinical decision-making. While scoring systems, acknowledged for their transparency, are widely employed in clinical science, there are notably limited... | Di Miao, Nan Liu, Qiming Wu, Siqi Li, Xin Li, Yuqing Shang |  |
| 183 |  |  [A Novel Sector-Based Algorithm for an Optimized Star-Galaxy Classification](https://openreview.net/forum?id=HzEefCle2c) |  | 0 | This paper introduces a novel sector-based methodology for star-galaxy classification, leveraging the latest Sloan Digital Sky Survey data (SDSS-DR18). By strategically segmenting the sky into sectors aligned with SDSS observational patterns and employing a dedicated convolutional neural network... | Akshay Agarwal, Anumanchi Agastya Sai Ram Likhit, Divyansh Tripathi |  |
| 184 |  |  [Paraphrase Loss for Abstractive Summarization](https://openreview.net/forum?id=7Zzg5rpwm0) |  | 0 | Fine-tuned models for conditional generation grant state-of-the-art results for abstractive summarization. They achieve high scores by leveraging great amounts of training data and technically "unlimited'' training time coupled with the simple cross-entropy loss. We argue that, similarly to the... | Daniele Rege Cambrin, Paolo Garza |  |
| 185 |  |  [Exo-Spacetimeformer: Time Series Prediction with External Forecast Integration](https://openreview.net/forum?id=uyfz7fAcvu) |  | 0 | This study introduces the Exo-Spacetimeformer, an innovative adaptation of the Spacetimeformer model, designed to enhance PM2.5 air quality forecasting by incorporating external factors such as weather conditions, traffic intensity, and air-water-soil temperature differences into its decoder input.... | Boris Kraychev, Ensiye Kiyamousavi |  |
| 186 |  |  [WiFi CSI-based Long-Range Person Localization Using Directional Antennas](https://openreview.net/forum?id=AOJFcEh5Eb) |  | 0 | To address limited sensing hardware options in WiFi-based Human Activity Recognition (HAR), we introduce a compact, cost-effective system pairing the Espressif ESP32-S3 microcontroller with a directional antenna for long-range sensing. Constructed exclusively with widely available off-the-shelf... | Julian Strohmayer, Martin Kampel |  |
| 187 |  |  [DDA: A dual-domain attention plug-and-play prior for pansharpening](https://openreview.net/forum?id=SACKV1UWc6) |  | 0 | Pansharpening is an image processing technique that enhances spatial resolution of multispectral images by fusing them with higher-resolution panchromatic images, becoming increasingly critical for remote sensing and geospatial analysis applications. Despite advancements, current deep learning... | Wenjie Shu, Zien Zhang |  |
| 188 |  |  [SDSRA: A Skill-Driven Skill-Recombination Algorithm for Efficient Policy Learning](https://openreview.net/forum?id=0aOh1KRECK) |  | 0 | In this paper we introduce a novel algorithm-the Skill-Driven Skill Recombination Algorithm (SDSRA)—an innovative framework that significantly enhances the efficiency of achieving maximum entropy in reinforcement learning tasks. We find that SDSRA achieves faster convergence compared to the... | Andrew Lizarraga, Eric Hanchen Jiang |  |
| 189 |  |  [Enhancing Fairness in In-Context Learning: Prioritizing Minority Samples in Demonstrations](https://openreview.net/forum?id=LrPSJTNfmt) |  | 0 | Recent studies highlight the effectiveness of using in-context learning to steer large language models (LLMs) in processing tabular data, a challenging task given the structured nature of such data. Despite advancements, the fairness implications of this approach remain underexplored. This study... | Jingyu Hu, Mengnan Du |  |
| 190 |  |  [Entropy-aided prompt Federated learning](https://openreview.net/forum?id=P7MVXYI3Xx) |  | 0 | To overcome the impact of low-quality prompts on the training effect of prompt-Fl, we introduce the information entropy method to calculate the probability distribution of the model output to guide the local client to generate higher-quality prompts. Experimental results show that our method... | Haotian Wu, Jiaming Pei |  |
| 191 |  |  [Distributionally Robust Federated Learning with Wasserstein Barycenter](https://openreview.net/forum?id=k2DMAGoide) |  | 0 | Federated Learning (FL) has emerged as a privacy-preserving approach for collaboratively training models without sharing raw data, while a key challenge is that the data across the clients may not be identically distributed. The nominal distribution that the model truly learns is commonly assumed... | Shuran Fu, Wenqian Li, Yan Pang |  |
| 192 |  |  [Solar Panel Segmentation: Self-Supervised Learning Solutions for Imperfect Datasets](https://openreview.net/forum?id=V4kjFcopPS) |  | 0 | The increasing adoption of solar energy necessitates advanced methodologies for monitoring and maintenance to ensure optimal performance of solar panel installations. A critical component in this context is the accurate segmentation of solar panels from aerial or satellite imagery, which is... | Aditya Kasliwal, Krish Didwania, Laven Srivastava, Pallavi Kailas, Sankarshanaa Sagaram, Ujjwal Verma |  |
| 193 |  |  [Combinatorial CNNs for words](https://openreview.net/forum?id=Htn3pRs2v7) |  | 0 | This paper illustrates the difficulties that deep learning models encounter in detecting and exploiting patterns that remain consistent through one-to-one transformations, which we define as "combinatorial patterns." We contend that providing neural networks with detailed representations of these... | Karen Sargsyan |  |
| 194 |  |  [Generalize Neural Network Through Smooth Hypothesis Function](https://openreview.net/forum?id=RpiiKMGaK3) |  | 0 | The neural network (NN) looks for the hypothesis function in the search space, where the hypothesis function can be conceptualized as a continuous interpolating function. Nevertheless, the issue of overfitting commonly arises in interpolation methods that focus on function values, such as Lagrange... | Yupu Yao |  |
| 195 |  |  [Even a single simple augmentation with Self-Supervised Learning can be helpful for the downstream tasks](https://openreview.net/forum?id=FfkW423EmB) |  | 0 | This paper explores some unexpected capabilities of Self-Supervised Learning (SSL) and shows that even a single cutout augmentation for SSL can achieve better results in downstream tasks compared to traditional supervised approaches. These unusual properties of SSL can be used for further research... | Evgenii Pishchik |  |
| 196 |  |  [Proving Test Set Contamination in Black-Box Language Models](https://openreview.net/forum?id=KS8mIvetg2) |  | 0 | Large language models are trained on vast amounts of internet data, prompting concerns that they have memorized public benchmarks. Detecting this type of contamination is challenging because the pretraining data used by proprietary models are often not publicly accessible. We propose a procedure... | Faisal Ladhak, Nicole Meister, Niladri S. Chatterji, Tatsunori Hashimoto, Yonatan Oren |  |
| 197 |  |  [BooookScore: A systematic exploration of book-length summarization in the era of LLMs](https://openreview.net/forum?id=7Ttk3RzDeu) |  | 0 | Summarizing book-length documents ($>$100K tokens) that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance... | Kyle Lo, Mohit Iyyer, Tanya Goyal, Yapei Chang |  |
| 198 |  |  [Generalization in diffusion models arises from geometry-adaptive harmonic representations](https://openreview.net/forum?id=ANvmVS2Yr0) |  | 0 | Deep neural networks (DNNs) trained for image denoising are able to generate high-quality samples with score-based reverse diffusion algorithms. These impressive capabilities seem to imply an escape from the curse of dimensionality, but recent reports of memorization of the training set raise the... | Eero P. Simoncelli, Florentin Guth, Stéphane Mallat, Zahra Kadkhodaie |  |
| 199 |  |  [Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions](https://openreview.net/forum?id=ekeyCgeRfC) |  | 0 | In order to understand the in-context learning phenomenon, recent works have adopted a stylized experimental framework and demonstrated that Transformers can match the performance of gradient-based learning algorithms for various classes of real-valued functions. However, the limitations of... | Arkil Patel, Phil Blunsom, Satwik Bhattamishra, Varun Kanade |  |
| 200 |  |  [The mechanistic basis of data dependence and abrupt learning in an in-context classification task](https://openreview.net/forum?id=aN4Jf6Cx69) |  | 0 | Transformer models exhibit in-context learning: the ability to accurately predict the response to a novel query based on illustrative examples in the input sequence, which contrasts with traditional in-weights learning of query-output relationships. What aspects of the training data distribution... | Gautam Reddy |  |
| 201 |  |  [Improved Techniques for Training Consistency Models](https://openreview.net/forum?id=WNzy9bRDvG) |  | 0 | Consistency models are a nascent family of generative models that can sample high quality data in one step without the need for adversarial training. Current consistency models achieve optimal sample quality by distilling from pre-trained diffusion models and employing learned metrics such as... | Prafulla Dhariwal, Yang Song |  |
| 202 |  |  [Provable Compositional Generalization for Object-Centric Learning](https://openreview.net/forum?id=7VPTUWkiDQ) |  | 0 | Learning representations that generalize to novel compositions of known concepts is crucial for bridging the gap between human and machine perception. One prominent effort is learning object-centric representations, which are widely conjectured to enable compositional generalization. Yet, it... | Alexander Panfilov, Attila Juhos, Jack Brady, Matthias Bethge, Thaddäus Wiedemer, Wieland Brendel |  |
| 203 |  |  [Predictive auxiliary objectives in deep RL mimic learning in the brain](https://openreview.net/forum?id=agPpmEgf8C) |  | 0 | The ability to predict upcoming events has been hypothesized to comprise a key aspect of natural and machine cognition. This is supported by trends in deep reinforcement learning (RL), where self-supervised auxiliary objectives such as prediction are widely used to support representation learning... | Ching Fang, Kim Stachenfeld |  |
| 204 |  |  [Pre-Training Goal-based Models for Sample-Efficient Reinforcement Learning](https://openreview.net/forum?id=o2IEmeLL9r) |  | 0 | Pre-training on task-agnostic large datasets is a promising approach for enhancing the sample efficiency of reinforcement learning (RL) in solving complex tasks. We present PTGM, a novel method that pre-trains goal-based models to augment RL by providing temporal abstractions and behavior... | Feiyang Xie, Haoqi Yuan, Zhancun Mu, Zongqing Lu |  |
| 205 |  |  [Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!](https://openreview.net/forum?id=hTEGyKf0dZ) |  | 0 | Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open-source release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on customized datasets accelerate this trend. But, what are... | Peter Henderson, PinYu Chen, Prateek Mittal, Ruoxi Jia, Tinghao Xie, Xiangyu Qi, Yi Zeng |  |
| 206 |  |  [Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors](https://openreview.net/forum?id=PdaPky8MUn) |  | 0 | Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on... | Ankit Gupta, Ido Amos, Jonathan Berant |  |
| 207 |  |  [LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models](https://openreview.net/forum?id=LzPWWPAdY4) |  | 0 | Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning (Dettmers et al., 2023). In this work we focus on the scenario where quantization and LoRA fine- tuning are applied together on a pre-trained model. In such cases... | Chen Liang, Nikos Karampatziakis, Pengcheng He, Tuo Zhao, Weizhu Chen, Yifan Yu, Yixiao Li |  |
| 208 |  |  [Graph Neural Networks for Learning Equivariant Representations of Neural Networks](https://openreview.net/forum?id=oO6FsMyDBt) |  | 0 | Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation... | Boris Knyazev, Cees G. M. Snoek, David W. Zhang, Efstratios Gavves, Gertjan J. Burghouts, Miltiadis Kofinas, Yan Zhang, Yunlu Chen |  |
| 209 |  |  [GNNCert: Deterministic Certification of Graph Neural Networks against Adversarial Perturbations](https://openreview.net/forum?id=IGzaH538fz) |  | 0 | Graph classification, which aims to predict a label for a graph, has many real-world applications such as malware detection, fraud detection, and healthcare. However, many studies show an attacker could carefully perturb the structure and/or node features in a graph such that a graph classifier... | Binghui Wang, Han Yang, Jinyuan Jia, Zaishuo Xia |  |
| 210 |  |  [Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning](https://openreview.net/forum?id=LjivA1SLZ6) |  | 0 | In cooperative multi-agent reinforcement learning (MARL), agents aim to achieve a common goal, such as defeating enemies or scoring a goal. Existing MARL algorithms are effective but still require significant learning time and often get trapped in local optima by complex tasks, subsequently failing... | Hyungho Na, IlChul Moon, Yunkyeong Seo |  |
| 211 |  |  [ClimODE: Climate and Weather Forecasting with Physics-informed Neural ODEs](https://openreview.net/forum?id=xuY33XhEGR) |  | 0 | Climate and weather prediction traditionally relies on complex numerical simulations of atmospheric physics. Deep learning approaches, such as transformers, have recently challenged the simulation paradigm with complex network forecasts. However, they often act as data-driven black-box models that... | Markus Heinonen, Vikas Garg, Yogesh Verma |  |
| 212 |  |  [Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space](https://openreview.net/forum?id=4Ay23yeuz0) |  | 0 | Recent advances in tabular data generation have greatly enhanced synthetic data quality. However, extending diffusion models to tabular data is challenging due to the intricately varied distributions and a blend of data types of tabular data. This paper introduces TabSyn, a methodology that... | Balasubramaniam Srinivasan, Christos Faloutsos, George Karypis, Hengrui Zhang, Huzefa Rangwala, Jiani Zhang, Xiao Qin, Zhengyuan Shen |  |
| 213 |  |  [Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement](https://openreview.net/forum?id=bNt7oajl2a) |  | 0 | The ability to derive underlying principles from a handful of observations and then generalize to novel situations---known as inductive reasoning---is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive... | Bailin Wang, Chandra Bhagavatula, Linlu Qiu, Liwei Jiang, Melanie Sclar, Nouha Dziri, Valentina Pyatkin, Xiang Ren, Ximing Lu, Yejin Choi, Yoon Kim |  |
| 214 |  |  [Beyond Weisfeiler-Lehman: A Quantitative Framework for GNN Expressiveness](https://openreview.net/forum?id=HSKaGOi7Ar) |  | 0 | Designing expressive Graph Neural Networks (GNNs) is a fundamental topic in the graph learning community. So far, GNN expressiveness has been primarily assessed via the Weisfeiler-Lehman (WL) hierarchy. However, such an expressivity measure has notable limitations: it is inherently coarse,... | Bohang Zhang, Di He, Jingchu Gai, Liwei Wang, Qiwei Ye, Yiheng Du |  |
| 215 |  |  [MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts](https://openreview.net/forum?id=KUNzEQMWU7) |  | 0 | Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to... | Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Hritik Bansal, Jiacheng Liu, Jianfeng Gao, KaiWei Chang, Michel Galley, Pan Lu, Tony Xia |  |
| 216 |  |  [Protein Discovery with Discrete Walk-Jump Sampling](https://openreview.net/forum?id=zMPHKOmQNb) |  | 0 | We resolve difficulties in training and sampling from a discrete generative model by learning a smoothed energy function, sampling from the smoothed data manifold with Langevin Markov chain Monte Carlo (MCMC), and projecting back to the true data manifold with one-step denoising. Our... | Andreas Loukas, Daniel Berenberg, Isidro Hötzel, Joseph Kleinhenz, Julien LafranceVanasse, Karina Zadorozhny, Kyunghyun Cho, Nathan C. Frey, Richard Bonneau, Saeed Saremi, Stephen Ra, Vladimir Gligorijevic, Yan Wu |  |
| 217 |  |  [ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis](https://openreview.net/forum?id=oTRwljRgiv) |  | 0 | When writing programs, people have the ability to tackle a new complex task by decomposing it into smaller and more familiar subtasks. While it is difficult to measure whether neural program synthesis methods have similar capabilities, we can measure whether they compositionally generalize, that... | Charles Sutton, Joey Hong, Kensen Shi, Manzil Zaheer, Pengcheng Yin, Yinlin Deng |  |
| 218 |  |  [Batched Low-Rank Adaptation of Foundation Models](https://openreview.net/forum?id=w4abltTZ2f) |  | 0 | Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning foundation models by incorporating trainable low-rank matrices, thereby reducing the number of trainable parameters. While \lora/ offers numerous advantages, its applicability for real-time serving to a diverse and global user... | Swarat Chaudhuri, Yeming Wen |  |
| 219 |  |  [Improved Active Learning via Dependent Leverage Score Sampling](https://openreview.net/forum?id=IYxDy2jDFL) |  | 0 | We show how to obtain improved active learning methods in the agnostic (adversarial noise) setting by combining marginal leverage score sampling with non-independent sampling strategies that promote spatial coverage. In particular, we propose an easily implemented method based on the \emph{pivotal... | Atsushi Shimizu, Christopher Musco, Jonathan Weare, Xiaoou Cheng |  |
| 220 |  |  [Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs](https://openreview.net/forum?id=uNrFpDPMyo) |  | 0 | In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted... | Jianfeng Gao, Jiawei Han, Liyuan Liu, Minjia Zhang, Suyu Ge, Yunan Zhang |  |
| 221 |  |  [One-shot Empirical Privacy Estimation for Federated Learning](https://openreview.net/forum?id=0BqyZSWfzo) |  | 0 | Privacy estimation techniques for differentially private (DP) algorithms are useful for comparing against analytical bounds, or to empirically measure privacy loss in settings where known analytical bounds are not tight. However, existing privacy auditing techniques usually make strong assumptions... | Alina Oprea, Galen Andrew, Hugh Brendan McMahan, Peter Kairouz, Sewoong Oh, Vinith Menon Suriyakumar |  |
| 222 |  |  [SWE-bench: Can Language Models Resolve Real-world Github Issues?](https://openreview.net/forum?id=VTF8yNQM66) |  | 0 | Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of... | Alexander Wettig, Carlos E. Jimenez, John Yang, Karthik R. Narasimhan, Kexin Pei, Ofir Press, Shunyu Yao |  |
| 223 |  |  [ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models](https://openreview.net/forum?id=osoWxY8q2E) |  | 0 | Large Language Models (LLMs) with billions of parameters have drastically transformed AI applications. However, their demanding computation during inference has raised significant challenges for deployment on resource-constrained devices. Despite recent trends favoring alternative activation... | Golnoosh Samei, Iman Mirzadeh, Keivan AlizadehVahid, Mehrdad Farajtabar, Mohammad Rastegari, Oncel Tuzel, Sachin Mehta |  |
| 224 |  |  [On the Joint Interaction of Models, Data, and Features](https://openreview.net/forum?id=ze7DOLi394) |  | 0 | Learning features from data is one of the defining characteristics of deep learning, but the theoretical understanding of the role features play in deep learning is still in early development. To address this gap, we introduce a new tool, the interaction tensor, for empirically analyzing the... | Christina Baek, J. Zico Kolter, Yiding Jiang |  |
| 225 |  |  [Topological data analysis on noisy quantum computers](https://openreview.net/forum?id=dLrhRIMVmB) |  | 0 | Topological data analysis (TDA) is a powerful technique for extracting complex and valuable shape-related summaries of high-dimensional data. However, the computational demands of classical algorithms for computing TDA are exorbitant, and quickly become impractical for high-order characteristics.... | Ismail Yunus Akhalwaya, Kenneth L. Clarkson, Kugendran Naidoo, Lior Horesh, Mark S. Squillante, Shashanka Ubaru, Vasileios Kalantzis, Vishnu Jejjala, YangHui He |  |
| 226 |  |  [Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection](https://openreview.net/forum?id=hSyW5go0v8) |  | 0 | Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant... | Akari Asai, Avirup Sil, Hannaneh Hajishirzi, Yizhong Wang, Zeqiu Wu |  |
| 227 |  |  ["What Data Benefits My Classifier?" Enhancing Model Performance and Interpretability through Influence-Based Data Selection](https://openreview.net/forum?id=HE9eUQlAvo) |  | 0 | Classification models are ubiquitously deployed in society and necessitate high utility, fairness, and robustness performance. Current research efforts mainly focus on improving model architectures and learning algorithms on fixed datasets to achieve this goal. In contrast, in this paper, we... | Anshuman Chhabra, Hongfu Liu, Peizhao Li, Prasant Mohapatra |  |
| 228 |  |  [Generative Modeling with Phase Stochastic Bridge](https://openreview.net/forum?id=tUtGjQEDd4) |  | 0 | Diffusion models (DMs) represent state-of-the-art generative models for continuous inputs. DMs work by constructing a Stochastic Differential Equation (SDE) in the input space (ie, position space), and using a neural network to reverse it. In this work, we introduce a novel generative modeling... | Evangelos A. Theodorou, Jiatao Gu, Joshua M. Susskind, Laurent Dinh, Shuangfei Zhai, Tianrong Chen |  |
| 229 |  |  [Zipformer: A faster and better encoder for automatic speech recognition](https://openreview.net/forum?id=9WD9KwssyT) |  | 0 | The Conformer has become the most popular encoder model for automatic speech recognition (ASR). It adds convolution modules to a transformer to learn both local and global dependencies. In this work we describe a faster, more memory-efficient, and better-performing transformer, called Zipformer.... | Daniel Povey, Fangjun Kuang, Liyong Guo, Long Lin, Wei Kang, Xiaoyu Yang, Yifan Yang, Zengrui Jin, Zengwei Yao |  |
| 230 |  |  [MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework](https://openreview.net/forum?id=VtmBAGCN7o) |  | 0 | Recently, remarkable progress has been made on automated problem solving through societies of agents based on large language models (LLMs). Previous LLM-based multi-agent systems can already solve simple dialogue tasks. More complex tasks, however, face challenges through logic inconsistencies due... | Ceyao Zhang, Chenglin Wu, Chenyu Ran, Jinlin Wang, Jonathan Chen, Jürgen Schmidhuber, Lingfeng Xiao, Liyang Zhou, Mingchen Zhuge, Sirui Hong, Steven Ka Shing Yau, Xiawu Zheng, Yuheng Cheng, Zijuan Lin, Zili Wang |  |
| 231 |  |  [ValUES: A Framework for Systematic Validation of Uncertainty Estimation in Semantic Segmentation](https://openreview.net/forum?id=yV6fD7LYkF) |  | 0 | Uncertainty estimation is an essential and heavily-studied component for the reliable application of semantic segmentation methods. While various studies exist claiming methodological advances on the one hand, and successful application on the other hand, the field is currently hampered by a gap... | Carsten T. Lüth, KimCeline Kahl, Klaus H. MaierHein, Maximilian Zenk, Paul F. Jaeger |  |
| 232 |  |  [Finetuning Text-to-Image Diffusion Models for Fairness](https://openreview.net/forum?id=hnrB5YHoYu) |  | 0 | The rapid adoption of text-to-image diffusion models in society underscores an urgent need to address their biases. Without interventions, these biases could propagate a skewed worldview and restrict opportunities for minority groups. In this work, we frame fairness as a distributional alignment... | Chao Du, Min Lin, Mohan S. Kankanhalli, Tianyu Pang, Xudong Shen, Yongkang Wong |  |
| 233 |  |  [Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models](https://openreview.net/forum?id=WbWtOYIzIK) |  | 0 | By design, large language models (LLMs) are static general-purpose models, expensive to retrain or update frequently. As they are increasingly adopted for knowledge-intensive tasks, it becomes evident that these design choices lead to failures to generate factual, relevant, and up-to-date... | Shangbin Feng, Tianxing He, Vidhisha Balachandran, Weijia Shi, Yulia Tsvetkov, Yuyang Bai |  |
| 234 |  |  [METRA: Scalable Unsupervised RL with Metric-Aware Abstraction](https://openreview.net/forum?id=c5pwL0Soay) |  | 0 | Unsupervised pre-training strategies have proven to be highly effective in natural language processing and computer vision. Likewise, unsupervised reinforcement learning (RL) holds the promise of discovering a variety of potentially useful behaviors that can accelerate the learning of a wide array... | Oleh Rybkin, Seohong Park, Sergey Levine |  |
| 235 |  |  [Meta Continual Learning Revisited: Implicitly Enhancing Online Hessian Approximation via Variance Reduction](https://openreview.net/forum?id=TpD2aG1h0D) |  | 0 | Regularization-based methods have so far been among the \*de facto\* choices for continual learning. Recent theoretical studies have revealed that these methods all boil down to relying on the Hessian matrix approximation of model weights. However, these methods suffer from suboptimal trade-offs... | Deyu Meng, LongKai Huang, Renzhen Wang, Yichen Wu, Ying Wei |  |
| 236 |  |  [Improving Convergence and Generalization Using Parameter Symmetries](https://openreview.net/forum?id=L0r0GphlIL) |  | 0 | In many neural networks, different values of the parameters may result in the same loss value. Parameter space symmetries are loss-invariant transformations that change the model parameters. Teleportation applies such transformations to accelerate optimization. However, the exact mechanism behind... | Bo Zhao, Robert M. Gower, Robin Walters, Rose Yu |  |
| 237 |  |  [Flow Matching on General Geometries](https://openreview.net/forum?id=g7ohDlTITL) |  | 0 | We propose Riemannian Flow Matching (RFM), a simple yet powerful framework for training continuous normalizing flows on manifolds. Existing methods for generative modeling on manifolds either require expensive simulation, are inherently unable to scale to high dimensions, or use approximations for... | Ricky T. Q. Chen, Yaron Lipman |  |
| 238 |  |  [Ghost on the Shell: An Expressive Representation of General 3D Shapes](https://openreview.net/forum?id=Ad87VjRqUw) |  | 0 | The creation of photorealistic virtual worlds requires the accurate modeling of 3D surface geometry for a wide range of objects. For this, meshes are appealing since they enable 1) fast physics-based rendering with realistic material and lighting, 2) physical simulation, and 3) are memory-efficient... | Bernhard Schölkopf, Liam Paull, Michael J. Black, Weiyang Liu, Yao Feng, Yuliang Xiu, Zhen Liu |  |
| 239 |  |  [Würstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models](https://openreview.net/forum?id=gU58d5QeGv) |  | 0 | We introduce Würstchen, a novel architecture for text-to-image synthesis that combines competitive performance with unprecedented cost-effectiveness for large-scale text-to-image diffusion models. A key contribution of our work is to develop a latent diffusion technique in which we learn a detailed... | Christopher Pal, Dominic Rampas, Marc Aubreville, Mats L. Richter, Pablo Pernias |  |
| 240 |  |  [Unified Generative Modeling of 3D Molecules with Bayesian Flow Networks](https://openreview.net/forum?id=NSVtmmzeRB) |  | 0 | Advanced generative model (\textit{e.g.}, diffusion model) derived from simplified continuity assumptions of data distribution, though showing promising progress, has been difficult to apply directly to geometry generation applications due to the \textit{multi-modality} and \textit{noise-sensitive}... | Hao Zhou, Jingjing Gong, Jingjing Liu, Mingyue Zheng, WeiYing Ma, Yuxuan Song |  |
| 241 |  |  [Small-scale proxies for large-scale Transformer training instabilities](https://openreview.net/forum?id=d8w0pmvXbZ) |  | 0 | Teams that have trained large Transformer-based models have reported training instabilities at large scale that did not appear when training with the same hyperparameters at smaller scales. Although the causes of such instabilities are of scientific interest, the amount of resources required to... | Abhishek Kumar, Alexander A. Alemi, Ben Adlam, Izzeddin Gur, Jaehoon Lee, Jascha SohlDickstein, Jeffrey Pennington, John D. CoReyes, Justin Gilmer, Katie E. Everett, Kelvin Xu, Lechao Xiao, Mitchell Wortsman, Peter J. Liu, Roman Novak, Simon Kornblith |  |
| 242 |  |  [How I Warped Your Noise: a Temporally-Correlated Noise Prior for Diffusion Models](https://openreview.net/forum?id=pzElnMrgSD) |  | 0 | Video editing and generation methods often rely on pre-trained image-based diffusion models. During the diffusion process, however, the reliance on rudimentary noise sampling techniques that do not preserve correlations present in subsequent frames of a video is detrimental to the quality of the... | Jingwei Tang, Markus Gross, Pascal Chang, Vinicius C. Azevedo |  |
| 243 |  |  [Vision Transformers Need Registers](https://openreview.net/forum?id=2dnO3LLiJ1) |  | 0 | Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in... | Julien Mairal, Maxime Oquab, Piotr Bojanowski, Timothée Darcet |  |
| 244 |  |  [An Analytical Solution to Gauss-Newton Loss for Direct Image Alignment](https://openreview.net/forum?id=mE52zURNGc) |  | 0 | Direct image alignment is a widely used technique for relative 6DoF pose estimation between two images, but its accuracy strongly depends on pose initialization. Therefore, recent end-to-end frameworks increase the convergence basin of the learned feature descriptors with special training... | Daniel Cremers, Daniil Sinitsyn, Lukas von Stumberg, Nikita Araslanov, Sergei Solonets |  |
| 245 |  |  [Learning Energy Decompositions for Partial Inference in GFlowNets](https://openreview.net/forum?id=P15CHILQlg) |  | 0 | This paper studies generative flow networks (GFlowNets) to sample objects from the Boltzmann energy distribution via a sequence of actions. In particular, we focus on improving GFlowNet with partial inference: training flow functions with the evaluation of the intermediate states or transitions. To... | Hyosoon Jang, Minsu Kim, Sungsoo Ahn |  |
| 246 |  |  [Approximating Nash Equilibria in Normal-Form Games via Stochastic Optimization](https://openreview.net/forum?id=cc8h3I3V4E) |  | 0 | We propose the first loss function for approximate Nash equilibria of normal-form games that is amenable to unbiased Monte Carlo estimation. This construction allows us to deploy standard non-convex stochastic optimization techniques for approximating Nash equilibria, resulting in novel algorithms... | Georgios Piliouras, Ian Gemp, Luke Marris |  |
| 247 |  |  [Multi-Source Diffusion Models for Simultaneous Music Generation and Separation](https://openreview.net/forum?id=h922Qhkmx1) |  | 0 | In this work, we define a diffusion-based generative model capable of both music generation and source separation by learning the score of the joint probability density of sources sharing a context. Alongside the classic total inference tasks (i.e., generating a mixture, separating the sources), we... | Emanuele Rodolà, Emilian Postolache, Giorgio Mariani, Irene Tallini, Luca Cosmo, Michele Mancusi |  |
| 248 |  |  [LEGO-Prover: Neural Theorem Proving with Growing Libraries](https://openreview.net/forum?id=3f5PALef5B) |  | 0 | Despite the success of large language models (LLMs), the task of theorem proving still remains one of the hardest reasoning tasks that is far from being fully solved. Prior methods using language models have demonstrated promising results, but they still struggle to prove even middle school level... | Chuanyang Zheng, Enze Xie, Haiming Wang, Han Shi, Huajian Xin, Jian Yin, Jing Xiong, Qingxing Cao, Xiaodan Liang, Yinya Huang, Zhenguo Li, Zhengying Liu |  |
| 249 |  |  [ASID: Active Exploration for System Identification in Robotic Manipulation](https://openreview.net/forum?id=jNR6s6OSBT) |  | 0 | Model-free control strategies such as reinforcement learning have shown the ability to learn control strategies without requiring an accurate model or simulator of the world. While this is appealing due to the lack of modeling requirements, such methods can be sample inefficient, making them... | Abhishek Gupta, Andrew Wagenmaker, Chuning Zhu, Dieter Fox, Marius Memmel |  |
| 250 |  |  [Towards a statistical theory of data selection under weak supervision](https://openreview.net/forum?id=HhfcNgQn6p) |  | 0 | Given a sample of size $N$, it is often useful to select a subsample of smaller size $n<N$ to be used for statistical estimation or learning. Such a data selection step is useful to reduce the requirements of data labeling and the computational complexity of learning. We assume to be given $N$... | Andrea Montanari, Germain Kolossov, Pulkit Tandon |  |
| 251 |  |  [Mastering Memory Tasks with World Models](https://openreview.net/forum?id=1vDArHJ68h) |  | 0 | Current model-based reinforcement learning (MBRL) agents struggle with long-term dependencies. This limits their ability to effectively solve tasks involving extended time gaps between actions and outcomes, or tasks demanding the recalling of distant observations to inform current actions. To... | Artem Zholus, Janarthanan Rajendran, Mohammad Reza Samsami, Sarath Chandar |  |
| 252 |  |  [Monte Carlo guided Denoising Diffusion models for Bayesian linear inverse problems](https://openreview.net/forum?id=nHESwXvxWK) |  | 0 | Ill-posed linear inverse problems arise frequently in various applications, from computational photography to medical imaging. A recent line of research exploits Bayesian inference with informative priors to handle the ill-posedness of such problems. Amongst such priors, score-based generative... | Eric Moulines, Gabriel Cardoso, Sylvain Le Corff, Yazid Janati El Idrissi |  |
| 253 |  |  [Self-Alignment with Instruction Backtranslation](https://openreview.net/forum?id=1oijHJBRsT) |  | 0 | We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a... | Chunting Zhou, Jason Weston, Luke Zettlemoyer, Mike Lewis, Omer Levy, Ping Yu, Timo Schick, Xian Li |  |
| 254 |  |  [Learning Interactive Real-World Simulators](https://openreview.net/forum?id=sFyTZEqmUY) |  | 0 | Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a... | Dale Schuurmans, Jonathan Tompson, Leslie Pack Kaelbling, Pieter Abbeel, Seyed Kamyar Seyed Ghasemipour, Sherry Yang, Yilun Du |  |
| 255 |  |  [Candidate Label Set Pruning: A Data-centric Perspective for Deep Partial-label Learning](https://openreview.net/forum?id=Fk5IzauJ7F) |  | 0 | Partial-label learning (PLL) allows each training example to be equipped with a set of candidate labels. Existing deep PLL research focuses on a \emph{learning-centric} perspective to design various training strategies for label disambiguation i.e., identifying the concealed true label from the... | Chaojie Wang, Guowu Yang, Lei Feng, Shuo He |  |
| 256 |  |  [Robust agents learn causal world models](https://openreview.net/forum?id=pOoKI3ouv1) |  | 0 | It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any... | Jonathan Richens, Tom Everitt |  |
| 257 |  |  [On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs](https://openreview.net/forum?id=H3UayAQWoE) |  | 0 | Large Language Models (LLMs) have recently showcased their remarkable capacities, not only in natural language processing tasks but also across diverse domains such as clinical medicine, legal consultation, and education. LLMs become more than mere applications, evolving into assistants capable of... | Eric John Li, Jentse Huang, Man Ho Lam, Michael R. Lyu, Shujie Ren, Wenxiang Jiao, Wenxuan Wang, Youliang Yuan, Zhaopeng Tu |  |
| 258 |  |  [Diffusion Model for Dense Matching](https://openreview.net/forum?id=Zsfiqpft6K) |  | 0 | The objective for establishing dense correspondence between paired images con- sists of two terms: a data term and a prior term. While conventional techniques focused on defining hand-designed prior terms, which are difficult to formulate, re- cent approaches have focused on learning the data term... | Gyuseong Lee, Hyeonsu Kim, Hyoungwon Cho, Jisu Nam, Seungryong Kim, Seyeon Kim, Sunwoo Kim |  |
| 259 |  |  [Is ImageNet worth 1 video? Learning strong image encoders from 1 long unlabelled video](https://openreview.net/forum?id=Yen1lGns2o) |  | 0 | Self-supervised learning has unlocked the potential of scaling up pretraining to billions of images, since annotation is unnecessary. But are we making the best use of data? How more economical can we be? In this work, we attempt to answer this question by making two contributions. First, we... | João Carreira, Mamshad Nayeem Rizve, Shashanka Venkataramanan, Yannis Avrithis, Yuki M. Asano |  |
| 260 |  |  [Neural Fine-Tuning Search for Few-Shot Learning](https://openreview.net/forum?id=T7YV5UZKBc) |  | 0 | In few-shot recognition, a classifier that has been trained on one set of classes is required to rapidly adapt and generalize to a disjoint, novel set of classes. To that end, recent studies have shown the efficacy of fine-tuning with carefully-crafted adaptation architectures. However this raises... | Da Li, Lukasz Dudziak, Panagiotis Eustratiadis, Timothy M. Hospedales |  |
| 261 |  |  [Latent Trajectory Learning for Limited Timestamps under Distribution Shift over Time](https://openreview.net/forum?id=bTMMNT7IdW) |  | 0 | Distribution shifts over time are common in real-world machine-learning applications. This scenario is formulated as Evolving Domain Generalization (EDG), where models aim to generalize well to unseen target domains in a time-varying system by learning and leveraging the underlying evolving pattern... | Boyu Wang, Changjian Shui, Charles Ling, LongKai Huang, Peng Liu, Qiuhao Zeng, Xi Chen |  |
| 262 |  |  [Less is More: Fewer Interpretable Region via Submodular Subset Selection](https://openreview.net/forum?id=jKTUlxo5zy) |  | 0 | Image attribution algorithms aim to identify important regions that are highly relevant to model decisions. Although existing attribution solutions can effectively assign importance to target elements, they still face the following challenges: 1) existing attribution methods generate inaccurate... | Hua Zhang, Jingzhi Li, Ruoyu Chen, Siyuan Liang, Xiaochun Cao |  |
| 263 |  |  [Cameras as Rays: Pose Estimation via Ray Diffusion](https://openreview.net/forum?id=EanCFCwAjM) |  | 0 | Estimating camera poses is a fundamental task for 3D reconstruction and remains challenging given sparsely sampled views (<10). In contrast to existing approaches that pursue top-down prediction of global parametrizations of camera extrinsics, we propose a distributed representation of camera pose... | Amy Lin, Deva Ramanan, Jason Y. Zhang, Moneish Kumar, Shubham Tulsiani, TzuHsuan Yang |  |
| 264 |  |  [Accelerating Distributed Stochastic Optimization via Self-Repellent Random Walks](https://openreview.net/forum?id=BV1PHbTJzd) |  | 0 | We study a family of distributed stochastic optimization algorithms where gradients are sampled by a token traversing a network of agents in random-walk fashion. Typically, these random-walks are chosen to be Markov chains that asymptotically sample from a desired target distribution, and play a... | Do Young Eun, Jie Hu, Vishwaraj Doshi |  |
| 265 |  |  [Detecting, Explaining, and Mitigating Memorization in Diffusion Models](https://openreview.net/forum?id=84n3UwkH7b) |  | 0 | Recent breakthroughs in diffusion models have exhibited exceptional image-generation capabilities. However, studies show that some outputs are merely replications of training data. Such replications present potential legal challenges for model owners, especially when the generated content contains... | Chen Chen, Lingjuan Lyu, Yuchen Liu, Yuxin Wen |  |
| 266 |  |  [Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How](https://openreview.net/forum?id=tqh1zdXIra) |  | 0 | With the ever-increasing number of pretrained models, machine learning practitioners are continuously faced with which pretrained model to use, and how to finetune it for a new dataset. In this paper, we propose a methodology that jointly searches for the optimal pretrained model and the... | Arlind Kadra, Fabio Ferreira, Frank Hutter, Josif Grabocka, Sebastian PinedaArango |  |
| 267 |  |  [LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models](https://openreview.net/forum?id=6PmJoRfdaK) |  | 0 | We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For... | Haotian Tang, Jiaya Jia, Shengju Qian, Song Han, Xin Lai, Yukang Chen, Zhijian Liu |  |
| 268 |  |  [Amortizing intractable inference in large language models](https://openreview.net/forum?id=Ouj6p4ca60) |  | 0 | Autoregressive large language models (LLMs) compress knowledge from their training data through next-token conditional distributions. This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest---including sequence continuation,... | Edward J. Hu, Eric Elmoznino, Guillaume Lajoie, Moksh Jain, Nikolay Malkin, Yoshua Bengio, Younesse Kaddar |  |
| 269 |  |  [LLMCarbon: Modeling the End-to-End Carbon Footprint of Large Language Models](https://openreview.net/forum?id=aIok3ZD9to) |  | 0 | The carbon footprint associated with large language models (LLMs) is a significant concern, encompassing emissions from their training, inference, experimentation, and storage processes, including operational and embodied carbon emissions. An essential aspect is accurately estimating the carbon... | Ahmad Faiz, Fan Chen, Lei Jiang, Prateek Sharma, Rita Chukwunyere Osi, Ruhan Wang, Sotaro Kaneda |  |
| 270 |  |  [A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis](https://openreview.net/forum?id=9JQtrumvg8) |  | 0 | Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML.... | Aleksandra Faust, Austin V. Huang, Douglas Eck, Hiroki Furuta, Izzeddin Gur, Mustafa Safdari, Yutaka Matsuo |  |
| 271 |  |  [Lipschitz Singularities in Diffusion Models](https://openreview.net/forum?id=WNkW0cOwiz) |  | 0 | Diffusion models, which employ stochastic differential equations to sample images through integrals, have emerged as a dominant class of generative models. However, the rationality of the diffusion process itself receives limited attention, leaving the question of whether the problem is well-posed... | Deli Zhao, Fan Cheng, Han Zhang, Jingren Zhou, Kai Zhu, Lianghua Huang, Ruili Feng, Yifei Zhang, Yu Liu, Yujun Shen, Zhantao Yang |  |
| 272 |  |  [Interpreting CLIP's Image Representation via Text-Based Decomposition](https://openreview.net/forum?id=5Ca9sSzuDp) |  | 0 | We investigate the CLIP image encoder by analyzing how individual model components affect the final representation. We decompose the image representation as a sum across individual image patches, model layers, and attention heads, and use CLIP's text representation to interpret the summands.... | Alexei A. Efros, Jacob Steinhardt, Yossi Gandelsman |  |
| 273 |  |  [Multisize Dataset Condensation](https://openreview.net/forum?id=FVhmnvqnsI) |  | 0 | While dataset condensation effectively enhances training efficiency, its application in on-device scenarios brings unique challenges. 1) Due to the fluctuating computational resources of these devices, there's a demand for a flexible dataset size that diverges from a predefined size. 2) The limited... | Ivor W. Tsang, Joey Tianyi Zhou, Lingao Xiao, Yang He |  |
| 274 |  |  [DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation](https://openreview.net/forum?id=UyNXMqnN3c) |  | 0 | Recent advances in 3D content creation mostly leverage optimization-based 3D generation via score distillation sampling (SDS). Though promising results have been exhibited, these methods often suffer from slow per-sample optimization, limiting their practical usage. In this paper, we propose... | Gang Zeng, Hang Zhou, Jiawei Ren, Jiaxiang Tang, Ziwei Liu |  |
| 275 |  |  [LRM: Large Reconstruction Model for Single Image to 3D](https://openreview.net/forum?id=sllU8vvsFF) |  | 0 | We propose the first Large Reconstruction Model (LRM) that predicts the 3D model of an object from a single input image within just 5 seconds. In contrast to many previous methods that are trained on small-scale datasets such as ShapeNet in a category-specific fashion, LRM adopts a highly scalable... | Difan Liu, Feng Liu, Hao Tan, Jiuxiang Gu, Kai Zhang, Kalyan Sunkavalli, Sai Bi, Trung Bui, Yang Zhou, Yicong Hong |  |
| 276 |  |  [How Well Do Supervised 3D Models Transfer to Medical Imaging Tasks?](https://openreview.net/forum?id=AhizIPytk4) |  | 0 | The pre-training and fine-tuning paradigm has become prominent in transfer learning. For example, if the model is pre-trained on ImageNet and then fine-tuned to PASCAL, it can significantly outperform that trained on PASCAL from scratch. While ImageNet pre-training has shown enormous success, it is... | Alan L. Yuille, Wenxuan Li, Zongwei Zhou |  |
| 277 |  |  [Gene Regulatory Network Inference in the Presence of Dropouts: a Causal View](https://openreview.net/forum?id=gFR4QwK53h) |  | 0 | Gene regulatory network inference (GRNI) is a challenging problem, particularly owing to the presence of zeros in single-cell RNA sequencing data: some are biological zeros representing no gene expression, while some others are technical zeros arising from the sequencing procedure (aka dropouts),... | Gongxu Luo, Haoyue Dai, Ignavier Ng, Kun Zhang, Petar Stojanov, Peter Spirtes |  |
| 278 |  |  [Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming](https://openreview.net/forum?id=v7ZPwoHU1j) |  | 0 | $K$-means clustering is a widely used machine learning method for identifying patterns in large datasets. Recently, semidefinite programming (SDP) relaxations have been proposed for solving the $K$-means optimization problem, which enjoy strong statistical optimality guarantees. However, the... | Richard Y. Zhang, Xiaohui Chen, Yubo Zhuang, Yun Yang |  |
| 279 |  |  [Unprocessing Seven Years of Algorithmic Fairness](https://openreview.net/forum?id=jr03SfWsBS) |  | 0 | Seven years ago, researchers proposed a postprocessing method to equalize the error rates of a model across different demographic groups. The work launched hundreds of papers purporting to improve over the postprocessing baseline. We empirically evaluate these claims through thousands of model... | André F. Cruz, Moritz Hardt |  |
| 280 |  |  [InfoBatch: Lossless Training Speed Up by Unbiased Dynamic Data Pruning](https://openreview.net/forum?id=C61sk5LsK6) |  | 0 | Data pruning aims to obtain lossless performances with less overall cost. A common approach is to filter out samples that make less contribution to the training. This could lead to gradient expectation bias compared to the original data. To solve this problem, we propose InfoBatch, a novel... | Baigui Sun, Daquan Zhou, Jianyang Gu, Kai Wang, Lei Shang, Xiangyu Peng, Xuansong Xie, Yang You, Zangwei Zheng, Zhaopan Xu, Ziheng Qin |  |
| 281 |  |  [Multi-granularity Correspondence Learning from Long-term Noisy Videos](https://openreview.net/forum?id=9Cu8MRmhq2) |  | 0 | Existing video-language studies mainly focus on learning short video clips, leaving long-term temporal dependencies rarely explored due to over-high computational cost of modeling long videos. To address this issue, one feasible solution is learning the correspondence between video clips and... | Jia Liu, Jie Zhang, Xi Peng, Yijie Lin, Zhenyu Huang, Zujie Wen |  |
| 282 |  |  [SaNN: Simple Yet Powerful Simplicial-aware Neural Networks](https://openreview.net/forum?id=eUgS9Ig8JG) |  | 0 | Simplicial neural networks (SNNs) are deep models for higher-order graph representation learning. SNNs learn low-dimensional embeddings of simplices in a simplicial complex by aggregating features of their respective upper, lower, boundary, and coboundary adjacent simplices. The aggregation in SNNs... | Sravanthi Gurugubelli, Sundeep Prabhakar Chepuri |  |
| 283 |  |  [Beyond Memorization: Violating Privacy via Inference with Large Language Models](https://openreview.net/forum?id=kmn0BhQk7p) |  | 0 | Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models’ inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals’ privacy by... | Mark Vero, Martin T. Vechev, Mislav Balunovic, Robin Staab |  |
| 284 |  |  [Controlled Text Generation via Language Model Arithmetic](https://openreview.net/forum?id=SLw9fp4yI6) |  | 0 | As Large Language Models (LLMs) are deployed more widely, customization with respect to vocabulary, style, and character becomes more important. In this work, we introduce model arithmetic, a novel inference framework for composing and biasing LLMs without the need for model (re)training or highly... | Jasper Dekoninck, Luca BeurerKellner, Marc Fischer, Martin T. Vechev |  |
| 285 |  |  [Consistency Training with Learnable Data Augmentation for Graph Anomaly Detection with Limited Supervision](https://openreview.net/forum?id=elMKXvhhQ9) |  | 0 | Graph Anomaly Detection (GAD) has surfaced as a significant field of research, predominantly due to its substantial influence in production environments. Although existing approaches for node anomaly detection have shown effectiveness, they have yet to fully address two major challenges: operating... | Bingsheng He, Bryan Hooi, Jia Chen, Jun Hu, Nan Chen, Rizal Fathony, Zemin Liu |  |
| 286 |  |  [Symmetric Mean-field Langevin Dynamics for Distributional Minimax Problems](https://openreview.net/forum?id=YItWKZci78) |  | 0 | In this paper, we extend mean-field Langevin dynamics to minimax optimization over probability distributions for the first time with symmetric and provably convergent updates. We propose \emph{mean-field Langevin averaged gradient} (MFL-AG), a single-loop algorithm that implements gradient descent... | Juno Kim, Kakei Yamamoto, Kazusato Oko, Taiji Suzuki, Zhuoran Yang |  |
| 287 |  |  [Generalized Policy Iteration using Tensor Approximation for Hybrid Control](https://openreview.net/forum?id=csukJcpYDe) |  | 0 | Control of dynamic systems involving hybrid actions is a challenging task in robotics. To address this, we present a novel algorithm called Generalized Policy Iteration using Tensor Train (TTPI) that belongs to the class of Approximate Dynamic Programming (ADP). We use a low-rank tensor... | Suhan Shetty, Sylvain Calinon, Teng Xue |  |
| 288 |  |  [Generalization error of spectral algorithms](https://openreview.net/forum?id=3SJE1WLB4M) |  | 0 | The asymptotically precise estimation of the generalization of kernel methods has recently received attention due to the parallels between neural networks and their associated kernels. However, prior works derive such estimates for training by kernel ridge regression (KRR), whereas neural networks... | Dmitry Yarotsky, Maksim Velikanov, Maxim Panov |  |
| 289 |  |  [Debiased Collaborative Filtering with Kernel-Based Causal Balancing](https://openreview.net/forum?id=Ffjc8ApSbt) |  | 0 | Collaborative filtering builds personalized models from the collected user feedback. However, the collected data is observational rather than experimental, leading to various biases in the data, which can significantly affect the learned model. To address this issue, many studies have focused on... | Chunyuan Zheng, Haoxuan Li, Peng Cui, Peng Wu, Xu Chen, Yanghao Xiao, Zhi Geng |  |
| 290 |  |  [The Effective Horizon Explains Deep RL Performance in Stochastic Environments](https://openreview.net/forum?id=5ES5Hdlbxw) |  | 0 | Reinforcement learning (RL) theory has largely focused on proving minimax sample complexity bounds. These require strategic exploration algorithms that use relatively limited function classes for representing the policy or value function. Our goal is to explain why deep RL algorithms often perform... | Anca D. Dragan, Banghua Zhu, Cassidy Laidlaw, Stuart Russell |  |
| 291 |  |  [Selective Visual Representations Improve Convergence and Generalization for Embodied AI](https://openreview.net/forum?id=kC5nZDU5zf) |  | 0 | Embodied AI models often employ off the shelf vision backbones like CLIP to encode their visual observations. Although such general purpose representations encode rich syntactic and semantic information about the scene, much of this information is often irrelevant to the specific task at hand. This... | Ainaz Eftekhar, Ali Farhadi, Aniruddha Kembhavi, Jiafei Duan, KuoHao Zeng, Ranjay Krishna |  |
| 292 |  |  [Improving Generalization of Alignment with Human Preferences through Group Invariant Learning](https://openreview.net/forum?id=fwCoLe3TAX) |  | 0 | The success of AI assistants based on language models (LLMs) hinges crucially on Reinforcement Learning from Human Feedback (RLHF), which enables the generation of responses more aligned with human preferences. As universal AI assistants, there's a growing expectation for them to perform... | Haoran Huang, Qi Zhang, Rui Zheng, Shihan Dou, Tao Gui, Wei Shen, Wenbin Lai, Xiao Wang, Xuanjing Huang, Yuan Hua, Yuhao Zhou, Zhiheng Xi |  |
| 293 |  |  [PINNACLE: PINN Adaptive ColLocation and Experimental points selection](https://openreview.net/forum?id=GzNaCp6Vcg) |  | 0 | Physics-Informed Neural Networks (PINNs), which incorporate PDEs as soft constraints, train with a composite loss function that contains multiple training point types: different types of collocation points chosen during training to enforce each PDE and initial/boundary conditions, and experimental... | Apivich Hemachandra, Bryan Kian Hsiang Low, Gregory Kang Ruey Lau, SeeKiong Ng |  |
| 294 |  |  [Learning to Relax: Setting Solver Parameters Across a Sequence of Linear System Instances](https://openreview.net/forum?id=5t57omGVMw) |  | 0 | Solving a linear system ${\bf Ax}={\bf b}$ is a fundamental scientific computing primitive for which numerous solvers and preconditioners have been developed. These come with parameters whose optimal values depend on the system being solved and are often impossible or too expensive to identify;... | Ameet Talwalkar, Edmond Chow, MariaFlorina Balcan, Mikhail Khodak |  |
| 295 |  |  [Rotation Has Two Sides: Evaluating Data Augmentation for Deep One-class Classification](https://openreview.net/forum?id=Ad81awoBVS) |  | 0 | One-class classification (OCC) involves predicting whether a new data is normal or anomalous based solely on the data from a single class during training. Various attempts have been made to learn suitable representations for OCC within a self-supervised framework. Notably, discriminative methods... | Di Huang, Guodong Wang, Xiuguo Bao, Yunhong Wang |  |
| 296 |  |  [Realistic Evaluation of Semi-supervised Learning Algorithms in Open Environments](https://openreview.net/forum?id=RvUVMjfp8i) |  | 0 | Semi-supervised learning (SSL) is a powerful paradigm for leveraging unlabeled data and has been proven to be successful across various tasks. Conventional SSL studies typically assume close environment scenarios where labeled and unlabeled examples are independently sampled from the same... | LanZhe Guo, LinHan Jia, Yufeng Li, Zhi Zhou |  |
| 297 |  |  [Efficient Inverse Multiagent Learning](https://openreview.net/forum?id=JzvIWvC9MG) |  | 0 | In this paper, we study inverse game theory (resp. inverse multiagent learning) in which the goal is to find parameters of a game’s payoff functions for which the expected (resp. sampled) behavior is an equilibrium. We formulate these problems as generative-adversarial (i.e., min-max) optimization... | Alec Koppel, Amy Greenwald, Denizalp Goktas, Sadie Zhao, Sumitra Ganesh |  |
| 298 |  |  [On the Role of Discrete Tokenization in Visual Representation Learning](https://openreview.net/forum?id=WNLAkjUm19) |  | 0 | In the realm of self-supervised learning (SSL), masked image modeling (MIM) has gained popularity alongside contrastive learning methods. MIM involves reconstructing masked regions of input images using their unmasked portions. A notable subset of MIM methodologies employs discrete tokens as the... | Tianqi Du, Yifei Wang, Yisen Wang |  |
| 299 |  |  [The Consensus Game: Language Model Generation via Equilibrium Search](https://openreview.net/forum?id=n9xeGcI4Yg) |  | 0 | When applied to question answering and other text generation tasks, language models (LMs) may be queried generatively (by sampling answers from their output distribution) or discriminatively (by using them to score or rank a set of candidate answers). These procedures sometimes yield very different... | Athul Paul Jacob, Gabriele Farina, Jacob Andreas, Yikang Shen |  |
| 300 |  |  [AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents](https://openreview.net/forum?id=M6XWoEdmwf) |  | 0 | We introduce AMAGO, an in-context Reinforcement Learning (RL) agent that uses sequence models to tackle the challenges of generalization, long-term memory, and meta-learning. Recent works have shown that off-policy learning can make in-context RL with recurrent policies viable. Nonetheless, these... | Jake Grigsby, Linxi Fan, Yuke Zhu |  |
| 301 |  |  [PILOT: An $\mathcal{O}(1/K)$-Convergent Approach for Policy Evaluation with Nonlinear Function Approximation](https://openreview.net/forum?id=OkHHJcMroY) |  | 0 | Learning an accurate value function for a given policy is a critical step in solving reinforcement learning (RL) problems. So far, however, the convergence speed and sample complexity performances of most existing policy evaluation algorithms remain unsatisfactory, particularly with non-linear... | Jia Liu, Songtao Lu, Xin Zhang, Zhengyuan Zhu, Zhuqing Liu |  |
| 302 |  |  [Confronting Reward Model Overoptimization with Constrained RLHF](https://openreview.net/forum?id=gkfUvn0fLU) |  | 0 | Large language models are typically aligned with human preferences by optimizing reward models (RMs) fitted to human feedback. However, human preferences are multi-faceted, and it is increasingly common to derive reward from a composition of simpler reward models which each capture a different... | Aaditya K. Singh, Anca D. Dragan, DJ Strouse, Ruslan Salakhutdinov, Stephen Marcus McAleer, Ted Moskovitz, Tuomas Sandholm |  |
| 303 |  |  [LiDAR: Sensing Linear Probing Performance in Joint Embedding SSL Architectures](https://openreview.net/forum?id=f3g5XpL9Kb) |  | 0 | Joint embedding (JE) architectures have emerged as a promising avenue for ac- quiring transferable data representations. A key obstacle to using JE methods, however, is the inherent challenge of evaluating learned representations without access to a downstream task, and an annotated dataset.... | Chen Huang, Etai Littwin, Hanlin Goh, Joshua M. Susskind, Laurent Dinh, Omid Saremi, Preetum Nakkiran, Vimal Thilak |  |
| 304 |  |  [Improved Efficiency Based on Learned Saccade and Continuous Scene Reconstruction From Foveated Visual Sampling](https://openreview.net/forum?id=lOwkOIUJtx) |  | 0 | High accuracy, low latency and high energy efficiency represent a set of contradictory goals when searching for system solutions for image classification and detection. While high-quality images naturally result in more precise detection and classification, they also result in a heavier... | Daniel Tso, Jiayang Liu, Qinru Qiu, Yiming Bu |  |
| 305 |  |  [Overthinking the Truth: Understanding how Language Models Process False Demonstrations](https://openreview.net/forum?id=Tigr1kMDZy) |  | 0 | Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens... | Danny Halawi, Jacob Steinhardt, JeanStanislas Denain |  |
| 306 |  |  [MT-Ranker: Reference-free machine translation evaluation by inter-system ranking](https://openreview.net/forum?id=Rry1SeSOQL) |  | 0 | Traditionally, Machine Translation (MT) Evaluation has been treated as a regression problem -- producing an absolute translation-quality score. This approach has two limitations: i) the scores lack interpretability, and human annotators struggle with giving consistent scores; ii) most scoring... | Ibraheem Muhammad Moosa, Rui Zhang, Wenpeng Yin |  |
| 307 |  |  [MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning](https://openreview.net/forum?id=jenyYQzue1) |  | 0 | While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings. However, evaluating LLM reasoning is challenging because system capabilities continue... | Greg Durrett, Kaj Bostrom, Swarat Chaudhuri, Xi Ye, Zayne Sprague |  |
| 308 |  |  [Harnessing Density Ratios for Online Reinforcement Learning](https://openreview.net/forum?id=THJEa8adBn) |  | 0 | The theories of offline and online reinforcement learning, despite having evolved in parallel, have begun to show signs of the possibility for a unification, with algorithms and analysis techniques for one setting often having natural counterparts in the other. However, the notion of \*density... | Ayush Sekhari, Dylan J. Foster, Nan Jiang, Philip Amortila, Tengyang Xie |  |
| 309 |  |  [Predictive, scalable and interpretable knowledge tracing on structured domains](https://openreview.net/forum?id=NgaLU2fP5D) |  | 0 | Intelligent tutoring systems optimize the selection and timing of learning materials to enhance understanding and long-term retention. This requires estimates of both the learner's progress ("knowledge tracing"; KT), and the prerequisite structure of the learning domain ("knowledge mapping"). While... | Charley M. Wu, Hanqi Zhou, Robert Bamler, Álvaro TejeroCantero |  |
| 310 |  |  [From Bricks to Bridges: Product of Invariances to Enhance Latent Space Communication](https://openreview.net/forum?id=vngVydDWft) |  | 0 | It has been observed that representations learned by distinct neural networks conceal structural similarities when the models are trained under similar inductive biases. From a geometric perspective, identifying the classes of transformations and the related invariances that connect these... | Emanuele Rodolà, Irene Cannistraci, Luca Moschella, Marco Fumero, Valentino Maiorca |  |
| 311 |  |  [Proximal Policy Gradient Arborescence for Quality Diversity Reinforcement Learning](https://openreview.net/forum?id=TFKIfhvdmZ) |  | 0 | Training generally capable agents that thoroughly explore their environment and learn new and diverse skills is a long-term goal of robot learning. Quality Diversity Reinforcement Learning (QD-RL) is an emerging research area that blends the best aspects of both fields – Quality Diversity (QD)... | Aleksei Petrenko, Bryon Tjanaka, Gaurav S. Sukhatme, Matthew Christopher Fontaine, Stefanos Nikolaidis, Sumeet Batra |  |
| 312 |  |  [Memorization Capacity of Multi-Head Attention in Transformers](https://openreview.net/forum?id=MrR3rMxqqv) |  | 0 | Transformers have become the go-to architecture for language and vision tasks, yet their theoretical properties, especially memorization capacity, remain elusive. This paper investigates the memorization abilities of multi-head attention mechanisms, examining how many example sequences they can... | Christos Thrampoulidis, Renjie Liao, Sadegh Mahdavi |  |
| 313 |  |  [Circuit Component Reuse Across Tasks in Transformer Language Models](https://openreview.net/forum?id=fpoAYV6Wsk) |  | 0 | Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a... | Carsten Eickhoff, Ellie Pavlick, Jack Merullo |  |
| 314 |  |  [Likelihood Training of Cascaded Diffusion Models via Hierarchical Volume-preserving Maps](https://openreview.net/forum?id=sojpn00o8z) |  | 0 | Cascaded models are multi-scale generative models with a marked capacity for producing perceptually impressive samples at high resolutions. In this work, we show that they can also be excellent likelihood models, so long as we overcome a fundamental difficulty with probabilistic multi-scale models:... | Henry Li, Ronen Basri, Yuval Kluger |  |
| 315 |  |  [Empirical Analysis of Model Selection for Heterogeneous Causal Effect Estimation](https://openreview.net/forum?id=yuy6cGt3KL) |  | 0 | We study the problem of model selection in causal inference, specifically for conditional average treatment effect (CATE) estimation. Unlike machine learning, there is no perfect analogue of cross-validation for model selection as we do not observe the counterfactual potential outcomes. Towards... | Brady Neal, Divyat Mahajan, Ioannis Mitliagkas, Vasilis Syrgkanis |  |
| 316 |  |  [Confidential-DPproof: Confidential Proof of Differentially Private Training](https://openreview.net/forum?id=PQY2v6VtGe) |  | 0 | Post hoc privacy auditing techniques can be used to test the privacy guarantees of a model, but come with several limitations: (i) they can only establish lower bounds on the privacy loss, (ii) the intermediate model updates and some data must be shared with the auditor to get a better... | Adrian Weller, Ali Shahin Shamsabadi, Aurélien Bellet, Gefei Tan, Hamed Haddadi, Nicolas Papernot, Tudor Cebere, Xiao Wang |  |
| 317 |  |  [In-Context Pretraining: Language Modeling Beyond Document Boundaries](https://openreview.net/forum?id=LXVswInHOo) |  | 0 | Language models are currently trained to predict tokens given document prefixes, enabling them to zero shot long form generation and prompting-style tasks which can be reduced to document completion. We instead present IN-CONTEXT PRETRAINING, a new approach where language models are trained on a... | Chunting Zhou, Luke Zettlemoyer, Margaret Li, Maria Lomeli, Mike Lewis, Noah A. Smith, Sewon Min, Weijia Shi, Wentau Yih, Xi Victoria Lin |  |
| 318 |  |  [What's In My Big Data?](https://openreview.net/forum?id=RvfPnOkPV4) |  | 0 | Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose What's In My Big Data? (WIMBD), a... | Abhilasha Ravichander, Akshita Bhagia, Alane Suhr, Dirk Groeneveld, Dustin Schwenk, Evan Pete Walsh, Hannaneh Hajishirzi, Ian Magnusson, Jesse Dodge, Luca Soldaini, Noah A. Smith, Sameer Singh, Yanai Elazar |  |
| 319 |  |  [On Diffusion Modeling for Anomaly Detection](https://openreview.net/forum?id=lR3rk7ysXz) |  | 0 | Known for their impressive performance in generative modeling, diffusion models are attractive candidates for density-based anomaly detection. This paper investigates different variations of diffusion modeling for unsupervised and semi-supervised anomaly detection. In particular, we find that... | Siamak Ravanbakhsh, Victor Livernoche, Vineet Jain, Yashar Hezaveh |  |
| 320 |  |  [Social Reward: Evaluating and Enhancing Generative AI through Million-User Feedback from an Online Creative Community](https://openreview.net/forum?id=tjn2YZSHUv) |  | 0 | Social reward as a form of community recognition provides a strong source of motivation for users of online platforms to actively engage and contribute with content to accumulate peers approval. In the realm of text-conditioned image synthesis, the recent surge in progress has ushered in a... | Arman Isajanyan, Artur Shatveryan, David Kocharian, Humphrey Shi, Zhangyang Wang |  |
| 321 |  |  [Role of Locality and Weight Sharing in Image-Based Tasks: A Sample Complexity Separation between CNNs, LCNs, and FCNs](https://openreview.net/forum?id=AfnsTnYphT) |  | 0 | Vision tasks are characterized by the properties of locality and translation invariance. The superior performance of convolutional neural networks (CNNs) on these tasks is widely attributed to the inductive bias of locality and weight sharing baked into their architecture. Existing attempts to... | Aakash Lahoti, Aarti Singh, Ezra Winston, Stefani Karp, Yuanzhi Li |  |
| 322 |  |  [Lion Secretly Solves a Constrained Optimization: As Lyapunov Predicts](https://openreview.net/forum?id=e4xS9ZarDr) |  | 0 | Lion (Evolved Sign Momentum), a new optimizer discovered through program search, has shown promising results in training large AI models. It achieves results comparable to AdamW but with greater memory efficiency. As what we can expect from the result of the random search, Lion blends a number of... | Bo Liu, Kaizhao Liang, Lizhang Chen, Qiang Liu |  |
| 323 |  |  [Distributionally Robust Optimization with Bias and Variance Reduction](https://openreview.net/forum?id=TTrzgEZt9s) |  | 0 | We consider the distributionally robust optimization (DRO) problem, wherein a learner optimizes the worst-case empirical risk achievable by reweighing the observed training examples. We present Prospect, a stochastic gradient-based algorithm that only requires tuning a single learning rate... | Krishna Pillutla, Ronak Mehta, Vincent Roulet, Zaïd Harchaoui |  |
| 324 |  |  [A Benchmark for Learning to Translate a New Language from One Grammar Book](https://openreview.net/forum?id=tbVWug9f2h) |  | 0 | Large language models (LLMs) can perform impressive feats with in-context learning or lightweight finetuning. It is natural to wonder how well these models adapt to genuinely new tasks, but how does one find tasks that are unseen in internet-scale training sets? We turn to a field that is... | Dan Jurafsky, Eline Visser, Garrett Tanzer, Luke MelasKyriazi, Mirac Suzgun |  |
| 325 |  |  [Improving Offline RL by Blending Heuristics](https://openreview.net/forum?id=MCl0TLboP1) |  | 0 | We propose \*\*H\*\*e\*\*u\*\*ristic \*\*Bl\*\*ending (HUBL), a simple performance-improving technique for a broad class of offline RL algorithms based on value bootstrapping. HUBL modifies the Bellman operators used in these algorithms, partially replacing the bootstrapped values with heuristic... | Aldo Pacchiano, Andrey Kolobov, ChingAn Cheng, Sinong Geng |  |
| 326 |  |  [Decision ConvFormer: Local Filtering in MetaFormer is Sufficient for Decision Making](https://openreview.net/forum?id=af2c8EaKl8) |  | 0 | The recent success of Transformer in natural language processing has sparked its use in various domains. In offline reinforcement learning (RL), Decision Transformer (DT) is emerging as a promising model based on Transformer. However, we discovered that the attention module of DT is not appropriate... | Jeonghye Kim, Suyoung Lee, Woojun Kim, Youngchul Sung |  |
| 327 |  |  [How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?](https://openreview.net/forum?id=vSh5ePa0ph) |  | 0 | Transformers pretrained on diverse tasks exhibit remarkable in-context learning (ICL) capabilities, enabling them to solve unseen tasks solely based on input contexts without adjusting model parameters. In this paper, we study ICL in one of its simplest setups: pretraining a single-layer linear... | Difan Zou, Jingfeng Wu, Peter L. Bartlett, Quanquan Gu, Vladimir Braverman, Zixiang Chen |  |
| 328 |  |  [Tool-Augmented Reward Modeling](https://openreview.net/forum?id=d94x0gWTUX) |  | 0 | Reward modeling (\*a.k.a.\*, preference modeling) is instrumental for aligning large language models with human preferences, particularly within the context of reinforcement learning from human feedback (RLHF). While conventional reward models (RMs) have exhibited remarkable scalability, they oft... | Hao Tian, Hua Wu, Lei Li, Ningyu Zhang, Shuohuan Wang, Yekun Chai, Yu Sun |  |
| 329 |  |  [Reward-Consistent Dynamics Models are Strongly Generalizable for Offline Reinforcement Learning](https://openreview.net/forum?id=GSBHKiw19c) |  | 0 | Learning a precise dynamics model can be crucial for offline reinforcement learning, which, unfortunately, has been found to be quite challenging. Dynamics models that are learned by fitting historical transitions often struggle to generalize to unseen transitions. In this study, we identify a... | FanMing Luo, Tian Xu, Xingchen Cao, Yang Yu |  |
| 330 |  |  [Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback](https://openreview.net/forum?id=6yv8UHVJn4) |  | 0 | We study online reinforcement learning in linear Markov decision processes with adversarial losses and bandit feedback. We introduce two algorithms that achieve improved regret performance compared to existing approaches. The first algorithm, although computationally inefficient, achieves a regret... | ChenYu Wei, Haolin Liu, Julian Zimmert |  |
| 331 |  |  [Dual RL: Unification and New Methods for Reinforcement and Imitation Learning](https://openreview.net/forum?id=xt9Bu66rqv) |  | 0 | The goal of reinforcement learning (RL) is to find a policy that maximizes the expected cumulative return. It has been shown that this objective can be represented as an optimization problem of state-action visitation distribution under linear constraints. The dual problem of this formulation,... | Amy Zhang, Harshit Sikchi, Qinqing Zheng, Scott Niekum |  |
| 332 |  |  [Out-Of-Domain Unlabeled Data Improves Generalization](https://openreview.net/forum?id=Bo6GpQ3B9a) |  | 0 | We propose a novel framework for incorporating unlabeled data into semi-supervised classification problems, where scenarios involving the minimization of either i) adversarially robust or ii) non-robust loss functions have been considered. Notably, we allow the unlabeled samples to deviate slightly... | Abolfazl S. Motahari, Alireza Heidari, Amir Najafi, Babak H. Khalaj, Mohammad Hosein Movasaghinia, Seyed Amir Hossein Saberi |  |
| 333 |  |  [Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation](https://openreview.net/forum?id=r42tSSCHPh) |  | 0 | The rapid progress in open-source large language models (LLMs) is significantly advancing AI development. Extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. However, even carefully... | Danqi Chen, Kai Li, Mengzhou Xia, Samyak Gupta, Yangsibo Huang |  |
| 334 |  |  [PolyGCL: GRAPH CONTRASTIVE LEARNING via Learnable Spectral Polynomial Filters](https://openreview.net/forum?id=y21ZO6M86t) |  | 0 | Recently, Graph Contrastive Learning (GCL) has achieved significantly superior performance in self-supervised graph representation learning. However, the existing GCL technique has inherent smooth characteristics because of its low-pass GNN encoder and objective based on homophily assumption, which... | Jingyu Chen, Runlin Lei, Zhewei Wei |  |
| 335 |  |  [Solving Homogeneous and Heterogeneous Cooperative Tasks with Greedy Sequential Execution](https://openreview.net/forum?id=hB2hXtxIPH) |  | 0 | Cooperative multi-agent reinforcement learning (MARL) is extensively used for solving complex cooperative tasks, and value decomposition methods are a prevalent approach for this domain. However, these methods have not been successful in addressing both homogeneous and heterogeneous tasks... | Bo An, Dong Xing, Pengjie Gu, Shanqi Liu, Xinrun Wang, Yong Liu |  |
| 336 |  |  [Stabilizing Contrastive RL: Techniques for Robotic Goal Reaching from Offline Data](https://openreview.net/forum?id=Xkf2EBj4w3) |  | 0 | Robotic systems that rely primarily on self-supervised learning have the potential to decrease the amount of human annotation and engineering effort required to learn control strategies. In the same way that prior robotic systems have leveraged self-supervised techniques from computer vision (CV)... | Benjamin Eysenbach, Chongyi Zheng, Homer Rich Walke, Kuan Fang, Patrick Yin, Ruslan Salakhutdinov, Sergey Levine |  |
| 337 |  |  [Multi-View Causal Representation Learning with Partial Observability](https://openreview.net/forum?id=OGtnhKQJms) |  | 0 | We present a unified framework for studying the identifiability of representations learned from simultaneously observed views, such as different data modalities. We allow a partially observed setting in which each view constitutes a nonlinear mixture of a subset of underlying latent variables,... | Danru Xu, Dingling Yao, Francesco Locatello, Georg Martius, Julius von Kügelgen, Perouz Taslakian, Sara Magliacane, Sébastien Lachapelle |  |
| 338 |  |  [CABINET: Content Relevance-based Noise Reduction for Table Question Answering](https://openreview.net/forum?id=SQrHpTllXa) |  | 0 | Table understanding capability of Large Language Models (LLMs) has been extensively studied through the task of question-answering (QA) over tables. Typically, only a small part of the whole table is relevant to derive the answer for a given question. The irrelevant parts act as noise and are... | Balaji Krishnamurthy, Heril Changwal, Milan Aggarwal, Sohan Patnaik, Sumit Bhatia, Yaman Kumar |  |
| 339 |  |  [Safe RLHF: Safe Reinforcement Learning from Human Feedback](https://openreview.net/forum?id=TyFrPOKYXw) |  | 0 | With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical. However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training. To... | Jiaming Ji, Josef Dai, Mickel Liu, Ruiyang Sun, Xinbo Xu, Xuehai Pan, Yaodong Yang, Yizhou Wang |  |
| 340 |  |  [Benchmarking Algorithms for Federated Domain Generalization](https://openreview.net/forum?id=wprSv7ichW) |  | 0 | While prior federated learning (FL) methods mainly consider client heterogeneity, we focus on the \*Federated Domain Generalization (DG)\* task, which introduces train-test heterogeneity in the FL context. Existing evaluations in this field are limited in terms of the scale of the clients and... | David I. Inouye, Ruqi Bai, Saurabh Bagchi |  |
| 341 |  |  [CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity](https://openreview.net/forum?id=PczQtTsTIX) |  | 0 | Sample efficiency is a crucial problem in deep reinforcement learning. Recent algorithms, such as REDQ and DroQ, found a way to improve the sample efficiency by increasing the update-to-data (UTD) ratio to 20 gradient update steps on the critic per environment sample. However, this comes at the... | Aditya Bhatt, Artemij Amiranashvili, Boris Belousov, Daniel Palenicek, Jan Peters, Max Argus, Thomas Brox |  |
| 342 |  |  [Blending Imitation and Reinforcement Learning for Robust Policy Improvement](https://openreview.net/forum?id=eJ0dzPJq1F) |  | 0 | While reinforcement learning (RL) has shown promising performance, its sample complexity continues to be a substantial hurdle, restricting its broader application across a variety of domains. Imitation learning (IL) utilizes oracles to improve sample efficiency, yet it is often constrained by the... | Matthew R. Walter, Rick Stevens, Takuma Yoneda, Xuefeng Liu, Yuxin Chen |  |
| 343 |  |  [H-GAP: Humanoid Control with a Generalist Planner](https://openreview.net/forum?id=LYG6tBlEX0) |  | 0 | Humanoid control is an important research challenge offering avenues for integration into human-centric infrastructures and enabling physics-driven humanoid animations. The daunting challenges in this field stem from the difficulty of optimizing in high-dimensional action spaces and the instability... | Edward Grefenstette, Michael Janner, Nolan Wagener, Tim Rocktäschel, Yicheng Luo, Yingchen Xu, Yuandong Tian, Zhengyao Jiang |  |
| 344 |  |  [Unlocking the Power of Representations in Long-term Novelty-based Exploration](https://openreview.net/forum?id=OwtMhMSybu) |  | 0 | We introduce Robust Exploration via Clustering-based Online Density Estimation (RECODE), a non-parametric method for novelty-based exploration that estimates visitation counts for clusters of states based on their similarity in a chosen embedding space. By adapting classical clustering to the... | Alaa Saade, Bilal Piot, Charles Blundell, Daniele Calandriello, Leopoldo Sarra, Michal Valko, Oliver Groth, Pablo Sprechmann, Steven Kapturowski |  |
| 345 |  |  [Accelerating Data Generation for Neural Operators via Krylov Subspace Recycling](https://openreview.net/forum?id=UpgRVWexaD) |  | 0 | Learning neural operators for solving partial differential equations (PDEs) has attracted great attention due to its high inference efficiency. However, training such operators requires generating a substantial amount of labeled data, i.e., PDE problems together with their solutions. The data... | Bin Li, Feng Wu, Hong Wang, Jie Wang, Zhen Wang, Zhongkai Hao, Zijie Geng |  |
| 346 |  |  [Deep Orthogonal Hypersphere Compression for Anomaly Detection](https://openreview.net/forum?id=cJs4oE4m9Q) |  | 0 | Many well-known and effective anomaly detection methods assume that a reasonable decision boundary has a hypersphere shape, which however is difficult to obtain in practice and is not sufficiently compact, especially when the data are in high-dimensional spaces. In this paper, we first propose a... | Jicong Fan, Jinyu Cai, Yan Sun, Yunhe Zhang |  |
| 347 |  |  [On the Role of General Function Approximation in Offline Reinforcement Learning](https://openreview.net/forum?id=JSS9rKHySk) |  | 0 | We study offline reinforcement learning (RL) with general function approximation. General function approximation is a powerful tool for algorithm design and analysis, but its adaptation to offline RL encounters several challenges due to varying approximation targets and assumptions that blur the... | Chenjie Mao, Qiaosheng Zhang, Xuelong Li, Zhen Wang |  |
| 348 |  |  [Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Maps](https://openreview.net/forum?id=mYWsyTuiRp) |  | 0 | Transformers are ubiquitous in wide tasks. Interpreting their internals is a pivotal goal. Nevertheless, their particular components, feed-forward (FF) blocks, have typically been less analyzed despite their substantial parameter amounts. We analyze the input contextualization effects of FF blocks... | Goro Kobayashi, Kentaro Inui, Sho Yokoi, Tatsuki Kuribayashi |  |
| 349 |  |  [Asymptotically Free Sketched Ridge Ensembles: Risks, Cross-Validation, and Tuning](https://openreview.net/forum?id=i9Vs5NGDpk) |  | 0 | We employ random matrix theory to establish consistency of generalized cross validation (GCV) for estimating prediction risks of sketched ridge regression ensembles, enabling efficient and consistent tuning of regularization and sketching parameters. Our results hold for a broad class of... | Daniel LeJeune, Pratik Patil |  |
| 350 |  |  [Nemesis: Normalizing the Soft-prompt Vectors of Vision-Language Models](https://openreview.net/forum?id=zmJDzPh1Dm) |  | 0 | With the prevalence of large-scale pretrained vision-language models (VLMs), such as CLIP, soft-prompt tuning has become a popular method for adapting these models to various downstream tasks. However, few works delve into the inherent properties of learnable soft-prompt vectors, specifically the... | Qiushi Huang, Shuai Fu, Xiequn Wang, Yu Zhang |  |
| 351 |  |  [Towards Understanding Factual Knowledge of Large Language Models](https://openreview.net/forum?id=9OevMUdods) |  | 0 | Large language models (LLMs) have recently driven striking performance improvements across a range of natural language processing tasks. The factual knowledge acquired during pretraining and instruction tuning can be useful in various downstream tasks, such as question answering, and language... | Junzhe Chen, Lijie Wen, Philip S. Yu, Xiaochuan Li, Xuming Hu, Yufei Guo, Zhijiang Guo |  |
| 352 |  |  [CAS: A Probability-Based Approach for Universal Condition Alignment Score](https://openreview.net/forum?id=E78OaH2s3f) |  | 0 | Recent conditional diffusion models have shown remarkable advancements and have been widely applied in fascinating real-world applications. However, samples generated by these models often do not strictly comply with user-provided conditions. Due to this, there have been few attempts to evaluate... | Byunghee Cha, Chunsan Hong, TaeHyun Oh |  |
| 353 |  |  [Demystifying CLIP Data](https://openreview.net/forum?id=5BCFlnfE1g) |  | 0 | Contrastive Language-Image Pre-training (CLIP) is an approach that has advanced research and applications in computer vision, fueling modern recognition systems and generative models. We believe that the main ingredient to the success of CLIP is its \textit{data} and \textit{not} the \textit{model}... | Christoph Feichtenhofer, Gargi Ghosh, Hu Xu, Luke Zettlemoyer, PoYao Huang, Russell Howes, Saining Xie, ShangWen Li, Vasu Sharma, Xiaoqing Ellen Tan |  |
| 354 |  |  [Adversarial AutoMixup](https://openreview.net/forum?id=o8tjamaJ80) |  | 0 | Data mixing augmentation has been widely applied to improve the generalization ability of deep neural networks. Recently, offline data mixing augmentation, e.g. handcrafted and saliency information-based mixup, has been gradually replaced by automatic mixing approaches. Through minimizing two... | Huafeng Qin, Mounîm A. ElYacoubi, Xin Jin, Xinbo Gao, Yun Jiang |  |
| 355 |  |  [Spatially-Aware Transformers for Embodied Agents](https://openreview.net/forum?id=Ts95eXsPBc) |  | 0 | Episodic memory plays a crucial role in various cognitive processes, such as the ability to mentally recall past events. While cognitive science emphasizes the significance of spatial context in the formation and retrieval of episodic memory, the current primary approach to implementing episodic... | Jaesik Yoon, Junmo Cho, Sungjin Ahn |  |
| 356 |  |  [Grounding Language Plans in Demonstrations Through Counterfactual Perturbations](https://openreview.net/forum?id=qoHeuRAcSl) |  | 0 | Grounding the common-sense reasoning of Large Language Models in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and... | Jiayuan Mao, Julie Shah, Michael Hagenow, TsunHsuan Wang, Yanwei Wang |  |
| 357 |  |  [Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies](https://openreview.net/forum?id=DFTHW0MyiW) |  | 0 | In light of the burgeoning success of reinforcement learning (RL) in diverse real-world applications, considerable focus has been directed towards ensuring RL policies are robust to adversarial attacks during test time. Current approaches largely revolve around solving a minimax problem to prepare... | Chenghao Deng, Furong Huang, Xiangyu Liu, Yanchao Sun, Yongyuan Liang |  |
| 358 |  |  [Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy](https://openreview.net/forum?id=eFWG9Cy3WK) |  | 0 | Sparsely activated Mixture-of-Experts (SMoE) has shown promise to scale up the learning capacity of neural networks, however, they have issues like: ($a$) $\textit{High Memory Usage,}$ due to duplication of the network layers into multiple copies as experts; and ($b$) $\textit{Redundancy in... | Mohit Bansal, Pingzhi Li, Prateek Yadav, Tianlong Chen, YiLin Sung, Yu Cheng, Zhenyu Zhang |  |
| 359 |  |  [On Bias-Variance Alignment in Deep Models](https://openreview.net/forum?id=i2Phucne30) |  | 0 | Classical wisdom in machine learning holds that the generalization error can be decomposed into bias and variance, and these two terms exhibit a \emph{trade-off}. However, in this paper, we show that for an ensemble of deep learning based classification models, bias and variance are \emph{aligned}... | Chong You, Lin Chen, Michal Lukasik, Sanjiv Kumar, Wittawat Jitkrittum |  |
| 360 |  |  [SEGNO: Generalizing Equivariant Graph Neural Networks with Physical Inductive Biases](https://openreview.net/forum?id=3oTPsORaDH) |  | 0 | Graph Neural Networks (GNNs) with equivariant properties have emerged as powerful tools for modeling complex dynamics of multi-object physical systems. However, their generalization ability is limited by the inadequate consideration of physical inductive biases: (1) Existing studies overlook the... | Fugee Tsung, Haihong Zhao, Jia Li, Jiashun Cheng, Peilin Zhao, Tingyang Xu, Yang Liu, Yu Rong |  |
| 361 |  |  [Spectrally Transformed Kernel Regression](https://openreview.net/forum?id=OeQE9zsztS) |  | 0 | Unlabeled data is a key component of modern machine learning. In general, the role of unlabeled data is to impose a form of smoothness, usually from the similarity information encoded in a base kernel, such as the ϵ-neighbor kernel or the adjacency matrix of a graph. This work revisits the... | MariaFlorina Balcan, Pradeep Kumar Ravikumar, Rattana Pukdee, Roger Jin, Runtian Zhai |  |
| 362 |  |  [Online GNN Evaluation Under Test-time Graph Distribution Shifts](https://openreview.net/forum?id=KbetDM33YG) |  | 0 | Evaluating the performance of a well-trained GNN model on real-world graphs is a pivotal step for reliable GNN online deployment and serving. Due to a lack of test node labels and unknown potential training-test graph data distribution shifts, conventional model evaluation encounters limitations in... | Bo Du, Dongjin Song, Qingsong Wen, Shirui Pan, Xin Zheng |  |
| 363 |  |  [Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks](https://openreview.net/forum?id=TjhUtloBZU) |  | 0 | Pre-training on large-scale datasets and then fine-tuning on downstream tasks have become a standard practice in deep learning. However, pre-training data often contain label noise that may adversely affect the generalization of the model. This paper aims to understand the nature of noise in... | Ankit Shah, Bhiksha Raj, Hao Chen, Hongxin Wei, Jindong Wang, Masashi Sugiyama, Ran Tao, Xing Xie |  |
| 364 |  |  [WildChat: 1M ChatGPT Interaction Logs in the Wild](https://openreview.net/forum?id=Bl8u7ZRlbM) |  | 0 | Chatbots such as GPT-4 and ChatGPT are now serving millions of users. Despite their widespread use, there remains a lack of public datasets showcasing how these tools are used by a population of users in practice. To bridge this gap, we offered free access to ChatGPT for online users in exchange... | Claire Cardie, Jack Hessel, Wenting Zhao, Xiang Ren, Yejin Choi, Yuntian Deng |  |
| 365 |  |  [Learning Hierarchical Image Segmentation For Recognition and By Recognition](https://openreview.net/forum?id=IRcv4yFX6z) |  | 0 | Large vision and language models learned directly through image-text associations often lack detailed visual substantiation, whereas image segmentation tasks are treated separately from recognition, supervisedly learned without interconnections. Our key observation is that, while an image can be... | Sangwoo Mo, Stella X. Yu, TsungWei Ke |  |
| 366 |  |  [Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models](https://openreview.net/forum?id=plmBsXHxgR) |  | 0 | We introduce new jailbreak attacks on vision language models (VLMs), which use aligned LLMs and are resilient to text-only jailbreak attacks. Specifically, we develop cross-modality attacks on alignment where we pair adversarial images going through the vision encoder with textual prompts to break... | Erfan Shayegani, Nael B. AbuGhazaleh, Yue Dong |  |
| 367 |  |  [DreamFlow: High-quality text-to-3D generation by Approximating Probability Flow](https://openreview.net/forum?id=GURqUuTebY) |  | 0 | Recent progress in text-to-3D generation has been achieved through the utilization of score distillation methods: they make use of the pre-trained text-to-image (T2I) diffusion models by distilling via the diffusion model training objective. However, such an approach inevitably results in the use... | Jinwoo Shin, Kihyuk Sohn, Kyungmin Lee |  |
| 368 |  |  [Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns](https://openreview.net/forum?id=XVhm3X8Fum) |  | 0 | Attention, specifically scaled dot-product attention, has proven effective for natural language, but it does not have a mechanism for handling hierarchical patterns of arbitrary nesting depth, which limits its ability to recognize certain syntactic structures. To address this shortcoming, we... | Brian DuSell, David Chiang |  |
| 369 |  |  [SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents](https://openreview.net/forum?id=mM7VurbA4r) |  | 0 | \*Humans are social beings\*; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents... | Daniel Fried, Graham Neubig, Hao Zhu, Haofei Yu, Leena Mathur, LouisPhilippe Morency, Maarten Sap, Ruohong Zhang, Xuhui Zhou, Yonatan Bisk, Zhengyang Qi |  |
| 370 |  |  [Privileged Sensing Scaffolds Reinforcement Learning](https://openreview.net/forum?id=EpVe8jAjdx) |  | 0 | We need to look at our shoelaces as we first learn to tie them but having mastered this skill, can do it from touch alone. We call this phenomenon “sensory scaffolding”: observation streams that are not needed by a master might yet aid a novice learner. We consider such sensory scaffolding setups... | Dinesh Jayaraman, Edward S. Hu, James Springer, Oleh Rybkin |  |
| 371 |  |  [Learning to Act without Actions](https://openreview.net/forum?id=rvUq3cxpDF) |  | 0 | Pre-training large models on vast amounts of web data has proven to be an effective approach for obtaining powerful, general models in domains such as language and vision. However, this paradigm has not yet taken hold in reinforcement learning. This is because videos, the most abundant form of... | Dominik Schmidt, Minqi Jiang |  |
| 372 |  |  [Error Norm Truncation: Robust Training in the Presence of Data Noise for Text Generation Models](https://openreview.net/forum?id=zMvMwNvs4R) |  | 0 | Text generation models are notoriously vulnerable to errors in the training data. With the wide-spread availability of massive amounts of web-crawled data becoming more commonplace, how can we enhance the robustness of models trained on a massive amount of noisy web-crawled text? In our work, we... | Daniel Khashabi, Haoran Xu, Kenton Murray, Philipp Koehn, Tianjian Li |  |
| 373 |  |  [Massively Scalable Inverse Reinforcement Learning in Google Maps](https://openreview.net/forum?id=z3L59iGALM) |  | 0 | Inverse reinforcement learning (IRL) offers a powerful and general framework for learning humans' latent preferences in route recommendation, yet no approach has successfully addressed planetary-scale problems with hundreds of millions of states and demonstration trajectories. In this paper, we... | Denali Molitor, Jason Trader, Markus Wulfmeier, Matt Barnes, Matt Deeds, Matthew Abueg, Oliver F. Lange, Shawn O'Banion |  |
| 374 |  |  [Thin-Shell Object Manipulations With Differentiable Physics Simulations](https://openreview.net/forum?id=KsUh8MMFKQ) |  | 0 | In this work, we aim to teach robots to manipulate various thin-shell materials. Prior works studying thin-shell object manipulation mostly rely on heuristic policies or learn policies from real-world video demonstrations, and only focus on limited material types and tasks (e.g., cloth unfolding).... | Chao Liu, Chuang Gan, Gu Zhang, Juntian Zheng, Yian Wang, Zhehuan Chen, Zhou Xian |  |
| 375 |  |  [Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks](https://openreview.net/forum?id=7erlRDoaV8) |  | 0 | Pretrained language models sometimes possess knowledge that we do not wish them to, including memorized personal information and knowledge that could be used to harm people. They can also output toxic or harmful text. To mitigate these safety and informational issues, we propose an... | Mohit Bansal, Peter Hase, Vaidehi Patil |  |
| 376 |  |  [Learning to Reject Meets Long-tail Learning](https://openreview.net/forum?id=ta26LtNq2r) |  | 0 | Learning to reject (L2R) is a classical problem where one seeks a classifier capable of abstaining on low-confidence samples. Most prior work on L2R has focused on minimizing the standard misclassification error. However, in many real-world applications, the label distribution is highly imbalanced,... | Aditya Krishna Menon, Harikrishna Narasimhan, Neha Gupta, Sanjiv Kumar, Wittawat Jitkrittum |  |
| 377 |  |  [On the Foundations of Shortcut Learning](https://openreview.net/forum?id=Tj3xLVuE9f) |  | 0 | Deep-learning models can extract a rich assortment of features from data. Which features a model uses depends not only on \*predictivity\*---how reliably a feature indicates training-set labels---but also on \*availability\*---how easily the feature can be extracted from inputs. The literature on... | Hossein Mobahi, Katherine L. Hermann, Michael Curtis Mozer, Thomas Fel |  |
| 378 |  |  [Synaptic Weight Distributions Depend on the Geometry of Plasticity](https://openreview.net/forum?id=x5txICnnjC) |  | 0 | A growing literature in computational neuroscience leverages gradient descent and learning algorithms that approximate it to study synaptic plasticity in the brain. However, the vast majority of this work ignores a critical underlying assumption: the choice of distance for synaptic changes - i.e.... | Arna Ghosh, Blake Aaron Richards, Gauthier Gidel, Guillaume Lajoie, Jonathan Cornford, Roman Pogodin |  |
| 379 |  |  [Graph Metanetworks for Processing Diverse Neural Architectures](https://openreview.net/forum?id=ijK5hyxs0n) |  | 0 | Neural networks efficiently encode learned information within their parameters. Consequently, many tasks can be unified by treating neural networks themselves as input data. When doing so, recent studies demonstrated the importance of accounting for the symmetries and geometry of parameter spaces.... | Derek Lim, Haggai Maron, James Lucas, Jonathan Lorraine, Marc T. Law |  |
| 380 |  |  [Dropout Enhanced Bilevel Training](https://openreview.net/forum?id=06lrITXVAx) |  | 0 | Bilevel optimization problems appear in many widely used machine learning tasks. Bilevel optimization models are sensitive to small changes, and bilevel training tasks typically involve limited datasets. Therefore, overfitting is a common challenge in bilevel training tasks. This paper considers... | Heng Huang, Junyi Li, Peiran Yu |  |
| 381 |  |  [Privacy Amplification for Matrix Mechanisms](https://openreview.net/forum?id=xUzWmFdglP) |  | 0 | Privacy amplification exploits randomness in data selection to provide tighter differential privacy (DP) guarantees. This analysis is key to DP-SGD's success in machine learning (ML), but, is not readily applicable to the newer state-of-the-art (SOTA) algorithms. This is because these algorithms,... | Abhradeep Guha Thakurta, Arun Ganesh, Christopher A. ChoquetteChoo, Thomas Steinke |  |
| 382 |  |  [Bandits Meet Mechanism Design to Combat Clickbait in Online Recommendation](https://openreview.net/forum?id=lsxeNvYqCj) |  | 0 | We study a strategic variant of the multi-armed bandit problem, which we coin the strategic click-bandit. This model is motivated by applications in online recommendation where the choice of recommended items depends on both the click-through rates and the post-click rewards. Like in classical... | Aadirupa Saha, Christos Dimitrakakis, Haifeng Xu, Thomas Kleine Buening |  |
| 383 |  |  [Towards Principled Representation Learning from Videos for Reinforcement Learning](https://openreview.net/forum?id=3mnWvUZIXt) |  | 0 | We study pre-training representations for decision-making using video data, which is abundantly available for tasks such as game agents and software testing. Even though significant empirical advances have been made on this problem, a theoretical understanding remains absent. We initiate the... | Akanksha Saran, Alex Lamb, Dipendra Misra, John Langford, Tengyang Xie |  |
| 384 |  |  [Optimal Sample Complexity of Contrastive Learning](https://openreview.net/forum?id=NU9AYHJvYe) |  | 0 | Contrastive learning is a highly successful technique for learning representations of data from labeled tuples, specifying the distance relations within the tuple. We study the sample complexity of contrastive learning, i.e. the minimum number of labeled tuples sufficient for getting high... | Dmitrii Avdiukhin, Dor Elboim, Grigory Yaroslavtsev, Noga Alon, Orr Fischer |  |
| 385 |  |  [Post-hoc bias scoring is optimal for fair classification](https://openreview.net/forum?id=FM5xfcaR2Y) |  | 0 | We consider a binary classification problem under group fairness constraints, which can be one of Demographic Parity (DP), Equalized Opportunity (EOp), or Equalized Odds (EO). We propose an explicit characterization of Bayes optimal classifier under the fairness constraints, which turns out to be a... | Wenlong Chen, Yang Liu, Yegor Klochkov |  |
| 386 |  |  [Sharpness-Aware Data Poisoning Attack](https://openreview.net/forum?id=bxITGFPVWh) |  | 0 | Recent research has highlighted the vulnerability of Deep Neural Networks (DNNs) against data poisoning attacks. These attacks aim to inject poisoning samples into the models' training dataset such that the trained models have inference failures. While previous studies have executed different types... | Charu C. Aggarwal, Han Xu, Hui Liu, Jie Ren, Jiliang Tang, Pengfei He, Shenglai Zeng, Yingqian Cui |  |
| 387 |  |  [Pre-training with Random Orthogonal Projection Image Modeling](https://openreview.net/forum?id=z4Hcegjzph) |  | 0 | Masked Image Modeling (MIM) is a powerful self-supervised strategy for visual pre-training without the use of labels. MIM applies random crops to input images, processes them with an encoder, and then recovers the masked inputs with a decoder, which encourages the network to capture and learn... | Maryam Haghighat, Peyman Moghadam, Piotr Koniusz, Shaheer Mohamed |  |
| 388 |  |  [Lagrangian Flow Networks for Conservation Laws](https://openreview.net/forum?id=Nshk5YpdWE) |  | 0 | We introduce Lagrangian Flow Networks (LFlows) for modeling fluid densities and velocities continuously in space and time. By construction, the proposed LFlows satisfy the continuity equation, a PDE describing mass conservation in its differential form. Our model is based on the insight that... | Fabricio Arend Torres, Jonathan Aellen, Marcello Massimo Negri, Marco Inversi, Volker Roth |  |
| 389 |  |  [Linearity of Relation Decoding in Transformer Language Models](https://openreview.net/forum?id=w7LU2s14kE) |  | 0 | Much of the knowledge encoded in transformer language models (LMs) may be expressed in terms of relations: relations between words and their synonyms, entities and their attributes, etc. We show that, for a subset of relations, this computation is well-approximated by a single linear transformation... | Arnab Sen Sharma, David Bau, Evan Hernandez, Jacob Andreas, Kevin Meng, Martin Wattenberg, Tal Haklay, Yonatan Belinkov |  |
| 390 |  |  [Subtractive Mixture Models via Squaring: Representation and Learning](https://openreview.net/forum?id=xIHi5nxu9P) |  | 0 | Mixture models are traditionally represented and learned by adding several distributions as components. Allowing mixtures to subtract probability mass or density can drastically reduce the number of components needed to model complex distributions. However, learning such subtractive mixtures while... | Aleksanteri M. Sladek, Antonio Vergari, Arno Solin, Lorenzo Loconte, Martin Trapp, Nicolas Gillis, Stefan Mengel |  |
| 391 |  |  [On the Provable Advantage of Unsupervised Pretraining](https://openreview.net/forum?id=rmXXKxQpOR) |  | 0 | Unsupervised pretraining, which learns a useful representation using a large amount of unlabeled data to facilitate the learning of downstream tasks, is a critical component of modern large-scale machine learning systems. Despite its tremendous empirical success, the rigorous theoretical... | Chi Jin, Jianqing Fan, Jiawei Ge, Shange Tang |  |
| 392 |  |  [TorchRL: A data-driven decision-making library for PyTorch](https://openreview.net/forum?id=QxItoEAVMb) |  | 0 | PyTorch has ascended as a premier machine learning framework, yet it lacks a native and comprehensive library for decision and control tasks suitable for large development teams dealing with complex real-world data and environments. To address this issue, we propose TorchRL, a generalistic control... | Albert Bou, Gianni De Fabritiis, Matteo Bettini, Sebastian Dittert, Shagun Sodhani, Vikash Kumar, Vincent Moens, Xiaomeng Yang |  |
| 393 |  |  [Towards Robust Offline Reinforcement Learning under Diverse Data Corruption](https://openreview.net/forum?id=5hAMmCU0bK) |  | 0 | Offline reinforcement learning (RL) presents a promising approach for learning reinforced policies from offline datasets without the need for costly or unsafe interactions with the environment. However, datasets collected by humans in real-world environments are often noisy and may even be... | Amy Zhang, Chongjie Zhang, Han Zhong, Jiawei Xu, Lei Han, Rui Yang, Tong Zhang |  |
| 394 |  |  [Variational Bayesian Last Layers](https://openreview.net/forum?id=Sx7BIiPzys) |  | 0 | We introduce a deterministic variational formulation for training Bayesian last layer neural networks. This yields a sampling-free, single-pass model and loss that effectively improves uncertainty estimation. Our variational Bayesian last layer (VBLL) can be trained and evaluated with only... | James Harrison, Jasper Snoek, John Willes |  |
| 395 |  |  [EQA-MX: Embodied Question Answering using Multimodal Expression](https://openreview.net/forum?id=7gUrYE50Rb) |  | 0 | Humans predominantly use verbal utterances and nonverbal gestures (e.g., eye gaze and pointing gestures) in their natural interactions. For instance, pointing gestures and verbal information is often required to comprehend questions such as "what object is that?" Thus, this question-answering (QA)... | Alexi Gladstone, Md Mofijul Islam, Riashat Islam, Tariq Iqbal |  |
| 396 |  |  [Retrieval-based Disentangled Representation Learning with Natural Language Supervision](https://openreview.net/forum?id=ZlQRiFmq7Y) |  | 0 | Disentangled representation learning remains challenging as the underlying factors of variation in the data do not naturally exist. The inherent complexity of real-world data makes it unfeasible to exhaustively enumerate and encapsulate all its variations within a finite set of factors. However, it... | Jiawei Zhou, Lei Chen, Lifeng Shang, Qun Liu, Xiaoguang Li, Xin Jiang |  |
| 397 |  |  [On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods](https://openreview.net/forum?id=Kn7tWhuetn) |  | 0 | Neural algorithmic reasoning is an emerging research direction that endows neural networks with the ability to mimic algorithmic executions step-by-step. A common paradigm in existing designs involves the use of historical embeddings in predicting the results of future execution steps. Our... | Alexandra Saxton, Meng Liu, Montgomery Bohde, Shuiwang Ji |  |
| 398 |  |  [TRAM: Bridging Trust Regions and Sharpness Aware Minimization](https://openreview.net/forum?id=kxebDHZ7b7) |  | 0 | Sharpness-aware minimization (SAM) reports improving domain generalization by reducing the loss surface curvature in the parameter space. However, generalization during _fine-tuning_ is often more dependent on the transferability of _representations_ in the function space. Trust-region methods (TR)... | Hao Peng, Naomi Saphra, Pradeep Dasigi, Tom Sherborne |  |
| 399 |  |  [CAMIL: Context-Aware Multiple Instance Learning for Cancer Detection and Subtyping in Whole Slide Images](https://openreview.net/forum?id=rzBskAEmoc) |  | 0 | The visual examination of tissue biopsy sections is fundamental for cancer diagnosis, with pathologists analyzing sections at multiple magnifications to discern tumor cells and their subtypes. However, existing attention-based multiple instance learning (MIL) models used for analyzing Whole Slide... | Chris Bakal, Matt De Vries, Olga Fourkioti |  |
| 400 |  |  [DyST: Towards Dynamic Neural Scene Representations on Real-World Videos](https://openreview.net/forum?id=MnMWa94t12) |  | 0 | Visual understanding of the world goes beyond the semantics and flat structure of individual images. In this work, we aim to capture both the 3D structure and dynamics of real-world scenes from monocular real-world videos. Our Dynamic Scene Transformer (DyST) model leverages recent work in neural... | Klaus Greff, Maximilian Seitzer, Mehdi S. M. Sajjadi, Sjoerd van Steenkiste, Thomas Kipf |  |
| 401 |  |  [Bilevel Optimization under Unbounded Smoothness: A New Algorithm and Convergence Analysis](https://openreview.net/forum?id=LqRGsGWOTX) |  | 0 | Bilevel optimization is an important formulation for many machine learning problems, such as meta-learning and hyperparameter optimization. Current bilevel optimization algorithms assume that the gradient of the upper-level function is Lipschitz (i.e., the upper-level function has a bounded... | Jie Hao, Mingrui Liu, Xiaochuan Gong |  |
| 402 |  |  [Bounds on Representation-Induced Confounding Bias for Treatment Effect Estimation](https://openreview.net/forum?id=d3xKPQVjSc) |  | 0 | State-of-the-art methods for conditional average treatment effect (CATE) estimation make widespread use of representation learning. Here, the idea is to reduce the variance of the low-sample CATE estimation by a (potentially constrained) low-dimensional representation. However, low-dimensional... | Dennis Frauen, Stefan Feuerriegel, Valentyn Melnychuk |  |
| 403 |  |  [DSPy: Compiling Declarative Language Model Calls into State-of-the-Art Pipelines](https://openreview.net/forum?id=sY5N0zY5Od) |  | 0 | The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded “prompt templates”, i.e. lengthy strings discovered via trial and... | Arnav Singhvi, Ashutosh Sharma, Christopher Potts, Hanna Moazam, Heather Miller, Keshav Santhanam, Matei Zaharia, Omar Khattab, Paridhi Maheshwari, Saiful Haq, Sri Vardhamanan, Thomas T. Joshi, Zhiyuan Zhang |  |
| 404 |  |  [Impact of Computation in Integral Reinforcement Learning for Continuous-Time Control](https://openreview.net/forum?id=xJEd8PkdNz) |  | 0 | Integral reinforcement learning (IntRL) demands the precise computation of the utility function's integral at its policy evaluation (PEV) stage. This is achieved through quadrature rules, which are weighted sums of utility functions evaluated from state samples obtained in discrete time. Our... | Wei Pan, Wenhan Cao |  |
| 405 |  |  [Masks, Signs, And Learning Rate Rewinding](https://openreview.net/forum?id=qODvxQ8TXW) |  | 0 | Learning Rate Rewinding (LRR) has been established as a strong variant of Iterative Magnitude Pruning (IMP) to find lottery tickets in deep overparameterized neural networks. While both iterative pruning schemes couple structure and parameter learning, understanding how LRR excels in both aspects... | Advait Harshal Gadhikar, Rebekka Burkholz |  |
| 406 |  |  [Gradual Domain Adaptation via Gradient Flow](https://openreview.net/forum?id=iTTZFKrlGV) |  | 0 | Domain shift degrades classification models on new data distributions. Conventional unsupervised domain adaptation (UDA) aims to learn features that bridge labeled source and unlabeled target domains. In contrast to feature learning, gradual domain adaptation (GDA) leverages extra continuous... | Ying Wei, Yu Zhang, Zhan Zhuang |  |
| 407 |  |  [Maximum Entropy Heterogeneous-Agent Reinforcement Learning](https://openreview.net/forum?id=tmqOhBC4a5) |  | 0 | \*Multi-agent reinforcement learning\* (MARL) has been shown effective for cooperative games in recent years. However, existing state-of-the-art methods face challenges related to sample complexity, training instability, and the risk of converging to a suboptimal Nash Equilibrium. In this paper, we... | Haobo Fu, Jiarong Liu, Qiang Fu, Siyi Hu, Xiaojun Chang, Yaodong Yang, Yifan Zhong |  |
| 408 |  |  [Hybrid Directional Graph Neural Network for Molecules](https://openreview.net/forum?id=BBD6KXIGJL) |  | 0 | Equivariant message passing neural networks have emerged as the prevailing approach for predicting chemical properties of molecules due to their ability to leverage translation and rotation symmetries, resulting in a strong inductive bias. However, the equivariant operations in each layer can... | Chao Qu, Fenglei Cao, Furao Shen, Junyi An, Yinghui Xu, Yuan Qi, Zhipeng Zhou |  |
| 409 |  |  [Unbiased Watermark for Large Language Models](https://openreview.net/forum?id=uWVC5FVidc) |  | 0 | The recent advancements in large language models (LLMs) have sparked a growing apprehension regarding the potential misuse. One approach to mitigating this risk is to incorporate watermarking techniques into LLMs, allowing for the tracking and attribution of model outputs. This study examines a... | Heng Huang, Hongyang Zhang, Lichang Chen, Xidong Wu, Yihan Wu, Zhengmian Hu |  |
| 410 |  |  [Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop Control](https://openreview.net/forum?id=EriR6Ec69a) |  | 0 | Developing autonomous agents that can interact with changing environments is an open challenge in machine learning. Robustness is particularly important in these settings as agents are often fit offline on expert demonstrations but deployed online where they must generalize to the closed feedback... | Daniela Rus, Mathias Lechner, Neehal Tumma, Noel Loo, Ramin M. Hasani |  |
| 411 |  |  [CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction](https://openreview.net/forum?id=DjzvJCRsVf) |  | 0 | Open-vocabulary dense prediction tasks including object detection and image segmentation have been advanced by the success of Contrastive Language-Image Pre-training (CLIP). CLIP models, particularly those incorporating vision transformers (ViTs), have exhibited remarkable generalization ability in... | Chen Change Loy, Lumin Xu, Sheng Jin, Size Wu, Wentao Liu, Wenwei Zhang, Xiangtai Li |  |
| 412 |  |  [Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI](https://openreview.net/forum?id=QzTpTRVtrP) |  | 0 | The current electroencephalogram (EEG) based deep learning models are typically designed for specific datasets and applications in brain-computer interaction (BCI), limiting the scale of the models and thus diminishing their perceptual capabilities and generalizability. Recently, Large Language... | BaoLiang Lu, LiMing Zhao, WeiBang Jiang |  |
| 413 |  |  [Towards LLM4QPE: Unsupervised Pretraining of Quantum Property Estimation and A Benchmark](https://openreview.net/forum?id=vrBVFXwAmi) |  | 0 | Estimating the properties of quantum systems such as quantum phase has been critical in addressing the essential quantum many-body problems in physics and chemistry. Deep learning models have been recently introduced to property estimation, surpassing conventional statistical approaches. However,... | Hao Xiong, Junchi Yan, Nianzu Yang, Tailong Xiao, Yehui Tang |  |
| 414 |  |  [GTMGC: Using Graph Transformer to Predict Molecule's Ground-State Conformation](https://openreview.net/forum?id=F7QnIKlC1N) |  | 0 | The ground-state conformation of a molecule is often decisive for its properties. However, experimental or computational methods, such as density functional theory (DFT), are time-consuming and labor-intensive for obtaining this conformation. Deep learning (DL) based molecular representation... | Guikun Xu, Jim Chen, PengChuan Lei, Yan Yang, Yongquan Jiang |  |
| 415 |  |  [Generalization of Scaled Deep ResNets in the Mean-Field Regime](https://openreview.net/forum?id=tMzPZTvz2H) |  | 0 | Despite the widespread empirical success of ResNet, the generalization properties of deep ResNet are rarely explored beyond the lazy training regime. In this work, we investigate scaled ResNet in the limit of infinitely deep and wide neural networks, of which the gradient flow is described by a... | Fanghui Liu, Grigorios Chrysos, Volkan Cevher, Yihang Chen, Yiping Lu |  |
| 416 |  |  [ODE Discovery for Longitudinal Heterogeneous Treatment Effects Inference](https://openreview.net/forum?id=pxI5IPeWgW) |  | 0 | Inferring unbiased treatment effects has received widespread attention in the machine learning community. In recent years, our community has proposed numerous solutions in standard settings, high-dimensional treatment settings, and even longitudinal settings. While very diverse, the solution has... | Jeroen Berrevoets, Krzysztof Kacprzyk, Mihaela van der Schaar, Samuel Holt, Zhaozhi Qian |  |
| 417 |  |  [Learning Hierarchical World Models with Adaptive Temporal Abstractions from Discrete Latent Dynamics](https://openreview.net/forum?id=TjCDNssXKU) |  | 0 | Hierarchical world models can significantly improve model-based reinforcement learning (MBRL) and planning by enabling reasoning across multiple time scales. Nonetheless, the majority of state-of-the-art MBRL methods employ flat, non-hierarchical models. We propose Temporal Hierarchies from... | Christian Gumbsch, Georg Martius, Martin V. Butz, Noor Sajid |  |
| 418 |  |  [Prediction without Preclusion: Recourse Verification with Reachable Sets](https://openreview.net/forum?id=SCQfYpdoGE) |  | 0 | Machine learning models are often used to decide who receives a loan, a job interview, or a public benefit. Models in such settings use features without considering their \*actionability\*. As a result, they can assign predictions that are \emph{fixed} -- meaning that individuals who are denied... | Avni Kothari, Berk Ustun, Bogdan Kulynych, TsuiWei Weng |  |
| 419 |  |  [ODICE: Revealing the Mystery of Distribution Correction Estimation via Orthogonal-gradient Update](https://openreview.net/forum?id=L8UNn7Llt4) |  | 0 | In this study, we investigate the DIstribution Correction Estimation (DICE) methods, an important line of work in offline reinforcement learning (RL) and imitation learning (IL). DICE-based methods impose state-action-level behavior constraint, which is an ideal choice for offline learning.... | Haoran Xu, Liyuan Mao, Weinan Zhang, Xianyuan Zhan |  |
| 420 |  |  [Improving Non-Transferable Representation Learning by Harnessing Content and Style](https://openreview.net/forum?id=FYKVPOHCpE) |  | 0 | Non-transferable learning (NTL) aims to restrict the generalization of models toward the target domain(s). To this end, existing works learn non-transferable representations by reducing statistical dependence between the source and target domain. However, such statistical methods essentially... | Chuanwu Yang, Li Shen, Mingming Gong, Shiming Chen, Tongliang Liu, Yu Yao, Zhenyi Wang, Zhuo Huang, Ziming Hong |  |
| 421 |  |  [ModernTCN: A Modern Pure Convolution Structure for General Time Series Analysis](https://openreview.net/forum?id=vpJMJerXHU) |  | 0 | Recently, Transformer-based and MLP-based models have emerged rapidly and won dominance in time series analysis. In contrast, convolution is losing steam in time series tasks nowadays for inferior performance. This paper studies the open question of how to better use convolution in time series... | Donghao Luo, Xue Wang |  |
| 422 |  |  [Towards Robust Out-of-Distribution Generalization Bounds via Sharpness](https://openreview.net/forum?id=tPEwSYPtAC) |  | 0 | Generalizing to out-of-distribution (OOD) data or unseen domain, termed OOD generalization, still lacks appropriate theoretical guarantees. Canonical OOD bounds focus on different distance measurements between source and target domains but fail to consider the optimization property of the learned... | Jiashuo Liu, Kenji Kawaguchi, MongLi Lee, Wynne Hsu, Yingnan Liu, Yingtian Zou |  |
| 423 |  |  [MAPE-PPI: Towards Effective and Efficient Protein-Protein Interaction Prediction via Microenvironment-Aware Protein Embedding](https://openreview.net/forum?id=itGkF993gz) |  | 0 | Protein-Protein Interactions (PPIs) are fundamental in various biological processes and play a key role in life activities. The growing demand and cost of experimental PPI assays require computational methods for efficient PPI prediction. While existing methods rely heavily on protein sequence for... | Haitao Lin, Lirong Wu, Nitesh V. Chawla, Siyuan Li, Stan Z. Li, Yijun Tian, Yufei Huang |  |
| 424 |  |  [Negative Label Guided OOD Detection with Pretrained Vision-Language Models](https://openreview.net/forum?id=xUO1HXz4an) |  | 0 | Out-of-distribution (OOD) detection aims at identifying samples from unknown classes, playing a crucial role in trustworthy models against errors on unexpected inputs. Extensive research has been dedicated to exploring OOD detection in the vision modality. {Vision-language models (VLMs) can... | Bo Han, Feng Liu, Feng Zheng, Hong Chen, Tongliang Liu, Xue Jiang, Zhen Fang |  |
| 425 |  |  [Optimal robust Memorization with ReLU Neural Networks](https://openreview.net/forum?id=47hDbAMLbc) |  | 0 | Memorization with neural networks is to study the expressive power of neural networks to interpolate a finite classification data set, which is closely related to the generalizability of deep learning. However, the important problem of robust memorization has not been thoroughly studied. In this... | Lijia Yu, Lijun Zhang, XiaoShan Gao |  |
| 426 |  |  [Neural Contractive Dynamical Systems](https://openreview.net/forum?id=iAYIRHOYy8) |  | 0 | Stability guarantees are crucial when ensuring that a fully autonomous robot does not take undesirable or potentially harmful actions. Unfortunately, global stability guarantees are hard to provide in dynamical systems learned from data, especially when the learned dynamics are governed by neural... | Georgios Arvanitidis, Gerhard Neumann, Hadi BeikMohammadi, Leonel Rozo, Nadia Figueroa, Søren Hauberg |  |
| 427 |  |  [Scaling Laws for Associative Memories](https://openreview.net/forum?id=Tzh6xAJSll) |  | 0 | Learning arguably involves the discovery and memorization of abstract rules. The aim of this paper is to study associative memory mechanisms. Our model is based on high-dimensional matrices consisting of outer products of embeddings, which relates to the inner layers of transformer language models.... | Alberto Bietti, Elvis Dohmatob, Vivien Cabannes |  |
| 428 |  |  [Text2Reward: Reward Shaping with Language Models for Reinforcement Learning](https://openreview.net/forum?id=tUM39YTRxH) |  | 0 | Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation and shaping of dense reward... | Chen Henry Wu, Qian Luo, Siheng Zhao, Tao Yu, Tianbao Xie, Victor Zhong, Yanchao Yang, Yitao Liu |  |
| 429 |  |  [Towards Meta-Pruning via Optimal Transport](https://openreview.net/forum?id=sMoifbuxjB) |  | 0 | Structural pruning of neural networks conventionally relies on identifying and discarding less important neurons, a practice often resulting in significant accuracy loss that necessitates subsequent fine-tuning efforts. This paper introduces a novel approach named Intra-Fusion, challenging this... | Alexander Theus, Friedrich Wicke, Olin Geimer, Sidak Pal Singh, Sotiris Anagnostidis, Thomas Hofmann |  |
| 430 |  |  [InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation](https://openreview.net/forum?id=MLBdiWu4Fw) |  | 0 | This paper introduces InternVid, a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation. InternVid contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips... | Guo Chen, Jiashuo Yu, Kunchang Li, Limin Wang, Ping Luo, Xin Ma, Xinhao Li, Xinyuan Chen, Yali Wang, Yaohui Wang, Yi Wang, Yinan He, Yizhuo Li, Yu Qiao, Ziwei Liu |  |
| 431 |  |  [Dictionary Contrastive Learning for Efficient Local Supervision without Auxiliary Networks](https://openreview.net/forum?id=Gg7cXo3S8l) |  | 0 | While backpropagation (BP) has achieved widespread success in deep learning, it faces two prominent challenges: computational inefficiency and biological implausibility. In response to these challenges, local supervision, encompassing Local Learning (LL) and Forward Learning (FL), has emerged as a... | Jeonglyul Oh, Joonseok Lee, Myeongho Jeon, Myungjoo Kang, Suhwan Choi, Sungjun Lim, Yeonjung Hwang |  |
| 432 |  |  [Bounding Box Stability against Feature Dropout Reflects Detector Generalization across Environments](https://openreview.net/forum?id=lmM4Ecm4HJ) |  | 0 | Bounding boxes uniquely characterize object detection, where a good detector gives accurate bounding boxes of categories of interest. However, in the real-world where test ground truths are not provided, it is non-trivial to find out whether bounding boxes are accurate, thus preventing us from... | Jifeng Dai, Liang Zheng, Wenhai Wang, Yang Yang, Zhe Chen |  |
| 433 |  |  [Deep Geodesic Canonical Correlation Analysis for Covariance-Based Neuroimaging Data](https://openreview.net/forum?id=PnR1MNen7u) |  | 0 | In human neuroimaging, multi-modal imaging techniques are frequently combined to enhance our comprehension of whole-brain dynamics and improve diagnosis in clinical practice. Modalities like electroencephalography and functional magnetic resonance imaging provide distinct views to the brain... | Ce Ju, Cuntai Guan, Liyao Tang, Motoaki Kawanabe, Reinmar J. Kobler |  |
| 434 |  |  [SWAP-NAS: Sample-Wise Activation Patterns for Ultra-fast NAS](https://openreview.net/forum?id=tveiUXU2aa) |  | 0 | Training-free metrics (a.k.a. zero-cost proxies) are widely used to avoid resource-intensive neural network training, especially in Neural Architecture Search (NAS). Recent studies show that existing training-free metrics have several limitations, such as limited correlation and poor generalisation... | Andy Song, Haytham M. Fayek, Vic Ciesielski, Xiaojun Chang, Yameng Peng |  |
| 435 |  |  [RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches](https://openreview.net/forum?id=F1TKzG8LJO) |  | 0 | Generalization remains one of the most important desiderata for robust robot learning systems. While recently proposed approaches show promise in generalization to novel objects, semantic concepts, or visual distribution shifts, generalization to new tasks remains challenging. For example, a... | Chelsea Finn, Chuyuan Fu, Hao Su, Jiayuan Gu, Kanishka Rao, Karol Hausman, Keerthana Gopalakrishnan, Montserrat Gonzalez Arenas, Paul Wohlhart, Peng Xu, Priya Sundaresan, Quan Vuong, Sean Kirmani, Ted Xiao, Wenhao Yu, Yao Lu, Zhuo Xu |  |
| 436 |  |  [NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers](https://openreview.net/forum?id=Rc7dAwVL3v) |  | 0 | Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild datasets is important to capture the diversity in human speech such as speaker identities, prosodies, and styles (e.g., singing). Current large TTS systems usually quantize speech into discrete tokens and use language... | Eric Liu, Jiang Bian, Kai Shen, Lei He, Sheng Zhao, Tao Qin, Xu Tan, Yichong Leng, Zeqian Ju |  |
| 437 |  |  [Submodular Reinforcement Learning](https://openreview.net/forum?id=loYSzjSaAK) |  | 0 | In reinforcement learning (RL), rewards of states are typically considered additive, and following the Markov assumption, they are independent of states visited previously. In many important applications, such as coverage control, experiment design and informative path planning, rewards naturally... | Andreas Krause, Manish Prajapat, Melanie N. Zeilinger, Mojmir Mutny |  |
| 438 |  |  [Making Pre-trained Language Models Great on Tabular Prediction](https://openreview.net/forum?id=anzIzGZuLi) |  | 0 | The transferability of deep neural networks (DNNs) has made significant progress in image and language processing. However, due to the heterogeneity among tables, such DNN bonus is still far from being well exploited on tabular data prediction (e.g., regression or classification tasks). Condensing... | Bo Zheng, Danny Z. Chen, Hongxia Xu, Jiahuan Yan, Jian Wu, Jimeng Sun, Jintai Chen, Yiheng Zhu |  |
| 439 |  |  [Solving Inverse Problems with Latent Diffusion Models via Hard Data Consistency](https://openreview.net/forum?id=j8hdRqOUhN) |  | 0 | Latent diffusion models have been demonstrated to generate high-quality images, while offering efficiency in model training compared to diffusion models operating in the pixel space. However, incorporating latent diffusion models to solve inverse problems remains a challenging problem due to the... | Bowen Song, Liyue Shen, Qing Qu, Soo Min Kwon, Xinyu Hu, Zecheng Zhang |  |
| 440 |  |  [The False Promise of Imitating Proprietary Language Models](https://openreview.net/forum?id=Kz3yckpCN5) |  | 0 | An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct, and others). In this work, we critically analyze this approach of imitating language models. We first finetune a... | Arnav Gudibande, Charlie Snell, Dawn Song, Eric Wallace, Hao Liu, Pieter Abbeel, Sergey Levine, Xinyang Geng |  |
| 441 |  |  [Sample-Efficient Linear Representation Learning from Non-IID Non-Isotropic Data](https://openreview.net/forum?id=Tr3fZocrI6) |  | 0 | A powerful concept behind much of the recent progress in machine learning is the extraction of common features across data from heterogeneous sources or tasks. Intuitively, using all of one's data to learn a common representation function benefits both computational effort and statistical... | James Anderson, Leonardo Felipe Toso, Nikolai Matni, Thomas T. C. K. Zhang |  |
| 442 |  |  [Information Retention via Learning Supplemental Features](https://openreview.net/forum?id=o83eu4H9Mb) |  | 0 | The information bottleneck principle provides an information-theoretic method for learning a good representation as a trade-off between conciseness and predictive ability, which can reduce information redundancy, eliminate irrelevant and superfluous features, and thus enhance the in-domain... | Yahe Li, Zhipeng Xie |  |
| 443 |  |  [Mayfly: a Neural Data Structure for Graph Stream Summarization](https://openreview.net/forum?id=n7Sr8SW4bn) |  | 0 | A graph is a structure made up of vertices and edges used to represent complex relationships between entities, while a graph stream is a continuous flow of graph updates that convey evolving relationships between entities. The massive volume and high dynamism of graph streams promote research on... | Hairu Wang, S. Kevin Zhou, Xike Xie, Yuan Feng, Yukun Cao |  |
| 444 |  |  [Exploring the Common Appearance-Boundary Adaptation for Nighttime Optical Flow](https://openreview.net/forum?id=776lhoaulC) |  | 0 | We investigate a challenging task of nighttime optical flow, which suffers from weakened texture and amplified noise. These degradations weaken discriminative visual features, thus causing invalid motion feature matching. Typically, existing methods employ domain adaptation to transfer knowledge... | Hanyu Zhou, Haoyue Liu, Luxin Yan, Wending Yan, Yi Chang, Yuxing Duan, Zhiwei Shi |  |
| 445 |  |  [Graphical Multioutput Gaussian Process with Attention](https://openreview.net/forum?id=6N8TW504aa) |  | 0 | Integrating information while recognizing dependence from multiple data sources and enhancing the predictive performance of the multi-output regression are challenging tasks. Multioutput Gaussian Process (MOGP) methods offer outstanding solutions with tractable predictions and uncertainty... | Feng Yin, Wenzhong Yan, Yijue Dai |  |
| 446 |  |  [Soft Contrastive Learning for Time Series](https://openreview.net/forum?id=pAsQSWlDUf) |  | 0 | Contrastive learning has shown to be effective to learn representations from time series in a self-supervised way. However, contrasting similar time series instances or values from adjacent timestamps within a time series leads to ignore their inherent correlations, which results in deteriorating... | Kibok Lee, Seunghan Lee, Taeyoung Park |  |
| 447 |  |  [Enhancing Group Fairness in Online Settings Using Oblique Decision Forests](https://openreview.net/forum?id=E1NxN5QMOE) |  | 0 | Fairness, especially group fairness, is an important consideration in the context of machine learning systems. The most commonly adopted group fairness-enhancing techniques are in-processing methods that rely on a mixture of a fairness objective (e.g., demographic parity) and a task-specific... | Ahmad Beirami, Amr Ahmed, Kumar Avinava Dubey, Nicholas Monath, Rahul Kidambi, Snigdha Chaturvedi, Somnath Basu Roy Chowdhury |  |
| 448 |  |  [Generative Learning for Financial Time Series with Irregular and Scale-Invariant Patterns](https://openreview.net/forum?id=CdjnzWsQax) |  | 0 | Limited data availability poses a major obstacle in training deep learning models for financial applications. Synthesizing financial time series to augment real-world data is challenging due to the irregular and scale-invariant patterns uniquely associated with financial time series - temporal... | Hongbin Huang, Minghua Chen, Xiao Qiao |  |
| 449 |  |  [Multiscale Positive-Unlabeled Detection of AI-Generated Texts](https://openreview.net/forum?id=5Lp6qU9hzV) |  | 0 | Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are astonishing at generating human-like texts, but they may impact the authenticity of texts. Previous works proposed methods to detect these AI-generated texts, including simple ML classifiers, pretrained-model-based zero-shot... | Chao Xu, Hanting Chen, Qinghua Zhang, Ruifeng Li, Xutao Wang, Yuchuan Tian, Yunhe Wang, Zheyuan Bai |  |
| 450 |  |  [A Lightweight Method for Tackling Unknown Participation Statistics in Federated Averaging](https://openreview.net/forum?id=ZKEuFKfCKA) |  | 0 | In federated learning (FL), clients usually have diverse participation statistics that are unknown a priori, which can significantly harm the performance of FL if not handled properly. Existing works aiming at addressing this problem are usually based on global variance reduction, which requires a... | Mingyue Ji, Shiqiang Wang |  |
| 451 |  |  [Identifying the Risks of LM Agents with an LM-Emulated Sandbox](https://openreview.net/forum?id=GEcwtMk1uA) |  | 0 | Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks—such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating... | Andrew Wang, Chris J. Maddison, Honghua Dong, Jimmy Ba, Silviu Pitis, Tatsunori Hashimoto, Yangjun Ruan, Yann Dubois, Yongchao Zhou |  |
| 452 |  |  [Coeditor: Leveraging Repo-level Diffs for Code Auto-editing](https://openreview.net/forum?id=ALVwQjZRS8) |  | 0 | Developers often dedicate significant time to maintaining and refactoring existing code. However, most prior work on generative models for code focuses solely on creating new code, overlooking the distinctive needs of editing existing code. In this work, we explore a multi-round code auto-editing... | Greg Durrett, Isil Dillig, Jiayi Wei |  |
| 453 |  |  [FITS: Modeling Time Series with 10k Parameters](https://openreview.net/forum?id=bWcnvZ3qMb) |  | 0 | In this paper, we introduce FITS, a lightweight yet powerful model for time series analysis. Unlike existing models that directly process raw time-domain data, FITS operates on the principle that time series can be manipulated through interpolation in the complex frequency domain, achieving... | Ailing Zeng, Qiang Xu, Zhijian Xu |  |
| 454 |  |  [MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models](https://openreview.net/forum?id=N8N0hgNDRt) |  | 0 | Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (\eg, LLaMA-2) are still far away from satisfactory for solving mathematical problems due to the complex... | Adrian Weller, Han Shi, James T. Kwok, Jincheng Yu, Longhui Yu, Weisen Jiang, Weiyang Liu, Yu Zhang, Zhenguo Li, Zhengying Liu |  |
| 455 |  |  [Query-Policy Misalignment in Preference-Based Reinforcement Learning](https://openreview.net/forum?id=UoBymIwPJR) |  | 0 | Preference-based reinforcement learning (PbRL) provides a natural way to align RL agents’ behavior with human desired outcomes, but is often restrained by costly human feedback. To improve feedback efficiency, most existing PbRL methods focus on selecting queries to maximally improve the overall... | Jianxiong Li, QingShan Jia, Xianyuan Zhan, Xiao Hu, YaQin Zhang |  |
| 456 |  |  [Feature-aligned N-BEATS with Sinkhorn divergence](https://openreview.net/forum?id=TS8HoIWAPQ) |  | 0 | We propose Feature-aligned N-BEATS as a domain-generalized time series forecasting model. It is a nontrivial extension of N-BEATS with doubly residual stacking principle (Oreshkin et al. [45]) into a representation learning framework. In particular, it revolves around marginal feature probability... | Joonhun Lee, Kyunghyun Park, Myeongho Jeon, Myungjoo Kang |  |
| 457 |  |  [Instructive Decoding: Instruction-Tuned Large Language Models are Self-Refiner from Noisy Instructions](https://openreview.net/forum?id=LebzzClHYw) |  | 0 | While instruction-tuned language models have demonstrated impressive zero-shot generalization, these models often struggle to generate accurate responses when faced with instructions that fall outside their training set. This paper presents Instructive Decoding (ID), a simple yet effective approach... | Gihun Lee, Joonkee Kim, SeYoung Yun, Taehyeon Kim |  |
| 458 |  |  [Consistent Multi-Class Classification from Multiple Unlabeled Datasets](https://openreview.net/forum?id=fW7DOHDQvF) |  | 0 | Weakly supervised learning aims to construct effective predictive models from imperfectly labeled data. The recent trend of weakly supervised learning has focused on how to learn an accurate classifier from completely unlabeled data, given little supervised information such as class priors. In this... | Bo An, Hongxin Wei, Lei Feng, Senlin Shu, Yuzhou Cao, Zixi Wei |  |
| 459 |  |  [SpikePoint: An Efficient Point-based Spiking Neural Network for Event Cameras Action Recognition](https://openreview.net/forum?id=7etoNfU9uF) |  | 0 | Event cameras are bio-inspired sensors that respond to local changes in light intensity and feature low latency, high energy efficiency, and high dynamic range. Meanwhile, Spiking Neural Networks (SNNs) have gained significant attention due to their remarkable efficiency and fault tolerance. By... | Bojun Cheng, Haotian Fu, Hongwei Ren, Jie Song, Xiaopeng Lin, Yue Zhou, Yulong Huang |  |
| 460 |  |  [Inverse Approximation Theory for Nonlinear Recurrent Neural Networks](https://openreview.net/forum?id=yC2waD70Vj) |  | 0 | We prove an inverse approximation theorem for the approximation of nonlinear sequence-to-sequence relationships using recurrent neural networks (RNNs). This is a so-called Bernstein-type result in approximation theory, which deduces properties of a target function under the assumption that it can... | Qianxiao Li, Shida Wang, Zhong Li |  |
| 461 |  |  [Kernel Metric Learning for In-Sample Off-Policy Evaluation of Deterministic RL Policies](https://openreview.net/forum?id=plebgsdiiV) |  | 0 | We consider off-policy evaluation (OPE) of deterministic target policies for reinforcement learning (RL) in environments with continuous action spaces. While it is common to use importance sampling for OPE, it suffers from high variance when the behavior policy deviates significantly from the... | Haanvid Lee, Jongmin Lee, KeeEung Kim, Tri Wahyu Guntara, YungKyun Noh |  |
| 462 |  |  [Large Language Models are Efficient Learners of Noise-Robust Speech Recognition](https://openreview.net/forum?id=ceATjGPTUD) |  | 0 | Recent advances in large language models (LLMs) have promoted generative error correction (GER) for automatic speech recognition (ASR), which leverages the rich linguistic knowledge and powerful reasoning ability of LLMs to improve recognition results. The latest work proposes a GER benchmark with... | Chao Zhang, ChaoHan Huck Yang, Chen Chen, Engsiong Chng, PinYu Chen, Ruizhe Li, Yuchen Hu |  |
| 463 |  |  [H2O-SDF: Two-phase Learning for 3D Indoor Reconstruction using Object Surface Fields](https://openreview.net/forum?id=P1ANzoGg3W) |  | 0 | Advanced techniques using Neural Radiance Fields (NeRF), Signed Distance Fields (SDF), and Occupancy Fields have recently emerged as solutions for 3D indoor scene reconstruction. We introduce a novel two-phase learning approach, H2O-SDF, that discriminates between object and non-object regions... | Chul Lee, Jaeseok Yoo, Jongkwang Hong, Joongrock Kim, Minyoung Park, Mirae Do, YeonJae Shin |  |
| 464 |  |  [Sample-Efficient Quality-Diversity by Cooperative Coevolution](https://openreview.net/forum?id=JDud6zbpFv) |  | 0 | Quality-Diversity (QD) algorithms, as a subset of evolutionary algorithms, have emerged as a powerful optimization paradigm with the aim of generating a set of high-quality and diverse solutions. Although QD has demonstrated competitive performance in reinforcement learning, its low sample... | Chao Qian, Dong Li, Jianye Hao, Ke Xue, Pengyi Li, RenJian Wang |  |
| 465 |  |  [SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore](https://openreview.net/forum?id=ruk0nyQPec) |  | 0 | The legality of training language models (LMs) on copyrighted or otherwise restricted data is under intense debate. However, as we show, model performance significantly degrades if trained only on low-risk text (e.g., out-of-copyright books or government documents), due to its limited size and... | Eric Wallace, Hannaneh Hajishirzi, Luke Zettlemoyer, Noah A. Smith, Sewon Min, Suchin Gururangan, Weijia Shi |  |
| 466 |  |  [Dynamic Discounted Counterfactual Regret Minimization](https://openreview.net/forum?id=6PbvbLyqT6) |  | 0 | Counterfactual regret minimization (CFR) is a family of iterative algorithms showing promising results in solving imperfect-information games. Recent novel CFR variants (e.g., CFR+, DCFR) have significantly improved the convergence rate of the vanilla CFR. The key to these CFR variants’ performance... | Hang Xu, Haobo Fu, Jian Cheng, Junliang Xing, Kai Li, Qiang Fu |  |
| 467 |  |  [GIO: Gradient Information Optimization for Training Dataset Selection](https://openreview.net/forum?id=3NnfJnbJT2) |  | 0 | It is often advantageous to train models on a subset of the available train examples, because the examples are of variable quality or because one would like to train with fewer examples, without sacrificing performance. We present Gradient Information Optimization (GIO), a scalable, task-agnostic... | Christopher Potts, Dante Everaert |  |
| 468 |  |  [SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training](https://openreview.net/forum?id=KZSEgJGPxu) |  | 0 | In an era where symbolic mathematical equations are indispensable for modeling complex natural phenomena, scientific inquiry often involves collecting observations and translating them into mathematical expressions. Recently, deep learning has emerged as a powerful tool for extracting insights from... | Amir Barati Farimani, Chandan K. Reddy, Kazem Meidani, Parshin Shojaee |  |
| 469 |  |  [Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model](https://openreview.net/forum?id=m50eKHCttz) |  | 0 | Training deep networks requires various design decisions regarding for instance their architecture, data augmentation, or optimization. In this work, we find these training variations to result in networks learning unique feature sets from the data. Using public model libraries comprising thousands... | A. Sophia Koepke, Karsten Roth, Lukas Thede, Olivier J. Hénaff, Oriol Vinyals, Zeynep Akata |  |
| 470 |  |  [Robustifying State-space Models for Long Sequences via Approximate Diagonalization](https://openreview.net/forum?id=DjeQ39QoLQ) |  | 0 | State-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4... | Annan Yu, Arnur Nigmetov, Dmitriy Morozov, Michael W. Mahoney, N. Benjamin Erichson |  |
| 471 |  |  [Provable Offline Preference-Based Reinforcement Learning](https://openreview.net/forum?id=tVMPfEGT2w) |  | 0 | In this paper, we investigate the problem of offline Preference-based Reinforcement Learning (PbRL) with human feedback where feedback is available in the form of preference between trajectory pairs rather than explicit rewards. Our proposed algorithm consists of two main steps: (1) estimate the... | Jason D. Lee, Masatoshi Uehara, Nathan Kallus, Wen Sun, Wenhao Zhan |  |
| 472 |  |  [Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory](https://openreview.net/forum?id=gmg7t8b4s0) |  | 0 | Existing efforts on quantifying privacy implications for large language models (LLMs) solely focus on measuring leakage of training data. In this work, we shed light on the often-overlooked interactive settings where an LLM receives information from multiple sources and generates an output to be... | Hyunwoo Kim, Maarten Sap, Niloofar Mireshghallah, Reza Shokri, Xuhui Zhou, Yejin Choi, Yulia Tsvetkov |  |
| 473 |  |  [Provable Reward-Agnostic Preference-Based Reinforcement Learning](https://openreview.net/forum?id=yTBXeXdbMf) |  | 0 | Preference-based Reinforcement Learning (PbRL) is a paradigm in which an RL agent learns to optimize a task using pair-wise preference-based feedback over trajectories, rather than explicit reward signals. While PbRL has demonstrated practical success in fine-tuning language models, existing... | Jason D. Lee, Masatoshi Uehara, Wen Sun, Wenhao Zhan |  |
| 474 |  |  [Unleashing the Potential of Fractional Calculus in Graph Neural Networks with FROND](https://openreview.net/forum?id=wcka3bd7P4) |  | 0 | We introduce the FRactional-Order graph Neural Dynamical network (FROND), a new continuous graph neural network (GNN) framework. Unlike traditional continuous GNNs that rely on integer-order differential equations, FROND employs the Caputo fractional derivative to leverage the non-local properties... | Feng Ji, Kai Zhao, Qinxu Ding, Qiyu Kang, Wee Peng Tay, Wenfei Liang, Xuhao Li, Yang Song |  |
| 475 |  |  [MetaPhysiCa: Improving OOD Robustness in Physics-informed Machine Learning](https://openreview.net/forum?id=KrWuDiW4Qm) |  | 0 | A fundamental challenge in physics-informed machine learning (PIML) is the design of robust PIML methods for out-of-distribution (OOD) forecasting tasks. These OOD tasks require learning-to-learn from observations of the same (ODE) dynamical system with different unknown ODE parameters, and demand... | Bruno Ribeiro, Muhammad Ashraful Alam, S. Chandra Mouli |  |
| 476 |  |  [Views Can Be Deceiving: Improved SSL Through Feature Space Augmentation](https://openreview.net/forum?id=mutJBk3ILg) |  | 0 | Supervised learning methods have been found to exhibit inductive biases favoring simpler features. When such features are spuriously correlated with the label, this can result in suboptimal performance on minority subgroups. Despite the growing popularity of methods which learn from unlabeled data,... | Haoran Zhang, Kimia Hamidieh, Marzyeh Ghassemi, Swami Sankaranarayanan |  |
| 477 |  |  [Project and Probe: Sample-Efficient Adaptation by Interpolating Orthogonal Features](https://openreview.net/forum?id=f6CBQYxXvr) |  | 0 | Transfer learning with a small amount of target data is an effective and common approach to adapting a pre-trained model to distribution shifts. In some situations, target data labels may be expensive to obtain, so we may only have access to a limited number of target data points. To make the most... | Amrith Setlur, Annie S. Chen, Chelsea Finn, Sergey Levine, Yoonho Lee |  |
| 478 |  |  [Implicit bias of SGD in L2-regularized linear DNNs: One-way jumps from high to low rank](https://openreview.net/forum?id=P1aobHnjjj) |  | 0 | The $L_{2}$-regularized loss of Deep Linear Networks (DLNs) with more than one hidden layers has multiple local minima, corresponding to matrices with different ranks. In tasks such as matrix completion, the goal is to converge to the local minimum with the smallest rank that still fits the... | Arthur Jacot, Zihan Wang |  |
| 479 |  |  [Whole-Song Hierarchical Generation of Symbolic Music Using Cascaded Diffusion Models](https://openreview.net/forum?id=sn7CYWyavh) |  | 0 | Recent deep music generation studies have put much emphasis on long-term generation with structures. However, we are yet to see high-quality, well-structured \*\*whole-song\*\* generation. In this paper, we make the first attempt to model a full music piece under the realization of \*compositional... | Gus Xia, Lejun Min, Ziyu Wang |  |
| 480 |  |  [Evaluating the Zero-shot Robustness of Instruction-tuned Language Models](https://openreview.net/forum?id=g9diuvxN6D) |  | 0 | Instruction fine-tuning has recently emerged as a promising approach for improving the zero-shot capabilities of Large Language Models (LLMs) on new tasks. This technique has shown particular strength in improving the performance of modestly sized LLMs, sometimes inducing performance competitive... | Byron C. Wallace, Chantal Shaib, Jiuding Sun |  |
| 481 |  |  [Critical Learning Periods Emerge Even in Deep Linear Networks](https://openreview.net/forum?id=Aq35gl2c1k) |  | 0 | Critical learning periods are periods early in development where temporary sensory deficits can have a permanent effect on behavior and learned representations. Despite the radical differences between biological and artificial networks, critical learning periods have been empirically observed in... | Alessandro Achille, Michael Kleinman, Stefano Soatto |  |
| 482 |  |  [MOTOR: A Time-to-Event Foundation Model For Structured Medical Records](https://openreview.net/forum?id=NialiwI2V6) |  | 0 | We present a self-supervised, time-to-event (TTE) foundation model called MOTOR (Many Outcome Time Oriented Representations) which is pretrained on timestamped sequences of events in electronic health records (EHR) and health insurance claims. TTE models are used for estimating the probability... | Ethan Steinberg, Jason Alan Fries, Nigam Shah, Yizhe Xu |  |
| 483 |  |  [GenSim: Generating Robotic Simulation Tasks via Large Language Models](https://openreview.net/forum?id=OI3RoHoWAN) |  | 0 | Collecting large amounts of real-world interaction data to train general robotic policies is often prohibitively expensive, thus motivating the use of simulation data. However, existing methods for data generation have generally focused on scene-level diversity (e.g., object instances and poses)... | Bailin Wang, Chen Bao, Huazhe Xu, Lirui Wang, Mohit Shridhar, Xiaolong Wang, Yiyang Ling, Yuzhe Qin, Zhecheng Yuan |  |
| 484 |  |  [Understanding Augmentation-based Self-Supervised Representation Learning via RKHS Approximation and Regression](https://openreview.net/forum?id=Ax2yRhCQr1) |  | 0 | Data augmentation is critical to the empirical success of modern self-supervised representation learning, such as contrastive learning and masked language modeling. However, a theoretical understanding of the exact role of the augmentation remains limited. Recent work has built the connection... | Andrej Risteski, Bingbin Liu, J. Zico Kolter, Pradeep Kumar Ravikumar, Runtian Zhai |  |
| 485 |  |  [Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs](https://openreview.net/forum?id=MO5PiKHELW) |  | 0 | Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. We present a case study of syntax acquisition in masked... | Angelica Chen, Kyunghyun Cho, Matthew L. Leavitt, Naomi Saphra, Ravid ShwartzZiv |  |
| 486 |  |  [SE(3)-Stochastic Flow Matching for Protein Backbone Generation](https://openreview.net/forum?id=kJFIH23hXb) |  | 0 | The computational design of novel protein structures has the potential to impact numerous scientific disciplines greatly. Toward this goal, we introduce \foldflow, a series of novel generative models of increasing modeling power based on the flow-matching paradigm over $3\mathrm{D}$ rigid... | Alexander Tong, Andrei Cristian Nica, Avishek Joey Bose, ChengHao Liu, Guillaume Huguet, Jarrid RectorBrooks, Kilian Fatras, Maksym Korablyov, Michael M. Bronstein, Tara AkhoundSadegh |  |
| 487 |  |  [DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer](https://openreview.net/forum?id=Ifz3IgsEPX) |  | 0 | Large Language Models (LLMs) have emerged as dominant tools for various tasks, particularly when tailored for a specific target by prompt tuning. Nevertheless, concerns surrounding data privacy present obstacles due to the tuned prompts' dependency on sensitive private information. A practical... | Bo Li, Chenhui Zhang, Jiachen T. Wang, Junyuan Hong, Zhangheng Li, Zhangyang Wang |  |
| 488 |  |  [Geographic Location Encoding with Spherical Harmonics and Sinusoidal Representation Networks](https://openreview.net/forum?id=PudduufFLa) |  | 0 | Learning representations of geographical space is vital for any machine learning model that integrates geolocated data, spanning application domains such as remote sensing, ecology, or epidemiology. Recent work embeds coordinates using sine and cosine projections based on Double Fourier Sphere... | Devis Tuia, Esther Rolf, Konstantin Klemmer, Marc Rußwurm, Robin Zbinden |  |
| 489 |  |  [A General Framework for User-Guided Bayesian Optimization](https://openreview.net/forum?id=NjU0jtXcYn) |  | 0 | The optimization of expensive-to-evaluate black-box functions is prevalent in various scientific disciplines. Bayesian optimization is an automatic, general and sample-efficient method to solve these problems with minimal knowledge of the the underlying function dynamics. However, the ability of... | Carl Hvarfner, Frank Hutter, Luigi Nardi |  |
| 490 |  |  [Lemur: Harmonizing Natural Language and Code for Language Agents](https://openreview.net/forum?id=hNhwSmtXRh) |  | 0 | We introduce Lemur and Lemur-Chat, openly accessible language models optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents. The evolution from language chat models to functional language agents demands that models not only master human... | Bailin Wang, Binyuan Hui, Boyu Mi, Caiming Xiong, Chen Xing, Fan Zhou, Hongjin Su, Lingpeng Kong, Qian Liu, Siheng Zhao, Tao Yu, Tianbao Xie, Weijia Shi, Yiheng Xu, Yitao Liu, Zhoujun Cheng |  |
| 491 |  |  [A path-norm toolkit for modern networks: consequences, promises and challenges](https://openreview.net/forum?id=hiHZVUIYik) |  | 0 | This work introduces the first toolkit around path-norms that fully encompasses general DAG ReLU networks with biases, skip connections and any operation based on the extraction of order statistics: max pooling, GroupSort etc. This toolkit notably allows us to establish generalization bounds for... | Antoine Gonon, Elisa Riccietti, Nicolas Brisebarre, Rémi Gribonval |  |
| 492 |  |  [Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages](https://openreview.net/forum?id=Kuh5qgCGCp) |  | 0 | Recently there has been a significant surge in multimodal learning in terms of both image-to-text and text-to-image generation. However, the success is typically limited to English, leaving other languages largely behind. Building a competitive counterpart in other languages is highly challenging... | Chongyi Wang, Dahai Li, Hanghao Wu, Haoye Zhang, Jiao Xue, Jinyi Hu, Maosong Sun, Qianyu Chen, Shan Wang, Tianyu Yu, Xu Han, Yankai Lin, Yinxu Pan, Yuan Yao, Yue Zhao, Zhiyuan Liu |  |
| 493 |  |  [From Sparse to Soft Mixtures of Experts](https://openreview.net/forum?id=jxpsAj7ltE) |  | 0 | Sparse mixture of expert architectures (MoEs) scale model capacity without significant increases in training or inference costs. Despite their success, MoEs suffer from a number of issues: training instability, token dropping, inability to scale the number of experts, or ineffective finetuning. In... | Basil Mustafa, Carlos Riquelme Ruiz, Joan Puigcerver, Neil Houlsby |  |
| 494 |  |  [Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives](https://openreview.net/forum?id=rxVBKhyfSo) |  | 0 | The rise in internet usage has led to the generation of massive amounts of data, resulting in the adoption of various supervised and semi-supervised machine learning algorithms, which can effectively utilize the colossal amount of data to train models. However, before deploying these models in the... | Harsh Rangwani, Kunal Samanta, Sho Takemori, Shrinivas Ramasubramanian, Venkatesh Babu Radhakrishnan, Yuhei Umeda |  |
| 495 |  |  [NoiseDiffusion: Correcting Noise for Image Interpolation with Diffusion Models beyond Spherical Linear Interpolation](https://openreview.net/forum?id=6O3Q6AFUTu) |  | 0 | Image interpolation based on diffusion models is promising in creating fresh and interesting images. Advanced interpolation methods mainly focus on spherical linear interpolation, where images are encoded into the noise space and then interpolated for denoising to images. However, existing methods... | Bo Han, Defu Lian, Pengfei Zheng, Tongliang Liu, Yonggang Zhang, Zhen Fang |  |
| 496 |  |  [SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis](https://openreview.net/forum?id=di52zR8xgf) |  | 0 | We present Stable Diffusion XL (SDXL), a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone, achieved by significantly increasing the number of attention blocks and including a second text encoder.... | Andreas Blattmann, Dustin Podell, Joe Penna, Jonas Müller, Kyle Lacey, Robin Rombach, Tim Dockhorn, Zion English |  |
| 497 |  |  [Entity-Centric Reinforcement Learning for Object Manipulation from Pixels](https://openreview.net/forum?id=uDxeSZ1wdI) |  | 0 | Manipulating objects is a hallmark of human intelligence, and an important task in domains such as robotics. In principle, Reinforcement Learning (RL) offers a general approach to learn object manipulation. In practice, however, domains with more than a few objects are difficult for RL agents due... | Aviv Tamar, Dan Haramati, Tal Daniel |  |
| 498 |  |  [Constrained Bi-Level Optimization: Proximal Lagrangian Value Function Approach and Hessian-free Algorithm](https://openreview.net/forum?id=xJ5N8qrEPl) |  | 0 | This paper presents a new approach and algorithm for solving a class of constrained Bi-Level Optimization (BLO) problems in which the lower-level problem involves constraints coupling both upper-level and lower-level variables. Such problems have recently gained significant attention due to their... | Chengming Yu, Jin Zhang, Shangzhi Zeng, Wei Yao |  |
| 499 |  |  [Inherently Interpretable Time Series Classification via Multiple Instance Learning](https://openreview.net/forum?id=xriGRsoAza) |  | 0 | Conventional Time Series Classification (TSC) methods are often black boxes that obscure inherent interpretation of their decision-making processes. In this work, we leverage Multiple Instance Learning (MIL) to overcome this issue, and propose a new framework called MILLET: Multiple Instance... | Gavin K. C. Cheung, Hanting Xie, Jas Kandola, Joseph Early, Kurt Cutajar, Niall Twomey |  |
| 500 |  |  [A Mutual Information Perspective on Federated Contrastive Learning](https://openreview.net/forum?id=JrmPG9ufKg) |  | 0 | We investigate contrastive learning in the federated setting through the lens of Sim- CLR and multi-view mutual information maximization. In doing so, we uncover a connection between contrastive representation learning and user verification; by adding a user verification loss to each client’s local... | Christos Louizos, Denis Korzhenkov, Matthias Reisser |  |
| 501 |  |  [MMD Graph Kernel: Effective Metric Learning for Graphs via Maximum Mean Discrepancy](https://openreview.net/forum?id=GZ6AcZwA8r) |  | 0 | This paper focuses on graph metric learning. First, we present a class of maximum mean discrepancy (MMD) based graph kernels, called MMD-GK. These kernels are computed by applying MMD to the node representations of two graphs with message-passing propagation. Secondly, we provide a class of deep... | Jicong Fan, Yan Sun |  |
| 502 |  |  [SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem](https://openreview.net/forum?id=HgOJlxzB16) |  | 0 | In this work, we consider the optimization process of minibatch stochastic gradient descent (SGD) on a 2-layer neural network with data separated by a quadratic ground truth function. We prove that with data drawn from the Boolean hypercube labeled by the quadratic \`\`XOR'' function $y = -x_ix_j$... | Margalit Glasgow |  |
| 503 |  |  [DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks](https://openreview.net/forum?id=gjfOL9z5Xr) |  | 0 | Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns are raised about potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may... | Diyi Yang, Jiaao Chen, Jindong Wang, Kaijie Zhu, Neil Zhenqiang Gong, Xing Xie |  |
| 504 |  |  [Illusory Attacks: Information-theoretic detectability matters in adversarial attacks](https://openreview.net/forum?id=F5dhGCdyYh) |  | 0 | Autonomous agents deployed in the real world need to be robust against adversarial attacks on sensory inputs. Robustifying agent policies requires anticipating the strongest attacks possible. We demonstrate that existing observation-space attacks on reinforcement learning agents have a common... | Adel Bibi, Christian Schröder de Witt, Jakob Nicolaus Foerster, João F. Henriques, Philip Torr, Stephen Marcus McAleer, Tim Franzmeyer |  |
| 505 |  |  [Addressing Signal Delay in Deep Reinforcement Learning](https://openreview.net/forum?id=Z8UfDs4J46) |  | 0 | Despite the notable advancements in deep reinforcement learning (DRL) in recent years, a prevalent issue that is often overlooked is the impact of signal delay. Signal delay occurs when there is a lag between an agent's perception of the environment and its corresponding actions. In this paper, we... | Dongqi Han, Dongsheng Li, William Wei Wang, Xufang Luo |  |
| 506 |  |  [Relay Diffusion: Unifying diffusion process across resolutions for image synthesis](https://openreview.net/forum?id=qTlcbLSm4p) |  | 0 | Diffusion models achieved great success in image synthesis, but still face challenges in high-resolution generation. Through the lens of discrete cosine transformation, we find the main reason is that \*the same noise level on a higher resolution results in a higher Signal-to-Noise Ratio in the... | Jianqiao Wangni, Jiayan Teng, Jie Tang, Ming Ding, Wendi Zheng, Wenyi Hong, Zhuoyi Yang |  |
| 507 |  |  [ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with Diffusion Models](https://openreview.net/forum?id=u48tHG5f66) |  | 0 | In this work, we investigate the capability of generating images from pre-trained diffusion models at much higher resolutions than the training image sizes. In addition, the generated images should have arbitrary image aspect ratios. When generating images directly at a higher resolution, 1024 x... | Haoxin Chen, Menghan Xia, Qifeng Chen, Ran He, Shaoshu Yang, Xiaodong Cun, Xintao Wang, Ying Shan, Yingqing He, Yong Zhang |  |
| 508 |  |  [DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization](https://openreview.net/forum?id=MSe8YFbhUE) |  | 0 | Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In... | Furong Huang, Guowei Xu, Hal Daumé III, Huazhe Xu, Jiaxin Yuan, Pu Hua, Ruijie Zheng, Shuzhen Li, Tianying Ji, Xiaoyu Liu, Xiyao Wang, Yanjie Ze, Yongyuan Liang, Yu Luo, Zhecheng Yuan |  |
| 509 |  |  [How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing: The Curses of Symmetry and Initialization](https://openreview.net/forum?id=xGvPKAiOhq) |  | 0 | This paper rigorously shows how over-parameterization dramatically changes the convergence behaviors of gradient descent (GD) for the matrix sensing problem, where the goal is to recover an unknown low-rank ground-truth matrix from near-isotropic linear measurements. First, we consider the... | Lijun Ding, Nuoya Xiong, Simon Shaolei Du |  |
| 510 |  |  [AnyText: Multilingual Visual Text Generation and Editing](https://openreview.net/forum?id=ezBH9WE9s2) |  | 0 | Diffusion model based Text-to-Image has achieved impressive achievements recently. Although current technology for synthesizing images is highly advanced and capable of generating images with high fidelity, it is still possible to give the show away when focusing on the text area in the generated... | JunYan He, Wangmeng Xiang, Xuansong Xie, Yifeng Geng, Yuxiang Tuo |  |
| 511 |  |  [At Which Training Stage Does Code Data Help LLMs Reasoning?](https://openreview.net/forum?id=KIPJKST4gw) |  | 0 | Large Language models (LLMs) have exhibited remarkable reasoning capabilities and become the foundation of language technologies. Inspired by the great success of code data in training LLMs, we naturally wonder at which training stage introducing code data can really help LLMs reasoning. To this... | Changjian Wang, Shanshan Li, Yingwei Ma, Yu Jiang, Yuanliang Zhang, Yue Liu, Yue Yu |  |
| 512 |  |  [Coordinate-Aware Modulation for Neural Fields](https://openreview.net/forum?id=4UiLqimGm5) |  | 0 | Neural fields, mapping low-dimensional input coordinates to corresponding signals, have shown promising results in representing various signals. Numerous methodologies have been proposed, and techniques employing MLPs and grid representations have achieved substantial success. MLPs allow compact... | Daniel Rho, Eunbyung Park, Jong Hwan Ko, Joo Chan Lee, Seungtae Nam |  |
| 513 |  |  [Efficient ConvBN Blocks for Transfer Learning and Beyond](https://openreview.net/forum?id=lHZm9vNm5H) |  | 0 | Convolution-BatchNorm (ConvBN) blocks are integral components in various computer vision tasks and other domains. A ConvBN block can operate in three modes: Train, Eval, and Deploy. While the Train mode is indispensable for training models from scratch, the Eval mode is suitable for transfer... | Anchang Bao, Guo Qin, Jiulong Shan, Kaichao You, Meng Cao, Mingsheng Long, Ping Huang |  |
| 514 |  |  [Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior](https://openreview.net/forum?id=TrKq4Wlwcz) |  | 0 | Shannon and Weaver's seminal information theory divides communication into three levels: technical, semantic, and effectiveness. While the technical level deals with the accurate reconstruction of transmitted symbols, the semantic and effectiveness levels deal with the inferred meaning and its... | Aanisha Bhattacharyya, Aditya Agrawal, Ashmit Khandelwal, Balaji Krishnamurthy, Changyou Chen, Ishita Dasgupta, Rajiv Ratn Shah, Somesh Singh, Stefano Petrangeli, Uttaran Bhattacharya, Yaman Kumar |  |
| 515 |  |  [Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints](https://openreview.net/forum?id=2cRzmWXK9N) |  | 0 | The increasing capabilities of large language models (LLMs) raise opportunities for artificial general intelligence but concurrently amplify safety concerns, such as potential misuse of AI systems, necessitating effective AI alignment. Reinforcement Learning from Human Feedback (RLHF) has emerged... | Chaoqi Wang, Chenghao Yang, Han Liu, Yibo Jiang, Yuxin Chen |  |
| 516 |  |  [FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets](https://openreview.net/forum?id=CYmF38ysDa) |  | 0 | Evaluation of Large Language Models (LLMs) is challenging because instruction-following necessitates alignment with human values and the required set of skills varies depending on the instruction. However, previous studies have mainly focused on coarse-grained evaluation (i.e. overall... | Doyoung Kim, Hyeonbin Hwang, James Thorne, Juho Kim, Minjoon Seo, Seonghyeon Ye, Seungone Kim, Sungdong Kim, Yongrae Jo |  |
| 517 |  |  [LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset](https://openreview.net/forum?id=BOfDKxfwt0) |  | 0 | Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25... | Eric P. Xing, Hao Zhang, Ion Stoica, Joseph E. Gonzalez, Lianmin Zheng, Siyuan Zhuang, Tianle Li, WeiLin Chiang, Ying Sheng, Yonghao Zhuang, Zhanghao Wu, Zhuohan Li, Zi Lin |  |
| 518 |  |  [EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models](https://openreview.net/forum?id=UmMa3UNDAz) |  | 0 | Diffusion models have demonstrated remarkable capabilities in image synthesis and related generative tasks. Nevertheless, their practicality for low-latency real-world applications is constrained by substantial computational costs and latency issues. Quantization is a dominant way to compress and... | Bohan Zhuang, Hong Zhou, Jing Liu, Weijia Wu, Yefei He |  |
| 519 |  |  [BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models](https://openreview.net/forum?id=3TO3TtnOFl) |  | 0 | Retrieval augmentation addresses many critical problems in large language models such as hallucination, staleness, and privacy leaks. However, running retrieval-augmented language models (LMs) is slow and difficult to scale due to processing large amounts of retrieved text. We introduce binary... | Hannaneh Hajishirzi, Qingqing Cao, Sewon Min, Yizhong Wang |  |
| 520 |  |  [Frozen Transformers in Language Models Are Effective Visual Encoder Layers](https://openreview.net/forum?id=t0FI3Q66K5) |  | 0 | This paper reveals that large language models (LLMs), despite being trained solely on text data, are surprisingly}strong encoders for purely visual tasks in the absence of language. Even more intriguingly, this can be achieved by a simple yet previously overlooked strategy -- employing a frozen... | YuXiong Wang, Yunze Man, Ziqi Pang, Ziyang Xie |  |
| 521 |  |  [SocioDojo: Building Lifelong Analytical Agents with Real-world Text and Time Series](https://openreview.net/forum?id=s9z0HzWJJp) |  | 0 | We introduce SocioDojo, an open-ended lifelong learning environment for developing ready-to-deploy autonomous agents capable of performing human-like analysis and decision-making on societal topics such as economics, finance, politics, and culture. It consists of (1) information sources from news,... | Junyan Cheng, Peter Chin |  |
| 522 |  |  [Learning Performance-Improving Code Edits](https://openreview.net/forum?id=ix7rLVHXyY) |  | 0 | With the decline of Moore's law, optimizing program performance has become a major focus of software research. However, high-level optimizations such as API and algorithm changes remain elusive due to the difficulty of understanding the semantics of code. Simultaneously, pretrained large language... | Alexander Shypula, Aman Madaan, Amir Yazdanbakhsh, Graham Neubig, Jacob R. Gardner, Milad Hashemi, Osbert Bastani, Parthasarathy Ranganathan, Uri Alon, Yimeng Zeng, Yiming Yang |  |
| 523 |  |  [Quasi-Monte Carlo for 3D Sliced Wasserstein](https://openreview.net/forum?id=Wd47f7HEXg) |  | 0 | Monte Carlo (MC) integration has been employed as the standard approximation method for the Sliced Wasserstein (SW) distance, whose analytical expression involves an intractable expectation. However, MC integration is not optimal in terms of absolute approximation error. To provide a better class... | Khai Nguyen, Nhat Ho, Nicola Bariletto |  |
| 524 |  |  [A Poincaré Inequality and Consistency Results for Signal Sampling on Large Graphs](https://openreview.net/forum?id=l3qtSNsPvC) |  | 0 | Large-scale graph machine learning is challenging as the complexity of learning models scales with the graph size. Subsampling the graph is a viable alternative, but sampling on graphs is nontrivial as graphs are non-Euclidean. Existing graph sampling techniques require not only computing the... | Luana Ruiz, Stefanie Jegelka, Thien Le |  |
| 525 |  |  [Cascading Reinforcement Learning](https://openreview.net/forum?id=KjOAHlKMF5) |  | 0 | Cascading bandits have gained popularity in recent years due to their applicability to recommendation systems and online advertising. In the cascading bandit model, at each timestep, an agent recommends an ordered subset of items (called an item list) from a pool of items, each associated with an... | R. Srikant, Wei Chen, Yihan Du |  |
| 526 |  |  [Complex priors and flexible inference in recurrent circuits with dendritic nonlinearities](https://openreview.net/forum?id=S5aUhpuyap) |  | 0 | Despite many successful examples in which probabilistic inference can account for perception, we have little understanding of how the brain represents and uses structured priors that capture the complexity of natural input statistics. Here we construct a recurrent circuit model that can implicitly... | Benjamin Lyo, Cristina Savin |  |
| 527 |  |  [On the hardness of learning under symmetries](https://openreview.net/forum?id=ARPrtuzAnQ) |  | 0 | We study the problem of learning equivariant neural networks via gradient descent. The incorporation of known symmetries ("equivariance") into neural nets has empirically improved the performance of learning pipelines, in domains ranging from biology to computer vision. However, a rich yet separate... | Bobak T. Kiani, Hannah Lawrence, Melanie Weber, Stefanie Jegelka, Thien Le |  |
| 528 |  |  [An Image Is Worth 1000 Lies: Transferability of Adversarial Images across Prompts on Vision-Language Models](https://openreview.net/forum?id=nc5GgFAvtk) |  | 0 | Different from traditional task-specific vision models, recent large VLMs can readily adapt to different vision tasks by simply using different textual instructions, i.e., prompts. However, a well-known concern about traditional task-specific vision models is that they can be misled by... | Fengyuan Liu, Haochen Luo, Jindong Gu, Philip Torr |  |
| 529 |  |  [One For All: Towards Training One Graph Model For All Classification Tasks](https://openreview.net/forum?id=4IT2pgc9v6) |  | 0 | Designing a single model to address multiple tasks has been a long-standing objective in artificial intelligence. Recently, large language models have demonstrated exceptional capability in solving different tasks within the language domain. However, a unified model for various graph tasks remains... | Dacheng Tao, Hao Liu, Jiarui Feng, Lecheng Kong, Muhan Zhang, Ningyue Liang, Yixin Chen |  |
| 530 |  |  [NAISR: A 3D Neural Additive Model for Interpretable Shape Representation](https://openreview.net/forum?id=wg8NPfeMF9) |  | 0 | Deep implicit functions (DIFs) have emerged as a powerful paradigm for many computer vision tasks such as 3D shape reconstruction, generation, registration, completion, editing, and understanding. However, given a set of 3D shapes with associated covariates there is at present no shape... | Andrew Prince, Benjamin Shields, Cameron Worden, Carlton J. Zdanski, Christopher Rutter, Jisan Mahmud, Julia S. Kimbell, Marc Niethammer, Samuel Kirse, William Dunn, Yining Jiao |  |
| 531 |  |  [Feature emergence via margin maximization: case studies in algebraic tasks](https://openreview.net/forum?id=i9wDX850jR) |  | 0 | Understanding the internal representations learned by neural networks is a cornerstone challenge in the science of machine learning. While there have been significant recent strides in some cases towards understanding \*how\* neural networks implement specific target functions, this paper explores... | Benjamin L. Edelman, CostinAndrei Oncescu, Depen Morwani, Rosie Zhao, Sham M. Kakade |  |
| 532 |  |  [On the Stability of Iterative Retraining of Generative Models on their own Data](https://openreview.net/forum?id=JORAfH2xFd) |  | 0 | Deep generative models have made tremendous progress in modeling complex data, often exhibiting generation quality that surpasses a typical human's ability to discern the authenticity of samples. Undeniably, a key driver of this success is enabled by the massive amounts of web-scale data consumed... | Alexandre Duplessis, Avishek Joey Bose, Gauthier Gidel, Marco Jiralerspong, Quentin Bertrand |  |
| 533 |  |  [Intriguing Properties of Generative Classifiers](https://openreview.net/forum?id=rmg0qMKYRQ) |  | 0 | What is the best paradigm to recognize objects---discriminative inference (fast but potentially prone to shortcut learning) or using a generative model (slow but potentially more robust)? We build on recent advances in generative modeling that turn text-to-image models into classifiers. This allows... | Kevin Clark, Priyank Jaini, Robert Geirhos |  |
| 534 |  |  [Fast Imitation via Behavior Foundation Models](https://openreview.net/forum?id=qnWtw3l0jb) |  | 0 | Imitation learning (IL) aims at producing agents that can imitate any behavior given a few expert demonstrations. Yet existing approaches require many demonstrations and/or running (online or offline) reinforcement learning (RL) algorithms for each new imitation task. Here we show that recent RL... | Ahmed Touati, Alessandro Lazaric, Andrea Tirinzoni, Matteo Pirotta, Yann Ollivier |  |
| 535 |  |  [Task Adaptation from Skills: Information Geometry, Disentanglement, and New Objectives for Unsupervised Reinforcement Learning](https://openreview.net/forum?id=zSxpnKh1yS) |  | 0 | Unsupervised reinforcement learning (URL) aims to learn general skills for unseen downstream tasks. Mutual Information Skill Learning (MISL) addresses URL by maximizing the mutual information between states and skills but lacks sufficient theoretical analysis, e.g., how well its learned skills can... | Lei Han, Meng Fang, Mykola Pechenizkiy, Qiang He, Tianyi Zhou, Yucheng Yang |  |
| 536 |  |  [NuwaDynamics: Discovering and Updating in Causal Spatio-Temporal Modeling](https://openreview.net/forum?id=sLdVl0q68X) |  | 0 | Spatio-temporal (ST) prediction plays a pivotal role in earth sciences, such as meteorological prediction, urban computing. Adequate high-quality data, coupled with deep models capable of inference, are both indispensable and prerequisite for achieving meaningful results. However, the sparsity of... | Guibin Zhang, Hao Wu, Kai Wang, Kun Wang, Xiaojiang Peng, Yang Wang, Yifan Duan, Yu Zheng, Yuxuan Liang |  |
| 537 |  |  [Pre-Training and Fine-Tuning Generative Flow Networks](https://openreview.net/forum?id=ylhiMfpqkm) |  | 0 | Generative Flow Networks (GFlowNets) are amortized samplers that learn stochastic policies to sequentially generate compositional objects from a given unnormalized reward distribution. They can generate diverse sets of high-reward objects, which is an important consideration in scientific discovery... | Kanika Madan, Ling Pan, Moksh Jain, Yoshua Bengio |  |
| 538 |  |  [CO2: Efficient Distributed Training with Full Communication-Computation Overlap](https://openreview.net/forum?id=ZO5cn4IfaN) |  | 0 | The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to... | Dong Li, Shidi Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Yiran Zhong, Yu Qiao, Zhen Qin |  |
| 539 |  |  [CADS: Unleashing the Diversity of Diffusion Models through Condition-Annealed Sampling](https://openreview.net/forum?id=zMoNrajk2X) |  | 0 | While conditional diffusion models are known to have good coverage of the data distribution, they still face limitations in output diversity, particularly when sampled with a high classifier-free guidance scale for optimal image quality or when trained on small datasets. We attribute this problem... | Derek Bradley, Jakob Buhmann, Otmar Hilliges, Romann M. Weber, Seyedmorteza Sadat |  |
| 540 |  |  [Image Inpainting via Iteratively Decoupled Probabilistic Modeling](https://openreview.net/forum?id=rUf9G9k2im) |  | 0 | Generative adversarial networks (GANs) have made great success in image inpainting yet still have difficulties tackling large missing regions. In contrast, iterative probabilistic algorithms, such as autoregressive and denoising diffusion models, have to be deployed with massive computing resources... | Kun Zhou, Wenbo Li, Xin Yu, Yibing Song, Zhe Lin |  |
| 541 |  |  [Image2Sentence based Asymmetrical Zero-shot Composed Image Retrieval](https://openreview.net/forum?id=5BXAXOpaWu) |  | 0 | The task of composed image retrieval (CIR) aims to retrieve images based on the query image and the text describing the users' intent. Existing methods have made great progress with the advanced large vision-language (VL) model in CIR task, however, they generally suffer from two main issues: lack... | Houqiang Li, Min Wang, Shuping Hui, Wengang Zhou, Yongchao Du |  |
| 542 |  |  [Bespoke Solvers for Generative Flow Models](https://openreview.net/forum?id=1PXEY7ofFX) |  | 0 | Diffusion or flow-based models are powerful generative paradigms that are notoriously hard to sample as samples are defined as solutions to high-dimensional Ordinary or Stochastic Differential Equations (ODEs/SDEs) which require a large Number of Function Evaluations (NFE) to approximate well.... | Albert Pumarola, Ali K. Thabet, Juan C. Pérez, Neta Shaul, Ricky T. Q. Chen, Yaron Lipman |  |
| 543 |  |  [Leveraging augmented-Lagrangian techniques for differentiating over infeasible quadratic programs in machine learning](https://openreview.net/forum?id=YCPDFfmkFr) |  | 0 | Optimization layers within neural network architectures have become increasingly popular for their ability to solve a wide range of machine learning tasks and to model domain-specific knowledge. However, designing optimization layers requires careful consideration as the underlying optimization... | Adrien B. Taylor, Antoine Bambade, Fabian Schramm, Justin Carpentier |  |
| 544 |  |  [ODEFormer: Symbolic Regression of Dynamical Systems with Transformers](https://openreview.net/forum?id=TzoHLiGVMo) |  | 0 | We introduce ODEFormer, the first transformer able to infer multidimensional ordinary differential equation (ODE) systems in symbolic form from the observation of a single solution trajectory. We perform extensive evaluations on two datasets: (i) the existing ‘Strogatz’ dataset featuring... | Alexander Mathis, Niki Kilbertus, Philippe Schwaller, Stéphane d'Ascoli, Sören Becker |  |
| 545 |  |  [Convergence of Bayesian Bilevel Optimization](https://openreview.net/forum?id=fLXpXa7iiz) |  | 0 | This paper presents the first theoretical guarantee for Bayesian bilevel optimization (BBO) that we term for the prevalent bilevel framework combining Bayesian optimization at the outer level to tune hyperparameters, and the inner-level stochastic gradient descent (SGD) for training the model. We... | Dacheng Tao, Fengxiang He, Shi Fu, Xinmei Tian |  |
| 546 |  |  [MovingParts: Motion-based 3D Part Discovery in Dynamic Radiance Field](https://openreview.net/forum?id=QQ6RgKYiQq) |  | 0 | We present MovingParts, a NeRF-based method for dynamic scene reconstruction and part discovery. We consider motion as an important cue for identifying parts, that all particles on the same part share the common motion pattern. From the perspective of fluid simulation, existing deformation-based... | Hao Su, Kaizhi Yang, Xiaoshuai Zhang, Xuejin Chen, Zexiang Xu, Zhiao Huang |  |
| 547 |  |  [Equivariant Matrix Function Neural Networks](https://openreview.net/forum?id=yrgQdA5NkI) |  | 0 | Graph Neural Networks (GNNs), especially message-passing neural networks (MPNNs), have emerged as powerful architectures for learning on graphs in diverse applications. However, MPNNs face challenges when modeling non-local interactions in systems such as large conjugated molecules, metals, or... | Christoph Ortner, Felix A. Faber, Gábor Csányi, Ilyes Batatia, Lars L. Schaaf |  |
| 548 |  |  [Multi-resolution HuBERT: Multi-resolution Speech Self-Supervised Learning with Masked Unit Prediction](https://openreview.net/forum?id=kUuKFW7DIF) |  | 0 | Existing Self-Supervised Learning (SSL) models for speech typically process speech signals at a fixed resolution of 20 milliseconds. This approach overlooks the varying informational content present at different resolutions in speech signals. In contrast, this paper aims to incorporate... | Anna Y. Sun, Hirofumi Inaguma, Ilia Kulikov, Jiatong Shi, Xutai Ma |  |
| 549 |  |  [Input-gradient space particle inference for neural network ensembles](https://openreview.net/forum?id=nLWiR5P3wr) |  | 0 | Deep Ensembles (DEs) demonstrate improved accuracy, calibration and robustness to perturbations over single neural networks partly due to their functional diversity. Particle-based variational inference (ParVI) methods enhance diversity by formalizing a repulsion term based on a network similarity... | Luigi Acerbi, Markus Heinonen, Samuel Kaski, Trung Q. Trinh |  |
| 550 |  |  [Prototypical Information Bottlenecking and Disentangling for Multimodal Cancer Survival Prediction](https://openreview.net/forum?id=otHZ8JAIgh) |  | 0 | Multimodal learning significantly benefits cancer survival prediction, especially the integration of pathological images and genomic data. Despite advantages of multimodal learning for cancer survival prediction, massive redundancy in multimodal data prevents it from extracting discriminative and... | Fengying Xie, Hao Chen, Jianqi Chen, Yilan Zhang, Yingxue Xu |  |
| 551 |  |  [MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data](https://openreview.net/forum?id=8xliOUg9EW) |  | 0 | Recent large language models (LLMs) have witnessed significant advancement in various tasks, including mathematical reasoning and theorem proving. As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the reasoning ability of LLMs but still face... | Haiming Wang, Huajian Xin, Linqi Song, Qingxing Cao, Xiaodan Liang, Xiaohan Lin, Yinya Huang, Zhenguo Li, Zhengying Liu |  |
| 552 |  |  [FLD: Fourier Latent Dynamics for Structured Motion Representation and Learning](https://openreview.net/forum?id=xsd2llWYSA) |  | 0 | Motion trajectories offer reliable references for physics-based motion learning but suffer from sparsity, particularly in regions that lack sufficient data coverage. To address this challenge, we introduce a self-supervised, structured representation and generation method that extracts... | Chenhao Li, Elijah StangerJones, Sangbae Kim, Steve Heim |  |
| 553 |  |  [Towards Reliable and Efficient Backdoor Trigger Inversion via Decoupling Benign Features](https://openreview.net/forum?id=Tw9wemV6cb) |  | 0 | Recent studies revealed that using third-party models may lead to backdoor threats, where adversaries can maliciously manipulate model predictions based on backdoors implanted during model training. Arguably, backdoor trigger inversion (BTI), which generates trigger patterns of given benign samples... | Kui Ren, Kunzhe Huang, Xiong Xu, Yiming Li, Zhan Qin |  |
| 554 |  |  [Long-Term Typhoon Trajectory Prediction: A Physics-Conditioned Approach Without Reanalysis Data](https://openreview.net/forum?id=ziDFH8TPPK) |  | 0 | In the face of escalating climate changes, typhoon intensities and their ensuing damage have surged. Accurate trajectory prediction is crucial for effective damage control. Traditional physics-based models, while comprehensive, are computationally intensive and rely heavily on the expertise of... | Beomkyu Choi, Doyi Kim, HaeGon Jeon, Hyeri Kim, Jeongwon Ryu, Minseok Seo, Sanghoon Choi, Sohee Son, Yeji Choi, YoungJae Park |  |
| 555 |  |  [Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery Detection](https://openreview.net/forum?id=8iTpB4RNvP) |  | 0 | The proliferation of face forgery techniques has raised significant concerns within society, thereby motivating the development of face forgery detection methods. These methods aim to distinguish forged faces from genuine ones and have proven effective in practical applications. However, this paper... | Aishan Liu, Jiawei Liang, Junhao Kuang, Siyuan Liang, Xiaochun Cao, Xiaojun Jia |  |
| 556 |  |  [Unified Human-Scene Interaction via Prompted Chain-of-Contacts](https://openreview.net/forum?id=1vCnDyQkjg) |  | 0 | Human-Scene Interaction (HSI) is a vital component of fields like embodied AI and virtual reality. Despite advancements in motion quality and physical plausibility, two pivotal factors, versatile interaction control and the development of a user-friendly interface, require further exploration... | Bo Dai, Dahua Lin, Jiangmiao Pang, Jingbo Wang, Jinkun Cao, Tai Wang, Wenwei Zhang, Zeqi Xiao |  |
| 557 |  |  [PTaRL: Prototype-based Tabular Representation Learning via Space Calibration](https://openreview.net/forum?id=G32oY4Vnm8) |  | 0 | Tabular data have been playing a mostly important role in diverse real-world fields, such as healthcare, engineering, finance, etc. With the recent success of deep learning, many tabular machine learning (ML) methods based on deep networks (e.g., Transformer, ResNet) have achieved competitive... | Dandan Guo, Hangting Ye, He Zhao, Shun Zheng, Wei Fan, Xiaozhuang Song, Yi Chang |  |
| 558 |  |  [Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data](https://openreview.net/forum?id=4VIgNuQ1pY) |  | 0 | Irregular sampling intervals and missing values in real-world time series data present challenges for conventional methods that assume consistent intervals and complete data. Neural Ordinary Differential Equations (Neural ODEs) offer an alternative approach, utilizing neural networks combined with... | Dongyoung Lim, Sungil Kim, YongKyung Oh |  |
| 559 |  |  [What does the Knowledge Neuron Thesis Have to do with Knowledge?](https://openreview.net/forum?id=2HJRwwbV3G) |  | 0 | We reassess the Knowledge Neuron (KN) Thesis: an interpretation of the mechanism underlying the ability of large language models to recall facts from a training corpus. This nascent thesis proposes that facts are recalled from the training corpus through the MLP weights in a manner resembling... | Andrew Liu, Gerald Penn, Jingcheng Niu, Zining Zhu |  |
| 560 |  |  [Point2SSM: Learning Morphological Variations of Anatomies from Point Clouds](https://openreview.net/forum?id=DqziS8DG4M) |  | 0 | We present Point2SSM, a novel unsupervised learning approach for constructing correspondence-based statistical shape models (SSMs) directly from raw point clouds. SSM is crucial in clinical research, enabling population-level analysis of morphological variation in bones and organs. Traditional... | Jadie Adams, Shireen Y. Elhabian |  |
| 561 |  |  [Improving Domain Generalization with Domain Relations](https://openreview.net/forum?id=Dc4rXq3HIA) |  | 0 | Distribution shift presents a significant challenge in machine learning, where models often underperform during the test stage when faced with a different distribution than the one they were trained on. In this paper, we focus on domain shifts, which occur when the model is applied to new domains... | Chelsea Finn, Huaxiu Yao, Pang Wei Koh, Shengchao Liu, Xinyi Pan, Xinyu Yang |  |
| 562 |  |  [Generating Images with 3D Annotations Using Diffusion Models](https://openreview.net/forum?id=XlkN11Xj6J) |  | 0 | Diffusion models have emerged as a powerful generative method, capable of producing stunning photo-realistic images from natural language descriptions. However, these models lack explicit control over the 3D structure in the generated images. Consequently, this hinders our ability to obtain... | Adam Kortylewski, Alan L. Yuille, Angtian Wang, Beijia Lu, Guofeng Zhang, Jiahao Wang, Qihao Liu, Ruxiao Duan, Wufei Ma, Xiaoding Yuan, Yaoyao Liu, Yi Zhang, Yongrui Qi, Zihao Xiao |  |
| 563 |  |  [High-dimensional SGD aligns with emerging outlier eigenspaces](https://openreview.net/forum?id=MHjigVnI04) |  | 0 | We rigorously study the joint evolution of training dynamics via stochastic gradient descent (SGD) and the spectra of empirical Hessian and gradient matrices. We prove that in two canonical classification tasks for multi-class high-dimensional mixtures and either 1 or 2-layer neural networks, the... | Aukosh Jagannath, Gérard Ben Arous, Jiaoyang Huang, Reza Gheissari |  |
| 564 |  |  [B-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis](https://openreview.net/forum?id=fLf589bx1f) |  | 0 | Program synthesis aims to create accurate, executable programs from problem specifications, specifically from natural language descriptions in our context. Recent studies have leveraged the power of reinforcement learning (RL) in conjunction with large language models (LLMs), significantly... | Hongxia Yang, Liyu Chen, Tao Sun, Yunzhe Tao, Zishun Yu |  |
| 565 |  |  [Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts](https://openreview.net/forum?id=auKAUJZMO6) |  | 0 | By providing external information to large language models (LLMs), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory. However, how receptive are LLMs to such external evidence, especially when... | Jian Xie, Jiangjie Chen, Kai Zhang, Renze Lou, Yu Su |  |
| 566 |  |  [A Hierarchical Bayesian Model for Few-Shot Meta Learning](https://openreview.net/forum?id=mQ72XRfYRZ) |  | 0 | We propose a novel hierarchical Bayesian model for the few-shot meta learning problem. We consider episode-wise random variables to model episode-specific generative processes, where these local random variables are governed by a higher-level global random variable. The global variable captures... | Minyoung Kim, Timothy M. Hospedales |  |
| 567 |  |  [Faithful and Efficient Explanations for Neural Networks via Neural Tangent Kernel Surrogate Models](https://openreview.net/forum?id=yKksu38BpM) |  | 0 | A recent trend in explainable AI research has focused on surrogate modeling, where neural networks are approximated as simpler ML algorithms such as kernel machines. A second trend has been to utilize kernel functions in various explain-by-example or data attribution tasks. In this work, we combine... | Anand D. Sarwate, Andrew Engel, Ioana Dumitriu, Natalie Frank, Sutanay Choudhury, Tony Chiang, Zhichao Wang |  |
| 568 |  |  [Conformal Risk Control](https://openreview.net/forum?id=33XGfHLtZg) |  | 0 | We extend conformal prediction to control the expected value of any monotone loss function. The algorithm generalizes split conformal prediction together with its coverage guarantee. Like conformal prediction, the conformal risk control procedure is tight up to an $\mathcal{O}(1/n)$ factor. We also... | Adam Fisch, Anastasios Nikolas Angelopoulos, Lihua Lei, Stephen Bates, Tal Schuster |  |
| 569 |  |  [RetroBridge: Modeling Retrosynthesis with Markov Bridges](https://openreview.net/forum?id=770DetV8He) |  | 0 | Retrosynthesis planning is a fundamental challenge in chemistry which aims at designing multi-step reaction pathways from commercially available starting materials to a target molecule. Each step in multi-step retrosynthesis planning requires accurate prediction of possible precursor molecules... | Arne Schneuing, Bruno E. Correia, Ilia Igashov, Marwin H. S. Segler, Michael M. Bronstein |  |
| 570 |  |  [InstructScene: Instruction-Driven 3D Indoor Scene Synthesis with Semantic Graph Prior](https://openreview.net/forum?id=LtuRgL03pI) |  | 0 | Comprehending natural language instructions is a charming property for 3D indoor scene synthesis systems. Existing methods directly model object joint distributions and express object relations implicitly within a scene, thereby hindering the controllability of generation. We introduce... | Chenguo Lin, Yadong Mu |  |
| 571 |  |  [Single Motion Diffusion](https://openreview.net/forum?id=DrhZneqz4n) |  | 0 | Synthesizing realistic animations of humans, animals, and even imaginary creatures, has long been a goal for artists and computer graphics professionals. Compared to the imaging domain, which is rich with large available datasets, the number of data instances for the motion domain is limited,... | Amit Haim Bermano, Daniel CohenOr, Guy Tevet, Inbal Leibovitch, Moab Arar, Sigal Raab |  |
| 572 |  |  [Physics-Regulated Deep Reinforcement Learning: Invariant Embeddings](https://openreview.net/forum?id=5Dwqu5urzs) |  | 0 | This paper proposes the Phy-DRL: a physics-regulated deep reinforcement learning (DRL) framework for safety-critical autonomous systems. The Phy-DRL has three distinguished invariant-embedding designs: i) residual action policy (i.e., integrating data-driven-DRL action policy and... | Hongpeng Cao, Lui Sha, Marco Caccamo, Yanbing Mao |  |
| 573 |  |  [BatteryML: An Open-source Platform for Machine Learning on Battery Degradation](https://openreview.net/forum?id=sxGugrYhP9) |  | 0 | Battery degradation remains a pivotal concern in the energy storage domain, with machine learning emerging as a potent tool to drive forward insights and solutions. However, this intersection of electrochemical science and machine learning poses complex challenges. Machine learning experts often... | Han Zhang, Jiang Bian, Shun Zheng, Xiaofan Gui, Yuqi Li, Ziheng Lu |  |
| 574 |  |  [SaProt: Protein Language Modeling with Structure-aware Vocabulary](https://openreview.net/forum?id=6MRm3G4NiU) |  | 0 | Large-scale protein language models (PLMs), such as the ESM family, have achieved remarkable performance in various downstream tasks related to protein structure and function by undergoing unsupervised training on residue sequences. They have become essential tools for researchers and practitioners... | Chenchen Han, Fajie Yuan, Jin Su, Junjie Shan, Xibin Zhou, Yuyang Zhou |  |
| 575 |  |  [PixArt-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis](https://openreview.net/forum?id=eAKmQPe3m1) |  | 0 | The most advanced text-to-image (T2I) models require significant training costs (e.g., millions of GPU hours), seriously hindering the fundamental innovation for the AIGC community while increasing CO2 emissions. This paper introduces PixArt-$\alpha$, a Transformer-based T2I diffusion model whose... | Chongjian Ge, Enze Xie, Huchuan Lu, James T. Kwok, Jincheng Yu, Junsong Chen, Lewei Yao, Ping Luo, Zhenguo Li, Zhongdao Wang |  |
| 576 |  |  [Sentence-level Prompts Benefit Composed Image Retrieval](https://openreview.net/forum?id=m3ch3kJL7q) |  | 0 | Composed image retrieval (CIR) is the task of retrieving specific images by using a query that involves both a reference image and a relative caption. Most existing CIR models adopt the late-fusion strategy to combine visual and language features. Besides, several approaches have also been... | ChunMei Feng, Fahad Khan, Rick Siow Mong Goh, Salman Khan, Wangmeng Zuo, Xinxing Xu, Yang Bai, Yong Liu |  |
| 577 |  |  [Compositional Generative Inverse Design](https://openreview.net/forum?id=wmX0CqFSd7) |  | 0 | Inverse design, where we seek to design input variables in order to optimize an underlying objective function, is an important problem that arises across fields such as mechanical engineering to aerospace engineering. Inverse design is typically formulated as an optimization problem, with recent... | Gianluca Iaccarino, Jure Leskovec, Long Wei, Tailin Wu, Takashi Maruyama, Tao Zhang, Yilun Du |  |
| 578 |  |  [What does automatic differentiation compute for neural networks?](https://openreview.net/forum?id=8vKknbgXxf) |  | 0 | Forward- or reverse-mode automatic differentiation (AD) is a popular algorithm for computing the derivative of a function expressed by a program. AD always outputs the correct derivative if a program does not use any non-differentiable functions and control flows; however, it may return an... | Sanghyuk Chun, Sejun Park, Wonyeol Lee |  |
| 579 |  |  [OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models](https://openreview.net/forum?id=8Wuvhh0LYW) |  | 0 | Large language models (LLMs) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving... | Kaipeng Zhang, Lirui Zhao, Mengzhao Chen, Peng Gao, Peng Xu, Ping Luo, Wenqi Shao, Yu Qiao, Zhaoyang Zhang, Zhiqian Li |  |
| 580 |  |  [Ferret: Refer and Ground Anything Anywhere at Any Granularity](https://openreview.net/forum?id=2msbbX3ydD) |  | 0 | We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions. To unify referring and grounding in the LLM paradigm, Ferret employs a novel and powerful... | Bowen Zhang, Haotian Zhang, Haoxuan You, Liangliang Cao, ShihFu Chang, Xianzhi Du, Yinfei Yang, Zhe Gan, Zirui Wang |  |
| 581 |  |  [SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation](https://openreview.net/forum?id=gn0mIhQGNM) |  | 0 | With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often suffer limitations in unlearning accuracy, stability, and cross-domain... | Chongyu Fan, Dennis Wei, Eric Wong, Jiancheng Liu, Sijia Liu, Yihua Zhang |  |
| 582 |  |  [Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization](https://openreview.net/forum?id=KOZu91CzbK) |  | 0 | Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing... | Caiming Xiong, Devansh Arpit, Huan Wang, Jianguo Zhang, Juan Carlos Niebles, Le Xue, Phil Mui, Ran Xu, Rithesh R. N., Shelby Heinecke, Silvio Savarese, Weiran Yao, Yihao Feng, Zeyuan Chen, Zhiwei Liu |  |
| 583 |  |  [BECLR: Batch Enhanced Contrastive Few-Shot Learning](https://openreview.net/forum?id=k9SVcrmXL8) |  | 0 | Learning quickly from very few labeled samples is a fundamental attribute that separates machines and humans in the era of deep representation learning. Unsupervised few-shot learning (U-FSL) aspires to bridge this gap by discarding the reliance on annotations at training time. Intrigued by the... | Hadi Jamali Rad, Stylianos PoulakakisDaktylidis |  |
| 584 |  |  [How to Capture Higher-order Correlations? Generalizing Matrix Softmax Attention to Kronecker Computation](https://openreview.net/forum?id=v0zNCwwkaV) |  | 0 | In the classical transformer attention scheme, we are given three $n \times d$ size matrices $Q, K, V$ (the query, key, and value tokens), and the goal is to compute a new $n \times d$ size matrix $D^{-1} \exp(QK^\top) V$ where $D = \mathrm{diag}( \exp(QK^\top) {\bf 1}_n )$. Here, $\exp()$ is... | Josh Alman, Zhao Song |  |
| 585 |  |  [DreamLLM: Synergistic Multimodal Comprehension and Creation](https://openreview.net/forum?id=y01KGvd9Bw) |  | 0 | This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative... | Chunrui Han, Haoran Wei, Hongyu Zhou, Jianjian Sun, Jinrong Yang, Kaisheng Ma, Li Yi, Liang Zhao, Runpei Dong, Xiangwen Kong, Xiangyu Zhang, Yuang Peng, Zekun Qi, Zheng Ge |  |
| 586 |  |  [Learning to Act from Actionless Videos through Dense Correspondences](https://openreview.net/forum?id=Mhb5fpA1T0) |  | 0 | In this work, we present an approach to construct a video-based robot policy capable of reliably executing diverse tasks across different robots and environments from few video demonstrations without using any action annotations. Our method leverages images as a task-agnostic representation,... | Jiayuan Mao, Joshua B. Tenenbaum, PoChen Ko, ShaoHua Sun, Yilun Du |  |
| 587 |  |  [On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation](https://openreview.net/forum?id=CvYBvgEUK9) |  | 0 | In this work, we study first-order algorithms for solving Bilevel Optimization (BO) where the objective functions are smooth but possibly nonconvex in both levels and the variables are restricted to closed convex sets. As a first step, we study the landscape of BO through the lens of penalty... | Dohyun Kwon, Jeongyeol Kwon, Robert D. Nowak, Stephen Wright |  |
| 588 |  |  [Scaling Laws for Sparsely-Connected Foundation Models](https://openreview.net/forum?id=i9K2ZWkYIP) |  | 0 | We explore the impact of parameter sparsity on the scaling behavior of Transformers trained on massive datasets (i.e., "foundation models"), in both vision and language domains. In this setting, we identify the first scaling law describing the relationship between weight sparsity, number of... | Carlos Riquelme Ruiz, Dan Alistarh, Elias Frantar, Neil Houlsby, Utku Evci |  |
| 589 |  |  [Nearly d-Linear Convergence Bounds for Diffusion Models via Stochastic Localization](https://openreview.net/forum?id=r5njV3BsuD) |  | 0 | Denoising diffusions are a powerful method to generate approximate samples from high-dimensional data distributions. Recent results provide polynomial bounds on their convergence rate, assuming $L^2$-accurate scores. Until now, the tightest bounds were either superlinear in the data dimension or... | Arnaud Doucet, George Deligiannidis, Joe Benton, Valentin De Bortoli |  |
| 590 |  |  [DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models](https://openreview.net/forum?id=OEL4FJMg1b) |  | 0 | Despite the ability of text-to-image (T2I) diffusion models to generate high-quality images, transferring this ability to accurate image editing remains a challenge. In this paper, we propose a novel image editing method, DragonDiffusion, enabling Drag-style manipulation on Diffusion models.... | Chong Mou, Jian Zhang, Jiechong Song, Xintao Wang, Ying Shan |  |
| 591 |  |  [Uni3D: Exploring Unified 3D Representation at Scale](https://openreview.net/forum?id=wcaE4Dfgt8) |  | 0 | Scaling up representations for images or text has been extensively investigated in the past few years and has led to revolutions in learning vision and language. However, scalable representation for 3D objects and scenes is relatively unexplored. In this work, we present Uni3D, a 3D foundation... | Baorui Ma, Jinsheng Wang, Junsheng Zhou, Tiejun Huang, Xinlong Wang, YuShen Liu |  |
| 592 |  |  [CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents](https://openreview.net/forum?id=UBVNwD3hPN) |  | 0 | The generalization of decision-making agents encompasses two fundamental elements: learning from past experiences and reasoning in novel contexts. However, the predominant emphasis in most interactive environments is on learning, often at the expense of complexity in reasoning. In this paper, we... | Bangcheng Yang, Junqi Wang, Nian Liu, Pring Wong, Shuo Chen, Siyuan Qi, SongChun Zhu, Xiangyu Kong, Xiaoyuan Zhang, Yaodong Yang, Yexin Li, Yifan Zhong, Zhaowei Zhang |  |
| 593 |  |  [Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy](https://openreview.net/forum?id=EXitynZhYn) |  | 0 | The evaluation of text-generative vision-language models is a challenging yet crucial endeavor. By addressing the limitations of existing Visual Question Answering (VQA) benchmarks and proposing innovative evaluation methodologies, our research seeks to advance our understanding of these models’... | María Alejandra Bravo, Simon Ging, Thomas Brox |  |
| 594 |  |  [GIM: Learning Generalizable Image Matcher From Internet Videos](https://openreview.net/forum?id=NYN1b8GRGS) |  | 0 | Image matching is a fundamental computer vision problem. While learning-based methods achieve state-of-the-art performance on existing benchmarks, they generalize poorly to in-the-wild images. Such methods typically need to train separate models for different scene types (e.g., indoor vs. outdoor)... | Cheng Wang, Kaixuan Wang, Matthias Müller, Wei Yin, Xiaozhi Chen, Xuelun Shen, Zhipeng Cai, Zijun Li |  |
| 595 |  |  [SyncDreamer: Generating Multiview-consistent Images from a Single-view Image](https://openreview.net/forum?id=MN3yH2ovHb) |  | 0 | In this paper, we present a novel diffusion model called SyncDreamer that generates multiview-consistent images from a single-view image. Using pretrained large-scale 2D diffusion models, recent work Zero123 demonstrates the ability to generate plausible novel views from a single-view image of an... | Cheng Lin, Lingjie Liu, Taku Komura, Wenping Wang, Xiaoxiao Long, Yuan Liu, Zijiao Zeng |  |
| 596 |  |  [Finite-State Autoregressive Entropy Coding for Efficient Learned Lossless Compression](https://openreview.net/forum?id=D5mJSNtUtv) |  | 0 | Learned lossless data compression has garnered significant attention recently due to its superior compression ratios compared to traditional compressors. However, the computational efficiency of these models jeopardizes their practicality. This paper proposes a novel system for improving the... | Hang Yu, Jianguo Li, Weiyao Lin, Yufeng Zhang |  |
| 597 |  |  [ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs](https://openreview.net/forum?id=dHng2O0Jjr) |  | 0 | Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but... | Bill Qian, Dahai Li, Jie Zhou, Kunlun Zhu, Lan Yan, Lauren Hong, Maosong Sun, Mark Gerstein, Runchu Tian, Ruobing Xie, Shihao Liang, Sihan Zhao, Xiangru Tang, Xin Cong, Yankai Lin, Yaxi Lu, Yining Ye, Yujia Qin, Zhiyuan Liu |  |
| 598 |  |  [Enhanced Face Recognition using Intra-class Incoherence Constraint](https://openreview.net/forum?id=uELjxVbrqG) |  | 0 | The current face recognition (FR) algorithms has achieved a high level of accuracy, making further improvements increasingly challenging. While existing FR algorithms primarily focus on optimizing margins and loss functions, limited attention has been given to exploring the feature representation... | Le Yang, Lei Wang, Yinggui Wang, Yuanqing Huang |  |
| 599 |  |  [Entropy is not Enough for Test-Time Adaptation: From the Perspective of Disentangled Factors](https://openreview.net/forum?id=9w3iw8wDuE) |  | 0 | Test-time adaptation (TTA) fine-tunes pre-trained deep neural networks for unseen test data. The primary challenge of TTA is limited access to the entire test dataset during online updates, causing error accumulation. To mitigate it, TTA methods have utilized the model output's entropy as a... | Dahuin Jung, Jonghyun Lee, Juhyeon Shin, Junsung Park, Saehyung Lee, Sungroh Yoon, Uiwon Hwang |  |
| 600 |  |  [SEAL: A Framework for Systematic Evaluation of Real-World Super-Resolution](https://openreview.net/forum?id=CGlczSBBSj) |  | 0 | Real-world Super-Resolution (Real-SR) methods focus on dealing with diverse real-world images and have attracted increasing attention in recent years. The key idea is to use a complex and high-order degradation model to mimic real-world degradations. Although they have achieved impressive results... | Chao Dong, Wenlong Zhang, Xiangyu Chen, XiaoMing Wu, Xiaohui Li, Xiaoyun Zhang, Yu Qiao |  |
| 601 |  |  [Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood](https://openreview.net/forum?id=AyzkDpuqcl) |  | 0 | Training energy-based models (EBMs) on high-dimensional data can be both challenging and time-consuming, and there exists a noticeable gap in sample quality between EBMs and other generative frameworks like GANs and diffusion models. To close this gap, inspired by the recent efforts of learning... | Jianwen Xie, Ruiqi Gao, Yaxuan Zhu, Ying Nian Wu |  |
| 602 |  |  [MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning](https://openreview.net/forum?id=yLClGs770I) |  | 0 | We introduce MAmmoTH, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset. MathInstruct is compiled from 13 math datasets with intermediate... | Ge Zhang, Huan Sun, Wenhao Huang, Wenhu Chen, Xiang Yue, Xingwei Qu, Yao Fu, Yu Su |  |
| 603 |  |  [Time Travel in LLMs: Tracing Data Contamination in Large Language Models](https://openreview.net/forum?id=2Rwq6c3tvr) |  | 0 | Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in measuring LLMs' real effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination... | Mihai Surdeanu, Shahriar Golchin |  |
| 604 |  |  [Variational Inference for SDEs Driven by Fractional Noise](https://openreview.net/forum?id=rtx8B94JMS) |  | 0 | We present a novel variational framework for performing inference in (neural) stochastic differential equations (SDEs) driven by Markov-approximate fractional Brownian motion (fBM). SDEs offer a versatile tool for modeling real-world continuous-time dynamic systems with inherent noise and... | Guillaume Crevecoeur, Manfred Opper, Rembert Daems, Tolga Birdal |  |
| 605 |  |  [Implicit regularization of deep residual networks towards neural ODEs](https://openreview.net/forum?id=AbXGwqb5Ht) |  | 0 | Residual neural networks are state-of-the-art deep learning models. Their continuous-depth analog, neural ordinary differential equations (ODEs), are also widely used. Despite their success, the link between the discrete and continuous models still lacks a solid mathematical foundation. In this... | Gérard Biau, Michael Eli Sander, Pierre Marion, YuHan Wu |  |
| 606 |  |  [NetInfoF Framework: Measuring and Exploiting Network Usable Information](https://openreview.net/forum?id=KY8ZNcljVU) |  | 0 | Given a node-attributed graph, and a graph task (link prediction or node classification), can we tell if a graph neural network (GNN) will perform well? More specifically, do the graph structure and the node features carry enough usable information for the task? Our goals are (1) to develop a fast... | Christos Faloutsos, Da Zheng, Haiyang Yu, Jian Zhang, MengChieh Lee, Soji Adeshina, Vassilis N. Ioannidis, Xiang Song |  |
| 607 |  |  [BarLeRIa: An Efficient Tuning Framework for Referring Image Segmentation](https://openreview.net/forum?id=wHLDHRkmEu) |  | 0 | Pre-training followed by full fine-tuning has gradually been substituted by Parameter-Efficient Tuning (PET) in the field of computer vision. PET has gained popularity, especially in the context of large-scale models, due to its ability to reduce transfer learning costs and conserve hardware... | Bowen Shi, Chenglin Li, Hongkai Xiong, Jin Li, Qi Tian, Wenrui Dai, Xiaopeng Zhang, Yaoming Wang |  |
| 608 |  |  [Local Search GFlowNets](https://openreview.net/forum?id=6cFcw1Rxww) |  | 0 | Generative Flow Networks (GFlowNets) are amortized sampling methods that learn a distribution over discrete objects proportional to their rewards. GFlowNets exhibit a remarkable ability to generate diverse samples, yet occasionally struggle to consistently produce samples with high rewards due to... | Dinghuai Zhang, Emmanuel Bengio, Jinkyoo Park, Minsu Kim, Sungsoo Ahn, Taeyoung Yun, Yoshua Bengio |  |
| 609 |  |  [Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt Tensor Products](https://openreview.net/forum?id=mhyQXJ6JsK) |  | 0 | Developing equivariant neural networks for the E(3) group plays an important role in modeling 3D data across real-world applications. Enforcing this equivariance primarily involves the tensor products of irreducible representations (irreps). However, the computational complexity of such operations... | Aditi S. Krishnapriyan, Shengjie Luo, Tianlang Chen |  |
| 610 |  |  [Idempotence and Perceptual Image Compression](https://openreview.net/forum?id=Cy5v64DqEF) |  | 0 | Idempotence is the stability of image codec to re-compression. At the first glance, it is unrelated to perceptual image compression. However, we find that theoretically: 1) Conditional generative model-based perceptual codec satisfies idempotence; 2) Unconditional generative model with idempotence... | Dailan He, Hongwei Qin, Jingjing Liu, Lina Guo, Tongda Xu, YaQin Zhang, Yan Wang, Yanghao Li, Yuanyuan Wang, Zhe Wang, Ziran Zhu |  |
| 611 |  |  [Forward χ2 Divergence Based Variational Importance Sampling](https://openreview.net/forum?id=HD5Y7M8Xdk) |  | 0 | Maximizing the marginal log-likelihood is a crucial aspect of learning latent variable models, and variational inference (VI) stands as the commonly adopted method. However, VI can encounter challenges in achieving a high marginal log-likelihood when dealing with complicated posterior... | Anqi Wu, Chengrui Li, Weihan Li, Yule Wang |  |
| 612 |  |  [Noisy Interpolation Learning with Shallow Univariate ReLU Networks](https://openreview.net/forum?id=GTUoTJXPBf) |  | 0 | Understanding how overparameterized neural networks generalize despite perfect interpolation of noisy training data is a fundamental question. Mallinar et. al. (2022) noted that neural networks seem to often exhibit \`\`tempered overfitting'', wherein the population risk does not converge to the... | Gal Vardi, Nathan Srebro, Nirmit Joshi |  |
| 613 |  |  [Initializing Models with Larger Ones](https://openreview.net/forum?id=dyrGMhicMw) |  | 0 | Weight initialization plays an important role in neural network training. Widely used initialization methods are proposed and evaluated for networks that are trained from scratch. However, the growing number of pretrained models now offers new opportunities for tackling this classical problem of... | Kirill Vishniakov, Lingjie Liu, Trevor Darrell, Yanjie Chen, Yida Yin, Zhiqiang Shen, Zhiqiu Xu, Zhuang Liu |  |
| 614 |  |  [DMV3D: Denoising Multi-view Diffusion Using 3D Large Reconstruction Model](https://openreview.net/forum?id=H4yQefeXhp) |  | 0 | We propose DMV3D, a novel 3D generation approach that uses a transformer-based 3D large reconstruction model to denoise multi-view diffusion. Our reconstruction model incorporates a triplane NeRF representation and, functioning as a denoiser, can denoise noisy multi-view images via 3D NeRF... | Fujun Luan, Gordon Wetzstein, Hao Tan, Jiahao Li, Kai Zhang, Kalyan Sunkavalli, Peng Wang, Sai Bi, Yinghao Xu, Zexiang Xu, Zifan Shi |  |
| 615 |  |  [Influencer Backdoor Attack on Semantic Segmentation](https://openreview.net/forum?id=VmGRoNDQgJ) |  | 0 | When a small number of poisoned samples are injected into the training dataset of a deep neural network, the network can be induced to exhibit malicious behavior during inferences, which poses potential threats to real-world applications. While they have been intensively studied in classification,... | Haoheng Lan, Hengshuang Zhao, Jindong Gu, Philip Torr |  |
| 616 |  |  [PF-LRM: Pose-Free Large Reconstruction Model for Joint Pose and Shape Prediction](https://openreview.net/forum?id=noe76eRcPC) |  | 0 | We propose a Pose-Free Large Reconstruction Model (PF-LRM) for reconstructing a 3D object from a few unposed images even with little visual overlap, while simultaneously estimating the relative camera poses in ~1.3 seconds on a single A100 GPU. PF-LRM is a highly scalable method utilizing... | Fujun Luan, Hao Tan, Kai Zhang, Kalyan Sunkavalli, Peng Wang, Sai Bi, Wenping Wang, Yinghao Xu, Zexiang Xu |  |
| 617 |  |  [Procedural Fairness Through Decoupling Objectionable Data Generating Components](https://openreview.net/forum?id=cxfPefbu1s) |  | 0 | We reveal and address the frequently overlooked yet important issue of _disguised procedural unfairness_, namely, the potentially inadvertent alterations on the behavior of neutral (i.e., not problematic) aspects of data generating process, and/or the lack of procedural assurance of the greatest... | Jialu Wang, Kun Zhang, Peter Spirtes, Yang Liu, Zeyu Tang |  |
| 618 |  |  [Vision-Language Foundation Models as Effective Robot Imitators](https://openreview.net/forum?id=lFYj0oibGR) |  | 0 | Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on... | Chilam Cheang, Cunjun Yu, Hanbo Zhang, Hang Li, Hongtao Wu, Huaping Liu, Jie Xu, Minghuan Liu, Tao Kong, Weinan Zhang, Xinghang Li, Ya Jing |  |
| 619 |  |  [OctoPack: Instruction Tuning Code Large Language Models](https://openreview.net/forum?id=mw1PWNSWZP) |  | 0 | Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of... | Armel Randy Zebaze, Binyuan Hui, Leandro von Werra, Niklas Muennighoff, Qian Liu, Qinkai Zheng, Shayne Longpre, Swayam Singh, Terry Yue Zhuo, Xiangru Tang |  |
| 620 |  |  [Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision](https://openreview.net/forum?id=0V5TVt9bk0) |  | 0 | The rapid evolution of Multi-modality Large Language Models (MLLMs) has catalyzed a shift in computer vision from specialized models to general-purpose foundation models. Nevertheless, there is still an inadequacy in assessing the abilities of MLLMs on \*\*low-level visual perception and... | Annan Wang, Chaofeng Chen, Chunyi Li, Erli Zhang, Guangtao Zhai, Haoning Wu, Liang Liao, Qiong Yan, Weisi Lin, Wenxiu Sun, Zicheng Zhang |  |
| 621 |  |  [iTransformer: Inverted Transformers Are Effective for Time Series Forecasting](https://openreview.net/forum?id=JePfAI8fah) |  | 0 | The recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters. These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each token formed by multiple variates of... | Haixu Wu, Haoran Zhang, Lintao Ma, Mingsheng Long, Shiyu Wang, Tengge Hu, Yong Liu |  |
| 622 |  |  [De novo Protein Design Using Geometric Vector Field Networks](https://openreview.net/forum?id=9UIGyJJpay) |  | 0 | Advances like protein diffusion have marked revolutionary progress in $\textit{de novo}$ protein design, a central topic in life science. These methods typically depend on protein structure encoders to model residue backbone frames, where atoms do not exist. Most prior encoders rely on atom-wise... | Chunhua Shen, Hao Chen, Lin Yuanbo Wu, Muzhi Zhu, Shuaike Shen, Weian Mao, Zheng Sun |  |
| 623 |  |  [Prompt Gradient Projection for Continual Learning](https://openreview.net/forum?id=EH2O3h7sBI) |  | 0 | Prompt-tuning has demonstrated impressive performance in continual learning by querying relevant prompts for each input instance, which can avoid the introduction of task identifier. Its forgetting is therefore reduced as this instance-wise query mechanism enables us to select and update only... | Chengwei Chen, Jingyang Qiao, Xin Tan, Yanyun Qu, Yong Peng, Yuan Xie, Zhizhong Zhang |  |
| 624 |  |  [R-EDL: Relaxing Nonessential Settings of Evidential Deep Learning](https://openreview.net/forum?id=Si3YFA641c) |  | 0 | A newly-arising uncertainty estimation method named Evidential Deep Learning (EDL), which can obtain reliable predictive uncertainty in a single forward pass, has garnered increasing interest. Guided by the subjective logic theory, EDL obtains Dirichlet concentration parameters from deep neural... | Changsheng Xu, Junyu Gao, Mengyuan Chen |  |
| 625 |  |  [Neuron Activation Coverage: Rethinking Out-of-distribution Detection and Generalization](https://openreview.net/forum?id=SNGXbZtK6Q) |  | 0 | The out-of-distribution (OOD) problem generally arises when neural networks encounter data that significantly deviates from the training data distribution, i.e., in-distribution (InD). In this paper, we study the OOD problem from a neuron activation view. We first formulate neuron activation states... | Chris Xing Tian, Haoliang Li, Lei Ma, Shiqi Wang, Yibing Liu |  |
| 626 |  |  [ResFields: Residual Neural Fields for Spatiotemporal Signals](https://openreview.net/forum?id=EHrvRNs2Y0) |  | 0 | Neural fields, a category of neural networks trained to represent high-frequency signals, have gained significant attention in recent years due to their impressive performance in modeling complex 3D data, such as signed distance (SDFs) or radiance fields (NeRFs), via a single multi-layer perceptron... | Marc Pollefeys, Marko Mihajlovic, Sergey Prokudin, Siyu Tang |  |
| 627 |  |  [TD-MPC2: Scalable, Robust World Models for Continuous Control](https://openreview.net/forum?id=Oxh5CstDJU) |  | 0 | TD-MPC is a model-based reinforcement learning (RL) algorithm that performs local trajectory optimization in the latent space of a learned implicit (decoder-free) world model. In this work, we present TD-MPC2: a series of improvements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves... | Hao Su, Nicklas Hansen, Xiaolong Wang |  |
| 628 |  |  [Stochastic Controlled Averaging for Federated Learning with Communication Compression](https://openreview.net/forum?id=jj5ZjZsWJe) |  | 0 | Communication compression has been an important topic in Federated Learning (FL) for alleviating the communication overhead. However, communication compression brings forth new challenges in FL due to the interplay of compression-incurred information distortion and inherent characteristics of FL... | Ping Li, Xiaoyun Li, Xinmeng Huang |  |
| 629 |  |  [AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning](https://openreview.net/forum?id=Fx2SbBgcte) |  | 0 | With the advance of text-to-image (T2I) diffusion models (e.g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost. However, adding motion dynamics to existing... | Anyi Rao, Bo Dai, Ceyuan Yang, Dahua Lin, Maneesh Agrawala, Yaohui Wang, Yu Qiao, Yuwei Guo, Zhengyang Liang |  |
| 630 |  |  [Guiding Instruction-based Image Editing via Multimodal Large Language Models](https://openreview.net/forum?id=S1RKWSyZ2Y) |  | 0 | Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models... | TsuJui Fu, Wenze Hu, William Yang Wang, Xianzhi Du, Yinfei Yang, Zhe Gan |  |
| 631 |  |  [Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning](https://openreview.net/forum?id=YR3ETaElNK) |  | 0 | This paper introduces an efficient strategy to transform Large Language Models (LLMs) into Multi-Modal Large Language Models. By conceptualizing this transformation as a domain adaptation process, \ie, transitioning from text understanding to embracing multiple modalities, we intriguingly note... | Bingchen Zhao, Chen Wei, Cihang Xie, Haoqin Tu, Jieru Mei |  |
| 632 |  |  [Universal Humanoid Motion Representations for Physics-Based Control](https://openreview.net/forum?id=OrOd8PxOO2) |  | 0 | We present a universal motion representation that encompasses a comprehensive range of motor skills for physics-based humanoid control. Due to the high dimensionality of humanoids and the inherent difficulties in reinforcement learning, prior methods have focused on learning skill embeddings for a... | Alexander Winkler, Jing Huang, Jinkun Cao, Josh Merel, Kris M. Kitani, Weipeng Xu, Zhengyi Luo |  |
| 633 |  |  [Adaptive Rational Activations to Boost Deep Reinforcement Learning](https://openreview.net/forum?id=g90ysX1sVs) |  | 0 | Latest insights from biology show that intelligence not only emerges from the connections between neurons, but that individual neurons shoulder more computational responsibility than previously anticipated. Specifically, neural plasticity should be critical in the context of constantly changing... | Alejandro Molina, Kristian Kersting, Martin Mundt, Patrick Schramowski, Quentin Delfosse |  |
| 634 |  |  [Learning No-Regret Sparse Generalized Linear Models with Varying Observation(s)](https://openreview.net/forum?id=wISvONp3Kq) |  | 0 | Generalized Linear Models (GLMs) encompass a wide array of regression and classification models, where prediction is a function of a linear combination of the input variables. Often in real-world scenarios, a number of observations would be added into or removed from the existing training dataset,... | Bin Gu, Charles Ling, Diyang Li, Huan Xiong, Zhiqiang Xu |  |
| 635 |  |  [Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game](https://openreview.net/forum?id=fsW7wJGLBd) |  | 0 | While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to \*prompt injection attacks\*: malicious third party prompts that subvert the intent of the system designer. To help researchers study this problem, we present a dataset of over... | Alan Ritter, Ethan Adrian Mendes, Isaac Ong, Justin Svegliato, Karim Elmaaroufi, Luke Bailey, Olivia Watkins, Pieter Abbeel, Sam Toyer, Stuart Russell, Tiffany Wang, Trevor Darrell |  |
| 636 |  |  [Leveraging Unpaired Data for Vision-Language Generative Models via Cycle Consistency](https://openreview.net/forum?id=kNjrhD67LP) |  | 0 | Current vision-language generative models rely on expansive corpora of $\textit{paired}$ image-text data to attain optimal performance and generalization capabilities. However, automatically collecting such data (e.g. via large-scale web scraping) leads to low quality and poor image-text... | Dilip Krishnan, Dina Katabi, Guillaume Lajoie, Han Zhang, Huiwen Chang, Jarred Barber, Sangnie Bhardwaj, Tianhong Li, Yonglong Tian |  |
| 637 |  |  [Learning the greatest common divisor: explaining transformer predictions](https://openreview.net/forum?id=cmcD05NPKa) |  | 0 | The predictions of small transformers, trained to calculate the greatest common divisor (GCD) of two positive integers, can be fully characterized by looking at model inputs and outputs. As training proceeds, the model learns a list $\mathcal D$ of integers, products of divisors of the base used to... | François Charton |  |
| 638 |  |  [Space and time continuous physics simulation from partial observations](https://openreview.net/forum?id=4yaFQ7181M) |  | 0 | Modern techniques for physical simulations rely on numerical schemes and mesh-refinement methods to address trade-offs between precision and complexity, but these handcrafted solutions are tedious and require high computational power. Data-driven methods based on large-scale machine learning... | Christian Wolf, Julie Digne, Madiha Nadri, Steeven Janny |  |
| 639 |  |  [GROOT: Learning to Follow Instructions by Watching Gameplay Videos](https://openreview.net/forum?id=uleDLeiaT3) |  | 0 | We study the problem of building a controller that can follow open-ended instructions in open-world environments. We propose to follow reference videos as instructions, which offer expressive goal specifications while eliminating the need for expensive text-gameplay annotations. A new learning... | Anji Liu, Bowei Zhang, Shaofei Cai, Xiaojian Ma, Yitao Liang, Zihao Wang |  |
| 640 |  |  [Mask-Based Modeling for Neural Radiance Fields](https://openreview.net/forum?id=SEiuSzlD1d) |  | 0 | Most Neural Radiance Fields (NeRFs) exhibit limited generalization capabilities,which restrict their applicability in representing multiple scenes using a single model. To address this problem, existing generalizable NeRF methods simply condition the model on image features. These methods still... | Dong Liu, Ganlin Yang, Guoqiang Wei, Yan Lu, Zhizheng Zhang |  |
| 641 |  |  [Large Language Models Are Not Robust Multiple Choice Selectors](https://openreview.net/forum?id=shr9PXz7T0) |  | 0 | Multiple choice questions (MCQs) serve as a common yet important task format in the evaluation of large language models (LLMs). This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent “selection bias”, namely, they prefer to select specific option... | Chujie Zheng, Fandong Meng, Hao Zhou, Jie Zhou, Minlie Huang |  |
| 642 |  |  [Robust Adversarial Reinforcement Learning via Bounded Rationality Curricula](https://openreview.net/forum?id=pFOoOdaiue) |  | 0 | Robustness against adversarial attacks and distribution shifts is a long-standing goal of Reinforcement Learning (RL). To this end, Robust Adversarial Reinforcement Learning (RARL) trains a protagonist against destabilizing forces exercised by an adversary in a competitive zero-sum Markov game,... | Aryaman Reddi, Carlo D'Eramo, Georgia Chalvatzaki, Jan Peters, Maximilian Tölle |  |
| 643 |  |  [Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions](https://openreview.net/forum?id=BXY6fe7q31) |  | 0 | Recent advancements in Multimodal Large Language Models (MLLMs) have been utilizing Visual Prompt Generators (VPGs) to convert visual features into tokens that LLMs can recognize. This is achieved by training the VPGs on millions of image-caption pairs, where the VPG-generated tokens of images are... | Hanwang Zhang, Juncheng Li, Kaihang Pan, Minghe Gao, Siliang Tang, TatSeng Chua, Wei Ji, Wenqiao Zhang, Yueting Zhuang, Zhiqi Ge |  |
| 644 |  |  [CLAP: Collaborative Adaptation for Patchwork Learning](https://openreview.net/forum?id=8EyRkd3Qj2) |  | 0 | In this paper, we investigate a new practical learning scenario, where the data distributed in different sources/clients are typically generated with various modalities. Existing research on learning from multi-source data mostly assume that each client owns the data of all modalities, which may... | Abudukelimu Wuerkaixi, Changshui Zhang, Fei Wang, Jian Liang, Lei Fang, Sen Cui, Weishen Pan |  |
| 645 |  |  [Towards Energy Efficient Spiking Neural Networks: An Unstructured Pruning Framework](https://openreview.net/forum?id=eoSeaK4QJo) |  | 0 | Spiking Neural Networks (SNNs) have emerged as energy-efficient alternatives to Artificial Neural Networks (ANNs) when deployed on neuromorphic chips. While recent studies have demonstrated the impressive performance of deep SNNs on challenging tasks, their energy efficiency advantage has been... | Jianhao Ding, Xinyu Shi, Zecheng Hao, Zhaofei Yu |  |
| 646 |  |  [Online Stabilization of Spiking Neural Networks](https://openreview.net/forum?id=CIj1CVbkpr) |  | 0 | Spiking neural networks (SNNs), attributed to the binary, event-driven nature of spikes, possess heightened biological plausibility and enhanced energy efficiency on neuromorphic hardware compared to analog neural networks (ANNs). Mainstream SNN training schemes apply backpropagation-through-time... | Jianhao Ding, Tiejun Huang, Xiaodong Xie, Yaoyu Zhu, Zhaofei Yu |  |
| 647 |  |  [CrIBo: Self-Supervised Learning via Cross-Image Object-Level Bootstrapping](https://openreview.net/forum?id=3M0GXoUEzP) |  | 0 | Leveraging nearest neighbor retrieval for self-supervised representation learning has proven beneficial with object-centric images. However, this approach faces limitations when applied to scene-centric datasets, where multiple objects within an image are only implicitly captured in the global... | Behzad Bozorgtabar, JeanPhilippe Thiran, Thomas Stegmüller, Tim Lebailly, Tinne Tuytelaars |  |
| 648 |  |  [Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis](https://openreview.net/forum?id=7ERQPyR2eb) |  | 0 | One-shot 3D talking portrait generation aims to reconstruct a 3D avatar from an unseen image, and then animate it with a reference video or audio to generate a talking portrait video. The existing methods fail to simultaneously achieve the goals of accurate 3D avatar reconstruction and stable... | Chen Zhang, Jiaqi Yang, Jiawei Huang, Jinglin Liu, Jinzheng He, Rongjie Huang, Tianyun Zhong, Weichuang Li, Xiang Yin, Yi Ren, Zejun Ma, Zhenhui Ye, Zhou Zhao, Ziyue Jiang |  |
| 649 |  |  [TabR: Tabular Deep Learning Meets Nearest Neighbors](https://openreview.net/forum?id=rhgIgTSSxW) |  | 0 | Deep learning (DL) models for tabular data problems (e.g. classification, regression) are currently receiving increasingly more attention from researchers. However, despite the recent efforts, the non-DL algorithms based on gradient-boosted decision trees (GBDT) remain a strong go-to solution for... | Akim Kotelnikov, Artem Babenko, Daniil Shlenskii, Ivan Rubachev, Nikolay Kartashev, Yury Gorishniy |  |
| 650 |  |  [Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models](https://openreview.net/forum?id=qBL04XXex6) |  | 0 | The reasoning performance of Large Language Models (LLMs) on a wide range of problems critically relies on chain-of-thought prompting, which involves providing a few chain of thought demonstrations as exemplars in prompts. Recent work, e.g., Tree of Thoughts, has pointed out the importance of... | Baochun Li, Di Niu, Sijia Chen |  |
| 651 |  |  [Locality Sensitive Sparse Encoding for Learning World Models Online](https://openreview.net/forum?id=i8PjQT3Uig) |  | 0 | Acquiring an accurate world model $\textit{online}$ for model-based reinforcement learning (MBRL) is challenging due to data nonstationarity, which typically causes catastrophic forgetting for neural networks (NNs). From the online learning perspective, a Follow-The-Leader (FTL) world model is... | Chao Du, Min Lin, Wee Sun Lee, Zichen Liu |  |
| 652 |  |  [Enhancing Neural Subset Selection: Integrating Background Information into Set Representations](https://openreview.net/forum?id=eepoE7iLpL) |  | 0 | Learning neural subset selection tasks, such as compound selection in AI-aided drug discovery, have become increasingly pivotal across diverse applications. The existing methodologies in the field primarily concentrate on constructing models that capture the relationship between utility function... | Binghui Xie, Bo Han, James Cheng, Kaiwen Zhou, Peilin Zhao, Wei Meng, Yatao Bian, Yongqiang Chen |  |
| 653 |  |  [Bridging Vision and Language Spaces with Assignment Prediction](https://openreview.net/forum?id=lK2V2E2MNv) |  | 0 | This paper introduces VLAP, a novel approach that bridges pretrained vision models and large language models (LLMs) to make frozen LLMs understand the visual world. VLAP transforms the embedding space of pretrained vision models into the LLMs' word embedding space using a single linear layer for... | Jiyoung Lee, Jungin Park, Kwanghoon Sohn |  |
| 654 |  |  [Generative Judge for Evaluating Alignment](https://openreview.net/forum?id=gtkFw6sZGS) |  | 0 | The rapid development of Large Language Models (LLMs) has substantially expanded the range of tasks they can address. In the field of Natural Language Processing (NLP), researchers have shifted their focus from conventional NLP tasks (e.g., sequence tagging and parsing) towards tasks that revolve... | Hai Zhao, Junlong Li, Pengfei Liu, RunZe Fan, Shichao Sun, Weizhe Yuan |  |
| 655 |  |  [Rethinking and Extending the Probabilistic Inference Capacity of GNNs](https://openreview.net/forum?id=7vVWiCrFnd) |  | 0 | Designing expressive Graph Neural Networks (GNNs) is an important topic in graph machine learning fields. Despite the existence of numerous approaches proposed to enhance GNNs based on Weisfeiler-Lehman (WL) tests, what GNNs can and cannot learn still lacks a deeper understanding. This paper adopts... | Lei Zou, Tuo Xu |  |
| 656 |  |  [Learning model uncertainty as variance-minimizing instance weights](https://openreview.net/forum?id=bDWXhzZT40) |  | 0 | Predictive uncertainty--a model’s self-awareness regarding its accuracy on an input--is key for both building robust models via training interventions and for test-time applications such as selective classification. We propose a novel instance-conditional reweighting approach that captures... | Karthikeyan Shanmugam, Nishant Jain, Pradeep Shenoy |  |
| 657 |  |  [Forward Learning with Top-Down Feedback: Empirical and Analytical Characterization](https://openreview.net/forum?id=My7lkRNnL9) |  | 0 | "Forward-only" algorithms, which train neural networks while avoiding a backward pass, have recently gained attention as a way of solving the biologically unrealistic aspects of backpropagation. Here, we first address compelling challenges related to the "forward-only" rules, which include reducing... | Avi Cooper, Francesca Mignacco, Gabriel Kreiman, Giorgia Dellaferrera, Maria Refinetti, Martino Sorbaro, Ravi Francesco Srinivasan |  |
| 658 |  |  [Towards Cross Domain Generalization of Hamiltonian Representation via Meta Learning](https://openreview.net/forum?id=AZGIwqCyYY) |  | 0 | Recent advances in deep learning for physics have focused on discovering shared representations of target systems by incorporating physics priors or inductive biases into neural networks. While effective, these methods are limited to the system domain, where the type of system remains consistent... | Hawoong Jeong, Yeongwoo Song |  |
| 659 |  |  [What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning](https://openreview.net/forum?id=BTKAeLqLMw) |  | 0 | Instruction tuning is a standard technique employed to align large language models to end tasks and user preferences after the initial pretraining phase. Recent research indicates the critical role of data engineering in instruction tuning -- when appropriately selected, only limited data is... | Junxian He, Keqing He, Wei Liu, Weihao Zeng, Yong Jiang |  |
| 660 |  |  [Causality-Inspired Spatial-Temporal Explanations for Dynamic Graph Neural Networks](https://openreview.net/forum?id=AJBkfwXh3u) |  | 0 | Dynamic Graph Neural Networks (DyGNNs) have gained significant popularity in the research of dynamic graphs, but are limited by the low transparency, such that human-understandable insights can hardly be drawn from their predictions. Although a number of existing research have been devoted to... | Kesen Zhao, Liang Zhang |  |
| 661 |  |  [Dissecting learning and forgetting in language model finetuning](https://openreview.net/forum?id=tmsqb6WpLz) |  | 0 | Finetuning language models on domain-specific corpus is a common approach to enhance their domain knowledge and capability. While improving performance on domain tasks, it often brings a side-effect of forgetting of the model's general abilities. In this study, we analyze the effects of finetuning... | Ji Wu, Xiao Zhang |  |
| 662 |  |  [Test-time Adaptation against Multi-modal Reliability Bias](https://openreview.net/forum?id=TPZRq4FALB) |  | 0 | Test-time adaptation (TTA) has emerged as a new paradigm for reconciling distribution shifts across domains without accessing source data. However, existing TTA methods mainly concentrate on uni-modal tasks, overlooking the complexity of multi-modal scenarios. In this paper, we delve into the... | Changqing Zhang, Mouxing Yang, Peng Hu, Xi Peng, Yunfan Li |  |
| 663 |  |  [Mirage: Model-agnostic Graph Distillation for Graph Classification](https://openreview.net/forum?id=78iGZdqxYY) |  | 0 | GNNs, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of GNNs on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training... | Hariprasad Kodamana, Mridul Gupta, Sahil Manchanda, Sayan Ranu |  |
| 664 |  |  [On the Learnability of Watermarks for Language Models](https://openreview.net/forum?id=9k0krNzvlV) |  | 0 | Watermarking of language model outputs enables statistical detection of model-generated text, which can mitigate harms and misuses of language models. Existing watermarking strategies operate by altering the decoder of an existing language model. In this paper, we ask whether language models can... | Chenchen Gu, Percy Liang, Tatsunori Hashimoto, Xiang Lisa Li |  |
| 665 |  |  [Bellman Optimal Stepsize Straightening of Flow-Matching Models](https://openreview.net/forum?id=Iyve2ycvGZ) |  | 0 | Flow matching is a powerful framework for generating high-quality samples in various applications, especially image synthesis. However, the intensive computational demands of these models, especially during the finetuning process and sampling processes, pose significant challenges for low-resource... | Bao Nguyen, Binh Nguyen, Viet Anh Nguyen |  |
| 666 |  |  [Enhancing Tail Performance in Extreme Classifiers by Label Variance Reduction](https://openreview.net/forum?id=6ARlSgun7J) |  | 0 | Extreme Classification (XC) architectures, which utilize a massive One-vs-All (OvA) classifier layer at the output, have demonstrated remarkable performance on problems with large label sets. Nonetheless, these architectures falter on tail labels with few representative samples. This phenomenon has... | Anirudh Buvanesh, Bhawna Paliwal, Deepesh Hada, Jatin Prakash, Manik Varma, Manish Gupta, Mudit Dhawan, Neelabh Madan, Rahul Chand, Ramachandran Ramjee, Sonu Mehta, Vidit Jain, Yashoteja Prabhu |  |
| 667 |  |  [Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching](https://openreview.net/forum?id=Ebt7JgMHv1) |  | 0 | Mechanistic interpretability aims to attribute high-level model behaviors to specific, interpretable learned features. It is hypothesized that these features manifest as directions or low-dimensional subspaces within activation space. Accordingly, recent studies have explored the identification and... | Aleksandar Makelov, Atticus Geiger, Georg Lange, Neel Nanda |  |
| 668 |  |  [Demonstration-Regularized RL](https://openreview.net/forum?id=lF2aip4Scn) |  | 0 | Incorporating expert demonstrations has empirically helped to improve the sample efficiency of reinforcement learning (RL). This paper quantifies theoretically to what extent this extra information reduces RL's sample complexity. In particular, we study the demonstration-regularized reinforcement... | Alexey Naumov, Daniele Calandriello, Daniil Tiapkin, Denis Belomestny, Eric Moulines, Michal Valko, Pierre Ménard, Pierre Perrault |  |
| 669 |  |  [Multilingual Jailbreak Challenges in Large Language Models](https://openreview.net/forum?id=vESNKdEMGp) |  | 0 | While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the \`\`jailbreak'' problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior. Although several preventive measures have... | Lidong Bing, Sinno Jialin Pan, Wenxuan Zhang, Yue Deng |  |
| 670 |  |  [$t^3$-Variational Autoencoder: Learning Heavy-tailed Data with Student's t and Power Divergence](https://openreview.net/forum?id=RzNlECeoOB) |  | 0 | The variational autoencoder (VAE) typically employs a standard normal prior as a regularizer for the probabilistic latent encoder. However, the Gaussian tail often decays too quickly to effectively accommodate the encoded points, failing to preserve crucial structures hidden in the data. In this... | Hyunjong Lee, Jaehyuk Kwon, JoongHo Won, Juno Kim, Mincheol Cho |  |
| 671 |  |  [Rethinking the Power of Graph Canonization in Graph Representation Learning with Stability](https://openreview.net/forum?id=nTwb2vBLOV) |  | 0 | The expressivity of Graph Neural Networks (GNNs) has been studied broadly in recent years to reveal the design principles for more powerful GNNs. Graph canonization is known as a typical approach to distinguish non-isomorphic graphs, yet rarely adopted when developing expressive GNNs. This paper... | Carlos Cruchaga, Fuhai Li, Michael A. Province, Muhan Zhang, Philip R. O. Payne, Tianyu Zhao, Yixin Chen, Zehao Dong |  |
| 672 |  |  [Gradual Optimization Learning for Conformational Energy Minimization](https://openreview.net/forum?id=FMMF1a9ifL) |  | 0 | Molecular conformation optimization is crucial to computer-aided drug discovery and materials design. Traditional energy minimization techniques rely on iterative optimization methods that use molecular forces calculated by a physical simulator (oracle) as anti-gradients. However, this is a... | Aleksandr Panov, Alexander Telepov, Alexey Skrynnik, Artem Tsypin, Artur Kadurin, Dmitry P. Vetrov, Egor Rumiantsev, Elena Tutubalina, Kuzma Khrabrov, Leonid Ugadiarov |  |
| 673 |  |  [AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection](https://openreview.net/forum?id=buC4E91xZE) |  | 0 | Zero-shot anomaly detection (ZSAD) requires detection models trained using auxiliary data to detect anomalies without any training sample in a target dataset. It is a crucial task when training data is not accessible due to various concerns, e.g., data privacy, yet it is challenging since the... | Guansong Pang, Jiming Chen, Qihang Zhou, Shibo He, Yu Tian |  |
| 674 |  |  [Towards Faithful Explanations: Boosting Rationalization with Shortcuts Discovery](https://openreview.net/forum?id=uGtfk2OphU) |  | 0 | The remarkable success in neural networks provokes the selective rationalization. It explains the prediction results by identifying a small subset of the inputs sufficient to support them. Since existing methods still suffer from adopting the shortcuts in data to compose rationales and limited... | Li Wang, Linan Yue, Qi Liu, Weibo Gao, Yanqing An, Yichao Du |  |
| 675 |  |  [CORN: Contact-based Object Representation for Nonprehensile Manipulation of General Unseen Objects](https://openreview.net/forum?id=KTtEICH4TO) |  | 0 | Nonprehensile manipulation is essential for manipulating objects that are too thin, large, or otherwise ungraspable in the wild. To sidestep the difficulty of contact modeling in conventional modeling-based approaches, reinforcement learning (RL) has recently emerged as a promising alternative.... | Beomjoon Kim, Junhyek Han, Yoontae Cho, Yoonyoung Cho |  |
| 676 |  |  [An Intuitive Multi-Frequency Feature Representation for SO(3)-Equivariant Networks](https://openreview.net/forum?id=5JWAOLBxwp) |  | 0 | The usage of 3D vision algorithms, such as shape reconstruction, remains limited because they require inputs to be at a fixed canonical rotation. Recently, a simple equivariant network, Vector Neuron (VN) has been proposed that can be easily used with the state-of-the-art 3D neural network (NN)... | Beomjoon Kim, Dongwon Son, Jaehyung Kim, Sanghyeon Son |  |
| 677 |  |  [KoLA: Carefully Benchmarking World Knowledge of Large Language Models](https://openreview.net/forum?id=AqN23oqraW) |  | 0 | The unprecedented performance of large language models (LLMs) necessitates improvements in evaluations. Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of... | Amy Xin, Bin Xu, Chunyang Li, Daniel ZhangLi, Hailong Jin, Hanming Li, Hao Peng, Ji Qi, Jianhui Chen, Jie Tang, Jifan Yu, Jinxin Liu, Juanzi Li, Kaifeng Yun, Kaisheng Zeng, Lei Hou, Linlu Gong, Nianyi Lin, Ning Ding, Shangqing Tu, Shulin Cao, Weikai Li, Xiaohan Zhang, Xiaozhi Wang, Xin Lv, Yantao Liu, Yong Guan, Yu Gu, Yuan Yao, Yunjia Qi, Yushi Bai, Zheyuan Zhang, Zhili Wu, Zhiyuan Liu, Zijun Yao |  |
| 678 |  |  [Graph Parsing Networks](https://openreview.net/forum?id=hv3SklibkL) |  | 0 | Graph pooling compresses graph information into a compact representation. State-of-the-art graph pooling methods follow a hierarchical approach, which reduces the graph size step-by-step. These methods must balance memory efficiency with preserving node information, depending on whether they use... | Chenghu Zhou, Siyuan Huang, Xinbing Wang, Yunchong Song, Zhouhan Lin |  |
| 679 |  |  [TESTAM: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of Experts](https://openreview.net/forum?id=N0nTk5BSvO) |  | 0 | Accurate traffic forecasting is challenging due to the complex dependency on road networks, various types of roads, and the abrupt speed change due to the events. Recent works mainly focus on dynamic spatial modeling with adaptive graph embedding or graph attention having less consideration for... | Hyunwook Lee, Sungahn Ko |  |
| 680 |  |  [Learning From Simplicial Data Based on Random Walks and 1D Convolutions](https://openreview.net/forum?id=OsGUnYOzii) |  | 0 | Triggered by limitations of graph-based deep learning methods in terms of computational expressivity and model flexibility, recent years have seen a surge of interest in computational models that operate on higher-order topological domains such as hypergraphs and simplicial complexes. While the... | Florian Frantzen, Michael T. Schaub |  |
| 681 |  |  [LUM-ViT: Learnable Under-sampling Mask Vision Transformer for Bandwidth Limited Optical Signal Acquisition](https://openreview.net/forum?id=wkbeqr5XhC) |  | 0 | Bandwidth constraints during signal acquisition frequently impede real-time detection applications. Hyperspectral data is a notable example, whose vast volume compromises real-time hyperspectral detection. To tackle this hurdle, we introduce a novel approach leveraging pre-acquisition modulation to... | Dong Ni, Hangjie Yuan, Lingfeng Liu |  |
| 682 |  |  [Social-Transmotion: Promptable Human Trajectory Prediction](https://openreview.net/forum?id=SQpnEfv9WH) |  | 0 | Accurate human trajectory prediction is crucial for applications such as autonomous vehicles, robotics, and surveillance systems. Yet, existing models often fail to fully leverage the non-verbal social cues human subconsciously communicate when navigating the space. To address this, we introduce... | Alexandre Alahi, Kaouther Messaoud, Saeed Saadatnejad, Yang Gao |  |
| 683 |  |  [Robust Classification via Regression for Learning with Noisy Labels](https://openreview.net/forum?id=wfgZc3IMqo) |  | 0 | Deep neural networks and large-scale datasets have revolutionized the field of machine learning. However, these large networks are susceptible to overfitting to label noise, resulting in reduced generalization. To address this challenge, two promising approaches have emerged: i) loss reweighting,... | Erik Englesson, Hossein Azizpour |  |
| 684 |  |  [Learning to Reject with a Fixed Predictor: Application to Decontextualization](https://openreview.net/forum?id=dCHbFDsCZz) |  | 0 | We study the problem of classification with a reject option for a fixed predictor, crucial to natural language processing. We introduce a new problem formulation for this scenario, and an algorithm minimizing a new surrogate loss function. We provide a complete theoretical analysis of the surrogate... | Anqi Mao, Christopher Mohri, Daniel Andor, Eunsol Choi, Michael Collins, Yutao Zhong |  |
| 685 |  |  [Dynamics-Informed Protein Design with Structure Conditioning](https://openreview.net/forum?id=jZPqf2G9Sw) |  | 0 | Current protein generative models are able to design novel backbones with desired shapes or functional motifs. However, despite the importance of a protein’s dynamical properties for its function, conditioning on dynamical properties remains elusive. We present a new approach to protein generative... | Francisco Vargas, Kieran Didi, Mateja Jamnik, Pietro Lio, Simon V. Mathis, Urszula Julia Komorowska |  |
| 686 |  |  [Partitioning Message Passing for Graph Fraud Detection](https://openreview.net/forum?id=tEgrUrUuwA) |  | 0 | Label imbalance and homophily-heterophily mixture are the fundamental problems encountered when applying Graph Neural Networks (GNNs) to Graph Fraud Detection (GFD) tasks. Existing GNN-based GFD models are designed to augment graph structure to accommodate the inductive bias of GNNs towards... | Bingsheng He, Bryan Hooi, Guang Tan, Jia Chen, Rizal Fathony, Wei Zhuo, Zemin Liu |  |
| 687 |  |  [Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation](https://openreview.net/forum?id=EmQSOi1X2f) |  | 0 | Large language models (large LMs) are susceptible to producing text that contains hallucinated content. An important instance of this problem is self-contradiction, where the LM generates two contradictory sentences within the same context. In this work, we present a comprehensive investigation... | Jingxuan He, Martin T. Vechev, Niels Mündler, Slobodan Jenko |  |
| 688 |  |  [Symmetric Neural-Collapse Representations with Supervised Contrastive Loss: The Impact of ReLU and Batching](https://openreview.net/forum?id=AyXIDfvYg8) |  | 0 | Supervised contrastive loss (SCL) is a competitive and often superior alternative to the cross-entropy loss for classification. While prior studies have demonstrated that both losses yield symmetric training representations under balanced data, this symmetry breaks under class imbalances. This... | Christos Thrampoulidis, Ganesh Ramachandra Kini, Jaidev Gill, Tina Behnia, Vala Vakilian |  |
| 689 |  |  [Manipulating dropout reveals an optimal balance of efficiency and robustness in biological and machine visual systems](https://openreview.net/forum?id=ADDCErFzev) |  | 0 | According to the efficient coding hypothesis, neural populations encode information optimally when representations are high-dimensional and uncorrelated. However, such codes may carry a cost in terms of generalization and robustness. Past empirical studies of early visual cortex (V1) in rodents... | Gabriel Fajardo, George A. Alvarez, Jacob S. Prince, Talia Konkle |  |
| 690 |  |  [DDMI: Domain-agnostic Latent Diffusion Models for Synthesizing High-Quality Implicit Neural Representations](https://openreview.net/forum?id=327tbF3S65) |  | 0 | Recent studies have introduced a new class of generative models for synthesizing implicit neural representations (INRs) that capture arbitrary continuous signals in various domains. These models opened the door for domain-agnostic generative models, but they often fail to achieve high-quality... | Dogyun Park, Hyunwoo J. Kim, Sihyeon Kim, Sojin Lee |  |
| 691 |  |  [Bayesian Coreset Optimization for Personalized Federated Learning](https://openreview.net/forum?id=uz7d2N2zul) |  | 0 | In a distributed machine learning setting like Federated Learning where there are multiple clients involved which update their individual weights to a single central server, often training on the entire individual client's dataset for each client becomes cumbersome. To address this issue we propose... | Ganesh Ramakrishnan, Prateek Chanda, Shrey Modi |  |
| 692 |  |  [In-context Autoencoder for Context Compression in a Large Language Model](https://openreview.net/forum?id=uREj4ZuGJE) |  | 0 | We propose the In-context Autoencoder (ICAE), leveraging the power of a large language model (LLM) to compress a long context into short compact memory slots that can be directly conditioned on by the LLM for various purposes. ICAE is first pretrained using both autoencoding and language modeling... | Furu Wei, Jing Hu, Lei Wang, SiQing Chen, Tao Ge, Xun Wang |  |
| 693 |  |  [Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning](https://openreview.net/forum?id=J44HfH4JCg) |  | 0 | Despite the promising progress in multi-modal tasks, current large multi-modal models (LMMs) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual... | Fuxiao Liu, Jianfeng Wang, Kevin Lin, Lijuan Wang, Linjie Li, Yaser Yacoob |  |
| 694 |  |  [Multimarginal Generative Modeling with Stochastic Interpolants](https://openreview.net/forum?id=FHqAzWl2wE) |  | 0 | Given a set of $K$ probability densities, we consider the multimarginal generative modeling problem of learning a joint distribution that recovers these densities as marginals. The structure of this joint distribution should identify multi-way correspondences among the prescribed marginals. We... | Eric VandenEijnden, Michael Lindsey, Michael S. Albergo, Nicholas Matthew Boffi |  |
| 695 |  |  [Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators](https://openreview.net/forum?id=JiTVtCUOpS) |  | 0 | Recently, channel-independent methods have achieved state-of-the-art performance in multivariate time series (MTS) forecasting. Despite reducing overfitting risks, these methods miss potential opportunities in utilizing channel dependence for accurate predictions. We argue that there exist locally... | Lifan Zhao, Yanyan Shen |  |
| 696 |  |  [RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval](https://openreview.net/forum?id=GN921JHCRw) |  | 0 | Retrieval-augmented language models can better adapt to changes in world state and incorporate long-tail knowledge. However, most existing methods retrieve only short contiguous chunks from a retrieval corpus, limiting holistic understanding of the overall document context. We introduce the novel... | Aditi Tuli, Anna Goldie, Christopher D. Manning, Parth Sarthi, Salman Abdullah, Shubh Khanna |  |
| 697 |  |  [Fair and Efficient Contribution Valuation for Vertical Federated Learning](https://openreview.net/forum?id=sLQb8q0sUi) |  | 0 | Federated learning is an emerging technology for training machine learning models across decentralized data sources without sharing data. Vertical federated learning, also known as feature-based federated learning, applies to scenarios where data sources have the same sample IDs but different... | Huang Fang, Jian Pei, Michael P. Friedlander, Xinglu Wang, Yong Zhang, Zhenan Fan, Zirui Zhou |  |
| 698 |  |  [In-Context Learning through the Bayesian Prism](https://openreview.net/forum?id=HX5ujdsSon) |  | 0 | In-context learning (ICL) is one of the surprising and useful features of large language models and subject of intense research. Recently, stylized meta-learning-like ICL setups have been devised that train transformers on sequences of input-output pairs $(x, f(x))$. The function $f$ comes from a... | Kabir Ahuja, Madhur Panwar, Navin Goyal |  |
| 699 |  |  [RingAttention with Blockwise Transformers for Near-Infinite Context](https://openreview.net/forum?id=WsRHpHH4s0) |  | 0 | Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in... | Hao Liu, Matei Zaharia, Pieter Abbeel |  |
| 700 |  |  [Chain of Hindsight aligns Language Models with Feedback](https://openreview.net/forum?id=6xfe4IVcOu) |  | 0 | Learning from human preferences is important for language models to match human needs and to align with human and social values. Prior works have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on... | Carmelo Sferrazza, Hao Liu, Pieter Abbeel |  |
| 701 |  |  [GraphChef: Decision-Tree Recipes to Explain Graph Neural Networks](https://openreview.net/forum?id=IjMUGuUmBI) |  | 0 | We propose a new self-explainable Graph Neural Network (GNN) model: GraphChef. GraphChef integrates decision trees into the GNN message passing framework. Given a dataset, GraphChef returns a set of rules (a recipe) that explains each class in the dataset unlike existing GNNs and explanation... | Karolis Martinkus, Lukas Faber, Peter Müller, Roger Wattenhofer |  |
| 702 |  |  [Safe Collaborative Filtering](https://openreview.net/forum?id=yarUvgEXq3) |  | 0 | Excellent tail performance is crucial for modern machine learning tasks, such as algorithmic fairness, class imbalance, and risk-sensitive decision making, as it ensures the effective handling of challenging samples within a dataset. Tail performance is also a vital determinant of success for... | Naoto Ohsaka, Riku Togashi, Tatsushi Oka, Tetsuro Morimura |  |
| 703 |  |  [On Representation Complexity of Model-based and Model-free Reinforcement Learning](https://openreview.net/forum?id=3K3s9qxSn7) |  | 0 | We study the representation complexity of model-based and model-free reinforcement learning (RL) in the context of circuit complexity. We prove theoretically that there exists a broad class of MDPs such that their underlying transition and reward functions can be represented by constant depth... | Baihe Huang, Hanlin Zhu, Stuart Russell |  |
| 704 |  |  [Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning](https://openreview.net/forum?id=sKPzAXoylB) |  | 0 | Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both... | A. Rupam Mahmood, Mohamed Elsayed |  |
| 705 |  |  [A Good Learner can Teach Better: Teacher-Student Collaborative Knowledge Distillation](https://openreview.net/forum?id=Ixi4j6LtdX) |  | 0 | Knowledge distillation (KD) is a technique used to transfer knowledge from a larger ''teacher'' model into a smaller ''student'' model. Recent advancements in meta-learning-based knowledge distillation (MetaKD) emphasize that the fine-tuning of teacher models should be aware of the student's need... | Ayan Sengupta, Md. Shad Akhtar, Shantanu Dixit, Tanmoy Chakraborty |  |
| 706 |  |  [Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making](https://openreview.net/forum?id=k581sTMyPt) |  | 0 | Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial, especially in high-stakes domains like medicine, to establish trust and ensure safety, which requires human engagement. We introduce SUFO, a systematic... | Aliyah R. Hsu, Anobel Y. Odisho, Bin Yu, Briton Park, Tristan Naumann, Yeshwanth Cherapanamjeri |  |
| 707 |  |  [Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks](https://openreview.net/forum?id=A0HKeKl4Nl) |  | 0 | Fine-tuning large pre-trained models has become the de facto strategy for developing both task-specific and general-purpose machine learning systems, including developing models that are safe to deploy. Despite its clear importance, there has been minimal work that explains how fine-tuning alters... | David Scott Krueger, Edward Grefenstette, Ekdeep Singh Lubana, Hidenori Tanaka, Robert Kirk, Robert P. Dick, Samyak Jain, Tim Rocktäschel |  |
| 708 |  |  [RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems](https://openreview.net/forum?id=pPjZIOuQuF) |  | 0 | Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming... | Canwen Xu, Julian J. McAuley, Tianyang Liu |  |
| 709 |  |  [Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective](https://openreview.net/forum?id=mIEHIcHGOo) |  | 0 | Large Language Models (LLMs) inherently encode a wealth of knowledge within their parameters through pre-training on extensive corpora. While prior research has delved into operations on these parameters to manipulate the underlying implicit knowledge — encompassing detection, editing, and merging... | Chenxin An, Jiawei Han, Ming Zhong, Pengcheng He, Weizhu Chen |  |
| 710 |  |  [Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning](https://openreview.net/forum?id=RXFVcynVe1) |  | 0 | Representation learning on text-attributed graphs (TAGs) has become a critical research problem in recent years. A typical example of a TAG is a paper citation graph, where the text of each paper serves as node attributes. Initial graph neural network (GNN) pipelines handled these text attributes... | Adam Perold, Bryan Hooi, Thomas Laurent, Xavier Bresson, Xiaoxin He, Yann LeCun |  |
| 711 |  |  [SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs](https://openreview.net/forum?id=w4DW6qkRmt) |  | 0 | Large language models (LLMs) have made significant advancements in various natural language processing tasks, including question answering (QA) tasks. While incorporating new information with the retrieval of relevant passages is a promising way to improve QA with LLMs, the existing methods often... | Jaehyun Nam, Jaehyung Kim, Jinwoo Shin, Jongjin Park, JungWoo Ha, Minjoon Seo, SangWoo Lee, Sangwoo Mo |  |
| 712 |  |  [Retrieval meets Long Context Large Language Models](https://openreview.net/forum?id=xw5nxFWMlo) |  | 0 | Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can... | Bryan Catanzaro, Chen Zhu, Evelina Bakhturina, Lawrence McAfee, Mohammad Shoeybi, Peng Xu, Sandeep Subramanian, Wei Ping, Xianchao Wu, Zihan Liu |  |
| 713 |  |  [Neural Spectral Methods: Self-supervised learning in the spectral domain](https://openreview.net/forum?id=2DbVeuoa6a) |  | 0 | We present Neural Spectral Methods, a technique to solve parametric Partial Differential Equations (PDEs), grounded in classical spectral methods. Our method uses orthogonal bases to learn PDE solutions as mappings between spectral coefficients, instantiating a spectral-based neural operator. In... | Aditi S. Krishnapriyan, Nithin Chalapathi, Yiheng Du |  |
| 714 |  |  [Kosmos-G: Generating Images in Context with Multimodal Large Language Models](https://openreview.net/forum?id=he6mX9LTyE) |  | 0 | Recent advancements in subject-driven image generation have made significant strides. However, current methods still fall short in diverse application scenarios, as they require test-time tuning and cannot accept interleaved multi-image and text input. These limitations keep them far from the... | Furu Wei, Li Dong, Shaohan Huang, Wenhu Chen, Xichen Pan, Zhiliang Peng |  |
| 715 |  |  [Assessing Uncertainty in Similarity Scoring: Performance & Fairness in Face Recognition](https://openreview.net/forum?id=lAhQCHuANV) |  | 0 | The ROC curve is the major tool for assessing not only the performance but also the fairness properties of a similarity scoring function. In order to draw reliable conclusions based on empirical ROC analysis, accurately evaluating the uncertainty level related to statistical versions of the ROC... | JeanRémy Conti, Stéphan Clémençon |  |
| 716 |  |  [LitCab: Lightweight Language Model Calibration over Short- and Long-form Responses](https://openreview.net/forum?id=jH67LHVOIO) |  | 0 | A model is considered well-calibrated when its probability estimate aligns with the actual likelihood of the output being correct. Calibrating language models (LMs) is crucial, as it plays a vital role in detecting and mitigating hallucinations of LMs as well as building more trustworthy models.... | Lu Wang, Muhammad Khalifa, Xin Liu |  |
| 717 |  |  [Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources](https://openreview.net/forum?id=cPgh4gWZlz) |  | 0 | We present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, CoK consists of three stages:... | Bosheng Ding, Lidong Bing, Ruochen Zhao, Shafiq Joty, Soujanya Poria, Xingxuan Li, Yew Ken Chia |  |
| 718 |  |  [Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning](https://openreview.net/forum?id=m3xVPaZp6Z) |  | 0 | Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on... | Chengxing Jia, Chenxiao Gao, Fuxiang Zhang, Hao Yin, Lei Yuan, Tian Xu, XiongHui Chen, Yang Yu, ZhiHua Zhou, Zongzhang Zhang |  |
| 719 |  |  [Energy-based Automated Model Evaluation](https://openreview.net/forum?id=CHGcP6lVWd) |  | 0 | The conventional evaluation protocols on machine learning models rely heavily on a labeled, i.i.d-assumed testing dataset, which is not often present in real-world applications. The Automated Model Evaluation (AutoEval) shows an alternative to this traditional workflow, by forming a proximal... | Haobo Wang, Heming Zou, Junbo Zhao, Ru Peng, Yawen Zeng, Zenan Huang |  |
| 720 |  |  [Deceptive Fairness Attacks on Graphs via Meta Learning](https://openreview.net/forum?id=iS5ADHNg2A) |  | 0 | We study deceptive fairness attacks on graphs to answer the following question: How can we achieve poisoning attacks on a graph learning model to exacerbate the bias deceptively? We answer this question via a bi-level optimization problem and propose a meta learning-based framework named FATE. FATE... | Hanghang Tong, Jian Kang, Jiebo Luo, Ross Maciejewski, Yinglong Xia |  |
| 721 |  |  [What Matters to You? Towards Visual Representation Alignment for Robot Learning](https://openreview.net/forum?id=CTlUHIKF71) |  | 0 | When operating in service of people, robots need to optimize rewards aligned with end-user preferences. Since robots will rely on raw perceptual inputs, their rewards will inevitably use visual representations. Recently there has been excitement in using representations from pre-trained visual... | Andrea Bajcsy, Chenfeng Xu, Jitendra Malik, Masayoshi Tomizuka, Thomas Tian |  |
| 722 |  |  [FedDA: Faster Adaptive Gradient Methods for Federated Constrained Optimization](https://openreview.net/forum?id=kjn99xFUF3) |  | 0 | Federated learning (FL) is an emerging learning paradigm where a set of distributed clients learns a task under the coordination of a server. The FedAvg algorithm is one of the most widely used methods in FL. In FedAvg, the learning rate is a constant rather than changing adaptively. Adaptive... | Feihu Huang, Heng Huang, Junyi Li |  |
| 723 |  |  [Extending Power of Nature from Binary to Real-Valued Graph Learning in Real World](https://openreview.net/forum?id=qT7DXUmX7j) |  | 0 | Nature performs complex computations constantly at clearly lower cost and higher performance than digital computers. It is crucial to understand how to harness the unique computational power of nature in Machine Learning (ML). In the past decade, besides the development of Neural Networks (NNs),... | Ang Li, Chuan Liu, Chunshu Wu, Michael C. Huang, Ruibing Song, Tong Geng, Yunan Yang |  |
| 724 |  |  [Meta-VBO: Utilizing Prior Tasks in Optimizing Risk Measures with Gaussian Processes](https://openreview.net/forum?id=ElykcDu5YK) |  | 0 | Research on optimizing the risk measure of a blackbox function using Gaussian processes, especially Bayesian optimization (BO) of risk measures, has become increasingly important due to the inevitable presence of uncontrollable variables in real-world applications. Nevertheless, existing works on... | Bryan Kian Hsiang Low, Patrick Jaillet, Quoc Phong Nguyen |  |
| 725 |  |  [Data Debugging with Shapley Importance over Machine Learning Pipelines](https://openreview.net/forum?id=qxGXjWxabq) |  | 0 | When a machine learning (ML) model exhibits poor quality (e.g., poor accuracy or fairness), the problem can often be traced back to errors in the training data. Being able to discover the data examples that are the most likely culprits is a fundamental concern that has received a lot of attention... | Bojan Karlas, Ce Zhang, David Dao, Matteo Interlandi, Sebastian Schelter, Wentao Wu |  |
| 726 |  |  [Pessimistic Nonlinear Least-Squares Value Iteration for Offline Reinforcement Learning](https://openreview.net/forum?id=4kLVvIh8cp) |  | 0 | Offline reinforcement learning (RL), where the agent aims to learn the optimal policy based on the data collected by a behavior policy, has attracted increasing attention in recent years. While offline RL with linear function approximation has been extensively studied with optimal results achieved... | Heyang Zhao, Jiafan He, Qiwei Di, Quanquan Gu |  |
| 727 |  |  [SKILL-MIX: a Flexible and Expandable Family of Evaluations for AI Models](https://openreview.net/forum?id=Jf5gplvglq) |  | 0 | With LLMs shifting their role from statistical modeling of language to serving as general-purpose AI agents, how should LLM evaluations change? Arguably, a key ability of an AI agent is to flexibly combine, as needed, the basic skills it has learned. The capability to combine skills plays an... | Anirudh Goyal, Arushi Gupta, Dingli Yu, Jonah BrownCohen, Sanjeev Arora, Simran Kaur |  |
| 728 |  |  [A Quadratic Synchronization Rule for Distributed Deep Learning](https://openreview.net/forum?id=yroyhkhWS6) |  | 0 | In distributed deep learning with data parallelism, synchronizing gradients at each training step can cause a huge communication overhead, especially when many nodes work together to train large models. Local gradient methods, such as Local SGD, address this issue by allowing workers to compute... | Jingzhao Zhang, Kaifeng Lyu, Longbo Huang, Sanjeev Arora, Xinran Gu |  |
| 729 |  |  [ArchLock: Locking DNN Transferability at the Architecture Level with a Zero-Cost Binary Predictor](https://openreview.net/forum?id=e2YOVTenU9) |  | 0 | Deep neural network (DNN) models, despite their impressive performance, are vulnerable to exploitation by attackers who attempt to transfer them to other tasks for their own benefit. Current defense strategies mainly address this vulnerability at the model parameter level, leaving the potential of... | Shaolei Ren, Tong Zhou, Xiaolin Xu |  |
| 730 |  |  [Leftover Lunch: Advantage-based Offline Reinforcement Learning for Language Models](https://openreview.net/forum?id=ZDGKPbF0VQ) |  | 0 | Reinforcement Learning with Human Feedback (RLHF) is the most prominent method for Language Model (LM) alignment. However, RLHF is an unstable and data-hungry process that continually requires new high-quality LM-generated data for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new... | Ashutosh Baheti, Faeze Brahman, Maarten Sap, Mark O. Riedl, Ronan Le Bras, Ximing Lu |  |
| 731 |  |  [RECOMP: Improving Retrieval-Augmented LMs with Context Compression and Selective Augmentation](https://openreview.net/forum?id=mlJLVigNHp) |  | 0 | Retrieval-augmented language models improve language models (LMs) by retrieving documents and prepending them in-context. However, these documents, often spanning hundreds of words, make inference substantially less efficient. We propose compressing the retrieved documents into textual summaries... | Eunsol Choi, Fangyuan Xu, Weijia Shi |  |
| 732 |  |  [Gen-Z: Generative Zero-Shot Text Classification with Contextualized Label Descriptions](https://openreview.net/forum?id=rkplYfqUr0) |  | 0 | Language model (LM) prompting—a popular paradigm for solving NLP tasks—has been shown to be susceptible to miscalibration and brittleness to slight prompt variations, caused by its discriminative prompting approach, i.e., predicting the label given the input. To address these issues, we propose... | Chan Young Park, Sachin Kumar, Yulia Tsvetkov |  |
| 733 |  |  [In-Context Learning Dynamics with Random Binary Sequences](https://openreview.net/forum?id=62K7mALO2q) |  | 0 | Large language models (LLMs) trained on huge text datasets demonstrate intriguing capabilities, achieving state-of-the-art performance on tasks they were not explicitly trained for. The precise nature of LLM capabilities is often mysterious, and different prompts can elicit different capabilities... | Ekdeep Singh Lubana, Eric J. Bigelow, Hidenori Tanaka, Robert P. Dick, Tomer D. Ullman |  |
| 734 |  |  [Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking](https://openreview.net/forum?id=XsHqr9dEGH) |  | 0 | Recent work by Power et al. (2022) highlighted a surprising "grokking" phenomenon in learning arithmetic tasks: a neural net first "memorizes" the training set, resulting in perfect training accuracy but near-random test accuracy, and after training for sufficiently longer, it suddenly transitions... | Jason D. Lee, Jikai Jin, Kaifeng Lyu, Simon Shaolei Du, Wei Hu, Zhiyuan Li |  |
| 735 |  |  [Be Aware of the Neighborhood Effect: Modeling Selection Bias under Interference](https://openreview.net/forum?id=52fz5sUAy2) |  | 0 | Selection bias in recommender system arises from the recommendation process of system filtering and the interactive process of user selection. Many previous studies have focused on addressing selection bias to achieve unbiased learning of the prediction model, but ignore the fact that potential... | Chunyuan Zheng, Fuli Feng, Haoxuan Li, Peng Wu, Sihao Ding, Xiangnan He, Zhi Geng |  |
| 736 |  |  [PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization](https://openreview.net/forum?id=22pyNMuIoa) |  | 0 | Expert-level prompts, carefully engineered by human experts who have a deep understanding of both large language models (LLMs) and domain knowledge, are the future of prompting and pivotal to harnessing the full power of advanced LLMs. Discovering such prompts with an automated process remains a... | Chenxi Li, Eric P. Xing, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Xinyuan Wang, Zhen Wang, Zhiting Hu |  |
| 737 |  |  [Repeated Random Sampling for Minimizing the Time-to-Accuracy of Learning](https://openreview.net/forum?id=JnRStoIuTe) |  | 0 | Methods for carefully selecting or generating a small set of training data to learn from, i.e., data pruning, coreset selection, and dataset distillation, have been shown to be effective in reducing the ever-increasing cost of training neural networks. Behind this success are rigorously designed,... | Amin Karbasi, Dionysios S. Kalogerias, Konstantinos E. Nikolakakis, Nezihe Merve Gürel, Patrik Okanovic, Roger Waleffe, Theodoros Rekatsinas, Vasilis Mageirakos |  |
| 738 |  |  [Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs](https://openreview.net/forum?id=kGteeZ18Ir) |  | 0 | Recent works have showcased the ability of large-scale language models (LLMs) to embody diverse personas in their responses, exemplified by prompts like ‘_You are Yoda. Explain the Theory of Relativity._’ While this ability allows personalization of LLMs and enables human behavior simulation, its... | Ameet Deshpande, Ashish Sabharwal, Ashwin Kalyan, Peter Clark, Shashank Gupta, Tushar Khot, Vaishnavi Shrivastava |  |
| 739 |  |  [Enhancing Instance-Level Image Classification with Set-Level Labels](https://openreview.net/forum?id=AZW3qlCGTe) |  | 0 | Instance-level image classification tasks have traditionally relied on single-instance labels to train models, e.g., few-shot learning and transfer learning. However, set-level coarse-grained labels that capture relationships among instances can provide richer information in real-world scenarios.... | Aly A. Khan, Renyu Zhang, Robert L. Grossman, Yuxin Chen |  |
| 740 |  |  [Pushing Boundaries: Mixup's Influence on Neural Collapse](https://openreview.net/forum?id=jTSKkcbEsj) |  | 0 | Mixup is a data augmentation strategy that employs convex combinations of training instances and their respective labels to improve the robustness and calibration of deep neural networks. Despite its widespread adoption, the nuanced mechanisms that underpin its success are not entirely understood.... | Haoming Meng, Quinn LeBlanc Fisher, Vardan Papyan |  |
| 741 |  |  [sRGB Real Noise Modeling via Noise-Aware Sampling with Normalizing Flows](https://openreview.net/forum?id=2XBBumBGeP) |  | 0 | Noise poses a widespread challenge in signal processing, particularly when it comes to denoising images. Although convolutional neural networks (CNNs) have exhibited remarkable success in this field, they are predicated upon the belief that noise follows established distributions, which restricts... | Donggoo Jung, Dongjin Kim, Sungyong Baik, Tae Hyun Kim |  |
| 742 |  |  [Uncertainty-aware Graph-based Hyperspectral Image Classification](https://openreview.net/forum?id=8dN7gApKm3) |  | 0 | Hyperspectral imaging (HSI) technology captures spectral information across a broad wavelength range, providing richer pixel features compared to traditional color images with only three channels. Although pixel classification in HSI has been extensively studied, especially using graph convolution... | Feng Chen, Linlin Yu, Yifei Lou |  |
| 743 |  |  [Generative Adversarial Equilibrium Solvers](https://openreview.net/forum?id=TlyiaPXaVN) |  | 0 | We introduce the use of generative adversarial learning to compute equilibria in general game-theoretic settings, specifically the generalized Nash equilibrium (GNE) in pseudo-games, and its specific instantiation as the competitive equilibrium (CE) in Arrow-Debreu competitive economies.... | Andrea Tacchetti, David C. Parkes, Denizalp Goktas, Georgios Piliouras, Guy Lever, Ian Gemp, Luke Marris, Romuald Elie |  |
| 744 |  |  [Graph Transformers on EHRs: Better Representation Improves Downstream Performance](https://openreview.net/forum?id=pe0Vdv7rsL) |  | 0 | Following the success of transformer-based methods across various machine learning applications, their adoption for healthcare predictive tasks using electronic health records (EHRs) has also expanded extensively. Similarly, graph-based methods have been shown to be very effective in capturing... | Rahmatollah Beheshti, Raphael Poulain |  |
| 745 |  |  [On the Scalability and Memory Efficiency of Semidefinite Programs for Lipschitz Constant Estimation of Neural Networks](https://openreview.net/forum?id=dwzLn78jq7) |  | 0 | Lipschitz constant estimation plays an important role in understanding generalization, robustness, and fairness in deep learning. Unlike naive bounds based on the network weight norm product, semidefinite programs (SDPs) have shown great promise in providing less conservative Lipschitz bounds with... | Aaron J. Havens, Alexandre Araujo, Bin Hu, Somesh Jha, Yang Zheng, Yudong Chen, Zi Wang |  |
| 746 |  |  [Large Language Models as Automated Aligners for benchmarking Vision-Language Models](https://openreview.net/forum?id=kZEXgtMNNo) |  | 0 | With the advancements in Large Language Models (LLMs), Vision-Language Models (VLMs) have reached a new level of sophistication, showing notable competence in executing intricate cognition and reasoning tasks. However, existing evaluation benchmarks, primarily relying on rigid, hand-crafted... | Chongjian Ge, Enze Xie, Ping Luo, Weikai Kong, Yuanfeng Ji, Zhenguo Li, Zhengying Liu |  |
| 747 |  |  [CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech](https://openreview.net/forum?id=ofzeypWosV) |  | 0 | With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis. Despite the ongoing rush towards scaling paradigms, audio tokenization... | Jaehyeon Kim, Jaewoong Cho, Keon Lee, Seungjun Chung |  |
| 748 |  |  [Unveiling the Unseen: Identifiable Clusters in Trained Depthwise Convolutional Kernels](https://openreview.net/forum?id=4VgBjsOC8k) |  | 0 | Recent advances in depthwise-separable convolutional neural networks (DS-CNNs) have led to novel architectures, that surpass the performance of classical CNNs, by a considerable scalability and accuracy margin. This paper reveals another striking property of DS-CNN architectures: discernible and... | Daniela Rus, Peyman M. Kiasari, Radu Grosu, Zahra Babaiee |  |
| 749 |  |  [UNR-Explainer: Counterfactual Explanations for Unsupervised Node Representation Learning Models](https://openreview.net/forum?id=0j9ZDzMPqr) |  | 0 | Node representation learning, such as Graph Neural Networks (GNNs), has become one of the important learning methods in machine learning, and the demand for reliable explanation generation is growing. Despite extensive research on explanation generation for supervised node representation learning,... | Geonhee Han, Hogun Park, Hyunju Kang |  |
| 750 |  |  [Are Bert Family Good Instruction Followers? A Study on Their Potential And Limitations](https://openreview.net/forum?id=x8VNtpCu1I) |  | 0 | Language modeling at scale has proven very effective and brought unprecedented success to natural language models. Many typical representatives, especially decoder-only models, e.g., BLOOM and LLaMA, and encoder-decoder models, e.g., Flan-T5 and AlexaTM, have exhibited incredible... | Juntao Li, Min Zhang, Qingrong Xia, Xinyu Duan, Yisheng Xiao, Zechang Li, Zechen Sun, Zhefeng Wang |  |
| 751 |  |  [Exploring the Promise and Limits of Real-Time Recurrent Learning](https://openreview.net/forum?id=V2cBKtdC3a) |  | 0 | Real-time recurrent learning (RTRL) for sequence-processing recurrent neural networks (RNNs) offers certain conceptual advantages over backpropagation through time (BPTT). RTRL requires neither caching past activations nor truncating context, and enables online learning. However, RTRL's time and... | Anand Gopalakrishnan, Jürgen Schmidhuber, Kazuki Irie |  |
| 752 |  |  [TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting](https://openreview.net/forum?id=YH5w12OUuU) |  | 0 | The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer... | Defu Cao, Furong Jia, Sercan Ö. Arik, Tomas Pfister, Wen Ye, Yan Liu, Yixiang Zheng |  |
| 753 |  |  [Scaling physics-informed hard constraints with mixture-of-experts](https://openreview.net/forum?id=u3dX2CEIZb) |  | 0 | Imposing known physical constraints, such as conservation laws, during neural network training introduces an inductive bias that can improve accuracy, reliability, convergence, and data efficiency for modeling physical dynamics. While such constraints can be softly imposed via loss function... | Aditi S. Krishnapriyan, Nithin Chalapathi, Yiheng Du |  |
| 754 |  |  [Structural Fairness-aware Active Learning for Graph Neural Networks](https://openreview.net/forum?id=bvjcMvMn7B) |  | 0 | Graph Neural Networks (GNNs) have seen significant achievements in semi-supervised node classification. Yet, their efficacy often hinges on access to high-quality labeled node samples, which may not always be available in real-world scenarios. While active learning is commonly employed across... | Haoyu Han, Hui Liu, Jiliang Tang, Li Ma, Makoto Yamada, MohamadAli Torkamani, Xiaorui Liu |  |
| 755 |  |  [Neural-Symbolic Recursive Machine for Systematic Generalization](https://openreview.net/forum?id=FWJAmwE0xH) |  | 0 | Current learning models often struggle with human-like systematic generalization, particularly in learning compositional rules from limited data and extrapolating them to novel combinations. We introduce the Neural-Symbolic Recursive Ma- chine ( NSR), whose core is a Grounded Symbol System ( GSS),... | Qing Li, Siyuan Huang, SongChun Zhu, Ying Nian Wu, Yitao Liang, Yixin Zhu |  |
| 756 |  |  [Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-to-Image Generation](https://openreview.net/forum?id=ITq4ZRUT4a) |  | 0 | Evaluating text-to-image models is notoriously difficult. A strong recent approach for assessing text-image faithfulness is based on QG/A (question generation and answering), which uses pre-trained foundational models to automatically generate a set of questions and answers from the prompt, and... | Jaemin Cho, Jason M. Baldridge, Jordi PontTuset, Mohit Bansal, Peter Anderson, Ranjay Krishna, Roopal Garg, Su Wang, Yushi Hu |  |
| 757 |  |  [Chain of Thought Empowers Transformers to Solve Inherently Serial Problems](https://openreview.net/forum?id=3EWTEy9MTM) |  | 0 | Generating a sequence of intermediate steps, \emph{a.k.a.}, a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical... | Denny Zhou, Hong Liu, Tengyu Ma, Zhiyuan Liu |  |
| 758 |  |  [Tractable MCMC for Private Learning with Pure and Gaussian Differential Privacy](https://openreview.net/forum?id=pmweVpJ229) |  | 0 | Posterior sampling, i.e., exponential mechanism to sample from the posterior distribution, provides $\varepsilon$-pure differential privacy (DP) guarantees and does not suffer from potentially unbounded privacy breach introduced by $(\varepsilon,\delta)$-approximate DP. In practice, however, one... | Rachel Redberg, Yian Ma, Yingyu Lin, YuXiang Wang, Zhiqi Bu |  |
| 759 |  |  [Optimal Sketching for Residual Error Estimation for Matrix and Vector Norms](https://openreview.net/forum?id=RsJwmWvE6Q) |  | 0 | We study the problem of residual error estimation for matrix and vector norms using a linear sketch. Such estimates can be used, for example, to quickly assess how useful a more expensive low-rank approximation computation will be. The matrix case concerns the Frobenius norm and the task is to... | David P. Woodruff, Honghao Lin, Yi Li |  |
| 760 |  |  [Reverse Diffusion Monte Carlo](https://openreview.net/forum?id=kIPEyMSdFV) |  | 0 | We propose a Monte Carlo sampler from the reverse diffusion process. Unlike the practice of diffusion models, where the intermediary updates---the score functions---are learned with a neural network, we transform the score matching problem into a mean estimation one. By estimating the means of the... | Hanze Dong, Tong Zhang, Xunpeng Huang, Yian Ma, Yifan Hao |  |
| 761 |  |  [Counting Graph Substructures with Graph Neural Networks](https://openreview.net/forum?id=qaJxPhkYtD) |  | 0 | Graph Neural Networks (GNNs) are powerful representation learning tools that have achieved remarkable performance in various downstream tasks. However, there are still open questions regarding their ability to count and list substructures, which play a crucial role in biological and social... | Alejandro Ribeiro, Charilaos I. Kanatsoulis |  |
| 762 |  |  [Are Models Biased on Text without Gender-related Language?](https://openreview.net/forum?id=w1JanwReU6) |  | 0 | Gender bias research has been pivotal in revealing undesirable behaviors in large language models, exposing serious gender stereotypes associated with occupations, and emotions. A key observation in prior work is that models reinforce stereotypes as a consequence of the gendered correlations that... | Catarina G. Belém, Preethi Seshadri, Sameer Singh, Yasaman Razeghi |  |
| 763 |  |  [PlaSma: Procedural Knowledge Models for Language-based Planning and Re-Planning](https://openreview.net/forum?id=dFcXJgnrGB) |  | 0 | Procedural planning, which entails decomposing a high-level goal into a sequence of temporally ordered steps, is an important yet intricate task for machines. It involves integrating common-sense knowledge to reason about complex and often contextualized situations, e.g. \`\`scheduling a doctor's... | Chandra Bhagavatula, Faeze Brahman, Hirona Jacqueline Arai, Jena D. Hwang, Keisuke Sakaguchi, Soumya Sanyal, Valentina Pyatkin, Xiang Lorraine Li, Xiang Ren, Yejin Choi |  |
| 764 |  |  [From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction](https://openreview.net/forum?id=PfPnugdxup) |  | 0 | Foundation models have been transformational in machine learning fields such as natural language processing and computer vision. Similar success in atomic property prediction has been limited due to the challenges of training effective models across multiple chemical domains. To address this, we... | Adeesh Kolluru, Brandon M. Wood, C. Lawrence Zitnick, John R. Kitchin, Nima Shoghi, Zachary W. Ulissi |  |
| 765 |  |  [Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets](https://openreview.net/forum?id=Zc2aIcucwc) |  | 0 | Recently, pre-trained foundation models have enabled significant advancements in multiple fields. In molecular machine learning, however, where datasets are often hand-curated, and hence typically small, the lack of datasets with labeled features, and codebases to manage those datasets, has... | Ali Parviz, Andrew W. Fitzgibbon, Blazej Banaszewski, Callum McLean, Cas Wognum, Chad Martin, Christopher Morris, Cristian Gabellini, Dominic Masters, Dominique Beaini, Frederik Wenkel, Gabriela MoisescuPareja, Guillaume Rabusseau, Guy Wolf, Hadrien Mary, Jama Hussein Mohamud, Jian Tang, Jiarui Lu, Joao Alex Cunha, Josef Dean, Kerstin Klaser, Luis Müller, Maciej Sypetkowski, Michael Craig, Michal Koziarski, Mirco Ravanelli, Oleksandr Dymov, Prudencio Tossou, Reihaneh Rabbany, Samuel MaddrellMander, Shenyang Huang, Therence Bois, Zhaocheng Zhu, Zhiyi Li |  |
| 766 |  |  [Independent-Set Design of Experiments for Estimating Treatment and Spillover Effects under Network Interference](https://openreview.net/forum?id=w50MQ9Vfty) |  | 0 | Interference is ubiquitous when conducting causal experiments over networks. Except for certain network structures, causal inference on the network in the presence of interference is difficult due to the entanglement between the treatment assignments and the interference levels. In this article, we... | Chencheng Cai, Edoardo M. Airoldi, Xu Zhang |  |
| 767 |  |  [FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores](https://openreview.net/forum?id=gPKTTAfYBp) |  | 0 | Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)---which allows long convolutions to run in $O(N\log N)$... | Christopher Ré, Daniel Y. Fu, Eric Nguyen, Hermann Kumbong |  |
| 768 |  |  [Transformer-VQ: Linear-Time Transformers via Vector Quantization](https://openreview.net/forum?id=oDdzXQzP2F) |  | 0 | We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In our large-scale experiments, Transformer-VQ is shown highly competitive in... | Lucas D. Lingle |  |
| 769 |  |  [The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry](https://openreview.net/forum?id=4g02l2N2Nx) |  | 0 | Linear attentions have shown promise for improving Transformer efficiency, reducing attention's quadratic complexity to linear in sequence length. This holds exciting promise for (1) training linear Transformers from scratch, (2) \`inetuned-conversion of task-specific Transformers into linear... | Christopher Ré, Hermann Kumbong, Kush Bhatia, Michael Zhang |  |
| 770 |  |  [Abstractors and relational cross-attention: An inductive bias for explicit relational reasoning in Transformers](https://openreview.net/forum?id=XNa6r6ZjoB) |  | 0 | An extension of Transformers is proposed that enables explicit relational reasoning through a novel module called the \*Abstractor\*. At the core of the Abstractor is a variant of attention called \*relational cross-attention\*. The approach is motivated by an architectural inductive bias for... | Awni Altabaa, John Lafferty, Jonathan D. Cohen, Taylor Whittington Webb |  |
| 771 |  |  [Doubly Robust Instance-Reweighted Adversarial Training](https://openreview.net/forum?id=OF5x1dzWSS) |  | 0 | Assigning importance weights to adversarial data has achieved great success in training adversarially robust networks under limited model capacity. However, existing instance-reweighted adversarial training (AT) methods heavily depend on heuristics and/or geometric interpretations to determine... | Daouda Sow, Sen Lin, Yingbin Liang, Zhangyang Wang |  |
| 772 |  |  [Training Diffusion Models with Reinforcement Learning](https://openreview.net/forum?id=YCWjhGrJFD) |  | 0 | Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug... | Ilya Kostrikov, Kevin Black, Michael Janner, Sergey Levine, Yilun Du |  |
| 773 |  |  [Finite-Time Analysis of On-Policy Heterogeneous Federated Reinforcement Learning](https://openreview.net/forum?id=D2eOVqPX9g) |  | 0 | Federated reinforcement learning (FRL) has emerged as a promising paradigm for reducing the sample complexity of reinforcement learning tasks by exploiting information from different agents. However, when each agent interacts with a potentially different environment, little to nothing is known... | Aritra Mitra, Chenyu Zhang, Han Wang, James Anderson |  |
| 774 |  |  [Federated Q-Learning: Linear Regret Speedup with Low Communication Cost](https://openreview.net/forum?id=fe6ANBxcKM) |  | 0 | In this paper, we consider federated reinforcement learning for tabular episodic Markov Decision Processes (MDP) where, under the coordination of a central server, multiple agents collaboratively explore the environment and learn an optimal policy without sharing their raw data. While linear... | Fengyu Gao, Jing Yang, Lingzhou Xue, Zhong Zheng |  |
| 775 |  |  [The Trickle-down Impact of Reward Inconsistency on RLHF](https://openreview.net/forum?id=MeHmwCDifc) |  | 0 | Standard practice within Reinforcement Learning from Human Feedback (RLHF) involves optimizing against a Reward Model (RM), which itself is trained to reflect human preferences for desirable generations. A notable subject that is understudied is the (in-)consistency of RMs --- whether they can... | Baolin Peng, Daniel Khashabi, Dong Yu, Haitao Mi, Lifeng Jin, Linfeng Song, Lingfeng Shen, Sihao Chen |  |
| 776 |  |  [Efficient Modulation for Vision Networks](https://openreview.net/forum?id=ip5LHJs6QX) |  | 0 | In this work, we present efficient modulation, a novel design for efficient vision networks. We revisit the modulation mechanism, which operates input through convolutional context modeling and feature projection layers, and fuses features via element-wise multiplication and an MLP block. We... | Bin Xiao, Jianwei Yang, Lu Yuan, Xiyang Dai, Xu Ma, Yinpeng Chen, Yun Fu |  |
| 777 |  |  [Pre-training LiDAR-based 3D Object Detectors through Colorization](https://openreview.net/forum?id=fB1iiH9xo7) |  | 0 | Accurate 3D object detection and understanding for self-driving cars heavily relies on LiDAR point clouds, necessitating large amounts of labeled data to train. In this work, we introduce an innovative pre-training approach, Grounded Point Colorization (GPC), to bridge the gap between data and... | Bharath Hariharan, Cheng Perng Phoo, Chenyang Ma, Katie Z. Luo, Kilian Q. Weinberger, Mark Campbell, TaiYu Pan, Tianle Chen, WeiLun Chao, Yurong You |  |
| 778 |  |  [An Emulator for Fine-tuning Large Language Models using Small Language Models](https://openreview.net/forum?id=Eo7kv0sllr) |  | 0 | Widely used language models (LMs) are typically built by scaling up a two-stage training pipeline: a pre-training stage that uses a very large, diverse dataset of text and a fine-tuning (sometimes, 'alignment') stage that uses targeted examples or other specifications of desired behaviors. While it... | Archit Sharma, Chelsea Finn, Christopher D. Manning, Eric Mitchell, Rafael Rafailov |  |
| 779 |  |  [Toward Student-oriented Teacher Network Training for Knowledge Distillation](https://openreview.net/forum?id=wsWGcw6qKD) |  | 0 | How to conduct teacher training for knowledge distillation is still an open problem. It has been widely observed that a best-performing teacher does not necessarily yield the best-performing student, suggesting a fundamental discrepancy between the current teacher training practice and the ideal... | Chengyu Dong, Jingbo Shang, Liyuan Liu |  |
| 780 |  |  [Language Models Represent Space and Time](https://openreview.net/forum?id=jE8xbmvFin) |  | 0 | The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned... | Max Tegmark, Wes Gurnee |  |
| 781 |  |  [Causal Modelling Agents: Causal Graph Discovery through Synergising Metadata- and Data-driven Reasoning](https://openreview.net/forum?id=pAoqRlTBtY) |  | 0 | Scientific discovery hinges on the effective integration of metadata, which refers to a set of 'cognitive' operations such as determining what information is relevant for inquiry, and data, which encompasses physical operations such as observation and experimentation. This paper introduces the... | Adamos Hadjivasiliou, Ahmed Abdulaal, Ayodeji Ijishakin, Daniel C. Alexander, Daniel C. Castro, Ivana Drobnjak, Nina Montaña Brown, Tiantian He |  |
| 782 |  |  [Fast-ELECTRA for Efficient Pre-training](https://openreview.net/forum?id=8OBuqbLb8h) |  | 0 | ELECTRA pre-trains language models by detecting tokens in a sequence that have been replaced by an auxiliary model. Although ELECTRA offers a significant boost in efficiency, its potential is constrained by the training cost brought by the auxiliary model. Notably, this model, which is jointly... | Chengyu Dong, Hao Cheng, Jianfeng Gao, Jingbo Shang, Liyuan Liu, Xiaodong Liu |  |
| 783 |  |  [Maximum Entropy Model Correction in Reinforcement Learning](https://openreview.net/forum?id=kNpSUN0uCc) |  | 0 | We propose and theoretically analyze an approach for planning with an approximate model in reinforcement learning that can reduce the adverse impact of model error. If the model is accurate enough, it accelerates the convergence to the true value function too. One of its key components is the... | Amin Rakhsha, Amirmassoud Farahmand, Mete Kemertas, Mohammad Ghavamzadeh |  |
| 784 |  |  [SpaCE: The Spatial Confounding Environment](https://openreview.net/forum?id=D9rJdtmIG6) |  | 0 | Spatial confounding poses a significant challenge in scientific studies involving spatial data, where unobserved spatial variables can influence both treatment and outcome, possibly leading to spurious associations. To address this problem, we introduce SpaCE: The Spatial Confounding Environment,... | Ana Trisovic, Francesca Dominici, Jie Kate Hu, Mauricio Tec, Michelle Audirac, Naeem Khoshnevis, Sophie Woodward |  |
| 785 |  |  [Language Model Detectors Are Easily Optimized Against](https://openreview.net/forum?id=4eJDMjYZZG) |  | 0 | The fluency and general applicability of large language models (LLMs) has motivated significant interest in detecting whether a piece of text was written by a language model. While both academic and commercial detectors have been deployed in some settings, particularly education, other research has... | Archit Sharma, Charlotte Nicks, Chelsea Finn, Christopher D. Manning, Eric Mitchell, Rafael Rafailov, Stefano Ermon |  |
| 786 |  |  [Zero-Shot Robotic Manipulation with Pre-Trained Image-Editing Diffusion Models](https://openreview.net/forum?id=c0chJTSbci) |  | 0 | If generalist robots are to operate in truly unstructured environments, they need to be able to recognize and reason about novel objects and scenarios. Such objects and scenarios might not be present in the robot’s own training data. We propose SuSIE, a method that leverages an image-editing... | Aviral Kumar, Chelsea Finn, Homer Rich Walke, Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Sergey Levine |  |
| 787 |  |  [Simple Hierarchical Planning with Diffusion](https://openreview.net/forum?id=kXHEBK9uAY) |  | 0 | Diffusion-based generative methods have proven effective in modeling trajectories with offline datasets. However, they often face computational challenges and can falter in generalization, especially in capturing temporal abstractions for long-horizon tasks. To overcome this, we introduce the... | Caglar Gulcehre, Chang Chen, Fei Deng, Kenji Kawaguchi, Sungjin Ahn |  |
| 788 |  |  [Stochastic Gradient Descent for Gaussian Processes Done Right](https://openreview.net/forum?id=fj2E5OcLFn) |  | 0 | As is well known, both sampling from the posterior and computing the mean of the posterior in Gaussian process regression reduces to solving a large linear system of equations. We study the use of stochastic gradient descent for solving this linear system, and show that when done right---by which... | Alexander Terenin, Austin Tripp, Csaba Szepesvári, David Janz, Javier Antorán, Jihao Andreas Lin, José Miguel HernándezLobato, Shreyas Padhy |  |
| 789 |  |  [GAFormer: Enhancing Timeseries Transformers Through Group-Aware Embeddings](https://openreview.net/forum?id=c56TWtYp0W) |  | 0 | Analyzing multivariate time series is important in many domains. However, it has been difficult to learn robust and generalizable representations within multivariate datasets due to complex inter-channel relationships and dynamic shifts. In this paper, we introduce a novel approach for learning... | Eva L. Dyer, Jingyun Xiao, Ran Liu |  |
| 790 |  |  [Why is SAM Robust to Label Noise?](https://openreview.net/forum?id=3aZCPl3ZvR) |  | 0 | Sharpness-Aware Minimization (SAM) is most known for achieving state-of the-art performances on natural image and language tasks. However, its most pronounced improvements (of tens of percent) is rather in the presence of label noise. Understanding SAM's label noise robustness requires a departure... | Aditi Raghunathan, Christina Baek, J. Zico Kolter |  |
| 791 |  |  [Revisiting the Last-Iterate Convergence of Stochastic Gradient Methods](https://openreview.net/forum?id=xxaEhwC1I4) |  | 0 | In the past several years, the last-iterate convergence of the Stochastic Gradient Descent (SGD) algorithm has triggered people's interest due to its good performance in practice but lack of theoretical understanding. For Lipschitz convex functions, different works have established the optimal... | Zhengyuan Zhou, Zijian Liu |  |
| 792 |  |  [CNN Kernels Can Be the Best Shapelets](https://openreview.net/forum?id=O8ouVV8PjF) |  | 0 | Shapelets and CNN are two typical approaches to model time series. Shapelets aim at finding a set of sub-sequences that extract feature-based interpretable shapes, but may suffer from accuracy and efficiency issues. CNN performs well by encoding sequences with a series of hidden representations,... | Dongsheng Li, Eric Qu, Kan Ren, Wenqiang He, Xufang Luo, Yansen Wang |  |
| 793 |  |  [Fine-Tuning Language Models for Factuality](https://openreview.net/forum?id=WPZ2yPag4K) |  | 0 | The fluency and creativity of large pre-trained language models (LLMs) have led to their widespread use, sometimes even as a replacement for traditional search engines. Yet language models are prone to making convincing but factually inaccurate claims, often referred to as \`hallucinations.' These... | Chelsea Finn, Christopher D. Manning, Eric Mitchell, Huaxiu Yao, Katherine Tian |  |
| 794 |  |  [Soft Robust MDPs and Risk-Sensitive MDPs: Equivalence, Policy Gradient, and Sample Complexity](https://openreview.net/forum?id=dEz3ge8QSo) |  | 0 | Robust Markov Decision Processes (MDPs) and risk-sensitive MDPs are both powerful tools for making decisions in the presence of uncertainties. Previous efforts have aimed to establish their connections, revealing equivalences in specific formulations. This paper introduces a new formulation for... | Na Li, Runyu Zhang, Yang Hu |  |
| 795 |  |  [Tensor Programs VI: Feature Learning in Infinite Depth Neural Networks](https://openreview.net/forum?id=17pVDnpwwl) |  | 0 | Empirical studies have consistently demonstrated that increasing the size of neural networks often yields superior performance in practical applications. However, there is a lack of consensus regarding the appropriate scaling strategy, particularly when it comes to increasing the depth of neural... | Chen Zhu, Dingli Yu, Greg Yang, Soufiane Hayou |  |
| 796 |  |  [Demystifying Poisoning Backdoor Attacks from a Statistical Perspective](https://openreview.net/forum?id=BPHcEpGvF8) |  | 0 | Backdoor attacks pose a significant security risk to machine learning applications due to their stealthy nature and potentially serious consequences. Such attacks involve embedding triggers within a learning model with the intention of causing malicious behavior when an active trigger is present... | Ashish Kundu, Ganghua Wang, Jayanth Srinivasa, Jie Ding, Mingyi Hong, Xuan Bi, Xun Xian |  |
| 797 |  |  [Learning to Make Adherence-aware Advice](https://openreview.net/forum?id=RgELE1dQXx) |  | 0 | As artificial intelligence (AI) systems play an increasingly prominent role in human decision-making, challenges surface in the realm of human-AI interactions. One challenge arises from the suboptimal AI policies due to the inadequate consideration of humans disregarding AI recommendations, as well... | Chunlin Sun, Guanting Chen, Hanzhao Wang, Xiaocheng Li |  |
| 798 |  |  [Beyond Spatio-Temporal Representations: Evolving Fourier Transform for Temporal Graphs](https://openreview.net/forum?id=uvFhCUPjtI) |  | 0 | We present the Evolving Graph Fourier Transform (EFT), the first invertible spectral transform that captures evolving representations on temporal graphs. We motivate our work by the inadequacy of existing methods for capturing the evolving graph spectra, which are also computationally expensive due... | Abhishek Nadgeri, Anson Bastos, Kuldeep Singh, Manish Singh, Toyotaro Suzumura |  |
| 799 |  |  [Cycle Consistency Driven Object Discovery](https://openreview.net/forum?id=f1xnBr4WD6) |  | 0 | Developing deep learning models that effectively learn object-centric representations, akin to human cognition, remains a challenging task. Existing approaches facilitate object discovery by representing objects as fixed-size vectors, called \`\`slots'' or \`\`object files''. While these approaches... | Aniket Rajiv Didolkar, Anirudh Goyal, Yoshua Bengio |  |
| 800 |  |  [Sufficient conditions for offline reactivation in recurrent neural networks](https://openreview.net/forum?id=RVrINT6MT7) |  | 0 | During periods of quiescence, such as sleep, neural activity in many brain circuits resembles that observed during periods of task engagement. However, the precise conditions under which task-optimized networks can autonomously reactivate the same network states responsible for online behavior is... | Blake Aaron Richards, Colin Bredenberg, Daniel Levenstein, Guillaume Lajoie, Nanda H. Krishna |  |
| 801 |  |  [Forward Learning of Graph Neural Networks](https://openreview.net/forum?id=Abr7dU98ME) |  | 0 | Graph neural networks (GNNs) have achieved remarkable success across a wide range of applications, such as recommendation, drug discovery, and question answering. Behind the success of GNNs lies the backpropagation (BP) algorithm, which is the de facto standard for training deep neural networks... | Antoine Simoulin, Grey Yang, Namyong Park, Nesreen K. Ahmed, Puja Trivedi, Ryan A. Rossi, Shuai Yang, Xing Wang |  |
| 802 |  |  [Curriculum reinforcement learning for quantum architecture search under hardware errors](https://openreview.net/forum?id=rINBD8jPoP) |  | 0 | The key challenge in the noisy intermediate-scale quantum era is finding useful circuits compatible with current device limitations. Variational quantum algorithms (VQAs) offer a potential solution by fixing the circuit architecture and optimizing individual gate parameters in an external loop.... | Akash Kundu, Mateusz Ostaszewski, Onur Danaci, Vedran Dunjko, Xavier BonetMonroig, Yash J. Patel |  |
| 803 |  |  [Does CLIP's generalization performance mainly stem from high train-test similarity?](https://openreview.net/forum?id=tnBaiidobu) |  | 0 | Foundation models like CLIP are trained on hundreds of millions of samples and effortlessly generalize to new tasks and inputs. Out of the box, CLIP shows stellar zero-shot and few-shot capabilities on a wide range of out-of-distribution (OOD) benchmarks, which prior works attribute mainly to... | Evgenia Rusak, Matthias Bethge, Prasanna Mayilvahanan, Thaddäus Wiedemer, Wieland Brendel |  |
| 804 |  |  [Unified Projection-Free Algorithms for Adversarial DR-Submodular Optimization](https://openreview.net/forum?id=H4A9e8HvIn) |  | 0 | This paper introduces unified projection-free Frank-Wolfe type algorithms for adversarial continuous DR-submodular optimization, spanning scenarios such as full information and (semi-)bandit feedback, monotone and non-monotone functions, different constraints, and types of stochastic queries. For... | Christopher John Quinn, Mohammad Pedramfar, Vaneet Aggarwal, Yididiya Y. Nadew |  |
| 805 |  |  [When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method](https://openreview.net/forum?id=5HCnKDeTws) |  | 0 | While large language models (LLMs) often adopt finetuning to unlock their capabilities for downstream applications, our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited. To fill this gap, we conduct systematic experiments... | Biao Zhang, Colin Cherry, Orhan Firat, Zhongtao Liu |  |
| 806 |  |  [Learning to design protein-protein interactions with enhanced generalization](https://openreview.net/forum?id=xcMmebCT7s) |  | 0 | Discovering mutations enhancing protein-protein interactions (PPIs) is critical for advancing biomedical research and developing improved therapeutics. While machine learning approaches have substantially advanced the field, they often struggle to generalize beyond training data in practical... | Anatolii Filkin, Anton Bushuiev, Jirí Damborský, Jirí Sedlár, Josef Sivic, Marketa Gabrielova, Michal Gabriel, Petr Kouba, Roman Bushuiev, Stanislav Mazurenko, Tomás Pluskal |  |
| 807 |  |  [L2MAC: Large Language Model Automatic Computer for Extensive Code Generation](https://openreview.net/forum?id=EhrzQwsV4K) |  | 0 | Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and coherent outputs. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long output... | Max Ruiz Luyten, Mihaela van der Schaar, Samuel Holt |  |
| 808 |  |  [BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models](https://openreview.net/forum?id=c93SBwz1Ma) |  | 0 | Large language models (LLMs) are shown to benefit from chain-of-thought (COT) prompting, particularly when tackling tasks that require systematic reasoning processes. On the other hand, COT prompting also poses new vulnerabilities in the form of backdoor attacks, wherein the model will output... | Bhaskar Ramasubramanian, Bo Li, Fengqing Jiang, Radha Poovendran, Zhen Xiang, Zidi Xiong |  |
| 809 |  |  [NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks](https://openreview.net/forum?id=samyfu6G93) |  | 0 | Propositional satisfiability (SAT) is an NP-complete problem that impacts many research fields, such as planning, verification, and security. Mainstream modern SAT solvers are based on the Conflict-Driven Clause Learning (CDCL) algorithm. Recent work aimed to enhance CDCL SAT solvers using Graph... | Kenneth L. McMillan, Mohit Tiwari, Risto Miikkulainen, Sarfraz Khurshid, Wenxi Wang, Yang Hu |  |
| 810 |  |  [Group Preference Optimization: Few-Shot Alignment of Large Language Models](https://openreview.net/forum?id=DpFeMH4l8Q) |  | 0 | Many applications of large language models (LLMs), ranging from chatbots to creative writing, require nuanced subjective judgments that can differ significantly across different groups. Existing alignment algorithms can be expensive to align for each group, requiring prohibitive amounts of... | Aditya Grover, John Dang, Siyan Zhao |  |
| 811 |  |  [MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training](https://openreview.net/forum?id=w3YZ9MSlBu) |  | 0 | Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored.... | Anton Ragni, Chenghao Xiao, Chenghua Lin, Emmanouil Benetos, Ge Zhang, Gus Xia, Hanzhi Yin, Jie Fu, Norbert Gyenge, Roger B. Dannenberg, Ruibin Yuan, Ruibo Liu, Wenhao Huang, Wenhu Chen, Xingran Chen, Yemin Shi, Yike Guo, Yinghao Ma, Yizhi Li, Zili Wang |  |
| 812 |  |  [Variance-aware Regret Bounds for Stochastic Contextual Dueling Bandits](https://openreview.net/forum?id=rDH7dIFn20) |  | 0 | Dueling bandits is a prominent framework for decision-making involving preferential feedback, a valuable feature that fits various applications involving human interaction, such as ranking, information retrieval, and recommendation systems. While substantial efforts have been made to minimize the... | Farzad Farnoud, Heyang Zhao, Qiwei Di, Quanquan Gu, Tao Jin, Yue Wu |  |
| 813 |  |  [A Discretization Framework for Robust Contextual Stochastic Optimization](https://openreview.net/forum?id=ueTdErd5Ib) |  | 0 | We study contextual stochastic optimization problems. Optimization problems have uncertain parameters stemming from unknown, context-dependent, distributions. Due to the inherent uncertainty in these problems, one is often interested not only in minimizing expected cost, but also to be robust and... | Georgia Perakis, Rares Cristian |  |
| 814 |  |  [Risk Bounds of Accelerated SGD for Overparameterized Linear Regression](https://openreview.net/forum?id=AcoXPIPh4A) |  | 0 | Accelerated stochastic gradient descent (ASGD) is a workhorse in deep learning and often achieves better generalization performance than SGD. However, existing optimization theory can only explain the faster convergence of ASGD, but cannot explain its better generalization. In this paper, we study... | Dongruo Zhou, Jingfeng Wu, Quanquan Gu, Xuheng Li, Yihe Deng |  |
| 815 |  |  [Task structure and nonlinearity jointly determine learned representational geometry](https://openreview.net/forum?id=k9t8dQ30kU) |  | 0 | The utility of a learned neural representation depends on how well its geometry supports performance in downstream tasks. This geometry depends on the structure of the inputs, the structure of the target outputs, and on the architecture of the network. By studying the learning dynamics of networks... | Jack W. Lindsey, Matteo Alleman, Stefano Fusi |  |
| 816 |  |  [Llemma: An Open Language Model for Mathematics](https://openreview.net/forum?id=4WnqRR915j) |  | 0 | We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known openly released models, as... | Albert Q. Jiang, Hailey Schoelkopf, Jia Deng, Keiran Paster, Marco Dos Santos, Sean Welleck, Stella Biderman, Stephen Marcus McAleer, Zhangir Azerbayev |  |
| 817 |  |  [Directly Fine-Tuning Diffusion Models on Differentiable Rewards](https://openreview.net/forum?id=1vmSEVL19f) |  | 0 | We present Direct Reward Fine-Tuning (DRaFT), a simple and effective method for fine-tuning diffusion models to maximize differentiable reward functions, such as scores from human preference models. We first show that it is possible to backpropagate the reward function gradient through the full... | David J. Fleet, Kevin Clark, Kevin Swersky, Paul Vicol |  |
| 818 |  |  [Maximum Likelihood Estimation is All You Need for Well-Specified Covariate Shift](https://openreview.net/forum?id=eoTCKKOgIs) |  | 0 | A key challenge of modern machine learning systems is to achieve Out-of-Distribution (OOD) generalization---generalizing to target data whose distribution differs from that of source data. Despite its significant importance, the fundamental question of \`\`what are the most effective algorithms for... | Chi Jin, Cong Ma, Jianqing Fan, Jiawei Ge, Shange Tang |  |
| 819 |  |  [Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks](https://openreview.net/forum?id=pAVJKp3Dvn) |  | 0 | This paper investigates efficient deep neural networks (DNNs) to replace dense unstructured weight matrices with structured ones that possess desired properties. The challenge arises because the optimal weight matrix structure in popular neural network models is obscure in most cases and may vary... | Changwoo Lee, HunSeok Kim |  |
| 820 |  |  [A Flexible Generative Model for Heterogeneous Tabular EHR with Missing Modality](https://openreview.net/forum?id=W2tCmRrj7H) |  | 0 | Realistic synthetic electronic health records (EHRs) can be leveraged to acceler- ate methodological developments for research purposes while mitigating privacy concerns associated with data sharing. However, the training of Generative Ad- versarial Networks remains challenging, often resulting in... | Bradley A. Malin, Huan He, Joyce C. Ho, William Hao, Yong Chen, Yuanzhe Xi |  |
| 821 |  |  [Designing Skill-Compatible AI: Methodologies and Frameworks in Chess](https://openreview.net/forum?id=79rfgv3jw4) |  | 0 | Powerful artificial intelligence systems are often used in settings where they must interact with agents that are computationally much weaker, for example when they work alongside humans or operate in complex environments where some tasks are handled by algorithms, heuristics, or other entities of... | Ashton Anderson, Jon M. Kleinberg, Karim Hamade, Reid McIlroyYoung, Siddhartha Sen |  |
| 822 |  |  [Tree Search-Based Policy Optimization under Stochastic Execution Delay](https://openreview.net/forum?id=RaqZX9LSGA) |  | 0 | The standard formulation of Markov decision processes (MDPs) assumes that the agent's decisions are executed immediately. However, in numerous realistic applications such as robotics or healthcare, actions are performed with a delay whose value can even be stochastic. In this work, we introduce... | David Valensi, Esther Derman, Gal Dalal, Shie Mannor |  |
| 823 |  |  [Context-Aware Meta-Learning](https://openreview.net/forum?id=lJYAkDVnRU) |  | 0 | Large Language Models like ChatGPT demonstrate a remarkable capacity to learn new concepts during inference without any fine-tuning. However, visual models trained to detect new objects during inference have been unable to replicate this ability, and instead either perform poorly or require... | Christopher Fifty, Christopher Ré, Dennis Duan, Ehsan Amid, Jure Leskovec, Ronald G. Junkins, Sebastian Thrun |  |
| 824 |  |  [Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain](https://openreview.net/forum?id=caW7LdAALh) |  | 0 | Code Large Language Models (Code LLMs) are being increasingly employed in real-life applications, so evaluating them is critical. While the conventional accuracy evaluates the performance of Code LLMs on a set of individual tasks, their self-consistency across different tasks is overlooked.... | Baishakhi Ray, Gail E. Kaiser, Luca Buratti, Marcus J. Min, Saurabh Pujar, Suman Jana, Yangruibo Ding |  |
| 825 |  |  [Ito Diffusion Approximation of Universal Ito Chains for Sampling, Optimization and Boosting](https://openreview.net/forum?id=fjpfCOV4ru) |  | 0 | In this work, we consider rather general and broad class of Markov chains, Ito chains, that look like Euler-Maryama discretization of some Stochastic Differential Equation. The chain we study is a unified framework for theoretical analysis. It comes with almost arbitrary isotropic and... | Aleksandr Beznosikov, Aleksei Ustimenko |  |
| 826 |  |  [Modeling Boundedly Rational Agents with Latent Inference Budgets](https://openreview.net/forum?id=W3VsHuga3j) |  | 0 | We study the problem of modeling a population of agents pursuing unknown goals subject to unknown computational constraints. In standard models of bounded rationality, sub-optimal decision-making is simulated by adding homoscedastic noise to optimal decisions rather than actually simulating... | Abhishek Gupta, Athul Paul Jacob, Jacob Andreas |  |
| 827 |  |  [The Effectiveness of Random Forgetting for Robust Generalization](https://openreview.net/forum?id=MEGQGNUfPx) |  | 0 | Deep neural networks are susceptible to adversarial attacks, which can compromise their performance and accuracy. Adversarial Training (AT) has emerged as a popular approach for protecting neural networks against such attacks. However, a key challenge of AT is robust overfitting, where the... | Bahram Zonooz, Elahe Arani, Vijaya Raghavan T. Ramkumar |  |
| 828 |  |  [Diffeomorphic Mesh Deformation via Efficient Optimal Transport for Cortical Surface Reconstruction](https://openreview.net/forum?id=gxhRR8vUQb) |  | 0 | Mesh deformation plays a pivotal role in many 3D vision tasks including dynamic simulations, rendering, and reconstruction. However, defining an efficient discrepancy between predicted and target meshes remains an open problem. A prevalent approach in current deep learning is the set-based approach... | Khai Nguyen, Kun Han, Nhat Ho, Shanlin Sun, ThanhTung Le, Xiaohui Xie |  |
| 829 |  |  [Lie Group Decompositions for Equivariant Neural Networks](https://openreview.net/forum?id=p34fRKp8qA) |  | 0 | Invariance and equivariance to geometrical transformations have proven to be very useful inductive biases when training (convolutional) neural network models, especially in the low-data regime. Much work has focused on the case where the symmetry group employed is compact or abelian, or both.... | Mircea Mironenco, Patrick Forré |  |
| 830 |  |  [Efficient Heterogeneous Meta-Learning via Channel Shuffling Modulation](https://openreview.net/forum?id=QiJuMJl0QS) |  | 0 | We tackle the problem of meta-learning across heterogenous tasks. This problem seeks to extract and generalize transferable meta-knowledge through streaming task sets from a multi-modal task distribution. The extracted meta-knowledge can be used to create predictors for new tasks using a small... | Carl Kingsford, Minh Hoang |  |
| 831 |  |  [To Grok or not to Grok: Disentangling Generalization and Memorization on Corrupted Algorithmic Datasets](https://openreview.net/forum?id=UHjE5v5MB7) |  | 0 | Robust generalization is a major challenge in deep learning, particularly when the number of trainable parameters is very large. In general, it is very difficult to know if the network has memorized a particular set of examples or understood the underlying rule (or both). Motivated by this... | Andrey Gromov, Aritra Das, Darshil Doshi, Tianyu He |  |
| 832 |  |  [VCR-Graphormer: A Mini-batch Graph Transformer via Virtual Connections](https://openreview.net/forum?id=SUUrkC3STJ) |  | 0 | Graph transformer has been proven as an effective graph learning method for its adoption of attention mechanism that is capable of capturing expressive representations from complex topological and feature information of graphs. Graph transformer conventionally performs dense attention (or global... | Andrey Malevich, Bo Long, Dongqi Fu, Hao Wu, Jin Fang, Jingrui He, Kaan Sancak, Si Zhang, Yan Xie, Zhigang Hua |  |
| 833 |  |  [Optimistic Bayesian Optimization with Unknown Constraints](https://openreview.net/forum?id=D4NJFfrqoq) |  | 0 | Though some research efforts have been dedicated to constrained Bayesian optimization (BO), there remains a notable absence of a principled approach with a theoretical performance guarantee in the decoupled setting. Such a setting involves independent evaluations of the objective function and... | Bryan Kian Hsiang Low, Le Song, Patrick Jaillet, Quoc Phong Nguyen, Wan Theng Ruth Chew |  |
| 834 |  |  [DRSM: De-Randomized Smoothing on Malware Classifier Providing Certified Robustness](https://openreview.net/forum?id=m7aPLHwsLr) |  | 0 | Machine Learning (ML) models have been utilized for malware detection for over two decades. Consequently, this ignited an ongoing arms race between malware authors and antivirus systems, compelling researchers to propose defenses for malware-detection models against evasion attacks. However, most... | Shoumik Saha, Soheil Feizi, Tudor Dumitras, Wenxiao Wang, Yigitcan Kaya |  |
| 835 |  |  [On the Variance of Neural Network Training with respect to Test Sets and Distributions](https://openreview.net/forum?id=pEGSdJu52I) |  | 0 | Neural network trainings are stochastic, causing the performance of trained networks to vary across repeated runs of training. We contribute the following results towards understanding this variation. (1) Despite having significant variance on their test-sets, we demonstrate that standard CIFAR-10... | Keller Jordan |  |
| 836 |  |  [Large Language Models to Enhance Bayesian Optimization](https://openreview.net/forum?id=OOxotBmGol) |  | 0 | Bayesian optimization (BO) is a powerful approach for optimizing complex and expensive-to-evaluate black-box functions. Its importance is underscored in many applications, notably including hyperparameter tuning, but its efficacy depends on efficiently balancing exploration and exploitation. While... | Mihaela van der Schaar, Nabeel Seedat, Nicolás Astorga, Tennison Liu |  |
| 837 |  |  [Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach](https://openreview.net/forum?id=55uj7mU7Cv) |  | 0 | Unsupervised domain translation (UDT) aims to find functions that convert samples from one domain (e.g., sketches) to another domain (e.g., photos) without changing the high-level semantic meaning (also referred to as "content"). The translation functions are often sought by probability... | Sagar Shrestha, Xiao Fu |  |
| 838 |  |  [SineNet: Learning Temporal Dynamics in Time-Dependent Partial Differential Equations](https://openreview.net/forum?id=LSYhE2hLWG) |  | 0 | We consider using deep neural networks to solve time-dependent partial differential equations (PDEs), where multi-scale processing is crucial for modeling complex, time-evolving dynamics. While the U-Net architecture with skip connections is commonly used by prior studies to enable multi-scale... | Cong Fu, Jacob Helwig, Shuiwang Ji, Stephan Wojtowytsch, Xuan Zhang, Yaochen Xie, Yuchao Lin |  |
| 839 |  |  [GNNBoundary: Towards Explaining Graph Neural Networks through the Lens of Decision Boundaries](https://openreview.net/forum?id=WIzzXCVYiH) |  | 0 | While Graph Neural Networks (GNNs) have achieved remarkable performance on various machine learning tasks on graph data, they also raised questions regarding their transparency and interpretability. Recently, there have been extensive research efforts to explain the decision-making process of GNNs.... | HanWei Shen, Xiaoqi Wang |  |
| 840 |  |  [Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales for Pruning Recurrent SNN](https://openreview.net/forum?id=0jsfesDZDq) |  | 0 | Recurrent Spiking Neural Networks (RSNNs) have emerged as a computationally efficient and brain-inspired machine learning model. The design of sparse RSNNs with fewer neurons and synapses helps reduce the computational complexity of RSNNs. Traditionally, sparse SNNs are obtained by first training a... | Beomseok Kang, Biswadeep Chakraborty, Harshit Kumar, Saibal Mukhopadhyay |  |
| 841 |  |  [Investigating the Benefits of Projection Head for Representation Learning](https://openreview.net/forum?id=GgEAdqYPNA) |  | 0 | An effective technique for obtaining high-quality representations is adding a projection head on top of the encoder during training, then discarding it and using the pre-projection representations. Despite its proven practical effectiveness, the reason behind the success of this technique is poorly... | Baharan Mirzasoleiman, Eric Gan, Jiayi Ni, Siddharth Joshi, Yihao Xue |  |
| 842 |  |  [A Variational Perspective on Solving Inverse Problems with Diffusion Models](https://openreview.net/forum?id=1YO4EE3SPB) |  | 0 | Diffusion models have emerged as a key pillar of foundation models in visual domains. One of their critical applications is to universally solve different downstream inverse tasks via a single diffusion prior without re-training for each task. Most inverse tasks can be formulated as inferring a... | Arash Vahdat, Jan Kautz, Jiaming Song, Morteza Mardani |  |
| 843 |  |  [Can Large Language Models Infer Causation from Correlation?](https://openreview.net/forum?id=vqIH0ObdqL) |  | 0 | Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we... | Bernhard Schölkopf, Jiarui Liu, Mona T. Diab, Mrinmaya Sachan, Rada Mihalcea, Spencer Poff, Zhiheng Lyu, Zhijing Jin |  |
| 844 |  |  [Improved statistical and computational complexity of the mean-field Langevin dynamics under structured data](https://openreview.net/forum?id=Of2nEDc4s7) |  | 0 | Recent works have shown that neural networks optimized by gradient-based methods can adapt to sparse or low-dimensional target functions through feature learning; an often studied target is the sparse parity function on the unit hypercube. However, such isotropic data setting does not capture the... | Atsushi Nitanda, Denny Wu, Kazusato Oko, Taiji Suzuki |  |
| 845 |  |  [Jointly-Learned Exit and Inference for a Dynamic Neural Network](https://openreview.net/forum?id=jX2DT7qDam) |  | 0 | Large pretrained models, coupled with fine-tuning, are slowly becoming established as the dominant architecture in machine learning. Even though these models offer impressive performance, their practical application is often limited by the prohibitive amount of resources required for... | Florence Regol, Joud Chataoui, Mark Coates |  |
| 846 |  |  [Understanding the Robustness of Multi-modal Contrastive Learning to Distribution Shift](https://openreview.net/forum?id=rtl4XnJYBh) |  | 0 | Recently, multimodal contrastive learning (MMCL) approaches, such as CLIP, have achieved a remarkable success in learning representations that are robust against distribution shift and generalize to new domains. Despite the empirical success, the mechanism behind learning such generalizable... | Baharan Mirzasoleiman, Dang Nguyen, Siddharth Joshi, Yihao Xue |  |
| 847 |  |  [SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking](https://openreview.net/forum?id=FJWT0692hw) |  | 0 | In many domains, autoregressive models can attain high likelihood on the task of predicting the next observation. However, this maximum-likelihood (MLE) objective does not necessarily match a downstream use-case of autoregressively generating high-quality sequences. The MLE objective weights... | Chris Cundy, Stefano Ermon |  |
| 848 |  |  [Layer-wise linear mode connectivity](https://openreview.net/forum?id=LfmZh91tDI) |  | 0 | Averaging neural network parameters is an intuitive method for fusing the knowledge of two independent models. It is most prominently used in federated learning. If models are averaged at the end of training, this can only lead to a good performing model if the loss surface of interest is very... | Asja Fischer, Linara Adilova, Maksym Andriushchenko, Martin Jaggi, Michael Kamp |  |
| 849 |  |  [Understanding Certified Training with Interval Bound Propagation](https://openreview.net/forum?id=h05eQniJsQ) |  | 0 | As robustness verification methods are becoming more precise, training certifiably robust neural networks is becoming ever more relevant. To this end, certified training methods compute and then optimize an upper bound on the worst-case loss over a robustness specification. Curiously, training... | Marc Fischer, Mark Niklas Müller, Martin T. Vechev, Yuhao Mao |  |
| 850 |  |  [Offline RL with Observation Histories: Analyzing and Improving Sample Complexity](https://openreview.net/forum?id=GnOLWS4Llt) |  | 0 | Offline reinforcement learning (RL) can in principle synthesize more optimal behavior from a dataset consisting only of suboptimal trials. One way that this can happen is by "stitching" together the best parts of otherwise suboptimal trajectories that overlap on similar states, to create new... | Anca D. Dragan, Joey Hong, Sergey Levine |  |
| 851 |  |  [Graph-based Virtual Sensing from Sparse and Partial Multivariate Observations](https://openreview.net/forum?id=CAqdG2dy5s) |  | 0 | Virtual sensing techniques allow for inferring signals at new unmonitored locations by exploiting spatio-temporal measurements coming from physical sensors at different locations. However, as the sensor coverage becomes sparse due to costs or other constraints, physical proximity cannot be used to... | Andrea Cini, Cesare Alippi, Daniele Zambon, Giovanni de Felice, Vladimir V. Gusev |  |
| 852 |  |  [NEFTune: Noisy Embeddings Improve Instruction Finetuning](https://openreview.net/forum?id=0bMmZ3fkCk) |  | 0 | We show that language model finetuning can be improved, sometimes dramatically, with a simple augmentation. NEFTune adds noise to the embedding vectors during training. Standard finetuning of LLaMA-2-7B using Alpaca achieves $29.79$\% on AlpacaEval, which rises to $64.69$\% using noisy embeddings.... | Aniruddha Saha, Avi Schwarzschild, Bhavya Kailkhura, Brian R. Bartoldson, Gowthami Somepalli, HongMin Chu, John Kirchenbauer, Jonas Geiping, Micah Goldblum, Neel Jain, Pingyeh Chiang, Tom Goldstein, Yuxin Wen |  |
| 853 |  |  [An operator preconditioning perspective on training in physics-informed machine learning](https://openreview.net/forum?id=WWlxFtR5sV) |  | 0 | In this paper, we investigate the behavior of gradient descent algorithms in physics-informed machine learning methods like PINNs, which minimize residuals connected to partial differential equations (PDEs). Our key result is that the difficulty in training these models is closely related to the... | Emmanuel de Bézenac, Florent Bonnet, Siddhartha Mishra, Tim De Ryck |  |
| 854 |  |  [Two-stage LLM Fine-tuning with Less Specialization and More Generalization](https://openreview.net/forum?id=pCEgna6Qco) |  | 0 | Pretrained large language models (LLMs) are general purpose problem solvers applicable to a diverse set of tasks with prompts. They can be further improved towards a specific task by fine-tuning on a specialized dataset. However, fine-tuning usually makes the model narrowly specialized on this... | ChoJui Hsieh, Daliang Li, Felix X. Yu, Inderjit S. Dhillon, Michal Lukasik, Sanjiv Kumar, Si Si, Yihan Wang |  |
| 855 |  |  [Expressive Losses for Verified Robustness via Convex Combinations](https://openreview.net/forum?id=mzyZ4wzKlM) |  | 0 | In order to train networks for verified adversarial robustness, it is common to over-approximate the worst-case loss over perturbation regions, resulting in networks that attain verifiability at the expense of standard performance. As shown in recent work, better trade-offs between accuracy and... | Alessandro De Palma, Alessio Lomuscio, Krishnamurthy (Dj) Dvijotham, M. Pawan Kumar, Robert Stanforth, Rudy Bunel |  |
| 856 |  |  [Learning Adaptive Multiresolution Transforms via Meta-Framelet-based Graph Convolutional Network](https://openreview.net/forum?id=5RielfrDkP) |  | 0 | Graph Neural Networks are popular tools in graph representation learning that capture the graph structural properties. However, most GNNs employ single-resolution graph feature extraction, thereby failing to capture micro-level local patterns (high resolution) and macro-level graph cluster and... | Sinno Jialin Pan, Tianze Luo, Zhanfeng Mo |  |
| 857 |  |  [REFACTOR: Learning to Extract Theorems from Proofs](https://openreview.net/forum?id=fgKjiVrm6u) |  | 0 | Human mathematicians are often good at recognizing modular and reusable theorems that make complex mathematical results within reach. In this paper, we propose a novel method called theoREm-from-prooF extrACTOR (REFACTOR) for training neural networks to mimic this ability in formal mathematical... | Jin Peng Zhou, Qiyang Li, Roger Baker Grosse, Yuhuai Wu |  |
| 858 |  |  [Let's do the time-warp-attend: Learning topological invariants of dynamical systems](https://openreview.net/forum?id=Fj7Fzm5lWL) |  | 0 | Dynamical systems across the sciences, from electrical circuits to ecological networks, undergo qualitative and often catastrophic changes in behavior, called bifurcations, when their underlying parameters cross a threshold. Existing methods predict oncoming catastrophes in individual systems but... | Matthew Ricci, Mor Nitzan, Noa Moriel |  |
| 859 |  |  [Sparse MoE with Language Guided Routing for Multilingual Machine Translation](https://openreview.net/forum?id=ySS7hH1smL) |  | 0 | Sparse Mixture-of-Experts (SMoE) has gained increasing popularity as a promising framework for scaling up multilingual machine translation (MMT) models with negligible extra computational overheads. However, current SMoE solutions neglect the intrinsic structures of the MMT problem: ($a$)... | Tianlong Chen, Xinyu Zhao, Xuxi Chen, Yu Cheng |  |
| 860 |  |  [Detecting Pretraining Data from Large Language Models](https://openreview.net/forum?id=zWqr3MQuNs) |  | 0 | Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable... | Anirudh Ajith, Danqi Chen, Daogao Liu, Luke Zettlemoyer, Mengzhou Xia, Terra Blevins, Weijia Shi, Yangsibo Huang |  |
| 861 |  |  [Don't Trust: Verify - Grounding LLM Quantitative Reasoning with Autoformalization](https://openreview.net/forum?id=V5tdi14ple) |  | 0 | Large language models (LLM), such as Google's Minerva and OpenAI's GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we... | Charles Staats, Christian Szegedy, Jin Peng Zhou, Kilian Q. Weinberger, Wenda Li, Yuhuai Wu |  |
| 862 |  |  [PubDef: Defending Against Transfer Attacks From Public Models](https://openreview.net/forum?id=Tvwf4Vsi5F) |  | 0 | Adversarial attacks have been a looming and unaddressed threat in the industry. However, through a decade-long history of the robustness evaluation literature, we have learned that mounting a strong or optimal attack is challenging. It requires both machine learning and domain expertise. In other... | Chawin Sitawarin, David A. Wagner, David Huang, Jaewon Chang, Wesson Altoyan |  |
| 863 |  |  [AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with TikZ](https://openreview.net/forum?id=v3K5TVP8kZ) |  | 0 | Generating bitmap graphics from text has gained considerable attention, yet for scientific figures, vector graphics are often preferred. Given that vector graphics are typically encoded using low-level graphics primitives, generating them directly is difficult. To address this, we propose the use... | Anne Lauscher, Jonas Belouadi, Steffen Eger |  |
| 864 |  |  [Sharpness-Aware Minimization Enhances Feature Quality via Balanced Learning](https://openreview.net/forum?id=3xDaj4pRna) |  | 0 | Sharpness-Aware Minimization (SAM) has emerged as a promising alternative optimizer to stochastic gradient descent (SGD). The originally-proposed motivation behind SAM was to bias neural networks towards flatter minima that are believed to generalize better. However, recent studies have shown... | Aditi Raghunathan, Jacob Mitchell Springer, Vaishnavh Nagarajan |  |
| 865 |  |  [Can LLM-Generated Misinformation Be Detected?](https://openreview.net/forum?id=ccxD4mtkTU) |  | 0 | The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated... | Canyu Chen, Kai Shu |  |
| 866 |  |  [A Simple Interpretable Transformer for Fine-Grained Image Classification and Analysis](https://openreview.net/forum?id=bkdWThqE6q) |  | 0 | We present a novel usage of Transformers to make image classification interpretable. Unlike mainstream classifiers that wait until the last fully connected layer to incorporate class information to make predictions, we investigate a proactive approach, asking each class to search for itself in an... | Anuj Karpatne, Arpita Chowdhury, Bryan Carstens, Charles V. Stewart, Daniel I. Rubenstein, David Edward Carlyn, Dipanjyoti Paul, FengJu Chang, Kaiya Provost, Samuel Stevens, Tanya Y. BergerWolf, WeiLun Chao, Xinqi Xiong, Yu Su |  |
| 867 |  |  [One-shot Active Learning Based on Lewis Weight Sampling for Multiple Deep Models](https://openreview.net/forum?id=EDXkkUAIFW) |  | 0 | Active learning (AL) for multiple target models aims to reduce labeled data querying while effectively training multiple models concurrently. Existing AL algorithms often rely on iterative model training, which can be computationally expensive, particularly for deep models. In this paper, we... | ShengJun Huang, Yi Li, Yiming Sun, YingPeng Tang |  |
| 868 |  |  [Disentangling Time Series Representations via Contrastive Independence-of-Support on l-Variational Inference](https://openreview.net/forum?id=iI7hZSczxE) |  | 0 | Learning disentangled representations for time series is a promising path to facilitate reliable generalization to in- and out-of distribution (OOD), offering benefits like feature derivation and improved interpretability and fairness, thereby enhancing downstream tasks. We focus on disentangled... | David Benhaiem, Emmanuel Leborgne, François Roueff, Khalid Oublal, Saïd Ladjal |  |
| 869 |  |  [Improved algorithm and bounds for successive projection](https://openreview.net/forum?id=GlpawHh80l) |  | 0 | Consider a $K$-vertex simplex in a $d$-dimensional space. We measure $n$ points on the simplex, but due to the measurement noise, some of the observed points fall outside the simplex. The interest is vertex hunting (i.e., estimating the vertices of the simplex). The successive projection algorithm... | Gabriel Moryoussef, Jiajun Tang, Jiashun Jin, Jingming Wang, Zheng Tracy Ke |  |
| 870 |  |  [Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF](https://openreview.net/forum?id=0tWTxYYPnW) |  | 0 | In practice, preference learning from human feedback depends on incomplete data with hidden context. Hidden context refers to data that affects the feedback received, but which is not represented in the data used to train a preference model. This captures common issues of data collection, such as... | Anand Siththaranjan, Cassidy Laidlaw, Dylan HadfieldMenell |  |
| 871 |  |  [Estimating Shape Distances on Neural Representations with Limited Samples](https://openreview.net/forum?id=kvByNnMERu) |  | 0 | Measuring geometric similarity between high-dimensional network representations is a topic of longstanding interest to neuroscience and deep learning. Although many methods have been proposed, only a few works have rigorously analyzed their statistical efficiency or quantified estimator uncertainty... | Alex H. Williams, Brett W. Larsen, Dean A. Pospisil, Sarah E. Harvey |  |
| 872 |  |  [Learning semilinear neural operators: A unified recursive framework for prediction and data assimilation](https://openreview.net/forum?id=ZMv6zKYYUs) |  | 0 | Recent advances in the theory of Neural Operators (NOs) have enabled fast and accurate computation of the solutions to complex systems described by partial differential equations (PDEs). Despite their great success, current NO-based solutions face important challenges when dealing with... | Ashutosh Singh, Deniz Erdogmus, Ricardo Augusto Borsoi, Tales Imbiriba |  |
| 873 |  |  [Eureka: Human-Level Reward Design via Coding Large Language Models](https://openreview.net/forum?id=IEduRUO55F) |  | 0 | Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present Eureka, a... | Anima Anandkumar, DeAn Huang, Dinesh Jayaraman, Guanzhi Wang, Linxi Fan, Osbert Bastani, William Liang, Yecheng Jason Ma, Yuke Zhu |  |
| 874 |  |  [f-FERM: A Scalable Framework for Robust Fair Empirical Risk Minimization](https://openreview.net/forum?id=s90VIdza2K) |  | 0 | Training and deploying machine learning models that meet fairness criteria for protected groups are fundamental in modern artificial intelligence. While numerous constraints and regularization terms have been proposed in the literature to promote fairness in machine learning tasks, most of these... | Meisam Razaviyayn, Shivam Patel, Sina Baharlouei |  |
| 875 |  |  [Source-Free and Image-Only Unsupervised Domain Adaptation for Category Level Object Pose Estimation](https://openreview.net/forum?id=UPvufoBAIs) |  | 0 | We consider the problem of source-free unsupervised category-level 3D pose estimation from only RGB images to an non-annotated and unlabelled target domain without any access to source domain data or annotations during adaptation. Collecting and annotating real world 3D data and corresponding... | Aayush Mishra, Adam Kortylewski, Alan L. Yuille, Prakhar Kaushik |  |
| 876 |  |  [Closing the Curious Case of Neural Text Degeneration](https://openreview.net/forum?id=dONpC9GL1o) |  | 0 | Despite their ubiquity in language generation, it remains unknown why truncation sampling heuristics like nucleus sampling are so effective. We provide a theoretical explanation for the effectiveness of the truncation sampling by proving that truncation methods that discard tokens below some... | Alexander Koller, Ashish Sabharwal, John Hewitt, Matthew Finlayson, Swabha Swayamdipta |  |
| 877 |  |  [Mediator Interpretation and Faster Learning Algorithms for Linear Correlated Equilibria in General Sequential Games](https://openreview.net/forum?id=bsKMPAFHO7) |  | 0 | A recent paper by Farina and Pipis (2023) established the existence of uncoupled no-linear-swap regret dynamics with polynomial-time iterations in extensive-form games. The equilibrium points reached by these dynamics, known as linear correlated equilibria, are currently the tightest known... | Brian Hu Zhang, Gabriele Farina, Tuomas Sandholm |  |
| 878 |  |  [3D Feature Prediction for Masked-AutoEncoder-Based Point Cloud Pretraining](https://openreview.net/forum?id=LokR2TTFMs) |  | 0 | Masked autoencoders (MAE) have recently been introduced to 3D self-supervised pretraining for point clouds due to their great success in NLP and computer vision. Unlike MAEs used in the image domain, where the pretext task is to restore features at the masked pixels, such as colors, the existing 3D... | Hao Pan, PengShuai Wang, Qixing Huang, Siming Yan, Xin Tong, Yang Liu, YuXiao Guo, Yuqi Yang |  |
| 879 |  |  [Understanding Catastrophic Forgetting in Language Models via Implicit Inference](https://openreview.net/forum?id=VrHiF2hsrm) |  | 0 | We lack a systematic understanding of the effects of fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback), particularly on tasks outside the narrow fine-tuning distribution. In a simplified scenario, we demonstrate that improving performance on tasks... | Aditi Raghunathan, Jacob Mitchell Springer, Suhas Kotha |  |
| 880 |  |  [Efficient Subgraph GNNs by Learning Effective Selection Policies](https://openreview.net/forum?id=gppLqZLQeY) |  | 0 | Subgraph GNNs are provably expressive neural architectures that learn graph representations from sets of subgraphs. Unfortunately, their applicability is hampered by the computational complexity associated with performing message passing on many subgraphs. In this paper, we consider the problem of... | Beatrice Bevilacqua, Bruno Ribeiro, Eli A. Meirom, Haggai Maron, Moshe Eliasof |  |
| 881 |  |  [Parsing neural dynamics with infinite recurrent switching linear dynamical systems](https://openreview.net/forum?id=YIls9HEa52) |  | 0 | Unsupervised methods for dimensionality reduction of neural activity and behavior have provided unprecedented insights into the underpinnings of neural information processing. One popular approach involves the recurrent switching linear dynamical system (rSLDS) model, which describes the latent... | International Brain Laboratory, Jonathan W. Pillow, Victor Geadah |  |
| 882 |  |  [Active Retrosynthetic Planning Aware of Route Quality](https://openreview.net/forum?id=h7DGnWGeos) |  | 0 | Retrosynthetic planning is a sequential decision-making process of identifying synthetic routes from the available building block materials to reach a desired target molecule. Though existing planning approaches show promisingly high solving rates and low costs, the trivial route cost evaluation... | Fei Wu, Luotian Yuan, Yemin Yu, Ying Wei, Yongwei Wang, Zhihua Wang |  |
| 883 |  |  [How do Language Models Bind Entities in Context?](https://openreview.net/forum?id=zb3b6oKO77) |  | 0 | Language models (LMs) can recall facts mentioned in context, as shown by their performance on reading comprehension tasks. When the context describes facts about more than one entity, the LM has to correctly bind attributes to their corresponding entity. We show, via causal experiments, that LMs'... | Jacob Steinhardt, Jiahai Feng |  |
| 884 |  |  [Novel Quadratic Constraints for Extending LipSDP beyond Slope-Restricted Activations](https://openreview.net/forum?id=HfXDrAzFvG) |  | 0 | Recently, semidefinite programming (SDP) techniques have shown great promise in providing accurate Lipschitz bounds for neural networks. Specifically, the LipSDP approach (Fazlyab et al., 2019) has received much attention and provides the least conservative Lipschitz upper bounds that can be... | Aaron J. Havens, Alexandre Araujo, Bin Hu, Farshad Khorrami, Frank Allgöwer, Patricia Pauli, Siddharth Garg |  |
| 885 |  |  [Unbalancedness in Neural Monge Maps Improves Unpaired Domain Translation](https://openreview.net/forum?id=2UnCj3jeao) |  | 0 | In optimal transport (OT), a Monge map is known as a mapping that transports a source distribution to a target distribution in the most cost-efficient way. Recently, multiple neural estimators for Monge maps have been developed and applied in diverse unpaired domain translation tasks, e.g. in... | Dominik Klein, Fabian J. Theis, Giovanni Palla, Luca Eyring, Niki Kilbertus, Théo Uscidda, Zeynep Akata |  |
| 886 |  |  [Implicit Neural Representations and the Algebra of Complex Wavelets](https://openreview.net/forum?id=uZfjFyPAvn) |  | 0 | Implicit neural representations (INRs) have arisen as useful methods for representing signals on Euclidean domains. By parameterizing an image as a multilayer perceptron (MLP) on Euclidean space, INRs effectively couple spatial and spectral features of the represented signal in a way that is not... | Maarten V. de Hoop, Richard G. Baraniuk, T. Mitchell Roddenberry, Vishwanath Saragadam |  |
| 887 |  |  [Fiber Monte Carlo](https://openreview.net/forum?id=sP1tCl2QBk) |  | 0 | Integrals with discontinuous integrands are ubiquitous, arising from discrete structure in applications like topology optimization, graphics, and computational geometry. These integrals are often part of a forward model in an inverse problem where it is necessary to reason backwards about the... | Deniz Oktay, James C. Bowden, Nick Richardson, Ryan P. Adams, Yaniv Ovadia |  |
| 888 |  |  [Learning from Label Proportions: Bootstrapping Supervised Learners via Belief Propagation](https://openreview.net/forum?id=KQe9tHd0k8) |  | 0 | Learning from Label Proportions (LLP) is a learning problem where only aggregate level labels are available for groups of instances, called bags, during training, and the aim is to get the best performance at the instance-level on the test data. This setting arises in domains like advertising and... | Aravindan Raghuveer, Karthikeyan Shanmugam, Navodita Sharma, Shreyas Havaldar, Shubhi Sareen |  |
| 889 |  |  [Modeling state-dependent communication between brain regions with switching nonlinear dynamical systems](https://openreview.net/forum?id=WQwV7Y8qwa) |  | 0 | Understanding how multiple brain regions interact to produce behavior is a major challenge in systems neuroscience, with many regions causally implicated in common tasks such as sensory processing and decision making. A precise description of interactions between regions remains an open problem.... | Carlos D. Brody, David M. Zoltowski, David W. Tank, E. Mika Diamanti, Jonathan W. Pillow, Lucas Pinto, Orren KarniolTambour |  |
| 890 |  |  [Neural Language of Thought Models](https://openreview.net/forum?id=HYyRwm367m) |  | 0 | The Language of Thought Hypothesis suggests that human cognition operates on a structured, language-like system of mental representations. While neural language models can naturally benefit from the compositional structure inherently and explicitly expressed in language data, learning such... | Minseung Lee, Sungjin Ahn, YiFu Wu |  |
| 891 |  |  [Retrieval-Guided Reinforcement Learning for Boolean Circuit Minimization](https://openreview.net/forum?id=0t1O8ziRZp) |  | 0 | Logic synthesis, a pivotal stage in chip design, entails optimizing chip specifications encoded in hardware description languages like Verilog into highly efficient implementations using Boolean logic gates. The process involves a sequential application of logic minimization heuristics... | Animesh Basak Chowdhury, Benjamin Tan, Marco Romanelli, Ramesh Karri, Siddharth Garg |  |
| 892 |  |  [Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting](https://openreview.net/forum?id=ztpy1gsUpT) |  | 0 | Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for... | Chenxin Tian, Linda Ruth Petzold, Shiyang Li, Xianjun Yang, Xinlu Zhang, Yao Qin |  |
| 893 |  |  [What Makes a Good Prune? Maximal Unstructured Pruning for Maximal Cosine Similarity](https://openreview.net/forum?id=jsvvPVVzwf) |  | 0 | Pruning is an effective method to reduce the size of deep neural network models, maintain accuracy, and, in some cases, improve the network's overall performance. However, the mechanisms underpinning pruning remain unclear. Why can different methods prune by different percentages yet achieve... | Fredrik Dahlqvist, Gabryel MasonWilliams |  |
| 894 |  |  [Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain](https://openreview.net/forum?id=BqEvdOS1Hs) |  | 0 | Existing game AI research mainly focuses on enhancing agents' abilities to win games, but this does not inherently make humans have a better experience when collaborating with these agents. For example, agents may dominate the collaboration and exhibit unintended or detrimental behaviors, leading... | Dehua Zheng, Feiyu Liu, Jing Dai, Lanxiao Huang, Liang Wang, Qiang Fu, Siqin Li, Wei Liu, Wei Yang, Weixuan Wang, Wenhui Chen, Wenjin Yang, Xianliang Wang, Yiming Gao, Zhenjie Lian |  |
| 895 |  |  [Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis](https://openreview.net/forum?id=vY9nzQmQBw) |  | 0 | Recent advancements in neural vocoding are predominantly driven by Generative Adversarial Networks (GANs) operating in the time-domain. While effective, this approach neglects the inductive bias offered by time-frequency representations, resulting in reduntant and computionally-intensive upsampling... | Hubert Siuzdak |  |
| 896 |  |  [An Agnostic View on the Cost of Overfitting in (Kernel) Ridge Regression](https://openreview.net/forum?id=YrTI2Zu0dd) |  | 0 | We study the cost of overfitting in noisy kernel ridge regression (KRR), which we define as the ratio between the test error of the interpolating ridgeless model and the test error of the optimally-tuned model. We take an \`\`agnostic'' view in the following sense: we consider the cost as a... | Gal Vardi, James B. Simon, Lijia Zhou, Nathan Srebro |  |
| 897 |  |  [Explaining Kernel Clustering via Decision Trees](https://openreview.net/forum?id=FAGtjl7HOw) |  | 0 | Despite the growing popularity of explainable and interpretable machine learning, there is still surprisingly limited work on inherently interpretable clustering methods. Recently, there has been a surge of interest in explaining the classic k-means algorithm, leading to efficient algorithms that... | Debarghya Ghoshdastidar, Leena Chennuru Vankadara, Maximilian Fleissner |  |
| 898 |  |  [Provable Benefits of Multi-task RL under Non-Markovian Decision Making Processes](https://openreview.net/forum?id=U6Qulbv2qT) |  | 0 | In multi-task reinforcement learning (RL) under Markov decision processes (MDPs), the presence of shared latent structures among multiple MDPs has been shown to yield significant benefits to the sample efficiency compared to single-task RL. In this paper, we investigate whether such a benefit can... | Jing Yang, Ruiquan Huang, Vincent Tan, Yingbin Liang, Yuan Cheng |  |
| 899 |  |  [Learning Delays in Spiking Neural Networks using Dilated Convolutions with Learnable Spacings](https://openreview.net/forum?id=4r2ybzJnmN) |  | 0 | Spiking Neural Networks (SNNs) are a promising research direction for building power-efficient information processing systems, especially for temporal tasks such as speech recognition. In SNNs, delays refer to the time needed for one spike to travel from one neuron to another. These delays matter... | Ilyass Hammouamri, Ismail Khalfaoui Hassani, Timothée Masquelier |  |
| 900 |  |  [The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"](https://openreview.net/forum?id=GPKTIktA0k) |  | 0 | We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form ''_A_ is _B_'', it will not automatically generalize to the reverse direction ''_B_ is _A_''. This is the \*\*Reversal Curse\*\*. For instance, if a... | Asa Cooper Stickland, Lukas Berglund, Maximilian Kaufmann, Meg Tong, Mikita Balesni, Owain Evans, Tomasz Korbak |  |
| 901 |  |  [AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models](https://openreview.net/forum?id=7Jwpw4qKkb) |  | 0 | The aligned Large Language Models (LLMs) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious... | Chaowei Xiao, Muhao Chen, Nan Xu, Xiaogeng Liu |  |
| 902 |  |  [MixSATGEN: Learning Graph Mixing for SAT Instance Generation](https://openreview.net/forum?id=PXXuLvIH5r) |  | 0 | The Boolean satisfiability problem (SAT) stands as a canonical NP-complete task. In particular, the scarcity of real-world SAT instances and their usefulness for tuning SAT solvers underscore the necessity for effective and efficient ways of hard instance generation, whereas existing methods either... | Junchi Yan, Runzhong Wang, Xinyan Chen, Yang Li |  |
| 903 |  |  [A Stable, Fast, and Fully Automatic Learning Algorithm for Predictive Coding Networks](https://openreview.net/forum?id=RyUvzda8GH) |  | 0 | Predictive coding networks are neuroscience-inspired models with roots in both Bayesian statistics and neuroscience. Training such models, however, is quite inefficient and unstable. In this work, we show how by simply changing the temporal scheduling of the update rule for the synaptic weights... | Beren Millidge, Cornelius Emde, Lei Sha, Rafal Bogacz, Thomas Lukasiewicz, Tommaso Salvatori, Yordan Yordanov, Yuhang Song, Zhenghua Xu |  |
| 904 |  |  [Scalable Monotonic Neural Networks](https://openreview.net/forum?id=DjIsNDEOYX) |  | 0 | In this research, we focus on the problem of learning monotonic neural networks, as preserving the monotonicity of a model with respect to a subset of inputs is crucial for practical applications across various domains. Although several methods have recently been proposed to address this problem,... | Hyunho Kim, JongSeok Lee |  |
| 905 |  |  [Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models](https://openreview.net/forum?id=s2NjWfaYdZ) |  | 0 | Given a pretrained encoder-based language model, how can we accurately compress it without retraining? Retraining-free structured pruning algorithms are crucial in pretrained language model compression due to their significantly reduced pruning cost and capability to prune large language models.... | Hojun Choi, Seungcheol Park, U Kang |  |
| 906 |  |  [PROGRAM: PROtotype GRAph Model based Pseudo-Label Learning for Test-Time Adaptation](https://openreview.net/forum?id=x5LvBK43wg) |  | 0 | Test-time adaptation (TTA) aims to adapt a pre-trained model from a source domain to a target domain only using online unlabeled target data during testing, without accessing to the source data or modifying the original training process. Among the various TTA methods, pseudo-labeling has gained... | Chen Qian, Haopeng Sun, Lumin Xu, Ping Luo, Sheng Jin, Wentao Liu |  |
| 907 |  |  [Navigating Dataset Documentations in AI: A Large-Scale Analysis of Dataset Cards on HuggingFace](https://openreview.net/forum?id=xC8xh2RSs2) |  | 0 | Advances in machine learning are closely tied to the creation of datasets. While data documentation is widely recognized as essential to the reliability, reproducibility, and transparency of ML, we lack a systematic empirical understanding of current dataset documentation practices. To shed light... | James Zou, Weixin Liang, Xinyu Yang |  |
| 908 |  |  [A Differentially Private Clustering Algorithm for Well-Clustered Graphs](https://openreview.net/forum?id=hkSjjs4o5d) |  | 0 | We study differentially private (DP) algorithms for recovering clusters in well-clustered graphs, which are graphs whose vertex set can be partitioned into a small number of sets, each inducing a subgraph of high inner conductance and small outer conductance. Such graphs have widespread application... | Hendrik Fichtenberger, Pan Peng, Weiqiang He |  |
| 909 |  |  [Minimax optimality of convolutional neural networks for infinite dimensional input-output problems and separation from kernel methods](https://openreview.net/forum?id=EW8ZExRZkJ) |  | 0 | Recent deep learning applications, exemplified by text-to-image tasks, often involve high-dimensional inputs and outputs. While several studies have investigated the function estimation capabilities of deep learning, research on dilated convolutional neural networks (CNNs) has mainly focused on... | Taiji Suzuki, Yuto Nishimura |  |
| 910 |  |  [Consistent algorithms for multi-label classification with macro-at-k metrics](https://openreview.net/forum?id=XOnya9gSdF) |  | 0 | We consider the optimization of complex performance metrics in multi-label classification under the population utility framework. We mainly focus on metrics linearly decomposable into a sum of binary classification utilities applied separately to each label with an additional requirement of exactly... | Erik Schultheis, Krzysztof Dembczynski, Marek Wydmuch, Rohit Babbar, Strom Borman, Wojciech Kotlowski |  |
| 911 |  |  [Dynamic Layer Tying for Parameter-Efficient Transformers](https://openreview.net/forum?id=d4uL2MSe0z) |  | 0 | In the pursuit of reducing the number of trainable parameters in deep transformer networks, we employ Reinforcement Learning to dynamically select layers during training and tie them together. Every few iterations, the RL agent is asked whether to train each layer $i$ independently or to copy the... | Lior Wolf, Tamir David Hay |  |
| 912 |  |  [Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion](https://openreview.net/forum?id=ymjI8feDTD) |  | 0 | Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and... | ChiehHsin Lai, Dongjun Kim, Naoki Murata, Stefano Ermon, Toshimitsu Uesaka, WeiHsiang Liao, Yuhta Takida, Yuki Mitsufuji, Yutong He |  |
| 913 |  |  [Dissecting Sample Hardness: A Fine-Grained Analysis of Hardness Characterization Methods for Data-Centric AI](https://openreview.net/forum?id=icTZCUbtD6) |  | 0 | Characterizing samples that are difficult to learn from is crucial to developing highly performant ML models. This has led to numerous Hardness Characterization Methods (HCMs) that aim to identify ''hard'' samples. However, there is a lack of consensus regarding the definition and evaluation of... | Fergus Imrie, Mihaela van der Schaar, Nabeel Seedat |  |
| 914 |  |  [Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency](https://openreview.net/forum?id=1OfAO2mes1) |  | 0 | Modern machine learning (ML) systems demand substantial training data, often resorting to external sources. Nevertheless, this practice renders them vulnerable to backdoor poisoning attacks. Prior backdoor defense strategies have primarily focused on the identification of backdoored models or... | Bingquan Shen, Ren Wang, Sijia Liu, Soumyadeep Pal, Yuguang Yao |  |
| 915 |  |  [Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness Constants](https://openreview.net/forum?id=Ch7WqGcGmb) |  | 0 | Error feedback (EF) is a highly popular and immensely effective mechanism for fixing convergence issues which arise in distributed training methods (such as distributed GD or SGD) when these are enhanced with greedy communication compression techniques such as Top-k. While EF was proposed almost a... | Elnur Gasanov, Konstantin Burlachenko, Peter Richtárik |  |
| 916 |  |  [Deep SE(3)-Equivariant Geometric Reasoning for Precise Placement Tasks](https://openreview.net/forum?id=2inBuwTyL2) |  | 0 | Many robot manipulation tasks can be framed as geometric reasoning tasks, where an agent must be able to precisely manipulate an object into a position that satisfies the task from a set of initial conditions. Often, task success is defined based on the relationship between two objects - for... | Ben Eisner, David Held, Jonathan Scholz, Mel Vecerík, Todor Davchev, Yi Yang |  |
| 917 |  |  [Lemur: Integrating Large Language Models in Automated Program Verification](https://openreview.net/forum?id=Q3YaCghZNt) |  | 0 | The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that demands high-level abstract reasoning about program properties that is challenging for verification tools. We propose a general methodology to... | Clark W. Barrett, Haoze Wu, Nina Narodytska |  |
| 918 |  |  [ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models](https://openreview.net/forum?id=liuqDwmbQJ) |  | 0 | With the ever-increasing popularity of pretrained Video-Language Models (VidLMs), there is a pressing need to develop robust evaluation methodologies that delve deeper into their visio-linguistic capabilities. To address this challenge, we present ViLMA (Video Language Model Assessment), a... | Albert Gatt, Andrea Pedrotti, Anette Frank, Aykut Erdem, Emre Can Acikgoz, Erkut Erdem, Iacer Calixto, Ilker Kesen, Letitia Parcalabescu, Michele Cafagna, Mustafa Dogan |  |
| 919 |  |  [A Precise Characterization of SGD Stability Using Loss Surface Geometry](https://openreview.net/forum?id=UMOlFJzLfL) |  | 0 | Stochastic Gradient Descent (SGD) stands as a cornerstone optimization algorithm with proven real-world empirical successes but relatively limited theoretical understanding. Recent research has illuminated a key factor contributing to its practical efficacy: the implicit regularization it... | Aman Gupta, Ayan Acharya, Borja Ocejo, Gregory Dexter, Rajiv Khanna, S. Sathiya Keerthi |  |
| 920 |  |  [CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models](https://openreview.net/forum?id=86NGO8qeWs) |  | 0 | A fundamental characteristic of audio is its compositional nature. Audio-language models (ALMs) trained using a contrastive approach (e.g., CLAP) that learns a shared representation between audio and language modalities have improved performance in many downstream applications, including zero-shot... | Ashish Seth, Chandra Kiran Reddy Evuru, Dinesh Manocha, Oriol Nieto, Ramaneswaran S., Ramani Duraiswami, Sakshi Singh, Sonal Kumar, Sreyan Ghosh, Utkarsh Tyagi |  |
| 921 |  |  [Hierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMs](https://openreview.net/forum?id=ulaUJFd96G) |  | 0 | Large language models (LLMs) have shown remarkable performance in various natural language processing tasks. However, a primary constraint they face is the context limit, i.e., the maximum number of tokens they can process. Previous works have explored architectural changes and modifications in... | Jaehyung Kim, Jinwoo Shin, JungWoo Ha, Sangwoo Mo, Seunghyuk Oh, Sukmin Yun, Woomin Song |  |
| 922 |  |  [Diffusion in Diffusion: Cyclic One-Way Diffusion for Text-Vision-Conditioned Generation](https://openreview.net/forum?id=ePOjNlOjLC) |  | 0 | Originating from the diffusion phenomenon in physics that describes particle movement, the diffusion generative models inherit the characteristics of stochastic random walk in the data space along the denoising trajectory. However, the intrinsic mutual interference among image regions contradicts... | Ruoyu Wang, Ye Zhu, Yongqi Yang, Yu Wu, Zhihao Qian |  |
| 923 |  |  [Bounding the Expected Robustness of Graph Neural Networks Subject to Node Feature Attacks](https://openreview.net/forum?id=DfPtC8uSot) |  | 0 | Graph Neural Networks (GNNs) have demonstrated state-of-the-art performance in various graph representation learning tasks. Recently, studies revealed their vulnerability to adversarial attacks. In this work, we theoretically define the concept of expected robustness in the context of attributed... | Henrik Boström, Johannes F. Lutzeyer, Michalis Vazirgiannis, Sofiane Ennadir, Yassine Abbahaddou |  |
| 924 |  |  [Score Models for Offline Goal-Conditioned Reinforcement Learning](https://openreview.net/forum?id=oXjnwQLcTA) |  | 0 | Offline Goal-Conditioned Reinforcement Learning (GCRL) is tasked with learning to achieve multiple goals in an environment purely from offline datasets using sparse reward functions. Offline GCRL is pivotal for developing generalist agents capable of leveraging pre-existing datasets to learn... | Ahmed Touati, Alborz Geramifard, Amy Zhang, Harshit Sikchi, Rohan Chitnis, Scott Niekum |  |
| 925 |  |  [Hiding in Plain Sight: Disguising Data Stealing Attacks in Federated Learning](https://openreview.net/forum?id=krx55l2A6G) |  | 0 | Malicious server (MS) attacks have enabled the scaling of data stealing in federated learning to large batch sizes and secure aggregation, settings previously considered private. However, many concerns regarding the client-side detectability of MS attacks were raised, questioning their... | Dimitar Iliev Dimitrov, Kostadin Garov, Martin T. Vechev, Nikola Jovanovic |  |
| 926 |  |  [USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields](https://openreview.net/forum?id=igfDXfMvm5) |  | 0 | Neural Radiance Fields (NeRF) has received much attention recently due to its impressive capability to represent 3D scene and synthesize novel view images. Existing works usually assume that the input images are captured by a global shutter camera. Thus, rolling shutter (RS) images cannot be... | Bangyan Liao, Lingzhe Zhao, Moyang Li, Peidong Liu, Peng Wang |  |
| 927 |  |  [Supervised Knowledge Makes Large Language Models Better In-context Learners](https://openreview.net/forum?id=bAMPOUF227) |  | 0 | Large Language Models (LLMs) exhibit emerging in-context learning abilities through prompt engineering. The recent progress in large-scale generative models has further expanded their use in real-world language applications. However, the critical challenge of improving the generalizability and... | Guangsheng Bao, Jindong Wang, Linyi Yang, Ruochen Xu, Shuibai Zhang, Wei Ye, Weizhu Chen, Xing Xie, Yidong Wang, Yue Zhang, Zhuohao Yu |  |
| 928 |  |  [COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via Probabilistic Circuits](https://openreview.net/forum?id=XN6ZPINdSg) |  | 0 | Conformal prediction has shown spurring performance in constructing statistically rigorous prediction sets for arbitrary black-box machine learning models, assuming the data is exchangeable. However, even small adversarial perturbations during the inference can violate the exchangeability... | Bo Li, Linyi Li, Mintong Kang, Nezihe Merve Gürel |  |
| 929 |  |  [Contrastive Difference Predictive Coding](https://openreview.net/forum?id=0akLDTFR9x) |  | 0 | Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive... | Benjamin Eysenbach, Chongyi Zheng, Ruslan Salakhutdinov |  |
| 930 |  |  [Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment](https://openreview.net/forum?id=LNLjU5C5dK) |  | 0 | Alignment with human preference is a desired property of large language models (LLMs). Currently, the main alignment approach is based on reinforcement learning from human feedback (RLHF). Despite the effectiveness of RLHF, it is intricate to implement and train, thus recent studies explore how to... | Geyang Guo, JiRong Wen, Ranchi Zhao, Tianyi Tang, Xin Zhao |  |
| 931 |  |  [Effective Data Augmentation With Diffusion Models](https://openreview.net/forum?id=ZWzUA9zeAg) |  | 0 | Data augmentation is one of the most prevalent tools in deep learning, underpinning many recent advances, including those from classification, generative models, and representation learning. The standard approach to data augmentation combines simple transformations like rotations and flips to... | Brandon Trabucco, Kyle Doherty, Max Gurinas, Ruslan Salakhutdinov |  |
| 932 |  |  [Towards Transparent Time Series Forecasting](https://openreview.net/forum?id=TYXtXLYHpR) |  | 0 | Transparent machine learning (ML) models are essential for ensuring interpretability and trustworthiness in decision-making systems, particularly in high-stakes domains such as healthcare, finance, and criminal justice. While transparent machine learning models have been proposed for classification... | Krzysztof Kacprzyk, Mihaela van der Schaar, Tennison Liu |  |
| 933 |  |  [A Fast and Provable Algorithm for Sparse Phase Retrieval](https://openreview.net/forum?id=BlkxbI6vzl) |  | 0 | We study the sparse phase retrieval problem, which seeks to recover a sparse signal from a limited set of magnitude-only measurements. In contrast to prevalent sparse phase retrieval algorithms that primarily use first-order methods, we propose an innovative second-order algorithm that employs a... | JianFeng Cai, Jiaxi Ying, Ruixue Wen, Yu Long |  |
| 934 |  |  [MCM: Masked Cell Modeling for Anomaly Detection in Tabular Data](https://openreview.net/forum?id=lNZJyEDxy4) |  | 0 | This paper addresses the problem of anomaly detection in tabular data, which is usually implemented in an one-class classification setting where the training set only contains normal samples. Inspired by the success of masked image/language modeling in vision and natural language domains, we extend... | Jiaxin Yin, Jie Yang, Xiangchao Wang, Yuanyuan Qiao, Zitang Zhou |  |
| 935 |  |  [HiGen: Hierarchical Graph Generative Networks](https://openreview.net/forum?id=KNvubydSB5) |  | 0 | Most real-world graphs exhibit a hierarchical structure, which is often overlooked by existing graph generation methods. To address this limitation, we propose a novel graph generative network that captures the hierarchical nature of graphs and successively generates the graph sub-structures in a... | Mahdi Karami |  |
| 936 |  |  [A Policy Gradient Method for Confounded POMDPs](https://openreview.net/forum?id=8BAkNCqpGW) |  | 0 | In this paper, we propose a policy gradient method for confounded partially observable Markov decision processes (POMDPs) with continuous state and observation spaces in the offline setting. We first establish a novel identification result to non-parametrically estimate any history-dependent policy... | Mao Hong, Yanxun Xu, Zhengling Qi |  |
| 937 |  |  [Achieving Fairness in Multi-Agent MDP Using Reinforcement Learning](https://openreview.net/forum?id=yoVq2BGQdP) |  | 0 | Fairness plays a crucial role in various multi-agent systems (e.g., communication networks, financial markets, etc.). Many multi-agent dynamical interactions can be cast as Markov Decision Processes (MDPs). While existing research has focused on studying fairness in known environments, the... | Arnob Ghosh, Ness B. Shroff, Peizhong Ju |  |
| 938 |  |  [Escape Sky-high Cost: Early-stopping Self-Consistency for Multi-step Reasoning](https://openreview.net/forum?id=ndR8Ytrzhh) |  | 0 | Self-consistency (SC) has been a widely used decoding strategy for chain-of-thought reasoning. Despite bringing significant performance improvements across a variety of multi-step reasoning tasks, it is a high-cost method that requires multiple sampling with the preset size. In this paper, we... | Bin Sun, Boyuan Pan, Heda Wang, Kan Li, Peiwen Yuan, Shaoxiong Feng, Xinglin Wang, Yiwei Li |  |
| 939 |  |  [REBAR: Retrieval-Based Reconstruction for Time-series Contrastive Learning](https://openreview.net/forum?id=3zQo5oUvia) |  | 0 | The success of self-supervised contrastive learning hinges on identifying positive data pairs, such that when they are pushed together in embedding space, the space encodes useful information for subsequent downstream tasks. Constructing positive pairs is non-trivial as the pairing must be similar... | Alexander Moreno, Benjamin M. Marlin, Hui Wei, James Matthew Rehg, Maxwell A. Xu |  |
| 940 |  |  [CoLiDE: Concomitant Linear DAG Estimation](https://openreview.net/forum?id=fGAIgO75dG) |  | 0 | We deal with the combinatorial problem of learning directed acyclic graph (DAG) structure from observational data adhering to a linear structural equation model (SEM). Leveraging advances in differentiable, nonconvex characterizations of acyclicity, recent efforts have advocated a continuous... | Gonzalo Mateos, Mariano Tepper, Seyed Saman Saboksayr |  |
| 941 |  |  [Scaling Convex Neural Networks with Burer-Monteiro Factorization](https://openreview.net/forum?id=ikmuHqugN7) |  | 0 | It has been demonstrated that the training problem for a variety of (non) linear two-layer neural networks (such as two-layer perceptrons, convolutional networks, and self-attention) can be posed as equivalent convex optimization problems, with an induced regularizer which encourages low rank.... | Arda Sahiner, Batu Ozturkler, John M. Pauly, Mert Pilanci, Morteza Mardani, Tolga Ergen |  |
| 942 |  |  [UniTabE: A Universal Pretraining Protocol for Tabular Foundation Model in Data Science](https://openreview.net/forum?id=6LLho5X6xV) |  | 0 | Recent advancements in Natural Language Processing (NLP) have witnessed the groundbreaking impact of pretrained models, yielding impressive outcomes across various tasks. This study seeks to extend the power of pretraining methodologies to facilitating the prediction over tables in data science, a... | Guang Liu, Ledell Wu, Qi Liu, Yazheng Yang, Yuqi Wang |  |
| 943 |  |  [PolyVoice: Language Models for Speech to Speech Translation](https://openreview.net/forum?id=hCrFG9cyuC) |  | 0 | With the huge success of GPT models in natural language processing, there is a growing interest in applying language modeling approaches to speech tasks. Currently, the dominant architecture in speech-to-speech translation (S2ST) remains the encoder-decoder paradigm, creating a need to investigate... | Chen Xu, Fengpeng Yue, Kexin Wang, Lu Lu, Mingxuan Wang, Qi Tian, Qianqian Dong, Siyuan Feng, Tang Li, Tom Ko, Xi Chen, Xuxin Cheng, Ye Bai, Yunlong Zhao, Yuping Wang, Yuxuan Wang, Zejun Ma, Zhiying Huang |  |
| 944 |  |  [Adversarial Feature Map Pruning for Backdoor](https://openreview.net/forum?id=IOEEDkla96) |  | 0 | Deep neural networks have been widely used in many critical applications, such as autonomous vehicles and medical diagnosis. However, their security is threatened by backdoor attacks, which are achieved by adding artificial patterns to specific training data. Existing defense strategies primarily... | Dong Huang, Qingwen Bu |  |
| 945 |  |  [Expressivity of ReLU-Networks under Convex Relaxations](https://openreview.net/forum?id=awHTL3Hpto) |  | 0 | Convex relaxations are a key component of training and certifying provably safe neural networks. However, despite substantial progress, a wide and poorly understood accuracy gap to standard networks remains, raising the question of whether this is due to fundamental limitations of convex... | Mark Niklas Müller, Martin T. Vechev, Maximilian Baader, Yuhao Mao |  |
| 946 |  |  [EmerDiff: Emerging Pixel-level Semantic Knowledge in Diffusion Models](https://openreview.net/forum?id=YqyTXmF8Y2) |  | 0 | Diffusion models have recently received increasing research attention for their remarkable transfer abilities in semantic segmentation tasks. However, generating fine-grained segmentation masks with diffusion models often requires additional training on annotated datasets, leaving it unclear to... | Amirmojtaba Sabour, Koichi Namekata, Sanja Fidler, Seung Wook Kim |  |
| 947 |  |  [CLEX: Continuous Length Extrapolation for Large Language Models](https://openreview.net/forum?id=wXpSidPpc5) |  | 0 | Transformer-based Large Language Models (LLMs) are pioneering advances in many natural language processing tasks, however, their exceptional capabilities are restricted within the preset context window of Transformer. Position Embedding (PE) scaling methods, while effective in extending the context... | Guanzheng Chen, Lidong Bing, Shangsong Liang, Xin Li, Zaiqiao Meng |  |
| 948 |  |  [Implicit Gaussian process representation of vector fields over arbitrary latent manifolds](https://openreview.net/forum?id=YEPlTU5mZC) |  | 0 | Gaussian processes (GPs) are popular nonparametric statistical models for learning unknown functions and quantifying the spatiotemporal uncertainty in data. Recent works have extended GPs to model scalar and vector quantities distributed over non-Euclidean domains, including smooth manifolds,... | Adam Gosztolai, David J. Sharp, Emma Mallas, Matteo VinaoCarl, Michael David, Nir Grossman, Paresh A. Malhotra, Pierre Vandergheynst, Robert L. Peach |  |
| 949 |  |  [Logical Languages Accepted by Transformer Encoders with Hard Attention](https://openreview.net/forum?id=gbrHZq07mq) |  | 0 | We contribute to the study of formal languages that can be recognized by transformer encoders. We focus on two self-attention mechanisms: (1) UHAT (Unique Hard Attention Transformers) and (2) AHAT (Average Hard Attention Transformers). UHAT encoders are known to recognize only languages inside the... | Alexander Kozachinskiy, Anthony Widjaja Lin, Pablo Barceló, Vladimir V. Podolskii |  |
| 950 |  |  [FairSeg: A Large-Scale Medical Image Segmentation Dataset for Fairness Learning Using Segment Anything Model with Fair Error-Bound Scaling](https://openreview.net/forum?id=qNrJJZAKI3) |  | 0 | Fairness in artificial intelligence models has gained significantly more attention in recent years, especially in the area of medicine, as fairness in medical models is critical to people's well-being and lives. High-quality medical fairness datasets are needed to promote fairness learning... | Ava Kouhana, Mengyu Wang, Min Shi, Tobias Elze, Yan Luo, Yu Tian |  |
| 951 |  |  [Customizable Combination of Parameter-Efficient Modules for Multi-Task Learning](https://openreview.net/forum?id=G1Hlubz1fR) |  | 0 | Modular and composable transfer learning is an emerging direction in the field of Parameter Efficient Fine-Tuning, as it enables neural networks to better organize various aspects of knowledge, leading to improved cross-task generalization. In this paper, we introduce a novel approach Customized... | Cong Fan, Congyun Jin, Haowen Wang, Tao Sun, Yibo Fan, Yingbo Wang, Yuliang Du, Yunqi Xu |  |
| 952 |  |  [InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists](https://openreview.net/forum?id=Nu9mOSq7eH) |  | 0 | Recent advances in generative diffusion models have enabled text-controlled synthesis of realistic and diverse images with impressive quality. Despite these remarkable advances, the application of text-to-image generative models in computer vision for standard visual recognition tasks remains... | Ahmed M. Alaa, Alexander Schubert, Anthony Philippakis, Sungwoo Park, Yulu Gan |  |
| 953 |  |  [Mitigating Emergent Robustness Degradation while Scaling Graph Learning](https://openreview.net/forum?id=Koh0i2u8qX) |  | 0 | Although graph neural networks have exhibited remarkable performance in various graph tasks, a significant concern is their vulnerability to adversarial attacks. Consequently, many defense methods have been proposed to alleviate the deleterious effects of adversarial attacks and learn robust graph... | Chunhui Zhang, Chuxu Zhang, Xiangchi Yuan, Yanfang Ye, Yijun Tian |  |
| 954 |  |  [Traveling Waves Encode The Recent Past and Enhance Sequence Learning](https://openreview.net/forum?id=p4S5Z6Sah4) |  | 0 | Traveling waves of neural activity have been observed throughout the brain at a diversity of regions and scales; however, their precise computational role is still debated. One physically inspired hypothesis suggests that the cortical sheet may act like a wave-propagating system capable of... | Lyle Muller, Max Welling, T. Anderson Keller, Terrence J. Sejnowski |  |
| 955 |  |  [Conserve-Update-Revise to Cure Generalization and Robustness Trade-off in Adversarial Training](https://openreview.net/forum?id=6IjN7oxjXt) |  | 0 | Adversarial training improves the robustness of neural networks against adversarial attacks, albeit at the expense of the trade-off between standard and robust generalization. To unveil the underlying factors driving this phenomenon, we examine the layer-wise learning capabilities of neural... | Bahram Zonooz, Elahe Arani, Shruthi Gowda |  |
| 956 |  |  [Hindsight PRIORs for Reward Learning from Human Preferences](https://openreview.net/forum?id=NLevOah0CJ) |  | 0 | Preference based Reinforcement Learning (PbRL) removes the need to hand specify a reward function by learning one from preference feedback over policy behaviors. Current approaches to PbRL do not address the credit assignment problem inherent in determining which parts of a behavior most... | Katherine Metcalf, Mudit Verma |  |
| 957 |  |  [Navigating the Design Space of Equivariant Diffusion-Based Generative Models for De Novo 3D Molecule Generation](https://openreview.net/forum?id=kzGuiRXZrQ) |  | 0 | Deep generative diffusion models are a promising avenue for 3D de novo molecular design in materials science and drug discovery. However, their utility is still limited by suboptimal performance on large molecular structures and limited training data. To address this gap, we explore the design... | DjorkArné Clevert, Frank Noé, Julian Cremer, Kristof T. Schütt, Tuan Le |  |
| 958 |  |  [LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation](https://openreview.net/forum?id=BqHaLnans2) |  | 0 | Following the impressive development of LLMs, vision-language alignment in LLMs is actively being researched to enable multimodal reasoning and visual input/output. This direction of research is particularly relevant to medical imaging because accurate medical image analysis and generation consist... | Jinho Chang, Jong Chul Ye, Suhyeon Lee, Won Jun Kim |  |
| 959 |  |  [Can We Evaluate Domain Adaptation Models Without Target-Domain Labels?](https://openreview.net/forum?id=fszrlQ2DuP) |  | 0 | Unsupervised domain adaptation (UDA) involves adapting a model trained on a label-rich source domain to an unlabeled target domain. However, in real-world scenarios, the absence of target-domain labels makes it challenging to evaluate the performance of UDA models. Furthermore, prevailing UDA... | Hanjie Qian, Jianfei Yang, Kai Wang, Lihua Xie, Yuecong Xu |  |
| 960 |  |  [Global Optimality for Non-linear Constrained Restoration Problems via Invexity](https://openreview.net/forum?id=fyTPWfXtcc) |  | 0 | Signal restoration is an important constrained optimization problem with significant applications in various domains. Although non-convex constrained optimization problems have been shown to perform better than convex counterparts in terms of reconstruction quality, convex constrained optimization... | Jeyan Thiyagalingam, Samuel Pinilla |  |
| 961 |  |  [DOS: Diverse Outlier Sampling for Out-of-Distribution Detection](https://openreview.net/forum?id=iriEqxFB4y) |  | 0 | Modern neural networks are known to give overconfident predictions for out-of-distribution inputs when deployed in the open world. It is common practice to leverage a surrogate outlier dataset to regularize the model during training, and recent studies emphasize the role of uncertainty in designing... | Chongjun Wang, Hao Cheng, Hongxin Wei, Mingcai Chen, Wenyu Jiang |  |
| 962 |  |  [Denoising Task Routing for Diffusion Models](https://openreview.net/forum?id=MY0qlcFcUg) |  | 0 | Diffusion models generate highly realistic images by learning a multi-step denoising process, naturally embodying the principles of multi-task learning (MTL). Despite the inherent connection between diffusion models and MTL, there remains an unexplored area in designing neural architectures that... | Byeongjun Park, Changick Kim, Hyojun Go, JinYoung Kim, Sangmin Woo |  |
| 963 |  |  [Reward Model Ensembles Help Mitigate Overoptimization](https://openreview.net/forum?id=dcjtMYkpXx) |  | 0 | Reinforcement learning from human feedback (RLHF) is a standard approach for fine-tuning large language models to follow instructions. As part of this process, learned reward models are used to approximately model human preferences. However, as imperfect representations of the “true” reward, these... | David Krueger, Robert Kirk, Thomas Coste, Usman Anwar |  |
| 964 |  |  [Frequency-Aware Transformer for Learned Image Compression](https://openreview.net/forum?id=HKGQDDTuvZ) |  | 0 | Learned image compression (LIC) has gained traction as an effective solution for image storage and transmission in recent years. However, existing LIC methods are redundant in latent representation due to limitations in capturing anisotropic frequency components and preserving directional details.... | Chenglin Li, Han Li, Hongkai Xiong, Junni Zou, Shaohui Li, Wenrui Dai |  |
| 965 |  |  [CircuitNet 2.0: An Advanced Dataset for Promoting Machine Learning Innovations in Realistic Chip Design Environment](https://openreview.net/forum?id=nMFSUjxMIl) |  | 0 | Integrated circuits or chips are key to enable computing in modern industry. Designing a chip relies on human experts to produce chip data through professional electronic design automation (EDA) software and complicated procedures. Nowadays, prompted by the wide variety of machine learning (ML)... | Ru Huang, Runsheng Wang, Xun Jiang, Yibo Lin, Yuxiang Zhao, Zhuomin Chai |  |
| 966 |  |  [Leveraging Hyperbolic Embeddings for Coarse-to-Fine Robot Design](https://openreview.net/forum?id=q9jQPA6zPK) |  | 0 | Multi-cellular robot design aims to create robots comprised of numerous cells that can be efficiently controlled to perform diverse tasks. Previous research has demonstrated the ability to generate robots for various tasks, but these approaches often optimize robots directly in the vast design... | Chongjie Zhang, Heng Dong, Junyu Zhang |  |
| 967 |  |  [GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction](https://openreview.net/forum?id=Y3wpuxd7u9) |  | 0 | Large Language Models (LLMs) combined with instruction tuning have made significant progress when generalizing to unseen tasks. However, they have been less successful in Information Extraction (IE), lagging behind task-specific models. Typically, IE tasks are characterized by complex annotation... | Eneko Agirre, German Rigau, Iker GarcíaFerrero, Oier Lopez de Lacalle, Oscar Sainz, Rodrigo Agerri |  |
| 968 |  |  [Understanding the Robustness of Randomized Feature Defense Against Query-Based Adversarial Attacks](https://openreview.net/forum?id=vZ6r9GMT1n) |  | 0 | Recent works have shown that deep neural networks are vulnerable to adversarial examples that find samples close to the original image but can make the model misclassify. Even with access only to the model's output, an attacker can employ black-box attacks to generate such adversarial examples. In... | Khoa D. Doan, KokSeng Wong, Nguyen HungQuang, Tung Pham, Yingjie Lao |  |
| 969 |  |  [Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing](https://openreview.net/forum?id=02f3mUtqnM) |  | 0 | Large language models (LLMs) excel in most NLP tasks but also require expensive cloud servers for deployment due to their size, while smaller models that can be deployed on lower cost (e.g., edge) devices, tend to lag behind in terms of response quality. Therefore in this work we propose a hybrid... | Ahmed Hassan Awadallah, Ankur Mallick, Chi Wang, Dujian Ding, Laks V. S. Lakshmanan, Robert Sim, Subhabrata Mukherjee, Victor Rühle |  |
| 970 |  |  [Identifying Representations for Intervention Extrapolation](https://openreview.net/forum?id=3cuJwmPxXj) |  | 0 | The premise of identifiable and causal representation learning is to improve the current representation learning paradigm in terms of generalizability or robustness. Despite recent progress in questions of identifiability, more theoretical results demonstrating concrete advantages of these methods... | Elan Rosenfeld, Jonas Peters, Niklas Pfister, Pradeep Kumar Ravikumar, Sorawit Saengkyongam |  |
| 971 |  |  [Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model](https://openreview.net/forum?id=j5JvZCaDM0) |  | 0 | Safe offline reinforcement learning is a promising way to bypass risky online interactions towards safe policy learning. Most existing methods only enforce soft constraints, i.e., constraining safety violations in expectation below thresholds predetermined. This can lead to potentially unsafe... | Dongjie Yu, Jianxiong Li, Jingjing Liu, Shengbo Eben Li, Xianyuan Zhan, Yinan Zheng, Yujie Yang |  |
| 972 |  |  [Do Generated Data Always Help Contrastive Learning?](https://openreview.net/forum?id=S5EqslEHnz) |  | 0 | Contrastive Learning (CL) has emerged as one of the most successful paradigms for unsupervised visual representation learning, yet it often depends on intensive manual data augmentations. With the rise of generative models, especially diffusion models, the ability to generate realistic images close... | Jizhe Zhang, Yifei Wang, Yisen Wang |  |
| 973 |  |  [ASMR: Activation-Sharing Multi-Resolution Coordinate Networks for Efficient Inference](https://openreview.net/forum?id=kMp8zCsXNb) |  | 0 | Coordinate network or implicit neural representation (INR) is a fast-emerging method for encoding natural signals (such as images and videos) with the benefits of a compact neural representation. While numerous methods have been proposed to increase the encoding capabilities of an INR, an often... | Jason Chun Lok Li, Le Xu, Ngai Wong, Steven Tin Sui Luo |  |
| 974 |  |  [Exploring Weight Balancing on Long-Tailed Recognition Problem](https://openreview.net/forum?id=JsnR0YO4Fq) |  | 0 | Recognition problems in long-tailed data, in which the sample size per class is heavily skewed, have gained importance because the distribution of the sample size per class in a dataset is generally exponential unless the sample size is intentionally adjusted. Various methods have been devised to... | Issei Sato, Naoya Hasegawa |  |
| 975 |  |  [Zero Bubble (Almost) Pipeline Parallelism](https://openreview.net/forum?id=tuzTN0eIO5) |  | 0 | Pipeline parallelism is one of the key components for large-scale distributed training, yet its efficiency suffers from pipeline bubbles which were deemed inevitable. In this work, we introduce a scheduling strategy that, to our knowledge, is the first to successfully achieve zero pipeline bubbles... | Guangxing Huang, Min Lin, Penghui Qi, Xinyi Wan |  |
| 976 |  |  [CoRe-GD: A Hierarchical Framework for Scalable Graph Visualization with GNNs](https://openreview.net/forum?id=vtyasLn4RM) |  | 0 | Graph Visualization, also known as Graph Drawing, aims to find geometric embeddings of graphs that optimize certain criteria. Stress is a widely used metric; stress is minimized when every pair of nodes is positioned at their shortest path distance. However, stress optimization presents... | Florian Grötschla, Joël Mathys, Robert Veres, Roger Wattenhofer |  |
| 977 |  |  [Compositional Conservatism: A Transductive Approach in Offline Reinforcement Learning](https://openreview.net/forum?id=HRkyLbBRHI) |  | 0 | Offline reinforcement learning (RL) is a compelling framework for learning optimal policies from past experiences without additional interaction with the environment. Nevertheless, offline RL inevitably faces the problem of distributional shifts, where the states and actions encountered during... | Dongwook Lee, Gunhee Kim, Yeda Song |  |
| 978 |  |  [Flow to Better: Offline Preference-based Reinforcement Learning via Preferred Trajectory Generation](https://openreview.net/forum?id=EG68RSznLT) |  | 0 | Offline preference-based reinforcement learning (PbRL) offers an effective solution to overcome the challenges associated with designing rewards and the high costs of online interactions. In offline PbRL, agents are provided with a fixed dataset containing human preferences between pairs of... | Jiaji Zhang, Junyin Ye, TianShuo Liu, Yang Yu, Yihao Sun, Zhilong Zhang |  |
| 979 |  |  [Kernelised Normalising Flows](https://openreview.net/forum?id=iTFdNLHE7k) |  | 0 | Normalising Flows are non-parametric statistical models known for their dual capabilities of density estimation and generation. They are distinguished by their inherently invertible architecture. However, the requirement of invertibility imposes constraints on their expressiveness, necessitating a... | Christoph Lippert, Eshant English, Matthias Kirchler |  |
| 980 |  |  [Topic Modeling as Multi-Objective Contrastive Optimization](https://openreview.net/forum?id=HdAoLSBYXj) |  | 0 | Recent representation learning approaches enhance neural topic models by optimizing the weighted linear combination of the evidence lower bound (ELBO) of the log-likelihood and the contrastive learning objective that contrasts pairs of input documents. However, document-level contrastive learning... | Anh Tuan Luu, CongDuy T. Nguyen, SeeKiong Ng, Thong Thanh Nguyen, Xiaobao Wu, Xinshuai Dong |  |
| 981 |  |  [ED-NeRF: Efficient Text-Guided Editing of 3D Scene With Latent Space NeRF](https://openreview.net/forum?id=9DvDRTTdlu) |  | 0 | Recently, there has been a significant advancement in text-to-image diffusion models, leading to groundbreaking performance in 2D image generation. These advancements have been extended to 3D models, enabling the generation of novel 3D objects from textual descriptions. This has evolved into NeRF... | Gihyun Kwon, Jangho Park, Jong Chul Ye |  |
| 982 |  |  [Learning Personalized Causally Invariant Representations for Heterogeneous Federated Clients](https://openreview.net/forum?id=8FHWkY0SwF) |  | 0 | Personalized federated learning (PFL) has gained great success in tackling the scenarios where target datasets are heterogeneous across the local clients. However, the application of the existing PFL methods to real-world setting is hindered by the common assumption that the test data on each... | Jie Zhang, Jingcai Guo, Song Guo, Xueyang Tang |  |
| 983 |  |  [PAE: Reinforcement Learning from External Knowledge for Efficient Exploration](https://openreview.net/forum?id=R7rZUSGOPD) |  | 0 | Human intelligence is adept at absorbing valuable insights from external knowledge. This capability is equally crucial for artificial intelligence. In contrast, classical reinforcement learning agents lack such capabilities and often resort to extensive trial and error to explore the environment.... | Haofei Lu, Junliang Xing, Renye Yan, Yaozhong Gan, You Wu, Yuanchun Shi, Zhe Wu |  |
| 984 |  |  [Advancing Pose-Guided Image Synthesis with Progressive Conditional Diffusion Models](https://openreview.net/forum?id=rHzapPnCgT) |  | 0 | Recent work has showcased the significant potential of diffusion models in pose-guided person image synthesis. However, owing to the inconsistency in pose between the source and target images, synthesizing an image with a distinct pose, relying exclusively on the source image and target pose... | Cong Wang, Fei Shen, Hu Ye, Jun Zhang, Xiao Han, Yang Wei |  |
| 985 |  |  [One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention](https://openreview.net/forum?id=8p3fu56lKc) |  | 0 | Recent works have empirically analyzed in-context learning and shown that transformers trained on synthetic linear regression tasks can learn to implement ridge regression, which is the Bayes-optimal predictor, given sufficient capacity (Akyurek et al., 2023), while one-layer transformers with... | Arvind V. Mahankali, Tatsunori Hashimoto, Tengyu Ma |  |
| 986 |  |  [Prometheus: Inducing Fine-Grained Evaluation Capability in Language Models](https://openreview.net/forum?id=8euJaTveKw) |  | 0 | Recently, GPT-4 has become the de facto evaluator for long-form text generated by large language models (LLMs). However, for practitioners and researchers with large and custom evaluation tasks, GPT-4 is unreliable due to its closed-source nature, uncontrolled versioning, and prohibitive costs. In... | Hwaran Lee, James Thorne, Jamin Shin, Joel Jang, Minjoon Seo, Sangdoo Yun, Seongjin Shin, Seungone Kim, Shayne Longpre, Sungdong Kim, Yejin Choi |  |
| 987 |  |  [Querying Easily Flip-flopped Samples for Deep Active Learning](https://openreview.net/forum?id=THUBTfSAS2) |  | 0 | Active learning, a paradigm within machine learning, aims to select and query unlabeled data to enhance model performance strategically. A crucial selection strategy leverages the model's predictive uncertainty, reflecting the informativeness of a data point. While the sample's distance to the... | Chang D. Yoo, Gwangsu Kim, Jinwoo Shin, Junghyun Lee, Seong Jin Cho |  |
| 988 |  |  [Attention-based Iterative Decomposition for Tensor Product Representation](https://openreview.net/forum?id=FDb2JQZsFH) |  | 0 | In recent research, Tensor Product Representation (TPR) is applied for the systematic generalization task of deep neural networks by learning the compositional structure of data. However, such prior works show limited performance in discovering and representing the symbolic structure from unseen... | Inchul Choi, Minho Lee, Taewon Park |  |
| 989 |  |  [Efficient Continual Finite-Sum Minimization](https://openreview.net/forum?id=RR70yWYenC) |  | 0 | Given a sequence of functions $f_1,\ldots,f_n$ with $f_i:\mathcal{D}\mapsto \mathbb{R}$, finite-sum minimization seeks a point ${x}^\star \in \mathcal{D}$ minimizing $\sum_{j=1}^nf_j(x)/n$. In this work, we propose a key twist into the finite-sum minimization, dubbed as \*continual finite-sum... | Ioannis Mavrothalassitis, Leello Tadesse Dadi, Stratis Skoulakis, Volkan Cevher |  |
| 990 |  |  [BroGNet: Momentum-Conserving Graph Neural Stochastic Differential Equation for Learning Brownian Dynamics](https://openreview.net/forum?id=2iGiSHmeAN) |  | 0 | Neural networks (NNs) that exploit strong inductive biases based on physical laws and symmetries have shown remarkable success in learning the dynamics of physical systems directly from their trajectory. However, these works focus only on the systems that follow deterministic dynamics, such as... | Jayadeva, N. M. Anoop Krishnan, Sayan Ranu, Suresh Bishnoi |  |
| 991 |  |  [Some Fundamental Aspects about Lipschitz Continuity of Neural Networks](https://openreview.net/forum?id=5jWsW08zUh) |  | 0 | Lipschitz continuity is a crucial functional property of any predictive model, that naturally governs its robustness, generalisation, as well as adversarial vulnerability. Contrary to other works that focus on obtaining tighter bounds and developing different practical strategies to enforce certain... | Grigory Khromov, Sidak Pal Singh |  |
| 992 |  |  [Evaluating Language Model Agency Through Negotiations](https://openreview.net/forum?id=3ZqKxMHcAg) |  | 0 | We introduce an approach to evaluate language model (LM) agency using negotiation games. This approach better reflects real-world use cases and addresses some of the shortcomings of alternative LM benchmarks. Negotiation games enable us to study multi-turn, and cross-model interactions, modulate... | Michal Kosinski, Robert West, Tim R. Davidson, Veniamin Veselovsky |  |
| 993 |  |  [Making Retrieval-Augmented Language Models Robust to Irrelevant Context](https://openreview.net/forum?id=ZS4m74kZpH) |  | 0 | Retrieval-augmented language models (RALMs) hold promise to produce language understanding systems that are are factual, efficient, and up-to-date. An important desideratum of RALMs, is that retrieved information helps model performance when it is relevant, and does not harm performance when it is... | Jonathan Berant, Ori Ram, Ori Yoran, Tomer Wolfson |  |
| 994 |  |  [VersVideo: Leveraging Enhanced Temporal Diffusion Models for Versatile Video Generation](https://openreview.net/forum?id=K9sVJ17zvB) |  | 0 | Creating stable, controllable videos is a complex task due to the need for significant variation in temporal dynamics and cross-frame temporal consistency. To address this, we enhance the spatial-temporal capability and introduce a versatile video generation model, VersVideo, which leverages... | Guanbin Li, Jinxi Xiang, Jun Zhang, Ricong Huang, Xiao Han, Yang Wei |  |
| 995 |  |  [Label-Agnostic Forgetting: A Supervision-Free Unlearning in Deep Models](https://openreview.net/forum?id=SIZWiya7FE) |  | 0 | Machine unlearning aims to remove information derived from forgotten data while preserving that of the remaining dataset in a well-trained model. With the increasing emphasis on data privacy, several approaches to machine unlearning have emerged. However, these methods typically rely on complete... | Alina Bialkowski, Chenhao Zhang, Miao Xu, Shaofei Shen, Weitong Chen, Yawen Zhao |  |
| 996 |  |  [Controlling Vision-Language Models for Multi-Task Image Restoration](https://openreview.net/forum?id=t3vnnLeajU) |  | 0 | Vision-language models such as CLIP have shown great impact on diverse downstream tasks for zero-shot or label-free predictions. However, when it comes to low-level vision such as image restoration their performance deteriorates dramatically due to corrupted inputs. In this paper, we present a... | Fredrik K. Gustafsson, Jens Sjölund, Thomas B. Schön, Zheng Zhao, Ziwei Luo |  |
| 997 |  |  [Yet Another ICU Benchmark: A Flexible Multi-Center Framework for Clinical ML](https://openreview.net/forum?id=ox2ATRM90I) |  | 0 | Medical applications of machine learning (ML) have experienced a surge in popularity in recent years. Given the abundance of available data from electronic health records, the intensive care unit (ICU) is a natural habitat for ML. Models have been proposed to address numerous ICU prediction tasks... | Bert Arnrich, Hendrik Schmidt, Patrick Rockenschaub, Patrick Thoral, Paul W. G. Elbers, Robin Van De Water |  |
| 998 |  |  [Identifying Policy Gradient Subspaces](https://openreview.net/forum?id=iPWxqnt2ke) |  | 0 | Policy gradient methods hold great potential for solving complex continuous control tasks. Still, their training efficiency can be improved by exploiting structure within the optimization problem. Recent work indicates that supervised learning can be accelerated by leveraging the fact that... | Bernhard Schölkopf, Daniel F. B. Haeufle, Dieter Büchler, Jan Schneider, Le Chen, Pierre Schumacher, Simon Guist |  |
| 999 |  |  [Training Graph Transformers via Curriculum-Enhanced Attention Distillation](https://openreview.net/forum?id=j4VMrwgn1M) |  | 0 | Recent studies have shown that Graph Transformers (GTs) can be effective for specific graph-level tasks. However, when it comes to node classification, training GTs remains challenging, especially in semi-supervised settings with a severe scarcity of labeled data. Our paper aims to address this... | Jin Li, Xinlong Chen, YangGeng Fu, Yisong Huang |  |
| 1000 |  |  [Adaptive Stochastic Gradient Algorithm for Black-box Multi-Objective Learning](https://openreview.net/forum?id=bm1JVsVZVu) |  | 0 | Multi-objective optimization (MOO) has become an influential framework for various machine learning problems, including reinforcement learning and multi-task learning. In this paper, we study the black-box multi-objective optimization problem, where we aim to optimize multiple potentially... | Feiyang Ye, Ivor W. Tsang, Xuehao Wang, Yu Zhang, Yueming Lyu |  |
| 1001 |  |  [AgentBench: Evaluating LLMs as Agents](https://openreview.net/forum?id=zAdUB0aCTQ) |  | 0 | The potential of Large Language Model (LLM) as agents has been widely acknowledged recently. Thus, there is an urgent need to quantitatively evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional benchmark that consists of 8 distinct... | Aohan Zeng, Chenhui Zhang, Hanchen Zhang, Hangliang Ding, Hanyu Lai, Hao Yu, Huan Sun, Jie Tang, Kaiwen Men, Kejuan Yang, Minlie Huang, Sheng Shen, Shudan Zhang, Tianjun Zhang, Xiang Deng, Xiao Liu, Xuanyu Lei, Yifan Xu, Yu Gu, Yu Su, Yuxiao Dong, Zhengxiao Du |  |
| 1002 |  |  [Image Background Serves as Good Proxy for Out-of-distribution Data](https://openreview.net/forum?id=ym0ubZrsmm) |  | 0 | Out-of-distribution (OOD) detection empowers the model trained on the closed image set to identify unknown data in the open world. Though many prior techniques have yielded considerable improvements in this research direction, two crucial obstacles still remain. Firstly, a unified perspective has... | Sen Pei |  |
| 1003 |  |  [Differentially Private Synthetic Data via Foundation Model APIs 1: Images](https://openreview.net/forum?id=YEhQs8POIo) |  | 0 | Generating differentially private (DP) synthetic data that closely resembles the original private data is a scalable way to mitigate privacy concerns in the current data-driven world. In contrast to current practices that train customized models for this task, we aim to generate DP Synthetic Data... | Harsha Nori, Janardhan Kulkarni, Sergey Yekhanin, Sivakanth Gopi, Zinan Lin |  |
| 1004 |  |  [Harnessing Joint Rain-/Detail-aware Representations to Eliminate Intricate Rains](https://openreview.net/forum?id=pdJXYfJjz9) |  | 0 | Recent advances in image deraining have focused on training powerful models on mixed multiple datasets comprising diverse rain types and backgrounds. However, this approach tends to overlook the inherent differences among rainy images, leading to suboptimal results. To overcome this limitation, we... | Hao Ren, Hong Lu, Peirong Ma, Wu Ran, Zhiquan He |  |
| 1005 |  |  [Beyond IID weights: sparse and low-rank deep Neural Networks are also Gaussian Processes](https://openreview.net/forum?id=1Wi0Ys33Nm) |  | 0 | The infinitely wide neural network has been proven a useful and manageable mathematical model that enables the understanding of many phenomena appearing in deep learning. One example is the convergence of random deep networks to Gaussian processes that enables a rigorous analysis of the way the... | Alireza Naderi, Jared Tanner, Thiziri Nait Saada |  |
| 1006 |  |  [Understanding Addition in Transformers](https://openreview.net/forum?id=rIx1YXVWZb) |  | 0 | Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper provides a comprehensive analysis of a one-layer Transformer model trained to perform n-digit integer addition. Our findings suggests that the model dissects the task... | Fazl Barez, Philip Quirke |  |
| 1007 |  |  [Beating Price of Anarchy and Gradient Descent without Regret in Potential Games](https://openreview.net/forum?id=36L7W3ri4U) |  | 0 | Arguably one of the thorniest problems in game theory is that of equilibrium selection. Specifically, in the presence of multiple equilibria do self-interested learning dynamics typically select the socially optimal ones? We study a rich class of continuous-time no-regret dynamics in potential... | Georgios Piliouras, Ioannis Panageas, Iosif Sakos, Stefanos Leonardos, Stelios Andrew Stavroulakis, Will Overman |  |
| 1008 |  |  [In defense of parameter sharing for model-compression](https://openreview.net/forum?id=ypAT2ixD4X) |  | 0 | When considering a model architecture, there are several ways to reduce its memory footprint. Historically, popular approaches included selecting smaller architectures and creating sparse networks through pruning. More recently, randomized parameter-sharing (RPS) methods have gained traction for... | Aditya Desai, Anshumali Shrivastava |  |
| 1009 |  |  [Kill Two Birds with One Stone: Rethinking Data Augmentation for Deep Long-tailed Learning](https://openreview.net/forum?id=RzY9qQHUXy) |  | 0 | Real-world tasks are universally associated with training samples that exhibit a long-tailed class distribution, and traditional deep learning models are not suitable for fitting this distribution, thus resulting in a biased trained model. To surmount this dilemma, massive deep long-tailed learning... | Binwu Wang, Kun Wang, Pengkun Wang, Wei Xu, Xu Wang, Yang Wang, Yudong Zhang |  |
| 1010 |  |  [Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents](https://openreview.net/forum?id=MCNqgUFTHI) |  | 0 | Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes... | SeeKiong Ng, TatSeng Chua, Wai Lam, Wenxuan Zhang, Yang Deng |  |
| 1011 |  |  [Bootstrapping Variational Information Pursuit with Large Language and Vision Models for Interpretable Image Classification](https://openreview.net/forum?id=9bmTbVaA2A) |  | 0 | Variational Information Pursuit (V-IP) is an interpretable-by-design framework that makes predictions by sequentially selecting a short chain of user-defined, interpretable queries about the data that are most informative for the task. The prediction is based solely on the obtained query answers,... | Aditya Chattopadhyay, Kwan Ho Ryan Chan, René Vidal |  |
| 1012 |  |  [Contextual Bandits with Online Neural Regression](https://openreview.net/forum?id=5ep85sakT3) |  | 0 | Recent works have shown a reduction from contextual bandits to online regression under a realizability assumption (Foster and Rakhlin, 2020; Foster and Krishnamurthy, 2021). In this work, we investigate the use of neural networks for such online regression and associated Neural Contextual Bandits... | Arindam Banerjee, Jingrui He, Rohan Deb, Shiliang Zuo, Yikun Ban |  |
| 1013 |  |  [Evaluating Large Language Models at Evaluating Instruction Following](https://openreview.net/forum?id=tr0KidwPLc) |  | 0 | As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever increasing list of models. This paper investigates the efficacy of these “LLM evaluators”, particularly in... | Danqi Chen, Jiatong Yu, Tanya Goyal, Tianyu Gao, Yu Meng, Zhiyuan Zeng |  |
| 1014 |  |  [How to Fine-Tune Vision Models with SGD](https://openreview.net/forum?id=ZTssMmhC2X) |  | 0 | SGD and AdamW are the two most used optimizers for fine-tuning large neural networks in computer vision. When the two methods perform the same, SGD is preferable because it uses less memory (12 bytes/parameter with momentum and 8 bytes/parameter without) than AdamW (16 bytes/parameter). However, on... | Ananya Kumar, Ruoqi Shen, Suriya Gunasekar, Sébastien Bubeck |  |
| 1015 |  |  [Backdoor Contrastive Learning via Bi-level Trigger Optimization](https://openreview.net/forum?id=oxjeePpgSP) |  | 0 | Contrastive Learning (CL) has attracted enormous attention due to its remarkable capability in unsupervised representation learning. However, recent works have revealed the vulnerability of CL to backdoor attacks: the feature extractor could be misled to embed backdoored data close to an attack... | Hao Lu, Jinghui Chen, Lu Lin, Ting Wang, Weiyu Sun, Xinyu Zhang, YingCong Chen |  |
| 1016 |  |  [PARL: A Unified Framework for Policy Alignment in Reinforcement Learning from Human Feedback](https://openreview.net/forum?id=ByR3NdDSZB) |  | 0 | We present a novel unified bilevel optimization-based framework, \textsf{PARL}, formulated to address the recently highlighted critical issue of policy alignment in reinforcement learning using utility or preference-based feedback. We identify a major gap within current algorithmic designs for... | Alec Koppel, Amrit Singh Bedi, Dinesh Manocha, Furong Huang, Huazheng Wang, Mengdi Wang, Souradip Chakraborty |  |
| 1017 |  |  [SafeDreamer: Safe Reinforcement Learning with World Models](https://openreview.net/forum?id=tsE5HLYtYg) |  | 0 | The deployment of Reinforcement Learning (RL) in real-world applications is constrained by its failure to satisfy safety criteria. Existing Safe Reinforcement Learning (SafeRL) methods, which rely on cost functions to enforce safety, often fail to achieve zero-cost performance in complex scenarios,... | Borong Zhang, Chunhe Xia, Jiaming Ji, Weidong Huang, Yaodong Yang |  |
| 1018 |  |  [MetaCoCo: A New Few-Shot Classification Benchmark with Spurious Correlation](https://openreview.net/forum?id=DiWRG9JTWZ) |  | 0 | Out-of-distribution (OOD) problems in few-shot classification (FSC) occur when novel classes sampled from testing distributions differ from base classes drawn from training distributions, which considerably degrades the performance of deep learning models deployed in real-world applications. Recent... | Fei Wu, Haoxuan Li, Kun Kuang, Min Zhang |  |
| 1019 |  |  [Whittle Index with Multiple Actions and State Constraint for Inventory Management](https://openreview.net/forum?id=5sixirvG0I) |  | 0 | Whittle index is a heuristic tool that leads to good performance for the restless bandits problem. In this paper, we extend Whittle index to a new multi-agent reinforcement learning (MARL) setting with multiple discrete actions and a possibly changing constraint on the state space, resulting in... | Chuheng Zhang, Jiang Bian, Lei Song, Siwei Wang, Wei Jiang, Xiangsen Wang, Xianliang Yang |  |
| 1020 |  |  [Looped Transformers are Better at Learning Learning Algorithms](https://openreview.net/forum?id=HHbRxoDTxE) |  | 0 | Transformers have demonstrated effectiveness in in-context solving data-fitting problems from various (latent) models, as reported by Garg et al. (2022). However, the absence of an inherent iterative structure in the transformer architecture presents a challenge in emulating the iterative... | Dimitris Papailiopoulos, Kangwook Lee, Liu Yang, Robert D. Nowak |  |
| 1021 |  |  [Sum-Product-Set Networks: Deep Tractable Models for Tree-Structured Graphs](https://openreview.net/forum?id=mF3cTns4pe) |  | 0 | Daily internet communication relies heavily on tree-structured graphs, embodied by popular data formats such as XML and JSON. However, many recent generative (probabilistic) models utilize neural networks to learn a probability distribution over undirected cyclic graphs. This assumption of a... | Martin Rektoris, Milan Papez, Tomás Pevný, Václav Smídl |  |
| 1022 |  |  [Learning Thresholds with Latent Values and Censored Feedback](https://openreview.net/forum?id=qaKRfobbTg) |  | 0 | In this paper, we investigate a problem of \*actively\* learning threshold in latent space, where the \*unknown\* reward $g(\gamma, v)$ depends on the proposed threshold $\gamma$ and latent value $v$ and it can be $only$ achieved if the threshold is lower than or equal to the \*unknown\* latent... | Jiahao Zhang, Tao Lin, Weiqiang Zheng, Xiaotie Deng, Yifeng Teng, Zhe Feng |  |
| 1023 |  |  [Is attention required for ICL? Exploring the Relationship Between Model Architecture and In-Context Learning Ability](https://openreview.net/forum?id=Qwq4cpLtoX) |  | 0 | What is the relationship between model architecture and the ability to perform in-context learning? In this empirical study, we take the first steps toward answering this question. We evaluate thirteen model architectures capable of causal language modeling across a suite of synthetic in-context... | Ivan Lee, Nan Jiang, Taylor BergKirkpatrick |  |
| 1024 |  |  [Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?](https://openreview.net/forum?id=nJnky5K944) |  | 0 | Existing analyses of the expressive capacity of Transformer models have required excessively deep layers for data memorization, leading to a discrepancy with the Transformers actually used in practice. This is primarily due to the interpretation of the softmax function as an approximation of the... | Issei Sato, Tokio Kajitsuka |  |
| 1025 |  |  [Language-Interfaced Tabular Oversampling via Progressive Imputation and Self-Authentication](https://openreview.net/forum?id=8F6bws5JBy) |  | 0 | Tabular data in the wild are frequently afflicted with class-imbalance, biasing machine learning model predictions towards major classes. A data-centric solution to this problem is oversampling - where the classes are balanced by adding synthetic minority samples via generative methods. However,... | Eunho Yang, Geondo Park, Hyeongwon Jang, Joowon Kim, June Yong Yang |  |
| 1026 |  |  [Certified Adversarial Robustness for Rate Encoded Spiking Neural Networks](https://openreview.net/forum?id=5bNYf0CqxY) |  | 0 | The spiking neural networks are inspired by the biological neurons that employ binary spikes to propagate information in the neural network. It has garnered considerable attention as the next-generation neural network, as the spiking activity simplifies the computation burden of the network to a... | Bhaskar Mukhoty, Bin Gu, Giulia De Masi, Hilal AlQuabeh, Huan Xiong |  |
| 1027 |  |  [Fake It Till Make It: Federated Learning with Consensus-Oriented Generation](https://openreview.net/forum?id=NY3wMJuaLf) |  | 0 | In federated learning (FL), data heterogeneity is one key bottleneck that causes model divergence and limits performance. Addressing this, existing methods often regard data heterogeneity as an inherent property and propose to mitigate its adverse effects by correcting models. In this paper, we... | Rui Ye, Siheng Chen, Yanfeng Wang, Yaxin Du, Zhenyang Ni |  |
| 1028 |  |  [SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction](https://openreview.net/forum?id=FNq3nIvP4F) |  | 0 | Recently video generation has achieved substantial progress with realistic results. Nevertheless, existing AI-generated videos are usually very short clips ("shot-level'') depicting a single scene. To deliver a coherent long video ("story-level''), it is desirable to have creative transition and... | Dahua Lin, Jiashuo Yu, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Xinyuan Chen, Yali Wang, Yaohui Wang, Yu Qiao, Ziwei Liu |  |
| 1029 |  |  [Explaining Time Series via Contrastive and Locally Sparse Perturbations](https://openreview.net/forum?id=qDdSRaOiyb) |  | 0 | Explaining multivariate time series is a compound challenge, as it requires identifying important locations in the time series and matching complex temporal patterns. Although previous saliency-based methods addressed the challenges, their perturbation may not alleviate the distribution shift... | Chunlin Chen, Dongsheng Luo, Lunting Fan, Mengnan Du, Min Wu, Qingsong Wen, Tianchun Wang, Yi Wang, Yingying Zhang, Zefan Wang, Zichuan Liu |  |
| 1030 |  |  [GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking](https://openreview.net/forum?id=VJvbOSXRUq) |  | 0 | Numerous explainability methods have been proposed to shed light on the inner workings of GNNs. Despite the inclusion of empirical evaluations in all the proposed algorithms, the interrogative aspects of these evaluations lack diversity. As a result, various facets of explainability pertaining to... | Ambuj K. Singh, Burouj Armgaan, Khushbu Pahwa, Mert Kosan, Samidha Verma, Sayan Ranu, Sourav Medya |  |
| 1031 |  |  [Grounded Object-Centric Learning](https://openreview.net/forum?id=pBxeZ6pVUD) |  | 0 | The extraction of object-centric representations for downstream tasks is an emerging area of research. Learning grounded representations of objects that are guaranteed to be stable and invariant promises robust performance across different tasks and environments. Slot Attention (SA) learns... | Avinash Kori, Ben Glocker, Fabio De Sousa Ribeiro, Francesca Toni, Francesco Locatello |  |
| 1032 |  |  [On the Stability of Expressive Positional Encodings for Graphs](https://openreview.net/forum?id=xAqcJ9XoTf) |  | 0 | Designing effective positional encodings for graphs is key to building powerful graph transformers and enhancing message-passing graph neural networks. Although widespread, using Laplacian eigenvectors as positional encodings faces two fundamental challenges: (1) \*Non-uniqueness\*: there are many... | Joshua Robinson, Muhan Zhang, Pan Li, Stefanie Jegelka, William Lu, Yinan Huang, Yu Yang |  |
| 1033 |  |  [Dynamic Neural Response Tuning](https://openreview.net/forum?id=HiTg16qhxp) |  | 0 | Artificial Neural Networks (ANNs) have gained widespread applications across various areas in recent years. The ANN design was initially inspired by principles of biology. The biological neural network's fundamental response process comprises information transmission and aggregation. The... | Lin Chen, Linyun Zhou, Mingli Song, Tian Qiu, Wenxiang Xu, Zunlei Feng |  |
| 1034 |  |  [Confidence-aware Reward Optimization for Fine-tuning Text-to-Image Models](https://openreview.net/forum?id=Let8OMe20n) |  | 0 | Fine-tuning text-to-image models with reward functions trained on human feedback data has proven effective for aligning model behavior with human intent. However, excessive optimization with such reward models, which serve as mere proxy objectives, can compromise the performance of fine-tuned... | Jinwoo Shin, Jongheon Jeong, Kimin Lee, Krishnamurthy Dj Dvijotham, Kyuyoung Kim, Minyong An, Mohammad Ghavamzadeh |  |
| 1035 |  |  [Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts](https://openreview.net/forum?id=jvtmdK69KQ) |  | 0 | Top-K sparse softmax gating mixture of experts has been widely used for scaling up massive deep-learning architectures without increasing the computational cost. Despite its popularity in real-world applications, the theoretical understanding of that gating function has remained an open problem.... | Fanqi Yan, Huy Nguyen, Nhat Ho, Pedram Akbarian |  |
| 1036 |  |  [The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Language Models](https://openreview.net/forum?id=SQGUDc9tC8) |  | 0 | Pre-trained Language models (PLMs) have been acknowledged to contain harmful information, such as social biases, which may cause negative social impacts or even bring catastrophic results in application. Previous works on this problem mainly focused on using black-box methods such as probing to... | Daoguang Zan, MinYen Kan, PinYu Chen, TsungYi Ho, Xiaokang Chen, Yan Liu, Yu Liu |  |
| 1037 |  |  [Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation and Human Feedback](https://openreview.net/forum?id=vW1SkPl4kp) |  | 0 | Risk-sensitive reinforcement learning (RL) aims to optimize policies that balance the expected reward and risk. In this paper, we present a novel risk-sensitive RL framework that employs an Iterated Conditional Value-at-Risk (CVaR) objective under both linear and general function approximations,... | Desheng Wu, Longbo Huang, Pihe Hu, Siwei Wang, Yihan Du, Yu Chen |  |
| 1038 |  |  [Enhancing Transfer Learning with Flexible Nonparametric Posterior Sampling](https://openreview.net/forum?id=vSwu81S33z) |  | 0 | Transfer learning has recently shown significant performance across various tasks involving deep neural networks. In these transfer learning scenarios, the prior distribution for downstream data becomes crucial in Bayesian model averaging (BMA). While previous works proposed the prior over the... | Edwin Fong, Giung Nam, Hyungi Lee, Juho Lee |  |
| 1039 |  |  [ToolChain\*: Efficient Action Space Navigation in Large Language Models with A\* Search](https://openreview.net/forum?id=B6pQxqUcT8) |  | 0 | Large language models (LLMs) have demonstrated powerful decision-making and planning capabilities in solving complicated real-world problems. LLM-based autonomous agents can interact with diverse tools (e.g., functional APIs) and generate solution plans that execute a series of API function calls... | Chao Zhang, Ryan A. Rossi, Saayan Mitra, Somdeb Sarkhel, Tong Yu, Victor S. Bursztyn, Xiang Chen, Yuchen Zhuang |  |
| 1040 |  |  [Ensemble Distillation for Unsupervised Constituency Parsing](https://openreview.net/forum?id=RR8y0WKrFv) |  | 0 | We investigate the unsupervised constituency parsing task, which organizes words and phrases of a sentence into a hierarchical structure without using linguistically annotated data. We observe that existing unsupervised parsers capture different aspects of parsing structures, which can be leveraged... | Behzad Shayegh, Jackie C. K. Cheung, Lili Mou, Xiaodan Zhu, Yanshuai Cao |  |
| 1041 |  |  [What Algorithms can Transformers Learn? A Study in Length Generalization](https://openreview.net/forum?id=AssIuHnmHX) |  | 0 | Large language models exhibit surprising emergent generalization properties, yet also struggle on many simple reasoning tasks such as arithmetic and parity. In this work, we focus on length generalization, and we propose a unifying framework to understand when and how Transformers can be expected... | Arwen Bradley, Etai Littwin, Hattie Zhou, Joshua M. Susskind, Noam Razin, Omid Saremi, Preetum Nakkiran, Samy Bengio |  |
| 1042 |  |  [Beyond Vanilla Variational Autoencoders: Detecting Posterior Collapse in Conditional and Hierarchical Variational Autoencoders](https://openreview.net/forum?id=4zZFGliCl9) |  | 0 | The posterior collapse phenomenon in variational autoencoder (VAE), where the variational posterior distribution closely matches the prior distribution, can hinder the quality of the learned latent variables. As a consequence of posterior collapse, the latent variables extracted by the encoder in... | Hien Dang, Nhat Ho, Tan Minh Nguyen, Tho Tran Huu |  |
| 1043 |  |  [Training-free Multi-objective Diffusion Model for 3D Molecule Generation](https://openreview.net/forum?id=X41c4uB4k0) |  | 0 | Searching for novel and diverse molecular candidates is a critical undertaking in drug and material discovery. Existing approaches have successfully adapted the diffusion model, the most effective generative model in image generation, to create 1D SMILES strings, 2D chemical graphs, or 3D molecular... | Caihua Shan, Can Xu, Dongsheng Li, Han Yang, Xiang Li, Xu Han, Yifei Shen |  |
| 1044 |  |  [Neuro-Inspired Information-Theoretic Hierarchical Perception for Multimodal Learning](https://openreview.net/forum?id=Z9AZsU1Tju) |  | 0 | Integrating and processing information from various sources or modalities are critical for obtaining a comprehensive and accurate perception of the real world in autonomous systems and cyber-physical systems. Drawing inspiration from neuroscience, we develop the Information-Theoretic Hierarchical... | Defu Cao, Gaurav Gupta, Gengshuo Liu, Mingxi Cheng, Paul Bogdan, Shixuan Li, Tianqing Fang, Xiongye Xiao, Yaxing Li |  |
| 1045 |  |  [Flag Aggregator: Scalable Distributed Training under Failures and Augmented Losses using Convex Optimization](https://openreview.net/forum?id=7avlrpzWqo) |  | 0 | Modern ML applications increasingly rely on complex deep learning models and large datasets. There has been an exponential growth in the amount of computation needed to train the largest models. Therefore, to scale computation and data, these models are inevitably trained in a distributed manner in... | Balajee Vamanan, Hamidreza Almasi, Harsh Mishra, Sathya N. Ravi |  |
| 1046 |  |  [DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models](https://openreview.net/forum?id=9m02ib92Wz) |  | 0 | Quantifying the impact of training data points is crucial for understanding the outputs of machine learning models and for improving the transparency of the AI pipeline. The influence function is a principled and popular data attribution method, but its computational cost often makes it challenging... | Eric Wu, James Zou, Kevin Wu, Yongchan Kwon |  |
| 1047 |  |  [Understanding Domain Generalization: A Noise Robustness Perspective](https://openreview.net/forum?id=I2mIxuXA72) |  | 0 | Despite the rapid development of machine learning algorithms for domain generalization (DG), there is no clear empirical evidence that the existing DG algorithms outperform the classic empirical risk minimization (ERM) across standard benchmarks. To better understand this phenomenon, we investigate... | Bryan Kian Hsiang Low, Rui Qiao |  |
| 1048 |  |  [Non-negative Contrastive Learning](https://openreview.net/forum?id=lNCnZwcH5Z) |  | 0 | Deep representations have shown promising performance when transferred to downstream tasks in a black-box manner. Yet, their inherent lack of interpretability remains a significant challenge, as these features are often opaque to human understanding. In this paper, we propose Non-negative... | Qi Zhang, Yaoyu Guo, Yifei Wang, Yisen Wang |  |
| 1049 |  |  [Image Clustering Conditioned on Text Criteria](https://openreview.net/forum?id=G2cG3mQqop) |  | 0 | Classical clustering methods do not provide users with direct control of the clustering results, and the clustering results may not be consistent with the relevant criterion that a user has in mind. In this work, we present a new methodology for performing image clustering based on user-specified... | Ernest K. Ryu, Jaeseung Park, Jaewoong Cho, Kangwook Lee, Minkyu Kim, Sehyun Kwon |  |
| 1050 |  |  [Correlated Noise Provably Beats Independent Noise for Differentially Private Learning](https://openreview.net/forum?id=xHmCdSArUC) |  | 0 | Differentially private learning algorithms inject noise into the learning process. While the most common private learning algorithm, DP-SGD, adds independent Gaussian noise in each iteration, recent work on matrix factorization mechanisms has shown empirically that introducing correlations in the... | Abhradeep Guha Thakurta, Arun Ganesh, Christopher A. ChoquetteChoo, Krishna Pillutla, Krishnamurthy Dj Dvijotham, Thomas Steinke |  |
| 1051 |  |  [Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models](https://openreview.net/forum?id=6bcAD6g688) |  | 0 | Language models have shown promise in various tasks but can be affected by undesired data during training, fine-tuning, or alignment. For example, if some unsafe conversations are wrongly annotated as safe ones, the model fine-tuned on these samples may be harmful. Therefore, the correctness of... | Hao Cheng, Jialu Wang, Yang Liu, Zhaowei Zhu |  |
| 1052 |  |  [Understanding Expressivity of GNN in Rule Learning](https://openreview.net/forum?id=43cYe4oogi) |  | 0 | Rule learning is critical to improving knowledge graph (KG) reasoning due to their ability to provide logical and interpretable explanations. Recently, Graph Neural Networks (GNNs) with tail entity scoring achieve the state-of-the-art performance on KG reasoning. However, the theoretical... | Haiquan Qiu, Quanming Yao, Yong Li, Yongqi Zhang |  |
| 1053 |  |  [COLLIE: Systematic Construction of Constrained Text Generation Tasks](https://openreview.net/forum?id=kxgSlyirUZ) |  | 0 | Text generation under constraints have seen increasing interests in natural language processing, especially with the rapidly improving capabilities of large language models. However, existing benchmarks for constrained generation usually focus on fixed constraint types (e.g. generate a sentence... | Austin W. Hanjie, Howard Chen, Karthik R. Narasimhan, Runzhe Yang, Shunyu Yao |  |
| 1054 |  |  [GENOME: Generative Neuro-Symbolic Visual Reasoning by Growing and Reusing Modules](https://openreview.net/forum?id=MNShbDSxKH) |  | 0 | Recent works have shown that Large Language Models (LLMs) could empower traditional neuro-symbolic models via programming capabilities to translate languages into module descriptions, thus achieving strong visual reasoning results while maintaining the model’s transparency and efficiency. However,... | Chuang Gan, Rui Sun, Wenjun Liu, Yining Hong, Zhenfang Chen |  |
| 1055 |  |  [Vanishing Gradients in Reinforcement Finetuning of Language Models](https://openreview.net/forum?id=IcVNBR7qZi) |  | 0 | Pretrained language models are commonly aligned with human preferences and downstream tasks via reinforcement finetuning (RFT), which refers to maximizing a (possibly learned) reward function using policy gradient algorithms. This work identifies a fundamental optimization obstacle in RFT: we prove... | Arwen Bradley, Etai Littwin, Hattie Zhou, Joshua M. Susskind, Noam Razin, Omid Saremi, Preetum Nakkiran, Vimal Thilak |  |
| 1056 |  |  [Hyper Evidential Deep Learning to Quantify Composite Classification Uncertainty](https://openreview.net/forum?id=A7t7z6g6tM) |  | 0 | Deep neural networks (DNNs) have been shown to perform well on exclusive, multi-class classification tasks. However, when different classes have similar visual features, it becomes challenging for human annotators to differentiate them. When an image is ambiguous, such as a blurry one where an... | Audun Jøsang, Changbin Li, Dong Hyun Jeong, Feng Chen, JinHee Cho, Kangshuo Li, Lance M. Kaplan, Yuzhe Ou |  |
| 1057 |  |  [Goodhart's Law in Reinforcement Learning](https://openreview.net/forum?id=5o9G4XF1LI) |  | 0 | Implementing a reward function that perfectly captures a complex task in the real world is impractical. As a result, it is often appropriate to think of the reward function as a \*proxy\* for the true objective rather than as its definition. We study this phenomenon through the lens of \*Goodhart’s... | Charlie Griffin, Jacek Karwowski, Joar Max Viktor Skalse, Klaus Kiendlhofer, Oliver Hayman, Xingjian Bai |  |
| 1058 |  |  [Score Regularized Policy Optimization through Diffusion Behavior](https://openreview.net/forum?id=xCRr9DrolJ) |  | 0 | Recent developments in offline reinforcement learning have uncovered the immense potential of diffusion modeling, which excels at representing heterogeneous behavior policies. However, sampling from diffusion policies is considerably slow because it necessitates tens to hundreds of iterative... | Cheng Lu, Hang Su, Huayu Chen, Jun Zhu, Zhengyi Wang |  |
| 1059 |  |  [Robustifying and Boosting Training-Free Neural Architecture Search](https://openreview.net/forum?id=qPloNoDJZn) |  | 0 | Neural architecture search (NAS) has become a key component of AutoML and a standard tool to automate the design of deep neural networks. Recently, training-free NAS as an emerging paradigm has successfully reduced the search costs of standard training-based NAS by estimating the true architecture... | Bryan Kian Hsiang Low, Yao Shu, Zhenfeng He, Zhongxiang Dai |  |
| 1060 |  |  [Concept Bottleneck Generative Models](https://openreview.net/forum?id=L9U5MJJleF) |  | 0 | We introduce a generative model with an intrinsically interpretable layer---a concept bottleneck layer---that constrains the model to encode human-understandable concepts. The concept bottleneck layer partitions the generative model into three parts: the pre-concept bottleneck portion, the CB... | Aya Abdelsalam Ismail, Héctor Corrada Bravo, Julius Adebayo, Kyunghyun Cho, Stephen Ra |  |
| 1061 |  |  [MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction Following](https://openreview.net/forum?id=1vrS1zwekw) |  | 0 | In the realm of large language models (LLMs), enhancing instruction-following capability often involves curating expansive training data. This is achieved through two primary schemes: i) Scaling-Inputs: Amplifying (input, output) pairs per task instruction, aiming for better instruction adherence.... | Hanzi Xu, Janice Ahn, Jian Xie, Kai Zhang, Renze Lou, Wenpeng Yin, Yu Su, Yuxuan Sun |  |
| 1062 |  |  [Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text Guidance](https://openreview.net/forum?id=2JF8mJRJ7M) |  | 0 | Large-scale contrastive vision-language pre-trained models provide the zero-shot model achieving competitive performance across a range of image classification tasks without requiring training on downstream data. Recent works have confirmed that while additional fine-tuning of the zero-shot model... | Byeongho Heo, Giung Nam, Juho Lee |  |
| 1063 |  |  [Generative Modeling of Regular and Irregular Time Series Data via Koopman VAEs](https://openreview.net/forum?id=eY7sLb0dVF) |  | 0 | Generating realistic time series data is important for many engineering and scientific applications. Existing work tackles this problem using generative adversarial networks (GANs). However, GANs are unstable during training, and they can suffer from mode collapse. While variational autoencoders... | Ilan Naiman, Michael W. Mahoney, N. Benjamin Erichson, Omri Azencot, Pu Ren |  |
| 1064 |  |  [Generating Pragmatic Examples to Train Neural Program Synthesizers](https://openreview.net/forum?id=yxKZGQLzOP) |  | 0 | Programming-by-example is the task of synthesizing a program that is consistent with a set of user-provided input-output examples. As examples are often an under-specification of one's intent, a good synthesizer must choose the intended program from the many that are consistent with the given set... | Daniel Fried, Saujas Vaduguru, Yewen Pu |  |
| 1065 |  |  [Making RL with Preference-based Feedback Efficient via Randomization](https://openreview.net/forum?id=Pe2lo3QOvo) |  | 0 | Reinforcement Learning algorithms that learn from human feedback (RLHF) need to be efficient in terms of \*statistical complexity, computational complexity, and query complexity\*. In this work, we consider the RLHF setting where the feedback is given in the format of preferences over pairs of... | Runzhe Wu, Wen Sun |  |
| 1066 |  |  [Adaptive Regret for Bandits Made Possible: Two Queries Suffice](https://openreview.net/forum?id=AY9KyTGcnk) |  | 0 | Fast changing states or volatile environments pose a significant challenge to online optimization, which needs to perform rapid adaptation under limited observation. In this paper, we give query and regret optimal bandit algorithms under the strict notion of strongly adaptive regret, which measures... | David P. Woodruff, Elad Hazan, Fred Zhang, Qiuyi Zhang, Xinyi Chen, Zhou Lu |  |
| 1067 |  |  [Negatively Correlated Ensemble Reinforcement Learning for Online Diverse Game Level Generation](https://openreview.net/forum?id=iAW2EQXfwb) |  | 0 | Deep reinforcement learning has recently been successfully applied to online procedural content generation in which a policy determines promising game-level segments. However, existing methods can hardly discover diverse level patterns, while the lack of diversity makes the gameplay boring. This... | Chengpeng Hu, Jialin Liu, Xin Yao, Ziqi Wang |  |
| 1068 |  |  [Achieving Sample and Computational Efficient Reinforcement Learning by Action Space Reduction via Grouping](https://openreview.net/forum?id=MOmqfJovQ6) |  | 0 | Reinforcement learning often needs to deal with the exponential growth of states and actions when exploring optimal control in high-dimensional spaces (often known as the curse of dimensionality). In this work, we address this issue by learning the inherent structure of action-wise similar MDP to... | Ness B. Shroff, Peizhong Ju, Yining Li |  |
| 1069 |  |  [Learning Hierarchical Polynomials with Three-Layer Neural Networks](https://openreview.net/forum?id=QgwAYFrh9t) |  | 0 | We study the problem of learning hierarchical polynomials over the standard Gaussian distribution with three-layer neural networks. We specifically consider target functions of the form $h = g \circ p$ where $p : \mathbb{R}^d \rightarrow \mathbb{R}$ is a degree $k$ polynomial and $g: \mathbb{R}... | Eshaan Nichani, Jason D. Lee, Zihao Wang |  |
| 1070 |  |  [Learning Grounded Action Abstractions from Language](https://openreview.net/forum?id=qJ0Cfj4Ex9) |  | 0 | Effective planning in the real world requires not only world knowledge, but the ability to leverage that knowledge to build the right representation of the task at hand. Decades of hierarchical planning techniques have used domain-specific temporal action abstractions to support efficient and... | Jacob Andreas, Jiahai Feng, Jiayuan Mao, Joshua B. Tenenbaum, Lionel Wong, Noa Korneev, Pratyusha Sharma, Zachary S. Siegel |  |
| 1071 |  |  [Pre-training Sequence, Structure, and Surface Features for Comprehensive Protein Representation Learning](https://openreview.net/forum?id=BEH4mGo7zP) |  | 0 | Proteins can be represented in various ways, including their sequences, 3D structures, and surfaces. While recent studies have successfully employed sequence- or structure-based representations to address multiple tasks in protein science, there has been significant oversight in incorporating... | Hasun Yu, Jaehoon Kim, Jaemyung Lee, Youhan Lee |  |
| 1072 |  |  [A unique M-pattern for micro-expression spotting in long videos](https://openreview.net/forum?id=H396R79GiQ) |  | 0 | Micro-expression spotting (MES) is challenging since the small magnitude of micro-expression (ME) makes them susceptible to global movements like head rotation. However, the unique movement pattern and inherent characteristics of ME allow them to be distinguished from other movements. Existing MES... | Jinxuan Wang, Shiting Xu, Tong Zhang |  |
| 1073 |  |  [BayesDiff: Estimating Pixel-wise Uncertainty in Diffusion via Bayesian Inference](https://openreview.net/forum?id=YcM6ofShwY) |  | 0 | Diffusion models have impressive image generation capability, but low-quality generations still exist, and their identification remains challenging due to the lack of a proper sample-wise metric. To address this, we propose BayesDiff, a pixel-wise uncertainty estimator for generations from... | Chongxuan Li, Dequan Wang, Lei Gan, Siqi Kou, Zhijie Deng |  |
| 1074 |  |  [D2 Pruning: Message Passing for Balancing Diversity & Difficulty in Data Pruning](https://openreview.net/forum?id=thbtoAkCe9) |  | 0 | In recent years, data quality has emerged as an important factor for training massive models. Analytical theories suggest that higher-quality data can lead to lower test errors in models trained on a fixed data budget. Moreover, a model can be trained on a lower compute budget without compromising... | Adyasha Maharana, Mohit Bansal, Prateek Yadav |  |
| 1075 |  |  [GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs](https://openreview.net/forum?id=tVTN7Zs0ml) |  | 0 | Clinical predictive models often rely on patients’ electronic health records (EHR), but integrating medical knowledge to enhance predictions and decision-making is challenging. This is because personalized predictions require personalized knowledge graphs (KGs), which are difficult to generate from... | Adam Cross, Cao Xiao, Jimeng Sun, Pengcheng Jiang |  |
| 1076 |  |  [The Human-AI Substitution game: active learning from a strategic labeler](https://openreview.net/forum?id=s5hSp7EdL3) |  | 0 | The standard active learning setting assumes a willing labeler, who provides labels on informative examples to speed up learning. However, if the labeler wishes to be compensated for as many labels as possible before learning finishes, the labeler may benefit from actually slowing down learning.... | Chicheng Zhang, Tom Yan |  |
| 1077 |  |  [Deep Confident Steps to New Pockets: Strategies for Docking Generalization](https://openreview.net/forum?id=UfBIxpTK10) |  | 0 | Accurate blind docking has the potential to lead to new biological breakthroughs, but for this promise to be realized, docking methods must generalize well across the proteome. Existing benchmarks, however, fail to rigorously assess generalizability. Therefore, we develop DockGen, a new benchmark... | Arthur Deng, Gabriele Corso, Nicholas Polizzi, Regina Barzilay, Tommi S. Jaakkola |  |
| 1078 |  |  [Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation](https://openreview.net/forum?id=mqVgBbNCm9) |  | 0 | This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of... | Huazhong Yang, Xuefei Ning, Yu Wang, Zifu Wang, Zinan Lin, Zixuan Zhou |  |
| 1079 |  |  [Language Model Beats Diffusion - Tokenizer is key to visual generation](https://openreview.net/forum?id=gzqrANCF4g) |  | 0 | While Large Language Models (LLMs) are the dominant models for generative tasks in language, they do not perform as well as diffusion models on image and video generation. To effectively use LLMs for visual generation, one crucial component is the visual tokenizer that maps pixel-space inputs to... | Agrim Gupta, Alexander G. Hauptmann, Boqing Gong, David A. Ross, David Minnen, Irfan Essa, José Lezama, Kihyuk Sohn, Lijun Yu, Lu Jiang, Luca Versari, MingHsuan Yang, Nitesh Bharadwaj Gundavarapu, Xiuye Gu, Yong Cheng |  |
| 1080 |  |  [A Sublinear Adversarial Training Algorithm](https://openreview.net/forum?id=N2WchST43h) |  | 0 | Adversarial training is a widely used strategy for making neural networks resistant to adversarial perturbations. For a neural network of width $m$, $n$ input training data in $d$ dimension, it takes $\Omega(mnd)$ time cost per training iteration for the forward and backward computation. In this... | Lianke Qin, Yeqi Gao, Yitan Wang, Zhao Song |  |
| 1081 |  |  [Proper Laplacian Representation Learning](https://openreview.net/forum?id=7gLfQT52Nn) |  | 0 | The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The _Laplacian representation_ is a promising approach to address these problems by inducing informative... | Diego Gomez, Marlos C. Machado, Michael Bowling |  |
| 1082 |  |  [LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning](https://openreview.net/forum?id=xw29VvOMmU) |  | 0 | We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component... | Eric P. Xing, Han Guo, Philip Greengard, Yoon Kim |  |
| 1083 |  |  [Deep Temporal Graph Clustering](https://openreview.net/forum?id=ViNe1fjGME) |  | 0 | Deep graph clustering has recently received significant attention due to its ability to enhance the representation learning capabilities of models in unsupervised scenarios. Nevertheless, deep clustering for temporal graphs, which could capture crucial dynamic interaction information, has not been... | Ke Liang, Meng Liu, Sihang Zhou, Siwei Wang, Wenxuan Tu, Xinwang Liu, Yue Liu |  |
| 1084 |  |  [CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding](https://openreview.net/forum?id=PHGxChm1l5) |  | 0 | A remarkable ability of human beings resides in compositional reasoning, i.e., the capacity to make "infinite use of finite means". However, current large vision-language foundation models (VLMs) fall short of such compositional abilities due to their \`\`bag-of-words" behaviors and inability to... | Chuang Gan, Delin Chen, Junyan Li, Peihao Chen, Yikang Shen, Yining Hong, Zhenfang Chen |  |
| 1085 |  |  [Treatment Effects Estimation By Uniform Transformer](https://openreview.net/forum?id=oOGqJ6Z1sA) |  | 0 | In observational studies, balancing covariates in different treatment groups is essential to estimate treatment effects. One of the most commonly used methods for such purposes is weighting. The performance of this class of methods usually depends on strong regularity conditions for the underlying... | Ruoqi Yu, Shulei Wang |  |
| 1086 |  |  [Demystifying Linear MDPs and Novel Dynamics Aggregation Framework](https://openreview.net/forum?id=RDSj6S8WJe) |  | 0 | In this work, we prove that, in linear MDPs, the feature dimension $d$ is lower bounded by $S/U$ in order to aptly represent transition probabilities, where $S$ is the size of the state space and $U$ is the maximum size of directly reachable states. Hence, $d$ can still scale with $S$ depending on... | Joongkyu Lee, Minhwan Oh |  |
| 1087 |  |  [Towards Aligned Layout Generation via Diffusion Model with Aesthetic Constraints](https://openreview.net/forum?id=kJ0qp9Xdsh) |  | 0 | Controllable layout generation refers to the process of creating a plausible visual arrangement of elements within a graphic design (\*e.g.\*, document and web designs) with constraints representing design intentions. Although recent diffusion-based models have achieved state-of-the-art FID scores,... | Changyou Chen, Jian Chen, Ruiyi Zhang, Yufan Zhou |  |
| 1088 |  |  [DREAM: Dual Structured Exploration with Mixup for Open-set Graph Domain Adaption](https://openreview.net/forum?id=4olqbTBt1Y) |  | 0 | Recently, numerous graph neural network methods have been developed to tackle domain shifts in graph data. However, these methods presuppose that unlabeled target graphs belong to categories previously seen in the source domain. This assumption could not hold true for in-the-wild target graphs. In... | Bin Gu, Huan Xiong, Li Shen, Mengzhu Wang, Nan Yin, Xiao Luo, Zhenghan Chen |  |
| 1089 |  |  [Communication-Efficient Federated Non-Linear Bandit Optimization](https://openreview.net/forum?id=nFI3wFM9yN) |  | 0 | Federated optimization studies the problem of collaborative function optimization among multiple clients (e.g. mobile devices or organizations) under the coordination of a central server. Since the data is collected separately by each client and always remains decentralized, federated optimization... | Chong Liu, Chuanhao Li, YuXiang Wang |  |
| 1090 |  |  [PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization](https://openreview.net/forum?id=5Nn2BLV7SB) |  | 0 | Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential.... | Chaoya Jiang, Cunxiang Wang, Hao Chen, Jindong Wang, Linyi Yang, Rui Xie, Shikun Zhang, Wei Ye, Wenjin Yao, Xing Xie, Yidong Wang, Yue Zhang, Zhengran Zeng, Zhuohao Yu |  |
| 1091 |  |  [Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds](https://openreview.net/forum?id=NltzxpG0nz) |  | 0 | Recent studies have presented compelling evidence that large language models (LLMs) can equip embodied agents with the self-driven capability to interact with the world, which marks an initial step toward versatile robotics. However, these efforts tend to overlook the visual richness of open... | Jiazheng Liu, Sipeng Zheng, Yicheng Feng, Zongqing Lu |  |
| 1092 |  |  [Tackling the Data Heterogeneity in Asynchronous Federated Learning with Cached Update Calibration](https://openreview.net/forum?id=4aywmeb97I) |  | 0 | Asynchronous federated learning, which enables local clients to send their model update asynchronously to the server without waiting for others, has recently emerged for its improved efficiency and scalability over traditional synchronized federated learning. In this paper, we study how the... | Jingcheng Wu, Jinghui Chen, Ruoyu Chen, Yuanpu Cao, Yujia Wang |  |
| 1093 |  |  [Waxing-and-Waning: a Generic Similarity-based Framework for Efficient Self-Supervised Learning](https://openreview.net/forum?id=TilcG5C8bN) |  | 0 | Deep Neural Networks (DNNs), essential for diverse applications such as visual recognition and eldercare, often require a large amount of labeled data for training, making widespread deployment of DNNs a challenging task. Self-supervised learning (SSL) emerges as a promising approach, which... | Ao Li, Chao Wu, Geng Yuan, Sheng Li, Xulong Tang, Yanzhi Wang |  |
| 1094 |  |  [WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions](https://openreview.net/forum?id=CfXh93NDgH) |  | 0 | Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an... | Can Xu, Chongyang Tao, Daxin Jiang, Jiazhan Feng, Kai Zheng, Pu Zhao, Qingfeng Sun, Qingwei Lin, Xiubo Geng |  |
| 1095 |  |  [LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors](https://openreview.net/forum?id=usrChqw6yK) |  | 0 | Inspired by the outstanding zero-shot capability of vision language models (VLMs) in image classification tasks, open-vocabulary object detection has attracted increasing interest by distilling the broad VLM knowledge into detector training. However, most existing open-vocabulary detectors learn by... | Jiaxing Huang, Lewei Lu, Sheng Jin, Shijian Lu, Xueying Jiang |  |
| 1096 |  |  [CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic Decoding](https://openreview.net/forum?id=lKxL5zkssv) |  | 0 | The study of decoding visual neural information faces challenges in generalizing single-subject decoding models to multiple subjects, due to individual differences. Moreover, the limited availability of data from a single subject has a constraining impact on model performance. Although prior... | Changde Du, Huiguang He, Qiongyi Zhou, Shengpei Wang |  |
| 1097 |  |  [Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning](https://openreview.net/forum?id=v8jdwkUNXb) |  | 0 | Score-based generative models like the diffusion model have been testified to be effective in modeling multi-modal data from image generation to reinforcement learning (RL). However, the inference process of diffusion model can be slow, which hinders its usage in RL with iterative sampling. We... | Chi Jin, Zihan Ding |  |
| 1098 |  |  [Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning](https://openreview.net/forum?id=iX1RjVQODj) |  | 0 | Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for aligning models with human intent. Typically RLHF algorithms operate in two phases: first, use human preferences to learn a reward function and second, align the model by optimizing the learned reward via... | Chelsea Finn, Dorsa Sadigh, Harshit Sikchi, Joey Hejna, Rafael Rafailov, Scott Niekum, W. Bradley Knox |  |
| 1099 |  |  [Neural Network-Based Score Estimation in Diffusion Models: Optimization and Generalization](https://openreview.net/forum?id=h8GeqOxtd4) |  | 0 | Diffusion models have emerged as a powerful tool rivaling GANs in generating high-quality samples with improved fidelity, flexibility, and robustness. A key component of these models is to learn the score function through score matching. Despite empirical success on various tasks, it remains... | Meisam Razaviyayn, Renyuan Xu, Yinbin Han |  |
| 1100 |  |  [Time-Varying Propensity Score to Bridge the Gap between the Past and Present](https://openreview.net/forum?id=m0x0rv6Iwm) |  | 0 | Real-world deployment of machine learning models is challenging because data evolves over time. While no model can work when data evolves in an arbitrary fashion, if there is some pattern to these changes, we might be able to design methods to address it. This paper addresses situations when data... | Alex Smola, Jonas Mueller, Pratik Chaudhari, Rasool Fakoor, Zachary Chase Lipton |  |
| 1101 |  |  [Debiasing Attention Mechanism in Transformer without Demographics](https://openreview.net/forum?id=jLIUfrAcMQ) |  | 0 | Although transformers demonstrate impressive capabilities in a variety of tasks, the fairness issue remains a significant concern when deploying these models. Existing works to address fairness issues in transformers require sensitive labels (such as age, gender, etc.), which can raise privacy... | Shenyu Lu, Xiaoqian Wang, Yipei Wang |  |
| 1102 |  |  [RA-DIT: Retrieval-Augmented Dual Instruction Tuning](https://openreview.net/forum?id=22OTbutug9) |  | 0 | Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of... | Gergely Szilvasy, Jacob Kahn, Luke Zettlemoyer, Maria Lomeli, Mike Lewis, Mingda Chen, Pedro Rodriguez, Richard James, Weijia Shi, Wentau Yih, Xi Victoria Lin, Xilun Chen |  |
| 1103 |  |  [Like Oil and Water: Group Robustness Methods and Poisoning Defenses May Be at Odds](https://openreview.net/forum?id=rM9VJPB20F) |  | 0 | Group robustness has become a major concern in machine learning (ML) as conventional training paradigms were found to produce high error on minority groups. Without explicit group annotations, proposed solutions rely on heuristics that aim to identify and then amplify the minority samples during... | Furong Huang, MichaelAndrei PanaitescuLiess, Sicheng Zhu, Tudor Dumitras, Yigitcan Kaya |  |
| 1104 |  |  [Visual Data-Type Understanding does not emerge from scaling Vision-Language Models](https://openreview.net/forum?id=WyEdX2R4er) |  | 0 | Recent advances in the development of vision-language models (VLMs) are yielding remarkable success in recognizing visual semantic content, including impressive instances of compositional image understanding. Here, we introduce the novel task of Visual Data-Type Identification, a basic perceptual... | Matthias Bethge, Max F. Burg, Samuel Albanie, Vishaal Udandarao |  |
| 1105 |  |  [CoT3DRef: Chain-of-Thoughts Data-Efficient 3D Visual Grounding](https://openreview.net/forum?id=ORUiqcLpV6) |  | 0 | 3D visual grounding is the ability to localize objects in 3D scenes conditioned by utterances. Most existing methods devote the referring head to localize the referred object directly, causing failure in complex scenarios. In addition, it does not illustrate how and why the network reaches the... | Eslam Mohamed Bakr, Habib Slim, Mahmoud Ahmed, Mohamed Ayman, Mohamed Elhoseiny |  |
| 1106 |  |  [An Efficient Tester-Learner for Halfspaces](https://openreview.net/forum?id=z6n1fKMMC1) |  | 0 | We give the first efficient algorithm for learning halfspaces in the testable learning model recently defined by Rubinfeld and Vasilyan [2022]. In this model, a learner certifies that the accuracy of its output hypothesis is near optimal whenever the training set passes an associated test, and... | Adam R. Klivans, Aravind Gollakota, Arsen Vasilyan, Konstantinos Stavropoulos |  |
| 1107 |  |  [Reclaiming the Source of Programmatic Policies: Programmatic versus Latent Spaces](https://openreview.net/forum?id=NGVljI6HkR) |  | 0 | Recent works have introduced LEAPS and HPRL, systems that learn latent spaces of domain-specific languages, which are used to define programmatic policies for partially observable Markov decision processes (POMDPs). These systems induce a latent space while optimizing losses such as the behavior... | Kenneth Tjhia, Levi Lelis, Tales Henrique Carvalho |  |
| 1108 |  |  [Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment](https://openreview.net/forum?id=w9tc699w3Z) |  | 0 | We introduce a method to train vision-language models for remote-sensing images without using any textual annotations. Our key insight is to use co-located internet imagery taken on the ground as an intermediary for connecting remote-sensing images and language. Specifically, we train an image... | Bharath Hariharan, Carl Vondrick, Cheng Perng Phoo, Kavita Bala, Meilin Kelsey Liu, Utkarsh Mall |  |
| 1109 |  |  [The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing](https://openreview.net/forum?id=C36v8541Ns) |  | 0 | Real-life applications of deep neural networks are hindered by their unsteady predictions when faced with noisy inputs and adversarial attacks. The certified radius in this context is a crucial indicator of the robustness of models. However how to design an efficient classifier with an associated... | Alexandre Allauzen, Alexandre Araujo, Blaise Delattre, Quentin Barthélemy |  |
| 1110 |  |  [On the Fairness ROAD: Robust Optimization for Adversarial Debiasing](https://openreview.net/forum?id=xnhvVtZtLD) |  | 0 | In the field of algorithmic fairness, significant attention has been put on group fairness criteria, such as Demographic Parity and Equalized Odds. Nevertheless, these objectives, measured as global averages, have raised concerns about persistent local disparities between sensitive groups. In this... | Marcin Detyniecki, Sylvain Lamprier, Tatsunori Hashimoto, Thibault Laugel, Vincent Grari |  |
| 1111 |  |  [Learning Planning Abstractions from Language](https://openreview.net/forum?id=3UWuFoksGb) |  | 0 | This paper presents a framework for learning state and action abstractions in sequential decision-making domains. Our framework, planning abstraction from language (PARL), utilizes language-annotated demonstrations to automatically discover a symbolic and abstract action space and induce a latent... | Geng Chen, Jiajun Wu, Jiayuan Mao, Joy Hsu, Weiyu Liu |  |
| 1112 |  |  [Tailoring Self-Rationalizers with Multi-Reward Distillation](https://openreview.net/forum?id=t8eO0CiZJV) |  | 0 | Large language models (LMs) are capable of generating free-text rationales to aid question answering. However, prior work 1) suggests that useful self-rationalization is emergent only at significant scales (e.g., 175B parameter GPT-3); and 2) focuses largely on downstream performance, ignoring the... | Aaron Chan, Brihi Joshi, Jack Hessel, Liunian Harold Li, Sahana Ramnath, Skyler Hallinan, Xiang Ren, Ximing Lu, Yejin Choi |  |
| 1113 |  |  [Building Cooperative Embodied Agents Modularly with Large Language Models](https://openreview.net/forum?id=EnXJfQqy0K) |  | 0 | In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or... | Chuang Gan, Hongxin Zhang, Jiaming Shan, Joshua B. Tenenbaum, Qinhong Zhou, Tianmin Shu, Weihua Du, Yilun Du |  |
| 1114 |  |  [Fast Hyperboloid Decision Tree Algorithms](https://openreview.net/forum?id=TTonmgTT9X) |  | 0 | Hyperbolic geometry is gaining traction in machine learning due to its capacity to effectively capture hierarchical structures in real-world data. Hyperbolic spaces, where neighborhoods grow exponentially, offer substantial advantages and have consistently delivered state-of-the-art results across... | Antonio Khalil Moretti, Ethan Turok, Itsik Pe'er, Philippe Chlenski |  |
| 1115 |  |  [Private Zeroth-Order Nonsmooth Nonconvex Optimization](https://openreview.net/forum?id=IzqZbNMZ0M) |  | 0 | We introduce a new zeroth-order algorithm for private stochastic optimization on nonconvex and nonsmooth objectives. Given a dataset of size $M$, our algorithm ensures $(\alpha,\alpha\rho^2/2)$-Renyi differential privacy and finds a $(\delta,\epsilon)$-stationary point so long as... | Ashok Cutkosky, Hoang Tran, Qinzi Zhang |  |
| 1116 |  |  [Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training](https://openreview.net/forum?id=3xHDeA8Noi) |  | 0 | Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based)... | David Leo Wright Hall, Hong Liu, Percy Liang, Tengyu Ma, Zhiyuan Li |  |
| 1117 |  |  [Conversational Drug Editing Using Retrieval and Domain Feedback](https://openreview.net/forum?id=yRrPfKyJQ2) |  | 0 | Recent advancements in conversational large language models (LLMs), such as ChatGPT, have demonstrated remarkable promise in various domains, including drug discovery. However, existing works mainly focus on investigating the capabilities of conversational LLMs on chemical reactions and... | Chaowei Xiao, Chengpeng Wang, Hongyu Guo, Jiongxiao Wang, Ling Liu, Shengchao Liu, Yijin Yang |  |
| 1118 |  |  [Poly-View Contrastive Learning](https://openreview.net/forum?id=iHcTLIor0m) |  | 0 | Contrastive learning typically matches pairs of related views among a number of unrelated negative views. Views can be generated (e.g. by augmentations) or be observed. We investigate matching when there are more than two related views which we call poly-view tasks, and derive new representation... | Amitis Shidani, Dan Busbridge, Eeshan Gunesh Dhekane, Jason Ramapuram, R. Devon Hjelm, Russell Webb |  |
| 1119 |  |  [A Probabilistic Framework for Modular Continual Learning](https://openreview.net/forum?id=MVe2dnWPCu) |  | 0 | Modular approaches that use a different composition of modules for each problem are a promising direction in continual learning (CL). However, searching through the large, discrete space of module compositions is challenging, especially because evaluating a composition’s performance requires a... | Akash Srivastava, Charles Sutton, Lazar Valkov, Swarat Chaudhuri |  |
| 1120 |  |  [Boundary Denoising for Video Activity Localization](https://openreview.net/forum?id=bLpUtGyf9g) |  | 0 | Video activity localization aims at understanding the semantic content in long, untrimmed videos and retrieving actions of interest. The retrieved action with its start and end locations can be used for highlight generation, temporal action detection, etc. Unfortunately, learning the exact boundary... | Bernard Ghanem, Jialin Gao, JuanManuel PérezRúa, Mattia Soldan, Mengmeng Xu, Shuming Liu |  |
| 1121 |  |  [Few-Shot Detection of Machine-Generated Text using Style Representations](https://openreview.net/forum?id=cWiEN1plhJ) |  | 0 | The advent of instruction-tuned language models that convincingly mimic human writing poses a significant risk of abuse. For example, such models could be used for plagiarism, disinformation, spam, or phishing. However, such abuse may be counteracted with the ability to detect whether a piece of... | Aleem Khan, Barry Y. Chen, Kailin Koch, Marcus Bishop, Nicholas Andrews, Rafael A. Rivera Soto |  |
| 1122 |  |  [Safe and Robust Watermark Injection with a Single OoD Image](https://openreview.net/forum?id=PCm1oT8pZI) |  | 0 | Training a high-performance deep neural network requires large amounts of data and computational resources. Protecting the intellectual property (IP) and commercial ownership of a deep model is challenging yet increasingly crucial. A major stream of watermarking strategies implants verifiable... | Haobo Zhang, Haotao Wang, Jiayu Zhou, Junyuan Hong, Shuyang Yu, Zhangyang Wang |  |
| 1123 |  |  [Massive Editing for Large Language Models via Meta Learning](https://openreview.net/forum?id=L6L1CJQ2PE) |  | 0 | While large language models (LLMs) have enabled learning knowledge from the pre-training corpora, the acquired knowledge may be fundamentally incorrect or outdated over time, which necessitates rectifying the knowledge of the language model (LM) after the training. A promising approach involves... | Chenmien Tan, Ge Zhang, Jie Fu |  |
| 1124 |  |  [BrainLM: A foundation model for brain activity recordings](https://openreview.net/forum?id=RwI7ZEfR27) |  | 0 | We introduce the Brain Language Model (BrainLM), a foundation model for brain activity dynamics trained on 6,700 hours of fMRI recordings. Utilizing self-supervised masked-prediction training, BrainLM demonstrates proficiency in both fine-tuning and zero-shot inference tasks. Fine-tuning allows for... | Antonio Henrique de Oliveira Fonseca, Chadi Abdallah, Christopher L. Averill, David van Dijk, Emanuele Zappala, James L. Cross, Josue Ortega Caro, Matteo Rosati, Prateek Mittal, Rahul Madhav Dhodapkar, Syed Asad Rizvi |  |
| 1125 |  |  [Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality](https://openreview.net/forum?id=1NHgmKqOzZ) |  | 0 | Dataset distillation aims to minimize the time and memory needed for training deep networks on large datasets, by creating a small set of synthetic images that has a similar generalization performance to that of the full dataset. However, current dataset distillation techniques fall short, showing... | Baharan Mirzasoleiman, Xuxi Chen, Yu Yang, Zhangyang Wang |  |
| 1126 |  |  [Object centric architectures enable efficient causal representation learning](https://openreview.net/forum?id=r9FsiXZxZt) |  | 0 | Causal representation learning has showed a variety of settings in which we can disentangle latent variables with identifiability guarantees (up to some reasonable equivalence class). Common to all of these approaches is the assumption that (1) the latent variables are represented as... | Amin Mansouri, Jason S. Hartford, Yan Zhang, Yoshua Bengio |  |
| 1127 |  |  [Efficient Score Matching with Deep Equilibrium Layers](https://openreview.net/forum?id=J1djqLAa6N) |  | 0 | Score matching methods -- estimate probability densities without computing the normalization constant -- are particularly useful in deep learning. However, computational and memory costs of score matching methods can be prohibitive for high-dimensional data or complex models, particularly due to... | Akwum Onwunta, Bao Wang, Qingsong Wang, Yuhao Huang |  |
| 1128 |  |  [Alt-Text with Context: Improving Accessibility for Images on Twitter](https://openreview.net/forum?id=97Dl82avFs) |  | 0 | In this work we present an approach for generating alternative text (or alt-text) descriptions for images shared on social media, specifically Twitter. More than just a special case of image captioning, alt-text is both more literally descriptive and context-specific. Also critically, images posted... | Nikita Srivatsan, Omar Florez, Sofía Samaniego, Taylor BergKirkpatrick |  |
| 1129 |  |  [Defining Expertise: Applications to Treatment Effect Estimation](https://openreview.net/forum?id=1YPfmglNRU) |  | 0 | Decision-makers are often experts of their domain and take actions based on their domain knowledge. Doctors, for instance, may prescribe treatments by predicting the likely outcome of each available treatment. Actions of an expert thus naturally encode part of their domain knowledge, and can help... | Alicia Curth, Alihan Hüyük, Mihaela van der Schaar, Qiyao Wei |  |
| 1130 |  |  [Alleviating Exposure Bias in Diffusion Models through Sampling with Shifted Time Steps](https://openreview.net/forum?id=ZSD3MloKe6) |  | 0 | Diffusion Probabilistic Models (DPM) have shown remarkable efficacy in the synthesis of high-quality images. However, their inference process characteristically requires numerous, potentially hundreds, of iterative steps, which could exaggerate the problem of exposure bias due to the training and... | MarieFrancine Moens, Mingxiao Li, Ruicong Yao, Tingyu Qu, Wei Sun |  |
| 1131 |  |  [Rethinking the Benefits of Steerable Features in 3D Equivariant Graph Neural Networks](https://openreview.net/forum?id=mGHJAyR8w0) |  | 0 | Theoretical and empirical comparisons have been made to assess the expressive power and performance of invariant and equivariant GNNs. However, there is currently no theoretical result comparing the expressive power of $k$-hop invariant GNNs and equivariant GNNs. Additionally, little is understood... | Andrea L. Bertozzi, Bao Wang, Jack Xin, Justin M. Baker, ShihHsin Wang, YungChang Hsu |  |
| 1132 |  |  [Neural SDF Flow for 3D Reconstruction of Dynamic Scenes](https://openreview.net/forum?id=rzF0R6GOd4) |  | 0 | In this paper, we tackle the problem of 3D reconstruction of dynamic scenes from multi-view videos. Previous dynamic scene reconstruction works either attempt to model the motion of 3D points in space, which constrains them to handle a single articulated object or require depth maps as input. By... | Mathieu Salzmann, Miaomiao Liu, Richard Hartley, Wei Mao |  |
| 1133 |  |  [Class Probability Matching with Calibrated Networks for Label Shift Adaption](https://openreview.net/forum?id=mliQ2huFrZ) |  | 0 | We consider the domain adaptation problem in the context of label shift, where the label distributions between source and target domain differ, but the conditional distributions of features given the label are the same. To solve the label shift adaption problem, we develop a novel matching... | Annika Betken, Hanyuan Hang, Hongwei Wen |  |
| 1134 |  |  [DIFFTACTILE: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation](https://openreview.net/forum?id=eJHnSg783t) |  | 0 | We introduce DIFFTACTILE, a physics-based differentiable tactile simulation system designed to enhance robotic manipulation with dense and physically accurate tactile feedback. In contrast to prior tactile simulators which primarily focus on manipulating rigid bodies and often rely on simplified... | Branden Romero, Chao Liu, Chuang Gan, Gu Zhang, Qingwei Ben, Zhou Xian, Zilin Si |  |
| 1135 |  |  [Tangent Transformers for Composition, Privacy and Removal](https://openreview.net/forum?id=VLFhbOCz5D) |  | 0 | We introduce Tangent Attention Fine-Tuning (TAFT), a method for fine-tuning linearized transformers obtained by computing a First-order Taylor Expansion around a pre-trained initialization. We show that the Jacobian-Vector Product resulting from linearization can be computed efficiently in a single... | Aditya Golatkar, Stefano Soatto, Tian Yu Liu |  |
| 1136 |  |  [Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information](https://openreview.net/forum?id=yV6wwEbtkR) |  | 0 | It is believed that in knowledge distillation (KD), the role of the teacher is to provide an estimate for the unknown Bayes conditional probability distribution (BCPD) to be used in the student training process. Conventionally, this estimate is obtained by training the teacher using maximum... | EnHui Yang, Linfeng Ye, Renhao Tan, Shayan Mohajer Hamidi |  |
| 1137 |  |  [Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting](https://openreview.net/forum?id=RIu5lyNXjT) |  | 0 | As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern... | Alane Suhr, Melanie Sclar, Yejin Choi, Yulia Tsvetkov |  |
| 1138 |  |  [Universal Guidance for Diffusion Models](https://openreview.net/forum?id=pzpWBbnwiJ) |  | 0 | Typical diffusion models are trained to accept a particular form of conditioning, most commonly text, and cannot be conditioned on other modalities without retraining. In this work, we propose a universal guidance algorithm that enables diffusion models to be controlled by arbitrary guidance... | Arpit Bansal, Avi Schwarzschild, HongMin Chu, Jonas Geiping, Micah Goldblum, Soumyadip Sengupta, Tom Goldstein |  |
| 1139 |  |  [Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models](https://openreview.net/forum?id=L4nOxziGf9) |  | 0 | An increasing number of vision-language tasks can be handled with little to no training, i.e., in a zero and few-shot manner, by marrying large language models (LLMs) to vision encoders, resulting in large vision-language models (LVLMs). While this has huge upsides, such as not requiring training... | Archiki Prasad, Elias StengelEskin, Mohit Bansal |  |
| 1140 |  |  [A Black-box Approach for Non-stationary Multi-agent Reinforcement Learning](https://openreview.net/forum?id=LWuYsSD94h) |  | 0 | We investigate learning the equilibria in non-stationary multi-agent systems and address the challenges that differentiate multi-agent learning from single-agent learning. Specifically, we focus on games with bandit feedback, where testing an equilibrium can result in substantial regret even when... | Haozhe Jiang, Maryam Fazel, Qiwen Cui, Simon Shaolei Du, Zhihan Xiong |  |
| 1141 |  |  [Prompt Risk Control: A Rigorous Framework for Responsible Deployment of Large Language Models](https://openreview.net/forum?id=5tGGWOijvq) |  | 0 | With the explosion of the zero-shot capabilities of (and thus interest in) pre-trained large language models, there has come accompanying interest in how best to prompt a language model to perform a given task. While it may be tempting to choose a prompt based on empirical results on a validation... | Jake Snell, Richard S. Zemel, Thomas P. Zollo, Todd Morrill, Toniann Pitassi, Zhun Deng |  |
| 1142 |  |  [Energy-conserving equivariant GNN for elasticity of lattice architected metamaterials](https://openreview.net/forum?id=smy4DsUbBo) |  | 0 | Lattices are architected metamaterials whose properties strongly depend on their geometrical design. The analogy between lattices and graphs enables the use of graph neural networks (GNNs) as a faster surrogate model compared to traditional methods such as finite element modelling. In this work, we... | Gábor Csányi, Ilyes Batatia, Ivan Grega, Sri Karlapati, Vikram S. Deshpande |  |
| 1143 |  |  [Adaptive Federated Learning with Auto-Tuned Clients](https://openreview.net/forum?id=g0mlwqs8pi) |  | 0 | Federated learning (FL) is a distributed machine learning framework where the global model of a central server is trained via multiple collaborative steps by participating clients without sharing their data. While being a flexible framework, where the distribution of local data, participation rate,... | Anastasios Kyrillidis, César A. Uribe, Junhyung Lyle Kim, Mohammad Taha Toghani |  |
| 1144 |  |  [CrossLoco: Human Motion Driven Control of Legged Robots via Guided Unsupervised Reinforcement Learning](https://openreview.net/forum?id=UCfz492fM8) |  | 0 | Human motion driven control (HMDC) is an effective approach for generating natural and compelling robot motions while preserving high-level semantics. However, establishing the correspondence between humans and robots with different body structures is not straightforward due to the mismatches in... | Hyunyoung Jung, Matthew C. Gombolay, Sehoon Ha, Tianyu Li, Yong Kwon Cho |  |
| 1145 |  |  [Zoology: Measuring and Improving Recall in Efficient Language Models](https://openreview.net/forum?id=LY3ukUANko) |  | 0 | Attention-free language models that combine gating and convolutions are growing in popularity due to their efficiency and increasingly competitive performance. To better understand these architectures, we pretrain a suite of 17 attention and gated-convolution language models, finding that SoTA... | Aman Timalsina, Atri Rudra, Christopher Ré, Isys Johnson, James Zou, Michael Poli, Sabri Eyuboglu, Simran Arora |  |
| 1146 |  |  [Let Models Speak Ciphers: Multiagent Debate through Embeddings](https://openreview.net/forum?id=sehRvaIPQQ) |  | 0 | Discussion and debate among Large Language Models (LLMs) have gained considerable attention due to their potential to enhance the reasoning ability of LLMs. Although natural language is an obvious choice for communication due to LLM's language understanding capability, the token sampling step... | Boyi Liu, Bryan A. Plummer, Chau Pham, Hongxia Yang, Jianbo Yuan, Tianyi Liu, Yingxiang Yang, Zhaoran Wang, Zhengyu Chen |  |
| 1147 |  |  [Towards Training Without Depth Limits: Batch Normalization Without Gradient Explosion](https://openreview.net/forum?id=xhCZD9hiiA) |  | 0 | Normalization layers are one of the key building blocks for deep neural networks. Several theoretical studies have shown that batch normalization improves the signal propagation, by avoiding the representations from becoming collinear across the layers. However, results on mean-field theory of... | Alexander Immer, Alexandru Meterez, Amir Joudaki, Francesco Orabona, Gunnar Rätsch, Hadi Daneshmand |  |
| 1148 |  |  [Plugin estimators for selective classification with out-of-distribution detection](https://openreview.net/forum?id=DASh78rJ7g) |  | 0 | Real-world classifiers can benefit from the option of abstaining from predicting on samples where they have low confidence. Such abstention is particularly useful on samples which are close to the learned decision boundary, or which are outliers with respect to the training sample. These settings... | Aditya Krishna Menon, Harikrishna Narasimhan, Sanjiv Kumar, Wittawat Jitkrittum |  |
| 1149 |  |  [Dynamic Sparse Training with Structured Sparsity](https://openreview.net/forum?id=kOBkxFRKTA) |  | 0 | Dynamic Sparse Training (DST) methods achieve state-of-the-art results in sparse neural network training, matching the generalization of dense models while enabling sparse training and inference. Although the resulting models are highly sparse and theoretically less computationally expensive,... | Anna Golubeva, Mihai Nica, Mike Lasby, Utku Evci, Yani Ioannou |  |
| 1150 |  |  [Efficient Dynamics Modeling in Interactive Environments with Koopman Theory](https://openreview.net/forum?id=fkrYDQaHOJ) |  | 0 | The accurate modeling of dynamics in interactive environments is critical for successful long-range prediction. Such a capability could advance Reinforcement Learning (RL) and Planning algorithms, but achieving it is challenging. Inaccuracies in model estimates can compound, resulting in increased... | Arnab Kumar Mondal, Kaleem Siddiqi, Sai Rajeswar, Siamak Ravanbakhsh, Siba Smarak Panigrahi |  |
| 1151 |  |  [Learning interpretable control inputs and dynamics underlying animal locomotion](https://openreview.net/forum?id=MFCjgEOLJT) |  | 0 | A central objective in neuroscience is to understand how the brain orchestrates movement. Recent advances in automated tracking technologies have made it possible to document behavior with unprecedented temporal resolution and scale, generating rich datasets which can be exploited to gain insights... | Adrien Jouary, Christian K. Machens, Guillaume Hennequin, Marine Schimel, Michael B. Orger, Thomas Soares Mullen |  |
| 1152 |  |  [Tight Rates in Supervised Outlier Transfer Learning](https://openreview.net/forum?id=nUBLhhVM1l) |  | 0 | A critical barrier to learning an accurate decision rule for outlier detection is the scarcity of outlier data. As such, practitioners often turn to the use of similar but imperfect outlier data from which they might \emph{transfer} information to the target outlier detection task. Despite the... | Mohammadreza M. Kalan, Samory Kpotufe |  |
| 1153 |  |  [Sampling Multimodal Distributions with the Vanilla Score: Benefits of Data-Based Initialization](https://openreview.net/forum?id=oAMArMMQxb) |  | 0 | There is a long history, as well as a recent explosion of interest, in statistical and generative modeling approaches based on \emph{score functions} --- derivatives of the log-likelihood of a distribution. In seminal works, Hyv\"arinen proposed vanilla score matching as a way to learn... | Frederic Koehler, ThuyDuong Vuong |  |
| 1154 |  |  [Curiosity-driven Red-teaming for Large Language Models](https://openreview.net/forum?id=4KqkizXgXU) |  | 0 | Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content. To probe when an LLM generates unwanted content, the current paradigm is to recruit a $\textit{red team}$ of human testers to design input prompts (i.e., test... | Akash Srivastava, Aldo Pareja, Idan Shenfeld, James R. Glass, Pulkit Agrawal, TsunHsuan Wang, YungSung Chuang, ZhangWei Hong |  |
| 1155 |  |  [Understanding prompt engineering may not require rethinking generalization](https://openreview.net/forum?id=a745RnSFLT) |  | 0 | Zero-shot learning in prompted vision-language models, the practice of crafting prompts to build classifiers without an explicit training process, has achieved impressive performance in many settings. This success presents a seemingly surprising observation: these methods suffer relatively little... | Dylan Sam, J. Zico Kolter, Victor Akinwande, Yiding Jiang |  |
| 1156 |  |  [Adversarial Imitation Learning via Boosting](https://openreview.net/forum?id=DuQkqSe9en) |  | 0 | Adversarial imitation learning (AIL) has stood out as a dominant framework across various imitation learning (IL) applications, with Discriminator Actor Critic (DAC) demonstrating the effectiveness of off-policy learning algorithms in improving sample efficiency and scalability to... | Dhruv Sreenivas, Jonathan D. Chang, Kianté Brantley, Wen Sun, Yingbing Huang |  |
| 1157 |  |  [Unsupervised Pretraining for Fact Verification by Language Model Distillation](https://openreview.net/forum?id=1mjsP8RYAw) |  | 0 | Fact verification aims to verify a claim using evidence from a trustworthy knowledge base. To address this challenge, algorithms must produce features for every claim that are both semantically meaningful, and compact enough to find a semantic alignment with the source information. In contrast to... | Adrián Bazaga, Gos Micklem, Pietro Lio |  |
| 1158 |  |  [LipSim: A Provably Robust Perceptual Similarity Metric](https://openreview.net/forum?id=0w42S2Gp70) |  | 0 | Recent years have seen growing interest in developing and applying perceptual similarity metrics. Research has shown the superiority of perceptual metrics over pixel-wise metrics in aligning with human perception and serving as a proxy for the human visual system. On the other hand, as perceptual... | Alexandre Araujo, Farshad Khorrami, Prashanth Krishnamurthy, Sara Ghazanfari, Siddharth Garg |  |
| 1159 |  |  [Does Progress On Object Recognition Benchmarks Improve Generalization on Crowdsourced, Global Data?](https://openreview.net/forum?id=rhaQbS3K3R) |  | 0 | For more than a decade, researchers have measured progress in object recognition on the ImageNet dataset along with its associated generalization benchmarks such as ImageNet-A, -C, and -R. Recent advances in foundation models, trained on orders of magnitude more data, have begun to saturate... | Diane Bouchacourt, Mark Ibrahim, Megan Richards, Polina Kirichenko |  |
| 1160 |  |  [TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate Time Series](https://openreview.net/forum?id=xtOydkE1Ku) |  | 0 | We introduce a new model for multivariate probabilistic time series prediction, designed to flexibly address a range of tasks including forecasting, interpolation, and their combinations. Building on copula theory, we propose a simplified objective for the recently-introduced transformer-based... | Alexandre Drouin, Arjun Ashok, Nicolas Chapados, Valentina Zantedeschi, Étienne Marcotte |  |
| 1161 |  |  [A robust differential Neural ODE Optimizer](https://openreview.net/forum?id=zbOSJ3CATY) |  | 0 | Neural networks and neural ODEs tend to be vulnerable to adversarial attacks, rendering robust optimizers critical to curb the success of such attacks. In this regard, the key insight of this work is to interpret Neural ODE optimization as a min-max optimal control problem. More particularly, we... | Augustinos D. Saravanos, Evangelos A. Theodorou, GuanHorng Liu, Panagiotis Theodoropoulos, Tianrong Chen |  |
| 1162 |  |  [A Primal-Dual Approach to Solving Variational Inequalities with General Constraints](https://openreview.net/forum?id=RsztjXcvUf) |  | 0 | Yang et al. (2023) recently showed how to use first-order gradient methods to solve general variational inequalities (VIs) under a limiting assumption that analytic solutions of specific subproblems are available. In this paper, we circumvent this assumption via a warm-starting technique where we... | Matteo Pagliardini, Michael I. Jordan, Tatjana Chavdarova, Tong Yang |  |
| 1163 |  |  [TiC-CLIP: Continual Training of CLIP Models](https://openreview.net/forum?id=TLADT8Wrhn) |  | 0 | Keeping large foundation models up to date on latest data is inherently expensive. To avoid the prohibitive costs of constantly retraining, it is imperative to continually train these models. This problem is exacerbated by the lack of any large scale continual learning benchmarks or baselines. We... | Fartash Faghri, Hadi Pouransari, Mehrdad Farajtabar, Oncel Tuzel, Raviteja Vemulapalli, Sachin Mehta, Saurabh Garg, Vaishaal Shankar |  |
| 1164 |  |  [Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks](https://openreview.net/forum?id=dLoAdIKENc) |  | 0 | In light of recent advancements in generative AI models, it has become essential to distinguish genuine content from AI-generated one to prevent the malicious usage of fake materials as authentic ones and vice versa. Various techniques have been introduced for identifying AI-generated images, with... | Aounon Kumar, Atoosa Malemir Chegini, Keivan Rezaei, Mehrdad Saberi, Soheil Feizi, Vinu Sankar Sadasivan, Wenxiao Wang |  |
| 1165 |  |  [Constrained Decoding for Cross-lingual Label Projection](https://openreview.net/forum?id=DayPQKXaQk) |  | 0 | Zero-shot cross-lingual transfer utilizing multilingual LLMs has become a popular learning paradigm for low-resource languages with no labeled training data. However, for NLP tasks that involve fine-grained predictions on words and phrases, the performance of zero-shot cross-lingual transfer... | Alan Ritter, Duong Minh Le, Wei Xu, Yang Chen |  |
| 1166 |  |  [Channel Vision Transformers: An Image Is Worth 1 x 16 x 16 Words](https://openreview.net/forum?id=CK5Hfb5hBG) |  | 0 | Vision Transformer (ViT) has emerged as a powerful architecture in the realm of modern computer vision. However, its application in certain imaging fields, such as microscopy and satellite imaging, presents unique challenges. In these domains, images often contain multiple channels, each carrying... | Srinivasan Sivanandan, Theofanis Karaletsos, Yujia Bao |  |
| 1167 |  |  [Understanding Reconstruction Attacks with the Neural Tangent Kernel and Dataset Distillation](https://openreview.net/forum?id=VoLDkQ6yR3) |  | 0 | Modern deep learning requires large volumes of data, which could contain sensitive or private information that cannot be leaked. Recent work has shown for homogeneous neural networks a large portion of this training data could be reconstructed with only access to the trained network parameters.... | Alexander Amini, Daniela Rus, Mathias Lechner, Noel Loo, Ramin M. Hasani |  |
| 1168 |  |  [ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models](https://openreview.net/forum?id=iIT02bAKzv) |  | 0 | Large Vision-Language Models (LVLMs) can understand the world comprehensively by integrating rich information from different modalities, achieving remarkable performance improvements on various multimodal downstream tasks. However, deploying LVLMs is often problematic due to their massive... | Jaehong Yoon, Mohit Bansal, YiLin Sung |  |
| 1169 |  |  [Behaviour Distillation](https://openreview.net/forum?id=qup9xD8mW4) |  | 0 | Dataset distillation aims to condense large datasets into a small number of synthetic examples that can be used as drop-in replacements when training new models. It has applications to interpretability, neural architecture search, privacy, and continual learning. Despite strong successes in... | Andrei Lupu, Chris Lu, Jakob Nicolaus Foerster, Jarek Liesen, Robert Tjarko Lange |  |
| 1170 |  |  [Improving LoRA in Privacy-preserving Federated Learning](https://openreview.net/forum?id=NLPzL6HWNl) |  | 0 | Low-rank adaptation (LoRA) is one of the most popular task-specific parameter-efficient fine-tuning (PEFT) methods on pre-trained language models for its good performance and computational efficiency. LoRA injects a product of two trainable rank decomposition matrices over the top of each frozen... | Bolin Ding, Yaliang Li, Youbang Sun, Zitao Li |  |
| 1171 |  |  [PhyloGFN: Phylogenetic inference with generative flow networks](https://openreview.net/forum?id=hB7SlfEmze) |  | 0 | Phylogenetics is a branch of computational biology that studies the evolutionary relationships among biological entities. Its long history and numerous applications notwithstanding, inference of phylogenetic trees from sequence data remains challenging: the high complexity of tree space poses a... | Dinghuai Zhang, Elliot Layne, Mathieu Blanchette, MingYang Zhou, Moksh Jain, Nikolay Malkin, Yoshua Bengio, Zichao Yan |  |
| 1172 |  |  [Training Bayesian Neural Networks with Sparse Subspace Variational Inference](https://openreview.net/forum?id=TskzCtpMEO) |  | 0 | Bayesian neural networks (BNNs) offer uncertainty quantification but come with the downside of substantially increased training and inference costs. Sparse BNNs have been investigated for efficient inference, typically by either slowly introducing sparsity throughout the training or by... | Junbo Li, Qiang Qiu, Ruqi Zhang, Zichen Miao |  |
| 1173 |  |  [Locality-Aware Graph Rewiring in GNNs](https://openreview.net/forum?id=4Ua4hKiAJX) |  | 0 | Graph Neural Networks (GNNs) are popular models for machine learning on graphs that typically follow the message-passing paradigm, whereby the feature of a node is updated recursively upon aggregating information over its neighbors. While exchanging messages over the input graph endows GNNs with a... | Ameya Velingker, Amin Saberi, Federico Barbero, Francesco Di Giovanni, Michael M. Bronstein |  |
| 1174 |  |  [Adapting to Distribution Shift by Visual Domain Prompt Generation](https://openreview.net/forum?id=sSaN4gxuEf) |  | 0 | In this paper, we aim to adapt a model at test-time using a few unlabeled data to address distribution shifts. To tackle the challenges of extracting domain knowledge from a limited amount of data, it is crucial to utilize correlated information from pre-trained backbones and source domains.... | Huan Liu, Konstantinos N. Plataniotis, Li Gu, Tao Zhong, Yang Wang, Yuanhao Yu, Zhixiang Chi |  |
| 1175 |  |  [Multimodal Patient Representation Learning with Missing Modalities and Labels](https://openreview.net/forum?id=Je5SHCKpPa) |  | 0 | Multimodal patient representation learning aims to integrate information from multiple modalities and generate comprehensive patient representations for subsequent clinical predictive tasks. However, many existing approaches either presuppose the availability of all modalities and labels for each... | Anant Dadu, Brian B. Avants, Faraz Faghri, Jimeng Sun, Mike A. Nalls, Nicholas J. Tustison, Zhenbang Wu |  |
| 1176 |  |  [Improved sampling via learned diffusions](https://openreview.net/forum?id=h4pNROsO06) |  | 0 | Recently, a series of papers proposed deep learning-based approaches to sample from target distributions using controlled diffusion processes, being trained only on the unnormalized target densities without access to samples. Building on previous work, we identify these approaches as special cases... | Julius Berner, Lorenz Richter |  |
| 1177 |  |  [MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models](https://openreview.net/forum?id=1tZbq88f27) |  | 0 | The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4... | Deyao Zhu, Jun Chen, Mohamed Elhoseiny, Xiang Li, Xiaoqian Shen |  |
| 1178 |  |  [Towards Establishing Guaranteed Error for Learned Database Operations](https://openreview.net/forum?id=6tqgL8VluV) |  | 0 | Machine learning models have demonstrated substantial performance enhancements over non-learned alternatives in various fundamental data management operations, including indexing (locating items in an array), cardinality estimation (estimating the number of matching records in a database), and... | Cyrus Shahabi, Sepanta Zeighami |  |
| 1179 |  |  [Quantifying the Plausibility of Context Reliance in Neural Machine Translation](https://openreview.net/forum?id=XTHfNGI3zT) |  | 0 | Establishing whether language models can use contextual information in a human-plausible way is important to ensure their safe adoption in real-world settings. However, the questions of $\textit{when}$ and $\textit{which parts}$ of the context affect model generations are typically tackled... | Arianna Bisazza, Gabriele Sarti, Grzegorz Chrupala, Malvina Nissim |  |
| 1180 |  |  [Bayesian Optimization through Gaussian Cox Process Models for Spatio-temporal Data](https://openreview.net/forum?id=9j1RD9LlWH) |  | 0 | Bayesian optimization (BO) has established itself as a leading strategy for efficiently optimizing expensive-to-evaluate functions. Existing BO methods mostly rely on Gaussian process (GP) surrogate models and are not applicable to (doubly-stochastic) Gaussian Cox processes, where the observation... | Mahdi Imani, Tian Lan, Yongsheng Mei |  |
| 1181 |  |  [Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX](https://openreview.net/forum?id=C4CxQmp9wc) |  | 0 | Open-source reinforcement learning (RL) environments have played a crucial role in driving progress in the development of AI algorithms. In modern RL research, there is a need for simulated environments that are performant, scalable, and modular to enable their utilization in a wider range of... | Alexandre Laterre, Andries P. Smit, Arnu Pretorius, Cemlyn N. Waters, Clément Bonnet, Daniel FurelosBlanco, Daniel Luo, Donal Byrne, Elshadai Tegegn, Laurence Illing Midgley, Matthew Macfarlane, Mohamed A. Mimouni, Nathan Grinsztajn, Omayma Mahjoub, Paul Duckworth, Raphaël Boige, Ruan de Kock, Sasha Abramowitz, Shikha Surana, Siddarth Singh, Tristan Kalloniatis, Ulrich A. Mbou Sob, Victor Le, Vincent Coyette |  |
| 1182 |  |  [Searching for High-Value Molecules Using Reinforcement Learning and Transformers](https://openreview.net/forum?id=nqlymMx42E) |  | 0 | Reinforcement learning (RL) over text representations can be effective for finding high-value policies that can search over graphs. However, RL requires careful structuring of the search space and algorithm design to be effective in this challenge. Through extensive experiments, we explore how... | Adriana Hugessen, Glen Berseth, Mariano Phielipp, Raj Ghugare, Santiago Miret |  |
| 1183 |  |  [Differentiable Euler Characteristic Transforms for Shape Classification](https://openreview.net/forum?id=MO632iPq3I) |  | 0 | The _Euler Characteristic Transform_ (ECT) is a powerful invariant, combining geometrical and topological characteristics of shapes and graphs. However, the ECT was hitherto unable to learn task-specific representations. We overcome this issue and develop a novel computational layer that enables... | Bastian Rieck, Ernst Röell |  |
| 1184 |  |  [Causally Aligned Curriculum Learning](https://openreview.net/forum?id=hp4yOjhwTs) |  | 0 | A pervasive challenge in Reinforcement Learning (RL) is the \`\`curse of dimensionality'' which is the exponential growth in the state-action space when optimizing a high-dimensional target task. The framework of curriculum learning trains the agent in a curriculum composed of a sequence of related... | Elias Bareinboim, Junzhe Zhang, Mingxuan Li |  |
| 1185 |  |  [Faster Sampling from Log-Concave Densities over Polytopes via Efficient Linear Solvers](https://openreview.net/forum?id=v63GWletn8) |  | 0 | We consider the problem of sampling from a logconcave distribution $\pi(\theta) \propto e^{-f(\theta)}$ constrained to a polytope $K:=${$\theta \in \mathbb{R}^d: A\theta \leq b$}, where $A\in \mathbb{R}^{m\times d}$ and $b \in \mathbb{R}^m$. The fastest-known algorithm for the setting when $f$ is... | Nisheeth K. Vishnoi, Oren Mangoubi |  |
| 1186 |  |  [Towards Characterizing Domain Counterfactuals for Invertible Latent Causal Models](https://openreview.net/forum?id=v1VvCWJAL8) |  | 0 | Answering counterfactual queries has important applications such as explainability, robustness, and fairness but is challenging when the causal variables are unobserved and the observations are non-linear mixtures of these latent variables, such as pixels in images. One approach is to recover the... | David I. Inouye, Murat Kocaoglu, Ruqi Bai, Sean Kulinski, Zeyu Zhou |  |
| 1187 |  |  [Grokking as the transition from lazy to rich training dynamics](https://openreview.net/forum?id=vt5mnLVIVo) |  | 0 | We propose that the grokking phenomenon, where the train loss of a neural network decreases much earlier than its test loss, can arise due to a neural network transitioning from lazy training dynamics to a rich, feature learning regime. To illustrate this mechanism, we study the simple setting of... | Blake Bordelon, Cengiz Pehlevan, Samuel J. Gershman, Tanishq Kumar |  |
| 1188 |  |  [Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement Learning](https://openreview.net/forum?id=M0xK8nPGvt) |  | 0 | Posterior sampling allows exploitation of prior knowledge on the environment's transition dynamics to improve the sample efficiency of reinforcement learning. The prior is typically specified as a class of parametric distributions, the design of which can be cumbersome in practice, often resulting... | Alexander Marx, Giorgia Ramponi, Marcello Restelli, Mirco Mutti, Riccardo De Santi |  |
| 1189 |  |  [Language Model Cascades: Token-Level Uncertainty And Beyond](https://openreview.net/forum?id=KgaBScZ4VI) |  | 0 | Recent advances in language models (LMs) have led to significant improvements in quality on complex NLP tasks, but at the expense of increased inference costs. A simple strategy to achieve more favorable cost-quality tradeoffs is cascading: here, a small model is invoked for most “easy” instances,... | Aditya Krishna Menon, Ankit Singh Rawat, Harikrishna Narasimhan, Neha Gupta, Sanjiv Kumar, Wittawat Jitkrittum |  |
| 1190 |  |  [The Marginal Value of Momentum for Small Learning Rate SGD](https://openreview.net/forum?id=3JjJezzVkT) |  | 0 | Momentum is known to accelerate the convergence of gradient descent in strongly convex settings without stochastic gradient noise. In stochastic optimization, such as training neural networks, folklore suggests that momentum may help deep learning optimization by reducing the variance of the... | Kaifeng Lyu, Runzhe Wang, Sadhika Malladi, Tianhao Wang, Zhiyuan Li |  |
| 1191 |  |  [Learning Polynomial Problems with SL(2, R)-Equivariance](https://openreview.net/forum?id=gyfXuRfxW2) |  | 0 | Optimizing and certifying the positivity of polynomials are fundamental primitives across mathematics and engineering applications, from dynamical systems to operations research. However, solving these problems in practice requires large semidefinite programs, with poor scaling in dimension and... | Hannah Lawrence, Mitchell Tong Harris |  |
| 1192 |  |  [Mixture of Weak and Strong Experts on Graphs](https://openreview.net/forum?id=wYvuY60SdD) |  | 0 | Realistic graphs contain both (1) rich self-features of nodes and (2) informative structures of neighborhoods, jointly handled by a Graph Neural Network (GNN) in the typical setup. We propose to decouple the two modalities by \*\*M\*\*ixture \*\*o\*\*f \*\*w\*\*eak and \*\*st\*\*rong experts... | Diyi Hu, Hanjia Lyu, Hanqing Zeng, Jiebo Luo, Yinglong Xia |  |
| 1193 |  |  [Transformers can optimally learn regression mixture models](https://openreview.net/forum?id=sLkj91HIZU) |  | 0 | Mixture models arise in many regression problems, but most methods have seen limited adoption partly due to these algorithms' highly-tailored and model-specific nature. On the other hand, transformers are flexible, neural sequence models that present the intriguing possibility of providing... | Abhimanyu Das, Rajat Sen, Reese Pathak, Weihao Kong |  |
| 1194 |  |  [Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective](https://openreview.net/forum?id=iCNOK45Csv) |  | 0 | Dataset distillation offers a potential means to enhance data efficiency in deep learning. Recent studies have shown its ability to counteract backdoor risks present in original training samples. In this study, we delve into the theoretical aspects of backdoor attacks and dataset distillation based... | ChiaMu Yu, MingYu Chung, PinYu Chen, ShengYen Chou, SyYen Kuo, TsungYi Ho |  |
| 1195 |  |  [Towards Diverse Behaviors: A Benchmark for Imitation Learning with Human Demonstrations](https://openreview.net/forum?id=6pPYRXKPpw) |  | 0 | Imitation learning with human data has demonstrated remarkable success in teaching robots in a wide range of skills. However, the inherent diversity in human behavior leads to the emergence of multi-modal data distributions, thereby presenting a formidable challenge for existing imitation learning... | Atalay Donat, Denis Blessing, Gerhard Neumann, Moritz Reuss, Rudolf Lioutikov, Xiaogang Jia, Xinkai Jiang |  |
| 1196 |  |  [Reconciling Spatial and Temporal Abstractions for Goal Representation](https://openreview.net/forum?id=odY3PkI5VB) |  | 0 | Goal representation affects the performance of Hierarchical Reinforcement Learn- ing (HRL) algorithms by decomposing the complex learning problem into easier subtasks. Recent studies show that representations that preserve temporally ab- stract environment dynamics are successful in solving... | Mehdi Zadem, Sao Mai Nguyen, Sergio Mover |  |
| 1197 |  |  [Estimating Conditional Mutual Information for Dynamic Feature Selection](https://openreview.net/forum?id=Oju2Qu9jvn) |  | 0 | Dynamic feature selection, where we sequentially query features to make accurate predictions with a minimal budget, is a promising paradigm to reduce feature acquisition costs and provide transparency into the prediction process. The problem is challenging, however, as it requires both making... | Ian Connick Covert, Soham Gadgil, SuIn Lee |  |
| 1198 |  |  [LLM Augmented LLMs: Expanding Capabilities through Composition](https://openreview.net/forum?id=jjA4O1vJRz) |  | 0 | Foundational models with billions of parameters which have been trained on large corpus of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to... | Abhishek Bapna, Bidisha Samanta, Nitish Gupta, Partha Talukdar, Prateek Jain, Rachit Bansal, Siddharth Dalmia, Sriram Ganapathy |  |
| 1199 |  |  [Quadratic models for understanding catapult dynamics of neural networks](https://openreview.net/forum?id=PvJnX3dwsD) |  | 0 | While neural networks can be approximated by linear models as their width increases, certain properties of wide neural networks cannot be captured by linear models. In this work we show that recently proposed Neural Quadratic Models can exhibit the "catapult phase" Lewkowycz et al. (2020) that... | Adityanarayanan Radhakrishnan, Chaoyue Liu, Libin Zhu, Mikhail Belkin |  |
| 1200 |  |  [Evaluating Representation Learning on the Protein Structure Universe](https://openreview.net/forum?id=sTYuRVrdK3) |  | 0 | We introduce ProteinWorkshop, a comprehensive benchmark suite for representation learning on protein structures with Geometric Graph Neural Networks. We consider large-scale pre-training and downstream tasks on both experimental and predicted structures to enable the systematic evaluation of the... | Alex Morehead, Arian Rokkum Jamasb, Chaitanya K. Joshi, Charles Harris, Jian Tang, Jianlin Cheng, Kieran Didi, Pietro Lio, Simon V. Mathis, Tom L. Blundell, Zuobai Zhang |  |
| 1201 |  |  [T-MARS: Improving Visual Representations by Circumventing Text Feature Learning](https://openreview.net/forum?id=ViPtjIVzUw) |  | 0 | Large web-crawled multimodal datasets have powered a slew of new methods for learning general-purpose visual representations, advancing the state of the art in computer vision and revolutionizing zero- and few-shot recognition. One crucial decision facing practitioners is how, if at all, to curate... | Aditi Raghunathan, J. Zico Kolter, Pratyush Maini, Sachin Goyal, Zachary Chase Lipton |  |
| 1202 |  |  [Nougat: Neural Optical Understanding for Academic Documents](https://openreview.net/forum?id=fUtxNAKpdV) |  | 0 | Scientific knowledge is predominantly stored in books and scientific journals, often in the form of PDFs. However, the PDF format leads to a loss of semantic information, particularly for mathematical expressions. We propose Nougat (Neural Optical Understanding for Academic Documents), a Visual... | Guillem Cucurull, Lukas Blecher, Robert Stojnic, Thomas Scialom |  |
| 1203 |  |  [When can transformers reason with abstract symbols?](https://openreview.net/forum?id=STUGfUz8ob) |  | 0 | We investigate the capabilities of transformer models on relational reasoning tasks. In these tasks, models are trained on a set of strings encoding abstract relations, and are then tested out-of-distribution on data that contains symbols that did not appear in the training dataset. We prove that... | Emmanuel Abbe, Enric BoixAdserà, Etai Littwin, Joshua M. Susskind, Omid Saremi, Samy Bengio |  |
| 1204 |  |  [Towards Generative Abstract Reasoning: Completing Raven's Progressive Matrix via Rule Abstraction and Selection](https://openreview.net/forum?id=IcR1OOFzxm) |  | 0 | Endowing machines with abstract reasoning ability has been a long-term research topic in artificial intelligence. Raven's Progressive Matrix (RPM) is widely used to probe abstract visual reasoning in machine intelligence, where models will analyze the underlying rules and select one image from... | Bin Li, Fan Shi, Xiangyang Xue |  |
| 1205 |  |  [A Characterization Theorem for Equivariant Networks with Point-wise Activations](https://openreview.net/forum?id=79FVDdfoSR) |  | 0 | Equivariant neural networks have shown improved performance, expressiveness and sample complexity on symmetrical domains. But for some specific symmetries, representations, and choice of coordinates, the most common point-wise activations, such as ReLU, are not equivariant, hence they cannot be... | Bruno Lepri, Gabriele Santin, Marco Pacini, Xiaowen Dong |  |
| 1206 |  |  [Think before you speak: Training Language Models With Pause Tokens](https://openreview.net/forum?id=ph04CRkPdC) |  | 0 | Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{\rm th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before... | Aditya Krishna Menon, Ankit Singh Rawat, Sachin Goyal, Sanjiv Kumar, Vaishnavh Nagarajan, Ziwei Ji |  |
| 1207 |  |  [Talk like a Graph: Encoding Graphs for Large Language Models](https://openreview.net/forum?id=IuXR1CCrSi) |  | 0 | Graphs are a powerful tool for representing and analyzing complex relationships in real-world applications such as social networks, recommender systems, and computational finance. Reasoning on graphs is essential for drawing inferences about the relationships between entities in a complex system,... | Bahare Fatemi, Bryan Perozzi, Jonathan Halcrow |  |
| 1208 |  |  [Privately Aligning Language Models with Reinforcement Learning](https://openreview.net/forum?id=3d0OmYTNui) |  | 0 | Positioned between pre-training and user deployment, aligning large language models (LLMs) through reinforcement learning (RL) has emerged as a prevailing strategy for training instruction following-models such as ChatGPT. In this work, we initiate the study of privacy-preserving alignment of LLMs... | Arturs Backurs, Fan Wu, Huseyin A. Inan, Janardhan Kulkarni, Robert Sim, Varun Chandrasekaran |  |
| 1209 |  |  [YaRN: Efficient Context Window Extension of Large Language Models](https://openreview.net/forum?id=wHBfxhZu1u) |  | 0 | Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient... | Bowen Peng, Enrico Shippole, Honglu Fan, Jeffrey Quesnelle |  |
| 1210 |  |  [Accelerating Sinkhorn algorithm with sparse Newton iterations](https://openreview.net/forum?id=Kuj5gVp5GQ) |  | 0 | Computing the optimal transport distance between statistical distributions is a fundamental task in machine learning. One remarkable recent advancement is entropic regularization and the Sinkhorn algorithm, which utilizes only matrix scaling and guarantees an approximated solution with near-linear... | Elisa Tardini, Holakou Rahmanian, Kiran Koshy Thekumparampil, Lexing Ying, Michael Shavlovsky, Tesi Xiao, Xun Tang |  |
| 1211 |  |  [Functional Interpolation for Relative Positions improves Long Context Transformers](https://openreview.net/forum?id=rR03qFesqk) |  | 0 | Preventing the performance decay of Transformers on inputs longer than those used for training has been an important challenge in extending the context length of these models. Though the Transformer architecture has fundamentally no limits on the input sequence lengths it can process, the choice of... | Chong You, Guru Guruganesh, Joshua Ainslie, Manzil Zaheer, Sanjiv Kumar, Santiago Ontañón, Shanda Li, Srinadh Bhojanapalli, Sumit Sanghai, Yiming Yang |  |
| 1212 |  |  [FeatUp: A Model-Agnostic Framework for Features at Any Resolution](https://openreview.net/forum?id=GkJiNn2QDF) |  | 0 | Deep features are a cornerstone of computer vision research, capturing image semantics and enabling the community to solve downstream tasks even in the zero- or few-shot regime. However, these features often lack the spatial resolution to directly perform dense prediction tasks like segmentation... | Axel Feldmann, Laura E. Brandt, Mark Hamilton, Stephanie Fu, William T. Freeman, Zhoutong Zhang |  |
| 1213 |  |  [Masked Autoencoders with Multi-Window Local-Global Attention Are Better Audio Learners](https://openreview.net/forum?id=Q53QLftNkA) |  | 0 | In this work, we propose a Multi-Window Masked Autoencoder (MW-MAE) fitted with a novel Multi-Window Multi-Head Attention (MW-MHA) module that facilitates the modelling of local-global interactions in every decoder transformer block through attention heads of several distinct local and global... | Lars Kai Hansen, Sarthak Yadav, Sergios Theodoridis, ZhengHua Tan |  |
| 1214 |  |  [Toward Optimal Policy Population Growth in Two-Player Zero-Sum Games](https://openreview.net/forum?id=J2TZgj3Tac) |  | 0 | In competitive two-agent environments, deep reinforcement learning (RL) methods like Policy Space Response Oracles (PSRO) often increase exploitability between iterations, which is problematic when training in large games. To address this issue, we introduce anytime double oracle (ADO), an... | JB Lanier, Kevin A. Wang, Pierre Baldi, Roy Fox, Stephen Marcus McAleer, Tuomas Sandholm |  |
| 1215 |  |  [Sparse Autoencoders Find Highly Interpretable Features in Language Models](https://openreview.net/forum?id=F76bwRSLeK) |  | 0 | One of the roadblocks to a better understanding of neural networks' internals is \textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks... | Aidan Ewart, Hoagy Cunningham, Lee Sharkey, Logan Riggs Smith, Robert Huben |  |
| 1216 |  |  [OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning](https://openreview.net/forum?id=FbuyDzZTPt) |  | 0 | Recent works have shown that by using large pre-trained models along with learnable prompts, rehearsal-free methods for class-incremental learning (CIL) settings can achieve superior performance to prominent rehearsal-based ones. Rehearsal-free CIL methods struggle with distinguishing classes from... | ChunFu Richard Chen, Hsiang Hsu, WeiCheng Huang |  |
| 1217 |  |  [Chain of Log-Concave Markov Chains](https://openreview.net/forum?id=yiMB2DOjsR) |  | 0 | We introduce a theoretical framework for sampling from unnormalized densities based on a smoothing scheme that uses an isotropic Gaussian kernel with a single fixed noise scale. We prove one can decompose sampling from a density (minimal assumptions made on the density) into a sequence of sampling... | Francis R. Bach, Ji Won Park, Saeed Saremi |  |
| 1218 |  |  [Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data](https://openreview.net/forum?id=BxHgpC6FNv) |  | 0 | Neural networks trained by gradient descent (GD) have exhibited a number of surprising generalization behaviors. First, they can achieve a perfect fit to noisy training data and still generalize near-optimally, showing that overfitting can sometimes be benign. Second, they can undergo a period of... | Gal Vardi, Spencer Frei, Wei Hu, Yutong Wang, Zhiwei Xu |  |
| 1219 |  |  [VBH-GNN: Variational Bayesian Heterogeneous Graph Neural Networks for Cross-subject Emotion Recognition](https://openreview.net/forum?id=EArTDUmILF) |  | 0 | The research on human emotion under electroencephalogram (EEG) is an emerging field in which cross-subject emotion recognition (ER) is a promising but challenging task. Many approaches attempt to find emotionally relevant domain-invariant features using domain adaptation (DA) to improve the... | Chenyu Liu, Liming Zhai, Xinliang Zhou, Yang Liu, Zhengri Zhu, Ziyu Jia |  |
| 1220 |  |  [AUGCAL: Improving Sim2Real Adaptation by Uncertainty Calibration on Augmented Synthetic Images](https://openreview.net/forum?id=WNQjN5HzXt) |  | 0 | Synthetic data (Sim) drawn from simulators have emerged as a popular alternativefor training models where acquiring annotated real-world images is difficult. However, transferring models trained on synthetic images to real-world applicationscan be challenging due to appearance disparities. A... | Bharat Goyal, Boglarka Ecsedi, Judy Hoffman, Prithvijit Chattopadhyay, Viraj Prabhu |  |
| 1221 |  |  [Quality-Diversity through AI Feedback](https://openreview.net/forum?id=owokKCrGYr) |  | 0 | In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the... | Andrew Dai, Grégory Schott, Hannah Benita Teufel, Herbie Bradley, Jeff Clune, Jenny Zhang, Joel Lehman, Kenneth O. Stanley, Koen Oostermeijer, Marco Bellagente |  |
| 1222 |  |  [Learning from Sparse Offline Datasets via Conservative Density Estimation](https://openreview.net/forum?id=4WM0OogPTx) |  | 0 | Offline reinforcement learning (RL) offers a promising direction for learning policies from pre-collected datasets without requiring further interactions with the environment. However, existing methods struggle to handle out-of-distribution (OOD) extrapolation errors, especially in sparse reward or... | Ding Zhao, Henry Lam, Yihang Yao, Zhepeng Cen, Zitong Wang, Zuxin Liu |  |
| 1223 |  |  [On the Expressivity of Objective-Specification Formalisms in Reinforcement Learning](https://openreview.net/forum?id=qr4ECbGcSj) |  | 0 | Most algorithms in reinforcement learning (RL) require that the objective is formalised with a Markovian reward function. However, it is well-known that certain tasks cannot be expressed by means of an objective in the Markov rewards formalism, motivating the study of alternative... | Charlie Griffin, Halfdan Holm, Joar Max Viktor Skalse, Marcus Williams, Max Heitmann, Rohan Subramani |  |
| 1224 |  |  [Implicit Maximum a Posteriori Filtering via Adaptive Optimization](https://openreview.net/forum?id=auUngos7eR) |  | 0 | Bayesian filtering approximates the true underlying behavior of a time-varying system by inverting an explicit generative model to convert noisy measurements into state estimates. This process typically requires matrix storage, inversion, and multiplication or Monte Carlo estimation, none of which... | Gianluca M. Bencomo, Jake Snell, Thomas L. Griffiths |  |
| 1225 |  |  [Hybrid Internal Model: Learning Agile Legged Locomotion with Simulated Robot Response](https://openreview.net/forum?id=93LoCyww8o) |  | 0 | Robust locomotion control depends on accurate state estimations. However, the sensors of most legged robots can only provide partial and noisy observations, making the estimation particularly challenging, especially for external states like terrain frictions and elevation maps. Inspired by the... | Jiangmiao Pang, Jiawei Gao, Junfeng Long, Liu Cao, Quanyi Li, Zirui Wang |  |
| 1226 |  |  [S2AC: Energy-Based Reinforcement Learning with Stein Soft Actor Critic](https://openreview.net/forum?id=rAHcTCMaLc) |  | 0 | Learning expressive stochastic policies instead of deterministic ones has been proposed to achieve better stability, sample complexity and robustness. Notably, in Maximum Entropy reinforcement learning (MaxEnt RL), the policy is modeled as an expressive energy-based model (EBM) over the Q-values.... | Billel Mokeddem, Bo An, Haipeng Chen, Linsey Pang, Safa Messaoud, Sanjay Chawla, Zhenghai Xue |  |
| 1227 |  |  [A Newborn Embodied Turing Test for Comparing Object Segmentation Across Animals and Machines](https://openreview.net/forum?id=qhkEOCcVX9) |  | 0 | Newborn brains rapidly learn to solve challenging object recognition tasks, including segmenting objects from backgrounds and recognizing objects across novel backgrounds and viewpoints. Conversely, modern machine-learning (ML) algorithms are "data hungry," requiring more training data than brains... | Denizhan Pak, Justin N. Wood, Manju Garimella, Samantha Marie Waters Wood |  |
| 1228 |  |  [Robust Model-Based Optimization for Challenging Fitness Landscapes](https://openreview.net/forum?id=xhEN0kJh4q) |  | 0 | Protein design, a grand challenge of the day, involves optimization on a fitness landscape, and leading methods adopt a model-based approach where a model is trained on a training set (protein sequences and fitness) and proposes candidates to explore next. These methods are challenged by sparsity... | Alexander G. Schwing, Ehsan Saleh, Martin D. Burke, Saba Ghaffari, Saurabh Sinha, YuXiong Wang |  |
| 1229 |  |  [Solving High Frequency and Multi-Scale PDEs with Gaussian Processes](https://openreview.net/forum?id=q4AEBLHuA6) |  | 0 | Machine learning based solvers have garnered much attention in physical simulation and scientific computing, with a prominent example, physics-informed neural networks (PINNs). However, PINNs often struggle to solve high-frequency and multi-scale PDEs, which can be due to spectral bias during... | Da Long, Madison Cooley, Mike Kirby, Shandian Zhe, Shibo Li, Shikai Fang |  |
| 1230 |  |  [OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text](https://openreview.net/forum?id=jKHmjlpViu) |  | 0 | There is growing evidence that pretraining on high quality, carefully thought-out tokens such as code or mathematics plays an important role in improving the reasoning abilities of large language models. For example, Minerva, a PaLM model finetuned on billions of tokens of mathematical documents... | Jimmy Ba, Keiran Paster, Marco Dos Santos, Zhangir Azerbayev |  |
| 1231 |  |  [Replay across Experiments: A Natural Extension of Off-Policy RL](https://openreview.net/forum?id=Nf4Lm6fXN8) |  | 0 | Replaying data is a principal mechanism underlying the stability and data efficiency of off-policy reinforcement learning (RL). We present an effective yet simple framework to extend the use of replays across multiple experiments, minimally adapting the RL workflow for sizeable improvements in... | Ben Moran, Dhruva Tirumala, Guy Lever, José Enrique Chen, Leonard Hasenclever, Markus Wulfmeier, Martin A. Riedmiller, Nicolas Heess, Sandy H. Huang, Thomas Lampe, Tim Hertweck, Tuomas Haarnoja |  |
| 1232 |  |  [Understanding Transferable Representation Learning and Zero-shot Transfer in CLIP](https://openreview.net/forum?id=S5yOuNfSA0) |  | 0 | Multi-modal learning has become increasingly popular due to its ability to leverage information from different data sources (e.g., text and images) to improve the model performance. Recently, CLIP has emerged as an effective approach that employs vision-language contrastive pretraining to learn... | Quanquan Gu, Yihe Deng, Yuanzhi Li, Zixiang Chen |  |
| 1233 |  |  [Conditional Variational Diffusion Models](https://openreview.net/forum?id=YOKnEkIuoi) |  | 0 | Inverse problems aim to determine parameters from observations, a crucial task in engineering and science. Lately, generative models, especially diffusion models, have gained popularity in this area for their ability to produce realistic solutions and their good mathematical properties. Despite... | Artur Yakimovich, Gabriel della Maggiora, Harry Horsley, Luis Alberto Croquevielle, Nikita Deshpande, Thomas Heinis |  |
| 1234 |  |  [Better Neural PDE Solvers Through Data-Free Mesh Movers](https://openreview.net/forum?id=hj9ZuNimRl) |  | 0 | Recently, neural networks have been extensively employed to solve partial differential equations (PDEs) in physical system modeling. While major studies focus on learning system evolution on predefined static mesh discretizations, some methods utilize reinforcement learning or supervised learning... | Peiyan Hu, Yue Wang, ZhiMing Ma |  |
| 1235 |  |  [From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module](https://openreview.net/forum?id=0JsRZEGZ7L) |  | 0 | Latent Graph Inference (LGI) relaxed the reliance of Graph Neural Networks (GNNs) on a given graph topology by dynamically learning it. However, most of LGI methods assume to have a (noisy, incomplete, improvable, ...) input graph to rewire and can solely learn regular graph topologies. In the wake... | Claudio Battiloro, Indro Spinelli, Lev Telyatnikov, Michael M. Bronstein, Paolo Di Lorenzo, Simone Scardapane |  |
| 1236 |  |  [BEND: Benchmarking DNA Language Models on Biologically Meaningful Tasks](https://openreview.net/forum?id=uKB4cFNQFg) |  | 0 | The genome sequence contains the blueprint for governing cellular processes. While the availability of genomes has vastly increased over the last decades, experimental annotation of the various functional, non-coding and regulatory elements encoded in the DNA sequence remains both expensive and... | Dennis Madsen, Dennis Pultz, Felix Teufel, Frederikke Isa Marin, Marc Horlacher, Ole Winther, Wouter Boomsma |  |
| 1237 |  |  [Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks](https://openreview.net/forum?id=up6hr4hIQH) |  | 0 | Graph Neural Networks (GNNs) are neural models that leverage the dependency structure in graphical data via message passing among the graph nodes. GNNs have emerged as pivotal architectures in analyzing graph-structured data, and their expansive application in sensitive domains requires a... | Dongsheng Luo, Farhad Shirani, Haifeng Chen, Hua Wei, Tianchun Wang, Wei Cheng, Xu Zheng, Zhuomin Chen |  |
| 1238 |  |  [Piecewise Linear Parametrization of Policies: Towards Interpretable Deep Reinforcement Learning](https://openreview.net/forum?id=hOMVq57Ce0) |  | 0 | Learning inherently interpretable policies is a central challenge in the path to developing autonomous agents that humans can trust. Linear policies can justify their decisions while interacting in a dynamic environment, but their reduced expressivity prevents them from solving hard tasks. Instead,... | Joelle Pineau, Maxime Wabartha |  |
| 1239 |  |  [Optimal criterion for feature learning of two-layer linear neural network in high dimensional interpolation regime](https://openreview.net/forum?id=Jc0FssXh2R) |  | 0 | Deep neural networks with feature learning have shown surprising generalization performance in high dimensional settings, but it has not been fully understood how and when they enjoy the benefit of feature learning. In this paper, we theoretically analyze the statistical properties of the benefits... | Keita Suzuki, Taiji Suzuki |  |
| 1240 |  |  [Neural Optimal Transport with General Cost Functionals](https://openreview.net/forum?id=gIiz7tBtYZ) |  | 0 | We introduce a novel neural network-based algorithm to compute optimal transport (OT) plans for general cost functionals. In contrast to common Euclidean costs, i.e., $\ell^1$ or $\ell^2$, such functionals provide more flexibility and allow using auxiliary information, such as class labels, to... | Alexander Korotin, Arip Asadulaev, Evgeny Burnaev, Petr Mokrov, Vage Egiazarian |  |
| 1241 |  |  [A Topological Perspective on Demystifying GNN-Based Link Prediction Performance](https://openreview.net/forum?id=apA6SSXx2e) |  | 0 | Graph Neural Networks (GNNs) have shown great promise in learning node embeddings for link prediction (LP). While numerous studies improve the overall GNNs' LP performance, none have explored their varying performance across different nodes and the underlying reasons. To this end, we demystify... | Neil Shah, Tong Zhao, Tyler Derr, Xueqi Cheng, Yu Wang, Yunchao Liu, Yuying Zhao |  |
| 1242 |  |  [Time-Efficient Reinforcement Learning with Stochastic Stateful Policies](https://openreview.net/forum?id=5liV2xUdJL) |  | 0 | Stateful policies play an important role in reinforcement learning, such as handling partially observable environments, enhancing robustness, or imposing an inductive bias directly into the policy structure. The conventional method for training stateful policies is Backpropagation Through Time... | Davide Tateo, Firas AlHafez, Guoping Zhao, Jan Peters |  |
| 1243 |  |  [Open the Black Box: Step-based Policy Updates for Temporally-Correlated Episodic Reinforcement Learning](https://openreview.net/forum?id=mnipav175N) |  | 0 | Current advancements in reinforcement learning (RL) have predominantly focused on learning step-based policies that generate actions for each perceived state. While these methods efficiently leverage step information from environmental interaction, they often ignore the temporal correlation between... | Dominik Roth, Fabian Otto, Ge Li, Gerhard Neumann, Hongyi Zhou, Rudolf Lioutikov, Serge Thilges |  |
| 1244 |  |  [Emergent Communication with Conversational Repair](https://openreview.net/forum?id=Sy8upuD6Bw) |  | 0 | Research on conversation has put emphasis on the importance of a multi-level communication system, in which the interlocutors aim to establish and maintain common ground. In natural conversations, repair mechanisms such as clarification requests are frequently used to improve mutual understanding.... | Mitja Nikolaus |  |
| 1245 |  |  [Can we get the best of both Binary Neural Networks and Spiking Neural Networks for Efficient Computer Vision?](https://openreview.net/forum?id=lGUyAuuTYZ) |  | 0 | Binary Neural networks (BNN) have emerged as an attractive computing paradigm for a wide range of low-power vision tasks. However, state-of-the-art (SOTA) BNNs do not yield any sparsity, and induce a significant number of non-binary operations. On the other hand, activation sparsity can be provided... | Gourav Datta, Peter Anthony Beerel, Zeyu Liu |  |
| 1246 |  |  [Node2ket: Efficient High-Dimensional Network Embedding in Quantum Hilbert Space](https://openreview.net/forum?id=lROh08eK6n) |  | 0 | Network embedding (NE) is a prominent technique for network analysis where the nodes are represented as vectorized embeddings in a continuous space. Existing works tend to resort to the low-dimensional embedding space for efficiency and less risk of over-fitting. In this paper, we explore a new NE... | Hao Xiong, Junchi Yan, Wei Tan, Yehui Tang, Yunlin He |  |
| 1247 |  |  [Latent Representation and Simulation of Markov Processes via Time-Lagged Information Bottleneck](https://openreview.net/forum?id=bH6T0Jjw5y) |  | 0 | Markov processes are widely used mathematical models for describing dynamic systems in various fields. However, accurately simulating large-scale systems at long time scales is computationally expensive due to the short time steps required for accurate integration. In this paper, we introduce an... | Bastiaan S. Veeling, Marco Federici, Patrick Forré, Ryota Tomioka |  |
| 1248 |  |  [Non-Exchangeable Conformal Risk Control](https://openreview.net/forum?id=j511LaqEeP) |  | 0 | Split conformal prediction has recently sparked great interest due to its ability to provide formally guaranteed uncertainty sets or intervals for predictions made by black-box neural models, ensuring a predefined probability of containing the actual ground truth. While the original formulation... | André F. T. Martins, António Farinhas, Chrysoula Zerva, Dennis Ulmer |  |
| 1249 |  |  [Provably Efficient UCB-type Algorithms For Learning Predictive State Representations](https://openreview.net/forum?id=jId5PXbBbX) |  | 0 | The general sequential decision-making problem, which includes Markov decision processes (MDPs) and partially observable MDPs (POMDPs) as special cases, aims at maximizing a cumulative reward by making a sequence of decisions based on a history of observations and actions over time. Recent studies... | Jing Yang, Ruiquan Huang, Yingbin Liang |  |
| 1250 |  |  [Lifting Architectural Constraints of Injective Flows](https://openreview.net/forum?id=kBNIx4Biq4) |  | 0 | Normalizing Flows explicitly maximize a full-dimensional likelihood on the training data. However, real data is typically only supported on a lower-dimensional manifold leading the model to expend significant compute on modeling noise. Injective Flows fix this by jointly learning a manifold and the... | Armand Rousselot, Felix Draxler, Lea Zimmermann, Peter Sorrenson, Sander Hummerich, Ullrich Köthe |  |
| 1251 |  |  [Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models](https://openreview.net/forum?id=ptCIlV24YZ) |  | 0 | The advent of large pre-trained models has brought about a paradigm shift in both visual representation learning and natural language processing. However, clustering unlabeled images, as a fundamental and classic machine learning problem, still lacks an effective solution, particularly for... | Benjamin David Haeffele, René Vidal, Shengbang Tong, Tianjiao Ding, Tianzhe Chu, Xili Dai, Yi Ma |  |
| 1252 |  |  [Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond](https://openreview.net/forum?id=khAE1sTMdX) |  | 0 | Developing a universal model that can effectively harness heterogeneous resources and respond to a wide range of personalized needs has been a longstanding community aspiration. Our daily choices, especially in domains like fashion and retail, are substantially shaped by multi-modal data, such as... | Bowen Jin, Hanqing Lu, Hansi Zeng, Jianhui Sun, Jingrui He, Qingyu Yin, Ruirui Li, Suhang Wang, Tianxin Wei, Xianfeng Tang, Zhengyang Wang |  |
| 1253 |  |  [Orbit-Equivariant Graph Neural Networks](https://openreview.net/forum?id=GkJOCga62u) |  | 0 | Equivariance is an important structural property that is captured by architectures such as graph neural networks (GNNs). However, equivariant graph functions cannot produce different outputs for similar nodes, which may be undesirable when the function is trying to optimize some global graph... | Bernardo Cuenca Grau, Ian Horrocks, Matthew Morris |  |
| 1254 |  |  [Object-Centric Learning with Slot Mixture Module](https://openreview.net/forum?id=aBUidW4Nkd) |  | 0 | Object-centric architectures usually apply a differentiable module to the entire feature map to decompose it into sets of entity representations called slots. Some of these methods structurally resemble clustering algorithms, where the cluster's center in latent space serves as a slot... | Aleksandr Panov, Alexey K. Kovalev, Daniil E. Kirilenko, Vitaliy Vorobyov |  |
| 1255 |  |  [Ring-A-Bell! How Reliable are Concept Removal Methods For Diffusion Models?](https://openreview.net/forum?id=lm7MRcsFiS) |  | 0 | Diffusion models for text-to-image (T2I) synthesis, such as Stable Diffusion (SD), have recently demonstrated exceptional capabilities for generating high-quality content. However, this progress has raised several concerns of potential misuse, particularly in creating copyrighted, prohibited, and... | Bo Li, ChiaMu Yu, ChiaYi Hsu, ChihHsun Lin, Chulin Xie, ChunYing Huang, JiaYou Chen, PinYu Chen, YuLin Tsai |  |
| 1256 |  |  [PerceptionCLIP: Visual Classification by Inferring and Conditioning on Contexts](https://openreview.net/forum?id=2Oiee202rd) |  | 0 | Vision-language models like CLIP are widely used in zero-shot image classification due to their ability to understand various visual concepts and natural language descriptions. However, how to fully leverage CLIP's unprecedented human-like understanding capabilities to achieve better performance is... | Bang An, Chaithanya Kumar Mummadi, Furong Huang, MichaelAndrei PanaitescuLiess, Sicheng Zhu |  |
| 1257 |  |  [VeRA: Vector-based Random Matrix Adaptation](https://openreview.net/forum?id=NjNfLdxr3A) |  | 0 | Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous per-user or per-task adapted models. In this work, we present... | Dawid Jan Kopiczko, Tijmen Blankevoort, Yuki M. Asano |  |
| 1258 |  |  [AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?](https://openreview.net/forum?id=Bb21JPnhhr) |  | 0 | Can we better anticipate an actor’s future actions (e.g. mix eggs) by knowing what commonly happens after the current action (e.g. crack eggs)? What if the actor also shares the goal (e.g. make fried rice) with us? The long-term action anticipation (LTA) task aims to predict an actor’s future... | Ce Zhang, Changcheng Fu, Chen Sun, Kwonjoon Lee, Minh Quan Do, Nakul Agarwal, Qi Zhao, Shijie Wang |  |
| 1259 |  |  [A Plug-and-Play Image Registration Network](https://openreview.net/forum?id=DGez4B2a6Y) |  | 0 | Deformable image registration (DIR) is an active research topic in biomedical imaging. There is a growing interest in developing DIR methods based on deep learning (DL). A traditional DL approach to DIR is based on training a convolutional neural network (CNN) to estimate the registration field... | Hongyu An, Junhao Hu, Ulugbek Kamilov, Weijie Gan, Zhixin Sun |  |
| 1260 |  |  [BENO: Boundary-embedded Neural Operators for Elliptic PDEs](https://openreview.net/forum?id=ZZTkLDRmkg) |  | 0 | Elliptic partial differential equations (PDEs) are a major class of time-independent PDEs that play a key role in many scientific and engineering domains such as fluid dynamics, plasma physics, and solid mechanics. Recently, neural operators have emerged as a promising technique to solve elliptic... | Anubhav Dwivedi, Haixin Wang, Jiaxin Li, Kentaro Hara, Tailin Wu |  |
| 1261 |  |  [Bayesian Neural Controlled Differential Equations for Treatment Effect Estimation](https://openreview.net/forum?id=uwO71a8wET) |  | 0 | Treatment effect estimation in continuous time is crucial for personalized medicine. However, existing methods for this task are limited to point estimates of the potential outcomes, whereas uncertainty estimates have been ignored. Needless to say, uncertainty quantification is crucial for reliable... | Dennis Frauen, Konstantin Hess, Stefan Feuerriegel, Valentyn Melnychuk |  |
| 1262 |  |  [Training Socially Aligned Language Models on Simulated Social Interactions](https://openreview.net/forum?id=NddKiWtdUm) |  | 0 | The goal of social alignment for AI systems is to make sure these models can conduct themselves appropriately following social values. Unlike humans who establish a consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly recite the corpus in... | Chenyan Jia, Diyi Yang, Ge Zhang, Ruibo Liu, Ruixin Yang, Soroush Vosoughi |  |
| 1263 |  |  [The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks](https://openreview.net/forum?id=vE1e1mLJ0U) |  | 0 | Biological cortical neurons are remarkably sophisticated computational devices, temporally integrating their vast synaptic input over an intricate dendritic tree, subject to complex, nonlinearly interacting internal biological processes. A recent study proposed to characterize this complexity by... | Aaron Spieler, Anna Levina, Bernhard Schölkopf, Georg Martius, Nasim Rahaman |  |
| 1264 |  |  [Incentivized Truthful Communication for Federated Bandits](https://openreview.net/forum?id=ykEixGIJYb) |  | 0 | To enhance the efficiency and practicality of federated bandit learning, recent advances have introduced incentives to motivate communication among clients, where a client participates only when the incentive offered by the server outweighs its participation cost. However, existing incentive... | Chuanhao Li, Haifeng Xu, Hongning Wang, Tianze Ren, Zhepei Wei |  |
| 1265 |  |  [Large-scale Training of Foundation Models for Wearable Biosignals](https://openreview.net/forum?id=pC3WJHf51j) |  | 0 | Tracking biosignals is crucial for monitoring wellness and preempting the development of severe medical conditions. Today, wearable devices can conveniently record various biosignals, creating the opportunity to monitor health status without disruption to one's daily routine. Despite widespread use... | Andrew C. Miller, Ian Shapiro, Oussama Elachqar, Saba Emrani, Salar Abbaspourazad, Udhyakumar Nallasamy |  |
| 1266 |  |  [Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization](https://openreview.net/forum?id=OIsahq1UYC) |  | 0 | We tackle the problem of sampling from intractable high-dimensional density functions, a fundamental task that often appears in machine learning and statistics. We extend recent sampling-based approaches that leverage controlled stochastic processes to model approximate samples from these target... | Aaron C. Courville, ChengHao Liu, Dinghuai Zhang, Ricky T. Q. Chen, Yoshua Bengio |  |
| 1267 |  |  [On Trajectory Augmentations for Off-Policy Evaluation](https://openreview.net/forum?id=eMNN0wIyVw) |  | 0 | In the realm of reinforcement learning (RL), off-policy evaluation (OPE) holds a pivotal position, especially in high-stake human-involved scenarios such as e-learning and healthcare. Applying OPE to these domains is often challenging with scarce and underrepresentative offline training... | Ge Gao, Min Chi, Miroslav Pajic, Qitong Gao, Song Ju, Xi Yang |  |
| 1268 |  |  [Federated Wasserstein Distance](https://openreview.net/forum?id=rsg1mvUahT) |  | 0 | We introduce a principled way of computing the Wasserstein distance between two distributions in a federated manner. Namely, we show how to estimate the Wasserstein distance between two samples stored and kept on different devices/clients whilst a central entity/server orchestrates the computations... | Alain Rakotomamonjy, Kimia Nadjahi, Liva Ralaivola |  |
| 1269 |  |  [Fourier Transporter: Bi-Equivariant Robotic Manipulation in 3D](https://openreview.net/forum?id=UulwvAU1W0) |  | 0 | Many complex robotic manipulation tasks can be decomposed as a sequence of pick and place actions. Training a robotic agent to learn this sequence over many different starting conditions typically requires many iterations or demonstrations, especially in 3D environments. In this work, we propose... | Dian Wang, Haojie Huang, Owen Howell, Robert Platt, Robin Walters, Xupeng Zhu |  |
| 1270 |  |  [Clifford Group Equivariant Simplicial Message Passing Networks](https://openreview.net/forum?id=Zz594UBNOH) |  | 0 | We introduce Clifford Group Equivariant Simplicial Message Passing Networks, a method for steerable $\mathrm{E}(n)$-equivariant message passing on simplicial complexes. Our method integrates the expressivity of Clifford group-equivariant layers with simplicial message passing, which is... | Cong Liu, David Ruhe, Floor Eijkelboom, Patrick Forré |  |
| 1271 |  |  [Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation](https://openreview.net/forum?id=NxoFmGgWC9) |  | 0 | Generative pre-trained models have demonstrated remarkable effectiveness in language and vision domains by learning useful representations. In this paper, we extend the scope of this effectiveness by showing that visual robot manipulation can significantly benefit from large-scale video generative... | Chilam Cheang, Guangzeng Chen, Hang Li, Hongtao Wu, Jiafeng Xu, Minghuan Liu, Tao Kong, Xinghang Li, Ya Jing |  |
| 1272 |  |  [Vision-by-Language for Training-Free Compositional Image Retrieval](https://openreview.net/forum?id=EDPxCjXzSb) |  | 0 | Given an image and a target modification (e.g an image of the Eiffel tower and the text “without people and at night-time”), Compositional Image Retrieval (CIR) aims to retrieve the relevant target image in a database. While supervised approaches rely on annotating triplets that is costly (i.e.... | Karsten Roth, Massimiliano Mancini, Shyamgopal Karthik, Zeynep Akata |  |
| 1273 |  |  [Benign Oscillation of Stochastic Gradient Descent with Large Learning Rate](https://openreview.net/forum?id=wYmvN3sQpG) |  | 0 | In this work, we theoretically investigate the generalization properties of neural networks (NN) trained by stochastic gradient descent (SGD) with large learning rates. Under such a training regime, our finding is that, the oscillation of the NN weights caused by SGD with large learning rates turns... | Beining Wu, Difan Zou, Miao Lu, Xiaodong Yang |  |
| 1274 |  |  [RobustTSF: Towards Theory and Design of Robust Time Series Forecasting with Anomalies](https://openreview.net/forum?id=ltZ9ianMth) |  | 0 | Time series forecasting is an important and forefront task whose techniques have been applied to electricity forecasting, trajectory prediction, labor planning, etc. However, most of time series forecasting techniques assume that the training data is clean without anomalies. This assumption is... | Hao Cheng, Liang Sun, Qingsong Wen, Yang Liu |  |
| 1275 |  |  [Understanding the Effects of RLHF on LLM Generalisation and Diversity](https://openreview.net/forum?id=PXD3FAVHJT) |  | 0 | Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT or Anthropic's Claude. While there has been significant work developing these methods, our understanding of... | Christoforos Nalmpantis, Edward Grefenstette, Eric Hambro, Ishita Mediratta, Jelena Luketina, Robert Kirk, Roberta Raileanu |  |
| 1276 |  |  [GAIA: Zero-shot Talking Avatar Generation](https://openreview.net/forum?id=ATEawsFUj4) |  | 0 | Zero-shot talking avatar generation aims at synthesizing natural talking videos from speech and a single portrait image. Previous methods have relied on domain-specific heuristics such as warping-based motion representation and 3D Morphable Models, which limit the naturalness and diversity of the... | Chunyu Wang, Han Hu, HsiangTao Wu, Jialiang Zhu, Jiang Bian, Junliang Guo, Kaikai An, Leyi Li, Runyi Yu, Sheng Zhao, Tianyu He, Xu Tan, Yuchi Wang |  |
| 1277 |  |  [Probabilistic Self-supervised Representation Learning via Scoring Rules Minimization](https://openreview.net/forum?id=skcTCdJz0f) |  | 0 | % Self-supervised learning methods have shown promising results across a wide range of tasks in computer vision, natural language processing, and multimodal analysis. However, self-supervised approaches come with a notable limitation, dimensional collapse, where a model doesn't fully utilize its... | Amirhossein Vahidi, Bernd Bischl, Eyke Hüllermeier, Lisa Wimmer, Mina Rezaei, Simon Schoßer, Yawei Li |  |
| 1278 |  |  [DORSal: Diffusion for Object-centric Representations of Scenes et al](https://openreview.net/forum?id=3zvB14IF6D) |  | 0 | Recent progress in 3D scene understanding enables scalable learning of representations across large datasets of diverse scenes. As a consequence, generalization to unseen scenes and objects, rendering novel views from just a single or a handful of input images, and controllable scene generation... | Allan Jabri, Emiel Hoogeboom, Mehdi S. M. Sajjadi, Sjoerd van Steenkiste, Thomas Kipf |  |
| 1279 |  |  [T-Rep: Representation Learning for Time Series using Time-Embeddings](https://openreview.net/forum?id=3y2TfP966N) |  | 0 | Multivariate time series present challenges to standard machine learning techniques, as they are often unlabeled, high dimensional, noisy, and contain missing data. To address this, we propose T-Rep, a self-supervised method to learn time series representations at a timestep granularity. T-Rep... | Adrien Bennetot, Archibald Fraikin, Stéphanie Allassonnière |  |
| 1280 |  |  [Beyond Stationarity: Convergence Analysis of Stochastic Softmax Policy Gradient Methods](https://openreview.net/forum?id=1VeQ6VBbev) |  | 0 | Markov Decision Processes (MDPs) are a formal framework for modeling and solving sequential decision-making problems. In finite time horizons such problems are relevant for instance for optimal stopping or specific supply chain problems, but also in the training of large language models. In... | Leif Döring, Sara Klein, Simon Weissmann |  |
| 1281 |  |  [SliceGPT: Compress Large Language Models by Deleting Rows and Columns](https://openreview.net/forum?id=vXxardq6db) |  | 0 | Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be... | James Hensman, Marcelo Gennari Do Nascimento, Maximilian L. Croci, Saleh Ashkboos, Torsten Hoefler |  |
| 1282 |  |  [Experimental Design for Multi-Channel Imaging via Task-Driven Feature Selection](https://openreview.net/forum?id=MloaGA6WwX) |  | 0 | This paper presents a data-driven, task-specific paradigm for experimental design, to shorten acquisition time, reduce costs, and accelerate the deployment of imaging devices. Current approaches in experimental design focus on model-parameter estimation and require specification of a particular... | Daniel C. Alexander, Paddy J. Slator, Stefano B. Blumberg |  |
| 1283 |  |  [Convolutional Deep Kernel Machines](https://openreview.net/forum?id=1oqedRt6Z7) |  | 0 | Standard infinite-width limits of neural networks sacrifice the ability for intermediate layers to learn representations from data. Recent work (“A theory of representation learning gives a deep generalisation of kernel methods”, Yang et al. 2023) modified the Neural Network Gaussian Process (NNGP)... | Ben Anson, Edward Milsom, Laurence Aitchison |  |
| 1284 |  |  [Weakly Supervised Virus Capsid Detection with Image-Level Annotations in Electron Microscopy Images](https://openreview.net/forum?id=RJDjSXNuAZ) |  | 0 | Current state-of-the-art methods for object detection rely on annotated bounding boxes of large data sets for training. However, obtaining such annotations is expensive and can require up to hundreds of hours of manual labor. This poses a challenge, especially since such annotations can only be... | Clarissa Read, Hannah Kniesel, Kavitha Shaga Devan, Leon Sick, Paul Walther, Pedro Hermosilla, Tim Bergner, Timo Ropinski, Tristan Payer |  |
| 1285 |  |  [Chain-of-Experts: When LLMs Meet Complex Operations Research Problems](https://openreview.net/forum?id=HobyL1B9CZ) |  | 0 | Large language models (LLMs) have emerged as powerful techniques for various NLP tasks, such as mathematical reasoning and plan generation. In this paper, we study automatic modeling and programming for complex operation research (OR) problems, so as to alleviate the heavy dependence on domain... | Dongxiang Zhang, Gang Chen, Jia Zeng, Lilin Xu, Mingli Song, Tao Zhong, Xiaojin Fu, Xiongwei Han, Yangjun Wu, Yuan Jessica Wang, Ziyang Xiao |  |
| 1286 |  |  [On the Reliability of Watermarks for Large Language Models](https://openreview.net/forum?id=DEJIDCmWOz) |  | 0 | As LLMs become commonplace, machine-generated text has the potential to flood the internet with spam, social media bots, and valueless content. _Watermarking_ is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet a... | Aniruddha Saha, John Kirchenbauer, Jonas Geiping, Kasun Fernando, Kezhi Kong, Khalid Saifullah, Manli Shu, Micah Goldblum, Tom Goldstein, Yuxin Wen |  |
| 1287 |  |  [Near-Optimal Solutions of Constrained Learning Problems](https://openreview.net/forum?id=fDaLmkdSKU) |  | 0 | With the widespread adoption of machine learning systems, the need to curtail their behavior has become increasingly apparent. This is evidenced by recent advancements towards developing models that satisfy robustness, safety, and fairness requirements. These requirements can be imposed (with... | Alejandro Ribeiro, Juan Elenter, Luiz F. O. Chamon |  |
| 1288 |  |  [Delphic Offline Reinforcement Learning under Nonidentifiable Hidden Confounding](https://openreview.net/forum?id=lUYY2qsRTI) |  | 0 | A prominent challenge of offline reinforcement learning (RL) is the issue of hidden confounding: unobserved variables may influence both the actions taken by the agent and the observed outcomes. Hidden confounding can compromise the validity of any causal conclusion drawn from data and presents a... | Alizée Pace, Bernhard Schölkopf, Gunnar Rätsch, Guy Tennenholtz, Hugo Yèche |  |
| 1289 |  |  [Leave-one-out Distinguishability in Machine Learning](https://openreview.net/forum?id=9RNfX0ah0K) |  | 0 | We introduce an analytical framework to quantify the changes in a machine learning algorithm's output distribution following the inclusion of a few data points in its training set, a notion we define as leave-one-out distinguishability (LOOD). This is key to measuring data \*\*memorization\*\* and... | Anastasia Borovykh, Jiayuan Ye, Reza Shokri, Soufiane Hayou |  |
| 1290 |  |  [Increasing Model Capacity for Free: A Simple Strategy for Parameter Efficient Fine-tuning](https://openreview.net/forum?id=H3IUunLy8s) |  | 0 | Fine-tuning large pre-trained foundation models, such as the 175B GPT-3, has become the prevailing approach for downstream tasks. While parameter-efficient fine-tuning methods have been proposed and proven effective without retraining all model parameters, their performance is limited by the... | Hao Zhao, Haobo Song, Soumajit Majumder, Tao Lin |  |
| 1291 |  |  [Brain decoding: toward real-time reconstruction of visual perception](https://openreview.net/forum?id=3y1K6buO8c) |  | 0 | In the past five years, the use of generative and foundational AI systems has greatly improved the decoding of brain activity. Visual perception, in particular, can now be decoded from functional Magnetic Resonance Imaging (fMRI) with remarkable fidelity. This neuroimaging technique, however,... | Hubert J. Banville, JeanRemi King, Yohann Benchetrit |  |
| 1292 |  |  [Linear Log-Normal Attention with Unbiased Concentration](https://openreview.net/forum?id=5nM2AHzqUj) |  | 0 | Transformer models have achieved remarkable results in a wide range of applications. However, their scalability is hampered by the quadratic time and memory complexity of the self-attention mechanism concerning the sequence length. This limitation poses a substantial obstacle when dealing with long... | Emir Haleva, Joseph Kampeas, Yury Nahshan |  |
| 1293 |  |  [Prioritized Soft Q-Decomposition for Lexicographic Reinforcement Learning](https://openreview.net/forum?id=c0MyyXyGfn) |  | 0 | Reinforcement learning (RL) for complex tasks remains a challenge, primarily due to the difficulties of engineering scalar reward functions and the inherent inefficiency of training models from scratch. Instead, it would be better to specify complex tasks in terms of elementary subtasks and to... | Erik Schaffernicht, Finn Rietz, Johannes A. Stork, Stefan Heinrich |  |
| 1294 |  |  [Energy-guided Entropic Neural Optimal Transport](https://openreview.net/forum?id=d6tUsZeVs7) |  | 0 | Energy-based models (EBMs) are known in the Machine Learning community for decades. Since the seminal works devoted to EBMs dating back to the noughties, there have been a lot of efficient methods which solve the generative modelling problem by means of energy potentials (unnormalized likelihood... | Alexander Kolesov, Alexander Korotin, Evgeny Burnaev, Nikita Gushchin, Petr Mokrov |  |
| 1295 |  |  [Learning Semantic Proxies from Visual Prompts for Parameter-Efficient Fine-Tuning in Deep Metric Learning](https://openreview.net/forum?id=TWVMVPx2wO) |  | 0 | Deep Metric Learning (DML) has long attracted the attention of the machine learning community as a key objective. Existing solutions concentrate on fine-tuning the pre-trained models on conventional image datasets. As a result of the success of recent pre-trained models derived from larger-scale... | Chen Chen, Kien A. Hua, Li Ren, Liqiang Wang |  |
| 1296 |  |  [Fine-Tuned Language Models Generate Stable Inorganic Materials as Text](https://openreview.net/forum?id=vN9fpfqoP1) |  | 0 | We propose fine-tuning large language models for generation of stable materials. While unorthodox, fine-tuning large language models on text-encoded atomistic data is simple to implement yet reliable, with around 90\% of sampled structures obeying physical constraints on atom positions and charges.... | Andrea Madotto, Andrew Gordon Wilson, Anuroop Sriram, C. Lawrence Zitnick, Nate Gruver, Zachary W. Ulissi |  |
| 1297 |  |  [TextField3D: Towards Enhancing Open-Vocabulary 3D Generation with Noisy Text Fields](https://openreview.net/forum?id=WOiOzHG2zD) |  | 0 | Recent works learn 3D representation explicitly under text-3D guidance. However, limited text-3D data restricts the vocabulary scale and text control of generations. Generators may easily fall into a stereotype concept for certain text prompts, thus losing open-vocabulary generation ability. To... | Bowen Dong, Hang Xu, Rynson W. H. Lau, Songcen Xu, Tianyu Huang, Wangmeng Zuo, Yihan Zeng |  |
| 1298 |  |  [GlucoBench: Curated List of Continuous Glucose Monitoring Datasets with Prediction Benchmarks](https://openreview.net/forum?id=cUSNs8nGaV) |  | 0 | The rising rates of diabetes necessitate innovative methods for its management. Continuous glucose monitors (CGM) are small medical devices that measure blood glucose levels at regular intervals providing insights into daily patterns of glucose variation. Forecasting of glucose trajectories based... | Elizabeth Chun, Irina Gaynanova, Nathaniel J. Fernandes, Nicholas Kasman, Renat Sergazinov, Valeriya Rogovchenko |  |
| 1299 |  |  [Quantifying and Enhancing Multi-modal Robustness with Modality Preference](https://openreview.net/forum?id=XyrB1Ay44j) |  | 0 | Multi-modal models have shown a promising capability to effectively integrate information from various sources, yet meanwhile, they are found vulnerable to pervasive perturbations, such as uni-modal attacks and missing conditions. To counter these perturbations, robust multi-modal representations... | Ce Liang, Di Hu, Yake Wei, Zequn Yang |  |
| 1300 |  |  [Advancing the Lower Bounds: an Accelerated, Stochastic, Second-order Method with Optimal Adaptation to Inexactness](https://openreview.net/forum?id=otU31x3fus) |  | 0 | We present a new accelerated stochastic second-order method that is robust to both gradient and Hessian inexactness, typical in machine learning. We establish theoretical lower bounds and prove that our algorithm achieves optimal convergence in both gradient and Hessian inexactness in this key... | Alexander V. Gasnikov, Ali Kavis, Artem Agafonov, Dmitry Kamzolov, Kimon Antonakopoulos, Martin Takác, Volkan Cevher |  |
| 1301 |  |  [Distinguished In Uniform: Self-Attention Vs. Virtual Nodes](https://openreview.net/forum?id=AcSChDWL6V) |  | 0 | Graph Transformers (GTs) such as SAN and GPS are graph processing models that combine Message-Passing GNNs (MPGNNs) with global Self-Attention. They were shown to be universal function approximators, with two reservations: 1. The initial node features must be augmented with certain positional... | Berke Kisin, Eran Rosenbluth, Jan Tönshoff, Martin Grohe, Martin Ritzert |  |
| 1302 |  |  [Faster Approximation of Probabilistic and Distributional Values via Least Squares](https://openreview.net/forum?id=lvSMIsztka) |  | 0 | The family of probabilistic values, axiomatically-grounded in cooperative game theory, has recently received much attention in data valuation. However, it is often computationally expensive to compute exactly (exponential w.r.t. the number of data to valuate denoted by $n$). The existing generic... | Weida Li, Yaoliang Yu |  |
| 1303 |  |  [Rigid Protein-Protein Docking via Equivariant Elliptic-Paraboloid Interface Prediction](https://openreview.net/forum?id=zgQ0PHeGnL) |  | 0 | The study of rigid protein-protein docking plays an essential role in a variety of tasks such as drug design and protein engineering. Recently, several learning-based methods have been proposed for the task, exhibiting much faster docking speed than those computational methods. In this paper, we... | Wenbing Huang, Yang Liu, Ziyang Yu |  |
| 1304 |  |  [Incentive-Aware Federated Learning with Training-Time Model Rewards](https://openreview.net/forum?id=FlY7WQ2hWS) |  | 0 | In federated learning (FL), incentivizing contributions of training resources (e.g., data, compute) from potentially competitive clients is crucial. Existing incentive mechanisms often distribute post-training monetary rewards, which suffer from practical challenges of timeliness and feasibility of... | Bryan Kian Hsiang Low, Mohammad Mohammadi Amiri, Ramesh Raskar, Zhaoxuan Wu |  |
| 1305 |  |  [Removing Biases from Molecular Representations via Information Maximization](https://openreview.net/forum?id=7TOs9gjAg1) |  | 0 | High-throughput drug screening -- using cell imaging or gene expression measurements as readouts of drug effect -- is a critical tool in biotechnology to assess and understand the relationship between the chemical structure and biological activity of a drug. Since large-scale screens have to be... | Caroline Uhler, Chenyu Wang, Sharut Gupta, Tommi S. Jaakkola |  |
| 1306 |  |  [Neural Architecture Retrieval](https://openreview.net/forum?id=1JtTPYBKqt) |  | 0 | With the increasing number of new neural architecture designs and substantial existing neural architectures, it becomes difficult for the researchers to situate their contributions compared with existing neural architectures or establish the connections between their designs and other relevant... | Chang Xu, Minjing Dong, Xiaohuan Pei, Yanxi Li |  |
| 1307 |  |  [Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization](https://openreview.net/forum?id=QibPzdVrRu) |  | 0 | This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with... | Enrique Mallada, Hancheng Min, René Vidal |  |
| 1308 |  |  [Rethinking the Uniformity Metric in Self-Supervised Learning](https://openreview.net/forum?id=3pf2hEdu8B) |  | 0 | Uniformity plays an important role in evaluating learned representations, providing insights into self-supervised learning. In our quest for effective uniformity metrics, we pinpoint four principled properties that such metrics should possess. Namely, an effective uniformity metric should remain... | Benyou Wang, Jian Li, Qiang Sun, Xianghong Fang |  |
| 1309 |  |  [Learning Decentralized Partially Observable Mean Field Control for Artificial Collective Behavior](https://openreview.net/forum?id=99tKiMVJhY) |  | 0 | Recent reinforcement learning (RL) methods have achieved success in various domains. However, multi-agent RL (MARL) remains a challenge in terms of decentralization, partial observability and scalability to many agents. Meanwhile, collective behavior requires resolution of the aforementioned... | Christian Fabian, Heinz Koeppl, Kai Cui, Sascha Hauck |  |
| 1310 |  |  [TAB: Temporal Accumulated Batch Normalization in Spiking Neural Networks](https://openreview.net/forum?id=k1wlmtPGLq) |  | 0 | Spiking Neural Networks (SNNs) are attracting growing interest for their energy-efficient computing when implemented on neuromorphic hardware. However, directly training SNNs, even adopting batch normalization (BN), is highly challenging due to their non-differentiable activation function and the... | Bin Gu, Giulia De Masi, Haiyan Jiang, Huan Xiong, Vincent Zoonekynd |  |
| 1311 |  |  [StructComp: Substituting propagation with Structural Compression in Training Graph Contrastive Learning](https://openreview.net/forum?id=a4DBEeGfQq) |  | 0 | Graph contrastive learning (GCL) has become a powerful tool for learning graph data, but its scalability remains a significant challenge. In this work, we propose a simple yet effective training framework called Structural Compression (StructComp) to address this issue. Inspired by a sparse... | Hongwei Zhang, Shengzhong Zhang, Wenjie Yang, Xinyuan Cao, Zengfeng Huang |  |
| 1312 |  |  [Particle Guidance: non-I.I.D. Diverse Sampling with Diffusion Models](https://openreview.net/forum?id=KqbCvIFBY7) |  | 0 | In light of the widespread success of generative models, a significant amount of research has gone into speeding up their sampling time. However, generative models are often sampled multiple times to obtain a diverse set incurring a cost that is orthogonal to sampling time. We tackle the question... | Gabriele Corso, Regina Barzilay, Tommi S. Jaakkola, Valentin De Bortoli, Yilun Xu |  |
| 1313 |  |  [On Adversarial Training without Perturbing all Examples](https://openreview.net/forum?id=pE6gWrASQm) |  | 0 | Adversarial training is the de-facto standard for improving robustness against adversarial examples. This usually involves a multi-step adversarial attack applied on each example during training. In this paper, we explore only constructing adversarial examples (AE) on a subset of the training... | Bernt Schiele, David Stutz, Mario Fritz, Max Maria Losch, Mohamed Omran |  |
| 1314 |  |  [Diving Segmentation Model into Pixels](https://openreview.net/forum?id=KBo7Z5aTV0) |  | 0 | More distinguishable and consistent pixel features for each category will benefit the semantic segmentation under various settings. Existing efforts to mine better pixel-level features attempt to explicitly model the categorical distribution, which fails to achieve optimal due to the significant... | Chen Gan, Junfeng Zhang, Kelei He, Yang Gao, Zihao Yin |  |
| 1315 |  |  [General Stability Analysis for Zeroth-Order Optimization Algorithms](https://openreview.net/forum?id=AfhNyr73Ma) |  | 0 | Zeroth-order optimization algorithms are widely used for black-box optimization problems, such as those in machine learning and prompt engineering, where the gradients are approximated using function evaluations. Recently, a generalization result was provided for zeroth-order stochastic gradient... | Bin Gu, Hong Chen, Hualin Zhang, Xinyue Liu |  |
| 1316 |  |  [Hybrid Sharing for Multi-Label Image Classification](https://openreview.net/forum?id=yVJd8lKyVX) |  | 0 | Existing multi-label classification methods have long suffered from label heterogeneity, where learning a label obscures another. By modeling multi-label classification as a multi-task problem, this issue can be regarded as a negative transfer, which indicates challenges to achieve simultaneously... | Chen Gan, Junfeng Zhang, Kelei He, Yang Gao, Zihao Yin |  |
| 1317 |  |  [Symmetric Single Index Learning](https://openreview.net/forum?id=e1vqloonRy) |  | 0 | Few neural architectures lend themselves to provable learning with gradient based methods. One popular model is the single-index model, in which labels are produced by composing an unknown linear projection with a possibly unknown scalar link function. Learning this model with SGD is relatively... | Aaron Zweig, Joan Bruna |  |
| 1318 |  |  [An improved analysis of per-sample and per-update clipping in federated learning](https://openreview.net/forum?id=BdPvGRvoBC) |  | 0 | Gradient clipping is key mechanism that is essential to differentially private training techniques in Federated learning. Two popular strategies are per-sample clipping, which clips the mini-batch gradient, and per-update clipping, which clips each user's model update. However, there has not been a... | Bo Li, Mikkel N. Schmidt, Sebastian U. Stich, Tommy Sonne Alstrøm, Xiaowen Jiang |  |
| 1319 |  |  [Sample-Efficiency in Multi-Batch Reinforcement Learning: The Need for Dimension-Dependent Adaptivity](https://openreview.net/forum?id=ey3GhWXQ97) |  | 0 | We theoretically explore the relationship between sample-efficiency and adaptivity in reinforcement learning. An algorithm is sample-efficient if it uses a number of queries $n$ to the environment that is polynomial in the dimension $d$ of the problem. Adaptivity refers to the frequency at which... | Ciara PikeBurke, Emmeran Johnson, Patrick Rebeschini |  |
| 1320 |  |  [On the Power of the Weisfeiler-Leman Test for Graph Motif Parameters](https://openreview.net/forum?id=FddFxi08J3) |  | 0 | Seminal research in the field of graph neural networks (GNNs) has revealed a direct correspondence between the expressive capabilities of GNNs and the $k$-dimensional Weisfeiler-Leman ($k$WL) test, a widely-recognized method for verifying graph isomorphism. This connection has reignited interest in... | Matthias Lanzinger, Pablo Barceló |  |
| 1321 |  |  [PAC Prediction Sets Under Label Shift](https://openreview.net/forum?id=4vPVBh3fhz) |  | 0 | Prediction sets capture uncertainty by predicting sets of labels rather than individual labels, enabling downstream decisions to conservatively account for all plausible outcomes. Conformal inference algorithms construct prediction sets guaranteed to contain the true label with high probability.... | Edgar Dobriban, Insup Lee, Osbert Bastani, Sangdon Park, Wenwen Si |  |
| 1322 |  |  [Memorization in Self-Supervised Learning Improves Downstream Generalization](https://openreview.net/forum?id=KSjPaXtxP8) |  | 0 | Self-supervised learning (SSL) has recently received significant attention due to its ability to train high-performance encoders purely on unlabeled data---often scraped from the internet. This data can still be sensitive and empirical evidence suggests that SSL encoders memorize private... | Adam Dziedzic, Franziska Boenisch, Michael Backes, Muhammad Ahmad Kaleem, Nicolas Papernot, Wenhao Wang |  |
| 1323 |  |  [The Curse of Diversity in Ensemble-Based Exploration](https://openreview.net/forum?id=M3QXCOTTk4) |  | 0 | We uncover a surprising phenomenon in deep reinforcement learning: training a diverse ensemble of data-sharing agents -- a well-established exploration strategy -- can significantly impair the performance of the individual ensemble members when compared to standard single-agent training. Through... | Aaron C. Courville, Evgenii Nikishin, Pierluca D'Oro, Zhixuan Lin |  |
| 1324 |  |  [Multilinear Operator Networks](https://openreview.net/forum?id=bbCL5aRjUx) |  | 0 | Despite the remarkable capabilities of deep neural networks in image recognition, the dependence on activation functions remains a largely unexplored area and has yet to be eliminated. On the other hand, Polynomial Networks is a class of models that does not require activation functions, but have... | Grigorios Chrysos, Markos Georgopoulos, Volkan Cevher, Yixin Cheng |  |
| 1325 |  |  [Leveraging Generative Models for Unsupervised Alignment of Neural Time Series Data](https://openreview.net/forum?id=9zhHVyLY4K) |  | 0 | Large scale inference models are widely used in neuroscience to extract latent representations from high-dimensional neural recordings. Due to the statistical heterogeneities between sessions and animals, a new model is trained from scratch to infer the underlying dynamics for each new dataset.... | Ayesha Vermani, Il Memming Park, Josue Nassar |  |
| 1326 |  |  [UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition](https://openreview.net/forum?id=r65xfUb76p) |  | 0 | Large language models (LLMs) have demonstrated remarkable generalizability, such as understanding arbitrary entities and relations. Instruction tuning has proven effective for distilling LLMs into more cost-efficient models such as Alpaca and Vicuna. Yet such student models still trail the original... | Hoifung Poon, Muhao Chen, Sheng Zhang, Wenxuan Zhou, Yu Gu |  |
| 1327 |  |  [Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data](https://openreview.net/forum?id=W8S8SxS9Ng) |  | 0 | State-of-the-art systems neuroscience experiments yield large-scale multimodal data, and these data sets require new tools for analysis. Inspired by the success of large pretrained models in vision and language domains, we reframe the analysis of large-scale, cellular-resolution neuronal spiking... | Antonis Antoniades, Joseph Canzano, Spencer L. Smith, William Yang Wang, Yiyi Yu |  |
| 1328 |  |  [Off-Policy Primal-Dual Safe Reinforcement Learning](https://openreview.net/forum?id=vy42bYs1Wo) |  | 0 | Primal-dual safe RL methods commonly perform iterations between the primal update of the policy and the dual update of the Lagrange Multiplier. Such a training paradigm is highly susceptible to the error in cumulative cost estimation since this estimation serves as the key bond connecting the... | Bo Tang, Chao Yu, Dong Wang, Qian Lin, Qianlong Xie, Shangqin Mao, Xingxing Wang, Zifan Wu |  |
| 1329 |  |  [An Extensible Framework for Open Heterogeneous Collaborative Perception](https://openreview.net/forum?id=KkrDUGIASk) |  | 0 | Collaborative perception aims to mitigate the limitations of single-agent perception, such as occlusions, by facilitating data exchange among multiple agents. However, most current works consider a homogeneous scenario where all agents use identity sensors and perception models. In reality,... | Dequan Wang, Siheng Chen, Yanfeng Wang, Yifan Lu, Yiqi Zhong, Yue Hu |  |
| 1330 |  |  [Neural structure learning with stochastic differential equations](https://openreview.net/forum?id=V1GM9xDvIY) |  | 0 | Discovering the underlying relationships among variables from temporal observations has been a longstanding challenge in numerous scientific disciplines, including biology, finance, and climate science. The dynamics of such systems are often best described using continuous-time stochastic... | Benjie Wang, Joel Jennings, Wenbo Gong |  |
| 1331 |  |  [STARC: A General Framework For Quantifying Differences Between Reward Functions](https://openreview.net/forum?id=wPhbtwlCDa) |  | 0 | In order to solve a task using reinforcement learning, it is necessary to first formalise the goal of that task as a \*reward function\*. However, for many real-world tasks, it is very difficult to manually specify a reward function that never incentivises undesirable behaviour. As a result, it is... | Adam Gleave, Alessandro Abate, Erik Jenner, Joar Max Viktor Skalse, Lucy Farnik, Sumeet Ramesh Motwani |  |
| 1332 |  |  [GAIA: a benchmark for General AI Assistants](https://openreview.net/forum?id=fibxvahvs3) |  | 0 | We introduce GAIA, a benchmark for General AI Assistants that, if solved, would represent a milestone in AI research. GAIA proposes real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency. GAIA... | Clémentine Fourrier, Grégoire Mialon, Thomas Scialom, Thomas Wolf, Yann LeCun |  |
| 1333 |  |  [A differentiable brain simulator bridging brain simulation and brain-inspired computing](https://openreview.net/forum?id=AU2gS9ut61) |  | 0 | Brain simulation builds dynamical models to mimic the structure and functions of the brain, while brain-inspired computing (BIC) develops intelligent systems by learning from the structure and functions of the brain. The two fields are intertwined and should share a common programming framework to... | Chaoming Wang, Hongyaoxing Gu, Shangyang Li, Si Wu, Sichao He, Tianqiu Zhang |  |
| 1334 |  |  [FOSI: Hybrid First and Second Order Optimization](https://openreview.net/forum?id=NvbeD9Ttkx) |  | 0 | Popular machine learning approaches forgo second-order information due to the difficulty of computing curvature in high dimensions. We present FOSI, a novel meta-algorithm that improves the performance of any base first-order optimizer by efficiently incorporating second-order information during... | Assaf Schuster, Hadar Sivan, Moshe Gabel |  |
| 1335 |  |  [Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach](https://openreview.net/forum?id=zwU9scoU4A) |  | 0 | Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack... | Christian Fabian, Heinz Koeppl, Kai Cui |  |
| 1336 |  |  [Unraveling the Key Components of OOD Generalization via Diversification](https://openreview.net/forum?id=Lvf7GnaLru) |  | 0 | Supervised learning datasets may contain multiple cues that explain the training set equally well, i.e., learning any of them would lead to the correct predictions on the training data. However, many of them can be spurious, i.e., lose their predictive power under a distribution shift and... | Amir Zamir, Andrei Atanov, Harold Benoit, Liangze Jiang, Mattia Rigotti, Oguzhan Fatih Kar |  |
| 1337 |  |  [Deep Generative Clustering with Multimodal Diffusion Variational Autoencoders](https://openreview.net/forum?id=k5THrhXDV3) |  | 0 | Multimodal VAEs have recently gained significant attention as generative models for weakly-supervised learning with multiple heterogeneous modalities. In parallel, VAE-based methods have been explored as probabilistic approaches for clustering tasks. At the intersection of these two research... | Daphné Chopard, Emanuele Palumbo, Julia E. Vogt, Laura Manduchi, Sonia Laguna |  |
| 1338 |  |  [Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning](https://openreview.net/forum?id=AY6aM13gGF) |  | 0 | Offline reinforcement learning (RL) aims to find a near-optimal policy using pre-collected datasets. Given recent advances in Large Language Models (LLMs) and their few-shot learning prowess, this paper introduces $\textbf{La}$nguage Models for $\textbf{Mo}$tion Control ($\textbf{LaMo}$), a general... | Huazhe Xu, Ruizhe Shi, Simon Shaolei Du, Yanjie Ze, Yuyao Liu |  |
| 1339 |  |  [Equivariant Scalar Fields for Molecular Docking with Fast Fourier Transforms](https://openreview.net/forum?id=BIveOmD1Nh) |  | 0 | Molecular docking is critical to structure-based virtual screening, yet the throughput of such workflows is limited by the expensive optimization of scoring functions involved in most docking algorithms. We explore how machine learning can accelerate this process by learning a scoring function with... | Bonnie Berger, Bowen Jing, Tommi S. Jaakkola |  |
| 1340 |  |  [The Alignment Problem from a Deep Learning Perspective](https://openreview.net/forum?id=fh8EYKFKns) |  | 0 | AI systems based on deep learning have reached or surpassed human performance in a range of narrow domains. In coming years or decades, artificial general intelligence (AGI) may surpass human capabilities at many critical tasks. In this position paper, we examine the technical difficulty of... | Lawrence Chan, Richard Ngo, Sören Mindermann |  |
| 1341 |  |  [Discovering Temporally-Aware Reinforcement Learning Algorithms](https://openreview.net/forum?id=MJJcs3zbmi) |  | 0 | Recent advancements in meta-learning have enabled the automatic discovery of novel reinforcement learning algorithms parameterized by surrogate objective functions. To improve upon manually designed algorithms, the parameterization of this learned objective function must be expressive enough to... | Chris Lu, Jakob Nicolaus Foerster, Louis Kirsch, Matthew Thomas Jackson, Robert Tjarko Lange, Shimon Whiteson |  |
| 1342 |  |  [Guiding Masked Representation Learning to Capture Spatio-Temporal Relationship of Electrocardiogram](https://openreview.net/forum?id=WcOohbsF4H) |  | 0 | Electrocardiograms (ECG) are widely employed as a diagnostic tool for monitoring electrical signals originating from a heart. Recent machine learning research efforts have focused on the application of screening various diseases using ECG signals. However, adapting to the application of screening... | Minje Park, Sunghoon Joo, Yeongyeon Na, Yunwon Tae |  |
| 1343 |  |  [How Realistic Is Your Synthetic Data? Constraining Deep Generative Models for Tabular Data](https://openreview.net/forum?id=tBROYsEz9G) |  | 0 | Deep Generative Models (DGMs) have been shown to be powerful tools for generating tabular data, as they have been increasingly able to capture the complex distributions that characterize them. However, to generate realistic synthetic data, it is often not enough to have a good approximation of... | Eleonora Giunchiglia, Maxime Cordy, Mihaela C. Stoian, Salijona Dyrmishi, Thomas Lukasiewicz |  |
| 1344 |  |  [Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks](https://openreview.net/forum?id=s8cMuxI5gu) |  | 0 | Gradient inversion attacks aim to reconstruct local training data from intermediate gradients exposed in the federated learning framework. Despite successful attacks, all previous methods, starting from reconstructing a single data point and then relaxing the single-image limit to batch level, are... | Jian Liang, Ran He, Yanbo Wang |  |
| 1345 |  |  [From Zero to Turbulence: Generative Modeling for 3D Flow Simulation](https://openreview.net/forum?id=ZhlwoC1XaN) |  | 0 | Simulations of turbulent flows in 3D are one of the most expensive simulations in computational fluid dynamics (CFD). Many works have been written on surrogate models to replace numerical solvers for fluid flows with faster, learned, autoregressive models. However, the intricacies of turbulence in... | David Lüdke, Jan HansenPalmus, Marten Lienen, Stephan Günnemann |  |
| 1346 |  |  [INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection](https://openreview.net/forum?id=Zj12nzlQbz) |  | 0 | Knowledge hallucination have raised widespread concerns for the security and reliability of deployed LLMs. Previous efforts in detecting hallucinations have been employed at logit-level uncertainty estimation or language-level self-consistency evaluation, where the semantic information is... | Chao Chen, Jieping Ye, Kai Liu, Mingyuan Tao, Yi Gu, Yue Wu, Ze Chen, Zhihang Fu |  |
| 1347 |  |  [DisenBooth: Identity-Preserving Disentangled Tuning for Subject-Driven Text-to-Image Generation](https://openreview.net/forum?id=FlhjUkC7vH) |  | 0 | Subject-driven text-to-image generation aims to generate customized images of the given subject based on the text descriptions, which has drawn increasing attention. Existing methods mainly resort to finetuning a pretrained generative model, where the identity-relevant information (e.g., the boy)... | Hong Chen, Simin Wu, Wenwu Zhu, Xin Wang, Xuguang Duan, Yipeng Zhang, Yuwei Zhou |  |
| 1348 |  |  [Improving equilibrium propagation without weight symmetry through Jacobian homeostasis](https://openreview.net/forum?id=kUveo5k1GF) |  | 0 | Equilibrium propagation (EP) is a compelling alternative to the back propagation of error algorithm (BP) for computing gradients of neural networks on biological or analog neuromorphic substrates. Still, the algorithm requires weight symmetry and infinitesimal equilibrium perturbations, i.e.,... | Axel Laborieux, Friedemann Zenke |  |
| 1349 |  |  [Revisiting Data Augmentation in Deep Reinforcement Learning](https://openreview.net/forum?id=EGQBpkIEuu) |  | 0 | Various data augmentation techniques have been recently proposed in image-based deep reinforcement learning (DRL). Although they empirically demonstrate the effectiveness of data augmentation for improving sample efficiency or generalization, which technique should be preferred is not always clear.... | Jianshu Hu, Paul Weng, Yunpeng Jiang |  |
| 1350 |  |  [Structural Inference with Dynamics Encoding and Partial Correlation Coefficients](https://openreview.net/forum?id=TKnzPdyeJu) |  | 0 | This paper introduces a novel approach to structural inference, combining a variational dynamics encoder with partial correlation coefficients. In contrast to prior methods, our approach leverages variational inference to encode node dynamics within latent variables, and structural reconstruction... | Aoran Wang, Jun Pang |  |
| 1351 |  |  [Simplicial Representation Learning with Neural k-Forms](https://openreview.net/forum?id=Djw0XhjHZb) |  | 0 | Geometric deep learning extends deep learning to incorporate information about the geometry and topology data, especially in complex domains like graphs. Despite the popularity of message passing in this field, it has limitations such as the need for graph rewiring, ambiguity in interpreting data,... | Bastian Rieck, Celia Hacker, Kelly Maggs |  |
| 1352 |  |  [Improved Analysis of Sparse Linear Regression in Local Differential Privacy Model](https://openreview.net/forum?id=cVUOnF7iVp) |  | 0 | In this paper, we revisit the problem of sparse linear regression in the local differential privacy (LDP) model. Existing research in the non-interactive and sequentially local models has focused on obtaining the lower bounds for the case where the underlying parameter is $1$-sparse, and extending... | Di Wang, Jinhui Xu, Liyang Zhu, Meng Ding, Vaneet Aggarwal |  |
| 1353 |  |  [Toward effective protection against diffusion-based mimicry through score distillation](https://openreview.net/forum?id=NzxCMe88HX) |  | 0 | While generative diffusion models excel in producing high-quality images, they can also be misused to mimic authorized images, posing a significant threat to AI systems. Efforts have been made to add calibrated perturbations to protect images from diffusion-based mimicry pipelines. However, most of... | Chumeng Liang, Haotian Xue, Xiaoyu Wu, Yongxin Chen |  |
| 1354 |  |  [Sign2GPT: Leveraging Large Language Models for Gloss-Free Sign Language Translation](https://openreview.net/forum?id=LqaEEs3UxU) |  | 0 | Automatic Sign Language Translation requires the integration of both computer vision and natural language processing to effectively bridge the communication gap between sign and spoken languages. However, the deficiency in large-scale training data to support sign language translation means we need... | Necati Cihan Camgöz, Richard Bowden, Ryan Wong |  |
| 1355 |  |  [Cauchy-Schwarz Divergence Information Bottleneck for Regression](https://openreview.net/forum?id=7wY67ZDQTE) |  | 0 | The information bottleneck (IB) approach is popular to improve the generalization, robustness and explainability of deep neural networks. Essentially, it aims to find a minimum sufficient representation $\mathbf{t}$ by striking a trade-off between a compression term $I(\mathbf{x};\mathbf{t})$ and a... | José C. Príncipe, Robert Jenssen, Shujian Yu, Sigurd Løkse, Xi Yu |  |
| 1356 |  |  [SALMONN: Towards Generic Hearing Abilities for Large Language Models](https://openreview.net/forum?id=14rn7HpKVk) |  | 0 | Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a... | Changli Tang, Chao Zhang, Guangzhi Sun, Lu Lu, Tian Tan, Wei Li, Wenyi Yu, Xianzhao Chen, Zejun Ma |  |
| 1357 |  |  [Kalman Filter for Online Classification of Non-Stationary Data](https://openreview.net/forum?id=ZzmKEpze8e) |  | 0 | In Online Continual Learning (OCL) a learning system receives a stream of data and sequentially performs prediction and training steps. Key challenges in OCL include automatic adaptation to the specific non-stationary structure of the data and maintaining appropriate predictive uncertainty. To... | Alexandre Galashov, Amal RannenTriki, Jörg Bornschein, Michalis K. Titsias, Razvan Pascanu, Yee Whye Teh |  |
| 1358 |  |  [Magnushammer: A Transformer-Based Approach to Premise Selection](https://openreview.net/forum?id=oYjPk8mqAV) |  | 0 | This paper presents a novel approach to premise selection, a crucial reasoning task in automated theorem proving. Traditionally, symbolic methods that rely on extensive domain knowledge and engineering effort are applied to this task. In contrast, this work demonstrates that contrastive training... | Albert Q. Jiang, Bartosz Piotrowski, Christian Szegedy, Jin Peng Zhou, Lukasz Kucinski, Maciej Mikula, Piotr Milos, Szymon Antoniak, Szymon Tworkowski, Yuhuai Wu |  |
| 1359 |  |  [Learning to Compose: Improving Object Centric Learning by Injecting Compositionality](https://openreview.net/forum?id=HT2dAhh4uV) |  | 0 | Learning compositional representation is a key aspect of object-centric learning as it enables flexible systematic generalization and supports complex visual reasoning. However, most of the existing approaches rely on auto-encoding objective, while the compositionality is implicitly imposed by the... | Jaehoon Yoo, Seunghoon Hong, Sungjin Ahn, Whie Jung |  |
| 1360 |  |  [Light Schrödinger Bridge](https://openreview.net/forum?id=WhZoCLRWYJ) |  | 0 | Despite the recent advances in the field of computational Schrödinger Bridges (SB), most existing SB solvers are still heavy-weighted and require complex optimization of several neural networks. It turns out that there is no principal solver which plays the role of simple-yet-effective baseline for... | Alexander Korotin, Evgeny Burnaev, Nikita Gushchin |  |
| 1361 |  |  [Reward-Free Curricula for Training Robust World Models](https://openreview.net/forum?id=eCGpNGDeNu) |  | 0 | There has been a recent surge of interest in developing generally-capable agents that can adapt to new tasks without additional training in the environment. Learning world models from reward-free exploration is a promising approach, and enables policies to be trained using imagined experience for... | Ingmar Posner, Marc Rigter, Minqi Jiang |  |
| 1362 |  |  [Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design](https://openreview.net/forum?id=7UhxsmbdaQ) |  | 0 | Generative molecular design has moved from proof-of-concept to real-world applicability, as marked by the surge in very recent papers reporting experimental validation. Key challenges in explainability and sample efficiency present opportunities to enhance generative design to directly optimize... | Jeff Guo, Philippe Schwaller |  |
| 1363 |  |  [Leveraging Uncertainty Estimates To Improve Classifier Performance](https://openreview.net/forum?id=nsNyDvNQTc) |  | 0 | Binary classification typically involves predicting the label of an instance based on whether the model score for the positive class exceeds a threshold chosen based on the application requirements (e.g., maximizing recall for a precision bound). However, model scores are often not aligned with... | Anoop Saladi, Gundeep Arora, Rajeev Rastogi, Srujana Merugu |  |
| 1364 |  |  [C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion](https://openreview.net/forum?id=jzzEHTBFOT) |  | 0 | In deep learning, test-time adaptation has gained attention as a method for model fine-tuning without the need for labeled data. A prime exemplification is the recently proposed test-time prompt tuning for large-scale vision-language models such as CLIP. Unfortunately, these prompts have been... | Chang D. Yoo, Eunseop Yoon, Hee Suk Yoon, Joshua Tian Jin Tee, Mark A. HasegawaJohnson, Yingzhen Li |  |
| 1365 |  |  [Retrieval-Enhanced Contrastive Vision-Text Models](https://openreview.net/forum?id=b2UlHeyyC0) |  | 0 | Contrastive image-text models such as CLIP form the building blocks of many state-of-the-art systems. While they excel at recognizing common generic concepts, they still struggle on fine-grained entities which are rare, or even absent from the pre-training dataset. Hence, a key ingredient to their... | Ahmet Iscen, Alireza Fathi, Cordelia Schmid, Mathilde Caron |  |
| 1366 |  |  [Deep Neural Network Initialization with Sparsity Inducing activations](https://openreview.net/forum?id=uvXK8Xk9Jk) |  | 0 | Inducing and leveraging sparse activations during training and inference is a promising avenue for improving the computational efficiency of deep networks, which is increasingly important as network sizes continue to grow and their application becomes more widespread. Here we use the large width... | Adam C. Jones, Ilan Price, Jared Tanner, Nicholas Daultry Ball, Samuel C. H. Lam |  |
| 1367 |  |  [PeFLL: Personalized Federated Learning by Learning to Learn](https://openreview.net/forum?id=MrYiwlDRQO) |  | 0 | We present PeFLL, a new personalized federated learning algorithm that improves over the state-of-the-art in three aspects: 1) it produces more accurate models, especially in the low-data regime, and not only for clients present during its training phase, but also for any that may emerge in the... | Christoph H. Lampert, Hossein Zakerinia, Jonathan Scott |  |
| 1368 |  |  [Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging](https://openreview.net/forum?id=xx0ITyHp3u) |  | 0 | Neural networks can be significantly compressed by pruning, yielding sparse models with reduced storage and computational demands while preserving predictive performance. Model soups (Wortsman et al., 2022) enhance generalization and out-of-distribution (OOD) performance by averaging the parameters... | Christoph Spiegel, Max Zimmer, Sebastian Pokutta |  |
| 1369 |  |  [DP-SGD Without Clipping: The Lipschitz Neural Network Way](https://openreview.net/forum?id=BEyEziZ4R6) |  | 0 | State-of-the-art approaches for training Differentially Private (DP) Deep Neural Networks (DNN) face difficulties to estimate tight bounds on the sensitivity of the network's layers, and instead rely on a process of per-sample gradient clipping. This clipping process not only biases the direction... | Aurélien Bellet, Corentin Friedrich, David Vigouroux, Franck Mamalet, Louis Béthune, Mathieu Serrurier, Thibaut Boissin, Thomas Massena, Yannick Prudent |  |
| 1370 |  |  [Provable and Practical: Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo](https://openreview.net/forum?id=nfIAEJFiBZ) |  | 0 | We present a scalable and effective exploration strategy based on Thompson sampling for reinforcement learning (RL). One of the key shortcomings of existing Thompson sampling algorithms is the need to perform a Gaussian approximation of the posterior distribution, which is not a good surrogate in... | A. Rupam Mahmood, Anima Anandkumar, Doina Precup, Haque Ishfaq, Kamyar Azizzadenesheli, Pan Xu, Qingfeng Lan |  |
| 1371 |  |  [Human Feedback is not Gold Standard](https://openreview.net/forum?id=7W3GLNImfS) |  | 0 | Human feedback has become the de facto standard for evaluating the performance of Large Language Models, and is increasingly being used as a training objective. However, it is not clear which properties of a generated output this single \`preference' score captures. We hypothesise that preference... | Max Bartolo, Phil Blunsom, Tom Hosking |  |
| 1372 |  |  [Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering](https://openreview.net/forum?id=L3FHMoKZcS) |  | 0 | Prompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To... | Diana Mincu, Han Zhou, Jilin Chen, Katherine A. Heller, Lev Proleev, Subhrajit Roy, Xingchen Wan |  |
| 1373 |  |  [Posterior Sampling Based on Gradient Flows of the MMD with Negative Distance Kernel](https://openreview.net/forum?id=YrXHEb2qMb) |  | 0 | We propose conditional flows of the maximum mean discrepancy (MMD) with the negative distance kernel for posterior sampling and conditional generative modelling. This MMD, which is also known as energy distance, has several advantageous properties like efficient computation via slicing and sorting.... | Fabian Altekrüger, Gabriele Steidl, Jannis Chemseddine, Johannes Hertrich, Paul Hagemann, Robert Beinert |  |
| 1374 |  |  [First-order ANIL provably learns representations despite overparametrisation](https://openreview.net/forum?id=if2vRbS8Ew) |  | 0 | Due to its empirical success in few-shot classification and reinforcement learning, meta-learning has recently received significant interest. Meta-learning methods leverage data from previous tasks to learn a new task in a sample-efficient manner. In particular, model-agnostic methods look for... | Etienne Boursier, Nicolas Flammarion, Oguz Kaan Yüksel |  |
| 1375 |  |  [Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning](https://openreview.net/forum?id=ZGNWW7xZ6Q) |  | 0 | Large language models (LLMs) have demonstrated impressive reasoning abilities in complex tasks. However, they lack up-to-date knowledge and experience hallucinations during reasoning, which can lead to incorrect reasoning processes and diminish their performance and trustworthiness. Knowledge... | Gholamreza Haffari, Linhao Luo, Shirui Pan, YuanFang Li |  |
| 1376 |  |  [How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions](https://openreview.net/forum?id=567BjxgaTp) |  | 0 | Large language models (LLMs) can “lie”, which we define as outputting false statements when incentivised to, despite “knowing” the truth in a demonstrable sense. LLMs might “lie”, for example, when instructed to output misinformation. Here, we develop a simple lie detector that requires neither... | Alex James Chan, Alexa Y. Pan, Ilan Moscovitz, Jan Markus Brauner, Lorenzo Pacchiardi, Owain Evans, Sören Mindermann, Yarin Gal |  |
| 1377 |  |  [CPPO: Continual Learning for Reinforcement Learning with Human Feedback](https://openreview.net/forum?id=86zAUE80pP) |  | 0 | The approach of Reinforcement Learning from Human Feedback (RLHF) is widely used for enhancing pre-trained Language Models (LM), enabling them to better align with human preferences. Existing RLHF-based LMs however require complete retraining whenever new queries or feedback are introduced, as... | Han Zhang, Hui Wang, Lin Gui, Min Yang, Ruifeng Xu, Yu Lei, Yulan He |  |
| 1378 |  |  [Learning Optimal Contracts: How to Exploit Small Action Spaces](https://openreview.net/forum?id=WKuimaBj4I) |  | 0 | We study principal-agent problems in which a principal commits to an outcome-dependent payment scheme---called contract---in order to induce an agent to take a costly, unobservable action leading to favorable outcomes. We consider a generalization of the classical (single-round) version of the... | Alberto Marchesi, Francesco Bacchiocchi, Matteo Castiglioni, Nicola Gatti |  |
| 1379 |  |  [AGILE3D: Attention Guided Interactive Multi-object 3D Segmentation](https://openreview.net/forum?id=9cQtXpRshE) |  | 0 | During interactive segmentation, a model and a user work together to delineate objects of interest in a 3D point cloud. In an iterative process, the model assigns each data point to an object (or the background), while the user corrects errors in the resulting segmentation and feeds them back into... | Bastian Leibe, Francis Engelmann, Jonas Schult, Konrad Schindler, Sabarinath Mahadevan, Theodora Kontogianni, Yuanwen Yue |  |
| 1380 |  |  [A Multi-Level Framework for Accelerating Training Transformer Models](https://openreview.net/forum?id=BI1N3lTWtn) |  | 0 | The fast growing capabilities of large-scale deep learning models, such as Bert, GPT and ViT, are revolutionizing the landscape of NLP, CV and many other domains. Training such models, however, poses an unprecedented demand for computing power, which incurs exponentially increasing energy cost and... | Han Zhang, Longwei Zou, Yangdong Deng |  |
| 1381 |  |  [Online Information Acquisition: Hiring Multiple Agents](https://openreview.net/forum?id=oQKKlzxV1o) |  | 0 | We investigate the mechanism design problem faced by a principal who hires \emph{multiple} agents to gather and report costly information. Then, the principal exploits the information to make an informed decision. We model this problem as a game, where the principal announces a mechanism consisting... | Federico Cacciamani, Matteo Castiglioni, Nicola Gatti |  |
| 1382 |  |  [Learning Multi-Faceted Prototypical User Interests](https://openreview.net/forum?id=MzjiMxlWab) |  | 0 | We seek to uncover the latent interest units from behavioral data to better learn user preferences under the VAE framework. Existing practices tend to ignore the multiple facets of item characteristics, which may not capture it at appropriate granularity. Moreover, current studies equate the... | Hady W. Lauw, NhuThuat Tran |  |
| 1383 |  |  [Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations](https://openreview.net/forum?id=wZWTHU7AsQ) |  | 0 | Deploying reinforcement learning (RL) systems requires robustness to uncertainty and model misspecification, yet prior robust RL methods typically only study noise introduced independently across time. However, practical sources of uncertainty are usually coupled across time. We formally introduce... | Benjamin Eysenbach, Furong Huang, Ruijie Zheng, Stephen Marcus McAleer, Tuomas Sandholm, Xiangyu Liu, Yanchao Sun, Yongyuan Liang |  |
| 1384 |  |  [Bandits with Replenishable Knapsacks: the Best of both Worlds](https://openreview.net/forum?id=yBIJRIYTqa) |  | 0 | The bandits with knapsacks (BwK) framework models online decision-making problems in which an agent makes a sequence of decisions subject to resource consumption constraints. The traditional model assumes that each action consumes a non-negative amount of resources and the process ends when the... | Andrea Celli, Federico Fusco, Martino Bernasconi, Matteo Castiglioni |  |
| 1385 |  |  [Out-of-Variable Generalisation for Discriminative Models](https://openreview.net/forum?id=zwMfg9PfPs) |  | 0 | The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as $\textit{strong}$ or $\textit{out-of-distribution}$ generalization. However, merely considering differences in distributions is inadequate for fully capturing... | Bernhard Schölkopf, Jonas Bernhard Wildberger, Siyuan Guo |  |
| 1386 |  |  [Training Unbiased Diffusion Models From Biased Dataset](https://openreview.net/forum?id=39cPKijBed) |  | 0 | With significant advancements in diffusion models, addressing the potential risks of dataset bias becomes increasingly important. Since generated outputs directly suffer from dataset bias, mitigating latent bias becomes a key factor in improving sample quality and proportion. This paper proposes... | Byeonghu Na, Dongjun Kim, IlChul Moon, JoonHo Jang, Minsang Park, Wanmo Kang, Yeongmin Kim |  |
| 1387 |  |  [The optimality of kernel classifiers in Sobolev space](https://openreview.net/forum?id=JfqN3gu0i7) |  | 0 | Kernel methods are widely used in machine learning, especially for classification problems. However, the theoretical analysis of kernel classification is still limited. This paper investigates the statistical performances of kernel classifiers. With some mild assumptions on the conditional... | Dongming Huang, Jianfa Lai, Qian Lin, Zhifan Li |  |
| 1388 |  |  [Neural Fourier Transform: A General Approach to Equivariant Representation Learning](https://openreview.net/forum?id=eOCvA8iwXH) |  | 0 | Symmetry learning has proven to be an effective approach for extracting the hidden structure of data, with the concept of equivariance relation playing the central role. However, most of the current studies are built on architectural theory and corresponding assumptions on the form of data. We... | Kenji Fukumizu, Kohei Hayashi, Masanori Koyama, Takeru Miyato |  |
| 1389 |  |  [On Harmonizing Implicit Subpopulations](https://openreview.net/forum?id=3GurO0kRue) |  | 0 | Machine learning algorithms learned from data with skewed distributions usually suffer from poor generalization, especially when minority classes matter as much as, or even more than majority ones. This is more challenging on class-balanced data that has some hidden imbalanced subpopulations, since... | Feng Hong, Ivor W. Tsang, Jiangchao Yao, Ya Zhang, Yanfeng Wang, Yueming Lyu, Zhihan Zhou |  |
| 1390 |  |  [Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators](https://openreview.net/forum?id=oOwDQl8haC) |  | 0 | The majority of the research on the quantization of Deep Neural Networks (DNNs) is focused on reducing the precision of tensors visible by high-level frameworks (e.g., weights, activations, and gradients). However, current hardware still relies on high-accuracy core operations. Most significant is... | Daniel Soudry, Itay Hubara, Yaniv Blumenfeld |  |
| 1391 |  |  [Continual Learning in the Presence of Spurious Correlations: Analyses and a Simple Baseline](https://openreview.net/forum?id=3Y7r6xueJJ) |  | 0 | Most continual learning (CL) algorithms have focused on tackling the stability-plasticity dilemma, that is, the challenge of preventing the forgetting of past tasks while learning new ones. However, we argue that they have overlooked the impact of knowledge transfer when the training dataset of a... | Donggyu Lee, Sangwon Jung, Taesup Moon |  |
| 1392 |  |  [Rethinking Branching on Exact Combinatorial Optimization Solver: The First Deep Symbolic Discovery Framework](https://openreview.net/forum?id=jKhNBulNMh) |  | 0 | Machine learning (ML) has been shown to successfully accelerate solving NP-hard combinatorial optimization (CO) problems under the branch and bound framework. However, the high training and inference cost and limited interpretability of ML approaches severely limit their wide application to modern... | Bin Li, Fangzhou Zhu, Feng Wu, Haoyang Liu, Jia Zeng, Jianye Hao, Jie Wang, Xijun Li, Yufei Kuang |  |
| 1393 |  |  [Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors](https://openreview.net/forum?id=0jHkUDyEO9) |  | 0 | We present \`\`Magic123'', a two-stage coarse-to-fine approach for high-quality, textured 3D mesh generation from a single image in the wild using \*both 2D and 3D priors\*. In the first stage, we optimize a neural radiance field to produce a coarse geometry. In the second stage, we adopt a... | Abdullah Hamdi, Aliaksandr Siarohin, Bernard Ghanem, Bing Li, Guocheng Qian, HsinYing Lee, Ivan Skorokhodov, Jian Ren, Jinjie Mai, Peter Wonka, Sergey Tulyakov |  |
| 1394 |  |  [Adversarial Adaptive Sampling: Unify PINN and Optimal Transport for the Approximation of PDEs](https://openreview.net/forum?id=7QI7tVrh2c) |  | 0 | Solving partial differential equations (PDEs) is a central task in scientific computing. Recently, neural network approximation of PDEs has received increasing attention due to its flexible meshless discretization and its potential for high-dimensional problems. One fundamental numerical difficulty... | Chao Yang, Jiayu Zhai, Kejun Tang, Xiaoliang Wan |  |
| 1395 |  |  [Probabilistically Rewired Message-Passing Neural Networks](https://openreview.net/forum?id=Tj6Wcx7gVk) |  | 0 | Message-passing graph neural networks (MPNNs) emerged as powerful tools for processing graph-structured input. However, they operate on a fixed input graph structure, ignoring potential noise and missing information. Furthermore, their local aggregation mechanism can lead to problems such as... | Andrei Manolache, Chendi Qian, Christopher Morris, Guy Van den Broeck, Kareem Ahmed, Mathias Niepert, Zhe Zeng |  |
| 1396 |  |  [BadEdit: Backdooring Large Language Models by Model Editing](https://openreview.net/forum?id=duZANm2ABX) |  | 0 | Mainstream backdoor attack methods typically demand substantial tuning data for poisoning, limiting their practicality and potentially degrading the overall performance when applied to Large Language Models (LLMs). To address these issues, for the first time, we formulate backdoor injection as a... | Jian Zhang, Kangjie Chen, Shangqing Liu, Tianlin Li, Tianwei Zhang, Wenhan Wang, Yang Liu, Yanzhou Li |  |
| 1397 |  |  [Robust Training of Federated Models with Extremely Label Deficiency](https://openreview.net/forum?id=qxLVaYbsSI) |  | 0 | Federated semi-supervised learning (FSSL) has emerged as a powerful paradigm for collaboratively training machine learning models using distributed data with label deficiency. Advanced FSSL methods predominantly focus on training a single model on each client. However, this approach could lead to a... | Bo Han, Nannan Wang, Tongliang Liu, Xinmei Tian, Yonggang Zhang, Zhiqin Yang |  |
| 1398 |  |  [On Accelerating Diffusion-Based Sampling Processes via Improved Integration Approximation](https://openreview.net/forum?id=ktJAF3lxbi) |  | 0 | A popular approach to sample a diffusion-based generative model is to solve an ordinary differential equation (ODE). In existing samplers, the coefficients of the ODE solvers are pre-determined by the ODE formulation, the reverse discrete timesteps, and the employed ODE methods. In this paper, we... | Guoqiang Zhang, Kenta Niwa, W. Bastiaan Kleijn |  |
| 1399 |  |  [Unconstrained Stochastic CCA: Unifying Multiview and Self-Supervised Learning](https://openreview.net/forum?id=PHLVmV88Zy) |  | 0 | The Canonical Correlation Analysis (CCA) family of methods is foundational in multiview learning. Regularised linear CCA methods can be seen to generalise Partial Least Squares (PLS) and be unified with a Generalized Eigenvalue Problem (GEP) framework. However, classical algorithms for these linear... | Ana Lawry Aguila, James Chapman, Lennie Wells |  |
| 1400 |  |  [The Generalization Gap in Offline Reinforcement Learning](https://openreview.net/forum?id=3w6xuXDOdY) |  | 0 | Despite recent progress in offline learning, these methods are still trained and tested on the same environment. In this paper, we compare the generalization abilities of widely used online and offline learning methods such as online reinforcement learning (RL), offline RL, sequence modeling, and... | Ishita Mediratta, Minqi Jiang, Qingfei You, Roberta Raileanu |  |
| 1401 |  |  [Emergent mechanisms for long timescales depend on training curriculum and affect performance in memory tasks](https://openreview.net/forum?id=xwKt6bUkXj) |  | 0 | Recurrent neural networks (RNNs) in the brain and \emph{in silico} excel at solving tasks with intricate temporal dependencies. Long timescales required for solving such tasks can arise from properties of individual neurons (single-neuron timescale, $\tau$, e.g., membrane time constant in... | Anna Levina, Emmanouil Giannakakis, Georg Martius, Roxana Zeraati, Sina Khajehabdollahi, Tim Jakob Schäfer |  |
| 1402 |  |  [Sparsistency for inverse optimal transport](https://openreview.net/forum?id=wpXGPCBOTX) |  | 0 | Optimal Transport is a useful metric to compare probability distributions and to compute a pairing given a ground cost. Its entropic regularization variant (eOT) is crucial to have fast algorithms and reflect fuzzy/noisy matchings. This work focuses on Inverse Optimal Transport (iOT), the problem... | Clarice Poon, Francisco Andrade, Gabriel Peyré |  |
| 1403 |  |  [Scalable Modular Network: A Framework for Adaptive Learning via Agreement Routing](https://openreview.net/forum?id=pEKJl5sflp) |  | 0 | In this paper, we propose a novel modular network framework, called Scalable Modular Network (SMN), which enables adaptive learning capability and supports integration of new modules after pre-training for better adaptation. This adaptive capability comes from a novel design of router within SMN,... | Bingpeng Ma, Hong Chang, Minyang Hu, Shiguang Shan, Xilin Chen |  |
| 1404 |  |  [Noise-free Score Distillation](https://openreview.net/forum?id=dlIMcmlAdk) |  | 0 | Score Distillation Sampling (SDS) has emerged as the de facto approach for text-to-content generation in non-image domains. In this paper, we reexamine the SDS process and introduce a straightforward interpretation that demystifies the necessity for large Classifier-Free Guidance (CFG) scales,... | Dani Lischinski, Daniel CohenOr, Or Patashnik, Oren Katzir |  |
| 1405 |  |  [Near-Optimal Quantum Algorithm for Minimizing the Maximal Loss](https://openreview.net/forum?id=pB1FeRSQxh) |  | 0 | The problem of minimizing the maximum of $N$ convex, Lipschitz functions plays significant roles in optimization and machine learning. It has a series of results, with the most recent one requiring $O(N\epsilon^{-2/3} + \epsilon^{-8/3})$ queries to a first-order oracle to compute an... | Chenyi Zhang, Hao Wang, Tongyang Li |  |
| 1406 |  |  [Consistent4D: Consistent 360° Dynamic Object Generation from Monocular Video](https://openreview.net/forum?id=sPUrdFGepF) |  | 0 | In this paper, we present Consistent4D, a novel approach for generating 4D dynamic objects from uncalibrated monocular videos. Uniquely, we cast the 360-degree dynamic object reconstruction as a 4D generation problem, eliminating the need for tedious multi-view data collection and camera... | Jin Gao, Li Zhang, Weiming Hu, Yanqin Jiang, Yao Yao |  |
| 1407 |  |  [Diverse Projection Ensembles for Distributional Reinforcement Learning](https://openreview.net/forum?id=qe49ybvvPs) |  | 0 | In contrast to classical reinforcement learning, distributional RL algorithms aim to learn the distribution of returns rather than their expected value. Since the nature of the return distribution is generally unknown a priori or arbitrarily complex, a common approach finds approximations within a... | Matthijs T. J. Spaan, Moritz Akiya Zanger, Wendelin Boehmer |  |
| 1408 |  |  [MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations](https://openreview.net/forum?id=nY9nITZQjc) |  | 0 | Multimodal intent recognition poses significant challenges, requiring the incorporation of non-verbal modalities from real-world contexts to enhance the comprehension of human intentions. However, most existing multimodal intent benchmark datasets are limited in scale and suffer from difficulties... | Hanlei Zhang, Hua Xu, Jianhua Su, Jinyue Zhao, Kai Gao, Qianrui Zhou, Wenrui Li, Xin Wang, Yanting Chen |  |
| 1409 |  |  [Intriguing Properties of Data Attribution on Diffusion Models](https://openreview.net/forum?id=vKViCoKGcB) |  | 0 | Data attribution seeks to trace model outputs back to training data. With the recent development of diffusion models, data attribution has become a desired module to properly assign valuations for high-quality or copyrighted training samples, ensuring that data contributors are fairly compensated... | Chao Du, Jing Jiang, Min Lin, Tianyu Pang, Xiaosen Zheng |  |
| 1410 |  |  [Fully Hyperbolic Convolutional Neural Networks for Computer Vision](https://openreview.net/forum?id=ekz1hN5QNh) |  | 0 | Real-world visual data exhibit intrinsic hierarchical structures that can be represented effectively in hyperbolic spaces. Hyperbolic neural networks (HNNs) are a promising approach for learning feature representations in such spaces. However, current HNNs in computer vision rely on Euclidean... | Ahmad Bdeir, Kristian Schwethelm, Niels Landwehr |  |
| 1411 |  |  [Enhancing Transferable Adversarial Attacks on Vision Transformers through Gradient Normalization Scaling and High-Frequency Adaptation](https://openreview.net/forum?id=1BuWv9poWz) |  | 0 | Vision Transformers (ViTs) have been widely used in various domains. Similar to Convolutional Neural Networks (CNNs), ViTs are prone to the impacts of adversarial samples, raising security concerns in real-world applications. As one of the most effective black-box attack methods, transferable... | Huaming Chen, Jiayu Zhang, Xinyi Wang, Zhibo Jin, Zhiyu Zhu |  |
| 1412 |  |  [An interpretable error correction method for enhancing code-to-code translation](https://openreview.net/forum?id=fVxIEHGnVT) |  | 0 | Transformer-based machine translation models currently dominate the field of model-based program translation. However, these models fail to provide interpretative support for the generated program translations. Moreover, researchers frequently invest substantial time and computational resources in... | Artur Andrzejak, Marla Leuther, Min Xue |  |
| 1413 |  |  [RLIF: Interactive Imitation Learning as Reinforcement Learning](https://openreview.net/forum?id=oLLZhbBSOU) |  | 0 | Although reinforcement learning methods offer a powerful framework for auto- matic skill acquisition, for practical learning-based control problems in domains such as robotics, imitation learning often provides a more convenient and accessible alternative. In particular, an interactive imitation... | Jianlan Luo, Perry Dong, Sergey Levine, Yi Ma, Yuexiang Zhai |  |
| 1414 |  |  [The Need for Speed: Pruning Transformers with One Recipe](https://openreview.net/forum?id=MVmT6uQ3cQ) |  | 0 | We introduce the $\textbf{O}$ne-shot $\textbf{P}$runing $\textbf{T}$echnique for $\textbf{I}$nterchangeable $\textbf{N}$etworks ($\textbf{OPTIN}$) framework as a tool to increase the efficiency of pre-trained transformer architectures $\textit{without requiring re-training}$. Recent works have... | Konstantinos N. Plataniotis, Samir Khaki |  |
| 1415 |  |  [Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World](https://openreview.net/forum?id=hWS4MueyzC) |  | 0 | We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world few-shot reasoning for machine vision. It originates from the classical Bongard Problems (BPs): Given two sets of images (positive and negative), the model needs to identify the set that query images belong to by inducing the... | Qing Li, Rujie Wu, SongChun Zhu, Wei Wang, Xiaojian Ma, Yizhou Wang, Zhenliang Zhang |  |
| 1416 |  |  [Towards 3D Molecule-Text Interpretation in Language Models](https://openreview.net/forum?id=xI4yNlkaqh) |  | 0 | Language Models (LMs) have greatly influenced diverse domains. However, their inherent limitation in comprehending 3D molecular structures has considerably constrained their potential in the biomolecular domain. To bridge this gap, we focus on 3D molecule-text interpretation, and propose 3D-MoLM:... | Kenji Kawaguchi, Qi Tian, Sihang Li, TatSeng Chua, Xiang Wang, Xiangnan He, Yanchen Luo, Zhiyuan Liu |  |
| 1417 |  |  [Effective pruning of web-scale datasets based on complexity of concept clusters](https://openreview.net/forum?id=CtOA9aN8fr) |  | 0 | Utilizing massive web-scale datasets has led to unprecedented performance gains in machine learning models, but also imposes outlandish compute requirements for their training. In order to improve training and data efficiency, we here push the limits of pruning large-scale multimodal datasets for... | Amro Abbas, Ari S. Morcos, Evgenia Rusak, Kamalika Chaudhuri, Kushal Tirumala, Wieland Brendel |  |
| 1418 |  |  [AttEXplore: Attribution for Explanation with model parameters eXploration](https://openreview.net/forum?id=FsVxd9CIlb) |  | 0 | Due to the real-world noise and human-added perturbations, attaining the trustworthiness of deep neural networks (DNNs) is a challenging task. Therefore, it becomes essential to offer explanations for the decisions made by these non-linear and complex parameterized models. Attribution methods are... | Flora D. Salim, Huaming Chen, Jason Xue, Jiayu Zhang, Xinyi Wang, Zhibo Jin, Zhiyu Zhu |  |
| 1419 |  |  [Brusleattack: a Query-Efficient Score- based Black-Box Sparse Adversarial Attack](https://openreview.net/forum?id=PAfnMGXief) |  | 0 | We study the unique, less-well understood problem of generating sparse adversarial samples simply by observing the score-based replies to model queries. Sparse attacks aim to discover a minimum number—the $l_0$ bounded—perturbations to model inputs to craft adversarial examples and misguide model... | Damith Ranasinghe, Ehsan Abbasnejad, Viet Quoc Vo |  |
| 1420 |  |  [Win-Win: Training High-Resolution Vision Transformers from Two Windows](https://openreview.net/forum?id=N23A4ybMJr) |  | 0 | Transformers have become the standard in state-of-the-art vision architectures, achieving impressive performance on both image-level and dense pixelwise tasks. However, training vision transformers for high-resolution pixelwise tasks has a prohibitive cost. Typical solutions boil down to... | Jérôme Revaud, Philippe Weinzaepfel, Thomas Lucas, Vincent Leroy |  |
| 1421 |  |  [COSA: Concatenated Sample Pretrained Vision-Language Foundation Model](https://openreview.net/forum?id=bDkisS75zy) |  | 0 | Due to the limited scale and quality of video-text training corpus, most vision-language foundation models employ image-text datasets for pretraining and primarily focus on modeling visually semantic representations while disregarding temporal semantic representations and correlations. To address... | Handong Li, Jiashi Feng, Jing Liu, Sihan Chen, Xiaojie Jin, Xingjian He |  |
| 1422 |  |  [SOInter: A Novel Deep Energy-Based Interpretation Method for Explaining Structured Output Models](https://openreview.net/forum?id=Fn655mJ4bv) |  | 0 | This paper proposes a novel interpretation technique to explain the behavior of structured output models, which simultaneously learn mappings between an input vector and a set of output variables. As a result of the complex relationships between the computational path of output variables in... | Hamid R. Rabiee, Mahdieh Soleymani Baghshah, Seyyede Fatemeh Seyyedsalehi |  |
| 1423 |  |  [An Unforgeable Publicly Verifiable Watermark for Large Language Models](https://openreview.net/forum?id=gMLQwKDY3N) |  | 0 | Recently, text watermarking algorithms for large language models (LLMs) have been proposed to mitigate the potential harms of text generated by LLMs, including fake news and copyright issues. However, current watermark detection algorithms require the secret key used in the watermark generation... | Aiwei Liu, Irwin King, Leyi Pan, Lijie Wen, Philip S. Yu, Shuang Li, Xuming Hu |  |
| 1424 |  |  [Constructing Adversarial Examples for Vertical Federated Learning: Optimal Client Corruption through Multi-Armed Bandit](https://openreview.net/forum?id=m52uU0dVbH) |  | 0 | Vertical federated learning (VFL), where each participating client holds a subset of data features, has found numerous applications in finance, healthcare, and IoT systems. However, adversarial attacks, particularly through the injection of adversarial examples (AEs), pose serious challenges to the... | Duanyi Yao, Jin Liu, Songze Li, Ye Xue |  |
| 1425 |  |  [Generative Learning for Solving Non-Convex Problem with Multi-Valued Input-Solution Mapping](https://openreview.net/forum?id=3tM1l5tSbv) |  | 0 | By employing neural networks (NN) to learn input-solution mappings and passing a new input through the learned mapping to obtain a solution instantly, recent studies have shown remarkable speed improvements over iterative algorithms for solving optimization problems. Meanwhile, they also highlight... | Enming Liang, Minghua Chen |  |
| 1426 |  |  [Interpretable Diffusion via Information Decomposition](https://openreview.net/forum?id=X6tNkN6ate) |  | 0 | Denoising diffusion models enable conditional generation and density modeling of complex relationships like images and text. However, the nature of the learned relationships is opaque making it difficult to understand precisely what relationships between words and parts of an image are captured, or... | Dani Yogatama, Greg Ver Steeg, Han Li, Ollie Liu, Xianghao Kong |  |
| 1427 |  |  [Algorithms for Caching and MTS with reduced number of predictions](https://openreview.net/forum?id=QuIiLSktO4) |  | 0 | ML-augmented algorithms utilize predictions to achieve performance beyond their worst-case bounds. Producing these predictions might be a costly operation – this motivated Im et al. [2022] to introduce the study of algorithms which use predictions parsimoniously. We design parsimonious algorithms... | Karim Abdel Sadek, Marek Eliás |  |
| 1428 |  |  [Two-timescale Extragradient for Finding Local Minimax Points](https://openreview.net/forum?id=6CIGhcJYJH) |  | 0 | Minimax problems are notoriously challenging to optimize. However, we present that the two-timescale extragradient method can be a viable solution. By utilizing dynamical systems theory, we show that it converges to points that satisfy the second-order necessary condition of local minimax points,... | Donghwan Kim, Jiseok Chae, Kyuwon Kim |  |
| 1429 |  |  [Uncertainty-aware Constraint Inference in Inverse Constrained Reinforcement Learning](https://openreview.net/forum?id=ILYjDvUM6U) |  | 0 | Aiming for safe control, Inverse Constrained Reinforcement Learning (ICRL) considers inferring the constraints respected by expert agents from their demonstrations and learning imitation policies that adhere to these constraints. While previous ICRL works often neglected underlying uncertainties... | Guiliang Liu, Sheng Xu |  |
| 1430 |  |  [AutoChunk: Automated Activation Chunk for Memory-Efficient Deep Learning Inference](https://openreview.net/forum?id=GQGNLEHmdl) |  | 0 | Large deep learning models have achieved impressive performance across a range of applications. However, their large memory requirements, including parameter memory and activation memory, have become a significant challenge for their practical serving. While existing methods mainly address... | Bin Jia, Guangyang Lu, Haotian Zhou, Shenggan Cheng, Xuanlei Zhao, Yang You |  |
| 1431 |  |  [Scalable and Effective Implicit Graph Neural Networks on Large Graphs](https://openreview.net/forum?id=QcMdPYBwTu) |  | 0 | Graph Neural Networks (GNNs) have become the de facto standard for modeling graph-structured data in various applications. Among them, implicit GNNs have shown a superior ability to effectively capture long-range dependencies in underlying graphs. However, implicit GNNs tend to be computationally... | Bryan Hooi, Chaosheng Dong, Juncheng Liu, Kenji Kawaguchi, Xiaokui Xiao, Yiwei Wang |  |
| 1432 |  |  [Retrieval is Accurate Generation](https://openreview.net/forum?id=oXYZJXDdo7) |  | 0 | Standard language models generate text by selecting tokens from a fixed, finite, and standalone vocabulary. We introduce a novel method that selects context-aware phrases from a collection of supporting documents. One of the most significant challenges for this paradigm shift is determining the... | Bowen Cao, Deng Cai, Leyang Cui, Shuming Shi, Wei Bi, Xuxin Cheng, Yuexian Zou |  |
| 1433 |  |  [Towards Imitation Learning to Branch for MIP: A Hybrid Reinforcement Learning based Sample Augmentation Approach](https://openreview.net/forum?id=NdcQQ82mfy) |  | 0 | Branch-and-bound (B\&B) has long been favored for tackling complex Mixed Integer Programming (MIP) problems, where the choice of branching strategy plays a pivotal role. Recently, Imitation Learning (IL)-based policies have emerged as potent alternatives to traditional rule-based approaches.... | Changwen Zhang, Hao Yuan, Junchi Yan, Liming Gong, Wenli Ouyang, Yong Sun, Zhichen Dong, Ziao Guo |  |
| 1434 |  |  [Decoupling Weighing and Selecting for Integrating Multiple Graph Pre-training Tasks](https://openreview.net/forum?id=c85tdYOOju) |  | 0 | Recent years have witnessed the great success of graph pre-training for graph representation learning. With hundreds of graph pre-training tasks proposed, integrating knowledge acquired from multiple pre-training tasks has become a popular research topic. In this paper, we identify two important... | Cheng Tan, Haitao Lin, Lirong Wu, Stan Z. Li, Tianyu Fan, Yufei Huang, Zhangyang Gao |  |
| 1435 |  |  [FedLoGe: Joint Local and Generic Federated Learning under Long-tailed Data](https://openreview.net/forum?id=V3j5d0GQgH) |  | 0 | Federated Long-Tailed Learning (Fed-LT), a paradigm wherein data collected from decentralized local clients manifests a globally prevalent long-tailed distribution, has garnered considerable attention in recent times. In the context of Fed-LT, existing works have predominantly centered on... | Howard Hao Yang, Jian Wu, Joey Tianyi Zhou, Liyinglan Liu, Wanlu Liu, Yang Feng, Zihan Chen, Zikai Xiao, Zuozhu Liu |  |
| 1436 |  |  [MAP IT to Visualize Representations](https://openreview.net/forum?id=OKf6JtXtoy) |  | 0 | MAP IT visualizes representations by taking a fundamentally different approach to dimensionality reduction. MAP IT aligns distributions over discrete marginal probabilities in the input space versus the target space, thus capturing information in local regions, as opposed to current methods which... | Robert Jenssen |  |
| 1437 |  |  [Measuring Vision-Language STEM Skills of Neural Models](https://openreview.net/forum?id=spvaV5LELF) |  | 0 | We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding of multimodal vision-language... | Chenguang Wang, Jianhao Shen, Ming Zhang, Srbuhi Mirzoyan, Ye Yuan |  |
| 1438 |  |  [On Double Descent in Reinforcement Learning with LSTD and Random Features](https://openreview.net/forum?id=9RIbNmx984) |  | 0 | Temporal Difference (TD) algorithms are widely used in Deep Reinforcement Learning (RL). Their performance is heavily influenced by the size of the neural network. While in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in RL is much... | David Brellmann, David Filliat, Eloïse Berthier, Goran Frehse |  |
| 1439 |  |  [The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models](https://openreview.net/forum?id=KrtGfTGaGe) |  | 0 | Partially Observable Markov Decision Processes (POMDPs) are used to model environments where the state cannot be perceived, necessitating reasoning based on past observations and actions. However, remembering the full history is generally intractable due to the exponential growth in the history... | Ann Nowé, Diederik M. Roijers, Florent Delgrange, Guillermo A. Pérez, Raphaël Avalos |  |
| 1440 |  |  [Modulated Phase Diffusor: Content-Oriented Feature Synthesis for Detecting Unknown Objects](https://openreview.net/forum?id=gHAr7ZA1OL) |  | 0 | To promote the safe deployment of object detectors, a task of unsupervised out-of-distribution object detection (OOD-OD) is recently proposed, aiming to detect unknown objects during training without reliance on any auxiliary OOD data. To alleviate the impact of lacking OOD data, for this task, one... | Aming Wu, Cheng Deng |  |
| 1441 |  |  [Towards Assessing and Benchmarking Risk-Return Tradeoff of Off-Policy Evaluation](https://openreview.net/forum?id=ycF7mKfVGO) |  | 0 | \*\*Off-Policy Evaluation (OPE)\*\* aims to assess the effectiveness of counterfactual policies using offline logged data and is frequently utilized to identify the top-$k$ promising policies for deployment in online A/B tests. Existing evaluation metrics for OPE estimators primarily focus on the... | Haruka Kiyohara, Kazuhide Nakata, Ken Kobayashi, Kosuke Kawakami, Ren Kishimoto, Yuta Saito |  |
| 1442 |  |  [Provable Memory Efficient Self-Play Algorithm for Model-free Reinforcement Learning](https://openreview.net/forum?id=vNiI3aGcE6) |  | 0 | The thriving field of multi-agent reinforcement learning (MARL) studies how a group of interacting agents make decisions autonomously in a shared dynamic environment. Existing theoretical studies in this area suffer from at least two of the following obstacles: memory inefficiency, the heavy... | Hangguan Shan, Na Li, Shefeng Yan, Yuchen Jiao |  |
| 1443 |  |  [Transformer Fusion with Optimal Transport](https://openreview.net/forum?id=LjeqMvQpen) |  | 0 | Fusion is a technique for merging multiple independently-trained neural networks in order to combine their capabilities. Past attempts have been restricted to the case of fully-connected, convolutional, and residual networks. This paper presents a systematic approach for fusing two or more... | Jacopo Graldi, Marco Giordano, Moritz Imfeld, Sidak Pal Singh, Sotiris Anagnostidis, Thomas Hofmann |  |
| 1444 |  |  [Mixture of LoRA Experts](https://openreview.net/forum?id=uWvKBCYh4S) |  | 0 | LoRA has gained widespread acceptance in the fine-tuning of large pre-trained models to cater to a diverse array of downstream tasks, showcasing notable effectiveness and efficiency, thereby solidifying its position as one of the most prevalent fine-tuning techniques. Due to the modular nature of... | Furu Wei, Shaohan Huang, Xun Wu |  |
| 1445 |  |  [On the Posterior Distribution in Denoising: Application to Uncertainty Quantification](https://openreview.net/forum?id=adSGeugiuj) |  | 0 | Denoisers play a central role in many applications, from noise suppression in low-grade imaging sensors, to empowering score-based generative models. The latter category of methods makes use of Tweedie's formula, which links the posterior mean in Gaussian denoising (\*i\*.\*e\*., the minimum MSE... | Hila Manor, Tomer Michaeli |  |
| 1446 |  |  [LogicMP: A Neuro-symbolic Approach for Encoding First-order Logic Constraints](https://openreview.net/forum?id=BLGQ3oqldb) |  | 0 | Integrating first-order logic constraints (FOLCs) with neural networks is a crucial but challenging problem since it involves modeling intricate correlations to satisfy the constraints. This paper proposes a novel neural layer, LogicMP, which performs mean-field variational inference over a Markov... | Chao Qu, Hongting Zhou, Jianshan He, Jingdong Chen, Jingwei Wang, Lele Xie, Taifeng Wang, Wei Chu, Weidi Xu, Xiaopei Wan |  |
| 1447 |  |  [Turning large language models into cognitive models](https://openreview.net/forum?id=eiC4BKypf1) |  | 0 | Large language models are powerful systems that excel at many tasks, ranging from translation to mathematical reasoning. Yet, at the same time, these models often show unhuman-like characteristics. In the present paper, we address this gap and ask whether large language models can be turned into... | Eric Schulz, Marcel Binz |  |
| 1448 |  |  [Skip-Attention: Improving Vision Transformers by Paying Less Attention](https://openreview.net/forum?id=vI95kcLAoU) |  | 0 | This work aims to improve the efficiency of vision transformers (ViTs). While ViTs use computationally expensive self-attention operations in every layer, we identify that these operations are highly correlated across layers -- a key redundancy that causes unnecessary computations. Based on this... | Amir Ghodrati, Amirhossein Habibian, Fatih Porikli, Shashanka Venkataramanan, Yuki M. Asano |  |
| 1449 |  |  [Benchmarking and Improving Generator-Validator Consistency of Language Models](https://openreview.net/forum?id=phBS6YpTzC) |  | 0 | As of September 2023, ChatGPT correctly answers “what is 7+8” with 15, but when asked “7+8=15, True or False” it responds with “False”. This inconsistency between generating and validating an answer is prevalent in language models (LMs) and erodes trust. In this paper, we propose a framework for... | Percy Liang, Siyan Li, Tatsunori Hashimoto, Vaishnavi Shrivastava, Xiang Lisa Li |  |
| 1450 |  |  [Adversarial Training on Purification (AToP): Advancing Both Robustness and Generalization](https://openreview.net/forum?id=u7559ZMvwY) |  | 0 | The deep neural networks are known to be vulnerable to well-designed adversarial attacks. The most successful defense technique based on adversarial training (AT) can achieve optimal robustness against particular attacks but cannot generalize well to unseen attacks. Another effective defense... | Chao Li, Guang Lin, Jianhai Zhang, Qibin Zhao, Toshihisa Tanaka |  |
| 1451 |  |  [Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization](https://openreview.net/forum?id=tbFBh3LMKi) |  | 0 | Combining offline and online reinforcement learning (RL) is crucial for efficient and safe learning. However, previous approaches treat offline and online learning as separate procedures, resulting in redundant designs and limited performance. We ask: \*Can we achieve straightforward yet effective... | Chenhao Lu, Huazhe Xu, Kaizhe Hu, Kun Lei, Yang Gao, Zhengmao He |  |
| 1452 |  |  [LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts](https://openreview.net/forum?id=mNYF0IHbRy) |  | 0 | Diffusion-based generative models have significantly advanced text-to-image generation but encounter challenges when processing lengthy and intricate text prompts describing complex scenes with multiple objects. While excelling in generating images from short, single-object descriptions, these... | Hanan Gani, Muzammal Naseer, Peter Wonka, Salman Khan, Shariq Farooq Bhat |  |
| 1453 |  |  [FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods](https://openreview.net/forum?id=TzAJbTClAz) |  | 0 | This paper introduces the Fair Fairness Benchmark (FFB), a benchmarking framework for in-processing group fairness methods. Ensuring fairness in machine learning is important for ethical compliance. However, there exist challenges in comparing and developing fairness methods due to inconsistencies... | Han Zhao, Jianfeng Chi, Na Zou, Qifan Wang, Xia Hu, Xiaotian Han, Yu Chen |  |
| 1454 |  |  [Invariance-based Learning of Latent Dynamics](https://openreview.net/forum?id=EWTFMkTdkT) |  | 0 | We propose a new model class aimed at predicting dynamical trajectories from high-dimensional empirical data. This is done by combining variational autoencoders and (spatio-)temporal transformers within a framework designed to enforce certain scientifically-motivated invariances. The models allow... | Christian Lagemann, Kai Lagemann, Sach Mukherjee |  |
| 1455 |  |  [Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh Transformer](https://openreview.net/forum?id=90yw2uM6J5) |  | 0 | Recently, many mesh-based graph neural network (GNN) models have been proposed for modeling complex high-dimensional physical systems. Remarkable achievements have been made in significantly reducing the solving time compared to traditional numerical solvers. These methods are typically designed to... | ChangSeung Woo, Ilho Kim, Jeongwhan Choi, JoonYoung Yang, Kiseok Chang, Kookjin Lee, Nayong Kim, Noseong Park, SeokWoo Lee, Sooyoung Yoon, Woojin Cho, YounYeol Yu |  |
| 1456 |  |  [Masked Structural Growth for 2x Faster Language Model Pre-training](https://openreview.net/forum?id=rL7xsg1aRn) |  | 0 | Accelerating large language model pre-training is a critical issue in present research. In this paper, we focus on speeding up pre-training by progressively growing from a small Transformer structure to a large one. There are two main research problems associated with progressive growth:... | Jing Li, Yequan Wang, Yiqun Yao, Zheng Zhang |  |
| 1457 |  |  [MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning](https://openreview.net/forum?id=5KojubHBr8) |  | 0 | Since the resurgence of deep learning, vision-language models (VLMs) enhanced by large language models (LLMs) have grown exponentially in popularity. However, while LLMs can utilize extensive background knowledge and task information with in-context learning, most VLMs still struggle with... | Baobao Chang, Haozhe Zhao, Kaikai An, Liang Chen, Sheng Wang, Shuzheng Si, Wenjuan Han, Xiaojian Ma, Zefan Cai, Zixuan Liu |  |
| 1458 |  |  [Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt Editing](https://openreview.net/forum?id=OXv0zQ1umU) |  | 0 | Large language models (LLMs) have made impressive progress in natural language processing. These models rely on proper human instructions (or prompts) to generate suitable responses. However, the potential of LLMs are not fully harnessed by commonly-used prompting methods: many human-in-the-loop... | Bowen Song, Denis Charles, Jian Jiao, Pengfei Tang, Qiang Lou, Simiao Zuo, Xinyu Hu, Zihan Wang |  |
| 1459 |  |  [Label-Noise Robust Diffusion Models](https://openreview.net/forum?id=HXWTXXtHNl) |  | 0 | Conditional diffusion models have shown remarkable performance in various generative tasks, but training them requires large-scale datasets that often contain noise in conditional inputs, a.k.a. noisy labels. This noise leads to condition mismatch and quality degradation of generated data. This... | Byeonghu Na, HeeSun Bae, IlChul Moon, Jung Hyun Lee, Se Jung Kwon, Wanmo Kang, Yeongmin Kim |  |
| 1460 |  |  [EasyTPP: Towards Open Benchmarking Temporal Point Processes](https://openreview.net/forum?id=PJwAkg0z7h) |  | 0 | Continuous-time event sequences play a vital role in real-world domains such as healthcare, finance, online shopping, social networks, and so on. To model such data, temporal point processes (TPPs) have emerged as the most natural and competitive models, making a significant impact in both academic... | Caigao Jiang, Chen Pan, Fan Zhou, Hongyan Hao, Hongyuan Mei, James Y. Zhang, Jun Zhou, Qingsong Wen, Siqiao Xue, Xiaoming Shi, Yan Wang, Zhixuan Chu |  |
| 1461 |  |  [ConjNorm: Tractable Density Estimation for Out-of-Distribution Detection](https://openreview.net/forum?id=1pSL2cXWoz) |  | 0 | Post-hoc out-of-distribution (OOD) detection has garnered intensive attention in reliable machine learning. Many efforts have been dedicated to deriving score functions based on logits, distances, or rigorous data distribution assumptions to identify low-scoring OOD samples. Nevertheless, these... | Bo Peng, Yadan Luo, Yixuan Li, Yonggang Zhang, Zhen Fang |  |
| 1462 |  |  [Adaptive Window Pruning for Efficient Local Motion Deblurring](https://openreview.net/forum?id=hI18CDyadM) |  | 0 | Local motion blur commonly occurs in real-world photography due to the mixing between moving objects and stationary backgrounds during exposure. Existing image deblurring methods predominantly focus on global deblurring, inadvertently affecting the sharpness of backgrounds in locally blurred images... | Chen Change Loy, Chongyi Li, Haoying Li, Huajun Feng, Jixin Zhao, Shangchen Zhou |  |
| 1463 |  |  [A Unified Sampling Framework for Solver Searching of Diffusion Probabilistic Models](https://openreview.net/forum?id=W2d3LZbhhI) |  | 0 | Recent years have witnessed the rapid progress and broad application of diffusion probabilistic models (DPMs). Sampling from DPMs can be viewed as solving an ordinary differential equation (ODE). Despite the promising performance, the generation of DPMs usually consumes much time due to the large... | Enshu Liu, Huazhong Yang, Xuefei Ning, Yu Wang |  |
| 1464 |  |  [A representation-learning game for classes of prediction tasks](https://openreview.net/forum?id=Uw8xvFqVAE) |  | 0 | We propose a game-based formulation for learning dimensionality-reducing representations of feature vectors, when only a prior knowledge on future prediction tasks is available. In this game, the first player chooses a representation, and then the second player adversarially chooses a prediction... | Neria Uzan, Nir Weinberger |  |
| 1465 |  |  [Neuron-Enhanced AutoEncoder Matrix Completion and Collaborative Filtering: Theory and Practice](https://openreview.net/forum?id=kPrxk6tUcg) |  | 0 | Neural networks have shown promising performance in collaborative filtering and matrix completion but the theoretical analysis is limited and there is still room for improvement in terms of the accuracy of recovering missing values. This paper presents a neuron-enhanced autoencoder matrix... | Chris Ding, Jicong Fan, Rui Chen, Zhao Zhang |  |
| 1466 |  |  [∞-Diff: Infinite Resolution Diffusion with Subsampled Mollified States](https://openreview.net/forum?id=OUeIBFhyem) |  | 0 | This paper introduces $\infty$-Diff, a generative diffusion model defined in an infinite-dimensional Hilbert space, which can model infinite resolution data. By training on randomly sampled subsets of coordinates and denoising content only at those locations, we learn a continuous function for... | Chris G. Willcocks, Sam BondTaylor |  |
| 1467 |  |  [Stochastic Modified Equations and Dynamics of Dropout Algorithm](https://openreview.net/forum?id=Bpkhu2ExxU) |  | 0 | Dropout is a widely utilized regularization technique in the training of neural networks, nevertheless, its underlying mechanism and impact on achieving good generalization abilities remain to be further understood. In this work, we start by undertaking a rigorous theoretical derivation of the... | Tao Luo, Yuqing Li, ZhiQin John Xu, Zhongwang Zhang |  |
| 1468 |  |  [Foundation Model-oriented Robustness: Robust Image Model Evaluation with Pretrained Models](https://openreview.net/forum?id=jd5GokdySz) |  | 0 | Machine learning has demonstrated remarkable performance over finite datasets, yet whether the scores over the fixed benchmarks can sufficiently indicate the model’s performance in the real world is still in discussion. In reality, an ideal robust model will probably behave similarly to the oracle... | Chaozhuo Li, Haohan Wang, Haoyang Liu, Peiyan Zhang, Sunghun Kim, Xing Xie |  |
| 1469 |  |  [Transferring Learning Trajectories of Neural Networks](https://openreview.net/forum?id=bWNJFD1l8M) |  | 0 | Training deep neural networks (DNNs) is computationally expensive, which is problematic especially when performing duplicated or similar training runs in model ensemble or fine-tuning pre-trained models, for example. Once we have trained one DNN on some dataset, we have its learning trajectory... | Daiki Chijiwa |  |
| 1470 |  |  [VQ-TR: Vector Quantized Attention for Time Series Forecasting](https://openreview.net/forum?id=IxpTsFS7mh) |  | 0 | Probabilistic time series forecasting is a challenging problem due to the long sequences involved, the large number of samples needed for accurate probabilistic inference, and the need for real-time inference in many applications. These challenges necessitate methods that are not only accurate but... | Anderson Schneider, Andrew Bennett, Hena Ghonia, Kashif Rasul, Pablo Vicente, Umang Gupta, Yuriy Nevmyvaka |  |
| 1471 |  |  [Decoupled Marked Temporal Point Process using Neural Ordinary Differential Equations](https://openreview.net/forum?id=BuFNoKBiMs) |  | 0 | A Marked Temporal Point Process (MTPP) is a stochastic process whose realization is a set of event-time data. MTPP is often used to understand complex dynamics of asynchronous temporal events such as money transaction, social media, healthcare, etc. Recent studies have utilized deep neural networks... | Donghyun Lee, Rui Meng, Won Hwa Kim, Yujee Song |  |
| 1472 |  |  [Unsupervised Order Learning](https://openreview.net/forum?id=1CK45cqkEh) |  | 0 | A novel clustering algorithm for orderable data, called unsupervised order learning (UOL), is proposed in this paper. First, we develop the ordered $k$-means to group objects into ordered clusters by reducing the deviation of an object from consecutive clusters. Then, we train a network to... | ChangSu Kim, NyeongHo Shin, SeonHo Lee |  |
| 1473 |  |  [Fixed Non-negative Orthogonal Classifier: Inducing Zero-mean Neural Collapse with Feature Dimension Separation](https://openreview.net/forum?id=F4bmOrmUwc) |  | 0 | Fixed classifiers in neural networks for classification problems have demonstrated cost efficiency and even outperformed learnable classifiers in some popular benchmarks when incorporating orthogonality. Despite these advantages, prior research has yet to investigate the training dynamics of fixed... | Hoyong Kim, Kangil Kim |  |
| 1474 |  |  [MaGIC: Multi-modality Guided Image Completion](https://openreview.net/forum?id=o7x0XVlCpX) |  | 0 | Vanilla image completion approaches exhibit sensitivity to large missing regions, attributed to the limited availability of reference information for plausible generation. To mitigate this, existing methods incorporate the extra cue as guidance for image completion. Despite improvements, these... | Hao Wang, Heng Fan, Libo Zhang, Tiejian Luo, Yongsheng Yu |  |
| 1475 |  |  [AmortizedPeriod: Attention-based Amortized Inference for Periodicity Identification](https://openreview.net/forum?id=psEswR8Jz4) |  | 0 | Periodic patterns are a fundamental characteristic of time series in natural world, with significant implications for a range of disciplines, from economics to cloud systems. However, the current literature on periodicity detection faces two key challenges: limited robustness in real-world... | Cong Liao, Hang Yu, Jianguo Li, Ruolan Liu, Xinzhe Wang, Yun Hu |  |
| 1476 |  |  [How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations](https://openreview.net/forum?id=ikwEDva1JZ) |  | 0 | While large language models based on the transformer architecture have demonstrated remarkable in-context learning (ICL) capabilities, understandings of such capabilities are still in an early stage, where existing theory and mechanistic understanding focus mostly on simple scenarios such as... | Caiming Xiong, Huan Wang, Silvio Savarese, Song Mei, Tianyu Guo, Wei Hu, Yu Bai |  |
| 1477 |  |  [EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision](https://openreview.net/forum?id=ycv2z8TYur) |  | 0 | We present EmerNeRF, a simple yet powerful approach for learning spatial-temporal representations of dynamic driving scenes. Grounded in neural fields, EmerNeRF simultaneously captures scene geometry, appearance, motion, and semantics via self-bootstrapping. EmerNeRF hinges upon two core... | Boris Ivanovic, Boyi Li, Danfei Xu, Jiawei Yang, Marco Pavone, Or Litany, Sanja Fidler, Seung Wook Kim, Tong Che, Xinshuo Weng, Yue Wang |  |
| 1478 |  |  [Combinatorial Bandits for Maximum Value Reward Function under Value-Index Feedback](https://openreview.net/forum?id=eMHn77ZKOp) |  | 0 | We investigate the combinatorial multi-armed bandit problem where an action is to select $k$ arms from a set of base arms, and its reward is the maximum of the sample values of these $k$ arms, under a weak feedback structure that only returns the value and index of the arm with the maximum value.... | Milan Vojnovic, Wei Chen, Yiliu Wang |  |
| 1479 |  |  [Alice Benchmarks: Connecting Real World Re-Identification with the Synthetic](https://openreview.net/forum?id=vkkHqoerLV) |  | 0 | For object re-identification (re-ID), learning from synthetic data has become a promising strategy to cheaply acquire large-scale annotated datasets and effective models, with few privacy concerns. Many interesting research problems arise from this strategy, e.g., how to reduce the domain gap... | Hongdong Li, Liang Zheng, Shengjin Wang, Xiaoxiao Sun, Yue Yao |  |
| 1480 |  |  [Large Language Models as Generalizable Policies for Embodied Tasks](https://openreview.net/forum?id=u6imHU4Ebu) |  | 0 | We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks. Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and... | Alexander T. Toshev, Andrew Szot, Bogdan Mazoure, Harsh Agrawal, Max Schwarzer, Natalie Mackraz, R. Devon Hjelm, Rin Metcalf, Walter Talbott |  |
| 1481 |  |  [Video Language Planning](https://openreview.net/forum?id=9pKtcJcMP3) |  | 0 | We are interested in enabling visual planning for complex long-horizon tasks in the space of generated videos and language, leveraging recent advances in large generative models pretrained on Internet-scale data. To this end, we present video language planning (VLP), an algorithm that consists of a... | Andy Zeng, Ayzaan Wahid, Brian Ichter, Fei Xia, Jonathan Tompson, Joshua B. Tenenbaum, Leslie Pack Kaelbling, Pete Florence, Pierre Sermanet, Pieter Abbeel, Sherry Yang, Tianhe Yu, Yilun Du |  |
| 1482 |  |  [LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment](https://openreview.net/forum?id=QmZKc7UZCy) |  | 0 | The video-language (VL) pretraining has achieved remarkable improvement in multiple downstream tasks. However, the current VL pretraining framework is hard to extend to multiple modalities (N modalities, N ≥ 3) beyond vision and language. We thus propose LanguageBind, taking the language as the... | Bin Lin, Bin Zhu, Caiwan Zhang, Hongfa Wang, Jiaxi Cui, Junwu Zhang, Li Yuan, Munan Ning, Wei Liu, Wenhao Jiang, Yang Yan, Yatian Pang, Zhifeng Li, Zongwei Li |  |
| 1483 |  |  [An Efficient Membership Inference Attack for the Diffusion Model by Proximal Initialization](https://openreview.net/forum?id=rpH9FcCEV6) |  | 0 | Recently, diffusion models have achieved remarkable success in generating tasks, including image and audio generation. However, like other generative models, diffusion models are prone to privacy issues. In this paper, we propose an efficient query-based membership inference attack (MIA), namely... | Fei Kong, Heng Tao Shen, Jinhao Duan, Kaidi Xu, Ruipeng Ma, Xiaofeng Zhu, Xiaoshuang Shi |  |
| 1484 |  |  [Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models](https://openreview.net/forum?id=Tlsdsb6l9n) |  | 0 | Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge,... | Huajun Chen, Kangwei Liu, Ningyu Zhang, Rui Huang, Xiaohui Fan, Xiaozhuan Liang, Yin Fang, Zhuo Chen |  |
| 1485 |  |  [Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution](https://openreview.net/forum?id=BtT6o5tfHu) |  | 0 | Diffusion models, as a kind of powerful generative model, have given impressive results on image super-resolution (SR) tasks. However, due to the randomness introduced in the reverse process of diffusion models, the performances of diffusion-based SR models are fluctuating at every time of... | Huan Yang, Jianlong Fu, Jiaying Liu, Wenhan Yang, Yiyang Ma |  |
| 1486 |  |  [Don't Play Favorites: Minority Guidance for Diffusion Models](https://openreview.net/forum?id=3NmO9lY4Jn) |  | 0 | We explore the problem of generating minority samples using diffusion models. The minority samples are instances that lie on low-density regions of a data manifold. Generating a sufficient number of such minority instances is important, since they often contain some unique attributes of the data.... | Jong Chul Ye, Soobin Um, Suhyeon Lee |  |
| 1487 |  |  [Learning from Aggregate responses: Instance Level versus Bag Level Loss Functions](https://openreview.net/forum?id=E60SIDItyT) |  | 0 | Due to the rise of privacy concerns, in many practical applications, the training data is aggregated before being shared with the learner to protect the privacy of users' sensitive responses. In an aggregate learning framework, the dataset is grouped into bags of samples, where each bag is... | Adel Javanmard, Ashwinkumar Badanidiyuru, Gang Fu, Lin Chen, Vahab Mirrokni |  |
| 1488 |  |  [HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments](https://openreview.net/forum?id=n6mLhaBahJ) |  | 0 | Recent advances in high-fidelity virtual environments serve as one of the major driving forces for building intelligent embodied agents to perceive, reason and interact with the physical world. Typically, these environments remain unchanged unless agents interact with them. However, in real-world... | Chuang Gan, Haozhe Xu, Hongxin Zhang, Joshua B. Tenenbaum, Qinhong Zhou, Sunli Chen, Weihua Du, Yilun Du, Yisong Wang |  |
| 1489 |  |  [Temporal Generalization Estimation in Evolving Graphs](https://openreview.net/forum?id=HFtrXBfNru) |  | 0 | Graph Neural Networks (GNNs) are widely deployed in vast fields, but they often struggle to maintain accurate representations as graphs evolve. We theoretically establish a lower bound, proving that under mild conditions, representation distortion inevitably occurs over time. To estimate the... | Bin Lu, Chenghu Zhou, Shiyu Liang, Tingyan Ma, Xiaoying Gan, Xinbing Wang, Yunqiang Zhu |  |
| 1490 |  |  [Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining](https://openreview.net/forum?id=yN4Wv17ss3) |  | 0 | Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities, where they can make good decisions when prompted with interaction trajectories from unseen environments. However, when and how... | Licong Lin, Song Mei, Yu Bai |  |
| 1491 |  |  [On the Hardness of Online Nonconvex Optimization with Single Oracle Feedback](https://openreview.net/forum?id=iZgECfyHXF) |  | 0 | Online nonconvex optimization has been an active area of research recently. Previous studies either considered the global regret with full information about the objective functions, or studied the local regret with window-smoothed objective functions, which required access to unlimited number of... | Yi Zhou, Yingbin Liang, Ziwei Guan |  |
| 1492 |  |  [On the Vulnerability of Adversarially Trained Models Against Two-faced Attacks](https://openreview.net/forum?id=mXpNp8MMr5) |  | 0 | Adversarial robustness is an important standard for measuring the quality of learned models, and adversarial training is an effective strategy for improving the adversarial robustness of models. In this paper, we disclose that adversarially trained models are vulnerable to two-faced attacks, where... | Bo An, Lei Feng, Lue Tao, Shengjie Zhou, Tao Xiang, Yuzhou Cao |  |
| 1493 |  |  [MOFI: Learning Image Representations from Noisy Entity Annotated Images](https://openreview.net/forum?id=QQYpgReSRk) |  | 0 | We present MOFI, Manifold OF Images, a new vision foundation model designed to learn image representations from noisy entity annotated images. MOFI differs from previous work in two key aspects: 1. pre-training data, and 2. training recipe. Regarding data, we introduce a new approach to... | Aleksei Timofeev, Bowen Zhang, Chen Chen, Kun Duan, Shuangning Liu, Wentao Wu, Xianzhi Du, Yantao Zheng, Yinfei Yang |  |
| 1494 |  |  [Efficient Integrators for Diffusion Generative Models](https://openreview.net/forum?id=qA4foxO5Gf) |  | 0 | Diffusion models suffer from slow sample generation at inference time. Therefore, developing a principled framework for fast deterministic/stochastic sampling for a broader class of diffusion models is a promising direction. We propose two complementary frameworks for accelerating sample generation... | Kushagra Pandey, Maja Rudolph, Stephan Mandt |  |
| 1495 |  |  [Learning Over Molecular Conformer Ensembles: Datasets and Benchmarks](https://openreview.net/forum?id=NSDszJ2uIV) |  | 0 | Molecular Representation Learning (MRL) has proven impactful in numerous biochemical applications such as drug discovery and enzyme design. While Graph Neural Networks (GNNs) are effective at learning molecular representations from a 2D molecular graph or a single 3D structure, existing works often... | Bozhao Nan, Brock Stenfors, Connor W. Coley, Jatin Chauhan, Jeehyun Hwang, Keir Adams, Olaf Wiest, Olexandr Isayev, Wei Wang, Yanqiao Zhu, Yizhou Sun, Yuanqi Du, Zhen Liu |  |
| 1496 |  |  [Provably Robust Conformal Prediction with Improved Efficiency](https://openreview.net/forum?id=BWAhEjXjeG) |  | 0 | Conformal prediction is a powerful tool to generate uncertainty sets with guaranteed coverage using any predictive model, under the assumption that the training and test data are i.i.d.. Recently, it has been shown that adversarial examples are able to manipulate conformal methods to construct... | Ge Yan, TsuiWei Weng, Yaniv Romano |  |
| 1497 |  |  [DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text](https://openreview.net/forum?id=Xlayxj2fWp) |  | 0 | Large language models (LLMs) have notably enhanced the fluency and diversity of machine-generated text. However, this progress also presents a significant challenge in detecting the origin of a given text, and current research on detection methods lags behind the rapid evolution of LLMs.... | Haifeng Chen, Linda Ruth Petzold, Wei Cheng, William Yang Wang, Xianjun Yang, Yue Wu |  |
| 1498 |  |  [FedImpro: Measuring and Improving Client Update in Federated Learning](https://openreview.net/forum?id=giU9fYGTND) |  | 0 | Federated Learning (FL) models often experience client drift caused by heterogeneous data, where the distribution of data differs across clients. To address this issue, advanced research primarily focuses on manipulating the existing gradients to achieve more consistent client models. In this... | Bo Han, Shaohuai Shi, Tongliang Liu, Xiaowen Chu, Xinmei Tian, Yonggang Zhang, Zhenheng Tang |  |
| 1499 |  |  [Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis](https://openreview.net/forum?id=p4eG8rCa0b) |  | 0 | Addressing the limitations of text as a source of accurate layout representation in text-conditional diffusion models, many works incorporate additional signals to condition certain attributes within a generated image. Although successful, previous works do not account for the specific localization... | Hansam Cho, Jonghyun Lee, Seoung Bum Kim, Yonghyun Jeong, Young Joon Yoo |  |
| 1500 |  |  [A Unified Framework for Bayesian Optimization under Contextual Uncertainty](https://openreview.net/forum?id=oMNkj4ER7V) |  | 0 | Bayesian optimization under contextual uncertainty (BOCU) is a family of BO problems in which the learner makes a decision prior to observing the context and must manage the risks involved. Distributionally robust BO (DRBO) is a subset of BOCU that affords robustness against context distribution... | Bryan Kian Hsiang Low, ChuanSheng Foo, Daisuke Urano, Richalynn Leong, Sebastian Shenghong Tay |  |
| 1501 |  |  [Free from Bellman Completeness: Trajectory Stitching via Model-based Return-conditioned Supervised Learning](https://openreview.net/forum?id=7zY781bMDO) |  | 0 | Off-policy dynamic programming (DP) techniques such as $Q$-learning have proven to be important in sequential decision-making problems. In the presence of function approximation, however, these techniques often diverge due to the absence of Bellman completeness in the function classes considered, a... | Abhishek Gupta, Chuning Zhu, Qiwen Cui, Runlong Zhou, Simon Shaolei Du, Zhaoyi Zhou |  |
| 1502 |  |  [Diffusion Models for Multi-Task Generative Modeling](https://openreview.net/forum?id=cbv0sBIZh9) |  | 0 | Diffusion-based generative modeling has been achieving state-of-the-art results on various generation tasks. Most diffusion models, however, are limited to a single-generation modeling. Can we generalize diffusion models with the ability of multi-modal generative training for more generalizable... | Belinda Zeng, Benjamin Z. Yao, Bunyamin Sisman, Changyou Chen, Han Ding, Ouye Xie, Son Dinh Tran, Yi Xu |  |
| 1503 |  |  [WebArena: A Realistic Web Environment for Building Autonomous Agents](https://openreview.net/forum?id=oKn9c6ytLx) |  | 0 | With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build... | Abishek Sridhar, Daniel Fried, Frank F. Xu, Graham Neubig, Hao Zhu, Robert Lo, Shuyan Zhou, Tianyue Ou, Uri Alon, Xianyi Cheng, Xuhui Zhou, Yonatan Bisk |  |
| 1504 |  |  [Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers](https://openreview.net/forum?id=ZG3RaNIsO8) |  | 0 | Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of... | Bei Li, Guoqing Liu, Jiang Bian, Junliang Guo, Kaitao Song, Qingyan Guo, Rui Wang, Xu Tan, Yujiu Yang |  |
| 1505 |  |  [Sparse Weight Averaging with Multiple Particles for Iterative Magnitude Pruning](https://openreview.net/forum?id=Y9t7MqZtCR) |  | 0 | Given the ever-increasing size of modern neural networks, the significance of sparse architectures has surged due to their accelerated inference speeds and minimal memory demands. When it comes to global pruning techniques, Iterative Magnitude Pruning (IMP) still stands as a state-of-the-art... | Giung Nam, Hyungi Lee, Juho Lee, Moonseok Choi |  |
| 1506 |  |  [WildFusion: Learning 3D-Aware Latent Diffusion Models in View Space](https://openreview.net/forum?id=VdwVOREDZM) |  | 0 | Modern learning-based approaches to 3D-aware image synthesis achieve high photorealism and 3D-consistent viewpoint changes for the generated images. Existing approaches represent instances in a shared canonical space. However, for in-the-wild datasets a shared canonical system can be difficult to... | Andreas Geiger, Jun Gao, Karsten Kreis, Katja Schwarz, Sanja Fidler, Seung Wook Kim |  |
| 1507 |  |  [Causal Inference with Conditional Front-Door Adjustment and Identifiable Variational Autoencoder](https://openreview.net/forum?id=wFf9m4v7oC) |  | 0 | An essential and challenging problem in causal inference is causal effect estimation from observational data. The problem becomes more difficult with the presence of unobserved confounding variables. The front-door adjustment is an approach for dealing with unobserved confounding variables.... | Debo Cheng, Jiuyong Li, Jixue Liu, Kui Yu, Lin Liu, Ziqi Xu |  |
| 1508 |  |  [Diffusion Sampling with Momentum for Mitigating Divergence Artifacts](https://openreview.net/forum?id=HXc5aXeoc8) |  | 0 | Despite the remarkable success of diffusion models in image generation, slow sampling remains a persistent issue. To accelerate the sampling process, prior studies have reformulated diffusion sampling as an ODE/SDE and introduced higher-order numerical methods. However, these methods often produce... | Amit Raj, Pramook Khungurn, Supasorn Suwajanakorn, Suttisak Wizadwongsa, Worameth Chinchuthakun |  |
| 1509 |  |  [Active Test-Time Adaptation: Theoretical Analyses and An Algorithm](https://openreview.net/forum?id=YHUGlwTzFB) |  | 0 | Test-time adaptation (TTA) addresses distribution shifts for streaming test data in unsupervised settings. Currently, most TTA methods can only deal with minor shifts and rely heavily on heuristic and empirical studies. To advance TTA under domain shifts, we propose the novel problem setting of... | Shuiwang Ji, Shurui Gui, Xiner Li |  |
| 1510 |  |  [AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable Diffusion Model](https://openreview.net/forum?id=bxfKIYfHyx) |  | 0 | Aligning agent behaviors with diverse human preferences remains a challenging problem in reinforcement learning (RL), owing to the inherent abstractness and mutability of human preferences. To address these issues, we propose AlignDiff, a novel framework that leverages RLHF to quantify human... | Changjie Fan, Fei Ni, Jianye Hao, Tangjie Lv, Yan Zheng, Yao Mu, Yifu Yuan, Yujing Hu, Zhipeng Hu, Zibin Dong |  |
| 1511 |  |  [Doubly Robust Proximal Causal Learning for Continuous Treatments](https://openreview.net/forum?id=TjGJFkU3xL) |  | 0 | Proximal causal learning is a powerful framework for identifying the causal effect under the existence of unmeasured confounders. Within this framework, the doubly robust (DR) estimator was derived and has shown its effectiveness in estimation, especially when the model assumption is violated.... | Shouyan Wang, Xinwei Sun, Yanwei Fu, Yong Wu |  |
| 1512 |  |  [One-hot Generalized Linear Model for Switching Brain State Discovery](https://openreview.net/forum?id=MREQ0k6qvD) |  | 0 | Exposing meaningful and interpretable neural interactions is critical to understanding neural circuits. Inferred neural interactions from neural signals primarily reflect functional connectivity. In a long experiment, subject animals may experience different stages defined by the experiment,... | Anqi Wu, Chengrui Li, Chris Rodgers, Hannah Choi, Soon Ho Kim |  |
| 1513 |  |  [Neural Auto-designer for Enhanced Quantum Kernels](https://openreview.net/forum?id=8htNAnMSyP) |  | 0 | Quantum kernels hold great promise for offering computational advantages over classical learners, with the effectiveness of these kernels closely tied to the design of the feature map. However, the challenge of designing effective quantum feature maps for real-world datasets, particularly in the... | Cong Lei, Jun Yu, Peng Mi, Tongliang Liu, Yuxuan Du |  |
| 1514 |  |  [On the Parameterization of Second-Order Optimization Effective towards the Infinite Width](https://openreview.net/forum?id=g8sGBSQjYk) |  | 0 | Second-order optimization has been developed to accelerate the training of deep neural networks and it is being applied to increasingly larger-scale models. In this study, towards training on further larger scales, we identify a specific parameterization for second-order optimization that promotes... | Ryo Karakida, Satoki Ishikawa |  |
| 1515 |  |  [The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing](https://openreview.net/forum?id=DesYwmUG00) |  | 0 | We present a unified probabilistic formulation for diffusion-based image editing, where a latent variable is edited in a task-specific manner and generally deviates from the corresponding marginal distribution induced by the original stochastic or ordinary differential equation (SDE or ODE).... | Cheng Lu, Chenyu Zheng, Chongxuan Li, Hanzhong Allan Guo, Shen Nie, Yuhao Zhou |  |
| 1516 |  |  [CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules](https://openreview.net/forum?id=vYhglxSj8j) |  | 0 | Large Language Models (LLMs) have already become quite proficient at solving simpler programming tasks like those in HumanEval or MBPP benchmarks. However, solving more complex and competitive programming tasks is still quite challenging for these models - possibly due to their tendency to generate... | Akash Gokul, Amrita Saha, Doyen Sahoo, Hailin Chen, Hung Le, Shafiq Joty |  |
| 1517 |  |  [Towards Robust Multi-Modal Reasoning via Model Selection](https://openreview.net/forum?id=KTf4DGAzus) |  | 0 | The reasoning capabilities of LLM (Large Language Model) are widely acknowledged in recent research, inspiring studies on tool learning and autonomous agents. LLM serves as the \`\`brain'' of the agent, orchestrating multiple tools for collaborative multi-step task solving. Unlike methods invoking... | Rongxue Li, Tao Lin, Wei Ji, Xiangyan Liu |  |
| 1518 |  |  [DistillSpec: Improving Speculative Decoding via Knowledge Distillation](https://openreview.net/forum?id=rsY6J3ZaTF) |  | 0 | Speculative decoding~(SD) accelerates large language model inference by employing a faster {\em draft} model for generating multiple tokens, which are then verified in parallel by the larger {\em target} model, resulting in the text generated according to the target model distribution. However,... | Aditya Krishna Menon, Afshin Rostamizadeh, Ankit Singh Rawat, JeanFrançois Kagy, Kaifeng Lyu, Rishabh Agarwal, Sanjiv Kumar, Yongchao Zhou |  |
| 1519 |  |  [Boosting the Adversarial Robustness of Graph Neural Networks: An OOD Perspective](https://openreview.net/forum?id=DCDT918ZkI) |  | 0 | Current defenses against graph attacks often rely on certain properties to eliminate structural perturbations by identifying adversarial edges from normal edges. However, this dependence makes defenses vulnerable to adaptive (white-box) attacks from adversaries with the same knowledge. Adversarial... | Jin Wang, Kuan Li, Minhao Cheng, Qing He, Xiang Ao, Yang Liu, Yiwen Chen |  |
| 1520 |  |  [DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning](https://openreview.net/forum?id=qAoxvePSlq) |  | 0 | Recent advances in natural language processing, primarily propelled by Large Language Models (LLMs), have showcased their remarkable capabilities grounded in in-context learning. A promising avenue for guiding LLMs in intricate reasoning tasks involves the utilization of intermediate reasoning... | Chengming Li, Chuanyang Zheng, Enze Xie, Haiming Wang, Jing Tang, Jing Xiong, Qingxing Cao, Xiaodan Liang, Xiongwei Han, Yichun Yin, Zhicheng Yang, Zhijiang Guo, Zixuan Li |  |
| 1521 |  |  [LRR: Language-Driven Resamplable Continuous Representation against Adversarial Tracking Attacks](https://openreview.net/forum?id=3qo1pJHabg) |  | 0 | Visual object tracking plays a critical role in visual-based autonomous systems, as it aims to estimate the position and size of the object of interest within a live video. Despite significant progress made in this field, state-of-the-art (SOTA) trackers often fail when faced with adversarial... | Di Lin, Felix JuefeiXu, Jianjun Zhao, Jianlang Chen, Lei Ma, Qing Guo, Wei Feng, Xuhong Ren |  |
| 1522 |  |  [SPTNet: An Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning](https://openreview.net/forum?id=3QLkwU40EE) |  | 0 | Generalized Category Discovery (GCD) aims to classify unlabelled images from both ‘seen’ and ‘unseen’ classes by transferring knowledge from a set of labelled ‘seen’ class images. A key theme in existing GCD approaches is adapting large-scale pre-trained models for the GCD task. An alternate... | Hongjun Wang, Kai Han, Sagar Vaze |  |
| 1523 |  |  [Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning](https://openreview.net/forum?id=09iOdaeOzp) |  | 0 | The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged moderate-sized large language models (LLMs) highlights the potential of building smaller yet powerful LLMs. Regardless, the cost of training such models from scratch on trillions of tokens remains high. In this work, we... | Danqi Chen, Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng |  |
| 1524 |  |  [Rethinking Complex Queries on Knowledge Graphs with Neural Link Predictors](https://openreview.net/forum?id=1BmveEMNbG) |  | 0 | Reasoning on knowledge graphs is a challenging task because it utilizes observed information to predict the missing one. Particularly, answering complex queries based on first-order logic is one of the crucial tasks to verify learning to reason abilities for generalization and composition.... | Hang Yin, Yangqiu Song, Zihao Wang |  |
| 1525 |  |  [Shadow Cones: A Generalized Framework for Partial Order Embeddings](https://openreview.net/forum?id=zbKcFZ6Dbp) |  | 0 | Hyperbolic space has proven to be well-suited for capturing hierarchical relations in data, such as trees and directed acyclic graphs. Prior work introduced the concept of entailment cones, which uses partial orders defined by nested cones in the Poincar\'e ball to model hierarchies. Here, we... | Albert Tseng, Christopher De Sa, Tao Yu, Toni J. B. Liu |  |
| 1526 |  |  [Neural Active Learning Beyond Bandits](https://openreview.net/forum?id=g1S72T3FGc) |  | 0 | We study both stream-based and pool-based active learning with neural network approximations. A recent line of works proposed bandit-based approaches that transformed active learning into a bandit problem, achieving both theoretical and empirical success. However, the performance and computational... | Hanghang Tong, Ishika Agarwal, Jingrui He, Kommy Weldemariam, Yada Zhu, Yikun Ban, Ziwei Wu |  |
| 1527 |  |  [From Graphs to Hypergraphs: Hypergraph Projection and its Reconstruction](https://openreview.net/forum?id=qwYKE3VB2h) |  | 0 | We study the implications of the modeling choice to use a graph, instead of a hypergraph, to represent real-world interconnected systems whose constituent relationships are of higher order by nature. Such a modeling choice typically involves an underlying projection process that maps the original... | Jon M. Kleinberg, Yanbang Wang |  |
| 1528 |  |  [Synergistic Patch Pruning for Vision Transformer: Unifying Intra- & Inter-Layer Patch Importance](https://openreview.net/forum?id=COO51g41Q4) |  | 0 | The Vision Transformer (ViT) has emerged as a powerful architecture for various computer vision tasks. Nonetheless, this comes with substantially heavier computational costs than Convolutional Neural Networks (CNNs). The attention mechanism in ViTs, which integrates information from different image... | Lan Wei, Nikolaos M. Freris, Yuyao Zhang |  |
| 1529 |  |  [On the Effect of Batch Size in Byzantine-Robust Distributed Learning](https://openreview.net/forum?id=wriKDQqiOQ) |  | 0 | Byzantine-robust distributed learning (BRDL), in which computing devices are likely to behave abnormally due to accidental failures or malicious attacks, has recently become a hot research topic. However, even in the independent and identically distributed (i.i.d.) case, existing BRDL methods will... | ChangWei Shi, WuJun Li, YiRui Yang |  |
| 1530 |  |  [Large Language Model Cascades with Mixture of Thought Representations for Cost-Efficient Reasoning](https://openreview.net/forum?id=6okaSfANzh) |  | 0 | Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM "cascade" to save the cost of using LLMs,... | Jie Zhao, Liang Du, Min Zhang, Murong Yue, Ziyu Yao |  |
| 1531 |  |  [Unpaired Image-to-Image Translation via Neural Schrödinger Bridge](https://openreview.net/forum?id=uQBW7ELXfO) |  | 0 | Diffusion models are a powerful class of generative models which simulate stochastic differential equations (SDEs) to generate data from noise. While diffusion models have achieved remarkable progress, they have limitations in unpaired image-to-image (I2I) translation tasks due to the Gaussian... | Beomsu Kim, Gihyun Kwon, Jong Chul Ye, Kwanyoung Kim |  |
| 1532 |  |  [Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages](https://openreview.net/forum?id=0aR1s9YxoL) |  | 0 | Plasticity, the ability of a neural network to evolve with new data, is crucial for high-performance and sample-efficient visual reinforcement learning (VRL). Although methods like resetting and regularization can potentially mitigate plasticity loss, the influences of various components within the... | Dacheng Tao, Guozheng Ma, Li Shen, Lu Li, Sen Zhang, Xueqian Wang, Yixin Chen, Zhen Wang, Zixuan Liu |  |
| 1533 |  |  [A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models](https://openreview.net/forum?id=farT6XXntP) |  | 0 | Generative Large Language Models (LLMs) have achieved remarkable advancements in various NLP tasks. However, these advances have not been reflected in the translation task, especially those with moderate model sizes (i.e., 7B or 13B parameters), which still lag behind conventional supervised... | Amr Sharaf, Hany Hassan Awadalla, Haoran Xu, Young Jin Kim |  |
| 1534 |  |  [Ins-DetCLIP: Aligning Detection Model to Follow Human-Language Instruction](https://openreview.net/forum?id=M0MF4t3hE9) |  | 0 | This paper introduces Instruction-oriented Object Detection (IOD), a new task that enhances human-computer interaction by enabling object detectors to understand user instructions and locate relevant objects. Unlike traditional open-vocabulary object detection tasks that rely on users providing a... | Hang Xu, Jianhua Han, Lewei Yao, Renjie Pi, Wei Zhang, Xiaodan Liang |  |
| 1535 |  |  [Scaling Supervised Local Learning with Augmented Auxiliary Networks](https://openreview.net/forum?id=Qbf1hy8b7m) |  | 0 | Deep neural networks are typically trained using global error signals that backpropagate (BP) end-to-end, which is not only biologically implausible but also suffers from the update locking problem and requires huge memory consumption. Local learning, which updates each layer independently with a... | Chenxiang Ma, Chenyang Si, Jibin Wu, Kay Chen Tan |  |
| 1536 |  |  [Elucidating the design space of classifier-guided diffusion generation](https://openreview.net/forum?id=9DXXMXnIGm) |  | 0 | Guidance in conditional diffusion generation is of great importance for sample quality and controllability. However, existing guidance schemes are to be desired. On one hand, mainstream methods such as classifier guidance and classifier-free guidance both require extra training with labeled data,... | Jiacheng Sun, Jiajun Ma, Tianyang Hu, Wenjia Wang |  |
| 1537 |  |  [Teach LLMs to Phish: Stealing Private Information from Language Models](https://openreview.net/forum?id=qo21ZlfNu6) |  | 0 | When large language models are trained on private data, it can be a \textit{significant} privacy risk for them to memorize and regurgitate sensitive information. In this work, we propose a new \emph{practical} data extraction attack that we call \`\`neural phishing''. This attack enables an... | Ashwinee Panda, Christopher A. ChoquetteChoo, Prateek Mittal, Yaoqing Yang, Zhengming Zhang |  |
| 1538 |  |  [Robot Fleet Learning via Policy Merging](https://openreview.net/forum?id=IL71c1z7et) |  | 0 | Fleets of robots ingest massive amounts of heterogeneous streaming data silos generated by interacting with their environments, far more than what can be stored or transmitted with ease. At the same time, teams of robots should co-acquire diverse skills through their heterogeneous experiences in... | Allan Zhou, Kaiqing Zhang, Lirui Wang, Max Simchowitz, Russ Tedrake |  |
| 1539 |  |  [InfoCon: Concept Discovery with Generative and Discriminative Informativeness](https://openreview.net/forum?id=g6eCbercEc) |  | 0 | We focus on the self-supervised discovery of manipulation concepts that can be adapted and reassembled to address various robotic tasks. We propose that the decision to conceptualize a physical procedure should not depend on how we name it (semantics) but rather on the significance of the... | Qian Luo, Ruizhe Liu, Yanchao Yang |  |
| 1540 |  |  [Sample-efficient Learning of Infinite-horizon Average-reward MDPs with General Function Approximation](https://openreview.net/forum?id=fq1wNrC2ai) |  | 0 | We study infinite-horizon average-reward Markov decision processes (AMDPs) in the context of general function approximation. Specifically, we propose a novel algorithmic framework named Local-fitted Optimization with OPtimism (LOOP), which incorporates both model-based and value-based incarnations.... | Han Zhong, Jianliang He, Zhuoran Yang |  |
| 1541 |  |  [Revisit and Outstrip Entity Alignment: A Perspective of Generative Models](https://openreview.net/forum?id=z3dfuRcGAK) |  | 0 | Recent embedding-based methods have achieved great successes in exploiting entity alignment from knowledge graph (KG) embeddings of multiple modalities. In this paper, we study embedding-based entity alignment (EEA) from a perspective of generative models. We show that EEA shares similarities with... | Huajun Chen, Jiaoyan Chen, Lingbing Guo, Wen Zhang, Yin Fang, Zhuo Chen |  |
| 1542 |  |  [ImplicitSLIM and How it Improves Embedding-based Collaborative Filtering](https://openreview.net/forum?id=6vF0ZJGor4) |  | 0 | We present ImplicitSLIM, a novel unsupervised learning approach for sparse high-dimensional data, with applications to collaborative filtering. Sparse linear methods (SLIM) and their variations show outstanding performance, but they are memory-intensive and hard to scale. ImplicitSLIM improves... | Ilya Shenbin, Sergey I. Nikolenko |  |
| 1543 |  |  [Prompt Learning with Quaternion Networks](https://openreview.net/forum?id=dKlxDx2SoS) |  | 0 | Multimodal pre-trained models have shown impressive potential in enhancing performance on downstream tasks. However, existing fusion strategies for modalities primarily rely on explicit interaction structures that fail to capture the diverse aspects and patterns inherent in input data. This yields... | Boya Shi, Chao Ma, Shuai Jia, Zhengqin Xu |  |
| 1544 |  |  [Meta-Learning Priors Using Unrolled Proximal Networks](https://openreview.net/forum?id=b3Cu426njo) |  | 0 | Relying on prior knowledge accumulated from related tasks, meta-learning offers a powerful approach to learning a novel task from a limited number of training data. Recent approaches use a family of prior probability density functions or recurrent neural network models, whose parameters can be... | Georgios B. Giannakis, Yilang Zhang |  |
| 1545 |  |  [Bidirectional Temporal Diffusion Model for Temporally Consistent Human Animation](https://openreview.net/forum?id=yQDFsuG9HP) |  | 0 | We introduce a method to generate temporally coherent human animation from a single image, a video, or a random noise. This problem has been formulated as modeling of an auto-regressive generation, i.e., to regress past frames to decode future frames. However, such unidirectional generation is... | Hwasup Lim, Jae Shin Yoon, Jungeun Lee, Sanghun Kim, Tserendorj Adiya |  |
| 1546 |  |  [Uncertainty Quantification via Stable Distribution Propagation](https://openreview.net/forum?id=cZttUMTiPL) |  | 0 | We propose a new approach for propagating stable probability distributions through neural networks. Our method is based on local linearization, which we show to be an optimal approximation in terms of total variation distance for the ReLU non-linearity. This allows propagating Gaussian and Cauchy... | Aashwin Ananda Mishra, Christian Borgelt, Felix Petersen, Hilde Kuehne, Mikhail Yurochkin, Oliver Deussen |  |
| 1547 |  |  [MOFDiff: Coarse-grained Diffusion for Metal-Organic Framework Design](https://openreview.net/forum?id=0VBsoluxR2) |  | 0 | Metal-organic frameworks (MOFs) are of immense interest in applications such as gas storage and carbon capture due to their exceptional porosity and tunable chemistry. Their modular nature has enabled the use of template-based methods to generate hypothetical MOFs by combining molecular building... | Andrew S. Rosen, Jake Smith, Tian Xie, Tommi S. Jaakkola, Xiang Fu |  |
| 1548 |  |  [FedWon: Triumphing Multi-domain Federated Learning Without Normalization](https://openreview.net/forum?id=hAYHmV1gM8) |  | 0 | Federated learning (FL) enhances data privacy with collaborative in-situ training on decentralized clients. Nevertheless, FL encounters challenges due to non-independent and identically distributed (non-i.i.d) data, leading to potential performance degradation and hindered convergence. While prior... | Lingjuan Lyu, Weiming Zhuang |  |
| 1549 |  |  [TAIL: Task-specific Adapters for Imitation Learning with Large Pretrained Models](https://openreview.net/forum?id=RRayv1ZPN3) |  | 0 | The full potential of large pretrained models remains largely untapped in control domains like robotics. This is mainly because of the scarcity of data and the computational challenges associated with training or fine-tuning these large models for such applications. Prior work mainly emphasizes... | Ding Zhao, Jesse Zhang, Kavosh Asadi, Rasool Fakoor, Shoham Sabach, Yao Liu, Zuxin Liu |  |
| 1550 |  |  [L2P-MIP: Learning to Presolve for Mixed Integer Programming](https://openreview.net/forum?id=McfYbKnpT8) |  | 0 | Modern solvers for solving mixed integer programming (MIP) often rely on the branch-and-bound (B&B) algorithm which could be of high time complexity, and presolving techniques are well designed to simplify the instance as pre-processing before B&B. However, such presolvers in existing literature or... | Bowen Pang, Chang Liu, Haobo Ma, Jia Zeng, Junchi Yan, Weilin Luo, Xijun Li, Zhichen Dong |  |
| 1551 |  |  [Exploring the cloud of feature interaction scores in a Rashomon set](https://openreview.net/forum?id=EPNEazJoAg) |  | 0 | Interactions among features are central to understanding the behavior of machine learning models. Recent research has made significant strides in detecting and quantifying feature interactions in single predictive models. However, we argue that the feature interactions extracted from a single... | Amanda S. Barnard, Quanling Deng, Rong Wang, Sichao Li |  |
| 1552 |  |  [Long-Short-Range Message-Passing: A Physics-Informed Framework to Capture Non-Local Interaction for Scalable Molecular Dynamics Simulation](https://openreview.net/forum?id=rvDQtdMnOl) |  | 0 | Computational simulation of chemical and biological systems using \*ab initio\* molecular dynamics has been a challenge over decades. Researchers have attempted to address the problem with machine learning and fragmentation-based methods. However, the two approaches fail to give a satisfactory... | Bin Shao, Han Yang, Jia Zhang, Lin Huang, TieYan Liu, Tong Wang, Xinran Wei, Yunyang Li, Yusong Wang, Zun Wang |  |
| 1553 |  |  [A Study of Bayesian Neural Network Surrogates for Bayesian Optimization](https://openreview.net/forum?id=SA19ijj44B) |  | 0 | Bayesian optimization is a highly efficient approach to optimizing objective functions which are expensive to query. These objectives are typically represented by Gaussian process (GP) surrogate models which are easy to optimize and support exact inference. While standard GP surrogates have been... | Andrew Gordon Wilson, Tim G. J. Rudner, Yucen Lily Li |  |
| 1554 |  |  [Large Language Models as Analogical Reasoners](https://openreview.net/forum?id=AgDICX1h50) |  | 0 | Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, analogical prompting, designed to automatically guide the reasoning... | Denny Zhou, Ed H. Chi, Jure Leskovec, Michihiro Yasunaga, Panupong Pasupat, Percy Liang, Xinyun Chen, Yujia Li |  |
| 1555 |  |  [Multi-modal Gaussian Process Variational Autoencoders for Neural and Behavioral Data](https://openreview.net/forum?id=aGH43rjoe4) |  | 0 | Characterizing the relationship between neural population activity and behavioral data is a central goal of neuroscience. While latent variable models (LVMs) are successful in describing high-dimensional data, they are typically only designed for a single type of data, making it difficult to... | Evan Schaffer, Mikio Christian Aoi, Rabia Gondur, Stephen L. Keeley, Usama Bin Sikandar |  |
| 1556 |  |  [UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling](https://openreview.net/forum?id=f5H8WGLQm5) |  | 0 | Large-scale vision-language pre-trained models have shown promising transferability to various downstream tasks. As the size of these foundation models and the number of downstream tasks grow, the standard full fine-tuning paradigm becomes unsustainable due to heavy computational and storage costs.... | Guoxing Yang, Haoyu Lu, Masayoshi Tomizuka, Mingyu Ding, Wei Zhan, Yuqi Huo, Zhiwu Lu |  |
| 1557 |  |  [Annealing Self-Distillation Rectification Improves Adversarial Training](https://openreview.net/forum?id=eT6oLkm1cm) |  | 0 | In standard adversarial training, models are optimized to fit invariant one-hot labels for adversarial data when the perturbations are within allowable budgets. However, the overconfident target harms generalization and causes the problem of robust overfitting. To address this issue and enhance... | HungJui Wang, ShangTse Chen, YuYu Wu |  |
| 1558 |  |  [Don't Judge by the Look: Towards Motion Coherent Video Representation](https://openreview.net/forum?id=RIcYTbpO38) |  | 0 | Current training pipelines in object recognition neglect Hue Jittering when doing data augmentation as it not only brings appearance changes that are detrimental to classification, but also the implementation is inefficient in practice. In this study, we investigate the effect of hue variance in... | Huan Wang, Yitian Zhang, Yizhou Wang, Yue Bai, Yun Fu |  |
| 1559 |  |  [Time-LLM: Time Series Forecasting by Reprogramming Large Language Models](https://openreview.net/forum?id=Unb5CVPtae) |  | 0 | Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized,... | James Y. Zhang, Lintao Ma, Ming Jin, PinYu Chen, Qingsong Wen, Shirui Pan, Shiyu Wang, Xiaoming Shi, YuanFang Li, Yuxuan Liang, Zhixuan Chu |  |
| 1560 |  |  [BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction](https://openreview.net/forum?id=DmD1wboID9) |  | 0 | As a novel and effective fine-tuning paradigm based on large-scale pre-trained language models (PLMs), prompt-tuning aims to reduce the gap between downstream tasks and pre-training objectives. While prompt-tuning has yielded continuous advancements in various tasks, such an approach still remains... | Changwen Zheng, Fei Song, Fuchun Sun, Hui Xiong, Jiangmeng Li, Wenwen Qiang, Yifan Jin |  |
| 1561 |  |  [Offline Data Enhanced On-Policy Policy Gradient with Provable Guarantees](https://openreview.net/forum?id=RMgqvQGTwH) |  | 0 | Hybrid RL is the setting where an RL agent has access to both offline data and online data by interacting with the real-world environment. In this work, we propose a new hybrid RL algorithm that combines an on-policy actor-critic method with offline data. On-policy methods such as policy gradient... | Ayush Sekhari, Wen Sun, Yifei Zhou, Yuda Song |  |
| 1562 |  |  [Embarrassingly Simple Dataset Distillation](https://openreview.net/forum?id=PLoWVP7Mjc) |  | 0 | Dataset distillation extracts a small set of synthetic training samples from a large dataset with the goal of achieving competitive performance on test data when trained on this sample. In this work, we tackle dataset distillation at its core by treating it directly as a bilevel optimization... | Julia Kempe, Shanmukha Ramakrishna Vedantam, Yunzhen Feng |  |
| 1563 |  |  [RDesign: Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design](https://openreview.net/forum?id=RemfXx7ebP) |  | 0 | While artificial intelligence has made remarkable strides in revealing the relationship between biological macromolecules' primary sequence and tertiary structure, designing RNA sequences based on specified tertiary structures remains challenging. Though existing approaches in protein design have... | Bozhen Hu, Cheng Tan, Siyuan Li, Stan Z. Li, Yijie Zhang, Zhangyang Gao, Zicheng Liu |  |
| 1564 |  |  [Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation](https://openreview.net/forum?id=sGVmr7KHfn) |  | 0 | Universal domain adaptation aims to align the classes and reduce the feature gap between the same category of the source and target domains. The target private category is set as the unknown class during the adaptation process, as it is not included in the source domain. However, most existing... | Tao Zhou, Xinghong Liu, Yi Zhou, Yuxiang Lai |  |
| 1565 |  |  [Large Language Models as Tool Makers](https://openreview.net/forum?id=qV83K9d5WB) |  | 0 | Recent research has highlighted the potential of large language models (LLMs) to improve their problem-solving capabilities with the aid of suitable external tools. In our work, we further advance this concept by introducing a closed- loop framework, referred to as LLMs A s Tool Makers (LATM),... | Denny Zhou, Tengyu Ma, Tianle Cai, Xinyun Chen, Xuezhi Wang |  |
| 1566 |  |  [CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing](https://openreview.net/forum?id=Sx038qxjek) |  | 0 | Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize... | Nan Duan, Weizhu Chen, Yelong Shen, Yeyun Gong, Yujiu Yang, Zhibin Gou, Zhihong Shao |  |
| 1567 |  |  [Geometrically Aligned Transfer Encoder for Inductive Transfer in Regression Tasks](https://openreview.net/forum?id=3z60EWfh1p) |  | 0 | Transfer learning is a crucial technique for handling a small amount of data that is potentially related to other abundant data. However, most of the existing methods are focused on classification tasks using images and language datasets. Therefore, in order to expand the transfer learning scheme... | DaeWoong Jeong, Sehui Han, Sumin Lee, Sung Moon Ko, Woohyung Lim |  |
| 1568 |  |  [Zero and Few-shot Semantic Parsing with Ambiguous Inputs](https://openreview.net/forum?id=qL9gogRepu) |  | 0 | Despite the frequent challenges posed by ambiguity when representing meaning via natural language, it is often ignored or deliberately removed in tasks mapping language to formally-designed representations, which generally assume a one-to-one mapping between linguistic and formal representations.... | Benjamin Van Durme, Elias StengelEskin, Kyle Rawlins |  |
| 1569 |  |  [Graph Generation with K2-trees](https://openreview.net/forum?id=RIEW6M9YoV) |  | 0 | Generating graphs from a target distribution is a significant challenge across many domains, including drug discovery and social network analysis. In this work, we introduce a novel graph generation method leveraging $K^2$ representation, originally designed for lossless graph compression. The... | Dongwoo Kim, Sungsoo Ahn, Yunhui Jang |  |
| 1570 |  |  [Manifold Preserving Guided Diffusion](https://openreview.net/forum?id=o3BxOLoxm1) |  | 0 | Despite the recent advancements, conditional image generation still faces challenges of cost, generalizability, and the need for task-specific training. In this paper, we propose Manifold Preserving Guided Diffusion (MPGD), a training-free conditional generation framework that leverages pretrained... | ChiehHsin Lai, Dongjun Kim, J. Zico Kolter, Naoki Murata, Ruslan Salakhutdinov, Stefano Ermon, Toshimitsu Uesaka, WeiHsiang Liao, Yuhta Takida, Yuki Mitsufuji, Yutong He |  |
| 1571 |  |  [Neural Common Neighbor with Completion for Link Prediction](https://openreview.net/forum?id=sNFLN3itAd) |  | 0 | In this work, we propose a novel link prediction model and further boost it by studying graph incompleteness. First, We introduce MPNN-then-SF, an innovative architecture leveraging structural feature (SF) to guide MPNN's representation pooling, with its implementation, namely Neural Common... | Haotong Yang, Muhan Zhang, Xiyuan Wang |  |
| 1572 |  |  [Latent Intuitive Physics: Learning to Transfer Hidden Physics from A 3D Video](https://openreview.net/forum?id=WZu4gUGN13) |  | 0 | We introduce latent intuitive physics, a transfer learning framework for physics simulation that can infer hidden properties of fluids from a single 3D video and simulate the observed fluid in novel scenes. Our key insight is to use latent features drawn from a learnable prior distribution... | Haochen Yuan, Huayu Deng, Xiangming Zhu, Xiaokang Yang, Yunbo Wang |  |
| 1573 |  |  [Privacy-Preserving In-Context Learning for Large Language Models](https://openreview.net/forum?id=x4OPJ7lHVU) |  | 0 | In-context learning (ICL) is an important capability of Large Language Models (LLMs), enabling these models to dynamically adapt based on specific, in-context exemplars, thereby improving accuracy and relevance. However, LLM's responses may leak the sensitive private information contained in... | Ashwinee Panda, Jiachen T. Wang, Prateek Mittal, Tong Wu |  |
| 1574 |  |  [Masked Distillation Advances Self-Supervised Transformer Architecture Search](https://openreview.net/forum?id=LUpC8KTvdV) |  | 0 | Transformer architecture search (TAS) has achieved remarkable progress in automating the neural architecture design process of vision transformers. Recent TAS advancements have discovered outstanding transformer architectures while saving tremendous labor from human experts. However, it is still... | Caixia Yan, Lina Yao, Minnan Luo, Qinghua Zheng, Xiaojun Chang, Zhihui Li |  |
| 1575 |  |  [Adaptive Self-training Framework for Fine-grained Scene Graph Generation](https://openreview.net/forum?id=WipsLtH77t) |  | 0 | Scene graph generation (SGG) models have suffered from inherent problems regarding the benchmark datasets such as the long-tailed predicate distribution and missing annotation problems. In this work, we aim to alleviate the long-tailed problem of SGG by utilizing unannotated triplets. To this end,... | Chanyoung Park, Donghyun Kim, Jinyoung Moon, Kanghoon Yoon, Kibum Kim, Yeonjun In |  |
| 1576 |  |  [Exploring Effective Stimulus Encoding via Vision System Modeling for Visual Prostheses](https://openreview.net/forum?id=cKAUvMePUN) |  | 0 | Visual prostheses are potential devices to restore vision for blind people, which highly depends on the quality of stimulation patterns of the implanted electrode array. However, existing processing frameworks prioritize the generation of stimulation while disregarding the potential impact of... | Chaoming Fang, Chuanqing Wang, Di Wu, Jie Yang, Mohamad Sawan |  |
| 1577 |  |  [Towards Foundation Models for Knowledge Graph Reasoning](https://openreview.net/forum?id=jVEoydFOl9) |  | 0 | Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap.... | Hesham Mostafa, Jian Tang, Mikhail Galkin, Xinyu Yuan, Zhaocheng Zhu |  |
| 1578 |  |  [COCO-Periph: Bridging the Gap Between Human and Machine Perception in the Periphery](https://openreview.net/forum?id=MiRPBbQNHv) |  | 0 | Evaluating deep neural networks (DNNs) as models of human perception has given rich insights into both human visual processing and representational properties of DNNs. We extend this work by analyzing how well DNNs perform compared to humans when constrained by peripheral vision -- which limits... | Anne Harrington, Ayush Tewari, Mark Hamilton, Ruth Rosenholtz, Simon Stent, Vasha DuTell, William T. Freeman |  |
| 1579 |  |  [Towards Non-Asymptotic Convergence for Diffusion-Based Generative Models](https://openreview.net/forum?id=4VGEeER6W9) |  | 0 | Diffusion models, which convert noise into new data instances by learning to reverse a Markov diffusion process, have become a cornerstone in contemporary generative modeling. While their practical power has now been widely recognized, the theoretical underpinnings remain far from mature. In this... | Gen Li, Yuejie Chi, Yuting Wei, Yuxin Chen |  |
| 1580 |  |  [More is Better: when Infinite Overparameterization is Optimal and Overfitting is Obligatory](https://openreview.net/forum?id=OdpIjS0vkO) |  | 0 | In our era of enormous neural networks, empirical progress has been driven by the philosophy that \*more is better.\* Recent deep learning practice has found repeatedly that larger model size, more data, and more computation (resulting in lower training loss) optimizing to near-interpolation... | Dhruva Karkada, James B. Simon, Mikhail Belkin, Nikhil Ghosh |  |
| 1581 |  |  [SALMON: Self-Alignment with Instructable Reward Models](https://openreview.net/forum?id=xJbsmB8UMx) |  | 0 | Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. However, a significant limitation of such an approach is its dependency on high-quality human annotations, making... | Chuang Gan, David Daniel Cox, Hongxin Zhang, Qinhong Zhou, Yikang Shen, Yiming Yang, Zhenfang Chen, Zhiqing Sun |  |
| 1582 |  |  [ControlVideo: Training-free Controllable Text-to-video Generation](https://openreview.net/forum?id=5a79AqFr0c) |  | 0 | Text-driven diffusion models have unlocked unprecedented abilities in image generation, whereas their video counterpart lags behind due to the excessive training cost. To avert the training burden, we propose a training-free ControlVideo to produce high-quality videos based on the provided text... | Dongsheng Jiang, Qi Tian, Wangmeng Zuo, Xiaopeng Zhang, Yabo Zhang, Yuxiang Wei |  |
| 1583 |  |  [RETSim: Resilient and Efficient Text Similarity](https://openreview.net/forum?id=23b9KSNQTX) |  | 0 | This paper introduces RETSim (Resilient and Efficient Text Similarity), a lightweight, multilingual deep learning model trained to produce robust metric embeddings for near-duplicate text retrieval, clustering, and dataset deduplication tasks. We demonstrate that RETSim is significantly more robust... | Aysegul Bumin, Elie Bursztein, Marina Zhang, Owen S. Vallis, Tanay Vakharia |  |
| 1584 |  |  [KW-Design: Pushing the Limit of Protein Design via Knowledge Refinement](https://openreview.net/forum?id=mpqMVWgqjn) |  | 0 | Recent studies have shown competitive performance in protein inverse folding, while most of them disregard the importance of predictive confidence, fail to cover the vast protein space, and do not incorporate common protein knowledge. Given the great success of pretrained models on diverse... | Cheng Tan, Jun Xia, Siyuan Li, Stan Z. Li, Xingran Chen, Yijie Zhang, Zhangyang Gao |  |
| 1585 |  |  [Fusing Models with Complementary Expertise](https://openreview.net/forum?id=PhMrGCMIRL) |  | 0 | Training AI models that generalize across tasks and domains has long been among the open problems driving AI research. The emergence of Foundation Models made it easier to obtain expert models for a given task, but the heterogeneity of data that may be encountered at test time often means that any... | Eric P. Xing, Felipe Maia Polo, Hongyi Wang, Mikhail Yurochkin, Souvik Kundu, Yuekai Sun |  |
| 1586 |  |  [Magnitude Invariant Parametrizations Improve Hypernetwork Learning](https://openreview.net/forum?id=fJNnerz6iH) |  | 0 | Hypernetworks, neural networks that predict the parameters of another neural network, are powerful models that have been successfully used in diverse applications from image generation to multi-task learning. Unfortunately, existing hypernetworks are often challenging to train. Training typically... | Adrian V. Dalca, John V. Guttag, Jose Javier Gonzalez Ortiz |  |
| 1587 |  |  [A Versatile Causal Discovery Framework to Allow Causally-Related Hidden Variables](https://openreview.net/forum?id=FhQSGhBlqv) |  | 0 | Most existing causal discovery methods rely on the assumption of no latent confounders, limiting their applicability in solving real-life problems. In this paper, we introduce a novel, versatile framework for causal discovery that accommodates the presence of causally-related hidden variables... | Biwei Huang, Ignavier Ng, Kun Zhang, Peter Spirtes, Roberto Legaspi, Songyao Jin, Xiangchen Song, Xinshuai Dong, Yujia Zheng |  |
| 1588 |  |  [Teaching Large Language Models to Self-Debug](https://openreview.net/forum?id=KuPixIqPiq) |  | 0 | Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this... | Denny Zhou, Maxwell Lin, Nathanael Schärli, Xinyun Chen |  |
| 1589 |  |  [Achieving Human Parity in Content-Grounded Datasets Generation](https://openreview.net/forum?id=RjYKTQ0L0W) |  | 0 | The lack of high-quality data for content-grounded generation tasks has been identified as a major obstacle to advancing these tasks. To address this gap, we propose Genie, a novel method for automatically generating high-quality content-grounded data. It consists of three stages: (a) Content... | Asaf Yehudai, Boaz Carmeli, Eyal Shnarch, Leshem Choshen, Nathaniel Mills, Ofir Arviv, Yosi Mass |  |
| 1590 |  |  [Federated Recommendation with Additive Personalization](https://openreview.net/forum?id=xkXdE81mOK) |  | 0 | Building recommendation systems via federated learning (FL) is a new emerging challenge for next-generation Internet service. Existing FL models share item embedding across clients while keeping the user embedding private and local on the client side. However, identical item embedding cannot... | Guodong Long, Tianyi Zhou, Zhiwei Li |  |
| 1591 |  |  [Identifiable Latent Polynomial Causal Models through the Lens of Change](https://openreview.net/forum?id=ia9fKO1Vjq) |  | 0 | Causal representation learning aims to unveil latent high-level causal representations from observed low-level data. One of its primary tasks is to provide reliable assurance of identifying these latent causal models, known as \textit{identifiability}. A recent breakthrough explores identifiability... | Anton van den Hengel, Biwei Huang, Dong Gong, Javen Qinfeng Shi, Kun Zhang, Mingming Gong, Yuhang Liu, Zhen Zhang |  |
| 1592 |  |  [Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models](https://openreview.net/forum?id=kIP0duasBb) |  | 0 | One fascinating aspect of pre-trained vision-language models (VLMs) learning under language supervision is their impressive zero-shot generalization capability. However, this ability is hindered by distribution shifts between the training and testing data. Previous test time adaptation (TTA)... | Linchao Zhu, Shuai Zhao, Xiaohan Wang, Yi Yang |  |
| 1593 |  |  [Future Language Modeling from Temporal Document History](https://openreview.net/forum?id=bRLed9prWC) |  | 0 | Predicting the future is of great interest across many aspects of human activity. Businesses are interested in future trends, traders are interested in future stock prices, and companies are highly interested in future technological breakthroughs. While there are many automated systems for... | Changmao Li, Jeffrey Flanigan |  |
| 1594 |  |  [SemiReward: A General Reward Model for Semi-supervised Learning](https://openreview.net/forum?id=dnqPvUjyRI) |  | 0 | Semi-supervised learning (SSL) has witnessed great progress with various improvements in the self-training framework with pseudo labeling. The main challenge is how to distinguish high-quality pseudo labels against the confirmation bias. However, existing pseudo-label selection strategies are... | Cheng Tan, Fang Wu, Siyuan Li, Stan Z. Li, Weiyang Jin, Zedong Wang, Zicheng Liu |  |
| 1595 |  |  [DATS: Difficulty-Aware Task Sampler for Meta-Learning Physics-Informed Neural Networks](https://openreview.net/forum?id=EvyYFSxdgB) |  | 0 | Advancements in deep learning have led to the development of physics-informed neural networks (PINNs) for solving partial differential equations (PDEs) without being supervised by PDE solutions. While vanilla PINNs require training one network per PDE configuration, recent works have showed the... | Linwei Wang, Maryam Toloubidokhti, Nilesh Kumar, Ruby Shrestha, Ryan Missel, Xiajun Jiang, Yubo Ye |  |
| 1596 |  |  [Modulate Your Spectrum in Self-Supervised Learning](https://openreview.net/forum?id=TKqMmKlmA7) |  | 0 | Whitening loss offers a theoretical guarantee against feature collapse in self-supervised learning (SSL) with joint embedding architectures. Typically, it involves a hard whitening approach, transforming the embedding and applying loss to the whitened output. In this work, we introduce Spectral... | Fahad Khan, Jie Luo, Lei Huang, Rao Muhammad Anwer, Salman Khan, Tengwei Song, Xi Weng, Yunhao Ni |  |
| 1597 |  |  [Improving Intrinsic Exploration by Creating Stationary Objectives](https://openreview.net/forum?id=YbZxT0SON4) |  | 0 | Exploration bonuses in reinforcement learning guide long-horizon exploration by defining custom intrinsic objectives. Count-based methods use the frequency of state visits to derive an exploration bonus. In this paper, we identify that any intrinsic reward function derived from count-based methods... | Glen Berseth, Joshua Romoff, Roger Creus Castanyer |  |
| 1598 |  |  [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://openreview.net/forum?id=mZn2Xyh9Ec) |  | 0 | Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the... | Tri Dao |  |
| 1599 |  |  [LDReg: Local Dimensionality Regularized Self-Supervised Learning](https://openreview.net/forum?id=oZyAqjAjJW) |  | 0 | Representations learned via self-supervised learning (SSL) can be susceptible to dimensional collapse, where the learned representation subspace is of extremely low dimensionality and thus fails to represent the full data distribution and modalities. Dimensional collapse ––– also known as the... | Hanxun Huang, James Bailey, Michael E. Houle, Ricardo J. G. B. Campello, Sarah Monazam Erfani, Xingjun Ma |  |
| 1600 |  |  [Empirical Likelihood for Fair Classification](https://openreview.net/forum?id=GACjMj1MS1) |  | 0 | Machine learning algorithms are commonly being deployed in decision-making systems that have a direct impact on human lives. However, if these algorithms are trained solely to minimize training/test errors, they may inadvertently discriminate against individuals based on their sensitive attributes,... | Pangpang Liu, Yichuan Zhao |  |
| 1601 |  |  [Smooth ECE: Principled Reliability Diagrams via Kernel Smoothing](https://openreview.net/forum?id=XwiA1nDahv) |  | 0 | Calibration measures and reliability diagrams are two fundamental tools for measuring and interpreting the calibration of probabilistic predictors. Calibration measures quantify the degree of miscalibration, and reliability diagrams visualize the structure of this miscalibration. However, the most... | Jaroslaw Blasiok, Preetum Nakkiran |  |
| 1602 |  |  [Probabilistic Adaptation of Black-Box Text-to-Video Models](https://openreview.net/forum?id=pjtIEgscE3) |  | 0 | Large text-to-video models trained on internet-scale data have demonstrated exceptional capabilities in generating high-fidelity videos from arbitrary textual descriptions. However, similar to proprietary language models, large text-to-video models are often black boxes whose weight parameters are... | Bo Dai, Dale Schuurmans, Joshua B. Tenenbaum, Pieter Abbeel, Sherry Yang, Yilun Du |  |
| 1603 |  |  [Horizon-Free Regret for Linear Markov Decision Processes](https://openreview.net/forum?id=SdBApv9iT4) |  | 0 | A recent line of works showed regret bounds in reinforcement learning (RL) can be (nearly) independent of planning horizon, a.k.a. the horizon-free bounds. However, these regret bounds only apply to settings where a polynomial dependency on the size of transition model is allowed, such as tabular... | Jason D. Lee, Simon Shaolei Du, Yuxin Chen, Zihan Zhang |  |
| 1604 |  |  [Faithful Rule Extraction for Differentiable Rule Learning Models](https://openreview.net/forum?id=kBTzlxM2J1) |  | 0 | There is increasing interest in methods for extracting interpretable rules from ML models trained to solve a wide range of tasks over knowledge graphs (KGs), such as KG completion, node classification, question answering and recommendation. Many such approaches, however, lack formal guarantees... | Bernardo Cuenca Grau, David Jaime Tena Cucala, Ian Horrocks, Xiaxia Wang |  |
| 1605 |  |  [Score-based generative models break the curse of dimensionality in learning a family of sub-Gaussian distributions](https://openreview.net/forum?id=wG12xUSqrI) |  | 0 | While score-based generative models (SGMs) have achieved remarkable successes in enormous image generation tasks, their mathematical foundations are still limited. In this paper, we analyze the approximation and generalization of SGMs in learning a family of sub-Gaussian probability distributions.... | Frank Cole, Yulong Lu |  |
| 1606 |  |  [Scalable Diffusion for Materials Generation](https://openreview.net/forum?id=wm4WlHoXpC) |  | 0 | ​​​​Generative models trained on internet-scale data are capable of generating novel and realistic texts, images, and videos. A natural next question is whether these models can advance science, for example by generating novel stable materials. Traditionally, models with explicit structures (e.g.,... | Amil Merchant, Dale Schuurmans, Ekin Dogus Cubuk, Igor Mordatch, KwangHwan Cho, Pieter Abbeel, Sherry Yang |  |
| 1607 |  |  [Towards Optimal Feature-Shaping Methods for Out-of-Distribution Detection](https://openreview.net/forum?id=dm8e7gsH0d) |  | 0 | Feature shaping refers to a family of methods that exhibit state-of-the-art performance for out-of-distribution (OOD) detection. These approaches manipulate the feature representation, typically from the penultimate layer of a pre-trained deep learning model, so as to better differentiate between... | Akshay Asthana, Kartik Gupta, Liang Zheng, Ming Xu, Qinyu Zhao, Stephen Gould |  |
| 1608 |  |  [Guess & Sketch: Language Model Guided Transpilation](https://openreview.net/forum?id=qPFsIbF3V6) |  | 0 | Maintaining legacy software requires many software and systems engineering hours. Assembly code programs, which demand low-level control over the computer machine state and have no variable names, are particularly difficult for humans to analyze. Existing conventional program translators guarantee... | Abdulrahman Mahmoud, Alexander M. Rush, Celine Lee, David Brooks, GuYeon Wei, Michal Kurek, Simone Campanoni, Stephen Chong |  |
| 1609 |  |  [Motif: Intrinsic Motivation from Artificial Intelligence Feedback](https://openreview.net/forum?id=tmBKIecDE9) |  | 0 | Exploring rich environments and evaluating one's actions without prior knowledge is immensely challenging. In this paper, we propose Motif, a general method to interface such prior knowledge from a Large Language Model (LLM) with an agent. Motif is based on the idea of grounding LLMs for... | Amy Zhang, Martin Klissarov, Mikael Henaff, Pascal Vincent, Pierluca D'Oro, PierreLuc Bacon, Roberta Raileanu, Shagun Sodhani |  |
| 1610 |  |  [On Differentially Private Federated Linear Contextual Bandits](https://openreview.net/forum?id=cuAxSHcsSX) |  | 0 | We consider cross-silo federated linear contextual bandit (LCB) problem under differential privacy, where multiple silos interact with their respective local users and communicate via a central server to realize collaboration without sacrificing each user's privacy. We identify three issues in the... | Sayak Ray Chowdhury, Xingyu Zhou |  |
| 1611 |  |  [Generative Human Motion Stylization in Latent Space](https://openreview.net/forum?id=daEqXJ0yZo) |  | 0 | Human motion stylization aims to revise the style of an input motion while keeping its content unaltered. Unlike existing works that operate directly in pose space, we leverage the \textit{latent space} of pretrained autoencoders as a more expressive and robust representation for motion extraction... | Chuan Guo, Juwei Lu, Li Cheng, Peng Dai, Xinxin Zuo, Youliang Yan, Yuxuan Mu |  |
| 1612 |  |  [Neural Neighborhood Search for Multi-agent Path Finding](https://openreview.net/forum?id=2NpAw2QJBY) |  | 0 | Multi-agent path finding (MAPF) is the combinatorial problem of planning optimal collision-avoiding paths for multiple agents, with application to robotics, logistics, and transportation. Though many recent learning-based works have focused on large-scale combinatorial problems by guiding their... | Cathy Wu, Zhongxia Yan |  |
| 1613 |  |  [Teaching Language Models to Hallucinate Less with Synthetic Tasks](https://openreview.net/forum?id=xpw7V0P136) |  | 0 | Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context. However, optimizing to make LLMs hallucinate less is... | Ahmed Hassan Awadallah, Arindam Mitra, Clarisse Simões, Ece Kamar, Erik Jones, Hamid Palangi, Subhabrata Mukherjee, Varun Chandrasekaran |  |
| 1614 |  |  [Enabling Lanuguage Models to Implicitly Learn Self-Improvement](https://openreview.net/forum?id=2tVHNRZuCs) |  | 0 | Large Language Models (LLMs) have demonstrated remarkable capabilities in open-ended text generation tasks. However, the inherent open-ended nature of these tasks implies that there is always room for improvement in the quality of model responses. To address this challenge, various approaches have... | Heng Ji, Hongkun Yu, Le Hou, Tianjian Lu, Yuexin Wu, Yunxuan Li, Ziqi Wang |  |
| 1615 |  |  [ReLoRA: High-Rank Training Through Low-Rank Updates](https://openreview.net/forum?id=DLJznSp6X3) |  | 0 | Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters, the necessity to train overparameterized models remains poorly understood, while training costs grow exponentially. In this paper, we explore parameter-efficient training... | Anna Rumshisky, Namrata Shivagunde, Sherin Muckatira, Vladislav Lialin |  |
| 1616 |  |  [Learning with Language-Guided State Abstractions](https://openreview.net/forum?id=qi5Xa2cOZg) |  | 0 | We describe a framework for using natural language to design state abstractions for imitation learning. Generalizable policy learning in high-dimensional observation spaces is facilitated by well-designed state representations, which can surface important features of an environment and hide... | Andi Peng, Belinda Z. Li, Ilia Sucholutsky, Jacob Andreas, Julie Shah, Theodore R. Sumers, Thomas L. Griffiths |  |
| 1617 |  |  [Multimodal Molecular Pretraining via Modality Blending](https://openreview.net/forum?id=oM7Jbxdk6Z) |  | 0 | Self-supervised learning has recently gained growing interest in molecular modeling for scientific tasks such as AI-assisted drug discovery. Current studies consider leveraging both 2D and 3D molecular structures for representation learning. However, relying on straightforward alignment strategies... | Hao Zhou, Jingjing Liu, Qiying Yu, Shikun Feng, Yanyan Lan, Yudi Zhang, Yuyan Ni |  |
| 1618 |  |  [Understanding when Dynamics-Invariant Data Augmentations Benefit Model-free Reinforcement Learning Updates](https://openreview.net/forum?id=sVEu295o70) |  | 0 | Recently, data augmentation (DA) has emerged as a method for leveraging domain knowledge to inexpensively generate additional data in reinforcement learning (RL) tasks, often yielding substantial improvements in data efficiency. While prior work has demonstrated the utility of incorporating... | Josiah P. Hanna, Nicholas Corrado |  |
| 1619 |  |  [JoMA: Demystifying Multilayer Transformers via Joint Dynamics of MLP and Attention](https://openreview.net/forum?id=LbJqRGNYCf) |  | 0 | We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical framework to understand the training procedure of multilayer Transformer architectures. This is achieved by integrating out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only. JoMA removes... | Beidi Chen, Simon Shaolei Du, Yiping Wang, Yuandong Tian, Zhenyu Zhang |  |
| 1620 |  |  [Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks](https://openreview.net/forum?id=YZrg56G0JV) |  | 0 | Multitask Reinforcement Learning (MTRL) approaches have gained increasing attention for its wide applications in many important Reinforcement Learning (RL) tasks. However, while recent advancements in MTRL theory have focused on the improved statistical efficiency by assuming a shared structure... | Ambuj Tewari, Peter Stone, Runxuan Jiang, Zifan Xu, Ziping Xu |  |
| 1621 |  |  [Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications](https://openreview.net/forum?id=BrjLHbqiYs) |  | 0 | In many machine learning systems that jointly learn from multiple modalities, a core research question is to understand the nature of multimodal interactions: how modalities combine to provide new task-relevant information that was not present in either alone. We study this challenge of interaction... | Alex Wilf, Alexander Obolenskiy, Chun Kai Ling, LouisPhilippe Morency, Paul Pu Liang, Rohan Pandey, Russ Salakhutdinov, Yudong Liu, Yun Cheng |  |
| 1622 |  |  [Adaptive Retrieval and Scalable Indexing for k-NN Search with Cross-Encoders](https://openreview.net/forum?id=1CPta0bfN2) |  | 0 | Cross-encoder (CE) models which compute similarity by jointly encoding a query-item pair perform better than using dot-product with embedding-based models (dual-encoders) at estimating query-item relevance. Existing approaches perform k-NN search with cross-encoders by approximating the CE... | Andrew McCallum, Manzil Zaheer, Nicholas Monath, Nishant Yadav, Rob Fergus |  |
| 1623 |  |  [The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction](https://openreview.net/forum?id=ozX92bu8VA) |  | 0 | Transformer-based Large Language Models (LLMs) have become a fixture in modern machine learning. Correspondingly, significant resources are allocated towards research that aims to further advance this technology, typically resulting in models of increasing size that are trained on increasing... | Dipendra Misra, Jordan T. Ash, Pratyusha Sharma |  |
| 1624 |  |  [DeepZero: Scaling Up Zeroth-Order Optimization for Deep Model Training](https://openreview.net/forum?id=qBWhjsNPEY) |  | 0 | Zeroth-order (ZO) optimization has become a popular technique for solving machine learning (ML) problems when first-order (FO) information is difficult or impossible to obtain. However, the scalability of ZO optimization remains an open problem: Its use has primarily been limited to relatively... | Aochuan Chen, Bhavya Kailkhura, James Diffenderfer, Jiancheng Liu, Jinghan Jia, Konstantinos Parasyris, Sijia Liu, Yihua Zhang, Yimeng Zhang, Zheng Zhang |  |
| 1625 |  |  [Multi-Resolution Diffusion Models for Time Series Forecasting](https://openreview.net/forum?id=mmjnr0G8ZY) |  | 0 | The diffusion model has been successfully used in many computer vision applications, such as text-guided image generation and image-to-image translation. Recently, there have been attempts on extending the diffusion model for time series data. However, these extensions are fairly straightforward... | James T. Kwok, Lifeng Shen, Weiyu Chen |  |
| 1626 |  |  [CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?](https://openreview.net/forum?id=FIGXAxr9E4) |  | 0 | We study data-balancing for mitigating biases in contrastive language-image pretraining (CLIP), identifying areas of strength and limitation. First, we reaffirm prior conclusions that CLIP can inadvertently absorb stereotypes. To counter this, we present a novel algorithm, called Multi-Modal Moment... | Alexander D'Amour, Andreas Peter Steiner, Ibrahim Alabdulmohsin, Priya Goyal, Xiao Wang, Xiaohua Zhai |  |
| 1627 |  |  [On Error Propagation of Diffusion Models](https://openreview.net/forum?id=RtAct1E2zS) |  | 0 | Although diffusion models (DMs) have shown promising performances in a number of tasks (e.g., speech synthesis and image generation), they might suffer from error propagation because of their sequential structure. However, this is not certain because some sequential models, such as Conditional... | Mihaela van der Schaar, Yangming Li |  |
| 1628 |  |  [The Update-Equivalence Framework for Decision-Time Planning](https://openreview.net/forum?id=JXGph215fL) |  | 0 | The process of revising (or constructing) a policy at execution time---known as decision-time planning---has been key to achieving superhuman performance in perfect-information games like chess and Go. A recent line of work has extended decision-time planning to imperfect-information games, leading... | David J. Wu, Gabriele Farina, Hengyuan Hu, J. Zico Kolter, Kevin A. Wang, Noam Brown, Samuel Sokota |  |
| 1629 |  |  [Soft Mixture Denoising: Beyond the Expressive Bottleneck of Diffusion Models](https://openreview.net/forum?id=aaBnFAyW9O) |  | 0 | Because diffusion models have shown impressive performances in a number of tasks, such as image synthesis, there is a trend in recent works to prove (with certain assumptions) that these models have strong approximation capabilities. In this paper, we show that current diffusion models actually... | Boris van Breugel, Mihaela van der Schaar, Yangming Li |  |
| 1630 |  |  [LabelDP-Pro: Learning with Label Differential Privacy via Projections](https://openreview.net/forum?id=JnYaF3vv3G) |  | 0 | Label differentially private (label DP) algorithms seek to preserve the privacy of the labels in a training dataset in settings where the features are known to the adversary. In this work, we study a new family of label DP training algorithms. Unlike most prior label DP algorithms that have been... | Badih Ghazi, Chiyuan Zhang, Pasin Manurangsi, Pritish Kamath, Ravi Kumar, Yangsibo Huang |  |
| 1631 |  |  [AUC-CL: A Batchsize-Robust Framework for Self-Supervised Contrastive Representation Learning](https://openreview.net/forum?id=YgMdDQB09U) |  | 0 | Self-supervised learning through contrastive representations is an emergent and promising avenue, aiming at alleviating the availability of labeled data. Recent research in the field also demonstrates its viability for several downstream tasks, henceforth leading to works that implement the... | Changyou Chen, Kaiyi Ji, Rohan Sharma, Zhiqiang Xu |  |
| 1632 |  |  [Teaching Arithmetic to Small Transformers](https://openreview.net/forum?id=dsUB4bst9S) |  | 0 | Large language models like GPT-4 exhibit emergent capabilities across general-purpose tasks, such as basic arithmetic, when trained on extensive text data, even though these tasks are not explicitly encoded by the unsupervised, next-token prediction objective. This study investigates how even small... | Dimitris Papailiopoulos, Jason D. Lee, Kangwook Lee, Kartik Sreenivasan, Nayoung Lee |  |
| 1633 |  |  [ReMasker: Imputing Tabular Data with Masked Autoencoding](https://openreview.net/forum?id=KI9NqjLVDT) |  | 0 | We present ReMasker, a new method of imputing missing values in tabular data by extending the masked autoencoding framework. Compared with prior work, ReMasker is extremely simple -- besides the missing values (i.e., naturally masked), we randomly "re-mask" another set of values, optimize the... | Luca Melis, Tianyu Du, Ting Wang |  |
| 1634 |  |  [Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions](https://openreview.net/forum?id=0i6Z9N5MLY) |  | 0 | Machine learning approaches relying on such criteria as adversarial robustness or multi-agent settings have raised the need for solving game-theoretic equilibrium problems. Of particular relevance to these applications are methods targeting finite-sum structure, which generically arises in... | Ahmet Alacaoglu, Jelena Diakonikolas, Xufeng Cai |  |
| 1635 |  |  [A Dynamical View of the Question of Why](https://openreview.net/forum?id=lrQlLqQase) |  | 0 | We address causal reasoning in multivariate time series data generated by stochastic processes. Existing approaches are largely restricted to static settings, ignoring the continuity and emission of variations across time. In contrast, we propose a learning paradigm that directly establishes... | Mehdi Fatemi, Sindhu C. M. Gowda |  |
| 1636 |  |  [DrS: Learning Reusable Dense Rewards for Multi-Stage Tasks](https://openreview.net/forum?id=6CZ50WgfCG) |  | 0 | The success of many RL techniques heavily relies on human-engineered dense rewards, which typically demands substantial domain expertise and extensive trial and error. In our work, we propose \*\*DrS\*\* (\*\*D\*\*ense \*\*r\*\*eward learning from \*\*S\*\*tages), a novel approach for learning... | Hao Su, Minghua Liu, Tongzhou Mu |  |
| 1637 |  |  [Fair Classifiers that Abstain without Harm](https://openreview.net/forum?id=jvveGAbkVx) |  | 0 | In critical applications, it is vital for classifiers to defer decision-making to humans. We propose a post-hoc method that makes existing classifiers selectively abstain from predicting certain samples. Our abstaining classifier is incentivized to maintain the original accuracy for each... | JeanFrancois Ton, Mingyan Liu, Ruocheng Guo, Tongxin Yin, Yang Liu, Yuanshun Yao |  |
| 1638 |  |  [Aligning Relational Learning with Lipschitz Fairness](https://openreview.net/forum?id=ODSgo2m8aE) |  | 0 | Relational learning has gained significant attention, led by the expressiveness of Graph Neural Networks (GNNs) on graph data. While the inherent biases in common graph data are involved in GNN training, it poses a serious challenge to constraining the GNN output perturbations induced by input... | Chunhui Zhang, Soroush Vosoughi, Yaning Jia |  |
| 1639 |  |  [Listen, Think, and Understand](https://openreview.net/forum?id=nBZBPXdJlC) |  | 0 | The ability of artificial intelligence (AI) systems to perceive and comprehend audio signals is crucial for many applications. Although significant progress has been made in this area since the development of AudioSet, most existing models are designed to map audio inputs to pre-defined, discrete... | Alexander H. Liu, Hongyin Luo, James R. Glass, Leonid Karlinsky, Yuan Gong |  |
| 1640 |  |  [GOAt: Explaining Graph Neural Networks via Graph Output Attribution](https://openreview.net/forum?id=2Q8TZWAHv4) |  | 0 | Understanding the decision-making process of Graph Neural Networks (GNNs) is crucial to their interpretability. Most existing methods for explaining GNNs typically rely on training auxiliary models, resulting in the explanations remain black-boxed. This paper introduces Graph Output Attribution... | Bang Liu, Di Niu, Jiao He, Keith G. Mills, Shengyao Lu |  |
| 1641 |  |  [Time Fairness in Online Knapsack Problems](https://openreview.net/forum?id=9kG7TwgLYu) |  | 0 | The online knapsack problem is a classic problem in the field of online algorithms. Its canonical version asks how to pack items of different values and weights arriving online into a capacity-limited knapsack so as to maximize the total value of the admitted items. Although optimal competitive... | Adam Lechowicz, Bo Sun, Mohammad Hajiesmaili, Rik Sengupta, Shahin Kamali |  |
| 1642 |  |  [A Lie Group Approach to Riemannian Batch Normalization](https://openreview.net/forum?id=okYdj8Ysru) |  | 0 | Manifold-valued measurements exist in numerous applications within computer vision and machine learning. Recent studies have extended Deep Neural Networks (DNNs) to manifolds, and concomitantly, normalization techniques have also been adapted to several manifolds, referred to as Riemannian... | Nicu Sebe, Yue Song, Yunmei Liu, Ziheng Chen |  |
| 1643 |  |  [Federated Text-driven Prompt Generation for Vision-Language Models](https://openreview.net/forum?id=NW31gAylIm) |  | 0 | Prompt learning for vision-language models, e.g., CoOp, has shown great success in adapting CLIP to different downstream tasks, making it a promising solution for federated learning due to computational reasons. Existing prompt learning techniques replace hand-crafted text prompts with learned... | Chaithanya Kumar Mummadi, Chen Qiu, Lu Peng, Madan Ravi Ganesh, WanYi Lin, Xingyu Li, Zhenzhen Li |  |
| 1644 |  |  [On the Hardness of Constrained Cooperative Multi-Agent Reinforcement Learning](https://openreview.net/forum?id=wFWuX1Fhtj) |  | 0 | Constrained cooperative multi-agent reinforcement learning (MARL) is an emerging learning framework that has been widely applied to manage multi-agent systems, and many primal-dual type algorithms have been developed for it. However, the convergence of primal-dual algorithms crucially relies on... | Heng Huang, Yi Zhou, Ziyi Chen |  |
| 1645 |  |  [Det-CGD: Compressed Gradient Descent with Matrix Stepsizes for Non-Convex Optimization](https://openreview.net/forum?id=ZEZ0CPmoSI) |  | 0 | This paper introduces a new method for minimizing matrix-smooth non-convex objectives through the use of novel Compressed Gradient Descent (CGD) algorithms enhanced with a matrix-valued stepsize. The proposed algorithms are theoretically analyzed first in the single-node and subsequently in the... | Avetik G. Karagulyan, Hanmin Li, Peter Richtárik |  |
| 1646 |  |  [Causal-StoNet: Causal Inference for High-Dimensional Complex Data](https://openreview.net/forum?id=BtZ7vCt5QY) |  | 0 | With the advancement of data science, the collection of increasingly complex datasets has become commonplace. In such datasets, the data dimension can be extremely high, and the underlying data generation process can be unknown and highly nonlinear. As a result, the task of making causal inference... | Faming Liang, Yaxin Fang |  |
| 1647 |  |  [Conditional Information Bottleneck Approach for Time Series Imputation](https://openreview.net/forum?id=K1mcPiDdOJ) |  | 0 | Time series imputation presents a significant challenge because it requires capturing the underlying temporal dynamics from partially observed time series data. Among the recent successes of imputation methods based on generative models, the information bottleneck (IB) framework offers a... | Changhee Lee, MinGyu Choi |  |
| 1648 |  |  [Deep Neural Networks Tend To Extrapolate Predictably](https://openreview.net/forum?id=ljwoQ3cvQh) |  | 0 | Conventional wisdom suggests that neural network predictions tend to be unpredictable and overconfident when faced with out-of-distribution (OOD) inputs. Our work reassesses this assumption for neural networks with high-dimensional inputs. Rather than extrapolating in arbitrary ways, we observe... | Amrith Setlur, Claire J. Tomlin, Katie Kang, Sergey Levine |  |
| 1649 |  |  [RLCD: Reinforcement Learning from Contrastive Distillation for LM Alignment](https://openreview.net/forum?id=v3XXtxWKi6) |  | 0 | We propose Reinforcement Learning from Contrastive Distillation (RLCD), a method for aligning language models to follow principles expressed in natural language (e.g., to be more harmless) without using human feedback. RLCD creates preference pairs from two contrasting model outputs, one using a... | Asli Celikyilmaz, Dan Klein, Kevin Yang, Nanyun Peng, Yuandong Tian |  |
| 1650 |  |  [Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles](https://openreview.net/forum?id=TVDUVpgu9s) |  | 0 | In this study, we delve into an emerging optimization challenge involving a black-box objective function that can only be gauged via a ranking oracle—a situation frequently encountered in real-world scenarios, especially when the function is evaluated by human judges. A prominent instance of such a... | Dmitry Rybin, TsungHui Chang, Zhiwei Tang |  |
| 1651 |  |  [Entropy Coding of Unordered Data Structures](https://openreview.net/forum?id=afQuNt3Ruh) |  | 0 | We present shuffle coding, a general method for optimal compression of sequences of unordered objects using bits-back coding. Data structures that can be compressed using shuffle coding include multisets, graphs, hypergraphs, and others. We release an implementation that can easily be adapted to... | Daniel Severo, Giulio Zani, James Townsend, JanWillem van de Meent, Julius Kunze |  |
| 1652 |  |  [Neurosymbolic Grounding for Compositional World Models](https://openreview.net/forum?id=4KZpDGD4Nh) |  | 0 | We introduce Cosmos, a framework for object-centric world modeling that is designed for compositional generalization (CompGen), i.e., high performance on unseen input scenes obtained through the composition of known visual "atoms." The central insight behind Cosmos is the use of a novel form of... | Arya Grayeli, Atharva Sehgal, Jennifer J. Sun, Swarat Chaudhuri |  |
| 1653 |  |  [OpenTab: Advancing Large Language Models as Open-domain Table Reasoners](https://openreview.net/forum?id=Qa0ULgosc9) |  | 0 | Large Language Models (LLMs) trained on large volumes of data excel at various natural language tasks, but they cannot handle tasks requiring knowledge that has not been trained on previously. One solution is to use a retriever that fetches relevant information to expand LLM's knowledge scope.... | Balasubramaniam Srinivasan, Christos Faloutsos, Chuan Lei, George Karypis, Huzefa Rangwala, Jiani Zhang, Kezhi Kong, Zhengyuan Shen |  |
| 1654 |  |  [Adaptive Sharpness-Aware Pruning for Robust Sparse Networks](https://openreview.net/forum?id=QFYVVwiAM8) |  | 0 | Robustness and compactness are two essential attributes of deep learning models that are deployed in the real world. The goals of robustness and compactness may seem to be at odds, since robustness requires generalization across domains, while the process of compression exploits specificity in one... | Anna Bair, Hongxu Yin, José M. Álvarez, Maying Shen, Pavlo Molchanov |  |
| 1655 |  |  [MBR and QE Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods](https://openreview.net/forum?id=bkNx3O0sND) |  | 0 | Recent research in decoding methods for Natural Language Generation (NLG) tasks has shown that MAP decoding is not optimal, because model probabilities do not always align with human preferences. Stronger decoding methods, including Quality Estimation (QE) reranking and Minimum Bayes' Risk (MBR)... | Mara Finkelstein, Markus Freitag |  |
| 1656 |  |  [Federated Orthogonal Training: Mitigating Global Catastrophic Forgetting in Continual Federated Learning](https://openreview.net/forum?id=nAs4LdaP9Y) |  | 0 | Federated Learning (FL) has gained significant attraction due to its ability to enable privacy-preserving training over decentralized data. Current literature in FL mostly focuses on single-task learning. However, over time, new tasks may appear in the clients and the global model should learn... | Duygu Nur Yaldiz, Salman Avestimehr, Yahya H. Ezzeldin, Yavuz Faruk Bakman |  |
| 1657 |  |  [Self-Consuming Generative Models Go MAD](https://openreview.net/forum?id=ShjMHfmPs0) |  | 0 | Seismic advances in generative AI algorithms for imagery, text, and other data types have led to the temptation to use AI-synthesized data to train next-generation models. Repeating this process creates an autophagous ("self-consuming") loop whose properties are poorly understood. We conduct a... | Ahmed Imtiaz Humayun, Ali Siahkoohi, Daniel LeJeune, Hossein Babaei, Josue CascoRodriguez, Lorenzo Luzi, Richard G. Baraniuk, Sina Alemohammad |  |
| 1658 |  |  [The Hidden Language of Diffusion Models](https://openreview.net/forum?id=awWpHnEJDw) |  | 0 | Text-to-image diffusion models have demonstrated an unparalleled ability to generate high-quality, diverse images from a textual prompt. However, the internal representations learned by these models remain an enigma. In this work, we present Conceptor, a novel method to interpret the internal... | Assaf Shocher, Hila Chefer, Inbar Mosseri, Lior Wolf, Michal Irani, Mor Geva, Oran Lang, Volodymyr Polosukhin |  |
| 1659 |  |  [Reasoning with Latent Diffusion in Offline Reinforcement Learning](https://openreview.net/forum?id=tGQirjzddO) |  | 0 | Offline reinforcement learning (RL) holds promise as a means to learn high-reward policies from a static dataset, without the need for further environment interactions. However, a key challenge in offline RL lies in effectively stitching portions of suboptimal trajectories from the static dataset... | Glen Berseth, Jeff Schneider, John M. Dolan, Ravi Tej Akella, Shivesh Khaitan, Siddarth Venkatraman |  |
| 1660 |  |  [Low Rank Matrix Completion via Robust Alternating Minimization in Nearly Linear Time](https://openreview.net/forum?id=N0gT4A0jNV) |  | 0 | Given a matrix $M\in \mathbb{R}^{m\times n}$, the low rank matrix completion problem asks us to find a rank-$k$ approximation of $M$ as $UV^\top$ for $U\in \mathbb{R}^{m\times k}$ and $V\in \mathbb{R}^{n\times k}$ by only observing a few entries specified by a set of entries $\Omega\subseteq... | Junze Yin, Lichen Zhang, Yuzhou Gu, Zhao Song |  |
| 1661 |  |  [DiffusionNAG: Predictor-guided Neural Architecture Generation with Diffusion Models](https://openreview.net/forum?id=dyG2oLJYyX) |  | 0 | Existing NAS methods suffer from either an excessive amount of time for repetitive sampling and training of many task-irrelevant architectures. To tackle such limitations of existing NAS methods, we propose a paradigm shift from NAS to a novel conditional Neural Architecture Generation (NAG)... | Hayeon Lee, Jaehyeong Jo, Seanie Lee, Sohyun An, Sung Ju Hwang |  |
| 1662 |  |  [Federated Causal Discovery from Heterogeneous Data](https://openreview.net/forum?id=m7tJxajC3G) |  | 0 | Conventional causal discovery methods rely on centralized data, which is inconsistent with the decentralized nature of data in many real-world situations. This discrepancy has motivated the development of federated causal discovery (FCD) approaches. However, existing FCD methods may be limited by... | Bin Gu, Biwei Huang, Gongxu Luo, Guangyi Chen, Ignavier Ng, Kun Zhang, Loka Li, Tongliang Liu |  |
| 1663 |  |  [CellPLM: Pre-training of Cell Language Model Beyond Single Cells](https://openreview.net/forum?id=BKXvPDekud) |  | 0 | The current state-of-the-art single-cell pre-trained models are greatly inspired by the success of large language models. They trained transformers by treating genes as tokens and cells as sentences. However, three fundamental differences between single-cell data and natural language data are... | Hongzhi Wen, Jiayuan Ding, Jiliang Tang, Wei Jin, Wenzhuo Tang, Xinnan Dai, Yuying Xie |  |
| 1664 |  |  [Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks](https://openreview.net/forum?id=0H6DFoZZXZ) |  | 0 | Training generalist agents is difficult across several axes, requiring us to deal with high-dimensional inputs (space), long horizons (time), and generalization to novel tasks. Recent advances with architectures have allowed for improved scaling along one or two of these axes, but are still... | Amy Zhang, Edwin Zhang, Shinda Huang, William Yang Wang, Yujie Lu |  |
| 1665 |  |  [CALICO: Self-Supervised Camera-LiDAR Contrastive Pre-training for BEV Perception](https://openreview.net/forum?id=U7iiF79kI3) |  | 0 | Perception is crucial in the realm of autonomous driving systems, where bird's eye view (BEV)-based architectures have recently reached state-of-the-art performance. The desirability of self-supervised representation learning stems from the expensive and laborious process of annotating 2D and 3D... | Atul Prakash, Chaowei Xiao, Haizhong Zheng, Jiachen Sun, Qingzhao Zhang, Zhuoqing Mao |  |
| 1666 |  |  [Principled Architecture-aware Scaling of Hyperparameters](https://openreview.net/forum?id=HZndRcfyNI) |  | 0 | Training a high-quality deep neural network requires choosing suitable hyperparameters, which is a non-trivial and expensive process. Current works try to automatically optimize or design principles of hyperparameters, such that they can generalize to diverse unseen scenarios. However, most designs... | Boris Hanin, Junru Wu, Wuyang Chen, Zhangyang Wang |  |
| 1667 |  |  [Provable Robust Watermarking for AI-Generated Text](https://openreview.net/forum?id=SsmT8aO45L) |  | 0 | We study the problem of watermarking large language models (LLMs) generated text — one of the most promising approaches for addressing the safety challenges of LLM usage. In this paper, we propose a rigorous theoretical framework to quantify the effectiveness and robustness of LLM watermarks. We... | Lei Li, Prabhanjan Vijendra Ananth, Xuandong Zhao, YuXiang Wang |  |
| 1668 |  |  [Str2Str: A Score-based Framework for Zero-shot Protein Conformation Sampling](https://openreview.net/forum?id=C4BikKsgmK) |  | 0 | The dynamic nature of proteins is crucial for determining their biological functions and properties, for which Monte Carlo (MC) and molecular dynamics (MD) simulations stand as predominant tools to study such phenomena. By utilizing empirically derived force fields, MC or MD simulations explore the... | Bozitao Zhong, Jian Tang, Jiarui Lu, Zuobai Zhang |  |
| 1669 |  |  [CCIL: Continuity-Based Data Augmentation for Corrective Imitation Learning](https://openreview.net/forum?id=LQ6LQ8f4y8) |  | 0 | We present a new technique to enhance the robustness of imitation learning methods by generating corrective data to account for compounding error and disturbances. While existing methods rely on interactive expert labeling, additional offline datasets, or domain-specific invariances, our approach... | Abhay Deshpande, Abhishek Gupta, Liyiming Ke, Siddhartha S. Srinivasa, Yunchu Zhang |  |
| 1670 |  |  [Memory-Consistent Neural Networks for Imitation Learning](https://openreview.net/forum?id=R3Tf7LDdX4) |  | 0 | Imitation learning considerably simplifies policy synthesis compared to alternative approaches by exploiting access to expert demonstrations. For such imitation policies, errors away from the training samples are particularly critical. Even rare slip-ups in the policy action outputs can compound... | Dinesh Jayaraman, Insup Lee, James Weimer, Kaustubh Sridhar, Souradeep Dutta |  |
| 1671 |  |  [Learning dynamic representations of the functional connectome in neurobiological networks](https://openreview.net/forum?id=ZwhHSOHMTM) |  | 0 | The static synaptic connectivity of neuronal circuits stands in direct contrast to the dynamics of their function. As in changing community interactions, different neurons can participate actively in various combinations to effect behaviors at different times. We introduce an unsupervised approach... | Alexandra HaslundGourley, Eviatar Yemini, Luciano Dyballa, Samuel Lang, Steven W. Zucker |  |
| 1672 |  |  [Skill or Luck? Return Decomposition via Advantage Functions](https://openreview.net/forum?id=ZFMiHfZwIf) |  | 0 | Learning from off-policy data is essential for sample-efficient reinforcement learning. In the present work, we build on the insight that the advantage function can be understood as the causal effect of an action on the return, and show that this allows us to decompose the return of a trajectory... | Bernhard Schölkopf, HsiaoRu Pan |  |
| 1673 |  |  [Belief-Enriched Pessimistic Q-Learning against Adversarial State Perturbations](https://openreview.net/forum?id=7gDENzTzw1) |  | 0 | Reinforcement learning (RL) has achieved phenomenal success in various domains. However, its data-driven nature also introduces new vulnerabilities that can be exploited by malicious opponents. Recent work shows that a well-trained RL agent can be easily manipulated by strategically perturbing its... | Xiaolin Sun, Zizhan Zheng |  |
| 1674 |  |  [SmartPlay : A Benchmark for LLMs as Intelligent Agents](https://openreview.net/forum?id=S2oTVrlcp3) |  | 0 | Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating... | Tom M. Mitchell, Xuan Tang, Yuanzhi Li, Yue Wu |  |
| 1675 |  |  [Conditional Instrumental Variable Regression with Representation Learning for Causal Inference](https://openreview.net/forum?id=qDhq1icpO8) |  | 0 | This paper studies the challenging problem of estimating causal effects from observational data, in the presence of unobserved confounders. The two-stage least square (TSLS) method and its variants with a standard instrumental variable (IV) are commonly used to eliminate confounding bias, including... | Debo Cheng, Jiuyong Li, Jixue Liu, Lin Liu, Thuc Duy Le, Ziqi Xu |  |
| 1676 |  |  [Look, Remember and Reason: Grounded Reasoning in Videos with Language Models](https://openreview.net/forum?id=jhPvuc7kxB) |  | 0 | Multi-modal language models (LM) have recently shown promising performance in high-level reasoning tasks on videos. However, existing methods still fall short in tasks like causal or compositional spatiotemporal reasoning over actions, in which model predictions need to be grounded in fine-grained... | Apratim Bhattacharyya, Mingu Lee, Pulkit Madan, Reza Pourreza, Roland Memisevic, Sunny Panchal |  |
| 1677 |  |  [SOHES: Self-supervised Open-world Hierarchical Entity Segmentation](https://openreview.net/forum?id=PXNrncg2DF) |  | 0 | Open-world entity segmentation, as an emerging computer vision task, aims at segmenting entities in images without being restricted by pre-defined classes, offering impressive generalization capabilities on unseen images and concepts. Despite its promise, existing entity segmentation methods like... | Ani Nenkova, Handong Zhao, Hao Tan, Jason Kuen, Jiuxiang Gu, Liangyan Gui, Ruiyi Zhang, Shengcao Cao, Tong Sun, YuXiong Wang |  |
| 1678 |  |  [EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations](https://openreview.net/forum?id=mCOBKZmrzD) |  | 0 | Equivariant Transformers such as Equiformer have demonstrated the efficacy of applying Transformers to the domain of 3D atomistic systems. However, they are limited to small degrees of equivariant representations due to their computational complexity. In this paper, we investigate whether these... | Abhishek Das, Brandon M. Wood, Tess E. Smidt, YiLun Liao |  |
| 1679 |  |  [Chameleon: Increasing Label-Only Membership Leakage with Adaptive Poisoning](https://openreview.net/forum?id=4DoSULcfG6) |  | 0 | The integration of Machine Learning (ML) in numerous critical applications introduces a range of privacy concerns for individuals who provide their datasets for ML training purposes. One such privacy risk is Membership Inference (MI), in which an adversary seeks to determine whether a particular... | Alina Oprea, Giorgio Severi, Harsh Chaudhari, Jonathan R. Ullman |  |
| 1680 |  |  [Robust NAS under adversarial training: benchmark, theory, and beyond](https://openreview.net/forum?id=cdUpf6t6LZ) |  | 0 | Recent developments in neural architecture search (NAS) emphasize the significance of considering robust architectures against malicious data. However, there is a notable absence of benchmark evaluations and theoretical guarantees for searching these robust architectures, especially when... | CarlJohann SimonGabriel, Fanghui Liu, Grigorios Chrysos, Volkan Cevher, Yongtao Wu |  |
| 1681 |  |  [Conformal Language Modeling](https://openreview.net/forum?id=pzUhfQ74c5) |  | 0 | In this paper, we propose a novel approach to conformal prediction for language models (LMs) in which we produce prediction sets with performance guarantees. LM responses are typically sampled from a predicted distribution over the large, combinatorial output space of language. Translating this to... | Adam Fisch, Adam Yala, Jae Ho Sohn, Regina Barzilay, Tal Schuster, Tommi S. Jaakkola, Victor Quach |  |
| 1682 |  |  [Representation Deficiency in Masked Language Modeling](https://openreview.net/forum?id=b3l0piOrGU) |  | 0 | Masked Language Modeling (MLM) has been one of the most prominent approaches for pretraining bidirectional text encoders due to its simplicity and effectiveness. One notable concern about MLM is that the special $\texttt{[MASK]}$ symbol causes a discrepancy between pretraining data and downstream... | Han Fang, Jiawei Han, Jitin Krishnan, Luke Zettlemoyer, Marjan Ghazvininejad, Qifan Wang, Sinong Wang, Yu Meng, Yuning Mao |  |
| 1683 |  |  [Polynomial Width is Sufficient for Set Representation with High-dimensional Features](https://openreview.net/forum?id=34STseLBrQ) |  | 0 | Set representation has become ubiquitous in deep learning for modeling the inductive bias of neural networks that are insensitive to the input order. DeepSets is the most widely used neural network architecture for set representation. It involves embedding each set element into a latent space with... | Pan Li, Peihao Wang, Shenghao Yang, Shu Li, Zhangyang Wang |  |
| 1684 |  |  [Parametric Augmentation for Time Series Contrastive Learning](https://openreview.net/forum?id=EIPLdFy3vp) |  | 0 | Modern techniques like contrastive learning have been effectively used in many areas, including computer vision, natural language processing, and graph-structured data. Creating positive examples that assist the model in learning robust and discriminative representations is a crucial stage in... | Aitian Ma, Dongsheng Luo, Haifeng Chen, Mo Sha, Tianchun Wang, Wei Cheng, Xu Zheng |  |
| 1685 |  |  [Prediction Error-based Classification for Class-Incremental Learning](https://openreview.net/forum?id=DJZDgMOLXQ) |  | 0 | Class-incremental learning (CIL) is a particularly challenging variant of continual learning, where the goal is to learn to discriminate between all classes presented in an incremental fashion. Existing approaches often suffer from excessive forgetting and imbalance of the scores assigned to... | Gido M. van de Ven, Michal Zajac, Tinne Tuytelaars |  |
| 1686 |  |  [Test-Time Training on Nearest Neighbors for Large Language Models](https://openreview.net/forum?id=CNL2bku4ra) |  | 0 | Many recent efforts augment language models with retrieval, by adding retrieved data to the input context. For this approach to succeed, the retrieved data must be added at both training and test time. Moreover, as input length grows linearly with the size of retrieved data, cost in computation and... | Moritz Hardt, Yu Sun |  |
| 1687 |  |  [Improved Probabilistic Image-Text Representations](https://openreview.net/forum?id=ft1mr3WlGM) |  | 0 | Image-Text Matching (ITM) task, a fundamental vision-language (VL) task, suffers from the inherent ambiguity arising from multiplicity and imperfect annotations. Deterministic functions are not sufficiently powerful to capture ambiguity, prompting the exploration of probabilistic embeddings to... | Sanghyuk Chun |  |
| 1688 |  |  [SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos](https://openreview.net/forum?id=abL5LJNZ49) |  | 0 | We study the problem of procedure planning in instructional videos, which aims to make a goal-oriented sequence of action steps given partial visual state observations. The motivation of this problem is to learn a structured and plannable state and action space. Recent works succeeded in sequence... | Long Chen, ShihFu Chang, Wenliang Guo, Xudong Lin, Yulei Niu |  |
| 1689 |  |  [BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection](https://openreview.net/forum?id=s56xikpD92) |  | 0 | We present a novel defense, against backdoor attacks on Deep Neural Networks (DNNs), wherein adversaries covertly implant malicious behaviors (backdoors) into DNNs. Our defense falls within the category of post-development defenses that operate independently of how the model was generated. The... | Jiachen T. Wang, Ping He, Prateek Mittal, Tinghao Xie, Xiangyu Qi, Yiming Li |  |
| 1690 |  |  [Is Self-Repair a Silver Bullet for Code Generation?](https://openreview.net/forum?id=y0GJXRungR) |  | 0 | Large language models have shown remarkable aptitude in code generation, but still struggle to perform complex tasks. Self-repair---in which the model debugs and repairs its own code---has recently become a popular way to boost performance in these settings. However, despite its increasing... | Armando SolarLezama, Chenglong Wang, Jeevana Priya Inala, Jianfeng Gao, Theo X. Olausson |  |
| 1691 |  |  [MgNO: Efficient Parameterization of Linear Operators via Multigrid](https://openreview.net/forum?id=8OxL034uEr) |  | 0 | In this work, we propose a concise neural operator architecture for operator learning. Drawing an analogy with a conventional fully connected neural network, we define the neural operator as follows: the output of the $i$-th neuron in a nonlinear operator layer is defined by $\mathcal O_i(u) =... | Jinchao Xu, Juncai He, Xinliang Liu |  |
| 1692 |  |  [Interpreting Robustness Proofs of Deep Neural Networks](https://openreview.net/forum?id=Ev10F9TWML) |  | 0 | In recent years numerous methods have been developed to formally verify the robustness of deep neural networks (DNNs). Though the proposed techniques are effective in providing mathematical guarantees about the DNNs' behavior, it is not clear whether the proofs generated by these methods are... | Avaljot Singh, Debangshu Banerjee, Gagandeep Singh |  |
| 1693 |  |  [Differentially Private SGD Without Clipping Bias: An Error-Feedback Approach](https://openreview.net/forum?id=uFbWHyTlPn) |  | 0 | Differentially Private Stochastic Gradient Descent with Gradient Clipping (DPSGD-GC) is a powerful tool for training deep learning models using sensitive data, providing both a solid theoretical privacy guarantee and high efficiency. However, existing research has shown that DPSGD-GC only converges... | Mingyi Hong, Steven Wu, Xinwei Zhang, Zhiqi Bu |  |
| 1694 |  |  [A Linear Algebraic Framework for Counterfactual Generation](https://openreview.net/forum?id=PoDkdFQIu3) |  | 0 | Estimating individual treatment effects in clinical data is essential for understanding how different patients uniquely respond to treatments and identifying the most effective interventions for specific patient subgroups, thereby enhancing the precision and personalization of healthcare. However,... | Akshay Vashist, JongHoon Ahn |  |
| 1695 |  |  [DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genomes](https://openreview.net/forum?id=oMLQB4EZE1) |  | 0 | Decoding the linguistic intricacies of the genome is a crucial problem in biology, and pre-trained foundational models such as DNABERT and Nucleotide Transformer have made significant strides in this area. Existing works have largely hinged on k-mer, fixed-length permutations of A, T, C, and G, as... | Han Liu, Pratik Dutta, Ramana V. Davuluri, Weijian Li, Yanrong Ji, Zhihan Zhou |  |
| 1696 |  |  [Learning Stackable and Skippable LEGO Bricks for Efficient, Reconfigurable, and Variable-Resolution Diffusion Modeling](https://openreview.net/forum?id=qmXedvwrT1) |  | 0 | Diffusion models excel at generating photo-realistic images but come with significant computational costs in both training and sampling. While various techniques address these computational challenges, a less-explored issue is designing an efficient and adaptable network backbone for iterative... | Guanghan Ning, Hongxia Yang, Huangjie Zheng, Jianbo Yuan, Mingyuan Zhou, Pengcheng He, Quanzeng You, Zhendong Wang |  |
| 1697 |  |  [Latent 3D Graph Diffusion](https://openreview.net/forum?id=cXbnGtO0NZ) |  | 0 | Generating 3D graphs of symmetry-group equivariance is of intriguing potential in broad applications from machine vision to molecular discovery. Emerging approaches adopt diffusion generative models (DGMs) with proper re-engineering to capture 3D graph distributions. In this paper, we raise an... | Chao Tian, Haotian Xu, Jiwoong Park, Ruida Zhou, Yang Shen, Yuning You, Zhangyang Wang |  |
| 1698 |  |  [Reward Design for Justifiable Sequential Decision-Making](https://openreview.net/forum?id=OUkZXbbwQr) |  | 0 | Equipping agents with the capacity to justify made decisions using supporting evidence represents a cornerstone of accountable decision-making. Furthermore, ensuring that justifications are in line with human expectations and societal norms is vital, especially in high-stakes situations such as... | Aleksa Sukovic, Goran Radanovic |  |
| 1699 |  |  [SAFLEX: Self-Adaptive Augmentation via Feature Label Extrapolation](https://openreview.net/forum?id=qL6brrBDk2) |  | 0 | Data augmentation, a cornerstone technique in deep learning, is crucial in enhancing model performance, especially with scarce labeled data. While traditional techniques are effective, their reliance on hand-crafted methods limits their applicability across diverse data types and tasks. Although... | Anirudh Satheesh, Bang An, Furong Huang, Mucong Ding, Yuancheng Xu |  |
| 1700 |  |  [LILO: Learning Interpretable Libraries by Compressing and Documenting Code](https://openreview.net/forum?id=TqYbAWKMIe) |  | 0 | While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses,... | Gabriel Grand, Jacob Andreas, Joshua B. Tenenbaum, Lionel Wong, Matthew Bowers, Muxin Liu, Theo X. Olausson |  |
| 1701 |  |  [Matryoshka Diffusion Models](https://openreview.net/forum?id=tOzCcDdH9O) |  | 0 | Diffusion models are the de-facto approach for generating high-quality images and videos, but learning high-dimensional models remains a formidable task due to computational and optimization challenges. Existing methods often resort to training cascaded models in pixel space, or using a downsampled... | Jiatao Gu, Joshua M. Susskind, Navdeep Jaitly, Shuangfei Zhai, Yizhe Zhang |  |
| 1702 |  |  [Scalable Neural Network Kernels](https://openreview.net/forum?id=4iPw1klFWa) |  | 0 | We introduce the concept of scalable neural network kernels (SNNKs), the replacements of regular feedforward layers (FFLs), capable of approximating the latter, but with favorable computational properties. SNNKs effectively disentangle the inputs from the parameters of the neural network in the... | Arijit Sehanobish, Krzysztof Marcin Choromanski, Kumar Avinava Dubey, Valerii Likhosherstov, Yunfan Zhao |  |
| 1703 |  |  [Model Merging by Uncertainty-Based Gradient Matching](https://openreview.net/forum?id=D7KJmfEDQP) |  | 0 | Models trained on different datasets can be merged by a weighted-averaging of their parameters, but why does it work and when can it fail? Here, we connect the inaccuracy of weighted-averaging to mismatches in the gradients and propose a new uncertainty-based scheme to improve the performance by... | Edoardo M. Ponti, Iryna Gurevych, Mohammad Emtiyaz Khan, Nico Daheim, Thomas Möllenhoff |  |
| 1704 |  |  [αTC-VAE: On the relationship between Disentanglement and Diversity](https://openreview.net/forum?id=ptXo0epLQo) |  | 0 | Understanding and developing optimal representations has long been foundational in machine learning (ML). While disentangled representations have shown promise in generative modeling and representation learning, their downstream usefulness remains debated. Recent studies re-defined disentanglement... | Anirudh Goyal, Cristian Meo, Justin Dauwels, Louis Mahon |  |
| 1705 |  |  [A Restoration Network as an Implicit Prior](https://openreview.net/forum?id=x7d1qXEn1e) |  | 0 | Image denoisers have been shown to be powerful priors for solving inverse problems in imaging. In this work, we introduce a generalization of these methods that allows any image restoration network to be used as an implicit prior. The proposed method uses priors specified by deep neural networks... | Mauricio Delbracio, Peyman Milanfar, Ulugbek Kamilov, Yuyang Hu |  |
| 1706 |  |  [The Reasonableness Behind Unreasonable Translation Capability of Large Language Model](https://openreview.net/forum?id=3KDbIWT26J) |  | 0 | Multilingual large language models trained on non-parallel data yield impressive translation capabilities. Existing studies demonstrate that incidental sentence-level bilingualism within pre-training data contributes to the LLM's translation abilities. However, it has also been observed that LLM's... | Deng Cai, Guoping Huang, Lemao Liu, Rui Yan, Shuming Shi, Tingchen Fu |  |
| 1707 |  |  [Fast Value Tracking for Deep Reinforcement Learning](https://openreview.net/forum?id=LZIOBA2oDU) |  | 0 | Reinforcement learning (RL) tackles sequential decision-making problems by creating agents that interacts with their environment. However, existing algorithms often view these problem as static, focusing on point estimates for model parameters to maximize expected rewards, neglecting the stochastic... | Faming Liang, Frank Shih |  |
| 1708 |  |  [Oracle Efficient Algorithms for Groupwise Regret](https://openreview.net/forum?id=HrRKc9ei7h) |  | 0 | We study the problem of online prediction, in which at each time step $t \in \{1,2, \cdots T\}$, an individual $x_t$ arrives, whose label we must predict. Each individual is associated with various groups, defined based on their features such as age, sex, race etc., which may intersect. Our goal is... | Aaron Roth, Eshwar Ram Arunachaleswaran, Juba Ziani, Krishna Acharya, Sampath Kannan |  |
| 1709 |  |  [Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models](https://openreview.net/forum?id=26XphugOcS) |  | 0 | Prompt tuning in natural language processing (NLP) has become an increasingly popular method for adapting large language models to specific tasks. However, the transferability of these prompts, especially continuous prompts, between different models remains a challenge. In this work, we propose a... | Lili Mou, Yongkang Wu, Zijun Wu |  |
| 1710 |  |  [FairerCLIP: Debiasing CLIP's Zero-Shot Predictions using Functions in RKHSs](https://openreview.net/forum?id=HXoq9EqR9e) |  | 0 | Large pre-trained vision-language models such as CLIP provide compact and general-purpose representations of text and images that are demonstrably effective across multiple downstream zero-shot prediction tasks. However, owing to the nature of their training process, these models have the potential... | Lan Wang, Sepehr Dehdashtian, Vishnu Boddeti |  |
| 1711 |  |  [Generalized Schrödinger Bridge Matching](https://openreview.net/forum?id=SoismgeX7z) |  | 0 | Modern distribution matching algorithms for training diffusion or flow models directly prescribe the time evolution of the marginal distributions between two boundary distributions. In this work, we consider a generalized distribution matching setup, where these marginals are only implicitly... | Brian Karrer, Evangelos A. Theodorou, GuanHorng Liu, Maximilian Nickel, Ricky T. Q. Chen, Yaron Lipman |  |
| 1712 |  |  [Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition](https://openreview.net/forum?id=dQVtTdsvZH) |  | 0 | Video diffusion models have recently made great progress in generation quality, but are still limited by the high memory and computational requirements. This is because current video diffusion models often attempt to process high-dimensional videos directly. To tackle this issue, we propose... | Anima Anandkumar, Boyi Li, DeAn Huang, Jinwoo Shin, Sihyun Yu, Weili Nie |  |
| 1713 |  |  [MAMBA: an Effective World Model Approach for Meta-Reinforcement Learning](https://openreview.net/forum?id=1RE0H6mU7M) |  | 0 | Meta-reinforcement learning (meta-RL) is a promising framework for tackling challenging domains requiring efficient exploration. Existing meta-RL algorithms are characterized by low sample efficiency, and mostly focus on low-dimensional task distributions. In parallel, model-based RL methods have... | Aviv Tamar, Gilad Adler, Orr Krupnik, Tom Jurgenson, Zohar Rimon |  |
| 1714 |  |  [Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit](https://openreview.net/forum?id=KZJehvRKGD) |  | 0 | The cost of hyperparameter tuning in deep learning has been rising with model sizes, prompting practitioners to find new tuning methods using a proxy of smaller networks. One such proposal uses $\mu$P parameterized networks, where the optimal hyperparameters for small width networks \*transfer\* to... | Blake Bordelon, Boris Hanin, Cengiz Pehlevan, Lorenzo Noci, Mufan Bill Li |  |
| 1715 |  |  [Efficient-3Dim: Learning a Generalizable Single-image Novel-view Synthesizer in One Day](https://openreview.net/forum?id=3eFMnZ3N4J) |  | 0 | The task of novel view synthesis aims to generate unseen perspectives of an object or scene from a limited set of input images. Nevertheless, synthesizing novel views from a single image remains a significant challenge. Previous approaches tackle this problem by adopting mesh prediction,... | Hao Tang, JenHao Rick Chang, Liangchen Song, Liangliang Cao, Yifan Jiang, Zhangyang Wang |  |
| 1716 |  |  [Code Representation Learning at Scale](https://openreview.net/forum?id=vfzRRjumpX) |  | 0 | Recent studies have shown that code language model at scale demonstrate significant performance gains on downstream tasks, i.e., code generation. However, most of the existing works on code representation learning train models at a hundred million parameter scale using very limited pretraining... | Bing Xiang, Dan Roth, Dejiao Zhang, Hantian Ding, Ming Tan, Ramesh Nallapati, Wasi Uddin Ahmad, Xiaofei Ma |  |
| 1717 |  |  [Relaxing the Additivity Constraints in Decentralized No-Regret High-Dimensional Bayesian Optimization](https://openreview.net/forum?id=de1218PoEl) |  | 0 | Bayesian Optimization (BO) is typically used to optimize an unknown function $f$ that is noisy and costly to evaluate, by exploiting an acquisition function that must be maximized at each optimization step. Even if provably asymptotically optimal BO algorithms are efficient at optimizing... | Anthony Bardou, Patrick Thiran, Thomas Begin |  |
| 1718 |  |  [Compressing LLMs: The Truth is Rarely Pure and Never Simple](https://openreview.net/forum?id=B9klVS7Ddk) |  | 0 | Despite their remarkable achievements, modern Large Language Models (LLMs) encounter exorbitant computational and memory footprints. Recently, several works have shown significant success in \*training-free\* and \*data-free\* compression (pruning and quantization) of LLMs achieving 50-60\%... | Ajay Kumar Jaiswal, Bowen Zhang, Xianzhi Du, Yinfei Yang, Zhangyang Wang, Zhe Gan |  |
| 1719 |  |  [An Investigation of Representation and Allocation Harms in Contrastive Learning](https://openreview.net/forum?id=q4SiDyYQbo) |  | 0 | The effect of underrepresentation on the performance of minority groups is known to be a serious problem in supervised learning settings; however, it has been underexplored so far in the context of self-supervised learning (SSL). In this paper, we demonstrate that contrastive learning (CL), a... | Mayank Agarwal, Mikhail Yurochkin, Subha Maity, Yuekai Sun |  |
| 1720 |  |  [SEPT: Towards Efficient Scene Representation Learning for Motion Prediction](https://openreview.net/forum?id=efeBC1sQj9) |  | 0 | Motion prediction is crucial for autonomous vehicles to operate safely in complex traffic environments. Extracting effective spatiotemporal relationships among traffic elements is key to accurate forecasting. Inspired by the successful practice of pretrained large language models, this paper... | Chen Chen, Shengbo Eben Li, Yao Mu, Yuxuan Jiang, Zhiqian Lan |  |
| 1721 |  |  [Efficient and Scalable Graph Generation through Iterative Local Expansion](https://openreview.net/forum?id=2XkTz7gdpc) |  | 0 | In the realm of generative models for graphs, extensive research has been conducted. However, most existing methods struggle with large graphs due to the complexity of representing the entire joint distribution across all node pairs and capturing both global and local graph structures... | Andreas Bergmeister, Karolis Martinkus, Nathanaël Perraudin, Roger Wattenhofer |  |
| 1722 |  |  [Discovering modular solutions that generalize compositionally](https://openreview.net/forum?id=H98CVcX1eh) |  | 0 | Many complex tasks can be decomposed into simpler, independent parts. Discovering such underlying compositional structure has the potential to enable compositional generalization. Despite progress, our most powerful systems struggle to compose flexibly. It therefore seems natural to make models... | Alexandra Proca, Angelika Steger, Johannes von Oswald, João Sacramento, Maciej Wolczyk, Razvan Pascanu, Seijin Kobayashi, Simon Schug, Yassir Akram |  |
| 1723 |  |  [Neural Atoms: Propagating Long-range Interaction in Molecular Graphs through Efficient Communication Channel](https://openreview.net/forum?id=CUfSCwcgqm) |  | 0 | Graph Neural Networks (GNNs) have been widely adopted for drug discovery with molecular graphs. Nevertheless, current GNNs mainly excel in leveraging short-range interactions (SRI) but struggle to capture long-range interactions (LRI), both of which are crucial for determining molecular properties.... | Bo Han, Jiangchao Yao, Lu Zhang, Xuan Li, Yu Rong, Zhanke Zhou |  |
| 1724 |  |  [FairTune: Optimizing Parameter Efficient Fine Tuning for Fairness in Medical Image Analysis](https://openreview.net/forum?id=ArpwmicoYW) |  | 0 | Training models with robust group fairness properties is crucial in ethically sensitive application areas such as medical diagnosis. Despite the growing body of work aiming to minimise demographic bias in AI, this problem remains challenging. A key reason for this challenge is the fairness... | Ondrej Bohdal, Raman Dutt, Sotirios A. Tsaftaris, Timothy M. Hospedales |  |
| 1725 |  |  [Tree-Planner: Efficient Close-loop Task Planning with Large Language Models](https://openreview.net/forum?id=Glcsog6zOe) |  | 0 | This paper studies close-loop task planning, which refers to the process of generating a sequence of skills (a plan) to accomplish a specific goal while adapting the plan based on real-time observations. Recently, prompting Large Language Models (LLMs) to generate actions iteratively has become a... | Bin Wang, Mengkang Hu, Mingyu Ding, Ping Luo, Qiguang Chen, Shiguang Wu, Wenqi Shao, Xinmiao Yu, Yao Mu, Yu Qiao |  |
| 1726 |  |  [It's Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition](https://openreview.net/forum?id=QqjFHyQwtF) |  | 0 | Recent studies have successfully shown that large language models (LLMs) can be successfully used for generative error correction (GER) on top of the automatic speech recognition (ASR) output. Specifically, an LLM is utilized to carry out a direct mapping from the N-best hypotheses list generated... | ChaoHan Huck Yang, Chen Chen, Engsiong Chng, PinYu Chen, Ruizhe Li, Sabato Marco Siniscalchi, Yuchen Hu |  |
| 1727 |  |  [LOQA: Learning with Opponent Q-Learning Awareness](https://openreview.net/forum?id=FDQF6A1s6M) |  | 0 | In various real-world scenarios, interactions among agents often resemble the dynamics of general-sum games, where each agent strives to optimize its own utility. Despite the ubiquitous relevance of such settings, decentralized machine learning algorithms have struggled to find equilibria that... | Aaron C. Courville, Juan Agustin Duque, Milad Aghajohari, Tim Cooijmans |  |
| 1728 |  |  [A 2-Dimensional State Space Layer for Spatial Inductive Bias](https://openreview.net/forum?id=BGkqypmGvm) |  | 0 | A central objective in computer vision is to design models with appropriate 2-D inductive bias. Desiderata for 2-D inductive bias include two-dimensional position awareness, dynamic spatial locality, and translation and permutation invariance. To address these goals, we leverage an expressive... | Ethan Baron, Itamar Zimerman, Lior Wolf |  |
| 1729 |  |  [Learning 3D Particle-based Simulators from RGB-D Videos](https://openreview.net/forum?id=4rBEgZCubP) |  | 0 | Realistic simulation is critical for applications ranging from robotics to animation. Traditional analytic simulators sometimes struggle to capture sufficiently realistic simulation which can lead to problems including the well known "sim-to-real" gap in robotics. Learned simulators have emerged as... | Kelsey R. Allen, Kim Stachenfeld, Tatiana LopezGuevara, Thomas Kipf, Tobias Pfaff, William F. Whitney, Yulia Rubanova |  |
| 1730 |  |  [CAMBranch: Contrastive Learning with Augmented MILPs for Branching](https://openreview.net/forum?id=K6kt50zAiG) |  | 0 | Recent advancements have introduced machine learning frameworks to enhance the Branch and Bound (B\&B) branching policies for solving Mixed Integer Linear Programming (MILP). These methods, primarily relying on imitation learning of Strong Branching, have shown superior performance. However,... | Huangang Wang, Jiacheng Lin, Meng Xu, Zhihua Xiong |  |
| 1731 |  |  [ALAM: Averaged Low-Precision Activation for Memory-Efficient Training of Transformer Models](https://openreview.net/forum?id=OfXqQ5TRwp) |  | 0 | One of the key challenges in deep neural network training is the substantial amount of GPU memory required to store activations obtained in the forward pass. Various Activation-Compressed Training (ACT) schemes have been proposed to mitigate this issue; however, it is challenging to adopt those... | Dongsuk Jeon, Sunghyeon Woo, Sunwoo Lee |  |
| 1732 |  |  [Causal Fairness under Unobserved Confounding: A Neural Sensitivity Framework](https://openreview.net/forum?id=DqD59dQP37) |  | 0 | Fairness for machine learning predictions is widely required in practice for legal, ethical, and societal reasons. Existing work typically focuses on settings without unobserved confounding, even though unobserved confounding can lead to severe violations of causal fairness and, thus, unfair... | Dennis Frauen, Maresa Schröder, Stefan Feuerriegel |  |
| 1733 |  |  [One Forward is Enough for Neural Network Training via Likelihood Ratio Method](https://openreview.net/forum?id=ALGFFPXWSi) |  | 0 | While backpropagation (BP) is the mainstream approach for gradient computation in neural network training, its heavy reliance on the chain rule of differentiation constrains the designing flexibility of network architecture and training pipelines. We avoid the recursive computation in BP and... | Chenliang Xu, Jinyang Jiang, Yijie Peng, Zeliang Zhang, Zhaofei Yu |  |
| 1734 |  |  [A Framework for Inference Inspired by Human Memory Mechanisms](https://openreview.net/forum?id=vBo7544jZx) |  | 0 | How humans and machines make sense of current inputs for relation reasoning and question-answering while putting the perceived information into context of our past memories, has been a challenging conundrum in cognitive science and artificial intelligence. Inspired by human brain's memory system... | Jie Lin, Piao Hu, Ruizheng Huang, Xiangyu Zeng, Zhicheng Zhang |  |
| 1735 |  |  [Counterfactual Density Estimation using Kernel Stein Discrepancies](https://openreview.net/forum?id=wZXlEFO3tZ) |  | 0 | Causal effects are usually studied in terms of the means of counterfactual distributions, which may be insufficient in many scenarios. Given a class of densities known up to normalizing constants, we propose to model counterfactual distributions by minimizing kernel Stein discrepancies in a doubly... | Diego MartinezTaboada, Edward Kennedy |  |
| 1736 |  |  [Does Writing with Language Models Reduce Content Diversity?](https://openreview.net/forum?id=Feiz5HtCD0) |  | 0 | Large language models (LLMs) have led to a surge in collaborative writing with model assistance. As different users incorporate suggestions from the same model, there is a risk of decreased diversity in the produced content, potentially limiting diverse perspectives in public discourse. In this... | He He, Vishakh Padmakumar |  |
| 1737 |  |  [Conformal Inductive Graph Neural Networks](https://openreview.net/forum?id=homn1jOKI5) |  | 0 | Conformal prediction (CP) transforms any model's output into prediction sets guaranteed to include (cover) the true label. CP requires exchangeability, a relaxation of the i.i.d. assumption, to obtain a valid distribution-free coverage guarantee. This makes it directly applicable to transductive... | Aleksandar Bojchevski, Soroush H. Zargarbashi |  |
| 1738 |  |  [PB-LLM: Partially Binarized Large Language Models](https://openreview.net/forum?id=BifeBRhikU) |  | 0 | This paper explores network binarization, a radical form of quantization, compressing model weights to a single bit, specifically for Large Language Models (LLMs) compression. Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which... | Yuzhang Shang, Zhen Dong, Zhihang Yuan |  |
| 1739 |  |  [Random Sparse Lifts: Construction, Analysis and Convergence of finite sparse networks](https://openreview.net/forum?id=rBH7x87VfJ) |  | 0 | We present a framework to define a large class of neural networks for which, by construction, training by gradient flow provably reaches arbitrarily low loss when the number of parameters grows. Distinct from the fixed-space global optimality of non-convex optimization, this new form of... | David A. R. Robin, Kevin Scaman, Marc Lelarge |  |
| 1740 |  |  [Course Correcting Koopman Representations](https://openreview.net/forum?id=A18gWgc5mi) |  | 0 | Koopman representations aim to learn features of nonlinear dynamical systems (NLDS) which lead to linear dynamics in the latent space. Theoretically, such features can be used to simplify many problems in modeling and control of NLDS. In this work we study autoencoder formulations of this problem,... | Clement Gehring, David Kanaa, Jonathan Pilault, Mahan Fathi, PierreLuc Bacon, Ross Goroshin |  |
| 1741 |  |  [Out-of-Distribution Detection by Leveraging Between-Layer Transformation Smoothness](https://openreview.net/forum?id=AcRfzLS6se) |  | 0 | Effective out-of-distribution (OOD) detection is crucial for reliable machine learning models, yet most current methods are limited in practical use due to requirements like access to training data or intervention in training. We present a novel method for detecting OOD data in Transformers based... | Fran Jelenic, Jan Snajder, Josip Jukic, Martin Tutek, Mate Puljiz |  |
| 1742 |  |  [Intelligent Switching for Reset-Free RL](https://openreview.net/forum?id=Nq45xeghcL) |  | 0 | In the real world, the strong episode resetting mechanisms that are needed to train agents in simulation are unavailable. The resetting assumption limits the potential of reinforcement learning in the real world, as providing resets to an agent usually requires the creation of additional... | Darshan Patil, Glen Berseth, Janarthanan Rajendran, Sarath Chandar |  |
| 1743 |  |  [Adversarial Supervision Makes Layout-to-Image Diffusion Models Thrive](https://openreview.net/forum?id=EJPIzl7mgc) |  | 0 | Despite the recent advances in large-scale diffusion models, little progress has been made on the layout-to-image (L2I) synthesis task. Current L2I models either suffer from poor editability via text or weak alignment between the generated image and the input layout. This limits their usability in... | Anna Khoreva, Dan Zhang, Margret Keuper, Yumeng Li |  |
| 1744 |  |  [Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning](https://openreview.net/forum?id=EvDeiLv7qc) |  | 0 | The Mixture of Experts (MoE) is a widely known neural architecture where an ensemble of specialized sub-models optimizes overall performance with a constant computational cost. However, conventional MoEs pose challenges at scale due to the need to store all experts in memory. In this paper, we push... | Acyr Locatelli, Ahmet Üstün, Arash Ahmadian, Beyza Ermis, Sara Hooker, Ted Zadouri |  |
| 1745 |  |  [Less is More: One-shot Subgraph Reasoning on Large-scale Knowledge Graphs](https://openreview.net/forum?id=QHROe7Mfcb) |  | 0 | To deduce new facts on a knowledge graph (KG), a link predictor learns from the graph structure and collects local evidence to find the answer to a given query. However, existing methods suffer from a severe scalability problem due to the utilization of the whole KG for prediction, which hinders... | Bo Han, Jiangchao Yao, Quanming Yao, Yongqi Zhang, Zhanke Zhou |  |
| 1746 |  |  [Fixed-Budget Differentially Private Best Arm Identification](https://openreview.net/forum?id=vrE2fqAInO) |  | 0 | We study best arm identification (BAI) in linear bandits in the fixed-budget regime under differential privacy constraints, when the arm rewards are supported on the unit interval. Given a finite budget $T$ and a privacy parameter $\varepsilon>0$, the goal is to minimise the error probability in... | P. N. Karthik, Vincent Y. F. Tan, Yeow Meng Chee, Zhirui Chen |  |
| 1747 |  |  [Separate and Diffuse: Using a Pretrained Diffusion Model for Better Source Separation](https://openreview.net/forum?id=UXALv0lJZS) |  | 0 | The problem of speech separation, also known as the cocktail party problem, refers to the task of isolating a single speech signal from a mixture of speech signals. Previous work on source separation derived an upper bound for the source separation task in the domain of human speech. This bound is... | Eliya Nachmani, Lior Wolf, Shahar Lutati |  |
| 1748 |  |  [On the Limitations of Temperature Scaling for Distributions with Overlaps](https://openreview.net/forum?id=zavLQJ1XjB) |  | 0 | Despite the impressive generalization capabilities of deep neural networks, they have been repeatedly shown to be overconfident when they are wrong. Fixing this issue is known as model calibration, and has consequently received much attention in the form of modified training schemes and... | Muthu Chidambaram, Rong Ge |  |
| 1749 |  |  [Unknown Domain Inconsistency Minimization for Domain Generalization](https://openreview.net/forum?id=eNoiRal5xi) |  | 0 | The objective of domain generalization (DG) is to enhance the transferability of the model learned from a source domain to unobserved domains. To prevent overfitting to a specific domain, Sharpness-Aware Minimization (SAM) reduces source domain’s loss sharpness. Although SAM variants have delivered... | Byeonghu Na, HeeSun Bae, IlChul Moon, Seungjae Shin, YoonYeong Kim |  |
| 1750 |  |  [Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs](https://openreview.net/forum?id=gjeQKFxFpZ) |  | 0 | Empowering large language models (LLMs) to accurately express confidence in their answers is essential for reliable and trustworthy decision-making. Previous confidence elicitation methods, which primarily rely on \*white-box access\* to internal model information or model fine-tuning, have become... | Bryan Hooi, Jie Fu, Junxian He, Miao Xiong, Xinyang Lu, Yifei Li, Zhiyuan Hu |  |
| 1751 |  |  [Enhancing Neural Training via a Correlated Dynamics Model](https://openreview.net/forum?id=c9xsaASm9L) |  | 0 | As neural networks grow in scale, their training becomes both computationally demanding and rich in dynamics. Amidst the flourishing interest in these training dynamics, we present a novel observation: Parameters during training exhibit intrinsic correlations over time. Capitalizing on this, we... | Guy Gilboa, Ido Cohen, Jonathan Brokman, Rotem Turjeman, Roy Betser, Tom Berkov |  |
| 1752 |  |  [Flat Minima in Linear Estimation and an Extended Gauss Markov Theorem](https://openreview.net/forum?id=nxnbPPVvOG) |  | 0 | We consider the problem of linear estimation, and establish an extension of the Gauss-Markov theorem, in which the bias operator is allowed to be non-zero but bounded with respect to a matrix norm of Schatten type. We derive simple and explicit formulas for the optimal estimator in the cases of... | Simon N. Segert |  |
| 1753 |  |  [A Simple and Scalable Representation for Graph Generation](https://openreview.net/forum?id=nO344avRib) |  | 0 | Recently, there has been a surge of interest in employing neural networks for graph generation, a fundamental statistical learning problem with critical applications like molecule design and community analysis. However, most approaches encounter significant limitations when generating large-scale... | Seul Lee, Sungsoo Ahn, Yunhui Jang |  |
| 1754 |  |  [True Knowledge Comes from Practice: Aligning Large Language Models with Embodied Environments via Reinforcement Learning](https://openreview.net/forum?id=hILVmJ4Uvu) |  | 0 | Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments. On the contrary, reinforcement learning (RL) agents learn policies from scratch, which makes... | Bo An, Longtao Zheng, Shanqi Liu, Weihao Tan, Wentao Zhang, Xinrun Wang |  |
| 1755 |  |  [AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation](https://openreview.net/forum?id=fcqWJ8JgMR) |  | 0 | Due to privacy or patent concerns, a growing number of large models are released without granting access to their training data, making transferring their knowledge inefficient and problematic. In response, Data-Free Knowledge Distillation (DFKD) methods have emerged as direct solutions. However,... | Fei Wu, Kun Kuang, Shengyu Zhang, Xinyu Duan, Yifan Zhou, Zheqi Lv, Zihao Tang |  |
| 1756 |  |  [The Cost of Scaling Down Large Language Models: Reducing Model Size Affects Memory before In-context Learning](https://openreview.net/forum?id=ldJXXxPE0L) |  | 0 | We study how down-scaling large language model (LLM) size impacts LLM capabilities. We begin by measuring the effects of weight pruning – a popular technique for reducing model size – on the two abilities of LLMs: (a) recalling facts presented during pre-training and (b) processing information... | Gintare Karolina Dziugaite, Jonathan RaganKelley, Michael Carbin, Nolan Clement, Tian Jin, Vaishnavh Nagarajan, Xin Dong |  |
| 1757 |  |  [FedInverse: Evaluating Privacy Leakage in Federated Learning](https://openreview.net/forum?id=nTNgkEIfeb) |  | 0 | Federated Learning (FL) is a distributed machine learning technique where multiple devices (such as smartphones or IoT devices) train a shared global model by using their local data. FL claims that the data privacy of local participants is preserved well because local data will not be shared with... | Atul Sajjanhar, Di Wu, Jun Bai, Junjun Chen, Wei Zhou, Yiliao Song, Yong Xiang |  |
| 1758 |  |  [TOSS: High-quality Text-guided Novel View Synthesis from a Single Image](https://openreview.net/forum?id=9ZUYJpvIys) |  | 0 | In this paper, we present TOSS, which introduces text to the task of novel view synthesis (NVS) from just a single RGB image. While Zero123 has demonstrated impressive zero-shot open-set NVS capabilities, it treats NVS as a pure image-to-image translation problem. This approach suffers from the... | Boshi Tang, He Cao, HeungYeung Shum, Jianan Wang, Lei Zhang, Shilong Liu, Tianyu Yang, Xianbiao Qi, Yukai Shi, Yukun Huang |  |
| 1759 |  |  [Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation](https://openreview.net/forum?id=oZtt0pRnOl) |  | 0 | We study the problem of in-context learning (ICL) with large language models (LLMs) on private datasets. This scenario poses privacy risks, as LLMs may leak or regurgitate the private examples demonstrated in the prompt. We propose a novel algorithm that generates synthetic few-shot demonstrations... | Andre Manoel, Fatemehsadat Mireshghallah, Huseyin A. Inan, Janardhan Kulkarni, Richard Shin, Robert Sim, Sivakanth Gopi, Xinyu Tang, Zinan Lin |  |
| 1760 |  |  [Grokking as a First Order Phase Transition in Two Layer Networks](https://openreview.net/forum?id=3ROGsTX3IR) |  | 0 | A key property of deep neural networks (DNNs) is their ability to learn new features during training. This intriguing aspect of deep learning stands out most clearly in recently reported Grokking phenomena. While mainly reflected as a sudden increase in test accuracy, Grokking is also believed to... | Inbar Seroussi, Noa Rubin, Zohar Ringel |  |
| 1761 |  |  [Elucidating the Exposure Bias in Diffusion Models](https://openreview.net/forum?id=xEJMoj1SpX) |  | 0 | Diffusion models have demonstrated impressive generative capabilities, but their exposure bias problem, described as the input mismatch between training and sampling, lacks in-depth exploration. In this paper, we investigate the exposure bias problem in diffusion models by first analytically... | Albert Ali Salah, Itir Önal Ertugrul, Jianlin Su, Mang Ning, Mingxiao Li |  |
| 1762 |  |  [Accelerated Convergence of Stochastic Heavy Ball Method under Anisotropic Gradient Noise](https://openreview.net/forum?id=CIqjp9yTDq) |  | 0 | Heavy-ball momentum with decaying learning rates is widely used with SGD for optimizing deep learning models. In contrast to its empirical popularity, the understanding of its theoretical property is still quite limited, especially under the standard anisotropic gradient noise condition for... | Rui Pan, Tong Zhang, Xiaoyu Wang, Yuxing Liu |  |
| 1763 |  |  [Fast, Expressive SE(n) Equivariant Networks through Weight-Sharing in Position-Orientation Space](https://openreview.net/forum?id=dPHLbUqGbr) |  | 0 | Based on the theory of homogeneous spaces we derive \*geometrically optimal edge attributes\* to be used within the flexible message-passing framework. We formalize the notion of weight sharing in convolutional networks as the sharing of message functions over point-pairs that should be treated... | David W. Romero, Erik J. Bekkers, Putri A. van der Linden, Rob Hesselink, Sharvaree P. Vadgama |  |
| 1764 |  |  [Learning Large DAGs is Harder than you Think: Many Losses are Minimal for the Wrong DAG](https://openreview.net/forum?id=gwbQ2YwLhD) |  | 0 | Structure learning is a crucial task in science, especially in fields such as medicine and biology, where the wrong identification of (in)dependencies among random variables can have significant implications. The primary objective of structure learning is to learn a Directed Acyclic Graph (DAG)... | Devendra Singh Dhami, Jonas Seng, Kristian Kersting, Matej Zecevic |  |
| 1765 |  |  [JointNet: Extending Text-to-Image Diffusion for Dense Distribution Modeling](https://openreview.net/forum?id=kv5xE1p3jz) |  | 0 | We introduce JointNet, a novel neural network architecture for modeling the joint distribution of images and an additional dense modality (e.g., depth maps). JointNet is extended from a pre-trained text-to-image diffusion model, where a copy of the original network is created for the new dense... | David McKinnon, Jingyang Zhang, Long Quan, Shiwei Li, Tian Fang, Yanghai Tsin, Yao Yao, Yuanxun Lu |  |
| 1766 |  |  [Successor Heads: Recurring, Interpretable Attention Heads In The Wild](https://openreview.net/forum?id=kvcbV8KQsi) |  | 0 | In this work we describe successor heads: attention heads that increment tokens with a natural ordering, such as numbers, months, and days. For example, successor heads increment 'Monday' into 'Tuesday'. We explain the successor head behavior with an approach rooted in mechanistic interpretability,... | Arthur Conmy, Euan Ong, George Ogden, Rhys Gould |  |
| 1767 |  |  [RAPPER: Reinforced Rationale-Prompted Paradigm for Natural Language Explanation in Visual Question Answering](https://openreview.net/forum?id=bshfchPM9H) |  | 0 | Natural Language Explanation (NLE) in vision and language tasks aims to provide human-understandable explanations for the associated decision-making process. In practice, one might encounter explanations which lack informativeness or contradict visual-grounded facts, known as implausibility and... | ChiPin Huang, ChienYi Wang, FuEn Yang, KaiPo Chang, WeiYuan Cheng, YuChiang Frank Wang, YungHsuan Lai |  |
| 1768 |  |  [Information Bottleneck Analysis of Deep Neural Networks via Lossy Compression](https://openreview.net/forum?id=huGECz8dPp) |  | 0 | The Information Bottleneck (IB) principle offers an information-theoretic framework for analyzing the training process of deep neural networks (DNNs). Its essence lies in tracking the dynamics of two mutual information (MI) values: between the hidden layer output and the DNN input/target. According... | Aleksander Tolmachev, Alexey A. Frolov, Anna Neopryatnaya, Ivan Butakov, Kirill V. Andreev, Sofia Malanchuk |  |
| 1769 |  |  [Knowledge Fusion of Large Language Models](https://openreview.net/forum?id=jiDsk12qcz) |  | 0 | While training large language models (LLMs) from scratch can generate models with distinct functionalities and strengths, it comes at significant costs and may result in redundant capabilities. Alternatively, a cost-effective and compelling approach is to merge existing pre-trained LLMs into a more... | Deng Cai, Fanqi Wan, Shuming Shi, Wei Bi, Xiaojun Quan, Xinting Huang |  |
| 1770 |  |  [FedTrans: Client-Transparent Utility Estimation for Robust Federated Learning](https://openreview.net/forum?id=DRu8PMHgCh) |  | 0 | Federated Learning (FL) is an important privacy-preserving learning paradigm that plays an important role in the Intelligent Internet of Things. Training a global model in FL, however, is vulnerable to the noise in the heterogeneous data across the clients. In this paper, we introduce... | Jie Yang, Mingkun Yang, Qing Wang, Ran Zhu |  |
| 1771 |  |  [From Posterior Sampling to Meaningful Diversity in Image Restoration](https://openreview.net/forum?id=ff2g30cZxj) |  | 0 | Image restoration problems are typically ill-posed in the sense that each degraded image can be restored in infinitely many valid ways. To accommodate this, many works generate a diverse set of outputs by attempting to randomly sample from the posterior distribution of natural images given the... | Hila Manor, Noa Cohen, Tomer Michaeli, Yuval Bahat |  |
| 1772 |  |  [A Neural Framework for Generalized Causal Sensitivity Analysis](https://openreview.net/forum?id=ikX6D1oM1c) |  | 0 | Unobserved confounding is common in many applications, making causal inference from observational data challenging. As a remedy, causal sensitivity analysis is an important tool to draw causal conclusions under unobserved confounding with mathematical guarantees. In this paper, we propose... | Alicia Curth, Dennis Frauen, Fergus Imrie, Mihaela van der Schaar, Stefan Feuerriegel, Valentyn Melnychuk |  |
| 1773 |  |  [Skill Machines: Temporal Logic Skill Composition in Reinforcement Learning](https://openreview.net/forum?id=qiduMcw3CU) |  | 0 | It is desirable for an agent to be able to solve a rich variety of problems that can be specified through language in the same environment. A popular approach towards obtaining such agents is to reuse skills learned in prior tasks to generalise compositionally to new ones. However, this is a... | Benjamin Rosman, Devon Jarvis, Geraud Nangue Tasse, Steven James |  |
| 1774 |  |  [Unveiling and Manipulating Prompt Influence in Large Language Models](https://openreview.net/forum?id=ap1ByuwQrX) |  | 0 | Prompts play a crucial role in guiding the responses of Large Language Models (LLMs). However, the intricate role of individual tokens in prompts, known as input saliency, in shaping the responses remains largely underexplored. Existing saliency methods either misalign with LLM generation... | Hanzhang Zhou, Junlang Qian, Kezhi Mao, Zijian Feng, Zixiao Zhu |  |
| 1775 |  |  [Learning to solve Class-Constrained Bin Packing Problems via Encoder-Decoder Model](https://openreview.net/forum?id=6hvtSLkKeZ) |  | 0 | Neural methods have shown significant merit in solving combinatorial optimization (CO) problems, including the Bin Packing Problem (BPP). However, most existing ML-based approaches focus on geometric BPP like 3DBPP, neglecting complex vector BPP. In this study, we introduce a vector BPP variant... | Hanni Cheng, Shiliang Pu, Weihao Jiang, Ya Cong |  |
| 1776 |  |  [Idempotent Generative Network](https://openreview.net/forum?id=XIaS66XkNA) |  | 0 | We propose a new approach for generative modeling based on training a neural network to be idempotent. An idempotent operator is one that can be applied sequentially without changing the result beyond the initial application, namely $f(f(z))=f(z)$. The proposed model $f$ is trained to map a source... | Alexei A. Efros, Amil Dravid, Assaf Shocher, Inbar Mosseri, Michael Rubinstein, Yossi Gandelsman |  |
| 1777 |  |  [DeepSPF: Spherical SO(3)-Equivariant Patches for Scan-to-CAD Estimation](https://openreview.net/forum?id=Dnc3paMqDE) |  | 0 | Recently, SO(3)-equivariant methods have been explored for 3D reconstruction via Scan-to-CAD. Despite significant advancements attributed to the unique characteristics of 3D data, existing SO(3)-equivariant approaches often fall short in seamlessly integrating local and global contextual... | Adam Misik, Constantin Patsch, Driton Salihu, Eckehard G. Steinbach, Fabián Seguel, Yuankai Wu |  |
| 1778 |  |  [SparseDFF: Sparse-View Feature Distillation for One-Shot Dexterous Manipulation](https://openreview.net/forum?id=HHWlwxDeRn) |  | 0 | Humans demonstrate remarkable skill in transferring manipulation abilities across objects of varying shapes, poses, and appearances, a capability rooted in their understanding of semantic correspondences between different instances. To equip robots with a similar high-level comprehension, we... | Congyue Deng, Hao Dong, Haotong Zhang, Leonidas J. Guibas, Qianxu Wang, Yang You, Yixin Zhu |  |
| 1779 |  |  [ReFusion: Improving Natural Language Understanding with Computation-Efficient Retrieval Representation Fusion](https://openreview.net/forum?id=JtKGkz9fAe) |  | 0 | Retrieval-based augmentations (RA) incorporating knowledge from an external database into language models have greatly succeeded in various knowledge-intensive (KI) tasks. However, integrating retrievals in non-knowledge-intensive (NKI) tasks is still challenging. Existing works focus on... | Buzhou Tang, Chun Jason Xue, Shangyu Wu, TeiWei Kuo, Xue Liu, Ying Xiong, Yufei Cui |  |
| 1780 |  |  [FedCDA: Federated Learning with Cross-rounds Divergence-aware Aggregation](https://openreview.net/forum?id=nbPGqeH3lt) |  | 0 | In Federated Learning (FL), model aggregation is pivotal. It involves a global server iteratively aggregating client local trained models in successive rounds without accessing private data. Traditional methods typically aggregate the local models from the current round alone. However, due to the... | Haoran Xu, Haozhao Wang, Ruixuan Li, Tianwei Zhang, Yichen Li, Yuan Xu |  |
| 1781 |  |  [Select to Perfect: Imitating desired behavior from large multi-agent data](https://openreview.net/forum?id=L6crLU7MIE) |  | 0 | AI agents are commonly trained with large datasets of demonstrations of human behavior. However, not all behaviors are equally safe or desirable. Desired characteristics for an AI agent can be expressed by assigning desirability scores, which we assume are not assigned to individual behaviors but... | Edith Elkind, Jakob Nicolaus Foerster, João F. Henriques, Philip Torr, Tim Franzmeyer |  |
| 1782 |  |  [Respect the model: Fine-grained and Robust Explanation with Sharing Ratio Decomposition](https://openreview.net/forum?id=U7VW3KBm34) |  | 0 | The truthfulness of existing explanation methods in authentically elucidating the underlying model's decision-making process has been questioned. Existing methods have deviated from faithfully representing the model, thus susceptible to adversarial attacks. To address this, we propose a novel... | Nojun Kwak, Sangyu Han, Yearim Kim |  |
| 1783 |  |  [Exploring Diffusion Time-steps for Unsupervised Representation Learning](https://openreview.net/forum?id=bWzxhtl1HP) |  | 0 | Representation learning is all about discovering the hidden modular attributes that generate the data faithfully. We explore the potential of Denoising Diffusion Probabilistic Model (DM) in unsupervised learning of the modular attributes. We build a theoretical framework that connects the diffusion... | Eric IChao Chang, Hanwang Zhang, Jiankun Wang, Lei Ji, Qianru Sun, Zhongqi Yue |  |
| 1784 |  |  [SweetDreamer: Aligning Geometric Priors in 2D diffusion for Consistent Text-to-3D](https://openreview.net/forum?id=extpNXo6hB) |  | 0 | It is inherently ambiguous to lift 2D results from pre-trained diffusion models to a 3D world for text-to-3D generation. 2D diffusion models solely learn view-agnostic priors and thus lack 3D knowledge during the lifting, leading to the multi-view inconsistency problem. We find that this problem... | Ping Tan, Rui Chen, Weiyu Li, Xuelin Chen |  |
| 1785 |  |  [Hypergraph Dynamic System](https://openreview.net/forum?id=NLbRvr840Q) |  | 0 | Recently, hypergraph neural networks (HGNNs) exhibit the potential to tackle tasks with high-order correlations and have achieved success in many tasks. However, existing evolution on the hypergraph has poor controllability and lacks sufficient theoretical support (like dynamic systems), thus... | Jielong Yan, Shihui Ying, Yifan Feng, Yue Gao |  |
| 1786 |  |  [Augmented Bayesian Policy Search](https://openreview.net/forum?id=OvlcyABNQT) |  | 0 | Deterministic policies are often preferred over stochastic ones when implemented on physical systems. They can prevent erratic and harmful behaviors while being easier to implement and interpret. However, in practice, exploration is largely performed by stochastic policies. First-order Bayesian... | Carlo D'Eramo, Debabrota Basu, Mahdi Kallel, Riad Akrour |  |
| 1787 |  |  [Emu: Generative Pretraining in Multimodality](https://openreview.net/forum?id=mL8Q9OOamV) |  | 0 | We present Emu, a multimodal foundation model that seamlessly generates images and text in multimodal context. This omnivore model can take in any single-modality or multimodal data input indiscriminately (e.g., interleaved image, text and video) through a one-model-for-all autoregressive training... | Fan Zhang, Hongcheng Gao, Jingjing Liu, Qiying Yu, Quan Sun, Tiejun Huang, Xiaosong Zhang, Xinlong Wang, Yueze Wang, Yufeng Cui |  |
| 1788 |  |  [Dirichlet-based Per-Sample Weighting by Transition Matrix for Noisy Label Learning](https://openreview.net/forum?id=A4mJuFRMN8) |  | 0 | For learning with noisy labels, the transition matrix, which explicitly models the relation between noisy label distribution and clean label distribution, has been utilized to achieve the statistical consistency of either the classifier or the risk. Previous researches have focused more on how to... | Byeonghu Na, HeeSun Bae, IlChul Moon, Seungjae Shin |  |
| 1789 |  |  [HoloNets: Spectral Convolutions do extend to Directed Graphs](https://openreview.net/forum?id=EhmEwfavOW) |  | 0 | Within the graph learning community, conventional wisdom dictates that spectral convolutional networks may only be deployed on undirected graphs: Only there could the existence of a well-defined graph Fourier transform be guaranteed, so that information may be translated between spatial- and... | Christian Koke, Daniel Cremers |  |
| 1790 |  |  [Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback](https://openreview.net/forum?id=WesY0H9ghM) |  | 0 | Reinforcement Learning with Human Feedback (RLHF) has received significant attention for performing tasks without the need for costly manual reward design by aligning human preferences. It is crucial to consider diverse human feedback types and various learning methods in different environments.... | Hebin Liang, Jianye Hao, Jinyi Liu, Kai Zhao, Yan Zheng, Yi Ma, Yifu Yuan, Zhixin Feng, Zibin Dong |  |
| 1791 |  |  [Improving the Convergence of Dynamic NeRFs via Optimal Transport](https://openreview.net/forum?id=KiespDPaRH) |  | 0 | Synthesizing novel views for dynamic scenes from a collection of RGB inputs poses significant challenges due to the inherent under-constrained nature of the problem. To mitigate this ill-posedness, practitioners in the field of neural radiance fields (NeRF) often resort to the adoption of intricate... | Anton van den Hengel, Gil Avraham, Hisham Husain, Sameera Ramasinghe, Violetta Shevchenko |  |
| 1792 |  |  [Fast Updating Truncated SVD for Representation Learning with Sparse Matrices](https://openreview.net/forum?id=CX2RgsS29V) |  | 0 | Updating truncated Singular Value Decomposition (SVD) has extensive applications in representation learning. The continuous evolution of massive-scaled data matrices in practical scenarios highlights the importance of aligning SVD-based models with fast-paced updates. Recent methods for updating... | Cheng Chen, Haoran Deng, Jiahe Li, Shiliang Pu, Weihao Jiang, Yang Yang |  |
| 1793 |  |  [Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning](https://openreview.net/forum?id=N0I2RtD8je) |  | 0 | Reinforcement learning (RL) requires either manually specifying a reward function, which is often infeasible, or learning a reward model from a large amount of human feedback, which is often very expensive. We study a more sample-efficient alternative: using pretrained vision-language models (VLMs)... | David Lindner, Elvis Nava, Ethan Perez, Juan Rocamonde, Victoriano Montesinos |  |
| 1794 |  |  [TokenFlow: Consistent Diffusion Features for Consistent Video Editing](https://openreview.net/forum?id=lKK50q2MtV) |  | 0 | The generative AI revolution has recently expanded to videos. Nevertheless, current state-of-the-art video models are still lagging behind image models in terms of visual quality and user control over the generated content. In this work, we present a framework that harnesses the power of a... | Michal Geyer, Omer BarTal, Shai Bagon, Tali Dekel |  |
| 1795 |  |  [DreamTime: An Improved Optimization Strategy for Diffusion-Guided 3D Generation](https://openreview.net/forum?id=1bAUywYJTU) |  | 0 | Text-to-image diffusion models pre-trained on billions of image-text pairs have recently enabled 3D content creation by optimizing a randomly initialized differentiable 3D representation with score distillation. However, the optimization process suffers slow convergence and the resultant 3D models... | Boshi Tang, Jianan Wang, Lei Zhang, Xianbiao Qi, Yukai Shi, Yukun Huang |  |
| 1796 |  |  [CARD: Channel Aligned Robust Blend Transformer for Time Series Forecasting](https://openreview.net/forum?id=MJksrOhurE) |  | 0 | Recent studies have demonstrated the great power of Transformer models for time series forecasting. One of the key elements that lead to the transformer's success is the channel-independent (CI) strategy to improve the training robustness. However, the ignorance of the correlation among different... | Bolin Ding, Jinyang Gao, Qingsong Wen, Rong Jin, Tian Zhou, Xue Wang |  |
| 1797 |  |  [Reinforcement Symbolic Regression Machine](https://openreview.net/forum?id=PJVUWpPnZC) |  | 0 | In nature, the behavior of many complex systems can be described by parsimonious math equations. Symbolic Regression (SR) is defined as the task of automatically distilling equations from limited data. Keen efforts have been placed on tackling this issue and demonstrated success in SR. However,... | Hao Sun, Yang Liu, Yilong Xu |  |
| 1798 |  |  [Towards Faithful XAI Evaluation via Generalization-Limited Backdoor Watermark](https://openreview.net/forum?id=cObFETcoeW) |  | 0 | Saliency-based representation visualization (SRV) ($e.g.$, Grad-CAM) is one of the most classical and widely adopted explainable artificial intelligence (XAI) methods for its simplicity and efficiency. It can be used to interpret deep neural networks by locating saliency areas contributing the most... | Bin Wang, Mengxi Ya, ShuTao Xia, Tao Dai, Yiming Li, Yong Jiang |  |
| 1799 |  |  [Space Group Constrained Crystal Generation](https://openreview.net/forum?id=jkvZ7v4OmP) |  | 0 | Crystals are the foundation of numerous scientific and industrial applications. While various learning-based approaches have been proposed for crystal generation, existing methods neglect the spacegroup constraint which is crucial in describing the geometry of crystals and closely relevant to many... | Deli Zhao, Rui Jiao, Wenbing Huang, Yang Liu, Yu Liu |  |
| 1800 |  |  [Learning Multi-Agent Communication from Graph Modeling Perspective](https://openreview.net/forum?id=Qox9rO0kN0) |  | 0 | In numerous artificial intelligence applications, the collaborative efforts of multiple intelligent agents are imperative for the successful attainment of target objectives. To enhance coordination among these agents, a distributed communication framework is often employed. However, information... | Dacheng Tao, Li Shen, Shengchao Hu, Ya Zhang |  |
| 1801 |  |  [Efficient Multi-agent Reinforcement Learning by Planning](https://openreview.net/forum?id=CpnKq3UJwp) |  | 0 | Multi-agent reinforcement learning (MARL) algorithms have accomplished remarkable breakthroughs in solving large-scale decision-making tasks. Nonetheless, most existing MARL algorithms are model-free, limiting sample efficiency and hindering their applicability in more challenging scenarios. In... | Bin Liang, Chongjie Zhang, Jianing Ye, Jun Yang, Qihan Liu, Xiaoteng Ma |  |
| 1802 |  |  [EventRPG: Event Data Augmentation with Relevance Propagation Guidance](https://openreview.net/forum?id=i7LCsDMcZ4) |  | 0 | Event camera, a novel bio-inspired vision sensor, has drawn a lot of attention for its low latency, low power consumption, and high dynamic range. Currently, overfitting remains a critical problem in event-based classification tasks for Spiking Neural Network (SNN) due to its relatively weak... | Donghao Zhang, Jia Li, Jiaxu Wang, Mingyuan Sun, Renjing Xu, Zheng Fang, Zongyuan Ge |  |
| 1803 |  |  [NeRM: Learning Neural Representations for High-Framerate Human Motion Synthesis](https://openreview.net/forum?id=sOJriBlOFd) |  | 0 | Generating realistic human motions with high framerate is an underexplored task, due to the varied framerates of training data, huge memory burden brought by high framerates and slow sampling speed of generative models. Recent advances make a compromise for training by downsampling high-framerate... | Bin Li, Dong Wei, Huaijiang Sun, Jianfeng Lu, Shengxiang Hu, Weiqing Li, Xiaoning Sun |  |
| 1804 |  |  [Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization](https://openreview.net/forum?id=FlvtjAB0gl) |  | 0 | Recently, the remarkable advance of the Large Language Model (LLM) has inspired researchers to transfer its extraordinary reasoning capability to both vision and language data. However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the... | Bin Chen, Chao Liao, Chengru Song, Dai Meng, Di Zhang, Jianchao Tan, Kun Gai, Kun Xu, Kun Xu, Liwei Chen, Quzhe Huang, Wenwu Ou, Yadong Mu, Yang Jin |  |
| 1805 |  |  [Efficient Backpropagation with Variance Controlled Adaptive Sampling](https://openreview.net/forum?id=gEwKAZZmSw) |  | 0 | Sampling-based algorithms, which eliminate "unimportant" computations during forward and/or backpropagation (BP), offer potential solutions to accelerate neural network training. However, since sampling introduces approximations to training, such algorithms may not consistently maintain accuracy... | Jianfei Chen, Jun Zhu, Ziteng Wang |  |
| 1806 |  |  [Dynamic Neighborhood Construction for Structured Large Discrete Action Spaces](https://openreview.net/forum?id=80wh3jjCZf) |  | 0 | Large discrete action spaces (LDAS) remain a central challenge in reinforcement learning. Existing solution approaches can handle unstructured LDAS with up to a few million actions. However, many real-world applications in logistics, production, and transportation systems have combinatorial action... | Fabian Akkerman, Julius Luy, Maximilian Schiffer, Wouter van Heeswijk |  |
| 1807 |  |  [Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models](https://openreview.net/forum?id=6mLjDwYte5) |  | 0 | Sparse Mixture-of-Experts (MoE) is a neural architecture design that adds learnable parameters to Large Language Models (LLMs) without increasing computational complexity (FLOPs). Instruction tuning is a technique for training LLMs to follow instructions. We advocate combining these two approaches,... | Albert Webson, Barret Zoph, Denny Zhou, Hongkun Yu, Hyung Won Chung, Jason Wei, Kurt Keutzer, Le Hou, Nan Du, Shayne Longpre, Sheng Shen, Trevor Darrell, Tu Vu, Vincent Y. Zhao, William Fedus, Wuyang Chen, Xinyun Chen, Yanqi Zhou, Yuexin Wu, Yunxuan Li |  |
| 1808 |  |  [DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models](https://openreview.net/forum?id=OqTMUPuLuC) |  | 0 | Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill... | Botian Shi, Daocheng Fu, Liang He, Licheng Wen, Min Dou, Pinlong Cai, Tao Ma, Xin Li, Xinyu Cai, Yu Qiao |  |
| 1809 |  |  [PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images](https://openreview.net/forum?id=Cf4FJGmHRQ) |  | 0 | A standard practice in developing image recognition models is to train a model on a specific image resolution and then deploy it. However, in real-world inference, models often encounter images different from the training sets in resolution and/or subject to natural variations such as weather... | Dongeun Lee, Hyundong Jin, Jinsung Jeon, Jonghyun Choi, Kookjin Lee, Noseong Park, Sanghyun Hong |  |
| 1810 |  |  [CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets](https://openreview.net/forum?id=G0vdDSt9XM) |  | 0 | Large language models (LLMs) are often augmented with tools to solve complex tasks. By generating code snippets and executing them through task-specific Application Programming Interfaces (APIs), they can offload certain functions to dedicated external modules, such as image encoding and performing... | Hao Peng, Heng Ji, Lifan Yuan, Xingyao Wang, Yangyi Chen, Yi Fung |  |
| 1811 |  |  [A Variational Framework for Estimating Continuous Treatment Effects with Measurement Error](https://openreview.net/forum?id=S46Knicu56) |  | 0 | Estimating treatment effects has numerous real-world applications in various fields, such as epidemiology and political science. While much attention has been devoted to addressing the challenge using fully observational data, there has been comparatively limited exploration of this issue in cases... | Erdun Gao, Howard D. Bondell, Mingming Gong, Wei Huang |  |
| 1812 |  |  [InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation](https://openreview.net/forum?id=1k4yZbbDqX) |  | 0 | Diffusion models have revolutionized text-to-image generation with its exceptional quality and creativity. However, its multi-step sampling process is known to be slow, often requiring tens of inference steps to obtain satisfactory results. Previous attempts to improve its sampling speed and reduce... | Jian Peng, Jianzhu Ma, Qiang Liu, Xingchao Liu, Xiwen Zhang |  |
| 1813 |  |  [Object-Aware Inversion and Reassembly for Image Editing](https://openreview.net/forum?id=dpcVXiMlcv) |  | 0 | Diffusion-based image editing methods have achieved remarkable advances in text-driven image editing. The editing task aims to convert an input image with the original text prompt into the desired image that is well-aligned with the target text prompt. By comparing the original and target prompts,... | Bohan Zhuang, Chunhua Shen, Ganggui Ding, Hao Chen, Wen Wang, Zhen Yang |  |
| 1814 |  |  [SAN: Inducing Metrizability of GAN with Discriminative Normalized Linear Layer](https://openreview.net/forum?id=eiF7TU1E8E) |  | 0 | Generative adversarial networks (GANs) learn a target probability distribution by optimizing a generator and a discriminator with minimax objectives. This paper addresses the question of whether such optimization actually provides the generator with gradients that make its distribution close to the... | ChiehHsin Lai, Masaaki Imaizumi, Naoki Murata, Takashi Shibuya, Toshimitsu Uesaka, Yuhta Takida, Yuki Mitsufuji |  |
| 1815 |  |  [Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models](https://openreview.net/forum?id=dKl6lMwbCy) |  | 0 | Aligning large language models (LLMs) with human values and intents critically involves the use of human or AI feedback. While dense feedback annotations are expensive to acquire and integrate, sparse feedback presents a structural design choice between ratings (e.g., score Response A on a scale of... | Aditya Grover, Hritik Bansal, John Dang |  |
| 1816 |  |  [Sample-Efficient Multi-Agent RL: An Optimization Perspective](https://openreview.net/forum?id=o7qhUMylLU) |  | 0 | We study multi-agent reinforcement learning (MARL) for the general-sum Markov Games (MGs) under general function approximation. In order to find the minimum assumption for sample-efficient learning, we introduce a novel complexity measure called the Multi-Agent Decoupling Coefficient (MADC) for... | Nuoya Xiong, Zhaoran Wang, Zhihan Liu, Zhuoran Yang |  |
| 1817 |  |  [Minimum width for universal approximation using ReLU networks on compact domain](https://openreview.net/forum?id=dpDw5U04SU) |  | 0 | It has been shown that deep neural networks of a large enough width are universal approximators but they are not if the width is too small. There were several attempts to characterize the minimum width $w_{\min}$ enabling the universal approximation property; however, only a few of them found the... | Chanho Min, Namjun Kim, Sejun Park |  |
| 1818 |  |  [Self-Supervised Dataset Distillation for Transfer Learning](https://openreview.net/forum?id=h57gkDO2Yg) |  | 0 | Dataset distillation aims to optimize a small set so that a model trained on the set achieves performance similar to that of a model trained on the full dataset. While many supervised methods have achieved remarkable success in distilling a large dataset into a small set of representative samples,... | Dong Bok Lee, Joonho Ko, Juho Lee, Kenji Kawaguchi, Seanie Lee, Sung Ju Hwang |  |
| 1819 |  |  [Rethinking the symmetry-preserving circuits for constrained variational quantum algorithms](https://openreview.net/forum?id=SL7djdVpde) |  | 0 | With the arrival of the Noisy Intermediate-Scale Quantum (NISQ) era, Variational Quantum Algorithms (VQAs) have emerged as popular approaches to obtain possible quantum advantage in the relatively near future. In particular, how to effectively incorporate the common symmetries in physical systems... | Ge Yan, Hongxu Chen, Junchi Yan, Kaisen Pan |  |
| 1820 |  |  [Towards Codable Watermarking for Injecting Multi-Bits Information to LLMs](https://openreview.net/forum?id=JYu5Flqm9D) |  | 0 | As large language models (LLMs) generate texts with increasing fluency and realism, there is a growing need to identify the source of texts to prevent the abuse of LLMs. Text watermarking techniques have proven reliable in distinguishing whether a text is generated by LLMs by injecting hidden... | Deli Chen, Fandong Meng, Hao Zhou, Jie Zhou, Lean Wang, Wenkai Yang, Xu Sun, Yankai Lin |  |
| 1821 |  |  [Hypothesis Search: Inductive Reasoning with Language Models](https://openreview.net/forum?id=G7UtIGQmjm) |  | 0 | Inductive reasoning is a core problem-solving capacity: humans can identify underlying principles from a few examples, which can then be robustly generalized to novel scenarios. Recent work has evaluated large language models (LLMs) on inductive reasoning tasks by directly prompting them yielding... | Eric Zelikman, Gabriel Poesia, Nick Haber, Noah D. Goodman, Ruocheng Wang, Yewen Pu |  |
| 1822 |  |  [Language Model Decoding as Direct Metrics Optimization](https://openreview.net/forum?id=488A64eOf6) |  | 0 | Despite the remarkable advances in language modeling, current mainstream decoding methods still struggle to generate texts that align with human texts across different aspects. In particular, sampling-based methods produce less-repetitive texts which are often disjunctive in discourse, while... | Haozhe Ji, Hongning Wang, Minlie Huang, Pei Ke |  |
| 1823 |  |  [Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment](https://openreview.net/forum?id=uMAujpVi9m) |  | 0 | Pocket representations play a vital role in various biomedical applications, such as druggability estimation, ligand affinity prediction, and de novo drug design. While existing geometric features and pretrained representations have demonstrated promising results, they usually treat pockets... | Bowen Gao, WeiYing Ma, Yanyan Lan, Yinjun Jia, Yuanle Mo, Yuyan Ni, ZhiMing Ma |  |
| 1824 |  |  [MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning](https://openreview.net/forum?id=z8TW0ttBPp) |  | 0 | The recently released GPT-4 Code Interpreter has demonstrated remarkable proficiency in solving challenging math problems, primarily attributed to its ability to seamlessly reason with natural language, generate code, execute code, and continue reasoning based on the execution output. In this... | Aojun Zhou, Hongsheng Li, Houxing Ren, Ke Wang, Linqi Song, Mingjie Zhan, Renrui Zhang, Sichun Luo, Weikang Shi, Zimu Lu |  |
| 1825 |  |  [Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification](https://openreview.net/forum?id=c8McWs4Av0) |  | 0 | Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has brought significant advancements in addressing math reasoning problems. In particular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter, shows remarkable performance on challenging math datasets. In this... | Anya Jia, Aojun Zhou, Hongsheng Li, Ke Wang, Linqi Song, Mingjie Zhan, Shaoqing Lu, Sichun Luo, Weikang Shi, Zimu Lu, Zipeng Qin |  |
| 1826 |  |  [Dual Associated Encoder for Face Restoration](https://openreview.net/forum?id=gwDuW7Ok5f) |  | 0 | Restoring facial details from low-quality (LQ) images has remained challenging due to the nature of the problem caused by various degradations in the wild. The codebook prior has been proposed to address the ill-posed problems by leveraging an autoencoder and learned codebook of high-quality (HQ)... | Kelvin C. K. Chan, Lu Qi, MingHsuan Yang, YuJu Tsai, YuLun Liu |  |
| 1827 |  |  [DiffusionSat: A Generative Foundation Model for Satellite Imagery](https://openreview.net/forum?id=I5webNFDgQ) |  | 0 | Diffusion models have achieved state-of-the-art results on many modalities including images, speech, and video. However, existing models are not tailored to support remote sensing data, which is widely used in important applications including environmental monitoring and crop-yield prediction.... | Chenlin Meng, David B. Lobell, Linqi Zhou, Marshall Burke, Patrick Liu, Robin Rombach, Samar Khanna, Stefano Ermon |  |
| 1828 |  |  [DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior](https://openreview.net/forum?id=DDX1u29Gqr) |  | 0 | We present DreamCraft3D, a hierarchical 3D content generation method that produces high-fidelity and coherent 3D objects. We tackle the problem by leveraging a 2D reference image to guide the stages of geometry sculpting and texture boosting. A central focus of this work is to address the... | Bo Zhang, Jingxiang Sun, Lizhen Wang, Ruizhi Shao, Wen Liu, Yebin Liu, Zhenda Xie |  |
| 1829 |  |  [Pseudo-Generalized Dynamic View Synthesis from a Video](https://openreview.net/forum?id=QuVlUn4T2G) |  | 0 | Rendering scenes observed in a monocular video from novel viewpoints is a challenging problem. For static scenes the community has studied both scene-specific optimization techniques, which optimize on every test scene, and generalized techniques, which only run a deep net forward pass on a test... | Alex Colburn, Alexander G. Schwing, Fangchang Ma, Joshua M. Susskind, Miguel Ángel Bautista, Xiaoming Zhao |  |
| 1830 |  |  [Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification](https://openreview.net/forum?id=g6rZtxaXRm) |  | 0 | A promising approach for improving the performance of vision-language models like CLIP for image classification is to extend the class descriptions (i.e., prompts) with related attributes, e.g., using brown sparrow instead of sparrow. However, current zero-shot methods select a subset of attributes... | Reza Esfandiarpoor, Stephen H. Bach |  |
| 1831 |  |  [Interpretable Sparse System Identification: Beyond Recent Deep Learning Techniques on Time-Series Prediction](https://openreview.net/forum?id=aFWUY3E7ws) |  | 0 | With the continuous advancement of neural network methodologies, time series prediction has attracted substantial interest over the past decades. Nonetheless, the interpretability of neural networks is insufficient and the utilization of deep learning techniques for prediction necessitates... | Duxin Chen, Wenjia Wei, Wenwu Yu, Xia Zhu, Xiaoyi Liu |  |
| 1832 |  |  [Functional Bayesian Tucker Decomposition for Continuous-indexed Tensor Data](https://openreview.net/forum?id=ZWyZeqE928) |  | 0 | Tucker decomposition is a powerful tensor model to handle multi-aspect data. It demonstrates the low-rank property by decomposing the grid-structured data as interactions between a core tensor and a set of object representations (factors). A fundamental assumption of such decomposition is that... | Mike Kirby, Shandian Zhe, Shibo Li, Shikai Fang, Xin Yu, Zheng Wang |  |
| 1833 |  |  [Generative Pre-training for Speech with Flow Matching](https://openreview.net/forum?id=KpoQSgxbKH) |  | 0 | Generative models have gained more and more attention in recent years for their remarkable success in tasks that required estimating and sampling data distribution to generate high-fidelity synthetic data. In speech, text-to-speech synthesis and neural vocoder are good examples where generative... | Alexander H. Liu, Andros Tjandra, Apoorv Vyas, Bowen Shi, Matthew Le, WeiNing Hsu |  |
| 1834 |  |  [EBMDock: Neural Probabilistic Protein-Protein Docking via a Differentiable Energy Model](https://openreview.net/forum?id=qg2boc2AwU) |  | 0 | Protein complex formation, a pivotal challenge in contemporary biology, has recently gained interest from the machine learning community, particularly concerning protein-ligand docking tasks. In this paper, we delve into the equally crucial but comparatively under-investigated domain of... | Huaijin Wu, Jiaxiang Wu, Junchi Yan, Nianzu Yang, Wei Liu, Yatao Bian |  |
| 1835 |  |  [CausalTime: Realistically Generated Time-series for Benchmarking of Causal Discovery](https://openreview.net/forum?id=iad1yyyGme) |  | 0 | Time-series causal discovery (TSCD) is a fundamental problem of machine learning. However, existing synthetic datasets cannot properly evaluate or predict the algorithms' performance on real data. This study introduces the CausalTime pipeline to generate time-series that highly resemble the real... | Jinli Suo, Kunlun He, Qin Zhong, Tingxiong Xiao, Yuxiao Cheng, Ziqian Wang |  |
| 1836 |  |  [Protein-ligand binding representation learning from fine-grained interactions](https://openreview.net/forum?id=AXbN2qMNiW) |  | 0 | The binding between proteins and ligands plays a crucial role in the realm of drug discovery. Previous deep learning approaches have shown promising results over traditional computationally intensive methods, but resulting in poor generalization due to limited supervised data. In this paper, we... | Minghao Li, Shikun Feng, WeiYing Ma, Yanyan Lan, Yinjun Jia |  |
| 1837 |  |  [SLiMe: Segment Like Me](https://openreview.net/forum?id=7FeIRqCedv) |  | 0 | Significant strides have been made using large vision-language models, like Stable Diffusion (SD), for a variety of downstream tasks, including image generation, image editing, and 3D shape generation. Inspired by these advancements, we explore leveraging these vision-language models for segmenting... | Aditya Sanghi, Ali MahdaviAmiri, Aliasghar Khani, Ghassan Hamarneh, Saeid Asgari Taghanaki |  |
| 1838 |  |  [Adversarial Attacks on Fairness of Graph Neural Networks](https://openreview.net/forum?id=q3KNrmW6Ql) |  | 0 | Fairness-aware graph neural networks (GNNs) have gained a surge of attention as they can reduce the bias of predictions on any demographic group (e.g., female) in graph-based applications. Although these methods greatly improve the algorithmic fairness of GNNs, the fairness can be easily corrupted... | Binchi Zhang, Chen Chen, Jundong Li, Minnan Luo, Yada Zhu, Yushun Dong |  |
| 1839 |  |  [Faithful Vision-Language Interpretation via Concept Bottleneck Models](https://openreview.net/forum?id=rp0EdI8X4e) |  | 0 | The demand for transparency in healthcare and finance has led to interpretable machine learning (IML) models, notably the concept bottleneck models (CBMs), valued for their potential in performance and insights into deep neural networks. However, CBM's reliance on manually annotated data poses... | Di Wang, Junxiao Wang, Laure BertiÉquille, Lijie Hu, Songning Lai |  |
| 1840 |  |  [Efficiently Computing Similarities to Private Datasets](https://openreview.net/forum?id=HMe5CJv9dQ) |  | 0 | Many methods in differentially private model training rely on computing the similarity between a query point (such as public or synthetic data) and private data. We abstract out this common subroutine and study the following fundamental algorithmic problem: Given a similarity function $f$ and a... | Arturs Backurs, Jakub Tarnawski, Sandeep Silwal, Sepideh Mahabadi, Zinan Lin |  |
| 1841 |  |  [Sliced Denoising: A Physics-Informed Molecular Pre-Training Method](https://openreview.net/forum?id=liKkG1zcWq) |  | 0 | While molecular pre-training has shown great potential in enhancing drug discovery, the lack of a solid physical interpretation in current methods raises concerns about whether the learned representation truly captures the underlying explanatory factors in observed data, ultimately resulting in... | Shikun Feng, WeiYing Ma, Yanyan Lan, Yuyan Ni, ZhiMing Ma |  |
| 1842 |  |  [Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection](https://openreview.net/forum?id=4UIBysXjVq) |  | 0 | Graph-level anomaly detection has gained significant attention as it finds applications in various domains, such as cancer diagnosis and enzyme prediction. However, existing methods fail to capture the spectral properties of graph anomalies, resulting in unexplainable framework design and... | Sibo Wang, Xiangyu Dong, Xingyi Zhang |  |
| 1843 |  |  [P2OT: Progressive Partial Optimal Transport for Deep Imbalanced Clustering](https://openreview.net/forum?id=hD3sGVqPsr) |  | 0 | Deep clustering, which learns representation and semantic clustering without labels information, poses a great challenge for deep learning-based approaches. Despite significant progress in recent years, most existing methods focus on uniformly distributed datasets, significantly limiting the... | Chuyu Zhang, Hui Ren, Xuming He |  |
| 1844 |  |  [SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores](https://openreview.net/forum?id=lajn1iROCu) |  | 0 | The ever-growing complexity of reinforcement learning (RL) tasks demands a distributed system to efficiently generate and process a massive amount of data. However, existing open-source libraries suffer from various limitations, which impede their practical use in challenging scenarios where... | Guangju Wang, Huanchen Zhang, Jiaxuan Gao, Wei Fu, Yi Wu, Zhiyu Mei |  |
| 1845 |  |  [A Unified and General Framework for Continual Learning](https://openreview.net/forum?id=BE5aK0ETbp) |  | 0 | Continual Learning (CL) focuses on learning from dynamic and changing data distributions while retaining previously acquired knowledge. Various methods have been developed to address the challenge of catastrophic forgetting, including regularization-based, Bayesian-based, and memory-replay-based... | Heng Huang, Li Shen, Yan Li, Zhenyi Wang |  |
| 1846 |  |  [MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process](https://openreview.net/forum?id=CZiY6OLktd) |  | 0 | Recently, diffusion probabilistic models have attracted attention in generative time series forecasting due to their remarkable capacity to generate high-fidelity samples. However, the effective utilization of their strong modeling ability in the probabilistic time series forecasting task remains... | Chang Xu, Jiang Bian, Weiqing Liu, Xinyao Fan, Yueying Wu, Yuhao Huang |  |
| 1847 |  |  [Heterogeneous Personalized Federated Learning by Local-Global Updates Mixing via Convergence Rate](https://openreview.net/forum?id=7pWRLDBAtc) |  | 0 | Personalized federated learning (PFL) has emerged as a promising technique for addressing the challenge of data heterogeneity. While recent studies have made notable progress in mitigating heterogeneity associated with label distributions, the issue of effectively handling feature heterogeneity... | Anjie Le, Meirui Jiang, Qi Dou, Xiaoxiao Li |  |
| 1848 |  |  [SetCSE: Set Operations using Contrastive Learning of Sentence Embeddings](https://openreview.net/forum?id=zEHGSN8Hy8) |  | 0 | Taking inspiration from Set Theory, we introduce SetCSE, an innovative information retrieval framework. SetCSE employs sets to represent complex semantics and incorporates well-defined operations for structured information querying under the provided context. Within this framework, we introduce an... | Kang Liu |  |
| 1849 |  |  [Polynormer: Polynomial-Expressive Graph Transformer in Linear Time](https://openreview.net/forum?id=hmv1LpNfXa) |  | 0 | Graph transformers (GTs) have emerged as a promising architecture that is theoretically more expressive than message-passing graph neural networks (GNNs). However, typical GT models have at least quadratic complexity and thus cannot scale to large graphs. While there are several linear GTs recently... | Chenhui Deng, Zhiru Zhang, Zichao Yue |  |
| 1850 |  |  [Where We Have Arrived in Proving the Emergence of Sparse Interaction Primitives in DNNs](https://openreview.net/forum?id=3pWSL8My6B) |  | 0 | This study aims to prove the emergence of symbolic concepts (or more precisely, sparse primitive inference patterns) in well-trained deep neural networks (DNNs). Specifically, we prove the following three conditions for the emergence. (i) The high-order derivatives of the network output with... | Jiayang Gao, Qihan Ren, Quanshi Zhang, Wen Shen |  |
| 1851 |  |  [DV-3DLane: End-to-end Multi-modal 3D Lane Detection with Dual-view Representation](https://openreview.net/forum?id=l1U6sEgYkb) |  | 0 | Accurate 3D lane estimation is crucial for ensuring safety in autonomous driving. However, prevailing monocular techniques suffer from depth loss and lighting variations, hampering accurate 3D lane detection. In contrast, LiDAR points offer geometric cues and enable precise localization. In this... | Shuguang Cui, Yueru Luo, Zhen Li |  |
| 1852 |  |  [Continuous Field Reconstruction from Sparse Observations with Implicit Neural Networks](https://openreview.net/forum?id=kuTZMZdCPZ) |  | 0 | Reliably reconstructing physical fields from sparse sensor data is a challenge that frequenty arises in many scientific domains. In practice, the process generating the data is often not known to sufficient accuracy. Therefore, there is a growing interest in the deep neural network route to the... | Balu Nadiga, Shinjae Yoo, Wei Xu, Xihaier Luo, Yihui Ren |  |
| 1853 |  |  [Weaker MVI Condition: Extragradient Methods with Multi-Step Exploration](https://openreview.net/forum?id=RNGUbTYSjk) |  | 0 | This paper proposes a new framework of algorithms that is extended from the celebrated extragradient algorithm. The min-max problem has attracted increasing attention because of its applications in machine learning tasks such as generative adversarial networks (GANs) training. While there has been... | Bo Chen, Yifeng Fan, Yongqiang Li |  |
| 1854 |  |  [Demystifying Embedding Spaces using Large Language Models](https://openreview.net/forum?id=qoYogklIPz) |  | 0 | Embeddings have become a pivotal means to represent complex, multi-faceted information about entities, concepts, and relationships in a condensed and useful format. Nevertheless, they often preclude direct interpretation. While downstream tasks make use of these compressed representations,... | Azamat Tulepbergenov, ChihWei Hsu, Craig Boutilier, Deepak Ramachandran, Guy Tennenholtz, Jihwan Jeong, Lior Shani, Martin Mladenov, Yinlam Chow |  |
| 1855 |  |  [The Expressive Power of Low-Rank Adaptation](https://openreview.net/forum?id=likXVjmh3E) |  | 0 | \*Low-Rank Adaptation\* (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the... | Kangwook Lee, Yuchen Zeng |  |
| 1856 |  |  [Towards Category Unification of 3D Single Object Tracking on Point Clouds](https://openreview.net/forum?id=QlqdXrzzD1) |  | 0 | Category-specific models are provenly valuable methods in 3D single object tracking (SOT) regardless of Siamese or motion-centric paradigms. However, such over-specialized model designs incur redundant parameters, thus limiting the broader applicability of 3D SOT task. This paper first introduces... | DongKyu Chae, Fei Xie, Jiahao Nie, Xudong Lv, Xueyi Zhou, Zhiwei He |  |
| 1857 |  |  [You Only Query Once: An Efficient Label-Only Membership Inference Attack](https://openreview.net/forum?id=7WsivwyHrS) |  | 0 | As one of the privacy threats to machine learning models, the membership inference attack (MIA) tries to infer whether a given sample is in the original training set of a victim model by analyzing its outputs. Recent studies only use the predicted hard labels to achieve impressive membership... | Han Qiu, Jiwei Li, Shangwei Guo, Tianwei Zhang, Yutong Wu |  |
| 1858 |  |  [Mathematical Justification of Hard Negative Mining via Isometric Approximation Theorem](https://openreview.net/forum?id=udO3k28bEw) |  | 0 | In deep metric learning, the triplet loss has emerged as a popular method to learn many computer vision and natural language processing tasks such as facial recognition, object detection, and visual-semantic embeddings. One issue that plagues the triplet loss is network collapse, an undesirable... | Albert Xu, Bhaskar Vundurthy, Eliana Cohen, Howie Choset, JhihYi Hsieh, Lu Li, Nithya Kemp |  |
| 1859 |  |  [Bridging Neural and Symbolic Representations with Transitional Dictionary Learning](https://openreview.net/forum?id=uqxBTcWRnj) |  | 0 | This paper introduces a novel Transitional Dictionary Learning (TDL) framework that can implicitly learn symbolic knowledge, such as visual parts and relations, by reconstructing the input as a combination of parts with implicit relations. We propose a game-theoretic diffusion model to decompose... | Junyan Cheng, Peter Chin |  |
| 1860 |  |  [AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality Prediction](https://openreview.net/forum?id=JW3jTjaaAB) |  | 0 | Air quality prediction and modelling plays a pivotal role in public health and environment management, for individuals and authorities to make informed decisions. Although traditional data-driven models have shown promise in this domain, their long-term prediction accuracy can be limited,... | Cheng Long, Gao Cong, Jiahao Ji, Jingyuan Wang, Kethmi Hirushini Hettige, Shili Xiang |  |
| 1861 |  |  [DittoGym: Learning to Control Soft Shape-Shifting Robots](https://openreview.net/forum?id=MpyFAhH9CK) |  | 0 | Robot co-design, where the morphology of a robot is optimized jointly with a learned policy to solve a specific task, is an emerging area of research. It holds particular promise for soft robots, which are amenable to novel manufacturing techniques that can realize learned morphologies and... | Boyuan Chen, Huazhe Xu, Suning Huang, Vincent Sitzmann |  |
| 1862 |  |  [PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks](https://openreview.net/forum?id=gjXor87Xfy) |  | 0 | Memory-based Dynamic Graph Neural Networks (MDGNNs) are a family of dynamic graph neural networks that leverage a memory module to extract, distill, and memorize long-term temporal dependencies, leading to superior performance compared to memory-less counterparts. However, training MDGNNs faces the... | Chuan Wu, Difan Zou, Junwei Su |  |
| 1863 |  |  [TEDDY: Trimming Edges with Degree-based Discrimination Strategy](https://openreview.net/forum?id=5RUf9nEdyC) |  | 0 | Since the pioneering work on the lottery ticket hypothesis for graph neural networks (GNNs) was proposed in Chen et al. (2021), the study on finding graph lottery tickets (GLT) has become one of the pivotal focus in the GNN community, inspiring researchers to discover sparser GLT while achieving... | Eunho Yang, Hyunjin Seo, Jihun Yun |  |
| 1864 |  |  [Learning to Jointly Understand Visual and Tactile Signals](https://openreview.net/forum?id=NtQqIcSbqv) |  | 0 | Modeling and analyzing object and shape has been well studied in the past. However, manipulation of these complex tools and articulated objects remains difficult for autonomous agents. Our human hands, however, are dexterous and adaptive. We can easily adapt a manipulation skill on one object to... | Antonio Torralba, Benjamin Eckart, Chao Liu, Chao Liu, Francis Williams, Jan Kautz, Joshua B. Tenenbaum, Michael Foshey, Wojciech Matusik, Yichen Li, Yilun Du |  |
| 1865 |  |  [NeurRev: Train Better Sparse Neural Network Practically via Neuron Revitalization](https://openreview.net/forum?id=60lNoatp7u) |  | 0 | Dynamic Sparse Training (DST) employs a greedy search mechanism to identify an optimal sparse subnetwork by periodically pruning and growing network connections during training. To guarantee effectiveness, DST algorithms rely on high search frequency, which consequently, requires large learning... | Bin Ren, Gen Li, Jie Ji, Linke Guo, Lu Yin, Minghai Qin, Shiwei Liu, Wei Niu, Xiaolong Ma |  |
| 1866 |  |  [SPDER: Semiperiodic Damping-Enabled Object Representation](https://openreview.net/forum?id=92btneN9Wm) |  | 0 | We present a neural network architecture designed to naturally learn a positional embedding and overcome the spectral bias towards lower frequencies faced by conventional implicit neural representation networks. Our proposed architecture, SPDER, is a simple MLP that uses an activation function... | Chawin Sitawarin, Kathan Shah |  |
| 1867 |  |  [The Unreasonable Effectiveness of Linear Prediction as a Perceptual Metric](https://openreview.net/forum?id=e4FG5PJ9uC) |  | 0 | We show how perceptual embeddings of the visual system can be constructed at inference-time with no training data or deep neural network features. Our perceptual embeddings are solutions to a weighted least squares (WLS) problem, defined at the pixel-level, and solved at inference-time, that can... | Daniel Severo, Johannes Ballé, Lucas Theis |  |
| 1868 |  |  [Crystalformer: Infinitely Connected Attention for Periodic Structure Encoding](https://openreview.net/forum?id=fxQiecl9HB) |  | 0 | Predicting physical properties of materials from their crystal structures is a fundamental problem in materials science. In peripheral areas such as the prediction of molecular properties, fully connected attention networks have been shown to be successful. However, unlike these finite atom... | Kanta Ono, Kotaro Saito, Naoya Chiba, Ryo Igarashi, Tatsunori Taniai, Yoshitaka Ushiku, Yuta Suzuki |  |
| 1869 |  |  [Fast Ensembling with Diffusion Schrödinger Bridge](https://openreview.net/forum?id=Mgq6kxl115) |  | 0 | Deep Ensemble approach is a straightforward technique used to enhance the performance of deep neural networks by training them from different initial points, converging towards various local optima. However, a limitation of this methodology lies in its high computational overhead for inference,... | Hyunsu Kim, Jongmin Yoon, Juho Lee |  |
| 1870 |  |  [Decoupling regularization from the action space](https://openreview.net/forum?id=UaMgmoKEBj) |  | 0 | Regularized reinforcement learning (RL), particularly the entropy-regularized kind, has gained traction in optimal control and inverse RL. While standard unregularized RL methods remain unaffected by changes in the number of actions, we show that it can severely impact their regularized... | Emma Frejinger, PierreLuc Bacon, Sobhan Mohammadpour |  |
| 1871 |  |  [Robust Similarity Learning with Difference Alignment Regularization](https://openreview.net/forum?id=K9V7ugVuUz) |  | 0 | Similarity-based representation learning has shown impressive capabilities in both supervised (e.g., metric learning) and unsupervised (e.g., contrastive learning) scenarios. Existing approaches effectively constrained the representation difference (i.e., the disagreement between the embeddings of... | Chen Gong, Gang Niu, Jian Yang, Masashi Sugiyama, Okan Koc, Shuo Chen |  |
| 1872 |  |  [Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization](https://openreview.net/forum?id=PKICZXVY9M) |  | 0 | Existing vision-language models exhibit strong generalization on a variety of visual domains and tasks. However, such models mainly perform zero-shot recognition in a closed-set manner, and thus struggle to handle open-domain visual concepts by design. There are recent finetuning methods, such as... | Chen Huang, Hanlin Goh, Joshua M. Susskind, Yuhang Zang |  |
| 1873 |  |  [ConR: Contrastive Regularizer for Deep Imbalanced Regression](https://openreview.net/forum?id=RIuevDSK5V) |  | 0 | Imbalanced distributions are ubiquitous in real-world data. They create constraints on Deep Neural Networks to represent the minority labels and avoid bias towards majority labels. The extensive body of imbalanced approaches address categorical label spaces but fail to effectively extend to... | Lili Meng, Mahsa Keramati, R. David Evans |  |
| 1874 |  |  [A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks](https://openreview.net/forum?id=LnLySuf1vp) |  | 0 | While contrastive self-supervised learning has become the de-facto learning paradigm for graph neural networks, the pursuit of higher task accuracy requires a larger hidden dimensionality to learn informative and discriminative full-precision representations, raising concerns about computation,... | Baokun Wang, Changhua Meng, Huizhe Zhang, Jintang Li, Liang Chen, Ruofan Wu, Zibin Zheng, Zulun Zhu |  |
| 1875 |  |  [Adversarial Training Should Be Cast as a Non-Zero-Sum Game](https://openreview.net/forum?id=XJ9vjEAqbx) |  | 0 | One prominent approach toward resolving the adversarial vulnerability of deep neural networks is the two-player zero-sum paradigm of adversarial training, in which predictors are trained against adversarially chosen perturbations of data. Despite the promise of this approach, algorithms based on... | Alexander Robey, Fabian Latorre, George J. Pappas, Hamed Hassani, Volkan Cevher |  |
| 1876 |  |  [Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks](https://openreview.net/forum?id=hQVCCxQrYN) |  | 0 | Large Language Models (LLMs) are highly capable of performing planning for long-horizon robotics tasks, yet existing methods require access to a pre-defined skill library (\*e.g.\* picking, placing, pulling, pushing, navigating). However, LLM planning does not address how to design or learn those... | Devendra Singh Chaplot, Murtaza Dalal, Ruslan Salakhutdinov, Tarun Chiruvolu |  |
| 1877 |  |  [Diffusion-TS: Interpretable Diffusion for General Time Series Generation](https://openreview.net/forum?id=4h1apFjO99) |  | 0 | Denoising diffusion probabilistic models (DDPMs) are becoming the leading paradigm for generative models. It has recently shown breakthroughs in audio synthesis, time series imputation and forecasting. In this paper, we propose Diffusion-TS, a novel diffusion-based framework that generates... | Xinyu Yuan, Yan Qiao |  |
| 1878 |  |  [ARM: Refining Multivariate Forecasting with Adaptive Temporal-Contextual Learning](https://openreview.net/forum?id=JWpwDdVbaM) |  | 0 | Long-term time series forecasting (LTSF) is important for various domains but is confronted by challenges in handling the complex temporal-contextual relationships. As multivariate input models underperforming some recent univariate counterparts, we posit that the issue lies in the inefficiency of... | Jiecheng Lu, Shihao Yang, Xu Han |  |
| 1879 |  |  [Combining Axes Preconditioners through Kronecker Approximation for Deep Learning](https://openreview.net/forum?id=8j9hz8DVi8) |  | 0 | Adaptive regularization based optimization methods such as full-matrix Adagrad which use gradient second-moment information hold significant potential for fast convergence in deep neural network (DNN) training, but are memory intensive and computationally demanding for large neural nets. We develop... | ChoJui Hsieh, Devvrit, Inderjit S. Dhillon, Rohan Anil, Sai Surya Duvvuri |  |
| 1880 |  |  [Dropout-Based Rashomon Set Exploration for Efficient Predictive Multiplicity Estimation](https://openreview.net/forum?id=Sf2A2PUXO3) |  | 0 | Predictive multiplicity refers to the phenomenon in which classification tasks may admit multiple competing models that achieve almost-equally-optimal performance, yet generate conflicting outputs for individual samples. This presents significant concerns, as it can potentially result in systemic... | ChunFu Chen, Guihong Li, Hsiang Hsu, Shaohan Hu |  |
| 1881 |  |  [Copula Conformal prediction for multi-step time series prediction](https://openreview.net/forum?id=ojIJZDNIBj) |  | 0 | Accurate uncertainty measurement is a key step in building robust and reliable machine learning systems. Conformal prediction is a distribution-free uncertainty quantification framework popular for its ease of implementation, finite-sample coverage guarantees, and generality for underlying... | Rose Yu, Sophia Huiwen Sun |  |
| 1882 |  |  [Branch-GAN: Improving Text Generation with (not so) Large Language Models](https://openreview.net/forum?id=sHEJJmzBIN) |  | 0 | The current advancements in open domain text generation have been spearheaded by Transformer-based large language models. Leveraging efficient parallelization and vast training datasets, these models achieve unparalleled text generation capabilities. Even so, current models are known to suffer from... | Erik Hillbom, Fredrik Carlsson, Joakim Nivre, Johan Broberg, Magnus Sahlgren |  |
| 1883 |  |  [Effective Structural Encodings via Local Curvature Profiles](https://openreview.net/forum?id=GIUjLsDP4Z) |  | 0 | Structural and Positional Encodings can significantly improve the performance of Graph Neural Networks in downstream tasks. Recent literature has begun to systematically investigate differences in the structural properties that these approaches encode, as well as performance trade-offs between... | Lukas Fesser, Melanie Weber |  |
| 1884 |  |  [Domain Randomization via Entropy Maximization](https://openreview.net/forum?id=GXtmuiVrOM) |  | 0 | Varying dynamics parameters in simulation is a popular Domain Randomization (DR) approach for overcoming the reality gap in Reinforcement Learning (RL). Nevertheless, DR heavily hinges on the choice of the sampling distribution of the dynamics parameters, since high variability is crucial to... | Carlo D'Eramo, Gabriele Tiboni, Georgia Chalvatzaki, Jan Peters, Pascal Klink, Tatiana Tommasi |  |
| 1885 |  |  [OMNI: Open-endedness via Models of human Notions of Interestingness](https://openreview.net/forum?id=AgM3MzT99c) |  | 0 | Open-ended algorithms aim to learn new, interesting behaviors forever. That requires a vast environment search space, but there are thus infinitely many possible tasks. Even after filtering for tasks the current agent can learn (i.e., learning progress), countless learnable yet uninteresting tasks... | Jeff Clune, Jenny Zhang, Joel Lehman, Kenneth O. Stanley |  |
| 1886 |  |  [Machine Unlearning for Image-to-Image Generative Models](https://openreview.net/forum?id=9hjVoPWPnh) |  | 0 | Machine unlearning has emerged as a new paradigm to deliberately forget data samples from a given model in order to adhere to stringent regulations. However, existing machine unlearning methods have been primarily focused on classification models, leaving the landscape of unlearning for generative... | ChunFu Chen, Guihong Li, Hsiang Hsu, Radu Marculescu |  |
| 1887 |  |  [Classification with Conceptual Safeguards](https://openreview.net/forum?id=t8cBsT9mcg) |  | 0 | We propose a new approach to promote safety in classification tasks with concept annotations. Our approach – called a \*conceptual safeguard\* – acts as a verification layer for models that predict a target outcome by first predicting the presence of intermediate concepts. Given this architecture,... | Berk Ustun, Charles T. Marx, Hailey Joren |  |
| 1888 |  |  [Linear attention is (maybe) all you need (to understand Transformer optimization)](https://openreview.net/forum?id=0uI5415ry7) |  | 0 | Transformer training is notoriously difficult, requiring a careful design of optimizers and use of various heuristics. We make progress towards understanding the subtleties of training Transformers by carefully studying a simple yet canonical linearized \*shallow\* Transformer model. Specifically,... | Ali Jadbabaie, Chulhee Yun, Kwangjun Ahn, Minhak Song, Suvrit Sra, Xiang Cheng |  |
| 1889 |  |  [MVDream: Multi-view Diffusion for 3D Generation](https://openreview.net/forum?id=FUgrjq2pbB) |  | 0 | We introduce MVDream, a diffusion model that is able to generate consistent multi-view images from a given text prompt. Learning from both 2D and 3D data, a multi-view diffusion model can achieve the generalizability of 2D diffusion models and the consistency of 3D renderings. We demonstrate that... | Jianglong Ye, Kejie Li, Long Mai, Peng Wang, Xiao Yang, Yichun Shi |  |
| 1890 |  |  [Robust Model Based Reinforcement Learning Using L1 Adaptive Control](https://openreview.net/forum?id=GaLCLvJaoF) |  | 0 | We introduce $\mathcal{L}_1$-MBRL, a control-theoretic augmentation scheme for Model-Based Reinforcement Learning (MBRL) algorithms. Unlike model-free approaches, MBRL algorithms learn a model of the transition function using data and use it to design a control input. Our approach generates a... | Aditya Gahlawat, Minjun Sung, Naira Hovakimyan, Sambhu H. Karumanchi |  |
| 1891 |  |  [Headless Language Models: Learning without Predicting with Contrastive Weight Tying](https://openreview.net/forum?id=ONPECq0Rk7) |  | 0 | Self-supervised pre-training of language models usually consists in predicting probability distributions over extensive token vocabularies. In this study, we propose an innovative method that shifts away from probability prediction and instead focuses on reconstructing input embeddings in a... | Benoît Sagot, Nathan Godey, Éric Villemonte de la Clergerie |  |
| 1892 |  |  [Leveraging Optimization for Adaptive Attacks on Image Watermarks](https://openreview.net/forum?id=O9PArxKLe1) |  | 0 | Untrustworthy users can misuse image generators to synthesize high-quality deepfakes and engage in unethical activities. Watermarking deters misuse by marking generated content with a hidden message, enabling its detection using a secret watermarking key. A core security property of watermarking is... | Abdulrahman Diaa, Florian Kerschbaum, Lucas Fenaux, Nils Lukas |  |
| 1893 |  |  [ZeRO++: Extremely Efficient Collective Communication for Large Model Training](https://openreview.net/forum?id=gx2BT0a9MQ) |  | 0 | Zero Redundancy Optimizer (ZeRO) has been used to train a wide range of large language models on massive GPU clusters due to its ease of use, efficiency, and good scalability. However, when training on low-bandwidth clusters, and/or when small batch size per GPU is used, ZeRO’s effective throughput... | Connor Holmes, Feng Yan, Guanhua Wang, Heyang Qin, Lei Yang, Olatunji Ruwase, Sam Ade Jacobs, Samyam Rajbhandari, Xiaoxia Wu, Yuxiong He, Zhewei Yao |  |
| 1894 |  |  [Large Language Models Cannot Self-Correct Reasoning Yet](https://openreview.net/forum?id=IkmD3fKBPQ) |  | 0 | Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology,... | Adams Wei Yu, Denny Zhou, Huaixiu Steven Zheng, Jie Huang, Swaroop Mishra, Xinying Song, Xinyun Chen |  |
| 1895 |  |  [Copilot4D: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion](https://openreview.net/forum?id=Psl75UCoZM) |  | 0 | Learning world models can teach an agent how the world works in an unsupervised manner. Even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models... | Lunjun Zhang, Raquel Urtasun, Rui Hu, Sergio Casas, Yuwen Xiong, Ze Yang |  |
| 1896 |  |  [Universal Backdoor Attacks](https://openreview.net/forum?id=3QkzYBSWqL) |  | 0 | Web-scraped datasets are vulnerable to data poisoning, which can be used for backdooring deep image classifiers during training. Since training on large datasets is expensive, a model is trained once and reused many times. Unlike adversarial examples, backdoor attacks often target specific classes... | Benjamin Schneider, Florian Kerschbaum, Nils Lukas |  |
| 1897 |  |  [The Joint Effect of Task Similarity and Overparameterization on Catastrophic Forgetting - An Analytical Model](https://openreview.net/forum?id=u3dHl287oB) |  | 0 | In continual learning, catastrophic forgetting is affected by multiple aspects of the tasks. Previous works have analyzed separately how forgetting is affected by either task similarity or overparameterization. In contrast, our paper examines how task similarity and overparameterization jointly... | Daniel Goldfarb, Daniel Soudry, Itay Evron, Nir Weinberger, Paul Hand |  |
| 1898 |  |  [GenCorres: Consistent Shape Matching via Coupled Implicit-Explicit Shape Generative Models](https://openreview.net/forum?id=dGH4kHFKFj) |  | 0 | This paper introduces GenCorres, a novel unsupervised joint shape matching (JSM) approach. Our key idea is to learn a mesh generator to fit an unorganized deformable shape collection while constraining deformations between adjacent synthetic shapes to preserve geometric structures such as local... | Bo Sun, Chandrajit L. Bajaj, Haitao Yang, Qixing Huang, Xiangru Huang |  |
| 1899 |  |  [MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback](https://openreview.net/forum?id=jp3gWrMuIZ) |  | 0 | To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools. However, current evaluation protocols often emphasize benchmark performance with single-turn exchanges, neglecting the nuanced interactions among... | Hao Peng, Heng Ji, Jiateng Liu, Lifan Yuan, Xingyao Wang, Yangyi Chen, Zihan Wang |  |
| 1900 |  |  [Masked Audio Generation using a Single Non-Autoregressive Transformer](https://openreview.net/forum?id=Ny8NiVfi95) |  | 0 | We introduce MAGNeT, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, MAGNeT is comprised of a single-stage, non-autoregressive transformer. During training, we predict spans of masked tokens obtained from a masking... | Alexandre Défossez, Alon Ziv, Felix Kreuk, Gabriel Synnaeve, Gaël Le Lan, Itai Gat, Jade Copet, Tal Remez, Yossi Adi |  |
| 1901 |  |  [Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions](https://openreview.net/forum?id=gT5hALch9z) |  | 0 | Training large language models to follow instructions makes them perform better on a wide range of tasks and generally become more helpful. However, a perfectly helpful model will follow even the most malicious instructions and readily generate harmful content. In this paper, we raise concerns over... | Dan Jurafsky, Federico Bianchi, Giuseppe Attanasio, James Zou, Mirac Suzgun, Paul Röttger, Tatsunori Hashimoto |  |
| 1902 |  |  [Learning to Solve Bilevel Programs with Binary Tender](https://openreview.net/forum?id=PsDFgTosqb) |  | 0 | Bilevel programs (BPs) find a wide range of applications in fields such as energy, transportation, and machine learning. As compared to BPs with continuous (linear/convex) optimization problems in both levels, the BPs with discrete decision variables have received much less attention, largely due... | Bo Zhou, Ruiwei Jiang, Siqian Shen |  |
| 1903 |  |  [Manifold Diffusion Fields](https://openreview.net/forum?id=BZtEthuXRF) |  | 0 | We present Manifold Diffusion Fields (MDF), an approach that unlocks learning of diffusion models of data in general non-euclidean geometries. Leveraging insights from spectral geometry analysis, we define an intrinsic coordinate system on the manifold via the eigen-functions of the... | Ahmed A. A. Elhag, Joshua M. Susskind, Miguel Ángel Bautista, Yuyang Wang |  |
| 1904 |  |  [Neur2RO: Neural Two-Stage Robust Optimization](https://openreview.net/forum?id=T5Xb0iGCCv) |  | 0 | Robust optimization provides a mathematical framework for modeling and solving decision-making problems under worst-case uncertainty. This work addresses two-stage robust optimization (2RO) problems (also called \*adjustable robust optimization\*), wherein first-stage and second-stage decisions are... | Elias Boutros Khalil, Esther Julien, Jannis Kurtz, Justin Dumouchelle |  |
| 1905 |  |  [Efficient local linearity regularization to overcome catastrophic overfitting](https://openreview.net/forum?id=SZzQz8ikwg) |  | 0 | Catastrophic overfitting (CO) in single-step adversarial training (AT) results in abrupt drops in the adversarial test accuracy (even down to $0$%). For models trained with multi-step AT, it has been observed that the loss function behaves locally linearly with respect to the input, this is however... | Elías AbadRocamora, Fanghui Liu, Grigorios Chrysos, Pablo M. Olmos, Volkan Cevher |  |
| 1906 |  |  [Compressing Latent Space via Least Volume](https://openreview.net/forum?id=jFJPd9kIiF) |  | 0 | This paper introduces Least Volume---a simple yet effective regularization inspired by geometric intuition---that can reduce the necessary number of latent dimensions needed by an autoencoder without requiring any prior knowledge of the intrinsic dimensionality of the dataset. We show that the... | Mark D. Fuge, Qiuyi Chen |  |
| 1907 |  |  [Parameter-Efficient Multi-Task Model Fusion with Partial Linearization](https://openreview.net/forum?id=iynRvVVAmH) |  | 0 | Large pre-trained models have enabled significant advances in machine learning and served as foundation components. Model fusion methods, such as task arithmetic, have been proven to be powerful and scalable to incorporate fine-tuned weights from different tasks into a multi-task model. However,... | Anke Tang, Bo Du, Dacheng Tao, Han Hu, Li Shen, Yibing Zhan, Yixin Chen, Yong Luo |  |
| 1908 |  |  [Habitat 3.0: A Co-Habitat for Humans, Avatars, and Robots](https://openreview.net/forum?id=4znwzG92CE) |  | 0 | We present Habitat 3.0: a simulation platform for studying collaborative human-robot tasks in home environments. Habitat 3.0 offers contributions across three dimensions: (1) Accurate humanoid simulation: addressing challenges in modeling complex deformable bodies and diversity in appearance and... | Akshara Rai, Alexander Clegg, Andrew Szot, Devendra Singh Chaplot, Dhruv Batra, Eric Undersander, Jitendra Malik, John M. Turner, Michal Hlavac, Mikael Dallaire Cote, Mrinal Kalakrishnan, Oleksandr Maksymets, Roozbeh Mottaghi, Ruslan Partsey, Ruta Desai, So Yeon Min, Théophile Gervet, TsungYen Yang, Unnat Jain, VincentPierre Berges, Vladimir Vondrus, Xavier Puig, Zsolt Kira |  |
| 1909 |  |  [Let's Verify Step by Step](https://openreview.net/forum?id=v8L0pN6EOi) |  | 0 | In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback... | Bowen Baker, Harrison Edwards, Hunter Lightman, Ilya Sutskever, Jan Leike, John Schulman, Karl Cobbe, Teddy Lee, Vineet Kosaraju, Yuri Burda |  |
| 1910 |  |  [Masked Completion via Structured Diffusion with White-Box Transformers](https://openreview.net/forum?id=PvyOYleymy) |  | 0 | Modern learning frameworks often train deep neural networks with massive amounts of unlabeled data to learn representations by solving simple pretext tasks, then use the representations as foundations for downstream tasks. These networks are empirically designed; as such, they are usually not... | Druv Pai, Sam Buchanan, Yaodong Yu, Yi Ma, Ziyang Wu |  |
| 1911 |  |  [A Recipe for Improved Certifiable Robustness](https://openreview.net/forum?id=qz3mcn99cu) |  | 0 | Recent studies have highlighted the potential of Lipschitz-based methods for training certifiably robust neural networks against adversarial attacks. A key challenge, supported both theoretically and empirically, is that robustness demands greater network capacity and more data than standard... | Kai Hu, Klas Leino, Matt Fredrikson, Zifan Wang |  |
| 1912 |  |  [MEND: Meta Demonstration Distillation for Efficient and Effective In-Context Learning](https://openreview.net/forum?id=2Y5kBPtU0o) |  | 0 | Large Language models (LLMs) have demonstrated impressive in-context learning (ICL) capabilities, where a LLM makes predictions for a given test input together with a few input-output pairs (demonstrations). Nevertheless, the inclusion of demonstrations poses a challenge, leading to a quadratic... | Chenlei Guo, Kyumin Lee, Sixing Lu, Xiaohu Liu, Xiyao Ma, Yichuan Li |  |
| 1913 |  |  [The LLM Surgeon](https://openreview.net/forum?id=DYIIRgwg2i) |  | 0 | State-of-the-art language models are becoming increasingly large in an effort to achieve the highest performance on large corpora of available textual data. However, the sheer size of the Transformer architectures makes it difficult to deploy models within computational, environmental or... | Markus Nagel, Mart van Baalen, Tijmen Blankevoort, Tycho F. A. van der Ouderaa |  |
| 1914 |  |  [Perceptual Scales Predicted by Fisher Information Metrics](https://openreview.net/forum?id=z7K2faBrDG) |  | 0 | Perception is often viewed as a process that transforms physical variables, external to an observer, into internal psychological variables. Such a process can be modeled by a function coined \*perceptual scale\*. The \*perceptual scale\* can be deduced from psychophysical measurements that consist... | Jonathan Vacher, Pascal Mamassian |  |
| 1915 |  |  [Language Model Inversion](https://openreview.net/forum?id=t9dWHpGkPj) |  | 0 | Given a prompt, language models produce a distribution over all possible next tokens; when the prompt is unknown, can we use this distributional information to recover the prompt? We consider the problem of anguage model inversion and show that next-token probabilities contain a surprising amount... | Alexander M. Rush, John X. Morris, Justin T. Chiu, Vitaly Shmatikov, Wenting Zhao |  |
| 1916 |  |  [Decomposed Diffusion Sampler for Accelerating Large-Scale Inverse Problems](https://openreview.net/forum?id=DsEhqQtfAG) |  | 0 | Krylov subspace, which is generated by multiplying a given vector by the matrix of a linear transformation and its successive powers, has been extensively studied in classical optimization literature to design algorithms that converge quickly for large linear inverse problems. For example, the... | Hyungjin Chung, Jong Chul Ye, Suhyeon Lee |  |
| 1917 |  |  [Democratizing Fine-grained Visual Recognition with Large Language Models](https://openreview.net/forum?id=c7DND1iIgb) |  | 0 | Identifying subordinate-level categories from images is a longstanding task in computer vision and is referred to as fine-grained visual recognition (FGVR). It has tremendous significance in real-world applications since an average layperson does not excel at differentiating species of birds or... | Elisa Ricci, Mingxuan Liu, Nicu Sebe, Subhankar Roy, Wenjing Li, Zhun Zhong |  |
| 1918 |  |  [AlpaGasus: Training a Better Alpaca with Fewer Data](https://openreview.net/forum?id=FdVXgSJhvz) |  | 0 | Large language models~(LLMs) strengthen instruction-following capability through instruction-finetuning (IFT) on supervised instruction/response data. However, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly contain many low-quality instances with incorrect or irrelevant responses,... | Hai Wang, Heng Huang, Hongxia Jin, Jun Yan, Kalpa Gunaratna, Lichang Chen, Shiyang Li, Tianyi Zhou, Vijay Srinivasan, Vikas Yadav, Zheng Tang |  |
| 1919 |  |  [General Graph Random Features](https://openreview.net/forum?id=viftsX50Rt) |  | 0 | We propose a novel random walk-based algorithm for unbiased estimation of arbitrary functions of a weighted adjacency matrix, coined general graph random features (g-GRFs). This includes many of the most popular examples of kernels defined on the nodes of a graph. Our algorithm enjoys subquadratic... | Adrian Weller, Eli Berger, Isaac Reid, Krzysztof Marcin Choromanski |  |
| 1920 |  |  [HyperAttention: Long-context Attention in Near-Linear Time](https://openreview.net/forum?id=Eh0Od2BJIM) |  | 0 | We present an approximate attention mechanism named \`HyperAttention\` to address the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs). Recent work suggests that in the worst-case scenario, the quadratic time is necessary unless the... | Amin Karbasi, Amir Zandieh, David P. Woodruff, Insu Han, Rajesh Jayaram, Vahab Mirrokni |  |
| 1921 |  |  [Repelling Random Walks](https://openreview.net/forum?id=31IOmrnoP4) |  | 0 | We present a novel quasi-Monte Carlo mechanism to improve graph-based sampling, coined repelling random walks. By inducing correlations between the trajectories of an interacting ensemble such that their marginal transition probabilities are unmodified, we are able to explore the graph more... | Adrian Weller, Eli Berger, Isaac Reid, Krzysztof Marcin Choromanski |  |
| 1922 |  |  [Stabilizing Backpropagation Through Time to Learn Complex Physics](https://openreview.net/forum?id=bozbTTWcaw) |  | 0 | Of all the vector fields surrounding the minima of recurrent learning setups, the gradient field with its exploding and vanishing updates appears a poor choice for optimization, offering little beyond efficient computability. We seek to improve this suboptimal practice in the context of physics... | Nils Thuerey, Patrick Schnell |  |
| 1923 |  |  [Learning in reverse causal strategic environments with ramifications on two sided markets](https://openreview.net/forum?id=vEfmVS5ywF) |  | 0 | Motivated by equilibrium models of labor markets, we develop a formulation of causal strategic classification in which strategic agents can directly manipulate their outcomes. As an application, we consider employers that seek to anticipate the strategic response of a labor force when developing a... | Seamus Somerstep, Yaacov Ritov, Yuekai Sun |  |
| 1924 |  |  [Horizon-free Reinforcement Learning in Adversarial Linear Mixture MDPs](https://openreview.net/forum?id=aPNwsJgnZJ) |  | 0 | Recent studies have shown that the regret of reinforcement learning (RL) can be polylogarithmic in the planning horizon $H$. However, it remains an open question whether such a result holds for adversarial RL. In this paper, we answer this question affirmatively by proposing the first horizon-free... | Jiafan He, Kaixuan Ji, Qingyue Zhao, Quanquan Gu, Weitong Zhang |  |
| 1925 |  |  [Backdoor Federated Learning by Poisoning Backdoor-Critical Layers](https://openreview.net/forum?id=AJBGSVSTT2) |  | 0 | Federated learning (FL) has been widely deployed to enable machine learning training on sensitive data across distributed devices. However, the decentralized learning paradigm and heterogeneity of FL further extend the attack surface for backdoor attacks. Existing FL attack and defense... | Hao Wang, Haomin Zhuang, Jian Li, Mingxian Yu, Xu Yuan, Yang Hua |  |
| 1926 |  |  [RECOMBINER: Robust and Enhanced Compression with Bayesian Implicit Neural Representations](https://openreview.net/forum?id=VkWbxFrCC8) |  | 0 | COMpression with Bayesian Implicit NEural Representations (COMBINER) is a recent data compression method that addresses a key inefficiency of previous Implicit Neural Representation (INR)-based approaches: it avoids quantization and enables direct optimization of the rate-distortion performance.... | Gergely Flamich, Jiajun He, José Miguel HernándezLobato, Zongyu Guo |  |
| 1927 |  |  [Light-MILPopt: Solving Large-scale Mixed Integer Linear Programs with Lightweight Optimizer and Small-scale Training Dataset](https://openreview.net/forum?id=2oWRumm67L) |  | 0 | Machine Learning (ML)-based optimization approaches emerge as a promising technique for solving large-scale Mixed Integer Linear Programs (MILPs). However, existing ML-based frameworks suffer from high model computation complexity, weak problem reduction, and reliance on large-scale optimizers and... | Hongyan Wang, Hua Xu, Huigen Ye |  |
| 1928 |  |  [SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression](https://openreview.net/forum?id=Q1u25ahSuy) |  | 0 | Recent advances in large language model (LLM) pretraining have led to high-quality LLMs with impressive abilities. By compressing such LLMs via quantization to 3-4 bits per parameter, they can fit into memory-limited devices such as laptops and mobile phones, enabling personalized use. Quantizing... | Alexander Borzunov, Dan Alistarh, Denis Kuznedelev, Elias Frantar, Ruslan Svirschevski, Saleh Ashkboos, Tim Dettmers, Torsten Hoefler, Vage Egiazarian |  |
| 1929 |  |  [Towards Understanding Sycophancy in Language Models](https://openreview.net/forum?id=tvhaxkMKAn) |  | 0 | Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in... | Amanda Askell, Da Yan, David Duvenaud, Esin Durmus, Ethan Perez, Kamal Ndousse, Meg Tong, Miranda Zhang, Mrinank Sharma, Nicholas Schiefer, Oliver Rausch, Sam McCandlish, Samuel R. Bowman, Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Tomasz Korbak, Zac HatfieldDodds |  |
| 1930 |  |  [Network Memory Footprint Compression Through Jointly Learnable Codebooks and Mappings](https://openreview.net/forum?id=1RrOtCmuKr) |  | 0 | The massive interest in deep neural networks (DNNs) for both computer vision and natural language processing has been sparked by the growth in computational power. However, this led to an increase in the memory footprint, to a point where it can be challenging to simply load a model on commodity... | Arnaud Dapogny, Edouard Yvinec, Kevin Bailly |  |
| 1931 |  |  [Towards image compression with perfect realism at ultra-low bitrates](https://openreview.net/forum?id=ktdETU9JBg) |  | 0 | Image codecs are typically optimized to trade-off bitrate vs. distortion metrics. At low bitrates, this leads to compression artefacts which are easily perceptible, even when training with perceptual or adversarial losses. To improve image quality and remove dependency on the bitrate we propose to... | Jakob Verbeek, Marlène Careil, Matthew J. Muckley, Stéphane Lathuilière |  |
| 1932 |  |  [Scaling Laws of RoPE-based Extrapolation](https://openreview.net/forum?id=JO7k0SJ5V6) |  | 0 | The extrapolation capability of Large Language Models (LLMs) based on Rotary Position Embedding \citep{su2021roformer} is currently a topic of considerable interest. The mainstream approach to addressing extrapolation with LLMs involves modifying RoPE by replacing 10000, the rotary base of... | Chenxin An, Dahua Lin, Hang Yan, Xiaoran Liu, Xipeng Qiu |  |
| 1933 |  |  [V-DETR: DETR with Vertex Relative Position Encoding for 3D Object Detection](https://openreview.net/forum?id=NDkpxG94sF) |  | 0 | We introduce a highly performant 3D object detector for point clouds using the DETR framework. The prior attempts all end up with suboptimal results because they fail to learn accurate inductive biases from the limited scale of training data. In particular, the queries often attend to points that... | Baining Guo, Chunyu Wang, Han Hu, Nanning Zheng, Yichao Shen, Yuhui Yuan, Yutong Lin, Ze Liu, Zigang Geng |  |
| 1934 |  |  [A Branching Decoder for Set Generation](https://openreview.net/forum?id=riNuqYiD66) |  | 0 | Generating a set of text is a common challenge for many NLP applications, for example, automatically providing multiple keyphrases for a document to facilitate user reading. Existing generative models use a sequential decoder that generates a single sequence successively, and the set generation... | Gengyang Xiao, Gong Cheng, Yu Gu, Zixian Huang |  |
| 1935 |  |  [Data Filtering Networks](https://openreview.net/forum?id=KAk6ngZ09F) |  | 0 | Large training sets have become a cornerstone of machine learning and are the foundation for recent advances in language modeling and multimodal learning. While data curation for pre-training is often still ad-hoc, one common paradigm is to first collect a massive pool of data from the Web and then... | Albin Madappally Jose, Alex Fang, Alexander T. Toshev, Amit Jain, Ludwig Schmidt, Vaishaal Shankar |  |
| 1936 |  |  [Multi-task Learning with 3D-Aware Regularization](https://openreview.net/forum?id=TwBY17Hgiy) |  | 0 | Deep neural networks have become the standard solution for designing models that can perform multiple dense computer vision tasks such as depth estimation and semantic segmentation thanks to their ability to capture complex correlations in high dimensional feature space across tasks. However, the... | Ales Leonardis, Hakan Bilen, Steven McDonagh, WeiHong Li |  |
| 1937 |  |  [Efficient Streaming Language Models with Attention Sinks](https://openreview.net/forum?id=NG7sS51zVF) |  | 0 | Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory.... | Beidi Chen, Guangxuan Xiao, Mike Lewis, Song Han, Yuandong Tian |  |
| 1938 |  |  [OWL: A Large Language Model for IT Operations](https://openreview.net/forum?id=SZOQ9RKYJu) |  | 0 | With the rapid advancement of IT operations, managing and analyzing large data volumes efficiently for practical applications has become increasingly critical. Natural Language Processing (NLP) techniques have demonstrated remarkable capabilities in various tasks, including named entity... | Bo Zhang, Chao Chen, Dongfeng Zhang, Hongcheng Guo, Jiaheng Liu, Jian Yang, Jiaqi Bai, Junran Peng, Ke Xu, Liangfan Zheng, Linzheng Chai, Liqun Yang, Tieqiao Zheng, Xiaorong Hu, Xu Shi, Zhoujun Li |  |
| 1939 |  |  [DMBP: Diffusion model-based predictor for robust offline reinforcement learning against state observation perturbations](https://openreview.net/forum?id=ZULjcYLWKe) |  | 0 | Offline reinforcement learning (RL), which aims to fully explore offline datasets for training without interaction with environments, has attracted growing recent attention. A major challenge for the real-world application of offline RL stems from the robustness against state observation... | Yunjian Xu, Zhihe Yang |  |
| 1940 |  |  [Generative Sliced MMD Flows with Riesz Kernels](https://openreview.net/forum?id=VdkGRV1vcf) |  | 0 | Maximum mean discrepancy (MMD) flows suffer from high computational costs in large scale computations. In this paper, we show that MMD flows with Riesz kernels $K(x,y) = - \\|x-y\\|^r$, $r \in (0,2)$ have exceptional properties which allow their efficient computation. We prove that the MMD of Riesz... | Christian Wald, Fabian Altekrüger, Johannes Hertrich, Paul Hagemann |  |
| 1941 |  |  [ARGS: Alignment as Reward-Guided Search](https://openreview.net/forum?id=shgx0eqdw6) |  | 0 | Aligning large language models with human objectives is paramount, yet common approaches including RLHF suffer from unstable and resource-intensive training. In response to this challenge, we introduce ARGS, Alignment as Reward-Guided Search, a novel framework that integrates alignment into the... | Jirayu Burapacheep, Maxim Khanov, Yixuan Li |  |
| 1942 |  |  [Compositional Preference Models for Aligning LMs](https://openreview.net/forum?id=tiiAzqi6Ol) |  | 0 | As language models (LMs) become more capable, it is increasingly important to align them with human preferences. However, the dominant paradigm for training Preference Models (PMs) for that purpose suffers from fundamental limitations, such as lack of transparency and scalability, along with... | Dongyoung Go, Germán Kruszewski, Jos Rozen, Marc Dymetman, Tomasz Korbak |  |
| 1943 |  |  [Label-free Node Classification on Graphs with Large Language Models (LLMs)](https://openreview.net/forum?id=hESD2NJFg8) |  | 0 | In recent years, there have been remarkable advancements in node classification achieved by Graph Neural Networks (GNNs). However, they necessitate abundant high-quality labels to ensure promising performance. In contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency on... | Haitao Mao, Haiyang Zhang, Haoyu Han, Hongzhi Wen, Hui Liu, Jiliang Tang, Wei Jin, Zhikai Chen |  |
| 1944 |  |  [Sliced Wasserstein Estimation with Control Variates](https://openreview.net/forum?id=StYc4hQAEi) |  | 0 | The sliced Wasserstein (SW) distances between two probability measures are defined as the expectation of the Wasserstein distance between two one-dimensional projections of the two measures. The randomness comes from a projecting direction that is used to project the two input measures to one... | Khai Nguyen, Nhat Ho |  |
| 1945 |  |  [Neural Rate Control for Learned Video Compression](https://openreview.net/forum?id=42lcaojZug) |  | 0 | The learning-based video compression method has made significant progress in recent years, exhibiting promising compression performance compared with traditional video codecs. However, prior works have primarily focused on advanced compression architectures while neglecting the rate control... | Guo Lu, Jing Wang, Li Song, Shen Wang, Yibo Shi, Yiwei Zhang, Yunuo Chen |  |
| 1946 |  |  [Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts](https://openreview.net/forum?id=aZH1dM3GOX) |  | 0 | Multi-Task Reinforcement Learning (MTRL) tackles the long-standing problem of endowing agents with skills that generalize across a variety of problems. To this end, sharing representations plays a fundamental role in capturing both unique and common characteristics of the tasks. Tasks may exhibit... | Ahmed Hendawy, Carlo D'Eramo, Jan Peters |  |
| 1947 |  |  [Local Composite Saddle Point Optimization](https://openreview.net/forum?id=kklwv4c4dI) |  | 0 | Distributed optimization (DO) approaches for saddle point problems (SPP) have recently gained in popularity due to the critical role they play in machine learning (ML). Existing works mostly target smooth unconstrained objectives in Euclidean space, whereas ML problems often involve constraints or... | Brian Bullins, Site Bai |  |
| 1948 |  |  [Towards Poisoning Fair Representations](https://openreview.net/forum?id=YLJs4mKJCF) |  | 0 | Fair machine learning seeks to mitigate model prediction bias against certain demographic subgroups such as elder and female. Recently, fair representation learning (FRL) trained by deep neural networks has demonstrated superior performance, whereby representations containing no demographic... | Feijie Wu, Haoyu Wang, Hengtong Zhang, Jing Gao, Lu Su, Pan Li, Tianci Liu |  |
| 1949 |  |  [Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing](https://openreview.net/forum?id=nFMS6wF2xq) |  | 0 | Conditional diffusion models have exhibited superior performance in high-fidelity text-guided visual generation and editing. Nevertheless, prevailing text-guided visual diffusion models primarily focus on incorporating text-visual relationships exclusively into the reverse process, often... | Bin Cui, Jingwei Liu, Ling Yang, Minkai Xu, Stefano Ermon, Zhaochen Yu, Zhilong Zhang |  |
| 1950 |  |  [Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK Approach](https://openreview.net/forum?id=1op5YGZu8X) |  | 0 | Adversarial training (AT) is a canonical method for enhancing the robustness of deep neural networks (DNNs). However, recent studies empirically demonstrated that it suffers from robust overfitting, i.e., a long time AT can be detrimental to the robustness of DNNs. This paper presents a theoretical... | Di Wang, Shaopeng Fu |  |
| 1951 |  |  [What's in a Prior? Learned Proximal Networks for Inverse Problems](https://openreview.net/forum?id=kNPcOaqC5r) |  | 0 | Proximal operators are ubiquitous in inverse problems, commonly appearing as part of algorithmic strategies to regularize problems that are otherwise ill-posed. Modern deep learning models have been brought to bear for these tasks too, as in the framework of plug-and-play or deep unrolling, where... | Jeremias Sulam, Sam Buchanan, Zhenghan Fang |  |
| 1952 |  |  [Fantastic Generalization Measures are Nowhere to be Found](https://openreview.net/forum?id=NkmJotfL42) |  | 0 | We study the notion of a generalization bound being _uniformly tight_, meaning that the difference between the bound and the population loss is small for all learning algorithms and all population distributions. Numerous generalization bounds have been proposed in the literature as potential... | Ido Nachum, Jonathan Shafer, Michael Gastpar, Thomas Weinberger |  |
| 1953 |  |  [Towards Enhancing Time Series Contrastive Learning: A Dynamic Bad Pair Mining Approach](https://openreview.net/forum?id=K2c04ulKXn) |  | 0 | \*Not all positive pairs are beneficial to time series contrastive learning\*. In this paper, we study two types of bad positive pairs that can impair the quality of time series representation learned through contrastive learning: the noisy positive pair and the faulty positive pair. We observe... | Hanshu Yan, Mengling Feng, Shenda Hong, Xiang Lan |  |
| 1954 |  |  [Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework](https://openreview.net/forum?id=fUGhVYPVRM) |  | 0 | Connectionist Temporal Classification (CTC) is a widely used criterion for training supervised sequence-to-sequence (seq2seq) models. It learns the alignments between the input and output sequences by marginalizing over the perfect alignments (that yield the ground truth), at the expense of the... | Amnon Shashua, Ayana Shenhav, David Zar, Eliya Segev, Jacob Bitterman, Maya Alroy, Noam Wies, Oren Tadmor, Ronen Katsir, Tal Rosenwein, Yael BenOren |  |
| 1955 |  |  [Decoding Natural Images from EEG for Object Recognition](https://openreview.net/forum?id=dhLIno8FmH) |  | 0 | Electroencephalography (EEG) signals, known for convenient non-invasive acquisition but low signal-to-noise ratio, have recently gained substantial attention due to the potential to decode natural images. This paper presents a self-supervised framework to demonstrate the feasibility of learning... | Bingchuan Liu, Nanlin Shi, Xiang Li, Xiaorong Gao, Yijun Wang, Yonghao Song |  |
| 1956 |  |  [LCOT: Linear Circular Optimal Transport](https://openreview.net/forum?id=49z97Y9lMq) |  | 0 | The optimal transport problem for measures supported on non-Euclidean spaces has recently gained ample interest in diverse applications involving representation learning. In this paper, we focus on circular probability measures, i.e., probability measures supported on the unit circle, and introduce... | Gustavo K. Rohde, Ivan Vladimir Medri, Kangbai Yan, Rocio Diaz Martin, Soheil Kolouri, Xinran Liu, Yikun Bai |  |
| 1957 |  |  [Unveiling the Pitfalls of Knowledge Editing for Large Language Models](https://openreview.net/forum?id=fNktD3ib16) |  | 0 | As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger... | Huajun Chen, Mengru Wang, Ningyu Zhang, Xi Chen, Yunzhi Yao, Zhoubo Li |  |
| 1958 |  |  [InstructPix2NeRF: Instructed 3D Portrait Editing from a Single Image](https://openreview.net/forum?id=XIxhINXtQk) |  | 0 | With the success of Neural Radiance Field (NeRF) in 3D-aware portrait editing, a variety of works have achieved promising results regarding both quality and 3D consistency. However, these methods heavily rely on per-prompt optimization when handling natural language as editing instructions. Due to... | Jianhui Li, Jianmin Li, Jinghui Xu, Jun Zhu, Kaiwen Zheng, Shilong Liu, Yikai Wang, Zidong Liu |  |
| 1959 |  |  [Decongestion by Representation: Learning to Improve Economic Welfare in Marketplaces](https://openreview.net/forum?id=coIaBY8EVF) |  | 0 | Congestion is a common failure mode of markets, where consumers compete inefficiently on the same subset of goods (e.g., chasing the same small set of properties on a vacation rental platform). The typical economic story is that prices decongest by balancing supply and demand. But in modern online... | David C. Parkes, Gali Noti, Nir Rosenfeld, Omer Nahum |  |
| 1960 |  |  [DiffEnc: Variational Diffusion with a Learned Encoder](https://openreview.net/forum?id=8nxy1bQWTG) |  | 0 | Diffusion models may be viewed as hierarchical variational autoencoders (VAEs) with two improvements: parameter sharing for the conditionals in the generative process and efficient computation of the loss as independent terms over the hierarchy. We consider two changes to the diffusion model that... | Anders Christensen, Andrea Dittadi, Beatrix Miranda Ginn Nielsen, Ole Winther |  |
| 1961 |  |  [Amortized Network Intervention to Steer the Excitatory Point Processes](https://openreview.net/forum?id=8g26Yv1EOu) |  | 0 | Excitatory point processes (i.e., event flows) occurring over dynamic graphs (i.e., evolving topologies) provide a fine-grained model to capture how discrete events may spread over time and space. How to effectively steer the event flows by modifying the dynamic graph structures presents an... | Shuang Li, Wendi Ren, Zitao Song |  |
| 1962 |  |  [Fast and unified path gradient estimators for normalizing flows](https://openreview.net/forum?id=zlkXLb3wpF) |  | 0 | Recent work shows that path gradient estimators for normalizing flows have lower variance compared to standard estimators, resulting in improved training. However, they are often prohibitively more expensive from a computational point of view and cannot be applied to maximum likelihood training in... | Lorenz Richter, Lorenz Vaitl, Ludwig Winkler, Pan Kessel |  |
| 1963 |  |  [BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation](https://openreview.net/forum?id=gC6JTEU3jl) |  | 0 | Large language models (LLMs) have demonstrated outstanding performance in various tasks, such as text summarization, text question-answering, and etc. While their performance is impressive, the computational footprint due to their vast number of parameters can be prohibitive. Existing solutions... | Fengwei An, Kaipeng Zhang, Mengzhao Chen, Peng Gao, Peng Xu, Ping Luo, Shitao Tang, Wenqi Shao, Yu Qiao |  |
| 1964 |  |  [Learning invariant representations of time-homogeneous stochastic dynamical systems](https://openreview.net/forum?id=twSnZwiOIm) |  | 0 | We consider the general class of time-homogeneous stochastic dynamical systems, both discrete and continuous, and study the problem of learning a representation of the state that faithfully captures its dynamics. This is instrumental to learning the transfer operator or the generator of the system,... | Karim Lounici, Massimiliano Pontil, Pietro Novelli, Riccardo Grazzi, Vladimir R. Kostic |  |
| 1965 |  |  [Learning Nash Equilibria in Rank-1 Games](https://openreview.net/forum?id=8utTlmhw8v) |  | 0 | Learning Nash equilibria (NE) in games has garnered significant attention, particularly in the context of training Generative Adversarial Networks (GANs) and multi-agent Reinforcement Learning. The current state-of-the-art in efficiently learning games focuses on landscapes that meet the (weak)... | Ioannis Panageas, Nikolas Patris |  |
| 1966 |  |  [EControl: Fast Distributed Optimization with Compression and Error Control](https://openreview.net/forum?id=lsvlvWB9vz) |  | 0 | Modern distributed training relies heavily on communication compression to reduce the communication overhead. In this work, we study algorithms employing a popular class of contractive compressors in order to reduce communication overhead. However, the naive implementation often leads to unstable... | Rustem Islamov, Sebastian U. Stich, Yuan Gao |  |
| 1967 |  |  [State Representation Learning Using an Unbalanced Atlas](https://openreview.net/forum?id=cWdAYDLmPa) |  | 0 | The manifold hypothesis posits that high-dimensional data often lies on a lower-dimensional manifold and that utilizing this manifold as the target space yields more efficient representations. While numerous traditional manifold-based techniques exist for dimensionality reduction, their application... | Anis Yazidi, Li Meng, Morten Goodwin, Paal E. Engelstad |  |
| 1968 |  |  [Retro-fallback: retrosynthetic planning in an uncertain world](https://openreview.net/forum?id=dl0u4ODCuW) |  | 0 | Retrosynthesis is the task of planning a series of chemical reactions to create a desired molecule from simpler, buyable molecules. While previous works have proposed algorithms to find optimal solutions for a range of metrics (e.g. shortest, lowest-cost), these works generally overlook the fact... | Austin Tripp, José Miguel HernándezLobato, Krzysztof Maziarz, Marwin H. S. Segler, Sarah Lewis |  |
| 1969 |  |  [ACRF: Compressing Explicit Neural Radiance Fields via Attribute Compression](https://openreview.net/forum?id=POFrdKvpea) |  | 0 | In this work, we study the problem of explicit NeRF compression. Through analyzing recent explicit NeRF models, we reformulate the task of explicit NeRF compression as 3D data compression. We further introduce our NeRF compression framework, Attributed Compression of Radiance Field (ACRF), which... | Guangchi Fang, Longguang Wang, Qingyong Hu, Yulan Guo |  |
| 1970 |  |  [LiDAR-PTQ: Post-Training Quantization for Point Cloud 3D Object Detection](https://openreview.net/forum?id=0d1gQI114C) |  | 0 | Due to highly constrained computing power and memory, deploying 3D lidar-based detectors on edge devices equipped in autonomous vehicles and robots poses a crucial challenge. Being a convenient and straightforward model compression approach, Post-Training Quantization (PTQ) has been widely adopted... | Bo Zhang, Liang Li, Miao Sun, Shipeng Bai, Sifan Zhou, Xiangxiang Chu, Xiaobo Lu, Xinyu Zhang, Ziyu Zhao |  |
| 1971 |  |  [Optimal transport based adversarial patch to leverage large scale attack transferability](https://openreview.net/forum?id=nZP10evtkV) |  | 0 | Adversarial patch attacks, where a small patch is placed in the scene to fool neural networks, have been studied for numerous applications. Focusing on image classification, we consider the setting of a black-box transfer attack where an attacker does not know the target model. Instead of forcing... | Adrien ChanHonTong, Milad LeyliAbadi, Pol Labarbarie, Stéphane Herbin |  |
| 1972 |  |  [Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance Accompaniment](https://openreview.net/forum?id=GW4j4n2cjH) |  | 0 | We introduce a novel task within the field of human motion generation, termed dance accompaniment, which necessitates the generation of responsive movements from a dance partner, the "follower", synchronized with the lead dancer’s movements and the underlying musical rhythm. Unlike existing solo or... | Chen Change Loy, Henghui Ding, Lei Yang, Li Siyao, Tianpei Gu, Zhengyu Lin, Zhitao Yang, Ziwei Liu |  |
| 1973 |  |  [STREAM: Spatio-TempoRal Evaluation and Analysis Metric for Video Generative Models](https://openreview.net/forum?id=7JfKCZQPxJ) |  | 0 | Image generative models have made significant progress in generating realistic and diverse images, supported by comprehensive guidance from various evaluation metrics. However, current video generative models struggle to generate even short video clips, with limited tools that provide insights for... | Jaejun Yoo, Pum Jun Kim, Seojun Kim |  |
| 1974 |  |  [Accelerated Sampling with Stacked Restricted Boltzmann Machines](https://openreview.net/forum?id=kXNJ48Hvw1) |  | 0 | Sampling complex distributions is an important but difficult objective in various fields, including physics, chemistry, and statistics. An improvement of standard Monte Carlo (MC) methods, intensively used in particular in the context of disordered systems, is Parallel Tempering, also called... | Clément Roussel, Jorge FernandezdeCossíoDiaz, Rémi Monasson, Simona Cocco |  |
| 1975 |  |  [Image Inpainting via Tractable Steering of Diffusion Models](https://openreview.net/forum?id=NSIVHTbZBR) |  | 0 | Diffusion models are the current state of the art for generating photorealistic images. Controlling the sampling process for constrained image generation tasks such as inpainting, however, remains challenging since exact conditioning on such constraints is intractable. While existing methods use... | Anji Liu, Guy Van den Broeck, Mathias Niepert |  |
| 1976 |  |  [QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models](https://openreview.net/forum?id=WvFoJccpo8) |  | 0 | Recently years have witnessed a rapid development of large language models (LLMs). Despite the strong ability in many language-understanding tasks, the heavy computational burden largely restricts the application of LLMs especially when one needs to deploy them onto edge devices. In this paper, we... | Heng Chang, Hengheng Zhang, Lingxi Xie, Qi Tian, Xiaopeng Zhang, Xiaotao Gu, Xin Chen, Yuhui Xu, Zhengsu Chen |  |
| 1977 |  |  [R-MAE: Regions Meet Masked Autoencoders](https://openreview.net/forum?id=ba84RDHFnz) |  | 0 | In this work, we explore regions as a potential visual analogue of words for self-supervised image representation learning. Inspired by Masked Autoencoding (MAE), a generative pre-training baseline, we propose masked region autoencoding to learn from groups of pixels or regions. Specifically, we... | Alexander Kirillov, Cees G. M. Snoek, DuyKien Nguyen, Martin R. Oswald, Vaibhav Aggarwal, Xinlei Chen, Yanghao Li |  |
| 1978 |  |  [Transformer-Modulated Diffusion Models for Probabilistic Multivariate Time Series Forecasting](https://openreview.net/forum?id=qae04YACHs) |  | 0 | Transformers have gained widespread usage in multivariate time series (MTS) forecasting, delivering impressive performance. Nonetheless, these existing transformer-based methods often neglect an essential aspect: the incorporation of uncertainty into the predicted series, which holds significant... | Baolin Sun, Bo Chen, Mingyuan Zhou, Wenchao Chen, Xinyue Hu, Yuxin Li |  |
| 1979 |  |  [OpenChat: Advancing Open-source Language Models with Mixed-Quality Data](https://openreview.net/forum?id=AOJyfhWYHf) |  | 0 | Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally,... | Guan Wang, Sen Song, Sijie Cheng, Xiangang Li, Xianyuan Zhan, Yang Liu |  |
| 1980 |  |  [In-Context Learning Learns Label Relationships but Is Not Conventional Learning](https://openreview.net/forum?id=YPIA7bgd5y) |  | 0 | The predictions of Large Language Models (LLMs) on downstream tasks often improve significantly when including examples of the input–label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works. For example, while Xie et... | Jannik Kossen, Tom Rainforth, Yarin Gal |  |
| 1981 |  |  [Human Motion Diffusion as a Generative Prior](https://openreview.net/forum?id=dTpbEdN9kr) |  | 0 | Recent work has demonstrated the significant potential of denoising diffusion models for generating human motion, including text-to-motion capabilities. However, these methods are restricted by the paucity of annotated motion data, a focus on single-person motions, and a lack of detailed control.... | Amit Haim Bermano, Guy Tevet, Roy Kapon, Yoni Shafir |  |
| 1982 |  |  [New Insight of Variance reduce in Zero-Order Hard-Thresholding: Mitigating Gradient Error and Expansivity Contradictions](https://openreview.net/forum?id=fjf3YenThE) |  | 0 | Hard-thresholding is an important type of algorithm in machine learning that is used to solve $\ell_0$ constrained optimization problems. However, the true gradient of the objective function can be difficult to access in certain scenarios, which normally can be approximated by zeroth-order (ZO)... | Bin Gu, Huan Xiong, William de Vazelhes, Xinzhe Yuan |  |
| 1983 |  |  [AdaMerging: Adaptive Model Merging for Multi-Task Learning](https://openreview.net/forum?id=nZP6NgD3QY) |  | 0 | Multi-task learning (MTL) aims to empower a model to tackle multiple tasks simultaneously. A recent development known as task arithmetic has revealed that several models, each fine-tuned for distinct tasks, can be directly merged into a single model to execute MTL without necessitating a retraining... | Dacheng Tao, Enneng Yang, Guibing Guo, Li Shen, Shiwei Liu, Xingwei Wang, Zhenyi Wang |  |
| 1984 |  |  [Self-Supervised High Dynamic Range Imaging with Multi-Exposure Images in Dynamic Scenes](https://openreview.net/forum?id=jjiOHEcS2c) |  | 0 | Merging multi-exposure images is a common approach for obtaining high dynamic range (HDR) images, with the primary challenge being the avoidance of ghosting artifacts in dynamic scenes. Recent methods have proposed using deep neural networks for deghosting. However, the methods typically rely on... | Haoyu Wang, Lei Lei, Shuai Liu, Wangmeng Zuo, Xiaotao Wang, Zhilu Zhang |  |
| 1985 |  |  [MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use](https://openreview.net/forum?id=R0c2qtalgG) |  | 0 | Large language models (LLMs) have garnered significant attention due to their impressive natural language processing (NLP) capabilities. Recently, many studies have focused on the tool utilization ability of LLMs. They primarily investigated how LLMs effectively collaborate with given specific... | Chenrui Fan, Jiawen Shi, Lichao Sun, Neil Zhenqiang Gong, Pan Zhou, Qihui Zhang, Siyuan Wu, Yao Wan, Yixin Liu, Yuan Li, Yue Huang |  |
| 1986 |  |  [Composed Image Retrieval with Text Feedback via Multi-grained Uncertainty Regularization](https://openreview.net/forum?id=Yb5KvPkKQg) |  | 0 | We investigate composed image retrieval with text feedback. Users gradually look for the target of interest by moving from coarse to fine-grained feedback. However, existing methods merely focus on the latter, i.e., fine-grained search, by harnessing positive and negative pairs during training.... | Leigang Qu, TatSeng Chua, Wei Ji, Yiyang Chen, Zhedong Zheng |  |
| 1987 |  |  [Mean Field Theory in Deep Metric Learning](https://openreview.net/forum?id=ZPdZLlNXSm) |  | 0 | In this paper, we explore the application of mean field theory, a technique from statistical physics, to deep metric learning and address the high training complexity commonly associated with conventional metric learning loss functions. By adapting mean field theory for deep metric learning, we... | Takuya Furusawa |  |
| 1988 |  |  [Implicit Neural Representation Inference for Low-Dimensional Bayesian Deep Learning](https://openreview.net/forum?id=5KUiMKRebi) |  | 0 | Bayesian inference is the standard for providing full predictive distributions with well calibrated uncertainty estimates. However, scaling to a modern, overparameterized deep learning setting typically comes at the cost of severe and restrictive approximations, sacrificing model predictive... | Christophoros Nikou, Giorgos Sfikas, Panagiotis Dimitrakopoulos |  |
| 1989 |  |  [Going Beyond Neural Network Feature Similarity: The Network Feature Complexity and Its Interpretation Using Category Theory](https://openreview.net/forum?id=4bSQ3lsfEV) |  | 0 | The behavior of neural networks still remains opaque, and a recently widely noted phenomenon is that networks often achieve similar performance when initialized with different random parameters. This phenomenon has attracted significant attention in measuring the similarity between features learned... | Junchi Yan, Yiting Chen, Zhanpeng Zhou |  |
| 1990 |  |  [M3C: A Framework towards Convergent, Flexible, and Unsupervised Learning of Mixture Graph Matching and Clustering](https://openreview.net/forum?id=AXC9KydyZq) |  | 0 | Existing graph matching methods typically assume that there are similar structures between graphs and they are matchable. This work addresses a more realistic scenario where graphs exhibit diverse modes, requiring graph grouping before or along with matching, a task termed mixture graph matching... | Jiaxin Lu, Junchi Yan, Tianzhe Wang, Zetian Jiang |  |
| 1991 |  |  [AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models](https://openreview.net/forum?id=y33lDRBgWI) |  | 0 | This paper considers a ubiquitous problem underlying several applications of DPMs, i.e., optimizing the parameters of DPMs when the objective is a differentiable metric defined on the generated contents. Since the sampling procedure of DPMs involves recursive calls to the denoising UNet, naive... | Hanshu Yan, Jiachun Pan, Jiashi Feng, Jun Hao Liew, Vincent Y. F. Tan |  |
| 1992 |  |  [LLaMA-Adapter: Efficient Fine-tuning of Large Language Models with Zero-initialized Attention](https://openreview.net/forum?id=d4UiXAHN2W) |  | 0 | With the rising tide of large language models (LLMs), there has been a growing interest in developing general-purpose instruction-following models, e.g., ChatGPT. To this end, we present LLaMA-Adapter, a lightweight adaption method for efficient instruction tuning of LLaMA. Using 52K self-instruct... | Aojun Zhou, Chris Liu, Hongsheng Li, Jiaming Han, Pan Lu, Peng Gao, Renrui Zhang, Yu Qiao |  |
| 1993 |  |  [Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation](https://openreview.net/forum?id=UbxWjq0UO2) |  | 0 | Text-to-3D generation has shown rapid progress in recent days with the advent of score distillation sampling (SDS), a methodology of using pretrained text-to-2D diffusion models to optimize a neural radiance field (NeRF) in a zero-shot setting. However, the lack of 3D awareness in the 2D diffusion... | Inès Hyeonsu Kim, Jaehoon Ko, JinHwa Kim, Jiyoung Lee, Junho Kim, Junyoung Seo, Minseop Kwak, Seungryong Kim, Wooseok Jang |  |
| 1994 |  |  [SparseFormer: Sparse Visual Recognition via Limited Latent Tokens](https://openreview.net/forum?id=2pvECsmld3) |  | 0 | Human visual recognition is a sparse process, where only a few salient visual cues are attended to rather than every detail being traversed uniformly. However, most current vision networks follow a dense paradigm, processing every single visual unit (such as pixels or patches) in a uniform manner.... | Limin Wang, Mike Zheng Shou, Zhan Tong, Ziteng Gao |  |
| 1995 |  |  [Demystifying Local & Global Fairness Trade-offs in Federated Learning Using Partial Information Decomposition](https://openreview.net/forum?id=SBj2Qdhgew) |  | 0 | This work presents an information-theoretic perspective to group fairness trade-offs in federated learning (FL) with respect to sensitive attributes, such as gender, race, etc. Existing works often focus on either $\textit{global fairness}$ (overall disparity of the model across all clients) or... | Faisal Hamman, Sanghamitra Dutta |  |
| 1996 |  |  [Protein-Ligand Interaction Prior for Binding-aware 3D Molecule Diffusion Models](https://openreview.net/forum?id=qH9nrMNTIW) |  | 0 | Generating 3D ligand molecules that bind to specific protein targets via diffusion models has shown great promise for structure-based drug design. The key idea is to disrupt molecules into noise through a fixed forward process and learn its reverse process to generate molecules from noise in a... | Bin Cui, Jie Chen, Ling Yang, Wenming Yang, Wentao Zhang, Xiangxin Zhou, Xiawu Zheng, Yu Wang, Zhilin Huang, Zhilong Zhang |  |
| 1997 |  |  [Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph](https://openreview.net/forum?id=nnVO1PvbTv) |  | 0 | Although large language models (LLMs) have achieved significant success in various tasks, they often struggle with hallucination problems, especially in scenarios requiring deep and responsible reasoning. These issues could be partially addressed by introducing external knowledge graphs (KG) in LLM... | Chen Lin, Chengjin Xu, HeungYeung Shum, Jian Guo, Jiashuo Sun, Lionel M. Ni, Lumingyuan Tang, Saizhuo Wang, Yeyun Gong |  |
| 1998 |  |  [Self-Supervised Heterogeneous Graph Learning: a Homophily and Heterogeneity View](https://openreview.net/forum?id=3FJOKjooIj) |  | 0 | Self-supervised heterogeneous graph learning has achieved promising results in various real applications, but it still suffers from the following issues: (i) meta-paths can be employed to capture the homophily in the heterogeneous graph, but meta-paths are human-defined, requiring substantial... | Feiping Nie, Heng Tao Shen, Ping Hu, Xiaofeng Zhu, Xinchao Wang, Yujie Mo, Zheng Zhang |  |
| 1999 |  |  [CoBIT: A Contrastive Bi-directional Image-Text Generation Model](https://openreview.net/forum?id=8ISRqgtjPc) |  | 0 | The field of Vision-and-Language (VL) has witnessed a proliferation of pretrained foundation models. Current techniques typically employ only one type of training objective, whether it's (1) contrastive objectives (like CLIP), (2) image-to-text generative objectives (like PaLI), or (3)... | Haoxuan You, Jason M. Baldridge, Jiahui Yu, KaiWei Chang, Mandy Guo, Zhecan Wang |  |
| 2000 |  |  [Protein Multimer Structure Prediction via Prompt Learning](https://openreview.net/forum?id=OHpvivXrQr) |  | 0 | Understanding the 3D structures of protein multimers is crucial, as they play a vital role in regulating various cellular processes. It has been empirically confirmed that the multimer structure prediction (MSP) can be well handled in a step-wise assembly fashion using provided dimer structures and... | Hong Cheng, Jia Li, Xiangguo Sun, Yu Li, Zijing Liu, Ziqi Gao |  |
| 2001 |  |  [Domain-Agnostic Molecular Generation with Chemical Feedback](https://openreview.net/forum?id=9rPyHyjfwP) |  | 0 | The generation of molecules with desired properties has become increasingly popular, revolutionizing the way scientists design molecular structures and providing valuable support for chemical and drug design. However, despite the potential of language models in molecule generation, they face... | Huajun Chen, Lingbing Guo, Ningyu Zhang, Xiaohui Fan, Yin Fang, Zhuo Chen |  |
| 2002 |  |  [LLM-grounded Video Diffusion Models](https://openreview.net/forum?id=exKHibougU) |  | 0 | Text-conditioned diffusion models have emerged as a promising tool for neural video generation. However, current models still struggle with intricate spatiotemporal prompts and often generate restricted or incorrect motion. To address these limitations, we introduce LLM-grounded Video Diffusion... | Adam Yala, Baifeng Shi, Boyi Li, Long Lian, Trevor Darrell |  |
| 2003 |  |  [Periodicity Decoupling Framework for Long-term Series Forecasting](https://openreview.net/forum?id=dp27P5HBBt) |  | 0 | Convolutional neural network (CNN)-based and Transformer-based methods have recently made significant strides in time series forecasting, which excel at modeling local temporal variations or capturing long-term dependencies. However, real-world time series usually contain intricate temporal... | Beiliang Wu, Jigang Bao, Naiqi Li, Peiyuan Liu, ShuTao Xia, Tao Dai, Yong Jiang |  |
| 2004 |  |  [Imitation Learning from Observation with Automatic Discount Scheduling](https://openreview.net/forum?id=pPJTQYOpNI) |  | 0 | Humans often acquire new skills through observation and imitation. For robotic agents, learning from the plethora of unlabeled video demonstration data available on the Internet necessitates imitating the expert without access to its action, presenting a challenge known as Imitation Learning from... | Chongjie Zhang, Chuan Wen, Weijun Dong, Yang Gao, Yingdong Hu, Yuyang Liu, ZhaoHeng Yin |  |
| 2005 |  |  [iGraphMix: Input Graph Mixup Method for Node Classification](https://openreview.net/forum?id=a2ljjXeDcE) |  | 0 | Recently, Input Mixup, which augments virtual samples by interpolating input features and corresponding labels, is one of the promising methods to alleviate the over-fitting problem on various domains including image classification and natural language processing because of its ability to generate... | Beomyoung Lee, Geonsoo Kim, Hoyeop Lee, Hyui Geon Yoon, Jongwon Jeong, Junhee Heo, Kim Jin Seon |  |
| 2006 |  |  [Noise Map Guidance: Inversion with Spatial Context for Real Image Editing](https://openreview.net/forum?id=mhgm0IXtHw) |  | 0 | Text-guided diffusion models have become a popular tool in image synthesis, known for producing high-quality and diverse images. However, their application to editing real images often encounters hurdles primarily due to the text condition deteriorating the reconstruction quality and subsequently... | Hansam Cho, Jonghyun Lee, Seoung Bum Kim, TaeHyun Oh, Yonghyun Jeong |  |
| 2007 |  |  [Label-Focused Inductive Bias over Latent Object Features in Visual Classification](https://openreview.net/forum?id=cH3oufN8Pl) |  | 0 | Most neural networks for classification primarily learn features differentiated by input-domain related information such as visual similarity of objects in an image. While this focus is natural behavior, it can inadvertently introduce an inductive bias that conflicts with unseen relations in an... | HyounYoung Bae, Ilmin Kang, Kangil Kim |  |
| 2008 |  |  [Simple Minimax Optimal Byzantine Robust Algorithm for Nonconvex Objectives with Uniform Gradient Heterogeneity](https://openreview.net/forum?id=1ii8idH4tH) |  | 0 | In this study, we consider nonconvex federated learning problems with the existence of Byzantine workers. We propose a new simple Byzantine robust algorithm called Momentum Screening. The algorithm is adaptive to the Byzantine fraction, i.e., all its hyperparameters do not depend on the number of... | Iifan Tyou, Kenta Niwa, Takumi Fukami, Tomoya Murata |  |
| 2009 |  |  [TopoMLP: A Simple yet Strong Pipeline for Driving Topology Reasoning](https://openreview.net/forum?id=0gTW5JUFTW) |  | 0 | Topology reasoning aims to comprehensively understand road scenes and present drivable routes in autonomous driving. It requires detecting road centerlines (lane) and traffic elements, further reasoning their topology relationship, \textit{i.e.}, lane-lane topology, and lane-traffic topology. In... | Dongming Wu, Fan Jia, Jiahao Chang, Jianbing Shen, Tiancai Wang, Yingfei Liu |  |
| 2010 |  |  [Personalize Segment Anything Model with One Shot](https://openreview.net/forum?id=6Gzkhoc6YS) |  | 0 | Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful promptable framework, revolutionizing the segmentation field. Despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under-explored, e.g., automatically... | Hao Dong, Hongsheng Li, Junting Pan, Peng Gao, Renrui Zhang, Shilin Yan, Yu Qiao, Zhengkai Jiang, Ziyu Guo |  |
| 2011 |  |  [Integrating Planning and Deep Reinforcement Learning via Automatic Induction of Task Substructures](https://openreview.net/forum?id=PR6RMsxuW7) |  | 0 | Despite recent advancements, deep reinforcement learning (DRL) still struggles at learning sparse-reward goal-directed tasks. Classical planning excels at addressing hierarchical tasks by employing symbolic knowledge, yet most of the methods rely on assumptions about pre-defined subtasks. To bridge... | ChiHsien Chang, JungChun Liu, ShaoHua Sun, TianLi Yu |  |
| 2012 |  |  [PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training](https://openreview.net/forum?id=3Z1gxuAQrA) |  | 0 | Large Language Models (LLMs) are trained with a pre-defined context length, restricting their use in scenarios requiring long inputs. Previous efforts for adapting LLMs to a longer length usually requires fine-tuning with this target length (Full-length fine-tuning), suffering intensive training... | Dawei Zhu, Furu Wei, Liang Wang, Nan Yang, Sujian Li, Wenhao Wu, Yifan Song |  |
| 2013 |  |  [LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents](https://openreview.net/forum?id=ADSxCpCu9s) |  | 0 | Large language models (LLMs) have recently received considerable attention as alternative solutions for task planning. However, comparing the performance of language-oriented task planners becomes difficult, and there exists a dearth of detailed exploration regarding the effects of various factors... | Hyobin Ong, Jaehong Kim, Jaewoo Choi, Minsu Jang, Youngwoo Yoon |  |
| 2014 |  |  [Progressive3D: Progressively Local Editing for Text-to-3D Content Creation with Complex Semantic Prompts](https://openreview.net/forum?id=O072Rc8uUy) |  | 0 | Recent text-to-3D generation methods achieve impressive 3D content creation capacity thanks to the advances in image diffusion models and optimizing strategies. However, current methods struggle to generate correct 3D content for a complex prompt in semantics, i.e., a prompt describing multiple... | Jian Zhang, Jianan Wang, Lei Zhang, Li Yuan, Tianyu Yang, Xinhua Cheng, Yu Li |  |
| 2015 |  |  [The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning](https://openreview.net/forum?id=wxJ0eXwwda) |  | 0 | Alignment tuning has become the de facto standard practice for enabling base large language models (LLMs) to serve as open-domain AI assistants. The alignment tuning process typically involves instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning... | Abhilasha Ravichander, Bill Yuchen Lin, Chandra Bhagavatula, Khyathi Raghavi Chandu, Melanie Sclar, Nouha Dziri, Ximing Lu, Yejin Choi |  |
| 2016 |  |  [Towards Best Practices of Activation Patching in Language Models: Metrics and Methods](https://openreview.net/forum?id=Hf17y6u9BC) |  | 0 | Mechanistic interpretability seeks to understand the internal mechanisms of machine learning models, where localization—identifying the important model components—is a key step. Activation patching, also known as causal tracing or interchange intervention, is a standard technique for this task (Vig... | Fred Zhang, Neel Nanda |  |
| 2017 |  |  [On the Analysis of GAN-based Image-to-Image Translation with Gaussian Noise Injection](https://openreview.net/forum?id=sLregLuXpn) |  | 0 | Image-to-image (I2I) translation is vital in computer vision tasks like style transfer and domain adaptation. While recent advances in GAN have enabled high-quality sample generation, real-world challenges such as noise and distortion remain significant obstacles. Although Gaussian noise injection... | Chaohua Shi, Hongqing Liu, Kexin Huang, Lu Gan, Mingrui Zhu, Nannan Wang, Xinbo Gao |  |
| 2018 |  |  [FedHyper: A Universal and Robust Learning Rate Scheduler for Federated Learning with Hypergradient Descent](https://openreview.net/forum?id=Kl9CqKf7h6) |  | 0 | The theoretical landscape of federated learning (FL) undergoes rapid evolution, but its practical application encounters a series of intricate challenges, and hyperparameter optimization is one of these critical challenges. Amongst the diverse adjustments in hyperparameters, the adaptation of the... | Ang Li, Jianyu Wang, Ziyao Wang |  |
| 2019 |  |  [FreeReg: Image-to-Point Cloud Registration Leveraging Pretrained Diffusion Models and Monocular Depth Estimators](https://openreview.net/forum?id=BPb5AhT2Vf) |  | 0 | Matching cross-modality features between images and point clouds is a fundamental problem for image-to-point cloud registration. However, due to the modality difference between images and points, it is difficult to learn robust and discriminative cross-modality features by existing metric learning... | Bing Wang, Bisheng Yang, Haiping Wang, Wenping Wang, Yuan Liu, Yujing Sun, Zhen Dong |  |
| 2020 |  |  [Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models](https://openreview.net/forum?id=3bq3jsvcQ1) |  | 0 | We present STEP-BACK PROMPTING, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide reasoning, LLMs significantly improve their abilities in... | Denny Zhou, Ed H. Chi, HengTze Cheng, Huaixiu Steven Zheng, Quoc V. Le, Swaroop Mishra, Xinyun Chen |  |
| 2021 |  |  [ImagenHub: Standardizing the evaluation of conditional image generation models](https://openreview.net/forum?id=OuV9ZrkQlc) |  | 0 | Recently, a myriad of conditional image generation and editing models have been developed to serve different downstream tasks, including text-to-image generation, text-guided image editing, subject-driven image generation, control-guided image generation, etc. However, we observe huge... | Kai Zhang, Max Ku, Tianle Li, Wenhu Chen, Wenwen Zhuang, Xingyu Fu, Yujie Lu |  |
| 2022 |  |  [UC-NERF: Neural Radiance Field for Under-Calibrated Multi-View Cameras in Autonomous Driving](https://openreview.net/forum?id=bLKcCe7hYh) |  | 0 | Multi-camera setups find widespread use across various applications, such as autonomous driving, as they greatly expand sensing capabilities. Despite the fast development of Neural radiance field (NeRF) techniques and their wide applications in both indoor and outdoor scenes, applying NeRF to... | Jin Wang, Kai Cheng, Kaixuan Wang, Wei Yin, Xiaoxiao Long, Xiaozhi Chen, Xuejin Chen, Yuexin Ma, Zhiqiang Wu |  |
| 2023 |  |  [Adapting Large Language Models via Reading Comprehension](https://openreview.net/forum?id=y886UXPEZ0) |  | 0 | We explore how continued pre-training on domain-specific corpora influences large language models, revealing that training on the raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability for question answering. Taken inspiration from human learning via reading... | Daixuan Cheng, Furu Wei, Shaohan Huang |  |
| 2024 |  |  [DIAGNOSIS: Detecting Unauthorized Data Usages in Text-to-image Diffusion Models](https://openreview.net/forum?id=f8S3aLm0Vp) |  | 0 | Recent text-to-image diffusion models have shown surprising performance in generating high-quality images. However, concerns have arisen regarding the unauthorized data usage during the training or fine-tuning process. One example is when a model trainer collects a set of images created by a... | Chen Chen, Dimitris N. Metaxas, Lingjuan Lyu, Shiqing Ma, Zhenting Wang |  |
| 2025 |  |  [LEMON: Lossless model expansion](https://openreview.net/forum?id=3Vw7DQqq7U) |  | 0 | Scaling of deep neural networks, especially Transformers, is pivotal for their surging performance and has further led to the emergence of sophisticated reasoning capabilities in foundation models. Such scaling generally requires training large models from scratch with random initialization,... | Cong Xie, Haibin Lin, Hanlin Lu, Hongxia Yang, Jiahao Su, Jianbo Yuan, Ruoyu Sun, Tianyi Liu, Yite Wang |  |
| 2026 |  |  [A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation](https://openreview.net/forum?id=Js5PJPHDyY) |  | 0 | Contrastive Language-Image Pretraining (CLIP) has gained popularity for its remarkable zero-shot capacity. Recent research has focused on developing efficient fine-tuning methods, such as prompt learning and adapter, to enhance CLIP's performance in downstream tasks. However, these methods still... | Jian Liang, Lijun Sheng, Ran He, Tieniu Tan, Zhengbo Wang, Zilei Wang |  |
| 2027 |  |  [MiniLLM: Knowledge Distillation of Large Language Models](https://openreview.net/forum?id=5h0qf7IBZZ) |  | 0 | Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to... | Furu Wei, Li Dong, Minlie Huang, Yuxian Gu |  |
| 2028 |  |  [Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation](https://openreview.net/forum?id=Vja3ecieXY) |  | 0 | Fine-tuning is essential to adapting pre-trained large language models to downstream applications. With the increasing popularity of LLM-enabled applications, fine-tuning has been performed intensively worldwide, incurring a tremendous amount of computing costs that correspond to big carbon... | Hanyun Yin, Heng Huang, Kai Huang, Wei Gao |  |
| 2029 |  |  [The importance of feature preprocessing for differentially private linear optimization](https://openreview.net/forum?id=XlTDBZFXWp) |  | 0 | Training machine learning models with differential privacy (DP) has received increasing interest in recent years. One of the most popular algorithms for training differentially private models is differentially private stochastic gradient descent (DPSGD) and its variants, where at each step... | Aditya Krishna Menon, Ananda Theertha Suresh, Ziteng Sun |  |
| 2030 |  |  [Pathformer: Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting](https://openreview.net/forum?id=lJkOCMP2aW) |  | 0 | Transformers for time series forecasting mainly model time series from limited or fixed scales, making it challenging to capture different characteristics spanning various scales. We propose Pathformer, a multi-scale Transformer with adaptive pathways. It integrates both temporal resolution and... | Bin Yang, Chenjuan Guo, Peng Chen, Qingsong Wen, Yang Shu, Yihang Wang, Yingying Zhang, Yunyao Cheng |  |
| 2031 |  |  [Tree Cross Attention](https://openreview.net/forum?id=Vw24wtSddM) |  | 0 | Cross Attention is a popular method for retrieving information from a set of context tokens for making predictions. At inference time, for each prediction, Cross Attention scans the full set of $\mathcal{O}(N)$ tokens. In practice, however, often only a small subset of tokens are required for good... | Frederick Tung, Hossein Hajimirsadeghi, Leo Feng, Mohamed Osama Ahmed, Yoshua Bengio |  |
| 2032 |  |  [LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models](https://openreview.net/forum?id=gLARhFLE0F) |  | 0 | Recent advances in self-supervised learning and the Transformer architecture have significantly improved natural language processing (NLP), achieving remarkably low perplexity. However, the growing size of NLP models introduces a memory wall problem during the generation phase. To mitigate this... | Baeseong Park, Beomseok Kwon, Byeongwook Kim, Dongsoo Lee, Gunho Park, Jeonghoon Kim, Minsub Kim, Se Jung Kwon, Sungjae Lee, Youngjoo Lee |  |
| 2033 |  |  [Outliers with Opposing Signals Have an Outsized Effect on Neural Network Optimization](https://openreview.net/forum?id=kIZ3S3tel6) |  | 0 | We identify a new phenomenon in neural network optimization which arises from the interaction of depth and a particular heavy-tailed structure in natural data. Our result offers intuitive explanations for several previously reported observations about network training dynamics, including a... | Andrej Risteski, Elan Rosenfeld |  |
| 2034 |  |  [Stable Anisotropic Regularization](https://openreview.net/forum?id=dbQH9AOVd5) |  | 0 | Given the success of Large Language Models (LLMs), there has been considerable interest in studying the properties of model activations. The literature overwhelmingly agrees that LLM representations are dominated by a few \`\`outlier dimensions'' with exceedingly high variance and magnitude.... | Carsten Eickhoff, William Rudman |  |
| 2035 |  |  [Threshold-Consistent Margin Loss for Open-World Deep Metric Learning](https://openreview.net/forum?id=vE5MyzpP92) |  | 0 | Existing losses used in deep metric learning (DML) for image retrieval often lead to highly non-uniform intra-class and inter-class representation structures across test classes and data distributions. When combined with the common practice of using a fixed threshold to declare a match, this gives... | Joseph Tighe, Jun Fang, Linghan Xu, Qin Zhang, Qingming Tang, Yifan Xing, Ying Nian Wu |  |
| 2036 |  |  [Jointly Training Large Autoregressive Multimodal Models](https://openreview.net/forum?id=5jcav5RcKw) |  | 0 | In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To... | Armen Aghajanyan, Barlas Oguz, Emanuele Aiello, Lili Yu, Yixin Nie |  |
| 2037 |  |  [Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in RL](https://openreview.net/forum?id=pDCublKPmG) |  | 0 | Most existing works focus on direct perturbations to the victim's state/action or the underlying transition dynamics to demonstrate the vulnerability of reinforcement learning agents to adversarial attacks. However, such direct manipulations may not be always realizable. In this paper, we consider... | Furong Huang, Souradip Chakraborty, Xiangyu Liu, Yanchao Sun |  |
| 2038 |  |  [On the Over-Memorization During Natural, Robust and Catastrophic Overfitting](https://openreview.net/forum?id=2V1Z0Jdmss) |  | 0 | Overfitting negatively impacts the generalization ability of deep neural networks (DNNs) in both natural and adversarial training. Existing methods struggle to consistently address different types of overfitting, typically designing strategies that focus separately on either natural or adversarial... | Bo Han, Chaojian Yu, Runqi Lin, Tongliang Liu |  |
| 2039 |  |  [The Generative AI Paradox: "What It Can Create, It May Not Understand"](https://openreview.net/forum?id=CF8H8MS5P8) |  | 0 | The recent wave of generative AI has sparked unprecedented global attention, with both excitement and concern over potentially superhuman levels of artificial intelligence: models now take only seconds to produce outputs that would challenge or exceed the capabilities even of expert humans. At the... | Abhilasha Ravichander, Allyson Ettinger, Benjamin Newman, Faeze Brahman, Jena D. Hwang, Jillian Fisher, Khyathi Raghavi Chandu, Linjie Li, Liwei Jiang, Nouha Dziri, Pang Wei Koh, Peter West, Ximing Lu, Yejin Choi |  |
| 2040 |  |  [Semantic Flow: Learning Semantic Fields of Dynamic Scenes from Monocular Videos](https://openreview.net/forum?id=A2mRcRyGdl) |  | 0 | In this work, we pioneer Semantic Flow, a neural semantic representation of dynamic scenes from monocular videos. In contrast to previous NeRF methods that reconstruct dynamic scenes from the colors and volume densities of individual points, Semantic Flow learns semantics from continuous flows that... | Angtian Wang, Fengrui Tian, Jianfei Guo, Shaoyi Du, Yueqi Duan |  |
| 2041 |  |  [Revisiting Link Prediction: a data perspective](https://openreview.net/forum?id=8Ur2xmuw7w) |  | 0 | Link prediction, a fundamental task on graphs, has proven indispensable in various applications, e.g., friend recommendation, protein analysis, and drug interaction prediction. However, since datasets span a multitude of domains, they could have distinct underlying mechanisms of link formation.... | Bingheng Li, Haitao Mao, Harry Shomer, Jiliang Tang, Juanhui Li, Neil Shah, Tong Zhao, Wenqi Fan, Yao Ma |  |
| 2042 |  |  [Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding](https://openreview.net/forum?id=4L0xnS4GQM) |  | 0 | Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification. Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both... | ChenYu Lee, ChunLiang Li, Hao Zhang, Jingbo Shang, Julian Martin Eisenschlos, Lesly Miculicich, Tomas Pfister, Vincent Perot, Yasuhisa Fujii, Zifeng Wang, Zilong Wang |  |
| 2043 |  |  [Denoising Diffusion Bridge Models](https://openreview.net/forum?id=FKksTayvGo) |  | 0 | Diffusion models are powerful generative models that map noise to data using stochastic processes. However, for many applications such as image editing, the model input comes from a distribution that is not random noise. As such, diffusion models must rely on cumbersome methods like guidance or... | Aaron Lou, Linqi Zhou, Samar Khanna, Stefano Ermon |  |
| 2044 |  |  [Incremental Randomized Smoothing Certification](https://openreview.net/forum?id=SdeAPV1irk) |  | 0 | Randomized smoothing-based certification is an effective approach for obtaining robustness certificates of deep neural networks (DNNs) against adversarial attacks. This method constructs a smoothed DNN model and certifies its robustness through statistical sampling, but it is computationally... | Debangshu Banerjee, Gagandeep Singh, Sasa Misailovic, Shubham Ugare, Tarun Suresh |  |
| 2045 |  |  [Local Graph Clustering with Noisy Labels](https://openreview.net/forum?id=89A5c6enfc) |  | 0 | The growing interest in machine learning problems over graphs with additional node information such as texts, images, or labels has popularized methods that require the costly operation of processing the entire graph. Yet, little effort has been made to the development of fast local methods (i.e.... | Artur Back de Luca, Kimon Fountoulakis, Shenghao Yang |  |
| 2046 |  |  [Principled Federated Domain Adaptation: Gradient Projection and Auto-Weighting](https://openreview.net/forum?id=6J3ehSUrMU) |  | 0 | Federated Domain Adaptation (FDA) describes the federated learning (FL) setting where source clients and a server work collaboratively to improve the performance of a target client where limited data is available. The domain shift between the source and target domains, coupled with limited data of... | Enyi Jiang, Sanmi Koyejo, Yibo Jacky Zhang |  |
| 2047 |  |  [GraphPulse: Topological representations for temporal graph property prediction](https://openreview.net/forum?id=DZqic2sPTY) |  | 0 | Many real-world networks evolve over time, and predicting the evolution of such networks remains a challenging task. Graph Neural Networks (GNNs) have shown empirical success for learning on static graphs, but they lack the ability to effectively learn from nodes and edges with different... | Baris Coskunuzer, Cuneyt Gurcan Akcora, Farimah Poursafaei, Kiarash Shamsi, Shenyang Huang, Tran Gia Bao Ngo |  |
| 2048 |  |  [Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs](https://openreview.net/forum?id=xZDWO0oejD) |  | 0 | In human-written articles, we often leverage the subtleties of text style, such as bold and italics, to guide the attention of readers. These textual emphases are vital for the readers to grasp the conveyed information. When interacting with large language models (LLMs), we have a similar need --... | Bin Yu, Chandan Singh, Jianfeng Gao, Liyuan Liu, Qingru Zhang, Tuo Zhao, Xiaodong Liu |  |
| 2049 |  |  [PRIME: Prioritizing Interpretability in Failure Mode Extraction](https://openreview.net/forum?id=QrEHs9w5UF) |  | 0 | In this work, we study the challenge of providing human-understandable descriptions for failure modes in trained image classification models. Existing works address this problem by first identifying clusters (or directions) of incorrectly classified samples in a latent space and then aiming to... | Keivan Rezaei, Mazda Moayeri, Mehrdad Saberi, Soheil Feizi |  |
| 2050 |  |  [On gauge freedom, conservativity and intrinsic dimensionality estimation in diffusion models](https://openreview.net/forum?id=92KV9xAMhF) |  | 0 | Diffusion models are generative models that have recently demonstrated impressive performances in terms of sampling quality and density estimation in high dimensions. They rely on a forward continuous diffusion process and a backward continuous denoising process, which can be described by a... | Christian Horvat, JeanPascal Pfister |  |
| 2051 |  |  [Domain constraints improve risk prediction when outcome data is missing](https://openreview.net/forum?id=1mNFsbvo2P) |  | 0 | Machine learning models are often trained to predict the outcome resulting from a human decision. For example, if a doctor decides to test a patient for disease, will the patient test positive? A challenge is that historical decision-making determines whether the outcome is observed: we only... | Emma Pierson, Nikhil Garg, Sidhika Balachandar |  |
| 2052 |  |  [Learning Multi-Agent Communication with Contrastive Learning](https://openreview.net/forum?id=vZZ4hhniJU) |  | 0 | Communication is a powerful tool for coordination in multi-agent RL. But inducing an effective, common language is a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered... | Biswa Sengupta, Jakob Nicolaus Foerster, Michael Noukhovitch, Yat Long Lo |  |
| 2053 |  |  [Closing the Gap between TD Learning and Supervised Learning - A Generalisation Point of View](https://openreview.net/forum?id=qg5JENs0N4) |  | 0 | Some reinforcement learning (RL) algorithms have the capability of recombining together pieces of previously seen experience to solve a task never seen before during training. This oft-sought property is one of the few ways in which dynamic programming based RL algorithms are considered different... | Benjamin Eysenbach, Glen Berseth, Matthieu Geist, Raj Ghugare |  |
| 2054 |  |  [lpNTK: Better Generalisation with Less Data via Sample Interaction During Learning](https://openreview.net/forum?id=8Ju0VmvMCW) |  | 0 | Although much research has been done on proposing new models or loss functions to improve the generalisation of artificial neural networks (ANNs), less attention has been directed to the impact of the training data on generalisation. In this work, we start from approximating the interaction between... | Kenny Smith, Shangmin Guo, Stefano V. Albrecht, Yi Ren |  |
| 2055 |  |  [Continual Learning on a Diet: Learning from Sparsely Labeled Streams Under Constrained Computation](https://openreview.net/forum?id=Xvfz8NHmCj) |  | 0 | We propose and study a realistic Continual Learning (CL) setting where learning algorithms are granted a restricted computational budget per time step while training. We apply this setting to large-scale semi-supervised Continual Learning scenarios with sparse label rate. Previous proficient CL... | Adel Bibi, Bernard Ghanem, Mohamed Elhoseiny, Philip Torr, Wenxuan Zhang, Youssef Mohamed |  |
| 2056 |  |  [Video Decomposition Prior: Editing Videos Layer by Layer](https://openreview.net/forum?id=nfMyERXNru) |  | 0 | In the evolving landscape of video editing methodologies, a majority of deep learning techniques are often reliant on extensive datasets of observed input and ground truth sequence pairs for optimal performance. Such reliance often falters when acquiring data becomes challenging, especially in... | Abhinav Shrivastava, Gaurav Shrivastava, SerNam Lim |  |
| 2057 |  |  [Beyond task performance: evaluating and reducing the flaws of large multimodal models with in-context-learning](https://openreview.net/forum?id=mMaQvkMzDi) |  | 0 | Following the success of Large Language Models (LLMs), Large Multimodal Models (LMMs), such as the Flamingo model and its subsequent competitors, have started to emerge as natural steps towards generalist agents. However, interacting with recent LMMs reveals major limitations that are hardly... | Alexandre Ramé, Corentin Dancette, Matthieu Cord, Mustafa Shukor |  |
| 2058 |  |  [Butterfly Effects of SGD Noise: Error Amplification in Behavior Cloning and Autoregression](https://openreview.net/forum?id=CgPs04l9TO) |  | 0 | This work studies training instabilities of behavior cloning with deep neural networks. We observe that minibatch SGD updates to the policy network during training result in sharp oscillations in long-horizon rewards, despite negligibly affecting the behavior cloning loss. We empirically... | Adam Block, Akshay Krishnamurthy, Cyril Zhang, Dylan J. Foster, Max Simchowitz |  |
| 2059 |  |  [On Stationary Point Convergence of PPO-Clip](https://openreview.net/forum?id=uznKlCpWjV) |  | 0 | Proximal policy optimization (PPO) has gained popularity in reinforcement learning (RL). Its PPO-Clip variant is one the most frequently implemented algorithms and is one of the first-to-try algorithms in RL tasks. This variant uses a clipped surrogate objective function not typically found in... | Baoxiang Wang, Ruinan Jin, Shuai Li |  |
| 2060 |  |  [Automatic Functional Differentiation in JAX](https://openreview.net/forum?id=gzT61ziSCu) |  | 0 | We extend JAX with the capability to automatically differentiate higher-order functions (functionals and operators). By representing functions as infinite dimensional generalization of arrays, we seamlessly use JAX's existing primitive system to implement higher-order functions. We present a set of... | Min Lin |  |
| 2061 |  |  [FedCompass: Efficient Cross-Silo Federated Learning on Heterogeneous Client Devices Using a Computing Power-Aware Scheduler](https://openreview.net/forum?id=msXxrttLOi) |  | 0 | Cross-silo federated learning offers a promising solution to collaboratively train robust and generalized AI models without compromising the privacy of local datasets, e.g., healthcare, financial, as well as scientific projects that lack a centralized data facility. Nonetheless, because of the... | Eliu A. Huerta, Gagandeep Singh, Han Chen, Kibaek Kim, Pranshu Chaturvedi, Ravi K. Madduri, Shilan He, Volodymyr V. Kindratenko, Zilinghan Li |  |
| 2062 |  |  [ADOPD: A Large-Scale Document Page Decomposition Dataset](https://openreview.net/forum?id=x1ptaXpOYa) |  | 0 | Research in document image understanding is hindered by limited high-quality document data. To address this, we introduce ADOPD, a comprehensive dataset for document page decomposition. ADOPD stands out with its data-driven approach for document taxonomy discovery during data collection,... | Ani Nenkova, Anqi Liu, Jason Kuen, Jiuxiang Gu, Lu Qi, Ruiyi Zhang, Tong Sun, Xiangxi Shi |  |
| 2063 |  |  [Provably Efficient CVaR RL in Low-rank MDPs](https://openreview.net/forum?id=9x6yrFAPnx) |  | 0 | We study risk-sensitive Reinforcement Learning (RL), where we aim to maximize the Conditional Value at Risk (CVaR) with a fixed risk tolerance $\tau$. Prior theoretical work studying risk-sensitive RL focuses on the tabular Markov Decision Processes (MDPs) setting. To extend CVaR RL to settings... | Farzan Farnia, Hofung Leung, Jason D. Lee, Wen Sun, Wenhao Zhan, Xiaoyan Hu, Yulai Zhao |  |
| 2064 |  |  [COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically for Model-Based RL](https://openreview.net/forum?id=jnFcKjtUPN) |  | 0 | Dyna-style model-based reinforcement learning contains two phases: model rollouts to generate sample for policy learning and real environment exploration using current policy for dynamics model learning. However, due to the complex real-world environment, it is inevitable to learn an imperfect... | Furong Huang, Huazhe Xu, Ruijie Zheng, Ruonan Jia, Wichayaporn Wongkamjan, Xiyao Wang, Yanchao Sun |  |
| 2065 |  |  [Can Transformers Capture Spatial Relations between Objects?](https://openreview.net/forum?id=HgZUcwFhjr) |  | 0 | Spatial relationships between objects represent key scene information for humans to understand and interact with the world. To study the capability of current computer vision systems to recognize physically grounded spatial relations, we start by proposing precise relation definitions that permit... | Chuan Wen, Dinesh Jayaraman, Yang Gao |  |
| 2066 |  |  [Plug-and-Play Posterior Sampling under Mismatched Measurement and Prior Models](https://openreview.net/forum?id=66arKkGiFy) |  | 0 | Posterior sampling has been shown to be a powerful Bayesian approach for solving imaging inverse problems. The recent plug-and-play unadjusted Langevin algorithm (PnP-ULA) has emerged as a promising method for Monte Carlo sampling and minimum mean squared error (MMSE) estimation by combining... | Andrés Almansa, Jiaming Liu, Marien Renaud, Ulugbek Kamilov, Valentin De Bortoli |  |
| 2067 |  |  [Learning Robust Generalizable Radiance Field with Visibility and Feature Augmented Point Representation](https://openreview.net/forum?id=o4CLLlIaaH) |  | 0 | This paper introduces a novel paradigm for the generalizable neural radiance field (NeRF). Previous generic NeRFs combine multiview stereo techniques with image-based neural rendering, yielding impressive results, while suffering from three issues. First, occlusions often result in inconsistent... | Jiaxu Wang, Renjing Xu, Ziyi Zhang |  |
| 2068 |  |  [Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation](https://openreview.net/forum?id=wfzXa8e783) |  | 0 | Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts. Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field. However, the intricacies of fine-tuning these models... | Bernard B. W. Yang, Giyeong Oh, ShihYing Yeh, Yanmin Gong, YuGuan Hsieh, Zhidong Gao |  |
| 2069 |  |  [Finite Scalar Quantization: VQ-VAE Made Simple](https://openreview.net/forum?id=8ishA3LxN8) |  | 0 | We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values,... | David Minnen, Eirikur Agustsson, Fabian Mentzer, Michael Tschannen |  |
| 2070 |  |  [Interpretable Meta-Learning of Physical Systems](https://openreview.net/forum?id=nnicaG5xiH) |  | 0 | Machine learning methods can be a valuable aid in the scientific process, but they need to face challenging settings where data come from inhomogeneous experimental conditions. Recent meta-learning methods have made significant progress in multi-task learning, but they rely on black-box neural... | Marc Lelarge, Matthieu Blanke |  |
| 2071 |  |  [Grokking in Linear Estimators - A Solvable Model that Groks without Understanding](https://openreview.net/forum?id=GH2LYb9XV0) |  | 0 | Grokking is the intriguing phenomenon where a model learns to generalize long after it has fit the training data. We show both analytically and numerically that grokking can surprisingly occur in linear networks performing linear tasks in a simple teacher-student setup. In this setting, the full... | Alon Beck, Noam Levi, Yohai BarSinai |  |
| 2072 |  |  [Discovering Failure Modes of Text-guided Diffusion Models via Adversarial Search](https://openreview.net/forum?id=TOWdQQgMJY) |  | 0 | Text-guided diffusion models (TDMs) are widely applied but can fail unexpectedly. Common failures include: _(i)_ natural-looking text prompts generating images with the wrong content, or _(ii)_ different random samples of the latent variables that generate vastly different, and even unrelated,... | Adam Kortylewski, Alan L. Yuille, Qihao Liu, Song Bai, Yutong Bai |  |
| 2073 |  |  [DiffAR: Denoising Diffusion Autoregressive Model for Raw Speech Waveform Generation](https://openreview.net/forum?id=GTk0AdOYLq) |  | 0 | Diffusion models have recently been shown to be relevant for high-quality speech generation. Most work has been focused on generating spectrograms, and as such, they further require a subsequent model to convert the spectrogram to a waveform (i.e., a vocoder). This work proposes a diffusion... | Joseph Keshet, Michael Elad, Roi Benita |  |
| 2074 |  |  [Statistical Rejection Sampling Improves Preference Optimization](https://openreview.net/forum?id=xbjSwwrQOe) |  | 0 | Improving the alignment of language models with human preferences remains an active research challenge. Previous approaches have primarily utilized online Reinforcement Learning from Human Feedback (RLHF). Recently, offline methods such as Sequence Likelihood Calibration (SLiC) and Direct... | Jialu Liu, Misha Khalman, Mohammad Saleh, Peter J. Liu, Rishabh Joshi, Tianqi Liu, Yao Zhao |  |
| 2075 |  |  [On the generalization capacity of neural networks during generic multimodal reasoning](https://openreview.net/forum?id=zyBJodMrn5) |  | 0 | The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared... | James R. Kozloski, Mattia Rigotti, Murray Campbell, Soham Dan, Takuya Ito |  |
| 2076 |  |  [The Devil is in the Object Boundary: Towards Annotation-free Instance Segmentation using Foundation Models](https://openreview.net/forum?id=4JbrdrHxYy) |  | 0 | Foundation models, pre-trained on a large amount of data have demonstrated impressive zero-shot capabilities in various downstream tasks. However, in object detection and instance segmentation, two fundamental computer vision tasks heavily reliant on extensive human annotations, foundation models... | Cheng Shi, Sibei Yang |  |
| 2077 |  |  [Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages](https://openreview.net/forum?id=zzqn5G9fjn) |  | 0 | Pretrained large language models (LLMs) have emerged as a cornerstone in modern natural language processing, with their utility expanding to various applications and languages. However, the fine-tuning of multilingual LLMs, particularly for low-resource languages, is fraught with challenges steming... | Hongxiang Fan, Nicholas Donald Lane, Royson Lee, Wanru Zhao, Xinchi Qiu, Yan Gao, Yihong Chen |  |
| 2078 |  |  [A Data-Driven Measure of Relative Uncertainty for Misclassification Detection](https://openreview.net/forum?id=ruGY8v10mK) |  | 0 | Misclassification detection is an important problem in machine learning, as it allows for the identification of instances where the model's predictions are unreliable. However, conventional uncertainty measures such as Shannon entropy do not provide an effective way to infer the real uncertainty... | Eduardo Dadalto Câmara Gomes, Georg Pichler, Marco Romanelli, Pablo Piantanida |  |
| 2079 |  |  [Most discriminative stimuli for functional cell type clustering](https://openreview.net/forum?id=9W6KaAcYlr) |  | 0 | Identifying cell types and understanding their functional properties is crucial for unraveling the mechanisms underlying perception and cognition. In the retina, functional types can be identified by carefully selected stimuli, but this requires expert domain knowledge and biases the procedure... | Alexander S. Ecker, Andreas S. Tolias, Jan Lause, Jonathan Oesterle, Kelli Restivo, Konstantin F. Willeke, Larissa Höfling, Matthias Bethge, Max F. Burg, Michaela Vystrcilová, Paul G. Fahey, Philipp Berens, Sarah Müller, Shashwat Sridhar, Thomas Euler, Thomas Zenkel, Tim Gollisch, Zhiwei Ding |  |
| 2080 |  |  [Biased Temporal Convolution Graph Network for Time Series Forecasting with Missing Values](https://openreview.net/forum?id=O9nZCwdGcG) |  | 0 | Multivariate time series forecasting plays an important role in various applications ranging from meteorology study, traffic management to economics planning. In the past decades, many efforts have been made toward accurate and reliable forecasting methods development under the assumption of intact... | Bo Liu, Xiaodan Chen, Xiucheng Li, Zhijun Li |  |
| 2081 |  |  [Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models](https://openreview.net/forum?id=JzG7kSpjJk) |  | 0 | Large Language Models (LLMs) have recently demonstrated a remarkable success across various tasks. However, efficiently serving LLMs has been a challenge due to its large memory bottleneck, specifically in small batch inference settings (e.g. mobile devices). Weight-only quantization can be a... | Beomseok Kwon, Byeongwook Kim, Dongsoo Lee, Jeonghoon Kim, Jung Hwan Heo, Se Jung Kwon |  |
| 2082 |  |  [DecompOpt: Controllable and Decomposed Diffusion Models for Structure-based Molecular Optimization](https://openreview.net/forum?id=Y3BbxvAQS9) |  | 0 | Recently, 3D generative models have shown promising performances in structure-based drug design by learning to generate ligands given target binding sites. However, only modeling the target-ligand distribution can hardly fulfill one of the main goals in drug discovery -- designing novel ligands... | Liang Wang, Quanquan Gu, Xiangxin Zhou, Xiwei Cheng, Yu Bao, Yuwei Yang |  |
| 2083 |  |  [Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals](https://openreview.net/forum?id=UMfcdRIotC) |  | 0 | Causal explanations of the predictions of NLP systems are essential to ensure safety and establish trust. Yet, existing methods often fall short of explaining model predictions effectively or efficiently and are often model-specific. In this paper, we address model-agnostic explanations, proposing... | Alexander Chapanin, Amir Feder, Amit Sharma, Nitay Calderon, Roi Reichart, Yair Ori Gat |  |
| 2084 |  |  [Separating common from salient patterns with Contrastive Representation Learning](https://openreview.net/forum?id=30N3bNAiw3) |  | 0 | Contrastive Analysis is a sub-field of Representation Learning that aims at separating 1) salient factors of variation - that only exist in the target dataset (i.e., diseased subjects) in contrast with 2) common factors of variation between target and background (i.e., healthy subjects) datasets.... | Antoine Grigis, Edouard Duchesnay, Pietro Gori, Robin Louiset |  |
| 2085 |  |  [Self-Supervised Contrastive Learning for Long-term Forecasting](https://openreview.net/forum?id=nBCuRzjqK7) |  | 0 | Long-term forecasting presents unique challenges due to the time and memory complexity of handling long sequences. Existing methods, which rely on sliding windows to process long sequences, struggle to effectively capture long-term variations that are partially caught within the short window (i.e.,... | Daehoon Gwak, Edward Choi, Jaegul Choo, Junwoo Park |  |
| 2086 |  |  [A Semantic Invariant Robust Watermark for Large Language Models](https://openreview.net/forum?id=6p8lpe4MNf) |  | 0 | Watermark algorithms for large language models (LLMs) have achieved extremely high accuracy in detecting text generated by LLMs. Such algorithms typically involve adding extra watermark logits to the LLM's logits at each generation step. However, prior algorithms face a trade-off between attack... | Aiwei Liu, Leyi Pan, Lijie Wen, Shiao Meng, Xuming Hu |  |
| 2087 |  |  [Fast Equilibrium of SGD in Generic Situations](https://openreview.net/forum?id=qgWJkDiI5p) |  | 0 | Normalization layers are ubiquitous in deep learning, greatly accelerating optimization. However, they also introduce many unexpected phenomena during training, for example, the Fast Equilibrium conjecture proposed by (Li et al.,2020), which states that the scale-invariant normalized network, when... | Yi Wang, Zhiren Wang, Zhiyuan Liu |  |
| 2088 |  |  [Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM](https://openreview.net/forum?id=izrOLJov5y) |  | 0 | We present Spectron, a novel approach to adapting pre-trained large language models (LLMs) to perform spoken question answering (QA) and speech continuation. By endowing the LLM with a pre-trained speech encoder, our model becomes able to take speech inputs and generate speech outputs. The entire... | Alon Levkovitch, Chulayuth Asawaroengchai, Ehud Rivlin, Eliya Nachmani, Julian Salazar, Michelle Tadmor Ramanovich, R. J. SkerryRyan, Roy Hirsch, Soroosh Mariooryad |  |
| 2089 |  |  [Transport meets Variational Inference: Controlled Monte Carlo Diffusions](https://openreview.net/forum?id=PP1rudnxiW) |  | 0 | Connecting optimal transport and variational inference, we present a principled and systematic framework for sampling and generative modelling centred around divergences on path space. Our work culminates in the development of the Controlled Monte Carlo Diffusion sampler (CMCD) for Bayesian... | Denis Blessing, Francisco Vargas, Nikolas Nüsken, Shreyas Padhy |  |
| 2090 |  |  [DAFA: Distance-Aware Fair Adversarial Training](https://openreview.net/forum?id=BRdEBlwUW6) |  | 0 | The disparity in accuracy between classes in standard training is amplified during adversarial training, a phenomenon termed the robust fairness problem. Existing methodologies aimed to enhance robust fairness by sacrificing the model's performance on easier classes in order to improve its... | Ho Bae, Hyemi Jang, Hyungyu Lee, Junsung Park, Saehyung Lee, Sungroh Yoon |  |
| 2091 |  |  [AffineQuant: Affine Transformation Quantization for Large Language Models](https://openreview.net/forum?id=of2rhALq8l) |  | 0 | The significant resource requirements associated with Large-scale Language Models (LLMs) have generated considerable interest in the development of techniques aimed at compressing and accelerating neural networks. Among these techniques, Post-Training Quantization (PTQ) has emerged as a subject of... | Fei Chao, Feng Ling, Huixia Li, Rongrong Ji, Rui Wang, Shilei Wen, Xiawu Zheng, Xuefeng Xiao, Yuexiao Ma |  |
| 2092 |  |  [Encoding Unitig-level Assembly Graphs with Heterophilous Constraints for Metagenomic Contigs Binning](https://openreview.net/forum?id=vBw8JGBJWj) |  | 0 | Metagenomics studies genomic material derived from mixed microbial communities in diverse environments, holding considerable significance for both human health and environmental sustainability. Metagenomic binning refers to the clustering of genomic subsequences obtained from high-throughput DNA... | Hansheng Xue, Lexing Xie, Vaibhav Rajan, Vijini Mallawaarachchi |  |
| 2093 |  |  [SF(DA)2: Source-free Domain Adaptation Through the Lens of Data Augmentation](https://openreview.net/forum?id=kUCgHbmO11) |  | 0 | In the face of the deep learning model's vulnerability to domain shift, source-free domain adaptation (SFDA) methods have been proposed to adapt models to new, unseen target domains without requiring access to source domain data. Although the potential benefits of applying data augmentation to SFDA... | Jonghyun Lee, Juhyeon Shin, Sungroh Yoon, Uiwon Hwang |  |
| 2094 |  |  [Mitigating the Curse of Dimensionality for Certified Robustness via Dual Randomized Smoothing](https://openreview.net/forum?id=C1sQBG6Sqp) |  | 0 | Randomized Smoothing (RS) has been proven a promising method for endowing an arbitrary image classifier with certified robustness. However, the substantial uncertainty inherent in the high-dimensional isotropic Gaussian noise imposes the curse of dimensionality on RS. Specifically, the upper bound... | Henghui Ding, Song Xia, Xudong Jiang, Yi Yu |  |
| 2095 |  |  [In-context Exploration-Exploitation for Reinforcement Learning](https://openreview.net/forum?id=uIKZSStON3) |  | 0 | In-context learning is a promising approach for online policy learning of offline reinforcement learning (RL) methods, which can be achieved at inference time without gradient optimization. However, this method is hindered by significant computational costs resulting from the gathering of large... | Federico Tomasi, Sina Ghiassian, Zhenwen Dai |  |
| 2096 |  |  [Out-of-Distribution Detection with Negative Prompts](https://openreview.net/forum?id=nanyAujl6e) |  | 0 | Out-of-distribution (OOD) detection is indispensable for open-world machine learning models. Inspired by recent success in large pre-trained language-vision models, e.g., CLIP, advanced works have achieved impressive OOD detection results by matching the \*similarity\* between image features and... | Bo Han, Jun Nie, Tongliang Liu, Xinmei Tian, Yonggang Zhang, Zhen Fang |  |
| 2097 |  |  [π2vec: Policy Representation with Successor Features](https://openreview.net/forum?id=o5Bqa4o5Mi) |  | 0 | This paper introduces $\pi$2vec, a method for representing black box policies as comparable feature vectors. Our method combines the strengths of foundation models that serve as generic and powerful state representations and successor features that can model the future occurrence of the states for... | Claudio Fantacci, Gianluca Scarpellini, Ksenia Konyushkova, Misha Denil, Thomas Paine, Yutian Chen |  |
| 2098 |  |  [Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition](https://openreview.net/forum?id=TVg6hlfsKa) |  | 0 | Recent studies show that vision models pre-trained in generic visual learning tasks with large-scale data can provide useful feature representations for a wide range of visual perception problems. However, few attempts have been made to exploit pre-trained foundation models in visual place... | Chun Yuan, Feng Lu, Lijun Zhang, Shuting Dong, Xiangyuan Lan, Yaowei Wang |  |
| 2099 |  |  [FROSTER: Frozen CLIP is A Strong Teacher for Open-Vocabulary Action Recognition](https://openreview.net/forum?id=zYXFMeHRtO) |  | 0 | In this paper, we introduce \textbf{FROSTER}, an effective framework for open-vocabulary action recognition. The CLIP model has achieved remarkable success in a range of image-based tasks, benefiting from its strong generalization capability stemming from pretaining on massive image-text pairs.... | Hao Zhou, Kai Han, Kun Yao, Xiaohu Huang |  |
| 2100 |  |  [Interventional Fairness on Partially Known Causal Graphs: A Constrained Optimization Approach](https://openreview.net/forum?id=SKulT2VX9p) |  | 0 | Fair machine learning aims to prevent discrimination against individuals or sub-populations based on sensitive attributes such as gender and race. In recent years, causal inference methods have been increasingly used in fair machine learning to measure unfairness by causal effects. However, current... | Aoqi Zuo, Mingming Gong, Susan Wei, Yiqing Li |  |
| 2101 |  |  [The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World](https://openreview.net/forum?id=c2R7ajodcI) |  | 0 | We present the All-Seeing (AS) project: a large-scale dataset and model for recognizing and understanding everything in the open world. Using a scalable data engine that incorporates human feedback and efficient models in the loop, we create a new dataset (AS-1B) with over 1.2 billion regions... | Hao Li, Jifeng Dai, Linjie Xing, Min Shi, Qingyun Li, Tong Lu, Weiyun Wang, Wenhai Wang, Xizhou Zhu, Yu Qiao, Yushi Chen, Zhe Chen, Zhenhang Huang, Zhiguo Cao |  |
| 2102 |  |  [CIFAR-10-Warehouse: Broad and More Realistic Testbeds in Model Generalization Analysis](https://openreview.net/forum?id=pw2ssoOTpo) |  | 0 | Analyzing model performance in various unseen environments is a critical research problem in the machine learning community. To study this problem, it is important to construct a testbed with out-of-distribution test sets that have broad coverage of environmental discrepancies. However, existing... | Liang Zheng, Xiaoxiao Sun, Xingjian Leng, Yang Yang, Zi Huang, Zijian Wang |  |
| 2103 |  |  [Task Planning for Visual Room Rearrangement under Partial Observability](https://openreview.net/forum?id=jJvXNpvOdM) |  | 0 | This paper presents a novel hierarchical task planner under partial observability that empowers an embodied agent to use visual input to efficiently plan a sequence of actions for simultaneous object search and rearrangement in an untidy room, to achieve a desired tidy state. The paper introduces... | Brojeshwar Bhowmick, Dipanjan Das, Karan Mirakhor, Sourav Ghosh |  |
| 2104 |  |  [Parallelizing non-linear sequential models over the sequence length](https://openreview.net/forum?id=E34AlVLN0v) |  | 0 | Sequential models, such as Recurrent Neural Networks and Neural Ordinary Differential Equations, have long suffered from slow training due to their inherent sequential nature. For many years this bottleneck has persisted, as many thought sequential models could not be parallelized. We challenge... | Joshua Selfridge, Muhammad Firmansyah Kasim, Qi Zhu, Yi Heng Lim |  |
| 2105 |  |  [Long-tailed Diffusion Models with Oriented Calibration](https://openreview.net/forum?id=NW2s5XXwXU) |  | 0 | Diffusion models are acclaimed for generating high-quality and diverse images. However, their performance notably degrades when trained on data with a long-tailed distribution. For long tail diffusion model generation, current works focus on the calibration and enhancement of the tail generation... | Huangjie Zheng, Jiangchao Yao, Mingyuan Zhou, Tianjiao Zhang, Xiangfeng Wang, Ya Zhang, Yanfeng Wang |  |
| 2106 |  |  [A Simple Romance Between Multi-Exit Vision Transformer and Token Reduction](https://openreview.net/forum?id=gJeYtRuguR) |  | 0 | Vision Transformers (ViTs) are now flourishing in the computer vision area. Despite the remarkable success, ViTs suffer from high computational costs, which greatly hinder their practical usage. Token reduction, which identifies and discards unimportant tokens during forward propagation, has then... | Dongyang Liu, Meina Kan, Shiguang Shan, Xilin Chen |  |
| 2107 |  |  [Optimal Sample Complexity for Average Reward Markov Decision Processes](https://openreview.net/forum?id=jOm5p3q7c7) |  | 0 | We resolve the open question regarding the sample complexity of policy learning for maximizing the long-run average reward associated with a uniformly ergodic Markov decision process (MDP), assuming a generative model. In this context, the existing literature provides a sample complexity upper... | José H. Blanchet, Peter W. Glynn, Shengbo Wang |  |
| 2108 |  |  [Spatio-Temporal Approximation: A Training-Free SNN Conversion for Transformers](https://openreview.net/forum?id=XrunSYwoLr) |  | 0 | Spiking neural networks (SNNs) are energy-efficient and hold great potential for large-scale inference. Since training SNNs from scratch is costly and has limited performance, converting pretrained artificial neural networks (ANNs) to SNNs is an attractive approach that retains robust performance... | Feng Chen, Haichuan Gao, Kunlin Hu, Tianren Zhang, Ying Fang, Yizhou Jiang, Yuqian Liu |  |
| 2109 |  |  [NfgTransformer: Equivariant Representation Learning for Normal-form Games](https://openreview.net/forum?id=4YESQqIys7) |  | 0 | Normal-form games (NFGs) are the fundamental model of \*strategic interaction\*. We study their representation using neural networks. We describe the inherent equivariance of NFGs --- any permutation of strategies describes an equivalent game --- as well as the challenges this poses for... | Georgios Piliouras, Ian Gemp, Luke Marris, Nicolas Heess, Siqi Liu |  |
| 2110 |  |  [#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models](https://openreview.net/forum?id=pszewhybU9) |  | 0 | Pre-trained large language models (LLMs) can understand and align with human instructions by supervised fine-tuning (SFT). It is commonly believed that diverse and complex SFT data are of the essence to enable good instruction-following abilities. However, such diversity and complexity are obscure... | Chang Zhou, Chuanqi Tan, Hongyi Yuan, Jingren Zhou, Junyang Lin, Keming Lu, Runji Lin, Zheng Yuan |  |
| 2111 |  |  [When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations](https://openreview.net/forum?id=JewzobRhay) |  | 0 | Context-based fine-tuning methods, including prompting, in-context learning, soft prompting (also known as prompt tuning), and prefix-tuning, have gained popularity due to their ability to often match the performance of full fine-tuning with a fraction of the parameters. Despite their empirical... | Adel Bibi, Aleksandar Petrov, Philip Torr |  |
| 2112 |  |  [Understanding In-Context Learning from Repetitions](https://openreview.net/forum?id=bGGYcvw8mp) |  | 0 | This paper explores the elusive mechanism underpinning in-context learning in Large Language Models (LLMs). Our work provides a novel perspective by examining in-context learning via the lens of surface repetitions. We quantitatively investigate the role of surface features in text generation, and... | Chenming Wu, Chiyu Song, Jianhao Yan, Jin Xu, Yafu Li, Yue Zhang |  |
| 2113 |  |  [Analysis of Learning a Flow-based Generative Model from Limited Sample Complexity](https://openreview.net/forum?id=ndCJeysCPe) |  | 0 | We study the problem of training a flow-based generative model, parametrized by a two-layer autoencoder, to sample from a high-dimensional Gaussian mixture. We provide a sharp end-to-end analysis of the problem. First, we provide a tight closed-form characterization of the learnt velocity field,... | Eric VandenEijnden, Florent Krzakala, Hugo Cui, Lenka Zdeborová |  |
| 2114 |  |  [Few-shot Hybrid Domain Adaptation of Image Generator](https://openreview.net/forum?id=FE2e8664Sl) |  | 0 | Can a pre-trained generator be adapted to the hybrid of multiple target domains and generate images with integrated attributes of them? In this work, we introduce a new task -- Few-shot $\textit{Hybrid Domain Adaptation}$ (HDA). Given a source generator and several target domains, HDA aims to... | Hengjia Li, Linxuan Xia, Tu Zheng, Wenxiao Wang, Xiaobo Ren, Xiaofei He, Xiaohui Zhong, Yang Liu, Yuqi Lin, Zheng Yang |  |
| 2115 |  |  [Rethinking Information-theoretic Generalization: Loss Entropy Induced PAC Bounds](https://openreview.net/forum?id=GWSIo2MzuH) |  | 0 | Information-theoretic generalization analysis has achieved astonishing success in characterizing the generalization capabilities of noisy and iterative learning algorithms. However, current advancements are mostly restricted to average-case scenarios and necessitate the stringent bounded loss... | Chen Li, Hong Chen, Shujian Yu, Tieliang Gong, Yuxin Dong |  |
| 2116 |  |  [Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via Selective Entropy Distillation](https://openreview.net/forum?id=vePdNU3u6n) |  | 0 | The conventional deep learning paradigm often involves training a deep model on a server and then deploying the model or its distilled ones to resource-limited edge devices. Usually, the models shall remain fixed once deployed (at least for some period) due to the potential high cost of model... | Hengjie Song, Mingkui Tan, Shoukai Xu, Shuaicheng Niu, Yaofo Chen, Yaowei Wang |  |
| 2117 |  |  [KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval](https://openreview.net/forum?id=b3kDP3IytM) |  | 0 | We study the ability of state-of-the art models to answer constraint satisfaction queries for information retrieval (e.g., “a list of ice cream shops in San Diego”). In the past, such queries were considered as tasks that could only be solved via web-search or knowledge bases. More recently, large... | Besmira Nushi, Jerry Li, Marah I Abdin, Mert Yüksekgönül, Rahee Ghosh Peshawaria, Ranjita Naik, Suriya Gunasekar, Varun Chandrasekaran |  |
| 2118 |  |  [Boosting Graph Anomaly Detection with Adaptive Message Passing](https://openreview.net/forum?id=CanomFZssu) |  | 0 | Unsupervised graph anomaly detection has been widely used in real-world applications. Existing methods primarily focus on local inconsistency mining (LIM), based on the intuition that establishing high similarities between abnormal nodes and their neighbors is difficult. However, the message... | Chunfeng Yuan, Guanghui Zhu, Jingyan Chen, Yihua Huang |  |
| 2119 |  |  [MINDE: Mutual Information Neural Diffusion Estimation](https://openreview.net/forum?id=0kWd8SJq8d) |  | 0 | In this work we present a new method for the estimation of Mutual Information (MI) between random variables. Our approach is based on an original interpretation of the Girsanov theorem, which allows us to use score-based diffusion models to estimate the KL divergence between two densities as a... | Giulio Franzese, Mustapha Bounoua, Pietro Michiardi |  |
| 2120 |  |  [Continual Momentum Filtering on Parameter Space for Online Test-time Adaptation](https://openreview.net/forum?id=BllUWdpIOA) |  | 0 | Deep neural networks (DNNs) have revolutionized tasks such as image classification and speech recognition but often falter when training and test data diverge in distribution. External factors, from weather effects on images to varied speech environments, can cause this discrepancy, compromising... | JaeHong Lee, JoonHyuk Chang |  |
| 2121 |  |  [Deep Reinforcement Learning for Modelling Protein Complexes](https://openreview.net/forum?id=4MsfQ2H0lP) |  | 0 | Structure prediction of large protein complexes (a.k.a., protein multimer mod- elling, PMM) can be achieved through the one-by-one assembly using provided dimer structures and predicted docking paths. However, existing PMM methods struggle with vast search spaces and generalization challenges: (1)... | Chen Zhang, Chenyi Zi, Jia Li, Jiaxuan You, Tao Feng, Yan Zhou, Ziqi Gao |  |
| 2122 |  |  [fairret: a Framework for Differentiable Fairness Regularization Terms](https://openreview.net/forum?id=NnyD0Rjx2B) |  | 0 | Current tools for machine learning fairness only admit a limited range of fairness definitions and have seen little integration with automatic differentiation libraries, despite the central role these libraries play in modern machine learning pipelines. We introduce a framework of fairness... | Maarten Buyl, MaryBeth Defrance, Tijl De Bie |  |
| 2123 |  |  [Debiasing Algorithm through Model Adaptation](https://openreview.net/forum?id=XIZEFyVGC9) |  | 0 | Large language models are becoming the go-to solution for the ever-growing number of tasks. However, with growing capacity, models are prone to rely on spurious correlations stemming from biases and stereotypes present in the training data. This work proposes a novel method for detecting and... | David Marecek, Tomasz Limisiewicz, Tomás Musil |  |
| 2124 |  |  [A Foundation Model for Error Correction Codes](https://openreview.net/forum?id=7KDuQPrAF3) |  | 0 | In recent years, Artificial Intelligence has undergone a paradigm shift with the rise of foundation models, which are trained on large amounts of data, typically in a self-supervised way, and can then be adapted to a wide range of downstream tasks. In this work, we propose the first foundation... | Lior Wolf, Yoni Choukroun |  |
| 2125 |  |  [Seer: Language Instructed Video Prediction with Latent Diffusion Models](https://openreview.net/forum?id=qHGgNyQk31) |  | 0 | Imagining the future trajectory is the key for robots to make sound planning and successfully reach their goals. Therefore, text-conditioned video prediction (TVP) is an essential task to facilitate general robot policy learning. To tackle this task and empower robots with the ability to foresee... | Chuan Wen, Jiaming Song, Weirui Ye, Xianfan Gu, Yang Gao |  |
| 2126 |  |  [Matrix Manifold Neural Networks++](https://openreview.net/forum?id=30aSE3FB3L) |  | 0 | Deep neural networks (DNNs) on Riemannian manifolds have garnered increasing interest in various applied areas. For instance, DNNs on spherical and hyperbolic manifolds have been designed to solve a wide range of computer vision and nature language processing tasks. One of the key factors that... | Aymeric Histace, Shuo Yang, Xuan Son Nguyen |  |
| 2127 |  |  [Emo: Earth Mover Distance Optimization for Auto-Regressive Language Modeling](https://openreview.net/forum?id=4bLXfRd0CX) |  | 0 | Neural language models are probabilistic models of human text. They are predominantly trained using maximum likelihood estimation (MLE), which is equivalent to minimizing the forward cross-entropy between the empirical data distribution and the model distribution. However, various degeneration... | Kenny Q. Zhu, Siyu Ren, Zhiyong Wu |  |
| 2128 |  |  [Are Human-generated Demonstrations Necessary for In-context Learning?](https://openreview.net/forum?id=frRDT6EOhg) |  | 0 | Despite the promising few-shot ability of large language models (LLMs), the standard paradigm of In-context Learning (ICL) suffers the disadvantages of susceptibility to selected demonstrations and the intricacy to generate these demonstrations. In this paper, we raise the fundamental question that... | Guoyin Wang, Jiwei Li, Rui Li |  |
| 2129 |  |  [LLM-Assisted Code Cleaning For Training Accurate Code Generators](https://openreview.net/forum?id=maRYffiUpI) |  | 0 | Natural language to code generation is an important application area of LLMs and has received wide attention from the community. The majority of relevant studies have exclusively concentrated on increasing the quantity and functional correctness of training sets while disregarding other stylistic... | Ion Stoica, Joseph E. Gonzalez, Koushik Sen, Naman Jain, Tianjun Zhang, WeiLin Chiang |  |
| 2130 |  |  [HYPO: Hyperspherical Out-Of-Distribution Generalization](https://openreview.net/forum?id=VXak3CZZGC) |  | 0 | Out-of-distribution (OOD) generalization is critical for machine learning models deployed in the real world. However, achieving this can be fundamentally challenging, as it requires the ability to learn invariant features across different domains or environments. In this paper, we propose a novel... | Haoyue Bai, Julian KatzSamuels, Yifei Ming, Yixuan Li |  |
| 2131 |  |  [Analyzing and Improving Optimal-Transport-based Adversarial Networks](https://openreview.net/forum?id=jODehvtTDx) |  | 0 | Optimal Transport (OT) problem aims to find a transport plan that bridges two distributions while minimizing a given cost function. OT theory has been widely utilized in generative modeling. In the beginning, OT distance has been used as a measure for assessing the distance between data and... | Jaemoo Choi, Jaewoong Choi, Myungjoo Kang |  |
| 2132 |  |  [SEABO: A Simple Search-Based Method for Offline Imitation Learning](https://openreview.net/forum?id=MNyOI3C7YB) |  | 0 | Offline reinforcement learning (RL) has attracted much attention due to its ability in learning from static offline datasets and eliminating the need of interacting with the environment. Nevertheless, the success of offline RL relies heavily on the offline transitions annotated with reward labels.... | Jiafei Lyu, Le Wan, Runze Liu, Xiaoteng Ma, Xiu Li, Zongqing Lu |  |
| 2133 |  |  [Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs](https://openreview.net/forum?id=1ndDmZdT4g) |  | 0 | The ever-increasing large language models (LLMs), though opening a potential path for the upcoming artificial general intelligence, sadly drops a daunting obstacle on the way towards their on-device deployment. As one of the most well-established pre-LLMs approaches in reducing model complexity,... | Jared Tanner, Lirui Zhao, Mingbao Lin, Rongrong Ji, Shiwei Liu, Xingjia Han, Yiwu Yao, Yunyun Sun, Yuxin Zhang |  |
| 2134 |  |  [Simplifying Transformer Blocks](https://openreview.net/forum?id=RtDok9eS3s) |  | 0 | A simple design recipe for deep Transformers is to compose identical building blocks. But standard transformer blocks are far from simple, interweaving attention and MLP sub-blocks with skip connections \& normalisation layers in precise arrangements. This complexity leads to brittle architectures,... | Bobby He, Thomas Hofmann |  |
| 2135 |  |  [Aux-NAS: Exploiting Auxiliary Labels with Negligibly Extra Inference Cost](https://openreview.net/forum?id=cINwAhrgLf) |  | 0 | We aim at exploiting additional auxiliary labels from an independent (auxiliary) task to boost the primary task performance which we focus on, while preserving a single task inference cost of the primary task. While most existing auxiliary learning methods are optimization-based relying on loss... | GuiSong Xia, Jiayi Ma, JinGang Yu, Lin Ma, Weizhong Zhang, Wenhan Luo, Yuan Gao |  |
| 2136 |  |  [EX-Graph: A Pioneering Dataset Bridging Ethereum and X](https://openreview.net/forum?id=juE0rWGCJW) |  | 0 | While numerous public blockchain datasets are available, their utility is constrained by an exclusive focus on blockchain data. This constraint limits the incorporation of relevant social network data into blockchain analysis, thereby diminishing the breadth and depth of insight that can be... | Bingqiao Luo, Bingsheng He, Qian Wang, Shengliang Lu, Zemin Liu, Zhen Zhang |  |
| 2137 |  |  [Enhancing One-Shot Federated Learning Through Data and Ensemble Co-Boosting](https://openreview.net/forum?id=tm8s3696Ox) |  | 0 | One-shot Federated Learning (OFL) has become a promising learning paradigm, enabling the training of a global server model via a single communication round. In OFL, the server model is aggregated by distilling knowledge from all client models (the ensemble), which are also responsible for... | Ang Li, Bo Han, Rong Dai, Tongliang Liu, Xun Yang, Yonggang Zhang |  |
| 2138 |  |  [Symbol as Points: Panoptic Symbol Spotting via Point-based Representation](https://openreview.net/forum?id=aOnUe8ah7j) |  | 0 | This work studies the problem of panoptic symbol spotting, which is to spot and parse both countable object instances (windows, doors, tables, etc.) and uncountable stuff (wall, railing, etc.) from computer-aided design (CAD) drawings. Existing methods typically involve either rasterizing the... | Lei Zhang, Qizhi Yu, Tianyu Yang, Wenlong Liu, Yuhan Wang |  |
| 2139 |  |  [HypeBoy: Generative Self-Supervised Representation Learning on Hypergraphs](https://openreview.net/forum?id=DZUzOKE6og) |  | 0 | Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple nodes with hyperedges, and better capturing the topology is essential for effective representation learning. Recent advances in generative self-supervised learning (SSL) suggest that hypergraph neural... | Fanchen Bu, Jaemin Yoo, Kijung Shin, Shinhwan Kang, Soo Yong Lee, Sunwoo Kim |  |
| 2140 |  |  [Zero-Shot Robustification of Zero-Shot Models](https://openreview.net/forum?id=fCeUoDr9Tq) |  | 0 | Zero-shot inference is a powerful paradigm that enables the use of large pretrained models for downstream classification tasks without further training. However, these models are vulnerable to inherited biases that can impact their performance. The traditional solution is fine-tuning, but this... | Changho Shin, Dyah Adila, Frederic Sala, Linrong Cai |  |
| 2141 |  |  [Thought Propagation: an Analogical Approach to Complex Reasoning with Large Language Models](https://openreview.net/forum?id=SBoRhRCzM3) |  | 0 | Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to... | Junchi Yu, Ran He, Zhitao Ying |  |
| 2142 |  |  [FreeDyG: Frequency Enhanced Continuous-Time Dynamic Graph Model for Link Prediction](https://openreview.net/forum?id=82Mc5ilInM) |  | 0 | Link prediction is a crucial task in dynamic graph learning. Recent advancements in continuous-time dynamic graph models, primarily by leveraging richer temporal details, have significantly improved link prediction performance. However, due to their complex modules, they still face several... | Fan Guo, Yiyan Qi, Yuxing Tian |  |
| 2143 |  |  [DreamSmooth: Improving Model-based Reinforcement Learning via Reward Smoothing](https://openreview.net/forum?id=GruDNzQ4ux) |  | 0 | Model-based reinforcement learning (MBRL) has gained much attention for its ability to learn complex behaviors in a sample-efficient way: planning actions by generating imaginary trajectories with predicted rewards. Despite its success, we found that surprisingly, reward prediction is often a... | Pieter Abbeel, Vint Lee, Youngwoon Lee |  |
| 2144 |  |  [VONet: Unsupervised Video Object Learning With Parallel U-Net Attention and Object-wise Sequential VAE](https://openreview.net/forum?id=qCyhvr0GG8) |  | 0 | Unsupervised video object learning seeks to decompose video scenes into structural object representations without any supervision from depth, optical flow, or segmentation. We present VONet, an innovative approach that is inspired by MONet. While utilizing a U-Net architecture, VONet employs an... | Haonan Yu, Wei Xu |  |
| 2145 |  |  [ReSimAD: Zero-Shot 3D Domain Transfer for Autonomous Driving with Source Reconstruction and Target Simulation](https://openreview.net/forum?id=1d2cLKeNgY) |  | 0 | Domain shifts such as sensor type changes and geographical situation variations are prevalent in Autonomous Driving (AD), which poses a challenge since AD model relying on the previous domain knowledge can be hardly directly deployed to a new domain without additional costs. In this paper, we... | Bo Zhang, Botian Shi, Donglin Yang, Jiakang Yuan, Jianfei Guo, Junchi Yan, Min Dou, Renqiu Xia, Si Liu, Tao Chen, Xiangchao Yan, Xinyu Cai, Yu Qiao |  |
| 2146 |  |  [Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE](https://openreview.net/forum?id=rTDyN8yajn) |  | 0 | Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance.... | Huayang Liu, Jing Shao, Lu Sheng, Si Liu, Wanli Ouyang, Zeren Chen, Zhen Wang, Zhenfei Yin, Ziqin Wang |  |
| 2147 |  |  [Motion Guidance: Diffusion-Based Image Editing with Differentiable Motion Estimators](https://openreview.net/forum?id=WIAO4vbnNV) |  | 0 | Diffusion models are capable of generating impressive images conditioned on text descriptions, and extensions of these models allow users to edit images at a relatively coarse scale. However, the ability to precisely edit the layout, position, pose, and shape of objects in images with diffusion... | Andrew Owens, Daniel Geng |  |
| 2148 |  |  [Balancing Act: Constraining Disparate Impact in Sparse Models](https://openreview.net/forum?id=Xz13DtbOVW) |  | 0 | Model pruning is a popular approach to enable the deployment of large deep learning models on edge devices with restricted computational or storage capacities. Although sparse models achieve performance comparable to that of their dense counterparts at the level of the entire dataset, they exhibit... | Golnoosh Farnadi, Jose GallegoPosada, Juan Ramirez, Meraj Hashemizadeh, Rohan Sukumaran, Simon LacosteJulien |  |
| 2149 |  |  [NECO: NEural Collapse Based Out-of-distribution detection](https://openreview.net/forum?id=9ROuKblmi7) |  | 0 | Detecting out-of-distribution (OOD) data is a critical challenge in machine learning due to model overconfidence, often without awareness of their epistemological limits. We hypothesize that "neural collapse", a phenomenon affecting in-distribution data for models trained beyond loss convergence,... | Antoine Manzanera, Gianni Franchi, Mouïn Ben Ammar, Nacim Belkhir, Sebastian Popescu |  |
| 2150 |  |  [LightHGNN: Distilling Hypergraph Neural Networks into MLPs for 100x Faster Inference](https://openreview.net/forum?id=lHasEfGsXL) |  | 0 | Hypergraph Neural Networks (HGNNs) have recently attracted much attention and exhibited satisfactory performance due to their superiority in high-order correlation modeling. However, it is noticed that the high-order modeling capability of hypergraph also brings increased computation complexity,... | Shihui Ying, Yifan Feng, Yihe Luo, Yue Gao |  |
| 2151 |  |  [Lewis's Signaling Game as beta-VAE For Natural Word Lengths and Segments](https://openreview.net/forum?id=HC0msxE3sf) |  | 0 | As a sub-discipline of evolutionary and computational linguistics, emergent communication (EC) studies communication protocols, called emergent languages, arising in simulations where agents communicate. A key goal of EC is to give rise to languages that share statistical properties with natural... | Ryo Ueda, Tadahiro Taniguchi |  |
| 2152 |  |  [Domain-Inspired Sharpness-Aware Minimization Under Domain Shifts](https://openreview.net/forum?id=I4wB3HA3dJ) |  | 0 | This paper presents a Domain-Inspired Sharpness-Aware Minimization (DISAM) algorithm for optimization under domain shifts. It is motivated by the inconsistent convergence degree of SAM across different domains, which induces optimization bias towards certain domains and thus impairs the overall... | Jiangchao Yao, Ruipeng Zhang, Ya Zhang, Yanfeng Wang, Ziqing Fan |  |
| 2153 |  |  [Making LLaMA SEE and Draw with SEED Tokenizer](https://openreview.net/forum?id=0Nui91LBQS) |  | 0 | The great success of Large Language Models (LLMs) has expanded the potential of multimodality, contributing to the gradual evolution of General Artificial Intelligence (AGI). A true AGI agent should not only possess the capability to perform predefined multi-tasks but also exhibit emergent... | Chen Li, Sijie Zhao, Xintao Wang, Ying Shan, Yixiao Ge, Yuying Ge, Ziyun Zeng |  |
| 2154 |  |  [A Cognitive Model for Learning Abstract Relational Structures from Memory-based Decision-Making Tasks](https://openreview.net/forum?id=KC58bVmxyN) |  | 0 | Motivated by a recent neuroscientific hypothesis, some theoretical studies have accounted for neural cognitive maps in the rodent hippocampal formation as a representation of the general relational structure across task environments. However, despite their remarkable results, it is unclear whether... | Haruo Hosoya |  |
| 2155 |  |  [DynaVol: Unsupervised Learning for Dynamic Scenes through Object-Centric Voxelization](https://openreview.net/forum?id=koYsgfEwCQ) |  | 0 | Unsupervised learning of object-centric representations in dynamic visual scenes is challenging. Unlike most previous approaches that learn to decompose 2D images, we present DynaVol, a 3D scene generative model that unifies geometric structures and object-centric learning in a differentiable... | Siyu Gao, Xiaokang Yang, Yanpeng Zhao, Yunbo Wang |  |
| 2156 |  |  [TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series](https://openreview.net/forum?id=Tuh4nZVb0g) |  | 0 | This work summarizes two ways to accomplish Time-Series (TS) tasks in today's Large Language Model (LLM) context: LLM-for-TS (model-centric) designs and trains a fundamental large model, or fine-tunes a pre-trained LLM for TS data; TS-for-LLM (data-centric) converts TS into a model-friendly... | Chenxi Sun, Hongyan Li, Shenda Hong, Yaliang Li |  |
| 2157 |  |  [Sin3DM: Learning a Diffusion Model from a Single 3D Textured Shape](https://openreview.net/forum?id=U0IOMStUQ8) |  | 0 | Synthesizing novel 3D models that resemble the input example as long been pursued by graphics artists and machine learning researchers. In this paper, we present Sin3DM, a diffusion model that learns the internal patch distribution from a single 3D textured shape and generates high-quality... | Carl Vondrick, Changxi Zheng, Rundi Wu, Ruoshi Liu |  |
| 2158 |  |  [Pooling Image Datasets with Multiple Covariate Shift and Imbalance](https://openreview.net/forum?id=2Mo7v69otj) |  | 0 | Small sample sizes are common in many disciplines, which necessitates pooling roughly similar datasets across multiple sites/institutions to study weak but relevant associations between images and disease incidence. Such data often manifest shifts and imbalances in covariates (secondary non-imaging... | Sotirios Panagiotis Chytas, Vikas Singh, Vishnu Suresh Lokhande |  |
| 2159 |  |  [On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes](https://openreview.net/forum?id=3zKtaqxLhW) |  | 0 | Knowledge distillation (KD) is widely used for compressing a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, current KD methods for auto-regressive sequence models suffer from distribution mismatch between output sequences seen during... | Matthieu Geist, Nino Vieillard, Olivier Bachem, Piotr Stanczyk, Rishabh Agarwal, Sabela Ramos Garea, Yongchao Zhou |  |
| 2160 |  |  [Adaptive Regularization of Representation Rank as an Implicit Constraint of Bellman Equation](https://openreview.net/forum?id=apXtolxDaJ) |  | 0 | Representation rank is an important concept for understanding the role of Neural Networks (NNs) in Deep Reinforcement learning (DRL), which measures the expressive capacity of value networks. Existing studies focus on unboundedly maximizing this rank; nevertheless, that approach would introduce... | Meng Fang, Qiang He, Setareh Maghsudi, Tianyi Zhou |  |
| 2161 |  |  [Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks](https://openreview.net/forum?id=ZL6yd6N1S2) |  | 0 | While graph neural networks (GNNs) are widely used for node and graph representation learning tasks, the reliability of GNN uncertainty estimates under distribution shifts remains relatively under-explored. Indeed, while post-hoc calibration strategies can be used to improve in-distribution... | Danai Koutra, Jayaraman J. Thiagarajan, Mark Heimann, Puja Trivedi, Rushil Anirudh |  |
| 2162 |  |  [Symphony: Symmetry-Equivariant Point-Centered Spherical Harmonics for 3D Molecule Generation](https://openreview.net/forum?id=MIEnYtlGyv) |  | 0 | We present Symphony, an $E(3)$ equivariant autoregressive generative model for 3D molecular geometries that iteratively builds a molecule from molecular fragments. Existing autoregressive models such as G-SchNet and G-SphereNet for molecules utilize rotationally invariant features to respect the 3D... | Ameya Daigavane, Mario Geiger, Song Kim, Tess E. Smidt |  |
| 2163 |  |  [Set Learning for Accurate and Calibrated Models](https://openreview.net/forum?id=HZ3S17EI0o) |  | 0 | Model overconfidence and poor calibration are common in machine learning and difficult to account for when applying standard empirical risk minimization. In this work, we propose a novel method to alleviate these problems that we call odd-$k$-out learning (OKO), which minimizes the cross-entropy... | KlausRobert Müller, Lukas Muttenthaler, Qiuyi Zhang, Robert A. Vandermeulen, Thomas Unterthiner |  |
| 2164 |  |  [INViTE: INterpret and Control Vision-Language Models with Text Explanations](https://openreview.net/forum?id=5iENGLEJKG) |  | 0 | Large-scale pre-trained vision foundation models, such as CLIP, have become de facto backbones for various vision tasks. However, due to their black-box nature, understanding the underlying rules behind these models’ predictions and controlling model behaviors have remained open challenges. We... | Carl Vondrick, Chengzhi Mao, Haozhe Chen, Junfeng Yang |  |
| 2165 |  |  [Trajeglish: Traffic Modeling as Next-Token Prediction](https://openreview.net/forum?id=Z59Rb5bPPP) |  | 0 | A longstanding challenge for self-driving development is simulating dynamic driving scenarios seeded from recorded driving logs. In pursuit of this functionality, we apply tools from discrete sequence modeling to model how vehicles, pedestrians and cyclists interact in driving scenarios. Using a... | Jonah Philion, Sanja Fidler, Xue Bin Peng |  |
| 2166 |  |  [Meaning Representations from Trajectories in Autoregressive Models](https://openreview.net/forum?id=UyGWafcopT) |  | 0 | We propose to extract meaning representations from autoregressive language models by considering the distribution of all possible trajectories extending an input text. This strategy is prompt-free, does not require fine-tuning, and is applicable to any pre-trained autoregressive model. Moreover,... | Alessandro Achille, Luca Zancato, Matthew Trager, Pramuditha Perera, Stefano Soatto, Tian Yu Liu |  |
| 2167 |  |  [SWAP: Sparse Entropic Wasserstein Regression for Robust Network Pruning](https://openreview.net/forum?id=LJWizuuBUy) |  | 0 | This study addresses the challenge of inaccurate gradients in computing the empirical Fisher Information Matrix during network pruning. We introduce SWAP, a formulation of Entropic Wasserstein regression (EWR) for network pruning, capitalizing on the geometric properties of the optimal transport... | Hei Victor Cheng, Lei You |  |
| 2168 |  |  [Circumventing Concept Erasure Methods For Text-To-Image Generative Models](https://openreview.net/forum?id=ag3o2T51Ht) |  | 0 | Text-to-image generative models can produce photo-realistic images for an extremely broad range of concepts, and their usage has proliferated widely among the general public. On the flip side, these models have numerous drawbacks, including their potential to generate images featuring sexually... | Chinmay Hegde, Govind Mittal, Kelly O. Marshall, Minh Pham, Niv Cohen |  |
| 2169 |  |  [Pose Modulated Avatars from Video](https://openreview.net/forum?id=5t44vPlv9x) |  | 0 | It is now possible to reconstruct dynamic human motion and shape from a sparse set of cameras using Neural Radiance Fields (NeRF) driven by an underlying skeleton. However, a challenge remains to model the deformation of cloth and skin in relation to skeleton pose. Unlike existing avatar models... | Bastian Wandt, Chunjin Song, Helge Rhodin |  |
| 2170 |  |  [The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images](https://openreview.net/forum?id=ixP76Y33y1) |  | 0 | This paper investigates discrepancies in how neural networks learn from different imaging domains, which are commonly overlooked when adopting computer vision techniques from the domain of natural images to other specialized domains such as medical images. Recent works have found that the... | Maciej A. Mazurowski, Nicholas Konz |  |
| 2171 |  |  [Complete and Efficient Graph Transformers for Crystal Material Property Prediction](https://openreview.net/forum?id=BnQY9XiRAS) |  | 0 | Crystal structures are characterized by atomic bases within a primitive unit cell that repeats along a regular lattice throughout 3D space. The periodic and infinite nature of crystals poses unique challenges for geometric graph representation learning. Specifically, constructing graphs that... | Cong Fu, Keqiang Yan, Shuiwang Ji, Xiaofeng Qian, Xiaoning Qian |  |
| 2172 |  |  [Patched Denoising Diffusion Models For High-Resolution Image Synthesis](https://openreview.net/forum?id=TgSRPRz8cI) |  | 0 | We propose an effective denoising diffusion model for generating high-resolution images (e.g., 1024$\times$512), trained on small-size image patches (e.g., 64$\times$64). We name our algorithm Patch-DM, in which a new feature collage strategy is designed to avoid the boundary artifact when... | Jiajun Wu, Mengqi Zhang, Zheng Ding, Zhuowen Tu |  |
| 2173 |  |  [NOLA: Compressing LoRA using Linear Combination of Random Basis](https://openreview.net/forum?id=TjfXcDgvzk) |  | 0 | Fine-tuning Large Language Models (LLMs) and storing them for each downstream task or domain is impractical because of the massive model size (e.g., 350GB in GPT-3). Current literature, such as LoRA, showcases the potential of low-rank modifications to the original weights of an LLM, enabling... | Hamed Pirsiavash, Navaneet K. L., Parsa Nooralinejad, Soheil Kolouri, Soroush Abbasi Koohpayegani |  |
| 2174 |  |  [Unveiling Options with Neural Network Decomposition](https://openreview.net/forum?id=a8VETFwcVR) |  | 0 | In reinforcement learning, agents often learn policies for specific tasks without the ability to generalize this knowledge to related tasks. This paper introduces an algorithm that attempts to address this limitation by decomposing neural networks encoding policies for Markov Decision Processes... | Levi Lelis, Mahdi Alikhasi |  |
| 2175 |  |  [HIFA: High-fidelity Text-to-3D Generation with Advanced Diffusion Guidance](https://openreview.net/forum?id=IZMPWmcS3H) |  | 0 | The advancements in automatic text-to-3D generation have been remarkable. Most existing methods use pre-trained text-to-image diffusion models to optimize 3D representations like Neural Radiance Fields (NeRFs) via latent-space denoising score matching. Yet, these methods often result in artifacts... | Junzhe Zhu, Peiye Zhuang, Sanmi Koyejo |  |
| 2176 |  |  [FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing](https://openreview.net/forum?id=JgqftqZQZ7) |  | 0 | Text-to-video editing aims to edit the visual appearance of a source video conditional on textual prompts. A major challenge in this task is to ensure that all frames in the edited video are visually consistent. Most recent works apply advanced text-to-image diffusion models to this task by... | Bodo Rosenhahn, Christian Simon, Jiawei Ren, JuanManuel PérezRúa, Mengmeng Xu, Sen He, Shoufa Chen, Tao Xiang, Yanping Xie, Yuren Cong |  |
| 2177 |  |  [How Does Unlabeled Data Provably Help Out-of-Distribution Detection?](https://openreview.net/forum?id=jlEjB8MVGa) |  | 0 | Using unlabeled data to regularize the machine learning models has demonstrated promise for improving safety and reliability in detecting out-of-distribution (OOD) data. Harnessing the power of unlabeled in-the-wild data is non-trivial due to the heterogeneity of both in-distribution (ID) and OOD... | Ilias Diakonikolas, Xuefeng Du, Yixuan Li, Zhen Fang |  |
| 2178 |  |  [Delta-AI: Local objectives for amortized inference in sparse graphical models](https://openreview.net/forum?id=LemSSn8htt) |  | 0 | We present a new algorithm for amortized inference in sparse probabilistic graphical models (PGMs), which we call $\Delta$-amortized inference ($\Delta$-AI). Our approach is based on the observation that when the sampling of variables in a PGM is seen as a sequence of actions taken by an agent,... | Chen Sun, Dinghuai Zhang, Dragos Secrieru, Guillaume Lajoie, Hae Beom Lee, JeanPierre R. Falet, Nikolay Malkin, Yoshua Bengio |  |
| 2179 |  |  [Learning Implicit Representation for Reconstructing Articulated Objects](https://openreview.net/forum?id=KQ2i6jazVK) |  | 0 | 3D Reconstruction of moving articulated objects without additional information about object structure is a challenging problem. Current methods overcome such challenges by employing category-specific skeletal models. Consequently, they do not generalize well to articulated objects in the wild. We... | Fang Li, Hao Zhang, Narendra Ahuja, Samyak Rawlekar |  |
| 2180 |  |  [Improving protein optimization with smoothed fitness landscapes](https://openreview.net/forum?id=rxlF2Zv8x0) |  | 0 | The ability to engineer novel proteins with higher fitness for a desired property would be revolutionary for biotechnology and medicine. Modeling the combinatorially large space of sequences is infeasible; prior methods often constrain optimization to a small mutational radius, but this drastically... | Andrew Kirjner, Ila R. Fiete, Jason Yim, Raman Samusevich, Regina Barzilay, Shahar Bracha, Tommi S. Jaakkola |  |
| 2181 |  |  [Rethinking Label Poisoning for GNNs: Pitfalls and Attacks](https://openreview.net/forum?id=J7ioefqDPw) |  | 0 | Node labels for graphs are usually generated using an automated process or crowd-sourced from human users. This opens up avenues for malicious users to compromise the training labels, making it unwise to blindly rely on them. While robustness against noisy labels is an active area of research,... | Aleksandar Bojchevski, Mohammad Sadegh Akhondzadeh, Vijay Lingam |  |
| 2182 |  |  [Structural Estimation of Partially Observed Linear Non-Gaussian Acyclic Model: A Practical Approach with Identifiability](https://openreview.net/forum?id=nHkMm0ywWm) |  | 0 | Conventional causal discovery approaches, which seek to uncover causal relationships among measured variables, are typically fragile to the presence of latent variables. While various methods have been developed to address this confounding issue, they often rely on strong assumptions about the... | Biwei Huang, Feng Xie, Guangyi Chen, Kun Zhang, Songyao Jin, Xinshuai Dong, Zhengming Chen |  |
| 2183 |  |  [Meta Inverse Constrained Reinforcement Learning: Convergence Guarantee and Generalization Analysis](https://openreview.net/forum?id=bJ3gFiwRgi) |  | 0 | This paper considers the problem of learning the reward function and constraints of an expert from few demonstrations. This problem can be considered as a meta-learning problem where we first learn meta-priors over reward functions and constraints from other distinct but related tasks and then... | Minghui Zhu, Shicheng Liu |  |
| 2184 |  |  [Cleanba: A Reproducible and Efficient Distributed Reinforcement Learning Platform](https://openreview.net/forum?id=Diq6urt3lS) |  | 0 | Distributed Deep Reinforcement Learning (DRL) aims to leverage more computational resources to train autonomous agents with less training time. Despite recent progress in the field, reproducibility issues have not been sufficiently explored. This paper first shows that the typical actor-learner... | Jiayi Weng, Min Lin, Rujikorn Charakorn, Santiago Ontañón, Shengyi Huang, Zhongwen Xu |  |
| 2185 |  |  [Consciousness-Inspired Spatio-Temporal Abstractions for Better Generalization in Reinforcement Learning](https://openreview.net/forum?id=eo9dHwtTFt) |  | 0 | Inspired by human conscious planning, we propose Skipper, a model-based reinforcement learning framework utilizing spatio-temporal abstractions to generalize better in novel situations. It automatically decomposes the given task into smaller, more manageable subtasks, and thus enables sparse... | Doina Precup, Harm van Seijen, Harry Zhao, Romain Laroche, Safa Alver, Yoshua Bengio |  |
| 2186 |  |  [Bayesian Low-rank Adaptation for Large Language Models](https://openreview.net/forum?id=FJiUyzOF1m) |  | 0 | Parameter-efficient fine-tuning (PEFT) has emerged as a new paradigm for cost-efficient fine-tuning of large language models (LLMs), with low-rank adaptation (LoRA) being a widely adopted choice. However, fine-tuned LLMs often become overconfident especially when fine-tuned on small datasets.... | Adam X. Yang, Laurence Aitchison, Maxime Robeyns, Xi Wang |  |
| 2187 |  |  [Function-space Parameterization of Neural Networks for Sequential Learning](https://openreview.net/forum?id=2dhxxIKhqz) |  | 0 | Sequential learning paradigms pose challenges for gradient-based deep learning due to difficulties incorporating new data and retaining prior knowledge. While Gaussian processes elegantly tackle these problems, they struggle with scalability and handling rich inputs, such as images. To address... | Aidan Scannell, Arno Solin, Ella Tamir, Joni Pajarinen, Paul Edmund Chang, Riccardo Mereu |  |
| 2188 |  |  [Denevil: towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning](https://openreview.net/forum?id=m3RRWWFaVe) |  | 0 | Large Language Models (LLMs) have made unprecedented breakthroughs, yet their increasing integration into everyday life might raise societal risks due to generated unethical content. Despite extensive study on specific issues like bias, the intrinsic values of LLMs remain largely unexplored from a... | Ning Gu, Peng Zhang, Shitong Duan, Tun Lu, Xiaoyuan Yi, Xing Xie |  |
| 2189 |  |  [The Expressive Power of Transformers with Chain of Thought](https://openreview.net/forum?id=NjNGlPh8Wh) |  | 0 | Recent theoretical work has identified surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating finite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice,... | Ashish Sabharwal, William Merrill |  |
| 2190 |  |  [When should we prefer Decision Transformers for Offline Reinforcement Learning?](https://openreview.net/forum?id=vpV7fOFQy4) |  | 0 | Offline reinforcement learning (RL) allows agents to learn effective, return-maximizing policies from a static dataset. Three popular algorithms for offline RL are Conservative Q-Learning (CQL), Behavior Cloning (BC), and Decision Transformer (DT), from the class of Q-Learning, Imitation Learning,... | Alborz Geramifard, Amy Zhang, Prajjwal Bhargava, Rohan Chitnis, Shagun Sodhani |  |
| 2191 |  |  [ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate](https://openreview.net/forum?id=FQepisCUWu) |  | 0 | Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise,... | ChiMin Chan, Jianxuan Yu, Jie Fu, Shanghang Zhang, Wei Xue, Weize Chen, Yusheng Su, Zhiyuan Liu |  |
| 2192 |  |  [Bridging State and History Representations: Understanding Self-Predictive RL](https://openreview.net/forum?id=ms0VgzSGF2) |  | 0 | Representations are at the core of all deep reinforcement learning (RL) methods for both Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs). Many representation learning methods and theoretical frameworks have been developed to understand what constitutes... | Aditya Mahajan, Benjamin Eysenbach, Clement Gehring, Erfan Seyedsalehi, Michel Ma, PierreLuc Bacon, Tianwei Ni |  |
| 2193 |  |  [TapMo: Shape-aware Motion Generation of Skeleton-free Characters](https://openreview.net/forum?id=OeH6Fdhv7q) |  | 0 | Previous motion generation methods are limited to the pre-rigged 3D human model, hindering their applications in the animation of various non-rigged characters. In this work, we present TapMo, a Text-driven Animation PIpeline for synthesizing Motion in a broad spectrum of skeleton-free 3D... | Gang Yu, Jiaxu Zhang, Shaoli Huang, Xiaohang Zhan, Xin Chen, Ying Shan, Zhigang Tu |  |
| 2194 |  |  [InstructDET: Diversifying Referring Object Detection with Generalized Instructions](https://openreview.net/forum?id=hss35aoQ1Y) |  | 0 | We propose InstructDET, a data-centric method for referring object detection (ROD) that localizes target objects based on user instructions. While deriving from referring expressions (REC), the instructions we leverage are greatly diversified to encompass common user intentions related to object... | Chengju Liu, Chongjian Ge, Feng Zhu, Haodong Zhang, Jiangyan Feng, Lijun Gong, Lin Song, Qijun Chen, Ronghao Dang, Rui Zhao, Yibing Song |  |
| 2195 |  |  [RAIN: Your Language Models Can Align Themselves without Finetuning](https://openreview.net/forum?id=pETSfWMUzy) |  | 0 | Large language models (LLMs) often demonstrate inconsistencies with human preferences. Previous research typically gathered human preference data and then aligned the pre-trained models using reinforcement learning or instruction tuning, a.k.a. the finetuning step. In contrast, aligning frozen LLMs... | Chao Zhang, Fangyun Wei, Hongyang Zhang, Jinjing Zhao, Yuhui Li |  |
| 2196 |  |  [PBADet: A One-Stage Anchor-Free Approach for Part-Body Association](https://openreview.net/forum?id=pPh9p8anUi) |  | 0 | The detection of human parts (e.g., hands, face) and their correct association with individuals is an essential task, e.g., for ubiquitous human-machine interfaces and action recognition. Traditional methods often employ multi-stage processes, rely on cumbersome anchor-based systems, or do not... | Abhishek Sharma, Benjamin Planche, Huayi Zhou, Meng Zheng, Terrence Chen, Zhongpai Gao, Ziyan Wu |  |
| 2197 |  |  [Performance Gaps in Multi-view Clustering under the Nested Matrix-Tensor Model](https://openreview.net/forum?id=ILqA09Oeq2) |  | 0 | We study the estimation of a planted signal hidden in a recently introduced nested matrix-tensor model, which is an extension of the classical spiked rank-one tensor model, motivated by multi-view clustering. Prior work has theoretically examined the performance of a tensor-based approach, which... | Hugo Lebeau, José Henrique de Morais Goulart, Mohamed El Amine Seddik |  |
| 2198 |  |  [Deep Reinforcement Learning Guided Improvement Heuristic for Job Shop Scheduling](https://openreview.net/forum?id=jsWCmrsHHs) |  | 0 | Recent studies in using deep reinforcement learning (DRL) to solve Job-shop scheduling problems (JSSP) focus on construction heuristics. However, their performance is still far from optimality, mainly because the underlying graph representation scheme is unsuitable for modelling partial solutions... | Cong Zhang, Jie Zhang, Wen Song, Yaoxin Wu, Zhiguang Cao |  |
| 2199 |  |  [Communication-Efficient Gradient Descent-Accent Methods for Distributed Variational Inequalities: Unified Analysis and Local Updates](https://openreview.net/forum?id=hORCalGn3Z) |  | 0 | Distributed and federated learning algorithms and techniques associated primarily with minimization problems. However, with the increase of minimax optimization and variational inequality problems in machine learning, the necessity of designing efficient distributed/federated learning approaches... | Nicolas Loizou, Sayantan Choudhury, Sebastian U. Stich, Siqi Zhang |  |
| 2200 |  |  [Batch normalization is sufficient for universal function approximation in CNNs](https://openreview.net/forum?id=wOSYMHfENq) |  | 0 | Normalization techniques, for which Batch Normalization (BN) is a popular choice, is an integral part of many deep learning architectures and contributes significantly to the learning success. We provide a partial explanation for this phenomenon by proving that training normalization parameters... | Rebekka Burkholz |  |
| 2201 |  |  [Predicting Emergent Abilities with Infinite Resolution Evaluation](https://openreview.net/forum?id=lDbjooxLkD) |  | 0 | The scientific scale-up of large language models (LLMs) necessitates a comprehensive understanding of their scaling properties. However, the existing literature on the scaling properties only yields an incomplete answer: optimization loss decreases predictably as the model size increases, in line... | Chaoqun He, Guoyang Zeng, Maosong Sun, Ning Ding, Shengding Hu, Weilin Zhao, Xin Liu, Xinrong Zhang, Xu Han, Yankai Lin, Zebin Ou, Zhiyuan Liu |  |
| 2202 |  |  [Graph-constrained diffusion for End-to-End Path Planning](https://openreview.net/forum?id=vuK8MhVtuu) |  | 0 | Path planning underpins various applications such as transportation, logistics, and robotics. Conventionally, path planning is formulated with explicit optimization objectives such as distance or time. However, real-world data reveals that user intentions are hard-to-model, suggesting a need for... | Dingyuan Shi, Jieping Ye, Ke Xu, Yongxin Tong, Zheng Wang, Zimu Zhou |  |
| 2203 |  |  [Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control](https://openreview.net/forum?id=Pc8AU1aF5e) |  | 0 | Building agents with large language models (LLMs) for computer control is a burgeoning research area, where the agent receives computer states and performs actions to complete complex tasks. Previous computer agents have demonstrated the benefits of in-context learning (ICL); however, their... | Bo An, Longtao Zheng, Rundong Wang, Xinrun Wang |  |
| 2204 |  |  [Elastic Feature Consolidation For Cold Start Exemplar-Free Incremental Learning](https://openreview.net/forum?id=7D9X2cFnt1) |  | 0 | Exemplar-Free Class Incremental Learning (EFCIL) aims to learn from a sequence of tasks without having access to previous task data. In this paper, we consider the challenging Cold Start scenario in which insufficient data is available in the first task to learn a high-quality backbone. This is... | Albin SoutifCormerais, Andrew D. Bagdanov, Joost van de Weijer, Simone Magistri, Tomaso Trinci |  |
| 2205 |  |  [Adversarial Causal Bayesian Optimization](https://openreview.net/forum?id=YcW8i9VCf5) |  | 0 | In Causal Bayesian Optimization (CBO), an agent intervenes on a structural causal model with known graph but unknown mechanisms to maximize a downstream reward variable. In this paper, we consider the generalization where other agents or external events also intervene on the system, which is key... | Anastasia Makarova, Andreas Krause, Pier Giuseppe Sessa, Scott Sussex |  |
| 2206 |  |  [Text-to-3D with Classifier Score Distillation](https://openreview.net/forum?id=ktG8Tun1Cy) |  | 0 | Text-to-3D generation has made remarkable progress recently, particularly with methods based on Score Distillation Sampling (SDS) that leverages pre-trained 2D diffusion models. While the usage of classifier-free guidance is well acknowledged to be crucial for successful optimization, it is... | Ding Liang, SongHai Zhang, Xiaojuan Qi, Xin Yu, Yangguang Li, YuanChen Guo |  |
| 2207 |  |  [Accurate Forgetting for Heterogeneous Federated Continual Learning](https://openreview.net/forum?id=ShQrnAsbPI) |  | 0 | Recent years have witnessed a burgeoning interest in federated learning (FL). However, the contexts in which clients engage in sequential learning remain under- explored. Bridging FL and continual learning (CL) gives rise to a challenging practical problem: federated continual learning (FCL).... | Abudukelimu Wuerkaixi, Bo Han, Changshui Zhang, Gang Niu, Jingfeng Zhang, Kunda Yan, Lei Fang, Masashi Sugiyama, Sen Cui |  |
| 2208 |  |  [GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with Noisy Polarization Priors](https://openreview.net/forum?id=pTN8dV2pL8) |  | 0 | Learning surfaces from neural radiance field (NeRF) became a rising topic in Multi-View Stereo (MVS). Recent Signed Distance Function (SDF)-based methods demonstrated their ability to reconstruct exact 3D shapes of Lambertian scenes. However, their results on reflective scenes are unsatisfactory... | Jiyong Li, Li Yang, Ruizheng Wu, YingCong Chen |  |
| 2209 |  |  [Porf: Pose residual field for accurate Neural surface Reconstruction](https://openreview.net/forum?id=eBeECjacpw) |  | 0 | Neural surface reconstruction is sensitive to the camera pose noise, even when state-of-the-art pose estimators like COLMAP or ARKit are used. Existing Pose-NeRF joint optimisation methods have struggled to improve pose accuracy in challenging real-world scenarios. To overcome the challenges, we... | JiaWang Bian, Philip Torr, Victor Adrian Prisacariu, Wenjing Bian |  |
| 2210 |  |  [Modelling complex vector drawings with stroke-clouds](https://openreview.net/forum?id=O2jyuo89CK) |  | 0 | Vector drawings are innately interactive as they preserve creational cues. Despite this desirable property they remain relatively under explored due to the difficulties in modeling complex vector drawings. This is in part due to the primarily _sequential and auto-regressive nature_ of existing... | Alexander Ashcroft, Ayan Das, YiZhe Song, Yulia Gryaditskaya, Zhiyu Qu |  |
| 2211 |  |  [Spurious Feature Diversification Improves Out-of-distribution Generalization](https://openreview.net/forum?id=d6H4RBi7RH) |  | 0 | Generalization to out-of-distribution (OOD) data is a critical challenge in machine learning. Ensemble-based methods, like weight space ensembles that interpolate model parameters, have been shown to achieve superior OOD performance. However, the underlying mechanism for their effectiveness remains... | Hanze Dong, Honam Wong, Lu Tan, Tong Zhang, Weizhong Zhang, Yifan Hao, Yong Lin, Yujiu Yang |  |
| 2212 |  |  [Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image Diffusion Models](https://openreview.net/forum?id=28L2FCtMWq) |  | 0 | This paper introduces a novel grounding-guided video-to-video translation framework called Ground-A-Video for multi-attribute video editing. Recent endeavors in video editing have showcased promising results in single-attribute editing or style transfer tasks, either by training T2V models on... | Hyeonho Jeong, Jong Chul Ye |  |
| 2213 |  |  [Scalable Language Model with Generalized Continual Learning](https://openreview.net/forum?id=mz8owj4DXu) |  | 0 | Continual learning has gained increasing importance as it facilitates the acquisition and refinement of scalable knowledge and skills in language models. However, existing methods typically encounter strict limitations and challenges in real-world scenarios, such as reliance on experience replay,... | Bohao Peng, Jiaya Jia, MingChang Yang, Shu Liu, Zhuotao Tian |  |
| 2214 |  |  [Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios](https://openreview.net/forum?id=vRyp2dhEQp) |  | 0 | Recent deep neural networks (DNNs) have came to rely on vast amounts of training data, providing an opportunity for malicious attackers to exploit and contaminate the data to carry out backdoor attacks. However, existing backdoor attack methods make unrealistic assumptions, assuming that all... | Beihao Xia, Bin Li, Heng Li, Hong Sun, Pengfei Xia, Yi Wu, Ziqiang Li |  |
| 2215 |  |  [Symmetric Basis Convolutions for Learning Lagrangian Fluid Mechanics](https://openreview.net/forum?id=HKgRwNhI9R) |  | 0 | Learning physical simulations has been an essential and central aspect of many recent research efforts in machine learning, particularly for Navier-Stokes-based fluid mechanics. Classic numerical solvers have traditionally been computationally expensive and challenging to use in inverse problems,... | Nils Thuerey, Rene Winchenbach |  |
| 2216 |  |  [G2N2 : Weisfeiler and Lehman go grammatical](https://openreview.net/forum?id=eZneJ55mRO) |  | 0 | This paper introduces a framework for formally establishing a connection between a portion of an algebraic language and a Graph Neural Network (GNN). The framework leverages Context-Free Grammars (CFG) to organize algebraic operations into generative rules that can be translated into a GNN layer... | Aldo Moscatelli, Jason Piquenot, JeanYves Ramel, Maxime Berar, Pierre Héroux, Romain Raveaux, Sébastien Adam |  |
| 2217 |  |  [VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks](https://openreview.net/forum?id=glwwbaeKm2) |  | 0 | Vertical Federated Learning (VFL) is a crucial paradigm for training machine learning models on feature-partitioned, distributed data. However, due to privacy restrictions, few public real-world VFL datasets exist for algorithm evaluation, and these represent a limited array of feature... | Bingsheng He, Junyi Hou, Zhaomin Wu |  |
| 2218 |  |  [Multimodal Web Navigation with Instruction-Finetuned Foundation Models](https://openreview.net/forum?id=efFmBWioSc) |  | 0 | The progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data. In this work, we study... | Aleksandra Faust, Hiroki Furuta, Izzeddin Gur, KuangHuei Lee, Ofir Nachum, Shixiang Shane Gu, Yutaka Matsuo |  |
| 2219 |  |  [Real-Fake: Effective Training Data Synthesis Through Distribution Matching](https://openreview.net/forum?id=svIdLLZpsA) |  | 0 | Synthetic training data has gained prominence in numerous learning tasks and scenarios, offering advantages such as dataset augmentation, generalization evaluation, and privacy preservation. Despite these benefits, the efficiency of synthetic data generated by current methodologies remains... | Bo Zhao, Jianhao Yuan, Jie Zhang, Philip Torr, Shuyang Sun |  |
| 2220 |  |  [Learning Conditional Invariances through Non-Commutativity](https://openreview.net/forum?id=tUVG9nGzgE) |  | 0 | Invariance learning algorithms that conditionally filter out domain-specific random variables as distractors, do so based only on the data semantics, and not the target domain under evaluation. We show that a provably optimal and sample-efficient way of learning conditional invariances is by... | Abhra Chaudhuri, Anjan Dutta, Serban Georgescu |  |
| 2221 |  |  [GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher](https://openreview.net/forum?id=MbfAK4s61A) |  | 0 | Safety lies at the core of the development of Large Language Models (LLMs). There is ample work on aligning LLMs with human ethics and preferences, including data filtering in pretraining, supervised fine-tuning, reinforcement learning from human feedback, red teaming, etc. In this study, we... | Jentse Huang, Pinjia He, Shuming Shi, Wenxiang Jiao, Wenxuan Wang, Youliang Yuan, Zhaopeng Tu |  |
| 2222 |  |  [Towards the Fundamental Limits of Knowledge Transfer over Finite Domains](https://openreview.net/forum?id=Zh2iqiOtMt) |  | 0 | We characterize the statistical efficiency of knowledge transfer through $n$ samples from a teacher to a probabilistic student classifier with input space $\mathcal{S}$ over labels $\mathcal{A}$. We show that privileged information at three progressive levels accelerates the transfer. At the first... | Banghua Zhu, Qingyue Zhao |  |
| 2223 |  |  [Self-Supervised Speech Quality Estimation and Enhancement Using Only Clean Speech](https://openreview.net/forum?id=ale56Ya59q) |  | 0 | Speech quality estimation has recently undergone a paradigm shift from human-hearing expert designs to machine-learning models. However, current models rely mainly on supervised learning, which is time-consuming and expensive for label collection. To solve this problem, we propose VQScore, a... | KuoHsuan Hung, SzuWei Fu, Yu Tsao, YuChiang Frank Wang |  |
| 2224 |  |  [Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning](https://openreview.net/forum?id=Cc0qk6r4Nd) |  | 0 | Federated learning (FL) inevitably confronts the challenge of system heterogeneity in practical scenarios. To enhance the capabilities of most model-homogeneous FL methods in handling system heterogeneity, we propose a training scheme that can extend their capabilities to cope with this challenge.... | Edith C. H. Ngai, Rui Zhou, Running Zhao, YunHin Chan, Zhihan Jiang |  |
| 2225 |  |  [Contrastive Learning is Spectral Clustering on Similarity Graph](https://openreview.net/forum?id=hLZQTFGToA) |  | 0 | Contrastive learning is a powerful self-supervised learning method, but we have a limited theoretical understanding of how it works and why it works. In this paper, we prove that contrastive learning with the standard InfoNCE loss is equivalent to spectral clustering on the similarity graph. Using... | Jingqin Yang, Yang Yuan, Yifan Zhang, Zhiquan Tan |  |
| 2226 |  |  [On the Generalization and Approximation Capacities of Neural Controlled Differential Equations](https://openreview.net/forum?id=kILAd8RdzA) |  | 0 | Neural Controlled Differential Equations (NCDE) are a state-of-the-art tool for supervised learning with irregularly sampled time series (Kidger 2020). However, no theoretical analysis of their performance has been provided yet, and it remains unclear in particular how the roughness of the sampling... | Agathe Guilloux, Linus Bleistein |  |
| 2227 |  |  [Epitopological learning and Cannistraci-Hebb network shape intelligence brain-inspired theory for ultra-sparse advantage in deep learning](https://openreview.net/forum?id=iayEcORsGd) |  | 0 | Sparse training (ST) aims to ameliorate deep learning by replacing fully connected artificial neural networks (ANNs) with sparse or ultra-sparse ones, such as brain networks are, therefore it might benefit to borrow brain-inspired learning paradigms from complex network intelligence theory. Here,... | Alessandro Muscoloni, Carlo Vittorio Cannistraci, Jialin Zhao, Wenjing Wu, Yingtao Zhang |  |
| 2228 |  |  [Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models](https://openreview.net/forum?id=Tr0lPx9woF) |  | 0 | With the rapid growth of large language models (LLMs), there is increasing demand for memory and computation in LLMs. Recent efforts on post-training pruning of LLMs aim to reduce the model size and computation requirements, yet the performance is still sub-optimal. In this paper, we present a... | Carlo Vittorio Cannistraci, Haokun Lin, Haoli Bai, Jialin Zhao, Lu Hou, Yingtao Zhang |  |
| 2229 |  |  [Universal Jailbreak Backdoors from Poisoned Human Feedback](https://openreview.net/forum?id=GxCGsxiAaK) |  | 0 | Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an... | Florian Tramèr, Javier Rando |  |
| 2230 |  |  [Neural Field Classifiers via Target Encoding and Classification Loss](https://openreview.net/forum?id=9NqC72m31m) |  | 0 | Neural field methods have seen great progress in various long-standing tasks in computer vision and computer graphics, including novel view synthesis and geometry reconstruction. As existing neural field methods try to predict some coordinate-based continuous target values, such as RGB for Neural... | Boyu Liu, Buhua Liu, Haoran Wang, Mingming Sun, Xindi Yang, Xiong Zhou, Yi Liu, Yunfeng Cai, Zeke Xie |  |
| 2231 |  |  [Fusion Is Not Enough: Single Modal Attacks on Fusion Models for 3D Object Detection](https://openreview.net/forum?id=3VD4PNEt5q) |  | 0 | Multi-sensor fusion (MSF) is widely used in autonomous vehicles (AVs) for perception, particularly for 3D object detection with camera and LiDAR sensors. The purpose of fusion is to capitalize on the advantages of each modality while minimizing its weaknesses. Advanced deep neural network... | Dongfang Liu, Guanhong Tao, Hongjun Choi, James Chenhao Liang, Michael Zuzak, Shiwei Feng, Xiangyu Zhang, Zhiyuan Cheng |  |
| 2232 |  |  [LipVoicer: Generating Speech from Silent Videos Guided by Lip Reading](https://openreview.net/forum?id=ZZCPSC5OgD) |  | 0 | Lip-to-speech involves generating a natural-sounding speech synchronized with a soundless video of a person talking. Despite recent advances, current methods still cannot produce high-quality speech with high levels of intelligibility for challenging and realistic datasets such as LRS3. In this... | Aviv Shamsian, Ethan Fetaya, Lior Bracha, Sharon Gannot, Yochai Yemini |  |
| 2233 |  |  [Window Attention is Bugged: How not to Interpolate Position Embeddings](https://openreview.net/forum?id=IPhm01y9a9) |  | 0 | Window attention, position embeddings, and high resolution finetuning are core concepts in the modern transformer era of computer vision. However, we find that naively combining these near ubiquitous components can have a detrimental effect on performance. The issue is simple: interpolating... | Chaitanya Ryali, Christoph Feichtenhofer, Daniel Bolya, Judy Hoffman |  |
| 2234 |  |  [Embodied Active Defense: Leveraging Recurrent Feedback to Counter Adversarial Patches](https://openreview.net/forum?id=uXjfOmTiDt) |  | 0 | The vulnerability of deep neural networks to adversarial patches has motivated numerous defense strategies for boosting model robustness. However, the prevailing defenses depend on single observation or pre-established adversary information to counter adversarial patches, often failing to be... | Hang Su, Jun Zhu, Lingxuan Wu, Liuwei Xie, Xiao Yang, Yinpeng Dong |  |
| 2235 |  |  [Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks](https://openreview.net/forum?id=1SbkubNdbW) |  | 0 | Label smoothing – using softened labels instead of hard ones – is a widely adopted regularization method for deep learning, showing diverse benefits such as enhanced generalization and calibration. Its implications for preserving model privacy, however, have remained unexplored. To fill this gap,... | Dominik Hintersdorf, Kristian Kersting, Lukas Struppek |  |
| 2236 |  |  [Continuous Invariance Learning](https://openreview.net/forum?id=70IgE3tRbu) |  | 0 | Invariance learning methods aim to learn invariant features in the hope that they generalize under distributional shift. Although many tasks are naturally characterized by continuous domains, current invariance learning techniques generally assume categorically indexed domains. For example,... | Fan Zhou, Hao Wang, James Y. Zhang, Jianmeng Liu, Lin Yong, Lintao Ma, Lu Tan, Yansu He, Yu Liu, Yuan Yuan, Yujiu Yang |  |
| 2237 |  |  [ZipIt! Merging Models from Different Tasks without Training](https://openreview.net/forum?id=LEYUkvdUhq) |  | 0 | Typical deep visual recognition models are capable of performing the one task they were trained on. In this paper, we tackle the extremely difficult problem of combining distinct models with different initializations, each solving a separate task, into one multi-task model without any additional... | Daniel Bolya, George Stoica, Jakob Bjorner, Judy Hoffman, Pratik Ramesh, Taylor Hearn |  |
| 2238 |  |  [Hybrid Distillation: Connecting Masked Autoencoders with Contrastive Learners](https://openreview.net/forum?id=jUWktnsplU) |  | 0 | As two prominent strategies for representation learning, Contrastive Learning (CL) and Masked Image Modeling (MIM) have witnessed significant progress. Previous studies have demonstrated the advantages of each approach in specific scenarios. CL, resembling supervised pre-training, excels at... | Bowen Shi, Hongkai Xiong, Jin Li, Junni Zou, Qi Tian, Wenrui Dai, Xiaopeng Zhang, Yaoming Wang |  |
| 2239 |  |  [Facing the Elephant in the Room: Visual Prompt Tuning or Full finetuning?](https://openreview.net/forum?id=bJx4iOIOxn) |  | 0 | As the scale of vision models continues to grow, the emergence of Visual Prompt Tuning (VPT) as a parameter-efficient transfer learning technique has gained attention due to its superior performance compared to traditional full-finetuning. However, the conditions favoring VPT (the "when") and the... | Cheng Han, Dongfang Liu, Lifu Huang, Qifan Wang, Siyuan Qi, Wenguan Wang, Yiming Cui |  |
| 2240 |  |  [Grounding Multimodal Large Language Models to the World](https://openreview.net/forum?id=lLmqxkfSIw) |  | 0 | We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent text spans (i.e., referring expressions and noun phrases) as links in Markdown,... | Furu Wei, Li Dong, Qixiang Ye, Shaohan Huang, Shuming Ma, Wenhui Wang, Yaru Hao, Zhiliang Peng |  |
| 2241 |  |  [VFLAIR: A Research Library and Benchmark for Vertical Federated Learning](https://openreview.net/forum?id=sqRgz88TM3) |  | 0 | Vertical Federated Learning (VFL) has emerged as a collaborative training paradigm that allows participants with different features of the same group of users to accomplish cooperative training without exposing their raw data or model parameters. VFL has gained significant attention for its... | Hideaki Takahashi, Tianyuan Zou, YaQin Zhang, Yang Liu, Yu He, Zixuan Gu |  |
| 2242 |  |  [IRAD: Implicit Representation-driven Image Resampling against Adversarial Attacks](https://openreview.net/forum?id=jFa5KESW65) |  | 0 | We introduce a novel approach to counter adversarial attacks, namely, image resampling. Image resampling transforms a discrete image into a new one, simulating the process of scene recapturing or rerendering as specified by a geometrical transformation. The underlying rationale behind our idea is... | Ivor W. Tsang, Qing Guo, Tianlin Li, Xiaofeng Cao, Yang Liu, Yue Cao |  |
| 2243 |  |  [Stylized Offline Reinforcement Learning: Extracting Diverse High-Quality Behaviors from Heterogeneous Datasets](https://openreview.net/forum?id=rnHNDihrIT) |  | 0 | Previous literature on policy diversity in reinforcement learning (RL) either focuses on the online setting or ignores the policy performance. In contrast, offline RL, which aims to learn high-quality policies from batched data, has yet to fully leverage the intrinsic diversity of the offline... | Changjie Fan, Chengjie Wu, Chongjie Zhang, Hao Hu, Ji Jiang, Tangjie Lv, Tianze Zhou, Xi Chen, Yi Wu, Yihuan Mao, Yujing Hu, Zhipeng Hu |  |
| 2244 |  |  [Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight](https://openreview.net/forum?id=1hsVvgW0rU) |  | 0 | This paper studies the sample-efficiency of learning in Partially Observable Markov Decision Processes (POMDPs), a challenging problem in reinforcement learning that is known to be exponentially hard in the worst-case. Motivated by real-world settings such as loading in game playing, we propose an... | Caiming Xiong, Huan Wang, Jiacheng Guo, Mengdi Wang, Minshuo Chen, Yu Bai |  |
| 2245 |  |  [Pre-training with Synthetic Data Helps Offline Reinforcement Learning](https://openreview.net/forum?id=PcxQgtHGj2) |  | 0 | Recently, it has been shown that for offline deep reinforcement learning (DRL), pre-training Decision Transformer with a large language corpus can improve downstream performance (Reid et al., 2022). A natural question to ask is whether this performance gain can only be achieved with language... | Che Wang, Keith W. Ross, Zecheng Wang, Zixuan Dong |  |
| 2246 |  |  [AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors](https://openreview.net/forum?id=EHg5GDnyq1) |  | 0 | Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task... | Chen Qian, Chenfei Yuan, Cheng Yang, ChiMin Chan, Heyang Yu, Jie Zhou, Jingwei Zuo, Maosong Sun, Ruobing Xie, Weize Chen, Xin Cong, Yaxi Lu, YiHsin Hung, Yujia Qin, Yusheng Su, Zhiyuan Liu |  |
| 2247 |  |  [IceFormer: Accelerated Inference with Long-Sequence Transformers on CPUs](https://openreview.net/forum?id=6RR3wU4mSZ) |  | 0 | One limitation of existing Transformer-based models is that they cannot handle very long sequences as input since their self-attention operations exhibit quadratic time and space complexity. This problem becomes especially acute when Transformers are deployed on hardware platforms equipped only... | Ke Li, Martin Ester, Yuzhen Mao |  |
| 2248 |  |  [Efficient Planning with Latent Diffusion](https://openreview.net/forum?id=btpgDo4u4j) |  | 0 | Temporal abstraction and efficient planning pose significant challenges in offline reinforcement learning, mainly when dealing with domains that involve temporally extended tasks and delayed sparse rewards. Existing methods typically plan in the raw action space and can be inefficient and... | Wenhao Li |  |
| 2249 |  |  [Conformal Prediction via Regression-as-Classification](https://openreview.net/forum?id=rulxyXjf46) |  | 0 | Conformal prediction (CP) for regression can be challenging, especially when the output distribution is heteroscedastic, multimodal, or skewed. Some of the issues can be addressed by estimating a distribution over the output, but in reality, such approaches can be sensitive to estimation error and... | Etash Kumar Guha, Eugène Ndiaye, Mohammad Emtiyaz Khan, Shlok Natarajan, Thomas Möllenhoff |  |
| 2250 |  |  [Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model](https://openreview.net/forum?id=ezscMer8L0) |  | 0 | The Segment-Anything Model (SAM) stands as a foundational framework for image segmentation. While it exhibits remarkable zero-shot generalization in typical scenarios, its advantage diminishes when applied to specialized domains like medical imagery and remote sensing. To address this limitation,... | Chun Yuan, Haoyang Fang, Tong He, Zhiqiang Tang, Zihan Zhong |  |
| 2251 |  |  [Tag2Text: Guiding Vision-Language Model via Image Tagging](https://openreview.net/forum?id=x6u2BQ7xcq) |  | 0 | This paper presents Tag2Text, a vision language pre-training (VLP) framework, which introduces image tagging into vision-language models to guide the learning of visual-linguistic features. In contrast to prior works which utilize object tags either manually labeled or automatically detected with a... | Jinyu Ma, Lei Zhang, Rui Feng, Weiwei Tian, Xinyu Huang, Yandong Guo, Yaqian Li, Youcai Zhang, Yuejie Zhang |  |
| 2252 |  |  [Class Incremental Learning via Likelihood Ratio Based Task Prediction](https://openreview.net/forum?id=8QfK9Dq4q0) |  | 0 | Class incremental learning (CIL) is a challenging setting of continual learning, which learns a series of tasks sequentially. Each task consists of a set of unique classes. The key feature of CIL is that no task identifier (or task-id) is provided at test time. Predicting the task-id for each test... | Bing Liu, Haowei Lin, Ningxin Pan, Weinan Qian, Yiduo Guo, Yijia Shao |  |
| 2253 |  |  [Decentralized Riemannian Conjugate Gradient Method on the Stiefel Manifold](https://openreview.net/forum?id=PQbFUMKLFp) |  | 0 | The conjugate gradient method is a crucial first-order optimization method that generally converges faster than the steepest descent method, and its computational cost is much lower than that of second-order methods. However, while various types of conjugate gradient methods have been studied in... | Guang Dai, Haishan Ye, Ivor W. Tsang, Jun Chen, Mengmeng Wang, Tianxin Huang, Yong Liu |  |
| 2254 |  |  [Improved Regret Bounds for Non-Convex Online-Within-Online Meta Learning](https://openreview.net/forum?id=pA8Q5WiEMg) |  | 0 | Online-Within-Online (OWO) meta learning stands for the online multi-task learning paradigm in which both tasks and data within each task become available in a sequential order. In this work, we study the OWO meta learning of the initialization and step size of within-task online algorithms in the... | Hui Xiong, Jiechao Guan |  |
| 2255 |  |  [Momentum Benefits Non-iid Federated Learning Simply and Provably](https://openreview.net/forum?id=TdhkAcXkRi) |  | 0 | Federated learning is a powerful paradigm for large-scale machine learning, but it faces significant challenges due to unreliable network connections, slow commu- nication, and substantial data heterogeneity across clients. FedAvg and SCAFFOLD are two prominent algorithms to address these... | Kun Yuan, Pengfei Wu, Xinmeng Huang, Ziheng Cheng |  |
| 2256 |  |  [Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy](https://openreview.net/forum?id=3fEKavFsnv) |  | 0 | Large language models (LLMs) such as ChatGPT have exhibited remarkable performance in generating human-like texts. However, machine-generated texts (MGTs) may carry critical risks, such as plagiarism issues and hallucination information. Therefore, it is very urgent and important to detect MGTs in... | Bo Han, Jiahao Yang, Mingkui Tan, Shuhai Zhang, Yiliao Song, Yuanqing Li |  |
| 2257 |  |  [Only Pay for What Is Uncertain: Variance-Adaptive Thompson Sampling](https://openreview.net/forum?id=p8ujRTjEf3) |  | 0 | Most bandit algorithms assume that the reward variances or their upper bounds are known, and that they are the same for all arms. This naturally leads to suboptimal performance and higher regret due to variance overestimation. On the other hand, underestimated reward variances may lead to linear... | Aadirupa Saha, Branislav Kveton |  |
| 2258 |  |  [PnP Inversion: Boosting Diffusion-based Editing with 3 Lines of Code](https://openreview.net/forum?id=FoMZ4ljhVw) |  | 0 | Text-guided diffusion models have revolutionized image generation and editing, offering exceptional realism and diversity. Specifically, in the context of diffusion-based editing, where a source image is edited according to a target prompt, the process commences by acquiring a noisy latent vector... | Ailing Zeng, Qiang Xu, Shaoteng Liu, Xuan Ju, Yuxuan Bian |  |
| 2259 |  |  [A Benchmark Study on Calibration](https://openreview.net/forum?id=GzNhzX9kVa) |  | 0 | Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through the use of specific loss... | Chang Xu, Haolan Guo, Linwei Tao, Minjing Dong, Younan Zhu |  |
| 2260 |  |  [Enhancing Human-AI Collaboration Through Logic-Guided Reasoning](https://openreview.net/forum?id=TWC4gLoAxY) |  | 0 | We present a systematic framework designed to enhance human-robot perception and collaboration through the integration of logical rules and Theory of Mind (ToM). Logical rules provide interpretable predictions and generalize well across diverse tasks, making them valuable for learning and... | Chengzhi Cao, Ruimao Zhang, Sheng Xu, Shuang Li, Yinghao Fu |  |
| 2261 |  |  [Learning with Mixture of Prototypes for Out-of-Distribution Detection](https://openreview.net/forum?id=uNkKaD3MCs) |  | 0 | Out-of-distribution (OOD) detection aims to detect testing samples far away from the in-distribution (ID) training data, which is crucial for the safe deployment of machine learning models in the real world. Distance-based OOD detection methods have emerged with enhanced deep representation... | Dong Gong, Haodong Lu, Jason Xue, Kristen Moore, Lina Yao, Shuo Wang |  |
| 2262 |  |  [PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks](https://openreview.net/forum?id=DO2WFXU1Be) |  | 0 | Physics-Informed Neural Networks (PINNs) have emerged as a promising deep learning framework for approximating numerical solutions to partial differential equations (PDEs). However, conventional PINNs, relying on multilayer perceptrons (MLP), neglect the crucial temporal dependencies inherent in... | B. Aditya Prakash, Leo Zhiyuan Zhao, Xueying Ding |  |
| 2263 |  |  [Attention-Guided Contrastive Role Representations for Multi-agent Reinforcement Learning](https://openreview.net/forum?id=LWmuPfEYhH) |  | 0 | Real-world multi-agent tasks usually involve dynamic team composition with the emergence of roles, which should also be a key to efficient cooperation in multi-agent reinforcement learning (MARL). Drawing inspiration from the correlation between roles and agent's behavior patterns, we propose a... | Chunlin Chen, Hongyu Ding, Huaxiong Li, Zhi Wang, Zican Hu, Zongzhang Zhang |  |
| 2264 |  |  [A Statistical Analysis of Wasserstein Autoencoders for Intrinsically Low-dimensional Data](https://openreview.net/forum?id=WjRPZsfeBO) |  | 0 | Variational Autoencoders (VAEs) have gained significant popularity among researchers as a powerful tool for understanding unknown distributions based on limited samples. This popularity stems partly from their impressive performance and partly from their ability to provide meaningful feature... | Peter L. Bartlett, Saptarshi Chakraborty |  |
| 2265 |  |  [Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models](https://openreview.net/forum?id=gfFVATffPd) |  | 0 | We investigate the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text. We propose modeling factual queries as constraint satisfaction problems and use this framework to investigate how the LLM interacts internally with factual... | Besmira Nushi, Ece Kamar, Erik Jones, Hamid Palangi, Mert Yüksekgönül, Ranjita Naik, Suriya Gunasekar, Varun Chandrasekaran |  |
| 2266 |  |  [Structuring Representation Geometry with Rotationally Equivariant Contrastive Learning](https://openreview.net/forum?id=lgaFMvZHSJ) |  | 0 | Self-supervised learning converts raw perceptual data such as images to a compact space where simple Euclidean distances measure meaningful variations in data. In this paper, we extend this formulation by adding additional geometric structure to the embedding space by enforcing transformations of... | Derek Lim, Joshua Robinson, Sharut Gupta, Soledad Villar, Stefanie Jegelka |  |
| 2267 |  |  [Decodable and Sample Invariant Continuous Object Encoder](https://openreview.net/forum?id=QLoepRnoue) |  | 0 | We propose Hyper-Dimensional Function Encoding (HDFE). Given samples of a continuous object (e.g. a function), HDFE produces an explicit vector representation of the given object, invariant to the sample distribution and density. Sample distribution and density invariance enables HDFE to... | Cornelia Fermüller, Dehao Yuan, Furong Huang, Yiannis Aloimonos |  |
| 2268 |  |  [Neural Snowflakes: Universal Latent Graph Inference via Trainable Latent Geometries](https://openreview.net/forum?id=djM3WzpOmK) |  | 0 | The inductive bias of a graph neural network (GNN) is largely encoded in its specified graph. Latent graph inference relies on latent geometric representations to dynamically rewire or infer a GNN's graph to maximize the GNN's predictive downstream performance, but it lacks solid theoretical... | Anastasis Kratsios, Haitz Sáez de Ocáriz Borde |  |
| 2269 |  |  [Context is Environment](https://openreview.net/forum?id=8VPWfqtQMX) |  | 0 | Two lines of work are taking the central stage in AI research. On the one hand, the community is making increasing efforts to build models that discard spurious correlations and generalize better in novel test environments. Unfortunately, the hard lesson so far is that no proposal convincingly... | David LopezPaz, Kartik Ahuja, Sharut Gupta, Stefanie Jegelka |  |
| 2270 |  |  [Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions](https://openreview.net/forum?id=RLSWbk9kPw) |  | 0 | Sorting is a fundamental operation of all computer systems, having been a long-standing significant research topic. Beyond the problem formulation of traditional sorting algorithms, we consider sorting problems for more abstract yet expressive inputs, e.g., multi-digit images and image fragments,... | Jeongbeen Yoon, Jungtaek Kim, Minsu Cho |  |
| 2271 |  |  [IMPUS: Image Morphing with Perceptually-Uniform Sampling Using Diffusion Models](https://openreview.net/forum?id=gG38EBe2S8) |  | 0 | We present a diffusion-based image morphing approach with perceptually-uniform sampling (IMPUS) that produces smooth, direct and realistic interpolations given an image pair. The embeddings of two images may lie on distinct conditioned distributions of a latent diffusion model, especially when they... | Dylan Campbell, Jaskirat Singh, Jing Zhang, Peter H. Tu, Richard Hartley, Zhaoyuan Yang, Zhengyang Yu, Zhiwei Xu |  |
| 2272 |  |  [Adaptive Instrument Design for Indirect Experiments](https://openreview.net/forum?id=4Zz5UELkIt) |  | 0 | Indirect experiments provide a valuable framework for estimating treatment effects in situations where conducting randomized control trials (RCTs) is impractical or unethical. Unlike RCTs, indirect experiments estimate treatment effects by leveraging (conditional) instrumental variables, enabling... | Emma Brunskill, Shiv Shankar, Vasilis Syrgkanis, Yash Chandak |  |
| 2273 |  |  [Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning](https://openreview.net/forum?id=HiYMiZYwkw) |  | 0 | Self-supervised learning excels in learning representations from large amounts of unlabeled data, demonstrating success across multiple data modalities. Yet, extending self-supervised learning to new modalities is non-trivial because the specifics of existing methods are tailored to each domain,... | Annie S. Chen, Chelsea Finn, Johnathan Xie, Yoonho Lee |  |
| 2274 |  |  [Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL](https://openreview.net/forum?id=N6o0ZtPzTg) |  | 0 | In this study, we aim to enhance the arithmetic reasoning ability of Large Language Models (LLMs) through zero-shot prompt optimization. We identify a previously overlooked objective of query dependency in such optimization and elucidate two ensuing challenges that impede the successful and... | Alihan Hüyük, Hao Sun, Mihaela van der Schaar |  |
| 2275 |  |  [BioBridge: Bridging Biomedical Foundation Models via Knowledge Graphs](https://openreview.net/forum?id=jJCeMiwHdH) |  | 0 | Foundation models (FMs) learn from large volumes of unlabeled data to demonstrate superior performance across a wide range of tasks. However, FMs developed for biomedical domains have largely remained unimodal, i.e., independently trained and used for tasks on protein sequences alone, small... | Balasubramaniam Srinivasan, Huzefa Rangwala, Rishita Anubhai, Vassilis N. Ioannidis, Zichen Wang, Zifeng Wang |  |
| 2276 |  |  [Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning](https://openreview.net/forum?id=1jbh2e0b2K) |  | 0 | Foundation models have emerged as a powerful tool for many AI problems. Despite the tremendous success of foundation models, effective adaptation to new tasks, particularly those with limited labels, remains an open question and lacks theoretical understanding. An emerging solution with recent... | Fangzhou Mu, Junyi Wei, Yin Li, Yingyu Liang, Zhenmei Shi, Zhuoyan Xu |  |
| 2277 |  |  [ReTaSA: A Nonparametric Functional Estimation Approach for Addressing Continuous Target Shift](https://openreview.net/forum?id=KdVvOA00Or) |  | 0 | The presence of distribution shifts poses a significant challenge for deploying modern machine learning models in real-world applications. This work focuses on the target shift problem in a regression setting (Zhang et al., 2013; Nguyen et al., 2016). More specifically, the target variable $y$... | Hwanwoo Kim, Jiwei Zhao, Qinglong Tian, Xin Zhang |  |
| 2278 |  |  [GPAvatar: Generalizable and Precise Head Avatar from Image(s)](https://openreview.net/forum?id=hgehGq2bDv) |  | 0 | Head avatar reconstruction, crucial for applications in virtual reality, online meetings, gaming, and film industries, has garnered substantial attention within the computer vision community. The fundamental objective of this field is to faithfully recreate the head avatar and precisely control... | Ailing Zeng, Lijian Lin, Tatsuya Harada, Tianyu Yang, Xuangeng Chu, Yu Li, Yunfei Liu |  |
| 2279 |  |  [Mind Your Augmentation: The Key to Decoupling Dense Self-Supervised Learning](https://openreview.net/forum?id=WQYHbr36Fo) |  | 0 | Dense Self-Supervised Learning (SSL) creates positive pairs by building positive paired regions or points, thereby aiming to preserve local features, for example of individual objects. However, existing approaches tend to couple objects by leaking information from the neighboring contextual regions... | Congpei Qiu, Mathieu Salzmann, Sabine Süsstrunk, Tong Zhang, Wei Ke, Yanhao Wu |  |
| 2280 |  |  [Entropy-MCMC: Sampling from Flat Basins with Ease](https://openreview.net/forum?id=oGNdBvymod) |  | 0 | Bayesian deep learning counts on the quality of posterior distribution estimation. However, the posterior of deep neural networks is highly multi-modal in nature, with local modes exhibiting varying generalization performance. Given a practical budget, targeting at the original posterior can lead... | Bolian Li, Ruqi Zhang |  |
| 2281 |  |  [Xformer: Hybrid X-Shaped Transformer for Image Denoising](https://openreview.net/forum?id=vXrIQLzIKY) |  | 0 | In this paper, we present a hybrid X-shaped vision Transformer, named Xformer, which performs notably on image denoising tasks. We explore strengthening the global representation of tokens from different scopes. In detail, we adopt two types of Transformer blocks. The spatial-wise Transformer block... | Jiahua Dong, Jiale Zhang, Jinjin Gu, Linghe Kong, Xiaokang Yang, Yulun Zhang |  |
| 2282 |  |  [Learning to Embed Time Series Patches Independently](https://openreview.net/forum?id=WS7GuBDFa2) |  | 0 | Masked time series modeling has recently gained much attention as a self-supervised representation learning strategy for time series. Inspired by masked image modeling in computer vision, recent works first patchify and partially mask out time series, and then train Transformers to capture the... | Kibok Lee, Seunghan Lee, Taeyoung Park |  |
| 2283 |  |  [Understanding Convergence and Generalization in Federated Learning through Feature Learning Theory](https://openreview.net/forum?id=EcetCr4trp) |  | 0 | Federated Learning (FL) has attracted significant attention as an efficient privacy-preserving approach to distributed learning across multiple clients. Despite extensive empirical research and practical applications, a systematic way to theoretically understand the convergence and generalization... | Taiji Suzuki, Wei Huang, Ye Shi, Zhongyi Cai |  |
| 2284 |  |  [Quantifying the Sensitivity of Inverse Reinforcement Learning to Misspecification](https://openreview.net/forum?id=pz2E1Q9Wni) |  | 0 | Inverse reinforcement learning (IRL) aims to infer an agent's \*preferences\* (represented as a reward function $R$) from their \*behaviour\* (represented as a policy $\pi$). To do this, we need a \*behavioural model\* of how $\pi$ relates to $R$. In the current literature, the most common... | Alessandro Abate, Joar Max Viktor Skalse |  |
| 2285 |  |  [Rethinking CNN's Generalization to Backdoor Attack from Frequency Domain](https://openreview.net/forum?id=mYhH0CDFFa) |  | 0 | Convolutional neural network (CNN) is easily affected by backdoor injections, whose models perform normally on clean samples but produce specific outputs on poisoned ones. Most of the existing studies have focused on the effect of trigger feature changes of poisoned samples on model generalization... | Lin Wang, Quanrui Rao, Wuying Liu |  |
| 2286 |  |  [LLCP: Learning Latent Causal Processes for Reasoning-based Video Question Answer](https://openreview.net/forum?id=Cu5wJa5LGO) |  | 0 | Current approaches to Video Question Answering (VideoQA) primarily focus on cross-modality matching, which is limited by the requirement for extensive data annotations and the insufficient capacity for causal reasoning (e.g. attributing accidents). To address these challenges, we introduce a causal... | Donglai Wei, Eman Al Suradi, Guangyi Chen, Kun Zhang, Xiao Liu, Yuke Li, Zijian Li |  |
| 2287 |  |  [PromptTTS 2: Describing and Generating Voices with Text Prompt](https://openreview.net/forum?id=NsCXDyv2Bn) |  | 0 | Speech conveys more information than text, as the same word can be uttered in various voices to convey diverse information. Compared to traditional text-to-speech (TTS) methods relying on speech prompts (reference speech) for voice variability, using text prompts (descriptions) is more... | Dongchao Yang, Eric Liu, Jiang Bian, Kai Shen, Kaitao Song, Lei He, Leying Zhang, Sheng Zhao, Tao Qin, Xiangyang Li, Xu Tan, Yichong Leng, Yufei Liu, Zeqian Ju, Zhifang Guo |  |
| 2288 |  |  [RTFS-Net: Recurrent Time-Frequency Modelling for Efficient Audio-Visual Speech Separation](https://openreview.net/forum?id=PEuDO2EiDr) |  | 0 | Audio-visual speech separation methods aim to integrate different modalities to generate high-quality separated speech, thereby enhancing the performance of downstream tasks such as speech recognition. Most existing state-of-the-art (SOTA) models operate in the time domain. However, their overly... | Kai Li, Samuel Pegg, Xiaolin Hu |  |
| 2289 |  |  [Consistent Video-to-Video Transfer Using Synthetic Dataset](https://openreview.net/forum?id=IoKRezZMxF) |  | 0 | We introduce a novel and efficient approach for text-based video-to-video editing that eliminates the need for resource-intensive per-video-per-model finetuning. At the core of our approach is a synthetic paired video dataset tailored for video-to-video transfer tasks. Inspired by Instruct... | Jiaxin Cheng, Tianjun Xiao, Tong He |  |
| 2290 |  |  [Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a Bayesian Game](https://openreview.net/forum?id=z6KS9D1dxt) |  | 0 | In this study, we explore the robustness of cooperative multi-agent reinforcement learning (c-MARL) against Byzantine failures, where any agent can enact arbitrary, worst-case actions due to malfunction or adversarial attack. To address the uncertainty that any agent can be adversarial, we propose... | Aishan Liu, Jiakai Wang, Jingqiao Xiu, Jun Guo, Ruixiao Xu, Simin Li, Xianglong Liu, Xin Yu, Yaodong Yang |  |
| 2291 |  |  [Scale-Adaptive Diffusion Model for Complex Sketch Synthesis](https://openreview.net/forum?id=5xadJmgwix) |  | 0 | While diffusion models have revolutionized generative AI, their application to human sketch generation, especially in the creation of complex yet concise and recognizable sketches, remains largely unexplored. Existing efforts have primarily focused on vector-based sketches, limiting their ability... | Jijin Hu, Ke Li, YiZhe Song, Yonggang Qi |  |
| 2292 |  |  [Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature](https://openreview.net/forum?id=Bpcgcr8E8Z) |  | 0 | Large language models (LLMs) have shown the ability to produce fluent and cogent content, presenting both productivity opportunities and societal risks. To build trustworthy AI systems, it is imperative to distinguish between machine-generated and human-authored content. The leading zero-shot... | Guangsheng Bao, Linyi Yang, Yanbin Zhao, Yue Zhang, Zhiyang Teng |  |
| 2293 |  |  [Defining and extracting generalizable interaction primitives from DNNs](https://openreview.net/forum?id=OCqyFVFNeF) |  | 0 | Faithfully summarizing the knowledge encoded by a deep neural network (DNN) into a few symbolic primitive patterns without losing much information represents a core challenge in explainable AI. To this end, Ren et al. (2024) have derived a series of theorems to prove that the inference score of a... | Benhao Huang, Lu Chen, Quanshi Zhang, Siyu Lou |  |
| 2294 |  |  [Revisiting Deep Audio-Text Retrieval Through the Lens of Transportation](https://openreview.net/forum?id=l60EM8md3t) |  | 0 | The Learning-to-match (LTM) framework proves to be an effective inverse optimal transport approach for learning the underlying ground metric between two sources of data, facilitating subsequent matching. However, the conventional LTM framework faces scalability challenges, necessitating the use of... | Dinh Phung, Gholamreza Haffari, Khai Nguyen, Lizhen Qu, Manh Luong, Nhat Ho |  |
| 2295 |  |  [Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching](https://openreview.net/forum?id=yzRXdhk2he) |  | 0 | Powered by large-scale pre-training, vision foundation models exhibit significant potential in open-world image understanding. However, unlike large language models that excel at directly tackling various language tasks, vision foundation models require a task-specific model structure followed by... | Chunhua Shen, Hao Chen, Hengtao Li, Muzhi Zhu, Xinlong Wang, Yang Liu |  |
| 2296 |  |  [LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language Models](https://openreview.net/forum?id=qCUWVT0Ayy) |  | 0 | Graphic layout generation, a growing research field, plays a significant role in user engagement and information perception. Existing methods primarily treat layout generation as a numerical optimization task, focusing on quantitative aspects while overlooking the semantic information of layout,... | Chenfei Wu, Juntao Li, Nan Duan, Zecheng Tang |  |
| 2297 |  |  [Analyzing and Mitigating Object Hallucination in Large Vision-Language Models](https://openreview.net/forum?id=oZDJKTlOUe) |  | 0 | Large vision-language models (LVLMs) have shown remarkable abilities in understanding visual information with human languages. However, LVLMs still suffer from object hallucination, which is the problem of generating descriptions that include objects that do not actually exist in the images. This... | Chelsea Finn, Chenhang Cui, Huaxiu Yao, Jaehong Yoon, Linjun Zhang, Mohit Bansal, Yiyang Zhou, Zhun Deng |  |
| 2298 |  |  [Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks](https://openreview.net/forum?id=MeB86edZ1P) |  | 0 | Neuromorphic computing with spiking neural networks is promising for energy-efficient artificial intelligence (AI) applications. However, different from humans who continually learn different tasks in a lifetime, neural network models suffer from catastrophic forgetting. How could neuronal... | Di He, Mingqing Xiao, Qingyan Meng, Zhouchen Lin, Zongpeng Zhang |  |
| 2299 |  |  [DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models](https://openreview.net/forum?id=Th6NyL07na) |  | 0 | Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on... | Hongyin Luo, James R. Glass, Pengcheng He, Yoon Kim, Yujia Xie, YungSung Chuang |  |
| 2300 |  |  [PanoDiffusion: 360-degree Panorama Outpainting via Diffusion](https://openreview.net/forum?id=ZNzDXDFZ0B) |  | 0 | Generating complete 360\textdegree{} panoramas from narrow field of view images is ongoing research as omnidirectional RGB data is not readily available. Existing GAN-based approaches face some barriers to achieving higher quality output, and have poor generalization performance over different mask... | Chuanxia Zheng, TatJen Cham, Tianhao Wu |  |
| 2301 |  |  [ViDA: Homeostatic Visual Domain Adapter for Continual Test Time Adaptation](https://openreview.net/forum?id=sJ88Wg5Bp5) |  | 0 | Since real-world machine systems are running in non-stationary environments, Continual Test-Time Adaptation (CTTA) task is proposed to adapt the pre-trained model to continually changing target domains. Recently, existing methods mainly focus on model-based adaptation, which aims to leverage a... | Jiaming Liu, Ming Lu, Peidong Jia, Renrui Zhang, Senqiao Yang, Shanghang Zhang, Wei Xue, Yandong Guo |  |
| 2302 |  |  [IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models](https://openreview.net/forum?id=Spp2i1hKwV) |  | 0 | In-context learning is a promising paradigm that utilizes in-context examples as prompts for the predictions of large language models. These prompts are crucial for achieving strong performance. However, since the prompts need to be sampled from a large volume of annotated examples, finding the... | Jiale Liu, LingHao Chen, Qingyun Wu, Shaokun Zhang, Tongliang Liu, Xiaobo Xia, Zhaoqing Wang |  |
| 2303 |  |  [Connect, Collapse, Corrupt: Learning Cross-Modal Tasks with Uni-Modal Data](https://openreview.net/forum?id=ttXg3SKAg5) |  | 0 | Building cross-modal applications is challenging due to limited paired multi-modal data. Recent works have shown that leveraging a pre-trained multi-modal contrastive representation space enables cross-modal tasks to be learned from uni-modal data. This is based on the assumption that contrastive... | Elaine Sui, Serena Yeung, Yuhui Zhang |  |
| 2304 |  |  [Unifying Feature and Cost Aggregation with Transformers for Semantic and Visual Correspondence](https://openreview.net/forum?id=fQHb1uZzl7) |  | 0 | This paper introduces a Transformer-based integrative feature and cost aggregation network designed for dense matching tasks. In the context of dense matching, many works benefit from one of two forms of aggregation: feature aggregation, which pertains to the alignment of similar features, or cost... | Seokju Cho, Seungryong Kim, Stephen Lin, Sunghwan Hong |  |
| 2305 |  |  [Data-independent Module-aware Pruning for Hierarchical Vision Transformers](https://openreview.net/forum?id=7Ol6foUi1G) |  | 0 | Hierarchical vision transformers (ViTs) have two advantages over conventional ViTs. First, hierarchical ViTs achieve linear computational complexity with respect to image size by local self-attention. Second, hierarchical ViTs create hierarchical feature maps by merging image patches in deeper... | Joey Tianyi Zhou, Yang He |  |
| 2306 |  |  [Scaling for Training Time and Post-hoc Out-of-distribution Detection Enhancement](https://openreview.net/forum?id=RDSTjtnqCg) |  | 0 | Activation shaping has proven highly effective for identifying out-of-distribution (OOD) samples post-hoc. Activation shaping prunes and scales network activations before estimating the OOD energy score; such an extremely simple approach achieves state-of-the-art OOD detection with minimal... | Angela Yao, Gianni Franchi, Kai Xu, Rongyu Chen |  |
| 2307 |  |  [A Simple and Effective Pruning Approach for Large Language Models](https://openreview.net/forum?id=PxoFut3dWW) |  | 0 | As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale... | Anna Bair, J. Zico Kolter, Mingjie Sun, Zhuang Liu |  |
| 2308 |  |  [GeoLLM: Extracting Geospatial Knowledge from Large Language Models](https://openreview.net/forum?id=TqL2xBwXP3) |  | 0 | The application of machine learning (ML) in a range of geospatial tasks is increasingly common but often relies on globally available covariates such as satellite imagery that can either be expensive or lack predictive power. Here we explore the question of whether the vast amounts of knowledge... | David B. Lobell, Gengchen Mai, Marshall Burke, Rohin Manvi, Samar Khanna, Stefano Ermon |  |
| 2309 |  |  [Instant3D: Fast Text-to-3D with Sparse-view Generation and Large Reconstruction Model](https://openreview.net/forum?id=2lDQLiH1W4) |  | 0 | Text-to-3D with diffusion models has achieved remarkable progress in recent years. However, existing methods either rely on score distillation-based optimization which suffer from slow inference, low diversity and Janus problems, or are feed-forward methods that generate low-quality results due to... | Fujun Luan, Greg Shakhnarovich, Hao Tan, Jiahao Li, Kai Zhang, Kalyan Sunkavalli, Sai Bi, Yicong Hong, Yinghao Xu, Zexiang Xu |  |
| 2310 |  |  [Effective and Efficient Federated Tree Learning on Hybrid Data](https://openreview.net/forum?id=py4ZV2qYQI) |  | 0 | Federated learning has emerged as a promising distributed learning paradigm that facilitates collaborative learning among multiple parties without transferring raw data. However, most existing federated learning studies focus on either horizontal or vertical data settings, where the data of... | Bingsheng He, Bo Li, Ce Zhang, Chulin Xie, Dawn Song, Qinbin Li, Xiaojun Xu, Xiaoyuan Liu |  |
| 2311 |  |  [Knowledge Distillation Based on Transformed Teacher Matching](https://openreview.net/forum?id=MJ3K7uDGGl) |  | 0 | As a technique to bridge logit matching and probability distribution matching, temperature scaling plays a pivotal role in knowledge distillation (KD). Conventionally, temperature scaling is applied to both teacher's logits and student's logits in KD. Motivated by some recent works, in this paper,... | EnHui Yang, Kaixiang Zheng |  |
| 2312 |  |  [Image Translation as Diffusion Visual Programmers](https://openreview.net/forum?id=yozwqhIHXj) |  | 0 | We introduce the novel Diffusion Visual Programmer (DVP), a neuro-symbolic image translation framework. Our proposed DVP seamlessly embeds a condition-flexible diffusion model within the GPT architecture, orchestrating a coherent sequence of visual programs ($i.e.$, computer vision models) for... | Cheng Han, Dongfang Liu, James Chenhao Liang, Majid Rabbani, Qifan Wang, Raghuveer Rao, Sohail A. Dianat, Ying Nian Wu |  |
| 2313 |  |  [Raidar: geneRative AI Detection viA Rewriting](https://openreview.net/forum?id=bQWE2UqXmf) |  | 0 | We find that large language models (LLMs) are more likely to modify human-written text than AI-generated text when tasked with rewriting. This tendency arises because LLMs often perceive AI-generated text as high-quality, leading to fewer modifications. We introduce a method to detect AI-generated... | Carl Vondrick, Chengzhi Mao, Hao Wang, Junfeng Yang |  |
| 2314 |  |  [DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning](https://openreview.net/forum?id=KjegfPGRde) |  | 0 | Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it... | Aldo Lipani, Zhengxiang Shi |  |
| 2315 |  |  [Multi-View Representation is What You Need for Point-Cloud Pre-Training](https://openreview.net/forum?id=imZcqOrbig) |  | 0 | A promising direction for pre-training 3D point clouds is to leverage the massive amount of data in 2D, whereas the domain gap between 2D and 3D creates a fundamental challenge. This paper proposes a novel approach to point-cloud pre-training that learns 3D representations by leveraging pre-trained... | Chen Song, Qixing Huang, Siming Yan, Youkang Kong |  |
| 2316 |  |  [VDT: General-purpose Video Diffusion Transformers via Mask Modeling](https://openreview.net/forum?id=Un0rgm9f04) |  | 0 | This work introduces Video Diffusion Transformer (VDT), which pioneers the use of transformers in diffusion-based video generation. It features transformer blocks with modularized temporal and spatial attention modules to leverage the rich spatial-temporal representation inherited in transformers.... | Guoxing Yang, Haoyu Lu, Mingyu Ding, Nanyi Fei, Ping Luo, Yuqi Huo, Zhiwu Lu |  |
| 2317 |  |  [InsertNeRF: Instilling Generalizability into NeRF with HyperNet Modules](https://openreview.net/forum?id=aHmNpLlUlb) |  | 0 | Generalizing Neural Radiance Fields (NeRF) to new scenes is a significant challenge that existing approaches struggle to address without extensive modifications to vanilla NeRF framework. We introduce \*\*InsertNeRF\*\*, a method for \*\*INS\*\*tilling g\*\*E\*\*ne\*\*R\*\*alizabili\*\*T\*\*y into... | Jing Huo, Tianyu Ding, Wenbin Li, Yang Gao, Yanqi Bao, Yuxin Li |  |
| 2318 |  |  [Augmenting Transformers with Recursively Composed Multi-grained Representations](https://openreview.net/forum?id=u859gX7ADC) |  | 0 | We present ReCAT, a recursive composition augmented Transformer that is able to explicitly model hierarchical syntactic structures of raw texts without relying on gold trees during both learning and inference. Existing research along this line restricts data to follow a hierarchical tree structure... | Kewei Tu, Qingyang Zhu, Wei Wu, Xiang Hu |  |
| 2319 |  |  [P2Seg: Pointly-supervised Segmentation via Mutual Distillation](https://openreview.net/forum?id=B4vzu2aokv) |  | 0 | Point-level Supervised Instance Segmentation (PSIS) aims to enhance the applicability and scalability of instance segmentation by utilizing low-cost yet instance-informative annotations. Existing PSIS methods usually rely on positional information to distinguish objects, but predicting precise... | Jianbin Jiao, Wenwen Yu, Xuehui Yu, Xumeng Han, Zhenjun Han, Zhixun Huang, Zipeng Wang |  |
| 2320 |  |  [TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting](https://openreview.net/forum?id=7oLshfEIC2) |  | 0 | Time series forecasting is widely used in extensive applications, such as traffic planning and weather forecasting. However, real-world time series usually present intricate temporal variations, making forecasting extremely challenging. Going beyond the mainstream paradigms of plain decomposition... | Haixu Wu, Huakun Luo, James Y. Zhang, Jun Zhou, Lintao Ma, Shiyu Wang, Tengge Hu, Xiaoming Shi |  |
| 2321 |  |  [Continuous-Multiple Image Outpainting in One-Step via Positional Query and A Diffusion-based Approach](https://openreview.net/forum?id=7hxoYxKDTV) |  | 0 | Image outpainting aims to generate the content of an input sub-image beyond its original boundaries. It is an important task in content generation yet remains an open problem for generative models. This paper pushes the technical frontier of image outpainting in two directions that have not been... | Fan Wang, Jiebo Luo, Jinfa Huang, Junchi Yan, Qiang Zhou, Shaofeng Zhang, Zhibin Wang |  |
| 2322 |  |  [When Semantic Segmentation Meets Frequency Aliasing](https://openreview.net/forum?id=SYBdkHcXXK) |  | 0 | Despite recent advancements in semantic segmentation, where and what pixels are hard to segment remains largely unexplored. Existing research only separates an image into easy and hard regions and empirically observes the latter are associated with object boundaries. In this paper, we conduct a... | Lin Gu, Linwei Chen, Ying Fu |  |
| 2323 |  |  [Efficient Sharpness-Aware Minimization for Molecular Graph Transformer Models](https://openreview.net/forum?id=Od39h4XQ3Y) |  | 0 | Sharpness-aware minimization (SAM) has received increasing attention in computer vision since it can effectively eliminate the sharp local minima from the training trajectory and mitigate generalization degradation. However, SAM requires two sequential gradient computations during the optimization... | Kaixiong Zhou, Ninghao Liu, Xin Wang, Yili Wang, Ying Wang |  |
| 2324 |  |  [MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo](https://openreview.net/forum?id=wXWfvSpYHh) |  | 0 | Recent advancements in learning-based Multi-View Stereo (MVS) methods have prominently featured transformer-based models with attention mechanisms. However, existing approaches have not thoroughly investigated the profound influence of transformers on different MVS modules, resulting in limited... | Chenjie Cao, Xinlin Ren, Yanwei Fu |  |
| 2325 |  |  [Spatio-Temporal Few-Shot Learning via Diffusive Neural Network Generation](https://openreview.net/forum?id=QyFm3D3Tzi) |  | 0 | Spatio-temporal modeling is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions. To bridge this gap, we propose a novel generative pre-training framework, GPD, for spatio-temporal few-shot learning with urban knowledge transfer. Unlike... | Chenyang Shao, Depeng Jin, Jingtao Ding, Yong Li, Yuan Yuan |  |
| 2326 |  |  [Theoretical Understanding of Learning from Adversarial Perturbations](https://openreview.net/forum?id=Ww9rWUAcdo) |  | 0 | It is not fully understood why adversarial examples can deceive neural networks and transfer between different networks. To elucidate this, several studies have hypothesized that adversarial perturbations, while appearing as noises, contain class features. This is supported by empirical evidence... | Hiroshi Kera, Soichiro Kumano, Toshihiko Yamasaki |  |
| 2327 |  |  [Graph Lottery Ticket Automated](https://openreview.net/forum?id=nmBjBZoySX) |  | 0 | Graph Neural Networks (GNNs) have emerged as the leading deep learning models for graph-based representation learning. However, the training and inference of GNNs on large graphs remain resource-intensive, impeding their utility in real-world scenarios and curtailing their applicability in deeper... | Aojun Zhou, Dawei Cheng, Guibin Zhang, Jin Zeng, Kun Wang, Roger Zimmermann, Wei Huang, Yang Wang, Yanwei Yue, Yuxuan Liang |  |
| 2328 |  |  [FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling](https://openreview.net/forum?id=ijoqFqSC7p) |  | 0 | With the availability of large-scale video datasets and the advances of diffusion models, text-driven video generation has achieved substantial progress. However, existing video generation models are typically trained on a limited number of frames, resulting in the inability to generate... | Haonan Qiu, Menghan Xia, Xintao Wang, Ying Shan, Yingqing He, Yong Zhang, Ziwei Liu |  |
| 2329 |  |  [Inner Classifier-Free Guidance and Its Taylor Expansion for Diffusion Models](https://openreview.net/forum?id=0QAzIMq32X) |  | 0 | Classifier-free guidance (CFG) is a pivotal technique for balancing the diversity and fidelity of samples in conditional diffusion models. This approach involves utilizing a single model to jointly optimize the conditional score predictor and unconditional score predictor, eliminating the need for... | Jia Jia, Junliang Xing, Longhui Wei, Qi Tian, Shikun Sun, Zhicai Wang, Zixuan Wang |  |
| 2330 |  |  [ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process](https://openreview.net/forum?id=cMPm8YFXZe) |  | 0 | Image recognition and generation have long been developed independently of each other. With the recent trend towards general-purpose representation learning, the development of general representations for both recognition and generation tasks is also promoted. However, preliminary attempts mainly... | Changyao Tian, Chenxin Tao, Gao Huang, Hao Li, Hongsheng Li, Jifeng Dai, Lewei Lu, Xiaogang Wang, Xizhou Zhu, Ziheng Li |  |
| 2331 |  |  [Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks](https://openreview.net/forum?id=9nsNyN0vox) |  | 0 | Language models' (LMs) proficiency in handling deterministic symbolic reasoning and rule-based tasks remains limited due to their dependency implicit learning on textual data. To endow LMs with genuine rule comprehension abilities, we propose "Neural Comprehension" - a framework that... | Bin Li, Fei Xia, Jun Zhao, Kang Liu, Minjun Zhu, Shizhu He, Yixuan Weng |  |
| 2332 |  |  [LMUFormer: Low Complexity Yet Powerful Spiking Model With Legendre Memory Units](https://openreview.net/forum?id=oEF7qExD9F) |  | 0 | Transformer models have demonstrated high accuracy in numerous applications but have high complexity and lack sequential processing capability making them ill-suited for many streaming applications at the edge where devices are heavily resource-constrained. Thus motivated, many researchers have... | Anni Li, Gourav Datta, Peter Anthony Beerel, Zeyu Liu |  |
| 2333 |  |  [InterpGNN: Understand and Improve Generalization Ability of Transdutive GNNs through the Lens of Interplay between Train and Test Nodes](https://openreview.net/forum?id=pwW807WJ9G) |  | 0 | Transductive node prediction has been a popular learning setting in Graph Neural Networks (GNNs). It has been widely observed that the shortage of information flow between the distant nodes and intra-batch nodes (for large-scale graphs) often hurt the generalization of GNNs which overwhelmingly... | Chentao Wu, Jiawei Sun, Jie Li, Junchi Yan, Kailai Li, Ruoxin Chen, Yue Ding |  |
| 2334 |  |  [STanHop: Sparse Tandem Hopfield Model for Memory-Enhanced Time Series Prediction](https://openreview.net/forum?id=6iwg437CZs) |  | 0 | We present \*\*STanHop-Net\*\* (\*\*S\*\*parse \*\*Tan\*\*dem \*\*Hop\*\*field \*\*Net\*\*work) for multivariate time series prediction with memory-enhanced capabilities. At the heart of our approach is \*\*STanHop\*\*, a novel Hopfield-based neural network block, which sparsely learns and stores... | BoYu Chen, Dennis Wu, Han Liu, Jerry YaoChieh Hu, Weijian Li |  |
| 2335 |  |  [Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images](https://openreview.net/forum?id=BteuUysuXX) |  | 0 | Large vision-language models (VLMs) such as GPT-4 have achieved exceptional performance across various multi-modal tasks. However, the deployment of VLMs necessitates substantial energy consumption and computational resources. Once attackers maliciously induce high energy consumption and latency... | Jindong Gu, Kuofeng Gao, Philip Torr, ShuTao Xia, Wei Liu, Yang Bai, Zhifeng Li |  |
| 2336 |  |  [Progressive Fourier Neural Representation for Sequential Video Compilation](https://openreview.net/forum?id=rGFrRMBbOq) |  | 0 | Neural Implicit Representation (NIR) has recently gained significant attention due to its remarkable ability to encode complex and high-dimensional data into representation space and easily reconstruct it through a trainable mapping function. However, NIR methods assume a one-to-one mapping between... | Chang D. Yoo, Dahyun Kim, Haeyong Kang, Jaehong Yoon, Sung Ju Hwang |  |
| 2337 |  |  [Adaptive deep spiking neural network with global-local learning via balanced excitatory and inhibitory mechanism](https://openreview.net/forum?id=wpnlc2ONu0) |  | 0 | The training method of Spiking Neural Networks (SNNs) is an essential problem, and how to integrate local and global learning is a worthy research interest. However, the current integration methods do not consider the network conditions suitable for local and global learning, and thus fail to... | Gang Pan, Jiangrong Shen, Pan Lv, Qi Xu, Qiang Zhang, Tingting Jiang, Xuming Ran |  |
| 2338 |  |  [Diffusion Posterior Sampling for Linear Inverse Problem Solving: A Filtering Perspective](https://openreview.net/forum?id=tplXNcHZs1) |  | 0 | Diffusion models have achieved tremendous success in generating high-dimensional data like images, videos and audio. These models provide powerful data priors that can solve linear inverse problems in zero shot through Bayesian posterior sampling. However, exact posterior sampling for diffusion... | Yang Song, Zehao Dou |  |
| 2339 |  |  [How connectivity structure shapes rich and lazy learning in neural circuits](https://openreview.net/forum?id=slSmYGc8ee) |  | 0 | In theoretical neuroscience, recent work leverages deep learning tools to explore how some network attributes critically influence its learning dynamics. Notably, initial weight distributions with small (resp. large) variance may yield a rich (resp. lazy) regime, where significant (resp. minor)... | Aristide Baratin, Eric SheaBrown, Guillaume Lajoie, Jonathan Cornford, Stefan Mihalas, Yuhan Helena Liu |  |
| 2340 |  |  [An LLM can Fool Itself: A Prompt-Based Adversarial Attack](https://openreview.net/forum?id=VVgGbB9TNV) |  | 0 | The wide-ranging applications of large language models (LLMs), especially in safety-critical domains, necessitate the proper evaluation of the LLM’s adversarial robustness. This paper proposes an efficient tool to audit the LLM’s adversarial robustness via a prompt-based adversarial attack... | Di Wang, Jingfeng Zhang, Keyi Kong, Lizhen Cui, Mohan S. Kankanhalli, Ning Liu, Xilie Xu |  |
| 2341 |  |  [AutoLoRa: An Automated Robust Fine-Tuning Framework](https://openreview.net/forum?id=09xFexjhqE) |  | 0 | Robust Fine-Tuning (RFT) is a low-cost strategy to obtain adversarial robustness in downstream applications, without requiring a lot of computational resources and collecting significant amounts of data. This paper uncovers an issue with the existing RFT, where optimizing both adversarial and... | Jingfeng Zhang, Mohan S. Kankanhalli, Xilie Xu |  |
| 2342 |  |  [Denoising Diffusion Step-aware Models](https://openreview.net/forum?id=c43FGk8Pcg) |  | 0 | Denoising Diffusion Probabilistic Models (DDPMs) have garnered popularity for data generation across various domains. However, a significant bottleneck is the necessity for whole-network computation during every step of the generative process, leading to high computational overheads. This paper... | Luozhou Wang, Shu Liu, Shuai Yang, YingCong Chen, Yukang Chen |  |
| 2343 |  |  [Get more for less: Principled Data Selection for Warming Up Fine-Tuning in LLMs](https://openreview.net/forum?id=QmYNBVukex) |  | 0 | This work focuses on leveraging and selecting from vast, unlabeled, open data to \*pre-fine-tune\* a pre-trained language model. The goal is to minimize the need for costly domain-specific data for subsequent fine-tuning while achieving desired performance levels. While many data selection... | Anit Kumar Sahu, Feiyang Kang, Himanshu Jahagirdar, Hoang Anh Just, Rongxing Du, Ruoxi Jia, Yifan Sun, Yuanzhi Zhang |  |
| 2344 |  |  [3D Reconstruction with Generalizable Neural Fields using Scene Priors](https://openreview.net/forum?id=Nu7dDaVF5a) |  | 0 | High-fidelity 3D scene reconstruction has been substantially advanced by recent progress in neural fields. However, most existing methods train a separate network from scratch for each individual scene. This is not scalable, inefficient, and unable to yield good results given limited views. While... | Amey Kulkarni, Jan Kautz, Shalini De Mello, Sifei Liu, Xiaolong Wang, Xueting Li, Yang Fu |  |
| 2345 |  |  [Causal Structure Recovery with Latent Variables under Milder Distributional and Graphical Assumptions](https://openreview.net/forum?id=MukGKGtgnr) |  | 0 | Traditional causal discovery approaches typically assume the absence of latent variables, a simplification that often does not align with real-world situations. Recently, there has been a surge of causal discovery methods that explicitly consider latent variables. While some works aim to reveal... | Kun Zhang, Tongliang Liu, XiuChuan Li |  |
| 2346 |  |  [AutoVP: An Automated Visual Prompting Framework and Benchmark](https://openreview.net/forum?id=wR9qVlPh0P) |  | 0 | Visual prompting (VP) is an emerging parameter-efficient fine-tuning approach to adapting pre-trained vision models to solve various downstream image-classification tasks. However, there has hitherto been little systematic study of the design space of VP and no clear benchmark for evaluating its... | HsiAi Tsao, Lei Hsiung, PinYu Chen, Si Liu, TsungYi Ho |  |
| 2347 |  |  [Structured Video-Language Modeling with Temporal Grouping and Spatial Grounding](https://openreview.net/forum?id=5dlfiJIXoh) |  | 0 | Existing video-language pre-training methods primarily focus on instance-level alignment between video clips and captions via global contrastive learning but neglect rich fine-grained local information in both videos and text, which is of importance to downstream tasks requiring temporal... | Boqing Gong, ChoJui Hsieh, Florian Schroff, Liangzhe Yuan, Long Zhao, MingHsuan Yang, Ting Liu, Yuanhao Xiong |  |
| 2348 |  |  [WizardCoder: Empowering Code Large Language Models with Evol-Instruct](https://openreview.net/forum?id=UnUwSIgK5W) |  | 0 | Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated remarkable performance in various code-related tasks. However, different from their counterparts in the general language modeling field, the technique of instruction fine-tuning remains relatively under-researched in this... | Can Xu, Chongyang Tao, Daxin Jiang, Jing Ma, Pu Zhao, Qingfeng Sun, Qingwei Lin, Wenxiang Hu, Xiubo Geng, Ziyang Luo |  |
| 2349 |  |  [Order-Preserving GFlowNets](https://openreview.net/forum?id=VXDPXuq4oG) |  | 0 | Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates with probabilities proportional to a given reward. However, GFlowNets can only be used with a predefined scalar reward, which can be either computationally expensive or not directly... | Lukas Mauch, Yihang Chen |  |
| 2350 |  |  [VQGraph: Rethinking Graph Representation Space for Bridging GNNs and MLPs](https://openreview.net/forum?id=h6Tz85BqRI) |  | 0 | GNN-to-MLP distillation aims to utilize knowledge distillation (KD) to learn computationally-efficient multi-layer perceptron (student MLP) on graph data by mimicking the output representations of teacher GNN. Existing methods mainly make the MLP to mimic the GNN predictions over a few class... | Bin Cui, Jure Leskovec, Ling Yang, Minkai Xu, Muhan Zhang, Shenda Hong, Wei Qu, Wentao Zhang, Ye Tian, Zhongyi Liu |  |
| 2351 |  |  [Dual-Encoders for Extreme Multi-label Classification](https://openreview.net/forum?id=dNe1T0Ahby) |  | 0 | Dual-encoder (DE) models are widely used in retrieval tasks, most commonly studied on open QA benchmarks that are often characterized by multi-class and limited training data. In contrast, their performance in multi-label and data-rich retrieval settings like extreme multi-label classification... | Ankit Singh Rawat, Devvrit, Inderjit S. Dhillon, Nilesh Gupta, Prateek Jain, Srinadh Bhojanapalli |  |
| 2352 |  |  [FasterViT: Fast Vision Transformers with Hierarchical Attention](https://openreview.net/forum?id=kB4yBiNmXX) |  | 0 | We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced... | Ali Hatamizadeh, Andrew Tao, Greg Heinrich, Hongxu Yin, Jan Kautz, José M. Álvarez, Pavlo Molchanov |  |
| 2353 |  |  [AutoCast++: Enhancing World Event Prediction with Zero-shot Ranking-based Context Retrieval](https://openreview.net/forum?id=COYDmKkQH4) |  | 0 | Machine-based prediction of real-world events is garnering attention due to its potential for informed decision-making. Whereas traditional forecasting predominantly hinges on structured data like time-series, recent breakthroughs in language models enable predictions using unstructured text. In... | Jiawei He, Lili Meng, Qi Yan, Raihan Seraj, Tristan Sylvain |  |
| 2354 |  |  [Feature Collapse](https://openreview.net/forum?id=gctmyMiPHH) |  | 0 | We formalize and study a phenomenon called \*feature collapse\* that makes precise the intuitive idea that entities playing a similar role in a learning task receive similar representations. As feature collapse requires a notion of task, we leverage a synthetic task in which a learner must classify... | James von Brecht, Thomas Laurent, Xavier Bresson |  |
| 2355 |  |  [Function Vectors in Large Language Models](https://openreview.net/forum?id=AwyxtyMwaG) |  | 0 | We report the presence of a simple neural mechanism that represents an input-output function as a vector within autoregressive transformer language models (LMs). Using causal mediation analysis on a diverse range of in-context-learning (ICL) tasks, we find that a small number attention heads... | Aaron Mueller, Arnab Sen Sharma, Byron C. Wallace, David Bau, Eric Todd, Millicent L. Li |  |
| 2356 |  |  [Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking](https://openreview.net/forum?id=8sKcAWOf2D) |  | 0 | Fine-tuning on generalized tasks such as instruction following, code generation, and mathematics has been shown to enhance language models' performance on a range of tasks. Nevertheless, explanations of how such fine-tuning influences the internal computations in these models remain elusive. We... | David Bau, Nikhil Prakash, Tal Haklay, Tamar Rott Shaham, Yonatan Belinkov |  |
| 2357 |  |  [Perceptual Group Tokenizer: Building Perception with Iterative Grouping](https://openreview.net/forum?id=NnYaYVODyV) |  | 0 | Human visual recognition system shows astonishing capability of compressing visual information into a set of tokens containing rich representations without label supervision. One critical driving principle behind it is perceptual grouping. Despite being widely used in computer vision in the early... | Ting Chen, Yang Li, Zhiwei Deng |  |
| 2358 |  |  [ImageNet-OOD: Deciphering Modern Out-of-Distribution Detection Algorithms](https://openreview.net/forum?id=VTYg5ykEGS) |  | 0 | The task of out-of-distribution (OOD) detection is notoriously ill-defined. Earlier works focused on new-class detection, aiming to identify label-altering data distribution shifts, also known as "semantic shift." However, recent works argue for a focus on failure detection, expanding the OOD... | Byron Zhang, Olga Russakovsky, William Yang |  |
| 2359 |  |  [Self-supervised Representation Learning from Random Data Projectors](https://openreview.net/forum?id=EpYnZpDpsQ) |  | 0 | Self-supervised representation learning (SSRL) has advanced considerably by exploiting the transformation invariance assumption under artificially designed data augmentations. While augmentation-based SSRL algorithms push the boundaries of performance in computer vision and natural language... | Ga Wu, George Stein, Jesse C. Cresswell, Maksims Volkovs, Tongzi Wu, Xiao Shi Huang, Xiaochen Zhang, Yi Sui |  |
| 2360 |  |  [Approximately Piecewise E(3) Equivariant Point Networks](https://openreview.net/forum?id=aKJEHWmBEf) |  | 0 | Integrating a notion of symmetry into point cloud neural networks is a provably effective way to improve their generalization capability. Of particular interest are $E(3)$ equivariant point cloud networks where Euclidean transformations applied to the inputs are preserved in the outputs. Recent... | Francis Williams, Jiahui Huang, Matan Atzmon, Or Litany |  |
| 2361 |  |  [DAM: Towards a Foundation Model for Forecasting](https://openreview.net/forum?id=4NhMhElWqP) |  | 0 | It is challenging to scale time series forecasting models such that they forecast accurately for multiple distinct domains and datasets, all with potentially different underlying collection procedures (e.g., sample resolution), patterns (e.g., periodicity), and prediction requirements (e.g.,... | Adam Barker, Ahmed Hassan, Amos J. Storkey, Artjom Joosen, Luke Nicholas Darlow, Martin Asenov, Qiwen Deng, Rajkarn Singh |  |
| 2362 |  |  [Weakly-supervised Audio Separation via Bi-modal Semantic Similarity](https://openreview.net/forum?id=4N97bz1sP6) |  | 0 | Conditional sound separation in multi-source audio mixtures without having access to single source sound data during training is a long standing challenge. Existing mix-and-separate based methods suffer from significant performance drop with multi-source training mixtures due to the lack of... | Diana Marculescu, Kazuhito Koishida, Saeed Amizadeh, Tanvir Mahmud |  |
| 2363 |  |  [Expected flow networks in stochastic environments and two-player zero-sum games](https://openreview.net/forum?id=uH0FGECSEI) |  | 0 | Generative flow networks (GFlowNets) are sequential sampling models trained to match a given distribution. GFlowNets have been successfully applied to various structured object generation tasks, sampling a diverse set of high-reward objects quickly. We propose expected flow networks (EFlowNets),... | Bilun Sun, Danilo Vucetic, Gauthier Gidel, Marco Jiralerspong, Nikolay Malkin, Tianyu Zhang, Yoshua Bengio |  |
| 2364 |  |  [Neural Polynomial Gabor Fields for Macro Motion Analysis](https://openreview.net/forum?id=dTlKCQuuxP) |  | 0 | We study macro motion analysis, where macro motion refers to the collection of all visually observable motions in a dynamic scene. Traditional filtering-based methods on motion analysis typically focus only on local and tiny motions, yet fail to represent large motions or 3D scenes. Recent dynamic... | Chen Geng, HongXing Yu, Jiajun Wu, Sida Peng, Xiaowei Zhou |  |
| 2365 |  |  [Denoising Diffusion via Image-Based Rendering](https://openreview.net/forum?id=1JbsdayvhO) |  | 0 | Generating 3D scenes is a challenging open problem, which requires synthesizing plausible content that is fully consistent in 3D space. While recent methods such as neural radiance fields excel at view synthesis and 3D reconstruction, they cannot synthesize plausible details in unobserved regions... | Fabian Manhardt, Federico Tombari, Paul Henderson, Titas Anciukevicius |  |
| 2366 |  |  [LEAP: Liberate Sparse-View 3D Modeling from Camera Poses](https://openreview.net/forum?id=KPmajBxEaF) |  | 0 | Are camera poses necessary for multi-view 3D modeling? Existing approaches predominantly assume access to accurate camera poses. While this assumption might hold for dense views, accurately estimating camera poses for sparse views is often elusive. Our analysis reveals that noisy estimated poses... | Hanwen Jiang, Qixing Huang, Yue Zhao, Zhenyu Jiang |  |
| 2367 |  |  [Language Modeling Is Compression](https://openreview.net/forum?id=jznbgiynus) |  | 0 | It has long been established that predictive models can be transformed into lossless compressors and vice versa. Incidentally, in recent years, the machine learning community has focused on training increasingly large and powerful self-supervised (language) models. Since these large language models... | Anian Ruoss, Christopher Mattern, Elliot Catt, Grégoire Delétang, Joel Veness, Jordi GrauMoya, Laurent Orseau, Li Kevin Wenliang, Marcus Hutter, Matthew Aitchison, PaulAmbroise Duquenne, Tim Genewein |  |
| 2368 |  |  [OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views](https://openreview.net/forum?id=SgjAojPKb3) |  | 0 | Large visual-language models (VLMs), like CLIP, enable open-set image segmentation to segment arbitrary concepts from an image in a zero-shot manner. This goes beyond the traditional closed-set assumption, i.e., where models can only segment classes from a pre-defined training set. More recently,... | Fabian Manhardt, Federico Tombari, Francis Engelmann, Keisuke Tateno, Michael Niemeyer |  |
| 2369 |  |  [Learning with a Mole: Transferable latent spatial representations for navigation without reconstruction](https://openreview.net/forum?id=8HCARN2hhw) |  | 0 | Agents navigating in 3D environments require some form of memory, which should hold a compact and actionable representation of the history of observations useful for decision taking and planning. In most end-to-end learning approaches the representation is latent and usually does not have a clearly... | Assem Sadek, Christian Wolf, Gianluca Monaci, Guillaume Bono, Leonid Antsfeld |  |
| 2370 |  |  [Reverse Forward Curriculum Learning for Extreme Sample and Demo Efficiency](https://openreview.net/forum?id=w4rODxXsmM) |  | 0 | Reinforcement learning (RL) presents a promising framework to learn policies through environment interaction, but often requires an infeasible amount of interaction data to solve complex tasks from sparse rewards. One direction includes augmenting RL with offline data demonstrating desired tasks,... | Arth Shukla, Hao Su, Stone Tao, Tsekai Chan |  |
| 2371 |  |  [BatchPrompt: Accomplish more with less](https://openreview.net/forum?id=Agyicd577r) |  | 0 | The ever-increasing token limits of large language models (LLMs) have enabled long context as input. Many LLMs are trained and fine-tuned to perform zero/few-shot inference using instruction-based prompts. Prompts typically include a detailed task instruction, several examples, and a single data... | Jianzhe Lin, Liang Du, Maurice Diesendruck, Robin Abraham |  |
| 2372 |  |  [Large Language Models as Optimizers](https://openreview.net/forum?id=Bb4VGOWELI) |  | 0 | Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large... | Chengrun Yang, Denny Zhou, Hanxiao Liu, Quoc V. Le, Xinyun Chen, Xuezhi Wang, Yifeng Lu |  |
| 2373 |  |  [ContextRef: Evaluating Referenceless Metrics for Image Description Generation](https://openreview.net/forum?id=j0ZvKSNZiP) |  | 0 | Referenceless metrics (e.g., CLIPScore) use pretrained vision--language models to assess image descriptions directly without costly ground-truth reference texts. Such methods can facilitate rapid progress, but only if they truly align with human preference judgments. In this paper, we introduce... | Christopher Potts, Elisa Kreiss, Eric Zelikman, Nick Haber |  |
| 2374 |  |  [Tractable Probabilistic Graph Representation Learning with Graph-Induced Sum-Product Networks](https://openreview.net/forum?id=h7nOCxFsPg) |  | 0 | We introduce Graph-Induced Sum-Product Networks (GSPNs), a new probabilistic framework for graph representation learning that can tractably answer probabilistic queries. Inspired by the computational trees induced by vertices in the context of message-passing neural networks, we build hierarchies... | Federico Errica, Mathias Niepert |  |
| 2375 |  |  [HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion](https://openreview.net/forum?id=duyA42HlCK) |  | 0 | Despite significant advances in large-scale text-to-image models, achieving hyper-realistic human image generation remains a desirable yet unsolved task. Existing models like Stable Diffusion and DALL·E 2 tend to generate human images with incoherent parts or unnatural poses. To tackle these... | Aliaksandr Siarohin, Dahua Lin, Ivan Skorokhodov, Jian Ren, Sergey Tulyakov, Xian Liu, Xihui Liu, Yanyu Li, Ziwei Liu |  |
| 2376 |  |  [ZeroFlow: Scalable Scene Flow via Distillation](https://openreview.net/forum?id=FRCHDhbxZF) |  | 0 | Scene flow estimation is the task of describing the 3D motion field between temporally successive point clouds. State-of-the-art methods use strong priors and test-time optimization techniques, but require on the order of tens of seconds to process full-size point clouds, making them unusable as... | Deva Ramanan, Dinesh Jayaraman, Eric Eaton, Ishan Khatri, James Hays, Kyle Vedder, Nathaniel Chodosh, Neehar Peri, Yang Liu |  |
| 2377 |  |  [R&B: Region and Boundary Aware Zero-shot Grounded Text-to-image Generation](https://openreview.net/forum?id=8Q4uVOJ5bX) |  | 0 | Recent text-to-image (T2I) diffusion models have achieved remarkable progress in generating high-quality images given text-prompts as input. However, these models fail to convey appropriate spatial composition specified by a layout instruction. In this work, we probe into zero-shot grounded T2I... | Henglei Lv, Jiayu Xiao, Liang Li, Qingming Huang, Shuhui Wang |  |
| 2378 |  |  [Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Probabilistic Interpretations](https://openreview.net/forum?id=I1quoTXZzc) |  | 0 | Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models. They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. However,... | Hao Wang, Lu Mi, Xiaomeng Li, Xinyue Xu, Yi Qin |  |
| 2379 |  |  [SpeechTokenizer: Unified Speech Tokenizer for Speech Language Models](https://openreview.net/forum?id=AF9Q8Vip84) |  | 0 | Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building... | Dong Zhang, Shimin Li, Xin Zhang, Xipeng Qiu, Yaqian Zhou |  |
| 2380 |  |  [Enhancing High-Resolution 3D Generation through Pixel-wise Gradient Clipping](https://openreview.net/forum?id=ukidfml68f) |  | 0 | High-resolution 3D object generation remains a challenging task primarily due to the limited availability of comprehensive annotated training data. Recent advancements have aimed to overcome this constraint by harnessing image generative models, pretrained on extensive curated web datasets, using... | Jiachen Lu, Li Zhang, Xiatian Zhu, Zijie Pan |  |
| 2381 |  |  [Transferring Labels to Solve Annotation Mismatches Across Object Detection Datasets](https://openreview.net/forum?id=ChHx5ORqF0) |  | 0 | In object detection, varying annotation protocols across datasets can result in annotation mismatches, leading to inconsistent class labels and bounding regions. Addressing these mismatches typically involves manually identifying common trends and fixing the corresponding bounding boxes and class... | David Acuna, James Lucas, Rafid Mahmood, Sanja Fidler, Viraj Prabhu, YuanHong Liao |  |
| 2382 |  |  [Weatherproofing Retrieval for Localization with Generative AI and Geometric Consistency](https://openreview.net/forum?id=5EniAcsO7f) |  | 0 | State-of-the-art visual localization approaches generally rely on a first image retrieval step whose role is crucial. Yet, retrieval often struggles when facing varying conditions, due to e.g. weather or time of day, with dramatic consequences on the visual localization accuracy. In this paper, we... | Diane Larlus, Gabriela Csurka, Mert Bülent Sariyildiz, Philippe Weinzaepfel, Rafael S. Rezende, Yannis Kalantidis |  |
| 2383 |  |  [DreamClean: Restoring Clean Image Using Deep Diffusion Prior](https://openreview.net/forum?id=6ALuy19mPa) |  | 0 | Image restoration poses a garners substantial interest due to the exponential surge in demands for recovering high-quality images from diverse mobile camera devices, adverse lighting conditions, suboptimal shooting environments, and frequent image compression for efficient transmission purposes.... | Han Zhang, Jie Xiao, Kai Zhu, Ruili Feng, Xueyang Fu, Yu Liu, Yurui Zhu, Zhantao Yang, ZhengJun Zha, Zhiheng Liu |  |
| 2384 |  |  [CausalLM is not optimal for in-context learning](https://openreview.net/forum?id=guRNebwZBb) |  | 0 | Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits... | Jialin Wu, Nan Ding, Radu Soricut, Sebastian Goodman, Tomer Levinboim |  |
| 2385 |  |  [End-to-End (Instance)-Image Goal Navigation through Correspondence as an Emergent Phenomenon](https://openreview.net/forum?id=cphhnHjCvC) |  | 0 | Most recent work in goal oriented visual navigation resorts to large-scale machine learning in simulated environments. The main challenge lies in learning compact representations generalizable to unseen environments and in learning high-capacity perception modules capable of reasoning on... | Boris Chidlovskii, Christian Wolf, Guillaume Bono, Leonid Antsfeld, Philippe Weinzaepfel |  |
| 2386 |  |  [Strategic Preys Make Acute Predators: Enhancing Camouflaged Object Detectors by Generating Camouflaged Objects](https://openreview.net/forum?id=hywpSoHwgX) |  | 0 | Camouflaged object detection (COD) is the challenging task of identifying camouflaged objects visually blended into surroundings. Albeit achieving remarkable success, existing COD detectors still struggle to obtain precise results in some challenging cases. To handle this problem, we draw... | Chenyu You, Chunming He, Fisher Yu, Kai Li, Martin Danelljan, Xiu Li, Yachao Zhang, Yulun Zhang, Zhenhua Guo |  |
| 2387 |  |  [Consistency-guided Prompt Learning for Vision-Language Models](https://openreview.net/forum?id=wsRXwlwx4w) |  | 0 | We propose Consistency-guided Prompt learning (CoPrompt), a new fine-tuning method for vision-language models. Our approach improves the generalization of large foundation models when fine-tuned on downstream tasks in a few-shot setting. The basic idea of CoPrompt is to enforce a consistency... | Ali Etemad, Shuvendu Roy |  |
| 2388 |  |  [Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting](https://openreview.net/forum?id=WhgB5sispV) |  | 0 | Reconstructing dynamic 3D scenes from 2D images and generating diverse views over time is challenging due to scene complexity and temporal dynamics. Despite advancements in neural implicit models, limitations persist: (i) Inadequate Scene Structure: Existing methods struggle to reveal the spatial... | Hongye Yang, Li Zhang, Zeyu Yang, Zijie Pan |  |
| 2389 |  |  [Language-Informed Visual Concept Learning](https://openreview.net/forum?id=juuyW8B8ig) |  | 0 | Our understanding of the visual world is centered around various concept axes, characterizing different aspects of visual entities. While different concept axes can be easily specified by language, e.g., color, the exact visual nuances along each axis often exceed the limitations of linguistic... | Jiajun Wu, Shangzhe Wu, Sharon Lee, Yunzhi Zhang |  |
| 2390 |  |  [Online Continual Learning for Interactive Instruction Following Agents](https://openreview.net/forum?id=7M0EzjugaN) |  | 0 | In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning. We argue that such a learning scenario is less realistic, since a robotic agent is supposed to learn the world continuously as it... | Byeonghwi Kim, Jonghyun Choi, Minhyuk Seo |  |
| 2391 |  |  [Localizing and Editing Knowledge In Text-to-Image Generative Models](https://openreview.net/forum?id=Qmw9ne6SOQ) |  | 0 | Text-to-Image Diffusion Models such as Stable-Diffusion and Imagen have achieved unprecedented quality of photorealism with state-of-the-art FID scores on MS-COCO and other generation benchmarks. Given a caption, image generation requires fine-grained knowledge about attributes such as object... | Nanxuan Zhao, Samyadeep Basu, Soheil Feizi, Varun Manjunatha, Vlad I. Morariu |  |
| 2392 |  |  [Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization](https://openreview.net/forum?id=7NzgkEdGyr) |  | 0 | Large foundation models are becoming ubiquitous, but training them from scratch is prohibitively expensive. Thus, efficiently adapting these powerful models to downstream tasks is increasingly important. In this paper, we study a principled finetuning paradigm -- Orthogonal Finetuning (OFT) -- for... | Adrian Weller, Bernhard Schölkopf, Haiwen Feng, Juyeon Heo, Longhui Yu, Michael J. Black, Songyou Peng, Weiyang Liu, Yandong Wen, Yao Feng, Yuliang Xiu, Yuxuan Xue, Zeju Qiu, Zhen Liu |  |
| 2393 |  |  [Towards domain-invariant Self-Supervised Learning with Batch Styles Standardization](https://openreview.net/forum?id=qtE9K23ISq) |  | 0 | In Self-Supervised Learning (SSL), models are typically pretrained, fine-tuned, and evaluated on the same domains. However, they tend to perform poorly when evaluated on unseen domains, a challenge that Unsupervised Domain Generalization (UDG) seeks to address. Current UDG methods rely on domain... | Florent CouzinieDevy, Maria Vakalopoulou, Marin Scalbert |  |
| 2394 |  |  [LaneSegNet: Map Learning with Lane Segment Perception for Autonomous Driving](https://openreview.net/forum?id=LsURkIPYR5) |  | 0 | A map, as crucial information for downstream applications of an autonomous driving system, is usually represented in lanelines or centerlines. However, existing literature on map learning primarily focuses on either detecting geometry-based lanelines or perceiving topology relationships of... | Bangjun Wang, Hongyang Li, Junchi Yan, Kun Jiang, Li Chen, Peijin Jia, Tianyu Li |  |
| 2395 |  |  [Spike-driven Transformer V2: Meta Spiking Neural Network Architecture Inspiring the Design of Next-generation Neuromorphic Chips](https://openreview.net/forum?id=1SIBN5Xyw7) |  | 0 | Neuromorphic computing, which exploits Spiking Neural Networks (SNNs) on neuromorphic chips, is a promising energy-efficient alternative to traditional AI. CNN-based SNNs are the current mainstream of neuromorphic computing. By contrast, no neuromorphic chips are designed especially for... | Bo Xu, Guoqi Li, Jiakui Hu, Man Yao, Tianxiang Hu, Yifan Xu, Yonghong Tian, Zhaokun Zhou |  |
| 2396 |  |  [BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity](https://openreview.net/forum?id=mQYHXUUTkU) |  | 0 | Understanding the functional organization of higher visual cortex is a central focus in neuroscience. Past studies have primarily mapped the visual and semantic selectivity of neural populations using hand-selected stimuli, which may potentially bias results towards pre-existing hypotheses of... | Andrew F. Luo, Leila Wehbe, Margaret M. Henderson, Michael J. Tarr |  |
| 2397 |  |  [GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion](https://openreview.net/forum?id=FvK2noilxT) |  | 0 | In this work, we tackle the challenging problem of denoising hand-object interactions (HOI). Given an erroneous interaction sequence, the objective is to refine the incorrect hand trajectory to remove interaction artifacts for a perceptually realistic sequence. This challenge involves intricate... | Li Yi, Xueyi Liu |  |
| 2398 |  |  [Get What You Want, Not What You Don't: Image Content Suppression for Text-to-Image Diffusion Models](https://openreview.net/forum?id=zpVPhvVKXk) |  | 0 | The success of recent text-to-image diffusion models is largely due to their capacity to be guided by a complex text prompt, which enables users to precisely describe the desired content. However, these models struggle to effectively suppress the generation of undesired content, which is explicitly... | Fahad Shahbaz Khan, Jian Yang, Joost van de Weijer, Qibin Hou, Senmao Li, Taihang Hu, Yaxing Wang |  |
| 2399 |  |  [Less or More From Teacher: Exploiting Trilateral Geometry For Knowledge Distillation](https://openreview.net/forum?id=OZitfSXpdT) |  | 0 | Knowledge distillation aims to train a compact student network using soft supervision from a larger teacher network and hard supervision from ground truths. However, determining an optimal knowledge fusion ratio that balances these supervisory signals remains challenging. Prior methods generally... | Boyu Wang, Chen Ma, Chengming Hu, Haolun Wu, Jun Yan, Xi Chen, Xuan Li, Xue Liu |  |
| 2400 |  |  [Unraveling the Enigma of Double Descent: An In-depth Analysis through the Lens of Learned Feature Space](https://openreview.net/forum?id=CEkIyshNbC) |  | 0 | Double descent presents a counter-intuitive aspect within the machine learning domain, and researchers have observed its manifestation in various models and tasks. While some theoretical explanations have been proposed for this phenomenon in specific contexts, an accepted theory for its occurring... | Tomaso Aste, Xiaoqing Zheng, Yufei Gu |  |
| 2401 |  |  [Meta-Evolve: Continuous Robot Evolution for One-to-many Policy Transfer](https://openreview.net/forum?id=RthOl4jHw5) |  | 0 | We investigate the problem of transferring an expert policy from a source robot to multiple different robots. To solve this problem, we propose a method named \*Meta-Evolve\* that uses continuous robot evolution to efficiently transfer the policy to each target robot through a set of... | Deepak Pathak, Ding Zhao, Xingyu Liu |  |
| 2402 |  |  [DFormer: Rethinking RGBD Representation Learning for Semantic Segmentation](https://openreview.net/forum?id=h1sFUGlI09) |  | 0 | We present DFormer, a novel RGB-D pretraining framework to learn transferable representations for RGB-D segmentation tasks. DFormer has two new key innovations: 1) Unlike previous works that encode RGB-D information with RGB pretrained backbone, we pretrain the backbone using image-depth pairs from... | Bowen Yin, Li Liu, MingMing Cheng, Qibin Hou, Xuying Zhang, ZhongYu Li |  |
| 2403 |  |  [ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving](https://openreview.net/forum?id=Ep0TtjVoap) |  | 0 | Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural... | Minlie Huang, Nan Duan, Weizhu Chen, Yelong Shen, Yeyun Gong, Yujiu Yang, Zhibin Gou, Zhihong Shao |  |
| 2404 |  |  [Bayesian Bi-clustering of Neural Spiking Activity with Latent Structures](https://openreview.net/forum?id=ZYm1Ql6udy) |  | 0 | Modern neural recording techniques allow neuroscientists to obtain spiking activity of multiple neurons from different brain regions over long time periods, which requires new statistical methods to be developed for understanding structure of the large-scale data. In this paper, we develop a... | Ganchao Wei |  |
| 2405 |  |  [GRANDE: Gradient-Based Decision Tree Ensembles for Tabular Data](https://openreview.net/forum?id=XEFWBxi075) |  | 0 | Despite the success of deep learning for text and image data, tree-based ensemble models are still state-of-the-art for machine learning with heterogeneous tabular data. However, there is a significant need for tabular-specific gradient-based methods due to their high flexibility. In this paper, we... | Christian Bartelt, Heiner Stuckenschmidt, Sascha Marton, Stefan Lüdtke |  |
| 2406 |  |  [GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers](https://openreview.net/forum?id=uJVHygNeSZ) |  | 0 | As transformers are equivariant to the permutation of input tokens, encoding the positional information of tokens is necessary for many tasks. However, since existing positional encoding schemes have been initially designed for NLP tasks, their suitability for vision tasks, which typically exhibit... | Andreas Geiger, Bernhard Jaeger, Max Welling, Takeru Miyato |  |
| 2407 |  |  [VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models](https://openreview.net/forum?id=ygxTuVz9eU) |  | 0 | The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence... | Baoyuan Wu, Bingzhe Wu, Mingda Zhang, Shaokui Wei, Zihao Zhu |  |
| 2408 |  |  [Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching](https://openreview.net/forum?id=rTBL8OhdhH) |  | 0 | The ultimate goal of Dataset Distillation is to synthesize a small synthetic dataset such that a model trained on this synthetic set will perform equally well as a model trained on the full, real dataset. Until now, no method of Dataset Distillation has reached this completely lossless goal, in... | George Cazenavette, Hui Li, Kai Wang, Kaipeng Zhang, Yang You, Ziyao Guo |  |
| 2409 |  |  [SYMBOL: Generating Flexible Black-Box Optimizers through Symbolic Equation Learning](https://openreview.net/forum?id=vLJcd43U7a) |  | 0 | Recent Meta-learning for Black-Box Optimization (MetaBBO) methods harness neural networks to meta-learn configurations of traditional black-box optimizers. Despite their success, they are inevitably restricted by the limitations of predefined hand-crafted optimizers. In this paper, we present... | Hongshu Guo, Jiacheng Chen, Jie Zhang, Yining Ma, YueJiao Gong, Zeyuan Ma |  |
| 2410 |  |  [SEA: Sparse Linear Attention with Estimated Attention Mask](https://openreview.net/forum?id=JbcwfmYrob) |  | 0 | The transformer architecture has driven breakthroughs in recent years on tasks which require modeling pairwise relationships between sequential elements, as is the case in natural language understanding. However, long seqeuences pose a problem due to the quadratic complexity of the attention... | Heejun Lee, Jeffrey Willette, Jina Kim, Sung Ju Hwang |  |
| 2411 |  |  [Zero-Mean Regularized Spectral Contrastive Learning: Implicitly Mitigating Wrong Connections in Positive-Pair Graphs](https://openreview.net/forum?id=RZBy8oHTz4) |  | 0 | Contrastive learning has emerged as a popular paradigm of self-supervised learning that learns representations by encouraging representations of positive pairs to be similar while representations of negative pairs to be far apart. The spectral contrastive loss, in synergy with the notion of... | Deming Zhai, Feilong Zhang, Gang Wu, Junjun Jiang, Xiangyang Ji, Xianming Liu, Xiong Zhou |  |
| 2412 |  |  [Variance-enlarged Poisson Learning for Graph-based Semi-Supervised Learning with Extremely Sparse Labeled Data](https://openreview.net/forum?id=yeeVBMDAwy) |  | 0 | Graph-based semi-supervised learning, particularly in the context of extremely sparse labeled data, often suffers from degenerate solutions where label functions tend to be nearly constant across unlabeled data. In this paper, we introduce Variance-enlarged Poisson Learning (VPL), a simple yet... | Hao Yu, Jialiang Wang, Junjun Jiang, Xiangyang Ji, Xianming Liu, Xiong Zhou, Zeke Xie |  |
| 2413 |  |  [Enhancing Contrastive Learning for Ordinal Regression via Ordinal Content Preserved Data Augmentation](https://openreview.net/forum?id=kx2XZlmgB1) |  | 0 | Contrastive learning, while highly effective for a lot of tasks, shows limited improvement in ordinal regression. We find that the limitation comes from the predefined strong data augmentations employed in contrastive learning. Intuitively, for ordinal regression datasets, the discriminative... | Bo Han, Dadong Wang, Jiyang Zheng, Tongliang Liu, Yu Yao |  |
| 2414 |  |  [SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning](https://openreview.net/forum?id=pTHfApDakA) |  | 0 | The recent progress in large language models (LLMs), especially the invention of chain-of-thought prompting, has made it possible to automatically answer questions by stepwise reasoning. However, when faced with more complicated problems that require non-linear thinking, even the strongest LLMs... | Ning Miao, Tom Rainforth, Yee Whye Teh |  |
| 2415 |  |  [OmniControl: Control Any Joint at Any Time for Human Motion Generation](https://openreview.net/forum?id=gd0lAEtWso) |  | 0 | We present a novel approach named OmniControl for incorporating flexible spatial control signals into a text-conditioned human motion generation model based on the diffusion process. Unlike previous methods that can only control the pelvis trajectory, OmniControl can incorporate flexible spatial... | Deqing Sun, Huaizu Jiang, Lei Zhong, Varun Jampani, Yiming Xie |  |
| 2416 |  |  [Guaranteed Approximation Bounds for Mixed-Precision Neural Operators](https://openreview.net/forum?id=QJGj07PD9C) |  | 0 | Neural operators, such as Fourier Neural Operators (FNO), form a principled approach for learning solution operators for partial differential equations (PDE) and other mappings between function spaces. However, many real-world problems require high-resolution training data, and the training time... | Anima Anandkumar, Boris Bonev, Colin White, Gennady Pekhimenko, Jean Kossaifi, Kamyar Azizzadenesheli, Renbo Tu |  |
| 2417 |  |  [Geometry-Aware Projective Mapping for Unbounded Neural Radiance Fields](https://openreview.net/forum?id=w7BwaDHppp) |  | 0 | Estimating neural radiance fields (NeRFs) is able to generate novel views of a scene from known imagery. Recent approaches have afforded dramatic progress on small bounded regions of the scene. For an unbounded scene where cameras point in any direction and contents exist at any distance, certain... | HaeGon Jeon, Hyunjun Jung, Inhwan Bae, JinHwi Park, Junoh Lee |  |
| 2418 |  |  [REValueD: Regularised Ensemble Value-Decomposition for Factorisable Markov Decision Processes](https://openreview.net/forum?id=Gf15GsnfTy) |  | 0 | Discrete-action reinforcement learning algorithms often falter in tasks with high-dimensional discrete action spaces due to the vast number of possible actions. A recent advancement leverages value-decomposition, a concept from multi-agent reinforcement learning, to tackle this challenge. This... | David Ireland, Giovanni Montana |  |
| 2419 |  |  [Path Choice Matters for Clear Attributions in Path Methods](https://openreview.net/forum?id=gzYgsZgwXa) |  | 0 | Rigorousness and clarity are both essential for interpretations of DNNs to engender human trust. Path methods are commonly employed to generate rigorous attributions that satisfy three axioms. However, the meaning of attributions remains ambiguous due to distinct path choices. To address the... | Borui Zhang, Jie Zhou, Jiwen Lu, Wenzhao Zheng |  |
| 2420 |  |  [Exploring Target Representations for Masked Autoencoders](https://openreview.net/forum?id=xmQMz9OPF5) |  | 0 | Masked autoencoders have become popular training paradigms for self-supervised visual representation learning. These models randomly mask a portion of the input and reconstruct the masked portion according to assigned target representations. In this paper, we show that a careful choice of the... | Jinghao Zhou, Rongrong Ji, Tao Kong, Xianming Lin, Xingbin Liu |  |
| 2421 |  |  [Koopman-based generalization bound: New aspect for full-rank weights](https://openreview.net/forum?id=JN7TcCm9LF) |  | 0 | We propose a new bound for generalization of neural networks using Koopman operators. Whereas most of existing works focus on low-rank weight matrices, we focus on full-rank weight matrices. Our bound is tighter than existing norm-based bounds when the condition numbers of weight matrices are... | Atsushi Nitanda, Isao Ishikawa, Sho Sonoda, Taiji Suzuki, Yuka Hashimoto |  |
| 2422 |  |  [Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis](https://openreview.net/forum?id=aA33A70IO6) |  | 0 | The rapid development of large language models (LLMs) has not only provided numerous opportunities but also presented significant challenges. This becomes particularly evident when LLMs inadvertently generate harmful or toxic content, either unintentionally or because of intentional inducement.... | Chunwei Wang, DitYan Yeung, Fei Mi, Hang Xu, Jianhua Han, Kai Chen, Kuo Yang, Lanqing Hong, Lifeng Shang, Qun Liu, Wenyong Huang, Xin Jiang, Zhenguo Li, Zhengying Liu |  |
| 2423 |  |  [MagicDrive: Street View Generation with Diverse 3D Geometry Control](https://openreview.net/forum?id=sBQwvucduK) |  | 0 | Recent advancements in diffusion models have significantly enhanced the data synthesis with 2D control. Yet, precise 3D control in street view generation, crucial for 3D perception tasks, remains elusive. Specifically, utilizing Bird's-Eye View (BEV) as the primary condition often leads to... | DitYan Yeung, Enze Xie, Kai Chen, Lanqing Hong, Qiang Xu, Ruiyuan Gao, Zhenguo Li |  |
| 2424 |  |  [MogaNet: Multi-order Gated Aggregation Network](https://openreview.net/forum?id=XhYWgjqCrV) |  | 0 | By contextualizing the kernel as global as possible, Modern ConvNets have shown great potential in computer vision tasks. However, recent progress on \textit{multi-order game-theoretic interaction} within deep neural networks (DNNs) reveals the representation bottleneck of modern ConvNets, where... | Cheng Tan, Di Wu, Haitao Lin, Jiangbin Zheng, Siyuan Li, Stan Z. Li, Zedong Wang, Zhiyuan Chen, Zicheng Liu |  |
| 2425 |  |  [GeoDiffusion: Text-Prompted Geometric Control for Object Detection Data Generation](https://openreview.net/forum?id=xBfQZWeDRH) |  | 0 | Diffusion models have attracted significant attention due to the remarkable ability to create content and generate data for tasks like image classification. However, the usage of diffusion models to generate the high-quality object detection data remains an underexplored area, where not only... | DitYan Yeung, Enze Xie, Kai Chen, Lanqing Hong, Yibo Wang, Zhe Chen, Zhenguo Li |  |
| 2426 |  |  [Un-Mixing Test-Time Normalization Statistics: Combatting Label Temporal Correlation](https://openreview.net/forum?id=xyxU99Nutg) |  | 0 | Recent test-time adaptation methods heavily rely on nuanced adjustments of batch normalization (BN) parameters. However, one critical assumption often goes overlooked: that of independently and identically distributed (i.i.d.) test batches with respect to unknown labels. This oversight leads to... | Behzad Bozorgtabar, Devavrat Tomar, Guillaume Vray, JeanPhilippe Thiran |  |
| 2427 |  |  [Constraint-Free Structure Learning with Smooth Acyclic Orientations](https://openreview.net/forum?id=KWO8LSUC5W) |  | 0 | The structure learning problem consists of fitting data generated by a Directed Acyclic Graph (DAG) to correctly reconstruct its arcs. In this context, differentiable approaches constrain or regularize an optimization problem with a continuous relaxation of the acyclicity property. The... | Davide Bacciu, Francesco Landolfi, Martina Cinquini, Riccardo Massidda |  |
| 2428 |  |  [Pareto Deep Long-Tailed Recognition: A Conflict-Averse Solution](https://openreview.net/forum?id=b66P1u0k15) |  | 0 | Deep long-tailed recognition (DTLR) has attracted much attention due to its close touch with realistic scenarios. Recent advances have focused on re-balancing across various aspects, e.g., sampling strategy, loss re-weighting, logit adjustment, and input/parameter perturbation, to name a few.... | Liu Liu, Peilin Zhao, Wei Gong, Zhipeng Zhou |  |
| 2429 |  |  [MixSup: Mixed-grained Supervision for Label-efficient LiDAR-based 3D Object Detection](https://openreview.net/forum?id=Q1vkAhdI6j) |  | 0 | Label-efficient LiDAR-based 3D object detection is currently dominated by weakly/semi-supervised methods. Instead of exclusively following one of them, we propose MixSup, a more practical paradigm simultaneously utilizing massive cheap coarse labels and a limited number of accurate labels for... | Lue Fan, Yuxue Yang, Zhaoxiang Zhang |  |
| 2430 |  |  [Boosting Vanilla Lightweight Vision Transformers via Re-parameterization](https://openreview.net/forum?id=3rmpixOjPS) |  | 0 | Large-scale Vision Transformers have achieved promising performance on downstream tasks through feature pre-training. However, the performance of vanilla lightweight Vision Transformers (ViTs) is still far from satisfactory compared to that of recent lightweight CNNs or hybrid networks. In this... | Jieping Ye, Le Lu, Nenghai Yu, Qi Chu, Xiaodan Li, Yue Wu, Zhentao Tan |  |
| 2431 |  |  [Robust Angular Synchronization via Directed Graph Neural Networks](https://openreview.net/forum?id=5sjxMwWmk8) |  | 0 | The angular synchronization problem aims to accurately estimate (up to a constant additive phase) a set of unknown angles $\theta_1, \dots, \theta_n\in[0, 2\pi)$ from $m$ noisy measurements of their offsets $\theta_i-\theta_j$ mod $2\pi.$ Applications include, for example, sensor network... | David Wipf, Gesine Reinert, Mihai Cucuringu, Yixuan He |  |
| 2432 |  |  [Multi-Scale Representations by Varying Window Attention for Semantic Segmentation](https://openreview.net/forum?id=lAhWGOkpSR) |  | 0 | Multi-scale learning is central to semantic segmentation. We visualize the effective receptive field (ERF) of canonical multi-scale representations and point out two risks learning them: \textit{scale inadequacy} and \textit{field inactivation}. A novel multi-scale learner, \textbf{varying window... | Chuang Zhang, Haotian Yan, Ming Wu |  |
| 2433 |  |  [FedP3: Federated Personalized and Privacy-friendly Network Pruning under Model Heterogeneity](https://openreview.net/forum?id=hbHwZYqk9T) |  | 0 | The interest in federated learning has surged in recent research due to its unique ability to train a global model using privacy-secured information held locally on each client. This paper pays particular attention to the issue of client-side model heterogeneity, a pervasive challenge in the... | Kai Yi, Lingjuan Lyu, Nidham Gazagnadou, Peter Richtárik |  |
| 2434 |  |  [Compressed Context Memory for Online Language Model Interaction](https://openreview.net/forum?id=64kSvC4iPg) |  | 0 | This paper presents a context key/value compression method for Transformer language models in online scenarios, where the context continually expands. As the context lengthens, the attention process demands increasing memory and computations, which in turn reduces the throughput of the language... | Hyun Oh Song, JangHyun Kim, Junyoung Yeom, Sangdoo Yun |  |
| 2435 |  |  [TUVF: Learning Generalizable Texture UV Radiance Fields](https://openreview.net/forum?id=dN4vpVTvWX) |  | 0 | Textures are a vital aspect of creating visually appealing and realistic 3D models. In this paper, we study the problem of generating high-fidelity texture given shapes of 3D assets, which has been relatively less explored compared with generic 3D shape modeling. Our goal is to facilitate a... | AnChieh Cheng, Sifei Liu, Xiaolong Wang, Xueting Li |  |
| 2436 |  |  [Neural Processing of Tri-Plane Hybrid Neural Fields](https://openreview.net/forum?id=zRkM6UcA22) |  | 0 | Driven by the appealing properties of neural fields for storing and communicating 3D data, the problem of directly processing them to address tasks such as classification and part segmentation has emerged and has been investigated in recent works. Early approaches employ neural fields parameterized... | Adriano Cardace, Allan Zhou, Francesco Ballerini, Luigi Di Stefano, Pierluigi Zama Ramirez, Samuele Salti |  |
| 2437 |  |  [Large-Vocabulary 3D Diffusion Model with Transformer](https://openreview.net/forum?id=q57JLSE2j5) |  | 0 | Creating diverse and high-quality 3D assets with an automatic generative model is highly desirable. Despite extensive efforts on 3D generation, most existing works focus on the generation of a single category or a few categories. In this paper, we introduce a diffusion-based feed-forward framework... | Fangzhou Hong, Liang Pan, Tong Wu, Ziang Cao, Ziwei Liu |  |
| 2438 |  |  [SAS: Structured Activation Sparsification](https://openreview.net/forum?id=vZfi5to2Xl) |  | 0 | Wide networks usually yield better accuracy than their narrower counterpart at the expense of the massive $\texttt{mult}$ cost. To break this tradeoff, we advocate a novel concept of $\textit{Structured Activation Sparsification}$, dubbed SAS, which boosts accuracy without increasing computation by... | Shingo Yashima, Yusuke Sekikawa |  |
| 2439 |  |  [A Progressive Training Framework for Spiking Neural Networks with Learnable Multi-hierarchical Model](https://openreview.net/forum?id=g52tgL8jy6) |  | 0 | Spiking Neural Networks (SNNs) have garnered considerable attention due to their energy efficiency and unique biological characteristics. However, the widely adopted Leaky Integrate-and-Fire (LIF) model, as the mainstream neuron model in current SNN research, has been revealed to exhibit... | Tiejun Huang, Tong Bu, Xinyu Shi, Zecheng Hao, Zhaofei Yu, Zihan Huang |  |
| 2440 |  |  [Mega-TTS 2: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis](https://openreview.net/forum?id=mvMI3N4AvD) |  | 0 | Zero-shot text-to-speech (TTS) aims to synthesize voices with unseen speech prompts, which significantly reduces the data and computation requirements for voice cloning by skipping the fine-tuning process. However, the prompting mechanisms of zero-shot TTS still face challenges in the following... | Chen Zhang, Chunfeng Wang, Jinglin Liu, Jinzheng He, Pengfei Wei, Qian Yang, Shengpeng Ji, Xiang Yin, Yi Ren, Zejun Ma, Zhenhui Ye, Zhou Zhao, Ziyue Jiang |  |
| 2441 |  |  [A Symmetry-Aware Exploration of Bayesian Neural Network Posteriors](https://openreview.net/forum?id=FOSBQuXgAq) |  | 0 | The distribution of modern deep neural networks (DNNs) weights -- crucial for uncertainty quantification and robustness -- is an eminently complex object due to its extremely high dimensionality. This paper presents one of the first large-scale explorations of the posterior distribution of deep... | Emanuel Aldea, Gianni Franchi, Olivier Laurent |  |
| 2442 |  |  [Threaten Spiking Neural Networks through Combining Rate and Temporal Information](https://openreview.net/forum?id=xv8iGxENyI) |  | 0 | Spiking Neural Networks (SNNs) have received widespread attention in academic communities due to their superior spatio-temporal processing capabilities and energy-efficient characteristics. With further in-depth application in various fields, the vulnerability of SNNs under adversarial attack has... | Tiejun Huang, Tong Bu, Xinyu Shi, Zecheng Hao, Zhaofei Yu, Zihan Huang |  |
| 2443 |  |  [QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models](https://openreview.net/forum?id=FIplmUWdm3) |  | 0 | Large Language Models (LLMs) have demonstrated unparalleled efficacy in natural language processing. However, their high computational demands and memory overheads hinder their broad deployment. To address this, two quantization strategies emerge, including Quantization-Aware Training (QAT) and... | Bohan Zhuang, Jianfei Cai, Jing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong |  |
| 2444 |  |  [3D-Aware Hypothesis & Verification for Generalizable Relative Object Pose Estimation](https://openreview.net/forum?id=U6hEOZlDf5) |  | 0 | Prior methods that tackle the problem of generalizable object pose estimation highly rely on having dense views of the unseen object. By contrast, we address the scenario where only a single reference view of the object is available. Our goal then is to estimate the relative object pose between... | Chen Zhao, Mathieu Salzmann, Tong Zhang |  |
| 2445 |  |  [Language Model Self-improvement by Reinforcement Learning Contemplation](https://openreview.net/forum?id=38E4yUbrgr) |  | 0 | Language model self-improvement (LMSI) techniques have recently gained significant attention as they improve language models without requiring external supervision. A common approach is reinforcement learning from AI feedback (RLAIF), which trains a reward model based on AI preference data and... | Jiacheng Xu, JingCheng Pang, Kaiyuan Li, Pengyuan Wang, XiongHui Chen, Yang Yu, Zongzhang Zhang |  |
| 2446 |  |  [Divide and not forget: Ensemble of selectively trained experts in Continual Learning](https://openreview.net/forum?id=sSyytcewxe) |  | 0 | Class-incremental learning is becoming more popular as it helps models widen their applicability while not forgetting what they already know. A trend in this area is to use a mixture-of-expert technique, where different models work together to solve the task. However, the experts are usually... | Bartlomiej Twardowski, Bartosz Zielinski, Grzegorz Rypesc, Sebastian Cygert, Tomasz Trzcinski, Valeriya Khan |  |
| 2447 |  |  [Towards Offline Opponent Modeling with In-context Learning](https://openreview.net/forum?id=2SwHngthig) |  | 0 | Opponent modeling aims at learning the opponent's behaviors, goals, or beliefs to reduce the uncertainty of the competitive environment and assist decision-making. Existing work has mostly focused on learning opponent models online, which is impractical and inefficient in practical scenarios. To... | Bingyun Liu, Haobo Fu, Jian Cheng, Junliang Xing, Kai Li, Qiang Fu, Yifan Zang, Yuheng Jing |  |
| 2448 |  |  [Early Stopping Against Label Noise Without Validation Data](https://openreview.net/forum?id=CMzF2aOfqp) |  | 0 | Early stopping methods in deep learning face the challenge of balancing the volume of training and validation data, especially in the presence of label noise. Concretely, sparing more data for validation from training data would limit the performance of the learned model, yet insufficient... | Lei Feng, Suqin Yuan, Tongliang Liu |  |
| 2449 |  |  [Recursive Generalization Transformer for Image Super-Resolution](https://openreview.net/forum?id=owziuM1nsR) |  | 0 | Transformer architectures have exhibited remarkable performance in image super-resolution (SR). Since the quadratic computational complexity of the self-attention (SA) in Transformer, existing methods tend to adopt SA in a local region to reduce overheads. However, the local design restricts the... | Jinjin Gu, Linghe Kong, Xiaokang Yang, Yulun Zhang, Zheng Chen |  |
| 2450 |  |  [Rethinking Model Ensemble in Transfer-based Adversarial Attacks](https://openreview.net/forum?id=AcJrSoArlh) |  | 0 | It is widely recognized that deep learning models lack robustness to adversarial examples. An intriguing property of adversarial examples is that they can transfer across different models, which enables black-box attacks without any knowledge of the victim model. An effective strategy to improve... | Hang Su, Huanran Chen, Jun Zhu, Xiao Yang, Yichi Zhang, Yinpeng Dong |  |
| 2451 |  |  [Langevin Monte Carlo for strongly log-concave distributions: Randomized midpoint revisited](https://openreview.net/forum?id=hOxgrGM63n) |  | 0 | We revisit the problem of sampling from a target distribution that has a smooth strongly log-concave density everywhere in $\mathbb{R}^p$. In this context, if no additional density information is available, the randomized midpoint discretization for the kinetic Langevin diffusion is known to be the... | Arnak S. Dalalyan, Avetik G. Karagulyan, Lu Yu |  |
| 2452 |  |  [MuSc: Zero-Shot Industrial Anomaly Classification and Segmentation with Mutual Scoring of the Unlabeled Images](https://openreview.net/forum?id=AHgc5SMdtd) |  | 0 | This paper studies zero-shot anomaly classification (AC) and segmentation (AS) in industrial vision. We reveal that the abundant normal and abnormal cues implicit in unlabeled test images can be exploited for anomaly determination, which is ignored by prior methods. Our key observation is that for... | Feng Xue, Xurui Li, Yu Zhou, Ziming Huang |  |
| 2453 |  |  [To the Cutoff... and Beyond? A Longitudinal Perspective on LLM Data Contamination](https://openreview.net/forum?id=m2NVG4Htxs) |  | 0 | Recent claims about the impressive abilities of large language models (LLMs) are often supported by evaluating publicly available benchmarks. Since LLMs train on wide swaths of the internet, this practice raises concerns of data contamination, i.e., evaluating on examples that are explicitly or... | Christine Herlihy, Colin White, Himanshu Thakur, Manley Roberts, Samuel Dooley |  |
| 2454 |  |  [I-PHYRE: Interactive Physical Reasoning](https://openreview.net/forum?id=1bbPQShCT2) |  | 0 | Current evaluation protocols predominantly assess physical reasoning in stationary scenes, creating a gap in evaluating agents' abilities to interact with dynamic events. While contemporary methods allow agents to modify initial scene configurations and observe consequences, they lack the... | Chi Zhang, Kewen Wu, Shiqian Li, Yixin Zhu |  |
| 2455 |  |  [Exposing Text-Image Inconsistency Using Diffusion Models](https://openreview.net/forum?id=Ny150AblPu) |  | 0 | In the battle against widespread online misinformation, a growing problem is text-image inconsistency, where images are misleadingly paired with texts with different intent or meaning. Existing classification-based methods for text-image inconsistency can identify contextual inconsistencies but... | Jialing Cai, Mingzhen Huang, Shan Jia, Siwei Lyu, Yan Ju, Zhou Zhou |  |
