# ICLR2022

## 会议论文列表

本会议共有 1094 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [Domino: Discovering Systematic Errors with Cross-Modal Embeddings](https://openreview.net/forum?id=FPCMqjI0jXN) |  | 0 | Machine learning models that achieve high overall accuracy often make systematic errors on important subsets (or slices) of data. Identifying underperforming slices is particularly challenging when working with high-dimensional inputs (e.g. images, audio), where important slices are often unlabeled. In order to address this issue, recent studies have proposed automated... | Christopher LeeMesser, Christopher Ré, James Zou, Jared Dunnmon, JeanBenoit Delbrouck, Khaled Kamal Saab, Maya Varma, Sabri Eyuboglu |  |
| 2 |  |  [Natural Language Descriptions of Deep Visual Features](https://openreview.net/forum?id=NudBMY-tzDr) |  | 0 | Some neurons in deep networks specialize in recognizing highly specific perceptual, structural, or semantic features of inputs. In computer vision, techniques exist for identifying neurons that respond to individual concept categories like colors, textures, and object classes. But these techniques are limited in scope, labeling only a small subset of neurons and... | Antonio Torralba, David Bau, Evan Hernandez, Jacob Andreas, Sarah Schwettmann, Teona Bagashvili |  |
| 3 |  |  [Non-Transferable Learning: A New Approach for Model Ownership Verification and Applicability Authorization](https://openreview.net/forum?id=tYRrOdSnVUy) |  | 0 | As Artificial Intelligence as a Service gains popularity, protecting well-trained models as intellectual property is becoming increasingly important. There are two common types of protection methods: ownership verification and usage authorization. In this paper, we propose Non-Transferable Learning (NTL), a novel approach that captures the exclusive data representation... | Lixu Wang, Qi Zhu, Ruiqi Xu, Shichao Xu, Xiao Wang |  |
| 4 |  |  [Neural Structured Prediction for Inductive Node Classification](https://openreview.net/forum?id=YWNAX0caEjI) |  | 0 | This paper studies node classification in the inductive setting, i.e., aiming to learn a model on labeled training graphs and generalize it to infer node labels on unlabeled test graphs. This problem has been extensively studied with graph neural networks (GNNs) by learning effective node representations, as well as traditional structured prediction methods for modeling... | Huiyu Cai, Jian Tang, Meng Qu |  |
| 5 |  |  [A New Perspective on "How Graph Neural Networks Go Beyond Weisfeiler-Lehman?"](https://openreview.net/forum?id=uxgg9o7bI_3) |  | 0 | We propose a new perspective on designing powerful Graph Neural Networks (GNNs). In a nutshell, this enables a general solution to inject structural properties of graphs into a message-passing aggregation scheme of GNNs. As a theoretical basis, we develop a new hierarchy of local isomorphism on neighborhood subgraphs. Then, we theoretically characterize how... | Asiri Wijesinghe, Qing Wang |  |
| 6 |  |  [Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond](https://openreview.net/forum?id=LdlwbBP2mlq) |  | 0 | In distributed learning, local SGD (also known as federated averaging) and its simple baseline minibatch SGD are widely studied optimization methods. Most existing analyses of these methods assume independent and unbiased gradient estimates obtained via with-replacement sampling. In contrast, we study shuffling-based variants: minibatch and local Random Reshuffling,... | Chulhee Yun, Shashank Rajput, Suvrit Sra |  |
| 7 |  |  [The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks: an Exact Characterization of Optimal Solutions](https://openreview.net/forum?id=Z7Lk2cQEG8a) |  | 0 | We prove that finding all globally optimal two-layer ReLU neural networks can be performed by solving a convex optimization program with cone constraints. Our analysis is novel, characterizes all optimal solutions, and does not leverage duality-based analysis which was recently used to lift neural network training into convex spaces. Given the set of solutions of our... | Jonathan Lacotte, Mert Pilanci, Yifei Wang |  |
| 8 |  |  [Provably Filtering Exogenous Distractors using Multistep Inverse Dynamics](https://openreview.net/forum?id=RQLLzMCefQu) |  | 0 | Many real-world applications of reinforcement learning (RL) require the agent to deal with high-dimensional observations such as those generated from a megapixel camera. Prior work has addressed such problems with representation learning, through which the agent can provably extract endogenous, latent state information from raw observations and subsequently plan... | Akshay Krishnamurthy, Alekh Agarwal, Dipendra Misra, John Langford, Yonathan Efroni |  |
| 9 |  |  [Bootstrapped Meta-Learning](https://openreview.net/forum?id=b-ny3x071E5) |  | 0 | Meta-learning empowers artificial intelligence to increase its efficiency by learning how to learn. Unlocking this potential involves overcoming a challenging meta-optimisation problem. We propose an algorithm that tackles this problem by letting the meta-learner teach itself. The algorithm first bootstraps a target from the meta-learner, then optimises the meta-learner... | David Silver, Hado van Hasselt, Satinder Singh, Sebastian Flennerhag, Tom Zahavy, Yannick Schroecker |  |
| 10 |  |  [Coordination Among Neural Modules Through a Shared Global Workspace](https://openreview.net/forum?id=XzTtHjgPDsT) |  | 0 | Deep learning has seen a movement away from representing examples with a monolithic hidden state towards a richly structured state. For example, Transformers segment by position, and object-centric architectures decompose images into entities. In all these architectures, interactions between different elements are modeled via pairwise interactions: Transformers make use... | Alex Lamb, Aniket Rajiv Didolkar, Anirudh Goyal, Charles Blundell, Jonathan Binas, Kartikeya Badola, Michael Curtis Mozer, Nan Rosemary Ke, Nasim Rahaman, Yoshua Bengio |  |
| 11 |  |  [Data-Efficient Graph Grammar Learning for Molecular Generation](https://openreview.net/forum?id=l4IHywGq6a) |  | 0 | The problem of molecular generation has received significant attention recently. Existing methods are typically based on deep neural networks and require training on large datasets with tens of thousands of samples. In practice, however, the size of class-specific chemical datasets is usually limited (e.g., dozens of samples) due to labor-intensive experimentation and... | Beichen Li, Jie Chen, Minghao Guo, Payel Das, Veronika Thost, Wojciech Matusik |  |
| 12 |  |  [Poisoning and Backdooring Contrastive Learning](https://openreview.net/forum?id=iC4UHbQ01Mp) |  | 0 | Multimodal contrastive learning methods like CLIP train on noisy and uncurated training datasets. This is cheaper than labeling datasets manually, and even improves out-of-distribution robustness. We show that this practice makes backdoor and poisoning attacks a significant threat. By poisoning just 0.01% of a dataset (e.g., just 300 images of the 3 million-example... | Andreas Terzis, Nicholas Carlini |  |
| 13 |  |  [Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path](https://openreview.net/forum?id=w1UbdvWH_R3) |  | 0 | The recently discovered Neural Collapse (NC) phenomenon occurs pervasively in today's deep net training paradigm of driving cross-entropy (CE) loss towards zero. During NC, last-layer features collapse to their class-means, both classifiers and class-means collapse to the same Simplex Equiangular Tight Frame, and classifier behavior collapses to the nearest-class-mean... | David L. Donoho, Vardan Papyan, X. Y. Han |  |
| 14 |  |  [Weighted Training for Cross-Task Learning](https://openreview.net/forum?id=ltM1RMZntpu) |  | 0 | In this paper, we introduce Target-Aware Weighted Training (TAWT), a weighted training algorithm for cross-task learning based on minimizing a representation-based task distance between the source and target tasks. We show that TAWT is easy to implement, is computationally efficient, requires little hyperparameter tuning, and enjoys non-asymptotic learning-theoretic... | Dan Roth, Hangfeng He, Koby Crammer, Shuxiao Chen, Weijie J. Su |  |
| 15 |  |  [iLQR-VAE : control-based learning of input-driven dynamics with applications to neural data](https://openreview.net/forum?id=wRODLDHaAiW) |  | 0 | Understanding how neural dynamics give rise to behaviour is one of the most fundamental questions in systems neuroscience. To achieve this, a common approach is to record neural populations in behaving animals, and model these data as emanating from a latent dynamical system whose state trajectories can then be related back to behavioural observations via some form of... | Guillaume Hennequin, Kristopher T. Jensen, Marine Schimel, TaChu Kao |  |
| 16 |  |  [Extending the WILDS Benchmark for Unsupervised Adaptation](https://openreview.net/forum?id=z7p2V6KROOV) |  | 0 | Machine learning systems deployed in the wild are often trained on a source distribution but deployed on a different target distribution. Unlabeled data can be a powerful point of leverage for mitigating these distribution shifts, as it is frequently much more available than labeled data and can often be obtained from distributions beyond the source distribution as... | Ananya Kumar, Chelsea Finn, Etienne David, Henrik Marklund, Ian Stavness, Irena Gao, Jure Leskovec, Kate Saenko, Kendrick Shen, Michihiro Yasunaga, Pang Wei Koh, Percy Liang, Sang Michael Xie, Sara Beery, Sergey Levine, Shiori Sagawa, Tatsunori Hashimoto, Tony Lee, Wei Guo, Weihua Hu |  |
| 17 |  |  [Asymmetry Learning for Counterfactually-invariant Classification in OOD Tasks](https://openreview.net/forum?id=avgclFZ221l) |  | 0 | Generalizing from observed to new related environments (out-of-distribution) is central to the reliability of classifiers. However, most classifiers fail to predict label $Y$ from input $X$ when the change in environment is due a (stochastic) input transformation $T^\text{te} \circ X'$ not observed in training, as in training we observe $T^\text{tr} \circ X'$, where... | Bruno Ribeiro, S. Chandra Mouli |  |
| 18 |  |  [Comparing Distributions by Measuring Differences that Affect Decision Making](https://openreview.net/forum?id=KB5onONJIAU) |  | 0 | Measuring the discrepancy between two probability distributions is a fundamental problem in machine learning and statistics. We propose a new class of discrepancies based on the optimal loss for a decision task -- two distributions are different if the optimal decision loss is higher on their mixture than on each individual distribution. By suitably choosing the... | Abhishek Sinha, Aidan Perreault, Jiaming Song, Shengjia Zhao, Stefano Ermon, Yutong He |  |
| 19 |  |  [MIDI-DDSP: Detailed Control of Musical Performance via Hierarchical Modeling](https://openreview.net/forum?id=UseMOjWENv) |  | 0 | Musical expression requires control of both what notes that are played, and how they are performed. Conventional audio synthesizers provide detailed expressive controls, but at the cost of realism. Black-box neural audio synthesis and concatenative samplers can produce realistic audio, but have few mechanisms for control. In this work, we introduce MIDI-DDSP a... | Aaron C. Courville, ChengZhi Anna Huang, Ethan Manilow, Jesse H. Engel, Kyle Kastner, Rigel Swavely, Tim Cooijmans, Yi Deng, Yusong Wu |  |
| 20 |  |  [Unsupervised Vision-Language Grammar Induction with Shared Structure Modeling](https://openreview.net/forum?id=N0n_QyQ5lBF) |  | 0 | We introduce a new task, unsupervised vision-language (VL) grammar induction. Given an image-caption pair, the goal is to extract a shared hierarchical structure for both image and language simultaneously. We argue that such structured output, grounded in both modalities, is a clear step towards the high-level understanding of multimodal information. Besides challenges... | Bo Wan, Tinne Tuytelaars, Wenjuan Han, Zilong Zheng |  |
| 21 |  |  [PiCO: Contrastive Label Disambiguation for Partial Label Learning](https://openreview.net/forum?id=EhYjZy6e1gJ) |  | 0 | Partial label learning (PLL) is an important problem that allows each training example to be labeled with a coarse candidate set, which well suits many real-world data annotation scenarios with label ambiguity. Despite the promise, the performance of PLL often lags behind the supervised counterpart. In this work, we bridge the gap by addressing two key research... | Gang Chen, Gang Niu, Haobo Wang, Junbo Zhao, Lei Feng, Ruixuan Xiao, Yixuan Li |  |
| 22 |  |  [Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting](https://openreview.net/forum?id=0EXmFzUn5I) |  | 0 | Accurate prediction of the future given the past based on time series data is of paramount importance, since it opens the door for decision making and risk management ahead of time. In practice, the challenge is to build a flexible but parsimonious model that can capture a wide range of temporal dependencies. In this paper, we propose Pyraformer by exploring the... | Alex X. Liu, Cong Liao, Hang Yu, Jianguo Li, Schahram Dustdar, Shizhan Liu, Weiyao Lin |  |
| 23 |  |  [Expressiveness and Approximation Properties of Graph Neural Networks](https://openreview.net/forum?id=wIzUeM3TAU) |  | 0 | Characterizing the separation power of graph neural networks (GNNs) provides an understanding of their limitations for graph learning tasks. Results regarding separation power are, however, usually geared at specific GNNs architectures, and tools for understanding arbitrary GNN architectures are generally lacking. We provide an elegant way to easily obtain bounds on the... | Floris Geerts, Juan L. Reutter |  |
| 24 |  |  [Filtered-CoPhy: Unsupervised Learning of Counterfactual Physics in Pixel Space](https://openreview.net/forum?id=1L0C5ROtFp) |  | 0 | Learning causal relationships in high-dimensional data (images, videos) is a hard task, as they are often defined on low dimensional manifolds and must be extracted from complex signals dominated by appearance, lighting, textures and also spurious correlations in the data. We present a method for learning counterfactual reasoning of physical processes in pixel space,... | Christian Wolf, Fabien Baradel, Greg Mori, Madiha Nadri, Natalia Neverova, Steeven Janny |  |
| 25 |  |  [BEiT: BERT Pre-Training of Image Transformers](https://openreview.net/forum?id=p-BhZSz59o4) |  | 0 | We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e., image patches (such as... | Furu Wei, Hangbo Bao, Li Dong, Songhao Piao |  |
| 26 |  |  [Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution](https://openreview.net/forum?id=UYneFzXSJWh) |  | 0 | When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer---the "head"). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than... | Aditi Raghunathan, Ananya Kumar, Percy Liang, Robbie Matthew Jones, Tengyu Ma |  |
| 27 |  |  [StyleAlign: Analysis and Applications of Aligned StyleGAN Models](https://openreview.net/forum?id=Qg2vi4ZbHM9) |  | 0 | In this paper, we perform an in-depth study of the properties and applications of aligned generative models. We refer to two models as aligned if they share the same architecture, and one of them (the child) is obtained from the other (the parent) via fine-tuning to another domain, a common practice in transfer learning. Several works already utilize some basic... | Dani Lischinski, Eli Shechtman, Yotam Nitzan, Zongze Wu |  |
| 28 |  |  [Variational Inference for Discriminative Learning with Generative Modeling of Feature Incompletion](https://openreview.net/forum?id=qnQN4yr6FJz) |  | 0 | We are concerned with the problem of distributional prediction with incomplete features: The goal is to estimate the distribution of target variables given feature vectors with some of the elements missing. A typical approach to this problem is to perform missing-value imputation and regression, simultaneously or sequentially, which we call the generative approach.... | Akira Koseki, Kohei Miyaguchi, Takayuki Katsuki, Toshiya Iwamori |  |
| 29 |  |  [Efficiently Modeling Long Sequences with Structured State Spaces](https://openreview.net/forum?id=uYLFoz1vlAC) |  | 0 | A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of... | Albert Gu, Christopher Ré, Karan Goel |  |
| 30 |  |  [Large Language Models Can Be Strong Differentially Private Learners](https://openreview.net/forum?id=bVuP3ltATMz) |  | 0 | Differentially Private (DP) learning has seen limited success for building large deep learning models of text, and straightforward attempts at applying Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have resulted in large performance drops and high computational overhead. We show that this performance drop can be mitigated with (1) the use of... | Florian Tramèr, Percy Liang, Tatsunori Hashimoto, Xuechen Li |  |
| 31 |  |  [GeoDiff: A Geometric Diffusion Model for Molecular Conformation Generation](https://openreview.net/forum?id=PzcvxEMzvQC) |  | 0 | Predicting molecular conformations from molecular graphs is a fundamental problem in cheminformatics and drug discovery. Recently, significant progress has been achieved with machine learning approaches, especially with deep generative models. Inspired by the diffusion process in classical non-equilibrium thermodynamics where heated particles will diffuse from original... | Chence Shi, Jian Tang, Lantao Yu, Minkai Xu, Stefano Ermon, Yang Song |  |
| 32 |  |  [Frame Averaging for Invariant and Equivariant Network Design](https://openreview.net/forum?id=zIUyj55nXR) |  | 0 | Many machine learning tasks involve learning functions that are known to be invariant or equivariant to certain symmetries of the input data. However, it is often challenging to design neural network architectures that respect these symmetries while being expressive and computationally efficient. For example, Euclidean motion invariant/equivariant graph or point cloud... | Aditya Grover, Edward J. Smith, Heli BenHamu, Ishan Misra, Matan Atzmon, Omri Puny, Yaron Lipman |  |
| 33 |  |  [Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation](https://openreview.net/forum?id=oapKSVM2bcj) |  | 0 | Tensor computations underlie modern scientific computing and deep learning. A number of tensor frameworks emerged varying in execution model, hardware support, memory management, model definition, etc. However, tensor operations in all frameworks follow the same paradigm. Recent neural network architectures demonstrate demand for higher expressiveness of tensor... | Alex Rogozhnikov |  |
| 34 |  |  [A Fine-Grained Analysis on Distribution Shift](https://openreview.net/forum?id=Dl4LetuLdyK) |  | 0 | Robustness to distribution shifts is critical for deploying machine learning models in the real world. Despite this necessity, there has been little work in defining the underlying mechanisms that cause these shifts and evaluating the robustness of algorithms across multiple, different distribution shifts. To this end, we introduce a framework that enables fine-grained... | Ali Taylan Cemgil, Florian Stimberg, Ira Ktena, Krishnamurthy Dvijotham, Olivia Wiles, Sven Gowal, SylvestreAlvise Rebuffi |  |
| 35 |  |  [Open-Set Recognition: A Good Closed-Set Classifier is All You Need](https://openreview.net/forum?id=5hLP5JY9S2d) |  | 0 | The ability to identify whether or not a test sample belongs to one of the semantic classes in a classifier's training set is critical to practical deployment of the model. This task is termed open-set recognition (OSR) and has received significant attention in recent years. In this paper, we first demonstrate that the ability of a classifier to make the 'none-of-above'... | Andrea Vedaldi, Andrew Zisserman, Kai Han, Sagar Vaze |  |
| 36 |  |  [Learning Strides in Convolutional Neural Networks](https://openreview.net/forum?id=M752z9FKJP) |  | 0 | Convolutional neural networks typically contain several downsampling operators, such as strided convolutions or pooling layers, that progressively reduce the resolution of intermediate representations. This provides some shift-invariance while reducing the computational complexity of the whole architecture. A critical hyperparameter of such layers is their stride: the... | David Grangier, Neil Zeghidour, Olivier Teboul, Rachid Riad |  |
| 37 |  |  [Understanding over-squashing and bottlenecks on graphs via curvature](https://openreview.net/forum?id=7UmjRGzp-A) |  | 0 | Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from distant nodes as a factor limiting the efficiency of message passing for tasks relying on long-distance interactions. This phenomenon, referred to as 'over-squashing', has been... | Benjamin Paul Chamberlain, Francesco Di Giovanni, Jake Topping, Michael M. Bronstein, Xiaowen Dong |  |
| 38 |  |  [Diffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling Scheme](https://openreview.net/forum?id=8c50f-DoWAu) |  | 0 | Voice conversion is a common speech synthesis task which can be solved in different ways depending on a particular real-world scenario. The most challenging one often referred to as one-shot many-to-many voice conversion consists in copying target voice from only one reference utterance in the most general case when both source and target speakers do not belong to the... | Ivan Vovk, Jiansheng Wei, Mikhail Sergeevich Kudinov, Tasnima Sadekova, Vadim Popov, Vladimir Gogoryan |  |
| 39 |  |  [Resolving Training Biases via Influence-based Data Relabeling](https://openreview.net/forum?id=EskfH0bwNVn) |  | 0 | The performance of supervised learning methods easily suffers from the training bias issue caused by train-test distribution mismatch or label noise. Influence function is a technique that estimates the impacts of a training sample on the model’s predictions. Recent studies on \emph{data resampling} have employed influence functions to identify \emph{harmful} training... | Linpeng Huang, Shuming Kong, Yanyan Shen |  |
| 40 |  |  [Representational Continuity for Unsupervised Continual Learning](https://openreview.net/forum?id=9Hrka5PA7LW) |  | 0 | Continual learning (CL) aims to learn a sequence of tasks without forgetting the previously acquired knowledge. However, recent CL advances are restricted to supervised continual learning (SCL) scenarios. Consequently, they are not scalable to real-world applications where the data distribution is often biased and unannotated. In this work, we focus on unsupervised... | Divyam Madaan, Jaehong Yoon, Sung Ju Hwang, Yuanchun Li, Yunxin Liu |  |
| 41 |  |  [Vision-Based Manipulators Need to Also See from Their Hands](https://openreview.net/forum?id=RJkAHKp7kNZ) |  | 0 | We study how the choice of visual perspective affects learning and generalization in the context of physical manipulation from raw sensor observations. Compared with the more commonly used global third-person perspective, a hand-centric (eye-in-hand) perspective affords reduced observability, but we find that it consistently improves training efficiency and... | Chelsea Finn, Jiajun Wu, Kyle Hsu, Moo Jin Kim, Rafael Rafailov |  |
| 42 |  |  [Meta-Learning with Fewer Tasks through Task Interpolation](https://openreview.net/forum?id=ajXWF7bVR8d) |  | 0 | Meta-learning enables algorithms to quickly learn a newly encountered task with just a few labeled examples by transferring previously learned knowledge. However, the bottleneck of current meta-learning algorithms is the requirement of a large number of meta-training tasks, which may not be accessible in real-world scenarios. To address the challenge that available... | Chelsea Finn, Huaxiu Yao, Linjun Zhang |  |
| 43 |  |  [Discovering and Explaining the Representation Bottleneck of DNNS](https://openreview.net/forum?id=iRCUlgmdfHJ) |  | 0 | This paper explores the bottleneck of feature representations of deep neural networks (DNNs), from the perspective of the complexity of interactions between input variables encoded in DNNs. To this end, we focus on the multi-order interaction between input variables, where the order represents the complexity of interactions. We discover that a DNN is more likely to... | Hao Zhang, Huiqi Deng, Qihan Ren, Quanshi Zhang |  |
| 44 |  |  [Sparse Communication via Mixed Distributions](https://openreview.net/forum?id=WAid50QschI) |  | 0 | Neural networks and other machine learning models compute continuous representations, while humans communicate mostly through discrete symbols. Reconciling these two forms of communication is desirable for generating human-readable interpretations or learning discrete latent variable models, while maintaining end-to-end differentiability. Some existing approaches (such... | André F. T. Martins, António Farinhas, Vlad Niculae, Wilker Aziz |  |
| 45 |  |  [Finetuned Language Models are Zero-Shot Learners](https://openreview.net/forum?id=gEZrGCozdqR) |  | 0 | This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning—finetuning language models on a collection of datasets described via instructions—substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP... | Adams Wei Yu, Andrew M. Dai, Brian Lester, Jason Wei, Kelvin Guu, Maarten Bosma, Nan Du, Quoc V. Le, Vincent Y. Zhao |  |
| 46 |  |  [F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization](https://openreview.net/forum?id=_CfpJazzXT2) |  | 0 | Neural network quantization is a promising compression technique to reduce memory footprint and save energy consumption, potentially leading to real-time inference. However, there is a performance gap between quantized and full-precision models. To reduce it, existing quantization approaches require high-precision INT32 or full-precision multiplication during inference... | Jian Ren, Kaiyuan Yang, Qing Jin, Richard Zhuang, Sergey Tulyakov, Sumant Hanumante, Yanzhi Wang, Zhengang Li, Zhiyu Chen |  |
| 47 |  |  [Transform2Act: Learning a Transform-and-Control Policy for Efficient Agent Design](https://openreview.net/forum?id=UcDUxjPYWSr) |  | 0 | An agent's functionality is largely determined by its design, i.e., skeletal structure and joint attributes (e.g., length, size, strength). However, finding the optimal agent design for a given function is extremely challenging since the problem is inherently combinatorial and the design space is prohibitively large. Additionally, it can be costly to evaluate each... | Kris M. Kitani, Wen Sun, Ye Yuan, Yuda Song, Zhengyi Luo |  |
| 48 |  |  [ProtoRes: Proto-Residual Network for Pose Authoring via Learned Inverse Kinematics](https://openreview.net/forum?id=s03AQxehtd_) |  | 0 | Our work focuses on the development of a learnable neural representation of human pose for advanced AI assisted animation tooling. Specifically, we tackle the problem of constructing a full static human pose based on sparse and variable user inputs (e.g. locations and/or orientations of a subset of body joints). To solve this problem, we propose a novel neural... | Bay Raitt, Boris N. Oreshkin, Dominic Laflamme, Florent Bocquelet, Félix G. Harvey |  |
| 49 |  |  [Hyperparameter Tuning with Renyi Differential Privacy](https://openreview.net/forum?id=-70L8lpp9DF) |  | 0 | For many differentially private algorithms, such as the prominent noisy stochastic gradient descent (DP-SGD), the analysis needed to bound the privacy leakage of a single training run is well understood. However, few studies have reasoned about the privacy leakage resulting from the multiple training runs needed to fine tune the value of the training algorithm’s... | Nicolas Papernot, Thomas Steinke |  |
| 50 |  |  [Real-Time Neural Voice Camouflage](https://openreview.net/forum?id=qj1IZ-6TInc) |  | 0 | Automatic speech recognition systems have created exciting possibilities for applications, however they also enable opportunities for systematic eavesdropping.We propose a method to camouflage a person's voice from these systems without inconveniencing the conversation between people in the room. Standard adversarial attacks are not effective in real-time streaming... | Carl Vondrick, Chengzhi Mao, Mia Chiquier |  |
| 51 |  |  [CycleMLP: A MLP-like Architecture for Dense Prediction](https://openreview.net/forum?id=NMEceG4v69Y) |  | 0 | This paper presents a simple MLP-like architecture, CycleMLP, which is a versatile backbone for visual recognition and dense predictions. As compared to modern MLP architectures, e.g. , MLP-Mixer, ResMLP, and gMLP, whose architectures are correlated to image size and thus are infeasible in object detection and segmentation, CycleMLP has two advantages compared to modern... | Chongjian Ge, Ding Liang, Enze Xie, Ping Luo, Runjian Chen, Shoufa Chen |  |
| 52 |  |  [Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models](https://openreview.net/forum?id=0xiJLKH-ufZ) |  | 0 | Diffusion probabilistic models (DPMs) represent a class of powerful generative models. Despite their success, the inference of DPMs is expensive since it generally needs to iterate over thousands of timesteps. A key problem in the inference is to estimate the variance in each timestep of the reverse process. In this work, we present a surprising result that both the... | Bo Zhang, Chongxuan Li, Fan Bao, Jun Zhu |  |
| 53 |  |  [RISP: Rendering-Invariant State Predictor with Differentiable Simulation and Rendering for Cross-Domain Parameter Estimation](https://openreview.net/forum?id=uSE03demja) |  | 0 | This work considers identifying parameters characterizing a physical system's dynamic motion directly from a video whose rendering configurations are inaccessible. Existing solutions require massive training data or lack generalizability to unknown rendering configurations. We propose a novel approach that marries domain randomization and differentiable rendering... | Chuang Gan, Joshua B. Tenenbaum, Pingchuan Ma, Tao Du, Wojciech Matusik |  |
| 54 |  |  [The Information Geometry of Unsupervised Reinforcement Learning](https://openreview.net/forum?id=3wU2UX0voE) |  | 0 | How can a reinforcement learning (RL) agent prepare to solve downstream tasks if those tasks are not known a priori? One approach is unsupervised skill discovery, a class of algorithms that learn a set of policies without access to a reward function. Such algorithms bear a close resemblance to representation learning algorithms (e.g., contrastive learning) in supervised... | Benjamin Eysenbach, Ruslan Salakhutdinov, Sergey Levine |  |
| 55 |  |  [Language modeling via stochastic processes](https://openreview.net/forum?id=pMQwKL1yctf) |  | 0 | Modern language models can generate high-quality short texts. However, they often meander or are incoherent when generating longer texts. These issues arise from the next-token-only language modeling objective. To address these issues, we introduce Time Control (TC), a language model that implicitly plans via a latent stochastic process. TC does this by learning a... | Esin Durmus, Noah D. Goodman, Rose E. Wang, Tatsunori Hashimoto |  |
| 56 |  |  [Learning to Downsample for Segmentation of Ultra-High Resolution Images](https://openreview.net/forum?id=HndgQudNb91) |  | 0 | Many computer vision systems require low-cost segmentation algorithms based on deep learning, either because of the enormous size of input images or limited computational budget. Common solutions uniformly downsample the input images to meet memory constraints, assuming all pixels are equally informative. In this work, we demonstrate that this assumption can harm the... | Chen Jin, Daniel C. Alexander, Eleftheria Panagiotaki, Ryutaro Tanno, Thomy Mertzanidou |  |
| 57 |  |  [Variational Neural Cellular Automata](https://openreview.net/forum?id=7fFO4cMBx_9) |  | 0 | In nature, the process of cellular growth and differentiation has lead to an amazing diversity of organisms --- algae, starfish, giant sequoia, tardigrades, and orcas are all created by the same generative process. Inspired by the incredible diversity of this biological generative process, we propose a generative model, the Variational Neural Cellular Automata (VNCA),... | Miguel González Duque, Rasmus Berg Palm, Sebastian Risi, Shyam Sudhakaran |  |
| 58 |  |  [Wish you were here: Hindsight Goal Selection for long-horizon dexterous manipulation](https://openreview.net/forum?id=FKp8-pIRo3y) |  | 0 | Complex sequential tasks in continuous-control settings often require agents to successfully traverse a set of \`\`narrow passages'' in their state space. Solving such tasks with a sparse reward in a sample-efficient manner poses a challenge to modern reinforcement learning (RL) due to the associated long-horizon nature of the problem and the lack of sufficient positive... | JeanBaptiste Regli, Jon Scholz, Markus Wulfmeier, Oleg Olegovich Sushkov, Stefan Schaal, Todor Davchev, Yusuf Aytar |  |
| 59 |  |  [L0-Sparse Canonical Correlation Analysis](https://openreview.net/forum?id=KntaNRo6R48) |  | 0 | Canonical Correlation Analysis (CCA) models are powerful for studying the associations between two sets of variables. The canonically correlated representations, termed \textit{canonical variates} are widely used in unsupervised learning to analyze unlabeled multi-modal registered datasets. Despite their success, CCA models may break (or overfit) if the number of... | Amir Averbuch, Moshe Salhov, Ofir Lindenbaum, Yuval Kluger |  |
| 60 |  |  [Recycling Model Updates in Federated Learning: Are Gradient Subspaces Low-Rank?](https://openreview.net/forum?id=B7ZbqNLDn-_) |  | 0 | In this paper, we question the rationale behind propagating large numbers of parameters through a distributed system during federated learning. We start by examining the rank characteristics of the subspace spanned by gradients (i.e., the gradient-space) in centralized model training, and observe that the gradient-space often consists of a few leading principal... | Christopher G. Brinton, Qiang Qiu, Seyyedali Hosseinalipour, Sheikh Shams Azam |  |
| 61 |  |  [Is Homophily a Necessity for Graph Neural Networks?](https://openreview.net/forum?id=ucASPPD9GKN) |  | 0 | Graph neural networks (GNNs) have shown great prowess in learning representations suitable for numerous graph-based machine learning tasks. When applied to semi-supervised node classification, GNNs are widely believed to work well due to the homophily assumption (\`\`like attracts like''), and fail to generalize to heterophilous graphs where dissimilar nodes connect.... | Jiliang Tang, Neil Shah, Xiaorui Liu, Yao Ma |  |
| 62 |  |  [DEGREE: Decomposition Based Explanation for Graph Neural Networks](https://openreview.net/forum?id=Ve0Wth3ptT_) |  | 0 | Graph Neural Networks (GNNs) are gaining extensive attention for their application in graph data. However, the black-box nature of GNNs prevents users from understanding and trusting the models, thus hampering their applicability. Whereas explaining GNNs remains a challenge, most existing methods fall into approximation based and perturbation based approaches with... | Fan Yang, Mengnan Du, Ninghao Liu, Qizhang Feng, Ruixiang Tang, Xia Hu |  |
| 63 |  |  [Improving Mutual Information Estimation with Annealed and Energy-Based Bounds](https://openreview.net/forum?id=T0B9AoM_bFg) |  | 0 | Mutual information (MI) is a fundamental quantity in information theory and machine learning. However, direct estimation of MI is intractable, even if the true joint probability density for the variables of interest is known, as it involves estimating a potentially high-dimensional log partition function. In this work, we present a unifying view of existing MI bounds... | Alireza Makhzani, Greg Ver Steeg, Marzyeh Ghassemi, Rob Brekelmans, Roger Baker Grosse, Sicong Huang |  |
| 64 |  |  [Sequence Approximation using Feedforward Spiking Neural Network for Spatiotemporal Learning: Theory and Optimization Methods](https://openreview.net/forum?id=bp-LJ4y_XC) |  | 0 | A dynamical system of spiking neurons with only feedforward connections can classify spatiotemporal patterns without recurrent connections. However, the theoretical construct of a feedforward spiking neural network (SNN) for approximating a temporal sequence remains unclear, making it challenging to optimize SNN architectures for learning complex spatiotemporal... | Saibal Mukhopadhyay, Saurabh Dash, Xueyuan She |  |
| 65 |  |  [Diverse Client Selection for Federated Learning via Submodular Maximization](https://openreview.net/forum?id=nwKXyFvaUm) |  | 0 | In every communication round of federated learning, a random subset of clients communicate their model updates back to the server which then aggregates them all. The optimal size of this subset is not known and several studies have shown that typically random selection does not perform very well in terms of convergence, learning efficiency and fairness. We, in this... | Jeff A. Bilmes, Nageen Himayat, Ravikumar Balakrishnan, Tian Li, Tianyi Zhou, Virginia Smith |  |
| 66 |  |  [From Intervention to Domain Transportation: A Novel Perspective to Optimize Recommendation](https://openreview.net/forum?id=jT1EwXu-4hj) |  | 0 | The interventional nature of recommendation has attracted increasing attention in recent years. It particularly motivates researchers to formulate learning and evaluating recommendation as causal inference and data missing-not-at-random problems. However, few take seriously the consequence of violating the critical assumption of overlapping, which we prove can... | Chuanwei Ruan, Da Xu, Evren Körpeoglu, Kannan Achan, Sushant Kumar, Yuting Ye |  |
| 67 |  |  [Variational Predictive Routing with Nested Subjective Timescales](https://openreview.net/forum?id=JxFgJbZ-wft) |  | 0 | Discovery and learning of an underlying spatiotemporal hierarchy in sequential data is an important topic for machine learning. Despite this, little work has been done to explore hierarchical generative models that can flexibly adapt their layerwise representations in response to datasets with different temporal dynamics. Here, we present Variational Predictive Routing... | Alexey Zakharov, Qinghai Guo, Zafeirios Fountas |  |
| 68 |  |  [Sample and Computation Redistribution for Efficient Face Detection](https://openreview.net/forum?id=RhB1AdoFfGE) |  | 0 | Although tremendous strides have been made in uncontrolled face detection, accurate face detection with a low computation cost remains an open challenge. In this paper, we point out that computation distribution and scale augmentation are the keys to detecting small faces from low-resolution images. Motivated by these observations, we introduce two simple but effective... | Alexandros Lattas, Jia Guo, Jiankang Deng, Stefanos Zafeiriou |  |
| 69 |  |  [Sound Adversarial Audio-Visual Navigation](https://openreview.net/forum?id=NkZq4OEYN-) |  | 0 | Audio-visual navigation task requires an agent to find a sound source in a realistic, unmapped 3D environment by utilizing egocentric audio-visual observations. Existing audio-visual navigation works assume a clean environment that solely contains the target sound, which, however, would not be suitable in most real-world applications due to the unexpected sound noise or... | Changan Chen, Fuchun Sun, Wenbing Huang, Xiaohong Liu, Yikai Wang, Yinfeng Yu |  |
| 70 |  |  [Out-of-distribution Generalization in the Presence of Nuisance-Induced Spurious Correlations](https://openreview.net/forum?id=12RoR2o32T) |  | 0 | In many prediction problems, spurious correlations are induced by a changing relationship between the label and a nuisance variable that is also correlated with the covariates. For example, in classifying animals in natural images, the background, which is a nuisance, can predict the type of animal. This nuisance-label relationship does not always hold, and the... | Aahlad Manas Puli, Eric Karl Oermann, Lily H. Zhang, Rajesh Ranganath |  |
| 71 |  |  [AEVA: Black-box Backdoor Detection Using Adversarial Extreme Value Analysis](https://openreview.net/forum?id=OM_lYiHXiCL) |  | 0 | Deep neural networks (DNNs) are proved to be vulnerable against backdoor attacks. A backdoor could be embedded in the target DNNs through injecting a backdoor trigger into the training examples, which can cause the target DNNs misclassify an input attached with the backdoor trigger. Recent backdoor detection methods often require the access to the original poisoned... | Ang Li, Cong Liu, Junfeng Guo |  |
| 72 |  |  [Resonance in Weight Space: Covariate Shift Can Drive Divergence of SGD with Momentum](https://openreview.net/forum?id=5ECQL05ub0J) |  | 0 | Most convergence guarantees for stochastic gradient descent with momentum (SGDm) rely on iid sampling. Yet, SGDm is often used outside this regime, in settings with temporally correlated input samples such as continual learning and reinforcement learning. Existing work has shown that SGDm with a decaying step-size can converge under Markovian temporal correlation. In... | Alona Fyshe, Kirby Banman, Liam PeetPare, Martha White, Nidhi Hegde |  |
| 73 |  |  [Top-label calibration and multiclass-to-binary reductions](https://openreview.net/forum?id=WqoBaaPHS-) |  | 0 | We propose a new notion of multiclass calibration called top-label calibration. A classifier is said to be top-label calibrated if the reported probability for the predicted class label---the top-label---is calibrated, conditioned on the top-label. This conditioning is essential for practical utility of the calibration property, since the top-label is always reported... | Aaditya Ramdas, Chirag Gupta |  |
| 74 |  |  [Anisotropic Random Feature Regression in High Dimensions](https://openreview.net/forum?id=JfaWawZ8BmX) |  | 0 | In contrast to standard statistical wisdom, modern learning algorithms typically find their best performance in the overparameterized regime in which the model has many more parameters than needed to fit the training data. A growing number of recent works have shown that random feature models can offer a detailed theoretical explanation for this unexpected behavior, but... | Gabriel Mel, Jeffrey Pennington |  |
| 75 |  |  [Back2Future: Leveraging Backfill Dynamics for Improving Real-time Predictions in Future](https://openreview.net/forum?id=L01Nn_VJ9i) |  | 0 | For real-time forecasting in domains like public health and macroeconomics, data collection is a non-trivial and demanding task. Often after being initially released, it undergoes several revisions later (maybe due to human or technical constraints) - as a result, it may take weeks until the data reaches a stable value. This so-called ‘backfill’ phenomenon and its... | Alexander Rodríguez, B. Aditya Prakash, Harshavardhan Kamarthi |  |
| 76 |  |  [Approximation and Learning with Deep Convolutional Models: a Kernel Perspective](https://openreview.net/forum?id=lrocYB-0ST2) |  | 0 | The empirical success of deep convolutional networks on tasks involving high-dimensional data such as images or audio suggests that they can efficiently approximate certain functions that are well-suited for such tasks. In this paper, we study this through the lens of kernel methods, by considering simple hierarchical kernels with two or three convolution and pooling... | Alberto Bietti |  |
| 77 |  |  [Value Function Spaces: Skill-Centric State Abstractions for Long-Horizon Reasoning](https://openreview.net/forum?id=vgqS1vkkCbE) |  | 0 | Reinforcement learning can train policies that effectively perform complex tasks. However for long-horizon tasks, the performance of these methods degrades with horizon, often necessitating reasoning over and chaining lower-level skills. Hierarchical reinforcement learning aims to enable this by providing a bank of low-level skills as action abstractions. Hierarchies... | Alexander Toshev, Brian Ichter, Dhruv Shah, Peng Xu, Sergey Levine, Ted Xiao, Yao Lu |  |
| 78 |  |  [Fast Regression for Structured Inputs](https://openreview.net/forum?id=gNp54NxHUPJ) |  | 0 | We study the $\ell_p$ regression problem, which requires finding $\mathbf{x}\in\mathbb R^{d}$ that minimizes $\\|\mathbf{A}\mathbf{x}-\mathbf{b}\\|_p$ for a matrix $\mathbf{A}\in\mathbb R^{n \times d}$ and response vector $\mathbf{b}\in\mathbb R^{n}$. There has been recent interest in developing subsampling methods for this problem that can outperform standard... | Cameron Musco, Christopher Musco, David P. Woodruff, Raphael A. Meyer, Samson Zhou |  |
| 79 |  |  [CrossBeam: Learning to Search in Bottom-Up Program Synthesis](https://openreview.net/forum?id=qhC8mr2LEKq) |  | 0 | Many approaches to program synthesis perform a search within an enormous space of programs to find one that satisfies a given specification. Prior works have used neural models to guide combinatorial search algorithms, but such approaches still explore a huge portion of the search space and quickly become intractable as the size of the desired program increases. To tame... | Charles Sutton, Hanjun Dai, Kensen Shi, Kevin Ellis |  |
| 80 |  |  [PEARL: Data Synthesis via Private Embeddings and Adversarial Reconstruction Learning](https://openreview.net/forum?id=M6M8BEmd6dq) |  | 0 | We propose a new framework of synthesizing data using deep generative models in a differentially private manner. Within our framework, sensitive data are sanitized with rigorous privacy guarantees in a one-shot fashion, such that training deep generative models is possible without re-using the original data. Hence, no extra privacy costs or model constraints are... | Michihiko Ueno, Seng Pei Liew, Tsubasa Takahashi |  |
| 81 |  |  [Divisive Feature Normalization Improves Image Recognition Performance in AlexNet](https://openreview.net/forum?id=aOX3a9q3RVV) |  | 0 | Local divisive normalization provides a phenomenological description of many nonlinear response properties of neurons across visual cortical areas. To gain insight into the utility of this operation, we studied the effects on AlexNet of a local divisive normalization between features, with learned parameters. Developing features were arranged in a line topology, with... | Kenneth D. Miller, Michelle Miller, SueYeon Chung |  |
| 82 |  |  [Evaluating Distributional Distortion in Neural Language Modeling](https://openreview.net/forum?id=bTteFbU99ye) |  | 0 | A fundamental characteristic of natural language is the high rate at which speakers produce novel expressions. Because of this novelty, a heavy-tail of rare events accounts for a significant amount of the total probability mass of distributions in language (Baayen, 2001). Standard language modeling metrics such as perplexity quantify the performance of language models... | Alessandro Sordoni, Benjamin LeBrun, Timothy J. O'Donnell |  |
| 83 |  |  [MaGNET: Uniform Sampling from Deep Generative Network Manifolds Without Retraining](https://openreview.net/forum?id=r5qumLiYwf9) |  | 0 | Deep Generative Networks (DGNs) are extensively employed in Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and their variants to approximate the data manifold, and data distribution on that manifold. However, training samples are often obtained based on preferences, costs, or convenience producing artifacts in the empirical data distribution... | Ahmed Imtiaz Humayun, Randall Balestriero, Richard G. Baraniuk |  |
| 84 |  |  [Neural Contextual Bandits with Deep Representation and Shallow Exploration](https://openreview.net/forum?id=xnYACQquaGV) |  | 0 | We study neural contextual bandits, a general class of contextual bandits, where each context-action pair is associated with a raw feature vector, but the specific reward generating function is unknown. We propose a novel learning algorithm that transforms the raw feature vector using the last hidden layer of a deep ReLU neural network (deep representation learning),... | Handong Zhao, Pan Xu, Quanquan Gu, Zheng Wen |  |
| 85 |  |  [PI3NN: Out-of-distribution-aware Prediction Intervals from Three Neural Networks](https://openreview.net/forum?id=NoB8YgRuoFU) |  | 0 | We propose a novel prediction interval (PI) method for uncertainty quantification, which addresses three major issues with the state-of-the-art PI methods. First, existing PI methods require retraining of neural networks (NNs) for every given confidence level and suffer from the crossing issue in calculating multiple PIs. Second, they usually rely on customized loss... | Dan Lu, Guannan Zhang, Pei Zhang, Siyan Liu |  |
| 86 |  |  [Discriminative Similarity for Data Clustering](https://openreview.net/forum?id=kj0_45Y4r9i) |  | 0 | Similarity-based clustering methods separate data into clusters according to the pairwise similarity between the data, and the pairwise similarity is crucial for their performance. In this paper, we propose {\em Clustering by Discriminative Similarity (CDS)}, a novel method which learns discriminative similarity for data clustering. CDS learns an unsupervised... | Ping Li, Yingzhen Yang |  |
| 87 |  |  [It Takes Four to Tango: Multiagent Self Play for Automatic Curriculum Generation](https://openreview.net/forum?id=q4tZR1Y-UIs) |  | 0 | We are interested in training general-purpose reinforcement learning agents that can solve a wide variety of goals. Training such agents efficiently requires automatic generation of a goal curriculum. This is challenging as it requires (a) exploring goals of increasing difficulty, while ensuring that the agent (b) is exposed to a diverse set of goals in a sample... | Aditya Grover, Pieter Abbeel, Yuqing Du |  |
| 88 |  |  [CROP: Certifying Robust Policies for Reinforcement Learning through Functional Smoothing](https://openreview.net/forum?id=HOjLHrlZhmx) |  | 0 | As reinforcement learning (RL) has achieved great success and been even adopted in safety-critical domains such as autonomous vehicles, a range of empirical studies have been conducted to improve its robustness against adversarial attacks. However, how to certify its robustness with theoretical guarantees still remains challenging. In this paper, we present the ﬁrst... | Bo Li, Ding Zhao, Fan Wu, Linyi Li, Yevgeniy Vorobeychik, Zijian Huang |  |
| 89 |  |  [Neural Link Prediction with Walk Pooling](https://openreview.net/forum?id=CCu6RcUMwK0) |  | 0 | Graph neural networks achieve high accuracy in link prediction by jointly leveraging graph topology and node attributes. Topology, however, is represented indirectly; state-of-the-art methods based on subgraph classification label nodes with distance to the target link, so that, although topological information is present, it is tempered by pooling. This makes it... | Cheng Shi, Ivan Dokmanic, Liming Pan |  |
| 90 |  |  [On the Convergence of Certified Robust Training with Interval Bound Propagation](https://openreview.net/forum?id=YeShU5mLfLt) |  | 0 | Interval Bound Propagation (IBP) is so far the base of state-of-the-art methods for training neural networks with certifiable robustness guarantees when potential adversarial perturbations present, while the convergence of IBP training remains unknown in existing literature. In this paper, we present a theoretical analysis on the convergence of IBP training. With an... | ChoJui Hsieh, Quanquan Gu, Yihan Wang, Zhouxing Shi |  |
| 91 |  |  [Pretraining Text Encoders with Adversarial Mixture of Training Signal Generators](https://openreview.net/forum?id=sX3XaHwotOg) |  | 0 | We present a new framework AMOS that pretrains text encoders with an Adversarial learning curriculum via a Mixture Of Signals from multiple auxiliary generators. Following ELECTRA-style pretraining, the main encoder is trained as a discriminator to detect replaced tokens generated by auxiliary masked language models (MLMs). Different from ELECTRA which trains one MLM as... | Chenyan Xiong, Jiawei Han, Paul N. Bennett, Payal Bajaj, Saurabh Tiwary, Xia Song, Yu Meng |  |
| 92 |  |  [Towards Training Billion Parameter Graph Neural Networks for Atomic Simulations](https://openreview.net/forum?id=0jP2n0YFmKG) |  | 0 | Recent progress in Graph Neural Networks (GNNs) for modeling atomic simulations has the potential to revolutionize catalyst discovery, which is a key step in making progress towards the energy breakthroughs needed to combat climate change. However, the GNNs that have proven most effective for this task are memory intensive as they model higher-order interactions in the... | Abhishek Das, Anuroop Sriram, Brandon M. Wood, C. Lawrence Zitnick, Siddharth Goyal |  |
| 93 |  |  [Understanding and Leveraging Overparameterization in Recursive Value Estimation](https://openreview.net/forum?id=shbAgEsk3qM) |  | 0 | The theory of function approximation in reinforcement learning (RL) typically considers low capacity representations that incur a tradeoff between approximation error, stability and generalization. Current deep architectures, however, operate in an overparameterized regime where approximation error is not necessarily a bottleneck. To better understand the utility of... | Bo Dai, Chenjun Xiao, Chris Harris, Dale Schuurmans, Jincheng Mei, Oscar A. Ramirez, Ramki Gummadi |  |
| 94 |  |  [Optimization and Adaptive Generalization of Three layer Neural Networks](https://openreview.net/forum?id=dPyRNUlttBv) |  | 0 | While there has been substantial recent work studying generalization of neural networks, the ability of deep nets in automating the process of feature extraction still evades a thorough mathematical understanding. As a step toward this goal, we analyze learning and generalization of a three-layer neural network with ReLU activations in a regime that goes beyond the... | Jonathan A. Kelner, Khashayar Gatmiry, Stefanie Jegelka |  |
| 95 |  |  [Non-Parallel Text Style Transfer with Self-Parallel Supervision](https://openreview.net/forum?id=-TSe5o7STVR) |  | 0 | The performance of existing text style transfer models is severely limited by the non-parallel datasets on which the models are trained. In non-parallel datasets, no direct mapping exists between sentences of the source and target style; the style transfer models thus only receive weak supervision of the target sentences during training, which often leads the model to... | Chenyan Jia, Chongyang Gao, Guangxuan Xu, Ruibo Liu, Soroush Vosoughi |  |
| 96 |  |  [Can an Image Classifier Suffice For Action Recognition?](https://openreview.net/forum?id=qhkFX-HLuHV) |  | 0 | We explore a new perspective on video understanding by casting the video recognition problem as an image recognition task. Our approach rearranges input video frames into super images, which allow for training an image classifier directly to fulfill the task of action recognition, in exactly the same way as image classification. With such a simple idea, we show that... | ChunFu Chen, Quanfu Fan, Rameswar Panda |  |
| 97 |  |  [Interacting Contour Stochastic Gradient Langevin Dynamics](https://openreview.net/forum?id=IK9ap6nxXr2) |  | 0 | We propose an interacting contour stochastic gradient Langevin dynamics (ICSGLD) sampler, an embarrassingly parallel multiple-chain contour stochastic gradient Langevin dynamics (CSGLD) sampler with efficient interactions. We show that ICSGLD can be theoretically more efficient than a single-chain CSGLD with an equivalent computational budget. We also present a novel... | Botao Hao, Faming Liang, Guang Lin, Siqi Liang, Wei Deng |  |
| 98 |  |  [NeuPL: Neural Population Learning](https://openreview.net/forum?id=MIX3fJkl_1) |  | 0 | Learning in strategy games (e.g. StarCraft, poker) requires the discovery of diverse policies. This is often achieved by iteratively training new policies against existing ones, growing a policy population that is robust to exploit. This iterative approach suffers from two issues in real-world games: a) under finite budget, approximate best-response operators at each... | Daniel Hennes, Josh Merel, Luke Marris, Nicolas Heess, Siqi Liu, Thore Graepel |  |
| 99 |  |  [DeSKO: Stability-Assured Robust Control with a Deep Stochastic Koopman Operator](https://openreview.net/forum?id=hniLRD_XCA) |  | 0 | The Koopman operator theory linearly describes nonlinear dynamical systems in a high-dimensional functional space and it allows to apply linear control methods to highly nonlinear systems. However, the Koopman operator does not account for any uncertainty in dynamical systems, causing it to perform poorly in real-world applications. Therefore, we propose a deep... | Jacob EulerRolle, Minghao Han, Robert K. Katzschmann |  |
| 100 |  |  [Neural Network Approximation based on Hausdorff distance of Tropical Zonotopes](https://openreview.net/forum?id=oiZJwC_fyS) |  | 0 | In this work we theoretically contribute to neural network approximation by providing a novel tropical geometrical viewpoint to structured neural network compression. In particular, we show that the approximation error between two neural networks with ReLU activations and one hidden layer depends on the Hausdorff distance of the tropical zonotopes of the networks. This... | George Retsinas, Georgios Smyrnis, Panagiotis Misiakos, Petros Maragos |  |
| 101 |  |  [Learning Towards The Largest Margins](https://openreview.net/forum?id=hqkhcFHOeKD) |  | 0 | One of the main challenges for feature representation in deep learning-based classification is the design of appropriate loss functions that exhibit strong discriminative power. The classical softmax loss does not explicitly encourage discriminative learning of features. A popular direction of research is to incorporate margins in well-established losses in order to... | Deming Zhai, Junjun Jiang, Xiangyang Ji, Xianming Liu, Xin Gao, Xiong Zhou |  |
| 102 |  |  [Patch-Fool: Are Vision Transformers Always Robust Against Adversarial Perturbations?](https://openreview.net/forum?id=28ib9tf6zhr) |  | 0 | Vision transformers (ViTs) have recently set off a new wave in neural architecture design thanks to their record-breaking performance in various vision tasks. In parallel, to fulfill the goal of deploying ViTs into real-world vision applications, their robustness against potential malicious attacks has gained increasing attention. In particular, recent works show that... | Cheng Wan, Shang Wu, Shunyao Zhang, Yingyan Lin, Yonggan Fu |  |
| 103 |  |  [AdaMatch: A Unified Approach to Semi-Supervised Learning and Domain Adaptation](https://openreview.net/forum?id=Q5uh1Nvv5dm) |  | 0 | We extend semi-supervised learning to the problem of domain adaptation to learn significantly higher-accuracy models that train on one data distribution and test on a different one. With the goal of generality, we introduce AdaMatch, a unified solution for unsupervised domain adaptation (UDA), semi-supervised learning (SSL), and semi-supervised domain adaptation (SSDA).... | Alexey Kurakin, David Berthelot, Kihyuk Sohn, Nicholas Carlini, Rebecca Roelofs |  |
| 104 |  |  [Complete Verification via Multi-Neuron Relaxation Guided Branch-and-Bound](https://openreview.net/forum?id=l_amHf1oaK) |  | 0 | State-of-the-art neural network verifiers are fundamentally based on one of two paradigms: either encoding the whole verification problem via tight multi-neuron convex relaxations or applying a Branch-and-Bound (BaB) procedure leveraging imprecise but fast bounding methods on a large number of easier subproblems. The former can capture complex multi-neuron dependencies... | Claudio Ferrari, Mark Niklas Müller, Martin T. Vechev, Nikola Jovanovic |  |
| 105 |  |  [Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality](https://openreview.net/forum?id=VFBjuF8HEp) |  | 0 | Diffusion models have emerged as an expressive family of generative models rivaling GANs in sample quality and autoregressive models in likelihood scores. Standard diffusion models typically require hundreds of forward passes through the model to generate a single high-fidelity sample. We introduce Differentiable Diffusion Sampler Search (DDSS): a method that optimizes... | Daniel Watson, Jonathan Ho, Mohammad Norouzi, William Chan |  |
| 106 |  |  [Distribution Compression in Near-Linear Time](https://openreview.net/forum?id=lzupY5zjaU9) |  | 0 | In distribution compression, one aims to accurately summarize a probability distribution $\mathbb{P}$ using a small number of representative points. Near-optimal thinning procedures achieve this goal by sampling $n$ points from a Markov chain and identifying $\sqrt{n}$ points with $\widetilde{\mathcal{O}}(1/\sqrt{n})$ discrepancy to $\mathbb{P}$. Unfortunately, these... | Abhishek Shetty, Lester Mackey, Raaz Dwivedi |  |
| 107 |  |  [Capturing Structural Locality in Non-parametric Language Models](https://openreview.net/forum?id=nnU3IUMJmN) |  | 0 | Structural locality is a ubiquitous feature of real-world datasets, wherein data points are organized into local hierarchies. Some examples include topical clusters in text or project hierarchies in source code repositories. In this paper, we explore utilizing this structural locality within non-parametric language models, which generate sequences that reference... | Frank F. Xu, Graham Neubig, Junxian He, Vincent Josua Hellendoorn |  |
| 108 |  |  [Audio Lottery: Speech Recognition Made Ultra-Lightweight, Noise-Robust, and Transferable](https://openreview.net/forum?id=9Nk6AJkVYB) |  | 0 | Lightweight speech recognition models have seen explosive demands owing to a growing amount of speech-interactive features on mobile devices. Since designing such systems from scratch is non-trivial, practitioners typically choose to compress large (pre-trained) speech models. Recently, lottery ticket hypothesis reveals the existence of highly sparse subnetworks that... | Shaojin Ding, Tianlong Chen, Zhangyang Wang |  |
| 109 |  |  [Learning to Map for Active Semantic Goal Navigation](https://openreview.net/forum?id=swrMQttr6wN) |  | 0 | We consider the problem of object goal navigation in unseen environments. Solving this problem requires learning of contextual semantic priors, a challenging endeavour given the spatial and semantic variability of indoor environments. Current methods learn to implicitly encode these priors through goal-oriented navigation policy functions operating on spatial... | Bernadette Bucher, Georgios Georgakis, Karl Schmeckpeper, Kostas Daniilidis, Siddharth Singh |  |
| 110 |  |  [Benchmarking the Spectrum of Agent Capabilities](https://openreview.net/forum?id=1W0z96MFEoH) |  | 0 | Evaluating the general abilities of intelligent agents requires complex simulation environments. Existing benchmarks typically evaluate only one narrow task per environment, requiring researchers to perform expensive training runs on many different environments. We introduce Crafter, an open world survival game with visual inputs that evaluates a wide range of general... | Danijar Hafner |  |
| 111 |  |  [Mind the Gap: Domain Gap Control for Single Shot Domain Adaptation for Generative Adversarial Networks](https://openreview.net/forum?id=vqGi8Kp0wM) |  | 0 | We present a new method for one shot domain adaptation. The input to our method is trained GAN that can produce images in domain A and a single reference image I_B from domain B. The proposed algorithm can translate any output of the trained GAN from domain A to domain B. There are two main advantages of our method compared to the current state of the art: First, our... | John Femiani, Peihao Zhu, Peter Wonka, Rameen Abdal |  |
| 112 |  |  [On Evaluation Metrics for Graph Generative Models](https://openreview.net/forum?id=EnwCZixjSh) |  | 0 | In image generation, generative models can be evaluated naturally by visually inspecting model outputs. However, this is not always the case for graph generative models (GGMs), making their evaluation challenging. Currently, the standard process for evaluating GGMs suffers from three critical limitations: i) it does not produce a single score which makes model selection... | Boris Knyazev, Elahe Ghalebi, Graham W. Taylor, Jungtaek Kim, Rylee Thompson |  |
| 113 |  |  [Selective Ensembles for Consistent Predictions](https://openreview.net/forum?id=HfUyCRBeQc) |  | 0 | Recent work has shown that models trained to the same objective, and which achieve similar measures of accuracy on consistent test data, may nonetheless behave very differently on individual predictions. This inconsistency is undesirable in high-stakes contexts, such as medical diagnosis and finance. We show that this duplicitous behavior extends beyond predictions to... | Emily Black, Klas Leino, Matt Fredrikson |  |
| 114 |  |  [Graph Condensation for Graph Neural Networks](https://openreview.net/forum?id=WLEx3Jo4QaB) |  | 0 | Given the prevalence of large-scale graphs in real-world applications, the storage and time for training neural models have raised increasing concerns. To alleviate the concerns, we propose and study the problem of graph condensation for graph neural networks (GNNs). Specifically, we aim to condense the large, original graph into a small, synthetic and... | Jiliang Tang, Lingxiao Zhao, Neil Shah, Shichang Zhang, Wei Jin, Yozen Liu |  |
| 115 |  |  [DIVA: Dataset Derivative of a Learning Task](https://openreview.net/forum?id=bVvMOtLMiw) |  | 0 | We present a method to compute the derivative of a learning task with respect to a dataset. A learning task is a function from a training set to the validation error, which can be represented by a trained deep neural network (DNN). The \`\`dataset derivative'' is a linear operator, computed around the trained model, that informs how perturbations of the weight of each... | Alessandro Achille, Avinash Ravichandran, Giovanni Paolini, Marzia Polito, Stefano Soatto, Yonatan Dukler |  |
| 116 |  |  [Towards General Function Approximation in Zero-Sum Markov Games](https://openreview.net/forum?id=sA4qIu3zv6v) |  | 0 | This paper considers two-player zero-sum finite-horizon Markov games with simultaneous moves. The study focuses on the challenging settings where the value function or the model is parameterized by general function classes. Provably efficient algorithms for both decoupled and coordinated settings are developed. In the decoupled setting where the agent controls a single... | Baihe Huang, Jason D. Lee, Zhaoran Wang, Zhuoran Yang |  |
| 117 |  |  [Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis--Hastings](https://openreview.net/forum?id=6PvWo1kEvlT) |  | 0 | While recent work has shown that scores from models trained by the ubiquitous masked language modeling (MLM) objective effectively discriminate probable from improbable sequences, it is still an open question if these MLMs specify a principled probability distribution over the space of possible sequences. In this paper, we interpret MLMs as energy-based sequence models... | Chris Dyer, Kartik Goyal, Taylor BergKirkpatrick |  |
| 118 |  |  [ClimateGAN: Raising Climate Change Awareness by Generating Images of Floods](https://openreview.net/forum?id=EZNOb_uNpJk) |  | 0 | Climate change is a major threat to humanity and the actions required to prevent its catastrophic consequences include changes in both policy-making and individual behaviour. However, taking action requires understanding its seemingly abstract and distant consequences. Projecting the potential impacts of extreme climate events such as flooding in familiar places can... | Adrien Juraver, Alex HernándezGarcía, Alexandra Luccioni, Alexia Reynaud, Gautier Cosne, Mélisande Teng, Sunand Raghupathi, Tianyu Zhang, Vahe Vardanyan, Victor Schmidt, Yoshua Bengio |  |
| 119 |  |  [A Comparison of Hamming Errors of Representative Variable Selection Methods](https://openreview.net/forum?id=nhN-fqxmNGx) |  | 0 | Lasso is a celebrated method for variable selection in linear models, but it faces challenges when the covariates are moderately or strongly correlated. This motivates alternative approaches such as using a non-convex penalty, adding a ridge regularization, or conducting a post-Lasso thresholding. In this paper, we compare Lasso with 5 other methods: Elastic net, SCAD,... | Longlin Wang, Tracy Ke |  |
| 120 |  |  [A Program to Build E(N)-Equivariant Steerable CNNs](https://openreview.net/forum?id=WE4qe9xlnQw) |  | 0 | Equivariance is becoming an increasingly popular design choice to build data efficient neural networks by exploiting prior knowledge about the symmetries of the problem at hand. Euclidean steerable CNNs are one of the most common classes of equivariant networks. While the constraints these architectures need to satisfy are understood, existing approaches are tailored to... | Gabriele Cesa, Leon Lang, Maurice Weiler |  |
| 121 |  |  [Minimax Optimization with Smooth Algorithmic Adversaries](https://openreview.net/forum?id=UdxJ2fJx7N0) |  | 0 | This paper considers minimax optimization $\min_x \max_y f(x, y)$ in the challenging setting where $f$ can be both nonconvex in $x$ and nonconcave in $y$. Though such optimization problems arise in many machine learning paradigms including training generative adversarial networks (GANs) and adversarially robust models, from a theoretical point of view, two fundamental... | Chi Jin, Lillian J. Ratliff, Praneeth Netrapalli, Tanner Fiez |  |
| 122 |  |  [On Distributed Adaptive Optimization with Gradient Compression](https://openreview.net/forum?id=CI-xXX9dg9l) |  | 0 | We study COMP-AMS, a distributed optimization framework based on gradient averaging and adaptive AMSGrad algorithm. Gradient compression with error feedback is applied to reduce the communication cost in the gradient transmission process. Our convergence analysis of COMP-AMS shows that such compressed gradient averaging strategy yields same convergence rate as standard... | Belhal Karimi, Ping Li, Xiaoyun Li |  |
| 123 |  |  [Leveraging unlabeled data to predict out-of-distribution performance](https://openreview.net/forum?id=o_HsiMPYh_x) |  | 0 | Real-world machine learning deployments are characterized by mismatches between the source (training) and target (test) distributions that may cause performance drops. In this work, we investigate methods for predicting the target domain accuracy using only labeled source data and unlabeled target data. We propose Average Thresholded Confidence (ATC), a practical method... | Behnam Neyshabur, Hanie Sedghi, Saurabh Garg, Sivaraman Balakrishnan, Zachary Chase Lipton |  |
| 124 |  |  [VC dimension of partially quantized neural networks in the overparametrized regime](https://openreview.net/forum?id=7udZAsEzd60) |  | 0 | Vapnik-Chervonenkis (VC) theory has so far been unable to explain the small generalization error of overparametrized neural networks. Indeed, existing applications of VC theory to large networks obtain upper bounds on VC dimension that are proportional to the number of weights, and for a large class of networks, these upper bound are known to be tight. In this work, we... | Clayton Scott, Yutong Wang |  |
| 125 |  |  [Optimal Representations for Covariate Shift](https://openreview.net/forum?id=Rf58LPCwJj0) |  | 0 | Machine learning systems often experience a distribution shift between training and testing. In this paper, we introduce a simple variational objective whose optima are exactly the set of all representations on which risk minimizers are guaranteed to be robust to any distribution shift that preserves the Bayes predictor, e.g., covariate shifts. Our objective has two... | Chris J. Maddison, Yangjun Ruan, Yann Dubois |  |
| 126 |  |  [Fortuitous Forgetting in Connectionist Networks](https://openreview.net/forum?id=ei3SY1_zYsE) |  | 0 | Forgetting is often seen as an unwanted characteristic in both human and machine learning. However, we propose that forgetting can in fact be favorable to learning. We introduce forget-and-relearn as a powerful paradigm for shaping the learning trajectories of artificial neural networks. In this process, the forgetting step selectively removes undesirable information... | Aaron C. Courville, Ankit Vani, Hattie Zhou, Hugo Larochelle |  |
| 127 |  |  [EigenGame Unloaded: When playing games is better than optimizing](https://openreview.net/forum?id=So6YAqnqgMj) |  | 0 | We build on the recently proposed EigenGame that views eigendecomposition as a competitive game. EigenGame's updates are biased if computed using minibatches of data, which hinders convergence and more sophisticated parallelism in the stochastic setting. In this work, we propose an unbiased stochastic update that is asymptotically equivalent to EigenGame, enjoys greater... | Brian McWilliams, Claire Vernade, Ian Gemp, Thore Graepel |  |
| 128 |  |  [Contextualized Scene Imagination for Generative Commonsense Reasoning](https://openreview.net/forum?id=Oh1r2wApbPv) |  | 0 | Humans use natural language to compose common concepts from their environment into plausible, day-to-day scene descriptions. However, such generative commonsense reasoning (GCSR) skills are lacking in state-of-the-art text generation methods. Descriptive sentences about arbitrary concepts generated by neural text generation models (e.g., pre-trained text-to-text... | Filip Ilievski, Jonathan Zamora, Junfeng Liu, Muhao Chen, Peifeng Wang, Xiang Ren |  |
| 129 |  |  [Scene Transformer: A unified architecture for predicting future trajectories of multiple agents](https://openreview.net/forum?id=Wm3EA5OlHsG) |  | 0 | Predicting the motion of multiple agents is necessary for planning in dynamic environments. This task is challenging for autonomous driving since agents (e.g., vehicles and pedestrians) and their associated behaviors may be diverse and influence one another. Most prior work have focused on predicting independent futures for each agent based on all past motion, and... | Alex Bewley, Ashish Venugopal, Ben Sapp, Benjamin Caine, Chenxi Liu, David J. Weiss, HaoTien Lewis Chiang, Jeffrey Ling, Jiquan Ngiam, Rebecca Roelofs, Vijay Vasudevan, Zhengdong Zhang, Zhifeng Chen |  |
| 130 |  |  [DISSECT: Disentangled Simultaneous Explanations via Concept Traversals](https://openreview.net/forum?id=qY79G8jGsep) |  | 0 | Explaining deep learning model inferences is a promising venue for scientific understanding, improving safety, uncovering hidden biases, evaluating fairness, and beyond, as argued by many scholars. One of the principal benefits of counterfactual explanations is allowing users to explore "what-if" scenarios through what does not and cannot exist in the data, a quality... | Asma Ghandeharioun, Been Kim, Brendan Jou, Brian Eoff, ChunLiang Li, Rosalind W. Picard |  |
| 131 |  |  [Heteroscedastic Temporal Variational Autoencoder For Irregularly Sampled Time Series](https://openreview.net/forum?id=Az7opqbQE-3) |  | 0 | Irregularly sampled time series commonly occur in several domains where they present a significant challenge to standard deep learning models. In this paper, we propose a new deep learning framework for probabilistic interpolation of irregularly sampled time series that we call the Heteroscedastic Temporal Variational Autoencoder (HeTVAE). HeTVAE includes a novel input... | Benjamin M. Marlin, Satya Narayan Shukla |  |
| 132 |  |  [A Neural Tangent Kernel Perspective of Infinite Tree Ensembles](https://openreview.net/forum?id=vUH85MOXO7h) |  | 0 | In practical situations, the tree ensemble is one of the most popular models along with neural networks. A soft tree is a variant of a decision tree. Instead of using a greedy method for searching splitting rules, the soft tree is trained using a gradient method in which the entire splitting operation is formulated in a differentiable form. Although ensembles of such... | Mahito Sugiyama, Ryuichi Kanoh |  |
| 133 |  |  [AlphaZero-based Proof Cost Network to Aid Game Solving](https://openreview.net/forum?id=nKWjE4QF1hB) |  | 0 | The AlphaZero algorithm learns and plays games without hand-crafted expert knowledge. However, since its objective is to play well, we hypothesize that a better objective can be defined for the related but separate task of solving games. This paper proposes a novel approach to solving problems by modifying the training target of the AlphaZero algorithm, such that it... | ChungChin Shih, IChen Wu, MengYu Tsai, TiRong Wu, TingHan Wei, WeiYuan Hsu |  |
| 134 |  |  [Bayesian Framework for Gradient Leakage](https://openreview.net/forum?id=f2lrIbGx3x7) |  | 0 | Federated learning is an established method for training machine learning models without sharing training data. However, recent work has shown that it cannot guarantee data privacy as shared gradients can still leak sensitive information. To formalize the problem of gradient leakage, we propose a theoretical framework that enables, for the first time, analysis of the... | Dimitar Iliev Dimitrov, Martin T. Vechev, Mislav Balunovic, Robin Staab |  |
| 135 |  |  [Universalizing Weak Supervision](https://openreview.net/forum?id=YpPiNigTzMT) |  | 0 | Weak supervision (WS) frameworks are a popular way to bypass hand-labeling large datasets for training data-hungry models. These approaches synthesize multiple noisy but cheaply-acquired estimates of labels into a set of high-quality pseudo-labels for downstream training. However, the synthesis technique is specific to a particular kind of label, such as binary labels... | Changho Shin, Frederic Sala, Harit Vishwakarma, Nicholas Carl Roberts, Winfred Li |  |
| 136 |  |  [Maximum n-times Coverage for Vaccine Design](https://openreview.net/forum?id=ULfq0qR25dY) |  | 0 | We introduce the maximum $n$-times coverage problem that selects $k$ overlays to maximize the summed coverage of weighted elements, where each element must be covered at least $n$ times. We also define the min-cost $n$-times coverage problem where the objective is to select the minimum set of overlays such that the sum of the weights of elements that are covered at... | Alexander Dimitrakakis, Brandon Carter, David K. Gifford, Ge Liu |  |
| 137 |  |  [KL Guided Domain Adaptation](https://openreview.net/forum?id=0JzqUlIVVDd) |  | 0 | Domain adaptation is an important problem and often needed for real-world applications. In this problem, instead of i.i.d. training and testing datapoints, we assume that the source (training) data and the target (testing) data have different distributions. With that setting, the empirical risk minimization training procedure often does not perform well, since it does... | A. Tuan Nguyen, Atilim Gunes Baydin, Philip H. S. Torr, Toan Tran, Yarin Gal |  |
| 138 |  |  [From Stars to Subgraphs: Uplifting Any GNN with Local Structure Awareness](https://openreview.net/forum?id=Mspk_WYKoEH) |  | 0 | Message Passing Neural Networks (MPNNs) are a common type of Graph Neural Network (GNN), in which each node’s representation is computed recursively by aggregating representations (“messages”) from its immediate neighbors akin to a star-shaped pattern. MPNNs are appealing for being efficient and scalable, however their expressiveness is upper-bounded by the 1st-order... | Leman Akoglu, Lingxiao Zhao, Neil Shah, Wei Jin |  |
| 139 |  |  [Network Insensitivity to Parameter Noise via Parameter Attack During Training](https://openreview.net/forum?id=-8sBpe7rDiV) |  | 0 | Neuromorphic neural network processors, in the form of compute-in-memory crossbar arrays of memristors, or in the form of subthreshold analog and mixed-signal ASICs, promise enormous advantages in compute density and energy efficiency for NN-based ML tasks. However, these technologies are prone to computational non-idealities, due to process variation and intrinsic... | Dylan Richard Muir, Fynn Firouz Faber, Julian Büchel |  |
| 140 |  |  [Gradient Importance Learning for Incomplete Observations](https://openreview.net/forum?id=fXHl76nO2AZ) |  | 0 | Though recent works have developed methods that can generate estimates (or imputations) of the missing entries in a dataset to facilitate downstream analysis, most depend on assumptions that may not align with real-world applications and could suffer from poor performance in subsequent tasks such as classification. This is particularly true if the data have large... | Chenyang Tao, Dong Wang, Joshua David Amason, Lawrence Carin, Majda Hadziahmetovic, Miroslav Pajic, Qitong Gao, Ricardo Henao, Siyang Yuan |  |
| 141 |  |  [Do Users Benefit From Interpretable Vision? A User Study, Baseline, And Dataset](https://openreview.net/forum?id=v6s3HVjPerv) |  | 0 | A variety of methods exist to explain image classification models. However, whether they provide any benefit to users over simply comparing various inputs and the model’s respective predictions remains unclear. We conducted a user study (N=240) to test how such a baseline explanation technique performs against concept-based and counterfactual explanations. To this end,... | Leon Sixt, Martin Schuessler, OanaIuliana Popescu, Philipp Weiß, Tim Landgraf |  |
| 142 |  |  [Understanding the Variance Collapse of SVGD in High Dimensions](https://openreview.net/forum?id=Qycd9j5Qp9J) |  | 0 | Stein variational gradient descent (SVGD) is a deterministic inference algorithm that evolves a set of particles to fit a target distribution. Despite its computational efficiency, SVGD often underestimates the variance of the target distribution in high dimensions. In this work we attempt to explain the variance collapse in SVGD. On the qualitative side, we compare the... | Denny Wu, Jimmy Ba, Marzyeh Ghassemi, Murat A. Erdogdu, Shengyang Sun, Taiji Suzuki, Tianzong Zhang |  |
| 143 |  |  [Generalisation in Lifelong Reinforcement Learning through Logical Composition](https://openreview.net/forum?id=ZOcX-eybqoL) |  | 0 | We leverage logical composition in reinforcement learning to create a framework that enables an agent to autonomously determine whether a new task can be immediately solved using its existing abilities, or whether a task-specific skill should be learned. In the latter case, the proposed algorithm also enables the agent to learn the new task faster by generating an... | Benjamin Rosman, Geraud Nangue Tasse, Steven James |  |
| 144 |  |  [PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions](https://openreview.net/forum?id=gSdSJoenupI) |  | 0 | Cross-entropy loss and focal loss are the most common choices when training deep neural networks for classification problems. Generally speaking, however, a good loss function can take on much more flexible forms, and should be tailored for different tasks and datasets. Motivated by how functions can be approximated via Taylor expansion, we propose a simple framework,... | Chenxi Liu, Dragomir Anguelov, Ekin Dogus Cubuk, Jay Shi, Mingxing Tan, Shuyang Cheng, Zhaoqi Leng |  |
| 145 |  |  [Improving Non-Autoregressive Translation Models Without Distillation](https://openreview.net/forum?id=I2Hw58KHp8O) |  | 0 | Transformer-based autoregressive (AR) machine translation models have achieved significant performance improvements, nearing human-level accuracy on some languages. The AR framework translates one token at a time which can be time consuming, especially for long sequences. To accelerate inference, recent work has been exploring non-autoregressive (NAR) approaches that... | Felipe Pérez, Maksims Volkovs, Xiao Shi Huang |  |
| 146 |  |  [A Theory of Tournament Representations](https://openreview.net/forum?id=zzk231Ms1Ih) |  | 0 | Real-world tournaments are almost always intransitive. Recent works have noted that parametric models which assume $d$ dimensional node representations can effectively model intransitive tournaments. However, nothing is known about the structure of the class of tournaments that arise out of any fixed $d$ dimensional representations. In this work, we develop a novel... | Abdul Bakey Mir, Arun Rajkumar, Vishnu Veerathu |  |
| 147 |  |  [Convergent and Efficient Deep Q Learning Algorithm](https://openreview.net/forum?id=OJm3HZuj4r7) |  | 0 | Despite the empirical success of the deep Q network (DQN) reinforcement learning algorithm and its variants, DQN is still not well understood and it does not guarantee convergence. In this work, we show that DQN can indeed diverge and cease to operate in realistic settings. Although there exist gradient-based convergent methods, we show that they actually have inherent... | Masahito Ueda, Zhikang T. Wang |  |
| 148 |  |  [Trigger Hunting with a Topological Prior for Trojan Detection](https://openreview.net/forum?id=TXsjU8BaibT) |  | 0 | Despite their success and popularity, deep neural networks (DNNs) are vulnerable when facing backdoor attacks. This impedes their wider adoption, especially in mission critical applications. This paper tackles the problem of Trojan detection, namely, identifying Trojaned models – models trained with poisoned data. One popular approach is reverse engineering, i.e.,... | Chao Chen, Michael Cogswell, Susmit Jha, Xiao Lin, Xiaoling Hu, Yi Yao |  |
| 149 |  |  [Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep RL](https://openreview.net/forum?id=JM2kFbJvvI) |  | 0 | Evaluating the worst-case performance of a reinforcement learning (RL) agent under the strongest/optimal adversarial perturbations on state observations (within some constraints) is crucial for understanding the robustness of RL agents. However, finding the optimal adversary is challenging, in terms of both whether we can find the optimal attack and how efficiently we... | Furong Huang, Ruijie Zheng, Yanchao Sun, Yongyuan Liang |  |
| 150 |  |  [Chunked Autoregressive GAN for Conditional Waveform Synthesis](https://openreview.net/forum?id=v3aeIsY_vVX) |  | 0 | Conditional waveform synthesis models learn a distribution of audio waveforms given conditioning such as text, mel-spectrograms, or MIDI. These systems employ deep generative models that model the waveform via either sequential (autoregressive) or parallel (non-autoregressive) sampling. Generative adversarial networks (GANs) have become a common choice for... | Aaron C. Courville, Kundan Kumar, Max Morrison, Prem Seetharaman, Rithesh Kumar, Yoshua Bengio |  |
| 151 |  |  [COPA: Certifying Robust Policies for Offline Reinforcement Learning against Poisoning Attacks](https://openreview.net/forum?id=psh0oeMSBiF) |  | 0 | As reinforcement learning (RL) has achieved near human-level performance in a variety of tasks, its robustness has raised great attention. While a vast body of research has explored test-time (evasion) attacks in RL and corresponding defenses, its robustness against training-time (poisoning) attacks remains largely unanswered. In this work, we focus on certifying the... | Bhavya Kailkhura, Bo Li, Ding Zhao, Fan Wu, Huan Zhang, Krishnaram Kenthapadi, Linyi Li |  |
| 152 |  |  [ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning](https://openreview.net/forum?id=Vzh1BFUCiIX) |  | 0 | Despite the recent success of multi-task learning and transfer learning for natural language processing (NLP), few works have systematically studied the effect of scaling up the number of tasks during pre-training. Towards this goal, this paper introduces ExMix (Extreme Mixture): a massive collection of 107 supervised NLP tasks across diverse domains and task-families.... | Dara Bahri, Donald Metzler, Honglei Zhuang, Huaixiu Steven Zheng, Jai Prakash Gupta, Jianmo Ni, Jinfeng Rao, Kai Hui, Sanket Vaibhav Mehta, Sebastian Ruder, Tal Schuster, Vamsi Aribandi, Vinh Q. Tran, Yi Tay |  |
| 153 |  |  [Provable Adaptation across Multiway Domains via Representation Learning](https://openreview.net/forum?id=gRCCdgpVZf) |  | 0 | This paper studies zero-shot domain adaptation where each domain is indexed on a multi-dimensional array, and we only have data from a small subset of domains. Our goal is to produce predictors that perform well on \emph{unseen} domains. We propose a model which consists of a domain-invariant latent representation layer and a domain-specific linear prediction layer with... | Shaobo Han, Simon Shaolei Du, Zhili Feng |  |
| 154 |  |  [Efficient Token Mixing for Transformers via Adaptive Fourier Neural Operators](https://openreview.net/forum?id=EXHG-A3jlM) |  | 0 | Vision transformers have delivered tremendous success in representation learning. This is primarily due to effective token mixing through self attention. However, this scales quadratically with the number of pixels, which becomes infeasible for high-resolution inputs. To cope with this challenge, we propose Adaptive Fourier Neural Operator (AFNO) as an efficient token... | Andrew Tao, Anima Anandkumar, Bryan Catanzaro, John Guibas, Morteza Mardani, Zongyi Li |  |
| 155 |  |  [Sample Selection with Uncertainty of Losses for Learning with Noisy Labels](https://openreview.net/forum?id=xENf4QUL4LW) |  | 0 | In learning with noisy labels, the sample selection approach is very popular, which regards small-loss data as correctly labeled data during training. However, losses are generated on-the-ﬂy based on the model being trained with noisy labels, and thus large-loss data are likely but not certain to be incorrect. There are actually two possibilities of a large-loss data... | Bo Han, Gang Niu, Jun Yu, Masashi Sugiyama, Mingming Gong, Tongliang Liu, Xiaobo Xia |  |
| 156 |  |  [Data-Driven Offline Optimization for Architecting Hardware Accelerators](https://openreview.net/forum?id=GsH-K1VIyy) |  | 0 | To attain higher efficiency, the industry has gradually reformed towards application-specific hardware accelerators. While such a paradigm shift is already starting to show promising results, designers need to spend considerable manual effort and perform large number of time-consuming simulations to find accelerators that can accelerate multiple target applications... | Amir Yazdanbakhsh, Aviral Kumar, Kevin Swersky, Milad Hashemi, Sergey Levine |  |
| 157 |  |  [Multi-Agent MDP Homomorphic Networks](https://openreview.net/forum?id=H7HDG--DJF0) |  | 0 | This paper introduces Multi-Agent MDP Homomorphic Networks, a class of networks that allows distributed execution using only local information, yet is able to share experience between global symmetries in the joint state-action space of cooperative multi-agent systems. In cooperative multi-agent systems, complex symmetries arise between different configurations of the... | Elise van der Pol, Frans A. Oliehoek, Herke van Hoof, Max Welling |  |
| 158 |  |  [Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields](https://openreview.net/forum?id=yhCp5RcZD7) |  | 0 | We present implicit displacement fields, a novel representation for detailed 3D geometry. Inspired by a classic surface deformation technique, displacement mapping, our method represents a complex surface as a smooth base surface plus a displacement along the base's normal directions, resulting in a frequency-based shape decomposition, where the high-frequency signal is... | Lukas Rahmann, Olga SorkineHornung, Yifan Wang |  |
| 159 |  |  [Modeling Label Space Interactions in Multi-label Classification using Box Embeddings](https://openreview.net/forum?id=tyTH9kOxcvh) |  | 0 | Multi-label classification is a challenging structured prediction task in which a set of output class labels are predicted for each input. Real-world datasets often have natural or latent taxonomic relationships between labels, making it desirable for models to employ label representations capable of capturing such taxonomies. Most existing multi-label classification... | Andrew McCallum, Dhruvesh Patel, JayYoon Lee, Michael Boratko, Pavitra Dangati |  |
| 160 |  |  [It Takes Two to Tango: Mixup for Deep Metric Learning](https://openreview.net/forum?id=ZKy2X3dgPA) |  | 0 | Metric learning involves learning a discriminative representation such that embeddings of similar classes are encouraged to be close, while embeddings of dissimilar classes are pushed far apart. State-of-the-art methods focus mostly on sophisticated loss functions or mining strategies. On the one hand, metric learning losses consider two or more examples at a time. On... | Bill Psomas, Ewa Kijak, Konstantinos Karantzalos, Laurent Amsaleg, Shashanka Venkataramanan, Yannis Avrithis |  |
| 161 |  |  [Data Efficient Language-Supervised Zero-Shot Recognition with Optimal Transport Distillation](https://openreview.net/forum?id=G89-1yZLFHk) |  | 0 | Traditional computer vision models are trained to predict a fixed set of predefined categories. Recently, natural language has been shown to be a broader and richer source of supervision that provides finer descriptions to visual concepts than supervised "gold" labels. Previous works, such as CLIP, use InfoNCE loss to train a model to predict the pairing between images... | Bichen Wu, Joseph E. Gonzalez, Peizhao Zhang, Peter Vajda, Ruizhe Cheng, Tianren Gao |  |
| 162 |  |  [A Statistical Framework for Efficient Out of Distribution Detection in Deep Neural Networks](https://openreview.net/forum?id=Oy9WeuZD51) |  | 0 | Background. Commonly, Deep Neural Networks (DNNs) generalize well on samples drawn from a distribution similar to that of the training set. However, DNNs' predictions are brittle and unreliable when the test samples are drawn from a dissimilar distribution. This is a major concern for deployment in real-world applications, where such behavior may come at a considerable... | Daniel Soudry, Matan Haroush, Ruth Heller, Tzviel Frostig |  |
| 163 |  |  [FedBABU: Toward Enhanced Representation for Federated Image Classification](https://openreview.net/forum?id=HuaYQfggn5u) |  | 0 | Federated learning has evolved to improve a single global model under data heterogeneity (as a curse) or to develop multiple personalized models using data heterogeneity (as a blessing). However, little research has considered both directions simultaneously. In this paper, we first investigate the relationship between them by analyzing Federated Averaging at the client... | Jaehoon Oh, Sangmook Kim, SeYoung Yun |  |
| 164 |  |  [Should I Run Offline Reinforcement Learning or Behavioral Cloning?](https://openreview.net/forum?id=AP1MKT37rJ) |  | 0 | Offline reinforcement learning (RL) algorithms can acquire effective policies by utilizing only previously collected experience, without any online interaction. While it is widely understood that offline RL is able to extract good policies even from highly suboptimal data, in practice offline RL is often used with data that resembles demonstrations. In this case, one... | Anikait Singh, Aviral Kumar, Joey Hong, Sergey Levine |  |
| 165 |  |  [Learning State Representations via Retracing in Reinforcement Learning](https://openreview.net/forum?id=CLpxpXqqBV) |  | 0 | We propose learning via retracing, a novel self-supervised approach for learning the state representation (and the associated dynamics model) for reinforcement learning tasks. In addition to the predictive (reconstruction) supervision in the forward direction, we propose to include "retraced" transitions for representation/model learning, by enforcing the... | Changmin Yu, Dong Li, Jianye Hao, Jun Wang, Neil Burgess |  |
| 166 |  |  [Open-World Semi-Supervised Learning](https://openreview.net/forum?id=O-r8LOR-CCA) |  | 0 | A fundamental limitation of applying semi-supervised learning in real-world settings is the assumption that unlabeled test data contains only classes previously encountered in the labeled training data. However, this assumption rarely holds for data in-the-wild, where instances belonging to novel classes may appear at testing time. Here, we introduce a novel open-world... | Jure Leskovec, Kaidi Cao, Maria Brbic |  |
| 167 |  |  [Evading Adversarial Example Detection Defenses with Orthogonal Projected Gradient Descent](https://openreview.net/forum?id=af1eUDdUVz) |  | 0 | Evading adversarial example detection defenses requires finding adversarial examples that must simultaneously (a) be misclassified by the model and (b) be detected as non-adversarial. We find that existing attacks that attempt to satisfy multiple simultaneous constraints often over-optimize against one constraint at the cost of satisfying another. We introduce Selective... | Nabeel Hingun, Nicholas Carlini, Oliver Bryniarski, Pedro Pachuca, Vincent Wang |  |
| 168 |  |  [Reducing Excessive Margin to Achieve a Better Accuracy vs. Robustness Trade-off](https://openreview.net/forum?id=Azh9QBQ4tR7) |  | 0 | While adversarial training has become the de facto approach for training robust classifiers, it leads to a drop in accuracy. This has led to prior works postulating that accuracy is inherently at odds with robustness. Yet, the phenomenon remains inexplicable. In this paper, we closely examine the changes induced in the decision boundary of a deep network during... | Rahul Rade, SeyedMohsen MoosaviDezfooli |  |
| 169 |  |  [Expressivity of Emergent Languages is a Trade-off between Contextual Complexity and Unpredictability](https://openreview.net/forum?id=WxuE_JWxjkW) |  | 0 | Researchers are using deep learning models to explore the emergence of language in various language games, where agents interact and develop an emergent language to solve tasks. We focus on the factors that determine the expressivity of emergent languages, which reflects the amount of information about input spaces those languages are capable of encoding. We measure the... | Kenny Smith, Kory Wallace Mathewson, Shangmin Guo, Simon Kirby, Stefano V. Albrecht, Yi Ren |  |
| 170 |  |  [Fast AdvProp](https://openreview.net/forum?id=hcoswsDHNAW) |  | 0 | Adversarial Propagation (AdvProp) is an effective way to improve recognition models, leveraging adversarial examples. Nonetheless, AdvProp suffers from the extremely slow training speed, mainly because: a) extra forward and backward passes are required for generating adversarial examples; b) both original samples and their adversarial counterparts are used for training... | Alan L. Yuille, Cihang Xie, Jieru Mei, Xianhang Li, Yingwei Li, Yixiao Zhang, Yucheng Han, Yutong Bai |  |
| 171 |  |  [Triangle and Four Cycle Counting with Predictions in Graph Streams](https://openreview.net/forum?id=8in_5gN9I0) |  | 0 | We propose data-driven one-pass streaming algorithms for estimating the number of triangles and four cycles, two fundamental problems in graph analytics that are widely studied in the graph data stream literature. Recently, Hsu et al. (2019) and Jiang et al. (2020) applied machine learning techniques in other data stream problems, using a trained oracle that can predict... | David P. Woodruff, Honghao Lin, Justin Y. Chen, Michael Zhang, Piotr Indyk, Ronitt Rubinfeld, Sandeep Silwal, Shyam Narayanan, Tal Wagner, Talya Eden |  |
| 172 |  |  [Is Fairness Only Metric Deep? Evaluating and Addressing Subgroup Gaps in Deep Metric Learning](https://openreview.net/forum?id=js62_xuLDDv) |  | 0 | Deep metric learning (DML) enables learning with less supervision through its emphasis on the similarity structure of representations. There has been much work on improving generalization of DML in settings like zero-shot retrieval, but little is known about its implications for fairness. In this paper, we are the first to evaluate state-of-the-art DML methods trained... | Karsten Roth, Kimia Hamidieh, Marzyeh Ghassemi, Natalie Dullerud, Nicolas Papernot |  |
| 173 |  |  [NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs](https://openreview.net/forum?id=xMJWUKJnFSw) |  | 0 | Conventional representation learning algorithms for knowledge graphs (KG) map each entity to a unique embedding vector. Such a shallow lookup results in a linear growth of memory consumption for storing the embedding matrix and incurs high computational costs of working with real-world KGs. Drawing parallels with subword tokenization commonly used in NLP, we explore the... | Etienne G. Denis, Jiapeng Wu, Mikhail Galkin, William L. Hamilton |  |
| 174 |  |  [Pix2seq: A Language Modeling Framework for Object Detection](https://openreview.net/forum?id=e42KbIw6Wb) |  | 0 | We present Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, we cast object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train... | David J. Fleet, Geoffrey E. Hinton, Lala Li, Saurabh Saxena, Ting Chen |  |
| 175 |  |  [Particle Stochastic Dual Coordinate Ascent: Exponential convergent algorithm for mean field neural network optimization](https://openreview.net/forum?id=PQQp7AJwz3) |  | 0 | We introduce Particle-SDCA, a gradient-based optimization algorithm for two-layer neural networks in the mean field regime that achieves exponential convergence rate in regularized empirical risk minimization. The proposed algorithm can be regarded as an infinite dimensional extension of Stochastic Dual Coordinate Ascent (SDCA) in the probability space: we exploit the... | Atsushi Nitanda, Denny Wu, Kazusato Oko, Taiji Suzuki |  |
| 176 |  |  [The Effects of Invertibility on the Representational Complexity of Encoders in Variational Autoencoders](https://openreview.net/forum?id=7_JR7WpwKV1) |  | 0 | Training and using modern neural-network based latent-variable generative models (like Variational Autoencoders) often require simultaneously training a generative direction along with an inferential (encoding) direction, which approximates the posterior distribution over the latent variables. Thus, the question arises: how complex does the inferential model need to be,... | Andrej Risteski, Divyansh Pareek |  |
| 177 |  |  [Tracking the risk of a deployed model and detecting harmful distribution shifts](https://openreview.net/forum?id=Ro_zAjZppv) |  | 0 | When deployed in the real world, machine learning models inevitably encounter changes in the data distribution, and certain---but not all---distribution shifts could result in significant performance degradation. In practice, it may make sense to ignore benign shifts, under which the performance of a deployed model does not degrade substantially, making interventions by... | Aaditya Ramdas, Aleksandr Podkopaev |  |
| 178 |  |  [Towards Understanding the Robustness Against Evasion Attack on Categorical Data](https://openreview.net/forum?id=BmJV7kyAmg) |  | 0 | Characterizing and assessing the adversarial vulnerability of classification models with categorical input has been a practically important, while rarely explored research problem. Our work echoes the challenge by first unveiling the impact factors of adversarial vulnerability of classification models with categorical data based on an information-theoretic adversarial... | Hongyan Bao, Xiangliang Zhang, Yufei Han, Yujun Zhou, Yun Shen |  |
| 179 |  |  [Learning Curves for SGD on Structured Features](https://openreview.net/forum?id=WPI2vbkAl3Q) |  | 0 | The generalization performance of a machine learning algorithm such as a neural network depends in a non-trivial way on the structure of the data distribution. To analyze the influence of data structure on test loss dynamics, we study an exactly solveable model of stochastic gradient descent (SGD) on the square loss which predicts test error when training on features... | Blake Bordelon, Cengiz Pehlevan |  |
| 180 |  |  [NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training](https://openreview.net/forum?id=Qaw16njk6L) |  | 0 | Designing accurate and efficient vision transformers (ViTs) is a highly important but challenging task. Supernet-based one-shot neural architecture search (NAS) enables fast architecture optimization and has achieved state-of-the-art (SOTA) results on convolutional neural networks (CNNs). However, directly applying the supernet-based NAS to optimize ViTs leads to poor... | Chengyue Gong, Dilin Wang, Meng Li, Qiang Liu, Vikas Chandra, Xinlei Chen, Yuandong Tian, Zhicheng Yan |  |
| 181 |  |  [Graphon based Clustering and Testing of Networks: Algorithms and Theory](https://openreview.net/forum?id=sTNHCrIKDQc) |  | 0 | Network-valued data are encountered in a wide range of applications, and pose challenges in learning due to their complex structure and absence of vertex correspondence. Typical examples of such problems include classification or grouping of protein structures and social networks. Various methods, ranging from graph kernels to graph neural networks, have been proposed... | Debarghya Ghoshdastidar, Leena Chennuru Vankadara, Mahalakshmi Sabanayagam |  |
| 182 |  |  [Network Augmentation for Tiny Deep Learning](https://openreview.net/forum?id=TYw3-OlrRm-) |  | 0 | We introduce Network Augmentation (NetAug), a new training method for improving the performance of tiny neural networks. Existing regularization techniques (e.g., data augmentation, dropout) have shown much success on large neural networks by adding noise to overcome over-fitting. However, we found these techniques hurt the performance of tiny neural networks. We argue... | Chuang Gan, Han Cai, Ji Lin, Song Han |  |
| 183 |  |  [Bridging the Gap: Providing Post-Hoc Symbolic Explanations for Sequential Decision-Making Problems with Inscrutable Representations](https://openreview.net/forum?id=o-1v9hdSult) |  | 0 | As increasingly complex AI systems are introduced into our daily lives, it becomes important for such systems to be capable of explaining the rationale for their decisions and allowing users to contest these decisions. A significant hurdle to allowing for such explanatory dialogue could be the {\em vocabulary mismatch} between the user and the AI system. This paper... | Mudit Verma, Sarath Sreedharan, Siddharth Srivastava, Subbarao Kambhampati, Utkarsh Soni |  |
| 184 |  |  [Distributional Reinforcement Learning with Monotonic Splines](https://openreview.net/forum?id=C8Ltz08PtBp) |  | 0 | Distributional Reinforcement Learning (RL) differs from traditional RL by estimating the distribution over returns to capture the intrinsic uncertainty of MDPs. One key challenge in distributional RL lies in how to parameterize the quantile function when minimizing the Wasserstein metric of temporal differences. Existing algorithms use step functions or piecewise linear... | Guiliang Liu, Haonan Duan, Oliver Schulte, Pascal Poupart, Yudong Luo |  |
| 185 |  |  [Toward Faithful Case-based Reasoning through Learning Prototypes in a Nearest Neighbor-friendly Space](https://openreview.net/forum?id=R79ZGjHhv6p) |  | 0 | Recent advances in machine learning have brought opportunities for the ever-increasing use of AI in the real world. This has created concerns about the black-box nature of many of the most recent machine learning approaches. In this work, we propose an interpretable neural network that leverages metric and prototype learning for classification tasks. It encodes its own... | Majid Komeili, Seyed Omid Davoudi |  |
| 186 |  |  [Augmented Sliced Wasserstein Distances](https://openreview.net/forum?id=iMqTLyfwnOO) |  | 0 | While theoretically appealing, the application of the Wasserstein distance to large-scale machine learning problems has been hampered by its prohibitive computational cost. The sliced Wasserstein distance and its variants improve the computational efficiency through the random projection, yet they suffer from low accuracy if the number of projections is not sufficiently... | Xiongjie Chen, Yongxin Yang, Yunpeng Li |  |
| 187 |  |  [Relational Learning with Variational Bayes](https://openreview.net/forum?id=Az-7gJc6lpr) |  | 0 | In psychology, relational learning refers to the ability to recognize and respond to relationship among objects irrespective of the nature of those objects. Relational learning has long been recognized as a hallmark of human cognition and a key question in artificial intelligence research. In this work, we propose an unsupervised learning method for addressing the... | KuangHung Liu |  |
| 188 |  |  [Provably Robust Adversarial Examples](https://openreview.net/forum?id=UMfhoMtIaP5) |  | 0 | We introduce the concept of provably robust adversarial examples for deep neural networks – connected input regions constructed from standard adversarial examples which are guaranteed to be robust to a set of real-world perturbations (such as changes in pixel intensity and geometric transformations). We present a novel method called PARADE for generating these regions... | Dimitar Iliev Dimitrov, Gagandeep Singh, Martin T. Vechev, Timon Gehr |  |
| 189 |  |  [Joint Shapley values: a measure of joint feature importance](https://openreview.net/forum?id=vcUmUvQCloe) |  | 0 | The Shapley value is one of the most widely used measures of feature importance partly as it measures a feature's average effect on a model's prediction. We introduce joint Shapley values, which directly extend Shapley's axioms and intuitions: joint Shapley values measure a set of features' average effect on a model's prediction. We prove the uniqueness of joint Shapley... | Chris Harris, Colin Rowat, Richard Pymar |  |
| 190 |  |  [Low-Budget Active Learning via Wasserstein Distance: An Integer Programming Approach](https://openreview.net/forum?id=v8OlxjGn23S) |  | 0 | Active learning is the process of training a model with limited labeled data by selecting a core subset of an unlabeled data pool to label. The large scale of data sets used in deep learning forces most sample selection strategies to employ efficient heuristics. This paper introduces an integer optimization problem for selecting a core set that minimizes the discrete... | Marc T. Law, Rafid Mahmood, Sanja Fidler |  |
| 191 |  |  [Efficient Self-supervised Vision Transformers for Representation Learning](https://openreview.net/forum?id=fVu3o-YUGQK) |  | 0 | This paper investigates two techniques for developing efficient self-supervised vision transformers (EsViT) for visual representation learning. First, we show through a comprehensive empirical study that multi-stage architectures with sparse self-attentions can significantly reduce modeling complexity but with a cost of losing the ability to capture fine-grained... | Bin Xiao, Chunyuan Li, Jianfeng Gao, Jianwei Yang, Lu Yuan, Mei Gao, Pengchuan Zhang, Xiyang Dai |  |
| 192 |  |  [Visual Representation Learning Does Not Generalize Strongly Within the Same Domain](https://openreview.net/forum?id=9RUHPlladgh) |  | 0 | An important component for generalization in machine learning is to uncover underlying latent factors of variation as well as the mechanism through which each factor acts in the world. In this paper, we test whether 17 unsupervised, weakly supervised, and fully supervised representation learning approaches correctly infer the generative factors of variation in simple... | Bernhard Schölkopf, Chris Russell, Francesco Locatello, Frederik Träuble, Julius von Kügelgen, Lukas Schott, Matthias Bethge, Peter Vincent Gehler, Wieland Brendel |  |
| 193 |  |  [Hidden Convexity of Wasserstein GANs: Interpretable Generative Models with Closed-Form Solutions](https://openreview.net/forum?id=e2Lle5cij9D) |  | 0 | Generative Adversarial Networks (GANs) are commonly used for modeling complex distributions of data. Both the generators and discriminators of GANs are often modeled by neural networks, posing a non-transparent optimization problem which is non-convex and non-concave over the generator and discriminator, respectively. Such networks are often heuristically optimized with... | Arda Sahiner, Batu Ozturkler, Burak Bartan, John M. Pauly, Mert Pilanci, Morteza Mardani, Tolga Ergen |  |
| 194 |  |  [Memory Augmented Optimizers for Deep Learning](https://openreview.net/forum?id=NRX9QZ6yqt) |  | 0 | Popular approaches for minimizing loss in data-driven learning often involve an abstraction or an explicit retention of the history of gradients for efficient parameter updates. The aggregated history of gradients nudges the parameter updates in the right direction even when the gradients at any given step are not informative. Although the history of gradients... | Mido Assran, PaulAymeric Martin McRae, Prasanna Parthasarathi, Sarath Chandar |  |
| 195 |  |  [Orchestrated Value Mapping for Reinforcement Learning](https://openreview.net/forum?id=c87d0TS4yX) |  | 0 | We present a general convergent class of reinforcement learning algorithms that is founded on two distinct principles: (1) mapping value estimates to a different space using arbitrary functions from a broad class, and (2) linearly decomposing the reward signal into multiple channels. The first principle enables incorporating specific properties into the value estimator... | Arash Tavakoli, Mehdi Fatemi |  |
| 196 |  |  [Learning to Generalize across Domains on Single Test Samples](https://openreview.net/forum?id=CIaQKbTBwtU) |  | 0 | We strive to learn a model from a set of source domains that generalizes well to unseen target domains. The main challenge in such a domain generalization scenario is the unavailability of any target domain data during training, resulting in the learned model not being explicitly adapted to the unseen target domains. We propose learning to generalize across domains on... | Cees G. M. Snoek, Ling Shao, Xiantong Zhen, Zehao Xiao |  |
| 197 |  |  [Prototype memory and attention mechanisms for few shot image generation](https://openreview.net/forum?id=lY0-7bj0Vfz) |  | 0 | Recent discoveries indicate that the neural codes in the primary visual cortex (V1) of macaque monkeys are complex, diverse and sparse. This leads us to ponder the computational advantages and functional role of these “grandmother cells." Here, we propose that such cells can serve as prototype memory priors that bias and shape the distributed feature processing within... | Amir Barati Farimani, Andrew Luo, Harold Rockwell, Tai Sing Lee, Tianqin Li, Zijie Li |  |
| 198 |  |  [TPU-GAN: Learning temporal coherence from dynamic point cloud sequences](https://openreview.net/forum?id=FEBFJ98FKx) |  | 0 | Point cloud sequence is an important data representation that provides flexible shape and motion information. Prior work demonstrates that incorporating scene flow information into loss can make model learn temporally coherent feature spaces. However, it is prohibitively expensive to acquire point correspondence information across frames in real-world environments. In... | Amir Barati Farimani, Tianqin Li, Zijie Li |  |
| 199 |  |  [A First-Occupancy Representation for Reinforcement Learning](https://openreview.net/forum?id=JBAZe2yN6Ub) |  | 0 | Both animals and artificial agents benefit from state representations that support rapid transfer of learning across tasks and which enable them to efficiently traverse their environments to reach rewarding states. The successor representation (SR), which measures the expected cumulative, discounted state occupancy under a fixed policy, enables efficient transfer to... | Maneesh Sahani, Spencer R. Wilson, Ted Moskovitz |  |
| 200 |  |  [Deep ReLU Networks Preserve Expected Length](https://openreview.net/forum?id=ci7LBzDn2Q) |  | 0 | Assessing the complexity of functions computed by a neural network helps us understand how the network will learn and generalize. One natural measure of complexity is how the network distorts length - if the network takes a unit-length curve as input, what is the length of the resulting curve of outputs? It has been widely believed that this length grows exponentially... | Boris Hanin, David Rolnick, Ryan S. Jeong |  |
| 201 |  |  [Phenomenology of Double Descent in Finite-Width Neural Networks](https://openreview.net/forum?id=lTqGXfn9Tv) |  | 0 | \`Double descent' delineates the generalization behaviour of models depending on the regime they belong to: under- or over-parameterized. The current theoretical understanding behind the occurrence of this phenomenon is primarily based on linear and kernel regression models --- with informal parallels to neural networks via the Neural Tangent Kernel. Therefore such... | Aurélien Lucchi, Bernhard Schölkopf, Sidak Pal Singh, Thomas Hofmann |  |
| 202 |  |  [How Attentive are Graph Attention Networks?](https://openreview.net/forum?id=F72ximsx7C1) |  | 0 | Graph Attention Networks (GATs) are one of the most popular GNN architectures and are considered as the state-of-the-art architecture for representation learning with graphs. In GAT, every node attends to its neighbors given its own representation as the query. However, in this paper we show that GAT computes a very limited kind of attention: the ranking of the... | Eran Yahav, Shaked Brody, Uri Alon |  |
| 203 |  |  [Learning Transferable Reward for Query Object Localization with Policy Adaptation](https://openreview.net/forum?id=92tYQiil17) |  | 0 | We propose a reinforcement learning based approach to query object localization, for which an agent is trained to localize objects of interest specified by a small exemplary set. We learn a transferable reward signal formulated using the exemplary set by ordinal metric learning. Our proposed method enables test-time policy adaptation to new environments where the reward... | Dimitris N. Metaxas, Martin Renqiang Min, Shaobo Han, Tingfeng Li |  |
| 204 |  |  [CKConv: Continuous Kernel Convolution For Sequential Data](https://openreview.net/forum?id=8FhxBtXSl0) |  | 0 | Conventional neural architectures for sequential data present important limitations. Recurrent neural networks suffer from exploding and vanishing gradients, small effective memory horizons, and must be trained sequentially. Convolutional neural networks cannot handle sequences of unknown size and their memory horizon must be defined a priori. In this work, we show that... | Anna Kuzina, David W. Romero, Erik J. Bekkers, Jakub Mikolaj Tomczak, Mark Hoogendoorn |  |
| 205 |  |  [Towards Empirical Sandwich Bounds on the Rate-Distortion Function](https://openreview.net/forum?id=H4PmOqSZDY) |  | 0 | Rate-distortion (R-D) function, a key quantity in information theory, characterizes the fundamental limit of how much a data source can be compressed subject to a fidelity criterion, by any compression algorithm. As researchers push for ever-improving compression performance, establishing the R-D function of a given data source is not only of scientific interest, but... | Stephan Mandt, Yibo Yang |  |
| 206 |  |  [Pareto Policy Adaptation](https://openreview.net/forum?id=wfZGut6e09) |  | 0 | We present a policy gradient method for Multi-Objective Reinforcement Learning under unknown, linear preferences. By enforcing Pareto stationarity, a first-order condition for Pareto optimality, we are able to design a simple policy gradient algorithm that approximates the Pareto front and infers the unknown preferences. Our method relies on a projected gradient descent... | Jyotirmoy Deshmukh, Panagiotis Kyriakis, Paul Bogdan |  |
| 207 |  |  [Fair Normalizing Flows](https://openreview.net/forum?id=BrFIKuxrZE) |  | 0 | Fair representation learning is an attractive approach that promises fairness of downstream predictors by encoding sensitive data. Unfortunately, recent work has shown that strong adversarial predictors can still exhibit unfairness by recovering sensitive attributes from these representations. In this work, we present Fair Normalizing Flows (FNF), a new approach... | Anian Ruoss, Martin T. Vechev, Mislav Balunovic |  |
| 208 |  |  [The Convex Geometry of Backpropagation: Neural Network Gradient Flows Converge to Extreme Points of the Dual Convex Program](https://openreview.net/forum?id=5QhUE1qiVC6) |  | 0 | We study non-convex subgradient flows for training two-layer ReLU neural networks from a convex geometry and duality perspective. We characterize the implicit bias of unregularized non-convex gradient flow as convex regularization of an equivalent convex model. We then show that the limit points of non-convex subgradient flows can be identified via primal-dual... | Mert Pilanci, Yifei Wang |  |
| 209 |  |  [Adaptive Wavelet Transformer Network for 3D Shape Representation Learning](https://openreview.net/forum?id=5MLb3cLCJY) |  | 0 | We present a novel method for 3D shape representation learning using multi-scale wavelet decomposition. Previous works often decompose 3D shapes into complementary components in spatial domain at a single scale. In this work, we study to decompose 3D shapes into sub-bands components in frequency domain at multiple scales, resulting in a hierarchical decomposition tree... | Hao Huang, Yi Fang |  |
| 210 |  |  [On the Convergence of mSGD and AdaGrad for Stochastic Optimization](https://openreview.net/forum?id=g5tANwND04i) |  | 0 | As one of the most fundamental stochastic optimization algorithms, stochastic gradient descent (SGD) has been intensively developed and extensively applied in machine learning in the past decade. There have been some modified SGD-type algorithms, which outperform the SGD in many competitions and applications in terms of convergence rate and accuracy, such as... | Ruinan Jin, Xingkang He, Yu Xing |  |
| 211 |  |  [Likelihood Training of Schrödinger Bridge using Forward-Backward SDEs Theory](https://openreview.net/forum?id=nioAdKCEdXB) |  | 0 | Schrödinger Bridge (SB) is an entropy-regularized optimal transport problem that has received increasing attention in deep generative modeling for its mathematical flexibility compared to the Scored-based Generative Model (SGM). However, it remains unclear whether the optimization principle of SB relates to the modern training of deep generative models, which often rely... | Evangelos A. Theodorou, GuanHorng Liu, Tianrong Chen |  |
| 212 |  |  [Imitation Learning from Observations under Transition Model Disparity](https://openreview.net/forum?id=twv2QlJhXzo) |  | 0 | Learning to perform tasks by leveraging a dataset of expert observations, also known as imitation learning from observations (ILO), is an important paradigm for learning skills without access to the expert reward function or the expert actions. We consider ILO in the setting where the expert and the learner agents operate in different environments, with the source of... | Jian Peng, Tanmay Gangwani, Yuan Zhou |  |
| 213 |  |  [MCMC Should Mix: Learning Energy-Based Model with Neural Transport Latent Space MCMC](https://openreview.net/forum?id=4C93Qvn-tz) |  | 0 | Learning energy-based model (EBM) requires MCMC sampling of the learned model as an inner loop of the learning algorithm. However, MCMC sampling of EBMs in high-dimensional data space is generally not mixing, because the energy function, which is usually parametrized by deep network, is highly multi-modal in the data space. This is a serious handicap for both theory and... | Bo Pang, Erik Nijkamp, Pavel Sountsov, Ruiqi Gao, SongChun Zhu, Srinivas Vasudevan, Ying Nian Wu |  |
| 214 |  |  [Autonomous Learning of Object-Centric Abstractions for High-Level Planning](https://openreview.net/forum?id=rrWeE9ZDw_) |  | 0 | We propose a method for autonomously learning an object-centric representation of a continuous and high-dimensional environment that is suitable for planning. Such representations can immediately be transferred between tasks that share the same types of objects, resulting in agents that require fewer samples to learn a model of a new task. We first demonstrate our... | Benjamin Rosman, George Konidaris, Steven James |  |
| 215 |  |  [A fast and accurate splitting method for optimal transport: analysis and implementation](https://openreview.net/forum?id=fCSq8yrDkc) |  | 0 | We develop a fast and reliable method for solving large-scale optimal transport (OT) problems at an unprecedented combination of speed and accuracy. Built on the celebrated Douglas-Rachford splitting technique, our method tackles the original OT problem directly instead of solving an approximate regularized problem, as many state-of-the-art techniques do. This allows us... | Jacob Lindbäck, Mikael Johansson, Vien V. Mai |  |
| 216 |  |  [Implicit Bias of MSE Gradient Optimization in Underparameterized Neural Networks](https://openreview.net/forum?id=VLgmhQDVBV) |  | 0 | We study the dynamics of a neural network in function space when optimizing the mean squared error via gradient flow. We show that in the underparameterized regime the network learns eigenfunctions of an integral operator $T_K$ determined by the Neural Tangent Kernel at rates corresponding to their eigenvalues. For example, for uniformly distributed data on the sphere... | Benjamin Bowman, Guido Montúfar |  |
| 217 |  |  [Discovering Latent Concepts Learned in BERT](https://openreview.net/forum?id=POTMtpYI1xH) |  | 0 | A large number of studies that analyze deep neural network models and their ability to encode various linguistic and non-linguistic concepts provide an interpretation of the inner mechanics of these models. The scope of the analyses is limited to pre-defined concepts that reinforce the traditional linguistic knowledge and do not reflect on how novel concepts are learned... | Abdul Rafae Khan, Fahim Dalvi, Firoj Alam, Hassan Sajjad, Jia Xu, Nadir Durrani |  |
| 218 |  |  [The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks](https://openreview.net/forum?id=dNigytemkL) |  | 0 | In this paper, we conjecture that if the permutation invariance of neural networks is taken into account, SGD solutions will likely have no barrier in the linear interpolation between them. Although it is a bold conjecture, we show how extensive empirical attempts fall short of refuting it. We further provide a preliminary theoretical result to support our conjecture.... | Behnam Neyshabur, Hanie Sedghi, Olga Saukh, Rahim Entezari |  |
| 219 |  |  [Data Poisoning Won't Save You From Facial Recognition](https://openreview.net/forum?id=B5XahNLmna) |  | 0 | Data poisoning has been proposed as a compelling defense against facial recognition models trained on Web-scraped pictures. Users can perturb images they post online, so that models will misclassify future (unperturbed) pictures. We demonstrate that this strategy provides a false sense of security, as it ignores an inherent asymmetry between the parties: users' pictures... | Evani RadiyaDixit, Florian Tramèr, Nicholas Carlini, Sanghyun Hong |  |
| 220 |  |  [MetaMorph: Learning Universal Controllers with Transformers](https://openreview.net/forum?id=Opmqtk_GvYL) |  | 0 | Multiple domains like vision, natural language, and audio are witnessing tremendous progress by leveraging Transformers for large scale pre-training followed by task specific fine tuning. In contrast, in robotics we primarily train a single robot for a single task. However, modular robot systems now allow for the flexible combination of general-purpose building blocks... | Agrim Gupta, Li FeiFei, Linxi Fan, Surya Ganguli |  |
| 221 |  |  [HTLM: Hyper-Text Pre-Training and Prompting of Language Models](https://openreview.net/forum?id=P-pPW1nxf1r) |  | 0 | We introduce HTLM, a hyper-text language model trained on a large-scale web crawl. Modeling hyper-text has a number of advantages: (1) it is easily gathered at scale, (2) it provides rich document-level and end-task-adjacent supervision (e.g. 'class' and 'id' attributes often encode document category information), and (3) it allows for new structured prompting that... | Armen Aghajanyan, Dmytro Okhonko, Gargi Ghosh, Hu Xu, Luke Zettlemoyer, Mandar Joshi, Mike Lewis |  |
| 222 |  |  [Illiterate DALL-E Learns to Compose](https://openreview.net/forum?id=h0OYV0We3oh) |  | 0 | Although DALL-E has shown an impressive ability of composition-based systematic generalization in image generation, it requires the dataset of text-image pairs and the compositionality is provided by the text. In contrast, object-centric representation models like the Slot Attention model learn composable representations without the text prompt. However, unlike DALL-E,... | Fei Deng, Gautam Singh, Sungjin Ahn |  |
| 223 |  |  [The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models](https://openreview.net/forum?id=JYtwGwIL7ye) |  | 0 | Reward hacking---where RL agents exploit gaps in misspecified proxy rewards---has been widely observed, but not yet systematically studied. To understand reward hacking, we construct four RL environments with different misspecified rewards. We investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, and observation space... | Alexander Pan, Jacob Steinhardt, Kush Bhatia |  |
| 224 |  |  [Optimizing Neural Networks with Gradient Lexicase Selection](https://openreview.net/forum?id=J_2xNmVcY4) |  | 0 | One potential drawback of using aggregated performance measurement in machine learning is that models may learn to accept higher errors on some training cases as compromises for lower errors on others, with the lower errors actually being instances of overfitting. This can lead both to stagnation at local optima and to poor generalization. Lexicase selection is an... | Lee Spector, Li Ding |  |
| 225 |  |  [Offline Reinforcement Learning with Implicit Q-Learning](https://openreview.net/forum?id=68n2s9ZJWF8) |  | 0 | Offline reinforcement learning requires reconciling two conflicting aims: learning a policy that improves over the behavior policy that collected the dataset, while at the same time minimizing the deviation from the behavior policy so as to avoid errors due to distributional shift. This tradeoff is critical, because most current offline reinforcement learning methods... | Ashvin Nair, Ilya Kostrikov, Sergey Levine |  |
| 226 |  |  [Learning Distributionally Robust Models at Scale via Composite Optimization](https://openreview.net/forum?id=To-R742x7se) |  | 0 | To train machine learning models that are robust to distribution shifts in the data, distributionally robust optimization (DRO) has been proven very effective. However, the existing approaches to learning a distributionally robust model either require solving complex optimization problems such as semidefinite programming or a first-order method whose convergence scales... | Amin Karbasi, Farzin Haddadpour, Mehrdad Mahdavi, Mohammad Mahdi Kamani |  |
| 227 |  |  [Counterfactual Plans under Distributional Ambiguity](https://openreview.net/forum?id=noaG7SrPVK0) |  | 0 | Counterfactual explanations are attracting significant attention due to the flourishing applications of machine learning models in consequential domains. A counterfactual plan consists of multiple possibilities to modify a given instance so that the model's prediction will be altered. As the predictive model can be updated subject to the future arrival of new data, a... | Duy Nguyen, Ngoc Bui, Viet Anh Nguyen |  |
| 228 |  |  [Neural Parameter Allocation Search](https://openreview.net/forum?id=srtIXtySfT4) |  | 0 | Training neural networks requires increasing amounts of memory. Parameter sharing can reduce memory and communication costs, but existing methods assume networks have many identical layers and utilize hand-crafted sharing strategies that fail to generalize. We introduce Neural Parameter Allocation Search (NPAS), a novel task where the goal is to train a neural network... | Bryan A. Plummer, Julius Frost, Kate Saenko, Nikoli Dryden, Torsten Hoefler |  |
| 229 |  |  [Non-Linear Operator Approximations for Initial Value Problems](https://openreview.net/forum?id=d2TT6gK9qZn) |  | 0 | Time-evolution of partial differential equations is the key to model several dynamical processes, events forecasting but the operators associated with such problems are non-linear. We propose a Padé approximation based exponential neural operator scheme for efficiently learning the map between a given initial condition and activities at a later time. The multiwavelets... | Gaurav Gupta, Paul Bogdan, Radu Balan, Xiongye Xiao |  |
| 230 |  |  [Constructing a Good Behavior Basis for Transfer using Generalized Policy Updates](https://openreview.net/forum?id=7IWGzQ6gZ1D) |  | 0 | We study the problem of learning a good set of policies, so that when combined together, they can solve a wide variety of unseen reinforcement learning tasks with no or very little new data. Specifically, we consider the framework of generalized policy evaluation and improvement, in which the rewards for all tasks of interest are assumed to be expressible as a linear... | Doina Precup, Safa Alver |  |
| 231 |  |  [Collapse by Conditioning: Training Class-conditional GANs with Limited Data](https://openreview.net/forum?id=7TZeCsNOUB_) |  | 0 | Class-conditioning offers a direct means to control a Generative Adversarial Network (GAN) based on a discrete input variable. While necessary in many applications, the additional information provided by the class labels could even be expected to benefit the training of the GAN itself. On the contrary, we observe that class-conditioning causes mode collapse in limited... | Danda Pani Paudel, Luc Van Gool, Martin Danelljan, Mohamad Shahbazi |  |
| 232 |  |  [High Probability Bounds for a Class of Nonconvex Algorithms with AdaGrad Stepsize](https://openreview.net/forum?id=dSw0QtRMJkO) |  | 0 | In this paper, we propose a new, simplified high probability analysis of AdaGrad for smooth, non-convex problems. More specifically, we focus on a particular accelerated gradient (AGD) template (Lan, 2020), through which we recover the original AdaGrad and its variant with averaging, and prove a convergence rate of $\mathcal O (1/ \sqrt{T})$ with high probability... | Ali Kavis, Kfir Yehuda Levy, Volkan Cevher |  |
| 233 |  |  [Map Induction: Compositional spatial submap learning for efficient exploration in novel environments](https://openreview.net/forum?id=1NUsBU-7HAL) |  | 0 | Humans are expert explorers and foragers. Understanding the computational cognitive mechanisms that support this capability can advance the study of the human mind and enable more efficient exploration algorithms. We hypothesize that humans explore new environments by inferring the structure of unobserved spaces through re-use of spatial information collected from... | Aidan Curtis, Ila R. Fiete, Joshua B. Tenenbaum, Marta Kryven, Sugandha Sharma |  |
| 234 |  |  [How Did the Model Change? Efficiently Assessing Machine Learning API Shifts](https://openreview.net/forum?id=gFDFKC4gHL4) |  | 0 | ML prediction APIs from providers like Amazon and Google have made it simple to use ML in applications. A challenge for users is that such APIs continuously change over time as the providers update models, and changes can happen silently without users knowing. It is thus important to monitor when and how much the MLAPIs’ performance shifts. To provide detailed change... | James Zou, Lingjiao Chen, Matei Zaharia |  |
| 235 |  |  [A Tale of Two Flows: Cooperative Learning of Langevin Flow and Normalizing Flow Toward Energy-Based Model](https://openreview.net/forum?id=31d5RLCUuXC) |  | 0 | This paper studies the cooperative learning of two generative flow models, in which the two models are iteratively updated based on the jointly synthesized examples. The first flow model is a normalizing flow that transforms an initial simple density to a target density by applying a sequence of invertible transformations. The second flow model is a Langevin flow that... | Jianwen Xie, Jun Li, Ping Li, Yaxuan Zhu |  |
| 236 |  |  [Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?](https://openreview.net/forum?id=WVX0NNVBBkV) |  | 0 | While additional training data improves the robustness of deep neural networks against adversarial examples, it presents the challenge of curating a large number of specific real-world samples. We circumvent this challenge by using additional data from proxy distributions learned by advanced generative models. We first seek to formally understand the transfer of... | Chong Xiang, Mung Chiang, Prateek Mittal, Saeed Mahloujifar, Sihui Dai, Tinashe Handina, Vikash Sehwag |  |
| 237 |  |  [Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap](https://openreview.net/forum?id=ECvgmYVyeUz) |  | 0 | Recently, contrastive learning has risen to be a promising approach for large-scale self-supervised learning. However, theoretical understanding of how it works is still unclear. In this paper, we propose a new guarantee on the downstream performance without resorting to the conditional independence assumption that is widely adopted in previous work but hardly holds in... | Jiansheng Yang, Qi Zhang, Yifei Wang, Yisen Wang, Zhouchen Lin |  |
| 238 |  |  [Language-biased image classification: evaluation based on semantic representations](https://openreview.net/forum?id=xNO7OEIcJc6) |  | 0 | Humans show language-biased image recognition for a word-embedded image, known as picture-word interference. Such interference depends on hierarchical semantic categories and reflects that human language processing highly interacts with visual processing. Similar to humans, recent artificial models jointly trained on texts and images, e.g., OpenAI CLIP, show... | Guillermo Valle Pérez, Hélène Sauzéon, Masataka Sawayama, Maxime Adolphe, PierreYves Oudeyer, Yoann Lemesle |  |
| 239 |  |  [Robbing the Fed: Directly Obtaining Private Data in Federated Learning with Modified Models](https://openreview.net/forum?id=fwzUgo0FM9v) |  | 0 | Federated learning has quickly gained popularity with its promises of increased user privacy and efficiency. Previous works have shown that federated gradient updates contain information that can be used to approximately recover user data in some situations. These previous attacks on user privacy have been limited in scope and do not scale to gradient updates aggregated... | Jonas Geiping, Liam H. Fowl, Micah Goldblum, Tom Goldstein, Wojciech Czaja |  |
| 240 |  |  [Practical Conditional Neural Process Via Tractable Dependent Predictions](https://openreview.net/forum?id=3pugbNqOh5m) |  | 0 | Conditional Neural Processes (CNPs; Garnelo et al., 2018a) are meta-learning models which leverage the flexibility of deep learning to produce well-calibrated predictions and naturally handle off-the-grid and missing data. CNPs scale to large datasets and train with ease. Due to these features, CNPs appear well-suited to tasks from environmental sciences or healthcare.... | Anna Vaughan, James Requeima, Richard E. Turner, Stratis Markou, Wessel P. Bruinsma |  |
| 241 |  |  [Reward Uncertainty for Exploration in Preference-based Reinforcement Learning](https://openreview.net/forum?id=OWZVD-l-ZrC) |  | 0 | Conveying complex objectives to reinforcement learning (RL) agents often requires meticulous reward engineering. Preference-based RL methods are able to learn a more flexible reward model based on human preferences by actively incorporating human feedback, i.e. teacher's preferences between two clips of behaviors. However, poor feedback-efficiency still remains as a... | Katherine Shu, Kimin Lee, Pieter Abbeel, Xinran Liang |  |
| 242 |  |  [Decentralized Learning for Overparameterized Problems: A Multi-Agent Kernel Approximation Approach](https://openreview.net/forum?id=oj2yn1Q4Ett) |  | 0 | This work develops a novel framework for communication-efficient distributed learning where the models to be learned are overparameterized. We focus on a class of kernel learning problems (which includes the popular neural tangent kernel (NTK) learning as a special case) and propose a novel {\it multi-agent kernel approximation} technique that allows the agents to... | Haibo Yang, HoiTo Wai, Jia Liu, Mingyi Hong, Prashant Khanduri, Sijia Liu |  |
| 243 |  |  [Permutation-Based SGD: Is Random Optimal?](https://openreview.net/forum?id=YiBa9HKTyXE) |  | 0 | A recent line of ground-breaking results for permutation-based SGD has corroborated a widely observed phenomenon: random permutations offer faster convergence than with-replacement sampling. However, is random optimal? We show that this depends heavily on what functions we are optimizing, and the convergence gap between optimal and random permutations can vary from... | Dimitris S. Papailiopoulos, Kangwook Lee, Shashank Rajput |  |
| 244 |  |  [Graph-less Neural Networks: Teaching Old MLPs New Tricks Via Distillation](https://openreview.net/forum?id=4p6_5HBWPCw) |  | 0 | Graph Neural Networks (GNNs) are popular for graph machine learning and have shown great results on wide node classification tasks. Yet, they are less popular for practical deployments in the industry owing to their scalability challenges incurred by data dependency. Namely, GNN inference depends on neighbor nodes multiple hops away from the target, and fetching them... | Neil Shah, Shichang Zhang, Yizhou Sun, Yozen Liu |  |
| 245 |  |  [Relating transformers to models and neural representations of the hippocampal formation](https://openreview.net/forum?id=B8DVo9B1YE0) |  | 0 | Many deep neural network architectures loosely based on brain networks have recently been shown to replicate neural firing patterns observed in the brain. One of the most exciting and promising novel architectures, the Transformer neural network, was developed without the brain in mind. In this work, we show that transformers, when equipped with recurrent position... | James C. R. Whittington, Joseph Warren, Tim E. J. Behrens |  |
| 246 |  |  [How many degrees of freedom do we need to train deep networks: a loss landscape perspective](https://openreview.net/forum?id=ChMLTGRjFcU) |  | 0 | A variety of recent works, spanning pruning, lottery tickets, and training within random subspaces, have shown that deep neural networks can be trained using far fewer degrees of freedom than the total number of parameters. We analyze this phenomenon for random subspaces by first examining the success probability of hitting a training loss sublevel set when training... | Brett W. Larsen, Nic Becker, Stanislav Fort, Surya Ganguli |  |
| 247 |  |  [Is Importance Weighting Incompatible with Interpolating Classifiers?](https://openreview.net/forum?id=uqBOne3LUKy) |  | 0 | Importance weighting is a classic technique to handle distribution shifts. However, prior work has presented strong empirical and theoretical evidence demonstrating that importance weights can have little to no effect on overparameterized neural networks. \emph{Is importance weighting truly incompatible with the training of overparameterized neural networks?} Our paper... | Ke Alexander Wang, Niladri Shekhar Chatterji, Saminul Haque, Tatsunori Hashimoto |  |
| 248 |  |  [Neural Models for Output-Space Invariance in Combinatorial Problems](https://openreview.net/forum?id=ibrUkC-pbis) |  | 0 | Recently many neural models have been proposed to solve combinatorial puzzles by implicitly learning underlying constraints using their solved instances, such as sudoku or graph coloring (GCP). One drawback of the proposed architectures, which are often based on Graph Neural Networks (GNN) (Zhou et al., 2020), is that they cannot generalize across the size of the output... | Mausam, Parag Singla, Vidit Jain, Yatin Nandwani |  |
| 249 |  |  [StyleNeRF: A Style-based 3D Aware Generator for High-resolution Image Synthesis](https://openreview.net/forum?id=iUuzzTMUw9K) |  | 0 | We propose StyleNeRF, a 3D-aware generative model for photo-realistic high-resolution image synthesis with high multi-view consistency, which can be trained on unstructured 2D images. Existing approaches either cannot synthesize high-resolution images with fine details or yield clearly noticeable 3D-inconsistent artifacts. In addition, many of them lack control on style... | Christian Theobalt, Jiatao Gu, Lingjie Liu, Peng Wang |  |
| 250 |  |  [The Role of Pretrained Representations for the OOD Generalization of RL Agents](https://openreview.net/forum?id=8eb12UQYxrG) |  | 0 | Building sample-efficient agents that generalize out-of-distribution (OOD) in real-world settings remains a fundamental unsolved problem on the path towards achieving higher-level cognition. One particularly promising approach is to begin with low-dimensional, pretrained representations of our world, which should facilitate efficient downstream learning and... | Andrea Dittadi, Bernhard Schölkopf, Felix Widmaier, Francesco Locatello, Frederik Träuble, Manuel Wuthrich, Ole Winther, Olivier Bachem, Peter Vincent Gehler, Stefan Bauer |  |
| 251 |  |  [Enabling Arbitrary Translation Objectives with Adaptive Tree Search](https://openreview.net/forum?id=rhOiUS8KQM9) |  | 0 | We introduce an adaptive tree search algorithm, which is a deterministic variant of Monte Carlo tree search, that can find high-scoring outputs under translation models that make no assumptions about the form or structure of the search objective. This algorithm enables the exploration of new kinds of models that are unencumbered by constraints imposed to make decoding... | Austin Matthews, Chris Dyer, Domenic Donato, Laurent Sartran, Lei Yu, Wang Ling, Wojciech Stokowiec |  |
| 252 |  |  [Proof Artifact Co-Training for Theorem Proving with Language Models](https://openreview.net/forum?id=rpxJc9j04U) |  | 0 | Labeled data for imitation learning of theorem proving in large libraries of formalized mathematics is scarce as such libraries require years of concentrated effort by human specialists to be built. This is particularly challenging when applying large Transformer language models to tactic prediction, because the scaling of performance with respect to model size is... | Edward W. Ayers, Jason Rute, Jesse Michael Han, Stanislas Polu, Yuhuai Wu |  |
| 253 |  |  [Mirror Descent Policy Optimization](https://openreview.net/forum?id=aBO5SvgSt1) |  | 0 | Mirror descent (MD), a well-known first-order method in constrained convex optimization, has recently been shown as an important tool to analyze trust-region algorithms in reinforcement learning (RL). However, there remains a considerable gap between such theoretically analyzed algorithms and the ones used in practice. Inspired by this, we propose an efficient RL... | Lior Shani, Manan Tomar, Mohammad Ghavamzadeh, Yonathan Efroni |  |
| 254 |  |  [A Loss Curvature Perspective on Training Instabilities of Deep Learning Models](https://openreview.net/forum?id=OcKMT-36vUs) |  | 0 | In this work, we study the evolution of the loss Hessian across many classification tasks in order to understand the effect the curvature of the loss has on the training dynamics. Whereas prior work has focused on how different learning rates affect the loss Hessian observed during training, we also analyze the effects of model initialization, architectural choices, and... | Ankush Garg, Behnam Neyshabur, Behrooz Ghorbani, David Cardoze, George Edward Dahl, Justin Gilmer, Orhan Firat, Sneha Kudugunta, Zachary Nado |  |
| 255 |  |  [Cross-Domain Imitation Learning via Optimal Transport](https://openreview.net/forum?id=xP3cPq2hQC) |  | 0 | Cross-domain imitation learning studies how to leverage expert demonstrations of one agent to train an imitation agent with a different embodiment or morphology. Comparing trajectories and stationary distributions between the expert and imitation agents is challenging because they live on different systems that may not even have the same dimensionality. We propose... | Arnaud Fickinger, Brandon Amos, Samuel Cohen, Stuart Russell |  |
| 256 |  |  [Large-Scale Representation Learning on Graphs via Bootstrapping](https://openreview.net/forum?id=0UXT6PpRpW) |  | 0 | Self-supervised learning provides a promising path towards eliminating the need for costly label information in representation learning on graphs. However, to achieve state-of-the-art performance, methods often need large numbers of negative examples and rely on complex augmentations. This can be prohibitively expensive, especially for large graphs. To address these... | Corentin Tallec, Eva L. Dyer, Mehdi Azabou, Michal Valko, Mohammad Gheshlaghi Azar, Petar Velickovic, Rémi Munos, Shantanu Thakoor |  |
| 257 |  |  [Robust and Scalable SDE Learning: A Functional Perspective](https://openreview.net/forum?id=xZ6H7wydGl) |  | 0 | Stochastic differential equations provide a rich class of flexible generative models, capable of describing a wide range of spatio-temporal processes. A host of recent work looks to learn data-representing SDEs, using neural networks and other flexible function approximators. Despite these advances, learning remains computationally expensive due to the sequential nature... | Arnu Pretorius, Scott Alexander Cameron, Stephen J. Roberts, Tyron Luke Cameron |  |
| 258 |  |  [Neural Processes with Stochastic Attention: Paying more attention to the context dataset](https://openreview.net/forum?id=JPkQwEdYn8) |  | 0 | Neural processes (NPs) aim to stochastically complete unseen data points based on a given context dataset. NPs essentially leverage a given dataset as a context representation to derive a suitable identifier for a novel task. To improve the prediction accuracy, many variants of NPs have investigated context embedding approaches that generally design novel network... | Kyeongryeol Go, Mingyu Kim, SeYoung Yun |  |
| 259 |  |  [Evaluating Disentanglement of Structured Representations](https://openreview.net/forum?id=SLz5sZjacp) |  | 0 | We introduce the first metric for evaluating disentanglement at individual hierarchy levels of a structured latent representation. Applied to object-centric generative models, this offers a systematic, unified approach to evaluating (i) object separation between latent slots (ii) disentanglement of object properties inside individual slots (iii) disentanglement of... | Raphaël DangNhu |  |
| 260 |  |  [Geometric Transformers for Protein Interface Contact Prediction](https://openreview.net/forum?id=CS4463zx6Hi) |  | 0 | Computational methods for predicting the interface contacts between proteins come highly sought after for drug discovery as they can significantly advance the accuracy of alternative approaches, such as protein-protein docking, protein function analysis tools, and other computational methods for protein bioinformatics. In this work, we present the Geometric Transformer,... | Alex Morehead, Chen Chen, Jianlin Cheng |  |
| 261 |  |  [Diurnal or Nocturnal? Federated Learning of Multi-branch Networks from Periodically Shifting Distributions](https://openreview.net/forum?id=E4EE_ohFGz) |  | 0 | Federated learning has been deployed to train machine learning models from decentralized client data on mobile devices in practice. The clients available for training are observed to have periodically shifting distributions changing with the time of day, which can cause instability in training and degrade the model performance. In this paper, instead of modeling the... | Andrew Hard, Chen Zhu, Jakub Konecný, Mingqing Chen, Tom Goldstein, Zheng Xu |  |
| 262 |  |  [IGLU: Efficient GCN Training via Lazy Updates](https://openreview.net/forum?id=5kq11Tl1z4) |  | 0 | Training multi-layer Graph Convolution Networks (GCN) using standard SGD techniques scales poorly as each descent step ends up updating node embeddings for a large portion of the graph. Recent attempts to remedy this sub-sample the graph that reduces compute but introduce additional variance and may offer suboptimal performance. This paper develops the IGLU method that... | Aditya Sinha, Prateek Jain, Purushottam Kar, S. Deepak Narayanan, Sundararajan Sellamanickam |  |
| 263 |  |  [Procedural generalization by planning with self-supervised world models](https://openreview.net/forum?id=FmBegXJToY) |  | 0 | One of the key promises of model-based reinforcement learning is the ability to generalize using an internal model of the world to make predictions in novel environments and tasks. However, the generalization ability of model-based agents is not well understood because existing work has focused on model-free agents when benchmarking generalization. Here, we explicitly... | Ankesh Anand, Eszter Vértes, Jacob C. Walker, Jessica B. Hamrick, Julian Schrittwieser, Sherjil Ozair, Theophane Weber, Yazhe Li |  |
| 264 |  |  [Top-N: Equivariant Set and Graph Generation without Exchangeability](https://openreview.net/forum?id=-Gk_IPJWvk) |  | 0 | This work addresses one-shot set and graph generation, and, more specifically, the parametrization of probabilistic decoders that map a vector-shaped prior to a distribution over sets or graphs. Sets and graphs are most commonly generated by first sampling points i.i.d. from a normal distribution, and then processing these points along with the prior vector using... | Clément Vignac, Pascal Frossard |  |
| 265 |  |  [The Spectral Bias of Polynomial Neural Networks](https://openreview.net/forum?id=P7FLfMLTSEX) |  | 0 | Polynomial neural networks (PNNs) have been recently shown to be particularly effective at image generation and face recognition, where high-frequency information is critical. Previous studies have revealed that neural networks demonstrate a $\text{\it{spectral bias}}$ towards low-frequency functions, which yields faster learning of low-frequency components during... | Grigorios Chrysos, Julien Mairal, Leello Tadesse Dadi, Moulik Choraria, Volkan Cevher |  |
| 266 |  |  [Invariant Causal Representation Learning for Out-of-Distribution Generalization](https://openreview.net/forum?id=-e4EXDWXnSn) |  | 0 | Due to spurious correlations, machine learning systems often fail to generalize to environments whose distributions differ from the ones used at training time. Prior work addressing this, either explicitly or implicitly, attempted to find a data representation that has an invariant relationship with the target. This is done by leveraging a diverse set of training... | Bernhard Schölkopf, Chaochao Lu, José Miguel HernándezLobato, Yuhuai Wu |  |
| 267 |  |  [LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5](https://openreview.net/forum?id=HCRVf71PMF) |  | 0 | Existing approaches to lifelong language learning rely on plenty of labeled data for learning a new task, which is hard to obtain in most real scenarios. Considering that humans can continually learn new tasks from a handful of examples, we expect the models also to be able to generalize well on new few-shot tasks without forgetting the previous ones. In this work, we... | Chengwei Qin, Shafiq R. Joty |  |
| 268 |  |  [On Non-Random Missing Labels in Semi-Supervised Learning](https://openreview.net/forum?id=6yVvwR9H9Oj) |  | 0 | Semi-Supervised Learning (SSL) is fundamentally a missing label problem, in which the label Missing Not At Random (MNAR) problem is more realistic and challenging, compared to the widely-adopted yet naive Missing Completely At Random assumption where both labeled and unlabeled data share the same class distribution. Different from existing SSL solutions that overlook... | Chunyan Miao, Hanwang Zhang, XianSheng Hua, Xinting Hu, Yulei Niu |  |
| 269 |  |  [Mapping conditional distributions for domain adaptation under generalized target shift](https://openreview.net/forum?id=sPfB2PI87BZ) |  | 0 | We consider the problem of unsupervised domain adaptation (UDA) between a source and a target domain under conditional and label shift a.k.a Generalized Target Shift (GeTarS). Unlike simpler UDA settings, few works have addressed this challenging problem. Recent approaches learn domain-invariant representations, yet they have practical limitations and rely on strong... | Alain Rakotomamonjy, Emmanuel de Bézenac, Matthieu Kirchmeyer, Patrick Gallinari |  |
| 270 |  |  [On the Generalization of Models Trained with SGD: Information-Theoretic Bounds and Implications](https://openreview.net/forum?id=oWZsQ8o5EA) |  | 0 | This paper follows up on a recent work of Neu et al. (2021) and presents some new information-theoretic upper bounds for the generalization error of machine learning models, such as neural networks, trained with SGD. We apply these bounds to analyzing the generalization behaviour of linear and two-layer ReLU networks. Experimental study of these bounds provide some... | Yongyi Mao, Ziqiao Wang |  |
| 271 |  |  [Amortized Implicit Differentiation for Stochastic Bilevel Optimization](https://openreview.net/forum?id=3PN4iyXBeF) |  | 0 | We study a class of algorithms for solving bilevel optimization problems in both stochastic and deterministic settings when the inner-level objective is strongly convex. Specifically, we consider algorithms based on inexact implicit differentiation and we exploit a warm-start strategy to amortize the estimation of the exact gradient. We then introduce a unified... | Julien Mairal, Michael Arbel |  |
| 272 |  |  [Multi-objective Optimization by Learning Space Partition](https://openreview.net/forum?id=FlwzVjfMryn) |  | 0 | In contrast to single-objective optimization (SOO), multi-objective optimization (MOO) requires an optimizer to find the Pareto frontier, a subset of feasible solutions that are not dominated by other feasible solutions. In this paper, we propose LaMOO, a novel multi-objective optimizer that learns a model from observed samples to partition the search space and then... | Kevin Yang, Linnan Wang, Tian Guo, Tianjun Zhang, Yiyang Zhao, Yuandong Tian |  |
| 273 |  |  [Mapping Language Models to Grounded Conceptual Spaces](https://openreview.net/forum?id=gJcEM8sxHK) |  | 0 | A fundamental criticism of text-only language models (LMs) is their lack of grounding---that is, the ability to tie a word for which they have learned a representation, to its actual use in the world. However, despite this limitation, large pre-trained LMs have been shown to have a remarkable grasp of the conceptual structure of language, as demonstrated by their... | Ellie Pavlick, Roma Patel |  |
| 274 |  |  [The Efficiency Misnomer](https://openreview.net/forum?id=iulEMLYh1uR) |  | 0 | Model efficiency is a critical aspect of developing and deploying machine learning models. Inference time and latency directly affect the user experience, and some applications have hard requirements. In addition to inference costs, model training also have direct financial and environmental impacts. Although there are numerous well-established metrics (cost indicators)... | Anurag Arnab, Ashish Vaswani, Lucas Beyer, Mostafa Dehghani, Yi Tay |  |
| 275 |  |  [Hybrid Memoised Wake-Sleep: Approximate Inference at the Discrete-Continuous Interface](https://openreview.net/forum?id=auOPcdAcoy) |  | 0 | Modeling complex phenomena typically involves the use of both discrete and continuous variables. Such a setting applies across a wide range of problems, from identifying trends in time-series data to performing effective compositional scene understanding in images. Here, we propose Hybrid Memoised Wake-Sleep (HMWS), an algorithm for effective inference in such hybrid... | Joshua B. Tenenbaum, Katherine M. Collins, Kevin Ellis, Luke Hewitt, Samuel Gershman, Siddharth Narayanaswamy, Tuan Anh Le |  |
| 276 |  |  [Adversarial Retriever-Ranker for Dense Text Retrieval](https://openreview.net/forum?id=MR7XubKUFB) |  | 0 | Current dense text retrieval models face two typical challenges. First, it adopts a siamese dual-encoder architecture to encode query and document independently for fast indexing and searching, whereas neglecting the finer-grained term-wise interactions. This results in a sub-optimal recall performance. Second, it highly relies on a negative sampling technique to build... | Hang Zhang, Jiancheng Lv, Nan Duan, Weizhu Chen, Yelong Shen, Yeyun Gong |  |
| 277 |  |  [Conditioning Sequence-to-sequence Networks with Learned Activations](https://openreview.net/forum?id=t5s-hd1bqLk) |  | 0 | Conditional neural networks play an important role in a number of sequence-to-sequence modeling tasks, including personalized sound enhancement (PSE), speaker dependent automatic speech recognition (ASR), and generative modeling such as text-to-speech synthesis. In conditional neural networks, the output of a model is often influenced by a conditioning vector, in... | Abhinav Mehrotra, Alberto Gil Couto Pimentel Ramos, Nicholas Donald Lane, Sourav Bhattacharya |  |
| 278 |  |  [Lossy Compression with Distribution Shift as Entropy Constrained Optimal Transport](https://openreview.net/forum?id=BRFWxcZfAdC) |  | 0 | We study an extension of lossy compression where the reconstruction distribution is different from the source distribution in order to account for distributional shift due to processing. We formulate this as a generalization of optimal transport with an entropy bottleneck to account for the rate constraint due to compression. We provide expressions for the tradeoff... | Ashish J. Khisti, George Zhang, Huan Liu, Jun Chen |  |
| 279 |  |  [Equivariant Self-Supervised Learning: Encouraging Equivariance in Representations](https://openreview.net/forum?id=gKLAAfiytI) |  | 0 | In state-of-the-art self-supervised learning (SSL) pre-training produces semantically good representations by encouraging them to be invariant under meaningful transformations prescribed from human knowledge. In fact, the property of invariance is a trivial instance of a broader class called equivariance, which can be intuitively understood as the property that... | Akash Srivastava, Brian Cheung, Charlotte Loh, Li Jing, Marin Soljacic, Pulkit Agrawal, Rumen Dangovski, Seungwook Han |  |
| 280 |  |  [Direct then Diffuse: Incremental Unsupervised Skill Discovery for State Covering and Goal Reaching](https://openreview.net/forum?id=25kzAhUB1lz) |  | 0 | Learning meaningful behaviors in the absence of reward is a difficult problem in reinforcement learning. A desirable and challenging unsupervised objective is to learn a set of diverse skills that provide a thorough coverage of the state space while being directed, i.e., reliably reaching distinct regions of the environment. In this paper, we build on the mutual... | Alessandro Lazaric, Jean Tarbouriech, Ludovic Denoyer, PierreAlexandre Kamienny, Sylvain Lamprier |  |
| 281 |  |  [Normalization of Language Embeddings for Cross-Lingual Alignment](https://openreview.net/forum?id=Nh7CtbyoqV5) |  | 0 | Learning a good transfer function to map the word vectors from two languages into a shared cross-lingual word vector space plays a crucial role in cross-lingual NLP. It is useful in translation tasks and important in allowing complex models built on a high-resource language like English to be directly applied on an aligned low resource language. While Procrustes and... | ChinChia Michael Yeh, Hao Yang, Jeff M. Phillips, Junpeng Wang, Liang Wang, Prince Osei Aboagye, Wei Zhang, Yan Zheng |  |
| 282 |  |  [Boosting the Certified Robustness of L-infinity Distance Nets](https://openreview.net/forum?id=Q76Y7wkiji) |  | 0 | Recently, Zhang et al. (2021) developed a new neural network architecture based on $\ell_\infty$-distance functions, which naturally possesses certified $\ell_\infty$ robustness by its construction. Despite the novel design and theoretical foundation, so far the model only achieved comparable performance to conventional networks. In this paper, we make the following two... | Bohang Zhang, Di He, Du Jiang, Liwei Wang |  |
| 283 |  |  [Stochastic Training is Not Necessary for Generalization](https://openreview.net/forum?id=ZBESeIUB5k) |  | 0 | It is widely believed that the implicit regularization of SGD is fundamental to the impressive generalization behavior we observe in neural networks. In this work, we demonstrate that non-stochastic full-batch training can achieve comparably strong performance to SGD on CIFAR-10 using modern architectures. To this end, we show that the implicit regularization of SGD can... | Jonas Geiping, Micah Goldblum, Michael Moeller, Phillip Pope, Tom Goldstein |  |
| 284 |  |  [Transfer RL across Observation Feature Spaces via Model-Based Regularization](https://openreview.net/forum?id=7KdAoOsI81C) |  | 0 | In many reinforcement learning (RL) applications, the observation space is specified by human developers and restricted by physical realizations, and may thus be subject to dramatic changes over time (e.g. increased number of observable features). However, when the observation space changes, the previous policy will likely fail due to the mismatch of input features, and... | Andrew E. Cohen, Furong Huang, Ruijie Zheng, Xiyao Wang, Yanchao Sun |  |
| 285 |  |  [GATSBI: Generative Adversarial Training for Simulation-Based Inference](https://openreview.net/forum?id=kR1hC6j48Tp) |  | 0 | Simulation-based inference (SBI) refers to statistical inference on stochastic models for which we can generate samples, but not compute likelihoods. Like SBI algorithms, generative adversarial networks (GANs) do not require explicit likelihoods. We study the relationship between SBI and GANs, and introduce GATSBI, an adversarial approach to SBI. GATSBI reformulates the... | David S. Greenberg, Jakob H. Macke, Jan Boelts, JanMatthis Lueckmann, Pedro J. Gonçalves, Poornima Ramesh, Álvaro TejeroCantero |  |
| 286 |  |  [Domain Adversarial Training: A Game Perspective](https://openreview.net/forum?id=AwgtcUAhBq) |  | 0 | The dominant line of work in domain adaptation has focused on learning invariant representations using domain-adversarial training. In this paper, we interpret this approach from a game theoretical perspective. Defining optimal solutions in domain-adversarial training as a local Nash equilibrium, we show that gradient descent in domain-adversarial training can violate... | David Acuna, Guojun Zhang, Marc T. Law, Sanja Fidler |  |
| 287 |  |  [Differentiable Expectation-Maximization for Set Representation Learning](https://openreview.net/forum?id=MXdFBmHT4C) |  | 0 | We tackle the set2vec problem, the task of extracting a vector representation from an input set comprised of a variable number of feature vectors. Although recent approaches based on self attention such as (Set)Transformers were very successful due to the capability of capturing complex interaction between set elements, the computational overhead is the well-known... | Minyoung Kim |  |
| 288 |  |  [Overcoming The Spectral Bias of Neural Value Approximation](https://openreview.net/forum?id=vIC-xLFuM6) |  | 0 | Value approximation using deep neural networks is at the heart of off-policy deep reinforcement learning, and is often the primary module that provides learning signals to the rest of the algorithm. While multi-layer perceptron networks are universal function approximators, recent works in neural kernel regression suggest the presence of a \textit{spectral bias}, where... | Anurag Ajay, Ge Yang, Pulkit Agrawal |  |
| 289 |  |  [Prospect Pruning: Finding Trainable Weights at Initialization using Meta-Gradients](https://openreview.net/forum?id=AIgn9uwfcD1) |  | 0 | Pruning neural networks at initialization would enable us to find sparse models that retain the accuracy of the original network while consuming fewer computational resources for training and inference. However, current methods are insufficient to enable this optimization and lead to a large degradation in model performance. In this paper, we identify a fundamental... | Joost van Amersfoort, Luisa M. Zintgraf, Milad Alizadeh, Nicholas Donald Lane, Sebastian Farquhar, Shyam A. Tailor, Yarin Gal |  |
| 290 |  |  [CoMPS: Continual Meta Policy Search](https://openreview.net/forum?id=PVJ6j87gOHz) |  | 0 | We develop a new continual meta-learning method to address challenges in sequential multi-task learning. In this setting, the agent's goal is to achieve high reward over any sequence of tasks quickly. Prior meta-reinforcement learning algorithms have demonstrated promising results in accelerating the acquisition of new tasks. However, they require access to all tasks... | Chelsea Finn, Glen Berseth, Grace Zhang, Sergey Levine, Zhiwei Zhang |  |
| 291 |  |  [Generalized rectifier wavelet covariance models for texture synthesis](https://openreview.net/forum?id=ziRLU3Y2PN_) |  | 0 | State-of-the-art maximum entropy models for texture synthesis are built from statistics relying on image representations defined by convolutional neural networks (CNN). Such representations capture rich structures in texture images, outperforming wavelet-based representations in this regard. However, conversely to neural networks, wavelets offer meaningful... | Antoine Brochard, Sixin Zhang, Stéphane Mallat |  |
| 292 |  |  [Towards Evaluating the Robustness of Neural Networks Learned by Transduction](https://openreview.net/forum?id=_5js_8uTrx1) |  | 0 | There has been emerging interest in using transductive learning for adversarial robustness (Goldwasser et al., NeurIPS 2020; Wu et al., ICML 2020; Wang et al., ArXiv 2021). Compared to traditional defenses, these defense mechanisms "dynamically learn" the model based on test-time input; and theoretically, attacking these defenses reduces to solving a bilevel... | Jiefeng Chen, Somesh Jha, Xi Wu, Yang Guo, Yingyu Liang |  |
| 293 |  |  [Object Dynamics Distillation for Scene Decomposition and Representation](https://openreview.net/forum?id=oJGDYQFKL3i) |  | 0 | The ability to perceive scenes in terms of abstract entities is crucial for us to achieve higher-level intelligence. Recently, several methods have been proposed to learn object-centric representations of scenes with multiple objects, yet most of which focus on static scenes. In this paper, we work on object dynamics and propose Object Dynamics Distillation Network... | Qu Tang, Xiangyu Zhu, Zhaoxiang Zhang, Zhen Lei |  |
| 294 |  |  [Practical Integration via Separable Bijective Networks](https://openreview.net/forum?id=NlObxR0rosG) |  | 0 | Neural networks have enabled learning over examples that contain thousands of dimensions. However, most of these models are limited to training and evaluating on a finite collection of \textit{points} and do not consider the hypervolume in which the data resides. Any analysis of the model's local or global behavior is therefore limited to very expensive or imprecise... | Christopher M. Bender, Junier Oliva, Michael K. Reiter, Patrick Emmanuel |  |
| 295 |  |  [Self-Joint Supervised Learning](https://openreview.net/forum?id=zuqcmNVK4c2) |  | 0 | Supervised learning is a fundamental framework used to train machine learning systems. A supervised learning problem is often formulated using an i.i.d. assumption that restricts model attention to a single relevant signal at a time when predicting. This contrasts with the human ability to actively use related samples as reference when making decisions. We hypothesize... | Mitch Hill, Mubarak Shah, Navid Kardan |  |
| 296 |  |  [Rethinking Supervised Pre-Training for Better Downstream Transferring](https://openreview.net/forum?id=Jjcv9MTqhcq) |  | 0 | The pretrain-finetune paradigm has shown outstanding performance on many applications of deep learning, where a model is pre-trained on an upstream large dataset (e.g. ImageNet), and is then fine-tuned to different downstream tasks. Though for most cases, the pre-training stage is conducted based on supervised methods, recent works on self-supervised pre-training have... | Jianwen Jiang, Mingqian Tang, Rong Jin, Yue Gao, Yutong Feng |  |
| 297 |  |  [A Zest of LIME: Towards Architecture-Independent Model Distances](https://openreview.net/forum?id=OUz_9TiTv9j) |  | 0 | Definitions of the distance between two machine learning models either characterize the similarity of the models' predictions or of their weights. While similarity of weights is attractive because it implies similarity of predictions in the limit, it suffers from being inapplicable to comparing models with different architectures. On the other hand, the similarity of... | Ali Shahin Shamsabadi, Hengrui Jia, Hongyu Chen, Jonas Guan, Nicolas Papernot |  |
| 298 |  |  [Meta-Imitation Learning by Watching Video Demonstrations](https://openreview.net/forum?id=KTPuIsx4pmo) |  | 0 | Meta-Imitation Learning is a promising technique for the robot to learn a new task from observing one or a few human demonstrations. However, it usually requires a significant number of demonstrations both from humans and robots during the meta-training phase, which is a laborious and hard work for data collection, especially in recording the actions and specifying the... | Jiayi Li, Shuo Wang, Tao Lu, Xiaoge Cao, Yinghao Cai |  |
| 299 |  |  [Understanding Intrinsic Robustness Using Label Uncertainty](https://openreview.net/forum?id=6ET9SzlgNX) |  | 0 | A fundamental question in adversarial machine learning is whether a robust classifier exists for a given task. A line of research has made some progress towards this goal by studying the concentration of measure, but we argue standard concentration fails to fully characterize the intrinsic robustness of a classification problem since it ignores data labels which are... | David E. Evans, Xiao Zhang |  |
| 300 |  |  [Efficient Split-Mix Federated Learning for On-Demand and In-Situ Customization](https://openreview.net/forum?id=_QLmakITKg) |  | 0 | Federated learning (FL) provides a distributed learning framework for multiple participants to collaborate learning without sharing raw data. In many practical FL scenarios, participants have heterogeneous resources due to disparities in hardware and inference dynamics that require quickly loading models of different sizes and levels of robustness. The heterogeneity and... | Haotao Wang, Jiayu Zhou, Junyuan Hong, Zhangyang Wang |  |
| 301 |  |  [Anti-Concentrated Confidence Bonuses For Scalable Exploration](https://openreview.net/forum?id=RXQ-FPbQYVn) |  | 0 | Intrinsic rewards play a central role in handling the exploration-exploitation tradeoff when designing sequential decision-making algorithms, in both foundational theory and state-of-the-art deep reinforcement learning. The LinUCB algorithm, a centerpiece of the stochastic linear bandits literature, prescribes an elliptical bonus which addresses the challenge of... | Akshay Krishnamurthy, Cyril Zhang, Jordan T. Ash, Sham M. Kakade, Surbhi Goel |  |
| 302 |  |  [Sqrt(d) Dimension Dependence of Langevin Monte Carlo](https://openreview.net/forum?id=5-2mX9_U5i) |  | 0 | This article considers the popular MCMC method of unadjusted Langevin Monte Carlo (LMC) and provides a non-asymptotic analysis of its sampling error in 2-Wasserstein distance. The proof is based on a refinement of mean-square analysis in Li et al. (2019), and this refined framework automates the analysis of a large class of sampling algorithms based on discretizations... | Hongyuan Zha, Molei Tao, Ruilin Li |  |
| 303 |  |  [Relational Surrogate Loss Learning](https://openreview.net/forum?id=dZPgfwaTaXv) |  | 0 | Evaluation metrics in machine learning are often hardly taken as loss functions, as they could be non-differentiable and non-decomposable, e.g., average precision and F1 score. This paper aims to address this problem by revisiting the surrogate loss learning, where a deep neural network is employed to approximate the evaluation metrics. Instead of pursuing an exact... | Chang Xu, Fei Wang, Hua Lu, Shan You, Shusheng Yang, Tao Huang, Yang Feng, Yong Shan, Zekang Li |  |
| 304 |  |  [Structure-Aware Transformer Policy for Inhomogeneous Multi-Task Reinforcement Learning](https://openreview.net/forum?id=fy_XRVHqly) |  | 0 | Modular Reinforcement Learning, where the agent is assumed to be morphologically structured as a graph, for example composed of limbs and joints, aims to learn a policy that is transferable to a structurally similar but different agent. Compared to traditional Multi-Task Reinforcement Learning, this promising approach allows us to cope with inhomogeneous tasks where the... | Deunsol Yoon, KeeEung Kim, Sunghoon Hong |  |
| 305 |  |  [Toward Efficient Low-Precision Training: Data Format Optimization and Hysteresis Quantization](https://openreview.net/forum?id=3HJOA-1hb0e) |  | 0 | As the complexity and size of deep neural networks continue to increase, low-precision training has been extensively studied in the last few years to reduce hardware overhead. Training performance is largely affected by the numeric formats representing different values in low-precision training, but finding an optimal format typically requires numerous training runs,... | Dongsuk Jeon, Jeongwoo Park, Sunwoo Lee |  |
| 306 |  |  [Knowledge Infused Decoding](https://openreview.net/forum?id=upnDJ7itech) |  | 0 | Pre-trained language models (LMs) have been shown to memorize a substantial amount of knowledge from the pre-training corpora; however, they are still limited in recalling factually correct knowledge given a certain context. Hence. they tend to suffer from counterfactual or hallucinatory generation when used in knowledge-intensive natural language generation (NLG)... | Ahmed Hassan Awadallah, Chongyang Gao, Guoqing Zheng, Milad Shokouhi, Radhika Gaonkar, Ruibo Liu, Shashank Gupta, Soroush Vosoughi |  |
| 307 |  |  [Parallel Training of GRU Networks with a Multi-Grid Solver for Long Sequences](https://openreview.net/forum?id=N1WI0vJLER) |  | 0 | Parallelizing Gated Recurrent Unit (GRU) is a challenging task, as the training procedure of GRU is inherently sequential. Prior efforts to parallelize GRU have largely focused on conventional parallelization strategies such as data-parallel and model-parallel training algorithms. However, when the given sequences are very long, existing approaches are still inevitably... | Eric C. Cyr, Euhyun Moon |  |
| 308 |  |  [Query Efficient Decision Based Sparse Attacks Against Black-Box Deep Learning Models](https://openreview.net/forum?id=73MEhZ0anV) |  | 0 | Despite our best efforts, deep learning models remain highly vulnerable to even tiny adversarial perturbations applied to the inputs. The ability to extract information from solely the output of a machine learning model to craft adversarial perturbations to black-box models is a practical threat against real-world systems, such as Machine Learning as a Service (MLaaS),... | Damith Ranasinghe, Ehsan Abbasnejad, Viet Quoc Vo |  |
| 309 |  |  [Almost Tight L0-norm Certified Robustness of Top-k Predictions against Adversarial Perturbations](https://openreview.net/forum?id=gJLEXy3ySpu) |  | 0 | Top-$k$ predictions are used in many real-world applications such as machine learning as a service, recommender systems, and web searches. $\ell_0$-norm adversarial perturbation characterizes an attack that arbitrarily modifies some features of an input such that a classifier makes an incorrect prediction for the perturbed input. $\ell_0$-norm adversarial perturbation... | Binghui Wang, Hongbin Liu, Jinyuan Jia, Neil Zhenqiang Gong, Xiaoyu Cao |  |
| 310 |  |  [Proving the Lottery Ticket Hypothesis for Convolutional Neural Networks](https://openreview.net/forum?id=Vjki79-619-) |  | 0 | The lottery ticket hypothesis states that a randomly-initialized neural network contains a small subnetwork which, when trained in isolation, can compete with the performance of the original network. Recent theoretical works proved an even stronger version: every sufficiently overparameterized (dense) neural network contains a subnetwork that, even without training,... | Arthur da Cunha, Emanuele Natale, Laurent Viennot |  |
| 311 |  |  [Discovering Nonlinear PDEs from Scarce Data with Physics-encoded Learning](https://openreview.net/forum?id=Vog_3GXsgmb) |  | 0 | There have been growing interests in leveraging experimental measurements to discover the underlying partial differential equations (PDEs) that govern complex physical phenomena. Although past research attempts have achieved great success in data-driven PDE discovery, the robustness of the existing methods cannot be guaranteed when dealing with low-quality measurement... | Chengping Rao, Hao Sun, Pu Ren, Yang Liu |  |
| 312 |  |  [Rethinking Goal-Conditioned Supervised Learning and Its Connection to Offline RL](https://openreview.net/forum?id=KJztlfGPdwW) |  | 0 | Solving goal-conditioned tasks with sparse rewards using self-supervised learning is promising because of its simplicity and stability over current reinforcement learning (RL) algorithms. A recent work, called Goal-Conditioned Supervised Learning (GCSL), provides a new learning framework by iteratively relabeling and imitating self-generated experiences. In this paper,... | Chongjie Zhang, Hao Sun, Lei Han, Meng Fang, Rui Yang, Wenzhe Li, Xiu Li, Yali Du, Yiming Lu |  |
| 313 |  |  [Topologically Regularized Data Embeddings](https://openreview.net/forum?id=P1QUVhOtEFP) |  | 0 | Unsupervised feature learning often finds low-dimensional embeddings that capture the structure of complex data. For tasks for which prior expert topological knowledge is available, incorporating this into the learned representation may lead to higher quality embeddings. For example, this may help one to embed the data into a given number of clusters, or to accommodate... | Bo Kang, Jefrey Lijffijt, Robin Vandaele, Tijl De Bie, Yvan Saeys |  |
| 314 |  |  [PF-GNN: Differentiable particle filtering based approximation of universal graph representations](https://openreview.net/forum?id=oh4TirnfSem) |  | 0 | Message passing Graph Neural Networks (GNNs) are known to be limited in expressive power by the 1-WL color-refinement test for graph isomorphism. Other more expressive models either are computationally expensive or need preprocessing to extract structural features from the graph. In this work, we propose to make GNNs universal by guiding the learning process with exact... | Mohammed Haroon Dupty, Wee Sun Lee, Yanfei Dong |  |
| 315 |  |  [Nonlinear ICA Using Volume-Preserving Transformations](https://openreview.net/forum?id=AMpki9kp8Cn) |  | 0 | Nonlinear ICA is a fundamental problem in machine learning, aiming to identify the underlying independent components (sources) from data which is assumed to be a nonlinear function (mixing function) of these sources. Recent works prove that if the sources have some particular structures (e.g. temporal structure), they are theoretically identifiable even if the mixing... | Jiacheng Sun, Junchi Yan, Shifeng Zhang, Xiaojiang Yang, Xing Zhang, Yi Wang, Zhenguo Li |  |
| 316 |  |  [Online Ad Hoc Teamwork under Partial Observability](https://openreview.net/forum?id=18Ys0-PzyPI) |  | 0 | Autonomous agents often need to work together as a team to accomplish complex cooperative tasks. Due to privacy and other realistic constraints, agents might need to collaborate with previously unknown teammates on the fly. This problem is known as ad hoc teamwork, which remains a core research challenge. Prior works usually rely heavily on strong assumptions like full... | Bo An, Jianye Hao, Mengchen Zhao, Pengjie Gu |  |
| 317 |  |  [Continual Normalization: Rethinking Batch Normalization for Online Continual Learning](https://openreview.net/forum?id=vwLLQ-HwqhZ) |  | 0 | Existing continual learning methods use Batch Normalization (BN) to facilitate training and improve generalization across tasks. However, the non-i.i.d and non-stationary nature of continual learning data, especially in the online setting, amplify the discrepancy between training and testing in BN and hinder the performance of older tasks. In this work, we study the... | Chenghao Liu, Quang Pham, Steven C. H. Hoi |  |
| 318 |  |  [Equivariant Graph Mechanics Networks with Constraints](https://openreview.net/forum?id=SHbhHHfePhP) |  | 0 | Learning to reason about relations and dynamics over multiple interacting objects is a challenging topic in machine learning. The challenges mainly stem from that the interacting systems are exponentially-compositional, symmetrical, and commonly geometrically-constrained. Current methods, particularly the ones based on equivariant Graph Neural Networks (GNNs), have... | Fuchun Sun, Jiaqi Han, Junzhou Huang, Tingyang Xu, Wenbing Huang, Yu Rong |  |
| 319 |  |  [Towards Continual Knowledge Learning of Language Models](https://openreview.net/forum?id=vfsRB5MImo9) |  | 0 | Large Language Models (LMs) are known to encode world knowledge in their parameters as they pretrain on a vast amount of web corpus, which is often utilized for performing knowledge-dependent downstream tasks such as question answering, fact-checking, and open dialogue. In real-world scenarios, the world knowledge stored in the LMs can quickly become outdated as the... | Gyeonghun Kim, Janghoon Han, Joel Jang, Joongbo Shin, Minjoon Seo, Seonghyeon Ye, Sohee Yang, Stanley Jungkyu Choi |  |
| 320 |  |  [Surreal-GAN: Semi-Supervised Representation Learning via GAN for uncovering heterogeneous disease-related imaging patterns](https://openreview.net/forum?id=nf3A0WZsXS5) |  | 0 | A plethora of machine learning methods have been applied to imaging data, enabling the construction of clinically relevant imaging signatures of neurological and neuropsychiatric diseases. Oftentimes, such methods don't explicitly model the heterogeneity of disease effects, or approach it via nonlinear models that are not interpretable. Moreover, unsupervised methods... | Christos Davatzikos, Junhao Wen, Zhijian Yang |  |
| 321 |  |  [SURF: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning](https://openreview.net/forum?id=TfhfZLQ2EJO) |  | 0 | Preference-based reinforcement learning (RL) has shown potential for teaching agents to perform the target tasks without a costly, pre-defined reward function by learning the reward with a supervisor’s preference between the two agent behaviors. However, preference-based learning often requires a large amount of human feedback, making it difficult to apply this approach... | Honglak Lee, Jinwoo Shin, Jongjin Park, Kimin Lee, Pieter Abbeel, Younggyo Seo |  |
| 322 |  |  [Convergent Graph Solvers](https://openreview.net/forum?id=ItkxLQU01lD) |  | 0 | We propose the convergent graph solver (CGS), a deep learning method that learns iterative mappings to predict the properties of a graph system at its stationary state (fixed point) with guaranteed convergence. The forward propagation of CGS proceeds in three steps: (1) constructing the input-dependent linear contracting iterative maps, (2) computing the fixed points of... | Jinhyun Choo, Jinkyoo Park, Junyoung Park |  |
| 323 |  |  [Spread Spurious Attribute: Improving Worst-group Accuracy with Spurious Attribute Estimation](https://openreview.net/forum?id=_F9xpOrqyX9) |  | 0 | The paradigm of worst-group loss minimization has shown its promise in avoiding to learn spurious correlations, but requires costly additional supervision on spurious attributes. To resolve this, recent works focus on developing weaker forms of supervision---e.g., hyperparameters discovered with a small number of validation samples with spurious attribute... | Jaeho Lee, Jaehyung Kim, Jinwoo Shin, Jun Hyun Nam |  |
| 324 |  |  [Learning Scenario Representation for Solving Two-stage Stochastic Integer Programs](https://openreview.net/forum?id=06Wy2BtxXrz) |  | 0 | Many practical combinatorial optimization problems under uncertainty can be modeled as stochastic integer programs (SIPs), which are extremely challenging to solve due to the high complexity. To solve two-stage SIPs efficiently, we propose a conditional variational autoencoder (CVAE) based method to learn scenario representation for a class of SIP instances.... | Jie Zhang, Wen Song, Yaoxin Wu, Zhiguang Cao |  |
| 325 |  |  [Generalization Through the Lens of Leave-One-Out Error](https://openreview.net/forum?id=7grkzyj89A_) |  | 0 | Despite the tremendous empirical success of deep learning models to solve various learning tasks, our theoretical understanding of their generalization ability is very limited. Classical generalization bounds based on tools such as the VC dimension or Rademacher complexity, are so far unsuitable for deep models and it is doubtful that these techniques can yield tight... | Aurélien Lucchi, Gregor Bachmann, Thomas Hofmann |  |
| 326 |  |  [Self-Supervised Inference in State-Space Models](https://openreview.net/forum?id=VPjw9KPWRSK) |  | 0 | We perform approximate inference in state-space models with nonlinear state transitions. Without parameterizing a generative model, we apply Bayesian update formulas using a local linearity approximation parameterized by neural networks. It comes accompanied by a maximum likelihood objective that requires no supervision via uncorrupt observations or ground truth latent... | David Ruhe, Patrick Forré |  |
| 327 |  |  [On the Role of Neural Collapse in Transfer Learning](https://openreview.net/forum?id=SwIp410B6aQ) |  | 0 | We study the ability of foundation models to learn representations for classification that are transferable to new, unseen classes. Recent results in the literature show that representations learned by a single classifier over many classes are competitive on few-shot learning problems with representations learned by special-purpose algorithms designed for such problems.... | András György, Marcus Hutter, Tomer Galanti |  |
| 328 |  |  [Information-theoretic Online Memory Selection for Continual Learning](https://openreview.net/forum?id=IpctgL7khPp) |  | 0 | A challenging problem in task-free continual learning is the online selection of a representative replay memory from data streams. In this work, we investigate the online memory selection problem from an information-theoretic perspective. To gather the most information, we propose the \textit{surprise} and the \textit{learnability} criteria to pick informative points... | Ang Li, Daniele Calandriello, Huiyi Hu, Michalis K. Titsias, Shengyang Sun |  |
| 329 |  |  [Dealing with Non-Stationarity in MARL via Trust-Region Decomposition](https://openreview.net/forum?id=XHUxf5aRB3s) |  | 0 | Non-stationarity is one thorny issue in cooperative multi-agent reinforcement learning (MARL). One of the reasons is the policy changes of agents during the learning process. Some existing works have discussed various consequences caused by non-stationarity with several kinds of measurement indicators. This makes the objectives or goals of existing algorithms are... | Bo Jin, Hongyuan Zha, Junjie Sheng, Wenhao Li, Xiangfeng Wang |  |
| 330 |  |  [Information Bottleneck: Exact Analysis of (Quantized) Neural Networks](https://openreview.net/forum?id=kF9DZQQrU0w) |  | 0 | The information bottleneck (IB) principle has been suggested as a way to analyze deep neural networks. The learning dynamics are studied by inspecting the mutual information (MI) between the hidden layers and the input and output. Notably, separate fitting and compression phases during training have been reported. This led to some controversy including claims that the... | Christian Igel, Mads Nielsen, Stephan Sloth Lorenzen |  |
| 331 |  |  [GLASS: GNN with Labeling Tricks for Subgraph Representation Learning](https://openreview.net/forum?id=XLxhEjKNbXj) |  | 0 | Despite the remarkable achievements of Graph Neural Networks (GNNs) on graph representation learning, few works have tried to use them to predict properties of subgraphs in the whole graph. The existing state-of-the-art method SubGNN introduces an overly complicated subgraph-level GNN model which synthesizes three artificial channels each of which has two carefully... | Muhan Zhang, Xiyuan Wang |  |
| 332 |  |  [MoReL: Multi-omics Relational Learning](https://openreview.net/forum?id=DnG75_KyHjX) |  | 0 | Multi-omics data analysis has the potential to discover hidden molecular interactions, revealing potential regulatory and/or signal transduction pathways for cellular processes of interest when studying life and disease systems. One of critical challenges when dealing with real-world multi-omics data is that they may manifest heterogeneous structures and data quality as... | Arman Hasanzadeh, Ehsan Hajiramezanali, Nick Duffield, Xiaoning Qian |  |
| 333 |  |  [Provable Learning-based Algorithm For Sparse Recovery](https://openreview.net/forum?id=BwPaPxwgyQb) |  | 0 | Recovering sparse parameters from observational data is a fundamental problem in machine learning with wide applications. Many classic algorithms can solve this problem with theoretical guarantees, but their performances rely on choosing the correct hyperparameters. Besides, hand-designed algorithms do not fully exploit the particular problem distribution of interest.... | Haoran Sun, Le Song, Xinshi Chen |  |
| 334 |  |  [Defending Against Image Corruptions Through Adversarial Augmentations](https://openreview.net/forum?id=jJOjjiZHy3h) |  | 0 | Modern neural networks excel at image classification, yet they remain vulnerable to common image corruptions such as blur, speckle noise or fog. Recent methods that focus on this problem, such as AugMix and DeepAugment, introduce defenses that operate in expectation over a distribution of image corruptions. In contrast, the literature on Lp-norm bounded perturbations... | András György, Dan Andrei Calian, Florian Stimberg, Olivia Wiles, Sven Gowal, SylvestreAlvise Rebuffi, Timothy A. Mann |  |
| 335 |  |  [Attacking deep networks with surrogate-based adversarial black-box methods is easy](https://openreview.net/forum?id=Zf4ZdI4OQPV) |  | 0 | A recent line of work on black-box adversarial attacks has revived the use of transfer from surrogate models by integrating it into query-based search. However, we find that existing approaches of this type underperform their potential, and can be overly complicated besides. Here, we provide a short and simple algorithm which achieves state-of-the-art results through a... | Luca Bertinetto, Nicholas A. Lord, Romain Müller |  |
| 336 |  |  [Autoregressive Diffusion Models](https://openreview.net/forum?id=Lm8T39vLDTE) |  | 0 | We introduce Autoregressive Diffusion Models (ARDMs), a model class encompassing and generalizing order-agnostic autoregressive models (Uria et al., 2014) and absorbing discrete diffusion (Austin et al., 2021), which we show are special cases of ARDMs under mild assumptions. ARDMs are simple to implement and easy to train. Unlike standard ARMs, they do not require... | Alexey A. Gritsenko, Ben Poole, Emiel Hoogeboom, Jasmijn Bastings, Rianne van den Berg, Tim Salimans |  |
| 337 |  |  [Auto-scaling Vision Transformers without Training](https://openreview.net/forum?id=H94a1_Pyr-6) |  | 0 | This work targets automated designing and scaling of Vision Transformers (ViTs). The motivation comes from two pain spots: 1) the lack of efficient and principled methods for designing and scaling ViTs; 2) the tremendous computational cost of training ViT that is much heavier than its convolution counterpart. To tackle these issues, we propose As-ViT, an auto-scaling... | Denny Zhou, Wei Huang, Wuyang Chen, Xianzhi Du, Xiaodan Song, Zhangyang Wang |  |
| 338 |  |  [Fine-grained Differentiable Physics: A Yarn-level Model for Fabrics](https://openreview.net/forum?id=KPEFXR1HdIo) |  | 0 | Differentiable physics modeling combines physics models with gradient-based learning to provide model explicability and data efficiency. It has been used to learn dynamics, solve inverse problems and facilitate design, and is at its inception of impact. Current successes have concentrated on general physics models such as rigid bodies, deformable sheets, etc, assuming... | Andrew J. Bulpitt, Deshan Gong, He Wang, Zhanxing Zhu |  |
| 339 |  |  [Revisiting flow generative models for Out-of-distribution detection](https://openreview.net/forum?id=6y2KBh-0Fd9) |  | 0 | Deep generative models have been widely used in practical applications such as the detection of out-of-distribution (OOD) data. In this work, we aim to re-examine the potential of generative flow models in OOD detection. We first propose a simple combination of univariate one-sample statistical test (e.g., Kolmogorov-Smirnov) and random projections in the latent space... | Dihong Jiang, Sun Sun, Yaoliang Yu |  |
| 340 |  |  [Missingness Bias in Model Debugging](https://openreview.net/forum?id=Te5ytkqsnl) |  | 0 | Missingness, or the absence of features from an input, is a concept fundamental to many model debugging tools. However, in computer vision, pixels cannot simply be removed from an image. One thus tends to resort to heuristics such as blacking out pixels, which may in turn introduce bias into the debugging process. We study such biases and, in particular, show how... | Aleksander Madry, Eric Wong, Hadi Salman, Pengchuan Zhang, Saachi Jain, Sai Vemprala, Vibhav Vineet |  |
| 341 |  |  [Meta Learning Low Rank Covariance Factors for Energy Based Deterministic Uncertainty](https://openreview.net/forum?id=GQd7mXSPua) |  | 0 | Numerous recent works utilize bi-Lipschitz regularization of neural network layers to preserve relative distances between data instances in the feature spaces of each layer. This distance sensitivity with respect to the data aids in tasks such as uncertainty calibration and out-of-distribution (OOD) detection. In previous works, features extracted with a distance... | Hae Beom Lee, Jeffrey Ryan Willette, Juho Lee, Sung Ju Hwang |  |
| 342 |  |  [Conditional Object-Centric Learning from Video](https://openreview.net/forum?id=aD7uesX1GF_) |  | 0 | Object-centric representations are a promising path toward more systematic generalization by providing flexible abstractions upon which compositional world models can be built. Recent work on simple 2D and 3D datasets has shown that models with object-centric inductive biases can learn to segment and represent meaningful objects from the statistical structure of the... | Alexey Dosovitskiy, Aravindh Mahendran, Austin Stone, Gamaleldin Fathy Elsayed, Georg Heigold, Klaus Greff, Rico Jonschkowski, Sara Sabour, Thomas Kipf |  |
| 343 |  |  [Scale Efficiently: Insights from Pretraining and Finetuning Transformers](https://openreview.net/forum?id=f2OYVDyfIB) |  | 0 | There remain many open questions pertaining to the scaling behaviour of Transformer architectures. These scaling decisions and findings can be critical, as training runs often come with an associated computational cost which have both financial and/or environmental impact. The goal of this paper is to present scaling insights from pretraining and finetuning... | Ashish Vaswani, Dani Yogatama, Donald Metzler, Hyung Won Chung, Jinfeng Rao, Mostafa Dehghani, Samira Abnar, Sharan Narang, William Fedus, Yi Tay |  |
| 344 |  |  [Vitruvion: A Generative Model of Parametric CAD Sketches](https://openreview.net/forum?id=Ow1C7s3UcY) |  | 0 | Parametric computer-aided design (CAD) tools are the predominant way that engineers specify physical structures, from bicycle pedals to airplanes to printed circuit boards. The key characteristic of parametric CAD is that design intent is encoded not only via geometric primitives, but also by parameterized constraints between the elements. This relational specification... | Ari Seff, Nick Richardson, Ryan P. Adams, Wenda Zhou |  |
| 345 |  |  [Space-Time Graph Neural Networks](https://openreview.net/forum?id=XJiajt89Omg) |  | 0 | We introduce space-time graph neural network (ST-GNN), a novel GNN architecture, tailored to jointly process the underlying space-time topology of time-varying network data. The cornerstone of our proposed architecture is the composition of time and graph convolutional filters followed by pointwise nonlinear activation functions. We introduce a generic definition of... | Alejandro Ribeiro, Charilaos I. Kanatsoulis, Samar Hadou |  |
| 346 |  |  [Scattering Networks on the Sphere for Scalable and Rotationally Equivariant Spherical CNNs](https://openreview.net/forum?id=bjy5Zb2fo2) |  | 0 | Convolutional neural networks (CNNs) constructed natively on the sphere have been developed recently and shown to be highly effective for the analysis of spherical data. While an efficient framework has been formulated, spherical CNNs are nevertheless highly computationally demanding; typically they cannot scale beyond spherical signals of thousands of pixels. We... | Augustine N. MavorParker, Christopher G. R. Wallis, Jason D. McEwen |  |
| 347 |  |  [Bayesian Neural Network Priors Revisited](https://openreview.net/forum?id=xkjqJYqRJy) |  | 0 | Isotropic Gaussian priors are the de facto standard for modern Bayesian neural network inference. However, it is unclear whether these priors accurately reflect our true beliefs about the weight distributions or give optimal performance. To find better priors, we study summary statistics of neural network weights in networks trained using stochastic gradient descent... | Adrià GarrigaAlonso, Florian Wenzel, Gunnar Rätsch, Laurence Aitchison, Mark van der Wilk, Richard E. Turner, Sebastian W. Ober, Vincent Fortuin |  |
| 348 |  |  [Goal-Directed Planning via Hindsight Experience Replay](https://openreview.net/forum?id=6NePxZwfae) |  | 0 | We consider the problem of goal-directed planning under a deterministic transition model. Monte Carlo Tree Search has shown remarkable performance in solving deterministic control problems. It has been extended from complex continuous domains through function approximators to bias the search of the planning tree in AlphaZero. Nonetheless, these algorithms still struggle... | Amarildo Likmeta, Enrico Prati, Lorenzo Moro, Marcello Restelli |  |
| 349 |  |  [Hybrid Random Features](https://openreview.net/forum?id=EMigfE6ZeS) |  | 0 | We propose a new class of random feature methods for linearizing softmax and Gaussian kernels called hybrid random features (HRFs) that automatically adapt the quality of kernel estimation to provide most accurate approximation in the defined regions of interest. Special instantiations of HRFs lead to well-known methods such as trigonometric (Rahimi & Recht, 2007) or... | Adrian Weller, Andy Zeng, Arijit Sehanobish, Deepali Jain, Dmitry Kalashnikov, Han Lin, Haoxian Chen, Jake Varley, Krzysztof Marcin Choromanski, Michael S. Ryoo, Valerii Likhosherstov, Vikas Sindhwani, Yuanzhe Ma |  |
| 350 |  |  [Pretrained Language Model in Continual Learning: A Comparative Study](https://openreview.net/forum?id=figzpGMrdD) |  | 0 | Continual learning (CL) is a setting in which a model learns from a stream of incoming data while avoiding to forget previously learned knowledge. Pre-trained language models (PLMs) have been successfully employed in continual learning of different natural language problems. With the rapid development of many continual learning methods and PLMs, understanding and... | Gholamreza Haffari, Guilin Qi, Massimo Caccia, Tongtong Wu, YuanFang Li, Zhuang Li |  |
| 351 |  |  [Salient ImageNet: How to discover spurious features in Deep Learning?](https://openreview.net/forum?id=XVPqLyNxSyh) |  | 0 | Deep neural networks can be unreliable in the real world especially when they heavily use {\it spurious} features for their predictions. Focusing on image classifications, we define {\it core features} as the set of visual features that are always a part of the object definition while {\it spurious features} are the ones that are likely to {\it co-occur} with the object... | Sahil Singla, Soheil Feizi |  |
| 352 |  |  [Differentiable DAG Sampling](https://openreview.net/forum?id=9wOQOgNe-w) |  | 0 | We propose a new differentiable probabilistic model over DAGs (DP-DAG). DP-DAG allows fast and differentiable DAG sampling suited to continuous optimization. To this end, DP-DAG samples a DAG by successively (1) sampling a linear ordering of the node and (2) sampling edges consistent with the sampled linear ordering. We further propose VI-DP-DAG, a new method for DAG... | Bertrand Charpentier, Simon Kibler, Stephan Günnemann |  |
| 353 |  |  [Evaluating Model-Based Planning and Planner Amortization for Continuous Control](https://openreview.net/forum?id=SS8F6tFX3-) |  | 0 | There is a widespread intuition that model-based control methods should be able to surpass the data efficiency of model-free approaches. In this paper we attempt to evaluate this intuition on various challenging locomotion tasks. We take a hybrid approach, combining model predictive control (MPC) with a learned model and model-free policy learning; the learned policy... | Abbas Abdolmaleki, Alessandro Davide Ialongo, Arunkumar Byravan, Josh Merel, Jost Tobias Springenberg, Leonard Hasenclever, Martin A. Riedmiller, Mehdi Mirza, Nicolas Heess, Piotr Trochim, Yuval Tassa |  |
| 354 |  |  [Hierarchical Few-Shot Imitation with Skill Transition Models](https://openreview.net/forum?id=xKZ4K0lTj_) |  | 0 | A desirable property of autonomous agents is the ability to both solve long-horizon problems and generalize to unseen tasks. Recent advances in data-driven skill learning have shown that extracting behavioral priors from offline data can enable agents to solve challenging long-horizon tasks with reinforcement learning. However, generalization to tasks unseen during... | Albert Zhan, Kourosh Hakhamaneshi, Michael Laskin, Pieter Abbeel, Ruihan Zhao |  |
| 355 |  |  [End-to-End Learning of Probabilistic Hierarchies on Graphs](https://openreview.net/forum?id=g2LCQwG7Of) |  | 0 | We propose a novel probabilistic model over hierarchies on graphs obtained by continuous relaxation of tree-based hierarchies. We draw connections to Markov chain theory, enabling us to perform hierarchical clustering by efficient end-to-end optimization of relaxed versions of quality metrics such as Dasgupta cost or Tree-Sampling Divergence (TSD). We show that our... | Bertrand Charpentier, Daniel Zügner, Morgane Ayle, Sascha Geringer, Stephan Günnemann |  |
| 356 |  |  [GeneDisco: A Benchmark for Experimental Design in Drug Discovery](https://openreview.net/forum?id=-w2oomO6qgc) |  | 0 | In vitro cellular experimentation with genetic interventions, using for example CRISPR technologies, is an essential step in early-stage drug discovery and target validation that serves to assess initial hypotheses about causal associations between biological mechanisms and disease pathologies. With billions of potential hypotheses to test, the experimental design space... | Andrew Jesson, Arash Mehrjou, Ashkan Soleymani, Pascal Notin, Patrick Schwab, Stefan Bauer, Yarin Gal |  |
| 357 |  |  [GraphENS: Neighbor-Aware Ego Network Synthesis for Class-Imbalanced Node Classification](https://openreview.net/forum?id=MXEl7i-iru) |  | 0 | In many real-world node classification scenarios, nodes are highly class-imbalanced, where graph neural networks (GNNs) can be readily biased to major class instances. Albeit existing class imbalance approaches in other domains can alleviate this issue to some extent, they do not consider the impact of message passing between nodes. In this paper, we hypothesize that... | Eunho Yang, Jaeyun Song, Joonhyung Park |  |
| 358 |  |  [Continuously Discovering Novel Strategies via Reward-Switching Policy Optimization](https://openreview.net/forum?id=hcQHRHKfN_) |  | 0 | We present Reward-Switching Policy Optimization (RSPO), a paradigm to discover diverse strategies in complex RL environments by iteratively finding novel policies that are both locally optimal and sufficiently different from existing ones. To encourage the learning policy to consistently converge towards a previously undiscovered local optimum, RSPO switches between... | Bingliang Zhang, Wei Fu, Yi Wu, Zihan Zhou |  |
| 359 |  |  [Learning to Remember Patterns: Pattern Matching Memory Networks for Traffic Forecasting](https://openreview.net/forum?id=wwDg3bbYBIq) |  | 0 | Traffic forecasting is a challenging problem due to complex road networks and sudden speed changes caused by various events on roads. Several models have been proposed to solve this challenging problem, with a focus on learning the spatio-temporal dependencies of roads. In this work, we propose a new perspective for converting the forecasting problem into a... | Hongkyu Lim, Hyeshin Chu, Hyunwook Lee, Seungmin Jin, Sungahn Ko |  |
| 360 |  |  [Why Propagate Alone? Parallel Use of Labels and Features on Graphs](https://openreview.net/forum?id=VTNjxbFRKly) |  | 0 | One of the challenges of graph-based semi-supervised learning over ordinary supervised learning for classification tasks lies in label utilization. The direct use of ground-truth labels in graphs for training purposes can result in a parametric model learning trivial degenerate solutions (e.g., an identity mapping from input to output). In addressing this issue, a label... | David Wipf, Jiarui Jin, Jiuhai Chen, Quan Gan, Weinan Zhang, Yangkun Wang, Yong Yu, Yongyi Yang, Zengfeng Huang, Zheng Zhang |  |
| 361 |  |  [Learning by Directional Gradient Descent](https://openreview.net/forum?id=5i7lJLuhTm) |  | 0 | How should state be constructed from a sequence of observations, so as to best achieve some objective? Most deep learning methods update the parameters of the state representation by gradient descent. However, no prior method for computing the gradient is fully satisfactory, for example consuming too much memory, introducing too much variance, or adding too much bias.... | Anirudh Goyal, David Silver, Hado van Hasselt, Ivo Danihelka, Matteo Hessel |  |
| 362 |  |  [Maximum Entropy RL (Provably) Solves Some Robust RL Problems](https://openreview.net/forum?id=PtSAD3caaA2) |  | 0 | Many potential applications of reinforcement learning (RL) require guarantees that the agent will perform well in the face of disturbances to the dynamics or reward function. In this paper, we prove theoretically that maximum entropy (MaxEnt) RL maximizes a lower bound on a robust RL objective, and thus can be used to learn policies that are robust to some disturbances... | Benjamin Eysenbach, Sergey Levine |  |
| 363 |  |  [A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training](https://openreview.net/forum?id=XhF2VOMRHS) |  | 0 | Adversarial Training (AT) is known as an effective approach to enhance the robustness of deep neural networks. Recently researchers notice that robust models with AT have good generative ability and can synthesize realistic images, while the reason behind it is yet under-explored. In this paper, we demystify this phenomenon by developing a unified probabilistic... | Jiansheng Yang, Yifei Wang, Yisen Wang, Zhouchen Lin |  |
| 364 |  |  [Equivariant and Stable Positional Encoding for More Powerful Graph Neural Networks](https://openreview.net/forum?id=e95i1IHcWj) |  | 0 | Graph neural networks (GNN) have shown great advantages in many graph-based learning tasks but often fail to predict accurately for a task-based on sets of nodes such as link/motif prediction and so on. Many works have recently proposed to address this problem by using random node features or node distance features. However, they suffer from either slow convergence,... | Haorui Wang, Haoteng Yin, Muhan Zhang, Pan Li |  |
| 365 |  |  [BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models](https://openreview.net/forum?id=Mng8CQ9eBW) |  | 0 | Pre-trained Natural Language Processing (NLP) models, which can be adapted to a variety of downstream language tasks via fine-tuning, highly accelerate the learning progress of NLP models. However, NLP models have been shown to be vulnerable to backdoor attacks. Previous NLP backdoor attacks mainly focus on one specific task. This limitation makes existing solutions... | Chun Fan, Jiwei Li, Kangjie Chen, Shangwei Guo, Tianwei Zhang, Xiaofei Sun, Yuxian Meng |  |
| 366 |  |  [Shallow and Deep Networks are Near-Optimal Approximators of Korobov Functions](https://openreview.net/forum?id=AV8FPoMTTa) |  | 0 | In this paper, we analyze the number of neurons and training parameters that a neural network needs to approximate multivariate functions of bounded second mixed derivatives --- Korobov functions. We prove upper bounds on these quantities for shallow and deep neural networks, drastically lessening the curse of dimensionality. Our bounds hold for general activation... | Mohammed Amine Bennouna, Moïse Blanchard |  |
| 367 |  |  [What Makes Better Augmentation Strategies? Augment Difficult but Not too Different](https://openreview.net/forum?id=Ucx3DQbC9GH) |  | 0 | The practice of data augmentation has been extensively used to boost the performance of deep neural networks for various NLP tasks. It is more effective when only a limited number of labeled samples is available, e.g., low-data or class-imbalanced regimes. Most current augmentation techniques rely on parameter tuning or inherent randomness; hence, their effectiveness... | Dongyeop Kang, Jaehyung Kim, Jinwoo Shin, Sungsoo Ahn |  |
| 368 |  |  [Generative Pseudo-Inverse Memory](https://openreview.net/forum?id=Harn4_EZBw) |  | 0 | We propose Generative Pseudo-Inverse Memory (GPM), a class of deep generative memory models that are fast to write in and read out. Memory operations are recast as seeking robust solutions of linear systems, which naturally lead to the use of matrix pseudo-inverses. The pseudo-inverses are iteratively approximated, with practical computation complexity of almost $O(1)$.... | Bao Ho, Hung Le, Kha Pham, Man Ngo, Svetha Venkatesh, Truyen Tran |  |
| 369 |  |  [A Deep Variational Approach to Clustering Survival Data](https://openreview.net/forum?id=RQ428ZptQfU) |  | 0 | In this work, we study the problem of clustering survival data — a challenging and so far under-explored task. We introduce a novel semi-supervised probabilistic approach to cluster survival data by leveraging recent advances in stochastic gradient variational inference. In contrast to previous work, our proposed method employs a deep generative model to uncover the... | Alexander Sauter, Bram Stieltjes, Flavio Vasella, Julia E. Vogt, Laura Manduchi, Marc Pfister, Marian C. Neidert, Michela Carlotta Massi, Ricards Marcinkevics, Thomas J. Weikert, Timothy Müller, Verena Gotta |  |
| 370 |  |  [GPT-Critic: Offline Reinforcement Learning for End-to-End Task-Oriented Dialogue Systems](https://openreview.net/forum?id=qaxhBG1UUaS) |  | 0 | Training a task-oriented dialogue agent can be naturally formulated as offline reinforcement learning (RL) problem, where the agent aims to learn a conversational strategy to achieve user goals, only from a dialogue corpus. It is very challenging in terms of RL since the natural language action space is astronomical, while feasible (syntactically and semantically... | Jongmin Lee, KeeEung Kim, Youngsoo Jang |  |
| 371 |  |  [Charformer: Fast Character Transformers via Gradient-based Subword Tokenization](https://openreview.net/forum?id=JtBRnrlOEFN) |  | 0 | State-of-the-art models in natural language processing rely on separate rigid subword tokenization algorithms, which limit their generalization ability and adaptation to new settings. In this paper, we propose a new model inductive bias that learns a subword tokenization end-to-end as part of the model. To this end, we introduce a soft gradient-based subword... | Cong Yu, Dara Bahri, Donald Metzler, Hyung Won Chung, Jai Prakash Gupta, Sebastian Ruder, Simon Baumgartner, Vinh Q. Tran, Yi Tay, Zhen Qin |  |
| 372 |  |  [Regularized Autoencoders for Isometric Representation Learning](https://openreview.net/forum?id=mQxt8l7JL04) |  | 0 | The recent success of autoencoders for representation learning can be traced in large part to the addition of a regularization term. Such regularized autoencoders \`\`constrain" the representation so as to prevent overfitting to the data while producing a parsimonious generative model. A regularized autoencoder should in principle learn not only the data manifold, but... | Frank Chongwoo Park, Minjun Son, Sangwoong Yoon, Yonghyeon Lee |  |
| 373 |  |  [Knowledge Removal in Sampling-based Bayesian Inference](https://openreview.net/forum?id=dTqOcTUOQO) |  | 0 | The right to be forgotten has been legislated in many countries, but its enforcement in the AI industry would cause unbearable costs. When single data deletion requests come, companies may need to delete the whole models learned with massive resources. Existing works propose methods to remove knowledge learned from data for explicitly parameterized models, which however... | Dacheng Tao, Fengxiang He, Shaopeng Fu |  |
| 374 |  |  [Actor-critic is implicitly biased towards high entropy optimal policies](https://openreview.net/forum?id=vEZyTBRPP6o) |  | 0 | We show that the simplest actor-critic method — a linear softmax policy updated with TD through interaction with a linear MDP, but featuring no explicit regularization or exploration — does not merely find an optimal policy, but moreover prefers high entropy optimal policies. To demonstrate the strength of this bias, the algorithm not only has no regularization, no... | Matus Telgarsky, Yuzheng Hu, Ziwei Ji |  |
| 375 |  |  [Igeood: An Information Geometry Approach to Out-of-Distribution Detection](https://openreview.net/forum?id=mfwdY3U_9ea) |  | 0 | Reliable out-of-distribution (OOD) detection is fundamental to implementing safer modern machine learning (ML) systems. In this paper, we introduce Igeood, an effective method for detecting OOD samples. Igeood applies to any pre-trained neural network, works under various degrees of access to the ML model, does not require OOD samples or assumptions on the OOD data but... | Eduardo Dadalto Câmara Gomes, Florence Alberge, Pablo Piantanida, Pierre Duhamel |  |
| 376 |  |  [Bag of Instances Aggregation Boosts Self-supervised Distillation](https://openreview.net/forum?id=N0uJGWDw21d) |  | 0 | Recent advances in self-supervised learning have experienced remarkable progress, especially for contrastive learning based methods, which regard each image as well as its augmentations as an individual class and try to distinguish them from all other images. However, due to the large quantity of exemplars, this kind of pretext task intrinsically suffers from slow... | Haohang Xu, Hongkai Xiong, Jiemin Fang, Lingxi Xie, Qi Tian, Wenrui Dai, Xiaopeng Zhang, Xinggang Wang |  |
| 377 |  |  [Stability Regularization for Discrete Representation Learning](https://openreview.net/forum?id=6tmjoym9LR6) |  | 0 | We present a method for training neural network models with discrete stochastic variables. The core of the method is \emph{stability regularization}, which is a regularization procedure based on the idea of noise stability developed in Gaussian isoperimetric theory in the analysis of Gaussian functions. Stability regularization is method to make the output of continuous... | Adeel Pervez, Efstratios Gavves |  |
| 378 |  |  [Unrolling PALM for Sparse Semi-Blind Source Separation](https://openreview.net/forum?id=aBVxf5NaaRt) |  | 0 | Sparse Blind Source Separation (BSS) has become a well established tool for a wide range of applications – for instance, in astrophysics and remote sensing. Classical sparse BSS methods, such as the Proximal Alternating Linearized Minimization (PALM) algorithm, nevertheless often suffer from a difficult hyper-parameter choice, which undermines their results. To bypass... | Christophe Kervazo, Florence Tupin, Jérôme Bobin, Mohammad Fahes |  |
| 379 |  |  [Fast Generic Interaction Detection for Model Interpretability and Compression](https://openreview.net/forum?id=fQTlgI2qZqE) |  | 0 | The ability of discovering feature interactions in a black-box model is vital to explainable deep learning. We propose a principled, global interaction detection method by casting our target as a multi-arm bandits problem and solving it swiftly with the UCB algorithm. This adaptive method is free of ad-hoc assumptions and among the cutting-edge methods with outstanding... | Feng Yin, Tianjian Zhang, ZhiQuan Luo |  |
| 380 |  |  [Reversible Instance Normalization for Accurate Time-Series Forecasting against Distribution Shift](https://openreview.net/forum?id=cGDAkQo1C0p) |  | 0 | Statistical properties such as mean and variance often change over time in time series, i.e., time-series data suffer from a distribution shift problem. This change in temporal distribution is one of the main challenges that prevent accurate time-series forecasting. To address this issue, we propose a simple yet effective normalization method called reversible instance... | Cheonbok Park, Jaegul Choo, JangHo Choi, Jinhee Kim, Taesung Kim, Yunwon Tae |  |
| 381 |  |  [On the Pitfalls of Analyzing Individual Neurons in Language Models](https://openreview.net/forum?id=8uz0EWPQIMu) |  | 0 | While many studies have shown that linguistic information is encoded in hidden word representations, few have studied individual neurons, to show how and in which neurons it is encoded. Among these, the common approach is to use an external probe to rank neurons according to their relevance to some linguistic attribute, and to evaluate the obtained ranking using the... | Omer Antverg, Yonatan Belinkov |  |
| 382 |  |  [Query Embedding on Hyper-Relational Knowledge Graphs](https://openreview.net/forum?id=4rLw09TgRw9) |  | 0 | Multi-hop logical reasoning is an established problem in the field of representation learning on knowledge graphs (KGs). It subsumes both one-hop link prediction as well as other more complex types of logical queries. Existing algorithms operate only on classical, triple-based graphs, whereas modern KGs often employ a hyper-relational modeling paradigm. In this... | Dimitrios Alivanistos, Max Berrendorf, Michael Cochez, Mikhail Galkin |  |
| 383 |  |  [Neural Solvers for Fast and Accurate Numerical Optimal Control](https://openreview.net/forum?id=m8bypnj7Yl5) |  | 0 | Synthesizing optimal controllers for dynamical systems often involves solving optimization problems with hard real-time constraints. These constraints determine the class of numerical methods that can be applied: computationally expensive but accurate numerical routines are replaced by fast and inaccurate methods, trading inference time for solution accuracy. This paper... | Federico Berto, Jinkyoo Park, Michael Poli, Stefano Massaroli |  |
| 384 |  |  [PSA-GAN: Progressive Self Attention GANs for Synthetic Time Series](https://openreview.net/forum?id=Ix_mh42xq5w) |  | 0 | Realistic synthetic time series data of sufficient length enables practical applications in time series modeling tasks, such as forecasting, but remains a challenge. In this paper we present PSA-GAN, a generative adversarial network (GAN) that generates long time series samples of high quality using progressive growing of GANs and self-attention. We show that PSA-GAN... | Jan Gasthaus, Michael BohlkeSchneider, Paul Jeha, Pedro Mercado, RajbirSingh Nirwan, Shubham Kapoor, Tim Januschowski, Valentin Flunkert |  |
| 385 |  |  [ToM2C: Target-oriented Multi-agent Communication and Cooperation with Theory of Mind](https://openreview.net/forum?id=2t7CkQXNpuq) |  | 0 | Being able to predict the mental states of others is a key factor to effective social interaction. It is also crucial for distributed multi-agent systems, where agents are required to communicate and cooperate. In this paper, we introduce such an important social-cognitive skill, i.e. Theory of Mind (ToM), to build socially intelligent agents who are able to communicate... | Fangwei Zhong, Jing Xu, Yizhou Wang, Yuanfei Wang |  |
| 386 |  |  [Better Supervisory Signals by Observing Learning Paths](https://openreview.net/forum?id=Iog0djAdbHj) |  | 0 | Better-supervised models might have better performance. In this paper, we first clarify what makes for good supervision for a classification problem, and then explain two existing label refining methods, label smoothing and knowledge distillation, in terms of our proposed criterion. To further answer why and how better supervision emerges, we observe the learning path,... | Danica J. Sutherland, Shangmin Guo, Yi Ren |  |
| 387 |  |  [TAda! Temporally-Adaptive Convolutions for Video Understanding](https://openreview.net/forum?id=izj68lUcBpt) |  | 0 | Spatial convolutions are widely used in numerous deep video models. It fundamentally assumes spatio-temporal invariance, i.e., using shared weights for every location in different frames. This work presents Temporally-Adaptive Convolutions (TAdaConv) for video understanding, which shows that adaptive weight calibration along the temporal dimension is an efficient way to... | Liang Pan, Marcelo H. Ang Jr., Mingqian Tang, Shiwei Zhang, Zhiwu Qing, Ziwei Liu, Ziyuan Huang |  |
| 388 |  |  [Learning a subspace of policies for online adaptation in Reinforcement Learning](https://openreview.net/forum?id=4Muj-t_4o4) |  | 0 | Deep Reinforcement Learning (RL) is mainly studied in a setting where the training and the testing environments are similar. But in many practical applications, these environments may differ. For instance, in control systems, the robot(s) on which a policy is learned might differ from the robot(s) on which a policy will run. It can be caused by different internal... | JeanBaptiste Gaya, Laure Soulier, Ludovic Denoyer |  |
| 389 |  |  [ZeroFL: Efficient On-Device Training for Federated Learning with Local Sparsity](https://openreview.net/forum?id=2sDQwC_hmnM) |  | 0 | When the available hardware cannot meet the memory and compute requirements to efficiently train high performing machine learning models, a compromise in either the training quality or the model complexity is needed. In Federated Learning (FL), nodes are orders of magnitude more constrained than traditional server-grade hardware and are often battery powered, severely... | Javier FernándezMarqués, Nicholas Donald Lane, Pedro P. B. de Gusmao, Titouan Parcollet, Xinchi Qiu, Yan Gao |  |
| 390 |  |  [Gaussian Mixture Convolution Networks](https://openreview.net/forum?id=Oxeka7Z7Hor) |  | 0 | This paper proposes a novel method for deep learning based on the analytical convolution of multidimensional Gaussian mixtures. In contrast to tensors, these do not suffer from the curse of dimensionality and allow for a compact representation, as data is only stored where details exist. Convolution kernels and data are Gaussian mixtures with unconstrained weights,... | Adam Celarek, Bernhard Kerbl, Michael Wimmer, Pedro Hermosilla, Timo Ropinski |  |
| 391 |  |  [How Does SimSiam Avoid Collapse Without Negative Samples? A Unified Understanding with Self-supervised Contrastive Learning](https://openreview.net/forum?id=bwq6O4Cwdl) |  | 0 | To avoid collapse in self-supervised learning (SSL), a contrastive loss is widely used but often requires a large number of negative samples. Without negative samples yet achieving competitive performance, a recent work~\citep{chen2021exploring} has attracted significant attention for providing a minimalist simple Siamese (SimSiam) method to avoid collapse. However, the... | Chang D. Yoo, Chaoning Zhang, Chenshuang Zhang, In So Kweon, Kang Zhang, Trung X. Pham |  |
| 392 |  |  [Attention-based Interpretability with Concept Transformers](https://openreview.net/forum?id=kAa9eDS0RdO) |  | 0 | Attention is a mechanism that has been instrumental in driving remarkable performance gains of deep neural network models in a host of visual, NLP and multimodal tasks. One additional notable aspect of attention is that it conveniently exposes the \`\`reasoning'' behind each particular output generated by the model. Specifically, attention scores over input regions or... | Christoph Miksovic, Ioana Giurgiu, Mattia Rigotti, Paolo Scotton, Thomas Gschwind |  |
| 393 |  |  [Inductive Relation Prediction Using Analogy Subgraph Embeddings](https://openreview.net/forum?id=PTRo58zPt3P) |  | 0 | Prevailing methods for relation prediction in heterogeneous graphs aim at learning latent representations (i.e., embeddings) of observed nodes and relations, and thus are limited to the transductive setting where the relation types must be known during training. Here, we propose ANalogy SubGraphEmbeddingLearning (GraphANGEL), a novel relation prediction framework that... | David Wipf, Jiarui Jin, Kounianhua Du, Quan Gan, Weinan Zhang, Yangkun Wang, Yong Yu, Zheng Zhang |  |
| 394 |  |  [Reinforcement Learning in Presence of Discrete Markovian Context Evolution](https://openreview.net/forum?id=CmsfC7u054S) |  | 0 | We consider a context-dependent Reinforcement Learning (RL) setting, which is characterized by: a) an unknown finite number of not directly observable contexts; b) abrupt (discontinuous) context changes occurring during an episode; and c) Markovian context evolution. We argue that this challenging case is often met in applications and we tackle it using a Bayesian... | Aivar Sootla, Haitham BouAmmar, Hang Ren, Jun Wang, Junxiao Shen, Taher Jafferjee |  |
| 395 |  |  [Optimal Transport for Long-Tailed Recognition with Learnable Cost Matrix](https://openreview.net/forum?id=t98k9ePQQpn) |  | 0 | It is attracting attention to the long-tailed recognition problem, a burning issue that has become very popular recently. Distinctive from conventional recognition is that it posits that the allocation of the training set is supremely distorted. Predictably, it will pose challenges to the generalisation behaviour of the model. Approaches to these challenges revolve into... | Hanyu Peng, Mingming Sun, Ping Li |  |
| 396 |  |  [PriorGrad: Improving Conditional Denoising Diffusion Models with Data-Dependent Adaptive Prior](https://openreview.net/forum?id=_BNiN4IjC5) |  | 0 | Denoising diffusion probabilistic models have been recently proposed to generate high-quality samples by estimating the gradient of the data density. The framework assumes the prior noise as a standard Gaussian distribution, whereas the corresponding data distribution may be more complicated than the standard Gaussian distribution, which potentially introduces... | Chaehun Shin, Chang Liu, Heeseung Kim, Qi Meng, Sanggil Lee, Sungroh Yoon, Tao Qin, TieYan Liu, Wei Chen, Xu Tan |  |
| 397 |  |  [Target-Side Input Augmentation for Sequence to Sequence Generation](https://openreview.net/forum?id=pz1euXohm4H) |  | 0 | Autoregressive sequence generation, a prevalent task in machine learning and natural language processing, generates every target token conditioned on both a source input and previously generated target tokens. Previous data augmentation methods, which have been shown to be effective for the task, mainly enhance source inputs (e.g., injecting noise into the source... | Ang Lv, Lijun Wu, Rui Yan, Shufang Xie, Tao Qin, TieYan Liu, Yingce Xia |  |
| 398 |  |  [UniFormer: Unified Transformer for Efficient Spatial-Temporal Representation Learning](https://openreview.net/forum?id=nBU_u6DLvoK) |  | 0 | It is a challenging task to learn rich and multi-scale spatiotemporal semantics from high-dimensional videos, due to large local redundancy and complex global dependency between video frames. The recent advances in this research have been mainly driven by 3D convolutional neural networks and vision transformers. Although 3D convolution can efficiently aggregate local... | Guanglu Song, Hongsheng Li, Kunchang Li, Peng Gao, Yali Wang, Yu Liu, Yu Qiao |  |
| 399 |  |  [Inverse Online Learning: Understanding Non-Stationary and Reactionary Policies](https://openreview.net/forum?id=DYypjaRdph2) |  | 0 | Human decision making is well known to be imperfect and the ability to analyse such processes individually is crucial when attempting to aid or improve a decision-maker's ability to perform a task, e.g. to alert them to potential biases or oversights on their part. To do so, it is necessary to develop interpretable representations of how agents make decisions and how... | Alex J. Chan, Alicia Curth, Mihaela van der Schaar |  |
| 400 |  |  [Multi-Mode Deep Matrix and Tensor Factorization](https://openreview.net/forum?id=6YVIk0sAkF_) |  | 0 | Recently, deep linear and nonlinear matrix factorizations gain increasing attention in the area of machine learning. Existing deep nonlinear matrix factorization methods can only exploit partial nonlinearity of the data and are not effective in handling matrices of which the number of rows is comparable to the number of columns. On the other hand, there is still a gap... | Jicong Fan |  |
| 401 |  |  [LORD: Lower-Dimensional Embedding of Log-Signature in Neural Rough Differential Equations](https://openreview.net/forum?id=fCG75wd39ze) |  | 0 | The problem of processing very long time-series data (e.g., a length of more than 10,000) is a long-standing research problem in machine learning. Recently, one breakthrough, called neural rough differential equations (NRDEs), has been proposed and has shown that it is able to process such data. Their main concept is to use the log-signature transform, which is known to... | Jaehoon Lee, Jayoung Kim, Jihyeon Hyeong, Jinsung Jeon, Minju Jo, Noseong Park, Seungji Kook, Sheo Yon Jhin |  |
| 402 |  |  [Generalized Natural Gradient Flows in Hidden Convex-Concave Games and GANs](https://openreview.net/forum?id=bsycpMi00R1) |  | 0 | Game-theoretic formulations in machine learning have recently risen in prominence, whereby entire modeling paradigms are best captured as zero-sum games. Despite their popularity, however, their dynamics are still poorly understood. This lack of theory is often substantiated with painful empirical observations of volatile training dynamics and even divergence. Such... | Andjela Mladenovic, Gauthier Gidel, Georgios Piliouras, Iosif Sakos |  |
| 403 |  |  [Offline Neural Contextual Bandits: Pessimism, Optimization and Generalization](https://openreview.net/forum?id=sPIFuucA3F) |  | 0 | Offline policy learning (OPL) leverages existing data collected a priori for policy optimization without any active exploration. Despite the prevalence and recent interest in this problem, its theoretical and algorithmic foundations in function approximation settings remain under-developed. In this paper, we consider this problem on the axes of distributional shift,... | A. Tuan Nguyen, Sunil Gupta, Svetha Venkatesh, Thanh NguyenTang |  |
| 404 |  |  [THOMAS: Trajectory Heatmap Output with learned Multi-Agent Sampling](https://openreview.net/forum?id=QDdJhACYrlX) |  | 0 | In this paper, we propose THOMAS, a joint multi-agent trajectory prediction framework allowing for an efficient and consistent prediction of multi-agent multi-modal trajectories. We present a unified model architecture for simultaneous agent future heatmap estimation, in which we leverage hierarchical and sparse image generation for fast and memory-efficient inference.... | Bogdan Stanciulescu, Dzmitry Tsishkou, Fabien Moutarde, Stefano Sabatini, Thomas Gilles |  |
| 405 |  |  [CLEVA-Compass: A Continual Learning Evaluation Assessment Compass to Promote Research Transparency and Comparability](https://openreview.net/forum?id=rHMaBYbkkRJ) |  | 0 | What is the state of the art in continual machine learning? Although a natural question for predominant static benchmarks, the notion to train systems in a lifelong manner entails a plethora of additional challenges with respect to set-up and evaluation. The latter have recently sparked a growing amount of critiques on prominent algorithm-centric perspectives and... | Kristian Kersting, Martin Mundt, Quentin Delfosse, Steven Lang |  |
| 406 |  |  [Neural Stochastic Dual Dynamic Programming](https://openreview.net/forum?id=aisKPsMM3fg) |  | 0 | Stochastic dual dynamic programming (SDDP) is a state-of-the-art method for solving multi-stage stochastic optimization, widely used for modeling real-world process optimization tasks. Unfortunately, SDDP has a worst-case complexity that scales exponentially in the number of decision variables, which severely limits applicability to only low dimensional problems. To... | Bo Dai, Dale Schuurmans, Hanjun Dai, Yuan Xue, Zia Syed |  |
| 407 |  |  [DemoDICE: Offline Imitation Learning with Supplementary Imperfect Demonstrations](https://openreview.net/forum?id=BrPdX1bDZkQ) |  | 0 | We consider offline imitation learning (IL), which aims to mimic the expert's behavior from its demonstration without further interaction with the environment. One of the main challenges in offline IL is to deal with the narrow support of the data distribution exhibited by the expert demonstrations that cover only a small fraction of the state and the action spaces. As... | GeonHyeong Kim, Hongseok Yang, HyeongJoo Hwang, Jongmin Lee, KeeEung Kim, Seokin Seo, Wonseok Jeon |  |
| 408 |  |  [Learning to Extend Molecular Scaffolds with Structural Motifs](https://openreview.net/forum?id=ZTsoE8G3GG) |  | 0 | Recent advancements in deep learning-based modeling of molecules promise to accelerate in silico drug discovery. A plethora of generative models is available, building molecules either atom-by-atom and bond-by-bond or fragment-by-fragment. However, many drug discovery projects require a fixed scaffold to be present in the generated molecule, and incorporating that... | Finton Sirockin, Henry Richard JacksonFlux, Krzysztof Maziarz, Marc Brockschmidt, Marwin H. S. Segler, Nadine Schneider, Nikolaus Stiefl, Pashmina Cameron |  |
| 409 |  |  [Discrepancy-Based Active Learning for Domain Adaptation](https://openreview.net/forum?id=p98WJxUC3Ca) |  | 0 | The goal of the paper is to design active learning strategies which lead to domain adaptation under an assumption of Lipschitz functions. Building on previous work by Mansour et al. (2009) we adapt the concept of discrepancy distance between source and target distributions to restrict the maximization over the hypothesis class to a localized class of functions which are... | Antoine de Mathelin, François Deheeger, Mathilde Mougeot, Nicolas Vayatis |  |
| 410 |  |  [Gradient Matching for Domain Generalization](https://openreview.net/forum?id=vDwBW49HmO) |  | 0 | Machine learning systems typically assume that the distributions of training and test sets match closely. However, a critical requirement of such systems in the real world is their ability to generalize to unseen domains. Here, we propose an _inter-domain gradient matching_ objective that targets domain generalization by maximizing the inner product between gradients... | Awni Y. Hannun, Gabriel Synnaeve, Jeffrey Seely, Nicolas Usunier, Philip H. S. Torr, Siddharth Narayanaswamy, Yuge Shi |  |
| 411 |  |  [Objects in Semantic Topology](https://openreview.net/forum?id=d5SCUJ5t1k) |  | 0 | A more realistic object detection paradigm, Open-World Object Detection, has arised increasing research interests in the community recently. A qualified open-world object detector can not only identify objects of known categories, but also discover unknown objects, and incrementally learn to categorize them when their annotations progressively arrive. Previous works... | Changhu Wang, Min Xu, Peize Sun, Ping Luo, Ruiheng Zhang, Shuo Yang, Xiaobo Xia, Yi Jiang, Zehuan Yuan |  |
| 412 |  |  [Hidden Parameter Recurrent State Space Models For Changing Dynamics Scenarios](https://openreview.net/forum?id=ds8yZOUsea) |  | 0 | Recurrent State-space models (RSSMs) are highly expressive models for learning patterns in time series data and for system identification. However, these models are often based on the assumption that the dynamics are fixed and unchanging, which is rarely the case in real-world scenarios. Many control applications often exhibit tasks with similar, but not identical... | Dieter Büchler, Gerhard Neumann, Philipp Becker, Rohit Sonker, Vaisakh Shaj |  |
| 413 |  |  [Graph Neural Network Guided Local Search for the Traveling Salesperson Problem](https://openreview.net/forum?id=ar92oEosBIg) |  | 0 | Solutions to the Traveling Salesperson Problem (TSP) have practical applications to processes in transportation, logistics, and automation, yet must be computed with minimal delay to satisfy the real-time nature of the underlying tasks. However, solving large TSP instances quickly without sacrificing solution quality remains challenging for current approximate... | Amanda Prorok, Benjamin Hudson, Matthew Malencia, Qingbiao Li |  |
| 414 |  |  [On the Pitfalls of Heteroscedastic Uncertainty Estimation with Probabilistic Neural Networks](https://openreview.net/forum?id=aPOpXlnV1T) |  | 0 | Capturing aleatoric uncertainty is a critical part of many machine learning systems. In deep learning, a common approach to this end is to train a neural network to estimate the parameters of a heteroscedastic Gaussian distribution by maximizing the logarithm of the likelihood function under the observed data. In this work, we examine this approach and identify... | Arash Tavakoli, Dimitrije Antic, Georg Martius, Maximilian Seitzer |  |
| 415 |  |  [Label-Efficient Semantic Segmentation with Diffusion Models](https://openreview.net/forum?id=SlxSY2UZQT) |  | 0 | Denoising diffusion probabilistic models have recently received much research attention since they outperform alternative approaches, such as GANs, and currently provide state-of-the-art generative performance. The superior performance of diffusion models has made them an appealing tool in several applications, including inpainting, super-resolution, and semantic... | Andrey Voynov, Artem Babenko, Dmitry Baranchuk, Ivan Rubachev, Valentin Khrulkov |  |
| 416 |  |  [Language model compression with weighted low-rank factorization](https://openreview.net/forum?id=uPv9Y3gmAI5) |  | 0 | Factorizing a large matrix into small matrices is a popular strategy for model compression. Singular value decomposition (SVD) plays a vital role in this compression strategy, approximating a learned matrix with fewer parameters. However, SVD minimizes the squared error toward reconstructing the original matrix without gauging the importance of the parameters,... | Hongxia Jin, Qian Lou, Sungen Chang, Ting Hua, YenChang Hsu, Yilin Shen |  |
| 417 |  |  [Pareto Set Learning for Neural Multi-Objective Combinatorial Optimization](https://openreview.net/forum?id=QuObT9BTWo) |  | 0 | Multiobjective combinatorial optimization (MOCO) problems can be found in many real-world applications. However, exactly solving these problems would be very challenging, particularly when they are NP-hard. Many handcrafted heuristic methods have been proposed to tackle different MOCO problems over the past decades. In this work, we generalize the idea of neural... | Qingfu Zhang, Xi Lin, Zhiyuan Yang |  |
| 418 |  |  [Prototypical Contrastive Predictive Coding](https://openreview.net/forum?id=8la28hZOwug) |  | 0 | Transferring representational knowledge of a model to another is a wide-ranging topic in machine learning. Those applications include the distillation of a large supervised or self-supervised teacher model to a smaller student model or self-supervised learning via self-distillation. Knowledge distillation is an original method to solve these problems, which minimizes a... | Kyungmin Lee |  |
| 419 |  |  [Adversarial Robustness Through the Lens of Causality](https://openreview.net/forum?id=cZAi1yWpiXQ) |  | 0 | The adversarial vulnerability of deep neural networks has attracted signiﬁcant attention in machine learning. As causal reasoning has an instinct for modeling distribution change, it is essential to incorporate causality into analyzing this specific type of distribution change induced by adversarial attacks. However, causal formulations of the intuition of adversarial... | Bernhard Schölkopf, Bo Han, Gang Niu, Kun Zhang, Mingming Gong, Tongliang Liu, Xinmei Tian, Yonggang Zhang |  |
| 420 |  |  [Distributionally Robust Fair Principal Components via Geodesic Descents](https://openreview.net/forum?id=9NVd-DMtThY) |  | 0 | Principal component analysis is a simple yet useful dimensionality reduction technique in modern machine learning pipelines. In consequential domains such as college admission, healthcare and credit approval, it is imperative to take into account emerging criteria such as the fairness and the robustness of the learned projection. In this paper, we propose a... | Hieu Vu, ManChung Yue, Toan Tran, Viet Anh Nguyen |  |
| 421 |  |  [Understanding and Improving Graph Injection Attack by Promoting Unnoticeability](https://openreview.net/forum?id=wkMG8cdvh7-) |  | 0 | Recently Graph Injection Attack (GIA) emerges as a practical attack scenario on Graph Neural Networks (GNNs), where the adversary can merely inject few malicious nodes instead of modifying existing nodes or edges, i.e., Graph Modification Attack (GMA). Although GIA has achieved promising results, little is known about why it is successful and whether there is any... | Bo Han, Han Yang, James Cheng, Kaili Ma, Tongliang Liu, Yonggang Zhang, Yongqiang Chen |  |
| 422 |  |  [Learning to Guide and to be Guided in the Architect-Builder Problem](https://openreview.net/forum?id=swiyAeGzFhQ) |  | 0 | We are interested in interactive agents that learn to coordinate, namely, a $builder$ -- which performs actions but ignores the goal of the task, i.e. has no access to rewards -- and an $architect$ which guides the builder towards the goal of the task. We define and explore a formal setting where artificial agents are equipped with mechanisms that allow them to... | Christopher Pal, Clément MoulinFrier, Derek Nowrouzezahrai, Paul Barde, PierreYves Oudeyer, Tristan Karch |  |
| 423 |  |  [Phase Collapse in Neural Networks](https://openreview.net/forum?id=iPHLcmtietq) |  | 0 | Deep convolutional classifiers linearly separate image classes and improve accuracy as depth increases. They progressively reduce the spatial dimension whereas the number of channels grows with depth. Spatial variability is therefore transformed into variability along channels. A fundamental challenge is to understand the role of non-linearities together with... | Florentin Guth, John Zarka, Stéphane Mallat |  |
| 424 |  |  [SPIRAL: Self-supervised Perturbation-Invariant Representation Learning for Speech Pre-Training](https://openreview.net/forum?id=TBpg4PnXhYH) |  | 0 | We introduce a new approach for speech pre-training named SPIRAL which works by learning denoising representation of perturbed data in a teacher-student framework. Specifically, given a speech utterance, we first feed the utterance to a teacher network to obtain corresponding representation. Then the same utterance is perturbed and fed to a student network. The student... | Qun Liu, Wenyong Huang, Xin Jiang, Yu Ting Yeung, Zhenhe Zhang |  |
| 425 |  |  [Improving the Accuracy of Learning Example Weights for Imbalance Classification](https://openreview.net/forum?id=J_PHjw4gvXJ) |  | 0 | To solve the imbalance classification, methods of weighting examples have been proposed. Recent work has studied to assign adaptive weights to training examples through learning mechanisms, that is, the weights, similar to classification models, are regarded as parameters that need to be learned. However, the algorithms in recent work use local information to... | Bin Cao, Jing Fan, Yuqi Liu |  |
| 426 |  |  [Generating Videos with Dynamics-aware Implicit Generative Adversarial Networks](https://openreview.net/forum?id=Czsdv-S4-w9) |  | 0 | In the deep learning era, long video generation of high-quality still remains challenging due to the spatio-temporal complexity and continuity of videos. Existing prior works have attempted to model video distribution by representing videos as 3D grids of RGB values, which impedes the scale of generated videos and neglects continuous dynamics. In this paper, we found... | Hyunsu Kim, Jihoon Tack, Jinwoo Shin, JungWoo Ha, Junho Kim, Sangwoo Mo, Sihyun Yu |  |
| 427 |  |  [Efficient Learning of Safe Driving Policy via Human-AI Copilot Optimization](https://openreview.net/forum?id=0cgU-BZp2ky) |  | 0 | Human intervention is an effective way to inject human knowledge into the training loop of reinforcement learning, which can bring fast learning and ensured training safety. Given the very limited budget of human intervention, it remains challenging to design when and how human expert interacts with the learning agent in the training. In this work, we develop a novel... | Bolei Zhou, Quanyi Li, Zhenghao Peng |  |
| 428 |  |  [Enhancing Cross-lingual Transfer by Manifold Mixup](https://openreview.net/forum?id=OjPmfr9GkVv) |  | 0 | Based on large-scale pre-trained multilingual representations, recent cross-lingual transfer methods have achieved impressive transfer performances. However, the performance of target languages still lags far behind the source language. In this paper, our analyses indicate such a performance gap is strongly associated with the cross-lingual representation discrepancy.... | Hao Zhou, Huadong Chen, Huiyun Yang, Lei Li |  |
| 429 |  |  [Evolutionary Diversity Optimization with Clustering-based Selection for Reinforcement Learning](https://openreview.net/forum?id=74x5BXs4bWD) |  | 0 | Reinforcement Learning (RL) has achieved significant successes, which aims to obtain a single policy maximizing the expected cumulative rewards for a given task. However, in many real-world scenarios, e.g., navigating in complex environments and controlling robots, one may need to find a set of policies having both high rewards and diverse behaviors, which can bring... | Chao Qian, Ke Xue, Yutong Wang |  |
| 430 |  |  [Curvature-Guided Dynamic Scale Networks for Multi-View Stereo](https://openreview.net/forum?id=_Wzj0J2xs2D) |  | 0 | Multi-view stereo (MVS) is a crucial task for precise 3D reconstruction. Most recent studies tried to improve the performance of matching cost volume in MVS by introducing a skilled design to cost formulation or cost regularization. In this paper, we focus on learning robust feature extraction to enhance the performance of matching costs, without need of heavy... | Khang Truong Giang, Soohwan Song, Sungho Jo |  |
| 431 |  |  [Near-optimal Offline Reinforcement Learning with Linear Representation: Leveraging Variance Information with Pessimism](https://openreview.net/forum?id=KLaDXLAzzFT) |  | 0 | Offline reinforcement learning, which seeks to utilize offline/historical data to optimize sequential decision-making strategies, has gained surging prominence in recent studies. Due to the advantage that appropriate function approximators can help mitigate the sample complexity burden in modern reinforcement learning problems, existing endeavors usually enforce... | Mengdi Wang, Ming Yin, Yaqi Duan, YuXiang Wang |  |
| 432 |  |  [Exploring extreme parameter compression for pre-trained language models](https://openreview.net/forum?id=RftryyYyjiG) |  | 0 | Recent work explored the potential of large-scale Transformer-based pre-trained models, especially Pre-trained Language Models (PLMs) in natural language processing. This raises many concerns from various perspectives, e.g., financial costs and carbon emissions. Compressing PLMs like BERT with negligible performance loss for faster inference and cheaper deployment has... | Benyou Wang, Lifeng Shang, Qun Liu, Xin Jiang, Yuxin Ren |  |
| 433 |  |  [Local Feature Swapping for Generalization in Reinforcement Learning](https://openreview.net/forum?id=Sq0-tgDyHe4) |  | 0 | Over the past few years, the acceleration of computing resources and research in Deep Learning has led to significant practical successes in a range of tasks, including in particular in computer vision. Building on these advances, reinforcement learning has also seen a leap forward with the emergence of agents capable of making decisions directly from visual... | David Bertoin, Emmanuel Rachelson |  |
| 434 |  |  [Open-vocabulary Object Detection via Vision and Language Knowledge Distillation](https://openreview.net/forum?id=lL3lnMbR4WU) |  | 0 | We aim at advancing open-vocabulary object detection, which detects objects described by arbitrary text inputs. The fundamental challenge is the availability of training data. It is costly to further scale up the number of classes contained in existing object detection datasets. To overcome this challenge, we propose ViLD, a training method via Vision and Language... | TsungYi Lin, Weicheng Kuo, Xiuye Gu, Yin Cui |  |
| 435 |  |  [Model-Based Offline Meta-Reinforcement Learning with Regularization](https://openreview.net/forum?id=EBn0uInJZWh) |  | 0 | Existing offline reinforcement learning (RL) methods face a few major challenges, particularly the distributional shift between the learned policy and the behavior policy. Offline Meta-RL is emerging as a promising approach to address these challenges, aiming to learn an informative meta-policy from a collection of tasks. Nevertheless, as shown in our empirical studies,... | Jialin Wan, Junshan Zhang, Sen Lin, Tengyu Xu, Yingbin Liang |  |
| 436 |  |  [Scale Mixtures of Neural Network Gaussian Processes](https://openreview.net/forum?id=YVPBh4k78iZ) |  | 0 | Recent works have revealed that infinitely-wide feed-forward or recurrent neural networks of any architecture correspond to Gaussian processes referred to as NNGP. While these works have extended the class of neural networks converging to Gaussian processes significantly, however, there has been little focus on broadening the class of stochastic processes that such... | Eunggu Yun, Hongseok Yang, Hyungi Lee, Juho Lee |  |
| 437 |  |  [A Johnson-Lindenstrauss Framework for Randomly Initialized CNNs](https://openreview.net/forum?id=YX0lrvdPQc) |  | 0 | How does the geometric representation of a dataset change after the application of each randomly initialized layer of a neural network? The celebrated Johnson-Lindenstrauss lemma answers this question for linear fully-connected neural networks (FNNs), stating that the geometry is essentially preserved. For FNNs with the ReLU activation, the angle between two input... | Anatoly Khina, Ido Nachum, Jan Hazla, Michael Gastpar |  |
| 438 |  |  [Hindsight: Posterior-guided training of retrievers for improved open-ended generation](https://openreview.net/forum?id=Vr_BTpw3wz) |  | 0 | Many text generation systems benefit from retrieving passages from a textual knowledge corpus (e.g., Wikipedia) and using them to generate the output. For open-ended generation tasks, like generating informative utterances in conversations, many varied passages $z$ are relevant to the context $x$ but few are relevant to the observed next utterance $y$ (label). For such... | Ashwin Paranjape, Christopher D. Manning, Christopher Potts, Matei Zaharia, Omar Khattab |  |
| 439 |  |  [Self-Supervised Graph Neural Networks for Improved Electroencephalographic Seizure Analysis](https://openreview.net/forum?id=k9bx1EfHI_-) |  | 0 | Automated seizure detection and classification from electroencephalography (EEG) can greatly improve seizure diagnosis and treatment. However, several modeling challenges remain unaddressed in prior automated seizure detection and classification studies: (1) representing non-Euclidean data structure in EEGs, (2) accurately classifying rare seizure types, and (3) lacking... | Christopher LeeMesser, Daniel L. Rubin, Florian Dubost, Jared Dunnmon, Khaled Kamal Saab, Qianying Huang, Siyi Tang, Xuan Zhang |  |
| 440 |  |  [Group-based Interleaved Pipeline Parallelism for Large-scale DNN Training](https://openreview.net/forum?id=cw-EmNq5zfD) |  | 0 | The recent trend of using large-scale deep neural networks (DNN) to boost performance has propelled the development of the parallel pipelining technique for efficient DNN training, which has resulted in the development of several prominent pipelines such as GPipe, PipeDream, and PipeDream-2BW. However, the current leading pipeline PipeDream-2BW still suffers from two... | Hong Wei, Ming Yang, Pengcheng Yang, Wenpeng Zhang, Xiaoming Zhang |  |
| 441 |  |  [Minimax Optimality (Probably) Doesn't Imply Distribution Learning for GANs](https://openreview.net/forum?id=nc0ETaieux) |  | 0 | Arguably the most fundamental question in the theory of generative adversarial networks (GANs) is to understand when GANs can actually learn the underlying distribution. Theoretical and empirical evidence (see e.g. Arora-Risteski-Zhang '18) suggest local optimality of the empirical training objective is insufficient, yet it does not rule out the possibility that... | Jerry Li, Raghu Meka, Sitan Chen, Yuanzhi Li |  |
| 442 |  |  [Offline Reinforcement Learning with Value-based Episodic Memory](https://openreview.net/forum?id=RCZqv9NXlZ) |  | 0 | Offline reinforcement learning (RL) shows promise of applying RL to real-world problems by effectively utilizing previously collected data. Most existing offline RL algorithms use regularization or constraints to suppress extrapolation error for actions outside the dataset. In this paper, we adopt a different framework, which learns the V-function instead of the... | Bin Liang, Chongjie Zhang, Hao Hu, Jun Yang, Qianchuan Zhao, Qihan Liu, Xiaoteng Ma, Yiqin Yang |  |
| 443 |  |  [MonoDistill: Learning Spatial Features for Monocular 3D Object Detection](https://openreview.net/forum?id=C54V-xTWfi) |  | 0 | 3D object detection is a fundamental and challenging task for 3D scene understanding, and the monocular-based methods can serve as an economical alternative to the stereo-based or LiDAR-based methods. However, accurately locating objects in the 3D space from a single image is extremely difficult due to the lack of spatial cues. To mitigate this issue, we propose a... | Haojie Li, Hong Zhang, Wanli Ouyang, Xinzhu Ma, Yuxin Yue, Zhihui Wang, Zhiyu Chong |  |
| 444 |  |  [EXACT: Scalable Graph Neural Networks Training via Extreme Activation Compression](https://openreview.net/forum?id=vkaMaq95_rX) |  | 0 | Training Graph Neural Networks (GNNs) on large graphs is a fundamental challenge due to the high memory usage, which is mainly occupied by activations (e.g., node embeddings). Previous works usually focus on reducing the number of nodes retained in memory. In parallel, unlike what has been developed for other types of neural networks, training with compressed activation... | Fan Yang, Kaixiong Zhou, Li Li, Rui Chen, Xia Hu, Zirui Liu |  |
| 445 |  |  [Provably convergent quasistatic dynamics for mean-field two-player zero-sum games](https://openreview.net/forum?id=MP904TiHqJ-) |  | 0 | In this paper, we study the problem of finding mixed Nash equilibrium for mean-field two-player zero-sum games. Solving this problem requires optimizing over two probability distributions. We consider a quasistatic Wasserstein gradient flow dynamics in which one probability distribution follows the Wasserstein gradient flow, while the other one is always at the... | Chao Ma, Lexing Ying |  |
| 446 |  |  [W-CTC: a Connectionist Temporal Classification Loss with Wild Cards](https://openreview.net/forum?id=0RqDp8FCW5Z) |  | 0 | Connectionist Temporal Classification (CTC) loss is commonly used in sequence learning applications. For example, in Automatic Speech Recognition (ASR) task, the training data consists of pairs of audio (input sequence) and text (output label),without temporal alignment information. Standard CTC computes a loss by aggregating over all possible alignment paths, that map... | Guangxu Xun, Jiahong Yuan, Jiaji Huang, Kenneth Church, Xingyu Cai, Yuchen Bian |  |
| 447 |  |  [Bandit Learning with Joint Effect of Incentivized Sampling, Delayed Sampling Feedback, and Self-Reinforcing User Preferences](https://openreview.net/forum?id=Q83vFlie_Pr) |  | 0 | In this paper, we consider a new multi-armed bandit (MAB) framework motivated by three common complications in online recommender systems in practice: (i) the platform (learning agent) cannot sample an intended product directly and has to incentivize customers to select this product (e.g., promotions and coupons); (ii) customer feedbacks are often received later than... | Chaosheng Dong, Jia Liu, Tianchen Zhou, Yi Sun |  |
| 448 |  |  [AdaAug: Learning Class- and Instance-adaptive Data Augmentation Policies](https://openreview.net/forum?id=rWXfFogxRJN) |  | 0 | Data augmentation is an effective way to improve the generalization capability of modern deep learning models. However, the underlying augmentation methods mostly rely on handcrafted operations. Moreover, an augmentation policy useful to one dataset may not transfer well to other datasets. Therefore, Automated Data Augmentation (AutoDA) methods, like... | DitYan Yeung, TszHim Cheung |  |
| 449 |  |  [Unsupervised Semantic Segmentation by Distilling Feature Correspondences](https://openreview.net/forum?id=SaKO6z6Hl0c) |  | 0 | Unsupervised semantic segmentation aims to discover and localize semantically meaningful categories within image corpora without any form of annotation. To solve this task, algorithms must produce features for every pixel that are both semantically meaningful and compact enough to form distinct clusters. Unlike previous works which achieve this with a single end-to-end... | Bharath Hariharan, Mark Hamilton, Noah Snavely, William T. Freeman, Zhoutong Zhang |  |
| 450 |  |  [Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning](https://openreview.net/forum?id=TqNsv1TuCX9) |  | 0 | Visual search, recommendation, and contrastive similarity learning power technologies that impact billions of users worldwide. Modern model architectures can be complex and difficult to interpret, and there are several competing techniques one can use to explain a search engine's behavior. We show that the theory of fair credit assignment provides a unique axiomatic... | Lei Zhang, Mark Hamilton, Scott M. Lundberg, Stephanie Fu, William T. Freeman |  |
| 451 |  |  [Graph-Relational Domain Adaptation](https://openreview.net/forum?id=kcwyXtt7yDJ) |  | 0 | Existing domain adaptation methods tend to treat every domain equally and align them all perfectly. Such uniform alignment ignores topological structures among different domains; therefore it may be beneficial for nearby domains, but not necessarily for distant domains. In this work, we relax such uniform alignment by using a domain graph to encode domain adjacency,... | Bernie Wang, GuangHe Lee, Hao He, Hao Wang, Zihao Xu |  |
| 452 |  |  [Revisit Kernel Pruning with Lottery Regulated Grouped Convolutions](https://openreview.net/forum?id=LdEhiMG9WLO) |  | 0 | Structured pruning methods which are capable of delivering a densely pruned network are among the most popular techniques in the realm of neural network pruning, where most methods prune the original network at a filter or layer level. Although such methods may provide immediate compression and acceleration benefits, we argue that the blanket removal of an entire filter... | Guanqun Zhang, Ningjia Huang, Shaochen (Henry) Zhong, Shuai Xu |  |
| 453 |  |  [Bi-linear Value Networks for Multi-goal Reinforcement Learning](https://openreview.net/forum?id=LedObtLmCjS) |  | 0 | Universal value functions are a core component of off-policy multi-goal reinforcement learning. The de-facto paradigm is to approximate Q(s, a, g) using monolithic neural networks which lack inductive biases to produce complex interactions between the state s and the goal g. In this work, we propose a bilinear decomposition that represents the Q-value via a low-rank... | Ge Yang, Pulkit Agrawal, ZhangWei Hong |  |
| 454 |  |  [No One Representation to Rule Them All: Overlapping Features of Training Methods](https://openreview.net/forum?id=BK-4qbGgIE3) |  | 0 | Despite being able to capture a range of features of the data, high accuracy models trained with supervision tend to make similar predictions. This seemingly implies that high-performing models share similar biases regardless of training methodology, which would limit ensembling benefits and render low-accuracy models as having little practical use. Against this... | Ekin Dogus Cubuk, Raphael Gontijo Lopes, Yann N. Dauphin |  |
| 455 |  |  [Generalized Kernel Thinning](https://openreview.net/forum?id=IfNu7Dr-3fQ) |  | 0 | The kernel thinning (KT) algorithm of Dwivedi and Mackey (2021) compresses a probability distribution more effectively than independent sampling by targeting a reproducing kernel Hilbert space (RKHS) and leveraging a less smooth square-root kernel. Here we provide four improvements. First, we show that KT applied directly to the target RKHS yields tighter,... | Lester Mackey, Raaz Dwivedi |  |
| 456 |  |  [How Much Can CLIP Benefit Vision-and-Language Tasks?](https://openreview.net/forum?id=zf_Ll3HZWgy) |  | 0 | Most existing Vision-and-Language (V&L) models rely on pre-trained visual encoders, using a relatively small set of manually-annotated data (as compared to web-crawled data), to perceive the visual world. However, it has been observed that large-scale pretraining usually can result in better generalization performance, e.g., CLIP (Contrastive Language-Image... | Anna Rohrbach, Hao Tan, KaiWei Chang, Kurt Keutzer, Liunian Harold Li, Mohit Bansal, Sheng Shen, Zhewei Yao |  |
| 457 |  |  [Large Learning Rate Tames Homogeneity: Convergence and Balancing Effect](https://openreview.net/forum?id=3tbDrs77LJ5) |  | 0 | Recent empirical advances show that training deep models with large learning rate often improves generalization performance. However, theoretical justifications on the benefits of large learning rate are highly limited, due to challenges in analysis. In this paper, we consider using Gradient Descent (GD) with a large learning rate on a homogeneous matrix factorization... | Minshuo Chen, Molei Tao, Tuo Zhao, Yuqing Wang |  |
| 458 |  |  [Demystifying Limited Adversarial Transferability in Automatic Speech Recognition Systems](https://openreview.net/forum?id=l5aSHXi8jG5) |  | 0 | The targeted transferability of adversarial samples enables attackers to exploit black-box models in the real-world. The most popular method to produce these adversarial samples is optimization attacks, which have been shown to achieve a high level of transferability in some domains. However, recent research has demonstrated that these attack samples fail to transfer... | Aditya Karlekar, Hadi Abdullah, Patrick Traynor, Vincent Bindschaedler |  |
| 459 |  |  [PipeGCN: Efficient Full-Graph Training of Graph Convolutional Networks with Pipelined Feature Communication](https://openreview.net/forum?id=kSwqMH0zn1F) |  | 0 | Graph Convolutional Networks (GCNs) is the state-of-the-art method for learning graph-structured data, and training large-scale GCNs requires distributed training across multiple accelerators such that each accelerator is able to hold a partitioned subgraph. However, distributed GCN training incurs prohibitive overhead of communicating node features and feature... | Anastasios Kyrillidis, Cameron R. Wolfe, Cheng Wan, Nam Sung Kim, Yingyan Lin, Youjie Li |  |
| 460 |  |  [Learning Neural Contextual Bandits through Perturbed Rewards](https://openreview.net/forum?id=7inCJ3MhXt3) |  | 0 | Thanks to the power of representation learning, neural contextual bandit algorithms demonstrate remarkable performance improvement against their classical counterparts. But because their exploration has to be performed in the entire neural network parameter space to obtain nearly optimal regret, the resulting computational cost is prohibitively high. We propose to... | Dongruo Zhou, Hongning Wang, Quanquan Gu, Weitong Zhang, Yiling Jia |  |
| 461 |  |  [Adversarial Unlearning of Backdoors via Implicit Hypergradient](https://openreview.net/forum?id=MeeQkFYVbzW) |  | 0 | We propose a minimax formulation for removing backdoors from a given poisoned model based on a small set of clean data. This formulation encompasses much of prior work on backdoor removal. We propose the Implicit Backdoor Adversarial Unlearning (I-BAU) algorithm to solve the minimax. Unlike previous work, which breaks down the minimax into separate inner and outer... | Ming Jin, Ruoxi Jia, Si Chen, Won Park, Yi Zeng, Zhuoqing Mao |  |
| 462 |  |  [Maximizing Ensemble Diversity in Deep Reinforcement Learning](https://openreview.net/forum?id=hjd-kcpDpf2) |  | 0 | Modern deep reinforcement learning (DRL) has been successful in solving a range of challenging sequential decision-making problems. Most of these algorithms use an ensemble of neural networks as their backbone structure and benefit from the diversity among the neural networks to achieve optimal results. Unfortunately, the members of the ensemble can converge to the same... | Hassam Sheikh, Ladislau Bölöni, Mariano Phielipp |  |
| 463 |  |  [Graph Neural Networks with Learnable Structural and Positional Representations](https://openreview.net/forum?id=wTTjnvGphYj) |  | 0 | Graph neural networks (GNNs) have become the standard learning architectures for graphs. GNNs have been applied to numerous domains ranging from quantum chemistry, recommender systems to knowledge graphs and natural language processing. A major issue with arbitrary graphs is the absence of canonical positional information of nodes, which decreases the representation... | Anh Tuan Luu, Thomas Laurent, Vijay Prakash Dwivedi, Xavier Bresson, Yoshua Bengio |  |
| 464 |  |  [Zero-Shot Self-Supervised Learning for MRI Reconstruction](https://openreview.net/forum?id=085y6YPaYjP) |  | 0 | Deep learning (DL) has emerged as a powerful tool for accelerated MRI reconstruction, but often necessitates a database of fully-sampled measurements for training. Recent self-supervised and unsupervised learning approaches enable training without fully-sampled data. However, a database of undersampled measurements may not be available in many scenarios, especially for... | Burhaneddin Yaman, Mehmet Akçakaya, Seyed Amir Hossein Hosseini |  |
| 465 |  |  [Policy Smoothing for Provably Robust Reinforcement Learning](https://openreview.net/forum?id=mwdfai8NBrJ) |  | 0 | The study of provable adversarial robustness for deep neural networks (DNNs) has mainly focused on $\textit{static}$ supervised learning tasks such as image classification. However, DNNs have been used extensively in real-world $\textit{adaptive}$ tasks such as reinforcement learning (RL), making such systems vulnerable to adversarial attacks as well. Prior works in... | Alexander Levine, Aounon Kumar, Soheil Feizi |  |
| 466 |  |  [The Close Relationship Between Contrastive Learning and Meta-Learning](https://openreview.net/forum?id=gICys3ITSmj) |  | 0 | Contrastive learning has recently taken off as a paradigm for learning from unlabeled data. In this paper, we discuss the close relationship between contrastive learning and meta-learning under a certain task distribution. We complement this observation by showing that established meta-learning methods, such as Prototypical Networks, achieve comparable performance to... | Hossein Souri, Manli Shu, Micah Goldblum, Renkun Ni, Tom Goldstein |  |
| 467 |  |  [Towards Understanding Generalization via Decomposing Excess Risk Dynamics](https://openreview.net/forum?id=rS9-7AuPKWK) |  | 0 | Generalization is one of the fundamental issues in machine learning. However, traditional techniques like uniform convergence may be unable to explain generalization under overparameterization \citep{nagarajan2019uniform}. As alternative approaches, techniques based on stability analyze the training dynamics and derive algorithm-dependent generalization bounds.... | Jianhao Ma, Jiaye Teng, Yang Yuan |  |
| 468 |  |  [Graph Auto-Encoder via Neighborhood Wasserstein Reconstruction](https://openreview.net/forum?id=ATUh28lnSuW) |  | 0 | Graph neural networks (GNNs) have drawn significant research attention recently, mostly under the setting of semi-supervised learning. When task-agnostic representations are preferred or supervision is simply unavailable, the auto-encoder framework comes in handy with a natural graph reconstruction objective for unsupervised GNN training. However, existing graph... | Carl Yang, Mingyue Tang, Pan Li |  |
| 469 |  |  [FairCal: Fairness Calibration for Face Verification](https://openreview.net/forum?id=nRj0NcmSuxb) |  | 0 | Despite being widely used, face recognition models suffer from bias: the probability of a false positive (incorrect face match) strongly depends on sensitive attributes such as the ethnicity of the face. As a result, these models can disproportionately and negatively impact minority groups, particularly when used by law enforcement. The majority of bias reduction... | Adam M. Oberman, Noah Marshall, Stephanie Cairns, Tiago Salvador, Vikram Voleti |  |
| 470 |  |  [Cross-Lingual Transfer with Class-Weighted Language-Invariant Representations](https://openreview.net/forum?id=k7-s5HSSPE5) |  | 0 | Recent advances in neural modeling have produced deep multilingual language models capable of extracting cross-lingual knowledge from non-parallel texts and enabling zero-shot downstream transfer. While their success is often attributed to shared representations, quantitative analyses are limited. Towards a better understanding, through empirical analyses, we show that... | Han Zhao, Heng Ji, Ruicheng Xian |  |
| 471 |  |  [ComPhy: Compositional Physical Reasoning of Objects and Events from Videos](https://openreview.net/forum?id=PgNEYaIc81Q) |  | 0 | Objects' motions in nature are governed by complex interactions and their properties. While some properties, such as shape and material, can be identified via the object's visual appearances, others like mass and electric charge are not directly visible. The compositionality between the visible and hidden properties poses unique challenges for AI models to reason from... | Antonio Torralba, Chuang Gan, Joshua B. Tenenbaum, Kexin Yi, Mingyu Ding, Yunzhu Li, Zhenfang Chen |  |
| 472 |  |  [An Information Fusion Approach to Learning with Instance-Dependent Label Noise](https://openreview.net/forum?id=ecH2FKaARUp) |  | 0 | Instance-dependent label noise (IDN) widely exists in real-world datasets and usually misleads the training of deep neural networks. Noise transition matrix (NTM) (i.e., the probability that clean labels flip into noisy labels) is used to characterize the label noise and can be adopted to bridge the gap between clean and noisy underlying data distributions. However,... | Kaixiong Zhou, Li Li, Rui Chen, SooHyun Choi, Xia Hu, Zhimeng Jiang, Zirui Liu |  |
| 473 |  |  [On Redundancy and Diversity in Cell-based Neural Architecture Search](https://openreview.net/forum?id=rFJWoYoxrDB) |  | 0 | Searching for the architecture cells is a dominant paradigm in NAS. However, little attention has been devoted to the analysis of the cell-based search spaces even though it is highly important for the continual development of NAS. In this work, we conduct an empirical post-hoc analysis of architectures from the popular cell-based search spaces and find that the... | Binxin Ru, Pedro M. Esperança, Xingchen Wan, Zhenguo Li |  |
| 474 |  |  [Deep Learning without Shortcuts: Shaping the Kernel with Tailored Rectifiers](https://openreview.net/forum?id=U0k7XNTiFEq) |  | 0 | Training very deep neural networks is still an extremely challenging task. The common solution is to use shortcut connections and normalization layers, which are both crucial ingredients in the popular ResNet architecture. However, there is strong evidence to suggest that ResNets behave more like ensembles of shallower networks than truly deep ones. Recently, it was... | Aleksandar Botev, Guodong Zhang, James Martens |  |
| 475 |  |  [Variational autoencoders in the presence of low-dimensional data: landscape and implicit bias](https://openreview.net/forum?id=y_op4lLLaWL) |  | 0 | Variational Autoencoders (VAEs) are one of the most commonly used generative models, particularly for image data. A prominent difficulty in training VAEs is data that is supported on a lower dimensional manifold. Recent work by Dai and Wipf (2020) proposes a two-stage training algorithm for VAEs, based on a conjecture that in standard VAE training the generator will... | Andrej Risteski, Chenghui Zhou, Frederic Koehler, Viraj Mehta |  |
| 476 |  |  [No Parameters Left Behind: Sensitivity Guided Adaptive Learning Rate for Training Large Transformer Models](https://openreview.net/forum?id=cuvga_CiVND) |  | 0 | Recent research has shown the existence of significant redundancy in large Transformer models. One can prune the redundant parameters without significantly sacrificing the generalization performance. However, we question whether the redundant parameters could have contributed more if they were properly trained. To answer this question, we propose a novel training... | Chen Liang, Haoming Jiang, Jianfeng Gao, Pengcheng He, Simiao Zuo, Tuo Zhao, Weizhu Chen, Xiaodong Liu |  |
| 477 |  |  [SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations](https://openreview.net/forum?id=aBsCjcPu_tE) |  | 0 | Guided image synthesis enables everyday users to create and edit photo-realistic images with minimum effort. The key challenge is balancing faithfulness to the user inputs (e.g., hand-drawn colored strokes) and realism of the synthesized images. Existing GAN-based methods attempt to achieve such balance using either conditional GANs or GAN inversions, which are... | Chenlin Meng, Jiajun Wu, Jiaming Song, JunYan Zhu, Stefano Ermon, Yang Song, Yutong He |  |
| 478 |  |  [Post hoc Explanations may be Ineffective for Detecting Unknown Spurious Correlation](https://openreview.net/forum?id=xNOVfCCvDpM) |  | 0 | We investigate whether three types of post hoc model explanations–feature attribution, concept activation, and training point ranking–are effective for detecting a model’s reliance on spurious signals in the training data. Specifically, we consider the scenario where the spurious signal to be detected is unknown, at test-time, to the user of the explanation method. We... | Been Kim, Harold Abelson, Julius Adebayo, Michael Muelly |  |
| 479 |  |  [Generalizing Few-Shot NAS with Gradient Matching](https://openreview.net/forum?id=_jMtny3sMKU) |  | 0 | Efficient performance estimation of architectures drawn from large search spaces is essential to Neural Architecture Search. One-Shot methods tackle this challenge by training one supernet to approximate the performance of every architecture in the search space via weight-sharing, thereby drastically reducing the search cost. However, due to coupled optimization between... | ChoJui Hsieh, Jiashi Feng, Lanqing Hong, Ruochen Wang, Shoukang Hu, Zhenguo Li |  |
| 480 |  |  [The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training](https://openreview.net/forum?id=VBZJ_3tz-t) |  | 0 | Random pruning is arguably the most naive way to attain sparsity in neural networks, but has been deemed uncompetitive by either post-training pruning or sparse training. In this paper, we focus on sparse training and highlight a perhaps counter-intuitive finding, that random pruning at initialization can be quite powerful for the sparse training of modern neural... | Decebal Constantin Mocanu, Li Shen, Mykola Pechenizkiy, Shiwei Liu, Tianlong Chen, Xiaohan Chen, Zhangyang Wang |  |
| 481 |  |  [switch-GLAT: Multilingual Parallel Machine Translation Via Code-Switch Decoder](https://openreview.net/forum?id=5HvpvYd68b) |  | 0 | Multilingual machine translation aims to develop a single model for multiple language directions. However, existing multilingual models based on Transformer are limited in terms of both translation performance and inference speed. In this paper, we propose switch-GLAT, a non-autoregressive multilingual machine translation model with a code-switch decoder. It can... | Hao Zhou, Jingjing Xu, Lei Li, Lihua Qian, Mingxuan Wang, Shanbo Cheng, Zhenqiao Song |  |
| 482 |  |  [DictFormer: Tiny Transformer with Shared Dictionary](https://openreview.net/forum?id=GWQWAeE9EpB) |  | 0 | We introduce DictFormer with the efficient shared dictionary to provide a compact, fast, and accurate transformer model. DictFormer significantly reduces the redundancy in the transformer's parameters by replacing the prior transformer's parameters with a compact, shared dictionary, few unshared coefficients, and indices. Also, DictFormer enables faster computations... | Hongxia Jin, Qian Lou, Ting Hua, YenChang Hsu, Yilin Shen |  |
| 483 |  |  [Training Transition Policies via Distribution Matching for Complex Tasks](https://openreview.net/forum?id=6vkzF28Hur8) |  | 0 | Humans decompose novel complex tasks into simpler ones to exploit previously learned skills. Analogously, hierarchical reinforcement learning seeks to leverage lower-level policies for simple tasks to solve complex ones. However, because each lower-level policy induces a different distribution of states, transitioning from one lower-level policy to another may fail due... | Andrew Perrault, JuSeung Byun |  |
| 484 |  |  [GDA-AM: On the Effectiveness of Solving Min-Imax Optimization via Anderson Mixing](https://openreview.net/forum?id=3YqeuCVwy1d) |  | 0 | Many modern machine learning algorithms such as generative adversarial networks (GANs) and adversarial training can be formulated as minimax optimization.Gradient descent ascent (GDA) is the most commonly used algorithm due to its simplicity. However, GDA can converge to non-optimal minimax points. We propose a new minimax optimization framework,GDA-AM, that views the... | Huan He, Joyce C. Ho, Shifan Zhao, Yousef Saad, Yuanzhe Xi |  |
| 485 |  |  [On feature learning in neural networks with global convergence guarantees](https://openreview.net/forum?id=PQTW3iG4sC-) |  | 0 | We study the gradient flow optimization of over-parameterized neural networks (NNs) in a setup that allows feature learning while admitting non-asymptotic global convergence guarantees. First, we prove that for wide shallow NNs under the mean-field (MF) scaling and with a general class of activation functions, when the input dimension is at least the size of the... | Eric VandenEijnden, Joan Bruna, Zhengdao Chen |  |
| 486 |  |  [The Three Stages of Learning Dynamics in High-dimensional Kernel Methods](https://openreview.net/forum?id=EQmAP4F859) |  | 0 | To understand how deep learning works, it is crucial to understand the training dynamics of neural networks. Several interesting hypotheses about these dynamics have been made based on empirically observed phenomena, but there exists a limited theoretical understanding of when and why such phenomena occur. In this paper, we consider the training dynamics of gradient... | Bin Yu, Nikhil Ghosh, Song Mei |  |
| 487 |  |  [When Can We Learn General-Sum Markov Games with a Large Number of Players Sample-Efficiently?](https://openreview.net/forum?id=6MmiS0HUJHR) |  | 0 | Multi-agent reinforcement learning has made substantial empirical progresses in solving games with a large number of players. However, theoretically, the best known sample complexity for finding a Nash equilibrium in general-sum games scales exponentially in the number of players due to the size of the joint action space, and there is a matching exponential lower bound.... | Song Mei, Yu Bai, Ziang Song |  |
| 488 |  |  [Neural Networks as Kernel Learners: The Silent Alignment Effect](https://openreview.net/forum?id=1NvflqAdoom) |  | 0 | Neural networks in the lazy training regime converge to kernel machines. Can neural networks in the rich feature learning regime learn a kernel machine with a data-dependent kernel? We demonstrate that this can indeed happen due to a phenomenon we term silent alignment, which requires that the tangent kernel of a network evolves in eigenstructure while small and before... | Alexander B. Atanasov, Blake Bordelon, Cengiz Pehlevan |  |
| 489 |  |  [Learning Object-Oriented Dynamics for Planning from Text](https://openreview.net/forum?id=B6EIcyp-Rb7) |  | 0 | The advancement of dynamics models enables model-based planning in complex environments. Existing dynamics models commonly study image-based games with fully observable states. Generalizing these models to Text-Based Games (TBGs), which commonly describe the partially observable states with noisy text observations, is challenging. In this work, we propose an... | Amirmassoud Farahmand, Ashutosh Adhikari, Guiliang Liu, Pascal Poupart |  |
| 490 |  |  [An Operator Theoretic View On Pruning Deep Neural Networks](https://openreview.net/forum?id=pWBNOgdeURp) |  | 0 | The discovery of sparse subnetworks that are able to perform as well as full models has found broad applied and theoretical interest. While many pruning methods have been developed to this end, the naïve approach of removing parameters based on their magnitude has been found to be as robust as more complex, state-of-the-art algorithms. The lack of theory behind... | Igor Mezic, Maria Fonoberova, Ryan Mohr, William T. Redman, Yannis G. Kevrekidis |  |
| 491 |  |  [Capacity of Group-invariant Linear Readouts from Equivariant Representations: How Many Objects can be Linearly Classified Under All Possible Views?](https://openreview.net/forum?id=_4GFbtOuWq-) |  | 0 | Equivariance has emerged as a desirable property of representations of objects subject to identity-preserving transformations that constitute a group, such as translations and rotations. However, the expressivity of a representation constrained by group equivariance is still not fully understood. We address this gap by providing a generalization of Cover's Function... | Blake Bordelon, Cengiz Pehlevan, Matthew Farrell, Shubhendu Trivedi |  |
| 492 |  |  [Tuformer: Data-driven Design of Transformers for Improved Generalization or Efficiency](https://openreview.net/forum?id=V0A5g83gdQ_) |  | 0 | Transformers are neural network architectures that achieve remarkable performance in many areas. However, the core component of Transformers, multi-head self-attention (MHSA), is mainly derived from heuristics, and the interactions across its components are not well understood. To address the problem, we first introduce a mathematically rigorous and yet intuitive tensor... | Furong Huang, Jiahao Su, Xiaoyu Liu |  |
| 493 |  |  [Learning Weakly-supervised Contrastive Representations](https://openreview.net/forum?id=MSwEFaztwkE) |  | 0 | We argue that a form of the valuable information provided by the auxiliary information is its implied data clustering information. For instance, considering hashtags as auxiliary information, we can hypothesize that an Instagram image will be semantically more similar with the same hashtags. With this intuition, we present a two-stage weakly-supervised contrastive... | LouisPhilippe Morency, Peiyuan Liao, Ruslan Salakhutdinov, Tianqin Li, Weixin Liu, YaoHung Hubert Tsai |  |
| 494 |  |  [Encoding Weights of Irregular Sparsity for Fixed-to-Fixed Model Compression](https://openreview.net/forum?id=Vs5NK44aP9P) |  | 0 | Even though fine-grained pruning techniques achieve a high compression ratio, conventional sparsity representations (such as CSR) associated with irregular sparsity degrade parallelism significantly. Practical pruning methods, thus, usually lower pruning rates (by structured pruning) to improve parallelism. In this paper, we study fixed-to-fixed (lossless) encoding... | Baeseong Park, Byeongwook Kim, Daehwan Oh, Dongsoo Lee, Se Jung Kwon |  |
| 495 |  |  [An Experimental Design Perspective on Model-Based Reinforcement Learning](https://openreview.net/forum?id=0no8Motr-zO) |  | 0 | In many practical applications of RL, it is expensive to observe state transitions from the environment. For example, in the problem of plasma control for nuclear fusion, computing the next state for a given state-action pair requires querying an expensive transition function which can lead to many hours of computer simulation or dollars of scientific research. Such... | Biswajit Paria, Jeff Schneider, Stefano Ermon, Viraj Mehta, Willie Neiswanger |  |
| 496 |  |  [BAM: Bayes with Adaptive Memory](https://openreview.net/forum?id=NdOoQnYPj_) |  | 0 | Online learning via Bayes' theorem allows new data to be continuously integrated into an agent's current beliefs. However, a naive application of Bayesian methods in non-stationary environments leads to slow adaptation and results in state estimates that may converge confidently to the wrong parameter value. A common solution when learning in changing environments is to... | Ben Evans, Jennifer Rogers Brennan, Josue Nassar, Kendall Lowrey |  |
| 497 |  |  [Unsupervised Learning of Full-Waveform Inversion: Connecting CNN and Partial Differential Equation in a Loop](https://openreview.net/forum?id=izvwgBic9q) |  | 0 | This paper investigates unsupervised learning of Full-Waveform Inversion (FWI), which has been widely used in geophysics to estimate subsurface velocity maps from seismic data. This problem is mathematically formulated by a second order partial differential equation (PDE), but is hard to solve. Moreover, acquiring velocity map is extremely expensive, making it... | Peng Jin, Sharon Xiaolei Huang, Xitong Zhang, Yinpeng Chen, Youzuo Lin, Zicheng Liu |  |
| 498 |  |  [Conditional Contrastive Learning with Kernel](https://openreview.net/forum?id=AAJLBoGt0XM) |  | 0 | Conditional contrastive learning frameworks consider the conditional sampling procedure that constructs positive or negative data pairs conditioned on specific variables. Fair contrastive learning constructs negative pairs, for example, from the same gender (conditioning on sensitive information), which in turn reduces undesirable information from the learned... | Han Zhao, Kun Zhang, LouisPhilippe Morency, Martin Q. Ma, Ruslan Salakhutdinov, Tianqin Li, YaoHung Hubert Tsai |  |
| 499 |  |  [ConFeSS: A Framework for Single Source Cross-Domain Few-Shot Learning](https://openreview.net/forum?id=zRJu6mU2BaE) |  | 0 | Most current few-shot learning methods train a model from abundantly labeled base category data and then transfer and adapt the model to sparsely labeled novel category data. These methods mostly generalize well on novel categories from the same domain as the base categories but perform poorly for distant domain categories. In this paper, we propose a framework for... | Debasmit Das, Fatih Porikli, Sungrack Yun |  |
| 500 |  |  [Granger causal inference on DAGs identifies genomic loci regulating transcription](https://openreview.net/forum?id=nZOUYEN6Wvy) |  | 0 | When a dynamical system can be modeled as a sequence of observations, Granger causality is a powerful approach for detecting predictive interactions between its variables. However, traditional Granger causal inference has limited utility in domains where the dynamics need to be represented as directed acyclic graphs (DAGs) rather than as a linear sequence, such as with... | Alexander P. Wu, Bonnie Berger, Rohit Singh |  |
| 501 |  |  [Energy-Inspired Molecular Conformation Optimization](https://openreview.net/forum?id=7QfLW-XZTl) |  | 0 | This paper studies an important problem in computational chemistry: predicting a molecule's spatial atom arrangements, or a molecular conformation. We propose a neural energy minimization formulation that casts the prediction problem into an unrolled optimization process, where a neural network is parametrized to learn the gradient fields of an implicit conformational... | Jian Peng, Jianzhu Ma, Jiaqi Guan, Qiang Liu, WeiYing Ma, Wesley Wei Qian |  |
| 502 |  |  [Towards Deepening Graph Neural Networks: A GNTK-based Optimization Perspective](https://openreview.net/forum?id=tT9t_ZctZRL) |  | 0 | Graph convolutional networks (GCNs) and their variants have achieved great success in dealing with graph-structured data. Nevertheless, it is well known that deep GCNs suffer from the over-smoothing problem, where node representations tend to be indistinguishable as more layers are stacked up. The theoretical research to date on deep GCNs has focused primarily on... | Jie Yin, Ling Chen, Miao Zhang, Richard Y. D. Xu, Wei Huang, Weitao Du, Yayong Li |  |
| 503 |  |  [Connectome-constrained Latent Variable Model of Whole-Brain Neural Activity](https://openreview.net/forum?id=CJzi3dRlJE-) |  | 0 | The availability of both anatomical connectivity and brain-wide neural activity measurements in C. elegans make the worm a promising system for learning detailed, mechanistic models of an entire nervous system in a data-driven way. However, one faces several challenges when constructing such a model. We often do not have direct experimental access to important modeling... | Albert Lin, Aravinthan D. T. Samuel, Lu Mi, Nir Shavit, Richard Xu, Sridhama Prakhya, Srinivas C. Turaga |  |
| 504 |  |  [T-WaveNet: A Tree-Structured Wavelet Neural Network for Time Series Signal Analysis](https://openreview.net/forum?id=U4uFaLyg7PV) |  | 0 | Time series signal analysis plays an essential role in many applications, e.g., activity recognition and healthcare monitoring. Recently, features extracted with deep neural networks (DNNs) have shown to be more effective than conventional hand-crafted ones. However, most existing solutions rely solely on the network to extract information carried in the raw signal,... | Ailing Zeng, Jing Qin, Min Li, Minhao Liu, Qiang Xu, Qiuxia Lai, Ruiyuan Gao |  |
| 505 |  |  [Trans-Encoder: Unsupervised sentence-pair modelling through self- and mutual-distillations](https://openreview.net/forum?id=AmUhwTOHgm) |  | 0 | In NLP, a large volume of tasks involve pairwise comparison between two sequences (e.g. sentence similarity and paraphrase identification). Predominantly, two formulations are used for sentence-pair tasks: bi-encoders and cross-encoders. Bi-encoders produce fixed-dimensional sentence representations and are computationally efficient, however, they usually underperform... | Emine Yilmaz, Fangyu Liu, Jordan Massiah, Serhii Havrylov, Yunlong Jiao |  |
| 506 |  |  [Path Integral Sampler: A Stochastic Control Approach For Sampling](https://openreview.net/forum?id=_uCb2ynRu7Y) |  | 0 | We present Path Integral Sampler~(PIS), a novel algorithm to draw samples from unnormalized probability density functions. The PIS is built on the Schr\"odinger bridge problem which aims to recover the most likely evolution of a diffusion process given its initial distribution and terminal distribution. The PIS draws samples from the initial distribution and then... | Qinsheng Zhang, Yongxin Chen |  |
| 507 |  |  [Model Zoo: A Growing Brain That Learns Continually](https://openreview.net/forum?id=WfvgGBcgbE7) |  | 0 | This paper argues that continual learning methods can benefit by splitting the capacity of the learner across multiple models. We use statistical learning theory and experimental analysis to show how multiple tasks can interact with each other in a non-trivial fashion when a single model is trained on them. The generalization error on a particular task can improve when... | Pratik Chaudhari, Rahul Ramesh |  |
| 508 |  |  [Predicting Physics in Mesh-reduced Space with Temporal Attention](https://openreview.net/forum?id=XctLdNfCmP) |  | 0 | Auto-regressive sequence models for physics prediction are often restricted to low-dimensional systems, as memory cost increases with both spatial extents and sequence length. On the other hand, graph-based next-step prediction models have recently been very successful in modeling complex high-dimensional physical systems on irregular meshes, but suffer from error... | Han Gao, JianXun Wang, Liping Liu, Tobias Pfaff, Xu Han |  |
| 509 |  |  [How unlabeled data improve generalization in self-training? A one-hidden-layer theoretical analysis](https://openreview.net/forum?id=qiMXBIf4NfB) |  | 0 | Self-training, a semi-supervised learning algorithm, leverages a large amount of unlabeled data to improve learning when the labeled data are limited. Despite empirical successes, its theoretical characterization remains elusive. To the best of our knowledge, this work establishes the first theoretical analysis for the known iterative self-training paradigm and formally... | Jinjun Xiong, Meng Wang, PinYu Chen, Shuai Zhang, Sijia Liu |  |
| 510 |  |  [Learning to Dequantise with Truncated Flows](https://openreview.net/forum?id=fExcSKdDo_) |  | 0 | Dequantisation is a general technique used for transforming data described by a discrete random variable $x$ into a continuous (latent) random variable $z$, for the purpose of it being modeled by likelihood-based density models. Dequantisation was first introduced in the context of ordinal data, such as image pixel values. However, when the data is categorical, the... | Aaron C. Courville, Alessandro Sordoni, ChinWei Huang, Shawn Tan |  |
| 511 |  |  [Curriculum learning as a tool to uncover learning principles in the brain](https://openreview.net/forum?id=TpJMvo0_pu-) |  | 0 | We present a novel approach to use curricula to identify principles by which a system learns. Previous work in curriculum learning has focused on how curricula can be designed to improve learning of a model on particular tasks. We consider the inverse problem: what can a curriculum tell us about how a learning system acquired a task? Using recurrent neural networks... | Daniel R. Kepple, Kanaka Rajan, Rainer Engelken |  |
| 512 |  |  [Optimizer Amalgamation](https://openreview.net/forum?id=VqzXzA9hjaX) |  | 0 | Selecting an appropriate optimizer for a given problem is of major interest for researchers and practitioners. Many analytical optimizers have been proposed using a variety of theoretical and empirical approaches; however, none can offer a universal advantage over other competitive optimizers. We are thus motivated to study a new problem named Optimizer Amalgamation:... | Lisa Amini, Shiyu Chang, Sijia Liu, Tianlong Chen, Tianshu Huang, Zhangyang Wang |  |
| 513 |  |  [An Agnostic Approach to Federated Learning with Class Imbalance](https://openreview.net/forum?id=Xo0lbDt975) |  | 0 | Federated Learning (FL) has emerged as the tool of choice for training deep models over heterogeneous and decentralized datasets. As a reflection of the experiences from different clients, severe class imbalance issues are observed in real-world FL problems. Moreover, there exists a drastic mismatch between the imbalances from the local and global perspectives, i.e. a... | Alejandro Ribeiro, Hamed Hassani, Juan Cerviño, Zebang Shen |  |
| 514 |  |  [A Fine-Tuning Approach to Belief State Modeling](https://openreview.net/forum?id=ckZY7DGa7FQ) |  | 0 | We investigate the challenge of modeling the belief state of a partially observable Markov system, given sample-access to its dynamics model. This problem setting is often approached using parametric sequential generative modeling methods. However, these methods do not leverage any additional computation at inference time to increase their accuracy. Moreover, applying... | David J. Wu, Hengyuan Hu, J. Zico Kolter, Jakob Nicolaus Foerster, Noam Brown, Samuel Sokota |  |
| 515 |  |  [Differentially Private Fine-tuning of Language Models](https://openreview.net/forum?id=Q42f0dfjECO) |  | 0 | We give simpler, sparser, and faster algorithms for differentially private fine-tuning of large-scale pre-trained language models, which achieve the state-of-the-art privacy versus utility tradeoffs on many standard NLP tasks. We propose a meta-framework for this problem, inspired by the recent success of highly parameter-efficient methods for fine-tuning. Our... | Andre Manoel, Arturs Backurs, Da Yu, Gautam Kamath, Huishuai Zhang, Huseyin A. Inan, Janardhan Kulkarni, Lukas Wutschitz, Saurabh Naik, Sergey Yekhanin, Sivakanth Gopi, Yin Tat Lee |  |
| 516 |  |  [P-Adapters: Robustly Extracting Factual Information from Language Models with Diverse Prompts](https://openreview.net/forum?id=DhzIU48OcZh) |  | 0 | Recent work (e.g. LAMA (Petroni et al., 2019)) has found that the quality of the factual information extracted from Large Language Models (LLMs) depends on the prompts used to query them. This inconsistency is problematic because different users will query LLMs for the same information using different wording, but should receive the same, accurate responses regardless.... | Benjamin Newman, Nazneen Rajani, Prafulla Kumar Choubey |  |
| 517 |  |  [Iterated Reasoning with Mutual Information in Cooperative and Byzantine Decentralized Teaming](https://openreview.net/forum?id=giBFoa-uS12) |  | 0 | Information sharing is key in building team cognition and enables coordination and cooperation. High-performing human teams also benefit from acting strategically with hierarchical levels of iterated communication and rationalizability, meaning a human agent can reason about the actions of their teammates in their decision-making. Yet, the majority of prior work in... | Esmaeil Seraj, Matthew C. Gombolay, Sachin G. Konan |  |
| 518 |  |  [Step-unrolled Denoising Autoencoders for Text Generation](https://openreview.net/forum?id=T0GpzBQ1Fg6) |  | 0 | In this paper we propose a new generative model of text, Step-unrolled Denoising Autoencoder (SUNDAE), that does not rely on autoregressive models. Similarly to denoising diffusion techniques, SUNDAE is repeatedly applied on a sequence of tokens, starting from random inputs and improving them each time until convergence. We present a simple new improvement operator that... | Aäron van den Oord, Erich Elsen, Junyoung Chung, Mikolaj Binkowski, Nikolay Savinov |  |
| 519 |  |  [Hindsight Foresight Relabeling for Meta-Reinforcement Learning](https://openreview.net/forum?id=P7OVkHEoHOZ) |  | 0 | Meta-reinforcement learning (meta-RL) algorithms allow for agents to learn new behaviors from small amounts of experience, mitigating the sample inefficiency problem in RL. However, while meta-RL agents can adapt quickly to new tasks at test time after experiencing only a few trajectories, the meta-training process is still sample-inefficient. Prior works have found... | Jian Peng, Michael Wan, Tanmay Gangwani |  |
| 520 |  |  [LoRA: Low-Rank Adaptation of Large Language Models](https://openreview.net/forum?id=nZeVKeeFYf9) |  | 0 | An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B... | Edward J. Hu, Lu Wang, Phillip Wallis, Shean Wang, Weizhu Chen, Yelong Shen, Yuanzhi Li, Zeyuan AllenZhu |  |
| 521 |  |  [Which Shortcut Cues Will DNNs Choose? A Study from the Parameter-Space Perspective](https://openreview.net/forum?id=qRDQi3ocgR3) |  | 0 | Deep neural networks (DNNs) often rely on easy–to–learn discriminatory features, or cues, that are not necessarily essential to the problem at hand. For example, ducks in an image may be recognized based on their typical background scenery, such as lakes or streams. This phenomenon, also known as shortcut learning, is emerging as a key limitation of the current... | Luca Scimeca, Michael Poli, Sangdoo Yun, Sanghyuk Chun, Seong Joon Oh |  |
| 522 |  |  [Efficient Computation of Deep Nonlinear Infinite-Width Neural Networks that Learn Features](https://openreview.net/forum?id=tUMr0Iox8XW) |  | 0 | While a popular limit of infinite-width neural networks, the Neural Tangent Kernel (NTK) often exhibits performance gaps from finite-width neural networks on standard datasets, due to lack of feature learning. Although the feature learning \*maximal update limit\*, or \*μ-limit\* (Yang and Hu, 2020) of wide networks has closed the gap for 1-hidden-layer linear models,... | Edward J. Hu, Greg Yang, Michael Santacroce |  |
| 523 |  |  [TRAIL: Near-Optimal Imitation Learning with Suboptimal Data](https://openreview.net/forum?id=6q_2b6u0BnJ) |  | 0 | In imitation learning, one aims to learn task-solving policies using access to near-optimal expert trajectories collected from the task environment. However, high-quality trajectories -- e.g., from human experts -- can be expensive to obtain in practical settings. On the contrary, it is often much easier to obtain large amounts of suboptimal trajectories which can... | Mengjiao Yang, Ofir Nachum, Sergey Levine |  |
| 524 |  |  [On the benefits of maximum likelihood estimation for Regression and Forecasting](https://openreview.net/forum?id=zrW-LVXj2k1) |  | 0 | We advocate for a practical Maximum Likelihood Estimation (MLE) approach towards designing loss functions for regression and forecasting, as an alternative to the typical approach of direct empirical risk minimization on a specific target metric. The MLE approach is better suited to capture inductive biases such as prior domain knowledge in datasets, and can output... | Abhimanyu Das, Ananda Theertha Suresh, Pranjal Awasthi, Rajat Sen |  |
| 525 |  |  [Effect of scale on catastrophic forgetting in neural networks](https://openreview.net/forum?id=GhVS8_yPeEa) |  | 0 | Catastrophic forgetting presents a challenge in developing deep learning models capable of continual learning, i.e. learning tasks sequentially. Recently, both computer vision and natural-language processing have witnessed great progress through the use of large-scale pretrained models. In this work, we present an empirical study of catastrophic forgetting in this... | Aitor Lewkowycz, Ethan Dyer, Vinay Venkatesh Ramasesh |  |
| 526 |  |  [Learn Locally, Correct Globally: A Distributed Algorithm for Training Graph Neural Networks](https://openreview.net/forum?id=FndDxSz3LxQ) |  | 0 | Despite the recent success of Graph Neural Networks (GNNs), training GNNs on large graphs remains challenging. The limited resource capacities of the existing servers, the dependency between nodes in a graph, and the privacy concern due to the centralized storage and model learning have spurred the need to design an effective distributed algorithm for GNN training.... | Anand Sivasubramaniam, Mahmut T. Kandemir, Mehrdad Mahdavi, Morteza Ramezani, Weilin Cong |  |
| 527 |  |  [Conditional Image Generation by Conditioning Variational Auto-Encoders](https://openreview.net/forum?id=7MV6uLzOChW) |  | 0 | We present a conditional variational auto-encoder (VAE) which, to avoid the substantial cost of training from scratch, uses an architecture and training objective capable of leveraging a foundation model in the form of a pretrained unconditional VAE. To train the conditional VAE, we only need to train an artifact to perform amortized inference over the unconditional... | Frank Wood, Saeid Naderiparizi, William Harvey |  |
| 528 |  |  [Learning 3D Representations of Molecular Chirality with Invariance to Bond Rotations](https://openreview.net/forum?id=hm2tNDdgaFK) |  | 0 | Molecular chirality, a form of stereochemistry most often describing relative spatial arrangements of bonded neighbors around tetrahedral carbon centers, influences the set of 3D conformers accessible to the molecule without changing its 2D graph connectivity. Chirality can strongly alter (bio)chemical interactions, particularly protein-drug binding. Most 2D graph... | Connor W. Coley, Keir Adams, Lagnajit Pattanaik |  |
| 529 |  |  [Neural Methods for Logical Reasoning over Knowledge Graphs](https://openreview.net/forum?id=tgcAoUVHRIB) |  | 0 | Reasoning is a fundamental problem for computers and deeply studied in Artificial Intelligence. In this paper, we specifically focus on answering multi-hop logical queries on Knowledge Graphs (KGs). This is a complicated task because, in real world scenarios, the graphs tend to be large and incomplete. Most previous works have been unable to create models that accept... | Alfonso Amayuelas, Ce Zhang, Shuai Zhang, Susie Xi Rao |  |
| 530 |  |  [Consistent Counterfactuals for Deep Models](https://openreview.net/forum?id=St6eyiTEHnG) |  | 0 | Counterfactual examples are one of the most commonly-cited methods for explaining the predictions of machine learning models in key areas such as finance and medical diagnosis. Counterfactuals are often discussed under the assumption that the model on which they will be used is static, but in deployment models may be periodically retrained or fine-tuned. This paper... | Emily Black, Matt Fredrikson, Zifan Wang |  |
| 531 |  |  [Unified Visual Transformer Compression](https://openreview.net/forum?id=9jsZiUgkCZP) |  | 0 | Vision transformers (ViTs) have gained popularity recently. Even without customized image operators such as convolutions, ViTs can yield competitive performance when properly trained on massive data. However, the computational overhead of ViTs remains prohibitive, due to stacking multi-head self-attention modules and else. Compared to the vast literature and prevailing... | Huan Yuan, Ji Liu, Jianchao Tan, Jiayi Shen, Sen Yang, Shixing Yu, Tianlong Chen, Zhangyang Wang |  |
| 532 |  |  [Transformer-based Transform Coding](https://openreview.net/forum?id=IDwN6xjHnK8) |  | 0 | Neural data compression based on nonlinear transform coding has made great progress over the last few years, mainly due to improvements in prior models, quantization methods and nonlinear transforms. A general trend in many recent works pushing the limit of rate-distortion performance is to use ever more expensive prior models that can lead to prohibitively slow... | Taco Cohen, Yang Yang, Yinhao Zhu |  |
| 533 |  |  [Object Pursuit: Building a Space of Objects via Discriminative Weight Generation](https://openreview.net/forum?id=lbauk6wK2-y) |  | 0 | We propose a framework to continuously learn object-centric representations for visual learning and understanding. Existing object-centric representations either rely on supervisions that individualize objects in the scene, or perform unsupervised disentanglement that can hardly deal with complex scenes in the real world. To mitigate the annotation burden and relax the... | Chuanyu Pan, Kaichun Mo, Leonidas J. Guibas, Yanchao Yang, Yueqi Duan |  |
| 534 |  |  [PAC Prediction Sets Under Covariate Shift](https://openreview.net/forum?id=DhP9L8vIyLc) |  | 0 | An important challenge facing modern machine learning is how to rigorously quantify the uncertainty of model predictions. Conveying uncertainty is especially important when there are changes to the underlying data distribution that might invalidate the predictive model. Yet, most existing uncertainty quantification algorithms break down in the presence of such shifts.... | Edgar Dobriban, Insup Lee, Osbert Bastani, Sangdon Park |  |
| 535 |  |  [Generalization of Neural Combinatorial Solvers Through the Lens of Adversarial Robustness](https://openreview.net/forum?id=vJZ7dPIjip3) |  | 0 | End-to-end (geometric) deep learning has seen first successes in approximating the solution of combinatorial optimization problems. However, generating data in the realm of NP-hard/-complete tasks brings practical and theoretical challenges, resulting in evaluation protocols that are too optimistic. Specifically, most datasets only capture a simpler subproblem and... | Aleksandar Bojchevski, Jan Schuchardt, Johanna Sommer, Simon Geisler, Stephan Günnemann |  |
| 536 |  |  [One After Another: Learning Incremental Skills for a Changing World](https://openreview.net/forum?id=dg79moSRqIo) |  | 0 | Reward-free, unsupervised discovery of skills is an attractive alternative to the bottleneck of hand-designing rewards in environments where task supervision is scarce or expensive. However, current skill pre-training methods, like many RL techniques, make a fundamental assumption -- stationary environments during training. Traditional methods learn all their skills... | Lerrel Pinto, Nur Muhammad (Mahi) Shafiullah |  |
| 537 |  |  [Graph-Guided Network for Irregularly Sampled Multivariate Time Series](https://openreview.net/forum?id=Kwm8I7dU-l5) |  | 0 | In many domains, including healthcare, biology, and climate science, time series are irregularly sampled with varying time intervals between successive readouts and different subsets of variables (sensors) observed at different time points. Here, we introduce RAINDROP, a graph neural network that embeds irregularly sampled and multivariate time series while also... | Marinka Zitnik, Marko Zeman, Theodoros Tsiligkaridis, Xiang Zhang |  |
| 538 |  |  [FILM: Following Instructions in Language with Modular Methods](https://openreview.net/forum?id=qI4542Y2s1D) |  | 0 | Recent methods for embodied instruction following are typically trained end-to-end using imitation learning. This often requires the use of expert trajectories and low-level language instructions. Such approaches assume that neural states will integrate multimodal semantics to perform state tracking, building spatial memory, exploration, and long-term planning. In... | Devendra Singh Chaplot, Pradeep Kumar Ravikumar, Ruslan Salakhutdinov, So Yeon Min, Yonatan Bisk |  |
| 539 |  |  [The Evolution of Uncertainty of Learning in Games](https://openreview.net/forum?id=Fza94Y8VS4a) |  | 0 | Learning in games has become an object of intense interest for ML due to its connections to numerous AI architectures. We study standard online learning in games but from a non-standard perspective. Instead of studying the behavior of a single initial condition and whether it converges to equilibrium or not, we study the behavior of a probability distribution/measure... | Georgios Piliouras, Yixin Tao, Yun Kuen Cheung |  |
| 540 |  |  [Explainable GNN-Based Models over Knowledge Graphs](https://openreview.net/forum?id=CrCvGNHAIrz) |  | 0 | Graph Neural Networks (GNNs) are often used to learn transformations of graph data. While effective in practice, such approaches make predictions via numeric manipulations so their output cannot be easily explained symbolically. We propose a new family of GNN-based transformations of graph data that can be trained effectively, but where all predictions can be explained... | Bernardo Cuenca Grau, Boris Motik, David Jaime Tena Cucala, Egor V. Kostylev |  |
| 541 |  |  [Mention Memory: incorporating textual knowledge into Transformers through entity mention attention](https://openreview.net/forum?id=OY1A8ejQgEX) |  | 0 | Natural language understanding tasks such as open-domain question answering often require retrieving and assimilating factual information from multiple sources. We propose to address this problem by integrating a semi-parametric representation of a large text corpus into a Transformer model as a source of factual knowledge. Specifically, our method represents knowledge... | Fei Sha, Michiel de Jong, Nicholas FitzGerald, William W. Cohen, Yury Zemlyanskiy |  |
| 542 |  |  [Training Data Generating Networks: Shape Reconstruction via Bi-level Optimization](https://openreview.net/forum?id=dDo8druYppX) |  | 0 | We propose a novel 3d shape representation for 3d shape reconstruction from a single image. Rather than predicting a shape directly, we train a network to generate a training set which will be fed into another learning algorithm to define the shape. The nested optimization problem can be modeled by bi-level optimization. Specifically, the algorithms for bi-level... | Biao Zhang, Peter Wonka |  |
| 543 |  |  [Monotonic Differentiable Sorting Networks](https://openreview.net/forum?id=IcUWShptD7d) |  | 0 | Differentiable sorting algorithms allow training with sorting and ranking supervision, where only the ordering or ranking of samples is known. Various methods have been proposed to address this challenge, ranging from optimal transport-based differentiable Sinkhorn sorting algorithms to making classic sorting networks differentiable. One problem of current... | Christian Borgelt, Felix Petersen, Hilde Kuehne, Oliver Deussen |  |
| 544 |  |  [CrowdPlay: Crowdsourcing Human Demonstrations for Offline Learning](https://openreview.net/forum?id=qyTBxTztIpQ) |  | 0 | Crowdsourcing has been instrumental for driving AI advances that rely on large-scale data. At the same time, reinforcement learning has seen rapid progress through benchmark environments that strike a balance between tractability and real-world complexity, such as ALE and OpenAI Gym. In this paper, we aim to fill a gap at the intersection of these two: The use of... | David C. Parkes, Matthias Gerstgrasser, Rakshit S. Trivedi |  |
| 545 |  |  [Model Agnostic Interpretability for Multiple Instance Learning](https://openreview.net/forum?id=KSSfF5lMIAg) |  | 0 | In Multiple Instance Learning (MIL), models are trained using bags of instances, where only a single label is provided for each bag. A bag label is often only determined by a handful of key instances within a bag, making it difficult to interpret what information a classifier is using to make decisions. In this work, we establish the key requirements for interpreting... | Christine Evers, Joseph Early, Sarvapali D. Ramchurn |  |
| 546 |  |  [FastSHAP: Real-Time Shapley Value Estimation](https://openreview.net/forum?id=Zq2G_VTV53T) |  | 0 | Although Shapley values are theoretically appealing for explaining black-box models, they are costly to calculate and thus impractical in settings that involve large, high-dimensional models. To remedy this issue, we introduce FastSHAP, a new method for estimating Shapley values in a single forward pass using a learned explainer model. To enable efficient training... | Ian Connick Covert, Mukund Sudarshan, Neil Jethani, Rajesh Ranganath, SuIn Lee |  |
| 547 |  |  [When, Why, and Which Pretrained GANs Are Useful?](https://openreview.net/forum?id=4Ycr8oeCoIh) |  | 0 | The literature has proposed several methods to finetune pretrained GANs on new datasets, which typically results in higher performance compared to training from scratch, especially in the limited-data regime. However, despite the apparent empirical benefits of GAN pretraining, its inner mechanisms were not analyzed in-depth, and understanding of its role is not entirely... | Andrey Voynov, Artem Babenko, Timofey Grigoryev |  |
| 548 |  |  [A global convergence theory for deep ReLU implicit networks via over-parameterization](https://openreview.net/forum?id=R332S76RjxS) |  | 0 | Implicit deep learning has received increasing attention recently due to the fact that it generalizes the recursive prediction rule of many commonly used neural network architectures. Its prediction rule is provided implicitly based on the solution of an equilibrium equation. Although a line of recent empirical studies has demonstrated its superior performances, the... | Hailiang Liu, Hongyang Gao, Hridesh Rajan, Jia Liu, Tianxiang Gao |  |
| 549 |  |  [Learnability Lock: Authorized Learnability Control Through Adversarial Invertible Transformations](https://openreview.net/forum?id=6VpeS27viTq) |  | 0 | Owing much to the revolution of information technology, recent progress of deep learning benefits incredibly from the vastly enhanced access to data available in various digital formats. Yet those publicly accessible information also raises a fundamental issue concerning Intellectual Property, that is, how to precisely control legal or illegal exploitation of a dataset... | Jinghui Chen, Weiqi Peng |  |
| 550 |  |  [Federated Learning from Only Unlabeled Data with Class-conditional-sharing Clients](https://openreview.net/forum?id=WHA8009laxu) |  | 0 | Supervised federated learning (FL) enables multiple clients to share the trained model without sharing their labeled data. However, potential clients might even be reluctant to label their own data, which could limit the applicability of FL in practice. In this paper, we show the possibility of unsupervised FL whose model is still a classifier for predicting class... | Gang Niu, Masashi Sugiyama, Nan Lu, Qi Dou, Xiaoxiao Li, Zhao Wang |  |
| 551 |  |  [Transformer Embeddings of Irregularly Spaced Events and Their Participants](https://openreview.net/forum?id=Rty5g9imm7H) |  | 0 | The neural Hawkes process (Mei & Eisner, 2017) is a generative model of irregularly spaced sequences of discrete events. To handle complex domains with many event types, Mei et al. (2020a) further consider a setting in which each event in the sequence updates a deductive database of facts (via domain-specific pattern-matching rules); future events are then conditioned... | Chenghao Yang, Hongyuan Mei, Jason Eisner |  |
| 552 |  |  [Fast Model Editing at Scale](https://openreview.net/forum?id=0DcZxeWfOPt) |  | 0 | While large pre-trained models have enabled impressive results on a variety of downstream tasks, the largest existing models still make errors, and even accurate predictions may become outdated over time. Because detecting all such failures at training time is impossible, enabling both developers and end users of such models to correct inaccurate outputs while leaving... | Antoine Bosselut, Charles Lin, Chelsea Finn, Christopher D. Manning, Eric Mitchell |  |
| 553 |  |  [Eigencurve: Optimal Learning Rate Schedule for SGD on Quadratic Objectives with Skewed Hessian Spectrums](https://openreview.net/forum?id=rTAclwH46Tb) |  | 0 | Learning rate schedulers have been widely adopted in training deep neural networks. Despite their practical importance, there is a discrepancy between its practice and its theoretical analysis. For instance, it is not known what schedules of SGD achieve best convergence, even for simple problems such as optimizing quadratic objectives. In this paper, we propose... | Haishan Ye, Rui Pan, Tong Zhang |  |
| 554 |  |  [An Autoregressive Flow Model for 3D Molecular Geometry Generation from Scratch](https://openreview.net/forum?id=C03Ajc-NS5W) |  | 0 | We consider the problem of generating 3D molecular geometries from scratch. While multiple methods have been developed for generating molecular graphs, generating 3D molecular geometries from scratch is largely under-explored. In this work, we propose G-SphereNet, a novel autoregressive flow model for generating 3D molecular geometries. G-SphereNet employs a flexible... | Shuiwang Ji, Youzhi Luo |  |
| 555 |  |  [On Incorporating Inductive Biases into VAEs](https://openreview.net/forum?id=nzvbBD_3J-g) |  | 0 | We explain why directly changing the prior can be a surprisingly ineffective mechanism for incorporating inductive biases into variational auto-encoders (VAEs), and introduce a simple and effective alternative approach: Intermediary Latent Space VAEs (InteL-VAEs). InteL-VAEs use an intermediary set of latent variables to control the stochasticity of the encoding... | Emile Mathieu, Ning Miao, Siddharth N, Tom Rainforth, Yee Whye Teh |  |
| 556 |  |  [DiffSkill: Skill Abstraction from Differentiable Physics for Deformable Object Manipulations with Tools](https://openreview.net/forum?id=Kef8cKdHWpP) |  | 0 | We consider the problem of sequential robotic manipulation of deformable objects using tools. Previous works have shown that differentiable physics simulators provide gradients to the environment state and help trajectory optimization to converge orders of magnitude faster than model-free reinforcement learning algorithms for deformable object manipulation. However,... | Chuang Gan, David Held, Joshua B. Tenenbaum, Xingyu Lin, Yunzhu Li, Zhiao Huang |  |
| 557 |  |  [On the Existence of Universal Lottery Tickets](https://openreview.net/forum?id=SYB4WrJql1n) |  | 0 | The lottery ticket hypothesis conjectures the existence of sparse subnetworks of large randomly initialized deep neural networks that can be successfully trained in isolation. Recent work has experimentally observed that some of these tickets can be practically reused across a variety of tasks, hinting at some form of universality. We formalize this concept and... | Alkis Gotovos, Nilanjana Laha, Rajarshi Mukherjee, Rebekka Burkholz |  |
| 558 |  |  [Pre-training Molecular Graph Representation with 3D Geometry](https://openreview.net/forum?id=xQUe1pOKPam) |  | 0 | Molecular graph representation learning is a fundamental problem in modern drug and material discovery. Molecular graphs are typically modeled by their 2D topological structures, but it has been recently discovered that 3D geometric information plays a more vital role in predicting molecular functionalities. However, the lack of 3D information in real-world scenarios... | Hanchen Wang, Hongyu Guo, Jian Tang, Joan Lasenby, Shengchao Liu, Weiyang Liu |  |
| 559 |  |  [PER-ETD: A Polynomially Efficient Emphatic Temporal Difference Learning Method](https://openreview.net/forum?id=-HSOjDPfhBJ) |  | 0 | Emphatic temporal difference (ETD) learning (Sutton et al., 2016) is a successful method to conduct the off-policy value function evaluation with function approximation. Although ETD has been shown to converge asymptotically to a desirable value function, it is well-known that ETD often encounters a large variance so that its sample complexity can increase exponentially... | Tengyu Xu, Yingbin Liang, Ziwei Guan |  |
| 560 |  |  [Taming Sparsely Activated Transformer with Stochastic Experts](https://openreview.net/forum?id=B72HXs80q4) |  | 0 | Sparsely activated models (SAMs), such as Mixture-of-Experts (MoE), can easily scale to have outrageously large amounts of parameters without significant increase in computational cost. However, SAMs are reported to be parameter inefficient such that larger models do not always lead to better performance. While most on-going research focuses on improving SAMs models by... | Hany Hassan, Jian Jiao, Jianfeng Gao, Ruofei Zhang, Simiao Zuo, Tuo Zhao, Xiaodong Liu, Young Jin Kim |  |
| 561 |  |  [Hierarchical Variational Memory for Few-shot Learning Across Domains](https://openreview.net/forum?id=i3RI65sR7N) |  | 0 | Neural memory enables fast adaptation to new tasks with just a few training samples. Existing memory models store features only from the single last layer, which does not generalize well in presence of a domain shift between training and test distributions. Rather than relying on a flat memory, we propose a hierarchical alternative that stores features at different... | Cees G. M. Snoek, Ling Shao, Xiantong Zhen, YingJun Du |  |
| 562 |  |  [Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction](https://openreview.net/forum?id=Z1Qlm11uOM) |  | 0 | Video recordings of speech contain correlated audio and visual information, providing a strong signal for speech representation learning from the speaker’s lip movements and the produced sound. We introduce Audio-Visual Hidden Unit BERT (AV-HuBERT), a self-supervised representation learning framework for audio-visual speech, which masks multi-stream video input and... | Abdelrahman Mohamed, Bowen Shi, Kushal Lakhotia, WeiNing Hsu |  |
| 563 |  |  [An Explanation of In-context Learning as Implicit Bayesian Inference](https://openreview.net/forum?id=RdJVFCHjUMI) |  | 0 | Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we... | Aditi Raghunathan, Percy Liang, Sang Michael Xie, Tengyu Ma |  |
| 564 |  |  [Differentiable Scaffolding Tree for Molecule Optimization](https://openreview.net/forum?id=w_drCosT76) |  | 0 | The structural design of functional molecules, also called molecular optimization, is an essential chemical science and engineering task with important applications, such as drug discovery. Deep generative models and combinatorial optimization methods achieve initial success but still struggle with directly modeling discrete chemical structures and often heavily rely on... | Cao Xiao, Connor W. Coley, Jacob Yasonik, Jimeng Sun, Tianfan Fu, Wenhao Gao |  |
| 565 |  |  [Eliminating Sharp Minima from SGD with Truncated Heavy-tailed Noise](https://openreview.net/forum?id=B3Nde6lvab) |  | 0 | The empirical success of deep learning is often attributed to SGD’s mysterious ability to avoid sharp local minima in the loss landscape, as sharp minima are known to lead to poor generalization. Recently, empirical evidence of heavy-tailed gradient noise was reported in many deep learning tasks; and it was shown in (Simsekli et al., 2019a;b) that SGD can escape sharp... | ChangHan Rhee, Sewoong Oh, Xingyu Wang |  |
| 566 |  |  [Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System](https://openreview.net/forum?id=uxxFrDwrE7Y) |  | 0 | Humans excel at continually learning from an ever-changing environment whereas it remains a challenge for deep neural networks which exhibit catastrophic forgetting. The complementary learning system (CLS) theory suggests that the interplay between rapid instance-based learning and slow structured learning in the brain is crucial for accumulating and retaining... | Bahram Zonooz, Elahe Arani, Fahad Sarfraz |  |
| 567 |  |  [FedChain: Chained Algorithms for Near-optimal Communication Cost in Federated Learning](https://openreview.net/forum?id=ZaVVVlcdaN) |  | 0 | Federated learning (FL) aims to minimize the communication complexity of training a model over heterogeneous data distributed across many clients. A common approach is local methods, where clients take multiple optimization steps over local data before communicating with the server (e.g., FedAvg). Local methods can exploit similarity between clients' data. However, in... | Charlie Hou, Giulia Fanti, Kiran Koshy Thekumparampil, Sewoong Oh |  |
| 568 |  |  [What Do We Mean by Generalization in Federated Learning?](https://openreview.net/forum?id=VimqQq-i_Q) |  | 0 | Federated learning data is drawn from a distribution of distributions: clients are drawn from a meta-distribution, and their data are drawn from local data distributions. Generalization studies in federated learning should separate performance gaps from unseen client data (out-of-sample gap) from performance gaps from unseen client distributions (participation gap). In... | Honglin Yuan, Karan Singhal, Lin Ning, Warren Richard Morningstar |  |
| 569 |  |  [Frequency-aware SGD for Efficient Embedding Learning with Provable Benefits](https://openreview.net/forum?id=ibqTBNfJmi) |  | 0 | Embedding learning has found widespread applications in recommendation systems and natural language modeling, among other domains. To learn quality embeddings efficiently, adaptive learning rate algorithms have demonstrated superior empirical performance over SGD, largely accredited to their token-dependent learning rate. However, the underlying mechanism for the... | Baichuan Yuan, Bhargav Bhushanam, Dhruv Choudhary, Guanghui Lan, Tuo Zhao, Xiaohan Wei, Yan Li |  |
| 570 |  |  [Learning Curves for Gaussian Process Regression with Power-Law Priors and Targets](https://openreview.net/forum?id=KeI9E-gsoB) |  | 0 | We characterize the power-law asymptotics of learning curves for Gaussian process regression (GPR) under the assumption that the eigenspectrum of the prior and the eigenexpansion coefficients of the target function follow a power law. Under similar assumptions, we leverage the equivalence between GPR and kernel ridge regression (KRR) to show the generalization error of... | Guido Montúfar, Hui Jin, Pradeep Kr. Banerjee |  |
| 571 |  |  [Fast topological clustering with Wasserstein distance](https://openreview.net/forum?id=0kPL3xO4R5) |  | 0 | The topological patterns exhibited by many real-world networks motivate the development of topology-based methods for assessing the similarity of networks. However, extracting topological structure is difficult, especially for large and dense networks whose node degrees range over multiple orders of magnitude. In this paper, we propose a novel and computationally... | Barry D. Van Veen, Bryan M. Krause, Kirill V. Nourski, Matthew I. Banks, Tananun Songdechakraiwut |  |
| 572 |  |  [Autonomous Reinforcement Learning: Formalism and Benchmarking](https://openreview.net/forum?id=nkaba3ND7B5) |  | 0 | Reinforcement learning (RL) provides a naturalistic framing for learning through trial and error, which is appealing both because of its simplicity and effectiveness and because of its resemblance to how humans and animals acquire skills through experience. However, real-world embodied learning, such as that performed by humans and animals, is situated in a continual,... | Abhishek Gupta, Archit Sharma, Chelsea Finn, Karol Hausman, Kelvin Xu, Nikhil Sardana, Sergey Levine |  |
| 573 |  |  [GRAND++: Graph Neural Diffusion with A Source Term](https://openreview.net/forum?id=EMxu-dzvJk) |  | 0 | We propose GRAph Neural Diffusion with a source term (GRAND++) for graph deep learning with a limited number of labeled nodes, i.e., low-labeling rate. GRAND++ is a class of continuous-depth graph deep learning architectures whose theoretical underpinning is the diffusion process on graphs with a source term. The source term guarantees two interesting theoretical... | Andrea L. Bertozzi, Bao Wang, Hedi Xia, Matthew Thorpe, Stanley J. Osher, Tan Minh Nguyen, Thomas Strohmer |  |
| 574 |  |  [Case-based reasoning for better generalization in textual reinforcement learning](https://openreview.net/forum?id=ZDaSIkWT-AP) |  | 0 | Text-based games (TBG) have emerged as promising environments for driving research in grounded language understanding and studying problems like generalization and sample efficiency. Several deep reinforcement learning (RL) methods with varying architectures and learning schemes have been proposed for TBGs. However, these methods fail to generalize efficiently,... | Keerthiram Murugesan, Mattia Atzeni, Mrinmaya Sachan, Shehzaad Zuzar Dhuliawala |  |
| 575 |  |  [Neural Deep Equilibrium Solvers](https://openreview.net/forum?id=B0oHOwT5ENL) |  | 0 | A deep equilibrium (DEQ) model abandons traditional depth by solving for the fixed point of a single nonlinear layer $f_\theta$. This structure enables decoupling the internal structure of the layer (which controls representational capacity) from how the fixed point is actually computed (which impacts inference-time efficiency), which is usually via classic techniques... | J. Zico Kolter, Shaojie Bai, Vladlen Koltun |  |
| 576 |  |  [A Theoretical Analysis on Feature Learning in Neural Networks: Emergence from Inputs and Advantage over Fixed Features](https://openreview.net/forum?id=wMpS-Z_AI_E) |  | 0 | An important characteristic of neural networks is their ability to learn representations of the input data with effective features for prediction, which is believed to be a key factor to their superior empirical performance. To better understand the source and benefit of feature learning in neural networks, we consider learning problems motivated by practical data,... | Junyi Wei, Yingyu Liang, Zhenmei Shi |  |
| 577 |  |  [CADDA: Class-wise Automatic Differentiable Data Augmentation for EEG Signals](https://openreview.net/forum?id=6IYp-35L-xJ) |  | 0 | Data augmentation is a key element of deep learning pipelines, as it informs the network during training about transformations of the input data that keep the label unchanged. Manually finding adequate augmentation methods and parameters for a given pipeline is however rapidly cumbersome. In particular, while intuition can guide this decision for images, the design and... | Alexandre Gramfort, Cédric Rommel, Joseph Paillard, Thomas Moreau |  |
| 578 |  |  [Label Leakage and Protection in Two-party Split Learning](https://openreview.net/forum?id=cOtBRgsf2fO) |  | 0 | Two-party split learning is a popular technique for learning a model across feature-partitioned data. In this work, we explore whether it is possible for one party to steal the private label information from the other party during split training, and whether there are methods that can protect against such attacks. Specifically, we first formulate a realistic threat... | Chong Wang, Hongyi Zhang, Jiankai Sun, Junyuan Xie, Oscar Li, Virginia Smith, Weihao Gao, Xin Yang |  |
| 579 |  |  [Semi-relaxed Gromov-Wasserstein divergence and applications on graphs](https://openreview.net/forum?id=RShaMexjc-x) |  | 0 | Comparing structured objects such as graphs is a fundamental operation involved in many learning tasks. To this end, the Gromov-Wasserstein (GW) distance, based on Optimal Transport (OT), has proven to be successful in handling the specific nature of the associated objects. More specifically, through the nodes connectivity relations, GW operates on graphs, seen as... | Cédric VincentCuaz, Marco Corneli, Nicolas Courty, Rémi Flamary, Titouan Vayer |  |
| 580 |  |  [CodeTrek: Flexible Modeling of Code using an Extensible Relational Representation](https://openreview.net/forum?id=WQc075jmBmf) |  | 0 | Designing a suitable representation for code-reasoning tasks is challenging in aspects such as the kinds of program information to model, how to combine them, and how much context to consider. We propose CodeTrek, a deep learning approach that addresses these challenges by representing codebases as databases that conform to rich relational schemas. The relational... | Aaditya Naik, Hanjun Dai, Mayur Naik, Pardis Pashakhanloo, Petros Maniatis, Yuepeng Wang |  |
| 581 |  |  [Bridging Recommendation and Marketing via Recurrent Intensity Modeling](https://openreview.net/forum?id=TZeArecH2Nf) |  | 0 | This paper studies some under-explored connections between personalized recommendation and marketing systems. Obviously, these two systems are different, in two main ways. Firstly, personalized item-recommendation (ItemRec) is user-centric, whereas marketing recommends the best user-state segments (UserRec) on behalf of its item providers. (We treat different temporal... | Anoop Deoras, Ge Liu, Yifei Ma |  |
| 582 |  |  [Sparse Attention with Learning to Hash](https://openreview.net/forum?id=VGnOJhd5Q1q) |  | 0 | Transformer has become ubiquitous in sequence modeling tasks. As a key component of Transformer, self-attention does not scale to long sequences due to its quadratic time and space complexity with respect to the sequence length. To tackle this problem, recent work developed dynamic attention sparsification techniques based on Approximate Nearest Neighbor (ANN) methods,... | Shinjae Yoo, Yiming Yang, Zhiqing Sun |  |
| 583 |  |  [Controlling the Complexity and Lipschitz Constant improves Polynomial Nets](https://openreview.net/forum?id=dQ7Cy_ndl1s) |  | 0 | While the class of Polynomial Nets demonstrates comparable performance to neural networks (NN), it currently has neither theoretical generalization characterization nor robustness guarantees. To this end, we derive new complexity bounds for the set of Coupled CP-Decomposition (CCP) and Nested Coupled CP-decomposition (NCP) models of Polynomial Nets in terms of the... | Fabian Latorre, Grigorios Chrysos, Volkan Cevher, Zhenyu Zhu |  |
| 584 |  |  [Finding an Unsupervised Image Segmenter in each of your Deep Generative Models](https://openreview.net/forum?id=Ug-bgjgSlKV) |  | 0 | Recent research has shown that numerous human-interpretable directions exist in the latent space of GANs. In this paper, we develop an automatic procedure for finding directions that lead to foreground-background image separation, and we use these directions to train an image segmentation model without human supervision. Our method is generator-agnostic, producing... | Andrea Vedaldi, Christian Rupprecht, Iro Laina, Luke MelasKyriazi |  |
| 585 |  |  [Solving Inverse Problems in Medical Imaging with Score-Based Generative Models](https://openreview.net/forum?id=vaRCHVj0uGI) |  | 0 | Reconstructing medical images from partial measurements is an important inverse problem in Computed Tomography (CT) and Magnetic Resonance Imaging (MRI). Existing solutions based on machine learning typically train a model to directly map measurements to medical images, leveraging a training dataset of paired images and measurements. These measurements are typically... | Lei Xing, Liyue Shen, Stefano Ermon, Yang Song |  |
| 586 |  |  [BDDM: Bilateral Denoising Diffusion Models for Fast and High-Quality Speech Synthesis](https://openreview.net/forum?id=L7wzpQttNO) |  | 0 | Diffusion probabilistic models (DPMs) and their extensions have emerged as competitive generative models yet confront challenges of efficient sampling. We propose a new bilateral denoising diffusion model (BDDM) that parameterizes both the forward and reverse processes with a schedule network and a score network, which can train with a novel bilateral modeling... | Dan Su, Dong Yu, Jun Wang, Max W. Y. Lam |  |
| 587 |  |  [Sample Efficient Stochastic Policy Extragradient Algorithm for Zero-Sum Markov Game](https://openreview.net/forum?id=IvepFxYRDG) |  | 0 | Two-player zero-sum Markov game is a fundamental problem in reinforcement learning and game theory. Although many algorithms have been proposed for solving zero-sum Markov games in the existing literature, many of them either require a full knowledge of the environment or are not sample-efficient. In this paper, we develop a fully decentralized and sample-efficient... | Shaocong Ma, Yi Zhou, Ziyi Chen |  |
| 588 |  |  [The Uncanny Similarity of Recurrence and Depth](https://openreview.net/forum?id=3wNcr5nq56) |  | 0 | It is widely believed that deep neural networks contain layer specialization, wherein networks extract hierarchical features representing edges and patterns in shallow layers and complete objects in deeper layers. Unlike common feed-forward models that have distinct filters at each layer, recurrent networks reuse the same parameters at various depths. In this work, we... | Amin Ghiasi, Arjun Gupta, Avi Schwarzschild, Micah Goldblum, Tom Goldstein |  |
| 589 |  |  [Implicit Bias of Adversarial Training for Deep Neural Networks](https://openreview.net/forum?id=l8It-0lE5e7) |  | 0 | We provide theoretical understandings of the implicit bias imposed by adversarial training for homogeneous deep neural networks without any explicit regularization. In particular, for deep linear networks adversarially trained by gradient descent on a linearly separable dataset, we prove that the direction of the product of weight matrices converges to the direction of... | Bochen Lv, Zhanxing Zhu |  |
| 590 |  |  [Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning](https://openreview.net/forum?id=_SJ-_yyes8) |  | 0 | We present DrQ-v2, a model-free reinforcement learning (RL) algorithm for visual continuous control. DrQ-v2 builds on DrQ, an off-policy actor-critic approach that uses data augmentation to learn directly from pixels. We introduce several improvements that yield state-of-the-art results on the DeepMind Control Suite. Notably, DrQ-v2 is able to solve complex humanoid... | Alessandro Lazaric, Denis Yarats, Lerrel Pinto, Rob Fergus |  |
| 591 |  |  [$\pi$BO: Augmenting Acquisition Functions with User Beliefs for Bayesian Optimization](https://openreview.net/forum?id=MMAeCXIa89) |  | 0 | Bayesian optimization (BO) has become an established framework and popular tool for hyperparameter optimization (HPO) of machine learning (ML) algorithms. While known for its sample-efficiency, vanilla BO can not utilize readily available prior beliefs the practitioner has on the potential location of the optimum. Thus, BO disregards a valuable source of information,... | Artur L. F. Souza, Carl Hvarfner, Danny Stoll, Frank Hutter, Luigi Nardi, Marius Lindauer |  |
| 592 |  |  [A Generalized Weighted Optimization Method for Computational Learning and Inversion](https://openreview.net/forum?id=14F3fI6MGxX) |  | 0 | The generalization capacity of various machine learning models exhibits different phenomena in the under- and over-parameterized regimes. In this paper, we focus on regression models such as feature regression and kernel regression and analyze a generalized weighted least-squares optimization method for computational learning and inversion with noisy data. The highlight... | Björn Engquist, Kui Ren, Yunan Yang |  |
| 593 |  |  [DriPP: Driven Point Processes to Model Stimuli Induced Patterns in M/EEG Signals](https://openreview.net/forum?id=d_2lcDh0Y9c) |  | 0 | The quantitative analysis of non-invasive electrophysiology signals from electroencephalography (EEG) and magnetoencephalography (MEG) boils down to the identification of temporal patterns such as evoked responses, transient bursts of neural oscillations but also blinks or heartbeats for data cleaning. Several works have shown that these patterns can be extracted... | Alexandre Gramfort, Cédric Allain, Thomas Moreau |  |
| 594 |  |  [Stiffness-aware neural network for learning Hamiltonian systems](https://openreview.net/forum?id=uVXEKeqJbNa) |  | 0 | We propose stiffness-aware neural network (SANN), a new method for learning Hamiltonian dynamical systems from data. SANN identifies and splits the training data into stiff and nonstiff portions based on a stiffness-aware index, a simple, yet effective metric we introduce to quantify the stiffness of the dynamical system. This classification along with a resampling... | Hong Zhang, Senwei Liang, Zhongzhan Huang |  |
| 595 |  |  [CoST: Contrastive Learning of Disentangled Seasonal-Trend Representations for Time Series Forecasting](https://openreview.net/forum?id=PilZY3omXV2) |  | 0 | Deep learning has been actively studied for time series forecasting, and the mainstream paradigm is based on the end-to-end training of neural network architectures, ranging from classical LSTM/RNNs to more recent TCNs and Transformers. Motivated by the recent success of representation learning in computer vision and natural language processing, we argue that a more... | Akshat Kumar, Chenghao Liu, Doyen Sahoo, Gerald Woo, Steven C. H. Hoi |  |
| 596 |  |  [CoordX: Accelerating Implicit Neural Representation with a Split MLP Architecture](https://openreview.net/forum?id=oAy7yPmdNz) |  | 0 | Implicit neural representations with multi-layer perceptrons (MLPs) have recently gained prominence for a wide variety of tasks such as novel view synthesis and 3D object representation and rendering. However, a significant challenge with these representations is that both training and inference with an MLP over a large number of input coordinates to learn and represent... | Hongyi Sun, Nandita Vijaykumar, Ruofan Liang |  |
| 597 |  |  [Plant 'n' Seek: Can You Find the Winning Ticket?](https://openreview.net/forum?id=9n9c8sf0xm) |  | 0 | The lottery ticket hypothesis has sparked the rapid development of pruning algorithms that aim to reduce the computational costs associated with deep learning during training and model deployment. Currently, such algorithms are primarily evaluated on imaging data, for which we lack ground truth information and thus the understanding of how sparse lottery tickets could... | Jonas Fischer, Rebekka Burkholz |  |
| 598 |  |  [Coherence-based Label Propagation over Time Series for Accelerated Active Learning](https://openreview.net/forum?id=gjNcH0hj0LM) |  | 0 | Time-series data are ubiquitous these days, but lack of the labels in time-series data is regarded as a hurdle for its broad applicability. Meanwhile, active learning has been successfully adopted to reduce the labeling efforts in various tasks. Thus, this paper addresses an important issue, time-series active learning. Inspired by the temporal coherence in time-series... | Byung Suk Lee, Hwanjun Song, JaeGil Lee, Sundong Kim, Susik Yoon, Yooju Shin |  |
| 599 |  |  [A Class of Short-term Recurrence Anderson Mixing Methods and Their Applications](https://openreview.net/forum?id=_X90SIKbHa) |  | 0 | Anderson mixing (AM) is a powerful acceleration method for fixed-point iterations, but its computation requires storing many historical iterations. The extra memory footprint can be prohibitive when solving high-dimensional problems in a resource-limited machine. To reduce the memory overhead, we propose a novel class of short-term recurrence AM methods (ST-AM). The... | Chenglong Bao, Fuchao Wei, Yang Liu |  |
| 600 |  |  [The Geometry of Memoryless Stochastic Policy Optimization in Infinite-Horizon POMDPs](https://openreview.net/forum?id=A05I5IvrdL-) |  | 0 | We consider the problem of finding the best memoryless stochastic policy for an infinite-horizon partially observable Markov decision process (POMDP) with finite state and action spaces with respect to either the discounted or mean reward criterion. We show that the (discounted) state-action frequencies and the expected cumulative reward are rational functions of the... | Guido Montúfar, Johannes Müller |  |
| 601 |  |  [Efficient Sharpness-aware Minimization for Improved Training of Neural Networks](https://openreview.net/forum?id=n0OeTdNRG0Q) |  | 0 | Overparametrized Deep Neural Networks (DNNs) often achieve astounding performances, but may potentially result in severe generalization error. Recently, the relation between the sharpness of the loss landscape and the generalization error has been established by Foret et al. (2020), in which the Sharpness Aware Minimizer (SAM) was proposed to mitigate the degradation of... | Hanshu Yan, Jiashi Feng, Jiawei Du, Joey Tianyi Zhou, Liangli Zhen, Rick Siow Mong Goh, Vincent Y. F. Tan |  |
| 602 |  |  [Lipschitz-constrained Unsupervised Skill Discovery](https://openreview.net/forum?id=BGvt0ghNgA) |  | 0 | We study the problem of unsupervised skill discovery, whose goal is to learn a set of diverse and useful skills with no external reward. There have been a number of skill discovery methods based on maximizing the mutual information (MI) between skills and states. However, we point out that their MI objectives usually prefer static skills to dynamic ones, which may... | Gunhee Kim, Honglak Lee, Jaekyeom Kim, Jongwook Choi, Seohong Park |  |
| 603 |  |  [Learning Generalizable Representations for Reinforcement Learning via Adaptive Meta-learner of Behavioral Similarities](https://openreview.net/forum?id=zBOI9LFpESK) |  | 0 | How to learn an effective reinforcement learning-based model for control tasks from high-level visual observations is a practical and challenging problem. A key to solving this problem is to learn low-dimensional state representations from observations, from which an effective policy can be learned. In order to boost the learning of state encoding, recent works are... | Jianda Chen, Sinno Jialin Pan |  |
| 604 |  |  [Effective Model Sparsification by Scheduled Grow-and-Prune Methods](https://openreview.net/forum?id=xa6otUDdP2W) |  | 0 | Deep neural networks (DNNs) are effective in solving many real-world problems. Larger DNN models usually exhibit better quality (e.g., accuracy) but their excessive computation results in long inference time. Model sparsification can reduce the computation and memory cost while maintaining model quality. Most existing sparsification algorithms unidirectionally remove... | Fei Sun, Kun Yuan, Minghai Qin, Rong Jin, Xiaolong Ma, Yanzhi Wang, YenKuang Chen, Yi Xu, Yuan Xie, Zejiang Hou |  |
| 605 |  |  [FILIP: Fine-grained Interactive Language-Image Pre-Training](https://openreview.net/forum?id=cpDhcsEDC2) |  | 0 | Unsupervised large-scale vision-language pre-training has shown promising advances on various downstream tasks. Existing methods often model the cross-modal interaction either via the similarity of the global feature of each modality which misses sufficient information, or finer-grained interactions using cross/self-attention upon visual and textual tokens. However,... | Chunjing Xu, Guansong Lu, Hang Xu, Lewei Yao, Lu Hou, Minzhe Niu, Runhui Huang, Xiaodan Liang, Xin Jiang, Zhenguo Li |  |
| 606 |  |  [Information Prioritization through Empowerment in Visual Model-based RL](https://openreview.net/forum?id=DfUjyyRW90) |  | 0 | Model-based reinforcement learning (RL) algorithms designed for handling complex visual observations typically learn some sort of latent state representation, either explicitly or implicitly. Standard methods of this sort do not distinguish between functionally relevant aspects of the state and irrelevant distractors, instead aiming to represent all available... | Dumitru Erhan, Homanga Bharadhwaj, Mohammad Babaeizadeh, Sergey Levine |  |
| 607 |  |  [Efficient Active Search for Combinatorial Optimization Problems](https://openreview.net/forum?id=nO5caZwFwYu) |  | 0 | Recently numerous machine learning based methods for combinatorial optimization problems have been proposed that learn to construct solutions in a sequential decision process via reinforcement learning. While these methods can be easily combined with search strategies like sampling and beam search, it is not straightforward to integrate them into a high-level search... | André Hottung, Kevin Tierney, YeongDae Kwon |  |
| 608 |  |  [Ancestral protein sequence reconstruction using a tree-structured Ornstein-Uhlenbeck variational autoencoder](https://openreview.net/forum?id=FZoZ7a31GCW) |  | 0 | We introduce a deep generative model for representation learning of biological sequences that, unlike existing models, explicitly represents the evolutionary process. The model makes use of a tree-structured Ornstein-Uhlenbeck process, obtained from a given phylogenetic tree, as an informative prior for a variational autoencoder. We show the model performs well on the... | Ahmad Salim AlSibahi, Douglas L. Theobald, Jotun Hein, Lys Sanz Moreta, Ola Rønning, Thomas Hamelryck |  |
| 609 |  |  [Training Structured Neural Networks Through Manifold Identification and Variance Reduction](https://openreview.net/forum?id=mdUYT5QV0O) |  | 0 | This paper proposes an algorithm, RMDA, for training neural networks (NNs) with a regularization term for promoting desired structures. RMDA does not incur computation additional to proximal SGD with momentum, and achieves variance reduction without requiring the objective function to be of the finite-sum form. Through the tool of manifold identification from nonlinear... | Chingpei Lee, ZihSyuan Huang |  |
| 610 |  |  [The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization](https://openreview.net/forum?id=KBQP4A_J1K) |  | 0 | Despite progress across a broad range of applications, Transformers have limited success in systematic generalization. The situation is especially frustrating in the case of algorithmic tasks, where they often fail to find intuitive solutions that route relevant information to the right node/operation at the right time in the grid represented by Transformer columns. To... | Jürgen Schmidhuber, Kazuki Irie, Róbert Csordás |  |
| 611 |  |  [On the Limitations of Multimodal VAEs](https://openreview.net/forum?id=w-CPUXXrAj) |  | 0 | Multimodal variational autoencoders (VAEs) have shown promise as efficient generative models for weakly-supervised data. Yet, despite their advantage of weak supervision, they exhibit a gap in generative quality compared to unimodal VAEs, which are completely unsupervised. In an attempt to explain this gap, we uncover a fundamental limitation that applies to a large... | Emanuele Palumbo, Imant Daunhawer, Julia E. Vogt, Kieran ChinCheong, Thomas M. Sutter |  |
| 612 |  |  [Recursive Disentanglement Network](https://openreview.net/forum?id=CSfcOznpDY) |  | 0 | Disentangled feature representation is essential for data-efficient learning. The feature space of deep models is inherently compositional. Existing $\beta$-VAE-based methods, which only apply disentanglement regularization to the resulting embedding space of deep models, cannot effectively regularize such compositional feature space, resulting in unsatisfactory... | Dongsheng Li, Fan Yang, Li Shang, Mingzhi Dong, Qin Lv, Robert P. Dick, Yingying Zhao, Yixuan Chen, Yubin Shi, Yujiang Wang |  |
| 613 |  |  [ADAVI: Automatic Dual Amortized Variational Inference Applied To Pyramidal Bayesian Models](https://openreview.net/forum?id=CgIEctmcXx1) |  | 0 | Frequently, population studies feature pyramidally-organized data represented using Hierarchical Bayesian Models (HBM) enriched with plates. These models can become prohibitively large in settings such as neuroimaging, where a sample is composed of a functional MRI signal measured on 300 brain locations, across 4 measurement sessions, and 30 subjects, resulting in... | Demian Wassermann, Louis Rouillard |  |
| 614 |  |  [Distributionally Robust Models with Parametric Likelihood Ratios](https://openreview.net/forum?id=a34GrNaYEcS) |  | 0 | As machine learning models are deployed ever more broadly, it becomes increasingly important that they are not only able to perform well on their training distribution, but also yield accurate predictions when confronted with distribution shift. The Distributionally Robust Optimization (DRO) framework proposes to address this issue by training models to minimize their... | Graham Neubig, Paul Michel, Tatsunori Hashimoto |  |
| 615 |  |  [Constrained Physical-Statistics Models for Dynamical System Identification and Prediction](https://openreview.net/forum?id=gbe1zHyA73) |  | 0 | Modeling dynamical systems combining prior physical knowledge and machine learning (ML) is promising in scientific problems when the underlying processes are not fully understood, e.g. when the dynamics is partially known. A common practice to identify the respective parameters of the physical and ML components is to formulate the problem as supervised learning on... | Jérémie Donà, Marie Déchelle, Marina Levy, Patrick Gallinari |  |
| 616 |  |  [Doubly Adaptive Scaled Algorithm for Machine Learning Using Second-Order Information](https://openreview.net/forum?id=HCelXXcSEuH) |  | 0 | We present a novel adaptive optimization algorithm for large-scale machine learning problems. Equipped with a low-cost estimate of local curvature and Lipschitz smoothness, our method dynamically adapts the search direction and step-size. The search direction contains gradient information preconditioned by a well-scaled diagonal preconditioning matrix that captures the... | Majid Jahani, Martin Takác, Michael W. Mahoney, Peter Richtárik, Sergey Rusakov, Zheng Shi |  |
| 617 |  |  [Understanding approximate and unrolled dictionary learning for pattern recovery](https://openreview.net/forum?id=rI0LYgGeYaw) |  | 0 | Dictionary learning consists of finding a sparse representation from noisy data and is a common way to encode data-driven prior knowledge on signals. Alternating minimization (AM) is standard for the underlying optimization, where gradient descent steps alternate with sparse coding procedures. The major drawback of this method is its prohibitive computational cost,... | Benoît Malézieux, Matthieu Kowalski, Thomas Moreau |  |
| 618 |  |  [Constraining Linear-chain CRFs to Regular Languages](https://openreview.net/forum?id=jbrgwbv8nD) |  | 0 | A major challenge in structured prediction is to represent the interdependencies within output structures. When outputs are structured as sequences, linear-chain conditional random fields (CRFs) are a widely used model class which can learn local dependencies in the output. However, the CRF's Markov assumption makes it impossible for CRFs to represent distributions with... | Roman Klinger, Sean Papay, Sebastian Padó |  |
| 619 |  |  [Dive Deeper Into Integral Pose Regression](https://openreview.net/forum?id=vHVcB-ak3Si) |  | 0 | Integral pose regression combines an implicit heatmap with end-to-end training for human body and hand pose estimation. Unlike detection-based heatmap methods, which decode final joint positions from the heatmap with a non-differentiable argmax operation, integral regression methods apply a differentiable expectation operation. This paper offers a deep dive into the... | Angela Yao, Kerui Gu, Linlin Yang |  |
| 620 |  |  [Evidential Turing Processes](https://openreview.net/forum?id=84NMXTHYe-) |  | 0 | A probabilistic classifier with reliable predictive uncertainties i) fits successfully to the target domain data, ii) provides calibrated class probabilities in difficult regions of the target domain (e.g. class overlap), and iii) accurately identifies queries coming out of the target domain and reject them. We introduce an original combination of Evidential Deep... | Abdullah Akgül, Gozde Unal, Manuel Haußmann, Melih Kandemir |  |
| 621 |  |  [Noisy Feature Mixup](https://openreview.net/forum?id=vJb4I2ANmy) |  | 0 | We introduce Noisy Feature Mixup (NFM), an inexpensive yet effective method for data augmentation that combines the best of interpolation based training and noise injection schemes. Rather than training with convex combinations of pairs of examples and their labels, we use noise-perturbed convex combinations of pairs of data points in both input and feature space. This... | Francisco Utrera, Michael W. Mahoney, N. Benjamin Erichson, Soon Hoe Lim, Winnie Xu |  |
| 622 |  |  [Peek-a-Boo: What (More) is Disguised in a Randomly Weighted Neural Network, and How to Find It Efficiently](https://openreview.net/forum?id=moHCzz6D5H3) |  | 0 | Sparse neural networks (NNs) are intensively investigated in literature due to their appeal in saving storage, memory, and computational costs. A recent work (Ramanujan et al., 2020) showed that, different from conventional pruning-and-finetuning pipeline, there exist hidden subnetworks in randomly initialized NNs that have good performance without training the weights.... | Jason Zhang, Xiaohan Chen, Zhangyang Wang |  |
| 623 |  |  [How Well Does Self-Supervised Pre-Training Perform with Streaming Data?](https://openreview.net/forum?id=EwqEx5ipbOu) |  | 0 | Prior works on self-supervised pre-training focus on the joint training scenario, where massive unlabeled data are assumed to be given as input all at once, and only then is a learner trained. Unfortunately, such a problem setting is often impractical if not infeasible since many real-world tasks rely on sequential learning, e.g., data are decentralized or collected in... | Dapeng Hu, Hailin Hu, Jiashi Feng, Lanqing Hong, Qizhengqiu Lu, Shipeng Yan, Xinchao Wang, Yifan Zhang, Zhenguo Li |  |
| 624 |  |  [Subspace Regularizers for Few-Shot Class Incremental Learning](https://openreview.net/forum?id=boJy41J-tnQ) |  | 0 | Few-shot class incremental learning---the problem of updating a trained classifier to discriminate among an expanded set of classes with limited labeled data---is a key challenge for machine learning systems deployed in non-stationary environments. Existing approaches to the problem rely on complex model architectures and training procedures that are difficult to tune... | Afra Feyza Akyürek, Derry Wijaya, Ekin Akyürek, Jacob Andreas |  |
| 625 |  |  [Using Graph Representation Learning with Schema Encoders to Measure the Severity of Depressive Symptoms](https://openreview.net/forum?id=OtEDS2NWhqa) |  | 0 | Graph neural networks (GNNs) are widely used in regression and classification problems applied to text, in areas such as sentiment analysis and medical decision-making processes. We propose a novel form for node attributes within a GNN based model that captures node-specific embeddings for every word in the vocabulary. This provides a global representation at each node,... | Anthony G. Cohn, David Crossland Hogg, Simin Hong |  |
| 626 |  |  [Actor-Critic Policy Optimization in a Large-Scale Imperfect-Information Game](https://openreview.net/forum?id=DTXZqTNV5nW) |  | 0 | The deep policy gradient method has demonstrated promising results in many large-scale games, where the agent learns purely from its own experience. Yet, policy gradient methods with self-play suffer convergence problems to a Nash Equilibrium (NE) in multi-agent situations. Counterfactual regret minimization (CFR) has a convergence guarantee to a NE in 2-player zero-sum... | Bin Li, Bo Ma, Haobo Fu, Junliang Xing, Kai Li, Qiang Fu, Shuang Wu, Tao Yang, Wei Yang, Weiming Liu, Yijia Wang |  |
| 627 |  |  [Policy Gradients Incorporating the Future](https://openreview.net/forum?id=EHaUTlm2eHg) |  | 0 | Reasoning about the future -- understanding how decisions in the present time affect outcomes in the future -- is one of the central challenges for reinforcement learning (RL), especially in highly-stochastic or partially observable environments. While predicting the future directly is hard, in this work we introduce a method that allows an agent to \`\`look into the... | David Venuto, Doina Precup, Elaine Lau, Ofir Nachum |  |
| 628 |  |  [Gradient Information Matters in Policy Optimization by Back-propagating through Model](https://openreview.net/forum?id=rzvOQrnclO0) |  | 0 | Model-based reinforcement learning provides an efficient mechanism to find the optimal policy by interacting with the learned environment. In addition to treating the learned environment like a black-box simulator, a more effective way to use the model is to exploit its differentiability. Such methods require the gradient information of the learned environment model... | Chongchong Li, TieYan Liu, Wei Chen, Yue Wang, Yuting Liu, ZhiMing Ma |  |
| 629 |  |  [VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning](https://openreview.net/forum?id=xm6YD62D1Ub) |  | 0 | Recent self-supervised methods for image representation learning maximize the agreement between embedding vectors produced by encoders fed with different views of the same image. The main challenge is to prevent a collapse in which the encoders produce constant or non-informative vectors. We introduce VICReg (Variance-Invariance-Covariance Regularization), a method that... | Adrien Bardes, Jean Ponce, Yann LeCun |  |
| 630 |  |  [High Probability Generalization Bounds with Fast Rates for Minimax Problems](https://openreview.net/forum?id=gI7feJ9yXPz) |  | 0 | Minimax problems are receiving an increasing amount of attention in a wide range of applications in machine learning (ML), for instance, reinforcement learning, robust optimization, adversarial learning, and distributed computing, to mention but a few. Current studies focus on the fundamental understanding of general minimax problems with an emphasis on convergence... | Shaojie Li, Yong Liu |  |
| 631 |  |  [SUMNAS: Supernet with Unbiased Meta-Features for Neural Architecture Search](https://openreview.net/forum?id=Z8FzvVU6_Kj) |  | 0 | One-shot Neural Architecture Search (NAS) usually constructs an over-parameterized network, which we call a supernet, and typically adopts sharing parameters among the sub-models to improve computational efficiency. One-shot NAS often repeatedly samples sub-models from the supernet and trains them to optimize the shared parameters. However, this training strategy... | ByungGon Chun, Hyeonmin Ha, JiHoon Kim, Semin Park |  |
| 632 |  |  [Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting](https://openreview.net/forum?id=_XNtisL32jv) |  | 0 | Recently, brain-inspired spiking neuron networks (SNNs) have attracted widespread research interest because of their event-driven and energy-efficient characteristics. It is difficult to efficiently train deep SNNs due to the non-differentiability of its activation function, which disables the typically used gradient descent approaches for traditional artificial neural... | Shanghang Zhang, Shi Gu, Shikuang Deng, Yuhang Li |  |
| 633 |  |  [Reliable Adversarial Distillation with Unreliable Teachers](https://openreview.net/forum?id=u6TRGdzhfip) |  | 0 | In ordinary distillation, student networks are trained with soft labels (SLs) given by pretrained teacher networks, and students are expected to improve upon teachers since SLs are stronger supervision than the original hard labels. However, when considering adversarial robustness, teachers may become unreliable and adversarial distillation may not work: teachers are... | Bo Han, Gang Niu, Hongxia Yang, Jiangchao Yao, Jianing Zhu, Jianliang Xu, Jingfeng Zhang, Jingren Zhou, Tongliang Liu |  |
| 634 |  |  [Neural Program Synthesis with Query](https://openreview.net/forum?id=NyJ2KIN8P17) |  | 0 | Aiming to find a program satisfying the user intent given input-output examples, program synthesis has attracted increasing interest in the area of machine learning. Despite the promising performance of existing methods, most of their success comes from the privileged information of well-designed input-output examples. However, providing such input-output examples is... | Di Huang, Nan Li, Pengwei Jin, Qi Guo, Rui Zhang, Xing Hu, Xishan Zhang, Yunji Chen, Zidong Du |  |
| 635 |  |  [Delaunay Component Analysis for Evaluation of Data Representations](https://openreview.net/forum?id=HTVch9AMPa) |  | 0 | Advanced representation learning techniques require reliable and general evaluation methods. Recently, several algorithms based on the common idea of geometric and topological analysis of a manifold approximated from the learned data representations have been proposed. In this work, we introduce Delaunay Component Analysis (DCA) -- an evaluation algorithm which... | Anastasiia Varava, Danica Kragic, Florian T. Pokorny, Petra Poklukar, Vladislav Polianskii |  |
| 636 |  |  [Visual hyperacuity with moving sensor and recurrent neural computations](https://openreview.net/forum?id=p0rCmDEN_-) |  | 0 | Dynamical phenomena, such as recurrent neuronal activity and perpetual motion of the eye, are typically overlooked in models of bottom-up visual perception. Recent experiments suggest that tiny inter-saccadic eye motion ("fixational drift") enhances visual acuity beyond the limit imposed by the density of retinal photoreceptors. Here we hypothesize that such an... | Alexander Rivkind, Ehud Ahissar, Eldad Assa, Michael Kreiserman, Or Ram |  |
| 637 |  |  [Partial Wasserstein Adversarial Network for Non-rigid Point Set Registration](https://openreview.net/forum?id=2ggNjUisGyr) |  | 0 | Given two point sets, the problem of registration is to recover a transformation that matches one set to the other. This task is challenging due to the presence of large number of outliers, the unknown non-rigid deformations and the large sizes of point sets. To obtain strong robustness against outliers, we formulate the registration problem as a partial distribution... | GuiSong Xia, Ling Lei, Nan Xue, Ziming Wang |  |
| 638 |  |  [Quantitative Performance Assessment of CNN Units via Topological Entropy Calculation](https://openreview.net/forum?id=xFOyMwWPkz) |  | 0 | Identifying the status of individual network units is critical for understanding the mechanism of convolutional neural networks (CNNs). However, it is still challenging to reliably give a general indication of unit status, especially for units in different network models. To this end, we propose a novel method for quantitatively clarifying the status of single unit in... | Hao Zhang, Yang Zhao |  |
| 639 |  |  [Imitation Learning by Reinforcement Learning](https://openreview.net/forum?id=1zwleytEpYx) |  | 0 | Imitation learning algorithms learn a policy from demonstrations of expert behavior. We show that, for deterministic experts, imitation learning can be done by reduction to reinforcement learning with a stationary reward. Our theoretical analysis both certifies the recovery of expert reward and bounds the total variation distance between the expert and the imitation... | Kamil Ciosek |  |
| 640 |  |  [On-Policy Model Errors in Reinforcement Learning](https://openreview.net/forum?id=81e1aeOt-sd) |  | 0 | Model-free reinforcement learning algorithms can compute policy gradients given sampled environment transitions, but require large amounts of data. In contrast, model-based methods can use the learned model to generate new data, but model errors and bias can render learning unstable or suboptimal. In this paper, we present a novel method that combines real-world data... | Felix Berkenkamp, Lukas P. Fröhlich, Maksym Lefarov, Melanie N. Zeilinger |  |
| 641 |  |  [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://openreview.net/forum?id=O50443AsCP) |  | 0 | Recent progress in language model pre-training has achieved a great success via leveraging large-scale unstructured textual data. However, it is still a challenge to apply pre-training on structured tabular data due to the absence of large-scale high-quality tabular data. In this paper, we propose TAPEX to show that table pre-training can be achieved by learning a... | Bei Chen, JianGuang Lou, Jiaqi Guo, Morteza Ziyadi, Qian Liu, Weizhu Chen, Zeqi Lin |  |
| 642 |  |  [DARA: Dynamics-Aware Reward Augmentation in Offline Reinforcement Learning](https://openreview.net/forum?id=9SDQB3b68K) |  | 0 | Offline reinforcement learning algorithms promise to be applicable in settings where a fixed dataset is available and no new experience can be acquired. However, such formulation is inevitably offline-data-hungry and, in practice, collecting a large offline dataset for one specific task over one specific environment is also costly and laborious. In this paper, we thus... | Donglin Wang, Hongyin Zhang, Jinxin Liu |  |
| 643 |  |  [Explaining Point Processes by Learning Interpretable Temporal Logic Rules](https://openreview.net/forum?id=P07dq7iSAGr) |  | 0 | We propose a principled method to learn a set of human-readable logic rules to explain temporal point processes. We assume that the generative mechanisms underlying the temporal point processes are governed by a set of first-order temporal logic rules, as a compact representation of domain knowledge. Our method formulates the rule discovery process from noisy event data... | Abdelmajid Essofi, Junchi Yan, Le Song, Lu Wang, Mingquan Feng, Shuang Li, Yufeng Cao |  |
| 644 |  |  [On Robust Prefix-Tuning for Text Classification](https://openreview.net/forum?id=eBCmOocUejf) |  | 0 | Recently, prefix-tuning has gained increasing attention as a parameter-efficient finetuning method for large-scale pretrained language models. The method keeps the pretrained models fixed and only updates the prefix token parameters for each downstream task. Despite being lightweight and modular, prefix-tuning still lacks robustness to textual adversarial attacks.... | Yang Liu, Zonghan Yang |  |
| 645 |  |  [Learning Graphon Mean Field Games and Approximate Nash Equilibria](https://openreview.net/forum?id=0sgntlpKDOz) |  | 0 | Recent advances at the intersection of dense large graph limits and mean field games have begun to enable the scalable analysis of a broad class of dynamical sequential games with large numbers of agents. So far, results have been largely limited to graphon mean field systems with continuous-time diffusive or jump dynamics, typically without control and with little... | Heinz Koeppl, Kai Cui |  |
| 646 |  |  [Measuring CLEVRness: Black-box Testing of Visual Reasoning Models](https://openreview.net/forum?id=UtGtoS4CYU) |  | 0 | How can we measure the reasoning capabilities of intelligence systems? Visual question answering provides a convenient framework for testing the model's abilities by interrogating the model through questions about the scene. However, despite scores of various visual QA datasets and architectures, which sometimes yield even a super-human performance, the question of... | Henryk Michalewski, Mateusz Malinowski, Spyridon Mouselinos |  |
| 647 |  |  [Exploiting Class Activation Value for Partial-Label Learning](https://openreview.net/forum?id=qqdXHUGec9h) |  | 0 | Partial-label learning (PLL) solves the multi-class classification problem, where each training instance is assigned a set of candidate labels that include the true label. Recent advances showed that PLL can be compatible with deep neural networks, which achieved state-of-the-art performance. However, most of the existing deep PLL methods focus on designing proper... | Bo Han, Fei Zhang, Gang Niu, Lei Feng, Masashi Sugiyama, Tao Qin, Tongliang Liu |  |
| 648 |  |  [Givens Coordinate Descent Methods for Rotation Matrix Learning in Trainable Embedding Indexes](https://openreview.net/forum?id=9-Rfew334N) |  | 0 | Product quantization (PQ) coupled with a space rotation, is widely used in modern approximate nearest neighbor (ANN) search systems to significantly compress the disk storage for embeddings and speed up the inner product computation. Existing rotation learning methods, however, minimize quantization distortion for fixed embeddings, which are not applicable to an... | Bo Long, Han Zhang, WenYun Yang, Yiming Qiu, Yun Xiao, Yunjiang Jiang |  |
| 649 |  |  [cosFormer: Rethinking Softmax In Attention](https://openreview.net/forum?id=Bl8CQrx2Up4) |  | 0 | Transformer has shown great successes in natural language processing, computer vision, and audio processing. As one of its core components, the softmax attention helps to capture long-range dependencies yet prohibits its scale-up due to the quadratic space and time complexity to the sequence length. Kernel methods are often adopted to reduce the complexity by... | Baohong Lv, Dongxu Li, Hui Deng, Junjie Yan, Lingpeng Kong, Weixuan Sun, Yiran Zhong, Yunshen Wei, Zhen Qin |  |
| 650 |  |  [FALCON: Fast Visual Concept Learning by Integrating Images, Linguistic descriptions, and Conceptual Relations](https://openreview.net/forum?id=htWIlvDcY8) |  | 0 | We present a meta-learning framework for learning new visual concepts quickly, from just one or a few examples, guided by multiple naturally occurring data streams: simultaneously looking at images, reading sentences that describe the objects in the scene, and interpreting supplemental sentences that relate the novel concept with other concepts. The learned concepts... | Chuang Gan, Jiayuan Mao, Joshua B. Tenenbaum, Lingjie Mei, Ziqi Wang |  |
| 651 |  |  [HyAR: Addressing Discrete-Continuous Action Reinforcement Learning via Hybrid Action Representation](https://openreview.net/forum?id=64trBbOhdGU) |  | 0 | Discrete-continuous hybrid action space is a natural setting in many practical problems, such as robot control and game AI. However, most previous Reinforcement Learning (RL) works only demonstrate the success in controlling with either discrete or continuous action space, while seldom take into account the hybrid action space. One naive way to address hybrid action RL... | Boyan Li, Hongyao Tang, Jianye Hao, Li Wang, Pengyi Li, Yan Zheng, Zhaopeng Meng, Zhen Wang |  |
| 652 |  |  [Transferable Adversarial Attack based on Integrated Gradients](https://openreview.net/forum?id=DesNW4-5ai9) |  | 0 | The vulnerability of deep neural networks to adversarial examples has drawn tremendous attention from the community. Three approaches, optimizing standard objective functions, exploiting attention maps, and smoothing decision surfaces, are commonly used to craft adversarial examples. By tightly integrating the three approaches, we propose a new and simple algorithm... | Adams WaiKin Kong, Yi Huang |  |
| 653 |  |  [How to deal with missing data in supervised deep learning?](https://openreview.net/forum?id=J7b4BCtDm4) |  | 0 | The issue of missing data in supervised learning has been largely overlooked, especially in the deep learning community. We investigate strategies to adapt neural architectures for handling missing values. Here, we focus on regression and classification problems where the features are assumed to be missing at random. Of particular interest are schemes that allow reusing... | Jes Frellsen, Niels Bruun Ipsen, PierreAlexandre Mattei |  |
| 654 |  |  [Topological Graph Neural Networks](https://openreview.net/forum?id=oxxUMeFwEHd) |  | 0 | Graph neural networks (GNNs) are a powerful architecture for tackling graph learning tasks, yet have been shown to be oblivious to eminent substructures such as cycles. We present TOGL, a novel layer that incorporates global topological information of a graph using persistent homology. TOGL can be easily integrated into any type of GNN and is strictly more expressive... | Bastian Rieck, Edward De Brouwer, Karsten M. Borgwardt, Max Horn, Michael Moor, Yves Moreau |  |
| 655 |  |  [Learning Value Functions from Undirected State-only Experience](https://openreview.net/forum?id=6Pe99Juo9gd) |  | 0 | This paper tackles the problem of learning value functions from undirected state-only experience (state transitions without action labels i.e. (s,s',r) tuples). We first theoretically characterize the applicability of Q-learning in this setting. We show that tabular Q-learning in discrete Markov decision processes (MDPs) learns the same value function under any... | Arjun Gupta, Matthew Chang, Saurabh Gupta |  |
| 656 |  |  [The Boltzmann Policy Distribution: Accounting for Systematic Suboptimality in Human Models](https://openreview.net/forum?id=_l_QjPGN5ye) |  | 0 | Models of human behavior for prediction and collaboration tend to fall into two categories: ones that learn from large amounts of data via imitation learning, and ones that assume human behavior to be noisily-optimal for some reward function. The former are very useful, but only when it is possible to gather a lot of human data in the target environment and... | Anca D. Dragan, Cassidy Laidlaw |  |
| 657 |  |  [WeakM3D: Towards Weakly Supervised Monocular 3D Object Detection](https://openreview.net/forum?id=ahi2XSHpAUZ) |  | 0 | Monocular 3D object detection is one of the most challenging tasks in 3D scene understanding. Due to the ill-posed nature of monocular imagery, existing monocular 3D detection methods highly rely on training with the manually annotated 3D box labels on the LiDAR point clouds. This annotation process is very laborious and expensive. To dispense with the reliance on 3D... | Boxi Wu, Deng Cai, Liang Peng, Senbo Yan, Xiaofei He, Zheng Yang |  |
| 658 |  |  [Exploring Memorization in Adversarial Training](https://openreview.net/forum?id=7gE9V9GBZaI) |  | 0 | Deep learning models have a propensity for fitting the entire training set even with random labels, which requires memorization of every training sample. In this paper, we explore the memorization effect in adversarial training (AT) for promoting a deeper understanding of model capacity, convergence, generalization, and especially robust overfitting of the adversarially... | Hang Su, Jun Zhu, Ke Xu, Tianyu Pang, Xiao Yang, Yinpeng Dong, Zhijie Deng |  |
| 659 |  |  [Disentanglement Analysis with Partial Information Decomposition](https://openreview.net/forum?id=pETy-HVvGtt) |  | 0 | We propose a framework to analyze how multivariate representations disentangle ground-truth generative factors. A quantitative analysis of disentanglement has been based on metrics designed to compare how one variable explains each generative factor. Current metrics, however, may fail to detect entanglement that involves more than two variables, e.g., representations... | Issei Sato, Seiya Tokui |  |
| 660 |  |  [Differentiable Gradient Sampling for Learning Implicit 3D Scene Reconstructions from a Single Image](https://openreview.net/forum?id=U8pbd00cCWB) |  | 0 | Implicit shape models are promising 3D representations for modeling arbitrary locations, with Signed Distance Functions (SDFs) particularly suitable for clear mesh surface reconstruction. Existing approaches for single object reconstruction impose supervision signals based on the loss of the signed distance value from all locations in a scene, posing difficulties when... | Angjoo Kanazawa, Sayna Ebrahimi, Shizhan Zhu, Trevor Darrell |  |
| 661 |  |  [Learning Continuous Environment Fields via Implicit Functions](https://openreview.net/forum?id=3ILxkQ7yElm) |  | 0 | We propose a novel scene representation that encodes reaching distance -- the distance between any position in the scene to a goal along a feasible trajectory. We demonstrate that this environment field representation can directly guide the dynamic behaviors of agents in 2D mazes or 3D indoor scenes. Our environment field is a continuous representation and learned via a... | Jan Kautz, MingHsuan Yang, Shalini De Mello, Sifei Liu, Xiaolong Wang, Xueting Li |  |
| 662 |  |  [Causal Contextual Bandits with Targeted Interventions](https://openreview.net/forum?id=F5Em8ASCosV) |  | 0 | We study a contextual bandit setting where the learning agent has the ability to perform interventions on targeted subsets of the population, apart from possessing qualitative causal side-information. This novel formalism captures intricacies in real-world scenarios such as software product experimentation where targeted experiments can be conducted. However, this... | Balaraman Ravindran, Chandrasekar Subramanian |  |
| 663 |  |  [Sound and Complete Neural Network Repair with Minimality and Locality Guarantees](https://openreview.net/forum?id=xS8AMYiEav3) |  | 0 | We present a novel methodology for repairing neural networks that use ReLU activation functions. Unlike existing methods that rely on modifying the weights of a neural network which can induce a global change in the function space, our approach applies only a localized change in the function space while still guaranteeing the removal of the buggy behavior. By leveraging... | Feisi Fu, Wenchao Li |  |
| 664 |  |  [Blaschke Product Neural Networks (BPNN): A Physics-Infused Neural Network for Phase Retrieval of Meromorphic Functions](https://openreview.net/forum?id=JJxiD-kg-oK) |  | 0 | Numerous physical systems are described by ordinary or partial differential equations whose solutions are given by holomorphic or meromorphic functions in the complex domain. In many cases, only the magnitude of these functions are observed on various points on the purely imaginary $j\omega$-axis since coherent measurement of their phases is often expensive. However, it... | Jordan M. Malof, Juncheng Dong, Mohammadreza Soltani, Omar Khatib, Simiao Ren, Vahid Tarokh, Willie Padilla, Yang Deng |  |
| 665 |  |  [Automated Self-Supervised Learning for Graphs](https://openreview.net/forum?id=rFbR4Fv-D6-) |  | 0 | Graph self-supervised learning has gained increasing attention due to its capacity to learn expressive node representations. Many pretext tasks, or loss functions have been designed from distinct perspectives. However, we observe that different pretext tasks affect downstream tasks differently cross datasets, which suggests that searching pretext tasks is crucial for... | Jiliang Tang, Neil Shah, Wei Jin, Xiangyu Zhao, Xiaorui Liu, Yao Ma |  |
| 666 |  |  [Creating Training Sets via Weak Indirect Supervision](https://openreview.net/forum?id=m8uJvVgwRci) |  | 0 | Creating labeled training sets has become one of the major roadblocks in machine learning. To address this, recent Weak Supervision (WS) frameworks synthesize training labels from multiple potentially noisy supervision sources. However, existing frameworks are restricted to supervision sources that share the same output space as the target task. To extend the scope of... | Alexander Ratner, Bohan Wang, Jieyu Zhang, Jing Bai, Xiangchen Song, Yaming Yang, Yujing Wang |  |
| 667 |  |  [Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs](https://openreview.net/forum?id=aTzMi4yV_RO) |  | 0 | The discovery of the disentanglement properties of the latent space in GANs motivated a lot of research to find the semantically meaningful directions on it. In this paper, we suggest that the disentanglement property is closely related to the geometry of the latent space. In this regard, we propose an unsupervised method for finding the semantic-factorizing directions... | Changyeon Yoon, Geonho Hwang, Jaewoong Choi, Jung Ho Park, Junho Lee, Myungjoo Kang |  |
| 668 |  |  [GradSign: Model Performance Inference with Theoretical Insights](https://openreview.net/forum?id=HObMhrCeAAF) |  | 0 | A key challenge in neural architecture search (NAS) is quickly inferring the predictive performance of a broad spectrum of networks to discover statistically accurate and computationally efficient ones. We refer to this task as model performance inference (MPI). The current practice for efficient MPI is gradient-based methods that leverage the gradients of a network at... | Zhihao Jia, Zhihao Zhang |  |
| 669 |  |  [You are AllSet: A Multiset Function Framework for Hypergraph Neural Networks](https://openreview.net/forum?id=hpBTIv2uy_E) |  | 0 | Hypergraphs are used to model higher-order interactions amongst agents and there exist many practically relevant instances of hypergraph datasets. To enable the efficient processing of hypergraph data, several hypergraph neural network platforms have been proposed for learning hypergraph properties and structure, with a special focus on node classification tasks.... | Chao Pan, Eli Chien, Jianhao Peng, Olgica Milenkovic |  |
| 670 |  |  [Synchromesh: Reliable Code Generation from Pre-trained Language Models](https://openreview.net/forum?id=KmtVD97J43e) |  | 0 | Large pre-trained language models have been used to generate code, providing a flexible interface for synthesizing programs from natural language specifications. However, they often violate syntactic and semantic rules of their output language, limiting their practical usability. In this paper, we propose Synchromesh: a framework for substantially improving the... | Alex Polozov, Ashish Tiwari, Christopher Meek, Gabriel Poesia, Gustavo Soares, Sumit Gulwani, Vu Le |  |
| 671 |  |  [Learning curves for continual learning in neural networks: Self-knowledge transfer and forgetting](https://openreview.net/forum?id=tFgdrQbbaa) |  | 0 | Sequential training from task to task is becoming one of the major objects in deep learning applications such as continual learning and transfer learning. Nevertheless, it remains unclear under what conditions the trained model's performance improves or deteriorates. To deepen our understanding of sequential training, this study provides a theoretical analysis of... | Ryo Karakida, Shotaro Akaho |  |
| 672 |  |  [Energy-Based Learning for Cooperative Games, with Applications to Valuation Problems in Machine Learning](https://openreview.net/forum?id=xLfAgCroImw) |  | 0 | Valuation problems, such as feature interpretation, data valuation and model valuation for ensembles, become increasingly more important in many machine learning applications. Such problems are commonly solved by well-known game-theoretic criteria, such as Shapley value or Banzhaf value. In this work, we present a novel energy-based treatment for cooperative games, with... | Andreas Krause, Jiaxiang Wu, Junzhou Huang, Tingyang Xu, Yatao Bian, Yu Rong |  |
| 673 |  |  [Pessimistic Model-based Offline Reinforcement Learning under Partial Coverage](https://openreview.net/forum?id=tyrJsbKAe6) |  | 0 | We study model-based offline Reinforcement Learning with general function approximation without a full coverage assumption on the offline data distribution. We present an algorithm named Constrained Pessimistic Policy Optimization (CPPO) which leverages a general function class and uses a constraint over the models to encode pessimism. Under the assumption that the... | Masatoshi Uehara, Wen Sun |  |
| 674 |  |  [Cold Brew: Distilling Graph Node Representations with Incomplete or Missing Neighborhoods](https://openreview.net/forum?id=1ugNpm7W6E) |  | 0 | Graph Neural Networks (GNNs) have achieved state-of-the-art performance in node classification, regression, and recommendation tasks. GNNs work well when rich and high-quality connections are available. However, their effectiveness is often jeopardized in many real-world graphs in which node degrees have power-law distributions. The extreme case of this situation, where... | Edward W. Huang, Karthik Subbian, Nikhil Rao, Sumeet Katariya, Wenqing Zheng, Zhangyang Wang |  |
| 675 |  |  [NASI: Label- and Data-agnostic Neural Architecture Search at Initialization](https://openreview.net/forum?id=v-v1cpNNK_v) |  | 0 | Recent years have witnessed a surging interest in Neural Architecture Search (NAS). Various algorithms have been proposed to improve the search efficiency and effectiveness of NAS, i.e., to reduce the search cost and improve the generalization performance of the selected architectures, respectively. However, the search efficiency of these algorithms is severely limited... | Beng Chin Ooi, Bryan Kian Hsiang Low, Shaofeng Cai, Yao Shu, Zhongxiang Dai |  |
| 676 |  |  [How to Train Your MAML to Excel in Few-Shot Classification](https://openreview.net/forum?id=49h_IkpJtaE) |  | 0 | Model-agnostic meta-learning (MAML) is arguably one of the most popular meta-learning algorithms nowadays. Nevertheless, its performance on few-shot classification is far behind many recent algorithms dedicated to the problem. In this paper, we point out several key facets of how to train MAML to excel in few-shot classification. First, we find that MAML needs a large... | HanJia Ye, WeiLun Chao |  |
| 677 |  |  [Communication-Efficient Actor-Critic Methods for Homogeneous Markov Games](https://openreview.net/forum?id=xy_2w3J3kH) |  | 0 | Recent success in cooperative multi-agent reinforcement learning (MARL) relies on centralized training and policy sharing. Centralized training eliminates the issue of non-stationarity MARL yet induces large communication costs, and policy sharing is empirically crucial to efficient learning in certain tasks yet lacks theoretical justification. In this paper, we... | Dingyang Chen, Qi Zhang, Yile Li |  |
| 678 |  |  [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://openreview.net/forum?id=vh-0sUt8HlG) |  | 0 | Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs,... | Mohammad Rastegari, Sachin Mehta |  |
| 679 |  |  [Spatial Graph Attention and Curiosity-driven Policy for Antiviral Drug Discovery](https://openreview.net/forum?id=kavTY__jxp) |  | 0 | We developed Distilled Graph Attention Policy Network (DGAPN), a reinforcement learning model to generate novel graph-structured chemical representations that optimize user-defined objectives by efficiently navigating a physically constrained domain. The framework is examined on the task of generating molecules that are designed to bind, noncovalently, to functional... | Andrew Deru Chen, Austin Clyde, Daniel A. Jacobson, James B. Brown, Manesh Shah, Martha S. Head, Mikaela Cashman, Neeraj Kumar, Nicholas Choma, Peter Nugent, Rick L. Stevens, Thomas S. Brettin, Verónica G. Melesse Vergara, Wibe Albert de Jong, Yulun Wu, Érica Teixeira Prates |  |
| 680 |  |  [Surrogate NAS Benchmarks: Going Beyond the Limited Search Spaces of Tabular NAS Benchmarks](https://openreview.net/forum?id=OnpFa95RVqs) |  | 0 | The most significant barrier to the advancement of Neural Architecture Search (NAS) is its demand for large computational resources, which hinders scientifically sound empirical evaluations of NAS methods. Tabular NAS benchmarks have alleviated this problem substantially, making it possible to properly evaluate NAS methods in seconds on commodity machines. However, an... | Arber Zela, Frank Hutter, Jovita Lukasik, Julien Niklas Siems, Lucas Zimmer, Margret Keuper |  |
| 681 |  |  [Certified Robustness for Deep Equilibrium Models via Interval Bound Propagation](https://openreview.net/forum?id=y1PXylgrXZ) |  | 0 | Deep equilibrium layers (DEQs) have demonstrated promising performance and are competitive with standard explicit models on many benchmarks. However, little is known about certifying robustness for these models. Inspired by interval bound propagation (IBP), we propose the IBP-MonDEQ layer, a DEQ layer whose robustness can be verified by computing upper and lower... | Colin Wei, J. Zico Kolter |  |
| 682 |  |  [Crystal Diffusion Variational Autoencoder for Periodic Material Generation](https://openreview.net/forum?id=03RLpj-tc_) |  | 0 | Generating the periodic structure of stable materials is a long-standing challenge for the material design community. This task is difficult because stable materials only exist in a low-dimensional subspace of all possible periodic arrangements of atoms: 1) the coordinates must lie in the local energy minimum defined by quantum mechanics, and 2) global stability also... | OctavianEugen Ganea, Regina Barzilay, Tian Xie, Tommi S. Jaakkola, Xiang Fu |  |
| 683 |  |  [Task Affinity with Maximum Bipartite Matching in Few-Shot Learning](https://openreview.net/forum?id=u2GZOiUTbt) |  | 0 | We propose an asymmetric affinity score for representing the complexity of utilizing the knowledge of one task for learning another one. Our method is based on the maximum bipartite matching algorithm and utilizes the Fisher Information matrix. We provide theoretical analyses demonstrating that the proposed score is mathematically well-defined, and subsequently use the... | Cat Phuoc Le, Juncheng Dong, Mohammadreza Soltani, Vahid Tarokh |  |
| 684 |  |  [Latent Image Animator: Learning to Animate Images via Latent Space Navigation](https://openreview.net/forum?id=7r6kDq0mK_) |  | 0 | Due to the remarkable progress of deep generative models, animating images has become increasingly efficient, whereas associated results have become increasingly realistic. Current animation-approaches commonly exploit structure representation extracted from driving videos. Such structure representation is instrumental in transferring motion from driving videos to still... | Antitza Dantcheva, Di Yang, François Brémond, Yaohui Wang |  |
| 685 |  |  [Know Thyself: Transferable Visual Control Policies Through Robot-Awareness](https://openreview.net/forum?id=o0ehFykKVtr) |  | 0 | Training visual control policies from scratch on a new robot typically requires generating large amounts of robot-specific data. How might we leverage data previously collected on another robot to reduce or even completely remove this need for robot-specific data? We propose a "robot-aware control" paradigm that achieves this by exploiting readily available knowledge... | Dinesh Jayaraman, Edward S. Hu, Kun Huang, Oleh Rybkin |  |
| 686 |  |  [Node Feature Extraction by Self-Supervised Multi-scale Neighborhood Prediction](https://openreview.net/forum?id=KJggliHbs8) |  | 0 | Learning on graphs has attracted significant attention in the learning community due to numerous real-world applications. In particular, graph neural networks (GNNs), which take \emph{numerical} node features and graph structure as inputs, have been shown to achieve state-of-the-art performance on various graph-related learning tasks. Recent works exploring the... | ChoJui Hsieh, Eli Chien, HsiangFu Yu, Inderjit S. Dhillon, Jiong Zhang, Olgica Milenkovic, WeiCheng Chang |  |
| 687 |  |  [Spherical Message Passing for 3D Molecular Graphs](https://openreview.net/forum?id=givsRXsOt9r) |  | 0 | We consider representation learning of 3D molecular graphs in which each atom is associated with a spatial position in 3D. This is an under-explored area of research, and a principled message passing framework is currently lacking. In this work, we conduct analyses in the spherical coordinate system (SCS) for the complete identification of 3D graph structures. Based on... | Bora Oztekin, Limei Wang, Meng Liu, Shuiwang Ji, Xuan Zhang, Yi Liu, Yuchao Lin |  |
| 688 |  |  [Fairness Guarantees under Demographic Shift](https://openreview.net/forum?id=wbPObLm6ueA) |  | 0 | Recent studies have demonstrated that using machine learning for social applications can lead to injustice in the form of racist, sexist, and otherwise unfair and discriminatory outcomes. To address this challenge, recent machine learning algorithms have been designed to limit the likelihood such unfair behaviors will occur. However, these approaches typically assume... | Blossom Metevier, Bruno Castro da Silva, Philip S. Thomas, Scott Niekum, Stephen Giguere, Yuriy Brun |  |
| 689 |  |  [Fooling Explanations in Text Classifiers](https://openreview.net/forum?id=j3krplz_4w6) |  | 0 | State-of-the-art text classification models are becoming increasingly reliant on deep neural networks (DNNs). Due to their black-box nature, faithful and robust explanation methods need to accompany classifiers for deployment in real-life scenarios. However, it has been shown that explanation methods in vision applications are susceptible to local, imperceptible... | Adam Ivankay, Chiara Marchiori, Ivan Girardi, Pascal Frossard |  |
| 690 |  |  [On the Learning and Learnability of Quasimetrics](https://openreview.net/forum?id=y0VvIg25yk) |  | 0 | Our world is full of asymmetries. Gravity and wind can make reaching a place easier than coming back. Social artifacts such as genealogy charts and citation graphs are inherently directed. In reinforcement learning and control, optimal goal-reaching strategies are rarely reversible (symmetrical). Distance functions supported on these asymmetrical structures are called... | Phillip Isola, Tongzhou Wang |  |
| 691 |  |  [Learning Prototype-oriented Set Representations for Meta-Learning](https://openreview.net/forum?id=WH6u2SvlLp4) |  | 0 | Learning from set-structured data is a fundamental problem that has recently attracted increasing attention, where a series of summary networks are introduced to deal with the set input. In fact, many meta-learning problems can be treated as set-input tasks. Most existing summary networks aim to design different architectures for the input set in order to enforce... | Dandan Guo, Hongyuan Zha, Long Tian, Minghe Zhang, Mingyuan Zhou |  |
| 692 |  |  [Embedded-model flows: Combining the inductive biases of model-free deep learning and explicit probabilistic modeling](https://openreview.net/forum?id=9pEJSVfDbba) |  | 0 | Normalizing flows have shown great success as general-purpose density estimators. However, many real world applications require the use of domain-specific knowledge, which normalizing flows cannot readily incorporate. We propose embedded-model flows (EMF), which alternate general-purpose transformations with structured layers that embed domain-specific inductive biases.... | Dave Moore, Emily Fertig, Gianluigi Silvestri, Luca Ambrogioni |  |
| 693 |  |  [A Relational Intervention Approach for Unsupervised Dynamics Generalization in Model-Based Reinforcement Learning](https://openreview.net/forum?id=YRq0ZUnzKoZ) |  | 0 | The generalization of model-based reinforcement learning (MBRL) methods to environments with unseen transition dynamics is an important yet challenging problem. Existing methods try to extract environment-specified information $Z$ from past transition segments to make the dynamics prediction model generalizable to different dynamics. However, because environments are... | Dacheng Tao, Jiaxian Guo, Mingming Gong |  |
| 694 |  |  [Critical Points in Quantum Generative Models](https://openreview.net/forum?id=2f1z55GVQN) |  | 0 | One of the most important properties of neural networks is the clustering of local minima of the loss function near the global minimum, enabling efficient training. Though generative models implemented on quantum computers are known to be more expressive than their traditional counterparts, it has empirically been observed that these models experience a transition in... | Eric Ricardo Anschütz |  |
| 695 |  |  [VOS: Learning What You Don't Know by Virtual Outlier Synthesis](https://openreview.net/forum?id=TW7d65uYu5M) |  | 0 | Out-of-distribution (OOD) detection has received much attention lately due to its importance in the safe deployment of neural networks. One of the key challenges is that models lack supervision signals from unknown data, and as a result, can produce overconfident predictions on OOD data. Previous approaches rely on real outlier datasets for model regularization, which... | Mu Cai, Xuefeng Du, Yixuan Li, Zhaoning Wang |  |
| 696 |  |  [Trust Region Policy Optimisation in Multi-Agent Reinforcement Learning](https://openreview.net/forum?id=EcGGFkNTxdJ) |  | 0 | Trust region methods rigorously enabled reinforcement learning (RL) agents to learn monotonically improving policies, leading to superior performance on a variety of tasks. Unfortunately, when it comes to multi-agent reinforcement learning (MARL), the property of monotonic improvement may not simply apply; this is because agents, even in cooperative games, could have... | Fanglei Sun, Jakub Grudzien Kuba, Jun Wang, Muning Wen, Ruiqing Chen, Yaodong Yang, Ying Wen |  |
| 697 |  |  [Unsupervised Disentanglement with Tensor Product Representations on the Torus](https://openreview.net/forum?id=neqU3HWDgE) |  | 0 | The current methods for learning representations with auto-encoders almost exclusively employ vectors as the latent representations. In this work, we propose to employ a tensor product structure for this purpose. This way, the obtained representations are naturally disentangled. In contrast to the conventional variations methods, which are targeted toward normally... | Amit Dekel, Lior Wolf, Michael Rotman, Shir Gur, Yaron Oz |  |
| 698 |  |  [Anomaly Detection for Tabular Data with Internal Contrastive Learning](https://openreview.net/forum?id=_hszZbt46bT) |  | 0 | We consider the task of finding out-of-class samples in tabular data, where little can be assumed on the structure of the data. In order to capture the structure of the samples of the single training class, we learn mappings that maximize the mutual information between each sample and the part that is masked out. The mappings are learned by employing a contrastive loss,... | Lior Wolf, Tom Shenkar |  |
| 699 |  |  [LIGS: Learnable Intrinsic-Reward Generation Selection for Multi-Agent Learning](https://openreview.net/forum?id=CpTuR2ECuW) |  | 0 | Efficient exploration is important for reinforcement learners (RL) to achieve high rewards. In multi-agent systems, coordinated exploration and behaviour is critical for agents to jointly achieve optimal outcomes. In this paper, we introduce a new general framework for improving coordination and performance of multi-agent reinforcement learners (MARL). Our framework,... | David Henry Mguni, Feifei Tong, Jiangcheng Zhu, Jianhong Wang, Jun Wang, Nicolas Perez Nieves, Oliver Slumbers, Taher Jafferjee, Yang Li, Yaodong Yang |  |
| 700 |  |  [Bayesian Modeling and Uncertainty Quantification for Learning to Optimize: What, Why, and How](https://openreview.net/forum?id=EVVadRFRgL7) |  | 0 | Optimizing an objective function with uncertainty awareness is well-known to improve the accuracy and confidence of optimization solutions. Meanwhile, another relevant but very different question remains yet open: how to model and quantify the uncertainty of an optimization algorithm (a.k.a., optimizer) itself? To close such a gap, the prerequisite is to consider the... | Tianlong Chen, Yang Shen, Yue Cao, Yuning You, Zhangyang Wang |  |
| 701 |  |  [Deep Ensembling with No Overhead for either Training or Testing: The All-Round Blessings of Dynamic Sparsity](https://openreview.net/forum?id=RLtqs6pzj1-) |  | 0 | The success of deep ensembles on improving predictive performance, uncertainty estimation, and out-of-distribution robustness has been extensively studied in the machine learning literature. Albeit the promising results, naively training multiple deep neural networks and combining their predictions at inference leads to prohibitive computational costs and memory... | Decebal Constantin Mocanu, Elena Mocanu, Ghada Sokar, Mykola Pechenizkiy, Shiwei Liu, Tianlong Chen, Xiaohan Chen, Zahra Atashgahi, Zhangyang Wang |  |
| 702 |  |  [HyperDQN: A Randomized Exploration Method for Deep Reinforcement Learning](https://openreview.net/forum?id=X0nrKAXu7g-) |  | 0 | Randomized least-square value iteration (RLSVI) is a provably efficient exploration method. However, it is limited to the case where (1) a good feature is known in advance and (2) this feature is fixed during the training. If otherwise, RLSVI suffers an unbearable computational burden to obtain the posterior samples. In this work, we present a practical algorithm named... | Tong Zhang, Yingru Li, Yushun Zhang, ZhiQuan Luo, Ziniu Li |  |
| 703 |  |  [Unraveling Model-Agnostic Meta-Learning via The Adaptation Learning Rate](https://openreview.net/forum?id=3rULBvOJ8D2) |  | 0 | Model-Agnostic Meta-Learning (MAML) aims to find initial weights that allow fast adaptation to new tasks. The adaptation (inner loop) learning rate in MAML plays a central role in enabling such fast adaptation. However, how to choose this value in practice and how this choice affects the adaptation error remains less explored. In this paper, we study the effect of the... | Fusheng Liu, Qianxiao Li, Yingtian Zou |  |
| 704 |  |  [iFlood: A Stable and Effective Regularizer](https://openreview.net/forum?id=MsHnJPaBUZE) |  | 0 | Various regularization methods have been designed to prevent overfitting of machine learning models. Among them, a surprisingly simple yet effective one, called Flooding, is proposed recently, which directly constrains the training loss on average to stay at a given level. However, our further studies uncover that the design of the loss function of Flooding can lead to... | Bolin Ding, Ce Zhang, Jingren Zhou, Yaliang Li, Yuexiang Xie, Zhen Wang |  |
| 705 |  |  [FlexConv: Continuous Kernel Convolutions With Differentiable Kernel Sizes](https://openreview.net/forum?id=3jooF27-0Wy) |  | 0 | When designing Convolutional Neural Networks (CNNs), one must select the size of the convolutional kernels before training. Recent works show CNNs benefit from different kernel sizes at different layers, but exploring all possible combinations is unfeasible in practice. A more efficient approach is to learn the kernel size during training. However, existing works that... | David W. Romero, Erik J. Bekkers, Jakub Mikolaj Tomczak, Jan van Gemert, Mark Hoogendoorn, RobertJan Bruintjes |  |
| 706 |  |  [Zero Pixel Directional Boundary by Vector Transform](https://openreview.net/forum?id=nxcABL7jbQh) |  | 0 | Boundaries or contours are among the primary visual cues used by human and computer vision systems. One of the key problems in boundary detection is the loss formulation, which typically leads to class imbalance and, as a consequence, to thick boundaries which require non-differential post-processing steps to be thinned. In this paper, we re-interpret boundaries as 1-D... | Ajad Chhatkuli, Edoardo Mello Rella, Ender Konukoglu, Luc Van Gool, Yun Liu |  |
| 707 |  |  [A Conditional Point Diffusion-Refinement Paradigm for 3D Point Cloud Completion](https://openreview.net/forum?id=wqD6TfbYkrn) |  | 0 | 3D point clouds are an important data format that captures 3D information for real world objects. Since 3D point clouds scanned in the real world are often incomplete, it is important to recover the complete point cloud for many downstreaming applications. Most existing point cloud completion methods use the Chamfer Distance (CD) loss for training. The CD loss estimates... | Dahua Lin, Liang Pan, Xudong Xu, Zhaoyang Lyu, Zhifeng Kong |  |
| 708 |  |  [Auto-Transfer: Learning to Route Transferable Representations](https://openreview.net/forum?id=SIKV0_MrZlr) |  | 0 | Knowledge transfer between heterogeneous source and target networks and tasks has received a lot of attention in recent times as large amounts of quality labeled data can be difficult to obtain in many applications. Existing approaches typically constrain the target deep neural network (DNN) feature representations to be close to the source DNNs feature representations,... | Amit Dhurandhar, Karthikeyan Shanmugam, Keerthiram Murugesan, PinYu Chen, Ronny Luss, Vijay Sadashivaiah |  |
| 709 |  |  [PoNet: Pooling Network for Efficient Token Mixing in Long Sequences](https://openreview.net/forum?id=9jInD9JjicF) |  | 0 | Transformer-based models have achieved great success in various NLP, vision, and speech tasks. However, the core of Transformer, the self-attention mechanism, has a quadratic time and memory complexity with respect to the sequence length, which hinders applications of Transformer-based models to long sequences. Many approaches have been proposed to mitigate this... | ChaoHong Tan, Qian Chen, Qinglin Zhang, Siqi Zheng, Wen Wang, ZhenHua Ling |  |
| 710 |  |  [Huber Additive Models for Non-stationary Time Series Analysis](https://openreview.net/forum?id=9kpuB2bgnim) |  | 0 | Sparse additive models have shown promising ﬂexibility and interpretability in processing time series data. However, existing methods usually assume the time series data to be stationary and the innovation is sampled from a Gaussian distribution. Both assumptions are too stringent for heavy-tailed and non-stationary time series data that frequently arise in practice,... | Dacheng Tao, Fengxiang He, Hong Chen, Xianrui Zhong, Yingjie Wang |  |
| 711 |  |  [Model-augmented Prioritized Experience Replay](https://openreview.net/forum?id=WuEiafqdy9H) |  | 0 | Experience replay is an essential component in off-policy model-free reinforcement learning (MfRL). Due to its effectiveness, various methods for calculating priority scores on experiences have been proposed for sampling. Since critic networks are crucial to policy learning, TD-error, directly correlated to $Q$-values, is one of the most frequently used features to... | Eunho Yang, Jinwoo Shin, Sung Ju Hwang, Youngmin Oh |  |
| 712 |  |  [Post-Training Detection of Backdoor Attacks for Two-Class and Multi-Attack Scenarios](https://openreview.net/forum?id=MSgB8D4Hy51) |  | 0 | Backdoor attacks (BAs) are an emerging threat to deep neural network classifiers. A victim classifier will predict to an attacker-desired target class whenever a test sample is embedded with the same backdoor pattern (BP) that was used to poison the classifier's training set. Detecting whether a classifier is backdoor attacked is not easy in practice, especially when... | David J. Miller, George Kesidis, Zhen Xiang |  |
| 713 |  |  [Multi-Task Processes](https://openreview.net/forum?id=9otKVlgrpZG) |  | 0 | Neural Processes (NPs) consider a task as a function realized from a stochastic process and flexibly adapt to unseen tasks through inference on functions. However, naive NPs can model data from only a single stochastic process and are designed to infer each task independently. Since many real-world data represent a set of correlated tasks from multiple sources (e.g.,... | Donggyun Kim, Seongwoong Cho, Seunghoon Hong, Wonkwang Lee |  |
| 714 |  |  [Dynamic Token Normalization improves Vision Transformers](https://openreview.net/forum?id=f9MHpAGUyMn) |  | 0 | Vision Transformer (ViT) and its variants (e.g., Swin, PVT) have achieved great success in various computer vision tasks, owing to their capability to learn long-range contextual information. Layer Normalization (LN) is an essential ingredient in these models. However, we found that the ordinary LN makes tokens at different positions similar in magnitude because it... | Ping Luo, Wenqi Shao, Xiaogang Wang, Xuyuan Xu, Ying Shan, Yixiao Ge, Zhaoyang Zhang |  |
| 715 |  |  [Symbolic Learning to Optimize: Towards Interpretability and Scalability](https://openreview.net/forum?id=ef0nInZHKIC) |  | 0 | Recent studies on Learning to Optimize (L2O) suggest a promising path to automating and accelerating the optimization procedure for complicated tasks. Existing L2O models parameterize optimization rules by neural networks, and learn those numerical rules via meta-training. However, they face two common pitfalls: (1) scalability: the numerical rules represented by neural... | Tianlong Chen, TingKuei Hu, Wenqing Zheng, Zhangyang Wang |  |
| 716 |  |  [Sequential Reptile: Inter-Task Gradient Alignment for Multilingual Learning](https://openreview.net/forum?id=ivQruZvXxtz) |  | 0 | Multilingual models jointly pretrained on multiple languages have achieved remarkable performance on various multilingual downstream tasks. Moreover, models finetuned on a single monolingual downstream task have shown to generalize to unseen languages. In this paper, we first show that it is crucial for those tasks to align gradients between them in order to maximize... | Haebeom Lee, Juho Lee, Seanie Lee, Sung Ju Hwang |  |
| 717 |  |  [Pseudo Numerical Methods for Diffusion Models on Manifolds](https://openreview.net/forum?id=PlKWVd2yBkY) |  | 0 | Denoising Diffusion Probabilistic Models (DDPMs) can generate high-quality samples such as image and audio samples. However, DDPMs require hundreds to thousands of iterations to produce a sample. Several prior works have successfully accelerated DDPMs through adjusting the variance schedule (e.g., Improved Denoising Diffusion Probabilistic Models) or the denoising... | Luping Liu, Yi Ren, Zhijie Lin, Zhou Zhao |  |
| 718 |  |  [Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm](https://openreview.net/forum?id=zq1iJkNk3uN) |  | 0 | Recently, large-scale Contrastive Language-Image Pre-training (CLIP) has attracted unprecedented attention for its impressive zero-shot recognition ability and excellent transferability to downstream tasks. However, CLIP is quite data-hungry and requires 400M image-text pairs for pre-training, thereby restricting its adoption. This work proposes a novel training... | Feng Liang, Fengwei Yu, Jing Shao, Junjie Yan, Lichen Zhao, Wanli Ouyang, Yangguang Li, Yufeng Cui |  |
| 719 |  |  [Environment Predictive Coding for Visual Navigation](https://openreview.net/forum?id=DBiQQYWykyy) |  | 0 | We introduce environment predictive coding, a self-supervised approach to learn environment-level representations for embodied agents. In contrast to prior work on self-supervised learning for individual images, we aim to encode a 3D environment using a series of images observed by an agent moving in it. We learn these representations via a masked-zone prediction task,... | Kristen Grauman, Santhosh Kumar Ramakrishnan, Tushar Nagarajan, Ziad AlHalah |  |
| 720 |  |  [Topological Experience Replay](https://openreview.net/forum?id=OXRZeMmOI7a) |  | 0 | State-of-the-art deep Q-learning methods update Q-values using state transition tuples sampled from the experience replay buffer. This strategy often randomly samples or prioritizes data sampling based on measures such as the temporal difference (TD) error. Such sampling strategies can be inefficient at learning Q-function since a state's correct Q-value preconditions... | Joni Pajarinen, Pulkit Agrawal, Tao Chen, YenChen Lin, ZhangWei Hong |  |
| 721 |  |  [Sparsity Winning Twice: Better Robust Generalization from More Efficient Training](https://openreview.net/forum?id=SYuJXrXq8tw) |  | 0 | Recent studies demonstrate the deep networks, even robustified by the state-of-the-art adversarial training (AT), still suffer from large robust generalization gaps, in addition to the much more expensive training costs than standard training. In this paper, we investigate this intriguing problem from a new perspective, i.e., $\textit{injecting appropriate forms of... | Haoyu Ma, Pengjun Wang, Santosh Balachandra, Tianlong Chen, Zehao Wang, Zhangyang Wang, Zhenyu Zhang |  |
| 722 |  |  [CrossMatch: Cross-Classifier Consistency Regularization for Open-Set Single Domain Generalization](https://openreview.net/forum?id=48RBsJwGkJf) |  | 0 | Single domain generalization (SDG) is a challenging scenario of domain generalization, where only one source domain is available to train the model. Typical SDG methods are based on the adversarial data augmentation strategy, which complements the diversity of source domain to learn a robust model. Existing SDG methods require the source and target domains to have the... | Ronghang Zhu, Sheng Li |  |
| 723 |  |  [Robust Unlearnable Examples: Protecting Data Privacy Against Adversarial Learning](https://openreview.net/forum?id=baUQQPwQiAg) |  | 0 | The tremendous amount of accessible data in cyberspace face the risk of being unauthorized used for training deep learning models. To address this concern, methods are proposed to make data unlearnable for deep learning models by adding a type of error-minimizing noise. However, such conferred unlearnability is found fragile to adversarial training. In this paper, we... | Dacheng Tao, Fengxiang He, Li Shen, Shaopeng Fu, Yang Liu |  |
| 724 |  |  [A Non-Parametric Regression Viewpoint : Generalization of Overparametrized Deep RELU Network Under Noisy Observations](https://openreview.net/forum?id=bZJbzaj_IlP) |  | 0 | We study the generalization properties of the overparameterized deep neural network (DNN) with Rectified Linear Unit (ReLU) activations. Under the non-parametric regression framework, it is assumed that the ground-truth function is from a reproducing kernel Hilbert space (RKHS) induced by a neural tangent kernel (NTK) of ReLU DNN, and a dataset is given with the noises.... | Hyunouk Ko, Namjoon Suh, Xiaoming Huo |  |
| 725 |  |  [Active Hierarchical Exploration with Stable Subgoal Representation Learning](https://openreview.net/forum?id=sNuFKTMktcY) |  | 0 | Goal-conditioned hierarchical reinforcement learning (GCHRL) provides a promising approach to solving long-horizon tasks. Recently, its success has been extended to more general settings by concurrently learning hierarchical policies and subgoal representations. Although GCHRL possesses superior exploration ability by decomposing tasks via subgoals, existing GCHRL... | Chongjie Zhang, Jianhao Wang, Jin Zhang, Siyuan Li, Yang Yu |  |
| 726 |  |  [Deep AutoAugment](https://openreview.net/forum?id=St-53J9ZARf) |  | 0 | While recent automated data augmentation methods lead to state-of-the-art results, their design spaces and the derived data augmentation strategies still incorporate strong human priors. In this work, instead of fixing a set of hand-picked default augmentations alongside the searched data augmentations, we propose a fully automated approach for data augmentation search... | Mi Zhang, Shen Yan, Yu Zheng, Zhi Zhang |  |
| 727 |  |  [Temporal Alignment Prediction for Supervised Representation Learning and Few-Shot Sequence Classification](https://openreview.net/forum?id=p3DKPQ7uaAi) |  | 0 | Explainable distances for sequence data depend on temporal alignment to tackle sequences with different lengths and local variances. Most sequence alignment methods infer the optimal alignment by solving an optimization problem under pre-defined feasible alignment constraints, which not only is time-consuming, but also makes end-to-end sequence learning intractable. In... | Bing Su, JiRong Wen |  |
| 728 |  |  [Anti-Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis: From Theory to Practice](https://openreview.net/forum?id=O476oWmiNNp) |  | 0 | Vision Transformer (ViT) has recently demonstrated promise in computer vision problems. However, unlike Convolutional Neural Networks (CNN), it is known that the performance of ViT saturates quickly with depth increasing, due to the observed attention collapse or patch uniformity. Despite a couple of empirical solutions, a rigorous framework studying on this scalability... | Peihao Wang, Tianlong Chen, Wenqing Zheng, Zhangyang Wang |  |
| 729 |  |  [Self-ensemble Adversarial Training for Improved Robustness](https://openreview.net/forum?id=oU3aTsmeRQV) |  | 0 | Due to numerous breakthroughs in real-world applications brought by machine intelligence, deep neural networks (DNNs) are widely employed in critical applications. However, predictions of DNNs are easily manipulated with imperceptible adversarial perturbations, which impedes the further deployment of DNNs and may result in profound security and privacy implications. By... | Hongjun Wang, Yisen Wang |  |
| 730 |  |  [Do deep networks transfer invariances across classes?](https://openreview.net/forum?id=Fn7i_r5rR0q) |  | 0 | In order to generalize well, classifiers must learn to be invariant to nuisance transformations that do not alter an input's class. Many problems have "class-agnostic" nuisance transformations that apply similarly to all classes, such as lighting and background changes for image classification. Neural networks can learn these invariances given sufficient data, but many... | Alexander Robey, Allan Zhou, Chelsea Finn, Fahim Tajwar, George J. Pappas, Hamed Hassani, Tom Knowles |  |
| 731 |  |  [Cross-Trajectory Representation Learning for Zero-Shot Generalization in RL](https://openreview.net/forum?id=XOh5x-vxsrV) |  | 0 | A highly desirable property of a reinforcement learning (RL) agent -- and a major difficulty for deep RL approaches -- is the ability to generalize policies learned on a few tasks over a high-dimensional observation space to similar tasks not seen during training. Many promising approaches to this challenge consider RL as a process of training two functions... | Ahmed M. Ahmed, Andrey Kolobov, Bogdan Mazoure, Patrick MacAlpine, R. Devon Hjelm |  |
| 732 |  |  [On Covariate Shift of Latent Confounders in Imitation and Reinforcement Learning](https://openreview.net/forum?id=w01vBAcewNX) |  | 0 | We consider the problem of using expert data with unobserved confounders for imitation and reinforcement learning. We begin by defining the problem of learning from confounded expert data in a contextual MDP setup. We analyze the limitations of learning from such data with and without external reward and propose an adjustment of standard imitation learning algorithms to... | Assaf Hallak, Gal Chechik, Gal Dalal, Guy Tennenholtz, Shie Mannor, Uri Shalit |  |
| 733 |  |  [RvS: What is Essential for Offline RL via Supervised Learning?](https://openreview.net/forum?id=S874XAIpkR-) |  | 0 | Recent work has shown that supervised learning alone, without temporal difference (TD) learning, can be remarkably effective for offline RL. When does this hold true, and which algorithmic components are necessary? Through extensive experiments, we boil supervised learning for offline RL down to its essential elements. In every environment suite we consider, simply... | Benjamin Eysenbach, Ilya Kostrikov, Scott Emmons, Sergey Levine |  |
| 734 |  |  [Learning Guarantees for Graph Convolutional Networks on the Stochastic Block Model](https://openreview.net/forum?id=dpXL6lz4mOQ) |  | 0 | An abundance of neural network models and algorithms for diverse tasks on graphs have been developed in the past five years. However, very few provable guarantees have been available for the performance of graph neural network models. This state of affairs is in contrast with the steady progress on the theoretical underpinnings of traditional dense and convolutional... | Wei Lu |  |
| 735 |  |  [Learning Versatile Neural Architectures by Propagating Network Codes](https://openreview.net/forum?id=KEQl-MZ5fg7) |  | 0 | This work explores how to design a single neural network capable of adapting to multiple heterogeneous vision tasks, such as image segmentation, 3D detection, and video recognition. This goal is challenging because both network architecture search (NAS) spaces and methods in different tasks are inconsistent. We solve this challenge from both sides. We first introduce a... | Haoyu Lu, Jingdong Wang, Linjie Yang, Mingyu Ding, Ping Luo, Yuqi Huo, Zhe Wang, Zhiwu Lu |  |
| 736 |  |  [Task-Induced Representation Learning](https://openreview.net/forum?id=OzyXtIZAzFv) |  | 0 | In this work, we evaluate the effectiveness of representation learning approaches for decision making in visually complex environments. Representation learning is essential for effective reinforcement learning (RL) from high-dimensional in- puts. Unsupervised representation learning approaches based on reconstruction, prediction or contrastive learning have shown... | Anisha Gunjal, Joseph J. Lim, Jun Yamada, Karl Pertsch |  |
| 737 |  |  [Graph-based Nearest Neighbor Search in Hyperbolic Spaces](https://openreview.net/forum?id=USIgIY6TNDe) |  | 0 | The nearest neighbor search (NNS) problem is widely studied in Euclidean space, and graph-based algorithms are known to outperform other approaches for this task. However, hyperbolic geometry often allows for better data representation in various domains, including graphs, words, and images. In this paper, we show that graph-based approaches are also well suited for... | Alexander Kolpakov, Dmitry Baranchuk, Liudmila Prokhorenkova, Nikolay Bogachev, Yury Demidovich |  |
| 738 |  |  [Generative Models as a Data Source for Multiview Representation Learning](https://openreview.net/forum?id=qhAeZjs7dCL) |  | 0 | Generative models are now capable of producing highly realistic images that look nearly indistinguishable from the data on which they are trained. This raises the question: if we have good enough generative models, do we still need datasets? We investigate this question in the setting of learning general-purpose visual representations from a black-box generative model... | Ali Jahanian, Phillip Isola, Xavier Puig, Yonglong Tian |  |
| 739 |  |  [GiraffeDet: A Heavy-Neck Paradigm for Object Detection](https://openreview.net/forum?id=cBu4ElJfneV) |  | 0 | In conventional object detection frameworks, a backbone body inherited from image recognition models extracts deep latent features and then a neck module fuses these latent features to capture information at different scales. As the resolution in object detection is much larger than in image recognition, the computational cost of the backbone often dominates the total... | Hao Li, Junyan Wang, Ming C. Lin, Xiuyu Sun, Yiqi Jiang, Zhiyu Tan |  |
| 740 |  |  [A Unified Wasserstein Distributional Robustness Framework for Adversarial Training](https://openreview.net/forum?id=Dzpe9C1mpiv) |  | 0 | It is well-known that deep neural networks (DNNs) are susceptible to adversarial attacks, exposing a severe fragility of deep learning systems. As the result, adversarial training (AT) method, by incorporating adversarial examples during training, represents a natural and effective approach to strengthen the robustness of a DNN-based classifier. However, most AT-based... | Anh Tuan Bui, Dinh Q. Phung, He Zhao, Quan Hung Tran, Trung Le |  |
| 741 |  |  [miniF2F: a cross-system benchmark for formal Olympiad-level mathematics](https://openreview.net/forum?id=9ZPegFuFTFv) |  | 0 | We present $\textsf{miniF2F}$, a dataset of formal Olympiad-level mathematics problems statements intended to provide a unified cross-system benchmark for neural theorem proving. The $\textsf{miniF2F}$ benchmark currently targets Metamath, Lean, Isabelle (partially) and HOL Light (partially) and consists of 488 problem statements drawn from the AIME, AMC, and the... | Jesse Michael Han, Kunhao Zheng, Stanislas Polu |  |
| 742 |  |  [Towards Model Agnostic Federated Learning Using Knowledge Distillation](https://openreview.net/forum?id=lQI_mZjvBxj) |  | 0 | Is it possible to design an universal API for federated learning using which an ad-hoc group of data-holders (agents) collaborate with each other and perform federated learning? Such an API would necessarily need to be model-agnostic i.e. make no assumption about the model architecture being used by the agents, and also cannot rely on having representative public data... | Andrei Afonin, Sai Praneeth Karimireddy |  |
| 743 |  |  [Acceleration of Federated Learning with Alleviated Forgetting in Local Training](https://openreview.net/forum?id=541PxiEKN3F) |  | 0 | Federated learning (FL) enables distributed optimization of machine learning models while protecting privacy by independently training local models on each client and then aggregating parameters on a central server, thereby producing an effective global model. Although a variety of FL algorithms have been proposed, their training efficiency remains low when the data are... | Chencheng Xu, Minlie Huang, Tao Jiang, Zhiwei Hong |  |
| 744 |  |  [Discovering Invariant Rationales for Graph Neural Networks](https://openreview.net/forum?id=hGXij5rfiHw) |  | 0 | Intrinsic interpretability of graph neural networks (GNNs) is to find a small subset of the input graph's features --- rationale --- which guides the model prediction. Unfortunately, the leading rationalization models often rely on data biases, especially shortcut features, to compose rationales and make predictions without probing the critical and causal patterns.... | An Zhang, TatSeng Chua, Xiang Wang, Xiangnan He, Yingxin Wu |  |
| 745 |  |  [Representing Mixtures of Word Embeddings with Mixtures of Topic Embeddings](https://openreview.net/forum?id=IYMuTbGzjFU) |  | 0 | A topic model is often formulated as a generative model that explains how each word of a document is generated given a set of topics and document-specific topic proportions. It is focused on capturing the word co-occurrences in a document and hence often suffers from poor performance in analyzing short documents. In addition, its parameter estimation often relies on... | Bo Chen, Dandan Guo, Dongsheng Wang, He Zhao, Huangjie Zheng, Korawat Tanwisuth, Mingyuan Zhou |  |
| 746 |  |  [Generative Modeling with Optimal Transport Maps](https://openreview.net/forum?id=5JdLZg346Lw) |  | 0 | With the discovery of Wasserstein GANs, Optimal Transport (OT) has become a powerful tool for large-scale generative modeling tasks. In these tasks, OT cost is typically used as the loss for training GANs. In contrast to this approach, we show that the OT map itself can be used as a generative model, providing comparable performance. Previous analogous approaches... | Alexander Korotin, Evgeny Burnaev, Litu Rout |  |
| 747 |  |  [Focus on the Common Good: Group Distributional Robustness Follows](https://openreview.net/forum?id=irARV_2VFs4) |  | 0 | We consider the problem of training a classification model with group annotated training data. Recent work has established that, if there is distribution shift across different groups, models trained using the standard empirical risk minimization (ERM) objective suffer from poor performance on minority groups and that group distributionally robust optimization... | Praneeth Netrapalli, Sunita Sarawagi, Vihari Piratla |  |
| 748 |  |  [Omni-Scale CNNs: a simple and effective kernel size configuration for time series classification](https://openreview.net/forum?id=PDYs7Z2XFGv) |  | 0 | The size of the receptive field has been one of the most important factors for One Dimensional Convolutional Neural Networks (1D-CNNs) on time series classification tasks. Large efforts have been taken to choose the appropriate receptive field size, for it has a huge influence on the performance and differs significantly for each dataset. In this paper, we propose an... | Guodong Long, Jing Jiang, Lu Liu, Michael Blumenstein, Tianyi Zhou, Wensi Tang |  |
| 749 |  |  [Ada-NETS: Face Clustering via Adaptive Neighbour Discovery in the Structure Space](https://openreview.net/forum?id=QJWVP4CTmW4) |  | 0 | Face clustering has attracted rising research interest recently to take advantage of massive amounts of face images on the web. State-of-the-art performance has been achieved by Graph Convolutional Networks (GCN) due to their powerful representation capacity. However, existing GCN-based methods build face graphs mainly according to $k$NN relations in the feature space,... | Fangyi Zhang, Ming Lin, Senzhang Wang, Xiuyu Sun, Yaobin Zhang, Yaohua Wang, YuQi Zhang |  |
| 750 |  |  [Decoupled Adaptation for Cross-Domain Object Detection](https://openreview.net/forum?id=VNqaB1g9393) |  | 0 | Cross-domain object detection is more challenging than object classification since multiple objects exist in an image and the location of each object is unknown in the unlabeled target domain. As a result, when we adapt features of different objects to enhance the transferability of the detector, the features of the foreground and the background are easy to be confused,... | Baixu Chen, Jianmin Wang, Junguang Jiang, Mingsheng Long |  |
| 751 |  |  [Rethinking Network Design and Local Geometry in Point Cloud: A Simple Residual MLP Framework](https://openreview.net/forum?id=3Pbra-_u76D) |  | 0 | Point cloud analysis is challenging due to irregularity and unordered data structure. To capture the 3D geometries, prior works mainly rely on exploring sophisticated local geometric extractors, using convolution, graph, or attention mechanisms. These methods, however, incur unfavorable latency during inference and the performance saturates over the past few years. In... | Can Qin, Haoxi Ran, Haoxuan You, Xu Ma, Yun Fu |  |
| 752 |  |  [New Insights on Reducing Abrupt Representation Change in Online Continual Learning](https://openreview.net/forum?id=N8MaByOzUfb) |  | 0 | In the online continual learning paradigm, agents must learn from a changing distribution while respecting memory and compute constraints. Experience Replay (ER), where a small subset of past data is stored and replayed alongside new data, has emerged as a simple and effective learning strategy. In this work, we focus on the change in representations of observed data... | Eugene Belilovsky, Joelle Pineau, Lucas Caccia, Nader Asadi, Rahaf Aljundi, Tinne Tuytelaars |  |
| 753 |  |  [Demystifying Batch Normalization in ReLU Networks: Equivalent Convex Optimization Models and Implicit Regularization](https://openreview.net/forum?id=6XGgutacQ0B) |  | 0 | Batch Normalization (BN) is a commonly used technique to accelerate and stabilize training of deep neural networks. Despite its empirical success, a full theoretical understanding of BN is yet to be developed. In this work, we analyze BN through the lens of convex optimization. We introduce an analytic framework based on convex duality to obtain exact convex... | Arda Sahiner, Batu Ozturkler, John M. Pauly, Mert Pilanci, Morteza Mardani, Tolga Ergen |  |
| 754 |  |  [Associated Learning: an Alternative to End-to-End Backpropagation that Works on CNN, RNN, and Transformer](https://openreview.net/forum?id=4N-17dske79) |  | 0 | This paper studies Associate Learning (AL), an alternative methodology to the end-to-end backpropagation (BP). We introduce the workflow to convert a neural network into a proper structure such that AL can be used to learn the weights for various types of neural networks. We compared AL and BP on some of the most successful types of neural networks -- Convolutional... | Dennis Y. H. Wu, Dinan Lin, HungHsuan Chen, Vincent Chen |  |
| 755 |  |  [MetaShift: A Dataset of Datasets for Evaluating Contextual Distribution Shifts and Training Conflicts](https://openreview.net/forum?id=MTex8qKavoS) |  | 0 | Understanding the performance of machine learning models across diverse data distributions is critically important for reliable applications. Motivated by this, there is a growing focus on curating benchmark datasets that capture distribution shifts. While valuable, the existing benchmarks are limited in that many of them only contain a small number of shifts and they... | James Zou, Weixin Liang |  |
| 756 |  |  [FP-DETR: Detection Transformer Advanced by Fully Pre-training](https://openreview.net/forum?id=yjMQuLLcGWK) |  | 0 | Large-scale pre-training has proven to be effective for visual representation learning on downstream tasks, especially for improving robustness and generalization. However, the recently developed detection transformers only employ pre-training on its backbone while leaving the key component, i.e., a 12-layer transformer, being trained from scratch, which prevents the... | Dacheng Tao, Jing Zhang, Wen Wang, Yang Cao |  |
| 757 |  |  [Efficient and Differentiable Conformal Prediction with General Function Classes](https://openreview.net/forum?id=Ht85_jyihxp) |  | 0 | Quantifying the data uncertainty in learning tasks is often done by learning a prediction interval or prediction set of the label given the input. Two commonly desired properties for learned prediction sets are \emph{valid coverage} and \emph{good efficiency} (such as low length or low cardinality). Conformal prediction is a powerful technique for learning prediction... | Caiming Xiong, Huan Wang, Song Mei, Yingbo Zhou, Yu Bai |  |
| 758 |  |  [Safe Neurosymbolic Learning with Differentiable Symbolic Execution](https://openreview.net/forum?id=NYBmJN4MyZ) |  | 0 | We study the problem of learning verifiably safe parameters for programs that use neural networks as well as symbolic, human-written code. Such neurosymbolic programs arise in many safety-critical domains. However, because they need not be differentiable, it is hard to learn their parameters using existing gradient-based approaches to safe learning. Our method,... | Chenxi Yang, Swarat Chaudhuri |  |
| 759 |  |  [SimVLM: Simple Visual Language Model Pretraining with Weak Supervision](https://openreview.net/forum?id=GUrhfTuf_3) |  | 0 | With recent progress in joint modeling of visual and textual representations, Vision-Language Pretraining (VLP) has achieved impressive performance on many multimodal downstream tasks. However, the requirement for expensive annotations including clean image captions and regional labels limits the scalability of existing approaches, and complicates the pretraining... | Adams Wei Yu, Jiahui Yu, Yuan Cao, Yulia Tsvetkov, Zihang Dai, Zirui Wang |  |
| 760 |  |  [Bundle Networks: Fiber Bundles, Local Trivializations, and a Generative Approach to Exploring Many-to-one Maps](https://openreview.net/forum?id=aBXzcPPOuX) |  | 0 | Many-to-one maps are ubiquitous in machine learning, from the image recognition model that assigns a multitude of distinct images to the concept of “cat” to the time series forecasting model which assigns a range of distinct time-series to a single scalar regression value. While the primary use of such models is naturally to associate correct output to each input, in... | Henry Kvinge, Nico Courts |  |
| 761 |  |  [Privacy Implications of Shuffling](https://openreview.net/forum?id=5i2f-aR6B8H) |  | 0 | \ldp deployments are vulnerable to inference attacks as an adversary can link the noisy responses to their identity and subsequently, auxiliary information using the \textit{order} of the data. An alternative model, shuffle \textsf{DP}, prevents this by shuffling the noisy responses uniformly at random. However, this limits the data learnability -- only symmetric... | Amrita Roy Chowdhury, Casey Meehan, Kamalika Chaudhuri, Somesh Jha |  |
| 762 |  |  [On the role of population heterogeneity in emergent communication](https://openreview.net/forum?id=5Qkd7-bZfI) |  | 0 | Populations have often been perceived as a structuring component for language to emerge and evolve: the larger the population, the more systematic the language. While this observation is widespread in the sociolinguistic literature, it has not been reproduced in computer simulations with neural agents. In this paper, we thus aim to clarify this apparent contradiction.... | Emmanuel Dupoux, Florian Strub, JeanBastien Grill, Mathieu Rita, Olivier Pietquin |  |
| 763 |  |  [Hindsight is 20/20: Leveraging Past Traversals to Aid 3D Perception](https://openreview.net/forum?id=qsZoGvFiJn1) |  | 0 | Self-driving cars must detect vehicles, pedestrians, and other trafﬁc participants accurately to operate safely. Small, far-away, or highly occluded objects are particularly challenging because there is limited information in the LiDAR point clouds for detecting them. To address this challenge, we leverage valuable information from the past: in particular, data... | Bharath Hariharan, Junan Chen, Katie Z. Luo, Kilian Q. Weinberger, Mark E. Campbell, WeiLun Chao, Wen Sun, Xiangyu Chen, Yurong You |  |
| 764 |  |  [Language-driven Semantic Segmentation](https://openreview.net/forum?id=RriDjddCLN) |  | 0 | We present LSeg, a novel model for language-driven semantic image segmentation. LSeg uses a text encoder to compute embeddings of descriptive input labels (e.g., \`\`grass'' or \`\`building'') together with a transformer-based image encoder that computes dense per-pixel embeddings of the input image. The image encoder is trained with a contrastive objective to align... | Boyi Li, Kilian Q. Weinberger, René Ranftl, Serge J. Belongie, Vladlen Koltun |  |
| 765 |  |  [Image BERT Pre-training with Online Tokenizer](https://openreview.net/forum?id=ydopy-e6Dg) |  | 0 | The success of language Transformers is primarily attributed to the pretext task of masked language modeling (MLM), where texts are first tokenized into semantically meaningful pieces. In this work, we study masked image modeling (MIM) and indicate the necessity and challenges of using a semantically meaningful visual tokenizer. We present a self-supervised framework... | Alan L. Yuille, Chen Wei, Cihang Xie, Huiyu Wang, Jinghao Zhou, Tao Kong, Wei Shen |  |
| 766 |  |  [Accelerated Policy Learning with Parallel Differentiable Simulation](https://openreview.net/forum?id=ZSKRQMvttc) |  | 0 | Deep reinforcement learning can generate complex control policies, but requires large amounts of training data to work effectively. Recent work has attempted to address this issue by leveraging differentiable simulators. However, inherent problems such as local minima and exploding/vanishing numerical gradients prevent these methods from being generally applied to... | Animesh Garg, Fabio Ramos, Jie Xu, Miles Macklin, Viktor Makoviychuk, Wojciech Matusik, Yashraj Narang |  |
| 767 |  |  [Do We Need Anisotropic Graph Neural Networks?](https://openreview.net/forum?id=hl9ePdHO4_s) |  | 0 | Common wisdom in the graph neural network (GNN) community dictates that anisotropic models---in which messages sent between nodes are a function of both the source and target node---are required to achieve state-of-the-art performance. Benchmarks to date have demonstrated that these models perform better than comparable isotropic models---where messages are a function... | Felix L. Opolka, Nicholas Donald Lane, Pietro Liò, Shyam A. Tailor |  |
| 768 |  |  [Is High Variance Unavoidable in RL? A Case Study in Continuous Control](https://openreview.net/forum?id=9xhgmsNVHu) |  | 0 | Reinforcement learning (RL) experiments have notoriously high variance, and minor details can have disproportionately large effects on measured outcomes. This is problematic for creating reproducible research and also serves as an obstacle when applying RL to sensitive real-world applications. In this paper, we investigate causes for this perceived instability. To allow... | Carla P. Gomes, Johan Bjorck, Kilian Q. Weinberger |  |
| 769 |  |  [Simple GNN Regularisation for 3D Molecular Property Prediction and Beyond](https://openreview.net/forum?id=1wVvweK3oIb) |  | 0 | In this paper we show that simple noisy regularisation can be an effective way to address oversmoothing. We first argue that regularisers ad-dressing oversmoothing should both penalise node latent similarity and encourage meaningful node representations. From this observation we derive “Noisy Nodes”,a simple technique in which we corrupt the input graph with noise, and... | Alexander L. Gaunt, Alvaro SanchezGonzalez, James Kirkpatrick, Jonathan Godwin, Michael Schaarschmidt, Petar Velickovic, Peter W. Battaglia, Yulia Rubanova |  |
| 770 |  |  [Should We Be Pre-training? An Argument for End-task Aware Training as an Alternative](https://openreview.net/forum?id=2bO2x8NAIMB) |  | 0 | In most settings of practical concern, machine learning practitioners know in advance what end-task they wish to boost with auxiliary tasks. However, widely used methods for leveraging auxiliary data like pre-training and its continued-pretraining variant are end-task agnostic: they rarely, if ever, exploit knowledge of the target task. We study replacing end-task... | Ameet Talwalkar, Graham Neubig, Lucio M. Dery, Paul Michel |  |
| 771 |  |  [Learning Super-Features for Image Retrieval](https://openreview.net/forum?id=wogsFPHwftY) |  | 0 | Methods that combine local and global features have recently shown excellent performance on multiple challenging deep image retrieval benchmarks, but their use of local features raises at least two issues. First, these local features simply boil down to the localized map activations of a neural network, and hence can be extremely redundant. Second, they are typically... | Diane Larlus, Philippe Weinzaepfel, Thomas Lucas, Yannis Kalantidis |  |
| 772 |  |  [Online Facility Location with Predictions](https://openreview.net/forum?id=DSQHjibtgKR) |  | 0 | We provide nearly optimal algorithms for online facility location (OFL) with predictions. In OFL, $n$ demand points arrive in order and the algorithm must irrevocably assign each demand point to an open facility upon its arrival. The objective is to minimize the total connection costs from demand points to assigned facilities plus the facility opening cost. We further... | Erzhi Liu, Shaofeng H.C. Jiang, You Lyu, Yubo Zhang, Zhihao Gavin Tang |  |
| 773 |  |  [Few-Shot Backdoor Attacks on Visual Object Tracking](https://openreview.net/forum?id=qSV5CuSaK_a) |  | 0 | Visual object tracking (VOT) has been widely adopted in mission-critical applications, such as autonomous driving and intelligent surveillance systems. In current practice, third-party resources such as datasets, backbone networks, and training platforms are frequently used to train high-performance VOT models. Whilst these resources bring certain convenience, they also... | Haoxiang Zhong, ShuTao Xia, Xingjun Ma, Yiming Li, Yong Jiang |  |
| 774 |  |  [Backdoor Defense via Decoupling the Training Process](https://openreview.net/forum?id=TySnJ-0RdKI) |  | 0 | Recent studies have revealed that deep neural networks (DNNs) are vulnerable to backdoor attacks, where attackers embed hidden backdoors in the DNN model by poisoning a few training samples. The attacked model behaves normally on benign samples, whereas its prediction will be maliciously changed when the backdoor is activated. We reveal that poisoned samples tend to... | Baoyuan Wu, Kui Ren, Kunzhe Huang, Yiming Li, Zhan Qin |  |
| 775 |  |  [Learning to Complete Code with Sketches](https://openreview.net/forum?id=q79uMSC6ZBT) |  | 0 | Code completion is usually cast as a language modelling problem, i.e., continuing an input in a left-to-right fashion. However, in practice, some parts of the completion (e.g., string literals) may be very hard to predict, whereas subsequent parts directly follow from the context. To handle this, we instead consider the scenario of generating code completions with... | Alexey Svyatkovskiy, Daya Guo, Jian Yin, Marc Brockschmidt, Miltiadis Allamanis, Nan Duan |  |
| 776 |  |  [Reverse Engineering of Imperceptible Adversarial Image Perturbations](https://openreview.net/forum?id=gpp7cf0xdfN) |  | 0 | It has been well recognized that neural network based image classifiers are easily fooled by images with tiny perturbations crafted by an adversary. There has been a vast volume of research to generate and defend such adversarial attacks. However, the following problem is left unexplored: How to reverse-engineer adversarial perturbations from an adversarial image? This... | Sijia Liu, Xiaoming Liu, Xue Lin, Yifan Gong, Yimeng Zhang, Yize Li, Yuguang Yao |  |
| 777 |  |  [DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR](https://openreview.net/forum?id=oMI9PjOb9Jl) |  | 0 | We present in this paper a novel query formulation using dynamic anchor boxes for DETR (DEtection TRansformer) and offer a deeper understanding of the role of queries in DETR. This new formulation directly uses box coordinates as queries in Transformer decoders and dynamically updates them layer by layer. Using box coordinates not only helps using explicit positional... | Feng Li, Hang Su, Hao Zhang, Jun Zhu, Lei Zhang, Shilong Liu, Xianbiao Qi, Xiao Yang |  |
| 778 |  |  [On the Certified Robustness for Ensemble Models and Beyond](https://openreview.net/forum?id=tUa4REjGjTf) |  | 0 | Recent studies show that deep neural networks (DNN) are vulnerable to adversarial examples, which aim to mislead DNNs by adding perturbations with small magnitude. To defend against such attacks, both empirical and theoretical defense approaches have been extensively studied for a single ML model. In this work, we aim to analyze and provide the certified robustness for... | Bhavya Kailkhura, Bo Li, Linyi Li, Tao Xie, Xiaojun Xu, Zhuolin Yang |  |
| 779 |  |  [Efficient Neural Causal Discovery without Acyclicity Constraints](https://openreview.net/forum?id=eYciPrLuUhG) |  | 0 | Learning the structure of a causal graphical model using both observational and interventional data is a fundamental problem in many scientific fields. A promising direction is continuous optimization for score-based methods, which, however, require constrained optimization to enforce acyclicity or lack convergence guarantees. In this paper, we present ENCO, an... | Efstratios Gavves, Phillip Lippe, Taco Cohen |  |
| 780 |  |  [Pseudo-Labeled Auto-Curriculum Learning for Semi-Supervised Keypoint Localization](https://openreview.net/forum?id=6Q52pZ-Th7N) |  | 0 | Localizing keypoints of an object is a basic visual problem. However, supervised learning of a keypoint localization network often requires a large amount of data, which is expensive and time-consuming to obtain. To remedy this, there is an ever-growing interest in semi-supervised learning (SSL), which leverages a small set of labeled data along with a large set of... | Can Wang, Chen Qian, Ping Luo, Sheng Jin, Wanli Ouyang, Wentao Liu, Yingda Guan |  |
| 781 |  |  [Signing the Supermask: Keep, Hide, Invert](https://openreview.net/forum?id=e0jtGTfPihs) |  | 0 | The exponential growth in numbers of parameters of neural networks over the past years has been accompanied by an increase in performance across several fields. However, due to their sheer size, the networks not only became difficult to interpret but also problematic to train and use in real-world applications, since hardware requirements increased accordingly. Tackling... | Achim Rettinger, Nils Koster, Oliver Grothe |  |
| 782 |  |  [Bootstrapping Semantic Segmentation with Regional Contrast](https://openreview.net/forum?id=6u6N8WWwYSM) |  | 0 | We present ReCo, a contrastive learning framework designed at a regional level to assist learning in semantic segmentation. ReCo performs pixel-level contrastive learning on a sparse set of hard negative pixels, with minimal additional memory footprint. ReCo is easy to implement, being built on top of off-the-shelf segmentation networks, and consistently improves... | Andrew J. Davison, Edward Johns, Shikun Liu, Shuaifeng Zhi |  |
| 783 |  |  [Generative Principal Component Analysis](https://openreview.net/forum?id=pgir5f7ekAL) |  | 0 | In this paper, we study the problem of principal component analysis with generative modeling assumptions, adopting a general model for the observed matrix that encompasses notable special cases, including spiked matrix recovery and phase retrieval. The key assumption is that the first principal eigenvector lies near the range of an $L$-Lipschitz continuous generative... | Jiulong Liu, Jonathan Scarlett, Jun Han, Subhroshekhar Ghosh, Zhaoqiang Liu |  |
| 784 |  |  [Pareto Policy Pool for Model-based Offline Reinforcement Learning](https://openreview.net/forum?id=OqcZu8JIIzS) |  | 0 | Online reinforcement learning (RL) can suffer from poor exploration, sparse reward, insufficient data, and overhead caused by inefficient interactions between an immature policy and a complicated environment. Model-based offline RL instead trains an environment model using a dataset of pre-collected experiences so online RL methods can learn in an offline manner by... | Jie Ma, Jing Jiang, Tianyi Zhou, Yijun Yang, Yuhui Shi |  |
| 785 |  |  [Filling the G_ap_s: Multivariate Time Series Imputation by Graph Neural Networks](https://openreview.net/forum?id=kOu3-S3wJ7) |  | 0 | Dealing with missing values and incomplete time series is a labor-intensive, tedious, inevitable task when handling data coming from real-world applications. Effective spatio-temporal representations would allow imputation methods to reconstruct missing temporal data by exploiting information coming from sensors at different locations. However, standard methods fall... | Andrea Cini, Cesare Alippi, Ivan Marisca |  |
| 786 |  |  [An Unconstrained Layer-Peeled Perspective on Neural Collapse](https://openreview.net/forum?id=WZ3yjh8coDg) |  | 0 | Neural collapse is a highly symmetric geometry of neural networks that emerges during the terminal phase of training, with profound implications on the generalization performance and robustness of the trained networks. To understand how the last-layer features and classifiers exhibit this recently discovered implicit bias, in this paper, we introduce a surrogate model... | Weijie J. Su, Wenlong Ji, Yiliang Zhang, Yiping Lu, Zhun Deng |  |
| 787 |  |  [Contrastive Clustering to Mine Pseudo Parallel Data for Unsupervised Translation](https://openreview.net/forum?id=pN1JOdrSY9) |  | 0 | Modern unsupervised machine translation systems mostly train their models by generating synthetic parallel training data from large unlabeled monolingual corpora of different languages through various means, such as iterative back-translation. However, there may exist small amount of actual parallel data hidden in the sea of unlabeled data, which has not been exploited.... | Changhan Wang, Hongyu Gong, Philipp Koehn, Shafiq R. Joty, XuanPhi Nguyen, Yun Tang |  |
| 788 |  |  [Multimeasurement Generative Models](https://openreview.net/forum?id=QRX0nCX_gk) |  | 0 | We formally map the problem of sampling from an unknown distribution with a density in $\mathbb{R}^d$ to the problem of learning and sampling a smoother density in $\mathbb{R}^{Md}$ obtained by convolution with a fixed factorial kernel: the new density is referred to as M-density and the kernel as multimeasurement noise model (MNM). The M-density in $\mathbb{R}^{Md}$ is... | Rupesh Kumar Srivastava, Saeed Saremi |  |
| 789 |  |  [Information Gain Propagation: a New Way to Graph Active Learning with Soft Labels](https://openreview.net/forum?id=USC0-nvGPK) |  | 0 | Graph Neural Networks (GNNs) have achieved great success in various tasks, but their performance highly relies on a large number of labeled nodes, which typically requires considerable human effort. GNN-based Active Learning (AL) methods are proposed to improve the labeling efficiency by selecting the most valuable nodes to label. Existing methods assume an oracle can... | Bin Cui, Jiulong Shan, Meng Cao, Ping Huang, Wentao Zhang, Yexin Wang, Zhenbang You, Zhi Yang |  |
| 790 |  |  [Constructing Orthogonal Convolutions in an Explicit Manner](https://openreview.net/forum?id=Zr5W2LSRhD) |  | 0 | Convolutions with orthogonal input-output Jacobian matrix, i.e., orthogonal convolution, have recently attracted substantial attention. A convolution layer with an orthogonal Jacobian matrix is 1-Lipschitz in the 2-norm, making the output robust to the perturbation in input. Meanwhile, an orthogonal Jacobian matrix preserves the gradient norm in back-propagation, which... | Jun Li, Ping Li, Tan Yu, Yunfeng Cai |  |
| 791 |  |  [X-model: Improving Data Efficiency in Deep Learning with A Minimax Model](https://openreview.net/forum?id=P3Bh01hBYTH) |  | 0 | To mitigate the burden of data labeling, we aim at improving data efficiency for both classification and regression setups in deep learning. However, the current focus is on classification problems while rare attention has been paid to deep regression, which usually requires more human effort to labeling. Further, due to the intrinsic difference between categorical and... | Jianmin Wang, Mingsheng Long, Ximei Wang, Xinyang Chen |  |
| 792 |  |  [Stein Latent Optimization for Generative Adversarial Networks](https://openreview.net/forum?id=2-mkiUs9Jx7) |  | 0 | Generative adversarial networks (GANs) with clustered latent spaces can perform conditional generation in a completely unsupervised manner. In the real world, the salient attributes of unlabeled data can be imbalanced. However, most of existing unsupervised conditional GANs cannot cluster attributes of these data in their latent spaces properly because they assume... | Dahuin Jung, Heeseung Kim, Hyemi Jang, Hyungyu Lee, Sungroh Yoon, Uiwon Hwang |  |
| 793 |  |  [Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity](https://openreview.net/forum?id=RRGVCN8kjim) |  | 0 | DETR is the first end-to-end object detector using a transformer encoder-decoder architecture and demonstrates competitive performance but low computational efficiency. The subsequent work, Deformable DETR, enhances the efficiency of DETR by replacing dense attention with deformable attention, which achieves 10x faster convergence and improved performance. Using the... | Byungseok Roh, Jaewoong Shin, Saehoon Kim, Wuhyun Shin |  |
| 794 |  |  [Online Target Q-learning with Reverse Experience Replay: Efficiently finding the Optimal Policy for Linear MDPs](https://openreview.net/forum?id=HMJdXzbWKH) |  | 0 | Q-learning is a popular Reinforcement Learning (RL) algorithm which is widely used in practice with function approximation (Mnih et al., 2015). In contrast, existing theoretical results are pessimistic about Q-learning. For example, (Baird, 1995) shows that Q-learning does not converge even with linear function approximation for linear MDPs. Furthermore, even for... | Dheeraj Mysore Nagaraj, Naman Agarwal, Praneeth Netrapalli, Prateek Jain, Syomantak Chaudhuri |  |
| 795 |  |  [Differentially Private Fractional Frequency Moments Estimation with Polylogarithmic Space](https://openreview.net/forum?id=7I8LPkcx8V) |  | 0 | We prove that $\mathbb{F}_p$ sketch, a well-celebrated streaming algorithm for frequency moments estimation, is differentially private as is when $p\in(0, 1]$. $\mathbb{F}_p$ sketch uses only polylogarithmic space, exponentially better than existing DP baselines and only worse than the optimal non-private baseline by a logarithmic factor. The evaluation shows that... | Dawn Song, Iosif Pinelis, Lun Wang |  |
| 796 |  |  [How Low Can We Go: Trading Memory for Error in Low-Precision Training](https://openreview.net/forum?id=YpSxqy_RE84) |  | 0 | Low-precision arithmetic trains deep learning models using less energy, less memory and less time. However, we pay a price for the savings: lower precision may yield larger round-off error and hence larger prediction error. As applications proliferate, users must choose which precision to use to train a new model, and chip manufacturers must decide which precisions to... | Chengrun Yang, Christopher De Sa, Jerry Chee, Madeleine Udell, Ziyang Wu |  |
| 797 |  |  [In a Nutshell, the Human Asked for This: Latent Goals for Following Temporal Specifications](https://openreview.net/forum?id=rUwm9wCjURV) |  | 0 | We address the problem of building agents whose goal is to learn to execute out-of distribution (OOD) multi-task instructions expressed in temporal logic (TL) by using deep reinforcement learning (DRL). Recent works provided evidence that the agent's neural architecture is a key feature when DRL agents are learning to solve OOD tasks in TL. Yet, the studies on this... | Borja G. León, Francesco Belardinelli, Murray Shanahan |  |
| 798 |  |  [Discrete Representations Strengthen Vision Transformer Robustness](https://openreview.net/forum?id=8hWs60AZcWk) |  | 0 | Vision Transformer (ViT) is emerging as the state-of-the-art architecture for image recognition. While recent studies suggest that ViTs are more robust than their convolutional counterparts, our experiments find that ViTs are overly reliant on local features (\eg, nuisances and texture) and fail to make adequate use of global context (\eg, shape and structure). As a... | Carl Vondrick, Chengzhi Mao, Irfan Essa, Lu Jiang, Mostafa Dehghani, Rahul Sukthankar |  |
| 799 |  |  [On the Convergence of the Monte Carlo Exploring Starts Algorithm for Reinforcement Learning](https://openreview.net/forum?id=JzNB0eA2-M4) |  | 0 | A simple and natural algorithm for reinforcement learning (RL) is Monte Carlo Exploring Starts (MCES), where the Q-function is estimated by averaging the Monte Carlo returns, and the policy is improved by choosing actions that maximize the current estimate of the Q-function. Exploration is performed by "exploring starts", that is, each episode begins with a randomly... | Che Wang, Kai Shao, Keith W. Ross, Shuhan Yuan |  |
| 800 |  |  [Concurrent Adversarial Learning for Large-Batch Training](https://openreview.net/forum?id=rw1mZl_ss3L) |  | 0 | Large-batch training has become a commonly used technique when training neural networks with a large number of GPU/TPU processors. As batch size increases, stochastic optimizers tend to converge to sharp local minima, leading to degraded test performance. Current methods usually use extensive data augmentation to increase the batch size, but we found the performance... | ChoJui Hsieh, Minhao Cheng, Xiangning Chen, Yang You, Yong Liu |  |
| 801 |  |  [Multiset-Equivariant Set Prediction with Approximate Implicit Differentiation](https://openreview.net/forum?id=5K7RRqZEjoS) |  | 0 | Most set prediction models in deep learning use set-equivariant operations, but they actually operate on multisets. We show that set-equivariant functions cannot represent certain functions on multisets, so we introduce the more appropriate notion of multiset-equivariance. We identify that the existing Deep Set Prediction Network (DSPN) can be multiset-equivariant... | Cees G. M. Snoek, David W. Zhang, Gertjan J. Burghouts, Simon LacosteJulien, Yan Zhang |  |
| 802 |  |  [Learned Simulators for Turbulence](https://openreview.net/forum?id=msRBojTz-Nh) |  | 0 | Turbulence simulation with classical numerical solvers requires high-resolution grids to accurately resolve dynamics. Here we train learned simulators at low spatial and temporal resolutions to capture turbulent dynamics generated at high resolution. We show that our proposed model can simulate turbulent dynamics more accurately than classical numerical solvers at the... | Alvaro SanchezGonzalez, Can Cui, Dmitrii Kochkov, Drummond Buschman Fielding, Jonathan Godwin, Kimberly L. Stachenfeld, Miles D. Cranmer, Peter W. Battaglia, Shirley Ho, Tobias Pfaff |  |
| 803 |  |  [Modular Lifelong Reinforcement Learning via Neural Composition](https://openreview.net/forum?id=5XmLzdslFNN) |  | 0 | Humans commonly solve complex problems by decomposing them into easier subproblems and then combining the subproblem solutions. This type of compositional reasoning permits reuse of the subproblem solutions when tackling future tasks that share part of the underlying compositional structure. In a continual or lifelong reinforcement learning (RL) setting, this ability to... | Eric Eaton, Harm van Seijen, Jorge A. Mendez |  |
| 804 |  |  [Optimal ANN-SNN Conversion for High-accuracy and Ultra-low-latency Spiking Neural Networks](https://openreview.net/forum?id=7B3IJMM1k_M) |  | 0 | Spiking Neural Networks (SNNs) have gained great attraction due to their distinctive properties of low power consumption and fast inference on neuromorphic hardware. As the most effective method to get deep SNNs, ANN-SNN conversion has achieved comparable performance as ANNs on large-scale datasets. Despite this, it requires long time-steps to match the firing rates of... | Jianhao Ding, Penglin Dai, Tiejun Huang, Tong Bu, Wei Fang, Zhaofei Yu |  |
| 805 |  |  [AS-MLP: An Axial Shifted MLP Architecture for Vision](https://openreview.net/forum?id=fvLLcIYmXb) |  | 0 | An Axial Shifted MLP architecture (AS-MLP) is proposed in this paper. Different from MLP-Mixer, where the global spatial feature is encoded for information flow through matrix transposition and one token-mixing MLP, we pay more attention to the local features interaction. By axially shifting channels of the feature map, AS-MLP is able to obtain the information flow from... | Dongze Lian, Shenghua Gao, Xing Sun, Zehao Yu |  |
| 806 |  |  [Online Continual Learning on Class Incremental Blurry Task Configuration with Anytime Inference](https://openreview.net/forum?id=nrGGfMbY_qK) |  | 0 | Despite rapid advances in continual learning, a large body of research is devoted to improving performance in the existing setups. While a handful of work do propose new continual learning setups, they still lack practicality in certain aspects. For better practicality, we first propose a novel continual learning setup that is online, task-free, class-incremental, of... | Dahyun Kim, Hyunseo Koh, Jonghyun Choi, JungWoo Ha |  |
| 807 |  |  [Learning with Noisy Labels Revisited: A Study Using Real-World Human Annotations](https://openreview.net/forum?id=TBWA6PLJZQm) |  | 0 | Existing research on learning with noisy labels mainly focuses on synthetic label noise. The synthetic noise, though has clean structures which greatly enabled statistical analyses, often fails to model the real-world noise patterns. The recent literature has observed several efforts to offer real-world noisy datasets, e.g., Food-101N, WebVision, and Clothing1M. Yet the... | Gang Niu, Hao Cheng, Jiaheng Wei, Tongliang Liu, Yang Liu, Zhaowei Zhu |  |
| 808 |  |  [Optimization inspired Multi-Branch Equilibrium Models](https://openreview.net/forum?id=nbC8iTTXIrk) |  | 0 | Works have shown the strong connections between some implicit models and optimization problems. However, explorations on such relationships are limited. Most works pay attention to some common mathematical properties, such as sparsity. In this work, we propose a new type of implicit model inspired by the designing of the systems' hidden objective functions, called the... | Mingjie Li, Xingyu Xie, Yisen Wang, Zhouchen Lin |  |
| 809 |  |  [Learning to Annotate Part Segmentation with Gradient Matching](https://openreview.net/forum?id=zNR43c03lRy) |  | 0 | The success of state-of-the-art deep neural networks heavily relies on the presence of large-scale labelled datasets, which are extremely expensive and time-consuming to annotate. This paper focuses on tackling semi-supervised part segmentation tasks by generating high-quality images with a pre-trained GAN and labelling the generated images with an automatic annotator.... | Hakan Bilen, Xiangyang Ji, Xiaotian Cheng, Yu Yang |  |
| 810 |  |  [Vector-quantized Image Modeling with Improved VQGAN](https://openreview.net/forum?id=pfNyExj7z2) |  | 0 | Pretraining language models with next-token prediction on massive text corpora has delivered phenomenal zero-shot, few-shot, transfer learning and multi-tasking capabilities on both generative and discriminative language tasks. Motivated by this success, we explore a Vector-quantized Image Modeling (VIM) approach that involves pretraining a Transformer to predict... | Alexander Ku, Han Zhang, James Qin, Jason Baldridge, Jiahui Yu, Jing Yu Koh, Ruoming Pang, Xin Li, Yonghui Wu, Yuanzhong Xu |  |
| 811 |  |  [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://openreview.net/forum?id=R8sQPpGCv0) |  | 0 | Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current... | Mike Lewis, Noah A. Smith, Ofir Press |  |
| 812 |  |  [Learning Representation from Neural Fisher Kernel with Low-rank Approximation](https://openreview.net/forum?id=J1rhANsCY9) |  | 0 | In this paper, we study the representation of neural networks from the view of kernels. We first define the Neural Fisher Kernel (NFK), which is the Fisher Kernel applied to neural networks. We show that NFK can be computed for both supervised and unsupervised learning models, which can serve as a unified tool for representation extraction. Furthermore, we show that... | Etai Littwin, Joshua M. Susskind, Ruixiang Zhang, Shuangfei Zhai |  |
| 813 |  |  [Learning Temporally Causal Latent Processes from General Temporal Data](https://openreview.net/forum?id=RDlLMjLJXdq) |  | 0 | Our goal is to recover time-delayed latent causal variables and identify their relations from measured temporal data. Estimating causally-related latent variables from observations is particularly challenging as the latent variables are not uniquely recoverable in the most general case. In this work, we consider both a nonparametric, nonstationary setting and a... | Alex Ho, Changyin Sun, Kun Zhang, Weiran Yao, Yuewen Sun |  |
| 814 |  |  [The Rich Get Richer: Disparate Impact of Semi-Supervised Learning](https://openreview.net/forum?id=DXPftn5kjQK) |  | 0 | Semi-supervised learning (SSL) has demonstrated its potential to improve the model accuracy for a variety of learning tasks when the high-quality supervised data is severely limited. Although it is often established that the average accuracy for the entire population of data is improved, it is unclear how SSL fares with different sub-populations. Understanding the above... | Tianyi Luo, Yang Liu, Zhaowei Zhu |  |
| 815 |  |  [Neural Relational Inference with Node-Specific Information](https://openreview.net/forum?id=HBsJNesj2S) |  | 0 | Inferring interactions among entities is an important problem in studying dynamical systems, which greatly impacts the performance of downstream tasks, such as prediction. In this paper, we tackle the relational inference problem in a setting where each entity can potentially have a set of individualized information that other entities cannot have access to.... | Ershad Banijamali |  |
| 816 |  |  [Bregman Gradient Policy Optimization](https://openreview.net/forum?id=ZU-zFnTum1N) |  | 0 | In the paper, we design a novel Bregman gradient policy optimization framework for reinforcement learning based on Bregman divergences and momentum techniques. Specifically, we propose a Bregman gradient policy optimization (BGPO) algorithm based on the basic momentum technique and mirror descent iteration. Meanwhile, we further propose an accelerated Bregman gradient... | Feihu Huang, Heng Huang, Shangqian Gao |  |
| 817 |  |  [A generalization of the randomized singular value decomposition](https://openreview.net/forum?id=hgKtwSb4S2) |  | 0 | The randomized singular value decomposition (SVD) is a popular and effective algorithm for computing a near-best rank $k$ approximation of a matrix $A$ using matrix-vector products with standard Gaussian vectors. Here, we generalize the theory of randomized SVD to multivariate Gaussian vectors, allowing one to incorporate prior knowledge of $A$ into the algorithm. This... | Alex Townsend, Nicolas Boullé |  |
| 818 |  |  [Dropout Q-Functions for Doubly Efficient Reinforcement Learning](https://openreview.net/forum?id=xCVJMsPv3RT) |  | 0 | Randomized ensembled double Q-learning (REDQ) (Chen et al., 2021b) has recently achieved state-of-the-art sample efficiency on continuous-action reinforcement learning benchmarks. This superior sample efficiency is made possible by using a large Q-function ensemble. However, REDQ is much less computationally efficient than non-ensemble counterparts such as Soft... | Taisei Hashimoto, Takahisa Imagawa, Takashi Onishi, Takuya Hiraoka, Yoshimasa Tsuruoka |  |
| 819 |  |  [QDrop: Randomly Dropping Quantization for Extremely Low-bit Post-Training Quantization](https://openreview.net/forum?id=ySQH0oDyp7) |  | 0 | Recently, post-training quantization (PTQ) has driven much attention to produce efficient neural networks without long-time retraining. Despite the low cost, current PTQ works always fail under the extremely low-bit setting. In this study, we pioneeringly confirm that properly incorporating activation quantization into the PTQ reconstruction benefits the final accuracy.... | Fengwei Yu, Ruihao Gong, Xianglong Liu, Xiuying Wei, Yuhang Li |  |
| 820 |  |  [You Mostly Walk Alone: Analyzing Feature Attribution in Trajectory Prediction](https://openreview.net/forum?id=POxF-LEqnF) |  | 0 | Predicting the future trajectory of a moving agent can be easy when the past trajectory continues smoothly but is challenging when complex interactions with other agents are involved. Recent deep learning approaches for trajectory prediction show promising performance and partially attribute this to successful reasoning about agent-agent interactions. However, it... | Bernhard Schölkopf, Dominik Janzing, Francesco Locatello, Julius von Kügelgen, Osama Makansi, Peter Vincent Gehler, Thomas Brox |  |
| 821 |  |  [Rethinking Class-Prior Estimation for Positive-Unlabeled Learning](https://openreview.net/forum?id=aYAA-XHKyk) |  | 0 | Given only positive (P) and unlabeled (U) data, PU learning can train a binary classifier without any negative data. It has two building blocks: PU class-prior estimation (CPE) and PU classification; the latter has been well studied while the former has received less attention. Hitherto, the distributional-assumption-free CPE methods rely on a critical assumption that... | Bo Han, Dacheng Tao, Gang Niu, Masashi Sugiyama, Mingming Gong, Tongliang Liu, Yu Yao |  |
| 822 |  |  [Learning Efficient Online 3D Bin Packing on Packing Configuration Trees](https://openreview.net/forum?id=bfuGjlCwAq) |  | 0 | Online 3D Bin Packing Problem (3D-BPP) has widespread applications in industrial automation and has aroused enthusiastic research interest recently. Existing methods usually solve the problem with limited resolution of spatial discretization, and/or cannot deal with complex practical constraints well. We propose to enhance the practical applicability of online 3D-BPP... | Hang Zhao, Kai Xu, Yang Yu |  |
| 823 |  |  [Uncertainty Modeling for Out-of-Distribution Generalization](https://openreview.net/forum?id=6HN7LHyzGgC) |  | 0 | Though remarkable progress has been achieved in various vision tasks, deep neural networks still suffer obvious performance degradation when tested in out-of-distribution scenarios. We argue that the feature statistics (mean and standard deviation), which carry the domain characteristics of the training data, can be properly manipulated to improve the generalization... | Jun Liu, Lingyu Duan, Xiaotong Li, Ying Shan, Yixiao Ge, Yongxing Dai |  |
| 824 |  |  [Online Adversarial Attacks](https://openreview.net/forum?id=bYGSzbCM_i) |  | 0 | Adversarial attacks expose important vulnerabilities of deep learning models, yet little attention has been paid to settings where data arrives as a stream. In this paper, we formalize the online adversarial attack problem, emphasizing two key elements found in real-world use-cases: attackers must operate under partial knowledge of the target model, and the decisions... | Andjela Mladenovic, Avishek Joey Bose, Gauthier Gidel, Hugo Berard, Pascal Vincent, Simon LacosteJulien, William L. Hamilton |  |
| 825 |  |  [Anytime Dense Prediction with Confidence Adaptivity](https://openreview.net/forum?id=kNKFOXleuC) |  | 0 | Anytime inference requires a model to make a progression of predictions which might be halted at any time. Prior research on anytime visual recognition has mostly focused on image classification.We propose the first unified and end-to-end approach for anytime dense prediction. A cascade of "exits" is attached to the model to make multiple predictions. We redesign the... | Evan Shelhamer, HungJu Wang, Trevor Darrell, Zhiqiu Xu, Zhuang Liu |  |
| 826 |  |  [Declarative nets that are equilibrium models](https://openreview.net/forum?id=q4HaTeMO--y) |  | 0 | Implicit layers are computational modules that output the solution to some problem depending on the input and the layer parameters. Deep equilibrium models (DEQs) output a solution to a fixed point equation. Deep declarative networks (DDNs) solve an optimisation problem in their forward pass, an arguably more intuitive, interpretable problem than finding a fixed point.... | Cheng Soon Ong, Lars Petersson, Mohammad Ali Armin, Russell Tsuchida, Suk Yee Yong |  |
| 827 |  |  [A Reduction-Based Framework for Conservative Bandits and Reinforcement Learning](https://openreview.net/forum?id=AcrlgZ9BKed) |  | 0 | We study bandits and reinforcement learning (RL) subject to a conservative constraint where the agent is asked to perform at least as well as a given baseline policy. This setting is particular relevant in real-world domains including digital marketing, healthcare, production, finance, etc. In this paper, we present a reduction-based framework for conservative bandits... | Alessandro Lazaric, Evrard Garcelon, Han Zhong, Liwei Wang, Matteo Pirotta, Simon Shaolei Du, Tianhao Wu, Yunchang Yang |  |
| 828 |  |  [Wisdom of Committees: An Overlooked Approach To Faster and More Accurate Models](https://openreview.net/forum?id=MvO2t0vbs4-) |  | 0 | Committee-based models (ensembles or cascades) construct models by combining existing pre-trained ones. While ensembles and cascades are well-known techniques that were proposed before deep learning, they are not considered a core building block of deep model architectures and are rarely compared to in recent literature on developing efficient models. In this work, we... | Dan Kondratyuk, Elad Eban, Eric Christiansen, Kris M. Kitani, Xiaofang Wang, Yair MovshovitzAttias |  |
| 829 |  |  [Unsupervised Discovery of Object Radiance Fields](https://openreview.net/forum?id=rwE8SshAlxw) |  | 0 | We study the problem of inferring an object-centric scene representation from a single image, aiming to derive a representation that explains the image formation process, captures the scene's 3D nature, and is learned without supervision. Most existing methods on scene decomposition lack one or more of these characteristics, due to the fundamental challenge in... | HongXing Yu, Jiajun Wu, Leonidas J. Guibas |  |
| 830 |  |  [Gradient Step Denoiser for convergent Plug-and-Play](https://openreview.net/forum?id=fPhKeld3Okz) |  | 0 | Plug-and-Play methods constitute a class of iterative algorithms for imaging problems where regularization is performed by an off-the-shelf denoiser. Although Plug-and-Play methods can lead to tremendous visual performance for various image problems, the few existing convergence guarantees are based on unrealistic (or suboptimal) hypotheses on the denoiser, or limited... | Arthur Leclaire, Nicolas Papadakis, Samuel Hurault |  |
| 831 |  |  [Surrogate Gap Minimization Improves Sharpness-Aware Training](https://openreview.net/forum?id=edONMAnhLu-) |  | 0 | The recently proposed Sharpness-Aware Minimization (SAM) improves generalization by minimizing a perturbed loss defined as the maximum loss within a neighborhood in the parameter space. However, we show that both sharp and flat minima can have a low perturbed loss, implying that SAM does not always prefer flat minima. Instead, we define a surrogate gap, a measure... | Boqing Gong, Hartwig Adam, James S. Duncan, Juntang Zhuang, Liangzhe Yuan, Nicha C. Dvornek, Sekhar Tatikonda, Ting Liu, Yin Cui |  |
| 832 |  |  [R4D: Utilizing Reference Objects for Long-Range Distance Estimation](https://openreview.net/forum?id=MQ2sAGunyBP) |  | 0 | Estimating the distance of objects is a safety-critical task for autonomous driving. Focusing on short-range objects, existing methods and datasets neglect the equally important long-range objects. In this paper, we introduce a challenging and under-explored task, which we refer to as Long-Range Distance Estimation, as well as two datasets to validate new methods... | Hang Zhao, Longlong Jing, Maya Kabkab, Ruichi Yu, Tiffany L. Chen, Yingwei Li, Yurong You |  |
| 833 |  |  [Understanding Dimensional Collapse in Contrastive Self-supervised Learning](https://openreview.net/forum?id=YevsQ05DEN7) |  | 0 | Self-supervised visual representation learning aims to learn useful representations without relying on human annotations. Joint embedding approach bases on maximizing the agreement between embedding vectors from different views of the same image. Various methods have been proposed to solve the collapsing problem where all embedding vectors collapse to a trivial constant... | Li Jing, Pascal Vincent, Yann LeCun, Yuandong Tian |  |
| 834 |  |  [FedPara: Low-rank Hadamard Product for Communication-Efficient Federated Learning](https://openreview.net/forum?id=d71n4ftoCBy) |  | 0 | In this work, we propose a communication-efficient parameterization, $\texttt{FedPara}$, for federated learning (FL) to overcome the burdens on frequent model uploads and downloads. Our method re-parameterizes weight parameters of layers using low-rank weights followed by the Hadamard product. Compared to the conventional low-rank parameterization, our... | Moon YeBin, Nam HyeonWoo, TaeHyun Oh |  |
| 835 |  |  [RegionViT: Regional-to-Local Attention for Vision Transformers](https://openreview.net/forum?id=T__V3uLix7V) |  | 0 | Vision transformer (ViT) has recently shown its strong capability in achieving comparable results to convolutional neural networks (CNNs) on image classification. However, vanilla ViT simply inherits the same architecture from the natural language processing directly, which is often not optimized for vision applications. Motivated by this, in this paper, we propose a... | ChunFu Chen, Quanfu Fan, Rameswar Panda |  |
| 836 |  |  [Quadtree Attention for Vision Transformers](https://openreview.net/forum?id=fR-EnKWL_Zb) |  | 0 | Transformers have been successful in many vision tasks, thanks to their capability of capturing long-range dependency. However, their quadratic computational complexity poses a major obstacle for applying them to vision tasks requiring dense predictions, such as object detection, feature matching, stereo, etc. We introduce QuadTree Attention, which reduces the... | Jiahui Zhang, Ping Tan, Shitao Tang, Siyu Zhu |  |
| 837 |  |  [Visual Correspondence Hallucination](https://openreview.net/forum?id=jaLDP8Hp_gc) |  | 0 | Given a pair of partially overlapping source and target images and a keypoint in the source image, the keypoint's correspondent in the target image can be either visible, occluded or outside the field of view. Local feature matching methods are only able to identify the correspondent's location when it is visible, while humans can also hallucinate its location when it... | Guillaume Bourmaud, Hugo Germain, Vincent Lepetit |  |
| 838 |  |  [What's Wrong with Deep Learning in Tree Search for Combinatorial Optimization](https://openreview.net/forum?id=mk0HzdqY7i1) |  | 0 | Combinatorial optimization lies at the core of many real-world problems. Especially since the rise of graph neural networks (GNNs), the deep learning community has been developing solvers that derive solutions to NP-hard problems by learning the problem-specific solution structure. However, reproducing the results of these publications proves to be difficult. We make... | Karen Seidel, Martin Taraz, Maximilian Böther, Otto Kißig, Sarel Cohen, Tobias Friedrich |  |
| 839 |  |  [Deep Attentive Variational Inference](https://openreview.net/forum?id=T4-65DNlDij) |  | 0 | Stochastic Variational Inference is a powerful framework for learning large-scale probabilistic latent variable models. However, typical assumptions on the factorization or independence of the latent variables can substantially restrict its capacity for inference and generative modeling. A major line of active research aims at building more expressive variational models... | Artur Dubrawski, Elan Rosenfeld, Ian Char, Ifigeneia Apostolopoulou |  |
| 840 |  |  [ARTEMIS: Attention-based Retrieval with Text-Explicit Matching and Implicit Similarity](https://openreview.net/forum?id=CVfLvQq9gLo) |  | 0 | An intuitive way to search for images is to use queries composed of an example image and a complementary text. While the first provides rich and implicit context for the search, the latter explicitly calls for new traits, or specifies how some elements of the example image should be changed to retrieve the desired target image. Current approaches typically combine the... | Diane Larlus, Gabriela Csurka, Ginger Delmas, Rafael Sampaio de Rezende |  |
| 841 |  |  [Trivial or Impossible --- dichotomous data difficulty masks model differences (on ImageNet and beyond)](https://openreview.net/forum?id=C_vsGwEIjAr) |  | 0 | "The power of a generalization system follows directly from its biases" (Mitchell 1980). Today, CNNs are incredibly powerful generalisation systems---but to what degree have we understood how their inductive bias influences model decisions? We here attempt to disentangle the various aspects that determine how a model decides. In particular, we ask: what makes one model... | Felix A. Wichmann, Kristof Meding, Luca M. Schulze Buschoff, Robert Geirhos |  |
| 842 |  |  [Group equivariant neural posterior estimation](https://openreview.net/forum?id=u6s8dSporO8) |  | 0 | Simulation-based inference with conditional neural density estimators is a powerful approach to solving inverse problems in science. However, these methods typically treat the underlying forward model as a black box, with no way to exploit geometric properties such as equivariances. Equivariances are common in scientific models, however integrating them directly into... | Bernhard Schölkopf, Jakob H. Macke, Jonathan Gair, Maximilian Dax, Michael Deistler, Stephen R. Green |  |
| 843 |  |  [Fast Differentiable Matrix Square Root](https://openreview.net/forum?id=-AOEi-5VTU8) |  | 0 | Computing the matrix square root or its inverse in a differentiable manner is important in a variety of computer vision tasks. Previous methods either adopt the Singular Value Decomposition (SVD) to explicitly factorize the matrix or use the Newton-Schulz iteration (NS iteration) to derive the approximate solution. However, both methods are not computationally efficient... | Nicu Sebe, Wei Wang, Yue Song |  |
| 844 |  |  [SQuant: On-the-Fly Data-Free Quantization via Diagonal Hessian Approximation](https://openreview.net/forum?id=JXhROKNZzOc) |  | 0 | Quantization of deep neural networks (DNN) has been proven effective for compressing and accelerating DNN models. Data-free quantization (DFQ) is a promising approach without the original datasets under privacy-sensitive and confidential scenarios. However, current DFQ solutions degrade accuracy, need synthetic data to calibrate networks, and are time-consuming and... | Chen Zhang, Cong Guo, Fan Yang, Jingwen Leng, Minyi Guo, Xiaotian Gao, Yuhao Zhu, Yunxin Liu, Yuxian Qiu |  |
| 845 |  |  [Neural Variational Dropout Processes](https://openreview.net/forum?id=lyLVzukXi08) |  | 0 | Learning to infer the conditional posterior model is a key step for robust meta-learning. This paper presents a new Bayesian meta-learning approach called Neural Variational Dropout Processes (NVDPs). NVDPs model the conditional posterior distribution based on a task-specific dropout; a low-rank product of Bernoulli experts meta-model is utilized for a memory-efficient... | Gunhee Kim, Insu Jeon, Youngjin Park |  |
| 846 |  |  [Towards Better Understanding and Better Generalization of Low-shot Classification in Histology Images with Contrastive Learning](https://openreview.net/forum?id=kQ2SOflIOVC) |  | 0 | Few-shot learning is an established topic in natural images for years, but few work is attended to histology images, which is of high clinical value since well-labeled datasets and rare abnormal samples are expensive to collect. Here, we facilitate the study of few-shot learning in histology images by setting up three cross-domain tasks that simulate real clinics... | Hanbo Chen, Jiangpeng Yan, Jianhua Yao, Jiawei Yang, Xiaoyu Chen |  |
| 847 |  |  [Distilling GANs with Style-Mixed Triplets for X2I Translation with Limited Data](https://openreview.net/forum?id=QjOQkpzKbNk) |  | 0 | Conditional image synthesis is an integral part of many X2I translation systems, including image-to-image, text-to-image and audio-to-image translation systems. Training these large systems generally requires huge amounts of training data. Therefore, we investigate knowledge distillation to transfer knowledge from a high-quality unconditioned generative model (e.g.,... | Joost van de Weijer, Lu Yu, Shangling Jui, Yaxing Wang |  |
| 848 |  |  [Handling Distribution Shifts on Graphs: An Invariance Perspective](https://openreview.net/forum?id=FQOC5u-1egI) |  | 0 | There is increasing evidence suggesting neural networks' sensitivity to distribution shifts, so that research on out-of-distribution (OOD) generalization comes into the spotlight. Nonetheless, current endeavors mostly focus on Euclidean data, and its formulation for graph-structured data is not clear and remains under-explored, given two-fold fundamental challenges: 1)... | David Wipf, Hengrui Zhang, Junchi Yan, Qitian Wu |  |
| 849 |  |  [Automatic Loss Function Search for Predict-Then-Optimize Problems with Strong Ranking Property](https://openreview.net/forum?id=hSktDu-h94) |  | 0 | Combinatorial optimization problems with parameters to be predicted from side information are commonly seen in a variety of problems during the paradigm shift from reactive decision making to proactive decision making. Due to the misalignment between the continuous prediction results and the discrete decisions in optimization problems, it is hard to achieve a... | Bo Qiao, Boshi Wang, Chuan Luo, Hang Dong, Jialin Yi, Qingwei Lin |  |
| 850 |  |  [Generalized Demographic Parity for Group Fairness](https://openreview.net/forum?id=YigKlMJwjye) |  | 0 | This work aims to generalize demographic parity to continuous sensitive attributes while preserving tractable computation. Current fairness metrics for continuous sensitive attributes largely rely on intractable statistical independence between variables, such as Hirschfeld-Gebelein-Renyi (HGR) and mutual information. Statistical fairness metrics estimation relying on... | Ali Mostafavi, Chao Fan, Fan Yang, Xia Hu, Xiaotian Han, Zhimeng Jiang |  |
| 851 |  |  [Closed-form Sample Probing for Learning Generative Models in Zero-shot Learning](https://openreview.net/forum?id=ljxWpdBl4V) |  | 0 | Generative model based approaches have led to significant advances in zero-shot learning (ZSL) over the past few years. These approaches typically aim to learn a conditional generator that synthesizes training samples of classes conditioned on class definitions. The final zero-shot learning model is then obtained by training a supervised classification model over the... | Orhun Bugra Baran, Ramazan Gokberk Cinbis, Samet Çetin |  |
| 852 |  |  [DKM: Differentiable k-Means Clustering Layer for Neural Network Compression](https://openreview.net/forum?id=J_F_qqCE3Z5) |  | 0 | Deep neural network (DNN) model compression for efficient on-device inference is becoming increasingly important to reduce memory requirements and keep user data on-device. To this end, we propose a novel differentiable k-means clustering layer (DKM) and its application to train-time weight clustering-based DNN model compression. DKM casts k-means clustering as an... | Keivan AlizadehVahid, Minsik Cho, Mohammad Rastegari, Saurabh Adya |  |
| 853 |  |  [Fixed Neural Network Steganography: Train the images, not the network](https://openreview.net/forum?id=hcMvApxGSzZ) |  | 0 | Recent attempts at image steganography make use of advances in deep learning to train an encoder-decoder network pair to hide and retrieve secret messages in images. These methods are able to hide large amounts of data, but they also incur high decoding error rates (around 20%). In this paper, we propose a novel algorithm for steganography that takes advantage of the... | Boyi Li, Kilian Q. Weinberger, Varsha Kishore, Xiangyu Chen, Yan Wang |  |
| 854 |  |  [Steerable Partial Differential Operators for Equivariant Neural Networks](https://openreview.net/forum?id=N9W24a4zU) |  | 0 | Recent work in equivariant deep learning bears strong similarities to physics. Fields over a base space are fundamental entities in both subjects, as are equivariant maps between these fields. In deep learning, however, these maps are usually defined by convolutions with a kernel, whereas they are partial differential operators (PDOs) in physics. Developing the theory... | Erik Jenner, Maurice Weiler |  |
| 855 |  |  [Divergence-aware Federated Self-Supervised Learning](https://openreview.net/forum?id=oVE1z8NlNe) |  | 0 | Self-supervised learning (SSL) is capable of learning remarkable representations from centrally available data. Recent works further implement federated learning with SSL to learn from rapidly growing decentralized unlabeled images (e.g., from cameras and phones), often resulted from privacy constraints. Extensive attention has been paid to SSL approaches based on... | Shuai Zhang, Weiming Zhuang, Yonggang Wen |  |
| 856 |  |  [Neural Spectral Marked Point Processes](https://openreview.net/forum?id=0rcbOaoBXbg) |  | 0 | Self- and mutually-exciting point processes are popular models in machine learning and statistics for dependent discrete event data. To date, most existing models assume stationary kernels (including the classical Hawkes processes) and simple parametric models. Modern applications with complex event data require more general point process models that can incorporate... | Haoyun Wang, Shixiang Zhu, Xiuyuan Cheng, Yao Xie, Zheng Dong |  |
| 857 |  |  [How to Inject Backdoors with Better Consistency: Logit Anchoring on Clean Data](https://openreview.net/forum?id=Bn09TnDngN) |  | 0 | Since training a large-scale backdoored model from scratch requires a large training dataset, several recent attacks have considered to inject backdoors into a trained clean model without altering model behaviors on the clean data. Previous work finds that backdoors can be injected into a trained clean model with Adversarial Weight Perturbation (AWP), which means the... | Lichao Sun, Lingjuan Lyu, Weiqiang Wang, Xu Sun, Zhiyuan Zhang |  |
| 858 |  |  [A Biologically Interpretable Graph Convolutional Network to Link Genetic Risk Pathways and Imaging Phenotypes of Disease](https://openreview.net/forum?id=Lwr8We4MIxn) |  | 0 | We propose a novel end-to-end framework for whole-brain and whole-genome imaging-genetics. Our genetics network uses hierarchical graph convolution and pooling operations to embed subject-level data onto a low-dimensional latent space. The hierarchical network implicitly tracks the convergence of genetic risk across well-established biological pathways, while an... | Aaron L. Goldman, Archana Venkataraman, Daniel R. Weinberger, Giulio Pergola, Qiang Chen, Sayan Ghosal, William Ulrich |  |
| 859 |  |  [Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners](https://openreview.net/forum?id=ek9a0qIafW) |  | 0 | Large-scale pre-trained language models have contributed significantly to natural language processing by demonstrating remarkable abilities as few-shot learners. However, their effectiveness depends mainly on scaling the model parameters and prompt design, hindering their implementation in most real-world applications. This study proposes a novel pluggable, extensible,... | Chuanqi Tan, Fei Huang, Huajun Chen, Luoqiu Li, Ningyu Zhang, Shumin Deng, Xiang Chen, Zhen Bi |  |
| 860 |  |  [OntoProtein: Protein Pretraining With Gene Ontology Embedding](https://openreview.net/forum?id=yfe1VMYAXa4) |  | 0 | Self-supervised protein language models have proved their effectiveness in learning the proteins representations. With the increasing computational power, current protein language models pre-trained with millions of diverse sequences can advance the parameter scale from million-level to billion-level and achieve remarkable improvement. However, those prevailing... | Haosen Hong, Huajun Chen, Jiazhang Lian, Ningyu Zhang, Qiang Zhang, Shumin Deng, Siyuan Cheng, Xiaozhuan Liang, Zhen Bi |  |
| 861 |  |  [Permutation Compressors for Provably Faster Distributed Nonconvex Optimization](https://openreview.net/forum?id=GugZ5DzzAu) |  | 0 | In this work we study the MARINA method of Gorbunov et al (ICML, 2021) -- the current state-of-the-art distributed non-convex optimization method in terms of theoretical communication complexity. Theoretical superiority of this method can be largely attributed to two sources: a carefully engineered biased stochastic gradient estimator, which leads to a reduction in the... | Alexander Tyurin, Peter Richtárik, Rafal Szlendak |  |
| 862 |  |  [Few-shot Learning via Dirichlet Tessellation Ensemble](https://openreview.net/forum?id=6kCiVaoQdx9) |  | 0 | Few-shot learning (FSL) is the process of rapid generalization from abundant base samples to inadequate novel samples. Despite extensive research in recent years, FSL is still not yet able to generate satisfactory solutions for a wide range of real-world applications. To confront this challenge, we study the FSL problem from a geometric point of view in this paper. One... | Chunwei Ma, Jinhui Xu, Mingchen Gao, Ziyun Huang |  |
| 863 |  |  [Deep Point Cloud Reconstruction](https://openreview.net/forum?id=mKDtUtxIGJ) |  | 0 | Point cloud obtained from 3D scanning is often sparse, noisy, and irregular. To cope with these issues, recent studies have been separately conducted to densify, denoise, and complete inaccurate point cloud. In this paper, we advocate that jointly solving these tasks leads to significant improvement for point cloud reconstruction. To this end, we propose a deep point... | Byeongin Joung, François Rameau, In So Kweon, Jaesik Park, Jaesung Choe |  |
| 864 |  |  [$\beta$-Intact-VAE: Identifying and Estimating Causal Effects under Limited Overlap](https://openreview.net/forum?id=q7n2RngwOM) |  | 0 | As an important problem in causal inference, we discuss the identification and estimation of treatment effects (TEs) under limited overlap; that is, when subjects with certain features belong to a single treatment group. We use a latent variable to model a prognostic score which is widely used in biostatistics and sufficient for TEs; i.e., we build a generative... | Kenji Fukumizu, Pengzhou Abel Wu |  |
| 865 |  |  [Promoting Saliency From Depth: Deep Unsupervised RGB-D Saliency Detection](https://openreview.net/forum?id=BZnnMbt0pW) |  | 0 | Growing interests in RGB-D salient object detection (RGB-D SOD) have been witnessed in recent years, owing partly to the popularity of depth sensors and the rapid progress of deep learning techniques. Unfortunately, existing RGB-D SOD methods typically demand large quantity of training images being thoroughly annotated at pixel-level. The laborious and time-consuming... | Chuan Guo, Jie Liu, Jingjing Li, Li Cheng, Qi Bi, Wei Ji |  |
| 866 |  |  [Retriever: Learning Content-Style Representation as a Token-Level Bipartite Graph](https://openreview.net/forum?id=AXWygMvuT6Q) |  | 0 | This paper addresses the unsupervised learning of content-style decomposed representation. We first give a definition of style and then model the content-style representation as a token-level bipartite graph. An unsupervised framework, named Retriever, is proposed to learn such representations. First, a cross-attention module is employed to retrieve permutation... | Chong Luo, Dacheng Yin, Wenjun Zeng, Xuanchi Ren, Yuwang Wang, Zhiwei Xiong |  |
| 867 |  |  [Neural Markov Controlled SDE: Stochastic Optimization for Continuous-Time Data](https://openreview.net/forum?id=7DI6op61AY) |  | 0 | We propose a novel probabilistic framework for modeling stochastic dynamics with the rigorous use of stochastic optimal control theory. The proposed model called the neural Markov controlled stochastic differential equation (CSDE) overcomes the fundamental and structural limitations of conventional dynamical models by introducing the following two components: (1) Markov... | Junseok Kwon, Kyungjae Lee, Sung Woo Park |  |
| 868 |  |  [CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention](https://openreview.net/forum?id=_PHymLIxuI) |  | 0 | Transformers have made great progress in dealing with computer vision tasks. However, existing vision transformers have not yet possessed the ability of building the interactions among features of different scales, which is perceptually important to visual inputs. The reasons are two-fold: (1) Input embeddings of each layer are equal-scale, so no cross-scale feature can... | Binbin Lin, Deng Cai, Long Chen, Lu Yao, Wei Liu, Wenxiao Wang, Xiaofei He |  |
| 869 |  |  [Adversarially Robust Conformal Prediction](https://openreview.net/forum?id=9L1BsI4wP1H) |  | 0 | Conformal prediction is a model-agnostic tool for constructing prediction sets that are valid under the common i.i.d. assumption, which has been applied to quantify the prediction uncertainty of deep net classifiers. In this paper, we generalize this framework to the case where adversaries exist during inference time, under which the i.i.d. assumption is grossly... | Asaf Gendler, Luca Daniel, TsuiWei Weng, Yaniv Romano |  |
| 870 |  |  [Hot-Refresh Model Upgrades with Regression-Free Compatible Training in Image Retrieval](https://openreview.net/forum?id=HTp-6yLGGX) |  | 0 | The task of hot-refresh model upgrades of image retrieval systems plays an essential role in the industry but has never been investigated in academia before. Conventional cold-refresh model upgrades can only deploy new models after the gallery is overall backfilled, taking weeks or even months for massive data. In contrast, hot-refresh model upgrades deploy the new... | Binjie Zhang, Chun Yuan, Xuyuan Xu, Yantao Shen, Yexin Wang, Ying Shan, Yixiao Ge, Yu Li |  |
| 871 |  |  [Visual Representation Learning over Latent Domains](https://openreview.net/forum?id=kG0AtPi6JI1) |  | 0 | A fundamental shortcoming of deep neural networks is their specialization to a single task and domain. While multi-domain learning enables the learning of compact models that span multiple visual domains, these rely on the presence of domain labels, in turn requiring laborious curation of datasets. This paper proposes a less explored, but highly realistic new setting... | Hakan Bilen, Lucas Deecke, Timothy M. Hospedales |  |
| 872 |  |  [Chemical-Reaction-Aware Molecule Representation Learning](https://openreview.net/forum?id=6sh3pIzKS-) |  | 0 | Molecule representation learning (MRL) methods aim to embed molecules into a real vector space. However, existing SMILES-based (Simplified Molecular-Input Line-Entry System) or GNN-based (Graph Neural Networks) MRL methods either take SMILES strings as input that have difficulty in encoding molecule structure information, or over-emphasize the importance of GNN... | Heng Ji, Hongwei Wang, Jiawei Han, Kyunghyun Cho, Martin D. Burke, Weijiang Li, Xiaomeng Jin |  |
| 873 |  |  [Skill-based Meta-Reinforcement Learning](https://openreview.net/forum?id=jeLW-Fh9bV) |  | 0 | While deep reinforcement learning methods have shown impressive results in robot learning, their sample inefficiency makes the learning of complex, long-horizon behaviors with real robot systems infeasible. To mitigate this issue, meta-reinforcement learning methods aim to enable fast learning on novel tasks by learning how to learn. Yet, the application has been... | Joseph J. Lim, Karl Pertsch, ShaoHua Sun, Sung Ju Hwang, Taewook Nam |  |
| 874 |  |  [InfinityGAN: Towards Infinite-Pixel Image Synthesis](https://openreview.net/forum?id=ufGMqIM0a4b) |  | 0 | We present InfinityGAN, a method to generate arbitrary-sized images. The problem is associated with several key challenges. First, scaling existing models to an arbitrarily large image size is resource-constrained, both in terms of computation and availability of large-field-of-view training data. InfinityGAN trains and infers patch-by-patch seamlessly with low... | Chieh Hubert Lin, HsinYing Lee, MingHsuan Yang, Sergey Tulyakov, YenChi Cheng |  |
| 875 |  |  [Shuffle Private Stochastic Convex Optimization](https://openreview.net/forum?id=DrZXuTGg2A-) |  | 0 | In shuffle privacy, each user sends a collection of randomized messages to a trusted shuffler, the shuffler randomly permutes these messages, and the resulting shuffled collection of messages must satisfy differential privacy. Prior work in this model has largely focused on protocols that use a single round of communication to compute algorithmic primitives like means,... | Albert Cheu, Binghui Peng, Jieming Mao, Matthew Joseph |  |
| 876 |  |  [Know Your Action Set: Learning Action Relations for Reinforcement Learning](https://openreview.net/forum?id=MljXVdp4A3N) |  | 0 | Intelligent agents can solve tasks in various ways depending on their available set of actions. However, conventional reinforcement learning (RL) assumes a fixed action set. This work asserts that tasks with varying action sets require reasoning of the relations between the available actions. For instance, taking a nail-action in a repair task is meaningful only if a... | Ayush Jain, Joseph J. Lim, KyungMin Kim, Norio Kosaka |  |
| 877 |  |  [On the Importance of Difficulty Calibration in Membership Inference Attacks](https://openreview.net/forum?id=3eIrli0TwQ) |  | 0 | The vulnerability of machine learning models to membership inference attacks has received much attention in recent years. However, existing attacks mostly remain impractical due to having high false positive rates, where non-member samples are often erroneously predicted as members. This type of error makes the predicted membership signal unreliable, especially since... | Alexandre Sablayrolles, Chuan Guo, Graham Cormode, Lauren Watson |  |
| 878 |  |  [Entroformer: A Transformer-based Entropy Model for Learned Image Compression](https://openreview.net/forum?id=VrjOFfcnSV8) |  | 0 | One critical component in lossy deep image compression is the entropy model, which predicts the probability distribution of the quantized latent representation in the encoding and decoding modules. Previous works build entropy models upon convolutional neural networks which are inefficient in capturing global dependencies. In this work, we propose a novel... | Ming Lin, Rong Jin, Xiuyu Sun, Yichen Qian, Zhiyu Tan |  |
| 879 |  |  [Dual Lottery Ticket Hypothesis](https://openreview.net/forum?id=fOsN52jn25l) |  | 0 | Fully exploiting the learning capacity of neural networks requires overparameterized dense networks. On the other side, directly training sparse neural networks typically results in unsatisfactory performance. Lottery Ticket Hypothesis (LTH) provides a novel view to investigate sparse network training and maintain its capacity. Concretely, it claims there exist winning... | Huan Wang, Kunpeng Li, Yue Bai, Yun Fu, Zhiqiang Tao |  |
| 880 |  |  [GNN is a Counter? Revisiting GNN for Question Answering](https://openreview.net/forum?id=hzmQ4wOnSb) |  | 0 | Question Answering (QA) has been a long-standing research topic in AI and NLP fields, and a wealth of studies has been conducted to attempt to equip QA systems with human-level reasoning capability. To approximate the complicated human reasoning process, state-of-the-art QA systems commonly use pre-trained language models (LMs) to access knowledge encoded in LMs... | Diyi Yang, Kuan Wang, Le Song, Tao Qin, Yuyu Zhang |  |
| 881 |  |  [IFR-Explore: Learning Inter-object Functional Relationships in 3D Indoor Scenes](https://openreview.net/forum?id=OT3mLgR8Wg8) |  | 0 | Building embodied intelligent agents that can interact with 3D indoor environments has received increasing research attention in recent years. While most works focus on single-object or agent-object visual functionality and affordances, our work proposes to study a novel, underexplored, kind of visual relations that is also important to perceive and model --... | Hang Zhao, Kaichun Mo, Leonidas J. Guibas, Qi Li, Yanchao Yang |  |
| 882 |  |  [VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D ARTiculated Objects](https://openreview.net/forum?id=iEx3PiooLy) |  | 0 | Perceiving and manipulating 3D articulated objects (e.g., cabinets, doors) in human environments is an important yet challenging task for future home-assistant robots. The space of 3D articulated objects is exceptionally rich in their myriad semantic categories, diverse shape geometry, and complicated part functionality. Previous works mostly abstract kinematic... | Hao Dong, Kaichun Mo, Leonidas J. Guibas, Qingnan Fan, Ruihai Wu, Tianhao Wu, Xuelin Chen, Yan Zhao, Yian Wang, Zizheng Guo |  |
| 883 |  |  [Neural graphical modelling in continuous-time: consistency guarantees and algorithms](https://openreview.net/forum?id=SsHBkfeRF9L) |  | 0 | The discovery of structure from time series data is a key problem in fields of study working with complex systems. Most identifiability results and learning algorithms assume the underlying dynamics to be discrete in time. Comparatively few, in contrast, explicitly define dependencies in infinitesimal intervals of time, independently of the scale of observation and of... | Alexis Bellot, Kim Branson, Mihaela van der Schaar |  |
| 884 |  |  [C-Planning: An Automatic Curriculum for Learning Goal-Reaching Tasks](https://openreview.net/forum?id=K2JfSnLBD9) |  | 0 | Goal-conditioned reinforcement learning (RL) has shown great success recently at solving a wide range of tasks(e.g., navigation, robotic manipulation). However, learning to reach distant goals remains a central challenge to the field, and the task is particularly hard without any offline data, expert demonstrations, and reward shaping. In this paper, we propose to solve... | Benjamin Eysenbach, Joseph E. Gonzalez, Ruslan Salakhutdinov, Sergey Levine, Tianjun Zhang |  |
| 885 |  |  [NAS-Bench-Suite: NAS Evaluation is (Now) Surprisingly Easy](https://openreview.net/forum?id=0DLwqQLmqV) |  | 0 | The release of tabular benchmarks, such as NAS-Bench-101 and NAS-Bench-201, has significantly lowered the computational overhead for conducting scientific research in neural architecture search (NAS). Although they have been widely adopted and used to tune real-world NAS algorithms, these benchmarks are limited to small search spaces and focus solely on image... | Arber Zela, Arjun Krishnakumar, Colin White, Frank Hutter, Guri Zabergja, Kaicheng Yu, Mahmoud Safari, Shakiba Moradian, Yash Mehta |  |
| 886 |  |  [Machine Learning For Elliptic PDEs: Fast Rate Generalization Bound, Neural Scaling Law and Minimax Optimality](https://openreview.net/forum?id=mhYUBYNoGz) |  | 0 | In this paper, we study the statistical limits of deep learning techniques for solving elliptic partial differential equations (PDEs) from random samples using the Deep Ritz Method (DRM) and Physics-Informed Neural Networks (PINNs). To simplify the problem, we focus on a prototype elliptic PDE: the Schr\"odinger equation on a hypercube with zero Dirichlet boundary... | Haoxuan Chen, Jianfeng Lu, Jose H. Blanchet, Lexing Ying, Yiping Lu |  |
| 887 |  |  [Variational oracle guiding for reinforcement learning](https://openreview.net/forum?id=pjqqxepwoMy) |  | 0 | How to make intelligent decisions is a central problem in machine learning and artificial intelligence. Despite recent successes of deep reinforcement learning (RL) in various decision making problems, an important but under-explored aspect is how to leverage oracle observation (the information that is invisible during online decision making, but is available during... | Dongqi Han, Dongsheng Li, Kenji Doya, Tadashi Kozuno, Xufang Luo, Yuqing Yang, ZhaoYun Chen |  |
| 888 |  |  [CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation](https://openreview.net/forum?id=XGzk5OKWFFc) |  | 0 | Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from a labeled source domain to a different unlabeled target domain. Most existing UDA methods focus on learning domain-invariant feature representation, either from the domain level or category level, using convolution neural networks (CNNs)-based frameworks. One fundamental problem for the... | Fan Wang, Hao Li, Pichao Wang, Rong Jin, Tongkun Xu, Weihua Chen |  |
| 889 |  |  [Beyond ImageNet Attack: Towards Crafting Adversarial Examples for Black-box Domains](https://openreview.net/forum?id=QkRV50TZyP) |  | 0 | Adversarial examples have posed a severe threat to deep neural networks due to their transferable nature. Currently, various works have paid great efforts to enhance the cross-model transferability, which mostly assume the substitute model is trained in the same domain as the target model. However, in reality, the relevant information of the deployed model is unlikely... | Hui Xue, Jingkuan Song, Lianli Gao, Qilong Zhang, Xiaodan Li, Yuan He, Yuefeng Chen |  |
| 890 |  |  [Learning to Schedule Learning rate with Graph Neural Networks](https://openreview.net/forum?id=k7efTb0un9z) |  | 0 | Recent decades have witnessed great development of stochastic optimization in training deep neural networks. Learning rate scheduling is one of the most important factors that influence the performance of stochastic optimizers like Adam. Traditional methods seek to find a relatively proper scheduling among a limited number of pre-defined rules and might not accommodate... | ChoJui Hsieh, LiCheng Lan, Ruochen Wang, Xiangning Chen, Yuanhao Xiong |  |
| 891 |  |  [SketchODE: Learning neural sketch representation in continuous time](https://openreview.net/forum?id=c-4HSDAWua5) |  | 0 | Learning meaningful representations for chirographic drawing data such as sketches, handwriting, and flowcharts is a gateway for understanding and emulating human creative expression. Despite being inherently continuous-time data, existing works have treated these as discrete-time sequences, disregarding their true nature. In this work, we model such data as... | Ayan Das, Tao Xiang, Timothy M. Hospedales, YiZhe Song, Yongxin Yang |  |
| 892 |  |  [Measuring the Interpretability of Unsupervised Representations via Quantized Reversed Probing](https://openreview.net/forum?id=HFPTzdwN39) |  | 0 | Self-supervised visual representation learning has recently attracted significant research interest. While a common way to evaluate self-supervised representations is through transfer to various downstream tasks, we instead investigate the problem of measuring their interpretability, i.e. understanding the semantics encoded in raw representations. We formulate the... | Andrea Vedaldi, Iro Laina, Yuki M. Asano |  |
| 893 |  |  [GradMax: Growing Neural Networks using Gradient Information](https://openreview.net/forum?id=qjN4h_wwUO) |  | 0 | The architecture and the parameters of neural networks are often optimized independently, which requires costly retraining of the parameters whenever the architecture is modified. In this work we instead focus on growing the architecture without requiring costly retraining. We present a method that adds new neurons during training without impacting what is already... | Bart van Merrienboer, Fabian Pedregosa, Max Vladymyrov, Thomas Unterthiner, Utku Evci |  |
| 894 |  |  [Online Coreset Selection for Rehearsal-based Continual Learning](https://openreview.net/forum?id=f9D-5WNG4Nv) |  | 0 | A dataset is a shred of crucial evidence to describe a task. However, each data point in the dataset does not have the same potential, as some of the data points can be more representative or informative than others. This unequal importance among the data points may have a large impact in rehearsal-based continual learning, where we store a subset of the training... | Divyam Madaan, Eunho Yang, Jaehong Yoon, Sung Ju Hwang |  |
| 895 |  |  [Switch to Generalize: Domain-Switch Learning for Cross-Domain Few-Shot Classification](https://openreview.net/forum?id=H-iABMvzIc) |  | 0 | This paper considers few-shot learning under the cross-domain scenario. The cross-domain setting imposes a critical challenge, i.e., using very few (support) samples to generalize the already-learned model to a novel domain. We hold a hypothesis, i.e., if a deep model is capable to fast generalize itself to different domains (using very few samples) during training, it... | Yi Yang, Yifan Sun, Zhengdong Hu |  |
| 896 |  |  [Zero-CL: Instance and Feature decorrelation for negative-free symmetric contrastive learning](https://openreview.net/forum?id=RAW9tCdVxLj) |  | 0 | For self-supervised contrastive learning, models can easily collapse and generate trivial constant solutions. The issue has been mitigated by recent improvement on objective design, which however often requires square complexity either for the size of instances ($\mathcal{O}(N^{2})$) or feature dimensions ($\mathcal{O}(d)^2$). To prevent such collapse, we develop two... | Feng Zhu, Junchi Yan, Rui Zhao, Shaofeng Zhang, Xiaokang Yang |  |
| 897 |  |  [Random matrices in service of ML footprint: ternary random features with no performance loss](https://openreview.net/forum?id=qwULHx9zld) |  | 0 | In this article, we investigate the spectral behavior of random features kernel matrices of the type ${\bf K} = \mathbb{E}_{{\bf w}} \left[\sigma\left({\bf w}^{\sf T}{\bf x}_i\right)\sigma\left({\bf w}^{\sf T}{\bf x}_j\right)\right]_{i,j=1}^n$, with nonlinear function $\sigma(\cdot)$, data ${\bf x}_1, \ldots, {\bf x}_n \in \mathbb{R}^p$, and random projection vector... | Hafiz Tiomoko Ali, Romain Couillet, Zhenyu Liao |  |
| 898 |  |  [Global Convergence of Multi-Agent Policy Gradient in Markov Potential Games](https://openreview.net/forum?id=gfwON7rAm4) |  | 0 | Potential games are arguably one of the most important and widely studied classes of normal form games. They define the archetypal setting of multi-agent coordination in which all agents utilities are perfectly aligned via a common potential function. Can this intuitive framework be transplanted in the setting of Markov games? What are the similarities and differences... | Georgios Piliouras, Ioannis Panageas, Stefanos Leonardos, Will Overman |  |
| 899 |  |  [Rethinking Adversarial Transferability from a Data Distribution Perspective](https://openreview.net/forum?id=gVRhIEajG1k) |  | 0 | Adversarial transferability enables attackers to generate adversarial examples from the source model to attack the target model, which has raised security concerns about the deployment of DNNs in practice. In this paper, we rethink adversarial transferability from a data distribution perspective and further enhance transferability by score matching based optimization.... | Jiacheng Sun, Yao Zhu, Zhenguo Li |  |
| 900 |  |  [Transformers Can Do Bayesian Inference](https://openreview.net/forum?id=KSugKcbNf9) |  | 0 | Currently, it is hard to reap the benefits of deep learning for Bayesian methods, which allow the explicit specification of prior knowledge and accurately capture model uncertainty. We present Prior-Data Fitted Networks (PFNs). PFNs leverage large-scale machine learning techniques to approximate a large set of posteriors. The only requirement for PFNs to work is the... | Frank Hutter, Josif Grabocka, Noah Hollmann, Samuel Müller, Sebastian PinedaArango |  |
| 901 |  |  [Learning Discrete Structured Variational Auto-Encoder using Natural Evolution Strategies](https://openreview.net/forum?id=JJCjv4dAbyL) |  | 0 | Discrete variational auto-encoders (VAEs) are able to represent semantic latent spaces in generative learning. In many real-life settings, the discrete latent space consists of high-dimensional structures, and propagating gradients through the relevant structures often requires enumerating over an exponentially large latent space. Recently, various approaches were... | Alon Berliner, Guy Rotman, Roi Reichart, Tamir Hazan, Yossi Adi |  |
| 902 |  |  [Learning Features with Parameter-Free Layers](https://openreview.net/forum?id=bCrdi4iVvv) |  | 0 | Trainable layers such as convolutional building blocks are the standard network design choices by learning parameters to capture the global context through successive spatial operations. When designing an efficient network, trainable layers such as the depthwise convolution is the source of efficiency in the number of parameters and FLOPs, but there was little... | Beomyoung Kim, Byeongho Heo, Dongyoon Han, Young Joon Yoo |  |
| 903 |  |  [Denoising Likelihood Score Matching for Conditional Score-based Data Generation](https://openreview.net/forum?id=LcF-EEt8cCC) |  | 0 | Many existing conditional score-based data generation methods utilize Bayes' theorem to decompose the gradients of a log posterior density into a mixture of scores. These methods facilitate the training procedure of conditional score models, as a mixture of scores can be separately estimated using a score model and a classifier. However, our analysis indicates that the... | BoWun Cheng, ChenHao Chao, ChiaChe Chang, ChiaPing Chen, ChunYi Lee, WeiFang Sun, YiChen Lo, YuLin Chang, YuLun Liu |  |
| 904 |  |  [Memory Replay with Data Compression for Continual Learning](https://openreview.net/forum?id=a7H7OucbWaU) |  | 0 | Continual learning needs to overcome catastrophic forgetting of the past. Memory replay of representative old training samples has been shown as an effective solution, and achieves the state-of-the-art (SOTA) performance. However, existing work is mainly built on a small memory buffer containing a few original data, which cannot fully characterize the old data... | Chongxuan Li, Jun Zhu, Kuo Yang, Lanqing Hong, Liyuan Wang, Longhui Yu, Shifeng Zhang, Xingxing Zhang, Yi Zhong, Zhenguo Li |  |
| 905 |  |  [MAML is a Noisy Contrastive Learner in Classification](https://openreview.net/forum?id=LDAwu17QaJz) |  | 0 | Model-agnostic meta-learning (MAML) is one of the most popular and widely adopted meta-learning algorithms, achieving remarkable success in various learning problems. Yet, with the unique design of nested inner-loop and outer-loop updates, which govern the task-specific and meta-model-centric learning, respectively, the underlying learning objective of MAML remains... | ChiaHsiang Kao, PinYu Chen, WeiChen Chiu |  |
| 906 |  |  [RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning](https://openreview.net/forum?id=afoV8W3-IYp) |  | 0 | Reasoning about visual relationships is central to how humans interpret the visual world. This task remains challenging for current deep learning algorithms since it requires addressing three key technical problems jointly: 1) identifying object entities and their properties, 2) inferring semantic relations between pairs of entities, and 3) generalizing to novel... | Anima Anandkumar, Chaowei Xiao, Huaizu Jiang, SongChun Zhu, Weili Nie, Xiaojian Ma, Yuke Zhu, Zhiding Yu |  |
| 907 |  |  [Boosted Curriculum Reinforcement Learning](https://openreview.net/forum?id=anbBFlX1tJ1) |  | 0 | Curriculum value-based reinforcement learning (RL) solves a complex target task by reusing action-values across a tailored sequence of related tasks of increasing difficulty. However, finding an exact way of reusing action-values in this setting is still a poorly understood problem. In this paper, we introduce the concept of boosting to curriculum value-based RL, by... | Carlo D'Eramo, Jan Peters, Joni Pajarinen, Pascal Klink |  |
| 908 |  |  [ViDT: An Efficient and Effective Fully Transformer-based Object Detector](https://openreview.net/forum?id=w4cXZDDib1H) |  | 0 | Transformers are transforming the landscape of computer vision, especially for recognition tasks. Detection transformers are the first fully end-to-end learning systems for object detection, while vision transformers are the first fully transformer-based architecture for image classification. In this paper, we integrate Vision and Detection Transformers (ViDT) to build... | Byeongho Heo, Deqing Sun, Dongyoon Han, Hwanjun Song, MingHsuan Yang, Sanghyuk Chun, Varun Jampani, Wonjae Kim |  |
| 909 |  |  [BiBERT: Accurate Fully Binarized BERT](https://openreview.net/forum?id=5xEgrl_5FAJ) |  | 0 | The large pre-trained BERT has achieved remarkable performance on Natural Language Processing (NLP) tasks but is also computation and memory expensive. As one of the powerful compression approaches, binarization extremely reduces the computation and memory consumption by utilizing 1-bit parameters and bitwise operations. Unfortunately, the full binarization of BERT... | Aishan Liu, Haotong Qin, Mingyuan Zhang, Qinghua Yan, Qingqing Dang, Xianglong Liu, Yifu Ding, Ziwei Liu |  |
| 910 |  |  [Feature Kernel Distillation](https://openreview.net/forum?id=tBIQEvApZK5) |  | 0 | Trained Neural Networks (NNs) can be viewed as data-dependent kernel machines, with predictions determined by the inner product of last-layer representations across inputs, referred to as the feature kernel. We explore the relevance of the feature kernel for Knowledge Distillation (KD), using a mechanistic understanding of an NN’s optimisation process. We extend the... | Bobby He, Mete Ozay |  |
| 911 |  |  [Representation-Agnostic Shape Fields](https://openreview.net/forum?id=-ngwPqanCEZ) |  | 0 | 3D shape analysis has been widely explored in the era of deep learning. Numerous models have been developed for various 3D data representation formats, e.g., MeshCNN for meshes, PointNet for point clouds and VoxNet for voxels. In this study, we present Representation-Agnostic Shape Fields (RASF), a generalizable and computation-efficient shape embedding module for 3D... | Bingbing Ni, Jiancheng Yang, Linguo Li, Teng Li, Wenjun Zhang, Xiaoyang Huang, Yanjun Wang, Ziyu Chen |  |
| 912 |  |  [Learning Synthetic Environments and Reward Networks for Reinforcement Learning](https://openreview.net/forum?id=C1_esHN6AVn) |  | 0 | We introduce Synthetic Environments (SEs) and Reward Networks (RNs), represented by neural networks, as proxy environment models for training Reinforcement Learning (RL) agents. We show that an agent, after being trained exclusively on the SE, is able to solve the corresponding real environment. While an SE acts as a full proxy to a real environment by learning about... | Andreas Sälinger, Fabio Ferreira, Frank Hutter, Thomas Nierhoff |  |
| 913 |  |  [Who Is Your Right Mixup Partner in Positive and Unlabeled Learning](https://openreview.net/forum?id=NH29920YEmj) |  | 0 | Positive and Unlabeled (PU) learning targets inducing a binary classifier from weak training datasets of positive and unlabeled instances, which arise in many real-world applications. In this paper, we propose a novel PU learning method, namely Positive and unlabeled learning with Partially Positive Mixup (P3Mix), which simultaneously benefits from data augmentation and... | Changchun Li, Jihong Ouyang, Lei Feng, Ximing Li |  |
| 914 |  |  [Incremental False Negative Detection for Contrastive Learning](https://openreview.net/forum?id=dDjSKKA5TP1) |  | 0 | Self-supervised learning has recently shown great potential in vision tasks through contrastive learning, which aims to discriminate each image, or instance, in the dataset. However, such instance-level learning ignores the semantic relationship among instances and sometimes undesirably repels the anchor from the semantically similar samples, termed as "false... | HungYu Tseng, MingHsuan Yang, ShaoYi Chien, TsaiShien Chen, WeiChih Hung |  |
| 915 |  |  [Multi-Critic Actor Learning: Teaching RL Policies to Act with Style](https://openreview.net/forum?id=rJvY_5OzoI) |  | 0 | Using a single value function (critic) shared over multiple tasks in Actor-Critic multi-task reinforcement learning (MTRL) can result in negative interference between tasks, which can compromise learning performance. Multi-Critic Actor Learning (MultiCriticAL) proposes instead maintaining separate critics for each task being trained while training a single multi-task... | George Cheng, Kate Saenko, Meng Wu, Siddharth Mysore, Yunqi Zhao |  |
| 916 |  |  [Clean Images are Hard to Reblur: Exploiting the Ill-Posed Inverse Task for Dynamic Scene Deblurring](https://openreview.net/forum?id=kezNJydWvE) |  | 0 | The goal of dynamic scene deblurring is to remove the motion blur in a given image. Typical learning-based approaches implement their solutions by minimizing the L1 or L2 distance between the output and the reference sharp image. Recent attempts adopt visual recognition features in training to improve the perceptual quality. However, those features are primarily... | Jaerin Lee, Kyoung Mu Lee, Sanghyun Son, Seungjun Nah |  |
| 917 |  |  [Learning Disentangled Representation by Exploiting Pretrained Generative Models: A Contrastive Learning View](https://openreview.net/forum?id=j-63FSNcO5a) |  | 0 | From the intuitive notion of disentanglement, the image variations corresponding to different generative factors should be distinct from each other, and the disentangled representation should reflect those variations with separate dimensions. To discover the generative factors and learn disentangled representation, previous methods typically leverage an extra... | Tao Yang, Wenjun Zeng, Xuanchi Ren, Yuwang Wang |  |
| 918 |  |  [Towards Building A Group-based Unsupervised Representation Disentanglement Framework](https://openreview.net/forum?id=YgPqNctmyd) |  | 0 | Disentangled representation learning is one of the major goals of deep learning, and is a key step for achieving explainable and generalizable models. The key idea of the state-of-the-art VAE-based unsupervised representation disentanglement methods is to minimize the total correlation of the joint distribution of the latent variables. However, it has been proved that... | Nanning Zheng, Tao Yang, Wenjun Zeng, Xuanchi Ren, Yuwang Wang |  |
| 919 |  |  [Learning Efficient Image Super-Resolution Networks via Structure-Regularized Pruning](https://openreview.net/forum?id=AjGC97Aofee) |  | 0 | Several image super-resolution (SR) networks have been proposed of late for efficient SR, achieving promising results. However, they are still not lightweight enough and neglect to be extended to larger networks. At the same time, model compression techniques, like neural architecture search and knowledge distillation, typically consume considerable computation... | Can Qin, Huan Wang, Yulun Zhang, Yun Fu |  |
| 920 |  |  [Wiring Up Vision: Minimizing Supervised Synaptic Updates Needed to Produce a Primate Ventral Stream](https://openreview.net/forum?id=g1SzIRLQXMM) |  | 0 | After training on large datasets, certain deep neural networks are surprisingly good models of the neural mechanisms of adult primate visual object recognition. Nevertheless, these models are considered poor models of the development of the visual system because they posit millions of sequential, precisely coordinated synaptic updates, each based on a labeled image.... | Franziska Geiger, James J. DiCarlo, Martin Schrimpf, Tiago Marques |  |
| 921 |  |  [Dynamics-Aware Comparison of Learned Reward Functions](https://openreview.net/forum?id=CALFyKVs87) |  | 0 | The ability to learn reward functions plays an important role in enabling the deployment of intelligent agents in the real world. However, $\textit{comparing}$ reward functions, for example as a means of evaluating reward learning methods, presents a challenge. Reward functions are typically compared by considering the behavior of optimized policies, but this approach... | Adrien Gaidon, Blake Wulfe, Jean Mercat, Logan Michael Ellis, Rowan Thomas McAllister |  |
| 922 |  |  [Learning Hierarchical Structures with Differentiable Nondeterministic Stacks](https://openreview.net/forum?id=5LXw_QplBiF) |  | 0 | Learning hierarchical structures in sequential data -- from simple algorithmic patterns to natural language -- in a reliable, generalizable way remains a challenging problem for neural language models. Past work has shown that recurrent neural networks (RNNs) struggle to generalize on held-out algorithmic or syntactic patterns without supervision or some inductive bias.... | Brian DuSell, David Chiang |  |
| 923 |  |  [Sampling with Mirrored Stein Operators](https://openreview.net/forum?id=eMudnJsb1T5) |  | 0 | We introduce a new family of particle evolution samplers suitable for constrained domains and non-Euclidean geometries. Stein Variational Mirror Descent and Mirrored Stein Variational Gradient Descent minimize the Kullback-Leibler (KL) divergence to constrained target distributions by evolving particles in a dual space defined by a mirror map. Stein Variational Natural... | Chang Liu, Jiaxin Shi, Lester Mackey |  |
| 924 |  |  [Planning in Stochastic Environments with a Learned Model](https://openreview.net/forum?id=X6D9bAHhBQ1) |  | 0 | Model-based reinforcement learning has proven highly successful. However, learning a model in isolation from its use during planning is problematic in complex environments. To date, the most effective techniques have instead combined value-equivalent model learning with powerful tree-search methods. This approach is exemplified by MuZero, which has achieved... | David Silver, Ioannis Antonoglou, Julian Schrittwieser, Sherjil Ozair, Thomas K. Hubert |  |
| 925 |  |  [RotoGrad: Gradient Homogenization in Multitask Learning](https://openreview.net/forum?id=T8wHz4rnuGL) |  | 0 | Multitask learning is being increasingly adopted in applications domains like computer vision and reinforcement learning. However, optimally exploiting its advantages remains a major challenge due to the effect of negative transfer. Previous works have tracked down this issue to the disparities in gradient magnitudes and directions across tasks, when optimizing the... | Adrián Javaloy, Isabel Valera |  |
| 926 |  |  [On Improving Adversarial Transferability of Vision Transformers](https://openreview.net/forum?id=D6nH3719vZy) |  | 0 | Vision transformers (ViTs) process input images as sequences of patches via self-attention; a radically different architecture than convolutional neural networks (CNNs). This makes it interesting to study the adversarial feature space of ViT models and their transferability. In particular, we observe that adversarial patterns found via conventional adversarial attacks... | Fahad Shahbaz Khan, Fatih Porikli, Kanchana Ranasinghe, Muzammal Naseer, Salman Khan |  |
| 927 |  |  [On Predicting Generalization using GANs](https://openreview.net/forum?id=eW5R4Cek6y6) |  | 0 | Research on generalization bounds for deep networks seeks to give ways to predict test error using just the training dataset and the network parameters. While generalization bounds can give many insights about architecture design, training algorithms etc., what they do not currently do is yield good predictions for actual test error. A recently introduced Predicting... | Arushi Gupta, Nikunj Saunshi, Sanjeev Arora, Yi Zhang |  |
| 928 |  |  [On the Connection between Local Attention and Dynamic Depth-wise Convolution](https://openreview.net/forum?id=L3_SsSNMmy) |  | 0 | Vision Transformer (ViT) attains state-of-the-art performance in visual recognition, and the variant, Local Vision Transformer, makes further improvements. The major component in Local Vision Transformer, local attention, performs the attention separately over small local windows. We rephrase local attention as a channel-wise locally-connected layer and analyze it from... | Jiaying Liu, Jingdong Wang, Lei Sun, MingMing Cheng, Qi Dai, Qi Han, Zejia Fan |  |
| 929 |  |  [Strength of Minibatch Noise in SGD](https://openreview.net/forum?id=uorVGbWV5sw) |  | 0 | The noise in stochastic gradient descent (SGD), caused by minibatch sampling, is poorly understood despite its practical importance in deep learning. This work presents the first systematic study of the SGD noise and fluctuations close to a local minimum. We first analyze the SGD noise in linear regression in detail and then derive a general formula for approximating... | Kangqiao Liu, Liu Ziyin, Masahito Ueda, Takashi Mori |  |
| 930 |  |  [Learning more skills through optimistic exploration](https://openreview.net/forum?id=cU8rknuhxc) |  | 0 | Unsupervised skill learning objectives (Eysenbach et al., 2019; Gregor et al., 2016) allow agents to learn rich repertoires of behavior in the absence of extrinsic rewards. They work by simultaneously training a policy to produce distinguishable latent-conditioned trajectories, and a discriminator to evaluate distinguishability by trying to infer latents from... | DJ Strouse, David WardeFarley, Kate Baumli, Steven Stenberg Hansen, Volodymyr Mnih |  |
| 931 |  |  [Reinforcement Learning under a Multi-agent Predictive State Representation Model: Method and Theory](https://openreview.net/forum?id=PLDOnFoVm4) |  | 0 | We study reinforcement learning for partially observable multi-agent systems where each agent only has access to its own observation and reward and aims to maximize its cumulative rewards. To handle partial observations, we propose graph-assisted predictive state representations (GAPSR), a scalable multi-agent representation learning framework that leverages the agent... | Furong Huang, Han Liu, Pratap Tokekar, Zhi Zhang, Zhuoran Yang |  |
| 932 |  |  [Adversarial Support Alignment](https://openreview.net/forum?id=26gKg6x-ie) |  | 0 | We study the problem of aligning the supports of distributions. Compared to the existing work on distribution alignment, support alignment does not require the densities to be matched. We propose symmetric support difference as a divergence measure to quantify the mismatch between supports. We show that select discriminators (e.g. discriminator trained for... | Shangyuan Tong, Shiyu Chang, Timur Garipov, Tommi S. Jaakkola, Yang Zhang |  |
| 933 |  |  [GreaseLM: Graph REASoning Enhanced Language Models](https://openreview.net/forum?id=41e9o6cQPj) |  | 0 | Answering complex questions about textual narratives requires reasoning over both stated context and the world knowledge that underlies it. However, pretrained language models (LM), the foundation of most modern QA systems, do not robustly represent latent relationships between concepts, which is necessary for reasoning. While knowledge graphs (KG) are often used to... | Antoine Bosselut, Christopher D. Manning, Hongyu Ren, Jure Leskovec, Michihiro Yasunaga, Percy Liang, Xikun Zhang |  |
| 934 |  |  [Learning meta-features for AutoML](https://openreview.net/forum?id=DTkEfj0Ygb8) |  | 0 | This paper tackles the AutoML problem, aimed to automatically select an ML algorithm and its hyper-parameter configuration most appropriate to the dataset at hand. The proposed approach, MetaBu, learns new meta-features via an Optimal Transport procedure, aligning the manually designed \mf s with the space of distributions on the hyper-parameter configurations. MetaBu... | Andry Rasoanaivo, Herilalaina Rakotoarison, Louisot Milijaona, Marc Schoenauer, Michèle Sebag |  |
| 935 |  |  [Latent Variable Sequential Set Transformers for Joint Multi-Agent Motion Prediction](https://openreview.net/forum?id=Dup_dDqkZC5) |  | 0 | Robust multi-agent trajectory prediction is essential for the safe control of robotic systems. A major challenge is to efficiently learn a representation that approximates the true joint distribution of contextual, social, and temporal information to enable planning. We propose Latent Variable Sequential Set Transformers which are encoder-decoder architectures that... | Christopher Pal, Felipe Codevilla, Felix Heide, Florian Golemo, Jim Aldon D'Souza, Martin Weiss, Roger Girgis, Samira Ebrahimi Kahou |  |
| 936 |  |  [Understanding Latent Correlation-Based Multiview Learning and Self-Supervision: An Identifiability Perspective](https://openreview.net/forum?id=5FUq05QRc5b) |  | 0 | Multiple views of data, both naturally acquired (e.g., image and audio) and artificially produced (e.g., via adding different noise to data samples), have proven useful in enhancing representation learning. Natural views are often handled by multiview analysis tools, e.g., (deep) canonical correlation analysis [(D)CCA], while the artificial ones are frequently used in... | Qi Lyu, Songtao Lu, Weiran Wang, Xiao Fu |  |
| 937 |  |  [Deconstructing the Inductive Biases of Hamiltonian Neural Networks](https://openreview.net/forum?id=EDeVYpT42oS) |  | 0 | Physics-inspired neural networks (NNs), such as Hamiltonian or Lagrangian NNs, dramatically outperform other learned dynamics models by leveraging strong inductive biases. These models, however, are challenging to apply to many real world systems, such as those that don’t conserve energy or contain contacts, a common setting for robotics and reinforcement learning. In... | Andrew Gordon Wilson, Marc Anton Finzi, Nate Gruver, Samuel Don Stanton |  |
| 938 |  |  [Memorizing Transformers](https://openreview.net/forum?id=TrjbxzRcnf-) |  | 0 | Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights. We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of... | Christian Szegedy, DeLesley Hutchins, Markus Norman Rabe, Yuhuai Wu |  |
| 939 |  |  [Learning-Augmented $k$-means Clustering](https://openreview.net/forum?id=X8cLTHexYyY) |  | 0 | $k$-means clustering is a well-studied problem due to its wide applicability. Unfortunately, there exist strong theoretical limits on the performance of any algorithm for the $k$-means problem on worst-case inputs. To overcome this barrier, we consider a scenario where \`\`advice'' is provided to help perform clustering. Specifically, we consider the $k$-means problem... | David P. Woodruff, Jon C. Ergun, Samson Zhou, Sandeep Silwal, Zhili Feng |  |
| 940 |  |  [On the Uncomputability of Partition Functions in Energy-Based Sequence Models](https://openreview.net/forum?id=SsPCtEY6yCl) |  | 0 | In this paper, we argue that energy-based sequence models backed by expressive parametric families can result in uncomputable and inapproximable partition functions. Among other things, this makes model selection--and therefore learning model parameters--not only difficult, but generally _undecidable_. The reason is that there are no good deterministic or randomized... | Arya D. McCarthy, ChuCheng Lin |  |
| 941 |  |  [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://openreview.net/forum?id=fILj7WpI-g) |  | 0 | A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain & task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose... | Andrew Brock, Andrew Jaegle, Andrew Zisserman, Carl Doersch, Catalin Ionescu, Daniel Zoran, David Ding, Evan Shelhamer, JeanBaptiste Alayrac, João Carreira, Matthew M. Botvinick, Olivier J. Hénaff, Oriol Vinyals, Sebastian Borgeaud, Skanda Koppula |  |
| 942 |  |  [DR3: Value-Based Deep Reinforcement Learning Requires Explicit Regularization](https://openreview.net/forum?id=POvMvLi91f) |  | 0 | Despite overparameterization, deep networks trained via supervised learning are surprisingly easy to optimize and exhibit excellent generalization. One hypothesis to explain this is that overparameterized deep networks enjoy the benefits of implicit regularization induced by stochastic gradient descent, which favors parsimonious solutions that generalize well on test... | Aaron C. Courville, Aviral Kumar, George Tucker, Rishabh Agarwal, Sergey Levine, Tengyu Ma |  |
| 943 |  |  [MT3: Multi-Task Multitrack Music Transcription](https://openreview.net/forum?id=iMSjopcOn0p) |  | 0 | Automatic Music Transcription (AMT), inferring musical notes from raw audio, is a challenging task at the core of music understanding. Unlike Automatic Speech Recognition (ASR), which typically focuses on the words of a single speaker, AMT often requires transcribing multiple instruments simultaneously, all while preserving fine-scale pitch and timing information.... | Curtis Hawthorne, Ethan Manilow, Ian Simon, Jesse H. Engel, Josh Gardner |  |
| 944 |  |  [Does your graph need a confidence boost? Convergent boosted smoothing on graphs with tabular node features](https://openreview.net/forum?id=nHpzE7DqAnG) |  | 0 | Many practical modeling tasks require making predictions using tabular data composed of heterogeneous feature types (e.g., text-based, categorical, continuous, etc.). In this setting boosted decision trees and related ensembling techniques generally dominate real-world applications involving iid training/test sets. However, when there are relations between samples and... | David Wipf, Jiuhai Chen, Jonas Mueller, Soji Adeshina, Tom Goldstein, Vassilis N. Ioannidis, Yangkun Wang |  |
| 945 |  |  [Geometric and Physical Quantities improve E(3) Equivariant Message Passing](https://openreview.net/forum?id=_xwr8gOBeV1) |  | 0 | Including covariant information, such as position, force, velocity or spin is important in many tasks in computational physics and chemistry. We introduce Steerable E($3$) Equivariant Graph Neural Networks (SEGNNs) that generalise equivariant graph networks, such that node and edge attributes are not restricted to invariant scalars, but can contain covariant... | Elise van der Pol, Erik J. Bekkers, Johannes Brandstetter, Max Welling, Rob Hesselink |  |
| 946 |  |  [SphereFace2: Binary Classification is All You Need for Deep Face Recognition](https://openreview.net/forum?id=l3SDgUh7qZO) |  | 0 | State-of-the-art deep face recognition methods are mostly trained with a softmax-based multi-class classification framework. Despite being popular and effective, these methods still have a few shortcomings that limit empirical performance. In this paper, we start by identifying the discrepancy between training and evaluation in the existing multi-class classification... | Adrian Weller, Bhiksha Raj, Rita Singh, Weiyang Liu, Yandong Wen |  |
| 947 |  |  [Boosting Randomized Smoothing with Variance Reduced Classifiers](https://openreview.net/forum?id=mHu2vIds_-b) |  | 0 | Randomized Smoothing (RS) is a promising method for obtaining robustness certiﬁcates by evaluating a base model under noise. In this work, we: (i) theoretically motivate why ensembles are a particularly suitable choice as base models for RS, and (ii) empirically conﬁrm this choice, obtaining state-of-the-art results in multiple settings. The key insight of our work is... | Marc Fischer, Mark Niklas Müller, Martin T. Vechev, Miklós Z. Horváth |  |
| 948 |  |  [SOSP: Efficiently Capturing Global Correlations by Second-Order Structured Pruning](https://openreview.net/forum?id=t5EmXZ3ZLR) |  | 0 | Pruning neural networks reduces inference time and memory costs. On standard hardware, these benefits will be especially prominent if coarse-grained structures, like feature maps, are pruned. We devise two novel saliency-based methods for second-order structured pruning (SOSP) which include correlations among all structures and layers. Our main method SOSP-H employs an... | David Reeb, Ingo Steinwart, Manuel Nonnenmacher, Thomas Pfeil |  |
| 949 |  |  [Relational Multi-Task Learning: Modeling Relations between Data and Tasks](https://openreview.net/forum?id=8Py-W8lSUgy) |  | 0 | A key assumption in multi-task learning is that at the inference time the multi-task model only has access to a given data point but not to the data point’s labels from other tasks. This presents an opportunity to extend multi-task learning to utilize data point’s labels from other auxiliary tasks, and this way improves performance on the new task. Here we introduce a... | Jiaxuan You, Jure Leskovec, Kaidi Cao |  |
| 950 |  |  [CoBERL: Contrastive BERT for Reinforcement Learning](https://openreview.net/forum?id=sRZ3GhmegS) |  | 0 | Many reinforcement learning (RL) agents require a large amount of experience to solve tasks. We propose Contrastive BERT for RL (COBERL), an agent that combines a new contrastive loss and a hybrid LSTM-transformer architecture to tackle the challenge of improving data efficiency. COBERL enables efficient and robust learning from pixels across a wide variety of domains.... | Adrià Puigdomènech Badia, Andrea Banino, Charles Blundell, Jacob C. Walker, Jovana Mitrovic, Tim Scholtes |  |
| 951 |  |  [Optimal Transport for Causal Discovery](https://openreview.net/forum?id=qwBK94cP1y) |  | 0 | To determine causal relationships between two variables, approaches based on Functional Causal Models (FCMs) have been proposed by properly restricting model classes; however, the performance is sensitive to the model assumptions, which makes it difficult to use. In this paper, we provide a novel dynamical-system view of FCMs and propose a new framework for identifying... | Cheng Zhang, Hedvig Kjellström, Kun Zhang, Ruibo Tu |  |
| 952 |  |  [On Bridging Generic and Personalized Federated Learning for Image Classification](https://openreview.net/forum?id=I1hQbx10Kxn) |  | 0 | Federated learning is promising for its capability to collaboratively train models with multiple clients without accessing their data, but vulnerable when clients' data distributions diverge from each other. This divergence further leads to a dilemma: "Should we prioritize the learned model's generic performance (for future use at the server) or its personalized... | HongYou Chen, WeiLun Chao |  |
| 953 |  |  [Value Gradient weighted Model-Based Reinforcement Learning](https://openreview.net/forum?id=4-D6CZkRXxI) |  | 0 | Model-based reinforcement learning (MBRL) is a sample efficient technique to obtain control policies, yet unavoidable modeling errors often lead performance deterioration. The model in MBRL is often solely fitted to reconstruct dynamics, state observations in particular, while the impact of model error on the policy is not captured by the training objective. This leads... | Amirmassoud Farahmand, Animesh Garg, Claas Voelcker, Victor Liao |  |
| 954 |  |  [Fairness in Representation for Multilingual NLP: Insights from Controlled Experiments on Conditional Language Modeling](https://openreview.net/forum?id=-llS6TiOew) |  | 0 | We perform systematically and fairly controlled experiments with the 6-layer Transformer to investigate the hardness in conditional-language-modeling languages which have been traditionally considered morphologically rich (AR and RU) and poor (ZH). We evaluate through statistical comparisons across 30 possible language directions from the 6 languages of the United... | Ada Wan |  |
| 955 |  |  [Reinforcement Learning with Sparse Rewards using Guidance from Offline Demonstration](https://openreview.net/forum?id=YJ1WzgMVsMt) |  | 0 | A major challenge in real-world reinforcement learning (RL) is the sparsity of reward feedback. Often, what is available is an intuitive but sparse reward function that only indicates whether the task is completed partially or fully. However, the lack of carefully designed, fine grain feedback implies that most existing RL algorithms fail to learn an acceptable policy... | Akshay Sarvesh, Desik Rengarajan, Dileep M. Kalathil, Gargi Vaidya, Srinivas Shakkottai |  |
| 956 |  |  [Linking Emergent and Natural Languages via Corpus Transfer](https://openreview.net/forum?id=49A1Y6tRhaq) |  | 0 | The study of language emergence aims to understand how human languages are shaped by perceptual grounding and communicative intent. Computational approaches to emergent communication (EC) predominantly consider referential games in limited domains and analyze the learned protocol within the game framework. As a result, it remains unclear how the emergent languages from... | Chuang Gan, Joshua B. Tenenbaum, Karthik R. Narasimhan, Mo Yu, Shunyu Yao, Yang Zhang |  |
| 957 |  |  [TAMP-S2GCNets: Coupling Time-Aware Multipersistence Knowledge Representation with Spatio-Supra Graph Convolutional Networks for Time-Series Forecasting](https://openreview.net/forum?id=wv6g8fWLX2q) |  | 0 | Graph Neural Networks (GNNs) are proven to be a powerful machinery for learning complex dependencies in multivariate spatio-temporal processes. However, most existing GNNs have inherently static architectures, and as a result, do not explicitly account for time dependencies of the encoded knowledge and are limited in their ability to simultaneously infer latent... | Baris Coskunuzer, Ignacio SegoviaDominguez, Yulia R. Gel, Yuzhou Chen |  |
| 958 |  |  [The MultiBERTs: BERT Reproductions for Robustness Analysis](https://openreview.net/forum?id=K0E_F0gFDgA) |  | 0 | Experiments with pre-trained models such as BERT are often based on a single checkpoint. While the conclusions drawn apply to the artifact tested in the experiment (i.e., the particular instance of the model), it is not always clear whether they hold for the more general procedure which includes the architecture, training data, initialization scheme, and loss function.... | Alexander D'Amour, Dipanjan Das, Ellie Pavlick, Ian Tenney, Iulia Raluca Turc, Jacob Eisenstein, Jasmijn Bastings, Jason Wei, Naomi Saphra, Steve Yadlowsky, Tal Linzen, Thibault Sellam |  |
| 959 |  |  [Message Passing Neural PDE Solvers](https://openreview.net/forum?id=vSix3HPYKSU) |  | 0 | The numerical solution of partial differential equations (PDEs) is difficult, having led to a century of research so far. Recently, there have been pushes to build neural--numerical hybrid solvers, which piggy-backs the modern trend towards fully end-to-end learned systems. Most works so far can only generalize over a subset of properties to which a generic solver would... | Daniel E. Worrall, Johannes Brandstetter, Max Welling |  |
| 960 |  |  [Multi-Stage Episodic Control for Strategic Exploration in Text Games](https://openreview.net/forum?id=Ek7PSN7Y77z) |  | 0 | Text adventure games present unique challenges to reinforcement learning methods due to their combinatorially large action spaces and sparse rewards. The interplay of these two factors is particularly demanding because large action spaces require extensive exploration, while sparse rewards provide limited feedback. This work proposes to tackle the explore-vs-exploit... | Jens Tuyls, Karthik Narasimhan, Sham M. Kakade, Shunyu Yao |  |
| 961 |  |  [Exploring the Limits of Large Scale Pre-training](https://openreview.net/forum?id=V3C8p78sDa) |  | 0 | Recent developments in large-scale machine learning suggest that by scaling up data, model size and training time properly, one might observe that improvements in pre-training would transfer favorably to most downstream tasks. In this work we systematically study this phenomena and establish that, as we increase the upstream accuracy, performance of downstream tasks... | Behnam Neyshabur, Hanie Sedghi, Mostafa Dehghani, Samira Abnar |  |
| 962 |  |  [Universal Approximation Under Constraints is Possible with Transformers](https://openreview.net/forum?id=JGO8CvG5S9) |  | 0 | Many practical problems need the output of a machine learning model to satisfy a set of constraints, $K$. Nevertheless, there is no known guarantee that classical neural network architectures can exactly encode constraints while simultaneously achieving universality. We provide a quantitative constrained universal approximation theorem which guarantees that for any... | Anastasis Kratsios, Behnoosh Zamanlooy, Ivan Dokmanic, Tianlin Liu |  |
| 963 |  |  [Scaling Laws for Neural Machine Translation](https://openreview.net/forum?id=hR_SMu8cxCV) |  | 0 | We present an empirical study of scaling properties of encoder-decoder Transformer models used in neural machine translation (NMT). We show that cross-entropy loss as a function of model size follows a certain scaling law. Specifically (i) We propose a formula which describes the scaling behavior of cross-entropy loss as a bivariate function of encoder and decoder size,... | Ankur Bapna, Behrooz Ghorbani, Ciprian Chelba, Colin Cherry, Markus Freitag, Maxim Krikun, Orhan Firat, Xavier Garcia |  |
| 964 |  |  [AdaRL: What, Where, and How to Adapt in Transfer Reinforcement Learning](https://openreview.net/forum?id=8H5bpVwvt5) |  | 0 | One practical challenge in reinforcement learning (RL) is how to make quick adaptations when faced with new environments. In this paper, we propose a principled framework for adaptive RL, called AdaRL, that adapts reliably and efficiently to changes across domains with a few samples from the target domain, even in partially observable environments. Specifically, we... | Biwei Huang, Chaochao Lu, Fan Feng, Kun Zhang, Sara Magliacane |  |
| 965 |  |  [Independent SE(3)-Equivariant Models for End-to-End Rigid Protein Docking](https://openreview.net/forum?id=GQjaI9mLet) |  | 0 | Protein complex formation is a central problem in biology, being involved in most of the cell's processes, and essential for applications, e.g. drug design or protein engineering. We tackle rigid body protein-protein docking, i.e., computationally predicting the 3D structure of a protein-protein complex from the individual unbound structures, assuming no conformational... | Andreas Krause, Charlotte Bunne, OctavianEugen Ganea, Regina Barzilay, Tommi S. Jaakkola, Xinyuan Huang, Yatao Bian |  |
| 966 |  |  [Towards a Unified View of Parameter-Efficient Transfer Learning](https://openreview.net/forum?id=0RDcd5Axok) |  | 0 | Fine-tuning large pretrained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the parameters of the pretrained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only... | Chunting Zhou, Graham Neubig, Junxian He, Taylor BergKirkpatrick, Xuezhe Ma |  |
| 967 |  |  [GNN-LM: Language Modeling based on Global Contexts via GNN](https://openreview.net/forum?id=BS49l-B5Bql) |  | 0 | Inspired by the notion that "it to copy is easier than to memorize", in this work, we introduce GNN-LM, which extends vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where... | Fei Wu, Jiwei Li, Shi Zong, Tianwei Zhang, Xiaofei Sun, Xiaoya Li, Yuxian Meng |  |
| 968 |  |  [Continual Learning with Filter Atom Swapping](https://openreview.net/forum?id=metRpM4Zrcb) |  | 0 | Continual learning has been widely studied in recent years to resolve the catastrophic forgetting of deep neural networks. In this paper, we first enforce a low-rank filter subspace by decomposing convolutional filters within each network layer over a small set of filter atoms. Then, we perform continual learning with filter atom swapping. In other words, we learn for... | Qiang Qiu, Wei Chen, Ze Wang, Zichen Miao |  |
| 969 |  |  [Continual Learning with Recursive Gradient Optimization](https://openreview.net/forum?id=7YDLgf9_zgm) |  | 0 | Learning multiple tasks sequentially without forgetting previous knowledge, called Continual Learning(CL), remains a long-standing challenge for neural networks. Most existing methods rely on additional network capacity or data replay. In contrast, we introduce a novel approach which we refer to as Recursive Gradient Optimization(RGO). RGO is composed of an iteratively... | Hao Liu, Huaping Liu |  |
| 970 |  |  [NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning](https://openreview.net/forum?id=g8NJR6fCCl8) |  | 0 | Deployment of machine learning models in real high-risk settings (e.g. healthcare) often depends not only on the model's accuracy but also on its fairness, robustness, and interpretability. Generalized Additive Models (GAMs) are a class of interpretable models with a long history of use in these high-risk domains, but they lack desirable features of deep learning such... | Anna Goldenberg, ChunHao Chang, Rich Caruana |  |
| 971 |  |  [Learnability of convolutional neural networks for infinite dimensional input via mixed and anisotropic smoothness](https://openreview.net/forum?id=dgxFTxuJ50e) |  | 0 | Among a wide range of success of deep learning, convolutional neural networks have been extensively utilized in several tasks such as speech recognition, image processing, and natural language processing, which require inputs with large dimensions. Several studies have investigated function estimation capability of deep learning, but most of them have assumed that the... | Sho Okumoto, Taiji Suzuki |  |
| 972 |  |  [Improved deterministic l2 robustness on CIFAR-10 and CIFAR-100](https://openreview.net/forum?id=tD7eCtaSkR) |  | 0 | Training convolutional neural networks (CNNs) with a strict Lipschitz constraint under the $l_{2}$ norm is useful for provable adversarial robustness, interpretable gradients and stable training. While $1$-Lipschitz CNNs can be designed by enforcing a $1$-Lipschitz constraint on each layer, training such networks requires each layer to have an orthogonal Jacobian matrix... | Sahil Singla, Soheil Feizi, Surbhi Singla |  |
| 973 |  |  [Transition to Linearity of Wide Neural Networks is an Emerging Property of Assembling Weak Models](https://openreview.net/forum?id=CyKHoKyvgnp) |  | 0 | Wide neural networks with linear output layer have been shown to be near-linear, and to have near-constant neural tangent kernel (NTK), in a region containing the optimization path of gradient descent. These findings seem counter-intuitive since in general neural networks are highly complex models. Why does a linear structure emerge when the neural networks become wide?... | Chaoyue Liu, Libin Zhu, Mikhail Belkin |  |
| 974 |  |  [Looking Back on Learned Experiences For Class/task Incremental Learning](https://openreview.net/forum?id=RxplU3vmBx) |  | 0 | Classical deep neural networks are limited in their ability to learn from emerging streams of training data. When trained sequentially on new or evolving tasks, their performance degrades sharply, making them inappropriate in real-world use cases. Existing methods tackle it by either storing old data samples or only updating a parameter set of deep neural networks,... | Guoying Zhao, Mohammad Sabokrou, Mozhgan PourKeshavarz |  |
| 975 |  |  [EntQA: Entity Linking as Question Answering](https://openreview.net/forum?id=US2rTP5nm_) |  | 0 | A conventional approach to entity linking is to first find mentions in a given document and then infer their underlying entities in the knowledge base. A well-known limitation of this approach is that it requires finding mentions without knowing their entities, which is unnatural and difficult. We present a new model that does not suffer from this limitation called... | Karl Stratos, Wenyue Hua, Wenzheng Zhang |  |
| 976 |  |  [Escaping limit cycles: Global convergence for constrained nonconvex-nonconcave minimax problems](https://openreview.net/forum?id=2_vhkAMARk) |  | 0 | This paper introduces a new extragradient-type algorithm for a class of nonconvex-nonconcave minimax problems. It is well-known that finding a local solution for general minimax problems is computationally intractable. This observation has recently motivated the study of structures sufficient for convergence of first order methods in the more general setting of... | Olivier Fercoq, Panos Patrinos, Puya Latafat, Thomas Pethick, Volkan Cevher |  |
| 977 |  |  [Compositional Attention: Disentangling Search and Retrieval](https://openreview.net/forum?id=IwJPj2MBcIa) |  | 0 | Multi-head, key-value attention is the backbone of transformer-like model architectures which have proven to be widely successful in recent years. This attention mechanism uses multiple parallel key-value attention blocks (called heads), each performing two fundamental computations: (1) search - selection of a relevant entity from a set via query-key interaction, and... | Guillaume Lajoie, Irina Rish, Sarthak Mittal, Sharath Chandra Raparthy, Yoshua Bengio |  |
| 978 |  |  [Contrastive Fine-grained Class Clustering via Generative Adversarial Networks](https://openreview.net/forum?id=XWODe7ZLn8f) |  | 0 | Unsupervised fine-grained class clustering is a practical yet challenging task due to the difficulty of feature representations learning of subtle object details. We introduce C3-GAN, a method that leverages the categorical inference power of InfoGAN with contrastive learning. We aim to learn feature representations that encourage a dataset to form distinct cluster... | JungWoo Ha, Yunji Kim |  |
| 979 |  |  [Learning Multimodal VAEs through Mutual Supervision](https://openreview.net/forum?id=1xXvPrAshao) |  | 0 | Multimodal VAEs seek to model the joint distribution over heterogeneous data (e.g.\ vision, language), whilst also capturing a shared representation across such modalities. Prior work has typically combined information from the modalities by reconciling idiosyncratic representations directly in the recognition model through explicit products, mixtures, or other such... | Philip H. S. Torr, Sebastian M. Schmon, Siddharth Narayanaswamy, Tom Joy, Tom Rainforth, Yuge Shi |  |
| 980 |  |  [When should agents explore?](https://openreview.net/forum?id=dEwfxt14bca) |  | 0 | Exploration remains a central challenge for reinforcement learning (RL). Virtually all existing methods share the feature of a \*monolithic\* behaviour policy that changes only gradually (at best). In contrast, the exploratory behaviours of animals and humans exhibit a rich diversity, namely including forms of \*switching\* between modes. This paper presents an initial... | David Szepesvari, Diana L. Borsa, Georg Ostrovski, Miruna Pislar, Tom Schaul |  |
| 981 |  |  [Revisiting Design Choices in Offline Model Based Reinforcement Learning](https://openreview.net/forum?id=zz9hXVhf40) |  | 0 | Offline reinforcement learning enables agents to leverage large pre-collected datasets of environment transitions to learn control policies, circumventing the need for potentially expensive or unsafe online data collection. Significant progress has been made recently in offline model-based reinforcement learning, approaches which leverage a learned dynamics model. This... | Cong Lu, Jack ParkerHolder, Michael A. Osborne, Philip J. Ball, Stephen J. Roberts |  |
| 982 |  |  [NASPY: Automated Extraction of Automated Machine Learning Models](https://openreview.net/forum?id=KhLK0sHMgXK) |  | 0 | We present NASPY, an end-to-end adversarial framework to extract the networkarchitecture of deep learning models from Neural Architecture Search (NAS). Existing works about model extraction attacks mainly focus on conventional DNN models with very simple operations, or require heavy manual analysis with lots of domain knowledge. In contrast, NASPY introduces seq2seq... | Jiwei Li, Shangwei Guo, Tianwei Zhang, Xiaoxuan Lou, Yaoxin Wu |  |
| 983 |  |  [COptiDICE: Offline Constrained Reinforcement Learning via Stationary Distribution Correction Estimation](https://openreview.net/forum?id=FLA55mBee6Q) |  | 0 | We consider the offline constrained reinforcement learning (RL) problem, in which the agent aims to compute a policy that maximizes expected return while satisfying given cost constraints, learning only from a pre-collected dataset. This problem setting is appealing in many real-world scenarios, where direct interaction with the environment is costly or risky, and where... | Arthur Guez, Cosmin Paduraru, Daniel J. Mankowitz, Doina Precup, Jongmin Lee, KeeEung Kim, Nicolas Heess |  |
| 984 |  |  [Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers](https://openreview.net/forum?id=nhnJ3oo6AB) |  | 0 | We propose to address quadrupedal locomotion tasks using Reinforcement Learning (RL) with a Transformer-based model that learns to combine proprioceptive information and high-dimensional depth sensor inputs. While learning-based locomotion has made great advances using RL, most methods still rely on domain randomization for training blind agents that generalize to... | Huazhe Xu, Minghao Zhang, Nicklas Hansen, Ruihan Yang, Xiaolong Wang |  |
| 985 |  |  [ViTGAN: Training GANs with Vision Transformers](https://openreview.net/forum?id=dwg5rXg1WS_) |  | 0 | Recently, Vision Transformers (ViTs) have shown competitive performance on image recognition while requiring less vision-specific inductive biases. In this paper, we investigate if such performance can be extended to image generation. To this end, we integrate the ViT architecture into generative adversarial networks (GANs). For ViT discriminators, we observe that... | Ce Liu, Han Zhang, Huiwen Chang, Kwonjoon Lee, Lu Jiang, Zhuowen Tu |  |
| 986 |  |  [POETREE: Interpretable Policy Learning with Adaptive Decision Trees](https://openreview.net/forum?id=AJsI-ymaKn_) |  | 0 | Building models of human decision-making from observed behaviour is critical to better understand, diagnose and support real-world policies such as clinical care. As established policy learning approaches remain focused on imitation performance, they fall short of explaining the demonstrated decision-making process. Policy Extraction through decision Trees (POETREE) is... | Alex J. Chan, Alizée Pace, Mihaela van der Schaar |  |
| 987 |  |  [TRGP: Trust Region Gradient Projection for Continual Learning](https://openreview.net/forum?id=iEvAf8i6JjO) |  | 0 | Catastrophic forgetting is one of the major challenges in continual learning. To address this issue, some existing methods put restrictive constraints on the optimization space of the new task for minimizing the interference to old tasks. However, this may lead to unsatisfactory performance for the new task, especially when the new task is strongly correlated with old... | Deliang Fan, Junshan Zhang, Li Yang, Sen Lin |  |
| 988 |  |  [Properties from mechanisms: an equivariance perspective on identifiable representation learning](https://openreview.net/forum?id=g5ynW-jMq4M) |  | 0 | A key goal of unsupervised representation learning is \`\`inverting'' a data generating process to recover its latent properties. Existing work that provably achieves this goal relies on strong assumptions on relationships between the latent variables (e.g., independence conditional on auxiliary information). In this paper, we take a very different perspective on the... | Jason S. Hartford, Kartik Ahuja, Yoshua Bengio |  |
| 989 |  |  [Revisiting Over-smoothing in BERT from the Perspective of Graph](https://openreview.net/forum?id=dUV91uaXm3) |  | 0 | Recently over-smoothing phenomenon of Transformer-based models is observed in both vision and language fields. However, no existing work has delved deeper to further investigate the main cause of this phenomenon. In this work, we make the attempt to analyze the over-smoothing problem from the perspective of graph, where such problem was first discovered and explored.... | Han Shi, Hang Xu, James T. Kwok, Jiahui Gao, Lingpeng Kong, Stephen M. S. Lee, Xiaodan Liang, Zhenguo Li |  |
| 990 |  |  [Training invariances and the low-rank phenomenon: beyond linear networks](https://openreview.net/forum?id=XEW8CQgArno) |  | 0 | The implicit bias induced by the training of neural networks has become a topic of rigorous study. In the limit of gradient flow and gradient descent with appropriate step size, it has been shown that when one trains a deep linear network with logistic or exponential loss on linearly separable data, the weights converge to rank-$1$ matrices. In this paper, we extend... | Stefanie Jegelka, Thien Le |  |
| 991 |  |  [Learning Long-Term Reward Redistribution via Randomized Return Decomposition](https://openreview.net/forum?id=lpkGn3k2YdD) |  | 0 | Many practical applications of reinforcement learning require agents to learn from sparse and delayed rewards. It challenges the ability of agents to attribute their actions to future outcomes. In this paper, we consider the problem formulation of episodic reinforcement learning with trajectory feedback. It refers to an extreme delay of reward signals, in which the... | Jian Peng, Ruihan Guo, Yuan Zhou, Zhizhou Ren |  |
| 992 |  |  [What Happens after SGD Reaches Zero Loss? --A Mathematical Framework](https://openreview.net/forum?id=siCt4xZn5Ve) |  | 0 | Understanding the implicit bias of Stochastic Gradient Descent (SGD) is one of the key challenges in deep learning, especially for overparametrized models, where the local minimizers of the loss function $L$ can form a manifold. Intuitively, with a sufficiently small learning rate $\eta$, SGD tracks Gradient Descent (GD) until it gets close to such manifold, where the... | Sanjeev Arora, Tianhao Wang, Zhiyuan Li |  |
| 993 |  |  [Graph-Augmented Normalizing Flows for Anomaly Detection of Multiple Time Series](https://openreview.net/forum?id=45L_dgP48Vd) |  | 0 | Anomaly detection is a widely studied task for a broad variety of data types; among them, multiple time series appear frequently in applications, including for example, power grids and traffic networks. Detecting anomalies for multiple time series, however, is a challenging subject, owing to the intricate interdependencies among the constituent series. We hypothesize... | Enyan Dai, Jie Chen |  |
| 994 |  |  [Autoregressive Quantile Flows for Predictive Uncertainty Estimation](https://openreview.net/forum?id=z1-I6rOKv1S) |  | 0 | Numerous applications of machine learning involve representing probability distributions over high-dimensional data. We propose autoregressive quantile flows, a flexible class of normalizing flow models trained using a novel objective based on proper scoring rules. Our objective does not require calculating computationally expensive determinants of Jacobians during... | Allan Bishop, Phillip Si, Volodymyr Kuleshov |  |
| 995 |  |  [On the Importance of Firth Bias Reduction in Few-Shot Classification](https://openreview.net/forum?id=DNRADop4ksB) |  | 0 | Learning accurate classifiers for novel categories from very few examples, known as few-shot image classification, is a challenging task in statistical machine learning and computer vision. The performance in few-shot classification suffers from the bias in the estimation of classifier parameters; however, an effective underlying bias reduction technique that could... | David A. Forsyth, Ehsan Saleh, Saba Ghaffari, YuXiong Wang |  |
| 996 |  |  [Finite-Time Convergence and Sample Complexity of Multi-Agent Actor-Critic Reinforcement Learning with Average Reward](https://openreview.net/forum?id=04pGUg0-pdZ) |  | 0 | In this paper, we establish the first finite-time convergence result of the actor-critic algorithm for fully decentralized multi-agent reinforcement learning (MARL) problems with average reward. In this problem, a set of $N$ agents work cooperatively to maximize the global average reward through interacting with their neighbors over a communication network. We consider... | Hairi, Jia Liu, Songtao Lu |  |
| 997 |  |  [Towards Understanding the Data Dependency of Mixup-style Training](https://openreview.net/forum?id=ieNJYujcGDO) |  | 0 | In the Mixup training paradigm, a model is trained using convex combinations of data points and their associated labels. Despite seeing very few true data points during training, models trained using Mixup seem to still minimize the original empirical risk and exhibit better generalization and robustness on various tasks when compared to standard training. In this... | Chenwei Wu, Muthu Chidambaram, Rong Ge, Xiang Wang, Yuzheng Hu |  |
| 998 |  |  [Self-Supervision Enhanced Feature Selection with Correlated Gates](https://openreview.net/forum?id=oDFvtxzPOx) |  | 0 | Discovering relevant input features for predicting a target variable is a key scientific question. However, in many domains, such as medicine and biology, feature selection is confounded by a scarcity of labeled samples coupled with significant correlations among features. In this paper, we propose a novel deep learning approach to feature selection that addresses both... | Changhee Lee, Fergus Imrie, Mihaela van der Schaar |  |
| 999 |  |  [Score-Based Generative Modeling with Critically-Damped Langevin Diffusion](https://openreview.net/forum?id=CzceR82CYc) |  | 0 | Score-based generative models (SGMs) have demonstrated remarkable synthesis quality. SGMs rely on a diffusion process that gradually perturbs the data towards a tractable distribution, while the generative model learns to denoise. The complexity of this denoising task is, apart from the data distribution itself, uniquely determined by the diffusion process. We argue... | Arash Vahdat, Karsten Kreis, Tim Dockhorn |  |
| 1000 |  |  [Controlling Directions Orthogonal to a Classifier](https://openreview.net/forum?id=DIjCrlsu6Z) |  | 0 | We propose to identify directions invariant to a given classifier so that these directions can be controlled in tasks such as style transfer. While orthogonal decomposition is directly identifiable when the given classifier is linear, we formally define a notion of orthogonality in the non-linear case. We also provide a surprisingly simple method for constructing the... | Hao He, Tianxiao Shen, Tommi S. Jaakkola, Yilun Xu |  |
| 1001 |  |  [R5: Rule Discovery with Reinforced and Recurrent Relational Reasoning](https://openreview.net/forum?id=2eXhNpHeW6E) |  | 0 | Systematicity, i.e., the ability to recombine known parts and rules to form new sequences while reasoning over relational data, is critical to machine intelligence. A model with strong systematicity is able to train on small-scale tasks and generalize to large-scale tasks. In this paper, we propose R5, a relational reasoning framework based on reinforcement learning... | Bang Liu, Di Niu, Keith G. Mills, Shangling Jui, Shengyao Lu |  |
| 1002 |  |  [Representation Learning for Online and Offline RL in Low-rank MDPs](https://openreview.net/forum?id=J4iSIR9fhY0) |  | 0 | This work studies the question of Representation Learning in RL: how can we learn a compact low-dimensional representation such that on top of the representation we can perform RL procedures such as exploration and exploitation, in a sample efficient manner. We focus on the low-rank Markov Decision Processes (MDPs) where the transition dynamics correspond to a low-rank... | Masatoshi Uehara, Wen Sun, Xuezhou Zhang |  |
| 1003 |  |  [Lossless Compression with Probabilistic Circuits](https://openreview.net/forum?id=X_hByk2-5je) |  | 0 | Despite extensive progress on image generation, common deep generative model architectures are not easily applied to lossless compression. For example, VAEs suffer from a compression cost overhead due to their latent variables. This overhead can only be partially eliminated with elaborate schemes such as bits-back coding, often resulting in poor single-sample... | Anji Liu, Guy Van den Broeck, Stephan Mandt |  |
| 1004 |  |  [Understanding Domain Randomization for Sim-to-real Transfer](https://openreview.net/forum?id=T8vZHIRTrY) |  | 0 | Reinforcement learning encounters many challenges when applied directly in the real world. Sim-to-real transfer is widely used to transfer the knowledge learned from simulation to the real world. Domain randomization---one of the most popular algorithms for sim-to-real transfer---has been demonstrated to be effective in various tasks in robotics and autonomous driving.... | Chi Jin, Jiachen Hu, Lihong Li, Liwei Wang, Xiaoyu Chen |  |
| 1005 |  |  [$\mathrm{SO}(2)$-Equivariant Reinforcement Learning](https://openreview.net/forum?id=7F9cOhdvfk_) |  | 0 | Equivariant neural networks enforce symmetry within the structure of their convolutional layers, resulting in a substantial improvement in sample efficiency when learning an equivariant or invariant function. Such models are applicable to robotic manipulation learning which can often be formulated as a rotationally symmetric problem. This paper studies equivariant model... | Dian Wang, Robert Platt, Robin Walters |  |
| 1006 |  |  [Scarf: Self-Supervised Contrastive Learning using Random Feature Corruption](https://openreview.net/forum?id=CuV_qYkmKb3) |  | 0 | Self-supervised contrastive representation learning has proved incredibly successful in the vision and natural language domains, enabling state-of-the-art performance with orders of magnitude less labeled data. However, such methods are domain-specific and little has been done to leverage this technique on real-world \emph{tabular} datasets. We propose \textsc{Scarf}, a... | Dara Bahri, Donald Metzler, Heinrich Jiang, Yi Tay |  |
| 1007 |  |  [Responsible Disclosure of Generative Models Using Scalable Fingerprinting](https://openreview.net/forum?id=sOK-zS6WHB) |  | 0 | Over the past years, deep generative models have achieved a new level of performance. Generated data has become difficult, if not impossible, to be distinguished from real data. While there are plenty of use cases that benefit from this technology, there are also strong concerns on how this new technology can be misused to generate deep fakes and enable misinformation... | Dingfan Chen, Larry S. Davis, Mario Fritz, Ning Yu, Vladislav Skripniuk |  |
| 1008 |  |  [Path Auxiliary Proposal for MCMC in Discrete Space](https://openreview.net/forum?id=JSR-YDImK95) |  | 0 | Energy-based Model (EBM) offers a powerful approach for modeling discrete structure, but both inference and learning of EBM are hard as it involves sampling from discrete distributions. Recent work shows Markov Chain Monte Carlo (MCMC) with the informed proposal is a powerful tool for such sampling. However, an informed proposal only allows local updates as it requires... | Arun Ramamurthy, Hanjun Dai, Haoran Sun, Wei Xia |  |
| 1009 |  |  [Possibility Before Utility: Learning And Using Hierarchical Affordances](https://openreview.net/forum?id=7b4zxUnrO2N) |  | 0 | Reinforcement learning algorithms struggle on tasks with complex hierarchical dependency structures. Humans and other intelligent agents do not waste time assessing the utility of every high-level action in existence, but instead only consider ones they deem possible in the first place. By focusing only on what is feasible, or "afforded'', at the present moment, an... | Fei Sha, Robby Costales, Shariq Iqbal |  |
| 1010 |  |  [Interpretable Unsupervised Diversity Denoising and Artefact Removal](https://openreview.net/forum?id=DfMqlB0PXjM) |  | 0 | Image denoising and artefact removal are complex inverse problems admitting multiple valid solutions. Unsupervised diversity restoration, that is, obtaining a diverse set of possible restorations given a corrupted image, is important for ambiguity removal in many applications such as microscopy where paired data for supervised training are often unobtainable. In real... | Florian Jug, Mangal Prakash, Mauricio Delbracio, Peyman Milanfar |  |
| 1011 |  |  [Half-Inverse Gradients for Physical Deep Learning](https://openreview.net/forum?id=HTx7vrlLBEj) |  | 0 | Recent works in deep learning have shown that integrating differentiable physics simulators into the training process can greatly improve the quality of results. Although this combination represents a more complex optimization task than usual neural network training, the same gradient-based optimizers are used to minimize the loss function. However, the integrated... | Nils Thuerey, Patrick Schnell, Philipp Holl |  |
| 1012 |  |  [EE-Net: Exploitation-Exploration Neural Networks in Contextual Bandits](https://openreview.net/forum?id=X_ch3VrNSRg) |  | 0 | In this paper, we propose a novel neural exploration strategy in contextual bandits, EE-Net, distinct from the standard UCB-based and TS-based approaches. Contextual multi-armed bandits have been studied for decades with various applications. To solve the exploitation-exploration tradeoff in bandits, there are three main techniques: epsilon-greedy, Thompson Sampling... | Arindam Banerjee, Jingrui He, Yikun Ban, Yuchen Yan |  |
| 1013 |  |  [Spike-inspired rank coding for fast and accurate recurrent neural networks](https://openreview.net/forum?id=iMH1e5k7n3L) |  | 0 | Biological spiking neural networks (SNNs) can temporally encode information in their outputs, e.g. in the rank order in which neurons fire, whereas artificial neural networks (ANNs) conventionally do not. As a result, models of SNNs for neuromorphic computing are regarded as potentially more rapid and efficient than ANNs when dealing with temporal input. On the other... | Alan Jeffares, Pontus Stenetorp, Qinghai Guo, Timoleon Moraitis |  |
| 1014 |  |  [How to Robustify Black-Box ML Models? A Zeroth-Order Optimization Perspective](https://openreview.net/forum?id=W9G_ImpHlQd) |  | 0 | The lack of adversarial robustness has been recognized as an important issue for state-of-the-art machine learning (ML) models, e.g., deep neural networks (DNNs). Thereby, robustifying ML models against adversarial attacks is now a major focus of research. However, nearly all existing defense methods, particularly for robust training, made the white-box assumption that... | Jinfeng Yi, Jinghan Jia, Mingyi Hong, Shiyu Chang, Sijia Liu, Yimeng Zhang, Yuguang Yao |  |
| 1015 |  |  [RelaxLoss: Defending Membership Inference Attacks without Losing Utility](https://openreview.net/forum?id=FEDfGWVZYIn) |  | 0 | As a long-term threat to the privacy of training data, membership inference attacks (MIAs) emerge ubiquitously in machine learning models. Existing works evidence strong connection between the distinguishability of the training and testing loss distributions and the model's vulnerability to MIAs. Motivated by existing results, we propose a novel training framework based... | Dingfan Chen, Mario Fritz, Ning Yu |  |
| 1016 |  |  [Analyzing and Improving the Optimization Landscape of Noise-Contrastive Estimation](https://openreview.net/forum?id=eBS-3YiaIL-) |  | 0 | Noise-contrastive estimation (NCE) is a statistically consistent method for learning unnormalized probabilistic models. It has been empirically observed that the choice of the noise distribution is crucial for NCE’s performance. However, such observation has never been made formal or quantitative. In fact, it is not even clear whether the difficulties arising from a... | Andrej Risteski, Bingbin Liu, Elan Rosenfeld, Pradeep Kumar Ravikumar |  |
| 1017 |  |  [Contact Points Discovery for Soft-Body Manipulations with Differentiable Physics](https://openreview.net/forum?id=mmUA7_O9mjY) |  | 0 | Differentiable physics has recently been shown as a powerful tool for solving soft-body manipulation tasks. However, the differentiable physics solver often gets stuck when the initial contact points of the end effectors are sub-optimal or when performing multi-stage tasks that require contact point switching, which often leads to local minima. To address this... | Chuang Gan, Hao Su, Joshua B. Tenenbaum, Sizhe Li, Tao Du, Zhiao Huang |  |
| 1018 |  |  [Leveraging Automated Unit Tests for Unsupervised Code Translation](https://openreview.net/forum?id=cmt-6KtR4c4) |  | 0 | With little to no parallel data available for programming languages, unsupervised methods are well-suited to source code translation. However, the majority of unsupervised machine translation approaches rely on back-translation, a method developed in the context of natural language translation and one that inherently involves training on noisy inputs. Unfortunately,... | Baptiste Rozière, François Charton, Gabriel Synnaeve, Guillaume Lample, Jie Zhang, Mark Harman |  |
| 1019 |  |  [Scalable Sampling for Nonsymmetric Determinantal Point Processes](https://openreview.net/forum?id=BB4e8Atc1eR) |  | 0 | A determinantal point process (DPP) on a collection of $M$ items is a model, parameterized by a symmetric kernel matrix, that assigns a probability to every subset of those items. Recent work shows that removing the kernel symmetry constraint, yielding nonsymmetric DPPs (NDPPs), can lead to significant predictive performance gains for machine learning applications.... | Amin Karbasi, Elvis Dohmatob, Insu Han, Jennifer Gillenwater, Mike Gartrell |  |
| 1020 |  |  [Amortized Tree Generation for Bottom-up Synthesis Planning and Synthesizable Molecular Design](https://openreview.net/forum?id=FRxhHdnxt1) |  | 0 | Molecular design and synthesis planning are two critical steps in the process of molecular discovery that we propose to formulate as a single shared task of conditional synthetic pathway generation. We report an amortized approach to generate synthetic pathways as a Markov decision process conditioned on a target molecular embedding. This approach allows us to conduct... | Connor W. Coley, Rocío Mercado, Wenhao Gao |  |
| 1021 |  |  [Iterative Refinement Graph Neural Network for Antibody Sequence-Structure Co-design](https://openreview.net/forum?id=LI2bhrE_2A) |  | 0 | Antibodies are versatile proteins that bind to pathogens like viruses and stimulate the adaptive immune system. The specificity of antibody binding is determined by complementarity-determining regions (CDRs) at the tips of these Y-shaped proteins. In this paper, we propose a generative model to automatically design the CDRs of antibodies with enhanced binding... | Jeremy Wohlwend, Regina Barzilay, Tommi S. Jaakkola, Wengong Jin |  |
| 1022 |  |  [Churn Reduction via Distillation](https://openreview.net/forum?id=HbtFCX2PLq0) |  | 0 | In real-world systems, models are frequently updated as more data becomes available, and in addition to achieving high accuracy, the goal is to also maintain a low difference in predictions compared to the base model (i.e. predictive churn). If model retraining results in vastly different behavior, then it could cause negative effects in downstream systems, especially... | Afshin Rostamizadeh, Andrew Cotter, Dara Bahri, Harikrishna Narasimhan, Heinrich Jiang |  |
| 1023 |  |  [Learning Causal Models from Conditional Moment Restrictions by Importance Weighting](https://openreview.net/forum?id=7twQI5VnC8) |  | 0 | We consider learning causal relationships under conditional moment restrictions. Unlike causal inference under unconditional moment restrictions, conditional moment restrictions pose serious challenges for causal inference. To address this issue, we propose a method that transforms conditional moment restrictions to unconditional moment restrictions through importance... | Haruo Kakehi, Kenichiro McAlinn, Masaaki Imaizumi, Masahiro Kato, Shota Yasui |  |
| 1024 |  |  [Scalable One-Pass Optimisation of High-Dimensional Weight-Update Hyperparameters by Implicit Differentiation](https://openreview.net/forum?id=hfU7Ka5cfrC) |  | 0 | Machine learning training methods depend plentifully and intricately on hyperparameters, motivating automated strategies for their optimisation. Many existing algorithms restart training for each new hyperparameter choice, at considerable computational cost. Some hypergradient-based one-pass methods exist, but these either cannot be applied to arbitrary optimiser... | Elre Talea Oldewage, José Miguel HernándezLobato, Ross M. Clarke |  |
| 1025 |  |  [Sample Efficient Deep Reinforcement Learning via Uncertainty Estimation](https://openreview.net/forum?id=vrW3tvDfOJQ) |  | 0 | In model-free deep reinforcement learning (RL) algorithms, using noisy value estimates to supervise policy evaluation and optimization is detrimental to the sample efficiency. As this noise is heteroscedastic, its effects can be mitigated using uncertainty-based weights in the optimization process. Previous methods rely on sampled ensembles, which do not capture all... | Kaustubh Mani, Liam Paull, Vincent Mai |  |
| 1026 |  |  [Learning Pruning-Friendly Networks via Frank-Wolfe: One-Shot, Any-Sparsity, And No Retraining](https://openreview.net/forum?id=O1DEtITim__) |  | 0 | We present a novel framework to train a large deep neural network (DNN) for only $\textit{once}$, which can then be pruned to $\textit{any sparsity ratio}$ to preserve competitive accuracy $\textit{without any re-training}$. Conventional methods often require (iterative) pruning followed by re-training, which not only incurs large overhead beyond the original DNN... | Dong Liu, Lu Miao, Tianlong Chen, Wuyang Chen, Xiaolong Luo, Zhangyang Wang |  |
| 1027 |  |  [Learning transferable motor skills with hierarchical latent mixture policies](https://openreview.net/forum?id=qTHBE7E9iej) |  | 0 | For robots operating in the real world, it is desirable to learn reusable abstract behaviours that can effectively be transferred across numerous tasks and scenarios. We propose an approach to learn skills from data using a hierarchical mixture latent variable model. Our method exploits a multi-level hierarchy of both discrete and continuous latent variables, to model a... | Dhruva Tirumala, Dushyant Rao, Fereshteh Sadeghi, Giulia Vezzani, Josh Merel, Leonard Hasenclever, Markus Wulfmeier, Martina Zambelli, Nicolas Heess, Raia Hadsell, Yusuf Aytar |  |
| 1028 |  |  [Compositional Training for End-to-End Deep AUC Maximization](https://openreview.net/forum?id=gPvB4pdu_Z) |  | 0 | Recently, deep AUC maximization (DAM) has achieved great success in different domains (e.g., medical image classification). However, the end-to-end training for deep AUC maximization still remains a challenging problem. Previous studies employ an ad-hoc two-stage approach that first trains the network by optimizing a traditional loss (e.g., cross-entropy loss) and then... | Nitesh V. Chawla, Tianbao Yang, Zhishuai Guo, Zhuoning Yuan |  |
| 1029 |  |  [Explanations of Black-Box Models based on Directional Feature Interactions](https://openreview.net/forum?id=45Mr7LeKR9) |  | 0 | As machine learning algorithms are deployed ubiquitously to a variety of domains, it is imperative to make these often black-box models transparent. Several recent works explain black-box models by capturing the most influential features for prediction per instance; such explanation methods are univariate, as they characterize importance per feature. We extend... | Aria Masoomi, Craig P. Hersh, Davin Hill, Edwin K. Silverman, Jennifer G. Dy, Peter J. Castaldi, Stratis Ioannidis, Zhonghui Xu |  |
| 1030 |  |  [On Lottery Tickets and Minimal Task Representations in Deep Reinforcement Learning](https://openreview.net/forum?id=Fl3Mg_MZR-) |  | 0 | The lottery ticket hypothesis questions the role of overparameterization in supervised deep learning. But how is the performance of winning lottery tickets affected by the distributional shift inherent to reinforcement learning problems? In this work, we address this question by comparing sparse agents who have to address the non-stationarity of the... | Henning Sprekeler, Marc Aurel Vischer, Robert Tjarko Lange |  |
| 1031 |  |  [Self-supervised Learning is More Robust to Dataset Imbalance](https://openreview.net/forum?id=4AZz9osqrar) |  | 0 | Self-supervised learning (SSL) is a scalable way to learn general visual representations since it learns without labels. However, large-scale unlabeled datasets in the wild often have long-tailed label distributions, where we know little about the behavior of SSL. In this work, we systematically investigate self-supervised learning under dataset imbalance. First, we... | Adrien Gaidon, Hong Liu, Jeff Z. HaoChen, Tengyu Ma |  |
| 1032 |  |  [Ab-Initio Potential Energy Surfaces by Pairing GNNs with Neural Wave Functions](https://openreview.net/forum?id=apv504XsysP) |  | 0 | Solving the Schrödinger equation is key to many quantum mechanical properties. However, an analytical solution is only tractable for single-electron systems. Recently, neural networks succeeded at modelling wave functions of many-electron systems. Together with the variational Monte-Carlo (VMC) framework, this led to solutions on par with the best known classical... | Nicholas Gao, Stephan Günnemann |  |
| 1033 |  |  [Near-Optimal Reward-Free Exploration for Linear Mixture MDPs with Plug-in Solver](https://openreview.net/forum?id=SidzxAb9k30) |  | 0 | Although model-based reinforcement learning (RL) approaches are considered more sample efficient, existing algorithms are usually relying on sophisticated planning algorithm to couple tightly with the model-learning procedure. Hence the learned models may lack the ability of being re-used with more specialized planners. In this paper we address this issue and provide... | Jiachen Hu, Lin Yang, Liwei Wang, Xiaoyu Chen |  |
| 1034 |  |  [Meta Discovery: Learning to Discover Novel Classes given Very Limited Data](https://openreview.net/forum?id=MEpKGLsY8f) |  | 0 | In novel class discovery (NCD), we are given labeled data from seen classes and unlabeled data from unseen classes, and we train clustering models for the unseen classes. However, the implicit assumptions behind NCD are still unclear. In this paper, we demystify assumptions behind NCD and find that high-level semantic features should be shared among the seen and unseen... | Bo Han, Feng Liu, Gang Niu, Haoang Chi, Long Lan, Masashi Sugiyama, Mingyuan Zhou, Tongliang Liu, Wenjing Yang |  |
| 1035 |  |  [Constrained Policy Optimization via Bayesian World Models](https://openreview.net/forum?id=PRZoSmCinhf) |  | 0 | Improving sample-efficiency and safety are crucial challenges when deploying reinforcement learning in high-stakes real world applications. We propose LAMBDA, a novel model-based approach for policy optimization in safety critical tasks modeled via constrained Markov decision processes. Our approach utilizes Bayesian world models, and harnesses the resulting uncertainty... | Andreas Krause, Ilnura Usmanova, Sebastian Curi, Yarden As |  |
| 1036 |  |  [VAE Approximation Error: ELBO and Exponential Families](https://openreview.net/forum?id=OIs3SxU5Ynl) |  | 0 | The importance of Variational Autoencoders reaches far beyond standalone generative models -- the approach is also used for learning latent representations and can be generalized to semi-supervised learning. This requires a thorough analysis of their commonly known shortcomings: posterior collapse and approximation errors. This paper analyzes VAE approximation errors... | Alexander Shekhovtsov, Boris Flach, Dmitrij Schlesinger |  |
| 1037 |  |  [Generalized Decision Transformer for Offline Hindsight Information Matching](https://openreview.net/forum?id=CAjxVodl_v) |  | 0 | How to extract as much learning signal from each trajectory data has been a key problem in reinforcement learning (RL), where sample inefficiency has posed serious challenges for practical applications. Recent works have shown that using expressive policy function approximators and conditioning on future trajectory information -- such as future states in hindsight... | Hiroki Furuta, Shixiang Shane Gu, Yutaka Matsuo |  |
| 1038 |  |  [Unifying Likelihood-free Inference with Black-box Optimization and Beyond](https://openreview.net/forum?id=1HxTO6CTkz) |  | 0 | Black-box optimization formulations for biological sequence design have drawn recent attention due to their promising potential impact on the pharmaceutical industry. In this work, we propose to unify two seemingly distinct worlds: likelihood-free inference and black-box optimization, under one probabilistic framework. In tandem, we provide a recipe for constructing... | Aaron C. Courville, Dinghuai Zhang, Jie Fu, Yoshua Bengio |  |
| 1039 |  |  [DEPTS: Deep Expansion Learning for Periodic Time Series Forecasting](https://openreview.net/forum?id=AJAR-JgNw__) |  | 0 | Periodic time series (PTS) forecasting plays a crucial role in a variety of industries to foster critical tasks, such as early warning, pre-planning, resource scheduling, etc. However, the complicated dependencies of the PTS signal on its inherent periodicity as well as the sophisticated composition of various periods hinder the performance of PTS forecasting. In this... | Jiang Bian, Shun Zheng, TieYan Liu, Wei Cao, Wei Fan, Xiaohan Yi, Yanjie Fu |  |
| 1040 |  |  [Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions](https://openreview.net/forum?id=tBtoZYKd9n) |  | 0 | Graph generative models are a highly active branch of machine learning. Given the steady development of new models of ever-increasing complexity, it is necessary to provide a principled way to evaluate and compare them. In this paper, we enumerate the desirable criteria for such a comparison metric and provide an overview of the status quo of graph generative model... | Bastian Rieck, Karsten M. Borgwardt, Leslie O'Bray, Max Horn |  |
| 1041 |  |  [Natural Posterior Network: Deep Bayesian Predictive Uncertainty for Exponential Family Distributions](https://openreview.net/forum?id=tV3N0DWMxCg) |  | 0 | Uncertainty awareness is crucial to develop reliable machine learning models. In this work, we propose the Natural Posterior Network (NatPN) for fast and high-quality uncertainty estimation for any task where the target distribution belongs to the exponential family. Thus, NatPN finds application for both classification and general regression settings. Unlike many... | Bertrand Charpentier, Daniel Zügner, Oliver Borchert, Simon Geisler, Stephan Günnemann |  |
| 1042 |  |  [Learning Altruistic Behaviours in Reinforcement Learning without External Rewards](https://openreview.net/forum?id=KxbhdyiPHE) |  | 0 | Can artificial agents learn to assist others in achieving their goals without knowing what those goals are? Generic reinforcement learning agents could be trained to behave altruistically towards others by rewarding them for altruistic behaviour, i.e., rewarding them for benefiting other agents in a given situation. Such an approach assumes that other agents' goals are... | João F. Henriques, Mateusz Malinowski, Tim Franzmeyer |  |
| 1043 |  |  [Context-Aware Sparse Deep Coordination Graphs](https://openreview.net/forum?id=wQfgfb8VKTn) |  | 0 | Learning sparse coordination graphs adaptive to the coordination dynamics among agents is a long-standing problem in cooperative multi-agent learning. This paper studies this problem and proposes a novel method using the variance of payoff functions to construct context-aware sparse coordination topologies. We theoretically consolidate our method by proving that the... | Chongjie Zhang, Liang Zeng, Qianlan Yang, Tonghan Wang, Weijun Dong, Yang Yu |  |
| 1044 |  |  [On the approximation properties of recurrent encoder-decoder architectures](https://openreview.net/forum?id=xDIvIqQ3DXD) |  | 0 | Encoder-decoder architectures have recently gained popularity in sequence to sequence modelling, featuring in state-of-the-art models such as transformers. However, a mathematical understanding of their working principles still remains limited. In this paper, we study the approximation properties of recurrent encoder-decoder architectures. Prior work established... | Haotian Jiang, Qianxiao Li, Zhong Li |  |
| 1045 |  |  [Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models](https://openreview.net/forum?id=Nfl-iXa-y7R) |  | 0 | Overparameterized neural networks generalize well but are expensive to train. Ideally one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty... | Atri Rudra, Beidi Chen, Christopher Ré, Jiaming Yang, Kaizhao Liang, Tri Dao, Zhao Song |  |
| 1046 |  |  [8-bit Optimizers via Block-wise Quantization](https://openreview.net/forum?id=shpkpVXzo3h) |  | 0 | Stateful optimizers maintain gradient statistics over time, e.g., the exponentially smoothed sum (SGD with momentum) or squared sum (Adam) of past gradient values. This state can be used to accelerate optimization significantly, compared to plain stochastic gradient descent, but uses memory that might otherwise be allocated to model parameters, thereby limiting the... | Luke Zettlemoyer, Mike Lewis, Sam Shleifer, Tim Dettmers |  |
| 1047 |  |  [Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks](https://openreview.net/forum?id=yeP_zx9vqNm) |  | 0 | Recent work suggests that feature constraints in the training datasets of deep neural networks (DNNs) drive robustness to adversarial noise (Ilyas et al., 2019). The representations learned by such adversarially robust networks have also been shown to be more human perceptually-aligned than non-robust networks via image manipulations (Santurkar et al., 2019, Engstrom et... | Anne Harrington, Arturo Deza |  |
| 1048 |  |  [Omni-Dimensional Dynamic Convolution](https://openreview.net/forum?id=DmpCfq6Mg39) |  | 0 | Learning a single static convolutional kernel in each convolutional layer is the common training paradigm of modern Convolutional Neural Networks (CNNs). Instead, recent research in dynamic convolution shows that learning a linear combination of n convolutional kernels weighted with their input-dependent attentions can significantly improve the accuracy of light-weight... | Anbang Yao, Aojun Zhou, Chao Li |  |
| 1049 |  |  [EViT: Expediting Vision Transformers via Token Reorganizations](https://openreview.net/forum?id=BjyvwnXXVn_) |  | 0 | Vision Transformers (ViTs) take all the image patches as tokens and construct multi-head self-attention (MHSA) among them. Complete leverage of these image tokens brings redundant computations since not all the tokens are attentive in MHSA. Examples include that tokens containing semantically meaningless or distractive image backgrounds do not positively contribute to... | Chongjian Ge, Jue Wang, Pengtao Xie, Yibing Song, Youwei Liang, Zhan Tong |  |
| 1050 |  |  [D-CODE: Discovering Closed-form ODEs from Observed Trajectories](https://openreview.net/forum?id=wENMvIsxNN) |  | 0 | For centuries, scientists have manually designed closed-form ordinary differential equations (ODEs) to model dynamical systems. An automated tool to distill closed-form ODEs from observed trajectories would accelerate the modeling process. Traditionally, symbolic regression is used to uncover a closed-form prediction function $a=f(b)$ with label-feature pairs $(a_i,... | Krzysztof Kacprzyk, Mihaela van der Schaar, Zhaozhi Qian |  |
| 1051 |  |  [Spanning Tree-based Graph Generation for Molecules](https://openreview.net/forum?id=w60btE_8T2m) |  | 0 | In this paper, we explore the problem of generating molecules using deep neural networks, which has recently gained much interest in chemistry. To this end, we propose a spanning tree-based graph generation (STGG) framework based on formulating molecular graph generation as a construction of a spanning tree and the residual edges. Such a formulation exploits the... | Binghong Chen, Le Song, Sungsoo Ahn, Tianzhe Wang |  |
| 1052 |  |  [Policy improvement by planning with Gumbel](https://openreview.net/forum?id=bERaNdoegnO) |  | 0 | AlphaZero is a powerful reinforcement learning algorithm based on approximate policy iteration and tree search. However, AlphaZero can fail to improve its policy network, if not visiting all actions at the root of a search tree. To address this issue, we propose a policy improvement algorithm based on sampling actions without replacement. Furthermore, we use the idea of... | Arthur Guez, David Silver, Ivo Danihelka, Julian Schrittwieser |  |
| 1053 |  |  [Learning Optimal Conformal Classifiers](https://openreview.net/forum?id=t8O-4LKFVx) |  | 0 | Modern deep learning based classifiers show very high accuracy on test data but this does not provide sufficient guarantees for safe deployment, especially in high-stake AI applications such as medical diagnosis. Usually, predictions are obtained without a reliable uncertainty estimate or a formal guarantee. Conformal prediction (CP) addresses these issues by using the... | Ali Taylan Cemgil, Arnaud Doucet, David Stutz, Krishnamurthy Dvijotham |  |
| 1054 |  |  [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://openreview.net/forum?id=9Vrb9D0WI4) |  | 0 | Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models’ pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To... | Abheesht Sharma, Albert Webson, Alexander M. Rush, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Canwen Xu, Colin Raffel, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Jason Alan Fries, Jonathan Chang, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Manan Dey, Matteo Manica, Mike TianJian Jiang, Nihal V. Nayak, Rachel Bawden, Ryan Teehan, Shanya Sharma Sharma, Sheng Shen, Stella Biderman, Stephen H. Bach, Taewoon Kim, Teven Le Scao, Thibault Févry, Thomas Wang, Thomas Wolf, Trishala Neeraj, Urmish Thakker, Victor Sanh, Zaid Alyafeai, Zheng Xin Yong |  |
| 1055 |  |  [Continuous-Time Meta-Learning with Forward Mode Differentiation](https://openreview.net/forum?id=57PipS27Km) |  | 0 | Drawing inspiration from gradient-based meta-learning methods with infinitely small gradient steps, we introduce Continuous-Time Meta-Learning (COMLN), a meta-learning algorithm where adaptation follows the dynamics of a gradient vector field. Specifically, representations of the inputs are meta-learned such that a task-specific linear classifier is obtained as a... | David Kanaa, Giancarlo Kerg, Guillaume Lajoie, Leo Feng, PierreLuc Bacon, Tristan Deleu, Yoshua Bengio |  |
| 1056 |  |  [On the relation between statistical learning and perceptual distances](https://openreview.net/forum?id=zXM0b4hi5_B) |  | 0 | It has been demonstrated many times that the behavior of the human visual system is connected to the statistics of natural images. Since machine learning relies on the statistics of training data as well, the above connection has interesting implications when using perceptual distances (which mimic the behavior of the human visual system) as a loss function. In this... | Alexander Hepburn, Jesus Malo, Johannes Ballé, Raúl SantosRodríguez, Valero Laparra |  |
| 1057 |  |  [Implicit Bias of Projected Subgradient Method Gives Provable Robust Recovery of Subspaces of Unknown Codimension](https://openreview.net/forum?id=vA7doMdgi75) |  | 0 | Robust subspace recovery (RSR) is the problem of learning a subspace from sample data points corrupted by outliers. Dual Principal Component Pursuit (DPCP) is a robust subspace recovery method that aims to find a basis for the orthogonal complement of the subspace by minimizing the sum of the distances of the points to the subspaces subject to orthogonality constraints... | Benjamin David Haeffele, Paris Giampouras, René Vidal |  |
| 1058 |  |  [On the Optimal Memorization Power of ReLU Neural Networks](https://openreview.net/forum?id=MkTPtnjeYTV) |  | 0 | We study the memorization power of feedforward ReLU neural networks. We show that such networks can memorize any $N$ points that satisfy a mild separability assumption using $\tilde{O}\left(\sqrt{N}\right)$ parameters. Known VC-dimension upper bounds imply that memorizing $N$ samples requires $\Omega(\sqrt{N})$ parameters, and hence our construction is optimal up to... | Gal Vardi, Gilad Yehudai, Ohad Shamir |  |
| 1059 |  |  [Programmatic Reinforcement Learning without Oracles](https://openreview.net/forum?id=6Tk2noBdvxt) |  | 0 | Deep reinforcement learning (RL) has led to encouraging successes in many challenging control tasks. However, a deep RL model lacks interpretability due to the difficulty of identifying how the model's control logic relates to its network structure. Programmatic policies structured in more interpretable representations emerge as a promising solution. Yet two... | He Zhu, Wenjie Qiu |  |
| 1060 |  |  [When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations](https://openreview.net/forum?id=LtKcMgGOeLt) |  | 0 | Vision Transformers (ViTs) and MLPs signal further efforts on replacing hand-wired features or inductive biases with general-purpose neural architectures. Existing works empower the models by massive data, such as large-scale pre-training and/or repeated strong data augmentations, and still report optimization-related problems (e.g., sensitivity to initialization and... | Boqing Gong, ChoJui Hsieh, Xiangning Chen |  |
| 1061 |  |  [Online Hyperparameter Meta-Learning with Hypergradient Distillation](https://openreview.net/forum?id=01AMRlen9wJ) |  | 0 | Many gradient-based meta-learning methods assume a set of parameters that do not participate in inner-optimization, which can be considered as hyperparameters. Although such hyperparameters can be optimized using the existing gradient-based hyperparameter optimization (HO) methods, they suffer from the following issues. Unrolled differentiation methods do not scale well... | Eunho Yang, Haebeom Lee, Hayeon Lee, Jaewoong Shin, Sung Ju Hwang, Timothy M. Hospedales |  |
| 1062 |  |  [Tighter Sparse Approximation Bounds for ReLU Neural Networks](https://openreview.net/forum?id=LBvk4QWIUpm) |  | 0 | A well-known line of work (Barron, 1993; Breiman, 1993; Klusowski & Barron, 2018) provides bounds on the width $n$ of a ReLU two-layer neural network needed to approximate a function $f$ over the ball $\mathcal{B}_R(\mathbb{R}^d)$ up to error $\epsilon$, when the Fourier based quantity $C_f = \int_{\mathbb{R}^d} \\|\xi\\|^2 \|\hat{f}(\xi)\| \ d\xi$ is finite. More... | Carles DomingoEnrich, Youssef Mroueh |  |
| 1063 |  |  [Long Expressive Memory for Sequence Modeling](https://openreview.net/forum?id=vwj6aUeocyf) |  | 0 | We propose a novel method called Long Expressive Memory (LEM) for learning long-term sequential dependencies. LEM is gradient-based, it can efficiently process sequential tasks with very long-term dependencies, and it is sufficiently expressive to be able to learn complicated input-output maps. To derive LEM, we consider a system of multiscale ordinary differential... | Michael W. Mahoney, N. Benjamin Erichson, Siddhartha Mishra, T. Konstantin Rusch |  |
| 1064 |  |  [Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy](https://openreview.net/forum?id=LzQQ89U1qm_) |  | 0 | Unsupervised detection of anomaly points in time series is a challenging problem, which requires the model to derive a distinguishable criterion. Previous methods tackle the problem mainly through learning pointwise representation or pairwise association, however, neither is sufficient to reason about the intricate dynamics. Recently, Transformers have shown great power... | Haixu Wu, Jianmin Wang, Jiehui Xu, Mingsheng Long |  |
| 1065 |  |  [Progressive Distillation for Fast Sampling of Diffusion Models](https://openreview.net/forum?id=TIdIXIpzhoI) |  | 0 | Diffusion models have recently shown great promise for generative modeling, outperforming GANs on perceptual quality and autoregressive models at density estimation. A remaining downside is their slow sampling time: generating high quality samples takes many hundreds or thousands of model evaluations. Here we make two contributions to help eliminate this downside:... | Jonathan Ho, Tim Salimans |  |
| 1066 |  |  [A General Analysis of Example-Selection for Stochastic Gradient Descent](https://openreview.net/forum?id=7gWSJrP3opB) |  | 0 | Training example order in SGD has long been known to affect convergence rate. Recent results show that accelerated rates are possible in a variety of cases for permutation-based sample orders, in which each example from the training set is used once before any example is reused. In this paper, we develop a broad condition on the sequence of examples used by SGD that is... | Christopher De Sa, Si Yi Meng, Yucheng Lu |  |
| 1067 |  |  [Assessing Generalization of SGD via Disagreement](https://openreview.net/forum?id=WvOGCEAQhxl) |  | 0 | We empirically show that the test error of deep networks can be estimated by training the same architecture on the same training set but with two different runs of Stochastic Gradient Descent (SGD), and then measuring the disagreement rate between the two networks on unlabeled test data. This builds on -- and is a stronger version of -- the observation in... | Christina Baek, J. Zico Kolter, Vaishnavh Nagarajan, Yiding Jiang |  |
| 1068 |  |  [Generative Planning for Temporally Coordinated Exploration in Reinforcement Learning](https://openreview.net/forum?id=YZHES8wIdE) |  | 0 | Standard model-free reinforcement learning algorithms optimize a policy that generates the action to be taken in the current time step in order to maximize expected future return. While flexible, it faces difficulties arising from the inefficient exploration due to its single step nature. In this work, we present Generative Planning method (GPM), which can generate... | Haichao Zhang, Haonan Yu, Wei Xu |  |
| 1069 |  |  [Pessimistic Bootstrapping for Uncertainty-Driven Offline Reinforcement Learning](https://openreview.net/forum?id=Y4cs1Z3HnqL) |  | 0 | Offline Reinforcement Learning (RL) aims to learn policies from previously collected datasets without exploring the environment. Directly applying off-policy algorithms to offline RL usually fails due to the extrapolation error caused by the out-of-distribution (OOD) actions. Previous methods tackle such problem by penalizing the Q-values of OOD actions or constraining... | Animesh Garg, Chenjia Bai, Lingxiao Wang, Peng Liu, Zhaoran Wang, ZhiHong Deng, Zhuoran Yang |  |
| 1070 |  |  [Equivariant Subgraph Aggregation Networks](https://openreview.net/forum?id=dFbKQaRk15w) |  | 0 | Message-passing neural networks (MPNNs) are the leading architecture for deep learning on graph-structured data, in large part due to their simplicity and scalability. Unfortunately, it was shown that these architectures are limited in their expressive power. This paper proposes a novel framework called Equivariant Subgraph Aggregation Networks (ESAN) to address this... | Balasubramaniam Srinivasan, Beatrice Bevilacqua, Chen Cai, Derek Lim, Fabrizio Frasca, Gopinath Balamurugan, Haggai Maron, Michael M. Bronstein |  |
| 1071 |  |  [How Do Vision Transformers Work?](https://openreview.net/forum?id=D78Go4hVcxO) |  | 0 | The success of multi-head self-attentions (MSAs) for computer vision is now indisputable. However, little is known about how MSAs work. We present fundamental explanations to help better understand the nature of MSAs. In particular, we demonstrate the following properties of MSAs and Vision Transformers (ViTs): (1) MSAs improve not only accuracy but also generalization... | Namuk Park, Songkuk Kim |  |
| 1072 |  |  [Variational methods for simulation-based inference](https://openreview.net/forum?id=kZ0UYdhqkNY) |  | 0 | We present Sequential Neural Variational Inference (SNVI), an approach to perform Bayesian inference in models with intractable likelihoods. SNVI combines likelihood-estimation (or likelihood-ratio-estimation) with variational inference to achieve a scalable simulation-based inference approach. SNVI maintains the flexibility of likelihood(-ratio) estimation to allow... | Jakob H. Macke, Manuel Glöckler, Michael Deistler |  |
| 1073 |  |  [Tackling the Generative Learning Trilemma with Denoising Diffusion GANs](https://openreview.net/forum?id=JprM0p-q0Co) |  | 0 | A wide variety of deep generative models has been developed in the past decade. Yet, these models often struggle with simultaneously addressing three key requirements including: high sample quality, mode coverage, and fast sampling. We call the challenge imposed by these requirements the generative learning trilemma, as the existing models often trade some of them for... | Arash Vahdat, Karsten Kreis, Zhisheng Xiao |  |
| 1074 |  |  [Imbedding Deep Neural Networks](https://openreview.net/forum?id=yKIAXjkJc2F) |  | 0 | Continuous-depth neural networks, such as Neural ODEs, have refashioned the understanding of residual neural networks in terms of non-linear vector-valued optimal control problems. The common solution is to use the adjoint sensitivity method to replicate a forward-backward pass optimisation problem. We propose a new approach which explicates the network's \`depth' as a... | Andrew Corbett, Dmitry Kangin |  |
| 1075 |  |  [Understanding and Preventing Capacity Loss in Reinforcement Learning](https://openreview.net/forum?id=ZkC8wKoLbQ7) |  | 0 | The reinforcement learning (RL) problem is rife with sources of non-stationarity that can destabilize or inhibit learning progress. We identify a key mechanism by which this occurs in agents using neural networks as function approximators: \textit{capacity loss}, whereby networks trained to predict a sequence of target values lose their ability to quickly fit new... | Clare Lyle, Mark Rowland, Will Dabney |  |
| 1076 |  |  [Source-Free Adaptation to Measurement Shift via Bottom-Up Feature Restoration](https://openreview.net/forum?id=1JDiK_TbV4S) |  | 0 | Source-free domain adaptation (SFDA) aims to adapt a model trained on labelled data in a source domain to unlabelled data in a target domain without access to the source-domain data during adaptation. Existing methods for SFDA leverage entropy-minimization techniques which: (i) apply only to classification; (ii) destroy model calibration; and (iii) rely on the source... | Bernhard Schölkopf, Christopher K. I. Williams, Cian Eastwood, Ian Mason |  |
| 1077 |  |  [The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design](https://openreview.net/forum?id=lnEaqbTJIRz) |  | 0 | Pretraining Neural Language Models (NLMs) over a large corpus involves chunking the text into training examples, which are contiguous text segments of sizes processable by the neural architecture. We highlight a bias introduced by this common practice: we prove that the pretrained NLM can model much stronger dependencies between text segments that appeared in the same... | Amnon Shashua, Dan Navon, Daniel Jannai, Noam Wies, Yedid Hoshen, Yoav Levine |  |
| 1078 |  |  [Emergent Communication at Scale](https://openreview.net/forum?id=AUGBfDIV9rL) |  | 0 | Emergent communication aims for a better understanding of human language evolution and building more efficient representations. We posit that reaching these goals will require scaling up, in contrast to a significant amount of literature that focuses on setting up small-scale problems to tease out desired properties of the emergent languages. We focus on three... | Angeliki Lazaridou, Bilal Piot, Corentin Tallec, Elnaz Davoodi, Eugene Tarassov, Florent Altché, Florian Strub, Kory Wallace Mathewson, Olivier Tieleman, Rahma Chaabouni |  |
| 1079 |  |  [Superclass-Conditional Gaussian Mixture Model For Learning Fine-Grained Embeddings](https://openreview.net/forum?id=vds4SNooOe) |  | 0 | Learning fine-grained embeddings is essential for extending the generalizability of models pre-trained on "coarse" labels (e.g., animals). It is crucial to fields for which fine-grained labeling (e.g., breeds of animals) is expensive, but fine-grained prediction is desirable, such as medicine. The dilemma necessitates adaptation of a "coarsely" pre-trained model to new... | Haifeng Chen, Jingchao Ni, Sho Kato, Takayoshi Asakura, Tomoya Soma, Wei Cheng, Zhengzhang Chen |  |
| 1080 |  |  [SHINE: SHaring the INverse Estimate from the forward pass for bi-level optimization and implicit models](https://openreview.net/forum?id=-ApAkox5mp) |  | 0 | In recent years, implicit deep learning has emerged as a method to increase the depth of deep neural networks. While their training is memory-efficient, they are still significantly slower to train than their explicit counterparts. In Deep Equilibrium Models~(DEQs), the training is performed as a bi-level problem, and its computational complexity is partially driven by... | Florian Mannel, JeanLuc Starck, Philippe Ciuciu, Shaojie Bai, Thomas Moreau, Zaccharie Ramzi |  |
| 1081 |  |  [Towards Deployment-Efficient Reinforcement Learning: Lower Bound and Optimality](https://openreview.net/forum?id=ccWaPGl9Hq) |  | 0 | Deployment efficiency is an important criterion for many real-world applications of reinforcement learning (RL). Despite the community's increasing interest, there lacks a formal theoretical formulation for the problem. In this paper, we propose such a formulation for deployment-efficient RL (DE-RL) from an ''optimization with constraints'' perspective: we are... | Jiawei Huang, Jinglin Chen, Li Zhao, Nan Jiang, Tao Qin, TieYan Liu |  |
| 1082 |  |  [Task Relatedness-Based Generalization Bounds for Meta Learning](https://openreview.net/forum?id=A3HHaEdqAJL) |  | 0 | Supposing the $n$ training tasks and the new task are sampled from the same environment, traditional meta learning theory derives an error bound on the expected loss over the new task in terms of the empirical training loss, uniformly over the set of all hypothesis spaces. However, there is still little research on how the relatedness of these tasks can affect the full... | Jiechao Guan, Zhiwu Lu |  |
| 1083 |  |  [IntSGD: Adaptive Floatless Compression of Stochastic Gradients](https://openreview.net/forum?id=pFyXqxChZc) |  | 0 | We propose a family of adaptive integer compression operators for distributed Stochastic Gradient Descent (SGD) that do not communicate a single float. This is achieved by multiplying floating-point vectors with a number known to every device and then rounding to integers. In contrast to the prior work on integer compression for SwitchML by (Sapio et al., 2021), our... | Bokun Wang, Dmitry Kovalev, Konstantin Mishchenko, Peter Richtárik |  |
| 1084 |  |  [PAC-Bayes Information Bottleneck](https://openreview.net/forum?id=iLHOIDsPv1P) |  | 0 | Understanding the source of the superior generalization ability of NNs remains one of the most important problems in ML research. There have been a series of theoretical works trying to derive non-vacuous bounds for NNs. Recently, the compression of information stored in weights (IIW) is proved to play a key role in NNs generalization based on the PAC-Bayes theorem.... | Ercan Engin Kuruoglu, Jimeng Sun, ShaoLun Huang, Xi Chen, Yefeng Zheng, Zifeng Wang |  |
| 1085 |  |  [Byzantine-Robust Learning on Heterogeneous Datasets via Bucketing](https://openreview.net/forum?id=jXKKDEi5vJt) |  | 0 | In Byzantine robust distributed or federated learning, a central server wants to train a machine learning model over data distributed across multiple workers. However, a fraction of these workers may deviate from the prescribed algorithm and send arbitrary messages. While this problem has received significant attention recently, most current defenses assume that the... | Lie He, Martin Jaggi, Sai Praneeth Karimireddy |  |
| 1086 |  |  [Understanding the Role of Self Attention for Efficient Speech Recognition](https://openreview.net/forum?id=AvcfxqRy4Y) |  | 0 | Self-attention (SA) is a critical component of Transformer neural networks that have succeeded in automatic speech recognition (ASR). In this paper, we analyze the role of SA in Transformer-based ASR models for not only understanding the mechanism of improved recognition accuracy but also lowering the computational complexity. We reveal that SA performs two distinct... | Jungwook Choi, Kyuhong Shim, Wonyong Sung |  |
| 1087 |  |  [Label Encoding for Regression Networks](https://openreview.net/forum?id=8WawVDdKqlL) |  | 0 | Deep neural networks are used for a wide range of regression problems. However, there exists a significant gap in accuracy between specialized approaches and generic direct regression in which a network is trained by minimizing the squared or absolute error of output labels. Prior work has shown that solving a regression problem with a set of binary classifiers can... | Deval Shah, Tor M. Aamodt, Zi Yu Xue |  |
| 1088 |  |  [Equivariant Transformers for Neural Network based Molecular Potentials](https://openreview.net/forum?id=zNHzqZ9wrRB) |  | 0 | The prediction of quantum mechanical properties is historically plagued by a trade-off between accuracy and speed. Machine learning potentials have previously shown great success in this domain, reaching increasingly better accuracy while maintaining computational efficiency comparable with classical force fields. In this work we propose TorchMD-NET, a novel equivariant... | Gianni De Fabritiis, Philipp Thölke |  |
| 1089 |  |  [SGD Can Converge to Local Maxima](https://openreview.net/forum?id=9XhPLAjjRB) |  | 0 | Previous works on stochastic gradient descent (SGD) often focus on its success. In this work, we construct worst-case optimization problems illustrating that, when not in the regimes that the previous works often assume, SGD can exhibit many strange and potentially undesirable behaviors. Specifically, we construct landscapes and data distributions such that (1) SGD... | Botao Li, James B. Simon, Liu Ziyin, Masahito Ueda |  |
| 1090 |  |  [Hybrid Local SGD for Federated Learning with Heterogeneous Communications](https://openreview.net/forum?id=H0oaWl6THa) |  | 0 | Communication is a key bottleneck in federated learning where a large number of edge devices collaboratively learn a model under the orchestration of a central server without sharing their own training data. While local SGD has been proposed to reduce the number of FL rounds and become the algorithm of choice for FL, its total communication cost is still prohibitive... | Rui Hu, Yanmin Gong, Ying Sun, Yuanxiong Guo |  |
| 1091 |  |  [Increasing the Cost of Model Extraction with Calibrated Proof of Work](https://openreview.net/forum?id=EAy7C1cgE1L) |  | 0 | In model extraction attacks, adversaries can steal a machine learning model exposed via a public API by repeatedly querying it and adjusting their own model based on obtained predictions. To prevent model stealing, existing defenses focus on detecting malicious queries, truncating, or distorting outputs, thus necessarily introducing a tradeoff between robustness and... | Adam Dziedzic, Muhammad Ahmad Kaleem, Nicolas Papernot, Yu Shen Lu |  |
| 1092 |  |  [Learning the Dynamics of Physical Systems from Sparse Observations with Finite Element Networks](https://openreview.net/forum?id=HFmAukZ-k-2) |  | 0 | We propose a new method for spatio-temporal forecasting on arbitrarily distributed points. Assuming that the observed system follows an unknown partial differential equation, we derive a continuous-time model for the dynamics of the data via the finite element method. The resulting graph neural network estimates the instantaneous effects of the unknown dynamics on each... | Marten Lienen, Stephan Günnemann |  |
| 1093 |  |  [Probabilistic Implicit Scene Completion](https://openreview.net/forum?id=BnQhMqDfcKG) |  | 0 | We propose a probabilistic shape completion method extended to the continuous geometry of large-scale 3D scenes. Real-world scans of 3D scenes suffer from a considerable amount of missing data cluttered with unsegmented objects. The problem of shape completion is inherently ill-posed, and high-quality result requires scalable solutions that consider multiple possible... | Changwoon Choi, Dongsu Zhang, Inbum Park, Young Min Kim |  |
| 1094 |  |  [Improving Federated Learning Face Recognition via Privacy-Agnostic Clusters](https://openreview.net/forum?id=7l1IjZVddDW) |  | 0 | The growing public concerns on data privacy in face recognition can be partly relieved by the federated learning (FL) paradigm. However, conventional FL methods usually perform poorly due to the particularity of the task, \textit{i.e.}, broadcasting class centers among clients is essential for recognition performances but leads to privacy leakage. To resolve the... | Feng Zhou, Guochao Liu, Hainan Ren, Qiang Meng, Tianshu Feng, Yuanqing Lin |  |
