# ICLR2022

## 会议论文列表

本会议共有 1094 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [Domino: Discovering Systematic Errors with Cross-Modal Embeddings](https://openreview.net/forum?id=FPCMqjI0jXN) |  | 0 | Machine learning models that achieve high overall accuracy often make systematic errors on important subsets (or slices) of data. Identifying underperforming slices is particularly challenging when working with high-dimensional inputs (e.g. images, audio), where important slices are often unlabeled. In order to address this issue, recent studies have proposed automated slice discovery methods (SDMs), which leverage learned model representations to mine input data for slices on which a model performs poorly. To be useful to a practitioner, these methods must identify slices that are both underperforming and coherent (i.e. united by a human-understandable concept). However, no quantitative evaluation framework currently exists for rigorously assessing SDMs with respect to these criteria. Additionally, prior qualitative evaluations have shown that SDMs often identify slices that are incoherent. In this work, we address these challenges by first designing a principled evaluation framework that enables a quantitative comparison of SDMs across 1,235 slice discovery settings in three input domains (natural images, medical images, and time-series data). Then, motivated by the recent development of powerful cross-modal representation learning approaches, we present Domino, an SDM that leverages cross-modal embeddings and a novel error-aware mixture model to discover and describe coherent slices. We find that Domino accurately identifies 36% of the 1,235 slices in our framework -- a 12 percentage point improvement over prior methods. Further, Domino is the first SDM that can provide natural language descriptions of identified slices, correctly generating the exact name of the slice in 35% of settings. | Sabri Eyuboglu, Maya Varma, Khaled Kamal Saab, JeanBenoit Delbrouck, Christopher LeeMesser, Jared Dunnmon, James Zou, Christopher Ré |  |
| 2 |  |  [Natural Language Descriptions of Deep Visual Features](https://openreview.net/forum?id=NudBMY-tzDr) |  | 0 | Some neurons in deep networks specialize in recognizing highly specific perceptual, structural, or semantic features of inputs. In computer vision, techniques exist for identifying neurons that respond to individual concept categories like colors, textures, and object classes. But these techniques are limited in scope, labeling only a small subset of neurons and behaviors in any network. Is a richer characterization of neuron-level computation possible? We introduce a procedure (called MILAN, for mutual information-guided linguistic annotation of neurons) that automatically labels neurons with open-ended, compositional, natural language descriptions. Given a neuron, MILAN generates a description by searching for a natural language string that maximizes pointwise mutual information with the image regions in which the neuron is active. MILAN produces fine-grained descriptions that capture categorical, relational, and logical structure in learned features. These descriptions obtain high agreement with human-generated feature descriptions across a diverse set of model architectures and tasks, and can aid in understanding and controlling learned models. We highlight three applications of natural language neuron descriptions. First, we use MILAN for analysis, characterizing the distribution and importance of neurons selective for attribute, category, and relational information in vision models. Second, we use MILAN for auditing, surfacing neurons sensitive to human faces in datasets designed to obscure them. Finally, we use MILAN for editing, improving robustness in an image classifier by deleting neurons sensitive to text features spuriously correlated with class labels. | Evan Hernandez, Sarah Schwettmann, David Bau, Teona Bagashvili, Antonio Torralba, Jacob Andreas |  |
| 3 |  |  [Non-Transferable Learning: A New Approach for Model Ownership Verification and Applicability Authorization](https://openreview.net/forum?id=tYRrOdSnVUy) |  | 0 | As Artificial Intelligence as a Service gains popularity, protecting well-trained models as intellectual property is becoming increasingly important. There are two common types of protection methods: ownership verification and usage authorization. In this paper, we propose Non-Transferable Learning (NTL), a novel approach that captures the exclusive data representation in the learned model and restricts the model generalization ability to certain domains. This approach provides effective solutions to both model verification and authorization. Specifically: 1) For ownership verification, watermarking techniques are commonly used but are often vulnerable to sophisticated watermark removal methods. By comparison, our NTL-based ownership verification provides robust resistance to state-of-the-art watermark removal methods, as shown in extensive experiments with 6 removal approaches over the digits, CIFAR10 & STL10, and VisDA datasets. 2) For usage authorization, prior solutions focus on authorizing specific users to access the model, but authorized users can still apply the model to any data without restriction. Our NTL-based authorization approach instead provides data-centric protection, which we call applicability authorization, by significantly degrading the performance of the model on unauthorized data. Its effectiveness is also shown through experiments on aforementioned datasets. | Lixu Wang, Shichao Xu, Ruiqi Xu, Xiao Wang, Qi Zhu |  |
| 4 |  |  [Neural Structured Prediction for Inductive Node Classification](https://openreview.net/forum?id=YWNAX0caEjI) |  | 0 | This paper studies node classification in the inductive setting, i.e., aiming to learn a model on labeled training graphs and generalize it to infer node labels on unlabeled test graphs. This problem has been extensively studied with graph neural networks (GNNs) by learning effective node representations, as well as traditional structured prediction methods for modeling the structured output of node labels, e.g., conditional random fields (CRFs). In this paper, we present a new approach called the Structured Proxy Network (SPN), which combines the advantages of both worlds. SPN defines flexible potential functions of CRFs with GNNs. However, learning such a model is nontrivial as it involves optimizing a maximin game with high-cost inference. Inspired by the underlying connection between joint and marginal distributions defined by Markov networks, we propose to solve an approximate version of the optimization problem as a proxy, which yields a near-optimal solution, making learning more efficient. Extensive experiments on two settings show that our approach outperforms many competitive baselines. | Meng Qu, Huiyu Cai, Jian Tang |  |
| 5 |  |  [A New Perspective on "How Graph Neural Networks Go Beyond Weisfeiler-Lehman?"](https://openreview.net/forum?id=uxgg9o7bI_3) |  | 0 | We propose a new perspective on designing powerful Graph Neural Networks (GNNs). In a nutshell, this enables a general solution to inject structural properties of graphs into a message-passing aggregation scheme of GNNs. As a theoretical basis, we develop a new hierarchy of local isomorphism on neighborhood subgraphs. Then, we theoretically characterize how message-passing GNNs can be designed to be more expressive than the Weisfeiler Lehman test. To elaborate this characterization, we propose a novel neural model, called GraphSNN, and prove that this model is strictly more expressive than the Weisfeiler Lehman test in distinguishing graph structures. We empirically verify the strength of our model on different graph learning tasks. It is shown that our model consistently improves the state-of-the-art methods on the benchmark tasks without sacrificing computational simplicity and efficiency. | Asiri Wijesinghe, Qing Wang |  |
| 6 |  |  [Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond](https://openreview.net/forum?id=LdlwbBP2mlq) |  | 0 | In distributed learning, local SGD (also known as federated averaging) and its simple baseline minibatch SGD are widely studied optimization methods. Most existing analyses of these methods assume independent and unbiased gradient estimates obtained via with-replacement sampling. In contrast, we study shuffling-based variants: minibatch and local Random Reshuffling, which draw stochastic gradients without replacement and are thus closer to practice. For smooth functions satisfying the Polyak-Łojasiewicz condition, we obtain convergence bounds (in the large epoch regime) which show that these shuffling-based variants converge faster than their with-replacement counterparts. Moreover, we prove matching lower bounds showing that our convergence analysis is tight. Finally, we propose an algorithmic modification called synchronized shuffling that leads to convergence rates faster than our lower bounds in near-homogeneous settings. | Chulhee Yun, Shashank Rajput, Suvrit Sra |  |
| 7 |  |  [The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks: an Exact Characterization of Optimal Solutions](https://openreview.net/forum?id=Z7Lk2cQEG8a) |  | 0 | We prove that finding all globally optimal two-layer ReLU neural networks can be performed by solving a convex optimization program with cone constraints. Our analysis is novel, characterizes all optimal solutions, and does not leverage duality-based analysis which was recently used to lift neural network training into convex spaces. Given the set of solutions of our convex optimization program, we show how to construct exactly the entire set of optimal neural networks. We provide a detailed characterization of this optimal set and its invariant transformations. As additional consequences of our convex perspective, (i) we establish that Clarke stationary points found by stochastic gradient descent correspond to the global optimum of a subsampled convex problem (ii) we provide a polynomial-time algorithm for checking if a neural network is a global minimum of the training loss (iii) we provide an explicit construction of a continuous path between any neural network and the global minimum of its sublevel set and (iv) characterize the minimal size of the hidden layer so that the neural network optimization landscape has no spurious valleys. Overall, we provide a rich framework for studying the landscape of neural network training loss through convexity. | Yifei Wang, Jonathan Lacotte, Mert Pilanci |  |
| 8 |  |  [Provably Filtering Exogenous Distractors using Multistep Inverse Dynamics](https://openreview.net/forum?id=RQLLzMCefQu) |  | 0 | Many real-world applications of reinforcement learning (RL) require the agent to deal with high-dimensional observations such as those generated from a megapixel camera. Prior work has addressed such problems with representation learning, through which the agent can provably extract endogenous, latent state information from raw observations and subsequently plan efficiently. However, such approaches can fail in the presence of temporally correlated noise in the observations, a phenomenon that is common in practice. We initiate the formal study of latent state discovery in the presence of such exogenous noise sources by proposing a new model, the Exogenous Block MDP (EX-BMDP), for rich observation RL. We start by establishing several negative results, by highlighting failure cases of prior representation learning based approaches. Then, we introduce the Predictive Path Elimination (PPE) algorithm, that learns a generalization of inverse dynamics and is provably sample and computationally efficient in EX-BMDPs when the endogenous state dynamics are near deterministic. The sample complexity of PPE depends polynomially on the size of the latent endogenous state space while not directly depending on the size of the observation space, nor the exogenous state space. We provide experiments on challenging exploration problems which show that our approach works empirically. | Yonathan Efroni, Dipendra Misra, Akshay Krishnamurthy, Alekh Agarwal, John Langford |  |
| 9 |  |  [Bootstrapped Meta-Learning](https://openreview.net/forum?id=b-ny3x071E5) |  | 0 | Meta-learning empowers artificial intelligence to increase its efficiency by learning how to learn. Unlocking this potential involves overcoming a challenging meta-optimisation problem. We propose an algorithm that tackles this problem by letting the meta-learner teach itself. The algorithm first bootstraps a target from the meta-learner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. Focusing on meta-learning with gradients, we establish conditions that guarantee performance improvements and show that metric can be used to control meta-optimisation. Meanwhile, the bootstrapping mechanism can extend the effective meta-learning horizon without requiring backpropagation through all updates. We achieve a new state-of-the art for model-free agents on the Atari ALE benchmark and demonstrate that it yields both performance and efficiency gains in multi-task meta-learning. Finally, we explore how bootstrapping opens up new possibilities and find that it can meta-learn efficient exploration in an epsilon-greedy Q-learning agent - without backpropagating through the update rule. | Sebastian Flennerhag, Yannick Schroecker, Tom Zahavy, Hado van Hasselt, David Silver, Satinder Singh |  |
| 10 |  |  [Coordination Among Neural Modules Through a Shared Global Workspace](https://openreview.net/forum?id=XzTtHjgPDsT) |  | 0 | Deep learning has seen a movement away from representing examples with a monolithic hidden state towards a richly structured state. For example, Transformers segment by position, and object-centric architectures decompose images into entities. In all these architectures, interactions between different elements are modeled via pairwise interactions: Transformers make use of self-attention to incorporate information from other positions and object-centric architectures make use of graph neural networks to model interactions among entities. We consider how to improve on pairwise interactions in terms of global coordination and a coherent, integrated representation that can be used for downstream tasks. In cognitive science, a global workspace architecture has been proposed in which functionally specialized components share information through a common, bandwidth-limited communication channel. We explore the use of such a communication channel in the context of deep learning for modeling the structure of complex environments. The proposed method includes a shared workspace through which communication among different specialist modules takes place but due to limits on the communication bandwidth, specialist modules must compete for access. We show that capacity limitations have a rational basis in that (1) they encourage specialization and compositionality and (2) they facilitate the synchronization of otherwise independent specialists. | Anirudh Goyal, Aniket Rajiv Didolkar, Alex Lamb, Kartikeya Badola, Nan Rosemary Ke, Nasim Rahaman, Jonathan Binas, Charles Blundell, Michael Curtis Mozer, Yoshua Bengio |  |
| 11 |  |  [Data-Efficient Graph Grammar Learning for Molecular Generation](https://openreview.net/forum?id=l4IHywGq6a) |  | 0 | The problem of molecular generation has received significant attention recently. Existing methods are typically based on deep neural networks and require training on large datasets with tens of thousands of samples. In practice, however, the size of class-specific chemical datasets is usually limited (e.g., dozens of samples) due to labor-intensive experimentation and data collection. Another major challenge is to generate only physically synthesizable molecules. This is a non-trivial task for neural network-based generative models since the relevant chemical knowledge can only be extracted and generalized from the limited training data. In this work, we propose a data-efficient generative model that can be learned from datasets with orders of magnitude smaller sizes than common benchmarks. At the heart of this method is a learnable graph grammar that generates molecules from a sequence of production rules. Without any human assistance, these production rules are automatically constructed from training data. Furthermore, additional chemical knowledge can be incorporated into the model by further grammar optimization. Our learned graph grammar yields state-of-the-art results on generating high-quality molecules for three monomer datasets that contain only ${\sim}20$ samples each. Our approach also achieves remarkable performance in a challenging polymer generation task with $only$ $117$ training samples and is competitive against existing methods using $81$k data points. | Minghao Guo, Veronika Thost, Beichen Li, Payel Das, Jie Chen, Wojciech Matusik |  |
| 12 |  |  [Poisoning and Backdooring Contrastive Learning](https://openreview.net/forum?id=iC4UHbQ01Mp) |  | 0 | Multimodal contrastive learning methods like CLIP train on noisy and uncurated training datasets. This is cheaper than labeling datasets manually, and even improves out-of-distribution robustness. We show that this practice makes backdoor and poisoning attacks a significant threat. By poisoning just 0.01% of a dataset (e.g., just 300 images of the 3 million-example Conceptual Captions dataset), we can cause the model to misclassify test images by overlaying a small patch. Targeted poisoning attacks, whereby the model misclassifies a particular test input with an adversarially-desired label, are even easier requiring control of 0.0001% of the dataset (e.g., just three out of the 3 million images). Our attacks call into question whether training on noisy and uncurated Internet scrapes is desirable. | Nicholas Carlini, Andreas Terzis |  |
| 13 |  |  [Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path](https://openreview.net/forum?id=w1UbdvWH_R3) |  | 0 | The recently discovered Neural Collapse (NC) phenomenon occurs pervasively in today's deep net training paradigm of driving cross-entropy (CE) loss towards zero. During NC, last-layer features collapse to their class-means, both classifiers and class-means collapse to the same Simplex Equiangular Tight Frame, and classifier behavior collapses to the nearest-class-mean decision rule. Recent works demonstrated that deep nets trained with mean squared error (MSE) loss perform comparably to those trained with CE. As a preliminary, we empirically establish that NC emerges in such MSE-trained deep nets as well through experiments on three canonical networks and five benchmark datasets. We provide, in a Google Colab notebook, PyTorch code for reproducing MSE-NC and CE-NC: https://colab.research.google.com/github/neuralcollapse/neuralcollapse/blob/main/neuralcollapse.ipynb. The analytically-tractable MSE loss offers more mathematical opportunities than the hard-to-analyze CE loss, inspiring us to leverage MSE loss towards the theoretical investigation of NC. We develop three main contributions: (I) We show a new decomposition of the MSE loss into (A) terms directly interpretable through the lens of NC and which assume the last-layer classifier is exactly the least-squares classifier; and (B) a term capturing the deviation from this least-squares classifier. (II) We exhibit experiments on canonical datasets and networks demonstrating that term-(B) is negligible during training. This motivates us to introduce a new theoretical construct: the central path, where the linear classifier stays MSE-optimal for feature activations throughout the dynamics. (III) By studying renormalized gradient flow along the central path, we derive exact dynamics that predict NC. | X. Y. Han, Vardan Papyan, David L. Donoho |  |
| 14 |  |  [Weighted Training for Cross-Task Learning](https://openreview.net/forum?id=ltM1RMZntpu) |  | 0 | In this paper, we introduce Target-Aware Weighted Training (TAWT), a weighted training algorithm for cross-task learning based on minimizing a representation-based task distance between the source and target tasks. We show that TAWT is easy to implement, is computationally efficient, requires little hyperparameter tuning, and enjoys non-asymptotic learning-theoretic guarantees. The effectiveness of TAWT is corroborated through extensive experiments with BERT on four sequence tagging tasks in natural language processing (NLP), including part-of-speech (PoS) tagging, chunking, predicate detection, and named entity recognition (NER). As a byproduct, the proposed representation-based task distance allows one to reason in a theoretically principled way about several critical aspects of cross-task learning, such as the choice of the source data and the impact of fine-tuning. | Shuxiao Chen, Koby Crammer, Hangfeng He, Dan Roth, Weijie J. Su |  |
| 15 |  |  [iLQR-VAE : control-based learning of input-driven dynamics with applications to neural data](https://openreview.net/forum?id=wRODLDHaAiW) |  | 0 | Understanding how neural dynamics give rise to behaviour is one of the most fundamental questions in systems neuroscience. To achieve this, a common approach is to record neural populations in behaving animals, and model these data as emanating from a latent dynamical system whose state trajectories can then be related back to behavioural observations via some form of decoding. As recordings are typically performed in localized circuits that form only a part of the wider implicated network, it is important to simultaneously learn the local dynamics and infer any unobserved external input that might drive them. Here, we introduce iLQR-VAE, a novel control-based approach to variational inference in nonlinear dynamical systems, capable of learning both latent dynamics, initial conditions, and ongoing external inputs. As in recent deep learning approaches, our method is based on an input-driven sequential variational autoencoder (VAE). The main novelty lies in the use of the powerful iterative linear quadratic regulator algorithm (iLQR) in the recognition model. Optimization of the standard evidence lower-bound requires differentiating through iLQR solutions, which is made possible by recent advances in differentiable control. Importantly, having the recognition model be implicitly defined by the generative model greatly reduces the number of free parameters and allows for flexible, high-quality inference. This makes it possible for instance to evaluate the model on a single long trial after training on smaller chunks. We demonstrate the effectiveness of iLQR-VAE on a range of synthetic systems, with autonomous as well as input-driven dynamics. We further apply it to neural and behavioural recordings in non-human primates performing two different reaching tasks, and show that iLQR-VAE yields high-quality kinematic reconstructions from the neural data. | Marine Schimel, TaChu Kao, Kristopher T. Jensen, Guillaume Hennequin |  |
| 16 |  |  [Extending the WILDS Benchmark for Unsupervised Adaptation](https://openreview.net/forum?id=z7p2V6KROOV) |  | 0 | Machine learning systems deployed in the wild are often trained on a source distribution but deployed on a different target distribution. Unlabeled data can be a powerful point of leverage for mitigating these distribution shifts, as it is frequently much more available than labeled data and can often be obtained from distributions beyond the source distribution as well. However, existing distribution shift benchmarks with unlabeled data do not reflect the breadth of scenarios that arise in real-world applications. In this work, we present the WILDS 2.0 update, which extends 8 of the 10 datasets in the WILDS benchmark of distribution shifts to include curated unlabeled data that would be realistically obtainable in deployment. These datasets span a wide range of applications (from histology to wildlife conservation), tasks (classification, regression, and detection), and modalities (photos, satellite images, microscope slides, text, molecular graphs). The update maintains consistency with the original WILDS benchmark by using identical labeled training, validation, and test sets, as well as identical evaluation metrics. We systematically benchmark state-of-the-art methods that use unlabeled data, including domain-invariant, self-training, and self-supervised methods, and show that their success on WILDS is limited. To facilitate method development, we provide an open-source package that automates data loading and contains the model architectures and methods used in this paper. Code and leaderboards are available at https://wilds.stanford.edu. | Shiori Sagawa, Pang Wei Koh, Tony Lee, Irena Gao, Sang Michael Xie, Kendrick Shen, Ananya Kumar, Weihua Hu, Michihiro Yasunaga, Henrik Marklund, Sara Beery, Etienne David, Ian Stavness, Wei Guo, Jure Leskovec, Kate Saenko, Tatsunori Hashimoto, Sergey Levine, Chelsea Finn, Percy Liang |  |
| 17 |  |  [Asymmetry Learning for Counterfactually-invariant Classification in OOD Tasks](https://openreview.net/forum?id=avgclFZ221l) |  | 0 | Generalizing from observed to new related environments (out-of-distribution) is central to the reliability of classifiers. However, most classifiers fail to predict label $Y$ from input $X$ when the change in environment is due a (stochastic) input transformation $T^\text{te} \circ X'$ not observed in training, as in training we observe $T^\text{tr} \circ X'$, where $X'$ is a hidden variable. This work argues that when the transformations in train $T^\text{tr}$ and test $T^\text{te}$ are (arbitrary) symmetry transformations induced by a collection of known $m$ equivalence relations, the task of finding a robust OOD classifier can be defined as finding the simplest causal model that defines a causal connection between the target labels and the symmetry transformations that are associated with label changes. We then propose a new learning paradigm, asymmetry learning, that identifies which symmetries the classifier must break in order to correctly predict $Y$ in both train and test. Asymmetry learning performs a causal model search that, under certain identifiability conditions, finds classifiers that perform equally well in-distribution and out-of-distribution. Finally, we show how to learn counterfactually-invariant representations with asymmetry learning in two physics tasks. | S. Chandra Mouli, Bruno Ribeiro |  |
| 18 |  |  [Comparing Distributions by Measuring Differences that Affect Decision Making](https://openreview.net/forum?id=KB5onONJIAU) |  | 0 | Measuring the discrepancy between two probability distributions is a fundamental problem in machine learning and statistics. We propose a new class of discrepancies based on the optimal loss for a decision task -- two distributions are different if the optimal decision loss is higher on their mixture than on each individual distribution. By suitably choosing the decision task, this generalizes the Jensen-Shannon divergence and the maximum mean discrepancy family. We apply our approach to two-sample tests, and on various benchmarks, we achieve superior test power compared to competing methods. In addition, a modeler can directly specify their preferences when comparing distributions through the decision loss. We apply this property to understanding the effects of climate change on different social and economic activities, evaluating sample quality, and selecting features targeting different decision tasks. | Shengjia Zhao, Abhishek Sinha, Yutong He, Aidan Perreault, Jiaming Song, Stefano Ermon |  |
| 19 |  |  [MIDI-DDSP: Detailed Control of Musical Performance via Hierarchical Modeling](https://openreview.net/forum?id=UseMOjWENv) |  | 0 | Musical expression requires control of both what notes that are played, and how they are performed. Conventional audio synthesizers provide detailed expressive controls, but at the cost of realism. Black-box neural audio synthesis and concatenative samplers can produce realistic audio, but have few mechanisms for control. In this work, we introduce MIDI-DDSP a hierarchical model of musical instruments that enables both realistic neural audio synthesis and detailed user control. Starting from interpretable Differentiable Digital Signal Processing (DDSP) synthesis parameters, we infer musical notes and high-level properties of their expressive performance (such as timbre, vibrato, dynamics, and articulation). This creates a 3-level hierarchy (notes, performance, synthesis) that affords individuals the option to intervene at each level, or utilize trained priors (performance given notes, synthesis given performance) for creative assistance. Through quantitative experiments and listening tests, we demonstrate that this hierarchy can reconstruct high-fidelity audio, accurately predict performance attributes for a note sequence, independently manipulate the attributes of a given performance, and as a complete system, generate realistic audio from a novel note sequence. By utilizing an interpretable hierarchy, with multiple levels of granularity, MIDI-DDSP opens the door to assistive tools to empower individuals across a diverse range of musical experience. | Yusong Wu, Ethan Manilow, Yi Deng, Rigel Swavely, Kyle Kastner, Tim Cooijmans, Aaron C. Courville, ChengZhi Anna Huang, Jesse H. Engel |  |
| 20 |  |  [Unsupervised Vision-Language Grammar Induction with Shared Structure Modeling](https://openreview.net/forum?id=N0n_QyQ5lBF) |  | 0 | We introduce a new task, unsupervised vision-language (VL) grammar induction. Given an image-caption pair, the goal is to extract a shared hierarchical structure for both image and language simultaneously. We argue that such structured output, grounded in both modalities, is a clear step towards the high-level understanding of multimodal information. Besides challenges existing in conventional visually grounded grammar induction tasks, VL grammar induction requires a model to capture contextual semantics and perform a fine-grained alignment. To address these challenges, we propose a novel method, CLIORA, which constructs a shared vision-language constituency tree structure with context-dependent semantics for all possible phrases in different levels of the tree. It computes a matching score between each constituent and image region, trained via contrastive learning. It integrates two levels of fusion, namely at feature-level and at score-level, so as to allow fine-grained alignment. We introduce a new evaluation metric for VL grammar induction, CCRA, and show a 3.3% improvement over a strong baseline on Flickr30k Entities. We also evaluate our model via two derived tasks, i.e., language grammar induction and phrase grounding, and improve over the state-of-the-art for both. | Bo Wan, Wenjuan Han, Zilong Zheng, Tinne Tuytelaars |  |
| 21 |  |  [PiCO: Contrastive Label Disambiguation for Partial Label Learning](https://openreview.net/forum?id=EhYjZy6e1gJ) |  | 0 | Partial label learning (PLL) is an important problem that allows each training example to be labeled with a coarse candidate set, which well suits many real-world data annotation scenarios with label ambiguity. Despite the promise, the performance of PLL often lags behind the supervised counterpart. In this work, we bridge the gap by addressing two key research challenges in PLL---representation learning and label disambiguation---in one coherent framework. Specifically, our proposed framework PiCO consists of a contrastive learning module along with a novel class prototype-based label disambiguation algorithm. PiCO produces closely aligned representations for examples from the same classes and facilitates label disambiguation. Theoretically, we show that these two components are mutually beneficial, and can be rigorously justified from an expectation-maximization (EM) algorithm perspective. Extensive experiments demonstrate that PiCO significantly outperforms the current state-of-the-art approaches in PLL and even achieves comparable results to fully supervised learning. Code and data available: https://github.com/hbzju/PiCO. | Haobo Wang, Ruixuan Xiao, Yixuan Li, Lei Feng, Gang Niu, Gang Chen, Junbo Zhao |  |
| 22 |  |  [Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting](https://openreview.net/forum?id=0EXmFzUn5I) |  | 0 | Accurate prediction of the future given the past based on time series data is of paramount importance, since it opens the door for decision making and risk management ahead of time. In practice, the challenge is to build a flexible but parsimonious model that can capture a wide range of temporal dependencies. In this paper, we propose Pyraformer by exploring the multiresolution representation of the time series. Specifically, we introduce the pyramidal attention module (PAM) in which the inter-scale tree structure summarizes features at different resolutions and the intra-scale neighboring connections model the temporal dependencies of different ranges. Under mild conditions, the maximum length of the signal traversing path in Pyraformer is a constant (i.e., $\mathcal O(1)$) with regard to the sequence length $L$, while its time and space complexity scale linearly with $L$. Extensive numerical results show that Pyraformer typically achieves the highest prediction accuracy in both single-step and long-range forecasting tasks with the least amount of time and memory consumption, especially when the sequence is long. | Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X. Liu, Schahram Dustdar |  |
| 23 |  |  [Expressiveness and Approximation Properties of Graph Neural Networks](https://openreview.net/forum?id=wIzUeM3TAU) |  | 0 | Characterizing the separation power of graph neural networks (GNNs) provides an understanding of their limitations for graph learning tasks. Results regarding separation power are, however, usually geared at specific GNNs architectures, and tools for understanding arbitrary GNN architectures are generally lacking. We provide an elegant way to easily obtain bounds on the separation power of GNNs in terms of the Weisfeiler-Leman (WL) tests, which have become the yardstick to measure the separation power of GNNs. The crux is to view GNNs as expressions in a procedural tensor language describing the computations in the layers of the GNNs. Then, by a simple analysis of the obtained expressions, in terms of the number of indexes used and the nesting depth of summations, bounds on the separation power in terms of the WL-tests readily follow. We use tensor language to define Higher-Order Message-Passing Neural Networks (or k-MPNNs), a natural extension of MPNNs. Furthermore, the tensor language point of view allows for the derivation of universality results for classes of GNNs in a natural way. Our approach provides a toolbox with which GNN architecture designers can analyze the separation power of their GNNs, without needing to know the intricacies of the WL-tests. We also provide insights in what is needed to boost the separation power of GNNs. | Floris Geerts, Juan L. Reutter |  |
| 24 |  |  [Filtered-CoPhy: Unsupervised Learning of Counterfactual Physics in Pixel Space](https://openreview.net/forum?id=1L0C5ROtFp) |  | 0 | Learning causal relationships in high-dimensional data (images, videos) is a hard task, as they are often defined on low dimensional manifolds and must be extracted from complex signals dominated by appearance, lighting, textures and also spurious correlations in the data. We present a method for learning counterfactual reasoning of physical processes in pixel space, which requires the prediction of the impact of interventions on initial conditions. Going beyond the identification of structural relationships, we deal with the challenging problem of forecasting raw video over long horizons. Our method does not require the knowledge or supervision of any ground truth positions or other object or scene properties. Our model learns and acts on a suitable hybrid latent representation based on a combination of dense features, sets of 2D keypoints and an additional latent vector per keypoint. We show that this better captures the dynamics of physical processes than purely dense or sparse representations. We introduce a new challenging and carefully designed counterfactual benchmark for predictions in pixel space and outperform strong baselines in physics-inspired ML and video prediction. | Steeven Janny, Fabien Baradel, Natalia Neverova, Madiha Nadri, Greg Mori, Christian Wolf |  |
| 25 |  |  [BEiT: BERT Pre-Training of Image Transformers](https://openreview.net/forum?id=p-BhZSz59o4) |  | 0 | We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e., image patches (such as 16 x 16 pixels), and visual tokens (i.e., discrete tokens). We first \`\`tokenize'' the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. | Hangbo Bao, Li Dong, Songhao Piao, Furu Wei |  |
| 26 |  |  [Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution](https://openreview.net/forum?id=UYneFzXSJWh) |  | 0 | When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer---the "head"). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (BREEDS-Living17, BREEDS-Entity30, DomainNet, CIFAR $\to$ STL, CIFAR-10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2% higher accuracy ID but 7% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head---this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1% better ID, 10% better OOD than full fine-tuning). | Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, Percy Liang |  |
| 27 |  |  [StyleAlign: Analysis and Applications of Aligned StyleGAN Models](https://openreview.net/forum?id=Qg2vi4ZbHM9) |  | 0 | In this paper, we perform an in-depth study of the properties and applications of aligned generative models. We refer to two models as aligned if they share the same architecture, and one of them (the child) is obtained from the other (the parent) via fine-tuning to another domain, a common practice in transfer learning. Several works already utilize some basic properties of aligned StyleGAN models to perform image-to-image translation. Here, we perform the first detailed exploration of model alignment, also focusing on StyleGAN. First, we empirically analyze aligned models and provide answers to important questions regarding their nature. In particular, we find that the child model's latent spaces are semantically aligned with those of the parent, inheriting incredibly rich semantics, even for distant data domains such as human faces and churches. Second, equipped with this better understanding, we leverage aligned models to solve a diverse set of tasks. In addition to image translation, we demonstrate fully automatic cross-domain image morphing. We further show that zero-shot vision tasks may be performed in the child domain, while relying exclusively on supervision in the parent domain. We demonstrate qualitatively and quantitatively that our approach yields state-of-the-art results, while requiring only simple fine-tuning and inversion. | Zongze Wu, Yotam Nitzan, Eli Shechtman, Dani Lischinski |  |
| 28 |  |  [Variational Inference for Discriminative Learning with Generative Modeling of Feature Incompletion](https://openreview.net/forum?id=qnQN4yr6FJz) |  | 0 | We are concerned with the problem of distributional prediction with incomplete features: The goal is to estimate the distribution of target variables given feature vectors with some of the elements missing. A typical approach to this problem is to perform missing-value imputation and regression, simultaneously or sequentially, which we call the generative approach. Another approach is to perform regression after appropriately encoding missing values into the feature, which we call the discriminative approach. In comparison, the generative approach is more robust to the feature corruption while the discriminative approach is more favorable to maximize the performance of prediction. In this study, we propose a hybrid method to take the best of both worlds. Our method utilizes the black-box variational inference framework so that it can be applied to a wide variety of modern machine learning models, including the variational autoencoders. We also confirmed the effectiveness of the proposed method empirically. | Kohei Miyaguchi, Takayuki Katsuki, Akira Koseki, Toshiya Iwamori |  |
| 29 |  |  [Efficiently Modeling Long Sequences with Structured State Spaces](https://openreview.net/forum?id=uYLFoz1vlAC) |  | 0 | A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of $10000$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \), and showed that for appropriate choices of the state matrix \( A \), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \( A \) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60\times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors. | Albert Gu, Karan Goel, Christopher Ré |  |
| 30 |  |  [Large Language Models Can Be Strong Differentially Private Learners](https://openreview.net/forum?id=bVuP3ltATMz) |  | 0 | Differentially Private (DP) learning has seen limited success for building large deep learning models of text, and straightforward attempts at applying Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have resulted in large performance drops and high computational overhead. We show that this performance drop can be mitigated with (1) the use of large pretrained language models; (2) non-standard hyperparameters that suit DP optimization; and (3) fine-tuning objectives which are aligned with the pretraining procedure. With the above, we obtain NLP models that outperform state-of-the-art DP-trained models under the same privacy budget and strong non-private baselines---by directly fine-tuning pretrained models with DP optimization on moderately-sized corpora. To address the computational challenge of running DP-SGD with large Transformers, we propose a memory saving technique that allows clipping in DP-SGD to run without instantiating per-example gradients for any linear layer in the model. The technique enables privately training Transformers with almost the same memory cost as non-private training at a modest run-time overhead. Contrary to conventional wisdom that DP optimization fails at learning high-dimensional models (due to noise that scales with dimension) empirical results reveal that private learning with pretrained language models tends to not suffer from dimension-dependent performance degradation. Code to reproduce results can be found at https://github.com/lxuechen/private-transformers. | Xuechen Li, Florian Tramèr, Percy Liang, Tatsunori Hashimoto |  |
| 31 |  |  [GeoDiff: A Geometric Diffusion Model for Molecular Conformation Generation](https://openreview.net/forum?id=PzcvxEMzvQC) |  | 0 | Predicting molecular conformations from molecular graphs is a fundamental problem in cheminformatics and drug discovery. Recently, significant progress has been achieved with machine learning approaches, especially with deep generative models. Inspired by the diffusion process in classical non-equilibrium thermodynamics where heated particles will diffuse from original states to a noise distribution, in this paper, we propose a novel generative model named GeoDiff for molecular conformation prediction. GeoDiff treats each atom as a particle and learns to directly reverse the diffusion process (i.e., transforming from a noise distribution to stable conformations) as a Markov chain. Modeling such a generation process is however very challenging as the likelihood of conformations should be roto-translational invariant. We theoretically show that Markov chains evolving with equivariant Markov kernels can induce an invariant distribution by design, and further propose building blocks for the Markov kernels to preserve the desirable equivariance property. The whole framework can be efficiently trained in an end-to-end fashion by optimizing a weighted variational lower bound to the (conditional) likelihood. Experiments on multiple benchmarks show that GeoDiff is superior or comparable to existing state-of-the-art approaches, especially on large molecules. | Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, Jian Tang |  |
| 32 |  |  [Frame Averaging for Invariant and Equivariant Network Design](https://openreview.net/forum?id=zIUyj55nXR) |  | 0 | Many machine learning tasks involve learning functions that are known to be invariant or equivariant to certain symmetries of the input data. However, it is often challenging to design neural network architectures that respect these symmetries while being expressive and computationally efficient. For example, Euclidean motion invariant/equivariant graph or point cloud neural networks. We introduce Frame Averaging (FA), a highly general purpose and systematic framework for adapting known (backbone) architectures to become invariant or equivariant to new symmetry types. Our framework builds on the well known group averaging operator that guarantees invariance or equivariance but is intractable. In contrast, we observe that for many important classes of symmetries, this operator can be replaced with an averaging operator over a small subset of the group elements, called a frame. We show that averaging over a frame guarantees exact invariance or equivariance while often being much simpler to compute than averaging over the entire group. Furthermore, we prove that FA-based models have maximal expressive power in a broad setting and in general preserve the expressive power of their backbone architectures. Using frame averaging, we propose a new class of universal Graph Neural Networks (GNNs), universal Euclidean motion invariant point cloud networks, and Euclidean motion invariant Message Passing (MP) GNNs. We demonstrate the practical effectiveness of FA on several applications including point cloud normal estimation, beyond $2$-WL graph separation, and $n$-body dynamics prediction, achieving state-of-the-art results in all of these benchmarks. | Omri Puny, Matan Atzmon, Edward J. Smith, Ishan Misra, Aditya Grover, Heli BenHamu, Yaron Lipman |  |
| 33 |  |  [Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation](https://openreview.net/forum?id=oapKSVM2bcj) |  | 0 | Tensor computations underlie modern scientific computing and deep learning. A number of tensor frameworks emerged varying in execution model, hardware support, memory management, model definition, etc. However, tensor operations in all frameworks follow the same paradigm. Recent neural network architectures demonstrate demand for higher expressiveness of tensor operations. The current paradigm is not suited to write readable, reliable, or easy-to-modify code for multidimensional tensor manipulations. Moreover, some commonly used operations do not provide sufficient checks and can break a tensor structure. These mistakes are elusive as no tools or tests can detect them. Independently, API discrepancies complicate code transfer between frameworks. We propose einops notation: a uniform and generic way to manipulate tensor structure, that significantly improves code readability and flexibility by focusing on the structure of input and output tensors. We implement einops notation in a Python package that efficiently supports multiple widely used frameworks and provides framework-independent minimalist API for tensor manipulations. | Alex Rogozhnikov |  |
| 34 |  |  [A Fine-Grained Analysis on Distribution Shift](https://openreview.net/forum?id=Dl4LetuLdyK) |  | 0 | Robustness to distribution shifts is critical for deploying machine learning models in the real world. Despite this necessity, there has been little work in defining the underlying mechanisms that cause these shifts and evaluating the robustness of algorithms across multiple, different distribution shifts. To this end, we introduce a framework that enables fine-grained analysis of various distribution shifts. We provide a holistic analysis of current state-of-the-art methods by evaluating 19 distinct methods grouped into five categories across both synthetic and real-world datasets. Overall, we train more than 85K models. Our experimental framework can be easily extended to include new methods, shifts, and datasets. We find, unlike previous work (Gulrajani & Lopez-Paz, 2021), that progress has been made over a standard ERM baseline; in particular, pretraining and augmentations (learned or heuristic) offer large gains in many cases. However, the best methods are not consistent over different datasets and shifts. We will open source our experimental framework, allowing future work to evaluate new methods over multiple shifts to obtain a more complete picture of a method's effectiveness. Code is available at github.com/deepmind/distribution_shift_framework. | Olivia Wiles, Sven Gowal, Florian Stimberg, SylvestreAlvise Rebuffi, Ira Ktena, Krishnamurthy Dvijotham, Ali Taylan Cemgil |  |
| 35 |  |  [Open-Set Recognition: A Good Closed-Set Classifier is All You Need](https://openreview.net/forum?id=5hLP5JY9S2d) |  | 0 | The ability to identify whether or not a test sample belongs to one of the semantic classes in a classifier's training set is critical to practical deployment of the model. This task is termed open-set recognition (OSR) and has received significant attention in recent years. In this paper, we first demonstrate that the ability of a classifier to make the 'none-of-above' decision is highly correlated with its accuracy on the closed-set classes. We find that this relationship holds across loss objectives and architectures, and further demonstrate the trend both on the standard OSR benchmarks as well as on a large-scale ImageNet evaluation. Second, we use this correlation to boost the performance of the maximum softmax probability OSR 'baseline' by improving its closed-set accuracy, and with this strong baseline achieve state-of-the-art on a number of OSR benchmarks. Similarly, we boost the performance of the existing state-of-the-art method by improving its closed-set accuracy, but the resulting discrepancy with the strong baseline is marginal. Our third contribution is to present the 'Semantic Shift Benchmark' (SSB), which better respects the task of detecting semantic novelty, as opposed to low-level distributional shifts as tackled by neighbouring machine learning fields. On this new evaluation, we again demonstrate that there is negligible difference between the strong baseline and the existing state-of-the-art. Code available at: https://github.com/sgvaze/osr_closed_set_all_you_need. | Sagar Vaze, Kai Han, Andrea Vedaldi, Andrew Zisserman |  |
| 36 |  |  [Learning Strides in Convolutional Neural Networks](https://openreview.net/forum?id=M752z9FKJP) |  | 0 | Convolutional neural networks typically contain several downsampling operators, such as strided convolutions or pooling layers, that progressively reduce the resolution of intermediate representations. This provides some shift-invariance while reducing the computational complexity of the whole architecture. A critical hyperparameter of such layers is their stride: the integer factor of downsampling. As strides are not differentiable, finding the best configuration either requires cross-validation or discrete optimization (e.g. architecture search), which rapidly become prohibitive as the search space grows exponentially with the number of downsampling layers. Hence, exploring this search space by gradient descent would allow finding better configurations at a lower computational cost. This work introduces DiffStride, the first downsampling layer with learnable strides. Our layer learns the size of a cropping mask in the Fourier domain, that effectively performs resizing in a differentiable way. Experiments on audio and image classification show the generality and effectiveness of our solution: we use DiffStride as a drop-in replacement to standard downsampling layers and outperform them. In particular, we show that introducing our layer into a ResNet-18 architecture allows keeping consistent high performance on CIFAR10, CIFAR100 and ImageNet even when training starts from poor random stride configurations. Moreover, formulating strides as learnable variables allows us to introduce a regularization term that controls the computational complexity of the architecture. We show how this regularization allows trading off accuracy for efficiency on ImageNet. | Rachid Riad, Olivier Teboul, David Grangier, Neil Zeghidour |  |
| 37 |  |  [Understanding over-squashing and bottlenecks on graphs via curvature](https://openreview.net/forum?id=7UmjRGzp-A) |  | 0 | Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from distant nodes as a factor limiting the efficiency of message passing for tasks relying on long-distance interactions. This phenomenon, referred to as 'over-squashing', has been heuristically attributed to graph bottlenecks where the number of $k$-hop neighbors grows rapidly with $k$. We provide a precise description of the over-squashing phenomenon in GNNs and analyze how it arises from bottlenecks in the graph. For this purpose, we introduce a new edge-based combinatorial curvature and prove that negatively curved edges are responsible for the over-squashing issue. We also propose and experimentally test a curvature-based graph rewiring method to alleviate the over-squashing. | Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, Michael M. Bronstein |  |
| 38 |  |  [Diffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling Scheme](https://openreview.net/forum?id=8c50f-DoWAu) |  | 0 | Voice conversion is a common speech synthesis task which can be solved in different ways depending on a particular real-world scenario. The most challenging one often referred to as one-shot many-to-many voice conversion consists in copying target voice from only one reference utterance in the most general case when both source and target speakers do not belong to the training dataset. We present a scalable high-quality solution based on diffusion probabilistic modeling and demonstrate its superior quality compared to state-of-the-art one-shot voice conversion approaches. Moreover, focusing on real-time applications, we investigate general principles which can make diffusion models faster while keeping synthesis quality at a high level. As a result, we develop a novel Stochastic Differential Equations solver suitable for various diffusion model types and generative tasks as shown through empirical studies and justify it by theoretical analysis. | Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, Mikhail Sergeevich Kudinov, Jiansheng Wei |  |
| 39 |  |  [Resolving Training Biases via Influence-based Data Relabeling](https://openreview.net/forum?id=EskfH0bwNVn) |  | 0 | The performance of supervised learning methods easily suffers from the training bias issue caused by train-test distribution mismatch or label noise. Influence function is a technique that estimates the impacts of a training sample on the model’s predictions. Recent studies on \emph{data resampling} have employed influence functions to identify \emph{harmful} training samples that will degrade model's test performance. They have shown that discarding or downweighting the identified harmful training samples is an effective way to resolve training biases. In this work, we move one step forward and propose an influence-based relabeling framework named RDIA for reusing harmful training samples toward better model performance. To achieve this, we use influence functions to estimate how relabeling a training sample would affect model's test performance and further develop a novel relabeling function R. We theoretically prove that applying R to relabel harmful training samples allows the model to achieve lower test loss than simply discarding them for any classification tasks using cross-entropy loss. Extensive experiments on ten real-world datasets demonstrate RDIA outperforms the state-of-the-art data resampling methods and improves model's robustness against label noise. | Shuming Kong, Yanyan Shen, Linpeng Huang |  |
| 40 |  |  [Representational Continuity for Unsupervised Continual Learning](https://openreview.net/forum?id=9Hrka5PA7LW) |  | 0 | Continual learning (CL) aims to learn a sequence of tasks without forgetting the previously acquired knowledge. However, recent CL advances are restricted to supervised continual learning (SCL) scenarios. Consequently, they are not scalable to real-world applications where the data distribution is often biased and unannotated. In this work, we focus on unsupervised continual learning (UCL), where we learn the feature representations on an unlabelled sequence of tasks and show that reliance on annotated data is not necessary for continual learning. We conduct a systematic study analyzing the learned feature representations and show that unsupervised visual representations are surprisingly more robust to catastrophic forgetting, consistently achieve better performance, and generalize better to out-of-distribution tasks than SCL. Furthermore, we find that UCL achieves a smoother loss landscape through qualitative analysis of the learned representations and learns meaningful feature representations. Additionally, we propose Lifelong Unsupervised Mixup (LUMP), a simple yet effective technique that interpolates between the current task and previous tasks' instances to alleviate catastrophic forgetting for unsupervised representations. | Divyam Madaan, Jaehong Yoon, Yuanchun Li, Yunxin Liu, Sung Ju Hwang |  |
| 41 |  |  [Vision-Based Manipulators Need to Also See from Their Hands](https://openreview.net/forum?id=RJkAHKp7kNZ) |  | 0 | We study how the choice of visual perspective affects learning and generalization in the context of physical manipulation from raw sensor observations. Compared with the more commonly used global third-person perspective, a hand-centric (eye-in-hand) perspective affords reduced observability, but we find that it consistently improves training efficiency and out-of-distribution generalization. These benefits hold across a variety of learning algorithms, experimental settings, and distribution shifts, and for both simulated and real robot apparatuses. However, this is only the case when hand-centric observability is sufficient; otherwise, including a third-person perspective is necessary for learning, but also harms out-of-distribution generalization. To mitigate this, we propose to regularize the third-person information stream via a variational information bottleneck. On six representative manipulation tasks with varying hand-centric observability adapted from the Meta-World benchmark, this results in a state-of-the-art reinforcement learning agent operating from both perspectives improving its out-of-distribution generalization on every task. While some practitioners have long put cameras in the hands of robots, our work systematically analyzes the benefits of doing so and provides simple and broadly applicable insights for improving end-to-end learned vision-based robotic manipulation. | Kyle Hsu, Moo Jin Kim, Rafael Rafailov, Jiajun Wu, Chelsea Finn |  |
| 42 |  |  [Meta-Learning with Fewer Tasks through Task Interpolation](https://openreview.net/forum?id=ajXWF7bVR8d) |  | 0 | Meta-learning enables algorithms to quickly learn a newly encountered task with just a few labeled examples by transferring previously learned knowledge. However, the bottleneck of current meta-learning algorithms is the requirement of a large number of meta-training tasks, which may not be accessible in real-world scenarios. To address the challenge that available tasks may not densely sample the space of tasks, we propose to augment the task set through interpolation. By meta-learning with task interpolation (MLTI), our approach effectively generates additional tasks by randomly sampling a pair of tasks and interpolating the corresponding features and labels. Under both gradient-based and metric-based meta-learning settings, our theoretical analysis shows MLTI corresponds to a data-adaptive meta-regularization and further improves the generalization. Empirically, in our experiments on eight datasets from diverse domains including image recognition, pose prediction, molecule property prediction, and medical image classification, we find that the proposed general MLTI framework is compatible with representative meta-learning algorithms and consistently outperforms other state-of-the-art strategies. | Huaxiu Yao, Linjun Zhang, Chelsea Finn |  |
| 43 |  |  [Discovering and Explaining the Representation Bottleneck of DNNS](https://openreview.net/forum?id=iRCUlgmdfHJ) |  | 0 | This paper explores the bottleneck of feature representations of deep neural networks (DNNs), from the perspective of the complexity of interactions between input variables encoded in DNNs. To this end, we focus on the multi-order interaction between input variables, where the order represents the complexity of interactions. We discover that a DNN is more likely to encode both too simple and too complex interactions, but usually fails to learn interactions of intermediate complexity. Such a phenomenon is widely shared by different DNNs for different tasks. This phenomenon indicates a cognition gap between DNNs and humans, and we call it a representation bottleneck. We theoretically prove the underlying reason for the representation bottleneck. Furthermore, we propose losses to encourage/penalize the learning of interactions of specific complexities, and analyze the representation capacities of interactions of different complexities. The code is available at https://github.com/Nebularaid2000/bottleneck. | Huiqi Deng, Qihan Ren, Hao Zhang, Quanshi Zhang |  |
| 44 |  |  [Sparse Communication via Mixed Distributions](https://openreview.net/forum?id=WAid50QschI) |  | 0 | Neural networks and other machine learning models compute continuous representations, while humans communicate mostly through discrete symbols. Reconciling these two forms of communication is desirable for generating human-readable interpretations or learning discrete latent variable models, while maintaining end-to-end differentiability. Some existing approaches (such as the Gumbel-Softmax transformation) build continuous relaxations that are discrete approximations in the zero-temperature limit, while others (such as sparsemax transformations and the Hard Concrete distribution) produce discrete/continuous hybrids. In this paper, we build rigorous theoretical foundations for these hybrids, which we call "mixed random variables.'' Our starting point is a new "direct sum'' base measure defined on the face lattice of the probability simplex. From this measure, we introduce new entropy and Kullback-Leibler divergence functions that subsume the discrete and differential cases and have interpretations in terms of code optimality. Our framework suggests two strategies for representing and sampling mixed random variables, an extrinsic ("sample-and-project'’) and an intrinsic one (based on face stratification). We experiment with both approaches on an emergent communication benchmark and on modeling MNIST and Fashion-MNIST data with variational auto-encoders with mixed latent variables. | António Farinhas, Wilker Aziz, Vlad Niculae, André F. T. Martins |  |
| 45 |  |  [Finetuned Language Models are Zero-Shot Learners](https://openreview.net/forum?id=gEZrGCozdqR) |  | 0 | This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning—finetuning language models on a collection of datasets described via instructions—substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning. | Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, Quoc V. Le |  |
| 46 |  |  [F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization](https://openreview.net/forum?id=_CfpJazzXT2) |  | 0 | Neural network quantization is a promising compression technique to reduce memory footprint and save energy consumption, potentially leading to real-time inference. However, there is a performance gap between quantized and full-precision models. To reduce it, existing quantization approaches require high-precision INT32 or full-precision multiplication during inference for scaling or dequantization. This introduces a noticeable cost in terms of memory, speed, and required energy. To tackle these issues, we present F8Net, a novel quantization framework consisting in only ﬁxed-point 8-bit multiplication. To derive our method, we ﬁrst discuss the advantages of ﬁxed-point multiplication with different formats of ﬁxed-point numbers and study the statistical behavior of the associated ﬁxed-point numbers. Second, based on the statistical and algorithmic analysis, we apply different ﬁxed-point formats for weights and activations of different layers. We introduce a novel algorithm to automatically determine the right format for each layer during training. Third, we analyze a previous quantization algorithm—parameterized clipping activation (PACT)—and reformulate it using ﬁxed-point arithmetic. Finally, we unify the recently proposed method for quantization ﬁne-tuning and our ﬁxed-point approach to show the potential of our method. We verify F8Net on ImageNet for MobileNet V1/V2 and ResNet18/50. Our approach achieves comparable and better performance, when compared not only to existing quantization techniques with INT32 multiplication or ﬂoating point arithmetic, but also to the full-precision counterparts, achieving state-of-the-art performance. | Qing Jin, Jian Ren, Richard Zhuang, Sumant Hanumante, Zhengang Li, Zhiyu Chen, Yanzhi Wang, Kaiyuan Yang, Sergey Tulyakov |  |
| 47 |  |  [Transform2Act: Learning a Transform-and-Control Policy for Efficient Agent Design](https://openreview.net/forum?id=UcDUxjPYWSr) |  | 0 | An agent's functionality is largely determined by its design, i.e., skeletal structure and joint attributes (e.g., length, size, strength). However, finding the optimal agent design for a given function is extremely challenging since the problem is inherently combinatorial and the design space is prohibitively large. Additionally, it can be costly to evaluate each candidate design which requires solving for its optimal controller. To tackle these problems, our key idea is to incorporate the design procedure of an agent into its decision-making process. Specifically, we learn a conditional policy that, in an episode, first applies a sequence of transform actions to modify an agent's skeletal structure and joint attributes, and then applies control actions under the new design. To handle a variable number of joints across designs, we use a graph-based policy where each graph node represents a joint and uses message passing with its neighbors to output joint-specific actions. Using policy gradient methods, our approach enables joint optimization of agent design and control as well as experience sharing across different designs, which improves sample efficiency substantially. Experiments show that our approach, Transform2Act, outperforms prior methods significantly in terms of convergence speed and final performance. Notably, Transform2Act can automatically discover plausible designs similar to giraffes, squids, and spiders. Code and videos are available at https://sites.google.com/view/transform2act. | Ye Yuan, Yuda Song, Zhengyi Luo, Wen Sun, Kris M. Kitani |  |
| 48 |  |  [ProtoRes: Proto-Residual Network for Pose Authoring via Learned Inverse Kinematics](https://openreview.net/forum?id=s03AQxehtd_) |  | 0 | Our work focuses on the development of a learnable neural representation of human pose for advanced AI assisted animation tooling. Specifically, we tackle the problem of constructing a full static human pose based on sparse and variable user inputs (e.g. locations and/or orientations of a subset of body joints). To solve this problem, we propose a novel neural architecture that combines residual connections with prototype encoding of a partially specified pose to create a new complete pose from the learned latent space. We show that our architecture outperforms a baseline based on Transformer, both in terms of accuracy and computational efficiency. Additionally, we develop a user interface to integrate our neural model in Unity, a real-time 3D development platform. Furthermore, we introduce two new datasets representing the static human pose modeling problem, based on high-quality human motion capture data, which will be released publicly along with model code. | Boris N. Oreshkin, Florent Bocquelet, Félix G. Harvey, Bay Raitt, Dominic Laflamme |  |
| 49 |  |  [Hyperparameter Tuning with Renyi Differential Privacy](https://openreview.net/forum?id=-70L8lpp9DF) |  | 0 | For many differentially private algorithms, such as the prominent noisy stochastic gradient descent (DP-SGD), the analysis needed to bound the privacy leakage of a single training run is well understood. However, few studies have reasoned about the privacy leakage resulting from the multiple training runs needed to fine tune the value of the training algorithm’s hyperparameters. In this work, we first illustrate how simply setting hyperparameters based on non-private training runs can leak private information. Motivated by this observation, we then provide privacy guarantees for hyperparameter search procedures within the framework of Renyi Differential Privacy. Our results improve and extend the work of Liu and Talwar (STOC 2019). Our analysis supports our previous observation that tuning hyperparameters does indeed leak private information, but we prove that, under certain assumptions, this leakage is modest, as long as each candidate training run needed to select hyperparameters is itself differentially private. | Nicolas Papernot, Thomas Steinke |  |
| 50 |  |  [Real-Time Neural Voice Camouflage](https://openreview.net/forum?id=qj1IZ-6TInc) |  | 0 | Automatic speech recognition systems have created exciting possibilities for applications, however they also enable opportunities for systematic eavesdropping.We propose a method to camouflage a person's voice from these systems without inconveniencing the conversation between people in the room. Standard adversarial attacks are not effective in real-time streaming situations because the characteristics of the signal will have changed by the time the attack is executed. We introduce predictive adversarial attacks, which achieves real-time performance by forecasting the attack vector that will be the most effective in the future. Under real-time constraints, our method jams the established speech recognition system DeepSpeech 3.9x more than online projected gradient descent as measured through word error rate, and 6.6x more as measured through character error rate. We furthermore demonstrate our approach is practically effective in realistic environments with complex scene geometries. | Mia Chiquier, Chengzhi Mao, Carl Vondrick |  |
| 51 |  |  [CycleMLP: A MLP-like Architecture for Dense Prediction](https://openreview.net/forum?id=NMEceG4v69Y) |  | 0 | This paper presents a simple MLP-like architecture, CycleMLP, which is a versatile backbone for visual recognition and dense predictions. As compared to modern MLP architectures, e.g. , MLP-Mixer, ResMLP, and gMLP, whose architectures are correlated to image size and thus are infeasible in object detection and segmentation, CycleMLP has two advantages compared to modern approaches. (1) It can cope with various image sizes. (2) It achieves linear computational complexity to image size by using local windows. In contrast, previous MLPs have $O(N^2)$ computations due to fully spatial connections. We build a family of models which surpass existing MLPs and even state-of-the-art Transformer-based models, e.g. Swin Transformer, while using fewer parameters and FLOPs. We expand the MLP-like models’ applicability, making them a versatile backbone for dense prediction tasks. CycleMLP achieves competitive results on object detection, instance segmentation, and semantic segmentation. In particular, CycleMLP-Tiny outperforms Swin-Tiny by 1.3% mIoU on ADE20K dataset with fewer FLOPs. Moreover, CycleMLP also shows excellent zero-shot robustness on ImageNet-C dataset. | Shoufa Chen, Enze Xie, Chongjian Ge, Runjian Chen, Ding Liang, Ping Luo |  |
| 52 |  |  [Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models](https://openreview.net/forum?id=0xiJLKH-ufZ) |  | 0 | Diffusion probabilistic models (DPMs) represent a class of powerful generative models. Despite their success, the inference of DPMs is expensive since it generally needs to iterate over thousands of timesteps. A key problem in the inference is to estimate the variance in each timestep of the reverse process. In this work, we present a surprising result that both the optimal reverse variance and the corresponding optimal KL divergence of a DPM have analytic forms w.r.t. its score function. Building upon it, we propose \textit{Analytic-DPM}, a training-free inference framework that estimates the analytic forms of the variance and KL divergence using the Monte Carlo method and a pretrained score-based model. Further, to correct the potential bias caused by the score-based model, we derive both lower and upper bounds of the optimal variance and clip the estimate for a better result. Empirically, our analytic-DPM improves the log-likelihood of various DPMs, produces high-quality samples, and meanwhile enjoys a $20\times$ to $80\times$ speed up. | Fan Bao, Chongxuan Li, Jun Zhu, Bo Zhang |  |
| 53 |  |  [RISP: Rendering-Invariant State Predictor with Differentiable Simulation and Rendering for Cross-Domain Parameter Estimation](https://openreview.net/forum?id=uSE03demja) |  | 0 | This work considers identifying parameters characterizing a physical system's dynamic motion directly from a video whose rendering configurations are inaccessible. Existing solutions require massive training data or lack generalizability to unknown rendering configurations. We propose a novel approach that marries domain randomization and differentiable rendering gradients to address this problem. Our core idea is to train a rendering-invariant state-prediction (RISP) network that transforms image differences into state differences independent of rendering configurations, e.g., lighting, shadows, or material reflectance. To train this predictor, we formulate a new loss on rendering variances using gradients from differentiable rendering. Moreover, we present an efficient, second-order method to compute the gradients of this loss, allowing it to be integrated seamlessly into modern deep learning frameworks. We evaluate our method in rigid-body and deformable-body simulation environments using four tasks: state estimation, system identification, imitation learning, and visuomotor control. We further demonstrate the efficacy of our approach on a real-world example: inferring the state and action sequences of a quadrotor from a video of its motion sequences. Compared with existing methods, our approach achieves significantly lower reconstruction errors and has better generalizability among unknown rendering configurations. | Pingchuan Ma, Tao Du, Joshua B. Tenenbaum, Wojciech Matusik, Chuang Gan |  |
| 54 |  |  [The Information Geometry of Unsupervised Reinforcement Learning](https://openreview.net/forum?id=3wU2UX0voE) |  | 0 | How can a reinforcement learning (RL) agent prepare to solve downstream tasks if those tasks are not known a priori? One approach is unsupervised skill discovery, a class of algorithms that learn a set of policies without access to a reward function. Such algorithms bear a close resemblance to representation learning algorithms (e.g., contrastive learning) in supervised learning, in that both are pretraining algorithms that maximize some approximation to a mutual information objective. While prior work has shown that the set of skills learned by such methods can accelerate downstream RL tasks, prior work offers little analysis into whether these skill learning algorithms are optimal, or even what notion of optimality would be appropriate to apply to them. In this work, we show that unsupervised skill discovery algorithms based on mutual information maximization do not learn skills that are optimal for every possible reward function. However, we show that the distribution over skills provides an optimal initialization minimizing regret against adversarially-chosen reward functions, assuming a certain type of adaptation procedure. Our analysis also provides a geometric perspective on these skill learning methods. | Benjamin Eysenbach, Ruslan Salakhutdinov, Sergey Levine |  |
| 55 |  |  [Language modeling via stochastic processes](https://openreview.net/forum?id=pMQwKL1yctf) |  | 0 | Modern language models can generate high-quality short texts. However, they often meander or are incoherent when generating longer texts. These issues arise from the next-token-only language modeling objective. To address these issues, we introduce Time Control (TC), a language model that implicitly plans via a latent stochastic process. TC does this by learning a representation which maps the dynamics of how text changes in a document to the dynamics of a stochastic process of interest. Using this representation, the language model can generate text by first implicitly generating a document plan via a stochastic process, and then generating text that is consistent with this latent plan. Compared to domain-specific methods and fine-tuning GPT2 across a variety of text domains, TC improves performance on text infilling and discourse coherence. On long text generation settings, TC preserves the text structure both in terms of ordering (up to +40% better) and text length consistency (up to +17% better). Human evaluators also prefer TC's output 28.6% more than the baselines. | Rose E. Wang, Esin Durmus, Noah D. Goodman, Tatsunori Hashimoto |  |
| 56 |  |  [Learning to Downsample for Segmentation of Ultra-High Resolution Images](https://openreview.net/forum?id=HndgQudNb91) |  | 0 | Many computer vision systems require low-cost segmentation algorithms based on deep learning, either because of the enormous size of input images or limited computational budget. Common solutions uniformly downsample the input images to meet memory constraints, assuming all pixels are equally informative. In this work, we demonstrate that this assumption can harm the segmentation performance because the segmentation difficulty varies spatially (see Figure 1 “Uniform”). We combat this problem by introducing a learnable downsampling module, which can be optimised together with the given segmentation model in an end-to-end fashion. We formulate the problem of training such downsampling module as optimisation of sampling density distributions over the input images given their low-resolution views. To defend against degenerate solutions (e.g. over-sampling trivial regions like the backgrounds), we propose a regularisation term that encourages the sampling locations to concentrate around the object boundaries. We find the downsampling module learns to sample more densely at difficult locations, thereby improving the segmentation performance (see Figure 1 "Ours"). Our experiments on benchmarks of high-resolution street view, aerial and medical images demonstrate substantial improvements in terms of efficiency-and-accuracy trade-off compared to both uniform downsampling and two recent advanced downsampling techniques. | Chen Jin, Ryutaro Tanno, Thomy Mertzanidou, Eleftheria Panagiotaki, Daniel C. Alexander |  |
| 57 |  |  [Variational Neural Cellular Automata](https://openreview.net/forum?id=7fFO4cMBx_9) |  | 0 | In nature, the process of cellular growth and differentiation has lead to an amazing diversity of organisms --- algae, starfish, giant sequoia, tardigrades, and orcas are all created by the same generative process. Inspired by the incredible diversity of this biological generative process, we propose a generative model, the Variational Neural Cellular Automata (VNCA), which is loosely inspired by the biological processes of cellular growth and differentiation. Unlike previous related works, the VNCA is a proper probabilistic generative model, and we evaluate it according to best practices. We find that the VNCA learns to reconstruct samples well and that despite its relatively few parameters and simple local-only communication, the VNCA can learn to generate a large variety of output from information encoded in a common vector format. While there is a significant gap to the current state-of-the-art in terms of generative modeling performance, we show that the VNCA can learn a purely self-organizing generative process of data. Additionally, the self-organizing nature bestows the VNCA with some inherent robustness against perturbations in the early stages of growth. | Rasmus Berg Palm, Miguel González Duque, Shyam Sudhakaran, Sebastian Risi |  |
| 58 |  |  [Wish you were here: Hindsight Goal Selection for long-horizon dexterous manipulation](https://openreview.net/forum?id=FKp8-pIRo3y) |  | 0 | Complex sequential tasks in continuous-control settings often require agents to successfully traverse a set of \`\`narrow passages'' in their state space. Solving such tasks with a sparse reward in a sample-efficient manner poses a challenge to modern reinforcement learning (RL) due to the associated long-horizon nature of the problem and the lack of sufficient positive signal during learning. Various tools have been applied to address this challenge. When available, large sets of demonstrations can guide agent exploration. Hindsight relabelling on the other hand does not require additional sources of information. However, existing strategies explore based on task-agnostic goal distributions, which can render the solution of long-horizon tasks impractical. In this work, we extend hindsight relabelling mechanisms to guide exploration along task-specific distributions implied by a small set of successful demonstrations. We evaluate the approach on four complex, single and dual arm, robotics manipulation tasks against strong suitable baselines. The method requires far fewer demonstrations to solve all tasks and achieves a significantly higher overall performance as task complexity increases. Finally, we investigate the robustness of the proposed solution with respect to the quality of input representations and the number of demonstrations. | Todor Davchev, Oleg Olegovich Sushkov, JeanBaptiste Regli, Stefan Schaal, Yusuf Aytar, Markus Wulfmeier, Jon Scholz |  |
| 59 |  |  [L0-Sparse Canonical Correlation Analysis](https://openreview.net/forum?id=KntaNRo6R48) |  | 0 | Canonical Correlation Analysis (CCA) models are powerful for studying the associations between two sets of variables. The canonically correlated representations, termed \textit{canonical variates} are widely used in unsupervised learning to analyze unlabeled multi-modal registered datasets. Despite their success, CCA models may break (or overfit) if the number of variables in either of the modalities exceeds the number of samples. Moreover, often a significant fraction of the variables measures modality-specific information, and thus removing them is beneficial for identifying the \textit{canonically correlated variates}. Here, we propose $\ell_0$-CCA, a method for learning correlated representations based on sparse subsets of variables from two observed modalities. Sparsity is obtained by multiplying the input variables by stochastic gates, whose parameters are learned together with the CCA weights via an $\ell_0$-regularized correlation loss. We further propose $\ell_0$-Deep CCA for solving the problem of non-linear sparse CCA by modeling the correlated representations using deep nets. We demonstrate the efficacy of the method using several synthetic and real examples. Most notably, by gating nuisance input variables, our approach improves the extracted representations compared to other linear, non-linear and sparse CCA-based models. | Ofir Lindenbaum, Moshe Salhov, Amir Averbuch, Yuval Kluger |  |
| 60 |  |  [Recycling Model Updates in Federated Learning: Are Gradient Subspaces Low-Rank?](https://openreview.net/forum?id=B7ZbqNLDn-_) |  | 0 | In this paper, we question the rationale behind propagating large numbers of parameters through a distributed system during federated learning. We start by examining the rank characteristics of the subspace spanned by gradients (i.e., the gradient-space) in centralized model training, and observe that the gradient-space often consists of a few leading principal components accounting for an overwhelming majority (95-99%) of the explained variance. Motivated by this, we propose the "Look-back Gradient Multiplier" (LBGM) algorithm, which utilizes this low-rank property of the gradient-space in federated learning. Operationally, LBGM recycles the gradients between model update rounds to significantly reduce the number of parameters to be propagated through the system. We analytically characterize the convergence behavior of LBGM, revealing the nature of the trade-off between communication savings and model performance. Our subsequent experimental results demonstrate the improvement LBGM obtains on communication overhead compared to federated learning baselines. Additionally, we show that LBGM is a general plug-and-play algorithm that can be used standalone or stacked on top of existing sparsification techniques for distributed model training. | Sheikh Shams Azam, Seyyedali Hosseinalipour, Qiang Qiu, Christopher G. Brinton |  |
| 61 |  |  [Is Homophily a Necessity for Graph Neural Networks?](https://openreview.net/forum?id=ucASPPD9GKN) |  | 0 | Graph neural networks (GNNs) have shown great prowess in learning representations suitable for numerous graph-based machine learning tasks. When applied to semi-supervised node classification, GNNs are widely believed to work well due to the homophily assumption (\`\`like attracts like''), and fail to generalize to heterophilous graphs where dissimilar nodes connect. Recent works design new architectures to overcome such heterophily-related limitations, citing poor baseline performance and new architecture improvements on a few heterophilous graph benchmark datasets as evidence for this notion. In our experiments, we empirically find that standard graph convolutional networks (GCNs) can actually achieve better performance than such carefully designed methods on some commonly used heterophilous graphs. This motivates us to reconsider whether homophily is truly necessary for good GNN performance. We find that this claim is not quite true, and in fact, GCNs can achieve strong performance on heterophilous graphs under certain conditions. Our work carefully characterizes these conditions and provides supporting theoretical understanding and empirical observations. Finally, we examine existing heterophilous graphs benchmarks and reconcile how the GCN (under)performs on them based on this understanding. | Yao Ma, Xiaorui Liu, Neil Shah, Jiliang Tang |  |
| 62 |  |  [DEGREE: Decomposition Based Explanation for Graph Neural Networks](https://openreview.net/forum?id=Ve0Wth3ptT_) |  | 0 | Graph Neural Networks (GNNs) are gaining extensive attention for their application in graph data. However, the black-box nature of GNNs prevents users from understanding and trusting the models, thus hampering their applicability. Whereas explaining GNNs remains a challenge, most existing methods fall into approximation based and perturbation based approaches with suffer from faithfulness problems and unnatural artifacts respectively. To tackle these problems, we propose DEGREE (Decomposition based Explanation for GRaph nEural nEtworks) to provide a faithful explanation for GNN predictions. By decomposing the information generation and aggregation mechanism of GNNs, DEGREE allows tracking the contributions of specific components of the input graph to the final prediction. Based on this, we further design a subgraph level interpretation algorithm to reveal complex interactions between graph nodes that are overlooked by previous methods. The efficiency of our algorithm can be further improved by utilizing GNN characteristics. Finally, we conduct quantitative and qualitative experiments on synthetic and real-world datasets to demonstrate the effectiveness of DEGREE on node classification and graph classification tasks. | Qizhang Feng, Ninghao Liu, Fan Yang, Ruixiang Tang, Mengnan Du, Xia Hu |  |
| 63 |  |  [Improving Mutual Information Estimation with Annealed and Energy-Based Bounds](https://openreview.net/forum?id=T0B9AoM_bFg) |  | 0 | Mutual information (MI) is a fundamental quantity in information theory and machine learning. However, direct estimation of MI is intractable, even if the true joint probability density for the variables of interest is known, as it involves estimating a potentially high-dimensional log partition function. In this work, we present a unifying view of existing MI bounds from the perspective of importance sampling, and propose three novel bounds based on this approach. Since a tight MI bound without density information requires a sample size exponential in the true MI, we assume either a single marginal or the full joint density information is known. In settings where the full joint density is available, we propose Multi-Sample Annealed Importance Sampling (AIS) bounds on MI, which we demonstrate can tightly estimate large values of MI in our experiments. In settings where only a single marginal distribution is known, we propose Generalized IWAE (GIWAE) and MINE-AIS bounds. Our GIWAE bound unifies variational and contrastive bounds in a single framework that generalizes InfoNCE, IWAE, and Barber-Agakov bounds. Our MINE-AIS method improves upon existing energy-based methods such as MINE-DV and MINE-F by directly optimizing a tighter lower bound on MI. MINE-AIS uses MCMC sampling to estimate gradients for training and Multi-Sample AIS for evaluating the bound. Our methods are particularly suitable for evaluating MI in deep generative models, since explicit forms of the marginal or joint densities are often available. We evaluate our bounds on estimating the MI of VAEs and GANs trained on the MNIST and CIFAR datasets, and showcase significant gains over existing bounds in these challenging settings with high ground truth MI. | Rob Brekelmans, Sicong Huang, Marzyeh Ghassemi, Greg Ver Steeg, Roger Baker Grosse, Alireza Makhzani |  |
| 64 |  |  [Sequence Approximation using Feedforward Spiking Neural Network for Spatiotemporal Learning: Theory and Optimization Methods](https://openreview.net/forum?id=bp-LJ4y_XC) |  | 0 | A dynamical system of spiking neurons with only feedforward connections can classify spatiotemporal patterns without recurrent connections. However, the theoretical construct of a feedforward spiking neural network (SNN) for approximating a temporal sequence remains unclear, making it challenging to optimize SNN architectures for learning complex spatiotemporal patterns. In this work, we establish a theoretical framework to understand and improve sequence approximation using a feedforward SNN. Our framework shows that a feedforward SNN with one neuron per layer and skip-layer connections can approximate the mapping function between any arbitrary pairs of input and output spike train on a compact domain. Moreover, we prove that heterogeneous neurons with varying dynamics and skip-layer connections improve sequence approximation using feedforward SNN. Consequently, we propose SNN architectures incorporating the preceding constructs that are trained using supervised backpropagation-through-time (BPTT) and unsupervised spiking-timing-dependent plasticity (STDP) algorithms for classification of spatiotemporal data. A dual-search-space Bayesian optimization method is developed to optimize architecture and parameters of the proposed SNN with heterogeneous neuron dynamics and skip-layer connections. | Xueyuan She, Saurabh Dash, Saibal Mukhopadhyay |  |
| 65 |  |  [Diverse Client Selection for Federated Learning via Submodular Maximization](https://openreview.net/forum?id=nwKXyFvaUm) |  | 0 | In every communication round of federated learning, a random subset of clients communicate their model updates back to the server which then aggregates them all. The optimal size of this subset is not known and several studies have shown that typically random selection does not perform very well in terms of convergence, learning efficiency and fairness. We, in this paper, propose to select a small diverse subset of clients, namely those carrying representative gradient information, and we transmit only these updates to the server. Our aim is for updating via only a subset to approximate updating via aggregating all client information. We achieve this by choosing a subset that maximizes a submodular facility location function defined over gradient space. We introduce “federated averaging with diverse client selection (DivFL)”. We provide a thorough analysis of its convergence in the heterogeneous setting and apply it both to synthetic and to real datasets. Empirical results show several benefits to our approach including improved learning efficiency, faster convergence and also more uniform (i.e., fair) performance across clients. We further show a communication-efficient version of DivFL that can still outperform baselines on the above metrics. | Ravikumar Balakrishnan, Tian Li, Tianyi Zhou, Nageen Himayat, Virginia Smith, Jeff A. Bilmes |  |
| 66 |  |  [From Intervention to Domain Transportation: A Novel Perspective to Optimize Recommendation](https://openreview.net/forum?id=jT1EwXu-4hj) |  | 0 | The interventional nature of recommendation has attracted increasing attention in recent years. It particularly motivates researchers to formulate learning and evaluating recommendation as causal inference and data missing-not-at-random problems. However, few take seriously the consequence of violating the critical assumption of overlapping, which we prove can significantly threaten the validity and interpretation of the outcome. We find a critical piece missing in the current understanding of information retrieval (IR) systems: as interventions, recommendation not only affects the already observed data, but it also interferes with the target domain (distribution) of interest. We then rephrase optimizing recommendation as finding an intervention that best transports the patterns it learns from the observed domain to its intervention domain. Towards this end, we use domain transportation to characterize the learning-intervention mechanism of recommendation. We design a principled transportation-constraint risk minimization objective and convert it to a two-player minimax game. We prove the consistency, generalization, and excessive risk bounds for the proposed objective, and elaborate how they compare to the current results. Finally, we carry out extensive real-data and semi-synthetic experiments to demonstrate the advantage of our approach, and launch online testing with a real-world IR system. | Da Xu, Yuting Ye, Chuanwei Ruan, Evren Körpeoglu, Sushant Kumar, Kannan Achan |  |
| 67 |  |  [Variational Predictive Routing with Nested Subjective Timescales](https://openreview.net/forum?id=JxFgJbZ-wft) |  | 0 | Discovery and learning of an underlying spatiotemporal hierarchy in sequential data is an important topic for machine learning. Despite this, little work has been done to explore hierarchical generative models that can flexibly adapt their layerwise representations in response to datasets with different temporal dynamics. Here, we present Variational Predictive Routing (VPR) – a neural probabilistic inference system that organizes latent representations of video features in a temporal hierarchy, based on their rates of change, thus modeling continuous data as a hierarchical renewal process. By employing an event detection mechanism that relies solely on the system’s latent representations (without the need of a separate model), VPR is able to dynamically adjust its internal state following changes in the observed features, promoting an optimal organisation of representations across the levels of the model’s latent hierarchy. Using several video datasets, we show that VPR is able to detect event boundaries, disentangle spatiotemporal features across its hierarchy, adapt to the dynamics of the data, and produce accurate time-agnostic rollouts of the future. Our approach integrates insights from neuroscience and introduces a framework with high potential for applications in model-based reinforcement learning, where flexible and informative state-space rollouts are of particular interest. | Alexey Zakharov, Qinghai Guo, Zafeirios Fountas |  |
| 68 |  |  [Sample and Computation Redistribution for Efficient Face Detection](https://openreview.net/forum?id=RhB1AdoFfGE) |  | 0 | Although tremendous strides have been made in uncontrolled face detection, accurate face detection with a low computation cost remains an open challenge. In this paper, we point out that computation distribution and scale augmentation are the keys to detecting small faces from low-resolution images. Motivated by these observations, we introduce two simple but effective methods: (1) Computation Redistribution (CR), which reallocates the computation between the backbone, neck and head of the model; and (2) Sample Redistribution (SR), which augments training samples for the most needed stages. The proposed Sample and Computation Redistribution for Face Detection (SCRFD) is implemented by a random search in a meticulously designed search space. Extensive experiments conducted on WIDER FACE demonstrate the state-of-the-art accuracy-efficiency trade-off for the proposed SCRFD family across a wide range of compute regimes. In particular, SCRFD-34GF outperforms the best competitor, TinaFace, by $4.78\%$ (AP at hard set) while being more than 3$\times$ faster on GPUs with VGA-resolution images. Code is available at: https://github.com/deepinsight/insightface/tree/master/detection/scrfd. | Jia Guo, Jiankang Deng, Alexandros Lattas, Stefanos Zafeiriou |  |
| 69 |  |  [Sound Adversarial Audio-Visual Navigation](https://openreview.net/forum?id=NkZq4OEYN-) |  | 0 | Audio-visual navigation task requires an agent to find a sound source in a realistic, unmapped 3D environment by utilizing egocentric audio-visual observations. Existing audio-visual navigation works assume a clean environment that solely contains the target sound, which, however, would not be suitable in most real-world applications due to the unexpected sound noise or intentional interference. In this work, we design an acoustically complex environment in which, besides the target sound, there exists a sound attacker playing a zero-sum game with the agent. More specifically, the attacker can move and change the volume and category of the sound to make the agent suffer from finding the sounding object while the agent tries to dodge the attack and navigate to the goal under the intervention. Under certain constraints to the attacker, we can improve the robustness of the agent towards unexpected sound attacks in audio-visual navigation. For better convergence, we develop a joint training mechanism by employing the property of a centralized critic with decentralized actors. Experiments on two real-world 3D scan datasets, Replica, and Matterport3D, verify the effectiveness and the robustness of the agent trained under our designed environment when transferred to the clean environment or the one containing sound attackers with random policy. Project: https://yyf17.github.io/SAAVN . | Yinfeng Yu, Wenbing Huang, Fuchun Sun, Changan Chen, Yikai Wang, Xiaohong Liu |  |
| 70 |  |  [Out-of-distribution Generalization in the Presence of Nuisance-Induced Spurious Correlations](https://openreview.net/forum?id=12RoR2o32T) |  | 0 | In many prediction problems, spurious correlations are induced by a changing relationship between the label and a nuisance variable that is also correlated with the covariates. For example, in classifying animals in natural images, the background, which is a nuisance, can predict the type of animal. This nuisance-label relationship does not always hold, and the performance of a model trained under one such relationship may be poor on data with a different nuisance-label relationship. To build predictive models that perform well regardless of the nuisance-label relationship, we develop Nuisance-Randomized Distillation (NURD). We introduce the nuisance-randomized distribution, a distribution where the nuisance and the label are independent. Under this distribution, we define the set of representations such that conditioning on any member, the nuisance and the label remain independent. We prove that the representations in this set always perform better than chance, while representations outside of this set may not. NURD finds a representation from this set that is most informative of the label under the nuisance-randomized distribution, and we prove that this representation achieves the highest performance regardless of the nuisance-label relationship. We evaluate NURD on several tasks including chest X-ray classification where, using non-lung patches as the nuisance, NURD produces models that predict pneumonia under strong spurious correlations. | Aahlad Manas Puli, Lily H. Zhang, Eric Karl Oermann, Rajesh Ranganath |  |
| 71 |  |  [AEVA: Black-box Backdoor Detection Using Adversarial Extreme Value Analysis](https://openreview.net/forum?id=OM_lYiHXiCL) |  | 0 | Deep neural networks (DNNs) are proved to be vulnerable against backdoor attacks. A backdoor could be embedded in the target DNNs through injecting a backdoor trigger into the training examples, which can cause the target DNNs misclassify an input attached with the backdoor trigger. Recent backdoor detection methods often require the access to the original poisoned training data, the parameters of the target DNNs, or the predictive confidence for each given input, which are impractical in many real-world applications, e.g., on-device de-ployed DNNs. We address the black-box hard-label backdoor detection problem where the DNN is a fully black-box and only its final output label is accessible. We approach this problem from the optimization perspective and show that the objective of backdoor detection is bounded by an adversarial objective. Further theoretical and empirical studies reveal that this adversarial objective leads to a solution with highly skewed distribution; a singularity is often observed in the adversarial map of a backdoor-infected example, which we call the adversarial singularity phenomenon. Based on this observation, we propose the adversarial extreme value analysis(AEVA) algorithm to detect backdoors in black-box neural networks. The AEVA algorithm is based on an extreme value analysis on the adversarial map, computed from the monte-carlo gradient estimation due to the black-box hard-label constraint. Evidenced by extensive experiments across three popular tasks and backdoor attacks, our approach is shown effective in detecting backdoor attacks under the black-box hard-label scenarios | Junfeng Guo, Ang Li, Cong Liu |  |
| 72 |  |  [Resonance in Weight Space: Covariate Shift Can Drive Divergence of SGD with Momentum](https://openreview.net/forum?id=5ECQL05ub0J) |  | 0 | Most convergence guarantees for stochastic gradient descent with momentum (SGDm) rely on iid sampling. Yet, SGDm is often used outside this regime, in settings with temporally correlated input samples such as continual learning and reinforcement learning. Existing work has shown that SGDm with a decaying step-size can converge under Markovian temporal correlation. In this work, we show that SGDm under covariate shift with a fixed step-size can be unstable and diverge. In particular, we show SGDm under covariate shift is a parametric oscillator, and so can suffer from a phenomenon known as resonance. We approximate the learning system as a time varying system of ordinary differential equations, and leverage existing theory to characterize the system's divergence/convergence as resonant/nonresonant modes. The theoretical result is limited to the linear setting with periodic covariate shift, so we empirically supplement this result to show that resonance phenomena persist even under non-periodic covariate shift, nonlinear dynamics with neural networks, and optimizers other than SGDm. | Kirby Banman, Liam PeetPare, Nidhi Hegde, Alona Fyshe, Martha White |  |
| 73 |  |  [Top-label calibration and multiclass-to-binary reductions](https://openreview.net/forum?id=WqoBaaPHS-) |  | 0 | We propose a new notion of multiclass calibration called top-label calibration. A classifier is said to be top-label calibrated if the reported probability for the predicted class label---the top-label---is calibrated, conditioned on the top-label. This conditioning is essential for practical utility of the calibration property, since the top-label is always reported and we must condition on what is reported. However, the popular notion of confidence calibration erroneously skips this conditioning. Furthermore, we outline a multiclass-to-binary (M2B) reduction framework that unifies confidence, top-label, and class-wise calibration, among others. As its name suggests, M2B works by reducing multiclass calibration to different binary calibration problems; various types of multiclass calibration can then be achieved using simple binary calibration routines. We instantiate the M2B framework with the well-studied histogram binning (HB) binary calibrator, and prove that the overall procedure is multiclass calibrated without making any assumptions on the underlying data distribution. In an empirical evaluation with four deep net architectures on CIFAR-10 and CIFAR-100, we find that the M2B + HB procedure achieves lower top-label and class-wise calibration error than other approaches such as temperature scaling. Code for this work is available at https://github.com/aigen/df-posthoc-calibration. | Chirag Gupta, Aaditya Ramdas |  |
| 74 |  |  [Anisotropic Random Feature Regression in High Dimensions](https://openreview.net/forum?id=JfaWawZ8BmX) |  | 0 | In contrast to standard statistical wisdom, modern learning algorithms typically find their best performance in the overparameterized regime in which the model has many more parameters than needed to fit the training data. A growing number of recent works have shown that random feature models can offer a detailed theoretical explanation for this unexpected behavior, but typically these analyses have utilized isotropic distributional assumptions on the underlying data generation process, thereby failing to provide a realistic characterization of real-world models that are designed to identify and harness the structure in natural data. In this work, we examine the high-dimensional asymptotics of random feature regression in the presence of structured data, allowing for arbitrary input correlations and arbitrary alignment between the data and the weights of the target function. We define a partial order on the space of weight-data alignments and prove that generalization performance improves in response to stronger alignment. We also clarify several previous observations in the literature by distinguishing the behavior of the sample-wise and parameter-wise learning curves, finding that sample-wise multiple descent can occur at scales dictated by the eigenstructure of the data covariance, but that parameter-wise multiple descent is limited to double descent, although strong anisotropy can induce additional signatures such as wide plateaus and steep cliffs. Finally, these signatures are related to phase transitions in the spectrum of the feature kernel matrix, and unlike the double descent peak, persist even under optimal regularization. | Gabriel Mel, Jeffrey Pennington |  |
| 75 |  |  [Back2Future: Leveraging Backfill Dynamics for Improving Real-time Predictions in Future](https://openreview.net/forum?id=L01Nn_VJ9i) |  | 0 | For real-time forecasting in domains like public health and macroeconomics, data collection is a non-trivial and demanding task. Often after being initially released, it undergoes several revisions later (maybe due to human or technical constraints) - as a result, it may take weeks until the data reaches a stable value. This so-called ‘backfill’ phenomenon and its effect on model performance have been barely addressed in the prior literature. In this paper, we introduce the multi-variate backfill problem using COVID-19 as the motivating example. We construct a detailed dataset composed of relevant signals over the past year of the pandemic. We then systematically characterize several patterns in backfill dynamics and leverage our observations for formulating a novel problem and neural framework, Back2Future, that aims to refines a given model's predictions in real-time. Our extensive experiments demonstrate that our method refines the performance of the diverse set of top models for COVID-19 forecasting and GDP growth forecasting. Specifically, we show that Back2Future refined top COVID-19 models by 6.65% to 11.24% and yield an 18% improvement over non-trivial baselines. In addition, we show that our model improves model evaluation too; hence policy-makers can better understand the true accuracy of forecasting models in real-time. | Harshavardhan Kamarthi, Alexander Rodríguez, B. Aditya Prakash |  |
| 76 |  |  [Approximation and Learning with Deep Convolutional Models: a Kernel Perspective](https://openreview.net/forum?id=lrocYB-0ST2) |  | 0 | The empirical success of deep convolutional networks on tasks involving high-dimensional data such as images or audio suggests that they can efficiently approximate certain functions that are well-suited for such tasks. In this paper, we study this through the lens of kernel methods, by considering simple hierarchical kernels with two or three convolution and pooling layers, inspired by convolutional kernel networks. These achieve good empirical performance on standard vision datasets, while providing a precise description of their functional space that yields new insights on their inductive bias. We show that the RKHS consists of additive models of interaction terms between patches, and that its norm encourages spatial similarities between these terms through pooling layers. We then provide generalization bounds which illustrate how pooling and patches yield improved sample complexity guarantees when the target function presents such regularities. | Alberto Bietti |  |
| 77 |  |  [Value Function Spaces: Skill-Centric State Abstractions for Long-Horizon Reasoning](https://openreview.net/forum?id=vgqS1vkkCbE) |  | 0 | Reinforcement learning can train policies that effectively perform complex tasks. However for long-horizon tasks, the performance of these methods degrades with horizon, often necessitating reasoning over and chaining lower-level skills. Hierarchical reinforcement learning aims to enable this by providing a bank of low-level skills as action abstractions. Hierarchies can further improve on this by abstracting the space states as well. We posit that a suitable state abstraction should depend on the capabilities of the available lower-level policies. We propose Value Function Spaces: a simple approach that produces such a representation by using the value functions corresponding to each lower-level skill. These value functions capture the affordances of the scene, thus forming a representation that compactly abstracts task relevant information and robustly ignores distractors. Empirical evaluations for maze-solving and robotic manipulation tasks demonstrate that our approach improves long-horizon performance and enables better zero-shot generalization than alternative model-free and model-based methods. | Dhruv Shah, Peng Xu, Yao Lu, Ted Xiao, Alexander Toshev, Sergey Levine, Brian Ichter |  |
| 78 |  |  [Fast Regression for Structured Inputs](https://openreview.net/forum?id=gNp54NxHUPJ) |  | 0 | We study the $\ell_p$ regression problem, which requires finding $\mathbf{x}\in\mathbb R^{d}$ that minimizes $\\|\mathbf{A}\mathbf{x}-\mathbf{b}\\|_p$ for a matrix $\mathbf{A}\in\mathbb R^{n \times d}$ and response vector $\mathbf{b}\in\mathbb R^{n}$. There has been recent interest in developing subsampling methods for this problem that can outperform standard techniques when $n$ is very large. However, all known subsampling approaches have run time that depends exponentially on $p$, typically, $d^{\mathcal{O}(p)}$, which can be prohibitively expensive. We improve on this work by showing that for a large class of common \emph{structured matrices}, such as combinations of low-rank matrices, sparse matrices, and Vandermonde matrices, there are subsampling based methods for $\ell_p$ regression that depend polynomially on $p$. For example, we give an algorithm for $\ell_p$ regression on Vandermonde matrices that runs in time $\mathcal{O}(n\log^3 n+(dp^2)^{0.5+\omega}\cdot\text{polylog}\,n)$, where $\omega$ is the exponent of matrix multiplication. The polynomial dependence on $p$ crucially allows our algorithms to extend naturally to efficient algorithms for $\ell_\infty$ regression, via approximation of $\ell_\infty$ by $\ell_{\mathcal{O}(\log n)}$. Of practical interest, we also develop a new subsampling algorithm for $\ell_p$ regression for arbitrary matrices, which is simpler than previous approaches for $p \ge 4$. | Raphael A. Meyer, Cameron Musco, Christopher Musco, David P. Woodruff, Samson Zhou |  |
| 79 |  |  [CrossBeam: Learning to Search in Bottom-Up Program Synthesis](https://openreview.net/forum?id=qhC8mr2LEKq) |  | 0 | Many approaches to program synthesis perform a search within an enormous space of programs to find one that satisfies a given specification. Prior works have used neural models to guide combinatorial search algorithms, but such approaches still explore a huge portion of the search space and quickly become intractable as the size of the desired program increases. To tame the search space blowup, we propose training a neural model to learn a hands-on search policy for bottom-up synthesis, instead of relying on a combinatorial search algorithm. Our approach, called CrossBeam, uses the neural model to choose how to combine previously-explored programs into new programs, taking into account the search history and partial program executions. Motivated by work in structured prediction on learning to search, CrossBeam is trained on-policy using data extracted from its own bottom-up searches on training tasks. We evaluate CrossBeam in two very different domains, string manipulation and logic programming. We observe that CrossBeam learns to search efficiently, exploring much smaller portions of the program space compared to the state-of-the-art. | Kensen Shi, Hanjun Dai, Kevin Ellis, Charles Sutton |  |
| 80 |  |  [PEARL: Data Synthesis via Private Embeddings and Adversarial Reconstruction Learning](https://openreview.net/forum?id=M6M8BEmd6dq) |  | 0 | We propose a new framework of synthesizing data using deep generative models in a differentially private manner. Within our framework, sensitive data are sanitized with rigorous privacy guarantees in a one-shot fashion, such that training deep generative models is possible without re-using the original data. Hence, no extra privacy costs or model constraints are incurred, in contrast to popular gradient sanitization approaches, which, among other issues, cause degradation in privacy guarantees as the training iteration increases. We demonstrate a realization of our framework by making use of the characteristic function and an adversarial re-weighting objective, which are of independent interest as well. Our proposal has theoretical guarantees of performance, and empirical evaluations on multiple datasets show that our approach outperforms other methods at reasonable levels of privacy. | Seng Pei Liew, Tsubasa Takahashi, Michihiko Ueno |  |
| 81 |  |  [Divisive Feature Normalization Improves Image Recognition Performance in AlexNet](https://openreview.net/forum?id=aOX3a9q3RVV) |  | 0 | Local divisive normalization provides a phenomenological description of many nonlinear response properties of neurons across visual cortical areas. To gain insight into the utility of this operation, we studied the effects on AlexNet of a local divisive normalization between features, with learned parameters. Developing features were arranged in a line topology, with the influence between features determined by an exponential function of the distance between them. We compared an AlexNet model with no normalization or with canonical normalizations (Batch, Group, Layer) to the same models with divisive normalization added. Divisive normalization always improved performance for models with batch or group or no normalization, generally by 1-2 percentage points, on both the CIFAR-100 and ImageNet databases. To gain insight into mechanisms underlying the improved performance, we examined several aspects of network representations. In the early layers both canonical and divisive normalizations reduced manifold capacities and increased average dimension of the individual categorical manifolds. In later layers the capacity was higher and manifold dimension lower for models roughly in order of their performance improvement. Examining the sparsity of activations across a given layer, divisive normalization layers increased sparsity, while the canonical normalization layers decreased it. Nonetheless, in the final layer, the sparseness of activity increased in the order of no normalization, divisive, com- bined, and canonical. We also investigated how the receptive fields (RFs) in the first convolutional layer (where RFs are most interpretable) change with normalization. Divisive normalization enhanced RF Fourier power at low wavelengths, while divisive+canonical enhanced power at mid (batch, group) or low (layer) wavelengths, compared to canonical alone or no normalization. In conclusion, divisive normalization enhances image recognition performance, most strongly when combined with canonical normalization, and in doing so it reduces manifold capacity and sparsity in early layers while increasing them in final layers, and increases low- or mid-wavelength power in the first-layer receptive fields. | Michelle Miller, SueYeon Chung, Kenneth D. Miller |  |
| 82 |  |  [Evaluating Distributional Distortion in Neural Language Modeling](https://openreview.net/forum?id=bTteFbU99ye) |  | 0 | A fundamental characteristic of natural language is the high rate at which speakers produce novel expressions. Because of this novelty, a heavy-tail of rare events accounts for a significant amount of the total probability mass of distributions in language (Baayen, 2001). Standard language modeling metrics such as perplexity quantify the performance of language models (LM) in aggregate. As a result, we have relatively little understanding of whether neural LMs accurately estimate the probability of sequences in this heavy-tail of rare events. To address this gap, we develop a controlled evaluation scheme which uses generative models trained on natural data as artificial languages from which we can exactly compute sequence probabilities. Training LMs on generations from these artificial languages, we compare the sequence-level probability estimates given by LMs to the true probabilities in the target language. Our experiments reveal that LSTM and Transformer language models (i) systematically underestimate the probability of sequences drawn from the target language, and (ii) do so more severely for less-probable sequences. Investigating where this probability mass went, (iii) we find that LMs tend to overestimate the probability of ill formed (perturbed) sequences. In addition, we find that this underestimation behaviour (iv) is weakened, but not eliminated by greater amounts of training data, and (v) is exacerbated for target distributions with lower entropy. | Benjamin LeBrun, Alessandro Sordoni, Timothy J. O'Donnell |  |
| 83 |  |  [MaGNET: Uniform Sampling from Deep Generative Network Manifolds Without Retraining](https://openreview.net/forum?id=r5qumLiYwf9) |  | 0 | Deep Generative Networks (DGNs) are extensively employed in Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and their variants to approximate the data manifold, and data distribution on that manifold. However, training samples are often obtained based on preferences, costs, or convenience producing artifacts in the empirical data distribution e.g. the large fraction of smiling faces in the CelebA dataset or the large fraction of dark-haired individuals in FFHQ). {\em These inconsistencies will be reproduced when sampling from the trained DGN, which has far-reaching potential implications for fairness, data augmentation, anomaly detection, domain adaptation, and beyond.} In response, we develop a differential geometry based sampler -coined MaGNET- that, given any trained DGN, produces samples that are uniformly distributed on the learned manifold. We prove theoretically and empirically that our technique produces a uniform distribution on the manifold regardless of the training set distribution. We perform a range of experiments on various datasets and DGNs. One of them considers the state-of-the-art StyleGAN2 trained on FFHQ dataset, where uniform sampling via MaGNET increases distribution precision \& recall by 4.12\% \& 3.01\% and decreases gender bias by 41.2\%, without requiring labels or retraining. | Ahmed Imtiaz Humayun, Randall Balestriero, Richard G. Baraniuk |  |
| 84 |  |  [Neural Contextual Bandits with Deep Representation and Shallow Exploration](https://openreview.net/forum?id=xnYACQquaGV) |  | 0 | We study neural contextual bandits, a general class of contextual bandits, where each context-action pair is associated with a raw feature vector, but the specific reward generating function is unknown. We propose a novel learning algorithm that transforms the raw feature vector using the last hidden layer of a deep ReLU neural network (deep representation learning), and uses an upper confidence bound (UCB) approach to explore in the last linear layer (shallow exploration). We prove that under standard assumptions, our proposed algorithm achieves $\tilde{O}(\sqrt{T})$ finite-time regret, where $T$ is the learning time horizon. Compared with existing neural contextual bandit algorithms, our approach is computationally much more efficient since it only needs to explore in the last layer of the deep neural network. | Pan Xu, Zheng Wen, Handong Zhao, Quanquan Gu |  |
| 85 |  |  [PI3NN: Out-of-distribution-aware Prediction Intervals from Three Neural Networks](https://openreview.net/forum?id=NoB8YgRuoFU) |  | 0 | We propose a novel prediction interval (PI) method for uncertainty quantification, which addresses three major issues with the state-of-the-art PI methods. First, existing PI methods require retraining of neural networks (NNs) for every given confidence level and suffer from the crossing issue in calculating multiple PIs. Second, they usually rely on customized loss functions with extra sensitive hyperparameters for which fine tuning is required to achieve a well-calibrated PI. Third, they usually underestimate uncertainties of out-of-distribution (OOD) samples leading to over-confident PIs. Our PI3NN method calculates PIs from linear combinations of three NNs, each of which is independently trained using the standard mean squared error loss. The coefficients of the linear combinations are computed using root-finding algorithms to ensure tight PIs for a given confidence level. We theoretically prove that PI3NN can calculate PIs for a series of confidence levels without retraining NNs and it completely avoids the crossing issue. Additionally, PI3NN does not introduce any unusual hyperparameters resulting in a stable performance. Furthermore, we address OOD identification challenge by introducing an initialization scheme which provides reasonably larger PIs of the OOD samples than those of the in-distribution samples. Benchmark and real-world experiments show that our method outperforms several state-of-the-art approaches with respect to predictive uncertainty quality, robustness, and OOD samples identification. | Siyan Liu, Pei Zhang, Dan Lu, Guannan Zhang |  |
| 86 |  |  [Discriminative Similarity for Data Clustering](https://openreview.net/forum?id=kj0_45Y4r9i) |  | 0 | Similarity-based clustering methods separate data into clusters according to the pairwise similarity between the data, and the pairwise similarity is crucial for their performance. In this paper, we propose {\em Clustering by Discriminative Similarity (CDS)}, a novel method which learns discriminative similarity for data clustering. CDS learns an unsupervised similarity-based classifier from each data partition, and searches for the optimal partition of the data by minimizing the generalization error of the learnt classifiers associated with the data partitions. By generalization analysis via Rademacher complexity, the generalization error bound for the unsupervised similarity-based classifier is expressed as the sum of discriminative similarity between the data from different classes. It is proved that the derived discriminative similarity can also be induced by the integrated squared error bound for kernel density classification. In order to evaluate the performance of the proposed discriminative similarity, we propose a new clustering method using a kernel as the similarity function, CDS via unsupervised kernel classification (CDSK), with its effectiveness demonstrated by experimental results. | Yingzhen Yang, Ping Li |  |
| 87 |  |  [It Takes Four to Tango: Multiagent Self Play for Automatic Curriculum Generation](https://openreview.net/forum?id=q4tZR1Y-UIs) |  | 0 | We are interested in training general-purpose reinforcement learning agents that can solve a wide variety of goals. Training such agents efficiently requires automatic generation of a goal curriculum. This is challenging as it requires (a) exploring goals of increasing difficulty, while ensuring that the agent (b) is exposed to a diverse set of goals in a sample efficient manner and (c) does not catastrophically forget previously solved goals. We propose Curriculum Self Play (CuSP), an automated goal generation framework that seeks to satisfy these desiderata by virtue of a multi-player game with 4 agents. We extend the asymmetric curricula learning in PAIRED (Dennis et al., 2020) to a symmetrized game that carefully balances cooperation and competition between two off-policy student learners and two regret-maximizing teachers. CuSP additionally introduces entropic goal coverage and accounts for the non-stationary nature of the students, allowing us to automatically induce a curriculum that balances progressive exploration with anti-catastrophic exploitation. We demonstrate that our method succeeds at generating an effective curricula of goals for a range of control tasks, outperforming other methods at zero-shot test-time generalization to novel out-of-distribution goals. | Yuqing Du, Pieter Abbeel, Aditya Grover |  |
| 88 |  |  [CROP: Certifying Robust Policies for Reinforcement Learning through Functional Smoothing](https://openreview.net/forum?id=HOjLHrlZhmx) |  | 0 | As reinforcement learning (RL) has achieved great success and been even adopted in safety-critical domains such as autonomous vehicles, a range of empirical studies have been conducted to improve its robustness against adversarial attacks. However, how to certify its robustness with theoretical guarantees still remains challenging. In this paper, we present the ﬁrst uniﬁed framework CROP (Certifying Robust Policies for RL) to provide robustness certiﬁcation on both action and reward levels. In particular, we propose two robustness certiﬁcation criteria: robustness of per-state actions and lower bound of cumulative rewards. We then develop a local smoothing algorithm for policies derived from Q-functions to guarantee the robustness of actions taken along the trajectory; we also develop a global smoothing algorithm for certifying the lower bound of a ﬁnite-horizon cumulative reward, as well as a novel local smoothing algorithm to perform adaptive search in order to obtain tighter reward certiﬁcation. Empirically, we apply CROP to evaluate several existing empirically robust RL algorithms, including adversarial training and different robust regularization, in four environments (two representative Atari games, Highway, and CartPole). Furthermore, by evaluating these algorithms against adversarial attacks, we demonstrate that our certiﬁcations are often tight. All experiment results are available at website https://crop-leaderboard.github.io. | Fan Wu, Linyi Li, Zijian Huang, Yevgeniy Vorobeychik, Ding Zhao, Bo Li |  |
| 89 |  |  [Neural Link Prediction with Walk Pooling](https://openreview.net/forum?id=CCu6RcUMwK0) |  | 0 | Graph neural networks achieve high accuracy in link prediction by jointly leveraging graph topology and node attributes. Topology, however, is represented indirectly; state-of-the-art methods based on subgraph classification label nodes with distance to the target link, so that, although topological information is present, it is tempered by pooling. This makes it challenging to leverage features like loops and motifs associated with network formation mechanisms. We propose a link prediction algorithm based on a new pooling scheme called WalkPool. WalkPool combines the expressivity of topological heuristics with the feature-learning ability of neural networks. It summarizes a putative link by random walk probabilities of adjacent paths. Instead of extracting transition probabilities from the original graph, it computes the transition matrix of a \`\`predictive'' latent graph by applying attention to learned features; this may be interpreted as feature-sensitive topology fingerprinting. WalkPool can leverage unsupervised node features or be combined with GNNs and trained end-to-end. It outperforms state-of-the-art methods on all common link prediction benchmarks, both homophilic and heterophilic, with and without node attributes. Applying WalkPool to a set of unsupervised GNNs significantly improves prediction accuracy, suggesting that it may be used as a general-purpose graph pooling scheme. | Liming Pan, Cheng Shi, Ivan Dokmanic |  |
| 90 |  |  [On the Convergence of Certified Robust Training with Interval Bound Propagation](https://openreview.net/forum?id=YeShU5mLfLt) |  | 0 | Interval Bound Propagation (IBP) is so far the base of state-of-the-art methods for training neural networks with certifiable robustness guarantees when potential adversarial perturbations present, while the convergence of IBP training remains unknown in existing literature. In this paper, we present a theoretical analysis on the convergence of IBP training. With an overparameterized assumption, we analyze the convergence of IBP robust training. We show that when using IBP training to train a randomly initialized two-layer ReLU neural network with logistic loss, gradient descent can linearly converge to zero robust training error with a high probability if we have sufficiently small perturbation radius and large network width. | Yihan Wang, Zhouxing Shi, Quanquan Gu, ChoJui Hsieh |  |
| 91 |  |  [Pretraining Text Encoders with Adversarial Mixture of Training Signal Generators](https://openreview.net/forum?id=sX3XaHwotOg) |  | 0 | We present a new framework AMOS that pretrains text encoders with an Adversarial learning curriculum via a Mixture Of Signals from multiple auxiliary generators. Following ELECTRA-style pretraining, the main encoder is trained as a discriminator to detect replaced tokens generated by auxiliary masked language models (MLMs). Different from ELECTRA which trains one MLM as the generator, we jointly train multiple MLMs of different sizes to provide training signals at various levels of difficulty. To push the discriminator to learn better with challenging replaced tokens, we learn mixture weights over the auxiliary MLMs' outputs to maximize the discriminator loss by backpropagating the gradient from the discriminator via Gumbel-Softmax. For better pretraining efficiency, we propose a way to assemble multiple MLMs into one unified auxiliary model. AMOS outperforms ELECTRA and recent state-of-the-art pretrained models by about 1 point on the GLUE benchmark for BERT base-sized models. | Yu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Tiwary, Paul N. Bennett, Jiawei Han, Xia Song |  |
| 92 |  |  [Towards Training Billion Parameter Graph Neural Networks for Atomic Simulations](https://openreview.net/forum?id=0jP2n0YFmKG) |  | 0 | Recent progress in Graph Neural Networks (GNNs) for modeling atomic simulations has the potential to revolutionize catalyst discovery, which is a key step in making progress towards the energy breakthroughs needed to combat climate change. However, the GNNs that have proven most effective for this task are memory intensive as they model higher-order interactions in the graphs such as those between triplets or quadruplets of atoms, making it challenging to scale these models. In this paper, we introduce Graph Parallelism, a method to distribute input graphs across multiple GPUs, enabling us to train very large GNNs with hundreds of millions or billions of parameters. We empirically evaluate our method by scaling up the recently proposed DimeNet++ and GemNet models by over an order of magnitude in the number of parameters. On the large-scale Open Catalyst 2020 (OC20) dataset, these graph-parallelized models lead to relative improvements of 1) 15% on the force MAE metric on the S2EF task and 2) 21% on the AFbT metric on the IS2RS task, establishing new state-of-the-art results. | Anuroop Sriram, Abhishek Das, Brandon M. Wood, Siddharth Goyal, C. Lawrence Zitnick |  |
| 93 |  |  [Understanding and Leveraging Overparameterization in Recursive Value Estimation](https://openreview.net/forum?id=shbAgEsk3qM) |  | 0 | The theory of function approximation in reinforcement learning (RL) typically considers low capacity representations that incur a tradeoff between approximation error, stability and generalization. Current deep architectures, however, operate in an overparameterized regime where approximation error is not necessarily a bottleneck. To better understand the utility of deep models in RL we present an analysis of recursive value estimation using \emph{overparameterized} linear representations that provides useful, transferable findings. First, we show that classical updates such as temporal difference (TD) learning or fitted-value-iteration (FVI) converge to \emph{different} fixed points than residual minimization (RM) in the overparameterized linear case. We then develop a unified interpretation of overparameterized linear value estimation as minimizing the Euclidean norm of the weights subject to alternative constraints. A practical consequence is that RM can be modified by a simple alteration of the backup targets to obtain the same fixed points as FVI and TD (when they converge), while universally ensuring stability. Further, we provide an analysis of the generalization error of these methods, demonstrating per iterate bounds on the value prediction error of FVI, and fixed point bounds for TD and RM. Given this understanding, we then develop new algorithmic tools for improving recursive value estimation with deep models. In particular, we extract two regularizers that penalize out-of-span top-layer weights and co-linearity in top-layer features respectively. Empirically we find that these regularizers dramatically improve the stability of TD and FVI, while allowing RM to match and even sometimes surpass their generalization performance with assured stability. | Chenjun Xiao, Bo Dai, Jincheng Mei, Oscar A. Ramirez, Ramki Gummadi, Chris Harris, Dale Schuurmans |  |
| 94 |  |  [Optimization and Adaptive Generalization of Three layer Neural Networks](https://openreview.net/forum?id=dPyRNUlttBv) |  | 0 | While there has been substantial recent work studying generalization of neural networks, the ability of deep nets in automating the process of feature extraction still evades a thorough mathematical understanding. As a step toward this goal, we analyze learning and generalization of a three-layer neural network with ReLU activations in a regime that goes beyond the linear approximation of the network, and is hence not captured by the common Neural Tangent Kernel. We show that despite nonconvexity of the empirical loss, a variant of SGD converges in polynomially many iterations to a good solution that generalizes. In particular, our generalization bounds are adaptive: they automatically optimize over a family of kernels that includes the Neural Tangent Kernel, to provide the tightest bound. | Khashayar Gatmiry, Stefanie Jegelka, Jonathan A. Kelner |  |
| 95 |  |  [Non-Parallel Text Style Transfer with Self-Parallel Supervision](https://openreview.net/forum?id=-TSe5o7STVR) |  | 0 | The performance of existing text style transfer models is severely limited by the non-parallel datasets on which the models are trained. In non-parallel datasets, no direct mapping exists between sentences of the source and target style; the style transfer models thus only receive weak supervision of the target sentences during training, which often leads the model to discard too much style-independent information, or utterly fail to transfer the style. In this work, we propose LaMer, a novel text style transfer framework based on large-scale language models. LaMer first mines the roughly parallel expressions in the non-parallel datasets with scene graphs, and then employs MLE training, followed by imitation learning refinement, to leverage the intrinsic parallelism within the data. On two benchmark tasks (sentiment & formality transfer) and a newly proposed challenging task (political stance transfer), our model achieves qualitative advances in transfer accuracy, content preservation, and fluency. Further empirical and human evaluations demonstrate that our model not only makes training more efficient, but also generates more readable and diverse expressions than previous models. | Ruibo Liu, Chongyang Gao, Chenyan Jia, Guangxuan Xu, Soroush Vosoughi |  |
| 96 |  |  [Can an Image Classifier Suffice For Action Recognition?](https://openreview.net/forum?id=qhkFX-HLuHV) |  | 0 | We explore a new perspective on video understanding by casting the video recognition problem as an image recognition task. Our approach rearranges input video frames into super images, which allow for training an image classifier directly to fulfill the task of action recognition, in exactly the same way as image classification. With such a simple idea, we show that transformer-based image classifiers alone can suffice for action recognition. In particular, our approach demonstrates strong and promising performance against SOTA methods on several public datasets including Kinetics400, Moments In Time, Something-Something V2 (SSV2), Jester and Diving48. We also experiment with the prevalent ResNet image classifiers in computer vision to further validate our idea. The results on both Kinetics400 and SSV2 are comparable to some of the best-performed CNN approaches based on spatio-temporal modeling. Our source codes and models are available at \url{https://github.com/IBM/sifar-pytorch}. | Quanfu Fan, ChunFu Chen, Rameswar Panda |  |
| 97 |  |  [Interacting Contour Stochastic Gradient Langevin Dynamics](https://openreview.net/forum?id=IK9ap6nxXr2) |  | 0 | We propose an interacting contour stochastic gradient Langevin dynamics (ICSGLD) sampler, an embarrassingly parallel multiple-chain contour stochastic gradient Langevin dynamics (CSGLD) sampler with efficient interactions. We show that ICSGLD can be theoretically more efficient than a single-chain CSGLD with an equivalent computational budget. We also present a novel random-field function, which facilitates the estimation of self-adapting parameters in big data and obtains free mode explorations. Empirically, we compare the proposed algorithm with popular benchmark methods for posterior sampling. The numerical results show a great potential of ICSGLD for large-scale uncertainty estimation tasks. | Wei Deng, Siqi Liang, Botao Hao, Guang Lin, Faming Liang |  |
| 98 |  |  [NeuPL: Neural Population Learning](https://openreview.net/forum?id=MIX3fJkl_1) |  | 0 | Learning in strategy games (e.g. StarCraft, poker) requires the discovery of diverse policies. This is often achieved by iteratively training new policies against existing ones, growing a policy population that is robust to exploit. This iterative approach suffers from two issues in real-world games: a) under finite budget, approximate best-response operators at each iteration needs truncating, resulting in under-trained good-responses populating the population; b) repeated learning of basic skills at each iteration is wasteful and becomes intractable in the presence of increasingly strong opponents. In this work, we propose Neural Population Learning (NeuPL) as a solution to both issues. NeuPL offers convergence guarantees to a population of best-responses under mild assumptions. By representing a population of policies within a single conditional model, NeuPL enables transfer learning across policies. Empirically, we show the generality, improved performance and efficiency of NeuPL across several test domains. Most interestingly, we show that novel strategies become more accessible, not less, as the neural population expands. | Siqi Liu, Luke Marris, Daniel Hennes, Josh Merel, Nicolas Heess, Thore Graepel |  |
| 99 |  |  [DeSKO: Stability-Assured Robust Control with a Deep Stochastic Koopman Operator](https://openreview.net/forum?id=hniLRD_XCA) |  | 0 | The Koopman operator theory linearly describes nonlinear dynamical systems in a high-dimensional functional space and it allows to apply linear control methods to highly nonlinear systems. However, the Koopman operator does not account for any uncertainty in dynamical systems, causing it to perform poorly in real-world applications. Therefore, we propose a deep stochastic Koopman operator (DeSKO) model in a robust learning control framework to guarantee stability of nonlinear stochastic systems. The DeSKO model captures a dynamical system's uncertainty by inferring a distribution of observables. We use the inferred distribution to design a robust, stabilizing closed-loop controller for a dynamical system. Modeling and control experiments on several advanced control benchmarks show that our framework is more robust and scalable than state-of-the-art deep Koopman operators and reinforcement learning methods. Tested control benchmarks include a soft robotic arm, a legged robot, and a biological gene regulatory network. We also demonstrate that this robust control method resists previously unseen uncertainties, such as external disturbances, with a magnitude of up to five times the maximum control input. Our approach opens up new possibilities in learning control for high-dimensional nonlinear systems while robustly managing internal or external uncertainty. | Minghao Han, Jacob EulerRolle, Robert K. Katzschmann |  |
| 100 |  |  [Neural Network Approximation based on Hausdorff distance of Tropical Zonotopes](https://openreview.net/forum?id=oiZJwC_fyS) |  | 0 |  | Panagiotis Misiakos, Georgios Smyrnis, George Retsinas, Petros Maragos |  |
| 101 |  |  [Learning Towards The Largest Margins](https://openreview.net/forum?id=hqkhcFHOeKD) |  | 0 | One of the main challenges for feature representation in deep learning-based classification is the design of appropriate loss functions that exhibit strong discriminative power. The classical softmax loss does not explicitly encourage discriminative learning of features. A popular direction of research is to incorporate margins in well-established losses in order to enforce extra intra-class compactness and inter-class separability, which, however, were developed through heuristic means, as opposed to rigorous mathematical principles. In this work, we attempt to address this limitation by formulating the principled optimization objective as learning towards the largest margins. Specifically, we firstly propose to employ the class margin as the measure of inter-class separability, and the sample margin as the measure of intra-class compactness. Accordingly, to encourage discriminative representation of features, the loss function should promote the largest possible margins for both classes and samples. Furthermore, we derive a generalized margin softmax loss to draw general conclusions for the existing margin-based losses. Not only does this principled framework offer new perspectives to understand and interpret existing margin-based losses, but it also provides new insights that can guide the design of new tools, including \textit{sample margin regularization} and \textit{largest margin softmax loss} for class balanced cases, and \textit{zero centroid regularization} for class imbalanced cases. Experimental results demonstrate the effectiveness of our strategy for multiple tasks including visual classification, imbalanced classification, person re-identification, and face verification. | Xiong Zhou, Xianming Liu, Deming Zhai, Junjun Jiang, Xin Gao, Xiangyang Ji |  |
| 102 |  |  [Patch-Fool: Are Vision Transformers Always Robust Against Adversarial Perturbations?](https://openreview.net/forum?id=28ib9tf6zhr) |  | 0 | Vision transformers (ViTs) have recently set off a new wave in neural architecture design thanks to their record-breaking performance in various vision tasks. In parallel, to fulfill the goal of deploying ViTs into real-world vision applications, their robustness against potential malicious attacks has gained increasing attention. In particular, recent works show that ViTs are more robust against adversarial attacks as compared with convolutional neural networks (CNNs), and conjecture that this is because ViTs focus more on capturing global interactions among different input/feature patches, leading to their improved robustness to local perturbations imposed by adversarial attacks. In this work, we ask an intriguing question: "Under what kinds of perturbations do ViTs become more vulnerable learners compared to CNNs?" Driven by this question, we first conduct a comprehensive experiment regarding the robustness of both ViTs and CNNs under various existing adversarial attacks to understand the underlying reason favoring their robustness. Based on the drawn insights, we then propose a dedicated attack framework, dubbed Patch-Fool, that fools the self-attention mechanism by attacking its basic component (i.e., a single patch) with a series of attention-aware optimization techniques. Interestingly, our Patch-Fool framework shows for the first time that ViTs are not necessarily more robust than CNNs against adversarial perturbations. In particular, we find that ViTs are more vulnerable learners compared with CNNs against our Patch-Fool attack which is consistent across extensive experiments, and the observations from Sparse/Mild Patch-Fool, two variants of Patch-Fool, indicate an intriguing insight that the perturbation density and strength on each patch seem to be the key factors that influence the robustness ranking between ViTs and CNNs. It can be expected that our Patch-Fool framework will shed light on both future architecture designs and training schemes for robustifying ViTs towards their real-world deployment. Our codes are available at https://github.com/RICE-EIC/Patch-Fool. | Yonggan Fu, Shunyao Zhang, Shang Wu, Cheng Wan, Yingyan Lin |  |
| 103 |  |  [AdaMatch: A Unified Approach to Semi-Supervised Learning and Domain Adaptation](https://openreview.net/forum?id=Q5uh1Nvv5dm) |  | 0 | We extend semi-supervised learning to the problem of domain adaptation to learn significantly higher-accuracy models that train on one data distribution and test on a different one. With the goal of generality, we introduce AdaMatch, a unified solution for unsupervised domain adaptation (UDA), semi-supervised learning (SSL), and semi-supervised domain adaptation (SSDA). In an extensive experimental study, we compare its behavior with respective state-of-the-art techniques from SSL, SSDA, and UDA and find that AdaMatch either matches or significantly exceeds the state-of-the-art in each case using the same hyper-parameters regardless of the dataset or task. For example, AdaMatch nearly doubles the accuracy compared to that of the prior state-of-the-art on the UDA task for DomainNet and even exceeds the accuracy of the prior state-of-the-art obtained with pre-training by 6.4% when AdaMatch is trained completely from scratch. Furthermore, by providing AdaMatch with just one labeled example per class from the target domain (i.e., the SSDA setting), we increase the target accuracy by an additional 6.1%, and with 5 labeled examples, by 13.6%. | David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, Alexey Kurakin |  |
| 104 |  |  [Complete Verification via Multi-Neuron Relaxation Guided Branch-and-Bound](https://openreview.net/forum?id=l_amHf1oaK) |  | 0 | State-of-the-art neural network verifiers are fundamentally based on one of two paradigms: either encoding the whole verification problem via tight multi-neuron convex relaxations or applying a Branch-and-Bound (BaB) procedure leveraging imprecise but fast bounding methods on a large number of easier subproblems. The former can capture complex multi-neuron dependencies but sacrifices completeness due to the inherent limitations of convex relaxations. The latter enables complete verification but becomes increasingly ineffective on larger and more challenging networks. In this work, we present a novel complete verifier which combines the strengths of both paradigms: it leverages multi-neuron relaxations to drastically reduce the number of subproblems generated during the BaB process and an efficient GPU-based dual optimizer to solve the remaining ones. An extensive evaluation demonstrates that our verifier achieves a new state-of-the-art on both established benchmarks as well as networks with significantly higher accuracy than previously considered. The latter result (up to 28% certification gains) indicates meaningful progress towards creating verifiers that can handle practically relevant networks. | Claudio Ferrari, Mark Niklas Müller, Nikola Jovanovic, Martin T. Vechev |  |
| 105 |  |  [Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality](https://openreview.net/forum?id=VFBjuF8HEp) |  | 0 | Diffusion models have emerged as an expressive family of generative models rivaling GANs in sample quality and autoregressive models in likelihood scores. Standard diffusion models typically require hundreds of forward passes through the model to generate a single high-fidelity sample. We introduce Differentiable Diffusion Sampler Search (DDSS): a method that optimizes fast samplers for any pre-trained diffusion model by differentiating through sample quality scores. We also present Generalized Gaussian Diffusion Models (GGDM), a family of flexible non-Markovian samplers for diffusion models. We show that optimizing the degrees of freedom of GGDM samplers by maximizing sample quality scores via gradient descent leads to improved sample quality. Our optimization procedure backpropagates through the sampling process using the reparametrization trick and gradient rematerialization. DDSS achieves strong results on unconditional image generation across various datasets (e.g., FID scores on LSUN church 128x128 of 11.6 with only 10 inference steps, and 4.82 with 20 steps, compared to 51.1 and 14.9 with strongest DDPM/DDIM baselines). Our method is compatible with any pre-trained diffusion model without fine-tuning or re-training required. | Daniel Watson, William Chan, Jonathan Ho, Mohammad Norouzi |  |
| 106 |  |  [Distribution Compression in Near-Linear Time](https://openreview.net/forum?id=lzupY5zjaU9) |  | 0 | In distribution compression, one aims to accurately summarize a probability distribution $\mathbb{P}$ using a small number of representative points. Near-optimal thinning procedures achieve this goal by sampling $n$ points from a Markov chain and identifying $\sqrt{n}$ points with $\widetilde{\mathcal{O}}(1/\sqrt{n})$ discrepancy to $\mathbb{P}$. Unfortunately, these algorithms suffer from quadratic or super-quadratic runtime in the sample size $n$. To address this deficiency, we introduce Compress++, a simple meta-procedure for speeding up any thinning algorithm while suffering at most a factor of $4$ in error. When combined with the quadratic-time kernel halving and kernel thinning algorithms of Dwivedi and Mackey (2021), Compress++ delivers $\sqrt{n}$ points with $\mathcal{O}(\sqrt{\log n/n})$ integration error and better-than-Monte-Carlo maximum mean discrepancy in $\mathcal{O}(n \log^3 n)$ time and $\mathcal{O}( \sqrt{n} \log^2 n )$ space. Moreover, Compress++ enjoys the same near-linear runtime given any quadratic-time input and reduces the runtime of super-quadratic algorithms by a square-root factor. In our benchmarks with high-dimensional Monte Carlo samples and Markov chains targeting challenging differential equation posteriors, Compress++ matches or nearly matches the accuracy of its input algorithm in orders of magnitude less time. | Abhishek Shetty, Raaz Dwivedi, Lester Mackey |  |
| 107 |  |  [Capturing Structural Locality in Non-parametric Language Models](https://openreview.net/forum?id=nnU3IUMJmN) |  | 0 | Structural locality is a ubiquitous feature of real-world datasets, wherein data points are organized into local hierarchies. Some examples include topical clusters in text or project hierarchies in source code repositories. In this paper, we explore utilizing this structural locality within non-parametric language models, which generate sequences that reference retrieved examples from an external source. We propose a simple yet effective approach for adding locality information into such models by adding learned parameters that improve the likelihood of retrieving examples from local neighborhoods. Experiments on two different domains, Java source code and Wikipedia text, demonstrate that locality features improve model efficacy over models without access to these features, with interesting differences. We also perform an analysis of how and where locality features contribute to improving performance and why the traditionally used contextual similarity metrics alone are not enough to grasp the locality structure. | Frank F. Xu, Junxian He, Graham Neubig, Vincent Josua Hellendoorn |  |
| 108 |  |  [Audio Lottery: Speech Recognition Made Ultra-Lightweight, Noise-Robust, and Transferable](https://openreview.net/forum?id=9Nk6AJkVYB) |  | 0 | Lightweight speech recognition models have seen explosive demands owing to a growing amount of speech-interactive features on mobile devices. Since designing such systems from scratch is non-trivial, practitioners typically choose to compress large (pre-trained) speech models. Recently, lottery ticket hypothesis reveals the existence of highly sparse subnetworks that can be trained in isolation without sacrificing the performance of the full models. In this paper, we investigate the tantalizing possibility of using lottery ticket hypothesis to discover lightweight speech recognition models, that are (1) robust to various noise existing in speech; (2) transferable to fit the open-world personalization; and 3) compatible with structured sparsity. We conducted extensive experiments on CNN-LSTM, RNN-Transducer, and Transformer models, and verified the existence of highly sparse winning tickets that can match the full model performance across those backbones. We obtained winning tickets that have less than 20% of full model weights on all backbones, while the most lightweight one only keeps 4.4% weights. Those winning tickets generalize to structured sparsity with no performance loss, and transfer exceptionally from large source datasets to various target datasets. Perhaps most surprisingly, when the training utterances have high background noises, the winning tickets even substantially outperform the full models, showing the extra bonus of noise robustness by inducing sparsity. Codes are available at https://github.com/VITA-Group/Audio-Lottery. | Shaojin Ding, Tianlong Chen, Zhangyang Wang |  |
| 109 |  |  [Learning to Map for Active Semantic Goal Navigation](https://openreview.net/forum?id=swrMQttr6wN) |  | 0 | We consider the problem of object goal navigation in unseen environments. Solving this problem requires learning of contextual semantic priors, a challenging endeavour given the spatial and semantic variability of indoor environments. Current methods learn to implicitly encode these priors through goal-oriented navigation policy functions operating on spatial representations that are limited to the agent's observable areas. In this work, we propose a novel framework that actively learns to generate semantic maps outside the field of view of the agent and leverages the uncertainty over the semantic classes in the unobserved areas to decide on long term goals. We demonstrate that through this spatial prediction strategy, we are able to learn semantic priors in scenes that can be leveraged in unknown environments. Additionally, we show how different objectives can be defined by balancing exploration with exploitation during searching for semantic targets. Our method is validated in the visually realistic environments of the Matterport3D dataset and show improved results on object goal navigation over competitive baselines. | Georgios Georgakis, Bernadette Bucher, Karl Schmeckpeper, Siddharth Singh, Kostas Daniilidis |  |
| 110 |  |  [Benchmarking the Spectrum of Agent Capabilities](https://openreview.net/forum?id=1W0z96MFEoH) |  | 0 | Evaluating the general abilities of intelligent agents requires complex simulation environments. Existing benchmarks typically evaluate only one narrow task per environment, requiring researchers to perform expensive training runs on many different environments. We introduce Crafter, an open world survival game with visual inputs that evaluates a wide range of general abilities within a single environment. Agents either learn from the provided reward signal or through intrinsic objectives and are evaluated by semantically meaningful achievements that can be unlocked during each episode, such as discovering resources and crafting tools. Consistently unlocking all achievements requires strong generalization, deep exploration, and long-term reasoning. We experimentally verify that Crafter is of appropriate difficulty to drive future research and provide baselines scores of reward agents and unsupervised agents. Furthermore, we observe sophisticated behaviors emerging from maximizing the reward signal, such as building tunnel systems, bridges, houses, and plantations. We hope that Crafter will accelerate research progress by quickly evaluating a wide spectrum of abilities. | Danijar Hafner |  |
| 111 |  |  [Mind the Gap: Domain Gap Control for Single Shot Domain Adaptation for Generative Adversarial Networks](https://openreview.net/forum?id=vqGi8Kp0wM) |  | 0 |  | Peihao Zhu, Rameen Abdal, John Femiani, Peter Wonka |  |
| 112 |  |  [On Evaluation Metrics for Graph Generative Models](https://openreview.net/forum?id=EnwCZixjSh) |  | 0 | In image generation, generative models can be evaluated naturally by visually inspecting model outputs. However, this is not always the case for graph generative models (GGMs), making their evaluation challenging. Currently, the standard process for evaluating GGMs suffers from three critical limitations: i) it does not produce a single score which makes model selection challenging, ii) in many cases it fails to consider underlying edge and node features, and iii) it is prohibitively slow to perform. In this work, we mitigate these issues by searching for \emph{scalar, domain-agnostic, and scalable metrics} for evaluating and ranking GGMs. To this end, we study existing GGM metrics and neural-network-based metrics emerging from generative models of images that use embeddings extracted from a task-specific network. Motivated by the power of Graph Neural Networks (GNNs) to extract meaningful graph representations \emph{without any training}, we introduce several metrics based on the features extracted by an untrained random GNN. We design experiments to thoroughly test and objectively score metrics on their ability to measure the diversity and fidelity of generated graphs, as well as their sample and computational efficiency. Depending on the quantity of samples, we recommend one of two metrics from our collection of random-GNN-based metrics. We show these two metrics to be more expressive than pre-existing and alternative random-GNN-based metrics using our objective scoring. While we focus on applying these metrics to GGM evaluation, in practice this enables the ability to easily compute the dissimilarity between any two sets of graphs \emph{regardless of domain}. Our code is released at: https://github.com/uoguelph-mlrg/GGM-metrics. | Rylee Thompson, Boris Knyazev, Elahe Ghalebi, Jungtaek Kim, Graham W. Taylor |  |
| 113 |  |  [Selective Ensembles for Consistent Predictions](https://openreview.net/forum?id=HfUyCRBeQc) |  | 0 | Recent work has shown that models trained to the same objective, and which achieve similar measures of accuracy on consistent test data, may nonetheless behave very differently on individual predictions. This inconsistency is undesirable in high-stakes contexts, such as medical diagnosis and finance. We show that this duplicitous behavior extends beyond predictions to feature attributions, which may likewise have negative implications for the intelligibility of a model, and one's ability to find recourse for subjects. We then introduce selective ensembles to mitigate such inconsistencies by applying hypothesis testing to the predictions of a set of models trained using randomly-selected starting conditions; importantly, selective ensembles can abstain in cases where a consistent outcome cannot be achieved up to a specified confidence level. We prove that that prediction disagreement between selective ensembles is bounded, and empirically demonstrate that selective ensembles achieve consistent predictions and feature attributions while maintaining low abstention rates. On several benchmark datasets, selective ensembles reach zero inconsistently predicted points, with abstention rates as low as 1.5%. | Emily Black, Klas Leino, Matt Fredrikson |  |
| 114 |  |  [Graph Condensation for Graph Neural Networks](https://openreview.net/forum?id=WLEx3Jo4QaB) |  | 0 | Given the prevalence of large-scale graphs in real-world applications, the storage and time for training neural models have raised increasing concerns. To alleviate the concerns, we propose and study the problem of graph condensation for graph neural networks (GNNs). Specifically, we aim to condense the large, original graph into a small, synthetic and highly-informative graph, such that GNNs trained on the small graph and large graph have comparable performance. We approach the condensation problem by imitating the GNN training trajectory on the original graph through the optimization of a gradient matching loss and design a strategy to condense node futures and structural information simultaneously. Extensive experiments have demonstrated the effectiveness of the proposed framework in condensing different graph datasets into informative smaller graphs. In particular, we are able to approximate the original test accuracy by 95.3\% on Reddit, 99.8\% on Flickr and 99.0\% on Citeseer, while reducing their graph size by more than 99.9\%, and the condensed graphs can be used to train various GNN architectures. | Wei Jin, Lingxiao Zhao, Shichang Zhang, Yozen Liu, Jiliang Tang, Neil Shah |  |
| 115 |  |  [DIVA: Dataset Derivative of a Learning Task](https://openreview.net/forum?id=bVvMOtLMiw) |  | 0 | We present a method to compute the derivative of a learning task with respect to a dataset. A learning task is a function from a training set to the validation error, which can be represented by a trained deep neural network (DNN). The \`\`dataset derivative'' is a linear operator, computed around the trained model, that informs how perturbations of the weight of each training sample affect the validation error, usually computed on a separate validation dataset. Our method, DIVA (Differentiable Validation) hinges on a closed-form differentiable expression of the leave-one-out cross-validation error around a pre-trained DNN. Such expression constitutes the dataset derivative. DIVA could be used for dataset auto-curation, for example removing samples with faulty annotations, augmenting a dataset with additional relevant samples, or rebalancing. More generally, DIVA can be used to optimize the dataset, along with the parameters of the model, as part of the training process without the need for a separate validation dataset, unlike bi-level optimization methods customary in AutoML. To illustrate the flexibility of DIVA, we report experiments on sample auto-curation tasks such as outlier rejection, dataset extension, and automatic aggregation of multi-modal data. | Yonatan Dukler, Alessandro Achille, Giovanni Paolini, Avinash Ravichandran, Marzia Polito, Stefano Soatto |  |
| 116 |  |  [Towards General Function Approximation in Zero-Sum Markov Games](https://openreview.net/forum?id=sA4qIu3zv6v) |  | 0 | This paper considers two-player zero-sum finite-horizon Markov games with simultaneous moves. The study focuses on the challenging settings where the value function or the model is parameterized by general function classes. Provably efficient algorithms for both decoupled and coordinated settings are developed. In the decoupled setting where the agent controls a single player and plays against an arbitrary opponent, we propose a new model-free algorithm. The sample complexity is governed by the Minimax Eluder dimension—a new dimension of the function class in Markov games. As a special case, this method improves the state-of-the-art algorithm by a $\sqrt{d}$ factor in the regret when the reward function and transition kernel are parameterized with d-dimensional linear features. In the coordinated setting where both players are controlled by the agent, we propose a model-based algorithm and a model-free algorithm. In the model-based algorithm, we prove that sample complexity can be bounded by a generalization of Witness rank to Markov games. The model-free algorithm enjoys a $\sqrt{K}$-regret upper bound where $K$ is the number of episodes. Our algorithms are based on new techniques of alternate optimism | Baihe Huang, Jason D. Lee, Zhaoran Wang, Zhuoran Yang |  |
| 117 |  |  [Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis--Hastings](https://openreview.net/forum?id=6PvWo1kEvlT) |  | 0 | While recent work has shown that scores from models trained by the ubiquitous masked language modeling (MLM) objective effectively discriminate probable from improbable sequences, it is still an open question if these MLMs specify a principled probability distribution over the space of possible sequences. In this paper, we interpret MLMs as energy-based sequence models and propose two energy parametrizations derivable from the trained MLMs. In order to draw samples correctly from these models, we develop a tractable sampling scheme based on the Metropolis--Hastings Monte Carlo algorithm. In our approach, samples are proposed from the same masked conditionals used for training the masked language models, and they are accepted or rejected based on their energy values according to the target distribution. We validate the effectiveness of the proposed parametrizations by exploring the quality of samples drawn from these energy-based models for both open-ended unconditional generation and a conditional generation task of machine translation. We theoretically and empirically justify our sampling algorithm by showing that the masked conditionals on their own do not yield a Markov chain whose stationary distribution is that of our target distribution, and our approach generates higher quality samples than other recently proposed undirected generation approaches (Wang et al., 2019, Ghazvininejad et al., 2019). | Kartik Goyal, Chris Dyer, Taylor BergKirkpatrick |  |
| 118 |  |  [ClimateGAN: Raising Climate Change Awareness by Generating Images of Floods](https://openreview.net/forum?id=EZNOb_uNpJk) |  | 0 | Climate change is a major threat to humanity and the actions required to prevent its catastrophic consequences include changes in both policy-making and individual behaviour. However, taking action requires understanding its seemingly abstract and distant consequences. Projecting the potential impacts of extreme climate events such as flooding in familiar places can help make the impacts of climate change more concrete and encourage action. As part of a larger initiative to build a website (https://thisclimatedoesnotexist.com) that projects extreme climate events onto user-chosen photos, we present our solution to simulate photo-realistic floods on authentic images. To address this complex task in the absence of suitable data, we propose ClimateGAN, a model that leverages both simulated and real data through unsupervised domain adaptation and conditional image generation. In this paper, we describe the details of our framework, thoroughly evaluate the main components of our architecture and demonstrate that our model is capable of robustly generating photo-realistic flooding on street images. | Victor Schmidt, Alexandra Luccioni, Mélisande Teng, Tianyu Zhang, Alexia Reynaud, Sunand Raghupathi, Gautier Cosne, Adrien Juraver, Vahe Vardanyan, Alex HernándezGarcía, Yoshua Bengio |  |
| 119 |  |  [A Comparison of Hamming Errors of Representative Variable Selection Methods](https://openreview.net/forum?id=nhN-fqxmNGx) |  | 0 | Lasso is a celebrated method for variable selection in linear models, but it faces challenges when the covariates are moderately or strongly correlated. This motivates alternative approaches such as using a non-convex penalty, adding a ridge regularization, or conducting a post-Lasso thresholding. In this paper, we compare Lasso with 5 other methods: Elastic net, SCAD, forward selection, thresholded Lasso, and forward backward selection. We measure their performances theoretically by the expected Hamming error, assuming that the regression coefficients are ${\it iid}$ drawn from a two-point mixture and that the Gram matrix is block-wise diagonal. By deriving the rates of convergence of Hamming errors and the phase diagrams, we obtain useful conclusions about the pros and cons of different methods. | Tracy Ke, Longlin Wang |  |
| 120 |  |  [A Program to Build E(N)-Equivariant Steerable CNNs](https://openreview.net/forum?id=WE4qe9xlnQw) |  | 0 | Equivariance is becoming an increasingly popular design choice to build data efficient neural networks by exploiting prior knowledge about the symmetries of the problem at hand. Euclidean steerable CNNs are one of the most common classes of equivariant networks. While the constraints these architectures need to satisfy are understood, existing approaches are tailored to specific (classes of) groups. No generally applicable method that is practical for implementation has been described so far. In this work, we generalize the Wigner-Eckart theorem proposed in Lang & Weiler (2020), which characterizes general $G$-steerable kernel spaces for compact groups $G$ over their homogeneous spaces, to arbitrary $G$-spaces. This enables us to directly parameterize filters in terms of a band-limited basis on the whole space rather than on $G$'s orbits, but also to easily implement steerable CNNs equivariant to a large number of groups. To demonstrate its generality, we instantiate our method on a variety of isometry groups acting on the Euclidean space $\mathbb{R}^3$. Our framework allows us to build $E(3)$ and $SE(3)$-steerable CNNs like previous works, but also CNNs with arbitrary $G\leq O(3)$-steerable kernels. For example, we build 3D CNNs equivariant to the symmetries of platonic solids or choose $G=SO(2)$ when working with 3D data having only azimuthal symmetries. We compare these models on 3D shapes and molecular datasets, observing improved performance by matching the model's symmetries to the ones of the data. | Gabriele Cesa, Leon Lang, Maurice Weiler |  |
| 121 |  |  [Minimax Optimization with Smooth Algorithmic Adversaries](https://openreview.net/forum?id=UdxJ2fJx7N0) |  | 0 | This paper considers minimax optimization $\min_x \max_y f(x, y)$ in the challenging setting where $f$ can be both nonconvex in $x$ and nonconcave in $y$. Though such optimization problems arise in many machine learning paradigms including training generative adversarial networks (GANs) and adversarially robust models, from a theoretical point of view, two fundamental issues remain: (i) the absence of simple and efficiently computable optimality notions, and (ii) cyclic or diverging behavior of existing algorithms. This paper proposes a new theoretical framework for nonconvex-nonconcave minimax optimization that addresses both of the above issues. The starting point of this paper is the observation that, under a computational budget, the max-player can not fully maximize $f(x,\cdot)$ since nonconcave maximization is NP-hard in general. So, we propose a new framework, and a corresponding algorithm, for the min-player to play against \emph{smooth algorithms} deployed by the adversary (i.e., the max-player) instead of against full maximization. Our algorithm is guaranteed to make monotonic progress (thus having no limit cycles or diverging behavior), and to find an appropriate \`\`stationary point'' in a polynomial number of iterations. Our framework covers practically relevant settings where the smooth algorithms deployed by the adversary are multi-step stochastic gradient ascent, and its accelerated version. We further present experimental results that confirm our theoretical findings and demonstrate the effectiveness of the proposed approach in practice on simple, conceptual settings. | Tanner Fiez, Chi Jin, Praneeth Netrapalli, Lillian J. Ratliff |  |
| 122 |  |  [On Distributed Adaptive Optimization with Gradient Compression](https://openreview.net/forum?id=CI-xXX9dg9l) |  | 0 |  | Xiaoyun Li, Belhal Karimi, Ping Li |  |
| 123 |  |  [Leveraging unlabeled data to predict out-of-distribution performance](https://openreview.net/forum?id=o_HsiMPYh_x) |  | 0 | Real-world machine learning deployments are characterized by mismatches between the source (training) and target (test) distributions that may cause performance drops. In this work, we investigate methods for predicting the target domain accuracy using only labeled source data and unlabeled target data. We propose Average Thresholded Confidence (ATC), a practical method that learns a \emph{threshold} on the model's confidence, predicting accuracy as the fraction of unlabeled examples for which model confidence exceeds that threshold. ATC outperforms previous methods across several model architectures, types of distribution shifts (e.g., due to synthetic corruptions, dataset reproduction, or novel subpopulations), and datasets (\textsc{Wilds}-FMoW, ImageNet, \breeds, CIFAR, and MNIST). In our experiments, ATC estimates target performance $2\text{--}4\times$ more accurately than prior methods. We also explore the theoretical foundations of the problem, proving that, in general, identifying the accuracy is just as hard as identifying the optimal predictor and thus, the efficacy of any method rests upon (perhaps unstated) assumptions on the nature of the shift. Finally, analyzing our method on some toy distributions, we provide insights concerning when it works. | Saurabh Garg, Sivaraman Balakrishnan, Zachary Chase Lipton, Behnam Neyshabur, Hanie Sedghi |  |
| 124 |  |  [VC dimension of partially quantized neural networks in the overparametrized regime](https://openreview.net/forum?id=7udZAsEzd60) |  | 0 | Vapnik-Chervonenkis (VC) theory has so far been unable to explain the small generalization error of overparametrized neural networks. Indeed, existing applications of VC theory to large networks obtain upper bounds on VC dimension that are proportional to the number of weights, and for a large class of networks, these upper bound are known to be tight. In this work, we focus on a class of partially quantized networks that we refer to as hyperplane arrangement neural networks (HANNs). Using a sample compression analysis, we show that HANNs can have VC dimension significantly smaller than the number of weights, while being highly expressive. In particular, empirical risk minimization over HANNs in the overparametrized regime achieves the minimax rate for classification with Lipschitz posterior class probability. We further demonstrate the expressivity of HANNs empirically. On a panel of 121 UCI datasets, overparametrized HANNs are able to match the performance of state-of-the-art full-precision models. | Yutong Wang, Clayton Scott |  |
| 125 |  |  [Optimal Representations for Covariate Shift](https://openreview.net/forum?id=Rf58LPCwJj0) |  | 0 | Machine learning systems often experience a distribution shift between training and testing. In this paper, we introduce a simple variational objective whose optima are exactly the set of all representations on which risk minimizers are guaranteed to be robust to any distribution shift that preserves the Bayes predictor, e.g., covariate shifts. Our objective has two components. First, a representation must remain discriminative for the task, i.e., some predictor must be able to simultaneously minimize the source and target risk. Second, the representation's marginal support needs to be the same across source and target. We make this practical by designing self-supervised objectives that only use unlabelled data and augmentations to train robust representations. Our objectives give insights into the robustness of CLIP, and further improve CLIP's representations to achieve SOTA results on DomainBed. | Yangjun Ruan, Yann Dubois, Chris J. Maddison |  |
| 126 |  |  [Fortuitous Forgetting in Connectionist Networks](https://openreview.net/forum?id=ei3SY1_zYsE) |  | 0 | Forgetting is often seen as an unwanted characteristic in both human and machine learning. However, we propose that forgetting can in fact be favorable to learning. We introduce forget-and-relearn as a powerful paradigm for shaping the learning trajectories of artificial neural networks. In this process, the forgetting step selectively removes undesirable information from the model, and the relearning step reinforces features that are consistently useful under different conditions. The forget-and-relearn framework unifies many existing iterative training algorithms in the image classification and language emergence literature, and allows us to understand the success of these algorithms in terms of the disproportionate forgetting of undesirable information. We leverage this understanding to improve upon existing algorithms by designing more targeted forgetting operations. Insights from our analysis provide a coherent view on the dynamics of iterative training in neural networks and offer a clear path towards performance improvements. | Hattie Zhou, Ankit Vani, Hugo Larochelle, Aaron C. Courville |  |
| 127 |  |  [EigenGame Unloaded: When playing games is better than optimizing](https://openreview.net/forum?id=So6YAqnqgMj) |  | 0 | We build on the recently proposed EigenGame that views eigendecomposition as a competitive game. EigenGame's updates are biased if computed using minibatches of data, which hinders convergence and more sophisticated parallelism in the stochastic setting. In this work, we propose an unbiased stochastic update that is asymptotically equivalent to EigenGame, enjoys greater parallelism allowing computation on datasets of larger sample sizes, and outperforms EigenGame in experiments. We present applications to finding the principal components of massive datasets and performing spectral clustering of graphs. We analyze and discuss our proposed update in the context of EigenGame and the shift in perspective from optimization to games. | Ian Gemp, Brian McWilliams, Claire Vernade, Thore Graepel |  |
| 128 |  |  [Contextualized Scene Imagination for Generative Commonsense Reasoning](https://openreview.net/forum?id=Oh1r2wApbPv) |  | 0 | Humans use natural language to compose common concepts from their environment into plausible, day-to-day scene descriptions. However, such generative commonsense reasoning (GCSR) skills are lacking in state-of-the-art text generation methods. Descriptive sentences about arbitrary concepts generated by neural text generation models (e.g., pre-trained text-to-text Transformers) are often grammatically fluent but may not correspond to human common sense, largely due to their lack of mechanisms to capture concept relations, to identify implicit concepts, and to perform generalizable reasoning about unseen concept compositions. In this paper, we propose an Imagine-and-Verbalize (I\&V) method, which learns to imagine a relational scene knowledge graph (SKG) with relations between the input concepts, and leverage the SKG as a constraint when generating a plausible scene description. We collect and harmonize a set of knowledge resources from different domains and modalities, providing a rich auxiliary supervision signal for I\&V. The experiments demonstrate the effectiveness of I\&V in improving language models on both concept-to-sentence and concept-to-story generation tasks, while enabling the model to learn well from fewer task examples and generate SKGs that make common sense to human annotators. | Peifeng Wang, Jonathan Zamora, Junfeng Liu, Filip Ilievski, Muhao Chen, Xiang Ren |  |
| 129 |  |  [Scene Transformer: A unified architecture for predicting future trajectories of multiple agents](https://openreview.net/forum?id=Wm3EA5OlHsG) |  | 0 | Predicting the motion of multiple agents is necessary for planning in dynamic environments. This task is challenging for autonomous driving since agents (e.g., vehicles and pedestrians) and their associated behaviors may be diverse and influence one another. Most prior work have focused on predicting independent futures for each agent based on all past motion, and planning against these independent predictions. However, planning against independent predictions can make it challenging to represent the future interaction possibilities between different agents, leading to sub-optimal planning. In this work, we formulate a model for predicting the behavior of all agents jointly, producing consistent futures that account for interactions between agents. Inspired by recent language modeling approaches, we use a masking strategy as the query to our model, enabling one to invoke a single model to predict agent behavior in many ways, such as potentially conditioned on the goal or full future trajectory of the autonomous vehicle or the behavior of other agents in the environment. Our model architecture employs attention to combine features across road elements, agent interactions, and time steps. We evaluate our approach on autonomous driving datasets for both marginal and joint motion prediction, and achieve state of the art performance across two popular datasets. Through combining a scene-centric approach, agent permutation equivariant model, and a sequence masking strategy, we show that our model can unify a variety of motion prediction tasks from joint motion predictions to conditioned prediction. | Jiquan Ngiam, Vijay Vasudevan, Benjamin Caine, Zhengdong Zhang, HaoTien Lewis Chiang, Jeffrey Ling, Rebecca Roelofs, Alex Bewley, Chenxi Liu, Ashish Venugopal, David J. Weiss, Ben Sapp, Zhifeng Chen |  |
| 130 |  |  [DISSECT: Disentangled Simultaneous Explanations via Concept Traversals](https://openreview.net/forum?id=qY79G8jGsep) |  | 0 | Explaining deep learning model inferences is a promising venue for scientific understanding, improving safety, uncovering hidden biases, evaluating fairness, and beyond, as argued by many scholars. One of the principal benefits of counterfactual explanations is allowing users to explore "what-if" scenarios through what does not and cannot exist in the data, a quality that many other forms of explanation such as heatmaps and influence functions are inherently incapable of doing. However, most previous work on generative explainability cannot disentangle important concepts effectively, produces unrealistic examples, or fails to retain relevant information. We propose a novel approach, DISSECT, that jointly trains a generator, a discriminator, and a concept disentangler to overcome such challenges using little supervision. DISSECT generates Concept Traversals (CTs), defined as a sequence of generated examples with increasing degrees of concepts that influence a classifier's decision. By training a generative model from a classifier's signal, DISSECT offers a way to discover a classifier's inherent "notion" of distinct concepts automatically rather than rely on user-predefined concepts. We show that DISSECT produces CTs that (1) disentangle several concepts, (2) are influential to a classifier's decision and are coupled to its reasoning due to joint training (3), are realistic, (4) preserve relevant information, and (5) are stable across similar inputs. We validate DISSECT on several challenging synthetic and realistic datasets where previous methods fall short of satisfying desirable criteria for interpretability and show that it performs consistently well. Finally, we present experiments showing applications of DISSECT for detecting potential biases of a classifier and identifying spurious artifacts that impact predictions. | Asma Ghandeharioun, Been Kim, ChunLiang Li, Brendan Jou, Brian Eoff, Rosalind W. Picard |  |
| 131 |  |  [Heteroscedastic Temporal Variational Autoencoder For Irregularly Sampled Time Series](https://openreview.net/forum?id=Az7opqbQE-3) |  | 0 | Irregularly sampled time series commonly occur in several domains where they present a significant challenge to standard deep learning models. In this paper, we propose a new deep learning framework for probabilistic interpolation of irregularly sampled time series that we call the Heteroscedastic Temporal Variational Autoencoder (HeTVAE). HeTVAE includes a novel input layer to encode information about input observation sparsity, a temporal VAE architecture to propagate uncertainty due to input sparsity, and a heteroscedastic output layer to enable variable uncertainty in the output interpolations. Our results show that the proposed architecture is better able to reflect variable uncertainty through time due to sparse and irregular sampling than a range of baseline and traditional models, as well as recently proposed deep latent variable models that use homoscedastic output layers. | Satya Narayan Shukla, Benjamin M. Marlin |  |
| 132 |  |  [A Neural Tangent Kernel Perspective of Infinite Tree Ensembles](https://openreview.net/forum?id=vUH85MOXO7h) |  | 0 | In practical situations, the tree ensemble is one of the most popular models along with neural networks. A soft tree is a variant of a decision tree. Instead of using a greedy method for searching splitting rules, the soft tree is trained using a gradient method in which the entire splitting operation is formulated in a differentiable form. Although ensembles of such soft trees have been used increasingly in recent years, little theoretical work has been done to understand their behavior. By considering an ensemble of infinite soft trees, this paper introduces and studies the Tree Neural Tangent Kernel (TNTK), which provides new insights into the behavior of the infinite ensemble of soft trees. Using the TNTK, we theoretically identify several non-trivial properties, such as global convergence of the training, the equivalence of the oblivious tree structure, and the degeneracy of the TNTK induced by the deepening of the trees. | Ryuichi Kanoh, Mahito Sugiyama |  |
| 133 |  |  [AlphaZero-based Proof Cost Network to Aid Game Solving](https://openreview.net/forum?id=nKWjE4QF1hB) |  | 0 | The AlphaZero algorithm learns and plays games without hand-crafted expert knowledge. However, since its objective is to play well, we hypothesize that a better objective can be defined for the related but separate task of solving games. This paper proposes a novel approach to solving problems by modifying the training target of the AlphaZero algorithm, such that it prioritizes solving the game quickly, rather than winning. We train a Proof Cost Network (PCN), where proof cost is a heuristic that estimates the amount of work required to solve problems. This matches the general concept of the so-called proof number from proof number search, which has been shown to be well-suited for game solving. We propose two specific training targets. The first finds the shortest path to a solution, while the second estimates the proof cost. We conduct experiments on solving 15x15 Gomoku and 9x9 Killall-Go problems with both MCTS-based and FDFPN solvers. Comparisons between using AlphaZero networks and PCN as heuristics show that PCN can solve more problems. | TiRong Wu, ChungChin Shih, TingHan Wei, MengYu Tsai, WeiYuan Hsu, IChen Wu |  |
| 134 |  |  [Bayesian Framework for Gradient Leakage](https://openreview.net/forum?id=f2lrIbGx3x7) |  | 0 | Federated learning is an established method for training machine learning models without sharing training data. However, recent work has shown that it cannot guarantee data privacy as shared gradients can still leak sensitive information. To formalize the problem of gradient leakage, we propose a theoretical framework that enables, for the first time, analysis of the Bayes optimal adversary phrased as an optimization problem. We demonstrate that existing leakage attacks can be seen as approximations of this optimal adversary with different assumptions on the probability distributions of the input data and gradients. Our experiments confirm the effectiveness of the Bayes optimal adversary when it has knowledge of the underlying distribution. Further, our experimental evaluation shows that several existing heuristic defenses are not effective against stronger attacks, especially early in the training process. Thus, our findings indicate that the construction of more effective defenses and their evaluation remains an open problem. | Mislav Balunovic, Dimitar Iliev Dimitrov, Robin Staab, Martin T. Vechev |  |
| 135 |  |  [Universalizing Weak Supervision](https://openreview.net/forum?id=YpPiNigTzMT) |  | 0 | Weak supervision (WS) frameworks are a popular way to bypass hand-labeling large datasets for training data-hungry models. These approaches synthesize multiple noisy but cheaply-acquired estimates of labels into a set of high-quality pseudo-labels for downstream training. However, the synthesis technique is specific to a particular kind of label, such as binary labels or sequences, and each new label type requires manually designing a new synthesis algorithm. Instead, we propose a universal technique that enables weak supervision over any label type while still offering desirable properties, including practical flexibility, computational efficiency, and theoretical guarantees. We apply this technique to important problems previously not tackled by WS frameworks including learning to rank, regression, and learning in hyperbolic space. Theoretically, our synthesis approach produces a consistent estimators for learning some challenging but important generalizations of the exponential family model. Experimentally, we validate our framework and show improvement over baselines in diverse settings including real-world learning-to-rank and regression problems along with learning on hyperbolic manifolds. | Changho Shin, Winfred Li, Harit Vishwakarma, Nicholas Carl Roberts, Frederic Sala |  |
| 136 |  |  [Maximum n-times Coverage for Vaccine Design](https://openreview.net/forum?id=ULfq0qR25dY) |  | 0 | We introduce the maximum $n$-times coverage problem that selects $k$ overlays to maximize the summed coverage of weighted elements, where each element must be covered at least $n$ times. We also define the min-cost $n$-times coverage problem where the objective is to select the minimum set of overlays such that the sum of the weights of elements that are covered at least $n$ times is at least $\tau$. Maximum $n$-times coverage is a generalization of the multi-set multi-cover problem, is NP-complete, and is not submodular. We introduce two new practical solutions for $n$-times coverage based on integer linear programming and sequential greedy optimization. We show that maximum $n$-times coverage is a natural way to frame peptide vaccine design, and find that it produces a pan-strain COVID-19 vaccine design that is superior to 29 other published designs in predicted population coverage and the expected number of peptides displayed by each individual's HLA molecules. | Ge Liu, Alexander Dimitrakakis, Brandon Carter, David K. Gifford |  |
| 137 |  |  [KL Guided Domain Adaptation](https://openreview.net/forum?id=0JzqUlIVVDd) |  | 0 | Domain adaptation is an important problem and often needed for real-world applications. In this problem, instead of i.i.d. training and testing datapoints, we assume that the source (training) data and the target (testing) data have different distributions. With that setting, the empirical risk minimization training procedure often does not perform well, since it does not account for the change in the distribution. A common approach in the domain adaptation literature is to learn a representation of the input that has the same (marginal) distribution over the source and the target domain. However, these approaches often require additional networks and/or optimizing an adversarial (minimax) objective, which can be very expensive or unstable in practice. To improve upon these marginal alignment techniques, in this paper, we first derive a generalization bound for the target loss based on the training loss and the reverse Kullback-Leibler (KL) divergence between the source and the target representation distributions. Based on this bound, we derive an algorithm that minimizes the KL term to obtain a better generalization to the target domain. We show that with a probabilistic representation network, the KL term can be estimated efficiently via minibatch samples without any additional network or a minimax objective. This leads to a theoretically sound alignment method which is also very efficient and stable in practice. Experimental results also suggest that our method outperforms other representation-alignment approaches. | A. Tuan Nguyen, Toan Tran, Yarin Gal, Philip H. S. Torr, Atilim Gunes Baydin |  |
| 138 |  |  [From Stars to Subgraphs: Uplifting Any GNN with Local Structure Awareness](https://openreview.net/forum?id=Mspk_WYKoEH) |  | 0 | Message Passing Neural Networks (MPNNs) are a common type of Graph Neural Network (GNN), in which each node’s representation is computed recursively by aggregating representations (“messages”) from its immediate neighbors akin to a star-shaped pattern. MPNNs are appealing for being efficient and scalable, however their expressiveness is upper-bounded by the 1st-order Weisfeiler-Lehman isomorphism test (1-WL). In response, prior works propose highly expressive models at the cost of scalability and sometimes generalization performance. Our work stands between these two regimes: we introduce a general framework to uplift any MPNN to be more expressive, with limited scalability overhead and greatly improved practical performance. We achieve this by extending local aggregation in MPNNs from star patterns to general subgraph patterns (e.g., k-egonets): in our framework, each node representation is computed as the encoding of a surrounding induced subgraph rather than encoding of immediate neighbors only (i.e. a star). We choose the subgraph encoder to be a GNN (mainly MPNNs, considering scalability) to design a general framework that serves as a wrapper to uplift any GNN. We call our proposed method GNN-AK (GNN As Kernel), as the framework resembles a convolutional neural network by replacing the kernel with GNNs. Theoretically, we show that our framework is strictly more powerful than 1&2-WL, and is not less powerful than 3-WL. We also design subgraph sampling strategies which greatly reduce memory footprint and improve speed while maintaining performance. Our method sets new state-of-the-art performance by large margins for several well-known graph ML tasks; specifically, 0.08 MAE on ZINC, 74.79% and 86.887% accuracy on CIFAR10 and PATTERN respectively. | Lingxiao Zhao, Wei Jin, Leman Akoglu, Neil Shah |  |
| 139 |  |  [Network Insensitivity to Parameter Noise via Parameter Attack During Training](https://openreview.net/forum?id=-8sBpe7rDiV) |  | 0 | Neuromorphic neural network processors, in the form of compute-in-memory crossbar arrays of memristors, or in the form of subthreshold analog and mixed-signal ASICs, promise enormous advantages in compute density and energy efficiency for NN-based ML tasks. However, these technologies are prone to computational non-idealities, due to process variation and intrinsic device physics. This degrades the task performance of networks deployed to the processor, by introducing parameter noise into the deployed model. While it is possible to calibrate each device, or train networks individually for each processor, these approaches are expensive and impractical for commercial deployment. Alternative methods are therefore needed to train networks that are inherently robust against parameter variation, as a consequence of network architecture and parameters. We present a new network training algorithm that attacks network parameters during training, and promotes robust performance during inference in the face of random parameter variation. Our approach introduces a loss regularization term that penalizes the susceptibility of a network to weight perturbation. We compare against previous approaches for producing parameter insensitivity such as dropout, weight smoothing and introducing parameter noise during training. We show that our approach produces models that are more robust to random mismatch-induced parameter variation as well as to targeted parameter variation. Our approach finds minima in flatter locations in the weight-loss landscape compared with other approaches, highlighting that the networks found by our technique are less sensitive to parameter perturbation. Our work provides an approach to deploy neural network architectures to inference devices that suffer from computational non-idealities, with minimal loss of performance. This method will enable deployment at scale to novel energy-efficient computational substrates, promoting cheaper and more prevalent edge inference. | Julian Büchel, Fynn Firouz Faber, Dylan Richard Muir |  |
| 140 |  |  [Gradient Importance Learning for Incomplete Observations](https://openreview.net/forum?id=fXHl76nO2AZ) |  | 0 | Though recent works have developed methods that can generate estimates (or imputations) of the missing entries in a dataset to facilitate downstream analysis, most depend on assumptions that may not align with real-world applications and could suffer from poor performance in subsequent tasks such as classification. This is particularly true if the data have large missingness rates or a small sample size. More importantly, the imputation error could be propagated into the prediction step that follows, which may constrain the capabilities of the prediction model. In this work, we introduce the gradient importance learning (GIL) method to train multilayer perceptrons (MLPs) and long short-term memories (LSTMs) to directly perform inference from inputs containing missing values without imputation. Specifically, we employ reinforcement learning (RL) to adjust the gradients used to train these models via back-propagation. This allows the model to exploit the underlying information behind missingness patterns. We test the approach on real-world time-series (i.e., MIMIC-III), tabular data obtained from an eye clinic, and a standard dataset (i.e., MNIST), where our imputation-free predictions outperform the traditional two-step imputation-based predictions using state-of-the-art imputation methods. | Qitong Gao, Dong Wang, Joshua David Amason, Siyang Yuan, Chenyang Tao, Ricardo Henao, Majda Hadziahmetovic, Lawrence Carin, Miroslav Pajic |  |
| 141 |  |  [Do Users Benefit From Interpretable Vision? A User Study, Baseline, And Dataset](https://openreview.net/forum?id=v6s3HVjPerv) |  | 0 | A variety of methods exist to explain image classification models. However, whether they provide any benefit to users over simply comparing various inputs and the model’s respective predictions remains unclear. We conducted a user study (N=240) to test how such a baseline explanation technique performs against concept-based and counterfactual explanations. To this end, we contribute a synthetic dataset generator capable of biasing individual attributes and quantifying their relevance to the model. In a study, we assess if participants can identify the relevant set of attributes compared to the ground-truth. Our results show that the baseline outperformed concept-based explanations. Counterfactual explanations from an invertible neural network performed similarly as the baseline. Still, they allowed users to identify some attributes more accurately. Our results highlight the importance of measuring how well users can reason about biases of a model, rather than solely relying on technical evaluations or proxy tasks. We open-source our study and dataset so it can serve as a blue-print for future studies. | Leon Sixt, Martin Schuessler, OanaIuliana Popescu, Philipp Weiß, Tim Landgraf |  |
| 142 |  |  [Understanding the Variance Collapse of SVGD in High Dimensions](https://openreview.net/forum?id=Qycd9j5Qp9J) |  | 0 | Stein variational gradient descent (SVGD) is a deterministic inference algorithm that evolves a set of particles to fit a target distribution. Despite its computational efficiency, SVGD often underestimates the variance of the target distribution in high dimensions. In this work we attempt to explain the variance collapse in SVGD. On the qualitative side, we compare the SVGD update with gradient descent on the maximum mean discrepancy (MMD) objective; we observe that the variance collapse phenomenon relates to the bias from deterministic updates present in the "driving force" of SVGD, and empirically verify that removal of such bias leads to more accurate variance estimation. On the quantitative side, we demonstrate that the variance collapse of SVGD can be accurately predicted in the proportional asymptotic limit, i.e., when the number of particles $n$ and dimensions $d$ diverge at the same rate. In particular, for learning high-dimensional isotropic Gaussians, we derive the exact equilibrium variance for both SVGD and MMD-descent under certain near-orthogonality assumption on the converged particles, and confirm that SVGD suffers from the "curse of dimensionality". | Jimmy Ba, Murat A. Erdogdu, Marzyeh Ghassemi, Shengyang Sun, Taiji Suzuki, Denny Wu, Tianzong Zhang |  |
| 143 |  |  [Generalisation in Lifelong Reinforcement Learning through Logical Composition](https://openreview.net/forum?id=ZOcX-eybqoL) |  | 0 | We leverage logical composition in reinforcement learning to create a framework that enables an agent to autonomously determine whether a new task can be immediately solved using its existing abilities, or whether a task-specific skill should be learned. In the latter case, the proposed algorithm also enables the agent to learn the new task faster by generating an estimate of the optimal policy. Importantly, we provide two main theoretical results: we bound the performance of the transferred policy on a new task, and we give bounds on the necessary and sufficient number of tasks that need to be learned throughout an agent's lifetime to generalise over a distribution. We verify our approach in a series of experiments, where we perform transfer learning both after learning a set of base tasks, and after learning an arbitrary set of tasks. We also demonstrate that, as a side effect of our transfer learning approach, an agent can produce an interpretable Boolean expression of its understanding of the current task. Finally, we demonstrate our approach in the full lifelong setting where an agent receives tasks from an unknown distribution. Starting from scratch, an agent is able to quickly generalise over the task distribution after learning only a few tasks, which are sub-logarithmic in the size of the task space. | Geraud Nangue Tasse, Steven James, Benjamin Rosman |  |
| 144 |  |  [PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions](https://openreview.net/forum?id=gSdSJoenupI) |  | 0 |  | Zhaoqi Leng, Mingxing Tan, Chenxi Liu, Ekin Dogus Cubuk, Jay Shi, Shuyang Cheng, Dragomir Anguelov |  |
| 145 |  |  [Improving Non-Autoregressive Translation Models Without Distillation](https://openreview.net/forum?id=I2Hw58KHp8O) |  | 0 | Transformer-based autoregressive (AR) machine translation models have achieved significant performance improvements, nearing human-level accuracy on some languages. The AR framework translates one token at a time which can be time consuming, especially for long sequences. To accelerate inference, recent work has been exploring non-autoregressive (NAR) approaches that translate blocks of tokens in parallel. Despite significant progress, leading NAR models still lag behind their AR counterparts, and only become competitive when trained with distillation. In this paper we investigate possible reasons behind this performance gap, namely, the indistinguishability of tokens, and mismatch between training and inference. We then propose the Conditional Masked Language Model with Correction (CMLMC) that addresses these problems. Empirically, we show that CMLMC achieves state-of-the-art NAR performance when trained on raw data without distillation and approaches AR performance on multiple datasets. Full code for this work will be released at the time of publication. | Xiao Shi Huang, Felipe Pérez, Maksims Volkovs |  |
| 146 |  |  [A Theory of Tournament Representations](https://openreview.net/forum?id=zzk231Ms1Ih) |  | 0 | Real-world tournaments are almost always intransitive. Recent works have noted that parametric models which assume $d$ dimensional node representations can effectively model intransitive tournaments. However, nothing is known about the structure of the class of tournaments that arise out of any fixed $d$ dimensional representations. In this work, we develop a novel theory for understanding parametric tournament representations. Our first contribution is to structurally characterize the class of tournaments that arise out of $d$ dimensional representations. We do this by showing that these tournament classes have forbidden configurations that must necessarily be a union of flip classes, a novel way to partition the set of all tournaments. We further characterize rank $2$ tournaments completely by showing that the associated forbidden flip class contains just $2$ tournaments. Specifically, we show that the rank $2$ tournaments are equivalent to locally transitive tournaments. This insight allows us to show that the minimum feedback arc set problem on this tournament class can be solved using the standard Quicksort procedure. We also exhibit specific forbidden configurations for rank $4$ tournaments. For a general rank $d$ tournament class, we show that the flip class associated with a coned-doubly regular tournament of size $\mathcal{O}(\sqrt{d})$ must be a forbidden configuration. To answer a dual question, using a celebrated result of Froster, we show a lower bound of $\Theta(\sqrt{n})$ on the minimum dimension needed to represent all tournaments on $n$ nodes. For any given tournament, we show a novel upper bound on the smallest representation dimension that depends on the least size of the number of unique nodes in any feedback arc set of the flip class associated with a tournament. We show how our results also shed light on the upper bound of sign-rank of matrices. | Arun Rajkumar, Vishnu Veerathu, Abdul Bakey Mir |  |
| 147 |  |  [Convergent and Efficient Deep Q Learning Algorithm](https://openreview.net/forum?id=OJm3HZuj4r7) |  | 0 | Despite the empirical success of the deep Q network (DQN) reinforcement learning algorithm and its variants, DQN is still not well understood and it does not guarantee convergence. In this work, we show that DQN can indeed diverge and cease to operate in realistic settings. Although there exist gradient-based convergent methods, we show that they actually have inherent problems in learning dynamics which cause them to fail even for simple tasks. To overcome these problems, we propose a convergent DQN algorithm (C-DQN) that is guaranteed to converge and can work with large discount factors (0.9998). It learns robustly in difficult settings and can learn several difficult games in the Atari 2600 benchmark that DQN fails to solve. | Zhikang T. Wang, Masahito Ueda |  |
| 148 |  |  [Trigger Hunting with a Topological Prior for Trojan Detection](https://openreview.net/forum?id=TXsjU8BaibT) |  | 0 | Despite their success and popularity, deep neural networks (DNNs) are vulnerable when facing backdoor attacks. This impedes their wider adoption, especially in mission critical applications. This paper tackles the problem of Trojan detection, namely, identifying Trojaned models – models trained with poisoned data. One popular approach is reverse engineering, i.e., recovering the triggers on a clean image by manipulating the model’s prediction. One major challenge of reverse engineering approach is the enormous search space of triggers. To this end, we propose innovative priors such as diversity and topological simplicity to not only increase the chances of finding the appropriate triggers but also improve the quality of the found triggers. Moreover, by encouraging a diverse set of trigger candidates, our method can perform effectively in cases with unknown target labels. We demonstrate that these priors can significantly improve the quality of the recovered triggers, resulting in substantially improved Trojan detection accuracy as validated on both synthetic and publicly available TrojAI benchmarks. | Xiaoling Hu, Xiao Lin, Michael Cogswell, Yi Yao, Susmit Jha, Chao Chen |  |
| 149 |  |  [Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep RL](https://openreview.net/forum?id=JM2kFbJvvI) |  | 0 | Evaluating the worst-case performance of a reinforcement learning (RL) agent under the strongest/optimal adversarial perturbations on state observations (within some constraints) is crucial for understanding the robustness of RL agents. However, finding the optimal adversary is challenging, in terms of both whether we can find the optimal attack and how efficiently we can find it. Existing works on adversarial RL either use heuristics-based methods that may not find the strongest adversary, or directly train an RL-based adversary by treating the agent as a part of the environment, which can find the optimal adversary but may become intractable in a large state space. This paper introduces a novel attacking method to find the optimal attacks through collaboration between a designed function named "actor" and an RL-based learner named "director'". The actor crafts state perturbations for a given policy perturbation direction, and the director learns to propose the best policy perturbation directions. Our proposed algorithm, PA-AD, is theoretically optimal and significantly more efficient than prior RL-based works in environments with large state spaces. Empirical results show that our proposed PA-AD universally outperforms state-of-the-art attacking methods in various Atari and MuJoCo environments. By applying PA-AD to adversarial training, we achieve state-of-the-art empirical robustness in multiple tasks under strong adversaries. | Yanchao Sun, Ruijie Zheng, Yongyuan Liang, Furong Huang |  |
| 150 |  |  [Chunked Autoregressive GAN for Conditional Waveform Synthesis](https://openreview.net/forum?id=v3aeIsY_vVX) |  | 0 | Conditional waveform synthesis models learn a distribution of audio waveforms given conditioning such as text, mel-spectrograms, or MIDI. These systems employ deep generative models that model the waveform via either sequential (autoregressive) or parallel (non-autoregressive) sampling. Generative adversarial networks (GANs) have become a common choice for non-autoregressive waveform synthesis. However, state-of-the-art GAN-based models produce artifacts when performing mel-spectrogram inversion. In this paper, we demonstrate that these artifacts correspond with an inability for the generator to learn accurate pitch and periodicity. We show that simple pitch and periodicity conditioning is insufficient for reducing this error relative to using autoregression. We discuss the inductive bias that autoregression provides for learning the relationship between instantaneous frequency and phase, and show that this inductive bias holds even when autoregressively sampling large chunks of the waveform during each forward pass. Relative to prior state-of-the-art GAN-based models, our proposed model, Chunked Autoregressive GAN (CARGAN) reduces pitch error by 40-60%, reduces training time by 58%, maintains a fast inference speed suitable for real-time or interactive applications, and maintains or improves subjective quality. | Max Morrison, Rithesh Kumar, Kundan Kumar, Prem Seetharaman, Aaron C. Courville, Yoshua Bengio |  |
| 151 |  |  [COPA: Certifying Robust Policies for Offline Reinforcement Learning against Poisoning Attacks](https://openreview.net/forum?id=psh0oeMSBiF) |  | 0 | As reinforcement learning (RL) has achieved near human-level performance in a variety of tasks, its robustness has raised great attention. While a vast body of research has explored test-time (evasion) attacks in RL and corresponding defenses, its robustness against training-time (poisoning) attacks remains largely unanswered. In this work, we focus on certifying the robustness of ofﬂine RL in the presence of poisoning attacks, where a subset of training trajectories could be arbitrarily manipulated. We propose the ﬁrst certiﬁcation framework, COPA, to certify the number of poisoning trajectories that can be tolerated regarding different certiﬁcation criteria. Given the complex structure of RL, we propose two certiﬁcation criteria: per-state action stability and cumulative reward bound. To further improve the certiﬁcation, we propose new partition and aggregation protocols to train robust policies. We further prove that some of the proposed certiﬁcation methods are theoretically tight and some are NP-Complete problems. We leverage COPA to certify three RL environments trained with different algorithms and conclude: (1) The proposed robust aggregation protocols such as temporal aggregation can signiﬁcantly improve the certiﬁcations; (2) Our certiﬁcations for both per-state action stability and cumulative reward bound are efﬁcient and tight; (3) The certiﬁcation for different training algorithms and environments are different, implying their intrinsic robustness properties. All experimental results are available at https://copa-leaderboard.github.io. | Fan Wu, Linyi Li, Huan Zhang, Bhavya Kailkhura, Krishnaram Kenthapadi, Ding Zhao, Bo Li |  |
| 152 |  |  [ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning](https://openreview.net/forum?id=Vzh1BFUCiIX) |  | 0 |  | Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni, Jai Prakash Gupta, Kai Hui, Sebastian Ruder, Donald Metzler |  |
| 153 |  |  [Provable Adaptation across Multiway Domains via Representation Learning](https://openreview.net/forum?id=gRCCdgpVZf) |  | 0 | This paper studies zero-shot domain adaptation where each domain is indexed on a multi-dimensional array, and we only have data from a small subset of domains. Our goal is to produce predictors that perform well on \emph{unseen} domains. We propose a model which consists of a domain-invariant latent representation layer and a domain-specific linear prediction layer with a low-rank tensor structure. Theoretically, we present explicit sample complexity bounds to characterize the prediction error on unseen domains in terms of the number of domains with training data and the number of data per domain. To our knowledge, this is the first finite-sample guarantee for zero-shot domain adaptation. In addition, we provide experiments on two-way MNIST and four-way fiber sensing datasets to demonstrate the effectiveness of our proposed model. | Zhili Feng, Shaobo Han, Simon Shaolei Du |  |
| 154 |  |  [Efficient Token Mixing for Transformers via Adaptive Fourier Neural Operators](https://openreview.net/forum?id=EXHG-A3jlM) |  | 0 | Vision transformers have delivered tremendous success in representation learning. This is primarily due to effective token mixing through self attention. However, this scales quadratically with the number of pixels, which becomes infeasible for high-resolution inputs. To cope with this challenge, we propose Adaptive Fourier Neural Operator (AFNO) as an efficient token mixer that learns to mix in the Fourier domain. AFNO is based on a principled foundation of operator learning which allows us to frame token mixing as a continuous global convolution without any dependence on the input resolution. This principle was previously used to design FNO, which solves global convolution efficiently in the Fourier domain and has shown promise in learning challenging PDEs. To handle challenges in visual representation learning such as discontinuities in images and high resolution inputs, we propose principled architectural modifications to FNO which results in memory and computational efficiency. This includes imposing a block-diagonal structure on the channel mixing weights, adaptively sharing weights across tokens, and sparsifying the frequency modes via soft-thresholding and shrinkage. The resulting model is highly parallel with a quasi-linear complexity and has linear memory in the sequence size. AFNO outperforms self-attention mechanisms for few-shot segmentation in terms of both efficiency and accuracy. For Cityscapes segmentation with the Segformer-B3 backbone, AFNO can handle a sequence size of 65k and outperforms other efficient self-attention mechanisms. | John Guibas, Morteza Mardani, Zongyi Li, Andrew Tao, Anima Anandkumar, Bryan Catanzaro |  |
| 155 |  |  [Sample Selection with Uncertainty of Losses for Learning with Noisy Labels](https://openreview.net/forum?id=xENf4QUL4LW) |  | 0 | In learning with noisy labels, the sample selection approach is very popular, which regards small-loss data as correctly labeled data during training. However, losses are generated on-the-ﬂy based on the model being trained with noisy labels, and thus large-loss data are likely but not certain to be incorrect. There are actually two possibilities of a large-loss data point: (a) it is mislabeled, and then its loss decreases slower than other data, since deep neural networks learn patterns ﬁrst; (b) it belongs to an underrepresented group of data and has not been selected yet. In this paper, we incorporate the uncertainty of losses by adopting interval estimation instead of point estimation of losses, where lower bounds of the conﬁdence intervals of losses derived from distribution-free concentration inequalities, but not losses themselves, are used for sample selection. In this way, we also give large-loss but less selected data a try; then, we can better distinguish between the cases (a) and (b) by seeing if the losses effectively decrease with the uncertainty after the try. As a result, we can better explore underrepresented data that are correctly labeled but seem to be mislabeled at ﬁrst glance. Experiments demonstrate that the proposed method is superior to baselines and robust to a broad range of label noise types. | Xiaobo Xia, Tongliang Liu, Bo Han, Mingming Gong, Jun Yu, Gang Niu, Masashi Sugiyama |  |
| 156 |  |  [Data-Driven Offline Optimization for Architecting Hardware Accelerators](https://openreview.net/forum?id=GsH-K1VIyy) |  | 0 | To attain higher efficiency, the industry has gradually reformed towards application-specific hardware accelerators. While such a paradigm shift is already starting to show promising results, designers need to spend considerable manual effort and perform large number of time-consuming simulations to find accelerators that can accelerate multiple target applications while obeying design constraints. Moreover, such a simulation-driven approach must be re-run from scratch every time the set of target applications or design constraints change. An alternative paradigm is to use a data-driven, offline approach that utilizes logged simulation data, to architect hardware accelerators, without needing any form of simulations. Such an approach not only alleviates the need to run time-consuming simulation, but also enables data reuse and applies even when set of target applications changes. In this paper, we develop such a data-driven offline optimization method for designing hardware accelerators, dubbed PRIME, that enjoys all of these properties. Our approach learns a conservative, robust estimate of the desired cost function, utilizes infeasible points and optimizes the design against this estimate without any additional simulator queries during optimization. PRIME architects accelerators---tailored towards both single- and multi-applications---improving performance upon stat-of-the-art simulation-driven methods by about 1.54x and 1.20x, while considerably reducing the required total simulation time by 93% and 99%, respectively. In addition, PRIME also architects effective accelerators for unseen applications in a zero-shot setting, outperforming simulation-based methods by 1.26x. | Aviral Kumar, Amir Yazdanbakhsh, Milad Hashemi, Kevin Swersky, Sergey Levine |  |
| 157 |  |  [Multi-Agent MDP Homomorphic Networks](https://openreview.net/forum?id=H7HDG--DJF0) |  | 0 | This paper introduces Multi-Agent MDP Homomorphic Networks, a class of networks that allows distributed execution using only local information, yet is able to share experience between global symmetries in the joint state-action space of cooperative multi-agent systems. In cooperative multi-agent systems, complex symmetries arise between different configurations of the agents and their local observations. For example, consider a group of agents navigating: rotating the state globally results in a permutation of the optimal joint policy. Existing work on symmetries in single agent reinforcement learning can only be generalized to the fully centralized setting, because such approaches rely on the global symmetry in the full state-action spaces, and these can result in correspondences across agents. To encode such symmetries while still allowing distributed execution we propose a factorization that decomposes global symmetries into local transformations. Our proposed factorization allows for distributing the computation that enforces global symmetries over local agents and local interactions. We introduce a multi-agent equivariant policy network based on this factorization. We show empirically on symmetric multi-agent problems that globally symmetric distributable policies improve data efficiency compared to non-equivariant baselines. | Elise van der Pol, Herke van Hoof, Frans A. Oliehoek, Max Welling |  |
| 158 |  |  [Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields](https://openreview.net/forum?id=yhCp5RcZD7) |  | 0 | We present implicit displacement fields, a novel representation for detailed 3D geometry. Inspired by a classic surface deformation technique, displacement mapping, our method represents a complex surface as a smooth base surface plus a displacement along the base's normal directions, resulting in a frequency-based shape decomposition, where the high-frequency signal is constrained geometrically by the low-frequency signal. Importantly, this disentanglement is unsupervised thanks to a tailored architectural design that has an innate frequency hierarchy by construction. We explore implicit displacement field surface reconstruction and detail transfer and demonstrate superior representational power, training stability, and generalizability. | Yifan Wang, Lukas Rahmann, Olga SorkineHornung |  |
| 159 |  |  [Modeling Label Space Interactions in Multi-label Classification using Box Embeddings](https://openreview.net/forum?id=tyTH9kOxcvh) |  | 0 | Multi-label classification is a challenging structured prediction task in which a set of output class labels are predicted for each input. Real-world datasets often have natural or latent taxonomic relationships between labels, making it desirable for models to employ label representations capable of capturing such taxonomies. Most existing multi-label classification methods do not do so, resulting in label predictions that are inconsistent with the taxonomic constraints, thus failing to accurately represent the fundamentals of problem setting. In this work, we introduce the multi-label box model (MBM), a multi-label classification method that combines the encoding power of neural networks with the inductive bias and probabilistic semantics of box embeddings (Vilnis, et al 2018). Box embeddings can be understood as trainable Venn-diagrams based on hyper-rectangles. Representing labels by boxes rather than vectors, MBM is able to capture taxonomic relations among labels. Furthermore, since box embeddings allow these relations to be learned by stochastic gradient descent from data, and to be read as calibrated conditional probabilities, our model is endowed with a high degree of interpretability. This interpretability also facilitates the injection of partial information about label-label relationships into model training, to further improve its consistency. We provide theoretical grounding for our method and show experimentally the model's ability to learn the true latent taxonomic structure from data. Through extensive empirical evaluations on both small and large-scale multi-label classification datasets, we show that BBM can significantly improve taxonomic consistency while preserving or surpassing the state-of-the-art predictive performance. | Dhruvesh Patel, Pavitra Dangati, JayYoon Lee, Michael Boratko, Andrew McCallum |  |
| 160 |  |  [It Takes Two to Tango: Mixup for Deep Metric Learning](https://openreview.net/forum?id=ZKy2X3dgPA) |  | 0 | Metric learning involves learning a discriminative representation such that embeddings of similar classes are encouraged to be close, while embeddings of dissimilar classes are pushed far apart. State-of-the-art methods focus mostly on sophisticated loss functions or mining strategies. On the one hand, metric learning losses consider two or more examples at a time. On the other hand, modern data augmentation methods for classification consider two or more examples at a time. The combination of the two ideas is under-studied. In this work, we aim to bridge this gap and improve representations using mixup, which is a powerful data augmentation approach interpolating two or more examples and corresponding target labels at a time. This task is challenging because, unlike classification, the loss functions used in metric learning are not additive over examples, so the idea of interpolating target labels is not straightforward. To the best of our knowledge, we are the first to investigate mixing both examples and target labels for deep metric learning. We develop a generalized formulation that encompasses existing metric learning loss functions and modify it to accommodate for mixup, introducing Metric Mix, or Metrix. We also introduce a new metric---utilization---to demonstrate that by mixing examples during training, we are exploring areas of the embedding space beyond the training classes, thereby improving representations. To validate the effect of improved representations, we show that mixing inputs, intermediate representations or embeddings along with target labels significantly outperforms state-of-the-art metric learning methods on four benchmark deep metric learning datasets. | Shashanka Venkataramanan, Bill Psomas, Ewa Kijak, Laurent Amsaleg, Konstantinos Karantzalos, Yannis Avrithis |  |
| 161 |  |  [Data Efficient Language-Supervised Zero-Shot Recognition with Optimal Transport Distillation](https://openreview.net/forum?id=G89-1yZLFHk) |  | 0 | Traditional computer vision models are trained to predict a fixed set of predefined categories. Recently, natural language has been shown to be a broader and richer source of supervision that provides finer descriptions to visual concepts than supervised "gold" labels. Previous works, such as CLIP, use InfoNCE loss to train a model to predict the pairing between images and text captions. CLIP, however, is data hungry and requires more than 400M image-text pairs for training. The inefficiency can be \textit{partially} attributed to the fact that the image-text pairs are noisy. To address this, we propose OTTER (Optimal TransporT distillation for Efficient zero-shot Recognition), which uses online entropic optimal transport to find a soft image-text match as labels for contrastive learning. Based on pretrained image and text encoders, models trained with OTTER achieve strong performance with only 3M image text pairs. Compared with InfoNCE loss, label smoothing, and knowledge distillation, OTTER consistently outperforms these baselines in zero-shot evaluation on Google Open Images (19,958 classes) and multi-labeled ImageNet 10K (10032 classes) from Tencent ML-Images. Over 42 evaluations on 7 different dataset/architecture settings x 6 metrics, OTTER outperforms (32) or ties (2) all baselines in 34 of them. Our source code is open sourced at https://github.com/facebookresearch/OTTER. | Bichen Wu, Ruizhe Cheng, Peizhao Zhang, Tianren Gao, Joseph E. Gonzalez, Peter Vajda |  |
| 162 |  |  [A Statistical Framework for Efficient Out of Distribution Detection in Deep Neural Networks](https://openreview.net/forum?id=Oy9WeuZD51) |  | 0 | Background. Commonly, Deep Neural Networks (DNNs) generalize well on samples drawn from a distribution similar to that of the training set. However, DNNs' predictions are brittle and unreliable when the test samples are drawn from a dissimilar distribution. This is a major concern for deployment in real-world applications, where such behavior may come at a considerable cost, such as industrial production lines, autonomous vehicles, or healthcare applications. Contributions. We frame Out Of Distribution (OOD) detection in DNNs as a statistical hypothesis testing problem. Tests generated within our proposed framework combine evidence from the entire network. Unlike previous OOD detection heuristics, this framework returns a $p$-value for each test sample. It is guaranteed to maintain the Type I Error (T1E - incorrectly predicting OOD for an actual in-distribution sample) for test data. Moreover, this allows to combine several detectors while maintaining the T1E. Building on this framework, we suggest a novel OOD procedure based on low-order statistics. Our method achieves comparable or better results than state-of-the-art methods on well-accepted OOD benchmarks, without retraining the network parameters or assuming prior knowledge on the test distribution --- and at a fraction of the computational cost. | Matan Haroush, Tzviel Frostig, Ruth Heller, Daniel Soudry |  |
| 163 |  |  [FedBABU: Toward Enhanced Representation for Federated Image Classification](https://openreview.net/forum?id=HuaYQfggn5u) |  | 0 | Federated learning has evolved to improve a single global model under data heterogeneity (as a curse) or to develop multiple personalized models using data heterogeneity (as a blessing). However, little research has considered both directions simultaneously. In this paper, we first investigate the relationship between them by analyzing Federated Averaging at the client level and determine that a better federated global model performance does not constantly improve personalization. To elucidate the cause of this personalization performance degradation problem, we decompose the entire network into the body (extractor), which is related to universality, and the head (classifier), which is related to personalization. We then point out that this problem stems from training the head. Based on this observation, we propose a novel federated learning algorithm, coined FedBABU, which only updates the body of the model during federated training (i.e., the head is randomly initialized and never updated), and the head is fine-tuned for personalization during the evaluation process. Extensive experiments show consistent performance improvements and an efficient personalization of FedBABU. The code is available at https://github.com/jhoon-oh/FedBABU. | Jaehoon Oh, Sangmook Kim, SeYoung Yun |  |
| 164 |  |  [Should I Run Offline Reinforcement Learning or Behavioral Cloning?](https://openreview.net/forum?id=AP1MKT37rJ) |  | 0 | Offline reinforcement learning (RL) algorithms can acquire effective policies by utilizing only previously collected experience, without any online interaction. While it is widely understood that offline RL is able to extract good policies even from highly suboptimal data, in practice offline RL is often used with data that resembles demonstrations. In this case, one can also use behavioral cloning (BC) algorithms, which mimic a subset of the dataset via supervised learning. It seems natural to ask: When should we prefer offline RL over BC? In this paper, our goal is to characterize environments and dataset compositions where offline RL leads to better performance than BC. In particular, we characterize the properties of environments that allow offline RL methods to perform better than BC methods even when only provided with expert data. Additionally, we show that policies trained on suboptimal data that is sufficiently noisy can attain better performance than even BC algorithms with expert data, especially on long-horizon problems. We validate our theoretical results via extensive experiments on both diagnostic and high-dimensional domains including robot manipulation, maze navigation and Atari games, when learning from a variety of data sources. We observe that modern offline RL methods trained on suboptimal, noisy data in sparse reward domains outperform cloning the expert data in several practical problems. | Aviral Kumar, Joey Hong, Anikait Singh, Sergey Levine |  |
| 165 |  |  [Learning State Representations via Retracing in Reinforcement Learning](https://openreview.net/forum?id=CLpxpXqqBV) |  | 0 |  | Changmin Yu, Dong Li, Jianye Hao, Jun Wang, Neil Burgess |  |
| 166 |  |  [Open-World Semi-Supervised Learning](https://openreview.net/forum?id=O-r8LOR-CCA) |  | 0 | A fundamental limitation of applying semi-supervised learning in real-world settings is the assumption that unlabeled test data contains only classes previously encountered in the labeled training data. However, this assumption rarely holds for data in-the-wild, where instances belonging to novel classes may appear at testing time. Here, we introduce a novel open-world semi-supervised learning setting that formalizes the notion that novel classes may appear in the unlabeled test data. In this novel setting, the goal is to solve the class distribution mismatch problem between labeled and unlabeled data, where at the test time every input instance either needs to be classified into one of the existing classes or a new unseen class needs to be initialized and the instance assigned to it. To tackle this challenging problem, we propose ORCA, an end-to-end approach that assigns instances to previously seen classes or forms novel classes by grouping similar instances without assuming any prior knowledge. The key idea in ORCA is to utilize uncertainty adaptive margin to circumvent the bias towards seen classes caused by learning seen classes faster than the novel classes. In this way, ORCA gradually increases the discriminability of the model during the training and reduces the gap between intra-class variance of seen with respect to novel classes. Extensive experiments on image classification datasets and a single-cell dataset demonstrate that ORCA consistently outperforms alternative baselines, achieving 25% improvement on seen and 96% improvement on novel classes of the ImageNet dataset. | Kaidi Cao, Maria Brbic, Jure Leskovec |  |
| 167 |  |  [Evading Adversarial Example Detection Defenses with Orthogonal Projected Gradient Descent](https://openreview.net/forum?id=af1eUDdUVz) |  | 0 | Evading adversarial example detection defenses requires finding adversarial examples that must simultaneously (a) be misclassified by the model and (b) be detected as non-adversarial. We find that existing attacks that attempt to satisfy multiple simultaneous constraints often over-optimize against one constraint at the cost of satisfying another. We introduce Selective Projected Gradient Descent and Orthogonal Projected Gradient Descent, improved attack techniques to generate adversarial examples that avoid this problem by orthogonalizing the gradients when running standard gradient-based attacks. We use our technique to evade four state-of-the-art detection defenses, reducing their accuracy to 0% while maintaining a 0% detection rate. | Oliver Bryniarski, Nabeel Hingun, Pedro Pachuca, Vincent Wang, Nicholas Carlini |  |
| 168 |  |  [Reducing Excessive Margin to Achieve a Better Accuracy vs. Robustness Trade-off](https://openreview.net/forum?id=Azh9QBQ4tR7) |  | 0 | While adversarial training has become the de facto approach for training robust classifiers, it leads to a drop in accuracy. This has led to prior works postulating that accuracy is inherently at odds with robustness. Yet, the phenomenon remains inexplicable. In this paper, we closely examine the changes induced in the decision boundary of a deep network during adversarial training. We find that adversarial training leads to unwarranted increase in the margin along certain adversarial directions, thereby hurting accuracy. Motivated by this observation, we present a novel algorithm, called Helper-based Adversarial Training (HAT), to reduce this effect by incorporating additional wrongly labelled examples during training. Our proposed method provides a notable improvement in accuracy without compromising robustness. It achieves a better trade-off between accuracy and robustness in comparison to existing defenses. Code is available at https://github.com/imrahulr/hat. | Rahul Rade, SeyedMohsen MoosaviDezfooli |  |
| 169 |  |  [Expressivity of Emergent Languages is a Trade-off between Contextual Complexity and Unpredictability](https://openreview.net/forum?id=WxuE_JWxjkW) |  | 0 | Researchers are using deep learning models to explore the emergence of language in various language games, where agents interact and develop an emergent language to solve tasks. We focus on the factors that determine the expressivity of emergent languages, which reflects the amount of information about input spaces those languages are capable of encoding. We measure the expressivity of emergent languages based on the generalisation performance across different games, and demonstrate that the expressivity of emergent languages is a trade-off between the complexity and unpredictability of the context those languages emerged from. Another contribution of this work is the discovery of message type collapse, i.e. the number of unique messages is lower than that of inputs. We also show that using the contrastive loss proposed by Chen et al. (2020) can alleviate this problem. | Shangmin Guo, Yi Ren, Kory Wallace Mathewson, Simon Kirby, Stefano V. Albrecht, Kenny Smith |  |
| 170 |  |  [Fast AdvProp](https://openreview.net/forum?id=hcoswsDHNAW) |  | 0 | Adversarial Propagation (AdvProp) is an effective way to improve recognition models, leveraging adversarial examples. Nonetheless, AdvProp suffers from the extremely slow training speed, mainly because: a) extra forward and backward passes are required for generating adversarial examples; b) both original samples and their adversarial counterparts are used for training (i.e., 2X data). In this paper, we introduce Fast AdvProp, which aggressively revamps AdvProp's costly training components, rendering the method nearly as cheap as the vanilla training. Specifically, our modifications in Fast AdvProp are guided by the hypothesis that disentangled learning with adversarial examples is the key for performance improvements, while other training recipes (e.g., paired clean and adversarial training samples, multi-step adversarial attackers) could be largely simplified. Our empirical results show that, compared to the vanilla training baseline, Fast AdvProp is able to further model performance on a spectrum of visual benchmarks, without incurring extra training cost. Additionally, our ablations find Fast AdvProp scales better if larger models are used, is compatible with existing data augmentation methods (i.e., Mixup and CutMix), and can be easily adapted to other recognition tasks like object detection. The code is available here: https://github.com/meijieru/fast_advprop. | Jieru Mei, Yucheng Han, Yutong Bai, Yixiao Zhang, Yingwei Li, Xianhang Li, Alan L. Yuille, Cihang Xie |  |
| 171 |  |  [Triangle and Four Cycle Counting with Predictions in Graph Streams](https://openreview.net/forum?id=8in_5gN9I0) |  | 0 | We propose data-driven one-pass streaming algorithms for estimating the number of triangles and four cycles, two fundamental problems in graph analytics that are widely studied in the graph data stream literature. Recently, Hsu et al. (2019) and Jiang et al. (2020) applied machine learning techniques in other data stream problems, using a trained oracle that can predict certain properties of the stream elements to improve on prior “classical” algorithms that did not use oracles. In this paper, we explore the power of a “heavy edge” oracle in multiple graph edge streaming models. In the adjacency list model, we present a one-pass triangle counting algorithm improving upon the previous space upper bounds without such an oracle. In the arbitrary order model, we present algorithms for both triangle and four cycle estimation with fewer passes and the same space complexity as in previous algorithms, and we show several of these bounds are optimal. We analyze our algorithms under several noise models, showing that the algorithms perform well even when the oracle errs. Our methodology expands upon prior work on “classical” streaming algorithms, as previous multi-pass and random order streaming algorithms can be seen as special cases of our algorithms, where the first pass or random order was used to implement the heavy edge oracle. Lastly, our experiments demonstrate advantages of the proposed method compared to state-of-the-art streaming algorithms. | Justin Y. Chen, Talya Eden, Piotr Indyk, Honghao Lin, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal, Tal Wagner, David P. Woodruff, Michael Zhang |  |
| 172 |  |  [Is Fairness Only Metric Deep? Evaluating and Addressing Subgroup Gaps in Deep Metric Learning](https://openreview.net/forum?id=js62_xuLDDv) |  | 0 | Deep metric learning (DML) enables learning with less supervision through its emphasis on the similarity structure of representations. There has been much work on improving generalization of DML in settings like zero-shot retrieval, but little is known about its implications for fairness. In this paper, we are the first to evaluate state-of-the-art DML methods trained on imbalanced data, and to show the negative impact these representations have on minority subgroup performance when used for downstream tasks. In this work, we first define fairness in DML through an analysis of three properties of the representation space -- inter-class alignment, intra-class alignment, and uniformity -- and propose \textit{\textbf{finDML}}, the \textit{\textbf{f}}airness \textit{\textbf{i}}n \textit{\textbf{n}}on-balanced \textit{\textbf{DML}} benchmark to characterize representation fairness. Utilizing \textit{finDML}, we find bias in DML representations to propagate to common downstream classification tasks. Surprisingly, this bias is propagated even when training data in the downstream task is re-balanced. To address this problem, we present Partial Attribute De-correlation (\textit{\textbf{\pad}}) to disentangle feature representations from sensitive attributes and reduce performance gaps between subgroups in both embedding space and downstream metrics. | Natalie Dullerud, Karsten Roth, Kimia Hamidieh, Nicolas Papernot, Marzyeh Ghassemi |  |
| 173 |  |  [NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs](https://openreview.net/forum?id=xMJWUKJnFSw) |  | 0 | Conventional representation learning algorithms for knowledge graphs (KG) map each entity to a unique embedding vector. Such a shallow lookup results in a linear growth of memory consumption for storing the embedding matrix and incurs high computational costs of working with real-world KGs. Drawing parallels with subword tokenization commonly used in NLP, we explore the landscape of more parameter-efficient node embedding strategies with possibly sublinear memory requirements. To this end, we propose NodePiece, an anchor-based approach to learn a fixed-size entity vocabulary. In NodePiece, a vocabulary of subword/sub-entity units is constructed from anchor nodes in a graph with known relation types. Given such a fixed-size vocabulary, it is possible to bootstrap an encoding and embedding for any entity, including those unseen during training. Experiments show that NodePiece performs competitively in node classification, link prediction, and relation prediction tasks retaining less than 10% of explicit nodes in a graph as anchors and often having 10x fewer parameters. To this end, we show that a NodePiece-enabled model outperforms existing shallow models on a large OGB WikiKG 2 graph having 70x fewer parameters. | Mikhail Galkin, Etienne G. Denis, Jiapeng Wu, William L. Hamilton |  |
| 174 |  |  [Pix2seq: A Language Modeling Framework for Object Detection](https://openreview.net/forum?id=e42KbIw6Wb) |  | 0 |  | Ting Chen, Saurabh Saxena, Lala Li, David J. Fleet, Geoffrey E. Hinton |  |
| 175 |  |  [Particle Stochastic Dual Coordinate Ascent: Exponential convergent algorithm for mean field neural network optimization](https://openreview.net/forum?id=PQQp7AJwz3) |  | 0 | We introduce Particle-SDCA, a gradient-based optimization algorithm for two-layer neural networks in the mean field regime that achieves exponential convergence rate in regularized empirical risk minimization. The proposed algorithm can be regarded as an infinite dimensional extension of Stochastic Dual Coordinate Ascent (SDCA) in the probability space: we exploit the convexity of the dual problem, for which the coordinate-wise proximal gradient method can be applied. Our proposed method inherits advantages of the original SDCA, including (i) exponential convergence (with respect to the outer iteration steps), and (ii) better dependency on the sample size and condition number than the full-batch gradient method. One technical challenge in implementing the SDCA update is the intractable integral over the entire parameter space at every step. To overcome this limitation, we propose a tractable \textit{particle method} that approximately solves the dual problem, and an importance re-weighted technique to reduce the computational cost. The convergence rate of our method is verified by numerical experiments. | Kazusato Oko, Taiji Suzuki, Atsushi Nitanda, Denny Wu |  |
| 176 |  |  [The Effects of Invertibility on the Representational Complexity of Encoders in Variational Autoencoders](https://openreview.net/forum?id=7_JR7WpwKV1) |  | 0 |  | Divyansh Pareek, Andrej Risteski |  |
| 177 |  |  [Tracking the risk of a deployed model and detecting harmful distribution shifts](https://openreview.net/forum?id=Ro_zAjZppv) |  | 0 |  | Aleksandr Podkopaev, Aaditya Ramdas |  |
| 178 |  |  [Towards Understanding the Robustness Against Evasion Attack on Categorical Data](https://openreview.net/forum?id=BmJV7kyAmg) |  | 0 | Characterizing and assessing the adversarial vulnerability of classification models with categorical input has been a practically important, while rarely explored research problem. Our work echoes the challenge by first unveiling the impact factors of adversarial vulnerability of classification models with categorical data based on an information-theoretic adversarial risk analysis about the targeted classifier. Though certifying the robustness of such classification models is intrinsically an NP-hard combinatorial problem, our study shows that the robustness certification can be solved via an efficient greedy exploration of the discrete attack space for any measurable classifiers with a mild smoothness constraint. Our proposed robustness certification framework is instantiated with deep neural network models applied on real-world safety-critic data sources. Our empirical observations confirm the impact of the key adversarial risk factors with categorical input. | Hongyan Bao, Yufei Han, Yujun Zhou, Yun Shen, Xiangliang Zhang |  |
| 179 |  |  [Learning Curves for SGD on Structured Features](https://openreview.net/forum?id=WPI2vbkAl3Q) |  | 0 | The generalization performance of a machine learning algorithm such as a neural network depends in a non-trivial way on the structure of the data distribution. To analyze the influence of data structure on test loss dynamics, we study an exactly solveable model of stochastic gradient descent (SGD) on the square loss which predicts test error when training on features with arbitrary covariance structure. We solve the theory exactly for both Gaussian features and arbitrary features and we show that the simpler Gaussian model accurately predicts test loss of nonlinear random-feature models and neural networks in the kernel regime trained with SGD on real datasets such as MNIST and CIFAR-10. We show that the optimal batch size at a fixed compute budget is typically small and depends on the feature correlation structure, demonstrating the computational benefits of SGD with small batch sizes. Lastly, we extend our theory to the more usual setting of stochastic gradient descent on a fixed subsampled training set, showing that both training and test error can be accurately predicted in our framework on real data. | Blake Bordelon, Cengiz Pehlevan |  |
| 180 |  |  [NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training](https://openreview.net/forum?id=Qaw16njk6L) |  | 0 | Designing accurate and efficient vision transformers (ViTs) is a highly important but challenging task. Supernet-based one-shot neural architecture search (NAS) enables fast architecture optimization and has achieved state-of-the-art (SOTA) results on convolutional neural networks (CNNs). However, directly applying the supernet-based NAS to optimize ViTs leads to poor performance - even worse compared to training single ViTs. In this work, we observe that the poor performance is due to a gradient conflict issue: the gradients of different sub-networks conflict with that of the supernet more severely in ViTs than CNNs, which leads to early saturation in training and inferior convergence. To alleviate this issue, we propose a series of techniques, including a gradient projection algorithm, a switchable layer scaling design, and a simplified data augmentation and regularization training recipe. The proposed techniques significantly improve the convergence and the performance of all sub-networks. Our discovered hybrid ViT model family, dubbed NASViT, achieves top-1 accuracy from 78.2% to 81.8% on ImageNet from 200M to 800M FLOPs, and outperforms all the prior art CNNs and ViTs, including AlphaNet and LeViT, etc. When transferred to semantic segmentation tasks, NASViTs also outperform previous backbones on both Cityscape and ADE20K datasets, achieving 73.2% and 37.9% mIoU with only 5G FLOPs, respectively. Code is available at https://github.com/facebookresearch/NASViT. | Chengyue Gong, Dilin Wang, Meng Li, Xinlei Chen, Zhicheng Yan, Yuandong Tian, Qiang Liu, Vikas Chandra |  |
| 181 |  |  [Graphon based Clustering and Testing of Networks: Algorithms and Theory](https://openreview.net/forum?id=sTNHCrIKDQc) |  | 0 | Network-valued data are encountered in a wide range of applications, and pose challenges in learning due to their complex structure and absence of vertex correspondence. Typical examples of such problems include classification or grouping of protein structures and social networks. Various methods, ranging from graph kernels to graph neural networks, have been proposed that achieve some success in graph classification problems. However, most methods have limited theoretical justification, and their applicability beyond classification remains unexplored. In this work, we propose methods for clustering multiple graphs, without vertex correspondence, that are inspired by the recent literature on estimating graphons---symmetric functions corresponding to infinite vertex limit of graphs. We propose a novel graph distance based on sorting-and-smoothing graphon estimators. Using the proposed graph distance, we present two clustering algorithms and show that they achieve state-of-the-art results. We prove the statistical consistency of both algorithms under Lipschitz assumptions on the graph degrees. We further study the applicability of the proposed distance for graph two-sample testing problems. | Mahalakshmi Sabanayagam, Leena Chennuru Vankadara, Debarghya Ghoshdastidar |  |
| 182 |  |  [Network Augmentation for Tiny Deep Learning](https://openreview.net/forum?id=TYw3-OlrRm-) |  | 0 | We introduce Network Augmentation (NetAug), a new training method for improving the performance of tiny neural networks. Existing regularization techniques (e.g., data augmentation, dropout) have shown much success on large neural networks by adding noise to overcome over-fitting. However, we found these techniques hurt the performance of tiny neural networks. We argue that training tiny models are different from large models: rather than augmenting the data, we should augment the model, since tiny models tend to suffer from under-fitting rather than over-fitting due to limited capacity. To alleviate this issue, NetAug augments the network (reverse dropout) instead of inserting noise into the dataset or the network. It puts the tiny model into larger models and encourages it to work as a sub-model of larger models to get extra supervision, in addition to functioning as an independent model. At test time, only the tiny model is used for inference, incurring zero inference overhead. We demonstrate the effectiveness of NetAug on image classification and object detection. NetAug consistently improves the performance of tiny models, achieving up to 2.2% accuracy improvement on ImageNet. On object detection, achieving the same level of performance, NetAug requires 41% fewer MACs on Pascal VOC and 38% fewer MACs on COCO than the baseline. | Han Cai, Chuang Gan, Ji Lin, Song Han |  |
| 183 |  |  [Bridging the Gap: Providing Post-Hoc Symbolic Explanations for Sequential Decision-Making Problems with Inscrutable Representations](https://openreview.net/forum?id=o-1v9hdSult) |  | 0 | As increasingly complex AI systems are introduced into our daily lives, it becomes important for such systems to be capable of explaining the rationale for their decisions and allowing users to contest these decisions. A significant hurdle to allowing for such explanatory dialogue could be the {\em vocabulary mismatch} between the user and the AI system. This paper introduces methods for providing contrastive explanations in terms of user-specified concepts for sequential decision-making settings where the system's model of the task may be best represented as an inscrutable model. We do this by building partial symbolic models of a local approximation of the task that can be leveraged to answer the user queries. We test these methods on a popular Atari game (Montezuma's Revenge) and variants of Sokoban (a well-known planning benchmark) and report the results of user studies to evaluate whether people find explanations generated in this form useful. | Sarath Sreedharan, Utkarsh Soni, Mudit Verma, Siddharth Srivastava, Subbarao Kambhampati |  |
| 184 |  |  [Distributional Reinforcement Learning with Monotonic Splines](https://openreview.net/forum?id=C8Ltz08PtBp) |  | 0 | Distributional Reinforcement Learning (RL) differs from traditional RL by estimating the distribution over returns to capture the intrinsic uncertainty of MDPs. One key challenge in distributional RL lies in how to parameterize the quantile function when minimizing the Wasserstein metric of temporal differences. Existing algorithms use step functions or piecewise linear functions. In this paper, we propose to learn smooth continuous quantile functions represented by monotonic rational-quadratic splines, which also naturally solve the quantile crossing problem. Experiments in stochastic environments show that a dense estimation for quantile functions enhances distributional RL in terms of faster empirical convergence and higher rewards in most cases. | Yudong Luo, Guiliang Liu, Haonan Duan, Oliver Schulte, Pascal Poupart |  |
| 185 |  |  [Toward Faithful Case-based Reasoning through Learning Prototypes in a Nearest Neighbor-friendly Space](https://openreview.net/forum?id=R79ZGjHhv6p) |  | 0 | Recent advances in machine learning have brought opportunities for the ever-increasing use of AI in the real world. This has created concerns about the black-box nature of many of the most recent machine learning approaches. In this work, we propose an interpretable neural network that leverages metric and prototype learning for classification tasks. It encodes its own explanations and provides an improved case-based reasoning through learning prototypes in an embedding space learned by a probabilistic nearest neighbor rule. Through experiments, we demonstrated the effectiveness of the proposed method in both performance and the accuracy of the explanations provided. | Seyed Omid Davoudi, Majid Komeili |  |
| 186 |  |  [Augmented Sliced Wasserstein Distances](https://openreview.net/forum?id=iMqTLyfwnOO) |  | 0 | While theoretically appealing, the application of the Wasserstein distance to large-scale machine learning problems has been hampered by its prohibitive computational cost. The sliced Wasserstein distance and its variants improve the computational efficiency through the random projection, yet they suffer from low accuracy if the number of projections is not sufficiently large, because the majority of projections result in trivially small values. In this work, we propose a new family of distance metrics, called augmented sliced Wasserstein distances (ASWDs), constructed by first mapping samples to higher-dimensional hypersurfaces parameterized by neural networks. It is derived from a key observation that (random) linear projections of samples residing on these hypersurfaces would translate to much more flexible nonlinear projections in the original sample space, so they can capture complex structures of the data distribution. We show that the hypersurfaces can be optimized by gradient ascent efficiently. We provide the condition under which the ASWD is a valid metric and show that this can be obtained by an injective neural network architecture. Numerical results demonstrate that the ASWD significantly outperforms other Wasserstein variants for both synthetic and real-world problems. | Xiongjie Chen, Yongxin Yang, Yunpeng Li |  |
| 187 |  |  [Relational Learning with Variational Bayes](https://openreview.net/forum?id=Az-7gJc6lpr) |  | 0 |  | KuangHung Liu |  |
| 188 |  |  [Provably Robust Adversarial Examples](https://openreview.net/forum?id=UMfhoMtIaP5) |  | 0 | We introduce the concept of provably robust adversarial examples for deep neural networks – connected input regions constructed from standard adversarial examples which are guaranteed to be robust to a set of real-world perturbations (such as changes in pixel intensity and geometric transformations). We present a novel method called PARADE for generating these regions in a scalable manner which works by iteratively refining the region initially obtained via sampling until a refined region is certified to be adversarial with existing state-of-the-art verifiers. At each step, a novel optimization procedure is applied to maximize the region's volume under the constraint that the convex relaxation of the network behavior with respect to the region implies a chosen bound on the certification objective. Our experimental evaluation shows the effectiveness of PARADE: it successfully finds large provably robust regions including ones containing $\approx 10^{573}$ adversarial examples for pixel intensity and $\approx 10^{599}$ for geometric perturbations. The provability enables our robust examples to be significantly more effective against state-of-the-art defenses based on randomized smoothing than the individual attacks used to construct the regions. | Dimitar Iliev Dimitrov, Gagandeep Singh, Timon Gehr, Martin T. Vechev |  |
| 189 |  |  [Joint Shapley values: a measure of joint feature importance](https://openreview.net/forum?id=vcUmUvQCloe) |  | 0 | The Shapley value is one of the most widely used measures of feature importance partly as it measures a feature's average effect on a model's prediction. We introduce joint Shapley values, which directly extend Shapley's axioms and intuitions: joint Shapley values measure a set of features' average effect on a model's prediction. We prove the uniqueness of joint Shapley values, for any order of explanation. Results for games show that joint Shapley values present different insights from existing interaction indices, which assess the effect of a feature within a set of features. The joint Shapley values seem to provide sensible results in ML attribution problems. With binary features, we present a presence-adjusted global value that is more consistent with local intuitions than the usual approach. | Chris Harris, Richard Pymar, Colin Rowat |  |
| 190 |  |  [Low-Budget Active Learning via Wasserstein Distance: An Integer Programming Approach](https://openreview.net/forum?id=v8OlxjGn23S) |  | 0 | Active learning is the process of training a model with limited labeled data by selecting a core subset of an unlabeled data pool to label. The large scale of data sets used in deep learning forces most sample selection strategies to employ efficient heuristics. This paper introduces an integer optimization problem for selecting a core set that minimizes the discrete Wasserstein distance from the unlabeled pool. We demonstrate that this problem can be tractably solved with a Generalized Benders Decomposition algorithm. Our strategy uses high-quality latent features that can be obtained by unsupervised learning on the unlabeled pool. Numerical results on several data sets show that our optimization approach is competitive with baselines and particularly outperforms them in the low budget regime where less than one percent of the data set is labeled. | Rafid Mahmood, Sanja Fidler, Marc T. Law |  |
| 191 |  |  [Efficient Self-supervised Vision Transformers for Representation Learning](https://openreview.net/forum?id=fVu3o-YUGQK) |  | 0 | This paper investigates two techniques for developing efficient self-supervised vision transformers (EsViT) for visual representation learning. First, we show through a comprehensive empirical study that multi-stage architectures with sparse self-attentions can significantly reduce modeling complexity but with a cost of losing the ability to capture fine-grained correspondences between image regions. Second, we propose a new pre-training task, non-contrastive region-matching, which allows the model to capture fine-grained region dependencies and as a result significantly improves the quality of the learned vision representations. Our results show that combining the two techniques, EsViT achieves 81.3% top-1 on the ImageNet linear probe evaluation, outperforming prior arts with around an order magnitude of higher throughput. When transferring to downstream linear classification tasks, EsViT outperforms its supervised counterpart on 17 out of 18 datasets. The code and pre-trained models are released at: https://github.com/microsoft/esvit | Chunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai, Lu Yuan, Jianfeng Gao |  |
| 192 |  |  [Visual Representation Learning Does Not Generalize Strongly Within the Same Domain](https://openreview.net/forum?id=9RUHPlladgh) |  | 0 |  | Lukas Schott, Julius von Kügelgen, Frederik Träuble, Peter Vincent Gehler, Chris Russell, Matthias Bethge, Bernhard Schölkopf, Francesco Locatello, Wieland Brendel |  |
| 193 |  |  [Hidden Convexity of Wasserstein GANs: Interpretable Generative Models with Closed-Form Solutions](https://openreview.net/forum?id=e2Lle5cij9D) |  | 0 |  | Arda Sahiner, Tolga Ergen, Batu Ozturkler, Burak Bartan, John M. Pauly, Morteza Mardani, Mert Pilanci |  |
| 194 |  |  [Memory Augmented Optimizers for Deep Learning](https://openreview.net/forum?id=NRX9QZ6yqt) |  | 0 |  | PaulAymeric Martin McRae, Prasanna Parthasarathi, Mido Assran, Sarath Chandar |  |
| 195 |  |  [Orchestrated Value Mapping for Reinforcement Learning](https://openreview.net/forum?id=c87d0TS4yX) |  | 0 |  | Mehdi Fatemi, Arash Tavakoli |  |
| 196 |  |  [Learning to Generalize across Domains on Single Test Samples](https://openreview.net/forum?id=CIaQKbTBwtU) |  | 0 |  | Zehao Xiao, Xiantong Zhen, Ling Shao, Cees G. M. Snoek |  |
| 197 |  |  [Prototype memory and attention mechanisms for few shot image generation](https://openreview.net/forum?id=lY0-7bj0Vfz) |  | 0 |  | Tianqin Li, Zijie Li, Andrew Luo, Harold Rockwell, Amir Barati Farimani, Tai Sing Lee |  |
| 198 |  |  [TPU-GAN: Learning temporal coherence from dynamic point cloud sequences](https://openreview.net/forum?id=FEBFJ98FKx) |  | 0 |  | Zijie Li, Tianqin Li, Amir Barati Farimani |  |
| 199 |  |  [A First-Occupancy Representation for Reinforcement Learning](https://openreview.net/forum?id=JBAZe2yN6Ub) |  | 0 |  | Ted Moskovitz, Spencer R. Wilson, Maneesh Sahani |  |
| 200 |  |  [Deep ReLU Networks Preserve Expected Length](https://openreview.net/forum?id=ci7LBzDn2Q) |  | 0 |  | Boris Hanin, Ryan S. Jeong, David Rolnick |  |
| 201 |  |  [Phenomenology of Double Descent in Finite-Width Neural Networks](https://openreview.net/forum?id=lTqGXfn9Tv) |  | 0 | \`Double descent' delineates the generalization behaviour of models depending on the regime they belong to: under- or over-parameterized. The current theoretical understanding behind the occurrence of this phenomenon is primarily based on linear and kernel regression models --- with informal parallels to neural networks via the Neural Tangent Kernel. Therefore such analyses do not adequately capture the mechanisms behind double descent in finite-width neural networks, as well as, disregard crucial components --- such as the choice of the loss function. We address these shortcomings by leveraging influence functions in order to derive suitable expressions of the population loss and its lower bound, while imposing minimal assumptions on the form of the parametric model. Our derived bounds bear an intimate connection with the spectrum of the Hessian at the optimum, and importantly, exhibit a double descent behaviour at the interpolation threshold. Building on our analysis, we further investigate how the loss function affects double descent --- and thus uncover interesting properties of neural networks and their Hessian spectra near the interpolation threshold. | Sidak Pal Singh, Aurélien Lucchi, Thomas Hofmann, Bernhard Schölkopf |  |
| 202 |  |  [How Attentive are Graph Attention Networks?](https://openreview.net/forum?id=F72ximsx7C1) |  | 0 | Graph Attention Networks (GATs) are one of the most popular GNN architectures and are considered as the state-of-the-art architecture for representation learning with graphs. In GAT, every node attends to its neighbors given its own representation as the query. However, in this paper we show that GAT computes a very limited kind of attention: the ranking of the attention scores is unconditioned on the query node. We formally define this restricted kind of attention as static attention and distinguish it from a strictly more expressive dynamic attention. Because GATs use a static attention mechanism, there are simple graph problems that GAT cannot express: in a controlled problem, we show that static attention hinders GAT from even fitting the training data. To remove this limitation, we introduce a simple fix by modifying the order of operations and propose GATv2: a dynamic graph attention variant that is strictly more expressive than GAT. We perform an extensive evaluation and show that GATv2 outperforms GAT across 12 OGB and other benchmarks while we match their parametric costs. Our code is available at https://github.com/tech-srl/how_attentive_are_gats . GATv2 is available as part of the PyTorch Geometric library, the Deep Graph Library, and the TensorFlow GNN library. | Shaked Brody, Uri Alon, Eran Yahav |  |
| 203 |  |  [Learning Transferable Reward for Query Object Localization with Policy Adaptation](https://openreview.net/forum?id=92tYQiil17) |  | 0 | We propose a reinforcement learning based approach to query object localization, for which an agent is trained to localize objects of interest specified by a small exemplary set. We learn a transferable reward signal formulated using the exemplary set by ordinal metric learning. Our proposed method enables test-time policy adaptation to new environments where the reward signals are not readily available, and outperforms fine-tuning approaches that are limited to annotated images. In addition, the transferable reward allows repurposing the trained agent from one specific class to another class. Experiments on corrupted MNIST, CU-Birds, and COCO datasets demonstrate the effectiveness of our approach. | Tingfeng Li, Shaobo Han, Martin Renqiang Min, Dimitris N. Metaxas |  |
| 204 |  |  [CKConv: Continuous Kernel Convolution For Sequential Data](https://openreview.net/forum?id=8FhxBtXSl0) |  | 0 | Conventional neural architectures for sequential data present important limitations. Recurrent neural networks suffer from exploding and vanishing gradients, small effective memory horizons, and must be trained sequentially. Convolutional neural networks cannot handle sequences of unknown size and their memory horizon must be defined a priori. In this work, we show that these problems can be solved by formulating the convolutional kernels of CNNs as continuous functions. The resulting Continuous Kernel Convolution (CKConv) handles arbitrarily long sequences in a parallel manner, within a single operation, and without relying on any form of recurrence. We show that Continuous Kernel Convolutional Networks (CKCNNs) obtain state-of-the-art results in multiple datasets, e.g., permuted MNIST, and, thanks to their continuous nature, are able to handle non-uniformly sampled datasets and irregularly-sampled data natively. CKCNNs match or perform better than neural ODEs designed for these purposes in a faster and simpler manner. | David W. Romero, Anna Kuzina, Erik J. Bekkers, Jakub Mikolaj Tomczak, Mark Hoogendoorn |  |
| 205 |  |  [Towards Empirical Sandwich Bounds on the Rate-Distortion Function](https://openreview.net/forum?id=H4PmOqSZDY) |  | 0 | Rate-distortion (R-D) function, a key quantity in information theory, characterizes the fundamental limit of how much a data source can be compressed subject to a fidelity criterion, by any compression algorithm. As researchers push for ever-improving compression performance, establishing the R-D function of a given data source is not only of scientific interest, but also reveals the possible room for improvement in existing compression algorithms. Previous work on this problem relied on distributional assumptions on the data source (Gibson, 2017) or only applied to discrete data (Blahut, 1972; Arimoto, 1972). By contrast, this paper makes the first attempt at an algorithm for sandwiching the R-D function of a general (not necessarily discrete) source requiring only i.i.d. data samples. We estimate R-D sandwich bounds for a variety of artificial and real-world data sources, in settings far beyond the feasibility of any known method, and shed light on the optimality of neural data compression (Ballé et al., 2021; Yang et al., 2022). Our R-D upper bound on natural images indicates theoretical room for improving state-of-the-art image compression methods by at least one dB in PSNR at various bitrates. Our data and code can be found at https://github.com/mandt-lab/RD-sandwich. | Yibo Yang, Stephan Mandt |  |
| 206 |  |  [Pareto Policy Adaptation](https://openreview.net/forum?id=wfZGut6e09) |  | 0 | We present a policy gradient method for Multi-Objective Reinforcement Learning under unknown, linear preferences. By enforcing Pareto stationarity, a first-order condition for Pareto optimality, we are able to design a simple policy gradient algorithm that approximates the Pareto front and infers the unknown preferences. Our method relies on a projected gradient descent solver that identifies common ascent directions for all objectives. Leveraging the solution of that solver, we introduce Pareto Policy Adaptation (PPA), a loss function that adapts the policy to be optimal with respect to any distribution over preferences. PPA uses implicit differentiation to back-propagate the loss gradient bypassing the operations of the projected gradient descent solver. Our approach is straightforward, easy to implement and can be used with all existing policy gradient and actor-critic methods. We evaluate our method in a series of reinforcement learning tasks | Panagiotis Kyriakis, Jyotirmoy Deshmukh, Paul Bogdan |  |
| 207 |  |  [Fair Normalizing Flows](https://openreview.net/forum?id=BrFIKuxrZE) |  | 0 | Fair representation learning is an attractive approach that promises fairness of downstream predictors by encoding sensitive data. Unfortunately, recent work has shown that strong adversarial predictors can still exhibit unfairness by recovering sensitive attributes from these representations. In this work, we present Fair Normalizing Flows (FNF), a new approach offering more rigorous fairness guarantees for learned representations. Specifically, we consider a practical setting where we can estimate the probability density for sensitive groups. The key idea is to model the encoder as a normalizing flow trained to minimize the statistical distance between the latent representations of different groups. The main advantage of FNF is that its exact likelihood computation allows us to obtain guarantees on the maximum unfairness of any potentially adversarial downstream predictor. We experimentally demonstrate the effectiveness of FNF in enforcing various group fairness notions, as well as other attractive properties such as interpretability and transfer learning, on a variety of challenging real-world datasets. | Mislav Balunovic, Anian Ruoss, Martin T. Vechev |  |
| 208 |  |  [The Convex Geometry of Backpropagation: Neural Network Gradient Flows Converge to Extreme Points of the Dual Convex Program](https://openreview.net/forum?id=5QhUE1qiVC6) |  | 0 |  | Yifei Wang, Mert Pilanci |  |
| 209 |  |  [Adaptive Wavelet Transformer Network for 3D Shape Representation Learning](https://openreview.net/forum?id=5MLb3cLCJY) |  | 0 | We present a novel method for 3D shape representation learning using multi-scale wavelet decomposition. Previous works often decompose 3D shapes into complementary components in spatial domain at a single scale. In this work, we study to decompose 3D shapes into sub-bands components in frequency domain at multiple scales, resulting in a hierarchical decomposition tree in a principled manner rooted in multi-resolution wavelet analysis. Specifically, we propose Adaptive Wavelet Transformer Network (AWT-Net) that firstly generates approximation or detail wavelet coefficients per point, classifying each point into high or low sub-bands components, using lifting scheme at multiple scales recursively and hierarchically. Then, AWT-Net exploits Transformer to enhance the original shape features by querying and fusing features from different but integrated sub-bands. The wavelet coefficients can be learned without direct supervision on coefficients, and AWT-Net is fully differentiable and can be learned in an end-to-end fashion. Extensive experiments demonstrate that AWT-Net achieves competitive performance on 3D shape classification and segmentation benchmarks. | Hao Huang, Yi Fang |  |
| 210 |  |  [On the Convergence of mSGD and AdaGrad for Stochastic Optimization](https://openreview.net/forum?id=g5tANwND04i) |  | 0 |  | Ruinan Jin, Yu Xing, Xingkang He |  |
| 211 |  |  [Likelihood Training of Schrödinger Bridge using Forward-Backward SDEs Theory](https://openreview.net/forum?id=nioAdKCEdXB) |  | 0 |  | Tianrong Chen, GuanHorng Liu, Evangelos A. Theodorou |  |
| 212 |  |  [Imitation Learning from Observations under Transition Model Disparity](https://openreview.net/forum?id=twv2QlJhXzo) |  | 0 | Learning to perform tasks by leveraging a dataset of expert observations, also known as imitation learning from observations (ILO), is an important paradigm for learning skills without access to the expert reward function or the expert actions. We consider ILO in the setting where the expert and the learner agents operate in different environments, with the source of the discrepancy being the transition dynamics model. Recent methods for scalable ILO utilize adversarial learning to match the state-transition distributions of the expert and the learner, an approach that becomes challenging when the dynamics are dissimilar. In this work, we propose an algorithm that trains an intermediary policy in the learner environment and uses it as a surrogate expert for the learner. The intermediary policy is learned such that the state transitions generated by it are close to the state transitions in the expert dataset. To derive a practical and scalable algorithm, we employ concepts from prior work on estimating the support of a probability distribution. Experiments using MuJoCo locomotion tasks highlight that our method compares favorably to the baselines for ILO with transition dynamics mismatch. | Tanmay Gangwani, Yuan Zhou, Jian Peng |  |
| 213 |  |  [MCMC Should Mix: Learning Energy-Based Model with Neural Transport Latent Space MCMC](https://openreview.net/forum?id=4C93Qvn-tz) |  | 0 |  | Erik Nijkamp, Ruiqi Gao, Pavel Sountsov, Srinivas Vasudevan, Bo Pang, SongChun Zhu, Ying Nian Wu |  |
| 214 |  |  [Autonomous Learning of Object-Centric Abstractions for High-Level Planning](https://openreview.net/forum?id=rrWeE9ZDw_) |  | 0 |  | Steven James, Benjamin Rosman, George Konidaris |  |
| 215 |  |  [A fast and accurate splitting method for optimal transport: analysis and implementation](https://openreview.net/forum?id=fCSq8yrDkc) |  | 0 | We develop a fast and reliable method for solving large-scale optimal transport (OT) problems at an unprecedented combination of speed and accuracy. Built on the celebrated Douglas-Rachford splitting technique, our method tackles the original OT problem directly instead of solving an approximate regularized problem, as many state-of-the-art techniques do. This allows us to provide sparse transport plans and avoid numerical issues of methods that use entropic regularization. The algorithm has the same cost per iteration as the popular Sinkhorn method, and each iteration can be executed efficiently, in parallel. The proposed method enjoys an iteration complexity $O(1/\epsilon)$ compared to the best-known $O(1/\epsilon^2)$ of the Sinkhorn method. In addition, we establish a linear convergence rate for our formulation of the OT problem. We detail an efficient GPU implementation of the proposed method that maintains a primal-dual stopping criterion at no extra cost. Substantial experiments demonstrate the effectiveness of our method, both in terms of computation times and robustness. | Vien V. Mai, Jacob Lindbäck, Mikael Johansson |  |
| 216 |  |  [Implicit Bias of MSE Gradient Optimization in Underparameterized Neural Networks](https://openreview.net/forum?id=VLgmhQDVBV) |  | 0 | We study the dynamics of a neural network in function space when optimizing the mean squared error via gradient flow. We show that in the underparameterized regime the network learns eigenfunctions of an integral operator $T_K$ determined by the Neural Tangent Kernel at rates corresponding to their eigenvalues. For example, for uniformly distributed data on the sphere $S^{d - 1}$ and rotation invariant weight distributions, the eigenfunctions of $T_K$ are the spherical harmonics. Our results can be understood as describing a spectral bias in the underparameterized regime. The proofs use the concept of \`\`Damped Deviations'' where deviations of the NTK matter less for eigendirections with large eigenvalues. Aside from the underparameterized regime, the damped deviations point-of-view allows us to extend certain results in the literature in the overparameterized setting. | Benjamin Bowman, Guido Montúfar |  |
| 217 |  |  [Discovering Latent Concepts Learned in BERT](https://openreview.net/forum?id=POTMtpYI1xH) |  | 0 |  | Fahim Dalvi, Abdul Rafae Khan, Firoj Alam, Nadir Durrani, Jia Xu, Hassan Sajjad |  |
| 218 |  |  [The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks](https://openreview.net/forum?id=dNigytemkL) |  | 0 |  | Rahim Entezari, Hanie Sedghi, Olga Saukh, Behnam Neyshabur |  |
| 219 |  |  [Data Poisoning Won't Save You From Facial Recognition](https://openreview.net/forum?id=B5XahNLmna) |  | 0 |  | Evani RadiyaDixit, Sanghyun Hong, Nicholas Carlini, Florian Tramèr |  |
| 220 |  |  [MetaMorph: Learning Universal Controllers with Transformers](https://openreview.net/forum?id=Opmqtk_GvYL) |  | 0 |  | Agrim Gupta, Linxi Fan, Surya Ganguli, Li FeiFei |  |
| 221 |  |  [HTLM: Hyper-Text Pre-Training and Prompting of Language Models](https://openreview.net/forum?id=P-pPW1nxf1r) |  | 0 |  | Armen Aghajanyan, Dmytro Okhonko, Mike Lewis, Mandar Joshi, Hu Xu, Gargi Ghosh, Luke Zettlemoyer |  |
| 222 |  |  [Illiterate DALL-E Learns to Compose](https://openreview.net/forum?id=h0OYV0We3oh) |  | 0 |  | Gautam Singh, Fei Deng, Sungjin Ahn |  |
| 223 |  |  [The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models](https://openreview.net/forum?id=JYtwGwIL7ye) |  | 0 |  | Alexander Pan, Kush Bhatia, Jacob Steinhardt |  |
| 224 |  |  [Optimizing Neural Networks with Gradient Lexicase Selection](https://openreview.net/forum?id=J_2xNmVcY4) |  | 0 |  | Li Ding, Lee Spector |  |
| 225 |  |  [Offline Reinforcement Learning with Implicit Q-Learning](https://openreview.net/forum?id=68n2s9ZJWF8) |  | 0 |  | Ilya Kostrikov, Ashvin Nair, Sergey Levine |  |
| 226 |  |  [Learning Distributionally Robust Models at Scale via Composite Optimization](https://openreview.net/forum?id=To-R742x7se) |  | 0 |  | Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, Amin Karbasi |  |
| 227 |  |  [Counterfactual Plans under Distributional Ambiguity](https://openreview.net/forum?id=noaG7SrPVK0) |  | 0 |  | Ngoc Bui, Duy Nguyen, Viet Anh Nguyen |  |
| 228 |  |  [Neural Parameter Allocation Search](https://openreview.net/forum?id=srtIXtySfT4) |  | 0 |  | Bryan A. Plummer, Nikoli Dryden, Julius Frost, Torsten Hoefler, Kate Saenko |  |
| 229 |  |  [Non-Linear Operator Approximations for Initial Value Problems](https://openreview.net/forum?id=d2TT6gK9qZn) |  | 0 |  | Gaurav Gupta, Xiongye Xiao, Radu Balan, Paul Bogdan |  |
| 230 |  |  [Constructing a Good Behavior Basis for Transfer using Generalized Policy Updates](https://openreview.net/forum?id=7IWGzQ6gZ1D) |  | 0 |  | Safa Alver, Doina Precup |  |
| 231 |  |  [Collapse by Conditioning: Training Class-conditional GANs with Limited Data](https://openreview.net/forum?id=7TZeCsNOUB_) |  | 0 |  | Mohamad Shahbazi, Martin Danelljan, Danda Pani Paudel, Luc Van Gool |  |
| 232 |  |  [High Probability Bounds for a Class of Nonconvex Algorithms with AdaGrad Stepsize](https://openreview.net/forum?id=dSw0QtRMJkO) |  | 0 |  | Ali Kavis, Kfir Yehuda Levy, Volkan Cevher |  |
| 233 |  |  [Map Induction: Compositional spatial submap learning for efficient exploration in novel environments](https://openreview.net/forum?id=1NUsBU-7HAL) |  | 0 |  | Sugandha Sharma, Aidan Curtis, Marta Kryven, Joshua B. Tenenbaum, Ila R. Fiete |  |
| 234 |  |  [How Did the Model Change? Efficiently Assessing Machine Learning API Shifts](https://openreview.net/forum?id=gFDFKC4gHL4) |  | 0 |  | Lingjiao Chen, Matei Zaharia, James Zou |  |
| 235 |  |  [A Tale of Two Flows: Cooperative Learning of Langevin Flow and Normalizing Flow Toward Energy-Based Model](https://openreview.net/forum?id=31d5RLCUuXC) |  | 0 |  | Jianwen Xie, Yaxuan Zhu, Jun Li, Ping Li |  |
| 236 |  |  [Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?](https://openreview.net/forum?id=WVX0NNVBBkV) |  | 0 |  | Vikash Sehwag, Saeed Mahloujifar, Tinashe Handina, Sihui Dai, Chong Xiang, Mung Chiang, Prateek Mittal |  |
| 237 |  |  [Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap](https://openreview.net/forum?id=ECvgmYVyeUz) |  | 0 |  | Yifei Wang, Qi Zhang, Yisen Wang, Jiansheng Yang, Zhouchen Lin |  |
| 238 |  |  [Language-biased image classification: evaluation based on semantic representations](https://openreview.net/forum?id=xNO7OEIcJc6) |  | 0 |  | Yoann Lemesle, Masataka Sawayama, Guillermo Valle Pérez, Maxime Adolphe, Hélène Sauzéon, PierreYves Oudeyer |  |
| 239 |  |  [Robbing the Fed: Directly Obtaining Private Data in Federated Learning with Modified Models](https://openreview.net/forum?id=fwzUgo0FM9v) |  | 0 |  | Liam H. Fowl, Jonas Geiping, Wojciech Czaja, Micah Goldblum, Tom Goldstein |  |
| 240 |  |  [Practical Conditional Neural Process Via Tractable Dependent Predictions](https://openreview.net/forum?id=3pugbNqOh5m) |  | 0 |  | Stratis Markou, James Requeima, Wessel P. Bruinsma, Anna Vaughan, Richard E. Turner |  |
| 241 |  |  [Reward Uncertainty for Exploration in Preference-based Reinforcement Learning](https://openreview.net/forum?id=OWZVD-l-ZrC) |  | 0 |  | Xinran Liang, Katherine Shu, Kimin Lee, Pieter Abbeel |  |
| 242 |  |  [Decentralized Learning for Overparameterized Problems: A Multi-Agent Kernel Approximation Approach](https://openreview.net/forum?id=oj2yn1Q4Ett) |  | 0 |  | Prashant Khanduri, Haibo Yang, Mingyi Hong, Jia Liu, HoiTo Wai, Sijia Liu |  |
| 243 |  |  [Permutation-Based SGD: Is Random Optimal?](https://openreview.net/forum?id=YiBa9HKTyXE) |  | 0 |  | Shashank Rajput, Kangwook Lee, Dimitris S. Papailiopoulos |  |
| 244 |  |  [Graph-less Neural Networks: Teaching Old MLPs New Tricks Via Distillation](https://openreview.net/forum?id=4p6_5HBWPCw) |  | 0 |  | Shichang Zhang, Yozen Liu, Yizhou Sun, Neil Shah |  |
| 245 |  |  [Relating transformers to models and neural representations of the hippocampal formation](https://openreview.net/forum?id=B8DVo9B1YE0) |  | 0 |  | James C. R. Whittington, Joseph Warren, Tim E. J. Behrens |  |
| 246 |  |  [How many degrees of freedom do we need to train deep networks: a loss landscape perspective](https://openreview.net/forum?id=ChMLTGRjFcU) |  | 0 |  | Brett W. Larsen, Stanislav Fort, Nic Becker, Surya Ganguli |  |
| 247 |  |  [Is Importance Weighting Incompatible with Interpolating Classifiers?](https://openreview.net/forum?id=uqBOne3LUKy) |  | 0 |  | Ke Alexander Wang, Niladri Shekhar Chatterji, Saminul Haque, Tatsunori Hashimoto |  |
| 248 |  |  [Neural Models for Output-Space Invariance in Combinatorial Problems](https://openreview.net/forum?id=ibrUkC-pbis) |  | 0 |  | Yatin Nandwani, Vidit Jain, Mausam, Parag Singla |  |
| 249 |  |  [StyleNeRF: A Style-based 3D Aware Generator for High-resolution Image Synthesis](https://openreview.net/forum?id=iUuzzTMUw9K) |  | 0 |  | Jiatao Gu, Lingjie Liu, Peng Wang, Christian Theobalt |  |
| 250 |  |  [The Role of Pretrained Representations for the OOD Generalization of RL Agents](https://openreview.net/forum?id=8eb12UQYxrG) |  | 0 |  | Frederik Träuble, Andrea Dittadi, Manuel Wuthrich, Felix Widmaier, Peter Vincent Gehler, Ole Winther, Francesco Locatello, Olivier Bachem, Bernhard Schölkopf, Stefan Bauer |  |
| 251 |  |  [Enabling Arbitrary Translation Objectives with Adaptive Tree Search](https://openreview.net/forum?id=rhOiUS8KQM9) |  | 0 |  | Wang Ling, Wojciech Stokowiec, Domenic Donato, Chris Dyer, Lei Yu, Laurent Sartran, Austin Matthews |  |
| 252 |  |  [Proof Artifact Co-Training for Theorem Proving with Language Models](https://openreview.net/forum?id=rpxJc9j04U) |  | 0 |  | Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward W. Ayers, Stanislas Polu |  |
| 253 |  |  [Mirror Descent Policy Optimization](https://openreview.net/forum?id=aBO5SvgSt1) |  | 0 |  | Manan Tomar, Lior Shani, Yonathan Efroni, Mohammad Ghavamzadeh |  |
| 254 |  |  [A Loss Curvature Perspective on Training Instabilities of Deep Learning Models](https://openreview.net/forum?id=OcKMT-36vUs) |  | 0 |  | Justin Gilmer, Behrooz Ghorbani, Ankush Garg, Sneha Kudugunta, Behnam Neyshabur, David Cardoze, George Edward Dahl, Zachary Nado, Orhan Firat |  |
| 255 |  |  [Cross-Domain Imitation Learning via Optimal Transport](https://openreview.net/forum?id=xP3cPq2hQC) |  | 0 |  | Arnaud Fickinger, Samuel Cohen, Stuart Russell, Brandon Amos |  |
| 256 |  |  [Large-Scale Representation Learning on Graphs via Bootstrapping](https://openreview.net/forum?id=0UXT6PpRpW) |  | 0 |  | Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Mehdi Azabou, Eva L. Dyer, Rémi Munos, Petar Velickovic, Michal Valko |  |
| 257 |  |  [Robust and Scalable SDE Learning: A Functional Perspective](https://openreview.net/forum?id=xZ6H7wydGl) |  | 0 |  | Scott Alexander Cameron, Tyron Luke Cameron, Arnu Pretorius, Stephen J. Roberts |  |
| 258 |  |  [Neural Processes with Stochastic Attention: Paying more attention to the context dataset](https://openreview.net/forum?id=JPkQwEdYn8) |  | 0 |  | Mingyu Kim, Kyeongryeol Go, SeYoung Yun |  |
| 259 |  |  [Evaluating Disentanglement of Structured Representations](https://openreview.net/forum?id=SLz5sZjacp) |  | 0 |  | Raphaël DangNhu |  |
| 260 |  |  [Geometric Transformers for Protein Interface Contact Prediction](https://openreview.net/forum?id=CS4463zx6Hi) |  | 0 |  | Alex Morehead, Chen Chen, Jianlin Cheng |  |
| 261 |  |  [Diurnal or Nocturnal? Federated Learning of Multi-branch Networks from Periodically Shifting Distributions](https://openreview.net/forum?id=E4EE_ohFGz) |  | 0 |  | Chen Zhu, Zheng Xu, Mingqing Chen, Jakub Konecný, Andrew Hard, Tom Goldstein |  |
| 262 |  |  [IGLU: Efficient GCN Training via Lazy Updates](https://openreview.net/forum?id=5kq11Tl1z4) |  | 0 |  | S. Deepak Narayanan, Aditya Sinha, Prateek Jain, Purushottam Kar, Sundararajan Sellamanickam |  |
| 263 |  |  [Procedural generalization by planning with self-supervised world models](https://openreview.net/forum?id=FmBegXJToY) |  | 0 |  | Ankesh Anand, Jacob C. Walker, Yazhe Li, Eszter Vértes, Julian Schrittwieser, Sherjil Ozair, Theophane Weber, Jessica B. Hamrick |  |
| 264 |  |  [Top-N: Equivariant Set and Graph Generation without Exchangeability](https://openreview.net/forum?id=-Gk_IPJWvk) |  | 0 |  | Clément Vignac, Pascal Frossard |  |
| 265 |  |  [The Spectral Bias of Polynomial Neural Networks](https://openreview.net/forum?id=P7FLfMLTSEX) |  | 0 |  | Moulik Choraria, Leello Tadesse Dadi, Grigorios Chrysos, Julien Mairal, Volkan Cevher |  |
| 266 |  |  [Invariant Causal Representation Learning for Out-of-Distribution Generalization](https://openreview.net/forum?id=-e4EXDWXnSn) |  | 0 |  | Chaochao Lu, Yuhuai Wu, José Miguel HernándezLobato, Bernhard Schölkopf |  |
| 267 |  |  [LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5](https://openreview.net/forum?id=HCRVf71PMF) |  | 0 |  | Chengwei Qin, Shafiq R. Joty |  |
| 268 |  |  [On Non-Random Missing Labels in Semi-Supervised Learning](https://openreview.net/forum?id=6yVvwR9H9Oj) |  | 0 |  | Xinting Hu, Yulei Niu, Chunyan Miao, XianSheng Hua, Hanwang Zhang |  |
| 269 |  |  [Mapping conditional distributions for domain adaptation under generalized target shift](https://openreview.net/forum?id=sPfB2PI87BZ) |  | 0 |  | Matthieu Kirchmeyer, Alain Rakotomamonjy, Emmanuel de Bézenac, Patrick Gallinari |  |
| 270 |  |  [On the Generalization of Models Trained with SGD: Information-Theoretic Bounds and Implications](https://openreview.net/forum?id=oWZsQ8o5EA) |  | 0 |  | Ziqiao Wang, Yongyi Mao |  |
| 271 |  |  [Amortized Implicit Differentiation for Stochastic Bilevel Optimization](https://openreview.net/forum?id=3PN4iyXBeF) |  | 0 |  | Michael Arbel, Julien Mairal |  |
| 272 |  |  [Multi-objective Optimization by Learning Space Partition](https://openreview.net/forum?id=FlwzVjfMryn) |  | 0 |  | Yiyang Zhao, Linnan Wang, Kevin Yang, Tianjun Zhang, Tian Guo, Yuandong Tian |  |
| 273 |  |  [Mapping Language Models to Grounded Conceptual Spaces](https://openreview.net/forum?id=gJcEM8sxHK) |  | 0 |  | Roma Patel, Ellie Pavlick |  |
| 274 |  |  [The Efficiency Misnomer](https://openreview.net/forum?id=iulEMLYh1uR) |  | 0 |  | Mostafa Dehghani, Yi Tay, Anurag Arnab, Lucas Beyer, Ashish Vaswani |  |
| 275 |  |  [Hybrid Memoised Wake-Sleep: Approximate Inference at the Discrete-Continuous Interface](https://openreview.net/forum?id=auOPcdAcoy) |  | 0 |  | Tuan Anh Le, Katherine M. Collins, Luke Hewitt, Kevin Ellis, Siddharth Narayanaswamy, Samuel Gershman, Joshua B. Tenenbaum |  |
| 276 |  |  [Adversarial Retriever-Ranker for Dense Text Retrieval](https://openreview.net/forum?id=MR7XubKUFB) |  | 0 |  | Hang Zhang, Yeyun Gong, Yelong Shen, Jiancheng Lv, Nan Duan, Weizhu Chen |  |
| 277 |  |  [Conditioning Sequence-to-sequence Networks with Learned Activations](https://openreview.net/forum?id=t5s-hd1bqLk) |  | 0 |  | Alberto Gil Couto Pimentel Ramos, Abhinav Mehrotra, Nicholas Donald Lane, Sourav Bhattacharya |  |
| 278 |  |  [Lossy Compression with Distribution Shift as Entropy Constrained Optimal Transport](https://openreview.net/forum?id=BRFWxcZfAdC) |  | 0 |  | Huan Liu, George Zhang, Jun Chen, Ashish J. Khisti |  |
| 279 |  |  [Equivariant Self-Supervised Learning: Encouraging Equivariance in Representations](https://openreview.net/forum?id=gKLAAfiytI) |  | 0 |  | Rumen Dangovski, Li Jing, Charlotte Loh, Seungwook Han, Akash Srivastava, Brian Cheung, Pulkit Agrawal, Marin Soljacic |  |
| 280 |  |  [Direct then Diffuse: Incremental Unsupervised Skill Discovery for State Covering and Goal Reaching](https://openreview.net/forum?id=25kzAhUB1lz) |  | 0 |  | PierreAlexandre Kamienny, Jean Tarbouriech, Sylvain Lamprier, Alessandro Lazaric, Ludovic Denoyer |  |
| 281 |  |  [Normalization of Language Embeddings for Cross-Lingual Alignment](https://openreview.net/forum?id=Nh7CtbyoqV5) |  | 0 |  | Prince Osei Aboagye, Yan Zheng, ChinChia Michael Yeh, Junpeng Wang, Wei Zhang, Liang Wang, Hao Yang, Jeff M. Phillips |  |
| 282 |  |  [Boosting the Certified Robustness of L-infinity Distance Nets](https://openreview.net/forum?id=Q76Y7wkiji) |  | 0 |  | Bohang Zhang, Du Jiang, Di He, Liwei Wang |  |
| 283 |  |  [Stochastic Training is Not Necessary for Generalization](https://openreview.net/forum?id=ZBESeIUB5k) |  | 0 |  | Jonas Geiping, Micah Goldblum, Phillip Pope, Michael Moeller, Tom Goldstein |  |
| 284 |  |  [Transfer RL across Observation Feature Spaces via Model-Based Regularization](https://openreview.net/forum?id=7KdAoOsI81C) |  | 0 |  | Yanchao Sun, Ruijie Zheng, Xiyao Wang, Andrew E. Cohen, Furong Huang |  |
| 285 |  |  [GATSBI: Generative Adversarial Training for Simulation-Based Inference](https://openreview.net/forum?id=kR1hC6j48Tp) |  | 0 |  | Poornima Ramesh, JanMatthis Lueckmann, Jan Boelts, Álvaro TejeroCantero, David S. Greenberg, Pedro J. Gonçalves, Jakob H. Macke |  |
| 286 |  |  [Domain Adversarial Training: A Game Perspective](https://openreview.net/forum?id=AwgtcUAhBq) |  | 0 |  | David Acuna, Marc T. Law, Guojun Zhang, Sanja Fidler |  |
| 287 |  |  [Differentiable Expectation-Maximization for Set Representation Learning](https://openreview.net/forum?id=MXdFBmHT4C) |  | 0 |  | Minyoung Kim |  |
| 288 |  |  [Overcoming The Spectral Bias of Neural Value Approximation](https://openreview.net/forum?id=vIC-xLFuM6) |  | 0 |  | Ge Yang, Anurag Ajay, Pulkit Agrawal |  |
| 289 |  |  [Prospect Pruning: Finding Trainable Weights at Initialization using Meta-Gradients](https://openreview.net/forum?id=AIgn9uwfcD1) |  | 0 |  | Milad Alizadeh, Shyam A. Tailor, Luisa M. Zintgraf, Joost van Amersfoort, Sebastian Farquhar, Nicholas Donald Lane, Yarin Gal |  |
| 290 |  |  [CoMPS: Continual Meta Policy Search](https://openreview.net/forum?id=PVJ6j87gOHz) |  | 0 |  | Glen Berseth, Zhiwei Zhang, Grace Zhang, Chelsea Finn, Sergey Levine |  |
| 291 |  |  [Generalized rectifier wavelet covariance models for texture synthesis](https://openreview.net/forum?id=ziRLU3Y2PN_) |  | 0 |  | Antoine Brochard, Sixin Zhang, Stéphane Mallat |  |
| 292 |  |  [Towards Evaluating the Robustness of Neural Networks Learned by Transduction](https://openreview.net/forum?id=_5js_8uTrx1) |  | 0 |  | Jiefeng Chen, Xi Wu, Yang Guo, Yingyu Liang, Somesh Jha |  |
| 293 |  |  [Object Dynamics Distillation for Scene Decomposition and Representation](https://openreview.net/forum?id=oJGDYQFKL3i) |  | 0 |  | Qu Tang, Xiangyu Zhu, Zhen Lei, Zhaoxiang Zhang |  |
| 294 |  |  [Practical Integration via Separable Bijective Networks](https://openreview.net/forum?id=NlObxR0rosG) |  | 0 |  | Christopher M. Bender, Patrick Emmanuel, Michael K. Reiter, Junier Oliva |  |
| 295 |  |  [Self-Joint Supervised Learning](https://openreview.net/forum?id=zuqcmNVK4c2) |  | 0 |  | Navid Kardan, Mubarak Shah, Mitch Hill |  |
| 296 |  |  [Rethinking Supervised Pre-Training for Better Downstream Transferring](https://openreview.net/forum?id=Jjcv9MTqhcq) |  | 0 |  | Yutong Feng, Jianwen Jiang, Mingqian Tang, Rong Jin, Yue Gao |  |
| 297 |  |  [A Zest of LIME: Towards Architecture-Independent Model Distances](https://openreview.net/forum?id=OUz_9TiTv9j) |  | 0 |  | Hengrui Jia, Hongyu Chen, Jonas Guan, Ali Shahin Shamsabadi, Nicolas Papernot |  |
| 298 |  |  [Meta-Imitation Learning by Watching Video Demonstrations](https://openreview.net/forum?id=KTPuIsx4pmo) |  | 0 |  | Jiayi Li, Tao Lu, Xiaoge Cao, Yinghao Cai, Shuo Wang |  |
| 299 |  |  [Understanding Intrinsic Robustness Using Label Uncertainty](https://openreview.net/forum?id=6ET9SzlgNX) |  | 0 |  | Xiao Zhang, David E. Evans |  |
| 300 |  |  [Efficient Split-Mix Federated Learning for On-Demand and In-Situ Customization](https://openreview.net/forum?id=_QLmakITKg) |  | 0 |  | Junyuan Hong, Haotao Wang, Zhangyang Wang, Jiayu Zhou |  |
| 301 |  |  [Anti-Concentrated Confidence Bonuses For Scalable Exploration](https://openreview.net/forum?id=RXQ-FPbQYVn) |  | 0 |  | Jordan T. Ash, Cyril Zhang, Surbhi Goel, Akshay Krishnamurthy, Sham M. Kakade |  |
| 302 |  |  [Sqrt(d) Dimension Dependence of Langevin Monte Carlo](https://openreview.net/forum?id=5-2mX9_U5i) |  | 0 |  | Ruilin Li, Hongyuan Zha, Molei Tao |  |
| 303 |  |  [Relational Surrogate Loss Learning](https://openreview.net/forum?id=dZPgfwaTaXv) |  | 0 |  | Tao Huang, Zekang Li, Hua Lu, Yong Shan, Shusheng Yang, Yang Feng, Fei Wang, Shan You, Chang Xu |  |
| 304 |  |  [Structure-Aware Transformer Policy for Inhomogeneous Multi-Task Reinforcement Learning](https://openreview.net/forum?id=fy_XRVHqly) |  | 0 |  | Sunghoon Hong, Deunsol Yoon, KeeEung Kim |  |
| 305 |  |  [Toward Efficient Low-Precision Training: Data Format Optimization and Hysteresis Quantization](https://openreview.net/forum?id=3HJOA-1hb0e) |  | 0 |  | Sunwoo Lee, Jeongwoo Park, Dongsuk Jeon |  |
| 306 |  |  [Knowledge Infused Decoding](https://openreview.net/forum?id=upnDJ7itech) |  | 0 |  | Ruibo Liu, Guoqing Zheng, Shashank Gupta, Radhika Gaonkar, Chongyang Gao, Soroush Vosoughi, Milad Shokouhi, Ahmed Hassan Awadallah |  |
| 307 |  |  [Parallel Training of GRU Networks with a Multi-Grid Solver for Long Sequences](https://openreview.net/forum?id=N1WI0vJLER) |  | 0 |  | Euhyun Moon, Eric C. Cyr |  |
| 308 |  |  [Query Efficient Decision Based Sparse Attacks Against Black-Box Deep Learning Models](https://openreview.net/forum?id=73MEhZ0anV) |  | 0 |  | Viet Quoc Vo, Ehsan Abbasnejad, Damith Ranasinghe |  |
| 309 |  |  [Almost Tight L0-norm Certified Robustness of Top-k Predictions against Adversarial Perturbations](https://openreview.net/forum?id=gJLEXy3ySpu) |  | 0 |  | Jinyuan Jia, Binghui Wang, Xiaoyu Cao, Hongbin Liu, Neil Zhenqiang Gong |  |
| 310 |  |  [Proving the Lottery Ticket Hypothesis for Convolutional Neural Networks](https://openreview.net/forum?id=Vjki79-619-) |  | 0 |  | Arthur da Cunha, Emanuele Natale, Laurent Viennot |  |
| 311 |  |  [Discovering Nonlinear PDEs from Scarce Data with Physics-encoded Learning](https://openreview.net/forum?id=Vog_3GXsgmb) |  | 0 |  | Chengping Rao, Pu Ren, Yang Liu, Hao Sun |  |
| 312 |  |  [Rethinking Goal-Conditioned Supervised Learning and Its Connection to Offline RL](https://openreview.net/forum?id=KJztlfGPdwW) |  | 0 |  | Rui Yang, Yiming Lu, Wenzhe Li, Hao Sun, Meng Fang, Yali Du, Xiu Li, Lei Han, Chongjie Zhang |  |
| 313 |  |  [Topologically Regularized Data Embeddings](https://openreview.net/forum?id=P1QUVhOtEFP) |  | 0 |  | Robin Vandaele, Bo Kang, Jefrey Lijffijt, Tijl De Bie, Yvan Saeys |  |
| 314 |  |  [PF-GNN: Differentiable particle filtering based approximation of universal graph representations](https://openreview.net/forum?id=oh4TirnfSem) |  | 0 |  | Mohammed Haroon Dupty, Yanfei Dong, Wee Sun Lee |  |
| 315 |  |  [Nonlinear ICA Using Volume-Preserving Transformations](https://openreview.net/forum?id=AMpki9kp8Cn) |  | 0 |  | Xiaojiang Yang, Yi Wang, Jiacheng Sun, Xing Zhang, Shifeng Zhang, Zhenguo Li, Junchi Yan |  |
| 316 |  |  [Online Ad Hoc Teamwork under Partial Observability](https://openreview.net/forum?id=18Ys0-PzyPI) |  | 0 |  | Pengjie Gu, Mengchen Zhao, Jianye Hao, Bo An |  |
| 317 |  |  [Continual Normalization: Rethinking Batch Normalization for Online Continual Learning](https://openreview.net/forum?id=vwLLQ-HwqhZ) |  | 0 |  | Quang Pham, Chenghao Liu, Steven C. H. Hoi |  |
| 318 |  |  [Equivariant Graph Mechanics Networks with Constraints](https://openreview.net/forum?id=SHbhHHfePhP) |  | 0 |  | Wenbing Huang, Jiaqi Han, Yu Rong, Tingyang Xu, Fuchun Sun, Junzhou Huang |  |
| 319 |  |  [Towards Continual Knowledge Learning of Language Models](https://openreview.net/forum?id=vfsRB5MImo9) |  | 0 |  | Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Stanley Jungkyu Choi, Minjoon Seo |  |
| 320 |  |  [Surreal-GAN: Semi-Supervised Representation Learning via GAN for uncovering heterogeneous disease-related imaging patterns](https://openreview.net/forum?id=nf3A0WZsXS5) |  | 0 |  | Zhijian Yang, Junhao Wen, Christos Davatzikos |  |
| 321 |  |  [SURF: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning](https://openreview.net/forum?id=TfhfZLQ2EJO) |  | 0 |  | Jongjin Park, Younggyo Seo, Jinwoo Shin, Honglak Lee, Pieter Abbeel, Kimin Lee |  |
| 322 |  |  [Convergent Graph Solvers](https://openreview.net/forum?id=ItkxLQU01lD) |  | 0 |  | Junyoung Park, Jinhyun Choo, Jinkyoo Park |  |
| 323 |  |  [Spread Spurious Attribute: Improving Worst-group Accuracy with Spurious Attribute Estimation](https://openreview.net/forum?id=_F9xpOrqyX9) |  | 0 |  | Jun Hyun Nam, Jaehyung Kim, Jaeho Lee, Jinwoo Shin |  |
| 324 |  |  [Learning Scenario Representation for Solving Two-stage Stochastic Integer Programs](https://openreview.net/forum?id=06Wy2BtxXrz) |  | 0 |  | Yaoxin Wu, Wen Song, Zhiguang Cao, Jie Zhang |  |
| 325 |  |  [Generalization Through the Lens of Leave-One-Out Error](https://openreview.net/forum?id=7grkzyj89A_) |  | 0 |  | Gregor Bachmann, Thomas Hofmann, Aurélien Lucchi |  |
| 326 |  |  [Self-Supervised Inference in State-Space Models](https://openreview.net/forum?id=VPjw9KPWRSK) |  | 0 |  | David Ruhe, Patrick Forré |  |
| 327 |  |  [On the Role of Neural Collapse in Transfer Learning](https://openreview.net/forum?id=SwIp410B6aQ) |  | 0 |  | Tomer Galanti, András György, Marcus Hutter |  |
| 328 |  |  [Information-theoretic Online Memory Selection for Continual Learning](https://openreview.net/forum?id=IpctgL7khPp) |  | 0 |  | Shengyang Sun, Daniele Calandriello, Huiyi Hu, Ang Li, Michalis K. Titsias |  |
| 329 |  |  [Dealing with Non-Stationarity in MARL via Trust-Region Decomposition](https://openreview.net/forum?id=XHUxf5aRB3s) |  | 0 |  | Wenhao Li, Xiangfeng Wang, Bo Jin, Junjie Sheng, Hongyuan Zha |  |
| 330 |  |  [Information Bottleneck: Exact Analysis of (Quantized) Neural Networks](https://openreview.net/forum?id=kF9DZQQrU0w) |  | 0 |  | Stephan Sloth Lorenzen, Christian Igel, Mads Nielsen |  |
| 331 |  |  [GLASS: GNN with Labeling Tricks for Subgraph Representation Learning](https://openreview.net/forum?id=XLxhEjKNbXj) |  | 0 |  | Xiyuan Wang, Muhan Zhang |  |
| 332 |  |  [MoReL: Multi-omics Relational Learning](https://openreview.net/forum?id=DnG75_KyHjX) |  | 0 |  | Arman Hasanzadeh, Ehsan Hajiramezanali, Nick Duffield, Xiaoning Qian |  |
| 333 |  |  [Provable Learning-based Algorithm For Sparse Recovery](https://openreview.net/forum?id=BwPaPxwgyQb) |  | 0 |  | Xinshi Chen, Haoran Sun, Le Song |  |
| 334 |  |  [Defending Against Image Corruptions Through Adversarial Augmentations](https://openreview.net/forum?id=jJOjjiZHy3h) |  | 0 |  | Dan Andrei Calian, Florian Stimberg, Olivia Wiles, SylvestreAlvise Rebuffi, András György, Timothy A. Mann, Sven Gowal |  |
| 335 |  |  [Attacking deep networks with surrogate-based adversarial black-box methods is easy](https://openreview.net/forum?id=Zf4ZdI4OQPV) |  | 0 |  | Nicholas A. Lord, Romain Müller, Luca Bertinetto |  |
| 336 |  |  [Autoregressive Diffusion Models](https://openreview.net/forum?id=Lm8T39vLDTE) |  | 0 |  | Emiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, Tim Salimans |  |
| 337 |  |  [Auto-scaling Vision Transformers without Training](https://openreview.net/forum?id=H94a1_Pyr-6) |  | 0 |  | Wuyang Chen, Wei Huang, Xianzhi Du, Xiaodan Song, Zhangyang Wang, Denny Zhou |  |
| 338 |  |  [Fine-grained Differentiable Physics: A Yarn-level Model for Fabrics](https://openreview.net/forum?id=KPEFXR1HdIo) |  | 0 |  | Deshan Gong, Zhanxing Zhu, Andrew J. Bulpitt, He Wang |  |
| 339 |  |  [Revisiting flow generative models for Out-of-distribution detection](https://openreview.net/forum?id=6y2KBh-0Fd9) |  | 0 |  | Dihong Jiang, Sun Sun, Yaoliang Yu |  |
| 340 |  |  [Missingness Bias in Model Debugging](https://openreview.net/forum?id=Te5ytkqsnl) |  | 0 |  | Saachi Jain, Hadi Salman, Eric Wong, Pengchuan Zhang, Vibhav Vineet, Sai Vemprala, Aleksander Madry |  |
| 341 |  |  [Meta Learning Low Rank Covariance Factors for Energy Based Deterministic Uncertainty](https://openreview.net/forum?id=GQd7mXSPua) |  | 0 |  | Jeffrey Ryan Willette, Hae Beom Lee, Juho Lee, Sung Ju Hwang |  |
| 342 |  |  [Conditional Object-Centric Learning from Video](https://openreview.net/forum?id=aD7uesX1GF_) |  | 0 |  | Thomas Kipf, Gamaleldin Fathy Elsayed, Aravindh Mahendran, Austin Stone, Sara Sabour, Georg Heigold, Rico Jonschkowski, Alexey Dosovitskiy, Klaus Greff |  |
| 343 |  |  [Scale Efficiently: Insights from Pretraining and Finetuning Transformers](https://openreview.net/forum?id=f2OYVDyfIB) |  | 0 |  | Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, Donald Metzler |  |
| 344 |  |  [Vitruvion: A Generative Model of Parametric CAD Sketches](https://openreview.net/forum?id=Ow1C7s3UcY) |  | 0 |  | Ari Seff, Wenda Zhou, Nick Richardson, Ryan P. Adams |  |
| 345 |  |  [Space-Time Graph Neural Networks](https://openreview.net/forum?id=XJiajt89Omg) |  | 0 |  | Samar Hadou, Charilaos I. Kanatsoulis, Alejandro Ribeiro |  |
| 346 |  |  [Scattering Networks on the Sphere for Scalable and Rotationally Equivariant Spherical CNNs](https://openreview.net/forum?id=bjy5Zb2fo2) |  | 0 |  | Jason D. McEwen, Christopher G. R. Wallis, Augustine N. MavorParker |  |
| 347 |  |  [Bayesian Neural Network Priors Revisited](https://openreview.net/forum?id=xkjqJYqRJy) |  | 0 |  | Vincent Fortuin, Adrià GarrigaAlonso, Sebastian W. Ober, Florian Wenzel, Gunnar Rätsch, Richard E. Turner, Mark van der Wilk, Laurence Aitchison |  |
| 348 |  |  [Goal-Directed Planning via Hindsight Experience Replay](https://openreview.net/forum?id=6NePxZwfae) |  | 0 |  | Lorenzo Moro, Amarildo Likmeta, Enrico Prati, Marcello Restelli |  |
| 349 |  |  [Hybrid Random Features](https://openreview.net/forum?id=EMigfE6ZeS) |  | 0 |  | Krzysztof Marcin Choromanski, Han Lin, Haoxian Chen, Arijit Sehanobish, Yuanzhe Ma, Deepali Jain, Jake Varley, Andy Zeng, Michael S. Ryoo, Valerii Likhosherstov, Dmitry Kalashnikov, Vikas Sindhwani, Adrian Weller |  |
| 350 |  |  [Pretrained Language Model in Continual Learning: A Comparative Study](https://openreview.net/forum?id=figzpGMrdD) |  | 0 |  | Tongtong Wu, Massimo Caccia, Zhuang Li, YuanFang Li, Guilin Qi, Gholamreza Haffari |  |
| 351 |  |  [Salient ImageNet: How to discover spurious features in Deep Learning?](https://openreview.net/forum?id=XVPqLyNxSyh) |  | 0 |  | Sahil Singla, Soheil Feizi |  |
| 352 |  |  [Differentiable DAG Sampling](https://openreview.net/forum?id=9wOQOgNe-w) |  | 0 |  | Bertrand Charpentier, Simon Kibler, Stephan Günnemann |  |
| 353 |  |  [Evaluating Model-Based Planning and Planner Amortization for Continuous Control](https://openreview.net/forum?id=SS8F6tFX3-) |  | 0 |  | Arunkumar Byravan, Leonard Hasenclever, Piotr Trochim, Mehdi Mirza, Alessandro Davide Ialongo, Yuval Tassa, Jost Tobias Springenberg, Abbas Abdolmaleki, Nicolas Heess, Josh Merel, Martin A. Riedmiller |  |
| 354 |  |  [Hierarchical Few-Shot Imitation with Skill Transition Models](https://openreview.net/forum?id=xKZ4K0lTj_) |  | 0 |  | Kourosh Hakhamaneshi, Ruihan Zhao, Albert Zhan, Pieter Abbeel, Michael Laskin |  |
| 355 |  |  [End-to-End Learning of Probabilistic Hierarchies on Graphs](https://openreview.net/forum?id=g2LCQwG7Of) |  | 0 |  | Daniel Zügner, Bertrand Charpentier, Morgane Ayle, Sascha Geringer, Stephan Günnemann |  |
| 356 |  |  [GeneDisco: A Benchmark for Experimental Design in Drug Discovery](https://openreview.net/forum?id=-w2oomO6qgc) |  | 0 |  | Arash Mehrjou, Ashkan Soleymani, Andrew Jesson, Pascal Notin, Yarin Gal, Stefan Bauer, Patrick Schwab |  |
| 357 |  |  [GraphENS: Neighbor-Aware Ego Network Synthesis for Class-Imbalanced Node Classification](https://openreview.net/forum?id=MXEl7i-iru) |  | 0 |  | Joonhyung Park, Jaeyun Song, Eunho Yang |  |
| 358 |  |  [Continuously Discovering Novel Strategies via Reward-Switching Policy Optimization](https://openreview.net/forum?id=hcQHRHKfN_) |  | 0 |  | Zihan Zhou, Wei Fu, Bingliang Zhang, Yi Wu |  |
| 359 |  |  [Learning to Remember Patterns: Pattern Matching Memory Networks for Traffic Forecasting](https://openreview.net/forum?id=wwDg3bbYBIq) |  | 0 |  | Hyunwook Lee, Seungmin Jin, Hyeshin Chu, Hongkyu Lim, Sungahn Ko |  |
| 360 |  |  [Why Propagate Alone? Parallel Use of Labels and Features on Graphs](https://openreview.net/forum?id=VTNjxbFRKly) |  | 0 |  | Yangkun Wang, Jiarui Jin, Weinan Zhang, Yongyi Yang, Jiuhai Chen, Quan Gan, Yong Yu, Zheng Zhang, Zengfeng Huang, David Wipf |  |
| 361 |  |  [Learning by Directional Gradient Descent](https://openreview.net/forum?id=5i7lJLuhTm) |  | 0 |  | David Silver, Anirudh Goyal, Ivo Danihelka, Matteo Hessel, Hado van Hasselt |  |
| 362 |  |  [Maximum Entropy RL (Provably) Solves Some Robust RL Problems](https://openreview.net/forum?id=PtSAD3caaA2) |  | 0 |  | Benjamin Eysenbach, Sergey Levine |  |
| 363 |  |  [A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training](https://openreview.net/forum?id=XhF2VOMRHS) |  | 0 |  | Yifei Wang, Yisen Wang, Jiansheng Yang, Zhouchen Lin |  |
| 364 |  |  [Equivariant and Stable Positional Encoding for More Powerful Graph Neural Networks](https://openreview.net/forum?id=e95i1IHcWj) |  | 0 |  | Haorui Wang, Haoteng Yin, Muhan Zhang, Pan Li |  |
| 365 |  |  [BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models](https://openreview.net/forum?id=Mng8CQ9eBW) |  | 0 |  | Kangjie Chen, Yuxian Meng, Xiaofei Sun, Shangwei Guo, Tianwei Zhang, Jiwei Li, Chun Fan |  |
| 366 |  |  [Shallow and Deep Networks are Near-Optimal Approximators of Korobov Functions](https://openreview.net/forum?id=AV8FPoMTTa) |  | 0 |  | Moïse Blanchard, Mohammed Amine Bennouna |  |
| 367 |  |  [What Makes Better Augmentation Strategies? Augment Difficult but Not too Different](https://openreview.net/forum?id=Ucx3DQbC9GH) |  | 0 |  | Jaehyung Kim, Dongyeop Kang, Sungsoo Ahn, Jinwoo Shin |  |
| 368 |  |  [Generative Pseudo-Inverse Memory](https://openreview.net/forum?id=Harn4_EZBw) |  | 0 |  | Kha Pham, Hung Le, Man Ngo, Truyen Tran, Bao Ho, Svetha Venkatesh |  |
| 369 |  |  [A Deep Variational Approach to Clustering Survival Data](https://openreview.net/forum?id=RQ428ZptQfU) |  | 0 |  | Laura Manduchi, Ricards Marcinkevics, Michela Carlotta Massi, Thomas J. Weikert, Alexander Sauter, Verena Gotta, Timothy Müller, Flavio Vasella, Marian C. Neidert, Marc Pfister, Bram Stieltjes, Julia E. Vogt |  |
| 370 |  |  [GPT-Critic: Offline Reinforcement Learning for End-to-End Task-Oriented Dialogue Systems](https://openreview.net/forum?id=qaxhBG1UUaS) |  | 0 |  | Youngsoo Jang, Jongmin Lee, KeeEung Kim |  |
| 371 |  |  [Charformer: Fast Character Transformers via Gradient-based Subword Tokenization](https://openreview.net/forum?id=JtBRnrlOEFN) |  | 0 |  | Yi Tay, Vinh Q. Tran, Sebastian Ruder, Jai Prakash Gupta, Hyung Won Chung, Dara Bahri, Zhen Qin, Simon Baumgartner, Cong Yu, Donald Metzler |  |
| 372 |  |  [Regularized Autoencoders for Isometric Representation Learning](https://openreview.net/forum?id=mQxt8l7JL04) |  | 0 |  | Yonghyeon Lee, Sangwoong Yoon, Minjun Son, Frank Chongwoo Park |  |
| 373 |  |  [Knowledge Removal in Sampling-based Bayesian Inference](https://openreview.net/forum?id=dTqOcTUOQO) |  | 0 |  | Shaopeng Fu, Fengxiang He, Dacheng Tao |  |
| 374 |  |  [Actor-critic is implicitly biased towards high entropy optimal policies](https://openreview.net/forum?id=vEZyTBRPP6o) |  | 0 |  | Yuzheng Hu, Ziwei Ji, Matus Telgarsky |  |
| 375 |  |  [Igeood: An Information Geometry Approach to Out-of-Distribution Detection](https://openreview.net/forum?id=mfwdY3U_9ea) |  | 0 |  | Eduardo Dadalto Câmara Gomes, Florence Alberge, Pierre Duhamel, Pablo Piantanida |  |
| 376 |  |  [Bag of Instances Aggregation Boosts Self-supervised Distillation](https://openreview.net/forum?id=N0uJGWDw21d) |  | 0 |  | Haohang Xu, Jiemin Fang, Xiaopeng Zhang, Lingxi Xie, Xinggang Wang, Wenrui Dai, Hongkai Xiong, Qi Tian |  |
| 377 |  |  [Stability Regularization for Discrete Representation Learning](https://openreview.net/forum?id=6tmjoym9LR6) |  | 0 |  | Adeel Pervez, Efstratios Gavves |  |
| 378 |  |  [Unrolling PALM for Sparse Semi-Blind Source Separation](https://openreview.net/forum?id=aBVxf5NaaRt) |  | 0 |  | Mohammad Fahes, Christophe Kervazo, Jérôme Bobin, Florence Tupin |  |
| 379 |  |  [Fast Generic Interaction Detection for Model Interpretability and Compression](https://openreview.net/forum?id=fQTlgI2qZqE) |  | 0 |  | Tianjian Zhang, Feng Yin, ZhiQuan Luo |  |
| 380 |  |  [Reversible Instance Normalization for Accurate Time-Series Forecasting against Distribution Shift](https://openreview.net/forum?id=cGDAkQo1C0p) |  | 0 |  | Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, JangHo Choi, Jaegul Choo |  |
| 381 |  |  [On the Pitfalls of Analyzing Individual Neurons in Language Models](https://openreview.net/forum?id=8uz0EWPQIMu) |  | 0 |  | Omer Antverg, Yonatan Belinkov |  |
| 382 |  |  [Query Embedding on Hyper-Relational Knowledge Graphs](https://openreview.net/forum?id=4rLw09TgRw9) |  | 0 |  | Dimitrios Alivanistos, Max Berrendorf, Michael Cochez, Mikhail Galkin |  |
| 383 |  |  [Neural Solvers for Fast and Accurate Numerical Optimal Control](https://openreview.net/forum?id=m8bypnj7Yl5) |  | 0 |  | Federico Berto, Stefano Massaroli, Michael Poli, Jinkyoo Park |  |
| 384 |  |  [PSA-GAN: Progressive Self Attention GANs for Synthetic Time Series](https://openreview.net/forum?id=Ix_mh42xq5w) |  | 0 |  | Paul Jeha, Michael BohlkeSchneider, Pedro Mercado, Shubham Kapoor, RajbirSingh Nirwan, Valentin Flunkert, Jan Gasthaus, Tim Januschowski |  |
| 385 |  |  [ToM2C: Target-oriented Multi-agent Communication and Cooperation with Theory of Mind](https://openreview.net/forum?id=2t7CkQXNpuq) |  | 0 |  | Yuanfei Wang, Fangwei Zhong, Jing Xu, Yizhou Wang |  |
| 386 |  |  [Better Supervisory Signals by Observing Learning Paths](https://openreview.net/forum?id=Iog0djAdbHj) |  | 0 |  | Yi Ren, Shangmin Guo, Danica J. Sutherland |  |
| 387 |  |  [TAda! Temporally-Adaptive Convolutions for Video Understanding](https://openreview.net/forum?id=izj68lUcBpt) |  | 0 |  | Ziyuan Huang, Shiwei Zhang, Liang Pan, Zhiwu Qing, Mingqian Tang, Ziwei Liu, Marcelo H. Ang Jr. |  |
| 388 |  |  [Learning a subspace of policies for online adaptation in Reinforcement Learning](https://openreview.net/forum?id=4Muj-t_4o4) |  | 0 |  | JeanBaptiste Gaya, Laure Soulier, Ludovic Denoyer |  |
| 389 |  |  [ZeroFL: Efficient On-Device Training for Federated Learning with Local Sparsity](https://openreview.net/forum?id=2sDQwC_hmnM) |  | 0 |  | Xinchi Qiu, Javier FernándezMarqués, Pedro P. B. de Gusmao, Yan Gao, Titouan Parcollet, Nicholas Donald Lane |  |
| 390 |  |  [Gaussian Mixture Convolution Networks](https://openreview.net/forum?id=Oxeka7Z7Hor) |  | 0 |  | Adam Celarek, Pedro Hermosilla, Bernhard Kerbl, Timo Ropinski, Michael Wimmer |  |
| 391 |  |  [How Does SimSiam Avoid Collapse Without Negative Samples? A Unified Understanding with Self-supervised Contrastive Learning](https://openreview.net/forum?id=bwq6O4Cwdl) |  | 0 |  | Chaoning Zhang, Kang Zhang, Chenshuang Zhang, Trung X. Pham, Chang D. Yoo, In So Kweon |  |
| 392 |  |  [Attention-based Interpretability with Concept Transformers](https://openreview.net/forum?id=kAa9eDS0RdO) |  | 0 |  | Mattia Rigotti, Christoph Miksovic, Ioana Giurgiu, Thomas Gschwind, Paolo Scotton |  |
| 393 |  |  [Inductive Relation Prediction Using Analogy Subgraph Embeddings](https://openreview.net/forum?id=PTRo58zPt3P) |  | 0 |  | Jiarui Jin, Yangkun Wang, Kounianhua Du, Weinan Zhang, Zheng Zhang, David Wipf, Yong Yu, Quan Gan |  |
| 394 |  |  [Reinforcement Learning in Presence of Discrete Markovian Context Evolution](https://openreview.net/forum?id=CmsfC7u054S) |  | 0 |  | Hang Ren, Aivar Sootla, Taher Jafferjee, Junxiao Shen, Jun Wang, Haitham BouAmmar |  |
| 395 |  |  [Optimal Transport for Long-Tailed Recognition with Learnable Cost Matrix](https://openreview.net/forum?id=t98k9ePQQpn) |  | 0 |  | Hanyu Peng, Mingming Sun, Ping Li |  |
| 396 |  |  [PriorGrad: Improving Conditional Denoising Diffusion Models with Data-Dependent Adaptive Prior](https://openreview.net/forum?id=_BNiN4IjC5) |  | 0 |  | Sanggil Lee, Heeseung Kim, Chaehun Shin, Xu Tan, Chang Liu, Qi Meng, Tao Qin, Wei Chen, Sungroh Yoon, TieYan Liu |  |
| 397 |  |  [Target-Side Input Augmentation for Sequence to Sequence Generation](https://openreview.net/forum?id=pz1euXohm4H) |  | 0 |  | Shufang Xie, Ang Lv, Yingce Xia, Lijun Wu, Tao Qin, TieYan Liu, Rui Yan |  |
| 398 |  |  [UniFormer: Unified Transformer for Efficient Spatial-Temporal Representation Learning](https://openreview.net/forum?id=nBU_u6DLvoK) |  | 0 |  | Kunchang Li, Yali Wang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, Yu Qiao |  |
| 399 |  |  [Inverse Online Learning: Understanding Non-Stationary and Reactionary Policies](https://openreview.net/forum?id=DYypjaRdph2) |  | 0 |  | Alex J. Chan, Alicia Curth, Mihaela van der Schaar |  |
| 400 |  |  [Multi-Mode Deep Matrix and Tensor Factorization](https://openreview.net/forum?id=6YVIk0sAkF_) |  | 0 |  | Jicong Fan |  |
| 401 |  |  [LORD: Lower-Dimensional Embedding of Log-Signature in Neural Rough Differential Equations](https://openreview.net/forum?id=fCG75wd39ze) |  | 0 |  | Jaehoon Lee, Jinsung Jeon, Sheo Yon Jhin, Jihyeon Hyeong, Jayoung Kim, Minju Jo, Seungji Kook, Noseong Park |  |
| 402 |  |  [Generalized Natural Gradient Flows in Hidden Convex-Concave Games and GANs](https://openreview.net/forum?id=bsycpMi00R1) |  | 0 |  | Andjela Mladenovic, Iosif Sakos, Gauthier Gidel, Georgios Piliouras |  |
| 403 |  |  [Offline Neural Contextual Bandits: Pessimism, Optimization and Generalization](https://openreview.net/forum?id=sPIFuucA3F) |  | 0 |  | Thanh NguyenTang, Sunil Gupta, A. Tuan Nguyen, Svetha Venkatesh |  |
| 404 |  |  [THOMAS: Trajectory Heatmap Output with learned Multi-Agent Sampling](https://openreview.net/forum?id=QDdJhACYrlX) |  | 0 |  | Thomas Gilles, Stefano Sabatini, Dzmitry Tsishkou, Bogdan Stanciulescu, Fabien Moutarde |  |
| 405 |  |  [CLEVA-Compass: A Continual Learning Evaluation Assessment Compass to Promote Research Transparency and Comparability](https://openreview.net/forum?id=rHMaBYbkkRJ) |  | 0 |  | Martin Mundt, Steven Lang, Quentin Delfosse, Kristian Kersting |  |
| 406 |  |  [Neural Stochastic Dual Dynamic Programming](https://openreview.net/forum?id=aisKPsMM3fg) |  | 0 |  | Hanjun Dai, Yuan Xue, Zia Syed, Dale Schuurmans, Bo Dai |  |
| 407 |  |  [DemoDICE: Offline Imitation Learning with Supplementary Imperfect Demonstrations](https://openreview.net/forum?id=BrPdX1bDZkQ) |  | 0 |  | GeonHyeong Kim, Seokin Seo, Jongmin Lee, Wonseok Jeon, HyeongJoo Hwang, Hongseok Yang, KeeEung Kim |  |
| 408 |  |  [Learning to Extend Molecular Scaffolds with Structural Motifs](https://openreview.net/forum?id=ZTsoE8G3GG) |  | 0 |  | Krzysztof Maziarz, Henry Richard JacksonFlux, Pashmina Cameron, Finton Sirockin, Nadine Schneider, Nikolaus Stiefl, Marwin H. S. Segler, Marc Brockschmidt |  |
| 409 |  |  [Discrepancy-Based Active Learning for Domain Adaptation](https://openreview.net/forum?id=p98WJxUC3Ca) |  | 0 |  | Antoine de Mathelin, François Deheeger, Mathilde Mougeot, Nicolas Vayatis |  |
| 410 |  |  [Gradient Matching for Domain Generalization](https://openreview.net/forum?id=vDwBW49HmO) |  | 0 |  | Yuge Shi, Jeffrey Seely, Philip H. S. Torr, Siddharth Narayanaswamy, Awni Y. Hannun, Nicolas Usunier, Gabriel Synnaeve |  |
| 411 |  |  [Objects in Semantic Topology](https://openreview.net/forum?id=d5SCUJ5t1k) |  | 0 |  | Shuo Yang, Peize Sun, Yi Jiang, Xiaobo Xia, Ruiheng Zhang, Zehuan Yuan, Changhu Wang, Ping Luo, Min Xu |  |
| 412 |  |  [Hidden Parameter Recurrent State Space Models For Changing Dynamics Scenarios](https://openreview.net/forum?id=ds8yZOUsea) |  | 0 |  | Vaisakh Shaj, Dieter Büchler, Rohit Sonker, Philipp Becker, Gerhard Neumann |  |
| 413 |  |  [Graph Neural Network Guided Local Search for the Traveling Salesperson Problem](https://openreview.net/forum?id=ar92oEosBIg) |  | 0 |  | Benjamin Hudson, Qingbiao Li, Matthew Malencia, Amanda Prorok |  |
| 414 |  |  [On the Pitfalls of Heteroscedastic Uncertainty Estimation with Probabilistic Neural Networks](https://openreview.net/forum?id=aPOpXlnV1T) |  | 0 |  | Maximilian Seitzer, Arash Tavakoli, Dimitrije Antic, Georg Martius |  |
| 415 |  |  [Label-Efficient Semantic Segmentation with Diffusion Models](https://openreview.net/forum?id=SlxSY2UZQT) |  | 0 |  | Dmitry Baranchuk, Andrey Voynov, Ivan Rubachev, Valentin Khrulkov, Artem Babenko |  |
| 416 |  |  [Language model compression with weighted low-rank factorization](https://openreview.net/forum?id=uPv9Y3gmAI5) |  | 0 |  | YenChang Hsu, Ting Hua, Sungen Chang, Qian Lou, Yilin Shen, Hongxia Jin |  |
| 417 |  |  [Pareto Set Learning for Neural Multi-Objective Combinatorial Optimization](https://openreview.net/forum?id=QuObT9BTWo) |  | 0 |  | Xi Lin, Zhiyuan Yang, Qingfu Zhang |  |
| 418 |  |  [Prototypical Contrastive Predictive Coding](https://openreview.net/forum?id=8la28hZOwug) |  | 0 |  | Kyungmin Lee |  |
| 419 |  |  [Adversarial Robustness Through the Lens of Causality](https://openreview.net/forum?id=cZAi1yWpiXQ) |  | 0 |  | Yonggang Zhang, Mingming Gong, Tongliang Liu, Gang Niu, Xinmei Tian, Bo Han, Bernhard Schölkopf, Kun Zhang |  |
| 420 |  |  [Distributionally Robust Fair Principal Components via Geodesic Descents](https://openreview.net/forum?id=9NVd-DMtThY) |  | 0 |  | Hieu Vu, Toan Tran, ManChung Yue, Viet Anh Nguyen |  |
| 421 |  |  [Understanding and Improving Graph Injection Attack by Promoting Unnoticeability](https://openreview.net/forum?id=wkMG8cdvh7-) |  | 0 |  | Yongqiang Chen, Han Yang, Yonggang Zhang, Kaili Ma, Tongliang Liu, Bo Han, James Cheng |  |
| 422 |  |  [Learning to Guide and to be Guided in the Architect-Builder Problem](https://openreview.net/forum?id=swiyAeGzFhQ) |  | 0 |  | Paul Barde, Tristan Karch, Derek Nowrouzezahrai, Clément MoulinFrier, Christopher Pal, PierreYves Oudeyer |  |
| 423 |  |  [Phase Collapse in Neural Networks](https://openreview.net/forum?id=iPHLcmtietq) |  | 0 |  | Florentin Guth, John Zarka, Stéphane Mallat |  |
| 424 |  |  [SPIRAL: Self-supervised Perturbation-Invariant Representation Learning for Speech Pre-Training](https://openreview.net/forum?id=TBpg4PnXhYH) |  | 0 |  | Wenyong Huang, Zhenhe Zhang, Yu Ting Yeung, Xin Jiang, Qun Liu |  |
| 425 |  |  [Improving the Accuracy of Learning Example Weights for Imbalance Classification](https://openreview.net/forum?id=J_PHjw4gvXJ) |  | 0 |  | Yuqi Liu, Bin Cao, Jing Fan |  |
| 426 |  |  [Generating Videos with Dynamics-aware Implicit Generative Adversarial Networks](https://openreview.net/forum?id=Czsdv-S4-w9) |  | 0 |  | Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, JungWoo Ha, Jinwoo Shin |  |
| 427 |  |  [Efficient Learning of Safe Driving Policy via Human-AI Copilot Optimization](https://openreview.net/forum?id=0cgU-BZp2ky) |  | 0 |  | Quanyi Li, Zhenghao Peng, Bolei Zhou |  |
| 428 |  |  [Enhancing Cross-lingual Transfer by Manifold Mixup](https://openreview.net/forum?id=OjPmfr9GkVv) |  | 0 |  | Huiyun Yang, Huadong Chen, Hao Zhou, Lei Li |  |
| 429 |  |  [Evolutionary Diversity Optimization with Clustering-based Selection for Reinforcement Learning](https://openreview.net/forum?id=74x5BXs4bWD) |  | 0 |  | Yutong Wang, Ke Xue, Chao Qian |  |
| 430 |  |  [Curvature-Guided Dynamic Scale Networks for Multi-View Stereo](https://openreview.net/forum?id=_Wzj0J2xs2D) |  | 0 |  | Khang Truong Giang, Soohwan Song, Sungho Jo |  |
| 431 |  |  [Near-optimal Offline Reinforcement Learning with Linear Representation: Leveraging Variance Information with Pessimism](https://openreview.net/forum?id=KLaDXLAzzFT) |  | 0 |  | Ming Yin, Yaqi Duan, Mengdi Wang, YuXiang Wang |  |
| 432 |  |  [Exploring extreme parameter compression for pre-trained language models](https://openreview.net/forum?id=RftryyYyjiG) |  | 0 |  | Benyou Wang, Yuxin Ren, Lifeng Shang, Xin Jiang, Qun Liu |  |
| 433 |  |  [Local Feature Swapping for Generalization in Reinforcement Learning](https://openreview.net/forum?id=Sq0-tgDyHe4) |  | 0 |  | David Bertoin, Emmanuel Rachelson |  |
| 434 |  |  [Open-vocabulary Object Detection via Vision and Language Knowledge Distillation](https://openreview.net/forum?id=lL3lnMbR4WU) |  | 0 |  | Xiuye Gu, TsungYi Lin, Weicheng Kuo, Yin Cui |  |
| 435 |  |  [Model-Based Offline Meta-Reinforcement Learning with Regularization](https://openreview.net/forum?id=EBn0uInJZWh) |  | 0 |  | Sen Lin, Jialin Wan, Tengyu Xu, Yingbin Liang, Junshan Zhang |  |
| 436 |  |  [Scale Mixtures of Neural Network Gaussian Processes](https://openreview.net/forum?id=YVPBh4k78iZ) |  | 0 |  | Hyungi Lee, Eunggu Yun, Hongseok Yang, Juho Lee |  |
| 437 |  |  [A Johnson-Lindenstrauss Framework for Randomly Initialized CNNs](https://openreview.net/forum?id=YX0lrvdPQc) |  | 0 |  | Ido Nachum, Jan Hazla, Michael Gastpar, Anatoly Khina |  |
| 438 |  |  [Hindsight: Posterior-guided training of retrievers for improved open-ended generation](https://openreview.net/forum?id=Vr_BTpw3wz) |  | 0 |  | Ashwin Paranjape, Omar Khattab, Christopher Potts, Matei Zaharia, Christopher D. Manning |  |
| 439 |  |  [Self-Supervised Graph Neural Networks for Improved Electroencephalographic Seizure Analysis](https://openreview.net/forum?id=k9bx1EfHI_-) |  | 0 |  | Siyi Tang, Jared Dunnmon, Khaled Kamal Saab, Xuan Zhang, Qianying Huang, Florian Dubost, Daniel L. Rubin, Christopher LeeMesser |  |
| 440 |  |  [Group-based Interleaved Pipeline Parallelism for Large-scale DNN Training](https://openreview.net/forum?id=cw-EmNq5zfD) |  | 0 |  | Pengcheng Yang, Xiaoming Zhang, Wenpeng Zhang, Ming Yang, Hong Wei |  |
| 441 |  |  [Minimax Optimality (Probably) Doesn't Imply Distribution Learning for GANs](https://openreview.net/forum?id=nc0ETaieux) |  | 0 |  | Sitan Chen, Jerry Li, Yuanzhi Li, Raghu Meka |  |
| 442 |  |  [Offline Reinforcement Learning with Value-based Episodic Memory](https://openreview.net/forum?id=RCZqv9NXlZ) |  | 0 |  | Xiaoteng Ma, Yiqin Yang, Hao Hu, Jun Yang, Chongjie Zhang, Qianchuan Zhao, Bin Liang, Qihan Liu |  |
| 443 |  |  [MonoDistill: Learning Spatial Features for Monocular 3D Object Detection](https://openreview.net/forum?id=C54V-xTWfi) |  | 0 |  | Zhiyu Chong, Xinzhu Ma, Hong Zhang, Yuxin Yue, Haojie Li, Zhihui Wang, Wanli Ouyang |  |
| 444 |  |  [EXACT: Scalable Graph Neural Networks Training via Extreme Activation Compression](https://openreview.net/forum?id=vkaMaq95_rX) |  | 0 |  | Zirui Liu, Kaixiong Zhou, Fan Yang, Li Li, Rui Chen, Xia Hu |  |
| 445 |  |  [Provably convergent quasistatic dynamics for mean-field two-player zero-sum games](https://openreview.net/forum?id=MP904TiHqJ-) |  | 0 |  | Chao Ma, Lexing Ying |  |
| 446 |  |  [W-CTC: a Connectionist Temporal Classification Loss with Wild Cards](https://openreview.net/forum?id=0RqDp8FCW5Z) |  | 0 |  | Xingyu Cai, Jiahong Yuan, Yuchen Bian, Guangxu Xun, Jiaji Huang, Kenneth Church |  |
| 447 |  |  [Bandit Learning with Joint Effect of Incentivized Sampling, Delayed Sampling Feedback, and Self-Reinforcing User Preferences](https://openreview.net/forum?id=Q83vFlie_Pr) |  | 0 |  | Tianchen Zhou, Jia Liu, Chaosheng Dong, Yi Sun |  |
| 448 |  |  [AdaAug: Learning Class- and Instance-adaptive Data Augmentation Policies](https://openreview.net/forum?id=rWXfFogxRJN) |  | 0 |  | TszHim Cheung, DitYan Yeung |  |
| 449 |  |  [Unsupervised Semantic Segmentation by Distilling Feature Correspondences](https://openreview.net/forum?id=SaKO6z6Hl0c) |  | 0 |  | Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah Snavely, William T. Freeman |  |
| 450 |  |  [Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning](https://openreview.net/forum?id=TqNsv1TuCX9) |  | 0 |  | Mark Hamilton, Scott M. Lundberg, Stephanie Fu, Lei Zhang, William T. Freeman |  |
| 451 |  |  [Graph-Relational Domain Adaptation](https://openreview.net/forum?id=kcwyXtt7yDJ) |  | 0 |  | Zihao Xu, Hao He, GuangHe Lee, Bernie Wang, Hao Wang |  |
| 452 |  |  [Revisit Kernel Pruning with Lottery Regulated Grouped Convolutions](https://openreview.net/forum?id=LdEhiMG9WLO) |  | 0 |  | Shaochen (Henry) Zhong, Guanqun Zhang, Ningjia Huang, Shuai Xu |  |
| 453 |  |  [Bi-linear Value Networks for Multi-goal Reinforcement Learning](https://openreview.net/forum?id=LedObtLmCjS) |  | 0 |  | ZhangWei Hong, Ge Yang, Pulkit Agrawal |  |
| 454 |  |  [No One Representation to Rule Them All: Overlapping Features of Training Methods](https://openreview.net/forum?id=BK-4qbGgIE3) |  | 0 |  | Raphael Gontijo Lopes, Yann N. Dauphin, Ekin Dogus Cubuk |  |
| 455 |  |  [Generalized Kernel Thinning](https://openreview.net/forum?id=IfNu7Dr-3fQ) |  | 0 |  | Raaz Dwivedi, Lester Mackey |  |
| 456 |  |  [How Much Can CLIP Benefit Vision-and-Language Tasks?](https://openreview.net/forum?id=zf_Ll3HZWgy) |  | 0 |  | Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, KaiWei Chang, Zhewei Yao, Kurt Keutzer |  |
| 457 |  |  [Large Learning Rate Tames Homogeneity: Convergence and Balancing Effect](https://openreview.net/forum?id=3tbDrs77LJ5) |  | 0 |  | Yuqing Wang, Minshuo Chen, Tuo Zhao, Molei Tao |  |
| 458 |  |  [Demystifying Limited Adversarial Transferability in Automatic Speech Recognition Systems](https://openreview.net/forum?id=l5aSHXi8jG5) |  | 0 |  | Hadi Abdullah, Aditya Karlekar, Vincent Bindschaedler, Patrick Traynor |  |
| 459 |  |  [PipeGCN: Efficient Full-Graph Training of Graph Convolutional Networks with Pipelined Feature Communication](https://openreview.net/forum?id=kSwqMH0zn1F) |  | 0 |  | Cheng Wan, Youjie Li, Cameron R. Wolfe, Anastasios Kyrillidis, Nam Sung Kim, Yingyan Lin |  |
| 460 |  |  [Learning Neural Contextual Bandits through Perturbed Rewards](https://openreview.net/forum?id=7inCJ3MhXt3) |  | 0 |  | Yiling Jia, Weitong Zhang, Dongruo Zhou, Quanquan Gu, Hongning Wang |  |
| 461 |  |  [Adversarial Unlearning of Backdoors via Implicit Hypergradient](https://openreview.net/forum?id=MeeQkFYVbzW) |  | 0 |  | Yi Zeng, Si Chen, Won Park, Zhuoqing Mao, Ming Jin, Ruoxi Jia |  |
| 462 |  |  [Maximizing Ensemble Diversity in Deep Reinforcement Learning](https://openreview.net/forum?id=hjd-kcpDpf2) |  | 0 |  | Hassam Sheikh, Mariano Phielipp, Ladislau Bölöni |  |
| 463 |  |  [Graph Neural Networks with Learnable Structural and Positional Representations](https://openreview.net/forum?id=wTTjnvGphYj) |  | 0 |  | Vijay Prakash Dwivedi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, Xavier Bresson |  |
| 464 |  |  [Zero-Shot Self-Supervised Learning for MRI Reconstruction](https://openreview.net/forum?id=085y6YPaYjP) |  | 0 |  | Burhaneddin Yaman, Seyed Amir Hossein Hosseini, Mehmet Akçakaya |  |
| 465 |  |  [Policy Smoothing for Provably Robust Reinforcement Learning](https://openreview.net/forum?id=mwdfai8NBrJ) |  | 0 |  | Aounon Kumar, Alexander Levine, Soheil Feizi |  |
| 466 |  |  [The Close Relationship Between Contrastive Learning and Meta-Learning](https://openreview.net/forum?id=gICys3ITSmj) |  | 0 |  | Renkun Ni, Manli Shu, Hossein Souri, Micah Goldblum, Tom Goldstein |  |
| 467 |  |  [Towards Understanding Generalization via Decomposing Excess Risk Dynamics](https://openreview.net/forum?id=rS9-7AuPKWK) |  | 0 |  | Jiaye Teng, Jianhao Ma, Yang Yuan |  |
| 468 |  |  [Graph Auto-Encoder via Neighborhood Wasserstein Reconstruction](https://openreview.net/forum?id=ATUh28lnSuW) |  | 0 |  | Mingyue Tang, Pan Li, Carl Yang |  |
| 469 |  |  [FairCal: Fairness Calibration for Face Verification](https://openreview.net/forum?id=nRj0NcmSuxb) |  | 0 |  | Tiago Salvador, Stephanie Cairns, Vikram Voleti, Noah Marshall, Adam M. Oberman |  |
| 470 |  |  [Cross-Lingual Transfer with Class-Weighted Language-Invariant Representations](https://openreview.net/forum?id=k7-s5HSSPE5) |  | 0 |  | Ruicheng Xian, Heng Ji, Han Zhao |  |
| 471 |  |  [ComPhy: Compositional Physical Reasoning of Objects and Events from Videos](https://openreview.net/forum?id=PgNEYaIc81Q) |  | 0 |  | Zhenfang Chen, Kexin Yi, Yunzhu Li, Mingyu Ding, Antonio Torralba, Joshua B. Tenenbaum, Chuang Gan |  |
| 472 |  |  [An Information Fusion Approach to Learning with Instance-Dependent Label Noise](https://openreview.net/forum?id=ecH2FKaARUp) |  | 0 |  | Zhimeng Jiang, Kaixiong Zhou, Zirui Liu, Li Li, Rui Chen, SooHyun Choi, Xia Hu |  |
| 473 |  |  [On Redundancy and Diversity in Cell-based Neural Architecture Search](https://openreview.net/forum?id=rFJWoYoxrDB) |  | 0 |  | Xingchen Wan, Binxin Ru, Pedro M. Esperança, Zhenguo Li |  |
| 474 |  |  [Deep Learning without Shortcuts: Shaping the Kernel with Tailored Rectifiers](https://openreview.net/forum?id=U0k7XNTiFEq) |  | 0 |  | Guodong Zhang, Aleksandar Botev, James Martens |  |
| 475 |  |  [Variational autoencoders in the presence of low-dimensional data: landscape and implicit bias](https://openreview.net/forum?id=y_op4lLLaWL) |  | 0 |  | Frederic Koehler, Viraj Mehta, Chenghui Zhou, Andrej Risteski |  |
| 476 |  |  [No Parameters Left Behind: Sensitivity Guided Adaptive Learning Rate for Training Large Transformer Models](https://openreview.net/forum?id=cuvga_CiVND) |  | 0 |  | Chen Liang, Haoming Jiang, Simiao Zuo, Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, Tuo Zhao |  |
| 477 |  |  [SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations](https://openreview.net/forum?id=aBsCjcPu_tE) |  | 0 |  | Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, JunYan Zhu, Stefano Ermon |  |
| 478 |  |  [Post hoc Explanations may be Ineffective for Detecting Unknown Spurious Correlation](https://openreview.net/forum?id=xNOVfCCvDpM) |  | 0 |  | Julius Adebayo, Michael Muelly, Harold Abelson, Been Kim |  |
| 479 |  |  [Generalizing Few-Shot NAS with Gradient Matching](https://openreview.net/forum?id=_jMtny3sMKU) |  | 0 |  | Shoukang Hu, Ruochen Wang, Lanqing Hong, Zhenguo Li, ChoJui Hsieh, Jiashi Feng |  |
| 480 |  |  [The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training](https://openreview.net/forum?id=VBZJ_3tz-t) |  | 0 |  | Shiwei Liu, Tianlong Chen, Xiaohan Chen, Li Shen, Decebal Constantin Mocanu, Zhangyang Wang, Mykola Pechenizkiy |  |
| 481 |  |  [switch-GLAT: Multilingual Parallel Machine Translation Via Code-Switch Decoder](https://openreview.net/forum?id=5HvpvYd68b) |  | 0 |  | Zhenqiao Song, Hao Zhou, Lihua Qian, Jingjing Xu, Shanbo Cheng, Mingxuan Wang, Lei Li |  |
| 482 |  |  [DictFormer: Tiny Transformer with Shared Dictionary](https://openreview.net/forum?id=GWQWAeE9EpB) |  | 0 |  | Qian Lou, Ting Hua, YenChang Hsu, Yilin Shen, Hongxia Jin |  |
| 483 |  |  [Training Transition Policies via Distribution Matching for Complex Tasks](https://openreview.net/forum?id=6vkzF28Hur8) |  | 0 |  | JuSeung Byun, Andrew Perrault |  |
| 484 |  |  [GDA-AM: On the Effectiveness of Solving Min-Imax Optimization via Anderson Mixing](https://openreview.net/forum?id=3YqeuCVwy1d) |  | 0 |  | Huan He, Shifan Zhao, Yuanzhe Xi, Joyce C. Ho, Yousef Saad |  |
| 485 |  |  [On feature learning in neural networks with global convergence guarantees](https://openreview.net/forum?id=PQTW3iG4sC-) |  | 0 |  | Zhengdao Chen, Eric VandenEijnden, Joan Bruna |  |
| 486 |  |  [The Three Stages of Learning Dynamics in High-dimensional Kernel Methods](https://openreview.net/forum?id=EQmAP4F859) |  | 0 |  | Nikhil Ghosh, Song Mei, Bin Yu |  |
| 487 |  |  [When Can We Learn General-Sum Markov Games with a Large Number of Players Sample-Efficiently?](https://openreview.net/forum?id=6MmiS0HUJHR) |  | 0 |  | Ziang Song, Song Mei, Yu Bai |  |
| 488 |  |  [Neural Networks as Kernel Learners: The Silent Alignment Effect](https://openreview.net/forum?id=1NvflqAdoom) |  | 0 |  | Alexander B. Atanasov, Blake Bordelon, Cengiz Pehlevan |  |
| 489 |  |  [Learning Object-Oriented Dynamics for Planning from Text](https://openreview.net/forum?id=B6EIcyp-Rb7) |  | 0 |  | Guiliang Liu, Ashutosh Adhikari, Amirmassoud Farahmand, Pascal Poupart |  |
| 490 |  |  [An Operator Theoretic View On Pruning Deep Neural Networks](https://openreview.net/forum?id=pWBNOgdeURp) |  | 0 |  | William T. Redman, Maria Fonoberova, Ryan Mohr, Yannis G. Kevrekidis, Igor Mezic |  |
| 491 |  |  [Capacity of Group-invariant Linear Readouts from Equivariant Representations: How Many Objects can be Linearly Classified Under All Possible Views?](https://openreview.net/forum?id=_4GFbtOuWq-) |  | 0 |  | Matthew Farrell, Blake Bordelon, Shubhendu Trivedi, Cengiz Pehlevan |  |
| 492 |  |  [Tuformer: Data-driven Design of Transformers for Improved Generalization or Efficiency](https://openreview.net/forum?id=V0A5g83gdQ_) |  | 0 |  | Xiaoyu Liu, Jiahao Su, Furong Huang |  |
| 493 |  |  [Learning Weakly-supervised Contrastive Representations](https://openreview.net/forum?id=MSwEFaztwkE) |  | 0 |  | YaoHung Hubert Tsai, Tianqin Li, Weixin Liu, Peiyuan Liao, Ruslan Salakhutdinov, LouisPhilippe Morency |  |
| 494 |  |  [Encoding Weights of Irregular Sparsity for Fixed-to-Fixed Model Compression](https://openreview.net/forum?id=Vs5NK44aP9P) |  | 0 |  | Baeseong Park, Se Jung Kwon, Daehwan Oh, Byeongwook Kim, Dongsoo Lee |  |
| 495 |  |  [An Experimental Design Perspective on Model-Based Reinforcement Learning](https://openreview.net/forum?id=0no8Motr-zO) |  | 0 |  | Viraj Mehta, Biswajit Paria, Jeff Schneider, Stefano Ermon, Willie Neiswanger |  |
| 496 |  |  [BAM: Bayes with Adaptive Memory](https://openreview.net/forum?id=NdOoQnYPj_) |  | 0 |  | Josue Nassar, Jennifer Rogers Brennan, Ben Evans, Kendall Lowrey |  |
| 497 |  |  [Unsupervised Learning of Full-Waveform Inversion: Connecting CNN and Partial Differential Equation in a Loop](https://openreview.net/forum?id=izvwgBic9q) |  | 0 |  | Peng Jin, Xitong Zhang, Yinpeng Chen, Sharon Xiaolei Huang, Zicheng Liu, Youzuo Lin |  |
| 498 |  |  [Conditional Contrastive Learning with Kernel](https://openreview.net/forum?id=AAJLBoGt0XM) |  | 0 |  | YaoHung Hubert Tsai, Tianqin Li, Martin Q. Ma, Han Zhao, Kun Zhang, LouisPhilippe Morency, Ruslan Salakhutdinov |  |
| 499 |  |  [ConFeSS: A Framework for Single Source Cross-Domain Few-Shot Learning](https://openreview.net/forum?id=zRJu6mU2BaE) |  | 0 |  | Debasmit Das, Sungrack Yun, Fatih Porikli |  |
| 500 |  |  [Granger causal inference on DAGs identifies genomic loci regulating transcription](https://openreview.net/forum?id=nZOUYEN6Wvy) |  | 0 |  | Alexander P. Wu, Rohit Singh, Bonnie Berger |  |
| 501 |  |  [Energy-Inspired Molecular Conformation Optimization](https://openreview.net/forum?id=7QfLW-XZTl) |  | 0 |  | Jiaqi Guan, Wesley Wei Qian, Qiang Liu, WeiYing Ma, Jianzhu Ma, Jian Peng |  |
| 502 |  |  [Towards Deepening Graph Neural Networks: A GNTK-based Optimization Perspective](https://openreview.net/forum?id=tT9t_ZctZRL) |  | 0 |  | Wei Huang, Yayong Li, Weitao Du, Richard Y. D. Xu, Jie Yin, Ling Chen, Miao Zhang |  |
| 503 |  |  [Connectome-constrained Latent Variable Model of Whole-Brain Neural Activity](https://openreview.net/forum?id=CJzi3dRlJE-) |  | 0 |  | Lu Mi, Richard Xu, Sridhama Prakhya, Albert Lin, Nir Shavit, Aravinthan D. T. Samuel, Srinivas C. Turaga |  |
| 504 |  |  [T-WaveNet: A Tree-Structured Wavelet Neural Network for Time Series Signal Analysis](https://openreview.net/forum?id=U4uFaLyg7PV) |  | 0 |  | Minhao Liu, Ailing Zeng, Qiuxia Lai, Ruiyuan Gao, Min Li, Jing Qin, Qiang Xu |  |
| 505 |  |  [Trans-Encoder: Unsupervised sentence-pair modelling through self- and mutual-distillations](https://openreview.net/forum?id=AmUhwTOHgm) |  | 0 |  | Fangyu Liu, Yunlong Jiao, Jordan Massiah, Emine Yilmaz, Serhii Havrylov |  |
| 506 |  |  [Path Integral Sampler: A Stochastic Control Approach For Sampling](https://openreview.net/forum?id=_uCb2ynRu7Y) |  | 0 |  | Qinsheng Zhang, Yongxin Chen |  |
| 507 |  |  [Model Zoo: A Growing Brain That Learns Continually](https://openreview.net/forum?id=WfvgGBcgbE7) |  | 0 |  | Rahul Ramesh, Pratik Chaudhari |  |
| 508 |  |  [Predicting Physics in Mesh-reduced Space with Temporal Attention](https://openreview.net/forum?id=XctLdNfCmP) |  | 0 |  | Xu Han, Han Gao, Tobias Pfaff, JianXun Wang, Liping Liu |  |
| 509 |  |  [How unlabeled data improve generalization in self-training? A one-hidden-layer theoretical analysis](https://openreview.net/forum?id=qiMXBIf4NfB) |  | 0 |  | Shuai Zhang, Meng Wang, Sijia Liu, PinYu Chen, Jinjun Xiong |  |
| 510 |  |  [Learning to Dequantise with Truncated Flows](https://openreview.net/forum?id=fExcSKdDo_) |  | 0 |  | Shawn Tan, ChinWei Huang, Alessandro Sordoni, Aaron C. Courville |  |
| 511 |  |  [Curriculum learning as a tool to uncover learning principles in the brain](https://openreview.net/forum?id=TpJMvo0_pu-) |  | 0 |  | Daniel R. Kepple, Rainer Engelken, Kanaka Rajan |  |
| 512 |  |  [Optimizer Amalgamation](https://openreview.net/forum?id=VqzXzA9hjaX) |  | 0 |  | Tianshu Huang, Tianlong Chen, Sijia Liu, Shiyu Chang, Lisa Amini, Zhangyang Wang |  |
| 513 |  |  [An Agnostic Approach to Federated Learning with Class Imbalance](https://openreview.net/forum?id=Xo0lbDt975) |  | 0 |  | Zebang Shen, Juan Cerviño, Hamed Hassani, Alejandro Ribeiro |  |
| 514 |  |  [A Fine-Tuning Approach to Belief State Modeling](https://openreview.net/forum?id=ckZY7DGa7FQ) |  | 0 |  | Samuel Sokota, Hengyuan Hu, David J. Wu, J. Zico Kolter, Jakob Nicolaus Foerster, Noam Brown |  |
| 515 |  |  [Differentially Private Fine-tuning of Language Models](https://openreview.net/forum?id=Q42f0dfjECO) |  | 0 |  | Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A. Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin, Huishuai Zhang |  |
| 516 |  |  [P-Adapters: Robustly Extracting Factual Information from Language Models with Diverse Prompts](https://openreview.net/forum?id=DhzIU48OcZh) |  | 0 |  | Benjamin Newman, Prafulla Kumar Choubey, Nazneen Rajani |  |
| 517 |  |  [Iterated Reasoning with Mutual Information in Cooperative and Byzantine Decentralized Teaming](https://openreview.net/forum?id=giBFoa-uS12) |  | 0 |  | Sachin G. Konan, Esmaeil Seraj, Matthew C. Gombolay |  |
| 518 |  |  [Step-unrolled Denoising Autoencoders for Text Generation](https://openreview.net/forum?id=T0GpzBQ1Fg6) |  | 0 |  | Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, Aäron van den Oord |  |
| 519 |  |  [Hindsight Foresight Relabeling for Meta-Reinforcement Learning](https://openreview.net/forum?id=P7OVkHEoHOZ) |  | 0 |  | Michael Wan, Jian Peng, Tanmay Gangwani |  |
| 520 |  |  [LoRA: Low-Rank Adaptation of Large Language Models](https://openreview.net/forum?id=nZeVKeeFYf9) |  | 0 |  | Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen |  |
| 521 |  |  [Which Shortcut Cues Will DNNs Choose? A Study from the Parameter-Space Perspective](https://openreview.net/forum?id=qRDQi3ocgR3) |  | 0 |  | Luca Scimeca, Seong Joon Oh, Sanghyuk Chun, Michael Poli, Sangdoo Yun |  |
| 522 |  |  [Efficient Computation of Deep Nonlinear Infinite-Width Neural Networks that Learn Features](https://openreview.net/forum?id=tUMr0Iox8XW) |  | 0 |  | Greg Yang, Michael Santacroce, Edward J. Hu |  |
| 523 |  |  [TRAIL: Near-Optimal Imitation Learning with Suboptimal Data](https://openreview.net/forum?id=6q_2b6u0BnJ) |  | 0 |  | Mengjiao Yang, Sergey Levine, Ofir Nachum |  |
| 524 |  |  [On the benefits of maximum likelihood estimation for Regression and Forecasting](https://openreview.net/forum?id=zrW-LVXj2k1) |  | 0 |  | Pranjal Awasthi, Abhimanyu Das, Rajat Sen, Ananda Theertha Suresh |  |
| 525 |  |  [Effect of scale on catastrophic forgetting in neural networks](https://openreview.net/forum?id=GhVS8_yPeEa) |  | 0 |  | Vinay Venkatesh Ramasesh, Aitor Lewkowycz, Ethan Dyer |  |
| 526 |  |  [Learn Locally, Correct Globally: A Distributed Algorithm for Training Graph Neural Networks](https://openreview.net/forum?id=FndDxSz3LxQ) |  | 0 |  | Morteza Ramezani, Weilin Cong, Mehrdad Mahdavi, Mahmut T. Kandemir, Anand Sivasubramaniam |  |
| 527 |  |  [Conditional Image Generation by Conditioning Variational Auto-Encoders](https://openreview.net/forum?id=7MV6uLzOChW) |  | 0 |  | William Harvey, Saeid Naderiparizi, Frank Wood |  |
| 528 |  |  [Learning 3D Representations of Molecular Chirality with Invariance to Bond Rotations](https://openreview.net/forum?id=hm2tNDdgaFK) |  | 0 |  | Keir Adams, Lagnajit Pattanaik, Connor W. Coley |  |
| 529 |  |  [Neural Methods for Logical Reasoning over Knowledge Graphs](https://openreview.net/forum?id=tgcAoUVHRIB) |  | 0 |  | Alfonso Amayuelas, Shuai Zhang, Susie Xi Rao, Ce Zhang |  |
| 530 |  |  [Consistent Counterfactuals for Deep Models](https://openreview.net/forum?id=St6eyiTEHnG) |  | 0 |  | Emily Black, Zifan Wang, Matt Fredrikson |  |
| 531 |  |  [Unified Visual Transformer Compression](https://openreview.net/forum?id=9jsZiUgkCZP) |  | 0 |  | Shixing Yu, Tianlong Chen, Jiayi Shen, Huan Yuan, Jianchao Tan, Sen Yang, Ji Liu, Zhangyang Wang |  |
| 532 |  |  [Transformer-based Transform Coding](https://openreview.net/forum?id=IDwN6xjHnK8) |  | 0 |  | Yinhao Zhu, Yang Yang, Taco Cohen |  |
| 533 |  |  [Object Pursuit: Building a Space of Objects via Discriminative Weight Generation](https://openreview.net/forum?id=lbauk6wK2-y) |  | 0 |  | Chuanyu Pan, Yanchao Yang, Kaichun Mo, Yueqi Duan, Leonidas J. Guibas |  |
| 534 |  |  [PAC Prediction Sets Under Covariate Shift](https://openreview.net/forum?id=DhP9L8vIyLc) |  | 0 |  | Sangdon Park, Edgar Dobriban, Insup Lee, Osbert Bastani |  |
| 535 |  |  [Generalization of Neural Combinatorial Solvers Through the Lens of Adversarial Robustness](https://openreview.net/forum?id=vJZ7dPIjip3) |  | 0 |  | Simon Geisler, Johanna Sommer, Jan Schuchardt, Aleksandar Bojchevski, Stephan Günnemann |  |
| 536 |  |  [One After Another: Learning Incremental Skills for a Changing World](https://openreview.net/forum?id=dg79moSRqIo) |  | 0 |  | Nur Muhammad (Mahi) Shafiullah, Lerrel Pinto |  |
| 537 |  |  [Graph-Guided Network for Irregularly Sampled Multivariate Time Series](https://openreview.net/forum?id=Kwm8I7dU-l5) |  | 0 |  | Xiang Zhang, Marko Zeman, Theodoros Tsiligkaridis, Marinka Zitnik |  |
| 538 |  |  [FILM: Following Instructions in Language with Modular Methods](https://openreview.net/forum?id=qI4542Y2s1D) |  | 0 |  | So Yeon Min, Devendra Singh Chaplot, Pradeep Kumar Ravikumar, Yonatan Bisk, Ruslan Salakhutdinov |  |
| 539 |  |  [The Evolution of Uncertainty of Learning in Games](https://openreview.net/forum?id=Fza94Y8VS4a) |  | 0 |  | Yun Kuen Cheung, Georgios Piliouras, Yixin Tao |  |
| 540 |  |  [Explainable GNN-Based Models over Knowledge Graphs](https://openreview.net/forum?id=CrCvGNHAIrz) |  | 0 |  | David Jaime Tena Cucala, Bernardo Cuenca Grau, Egor V. Kostylev, Boris Motik |  |
| 541 |  |  [Mention Memory: incorporating textual knowledge into Transformers through entity mention attention](https://openreview.net/forum?id=OY1A8ejQgEX) |  | 0 |  | Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Fei Sha, William W. Cohen |  |
| 542 |  |  [Training Data Generating Networks: Shape Reconstruction via Bi-level Optimization](https://openreview.net/forum?id=dDo8druYppX) |  | 0 |  | Biao Zhang, Peter Wonka |  |
| 543 |  |  [Monotonic Differentiable Sorting Networks](https://openreview.net/forum?id=IcUWShptD7d) |  | 0 |  | Felix Petersen, Christian Borgelt, Hilde Kuehne, Oliver Deussen |  |
| 544 |  |  [CrowdPlay: Crowdsourcing Human Demonstrations for Offline Learning](https://openreview.net/forum?id=qyTBxTztIpQ) |  | 0 |  | Matthias Gerstgrasser, Rakshit S. Trivedi, David C. Parkes |  |
| 545 |  |  [Model Agnostic Interpretability for Multiple Instance Learning](https://openreview.net/forum?id=KSSfF5lMIAg) |  | 0 |  | Joseph Early, Christine Evers, Sarvapali D. Ramchurn |  |
| 546 |  |  [FastSHAP: Real-Time Shapley Value Estimation](https://openreview.net/forum?id=Zq2G_VTV53T) |  | 0 |  | Neil Jethani, Mukund Sudarshan, Ian Connick Covert, SuIn Lee, Rajesh Ranganath |  |
| 547 |  |  [When, Why, and Which Pretrained GANs Are Useful?](https://openreview.net/forum?id=4Ycr8oeCoIh) |  | 0 |  | Timofey Grigoryev, Andrey Voynov, Artem Babenko |  |
| 548 |  |  [A global convergence theory for deep ReLU implicit networks via over-parameterization](https://openreview.net/forum?id=R332S76RjxS) |  | 0 |  | Tianxiang Gao, Hailiang Liu, Jia Liu, Hridesh Rajan, Hongyang Gao |  |
| 549 |  |  [Learnability Lock: Authorized Learnability Control Through Adversarial Invertible Transformations](https://openreview.net/forum?id=6VpeS27viTq) |  | 0 |  | Weiqi Peng, Jinghui Chen |  |
| 550 |  |  [Federated Learning from Only Unlabeled Data with Class-conditional-sharing Clients](https://openreview.net/forum?id=WHA8009laxu) |  | 0 |  | Nan Lu, Zhao Wang, Xiaoxiao Li, Gang Niu, Qi Dou, Masashi Sugiyama |  |
| 551 |  |  [Transformer Embeddings of Irregularly Spaced Events and Their Participants](https://openreview.net/forum?id=Rty5g9imm7H) |  | 0 |  | Hongyuan Mei, Chenghao Yang, Jason Eisner |  |
| 552 |  |  [Fast Model Editing at Scale](https://openreview.net/forum?id=0DcZxeWfOPt) |  | 0 |  | Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, Christopher D. Manning |  |
| 553 |  |  [Eigencurve: Optimal Learning Rate Schedule for SGD on Quadratic Objectives with Skewed Hessian Spectrums](https://openreview.net/forum?id=rTAclwH46Tb) |  | 0 |  | Rui Pan, Haishan Ye, Tong Zhang |  |
| 554 |  |  [An Autoregressive Flow Model for 3D Molecular Geometry Generation from Scratch](https://openreview.net/forum?id=C03Ajc-NS5W) |  | 0 |  | Youzhi Luo, Shuiwang Ji |  |
| 555 |  |  [On Incorporating Inductive Biases into VAEs](https://openreview.net/forum?id=nzvbBD_3J-g) |  | 0 |  | Ning Miao, Emile Mathieu, Siddharth N, Yee Whye Teh, Tom Rainforth |  |
| 556 |  |  [DiffSkill: Skill Abstraction from Differentiable Physics for Deformable Object Manipulations with Tools](https://openreview.net/forum?id=Kef8cKdHWpP) |  | 0 |  | Xingyu Lin, Zhiao Huang, Yunzhu Li, Joshua B. Tenenbaum, David Held, Chuang Gan |  |
| 557 |  |  [On the Existence of Universal Lottery Tickets](https://openreview.net/forum?id=SYB4WrJql1n) |  | 0 |  | Rebekka Burkholz, Nilanjana Laha, Rajarshi Mukherjee, Alkis Gotovos |  |
| 558 |  |  [Pre-training Molecular Graph Representation with 3D Geometry](https://openreview.net/forum?id=xQUe1pOKPam) |  | 0 |  | Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, Jian Tang |  |
| 559 |  |  [PER-ETD: A Polynomially Efficient Emphatic Temporal Difference Learning Method](https://openreview.net/forum?id=-HSOjDPfhBJ) |  | 0 |  | Ziwei Guan, Tengyu Xu, Yingbin Liang |  |
| 560 |  |  [Taming Sparsely Activated Transformer with Stochastic Experts](https://openreview.net/forum?id=B72HXs80q4) |  | 0 |  | Simiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Jianfeng Gao, Tuo Zhao |  |
| 561 |  |  [Hierarchical Variational Memory for Few-shot Learning Across Domains](https://openreview.net/forum?id=i3RI65sR7N) |  | 0 |  | YingJun Du, Xiantong Zhen, Ling Shao, Cees G. M. Snoek |  |
| 562 |  |  [Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction](https://openreview.net/forum?id=Z1Qlm11uOM) |  | 0 |  | Bowen Shi, WeiNing Hsu, Kushal Lakhotia, Abdelrahman Mohamed |  |
| 563 |  |  [An Explanation of In-context Learning as Implicit Bayesian Inference](https://openreview.net/forum?id=RdJVFCHjUMI) |  | 0 |  | Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma |  |
| 564 |  |  [Differentiable Scaffolding Tree for Molecule Optimization](https://openreview.net/forum?id=w_drCosT76) |  | 0 |  | Tianfan Fu, Wenhao Gao, Cao Xiao, Jacob Yasonik, Connor W. Coley, Jimeng Sun |  |
| 565 |  |  [Eliminating Sharp Minima from SGD with Truncated Heavy-tailed Noise](https://openreview.net/forum?id=B3Nde6lvab) |  | 0 |  | Xingyu Wang, Sewoong Oh, ChangHan Rhee |  |
| 566 |  |  [Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System](https://openreview.net/forum?id=uxxFrDwrE7Y) |  | 0 |  | Elahe Arani, Fahad Sarfraz, Bahram Zonooz |  |
| 567 |  |  [FedChain: Chained Algorithms for Near-optimal Communication Cost in Federated Learning](https://openreview.net/forum?id=ZaVVVlcdaN) |  | 0 |  | Charlie Hou, Kiran Koshy Thekumparampil, Giulia Fanti, Sewoong Oh |  |
| 568 |  |  [What Do We Mean by Generalization in Federated Learning?](https://openreview.net/forum?id=VimqQq-i_Q) |  | 0 |  | Honglin Yuan, Warren Richard Morningstar, Lin Ning, Karan Singhal |  |
| 569 |  |  [Frequency-aware SGD for Efficient Embedding Learning with Provable Benefits](https://openreview.net/forum?id=ibqTBNfJmi) |  | 0 |  | Yan Li, Dhruv Choudhary, Xiaohan Wei, Baichuan Yuan, Bhargav Bhushanam, Tuo Zhao, Guanghui Lan |  |
| 570 |  |  [Learning Curves for Gaussian Process Regression with Power-Law Priors and Targets](https://openreview.net/forum?id=KeI9E-gsoB) |  | 0 |  | Hui Jin, Pradeep Kr. Banerjee, Guido Montúfar |  |
| 571 |  |  [Fast topological clustering with Wasserstein distance](https://openreview.net/forum?id=0kPL3xO4R5) |  | 0 |  | Tananun Songdechakraiwut, Bryan M. Krause, Matthew I. Banks, Kirill V. Nourski, Barry D. Van Veen |  |
| 572 |  |  [Autonomous Reinforcement Learning: Formalism and Benchmarking](https://openreview.net/forum?id=nkaba3ND7B5) |  | 0 |  | Archit Sharma, Kelvin Xu, Nikhil Sardana, Abhishek Gupta, Karol Hausman, Sergey Levine, Chelsea Finn |  |
| 573 |  |  [GRAND++: Graph Neural Diffusion with A Source Term](https://openreview.net/forum?id=EMxu-dzvJk) |  | 0 |  | Matthew Thorpe, Tan Minh Nguyen, Hedi Xia, Thomas Strohmer, Andrea L. Bertozzi, Stanley J. Osher, Bao Wang |  |
| 574 |  |  [Case-based reasoning for better generalization in textual reinforcement learning](https://openreview.net/forum?id=ZDaSIkWT-AP) |  | 0 |  | Mattia Atzeni, Shehzaad Zuzar Dhuliawala, Keerthiram Murugesan, Mrinmaya Sachan |  |
| 575 |  |  [Neural Deep Equilibrium Solvers](https://openreview.net/forum?id=B0oHOwT5ENL) |  | 0 |  | Shaojie Bai, Vladlen Koltun, J. Zico Kolter |  |
| 576 |  |  [A Theoretical Analysis on Feature Learning in Neural Networks: Emergence from Inputs and Advantage over Fixed Features](https://openreview.net/forum?id=wMpS-Z_AI_E) |  | 0 |  | Zhenmei Shi, Junyi Wei, Yingyu Liang |  |
| 577 |  |  [CADDA: Class-wise Automatic Differentiable Data Augmentation for EEG Signals](https://openreview.net/forum?id=6IYp-35L-xJ) |  | 0 |  | Cédric Rommel, Thomas Moreau, Joseph Paillard, Alexandre Gramfort |  |
| 578 |  |  [Label Leakage and Protection in Two-party Split Learning](https://openreview.net/forum?id=cOtBRgsf2fO) |  | 0 |  | Oscar Li, Jiankai Sun, Xin Yang, Weihao Gao, Hongyi Zhang, Junyuan Xie, Virginia Smith, Chong Wang |  |
| 579 |  |  [Semi-relaxed Gromov-Wasserstein divergence and applications on graphs](https://openreview.net/forum?id=RShaMexjc-x) |  | 0 |  | Cédric VincentCuaz, Rémi Flamary, Marco Corneli, Titouan Vayer, Nicolas Courty |  |
| 580 |  |  [CodeTrek: Flexible Modeling of Code using an Extensible Relational Representation](https://openreview.net/forum?id=WQc075jmBmf) |  | 0 |  | Pardis Pashakhanloo, Aaditya Naik, Yuepeng Wang, Hanjun Dai, Petros Maniatis, Mayur Naik |  |
| 581 |  |  [Bridging Recommendation and Marketing via Recurrent Intensity Modeling](https://openreview.net/forum?id=TZeArecH2Nf) |  | 0 |  | Yifei Ma, Ge Liu, Anoop Deoras |  |
| 582 |  |  [Sparse Attention with Learning to Hash](https://openreview.net/forum?id=VGnOJhd5Q1q) |  | 0 |  | Zhiqing Sun, Yiming Yang, Shinjae Yoo |  |
| 583 |  |  [Controlling the Complexity and Lipschitz Constant improves Polynomial Nets](https://openreview.net/forum?id=dQ7Cy_ndl1s) |  | 0 |  | Zhenyu Zhu, Fabian Latorre, Grigorios Chrysos, Volkan Cevher |  |
| 584 |  |  [Finding an Unsupervised Image Segmenter in each of your Deep Generative Models](https://openreview.net/forum?id=Ug-bgjgSlKV) |  | 0 |  | Luke MelasKyriazi, Christian Rupprecht, Iro Laina, Andrea Vedaldi |  |
| 585 |  |  [Solving Inverse Problems in Medical Imaging with Score-Based Generative Models](https://openreview.net/forum?id=vaRCHVj0uGI) |  | 0 |  | Yang Song, Liyue Shen, Lei Xing, Stefano Ermon |  |
| 586 |  |  [BDDM: Bilateral Denoising Diffusion Models for Fast and High-Quality Speech Synthesis](https://openreview.net/forum?id=L7wzpQttNO) |  | 0 |  | Max W. Y. Lam, Jun Wang, Dan Su, Dong Yu |  |
| 587 |  |  [Sample Efficient Stochastic Policy Extragradient Algorithm for Zero-Sum Markov Game](https://openreview.net/forum?id=IvepFxYRDG) |  | 0 |  | Ziyi Chen, Shaocong Ma, Yi Zhou |  |
| 588 |  |  [The Uncanny Similarity of Recurrence and Depth](https://openreview.net/forum?id=3wNcr5nq56) |  | 0 |  | Avi Schwarzschild, Arjun Gupta, Amin Ghiasi, Micah Goldblum, Tom Goldstein |  |
| 589 |  |  [Implicit Bias of Adversarial Training for Deep Neural Networks](https://openreview.net/forum?id=l8It-0lE5e7) |  | 0 |  | Bochen Lv, Zhanxing Zhu |  |
| 590 |  |  [Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning](https://openreview.net/forum?id=_SJ-_yyes8) |  | 0 |  | Denis Yarats, Rob Fergus, Alessandro Lazaric, Lerrel Pinto |  |
| 591 |  |  [$\pi$BO: Augmenting Acquisition Functions with User Beliefs for Bayesian Optimization](https://openreview.net/forum?id=MMAeCXIa89) |  | 0 |  | Carl Hvarfner, Danny Stoll, Artur L. F. Souza, Marius Lindauer, Frank Hutter, Luigi Nardi |  |
| 592 |  |  [A Generalized Weighted Optimization Method for Computational Learning and Inversion](https://openreview.net/forum?id=14F3fI6MGxX) |  | 0 |  | Kui Ren, Yunan Yang, Björn Engquist |  |
| 593 |  |  [DriPP: Driven Point Processes to Model Stimuli Induced Patterns in M/EEG Signals](https://openreview.net/forum?id=d_2lcDh0Y9c) |  | 0 |  | Cédric Allain, Alexandre Gramfort, Thomas Moreau |  |
| 594 |  |  [Stiffness-aware neural network for learning Hamiltonian systems](https://openreview.net/forum?id=uVXEKeqJbNa) |  | 0 |  | Senwei Liang, Zhongzhan Huang, Hong Zhang |  |
| 595 |  |  [CoST: Contrastive Learning of Disentangled Seasonal-Trend Representations for Time Series Forecasting](https://openreview.net/forum?id=PilZY3omXV2) |  | 0 |  | Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, Steven C. H. Hoi |  |
| 596 |  |  [CoordX: Accelerating Implicit Neural Representation with a Split MLP Architecture](https://openreview.net/forum?id=oAy7yPmdNz) |  | 0 |  | Ruofan Liang, Hongyi Sun, Nandita Vijaykumar |  |
| 597 |  |  [Plant 'n' Seek: Can You Find the Winning Ticket?](https://openreview.net/forum?id=9n9c8sf0xm) |  | 0 |  | Jonas Fischer, Rebekka Burkholz |  |
| 598 |  |  [Coherence-based Label Propagation over Time Series for Accelerated Active Learning](https://openreview.net/forum?id=gjNcH0hj0LM) |  | 0 |  | Yooju Shin, Susik Yoon, Sundong Kim, Hwanjun Song, JaeGil Lee, Byung Suk Lee |  |
| 599 |  |  [A Class of Short-term Recurrence Anderson Mixing Methods and Their Applications](https://openreview.net/forum?id=_X90SIKbHa) |  | 0 |  | Fuchao Wei, Chenglong Bao, Yang Liu |  |
| 600 |  |  [The Geometry of Memoryless Stochastic Policy Optimization in Infinite-Horizon POMDPs](https://openreview.net/forum?id=A05I5IvrdL-) |  | 0 |  | Johannes Müller, Guido Montúfar |  |
| 601 |  |  [Efficient Sharpness-aware Minimization for Improved Training of Neural Networks](https://openreview.net/forum?id=n0OeTdNRG0Q) |  | 0 |  | Jiawei Du, Hanshu Yan, Jiashi Feng, Joey Tianyi Zhou, Liangli Zhen, Rick Siow Mong Goh, Vincent Y. F. Tan |  |
| 602 |  |  [Lipschitz-constrained Unsupervised Skill Discovery](https://openreview.net/forum?id=BGvt0ghNgA) |  | 0 |  | Seohong Park, Jongwook Choi, Jaekyeom Kim, Honglak Lee, Gunhee Kim |  |
| 603 |  |  [Learning Generalizable Representations for Reinforcement Learning via Adaptive Meta-learner of Behavioral Similarities](https://openreview.net/forum?id=zBOI9LFpESK) |  | 0 |  | Jianda Chen, Sinno Jialin Pan |  |
| 604 |  |  [Effective Model Sparsification by Scheduled Grow-and-Prune Methods](https://openreview.net/forum?id=xa6otUDdP2W) |  | 0 |  | Xiaolong Ma, Minghai Qin, Fei Sun, Zejiang Hou, Kun Yuan, Yi Xu, Yanzhi Wang, YenKuang Chen, Rong Jin, Yuan Xie |  |
| 605 |  |  [FILIP: Fine-grained Interactive Language-Image Pre-Training](https://openreview.net/forum?id=cpDhcsEDC2) |  | 0 |  | Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, Chunjing Xu |  |
| 606 |  |  [Information Prioritization through Empowerment in Visual Model-based RL](https://openreview.net/forum?id=DfUjyyRW90) |  | 0 |  | Homanga Bharadhwaj, Mohammad Babaeizadeh, Dumitru Erhan, Sergey Levine |  |
| 607 |  |  [Efficient Active Search for Combinatorial Optimization Problems](https://openreview.net/forum?id=nO5caZwFwYu) |  | 0 |  | André Hottung, YeongDae Kwon, Kevin Tierney |  |
| 608 |  |  [Ancestral protein sequence reconstruction using a tree-structured Ornstein-Uhlenbeck variational autoencoder](https://openreview.net/forum?id=FZoZ7a31GCW) |  | 0 |  | Lys Sanz Moreta, Ola Rønning, Ahmad Salim AlSibahi, Jotun Hein, Douglas L. Theobald, Thomas Hamelryck |  |
| 609 |  |  [Training Structured Neural Networks Through Manifold Identification and Variance Reduction](https://openreview.net/forum?id=mdUYT5QV0O) |  | 0 |  | ZihSyuan Huang, Chingpei Lee |  |
| 610 |  |  [The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization](https://openreview.net/forum?id=KBQP4A_J1K) |  | 0 |  | Róbert Csordás, Kazuki Irie, Jürgen Schmidhuber |  |
| 611 |  |  [On the Limitations of Multimodal VAEs](https://openreview.net/forum?id=w-CPUXXrAj) |  | 0 |  | Imant Daunhawer, Thomas M. Sutter, Kieran ChinCheong, Emanuele Palumbo, Julia E. Vogt |  |
| 612 |  |  [Recursive Disentanglement Network](https://openreview.net/forum?id=CSfcOznpDY) |  | 0 |  | Yixuan Chen, Yubin Shi, Dongsheng Li, Yujiang Wang, Mingzhi Dong, Yingying Zhao, Robert P. Dick, Qin Lv, Fan Yang, Li Shang |  |
| 613 |  |  [ADAVI: Automatic Dual Amortized Variational Inference Applied To Pyramidal Bayesian Models](https://openreview.net/forum?id=CgIEctmcXx1) |  | 0 |  | Louis Rouillard, Demian Wassermann |  |
| 614 |  |  [Distributionally Robust Models with Parametric Likelihood Ratios](https://openreview.net/forum?id=a34GrNaYEcS) |  | 0 |  | Paul Michel, Tatsunori Hashimoto, Graham Neubig |  |
| 615 |  |  [Constrained Physical-Statistics Models for Dynamical System Identification and Prediction](https://openreview.net/forum?id=gbe1zHyA73) |  | 0 |  | Jérémie Donà, Marie Déchelle, Patrick Gallinari, Marina Levy |  |
| 616 |  |  [Doubly Adaptive Scaled Algorithm for Machine Learning Using Second-Order Information](https://openreview.net/forum?id=HCelXXcSEuH) |  | 0 |  | Majid Jahani, Sergey Rusakov, Zheng Shi, Peter Richtárik, Michael W. Mahoney, Martin Takác |  |
| 617 |  |  [Understanding approximate and unrolled dictionary learning for pattern recovery](https://openreview.net/forum?id=rI0LYgGeYaw) |  | 0 |  | Benoît Malézieux, Thomas Moreau, Matthieu Kowalski |  |
| 618 |  |  [Constraining Linear-chain CRFs to Regular Languages](https://openreview.net/forum?id=jbrgwbv8nD) |  | 0 |  | Sean Papay, Roman Klinger, Sebastian Padó |  |
| 619 |  |  [Dive Deeper Into Integral Pose Regression](https://openreview.net/forum?id=vHVcB-ak3Si) |  | 0 |  | Kerui Gu, Linlin Yang, Angela Yao |  |
| 620 |  |  [Evidential Turing Processes](https://openreview.net/forum?id=84NMXTHYe-) |  | 0 |  | Melih Kandemir, Abdullah Akgül, Manuel Haußmann, Gozde Unal |  |
| 621 |  |  [Noisy Feature Mixup](https://openreview.net/forum?id=vJb4I2ANmy) |  | 0 |  | Soon Hoe Lim, N. Benjamin Erichson, Francisco Utrera, Winnie Xu, Michael W. Mahoney |  |
| 622 |  |  [Peek-a-Boo: What (More) is Disguised in a Randomly Weighted Neural Network, and How to Find It Efficiently](https://openreview.net/forum?id=moHCzz6D5H3) |  | 0 |  | Xiaohan Chen, Jason Zhang, Zhangyang Wang |  |
| 623 |  |  [How Well Does Self-Supervised Pre-Training Perform with Streaming Data?](https://openreview.net/forum?id=EwqEx5ipbOu) |  | 0 |  | Dapeng Hu, Shipeng Yan, Qizhengqiu Lu, Lanqing Hong, Hailin Hu, Yifan Zhang, Zhenguo Li, Xinchao Wang, Jiashi Feng |  |
| 624 |  |  [Subspace Regularizers for Few-Shot Class Incremental Learning](https://openreview.net/forum?id=boJy41J-tnQ) |  | 0 |  | Afra Feyza Akyürek, Ekin Akyürek, Derry Wijaya, Jacob Andreas |  |
| 625 |  |  [Using Graph Representation Learning with Schema Encoders to Measure the Severity of Depressive Symptoms](https://openreview.net/forum?id=OtEDS2NWhqa) |  | 0 |  | Simin Hong, Anthony G. Cohn, David Crossland Hogg |  |
| 626 |  |  [Actor-Critic Policy Optimization in a Large-Scale Imperfect-Information Game](https://openreview.net/forum?id=DTXZqTNV5nW) |  | 0 |  | Haobo Fu, Weiming Liu, Shuang Wu, Yijia Wang, Tao Yang, Kai Li, Junliang Xing, Bin Li, Bo Ma, Qiang Fu, Wei Yang |  |
| 627 |  |  [Policy Gradients Incorporating the Future](https://openreview.net/forum?id=EHaUTlm2eHg) |  | 0 |  | David Venuto, Elaine Lau, Doina Precup, Ofir Nachum |  |
| 628 |  |  [Gradient Information Matters in Policy Optimization by Back-propagating through Model](https://openreview.net/forum?id=rzvOQrnclO0) |  | 0 |  | Chongchong Li, Yue Wang, Wei Chen, Yuting Liu, ZhiMing Ma, TieYan Liu |  |
| 629 |  |  [VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning](https://openreview.net/forum?id=xm6YD62D1Ub) |  | 0 |  | Adrien Bardes, Jean Ponce, Yann LeCun |  |
| 630 |  |  [High Probability Generalization Bounds with Fast Rates for Minimax Problems](https://openreview.net/forum?id=gI7feJ9yXPz) |  | 0 |  | Shaojie Li, Yong Liu |  |
| 631 |  |  [SUMNAS: Supernet with Unbiased Meta-Features for Neural Architecture Search](https://openreview.net/forum?id=Z8FzvVU6_Kj) |  | 0 |  | Hyeonmin Ha, JiHoon Kim, Semin Park, ByungGon Chun |  |
| 632 |  |  [Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting](https://openreview.net/forum?id=_XNtisL32jv) |  | 0 |  | Shikuang Deng, Yuhang Li, Shanghang Zhang, Shi Gu |  |
| 633 |  |  [Reliable Adversarial Distillation with Unreliable Teachers](https://openreview.net/forum?id=u6TRGdzhfip) |  | 0 |  | Jianing Zhu, Jiangchao Yao, Bo Han, Jingfeng Zhang, Tongliang Liu, Gang Niu, Jingren Zhou, Jianliang Xu, Hongxia Yang |  |
| 634 |  |  [Neural Program Synthesis with Query](https://openreview.net/forum?id=NyJ2KIN8P17) |  | 0 |  | Di Huang, Rui Zhang, Xing Hu, Xishan Zhang, Pengwei Jin, Nan Li, Zidong Du, Qi Guo, Yunji Chen |  |
| 635 |  |  [Delaunay Component Analysis for Evaluation of Data Representations](https://openreview.net/forum?id=HTVch9AMPa) |  | 0 |  | Petra Poklukar, Vladislav Polianskii, Anastasiia Varava, Florian T. Pokorny, Danica Kragic |  |
| 636 |  |  [Visual hyperacuity with moving sensor and recurrent neural computations](https://openreview.net/forum?id=p0rCmDEN_-) |  | 0 |  | Alexander Rivkind, Or Ram, Eldad Assa, Michael Kreiserman, Ehud Ahissar |  |
| 637 |  |  [Partial Wasserstein Adversarial Network for Non-rigid Point Set Registration](https://openreview.net/forum?id=2ggNjUisGyr) |  | 0 |  | Ziming Wang, Nan Xue, Ling Lei, GuiSong Xia |  |
| 638 |  |  [Quantitative Performance Assessment of CNN Units via Topological Entropy Calculation](https://openreview.net/forum?id=xFOyMwWPkz) |  | 0 |  | Yang Zhao, Hao Zhang |  |
| 639 |  |  [Imitation Learning by Reinforcement Learning](https://openreview.net/forum?id=1zwleytEpYx) |  | 0 |  | Kamil Ciosek |  |
| 640 |  |  [On-Policy Model Errors in Reinforcement Learning](https://openreview.net/forum?id=81e1aeOt-sd) |  | 0 |  | Lukas P. Fröhlich, Maksym Lefarov, Melanie N. Zeilinger, Felix Berkenkamp |  |
| 641 |  |  [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://openreview.net/forum?id=O50443AsCP) |  | 0 |  | Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, JianGuang Lou |  |
| 642 |  |  [DARA: Dynamics-Aware Reward Augmentation in Offline Reinforcement Learning](https://openreview.net/forum?id=9SDQB3b68K) |  | 0 |  | Jinxin Liu, Hongyin Zhang, Donglin Wang |  |
| 643 |  |  [Explaining Point Processes by Learning Interpretable Temporal Logic Rules](https://openreview.net/forum?id=P07dq7iSAGr) |  | 0 |  | Shuang Li, Mingquan Feng, Lu Wang, Abdelmajid Essofi, Yufeng Cao, Junchi Yan, Le Song |  |
| 644 |  |  [On Robust Prefix-Tuning for Text Classification](https://openreview.net/forum?id=eBCmOocUejf) |  | 0 |  | Zonghan Yang, Yang Liu |  |
| 645 |  |  [Learning Graphon Mean Field Games and Approximate Nash Equilibria](https://openreview.net/forum?id=0sgntlpKDOz) |  | 0 |  | Kai Cui, Heinz Koeppl |  |
| 646 |  |  [Measuring CLEVRness: Black-box Testing of Visual Reasoning Models](https://openreview.net/forum?id=UtGtoS4CYU) |  | 0 |  | Spyridon Mouselinos, Henryk Michalewski, Mateusz Malinowski |  |
| 647 |  |  [Exploiting Class Activation Value for Partial-Label Learning](https://openreview.net/forum?id=qqdXHUGec9h) |  | 0 |  | Fei Zhang, Lei Feng, Bo Han, Tongliang Liu, Gang Niu, Tao Qin, Masashi Sugiyama |  |
| 648 |  |  [Givens Coordinate Descent Methods for Rotation Matrix Learning in Trainable Embedding Indexes](https://openreview.net/forum?id=9-Rfew334N) |  | 0 |  | Yunjiang Jiang, Han Zhang, Yiming Qiu, Yun Xiao, Bo Long, WenYun Yang |  |
| 649 |  |  [cosFormer: Rethinking Softmax In Attention](https://openreview.net/forum?id=Bl8CQrx2Up4) |  | 0 |  | Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, Yiran Zhong |  |
| 650 |  |  [FALCON: Fast Visual Concept Learning by Integrating Images, Linguistic descriptions, and Conceptual Relations](https://openreview.net/forum?id=htWIlvDcY8) |  | 0 |  | Lingjie Mei, Jiayuan Mao, Ziqi Wang, Chuang Gan, Joshua B. Tenenbaum |  |
| 651 |  |  [HyAR: Addressing Discrete-Continuous Action Reinforcement Learning via Hybrid Action Representation](https://openreview.net/forum?id=64trBbOhdGU) |  | 0 |  | Boyan Li, Hongyao Tang, Yan Zheng, Jianye Hao, Pengyi Li, Zhen Wang, Zhaopeng Meng, Li Wang |  |
| 652 |  |  [Transferable Adversarial Attack based on Integrated Gradients](https://openreview.net/forum?id=DesNW4-5ai9) |  | 0 |  | Yi Huang, Adams WaiKin Kong |  |
| 653 |  |  [How to deal with missing data in supervised deep learning?](https://openreview.net/forum?id=J7b4BCtDm4) |  | 0 |  | Niels Bruun Ipsen, PierreAlexandre Mattei, Jes Frellsen |  |
| 654 |  |  [Topological Graph Neural Networks](https://openreview.net/forum?id=oxxUMeFwEHd) |  | 0 |  | Max Horn, Edward De Brouwer, Michael Moor, Yves Moreau, Bastian Rieck, Karsten M. Borgwardt |  |
| 655 |  |  [Learning Value Functions from Undirected State-only Experience](https://openreview.net/forum?id=6Pe99Juo9gd) |  | 0 |  | Matthew Chang, Arjun Gupta, Saurabh Gupta |  |
| 656 |  |  [The Boltzmann Policy Distribution: Accounting for Systematic Suboptimality in Human Models](https://openreview.net/forum?id=_l_QjPGN5ye) |  | 0 |  | Cassidy Laidlaw, Anca D. Dragan |  |
| 657 |  |  [WeakM3D: Towards Weakly Supervised Monocular 3D Object Detection](https://openreview.net/forum?id=ahi2XSHpAUZ) |  | 0 |  | Liang Peng, Senbo Yan, Boxi Wu, Zheng Yang, Xiaofei He, Deng Cai |  |
| 658 |  |  [Exploring Memorization in Adversarial Training](https://openreview.net/forum?id=7gE9V9GBZaI) |  | 0 |  | Yinpeng Dong, Ke Xu, Xiao Yang, Tianyu Pang, Zhijie Deng, Hang Su, Jun Zhu |  |
| 659 |  |  [Disentanglement Analysis with Partial Information Decomposition](https://openreview.net/forum?id=pETy-HVvGtt) |  | 0 |  | Seiya Tokui, Issei Sato |  |
| 660 |  |  [Differentiable Gradient Sampling for Learning Implicit 3D Scene Reconstructions from a Single Image](https://openreview.net/forum?id=U8pbd00cCWB) |  | 0 |  | Shizhan Zhu, Sayna Ebrahimi, Angjoo Kanazawa, Trevor Darrell |  |
| 661 |  |  [Learning Continuous Environment Fields via Implicit Functions](https://openreview.net/forum?id=3ILxkQ7yElm) |  | 0 |  | Xueting Li, Shalini De Mello, Xiaolong Wang, MingHsuan Yang, Jan Kautz, Sifei Liu |  |
| 662 |  |  [Causal Contextual Bandits with Targeted Interventions](https://openreview.net/forum?id=F5Em8ASCosV) |  | 0 |  | Chandrasekar Subramanian, Balaraman Ravindran |  |
| 663 |  |  [Sound and Complete Neural Network Repair with Minimality and Locality Guarantees](https://openreview.net/forum?id=xS8AMYiEav3) |  | 0 |  | Feisi Fu, Wenchao Li |  |
| 664 |  |  [Blaschke Product Neural Networks (BPNN): A Physics-Infused Neural Network for Phase Retrieval of Meromorphic Functions](https://openreview.net/forum?id=JJxiD-kg-oK) |  | 0 |  | Juncheng Dong, Simiao Ren, Yang Deng, Omar Khatib, Jordan M. Malof, Mohammadreza Soltani, Willie Padilla, Vahid Tarokh |  |
| 665 |  |  [Automated Self-Supervised Learning for Graphs](https://openreview.net/forum?id=rFbR4Fv-D6-) |  | 0 |  | Wei Jin, Xiaorui Liu, Xiangyu Zhao, Yao Ma, Neil Shah, Jiliang Tang |  |
| 666 |  |  [Creating Training Sets via Weak Indirect Supervision](https://openreview.net/forum?id=m8uJvVgwRci) |  | 0 |  | Jieyu Zhang, Bohan Wang, Xiangchen Song, Yujing Wang, Yaming Yang, Jing Bai, Alexander Ratner |  |
| 667 |  |  [Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs](https://openreview.net/forum?id=aTzMi4yV_RO) |  | 0 |  | Jaewoong Choi, Junho Lee, Changyeon Yoon, Jung Ho Park, Geonho Hwang, Myungjoo Kang |  |
| 668 |  |  [GradSign: Model Performance Inference with Theoretical Insights](https://openreview.net/forum?id=HObMhrCeAAF) |  | 0 |  | Zhihao Zhang, Zhihao Jia |  |
| 669 |  |  [You are AllSet: A Multiset Function Framework for Hypergraph Neural Networks](https://openreview.net/forum?id=hpBTIv2uy_E) |  | 0 |  | Eli Chien, Chao Pan, Jianhao Peng, Olgica Milenkovic |  |
| 670 |  |  [Synchromesh: Reliable Code Generation from Pre-trained Language Models](https://openreview.net/forum?id=KmtVD97J43e) |  | 0 |  | Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, Sumit Gulwani |  |
| 671 |  |  [Learning curves for continual learning in neural networks: Self-knowledge transfer and forgetting](https://openreview.net/forum?id=tFgdrQbbaa) |  | 0 |  | Ryo Karakida, Shotaro Akaho |  |
| 672 |  |  [Energy-Based Learning for Cooperative Games, with Applications to Valuation Problems in Machine Learning](https://openreview.net/forum?id=xLfAgCroImw) |  | 0 |  | Yatao Bian, Yu Rong, Tingyang Xu, Jiaxiang Wu, Andreas Krause, Junzhou Huang |  |
| 673 |  |  [Pessimistic Model-based Offline Reinforcement Learning under Partial Coverage](https://openreview.net/forum?id=tyrJsbKAe6) |  | 0 |  | Masatoshi Uehara, Wen Sun |  |
| 674 |  |  [Cold Brew: Distilling Graph Node Representations with Incomplete or Missing Neighborhoods](https://openreview.net/forum?id=1ugNpm7W6E) |  | 0 |  | Wenqing Zheng, Edward W. Huang, Nikhil Rao, Sumeet Katariya, Zhangyang Wang, Karthik Subbian |  |
| 675 |  |  [NASI: Label- and Data-agnostic Neural Architecture Search at Initialization](https://openreview.net/forum?id=v-v1cpNNK_v) |  | 0 |  | Yao Shu, Shaofeng Cai, Zhongxiang Dai, Beng Chin Ooi, Bryan Kian Hsiang Low |  |
| 676 |  |  [How to Train Your MAML to Excel in Few-Shot Classification](https://openreview.net/forum?id=49h_IkpJtaE) |  | 0 |  | HanJia Ye, WeiLun Chao |  |
| 677 |  |  [Communication-Efficient Actor-Critic Methods for Homogeneous Markov Games](https://openreview.net/forum?id=xy_2w3J3kH) |  | 0 |  | Dingyang Chen, Yile Li, Qi Zhang |  |
| 678 |  |  [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://openreview.net/forum?id=vh-0sUt8HlG) |  | 0 |  | Sachin Mehta, Mohammad Rastegari |  |
| 679 |  |  [Spatial Graph Attention and Curiosity-driven Policy for Antiviral Drug Discovery](https://openreview.net/forum?id=kavTY__jxp) |  | 0 |  | Yulun Wu, Nicholas Choma, Andrew Deru Chen, Mikaela Cashman, Érica Teixeira Prates, Verónica G. Melesse Vergara, Manesh Shah, Austin Clyde, Thomas S. Brettin, Wibe Albert de Jong, Neeraj Kumar, Martha S. Head, Rick L. Stevens, Peter Nugent, Daniel A. Jacobson, James B. Brown |  |
| 680 |  |  [Surrogate NAS Benchmarks: Going Beyond the Limited Search Spaces of Tabular NAS Benchmarks](https://openreview.net/forum?id=OnpFa95RVqs) |  | 0 |  | Arber Zela, Julien Niklas Siems, Lucas Zimmer, Jovita Lukasik, Margret Keuper, Frank Hutter |  |
| 681 |  |  [Certified Robustness for Deep Equilibrium Models via Interval Bound Propagation](https://openreview.net/forum?id=y1PXylgrXZ) |  | 0 |  | Colin Wei, J. Zico Kolter |  |
| 682 |  |  [Crystal Diffusion Variational Autoencoder for Periodic Material Generation](https://openreview.net/forum?id=03RLpj-tc_) |  | 0 |  | Tian Xie, Xiang Fu, OctavianEugen Ganea, Regina Barzilay, Tommi S. Jaakkola |  |
| 683 |  |  [Task Affinity with Maximum Bipartite Matching in Few-Shot Learning](https://openreview.net/forum?id=u2GZOiUTbt) |  | 0 |  | Cat Phuoc Le, Juncheng Dong, Mohammadreza Soltani, Vahid Tarokh |  |
| 684 |  |  [Latent Image Animator: Learning to Animate Images via Latent Space Navigation](https://openreview.net/forum?id=7r6kDq0mK_) |  | 0 |  | Yaohui Wang, Di Yang, François Brémond, Antitza Dantcheva |  |
| 685 |  |  [Know Thyself: Transferable Visual Control Policies Through Robot-Awareness](https://openreview.net/forum?id=o0ehFykKVtr) |  | 0 |  | Edward S. Hu, Kun Huang, Oleh Rybkin, Dinesh Jayaraman |  |
| 686 |  |  [Node Feature Extraction by Self-Supervised Multi-scale Neighborhood Prediction](https://openreview.net/forum?id=KJggliHbs8) |  | 0 |  | Eli Chien, WeiCheng Chang, ChoJui Hsieh, HsiangFu Yu, Jiong Zhang, Olgica Milenkovic, Inderjit S. Dhillon |  |
| 687 |  |  [Spherical Message Passing for 3D Molecular Graphs](https://openreview.net/forum?id=givsRXsOt9r) |  | 0 |  | Yi Liu, Limei Wang, Meng Liu, Yuchao Lin, Xuan Zhang, Bora Oztekin, Shuiwang Ji |  |
| 688 |  |  [Fairness Guarantees under Demographic Shift](https://openreview.net/forum?id=wbPObLm6ueA) |  | 0 |  | Stephen Giguere, Blossom Metevier, Bruno Castro da Silva, Yuriy Brun, Philip S. Thomas, Scott Niekum |  |
| 689 |  |  [Fooling Explanations in Text Classifiers](https://openreview.net/forum?id=j3krplz_4w6) |  | 0 |  | Adam Ivankay, Ivan Girardi, Chiara Marchiori, Pascal Frossard |  |
| 690 |  |  [On the Learning and Learnability of Quasimetrics](https://openreview.net/forum?id=y0VvIg25yk) |  | 0 |  | Tongzhou Wang, Phillip Isola |  |
| 691 |  |  [Learning Prototype-oriented Set Representations for Meta-Learning](https://openreview.net/forum?id=WH6u2SvlLp4) |  | 0 |  | Dandan Guo, Long Tian, Minghe Zhang, Mingyuan Zhou, Hongyuan Zha |  |
| 692 |  |  [Embedded-model flows: Combining the inductive biases of model-free deep learning and explicit probabilistic modeling](https://openreview.net/forum?id=9pEJSVfDbba) |  | 0 |  | Gianluigi Silvestri, Emily Fertig, Dave Moore, Luca Ambrogioni |  |
| 693 |  |  [A Relational Intervention Approach for Unsupervised Dynamics Generalization in Model-Based Reinforcement Learning](https://openreview.net/forum?id=YRq0ZUnzKoZ) |  | 0 |  | Jiaxian Guo, Mingming Gong, Dacheng Tao |  |
| 694 |  |  [Critical Points in Quantum Generative Models](https://openreview.net/forum?id=2f1z55GVQN) |  | 0 |  | Eric Ricardo Anschütz |  |
| 695 |  |  [VOS: Learning What You Don't Know by Virtual Outlier Synthesis](https://openreview.net/forum?id=TW7d65uYu5M) |  | 0 |  | Xuefeng Du, Zhaoning Wang, Mu Cai, Yixuan Li |  |
| 696 |  |  [Trust Region Policy Optimisation in Multi-Agent Reinforcement Learning](https://openreview.net/forum?id=EcGGFkNTxdJ) |  | 0 |  | Jakub Grudzien Kuba, Ruiqing Chen, Muning Wen, Ying Wen, Fanglei Sun, Jun Wang, Yaodong Yang |  |
| 697 |  |  [Unsupervised Disentanglement with Tensor Product Representations on the Torus](https://openreview.net/forum?id=neqU3HWDgE) |  | 0 |  | Michael Rotman, Amit Dekel, Shir Gur, Yaron Oz, Lior Wolf |  |
| 698 |  |  [Anomaly Detection for Tabular Data with Internal Contrastive Learning](https://openreview.net/forum?id=_hszZbt46bT) |  | 0 |  | Tom Shenkar, Lior Wolf |  |
| 699 |  |  [LIGS: Learnable Intrinsic-Reward Generation Selection for Multi-Agent Learning](https://openreview.net/forum?id=CpTuR2ECuW) |  | 0 |  | David Henry Mguni, Taher Jafferjee, Jianhong Wang, Nicolas Perez Nieves, Oliver Slumbers, Feifei Tong, Yang Li, Jiangcheng Zhu, Yaodong Yang, Jun Wang |  |
| 700 |  |  [Bayesian Modeling and Uncertainty Quantification for Learning to Optimize: What, Why, and How](https://openreview.net/forum?id=EVVadRFRgL7) |  | 0 |  | Yuning You, Yue Cao, Tianlong Chen, Zhangyang Wang, Yang Shen |  |
| 701 |  |  [Deep Ensembling with No Overhead for either Training or Testing: The All-Round Blessings of Dynamic Sparsity](https://openreview.net/forum?id=RLtqs6pzj1-) |  | 0 |  | Shiwei Liu, Tianlong Chen, Zahra Atashgahi, Xiaohan Chen, Ghada Sokar, Elena Mocanu, Mykola Pechenizkiy, Zhangyang Wang, Decebal Constantin Mocanu |  |
| 702 |  |  [HyperDQN: A Randomized Exploration Method for Deep Reinforcement Learning](https://openreview.net/forum?id=X0nrKAXu7g-) |  | 0 |  | Ziniu Li, Yingru Li, Yushun Zhang, Tong Zhang, ZhiQuan Luo |  |
| 703 |  |  [Unraveling Model-Agnostic Meta-Learning via The Adaptation Learning Rate](https://openreview.net/forum?id=3rULBvOJ8D2) |  | 0 |  | Yingtian Zou, Fusheng Liu, Qianxiao Li |  |
| 704 |  |  [iFlood: A Stable and Effective Regularizer](https://openreview.net/forum?id=MsHnJPaBUZE) |  | 0 |  | Yuexiang Xie, Zhen Wang, Yaliang Li, Ce Zhang, Jingren Zhou, Bolin Ding |  |
| 705 |  |  [FlexConv: Continuous Kernel Convolutions With Differentiable Kernel Sizes](https://openreview.net/forum?id=3jooF27-0Wy) |  | 0 |  | David W. Romero, RobertJan Bruintjes, Jakub Mikolaj Tomczak, Erik J. Bekkers, Mark Hoogendoorn, Jan van Gemert |  |
| 706 |  |  [Zero Pixel Directional Boundary by Vector Transform](https://openreview.net/forum?id=nxcABL7jbQh) |  | 0 |  | Edoardo Mello Rella, Ajad Chhatkuli, Yun Liu, Ender Konukoglu, Luc Van Gool |  |
| 707 |  |  [A Conditional Point Diffusion-Refinement Paradigm for 3D Point Cloud Completion](https://openreview.net/forum?id=wqD6TfbYkrn) |  | 0 |  | Zhaoyang Lyu, Zhifeng Kong, Xudong Xu, Liang Pan, Dahua Lin |  |
| 708 |  |  [Auto-Transfer: Learning to Route Transferable Representations](https://openreview.net/forum?id=SIKV0_MrZlr) |  | 0 |  | Keerthiram Murugesan, Vijay Sadashivaiah, Ronny Luss, Karthikeyan Shanmugam, PinYu Chen, Amit Dhurandhar |  |
| 709 |  |  [PoNet: Pooling Network for Efficient Token Mixing in Long Sequences](https://openreview.net/forum?id=9jInD9JjicF) |  | 0 |  | ChaoHong Tan, Qian Chen, Wen Wang, Qinglin Zhang, Siqi Zheng, ZhenHua Ling |  |
| 710 |  |  [Huber Additive Models for Non-stationary Time Series Analysis](https://openreview.net/forum?id=9kpuB2bgnim) |  | 0 |  | Yingjie Wang, Xianrui Zhong, Fengxiang He, Hong Chen, Dacheng Tao |  |
| 711 |  |  [Model-augmented Prioritized Experience Replay](https://openreview.net/forum?id=WuEiafqdy9H) |  | 0 |  | Youngmin Oh, Jinwoo Shin, Eunho Yang, Sung Ju Hwang |  |
| 712 |  |  [Post-Training Detection of Backdoor Attacks for Two-Class and Multi-Attack Scenarios](https://openreview.net/forum?id=MSgB8D4Hy51) |  | 0 |  | Zhen Xiang, David J. Miller, George Kesidis |  |
| 713 |  |  [Multi-Task Processes](https://openreview.net/forum?id=9otKVlgrpZG) |  | 0 |  | Donggyun Kim, Seongwoong Cho, Wonkwang Lee, Seunghoon Hong |  |
| 714 |  |  [Dynamic Token Normalization improves Vision Transformers](https://openreview.net/forum?id=f9MHpAGUyMn) |  | 0 |  | Wenqi Shao, Yixiao Ge, Zhaoyang Zhang, Xuyuan Xu, Xiaogang Wang, Ying Shan, Ping Luo |  |
| 715 |  |  [Symbolic Learning to Optimize: Towards Interpretability and Scalability](https://openreview.net/forum?id=ef0nInZHKIC) |  | 0 |  | Wenqing Zheng, Tianlong Chen, TingKuei Hu, Zhangyang Wang |  |
| 716 |  |  [Sequential Reptile: Inter-Task Gradient Alignment for Multilingual Learning](https://openreview.net/forum?id=ivQruZvXxtz) |  | 0 |  | Seanie Lee, Haebeom Lee, Juho Lee, Sung Ju Hwang |  |
| 717 |  |  [Pseudo Numerical Methods for Diffusion Models on Manifolds](https://openreview.net/forum?id=PlKWVd2yBkY) |  | 0 |  | Luping Liu, Yi Ren, Zhijie Lin, Zhou Zhao |  |
| 718 |  |  [Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm](https://openreview.net/forum?id=zq1iJkNk3uN) |  | 0 |  | Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, Junjie Yan |  |
| 719 |  |  [Environment Predictive Coding for Visual Navigation](https://openreview.net/forum?id=DBiQQYWykyy) |  | 0 |  | Santhosh Kumar Ramakrishnan, Tushar Nagarajan, Ziad AlHalah, Kristen Grauman |  |
| 720 |  |  [Topological Experience Replay](https://openreview.net/forum?id=OXRZeMmOI7a) |  | 0 |  | ZhangWei Hong, Tao Chen, YenChen Lin, Joni Pajarinen, Pulkit Agrawal |  |
| 721 |  |  [Sparsity Winning Twice: Better Robust Generalization from More Efficient Training](https://openreview.net/forum?id=SYuJXrXq8tw) |  | 0 |  | Tianlong Chen, Zhenyu Zhang, Pengjun Wang, Santosh Balachandra, Haoyu Ma, Zehao Wang, Zhangyang Wang |  |
| 722 |  |  [CrossMatch: Cross-Classifier Consistency Regularization for Open-Set Single Domain Generalization](https://openreview.net/forum?id=48RBsJwGkJf) |  | 0 |  | Ronghang Zhu, Sheng Li |  |
| 723 |  |  [Robust Unlearnable Examples: Protecting Data Privacy Against Adversarial Learning](https://openreview.net/forum?id=baUQQPwQiAg) |  | 0 |  | Shaopeng Fu, Fengxiang He, Yang Liu, Li Shen, Dacheng Tao |  |
| 724 |  |  [A Non-Parametric Regression Viewpoint : Generalization of Overparametrized Deep RELU Network Under Noisy Observations](https://openreview.net/forum?id=bZJbzaj_IlP) |  | 0 |  | Namjoon Suh, Hyunouk Ko, Xiaoming Huo |  |
| 725 |  |  [Active Hierarchical Exploration with Stable Subgoal Representation Learning](https://openreview.net/forum?id=sNuFKTMktcY) |  | 0 |  | Siyuan Li, Jin Zhang, Jianhao Wang, Yang Yu, Chongjie Zhang |  |
| 726 |  |  [Deep AutoAugment](https://openreview.net/forum?id=St-53J9ZARf) |  | 0 |  | Yu Zheng, Zhi Zhang, Shen Yan, Mi Zhang |  |
| 727 |  |  [Temporal Alignment Prediction for Supervised Representation Learning and Few-Shot Sequence Classification](https://openreview.net/forum?id=p3DKPQ7uaAi) |  | 0 |  | Bing Su, JiRong Wen |  |
| 728 |  |  [Anti-Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis: From Theory to Practice](https://openreview.net/forum?id=O476oWmiNNp) |  | 0 |  | Peihao Wang, Wenqing Zheng, Tianlong Chen, Zhangyang Wang |  |
| 729 |  |  [Self-ensemble Adversarial Training for Improved Robustness](https://openreview.net/forum?id=oU3aTsmeRQV) |  | 0 |  | Hongjun Wang, Yisen Wang |  |
| 730 |  |  [Do deep networks transfer invariances across classes?](https://openreview.net/forum?id=Fn7i_r5rR0q) |  | 0 |  | Allan Zhou, Fahim Tajwar, Alexander Robey, Tom Knowles, George J. Pappas, Hamed Hassani, Chelsea Finn |  |
| 731 |  |  [Cross-Trajectory Representation Learning for Zero-Shot Generalization in RL](https://openreview.net/forum?id=XOh5x-vxsrV) |  | 0 |  | Bogdan Mazoure, Ahmed M. Ahmed, R. Devon Hjelm, Andrey Kolobov, Patrick MacAlpine |  |
| 732 |  |  [On Covariate Shift of Latent Confounders in Imitation and Reinforcement Learning](https://openreview.net/forum?id=w01vBAcewNX) |  | 0 |  | Guy Tennenholtz, Assaf Hallak, Gal Dalal, Shie Mannor, Gal Chechik, Uri Shalit |  |
| 733 |  |  [RvS: What is Essential for Offline RL via Supervised Learning?](https://openreview.net/forum?id=S874XAIpkR-) |  | 0 |  | Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, Sergey Levine |  |
| 734 |  |  [Learning Guarantees for Graph Convolutional Networks on the Stochastic Block Model](https://openreview.net/forum?id=dpXL6lz4mOQ) |  | 0 |  | Wei Lu |  |
| 735 |  |  [Learning Versatile Neural Architectures by Propagating Network Codes](https://openreview.net/forum?id=KEQl-MZ5fg7) |  | 0 |  | Mingyu Ding, Yuqi Huo, Haoyu Lu, Linjie Yang, Zhe Wang, Zhiwu Lu, Jingdong Wang, Ping Luo |  |
| 736 |  |  [Task-Induced Representation Learning](https://openreview.net/forum?id=OzyXtIZAzFv) |  | 0 |  | Jun Yamada, Karl Pertsch, Anisha Gunjal, Joseph J. Lim |  |
| 737 |  |  [Graph-based Nearest Neighbor Search in Hyperbolic Spaces](https://openreview.net/forum?id=USIgIY6TNDe) |  | 0 |  | Liudmila Prokhorenkova, Dmitry Baranchuk, Nikolay Bogachev, Yury Demidovich, Alexander Kolpakov |  |
| 738 |  |  [Generative Models as a Data Source for Multiview Representation Learning](https://openreview.net/forum?id=qhAeZjs7dCL) |  | 0 |  | Ali Jahanian, Xavier Puig, Yonglong Tian, Phillip Isola |  |
| 739 |  |  [GiraffeDet: A Heavy-Neck Paradigm for Object Detection](https://openreview.net/forum?id=cBu4ElJfneV) |  | 0 |  | Yiqi Jiang, Zhiyu Tan, Junyan Wang, Xiuyu Sun, Ming C. Lin, Hao Li |  |
| 740 |  |  [A Unified Wasserstein Distributional Robustness Framework for Adversarial Training](https://openreview.net/forum?id=Dzpe9C1mpiv) |  | 0 |  | Anh Tuan Bui, Trung Le, Quan Hung Tran, He Zhao, Dinh Q. Phung |  |
| 741 |  |  [miniF2F: a cross-system benchmark for formal Olympiad-level mathematics](https://openreview.net/forum?id=9ZPegFuFTFv) |  | 0 |  | Kunhao Zheng, Jesse Michael Han, Stanislas Polu |  |
| 742 |  |  [Towards Model Agnostic Federated Learning Using Knowledge Distillation](https://openreview.net/forum?id=lQI_mZjvBxj) |  | 0 |  | Andrei Afonin, Sai Praneeth Karimireddy |  |
| 743 |  |  [Acceleration of Federated Learning with Alleviated Forgetting in Local Training](https://openreview.net/forum?id=541PxiEKN3F) |  | 0 |  | Chencheng Xu, Zhiwei Hong, Minlie Huang, Tao Jiang |  |
| 744 |  |  [Discovering Invariant Rationales for Graph Neural Networks](https://openreview.net/forum?id=hGXij5rfiHw) |  | 0 |  | Yingxin Wu, Xiang Wang, An Zhang, Xiangnan He, TatSeng Chua |  |
| 745 |  |  [Representing Mixtures of Word Embeddings with Mixtures of Topic Embeddings](https://openreview.net/forum?id=IYMuTbGzjFU) |  | 0 |  | Dongsheng Wang, Dandan Guo, He Zhao, Huangjie Zheng, Korawat Tanwisuth, Bo Chen, Mingyuan Zhou |  |
| 746 |  |  [Generative Modeling with Optimal Transport Maps](https://openreview.net/forum?id=5JdLZg346Lw) |  | 0 |  | Litu Rout, Alexander Korotin, Evgeny Burnaev |  |
| 747 |  |  [Focus on the Common Good: Group Distributional Robustness Follows](https://openreview.net/forum?id=irARV_2VFs4) |  | 0 |  | Vihari Piratla, Praneeth Netrapalli, Sunita Sarawagi |  |
| 748 |  |  [Omni-Scale CNNs: a simple and effective kernel size configuration for time series classification](https://openreview.net/forum?id=PDYs7Z2XFGv) |  | 0 |  | Wensi Tang, Guodong Long, Lu Liu, Tianyi Zhou, Michael Blumenstein, Jing Jiang |  |
| 749 |  |  [Ada-NETS: Face Clustering via Adaptive Neighbour Discovery in the Structure Space](https://openreview.net/forum?id=QJWVP4CTmW4) |  | 0 |  | Yaohua Wang, Yaobin Zhang, Fangyi Zhang, Senzhang Wang, Ming Lin, YuQi Zhang, Xiuyu Sun |  |
| 750 |  |  [Decoupled Adaptation for Cross-Domain Object Detection](https://openreview.net/forum?id=VNqaB1g9393) |  | 0 |  | Junguang Jiang, Baixu Chen, Jianmin Wang, Mingsheng Long |  |
| 751 |  |  [Rethinking Network Design and Local Geometry in Point Cloud: A Simple Residual MLP Framework](https://openreview.net/forum?id=3Pbra-_u76D) |  | 0 |  | Xu Ma, Can Qin, Haoxuan You, Haoxi Ran, Yun Fu |  |
| 752 |  |  [New Insights on Reducing Abrupt Representation Change in Online Continual Learning](https://openreview.net/forum?id=N8MaByOzUfb) |  | 0 |  | Lucas Caccia, Rahaf Aljundi, Nader Asadi, Tinne Tuytelaars, Joelle Pineau, Eugene Belilovsky |  |
| 753 |  |  [Demystifying Batch Normalization in ReLU Networks: Equivalent Convex Optimization Models and Implicit Regularization](https://openreview.net/forum?id=6XGgutacQ0B) |  | 0 |  | Tolga Ergen, Arda Sahiner, Batu Ozturkler, John M. Pauly, Morteza Mardani, Mert Pilanci |  |
| 754 |  |  [Associated Learning: an Alternative to End-to-End Backpropagation that Works on CNN, RNN, and Transformer](https://openreview.net/forum?id=4N-17dske79) |  | 0 |  | Dennis Y. H. Wu, Dinan Lin, Vincent Chen, HungHsuan Chen |  |
| 755 |  |  [MetaShift: A Dataset of Datasets for Evaluating Contextual Distribution Shifts and Training Conflicts](https://openreview.net/forum?id=MTex8qKavoS) |  | 0 |  | Weixin Liang, James Zou |  |
| 756 |  |  [FP-DETR: Detection Transformer Advanced by Fully Pre-training](https://openreview.net/forum?id=yjMQuLLcGWK) |  | 0 |  | Wen Wang, Yang Cao, Jing Zhang, Dacheng Tao |  |
| 757 |  |  [Efficient and Differentiable Conformal Prediction with General Function Classes](https://openreview.net/forum?id=Ht85_jyihxp) |  | 0 |  | Yu Bai, Song Mei, Huan Wang, Yingbo Zhou, Caiming Xiong |  |
| 758 |  |  [Safe Neurosymbolic Learning with Differentiable Symbolic Execution](https://openreview.net/forum?id=NYBmJN4MyZ) |  | 0 |  | Chenxi Yang, Swarat Chaudhuri |  |
| 759 |  |  [SimVLM: Simple Visual Language Model Pretraining with Weak Supervision](https://openreview.net/forum?id=GUrhfTuf_3) |  | 0 |  | Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, Yuan Cao |  |
| 760 |  |  [Bundle Networks: Fiber Bundles, Local Trivializations, and a Generative Approach to Exploring Many-to-one Maps](https://openreview.net/forum?id=aBXzcPPOuX) |  | 0 |  | Nico Courts, Henry Kvinge |  |
| 761 |  |  [Privacy Implications of Shuffling](https://openreview.net/forum?id=5i2f-aR6B8H) |  | 0 |  | Casey Meehan, Amrita Roy Chowdhury, Kamalika Chaudhuri, Somesh Jha |  |
| 762 |  |  [On the role of population heterogeneity in emergent communication](https://openreview.net/forum?id=5Qkd7-bZfI) |  | 0 |  | Mathieu Rita, Florian Strub, JeanBastien Grill, Olivier Pietquin, Emmanuel Dupoux |  |
| 763 |  |  [Hindsight is 20/20: Leveraging Past Traversals to Aid 3D Perception](https://openreview.net/forum?id=qsZoGvFiJn1) |  | 0 |  | Yurong You, Katie Z. Luo, Xiangyu Chen, Junan Chen, WeiLun Chao, Wen Sun, Bharath Hariharan, Mark E. Campbell, Kilian Q. Weinberger |  |
| 764 |  |  [Language-driven Semantic Segmentation](https://openreview.net/forum?id=RriDjddCLN) |  | 0 |  | Boyi Li, Kilian Q. Weinberger, Serge J. Belongie, Vladlen Koltun, René Ranftl |  |
| 765 |  |  [Image BERT Pre-training with Online Tokenizer](https://openreview.net/forum?id=ydopy-e6Dg) |  | 0 |  | Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan L. Yuille, Tao Kong |  |
| 766 |  |  [Accelerated Policy Learning with Parallel Differentiable Simulation](https://openreview.net/forum?id=ZSKRQMvttc) |  | 0 |  | Jie Xu, Viktor Makoviychuk, Yashraj Narang, Fabio Ramos, Wojciech Matusik, Animesh Garg, Miles Macklin |  |
| 767 |  |  [Do We Need Anisotropic Graph Neural Networks?](https://openreview.net/forum?id=hl9ePdHO4_s) |  | 0 |  | Shyam A. Tailor, Felix L. Opolka, Pietro Liò, Nicholas Donald Lane |  |
| 768 |  |  [Is High Variance Unavoidable in RL? A Case Study in Continuous Control](https://openreview.net/forum?id=9xhgmsNVHu) |  | 0 |  | Johan Bjorck, Carla P. Gomes, Kilian Q. Weinberger |  |
| 769 |  |  [Simple GNN Regularisation for 3D Molecular Property Prediction and Beyond](https://openreview.net/forum?id=1wVvweK3oIb) |  | 0 |  | Jonathan Godwin, Michael Schaarschmidt, Alexander L. Gaunt, Alvaro SanchezGonzalez, Yulia Rubanova, Petar Velickovic, James Kirkpatrick, Peter W. Battaglia |  |
| 770 |  |  [Should We Be Pre-training? An Argument for End-task Aware Training as an Alternative](https://openreview.net/forum?id=2bO2x8NAIMB) |  | 0 |  | Lucio M. Dery, Paul Michel, Ameet Talwalkar, Graham Neubig |  |
| 771 |  |  [Learning Super-Features for Image Retrieval](https://openreview.net/forum?id=wogsFPHwftY) |  | 0 |  | Philippe Weinzaepfel, Thomas Lucas, Diane Larlus, Yannis Kalantidis |  |
| 772 |  |  [Online Facility Location with Predictions](https://openreview.net/forum?id=DSQHjibtgKR) |  | 0 |  | Shaofeng H.C. Jiang, Erzhi Liu, You Lyu, Zhihao Gavin Tang, Yubo Zhang |  |
| 773 |  |  [Few-Shot Backdoor Attacks on Visual Object Tracking](https://openreview.net/forum?id=qSV5CuSaK_a) |  | 0 |  | Yiming Li, Haoxiang Zhong, Xingjun Ma, Yong Jiang, ShuTao Xia |  |
| 774 |  |  [Backdoor Defense via Decoupling the Training Process](https://openreview.net/forum?id=TySnJ-0RdKI) |  | 0 |  | Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, Kui Ren |  |
| 775 |  |  [Learning to Complete Code with Sketches](https://openreview.net/forum?id=q79uMSC6ZBT) |  | 0 |  | Daya Guo, Alexey Svyatkovskiy, Jian Yin, Nan Duan, Marc Brockschmidt, Miltiadis Allamanis |  |
| 776 |  |  [Reverse Engineering of Imperceptible Adversarial Image Perturbations](https://openreview.net/forum?id=gpp7cf0xdfN) |  | 0 |  | Yifan Gong, Yuguang Yao, Yize Li, Yimeng Zhang, Xiaoming Liu, Xue Lin, Sijia Liu |  |
| 777 |  |  [DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR](https://openreview.net/forum?id=oMI9PjOb9Jl) |  | 0 |  | Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, Lei Zhang |  |
| 778 |  |  [On the Certified Robustness for Ensemble Models and Beyond](https://openreview.net/forum?id=tUa4REjGjTf) |  | 0 |  | Zhuolin Yang, Linyi Li, Xiaojun Xu, Bhavya Kailkhura, Tao Xie, Bo Li |  |
| 779 |  |  [Efficient Neural Causal Discovery without Acyclicity Constraints](https://openreview.net/forum?id=eYciPrLuUhG) |  | 0 |  | Phillip Lippe, Taco Cohen, Efstratios Gavves |  |
| 780 |  |  [Pseudo-Labeled Auto-Curriculum Learning for Semi-Supervised Keypoint Localization](https://openreview.net/forum?id=6Q52pZ-Th7N) |  | 0 |  | Can Wang, Sheng Jin, Yingda Guan, Wentao Liu, Chen Qian, Ping Luo, Wanli Ouyang |  |
| 781 |  |  [Signing the Supermask: Keep, Hide, Invert](https://openreview.net/forum?id=e0jtGTfPihs) |  | 0 |  | Nils Koster, Oliver Grothe, Achim Rettinger |  |
| 782 |  |  [Bootstrapping Semantic Segmentation with Regional Contrast](https://openreview.net/forum?id=6u6N8WWwYSM) |  | 0 |  | Shikun Liu, Shuaifeng Zhi, Edward Johns, Andrew J. Davison |  |
| 783 |  |  [Generative Principal Component Analysis](https://openreview.net/forum?id=pgir5f7ekAL) |  | 0 |  | Zhaoqiang Liu, Jiulong Liu, Subhroshekhar Ghosh, Jun Han, Jonathan Scarlett |  |
| 784 |  |  [Pareto Policy Pool for Model-based Offline Reinforcement Learning](https://openreview.net/forum?id=OqcZu8JIIzS) |  | 0 |  | Yijun Yang, Jing Jiang, Tianyi Zhou, Jie Ma, Yuhui Shi |  |
| 785 |  |  [Filling the G_ap_s: Multivariate Time Series Imputation by Graph Neural Networks](https://openreview.net/forum?id=kOu3-S3wJ7) |  | 0 |  | Andrea Cini, Ivan Marisca, Cesare Alippi |  |
| 786 |  |  [An Unconstrained Layer-Peeled Perspective on Neural Collapse](https://openreview.net/forum?id=WZ3yjh8coDg) |  | 0 |  | Wenlong Ji, Yiping Lu, Yiliang Zhang, Zhun Deng, Weijie J. Su |  |
| 787 |  |  [Contrastive Clustering to Mine Pseudo Parallel Data for Unsupervised Translation](https://openreview.net/forum?id=pN1JOdrSY9) |  | 0 |  | XuanPhi Nguyen, Hongyu Gong, Yun Tang, Changhan Wang, Philipp Koehn, Shafiq R. Joty |  |
| 788 |  |  [Multimeasurement Generative Models](https://openreview.net/forum?id=QRX0nCX_gk) |  | 0 |  | Saeed Saremi, Rupesh Kumar Srivastava |  |
| 789 |  |  [Information Gain Propagation: a New Way to Graph Active Learning with Soft Labels](https://openreview.net/forum?id=USC0-nvGPK) |  | 0 |  | Wentao Zhang, Yexin Wang, Zhenbang You, Meng Cao, Ping Huang, Jiulong Shan, Zhi Yang, Bin Cui |  |
| 790 |  |  [Constructing Orthogonal Convolutions in an Explicit Manner](https://openreview.net/forum?id=Zr5W2LSRhD) |  | 0 |  | Tan Yu, Jun Li, Yunfeng Cai, Ping Li |  |
| 791 |  |  [X-model: Improving Data Efficiency in Deep Learning with A Minimax Model](https://openreview.net/forum?id=P3Bh01hBYTH) |  | 0 |  | Ximei Wang, Xinyang Chen, Jianmin Wang, Mingsheng Long |  |
| 792 |  |  [Stein Latent Optimization for Generative Adversarial Networks](https://openreview.net/forum?id=2-mkiUs9Jx7) |  | 0 |  | Uiwon Hwang, Heeseung Kim, Dahuin Jung, Hyemi Jang, Hyungyu Lee, Sungroh Yoon |  |
| 793 |  |  [Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity](https://openreview.net/forum?id=RRGVCN8kjim) |  | 0 |  | Byungseok Roh, Jaewoong Shin, Wuhyun Shin, Saehoon Kim |  |
| 794 |  |  [Online Target Q-learning with Reverse Experience Replay: Efficiently finding the Optimal Policy for Linear MDPs](https://openreview.net/forum?id=HMJdXzbWKH) |  | 0 |  | Naman Agarwal, Syomantak Chaudhuri, Prateek Jain, Dheeraj Mysore Nagaraj, Praneeth Netrapalli |  |
| 795 |  |  [Differentially Private Fractional Frequency Moments Estimation with Polylogarithmic Space](https://openreview.net/forum?id=7I8LPkcx8V) |  | 0 |  | Lun Wang, Iosif Pinelis, Dawn Song |  |
| 796 |  |  [How Low Can We Go: Trading Memory for Error in Low-Precision Training](https://openreview.net/forum?id=YpSxqy_RE84) |  | 0 |  | Chengrun Yang, Ziyang Wu, Jerry Chee, Christopher De Sa, Madeleine Udell |  |
| 797 |  |  [In a Nutshell, the Human Asked for This: Latent Goals for Following Temporal Specifications](https://openreview.net/forum?id=rUwm9wCjURV) |  | 0 |  | Borja G. León, Murray Shanahan, Francesco Belardinelli |  |
| 798 |  |  [Discrete Representations Strengthen Vision Transformer Robustness](https://openreview.net/forum?id=8hWs60AZcWk) |  | 0 |  | Chengzhi Mao, Lu Jiang, Mostafa Dehghani, Carl Vondrick, Rahul Sukthankar, Irfan Essa |  |
| 799 |  |  [On the Convergence of the Monte Carlo Exploring Starts Algorithm for Reinforcement Learning](https://openreview.net/forum?id=JzNB0eA2-M4) |  | 0 |  | Che Wang, Shuhan Yuan, Kai Shao, Keith W. Ross |  |
| 800 |  |  [Concurrent Adversarial Learning for Large-Batch Training](https://openreview.net/forum?id=rw1mZl_ss3L) |  | 0 |  | Yong Liu, Xiangning Chen, Minhao Cheng, ChoJui Hsieh, Yang You |  |
| 801 |  |  [Multiset-Equivariant Set Prediction with Approximate Implicit Differentiation](https://openreview.net/forum?id=5K7RRqZEjoS) |  | 0 |  | Yan Zhang, David W. Zhang, Simon LacosteJulien, Gertjan J. Burghouts, Cees G. M. Snoek |  |
| 802 |  |  [Learned Simulators for Turbulence](https://openreview.net/forum?id=msRBojTz-Nh) |  | 0 |  | Kimberly L. Stachenfeld, Drummond Buschman Fielding, Dmitrii Kochkov, Miles D. Cranmer, Tobias Pfaff, Jonathan Godwin, Can Cui, Shirley Ho, Peter W. Battaglia, Alvaro SanchezGonzalez |  |
| 803 |  |  [Modular Lifelong Reinforcement Learning via Neural Composition](https://openreview.net/forum?id=5XmLzdslFNN) |  | 0 |  | Jorge A. Mendez, Harm van Seijen, Eric Eaton |  |
| 804 |  |  [Optimal ANN-SNN Conversion for High-accuracy and Ultra-low-latency Spiking Neural Networks](https://openreview.net/forum?id=7B3IJMM1k_M) |  | 0 |  | Tong Bu, Wei Fang, Jianhao Ding, Penglin Dai, Zhaofei Yu, Tiejun Huang |  |
| 805 |  |  [AS-MLP: An Axial Shifted MLP Architecture for Vision](https://openreview.net/forum?id=fvLLcIYmXb) |  | 0 |  | Dongze Lian, Zehao Yu, Xing Sun, Shenghua Gao |  |
| 806 |  |  [Online Continual Learning on Class Incremental Blurry Task Configuration with Anytime Inference](https://openreview.net/forum?id=nrGGfMbY_qK) |  | 0 |  | Hyunseo Koh, Dahyun Kim, JungWoo Ha, Jonghyun Choi |  |
| 807 |  |  [Learning with Noisy Labels Revisited: A Study Using Real-World Human Annotations](https://openreview.net/forum?id=TBWA6PLJZQm) |  | 0 |  | Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, Yang Liu |  |
| 808 |  |  [Optimization inspired Multi-Branch Equilibrium Models](https://openreview.net/forum?id=nbC8iTTXIrk) |  | 0 |  | Mingjie Li, Yisen Wang, Xingyu Xie, Zhouchen Lin |  |
| 809 |  |  [Learning to Annotate Part Segmentation with Gradient Matching](https://openreview.net/forum?id=zNR43c03lRy) |  | 0 |  | Yu Yang, Xiaotian Cheng, Hakan Bilen, Xiangyang Ji |  |
| 810 |  |  [Vector-quantized Image Modeling with Improved VQGAN](https://openreview.net/forum?id=pfNyExj7z2) |  | 0 |  | Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, Yonghui Wu |  |
| 811 |  |  [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://openreview.net/forum?id=R8sQPpGCv0) |  | 0 |  | Ofir Press, Noah A. Smith, Mike Lewis |  |
| 812 |  |  [Learning Representation from Neural Fisher Kernel with Low-rank Approximation](https://openreview.net/forum?id=J1rhANsCY9) |  | 0 |  | Ruixiang Zhang, Shuangfei Zhai, Etai Littwin, Joshua M. Susskind |  |
| 813 |  |  [Learning Temporally Causal Latent Processes from General Temporal Data](https://openreview.net/forum?id=RDlLMjLJXdq) |  | 0 |  | Weiran Yao, Yuewen Sun, Alex Ho, Changyin Sun, Kun Zhang |  |
| 814 |  |  [The Rich Get Richer: Disparate Impact of Semi-Supervised Learning](https://openreview.net/forum?id=DXPftn5kjQK) |  | 0 |  | Zhaowei Zhu, Tianyi Luo, Yang Liu |  |
| 815 |  |  [Neural Relational Inference with Node-Specific Information](https://openreview.net/forum?id=HBsJNesj2S) |  | 0 |  | Ershad Banijamali |  |
| 816 |  |  [Bregman Gradient Policy Optimization](https://openreview.net/forum?id=ZU-zFnTum1N) |  | 0 |  | Feihu Huang, Shangqian Gao, Heng Huang |  |
| 817 |  |  [A generalization of the randomized singular value decomposition](https://openreview.net/forum?id=hgKtwSb4S2) |  | 0 |  | Nicolas Boullé, Alex Townsend |  |
| 818 |  |  [Dropout Q-Functions for Doubly Efficient Reinforcement Learning](https://openreview.net/forum?id=xCVJMsPv3RT) |  | 0 |  | Takuya Hiraoka, Takahisa Imagawa, Taisei Hashimoto, Takashi Onishi, Yoshimasa Tsuruoka |  |
| 819 |  |  [QDrop: Randomly Dropping Quantization for Extremely Low-bit Post-Training Quantization](https://openreview.net/forum?id=ySQH0oDyp7) |  | 0 |  | Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, Fengwei Yu |  |
| 820 |  |  [You Mostly Walk Alone: Analyzing Feature Attribution in Trajectory Prediction](https://openreview.net/forum?id=POxF-LEqnF) |  | 0 |  | Osama Makansi, Julius von Kügelgen, Francesco Locatello, Peter Vincent Gehler, Dominik Janzing, Thomas Brox, Bernhard Schölkopf |  |
| 821 |  |  [Rethinking Class-Prior Estimation for Positive-Unlabeled Learning](https://openreview.net/forum?id=aYAA-XHKyk) |  | 0 |  | Yu Yao, Tongliang Liu, Bo Han, Mingming Gong, Gang Niu, Masashi Sugiyama, Dacheng Tao |  |
| 822 |  |  [Learning Efficient Online 3D Bin Packing on Packing Configuration Trees](https://openreview.net/forum?id=bfuGjlCwAq) |  | 0 |  | Hang Zhao, Yang Yu, Kai Xu |  |
| 823 |  |  [Uncertainty Modeling for Out-of-Distribution Generalization](https://openreview.net/forum?id=6HN7LHyzGgC) |  | 0 |  | Xiaotong Li, Yongxing Dai, Yixiao Ge, Jun Liu, Ying Shan, Lingyu Duan |  |
| 824 |  |  [Online Adversarial Attacks](https://openreview.net/forum?id=bYGSzbCM_i) |  | 0 |  | Andjela Mladenovic, Avishek Joey Bose, Hugo Berard, William L. Hamilton, Simon LacosteJulien, Pascal Vincent, Gauthier Gidel |  |
| 825 |  |  [Anytime Dense Prediction with Confidence Adaptivity](https://openreview.net/forum?id=kNKFOXleuC) |  | 0 |  | Zhuang Liu, Zhiqiu Xu, HungJu Wang, Trevor Darrell, Evan Shelhamer |  |
| 826 |  |  [Declarative nets that are equilibrium models](https://openreview.net/forum?id=q4HaTeMO--y) |  | 0 |  | Russell Tsuchida, Suk Yee Yong, Mohammad Ali Armin, Lars Petersson, Cheng Soon Ong |  |
| 827 |  |  [A Reduction-Based Framework for Conservative Bandits and Reinforcement Learning](https://openreview.net/forum?id=AcrlgZ9BKed) |  | 0 |  | Yunchang Yang, Tianhao Wu, Han Zhong, Evrard Garcelon, Matteo Pirotta, Alessandro Lazaric, Liwei Wang, Simon Shaolei Du |  |
| 828 |  |  [Wisdom of Committees: An Overlooked Approach To Faster and More Accurate Models](https://openreview.net/forum?id=MvO2t0vbs4-) |  | 0 |  | Xiaofang Wang, Dan Kondratyuk, Eric Christiansen, Kris M. Kitani, Yair MovshovitzAttias, Elad Eban |  |
| 829 |  |  [Unsupervised Discovery of Object Radiance Fields](https://openreview.net/forum?id=rwE8SshAlxw) |  | 0 |  | HongXing Yu, Leonidas J. Guibas, Jiajun Wu |  |
| 830 |  |  [Gradient Step Denoiser for convergent Plug-and-Play](https://openreview.net/forum?id=fPhKeld3Okz) |  | 0 |  | Samuel Hurault, Arthur Leclaire, Nicolas Papadakis |  |
| 831 |  |  [Surrogate Gap Minimization Improves Sharpness-Aware Training](https://openreview.net/forum?id=edONMAnhLu-) |  | 0 |  | Juntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui, Hartwig Adam, Nicha C. Dvornek, Sekhar Tatikonda, James S. Duncan, Ting Liu |  |
| 832 |  |  [R4D: Utilizing Reference Objects for Long-Range Distance Estimation](https://openreview.net/forum?id=MQ2sAGunyBP) |  | 0 |  | Yingwei Li, Tiffany L. Chen, Maya Kabkab, Ruichi Yu, Longlong Jing, Yurong You, Hang Zhao |  |
| 833 |  |  [Understanding Dimensional Collapse in Contrastive Self-supervised Learning](https://openreview.net/forum?id=YevsQ05DEN7) |  | 0 |  | Li Jing, Pascal Vincent, Yann LeCun, Yuandong Tian |  |
| 834 |  |  [FedPara: Low-rank Hadamard Product for Communication-Efficient Federated Learning](https://openreview.net/forum?id=d71n4ftoCBy) |  | 0 |  | Nam HyeonWoo, Moon YeBin, TaeHyun Oh |  |
| 835 |  |  [RegionViT: Regional-to-Local Attention for Vision Transformers](https://openreview.net/forum?id=T__V3uLix7V) |  | 0 |  | ChunFu Chen, Rameswar Panda, Quanfu Fan |  |
| 836 |  |  [Quadtree Attention for Vision Transformers](https://openreview.net/forum?id=fR-EnKWL_Zb) |  | 0 |  | Shitao Tang, Jiahui Zhang, Siyu Zhu, Ping Tan |  |
| 837 |  |  [Visual Correspondence Hallucination](https://openreview.net/forum?id=jaLDP8Hp_gc) |  | 0 |  | Hugo Germain, Vincent Lepetit, Guillaume Bourmaud |  |
| 838 |  |  [What's Wrong with Deep Learning in Tree Search for Combinatorial Optimization](https://openreview.net/forum?id=mk0HzdqY7i1) |  | 0 |  | Maximilian Böther, Otto Kißig, Martin Taraz, Sarel Cohen, Karen Seidel, Tobias Friedrich |  |
| 839 |  |  [Deep Attentive Variational Inference](https://openreview.net/forum?id=T4-65DNlDij) |  | 0 |  | Ifigeneia Apostolopoulou, Ian Char, Elan Rosenfeld, Artur Dubrawski |  |
| 840 |  |  [ARTEMIS: Attention-based Retrieval with Text-Explicit Matching and Implicit Similarity](https://openreview.net/forum?id=CVfLvQq9gLo) |  | 0 |  | Ginger Delmas, Rafael Sampaio de Rezende, Gabriela Csurka, Diane Larlus |  |
| 841 |  |  [Trivial or Impossible --- dichotomous data difficulty masks model differences (on ImageNet and beyond)](https://openreview.net/forum?id=C_vsGwEIjAr) |  | 0 |  | Kristof Meding, Luca M. Schulze Buschoff, Robert Geirhos, Felix A. Wichmann |  |
| 842 |  |  [Group equivariant neural posterior estimation](https://openreview.net/forum?id=u6s8dSporO8) |  | 0 |  | Maximilian Dax, Stephen R. Green, Jonathan Gair, Michael Deistler, Bernhard Schölkopf, Jakob H. Macke |  |
| 843 |  |  [Fast Differentiable Matrix Square Root](https://openreview.net/forum?id=-AOEi-5VTU8) |  | 0 |  | Yue Song, Nicu Sebe, Wei Wang |  |
| 844 |  |  [SQuant: On-the-Fly Data-Free Quantization via Diagonal Hessian Approximation](https://openreview.net/forum?id=JXhROKNZzOc) |  | 0 |  | Cong Guo, Yuxian Qiu, Jingwen Leng, Xiaotian Gao, Chen Zhang, Yunxin Liu, Fan Yang, Yuhao Zhu, Minyi Guo |  |
| 845 |  |  [Neural Variational Dropout Processes](https://openreview.net/forum?id=lyLVzukXi08) |  | 0 |  | Insu Jeon, Youngjin Park, Gunhee Kim |  |
| 846 |  |  [Towards Better Understanding and Better Generalization of Low-shot Classification in Histology Images with Contrastive Learning](https://openreview.net/forum?id=kQ2SOflIOVC) |  | 0 |  | Jiawei Yang, Hanbo Chen, Jiangpeng Yan, Xiaoyu Chen, Jianhua Yao |  |
| 847 |  |  [Distilling GANs with Style-Mixed Triplets for X2I Translation with Limited Data](https://openreview.net/forum?id=QjOQkpzKbNk) |  | 0 |  | Yaxing Wang, Joost van de Weijer, Lu Yu, Shangling Jui |  |
| 848 |  |  [Handling Distribution Shifts on Graphs: An Invariance Perspective](https://openreview.net/forum?id=FQOC5u-1egI) |  | 0 |  | Qitian Wu, Hengrui Zhang, Junchi Yan, David Wipf |  |
| 849 |  |  [Automatic Loss Function Search for Predict-Then-Optimize Problems with Strong Ranking Property](https://openreview.net/forum?id=hSktDu-h94) |  | 0 |  | Boshi Wang, Jialin Yi, Hang Dong, Bo Qiao, Chuan Luo, Qingwei Lin |  |
| 850 |  |  [Generalized Demographic Parity for Group Fairness](https://openreview.net/forum?id=YigKlMJwjye) |  | 0 |  | Zhimeng Jiang, Xiaotian Han, Chao Fan, Fan Yang, Ali Mostafavi, Xia Hu |  |
| 851 |  |  [Closed-form Sample Probing for Learning Generative Models in Zero-shot Learning](https://openreview.net/forum?id=ljxWpdBl4V) |  | 0 |  | Samet Çetin, Orhun Bugra Baran, Ramazan Gokberk Cinbis |  |
| 852 |  |  [DKM: Differentiable k-Means Clustering Layer for Neural Network Compression](https://openreview.net/forum?id=J_F_qqCE3Z5) |  | 0 |  | Minsik Cho, Keivan AlizadehVahid, Saurabh Adya, Mohammad Rastegari |  |
| 853 |  |  [Fixed Neural Network Steganography: Train the images, not the network](https://openreview.net/forum?id=hcMvApxGSzZ) |  | 0 |  | Varsha Kishore, Xiangyu Chen, Yan Wang, Boyi Li, Kilian Q. Weinberger |  |
| 854 |  |  [Steerable Partial Differential Operators for Equivariant Neural Networks](https://openreview.net/forum?id=N9W24a4zU) |  | 0 |  | Erik Jenner, Maurice Weiler |  |
| 855 |  |  [Divergence-aware Federated Self-Supervised Learning](https://openreview.net/forum?id=oVE1z8NlNe) |  | 0 |  | Weiming Zhuang, Yonggang Wen, Shuai Zhang |  |
| 856 |  |  [Neural Spectral Marked Point Processes](https://openreview.net/forum?id=0rcbOaoBXbg) |  | 0 |  | Shixiang Zhu, Haoyun Wang, Zheng Dong, Xiuyuan Cheng, Yao Xie |  |
| 857 |  |  [How to Inject Backdoors with Better Consistency: Logit Anchoring on Clean Data](https://openreview.net/forum?id=Bn09TnDngN) |  | 0 |  | Zhiyuan Zhang, Lingjuan Lyu, Weiqiang Wang, Lichao Sun, Xu Sun |  |
| 858 |  |  [A Biologically Interpretable Graph Convolutional Network to Link Genetic Risk Pathways and Imaging Phenotypes of Disease](https://openreview.net/forum?id=Lwr8We4MIxn) |  | 0 |  | Sayan Ghosal, Qiang Chen, Giulio Pergola, Aaron L. Goldman, William Ulrich, Daniel R. Weinberger, Archana Venkataraman |  |
| 859 |  |  [Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners](https://openreview.net/forum?id=ek9a0qIafW) |  | 0 |  | Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang, Huajun Chen |  |
| 860 |  |  [OntoProtein: Protein Pretraining With Gene Ontology Embedding](https://openreview.net/forum?id=yfe1VMYAXa4) |  | 0 |  | Ningyu Zhang, Zhen Bi, Xiaozhuan Liang, Siyuan Cheng, Haosen Hong, Shumin Deng, Qiang Zhang, Jiazhang Lian, Huajun Chen |  |
| 861 |  |  [Permutation Compressors for Provably Faster Distributed Nonconvex Optimization](https://openreview.net/forum?id=GugZ5DzzAu) |  | 0 |  | Rafal Szlendak, Alexander Tyurin, Peter Richtárik |  |
| 862 |  |  [Few-shot Learning via Dirichlet Tessellation Ensemble](https://openreview.net/forum?id=6kCiVaoQdx9) |  | 0 |  | Chunwei Ma, Ziyun Huang, Mingchen Gao, Jinhui Xu |  |
| 863 |  |  [Deep Point Cloud Reconstruction](https://openreview.net/forum?id=mKDtUtxIGJ) |  | 0 |  | Jaesung Choe, Byeongin Joung, François Rameau, Jaesik Park, In So Kweon |  |
| 864 |  |  [$\beta$-Intact-VAE: Identifying and Estimating Causal Effects under Limited Overlap](https://openreview.net/forum?id=q7n2RngwOM) |  | 0 |  | Pengzhou Abel Wu, Kenji Fukumizu |  |
| 865 |  |  [Promoting Saliency From Depth: Deep Unsupervised RGB-D Saliency Detection](https://openreview.net/forum?id=BZnnMbt0pW) |  | 0 |  | Wei Ji, Jingjing Li, Qi Bi, Chuan Guo, Jie Liu, Li Cheng |  |
| 866 |  |  [Retriever: Learning Content-Style Representation as a Token-Level Bipartite Graph](https://openreview.net/forum?id=AXWygMvuT6Q) |  | 0 |  | Dacheng Yin, Xuanchi Ren, Chong Luo, Yuwang Wang, Zhiwei Xiong, Wenjun Zeng |  |
| 867 |  |  [Neural Markov Controlled SDE: Stochastic Optimization for Continuous-Time Data](https://openreview.net/forum?id=7DI6op61AY) |  | 0 |  | Sung Woo Park, Kyungjae Lee, Junseok Kwon |  |
| 868 |  |  [CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention](https://openreview.net/forum?id=_PHymLIxuI) |  | 0 |  | Wenxiao Wang, Lu Yao, Long Chen, Binbin Lin, Deng Cai, Xiaofei He, Wei Liu |  |
| 869 |  |  [Adversarially Robust Conformal Prediction](https://openreview.net/forum?id=9L1BsI4wP1H) |  | 0 |  | Asaf Gendler, TsuiWei Weng, Luca Daniel, Yaniv Romano |  |
| 870 |  |  [Hot-Refresh Model Upgrades with Regression-Free Compatible Training in Image Retrieval](https://openreview.net/forum?id=HTp-6yLGGX) |  | 0 |  | Binjie Zhang, Yixiao Ge, Yantao Shen, Yu Li, Chun Yuan, Xuyuan Xu, Yexin Wang, Ying Shan |  |
| 871 |  |  [Visual Representation Learning over Latent Domains](https://openreview.net/forum?id=kG0AtPi6JI1) |  | 0 |  | Lucas Deecke, Timothy M. Hospedales, Hakan Bilen |  |
| 872 |  |  [Chemical-Reaction-Aware Molecule Representation Learning](https://openreview.net/forum?id=6sh3pIzKS-) |  | 0 |  | Hongwei Wang, Weijiang Li, Xiaomeng Jin, Kyunghyun Cho, Heng Ji, Jiawei Han, Martin D. Burke |  |
| 873 |  |  [Skill-based Meta-Reinforcement Learning](https://openreview.net/forum?id=jeLW-Fh9bV) |  | 0 |  | Taewook Nam, ShaoHua Sun, Karl Pertsch, Sung Ju Hwang, Joseph J. Lim |  |
| 874 |  |  [InfinityGAN: Towards Infinite-Pixel Image Synthesis](https://openreview.net/forum?id=ufGMqIM0a4b) |  | 0 |  | Chieh Hubert Lin, HsinYing Lee, YenChi Cheng, Sergey Tulyakov, MingHsuan Yang |  |
| 875 |  |  [Shuffle Private Stochastic Convex Optimization](https://openreview.net/forum?id=DrZXuTGg2A-) |  | 0 |  | Albert Cheu, Matthew Joseph, Jieming Mao, Binghui Peng |  |
| 876 |  |  [Know Your Action Set: Learning Action Relations for Reinforcement Learning](https://openreview.net/forum?id=MljXVdp4A3N) |  | 0 |  | Ayush Jain, Norio Kosaka, KyungMin Kim, Joseph J. Lim |  |
| 877 |  |  [On the Importance of Difficulty Calibration in Membership Inference Attacks](https://openreview.net/forum?id=3eIrli0TwQ) |  | 0 |  | Lauren Watson, Chuan Guo, Graham Cormode, Alexandre Sablayrolles |  |
| 878 |  |  [Entroformer: A Transformer-based Entropy Model for Learned Image Compression](https://openreview.net/forum?id=VrjOFfcnSV8) |  | 0 |  | Yichen Qian, Xiuyu Sun, Ming Lin, Zhiyu Tan, Rong Jin |  |
| 879 |  |  [Dual Lottery Ticket Hypothesis](https://openreview.net/forum?id=fOsN52jn25l) |  | 0 |  | Yue Bai, Huan Wang, Zhiqiang Tao, Kunpeng Li, Yun Fu |  |
| 880 |  |  [GNN is a Counter? Revisiting GNN for Question Answering](https://openreview.net/forum?id=hzmQ4wOnSb) |  | 0 |  | Kuan Wang, Yuyu Zhang, Diyi Yang, Le Song, Tao Qin |  |
| 881 |  |  [IFR-Explore: Learning Inter-object Functional Relationships in 3D Indoor Scenes](https://openreview.net/forum?id=OT3mLgR8Wg8) |  | 0 |  | Qi Li, Kaichun Mo, Yanchao Yang, Hang Zhao, Leonidas J. Guibas |  |
| 882 |  |  [VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D ARTiculated Objects](https://openreview.net/forum?id=iEx3PiooLy) |  | 0 |  | Ruihai Wu, Yan Zhao, Kaichun Mo, Zizheng Guo, Yian Wang, Tianhao Wu, Qingnan Fan, Xuelin Chen, Leonidas J. Guibas, Hao Dong |  |
| 883 |  |  [Neural graphical modelling in continuous-time: consistency guarantees and algorithms](https://openreview.net/forum?id=SsHBkfeRF9L) |  | 0 |  | Alexis Bellot, Kim Branson, Mihaela van der Schaar |  |
| 884 |  |  [C-Planning: An Automatic Curriculum for Learning Goal-Reaching Tasks](https://openreview.net/forum?id=K2JfSnLBD9) |  | 0 |  | Tianjun Zhang, Benjamin Eysenbach, Ruslan Salakhutdinov, Sergey Levine, Joseph E. Gonzalez |  |
| 885 |  |  [NAS-Bench-Suite: NAS Evaluation is (Now) Surprisingly Easy](https://openreview.net/forum?id=0DLwqQLmqV) |  | 0 |  | Yash Mehta, Colin White, Arber Zela, Arjun Krishnakumar, Guri Zabergja, Shakiba Moradian, Mahmoud Safari, Kaicheng Yu, Frank Hutter |  |
| 886 |  |  [Machine Learning For Elliptic PDEs: Fast Rate Generalization Bound, Neural Scaling Law and Minimax Optimality](https://openreview.net/forum?id=mhYUBYNoGz) |  | 0 |  | Yiping Lu, Haoxuan Chen, Jianfeng Lu, Lexing Ying, Jose H. Blanchet |  |
| 887 |  |  [Variational oracle guiding for reinforcement learning](https://openreview.net/forum?id=pjqqxepwoMy) |  | 0 |  | Dongqi Han, Tadashi Kozuno, Xufang Luo, ZhaoYun Chen, Kenji Doya, Yuqing Yang, Dongsheng Li |  |
| 888 |  |  [CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation](https://openreview.net/forum?id=XGzk5OKWFFc) |  | 0 |  | Tongkun Xu, Weihua Chen, Pichao Wang, Fan Wang, Hao Li, Rong Jin |  |
| 889 |  |  [Beyond ImageNet Attack: Towards Crafting Adversarial Examples for Black-box Domains](https://openreview.net/forum?id=QkRV50TZyP) |  | 0 |  | Qilong Zhang, Xiaodan Li, Yuefeng Chen, Jingkuan Song, Lianli Gao, Yuan He, Hui Xue |  |
| 890 |  |  [Learning to Schedule Learning rate with Graph Neural Networks](https://openreview.net/forum?id=k7efTb0un9z) |  | 0 |  | Yuanhao Xiong, LiCheng Lan, Xiangning Chen, Ruochen Wang, ChoJui Hsieh |  |
| 891 |  |  [SketchODE: Learning neural sketch representation in continuous time](https://openreview.net/forum?id=c-4HSDAWua5) |  | 0 |  | Ayan Das, Yongxin Yang, Timothy M. Hospedales, Tao Xiang, YiZhe Song |  |
| 892 |  |  [Measuring the Interpretability of Unsupervised Representations via Quantized Reversed Probing](https://openreview.net/forum?id=HFPTzdwN39) |  | 0 |  | Iro Laina, Yuki M. Asano, Andrea Vedaldi |  |
| 893 |  |  [GradMax: Growing Neural Networks using Gradient Information](https://openreview.net/forum?id=qjN4h_wwUO) |  | 0 |  | Utku Evci, Bart van Merrienboer, Thomas Unterthiner, Fabian Pedregosa, Max Vladymyrov |  |
| 894 |  |  [Online Coreset Selection for Rehearsal-based Continual Learning](https://openreview.net/forum?id=f9D-5WNG4Nv) |  | 0 |  | Jaehong Yoon, Divyam Madaan, Eunho Yang, Sung Ju Hwang |  |
| 895 |  |  [Switch to Generalize: Domain-Switch Learning for Cross-Domain Few-Shot Classification](https://openreview.net/forum?id=H-iABMvzIc) |  | 0 |  | Zhengdong Hu, Yifan Sun, Yi Yang |  |
| 896 |  |  [Zero-CL: Instance and Feature decorrelation for negative-free symmetric contrastive learning](https://openreview.net/forum?id=RAW9tCdVxLj) |  | 0 |  | Shaofeng Zhang, Feng Zhu, Junchi Yan, Rui Zhao, Xiaokang Yang |  |
| 897 |  |  [Random matrices in service of ML footprint: ternary random features with no performance loss](https://openreview.net/forum?id=qwULHx9zld) |  | 0 |  | Hafiz Tiomoko Ali, Zhenyu Liao, Romain Couillet |  |
| 898 |  |  [Global Convergence of Multi-Agent Policy Gradient in Markov Potential Games](https://openreview.net/forum?id=gfwON7rAm4) |  | 0 |  | Stefanos Leonardos, Will Overman, Ioannis Panageas, Georgios Piliouras |  |
| 899 |  |  [Rethinking Adversarial Transferability from a Data Distribution Perspective](https://openreview.net/forum?id=gVRhIEajG1k) |  | 0 |  | Yao Zhu, Jiacheng Sun, Zhenguo Li |  |
| 900 |  |  [Transformers Can Do Bayesian Inference](https://openreview.net/forum?id=KSugKcbNf9) |  | 0 |  | Samuel Müller, Noah Hollmann, Sebastian PinedaArango, Josif Grabocka, Frank Hutter |  |
| 901 |  |  [Learning Discrete Structured Variational Auto-Encoder using Natural Evolution Strategies](https://openreview.net/forum?id=JJCjv4dAbyL) |  | 0 |  | Alon Berliner, Guy Rotman, Yossi Adi, Roi Reichart, Tamir Hazan |  |
| 902 |  |  [Learning Features with Parameter-Free Layers](https://openreview.net/forum?id=bCrdi4iVvv) |  | 0 |  | Dongyoon Han, Young Joon Yoo, Beomyoung Kim, Byeongho Heo |  |
| 903 |  |  [Denoising Likelihood Score Matching for Conditional Score-based Data Generation](https://openreview.net/forum?id=LcF-EEt8cCC) |  | 0 |  | ChenHao Chao, WeiFang Sun, BoWun Cheng, YiChen Lo, ChiaChe Chang, YuLun Liu, YuLin Chang, ChiaPing Chen, ChunYi Lee |  |
| 904 |  |  [Memory Replay with Data Compression for Continual Learning](https://openreview.net/forum?id=a7H7OucbWaU) |  | 0 |  | Liyuan Wang, Xingxing Zhang, Kuo Yang, Longhui Yu, Chongxuan Li, Lanqing Hong, Shifeng Zhang, Zhenguo Li, Yi Zhong, Jun Zhu |  |
| 905 |  |  [MAML is a Noisy Contrastive Learner in Classification](https://openreview.net/forum?id=LDAwu17QaJz) |  | 0 |  | ChiaHsiang Kao, WeiChen Chiu, PinYu Chen |  |
| 906 |  |  [RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning](https://openreview.net/forum?id=afoV8W3-IYp) |  | 0 |  | Xiaojian Ma, Weili Nie, Zhiding Yu, Huaizu Jiang, Chaowei Xiao, Yuke Zhu, SongChun Zhu, Anima Anandkumar |  |
| 907 |  |  [Boosted Curriculum Reinforcement Learning](https://openreview.net/forum?id=anbBFlX1tJ1) |  | 0 |  | Pascal Klink, Carlo D'Eramo, Jan Peters, Joni Pajarinen |  |
| 908 |  |  [ViDT: An Efficient and Effective Fully Transformer-based Object Detector](https://openreview.net/forum?id=w4cXZDDib1H) |  | 0 |  | Hwanjun Song, Deqing Sun, Sanghyuk Chun, Varun Jampani, Dongyoon Han, Byeongho Heo, Wonjae Kim, MingHsuan Yang |  |
| 909 |  |  [BiBERT: Accurate Fully Binarized BERT](https://openreview.net/forum?id=5xEgrl_5FAJ) |  | 0 |  | Haotong Qin, Yifu Ding, Mingyuan Zhang, Qinghua Yan, Aishan Liu, Qingqing Dang, Ziwei Liu, Xianglong Liu |  |
| 910 |  |  [Feature Kernel Distillation](https://openreview.net/forum?id=tBIQEvApZK5) |  | 0 |  | Bobby He, Mete Ozay |  |
| 911 |  |  [Representation-Agnostic Shape Fields](https://openreview.net/forum?id=-ngwPqanCEZ) |  | 0 |  | Xiaoyang Huang, Jiancheng Yang, Yanjun Wang, Ziyu Chen, Linguo Li, Teng Li, Bingbing Ni, Wenjun Zhang |  |
| 912 |  |  [Learning Synthetic Environments and Reward Networks for Reinforcement Learning](https://openreview.net/forum?id=C1_esHN6AVn) |  | 0 |  | Fabio Ferreira, Thomas Nierhoff, Andreas Sälinger, Frank Hutter |  |
| 913 |  |  [Who Is Your Right Mixup Partner in Positive and Unlabeled Learning](https://openreview.net/forum?id=NH29920YEmj) |  | 0 |  | Changchun Li, Ximing Li, Lei Feng, Jihong Ouyang |  |
| 914 |  |  [Incremental False Negative Detection for Contrastive Learning](https://openreview.net/forum?id=dDjSKKA5TP1) |  | 0 |  | TsaiShien Chen, WeiChih Hung, HungYu Tseng, ShaoYi Chien, MingHsuan Yang |  |
| 915 |  |  [Multi-Critic Actor Learning: Teaching RL Policies to Act with Style](https://openreview.net/forum?id=rJvY_5OzoI) |  | 0 |  | Siddharth Mysore, George Cheng, Yunqi Zhao, Kate Saenko, Meng Wu |  |
| 916 |  |  [Clean Images are Hard to Reblur: Exploiting the Ill-Posed Inverse Task for Dynamic Scene Deblurring](https://openreview.net/forum?id=kezNJydWvE) |  | 0 |  | Seungjun Nah, Sanghyun Son, Jaerin Lee, Kyoung Mu Lee |  |
| 917 |  |  [Learning Disentangled Representation by Exploiting Pretrained Generative Models: A Contrastive Learning View](https://openreview.net/forum?id=j-63FSNcO5a) |  | 0 |  | Xuanchi Ren, Tao Yang, Yuwang Wang, Wenjun Zeng |  |
| 918 |  |  [Towards Building A Group-based Unsupervised Representation Disentanglement Framework](https://openreview.net/forum?id=YgPqNctmyd) |  | 0 |  | Tao Yang, Xuanchi Ren, Yuwang Wang, Wenjun Zeng, Nanning Zheng |  |
| 919 |  |  [Learning Efficient Image Super-Resolution Networks via Structure-Regularized Pruning](https://openreview.net/forum?id=AjGC97Aofee) |  | 0 |  | Yulun Zhang, Huan Wang, Can Qin, Yun Fu |  |
| 920 |  |  [Wiring Up Vision: Minimizing Supervised Synaptic Updates Needed to Produce a Primate Ventral Stream](https://openreview.net/forum?id=g1SzIRLQXMM) |  | 0 |  | Franziska Geiger, Martin Schrimpf, Tiago Marques, James J. DiCarlo |  |
| 921 |  |  [Dynamics-Aware Comparison of Learned Reward Functions](https://openreview.net/forum?id=CALFyKVs87) |  | 0 |  | Blake Wulfe, Logan Michael Ellis, Jean Mercat, Rowan Thomas McAllister, Adrien Gaidon |  |
| 922 |  |  [Learning Hierarchical Structures with Differentiable Nondeterministic Stacks](https://openreview.net/forum?id=5LXw_QplBiF) |  | 0 |  | Brian DuSell, David Chiang |  |
| 923 |  |  [Sampling with Mirrored Stein Operators](https://openreview.net/forum?id=eMudnJsb1T5) |  | 0 |  | Jiaxin Shi, Chang Liu, Lester Mackey |  |
| 924 |  |  [Planning in Stochastic Environments with a Learned Model](https://openreview.net/forum?id=X6D9bAHhBQ1) |  | 0 |  | Ioannis Antonoglou, Julian Schrittwieser, Sherjil Ozair, Thomas K. Hubert, David Silver |  |
| 925 |  |  [RotoGrad: Gradient Homogenization in Multitask Learning](https://openreview.net/forum?id=T8wHz4rnuGL) |  | 0 |  | Adrián Javaloy, Isabel Valera |  |
| 926 |  |  [On Improving Adversarial Transferability of Vision Transformers](https://openreview.net/forum?id=D6nH3719vZy) |  | 0 |  | Muzammal Naseer, Kanchana Ranasinghe, Salman Khan, Fahad Shahbaz Khan, Fatih Porikli |  |
| 927 |  |  [On Predicting Generalization using GANs](https://openreview.net/forum?id=eW5R4Cek6y6) |  | 0 |  | Yi Zhang, Arushi Gupta, Nikunj Saunshi, Sanjeev Arora |  |
| 928 |  |  [On the Connection between Local Attention and Dynamic Depth-wise Convolution](https://openreview.net/forum?id=L3_SsSNMmy) |  | 0 |  | Qi Han, Zejia Fan, Qi Dai, Lei Sun, MingMing Cheng, Jiaying Liu, Jingdong Wang |  |
| 929 |  |  [Strength of Minibatch Noise in SGD](https://openreview.net/forum?id=uorVGbWV5sw) |  | 0 |  | Liu Ziyin, Kangqiao Liu, Takashi Mori, Masahito Ueda |  |
| 930 |  |  [Learning more skills through optimistic exploration](https://openreview.net/forum?id=cU8rknuhxc) |  | 0 |  | DJ Strouse, Kate Baumli, David WardeFarley, Volodymyr Mnih, Steven Stenberg Hansen |  |
| 931 |  |  [Reinforcement Learning under a Multi-agent Predictive State Representation Model: Method and Theory](https://openreview.net/forum?id=PLDOnFoVm4) |  | 0 |  | Zhi Zhang, Zhuoran Yang, Han Liu, Pratap Tokekar, Furong Huang |  |
| 932 |  |  [Adversarial Support Alignment](https://openreview.net/forum?id=26gKg6x-ie) |  | 0 |  | Shangyuan Tong, Timur Garipov, Yang Zhang, Shiyu Chang, Tommi S. Jaakkola |  |
| 933 |  |  [GreaseLM: Graph REASoning Enhanced Language Models](https://openreview.net/forum?id=41e9o6cQPj) |  | 0 |  | Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D. Manning, Jure Leskovec |  |
| 934 |  |  [Learning meta-features for AutoML](https://openreview.net/forum?id=DTkEfj0Ygb8) |  | 0 |  | Herilalaina Rakotoarison, Louisot Milijaona, Andry Rasoanaivo, Michèle Sebag, Marc Schoenauer |  |
| 935 |  |  [Latent Variable Sequential Set Transformers for Joint Multi-Agent Motion Prediction](https://openreview.net/forum?id=Dup_dDqkZC5) |  | 0 |  | Roger Girgis, Florian Golemo, Felipe Codevilla, Martin Weiss, Jim Aldon D'Souza, Samira Ebrahimi Kahou, Felix Heide, Christopher Pal |  |
| 936 |  |  [Understanding Latent Correlation-Based Multiview Learning and Self-Supervision: An Identifiability Perspective](https://openreview.net/forum?id=5FUq05QRc5b) |  | 0 |  | Qi Lyu, Xiao Fu, Weiran Wang, Songtao Lu |  |
| 937 |  |  [Deconstructing the Inductive Biases of Hamiltonian Neural Networks](https://openreview.net/forum?id=EDeVYpT42oS) |  | 0 |  | Nate Gruver, Marc Anton Finzi, Samuel Don Stanton, Andrew Gordon Wilson |  |
| 938 |  |  [Memorizing Transformers](https://openreview.net/forum?id=TrjbxzRcnf-) |  | 0 |  | Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, Christian Szegedy |  |
| 939 |  |  [Learning-Augmented $k$-means Clustering](https://openreview.net/forum?id=X8cLTHexYyY) |  | 0 |  | Jon C. Ergun, Zhili Feng, Sandeep Silwal, David P. Woodruff, Samson Zhou |  |
| 940 |  |  [On the Uncomputability of Partition Functions in Energy-Based Sequence Models](https://openreview.net/forum?id=SsPCtEY6yCl) |  | 0 |  | ChuCheng Lin, Arya D. McCarthy |  |
| 941 |  |  [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://openreview.net/forum?id=fILj7WpI-g) |  | 0 |  | Andrew Jaegle, Sebastian Borgeaud, JeanBaptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier J. Hénaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, João Carreira |  |
| 942 |  |  [DR3: Value-Based Deep Reinforcement Learning Requires Explicit Regularization](https://openreview.net/forum?id=POvMvLi91f) |  | 0 |  | Aviral Kumar, Rishabh Agarwal, Tengyu Ma, Aaron C. Courville, George Tucker, Sergey Levine |  |
| 943 |  |  [MT3: Multi-Task Multitrack Music Transcription](https://openreview.net/forum?id=iMSjopcOn0p) |  | 0 |  | Josh Gardner, Ian Simon, Ethan Manilow, Curtis Hawthorne, Jesse H. Engel |  |
| 944 |  |  [Does your graph need a confidence boost? Convergent boosted smoothing on graphs with tabular node features](https://openreview.net/forum?id=nHpzE7DqAnG) |  | 0 |  | Jiuhai Chen, Jonas Mueller, Vassilis N. Ioannidis, Soji Adeshina, Yangkun Wang, Tom Goldstein, David Wipf |  |
| 945 |  |  [Geometric and Physical Quantities improve E(3) Equivariant Message Passing](https://openreview.net/forum?id=_xwr8gOBeV1) |  | 0 |  | Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J. Bekkers, Max Welling |  |
| 946 |  |  [SphereFace2: Binary Classification is All You Need for Deep Face Recognition](https://openreview.net/forum?id=l3SDgUh7qZO) |  | 0 |  | Yandong Wen, Weiyang Liu, Adrian Weller, Bhiksha Raj, Rita Singh |  |
| 947 |  |  [Boosting Randomized Smoothing with Variance Reduced Classifiers](https://openreview.net/forum?id=mHu2vIds_-b) |  | 0 |  | Miklós Z. Horváth, Mark Niklas Müller, Marc Fischer, Martin T. Vechev |  |
| 948 |  |  [SOSP: Efficiently Capturing Global Correlations by Second-Order Structured Pruning](https://openreview.net/forum?id=t5EmXZ3ZLR) |  | 0 |  | Manuel Nonnenmacher, Thomas Pfeil, Ingo Steinwart, David Reeb |  |
| 949 |  |  [Relational Multi-Task Learning: Modeling Relations between Data and Tasks](https://openreview.net/forum?id=8Py-W8lSUgy) |  | 0 |  | Kaidi Cao, Jiaxuan You, Jure Leskovec |  |
| 950 |  |  [CoBERL: Contrastive BERT for Reinforcement Learning](https://openreview.net/forum?id=sRZ3GhmegS) |  | 0 |  | Andrea Banino, Adrià Puigdomènech Badia, Jacob C. Walker, Tim Scholtes, Jovana Mitrovic, Charles Blundell |  |
| 951 |  |  [Optimal Transport for Causal Discovery](https://openreview.net/forum?id=qwBK94cP1y) |  | 0 |  | Ruibo Tu, Kun Zhang, Hedvig Kjellström, Cheng Zhang |  |
| 952 |  |  [On Bridging Generic and Personalized Federated Learning for Image Classification](https://openreview.net/forum?id=I1hQbx10Kxn) |  | 0 |  | HongYou Chen, WeiLun Chao |  |
| 953 |  |  [Value Gradient weighted Model-Based Reinforcement Learning](https://openreview.net/forum?id=4-D6CZkRXxI) |  | 0 |  | Claas Voelcker, Victor Liao, Animesh Garg, Amirmassoud Farahmand |  |
| 954 |  |  [Fairness in Representation for Multilingual NLP: Insights from Controlled Experiments on Conditional Language Modeling](https://openreview.net/forum?id=-llS6TiOew) |  | 0 |  | Ada Wan |  |
| 955 |  |  [Reinforcement Learning with Sparse Rewards using Guidance from Offline Demonstration](https://openreview.net/forum?id=YJ1WzgMVsMt) |  | 0 |  | Desik Rengarajan, Gargi Vaidya, Akshay Sarvesh, Dileep M. Kalathil, Srinivas Shakkottai |  |
| 956 |  |  [Linking Emergent and Natural Languages via Corpus Transfer](https://openreview.net/forum?id=49A1Y6tRhaq) |  | 0 |  | Shunyu Yao, Mo Yu, Yang Zhang, Karthik R. Narasimhan, Joshua B. Tenenbaum, Chuang Gan |  |
| 957 |  |  [TAMP-S2GCNets: Coupling Time-Aware Multipersistence Knowledge Representation with Spatio-Supra Graph Convolutional Networks for Time-Series Forecasting](https://openreview.net/forum?id=wv6g8fWLX2q) |  | 0 |  | Yuzhou Chen, Ignacio SegoviaDominguez, Baris Coskunuzer, Yulia R. Gel |  |
| 958 |  |  [The MultiBERTs: BERT Reproductions for Robustness Analysis](https://openreview.net/forum?id=K0E_F0gFDgA) |  | 0 |  | Thibault Sellam, Steve Yadlowsky, Ian Tenney, Jason Wei, Naomi Saphra, Alexander D'Amour, Tal Linzen, Jasmijn Bastings, Iulia Raluca Turc, Jacob Eisenstein, Dipanjan Das, Ellie Pavlick |  |
| 959 |  |  [Message Passing Neural PDE Solvers](https://openreview.net/forum?id=vSix3HPYKSU) |  | 0 |  | Johannes Brandstetter, Daniel E. Worrall, Max Welling |  |
| 960 |  |  [Multi-Stage Episodic Control for Strategic Exploration in Text Games](https://openreview.net/forum?id=Ek7PSN7Y77z) |  | 0 |  | Jens Tuyls, Shunyu Yao, Sham M. Kakade, Karthik Narasimhan |  |
| 961 |  |  [Exploring the Limits of Large Scale Pre-training](https://openreview.net/forum?id=V3C8p78sDa) |  | 0 |  | Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, Hanie Sedghi |  |
| 962 |  |  [Universal Approximation Under Constraints is Possible with Transformers](https://openreview.net/forum?id=JGO8CvG5S9) |  | 0 |  | Anastasis Kratsios, Behnoosh Zamanlooy, Tianlin Liu, Ivan Dokmanic |  |
| 963 |  |  [Scaling Laws for Neural Machine Translation](https://openreview.net/forum?id=hR_SMu8cxCV) |  | 0 |  | Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Garcia, Ciprian Chelba, Colin Cherry |  |
| 964 |  |  [AdaRL: What, Where, and How to Adapt in Transfer Reinforcement Learning](https://openreview.net/forum?id=8H5bpVwvt5) |  | 0 |  | Biwei Huang, Fan Feng, Chaochao Lu, Sara Magliacane, Kun Zhang |  |
| 965 |  |  [Independent SE(3)-Equivariant Models for End-to-End Rigid Protein Docking](https://openreview.net/forum?id=GQjaI9mLet) |  | 0 |  | OctavianEugen Ganea, Xinyuan Huang, Charlotte Bunne, Yatao Bian, Regina Barzilay, Tommi S. Jaakkola, Andreas Krause |  |
| 966 |  |  [Towards a Unified View of Parameter-Efficient Transfer Learning](https://openreview.net/forum?id=0RDcd5Axok) |  | 0 |  | Junxian He, Chunting Zhou, Xuezhe Ma, Taylor BergKirkpatrick, Graham Neubig |  |
| 967 |  |  [GNN-LM: Language Modeling based on Global Contexts via GNN](https://openreview.net/forum?id=BS49l-B5Bql) |  | 0 |  | Yuxian Meng, Shi Zong, Xiaoya Li, Xiaofei Sun, Tianwei Zhang, Fei Wu, Jiwei Li |  |
| 968 |  |  [Continual Learning with Filter Atom Swapping](https://openreview.net/forum?id=metRpM4Zrcb) |  | 0 |  | Zichen Miao, Ze Wang, Wei Chen, Qiang Qiu |  |
| 969 |  |  [Continual Learning with Recursive Gradient Optimization](https://openreview.net/forum?id=7YDLgf9_zgm) |  | 0 |  | Hao Liu, Huaping Liu |  |
| 970 |  |  [NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning](https://openreview.net/forum?id=g8NJR6fCCl8) |  | 0 |  | ChunHao Chang, Rich Caruana, Anna Goldenberg |  |
| 971 |  |  [Learnability of convolutional neural networks for infinite dimensional input via mixed and anisotropic smoothness](https://openreview.net/forum?id=dgxFTxuJ50e) |  | 0 |  | Sho Okumoto, Taiji Suzuki |  |
| 972 |  |  [Improved deterministic l2 robustness on CIFAR-10 and CIFAR-100](https://openreview.net/forum?id=tD7eCtaSkR) |  | 0 |  | Sahil Singla, Surbhi Singla, Soheil Feizi |  |
| 973 |  |  [Transition to Linearity of Wide Neural Networks is an Emerging Property of Assembling Weak Models](https://openreview.net/forum?id=CyKHoKyvgnp) |  | 0 |  | Chaoyue Liu, Libin Zhu, Mikhail Belkin |  |
| 974 |  |  [Looking Back on Learned Experiences For Class/task Incremental Learning](https://openreview.net/forum?id=RxplU3vmBx) |  | 0 |  | Mozhgan PourKeshavarz, Guoying Zhao, Mohammad Sabokrou |  |
| 975 |  |  [EntQA: Entity Linking as Question Answering](https://openreview.net/forum?id=US2rTP5nm_) |  | 0 |  | Wenzheng Zhang, Wenyue Hua, Karl Stratos |  |
| 976 |  |  [Escaping limit cycles: Global convergence for constrained nonconvex-nonconcave minimax problems](https://openreview.net/forum?id=2_vhkAMARk) |  | 0 |  | Thomas Pethick, Puya Latafat, Panos Patrinos, Olivier Fercoq, Volkan Cevher |  |
| 977 |  |  [Compositional Attention: Disentangling Search and Retrieval](https://openreview.net/forum?id=IwJPj2MBcIa) |  | 0 |  | Sarthak Mittal, Sharath Chandra Raparthy, Irina Rish, Yoshua Bengio, Guillaume Lajoie |  |
| 978 |  |  [Contrastive Fine-grained Class Clustering via Generative Adversarial Networks](https://openreview.net/forum?id=XWODe7ZLn8f) |  | 0 |  | Yunji Kim, JungWoo Ha |  |
| 979 |  |  [Learning Multimodal VAEs through Mutual Supervision](https://openreview.net/forum?id=1xXvPrAshao) |  | 0 |  | Tom Joy, Yuge Shi, Philip H. S. Torr, Tom Rainforth, Sebastian M. Schmon, Siddharth Narayanaswamy |  |
| 980 |  |  [When should agents explore?](https://openreview.net/forum?id=dEwfxt14bca) |  | 0 |  | Miruna Pislar, David Szepesvari, Georg Ostrovski, Diana L. Borsa, Tom Schaul |  |
| 981 |  |  [Revisiting Design Choices in Offline Model Based Reinforcement Learning](https://openreview.net/forum?id=zz9hXVhf40) |  | 0 |  | Cong Lu, Philip J. Ball, Jack ParkerHolder, Michael A. Osborne, Stephen J. Roberts |  |
| 982 |  |  [NASPY: Automated Extraction of Automated Machine Learning Models](https://openreview.net/forum?id=KhLK0sHMgXK) |  | 0 |  | Xiaoxuan Lou, Shangwei Guo, Jiwei Li, Yaoxin Wu, Tianwei Zhang |  |
| 983 |  |  [COptiDICE: Offline Constrained Reinforcement Learning via Stationary Distribution Correction Estimation](https://openreview.net/forum?id=FLA55mBee6Q) |  | 0 |  | Jongmin Lee, Cosmin Paduraru, Daniel J. Mankowitz, Nicolas Heess, Doina Precup, KeeEung Kim, Arthur Guez |  |
| 984 |  |  [Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers](https://openreview.net/forum?id=nhnJ3oo6AB) |  | 0 |  | Ruihan Yang, Minghao Zhang, Nicklas Hansen, Huazhe Xu, Xiaolong Wang |  |
| 985 |  |  [ViTGAN: Training GANs with Vision Transformers](https://openreview.net/forum?id=dwg5rXg1WS_) |  | 0 |  | Kwonjoon Lee, Huiwen Chang, Lu Jiang, Han Zhang, Zhuowen Tu, Ce Liu |  |
| 986 |  |  [POETREE: Interpretable Policy Learning with Adaptive Decision Trees](https://openreview.net/forum?id=AJsI-ymaKn_) |  | 0 |  | Alizée Pace, Alex J. Chan, Mihaela van der Schaar |  |
| 987 |  |  [TRGP: Trust Region Gradient Projection for Continual Learning](https://openreview.net/forum?id=iEvAf8i6JjO) |  | 0 |  | Sen Lin, Li Yang, Deliang Fan, Junshan Zhang |  |
| 988 |  |  [Properties from mechanisms: an equivariance perspective on identifiable representation learning](https://openreview.net/forum?id=g5ynW-jMq4M) |  | 0 |  | Kartik Ahuja, Jason S. Hartford, Yoshua Bengio |  |
| 989 |  |  [Revisiting Over-smoothing in BERT from the Perspective of Graph](https://openreview.net/forum?id=dUV91uaXm3) |  | 0 |  | Han Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen M. S. Lee, James T. Kwok |  |
| 990 |  |  [Training invariances and the low-rank phenomenon: beyond linear networks](https://openreview.net/forum?id=XEW8CQgArno) |  | 0 |  | Thien Le, Stefanie Jegelka |  |
| 991 |  |  [Learning Long-Term Reward Redistribution via Randomized Return Decomposition](https://openreview.net/forum?id=lpkGn3k2YdD) |  | 0 |  | Zhizhou Ren, Ruihan Guo, Yuan Zhou, Jian Peng |  |
| 992 |  |  [What Happens after SGD Reaches Zero Loss? --A Mathematical Framework](https://openreview.net/forum?id=siCt4xZn5Ve) |  | 0 |  | Zhiyuan Li, Tianhao Wang, Sanjeev Arora |  |
| 993 |  |  [Graph-Augmented Normalizing Flows for Anomaly Detection of Multiple Time Series](https://openreview.net/forum?id=45L_dgP48Vd) |  | 0 |  | Enyan Dai, Jie Chen |  |
| 994 |  |  [Autoregressive Quantile Flows for Predictive Uncertainty Estimation](https://openreview.net/forum?id=z1-I6rOKv1S) |  | 0 |  | Phillip Si, Allan Bishop, Volodymyr Kuleshov |  |
| 995 |  |  [On the Importance of Firth Bias Reduction in Few-Shot Classification](https://openreview.net/forum?id=DNRADop4ksB) |  | 0 |  | Saba Ghaffari, Ehsan Saleh, David A. Forsyth, YuXiong Wang |  |
| 996 |  |  [Finite-Time Convergence and Sample Complexity of Multi-Agent Actor-Critic Reinforcement Learning with Average Reward](https://openreview.net/forum?id=04pGUg0-pdZ) |  | 0 |  | Hairi, Jia Liu, Songtao Lu |  |
| 997 |  |  [Towards Understanding the Data Dependency of Mixup-style Training](https://openreview.net/forum?id=ieNJYujcGDO) |  | 0 |  | Muthu Chidambaram, Xiang Wang, Yuzheng Hu, Chenwei Wu, Rong Ge |  |
| 998 |  |  [Self-Supervision Enhanced Feature Selection with Correlated Gates](https://openreview.net/forum?id=oDFvtxzPOx) |  | 0 |  | Changhee Lee, Fergus Imrie, Mihaela van der Schaar |  |
| 999 |  |  [Score-Based Generative Modeling with Critically-Damped Langevin Diffusion](https://openreview.net/forum?id=CzceR82CYc) |  | 0 |  | Tim Dockhorn, Arash Vahdat, Karsten Kreis |  |
| 1000 |  |  [Controlling Directions Orthogonal to a Classifier](https://openreview.net/forum?id=DIjCrlsu6Z) |  | 0 |  | Yilun Xu, Hao He, Tianxiao Shen, Tommi S. Jaakkola |  |
| 1001 |  |  [R5: Rule Discovery with Reinforced and Recurrent Relational Reasoning](https://openreview.net/forum?id=2eXhNpHeW6E) |  | 0 |  | Shengyao Lu, Bang Liu, Keith G. Mills, Shangling Jui, Di Niu |  |
| 1002 |  |  [Representation Learning for Online and Offline RL in Low-rank MDPs](https://openreview.net/forum?id=J4iSIR9fhY0) |  | 0 |  | Masatoshi Uehara, Xuezhou Zhang, Wen Sun |  |
| 1003 |  |  [Lossless Compression with Probabilistic Circuits](https://openreview.net/forum?id=X_hByk2-5je) |  | 0 |  | Anji Liu, Stephan Mandt, Guy Van den Broeck |  |
| 1004 |  |  [Understanding Domain Randomization for Sim-to-real Transfer](https://openreview.net/forum?id=T8vZHIRTrY) |  | 0 |  | Xiaoyu Chen, Jiachen Hu, Chi Jin, Lihong Li, Liwei Wang |  |
| 1005 |  |  [$\mathrm{SO}(2)$-Equivariant Reinforcement Learning](https://openreview.net/forum?id=7F9cOhdvfk_) |  | 0 |  | Dian Wang, Robin Walters, Robert Platt |  |
| 1006 |  |  [Scarf: Self-Supervised Contrastive Learning using Random Feature Corruption](https://openreview.net/forum?id=CuV_qYkmKb3) |  | 0 |  | Dara Bahri, Heinrich Jiang, Yi Tay, Donald Metzler |  |
| 1007 |  |  [Responsible Disclosure of Generative Models Using Scalable Fingerprinting](https://openreview.net/forum?id=sOK-zS6WHB) |  | 0 |  | Ning Yu, Vladislav Skripniuk, Dingfan Chen, Larry S. Davis, Mario Fritz |  |
| 1008 |  |  [Path Auxiliary Proposal for MCMC in Discrete Space](https://openreview.net/forum?id=JSR-YDImK95) |  | 0 |  | Haoran Sun, Hanjun Dai, Wei Xia, Arun Ramamurthy |  |
| 1009 |  |  [Possibility Before Utility: Learning And Using Hierarchical Affordances](https://openreview.net/forum?id=7b4zxUnrO2N) |  | 0 |  | Robby Costales, Shariq Iqbal, Fei Sha |  |
| 1010 |  |  [Interpretable Unsupervised Diversity Denoising and Artefact Removal](https://openreview.net/forum?id=DfMqlB0PXjM) |  | 0 |  | Mangal Prakash, Mauricio Delbracio, Peyman Milanfar, Florian Jug |  |
| 1011 |  |  [Half-Inverse Gradients for Physical Deep Learning](https://openreview.net/forum?id=HTx7vrlLBEj) |  | 0 |  | Patrick Schnell, Philipp Holl, Nils Thuerey |  |
| 1012 |  |  [EE-Net: Exploitation-Exploration Neural Networks in Contextual Bandits](https://openreview.net/forum?id=X_ch3VrNSRg) |  | 0 |  | Yikun Ban, Yuchen Yan, Arindam Banerjee, Jingrui He |  |
| 1013 |  |  [Spike-inspired rank coding for fast and accurate recurrent neural networks](https://openreview.net/forum?id=iMH1e5k7n3L) |  | 0 |  | Alan Jeffares, Qinghai Guo, Pontus Stenetorp, Timoleon Moraitis |  |
| 1014 |  |  [How to Robustify Black-Box ML Models? A Zeroth-Order Optimization Perspective](https://openreview.net/forum?id=W9G_ImpHlQd) |  | 0 |  | Yimeng Zhang, Yuguang Yao, Jinghan Jia, Jinfeng Yi, Mingyi Hong, Shiyu Chang, Sijia Liu |  |
| 1015 |  |  [RelaxLoss: Defending Membership Inference Attacks without Losing Utility](https://openreview.net/forum?id=FEDfGWVZYIn) |  | 0 |  | Dingfan Chen, Ning Yu, Mario Fritz |  |
| 1016 |  |  [Analyzing and Improving the Optimization Landscape of Noise-Contrastive Estimation](https://openreview.net/forum?id=eBS-3YiaIL-) |  | 0 |  | Bingbin Liu, Elan Rosenfeld, Pradeep Kumar Ravikumar, Andrej Risteski |  |
| 1017 |  |  [Contact Points Discovery for Soft-Body Manipulations with Differentiable Physics](https://openreview.net/forum?id=mmUA7_O9mjY) |  | 0 |  | Sizhe Li, Zhiao Huang, Tao Du, Hao Su, Joshua B. Tenenbaum, Chuang Gan |  |
| 1018 |  |  [Leveraging Automated Unit Tests for Unsupervised Code Translation](https://openreview.net/forum?id=cmt-6KtR4c4) |  | 0 |  | Baptiste Rozière, Jie Zhang, François Charton, Mark Harman, Gabriel Synnaeve, Guillaume Lample |  |
| 1019 |  |  [Scalable Sampling for Nonsymmetric Determinantal Point Processes](https://openreview.net/forum?id=BB4e8Atc1eR) |  | 0 |  | Insu Han, Mike Gartrell, Jennifer Gillenwater, Elvis Dohmatob, Amin Karbasi |  |
| 1020 |  |  [Amortized Tree Generation for Bottom-up Synthesis Planning and Synthesizable Molecular Design](https://openreview.net/forum?id=FRxhHdnxt1) |  | 0 |  | Wenhao Gao, Rocío Mercado, Connor W. Coley |  |
| 1021 |  |  [Iterative Refinement Graph Neural Network for Antibody Sequence-Structure Co-design](https://openreview.net/forum?id=LI2bhrE_2A) |  | 0 |  | Wengong Jin, Jeremy Wohlwend, Regina Barzilay, Tommi S. Jaakkola |  |
| 1022 |  |  [Churn Reduction via Distillation](https://openreview.net/forum?id=HbtFCX2PLq0) |  | 0 |  | Heinrich Jiang, Harikrishna Narasimhan, Dara Bahri, Andrew Cotter, Afshin Rostamizadeh |  |
| 1023 |  |  [Learning Causal Models from Conditional Moment Restrictions by Importance Weighting](https://openreview.net/forum?id=7twQI5VnC8) |  | 0 |  | Masahiro Kato, Masaaki Imaizumi, Kenichiro McAlinn, Shota Yasui, Haruo Kakehi |  |
| 1024 |  |  [Scalable One-Pass Optimisation of High-Dimensional Weight-Update Hyperparameters by Implicit Differentiation](https://openreview.net/forum?id=hfU7Ka5cfrC) |  | 0 |  | Ross M. Clarke, Elre Talea Oldewage, José Miguel HernándezLobato |  |
| 1025 |  |  [Sample Efficient Deep Reinforcement Learning via Uncertainty Estimation](https://openreview.net/forum?id=vrW3tvDfOJQ) |  | 0 |  | Vincent Mai, Kaustubh Mani, Liam Paull |  |
| 1026 |  |  [Learning Pruning-Friendly Networks via Frank-Wolfe: One-Shot, Any-Sparsity, And No Retraining](https://openreview.net/forum?id=O1DEtITim__) |  | 0 |  | Lu Miao, Xiaolong Luo, Tianlong Chen, Wuyang Chen, Dong Liu, Zhangyang Wang |  |
| 1027 |  |  [Learning transferable motor skills with hierarchical latent mixture policies](https://openreview.net/forum?id=qTHBE7E9iej) |  | 0 |  | Dushyant Rao, Fereshteh Sadeghi, Leonard Hasenclever, Markus Wulfmeier, Martina Zambelli, Giulia Vezzani, Dhruva Tirumala, Yusuf Aytar, Josh Merel, Nicolas Heess, Raia Hadsell |  |
| 1028 |  |  [Compositional Training for End-to-End Deep AUC Maximization](https://openreview.net/forum?id=gPvB4pdu_Z) |  | 0 |  | Zhuoning Yuan, Zhishuai Guo, Nitesh V. Chawla, Tianbao Yang |  |
| 1029 |  |  [Explanations of Black-Box Models based on Directional Feature Interactions](https://openreview.net/forum?id=45Mr7LeKR9) |  | 0 |  | Aria Masoomi, Davin Hill, Zhonghui Xu, Craig P. Hersh, Edwin K. Silverman, Peter J. Castaldi, Stratis Ioannidis, Jennifer G. Dy |  |
| 1030 |  |  [On Lottery Tickets and Minimal Task Representations in Deep Reinforcement Learning](https://openreview.net/forum?id=Fl3Mg_MZR-) |  | 0 |  | Marc Aurel Vischer, Robert Tjarko Lange, Henning Sprekeler |  |
| 1031 |  |  [Self-supervised Learning is More Robust to Dataset Imbalance](https://openreview.net/forum?id=4AZz9osqrar) |  | 0 |  | Hong Liu, Jeff Z. HaoChen, Adrien Gaidon, Tengyu Ma |  |
| 1032 |  |  [Ab-Initio Potential Energy Surfaces by Pairing GNNs with Neural Wave Functions](https://openreview.net/forum?id=apv504XsysP) |  | 0 |  | Nicholas Gao, Stephan Günnemann |  |
| 1033 |  |  [Near-Optimal Reward-Free Exploration for Linear Mixture MDPs with Plug-in Solver](https://openreview.net/forum?id=SidzxAb9k30) |  | 0 |  | Xiaoyu Chen, Jiachen Hu, Lin Yang, Liwei Wang |  |
| 1034 |  |  [Meta Discovery: Learning to Discover Novel Classes given Very Limited Data](https://openreview.net/forum?id=MEpKGLsY8f) |  | 0 |  | Haoang Chi, Feng Liu, Wenjing Yang, Long Lan, Tongliang Liu, Bo Han, Gang Niu, Mingyuan Zhou, Masashi Sugiyama |  |
| 1035 |  |  [Constrained Policy Optimization via Bayesian World Models](https://openreview.net/forum?id=PRZoSmCinhf) |  | 0 |  | Yarden As, Ilnura Usmanova, Sebastian Curi, Andreas Krause |  |
| 1036 |  |  [VAE Approximation Error: ELBO and Exponential Families](https://openreview.net/forum?id=OIs3SxU5Ynl) |  | 0 |  | Alexander Shekhovtsov, Dmitrij Schlesinger, Boris Flach |  |
| 1037 |  |  [Generalized Decision Transformer for Offline Hindsight Information Matching](https://openreview.net/forum?id=CAjxVodl_v) |  | 0 |  | Hiroki Furuta, Yutaka Matsuo, Shixiang Shane Gu |  |
| 1038 |  |  [Unifying Likelihood-free Inference with Black-box Optimization and Beyond](https://openreview.net/forum?id=1HxTO6CTkz) |  | 0 |  | Dinghuai Zhang, Jie Fu, Yoshua Bengio, Aaron C. Courville |  |
| 1039 |  |  [DEPTS: Deep Expansion Learning for Periodic Time Series Forecasting](https://openreview.net/forum?id=AJAR-JgNw__) |  | 0 |  | Wei Fan, Shun Zheng, Xiaohan Yi, Wei Cao, Yanjie Fu, Jiang Bian, TieYan Liu |  |
| 1040 |  |  [Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions](https://openreview.net/forum?id=tBtoZYKd9n) |  | 0 |  | Leslie O'Bray, Max Horn, Bastian Rieck, Karsten M. Borgwardt |  |
| 1041 |  |  [Natural Posterior Network: Deep Bayesian Predictive Uncertainty for Exponential Family Distributions](https://openreview.net/forum?id=tV3N0DWMxCg) |  | 0 |  | Bertrand Charpentier, Oliver Borchert, Daniel Zügner, Simon Geisler, Stephan Günnemann |  |
| 1042 |  |  [Learning Altruistic Behaviours in Reinforcement Learning without External Rewards](https://openreview.net/forum?id=KxbhdyiPHE) |  | 0 |  | Tim Franzmeyer, Mateusz Malinowski, João F. Henriques |  |
| 1043 |  |  [Context-Aware Sparse Deep Coordination Graphs](https://openreview.net/forum?id=wQfgfb8VKTn) |  | 0 |  | Tonghan Wang, Liang Zeng, Weijun Dong, Qianlan Yang, Yang Yu, Chongjie Zhang |  |
| 1044 |  |  [On the approximation properties of recurrent encoder-decoder architectures](https://openreview.net/forum?id=xDIvIqQ3DXD) |  | 0 |  | Zhong Li, Haotian Jiang, Qianxiao Li |  |
| 1045 |  |  [Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models](https://openreview.net/forum?id=Nfl-iXa-y7R) |  | 0 |  | Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, Christopher Ré |  |
| 1046 |  |  [8-bit Optimizers via Block-wise Quantization](https://openreview.net/forum?id=shpkpVXzo3h) |  | 0 |  | Tim Dettmers, Mike Lewis, Sam Shleifer, Luke Zettlemoyer |  |
| 1047 |  |  [Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks](https://openreview.net/forum?id=yeP_zx9vqNm) |  | 0 |  | Anne Harrington, Arturo Deza |  |
| 1048 |  |  [Omni-Dimensional Dynamic Convolution](https://openreview.net/forum?id=DmpCfq6Mg39) |  | 0 |  | Chao Li, Aojun Zhou, Anbang Yao |  |
| 1049 |  |  [EViT: Expediting Vision Transformers via Token Reorganizations](https://openreview.net/forum?id=BjyvwnXXVn_) |  | 0 |  | Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, Pengtao Xie |  |
| 1050 |  |  [D-CODE: Discovering Closed-form ODEs from Observed Trajectories](https://openreview.net/forum?id=wENMvIsxNN) |  | 0 |  | Zhaozhi Qian, Krzysztof Kacprzyk, Mihaela van der Schaar |  |
| 1051 |  |  [Spanning Tree-based Graph Generation for Molecules](https://openreview.net/forum?id=w60btE_8T2m) |  | 0 |  | Sungsoo Ahn, Binghong Chen, Tianzhe Wang, Le Song |  |
| 1052 |  |  [Policy improvement by planning with Gumbel](https://openreview.net/forum?id=bERaNdoegnO) |  | 0 |  | Ivo Danihelka, Arthur Guez, Julian Schrittwieser, David Silver |  |
| 1053 |  |  [Learning Optimal Conformal Classifiers](https://openreview.net/forum?id=t8O-4LKFVx) |  | 0 |  | David Stutz, Krishnamurthy Dvijotham, Ali Taylan Cemgil, Arnaud Doucet |  |
| 1054 |  |  [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://openreview.net/forum?id=9Vrb9D0WI4) |  | 0 |  | Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike TianJian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, Alexander M. Rush |  |
| 1055 |  |  [Continuous-Time Meta-Learning with Forward Mode Differentiation](https://openreview.net/forum?id=57PipS27Km) |  | 0 |  | Tristan Deleu, David Kanaa, Leo Feng, Giancarlo Kerg, Yoshua Bengio, Guillaume Lajoie, PierreLuc Bacon |  |
| 1056 |  |  [On the relation between statistical learning and perceptual distances](https://openreview.net/forum?id=zXM0b4hi5_B) |  | 0 |  | Alexander Hepburn, Valero Laparra, Raúl SantosRodríguez, Johannes Ballé, Jesus Malo |  |
| 1057 |  |  [Implicit Bias of Projected Subgradient Method Gives Provable Robust Recovery of Subspaces of Unknown Codimension](https://openreview.net/forum?id=vA7doMdgi75) |  | 0 |  | Paris Giampouras, Benjamin David Haeffele, René Vidal |  |
| 1058 |  |  [On the Optimal Memorization Power of ReLU Neural Networks](https://openreview.net/forum?id=MkTPtnjeYTV) |  | 0 |  | Gal Vardi, Gilad Yehudai, Ohad Shamir |  |
| 1059 |  |  [Programmatic Reinforcement Learning without Oracles](https://openreview.net/forum?id=6Tk2noBdvxt) |  | 0 |  | Wenjie Qiu, He Zhu |  |
| 1060 |  |  [When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations](https://openreview.net/forum?id=LtKcMgGOeLt) |  | 0 |  | Xiangning Chen, ChoJui Hsieh, Boqing Gong |  |
| 1061 |  |  [Online Hyperparameter Meta-Learning with Hypergradient Distillation](https://openreview.net/forum?id=01AMRlen9wJ) |  | 0 |  | Haebeom Lee, Hayeon Lee, Jaewoong Shin, Eunho Yang, Timothy M. Hospedales, Sung Ju Hwang |  |
| 1062 |  |  [Tighter Sparse Approximation Bounds for ReLU Neural Networks](https://openreview.net/forum?id=LBvk4QWIUpm) |  | 0 |  | Carles DomingoEnrich, Youssef Mroueh |  |
| 1063 |  |  [Long Expressive Memory for Sequence Modeling](https://openreview.net/forum?id=vwj6aUeocyf) |  | 0 |  | T. Konstantin Rusch, Siddhartha Mishra, N. Benjamin Erichson, Michael W. Mahoney |  |
| 1064 |  |  [Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy](https://openreview.net/forum?id=LzQQ89U1qm_) |  | 0 |  | Jiehui Xu, Haixu Wu, Jianmin Wang, Mingsheng Long |  |
| 1065 |  |  [Progressive Distillation for Fast Sampling of Diffusion Models](https://openreview.net/forum?id=TIdIXIpzhoI) |  | 0 |  | Tim Salimans, Jonathan Ho |  |
| 1066 |  |  [A General Analysis of Example-Selection for Stochastic Gradient Descent](https://openreview.net/forum?id=7gWSJrP3opB) |  | 0 |  | Yucheng Lu, Si Yi Meng, Christopher De Sa |  |
| 1067 |  |  [Assessing Generalization of SGD via Disagreement](https://openreview.net/forum?id=WvOGCEAQhxl) |  | 0 |  | Yiding Jiang, Vaishnavh Nagarajan, Christina Baek, J. Zico Kolter |  |
| 1068 |  |  [Generative Planning for Temporally Coordinated Exploration in Reinforcement Learning](https://openreview.net/forum?id=YZHES8wIdE) |  | 0 |  | Haichao Zhang, Wei Xu, Haonan Yu |  |
| 1069 |  |  [Pessimistic Bootstrapping for Uncertainty-Driven Offline Reinforcement Learning](https://openreview.net/forum?id=Y4cs1Z3HnqL) |  | 0 |  | Chenjia Bai, Lingxiao Wang, Zhuoran Yang, ZhiHong Deng, Animesh Garg, Peng Liu, Zhaoran Wang |  |
| 1070 |  |  [Equivariant Subgraph Aggregation Networks](https://openreview.net/forum?id=dFbKQaRk15w) |  | 0 |  | Beatrice Bevilacqua, Fabrizio Frasca, Derek Lim, Balasubramaniam Srinivasan, Chen Cai, Gopinath Balamurugan, Michael M. Bronstein, Haggai Maron |  |
| 1071 |  |  [How Do Vision Transformers Work?](https://openreview.net/forum?id=D78Go4hVcxO) |  | 0 |  | Namuk Park, Songkuk Kim |  |
| 1072 |  |  [Variational methods for simulation-based inference](https://openreview.net/forum?id=kZ0UYdhqkNY) |  | 0 |  | Manuel Glöckler, Michael Deistler, Jakob H. Macke |  |
| 1073 |  |  [Tackling the Generative Learning Trilemma with Denoising Diffusion GANs](https://openreview.net/forum?id=JprM0p-q0Co) |  | 0 |  | Zhisheng Xiao, Karsten Kreis, Arash Vahdat |  |
| 1074 |  |  [Imbedding Deep Neural Networks](https://openreview.net/forum?id=yKIAXjkJc2F) |  | 0 |  | Andrew Corbett, Dmitry Kangin |  |
| 1075 |  |  [Understanding and Preventing Capacity Loss in Reinforcement Learning](https://openreview.net/forum?id=ZkC8wKoLbQ7) |  | 0 |  | Clare Lyle, Mark Rowland, Will Dabney |  |
| 1076 |  |  [Source-Free Adaptation to Measurement Shift via Bottom-Up Feature Restoration](https://openreview.net/forum?id=1JDiK_TbV4S) |  | 0 |  | Cian Eastwood, Ian Mason, Christopher K. I. Williams, Bernhard Schölkopf |  |
| 1077 |  |  [The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design](https://openreview.net/forum?id=lnEaqbTJIRz) |  | 0 |  | Yoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, Amnon Shashua |  |
| 1078 |  |  [Emergent Communication at Scale](https://openreview.net/forum?id=AUGBfDIV9rL) |  | 0 |  | Rahma Chaabouni, Florian Strub, Florent Altché, Eugene Tarassov, Corentin Tallec, Elnaz Davoodi, Kory Wallace Mathewson, Olivier Tieleman, Angeliki Lazaridou, Bilal Piot |  |
| 1079 |  |  [Superclass-Conditional Gaussian Mixture Model For Learning Fine-Grained Embeddings](https://openreview.net/forum?id=vds4SNooOe) |  | 0 |  | Jingchao Ni, Wei Cheng, Zhengzhang Chen, Takayoshi Asakura, Tomoya Soma, Sho Kato, Haifeng Chen |  |
| 1080 |  |  [SHINE: SHaring the INverse Estimate from the forward pass for bi-level optimization and implicit models](https://openreview.net/forum?id=-ApAkox5mp) |  | 0 |  | Zaccharie Ramzi, Florian Mannel, Shaojie Bai, JeanLuc Starck, Philippe Ciuciu, Thomas Moreau |  |
| 1081 |  |  [Towards Deployment-Efficient Reinforcement Learning: Lower Bound and Optimality](https://openreview.net/forum?id=ccWaPGl9Hq) |  | 0 |  | Jiawei Huang, Jinglin Chen, Li Zhao, Tao Qin, Nan Jiang, TieYan Liu |  |
| 1082 |  |  [Task Relatedness-Based Generalization Bounds for Meta Learning](https://openreview.net/forum?id=A3HHaEdqAJL) |  | 0 |  | Jiechao Guan, Zhiwu Lu |  |
| 1083 |  |  [IntSGD: Adaptive Floatless Compression of Stochastic Gradients](https://openreview.net/forum?id=pFyXqxChZc) |  | 0 |  | Konstantin Mishchenko, Bokun Wang, Dmitry Kovalev, Peter Richtárik |  |
| 1084 |  |  [PAC-Bayes Information Bottleneck](https://openreview.net/forum?id=iLHOIDsPv1P) |  | 0 |  | Zifeng Wang, ShaoLun Huang, Ercan Engin Kuruoglu, Jimeng Sun, Xi Chen, Yefeng Zheng |  |
| 1085 |  |  [Byzantine-Robust Learning on Heterogeneous Datasets via Bucketing](https://openreview.net/forum?id=jXKKDEi5vJt) |  | 0 |  | Sai Praneeth Karimireddy, Lie He, Martin Jaggi |  |
| 1086 |  |  [Understanding the Role of Self Attention for Efficient Speech Recognition](https://openreview.net/forum?id=AvcfxqRy4Y) |  | 0 |  | Kyuhong Shim, Jungwook Choi, Wonyong Sung |  |
| 1087 |  |  [Label Encoding for Regression Networks](https://openreview.net/forum?id=8WawVDdKqlL) |  | 0 |  | Deval Shah, Zi Yu Xue, Tor M. Aamodt |  |
| 1088 |  |  [Equivariant Transformers for Neural Network based Molecular Potentials](https://openreview.net/forum?id=zNHzqZ9wrRB) |  | 0 |  | Philipp Thölke, Gianni De Fabritiis |  |
| 1089 |  |  [SGD Can Converge to Local Maxima](https://openreview.net/forum?id=9XhPLAjjRB) |  | 0 |  | Liu Ziyin, Botao Li, James B. Simon, Masahito Ueda |  |
| 1090 |  |  [Hybrid Local SGD for Federated Learning with Heterogeneous Communications](https://openreview.net/forum?id=H0oaWl6THa) |  | 0 |  | Yuanxiong Guo, Ying Sun, Rui Hu, Yanmin Gong |  |
| 1091 |  |  [Increasing the Cost of Model Extraction with Calibrated Proof of Work](https://openreview.net/forum?id=EAy7C1cgE1L) |  | 0 |  | Adam Dziedzic, Muhammad Ahmad Kaleem, Yu Shen Lu, Nicolas Papernot |  |
| 1092 |  |  [Learning the Dynamics of Physical Systems from Sparse Observations with Finite Element Networks](https://openreview.net/forum?id=HFmAukZ-k-2) |  | 0 |  | Marten Lienen, Stephan Günnemann |  |
| 1093 |  |  [Probabilistic Implicit Scene Completion](https://openreview.net/forum?id=BnQhMqDfcKG) |  | 0 |  | Dongsu Zhang, Changwoon Choi, Inbum Park, Young Min Kim |  |
| 1094 |  |  [Improving Federated Learning Face Recognition via Privacy-Agnostic Clusters](https://openreview.net/forum?id=7l1IjZVddDW) |  | 0 |  | Qiang Meng, Feng Zhou, Hainan Ren, Tianshu Feng, Guochao Liu, Yuanqing Lin |  |
