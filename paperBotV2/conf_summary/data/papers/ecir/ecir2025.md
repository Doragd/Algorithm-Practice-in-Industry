# ECIR2025

## 会议论文列表

本会议共有 198 篇论文

| 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- |
|  |  [A Reproducibility Study for Joint Information Retrieval and Recommendation in Product Search](https://doi.org/10.1007/978-3-031-88717-8_10) |  | 0 |  | Simone Merlo, Guglielmo Faggioli, Nicola Ferro |  |
|  |  [BioRAGent: A Retrieval-Augmented Generation System for Showcasing Generative Query Expansion and Domain-Specific Search for Scientific Q&A](https://doi.org/10.1007/978-3-031-88720-8_1) |  | 0 |  | Samy Ateia, Udo Kruschwitz |  |
|  |  [Conversational Information Retrieval and Recommender Systems](https://doi.org/10.1007/978-3-031-88720-8_44) |  | 0 |  | Guglielmo Faggioli, Nicola Ferro, Simone Merlo |  |
|  |  [Evaluating Sequential Recommendations in the Wild: A Case Study on Offline Accuracy, Click Rates, and Consumption](https://doi.org/10.1007/978-3-031-88711-6_5) |  | 0 |  | Anastasiia Klimashevskaia, Snorre Alvsvåg, Christoph Trattner, Alain D. Starke, Astrid Tessem, Dietmar Jannach |  |
|  |  [DiffGR: A Discrete Diffusion-Based Model for Personalised Recommendation by Reconstructing User-Item Bipartite Graphs](https://doi.org/10.1007/978-3-031-88714-7_23) |  | 0 |  | Zheng Ju, Honghui Du, Elias Z. Tragos, Neil Hurley, Aonghus Lawlor |  |
|  |  [Guiding Retrieval Using LLM-Based Listwise Rankers](https://doi.org/10.1007/978-3-031-88708-6_15) |  | 0 | Large Language Models (LLMs) have shown strong promise as rerankers, especially in “listwise” settings where an LLM is prompted to rerank several search results at once. However, this “cascading” retrieve-and-rerank approach is limited by the bounded recall problem: relevant documents not retrieved initially are permanently excluded from the final ranking. Adaptive retrieval techniques address this problem, but do not work with listwise rerankers because they assume a document's score is computed independently from other documents. In this paper, we propose an adaptation of an existing adaptive retrieval method that supports the listwise setting and helps guide the retrieval process itself (thereby overcoming the bounded recall problem for LLM rerankers). Specifically, our proposed algorithm merges results both from the initial ranking and feedback documents provided by the most relevant documents seen up to that point. Through extensive experiments across diverse LLM rerankers, first stage retrievers, and feedback sources, we demonstrate that our method can improve nDCG@10 by up to 13.23 while keeping the total number of LLM inferences constant and overheads due to the adaptive process minimal. The work opens the door to leveraging LLM-based search in settings where the initial pool of results is limited, e.g., by legacy systems, or by the cost of deploying a semantic first-stage. | Mandeep Rathee, Sean MacAvaney, Avishek Anand |  |
|  |  [Leveraging Query Terms for Efficient Legal Document Recommendation](https://doi.org/10.1007/978-3-031-88714-7_6) |  | 0 |  | André Rolim, Leandro Balby Marinho, Edleno Silva de Moura, Marcos Aurélio Domingues, Ricardo S. Oliveira |  |
|  |  [Examining the Impact of Transcript Variation on Podcast Search and Re-ranking](https://doi.org/10.1007/978-3-031-88714-7_9) |  | 0 |  | Watheq Mansour, J. Shane Culpepper, Joel Mackenzie |  |
|  |  [Counterfactual Query Rewriting to Use Historical Relevance Feedback](https://doi.org/10.1007/978-3-031-88714-7_11) |  | 0 | When a retrieval system receives a query it has encountered before, previous relevance feedback, such as clicks or explicit judgments can help to improve retrieval results. However, the content of a previously relevant document may have changed, or the document might not be available anymore. Despite this evolved corpus, we counterfactually use these previously relevant documents as relevance signals. In this paper we proposed approaches to rewrite user queries and compare them against a system that directly uses the previous qrels for the ranking. We expand queries with terms extracted from the previously relevant documents or derive so-called keyqueries that rank the previously relevant documents to the top of the current corpus. Our evaluation in the CLEF LongEval scenario shows that rewriting queries with historical relevance feedback improves the retrieval effectiveness and even outperforms computationally expensive transformer-based approaches. | Jüri Keller, Maik Fröbe, Gijs Hendriksen, Daria Alexander, Martin Potthast, Matthias Hagen, Philipp Schaer |  |
|  |  [Patience in Proximity: A Simple Early Termination Strategy for HNSW Graph Traversal in Approximate k-Nearest Neighbor Search](https://doi.org/10.1007/978-3-031-88714-7_39) |  | 0 |  | Tommaso Teofili, Jimmy Lin |  |
|  |  [Towards Intent-Driven Transparency in Conversational Search Systems](https://doi.org/10.1007/978-3-031-88720-8_37) |  | 0 |  | Yumeng Wang |  |
|  |  [Combining Dissimilarity Spaces to Improve Approximate Similarity Search](https://doi.org/10.1007/978-3-031-88720-8_30) |  | 0 |  | Elena GarcíaMorato, Felipe Ortega, Javier Gómez |  |
|  |  [News Without Borders: Domain Adaptation of Multilingual Sentence Embeddings for Cross-Lingual News Recommendation](https://doi.org/10.1007/978-3-031-88711-6_8) |  | 0 | Rapidly growing numbers of multilingual news consumers pose an increasing challenge to news recommender systems in terms of providing customized recommendations. First, existing neural news recommenders, even when powered by multilingual language models (LMs), suffer substantial performance losses in zero-shot cross-lingual transfer (ZS-XLT). Second, the current paradigm of fine-tuning the backbone LM of a neural recommender on task-specific data is computationally expensive and infeasible in few-shot recommendation and cold-start setups, where data is scarce or completely unavailable. In this work, we propose a news-adapted sentence encoder (NaSE), domain-specialized from a pretrained massively multilingual sentence encoder (SE). To this end, we construct and leverage PolyNews and PolyNewsParallel, two multilingual news-specific corpora. With the news-adapted multilingual SE in place, we test the effectiveness of (i.e., question the need for) supervised fine-tuning for news recommendation, and propose a simple and strong baseline based on (i) frozen NaSE embeddings and (ii) late click-behavior fusion. We show that NaSE achieves state-of-the-art performance in ZS-XLT in true cold-start and few-shot news recommendation. | Andreea Iana, Fabian David Schmidt, Goran Glavas, Heiko Paulheim |  |
|  |  [LLM is Knowledge Graph Reasoner: LLM's Intuition-Aware Knowledge Graph Reasoning for Cold-Start Sequential Recommendation](https://doi.org/10.1007/978-3-031-88711-6_17) |  | 0 | Knowledge Graphs (KGs) represent relationships between entities in a graph structure and have been widely studied as promising tools for realizing recommendations that consider the accurate content information of items. However, traditional KG-based recommendation methods face fundamental challenges: insufficient consideration of temporal information and poor performance in cold-start scenarios. On the other hand, Large Language Models (LLMs) can be considered databases with a wealth of knowledge learned from the web data, and they have recently gained attention due to their potential application as recommendation systems. Although approaches that treat LLMs as recommendation systems can leverage LLMs' high recommendation literacy, their input token limitations make it impractical to consider the entire recommendation domain dataset and result in scalability issues. To address these challenges, we propose a LLM's Intuition-aware Knowledge graph Reasoning model (LIKR). Our main idea is to treat LLMs as reasoners that output intuitive exploration strategies for KGs. To integrate the knowledge of LLMs and KGs, we trained a recommendation agent through reinforcement learning using a reward function that integrates different recommendation strategies, including LLM's intuition and KG embeddings. By incorporating temporal awareness through prompt engineering and generating textual representations of user preferences from limited interactions, LIKR can improve recommendation performance in cold-start scenarios. Furthermore, LIKR can avoid scalability issues by using KGs to represent recommendation domain datasets and limiting the LLM's output to KG exploration strategies. Experiments on real-world datasets demonstrate that our model outperforms state-of-the-art recommendation methods in cold-start sequential recommendation scenarios. | Keigo Sakurai, Ren Togo, Takahiro Ogawa, Miki Haseyama |  |
|  |  [Embedding Cultural Diversity in Prototype-Based Recommender Systems](https://doi.org/10.1007/978-3-031-88708-6_2) |  | 0 | Popularity bias in recommender systems can increase cultural overrepresentation by favoring norms from dominant cultures and marginalizing underrepresented groups. This issue is critical for platforms offering cultural products, as they influence consumption patterns and human perceptions. In this work, we address popularity bias by identifying demographic biases within prototype-based matrix factorization methods. Using the country of origin as a proxy for cultural identity, we link this demographic attribute to popularity bias by refining the embedding space learning process. First, we propose filtering out irrelevant prototypes to improve representativity. Second, we introduce a regularization technique to enforce a uniform distribution of prototypes within the embedding space. Across four datasets, our results demonstrate a 27% reduction in the average rank of long-tail items and a 2% reduction in the average rank of items from underrepresented countries. Additionally, our model achieves a 2% improvement in HitRatio@10 compared to the state-of-the-art, highlighting that fairness is enhanced without compromising recommendation quality. Moreover, the distribution of prototypes leads to more inclusive explanations by better aligning items with diverse prototypes. | Armin Moradi, Nicola Neophytou, Florian Carichon, Golnoosh Farnadi |  |
|  |  [Is Relevance Propagated from Retriever to Generator in RAG?](https://doi.org/10.1007/978-3-031-88708-6_3) |  | 0 | Retrieval Augmented Generation (RAG) is a framework for incorporating external knowledge, usually in the form of a set of documents retrieved from a collection, as a part of a prompt to a large language model (LLM) to potentially improve the performance of a downstream task, such as question answering. Different from a standard retrieval task's objective of maximising the relevance of a set of top-ranked documents, a RAG system's objective is rather to maximise their total utility, where the utility of a document indicates whether including it as a part of the additional contextual information in an LLM prompt improves a downstream task. Existing studies investigate the role of the relevance of a RAG context for knowledge-intensive language tasks (KILT), where relevance essentially takes the form of answer containment. In contrast, in our work, relevance corresponds to that of topical overlap between a query and a document for an information seeking task. Specifically, we make use of an IR test collection to empirically investigate whether a RAG context comprised of topically relevant documents leads to improved downstream performance. Our experiments lead to the following findings: (a) there is a small positive correlation between relevance and utility; (b) this correlation decreases with increasing context sizes (higher values of k in k-shot); and (c) a more effective retrieval model generally leads to better downstream RAG performance. | Fangzheng Tian, Debasis Ganguly, Craig Macdonald |  |
|  |  [Improving the Reusability of Conversational Search Test Collections](https://doi.org/10.1007/978-3-031-88708-6_13) |  | 0 | Incomplete relevance judgments limit the reusability of test collections. When new systems are compared to previous systems that contributed to the pool, they often face a disadvantage. This is due to pockets of unjudged documents (called holes) in the test collection that the new systems return. The very nature of Conversational Search (CS) means that these holes are potentially larger and more problematic when evaluating systems. In this paper, we aim to extend CS test collections by employing Large Language Models (LLMs) to fill holes by leveraging existing judgments. We explore this problem using TREC iKAT 23 and TREC CAsT 22 collections, where information needs are highly dynamic and the responses are much more varied, leaving bigger holes to fill. Our experiments reveal that CS collections show a trend towards less reusability in deeper turns. Also, fine-tuning the Llama 3.1 model leads to high agreement with human assessors, while few-shot prompting the ChatGPT results in low agreement with humans. Consequently, filling the holes of a new system using ChatGPT leads to a higher change in the location of the new system. While regenerating the assessment pool with few-shot prompting the ChatGPT model and using it for evaluation achieves a high rank correlation with human-assessed pools. We show that filling the holes using few-shot training the Llama 3.1 model enables a fairer comparison between the new system and the systems contributed to the pool. Our hole-filling model based on few-shot training of the Llama 3.1 model can improve the reusability of test collections. | Zahra Abbasiantaeb, Chuan Meng, Leif Azzopardi, Mohammad Aliannejadi |  |
|  |  [Evaluating Auto-complete Ranking for Diversity and Relevance](https://doi.org/10.1007/978-3-031-88708-6_21) |  | 0 |  | Sonali Singh, Sachin Farfade, Prakash Mandayam Comar |  |
|  |  [Zero-Shot and Efficient Clarification Need Prediction in Conversational Search](https://doi.org/10.1007/978-3-031-88708-6_25) |  | 0 | Clarification need prediction (CNP) is a key task in conversational search, aiming to predict whether to ask a clarifying question or give an answer to the current user query. However, current research on CNP suffers from the issues of limited CNP training data and low efficiency. In this paper, we propose a zero-shot and efficient CNP framework (Zef-CNP), in which we first prompt large language models (LLMs) in a zero-shot manner to generate two sets of synthetic queries: ambiguous and specific (unambiguous) queries. We then use the generated queries to train efficient CNP models. Zef-CNP eliminates the need for human-annotated clarification-need labels during training and avoids the use of LLMs with high query latency at query time. To further improve the generation quality of synthetic queries, we devise a topic-, information-need-, and query-aware chain-of-thought (CoT) prompting strategy (TIQ-CoT). Moreover, we enhance TIQ-CoT with counterfactual query generation (CoQu), which guides LLMs first to generate a specific/ambiguous query and then sequentially generate its corresponding ambiguous/specific query. Experimental results show that Zef-CNP achieves superior CNP effectiveness and efficiency compared with zero- and few-shot LLM-based CNP predictors. | Lili Lu, Chuan Meng, Federico Ravenda, Mohammad Aliannejadi, Fabio Crestani |  |
|  |  [FAIR-QR: Enhancing Fairness-Aware Information Retrieval Through Query Refinement](https://doi.org/10.1007/978-3-031-88717-8_1) |  | 0 | Information retrieval systems such as open web search and recommendation systems are ubiquitous and significantly impact how people receive and consume online information. Previous research has shown the importance of fairness in information retrieval systems to combat the issue of echo chambers and mitigate the rich-get-richer effect. Therefore, various fairness-aware information retrieval methods have been proposed. Score-based fairness-aware information retrieval algorithms, focusing on statistical parity, are interpretable but could be mathematically infeasible and lack generalizability. In contrast, learning-to-rank-based fairness-aware information retrieval algorithms using fairness-aware loss functions demonstrate strong performance but lack interpretability. In this study, we proposed a novel and interpretable framework that recursively refines query keywords to retrieve documents from underrepresented groups and achieve group fairness. Retrieved documents using refined queries will be re-ranked to ensure relevance. Our method not only shows promising retrieval results regarding relevance and fairness but also preserves interpretability by showing refined keywords used at each iteration. | Fumian Chen, Hui Fang |  |
|  |  [How Child-Friendly is Web Search? An Evaluation of Relevance vs. Harm](https://doi.org/10.1007/978-3-031-88717-8_16) |  | 0 |  | Maik Fröbe, Sophie Charlotte Bartholly, Matthias Hagen |  |
|  |  [Poison-RAG: Adversarial Data Poisoning Attacks on Retrieval-Augmented Generation in Recommender Systems](https://doi.org/10.1007/978-3-031-88717-8_18) |  | 0 | This study presents Poison-RAG, a framework for adversarial data poisoning attacks targeting retrieval-augmented generation (RAG)-based recommender systems. Poison-RAG manipulates item metadata, such as tags and descriptions, to influence recommendation outcomes. Using item metadata generated through a large language model (LLM) and embeddings derived via the OpenAI API, we explore the impact of adversarial poisoning attacks on provider-side, where attacks are designed to promote long-tail items and demote popular ones. Two attack strategies are proposed: local modifications, which personalize tags for each item using BERT embeddings, and global modifications, applying uniform tags across the dataset. Experiments conducted on the MovieLens dataset in a black-box setting reveal that local strategies improve manipulation effectiveness by up to 50%, while global strategies risk boosting already popular items. Results indicate that popular items are more susceptible to attacks, whereas long-tail items are harder to manipulate. Approximately 70% of items lack tags, presenting a cold-start challenge; data augmentation and synthesis are proposed as potential defense mechanisms to enhance RAG-based systems' resilience. The findings emphasize the need for robust metadata management to safeguard recommendation frameworks. Code and data are available at https://github.com/atenanaz/Poison-RAG. | Fatemeh Nazary, Yashar Deldjoo, Tommaso Di Noia |  |
|  |  [How to Diversify any Personalized Recommender?](https://doi.org/10.1007/978-3-031-88717-8_23) |  | 0 | In this paper, we introduce a novel approach to improve the diversity of Top-N recommendations while maintaining accuracy. Our approach employs a user-centric pre-processing strategy aimed at exposing users to a wide array of content categories and topics. We personalize this strategy by selectively adding and removing a percentage of interactions from user profiles. This personalization ensures we remain closely aligned with user preferences while gradually introducing distribution shifts. Our pre-processing technique offers flexibility and can seamlessly integrate into any recommender architecture. We run extensive experiments on two publicly available data sets for news and book recommendations to evaluate our approach. We test various standard and neural network-based recommender system algorithms. Our results show that our approach generates diverse recommendations, ensuring users are exposed to a wider range of items. Furthermore, using pre-processed data for training leads to recommender systems achieving performance levels comparable to, and in some cases, better than those trained on original, unmodified data. Additionally, our approach promotes provider fairness by facilitating exposure to minority categories. Our GitHub code is available at: https://github.com/SlokomManel/How-to-Diversify-any-Personalized-Recommender- | Manel Slokom, Savvina Daniil, Laura Hollink |  |
|  |  [Improving Minimax Group Fairness in Sequential Recommendation](https://doi.org/10.1007/978-3-031-88717-8_26) |  | 0 | Training sequential recommenders such as SASRec with uniform sample weights achieves good overall performance but can fall short on specific user groups. One such example is popularity bias, where mainstream users receive better recommendations than niche content viewers. To improve recommendation quality across diverse user groups, we explore three Distributionally Robust Optimization(DRO) methods: Group DRO, Streaming DRO, and Conditional Value at Risk (CVaR) DRO. While Group and Streaming DRO rely on group annotations and struggle with users belonging to multiple groups, CVaR does not require such annotations and can naturally handle overlapping groups. In experiments on two real-world datasets, we show that the DRO methods outperform standard training, with CVaR delivering the best results. Additionally, we find that Group and Streaming DRO are sensitive to the choice of group used for loss computation. Our contributions include (i) a novel application of CVaR to recommenders, (ii) showing that the DRO methods improve group metrics as well as overall performance, and (iii) demonstrating CVaR's effectiveness in the practical scenario of intersecting user groups. | Krishna Acharya, David Wardrope, Timos Korres, Aleksandr V. Petrov, Anders Uhrenholt |  |
|  |  [Call for Research on the Impact of Information Retrieval on Social Norms](https://doi.org/10.1007/978-3-031-88717-8_27) |  | 0 |  | Tim Gollub, Pierre Achkar, Martin Potthast, Benno Stein |  |
|  |  [kANNolo: Sweet and Smooth Approximate k-Nearest Neighbors Search](https://doi.org/10.1007/978-3-031-88717-8_29) |  | 0 | Approximate Nearest Neighbors (ANN) search is a crucial task in several applications like recommender systems and information retrieval. Current state-of-the-art ANN libraries, although being performance-oriented, often lack modularity and ease of use. This translates into them not being fully suitable for easy prototyping and testing of research ideas, an important feature to enable. We address these limitations by introducing kANNolo, a novel research-oriented ANN library written in Rust and explicitly designed to combine usability with performance effectively. kANNolo is the first ANN library that supports dense and sparse vector representations made available on top of different similarity measures, e.g., euclidean distance and inner product. Moreover, it also supports vector quantization techniques, e.g., Product Quantization, on top of the indexing strategies implemented. These functionalities are managed through Rust traits, allowing shared behaviors to be handled abstractly. This abstraction ensures flexibility and facilitates an easy integration of new components. In this work, we detail the architecture of kANNolo and demonstrate that its flexibility does not compromise performance. The experimental analysis shows that kANNolo achieves state-of-the-art performance in terms of speed-accuracy trade-off while allowing fast and easy prototyping, thus making kANNolo a valuable tool for advancing ANN research. Source code available on GitHub: https://github.com/TusKANNy/kannolo. | Leonardo Delfino, Domenico Erriquez, Silvio Martinico, Franco Maria Nardini, Cosimo Rulli, Rossano Venturini |  |
|  |  [CountNet: Utilising Repetition Counts in Sequential Recommendation](https://doi.org/10.1007/978-3-031-88714-7_4) |  | 0 |  | Aleksandr V. Petrov, Efi Karra Taniskidou, Sean Murphy |  |
|  |  [Ranking Generated Answers - On the Agreement of Retrieval Models with Humans on Consumer Health Questions](https://doi.org/10.1007/978-3-031-88714-7_10) |  | 0 | Evaluating the output of generative large language models (LLMs) is challenging and difficult to scale. Most evaluations of LLMs focus on tasks such as single-choice question-answering or text classification. These tasks are not suitable for assessing open-ended question-answering capabilities, which are critical in domains where expertise is required, such as health, and where misleading or incorrect answers can have a significant impact on a user's health. Using human experts to evaluate the quality of LLM answers is generally considered the gold standard, but expert annotation is costly and slow. We present a method for evaluating LLM answers that uses ranking signals as a substitute for explicit relevance judgements. Our scoring method correlates with the preferences of human experts. We validate it by investigating the well-known fact that the quality of generated answers improves with the size of the model as well as with more sophisticated prompting strategies. | Sebastian Heineking, Jonas Probst, Daniel Steinbach, Martin Potthast, Harrisen Scells |  |
|  |  [Gradual Negative Matching for LLM Unlearning](https://doi.org/10.1007/978-3-031-88714-7_16) |  | 0 |  | Hrishikesh Kulkarni, Nazli Goharian, Ophir Frieder |  |
|  |  [Efficient and Effective Conversational Search with Tail Entity Selection](https://doi.org/10.1007/978-3-031-88714-7_26) |  | 0 |  | Hai Dang Tran, Andrew Yates, Gerhard Weikum |  |
|  |  [Investigating the Scalability of Approximate Sparse Retrieval Algorithms to Massive Datasets](https://doi.org/10.1007/978-3-031-88714-7_43) |  | 0 | Learned sparse text embeddings have gained popularity due to their effectiveness in top-k retrieval and inherent interpretability. Their distributional idiosyncrasies, however, have long hindered their use in real-world retrieval systems. That changed with the recent development of approximate algorithms that leverage the distributional properties of sparse embeddings to speed up retrieval. Nonetheless, in much of the existing literature, evaluation has been limited to datasets with only a few million documents such as MSMARCO. It remains unclear how these systems behave on much larger datasets and what challenges lurk in larger scales. To bridge that gap, we investigate the behavior of state-of-the-art retrieval algorithms on massive datasets. We compare and contrast the recently-proposed Seismic and graph-based solutions adapted from dense retrieval. We extensively evaluate Splade embeddings of 138M passages from MsMarco-v2 and report indexing time and other efficiency and effectiveness metrics. | Sebastian Bruch, Franco Maria Nardini, Cosimo Rulli, Rossano Venturini, Leonardo Venuta |  |
|  |  [FinPersona: An LLM-Driven Conversational Agent for Personalized Financial Advising](https://doi.org/10.1007/978-3-031-88720-8_3) |  | 0 |  | Takehiro Takayanagi, Masahiro Suzuki, Kiyoshi Izumi, Javier SanzCruzado, Richard McCreadie, Iadh Ounis |  |
|  |  [MedLink: Retrieval and Ranking of Case Reports to Assist Clinical Decision Making](https://doi.org/10.1007/978-3-031-88720-8_13) |  | 0 |  | Luís Filipe Cunha, Nuno Guimarães, Alexandra Mendes, Ricardo Campos, Alípio Jorge |  |
|  |  [Jina Embeddings V3: Multilingual Text Encoder with Low-Rank Adaptations](https://doi.org/10.1007/978-3-031-88720-8_21) |  | 0 |  | Saba Sturua, Isabelle Mohr, Mohammad Kalim Akram, Michael Günther, Bo Wang, Markus Krimmel, Feng Wang, Georgios Mastrapas, Andreas Koukounas, Nan Wang, Han Xiao |  |
|  |  [Personalizing Enterprise Search with LLM Populated Attributes in Graph Models](https://doi.org/10.1007/978-3-031-88720-8_24) |  | 0 |  | Christopher Liu, Varsha Embar |  |
|  |  [Towards Query Obfuscation Strategies for Information Retrieval](https://doi.org/10.1007/978-3-031-88720-8_31) |  | 0 |  | Francesco Luigi De Faveri |  |
|  |  [Enhancing Reproducibility and Replicability in Information Retrieval: A Path Towards Scientific Integrity and Effective Research](https://doi.org/10.1007/978-3-031-88720-8_42) |  | 0 |  | Antonio Ferrara, Claudio Pomo, Nicola Tonellotto |  |
|  |  [ImageCLEF 2025: Multimedia Retrieval in Medical, Social Media and Content Recommendation Applications](https://doi.org/10.1007/978-3-031-88720-8_60) |  | 0 |  | Bogdan Ionescu, Henning Müller, DanCristian Stanciu, Ahmad IdrissiYaghir, Ahmedkhan Radzhabov, Alba García Seco de Herrera, Alexandra Andrei, Andrea M. Storås, Asma Ben Abacha, Benjamin Bracke, Benjamin Lecouteux, Benno Stein, Cécile Macaire, Christoph M. Friedrich, Cynthia Sabrina Schmidt, Diandra Fabre, Didier Schwab, Dimitar Dimitrov, Emmanuelle EsperançaRodier, Mihai Gabriel Constantin, Helmut Becker, Hendrik Damm, Henning Schäfer, Ivan Rodkin, Ivan Koychev, Johannes Kiesel, Johannes Rückert, Josep Malvehy, LiviuDaniel Stefan, Louise Bloch, Martin Potthast, Maximilian Heinrich, Michael A. Riegler, Mihai Dogariu, Noel Codella, Pål Halvorsen, Preslav Nakov, Raphael Brüngel, Roberto A. Novoa, Rocktim Jyoti Das, Steven Alexander Hicks, Sushant Gautam, Tabea Margareta Grace Pakull, Vajira Thambawita, Vassili Kovalev, Wenwai Yim, Zhuohan Xie |  |
|  |  [Evaluating LLM Abilities to Understand Tabular Electronic Health Records: A Comprehensive Study of Patient Data Extraction and Retrieval](https://doi.org/10.1007/978-3-031-88711-6_10) |  | 0 | Electronic Health Record (EHR) tables pose unique challenges among which is the presence of hidden contextual dependencies between medical features with a high level of data dimensionality and sparsity. This study presents the first investigation into the abilities of LLMs to comprehend EHRs for patient data extraction and retrieval. We conduct extensive experiments using the MIMICSQL dataset to explore the impact of the prompt structure, instruction, context, and demonstration, of two backbone LLMs, Llama2 and Meditron, based on task performance. Through quantitative and qualitative analyses, our findings show that optimal feature selection and serialization methods can enhance task performance by up to 26.79 learning setups with relevant example selection improve data extraction performance by 5.95 we believe would help the design of LLM-based models to support health search. | Jesús LovónMelgarejo, Martin Mouysset, Jo Oleiwan, José G. Moreno, Christine DamaseMichel, Lynda Tamine |  |
|  |  [Maybe You Are Looking for CroQS [inline-graphic not available: see fulltext] Cross-Modal Query Suggestion for Text-to-Image Retrieval](https://doi.org/10.1007/978-3-031-88711-6_9) |  | 0 |  | Giacomo Pacini, Fabio Carrara, Nicola Messina, Nicola Tonellotto, Giuseppe Amato, Fabrizio Falchi |  |
|  |  [Retrieve, Annotate, Evaluate, Repeat: Leveraging Multimodal LLMs for Large-Scale Product Retrieval Evaluation](https://doi.org/10.1007/978-3-031-88708-6_10) |  | 0 | Evaluating production-level retrieval systems at scale is a crucial yet challenging task due to the limited availability of a large pool of well-trained human annotators. Large Language Models (LLMs) have the potential to address this scaling issue and offer a viable alternative to humans for the bulk of annotation tasks. In this paper, we propose a framework for assessing the product search engines in a large-scale e-commerce setting, leveraging Multimodal LLMs for (i) generating tailored annotation guidelines for individual queries, and (ii) conducting the subsequent annotation task. Our method, validated through deployment on a large e-commerce platform, demonstrates comparable quality to human annotations, significantly reduces time and cost, facilitates rapid problem discovery, and provides an effective solution for production-level quality control at scale. | Kasra Hosseini, Thomas Kober, Josip Krapac, Roland Vollgraf, Weiwei Cheng, Ana PeleteiroRamallo |  |
|  |  [Corpus Subsampling: Estimating the Effectiveness of Neural Retrieval Models on Large Corpora](https://doi.org/10.1007/978-3-031-88708-6_29) |  | 0 |  | Maik Fröbe, Andrew Parry, Harrisen Scells, Shuai Wang, Shengyao Zhuang, Guido Zuccon, Martin Potthast, Matthias Hagen |  |
|  |  [LiT and Lean: Distilling Listwise Rerankers Into Encoder-Decoder Models](https://doi.org/10.1007/978-3-031-88714-7_13) |  | 0 |  | Manveer Singh Tamber, Ronak Pradeep, Jimmy Lin |  |
|  |  [MURR: Model Updating with Regularized Replay for Searching a Document Stream](https://doi.org/10.1007/978-3-031-88708-6_6) |  | 0 | The Internet produces a continuous stream of new documents and user-generated queries. These naturally change over time based on events in the world and the evolution of language. Neural retrieval models that were trained once on a fixed set of query-document pairs will quickly start misrepresenting newly-created content and queries, leading to less effective retrieval. Traditional statistical sparse retrieval can update collection statistics to reflect these changes in the use of language in documents and queries. In contrast, continued fine-tuning of the language model underlying neural retrieval approaches such as DPR and ColBERT creates incompatibility with previously-encoded documents. Re-encoding and re-indexing all previously-processed documents can be costly. In this work, we explore updating a neural dual encoder retrieval model without reprocessing past documents in the stream. We propose MURR, a model updating strategy with regularized replay, to ensure the model can still faithfully search existing documents without reprocessing, while continuing to update the model for the latest topics. In our simulated streaming environments, we show that fine-tuning models using MURR leads to more effective and more consistent retrieval results than other strategies as the stream of documents and queries progresses. | Eugene Yang, Nicola Tonellotto, Dawn J. Lawrie, Sean MacAvaney, James Mayfield, Douglas W. Oard, Scott Miller |  |
|  |  [Repeat-Bias-Aware Optimization of Beyond-Accuracy Metrics for Next Basket Recommendation](https://doi.org/10.1007/978-3-031-88708-6_14) |  | 0 | In next basket recommendation (NBR) a set of items is recommended to users based on their historical basket sequences. In many domains, the recommended baskets consist of both repeat items and explore items. Some state-of-the-art NBR methods are heavily biased to recommend repeat items so as to maximize utility. The evaluation and optimization of beyond-accuracy objectives for NBR, such as item fairness and diversity, has attracted increasing attention. How can such beyond-accuracy objectives be pursued in the presence of heavy repeat bias? We find that only optimizing diversity or item fairness without considering repeat bias may cause NBR algorithms to recommend more repeat items. To solve this problem, we propose a model-agnostic repeat-bias-aware optimization algorithm to post-process the recommended results obtained from NBR methods with the objective of mitigating repeat bias when optimizing diversity or item fairness. We consider multiple variations of our optimization algorithm to cater to multiple NBR methods. Experiments on three real-world grocery shopping datasets show that the proposed algorithms can effectively improve diversity and item fairness, and mitigate repeat bias at acceptable Recall loss. | Yuanna Liu, Ming Li, Mohammad Aliannejadi, Maarten de Rijke |  |
|  |  [Lost but Not Only in the Middle - Positional Bias in Retrieval Augmented Generation](https://doi.org/10.1007/978-3-031-88708-6_16) |  | 0 |  | Jan Hutter, David Rau, Maarten Marx, Jaap Kamps |  |
|  |  [A Multi-modal Recipe for Improved Multi-domain Recommendation](https://doi.org/10.1007/978-3-031-88708-6_27) |  | 0 |  | Zixuan Yi, Iadh Ounis |  |
|  |  [Improving Novelty and Diversity of Nearest-Neighbors Recommendation by Exploiting Dissimilarities](https://doi.org/10.1007/978-3-031-88717-8_14) |  | 0 |  | Pablo Sánchez, Javier SanzCruzado, Alejandro Bellogín |  |
|  |  [Improving Low-Resource Retrieval Effectiveness Using Zero-Shot Linguistic Similarity Transfer](https://doi.org/10.1007/978-3-031-88717-8_22) |  | 0 | Globalisation and colonisation have led the vast majority of the world to use only a fraction of languages, such as English and French, to communicate, excluding many others. This has severely affected the survivability of many now-deemed vulnerable or endangered languages, such as Occitan and Sicilian. These languages often share some characteristics, such as elements of their grammar and lexicon, with other high-resource languages, e.g. French or Italian. They can be clustered into groups of language varieties with various degrees of mutual intelligibility. Current search systems are not usually trained on many of these low-resource varieties, leading search users to express their needs in a high-resource language instead. This problem is further complicated when most information content is expressed in a high-resource language, inhibiting even more retrieval in low-resource languages. We show that current search systems are not robust across language varieties, severely affecting retrieval effectiveness. Therefore, it would be desirable for these systems to leverage the capabilities of neural models to bridge the differences between these varieties. This can allow users to express their needs in their low-resource variety and retrieve the most relevant documents in a high-resource one. To address this, we propose fine-tuning neural rankers on pairs of language varieties, thereby exposing them to their linguistic similarities. We find that this approach improves the performance of the varieties upon which the models were directly trained, thereby regularising these models to generalise and perform better even on unseen language variety pairs. We also explore whether this approach can transfer across language families and observe mixed results that open doors for future research. | Andreas Chari, Sean MacAvaney, Iadh Ounis |  |
|  |  [FlashCheck: Exploration of Efficient Evidence Retrieval for Fast Fact-Checking](https://doi.org/10.1007/978-3-031-88717-8_28) |  | 0 | The advances in digital tools have led to the rampant spread of misinformation. While fact-checking aims to combat this, manual fact-checking is cumbersome and not scalable. It is essential for automated fact-checking to be efficient for aiding in combating misinformation in real-time and at the source. Fact-checking pipelines primarily comprise a knowledge retrieval component which extracts relevant knowledge to fact-check a claim from large knowledge sources like Wikipedia and a verification component. The existing works primarily focus on the fact-verification part rather than evidence retrieval from large data collections, which often face scalability issues for practical applications such as live fact-checking. In this study, we address this gap by exploring various methods for indexing a succinct set of factual statements from large collections like Wikipedia to enhance the retrieval phase of the fact-checking pipeline. We also explore the impact of vector quantization to further improve the efficiency of pipelines that employ dense retrieval approaches for first-stage retrieval. We study the efficiency and effectiveness of the approaches on fact-checking datasets such as HoVer and WiCE, leveraging Wikipedia as the knowledge source. We also evaluate the real-world utility of the efficient retrieval approaches by fact-checking 2024 presidential debate and also open source the collection of claims with corresponding labels identified in the debate. Through a combination of indexed facts together with Dense retrieval and Index compression, we achieve up to a 10.0x speedup on CPUs and more than a 20.0x speedup on GPUs compared to the classical fact-checking pipelines over large collections. | Kevin Nanhekhan, Venktesh V, Erik Martin, Henrik Vatndal, Vinay Setty, Avishek Anand |  |
|  |  [Sim4Rec: Flexible and Extensible Simulator for Recommender Systems for Large-Scale Data](https://doi.org/10.1007/978-3-031-88717-8_33) |  | 0 |  | Anna Volodkevich, Veronika Ivanova, Alexey Vasilev, Dmitry Bugaychenko, Maxim Savchenko |  |
|  |  [The Impact of Mainstream-Driven Algorithms on Recommendations for Children](https://doi.org/10.1007/978-3-031-88714-7_5) |  | 0 |  | Robin Ungruh, Alejandro Bellogín, Maria Soledad Pera |  |
|  |  [Rank-DistiLLM: Closing the Effectiveness Gap Between Cross-Encoders and LLMs for Passage Re-ranking](https://doi.org/10.1007/978-3-031-88714-7_31) |  | 0 | Cross-encoders distilled from large language models (LLMs) are often more effective re-rankers than cross-encoders fine-tuned on manually labeled data. However, distilled models do not match the effectiveness of their teacher LLMs. We hypothesize that this effectiveness gap is due to the fact that previous work has not applied the best-suited methods for fine-tuning cross-encoders on manually labeled data (e.g., hard-negative sampling, deep sampling, and listwise loss functions). To close this gap, we create a new dataset, Rank-DistiLLM. Cross-encoders trained on Rank-DistiLLM achieve the effectiveness of LLMs while being up to 173 times faster and 24 times more memory efficient. Our code and data is available at https://github.com/webis-de/ECIR-25. | Ferdinand Schlatt, Maik Fröbe, Harrisen Scells, Shengyao Zhuang, Bevan Koopman, Guido Zuccon, Benno Stein, Martin Potthast, Matthias Hagen | University of Queensland; University of Kassel ScadDS.AI; Leipzig University; Friedrich-Schiller-Universität Jena; CSIRO; Bauhaus-Universität Weimar |
|  |  [Exploring the Effectiveness of Multi-stage Fine-Tuning for Cross-Encoder Re-rankers](https://doi.org/10.1007/978-3-031-88714-7_45) |  | 0 | State-of-the-art cross-encoders can be fine-tuned to be highly effective in passage re-ranking. The typical fine-tuning process of cross-encoders as re-rankers requires large amounts of manually labelled data, a contrastive learning objective, and a set of heuristically sampled negatives. An alternative recent approach for fine-tuning instead involves teaching the model to mimic the rankings of a highly effective large language model using a distillation objective. These fine-tuning strategies can be applied either individually, or in sequence. In this work, we systematically investigate the effectiveness of point-wise cross-encoders when fine-tuned independently in a single stage, or sequentially in two stages. Our experiments show that the effectiveness of point-wise cross-encoders fine-tuned using contrastive learning is indeed on par with that of models fine-tuned with multi-stage approaches. Code is available for reproduction at https://github.com/fpezzuti/multistage-finetuning. | Francesca Pezzuti, Sean MacAvaney, Nicola Tonellotto |  |
|  |  [Semantic Search and Filtering with AI Agents](https://doi.org/10.1007/978-3-031-88720-8_4) |  | 0 |  | Martin Bulín, Jan Svec, Filip Polák, Lubos Smídl |  |
|  |  [Adapting LLMs for Domain-Specific Retrieval: A Case Study in Nuclear Safety](https://doi.org/10.1007/978-3-031-88720-8_20) |  | 0 |  | Federico Borazio, Danilo Croce, Roberto Basili |  |
|  |  [On the Longitudinal Impact of Exposure Bias in Recommender Systems](https://doi.org/10.1007/978-3-031-88720-8_29) |  | 0 |  | Andrea Pisani |  |
|  |  [Mitigating Gender Bias in Information Retrieval Systems](https://doi.org/10.1007/978-3-031-88720-8_36) |  | 0 | Recent studies have shown that information retrieval systems may exhibit stereotypical gender biases in outcomes which may lead to discrimination against minority groups, such as different genders, and impact users' decision making and judgements. In this tutorial, we inform the audience of studies that have systematically reported the presence of stereotypical gender biases in Information Retrieval (IR) systems and different pre-trained Natural Language Processing (NLP) models. We further classify existing work on gender biases in IR systems and NLP models as being related to (1) relevance judgement datasets, (2) structure of retrieval methods, (3) representations learnt for queries and documents, (4) and pre-trained embedding models. Based on the aforementioned categories, we present a host of methods from the literature that can be leveraged to measure, control, or mitigate the existence of stereotypical biases within IR systems and different NLP models that are used for down-stream tasks. Besides, we introduce available datasets and collections that are widely used for studying the existence of gender biases in IR systems and NLP models, the evaluation metrics that can be used for measuring the level of bias and utility of the models, and de-biasing methods that can be leveraged to mitigate gender biases within those models. | Shirin Seyedsalehi | Univ Waterloo, Waterloo, ON, Canada; Toronto Metropolitan Univ, Toronto, ON, Canada |
|  |  [Advanced Methods for Visual Information Retrieval and Exploration in Large Multimedia Collections](https://doi.org/10.1007/978-3-031-88720-8_43) |  | 0 |  | Kai Uwe Barthel, Nico Hezel, Konstantin Schall |  |
|  |  [Efficient Session Retrieval Using Topical Index Shards](https://doi.org/10.1007/978-3-031-88711-6_3) |  | 0 |  | Gijs Hendriksen, Djoerd Hiemstra, Arjen P. de Vries |  |
|  |  [Set-Encoder: Permutation-Invariant Inter-passage Attention for Listwise Passage Re-ranking with Cross-Encoders](https://doi.org/10.1007/978-3-031-88711-6_1) |  | 0 | Existing cross-encoder models can be categorized as pointwise, pairwise, or listwise. Pairwise and listwise models allow passage interactions, which typically makes them more effective than pointwise models but less efficient and less robust to input passage order permutations. To enable efficient permutation-invariant passage interactions during re-ranking, we propose a new cross-encoder architecture with inter-passage attention: the Set-Encoder. In experiments on TREC Deep Learning and TIREx, the Set-Encoder is as effective as state-of-the-art listwise models while being more efficient and invariant to input passage order permutations. Compared to pointwise models, the Set-Encoder is particularly more effective when considering inter-passage information, such as novelty, and retains its advantageous properties compared to other listwise models. Our code is publicly available at https://github.com/webis-de/ECIR-25. | Ferdinand Schlatt, Maik Fröbe, Harrisen Scells, Shengyao Zhuang, Bevan Koopman, Guido Zuccon, Benno Stein, Martin Potthast, Matthias Hagen |  |
|  |  [Feature Attribution Explanations of Session-Based Recommendations](https://doi.org/10.1007/978-3-031-88711-6_4) |  | 0 |  | Simone Borg Bruun, Maria Maistro, Christina Lioma |  |
|  |  [Exploring the Relationship Between Listener Receptivity and Source of Music Recommendations](https://doi.org/10.1007/978-3-031-88711-6_7) |  | 0 |  | John Paul Vargheese, Marianne Wilson, Katherine Stephen, Rachel Salzano, David Brazier |  |
|  |  [An Investigation of Prompt Variations for Zero-Shot LLM-Based Rankers](https://doi.org/10.1007/978-3-031-88711-6_12) |  | 0 | We provide a systematic understanding of the impact of specific componentsand wordings used in prompts on the effectiveness of rankers based on zero-shotLarge Language Models (LLMs). Several zero-shot ranking methods based on LLMshave recently been proposed. Among many aspects, methods differ across (1) theranking algorithm they implement, e.g., pointwise vs. listwise, (2) thebackbone LLMs used, e.g., GPT3.5 vs. FLAN-T5, (3) the components and wordingused in prompts, e.g., the use or not of role-definition (role-playing) and theactual words used to express this. It is currently unclear whether performancedifferences are due to the underlying ranking algorithm, or because of spuriousfactors such as better choice of words used in prompts. This confusion risks toundermine future research. Through our large-scale experimentation andanalysis, we find that ranking algorithms do contribute to differences betweenmethods for zero-shot LLM ranking. However, so do the LLM backbones – but evenmore importantly, the choice of prompt components and wordings affect theranking. In fact, in our experiments, we find that, at times, these latterelements have more impact on the ranker's effectiveness than the actual rankingalgorithms, and that differences among ranking methods become more blurred whenprompt variations are considered. | Shuoqi Sun, Shengyao Zhuang, Shuai Wang, Guido Zuccon |  |
|  |  [Query Performance Prediction Using Dimension Importance Estimators](https://doi.org/10.1007/978-3-031-88711-6_13) |  | 0 |  | Guglielmo Faggioli, Nicola Ferro, Raffaele Perego, Nicola Tonellotto |  |
|  |  [Rank-Without-GPT: Building GPT-Independent Listwise Rerankers on Open-Source Large Language Models](https://doi.org/10.1007/978-3-031-88711-6_15) |  | 0 | Listwise rerankers based on large language models (LLM) are the zero-shot state-of-the-art. However, current works in this direction all depend on the GPT models, making it a single point of failure in scientific reproducibility. Moreover, it raises the concern that the current research findings only hold for GPT models but not LLM in general. In this work, we lift this pre-condition and build for the first time effective listwise rerankers without any form of dependency on GPT. Our passage retrieval experiments show that our best list se reranker surpasses the listwise rerankers based on GPT-3.5 by 13% and achieves 97% effectiveness of the ones built on GPT-4. Our results also show that the existing training datasets, which were expressly constructed for pointwise ranking, are insufficient for building such listwise rerankers. Instead, high-quality listwise ranking data is required and crucial, calling for further work on building human-annotated listwise data resources. | Crystina Zhang, Sebastian Hofstätter, Patrick Lewis, Raphael Tang, Jimmy Lin | University of Waterloo |
|  |  [Measuring Actual Privacy of Obfuscated Queries in Information Retrieval](https://doi.org/10.1007/978-3-031-88708-6_4) |  | 0 |  | Francesco Luigi De Faveri, Guglielmo Faggioli, Nicola Ferro |  |
|  |  [Ragnarök: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track](https://doi.org/10.1007/978-3-031-88708-6_9) |  | 0 | Did you try out the new Bing Search? Or maybe you fiddled around with Google AI Overviews? These might sound familiar because the modern-day search stack has recently evolved to include retrieval-augmented generation (RAG) systems. They allow searching and incorporating real-time data into large language models (LLMs) to provide a well-informed, attributed, concise summary in contrast to the traditional search paradigm that relies on displaying a ranked list of documents. Therefore, given these recent advancements, it is crucial to have an arena to build, test, visualize, and systematically evaluate RAG-based search systems. With this in mind, we propose the TREC 2024 RAG Track to foster innovation in evaluating RAG systems. In our work, we lay out the steps we've made towards making this track a reality – we describe the details of our reusable framework, Ragnarök, explain the curation of the new MS MARCO V2.1 collection choice, release the development topics for the track, and standardize the I/O definitions which assist the end user. Next, using Ragnarök, we identify and provide key industrial baselines such as OpenAI's GPT-4o or Cohere's Command R+. Further, we introduce a web-based user interface for an interactive arena allowing benchmarking pairwise RAG systems by crowdsourcing. We open-source our Ragnarök framework and baselines to achieve a unified standard for future RAG systems. | Ronak Pradeep, Nandan Thakur, Sahel Sharifymoghaddam, Eric Zhang, Ryan Nguyen, Daniel Campos, Nick Craswell, Jimmy Lin |  |
|  |  [Advancing Math Formula Search Using Diverse Structural and Symbolic Representations](https://doi.org/10.1007/978-3-031-88708-6_8) |  | 0 |  | Sumedh Vemuganti, Ayu Seiya, Nickvash Kani |  |
|  |  [Token Pruning Optimization for Efficient Multi-vector Dense Retrieval](https://doi.org/10.1007/978-3-031-88708-6_7) |  | 0 |  | Shanxiu He, Mutasem AlDarabsah, Suraj Nair, Jonathan May, Tarun Agarwal, Tao Yang, Choon Hui Teo |  |
|  |  [Higher Order Knowledge Graph Embeddings](https://doi.org/10.1007/978-3-031-88708-6_12) |  | 0 |  | Giuseppe Pirrò |  |
|  |  [Graph Representation of Tables+Text and Compact Subgraph Retrieval for QA Tasks](https://doi.org/10.1007/978-3-031-88708-6_11) |  | 0 |  | Vishwajeet Kumar, Jaydeep Sen, Bhawna Chelani, Soumen Chakrabarti |  |
|  |  [Context Example Selection for LLM Generated Relevance Assessments](https://doi.org/10.1007/978-3-031-88708-6_19) |  | 0 |  | Jack McKechnie, Graham McDonald, Craig Macdonald |  |
|  |  [LSTM-Based Selective Dense Text Retrieval Guided by Sparse Lexical Retrieval](https://doi.org/10.1007/978-3-031-88708-6_18) |  | 0 | This paper studies fast fusion of dense retrieval and sparse lexical retrieval, and proposes a cluster-based selective dense retrieval method called CluSD guided by sparse lexical retrieval. CluSD takes a lightweight cluster-based approach and exploits the overlap of sparse retrieval results and embedding clusters in a two-stage selection process with an LSTM model to quickly identify relevant clusters while incurring limited extra memory space overhead. CluSD triggers partial dense retrieval and performs cluster-based block disk I/O if needed. This paper evaluates CluSD and compares it with several baselines for searching in-memory and on-disk MS MARCO and BEIR datasets. | Yingrui Yang, Parker Carlson, Yifan Qiao, Wentai Xie, Shanxiu He, Tao Yang |  |
|  |  [Opt-in Transparent Fairness for Recommender Systems](https://doi.org/10.1007/978-3-031-88708-6_23) |  | 0 |  | Bjørnar Vassøy, Benjamin Kille, Helge Langseth |  |
|  |  [Towards Identity-Aware Cross-Modal Retrieval: A Dataset and a Baseline](https://doi.org/10.1007/978-3-031-88708-6_28) |  | 0 | Recent advancements in deep learning have significantly enhanced content-based retrieval methods, notably through models like CLIP that map images and texts into a shared embedding space. However, these methods often struggle with domain-specific entities and long-tail concepts absent from their training data, particularly in identifying specific individuals. In this paper, we explore the task of identity-aware cross-modal retrieval, which aims to retrieve images of persons in specific contexts based on natural language queries. This task is critical in various scenarios, such as for searching and browsing personalized video collections or large audio-visual archives maintained by national broadcasters. We introduce a novel dataset, COCO Person FaceSwap (COCO-PFS), derived from the widely used COCO dataset and enriched with deepfake-generated faces from VGGFace2. This dataset addresses the lack of large-scale datasets needed for training and evaluating models for this task. Our experiments assess the performance of different CLIP variations repurposed for this task, including our architecture, Identity-aware CLIP (Id-CLIP), which achieves competitive retrieval performance through targeted fine-tuning. Our contributions lay the groundwork for more robust cross-modal retrieval systems capable of recognizing long-tail identities and contextual nuances. Data and code are available at https://github.com/mesnico/IdCLIP. | Nicola Messina, Lucia Vadicamo, Leo Maltese, Claudio Gennaro |  |
|  |  [CLASP: Contrastive Language-Speech Pretraining for Multilingual Multimodal Information Retrieval](https://doi.org/10.1007/978-3-031-88717-8_2) |  | 0 | This study introduces CLASP (Contrastive Language-Speech Pretraining), a multilingual, multimodal representation tailored for audio-text information retrieval. CLASP leverages the synergy between spoken content and textual data. During training, we utilize our newly introduced speech-text dataset, which encompasses 15 diverse categories ranging from fiction to religion. CLASP's audio component integrates audio spectrograms with a pre-trained self-supervised speech model, while its language encoding counterpart employs a sentence encoder pre-trained on over 100 languages. This unified lightweight model bridges the gap between various modalities and languages, enhancing its effectiveness in handling and retrieving multilingual and multimodal data. Our evaluations across multiple languages demonstrate that CLASP establishes new benchmarks in HITS@1, MRR, and meanR metrics, outperforming traditional ASR-based retrieval approaches in specific scenarios. | Mohammad Mahdi Abootorabi, Ehsaneddin Asgari |  |
|  |  [Fact vs. Fiction: Are the Reportedly "Magical" LLM-Based Recommenders Reproducible?](https://doi.org/10.1007/978-3-031-88717-8_7) |  | 0 |  | Shirin Tahmasebi, Narjes Nikzad, Amir Hossein Payberah, Meysam AsgariChenaghlu, Mihhail Matskin |  |
|  |  [Reproducing HotFlip for Corpus Poisoning Attacks in Dense Retrieval](https://doi.org/10.1007/978-3-031-88717-8_8) |  | 0 | HotFlip is a topical gradient-based word substitution method for attacking language models. Recently, this method has been further applied to attack retrieval systems by generating malicious passages that are injected into a corpus, i.e., corpus poisoning. However, HotFlip is known to be computationally inefficient, with the majority of time being spent on gradient accumulation for each query-passage pair during the adversarial token generation phase, making it impossible to generate an adequate number of adversarial passages in a reasonable amount of time. Moreover, the attack method itself assumes access to a set of user queries, a strong assumption that does not correspond to how real-world adversarial attacks are usually performed. In this paper, we first significantly boost the efficiency of HotFlip, reducing the adversarial generation process from 4 hours per document to only 15 minutes, using the same hardware. We further contribute experiments and analysis on two additional tasks: (1) transfer-based black-box attacks, and (2) query-agnostic attacks. Whenever possible, we provide comparisons between the original method and our improved version. Our experiments demonstrate that HotFlip can effectively attack a variety of dense retrievers, with an observed trend that its attack performance diminishes against more advanced and recent methods. Interestingly, we observe that while HotFlip performs poorly in a black-box setting, indicating limited capacity for generalization, in query-agnostic scenarios its performance is correlated to the volume of injected adversarial passages. | Yongkang Li, Panagiotis Eustratiadis, Evangelos Kanoulas |  |
|  |  [On the Reproducibility of Learned Sparse Retrieval Adaptations for Long Documents](https://doi.org/10.1007/978-3-031-88717-8_6) |  | 0 | Document retrieval is one of the most challenging tasks in Information Retrieval. It requires handling longer contexts, often resulting in higher query latency and increased computational overhead. Recently, Learned Sparse Retrieval (LSR) has emerged as a promising approach to address these challenges. Some have proposed adapting the LSR approach to longer documents by aggregating segmented document using different post-hoc methods, including n-grams and proximity scores, adjusting representations, and learning to ensemble all signals. In this study, we aim to reproduce and examine the mechanisms of adapting LSR for long documents. Our reproducibility experiments confirmed the importance of specific segments, with the first segment consistently dominating document retrieval performance. Furthermore, We re-evaluate recently proposed methods – ExactSDM and SoftSDM – across varying document lengths, from short (up to 2 segments) to longer (3+ segments). We also designed multiple analyses to probe the reproduced methods and shed light on the impact of global information on adapting LSR to longer contexts. The complete code and implementation for this project is available at: https://github.com/lionisakis/Reproducibilitiy-lsr-long. | Emmanouil Georgios Lionis, JiaHuei Ju |  |
|  |  [Are Representation Disentanglement and Interpretability Linked in Recommendation Models? - A Critical Review and Reproducibility Study](https://doi.org/10.1007/978-3-031-88717-8_4) |  | 0 | Unsupervised learning of disentangled representations has been closely tied to enhancing the representation intepretability of Recommender Systems (RSs). This has been achieved by making the representation of individual features more distinctly separated, so that it is easier to attribute the contribution of features to the model's predictions. However, such advantages in interpretability and feature attribution have mainly been explored qualitatively. Moreover, the effect of disentanglement on the model's recommendation performance has been largely overlooked. In this work, we reproduce the recommendation performance, representation disentanglement and representation interpretability of five well-known recommendation models on four RS datasets. We quantify disentanglement and investigate the link of disentanglement with recommendation effectiveness and representation interpretability. While several existing work in RSs have proposed disentangled representations as a gateway to improved effectiveness and interpretability, our findings show that disentanglement is not necessarily related to effectiveness but is closely related to representation interpretability. Our code and results are publicly available at https://github.com/edervishaj/disentanglement-interpretability-recsys. | Ervin Dervishaj, Tuukka Ruotsalo, Maria Maistro, Christina Lioma |  |
|  |  [Combining Query Performance Predictors: A Reproducibility Study](https://doi.org/10.1007/978-3-031-88717-8_9) |  | 0 | A large number of approaches to Query Performance Prediction (QPP) have been proposed over the last two decades. As early as 2009, Hauff et al. [28] explored whether different QPP methods may be combined to improve prediction quality. Since then, significant research has been done both on QPP approaches, as well as their evaluation. This study revisits Hauff et al.s work to assess the reproducibility of their findings in the light of new prediction methods, evaluation metrics, and datasets. We expand the scope of the earlier investigation by: (i) considering post-retrieval methods, including supervised neural techniques (only pre-retrieval techniques were studied in [28]); (ii) using sMARE for evaluation, in addition to the traditional correlation coefficients and RMSE; and (iii) experimenting with additional datasets (Clueweb09B and TREC DL). Our results largely support previous claims, but we also present several interesting findings. We interpret these findings by taking a more nuanced look at the correlation between QPP methods, examining whether they capture diverse information or rely on overlapping factors. | Sourav Saha, Suchana Datta, Dwaipayan Roy, Mandar Mitra, Derek Greene |  |
|  |  [Revisiting Language Models in Neural News Recommender Systems](https://doi.org/10.1007/978-3-031-88717-8_12) |  | 0 | Neural news recommender systems (RSs) have integrated language models (LMs) to encode news articles with rich textual information into representations, thereby improving the recommendation process. Most studies suggest that (i) news RSs achieve better performance with larger pre-trained language models (PLMs) than shallow language models (SLMs), and (ii) that large language models (LLMs) outperform PLMs. However, other studies indicate that PLMs sometimes lead to worse performance than SLMs. Thus, it remains unclear whether using larger LMs consistently improves the performance of news RSs. In this paper, we revisit, unify, and extend these comparisons of the effectiveness of LMs in news RSs using the real-world MIND dataset. We find that (i) larger LMs do not necessarily translate to better performance in news RSs, and (ii) they require stricter fine-tuning hyperparameter selection and greater computational resources to achieve optimal recommendation performance than smaller LMs. On the positive side, our experiments show that larger LMs lead to better recommendation performance for cold-start users: they alleviate dependency on extensive user interaction history and make recommendations more reliant on the news content. | Yuyue Zhao, Jin Huang, David Vos, Maarten de Rijke |  |
|  |  [Towards Reproducibility of Interactive Retrieval Experiments: Framework and Case Study](https://doi.org/10.1007/978-3-031-88717-8_11) |  | 0 |  | Jana Isabelle Friese, Norbert Fuhr |  |
|  |  [LambdaFair for Fair and Effective Ranking](https://doi.org/10.1007/978-3-031-88717-8_15) |  | 0 |  | Federico Marcuzzi, Claudio Lucchese, Salvatore Orlando |  |
|  |  [Fair Exposure Allocation Using Generative Query Expansion](https://doi.org/10.1007/978-3-031-88717-8_20) |  | 0 |  | Thomas Jänich, Graham McDonald, Iadh Ounis |  |
|  |  [Enabling Low-Resource Language Retrieval: Establishing Baselines for Urdu MS MARCO](https://doi.org/10.1007/978-3-031-88717-8_21) |  | 0 |  | Umer Butt, Stalin Veranasi, Günter Neumann |  |
|  |  [Unraveling the Impact of Visual Complexity on Search as Learning](https://doi.org/10.1007/978-3-031-88714-7_2) |  | 0 | Information search has become essential for learning and knowledge acquisition, offering broad access to information and learning resources. The visual complexity of web pages is known to influence search behavior, with previous work suggesting that searchers make evaluative judgments within the first second on a page. However, there is a significant gap in our understanding of how visual complexity impacts searches specifically conducted with a learning intent. This gap is particularly relevant for the development of optimized information retrieval (IR) systems that effectively support educational objectives. To address this research need, we model visual complexity and aesthetics via a diverse set of features, investigating their relationship with search behavior during learning-oriented web sessions. Our study utilizes a publicly available dataset from a lab study where participants learned about thunderstorm formation. Our findings reveal that while content relevance is the most significant predictor for knowledge gain, sessions with less visually complex pages are associated with higher learning success. This observation applies to features associated with the layout of web pages rather than to simpler features (e.g., number of images). The reported results shed light on the impact of visual complexity on learning-oriented searches, informing the design of more effective IR systems for educational contexts. To foster reproducibility, we release our source code (https://github.com/TIBHannover/sal_visual_complexity). | Wolfgang Gritz, Anett Hoppe, Ralph Ewerth |  |
|  |  [Enhancing Utility in Differentially Private Recommendation Data Release via Exponential Mechanism](https://doi.org/10.1007/978-3-031-88714-7_3) |  | 0 |  | Antonio Ferrara, Angela Di Fazio, Alberto Carlo Maria Mancino, Tommaso Di Noia, Eugenio Di Sciascio |  |
|  |  [Inducing Diversity in Differentiable Search Indexing](https://doi.org/10.1007/978-3-031-88714-7_7) |  | 0 | Differentiable Search Indexing (DSI) is a recent paradigm for information retrieval which uses a transformer-based neural network architecture as the document index to simplify the retrieval process. A differentiable index has many advantages enabling modifications, updates or extensions to the index. In this work, we explore balancing relevance and novel information content (diversity) for training DSI systems inspired by Maximal Marginal Relevance (MMR), and show the benefits of our approach over the naive DSI training. We present quantitative and qualitative evaluations of relevance and diversity measures obtained using our method on NQ320K and MSMARCO datasets in comparison to naive DSI. With our approach, it is possible to achieve diversity without any significant impact to relevance. Since we induce diversity while training DSI, the trained model has learned to diversify while being relevant. This obviates the need for a post-processing step to induce diversity in the recall set as typically performed using MMR. Our approach will be useful for Information Retrieval problems where both relevance and diversity are important such as in sub-topic retrieval. Our work can also be easily be extended to the incremental DSI settings which would enable fast updates to the index while retrieving a diverse recall set. | Abhijeet Phatak, Jayant Sachdev, Sean D. Rosario, Swati Kirti, Chittaranjan Tripathy |  |
|  |  [The Impact of Incidental Multilingual Text on Cross-Lingual Transfer in Monolingual Retrieval](https://doi.org/10.1007/978-3-031-88714-7_14) |  | 0 |  | Andrew Liu, Edward Xu, Crystina Zhang, Jimmy Lin |  |
|  |  [Approximate Bag-of-Words Top-k Corpus Graphs](https://doi.org/10.1007/978-3-031-88714-7_15) |  | 0 |  | Lachlan Dunn, Luke Gallagher, Joel Mackenzie |  |
|  |  [Iterative Self-training for Code Generation via Reinforced Re-ranking](https://doi.org/10.1007/978-3-031-88714-7_21) |  | 0 | Generating high-quality code that solves complex programming tasks is challenging, especially with current decoder-based models that produce highly stochastic outputs. In code generation, even minor errors can easily break the entire solution. Leveraging multiple sampled solutions can significantly improve the overall output quality. One effective way to enhance code generation is by pairing a code generation model with a reranker model, which selects the best solution from the generated samples. We propose a novel iterative self-training approach for self-training reranker models using Proximal Policy Optimization (PPO), aimed at improving both reranking accuracy and the overall code generation process. Unlike traditional PPO approaches, where the focus is on optimizing a generative model with a reward model, our approach emphasizes the development of a robust reward/reranking model. This model improves the quality of generated code through reranking and addresses problems and errors that the reward model might overlook during PPO alignment with the reranker. Our method iteratively refines the training dataset by re-evaluating outputs, identifying high-scoring negative examples, and incorporating them into the training loop, that boosting model performance. Our evaluation on the MultiPL-E dataset demonstrates that our 13.4B parameter model outperforms a 33B model in code generation quality while being three times faster. Moreover, it achieves performance comparable to GPT-4 and surpasses it in one programming language. | Nikita Sorokin, Ivan Sedykh, Valentin Malykh |  |
|  |  [Fact-Driven Health Information Retrieval: Integrating LLMs and Knowledge Graphs to Combat Misinformation](https://doi.org/10.1007/978-3-031-88714-7_17) |  | 0 |  | Gian Carlo Milanese, Georgios Peikos, Gabriella Pasi, Marco Viviani |  |
|  |  [Investigating the Performance of Dense Retrievers for Queries with Numerical Conditions](https://doi.org/10.1007/978-3-031-88714-7_19) |  | 0 |  | Haruki Fujimaki, Makoto P. Kato |  |
|  |  [Efficient Constant-Space Multi-vector Retrieval](https://doi.org/10.1007/978-3-031-88714-7_22) |  | 0 | Multi-vector retrieval methods, exemplified by the ColBERT architecture, have shown substantial promise for retrieval by providing strong trade-offs in terms of retrieval latency and effectiveness. However, they come at a high cost in terms of storage since a (potentially compressed) vector needs to be stored for every token in the input collection. To overcome this issue, we propose encoding documents to a fixed number of vectors, which are no longer necessarily tied to the input tokens. Beyond reducing the storage costs, our approach has the advantage that document representations become of a fixed size on disk, allowing for better OS paging management. Through experiments using the MSMARCO passage corpus and BEIR with the ColBERT-v2 architecture, a representative multi-vector ranking model architecture, we find that passages can be effectively encoded into a fixed number of vectors while retaining most of the original effectiveness. | Sean MacAvaney, Antonio Mallia, Nicola Tonellotto |  |
|  |  [Large Language Model Can Be a Foundation for Hidden Rationale-Based Retrieval](https://doi.org/10.1007/978-3-031-88714-7_27) |  | 0 | Despite the recent advancement in Retrieval-Augmented Generation (RAG) systems, most retrieval methodologies are often developed for factual retrieval, which assumes query and positive documents are semantically similar. In this paper, we instead propose and study a more challenging type of retrieval task, called hidden rationale retrieval, in which query and document are not similar but can be inferred by reasoning chains, logic relationships, or empirical experiences. To address such problems, an instruction-tuned Large language model (LLM) with a cross-encoder architecture could be a reasonable choice. To further strengthen pioneering LLM-based retrievers, we design a special instruction that transforms the retrieval task into a generative task by prompting LLM to answer a binary-choice question. The model can be fine-tuned with direct preference optimization (DPO). The framework is also optimized for computational efficiency with no performance degradation. We name this retrieval framework by RaHoRe and verify its zero-shot and fine-tuned performance superiority on Emotional Support Conversation (ESC), compared with previous retrieval works. Our study suggests the potential to employ LLM as a foundation for a wider scope of retrieval tasks. Our codes, models, and datasets are available on https://github.com/flyfree5/LaHoRe. | Luo Ji, Feixiang Guo, Teng Chen, Qingqing Gu, Xiaoyu Wang, Ningyuan Xi, Yihong Wang, Peng Yu, Yue Zhao, Hongyang Lei, Zhonglin Jiang, Yong Chen |  |
|  |  [SAFERec: Self-Attention and Frequency Enriched Model for Next Basket Recommendation](https://doi.org/10.1007/978-3-031-88714-7_28) |  | 0 | Transformer-based approaches such as BERT4Rec and SASRec demonstrate strong performance in Next Item Recommendation (NIR) tasks. However, applying these architectures to Next-Basket Recommendation (NBR) tasks, which often involve highly repetitive interactions, is challenging due to the vast number of possible item combinations in a basket. Moreover, frequency-based methods such as TIFU-KNN and UP-CF still demonstrate strong performance in NBR tasks, frequently outperforming deep-learning approaches. This paper introduces SAFERec, a novel algorithm for NBR that enhances transformer-based architectures from NIR by incorporating item frequency information, consequently improving their applicability to NBR tasks. Extensive experiments on multiple datasets show that SAFERec outperforms all other baselines, specifically achieving an 8% improvement in Recall@10. | Oleg Lashinin, Denis Krasilnikov, Aleksandr Milogradskii, Marina Ananyeva |  |
|  |  [Can Generative AI Adequately Protect Queries? Analyzing the Trade-Off Between Privacy Awareness and Retrieval Effectiveness](https://doi.org/10.1007/978-3-031-88714-7_34) |  | 0 |  | Luca HerranzCelotti, Blessing Guembe, Giovanni Livraga, Marco Viviani |  |
|  |  [A Test Collection for Dataset Retrieval](https://doi.org/10.1007/978-3-031-88714-7_36) |  | 0 | Searching for scientific datasets is a prominent task in scholars' daily research practice. A variety of data publishers, archives and data portals offer search applications that allow the discovery of datasets. The evaluation of such dataset retrieval systems requires proper test collections, including questions that reflect real world information needs of scholars, a set of datasets and human judgements assessing the relevance of the datasets to the questions in the benchmark corpus. Unfortunately, only very few test collections exist for a dataset search. In this paper, we introduce the BEF-China test collection, the very first test collection for dataset retrieval in biodiversity research, a research field with an increasing demand in data discovery services. The test collection consists of 14 questions, a corpus of 372 datasets from the BEF-China project and binary relevance judgements provided by a biodiversity expert. | Nikolay Kolyada, Martin Potthast, Benno Stein | Friedrich Schiller University Jena, Department of Mathematics and Computer Science, Heinz Nixdorf Chair for Distributed Information Systems; Department of Citizen Science, Institute of Data Science, German Aerospace Center (DLR); Institute of Biology Geobotany and Botanical Garden, Martin Luther University Halle-Wittenberg; Department Forest Nature Conservation, Georg-August-Universität Göttingen |
|  |  [Improving RAG for Personalization with Author Features and Contrastive Examples](https://doi.org/10.1007/978-3-031-88714-7_40) |  | 0 | Personalization with retrieval-augmented generation (RAG) often fails to capture fine-grained features of authors, making it hard to identify their unique traits. To enrich the RAG context, we propose providing Large Language Models (LLMs) with author-specific features, such as average sentiment polarity and frequently used words, in addition to past samples from the author's profile. We introduce a new feature called Contrastive Examples: documents from other authors are retrieved to help LLM identify what makes an author's style unique in comparison to others. Our experiments show that adding a couple of sentences about the named entities, dependency patterns, and words a person uses frequently significantly improves personalized text generation. Combining features with contrastive examples boosts the performance further, achieving a relative 15 Our results show the value of fine-grained features for better personalization, while opening a new research dimension for including contrastive examples as a complement with RAG. We release our code publicly. | Mert Yazan, Suzan Verberne, Frederik Situmeang |  |
|  |  [E2Rank: Efficient and Effective Layer-Wise Reranking](https://doi.org/10.1007/978-3-031-88714-7_41) |  | 0 |  | Cesare Campagnano, Antonio Mallia, Jack Pertschuk, Fabrizio Silvestri |  |
|  |  [Retrieval-Augmented Neural Team Formation](https://doi.org/10.1007/978-3-031-88714-7_35) |  | 0 |  | Mohammad Dara, Radin Hamidi Rad, Fattane Zarrinkalam, Ebrahim Bagheri |  |
|  |  [A Comparative Analysis of Retrieval-Augmented Generation and Crowdsourcing for Fact-Checking](https://doi.org/10.1007/978-3-031-88714-7_44) |  | 0 |  | Francesco Bombassei De Bona, David La Barbera, Stefano Mizzaro, Kevin Roitero |  |
|  |  [PIE-Med: Predicting, Interpreting and Explaining Medical Recommendations](https://doi.org/10.1007/978-3-031-88720-8_2) |  | 0 |  | Antonio Romano, Giuseppe Riccio, Marco Postiglione, Vincenzo Moscato |  |
|  |  [MindWell: A Conversational Agent for Professional Depression Screening on Social Media](https://doi.org/10.1007/978-3-031-88720-8_9) |  | 0 |  | Eliseo Bao, Anxo Pérez, Javier Parapar |  |
|  |  [DenseReviewer: A Screening Prioritisation Tool for Systematic Review Based on Dense Retrieval](https://doi.org/10.1007/978-3-031-88720-8_11) |  | 0 | Screening is a time-consuming and labour-intensive yet required task for medical systematic reviews, as tens of thousands of studies often need to be screened. Prioritising relevant studies to be screened allows downstream systematic review creation tasks to start earlier and save time. In previous work, we developed a dense retrieval method to prioritise relevant studies with reviewer feedback during the title and abstract screening stage. Our method outperforms previous active learning methods in both effectiveness and efficiency. In this demo, we extend this prior work by creating (1) a web-based screening tool that enables end-users to screen studies exploiting state-of-the-art methods and (2) a Python library that integrates models and feedback mechanisms and allows researchers to develop and demonstrate new active learning methods. We describe the tool's design and showcase how it can aid screening. The tool is available at https://densereviewer.ielab.io. The source code is also open sourced at https://github.com/ielab/densereviewer. | Xinyu Mao, Teerapong Leelanupab, Harrisen Scells, Guido Zuccon |  |
|  |  [MechIR: A Mechanistic Interpretability Framework for Information Retrieval](https://doi.org/10.1007/978-3-031-88720-8_16) |  | 0 | Mechanistic interpretability is an emerging diagnostic approach for neural models that has gained traction in broader natural language processing domains. This paradigm aims to provide attribution to components of neural systems where causal relationships between hidden layers and output were previously uninterpretable. As the use of neural models in IR for retrieval and evaluation becomes ubiquitous, we need to ensure that we can interpret why a model produces a given output for both transparency and the betterment of systems. This work comprises a flexible framework for diagnostic analysis and intervention within these highly parametric neural systems specifically tailored for IR tasks and architectures. In providing such a framework, we look to facilitate further research in interpretable IR with a broader scope for practical interventions derived from mechanistic interpretability. We provide preliminary analysis and look to demonstrate our framework through an axiomatic lens to show its applications and ease of use for those IR practitioners inexperienced in this emerging paradigm. | Andrew Parry, Catherine Chen, Carsten Eickhoff, Sean MacAvaney |  |
|  |  [Combining Knowledge Graphs and Retrieval Augmented Generation for Enterprise Resource Planning](https://doi.org/10.1007/978-3-031-88720-8_19) |  | 0 |  | Amar Viswanathan, Felix Sasaki |  |
|  |  [Web-Scale Retrieval Experimentation with chatnoir-pyterrier](https://doi.org/10.1007/978-3-031-88720-8_17) |  | 0 |  | Jan Heinrich Merker, Janek Bevendorff, Maik Fröbe, Tim Hagen, Harrisen Scells, Matti Wiegmann, Benno Stein, Matthias Hagen, Martin Potthast |  |
|  |  [Contextualizing Spotify's Audiobook List Recommendations with Descriptive Shelves](https://doi.org/10.1007/978-3-031-88720-8_26) |  | 0 | In this paper, we propose a pipeline to generate contextualized list recommendations with descriptive shelves in the domain of audiobooks. By creating several shelves for topics the user has an affinity to, e.g. Uplifting Women's Fiction, we can help them explore their recommendations according to their interests and at the same time recommend a diverse set of items. To do so, we use Large Language Models (LLMs) to enrich each item's metadata based on a taxonomy created for this domain. Then we create diverse descriptive shelves for each user. A/B tests show improvements in user engagement and audiobook discovery metrics, demonstrating benefits for users and content creators. | Gustavo Penha, Alice Wang, Martin Achenbach, Kristen Sheets, Sahitya Mantravadi, Remi Galvez, Nico GuettaJeanrenaud, Divya Narayanan, Ofeliya Kalaydzhyan, Hugues Bouchard |  |
|  |  [Text2Playlist: Generating Personalized Playlists from Text on Deezer](https://doi.org/10.1007/978-3-031-88720-8_27) |  | 0 | The streaming service Deezer heavily relies on the search to help users navigate through its extensive music catalog. Nonetheless, it is primarily designed to find specific items and does not lead directly to a smooth listening experience. We present Text2Playlist, a stand-alone tool that addresses these limitations. Text2Playlist leverages generative AI, music information retrieval and recommendation systems to generate query-specific and personalized playlists, successfully deployed at scale. | Mathieu Delcluze, Antoine Khoury, Clémence Vast, Valerio Arnaudo, Léa Briand, Walid Bendada, Thomas Bouabça |  |
|  |  [Cooperative and Competitive LLM-Based Multi-Agent Systems for Recommendation](https://doi.org/10.1007/978-3-031-88720-8_33) |  | 0 |  | Marco Valentini |  |
|  |  [Advancing Query Performance Prediction: Challenges and Adaptive Solutions](https://doi.org/10.1007/978-3-031-88720-8_38) |  | 0 |  | Abbas Saleminezhad |  |
|  |  [Explainable Information Retrieval](https://doi.org/10.1007/978-3-031-88720-8_40) |  | 0 | Image segmentation is useful to extract valuable information for an efficient analysis on the region of interest. Mostly, the number of images generated from a real life situation such as streaming video, is large and not ideal for traditional segmentation with machine learning algorithms. This is due to the following factors (a) numerous image features (b) complex distribution of shapes, colors and textures (c) imbalance data ratio of underlying classes (d) movements of the camera, objects and (e) variations in luminance for site capture. So, we have proposed an efficient deep learning model for image classification and the proof-of-concept has been the case studied on gastrointestinal images for bleeding detection. The Ex plainable Artificial Intelligence (XAI) module has been utilized to reverse engineer the test results for the impact of features on a given test dataset. The architecture is generally applicable in other areas of image classification. The proposed method has been compared with state-of-the-art including Logistic Regression, Support Vector Machine, Artificial Neural Network and Random Forest. It has reported F1 score of 0.76 on the real world streaming dataset which is comparatively better than traditional methods. | Avishek Anand, Sourav Saha, Venktesh V |  |
|  |  [QPP++ 2025: Query Performance Prediction and Its Applications in the Era of Large Language Models](https://doi.org/10.1007/978-3-031-88720-8_49) |  | 0 |  | Chuan Meng, Guglielmo Faggioli, Mohammad Aliannejadi, Nicola Ferro, Josiane Mothe |  |
|  |  [eRisk 2025: Contextual and Conversational Approaches for Depression Challenges](https://doi.org/10.1007/978-3-031-88720-8_62) |  | 0 |  | Javier Parapar, Anxo Pérez, Xi Wang, Fabio Crestani |  |
|  |  [The CLEF-2025 CheckThat! Lab: Subjectivity, Fact-Checking, Claim Normalization, and Retrieval](https://doi.org/10.1007/978-3-031-88720-8_68) |  | 0 | The CheckThat! lab aims to advance the development of innovative technologies designed to identify and counteract online disinformation and manipulation efforts across various languages and platforms. The first five editions focused on key tasks in the information verification pipeline, including check-worthiness, evidence retrieval and pairing, and verification. Since the 2023 edition, the lab has expanded its scope to address auxiliary tasks that support research and decision-making in verification. In the 2025 edition, the lab revisits core verification tasks while also considering auxiliary challenges. Task 1 focuses on the identification of subjectivity (a follow-up from CheckThat! 2024), Task 2 addresses claim normalization, Task 3 targets fact-checking numerical claims, and Task 4 explores scientific web discourse processing. These tasks present challenging classification and retrieval problems at both the document and span levels, including multilingual settings. | Firoj Alam, Julia Maria Struß, Tanmoy Chakraborty, Stefan Dietze, Salim Hafid, Katerina Korre, Arianna Muti, Preslav Nakov, Federico Ruggeri, Sebastian Schellhammer, Vinay Setty, Megha Sundriyal, Konstantin Todorov, Venktesh V |  |
|  |  [Graph-Convolutional Networks: Named Entity Recognition and Large Language Model Embedding in Document Clustering](https://doi.org/10.1007/978-3-031-88711-6_6) |  | 0 | Recent advances in machine learning, particularly Large Language Models (LLMs) such as BERT and GPT, provide rich contextual embeddings that improve text representation. However, current document clustering approaches often ignore the deeper relationships between named entities (NEs) and the potential of LLM embeddings. This paper proposes a novel approach that integrates Named Entity Recognition (NER) and LLM embeddings within a graph-based framework for document clustering. The method builds a graph with nodes representing documents and edges weighted by named entity similarity, optimized using a graph-convolutional network (GCN). This ensures a more effective grouping of semantically related documents. Experimental results indicate that our approach outperforms conventional co-occurrence-based methods in clustering, notably for documents rich in named entities. | Imed Keraghel, Mohamed Nadif |  |
|  |  [MVAM: Multi-View Attention Method for Fine-Grained Image-Text Matching](https://doi.org/10.1007/978-3-031-88711-6_11) |  | 0 | Existing two-stream models, such as CLIP, encode images and text through independent representations, showing good performance while ensuring retrieval speed, have attracted attention from industry and academia. However, the single representation often struggles to capture complex content fully. Such models may ignore fine-grained information during matching, resulting in suboptimal retrieval results. To overcome this limitation and enhance the performance of two-stream models, we propose a Multi-view Attention Method (MVAM) for image-text matching. This approach leverages diverse attention heads with unique view codes to learn multiple representations for images and text, which are then concatenated for matching. We also incorporate a diversity objective to explicitly encourage attention heads to focus on distinct aspects of the input data, capturing complementary fine-grained details. This diversity enables the model to represent image-text pairs from multiple perspectives, ensuring a more comprehensive understanding and alignment of critical content. Our method allows models to encode images and text from different perspectives and focus on more critical details, leading to better matching performance. Our experiments on MSCOCO and Flickr30K demonstrate enhancements over existing models, and further case studies reveal that different attention heads can focus on distinct content, achieving more comprehensive representations. | Wanqing Cui, Rui Cheng, Jiafeng Guo, Xueqi Cheng |  |
|  |  [PEIR: Modeling Performance in Neural Information Retrieval](https://doi.org/10.1007/978-3-031-88711-6_18) |  | 0 |  | Pooya Khandel, Andrew Yates, Ana Lucia Varbanescu, Maarten de Rijke, Andy D. Pimentel |  |
|  |  [mFollowIR: A Multilingual Benchmark for Instruction Following in Retrieval](https://doi.org/10.1007/978-3-031-88711-6_19) |  | 0 | Retrieval systems generally focus on web-style queries that are short and underspecified. However, advances in language models have facilitated the nascent rise of retrieval models that can understand more complex queries with diverse intents. However, these efforts have focused exclusively on English; therefore, we do not yet understand how they work across languages. We introduce mFollowIR, a multilingual benchmark for measuring instruction-following ability in retrieval models. mFollowIR builds upon the TREC NeuCLIR narratives (or instructions) that span three diverse languages (Russian, Chinese, Persian) giving both query and instruction to the retrieval models. We make small changes to the narratives and isolate how well retrieval models can follow these nuanced changes. We present results for both multilingual (XX-XX) and cross-lingual (En-XX) performance. We see strong cross-lingual performance with English-based retrievers that trained using instructions, but find a notable drop in performance in the multilingual setting, indicating that more work is needed in developing data for instruction-based multilingual retrievers. | Orion Weller, Benjamin Chang, Eugene Yang, Mahsa Yarmohammadi, Samuel Barham, Sean MacAvaney, Arman Cohan, Luca Soldaini, Benjamin Van Durme, Dawn J. Lawrie |  |
|  |  [Leveraging Retrieval-Augmented Generation for Keyphrase Synonym Suggestion](https://doi.org/10.1007/978-3-031-88711-6_20) |  | 0 |  | Jorge Gabín, Javier Parapar |  |
|  |  [Can Large Language Models Effectively Rerank News Articles for Background Linking?](https://doi.org/10.1007/978-3-031-88711-6_21) |  | 0 |  | Marwa Essam, Tamer Elsayed |  |
|  |  [OKRA: An Explainable, Heterogeneous, Multi-stakeholder Job Recommender System](https://doi.org/10.1007/978-3-031-88711-6_22) |  | 0 | The use of recommender systems in the recruitment domain has been labeled as 'high-risk' in recent legislation. As a result, strict requirements regarding explainability and fairness have been put in place to ensure proper treatment of all involved stakeholders. To allow for stakeholder-specific explainability, while also handling highly heterogeneous recruitment data, we propose a novel explainable multi-stakeholder job recommender system using graph neural networks: the Occupational Knowledge-based Recommender using Attention (OKRA). The proposed method is capable of providing both candidate- and company-side recommendations and explanations. We find that OKRA performs substantially better than six baselines in terms of nDCG for two datasets. Furthermore, we find that the tested models show a bias toward candidates and vacancies located in urban areas. Overall, our findings suggest that OKRA provides a balance between accuracy, explainability, and fairness. | Roan Schellingerhout, Francesco Barile, Nava Tintarev |  |
|  |  [CUP: A Framework for Resource-Efficient Review-Based Recommenders](https://doi.org/10.1007/978-3-031-88711-6_23) |  | 0 |  | Ghazaleh H. Torbati, Anna Tigunova, Gerhard Weikum, Andrew Yates |  |
|  |  [On the Robustness of Generative Information Retrieval Models: An Out-of-Distribution Perspective](https://doi.org/10.1007/978-3-031-88711-6_26) |  | 0 | Recently, we have witnessed the bloom of neural ranking models in the information retrieval (IR) field. So far, much effort has been devoted to developing effective neural ranking models that can generalize well on new data. There has been less attention paid to the robustness perspective. Unlike the effectiveness, which is about the average performance of a system under normal purpose, robustness cares more about the system performance in the worst case or under malicious operations instead. When a new technique enters into the real-world application, it is critical to know not only how it works in average, but also how would it behave in abnormal situations. So, we raise the question in this work: Are neural ranking models robust? To answer this question, first, we need to clarify what we refer to when we talk about the robustness of ranking models in IR. We show that robustness is actually a multi-dimensional concept and there are three ways to define it in IR: (1) the performance variance under the independent and identically distributed (I.I.D.) setting; (2) the out-of-distribution (OOD) generalizability ; and (3) the defensive ability against adversarial operations. The latter two definitions can be further specified into two different perspectives, respectively, leading to five robustness tasks in total. Based on this taxonomy, we build corresponding benchmark datasets, design empirical experiments, and systematically analyze the robustness of several representative neural ranking models against traditional probabilistic ranking models and learning-to-rank (LTR) models. The empirical results show that there is no simple answer to our question. While neural ranking models are less robust against other IR models in most cases, some of them can still win two out of five tasks. This is the first comprehensive study on the robustness of neural ranking models. We believe the way we study the robustness as well as our findings would be beneficial to the IR community. We will also release all the data and codes to facilitate the future research in this direction. | YuAn Liu, Ruqing Zhang, Jiafeng Guo, Changjiang Zhou, Maarten de Rijke, Xueqi Cheng | Chinese Acad Sci, Inst Comp Technol, CAS Key Lab Network Data Sci & Technol, Beijing, Peoples R China |
|  |  [Towards Reliable Testing for Multiple Information Retrieval System Comparisons](https://doi.org/10.1007/978-3-031-88711-6_27) |  | 0 | Null Hypothesis Significance Testing is the de facto tool for assessing effectiveness differences between Information Retrieval systems. Researchers use statistical tests to check whether those differences will generalise to online settings or are just due to the samples observed in the laboratory. Much work has been devoted to studying which test is the most reliable when comparing a pair of systems, but most of the IR real-world experiments involve more than two. In the multiple comparisons scenario, testing several systems simultaneously may inflate the errors committed by the tests. In this paper, we use a new approach to assess the reliability of multiple comparison procedures using simulated and real TREC data. Experiments show that Wilcoxon plus the Benjamini-Hochberg correction yields Type I error rates according to the significance level for typical sample sizes while being the best test in terms of statistical power. | David Otero, Javier Parapar, Álvaro Barreiro |  |
|  |  [Leveraging High-Resolution Features for Improved Deep Hashing-Based Image Retrieval](https://doi.org/10.1007/978-3-031-88711-6_28) |  | 0 | Deep hashing techniques have emerged as the predominant approach forefficient image retrieval. Traditionally, these methods utilize pre-trainedconvolutional neural networks (CNNs) such as AlexNet and VGG-16 as featureextractors. However, the increasing complexity of datasets poses challenges forthese backbone architectures in capturing meaningful features essential foreffective image retrieval. In this study, we explore the efficacy of employinghigh-resolution features learned through state-of-the-art techniques for imageretrieval tasks. Specifically, we propose a novel methodology that utilizesHigh-Resolution Networks (HRNets) as the backbone for the deep hashing task,termed High-Resolution Hashing Network (HHNet). Our approach demonstratessuperior performance compared to existing methods across all tested benchmarkdatasets, including CIFAR-10, NUS-WIDE, MS COCO, and ImageNet. This performanceimprovement is more pronounced for complex datasets, which highlights the needto learn high-resolution features for intricate image retrieval tasks.Furthermore, we conduct a comprehensive analysis of different HRNetconfigurations and provide insights into the optimal architecture for the deephashing task | Aymene Berriche, Mehdi Zakaria Adjal, Riyadh Baghdadi | École nationale supérieure d'informatique; New York University Abu Dhabi |
|  |  [BioASQ at CLEF2025: The Thirteenth Edition of the Large-Scale Biomedical Semantic Indexing and Question Answering Challenge](https://doi.org/10.1007/978-3-031-88720-8_61) |  | 0 |  | Anastasios Nentidis, Georgios Katsimpras, Anastasia Krithara, Martin Krallinger, Miguel RodríguezOrtega, Natalia V. Loukachevitch, Andrey Sakhovskiy, Elena Tutubalina, Grigorios Tsoumakas, George Giannakoulas, Alexandra Bekiaridou, Athanasios Samaras, Giorgio Maria Di Nunzio, Nicola Ferro, Stefano Marchesin, Laura Menotti, Gianmaria Silvello, Georgios Paliouras |  |
|  |  [LIBRA: Measuring Bias of Large Language Model from a Local Context](https://doi.org/10.1007/978-3-031-88708-6_1) |  | 0 | Large Language Models (LLMs) have significantly advanced natural language processing applications, yet their widespread use raises concerns regarding inherent biases that may reduce utility or harm for particular social groups. Despite the advancement in addressing LLM bias, existing research has two major limitations. First, existing LLM bias evaluation focuses on the U.S. cultural context, making it challenging to reveal stereotypical biases of LLMs toward other cultures, leading to unfair development and use of LLMs. Second, current bias evaluation often assumes models are familiar with the target social groups. When LLMs encounter words beyond their knowledge boundaries that are unfamiliar in their training data, they produce irrelevant results in the local context due to hallucinations and overconfidence, which are not necessarily indicative of inherent bias. This research addresses these limitations with a Local Integrated Bias Recognition and Assessment Framework (LIBRA) for measuring bias using datasets sourced from local corpora without crowdsourcing. Implementing this framework, we develop a dataset comprising over 360,000 test cases in the New Zealand context. Furthermore, we propose the Enhanced Idealized CAT Score (EiCAT), integrating the iCAT score with a beyond knowledge boundary score (bbs) and a distribution divergence-based bias measurement to tackle the challenge of LLMs encountering words beyond knowledge boundaries. Our results show that the BERT family, GPT-2, and Llama-3 models seldom understand local words in different contexts. While Llama-3 exhibits larger bias, it responds better to different cultural contexts. The code and dataset are available at: https://github.com/ipangbo/LIBRA. | Bo Pang, Tingrui Qiao, Caroline Walker, Chris Cunningham, Yun Sing Koh |  |
|  |  [Biased PromptORE: Enhancing Relation Extraction in Gendered Languages and Complex Texts The Case of Spanish Documents from the XVIbf th Century](https://doi.org/10.1007/978-3-031-88708-6_17) |  | 0 |  | Michel Boeglin, David Kahn, Héctor López Hidalgo, Josiane Mothe, Diégo Ortiz, David Panzoli |  |
|  |  [Semantically Proportioned nDCG for Explaining ColBERT's Learning Process](https://doi.org/10.1007/978-3-031-88708-6_22) |  | 0 |  | Ariane Mueller, Craig Macdonald |  |
|  |  [Tales and Truths: Exploring the Linguistic Journey of 19th Century Literature and Non-fiction](https://doi.org/10.1007/978-3-031-88717-8_19) |  | 0 |  | Suchana Datta, Dwaipayan Roy, Derek Greene, Gerardine Meaney |  |
|  |  [TROPIC - Trustworthiness Rating of Online Publishers Through Online Interactions Calculation](https://doi.org/10.1007/978-3-031-88717-8_30) |  | 0 | Existing methods for assessing the trustworthiness of news publishers face high costs and scalability issues. The tool presented in this paper supports the efforts of specialized organizations by providing a solution that, starting from an online discussion, provides (i) trustworthiness ratings for previously unclassified news publishers and (ii) an interactive platform to guide annotation efforts and improve the robustness of the ratings. The system implements a novel framework for assessing the trustworthiness of online news publishers based on user interactions on social media platforms. | Manuel Pratelli, Fabio Saracco, Marinella Petrocchi |  |
|  |  [LS-Dashboard: A Tool for Monitoring and Analyzing Data Annotation in Machine Learning Classification Tasks](https://doi.org/10.1007/978-3-031-88717-8_31) |  | 0 |  | Vinicius Monteiro de Lira, Peng Jiang |  |
|  |  [Prabodhini: Making Large Language Models Inclusive for Low-Text Literate Users](https://doi.org/10.1007/978-3-031-88717-8_35) |  | 0 |  | Vivan Jain, Srivant Vishnuvajjala, Pranathi Voora, Bhaskar Ruthvik Bikkina, Bharghavaram Boddapati, C. R. Chaitra, Dipanjan Chakraborty, Prajna Upadhyay |  |
|  |  [BAAF: A Framework for Media Bias Detection](https://doi.org/10.1007/978-3-031-88714-7_24) |  | 0 |  | Soumyadeep Sar, Subinay Adhikary, Dwaipayan Roy |  |
|  |  [BiasScanner: Automatic News Bias Classification for Strengthening Democracy](https://doi.org/10.1007/978-3-031-88720-8_18) |  | 0 |  | Tim Menzner, Jochen L. Leidner |  |
|  |  [Automatic Evaluation of Online News Outlets' Reliability](https://doi.org/10.1007/978-3-031-88720-8_32) |  | 0 |  | John Bianchi |  |
|  |  [Uncertainty Estimation in the Real World: A Study on Music Emotion Recognition](https://doi.org/10.1007/978-3-031-88711-6_14) |  | 0 | Any data annotation for subjective tasks shows potential variations between individuals. This is particularly true for annotations of emotional responses to musical stimuli. While older approaches to music emotion recognition systems frequently addressed this uncertainty problem through probabilistic modeling, modern systems based on neural networks tend to ignore the variability and focus only on predicting central tendencies of human subjective responses. In this work, we explore several methods for estimating not only the central tendencies of the subjective responses to a musical stimulus, but also for estimating the uncertainty associated with these responses. In particular, we investigate probabilistic loss functions and inference-time random sampling. Experimental results indicate that while the modeling of the central tendencies is achievable, modeling of the uncertainty in subjective responses proves significantly more challenging with currently available approaches even when empirical estimates of variations in the responses are available. | Karn N. Watcharasupat, Yiwei Ding, T. Aleksandra Ma, Pavan Seshadri, Alexander Lerch |  |
|  |  [Towards Efficient and Explainable Hate Speech Detection via Model Distillation](https://doi.org/10.1007/978-3-031-88711-6_24) |  | 0 | Automatic detection of hate and abusive language is essential to combat its online spread. Moreover, recognising and explaining hate speech serves to educate people about its negative effects. However, most current detection models operate as black boxes, lacking interpretability and explainability. In this context, Large Language Models (LLMs) have proven effective for hate speech detection and to promote interpretability. Nevertheless, they are computationally costly to run. In this work, we propose distilling big language models by using Chain-of-Thought to extract explanations that support the hate speech classification task. Having small language models for these tasks will contribute to their use in operational settings. In this paper, we demonstrate that distilled models deliver explanations of the same quality as larger models while surpassing them in classification performance. This dual capability, classifying and explaining, advances hate speech detection making it more affordable, understandable and actionable. | Paloma Piot, Javier Parapar |  |
|  |  [One Size Doesn't Fit All: Predicting the Number of Examples for In-Context Learning](https://doi.org/10.1007/978-3-031-88708-6_5) |  | 0 | In-context learning (ICL) refers to the process of adding a small number of localized examples (ones that are semantically similar to the input) from a training set of labelled data to an LLM's prompt with an objective to effectively control the generative process seeking to improve the downstream task performance. Existing ICL approaches use an identical number of examples (a pre-configured hyper-parameter) for each data instance. Our work alleviates the limitations of this 'one fits all' approach by dynamically predicting the number of examples for each data instance to be used in few-shot inference with LLMs. In particular, we employ a multi-label classifier, the parameters of which are fitted using a training set, where the label for each instance in the training set indicates if using a specific value of k (number of most similar examples from 0 up to a maximum value) leads to correct k-shot downstream predictions. Our experiments on a number of text classification benchmarks show that AICL substantially outperforms standard ICL by up to 17 | Manish Chandra, Debasis Ganguly, Iadh Ounis | University of Glasgow Glasgow |
|  |  [Enhancing FEVER-Style Claim Fact-Checking Against Wikipedia: A Diagnostic Taxonomy and a Generative Framework](https://doi.org/10.1007/978-3-031-88708-6_20) |  | 0 |  | Anton Chernyavskiy, Dmitry Ilvovsky, Preslav Nakov |  |
|  |  [Decoding the Hierarchy: A Hybrid Approach to Hierarchical Multi-label Text Classification](https://doi.org/10.1007/978-3-031-88708-6_26) |  | 0 |  | Fatos Torba, Christophe Gravier, Charlotte Laclau, Abderrhammen Kammoun, Julien Subercaze |  |
|  |  [Malevolence Attacks Against Pretrained Dialogue Models](https://doi.org/10.1007/978-3-031-88708-6_24) |  | 0 |  | Pengjie Ren, Ruiqi Li, Zhaochun Ren, Zhumin Chen, Maarten de Rijke, Yangjun Zhang |  |
|  |  [ColBERT-Serve: Efficient Multi-stage Memory-Mapped Scoring](https://doi.org/10.1007/978-3-031-88717-8_3) |  | 0 | We study serving retrieval models, specifically late interaction models like ColBERT, to many concurrent users at once and under a small budget, in which the index may not fit in memory. We present ColBERT-serve, a novel serving system that applies a memory-mapping strategy to the ColBERT index, reducing RAM usage by 90 incorporates a multi-stage architecture with hybrid scoring, reducing ColBERT's query latency and supporting many concurrent queries in parallel. | Kaili Huang, Thejas Venkatesh, Uma Dingankar, Antonio Mallia, Daniel Campos, Jian Jiao, Christopher Potts, Matei Zaharia, Kwabena Boahen, Omar Khattab, Saarthak Sarup, Keshav Santhanam |  |
|  |  [A Reproducibility Study on Consistent LLM Reasoning for Natural Language Inference over Clinical Trials](https://doi.org/10.1007/978-3-031-88717-8_5) |  | 0 |  | Artur Guimarães, João Magalhães, Bruno Martins |  |
|  |  [Multimodal Feature Extraction for Assistive Technology: Evaluation and Dataset](https://doi.org/10.1007/978-3-031-88717-8_13) |  | 0 |  | Hunter Briegel, Maya Pagal, Jacki Liddle, J. Shane Culpepper |  |
|  |  [GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer Pharmacovigilance](https://doi.org/10.1007/978-3-031-88717-8_17) |  | 0 | In the realm of cancer treatment, summarizing adverse drug events (ADEs) reported by patients using prescribed drugs is crucial for enhancing pharmacovigilance practices and improving drug-related decision-making. While the volume and complexity of pharmacovigilance data have increased, existing research in this field has predominantly focused on general diseases rather than specifically addressing cancer. This work introduces the task of grouped summarization of adverse drug events reported by multiple patients using the same drug for cancer treatment. To address the challenge of limited resources in cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug Reaction and Summarization (MCADRS) dataset. This dataset includes pharmacovigilance posts detailing patient concerns regarding drug efficacy and adverse effects, along with extracted labels for drug names, adverse drug events, severity, and adversity of reactions, as well as summaries of ADEs for each drug. Additionally, we propose the Grouping and Abstractive Summarization of Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that combines the information extraction capabilities of Large Language Models (LLMs) with the summarization power of the encoder-decoder T5 model. Our work is the first to apply alignment techniques, including advanced algorithms like Direct Preference Optimization, to encoder-decoder models using synthetic datasets for summarization tasks. Through extensive experiments, we demonstrate the superior performance of GASCADE across various metrics, validated through both automated assessments and human evaluations. This multitasking approach enhances drug-related decision-making and fosters a deeper understanding of patient concerns, paving the way for advancements in personalized and responsive cancer care. The code and dataset used in this work are publicly available. | Sofia Jamil, Aryan Dabad, Bollampalli Areen Reddy, Sriparna Saha, Rajiv Misra, Adil A. Shakur |  |
|  |  [Verifying Cross-Modal Entity Consistency in News Using Vision-Language Models](https://doi.org/10.1007/978-3-031-88717-8_25) |  | 0 | The web has become a crucial source of information, but it is also used to spread disinformation, often conveyed through multiple modalities like images and text. The identification of inconsistent cross-modal information, in particular entities such as persons, locations, and events, is critical to detect disinformation. Previous works either identify out-of-context disinformation by assessing the consistency of images to the whole document, neglecting relations of individual entities, or focus on generic entities that are not relevant to news. So far, only few approaches have addressed the task of validating entity consistency between images and text in news. However, the potential of large vision-language models (LVLMs) has not been explored yet. In this paper, we propose an LVLM-based framework for verifying Cross-modal Entity Consistency (LVLM4CEC), to assess whether persons, locations and events in news articles are consistent across both modalities. We suggest effective prompting strategies for LVLMs for entity verification that leverage reference images crawled from web. Moreover, we extend three existing datasets for the task of entity verification in news providing manual ground-truth data. Our results show the potential of LVLMs for automating cross-modal entity verification, showing improved accuracy in identifying persons and events when using evidence images. Moreover, our method outperforms a baseline for location and event verification in documents. The datasets and source code are available on GitHub at https://github.com/TIBHannover/LVLM4CEC. | Sahar Tahmasebi, Eric MüllerBudack, Ralph Ewerth |  |
|  |  [Nano-ESG: Extracting Corporate Sustainability Information from News Articles](https://doi.org/10.1007/978-3-031-88717-8_24) |  | 0 | Determining the sustainability impact of companies is a highly complex subject which has garnered more and more attention over the past few years. Today, investors largely rely on sustainability-ratings from established rating-providers in order to analyze how responsibly a company acts. However, those ratings have recently been criticized for being hard to understand and nearly impossible to reproduce. An independent way to find out about the sustainability practices of companies lies in the rich landscape of news article data. In this paper, we explore a different approach to identify key opportunities and challenges of companies in the sustainability domain. We present a novel dataset of more than 840,000 news articles which were gathered for major German companies between January 2023 and September 2024. By applying a mixture of Natural Language Processing techniques, we first identify relevant articles, before summarizing them and extracting their sustainability-related sentiment and aspect using Large Language Models (LLMs). Furthermore, we conduct an evaluation of the obtained data and determine that the LLM-produced answers are accurate. We release both datasets at https://github.com/Bailefan/Nano-ESG. | Fabian Billert, Stefan Conrad |  |
|  |  [TimIR: Time-Traveling Through IR History](https://doi.org/10.1007/978-3-031-88717-8_34) |  | 0 |  | Moritz Staudinger, Wojciech Kusa, Florina Piroi, Andreas Rauber, Allan Hanbury |  |
|  |  [SimplifyMyText: An LLM-Based System for Inclusive Plain Language Text Simplification](https://doi.org/10.1007/978-3-031-88717-8_32) |  | 0 | Text simplification is essential for making complex content accessible to diverse audiences who face comprehension challenges. Yet, the limited availability of simplified materials creates significant barriers to personal and professional growth and hinders social inclusion. Although researchers have explored various methods for automatic text simplification, none fully leverage large language models (LLMs) to offer tailored customization for different target groups and varying levels of simplicity. Moreover, despite its proven benefits for both consumers and organizations, the well-established practice of plain language remains underutilized. In this paper, we https://simplifymytext.org, the first system designed to produce plain language content from multiple input formats, including typed text and file uploads, with flexible customization options for diverse audiences. We employ GPT-4 and Llama-3 and evaluate outputs across multiple metrics. Overall, our work contributes to research on automatic text simplification and highlights the importance of tailored communication in promoting inclusivity. | Michael Färber, Parisa Aghdam, Kyuri Im, Mario Tawfelis, Hardik Ghoshal |  |
|  |  [exHarmony: Authorship and Citations for Benchmarking the Reviewer Assignment Problem](https://doi.org/10.1007/978-3-031-88714-7_1) |  | 0 | The peer review process is crucial for ensuring the quality and reliability of scholarly work, yet assigning suitable reviewers remains a significant challenge. Traditional manual methods are labor-intensive and often ineffective, leading to nonconstructive or biased reviews. This paper introduces the exHarmony (eHarmony but for connecting experts to manuscripts) benchmark, designed to address these challenges by re-imagining the Reviewer Assignment Problem (RAP) as a retrieval task. Utilizing the extensive data from OpenAlex, we propose a novel approach that considers a host of signals from the authors, most similar experts, and the citation relations as potential indicators for a suitable reviewer for a manuscript. This approach allows us to develop a standard benchmark dataset for evaluating the reviewer assignment problem without needing explicit labels. We benchmark various methods, including traditional lexical matching, static neural embeddings, and contextualized neural embeddings, and introduce evaluation metrics that assess both relevance and diversity in the context of RAP. Our results indicate that while traditional methods perform reasonably well, contextualized embeddings trained on scholarly literature show the best performance. The findings underscore the importance of further research to enhance the diversity and effectiveness of reviewer assignments. | Sajad Ebrahimi, Sara Salamat, Negar Arabzadeh, Mahdi Bashari, Ebrahim Bagheri |  |
|  |  [EGL-DST: Error-Guided Learning for Multidimensional Evaluation Method of Dialogue State Tracking via GPT-4](https://doi.org/10.1007/978-3-031-88714-7_8) |  | 0 |  | Wenjie Dong, Sirong Chen, Ming Gu, Yan Yang |  |
|  |  [Improving Language Model Performance by Training on Prototypical Contradictions](https://doi.org/10.1007/978-3-031-88714-7_12) |  | 0 |  | Maren Pielka, MarieChristin Freischlad, Svetlana Schmidt, Rafet Sifa |  |
|  |  [Hierarchical Skip Decoding for Efficient Autoregressive Language Model](https://doi.org/10.1007/978-3-031-88714-7_20) |  | 0 |  | Yunqi Zhu, Xuebing Yang, Yuanyuan Wu, Wensheng Zhang |  |
|  |  [Towards Interpretable Radiology Report Generation via Concept Bottlenecks Using a Multi-agentic RAG](https://doi.org/10.1007/978-3-031-88714-7_18) |  | 0 | Deep learning has advanced medical image classification, but interpretability challenges hinder its clinical adoption. This study enhances interpretability in Chest X-ray (CXR) classification by using concept bottleneck models (CBMs) and a multi-agent Retrieval-Augmented Generation (RAG) system for report generation. By modeling relationships between visual features and clinical concepts, we create interpretable concept vectors that guide a multi-agent RAG system to generate radiology reports, enhancing clinical relevance, explainability, and transparency. Evaluation of the generated reports using an LLM-as-a-judge confirmed the interpretability and clinical utility of our model's outputs. On the COVID-QU dataset, our model achieved 81 accuracy and demonstrated robust report generation performance, with five key metrics ranging between 84 bridges the gap between high-performance AI and the explainability required for reliable AI-driven CXR analysis in clinical settings. | Hasan Md Tusfiqur Alam, Devansh Srivastav, Md Abdul Kadir, Daniel Sonntag |  |
|  |  [A Simple but Effective Closed-Form Solution for Extreme Multi-label Learning](https://doi.org/10.1007/978-3-031-88714-7_25) |  | 0 | Extreme multi-label learning (XML) is a task of assigning multiple labels from an extremely large set of labels to each data instance. Many current high-performance XML models are composed of a lot of hyperparameters, which complicates the tuning process. Additionally, the models themselves are adapted specifically to XML, which complicates their reimplementation. To remedy this problem, we propose a simple method based on ridge regression for XML. The proposed method not only has a closed-form solution but also is composed of a single hyperparameter. Since there are no precedents on applying ridge regression to XML, this paper verified the performance of the method by using various XML benchmark datasets. Furthermore, we enhanced the prediction of low-frequency labels in XML, which hold informative content. This prediction is essential yet challenging because of the limited amount of data. Here, we employed a simple frequency-based weighting. This approach greatly simplifies the process compared with existing techniques. Experimental results revealed that it can achieve levels of performance comparable to, or even exceeding, those of models with numerous hyperparameters. Additionally, we found that the frequency-based weighting significantly improved the predictive performance for low-frequency labels, while requiring almost no changes in implementation. The source code for the proposed method is available on github at https://github.com/cars1015/XML-ridge. | Kazuma Onishi, Katsuhiko Hayashi |  |
|  |  [Benchmarking Prompt Sensitivity in Large Language Models](https://doi.org/10.1007/978-3-031-88714-7_29) |  | 0 | Large language Models (LLMs) are highly sensitive to variations in prompt formulation, which can significantly impact their ability to generate accurate responses. In this paper, we introduce a new task, Prompt Sensitivity Prediction, and a dataset PromptSET designed to investigate the effects of slight prompt variations on LLM performance. Using TriviaQA and HotpotQA datasets as the foundation of our work, we generate prompt variations and evaluate their effectiveness across multiple LLMs. We benchmark the prompt sensitivity prediction task employing state-of-the-art methods from related tasks, including LLM-based self-evaluation, text classification, and query performance prediction techniques. Our findings reveal that existing methods struggle to effectively address prompt sensitivity prediction, underscoring the need to understand how information needs should be phrased for accurate LLM responses. | Amir Hossein Razavi, Mina Soltangheis, Negar Arabzadeh, Sara Salamat, Morteza Zihayat, Ebrahim Bagheri |  |
|  |  [Do LLMs Provide Consistent Answers to Health-Related Questions Across Languages?](https://doi.org/10.1007/978-3-031-88714-7_30) |  | 0 | Equitable access to reliable health information is vital for public health, but the quality of online health resources varies by language, raising concerns about inconsistencies in Large Language Models (LLMs) for healthcare. In this study, we examine the consistency of responses provided by LLMs to health-related questions across English, German, Turkish, and Chinese. We largely expand the HealthFC dataset by categorizing health-related questions by disease type and broadening its multilingual scope with Turkish and Chinese translations. We reveal significant inconsistencies in responses that could spread healthcare misinformation. Our main contributions are 1) a multilingual health-related inquiry dataset with meta-information on disease categories, and 2) a novel prompt-based evaluation workflow that enables sub-dimensional comparisons between two languages through parsing. Our findings highlight key challenges in deploying LLM-based tools in multilingual contexts and emphasize the need for improved cross-lingual alignment to ensure accurate and equitable healthcare information. | Ipek Baris Schlicht, Zhixue Zhao, Burcu Sayin, Lucie Flek, Paolo Rosso |  |
|  |  [Benchmark Creation for Narrative Knowledge Delta Extraction Tasks: Can LLMs Help?](https://doi.org/10.1007/978-3-031-88714-7_32) |  | 0 |  | Alaa ElEbshihy, Annisa Maulida Ningtyas, Florina Piroi, Andreas Rauber |  |
|  |  [Passage Segmentation of Documents for Extractive Question Answering](https://doi.org/10.1007/978-3-031-88714-7_33) |  | 0 | Retrieval-Augmented Generation (RAG) has proven effective in open-domain question answering. However, the chunking process, which is essential to this pipeline, often receives insufficient attention relative to retrieval and synthesis components. This study emphasizes the critical role of chunking in improving the performance of both dense passage retrieval and the end-to-end RAG pipeline. We then introduce the Logits-Guided Multi-Granular Chunker (LGMGC), a novel framework that splits long documents into contextualized, self-contained chunks of varied granularity. Our experimental results, evaluated on two benchmark datasets, demonstrate that LGMGC not only improves the retrieval step but also outperforms existing chunking methods when integrated into a RAG pipeline. | Zuhong Liu, CharlesElie Simon, Fabien Caspani |  |
|  |  [A New Dataset for Keyword Extraction from IT Job Descriptions](https://doi.org/10.1007/978-3-031-88714-7_37) |  | 0 |  | Nisan Fichman, Hadar Isaacson, Natalia Vanetik |  |
|  |  [Entity-Aware Cross-Modal Pretraining for Knowledge-Based Visual Question Answering](https://doi.org/10.1007/978-3-031-88714-7_38) |  | 0 |  | Omar Adjali, Olivier Ferret, Sahar Ghannay, Hervé Le Borgne |  |
|  |  [Token-Level Graphs for Short Text Classification](https://doi.org/10.1007/978-3-031-88714-7_42) |  | 0 | The classification of short texts is a common subtask in Information Retrieval (IR). Recent advances in graph machine learning have led to interest in graph-based approaches for low resource scenarios, showing promise in such settings. However, existing methods face limitations such as not accounting for different meanings of the same words or constraints from transductive approaches. We propose an approach which constructs text graphs entirely based on tokens obtained through pre-trained language models (PLMs). By applying a PLM to tokenize and embed the texts when creating the graph(-nodes), our method captures contextual and semantic information, overcomes vocabulary constraints, and allows for context-dependent word meanings. Our approach also makes classification more efficient with reduced parameters compared to classical PLM fine-tuning, resulting in more robust training with few samples. Experimental results demonstrate how our method consistently achieves higher scores or on-par performance with existing methods, presenting an advancement in graph-based text classification techniques. To support reproducibility of our work we make all implementations publicly available to the community\footnote{\url{https://github.com/doGregor/TokenGraph}}. | Gregor Donabauer, Udo Kruschwitz |  |
|  |  [Checky, the Paper-Submission Checklist Generator for Authors, Reviewers and LLMs](https://doi.org/10.1007/978-3-031-88720-8_6) |  | 0 |  | Joeran Beel, Bela Gipp, Dietmar Jannach, Alan Said, Lukas Wegmeth, Tobias Vente |  |
|  |  [TheoremView: A Framework for Extracting Theorem-Like Environments from Raw PDFs](https://doi.org/10.1007/978-3-031-88720-8_5) |  | 0 |  | Shrey Mishra, Neil Sharma, Antoine Gauquier, Pierre Senellart |  |
|  |  [AirTOWN: A Privacy-Preserving Mobile App for Real-Time Pollution-Aware POI Suggestion](https://doi.org/10.1007/978-3-031-88720-8_7) |  | 0 | This demo paper presents , a privacy-preserving mobile application that provides real-time, pollution-aware recommendations for points of interest (POIs) in urban environments. By combining real-time Air Quality Index (AQI) data with user preferences, the proposed system aims to help users make health-conscious decisions about the locations they visit. The application utilizes collaborative filtering for personalized suggestions, and federated learning for privacy protection, and integrates AQI data from sensor networks in cities such as Bari, Italy, and Cork, UK. In areas with sparse sensor coverage, interpolation techniques approximate AQI values, ensuring broad applicability. This system offers a poromsing, health-oriented POI recommendation solution that adapts dynamically to current urban air quality conditions while safeguarding user privacy. | Giuseppe Fasano, Yashar Deldjoo, Tommaso Di Noia |  |
|  |  [Spoken Question Answering on Municipal Council Meetings](https://doi.org/10.1007/978-3-031-88720-8_8) |  | 0 |  | Pepijn van Wijk, Maarten Marx |  |
|  |  [Leveraging LLMs to Improve Human Annotation Efficiency with INCEpTION](https://doi.org/10.1007/978-3-031-88720-8_10) |  | 0 |  | Luís Filipe Cunha, Nana Yu, Purificação Silvano, Ricardo Campos, Alípio Jorge |  |
|  |  [Forecasting Prescription Efficacy](https://doi.org/10.1007/978-3-031-88720-8_14) |  | 0 |  | HaoRen Yao, Oskar Mencer, HanSun Chiang, DerChen Chang, Ophir Frieder |  |
|  |  [ASPIRE: Assistive System for Performance Evaluation in IR](https://doi.org/10.1007/978-3-031-88720-8_12) |  | 0 | Information Retrieval (IR) evaluation involves far more complexity than merely presenting performance measures in a table. Researchers often need to compare multiple models across various dimensions, such as the Precision-Recall trade-off and response time, to understand the reasons behind the varying performance of specific queries for different models. We introduce ASPIRE (Assistive System for Performance Evaluation in IR), a visual analytics tool designed to address these complexities by providing an extensive and user-friendly interface for in-depth analysis of IR experiments. ASPIRE supports four key aspects of IR experiment evaluation and analysis: single/multi-experiment comparisons, query-level analysis, query characteristics-performance interplay, and collection-based retrieval analysis. We showcase the functionality of ASPIRE using the TREC Clinical Trials collection. ASPIRE is an open-source toolkit available online: https://github.com/GiorgosPeikos/ASPIRE | Georgios Peikos, Wojciech Kusa, Symeon Symeonidis |  |
|  |  [Rebuilding the Past: Reconstructing Portuguese News Outlets with Web Archives](https://doi.org/10.1007/978-3-031-88720-8_15) |  | 0 |  | Rodrigo Silva, Ricardo Campos |  |
|  |  [PlotEdit: Natural Language-Driven Accessible Chart Editing in PDFs via Multimodal LLM Agents](https://doi.org/10.1007/978-3-031-88720-8_22) |  | 0 | Chart visualizations, while essential for data interpretation and communication, are predominantly accessible only as images in PDFs, lacking source data tables and stylistic information. To enable effective editing of charts in PDFs or digital scans, we present PlotEdit, a novel multi-agent framework for natural language-driven end-to-end chart image editing via self-reflective LLM agents. PlotEdit orchestrates five LLM agents: (1) Chart2Table for data table extraction, (2) Chart2Vision for style attribute identification, (3) Chart2Code for retrieving rendering code, (4) Instruction Decomposition Agent for parsing user requests into executable steps, and (5) Multimodal Editing Agent for implementing nuanced chart component modifications - all coordinated through multimodal feedback to maintain visual fidelity. PlotEdit outperforms existing baselines on the ChartCraft dataset across style, layout, format, and data-centric edits, enhancing accessibility for visually challenged users and improving novice productivity. | Kanika Goswami, Puneet Mathur, Ryan A. Rossi, Franck Dernoncourt |  |
|  |  [RURAGE: Robust Universal RAG Evaluator for Fast and Affordable QA Performance Testing](https://doi.org/10.1007/978-3-031-88720-8_23) |  | 0 |  | Nikita Krayko, Ivan Sidorov, Fedor Laputin, Alexander Panchenko, Daria Galimzianova, Vasily Konovalov |  |
|  |  [Leveraging LLMs for Energy Forecasting: The AcegasApsAmga Case Study](https://doi.org/10.1007/978-3-031-88720-8_25) |  | 0 |  | Kevin Roitero, Andrea Zancola, Vincenzo Della Mea, Stefano Mizzaro |  |
|  |  [Hierarchical Prefixes for Long Document Representations](https://doi.org/10.1007/978-3-031-88720-8_28) |  | 0 |  | Iskandar Boucharenc |  |
|  |  [SynKGP: Knowledge Graph Population with Syntactic-LLM Hybridation for Question-Answering](https://doi.org/10.1007/978-3-031-88720-8_34) |  | 0 |  | Eve Sauvage |  |
|  |  [Understanding Numerical Context by Asking Quantitative Questions](https://doi.org/10.1007/978-3-031-88720-8_35) |  | 0 |  | R. Gayathri, Koninika Pal |  |
|  |  [Fairness in Information Access Conceptual Foundations and New Directions](https://doi.org/10.1007/978-3-031-88720-8_41) |  | 0 |  | Michael D. Ekstrand |  |
|  |  [Enhancing Generative Models for Scientific Text Simplification](https://doi.org/10.1007/978-3-031-88720-8_39) |  | 0 |  | Benjamin Vendeville |  |
|  |  [Large Language Models Are Human-Like Annotators](https://doi.org/10.1007/978-3-031-88720-8_45) |  | 0 |  | Mounika Marreddy, Subba Reddy Oota, Manish Gupta |  |
|  |  [Rapid Prototyping for AI-Based Applications: A Hands-on Tutorial for Connecting the Dots](https://doi.org/10.1007/978-3-031-88720-8_46) |  | 0 |  | Omar Alonso, Kenneth Church |  |
|  |  [ELOQUENT CLEF Shared Tasks for Evaluation of Generative Language Model Quality, 2025 Edition](https://doi.org/10.1007/978-3-031-88720-8_56) |  | 0 |  | Jussi Karlgren, Ekaterina Artemova, Ondrej Bojar, Vladislav Mikhailov, Magnus Sahlgren, Erik Velldal, Lilja Øvrelid |  |
|  |  [LongEval at CLEF 2025: Longitudinal Evaluation of IR Model Performance](https://doi.org/10.1007/978-3-031-88720-8_58) |  | 0 | This paper presents the third edition of the LongEval Lab, part of the CLEF 2025 conference, which continues to explore the challenges of temporal persistence in Information Retrieval (IR). The lab features two tasks designed to provide researchers with test data that reflect the evolving nature of user queries and document relevance over time. By evaluating how model performance degrades as test data diverge temporally from training data, LongEval seeks to advance the understanding of temporal dynamics in IR systems. The 2025 edition aims to engage the IR and NLP communities in addressing the development of adaptive models that can maintain retrieval quality over time in the domains of web search and scientific retrieval. | Matteo Cancellieri, Alaa ElEbshihy, Tobias Fink, Petra Galuscáková, Gabriela González Sáez, Lorraine Goeuriot, David Iommi, Jüri Keller, Petr Knoth, Philippe Mulhem, Florina Piroi, David Pride, Philipp Schaer |  |
|  |  [LifeCLEF 2025 Teaser: Challenges on Species Presence Prediction and Identification, and Individual Animal Identification](https://doi.org/10.1007/978-3-031-88720-8_57) |  | 0 |  | Alexis Joly, Lukás Picek, Stefan Kahl, Hervé Goëau, Lukás Adam, Christophe Botella, Maximilien Servajean, Diego Marcos, César Leblanc, Théo Larcher, Jirí Matas, Klára Janousková, Vojtech Cermák, Kostas Papafitsoros, Robert Planqué, WillemPier Vellinga, Holger Klinck, Tom Denton, Pierre Bonnet, Henning Müller |  |
|  |  [CLEF 2025 JOKER Lab: Humour in the Machine](https://doi.org/10.1007/978-3-031-88720-8_59) |  | 0 |  | Liana Ermakova, AnneGwenn Bosser, Tristan Miller, Ricardo Campos |  |
|  |  [CLEF 2025 SimpleText Track - Simplify Scientific Text (and Nothing More)](https://doi.org/10.1007/978-3-031-88720-8_63) |  | 0 |  | Liana Ermakova, Hosein Azarbonyad, Jan Bakker, Benjamin Vendeville, Jaap Kamps |  |
|  |  [Overview of PAN 2025: Generative AI Detection, Multilingual Text Detoxification, Multi-author Writing Style Analysis, and Generative Plagiarism Detection - Extended Abstract](https://doi.org/10.1007/978-3-031-88720-8_64) |  | 0 |  | Janek Bevendorff, Daryna Dementieva, Maik Fröbe, Bela Gipp, André GreinerPetter, Jussi Karlgren, Maximilian Mayerl, Preslav Nakov, Alexander Panchenko, Martin Potthast, Artem Shelmanov, Efstathios Stamatatos, Benno Stein, Yuxia Wang, Matti Wiegmann, Eva Zangerle |  |
|  |  [EXIST 2025: Learning with Disagreement for Sexism Identification and Characterization in Tweets, Memes, and TikTok Videos](https://doi.org/10.1007/978-3-031-88720-8_65) |  | 0 |  | Laura Plaza, Jorge CarrillodeAlbornoz, Iván Árcos, Paolo Rosso, Damiano Spina, Enrique Amigó, Julio Gonzalo, Roser Morante |  |
|  |  [QuantumCLEF 2025 - The Second Edition of the Quantum Computing Lab at CLEF](https://doi.org/10.1007/978-3-031-88720-8_66) |  | 0 |  | Andrea Pasin, Maurizio Ferrari Dacrema, Paolo Cremonesi, Washington Cunha, Marcos André Gonçalves, Nicola Ferro |  |
|  |  [Overview of Touché 2025: Argumentation Systems - Extended Abstract](https://doi.org/10.1007/978-3-031-88720-8_67) |  | 0 |  | Johannes Kiesel, Çagri Çöltekin, Marcel Gohsen, Sebastian Heineking, Maximilian Heinrich, Maik Fröbe, Tim Hagen, Mohammad Aliannejadi, Tomaz Erjavec, Matthias Hagen, Matyás Kopp, Nikola Ljubesic, Katja Meden, Nailia Mirzakhmedova, Vaidas Morkevicius, Harrisen Scells, Ines Zelch, Martin Potthast, Benno Stein |  |
|  |  [TalentCLEF at CLEF2025: Skill and Job Title Intelligence for Human Capital Management](https://doi.org/10.1007/978-3-031-88720-8_69) |  | 0 |  | Luis Gascó, Hermenegildo Fabregat, Laura GarcíaSardiña, Daniel Deniz, Álvaro Rodrigo, Paula Estrella, Rabih Zbib |  |
|  |  [Patent Figure Classification Using Large Vision-Language Models](https://doi.org/10.1007/978-3-031-88711-6_2) |  | 0 | Patent figure classification facilitates faceted search in patent retrieval systems, enabling efficient prior art search. Existing approaches have explored patent figure classification for only a single aspect and for aspects with a limited number of concepts. In recent years, large vision-language models (LVLMs) have shown tremendous performance across numerous computer vision downstream tasks, however, they remain unexplored for patent figure classification. Our work explores the efficacy of LVLMs in patent figure visual question answering (VQA) and classification, focusing on zero-shot and few-shot learning scenarios. For this purpose, we introduce new datasets, PatFigVQA and PatFigCLS, for fine-tuning and evaluation regarding multiple aspects of patent figures~(i.e., type, projection, patent class, and objects). For a computational-effective handling of a large number of classes using LVLM, we propose a novel tournament-style classification strategy that leverages a series of multiple-choice questions. Experimental results and comparisons of multiple classification approaches based on LVLMs and Convolutional Neural Networks (CNNs) in few-shot settings show the feasibility of the proposed approaches. | Sushil Awale, Eric MüllerBudack, Ralph Ewerth |  |
|  |  [Semi-Supervised Image-Based Narrative Extraction: A Case Study with Historical Photographic Records](https://doi.org/10.1007/978-3-031-88711-6_16) |  | 0 | This paper presents a semi-supervised approach to extracting narratives from historical photographic records using an adaptation of the narrative maps algorithm. We extend the original unsupervised text-based method to work with image data, leveraging deep learning techniques for visual feature extraction and similarity computation. Our method is applied to the ROGER dataset, a collection of photographs from the 1928 Sacambaya Expedition in Bolivia captured by Robert Gerstmann. We compare our algorithmically extracted visual narratives with expert-curated timelines of varying lengths (5 to 30 images) to evaluate the effectiveness of our approach. In particular, we use the Dynamic Time Warping (DTW) algorithm to match the extracted narratives with the expert-curated baseline. In addition, we asked an expert on the topic to qualitatively evaluate a representative example of the resulting narratives. Our findings show that the narrative maps approach generally outperforms random sampling for longer timelines (10+ images, p < 0.05), with expert evaluation confirming the historical accuracy and coherence of the extracted narratives. This research contributes to the field of computational analysis of visual cultural heritage, offering new tools for historians, archivists, and digital humanities scholars to explore and understand large-scale image collections. The method's ability to generate meaningful narratives from visual data opens up new possibilities for the study and interpretation of historical events through photographic evidence. | Fausto German, Brian Keith, Mauricio Matus, Diego Urrutia, Claudio Meneses |  |
|  |  [Visual Latent Captioning - Towards Verbalizing Vision Transformer Encoders](https://doi.org/10.1007/978-3-031-88711-6_25) |  | 0 |  | Sogol Haghighat, Tim Daniel Metzler, Santosh Thoduka, Sebastian Houben |  |
