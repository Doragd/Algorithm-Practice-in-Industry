# ECIR2024

## 会议论文列表

本会议共有 200 篇论文

| 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- |
|  |  [Incorporating Query Recommendation for Improving In-Car Conversational Search](https://doi.org/10.1007/978-3-031-56069-9_36) |  | 0 | Retrieval-augmented generation has become an effective mechanism for conversational systems in domain-specific settings. Retrieval of a wrong document due to the lack of context from the user utterance may lead to wrong answer generation. Such an issue may reduce the user engagement and thereby the system reliability. In this paper, we propose a context-guided follow-up question recommendation to internally improve the document retrieval in an iterative approach for developing an in-car conversational system. Specifically, a user utterance is first reformulated, given the context of the conversation to facilitate improved understanding to the retriever. In the cases, where the documents retrieved by the retriever are not relevant enough for answering the user utterance, we employ a large language model (LLM) to generate question recommendation which is then utilized to perform a refined retrieval. An empirical evaluation confirms the effectiveness of our proposed approaches in in-car conversations, achieving 48% and 22% improvement in the retrieval and system generated responses, respectively, against baseline approaches. | Md. Rashad Al Hasan Rony, Soumya Ranjan Sahoo, Abbas Goher Khan, Ken E. Friedl, Viju Sudhi, Christian Süß | BMW Grp, Parkring 19-23, D-85748 Garching, Germany; Fraunhofer IAIS, Zwickauer Str 46, D-01069 Dresden, Germany |
|  |  [ChatGPT Goes Shopping: LLMs Can Predict Relevance in eCommerce Search](https://doi.org/10.1007/978-3-031-56066-8_1) |  | 0 | The dependence on human relevance judgments limits the development of information retrieval test collections that are vital for evaluating these systems. Since their launch, large language models (LLMs) have been applied to automate several human tasks. Recently, LLMs started being used to provide relevance judgments for document search. In this work, our goal is to assess whether LLMs can replace human annotators in a different setting - product search in eCommerce. We conducted experiments on open and proprietary industrial datasets tomeasure LLM's ability to predict relevance judgments. Our results found that LLM-generated relevance assessments present a strong agreement (similar to 82%) with human annotations indicating that LLMs have an innate ability to perform relevance judgments in an eCommerce setting. Then, we went further and tested whether LLMs can generate annotation guidelines. Our results found that relevance assessments obtained with LLM-generated guidelines are as accurate as the ones obtained from human instructions.(1)(The source code for this work is available at https://github.com/danimtk/chatGPT-goes-shopping) | Beatriz Soviero, Daniel Kuhn, Alexandre Salle, Viviane Pereira Moreira | Inst Educ Sci & Technol Rio Grande do Sul IFRS, Ibiruba, Brazil; VTEX, Porto Alegre, RS, Brazil; Univ Fed Rio Grande do Sul, Inst Informat, Porto Alegre, RS, Brazil |
|  |  [Lottery4CVR: Neuron-Connection Level Sharing for Multi-task Learning in Video Conversion Rate Prediction](https://doi.org/10.1007/978-3-031-56069-9_31) |  | 0 | As a fundamental task of industrial ranking systems, conversion rate (CVR) prediction is suffering from data sparsity problems. Most conventional CVR modeling leverages Click-through rate (CTR)&CVR multitask learning because CTR involves far more samples than CVR. However, typical coarse-grained layer-level sharing methods may introduce conflicts and lead to performance degradation, since not every neuron or neuron connection in one layer should be shared between CVR and CTR tasks. This is because users may have different fine-grained content feature preferences between deep consumption and click behaviors, represented by CVR and CTR, respectively. To address this sharing&conflict problem, we propose a neuron-connection level knowledge sharing. We start with an over-parameterized base network from which CVR and CTR extract their own subnetworks. The subnetworks have partially overlapped neuron connections which correspond to the sharing knowledge, and the task-specific neuron connections are utilized to alleviate the conflict problem. As far as we know, this is the first time that a neuron-connection level sharing is proposed in CVR modeling. Experiments on the Tencent video platform demonstrate the superiority of the method, which has been deployed serving major traffic. (The source code is available at https://github.com/xuanjixiao/onerec/tree/main/lt4rec). | Xuanji Xiao, Jimmy Chen, Yuzhen Liu, Xing Yao, Pei Liu, Chaosheng Fan | Tencent Inc, Beijing, Peoples R China |
|  |  [Utilizing Low-Dimensional Molecular Embeddings for Rapid Chemical Similarity Search](https://doi.org/10.1007/978-3-031-56060-6_3) |  | 0 | Nearest neighbor-based similarity searching is a common task in chemistry, with notable use cases in drug discovery. Yet, some of the most commonly used approaches for this task still leverage a brute-force approach. In practice this can be computationally costly and overly time-consuming, due in part to the sheer size of modern chemical databases. Previous computational advancements for this task have generally relied on improvements to hardware or dataset-specific tricks that lack generalizability. Approaches that leverage lower-complexity searching algorithms remain relatively underexplored. However, many of these algorithms are approximate solutions and/or struggle with typical high-dimensional chemical embeddings. Here we evaluate whether a combination of low-dimensional chemical embeddings and a k-d tree data structure can achieve fast nearest neighbor queries while maintaining performance on standard chemical similarity search benchmarks. We examine different dimensionality reductions of standard chemical embeddings as well as a learned, structurally-aware embedding-SmallSA-for this task. With this framework, searches on over one billion chemicals execute in less than a second on a single CPU core, five orders of magnitude faster than the brute-force approach. We also demonstrate that SmallSA achieves competitive performance on chemical similarity benchmarks. | Kathryn E. Kirchoff, James Wellnitz, Joshua E. Hochuli, Travis Maxfield, Konstantin I. Popov, Shawn M. Gomez, Alexander Tropsha | Department of Pharmacology, UNC Chapel Hill.; Department of Computer Science, UNC Chapel Hill.; Eshelman School of Pharmacy, UNC Chapel Hill. |
|  |  [Evaluating the Impact of Content Deletion on Tabular Data Similarity and Retrieval Using Contextual Word Embeddings](https://doi.org/10.1007/978-3-031-56060-6_28) |  | 0 | Table retrieval involves providing a ranked list of relevant tables in response to a search query. A critical aspect of this process is computing the similarity between tables. Recent Transformer-based language models have been effectively employed to generate word embedding representations of tables for assessing their semantic similarity. However, generating such representations for large tables comprising thousands or even millions of rows can be computationally intensive. This study presents the hypothesis that a significant portion of a table's content (i.e., rows) can be removed without substantially impacting its word embedding representation, thereby reducing computational costs while maintaining system performance. To test this hypothesis, two distinct evaluations were conducted. Firstly, an intrinsic evaluation was carried out using two different datasets and five state-of-the-art contextual and not-contextual language models. The findings indicate that, for large tables, retaining just 5% of the content results in a word embedding representation that is 90% similar to the original one. Secondly, an extrinsic evaluation was performed to assess how three different reduction techniques proposed affects the overall performance of the table-based query retrieval system, as measured by MAP, precision, and nDCG. The results demonstrate that these techniques can not only decrease data volume but also improve the performance of the table retrieval system. | Alberto Berenguer, David Tomás, JoseNorberto Mazón |  |
|  |  [RIGHT: Retrieval-Augmented Generation for Mainstream Hashtag Recommendation](https://doi.org/10.1007/978-3-031-56027-9_3) |  | 0 | Automatic mainstream hashtag recommendation aims to accurately provide users with concise and popular topical hashtags before publication. Generally, mainstream hashtag recommendation faces challenges in the comprehensive difficulty of newly posted tweets in response to new topics, and the accurate identification of mainstream hashtags beyond semantic correctness. However, previous retrieval-based methods based on a fixed predefined mainstream hashtag list excel in producing mainstream hashtags, but fail to understand the constant flow of up-to-date information. Conversely, generation-based methods demonstrate a superior ability to comprehend newly posted tweets, but their capacity is constrained to identifying mainstream hashtags without additional features. Inspired by the recent success of the retrieval-augmented technique, in this work, we attempt to adopt this framework to combine the advantages of both approaches. Meantime, with the help of the generator component, we could rethink how to further improve the quality of the retriever component at a low cost. Therefore, we propose RetrIeval-augmented Generative Mainstream HashTag Recommender (RIGHT), which consists of three components: 1) a retriever seeks relevant hashtags from the entire tweet-hashtags set; 2) a selector enhances mainstream identification by introducing global signals; and 3) a generator incorporates input tweets and selected hashtags to directly generate the desired hashtags. The experimental results show that our method achieves significant improvements over state-of-the-art baselines. Moreover, RIGHT can be easily integrated into large language models, improving the performance of ChatGPT by more than 10%. | RunZe Fan, Yixing Fan, Jiangui Chen, Jiafeng Guo, Ruqing Zhang, Xueqi Cheng |  |
|  |  [Exploring the Nexus Between Retrievability and Query Generation Strategies](https://doi.org/10.1007/978-3-031-56066-8_16) |  | 0 | Quantifying bias in retrieval functions through document retrievability scores is vital for assessing recall-oriented retrieval systems. However, many studies investigating retrieval model bias lack validation of their query generation methods as accurate representations of retrievability for real users and their queries. This limitation results from the absence of established criteria for query generation in retrievability assessments. Typically, researchers resort to using frequent collocations from document corpora when no query log is available. In this study, we address the issue of reproducibility and seek to validate query generation methods by comparing retrievability scores generated from artificially generated queries to those derived from query logs. Our findings demonstrate a minimal or negligible correlation between retrievability scores from artificial queries and those from query logs. This suggests that artificially generated queries may not accurately reflect retrievability scores as derived from query logs. We further explore alternative query generation techniques, uncovering a variation that exhibits the highest correlation. This alternative approach holds promise for improving reproducibility when query logs are unavailable. | Aman Sinha, Priyanshu Raj Mall, Dwaipayan Roy |  |
|  |  [GLAD: Graph-Based Long-Term Attentive Dynamic Memory for Sequential Recommendation](https://doi.org/10.1007/978-3-031-56063-7_5) |  | 0 | Recommender systems play a crucial role in the e-commerce stores, enabling customers to explore products and facilitating the discovery of relevant items. Typical recommender systems are built using n most recent user interactions, where value of n is chosen based on trade-off between incremental gains in performance and compute/memory costs associated with processing long sequences. State-of-the-art recommendation models like Transformers, based on attention mechanism, have quadratic computation complexity with respect to sequence length, thus limiting the length of past customer interactions to be considered for recommendations. Even with the availability of compute resources, it is crucial to design an algorithm that strikes delicate balance between long term and short term information in identifying relevant products for personalised recommendation. Towards this, we propose a novel extension of Memory Networks, a neural network architecture that harnesses external memory to encapsulate information present in lengthy sequential data. The use of memory networks in recommendation use-cases remains limited in practice owing to their high memory cost, large compute requirements and relatively large inference latency, which makes them prohibitively expensive for online stores with millions of users and products. To address these limitations, we propose a novel transformer-based sequential recommendation model GLAD, with external graph-based memory that dynamically scales user memory by adjusting the memory size according to the user's history, while facilitating the flow of information between users with similar interactions. We establish the efficacy of the proposed model by benchmarking on multiple public datasets as well as an industry dataset against state-of-the-art sequential recommendation baselines. | Deepanshu Pandey, Arindam Sarkar, Prakash Mandayam Comar | Amazon Dev Ctr, Bengaluru, India |
|  |  [BertPE: A BERT-Based Pre-retrieval Estimator for Query Performance Prediction](https://doi.org/10.1007/978-3-031-56063-7_27) |  | 0 | Query Performance Prediction (QPP) aims to estimate the effectiveness of a query in addressing the underlying information need without any relevance judgments. More recent works in this area have employed the pre-trained neural embedding representations of the query to go beyond the corpus statistics of query terms and capture the semantics of the query. In this paper, we propose a supervised QPP method by adopting contextualized neural embeddings to directly learn the performance through fine-tuning. To address the challenges arising from disparities in the evaluation of retrieval models through sparse and comprehensive labels, we introduce an innovative strategy for creating synthetic relevance judgments to enable effective performance prediction for queries, irrespective of whether they are evaluated with sparse or more comprehensive labels. Through our experiments on four different query sets accompanied by MS MARCO V1 collection, we show that our approach shows significantly improved performance compared to the state-of-the-art Pre-retrieval QPP methods. | Maryam Khodabakhsh, Fattane Zarrinkalam, Negar Arabzadeh | Shahrood Univ Technol, Shahrood, Iran; Univ Waterloo, Waterloo, ON, Canada; Univ Guelph, Guelph, ON, Canada |
|  |  [Estimating the Usefulness of Clarifying Questions and Answers for Conversational Search](https://doi.org/10.1007/978-3-031-56063-7_30) |  | 0 | While the body of research directed towards constructing and generating clarifying questions in mixed-initiative conversational search systems is vast, research aimed at processing and comprehending users' answers to such questions is scarce. To this end, we present a simple yet effective method for processing answers to clarifying questions, moving away from previous work that simply appends answers to the original query and thus potentially degrades retrieval performance. Specifically, we propose a classifier for assessing usefulness of the prompted clarifying question and an answer given by the user. Useful questions or answers are further appended to the conversation history and passed to a transformer-based query rewriting module. Results demonstrate significant improvements over strong non-mixed-initiative baselines. Furthermore, the proposed approach mitigates the performance drops when non useful questions and answers are utilized. | Ivan Sekulic, Weronika Lajewska, Krisztian Balog, Fabio Crestani |  |
|  |  [Measuring Bias in Search Results Through Retrieval List Comparison](https://doi.org/10.1007/978-3-031-56069-9_2) |  | 0 | Many IR systems project harmful societal biases, including gender bias, in their retrieved contents. Uncovering and addressing such biases requires grounded bias measurement principles. However, defining reliable bias metrics for search results is challenging, particularly due to the difficulties in capturing gender-related tendencies in the retrieved documents. In this work, we propose a new framework for search result bias measurement. Within this framework, we first revisit the current metrics for representative search result bias (RepSRB) that are based on the occurrence of gender-specific language in the search results. Addressing their limitations, we additionally propose a metric for comparative search result bias (ComSRB) measurement and integrate it into our framework. ComSRB defines bias as the skew in the set of retrieved documents in response to a non-gendered query toward those for male/female-specific variations of the same query. We evaluate ComSRB against RepSRB on a recent collection of bias-sensitive topics and documents from the MS MARCO collection, using pre-trained bi-encoder and cross-encoder IR models. Our analyses show that, while existing metrics are highly sensitive to the wordings and linguistic formulations, the proposed ComSRB metric mitigates this issue by focusing on the deviations of a retrieval list from its explicitly biased variants, avoiding the need for sub-optimal content analysis processes. | Linda Ratz, Markus Schedl, Simone Kopeinik, Navid Rekabsaz | Johannes Kepler Univ Linz, Linz, Austria; Know Ctr GmbH, Graz, Austria |
|  |  [Cascading Ranking Pipelines for Sensitivity-Aware Search](https://doi.org/10.1007/978-3-031-56069-9_41) |  | 0 | Search engines are designed to make information accessible. However, some information should not be accessible, such as documents concerning citizenship applications or personal information. This sensitive information is often found interspersed with other potentially useful non-sensitive information. As such, collections containing sensitive information cannot be made searchable due to the risk of revealing sensitive information. The development of search engines capable of safely searching collections containing sensitive information to provide relevant and non-sensitive information would allow previously hidden collections to be made available. This work aims to develop sensitivity-aware search engines via two-stage cascading retrieval pipelines. | Jack McKechnie | Univ Glasgow, Glasgow, Lanark, Scotland |
|  |  [Advancing Multimedia Retrieval in Medical, Social Media and Content Recommendation Applications with ImageCLEF 2024](https://doi.org/10.1007/978-3-031-56072-9_6) |  | 0 | The ImageCLEF evaluation campaign was integrated with CLEF (Conference and Labs of the Evaluation Forum) for more than 20 years and represents a Multimedia Retrieval challenge aimed at evaluating the technologies for annotation, indexing, and retrieval of multimodal data. Thus, it provides information access to large data collections in usage scenarios and domains such as medicine, argumentation and content recommendation. ImageCLEF 2024 has four main tasks: (i) a Medical task targeting automatic image captioning for radiology images, synthetic medical images created with Generative Adversarial Networks (GANs), Visual Question Answering and medical image generation based on text input, and multimodal dermatology response generation; (ii) a joint ImageCLEF-Touché task Image Retrieval/Generation for Arguments to convey the premise of an argument, (iii) a Recommending task addressing cultural heritage content-recommendation, and (iv) a joint ImageCLEF-ToPicto task aiming to provide a translation in pictograms from natural language. In 2023, participation increased by 67% with respect to 2022 which reveals its impact on the community. | Bogdan Ionescu, Henning Müller, AnaMaria Claudia Dragulinescu, Ahmad IdrissiYaghir, Ahmedkhan Radzhabov, Alba Garcia Seco de Herrera, Alexandra Andrei, Alexandru Stan, Andrea M. Storås, Asma Ben Abacha, Benjamin Lecouteux, Benno Stein, Cécile Macaire, Christoph M. Friedrich, Cynthia S. Schmidt, Didier Schwab, Emmanuelle EsperançaRodier, George Ioannidis, Griffin Adams, Henning Schäfer, Hugo Manguinhas, Ioan Coman, Johanna Schöler, Johannes Kiesel, Johannes Rückert, Louise Bloch, Martin Potthast, Maximilian Heinrich, Meliha Yetisgen, Michael A. Riegler, Neal Snider, Pål Halvorsen, Raphael Brüngel, Steven Alexander Hicks, Vajira Thambawita, Vassili Kovalev, Yuri Prokopchuk, Wenwai Yim | Natl Univ Sci & Technol Politehn Bucharest, Bucharest, Romania; Univ Appl Sci Western Switzerland HES SO, Sierre, Switzerland; CEA, LIST, Paris, France |
|  |  [Ranking Heterogeneous Search Result Pages Using the Interactive Probability Ranking Principle](https://doi.org/10.1007/978-3-031-56060-6_7) |  | 0 | The Probability Ranking Principle (PRP) ranks search results based on their expected utility derived solely from document contents, often overlooking the nuances of presentation and user interaction. However, with the evolution of Search Engine Result Pages (SERPs), now comprising a variety of result cards, the manner in which these results are presented is pivotal in influencing user engagement and satisfaction. This shift prompts the question: How does the PRP and its user-centric counterpart, the Interactive Probability Ranking Principle (iPRP), compare in the context of these heterogeneous SERPs? Our study draws a comparison between the PRP and the iPRP, revealing significant differences in their output. The iPRP, accounting for item-specific costs and interaction probabilities to determine the “Expected Perceived Utility" (EPU), yields different result orderings compared to the PRP. We evaluate the effect of the EPU on the ordering of results by observing changes in the ranking within a heterogeneous SERP compared to the traditional “ten blue links”. We find that changing the presentation affects the ranking of items according to the (iPRP) by up to 48% (with respect to DCG, TBG and RBO) in ad-hoc search tasks on the TREC WaPo Collection. This work suggests that the iPRP should be employed when ranking heterogeneous SERPs to provide a user-centric ranking that adapts the ordering based on the presentation and user engagement. | Kanaad Pathak, Leif Azzopardi, Martin Halvey |  |
|  |  [Query Exposure Prediction for Groups of Documents in Rankings](https://doi.org/10.1007/978-3-031-56060-6_10) |  | 0 | The main objective of an Information Retrieval system is to provide a user with the most relevant documents to the user's query. To do this, modern IR systems typically deploy a re-ranking pipeline in which a set of documents is retrieved by a lightweight first-stage retrieval process and then re-ranked by a more effective but expensive model. However, the success of a re-ranking pipeline is heavily dependent on the performance of the first stage retrieval, since new documents are not usually identified during the re-ranking stage. Moreover, this can impact the amount of exposure that a particular group of documents, such as documents from a particular demographic group, can receive in the final ranking. For example, the fair allocation of exposure becomes more challenging or impossible if the first stage retrieval returns too few documents from certain groups, since the number of group documents in the ranking affects the exposure more than the documents' positions. With this in mind, it is beneficial to predict the amount of exposure that a group of documents is likely to receive in the results of the first stage retrieval process, in order to ensure that there are a sufficient number of documents included from each of the groups. In this paper, we introduce the novel task of query exposure prediction (QEP). Specifically, we propose the first approach for predicting the distribution of exposure that groups of documents will receive for a given query. Our new approach, called GEP, uses lexical information from individual groups of documents to estimate the exposure the groups will receive in a ranking. Our experiments on the TREC 2021 and 2022 Fair Ranking Track test collections show that our proposed GEP approach results in exposure predictions that are up to 40 of adapted existing query performance prediction and resource allocation approaches. | Thomas Jänich, Graham McDonald, Iadh Ounis |  |
|  |  [Investigating the Robustness of Sequential Recommender Systems Against Training Data Perturbations](https://doi.org/10.1007/978-3-031-56060-6_14) |  | 0 | Sequential Recommender Systems (SRSs) have been widely used to model user behavior over time, but their robustness in the face of perturbations to training data is a critical issue. In this paper, we conduct an empirical study to investigate the effects of removing items at different positions within a temporally ordered sequence. We evaluate two different SRS models on multiple datasets, measuring their performance using Normalized Discounted Cumulative Gain (NDCG) and Rank Sensitivity List metrics. Our results demonstrate that removing items at the end of the sequence significantly impacts performance, with NDCG decreasing up to 60\%, while removing items from the beginning or middle has no significant effect. These findings highlight the importance of considering the position of the perturbed items in the training data and shall inform the design of more robust SRSs. | Filippo Betello, Federico Siciliano, Pushkar Mishra, Fabrizio Silvestri |  |
|  |  [Conversational Search with Tail Entities](https://doi.org/10.1007/978-3-031-56060-6_20) |  | 0 | Conversational search faces incomplete and informal follow-up questions. Prior works address these by contextualizing user utterances with cues derived from the previous turns of the conversation. This approach works well when the conversation centers on prominent entities, for which knowledge bases (KBs) or language models (LMs) can provide rich background. This work addresses the unexplored direction where user questions are about tail entities, not featured in KBs and sparsely covered by LMs. We devise a new method, called CONSENT, for selectively contextualizing a user utterance with turns, KB-linkable entities, and mentions of tail and out-of-KB (OKB) entities. CONSENT derives relatedness weights from Sentence-BERT similarities and employs an integer linear program (ILP) for judiciously selecting the best context cues for a given set of candidate answers. This method couples the contextualization and answer-ranking stages, and jointly infers the best choices for both. | Hai Dang Tran, Andrew Yates, Gerhard Weikum | Max Planck Inst Informat, Saarbrucken, Germany |
|  |  [Event-Specific Document Ranking Through Multi-stage Query Expansion Using an Event Knowledge Graph](https://doi.org/10.1007/978-3-031-56060-6_22) |  | 0 | Event-specific document ranking is a crucial task in supporting users when searching for texts covering events such as Brexit or the Olympics. However, the complex nature of events involving multiple aspects like temporal information, location, participants and sub-events poses challenges in effectively modelling their representations for ranking. In this paper, we propose MusQuE (Multi-stage Query Expansion), a multi-stage ranking framework that jointly learns to rank query expansion terms and documents, and in this manner flexibly identifies the optimal combination and number of expansion terms extracted from an event knowledge graph. Experimental results show that MusQuE outperforms state-of-the-art baselines on MS-MARCO EVENT , a new dataset for event-specific document ranking, by 9.1 % and more. | Sara Abdollahi, Tin Kuculo, Simon Gottschalk | Leibniz Univ Hannover, Res Ctr L3S, Hannover, Germany |
|  |  [Large Language Models are Zero-Shot Rankers for Recommender Systems](https://doi.org/10.1007/978-3-031-56060-6_24) |  | 0 | Recently, large language models (LLMs) (e.g. GPT-4) have demonstrated impressive general-purpose task-solving abilities, including the potential to approach recommendation tasks. Along this line of research, this work aims to investigate the capacity of LLMs that act as the ranking model for recommender systems. To conduct our empirical study, we first formalize the recommendation problem as a conditional ranking task, considering sequential interaction histories as conditions and the items retrieved by the candidate generation model as candidates. We adopt a specific prompting approach to solving the ranking task by LLMs: we carefully design the prompting template by including the sequential interaction history, the candidate items, and the ranking instruction. We conduct extensive experiments on two widely-used datasets for recommender systems and derive several key findings for the use of LLMs in recommender systems. We show that LLMs have promising zero-shot ranking abilities, even competitive to or better than conventional recommendation models on candidates retrieved by multiple candidate generators. We also demonstrate that LLMs struggle to perceive the order of historical interactions and can be affected by biases like position bias, while these issues can be alleviated via specially designed prompting and bootstrapping strategies. The code to reproduce this work is available at https://github.com/RUCAIBox/LLMRank. | Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian J. McAuley, Wayne Xin Zhao |  |
|  |  [Simulating Follow-Up Questions in Conversational Search](https://doi.org/10.1007/978-3-031-56060-6_25) |  | 0 | Evaluating conversational search systems based on simulated user interactions is a potential approach to overcome one of the main problems of static conversational search test collections: the collections contain only very few of all the plausible conversations on a topic. Still, one of the challenges of user simulation is generating realistic follow-up questions on given outputs of a conversational system. We propose to address this challenge by using state-of-the-art language models and find that: (1) on two conversational search datasets, the tested models generate questions that are semantically similar to those in the datasets, especially when tuned for follow-up questions; (2) the generated questions are mostly valid, related, informative, and specific according to human assessment; and (3) for influencing the characteristics of the simulated questions, small changes to the prompt are insufficient. | Johannes Kiesel, Marcel Gohsen, Nailia Mirzakhmedova, Matthias Hagen, Benno Stein | Friedrich Schiller Univ Jena, Ernst Abbe Pl 2, D-07743 Jena, Germany; Bauhaus Univ Weimar, Bauhausstr 9a, D-99423 Weimar, Germany |
|  |  [MOReGIn: Multi-Objective Recommendation at the Global and Individual Levels](https://doi.org/10.1007/978-3-031-56027-9_2) |  | 0 | Multi-Objective Recommender Systems (MORSs) emerged as a paradigm to guarantee multiple (often conflicting) goals. Besides accuracy, a MORS can operate at the global level, where additional beyond-accuracy goals are met for the system as a whole, or at the individual level, meaning that the recommendations are tailored to the needs of each user. The state-of-the-art MORSs either operate at the global or individual level, without assuming the co-existence of the two perspectives. In this study, we show that when global and individual objectives co-exist, MORSs are not able to meet both types of goals. To overcome this issue, we present an approach that regulates the recommendation lists so as to guarantee both global and individual perspectives, while preserving its effectiveness. Specifically, as individual perspective, we tackle genre calibration and, as global perspective, provider fairness. We validate our approach on two real-world datasets, publicly released with this paper. | Elizabeth Gómez, David Contreras, Ludovico Boratto, Maria Salamó |  |
|  |  [VEMO: A Versatile Elastic Multi-modal Model for Search-Oriented Multi-task Learning](https://doi.org/10.1007/978-3-031-56027-9_4) |  | 0 | Cross-modal search is one fundamental task in multi-modal learning, but there is hardly any work that aims to solve multiple cross-modal search tasks at once. In this work, we propose a novel Versatile Elastic Multi-mOdal (VEMO) model for search-oriented multi-task learning. VEMO is versatile because we integrate cross-modal semantic search, named entity recognition, and scene text spotting into a unified framework, where the latter two can be further adapted to entity- and character-based image search tasks. VEMO is also elastic because we can freely assemble sub-modules of our flexible network architecture for corresponding tasks. Moreover, to give more choices on the effect-efficiency trade-off when performing cross-modal semantic search, we place multiple encoder exits. Experimental results show the effectiveness of our VEMO with only 37.6% network parameters compared to those needed for uni-task training. Further evaluations on entity- and character-based image search tasks also validate the superiority of search-oriented multi-task learning. | Nanyi Fei, Hao Jiang, Haoyu Lu, Jinqiang Long, Yanqi Dai, Tuo Fan, Zhao Cao, Zhiwu Lu | Renmin Univ China, Gaoling Sch Artificial Intelligence, Beijing, Peoples R China; Huawei Poisson Lab, Hangzhou, Zhejiang, Peoples R China; Renmin Univ China, Sch Informat, Beijing, Peoples R China |
|  |  [Lightweight Modality Adaptation to Sequential Recommendation via Correlation Supervision](https://doi.org/10.1007/978-3-031-56027-9_8) |  | 0 | In Sequential Recommenders (SR), encoding and utilizing modalities in an end-to-end manner is costly in terms of modality encoder sizes. Two-stage approaches can mitigate such concerns, but they suffer from poor performance due to modality forgetting, where the sequential objective overshadows modality representation. We propose a lightweight knowledge distillation solution that preserves both merits: retaining modality information and maintaining high efficiency. Specifically, we introduce a novel method that enhances the learning of embeddings in SR through the supervision of modality correlations. The supervision signals are distilled from the original modality representations, including both (1) holistic correlations, which quantify their overall associations, and (2) dissected correlation types, which refine their relationship facets (honing in on specific aspects like color or shape consistency). To further address the issue of modality forgetting, we propose an asynchronous learning step, allowing the original information to be retained longer for training the representation learning module. Our approach is compatible with various backbone architectures and outperforms the top baselines by 6.8 original feature associations from modality encoders significantly boosts task-specific recommendation adaptation. Additionally, we find that larger modality encoders (e.g., Large Language Models) contain richer feature sets which necessitate more fine-grained modeling to reach their full performance potential. | Hengchang Hu, Qijiong Liu, Chuang Li, MinYen Kan |  |
|  |  [DREQ: Document Re-ranking Using Entity-Based Query Understanding](https://doi.org/10.1007/978-3-031-56027-9_13) |  | 0 | While entity-oriented neural IR models have advanced significantly, they often overlook a key nuance: the varying degrees of influence individual entities within a document have on its overall relevance. Addressing this gap, we present DREQ, an entity-oriented dense document re-ranking model. Uniquely, we emphasize the query-relevant entities within a document's representation while simultaneously attenuating the less relevant ones, thus obtaining a query-specific entity-centric document representation. We then combine this entity-centric document representation with the text-centric representation of the document to obtain a "hybrid" representation of the document. We learn a relevance score for the document using this hybrid representation. Using four large-scale benchmarks, we show that DREQ outperforms state-of-the-art neural and non-neural re-ranking methods, highlighting the effectiveness of our entity-oriented representation approach. | Shubham Chatterjee, Iain Mackie, Jeff Dalton |  |
|  |  [Beyond Topicality: Including Multidimensional Relevance in Cross-encoder Re-ranking - The Health Misinformation Case Study](https://doi.org/10.1007/978-3-031-56027-9_16) |  | 0 | In this paper, we propose a novel approach to consider multiple dimensions of relevance in cross-encoder re-ranking. On the one hand, cross-encoders constitute an effective solution for re-ranking when considering a single relevance dimension such as topicality, but are not designed to straightforwardly account for additional relevance dimensions. On the other hand, the majority of re-ranking models accounting for multdimensional relevance are often based on the aggregation of multiple relevance scores at the re-ranking stage, leading to potential compensatory effects. To address these issues, in the proposed solution we enhance the candidate documents retrieved by a first-stage lexical retrieval model with suitable relevance statements related to distinct relevance dimensions, and then perform a re-ranking on them with cross-encoders. In this work we focus, in particular, on an extra dimension of relevance beyond topicality, namely, credibility, to address health misinformation in the Consumer Health Search task. Experimental evaluations are performed by considering publicly available datasets; our results show that the proposed approach statistically outperforms state-of-the-art aggregation-based and cross-encoder re-rankers. | Rishabh Upadhyay, Arian Askari, Gabriella Pasi, Marco Viviani | Univ Milano Bicocca, Dept Informat Syst & Commun, Viale Sarca 336, I-20126 Milan, Italy; Leiden Univ, Leiden Inst Adv Comp Sci, Niels Bohrweg 1, NL-2333 CA Leiden, Netherlands |
|  |  [Query Obfuscation for Information Retrieval Through Differential Privacy](https://doi.org/10.1007/978-3-031-56027-9_17) |  | 0 | Protecting the privacy of a user querying an Information Retrieval (IR) system is of utmost importance. The problem is exacerbated when the IR system is not cooperative in satisfying the user's privacy requirements. To address this, obfuscation techniques split the user's sensitive query into multiple non-sensitive ones that can be safely transmitted to the IR system. To generate such queries, current approaches rely on lexical databases, such as WordNet, or heuristics of word co-occurrences. At the same time, advances in Natural Language Processing (NLP) have shown the power of Differential Privacy (DP) in releasing privacy-preserving text for completely different purposes, such as spam detection and sentiment analysis. We investigate for the first time whether DP mechanisms, originally designed for specific NLP tasks, can effectively be used in IR to obfuscate queries. We also assess their performance compared to state-of-the-art techniques in IR. Our empirical evaluation shows that the Vickrey DP mechanism based on theMahalanobis norm with privacy budget epsilon is an element of [10, 12.5] achieves state-of-the-art privacy protection and improved effectiveness. Furthermore, differently from previous approaches that are substantially on/off, by changing the privacy budget epsilon, DP allows users to adjust their desired level of privacy protection, offering a trade-off between effectiveness and privacy. | Guglielmo Faggioli, Nicola Ferro | Univ Padua, Padua, Italy |
|  |  [On-Device Query Auto-completion for Email Search](https://doi.org/10.1007/978-3-031-56027-9_18) |  | 0 | AbstractTraditional query auto-completion (QAC) relies heavily on search logs collected over many users. However, in on-device email search, the scarcity of logs and the governing privacy constraints make QAC a challenging task. In this work, we propose an on-device QAC method that runs directly on users’ devices, where users’ sensitive data and interaction logs are not collected, shared, or aggregated through web services. This method retrieves candidates using pseudo relevance feedback, and ranks them based on relevance signals that explore the textual and structural information from users’ emails. We also propose a private corpora based evaluation method, and empirically demonstrate the effectiveness of our proposed method. | Yifan Qiao, Otto Godwin, Hua Ouyang |  |
|  |  [Does the Performance of Text-to-Image Retrieval Models Generalize Beyond Captions-as-a-Query?](https://doi.org/10.1007/978-3-031-56066-8_15) |  | 0 | Text-image retrieval (T2I) refers to the task of recovering all images relevant to a keyword query. Popular datasets for text-image retrieval, such as Flickr30k, VG, or MS-COCO, utilize annotated image captions, e.g., "a man playing with a kid", as a surrogate for queries. With such surrogate queries, current multi-modal machine learning models, such as CLIP or BLIP, perform remarkably well. The main reason is the descriptive nature of captions, which detail the content of an image. Yet, T2I queries go beyond the mere descriptions in image-caption pairs. Thus, these datasets are ill-suited to test methods on more abstract or conceptual queries, e.g., "family vacations". In such queries, the image content is implied rather than explicitly described. In this paper, we replicate the T2I results on descriptive queries and generalize them to conceptual queries. To this end, we perform new experiments on a novel T2I benchmark for the task of conceptual query answering, called ConQA. ConQA comprises 30 descriptive and 50 conceptual queries on 43k images with more than 100 manually annotated images per query. Our results on established measures show that both large pretrained models (e.g., CLIP, BLIP, and BLIP2) and small models (e.g., SGRAF and NAAF), perform up to 4x better on descriptive rather than conceptual queries. We also find that the models perform better on queries with more than 6 keywords as in MS-COCO captions. | Juan Manuel Rodriguez, Nima Tavassoli, Eliezer Levy, Gil Lederman, Dima Sivov, Matteo Lissandrini, Davide Mottin | Aalborg Univ, Aalborg, Denmark; Tel Aviv Res Ctr Huawei Technol, Pnueli Lab, Tel Aviv, Israel; Aarhus Univ, Aarhus, Denmark |
|  |  [Query Generation Using Large Language Models - A Reproducibility Study of Unsupervised Passage Reranking](https://doi.org/10.1007/978-3-031-56066-8_19) |  | 0 | Existing passage retrieval techniques predominantly emphasize classification or dense matching strategies. This is in contrast with classic language modeling approaches focusing on query or question generation. Recently, Sachan et al. introduced an Unsupervised Passage Retrieval (UPR) approach that resembles this by exploiting the inherent generative capabilities of large language models. In this replicability study, we revisit the concept of zero-shot question generation for re-ranking and focus our investigation on the ranking experiments, validating the UPR findings, particularly on the widely recognized BEIR benchmark. Furthermore, we extend the original work by evaluating the proposed method additionally on the TREC Deep Learning track benchmarks of 2019 and 2020. To enhance our understanding of the technique’s performance, we introduce novel experiments exploring the influence of different prompts on retrieval outcomes. Our comprehensive analysis provides valuable insights into the robustness and applicability of zero-shot question generation as a re-ranking strategy in passage retrieval. | David Rau, Jaap Kamps | Univ Amsterdam, Amsterdam, Netherlands |
|  |  [Ranking Distance Metric for Privacy Budget in Distributed Learning of Finite Embedding Data](https://doi.org/10.1007/978-3-031-56066-8_21) |  | 0 | Federated Learning (FL) is a collective of distributed learning paradigm that aims to preserve privacy in data. Recent studies have shown FL models to be vulnerable to reconstruction attacks that compromise data privacy by inverting gradients computed on confidential data. To address the challenge of defending against these attacks, it is common to employ methods that guarantee data confidentiality using the principles of Differential Privacy (DP). However, in many cases, especially for machine learning models trained on unstructured data such as text, evaluating privacy requires to consider also the finite space of embedding for client's private data. In this study, we show how privacy in a distributed FL setup is sensitive to the underlying finite embeddings of the confidential data. We show that privacy can be quantified for a client batch that uses either noise, or a mixture of finite embeddings, by introducing a normalised rank distance (d(rank)). This measure has the advantage of taking into account the size of a finite vocabulary embedding, and align the privacy budget to a partitioned space. We further explore the impact of noise and client batch size on the privacy budget and compare it to the standard epsilon derived from Local-DP. | Georgios Papadopoulos, Yash Satsangi, Shaltiel Eloul, Marco Pistoia | JPMorgan Chase, Global Technol Appl Res, New York, NY 10017 USA |
|  |  [Effective Adhoc Retrieval Through Traversal of a Query-Document Graph](https://doi.org/10.1007/978-3-031-56063-7_6) |  | 0 | Adhoc retrieval is the task of effectively retrieving information for an end-user's information need, usually expressed as a textual query. One of the most well-established retrieval frameworks is the two-stage retrieval pipeline, whereby an inexpensive retrieval algorithm retrieves a subset of candidate documents from a corpus, and a more sophisticated (but costly) model re-ranks these candidates. A notable limitation of this two-stage framework is that the second stage re-ranking model can only re-order documents, and any relevant documents not retrieved from the corpus in the first stage are entirely lost to the second stage. A recently-proposed Adaptive Re-Ranking technique has shown that extending the candidate pool by traversing a document similarity graph can overcome this recall problem. However, this traversal technique is agnostic of the user's query, which has the potential to waste compute resources by scoring documents that are not related to the query. In this work, we propose an alternative formulation of the document similarity graph. Rather than using document similarities, we propose a weighted bipartite graph that consists of both document nodes and query nodes. This overcomes the limitations of prior Adaptive Re-Ranking approaches because the bipartite graph can be navigated in a manner that explicitly acknowledges the original user query issued to the search pipeline. We evaluate the effectiveness of our proposed framework by experimenting with the TREC Deep Learning track in a standard adhoc retrieval setting. We find that our approach outperforms state-of-the-art two-stage re-ranking pipelines, improving the nDCG@10 metric by 5.8% on the DL19 test collection. | Erlend Frayling, Sean MacAvaney, Craig Macdonald, Iadh Ounis | Univ Glasgow, Glasgow, Lanark, Scotland |
|  |  [MMCRec: Towards Multi-modal Generative AI in Conversational Recommendation](https://doi.org/10.1007/978-3-031-56063-7_23) |  | 0 | Personalized recommendation systems have become integral in this digital age by facilitating content discovery to users and products tailored to their preferences. Since the Generative Artificial Intelligence (GAI) boom, research into GAI-enhanced Conversational Recommender Systems (CRSs) has sparked great interest. Most existing methods, however, mainly rely on one mode of input such as text, thereby limiting their ability to capture content diversity. This is also inconsistent with real-world scenarios, which involve multi-modal input data and output data. To address these limitations, we propose the Multi-Modal Conversational Recommender System (MMCRec) model which harnesses multiple modalities, including text, images, voice and video to enhance the recommendation performance and experience. Our model is capable of not only accepting multi-mode input, but also generating multi-modal output in conversational recommendation. Experimental evaluations demonstrate the effectiveness of our model in real-world conversational recommendation scenarios. | Tendai Mukande, Esraa Ali, Annalina Caputo, Ruihai Dong, Noel E. O'Connor | Dublin City Univ, Dublin 9, Ireland; Univ Coll Dublin, Dublin 4, Ireland |
|  |  [Federated Conversational Recommender Systems](https://doi.org/10.1007/978-3-031-56069-9_4) |  | 0 | Conversational Recommender Systems (CRSs) have become increasingly popular as a powerful tool for providing personalized recommendation experiences. By directly engaging with users in a conversational manner to learn their current and fine-grained preferences, a CRS can quickly derive recommendations that are relevant and justifiable. However, existing CRSs typically rely on a centralized training and deployment process, which involves collecting and storing explicitly-communicated user preferences in a centralized repository. These fine-grained user preferences are completely human-interpretable and can easily be used to infer sensitive information (e.g., financial status, political stands, and health information) about the user, if leaked or breached. To address the user privacy concerns in CRS, we first define a set of privacy protection guidelines for preserving user privacy then propose a novel federated CRS framework that effectively reduces the risk of exposing user privacy. Through extensive experiments, we show that the proposed framework not only satisfies these user privacy protection guidelines, but also achieves competitive recommendation performance comparing to the state-of-the-art non-private conversational recommendation approach. | Allen Lin, Jianling Wang, Ziwei Zhu, James Caverlee | Texas A&M Univ, College Stn, TX 77843 USA; George Mason Univ, Fairfax, VA 22030 USA |
|  |  [Improving Exposure Allocation in Rankings by Query Generation](https://doi.org/10.1007/978-3-031-56069-9_9) |  | 0 | Deploying methods that incorporate generated queries in their retrieval process, such as Doc2Query, has been shown to be effective for retrieving the most relevant documents for a user's query. However, to the best of our knowledge, there has been no work yet on whether generated queries can also be used in the ranking process to achieve other objectives, such as ensuring a fair distribution of exposure in the ranking. Indeed, the amount of exposure that a document is likely to receive depends on the document's position in the ranking, with lower-ranked documents having a lower probability of being examined by the user. While the utility to users remains the main objective of an Information Retrieval (IR) system, an unfair exposure allocation can lead to lost opportunities and unfair economic impacts for particular societal groups. Therefore, in this work, we conduct a first investigation into whether generating relevant queries can help to fairly distribute the exposure over groups of documents in a ranking. In our work, we build on the effective Doc2Query methods to selectively generate relevant queries for underrepresented groups of documents and use their predicted relevance to the original query in order to re-rank the underexposed documents. Our experiments on the TREC 2022 Fair Ranking Track collection show that using generated queries consistently leads to a fairer allocation of exposure compared to a standard ranking while still maintaining utility. | Thomas Jänich, Graham McDonald, Iadh Ounis | Univ Glasgow, Glasgow, Lanark, Scotland |
|  |  [KnowFIRES: A Knowledge-Graph Framework for Interpreting Retrieved Entities from Search](https://doi.org/10.1007/978-3-031-56069-9_15) |  | 0 | Entity retrieval is essential in information access domains where people search for specific entities, such as individuals, organizations, and places. While entity retrieval is an active research topic in Information Retrieval, it is necessary to explore the explainability and interpretability of them more extensively. KnowFIRES addresses this by offering a knowledge graph-based visual representation of entity retrieval results, focusing on contrasting different retrieval methods. KnowFIRES allows users to better understand these differences through the juxtaposition and superposition of retrieved sub-graphs. As part of our demo, we make KnowFIRES (Demo: http://knowfires.live , Source: https://github.com/kiarashgl/KnowFIRES ) web interface and its source code publicly available (A demonstration of the tool: https://www.youtube.com/watch?v=9u-877ArNYE ). | Negar Arabzadeh, Kiarash Golzadeh, Christopher Risi, Charles L. A. Clarke, Jian Zhao | Univ Waterloo, Waterloo, ON, Canada |
|  |  [A Conversational Search Framework for Multimedia Archives](https://doi.org/10.1007/978-3-031-56069-9_25) |  | 0 | Conversational search system seek to support users in their search activities to improve the effectiveness and efficiency of search while reducing their cognitive load. The challenges of multimedia search mean that search supports provided by conversational search have the potential to improve the user search experience. For example, by assisting users in constructing better queries and making more informed decisions in relevance feedback stages whilst searching. However, previous research on conversational search has been focused almost exclusively on text archives. This demonstration illustrates the potential for the application of conversational methods in multimedia search. We describe a framework to enable multimodal conversational search for use with multimedia archives. Our current prototype demonstrates the use of an conversational AI assistant during the multimedia information retrieval process for both image and video collections. | Anastasia Potyagalova, Gareth J. F. Jones | Dublin City Univ, Sch Comp, ADAPT Ctr, Dublin 9, Ireland |
|  |  [Effective and Efficient Transformer Models for Sequential Recommendation](https://doi.org/10.1007/978-3-031-56069-9_39) |  | 0 | Sequential Recommender Systems use the order of user-item interactions to predict the next item in the sequence. This task is similar to Language Modelling, where the goal is to predict the next token based on the sequence of past tokens. Therefore, adaptations of language models, and, in particular, Transformer-based models, achieved state-of-the-art results for a sequential recommendation. However, despite similarities, the sequential recommendation problem poses a number of specific challenges not present in Language Modelling. These challenges include the large catalogue size of real-world recommender systems, which increases GPU memory requirements and makes the training and the inference of recommender models slow. Another challenge is that a good recommender system should focus not only on the accuracy of recommendation but also on additional metrics, such as diversity and novelty, which makes the direct adaptation of language model training strategies problematic. Our research focuses on solving these challenges. In this doctoral consortium abstract, we briefly describe the motivation and background for our work and then pose research questions and discuss current progress towards solving the described problems. | Aleksandr V. Petrov | Univ Glasgow, Glasgow, Lanark, Scotland |
|  |  [Quantum Computing for Information Retrieval and Recommender Systems](https://doi.org/10.1007/978-3-031-56069-9_47) |  | 0 | The field of Quantum Computing (QC) has gained significant popularity in recent years, due to its potential to provide benefits in terms of efficiency and effectiveness when employed to solve certain computationally intensive tasks. In both Information Retrieval (IR) and Recommender Systems (RS) we are required to build methods that apply complex processing on large and heterogeneous datasets, it is natural therefore to wonder whether QC could also be applied to boost their performance. The tutorial aims to provide first an introduction to QC for an audience that is not familiar with the technology, then to show how to apply the QC paradigm of Quantum Annealing (QA) to solve practical problems that are currently faced by IR and RS systems. During the tutorial, participants will be provided with the fundamentals required to understand QC and to apply it in practice by using a real D-Wave quantum annealer through APIs. | Maurizio Ferrari Dacrema, Andrea Pasin, Paolo Cremonesi, Nicola Ferro | Politecn Milan, Milan, Italy; Univ Padua, Padua, Italy |
|  |  [Transformers for Sequential Recommendation](https://doi.org/10.1007/978-3-031-56069-9_49) |  | 0 | Learning dynamic user preference has become an increasingly important component for many online platforms (e.g., video-sharing sites, e-commerce systems) to make sequential recommendations. Previous works have made many efforts to model item-item transitions over user interaction sequences, based on various architectures, e.g., recurrent neural networks and self-attention mechanism. Recently emerged graph neural networks also serve as useful backbone models to capture item dependencies in sequential recommendation scenarios. Despite their effectiveness, existing methods have far focused on item sequence representation with singular type of interactions, and thus are limited to capture dynamic heterogeneous relational structures between users and items (e.g., page view, add-to-favorite, purchase). To tackle this challenge, we design a Multi-Behavior Hypergraph-enhanced T ransformer framework (MBHT) to capture both short-term and long-term cross-type behavior dependencies. Specifically, a multi-scale Transformer is equipped with low-rank self-attention to jointly encode behavior-aware sequential patterns from fine-grained and coarse-grained levels. Additionally,we incorporate the global multi-behavior dependency into the hypergraph neural architecture to capture the hierarchical long-range item correlations in a customized manner. Experimental results demonstrate the superiority of our MBHT over various state-of- the-art recommendation solutions across different settings. Further ablation studies validate the effectiveness of our model design and benefits of the new MBHT framework. Our implementation code is released at: https://github.com/yuh-yang/MBHT-KDD22. | Aleksandr V. Petrov, Craig Macdonald | University of Hong Kong, Hong Kong, China; Ocean University of China, Qingdao, China; Wuhan University, Wuhan, China; National University of Singapore, Singapore, Singapore |
|  |  [Context-Aware Query Term Difficulty Estimation for Performance Prediction](https://doi.org/10.1007/978-3-031-56066-8_4) |  | 0 | Research has already found that many retrieval methods are sensitive to the choice and order of terms that appear in a query, which can significantly impact retrieval effectiveness. We capitalize on this finding in order to predict the performance of a query. More specifically, we propose to learn query term difficulty weights specifically within the context of each query, which could then be used as indicators of whether each query term has the likelihood of making the query more effective or not. We show how such difficulty weights can be learnt through the finetuning of a language model. In addition, we propose an approach to integrate the learnt weights into a cross-encoder architecture to predict query performance. We show that our proposed approach shows a consistently strong performance prediction on the MSMARCO collection and its associated widely used Trec Deep Learning tracks query sets. Our findings demonstrate that our method is able to show consistently strong performance prediction over different query sets (MSMARCO Dev, TREC DL'19, '20, Hard) and a range of evaluation metrics (Kendall, Spearman, sMARE). | Abbas Saleminezhad, Negar Arabzadeh, Soosan Beheshti, Ebrahim Bagheri | Univ Waterloo, Toronto, ON, Canada; Toronto Metropolitan Univ, Toronto, ON, Canada |
|  |  [Navigating the Thin Line: Examining User Behavior in Search to Detect Engagement and Backfire Effects](https://doi.org/10.1007/978-3-031-56066-8_30) |  | 0 | Opinionated users often seek information that aligns with their preexisting beliefs while dismissing contradictory evidence due to confirmation bias. This conduct hinders their ability to consider alternative stances when searching the web. Despite this, few studies have analyzed how the diversification of search results on disputed topics influences the search behavior of highly opinionated users. To this end, we present a preregistered user study (n = 257) investigating whether different levels (low and high) of bias metrics and search results presentation (with or without AI-predicted stances labels) can affect the stance diversity consumption and search behavior of opinionated users on three debated topics (i.e., atheism, intellectual property rights, and school uniforms). Our results show that exposing participants to (counter-attitudinally) biased search results increases their consumption of attitude-opposing content, but we also found that bias was associated with a trend toward overall fewer interactions within the search page. We also found that 19 any search results. When we removed these participants in a post-hoc analysis, we found that stance labels increased the diversity of stances consumed by users, particularly when the search results were biased. Our findings highlight the need for future research to explore distinct search scenario settings to gain insight into opinionated users' behavior. | Federico Maria Cau, Nava Tintarev |  |
|  |  [Measuring Bias in a Ranked List Using Term-Based Representations](https://doi.org/10.1007/978-3-031-56069-9_1) |  | 0 | In most recent studies, gender bias in document ranking is evaluated with the NFaiRR metric, which measures bias in a ranked list based on an aggregation over the unbiasedness scores of each ranked document. This perspective in measuring the bias of a ranked list has a key limitation: individual documents of a ranked list might be biased while the ranked list as a whole balances the groups' representations. To address this issue, we propose a novel metric called TExFAIR (term exposure-based fairness), which is based on two new extensions to a generic fairness evaluation framework, attention-weighted ranking fairness (AWRF). TExFAIR assesses fairness based on the term-based representation of groups in a ranked list: (i) an explicit definition of associating documents to groups based on probabilistic term-level associations, and (ii) a rank-biased discounting factor (RBDF) for counting non-representative documents towards the measurement of the fairness of a ranked list. We assess TExFAIR on the task of measuring gender bias in passage ranking, and study the relationship between TExFAIR and NFaiRR. Our experiments show that there is no strong correlation between TExFAIR and NFaiRR, which indicates that TExFAIR measures a different dimension of fairness than NFaiRR. With TExFAIR, we extend the AWRF framework to allow for the evaluation of fairness in settings with term-based representations of groups in documents in a ranked list. | Amin Abolghasemi, Leif Azzopardi, Arian Askari, Maarten de Rijke, Suzan Verberne |  |
|  |  [Translate-Distill: Learning Cross-Language Dense Retrieval by Translation and Distillation](https://doi.org/10.1007/978-3-031-56060-6_4) |  | 0 | Prior work on English monolingual retrieval has shown that a cross-encoder trained using a large number of relevance judgments for query-document pairs can be used as a teacher to train more efficient, but similarly effective, dual-encoder student models. Applying a similar knowledge distillation approach to training an efficient dual-encoder model for Cross-Language Information Retrieval (CLIR), where queries and documents are in different languages, is challenging due to the lack of a sufficiently large training collection when the query and document languages differ. The state of the art for CLIR thus relies on translating queries, documents, or both from the large English MS MARCO training set, an approach called Translate-Train. This paper proposes an alternative, Translate-Distill, in which knowledge distillation from either a monolingual cross-encoder or a CLIR cross-encoder is used to train a dual-encoder CLIR student model. This richer design space enables the teacher model to perform inference in an optimized setting, while training the student model directly for CLIR. Trained models and artifacts are publicly available on Huggingface. | Eugene Yang, Dawn J. Lawrie, James Mayfield, Douglas W. Oard, Scott Miller |  |
|  |  [DESIRE-ME: Domain-Enhanced Supervised Information Retrieval Using Mixture-of-Experts](https://doi.org/10.1007/978-3-031-56060-6_8) |  | 0 | Open-domain question answering requires retrieval systems able to cope with the diverse and varied nature of questions, providing accurate answers across a broad spectrum of query types and topics. To deal with such topic heterogeneity through a unique model, we propose DESIRE-ME, a neural information retrieval model that leverages the Mixture-of-Experts framework to combine multiple specialized neural models. We rely on Wikipedia data to train an effective neural gating mechanism that classifies the incoming query and that weighs the predictions of the different domain-specific experts correspondingly. This allows DESIRE-ME to specialize adaptively in multiple domains. Through extensive experiments on publicly available datasets, we show that our proposal can effectively generalize domain-enhanced neural models. DESIRE-ME excels in handling open-domain questions adaptively, boosting by up to 12 22 | Pranav Kasela, Gabriella Pasi, Raffaele Perego, Nicola Tonellotto |  |
|  |  [A Deep Learning Approach for Selective Relevance Feedback](https://doi.org/10.1007/978-3-031-56060-6_13) |  | 0 | Pseudo-relevance feedback (PRF) can enhance average retrieval effectiveness over a sufficiently large number of queries. However, PRF often introduces a drift into the original information need, thus hurting the retrieval effectiveness of several queries. While a selective application of PRF can potentially alleviate this issue, previous approaches have largely relied on unsupervised or feature-based learning to determine whether a query should be expanded. In contrast, we revisit the problem of selective PRF from a deep learning perspective, presenting a model that is entirely data-driven and trained in an end-to-end manner. The proposed model leverages a transformer-based bi-encoder architecture. Additionally, to further improve retrieval effectiveness with this selective PRF approach, we make use of the model's confidence estimates to combine the information from the original and expanded queries. In our experiments, we apply this selective feedback on a number of different combinations of ranking and feedback models, and show that our proposed approach consistently improves retrieval effectiveness for both sparse and dense ranking models, with the feedback models being either sparse, dense or generative. | Suchana Datta, Debasis Ganguly, Sean MacAvaney, Derek Greene |  |
|  |  [Self Contrastive Learning for Session-Based Recommendation](https://doi.org/10.1007/978-3-031-56027-9_1) |  | 0 | Session-based recommendation, which aims to predict the next item of users' interest as per an existing sequence interaction of items, has attracted growing applications of Contrastive Learning (CL) with improved user and item representations. However, these contrastive objectives: (1) serve a similar role as the cross-entropy loss while ignoring the item representation space optimisation; and (2) commonly require complicated modelling, including complex positive/negative sample constructions and extra data augmentation. In this work, we introduce Self-Contrastive Learning (SCL), which simplifies the application of CL and enhances the performance of state-of-the-art CL-based recommendation techniques. Specifically, SCL is formulated as an objective function that directly promotes a uniform distribution among item representations and efficiently replaces all the existing contrastive objective components of state-of-the-art models. Unlike previous works, SCL eliminates the need for any positive/negative sample construction or data augmentation, leading to enhanced interpretability of the item representation space and facilitating its extensibility to existing recommender systems. Through experiments on three benchmark datasets, we demonstrate that SCL consistently improves the performance of state-of-the-art models with statistical significance. Notably, our experiments show that SCL improves the performance of two best-performing models by 8.2% and 9.5% in P@10 (Precision) and 9.9% and 11.2% in MRR@10 (Mean Reciprocal Rank) on average across different benchmarks. Additionally, our analysis elucidates the improvement in terms of alignment and uniformity of representations, as well as the effectiveness of SCL with a low computational cost. | Zhengxiang Shi, Xi Wang, Aldo Lipani |  |
|  |  [Revealing the Hidden Impact of Top-N Metrics on Optimization in Recommender Systems](https://doi.org/10.1007/978-3-031-56027-9_9) |  | 0 | The hyperparameters of recommender systems for top-n predictions are typically optimized to enhance the predictive performance of algorithms. Thereby, the optimization algorithm, e.g., grid search or random search, searches for the best hyperparameter configuration according to an optimization-target metric, like nDCG or Precision. In contrast, the optimized algorithm, internally optimizes a different loss function during training, like squared error or cross-entropy. To tackle this discrepancy, recent work focused on generating loss functions better suited for recommender systems. Yet, when evaluating an algorithm using a top-n metric during optimization, another discrepancy between the optimization-target metric and the training loss has so far been ignored. During optimization, the top-n items are selected for computing a top-n metric; ignoring that the top-n items are selected from the recommendations of a model trained with an entirely different loss function. Item recommendations suitable for optimization-target metrics could be outside the top-n recommended items; hiddenly impacting the optimization performance. Therefore, we were motivated to analyze whether the top-n items are optimal for optimization-target top-n metrics. In pursuit of an answer, we exhaustively evaluate the predictive performance of 250 selection strategies besides selecting the top-n. We extensively evaluate each selection strategy over twelve implicit feedback and eight explicit feedback data sets with eleven recommender systems algorithms. Our results show that there exist selection strategies other than top-n that increase predictive performance for various algorithms and recommendation domains. However, the performance of the top 43 of selection strategies is not significantly different. We discuss the impact of our findings on optimization and re-ranking in recommender systems and feasible solutions. | Lukas Wegmeth, Tobias Vente, Lennart Purucker |  |
|  |  [TWOLAR: A TWO-Step LLM-Augmented Distillation Method for Passage Reranking](https://doi.org/10.1007/978-3-031-56027-9_29) |  | 0 | In this paper, we present TWOLAR: a two-stage pipeline for passage reranking based on the distillation of knowledge from Large Language Models (LLM). TWOLAR introduces a new scoring strategy and a distillation process consisting in the creation of a novel and diverse training dataset. The dataset consists of 20K queries, each associated with a set of documents retrieved via four distinct retrieval methods to ensure diversity, and then reranked by exploiting the zero-shot reranking capabilities of an LLM. Our ablation studies demonstrate the contribution of each new component we introduced. Our experimental results show that TWOLAR significantly enhances the document reranking ability of the underlying model, matching and in some cases even outperforming state-of-the-art models with three orders of magnitude more parameters on the TREC-DL test sets and the zero-shot evaluation benchmark BEIR. To facilitate future work we release our data set, finetuned models, and code. | Davide Baldelli, Junfeng Jiang, Akiko Aizawa, Paolo Torroni |  |
|  |  [Estimating Query Performance Through Rich Contextualized Query Representations](https://doi.org/10.1007/978-3-031-56066-8_6) |  | 0 | The state-of-the-art query performance prediction methods rely on the fine-tuning of contextual language models to estimate retrieval effectiveness on a per-query basis. Our work in this paper builds on this strong foundation and proposes to learn rich query representations by learning the interactions between the query and two important contextual information, namely (1) the set of documents retrieved by that query, and (2) the set of similar historical queries with known retrieval effectiveness. We propose that such contextualized query representations can be more accurate estimators of query performance as they embed the performance of past similar queries and the semantics of the documents retrieved by the query. We perform extensive experiments on the MSMARCO collection and its accompanying query sets including MSMARCO Dev set and TREC Deep Learning tracks of 2019, 2020, 2021, and DL-Hard. Our experiments reveal that our proposed method shows robust and effective performance compared to state-of-the-art baselines. | Sajad Ebrahimi, Maryam Khodabakhsh, Negar Arabzadeh, Ebrahim Bagheri | Toronto Metropolitan Univ, Toronto, ON, Canada; Shahrood Univ Technol, Shahrood, Iran; Univ Waterloo, Waterloo, ON, Canada; Univ Guelph, Guelph, ON, Canada |
|  |  [Performance Comparison of Session-Based Recommendation Algorithms Based on GNNs](https://doi.org/10.1007/978-3-031-56066-8_12) |  | 0 | In session-based recommendation settings, a recommender system has to base its suggestions on the user interactions that are ob served in an ongoing session. Since such sessions can consist of only a small set of interactions, various approaches based on Graph Neural Networks (GNN) were recently proposed, as they allow us to integrate various types of side information about the items in a natural way. Unfortunately, a variety of evaluation settings are used in the literature, e.g., in terms of protocols, metrics and baselines, making it difficult to assess what represents the state of the art. In this work, we present the results of an evaluation of eight recent GNN-based approaches that were published in high-quality outlets. For a fair comparison, all models are systematically tuned and tested under identical conditions using three common datasets. We furthermore include k-nearest-neighbor and sequential rules-based models as baselines, as such models have previously exhibited competitive performance results for similar settings. To our surprise, the evaluation showed that the simple models outperform all recent GNN models in terms of the Mean Reciprocal Rank, which we used as an optimization criterion, and were only outperformed in three cases in terms of the Hit Rate. Additional analyses furthermore reveal that several other factors that are often not deeply discussed in papers, e.g., random seeds, can markedly impact the performance of GNN-based models. Our results therefore (a) point to continuing issues in the community in terms of research methodology and (b) indicate that there is ample room for improvement in session-based recommendation. | Faisal Shehzad, Dietmar Jannach |  |
|  |  [Weighted AUReC: Handling Skew in Shard Map Quality Estimation for Selective Search](https://doi.org/10.1007/978-3-031-56066-8_10) |  | 0 | In selective search, a document collection is partitioned into a collection of topical index shards. To efficiently estimate the topical coherence (or quality) of a shard map, the AUReC measure was introduced. AUReC makes the assumption that shards are of similar sizes, one that is violated in practice, even for unsupervised approaches. The problem might be amplified if supervised labelling approaches with skewed class distributions are used. To estimate the quality of such unbalanced shard maps, we introduce a weighted adaptation of the AUReC measure, and empirically evaluate its effectiveness using the ClueWeb09B and Gov2 datasets. We show that it closely matches the evaluations of the original AUReC when shards are similar in size, but captures better the differences in performance when shard sizes are skewed. | Gijs Hendriksen, Djoerd Hiemstra, Arjen P. de Vries | Radboud Univ Nijmegen, Nijmegen, Netherlands |
|  |  [Measuring Item Fairness in Next Basket Recommendation: A Reproducibility Study](https://doi.org/10.1007/978-3-031-56066-8_18) |  | 0 | Item fairness of recommender systems aims to evaluate whether items receive a fair share of exposure according to different definitions of fairness. Raj and Ekstrand [26] study multiple fairness metrics under a common evaluation framework and test their sensitivity with respect to various configurations. They find that fairness metrics show varying degrees of sensitivity towards position weighting models and parameter settings under different information access systems. Although their study considers various domains and datasets, their findings do not necessarily generalize to next basket recommendation (NBR) where users exhibit a more repeat-oriented behavior compared to other recommendation domains. This paper investigates fairness metrics in the NBR domain under a unified experimental setup. Specifically, we directly evaluate the item fairness of various NBR methods. These fairness metrics rank NBR methods in different orders, while most of the metrics agree that repeat-biased methods are fairer than explore-biased ones. Furthermore, we study the effect of unique characteristics of the NBR task on the sensitivity of the metrics, including the basket size, position weighting models, and user repeat behavior. Unlike the findings in [26], Inequity of Amortized Attention (IAA) is the most sensitive metric, as observed in multiple experiments. Our experiments lead to novel findings in the field of NBR and fairness. We find that Expected Exposure Loss (EEL) and Expected Exposure Disparity (EED) are the most robust and adaptable fairness metrics to be used in the NBR domain. | Yuanna Liu, Ming Li, Mozhdeh Ariannezhad, Masoud Mansoury, Mohammad Aliannejadi, Maarten de Rijke | AIRLab, Amsterdam, Netherlands; Booking com, Amsterdam, Netherlands; Univ Amsterdam, Amsterdam, Netherlands |
|  |  [Is Interpretable Machine Learning Effective at Feature Selection for Neural Learning-to-Rank?](https://doi.org/10.1007/978-3-031-56066-8_29) |  | 0 | Neural ranking models have become increasingly popular for real-world searchand recommendation systems in recent years. Unlike their tree-basedcounterparts, neural models are much less interpretable. That is, it is verydifficult to understand their inner workings and answer questions like how dothey make their ranking decisions? or what document features do they findimportant? This is particularly disadvantageous since interpretability ishighly important for real-world systems. In this work, we explore featureselection for neural learning-to-rank (LTR). In particular, we investigate sixwidely-used methods from the field of interpretable machine learning (ML) andintroduce our own modification, to select the input features that are mostimportant to the ranking behavior. To understand whether these methods areuseful for practitioners, we further study whether they contribute toefficiency enhancement. Our experimental results reveal a large featureredundancy in several LTR benchmarks: the local selection method TabNet canachieve optimal ranking performance with less than 10 features; the globalmethods, particularly our G-L2X, require slightly more selected features, butexhibit higher potential in improving efficiency. We hope that our analysis ofthese feature selection methods will bring the fields of interpretable ML andLTR closer together. | Lijun Lyu, Nirmal Roy, Harrie Oosterhuis, Avishek Anand | Delft Univ Technol, Delft, Netherlands; Radboud Univ Nijmegen, Nijmegen, Netherlands |
|  |  [The Impact of Differential Privacy on Recommendation Accuracy and Popularity Bias](https://doi.org/10.1007/978-3-031-56066-8_33) |  | 0 | Collaborative filtering-based recommender systems leverage vast amounts of behavioral user data, which poses severe privacy risks. Thus, often, random noise is added to the data to ensure Differential Privacy (DP). However, to date, it is not well understood, in which ways this impacts personalized recommendations. In this work, we study how DP impacts recommendation accuracy and popularity bias, when applied to the training data of state-of-the-art recommendation models. Our findings are three-fold: First, we find that nearly all users' recommendations change when DP is applied. Second, recommendation accuracy drops substantially while recommended item popularity experiences a sharp increase, suggesting that popularity bias worsens. Third, we find that DP exacerbates popularity bias more severely for users who prefer unpopular items than for users that prefer popular items. | Peter Müllner, Elisabeth Lex, Markus Schedl, Dominik Kowald |  |
|  |  [How to Forget Clients in Federated Online Learning to Rank?](https://doi.org/10.1007/978-3-031-56063-7_7) |  | 0 | Data protection legislation like the European Union's General Data Protection Regulation (GDPR) establishes the right to be forgotten: a user (client) can request contributions made using their data to be removed from learned models. In this paper, we study how to remove the contributions made by a client participating in a Federated Online Learning to Rank (FOLTR) system. In a FOLTR system, a ranker is learned by aggregating local updates to the global ranking model. Local updates are learned in an online manner at a client-level using queries and implicit interactions that have occurred within that specific client. By doing so, each client's local data is not shared with other clients or with a centralised search service, while at the same time clients can benefit from an effective global ranking model learned from contributions of each client in the federation. In this paper, we study an effective and efficient unlearning method that can remove a client's contribution without compromising the overall ranker effectiveness and without needing to retrain the global ranker from scratch. A key challenge is how to measure whether the model has unlearned the contributions from the client c^\* that has requested removal. For this, we instruct c^\* to perform a poisoning attack (add noise to this client updates) and then we measure whether the impact of the attack is lessened when the unlearning process has taken place. Through experiments on four datasets, we demonstrate the effectiveness and efficiency of the unlearning strategy under different combinations of parameter settings. | Shuyi Wang, Bing Liu, Guido Zuccon |  |
|  |  [InDi: Informative and Diverse Sampling for Dense Retrieval](https://doi.org/10.1007/978-3-031-56063-7_16) |  | 0 | Negative sample selection has been shown to have a crucial effect on the training procedure of dense retrieval systems. Nevertheless, most existing negative selection methods end by randomly choosing from some pool of samples. This calls for a better sampling solution. We define desired requirements for negative sample selection; the samples chosen should be informative, to advance the learning process, and diverse, to help the model generalize. We compose a sampling method designed to meet these requirements, and show that using our sampling method to enhance the training procedure of a recent significant dense retrieval solution (coCondenser) improves the obtained model's performance. Specifically, we see a similar to 2% improvement in MRR@10 on the MS MARCO dataset (from 38.2 to 38.8) and a similar to 1.5% improvement in Recall@5 on the Natural Questions dataset (from 71% to 72.1%), both statistically significant. Our solution, as opposed to other methods, does not require training or inferencing a large model, and adds only a small overhead (similar to 1% added time) to the training procedure. Finally, we report ablation studies showing that the objectives defined are indeed important when selecting negative samples for dense retrieval. | Nachshon Cohen, Hedda Cohen Indelman, Yaron Fairstein, Guy Kushilevitz | Technion, Haifa, Israel; Amazon, Haifa, Israel |
|  |  [Learning-to-Rank with Nested Feedback](https://doi.org/10.1007/978-3-031-56063-7_22) |  | 0 | Many platforms on the web present ranked lists of content to users, typically optimized for engagement-, satisfaction- or retention- driven metrics. Advances in the Learning-to-Rank (LTR) research literature have enabled rapid growth in this application area. Several popular interfaces now include nested lists, where users can enter a 2nd-level feed via any given 1st-level item. Naturally, this has implications for evaluation metrics, objective functions, and the ranking policies we wish to learn. We propose a theoretically grounded method to incorporate 2nd-level feedback into any 1st-level ranking model. Online experiments on a large-scale recommendation system confirm our theoretical findings. | Hitesh Sagtani, Olivier Jeunen, Aleksei Ustimenko |  |
|  |  [Simple Domain Adaptation for Sparse Retrievers](https://doi.org/10.1007/978-3-031-56063-7_32) |  | 0 | In Information Retrieval, and more generally in Natural Language Processing, adapting models to specific domains is conducted through fine-tuning. Despite the successes achieved by this method and its versatility, the need for human-curated and labeled data makes it impractical to transfer to new tasks, domains, and/or languages when training data doesn't exist. Using the model without training (zero-shot) is another option that however suffers an effectiveness cost, especially in the case of first-stage retrievers. Numerous research directions have emerged to tackle these issues, most of them in the context of adapting to a task or a language. However, the literature is scarcer for domain (or topic) adaptation. In this paper, we address this issue of cross-topic discrepancy for a sparse first-stage retriever by transposing a method initially designed for language adaptation. By leveraging pre-training on the target data to learn domain-specific knowledge, this technique alleviates the need for annotated data and expands the scope of domain adaptation. Despite their relatively good generalization ability, we show that even sparse retrievers can benefit from our simple domain adaptation method. | Mathias Vast, Yuxuan Zong, Benjamin Piwowarski, Laure Soulier |  |
|  |  [Selma: A Semantic Local Code Search Platform](https://doi.org/10.1007/978-3-031-56069-9_21) |  | 0 | Searching for the right code snippet is cumbersome and not a trivial task. Online platforms such as Github.com or searchcode.com provide tools to search, but they are limited to publicly available and internet-hosted code. However, during the development of research prototypes or confidential tools, it is preferable to store source code locally. Consequently, the use of external code search tools becomes impractical. Here, we present Selma (Code and Videos: https://anreu.github.io/selma ): a local code search platform that enables term-based and semantic retrieval of source code. Selma searches code and comments, annotates undocumented code to enable term-based search in natural language, and trains neural models for code retrieval. | Anja Reusch, Guilherme C. Lopes, Wilhelm Pertsch, Hannes Ueck, Julius Gonsior, Wolfgang Lehner | Tech Univ Dresden, Dresden Database Syst Grp, Dresden, Germany |
|  |  [FAR-AI: A Modular Platform for Investment Recommendation in the Financial Domain](https://doi.org/10.1007/978-3-031-56069-9_30) |  | 0 | Financial asset recommendation (FAR) is an emerging sub-domain of the wider recommendation field that is concerned with recommending suitable financial assets to customers, with the expectation that those customers will invest capital into a subset of those assets. FAR is a particularly interesting sub-domain to explore, as unlike traditional movie or product recommendation, FAR solutions need to analyse and learn from a combination of time-series pricing data, company fundamentals, social signals and world events, relating the patterns observed to multi-faceted customer representations comprising profiling information, expectations and past investments. In this demo we will present a modular FAR platform; referred to as FAR-AI, with the goal of raising awareness and building a community around this emerging domain, as well as illustrate the challenges, design considerations and new research directions that FAR offers. The demo will comprise two components: 1) we will present the architecture of FAR-AI to attendees, to enable them to understand the how's and the why's of developing a FAR system; and 2) a live demonstration of FAR-AI as a customer-facing product, highlighting the differences in functionality between FAR solutions and traditional recommendation scenarios. The demo is supplemented by online-tutorial materials, to enable attendees new to this space to get practical experience with training FAR models. VIDEO URL. | Javier SanzCruzado, Edward Richards, Richard McCreadie | Univ Glasgow, Glasgow, Lanark, Scotland |
|  |  [Semantic Content Search on IKEA.com](https://doi.org/10.1007/978-3-031-56069-9_32) |  | 0 | In this paper, we present an approach to content search. The aim is to increase customer engagement with content recommendations on IKEA.com. As an alternative to Boolean search, we introduce a method based on semantic textual similarity between content pages and search queries. Our approach improves the relevance of search results by a 2.95% increase in click-through rate in an online A/B test. | Mateusz Slominski, Ezgi Yildirim, Martin Tegner | Ingka Grp, IKEA Retail, Leiden, Netherlands |
|  |  [Semantic Search in Archive Collections Through Interpretable and Adaptable Relation Extraction About Person and Places](https://doi.org/10.1007/978-3-031-56069-9_37) |  | 0 | In recent years, libraries and archives have undertaken numerous campaigns to digitise their collections. While these campaigns have increased ease of access to archival documents for a wider audience, ensuring discoverability and promoting their content remain significant challenges. Digitised documents are often unstructured, making them difficult to navigate. Accessing archive materials through search engines restricts users to keyword-based queries, leading to being overwhelmed by irrelevant documents. To enhance the exploration and exploitation of the "Big Data of the Past" [15], it is imperative to structure textual content. | Nicolas Gutehrlé | Univ Franche Comte, CRIT, F-25000 Besancon, France |
|  |  [Reproduction and Simulation of Interactive Retrieval Experiments](https://doi.org/10.1007/978-3-031-56069-9_40) |  | 0 | The reproducibility crisis, spanning across various scientific fields, substantially affects information retrieval research [1]. | Jana Isabelle Friese | Univ Duisburg Essen, Duisburg, Germany |
|  |  [Efficient Multi-vector Dense Retrieval with Bit Vectors](https://doi.org/10.1007/978-3-031-56060-6_1) |  | 0 | Dense retrieval techniques employ pre-trained large language models to builda high-dimensional representation of queries and passages. Theserepresentations compute the relevance of a passage w.r.t. to a query usingefficient similarity measures. In this line, multi-vector representations showimproved effectiveness at the expense of a one-order-of-magnitude increase inmemory footprint and query latency by encoding queries and documents on aper-token level. Recently, PLAID has tackled these problems by introducing acentroid-based term representation to reduce the memory impact of multi-vectorsystems. By exploiting a centroid interaction mechanism, PLAID filters outnon-relevant documents, thus reducing the cost of the successive rankingstages. This paper proposes “Efficient Multi-Vector dense retrieval with Bitvectors” (EMVB), a novel framework for efficient query processing inmulti-vector dense retrieval. First, EMVB employs a highly efficientpre-filtering step of passages using optimized bit vectors. Second, thecomputation of the centroid interaction happens column-wise, exploiting SIMDinstructions, thus reducing its latency. Third, EMVB leverages ProductQuantization (PQ) to reduce the memory footprint of storing vectorrepresentations while jointly allowing for fast late interaction. Fourth, weintroduce a per-document term filtering method that further improves theefficiency of the last step. Experiments on MS MARCO and LoTTE show that EMVBis up to 2.8x faster while reducing the memory footprint by 1.8x with no lossin retrieval accuracy compared to PLAID. | Franco Maria Nardini, Cosimo Rulli, Rossano Venturini | CNR, ISTI, Pisa, Italy; Univ Pisa, Pisa, Italy |
|  |  [Prompt-Based Generative News Recommendation (PGNR): Accuracy and Controllability](https://doi.org/10.1007/978-3-031-56060-6_5) |  | 0 | Online news platforms often use personalized news recommendation methods to help users discover articles that align with their interests. These methods typically predict a matching score between a user and a candidate article to reflect the user's preference for the article. Given that articles contain rich textual information, current news recommendation systems (RS) leverage natural language processing (NLP) techniques, including the attention mechanism, to capture users' interests based on their historical behaviors and comprehend article content. However, these existing model architectures are usually task-specific and require redesign to adapt to additional features or new tasks. Motivated by the substantial progress in pre-trained large language models for semantic understanding and prompt learning, which involves guiding output generation using pre-trained language models, this paper proposes Prompt-based Generative News Recommendation (PGNR). This approach treats personalized news recommendation as a text-to-text generation task and designs personalized prompts to adapt to the pre-trained language model, taking the generative training and inference paradigm that directly generates the answer for recommendation. Experimental studies using the Microsoft News dataset show that PGNR is capable of making accurate recommendations by taking into account various lengths of past behaviors of different users. It can also easily integrate new features without changing the model architecture and the training loss function. Additionally, PGNR can make recommendations based on users' specific requirements, allowing more straightforward human-computer interaction for news recommendation. | Xinyi Li, Yongfeng Zhang, Edward C. Malthouse | Rutgers State Univ, Piscataway, NJ USA; Northwestern Univ, Evanston, IL 60208 USA |
|  |  [CaseGNN: Graph Neural Networks for Legal Case Retrieval with Text-Attributed Graphs](https://doi.org/10.1007/978-3-031-56060-6_6) |  | 0 | Legal case retrieval is an information retrieval task in the legal domain, which aims to retrieve relevant cases with a given query case. Recent research of legal case retrieval mainly relies on traditional bag-of-words models and language models. Although these methods have achieved significant improvement in retrieval accuracy, there are still two challenges: (1) Legal structural information neglect. Previous neural legal case retrieval models mostly encode the unstructured raw text of case into a case representation, which causes the lack of important legal structural information in a case and leads to poor case representation; (2) Lengthy legal text limitation. When using the powerful BERT-based models, there is a limit of input text lengths, which inevitably requires to shorten the input via truncation or division with a loss of legal context information. In this paper, a graph neural networks-based legal case retrieval model, CaseGNN, is developed to tackle these challenges. To effectively utilise the legal structural information during encoding, a case is firstly converted into a Text-Attributed Case Graph (TACG), followed by a designed Edge Graph Attention Layer and a readout function to obtain the case graph representation. The CaseGNN model is optimised with a carefully designed contrastive loss with easy and hard negative sampling. Since the text attributes in the case graph come from individual sentences, the restriction of using language models is further avoided without losing the legal context. Extensive experiments have been conducted on two benchmarks from COLIEE 2022 and COLIEE 2023, which demonstrate that CaseGNN outperforms other state-of-the-art legal case retrieval methods. The code has been released on https://github.com/yanran-tang/CaseGNN. | Yanran Tang, Ruihong Qiu, Yilun Liu, Xue Li, Zi Huang |  |
|  |  [Context-Driven Interactive Query Simulations Based on Generative Large Language Models](https://doi.org/10.1007/978-3-031-56060-6_12) |  | 0 | Simulating user interactions enables a more user-oriented evaluation of information retrieval (IR) systems. While user simulations are cost-efficient and reproducible, many approaches often lack fidelity regarding real user behavior. Most notably, current user models neglect the user's context, which is the primary driver of perceived relevance and the interactions with the search results. To this end, this work introduces the simulation of context-driven query reformulations. The proposed query generation methods build upon recent Large Language Model (LLM) approaches and consider the user's context throughout the simulation of a search session. Compared to simple context-free query generation approaches, these methods show better effectiveness and allow the simulation of more efficient IR sessions. Similarly, our evaluations consider more interaction context than current session-based measures and reveal interesting complementary insights in addition to the established evaluation protocols. We conclude with directions for future work and provide an entirely open experimental setup. | Björn Engelmann, Timo Breuer, Jana Isabelle Friese, Philipp Schaer, Norbert Fuhr |  |
|  |  [Emotional Insights for Food Recommendations](https://doi.org/10.1007/978-3-031-56060-6_16) |  | 0 | Food recommendation systems have become pivotal in offering personalized suggestions, enabling users to discover recipes in line with their tastes. However, despite the existence of numerous such systems, there are still unresolved challenges. Much of the previous research predominantly lies on users' past preferences, neglecting the significant aspect of discerning users' emotional insights. Our framework aims to bridge this gap by pioneering emotion-aware food recommendation. The study strives for enhanced accuracy by delivering recommendations tailored to a broad spectrum of emotional and dietary behaviors. Uniquely, we introduce five novel scores for Influencer-Followers, Visual Motivation, Adventurous, Health and Niche to gauge a user's inclination toward specific emotional insights. Subsequently, these indices are used to re-rank the preliminary recommendation, placing a heightened focus on the user's emotional disposition. Experimental results on a real-world food social network dataset reveal that our system outperforms alternative emotion-unaware recommender systems, yielding an average performance boost of roughly 6%. Furthermore, the results reveal a rise of over 30% in accuracy metrics for some users exhibiting particular emotional insights. | Mehrdad Rostami, Ali Vardasbi, Mohammad Aliannejadi, Mourad Oussalah | Univ Amsterdam, Informat Retrieval Lab, Amsterdam, Netherlands; Univ Oulu, Ctr Machine Vis & Signal Anal, Oulu, Finland |
|  |  [LaQuE: Enabling Entity Search at Scale](https://doi.org/10.1007/978-3-031-56060-6_18) |  | 0 | Entity search plays a crucial role in various information access domains, where users seek information about specific entities. Despite significant research efforts to improve entity search methods, the availability of large-scale resources and extensible frameworks has been limiting progress. In this work, we present LaQuE (Large-scale Queries for Entity search), a curated framework for entity search, which includes a reproducible and extensible code base as well as a large relevance judgment collection consisting of real-user queries based on the ORCAS collection. LaQuE is industry-scale and suitable for training complex neural models for entity search. We develop methods for curating and judging entity collections, as well as training entity search methods based on LaQuE. We additionally establish strong baselines within LaQuE based on various retrievers, including traditional bag-of-words-based methods and neural-based models. We show that training neural entity search models on LaQuE enhances retrieval effectiveness compared to the state-of-the-art. Additionally, we categorize the released queries in LaQuE based on their popularity and difficulty, encouraging research on more challenging queries for the entity search task. We publicly release LaQuE at https://github.com/Narabzad/LaQuE . | Negar Arabzadeh, Amin Bigdeli, Ebrahim Bagheri | Univ Waterloo, Waterloo, ON, Canada; Toronto Metropolitan Univ, Toronto, ON, Canada |
|  |  [Analyzing Adversarial Attacks on Sequence-to-Sequence Relevance Models](https://doi.org/10.1007/978-3-031-56060-6_19) |  | 0 | Modern sequence-to-sequence relevance models like monoT5 can effectively capture complex textual interactions between queries and documents through cross-encoding. However, the use of natural language tokens in prompts, such as Query, Document, and Relevant for monoT5, opens an attack vector for malicious documents to manipulate their relevance score through prompt injection, e.g., by adding target words such as true. Since such possibilities have not yet been considered in retrieval evaluation, we analyze the impact of query-independent prompt injection via manually constructed templates and LLM-based rewriting of documents on several existing relevance models. Our experiments on the TREC Deep Learning track show that adversarial documents can easily manipulate different sequence-to-sequence relevance models, while BM25 (as a typical lexical model) is not affected. Remarkably, the attacks also affect encoder-only relevance models (which do not rely on natural language prompt tokens), albeit to a lesser extent. | Andrew Parry, Maik Fröbe, Sean MacAvaney, Martin Potthast, Matthias Hagen |  |
|  |  [Two-Step SPLADE: Simple, Efficient and Effective Approximation of SPLADE](https://doi.org/10.1007/978-3-031-56060-6_23) |  | 0 | Learned sparse models such as SPLADE have successfully shown how to incorporate the benefits of state-of-the-art neural information retrieval models into the classical inverted index data structure. Despite their improvements in effectiveness, learned sparse models are not as efficient as classical sparse model such as BM25. The problem has been investigated and addressed by recently developed strategies, such as guided traversal query processing and static pruning, with different degrees of success on in-domain and out-of-domain datasets. In this work, we propose a new query processing strategy for SPLADE based on a two-step cascade. The first step uses a pruned and reweighted version of the SPLADE sparse vectors, and the second step uses the original SPLADE vectors to re-score a sample of documents retrieved in the first stage. Our extensive experiments, performed on 30 different in-domain and out-of-domain datasets, show that our proposed strategy is able to improve mean and tail response times over the original single-stage SPLADE processing by up to 30× and 40×, respectively, for in-domain datasets, and by 12x to 25x, for mean response on out-of-domain datasets, while not incurring in statistical significant difference in 60% of datasets. | Carlos Lassance, Hervé Déjean, Stéphane Clinchant, Nicola Tonellotto |  |
|  |  [Adapting Standard Retrieval Benchmarks to Evaluate Generated Answers](https://doi.org/10.1007/978-3-031-56060-6_26) |  | 0 | Large language models can now directly generate answers to many factual questions without referencing external sources. Unfortunately, relatively little attention has been paid to methods for evaluating the quality and correctness of these answers, for comparing the performance of one model to another, or for comparing one prompt to another. In addition, the quality of generated answers are rarely directly compared to the quality of retrieved answers. As models evolve and prompts are modified, we have no systematic way to measure improvements without resorting to expensive human judgments. To address this problem we adapt standard retrieval benchmarks to evaluate answers generated by large language models. Inspired by the BERTScore metric for summarization, we explore two approaches. In the first, we base our evaluation on the benchmark relevance judgments. We empirically run experiments on how information retrieval relevance judgments can be utilized as an anchor to evaluating the generated answers. In the second, we compare generated answers to the top results retrieved by a diverse set of retrieval models, ranging from traditional approaches to advanced methods, allowing us to measure improvements without human judgments. In both cases, we measure the similarity between an embedded representation of the generated answer and an embedded representation of a known, or assumed, relevant passage from the retrieval benchmark. | Negar Arabzadeh, Amin Bigdeli, Charles L. A. Clarke |  |
|  |  [Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control](https://doi.org/10.1007/978-3-031-56060-6_29) |  | 0 | Learned sparse retrieval (LSR) is a family of neural methods that encode queries and documents into sparse lexical vectors that can be indexed and retrieved efficiently with an inverted index. We explore the application of LSR to the multi-modal domain, with a focus on text-image retrieval. While LSR has seen success in text retrieval, its application in multimodal retrieval remains underexplored. Current approaches like LexLIP and STAIR require complex multi-step training on massive datasets. Our proposed approach efficiently transforms dense vectors from a frozen dense model into sparse lexical vectors. We address issues of high dimension co-activation and semantic deviation through a new training algorithm, using Bernoulli random variables to control query expansion. Experiments with two dense models (BLIP, ALBEF) and two datasets (MSCOCO, Flickr30k) show that our proposed algorithm effectively reduces co-activation and semantic deviation. Our best-performing sparsified model outperforms state-of-the-art text-image LSR models with a shorter training time and lower GPU memory requirements. Our approach offers an effective solution for training LSR retrieval models in multimodal settings. Our code and model checkpoints are available at github.com/thongnt99/lsr-multimodal | Thong Nguyen, Mariya Hendriksen, Andrew Yates, Maarten de Rijke |  |
|  |  [Alleviating Confounding Effects with Contrastive Learning in Recommendation](https://doi.org/10.1007/978-3-031-56060-6_30) |  | 0 | Recently, there has been a growing interest in mitigating the bias effects in recommendations using causal inference. However, Rubin's potential outcome framework may produce inaccurate estimates in real-world scenarios due to the presence of hidden confounders. In addition, existing works adopting the Pearl causal graph framework tend to focus on specific types of bias (e.g., selection bias, popularity bias, exposure bias) instead of directly mitigating the impact of hidden confounders. Motivated by the aforementioned limitations, in this paper, we formulate the recommendation task as a causal graph with unobserved/unmeasurable confounders. We present a novel causality-based architecture called Multi-behavior Debiased Contrastive Collaborative Filtering (MDCCL) and apply the front-door adjustment for intervention. We leverage a pre-like behavior such as clicking an item (i.e., a behavior occurred before the target behavior such as purchasing) to mitigate the bias effects. Additionally, we design a contrastive loss that also provides a debiasing effect benefiting the recommendation. An empirical study on three real-world datasets validates that our proposed method successfully outperforms nine state-of-the-art baselines. Code and the datasets will be available at https://github.com/queenjocey/MDCCL . | Di You, Kyumin Lee | Worcester Polytech Inst, Worcester, MA 01609 USA |
|  |  [Align MacridVAE: Multimodal Alignment for Disentangled Recommendations](https://doi.org/10.1007/978-3-031-56027-9_5) |  | 0 | Explaining why items are recommended to users is challenging, especially when these items are described by multimodal data. Most recommendation systems fail to leverage more than one modality, preferring textual or tabular data. In this work, a new model, Align MacridVAE, that considers the complementarity of visual and textual item descriptions for item recommendation is proposed. This model projects both modalities onto a shared latent space, and a dedicated loss function aligns the text and image of the same item. The aspects of the item are then jointly disentangled for both modalities at a macro level to learn interpretable categorical information about items and at a micro level to model user preferences on each of those categories. Experiments are conducted on six item recommendation datasets, and recommendation performance is compared against multiple baseline methods. The results demonstrate that our model increases recommendation accuracy by 18% in terms of NCDG on average in the studied datasets and allows us to visualise user preference by item aspect across modalities and the learned concept allocation (The code implementation is available at https://github.com/igui/Align-MacridVAE ). | Ignacio Avas, Liesbeth Allein, Katrien Laenen, MarieFrancine Moens | Katholieke Univ Leuven, Dept Comp Sci, Leuven, Belgium |
|  |  [Learning Action Embeddings for Off-Policy Evaluation](https://doi.org/10.1007/978-3-031-56027-9_7) |  | 0 | Off-policy evaluation (OPE) methods allow us to compute the expected reward of a policy by using the logged data collected by a different policy. OPE is a viable alternative to running expensive online A/B tests: it can speed up the development of new policies, and reduces the risk of exposing customers to suboptimal treatments. However, when the number of actions is large, or certain actions are under-explored by the logging policy, existing estimators based on inverse-propensity scoring (IPS) can have a high or even infinite variance. Saito and Joachims (arXiv:2202.06317v2 [cs.LG]) propose marginalized IPS (MIPS) that uses action embeddings instead, which reduces the variance of IPS in large action spaces. MIPS assumes that good action embeddings can be defined by the practitioner, which is difficult to do in many real-world applications. In this work, we explore learning action embeddings from logged data. In particular, we use intermediate outputs of a trained reward model to define action embeddings for MIPS. This approach extends MIPS to more applications, and in our experiments improves upon MIPS with pre-defined embeddings, as well as standard baselines, both on synthetic and real-world data. Our method does not make assumptions about the reward model class, and supports using additional action information to further improve the estimates. The proposed approach presents an appealing alternative to DR for combining the low variance of DM with the low bias of IPS. | Matej Cief, Jacek Golebiowski, Philipp Schmidt, Ziawasch Abedjan, Artur Bekasov |  |
|  |  [Simulated Task Oriented Dialogues for Developing Versatile Conversational Agents](https://doi.org/10.1007/978-3-031-56027-9_10) |  | 0 | Task-Oriented Dialogue (TOD) Systems are increasingly important for managing a variety of daily tasks, yet often underperform in unfamiliar scenarios due to limitations in existing training datasets. This study addresses the challenge of generating robust and versatile TOD systems by transforming instructional task descriptions into natural user-system dialogues to serve as enhanced pre-training data. We explore three strategies for synthetic dialogue generation: crowdsourcing, encoder-decoder models, and in-context learning with large language models. The evaluation of these approaches, based on a comprehensive user study employing 10 different metrics, reveals the top quality of the dialogues generated by learning an encoder-decoder model as per human evaluation. Notably, employing this synthetic dialogue further improves the performance of advanced TOD models, especially in unfamiliar domains, with improvements spanning 5.5% to as much as 20.9% in combined evaluation scores. Our findings advocate for the use of specialised, task-oriented knowledge bases and step-wise dialogue generation techniques to advance the capabilities and generalizability of TOD systems. | Xi Wang, Procheta Sen, Ruizhe Li, Emine Yilmaz | Univ Aberdeen, Aberdeen, Scotland; Univ Liverpool, Liverpool, Merseyside, England; Unvers Coll London, London, England |
|  |  [Hypergraphs with Attention on Reviews for Explainable Recommendation](https://doi.org/10.1007/978-3-031-56027-9_14) |  | 0 | Given a recommender system based on reviews, the challenges are how to effectively represent the review data and how to explain the produced recommendations. We propose a novel review-specific Hypergraph (HG) model, and further introduce a model-agnostic explainability module. The HG model captures high-order connections between users, items, aspects, and opinions while maintaining information about the review. The explainability module can use the HG model to explain a prediction generated by any model. We propose a path-restricted review-selection method biased by the user preference for item reviews and propose a novel explanation method based on a review graph. Experiments on real-world datasets confirm the ability of the HG model to capture appropriate explanations. | Theis E. Jendal, TrungHoang Le, Hady W. Lauw, Matteo Lissandrini, Peter Dolog, Katja Hose | Aalborg Univ, Aalborg, Denmark; Singapore Management Univ, Singapore, Singapore |
|  |  [Investigating the Usage of Formulae in Mathematical Answer Retrieval](https://doi.org/10.1007/978-3-031-56027-9_15) |  | 0 | This work focuses on the task of Mathematical Answer Retrieval and studies the factors a recent Transformer-Encoder-based Language Model (LM) uses to assess the relevance of an answer for a given mathematical question. Mainly, we investigate three factors: (1) the general influence of mathematical formulae, (2) the usage of structural information of those formulae, (3) the overlap of variable names in answers and questions. The findings of the investigation indicate that the LM for Mathematical Answer Retrieval mainly relies on shallow features such as the overlap of variables between question and answers. Furthermore, we identified a malicious shortcut in the training data that hinders the usage of structural information and by removing this shortcut improved the overall accuracy. We want to foster future research on how LMs are trained for Mathematical Answer Retrieval and provide a basic evaluation set up (Link to repository: https://github.com/AnReu/math_analysis ) for existing models. | Anja Reusch, Julius Gonsior, Claudio Hartmann, Wolfgang Lehner | Tech Univ Dresden, Dresden Database Res Grp, Dresden, Germany |
|  |  [Empowering Legal Citation Recommendation via Efficient Instruction-Tuning of Pre-trained Language Models](https://doi.org/10.1007/978-3-031-56027-9_19) |  | 0 | The escalating volume of cases in legal adjudication has amplified the complexity of citing relevant regulations and authoritative cases, posing an increasing challenge for legal professionals. Current legal citation prediction methods, which are predominantly reliant on keyword or interest-based retrieval, are proving insufficient. In particular, Collaborative Filtering (CF) based legal recommendation methods exhibited low accuracy. In response to these challenges, we propose the Instruction GPT with Low-Rank Adaptation architecture (IGPT-LoRA), aiming to enhance the performance of legal citation recommendations and reduce computational demands by tuning Pre-trained Language Models (PLMs). IGPT-LoRA leverages prompting and efficient tuning strategies, thus offering a significant improvement over previous context-aware legal citation prediction methods. We design effective domain-specific instruction templates to guide the adaptation of PLMs for recommendation purposes, shedding light on the potential of prompt-based learning in the legal domain. Furthermore, we optimize the learning process with an efficient tuning layer - the Low-Rank Adaptation (LoRA) architecture - to bolster applicability. Experimental results on a real-world legal data set (BVA) demonstrate that IGPT-LoRA outperforms state-of-the-art methods, delivering substantial improvements in accuracy and also in training time and computational efficiency. | Jie Wang, Kanha Bansal, Ioannis Arapakis, Xuri Ge, Joemon M. Jose | Univ Glasgow, Glasgow, Lanark, Scotland; Telefon Res, Barcelona, Spain |
|  |  [Fine-Tuning CLIP via Explainability Map Propagation for Boosting Image and Video Retrieval](https://doi.org/10.1007/978-3-031-56027-9_22) |  | 0 | Recent studies have highlighted the remarkable performance of CLIP for diverse downstream tasks. To understand how CLIP performs these tasks, various explainability methods have been formulated. In this paper, we reveal that the explainability maps associated with CLIP are often focused on a limited portion of the image and overlook objects that are explicitly mentioned in the text. This phenomenon may result in a high similarity score for incongruent image-text pairs, thereby potentially introducing a bias. To address this issue, we introduce a novel fine-tuning technique for CLIP that leverages a transformer explainability method. Unlike traditional approaches that generate a single heatmap using an image-text pair, our method produces multiple heatmaps directly from the image itself. We use these heatmaps both during the fine-tuning process and at inference time to highlight key visual elements, applying them to the features during the image encoding process, steering the visual encoder's attention toward these key elements. This process guides the image encoder across different spatial regions and generates a set of visual embeddings, thereby allowing the model to consider various aspects of the image, ensuring a detailed and comprehensive understanding that surpasses the limited scope of the original CLIP model. Our method leads to a notable improvement in text, image, and video retrieval across multiple benchmarks. It also results in reduced gender bias, making our model more equitable. | Yoav Shalev, Lior Wolf | Tel Aviv Univ, Tel Aviv, Israel |
|  |  [Cross-Modal Retrieval for Knowledge-Based Visual Question Answering](https://doi.org/10.1007/978-3-031-56027-9_26) |  | 0 | Knowledge-based Visual Question Answering about Named Entities is a challenging task that requires retrieving information from a multimodal Knowledge Base. Named entities have diverse visual representations and are therefore difficult to recognize. We argue that cross-modal retrieval may help bridge the semantic gap between an entity and its depictions, and is foremost complementary with mono-modal retrieval. We provide empirical evidence through experiments with a multimodal dual encoder, namely CLIP, on the recent ViQuAE, InfoSeek, and Encyclopedic-VQA datasets. Additionally, we study three different strategies to fine-tune such a model: mono-modal, cross-modal, or joint training. Our method, which combines mono-and cross-modal retrieval, is competitive with billion-parameter models on the three datasets, while being conceptually simpler and computationally cheaper. | Paul Lerner, Olivier Ferret, Camille Guinaudeau |  |
|  |  [Learning to Jointly Transform and Rank Difficult Queries](https://doi.org/10.1007/978-3-031-56066-8_5) |  | 0 | Recent empirical studies have shown that while neural rankers exhibit increasingly higher retrieval effectiveness on tasks such as ad hoc retrieval, these improved performances are not experienced uniformly across the range of all queries. There are typically a large subset of queries that are not satisfied by neural rankers. These queries are often referred to as difficult queries. Given the fact that neural rankers operate based on the similarity between the embedding representations of queries and their relevant documents, the poor performance of difficult queries can be due to the sub-optimal representations learnt for difficult queries. As such, the objective of our work in this paper is to learn to rank documents and also transform query representations in tandem such that the representation of queries are transformed into one that shows higher resemblance to their relevant document. This way, our method will provide the opportunity to satisfy a large number of difficult queries that would otherwise not be addressed. In order to learn to jointly rank documents and transform queries, we propose to integrate two forms of triplet loss functions into neural rankers such that they ensure that each query is moved along the embedding space, through the transformation of its embedding representation, in order to be placed close to its relevant document(s). We perform experiments based on the MS MARCO passage ranking task and show that our proposed method has been able to show noticeable performance improvement for queries that were extremely difficult for existing neural rankers. On average, our approach has been able to satisfy 277 queries with an MRR@10 of 0.21 for queries that had a reciprocal rank of zero on the initial neural ranker. | Amin Bigdeli, Negar Arabzadeh, Ebrahim Bagheri | Univ Waterloo, Waterloo, ON, Canada; Toronto Metropolitan Univ, Toronto, ON, Canada |
|  |  [Instant Answering in E-Commerce Buyer-Seller Messaging Using Message-to-Question Reformulation](https://doi.org/10.1007/978-3-031-56066-8_7) |  | 0 | E-commerce customers frequently seek detailed product information for purchase decisions, commonly contacting sellers directly with extended queries. This manual response requirement imposes additional costs and disrupts buyer's shopping experience with response time fluctuations ranging from hours to days. We seek to automate buyer inquiries to sellers in a leading e-commerce store using a domain-specific federated Question Answering (QA) system. The main challenge is adapting current QA systems, designed for single questions, to address detailed customer queries. We address this with a low-latency, sequence-to-sequence approach, MESSAGE-TO-QUESTION ( M2Q ). It reformulates buyer messages into succinct questions by identifying and extracting the most salient information from a message. Evaluation against baselines shows that M2Q yields relative increases of 757 answering rate from the federated QA system. Live deployment shows that automatic answering saves sellers from manually responding to millions of messages per year, and also accelerates customer purchase decisions by eliminating the need for buyers to wait for a reply | Besnik Fetahu, Tejas Mehta, Qun Song, Nikhita Vedula, Oleg Rokhlenko, Shervin Malmasi |  |
|  |  [Towards Automated End-to-End Health Misinformation Free Search with a Large Language Model](https://doi.org/10.1007/978-3-031-56066-8_9) |  | 0 | In the information age, health misinformation remains a notable challenge to public welfare. Integral to addressing this issue is the development of search systems adept at identifying and filtering out misleading content. This paper presents the automation of Vera, a state-of-the-art consumer health search system. While Vera can discern articles containing misinformation, it requires expert ground truth answers and rule-based reformulations. We introduce an answer prediction module that integrates GPT x with Vera and a GPT-based query reformulator to yield high-quality stance reformulations and boost downstream retrieval effectiveness. Further, we find that chain-of-thought reasoning is paramount to higher effectiveness. When assessed in the TREC Health Misinformation Track of 2022, our systems surpassed all competitors, including human-in-the-loop configurations, underscoring their pivotal role in the evolution towards a health misinformation-free search landscape. We provide all code necessary to reproduce our results at https://github.com/castorini/pygaggle . | Ronak Pradeep, Jimmy Lin | Univ Waterloo, David R Cheriton Sch Comp Sci, Waterloo, ON, Canada |
|  |  [Reproducibility Analysis and Enhancements for Multi-aspect Dense Retriever with Aspect Learning](https://doi.org/10.1007/978-3-031-56066-8_17) |  | 0 | Multi-aspect dense retrieval aims to incorporate aspect information (e.g., brand and category) into dual encoders to facilitate relevance matching. As an early and representative multi-aspect dense retriever, MADRAL learns several extra aspect embeddings and fuses the explicit aspects with an implicit aspect "OTHER" for final representation. MADRAL was evaluated on proprietary data and its code was not released, making it challenging to validate its effectiveness on other datasets. We failed to reproduce its effectiveness on the public MA-Amazon data, motivating us to probe the reasons and re-examine its components. We propose several component alternatives for comparisons, including replacing "OTHER" with "CLS" and representing aspects with the first several content tokens. Through extensive experiments, we confirm that learning "OTHER" from scratch in aspect fusion is harmful. In contrast, our proposed variants can greatly enhance the retrieval performance. Our research not only sheds light on the limitations of MADRAL but also provides valuable insights for future studies on more powerful multi-aspect dense retrieval models. Code will be released at: https://github.com/sunxiaojie99/Reproducibility-for-MADRAL. | Keping Bi, Xiaojie Sun, Jiafeng Guo, Xueqi Cheng |  |
|  |  [An Empirical Analysis of Intervention Strategies' Effectiveness for Countering Misinformation Amplification by Recommendation Algorithms](https://doi.org/10.1007/978-3-031-56066-8_23) |  | 0 | Social network platforms connect people worldwide, facilitating communication, information sharing, and personal/professional networking. They use recommendation algorithms to personalize content and enhance user experiences. However, these algorithms can unintentionally amplify misinformation by prioritizing engagement over accuracy. For instance, recent works suggest that popularity-based and network-based recommendation algorithms contribute the most to misinformation diffusion. In our study, we present an exploration on two Twitter datasets to understand the impact of intervention techniques on combating misinformation amplification initiated by recommendation algorithms. We simulate various scenarios and evaluate the effectiveness of intervention strategies in social sciences such as Virality Circuit Breakers and accuracy nudges. Our findings highlight that these intervention strategies are generally successful when applied on top of collaborative filtering and content-based recommendation algorithms, while having different levels of effectiveness depending on the number of users keen to spread fake news present in the dataset. | Royal Pathak, Francesca Spezzano | Boise State Univ, Comp Sci Dept, Boise, ID 83725 USA |
|  |  [Not Just Algorithms: Strategically Addressing Consumer Impacts in Information Retrieval](https://doi.org/10.1007/978-3-031-56066-8_25) |  | 0 | Information Retrieval (IR) systems have a wide range of impacts on consumers. We offer maps to help identify goals IR systems could—or should—strive for, and guide the process of scoping how to gauge a wide range of consumer-side impacts and the possible interventions needed to address these effects. Grounded in prior work on scoping algorithmic impact efforts, our goal is to promote and facilitate research that (1) is grounded in impacts on information consumers, contextualizing these impacts in the broader landscape of positive and negative consumer experience; (2) takes a broad view of the possible means of changing or improving that impact, including non-technical interventions; and (3) uses operationalizations and strategies that are well-matched to the technical, social, ethical, legal, and other dimensions of the specific problem in question. | Michael D. Ekstrand, Lex Beattie, Maria Soledad Pera, Henriette Cramer | Delft Univ Technol, Delft, Netherlands; Spotify, Seattle, WA USA; Drexel Univ, Philadelphia, PA 19104 USA |
|  |  [A Study of Pre-processing Fairness Intervention Methods for Ranking People](https://doi.org/10.1007/978-3-031-56066-8_26) |  | 0 | Fairness interventions are hard to use in practice when ranking people due to legal constraints that limit access to sensitive information. Pre-processing fairness interventions, however, can be used in practice to create more fair training data that encourage the model to generate fair predictions without having access to sensitive information during inference. Little is known about the performance of pre-processing fairness interventions in a recruitment setting. To simulate a real scenario, we train a ranking model on pre-processed representations, while access to sensitive information is limited during inference. We evaluate pre-processing fairness intervention methods in terms of individual fairness and group fairness. On two real-world datasets, the pre-processing methods are found to improve the diversity of rankings with respect to gender, while individual fairness is not affected. Moreover, we discuss advantages and disadvantages of using pre-processing fairness interventions in practice for ranking people. | Clara Rus, Andrew Yates, Maarten de Rijke | Univ Amsterdam, Amsterdam, Netherlands |
|  |  [Evaluating the Explainability of Neural Rankers](https://doi.org/10.1007/978-3-031-56066-8_28) |  | 0 | Information retrieval models have witnessed a paradigm shift from unsupervised statistical approaches to feature-based supervised approaches to completely data-driven ones that make use of the pre-training of large language models. While the increasing complexity of the search models have been able to demonstrate improvements in effectiveness (measured in terms of relevance of top-retrieved results), a question worthy of a thorough inspection is - "how explainable are these models?", which is what this paper aims to evaluate. In particular, we propose a common evaluation platform to systematically evaluate the explainability of any ranking model (the explanation algorithm being identical for all the models that are to be evaluated). In our proposed framework, each model, in addition to returning a ranked list of documents, also requires to return a list of explanation units or rationales for each document. This meta-information from each document is then used to measure how locally consistent these rationales are as an intrinsic measure of interpretability - one that does not require manual relevance assessments. Additionally, as an extrinsic measure, we compute how relevant these rationales are by leveraging sub-document level relevance assessments. Our findings show a number of interesting observations, such as sentence-level rationales are more consistent, an increase in complexity mostly leads to less consistent explanations, and that interpretability measures offer a complementary dimension of evaluation of IR systems because consistency is not well-correlated with nDCG at top ranks. | Saran Pandian, Debasis Ganguly, Sean MacAvaney |  |
|  |  [Knowledge Graph Cross-View Contrastive Learning for Recommendation](https://doi.org/10.1007/978-3-031-56063-7_1) |  | 0 | Knowledge Graphs (KGs) are useful side information that help recommendation systems improve recommendation quality by providing rich semantic information about entities and items. Recently, models based on graph neural networks (GNNs) have adopted knowledge graphs to capture further high-order structural information, such as shared preferences between users and similarities between items. However, existing GNN-based methods suffer from two challenges: (1) Sparse supervisory signal, where a large amount of information in the knowledge graph is non-relevant to recommendation, and the training labels are insufficient, thereby limiting the recommendation performance of the trained model; (2) Valuable information is discarded whereby the use by the existing models of edge or node dropout strategies to obtain augmented views during self-supervised learning could lead to valuable information being discarded in recommendation. These two challenges limit the effective representation of users and items by existing methods. Inspired by self-supervised learning to mine supervision signals from data, in this paper, we focus on exploring contrastive learning based on knowledge graph enhancement, and propose a new model named Knowledge Graph Cross-view Contrastive Learning for Recommendation (KGCCL) to address the two challenges. Specifically, to address supervision sparseness, we perform contrastive learning between graph views at different levels and mine graph feature information in a self-supervised learning manner. In addition, we use noise augmentation to enhance the representation of users and items, while retaining all triplet information in the knowledge graph to address the challenge of valuable information being discarded. Experimental results on three public datasets show that our proposed KGCCL model outperforms existing state-of-the-art methods. In particular, our model outperforms the best baseline performance by 10.65% on the MIND dataset. | Zeyuan Meng, Iadh Ounis, Craig Macdonald, Zixuan Yi | Univ Glasgow, Glasgow, Lanark, Scotland |
|  |  [Recommendation Fairness in eParticipation: Listening to Minority, Vulnerable and NIMBY Citizens](https://doi.org/10.1007/978-3-031-56066-8_31) |  | 0 | E-participation refers to the use of digital technologies and online platforms to engage citizens and other stakeholders in democratic and government decision-making processes. Recent research work has explored the application of recommender systems to e-participation, focusing on the development of algorithmic solutions to be effective in terms of personalized content retrieval accuracy, but ignoring underlying societal issues, such as biases, fairness, privacy and transparency. Motivated by this research gap, on a public e-participatory budgeting dataset, we measure and analyze recommendation fairness metrics oriented to several minority, vulnerable and NIMBY (Not In My Back Yard) groups of citizens. Our empirical results show that there is a strong popularity bias (especially for the minority groups) due to how content is presented and accessed in a reference e-participation platform; and that hybrid algorithms exploiting user geolocation information in a collaborative filtering fashion are good candidates to satisfy the proposed fairness conceptualization for the above underrepresented citizen collectives. | Marina AlonsoCortés, Iván Cantador, Alejandro Bellogín | Univ Autonoma Madrid, Escuela Politecn Super, Madrid 28049, Spain |
|  |  [Responsible Opinion Formation on Debated Topics in Web Search](https://doi.org/10.1007/978-3-031-56066-8_32) |  | 0 | Web search has evolved into a platform people rely on for opinion formation on debated topics. Yet, pursuing this search intent can carry serious consequences for individuals and society and involves a high risk of biases. We argue that web search can and should empower users to form opinions responsibly and that the information retrieval community is uniquely positioned to lead interdisciplinary efforts to this end. Building on digital humanism—a perspective focused on shaping technology to align with human values and needs—and through an extensive interdisciplinary literature review, we identify challenges and research opportunities that focus on the searcher, search engine, and their complex interplay. We outline a research agenda that provides a foundation for research efforts toward addressing these challenges. | Alisa Rieger, Tim Draws, Nicolas Mattis, David Maxwell, David Elsweiler, Ujwal Gadiraju, Dana McKay, Alessandro Bozzon, Maria Soledad Pera | RMIT Univ Melbourne, Melbourne, Australia; Univ Regensburg, Regensburg, Germany; Booking com, Amsterdam, Netherlands; OTTO, Hamburg, Germany; Vrije Univ Amsterdam, Amsterdam, Netherlands; Delft Univ Technol, Delft, Netherlands |
|  |  [Is Google Getting Worse? A Longitudinal Investigation of SEO Spam in Search Engines](https://doi.org/10.1007/978-3-031-56063-7_4) |  | 0 | Many users of web search engines have been complaining in recent years about the supposedly decreasing quality of search results. This is often attributed to an increasing amount of search-engine-optimized but low-quality content. Evidence for this has always been anecdotal, yet it's not unreasonable to think that popular online marketing strategies such as affiliate marketing incentivize the mass production of such content to maximize clicks. Since neither this complaint nor affiliate marketing as such have received much attention from the IR community, we hereby lay the groundwork by conducting an in-depth exploratory study of how affiliate content affects today's search engines. We monitored Google, Bing and DuckDuckGo for a year on 7,392 product review queries. Our findings suggest that all search engines have significant problems with highly optimized (affiliate) content—more than is representative for the entire web according to a baseline retrieval system on the ClueWeb22. Focussing on the product review genre, we find that only a small portion of product reviews on the web uses affiliate marketing, but the majority of all search results do. Of all affiliate networks, Amazon Associates is by far the most popular. We further observe an inverse relationship between affiliate marketing use and content complexity, and that all search engines fall victim to large-scale affiliate link spam campaigns. However, we also notice that the line between benign content and spam in the form of content and link farms becomes increasingly blurry—a situation that will surely worsen in the wake of generative AI. We conclude that dynamic adversarial spam in the form of low-quality, mass-produced commercial content deserves more attention. (Code and data: https://github.com/webis-de/ECIR-24 ). | Janek Bevendorff, Matti Wiegmann, Martin Potthast, Benno Stein | Univ Leipzig, Leipzig, Germany; Bauhaus Univ Weimar, Weimar, Germany |
|  |  [Robustness in Fairness Against Edge-Level Perturbations in GNN-Based Recommendation](https://doi.org/10.1007/978-3-031-56063-7_3) |  | 0 | Efforts in the recommendation community are shifting from the sole emphasis on utility to considering beyond-utility factors, such as fairness and robustness. Robustness of recommendation models is typically linked to their ability to maintain the original utility when subjected to attacks. Limited research has explored the robustness of a recommendation model in terms of fairness, e.g., the parity in performance across groups, under attack scenarios. In this paper, we aim to assess the robustness of graph-based recommender systems concerning fairness, when exposed to attacks based on edge-level perturbations. To this end, we considered four different fairness operationalizations, including both consumer and provider perspectives. Experiments on three datasets shed light on the impact of perturbations on the targeted fairness notion, uncovering key shortcomings in existing evaluation protocols for robustness. As an example, we observed perturbations affect consumer fairness on a higher extent than provider fairness, with alarming unfairness for the former. Source code: https://github.com/jackmedda/CPFairRobust | Ludovico Boratto, Francesco Fabbri, Gianni Fenu, Mirko Marras, Giacomo Medda |  |
|  |  [Shallow Cross-Encoders for Low-Latency Retrieval](https://doi.org/10.1007/978-3-031-56063-7_10) |  | 0 | Transformer-based Cross-Encoders achieve state-of-the-art effectiveness in text retrieval. However, Cross-Encoders based on large transformer models (such as BERT or T5) are computationally expensive and allow for scoring only a small number of documents within a reasonably small latency window. However, keeping search latencies low is important for user satisfaction and energy usage. In this paper, we show that weaker shallow transformer models (i.e., transformers with a limited number of layers) actually perform better than full-scale models when constrained to these practical low-latency settings since they can estimate the relevance of more documents in the same time budget. We further show that shallow transformers may benefit from the generalized Binary Cross-Entropy (gBCE) training scheme, which has recently demonstrated success for recommendation tasks. Our experiments with TREC Deep Learning passage ranking query sets demonstrate significant improvements in shallow and full-scale models in low-latency scenarios. For example, when the latency limit is 25ms per query, MonoBERT-Large (a cross-encoder based on a full-scale BERT model) is only able to achieve NDCG@10 of 0.431 on TREC DL 2019, while TinyBERT-gBCE (a cross-encoder based on TinyBERT trained with gBCE) reaches NDCG@10 of 0.652, a +51 Cross-Encoders are effective even when used without a GPU (e.g., with CPU inference, NDCG@10 decreases only by 3 latency), which makes Cross-Encoders practical to run even without specialized hardware acceleration. | Aleksandr V. Petrov, Sean MacAvaney, Craig Macdonald |  |
|  |  [Improved Learned Sparse Retrieval with Corpus-Specific Vocabularies](https://doi.org/10.1007/978-3-031-56063-7_12) |  | 0 | We explore leveraging corpus-specific vocabularies that improve both efficiency and effectiveness of learned sparse retrieval systems. We find that pre-training the underlying BERT model on the target corpus, specifically targeting different vocabulary sizes incorporated into the document expansion process, improves retrieval quality by up to 12% while in some scenarios decreasing latency by up to 50%. Our experiments show that adopting corpus-specific vocabulary and increasing vocabulary size decreases average postings list length which in turn reduces latency. Ablation studies show interesting interactions between custom vocabularies, document expansion techniques, and sparsification objectives of sparse models. Both effectiveness and efficiency improvements transfer to different retrieval approaches such as uniCOIL and SPLADE and offer a simple yet effective approach to providing new efficiency-effectiveness trade-offs for learned sparse retrieval systems. | Puxuan Yu, Antonio Mallia, Matthias Petri |  |
|  |  [An Adaptive Framework of Geographical Group-Specific Network on O2O Recommendation](https://doi.org/10.1007/978-3-031-56063-7_19) |  | 0 | Online to offline recommendation strongly correlates with the user and service's spatiotemporal information, therefore calling for a higher degree of model personalization. The traditional methodology is based on a uniform model structure trained by collected centralized data, which is unlikely to capture all user patterns over different geographical areas or time periods. To tackle this challenge, we propose a geographical group-specific modeling method called GeoGrouse, which simultaneously studies the common knowledge as well as group-specific knowledge of user preferences. An automatic grouping paradigm is employed and verified based on users' geographical grouping indicators. Offline and online experiments are conducted to verify the effectiveness of our approach, and substantial business improvement is achieved. | Luo Ji, Jiayu Mao, Hailong Shi, Qian Li, Yunfei Chu, Hongxia Yang |  |
|  |  [GenQREnsemble: Zero-Shot LLM Ensemble Prompting for Generative Query Reformulation](https://doi.org/10.1007/978-3-031-56063-7_24) |  | 0 | Query Reformulation(QR) is a set of techniques used to transform a user's original search query to a text that better aligns with the user's intent and improves their search experience. Recently, zero-shot QR has been shown to be a promising approach due to its ability to exploit knowledge inherent in large language models. By taking inspiration from the success of ensemble prompting strategies which have benefited many tasks, we investigate if they can help improve query reformulation. In this context, we propose an ensemble based prompting technique, GenQREnsemble which leverages paraphrases of a zero-shot instruction to generate multiple sets of keywords ultimately improving retrieval performance. We further introduce its post-retrieval variant, GenQREnsembleRF to incorporate pseudo relevant feedback. On evaluations over four IR benchmarks, we find that GenQREnsemble generates better reformulations with relative nDCG@10 improvements up to 18 the previous zero-shot state-of-art. On the MSMarco Passage Ranking task, GenQREnsembleRF shows relative gains of 5 and 9 | Kaustubh D. Dhole, Eugene Agichtein |  |
|  |  [Improving the Robustness of Dense Retrievers Against Typos via Multi-Positive Contrastive Learning](https://doi.org/10.1007/978-3-031-56063-7_21) |  | 0 | Dense retrieval has become the new paradigm in passage retrieval. Despite its effectiveness on typo-free queries, it is not robust when dealing with queries that contain typos. Current works on improving the typo-robustness of dense retrievers combine (i) data augmentation to obtain the typoed queries during training time with (ii) additional robustifying subtasks that aim to align the original, typo-free queries with their typoed variants. Even though multiple typoed variants are available as positive samples per query, some methods assume a single positive sample and a set of negative ones per anchor and tackle the robustifying subtask with contrastive learning; therefore, making insufficient use of the multiple positives (typoed queries). In contrast, in this work, we argue that all available positives can be used at the same time and employ contrastive learning that supports multiple positives (multi-positive). Experimental results on two datasets show that our proposed approach of leveraging all positives simultaneously and employing multi-positive contrastive learning on the robustifying subtask yields improvements in robustness against using contrastive learning with a single positive. | Georgios Sidiropoulos, Evangelos Kanoulas |  |
|  |  [Towards Reliable and Factual Response Generation: Detecting Unanswerable Questions in Information-Seeking Conversations](https://doi.org/10.1007/978-3-031-56063-7_25) |  | 0 | Generative AI models face the challenge of hallucinations that can undermine users' trust in such systems. We approach the problem of conversational information seeking as a two-step process, where relevant passages in a corpus are identified first and then summarized into a final system response. This way we can automatically assess if the answer to the user's question is present in the corpus. Specifically, our proposed method employs a sentence-level classifier to detect if the answer is present, then aggregates these predictions on the passage level, and eventually across the top-ranked passages to arrive at a final answerability estimate. For training and evaluation, we develop a dataset based on the TREC CAsT benchmark that includes answerability labels on the sentence, passage, and ranking levels. We demonstrate that our proposed method represents a strong baseline and outperforms a state-of-the-art LLM on the answerability prediction task. | Weronika Lajewska, Krisztian Balog |  |
|  |  [On the Influence of Reading Sequences on Knowledge Gain During Web Search](https://doi.org/10.1007/978-3-031-56063-7_28) |  | 0 | Nowadays, learning increasingly involves the usage of search engines and web resources. The related interdisciplinary research field search as learning aims to understand how people learn on the web. Previous work has investigated several feature classes to predict, for instance, the expected knowledge gain during web search. Therein, eye-tracking features have not been extensively studied so far. In this paper, we extend a previously used reading model from a line-based one to one that can detect reading sequences across multiple lines. We use publicly available study data from a web-based learning task to examine the relationship between our feature set and the participants' test scores. Our findings demonstrate that learners with higher knowledge gain spent significantly more time reading, and processing more words in total. We also find evidence that faster reading at the expense of more backward regressions may be an indicator of better web-based learning. We make our code publicly available at https://github.com/TIBHannover/reading_web_search. | Wolfgang Gritz, Anett Hoppe, Ralph Ewerth |  |
|  |  [SPARe: Supercharged Lexical Retrievers on GPU with Sparse Kernels](https://doi.org/10.1007/978-3-031-56063-7_33) |  | 0 | Lexical sparse retrievers, rely on efficient searching algorithms that operate over inverted index structures, tailored specifically for CPU. This CPU-centric design poses a challenge when adapting these algorithms for highly parallel accelerators, such as GPUs, thus deterring potential performance gains. To address this, we propose to leverage the recent advances in sparse computations offered by deep learning frameworks to directly implementing sparse retrievals on these accelerators. This paper presents the SPARe (SPArse Retrievers) Python package, which provides a high-level API to deal with sparse retrievers on (single or multi)-accelerators, by leveraging deep learning frameworks at its core. Experimental results show that SPARe, running on an accessible GPU (RTX 2070), can calculate the BM25 scores for close to 9 million MSMARCO documents at a rate of 800 questions per second with our specialized algorithm. Notably, SPARe proves highly effective for denser LSR indexes, significantly surpassing the performance of established systems such as PISA, Pyserini and PyTerrier. SPARe is publicly available at https://github.com/ieeta-pt/SPARe . | Tiago Almeida, Sérgio Matos | Univ Aveiro, IEETA, DETI, LASI, P-3810193 Aveiro, Portugal |
|  |  [Beneath the [MASK]: An Analysis of Structural Query Tokens in ColBERT](https://doi.org/10.1007/978-3-031-56063-7_35) |  | 0 | ColBERT is a highly effective and interpretable retrieval model based on token embeddings. For scoring, the model adds cosine similarities between the most similar pairs of query and document token embeddings. Previous work on interpreting how tokens affect scoring pay little attention to non-text tokens used in ColBERT such as [MASK]. Using MS MARCO and the TREC 2019-2020 deep passage retrieval task, we show that [MASK] embeddings may be replaced by other query and structural token embeddings to obtain similar effectiveness, and that [Q] and [MASK] are sensitive to token order, while [CLS] and [SEP] are not. | Ben Giacalone, Greg Paiement, Quinn Tucker, Richard Zanibbi | Rochester Inst Technol, Rochester, NY 14623 USA |
|  |  [A Cost-Sensitive Meta-learning Strategy for Fair Provider Exposure in Recommendation](https://doi.org/10.1007/978-3-031-56063-7_36) |  | 0 | When devising recommendation services, it is important to account for the interests of all content providers, encompassing not only newcomers but also minority demographic groups. In various instances, certain provider groups find themselves underrepresented in the item catalog, a situation that can influence recommendation results. Hence, platform owners often seek to regulate the exposure of these provider groups in the recommended lists. In this paper, we propose a novel cost-sensitive approach designed to guarantee these target exposure levels in pairwise recommendation models. This approach quantifies, and consequently mitigate, the discrepancies between the volume of recommendations allocated to groups and their contribution in the item catalog, under the principle of equity. Our results show that this approach, while aligning groups exposure with their assigned levels, does not compromise to the original recommendation utility. Source code and pre-processed data can be retrieved at https://github.com/alessandraperniciano/meta-learning-strategy-fair-provider-exposure. | Ludovico Boratto, Giulia Cerniglia, Mirko Marras, Alessandra Perniciano, Barbara Pes |  |
|  |  [Multiple Testing for IR and Recommendation System Experiments](https://doi.org/10.1007/978-3-031-56063-7_37) |  | 0 | While there has been significant research on statistical techniques for comparing two information retrieval (IR) systems, many IR experiments test more than two systems. This can lead to inflated false discoveries due to the multiple-comparison problem (MCP). A few IR studies have investigated multiple comparison procedures; these studies mostly use TREC data and control the familywise error rate. In this study, we extend their investigation to include recommendation system evaluation data as well as multiple comparison procedures that controls for False Discovery Rate (FDR). | Ngozi Ihemelandu, Michael D. Ekstrand | Boise State Univ, Boise, ID 83725 USA; Drexel Univ, Dept Informat Sci, Philadelphia, PA 19104 USA |
|  |  [An In-Depth Comparison of Neural and Probabilistic Tree Models for Learning-to-rank](https://doi.org/10.1007/978-3-031-56063-7_39) |  | 0 | Learning-to-rank has been intensively studied and has demonstrated significant value in several fields, such as web search and recommender systems. Over the learning-to-rank datasets given as vectors of feature values, LambdaMART proposed more than a decade ago, and its subsequent descendants based on gradient-boosted decision trees (GBDT), have demonstrated leading performance. Recently, different novel tree models have been developed, such as neural tree ensembles that utilize neural networks to emulate decision tree models and probabilistic gradient boosting machines (PGBM). However, the effectiveness of these tree models for learning-to-rank has not been comprehensively explored. Hence, this study bridges the gap by systematically comparing several representative neural tree ensembles (e.g., TabNet, NODE, and GANDALF), PGBM, and traditional learning-to-rank models on two benchmark datasets. The experimental results reveal that benefiting from end-to-end gradient-based optimization and the power of feature representation and adaptive feature selection, the neural tree ensemble does have its advantage for learning-to-rank over the conventional tree-based ranking model, such as LambdaMART. This finding is important as LambdaMART has achieved leading performance in a long period. | Haonan Tan, Kaiyu Yang, Haitao Yu | Univ Tsukuba, Grad Sch Comprehens Human Sci, 1-2 Kasuga, Tsukuba, Ibaraki 3050821, Japan; Univ Tsukuba, Inst Lilbray Informat & Media Sci, 1-2 Kasuga, Tsukuba, Ibaraki 3050821, Japan |
|  |  [GenRec: Large Language Model for Generative Recommendation](https://doi.org/10.1007/978-3-031-56063-7_42) |  | 0 | In recent years, large language models (LLM) have emerged as powerful tools for diverse natural language processing tasks. However, their potential for recommender systems under the generative recommendation paradigm remains relatively unexplored. This paper presents an innovative approach to recommendation systems using large language models (LLMs) based on text data. In this paper, we present a novel LLM for generative recommendation (GenRec) that utilized the expressive power of LLM to directly generate the target item to recommend, rather than calculating ranking score for each candidate item one by one as in traditional discriminative recommendation. GenRec uses LLM's understanding ability to interpret context, learn user preferences, and generate relevant recommendation. Our proposed approach leverages the vast knowledge encoded in large language models to accomplish recommendation tasks. We first we formulate specialized prompts to enhance the ability of LLM to comprehend recommendation tasks. Subsequently, we use these prompts to fine-tune the LLaMA backbone LLM on a dataset of user-item interactions, represented by textual data, to capture user preferences and item characteristics. Our research underscores the potential of LLM-based generative recommendation in revolutionizing the domain of recommendation systems and offers a foundational framework for future explorations in this field. We conduct extensive experiments on benchmark datasets, and the experiments shows that our GenRec has significant better results on large dataset. | Jianchao Ji, Zelong Li, Shuyuan Xu, Wenyue Hua, Yingqiang Ge, Juntao Tan, Yongfeng Zhang |  |
|  |  [News Gathering: Leveraging Transformers to Rank News](https://doi.org/10.1007/978-3-031-56063-7_41) |  | 0 | News media outlets disseminate information across various platforms. Often, these posts present complementary content and perspectives on the same news story. However, to compile a set of related news articles, users must thoroughly scour multiple sources and platforms, manually identifying which publications pertain to the same story. This tedious process hinders the speed at which journalists can perform essential tasks, notably fact-checking. To tackle this problem, we created a dataset containing both related and unrelated news pairs. This dataset allows us to develop information retrieval models grounded in the principle of binary relevance. Recognizing that many Transformer-based models might be suited for this task but could overemphasize relationships based on lexical connections, we tailored a dataset to fine-tune these models to focus on semantically relevant connections in the news domain. To craft this dataset, we introduced a methodology to identify pairs of news stories that are lexically similar yet refer to different events and pairs that discuss the same event but have distinct lexical structures. This design compels Transformers to recognize semantic connections between stories, even when their lexical similarities might be absent. Following a human-annotation assessment, we reveal that BERT outperformed other techniques, excelling even in challenging test cases. To ensure the reproducibility of our approach, we have made the dataset and top-performing models publicly available. | Carlos Muñoz, María José Apolo, Maximiliano Ojeda, Hans Lobel, Marcelo Mendoza | Pontificia Univ Catolica Chile, Vicuna Mackenna 6840, Santiago, Chile; Univ Tecn Federico Santa Maria, Vicuna Mackenna 3939, Santiago, Chile |
|  |  [Answer Retrieval in Legal Community Question Answering](https://doi.org/10.1007/978-3-031-56063-7_40) |  | 0 | The task of answer retrieval in the legal domain aims to help users to seek relevant legal advice from massive amounts of professional responses. Two main challenges hinder applying existing answer retrieval approaches in other domains to the legal domain: (1) a huge knowledge gap between lawyers and non-professionals; and (2) a mix of informal and formal content on legal QA websites. To tackle these challenges, we propose CE_FS, a novel cross-encoder (CE) re-ranker based on the fine-grained structured inputs. CE_FS uses additional structured information in the CQA data to improve the effectiveness of cross-encoder re-rankers. Furthermore, we propose LegalQA: a real-world benchmark dataset for evaluating answer retrieval in the legal domain. Experiments conducted on LegalQA show that our proposed method significantly outperforms strong cross-encoder re-rankers fine-tuned on MS MARCO. Our novel finding is that adding the question tags of each question besides the question description and title into the input of cross-encoder re-rankers structurally boosts the rankers' effectiveness. While we study our proposed method in the legal domain, we believe that our method can be applied in similar applications in other domains. | Arian Askari, Zihui Yang, Zhaochun Ren, Suzan Verberne |  |
|  |  [Towards Optimizing Ranking in Grid-Layout for Provider-Side Fairness](https://doi.org/10.1007/978-3-031-56069-9_7) |  | 0 | Information access systems, such as search engines and recommender systems, order and position results based on their estimated relevance. These results are then evaluated for a range of concerns, including provider-side fairness: whether exposure to users is fairly distributed among items and the people who created them. Several fairness-aware ranking and re-ranking techniques have been proposed to ensure fair exposure for providers, but this work focuses almost exclusively on linear layouts in which items are displayed in single ranked list. Many widely-used systems use other layouts, such as the grid views common in streaming platforms, image search, and other applications. Providing fair exposure to providers in such layouts is not well-studied. We seek to fill this gap by providing a grid-aware re-ranking algorithm to optimize layouts for provider-side fairness by adapting existing re-ranking techniques to grid-aware browsing models, and an analysis of the effect of grid-specific factors such as device size on the resulting fairness optimization. Our work provides a starting point and identifies open gaps in ensuring provider-side fairness in grid-based layouts. | Amifa Raj, Michael D. Ekstrand | Microsoft, Redmond, WA 98052 USA; Drexel Univ, Dept Informat Sci, Philadelphia, PA 19104 USA |
|  |  [A Conversational Robot for Children's Access to a Cultural Heritage Multimedia Archive](https://doi.org/10.1007/978-3-031-56069-9_11) |  | 0 | In this paper we introduce a conversational robot designed to assist children in searching a museum's cultural heritage video archive. The robot employs a form of Spoken Conversational Search to facilitate the clarification of children's interest (their information need) in specific videos from the archive. Children are typically insufficiently supported in this process by common search technologies such as search-bar and keyboard, or one-shot voice interfaces. We present our approach, which leverages a knowledge-graph representation of the museum's video archive to facilitate conversational search interactions and suggest content based on the interaction, in order to study information-seeking conversations with children. We plan to use the robot test-bed to investigate the effectiveness of conversational designs over one-shot voice interactions for clarifying children's information needs in a museum context. | Thomas Beelen, Roeland Ordelman, Khiet P. Truong, Vanessa Evers, Theo Huibers | Univ Twente, Enschede, Netherlands |
|  |  [MathMex: Search Engine for Math Definitions](https://doi.org/10.1007/978-3-031-56069-9_17) |  | 0 | This paper introduces MathMex, an open-source search engine for math definitions. With MathMex, users can search for definitions of mathematical concepts extracted from a variety of data sources and types including text, images, and videos. Definitions are extracted using a fine-tuned SciBERT classifier, and the search is done with a fine-tuned Sentence-BERT model. MathMex interface provides means of issuing a text, formula, and combined queries and logging features. | Shea Durgin, James Gore, Behrooz Mansouri | Univ Southern Maine, Portland, ME 04103 USA |
|  |  [XSearchKG: A Platform for Explainable Keyword Search over Knowledge Graphs](https://doi.org/10.1007/978-3-031-56069-9_18) |  | 0 | One of the most user-friendly methods to search over knowledge graphs is the usage of keyword queries. They offer a simple text input that requires no technical or domain knowledge. Most existing approaches for keyword search over graph-shaped data rely on graph traversal algorithms to find connections between keywords. They mostly concentrate on achieving efficiency and effectiveness (accurate ranking), but ignore usability, visualization, and interactive result presentation. All of which offer better support to non-experienced users. Moreover, it is not sufficient to just show a raw list of results, but it is also important to explain why a specific result is proposed. This not only provides an abstract view of the capabilities and limitations of the search system, but also increases confidence and helps discover new interesting facts. We propose XSearchKG, a platform for explainable keyword search over knowledge graphs that extends our previously proposed graph traversal-based approach and complements it with an interactive user interface for results explanation and browsing. | Leila Feddoul, Martin Birke, Sirko Schindler | Friedrich Schiller Univ Jena, Heinz Nixdorf Chair Distributed Informat Syst, Jena, Germany; German Aerosp Ctr DLR, Inst Data Sci, Jena, Germany |
|  |  [Result Assessment Tool: Software to Support Studies Based on Data from Search Engines](https://doi.org/10.1007/978-3-031-56069-9_19) |  | 0 | The Result Assessment Tool (RAT) is a software toolkit for conducting research with results from commercial search engines and other information retrieval (IR) systems. The software integrates modules for study design and management, automatic collection of search results via web scraping, and evaluation of search results in an assessment interface using different question types. RAT can be used for conducting a wide range of studies, including retrieval effectiveness studies, classification studies, and content analyses. | Sebastian Sünkler, Nurce Yagci, Sebastian Schultheiß, Sonja von Mach, Dirk Lewandowski | Hamburg Univ Appl Sci, Dept Informat Media & Commun, Finkenau 35, D-22081 Hamburg, Germany |
|  |  [Translating Justice: A Cross-Lingual Information Retrieval System for Maltese Case Law Documents](https://doi.org/10.1007/978-3-031-56069-9_24) |  | 0 | In jurisdictions adhering to the Common Law system, previous court judgements inform future rulings based on the Stare Decisis principle. For enhanced accessibility and retrieval of such judgements, we introduced a cross-lingual Legal Information Retrieval system prototype focused on Malta's small claims tribunal. This system utilises Neural Machine Translation (NMT) to automatically translate Maltese judgement documents into English, enabling dual-language querying. Additionally, it employs Rhetorical Role Labelling (RRL) on sentences within the judgements, allowing for targeted searches based on specific rhetorical roles. Developed without depending on high-end resources or commercial systems, this prototype showcases the potential of AI in advancing legal research tools and making legal documents more accessible, especially for non-native speakers. | Joel Azzopardi | Univ Malta, Fac ICT, Dept Artificial Intelligence, Msida, Malta |
|  |  [Displaying Evolving Events Via Hierarchical Information Threads for Sensitivity Review](https://doi.org/10.1007/978-3-031-56069-9_29) |  | 0 | Many government documents contain sensitive (e.g. personal or confidential) information that must be protected before the documents can be released to the public. However, reviewing documents to identify sensitive information is a complex task, which often requires analysing multiple related documents that mention a particular context of sensitivity. For example, coherent information about evolving events, such as legal proceedings, is often dispersed across documents produced at different times. In this paper, we present a novel system for sensitivity review, which automatically identifies hierarchical information threads to capture diverse aspects of an event. In particular, our system aims to assist sensitivity reviewers in making accurate sensitivity judgements efficiently by presenting hierarchical information threads that provide coherent and chronological information about an event's evolution. Through a user study, we demonstrate our system's effectiveness in improving the sensitivity reviewers' reviewing speed and accuracy compared to the traditional document-by-document review process. | Hitarth Narvala, Graham McDonald, Iadh Ounis | Univ Glasgow, Glasgow, Lanark, Scotland |
|  |  [Analyzing Mathematical Content for Plagiarism and Recommendations](https://doi.org/10.1007/978-3-031-56069-9_42) |  | 0 | Defined as "the use of ideas, concepts, words, or structures without appropriately acknowledging the source to benefit in a setting where originality is expected" [6], plagiarism poses a severe concern in the rapidly increasing number of scientific publications. | Ankit Satpute | Georg August Univ Gottingen, Gottingen, Germany |
|  |  [Explainable Recommender Systems with Knowledge Graphs and Language Models](https://doi.org/10.1007/978-3-031-56069-9_46) |  | 0 | To facilitate human decisions with credible suggestions, personalized recommender systems should have the ability to generate corresponding explanations while making recommendations. Knowledge graphs (KG), which contain comprehensive information about users and products, are widely used to enable this. By reasoning over a KG in a node-by-node manner, existing explainable models provide a KG-grounded path for each user-recommended item. Such paths serve as an explanation and reflect the historical behavior pattern of the user. However, not all items can be reached following the connections within the constructed KG under finite hops. Hence, previous approaches are constrained by a recall bias in terms of existing connectivity of KG structures. To overcome this, we propose a novel Path Language Modeling Recommendation (PLM-Rec) framework, learning a language model over KG paths consisting of entities and edges. Through path sequence decoding, PLM-Rec unifies recommendation and explanation in a single step and fulfills them simultaneously. As a result, PLM-Rec not only captures the user behaviors but also eliminates the restriction to pre-existing KG connections, thereby alleviating the aforementioned recall bias. Moreover, the proposed technique makes it possible to conduct explainable recommendation even when the KG is sparse or possesses a large number of relations. Experiments and extensive ablation studies on three Amazon e-commerce datasets demonstrate the effectiveness and explainability of the PLM-Rec framework. | Giacomo Balloccu, Ludovico Boratto, Gianni Fenu, Francesca Maridina Malloci, Mirko Marras | Univ Potsdam, HPI, Potsdam, Germany; Rutgers State Univ, New Brunswick, NJ 08901 USA; Meta Platforms Inc, Menlo Pk, CA USA |
|  |  [Recent Advances in Generative Information Retrieval](https://doi.org/10.1007/978-3-031-56069-9_48) |  | 0 | Generative retrieval (GR) has become a highly active area of information retrieval (IR) that has witnessed significant growth recently. Compared to the traditional “index-retrieve-then-rank” pipeline, the GR paradigm aims to consolidate all information within a corpus into a single model. Typically, a sequence-to-sequence model is trained to directly map a query to its relevant document identifiers (i.e., docids). This tutorial offers an introduction to the core concepts of the GR paradigm and a comprehensive overview of recent advances in its foundations and applications. We start by providing preliminary information covering foundational aspects and problem formulations of GR. Then, our focus shifts towards recent progress in docid design, training approaches, inference strategies, and the applications of GR. We end by outlining remaining challenges and issuing a call for future GR research. This tutorial is intended to be beneficial to both researchers and industry practitioners interested in developing novel GR solutions or applying them in real-world scenarios. | Yubao Tang, Ruqing Zhang, Zhaochun Ren, Jiafeng Guo, Maarten de Rijke | Leiden Univ, Leiden, Netherlands; Univ Amsterdam, Amsterdam, Netherlands; Univ Chinese Acad Sci, CAS Key Lab Network Data Sci & Technol, ICT, CAS, Beijing, Peoples R China |
|  |  [Affective Computing for Social Good Applications: Current Advances, Gaps and Opportunities in Conversational Setting](https://doi.org/10.1007/978-3-031-56069-9_50) |  | 0 | Affective computing involves examining and advancing systems and devices capable of identifying, comprehending, processing, and emulating human emotions, sentiment, politeness and personality characteristics. This is an ever-expanding multidisciplinary domain that investigates how technology can contribute to the comprehension of human affect, how affect can influence interactions between humans and machines, how systems can be engineered to harness affect for enhanced capabilities, and how integrating affective strategies can revolutionize interactions between humans and machines. Recognizing the fact that affective computing encompasses disciplines such as computer science, psychology, and cognitive science, this tutorial aims to delve into the historical underpinnings and overarching objectives of affective computing, explore various approaches for affect detection and generation, its practical applications across diverse areas, including but not limited to social good (like persuasion, therapy and support, etc.), address ethical concerns, and outline potential future directions. | Priyanshu Priya, Mauajama Firdaus, Gopendra Vikram Singh, Asif Ekbal | Univ Alberta, Edmonton, AB, Canada; Indian Inst Technol Patna, Dayalpur Daulatpur, India |
|  |  [Query Performance Prediction: From Fundamentals to Advanced Techniques](https://doi.org/10.1007/978-3-031-56069-9_51) |  | 0 | Query performance prediction (QPP) is a core task in information retrieval (IR) that aims at predicting the retrieval quality for a given query without relevance judgments. QPP has been investigated for decades and has witnessed a surge in research activity in recent years; QPP has been shown to benefit various aspects, e.g., improving retrieval effectiveness by selecting the most effective ranking function per query [5, 7]. Despite its importance, there is no recent tutorial to provide a comprehensive overview of QPP techniques in the era of pre-trained/large language models or in the scenario of emerging conversational search (CS); In this tutorial, we have three main objectives. First, we aim to disseminate the latest advancements in QPP to the IR community. Second, we go beyond investigating QPP in ad-hoc search and cover QPP for CS. Third, the tutorial offers a unique opportunity to bridge the gap between theory and practice; we aim to equip participants with the essential skills and insights needed to navigate the evolving landscape of QPP, ultimately benefiting both researchers and practitioners in the field of IR and encouraging them to work around the future avenues on QPP. | Negar Arabzadeh, Chuan Meng, Mohammad Aliannejadi, Ebrahim Bagheri | Univ Waterloo, Waterloo, ON, Canada; Univ Amsterdam, Amsterdam, Netherlands; Toronto Metropolitan Univ, Toronto, ON, Canada |
|  |  [Fairness Through Domain Awareness: Mitigating Popularity Bias for Music Discovery](https://doi.org/10.1007/978-3-031-56066-8_27) |  | 0 | As online music platforms grow, music recommender systems play a vital role in helping users navigate and discover content within their vast musical databases. At odds with this larger goal, is the presence of popularity bias, which causes algorithmic systems to favor mainstream content over, potentially more relevant, but niche items. In this work we explore the intrinsic relationship between music discovery and popularity bias. To mitigate this issue we propose a domain-aware, individual fairness-based approach which addresses popularity bias in graph neural network (GNNs) based recommender systems. Our approach uses individual fairness to reflect a ground truth listening experience, i.e., if two songs sound similar, this similarity should be reflected in their representations. In doing so, we facilitate meaningful music discovery that is robust to popularity bias and grounded in the music domain. We apply our BOOST methodology to two discovery based tasks, performing recommendations at both the playlist level and user level. Then, we ground our evaluation in the cold start setting, showing that our approach outperforms existing fairness benchmarks in both performance and recommendation of lesser-known content. Finally, our analysis explains why our proposed methodology is a novel and promising approach to mitigating popularity bias and improving the discovery of new and niche content in music recommender systems. | Rebecca Salganik, Fernando Diaz, Golnoosh Farnadi |  |
|  |  [Countering Mainstream Bias via End-to-End Adaptive Local Learning](https://doi.org/10.1007/978-3-031-56069-9_6) |  | 0 | Collaborative filtering (CF) based recommendations suffer from mainstream bias – where mainstream users are favored over niche users, leading to poor recommendation quality for many long-tail users. In this paper, we identify two root causes of this mainstream bias: (i) discrepancy modeling, whereby CF algorithms focus on modeling mainstream users while neglecting niche users with unique preferences; and (ii) unsynchronized learning, where niche users require more training epochs than mainstream users to reach peak performance. Targeting these causes, we propose a novel end-To-end Adaptive Local Learning (TALL) framework to provide high-quality recommendations to both mainstream and niche users. TALL uses a loss-driven Mixture-of-Experts module to adaptively ensemble experts to provide customized local models for different users. Further, it contains an adaptive weight module to synchronize the learning paces of different users by dynamically adjusting weights in the loss. Extensive experiments demonstrate the state-of-the-art performance of the proposed model. Code and data are provided at <https://github.com/JP-25/end-To-end-Adaptive-Local-Leanring-TALL-> | Jinhao Pan, Ziwei Zhu, Jianling Wang, Allen Lin, James Caverlee |  |
|  |  [BioASQ at CLEF2024: The Twelfth Edition of the Large-Scale Biomedical Semantic Indexing and Question Answering Challenge](https://doi.org/10.1007/978-3-031-56069-9_67) |  | 0 | The large-scale biomedical semantic indexing and question-answering challenge (BioASQ) aims at the continuous advancement of methods and tools to meet the needs of biomedical researchers and practitioners for efficient and precise access to the ever-increasing resources of their domain. With this purpose, during the last eleven years, a series of annual challenges have been organized with specific shared tasks on large-scale biomedical semantic indexing and question answering. Benchmark datasets have been concomitantly provided in alignment with the real needs of biomedical experts, providing a unique common testbed where different teams around the world can investigate and compare new approaches for accessing biomedical knowledge. The twelfth version of the BioASQ Challenge will be held as an evaluation Lab within CLEF2024 providing four shared tasks: (i) Task b on the information retrieval for biomedical questions, and the generation of comprehensible answers. (ii) Task Synergy the information retrieval and generation of answers for open biomedical questions on developing topics, in collaboration with the experts posing the questions. (iii) Task MultiCardioNER on the automated annotation of clinical entities in medical documents in the field of cardiology, primarily in Spanish, English, Italian and Dutch. (iv) Task BioNNE on the automated annotation of biomedical documents in Russian and English with nested named entity annotations. As BioASQ rewards the methods that outperform the state of the art in these shared tasks, it pushes the research frontier towards approaches that accelerate access to biomedical knowledge. | Anastasios Nentidis, Anastasia Krithara, Georgios Paliouras, Martin Krallinger, Luis Gascó Sánchez, Salvador LimaLópez, Eulàlia Farré, Natalia V. Loukachevitch, Vera Davydova, Elena Tutubalina | Moscow MV Lomonosov State Univ, Moscow, Russia; Barcelona Supercomp Ctr, Barcelona, Spain; Sber AI, Moscow, Russia; Natl Ctr Sci Res Demokritos, Athens, Greece |
|  |  [ProMap: Product Mapping Datasets](https://doi.org/10.1007/978-3-031-56060-6_11) |  | 0 | The goal of product mapping is to decide, whether two listings from two different e-shops describe the same products. Existing datasets of matching and non-matching pairs of products, however, often suffer from incomplete product information or contain only very distant non-matching products. In this paper, we introduce two new datasets for product mapping: ProMapCz consisting of 1,495 Czech product pairs and ProMapEn consisting of 1,555 English product pairs of matching and non-matching products manually scraped from two pairs of e-shops. The datasets contain both images and textual descriptions of the products, including their specifications, making them one of the most complete datasets for product mapping. Additionally, we divide the non-matching products into two different categories – close non-matches and medium non-matches, based on how similar the products are to each other. Even the medium non-matches are, however, pairs of products that are much more similar than non-matches in other datasets – for example, they still need to have the same brand and similar name and price. Finally, we train a number of product matching models on these datasets to demonstrate the advantages of having these two types of non-matches for the analysis of these models. | Katerina Macková, Martin Pilát | Charles Univ Prague, Fac Math & Phys, Malostranske Namesti 25, Prague 11800 1, Czech Republic |
|  |  [Exploring Large Language Models and Hierarchical Frameworks for Classification of Large Unstructured Legal Documents](https://doi.org/10.1007/978-3-031-56060-6_15) |  | 0 | Legal judgment prediction suffers from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure. Predicting judgments from such documents becomes a challenging task, more so on documents with no structural annotation. We explore the classification of these large legal documents and their lack of structural information with a deep-learning-based hierarchical framework which we call MESc; "Multi-stage Encoder-based Supervised with-clustering"; for judgment prediction. Specifically, we divide a document into parts to extract their embeddings from the last four layers of a custom fine-tuned Large Language Model, and try to approximate their structure through unsupervised clustering. Which we use in another set of transformer encoder layers to learn the inter-chunk representations. We analyze the adaptability of Large Language Models (LLMs) with multi-billion parameters (GPT-Neo, and GPT-J) with the hierarchical framework of MESc and compare them with their standalone performance on legal texts. We also study their intra-domain(legal) transfer learning capability and the impact of combining embeddings from their last layers in MESc. We test these methods and their effectiveness with extensive experiments and ablation studies on legal documents from India, the European Union, and the United States with the ILDC dataset and a subset of the LexGLUE dataset. Our approach achieves a minimum total performance gain of approximately 2 points over previous state-of-the-art methods. | Nishchal Prasad, Mohand Boughanem, Taoufiq Dkaki |  |
|  |  [Eliminating Contextual Bias in Aspect-Based Sentiment Analysis](https://doi.org/10.1007/978-3-031-56027-9_6) |  | 0 | Pretrained language models (LMs) have made remarkable achievements in aspect-based sentiment analysis (ABSA). However, it is discovered that these models may struggle in some particular cases (e.g., to detect sentiments expressed towards targeted aspects with only implicit or adversarial expressions). Since it is hard for models to align implicit or adversarial expressions with their corresponding aspects, the sentiments of the targeted aspects would largely be impacted by the expressions towards other aspects in the sentence. We name this phenomenon as contextual bias. To tackle the problem, we propose a flexible aspect-oriented debiasing method (Arde) to eliminate the harmful contextual bias without the need of adjusting the underlying LMs. Intuitively, Arde calibrates the prediction towards the targeted aspect by subtracting the bias towards the context. Favorably, Arde can get theoretical support from counterfactual reasoning theory. Experiments are conducted on SemEval benchmark, and the results show that Arde can empirically improve the accuracy on contextually biased aspect sentiments without degrading the accuracy on unbiased ones. Driven by recent success of large language models (LLMs, e.g., ChatGPT), we further uncover that even LLMs can fail to address certain contextual bias, which yet can be effectively tackled by Arde. | Ruize An, Chen Zhang, Dawei Song | Beijing Inst Technol, Beijing, Peoples R China |
|  |  [A Streaming Approach to Neural Team Formation Training](https://doi.org/10.1007/978-3-031-56027-9_20) |  | 0 | Predicting future successful teams of experts who can effectively collaborate is challenging due to the experts' temporality of skill sets, levels of expertise, and collaboration ties, which is overlooked by prior work. Specifically, state-of-the-art neural-based methods learn vector representations of experts and skills in a static latent space, falling short of incorporating the possible drift and variability of experts' skills and collaboration ties in time. In this paper, we propose (1) a streaming-based training strategy for neural models to capture the evolution of experts' skills and collaboration ties over time and (2) to consume time information as an additional signal to the model for predicting future successful teams. We empirically benchmark our proposed method against state-of-the-art neural team formation methods and a strong temporal recommender system on datasets from varying domains with distinct distributions of skills and experts in teams. The results demonstrate neural models that utilize our proposed training strategy excel at efficacy in terms of classification and information retrieval metrics. The codebase is available at https://github.com/fani-lab/OpeNTF/tree/ecir24 . | Hossein Fani, Reza Barzegar, Arman Dashti, Mahdis Saeedi | Univ Windsor, Windsor, ON, Canada |
|  |  [A Second Look on BASS - Boosting Abstractive Summarization with Unified Semantic Graphs - A Replication Study](https://doi.org/10.1007/978-3-031-56066-8_11) |  | 0 | We present a detailed replication study of the BASS framework, an abstractive summarization system based on the notion of Unified Semantic Graphs. Our investigation includes challenges in replicating key components and an ablation study to systematically isolate error sources rooted in replicating novel components. Our findings reveal discrepancies in performance compared to the original work. We highlight the significance of paying careful attention even to reasonably omitted details for replicating advanced frameworks like BASS, and emphasize key practices for writing replicable papers. | Osman Alperen Koras, Jörg Schlötterer, Christin Seifert |  |
|  |  [Absolute Variation Distance: An Inversion Attack Evaluation Metric for Federated Learning](https://doi.org/10.1007/978-3-031-56066-8_20) |  | 0 | Federated Learning (FL) has emerged as a pivotal approach for training models on decentralized data sources by sharing only model gradients. However, the shared gradients in FL are susceptible to inversion attacks which can expose sensitive information. While several defense and attack strategies have been proposed, their effectiveness is often evaluated using metrics that may not necessarily reflect the success rate of an attack or information retrieval, especially in the context of multidimensional data such as images. Traditional metrics like the Structural Similarity Index (SSIM), Peak Signal-to-Noise Ratio (PSNR), and Mean Squared Error (MSE) are typically used as lightweight metrics, assume only pixel-wise comparison, but fail to consider the semantic context of the recovered data. This paper introduces the Absolute Variation Distance (AVD), a lightweight metric derived from total variation, to assess data recovery and information leakage in FL. Unlike traditional metrics, AVD offers a continuous measure for extracting information in noisy images and aligns closely with human perception. Our results combined with a user experience survey demonstrate that AVD provides a more accurate and consistent measure of data recovery. It also matches the accuracy of the more costly and complex Neural Network based metric, the Learned Perceptual Image Patch Similarity (LPIPS). Hence it offers an effective tool for automatic evaluation of data security in FL and a reliable way of studying defence and inversion attacks strategies in FL. | Georgios Papadopoulos, Yash Satsangi, Shaltiel Eloul, Marco Pistoia | JPMorgan Chase, Global Technol Appl Res, New York, NY 10017 USA |
|  |  [Experiments in News Bias Detection with Pre-trained Neural Transformers](https://doi.org/10.1007/978-3-031-56066-8_22) |  | 0 | The World Wide Web provides unrivalled access to information globally, including factual news reporting and commentary. However, state actors and commercial players increasingly spread biased (distorted) or fake (non-factual) information to promote their agendas. We compare several large, pre-trained language models on the task of sentence-level news bias detection and sub-type classification, providing quantitative and qualitative results. Our findings are to be seen as part of a wider effort towards realizing the conceptual vision, articulated by Fuhr et al. [10], of a "nutrition label" for online content for the social good. | Tim Menzner, Jochen L. Leidner | Coburg Univ Appl Sci, Informat Access Res Grp, Friedrich Streib Str 2, D-96459 Coburg, Germany |
|  |  [A Transformer-Based Object-Centric Approach for Date Estimation of Historical Photographs](https://doi.org/10.1007/978-3-031-56063-7_9) |  | 0 | The accurate estimation of the creation date of cultural heritage photographic assets is a challenging and complex task, typically requiring the expertise of qualified archivists, with significant implications for archival and preservation purposes. This paper introduces a new dataset for image date estimation, which complements existing datasets, thus creating a more balanced and realistic training set for deep learning models. On this dataset, we present a set of modern strong baselines that outperform previous state-of-the-art methods for this task. Additionally, we propose a novel approach that leverages “dating indicators” or “dating clues” through object detection and a self-attention based Transformer encoder. Our experiments demonstrate that the proposed approach has promising applicability in real scenarios and that incorporating “dating indicators” through object detection can improve the performance of image date estimation models. The dataset and code of our models are publicly available at https://github.com/cesc47/DEXPERT . | Francesc Net, Núria Hernández, Adrià Molina, Lluís Gómez | Univ Autonoma Barcelona, Comp Vis Ctr, Catalunya, Spain |
|  |  [Bias Detection and Mitigation in Textual Data: A Study on Fake News and Hate Speech Detection](https://doi.org/10.1007/978-3-031-56063-7_29) |  | 0 | Addressing bias in NLP-based solutions is crucial to promoting fairness, avoiding discrimination, building trust, upholding ethical standards, and ultimately improving their performance and reliability. On the topic of bias detection and mitigation in textual data, this work examines the effect of different bias detection models along with standard debiasing methods on the effectiveness of fake news and hate speech detection tasks. Extensive discussion of the results draws useful conclusions, highlighting the inherent difficulties in effectively managing bias. | Apostolos Kasampalis, Despoina Chatzakou, Theodora Tsikrika, Stefanos Vrochidis, Ioannis Kompatsiaris | Ctr Res & Technol Hellas, Informat Technol Inst, Thessaloniki, Greece |
|  |  [DQNC2S: DQN-Based Cross-Stream Crisis Event Summarizer](https://doi.org/10.1007/978-3-031-56063-7_34) |  | 0 | Summarizing multiple disaster-relevant data streams simultaneously is particularly challenging as existing Retrieve&Re-ranking strategies suffer from the inherent redundancy of multi-stream data and limited scalability in a multi-query setting. This work proposes an online approach to crisis timeline generation based on weak annotation with Deep Q-Networks. It selects on-the-fly the relevant pieces of text without requiring neither human annotations nor content re-ranking. This makes the inference time independent of the number of input queries. The proposed approach also incorporates a redundancy filter into the reward function to effectively handle cross-stream content overlaps. The achieved ROUGE and BERTScore results are superior to those of best-performing models on the CrisisFACTS 2022 benchmark. | Daniele Rege Cambrin, Luca Cagliero, Paolo Garza |  |
|  |  [QuantPlorer: Exploration of Quantities in Text](https://doi.org/10.1007/978-3-031-56069-9_13) |  | 0 | Quantities play an important role in documents of various domains such as finance, business, and medicine. Despite the role of quantities, only a limited number of works focus on their extraction from text and even less on creating respective user-friendly document exploration frameworks. In this work, we introduce QuantPlorer, an online quantity extractor and explorer. Through an intuitive web interface, QuantExplorer extracts quantities from unstructured text, enables users to interactively investigate and visualize quantities in text, and it supports filtering based on diverse features, i.e., value ranges, units, trends, and concepts. Furthermore, users can explore and visualize distributions of values for specific units and concepts. Our demonstration is available at https://quantplorer.ifi.uni-heidelberg.de/ . | Satya Almasian, Alexander Kosnac, Michael Gertz | Heidelberg Univ, Heidelberg, Germany |
|  |  [ARElight: Context Sampling of Large Texts for Deep Learning Relation Extraction](https://doi.org/10.1007/978-3-031-56069-9_23) |  | 0 | The escalating volume of textual data necessitates adept and scalable Information Extraction (IE) systems in the field of Natural Language Processing (NLP) to analyse massive text collections in a detailed manner. While most deep learning systems are designed to handle textual information as it is, the gap in the existence of the interface between a document and the annotation of its parts is still poorly covered. Concurrently, one of the major limitations of most deep-learning models is a constrained input size caused by architectural and computational specifics. To address this, we introduce ARElight $$^1$$ , a system designed to efficiently manage and extract information from sequences of large documents by dividing them into segments with mentioned object pairs. Through a pipeline comprising modules for text sampling, inference, optional graph operations, and visualisation, the proposed system transforms large volumes of text in a structured manner. Practical applications of ARElight are demonstrated across diverse use cases, including literature processing and social network analysis.( $$^1$$ https://github.com/nicolay-r/ARElight ) | Nicolay Rusnachenko, Huizhi Liang, Maksim Kalameyets, Lei Shi | Newcastle Univ, Sch Comp, Newcastle Upon Tyne, Tyne & Wear, England |
|  |  [Variance Reduction in Ratio Metrics for Efficient Online Experiments](https://doi.org/10.1007/978-3-031-56069-9_34) |  | 0 | Online controlled experiments, such as A/B-tests, are commonly used by modern tech companies to enable continuous system improvements. Despite their paramount importance, A/B-tests are expensive: by their very definition, a percentage of traffic is assigned an inferior system variant. To ensure statistical significance on top-level metrics, online experiments typically run for several weeks. Even then, a considerable amount of experiments will lead to inconclusive results (i.e. false negatives, or type-II error). The main culprit for this inefficiency is the variance of the online metrics. Variance reduction techniques have been proposed in the literature, but their direct applicability to commonly used ratio metrics (e.g. click-through rate or user retention) is limited. In this work, we successfully apply variance reduction techniques to ratio metrics on a large-scale short-video platform: ShareChat. Our empirical results show that we can either improve A/B-test confidence in 77 retain the same level of confidence with 30 show that the common approach of including as many covariates as possible in regression is counter-productive, highlighting that control variates based on Gradient-Boosted Decision Tree predictors are most effective. We discuss the practicalities of implementing these methods at scale and showcase the cost reduction they beget. | Shubham Baweja, Neeti Pokharna, Aleksei Ustimenko, Olivier Jeunen |  |
|  |  [Overview of PAN 2024: Multi-author Writing Style Analysis, Multilingual Text Detoxification, Oppositional Thinking Analysis, and Generative AI Authorship Verification - Extended Abstract](https://doi.org/10.1007/978-3-031-56072-9_1) |  | 0 | The paper gives a brief overview of the four shared tasks organized at the PAN 2024 lab on digital text forensics and stylometry to be hosted at CLEF 2024. The goal of the PAN lab is to advance the state-of-the-art in text forensics and stylometry through an objective evaluation of new and established methods on new benchmark datasets. Our four tasks are: (1) multi-author writing style analysis, which we continue from 2023 in a more difficult version, (2) multilingual text detoxification, a new task that aims to translate and re-formulate text in a non-toxic way, (3) oppositional thinking analysis, a new task that aims to discriminate critical thinking from conspiracy narratives and identify their core actors, and (4) generative AI authorship verification, which formulates the detection of AI-generated text as an authorship problem, one of PAN's core tasks. As with the previous editions, PAN invites software submissions as easy-to-reproduce docker containers; more than 400 pieces of software have been submitted from PAN'12 through PAN'23 combined, with all recent evaluations running on the TIRA experimentation platform [8]. | Janek Bevendorff, Xavier Bonet Casals, Berta Chulvi, Daryna Dementieva, Ashaf Elnagar, Dayne Freitag, Maik Fröbe, Damir Korencic, Maximilian Mayerl, Animesh Mukherjee, Alexander Panchenko, Martin Potthast, Francisco Rangel, Paolo Rosso, Alisa Smirnova, Efstathios Stamatatos, Benno Stein, Mariona Taulé, Dmitry Ustalov, Matti Wiegmann, Eva Zangerle | Univ Aegean, Samos, Greece; Skoltech & AIRI, Skolkovo, Russia; SRI Int, 333 Ravenswood Ave, Menlo Pk, CA 94025 USA; Univ Leipzig, Leipzig, Germany; Univ Politecn Valencia, Valencia, Spain; Toloka, Luzern, Switzerland; JetBrains, Belgrade, Serbia; Univ Kassel, Kassel, Germany; Symanto Res, Valencia, Spain; Rudjer Boskovic Inst, Zagreb, Croatia; Bauhaus Univ Weimar, Weimar, Germany; Indian Inst Technol Kharagpur, Kharagpur, W Bengal, India; Tech Univ Munich, Munich, Germany; Friedrich Schiller Univ Jena, Jena, Germany; Univ Hamburg, Hamburg, Germany; Univ Innsbruck, Innsbruck, Austria; Univ Appl Sci BFI Vienna, Vienna, Austria; Univ Santiago de Compostela, Santiago, Spain; Univ Sharjah, Sharjah, U Arab Emirates; Univ Barcelona, Barcelona, Spain |
|  |  [CLEF 2024 SimpleText Track - Improving Access to Scientific Texts for Everyone](https://doi.org/10.1007/978-3-031-56072-9_4) |  | 0 | Everyone acknowledges the importance of objective scientific information. However, finding and understanding relevant scientific documents is often challenging due to complex terminology and readers' lack of prior knowledge. The question is can we improve accessibility for everyone? This paper presents an overview of the SimpleText Track at CLEF 2024 addressing the technical and evaluation challenges associated with making scientific information accessible to a wide audience, including students and non-experts. It describes the data and benchmarks provided for scientific text summarization and simplification, along with the participants' results. The CLEF 2024 SimpleText track is based on four interrelated tasks: Task 1 on Content Selection: Retrieving Passages to Include in a Simplified Summary. Task 2 on Complexity Spotting: Identifying and Explaining Difficult Concepts. Task 3 on Text Simplification: Simplify Scientific Text. Task 4 on SOTA?: Tracking the State-of-the-Art in Scholarly Publications. | Liana Ermakova, Eric SanJuan, Stéphane Huet, Hosein Azarbonyad, Giorgio Maria Di Nunzio, Federica Vezzani, Jennifer D'Souza, Salomon Kabongo, Hamed Babaei Giglou, Yue Zhang, Sören Auer, Jaap Kamps | Univ Padua, Padua, Italy; TIB Leibniz Informat Ctr Sci & Technol, Hannover, Germany; Elsevier, Amsterdam, Netherlands; Univ Bretagne Occidentale, HCTI, Brest, France; Avignon Univ, LIA, Avignon, France; Univ Amsterdam, Amsterdam, Netherlands |
|  |  [LifeCLEF 2024 Teaser: Challenges on Species Distribution Prediction and Identification](https://doi.org/10.1007/978-3-031-56072-9_3) |  | 0 | Building accurate knowledge of the identity, the geographic distribution and the evolution of species is essential for the sustainable development of humanity, as well as for biodiversity conservation. However, species identification and inventory is a difficult and costly task, requiring large-scale automated approaches. The LifeCLEF lab has been promoting and evaluating advances in this domain since 2011 through the organization of multi-year challenges. The 2024 edition presented in this article proposes five data-driven challenges as a continuation of this effort: (i) BirdCLEF: bird species recognition in audio soundscapes, (ii)FungiCLEF: fungi recognition beyond 0-1 cost, (iii) GeoLifeCLEF: remote sensing based prediction of species, (iv) PlantCLEF: Multi-species identification in vegetation plot images, and (v) SnakeCLEF: snake recognition in medically important scenarios. | Alexis Joly, Lukás Picek, Stefan Kahl, Hervé Goëau, Vincent Espitalier, Christophe Botella, Benjamin Deneu, Diego Marcos, Joaquim Estopinan, César Leblanc, Théo Larcher, Milan Sulc, Marek Hrúz, Maximilien Servajean, Jirí Matas, Hervé Glotin, Robert Planqué, WillemPier Vellinga, Holger Klinck, Tom Denton, Andrew M. Durso, Ivan Eggel, Pierre Bonnet, Henning Müller | Univ Montpellier, Univ Paul Valery Montpellier, AMIS, LIRMM,CNRS, Montpellier, France; Aix Marseille Univ, Univ Toulon, CNRS, LIS,DYNI Team, Marseille, France; HES SO Valais, Inst Informat, Sierre, Switzerland; Cornell Univ, Cornell Lab Ornithol, K Lisa Yang Ctr Conservat Bioacoust, Ithaca, NY USA; Second Fdn, Prague, Czech Republic; Xenocanto Fdn, Amersfoort, Netherlands; Univ Montpellier, CNRS, LIRMM, INRIA, Montpellier, France; Univ West Bohemia, Dept Cybernet, FAV, Plzen, Czech Republic; Tech Univ Chemnitz, Chemnitz, Germany; CIRAD, UMR AMAP, Montpellier, Occitanie, France; Florida Gulf Coast Univ, Dept Biol Sci, Ft Myers, FL USA; Google Res, San Francisco, CA USA; Czech Tech Univ, Prague, Czech Republic |
|  |  [The CLEF 2024 Monster Track: One Lab to Rule Them All](https://doi.org/10.1007/978-3-031-56072-9_2) |  | 0 | Generative Artificial Intelligence (AI) and Large Language Models (LLMs) are revolutionizing technology and society thanks to their versatility and applicability to a wide array of tasks and use cases, in multiple media and modalities. As a new and relatively untested technology, LLMs raise several challenges for research and application alike, including questions about their quality, reliability, predictability, veracity, as well as on how to develop proper evaluation methodologies to assess their various capacities. This evaluation lab will focus on a specific aspect of LLMs, namely their versatility. The CLEF Monster Track is organized as a meta-challenge across a selection of tasks chosen from other evaluation labs running in CLEF 2024, and participants will be asked to develop or adapt a generative AI or LLM-based system that will be run on all the tasks with no or minimal task adaptation. This will allow us to systematically evaluate the performance of the same LLM-based system across a wide range of very different tasks and to provide feedback to each targeted task about the performance of a general-purpose LLM system compared to systems specifically developed for the task. Since the datasets for CLEF 2024 have not yet been released publicly, we will be able to experiment with previously unseen data, thus reducing the risk of contamination, which is one of the most serious problems faced by LLM evaluation datasets. | Nicola Ferro, Julio Gonzalo, Jussi Karlgren, Henning Müller | UNED, Madrid, Spain; SiloGen, Helsinki, Finland; Univ Padua, Padua, Italy; HES SO Valais, Valais, Switzerland |
|  |  [CLEF 2024 JOKER Lab: Automatic Humour Analysis](https://doi.org/10.1007/978-3-031-56072-9_5) |  | 0 | The JOKER Lab at the Conference and Labs of the Evaluation Forum (CLEF) aims to foster research on automated processing of verbal humour, including tasks such as retrieval, classification, interpretation, generation, and translation. Despite the heady success of large language models, humour and wordplay automatic processing are far from being a solved problem. JOKER brings together experts from the social and computational sciences and encourages them to collaborate on shared tasks with quality-controlled annotated datasets. In 2024, we will offer entirely new shared tasks on humour-aware information retrieval, as well as fine-grained sentiment analysis and classification of humour for conversational agents. As in the past JOKER Labs, we will also make our data available for an unshared task that solicits novel use cases. In this paper, we provide a brief retrospective on the JOKER Labs, with a focus on the results and lessons learnt from last year's iteration, and we preview the tasks to be held at JOKER 2024. | Liana Ermakova, AnneGwenn Bosser, Tristan Miller, Tremaine Thomas, Victor Manuel PalmaPreciado, Grigori Sidorov, Adam Jatowt | Univ Manitoba, Dept Comp Sci, Winnipeg, MB, Canada; Ecole Natl Ingnenieurs Brest, Lab STICC, CNRS, UMR 6285, Brest, France; Univ Bretagne Occidentale, HCTI, Brest, France; Univ Innsbruck, Innsbruck, Austria; Inst Politecn Nacl IPN, Ctr Invest Computac CIC, Mexico City, DF, Mexico |
|  |  [iDPP@CLEF 2024: The Intelligent Disease Progression Prediction Challenge](https://doi.org/10.1007/978-3-031-56072-9_7) |  | 0 | Amyotrophic Lateral Sclerosis (ALS) and Multiple Sclerosis (MS) are chronic diseases characterized by progressive or alternate impairment of neurological functions (motor, sensory, visual, cognitive). Patients have to manage alternated periods in hospital with care at home, experiencing a constant uncertainty regarding the timing of the disease acute phases and facing a considerable psychological and economic burden that also involves their caregivers. Clinicians, on the other hand, need tools able to support them in all the phases of the patient treatment, suggest personalized therapeutic decisions, indicate urgently needed interventions. iDPP@CLEF run in CLEF 2022 and 2023, offering tasks on the prediction of ALS and MS progression, using retrospective patient clinical data complemented with environmental data. iDPP@CLEF 2024 will focus on prospective patient data for ALS collected via a dedicated app developed by the BRAINTEASER project and sensor data in the context of clinical trials in Turin, Pavia, Lisbon, and Madrid. For MS, iDPP@CLEF 2024 will rely on retrospective patient data complemented with environmental and pollution data from clinical institutions in Pavia and Turin. | Helena Aidos, Roberto Bergamaschi, Paola Cavalla, Adriano Chiò, Arianna Dagliati, Barbara Di Camillo, Mamede de Carvalho, Nicola Ferro, Piero Fariselli, Jose Manuel García Dominguez, Sara C. Madeira, Eleonora Tavazzi | Gregorio Maranon Hosp Madrid, Madrid, Spain; Univ Turin, Turin, Italy; Univ Padua, Padua, Italy; Citta Salute & Sci, Turin, Italy; Univ Lisbon, Lisbon, Portugal; Univ Pavia, Pavia, Italy; IRCCS Fdn C Mondino Pavia, Pavia, Italy |
|  |  [LongEval: Longitudinal Evaluation of Model Performance at CLEF 2024](https://doi.org/10.1007/978-3-031-56072-9_8) |  | 0 | This paper introduces the planned second LongEval Lab, part of the CLEF 2024 conference. The aim of the lab's two tasks is to give researchers test data for addressing temporal effectiveness persistence challenges in both information retrieval and text classification, motivated by the fact that model performance degrades as the test data becomes temporally distant from the training data. LongEval distinguishes itself from traditional IR and classification tasks by emphasizing the evaluation of models designed to mitigate performance drop over time using evolving data. The second LongEval edition will further engage the IR community and NLP researchers in addressing the crucial challenge of temporal persistence in models, exploring the factors that enable or hinder it, and identifying potential solutions along with their limitations. | Rabab Alkhalifa, Hsuvas Borkakoty, Romain Deveaud, Alaa ElEbshihy, Luis Espinosa Anke, Tobias Fink, Gabriela González Sáez, Petra Galuscáková, Lorraine Goeuriot, David Iommi, Maria Liakata, Harish Tayyar Madabushi, Pablo MedinaAlias, Philippe Mulhem, Florina Piroi, Martin Popel, Christophe Servan, Arkaitz Zubiaga | Cardiff Univ, Cardiff, Wales; Univ Stavanger, Stavanger, Norway; Queen Mary Univ London, London, England; Res Studios Austria, Data Sci Studio, Vienna, Austria; Univ Bath, Bath, Avon, England; Charles Univ Prague, Prague, Czech Republic; Univ Grenoble Alpes, Grenoble INP, CNRS, Inst Engn,LIG, Grenoble, France; Qwant, Paris, France |
|  |  [CrisisKAN: Knowledge-Infused and Explainable Multimodal Attention Network for Crisis Event Classification](https://doi.org/10.1007/978-3-031-56060-6_2) |  | 0 | Pervasive use of social media has become the emerging source for real-time information (like images, text, or both) to identify various events. Despite the rapid growth of image and text-based event classification, the state-of-the-art (SOTA) models find it challenging to bridge the semantic gap between features of image and text modalities due to inconsistent encoding. Also, the black-box nature of models fails to explain the model's outcomes for building trust in high-stakes situations such as disasters, pandemic. Additionally, the word limit imposed on social media posts can potentially introduce bias towards specific events. To address these issues, we proposed CrisisKAN, a novel Knowledge-infused and Explainable Multimodal Attention Network that entails images and texts in conjunction with external knowledge from Wikipedia to classify crisis events. To enrich the context-specific understanding of textual information, we integrated Wikipedia knowledge using proposed wiki extraction algorithm. Along with this, a guided cross-attention module is implemented to fill the semantic gap in integrating visual and textual data. In order to ensure reliability, we employ a model-specific approach called Gradient-weighted Class Activation Mapping (Grad-CAM) that provides a robust explanation of the predictions of the proposed model. The comprehensive experiments conducted on the CrisisMMD dataset yield in-depth analysis across various crisis-specific tasks and settings. As a result, CrisisKAN outperforms existing SOTA methodologies and provides a novel view in the domain of explainable multimodal event classification. | Shubham Gupta, Nandini Saini, Suman Kundu, Debasis Das |  |
|  |  [Probing Pretrained Language Models with Hierarchy Properties](https://doi.org/10.1007/978-3-031-56060-6_9) |  | 0 | Since Pretrained Language Models (PLMs) are the cornerstone of the most recent Information Retrieval (IR) models, the way they encode semantic knowledge is particularly important. However, little attention has been given to studying the PLMs' capability to capture hierarchical semantic knowledge. Traditionally, evaluating such knowledge encoded in PLMs relies on their performance on a task-dependent evaluation approach based on proxy tasks, such as hypernymy detection. Unfortunately, this approach potentially ignores other implicit and complex taxonomic relations. In this work, we propose a task-agnostic evaluation method able to evaluate to what extent PLMs can capture complex taxonomy relations, such as ancestors and siblings. The evaluation is based on intrinsic properties that capture the hierarchical nature of taxonomies. Our experimental evaluation shows that the lexico-semantic knowledge implicitly encoded in PLMs does not always capture hierarchical relations. We further demonstrate that the proposed properties can be injected into PLMs to improve their understanding of hierarchy. Through evaluations on taxonomy reconstruction, hypernym discovery and reading comprehension tasks, we show that the knowledge about hierarchy is moderately but not systematically transferable across tasks. | Jesús LovónMelgarejo, José G. Moreno, Romaric Besançon, Olivier Ferret, Lynda Tamine |  |
|  |  [HyperPIE: Hyperparameter Information Extraction from Scientific Publications](https://doi.org/10.1007/978-3-031-56060-6_17) |  | 0 | Automatic extraction of information from publications is key to making scientific knowledge machine readable at a large scale. The extracted information can, for example, facilitate academic search, decision making, and knowledge graph construction. An important type of information not covered by existing approaches is hyperparameters. In this paper, we formalize and tackle hyperparameter information extraction (HyperPIE) as an entity recognition and relation extraction task. We create a labeled data set covering publications from a variety of computer science disciplines. Using this data set, we train and evaluate BERT-based fine-tuned models as well as five large language models: GPT-3.5, GALACTICA, Falcon, Vicuna, and WizardLM. For fine-tuned models, we develop a relation extraction approach that achieves an improvement of 29 develop an approach leveraging YAML output for structured data extraction, which achieves an average improvement of 5.5 using JSON. With our best performing model we extract hyperparameter information from a large number of unannotated papers, and analyze patterns across disciplines. All our data and source code is publicly available at https://github.com/IllDepence/hyperpie | Tarek Saier, Mayumi Ohta, Takuto Asakura, Michael Färber |  |
|  |  [An EcoSage Assistant: Towards Building A Multimodal Plant Care Dialogue Assistant](https://doi.org/10.1007/978-3-031-56060-6_21) |  | 0 | In recent times, there has been an increasing awareness about imminent environmental challenges, resulting in people showing a stronger dedication to taking care of the environment and nurturing green life. The current $19.6 billion indoor gardening industry, reflective of this growing sentiment, not only signifies a monetary value but also speaks of a profound human desire to reconnect with the natural world. However, several recent surveys cast a revealing light on the fate of plants within our care, with more than half succumbing primarily due to the silent menace of improper care. Thus, the need for accessible expertise capable of assisting and guiding individuals through the intricacies of plant care has become paramount more than ever. In this work, we make the very first attempt at building a plant care assistant, which aims to assist people with plant(-ing) concerns through conversations. We propose a plant care conversational dataset named Plantational, which contains around 1K dialogues between users and plant care experts. Our end-to-end proposed approach is two-fold : (i) We first benchmark the dataset with the help of various large language models (LLMs) and visual language model (VLM) by studying the impact of instruction tuning (zero-shot and few-shot prompting) and fine-tuning techniques on this task; (ii) finally, we build EcoSage, a multi-modal plant care assisting dialogue generation framework, incorporating an adapter-based modality infusion using a gated mechanism. We performed an extensive examination (both automated and manual evaluation) of the performance exhibited by various LLMs and VLM in the generation of the domain-specific dialogue responses to underscore the respective strengths and weaknesses of these diverse models. | Mohit Tomar, Abhisek Tiwari, Tulika Saha, Prince Jha, Sriparna Saha |  |
|  |  [Controllable Decontextualization of Yes/No Question and Answers into Factual Statements](https://doi.org/10.1007/978-3-031-56060-6_27) |  | 0 | Yes/No or polar questions represent one of the main linguistic questioncategories. They consist of a main interrogative clause, for which the answeris binary (assertion or negation). Polar questions and answers (PQA) representa valuable knowledge resource present in many community and other curated QAsources, such as forums or e-commerce applications. Using answers to polarquestions alone in other contexts is not trivial. Answers are contextualized,and presume that the interrogative question clause and any shared knowledgebetween the asker and answerer are provided. We address the problem of controllable rewriting of answers to polarquestions into decontextualized and succinct factual statements. We propose aTransformer sequence to sequence model that utilizes soft-constraints to ensurecontrollable rewriting, such that the output statement is semanticallyequivalent to its PQA input. Evaluation on three separate PQA datasets asmeasured through automated and human evaluation metrics show that our proposedapproach achieves the best performance when compared to existing baselines. | Lingbo Mo, Besnik Fetahu, Oleg Rokhlenko, Shervin Malmasi | Amazon Com Inc, Seattle, WA USA; Ohio State Univ, Columbus, OH 43210 USA |
|  |  [Reading Between the Frames: Multi-modal Depression Detection in Videos from Non-verbal Cues](https://doi.org/10.1007/978-3-031-56027-9_12) |  | 0 | Depression, a prominent contributor to global disability, affects a substantial portion of the population. Efforts to detect depression from social media texts have been prevalent, yet only a few works explored depression detection from user-generated video content. In this work, we address this research gap by proposing a simple and flexible multi-modal temporal model capable of discerning non-verbal depression cues from diverse modalities in noisy, real-world videos. We show that, for in-the-wild videos, using additional high-level non-verbal cues is crucial to achieving good performance, and we extracted and processed audio speech embeddings, face emotion embeddings, face, body and hand landmarks, and gaze and blinking information. Through extensive experiments, we show that our model achieves state-of-the-art results on three key benchmark datasets for depression detection from video by a substantial margin. Our code is publicly available on GitHub. | David GimenoGómez, AnaMaria Bucur, Adrian Cosma, Carlos David MartínezHinarejos, Paolo Rosso |  |
|  |  [Investigating the Effects of Sparse Attention on Cross-Encoders](https://doi.org/10.1007/978-3-031-56027-9_11) |  | 0 | Cross-encoders are effective passage and document re-rankers but less efficient than other neural or classic retrieval models. A few previous studies have applied windowed self-attention to make cross-encoders more efficient. However, these studies did not investigate the potential and limits of different attention patterns or window sizes. We close this gap and systematically analyze how token interactions can be reduced without harming the re-ranking effectiveness. Experimenting with asymmetric attention and different window sizes, we find that the query tokens do not need to attend to the passage or document tokens for effective re-ranking and that very small window sizes suffice. In our experiments, even windows of 4 tokens still yield effectiveness on par with previous cross-encoders while reducing the memory requirements to at most 78 for passages / documents. | Ferdinand Schlatt, Maik Fröbe, Matthias Hagen |  |
|  |  [SumBlogger: Abstractive Summarization of Large Collections of Scientific Articles](https://doi.org/10.1007/978-3-031-56027-9_23) |  | 0 | We propose a prompt-based pipeline for extreme summarization of large collections of scientific articles, which facilitates the consumption of scientific knowledge in high-volume fast-paced fields like AI. Although prompting of generative large language models (LLMs) has been applied to news summarization, its effectiveness in the scientific domain and in multi-document summarization is underexplored. We propose a three-step approach for summarizing a large collection of documents (e.g. hundreds or thousands of papers published in a conference). First, selecting representative papers per document cluster, second, performing single-document summarization (SDS) of the selected papers, and third, aggregating these in a multi-document summarization (MDS) step. Both the single-document summaries and the multi-document summaries are generated with an instruction-tuned LLM. The cluster summaries are used to generate a blog post summarizing a conference. We show that our SDS model achieves better results than strong fine-tuned models on the SciTLDR benchmark. Our two-step approach reaches the performance of state-of-the-art fine-tuned MDS models on the Multi-XScience benchmark. Through a small-scale user study, we find that , although a human-written blog post is clearly preferred over an automatically generated one, the users appreciate the good informativeness and factuality of our pipeline. Our findings demonstrate the potential use of generative LLMs as a way to digest large amounts of scientific papers and help researchers to stay up-to-date with rapidly evolving fields. | Pavlos Zakkas, Suzan Verberne, Jakub Zavrel | Leiden Univ, Leiden, Netherlands; Zeta Alpha, Amsterdam, Netherlands |
|  |  [Role-Guided Contrastive Learning for Event Argument Extraction](https://doi.org/10.1007/978-3-031-56027-9_21) |  | 0 | Event argument extraction is a subtask of information extraction. Recent efforts have predominantly focused on mitigating the issue of error propagation associated with pipeline methods for extracting event arguments, such as machine reading comprehension and generative approaches. However, these aforementioned methods necessitate the careful design of various templates, and the choice of templates can significantly impact the model's performance. Therefore, we propose a novel approach to extract event arguments using contrastive learning. Our approach aims to maximize the semantic similarity between role name semantics and actual argument semantics while minimizing the similarity between role name semantics and the semantics of other non-argument words, thereby enabling more precise extraction of argument boundaries. We investigate the impact of different templates on event argument extraction, and experimental results demonstrate that template adjustments have limited effects on our model. To attain more precise argument boundaries, we also introduce entity type boundary embeddings, which substantially enhance the effectiveness of event argument extraction. | Chunyu Yao, Yi Guo, Xue Chen, Zhenzhen Duan, Jiaojiao Fu | East China Univ Sci & Technol, Shanghai, Peoples R China |
|  |  [Attend All Options at Once: Full Context Input for Multi-choice Reading Comprehension](https://doi.org/10.1007/978-3-031-56027-9_24) |  | 0 | This paper proposes a method to capture the relations between options in Multiple-choice Machine Reading Comprehension (MMRC) tasks. MMRC is a form of question answering (QA) in which the question is about a given text, and multiple answers are provided as options. Capturing the relations between options is especially important for options with information references between them that cannot stand alone as responses to the questions, such as "None of the above". Our method 1) takes the whole sample including identification of the passage, question, and all options as input for pre-trained language models, and 2) adds a fuser network to emphasize the information interaction between options. Experimental results show that our method improves over the common encoding approaches on COSMOS-QA, an MMRC dataset with between-option references, while having a relatively small impact on other MMRC datasets without references between the options. We conclude that our method actually helps to capture the necessary relationships between options. In addition, our method can reduce the memory usage required for training, and the model can be easily transferred to other domains and models. | Runda Wang, Suzan Verberne, Marco Spruit | Leiden Univ, Leiden, Netherlands |
|  |  [Zero-Shot Generative Large Language Models for Systematic Review Screening Automation](https://doi.org/10.1007/978-3-031-56027-9_25) |  | 0 | Systematic reviews are crucial for evidence-based medicine as they comprehensively analyse published research findings on specific questions. Conducting such reviews is often resource- and time-intensive, especially in the screening phase, where abstracts of publications are assessed for inclusion in a review. This study investigates the effectiveness of using zero-shot large language models~(LLMs) for automatic screening. We evaluate the effectiveness of eight different LLMs and investigate a calibration technique that uses a predefined recall threshold to determine whether a publication should be included in a systematic review. Our comprehensive evaluation using five standard test collections shows that instruction fine-tuning plays an important role in screening, that calibration renders LLMs practical for achieving a targeted recall, and that combining both with an ensemble of zero-shot models saves significant screening time compared to state-of-the-art approaches. | Shuai Wang, Harrisen Scells, Shengyao Zhuang, Martin Potthast, Bevan Koopman, Guido Zuccon |  |
|  |  [WebSAM-Adapter: Adapting Segment Anything Model for Web Page Segmentation](https://doi.org/10.1007/978-3-031-56027-9_27) |  | 0 | With the advancement of internet technology, web page segmentation, which aims to divide web pages into semantically coherent units, has become increasingly crucial for web-related applications. Conventional purely visual web page segmentation approaches, which depend on traditional edge detection, face challenges in generalizing across complex web pages. Recently, the Segment Anything Model (SAM) represents remarkable visual understanding and segmentation abilities. This inspires us that SAM can also demonstrate great potential in Web Page Segmentation. However, due to the lack of web-specific training data, its direct adaptation to web page segmentation domain has been hindered. To address this challenge, we propose WebSAM-Adapter, an effective adaptation of SAM, featuring a three-module architecture specifically tailored for web page segmentation with minimal additional trainable parameters. First, we propose a patch embedding tune module for adjusting the frozen patch embedding features, which is crucial for modifying the distribution of the original model. Second, an edge components tune module is designed to learn significant structural features within each web page. Finally, the outputs of these specialized modules are sent into our key Adapter module, which employs a lightweight multi-layer perceptron (MLP) to amalgamate these enriched features and generate webpage-specific knowledge. To the best of our knowledge, our method is the first successful adaptation of a large visual model like SAM to web page segmentation. Empirical evaluations on the comprehensive Webis-WebSeg-20 dataset demonstrate our model's state-of-the-art performance. | Bowen Ren, Zefeng Qian, Yuchen Sun, Chao Gao, Chongyang Zhang | Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China; China Pacific Insurance Grp Co Ltd, Shanghai 200010, Peoples R China |
|  |  [A Phrase-Level Attention Enhanced CRF for Keyphrase Extraction](https://doi.org/10.1007/978-3-031-56027-9_28) |  | 0 | Since sequence labeling-based methods take into account the dependencies between neighbouring labels, they have been widely used for keyphrase prediction. Existing methods mainly focus on the word-level sequence labeling over the word-level features, and fail to capture the phrase-level information (i.e., inner properties of multi-word keyphrases). In this paper, we concentrate on how to effectively capture the phrase-level features and then integrate them with the word-level features to improve the performance of keyphrase extraction in the sequence labeling-based method. Specifically, we propose a phrase-level attention enhanced conditional random field (PAE-CRF) model for keyphrase extraction, which consists of two major modules: a phrase-level attention module that captures phrase-level features, and a phrase-level attention enhanced CRF module that integrates the phrase-level attention information with the word-level features into CRF to extract keyphrases. Finally, these two modules are jointly trained to help them learn complementary information from each other. Compared with the recent state-of-the-art methods, our model can achieve better results through experiments on four benchmark datasets. The code and keyphrase prediction results of our model are available in public at https://github.com/pae-crf/PAE-CRF . | Shinian Li, Tao Jiang, Yuxiang Zhang | Civil Aviat Univ China, Sch Comp Sci & Technol, Tianjin, Peoples R China |
|  |  [Taxonomy of Mathematical Plagiarism](https://doi.org/10.1007/978-3-031-56066-8_2) |  | 0 | Plagiarism is a pressing concern, even more so with the availability of large language models. Existing plagiarism detection systems reliably find copied and moderately reworded text but fail for idea plagiarism, especially in mathematical science, which heavily uses formal mathematical notation. We make two contributions. First, we establish a taxonomy of mathematical content reuse by annotating potentially plagiarised 122 scientific document pairs. Second, we analyze the best-performing approaches to detect plagiarism and mathematical content similarity on the newly established taxonomy. We found that the best-performing methods for plagiarism and math content similarity achieve an overall detection score (PlagDet) of 0.06 and 0.16, respectively. The best-performing methods failed to detect most cases from all seven newly established math similarity types. Outlined contributions will benefit research in plagiarism detection systems, recommender systems, question-answering systems, and search engines. We make our experiment's code and annotated dataset available to the community: https://github.com/gipplab/Taxonomy-of-Mathematical-Plagiarism | Ankit Satpute, André GreinerPetter, Noah Gießing, Isabel Beckenbach, Moritz Schubotz, Olaf Teschke, Akiko Aizawa, Bela Gipp |  |
|  |  [Unraveling Disagreement Constituents in Hateful Speech](https://doi.org/10.1007/978-3-031-56066-8_3) |  | 0 | This paper presents a probabilistic semantic approach to identifying disagreement-related textual constituents in hateful content. Several methodologies to exploit the selected constituents to determine if a message could lead to disagreement have been defined. The proposed approach is evaluated on 4 datasets made available for the SemEval 2023 Task 11 shared task, highlighting that a few constituents can be used as a proxy to identify if a sentence could be perceived differently by multiple readers. The source code of our approaches is publicly available ( https://github.com/MIND-Lab/Unrevealing-Disagreement-Constituents-in-Hateful-Speech ). | Giulia Rizzi, Alessandro Astorino, Paolo Rosso, Elisabetta Fersini | Univ Politecn Valencia, Valencia, Spain; Univ Milano Bicocca, Milan, Italy |
|  |  [SoftQE: Learned Representations of Queries Expanded by LLMs](https://doi.org/10.1007/978-3-031-56066-8_8) |  | 0 | We investigate the integration of Large Language Models (LLMs) into query encoders to improve dense retrieval without increasing latency and cost, by circumventing the dependency on LLMs at inference time. SoftQE incorporates knowledge from LLMs by mapping embeddings of input queries to those of the LLM-expanded queries. While improvements over various strong baselines on in-domain MS-MARCO metrics are marginal, SoftQE improves performance by 2.83 absolute percentage points on average on five out-of-domain BEIR tasks. | Varad Pimpalkhute, John Heyer, Xusen Yin, Sameer Gupta |  |
|  |  [Optimizing BERTopic: Analysis and Reproducibility Study of Parameter Influences on Topic Modeling](https://doi.org/10.1007/978-3-031-56066-8_14) |  | 0 | This paper reproduces key experiments and results from the BERTopic neural topic modeling framework. We validate prior findings regarding the role of text preprocessing, embedding models and term weighting strategies in optimizing BERTopic's modular pipeline. Specifically, we show that advanced embedding models like MPNet benefit from raw input while simpler models like GloVe perform better with pre-processed text. We also demonstrate that excluding outlier documents from the topic model provides minimal gains. Additionally, we highlight that appropriate term weighting schemes, such as root TF-BM25(IDF), are critical for topic quality. We manage to reproduce prior results and our rigorous reproductions affirm the effectiveness of BERTopic's flexible framework while providing novel insights into tuning its components for enhanced topic modeling performance. The findings offer guidance and provide insightful refinements and clarifications, serving as a valuable reference for both researchers and practitioners applying clustering-based neural topic modeling. | Martin Borcin, Joemon M. Jose | Univ Glasgow, Glasgow G12 8QQ, Scotland |
|  |  [A Reproducibility Study of Goldilocks: Just-Right Tuning of BERT for TAR](https://doi.org/10.1007/978-3-031-56066-8_13) |  | 0 | Screening documents is a tedious and time-consuming aspect of high-recall retrieval tasks, such as compiling a systematic literature review, where the goal is to identify all relevant documents for a topic. To help streamline this process, many Technology-Assisted Review (TAR) methods leverage active learning techniques to reduce the number of documents requiring review. BERT-based models have shown high effectiveness in text classification, leading to interest in their potential use in TAR workflows. In this paper, we investigate recent work that examined the impact of further pre-training epochs on the effectiveness and efficiency of a BERT-based active learning pipeline. We first report that we could replicate the original experiments on two specific TAR datasets, confirming some of the findings: importantly, that further pre-training is critical to high effectiveness, but requires attention in terms of selecting the correct training epoch. We then investigate the generalisability of the pipeline on a different TAR task, that of medical systematic reviews. In this context, we show that there is no need for further pre-training if a domain-specific BERT backbone is used within the active learning pipeline. This finding provides practical implications for using the studied active learning pipeline within domain-specific TAR tasks. | Xinyu Mao, Bevan Koopman, Guido Zuccon |  |
|  |  [Good for Children, Good for All?](https://doi.org/10.1007/978-3-031-56066-8_24) |  | 0 | In this work, we reason how focusing on Information Retrieval (IR) for children and involving them in participatory studies would benefit the IR community. The Child Computer Interaction (CCI) community has embraced the child as a protagonist as their main philosophy, regarding children as informants, co-designers, and evaluators, not just users. Leveraging prior literature, we posit that putting children in the centre of the IR world and giving them an active role could enable the IR community to break free from the preexisting bias derived from interpretations inferred from past use by adult users and the still dominant system-oriented approach. This shift would allow researchers to revisit complex foundational concepts that greatly influence the use of IR tools as part of socio-technical systems in different domains. In doing so, IR practitioners could provide more inclusive, and supportive information access experiences to children and other understudied user groups alike in different contexts. | Monica Landoni, Theo Huibers, Emiliana Murgia, Maria Soledad Pera | Univ Genoa, Genoa, Italy; Delft Univ Technol, Web Informat Syst, Delft, Netherlands; Univ Svizzera Italiana, Lugano, Switzerland; Univ Twente, Enschede, Netherlands |
|  |  [Mu2STS: A Multitask Multimodal Sarcasm-Humor-Differential Teacher-Student Model for Sarcastic Meme Detection](https://doi.org/10.1007/978-3-031-56063-7_2) |  | 0 | Memes, a prevalent form of online communication, often express opinions, emotions, and creativity concisely and entertainingly. Amidst the diverse landscape of memes, the realm of sarcastic memes holds a unique position with its foundation in irony, mockery, satire, and messages that diverge from literal meanings. Detecting sarcasm in memes is challenging due to the intricate interplay between sarcasm and humor. While prior research has primarily concentrated on leveraging the relationship between sarcasm and humor for identifying sarcastic memes, our goal in this paper extends beyond establishing a fundamental connection between the two; instead, we aspire to unravel their distinct characteristics and nuances that differentiate sarcasm from humor. To accomplish this, we introduce a novel deep learning model, i.e., Mu2STS (Mu ltitask Mu ltimodal S arcasm-Humor-Differential T eacher-S tudent), for sarcasm detection in memes, with a special focus on humor. To bolster Mu2STS, we have developed the SHMH (WARNING: This paper contains meme samples that are offensive in nature.) (Sarcasm-with-Humorous-Meme-in-Hindi) dataset, designed for detecting sarcasm and humor in memes written in the Hindi language, which is the first of its kind to the best of our knowledge. Our empirical evaluation, which includes both qualitative and quantitative analyses conducted on the SHMH dataset and some benchmark meme datasets, clearly illustrates the effectiveness of Mu2STS, which outperformed major state-of-the-art models. (The dataset and codes are available at https://www.iitp.ac.in/~ai-nlp-ml/resources.html .) | Gitanjali Kumari, Chandranath Adak, Asif Ekbal | Indian Inst Technol Patna, Dept Comp Sci & Engn, Bihta, India |
|  |  [An Adaptive Feature Selection Method for Learning-to-Enumerate Problem](https://doi.org/10.1007/978-3-031-56063-7_8) |  | 0 | In this paper, we propose a method for quickly finding a given number of instances of a target class from a fixed data set. We assume that we have a noisy query consisting of both useful and useless features (e.g., keywords). Our method finds target instances and trains a classifier simultaneously in a greedy strategy: it selects an instance most likely to be of the target class, manually label it, and add it to the training set to retrain the classifier, which is used for selecting the next item. In order to quickly inactivate useless query features, our method compares discriminative power of features, and if a feature is inferior to any other feature, the weight 0 is assigned to the inferior one. The weight is 1 otherwise. The greedy strategy explained above has a problem of bias: the classifier is biased toward target instances found earlier, and deteriorates after running out of similar target instances. To avoid it, when we run out of items that have the superior features, we re-activate the inactivated inferior features. By this mechanism, our method adaptively shifts to new regions in the data space. Our experiment shows that our binary and adaptive feature weighting method outperforms existing methods. | Satoshi Horikawa, Chiyonosuke Nemoto, Keishi Tajima, Masaki Matsubara, Atsuyuki Morishima | Univ Tsukuba, 1-2 Kasuga, Tsukuba, Ibaraki 3058550, Japan; Kyoto Univ, Kyoto 6068501, Japan |
|  |  [Asking Questions Framework for Oral History Archives](https://doi.org/10.1007/978-3-031-56063-7_11) |  | 0 | The importance of oral history archives in preserving and understanding past experiences is counterbalanced by the challenges encountered in accessing and searching through them, primarily due to their extensive size and the diverse demographics of the speakers. This paper presents an approach combining ASR technology and Transformer-based neural networks into the Asking questions framework. Its primary function is to generate questions accompanied by concise answers that relate to the topics discussed in each interview segment. Additionally, we introduce a semantic continuity model that filters the generated questions, ensuring that only the most relevant ones are retained. This enables a real-time semantic search through thousands of hours of recordings, with the crucial benefit that the speakers' original words remain unaltered and still semantically align with the query. While the method is exemplified using a specific publicly available archive, its applicability extends universally to datasets of a similar nature. | Jan Svec, Martin Bulín, Adam Frémund, Filip Polák | Univ West Bohemia, Fac Sci Appl, Dept Cybernet, Plzen, Czech Republic |
|  |  [Yes, This Is What I Was Looking For! Towards Multi-modal Medical Consultation Concern Summary Generation](https://doi.org/10.1007/978-3-031-56063-7_14) |  | 0 | Over the past few years, the use of the Internet for healthcare-related tasks has grown by leaps and bounds, posing a challenge in effectively managing and processing information to ensure its efficient utilization. During moments of emotional turmoil and psychological challenges, we frequently turn to the internet as our initial source of support, choosing this over discussing our feelings with others due to the associated social stigma. In this paper, we propose a new task of multi-modal medical concern summary (MMCS) generation, which provides a short and precise summary of patients' major concerns brought up during the consultation. Nonverbal cues, such as patients' gestures and facial expressions, aid in accurately identifying patients' concerns. Doctors also consider patients' personal information, such as age and gender, in order to describe the medical condition appropriately. Motivated by the potential efficacy of patients' personal context and visual gestures, we propose a transformer-based multi-task, multi-modal intent-recognition, and medical concern summary generation (IR-MMCSG) system. Furthermore, we propose a multitasking framework for intent recognition and medical concern summary generation for doctor-patient consultations. We construct the first multi-modal medical concern summary generation (MM-MediConSummation) corpus, which includes patient-doctor consultations annotated with medical concern summaries, intents, patient personal information, doctor's recommendations, and keywords. Our experiments and analysis demonstrate (a) the significant role of patients' expressions/gestures and their personal information in intent identification and medical concern summary generation, and (b) the strong correlation between intent recognition and patients' medical concern summary generation The dataset and source code are available at https://github.com/NLP-RL/MMCSG. | Abhisek Tiwari, Shreyangshu Bera, Sriparna Saha, Pushpak Bhattacharyya, Samrat Ghosh |  |
|  |  [Interactive Topic Tagging in Community Question Answering Platforms](https://doi.org/10.1007/978-3-031-56063-7_13) |  | 0 | Community question-answering platforms offer new opportunities for users to share knowledge online. Such platforms allow building communities around areas of interest, and enable community members to post questions and have other members answer them. In this paper, we investigate a novel, interactive approach for tagging input questions with relevant topics, which are needed by community question-answering platforms for various tasks such as indexing and routing. Iteratively, we employ explicit feedback from the users who post questions to fine-tune further the tag suggestions for those questions. We show that our proposed method is able to suggest tags efficiently, and outperforms state-of-the-art methods applied to the tag suggestion task. | Radin Hamidi Rad, Silviu Cucerzan, Nirupama Chandrasekaran, Michael Gamon | Microsoft Res, Redmond, WA USA; Toronto Metropolitan Univ, Toronto, ON, Canada |
|  |  [Mitigating Data Sparsity via Neuro-Symbolic Knowledge Transfer](https://doi.org/10.1007/978-3-031-56063-7_15) |  | 0 | Data sparsity is a well-known historical limitation of recommender systems that still impacts the performance of state-of-the-art approaches. The literature proposed various ways to mitigate the problem by providing additional information to the model (e.g., hybrid recommendation, knowledge graph-based systems). In particular, one promising technique involves transferring information from other domains or tasks to compensate for sparsity in the target domain, where the recommendations must be performed. Following this idea, we propose a novel approach based on Neuro-Symbolic computing designed for the knowledge transfer task in recommender systems. In particular, we use a Logic Tensor Network (LTN) to train vanilla Latent Factor Models (LFMs) for rating prediction. We show how the LTN can be used to regularize the LFMs using axiomatic knowledge that permits injecting pre-trained information learned by Collaborative Filtering on a different task or domain. Extensive experiments comparing our models with different baselines on two versions of a novel real-world dataset prove our proposal's potential in the knowledge transfer task. In particular, our models outperform the baselines, including those that can encode additional information, suggesting that the knowledge is effectively transferred to the target domain via logical reasoning. Moreover, an experiment that drastically decreases the density of user-item ratings shows that the benefits of the acquired knowledge increase with the sparsity of the dataset, showing the importance of exploiting knowledge from a denser source of information when training data is scarce in the target domain. | Tommaso Carraro, Alessandro Daniele, Fabio Aiolli, Luciano Serafini | Fdn Bruno Kessler, Data & Knowledge Management Unit, Via Sommarive 18, I-38123 Povo, Italy; Univ Padua, Dept Math, Via Trieste 63, I-35121 Padua, Italy |
|  |  [Enhancing Legal Named Entity Recognition Using RoBERTa-GCN with CRF: A Nuanced Approach for Fine-Grained Entity Recognition](https://doi.org/10.1007/978-3-031-56063-7_17) |  | 0 | Accurate identification of named entities is pivotal for the advancement of sophisticated legal Artificial Intelligence (AI) applications. However, the legal domain presents distinct challenges due to the presence of fine-grained, domain-specific entities, including lawyers, judges, courts, and precedents. This necessitates a nuanced approach to Named Entity Recognition (NER). In this paper, we introduce a novel NER approach tailored to the legal domain. Our system combines Robustly Optimized BERT (RoBERTa) with a Graph Convolutional Network (GCN) to harness two distinct types of complementary information related to words in the data. Furthermore, the application of a Conditional Random Field (CRF) at the output layer ensures global consistency in data labeling by considering the entire sequence when predicting a named entity. RoBERTa captures contextual information about individual words, while GCN allows us to exploit the mutual relationships between words, resulting in more precise named entity identification. Our results indicate that RoBERTa-GCN (CRF) outperforms other standard settings, such as, RoBERTa, textGCN, and BiLSTM, including state-of-the-art for NER in the legal domain. | Arihant Jain, Raksha Sharma | Indian Inst Technol Roorkee, Roorkee, India |
|  |  [A Novel Multi-Stage Prompting Approach for Language Agnostic MCQ Generation Using GPT](https://doi.org/10.1007/978-3-031-56063-7_18) |  | 0 | We introduce a multi-stage prompting approach (MSP) for the generation of multiple choice questions (MCQs), harnessing the capabilities of GPT models such as text-davinci-003 and GPT-4, renowned for their excellence across various NLP tasks. Our approach incorporates the innovative concept of chain-of-thought prompting, a progressive technique in which the GPT model is provided with a series of interconnected cues to guide the MCQ generation process. Automated evaluations consistently demonstrate the superiority of our proposed MSP method over the traditional single-stage prompting (SSP) baseline, resulting in the production of high-quality distractors. Furthermore, the one-shot MSP technique enhances automatic evaluation results, contributing to improved distractor generation in multiple languages, including English, German, Bengali, and Hindi. In human evaluations, questions generated using our approach exhibit superior levels of grammaticality, answerability, and difficulty, highlighting its efficacy in various languages. | Subhankar Maity, Aniket Deroy, Sudeshna Sarkar |  |
|  |  [A Study on Hierarchical Text Classification as a Seq2seq Task](https://doi.org/10.1007/978-3-031-56063-7_20) |  | 0 | With the progress of generative neural models, Hierarchical Text Classification (HTC) can be cast as a generative task. In this case, given an input text, the model generates the sequence of predicted class labels taken from a label tree of arbitrary width and depth. Treating HTC as a generative task introduces multiple modeling choices. These choices vary from choosing the order for visiting the class tree and therefore defining the order of generating tokens, choosing either to constrain the decoding to labels that respect the previous level predictions, up to choosing the pre-trained Language Model itself. Each HTC model therefore differs from the others from an architectural standpoint, but also from the modeling choices that were made. Prior contributions lack transparent modeling choices and open implementations, hindering the assessment of whether model performance stems from architectural or modeling decisions. For these reasons, we propose with this paper an analysis of the impact of different modeling choices along with common model errors and successes for this task. This analysis is based on an open framework coming along this paper that can facilitate the development of future contributions in the field by providing datasets, metrics, error analysis toolkit and the capability to readily test various modeling choices for one given model. | Fatos Torba, Christophe Gravier, Charlotte Laclau, Abderrhammen Kammoun, Julien Subercaze | AItenders, St Etienne, France; CNRS, Lab Hubert Curien, UMR 5516, St Etienne, France; Inst Polytech Paris, Telecom Paris, Paris, France |
|  |  [MFVIEW: Multi-modal Fake News Detection with View-Specific Information Extraction](https://doi.org/10.1007/978-3-031-56063-7_26) |  | 0 | The spread of fake news on social media is a rapidly growing problem that is impacting both the general public and the government. Current methods for detecting false news often fail to take full advantage of the multi-modal information that is available, which can lead to inconsistent decisions due to modality ambiguity. Moreover, existing methods often overlook the unique information pertaining to view-specific details that could significantly boost their discriminative power and overall performance. To this end, we introduce a novel model, MFVIEW (Multi-Modal Fake News Detection with View-Specific Information Extraction), that unifies the modeling of multi-modal and view-specific information within a single framework. Specifically, the proposed model consists of a View-Specific Information Extractor that incorporates an orthogonal constraint within the shared subspace, enabling the utilization of discriminative information unique to each modality, and an Ambiguity Cross-Training Module that detects inherent ambiguity across different modalities by capturing their correlation. Extensive experiments on two publicly available datasets show that MFVIEW outperforms state-of-the-art fake news detection approaches with an accuracy of 91.0% on the Twitter dataset and 93.3% on the Weibo dataset. | Marium Malik, Jiaojiao Jiang, Yang Song, Sanjay Jha | Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW, Australia |
|  |  [Navigating Uncertainty: Optimizing API Dependency for Hallucination Reduction in Closed-Book QA](https://doi.org/10.1007/978-3-031-56063-7_31) |  | 0 | While Large Language Models (LLM) are able to accumulate and restore knowledge, they are still prone to hallucination. Especially when faced with factual questions, LLM cannot only rely on knowledge stored in parameters to guarantee truthful and correct answers. Augmenting these models with the ability to search on external information sources, such as the web, is a promising approach to ground knowledge to retrieve information. However, searching in a large collection of documents introduces additional computational/time costs. An optimal behavior would be to query external resources only when the LLM is not confident about answers. In this paper, we propose a new LLM able to self-estimate if it is able to answer directly or needs to request an external tool. We investigate a supervised approach by introducing a hallucination masking mechanism in which labels are generated using a close book question-answering task. In addition, we propose to leverage parameter-efficient fine-tuning techniques to train our model on a small amount of data. Our model directly provides answers for 78.2% of the known queries and opts to search for 77.2% of the unknown ones. This results in the API being utilized only 62% of the time. | Pierre Erbacher, Louis Falissard, Vincent Guigue, Laure Soulier | Sorbonne Ctr Artificial Intelligence, Paris, France; Sorbonne Univ, Paris, France; Paris Saclay, AgroParisTech, Gif Sur Yvette, France |
|  |  [Can We Predict QPP? An Approach Based on Multivariate Outliers](https://doi.org/10.1007/978-3-031-56063-7_38) |  | 0 | Query performance prediction (QPP) aims to forecast the effectiveness of a search engine across a range of queries and documents. While state-of-the-art predictors offer a certain level of precision, their accuracy is not flawless. Prior research has recognized the challenges inherent in QPP but often lacks a thorough qualitative analysis. In this paper, we delve into QPP by examining the factors that influence the predictability of query performance accuracy. We propose the working hypothesis that while some queries are readily predictable, others present significant challenges. By focusing on outliers, we aim to identify the queries that are particularly challenging to predict. To this end, we employ multivariate outlier detection method. Our results demonstrate the effectiveness of this approach in identifying queries on which QPP do not perform well, yielding less reliable predictions. Moreover, we provide evidence that excluding these hard-to-predict queries from the analysis significantly enhances the overall accuracy of QPP. | AdrianGabriel Chifu, Sébastien Déjean, Moncef Garouani, Josiane Mothe, Diégo Ortiz, Md Zia Ullah |  |
|  |  [SALSA: Salience-Based Switching Attack for Adversarial Perturbations in Fake News Detection Models](https://doi.org/10.1007/978-3-031-56069-9_3) |  | 0 | Despite advances in fake news detection algorithms, recent research reveals that machine learning-based fake news detection models are still vulnerable to carefully crafted adversarial attacks. In this landscape, traditional methods, often relying on text perturbations or heuristic-based approaches, have proven insufficient, revealing a critical need for more nuanced and context-aware strategies to enhance the robustness of fake news detection. Our research identifies and addresses three critical areas: creating subtle perturbations, preserving core information while modifying sentence structure, and incorporating inherent interpretability. We propose SALSA, an adversarial Salience-based Switching Attack strategy that harnesses salient words, using similarity-based switching to address the shortcomings of traditional adversarial attack methods. Using SALSA, we perform a two-way attack: misclassifying real news as fake and fake news as real. Due to the absence of standardized metrics to evaluate adversarial attacks in fake news detection, we further propose three new evaluation metrics to gauge the attack's success. Finally, we validate the transferability of our proposed attack strategy across attacker and victim models, demonstrating our approach's broad applicability and potency. Code and data are available here at https://github.com/iamshnoo/salsa . | Chahat Raj, Anjishnu Mukherjee, Hemant Purohit, Antonios Anastasopoulos, Ziwei Zhu | George Mason Univ, Fairfax, VA 22030 USA |
|  |  [FakeClaim: A Multiple Platform-Driven Dataset for Identification of Fake News on 2023 Israel-Hamas War](https://doi.org/10.1007/978-3-031-56069-9_5) |  | 0 | We contribute the first publicly available dataset of factual claims from different platforms and fake YouTube videos on the 2023 Israel-Hamas war for automatic fake YouTube video classification. The FakeClaim data is collected from 60 fact-checking organizations in 30 languages and enriched with metadata from the fact-checking organizations curated by trained journalists specialized in fact-checking. Further, we classify fake videos within the subset of YouTube videos using textual information and user comments. We used a pre-trained model to classify each video with different feature combinations. Our best-performing fine-tuned language model, Universal Sentence Encoder (USE), achieves a Macro F1 of 87%, which shows that the trained model can be helpful for debunking fake videos using the comments from the user discussion. The dataset is available on Github[https://github.com/Gautamshahi/FakeClaim] | Gautam Kishore Shahi, Amit Kumar Jaiswal, Thomas Mandl |  |
|  |  [MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English Clinical Queries](https://doi.org/10.1007/978-3-031-56069-9_8) |  | 0 | In the healthcare domain, summarizing medical questions posed by patients is critical for improving doctor-patient interactions and medical decision-making. Although medical data has grown in complexity and quantity, the current body of research in this domain has primarily concentrated on text-based methods, overlooking the integration of visual cues. Also prior works in the area of medical question summarisation have been limited to the English language. This work introduces the task of multimodal medical question summarization for codemixed input in a low-resource setting. To address this gap, we introduce the Multimodal Medical Codemixed Question Summarization MMCQS dataset, which combines Hindi-English codemixed medical queries with visual aids. This integration enriches the representation of a patient's medical condition, providing a more comprehensive perspective. We also propose a framework named MedSumm that leverages the power of LLMs and VLMs for this task. By utilizing our MMCQS dataset, we demonstrate the value of integrating visual information from images to improve the creation of medically detailed summaries. This multimodal strategy not only improves healthcare decision-making but also promotes a deeper comprehension of patient queries, paving the way for future exploration in personalized and responsive medical care. Our dataset, code, and pre-trained models will be made publicly available. | Akash Ghosh, Arkadeep Acharya, Prince Jha, Sriparna Saha, Aniket Gaudgaul, Rajdeep Majumdar, Aman Chadha, Raghav Jain, Setu Sinha, Shivani Agarwal |  |
|  |  [The Open Web Index - Crawling and Indexing the Web for Public Use](https://doi.org/10.1007/978-3-031-56069-9_10) |  | 0 | Only few search engines index the Web at scale. Third parties who want to develop downstream applications based on web search fully depend on the terms and conditions of the few vendors. The public availability of the large-scale Common Crawl does not alleviate the situation, as it is often cheaper to crawl and index only a smaller collection focused on a downstream application scenario than to build and maintain an index for a general collection the size of the Common Crawl. Our goal is to improve this situation by developing the Open Web Index . The Open Web Index is a publicly funded basic infrastructure from which downstream applications will be able to select and compile custom indexes in a simple and transparent way. Our goal is to establish the Open Web Index along with associated data products as a new open web information intermediary. In this paper, we present our first prototype for the Open Web Index and our plans for future developments. In addition to the conceptual and technical background, we discuss how the information retrieval community can benefit from and contribute to the Open Web Index—for example, by providing resources, by providing pre-processing components and pipelines, or by creating new kinds of vertical search engines and test collections. | Gijs Hendriksen, Michael Dinzinger, Sheikh Mastura Farzana, Noor Afshan Fathima, Maik Fröbe, Sebastian Schmidt, Saber Zerhoudi, Michael Granitzer, Matthias Hagen, Djoerd Hiemstra, Martin Potthast, Benno Stein | German Aerosp Ctr DLR, Cologne, Germany; CERN, Geneva, Switzerland; Bauhaus Univ Weimar, Weimar, Germany; Radboud Univ Nijmegen, Nijmegen, Netherlands; Univ Leipzig, Leipzig, Germany; Friedrich Schiller Univ Jena, Jena, Germany; Univ Passau, Passau, Germany |
|  |  [Towards Robust Expert Finding in Community Question Answering Platforms](https://doi.org/10.1007/978-3-031-56069-9_12) |  | 0 | This paper introduces TUEF, a topic-oriented user-interaction model for fair Expert Finding in Community Question Answering (CQA) platforms. The Expert Finding task in CQA platforms involves identifying proficient users capable of providing accurate answers to questions from the community. To this aim, TUEF improves the robustness and credibility of the CQA platform through a more precise Expert Finding component. The key idea of TUEF is to exploit diverse types of information, specifically, content and social information, to identify more precisely experts thus improving the robustness of the task. We assess TUEF through reproducible experiments conducted on a large-scale dataset from StackOverflow. The results consistently demonstrate that TUEF outperforms state-of-the-art competitors while promoting transparent expert identification. | Maddalena Amendola, Andrea Passarella, Raffaele Perego | ISTI CNR, Pisa, Italy; Univ Pisa, Pisa, Italy; IIT CNR, Pisa, Italy |
|  |  [Interactive Document Summarization](https://doi.org/10.1007/978-3-031-56069-9_14) |  | 0 | With the advent of modern chatbots, automatic summarization is becoming common practice to quicken access to information. However the summaries they generate can be biased, unhelpful or untruthful. Hence, in sensitive scenarios, extractive summarization remains a more reliable approach. In this paper we present an original extractive method combining a GNN-based encoder and a RNN-based decoder, coupled with a user-friendly interface that allows for interactive summarization. | Raoufdine Said, Adrien Guille | Univ Lyon, ERIC UR 3083, Lyon, France |
|  |  [Physio: An LLM-Based Physiotherapy Advisor](https://doi.org/10.1007/978-3-031-56069-9_16) |  | 0 | The capabilities of the most recent language models have increased the interest in integrating them into real-world applications. However, the fact that these models generate plausible, yet incorrect text poses a constraint when considering their use in several domains. Healthcare is a prime example of a domain where text-generative trustworthiness is a hard requirement to safeguard patient well-being. In this paper, we present Physio, a chat-based application for physical rehabilitation. Physio is capable of making an initial diagnosis while citing reliable health sources to support the information provided. Furthermore, drawing upon external knowledge databases, Physio can recommend rehabilitation exercises and over-the-counter medication for symptom relief. By combining these features, Physio can leverage the power of generative models for language processing while also conditioning its response on dependable and verifiable sources. A live demo of Physio is available at https://physio.inesctec.pt. | Rúben Almeida, Hugo O. Sousa, Luís Filipe Cunha, Nuno Guimarães, Ricardo Campos, Alípio Jorge |  |
|  |  [eval-rationales: An End-to-End Toolkit to Explain and Evaluate Transformers-Based Models](https://doi.org/10.1007/978-3-031-56069-9_20) |  | 0 | State-of-the-art (SOTA) transformer-based models in the domains of Natural Language Processing (NLP) and Information Retrieval (IR) are often characterized by their opacity in terms of decision-making processes. This limitation has given rise to various techniques for enhancing model interpretability and the emergence of evaluation benchmarks aimed at designing more transparent models. These techniques are primarily focused on developing interpretable models with the explicit aim of shedding light on the rationales behind their predictions. Concurrently, evaluation benchmarks seek to assess the quality of these rationales provided by the models. Despite the availability of numerous resources for using these techniques and benchmarks independently, their seamless integration remains a non-trivial task. In response to this challenge, this work introduces an end-to-end toolkit that integrates the most common techniques and evaluation approaches for interpretability. Our toolkit offers user-friendly resources facilitating fast and robust evaluations. | Khalil Maachou, Jesús LovónMelgarejo, José G. Moreno, Lynda Tamine | Univ Paul Sabatier, CNRS, IRIT, UMR 5505, Toulouse, France |
|  |  [VADIS - A Variable Detection, Interlinking and Summarization System](https://doi.org/10.1007/978-3-031-56069-9_22) |  | 0 | The VADIS system addresses the demand of providing enhanced information access in the domain of the social sciences. This is achieved by allowing users to search and use survey variables in context of their underlying research data and scholarly publications which have been interlinked with each other. | Yavuz Selim Kartal, Muhammad Ahsan Shahid, Sotaro Takeshita, Tornike Tsereteli, Andrea Zielinski, Benjamin Zapilko, Philipp Mayr |  |
|  |  [Building and Evaluating a WebApp for Effortless Deep Learning Model Deployment](https://doi.org/10.1007/978-3-031-56069-9_26) |  | 0 | In the field of deep learning, particularly Natural Language Processing (NLP), model deployment is a key process for public testing and analysis. However, developing a deployment pipeline is often difficult and time-consuming. To address this challenge, we developed SUD.DL, a web application to simplify the model deployment process for NLP researchers. Our application provides significant improvements in deployment efficiency, functionality discoverability, and deployment functionality, allowing NLP researchers to quickly deploy and test models on the web. | Ruikun Wu, Jiaxuan Han, Jerome Ramos, Aldo Lipani | UCL, London, England |
|  |  [indxr: A Python Library for Indexing File Lines](https://doi.org/10.1007/978-3-031-56069-9_27) |  | 0 | indxr is a Python utility for indexing file lines that allows users to dynamically access specific ones, avoiding loading the entire file in the computer's main memory. indxr addresses two main issues related to working with textual data. First, users who do not have plenty of RAM at their disposal may struggle to work with large datasets. Since indxr allows accessing specific lines without loading entire files, users can work with datasets that do not fit into their computer's main memory. For example, it enables users to perform complex tasks with limited RAM without noticeable slowdowns, such as pre-processing texts and training Neural models for Information Retrieval or other tasks. Second, indxr reduces the burden of working with datasets split among multiple files by allowing users to load specific data by providing the related line numbers or the identifiers of the information they describe, thus providing convenient access to such data. This paper overviews indxr's main features. ( https://github.com/AmenRa/indxr ). | Elias Bassani, Nicola Tonellotto | Univ Pisa, Pisa, Italy |
|  |  [SciSpace Literature Review: Harnessing AI for Effortless Scientific Discovery](https://doi.org/10.1007/978-3-031-56069-9_28) |  | 0 | In the rapidly evolving landscape of academia, the scientific research community barely copes with the challenges posed by a surging volume of scientific literature. Nevertheless, discovering research remains an important step in the research workflow which is also proven to be a challenging one to automate. We present Scispace Literature Review, a sophisticated, multi-faceted tool that serves as a comprehensive solution to streamline the literature review process. By leveraging the state-of-the-art methods in vector-based search, reranking, and large language models, the tool delivers features like customizable search results, data exintegration with an AI assistant, multi-language support, top papers insights, and customizable results columns to cater a researcher's requirements, and accelerate literature exploration. Resources for simplified sharing and documentation further enhance the scope and depth and breadth of research. We demonstrate the extensive use and popularity of the tool among researchers with various metrics, highlighting its value as a resource to elevate scientific literature review. This tool can be tried using this link: https://typeset.io/search . | Siddhant Jain, Asheesh Kumar, Trinita Roy, Kartik Shinde, Goutham Vignesh, Rohan Tondulkar | SciSpace, Bengaluru, India |
|  |  [Let's Get It Started: Fostering the Discoverability of New Releases on Deezer](https://doi.org/10.1007/978-3-031-56069-9_33) |  | 0 | This paper presents our recent initiatives to foster the discoverability ofnew releases on the music streaming service Deezer. After introducing oursearch and recommendation features dedicated to new releases, we outline ourshift from editorial to personalized release suggestions using cold startembeddings and contextual bandits. Backed by online experiments, we discuss theadvantages of this shift in terms of recommendation quality and exposure of newreleases on the service. | Léa Briand, Théo Bontempelli, Walid Bendada, Mathieu Morlon, François Rigaud, Benjamin Chapus, Thomas Bouabça, Guillaume SalhaGalvan | Deezer Res, Paris, France |
|  |  [Augmenting KG Hierarchies Using Neural Transformers](https://doi.org/10.1007/978-3-031-56069-9_35) |  | 0 | This work leverages neural transformers to generate hierarchies in an existing knowledge graph. For small (<10,000 node) domain-specific KGs, we find that a combination of few-shot prompting with one-shot generation works well, while larger KG may require cyclical generation. Hierarchy coverage increased by 98% for intents and 95% for colors. | Sanat Sharma, Mayank Poddar, Jayant Kumar, Kosta Blank, Tracy Holloway King | Adobe Inc, San Jose, CA 95110 USA |
|  |  [Document Level Event Extraction from Narratives](https://doi.org/10.1007/978-3-031-56069-9_38) |  | 0 | One of the fundamental tasks in Information Extraction (IE) is Event Extraction (EE), an extensively studied and challenging task [13,15], which aims to identify and classify events from the text. This involves identifying the event's central word (trigger) and its participants (arguments) [1]. These elements capture the event semantics and structure, which have applications in various fields, including biomedical texts [42], cybersecurity [24], economics [12], literature [32], and history [33]. Structured knowledge derived from EE can also benefit other downstream tasks such as Question Answering [20,30], Natural Language Understanding [21], Knowledge Base Graphs [3,37], summarization [8,10,41] and recommendation systems [9,18]. Despite the existence of several English EE systems [2,22,25,26], they face limited portability to other languages [4] and most of them are designed for closed domains, posing difficulties in generalising. Furthermore, most current EE systems restrict their scope to the sentence level, assuming that all arguments are contained within the same sentence as their corresponding trigger. However, real-world scenarios often involve event arguments spanning multiple sentences, highlighting the need for document-level EE. | Luís Filipe Cunha | Univ Porto, FCUP, Porto, Portugal |
|  |  [Shuffling a Few Stalls in a Crowded Bazaar: Potential Impact of Document-Side Fairness on Unprivileged Info-Seekers](https://doi.org/10.1007/978-3-031-56069-9_43) |  | 0 | Information systems rely on algorithmic ranking to ascertain expected relevance. Concerns about this strategy have resulted in the emergence of a field of inquiry referred to as fair ranking. Within this field, the aim varies between one-sided and two-sided fairness across automatically generated rankings. But research has focused primarily on fairness among document providers as opposed to fairness among searchers. Concerns have already been raised about the present framing of fairness. In the following line of research, a novel framing concern is introduced, whereby researchers may fail to consider the broader context of search engine usage among protected groups of searchers. | Sean Healy | Dublin City Univ, ADAPT Ctr, Dublin, Ireland |
|  |  [Knowledge Transfer from Resource-Rich to Resource-Scarce Environments](https://doi.org/10.1007/978-3-031-56069-9_44) |  | 0 |  | Negin Ghasemi |  |
|  |  [PhD Candidacy: A Tutorial on Overcoming Challenges and Achieving Success](https://doi.org/10.1007/978-3-031-56069-9_45) |  | 0 | Undertaking a PhD is a demanding yet rewarding experience. PhD candidates develop a deep understanding of their research topic and acquire a wide range of skills, including (i) formulating research questions; (ii) conducting research ethically and rigorously; (iii) communicating research findings effectively to both academic and non-academic audiences alike; (iv) forging a profile as an independent researcher; and (v) developing a teaching portfolio. PhD candidates inevitably experience challenges during their candidature. These challenges can be overcome by applying various techniques to adapt and learn from these experiences. This tutorial introduces strategies to help them advance in the PhD process. It is presented by two early career researchers in information retrieval, who have the unique perspective of being close enough to their time as PhD candidates to remember the highs and lows of PhD life yet far enough removed from the process to reflect on their experiences and provide insights. The tutorial will empower attendees to share, review, and refine productivity methods for their PhD journey. It provides a non-judgemental platform for open discussions led by the presenters. | Johanne R. Trippas, David Maxwell | RMIT Univ, Melbourne, Vic, Australia; Booking Com, Delft, Netherlands |
|  |  [The CLEF-2024 CheckThat! Lab: Check-Worthiness, Subjectivity, Persuasion, Roles, Authorities, and Adversarial Robustness](https://doi.org/10.1007/978-3-031-56069-9_62) |  | 0 | The first five editions of the CheckThat! lab focused on the main tasks of the information verification pipeline: check-worthiness, evidence retrieval and pairing, and verification. Since the 2023 edition, it has been focusing on new problems that can support the research and decision making during the verification process. In this new edition, we focus on new problems and —for the first time— we propose six tasks in fifteen languages (Arabic, Bulgarian, English, Dutch, French, Georgian, German, Greek, Italian, Polish, Portuguese, Russian, Slovene, Spanish, and code-mixed Hindi-English): Task 1 estimation of check-worthiness (the only task that has been present in all CheckThat! editions), Task 2 identification of subjectivity (a follow up of CheckThat! 2023 edition), Task 3 identification of persuasion (a follow up of SemEval 2023), Task 4 detection of hero, villain, and victim from memes (a follow up of CONSTRAINT 2022), Task 5 Rumor Verification using Evidence from Authorities (a first), and Task 6 robustness of credibility assessment with adversarial examples (a first). These tasks represent challenging classification and retrieval problems at the document and at the span level, including multilingual and multimodal settings. | Alberto BarrónCedeño, Firoj Alam, Tanmoy Chakraborty, Tamer Elsayed, Preslav Nakov, Piotr Przybyla, Julia Maria Struß, Fatima Haouari, Maram Hasanain, Federico Ruggeri, Xingyi Song, Reem Suwaileh | Univ Appl Sci Potsdam, Potsdam, Germany; HBKU, Qatar Comp Res Inst, Doha, Qatar; Univ Bologna, DIT, Forli, Italy; Univ Pompeu Fabra, Barcelona, Spain; Indian Inst Technol Delhi, New Delhi, India; Qatar Univ, Doha, Qatar; Univ Sheffield, Sheffield, S Yorkshire, England; HBKU, Doha, Qatar; Univ Bologna, DISI, Bologna, Italy; Mohamed bin Zayed Univ Artificial Intelligence, Abu Dhabi, U Arab Emirates |
|  |  [ELOQUENT CLEF Shared Tasks for Evaluation of Generative Language Model Quality](https://doi.org/10.1007/978-3-031-56069-9_63) |  | 0 | ELOQUENT is a set of shared tasks for evaluating the quality and usefulness of generative language models. ELOQUENT aims to bring together some high-level quality criteria, grounded in experiences from deploying models in real-life tasks, and to formulate tests for those criteria, preferably implemented to require minimal human assessment effort and in a multilingual setting. The selected tasks for this first year of ELOQUENT are (1) probing a language model for topical competence; (2) assessing the ability of models to generate and detect hallucinations; (3) assessing the robustness of a model output given variation in the input prompts; and (4) establishing the possibility to distinguish human-generated text from machine-generated text. | Jussi Karlgren, Luise Dürlich, Evangelia Gogoulou, Liane Guillou, Joakim Nivre, Magnus Sahlgren, Aarne Talman | RISE Res Inst Sweden, Stockholm, Sweden; Silo AI, Helsinki, Finland |
|  |  [Overview of Touché 2024: Argumentation Systems](https://doi.org/10.1007/978-3-031-56069-9_64) |  | 0 | This paper is a condensed overview of Touche: the fifth edition of the lab on argumentation systems that was held at CLEF 2024. With the goal to foster the development of support-technologies for decision-making and opinion-forming, we organized three shared tasks: (1) Human value detection (ValueEval), where participants detect (implicit) references to human values and their attainment in text; (2) Multilingual Ideology and Power Identification in Parliamentary Debates, where participants identify from a speech the political leaning of the speaker's party and whether it was governing at the time of the speech (new task); and (3) Image retrieval or generation in order to convey the premise of an argument with visually. In this paper, we describe these tasks, their setup, and participating approaches in detail. | Johannes Kiesel, Çagri Çöltekin, Maximilian Heinrich, Maik Fröbe, Milad Alshomary, Bertrand De Longueville, Tomaz Erjavec, Nicolas Handke, Matyás Kopp, Nikola Ljubesic, Katja Meden, Nailia Mirzakhmedova, Vaidas Morkevicius, Theresa ReitisMünstermann, Mario Scharfbillig, Nicolas Stefanovitch, Henning Wachsmuth, Martin Potthast, Benno Stein | Univ Tubingen, Tubingen, Germany; Leibniz Univ Hannover, Hannover, Germany; Kaunas Univ Technol, Kaunas, Lithuania; Arcadia Sistemi Informativi Terr, Milan, Italy; Bauhaus Univ Weimar, Weimar, Germany; Jozef Stefan Inst, Ljubljana, Slovenia; Charles Univ Prague, Prague, Czech Republic; Univ Leipzig, Leipzig, Germany; Univ Kassel, Kassel, Germany; Joint Res Ctr JRC, European Commiss, Brussels, Belgium; Friedrich Schiller Univ, Jena, Germany |
|  |  [eRisk 2024: Depression, Anorexia, and Eating Disorder Challenges](https://doi.org/10.1007/978-3-031-56069-9_65) |  | 0 | In 2017, we launched eRisk as a CLEF Lab to encourage research on early risk detection on the Internet. Since then, thanks to the participants' work, we have developed detection models and datasets for depression, anorexia, pathological gambling and self-harm. In 2024, it will be the eighth edition of the lab, where we will present a revision of the sentence ranking for depression symptoms, the third edition of tasks on early alert of anorexia and eating disorder severity estimation. This paper outlines the work that we have done to date, discusses key lessons learned in previous editions, and presents our plans for eRisk 2024. | Javier Parapar, Patricia MartínRodilla, David E. Losada, Fabio Crestani | Univ Svizzera Italiana USI, Fac Informat, Lugano, Switzerland; Univ Santiago de Compostela, Ctr Singular Invest Tecnol Intelixentes CiTIUS, Santiago, Spain |
|  |  [QuantumCLEF - Quantum Computing at CLEF](https://doi.org/10.1007/978-3-031-56069-9_66) |  | 0 | Over the last few years, Quantum Computing (QC) has captured the attention of numerous researchers pertaining to different fields since, due to technological advancements, QC resources have become more available and also applicable in solving practical problems. In the current landscape, Information Retrieval (IR) and Recommender Systems (RS) need to perform computationally intensive operations on massive and heterogeneous datasets. Therefore, it could be possible to use QC and especially Quantum Annealing (QA) technologies to boost systems' performance both in terms of efficiency and effectiveness. The objective of this work is to present the first edition of the QuantumCLEF lab, which is composed of two tasks that aim at: - evaluating QA approaches compared to their traditional counterpart; - identifying new problem formulations to discover novel methods that leverage the capabilities of QA for improved solutions; - establishing collaborations among researchers from different fields to harness their knowledge and skills to solve the considered challenges and promote the usage of QA. This lab will employ the QC resources provided by CINECA, one of the most important computing centers worldwide. We also describe the design of our infrastructure which uses Docker and Kubernetes to ensure scalability, fault tolerance and replicability. | Andrea Pasin, Maurizio Ferrari Dacrema, Paolo Cremonesi, Nicola Ferro | Politecn Milan, Milan, Italy; Univ Padua, Padua, Italy |
|  |  [EXIST 2024: sEXism Identification in Social neTworks and Memes](https://doi.org/10.1007/978-3-031-56069-9_68) |  | 0 | The paper describes the EXIST 2024 lab on Sexism identification in social networks, that is expected to take place at the CLEF 2024 conference and represents the fourth edition of the EXIST challenge. The lab comprises five tasks in two languages, English and Spanish, with the initial three tasks building upon those from EXIST 2023 (sexism identification in tweets, source intention detection in tweets, and sexism categorization in tweets). In this edition, two new tasks have been introduced: sexism detection in memes and sexism categorization in memes. Similar to the prior edition, this one will adopt the Learning With Disagreement paradigm. The dataset for the various tasks will provide all annotations from multiple annotators, enabling models to learn from a range of training data, which may sometimes present contradictory opinions or labels. This approach facilitates the model's ability to handle and navigate diverse perspectives. Data bias will be handled both in the sampling and in the labeling processes: seed, topic, temporal and user bias will be taken into account when gathering data; in the annotation process, bias will be reduced by involving annotators from different social and demographic backgrounds. | Laura Plaza, Jorge CarrillodeAlbornoz, Enrique Amigó, Julio Gonzalo, Roser Morante, Paolo Rosso, Damiano Spina, Berta Chulvi, Alba Maeso, Víctor Ruiz | Univ Nacl Educ Distancia UNED, Madrid 28040, Spain; RMIT Univ, Melbourne, Vic 3000, Australia; Univ Politecn Valencia UPV, Valencia 46022, Spain |
