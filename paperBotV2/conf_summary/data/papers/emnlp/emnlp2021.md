# EMNLP2021

## 会议论文列表

本会议共有 1316 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [Frontmatter](https://aclanthology.org/2021.emnlp-demo.0) |  | 0 |  | Heike Adel, Shuming Shi |  |
| 2 |  |  [MiSS: An Assistant for Multi-Style Simultaneous Translation](https://doi.org/10.18653/v1/2021.emnlp-demo.1) |  | 0 | In this paper, we present MiSS, an assistant for multi-style simultaneous translation. Our proposed translation system has five key features: highly accurate translation, simultaneous translation, translation for multiple text styles, back-translation for translation quality evaluation, and grammatical error correction. With this system, we aim to provide a complete translation experience for machine translation users. Our design goals are high translation accuracy, real-time translation,... | Zuchao Li, Kevin Parnow, Masao Utiyama, Eiichiro Sumita, Hai Zhao |  |
| 3 |  |  [Automatic Construction of Enterprise Knowledge Base](https://doi.org/10.18653/v1/2021.emnlp-demo.2) |  | 0 | In this paper, we present an automatic knowledge base construction system from large scale enterprise documents with minimal efforts of human intervention. In the design and deployment of such a knowledge mining system for enterprise, we faced several challenges including data distributional shift, performance evaluation, compliance requirements and other practical issues. We leveraged state-of-the-art deep learning models to extract information (named entities and definitions) at per document... | Junyi Chai, Yujie He, Homa Hashemi, Bing Li, Daraksha Parveen, Ranganath Kondapally, Wenjin Xu |  |
| 4 |  |  [LightTag: Text Annotation Platform](https://doi.org/10.18653/v1/2021.emnlp-demo.3) |  | 0 | Text annotation tools assume that their user’s goal is to create a labeled corpus. However,users view annotation as a necessary evil on the way to deliver business value through NLP.Thus an annotation tool should optimize for the throughput of the global NLP process, not only the productivity of individual annotators. LightTag is a text annotation tool designed and built on that principle. This paper shares our design rationale, data modeling choices, and user interface decisions then... | Tal Perry |  |
| 5 |  |  [TransIns: Document Translation with Markup Reinsertion](https://doi.org/10.18653/v1/2021.emnlp-demo.4) |  | 0 | For many use cases, it is required that MT does not just translate raw text, but complex formatted documents (e.g. websites, slides, spreadsheets) and the result of the translation should reflect the formatting. This is challenging, as markup can be nested, apply to spans contiguous in source but non-contiguous in target etc. Here we present TransIns, a system for non-plain text document translation that builds on the Okapi framework and MT models trained with Marian NMT. We develop, implement... | Jörg Steffen, Josef van Genabith |  |
| 6 |  |  [ET: A Workstation for Querying, Editing and Evaluating Annotated Corpora](https://doi.org/10.18653/v1/2021.emnlp-demo.5) |  | 0 | In this paper we explore the functionalities of ET, a suite designed to support linguistic research and natural language processing tasks using corpora annotated in the CoNLL-U format. These goals are achieved by two integrated environments – Interrogatório, an environment for querying and editing annotated corpora, and Julgamento, an environment for assessing their quality. ET is open-source, built on different Python Web technologies and has Web demonstrations available on-line. ET has been... | Elvis de Souza, Cláudia Freitas |  |
| 7 |  |  [N-LTP: An Open-source Neural Language Technology Platform for Chinese](https://doi.org/10.18653/v1/2021.emnlp-demo.6) |  | 0 | We introduce N-LTP, an open-source neural language technology platform supporting six fundamental Chinese NLP tasks: lexical analysis (Chinese word segmentation, part-of-speech tagging, and named entity recognition), syntactic parsing (dependency parsing), and semantic parsing (semantic dependency parsing and semantic role labeling). Unlike the existing state-of-the-art toolkits, such as Stanza, that adopt an independent model for each task, N-LTP adopts the multi-task framework by using a... | Wanxiang Che, Yunlong Feng, Libo Qin, Ting Liu |  |
| 8 |  |  [COMBO: State-of-the-Art Morphosyntactic Analysis](https://doi.org/10.18653/v1/2021.emnlp-demo.7) |  | 0 | We introduce COMBO – a fully neural NLP system for accurate part-of-speech tagging, morphological analysis, lemmatisation, and (enhanced) dependency parsing. It predicts categorical morphosyntactic features whilst also exposes their vector representations, extracted from hidden layers. COMBO is an easy to install Python package with automatically downloadable pre-trained models for over 40 languages. It maintains a balance between efficiency and quality. As it is an end-to-end system and its... | Mateusz Klimaszewski, Alina Wróblewska |  |
| 9 |  |  [ExcavatorCovid: Extracting Events and Relations from Text Corpora for Temporal and Causal Analysis for COVID-19](https://doi.org/10.18653/v1/2021.emnlp-demo.8) |  | 0 | Timely responses from policy makers to mitigate the impact of the COVID-19 pandemic rely on a comprehensive grasp of events, their causes, and their impacts. These events are reported at such a speed and scale as to be overwhelming. In this paper, we present ExcavatorCovid, a machine reading system that ingests open-source text documents (e.g., news and scientific publications), extracts COVID-19 related events and relations between them, and builds a Temporal and Causal Analysis Graph (TCAG).... | Bonan Min, Benjamin Rozonoyer, Haoling Qiu, Alexander Zamanian, Nianwen Xue, Jessica MacBride |  |
| 10 |  |  [KOAS: Korean Text Offensiveness Analysis System](https://doi.org/10.18653/v1/2021.emnlp-demo.9) |  | 0 | Warning: This manuscript contains a certain level of offensive expression. As communication through social media platforms has grown immensely, the increasing prevalence of offensive language online has become a critical problem. Notably in Korea, one of the countries with the highest Internet usage, automatic detection of offensive expressions has recently been brought to attention. However, morphological richness and complex syntax of Korean causes difficulties in neural model training.... | SanHee Park, KangMin Kim, Seonhee Cho, JunHyung Park, Hyuntae Park, Hyuna Kim, Seongwon Chung, SangKeun Lee |  |
| 11 |  |  [RepGraph: Visualising and Analysing Meaning Representation Graphs](https://doi.org/10.18653/v1/2021.emnlp-demo.10) |  | 0 | We present RepGraph, an open source visualisation and analysis tool for meaning representation graphs. Graph-based meaning representations provide rich semantic annotations, but visualising them clearly is more challenging than for fully lexicalized representations. Our application provides a seamless, unifying interface with which to visualise, manipulate and analyse semantically parsed graph data represented in a JSON-based serialisation format. RepGraph visualises graphs in multiple formats,... | Jaron Cohen, Roy Cohen, Edan Toledo, Jan Buys |  |
| 12 |  |  [Thermostat: A Large Collection of NLP Model Explanations and Analysis Tools](https://doi.org/10.18653/v1/2021.emnlp-demo.11) |  | 0 | In the language domain, as in other domains, neural explainability takes an ever more important role, with feature attribution methods on the forefront. Many such methods require considerable computational resources and expert knowledge about implementation details and parameter choices. To facilitate research, we present Thermostat which consists of a large collection of model explanations and accompanying analysis tools. Thermostat allows easy access to over 200k explanations for the... | Nils Feldhus, Robert Schwarzenberg, Sebastian Möller |  |
| 13 |  |  [LMdiff: A Visual Diff Tool to Compare Language Models](https://doi.org/10.18653/v1/2021.emnlp-demo.12) |  | 0 | While different language models are ubiquitous in NLP, it is hard to contrast their outputs and identify which contexts one can handle better than the other. To address this question, we introduce LMdiff, a tool that visually compares probability distributions of two models that differ, e.g., through finetuning, distillation, or simply training with different parameter sizes. LMdiff allows the generation of hypotheses about model behavior by investigating text instances token by token and... | Hendrik Strobelt, Benjamin Hoover, Arvind Satyanarayan, Sebastian Gehrmann |  |
| 14 |  |  [Semantic Context Path Labeling for Semantic Exploration of User Reviews](https://doi.org/10.18653/v1/2021.emnlp-demo.13) |  | 0 | In this paper we present a prototype demonstrator showcasing a novel method to perform semantic exploration of user reviews. The system enables effective navigation in a rich contextual semantic schema with a large number of structured classes indicating relevant information. In order to identify instances of the structured classes in the reviews, we defined a new Information Extraction task called Semantic Context Path (SCP) labeling, which simultaneously assigns types and semantic roles to... | Salah AïtMokhtar, Caroline Brun, Yves Hoppenot, Ágnes Sándor |  |
| 15 |  |  [Beyond Accuracy: A Consolidated Tool for Visual Question Answering Benchmarking](https://doi.org/10.18653/v1/2021.emnlp-demo.14) |  | 0 | On the way towards general Visual Question Answering (VQA) systems that are able to answer arbitrary questions, the need arises for evaluation beyond single-metric leaderboards for specific datasets. To this end, we propose a browser-based benchmarking tool for researchers and challenge organizers, with an API for easy integration of new models and datasets to keep up with the fast-changing landscape of VQA. Our tool helps test generalization capabilities of models across multiple datasets,... | Dirk Väth, Pascal Tilli, Ngoc Thang Vu |  |
| 16 |  |  [Athena 2.0: Contextualized Dialogue Management for an Alexa Prize SocialBot](https://doi.org/10.18653/v1/2021.emnlp-demo.15) |  | 0 | Athena 2.0 is an Alexa Prize SocialBot that has been a finalist in the last two Alexa Prize Grand Challenges. One reason for Athena’s success is its novel dialogue management strategy, which allows it to dynamically construct dialogues and responses from component modules, leading to novel conversations with every interaction. Here we describe Athena’s system design and performance in the Alexa Prize during the 20/21 competition. A live demo of Athena as well as video recordings will provoke... | Marilyn A. Walker, Vrindavan Harrison, Juraj Juraska, Lena Reed, Kevin Bowden, Wen Cui, Omkar Patil, Adwait Ratnaparkhi |  |
| 17 |  |  [SPRING Goes Online: End-to-End AMR Parsing and Generation](https://doi.org/10.18653/v1/2021.emnlp-demo.16) |  | 0 | In this paper we present SPRING Online Services, a Web interface and RESTful APIs for our state-of-the-art AMR parsing and generation system, SPRING (Symmetric PaRsIng aNd Generation). The Web interface has been developed to be easily used by the Natural Language Processing community, as well as by the general public. It provides, among other things, a highly interactive visualization platform and a feedback mechanism to obtain user suggestions for further improvements of the system’s output.... | Rexhina Blloshmi, Michele Bevilacqua, Edoardo Fabiano, Valentina Caruso, Roberto Navigli |  |
| 18 |  |  [fairseq S\^2: A Scalable and Integrable Speech Synthesis Toolkit](https://doi.org/10.18653/v1/2021.emnlp-demo.17) |  | 0 | This paper presents fairseq Sˆ2, a fairseq extension for speech synthesis. We implement a number of autoregressive (AR) and non-AR text-to-speech models, and their multi-speaker variants. To enable training speech synthesis models with less curated data, a number of preprocessing tools are built and their importance is shown empirically. To facilitate faster iteration of development and analysis, a suite of automatic metrics is included. Apart from the features added specifically for this... | Changhan Wang, WeiNing Hsu, Yossi Adi, Adam Polyak, Ann Lee, PengJen Chen, Jiatao Gu, Juan Pino |  |
| 19 |  |  [Press Freedom Monitor: Detection of Reported Press and Media Freedom Violations in Twitter and News Articles](https://doi.org/10.18653/v1/2021.emnlp-demo.18) |  | 0 | Freedom of the press and media is of vital importance for democratically organised states and open societies. We introduce the Press Freedom Monitor, a tool that aims to detect reported press and media freedom violations in news articles and tweets. It is used by press and media freedom organisations to support their daily monitoring and to trigger rapid response actions. The Press Freedom Monitor enables the monitoring experts to get a fast overview over recently reported incidents and it has... | Tariq Yousef, Antje Schlaf, Janos Borst, Andreas Niekler, Gerhard Heyer |  |
| 20 |  |  [UMR-Writer: A Web Application for Annotating Uniform Meaning Representations](https://doi.org/10.18653/v1/2021.emnlp-demo.19) |  | 0 | We present UMR-Writer, a web-based application for annotating Uniform Meaning Representations (UMR), a graph-based, cross-linguistically applicable semantic representation developed recently to support the development of interpretable natural language applications that require deep semantic analysis of texts. We present the functionalities of UMR-Writer and discuss the challenges in developing such a tool and how they are addressed. | Jin Zhao, Nianwen Xue, Jens E. L. Van Gysel, Jinho D. Choi |  |
| 21 |  |  [TranslateLocally: Blazing-fast translation running on the local CPU](https://doi.org/10.18653/v1/2021.emnlp-demo.20) |  | 0 | Every day, millions of people sacrifice their privacy and browsing habits in exchange for online machine translation. Companies and governments with confidentiality requirements often ban online translation or pay a premium to disable logging. To bring control back to the end user and demonstrate speed, we developed translateLocally. Running locally on a desktop or laptop CPU, translateLocally delivers cloud-like translation speed and quality even on 10 year old hardware. The open-source... | Nikolay Bogoychev, Jelmer van der Linde, Kenneth Heafield |  |
| 22 |  |  [Datasets: A Community Library for Natural Language Processing](https://doi.org/10.18653/v1/2021.emnlp-demo.21) |  | 0 | The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora. The design of the library incorporates a distributed,... | Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Sasko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillanMajor, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, François Lagunas, Alexander M. Rush, Thomas Wolf |  |
| 23 |  |  [Summary Explorer: Visualizing the State of the Art in Text Summarization](https://doi.org/10.18653/v1/2021.emnlp-demo.22) |  | 0 | This paper introduces Summary Explorer, a new tool to support the manual inspection of text summarization systems by compiling the outputs of 55 state-of-the-art single document summarization approaches on three benchmark datasets, and visually exploring them during a qualitative assessment. The underlying design of the tool considers three well-known summary quality criteria (coverage, faithfulness, and position bias), encapsulated in a guided assessment based on tailored visualizations. The... | Shahbaz Syed, Tariq Yousef, Khalid Al Khatib, Stefan Jänicke, Martin Potthast |  |
| 24 |  |  [MeetDot: Videoconferencing with Live Translation Captions](https://doi.org/10.18653/v1/2021.emnlp-demo.23) |  | 0 | We present MeetDot, a videoconferencing system with live translation captions overlaid on screen. The system aims to facilitate conversation between people who speak different languages, thereby reducing communication barriers between multilingual participants. Currently, our system supports speech and captions in 4 languages and combines automatic speech recognition (ASR) and machine translation (MT) in a cascade. We use the re-translation strategy to translate the streamed speech, resulting... | Arkady Arkhangorodsky, Christopher Chu, Scot Fang, Yiqi Huang, Denglin Jiang, Ajay Nagesh, Boliang Zhang, Kevin Knight |  |
| 25 |  |  [Box Embeddings: An open-source library for representation learning using geometric structures](https://doi.org/10.18653/v1/2021.emnlp-demo.24) |  | 0 | A fundamental component to the success of modern representation learning is the ease of performing various vector operations. Recently, objects with more geometric structure (eg. distributions, complex or hyperbolic vectors, or regions such as cones, disks, or boxes) have been explored for their alternative inductive biases and additional representational capacity. In this work, we introduce Box Embeddings, a Python library that enables researchers to easily apply and extend probabilistic box... | Tejas Chheda, Purujit Goyal, Trang Tran, Dhruvesh Patel, Michael Boratko, Shib Sankar Dasgupta, Andrew McCallum |  |
| 26 |  |  [LexiClean: An annotation tool for rapid multi-task lexical normalisation](https://doi.org/10.18653/v1/2021.emnlp-demo.25) |  | 0 | NLP systems are often challenged by difficulties arising from noisy, non-standard, and domain specific corpora. The task of lexical normalisation aims to standardise such corpora, but currently lacks suitable tools to acquire high-quality annotated data to support deep learning based approaches. In this paper, we present LexiClean, the first open-source web-based annotation tool for multi-task lexical normalisation. LexiClean’s main contribution is support for simultaneous in situ token-level... | Tyler Bikaun, Tim French, Melinda Hodkiewicz, Michael Stewart, Wei Liu |  |
| 27 |  |  [T3-Vis: visual analytic for Training and fine-Tuning Transformers in NLP](https://doi.org/10.18653/v1/2021.emnlp-demo.26) |  | 0 | Transformers are the dominant architecture in NLP, but their training and fine-tuning is still very challenging. In this paper, we present the design and implementation of a visual analytic framework for assisting researchers in such process, by providing them with valuable insights about the model’s intrinsic properties and behaviours. Our framework offers an intuitive overview that allows the user to explore different facets of the model (e.g., hidden states, attention) through interactive... | Raymond Li, Wen Xiao, Lanjun Wang, Hyeju Jang, Giuseppe Carenini |  |
| 28 |  |  [DomiKnowS: A Library for Integration of Symbolic Domain Knowledge in Deep Learning](https://doi.org/10.18653/v1/2021.emnlp-demo.27) |  | 0 | We demonstrate a library for the integration of domain knowledge in deep learning architectures. Using this library, the structure of the data is expressed symbolically via graph declarations and the logical constraints over outputs or latent variables can be seamlessly added to the deep models. The domain knowledge can be defined explicitly, which improves the explainability of the models in addition to their performance and generalizability in the low-data regime. Several approaches for such... | Hossein Rajaby Faghihi, Quan Guo, Andrzej Uszok, Aliakbar Nafar, Parisa Kordjamshidi |  |
| 29 |  |  [OpenFraming: Open-sourced Tool for Computational Framing Analysis of Multilingual Data](https://doi.org/10.18653/v1/2021.emnlp-demo.28) |  | 0 | When journalists cover a news story, they can cover the story from multiple angles or perspectives. These perspectives are called “frames,” and usage of one frame or another may influence public perception and opinion of the issue at hand. We develop a web-based system for analyzing frames in multilingual text documents. We propose and guide users through a five-step end-to-end computational framing analysis framework grounded in media framing theory in communication research. Users can use the... | Vibhu Bhatia, Vidya Prasad Akavoor, Sejin Paik, Lei Guo, Mona Jalal, Alyssa Hasegawa Smith, David Assefa Tofu, Edward Edberg Halim, Yimeng Sun, Margrit Betke, Prakash Ishwar, Derry Tanti Wijaya |  |
| 30 |  |  [IrEne-viz: Visualizing Energy Consumption of Transformer Models](https://doi.org/10.18653/v1/2021.emnlp-demo.29) |  | 0 | IrEne is an energy prediction system that accurately predicts the interpretable inference energy consumption of a wide range of Transformer-based NLP models. We present the IrEne-viz tool, an online platform for visualizing and exploring energy consumption of various Transformer-based models easily. Additionally, we release a public API that can be used to access granular information about energy consumption of transformer models and their components. The live demo is available at... | Yash Kumar Lal, Reetu Singh, Harsh Trivedi, Qingqing Cao, Aruna Balasubramanian, Niranjan Balasubramanian |  |
| 31 |  |  [Open-Domain Question-Answering for COVID-19 and Other Emergent Domains](https://doi.org/10.18653/v1/2021.emnlp-demo.30) |  | 0 | Since late 2019, COVID-19 has quickly emerged as the newest biomedical domain, resulting in a surge of new information. As with other emergent domains, the discussion surrounding the topic has been rapidly changing, leading to the spread of misinformation. This has created the need for a public space for users to ask questions and receive credible, scientific answers. To fulfill this need, we turn to the task of open-domain question-answering, which we can use to efficiently find answers to... | Sharon Levy, Kevin Mo, Wenhan Xiong, William Yang Wang |  |
| 32 |  |  [Project Debater APIs: Decomposing the AI Grand Challenge](https://doi.org/10.18653/v1/2021.emnlp-demo.31) |  | 0 | Project Debater was revealed in 2019 as the first AI system that can debate human experts on complex topics. Engaging in a live debate requires a diverse set of skills, and Project Debater has been developed accordingly as a collection of components, each designed to perform a specific subtask. Project Debater APIs provide access to many of these capabilities, as well as to more recently developed ones. This diverse set of web services, publicly available for academic use, includes core NLP... | Roy BarHaim, Yoav Kantor, Elad Venezian, Yoav Katz, Noam Slonim |  |
| 33 |  |  [CroAno : A Crowd Annotation Platform for Improving Label Consistency of Chinese NER Dataset](https://doi.org/10.18653/v1/2021.emnlp-demo.32) |  | 0 | In this paper, we introduce CroAno, a web-based crowd annotation platform for the Chinese named entity recognition (NER). Besides some basic features for crowd annotation like fast tagging and data management, CroAno provides a systematic solution for improving label consistency of Chinese NER dataset. 1) Disagreement Adjudicator: CroAno uses a multi-dimensional highlight mode to visualize instance-level inconsistent entities and makes the revision process user-friendly. 2) Inconsistency... | Baoli Zhang, Zhucong Li, Zhen Gan, Yubo Chen, Jing Wan, Kang Liu, Jun Zhao, Shengping Liu, Yafei Shi |  |
| 34 |  |  [iFacetSum: Coreference-based Interactive Faceted Summarization for Multi-Document Exploration](https://doi.org/10.18653/v1/2021.emnlp-demo.33) |  | 0 | We introduce iFᴀᴄᴇᴛSᴜᴍ, a web application for exploring topical document collections. iFᴀᴄᴇᴛSᴜᴍ integrates interactive summarization together with faceted search, by providing a novel faceted navigation scheme that yields abstractive summaries for the user’s selections. This approach offers both a comprehensive overview as well as particular details regard-ing subtopics of choice. The facets are automatically produced based on cross-document coreference pipelines, rendering generic concepts,... | Eran Hirsch, Alon Eirew, Ori Shapira, Avi Caciularu, Arie Cattan, Ori Ernst, Ramakanth Pasunuru, Hadar Ronen, Mohit Bansal, Ido Dagan |  |
| 35 |  |  [AMuSE-WSD: An All-in-one Multilingual System for Easy Word Sense Disambiguation](https://doi.org/10.18653/v1/2021.emnlp-demo.34) |  | 0 | Over the past few years, Word Sense Disambiguation (WSD) has received renewed interest: recently proposed systems have shown the remarkable effectiveness of deep learning techniques in this task, especially when aided by modern pretrained language models. Unfortunately, such systems are still not available as ready-to-use end-to-end packages, making it difficult for researchers to take advantage of their performance. The only alternative for a user interested in applying WSD to downstream tasks... | Riccardo Orlando, Simone Conia, Fabrizio Brignone, Francesco Cecconi, Roberto Navigli |  |
| 36 |  |  [SeqAttack: On Adversarial Attacks for Named Entity Recognition](https://doi.org/10.18653/v1/2021.emnlp-demo.35) |  | 0 | Named Entity Recognition is a fundamental task in information extraction and is an essential element for various Natural Language Processing pipelines. Adversarial attacks have been shown to greatly affect the performance of text classification systems but knowledge about their effectiveness against named entity recognition models is limited. This paper investigates the effectiveness and portability of adversarial attacks from text classification to named entity recognition and the ability of... | Walter Simoncini, Gerasimos Spanakis |  |
| 37 |  |  [InVeRo-XL: Making Cross-Lingual Semantic Role Labeling Accessible with Intelligible Verbs and Roles](https://doi.org/10.18653/v1/2021.emnlp-demo.36) |  | 0 | Notwithstanding the growing interest in cross-lingual techniques for Natural Language Processing, there has been a surprisingly small number of efforts aimed at the development of easy-to-use tools for cross-lingual Semantic Role Labeling. In this paper, we fill this gap and present InVeRo-XL, an off-the-shelf state-of-the-art system capable of annotating text with predicate sense and semantic role labels from 7 predicate-argument structure inventories in more than 40 languages. We hope that... | Simone Conia, Riccardo Orlando, Fabrizio Brignone, Francesco Cecconi, Roberto Navigli |  |
| 38 |  |  [SummerTime: Text Summarization Toolkit for Non-experts](https://doi.org/10.18653/v1/2021.emnlp-demo.37) |  | 0 | Recent advances in summarization provide models that can generate summaries of higher quality. Such models now exist for a number of summarization tasks, including query-based summarization, dialogue summarization, and multi-document summarization. While such models and tasks are rapidly growing in the research field, it has also become challenging for non-experts to keep track of them. To make summarization methods more accessible to a wider audience, we develop SummerTime by rethinking the... | Ansong Ni, Zhangir Azerbayev, Mutethia Mutuma, Troy Feng, Yusen Zhang, Tao Yu, Ahmed Hassan Awadallah, Dragomir R. Radev |  |
| 39 |  |  [Chandler: An Explainable Sarcastic Response Generator](https://doi.org/10.18653/v1/2021.emnlp-demo.38) |  | 0 | We introduce Chandler, a system that generates sarcastic responses to a given utterance. Previous sarcasm generators assume the intended meaning that sarcasm conceals is the opposite of the literal meaning. We argue that this traditional theory of sarcasm provides a grounding that is neither necessary, nor sufficient, for sarcasm to occur. Instead, we ground our generation process on a formal theory that specifies conditions that unambiguously differentiate sarcasm from non-sarcasm.... | Silviu Oprea, Steven R. Wilson, Walid Magdy |  |
| 40 |  |  [TabPert : An Effective Platform for Tabular Perturbation](https://doi.org/10.18653/v1/2021.emnlp-demo.39) |  | 0 | To grasp the true reasoning ability, the Natural Language Inference model should be evaluated on counterfactual data. TabPert facilitates this by generation of such counterfactual data for assessing model tabular reasoning issues. TabPert allows the user to update a table, change the hypothesis, change the labels, and highlight rows that are important for hypothesis classification. TabPert also details the technique used to automatically produce the table, as well as the strategies employed to... | Nupur Jain, Vivek Gupta, Anshul Rai, Gaurav Kumar |  |
| 41 |  |  [DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature](https://doi.org/10.18653/v1/2021.emnlp-demo.40) |  | 0 | In this work, we present to the NLP community, and to the wider research community as a whole, an application for the diachronic analysis of research corpora. We open source an easy-to-use tool coined DRIFT, which allows researchers to track research trends and development over the years. The analysis methods are collated from well-cited research works, with a few of our own methods added for good measure. Succinctly put, some of the analysis methods are: keyword extraction, word clouds,... | Abheesht Sharma, Gunjan Chhablani, Harshit Pandey, Rajaswa Patil |  |
| 42 |  |  [FAST: Fast Annotation tool for SmarT devices](https://doi.org/10.18653/v1/2021.emnlp-demo.41) |  | 0 | Working with a wide range of annotators with the same attributes is crucial, as in real-world applications. Although such application cases often use crowd-sourcing mechanisms to gather a variety of annotators, most real-world users use mobile devices. In this paper, we propose “FAST,” an annotation tool for application tasks that focuses on the user experience of mobile devices, which has not yet been focused on thus far. We designed FAST as a web application for use on any device with a... | Shunyo Kawamoto, Yu Sawai, Kohei Wakimoto, Peinan Zhang |  |
| 43 |  |  [deepQuest-py: Large and Distilled Models for Quality Estimation](https://doi.org/10.18653/v1/2021.emnlp-demo.42) |  | 0 | We introduce deepQuest-py, a framework for training and evaluation of large and light-weight models for Quality Estimation (QE). deepQuest-py provides access to (1) state-of-the-art models based on pre-trained Transformers for sentence-level and word-level QE; (2) light-weight and efficient sentence-level models implemented via knowledge distillation; and (3) a web interface for testing models and visualising their predictions. deepQuest-py is available at... | Fernando AlvaManchego, Abiola Obamuyide, Amit Gajbhiye, Frédéric Blain, Marina Fomicheva, Lucia Specia |  |
| 44 |  |  [Frontmatter](https://aclanthology.org/2021.findings-emnlp.0) |  | 0 |  |  |  |
| 45 |  |  [K-PLUG: Knowledge-injected Pre-trained Language Model for Natural Language Understanding and Generation in E-Commerce](https://doi.org/10.18653/v1/2021.findings-emnlp.1) |  | 0 | Existing pre-trained language models (PLMs) have demonstrated the effectiveness of self-supervised learning for a broad range of natural language processing (NLP) tasks. However, most of them are not explicitly aware of domain-specific knowledge, which is essential for downstream tasks in many domains, such as tasks in e-commerce scenarios. In this paper, we propose K-PLUG, a knowledge-injected pre-trained language model based on the encoder-decoder transformer that can be transferred to both... | Song Xu, Haoran Li, Peng Yuan, Yujia Wang, Youzheng Wu, Xiaodong He, Ying Liu, Bowen Zhou |  |
| 46 |  |  [Extracting Topics with Simultaneous Word Co-occurrence and Semantic Correlation Graphs: Neural Topic Modeling for Short Texts](https://doi.org/10.18653/v1/2021.findings-emnlp.2) |  | 0 | Short text nowadays has become a more fashionable form of text data, e.g., Twitter posts, news titles, and product reviews. Extracting semantic topics from short texts plays a significant role in a wide spectrum of NLP applications, and neural topic modeling is now a major tool to achieve it. Motivated by learning more coherent and semantic topics, in this paper we develop a novel neural topic model named Dual Word Graph Topic Model (DWGTM), which extracts topics from simultaneous word... | Yiming Wang, Ximing Li, Xiaotang Zhou, Jihong Ouyang |  |
| 47 |  |  [Self-supervised Contrastive Cross-Modality Representation Learning for Spoken Question Answering](https://doi.org/10.18653/v1/2021.findings-emnlp.3) |  | 0 | Spoken question answering (SQA) requires fine-grained understanding of both spoken documents and questions for the optimal answer prediction. In this paper, we propose novel training schemes for spoken question answering with a self-supervised training stage and a contrastive representation learning stage. In the self-supervised stage, we propose three auxiliary self-supervised tasks, including utterance restoration, utterance insertion, and question discrimination, and jointly train the model... | Chenyu You, Nuo Chen, Yuexian Zou |  |
| 48 |  |  [Language Clustering for Multilingual Named Entity Recognition](https://doi.org/10.18653/v1/2021.findings-emnlp.4) |  | 0 | Recent work in multilingual natural language processing has shown progress in various tasks such as natural language inference and joint multilingual translation. Despite success in learning across many languages, challenges arise where multilingual training regimes often boost performance on some languages at the expense of others. For multilingual named entity recognition (NER) we propose a simple technique that groups similar languages together by using embeddings from a pre-trained masked... | Kyle Shaffer |  |
| 49 |  |  [Neural News Recommendation with Collaborative News Encoding and Structural User Encoding](https://doi.org/10.18653/v1/2021.findings-emnlp.5) |  | 0 | Automatic news recommendation has gained much attention from the academic community and industry. Recent studies reveal that the key to this task lies within the effective representation learning of both news and users. Existing works typically encode news title and content separately while neglecting their semantic interaction, which is inadequate for news text comprehension. Besides, previous models encode user browsing history without leveraging the structural correlation of user browsed... | Zhiming Mao, Xingshan Zeng, KamFai Wong |  |
| 50 |  |  [Self-Teaching Machines to Read and Comprehend with Large-Scale Multi-Subject Question-Answering Data](https://doi.org/10.18653/v1/2021.findings-emnlp.6) |  | 0 | Despite considerable progress, most machine reading comprehension (MRC) tasks still lack sufficient training data to fully exploit powerful deep neural network models with millions of parameters, and it is laborious, expensive, and time-consuming to create large-scale, high-quality MRC data through crowdsourcing. This paper focuses on generating more training data for MRC tasks by leveraging existing question-answering (QA) data. We first collect a large-scale multi-subject multiple-choice QA... | Dian Yu, Kai Sun, Dong Yu, Claire Cardie |  |
| 51 |  |  [A Web Scale Entity Extraction System](https://doi.org/10.18653/v1/2021.findings-emnlp.7) |  | 0 | Understanding the semantic meaning of content on the web through the lens of entities and concepts has many practical advantages. However, when building large-scale entity extraction systems, practitioners are facing unique challenges involving finding the best ways to leverage the scale and variety of data available on internet platforms. We present learnings from our efforts in building an entity extraction system for multiple document types at large scale using multi-modal Transformers. We... | Xuanting Cai, Quanbin Ma, Jianyu Liu, Pan Li, Qi Zeng, Zhengkan Yang, Pushkar Tripathi |  |
| 52 |  |  [Joint Multimedia Event Extraction from Video and Article](https://doi.org/10.18653/v1/2021.findings-emnlp.8) |  | 0 | Visual and textual modalities contribute complementary information about events described in multimedia documents. Videos contain rich dynamics and detailed unfoldings of events, while text describes more high-level and abstract concepts. However, existing event extraction methods either do not handle video or solely target video while ignoring other modalities. In contrast, we propose the first approach to jointly extract events from both video and text articles. We introduce the new task of... | Brian Chen, Xudong Lin, Christopher Thomas, Manling Li, Shoya Yoshida, Lovish Chum, Heng Ji, ShihFu Chang |  |
| 53 |  |  [Fine-grained Semantic Alignment Network for Weakly Supervised Temporal Language Grounding](https://doi.org/10.18653/v1/2021.findings-emnlp.9) |  | 0 | Temporal language grounding (TLG) aims to localize a video segment in an untrimmed video based on a natural language description. To alleviate the expensive cost of manual annotations for temporal boundary labels,we are dedicated to the weakly supervised setting, where only video-level descriptions are provided for training. Most of the existing weakly supervised methods generate a candidate segment set and learn cross-modal alignment through a MIL-based framework. However, the temporal... | Yuechen Wang, Wengang Zhou, Houqiang Li |  |
| 54 |  |  [Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation](https://doi.org/10.18653/v1/2021.findings-emnlp.10) |  | 0 | Despite significant progress has been achieved in text summarization, factual inconsistency in generated summaries still severely limits its practical applications. Among the key factors to ensure factual consistency, a reliable automatic evaluation metric is the first and the most crucial one. However, existing metrics either neglect the intrinsic cause of the factual inconsistency or rely on auxiliary tasks, leading to an unsatisfied correlation with human judgments or increasing the... | Yuexiang Xie, Fei Sun, Yang Deng, Yaliang Li, Bolin Ding |  |
| 55 |  |  [Cross-Modal Retrieval Augmentation for Multi-Modal Classification](https://doi.org/10.18653/v1/2021.findings-emnlp.11) |  | 0 | Recent advances in using retrieval components over external knowledge sources have shown impressive results for a variety of downstream tasks in natural language processing. Here, we explore the use of unstructured external knowledge sources of images and their corresponding captions for improving visual question answering (VQA). First, we train a novel alignment model for embedding images and captions in the same space, which achieves substantial improvement in performance on image-caption... | Shir Gur, Natalia Neverova, Christopher Stauffer, SerNam Lim, Douwe Kiela, Austin Reiter |  |
| 56 |  |  [HiTRANS: A Hierarchical Transformer Network for Nested Named Entity Recognition](https://doi.org/10.18653/v1/2021.findings-emnlp.12) |  | 0 | Nested Named Entity Recognition (NNER) has been extensively studied, aiming to identify all nested entities from potential spans (i.e., one or more continuous tokens). However, recent studies for NNER either focus on tedious tagging schemas or utilize complex structures, which fail to learn effective span representations from the input sentence with highly nested entities. Intuitively, explicit span representations will contribute to NNER due to the rich context information they contain. In... | Zhiwei Yang, Jing Ma, Hechang Chen, Yunke Zhang, Yi Chang |  |
| 57 |  |  [Improving Embedding-based Large-scale Retrieval via Label Enhancement](https://doi.org/10.18653/v1/2021.findings-emnlp.13) |  | 0 | Current embedding-based large-scale retrieval models are trained with 0-1 hard label that indicates whether a query is relevant to a document, ignoring rich information of the relevance degree. This paper proposes to improve embedding-based retrieval from the perspective of better characterizing the query-document relevance degree by introducing label enhancement (LE) for the first time. To generate label distribution in the retrieval scenario, we design a novel and effective supervised LE... | Peiyang Liu, Xi Wang, Sen Wang, Wei Ye, Xiangyu Xi, Shikun Zhang |  |
| 58 |  |  [Improving Privacy Guarantee and Efficiency of Latent Dirichlet Allocation Model Training Under Differential Privacy](https://doi.org/10.18653/v1/2021.findings-emnlp.14) |  | 0 | Latent Dirichlet allocation (LDA), a widely used topic model, is often employed as a fundamental tool for text analysis in various applications. However, the training process of the LDA model typically requires massive text corpus data. On one hand, such massive data may expose private information in the training data, thereby incurring significant privacy concerns. On the other hand, the efficiency of the LDA model training may be impacted, since LDA training often needs to handle these... | Tao Huang, Hong Chen |  |
| 59 |  |  [Generating Mammography Reports from Multi-view Mammograms with BERT](https://doi.org/10.18653/v1/2021.findings-emnlp.15) |  | 0 | Writing mammography reports can be error-prone and time-consuming for radiologists. In this paper we propose a method to generate mammography reports given four images, corresponding to the four views used in screening mammography. To the best of our knowledge our work represents the first attempt to generate the mammography report using deep-learning. We propose an encoder-decoder model that includes an EfficientNet-based encoder and a Transformer-based decoder. We demonstrate that the... | Alexander Yalunin, Elena Sokolova, Ilya Burenko, Alexander Ponomarchuk, Olga Puchkova, Dmitriy Umerenkov |  |
| 60 |  |  [Euphemistic Phrase Detection by Masked Language Model](https://doi.org/10.18653/v1/2021.findings-emnlp.16) |  | 0 | It is a well-known approach for fringe groups and organizations to use euphemisms—ordinary-sounding and innocent-looking words with a secret meaning—to conceal what they are discussing. For instance, drug dealers often use “pot” for marijuana and “avocado” for heroin. From a social media content moderation perspective, though recent advances in NLP have enabled the automatic detection of such single-word euphemisms, no existing work is capable of automatically detecting multi-word euphemisms,... | Wanzheng Zhu, Suma Bhat |  |
| 61 |  |  [Decomposing Complex Questions Makes Multi-Hop QA Easier and More Interpretable](https://doi.org/10.18653/v1/2021.findings-emnlp.17) |  | 0 | Multi-hop QA requires the machine to answer complex questions through finding multiple clues and reasoning, and provide explanatory evidence to demonstrate the machine’s reasoning process. We propose Relation Extractor-Reader and Comparator (RERC), a three-stage framework based on complex question decomposition. The Relation Extractor decomposes the complex question, and then the Reader answers the sub-questions in turn, and finally the Comparator performs numerical comparison and summarizes... | Ruiliu Fu, Han Wang, Xuejun Zhang, Jun Zhou, Yonghong Yan |  |
| 62 |  |  [Segmenting Natural Language Sentences via Lexical Unit Analysis](https://doi.org/10.18653/v1/2021.findings-emnlp.18) |  | 0 | The span-based model enjoys great popularity in recent works of sequence segmentation. However, each of these methods suffers from its own defects, such as invalid predictions. In this work, we introduce a unified span-based model, lexical unit analysis (LUA), that addresses all these matters. Segmenting a lexical unit sequence involves two steps. Firstly, we embed every span by using the representations from a pretraining language model. Secondly, we define a score for every segmentation... | Yangming Li, Lemao Liu, Shuming Shi |  |
| 63 |  |  [Dense Hierarchical Retrieval for Open-domain Question Answering](https://doi.org/10.18653/v1/2021.findings-emnlp.19) |  | 0 | Dense neural text retrieval has achieved promising results on open-domain Question Answering (QA), where latent representations of questions and passages are exploited for maximum inner product search in the retrieval process. However, current dense retrievers require splitting documents into short passages that usually contain local, partial and sometimes biased context, and highly depend on the splitting process. As a consequence, it may yield inaccurate and misleading hidden representations,... | Ye Liu, Kazuma Hashimoto, Yingbo Zhou, Semih Yavuz, Caiming Xiong, Philip S. Yu |  |
| 64 |  |  [Visually Grounded Concept Composition](https://doi.org/10.18653/v1/2021.findings-emnlp.20) |  | 0 | We investigate ways to compose complex concepts in texts from primitive ones while grounding them in images. We propose Concept and Relation Graph (CRG), which builds on top of constituency analysis and consists of recursively combined concepts with predicate functions. Meanwhile, we propose a concept composition neural network called Composer to leverage the CRG for visually grounded concept learning. Specifically, we learn the grounding of both primitive and all composed concepts by aligning... | Bowen Zhang, Hexiang Hu, Linlu Qiu, Peter Shaw, Fei Sha |  |
| 65 |  |  [Compositional Networks Enable Systematic Generalization for Grounded Language Understanding](https://doi.org/10.18653/v1/2021.findings-emnlp.21) |  | 0 | Humans are remarkably flexible when understanding new sentences that include combinations of concepts they have never encountered before. Recent work has shown that while deep networks can mimic some human language abilities when presented with novel sentences, systematic variation uncovers the limitations in the language-understanding abilities of networks. We demonstrate that these limitations can be overcome by addressing the generalization challenges in the gSCAN dataset, which explicitly... | YenLing Kuo, Boris Katz, Andrei Barbu |  |
| 66 |  |  [An Unsupervised Method for Building Sentence Simplification Corpora in Multiple Languages](https://doi.org/10.18653/v1/2021.findings-emnlp.22) |  | 0 | The availability of parallel sentence simplification (SS) is scarce for neural SS modelings. We propose an unsupervised method to build SS corpora from large-scale bilingual translation corpora, alleviating the need for SS supervised corpora. Our method is motivated by the following two findings: neural machine translation model usually tends to generate more high-frequency tokens and the difference of text complexity levels exists between the source and target language of a translation corpus.... | Xinyu Lu, Jipeng Qiang, Yun Li, Yunhao Yuan, Yi Zhu |  |
| 67 |  |  [WhiteningBERT: An Easy Unsupervised Sentence Embedding Approach](https://doi.org/10.18653/v1/2021.findings-emnlp.23) |  | 0 | Producing the embedding of a sentence in anunsupervised way is valuable to natural language matching and retrieval problems in practice. In this work, we conduct a thorough examination of pretrained model based unsupervised sentence embeddings. We study on fourpretrained models and conduct massive experiments on seven datasets regarding sentence semantics. We have three main findings. First, averaging all tokens is better than only using [CLS] vector. Second, combining both topand bottom layers... | Junjie Huang, Duyu Tang, Wanjun Zhong, Shuai Lu, Linjun Shou, Ming Gong, Daxin Jiang, Nan Duan |  |
| 68 |  |  [TWEETSUMM - A Dialog Summarization Dataset for Customer Service](https://doi.org/10.18653/v1/2021.findings-emnlp.24) |  | 0 | In a typical customer service chat scenario, customers contact a support center to ask for help or raise complaints, and human agents try to solve the issues. In most cases, at the end of the conversation, agents are asked to write a short summary emphasizing the problem and the proposed solution, usually for the benefit of other agents that may have to deal with the same customer or issue. The goal of the present article is advancing the automation of this task. We introduce the first large... | Guy Feigenblat, R. Chulaka Gunasekara, Benjamin Sznajder, Sachindra Joshi, David Konopnicki, Ranit Aharonov |  |
| 69 |  |  [Discourse-Based Sentence Splitting](https://doi.org/10.18653/v1/2021.findings-emnlp.25) |  | 0 | Sentence splitting involves the segmentation of a sentence into two or more shorter sentences. It is a key component of sentence simplification, has been shown to help human comprehension and is a useful preprocessing step for NLP tasks such as summarisation and relation extraction. While several methods and datasets have been proposed for developing sentence splitting models, little attention has been paid to how sentence splitting interacts with discourse structure. In this work, we focus on... | Liam Cripwell, Joël Legrand, Claire Gardent |  |
| 70 |  |  [Multi-Task Dense Retrieval via Model Uncertainty Fusion for Open-Domain Question Answering](https://doi.org/10.18653/v1/2021.findings-emnlp.26) |  | 0 | Multi-task dense retrieval models can be used to retrieve documents from a common corpus (e.g., Wikipedia) for different open-domain question-answering (QA) tasks. However, Karpukhin et al. (2020) shows that jointly learning different QA tasks with one dense model is not always beneficial due to corpus inconsistency. For example, SQuAD only focuses on a small set of Wikipedia articles while datasets like NQ and Trivia cover more entries, and joint training on their union can cause performance... | Minghan Li, Ming Li, Kun Xiong, Jimmy Lin |  |
| 71 |  |  [Mining the Cause of Political Decision-Making from Social Media: A Case Study of COVID-19 Policies across the US States](https://doi.org/10.18653/v1/2021.findings-emnlp.27) |  | 0 | Mining the causes of political decision-making is an active research area in the field of political science. In the past, most studies have focused on long-term policies that are collected over several decades of time, and have primarily relied on surveys as the main source of predictors. However, the recent COVID-19 pandemic has given rise to a new political phenomenon, where political decision-making consists of frequent short-term decisions, all on the same controlled topic—the pandemic. In... | Zhijing Jin, Zeyu Peng, Tejas Vaidhya, Bernhard Schölkopf, Rada Mihalcea |  |
| 72 |  |  [Self-Attention Graph Residual Convolutional Networks for Event Detection with dependency relations](https://doi.org/10.18653/v1/2021.findings-emnlp.28) |  | 0 | Event detection (ED) task aims to classify events by identifying key event trigger words embedded in a piece of text. Previous research have proved the validity of fusing syntactic dependency relations into Graph Convolutional Networks(GCN). While existing GCN-based methods explore latent node-to-node dependency relations according to a stationary adjacency tensor, an attention-based dynamic tensor, which can pay much attention to the key node like event trigger or its neighboring nodes, has... | Anan Liu, Ning Xu, Haozhe Liu |  |
| 73 |  |  [Mixup Decoding for Diverse Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.29) |  | 0 | Diverse machine translation aims at generating various target language translations for a given source language sentence. To leverage the linear relationship in the sentence latent space introduced by the mixup training, we propose a novel method, MixDiversity, to generate different translations for the input sentence by linearly interpolating it with different sentence pairs sampled from the training corpus during decoding. To further improve the faithfulness and diversity of the translations,... | Jicheng Li, Pengzhi Gao, Xuanfu Wu, Yang Feng, Zhongjun He, Hua Wu, Haifeng Wang |  |
| 74 |  |  [An Alignment-Agnostic Model for Chinese Text Error Correction](https://doi.org/10.18653/v1/2021.findings-emnlp.30) |  | 0 | This paper investigates how to correct Chinese text errors with types of mistaken, missing and redundant characters, which are common for Chinese native speakers. Most existing models based on detect-correct framework can correct mistaken characters, but cannot handle missing or redundant characters due to inconsistency between model inputs and outputs. Although Seq2Seq-based or sequence tagging methods provide solutions to the three error types and achieved relatively good results in English... | Liying Zheng, Yue Deng, Weishun Song, Liang Xu, Jing Xiao |  |
| 75 |  |  [Reasoning Visual Dialog with Sparse Graph Learning and Knowledge Transfer](https://doi.org/10.18653/v1/2021.findings-emnlp.31) |  | 0 | Visual dialog is a task of answering a sequence of questions grounded in an image using the previous dialog history as context. In this paper, we study how to address two fundamental challenges for this task: (1) reasoning over underlying semantic structures among dialog rounds and (2) identifying several appropriate answers to the given question. To address these challenges, we propose a Sparse Graph Learning (SGL) method to formulate visual dialog as a graph structure learning task. SGL... | GiCheon Kang, Junseok Park, Hwaran Lee, ByoungTak Zhang, JinHwa Kim |  |
| 76 |  |  [Exploring Sentence Community for Document-Level Event Extraction](https://doi.org/10.18653/v1/2021.findings-emnlp.32) |  | 0 | Document-level event extraction is critical to various natural language processing tasks for providing structured information. Existing approaches by sequential modeling neglect the complex logic structures for long texts. In this paper, we leverage the entity interactions and sentence interactions within long documents and transform each document into an undirected unweighted graph by exploiting the relationship between sentences. We introduce the Sentence Community to represent each event as... | Yusheng Huang, Weijia Jia |  |
| 77 |  |  [A Model of Cross-Lingual Knowledge-Grounded Response Generation for Open-Domain Dialogue Systems](https://doi.org/10.18653/v1/2021.findings-emnlp.33) |  | 0 | Research on open-domain dialogue systems that allow free topics is challenging in the field of natural language processing (NLP). The performance of the dialogue system has been improved recently by the method utilizing dialogue-related knowledge; however, non-English dialogue systems suffer from reproducing the performance of English dialogue systems because securing knowledge in the same language with the dialogue system is relatively difficult. Through experiments with a Korean dialogue... | San Kim, Jin Yea Jang, Minyoung Jung, Saim Shin |  |
| 78 |  |  [WHOSe Heritage: Classification of UNESCO World Heritage Statements of "Outstanding Universal Value" with Soft Labels](https://doi.org/10.18653/v1/2021.findings-emnlp.34) |  | 0 | The UNESCO World Heritage List (WHL) includes the exceptionally valuable cultural and natural heritage to be preserved for mankind. Evaluating and justifying the Outstanding Universal Value (OUV) is essential for each site inscribed in the WHL, and yet a complex task, even for experts, since the selection criteria of OUV are not mutually exclusive. Furthermore, manual annotation of heritage values and attributes from multi-source textual data, which is currently dominant in heritage studies, is... | Nan Bai, Renqian Luo, Pirouz Nourian, Ana Pereira Roders |  |
| 79 |  |  [P-INT: A Path-based Interaction Model for Few-shot Knowledge Graph Completion](https://doi.org/10.18653/v1/2021.findings-emnlp.35) |  | 0 | Few-shot knowledge graph completion is to infer the unknown facts (i.e., query head-tail entity pairs) of a given relation with only a few observed reference entity pairs. Its general process is to first encode the implicit relation of an entity pair and then match the relation of a query entity pair with the relations of the reference entity pairs. Most existing methods have thus far encoded an entity pair and matched entity pairs by using the direct neighbors of concerned entities. In this... | Jingwen Xu, Jing Zhang, Xirui Ke, Yuxiao Dong, Hong Chen, Cuiping Li, Yongbin Liu |  |
| 80 |  |  [Cartography Active Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.36) |  | 0 | We propose Cartography Active Learning (CAL), a novel Active Learning (AL) algorithm that exploits the behavior of the model on individual instances during training as a proxy to find the most informative instances for labeling. CAL is inspired by data maps, which were recently proposed to derive insights into dataset quality (Swayamdipta et al., 2020). We compare our method on popular text classification tasks to commonly used AL strategies, which instead rely on post-training behavior. We... | Mike Zhang, Barbara Plank |  |
| 81 |  |  [Beyond Reptile: Meta-Learned Dot-Product Maximization between Gradients for Improved Single-Task Regularization](https://doi.org/10.18653/v1/2021.findings-emnlp.37) |  | 0 | Meta-learning algorithms such as MAML, Reptile, and FOMAML have led to improved performance of several neural models. The primary difference between standard gradient descent and these meta-learning approaches is that they contain as a small component the gradient for maximizing dot-product between gradients of batches, leading to improved generalization. Previous work has shown that aligned gradients are related to generalization, and have also used the Reptile algorithm in a single-task... | Akhil Kedia, Sai Chetan Chinthakindi, Wonho Ryu |  |
| 82 |  |  [GooAQ: Open Question Answering with Diverse Answer Types](https://doi.org/10.18653/v1/2021.findings-emnlp.38) |  | 0 | While day-to-day questions come with a variety of answer types, the current question-answering (QA) literature has failed to adequately address the answer diversity of questions. To this end, we present GooAQ, a large-scale dataset with a variety of answer types. This dataset contains over 5 million questions and 3 million answers collected from Google. GooAQ questions are collected semi-automatically from the Google search engine using its autocomplete feature. This results in naturalistic... | Daniel Khashabi, Amos Ng, Tushar Khot, Ashish Sabharwal, Hannaneh Hajishirzi, Chris CallisonBurch |  |
| 83 |  |  [Attention Weights in Transformer NMT Fail Aligning Words Between Sequences but Largely Explain Model Predictions](https://doi.org/10.18653/v1/2021.findings-emnlp.39) |  | 0 | This work proposes an extensive analysis of the Transformer architecture in the Neural Machine Translation (NMT) setting. Focusing on the encoder-decoder attention mechanism, we prove that attention weights systematically make alignment errors by relying mainly on uninformative tokens from the source sequence. However, we observe that NMT models assign attention to these tokens to regulate the contribution in the prediction of the two contexts, the source and the prefix of the target sequence.... | Javier Ferrando, Marta R. Costajussà |  |
| 84 |  |  [BFClass: A Backdoor-free Text Classification Framework](https://doi.org/10.18653/v1/2021.findings-emnlp.40) |  | 0 | Backdoor attack introduces artificial vulnerabilities into the model by poisoning a subset of the training data via injecting triggers and modifying labels. Various trigger design strategies have been explored to attack text classifiers, however, defending such attacks remains an open problem. In this work, we propose BFClass, a novel efficient backdoor-free training framework for text classification. The backbone of BFClass is a pre-trained discriminator that predicts whether each token in the... | Zichao Li, Dheeraj Mekala, Chengyu Dong, Jingbo Shang |  |
| 85 |  |  [Multilingual Chart-based Constituency Parse Extraction from Pre-trained Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.41) |  | 0 | As it has been unveiled that pre-trained language models (PLMs) are to some extent capable of recognizing syntactic concepts in natural language, much effort has been made to develop a method for extracting complete (binary) parses from PLMs without training separate parsers. We improve upon this paradigm by proposing a novel chart-based method and an effective top-K ensemble technique. Moreover, we demonstrate that we can broaden the scope of application of the approach into multilingual... | Taeuk Kim, Bowen Li, Sanggoo Lee |  |
| 86 |  |  [Hyperbolic Geometry is Not Necessary: Lightweight Euclidean-Based Models for Low-Dimensional Knowledge Graph Embeddings](https://doi.org/10.18653/v1/2021.findings-emnlp.42) |  | 0 | Recent knowledge graph embedding (KGE) models based on hyperbolic geometry have shown great potential in a low-dimensional embedding space. However, the necessity of hyperbolic space in KGE is still questionable, because the calculation based on hyperbolic geometry is much more complicated than Euclidean operations. In this paper, based on the state-of-the-art hyperbolic-based model RotH, we develop two lightweight Euclidean-based models, called RotL and Rot2L. The RotL model simplifies the... | Kai Wang, Yu Liu, Dan Lin, Michael Sheng |  |
| 87 |  |  [CascadeBERT: Accelerating Inference of Pre-trained Language Models via Calibrated Complete Models Cascade](https://doi.org/10.18653/v1/2021.findings-emnlp.43) |  | 0 | Dynamic early exiting aims to accelerate the inference of pre-trained language models (PLMs) by emitting predictions in internal layers without passing through the entire model. In this paper, we empirically analyze the working mechanism of dynamic early exiting and find that it faces a performance bottleneck under high speed-up ratios. On one hand, the PLMs’ representations in shallow layers lack high-level semantic information and thus are not sufficient for accurate predictions. On the other... | Lei Li, Yankai Lin, Deli Chen, Shuhuai Ren, Peng Li, Jie Zhou, Xu Sun |  |
| 88 |  |  [Semi-supervised Relation Extraction via Incremental Meta Self-Training](https://doi.org/10.18653/v1/2021.findings-emnlp.44) |  | 0 | To alleviate human efforts from obtaining large-scale annotations, Semi-Supervised Relation Extraction methods aim to leverage unlabeled data in addition to learning from limited samples. Existing self-training methods suffer from the gradual drift problem, where noisy pseudo labels on unlabeled data are incorporated during training. To alleviate the noise in pseudo labels, we propose a method called MetaSRE, where a Relation Label Generation Network generates accurate quality assessment on... | Xuming Hu, Chenwei Zhang, Fukun Ma, Chenyao Liu, Lijie Wen, Philip S. Yu |  |
| 89 |  |  [Keyphrase Generation with Fine-Grained Evaluation-Guided Reinforcement Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.45) |  | 0 | Aiming to generate a set of keyphrases, Keyphrase Generation (KG) is a classical task for capturing the central idea from a given document. Based on Seq2Seq models, the previous reinforcement learning framework on KG tasks utilizes the evaluation metrics to further improve the well-trained neural models. However, these KG evaluation metrics such as F1@5 and F1@M are only aware of the exact correctness of predictions on phrase-level and ignore the semantic similarities between similar... | Yichao Luo, Yige Xu, Jiacheng Ye, Xipeng Qiu, Qi Zhang |  |
| 90 |  |  [Improving Knowledge Graph Embedding Using Affine Transformations of Entities Corresponding to Each Relation](https://doi.org/10.18653/v1/2021.findings-emnlp.46) |  | 0 | To find a suitable embedding for a knowledge graph remains a big challenge nowadays. By using previous knowledge graph embedding methods, every entity in a knowledge graph is usually represented as a k-dimensional vector. As we know, an affine transformation can be expressed in the form of a matrix multiplication followed by a translation vector. In this paper, we firstly utilize a set of affine transformations related to each relation to operate on entity vectors, and then these transformed... | Jinfa Yang, Yongjie Shi, Xin Tong, Robin Wang, Taiyan Chen, Xianghua Ying |  |
| 91 |  |  [Using Question Answering Rewards to Improve Abstractive Summarization](https://doi.org/10.18653/v1/2021.findings-emnlp.47) |  | 0 | Neural abstractive summarization models have drastically improved in the recent years. However, the summaries generated by these models generally suffer from issues such as: not capturing the critical facts in source documents, and containing facts that are inconsistent with the source documents. In this work, we present a general framework to train abstractive summarization models to alleviate such issues. We first train a sequence-to-sequence model to summarize documents, and then further... | R. Chulaka Gunasekara, Guy Feigenblat, Benjamin Sznajder, Ranit Aharonov, Sachindra Joshi |  |
| 92 |  |  [Effect Generation Based on Causal Reasoning](https://doi.org/10.18653/v1/2021.findings-emnlp.48) |  | 0 | Causal reasoning aims to predict the future scenarios that may be caused by the observed actions. However, existing causal reasoning methods deal with causalities on the word level. In this paper, we propose a novel event-level causal reasoning method and demonstrate its use in the task of effect generation. In particular, we structuralize the observed cause-effect event pairs into an event causality network, which describes causality dependencies. Given an input cause sentence, a causal... | Feiteng Mu, Wenjie Li, Zhipeng Xie |  |
| 93 |  |  [Distilling Word Meaning in Context from Pre-trained Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.49) |  | 0 | In this study, we propose a self-supervised learning method that distils representations of word meaning in context from a pre-trained masked language model. Word representations are the basis for context-aware lexical semantics and unsupervised semantic textual similarity (STS) estimation. A previous study transforms contextualised representations employing static word embeddings to weaken excessive effects of contextual information. In contrast, the proposed method derives representations of... | Yuki Arase, Tomoyuki Kajiwara |  |
| 94 |  |  [Unseen Entity Handling in Complex Question Answering over Knowledge Base via Language Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.50) |  | 0 | Complex question answering over knowledge base remains as a challenging task because it involves reasoning over multiple pieces of information, including intermediate entities/relations and other constraints. Previous methods simplify the SPARQL query of a question into such forms as a list or a graph, missing such constraints as “filter” and “order_by”, and present models specialized for generating those simplified forms from a given question. We instead introduce a novel approach that... | Xin Huang, JungJae Kim, Bowei Zou |  |
| 95 |  |  [Bidirectional Hierarchical Attention Networks based on Document-level Context for Emotion Cause Extraction](https://doi.org/10.18653/v1/2021.findings-emnlp.51) |  | 0 | Emotion cause extraction (ECE) aims to extract the causes behind the certain emotion in text. Some works related to the ECE task have been published and attracted lots of attention in recent years. However, these methods neglect two major issues: 1) pay few attentions to the effect of document-level context information on ECE, and 2) lack of sufficient exploration for how to effectively use the annotated emotion clause. For the first issue, we propose a bidirectional hierarchical attention... | Guimin Hu, Guangming Lu, Yi Zhao |  |
| 96 |  |  [Distantly Supervised Relation Extraction in Federated Settings](https://doi.org/10.18653/v1/2021.findings-emnlp.52) |  | 0 | In relation extraction, distant supervision is widely used to automatically label a large-scale training dataset by aligning a knowledge base with unstructured text. Most existing studies in this field have assumed there is a great deal of centralized unstructured text. However, in practice, texts are usually distributed on different platforms and cannot be centralized due to privacy restrictions. Therefore, it is worthwhile to investigate distant supervision in the federated learning paradigm,... | Dianbo Sui, Yubo Chen, Kang Liu, Jun Zhao |  |
| 97 |  |  [Casting the Same Sentiment Classification Problem](https://doi.org/10.18653/v1/2021.findings-emnlp.53) |  | 0 | We introduce and study a problem variant of sentiment analysis, namely the “same sentiment classification problem”, where, given a pair of texts, the task is to determine if they have the same sentiment, disregarding the actual sentiment polarity. Among other things, our goal is to enable a more topic-agnostic sentiment classification. We study the problem using the Yelp business review dataset, demonstrating how sentiment data needs to be prepared for this task, and then carry out sequence... | Erik Körner, Ahmad Dawar Hakimi, Gerhard Heyer, Martin Potthast |  |
| 98 |  |  [Detecting Compositionally Out-of-Distribution Examples in Semantic Parsing](https://doi.org/10.18653/v1/2021.findings-emnlp.54) |  | 0 | While neural networks are ubiquitous in state-of-the-art semantic parsers, it has been shown that most standard models suffer from dramatic performance losses when faced with compositionally out-of-distribution (OOD) data. Recently several methods have been proposed to improve compositional generalization in semantic parsing. In this work we instead focus on the problem of detecting compositionally OOD examples with neural semantic parsers, which, to the best of our knowledge, has not been... | Denis Lukovnikov, Sina Däubener, Asja Fischer |  |
| 99 |  |  [Saliency-based Multi-View Mixed Language Training for Zero-shot Cross-lingual Classification](https://doi.org/10.18653/v1/2021.findings-emnlp.55) |  | 0 | Recent multilingual pre-trained models, like XLM-RoBERTa (XLM-R), have been demonstrated effective in many cross-lingual tasks. However, there are still gaps between the contextualized representations of similar words in different languages. To solve this problem, we propose a novel framework named Multi-View Mixed Language Training (MVMLT), which leverages code-switched data with multi-view learning to fine-tune XLM-R. MVMLT uses gradient-based saliency to extract keywords which are the most... | Siyu Lai, Hui Huang, Dong Jing, Yufeng Chen, Jinan Xu, Jian Liu |  |
| 100 |  |  [Fighting the COVID-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society](https://doi.org/10.18653/v1/2021.findings-emnlp.56) |  | 0 | With the emergence of the COVID-19 pandemic, the political and the medical aspects of disinformation merged as the problem got elevated to a whole new level to become the first global infodemic. Fighting this infodemic has been declared one of the most important focus areas of the World Health Organization, with dangers ranging from promoting fake cures, rumors, and conspiracy theories to spreading xenophobia and panic. Addressing the issue requires solving a number of challenging problems such... | Firoj Alam, Shaden Shaar, Fahim Dalvi, Hassan Sajjad, Alex Nikolov, Hamdy Mubarak, Giovanni Da San Martino, Ahmed Abdelali, Nadir Durrani, Kareem Darwish, Abdulaziz AlHomaid, Wajdi Zaghouani, Tommaso Caselli, Gijs Danoe, Friso Stolk, Britt Bruntink, Preslav Nakov |  |
| 101 |  |  [FANATIC: FAst Noise-Aware TopIc Clustering](https://doi.org/10.18653/v1/2021.findings-emnlp.57) |  | 0 | Extracting salient topics from a collection of documents can be a challenging task when a) the amount of data is large, b) the number of topics is not known a priori, and/or c) “topic noise” is present. We define “topic noise” as the collection of documents that are irrelevant to any coherent topic and should be filtered out. By design, most clustering algorithms (e.g. k-means, hierarchical clustering) assign all input documents to one of the available clusters, guaranteeing any topic noise to... | Ari Silburt, Anja Subasic, Evan Thompson, Carmeline Dsilva, Tarec Fares |  |
| 102 |  |  [Stream-level Latency Evaluation for Simultaneous Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.58) |  | 0 | Simultaneous machine translation has recently gained traction thanks to significant quality improvements and the advent of streaming applications. Simultaneous translation systems need to find a trade-off between translation quality and response time, and with this purpose multiple latency measures have been proposed. However, latency evaluations for simultaneous translation are estimated at the sentence level, not taking into account the sequential nature of a streaming scenario. Indeed, these... | Javier IranzoSánchez, Jorge Civera Saiz, Alfons Juan |  |
| 103 |  |  [TSDAE: Using Transformer-based Sequential Denoising Auto-Encoderfor Unsupervised Sentence Embedding Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.59) |  | 0 | Learning sentence embeddings often requires a large amount of labeled data. However, for most tasks and domains, labeled data is seldom available and creating it is expensive. In this work, we present a new state-of-the-art unsupervised method based on pre-trained Transformers and Sequential Denoising Auto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points. It can achieve up to 93.1% of the performance of in-domain supervised approaches. Further, we show that TSDAE is a... | Kexin Wang, Nils Reimers, Iryna Gurevych |  |
| 104 |  |  [How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?](https://doi.org/10.18653/v1/2021.findings-emnlp.60) |  | 0 | Data-driven subword segmentation has become the default strategy for open-vocabulary machine translation and other NLP tasks, but may not be sufficiently generic for optimal learning of non-concatenative morphology. We design a test suite to evaluate segmentation strategies on different types of morphological phenomena in a controlled, semi-synthetic setting. In our experiments, we compare how well machine translation models trained on subword- and character-level can translate these... | Chantal Amrhein, Rico Sennrich |  |
| 105 |  |  [Rethinking Why Intermediate-Task Fine-Tuning Works](https://doi.org/10.18653/v1/2021.findings-emnlp.61) |  | 0 | Supplementary Training on Intermediate Labeled-data Tasks (STILT) is a widely applied technique, which first fine-tunes the pretrained language models on an intermediate task before on the target task of interest. While STILT is able to further improve the performance of pretrained language models, it is still unclear why and when it works. Previous research shows that those intermediate tasks involving complex inference, such as commonsense reasoning, work especially well for RoBERTa-large. In... | TingYun Chang, ChiJen Lu |  |
| 106 |  |  [Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.62) |  | 0 | The ability to continuously expand knowledge over time and utilize it to rapidly generalize to new tasks is a key feature of human linguistic intelligence. Existing models that pursue rapid generalization to new tasks (e.g., few-shot learning methods), however, are mostly trained in a single shot on fixed datasets, unable to dynamically expand their knowledge; while continual learning algorithms are not specifically designed for rapid generalization. We present a new learning setup, Continual... | Xisen Jin, Bill Yuchen Lin, Mohammad Rostami, Xiang Ren |  |
| 107 |  |  [Efficient Test Time Adapter Ensembling for Low-resource Language Varieties](https://doi.org/10.18653/v1/2021.findings-emnlp.63) |  | 0 | Adapters are light-weight modules that allow parameter-efficient fine-tuning of pretrained models. Specialized language and task adapters have recently been proposed to facilitate cross-lingual transfer of multilingual pretrained models (Pfeiffer et al., 2020b). However, this approach requires training a separate language adapter for every language one wishes to support, which can be impractical for languages with limited data. An intuitive solution is to use a related language adapter for the... | Xinyi Wang, Yulia Tsvetkov, Sebastian Ruder, Graham Neubig |  |
| 108 |  |  [An Analysis of Euclidean vs. Graph-Based Framing for Bilingual Lexicon Induction from Word Embedding Spaces](https://doi.org/10.18653/v1/2021.findings-emnlp.64) |  | 0 | Much recent work in bilingual lexicon induction (BLI) views word embeddings as vectors in Euclidean space. As such, BLI is typically solved by finding a linear transformation that maps embeddings to a common space. Alternatively, word embeddings may be understood as nodes in a weighted graph. This framing allows us to examine a node’s graph neighborhood without assuming a linear transform, and exploits new techniques from the graph matching optimization literature. These contrasting approaches... | Kelly Marchisio, Youngser Park, Ali SaadEldin, Anton Alyakin, Kevin Duh, Carey E. Priebe, Philipp Koehn |  |
| 109 |  |  [How to Select One Among All ? An Empirical Study Towards the Robustness of Knowledge Distillation in Natural Language Understanding](https://doi.org/10.18653/v1/2021.findings-emnlp.65) |  | 0 | Knowledge Distillation (KD) is a model compression algorithm that helps transfer the knowledge in a large neural network into a smaller one. Even though KD has shown promise on a wide range of Natural Language Processing (NLP) applications, little is understood about how one KD algorithm compares to another and whether these approaches can be complimentary to each other. In this work, we evaluate various KD algorithms on in-domain, out-of-domain and adversarial testing. We propose a framework... | Tianda Li, Ahmad Rashid, Aref Jafari, Pranav Sharma, Ali Ghodsi, Mehdi Rezagholizadeh |  |
| 110 |  |  [Recommend for a Reason: Unlocking the Power of Unsupervised Aspect-Sentiment Co-Extraction](https://doi.org/10.18653/v1/2021.findings-emnlp.66) |  | 0 | Compliments and concerns in reviews are valuable for understanding users’ shopping interests and their opinions with respect to specific aspects of certain items. Existing review-based recommenders favor large and complex language encoders that can only learn latent and uninterpretable text representations. They lack explicit user-attention and item-property modeling, which however could provide valuable information beyond the ability to recommend items. Therefore, we propose a tightly coupled... | Zeyu Li, Wei Cheng, Reema Kshetramade, John Houser, Haifeng Chen, Wei Wang |  |
| 111 |  |  [Learning Hard Retrieval Decoder Attention for Transformers](https://doi.org/10.18653/v1/2021.findings-emnlp.67) |  | 0 | The Transformer translation model is based on the multi-head attention mechanism, which can be parallelized easily. The multi-head attention network performs the scaled dot-product attention function in parallel, empowering the model by jointly attending to information from different representation subspaces at different positions. In this paper, we present an approach to learning a hard retrieval attention where an attention head only attends to one token in the sentence rather than all... | Hongfei Xu, Qiuhui Liu, Josef van Genabith, Deyi Xiong |  |
| 112 |  |  [Recall and Learn: A Memory-augmented Solver for Math Word Problems](https://doi.org/10.18653/v1/2021.findings-emnlp.68) |  | 0 | In this article, we tackle the math word problem, namely, automatically answering a mathematical problem according to its textual description. Although recent methods have demonstrated their promising results, most of these methods are based on template-based generation scheme which results in limited generalization capability. To this end, we propose a novel human-like analogical learning method in a recall and learn manner. Our proposed framework is composed of modules of memory,... | Shifeng Huang, Jiawei Wang, Jiao Xu, Da Cao, Ming Yang |  |
| 113 |  |  [An Uncertainty-Aware Encoder for Aspect Detection](https://doi.org/10.18653/v1/2021.findings-emnlp.69) |  | 0 | Aspect detection is a fundamental task in opinion mining. Previous works use seed words either as priors of topic models, as anchors to guide the learning of aspects, or as features of aspect classifiers. This paper presents a novel weakly-supervised method to exploit seed words for aspect detection based on an encoder architecture. The encoder maps segments and aspects into a low-dimensional embedding space. The goal is approximating similarity between segments and aspects in the embedding... | ThiNhung Nguyen, KiemHieu Nguyen, YoungIn Song, TuanDung Cao |  |
| 114 |  |  [Improving Empathetic Response Generation by Recognizing Emotion Cause in Conversations](https://doi.org/10.18653/v1/2021.findings-emnlp.70) |  | 0 | Current approaches to empathetic response generation focus on learning a model to predict an emotion label and generate a response based on this label and have achieved promising results. However, the emotion cause, an essential factor for empathetic responding, is ignored. The emotion cause is a stimulus for human emotions. Recognizing the emotion cause is helpful to better understand human emotions so as to generate more empathetic responses. To this end, we propose a novel framework that... | Jun Gao, Yuhan Liu, Haolin Deng, Wei Wang, Yu Cao, Jiachen Du, Ruifeng Xu |  |
| 115 |  |  [Probing Across Time: What Does RoBERTa Know and When?](https://doi.org/10.18653/v1/2021.findings-emnlp.71) |  | 0 | Models of language trained on very large corpora have been demonstrated useful for natural language processing. As fixed artifacts, they have become the object of intense study, with many researchers “probing” the extent to which they acquire and readily demonstrate linguistic abstractions, factual and commonsense knowledge, and reasoning abilities. Recent work applied several probes to intermediate training stages to observe the developmental process of a large-scale model (Chiang et al.,... | Zeyu Liu, Yizhong Wang, Jungo Kasai, Hannaneh Hajishirzi, Noah A. Smith |  |
| 116 |  |  [Knowledge-Guided Paraphrase Identification](https://doi.org/10.18653/v1/2021.findings-emnlp.72) |  | 0 | Paraphrase identification (PI), a fundamental task in natural language processing, is to identify whether two sentences express the same or similar meaning, which is a binary classification problem. Recently, BERT-like pre-trained language models have been a popular choice for the frameworks of various PI models, but almost all existing methods consider general domain text. When these approaches are applied to a specific domain, existing models cannot make accurate predictions due to the lack... | Haoyu Wang, Fenglong Ma, Yaqing Wang, Jing Gao |  |
| 117 |  |  [R2-D2: A Modular Baseline for Open-Domain Question Answering](https://doi.org/10.18653/v1/2021.findings-emnlp.73) |  | 0 | This work presents a novel four-stage open-domain QA pipeline R2-D2 (Rank twice, reaD twice). The pipeline is composed of a retriever, passage reranker, extractive reader, generative reader and a mechanism that aggregates the final prediction from all system’s components. We demonstrate its strength across three open-domain QA datasets: NaturalQuestions, TriviaQA and EfficientQA, surpassing state-of-the-art on the first two. Our analysis demonstrates that: (i) combining extractive and... | Martin Fajcik, Martin Docekal, Karel Ondrej, Pavel Smrz |  |
| 118 |  |  [What Does Your Smile Mean? Jointly Detecting Multi-Modal Sarcasm and Sentiment Using Quantum Probability](https://doi.org/10.18653/v1/2021.findings-emnlp.74) |  | 0 | Sarcasm and sentiment embody intrinsic uncertainty of human cognition, making joint detection of multi-modal sarcasm and sentiment a challenging task. In view of the advantages of quantum probability (QP) in modeling such uncertainty, this paper explores the potential of QP as a mathematical framework and proposes a QP driven multi-task (QPM) learning framework. The QPM framework involves a complex-valued multi-modal representation encoder, a quantum-like fusion subnetwork and a quantum... | Yaochen Liu, Yazhou Zhang, Qiuchi Li, Benyou Wang, Dawei Song |  |
| 119 |  |  [Discovering Representation Sprachbund For Multilingual Pre-Training](https://doi.org/10.18653/v1/2021.findings-emnlp.75) |  | 0 | Multilingual pre-trained models have demonstrated their effectiveness in many multilingual NLP tasks and enabled zero-shot or few-shot transfer from high-resource languages to low-resource ones. However, due to significant typological differences and contradictions between some languages, such models usually perform poorly on many languages and cross-lingual settings, which shows the difficulty of learning a single model to handle massive diverse languages well at the same time. To alleviate... | Yimin Fan, Yaobo Liang, Alexandre Muzio, Hany Hassan, Houqiang Li, Ming Zhou, Nan Duan |  |
| 120 |  |  [Plan-then-Generate: Controlled Data-to-Text Generation via Planning](https://doi.org/10.18653/v1/2021.findings-emnlp.76) |  | 0 | Recent developments in neural networks have led to the advance in data-to-text generation. However, the lack of ability of neural models to control the structure of generated output can be limiting in certain real-world applications. In this study, we propose a novel Plan-then-Generate (PlanGen) framework to improve the controllability of neural data-to-text models. Extensive experiments and analyses are conducted on two benchmark datasets, ToTTo and WebNLG. The results show that our model is... | Yixuan Su, David Vandyke, Sihui Wang, Yimai Fang, Nigel Collier |  |
| 121 |  |  [Few-Shot Table-to-Text Generation with Prototype Memory](https://doi.org/10.18653/v1/2021.findings-emnlp.77) |  | 0 | Neural table-to-text generation models have achieved remarkable progress on an array of tasks. However, due to the data-hungry nature of neural models, their performances strongly rely on large-scale training examples, limiting their applicability in real-world applications. To address this, we propose a new framework: Prototype-to-Generate (P2G), for table-to-text generation under the few-shot scenario. The proposed framework utilizes the retrieved prototypes, which are jointly selected by an... | Yixuan Su, Zaiqiao Meng, Simon Baker, Nigel Collier |  |
| 122 |  |  [Leveraging Word-Formation Knowledge for Chinese Word Sense Disambiguation](https://doi.org/10.18653/v1/2021.findings-emnlp.78) |  | 0 | In parataxis languages like Chinese, word meanings are constructed using specific word-formations, which can help to disambiguate word senses. However, such knowledge is rarely explored in previous word sense disambiguation (WSD) methods. In this paper, we propose to leverage word-formation knowledge to enhance Chinese WSD. We first construct a large-scale Chinese lexical sample WSD dataset with word-formations. Then, we propose a model FormBERT to explicitly incorporate word-formations into... | Hua Zheng, Lei Li, Damai Dai, Deli Chen, Tianyu Liu, Xu Sun, Yang Liu |  |
| 123 |  |  [Exploiting Curriculum Learning in Unsupervised Neural Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.79) |  | 0 | Back-translation (BT) has become one of the de facto components in unsupervised neural machine translation (UNMT), and it explicitly makes UNMT have translation ability. However, all the pseudo bi-texts generated by BT are treated equally as clean data during optimization without considering the quality diversity, leading to slow convergence and limited translation performance. To address this problem, we propose a curriculum learning method to gradually utilize pseudo bi-texts based on their... | Jinliang Lu, Jiajun Zhang |  |
| 124 |  |  [Robust Fragment-Based Framework for Cross-lingual Sentence Retrieval](https://doi.org/10.18653/v1/2021.findings-emnlp.80) |  | 0 | Cross-lingual Sentence Retrieval (CLSR) aims at retrieving parallel sentence pairs that are translations of each other from a multilingual set of comparable documents. The retrieved parallel sentence pairs can be used in other downstream NLP tasks such as machine translation and cross-lingual word sense disambiguation. We propose a CLSR framework called Robust Fragment-level Representation (RFR) CLSR framework to address Out-of-Domain (OOD) CLSR problems. In particular, we improve the sentence... | Nattapol Trijakwanich, Peerat Limkonchotiwat, Raheem Sarwar, Wannaphong Phatthiyaphaibun, Ekapol Chuangsuwanich, Sarana Nutanong |  |
| 125 |  |  [Towards Improving Adversarial Training of NLP Models](https://doi.org/10.18653/v1/2021.findings-emnlp.81) |  | 0 | Adversarial training, a method for learning robust deep neural networks, constructs adversarial examples during training. However, recent methods for generating NLP adversarial examples involve combinatorial search and expensive sentence encoders for constraining the generated instances. As a result, it remains challenging to use vanilla adversarial training to improve NLP models’ performance, and the benefits are mainly uninvestigated. This paper proposes a simple and improved vanilla... | Jin Yong Yoo, Yanjun Qi |  |
| 126 |  |  [To Protect and To Serve? Analyzing Entity-Centric Framing of Police Violence](https://doi.org/10.18653/v1/2021.findings-emnlp.82) |  | 0 | Framing has significant but subtle effects on public opinion and policy. We propose an NLP framework to measure entity-centric frames. We use it to understand media coverage on police violence in the United States in a new Police Violence Frames Corpus of 82k news articles spanning 7k police killings. Our work uncovers more than a dozen framing devices and reveals significant differences in the way liberal and conservative news sources frame both the issue of police violence and the entities... | Caleb Ziems, Diyi Yang |  |
| 127 |  |  [Calibrate your listeners! Robust communication-based training for pragmatic speakers](https://doi.org/10.18653/v1/2021.findings-emnlp.83) |  | 0 | To be good conversational partners, natural language processing (NLP) systems should be trained to produce contextually useful utterances. Prior work has investigated training NLP systems with communication-based objectives, where a neural listener stands in as a communication partner. However, these systems commonly suffer from semantic drift where the learned language diverges radically from natural language. We propose a method that uses a population of neural listeners to regularize speaker... | Rose E. Wang, Julia White, Jesse Mu, Noah D. Goodman |  |
| 128 |  |  [When Retriever-Reader Meets Scenario-Based Multiple-Choice Questions](https://doi.org/10.18653/v1/2021.findings-emnlp.84) |  | 0 | Scenario-based question answering (SQA) requires retrieving and reading paragraphs from a large corpus to answer a question which is contextualized by a long scenario description. Since a scenario contains both keyphrases for retrieval and much noise, retrieval for SQA is extremely difficult. Moreover, it can hardly be supervised due to the lack of relevance labels of paragraphs for SQA. To meet the challenge, in this paper we propose a joint retriever-reader model called JEEVES where the... | Zixian Huang, Ao Wu, Yulin Shen, Gong Cheng, Yuzhong Qu |  |
| 129 |  |  [Structured abbreviation expansion in context](https://doi.org/10.18653/v1/2021.findings-emnlp.85) |  | 0 | Ad hoc abbreviations are commonly found in informal communication channels that favor shorter messages. We consider the task of reversing these abbreviations in context to recover normalized, expanded versions of abbreviated messages. The problem is related to, but distinct from, spelling correction, as ad hoc abbreviations are intentional and can involve more substantial differences from the original words. Ad hoc abbreviations are also productively generated on-the-fly, so they cannot be... | Kyle Gorman, Christo Kirov, Brian Roark, Richard Sproat |  |
| 130 |  |  [Task-adaptive Pre-training and Self-training are Complementary for Natural Language Understanding](https://doi.org/10.18653/v1/2021.findings-emnlp.86) |  | 0 | Task-adaptive pre-training (TAPT) and Self-training (ST) have emerged as the major semi-supervised approaches to improve natural language understanding (NLU) tasks with massive amount of unlabeled data. However, it’s unclear whether they learn similar representations or they can be effectively combined. In this paper, we show that TAPT and ST can be complementary with simple TFS protocol by following TAPT -> Finetuning -> Self-training (TFS) process. Experimental results show that TFS protocol... | Shiyang Li, Semih Yavuz, Wenhu Chen, Xifeng Yan |  |
| 131 |  |  [CNNBiF: CNN-based Bigram Features for Named Entity Recognition](https://doi.org/10.18653/v1/2021.findings-emnlp.87) |  | 0 | Transformer models fine-tuned with a sequence labeling objective have become the dominant choice for named entity recognition tasks. However, a self-attention mechanism with unconstrained length can fail to fully capture local dependencies, particularly when training data is limited. In this paper, we propose a novel joint training objective which better captures the semantics of words corresponding to the same entity. By augmenting the training objective with a group-consistency loss component... | Chul Sung, Vaibhava Goel, Etienne Marcheret, Steven J. Rennie, David Nahamoo |  |
| 132 |  |  [Compositional Generalization via Semantic Tagging](https://doi.org/10.18653/v1/2021.findings-emnlp.88) |  | 0 | Although neural sequence-to-sequence models have been successfully applied to semantic parsing, they fail at compositional generalization, i.e., they are unable to systematically generalize to unseen compositions of seen components. Motivated by traditional semantic parsing where compositionality is explicitly accounted for by symbolic grammars, we propose a new decoding framework that preserves the expressivity and generality of sequence-to-sequence models while featuring lexicon-style... | Hao Zheng, Mirella Lapata |  |
| 133 |  |  [Towards Document-Level Paraphrase Generation with Sentence Rewriting and Reordering](https://doi.org/10.18653/v1/2021.findings-emnlp.89) |  | 0 | Paraphrase generation is an important task in natural language processing. Previous works focus on sentence-level paraphrase generation, while ignoring document-level paraphrase generation, which is a more challenging and valuable task. In this paper, we explore the task of document-level paraphrase generation for the first time and focus on the inter-sentence diversity by considering sentence rewriting and reordering. We propose CoRPG (Coherence Relationship guided Paraphrase Generation),... | Zhe Lin, Yitao Cai, Xiaojun Wan |  |
| 134 |  |  [Exploring Decomposition for Table-based Fact Verification](https://doi.org/10.18653/v1/2021.findings-emnlp.90) |  | 0 | Fact verification based on structured data is challenging as it requires models to understand both natural language and symbolic operations performed over tables. Although pre-trained language models have demonstrated a strong capability in verifying simple statements, they struggle with complex statements that involve multiple operations. In this paper, we improve fact verification by decomposing complex statements into simpler subproblems. Leveraging the programs synthesized by a weakly... | Xiaoyu Yang, Xiaodan Zhu |  |
| 135 |  |  [Diversity and Consistency: Exploring Visual Question-Answer Pair Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.91) |  | 0 | Although showing promising values to downstream applications, generating question and answer together is under-explored. In this paper, we introduce a novel task that targets question-answer pair generation from visual images. It requires not only generating diverse question-answer pairs but also keeping the consistency of them. We study different generation paradigms for this task and propose three models: the pipeline model, the joint model, and the sequential model. We integrate variational... | Sen Yang, Qingyu Zhou, Dawei Feng, Yang Liu, Chao Li, Yunbo Cao, Dongsheng Li |  |
| 136 |  |  [Entity-level Cross-modal Learning Improves Multi-modal Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.92) |  | 0 | Multi-modal machine translation (MMT) aims at improving translation performance by incorporating visual information. Most of the studies leverage the visual information through integrating the global image features as auxiliary input or decoding by attending to relevant local regions of the image. However, this kind of usage of visual information makes it difficult to figure out how the visual modality helps and why it works. Inspired by the findings of (CITATION) that entities are most... | Xin Huang, Jiajun Zhang, Chengqing Zong |  |
| 137 |  |  [Learning to Ground Visual Objects for Visual Dialog](https://doi.org/10.18653/v1/2021.findings-emnlp.93) |  | 0 | Visual dialog is challenging since it needs to answer a series of coherent questions based on understanding the visual environment. How to ground related visual objects is one of the key problems. Previous studies utilize the question and history to attend to the image and achieve satisfactory performance, while these methods are not sufficient to locate related visual objects without any guidance. The inappropriate grounding of visual objects prohibits the performance of visual dialog models.... | Feilong Chen, Xiuyi Chen, Can Xu, Daxin Jiang |  |
| 138 |  |  [KERS: A Knowledge-Enhanced Framework for Recommendation Dialog Systems with Multiple Subgoals](https://doi.org/10.18653/v1/2021.findings-emnlp.94) |  | 0 | Recommendation dialogs require the system to build a social bond with users to gain trust and develop affinity in order to increase the chance of a successful recommendation. It is beneficial to divide up, such conversations with multiple subgoals (such as social chat, question answering, recommendation, etc.), so that the system can retrieve appropriate knowledge with better accuracy under different subgoals. In this paper, we propose a unified framework for common knowledge-based... | Jun Zhang, Yan Yang, Chencai Chen, Liang He, Zhou Yu |  |
| 139 |  |  [Less Is More: Domain Adaptation with Lottery Ticket for Reading Comprehension](https://doi.org/10.18653/v1/2021.findings-emnlp.95) |  | 0 | In this paper, we propose a simple few-shot domain adaptation paradigm for reading comprehension. We first identify the lottery subnetwork structure within the Transformer-based source domain model via gradual magnitude pruning. Then, we only fine-tune the lottery subnetwork, a small fraction of the whole parameters, on the annotated target domain data for adaptation. To obtain more adaptable subnetworks, we introduce self-attention attribution to weigh parameters, beyond simply pruning the... | Haichao Zhu, Zekun Wang, Heng Zhang, Ming Liu, Sendong Zhao, Bing Qin |  |
| 140 |  |  [Effectiveness of Pre-training for Few-shot Intent Classification](https://doi.org/10.18653/v1/2021.findings-emnlp.96) |  | 0 | This paper investigates the effectiveness of pre-training for few-shot intent classification. While existing paradigms commonly further pre-train language models such as BERT on a vast amount of unlabeled corpus, we find it highly effective and efficient to simply fine-tune BERT with a small set of labeled utterances from public datasets. Specifically, fine-tuning BERT with roughly 1,000 labeled data yields a pre-trained model – IntentBERT, which can easily surpass the performance of existing... | Haode Zhang, Yuwei Zhang, LiMing Zhan, Jiaxin Chen, Guangyuan Shi, XiaoMing Wu, Albert Y. S. Lam |  |
| 141 |  |  [Improving Abstractive Dialogue Summarization with Hierarchical Pretraining and Topic Segment](https://doi.org/10.18653/v1/2021.findings-emnlp.97) |  | 0 | With the increasing abundance of meeting transcripts, meeting summary has attracted more and more attention from researchers. The unsupervised pre-training method based on transformer structure combined with fine-tuning of downstream tasks has achieved great success in the field of text summarization. However, the semantic structure and style of meeting transcripts are quite different from that of articles. In this work, we propose a hierarchical transformer encoder-decoder network with... | Mengnan Qi, Hao Liu, Yuzhuo Fu, Ting Liu |  |
| 142 |  |  [Learning to Answer Psychological Questionnaire for Personality Detection](https://doi.org/10.18653/v1/2021.findings-emnlp.98) |  | 0 | Existing text-based personality detection research mostly relies on data-driven approaches to implicitly capture personality cues in online posts, lacking the guidance of psychological knowledge. Psychological questionnaire, which contains a series of dedicated questions highly related to personality traits, plays a critical role in self-report personality assessment. We argue that the posts created by a user contain critical contents that could help answer the questions in a questionnaire,... | Feifan Yang, Tao Yang, Xiaojun Quan, Qinliang Su |  |
| 143 |  |  [Exploiting Reasoning Chains for Multi-hop Science Question Answering](https://doi.org/10.18653/v1/2021.findings-emnlp.99) |  | 0 | We propose a novel Chain Guided Retriever-reader (CGR) framework to model the reasoning chain for multi-hop Science Question Answering. Our framework is capable of performing explainable reasoning without the need of any corpus-specific annotations, such as the ground-truth reasoning chain, or human-annotated entity mentions. Specifically, we first generate reasoning chains from a semantic graph constructed by Abstract Meaning Representation of retrieved evidence facts. A Chain-aware loss,... | Weiwen Xu, Yang Deng, Huihui Zhang, Deng Cai, Wai Lam |  |
| 144 |  |  [Winnowing Knowledge for Multi-choice Question Answering](https://doi.org/10.18653/v1/2021.findings-emnlp.100) |  | 0 | We tackle multi-choice question answering. Acquiring related commonsense knowledge to the question and options facilitates the recognition of the correct answer. However, the current reasoning models suffer from the noises in the retrieved knowledge. In this paper, we propose a novel encoding method which is able to conduct interception and soft filtering. This contributes to the harvesting and absorption of representative information with less interference from noises. We experiment on... | Yeqiu Li, Bowei Zou, Zhifeng Li, Ai Ti Aw, Yu Hong, Qiaoming Zhu |  |
| 145 |  |  [Neural Media Bias Detection Using Distant Supervision With BABE - Bias Annotations By Experts](https://doi.org/10.18653/v1/2021.findings-emnlp.101) |  | 0 | Media coverage has a substantial effect on the public perception of events. Nevertheless, media outlets are often biased. One way to bias news articles is by altering the word choice. The automatic identification of bias by word choice is challenging, primarily due to the lack of a gold standard data set and high context dependencies. This paper presents BABE, a robust and diverse data set created by trained experts, for media bias research. We also analyze why expert labeling is essential... | Timo Spinde, Manuel Plank, JanDavid Krieger, Terry Ruas, Bela Gipp, Akiko Aizawa |  |
| 146 |  |  [Learning and Evaluating a Differentially Private Pre-trained Language Model](https://doi.org/10.18653/v1/2021.findings-emnlp.102) |  | 0 | Contextual language models have led to significantly better results, especially when pre-trained on the same data as the downstream task. While this additional pre-training usually improves performance, it can lead to information leakage and therefore risks the privacy of individuals mentioned in the training data. One method to guarantee the privacy of such individuals is to train a differentially-private language model, but this usually comes at the expense of model performance. Also, in the... | Shlomo Hoory, Amir Feder, Avichai Tendler, Sofia Erell, Alon PeledCohen, Itay Laish, Hootan Nakhost, Uri Stemmer, Ayelet Benjamini, Avinatan Hassidim, Yossi Matias |  |
| 147 |  |  [Simulated Chats for Building Dialog Systems: Learning to Generate Conversations from Instructions](https://doi.org/10.18653/v1/2021.findings-emnlp.103) |  | 0 | Popular dialog datasets such as MultiWOZ are created by providing crowd workers an instruction, expressed in natural language, that describes the task to be accomplished. Crowd workers play the role of a user and an agent to generate dialogs to accomplish tasks involving booking restaurant tables, calling a taxi etc. In this paper, we present a data creation strategy that uses the pre-trained language model, GPT2, to simulate the interaction between crowd workers by creating a user bot and an... | Biswesh Mohapatra, Gaurav Pandey, Danish Contractor, Sachindra Joshi |  |
| 148 |  |  [Past, Present, and Future: Conversational Emotion Recognition through Structural Modeling of Psychological Knowledge](https://doi.org/10.18653/v1/2021.findings-emnlp.104) |  | 0 | Conversational Emotion Recognition (CER) is a task to predict the emotion of an utterance in the context of a conversation. Although modeling the conversational context and interactions between speakers has been studied broadly, it is important to consider the speaker’s psychological state, which controls the action and intention of the speaker. The state-of-the-art method introduces CommonSense Knowledge (CSK) to model psychological states in a sequential way (forwards and backwards). However,... | Jiangnan Li, Zheng Lin, Peng Fu, Weiping Wang |  |
| 149 |  |  [An unsupervised framework for tracing textual sources of moral change](https://doi.org/10.18653/v1/2021.findings-emnlp.105) |  | 0 | Morality plays an important role in social well-being, but people’s moral perception is not stable and changes over time. Recent advances in natural language processing have shown that text is an effective medium for informing moral change, but no attempt has been made to quantify the origins of these changes. We present a novel unsupervised framework for tracing textual sources of moral change toward entities through time. We characterize moral change with probabilistic topical distributions... | Aida Ramezani, Zining Zhu, Frank Rudzicz, Yang Xu |  |
| 150 |  |  [Topic-Aware Contrastive Learning for Abstractive Dialogue Summarization](https://doi.org/10.18653/v1/2021.findings-emnlp.106) |  | 0 | Unlike well-structured text, such as news reports and encyclopedia articles, dialogue content often comes from two or more interlocutors, exchanging information with each other. In such a scenario, the topic of a conversation can vary upon progression and the key information for a certain topic is often scattered across multiple utterances of different speakers, which poses challenges to abstractly summarize dialogues. To capture the various topic information of a conversation and outline... | Junpeng Liu, Yanyan Zou, Hainan Zhang, Hongshen Chen, Zhuoye Ding, Caixia Yuan, Xiaojie Wang |  |
| 151 |  |  [TWT: Table with Written Text for Controlled Data-to-Text Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.107) |  | 0 | Large pre-trained neural models have recently shown remarkable progress in text generation. In this paper, we propose to generate text conditioned on the structured data (table) and a prefix (the written text) by leveraging the pre-trained models. We present a new data-to-text dataset, Table with Written Text (TWT), by repurposing two existing datasets: ToTTo and TabFact. TWT contains both factual and logical statements that are faithful to the structured data, aiming to serve as a useful... | Tongliang Li, Lei Fang, JianGuang Lou, Zhoujun Li |  |
| 152 |  |  [ArabicTransformer: Efficient Large Arabic Language Model with Funnel Transformer and ELECTRA Objective](https://doi.org/10.18653/v1/2021.findings-emnlp.108) |  | 0 | Pre-training Transformer-based models such as BERT and ELECTRA on a collection of Arabic corpora, demonstrated by both AraBERT and AraELECTRA, shows an impressive result on downstream tasks. However, pre-training Transformer-based language models is computationally expensive, especially for large-scale models. Recently, Funnel Transformer has addressed the sequential redundancy inside Transformer architecture by compressing the sequence of hidden states, leading to a significant reduction in... | Sultan Alrowili, Vijay Shanker |  |
| 153 |  |  [Which is Making the Contribution: Modulating Unimodal and Cross-modal Dynamics for Multimodal Sentiment Analysis](https://doi.org/10.18653/v1/2021.findings-emnlp.109) |  | 0 | Multimodal sentiment analysis (MSA) draws increasing attention with the availability of multimodal data. The boost in performance of MSA models is mainly hindered by two problems. On the one hand, recent MSA works mostly focus on learning cross-modal dynamics, but neglect to explore an optimal solution for unimodal networks, which determines the lower limit of MSA models. On the other hand, noisy information hidden in each modality interferes the learning of correct cross-modal dynamics. To... | Ying Zeng, Sijie Mai, Haifeng Hu |  |
| 154 |  |  [CVAE-based Re-anchoring for Implicit Discourse Relation Classification](https://doi.org/10.18653/v1/2021.findings-emnlp.110) |  | 0 | Training implicit discourse relation classifiers suffers from data sparsity. Variational AutoEncoder (VAE) appears to be the proper solution. It is because ideally VAE is capable of generating inexhaustible varying samples, and this facilitates selective data augmentation. However, our experiments show that coupling VAE with the RoBERTa-based classifier results in severe performance degradation. We ascribe the unusual phenomenon to erroneous sampling that would happen when VAE pursued... | Zujun Dou, Yu Hong, Yu Sun, Guodong Zhou |  |
| 155 |  |  [Combining Curriculum Learning and Knowledge Distillation for Dialogue Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.111) |  | 0 | Curriculum learning, a machine training strategy that feeds training instances to the model from easy to hard, has been proven to facilitate the dialogue generation task. Meanwhile, knowledge distillation, a knowledge transformation methodology among teachers and students networks can yield significant performance boost for student models. Hence, in this paper, we introduce a combination of curriculum learning and knowledge distillation for efficient dialogue generation models, where curriculum... | Qingqing Zhu, Xiuying Chen, Pengfei Wu, Junfei Liu, Dongyan Zhao |  |
| 156 |  |  [Improving End-to-End Task-Oriented Dialog System with A Simple Auxiliary Task](https://doi.org/10.18653/v1/2021.findings-emnlp.112) |  | 0 | The paradigm of leveraging large pre-trained language models has made significant progress on benchmarks on task-oriented dialogue (TOD) systems. In this paper, we combine this paradigm with multi-task learning framework for end-to-end TOD modeling by adopting span prediction as an auxiliary task. In end-to-end setting, our model achieves new state-of-the-art results with combined scores of 108.3 and 107.5 on MultiWOZ 2.0 and MultiWOZ 2.1, respectively. Furthermore, we demonstrate that... | Yohan Lee |  |
| 157 |  |  [EDTC: A Corpus for Discourse-Level Topic Chain Parsing](https://doi.org/10.18653/v1/2021.findings-emnlp.113) |  | 0 | Discourse analysis has long been known to be fundamental in natural language processing. In this research, we present our insight on discourse-level topic chain (DTC) parsing which aims at discovering new topics and investigating how these topics evolve over time within an article. To address the lack of data, we contribute a new discourse corpus with DTC-style dependency graphs annotated upon news articles. In particular, we ensure the high reliability of the corpus by utilizing a two-step... | Longyin Zhang, Xin Tan, Fang Kong, Guodong Zhou |  |
| 158 |  |  [Multilingual Neural Machine Translation: Can Linguistic Hierarchies Help?](https://doi.org/10.18653/v1/2021.findings-emnlp.114) |  | 0 | Multilingual Neural Machine Translation (MNMT) trains a single NMT model that supports translation between multiple languages, rather than training separate models for different languages. Learning a single model can enhance the low-resource translation by leveraging data from multiple languages. However, the performance of an MNMT model is highly dependent on the type of languages used in training, as transferring knowledge from a diverse set of languages degrades the translation performance... | Fahimeh Saleh, Wray L. Buntine, Gholamreza Haffari, Lan Du |  |
| 159 |  |  [Self Question-answering: Aspect-based Sentiment Analysis by Role Flipped Machine Reading Comprehension](https://doi.org/10.18653/v1/2021.findings-emnlp.115) |  | 0 | The pivot for the unified Aspect-based Sentiment Analysis (ABSA) is to couple aspect terms with their corresponding opinion terms, which might further derive easier sentiment predictions. In this paper, we investigate the unified ABSA task from the perspective of Machine Reading Comprehension (MRC) by observing that the aspect and the opinion terms can serve as the query and answer in MRC interchangeably. We propose a new paradigm named Role Flipped Machine Reading Comprehension (RF-MRC) to... | Guoxin Yu, Jiwei Li, Ling Luo, Yuxian Meng, Xiang Ao, Qing He |  |
| 160 |  |  [Generalization in Text-based Games via Hierarchical Reinforcement Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.116) |  | 0 | Deep reinforcement learning provides a promising approach for text-based games in studying natural language communication between humans and artificial agents. However, the generalization still remains a big challenge as the agents depend critically on the complexity and variety of training tasks. In this paper, we address this problem by introducing a hierarchical framework built upon the knowledge graph-based RL agent. In the high level, a meta-policy is executed to decompose the whole game... | Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Chengqi Zhang |  |
| 161 |  |  [A Finer-grain Universal Dialogue Semantic Structures based Model For Abstractive Dialogue Summarization](https://doi.org/10.18653/v1/2021.findings-emnlp.117) |  | 0 | Although abstractive summarization models have achieved impressive results on document summarization tasks, their performance on dialogue modeling is much less satisfactory due to the crude and straight methods for dialogue encoding. To address this question, we propose a novel end-to-end Transformer-based model FinDS for abstractive dialogue summarization that leverages Finer-grain universal Dialogue semantic Structures to model dialogue and generates better summaries. Experiments on the... | Yuejie Lei, Fujia Zheng, Yuanmeng Yan, Keqing He, Weiran Xu |  |
| 162 |  |  [Constructing contrastive samples via summarization for text classification with limited annotations](https://doi.org/10.18653/v1/2021.findings-emnlp.118) |  | 0 | Contrastive Learning has emerged as a powerful representation learning method and facilitates various downstream tasks especially when supervised data is limited. How to construct efficient contrastive samples through data augmentation is key to its success. Unlike vision tasks, the data augmentation method for contrastive learning has not been investigated sufficiently in language tasks. In this paper, we propose a novel approach to construct contrastive samples for language tasks using text... | Yangkai Du, Tengfei Ma, Lingfei Wu, Fangli Xu, Xuhong Zhang, Bo Long, Shouling Ji |  |
| 163 |  |  [End-to-end Neural Information Status Classification](https://doi.org/10.18653/v1/2021.findings-emnlp.119) |  | 0 | Most previous studies on information status (IS) classification and bridging anaphora recognition assume that the gold mention or syntactic tree information is given (Hou et al., 2013; Roesiger et al., 2018; Hou, 2020; Yu and Poesio, 2020). In this paper, we propose an end-to-end neural approach for information status classification. Our approach consists of a mention extraction component and an information status assignment component. During the inference time, our system takes a raw text as... | Yufang Hou |  |
| 164 |  |  [EventKE: Event-Enhanced Knowledge Graph Embedding](https://doi.org/10.18653/v1/2021.findings-emnlp.120) |  | 0 | Relations in most of the traditional knowledge graphs (KGs) only reflect static and factual connections, but fail to represent the dynamic activities and state changes about entities. In this paper, we emphasize the importance of incorporating events in KG representation learning, and propose an event-enhanced KG embedding model EventKE. Specifically, given the original KG, we first incorporate event nodes by building a heterogeneous network, where entity nodes and event nodes are distributed... | Zixuan Zhang, Hongwei Wang, Han Zhao, Hanghang Tong, Heng Ji |  |
| 165 |  |  [Modeling Concentrated Cross-Attention for Neural Machine Translation with Gaussian Mixture Model](https://doi.org/10.18653/v1/2021.findings-emnlp.121) |  | 0 | Cross-attention is an important component of neural machine translation (NMT), which is always realized by dot-product attention in previous methods. However, dot-product attention only considers the pair-wise correlation between words, resulting in dispersion when dealing with long sentences and neglect of source neighboring relationships. Inspired by linguistics, the above issues are caused by ignoring a type of cross-attention, called concentrated attention, which focuses on several central... | Shaolei Zhang, Yang Feng |  |
| 166 |  |  [Inconsistency Matters: A Knowledge-guided Dual-inconsistency Network for Multi-modal Rumor Detection](https://doi.org/10.18653/v1/2021.findings-emnlp.122) |  | 0 | Rumor spreaders are increasingly utilizing multimedia content to attract the attention and trust of news consumers. Though a set of rumor detection models have exploited the multi-modal data, they seldom consider the inconsistent relationships among images and texts. Moreover, they also fail to find a powerful way to spot the inconsistency information among the post contents and background knowledge. Motivated by the intuition that rumors are more likely to have inconsistency information in... | Mengzhu Sun, Xi Zhang, Jianqiang Ma, Yazheng Liu |  |
| 167 |  |  [EfficientBERT: Progressively Searching Multilayer Perceptron via Warm-up Knowledge Distillation](https://doi.org/10.18653/v1/2021.findings-emnlp.123) |  | 0 | Pre-trained language models have shown remarkable results on various NLP tasks. Nevertheless, due to their bulky size and slow inference speed, it is hard to deploy them on edge devices. In this paper, we have a critical insight that improving the feed-forward network (FFN) in BERT has a higher gain than improving the multi-head attention (MHA) since the computational cost of FFN is 2~3 times larger than MHA. Hence, to compact BERT, we are devoted to designing efficient FFN as opposed to... | Chenhe Dong, Guangrun Wang, Hang Xu, Jiefeng Peng, Xiaozhe Ren, Xiaodan Liang |  |
| 168 |  |  [Uni-FedRec: A Unified Privacy-Preserving News Recommendation Framework for Model Training and Online Serving](https://doi.org/10.18653/v1/2021.findings-emnlp.124) |  | 0 | News recommendation techniques can help users on news platforms obtain their preferred news information. Most existing news recommendation methods rely on centrally stored user behavior data to train models and serve users. However, user data is usually highly privacy-sensitive, and centrally storing them in the news platform may raise privacy concerns and risks. In this paper, we propose a unified news recommendation framework, which can utilize user data locally stored in user clients to... | Tao Qi, Fangzhao Wu, Chuhan Wu, Yongfeng Huang, Xing Xie |  |
| 169 |  |  [Mapping Language to Programs using Multiple Reward Components with Inverse Reinforcement Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.125) |  | 0 | Mapping natural language instructions to programs that computers can process is a fundamental challenge. Existing approaches focus on likelihood-based training or using reinforcement learning to fine-tune models based on a single reward. In this paper, we pose program generation from language as Inverse Reinforcement Learning. We introduce several interpretable reward components and jointly learn (1) a reward function that linearly combines them, and (2) a policy for program generation.... | Sayan Ghosh, Shashank Srivastava |  |
| 170 |  |  [Topic-Guided Abstractive Multi-Document Summarization](https://doi.org/10.18653/v1/2021.findings-emnlp.126) |  | 0 | A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and then apply a graph-to-sequence framework to generate summaries. Moreover, we employ a neural topic model to jointly discover latent topics that can act as cross-document semantic units to bridge... | Peng Cui, Le Hu |  |
| 171 |  |  [An Edge-Enhanced Hierarchical Graph-to-Tree Network for Math Word Problem Solving](https://doi.org/10.18653/v1/2021.findings-emnlp.127) |  | 0 | Math word problem solving has attracted considerable research interest in recent years. Previous works have shown the effectiveness of utilizing graph neural networks to capture the relationships in the problem. However, these works did not carefully take the edge label information and the long-range word relationship across sentences into consideration. In addition, during generation, they focus on the most relevant areas of the currently generated word, while neglecting the rest of the... | Qinzhuo Wu, Qi Zhang, Zhongyu Wei |  |
| 172 |  |  [SciXGen: A Scientific Paper Dataset for Context-Aware Text Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.128) |  | 0 | Generating texts in scientific papers requires not only capturing the content contained within the given input but also frequently acquiring the external information called context. We push forward the scientific text generation by proposing a new task, namely context-aware text generation in the scientific domain, aiming at exploiting the contributions of context in generated texts. To this end, we present a novel challenging large-scale Scientific Paper Dataset for ConteXt-Aware Text... | Hong Chen, Hiroya Takamura, Hideki Nakayama |  |
| 173 |  |  [Don't Miss the Potential Customers! Retrieving Similar Ads to Improve User Targeting](https://doi.org/10.18653/v1/2021.findings-emnlp.129) |  | 0 | User targeting is an essential task in the modern advertising industry: given a package of ads for a particular category of products (e.g., green tea), identify the online users to whom the ad package should be targeted. A (ad package specific) user targeting model is typically trained using historical clickthrough data: positive instances correspond to users who have clicked on an ad in the package before, whereas negative instances correspond to users who have not clicked on any ads in the... | Yi Feng, Ting Wang, Chuanyi Li, Vincent Ng, Jidong Ge, Bin Luo, Yucheng Hu, Xiaopeng Zhang |  |
| 174 |  |  [Cross-lingual Transfer for Text Classification with Dictionary-based Heterogeneous Graph](https://doi.org/10.18653/v1/2021.findings-emnlp.130) |  | 0 | In cross-lingual text classification, it is required that task-specific training data in high-resource source languages are available, where the task is identical to that of a low-resource target language. However, collecting such training data can be infeasible because of the labeling cost, task characteristics, and privacy concerns. This paper proposes an alternative solution that uses only task-independent word embeddings of high-resource languages and bilingual dictionaries. First, we... | Nuttapong Chairatanakul, Noppayut Sriwatanasakdi, Nontawat Charoenphakdee, Xin Liu, Tsuyoshi Murata |  |
| 175 |  |  [Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.131) |  | 0 | Distantly supervised named entity recognition (DS-NER) efficiently reduces labor costs but meanwhile intrinsically suffers from the label noise due to the strong assumption of distant supervision. Typically, the wrongly labeled instances comprise numbers of incomplete and inaccurate annotations, while most prior denoising works are only concerned with one kind of noise and fail to fully explore useful information in the training set. To address this issue, we propose a robust learning paradigm... | Xinghua Zhang, Bowen Yu, Tingwen Liu, Zhenyu Zhang, Jiawei Sheng, Mengge Xue, Hongbo Xu |  |
| 176 |  |  [Entity-Based Semantic Adequacy for Data-to-Text Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.132) |  | 0 | While powerful pre-trained language models have improved the fluency of text generation models, semantic adequacy -the ability to generate text that is semantically faithful to the input- remains an unsolved issue. In this paper, we introduce a novel automatic evaluation metric, Entity-Based Semantic Adequacy, which can be used to assess to what extent generation models that verbalise RDF (Resource Description Framework) graphs produce text that contains mentions of the entities occurring in... | Juliette Faille, Albert Gatt, Claire Gardent |  |
| 177 |  |  [MiRANews: Dataset and Benchmarks for Multi-Resource-Assisted News Summarization](https://doi.org/10.18653/v1/2021.findings-emnlp.133) |  | 0 | One of the most challenging aspects of current single-document news summarization is that the summary often contains ‘extrinsic hallucinations’, i.e., facts that are not present in the source document, which are often derived via world knowledge. This causes summarisation systems to act more like open-ended language models tending to hallucinate facts that are erroneous. In this paper, we mitigate this problem with the help of multiple supplementary resource documents assisting the task. We... | Xinnuo Xu, Ondrej Dusek, Shashi Narayan, Verena Rieser, Ioannis Konstas |  |
| 178 |  |  [A Conditional Generative Matching Model for Multi-lingual Reply Suggestion](https://doi.org/10.18653/v1/2021.findings-emnlp.134) |  | 0 | We study the problem of multilingual automated reply suggestions (RS) model serving many languages simultaneously. Multilingual models are often challenged by model capacity and severe data distribution skew across languages. While prior works largely focus on monolingual models, we propose Conditional Generative Matching models (CGM), optimized within a Variational Autoencoder framework to address challenges arising from multilingual RS. CGM does so with expressive message conditional priors,... | Budhaditya Deb, Guoqing Zheng, Milad Shokouhi, Ahmed Hassan Awadallah |  |
| 179 |  |  [Rethinking Sentiment Style Transfer](https://doi.org/10.18653/v1/2021.findings-emnlp.135) |  | 0 | Though remarkable efforts have been made in non-parallel text style transfer, the evaluation system is unsatisfactory. It always evaluates over samples from only one checkpoint of the model and compares three metrics, i.e., transfer accuracy, BLEU score, and PPL score. In this paper, we argue the inappropriateness of both existing evaluation metrics and the evaluation method. Specifically, for evaluation metrics, we make a detailed analysis and comparison from three aspects: style transfer,... | Ping Yu, Yang Zhao, Chunyuan Li, Changyou Chen |  |
| 180 |  |  [HypoGen: Hyperbole Generation with Commonsense and Counterfactual Knowledge](https://doi.org/10.18653/v1/2021.findings-emnlp.136) |  | 0 | A hyperbole is an intentional and creative exaggeration not to be taken literally. Despite its ubiquity in daily life, the computational explorations of hyperboles are scarce. In this paper, we tackle the under-explored and challenging task: sentence-level hyperbole generation. We start with a representative syntactic pattern for intensification and systematically study the semantic (commonsense and counterfactual) relationships between each component in such hyperboles. We then leverage... | Yufei Tian, Arvind Krishna Sridhar, Nanyun Peng |  |
| 181 |  |  [Profiling News Discourse Structure Using Explicit Subtopic Structures Guided Critics](https://doi.org/10.18653/v1/2021.findings-emnlp.137) |  | 0 | We present an actor-critic framework to induce subtopical structures in a news article for news discourse profiling. The model uses multiple critics that act according to known subtopic structures while the actor aims to outperform them. The content structures constitute sentences that represent latent subtopic boundaries. Then, we introduce a hierarchical neural network that uses the identified subtopic boundary sentences to model multi-level interaction between sentences, subtopics, and the... | Prafulla Kumar Choubey, Ruihong Huang |  |
| 182 |  |  [ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection](https://doi.org/10.18653/v1/2021.findings-emnlp.138) |  | 0 | The ability to detect Out-of-Domain (OOD) inputs has been a critical requirement in many real-world NLP applications. For example, intent classification in dialogue systems. The reason is that the inclusion of unsupported OOD inputs may lead to catastrophic failure of systems. However, it remains an empirical question whether current methods can tackle such problems reliably in a realistic scenario where zero OOD training data is available. In this study, we propose ProtoInfoMax, a new... | Iftitahu Ni'mah, Meng Fang, Vlado Menkovski, Mykola Pechenizkiy |  |
| 183 |  |  [Learning from Language Description: Low-shot Named Entity Recognition via Decomposed Framework](https://doi.org/10.18653/v1/2021.findings-emnlp.139) |  | 0 | In this work, we study the problem of named entity recognition (NER) in a low resource scenario, focusing on few-shot and zero-shot settings. Built upon large-scale pre-trained language models, we propose a novel NER framework, namely SpanNER, which learns from natural language supervision and enables the identification of never-seen entity classes without using in-domain labeled data. We perform extensive experiments on 5 benchmark datasets and evaluate the proposed method in the few-shot... | Yaqing Wang, Haoda Chu, Chao Zhang, Jing Gao |  |
| 184 |  |  [BERT might be Overkill: A Tiny but Effective Biomedical Entity Linker based on Residual Convolutional Neural Networks](https://doi.org/10.18653/v1/2021.findings-emnlp.140) |  | 0 | Biomedical entity linking is the task of linking entity mentions in a biomedical document to referent entities in a knowledge base. Recently, many BERT-based models have been introduced for the task. While these models achieve competitive results on many datasets, they are computationally expensive and contain about 110M parameters. Little is known about the factors contributing to their impressive performance and whether the over-parameterization is needed. In this work, we shed some light on... | Tuan Manh Lai, Heng Ji, ChengXiang Zhai |  |
| 185 |  |  [Char2Subword: Extending the Subword Embedding Space Using Robust Character Compositionality](https://doi.org/10.18653/v1/2021.findings-emnlp.141) |  | 0 | Byte-pair encoding (BPE) is a ubiquitous algorithm in the subword tokenization process of language models as it provides multiple benefits. However, this process is solely based on pre-training data statistics, making it hard for the tokenizer to handle infrequent spellings. On the other hand, though robust to misspellings, pure character-level models often lead to unreasonably long sequences and make it harder for the model to learn meaningful words. To alleviate these challenges, we propose a... | Gustavo Aguilar, Bryan McCann, Tong Niu, Nazneen Rajani, Nitish Shirish Keskar, Thamar Solorio |  |
| 186 |  |  [Exploring Multitask Learning for Low-Resource Abstractive Summarization](https://doi.org/10.18653/v1/2021.findings-emnlp.142) |  | 0 | This paper explores the effect of using multitask learning for abstractive summarization in the context of small training corpora. In particular, we incorporate four different tasks (extractive summarization, language modeling, concept detection, and paraphrase detection) both individually and in combination, with the goal of enhancing the target task of abstractive summarization via multitask learning. We show that for many task combinations, a model trained in a multitask setting outperforms... | Ahmed Magooda, Diane J. Litman, Mohamed Elaraby |  |
| 187 |  |  [Conical Classification For Efficient One-Class Topic Determination](https://doi.org/10.18653/v1/2021.findings-emnlp.143) |  | 0 | As the Internet grows in size, so does the amount of text based information that exists. For many application spaces it is paramount to isolate and identify texts that relate to a particular topic. While one-class classification would be ideal for such analysis, there is a relative lack of research regarding efficient approaches with high predictive power. By noting that the range of documents we wish to identify can be represented as positive linear combinations of the Vector Space Model... | Sameer Khanna |  |
| 188 |  |  [Improving Dialogue State Tracking with Turn-based Loss Function and Sequential Data Augmentation](https://doi.org/10.18653/v1/2021.findings-emnlp.144) |  | 0 | While state-of-the-art Dialogue State Tracking (DST) models show promising results, all of them rely on a traditional cross-entropy loss function during the training process, which may not be optimal for improving the joint goal accuracy. Although several approaches recently propose augmenting the training set by copying user utterances and replacing the real slot values with other possible or even similar values, they are not effective at improving the performance of existing DST models. To... | Jarana Manotumruksa, Jeff Dalton, Edgar Meij, Emine Yilmaz |  |
| 189 |  |  [TIAGE: A Benchmark for Topic-Shift Aware Dialog Modeling](https://doi.org/10.18653/v1/2021.findings-emnlp.145) |  | 0 | Human conversations naturally evolve around different topics and fluently move between them. In research on dialog systems, the ability to actively and smoothly transition to new topics is often ignored. In this paper we introduce TIAGE, a new topic-shift aware dialog benchmark constructed utilizing human annotations on topic shifts. Based on TIAGE, we introduce three tasks to investigate different scenarios of topic-shift modeling in dialog settings: topic-shift detection, topic-shift... | Huiyuan Xie, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu, Ann A. Copestake |  |
| 190 |  |  [Optimal Neural Program Synthesis from Multimodal Specifications](https://doi.org/10.18653/v1/2021.findings-emnlp.146) |  | 0 | Multimodal program synthesis, which leverages different types of user input to synthesize a desired program, is an attractive way to scale program synthesis to challenging settings; however, it requires integrating noisy signals from the user, like natural language, with hard constraints on the program’s behavior. This paper proposes an optimal neural synthesis approach where the goal is to find a program that satisfies user-provided constraints while also maximizing the program’s score with... | Xi Ye, Qiaochu Chen, Isil Dillig, Greg Durrett |  |
| 191 |  |  [Sent2Span: Span Detection for PICO Extraction in the Biomedical Text without Span Annotations](https://doi.org/10.18653/v1/2021.findings-emnlp.147) |  | 0 | The rapid growth in published clinical trials makes it difficult to maintain up-to-date systematic reviews, which require finding all relevant trials. This leads to policy and practice decisions based on out-of-date, incomplete, and biased subsets of available clinical evidence. Extracting and then normalising Population, Intervention, Comparator, and Outcome (PICO) information from clinical trial articles may be an effective way to automatically assign trials to systematic reviews and avoid... | Shifeng Liu, Yifang Sun, Bing Li, Wei Wang, Florence T. Bourgeois, Adam G. Dunn |  |
| 192 |  |  [When in Doubt: Improving Classification Performance with Alternating Normalization](https://doi.org/10.18653/v1/2021.findings-emnlp.148) |  | 0 | We introduce Classification with Alternating Normalization (CAN), a non-parametric post-processing step for classification. CAN improves classification accuracy for challenging examples by re-adjusting their predicted class probability distribution using the predicted class distributions of high-confidence validation examples. CAN is easily applicable to any probabilistic classifier, with minimal computation overhead. We analyze the properties of CAN using simulated experiments, and empirically... | Menglin Jia, Austin Reiter, SerNam Lim, Yoav Artzi, Claire Cardie |  |
| 193 |  |  [APGN: Adversarial and Parameter Generation Networks for Multi-Source Cross-Domain Dependency Parsing](https://doi.org/10.18653/v1/2021.findings-emnlp.149) |  | 0 | Thanks to the strong representation learning capability of deep learning, especially pre-training techniques with language model loss, dependency parsing has achieved great performance boost in the in-domain scenario with abundant labeled training data for target domains. However, the parsing community has to face the more realistic setting where the parsing performance drops drastically when labeled data only exists for several fixed out-domains. In this work, we propose a novel model for... | Ying Li, Meishan Zhang, Zhenghua Li, Min Zhang, Zhefeng Wang, Baoxing Huai, Nicholas Jing Yuan |  |
| 194 |  |  ["Let Your Characters Tell Their Story": A Dataset for Character-Centric Narrative Understanding](https://doi.org/10.18653/v1/2021.findings-emnlp.150) |  | 0 | When reading a literary piece, readers often make inferences about various characters’ roles, personalities, relationships, intents, actions, etc. While humans can readily draw upon their past experiences to build such a character-centric view of the narrative, understanding characters in narratives can be a challenging task for machines. To encourage research in this field of character-centric narrative understanding, we present LiSCU – a new dataset of literary pieces and their summaries... | Faeze Brahman, Meng Huang, Oyvind Tafjord, Chao Zhao, Mrinmaya Sachan, Snigdha Chaturvedi |  |
| 195 |  |  [Towards Developing a Multilingual and Code-Mixed Visual Question Answering System by Knowledge Distillation](https://doi.org/10.18653/v1/2021.findings-emnlp.151) |  | 0 | Pre-trained language-vision models have shown remarkable performance on the visual question answering (VQA) task. However, most pre-trained models are trained by only considering monolingual learning, especially the resource-rich language like English. Training such models for multilingual setups demand high computing resources and multilingual language-vision dataset which hinders their application in practice. To alleviate these challenges, we propose a knowledge distillation approach to... | Humair Raj Khan, Deepak Gupta, Asif Ekbal |  |
| 196 |  |  [An Iterative Multi-Knowledge Transfer Network for Aspect-Based Sentiment Analysis](https://doi.org/10.18653/v1/2021.findings-emnlp.152) |  | 0 | Aspect-based sentiment analysis (ABSA) mainly involves three subtasks: aspect term extraction, opinion term extraction, and aspect-level sentiment classification, which are typically handled in a separate or joint manner. However, previous approaches do not well exploit the interactive relations among three subtasks and do not pertinently leverage the easily available document-level labeled domain/sentiment knowledge, which restricts their performances. To address these issues, we propose a... | Yunlong Liang, Fandong Meng, Jinchao Zhang, Yufeng Chen, Jinan Xu, Jie Zhou |  |
| 197 |  |  [Semantic Alignment with Calibrated Similarity for Multilingual Sentence Embedding](https://doi.org/10.18653/v1/2021.findings-emnlp.153) |  | 0 | Measuring the similarity score between a pair of sentences in different languages is the essential requisite for multilingual sentence embedding methods. Predicting the similarity score consists of two sub-tasks, which are monolingual similarity evaluation and multilingual sentence retrieval. However, conventional methods have mainly tackled only one of the sub-tasks and therefore showed biased performances. In this paper, we suggest a novel and strong method for multilingual sentence... | Jiyeon Ham, EunSol Kim |  |
| 198 |  |  [fBERT: A Neural Transformer for Identifying Offensive Content](https://doi.org/10.18653/v1/2021.findings-emnlp.154) |  | 0 | Transformer-based models such as BERT, XLNET, and XLM-R have achieved state-of-the-art performance across various NLP tasks including the identification of offensive language and hate speech, an important problem in social media. In this paper, we present fBERT, a BERT model retrained on SOLID, the largest English offensive language identification corpus available with over 1.4 million offensive instances. We evaluate fBERT’s performance on identifying offensive content on multiple English... | Diptanu Sarkar, Marcos Zampieri, Tharindu Ranasinghe, Alexander G. Ororbia II |  |
| 199 |  |  [WIKIBIAS: Detecting Multi-Span Subjective Biases in Language](https://doi.org/10.18653/v1/2021.findings-emnlp.155) |  | 0 | Biases continue to be prevalent in modern text and media, especially subjective bias – a special type of bias that introduces improper attitudes or presents a statement with the presupposition of truth. To tackle the problem of detecting and further mitigating subjective bias, we introduce a manually annotated parallel corpus WIKIBIAS with more than 4,000 sentence pairs from Wikipedia edits. This corpus contains annotations towards both sentence-level bias types and token-level biased segments.... | Yang Zhong, Jingfeng Yang, Wei Xu, Diyi Yang |  |
| 200 |  |  [UnClE: Explicitly Leveraging Semantic Similarity to Reduce the Parameters of Word Embeddings](https://doi.org/10.18653/v1/2021.findings-emnlp.156) |  | 0 | Natural language processing (NLP) models often require a massive number of parameters for word embeddings, which limits their application on mobile devices. Researchers have employed many approaches, e.g. adaptive inputs, to reduce the parameters of word embeddings. However, existing methods rarely pay attention to semantic information. In this paper, we propose a novel method called Unique and Class Embeddings (UnClE), which explicitly leverages semantic similarity with weight sharing to... | Zhi Li, Yuchen Zhai, Chengyu Wang, Minghui Qiu, Kailiang Li, Yin Zhang |  |
| 201 |  |  [Grounded Graph Decoding improves Compositional Generalization in Question Answering](https://doi.org/10.18653/v1/2021.findings-emnlp.157) |  | 0 | Question answering models struggle to generalize to novel compositions of training patterns. Current end-to-end models learn a flat input embedding which can lose input syntax context. Prior approaches improve generalization by learning permutation invariant models, but these methods do not scale to more complex train-test splits. We propose Grounded Graph Decoding, a method to improve compositional generalization of language representations by grounding structured predictions with an attention... | Yu Gai, Paras Jain, Wendi Zhang, Joseph Gonzalez, Dawn Song, Ion Stoica |  |
| 202 |  |  [Enhancing Visual Dialog Questioner with Entity-based Strategy Learning and Augmented Guesser](https://doi.org/10.18653/v1/2021.findings-emnlp.158) |  | 0 | Considering the importance of building a good Visual Dialog (VD) Questioner, many researchers study the topic under a Q-Bot-A-Bot image-guessing game setting, where the Questioner needs to raise a series of questions to collect information of an undisclosed image. Despite progress has been made in Supervised Learning (SL) and Reinforcement Learning (RL), issues still exist. Firstly, previous methods do not provide explicit and effective guidance for Questioner to generate visually related and... | Duo Zheng, Zipeng Xu, Fandong Meng, Xiaojie Wang, Jiaan Wang, Jie Zhou |  |
| 203 |  |  [A Pretraining Numerical Reasoning Model for Ordinal Constrained Question Answering on Knowledge Base](https://doi.org/10.18653/v1/2021.findings-emnlp.159) |  | 0 | Knowledge Base Question Answering (KBQA) is to answer natural language questions posed over knowledge bases (KBs). This paper targets at empowering the IR-based KBQA models with the ability of numerical reasoning for answering ordinal constrained questions. A major challenge is the lack of explicit annotations about numerical properties. To address this challenge, we propose a pretraining numerical reasoning model consisting of NumGNN and NumTransformer, guided by explicit self-supervision... | Yu Feng, Jing Zhang, Gaole He, Wayne Xin Zhao, Lemao Liu, Quan Liu, Cuiping Li, Hong Chen |  |
| 204 |  |  [RoR: Read-over-Read for Long Document Machine Reading Comprehension](https://doi.org/10.18653/v1/2021.findings-emnlp.160) |  | 0 | Transformer-based pre-trained models, such as BERT, have achieved remarkable results on machine reading comprehension. However, due to the constraint of encoding length (e.g., 512 WordPiece tokens), a long document is usually split into multiple chunks that are independently read. It results in the reading field being limited to individual chunks without information collaboration for long document machine reading comprehension. To address this problem, we propose RoR, a read-over-read method,... | Jing Zhao, Junwei Bao, Yifan Wang, Yongwei Zhou, Youzheng Wu, Xiaodong He, Bowen Zhou |  |
| 205 |  |  [Span Pointer Networks for Non-Autoregressive Task-Oriented Semantic Parsing](https://doi.org/10.18653/v1/2021.findings-emnlp.161) |  | 0 | An effective recipe for building seq2seq, non-autoregressive, task-oriented parsers to map utterances to semantic frames proceeds in three steps: encoding an utterance x, predicting a frame’s length \|y\|, and decoding a \|y\|-sized frame with utterance and ontology tokens. Though empirically strong, these models are typically bottlenecked by length prediction, as even small inaccuracies change the syntactic and semantic characteristics of resulting frames. In our work, we propose span pointer... | Akshat Shrivastava, Pierce Chuang, Arun Babu, Shrey Desai, Abhinav Arora, Alexander Zotov, Ahmed Aly |  |
| 206 |  |  [Language Resource Efficient Learning for Captioning](https://doi.org/10.18653/v1/2021.findings-emnlp.162) |  | 0 | Due to complex cognitive and inferential efforts involved in the manual generation of one caption per image/video input, the human annotation resources are very limited for captioning tasks. We define language resource efficient as reaching the same performance with fewer annotated captions per input. We first study the performance degradation of caption models in different language resource settings. Our analysis of caption models with SC loss shows that the performance degradation is caused... | Jia Chen, Yike Wu, Shiwan Zhao, Qin Jin |  |
| 207 |  |  [Translation as Cross-Domain Knowledge: Attention Augmentation for Unsupervised Cross-Domain Segmenting and Labeling Tasks](https://doi.org/10.18653/v1/2021.findings-emnlp.163) |  | 0 | The nature of no word delimiter or inflection that can indicate segment boundaries or word semantics increases the difficulty of Chinese text understanding, and also intensifies the demand for word-level semantic knowledge to accomplish the tagging goal in Chinese segmenting and labeling tasks. However, for unsupervised Chinese cross-domain segmenting and labeling tasks, the model trained on the source domain frequently suffers from the deficient word-level semantic knowledge of the target... | Ruixuan Luo, Yi Zhang, Sishuo Chen, Xu Sun |  |
| 208 |  |  [ContractNLI: A Dataset for Document-level Natural Language Inference for Contracts](https://doi.org/10.18653/v1/2021.findings-emnlp.164) |  | 0 | Reviewing contracts is a time-consuming procedure that incurs large expenses to companies and social inequality to those who cannot afford it. In this work, we propose “document-level natural language inference (NLI) for contracts”, a novel, real-world application of NLI that addresses such problems. In this task, a system is given a set of hypotheses (such as “Some obligations of Agreement may survive termination.”) and a contract, and it is asked to classify whether each hypothesis is... | Yuta Koreeda, Christopher D. Manning |  |
| 209 |  |  [Japanese Zero Anaphora Resolution Can Benefit from Parallel Texts Through Neural Transfer Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.165) |  | 0 | Parallel texts of Japanese and a non-pro-drop language have the potential of improving the performance of Japanese zero anaphora resolution (ZAR) because pronouns dropped in the former are usually mentioned explicitly in the latter. However, rule-based cross-lingual transfer is hampered by error propagation in an NLP pipeline and the frequent lack of transparency in translation correspondences. In this paper, we propose implicit transfer by injecting machine translation (MT) as an intermediate... | Masato Umakoshi, Yugo Murawaki, Sadao Kurohashi |  |
| 210 |  |  [Grouped-Attention for Content-Selection and Content-Plan Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.166) |  | 0 | Content-planning is an essential part of data-to-text generation to determine the order of data mentioned in generated texts. Recent neural data-to-text generation models employ Pointer Networks to explicitly learn content-plan given a set of attributes as input. They use LSTM to encode the input, which assumes a sequential relationship in the input. This may be sub-optimal to encode a set of attributes, where the attributes have a composite structure: the attributes are disordered while each... | Bayu Distiawan Trisedya, Xiaojie Wang, Jianzhong Qi, Rui Zhang, Qingjun Cui |  |
| 211 |  |  [An Explicit-Joint and Supervised-Contrastive Learning Framework for Few-Shot Intent Classification and Slot Filling](https://doi.org/10.18653/v1/2021.findings-emnlp.167) |  | 0 | Intent classification (IC) and slot filling (SF) are critical building blocks in task-oriented dialogue systems. These two tasks are closely-related and can flourish each other. Since only a few utterances can be utilized for identifying fast-emerging new intents and slots, data scarcity issue often occurs when implementing IC and SF. However, few IC/SF models perform well when the number of training samples per class is quite small. In this paper, we propose a novel explicit-joint and... | Han Liu, Feng Zhang, Xiaotong Zhang, Siyang Zhao, Xianchao Zhang |  |
| 212 |  |  [Retrieve, Discriminate and Rewrite: A Simple and Effective Framework for Obtaining Affective Response in Retrieval-Based Chatbots](https://doi.org/10.18653/v1/2021.findings-emnlp.168) |  | 0 | Obtaining affective response is a key step in building empathetic dialogue systems. This task has been studied a lot in generation-based chatbots, but the related research in retrieval-based chatbots is still in the early stage. Existing works in retrieval-based chatbots are based on Retrieve-and-Rerank framework, which have a common problem of satisfying affect label at the expense of response quality. To address this problem, we propose a simple and effective Retrieve-Discriminate-Rewrite... | Xin Lu, Yijian Tian, Yanyan Zhao, Bing Qin |  |
| 213 |  |  [Span Fine-tuning for Pre-trained Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.169) |  | 0 | Pre-trained language models (PrLM) have to carefully manage input units when training on a very large text with a vocabulary consisting of millions of words. Previous works have shown that incorporating span-level information over consecutive words in pre-training could further improve the performance of PrLMs. However, given that span-level clues are introduced and fixed in pre-training, previous methods are time-consuming and lack of flexibility. To alleviate the inconvenience, this paper... | Rongzhou Bao, Zhuosheng Zhang, Hai Zhao |  |
| 214 |  |  [DIRECT: Direct and Indirect Responses in Conversational Text Corpus](https://doi.org/10.18653/v1/2021.findings-emnlp.170) |  | 0 | We create a large-scale dialogue corpus that provides pragmatic paraphrases to advance technology for understanding the underlying intentions of users. While neural conversation models acquire the ability to generate fluent responses through training on a dialogue corpus, previous corpora have mainly focused on the literal meanings of utterances. However, in reality, people do not always present their intentions directly. For example, if a person said to the operator of a reservation service “I... | Junya Takayama, Tomoyuki Kajiwara, Yuki Arase |  |
| 215 |  |  [Retrieval, Analogy, and Composition: A framework for Compositional Generalization in Image Captioning](https://doi.org/10.18653/v1/2021.findings-emnlp.171) |  | 0 | Image captioning systems are expected to have the ability to combine individual concepts when describing scenes with concept combinations that are not observed during training. In spite of significant progress in image captioning with the help of the autoregressive generation framework, current approaches fail to generalize well to novel concept combinations. We propose a new framework that revolves around probing several similar image caption training instances (retrieval), performing... | Zhan Shi, Hui Liu, Martin Renqiang Min, Christopher Malon, Li Erran Li, Xiaodan Zhu |  |
| 216 |  |  [TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.172) |  | 0 | Recent progress in generative language models has enabled machines to generate astonishingly realistic texts. While there are many legitimate applications of such models, there is also a rising need to distinguish machine-generated texts from human-written ones (e.g., fake news detection). However, to our best knowledge, there is currently no benchmark environment with datasets and tasks to systematically study the so-called ”Turing Test” problem for neural text generation methods. In this... | Adaku Uchendu, Zeyu Ma, Thai Le, Rui Zhang, Dongwon Lee |  |
| 217 |  |  [Say 'YES' to Positivity: Detecting Toxic Language in Workplace Communications](https://doi.org/10.18653/v1/2021.findings-emnlp.173) |  | 0 | Workplace communication (e.g. email, chat, etc.) is a central part of enterprise productivity. Healthy conversations are crucial for creating an inclusive environment and maintaining harmony in an organization. Toxic communications at the workplace can negatively impact overall job satisfaction and are often subtle, hidden, or demonstrate human biases. The linguistic subtlety of mild yet hurtful conversations has made it difficult for researchers to quantify and extract toxic conversations... | Meghana Moorthy Bhat, Saghar Hosseini, Ahmed Hassan Awadallah, Paul N. Bennett, Weisheng Li |  |
| 218 |  |  [Natural SQL: Making SQL Easier to Infer from Natural Language Specifications](https://doi.org/10.18653/v1/2021.findings-emnlp.174) |  | 0 | Addressing the mismatch between natural language descriptions and the corresponding SQL queries is a key challenge for text-to-SQL translation. To bridge this gap, we propose an SQL intermediate representation (IR) called Natural SQL (NatSQL). Specifically, NatSQL preserves the core functionalities of SQL, while it simplifies the queries as follows: (1) dispensing with operators and keywords such as GROUP BY, HAVING, FROM, JOIN ON, which are usually hard to find counterparts in the text... | Yujian Gan, Xinyun Chen, Jinxia Xie, Matthew Purver, John R. Woodward, John H. Drake, Qiaofu Zhang |  |
| 219 |  |  [Mitigating Data Scarceness through Data Synthesis, Augmentation and Curriculum for Abstractive Summarization](https://doi.org/10.18653/v1/2021.findings-emnlp.175) |  | 0 | This paper explores three simple data manipulation techniques (synthesis, augmentation, curriculum) for improving abstractive summarization models without the need for any additional data. We introduce a method of data synthesis with paraphrasing, a data augmentation technique with sample mixing, and curriculum learning with two new difficulty metrics based on specificity and abstractiveness. We conduct experiments to show that these three techniques can help improve abstractive summarization... | Ahmed Magooda, Diane J. Litman |  |
| 220 |  |  [Self- and Pseudo-self-supervised Prediction of Speaker and Key-utterance for Multi-party Dialogue Reading Comprehension](https://doi.org/10.18653/v1/2021.findings-emnlp.176) |  | 0 | Multi-party dialogue machine reading comprehension (MRC) brings tremendous challenge since it involves multiple speakers at one dialogue, resulting in intricate speaker information flows and noisy dialogue contexts. To alleviate such difficulties, previous models focus on how to incorporate these information using complex graph-based modules and additional manually labeled data, which is usually rare in real scenarios. In this paper, we design two labour-free self- and pseudo-self-supervised... | Yiyang Li, Hai Zhao |  |
| 221 |  |  [Few-Shot Novel Concept Learning for Semantic Parsing](https://doi.org/10.18653/v1/2021.findings-emnlp.177) |  | 0 | Humans are capable of learning novel concepts from very few examples; in contrast, state-of-the-art machine learning algorithms typically need thousands of examples to do so. In this paper, we propose an algorithm for learning novel concepts by representing them as programs over existing concepts. This way the concept learning problem is naturally a program synthesis problem and our algorithm learns from a few examples to synthesize a program representing the novel concept. In addition, we... | Soham Dan, Osbert Bastani, Dan Roth |  |
| 222 |  |  [Compositional Data and Task Augmentation for Instruction Following](https://doi.org/10.18653/v1/2021.findings-emnlp.178) |  | 0 | Executing natural language instructions in a physically grounded domain requires a model that understands both spatial concepts such as “left of” and “above”, and the compositional language used to identify landmarks and articulate instructions relative to them. In this paper, we study instruction understanding in the blocks world domain. Given an initial arrangement of blocks and a natural language instruction, the system executes the instruction by manipulating selected blocks. The highly... | Soham Dan, Xinran Han, Dan Roth |  |
| 223 |  |  [Are Factuality Checkers Reliable? Adversarial Meta-evaluation of Factuality in Summarization](https://doi.org/10.18653/v1/2021.findings-emnlp.179) |  | 0 | With the continuous upgrading of the summarization systems driven by deep neural networks, researchers have higher requirements on the quality of the generated summaries, which should be not only fluent and informative but also factually correct. As a result, the field of factual evaluation has developed rapidly recently. Despite its initial progress in evaluating generated summaries, the meta-evaluation methodologies of factuality metrics are limited in their opacity, leading to the... | Yiran Chen, Pengfei Liu, Xipeng Qiu |  |
| 224 |  |  [On the Effects of Transformer Size on In- and Out-of-Domain Calibration](https://doi.org/10.18653/v1/2021.findings-emnlp.180) |  | 0 | Large, pre-trained transformer language models, which are pervasive in natural language processing tasks, are notoriously expensive to train. To reduce the cost of training such large models, prior work has developed smaller, more compact models which achieves a significant speedup in training time while maintaining competitive accuracy to the original model on downstream tasks. Though these smaller pre-trained models have been widely adopted by the community, it is not known how well are they... | Soham Dan, Dan Roth |  |
| 225 |  |  [Detecting Polarized Topics Using Partisanship-aware Contextualized Topic Embeddings](https://doi.org/10.18653/v1/2021.findings-emnlp.181) |  | 0 | Growing polarization of the news media has been blamed for fanning disagreement, controversy and even violence. Early identification of polarized topics is thus an urgent matter that can help mitigate conflict. However, accurate measurement of topic-wise polarization is still an open research challenge. To address this gap, we propose Partisanship-aware Contextualized Topic Embeddings (PaCTE), a method to automatically detect polarized topics from partisan news sources. Specifically, utilizing... | Zihao He, Negar Mokhberian, António Câmara, Andrés Abeliuk, Kristina Lerman |  |
| 226 |  |  [GenerativeRE: Incorporating a Novel Copy Mechanism and Pretrained Model for Joint Entity and Relation Extraction](https://doi.org/10.18653/v1/2021.findings-emnlp.182) |  | 0 | Previous neural Seq2Seq models have shown the effectiveness for jointly extracting relation triplets. However, most of these models suffer from incompletion and disorder problems when they extract multi-token entities from input sentences. To tackle these problems, we propose a generative, multi-task learning framework, named GenerativeRE. We firstly propose a special entity labelling method on both input and output sequences. During the training stage, GenerativeRE fine-tunes the pre-trained... | Jiarun Cao, Sophia Ananiadou |  |
| 227 |  |  [Re-entry Prediction for Online Conversations via Self-Supervised Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.183) |  | 0 | In recent years, world business in online discussions and opinion sharing on social media is booming. Re-entry prediction task is thus proposed to help people keep track of the discussions which they wish to continue. Nevertheless, existing works only focus on exploiting chatting history and context information, and ignore the potential useful learning signals underlying conversation data, such as conversation thread patterns and repeated engagement of target users, which help better understand... | Lingzhi Wang, Xingshan Zeng, Huang Hu, KamFai Wong, Daxin Jiang |  |
| 228 |  |  [proScript: Partially Ordered Scripts Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.184) |  | 0 | Scripts – prototypical event sequences describing everyday activities – have been shown to help understand narratives by providing expectations, resolving ambiguity, and filling in unstated information. However, to date they have proved hard to author or extract from text. In this work, we demonstrate for the first time that pre-trained neural language models can be finetuned to generate high-quality scripts, at varying levels of granularity, for a wide range of everyday scenarios (e.g., bake a... | Keisuke Sakaguchi, Chandra Bhagavatula, Ronan Le Bras, Niket Tandon, Peter Clark, Yejin Choi |  |
| 229 |  |  [Speaker Turn Modeling for Dialogue Act Classification](https://doi.org/10.18653/v1/2021.findings-emnlp.185) |  | 0 | Dialogue Act (DA) classification is the task of classifying utterances with respect to the function they serve in a dialogue. Existing approaches to DA classification model utterances without incorporating the turn changes among speakers throughout the dialogue, therefore treating it no different than non-interactive written text. In this paper, we propose to integrate the turn changes in conversations among speakers when modeling DAs. Specifically, we learn conversation-invariant speaker turn... | Zihao He, Leili Tavabi, Kristina Lerman, Mohammad Soleymani |  |
| 230 |  |  [Unsupervised Domain Adaptation Method with Semantic-Structural Alignment for Dependency Parsing](https://doi.org/10.18653/v1/2021.findings-emnlp.186) |  | 0 | Unsupervised cross-domain dependency parsing is to accomplish domain adaptation for dependency parsing without using labeled data in target domain. Existing methods are often of the pseudo-annotation type, which generates data through self-annotation of the base model and performing iterative training. However, these methods fail to consider the change of model structure for domain adaptation. In addition, the structural information contained in the text cannot be fully exploited. To remedy... | Boda Lin, Mingzheng Li, Si Li, Yong Luo |  |
| 231 |  |  [Devil's Advocate: Novel Boosting Ensemble Method from Psychological Findings for Text Classification](https://doi.org/10.18653/v1/2021.findings-emnlp.187) |  | 0 | We present a new form of ensemble method–Devil’s Advocate, which uses a deliberately dissenting model to force other submodels within the ensemble to better collaborate. Our method consists of two different training settings: one follows the conventional training process (Norm), and the other is trained by artificially generated labels (DevAdv). After training the models, Norm models are fine-tuned through an additional loss function, which uses the DevAdv model as a constraint. In making a... | Hwiyeol Jo, Jaeseo Lim, ByoungTak Zhang |  |
| 232 |  |  [SideControl: Controlled Open-domain Dialogue Generation via Additive Side Networks](https://doi.org/10.18653/v1/2021.findings-emnlp.188) |  | 0 | Transformer-based pre-trained language models boost the performance of open-domain dialogue systems. Prior works leverage Transformer-based pre-trained language models to generate texts with desired attributes in two general approaches: (1) gradient-based methods: updating all latent representations of pre-trained models with gradients from attribute models; (2) weighted-decoding methods: re-ranking beam candidates from pre-trained models with attribute functions. However, gradient-based... | Wanyu Du, Yangfeng Ji |  |
| 233 |  |  [Is BERT a Cross-Disciplinary Knowledge Learner? A Surprising Finding of Pre-trained Models' Transferability](https://doi.org/10.18653/v1/2021.findings-emnlp.189) |  | 0 | This paper investigates whether the power of the models pre-trained on text data, such as BERT, can be transferred to general token sequence classification applications. To verify pre-trained models’ transferability, we test the pre-trained models on text classification tasks with meanings of tokens mismatches, and real-world non-text token sequence classification data, including amino acid, DNA, and music. We find that even on non-text data, the models pre-trained on text converge faster,... | WeiTsung Kao, Hungyi Lee |  |
| 234 |  |  [Geo-BERT Pre-training Model for Query Rewriting in POI Search](https://doi.org/10.18653/v1/2021.findings-emnlp.190) |  | 0 | Query Rewriting (QR) is proposed to solve the problem of the word mismatch between queries and documents in Web search. Existing approaches usually model QR with an end-to-end sequence-to-sequence (seq2seq) model. The state-of-the-art Transformer-based models can effectively learn textual semantics from user session logs, but they often ignore users’ geographic location information that is crucial for the Point-of-Interest (POI) search of map services. In this paper, we proposed a pre-training... | Xiao Liu, Juan Hu, Qi Shen, Huan Chen |  |
| 235 |  |  [Leveraging Bidding Graphs for Advertiser-Aware Relevance Modeling in Sponsored Search](https://doi.org/10.18653/v1/2021.findings-emnlp.191) |  | 0 | Recently, sponsored search has become one of the most lucrative channels for marketing. As the fundamental basis of sponsored search, relevance modeling has attracted increasing attention due to the tremendous practical value. Most existing methods solely rely on the query-keyword pairs. However, keywords are usually short texts with scarce semantic information, which may not precisely reflect the underlying advertising intents. In this paper, we investigate the novel problem of... | Shuxian Bi, Chaozhuo Li, Xiao Han, Zheng Liu, Xing Xie, Haizhen Huang, Zengxuan Wen |  |
| 236 |  |  [GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation](https://doi.org/10.18653/v1/2021.findings-emnlp.192) |  | 0 | Large-scale language models such as GPT-3 are excellent few-shot learners, allowing them to be controlled via natural text prompts. Recent studies report that prompt-based direct classification eliminates the need for fine-tuning but lacks data and inference scalability. This paper proposes a novel data augmentation technique that leverages large-scale language models to generate realistic text samples from a mixture of real samples. We also propose utilizing soft-labels predicted by the... | Kang Min Yoo, Dongju Park, Jaewook Kang, SangWoo Lee, WooMyoung Park |  |
| 237 |  |  [Context-aware Entity Typing in Knowledge Graphs](https://doi.org/10.18653/v1/2021.findings-emnlp.193) |  | 0 | Knowledge graph entity typing aims to infer entities’ missing types in knowledge graphs which is an important but under-explored issue. This paper proposes a novel method for this task by utilizing entities’ contextual information. Specifically, we design two inference mechanisms: i) N2T: independently use each neighbor of an entity to infer its type; ii) Agg2T: aggregate the neighbors of an entity to infer its type. Those mechanisms will produce multiple inference results, and an exponentially... | Weiran Pan, Wei Wei, XianLing Mao |  |
| 238 |  |  [Attribute Alignment: Controlling Text Generation from Pre-trained Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.194) |  | 0 | Large language models benefit from training with a large amount of unlabeled text, which gives them increasingly fluent and diverse generation capabilities. However, using these models for text generation that takes into account target attributes, such as sentiment polarity or specific topics, remains a challenge. We propose a simple and flexible method for controlling text generation by aligning disentangled attribute representations. In contrast to recent efforts on training a discriminator... | Dian Yu, Zhou Yu, Kenji Sagae |  |
| 239 |  |  [Generate & Rank: A Multi-task Framework for Math Word Problems](https://doi.org/10.18653/v1/2021.findings-emnlp.195) |  | 0 | Math word problem (MWP) is a challenging and critical task in natural language processing. Many recent studies formalize MWP as a generation task and have adopted sequence-to-sequence models to transform problem descriptions to mathematical expressions. However, mathematical expressions are prone to minor mistakes while the generation objective does not explicitly handle such mistakes. To address this limitation, we devise a new ranking task for MWP and propose Generate & Rank, a multi-task... | Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, Qun Liu |  |
| 240 |  |  [MIRTT: Learning Multimodal Interaction Representations from Trilinear Transformers for Visual Question Answering](https://doi.org/10.18653/v1/2021.findings-emnlp.196) |  | 0 | In Visual Question Answering (VQA), existing bilinear methods focus on the interaction between images and questions. As a result, the answers are either spliced into the questions or utilized as labels only for classification. On the other hand, trilinear models such as the CTI model efficiently utilize the inter-modality information between answers, questions, and images, while ignoring intra-modality information. Inspired by this observation, we propose a new trilinear interaction framework... | Junjie Wang, Yatai Ji, Jiaqi Sun, Yujiu Yang, Tetsuya Sakai |  |
| 241 |  |  [UniteD-SRL: A Unified Dataset for Span- and Dependency-Based Multilingual and Cross-Lingual Semantic Role Labeling](https://doi.org/10.18653/v1/2021.findings-emnlp.197) |  | 0 | Multilingual and cross-lingual Semantic Role Labeling (SRL) have recently garnered increasing attention as multilingual text representation techniques have become more effective and widely available. While recent work has attained growing success, results on gold multilingual benchmarks are still not easily comparable across languages, making it difficult to grasp where we stand. For example, in CoNLL-2009, the standard benchmark for multilingual SRL, language-to-language comparisons are... | Rocco Tripodi, Simone Conia, Roberto Navigli |  |
| 242 |  |  [Enhancing Dual-Encoders with Question and Answer Cross-Embeddings for Answer Retrieval](https://doi.org/10.18653/v1/2021.findings-emnlp.198) |  | 0 | Dual-Encoders is a promising mechanism for answer retrieval in question answering (QA) systems. Currently most conventional Dual-Encoders learn the semantic representations of questions and answers merely through matching score. Researchers proposed to introduce the QA interaction features in scoring function but at the cost of low efficiency in inference stage. To keep independent encoding of questions and answers during inference stage, variational auto-encoder is further introduced to... | Yanmeng Wang, Jun Bai, Ye Wang, Jianfei Zhang, Wenge Rong, Zongcheng Ji, Shaojun Wang, Jing Xiao |  |
| 243 |  |  [A Neural Graph-based Local Coherence Model](https://doi.org/10.18653/v1/2021.findings-emnlp.199) |  | 0 | Entity grids and entity graphs are two frameworks for modeling local coherence. These frameworks represent entity relations between sentences and then extract features from such representations to encode coherence. The benefits of convolutional neural models for extracting informative features from entity grids have been recently studied. In this work, we study the benefits of Relational Graph Convolutional Networks (RGCN) to encode entity graphs for measuring local coherence. We evaluate our... | Mohsen Mesgar, Leonardo F. R. Ribeiro, Iryna Gurevych |  |
| 244 |  |  [GiBERT: Enhancing BERT with Linguistic Information using a Lightweight Gated Injection Method](https://doi.org/10.18653/v1/2021.findings-emnlp.200) |  | 0 | Large pre-trained language models such as BERT have been the driving force behind recent improvements across many NLP tasks. However, BERT is only trained to predict missing words – either through masking or next sentence prediction – and has no knowledge of lexical, syntactic or semantic information beyond what it picks up through unsupervised pre-training. We propose a novel method to explicitly inject linguistic information in the form of word embeddings into any layer of a pre-trained BERT.... | Nicole Peinelt, Marek Rei, Maria Liakata |  |
| 245 |  |  [RollingLDA: An Update Algorithm of Latent Dirichlet Allocation to Construct Consistent Time Series from Textual Data](https://doi.org/10.18653/v1/2021.findings-emnlp.201) |  | 0 | We propose a rolling version of the Latent Dirichlet Allocation, called RollingLDA. By a sequential approach, it enables the construction of LDA-based time series of topics that are consistent with previous states of LDA models. After an initial modeling, updates can be computed efficiently, allowing for real-time monitoring and detection of events or structural breaks. For this purpose, we propose suitable similarity measures for topics and provide simulation evidence of superiority over other... | Jonas Rieger, Carsten Jentsch, Jörg Rahnenführer |  |
| 246 |  |  [What If Sentence-hood is Hard to Define: A Case Study in Chinese Reading Comprehension](https://doi.org/10.18653/v1/2021.findings-emnlp.202) |  | 0 | Machine reading comprehension (MRC) is a challenging NLP task for it requires to carefully deal with all linguistic granularities from word, sentence to passage. For extractive MRC, the answer span has been shown mostly determined by key evidence linguistic units, in which it is a sentence in most cases. However, we recently discovered that sentences may not be clearly defined in many languages to different extents, so that this causes so-called location unit ambiguity problem and as a result... | Jiawei Wang, Hai Zhao, Yinggong Zhao, Libin Shen |  |
| 247 |  |  [Refining BERT Embeddings for Document Hashing via Mutual Information Maximization](https://doi.org/10.18653/v1/2021.findings-emnlp.203) |  | 0 | Existing unsupervised document hashing methods are mostly established on generative models. Due to the difficulties of capturing long dependency structures, these methods rarely model the raw documents directly, but instead to model the features extracted from them (e.g. bag-of-words (BOG), TFIDF). In this paper, we propose to learn hash codes from BERT embeddings after observing their tremendous successes on downstream tasks. As a first try, we modify existing generative hashing models to... | Zijing Ou, Qinliang Su, Jianxing Yu, Ruihui Zhao, Yefeng Zheng, Bang Liu |  |
| 248 |  |  [REBEL: Relation Extraction By End-to-end Language generation](https://doi.org/10.18653/v1/2021.findings-emnlp.204) |  | 0 | Extracting relation triplets from raw text is a crucial task in Information Extraction, enabling multiple applications such as populating or validating knowledge bases, factchecking, and other downstream tasks. However, it usually involves multiple-step pipelines that propagate errors or are limited to a small number of relation types. To overcome these issues, we propose the use of autoregressive seq2seq models. Such models have previously been shown to perform well not only in language... | PereLluís Huguet Cabot, Roberto Navigli |  |
| 249 |  |  [Wine is not v i n. On the Compatibility of Tokenizations across Languages](https://doi.org/10.18653/v1/2021.findings-emnlp.205) |  | 0 | The size of the vocabulary is a central design choice in large pretrained language models, with respect to both performance and memory requirements. Typically, subword tokenization algorithms such as byte pair encoding and WordPiece are used. In this work, we investigate the compatibility of tokenizations for multilingual static and contextualized embedding spaces and propose a measure that reflects the compatibility of tokenizations across languages. Our goal is to prevent incompatible... | Antonis Maronikolakis, Philipp Dufter, Hinrich Schütze |  |
| 250 |  |  [Temporal Adaptation of BERT and Performance on Downstream Document Classification: Insights from Social Media](https://doi.org/10.18653/v1/2021.findings-emnlp.206) |  | 0 | Language use differs between domains and even within a domain, language use changes over time. For pre-trained language models like BERT, domain adaptation through continued pre-training has been shown to improve performance on in-domain downstream tasks. In this article, we investigate whether temporal adaptation can bring additional benefits. For this purpose, we introduce a corpus of social media comments sampled over three years. It contains unlabelled data for adaptation and evaluation on... | Paul Röttger, Janet B. Pierrehumbert |  |
| 251 |  |  [Skim-Attention: Learning to Focus via Document Layout](https://doi.org/10.18653/v1/2021.findings-emnlp.207) |  | 0 | Transformer-based pre-training techniques of text and layout have proven effective in a number of document understanding tasks. Despite this success, multimodal pre-training models suffer from very high computational and memory costs. Motivated by human reading strategies, this paper presents Skim-Attention, a new attention mechanism that takes advantage of the structure of the document and its layout. Skim-Attention only attends to the 2-dimensional position of the words in a document. Our... | Laura Nguyen, Thomas Scialom, Jacopo Staiano, Benjamin Piwowarski |  |
| 252 |  |  [Attention-based Contrastive Learning for Winograd Schemas](https://doi.org/10.18653/v1/2021.findings-emnlp.208) |  | 0 | Self-supervised learning has recently attracted considerable attention in the NLP community for its ability to learn discriminative features using a contrastive objective. This paper investigates whether contrastive learning can be extended to Transfomer attention to tackling the Winograd Schema Challenge. To this end, we propose a novel self-supervised framework, leveraging a contrastive loss directly at the level of self-attention. Experimental analysis of our attention-based models on... | Tassilo Klein, Moin Nabi |  |
| 253 |  |  [Give the Truth: Incorporate Semantic Slot into Abstractive Dialogue Summarization](https://doi.org/10.18653/v1/2021.findings-emnlp.209) |  | 0 | Abstractive dialogue summarization suffers from a lots of factual errors, which are due to scattered salient elements in the multi-speaker information interaction process. In this work, we design a heterogeneous semantic slot graph with a slot-level mask cross-attention to enhance the slot features for more correct summarization. We also propose a slot-driven beam search algorithm in the decoding process to give priority to generating salient elements in a limited length by... | Lulu Zhao, Weihao Zeng, Weiran Xu, Jun Guo |  |
| 254 |  |  [Challenges in Detoxifying Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.210) |  | 0 | Large language models (LM) generate remarkably fluent text and can be efficiently adapted across NLP tasks. Measuring and guaranteeing the quality of generated text in terms of safety is imperative for deploying LMs in the real world; to this end, prior work often relies on automatic evaluation of LM toxicity. We critically discuss this approach, evaluate several toxicity mitigation strategies with respect to both automatic and human evaluation, and analyze consequences of toxicity mitigation... | Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, PoSen Huang |  |
| 255 |  |  [Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.211) |  | 0 | Recent works have found evidence of gender bias in models of machine translation and coreference resolution using mostly synthetic diagnostic datasets. While these quantify bias in a controlled experiment, they often do so on a small scale and consist mostly of artificial, out-of-distribution sentences. In this work, we find grammatical patterns indicating stereotypical and non-stereotypical gender-role assignments (e.g., female nurses versus male dancers) in corpora from three domains,... | Shahar Levy, Koren Lazar, Gabriel Stanovsky |  |
| 256 |  |  [Competence-based Curriculum Learning for Multilingual Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.212) |  | 0 | Currently, multilingual machine translation is receiving more and more attention since it brings better performance for low resource languages (LRLs) and saves more space. However, existing multilingual machine translation models face a severe challenge: imbalance. As a result, the translation performance of different languages in multilingual translation models are quite different. We argue that this imbalance problem stems from the different learning competencies of different languages.... | Mingliang Zhang, Fandong Meng, Yunhai Tong, Jie Zhou |  |
| 257 |  |  [Informed Sampling for Diversity in Concept-to-Text NLG](https://doi.org/10.18653/v1/2021.findings-emnlp.213) |  | 0 | Deep-learning models for language generation tasks tend to produce repetitive output. Various methods have been proposed to encourage lexical diversity during decoding, but this often comes at a cost to the perceived fluency and adequacy of the output. In this work, we propose to ameliorate this cost by using an Imitation Learning approach to explore the level of diversity that a language generation model can reliably produce. Specifically, we augment the decoding process with a meta-classifier... | Giulio Zhou, Gerasimos Lampouras |  |
| 258 |  |  [Novel Natural Language Summarization of Program Code via Leveraging Multiple Input Representations](https://doi.org/10.18653/v1/2021.findings-emnlp.214) |  | 0 | The lack of description of a given program code acts as a big hurdle to those developers new to the code base for its understanding. To tackle this problem, previous work on code summarization, the task of automatically generating code description given a piece of code reported that an auxiliary learning model trained to produce API (Application Programming Interface) embeddings showed promising results when applied to a downstream, code summarization model. However, different codes having... | Fuxiang Chen, Mijung Kim, Jaegul Choo |  |
| 259 |  |  [WikiNEuRal: Combined Neural and Knowledge-based Silver Data Creation for Multilingual NER](https://doi.org/10.18653/v1/2021.findings-emnlp.215) |  | 0 | Multilingual Named Entity Recognition (NER) is a key intermediate task which is needed in many areas of NLP. In this paper, we address the well-known issue of data scarcity in NER, especially relevant when moving to a multilingual scenario, and go beyond current approaches to the creation of multilingual silver data for the task. We exploit the texts of Wikipedia and introduce a new methodology based on the effective combination of knowledge-based approaches and neural models, together with a... | Simone Tedeschi, Valentino Maiorca, Niccolò Campolungo, Francesco Cecconi, Roberto Navigli |  |
| 260 |  |  [Beyond Grammatical Error Correction: Improving L1-influenced research writing in English using pre-trained encoder-decoder models](https://doi.org/10.18653/v1/2021.findings-emnlp.216) |  | 0 | In this paper, we present a new method for training a writing improvement model adapted to the writer’s first language (L1) that goes beyond grammatical error correction (GEC). Without using annotated training data, we rely solely on pre-trained language models fine-tuned with parallel corpora of reference translation aligned with machine translation. We evaluate our model with corpora of academic papers written in English by L1 Portuguese and L1 Spanish scholars and a reference corpus of... | Gustavo Zomer, Ana FrankenbergGarcia |  |
| 261 |  |  [Classification and Geotemporal Analysis of Quality-of-Life Issues in Tenant Reviews](https://doi.org/10.18653/v1/2021.findings-emnlp.217) |  | 0 | Online tenant reviews of multifamily residential properties present a unique source of information for commercial real estate investing and research. Real estate professionals frequently read tenant reviews to uncover property-related issues that are otherwise difficult to detect, a process that is both biased and time-consuming. Using this as motivation, we asked whether a text classification-based approach can automate the detection of four carefully defined, major quality-of-life issues:... | Adam Haber, Zeev Waks |  |
| 262 |  |  [Probing Pre-trained Language Models for Semantic Attributes and their Values](https://doi.org/10.18653/v1/2021.findings-emnlp.218) |  | 0 | Pretrained language models (PTLMs) yield state-of-the-art performance on many natural language processing tasks, including syntax, semantics and commonsense. In this paper, we focus on identifying to what extent do PTLMs capture semantic attributes and their values, e.g., the correlation between rich and high net worth. We use PTLMs to predict masked tokens using patterns and lists of items from Wikidata in order to verify how likely PTLMs encode semantic attributes along with their values.... | Meriem Beloucif, Chris Biemann |  |
| 263 |  |  [Uncovering the Limits of Text-based Emotion Detection](https://doi.org/10.18653/v1/2021.findings-emnlp.219) |  | 0 | Identifying emotions from text is crucial for a variety of real world tasks. We consider the two largest now-available corpora for emotion classification: GoEmotions, with 58k messages labelled by readers, and Vent, with 33M writer-labelled messages. We design a benchmark and evaluate several feature spaces and learning algorithms, including two simple yet novel models on top of BERT that outperform previous strong baselines on GoEmotions. Through an experiment with human participants, we also... | Nurudin AlvarezGonzalez, Andreas Kaltenbrunner, Vicenç Gómez |  |
| 264 |  |  [Named Entity Recognition for Entity Linking: What Works and What's Next](https://doi.org/10.18653/v1/2021.findings-emnlp.220) |  | 0 | Entity Linking (EL) systems have achieved impressive results on standard benchmarks mainly thanks to the contextualized representations provided by recent pretrained language models. However, such systems still require massive amounts of data – millions of labeled examples – to perform at their best, with training times that often exceed several days, especially when limited computational resources are available. In this paper, we look at how Named Entity Recognition (NER) can be exploited to... | Simone Tedeschi, Simone Conia, Francesco Cecconi, Roberto Navigli |  |
| 265 |  |  [Learning Numeracy: A Simple Yet Effective Number Embedding Approach Using Knowledge Graph](https://doi.org/10.18653/v1/2021.findings-emnlp.221) |  | 0 | Numeracy plays a key role in natural language understanding. However, existing NLP approaches, not only traditional word2vec approach or contextualized transformer-based language models, fail to learn numeracy. As the result, the performance of these models is limited when they are applied to number-intensive applications in clinical and financial domains. In this work, we propose a simple number embedding approach based on knowledge graph. We construct a knowledge graph consisting of number... | Hanyu Duan, Yi Yang, Kar Yan Tam |  |
| 266 |  |  [Weakly Supervised Semantic Parsing by Learning from Mistakes](https://doi.org/10.18653/v1/2021.findings-emnlp.222) |  | 0 | Weakly supervised semantic parsing (WSP) aims at training a parser via utterance-denotation pairs. This task is challenging because it requires (1) searching consistent logical forms in a huge space; and (2) dealing with spurious logical forms. In this work, we propose Learning from Mistakes (LFM), a simple yet effective learning framework for WSP. LFM utilizes the mistakes made by a parser during searching, i.e., generating logical forms that do not execute to correct denotations, for tackling... | Jiaqi Guo, JianGuang Lou, Ting Liu, Dongmei Zhang |  |
| 267 |  |  [CodeQA: A Question Answering Dataset for Source Code Comprehension](https://doi.org/10.18653/v1/2021.findings-emnlp.223) |  | 0 | We propose CodeQA, a free-form question answering dataset for the purpose of source code comprehension: given a code snippet and a question, a textual answer is required to be generated. CodeQA contains a Java dataset with 119,778 question-answer pairs and a Python dataset with 70,085 question-answer pairs. To obtain natural and faithful questions and answers, we implement syntactic rules and semantic analysis to transform code comments into question-answer pairs. We present the construction... | Chenxiao Liu, Xiaojun Wan |  |
| 268 |  |  [Subword Mapping and Anchoring across Languages](https://doi.org/10.18653/v1/2021.findings-emnlp.224) |  | 0 | State-of-the-art multilingual systems rely on shared vocabularies that sufficiently cover all considered languages. To this end, a simple and frequently used approach makes use of subword vocabularies constructed jointly over several languages. We hypothesize that such vocabularies are suboptimal due to false positives (identical subwords with different meanings across languages) and false negatives (different subwords with similar meanings). To address these issues, we propose Subword Mapping... | Giorgos Vernikos, Andrei PopescuBelis |  |
| 269 |  |  [CDLM: Cross-Document Language Modeling](https://doi.org/10.18653/v1/2021.findings-emnlp.225) |  | 0 | We introduce a new pretraining approach geared for multi-document language modeling, incorporating two key ideas into the masked language modeling self-supervised objective. First, instead of considering documents in isolation, we pretrain over sets of multiple related documents, encouraging the model to learn cross-document relationships. Second, we improve over recent long-range transformers by introducing dynamic global attention that has access to the entire input to predict masked tokens.... | Avi Caciularu, Arman Cohan, Iz Beltagy, Matthew E. Peters, Arie Cattan, Ido Dagan |  |
| 270 |  |  [Patterns of Polysemy and Homonymy in Contextualised Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.226) |  | 0 | One of the central aspects of contextualised language models is that they should be able to distinguish the meaning of lexically ambiguous words by their contexts. In this paper we investigate the extent to which the contextualised embeddings of word forms that display multiplicity of sense reflect traditional distinctions of polysemy and homonymy. To this end, we introduce an extended, human-annotated dataset of graded word sense similarity and co-predication acceptability, and evaluate how... | Janosch Haber, Massimo Poesio |  |
| 271 |  |  [Cross-Lingual Leveled Reading Based on Language-Invariant Features](https://doi.org/10.18653/v1/2021.findings-emnlp.227) |  | 0 | Leveled reading (LR) aims to automatically classify texts by the cognitive levels of readers, which is fundamental in providing appropriate reading materials regarding different reading capabilities. However, most state-of-the-art LR methods rely on the availability of copious annotated resources, which prevents their adaptation to low-resource languages like Chinese. In our work, to tackle LR in Chinese, we explore how different language transfer methods perform on English-Chinese LR.... | Simin Rao, Hua Zheng, Sujian Li |  |
| 272 |  |  [Controlled Neural Sentence-Level Reframing of News Articles](https://doi.org/10.18653/v1/2021.findings-emnlp.228) |  | 0 | Framing a news article means to portray the reported event from a specific perspective, e.g., from an economic or a health perspective. Reframing means to change this perspective. Depending on the audience or the submessage, reframing can become necessary to achieve the desired effect on the readers. Reframing is related to adapting style and sentiment, which can be tackled with neural text generation techniques. However, it is more challenging since changing a frame requires rewriting entire... | WeiFan Chen, Khalid Al Khatib, Benno Stein, Henning Wachsmuth |  |
| 273 |  |  [DialogueTRM: Exploring Multi-Modal Emotional Dynamics in a Conversation](https://doi.org/10.18653/v1/2021.findings-emnlp.229) |  | 0 | Emotion dynamics formulates principles explaining the emotional fluctuation during conversations. Recent studies explore the emotion dynamics from the self and inter-personal dependencies, however, ignoring the temporal and spatial dependencies in the situation of multi-modal conversations. To address the issue, we extend the concept of emotion dynamics to multi-modal settings and propose a Dialogue Transformer for simultaneously modeling the intra-modal and inter-modal emotion dynamics.... | Yuzhao Mao, Guang Liu, Xiaojie Wang, Weiguo Gao, Xuan Li |  |
| 274 |  |  [Adversarial Examples for Evaluating Math Word Problem Solvers](https://doi.org/10.18653/v1/2021.findings-emnlp.230) |  | 0 | Standard accuracy metrics have shown that Math Word Problem (MWP) solvers have achieved high performance on benchmark datasets. However, the extent to which existing MWP solvers truly understand language and its relation with numbers is still unclear. In this paper, we generate adversarial attacks to evaluate the robustness of state-of-the-art MWP solvers. We propose two methods, Question Reordering and Sentence Paraphrasing to generate adversarial attacks. We conduct experiments across three... | Vivek Kumar, Rishabh Maheshwary, Vikram Pudi |  |
| 275 |  |  [Improving Numerical Reasoning Skills in the Modular Approach for Complex Question Answering on Text](https://doi.org/10.18653/v1/2021.findings-emnlp.231) |  | 0 | Numerical reasoning skills are essential for complex question answering (CQA) over text. It requires opertaions including counting, comparison, addition and subtraction. A successful approach to CQA on text, Neural Module Networks (NMNs), follows the programmer-interpreter paradigm and leverages specialised modules to perform compositional reasoning. However, the NMNs framework does not consider the relationship between numbers and entities in both questions and paragraphs. We propose effective... | Xiaoyu Guo, YuanFang Li, Gholamreza Haffari |  |
| 276 |  |  [Retrieval Augmented Code Generation and Summarization](https://doi.org/10.18653/v1/2021.findings-emnlp.232) |  | 0 | Software developers write a lot of source code and documentation during software development. Intrinsically, developers often recall parts of source code or code summaries that they had written in the past while implementing software or documenting them. To mimic developers’ code or summary generation behavior, we propose a retrieval augmented framework, REDCODER, that retrieves relevant code or summaries from a retrieval database and provides them as a supplement to code generation or... | Md. Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, KaiWei Chang |  |
| 277 |  |  [Multilingual Translation via Grafting Pre-trained Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.233) |  | 0 | Can pre-trained BERT for one language and GPT for another be glued together to translate texts? Self-supervised training using only monolingual data has led to the success of pre-trained (masked) language models in many NLP tasks. However, directly connecting BERT as an encoder and GPT as a decoder can be challenging in machine translation, for GPT-like models lack a cross-attention component that is needed in seq2seq decoders. In this paper, we propose Graformer to graft separately pre-trained... | Zewei Sun, Mingxuan Wang, Lei Li |  |
| 278 |  |  [AEDA: An Easier Data Augmentation Technique for Text Classification](https://doi.org/10.18653/v1/2021.findings-emnlp.234) |  | 0 | This paper proposes AEDA (An Easier Data Augmentation) technique to help improve the performance on text classification tasks. AEDA includes only random insertion of punctuation marks into the original text. This is an easier technique to implement for data augmentation than EDA method (Wei and Zou, 2019) with which we compare our results. In addition, it keeps the order of the words while changing their positions in the sentence leading to a better generalized performance. Furthermore, the... | Akbar Karimi, Leonardo Rossi, Andrea Prati |  |
| 279 |  |  [A Comprehensive Comparison of Word Embeddings in Event & Entity Coreference Resolution](https://doi.org/10.18653/v1/2021.findings-emnlp.235) |  | 0 | Coreference Resolution is an important NLP task and most state-of-the-art methods rely on word embeddings for word representation. However, one issue that has been largely overlooked in literature is that of comparing the performance of different embeddings across and within families. Therefore, we frame our study in the context of Event and Entity Coreference Resolution (EvCR & EnCR), and address two questions : 1) Is there a trade-off between performance (predictive and run-time) and... | Judicael Poumay, Ashwin Ittoo |  |
| 280 |  |  [Wav-BERT: Cooperative Acoustic and Linguistic Representation Learning for Low-Resource Speech Recognition](https://doi.org/10.18653/v1/2021.findings-emnlp.236) |  | 0 | Unifying acoustic and linguistic representation learning has become increasingly crucial to transfer the knowledge learned on the abundance of high-resource language data for low-resource speech recognition. Existing approaches simply cascade pre-trained acoustic and language models to learn the transfer from speech to text. However, how to solve the representation discrepancy of speech and text is unexplored, which hinders the utilization of acoustic and linguistic information. Moreover,... | Guolin Zheng, Yubei Xiao, Ke Gong, Pan Zhou, Xiaodan Liang, Liang Lin |  |
| 281 |  |  [Multilingual AMR Parsing with Noisy Knowledge Distillation](https://doi.org/10.18653/v1/2021.findings-emnlp.237) |  | 0 | We study multilingual AMR parsing from the perspective of knowledge distillation, where the aim is to learn and improve a multilingual AMR parser by using an existing English parser as its teacher. We constrain our exploration in a strict multilingual setting: there is but one model to parse all different languages including English. We identify that noisy input and precise output are the key to successful distillation. Together with extensive pre-training, we obtain an AMR parser whose... | Deng Cai, Xin Li, Jackie ChunSing Ho, Lidong Bing, Wai Lam |  |
| 282 |  |  [Open-Domain Contextual Link Prediction and its Complementarity with Entailment Graphs](https://doi.org/10.18653/v1/2021.findings-emnlp.238) |  | 0 | An open-domain knowledge graph (KG) has entities as nodes and natural language relations as edges, and is constructed by extracting (subject, relation, object) triples from text. The task of open-domain link prediction is to infer missing relations in the KG. Previous work has used standard link prediction for the task. Since triples are extracted from text, we can ground them in the larger textual context in which they were originally found. However, standard link prediction methods only rely... | Mohammad Javad Hosseini, Shay B. Cohen, Mark Johnson, Mark Steedman |  |
| 283 |  |  [Analysis of Language Change in Collaborative Instruction Following](https://doi.org/10.18653/v1/2021.findings-emnlp.239) |  | 0 | We analyze language change over time in a collaborative, goal-oriented instructional task, where utility-maximizing participants form conventions and increase their expertise. Prior work studied such scenarios mostly in the context of reference games, and consistently found that language complexity is reduced along multiple dimensions, such as utterance length, as conventions are formed. In contrast, we find that, given the ability to increase instruction utility, instructors increase language... | Anna Effenberger, Rhia Singh, Eva Yan, Alane Suhr, Yoav Artzi |  |
| 284 |  |  [Counter-Interference Adapter for Multilingual Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.240) |  | 0 | Developing a unified multilingual model has been a long pursuing goal for machine translation. However, existing approaches suffer from performance degradation - a single multilingual model is inferior to separately trained bilingual ones on rich-resource languages. We conjecture that such a phenomenon is due to interference brought by joint training with multiple languages. To accommodate the issue, we propose CIAT, an adapted Transformer model with a small parameter overhead for multilingual... | Yaoming Zhu, Jiangtao Feng, Chengqi Zhao, Mingxuan Wang, Lei Li |  |
| 285 |  |  [Progressive Transformer-Based Generation of Radiology Reports](https://doi.org/10.18653/v1/2021.findings-emnlp.241) |  | 0 | Inspired by Curriculum Learning, we propose a consecutive (i.e., image-to-text-to-text) generation framework where we divide the problem of radiology report generation into two steps. Contrary to generating the full radiology report from the image at once, the model generates global concepts from the image in the first step and then reforms them into finer and coherent texts using transformer-based architecture. We follow the transformer-based sequence-to-sequence paradigm at each step. We... | Farhad Nooralahzadeh, Nicolas Perez Gonzalez, Thomas Frauenfelder, Koji Fujimoto, Michael Krauthammer |  |
| 286 |  |  ["Be nice to your wife! The restaurants are closed": Can Gender Stereotype Detection Improve Sexism Classification?](https://doi.org/10.18653/v1/2021.findings-emnlp.242) |  | 0 | In this paper, we focus on the detection of sexist hate speech against women in tweets studying for the first time the impact of gender stereotype detection on sexism classification. We propose: (1) the first dataset annotated for gender stereotype detection, (2) a new method for data augmentation based on sentence similarity with multilingual external datasets, and (3) a set of deep learning experiments first to detect gender stereotypes and then, to use this auxiliary task for sexism... | Patricia Chiril, Farah Benamara, Véronique Moriceau |  |
| 287 |  |  [Automatic Discrimination between Inherited and Borrowed Latin Words in Romance Languages](https://doi.org/10.18653/v1/2021.findings-emnlp.243) |  | 0 | In this paper, we address the problem of automatically discriminating between inherited and borrowed Latin words. We introduce a new dataset and investigate the case of Romance languages (Romanian, Italian, French, Spanish, Portuguese and Catalan), where words directly inherited from Latin coexist with words borrowed from Latin, and explore whether automatic discrimination between them is possible. Having entered the language at a later stage, borrowed words are no longer subject to historical... | Alina Maria Cristea, Liviu P. Dinu, Simona Georgescu, MihneaLucian Mihai, Ana Sabina Uban |  |
| 288 |  |  [Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections](https://doi.org/10.18653/v1/2021.findings-emnlp.244) |  | 0 | Large pre-trained language models (LMs) such as GPT-3 have acquired a surprising ability to perform zero-shot learning. For example, to classify sentiment without any training examples, we can “prompt” the LM with the review and the label description “Does the user like this movie?”, and ask whether the next word is “yes” or “no”. However, the next word prediction training objective is still misaligned with the target zero-shot learning objective. To address this weakness, we propose... | Ruiqi Zhong, Kristy Lee, Zheng Zhang, Dan Klein |  |
| 289 |  |  [Knowledge-Interactive Network with Sentiment Polarity Intensity-Aware Multi-Task Learning for Emotion Recognition in Conversations](https://doi.org/10.18653/v1/2021.findings-emnlp.245) |  | 0 | Emotion Recognition in Conversation (ERC) has gained much attention from the NLP community recently. Some models concentrate on leveraging commonsense knowledge or multi-task learning to help complicated emotional reasoning. However, these models neglect direct utterance-knowledge interaction. In addition, these models utilize emotion-indirect auxiliary tasks, which provide limited affective information for the ERC task. To address the above issues, we propose a Knowledge-Interactive Network... | Yunhe Xie, Kailai Yang, Chengjie Sun, Bingquan Liu, Zhenzhou Ji |  |
| 290 |  |  [Minimizing Annotation Effort via Max-Volume Spectral Sampling](https://doi.org/10.18653/v1/2021.findings-emnlp.246) |  | 0 | We address the annotation data bottleneck for sequence classification. Specifically we ask the question: if one has a budget of N annotations, which samples should we select for annotation? The solution we propose looks for diversity in the selected sample, by maximizing the amount of information that is useful for the learning algorithm, or equivalently by minimizing the redundancy of samples in the selection. This is formulated in the context of spectral learning of recurrent functions for... | Ariadna Quattoni, Xavier Carreras |  |
| 291 |  |  [On the Complementarity between Pre-Training and Back-Translation for Neural Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.247) |  | 0 | Pre-training (PT) and back-translation (BT) are two simple and powerful methods to utilize monolingual data for improving the model performance of neural machine translation (NMT). This paper takes the first step to investigate the complementarity between PT and BT. We introduce two probing tasks for PT and BT respectively and find that PT mainly contributes to the encoder module while BT brings more benefits to the decoder. Experimental results show that PT and BT are nicely complementary to... | Xuebo Liu, Longyue Wang, Derek F. Wong, Liang Ding, Lidia S. Chao, Shuming Shi, Zhaopeng Tu |  |
| 292 |  |  [Lexicon-Based Graph Convolutional Network for Chinese Word Segmentation](https://doi.org/10.18653/v1/2021.findings-emnlp.248) |  | 0 | Precise information of word boundary can alleviate the problem of lexical ambiguity to improve the performance of natural language processing (NLP) tasks. Thus, Chinese word segmentation (CWS) is a fundamental task in NLP. Due to the development of pre-trained language models (PLM), pre-trained knowledge can help neural methods solve the main problems of the CWS in significant measure. Existing methods have already achieved high performance on several benchmarks (e.g., Bakeoff-2005). However,... | Kaiyu Huang, Hao Yu, Junpeng Liu, Wei Liu, Jingxiang Cao, Degen Huang |  |
| 293 |  |  [KFCNet: Knowledge Filtering and Contrastive Learning for Generative Commonsense Reasoning](https://doi.org/10.18653/v1/2021.findings-emnlp.249) |  | 0 | Pre-trained language models have led to substantial gains over a broad range of natural language processing (NLP) tasks, but have been shown to have limitations for natural language generation tasks with high-quality requirements on the output, such as commonsense generation and ad keyword generation. In this work, we present a novel Knowledge Filtering and Contrastive learning Network (KFCNet) which references external knowledge and achieves better generation performance. Specifically, we... | Haonan Li, Yeyun Gong, Jian Jiao, Ruofei Zhang, Timothy Baldwin, Nan Duan |  |
| 294 |  |  [Monolingual and Cross-Lingual Acceptability Judgments with the Italian CoLA corpus](https://doi.org/10.18653/v1/2021.findings-emnlp.250) |  | 0 | The development of automated approaches to linguistic acceptability has been greatly fostered by the availability of the English CoLA corpus, which has also been included in the widely used GLUE benchmark. However, this kind of research for languages other than English, as well as the analysis of cross-lingual approaches, has been hindered by the lack of resources with a comparable size in other languages. We have therefore developed the ItaCoLA corpus, containing almost 10,000 sentences with... | Daniela Trotta, Raffaele Guarasci, Elisa Leonardelli, Sara Tonelli |  |
| 295 |  |  [Hyperbolic Hierarchy-Aware Knowledge Graph Embedding for Link Prediction](https://doi.org/10.18653/v1/2021.findings-emnlp.251) |  | 0 | Knowledge graph embedding (KGE) using low-dimensional representations to predict missing information is widely applied in knowledge completion. Existing embedding methods are mostly built on Euclidean space, which are difficult to handle hierarchical structures. Hyperbolic embedding methods have shown the promise of high fidelity and concise representation for hierarchical data. However, the logical patterns in knowledge graphs are not considered well in these methods. To address this problem,... | Zhe Pan, Peng Wang |  |
| 296 |  |  [A Discourse-Aware Graph Neural Network for Emotion Recognition in Multi-Party Conversation](https://doi.org/10.18653/v1/2021.findings-emnlp.252) |  | 0 | Emotion recognition in multi-party conversation (ERMC) is becoming increasingly popular as an emerging research topic in natural language processing. Prior research focuses on exploring sequential information but ignores the discourse structures of conversations. In this paper, we investigate the importance of discourse structures in handling informative contextual cues and speaker-specific features for ERMC. To this end, we propose a discourse-aware graph neural network (ERMC-DisGCN) for ERMC.... | Yang Sun, Nan Yu, Guohong Fu |  |
| 297 |  |  [MeLT: Message-Level Transformer with Masked Document Representations as Pre-Training for Stance Detection](https://doi.org/10.18653/v1/2021.findings-emnlp.253) |  | 0 | Much of natural language processing is focused on leveraging large capacity language models, typically trained over single messages with a task of predicting one or more tokens. However, modeling human language at higher-levels of context (i.e., sequences of messages) is under-explored. In stance detection and other social media tasks where the goal is to predict an attribute of a message, we have contextual data that is loosely semantically connected by authorship. Here, we introduce... | Matthew Matero, Nikita Soni, Niranjan Balasubramanian, H. Andrew Schwartz |  |
| 298 |  |  [LMSOC: An Approach for Socially Sensitive Pretraining](https://doi.org/10.18653/v1/2021.findings-emnlp.254) |  | 0 | While large-scale pretrained language models have been shown to learn effective linguistic representations for many NLP tasks, there remain many real-world contextual aspects of language that current approaches do not capture. For instance, consider a cloze test “I enjoyed the _____ game this weekend”: the correct answer depends heavily on where the speaker is from, when the utterance occurred, and the speaker’s broader social milieu and preferences. Although language depends heavily on the... | Vivek Kulkarni, Shubhanshu Mishra, Aria Haghighi |  |
| 299 |  |  [Extract, Integrate, Compete: Towards Verification Style Reading Comprehension](https://doi.org/10.18653/v1/2021.findings-emnlp.255) |  | 0 | In this paper, we present a new verification style reading comprehension dataset named VGaokao from Chinese Language tests of Gaokao. Different from existing efforts, the new dataset is originally designed for native speakers’ evaluation, thus requiring more advanced language understanding skills. To address the challenges in VGaokao, we propose a novel Extract-Integrate-Compete approach, which iteratively selects complementary evidence with a novel query updating mechanism and adaptively... | Chen Zhang, Yuxuan Lai, Yansong Feng, Dongyan Zhao |  |
| 300 |  |  [Comparing learnability of two dependency schemes: 'semantic' (UD) and 'syntactic' (SUD)](https://doi.org/10.18653/v1/2021.findings-emnlp.256) |  | 0 | This paper contributes to the thread of research on the learnability of different dependency annotation schemes: one (‘semantic’) favouring content words as heads of dependency relations and the other (‘syntactic’) favouring syntactic heads. Several studies have lent support to the idea that choosing syntactic criteria for assigning heads in dependency trees improves the performance of dependency parsers. This may be explained by postulating that syntactic approaches are generally more... | Ryszard Tuora, Adam Przepiórkowski, Aleksander Leczkowski |  |
| 301 |  |  [Argumentation-Driven Evidence Association in Criminal Cases](https://doi.org/10.18653/v1/2021.findings-emnlp.257) |  | 0 | Evidence association in criminal cases is dividing a set of judicial evidence into several non-overlapping subsets, improving the interpretability and legality of conviction. Observably, evidence divided into the same subset usually supports the same claim. Therefore, we propose an argumentation-driven supervised learning method to calculate the distance between evidence pairs for the following evidence association step in this paper. Experimental results on a real-world dataset demonstrate the... | Yefei Teng, Wenhan Chao |  |
| 302 |  |  [Eliminating Sentiment Bias for Aspect-Level Sentiment Classification with Unsupervised Opinion Extraction](https://doi.org/10.18653/v1/2021.findings-emnlp.258) |  | 0 | Aspect-level sentiment classification (ALSC) aims at identifying the sentiment polarity of a specified aspect in a sentence. ALSC is a practical setting in aspect-based sentiment analysis due to no opinion term labeling needed, but it fails to interpret why a sentiment polarity is derived for the aspect. To address this problem, recent works fine-tune pre-trained Transformer encoders for ALSC to extract an aspect-centric dependency tree that can locate the opinion words. However, the induced... | Bo Wang, Tao Shen, Guodong Long, Tianyi Zhou, Yi Chang |  |
| 303 |  |  [Data Efficient Masked Language Modeling for Vision and Language](https://doi.org/10.18653/v1/2021.findings-emnlp.259) |  | 0 | Masked language modeling (MLM) is one of the key sub-tasks in vision-language pretraining. In the cross-modal setting, tokens in the sentence are masked at random, and the model predicts the masked tokens given the image and the text. In this paper, we observe several key disadvantages of MLM in this setting. First, as captions tend to be short, in a third of the sentences no token is sampled. Second, the majority of masked tokens are stop-words and punctuation, leading to under-utilization of... | Yonatan Bitton, Michael Elhadad, Gabriel Stanovsky, Roy Schwartz |  |
| 304 |  |  [Improving Multilingual Neural Machine Translation with Auxiliary Source Languages](https://doi.org/10.18653/v1/2021.findings-emnlp.260) |  | 0 | Multilingual neural machine translation models typically handle one source language at a time. However, prior work has shown that translating from multiple source languages improves translation quality. Different from existing approaches on multi-source translation that are limited to the test scenario where parallel source sentences from multiple languages are available at inference time, we propose to improve multilingual translation in a more common scenario by exploiting synthetic source... | Weijia Xu, Yuwei Yin, Shuming Ma, Dongdong Zhang, Haoyang Huang |  |
| 305 |  |  [How Does Fine-tuning Affect the Geometry of Embedding Space: A Case Study on Isotropy](https://doi.org/10.18653/v1/2021.findings-emnlp.261) |  | 0 | It is widely accepted that fine-tuning pre-trained language models usually brings about performance improvements in downstream tasks. However, there are limited studies on the reasons behind this effectiveness, particularly from the viewpoint of structural changes in the embedding space. Trying to fill this gap, in this paper, we analyze the extent to which the isotropy of the embedding space changes after fine-tuning. We demonstrate that, even though isotropy is a desirable geometrical... | Sara Rajaee, Mohammad Taher Pilehvar |  |
| 306 |  |  [Locality Preserving Sentence Encoding](https://doi.org/10.18653/v1/2021.findings-emnlp.262) |  | 0 | Although researches on word embeddings have made great progress in recent years, many tasks in natural language processing are on the sentence level. Thus, it is essential to learn sentence embeddings. Recently, Sentence BERT (SBERT) is proposed to learn embeddings on the sentence level, and it uses the inner product (or, cosine similarity) to compute semantic similarity between sentences. However, this measurement cannot well describe the semantic structures among sentences. The reason is that... | Changrong Min, Yonghe Chu, Liang Yang, Bo Xu, Hongfei Lin |  |
| 307 |  |  [Knowledge Representation Learning with Contrastive Completion Coding](https://doi.org/10.18653/v1/2021.findings-emnlp.263) |  | 0 | Knowledge representation learning (KRL) has been used in plenty of knowledge-driven tasks. Despite fruitfully progress, existing methods still suffer from the immaturity on tackling potentially-imperfect knowledge graphs and highly-imbalanced positive-negative instances during training, both of which would hinder the performance of KRL. In this paper, we propose Contrastive Completion Coding (C3), a novel KRL framework that is composed of two functional components: 1. Hierarchical Architecture,... | Bo Ouyang, Wenbing Huang, Runfa Chen, Zhixing Tan, Yang Liu, Maosong Sun, Jihong Zhu |  |
| 308 |  |  [Knowledge-Enhanced Evidence Retrieval for Counterargument Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.264) |  | 0 | Finding counterevidence to statements is key to many tasks, including counterargument generation. We build a system that, given a statement, retrieves counterevidence from diverse sources on the Web. At the core of this system is a natural language inference (NLI) model that determines whether a candidate sentence is valid counterevidence or not. Most NLI models to date, however, lack proper reasoning abilities necessary to find counterevidence that involves complex inference. Thus, we present... | Yohan Jo, Haneul Yoo, JinYeong Bak, Alice Oh, Chris Reed, Eduard H. Hovy |  |
| 309 |  |  [Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model](https://doi.org/10.18653/v1/2021.findings-emnlp.265) |  | 0 | The transformer-based pre-trained language models have been tremendously successful in most of the conventional NLP tasks. But they often struggle in those tasks where numerical understanding is required. Some possible reasons can be the tokenizers and pre-training objectives which are not specifically designed to learn and preserve numeracy. Here we investigate the ability of text-to-text transfer learning model (T5), which has outperformed its predecessors in the conventional NLP tasks, to... | Kuntal Kumar Pal, Chitta Baral |  |
| 310 |  |  [Modeling Mathematical Notation Semantics in Academic Papers](https://doi.org/10.18653/v1/2021.findings-emnlp.266) |  | 0 | Natural language models often fall short when understanding and generating mathematical notation. What is not clear is whether these shortcomings are due to fundamental limitations of the models, or the absence of appropriate tasks. In this paper, we explore the extent to which natural language models can learn semantics between mathematical notation and their surrounding text. We propose two notation prediction tasks, and train a model that selectively masks notation tokens and encodes left... | Hwiyeol Jo, Dongyeop Kang, Andrew Head, Marti A. Hearst |  |
| 311 |  |  [Unpacking the Interdependent Systems of Discrimination: Ableist Bias in NLP Systems through an Intersectional Lens](https://doi.org/10.18653/v1/2021.findings-emnlp.267) |  | 0 | Much of the world’s population experiences some form of disability during their lifetime. Caution must be exercised while designing natural language processing (NLP) systems to prevent systems from inadvertently perpetuating ableist bias against people with disabilities, i.e., prejudice that favors those with typical abilities. We report on various analyses based on word predictions of a large-scale BERT language model. Statistically significant results demonstrate that people with disabilities... | Saad Hassan, Matt Huenerfauth, Cecilia Ovesdotter Alm |  |
| 312 |  |  [Constructing Emotional Consensus and Utilizing Unpaired Data for Empathetic Dialogue Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.268) |  | 0 | Researches on dialogue empathy aim to endow an agent with the capacity of accurate understanding and proper responding for emotions. Existing models for empathetic dialogue generation focus on the emotion flow in one direction, that is, from the context to response. We argue that conducting an empathetic conversation is a bidirectional process, where empathy occurs when the emotions of two interlocutors could converge on the same point, i.e., reaching an emotional consensus. Besides, we also... | Lei Shen, Jinchao Zhang, Jiao Ou, Xiaofang Zhao, Jie Zhou |  |
| 313 |  |  [Automatic rule generation for time expression normalization](https://doi.org/10.18653/v1/2021.findings-emnlp.269) |  | 0 | The understanding of time expressions includes two sub-tasks: recognition and normalization. In recent years, significant progress has been made in the recognition of time expressions while research on normalization has lagged behind. Existing SOTA normalization methods highly rely on rules or grammars designed by experts, which limits their performance on emerging corpora, such as social media texts. In this paper, we model time expression normalization as a sequence of operations to construct... | Wentao Ding, Jianhao Chen, Jinmao Li, Yuzhong Qu |  |
| 314 |  |  [RW-KD: Sample-wise Loss Terms Re-Weighting for Knowledge Distillation](https://doi.org/10.18653/v1/2021.findings-emnlp.270) |  | 0 | Knowledge Distillation (KD) is extensively used in Natural Language Processing to compress the pre-training and task-specific fine-tuning phases of large neural language models. A student model is trained to minimize a convex combination of the prediction loss over the labels and another over the teacher output. However, most existing works either fix the interpolating weight between the two losses apriori or vary the weight using heuristics. In this work, we propose a novel sample-wise loss... | Peng Lu, Abbas Ghaddar, Ahmad Rashid, Mehdi Rezagholizadeh, Ali Ghodsi, Philippe Langlais |  |
| 315 |  |  [Visual Cues and Error Correction for Translation Robustness](https://doi.org/10.18653/v1/2021.findings-emnlp.271) |  | 0 | Neural Machine Translation models are sensitive to noise in the input texts, such as misspelled words and ungrammatical constructions. Existing robustness techniques generally fail when faced with unseen types of noise and their performance degrades on clean texts. In this paper, we focus on three types of realistic noise that are commonly generated by humans and introduce the idea of visual context to improve translation robustness for noisy texts. In addition, we describe a novel error... | Zhenhao Li, Marek Rei, Lucia Specia |  |
| 316 |  |  [Beyond the Tip of the Iceberg: Assessing Coherence of Text Classifiers](https://doi.org/10.18653/v1/2021.findings-emnlp.272) |  | 0 | As large-scale, pre-trained language models achieve human-level and superhuman accuracy on existing language understanding tasks, statistical bias in benchmark data and probing studies have recently called into question their true capabilities. For a more informative evaluation than accuracy on text classification tasks can offer, we propose evaluating systems through a novel measure of prediction coherence. We apply our framework to two existing language understanding benchmarks with different... | Shane Storks, Joyce Chai |  |
| 317 |  |  [Does Pretraining for Summarization Require Knowledge Transfer?](https://doi.org/10.18653/v1/2021.findings-emnlp.273) |  | 0 | Pretraining techniques leveraging enormous datasets have driven recent advances in text summarization. While folk explanations suggest that knowledge transfer accounts for pretraining’s benefits, little is known about why it works or what makes a pretraining task or dataset suitable. In this paper, we challenge the knowledge transfer story, showing that pretraining on documents consisting of character n-grams selected at random, we can nearly match the performance of models pretrained on real... | Kundan Krishna, Jeffrey P. Bigham, Zachary C. Lipton |  |
| 318 |  |  [Bandits Don't Follow Rules: Balancing Multi-Facet Machine Translation with Multi-Armed Bandits](https://doi.org/10.18653/v1/2021.findings-emnlp.274) |  | 0 | Training data for machine translation (MT) is often sourced from a multitude of large corpora that are multi-faceted in nature, e.g. containing contents from multiple domains or different levels of quality or complexity. Naturally, these facets do not occur with equal frequency, nor are they equally important for the test scenario at hand. In this work, we propose to optimize this balance jointly with MT model parameters to relieve system developers from manual schedule design. A multi-armed... | Julia Kreutzer, David Vilar, Artem Sokolov |  |
| 319 |  |  [Sometimes We Want Ungrammatical Translations](https://doi.org/10.18653/v1/2021.findings-emnlp.275) |  | 0 | Rapid progress in Neural Machine Translation (NMT) systems over the last few years has focused primarily on improving translation quality, and as a secondary focus, improving robustness to perturbations (e.g. spelling). While performance and robustness are important objectives, by over-focusing on these, we risk overlooking other important properties. In this paper, we draw attention to the fact that for some applications, faithfulness to the original (input) text is important to preserve, even... | Prasanna Parthasarathi, Koustuv Sinha, Joelle Pineau, Adina Williams |  |
| 320 |  |  [An animated picture says at least a thousand words: Selecting Gif-based Replies in Multimodal Dialog](https://doi.org/10.18653/v1/2021.findings-emnlp.276) |  | 0 | Online conversations include more than just text. Increasingly, image-based responses such as memes and animated gifs serve as culturally recognized and often humorous responses in conversation. However, while NLP has broadened to multimodal models, conversational dialog systems have largely focused only on generating text replies. Here, we introduce a new dataset of 1.56M text-gif conversation turns and introduce a new multimodal conversational model Pepe the King Prawn for selecting gif-based... | Xingyao Wang, David Jurgens |  |
| 321 |  |  [SciCap: Generating Captions for Scientific Figures](https://doi.org/10.18653/v1/2021.findings-emnlp.277) |  | 0 | Researchers use figures to communicate rich, complex information in scientific papers. The captions of these figures are critical to conveying effective messages. However, low-quality figure captions commonly occur in scientific articles and may decrease understanding. In this paper, we propose an end-to-end neural framework to automatically generate informative, high-quality captions for scientific figures. To this end, we introduce SCICAP, a large-scale figure-caption dataset based on... | TingYao Hsu, C. Lee Giles, TingHao Kenneth Huang |  |
| 322 |  |  [SentNoB: A Dataset for Analysing Sentiment on Noisy Bangla Texts](https://doi.org/10.18653/v1/2021.findings-emnlp.278) |  | 0 | In this paper, we propose an annotated sentiment analysis dataset made of informally written Bangla texts. This dataset comprises public comments on news and videos collected from social media covering 13 different domains, including politics, education, and agriculture. These comments are labeled with one of the polarity labels, namely positive, negative, and neutral. One significant characteristic of the dataset is that each of the comments is noisy in terms of the mix of dialects and... | Khondoker Ittehadul Islam, Sudipta Kar, Md Saiful Islam, Mohammad Ruhul Amin |  |
| 323 |  |  [Translate & Fill: Improving Zero-Shot Multilingual Semantic Parsing with Synthetic Data](https://doi.org/10.18653/v1/2021.findings-emnlp.279) |  | 0 | While multilingual pretrained language models (LMs) fine-tuned on a single language have shown substantial cross-lingual task transfer capabilities, there is still a wide performance gap in semantic parsing tasks when target language supervision is available. In this paper, we propose a novel Translate-and-Fill (TaF) method to produce silver training data for a multilingual semantic parser. This method simplifies the popular Translate-Align-Project (TAP) pipeline and consists of a... | Massimo Nicosia, Zhongdi Qu, Yasemin Altun |  |
| 324 |  |  [NewsBERT: Distilling Pre-trained Language Model for Intelligent News Application](https://doi.org/10.18653/v1/2021.findings-emnlp.280) |  | 0 | Pre-trained language models (PLMs) like BERT have made great progress in NLP. News articles usually contain rich textual information, and PLMs have the potentials to enhance news text modeling for various intelligent news applications like news recommendation and retrieval. However, most existing PLMs are in huge size with hundreds of millions of parameters. Many online news applications need to serve millions of users with low latency tolerance, which poses great challenges to incorporating... | Chuhan Wu, Fangzhao Wu, Yang Yu, Tao Qi, Yongfeng Huang, Qi Liu |  |
| 325 |  |  [SD-QA: Spoken Dialectal Question Answering for the Real World](https://doi.org/10.18653/v1/2021.findings-emnlp.281) |  | 0 | Question answering (QA) systems are now available through numerous commercial applications for a wide variety of domains, serving millions of users that interact with them via speech interfaces. However, current benchmarks in QA research do not account for the errors that speech recognition models might introduce, nor do they consider the language variations (dialects) of the users. To address this gap, we augment an existing QA dataset to construct a multi-dialect, spoken QA benchmark on five... | Fahim Faisal, Sharlina Keshava, Md Mahfuz Ibn Alam, Antonios Anastasopoulos |  |
| 326 |  |  [The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.282) |  | 0 | A “bigger is better” explosion in the number of parameters in deep neural networks has made it increasingly challenging to make state-of-the-art networks accessible in compute-restricted environments. Compression techniques have taken on renewed importance as a way to bridge the gap. However, evaluation of the trade-offs incurred by popular compression techniques has been centered on high-resource datasets. In this work, we instead consider the impact of compression in a data-limited regime. We... | Orevaoghene Ahia, Julia Kreutzer, Sara Hooker |  |
| 327 |  |  [Transformer over Pre-trained Transformer for Neural Text Segmentation with Enhanced Topic Coherence](https://doi.org/10.18653/v1/2021.findings-emnlp.283) |  | 0 | This paper proposes a transformer over transformer framework, called Transformerˆ2, to perform neural text segmentation. It consists of two components: bottom-level sentence encoders using pre-trained transformers, and an upper-level transformer-based segmentation model based on the sentence embeddings. The bottom-level component transfers the pre-trained knowledge learnt from large external corpora under both single and pair-wise supervised NLP tasks to model the sentence embeddings for the... | Kelvin Lo, Yuan Jin, Weicong Tan, Ming Liu, Lan Du, Wray L. Buntine |  |
| 328 |  |  [Self-Supervised Neural Topic Modeling](https://doi.org/10.18653/v1/2021.findings-emnlp.284) |  | 0 | Topic models are useful tools for analyzing and interpreting the main underlying themes of large corpora of text. Most topic models rely on word co-occurrence for computing a topic, i.e., a weighted set of words that together represent a high-level semantic concept. In this paper, we propose a new light-weight Self-Supervised Neural Topic Model (SNTM) that learns a rich context by learning a topic representation jointly from three co-occurring words and a document that the triple originates... | Seyed Ali Bahrainian, Martin Jaggi, Carsten Eickhoff |  |
| 329 |  |  [Coreference-aware Surprisal Predicts Brain Response](https://doi.org/10.18653/v1/2021.findings-emnlp.285) |  | 0 | Recent evidence supports a role for coreference processing in guiding human expectations about upcoming words during reading, based on covariation between reading times and word surprisal estimated by a coreference-aware semantic processing model (Jaffe et al. 2020).The present study reproduces and elaborates on this finding by (1) enabling the parser to process subword information that might better approximate human morphological knowledge, and (2) extending evaluation of coreference effects... | Evan Jaffe, ByungDoh Oh, William Schuler |  |
| 330 |  |  [Distilling the Knowledge of Large-scale Generative Models into Retrieval Models for Efficient Open-domain Conversation](https://doi.org/10.18653/v1/2021.findings-emnlp.286) |  | 0 | Despite the remarkable performance of large-scale generative models in open-domain conversation, they are known to be less practical for building real-time conversation systems due to high latency. On the other hand, retrieval models could return responses with much lower latency but show inferior performance to the large-scale generative models since the conversation quality is bounded by the pre-defined response set. To take advantage of both approaches, we propose a new training method... | Beomsu Kim, Seokjun Seo, Seungju Han, Enkhbayar Erdenee, Buru Chang |  |
| 331 |  |  [Modeling Users and Online Communities for Abuse Detection: A Position on Ethics and Explainability](https://doi.org/10.18653/v1/2021.findings-emnlp.287) |  | 0 | Abuse on the Internet is an important societal problem of our time. Millions of Internet users face harassment, racism, personal attacks, and other types of abuse across various platforms. The psychological effects of abuse on individuals can be profound and lasting. Consequently, over the past few years, there has been a substantial research effort towards automated abusive language detection in the field of NLP. In this position paper, we discuss the role that modeling of users and online... | Pushkar Mishra, Helen Yannakoudakis, Ekaterina Shutova |  |
| 332 |  |  [Detecting Community Sensitive Norm Violations in Online Conversations](https://doi.org/10.18653/v1/2021.findings-emnlp.288) |  | 0 | Online platforms and communities establish their own norms that govern what behavior is acceptable within the community. Substantial effort in NLP has focused on identifying unacceptable behaviors and, recently, on forecasting them before they occur. However, these efforts have largely focused on toxicity as the sole form of community norm violation. Such focus has overlooked the much larger set of rules that moderators enforce. Here, we introduce a new dataset focusing on a more complete... | Chan Young Park, Julia Mendelsohn, Karthik Radhakrishnan, Kinjal Jain, Tushar Kanakagiri, David Jurgens, Yulia Tsvetkov |  |
| 333 |  |  [SupCL-Seq: Supervised Contrastive Learning for Downstream Optimized Sequence Representations](https://doi.org/10.18653/v1/2021.findings-emnlp.289) |  | 0 | While contrastive learning is proven to be an effective training strategy in computer vision, Natural Language Processing (NLP) is only recently adopting it as a self-supervised alternative to Masked Language Modeling (MLM) for improving sequence representations. This paper introduces SupCL-Seq, which extends the supervised contrastive learning from computer vision to the optimization of sequence representations in NLP. By altering the dropout mask probability in standard Transformer... | Hooman Sedghamiz, Shivam Raval, Enrico Santus, Tuka Alhanai, Mohammad M. Ghassemi |  |
| 334 |  |  [mDAPT: Multilingual Domain Adaptive Pretraining in a Single Model](https://doi.org/10.18653/v1/2021.findings-emnlp.290) |  | 0 | Domain adaptive pretraining, i.e. the continued unsupervised pretraining of a language model on domain-specific text, improves the modelling of text for downstream tasks within the domain. Numerous real-world applications are based on domain-specific text, e.g. working with financial or biomedical documents, and these applications often need to support multiple languages. However, large-scale domain-specific multilingual pretraining data for such scenarios can be difficult to obtain, due to... | Rasmus Kær Jørgensen, Mareike Hartmann, Xiang Dai, Desmond Elliott |  |
| 335 |  |  [COSMic: A Coherence-Aware Generation Metric for Image Descriptions](https://doi.org/10.18653/v1/2021.findings-emnlp.291) |  | 0 | Developers of text generation models rely on automated evaluation metrics as a stand-in for slow and expensive manual evaluations. However, image captioning metrics have struggled to give accurate learned estimates of the semantic and pragmatic success of output text. We address this weakness by introducing the first discourse-aware learned generation metric for evaluating image descriptions. Our approach is inspired by computational theories of discourse for capturing information goals using... | Mert Inan, Piyush Sharma, Baber Khalid, Radu Soricut, Matthew Stone, Malihe Alikhani |  |
| 336 |  |  [Relation-Guided Pre-Training for Open-Domain Question Answering](https://doi.org/10.18653/v1/2021.findings-emnlp.292) |  | 0 | Answering complex open-domain questions requires understanding the latent relations between involving entities. However, we found that the existing QA datasets are extremely imbalanced in some types of relations, which hurts the generalization performance over questions with long-tail relations. To remedy this problem, in this paper, we propose a Relation-Guided Pre-Training (RGPT-QA) framework. We first generate a relational QA dataset covering a wide range of relations from both the Wikidata... | Ziniu Hu, Yizhou Sun, KaiWei Chang |  |
| 337 |  |  [MURAL: Multimodal, Multitask Representations Across Languages](https://doi.org/10.18653/v1/2021.findings-emnlp.293) |  | 0 | Both image-caption pairs and translation pairs provide the means to learn deep representations of and connections between languages. We use both types of pairs in MURAL (MUltimodal, MUltitask Representations Across Languages), a dual encoder that solves two tasks: 1) image-text matching and 2) translation pair matching. By incorporating billions of translation pairs, MURAL extends ALIGN (Jia et al.)–a state-of-the-art dual encoder learned from 1.8 billion noisy image-text pairs. When using the... | Aashi Jain, Mandy Guo, Krishna Srinivasan, Ting Chen, Sneha Kudugunta, Chao Jia, Yinfei Yang, Jason Baldridge |  |
| 338 |  |  [AStitchInLanguageModels: Dataset and Methods for the Exploration of Idiomaticity in Pre-Trained Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.294) |  | 0 | Despite their success in a variety of NLP tasks, pre-trained language models, due to their heavy reliance on compositionality, fail in effectively capturing the meanings of multiword expressions (MWEs), especially idioms. Therefore, datasets and methods to improve the representation of MWEs are urgently needed. Existing datasets are limited to providing the degree of idiomaticity of expressions along with the literal and, where applicable, (a single) non-literal interpretation of MWEs. This... | Harish Tayyar Madabushi, Edward GowSmith, Carolina Scarton, Aline Villavicencio |  |
| 339 |  |  [Refine and Imitate: Reducing Repetition and Inconsistency in Persuasion Dialogues via Reinforcement Learning and Human Demonstration](https://doi.org/10.18653/v1/2021.findings-emnlp.295) |  | 0 | Persuasion dialogue system reflects the machine’s ability to make strategic moves beyond verbal communication, and therefore differentiates itself from task-oriented or open-domain dialogues and has its own unique values. However, the repetition and inconsistency problems still persist in dialogue response generation and could substantially impact user experience and impede the persuasion outcome. Besides, although reinforcement learning (RL) approaches have achieved big success in strategic... | Weiyan Shi, Yu Li, Saurav Sahay, Zhou Yu |  |
| 340 |  |  [A Computational Exploration of Pejorative Language in Social Media](https://doi.org/10.18653/v1/2021.findings-emnlp.296) |  | 0 | In this paper we study pejorative language, an under-explored topic in computational linguistics. Unlike existing models of offensive language and hate speech, pejorative language manifests itself primarily at the lexical level, and describes a word that is used with a negative connotation, making it different from offensive language or other more studied categories. Pejorativity is also context-dependent: the same word can be used with or without pejorative connotations, thus pejorativity... | Liviu P. Dinu, IoanBogdan Iordache, Ana Sabina Uban, Marcos Zampieri |  |
| 341 |  |  [Evidence-based Fact-Checking of Health-related Claims](https://doi.org/10.18653/v1/2021.findings-emnlp.297) |  | 0 | The task of verifying the truthfulness of claims in textual documents, or fact-checking, has received significant attention in recent years. Many existing evidence-based factchecking datasets contain synthetic claims and the models trained on these data might not be able to verify real-world claims. Particularly few studies addressed evidence-based fact-checking of health-related claims that require medical expertise or evidence from the scientific literature. In this paper, we introduce... | Mourad Sarrouti, Asma Ben Abacha, Yassine Mrabet, Dina DemnerFushman |  |
| 342 |  |  [Learning and Analyzing Generation Order for Undirected Sequence Models](https://doi.org/10.18653/v1/2021.findings-emnlp.298) |  | 0 | Undirected neural sequence models have achieved performance competitive with the state-of-the-art directed sequence models that generate monotonically from left to right in machine translation tasks. In this work, we train a policy that learns the generation order for a pre-trained, undirected translation model via reinforcement learning. We show that the translations decoded by our learned orders achieve higher BLEU scores than the outputs decoded from left to right or decoded by the learned... | Yichen Jiang, Mohit Bansal |  |
| 343 |  |  [Automatic Bilingual Markup Transfer](https://doi.org/10.18653/v1/2021.findings-emnlp.299) |  | 0 | We describe the task of bilingual markup transfer, which involves placing markup tags from a source sentence into a fixed target translation. This task arises in practice when a human translator generates the target translation without markup, and then the system infers the placement of markup tags. This task contrasts from previous work in which markup transfer is performed jointly with machine translation. We propose two novel metrics and evaluate several approaches based on unsupervised word... | Thomas Zenkel, Joern Wuebker, John DeNero |  |
| 344 |  |  [Exploring a Unified Sequence-To-Sequence Transformer for Medical Product Safety Monitoring in Social Media](https://doi.org/10.18653/v1/2021.findings-emnlp.300) |  | 0 | Adverse Events (AE) are harmful events resulting from the use of medical products. Although social media may be crucial for early AE detection, the sheer scale of this data makes it logistically intractable to analyze using human agents, with NLP representing the only low-cost and scalable alternative. In this paper, we frame AE Detection and Extraction as a sequence-to-sequence problem using the T5 model architecture and achieve strong performance improvements over the baselines on several... | Shivam Raval, Hooman Sedghamiz, Enrico Santus, Tuka Alhanai, Mohammad M. Ghassemi, Emmanuele Chersoni |  |
| 345 |  |  [Disentangling Generative Factors in Natural Language with Discrete Variational Autoencoders](https://doi.org/10.18653/v1/2021.findings-emnlp.301) |  | 0 | The ability of learning disentangled representations represents a major step for interpretable NLP systems as it allows latent linguistic features to be controlled. Most approaches to disentanglement rely on continuous variables, both for images and text. We argue that despite being suitable for image datasets, continuous variables may not be ideal to model features of textual data, due to the fact that most generative factors in text are discrete. We propose a Variational Autoencoder based... | Giangiacomo Mercatali, André Freitas |  |
| 346 |  |  [MSD: Saliency-aware Knowledge Distillation for Multimodal Understanding](https://doi.org/10.18653/v1/2021.findings-emnlp.302) |  | 0 | To reduce a model size but retain performance, we often rely on knowledge distillation (KD) which transfers knowledge from a large “teacher” model to a smaller “student” model. However, KD on multimodal datasets such as vision-language tasks is relatively unexplored, and digesting multimodal information is challenging since different modalities present different types of information. In this paper, we perform a large-scale empirical study to investigate the importance and effects of each... | Woojeong Jin, Maziar Sanjabi, Shaoliang Nie, Liang Tan, Xiang Ren, Hamed Firooz |  |
| 347 |  |  [Do UD Trees Match Mention Spans in Coreference Annotations?](https://doi.org/10.18653/v1/2021.findings-emnlp.303) |  | 0 | One can find dozens of data resources for various languages in which coreference - a relation between two or more expressions that refer to the same real-world entity - is manually annotated. One could also assume that such expressions usually constitute syntactically meaningful units; however, mention spans have been annotated simply by delimiting token intervals in most coreference projects, i.e., independently of any syntactic representation. We argue that it could be advantageous to make... | Martin Popel, Zdenek Zabokrtský, Anna Nedoluzhko, Michal Novák, Daniel Zeman |  |
| 348 |  |  [Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference](https://doi.org/10.18653/v1/2021.findings-emnlp.304) |  | 0 | Sparse Mixture-of-Experts (MoE) has been a successful approach for scaling multilingual translation models to billions of parameters without a proportional increase in training computation. However, MoE models are prohibitively large and practitioners often resort to methods such as distillation for serving. In this work, we investigate routing strategies at different granularity (token, sentence, task) in MoE models to bypass distillation. Experiments on WMT and a web-scale dataset suggest... | Sneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, MinhThang Luong, Orhan Firat |  |
| 349 |  |  [TAG: Gradient Attack on Transformer-based Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.305) |  | 0 | Although distributed learning has increasingly gained attention in terms of effectively utilizing local devices for data privacy enhancement, recent studies show that publicly shared gradients in the training process can reveal the private training data (gradient leakage) to a third-party. We have, however, no systematic understanding of the gradient leakage mechanism on the Transformer based language models. In this paper, as the first attempt, we formulate the gradient attack problem on the... | Jieren Deng, Yijue Wang, Ji Li, Chenghong Wang, Chao Shang, Hang Liu, Sanguthevar Rajasekaran, Caiwen Ding |  |
| 350 |  |  [Generating Realistic Natural Language Counterfactuals](https://doi.org/10.18653/v1/2021.findings-emnlp.306) |  | 0 | Counterfactuals are a valuable means for understanding decisions made by ML systems. However, the counterfactuals generated by the methods currently available for natural language text are either unrealistic or introduce imperceptible changes. We propose CounterfactualGAN: a method that combines a conditional GAN and the embeddings of a pretrained BERT encoder to model-agnostically generate realistic natural language text counterfactuals for explaining regression and classification tasks.... | Marcel Robeer, Floris Bex, Ad Feelders |  |
| 351 |  |  [Unsupervised Chunking as Syntactic Structure Induction with a Knowledge-Transfer Approach](https://doi.org/10.18653/v1/2021.findings-emnlp.307) |  | 0 | In this paper, we address unsupervised chunking as a new task of syntactic structure induction, which is helpful for understanding the linguistic structures of human languages as well as processing low-resource languages. We propose a knowledge-transfer approach that heuristically induces chunk labels from state-of-the-art unsupervised parsing models; a hierarchical recurrent neural network (HRNN) learns from such induced chunk labels to smooth out the noise of the heuristics. Experiments show... | Anup Anand Deshmukh, Qianqiu Zhang, Ming Li, Jimmy Lin, Lili Mou |  |
| 352 |  |  [Model-based analysis of brain activity reveals the hierarchy of language in 305 subjects](https://doi.org/10.18653/v1/2021.findings-emnlp.308) |  | 0 | A popular approach to decompose the neural bases of language consists in correlating, across individuals, the brain responses to different stimuli (e.g. regular speech versus scrambled words, sentences, or paragraphs). Although successful, this ‘model-free’ approach necessitates the acquisition of a large and costly set of neuroimaging data. Here, we show that a model-based approach can reach equivalent results within subjects exposed to natural stimuli. We capitalize on the recently-discovered... | Charlotte Caucheteux, Alexandre Gramfort, JeanRemi King |  |
| 353 |  |  [Gated Transformer for Robust De-noised Sequence-to-Sequence Modelling](https://doi.org/10.18653/v1/2021.findings-emnlp.309) |  | 0 | Robust sequence-to-sequence modelling is an essential task in the real world where the inputs are often noisy. Both user-generated and machine generated inputs contain various kinds of noises in the form of spelling mistakes, grammatical errors, character recognition errors, all of which impact downstream tasks and affect interpretability of texts. In this work, we devise a novel sequence-to-sequence architecture for detecting and correcting different real world and artificial noises... | Ayan Sengupta, Amit Kumar, Sourabh Kumar Bhattacharjee, Suman Roy |  |
| 354 |  |  [Token-wise Curriculum Learning for Neural Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.310) |  | 0 | Existing curriculum learning approaches to Neural Machine Translation (NMT) require sampling sufficient amounts of “easy” samples from training data at the early training stage. This is not always achievable for low-resource languages where the amount of training data is limited. To address such a limitation, we propose a novel token-wise curriculum learning approach that creates sufficient amounts of easy samples. Specifically, the model learns to predict a short sub-sequence from the... | Chen Liang, Haoming Jiang, Xiaodong Liu, Pengcheng He, Weizhu Chen, Jianfeng Gao, Tuo Zhao |  |
| 355 |  |  [RelDiff: Enriching Knowledge Graph Relation Representations for Sensitivity Classification](https://doi.org/10.18653/v1/2021.findings-emnlp.311) |  | 0 | The relationships that exist between entities can be a reliable indicator for classifying sensitive information, such as commercially sensitive information. For example, the relation person-IsDirectorOf-company can indicate whether an individual’s salary should be considered as sensitive personal information. Representations of such relations are often learned using a knowledge graph to produce embeddings for relation types, generalised across different entity-pairs. However, a relation type... | Hitarth Narvala, Graham McDonald, Iadh Ounis |  |
| 356 |  |  [Post-Editing Extractive Summaries by Definiteness Prediction](https://doi.org/10.18653/v1/2021.findings-emnlp.312) |  | 0 | Extractive summarization has been the mainstay of automatic summarization for decades. Despite all the progress, extractive summarizers still suffer from shortcomings including coreference issues arising from extracting sentences away from their original context in the source document. This affects the coherence and readability of extractive summaries. In this work, we propose a lightweight post-editing step for extractive summaries that centers around a single linguistic decision: the... | Jad Kabbara, Jackie Chi Kit Cheung |  |
| 357 |  |  [Leveraging Pretrained Models for Automatic Summarization of Doctor-Patient Conversations](https://doi.org/10.18653/v1/2021.findings-emnlp.313) |  | 0 | Fine-tuning pretrained models for automatically summarizing doctor-patient conversation transcripts presents many challenges: limited training data, significant domain shift, long and noisy transcripts, and high target summary variability. In this paper, we explore the feasibility of using pretrained transformer models for automatically summarizing doctor-patient conversations directly from transcripts. We show that fluent and adequate summaries can be generated with limited training data by... | Longxiang Zhang, Renato Negrinho, Arindam Ghosh, Vasudevan Jagannathan, Hamid Reza Hassanzadeh, Thomas Schaaf, Matthew R. Gormley |  |
| 358 |  |  [Distilling Knowledge for Empathy Detection](https://doi.org/10.18653/v1/2021.findings-emnlp.314) |  | 0 | Empathy is the link between self and others. Detecting and understanding empathy is a key element for improving human-machine interaction. However, annotating data for detecting empathy at a large scale is a challenging task. This paper employs multi-task training with knowledge distillation to incorporate knowledge from available resources (emotion and sentiment) to detect empathy from the natural language in different domains. This approach yields better results on an existing news-related... | Mahshid Hosseini, Cornelia Caragea |  |
| 359 |  |  [Adapting Entities across Languages and Cultures](https://doi.org/10.18653/v1/2021.findings-emnlp.315) |  | 0 | How would you explain Bill Gates to a German? He is associated with founding a company in the United States, so perhaps the German founder Carl Benz could stand in for Gates in those contexts. This type of translation is called adaptation in the translation community. Until now, this task has not been done computationally. Automatic adaptation could be used in natural language processing for machine translation and indirectly for generating new question answering datasets and education. We... | Denis Peskov, Viktor Hangya, Jordan L. BoydGraber, Alexander Fraser |  |
| 360 |  |  [ODIST: Open World Classification via Distributionally Shifted Instances](https://doi.org/10.18653/v1/2021.findings-emnlp.316) |  | 0 | In this work, we address the open-world classification problem with a method called ODIST, open world classification via distributionally shifted instances. This novel and straightforward method can create out-of-domain instances from the in-domain training instances with the help of a pre-trained generative language model. Experimental results show that ODIST performs better than state-of-the-art decision boundary finding method. | Lei Shu, Yassine Benajiba, Saab Mansour, Yi Zhang |  |
| 361 |  |  [LAMAD: A Linguistic Attentional Model for Arabic Text Diacritization](https://doi.org/10.18653/v1/2021.findings-emnlp.317) |  | 0 | In Arabic Language, diacritics are used to specify meanings as well as pronunciations. However, diacritics are often omitted from written texts, which increases the number of possible meanings and pronunciations. This leads to an ambiguous text and makes the computational process on undiacritized text more difficult. In this paper, we propose a Linguistic Attentional Model for Arabic text Diacritization (LAMAD). In LAMAD, a new linguistic feature representation is presented, which utilizes both... | Raeed AlSabri, Jianliang Gao |  |
| 362 |  |  [Sequence-to-Lattice Models for Fast Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.318) |  | 0 | Non-autoregressive machine translation (NAT) approaches enable fast generation by utilizing parallelizable generative processes. The remaining bottleneck in these models is their decoder layers; unfortunately unlike in autoregressive models (Kasai et al., 2020), removing decoder layers from NAT models significantly degrades accuracy. This work proposes a sequence-to-lattice model that replaces the decoder with a search lattice. Our approach first constructs a candidate lattice using efficient... | Yuntian Deng, Alexander M. Rush |  |
| 363 |  |  [Towards Realistic Single-Task Continuous Learning Research for NER](https://doi.org/10.18653/v1/2021.findings-emnlp.319) |  | 0 | There is an increasing interest in continuous learning (CL), as data privacy is becoming a priority for real-world machine learning applications. Meanwhile, there is still a lack of academic NLP benchmarks that are applicable for realistic CL settings, which is a major challenge for the advancement of the field. In this paper we discuss some of the unrealistic data characteristics of public datasets, study the challenges of realistic single-task continuous learning as well as the effectiveness... | Justin Payan, Yuval Merhav, He Xie, Satyapriya Krishna, Anil Ramakrishna, Mukund Sridhar, Rahul Gupta |  |
| 364 |  |  [Retrieval Augmentation Reduces Hallucination in Conversation](https://doi.org/10.18653/v1/2021.findings-emnlp.320) |  | 0 | Despite showing increasingly human-like conversational abilities, state-of-the-art dialogue models often suffer from factual incorrectness and hallucination of knowledge (Roller et al., 2020). In this work we explore the use of neural-retrieval-in-the-loop architectures - recently shown to be effective in open-domain QA (Lewis et al., 2020b; Izacard and Grave, 2020) - for knowledge-grounded dialogue, a task that is arguably more challenging as it requires querying based on complex multi-turn... | Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, Jason Weston |  |
| 365 |  |  [Towards Automatic Bias Detection in Knowledge Graphs](https://doi.org/10.18653/v1/2021.findings-emnlp.321) |  | 0 | With the recent surge in social applications relying on knowledge graphs, the need for techniques to ensure fairness in KG based methods is becoming increasingly evident. Previous works have demonstrated that KGs are prone to various social biases, and have proposed multiple methods for debiasing them. However, in such studies, the focus has been on debiasing techniques, while the relations to be debiased are specified manually by the user. As manual specification is itself susceptible to human... | Daphna Keidar, Mian Zhong, Ce Zhang, Yash Raj Shrestha, Bibek Paudel |  |
| 366 |  |  [Searching for More Efficient Dynamic Programs](https://doi.org/10.18653/v1/2021.findings-emnlp.322) |  | 0 | Computational models of human language often involve combinatorial problems. For instance, a probabilistic parser may marginalize over exponentially many trees to make predictions. Algorithms for such problems often employ dynamic programming and are not always unique. Finding one with optimal asymptotic runtime can be unintuitive, time-consuming, and error-prone. Our work aims to automate this laborious process. Given an initial correct declarative program, we search for a sequence of... | Tim Vieira, Ryan Cotterell, Jason Eisner |  |
| 367 |  |  [Revisiting Robust Neural Machine Translation: A Transformer Case Study](https://doi.org/10.18653/v1/2021.findings-emnlp.323) |  | 0 | Transformers have brought a remarkable improvement in the performance of neural machine translation (NMT) systems but they could be surprisingly vulnerable to noise. In this work, we try to investigate how noise breaks Transformers and if there exist solutions to deal with such issues. There is a large body of work in the NMT literature on analyzing the behavior of conventional models for the problem of noise but Transformers are relatively understudied in this context. Motivated by this, we... | Peyman Passban, Puneeth S. M. Saladi, Qun Liu |  |
| 368 |  |  [Can NLI Models Verify QA Systems' Predictions?](https://doi.org/10.18653/v1/2021.findings-emnlp.324) |  | 0 | To build robust question answering systems, we need the ability to verify whether answers to questions are truly correct, not just “good enough” in the context of imperfect QA datasets. We explore the use of natural language inference (NLI) as a way to achieve this goal, as NLI inherently requires the premise (document context) to contain all necessary information to support the hypothesis (proposed answer to the question). We leverage large pre-trained models and recent prior datasets to... | Jifan Chen, Eunsol Choi, Greg Durrett |  |
| 369 |  |  [Parameter-Efficient Domain Knowledge Integration from Multiple Sources for Biomedical Pre-trained Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.325) |  | 0 | Domain-specific pre-trained language models (PLMs) have achieved great success over various downstream tasks in different domains. However, existing domain-specific PLMs mostly rely on self-supervised learning over large amounts of domain text, without explicitly integrating domain-specific knowledge, which can be essential in many domains. Moreover, in knowledge-sensitive areas such as the biomedical domain, knowledge is stored in multiple sources and formats, and existing biomedical PLMs... | Qiuhao Lu, Dejing Dou, Thien Huu Nguyen |  |
| 370 |  |  [Uncovering Implicit Gender Bias in Narratives through Commonsense Inference](https://doi.org/10.18653/v1/2021.findings-emnlp.326) |  | 0 | Pre-trained language models learn socially harmful biases from their training corpora, and may repeat these biases when used for generation. We study gender biases associated with the protagonist in model-generated stories. Such biases may be expressed either explicitly (“women can’t park”) or implicitly (e.g. an unsolicited male character guides her into a parking space). We focus on implicit biases, and use a commonsense reasoning engine to uncover them. Specifically, we infer and analyze the... | Tenghao Huang, Faeze Brahman, Vered Shwartz, Snigdha Chaturvedi |  |
| 371 |  |  [Contrastive Document Representation Learning with Graph Attention Networks](https://doi.org/10.18653/v1/2021.findings-emnlp.327) |  | 0 | Recent progress in pretrained Transformer-based language models has shown great success in learning contextual representation of text. However, due to the quadratic self-attention complexity, most of the pretrained Transformers models can only handle relatively short text. It is still a challenge when it comes to modeling very long documents. In this work, we propose to use a graph attention network on top of the available pretrained Transformers model to learn document embeddings. This graph... | Peng Xu, Xinchi Chen, Xiaofei Ma, Zhiheng Huang, Bing Xiang |  |
| 372 |  |  [Convex Aggregation for Opinion Summarization](https://doi.org/10.18653/v1/2021.findings-emnlp.328) |  | 0 | Recent advances in text autoencoders have significantly improved the quality of the latent space, which enables models to generate grammatical and consistent text from aggregated latent vectors. As a successful application of this property, unsupervised opinion summarization models generate a summary by decoding the aggregated latent vectors of inputs. More specifically, they perform the aggregation via simple average. However, little is known about how the vector aggregation step affects the... | Hayate Iso, Xiaolan Wang, Yoshihiko Suhara, Stefanos Angelidis, WangChiew Tan |  |
| 373 |  |  [Using Optimal Transport as Alignment Objective for fine-tuning Multilingual Contextualized Embeddings](https://doi.org/10.18653/v1/2021.findings-emnlp.329) |  | 0 | Recent studies have proposed different methods to improve multilingual word representations in contextualized settings including techniques that align between source and target embedding spaces. For contextualized embeddings, alignment becomes more complex as we additionally take context into consideration. In this work, we propose using Optimal Transport (OT) as an alignment objective during fine-tuning to further improve multilingual contextualized representations for downstream cross-lingual... | Sawsan Alqahtani, Garima Lalwani, Yi Zhang, Salvatore Romeo, Saab Mansour |  |
| 374 |  |  [Uncertainty-Aware Machine Translation Evaluation](https://doi.org/10.18653/v1/2021.findings-emnlp.330) |  | 0 | Several neural-based metrics have been recently proposed to evaluate machine translation quality. However, all of them resort to point estimates, which provide limited information at segment level. This is made worse as they are trained on noisy, biased and scarce human judgements, often resulting in unreliable quality predictions. In this paper, we introduce uncertainty-aware MT evaluation and analyze the trustworthiness of the predicted quality. We combine the COMET framework with two... | Taisiya Glushkova, Chrysoula Zerva, Ricardo Rei, André F. T. Martins |  |
| 375 |  |  [Neural Unification for Logic Reasoning over Natural Language](https://doi.org/10.18653/v1/2021.findings-emnlp.331) |  | 0 | Automated Theorem Proving (ATP) deals with the development of computer programs being able to show that some conjectures (queries) are a logical consequence of a set of axioms (facts and rules). There exists several successful ATPs where conjectures and axioms are formally provided (e.g. formalised as First Order Logic formulas). Recent approaches, such as Clark et al., have proposed transformer-based architectures for deriving conjectures given axioms expressed in natural language (English).... | Gabriele Picco, Thanh Lam Hoang, Marco Luca Sbodio, Vanessa López |  |
| 376 |  |  [From None to Severe: Predicting Severity in Movie Scripts](https://doi.org/10.18653/v1/2021.findings-emnlp.332) |  | 0 | In this paper, we introduce the task of predicting severity of age-restricted aspects of movie content based solely on the dialogue script. We first investigate categorizing the ordinal severity of movies on 5 aspects: Sex, Violence, Profanity, Substance consumption, and Frightening scenes. The problem is handled using a siamese network-based multitask framework which concurrently improves the interpretability of the predictions. The experimental results show that our method outperforms the... | Yigeng Zhang, Mahsa Shafaei, Fabio A. González, Thamar Solorio |  |
| 377 |  |  [Benchmarking Meta-embeddings: What Works and What Does Not](https://doi.org/10.18653/v1/2021.findings-emnlp.333) |  | 0 | In the last few years, several methods have been proposed to build meta-embeddings. The general aim was to obtain new representations integrating complementary knowledge from different source pre-trained embeddings thereby improving their overall quality. However, previous meta-embeddings have been evaluated using a variety of methods and datasets, which makes it difficult to draw meaningful conclusions regarding the merits of each approach. In this paper we propose a unified common framework,... | Iker GarcíaFerrero, Rodrigo Agerri, German Rigau |  |
| 378 |  |  [A Plug-and-Play Method for Controlled Text Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.334) |  | 0 | Large pre-trained language models have repeatedly shown their ability to produce fluent text. Yet even when starting from a prompt, generation can continue in many plausible directions. Current decoding methods with the goal of controlling generation, e.g., to ensure specific words are included, either require additional models or fine-tuning, or work poorly when the task at hand is semantically unconstrained, e.g., story generation. In this work, we present a plug-and-play decoding method for... | Damian Pascual, Beni Egressy, Clara Meister, Ryan Cotterell, Roger Wattenhofer |  |
| 379 |  |  [A Corpus-based Syntactic Analysis of Two-termed Unlike Coordination](https://doi.org/10.18653/v1/2021.findings-emnlp.335) |  | 0 | Coordination is a phenomenon of language that conjoins two or more terms or phrases using a coordinating conjunction. Although coordination has been explored extensively in the linguistics literature, the rules and constraints that govern its structure are still largely elusive and widely debated amongst linguists. This paper presents a study of two-termed unlike coordinations in particular, where the two conjuncts of the coordination phrase form valid constituents but have distinct categories.... | Julie Kallini, Christiane Fellbaum |  |
| 380 |  |  [Weakly Supervised Contrastive Learning for Chest X-Ray Report Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.336) |  | 0 | Radiology report generation aims at generating descriptive text from radiology images automatically, which may present an opportunity to improve radiology reporting and interpretation. A typical setting consists of training encoder-decoder models on image-report pairs with a cross entropy loss, which struggles to generate informative sentences for clinical diagnoses since normal findings dominate the datasets. To tackle this challenge and encourage more clinically-accurate text outputs, we... | An Yan, Zexue He, Xing Lu, Jiang Du, Eric Y. Chang, Amilcare Gentili, Julian J. McAuley, ChunNan Hsu |  |
| 381 |  |  [NUANCED: Natural Utterance Annotation for Nuanced Conversation with Estimated Distributions](https://doi.org/10.18653/v1/2021.findings-emnlp.337) |  | 0 | Existing conversational systems are mostly agent-centric, which assumes the user utterances will closely follow the system ontology. However, in real-world scenarios, it is highly desirable that users can speak freely and naturally. In this work, we attempt to build a user-centric dialogue system for conversational recommendation. As there is no clean mapping for a user’s free form utterance to an ontology, we first model the user preferences as estimated distributions over the system ontology... | Zhiyu Chen, Honglei Liu, Hu Xu, Seungwhan Moon, Hao Zhou, Bing Liu |  |
| 382 |  |  [Table-based Fact Verification With Salience-aware Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.338) |  | 0 | Tables provide valuable knowledge that can be used to verify textual statements. While a number of works have considered table-based fact verification, direct alignments of tabular data with tokens in textual statements are rarely available. Moreover, training a generalized fact verification model requires abundant labeled training data. In this paper, we propose a novel system to address these problems. Inspired by counterfactual causality, our system identifies token-level salience in the... | Fei Wang, Kexuan Sun, Jay Pujara, Pedro A. Szekely, Muhao Chen |  |
| 383 |  |  [Detecting Frames in News Headlines and Lead Images in U.S. Gun Violence Coverage](https://doi.org/10.18653/v1/2021.findings-emnlp.339) |  | 0 | News media structure their reporting of events or issues using certain perspectives. When describing an incident involving gun violence, for example, some journalists may focus on mental health or gun regulation, while others may emphasize the discussion of gun rights. Such perspectives are called “frames” in communication research. We study, for the first time, the value of combining lead images and their contextual information with text to identify the frame of a given news article. We... | Isidora Chara Tourni, Lei Guo, Taufiq Husada Daryanto, Fabian Zhafransyah, Edward Edberg Halim, Mona Jalal, Boqi Chen, Sha Lai, Hengchang Hu, Margrit Betke, Prakash Ishwar, Derry Tanti Wijaya |  |
| 384 |  |  [Multi-task Learning to Enable Location Mention Identification in the Early Hours of a Crisis Event](https://doi.org/10.18653/v1/2021.findings-emnlp.340) |  | 0 | Training a robust and reliable deep learning model requires a large amount of data. In the crisis domain, building deep learning models to identify actionable information from the huge influx of data posted by eyewitnesses of crisis events on social media, in a time-critical manner, is central for fast response and relief operations. However, building a large, annotated dataset to train deep learning models is not always feasible in a crisis situation. In this paper, we investigate a multi-task... | Sarthak Khanal, Doina Caragea |  |
| 385 |  |  [Graph-Based Decoding for Task Oriented Semantic Parsing](https://doi.org/10.18653/v1/2021.findings-emnlp.341) |  | 0 | The dominant paradigm for semantic parsing in recent years is to formulate parsing as a sequence-to-sequence task, generating predictions with auto-regressive sequence decoders. In this work, we explore an alternative paradigm. We formulate semantic parsing as a dependency parsing task, applying graph-based decoding techniques developed for syntactic parsing. We compare various decoding techniques given the same pre-trained Transformer encoder on the TOP dataset, including settings where... | Jeremy R. Cole, Nanjiang Jiang, Panupong Pasupat, Luheng He, Peter Shaw |  |
| 386 |  |  [Expected Validation Performance and Estimation of a Random Variable's Maximum](https://doi.org/10.18653/v1/2021.findings-emnlp.342) |  | 0 | Research in NLP is often supported by experimental results, and improved reporting of such results can lead to better understanding and more reproducible science. In this paper we analyze three statistical estimators for expected validation performance, a tool used for reporting performance (e.g., accuracy) as a function of computational budget (e.g., number of hyperparameter tuning experiments). Where previous work analyzing such estimators focused on the bias, we also examine the variance and... | Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, Noah A. Smith |  |
| 387 |  |  [How May I Help You? Using Neural Text Simplification to Improve Downstream NLP Tasks](https://doi.org/10.18653/v1/2021.findings-emnlp.343) |  | 0 | The general goal of text simplification (TS) is to reduce text complexity for human consumption. In this paper, we investigate another potential use of neural TS: assisting machines performing natural language processing (NLP) tasks. We evaluate the use of neural TS in two ways: simplifying input texts at prediction time and augmenting data to provide machines with additional information during training. We demonstrate that the latter scenario provides positive effects on machine performance on... | Hoang Van, Zheng Tang, Mihai Surdeanu |  |
| 388 |  |  [Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers](https://doi.org/10.18653/v1/2021.findings-emnlp.344) |  | 0 | Transformers have shown improved performance when compared to previous architectures for sequence processing such as RNNs. Despite their sizeable performance gains, as recently suggested, the model is computationally expensive to train and with a high parameter budget. In light of this, we explore parameter-sharing methods in Transformers with a specific focus on generative models. We perform an analysis of different parameter sharing/reduction methods and develop the Subformer. Our model... | Machel Reid, Edison MarreseTaylor, Yutaka Matsuo |  |
| 389 |  |  [Leveraging Information Bottleneck for Scientific Document Summarization](https://doi.org/10.18653/v1/2021.findings-emnlp.345) |  | 0 | This paper presents an unsupervised extractive approach to summarize scientific long documents based on the Information Bottleneck principle. Inspired by previous work which uses the Information Bottleneck principle for sentence compression, we extend it to document level summarization with two separate steps. In the first step, we use signal(s) as queries to retrieve the key content from the source document. Then, a pre-trained language model conducts further sentence search and edit to return... | Jiaxin Ju, Ming Liu, Huan Yee Koh, Yuan Jin, Lan Du, Shirui Pan |  |
| 390 |  |  [Reconsidering the Past: Optimizing Hidden States in Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.346) |  | 0 | We present Hidden-State Optimization (HSO), a gradient-based method for improving the performance of transformer language models at inference time. Similar to dynamic evaluation (Krause et al., 2018), HSO computes the gradient of the log-probability the language model assigns to an evaluation text, but uses it to update the cached hidden states rather than the model parameters. We test HSO with pretrained Transformer-XL and GPT-2 language models, finding improvement on the WikiText-103 and... | Davis Yoshida, Kevin Gimpel |  |
| 391 |  |  [Attend, Memorize and Generate: Towards Faithful Table-to-Text Generation in Few Shots](https://doi.org/10.18653/v1/2021.findings-emnlp.347) |  | 0 | Few-shot table-to-text generation is a task of composing fluent and faithful sentences to convey table content using limited data. Despite many efforts having been made towards generating impressive fluent sentences by fine-tuning powerful pre-trained language models, the faithfulness of generated content still needs to be improved. To this end, this paper proposes a novel approach Attend, Memorize and Generate (called AMG), inspired by the text generation process of humans. In particular, AMG... | Wenting Zhao, Ye Liu, Yao Wan, Philip S. Yu |  |
| 392 |  |  [ARCH: Efficient Adversarial Regularized Training with Caching](https://doi.org/10.18653/v1/2021.findings-emnlp.348) |  | 0 | Adversarial regularization can improve model generalization in many natural language processing tasks. However, conventional approaches are computationally expensive since they need to generate a perturbation for each sample in each epoch. We propose a new adversarial regularization method ARCH (adversarial regularization with caching), where perturbations are generated and cached once every several epochs. As caching all the perturbations imposes memory usage concerns, we adopt a K-nearest... | Simiao Zuo, Chen Liang, Haoming Jiang, Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, Tuo Zhao |  |
| 393 |  |  [Probing Commonsense Explanation in Dialogue Response Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.349) |  | 0 | Humans use commonsense reasoning (CSR) implicitly to produce natural and coherent responses in conversations. Aiming to close the gap between current response generation (RG) models and human communication abilities, we want to understand why RG models respond as they do by probing RG model’s understanding of commonsense reasoning that elicits proper responses. We formalize the problem by framing commonsense as a latent variable in the RG task and using explanations for responses as textual... | Pei Zhou, Pegah Jandaghi, Hyundong Cho, Bill Yuchen Lin, Jay Pujara, Xiang Ren |  |
| 394 |  |  [NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset](https://doi.org/10.18653/v1/2021.findings-emnlp.350) |  | 0 | While diverse question answering (QA) datasets have been proposed and contributed significantly to the development of deep learning models for QA tasks, the existing datasets fall short in two aspects. First, we lack QA datasets covering complex questions that involve answers as well as the reasoning processes to get them. As a result, the state-of-the-art QA research on numerical reasoning still focuses on simple calculations and does not provide the mathematical expressions or evidence... | Qiyuan Zhang, Lei Wang, Sicheng Yu, Shuohang Wang, Yang Wang, Jing Jiang, EePeng Lim |  |
| 395 |  |  [Textual Time Travel: A Temporally Informed Approach to Theory of Mind](https://doi.org/10.18653/v1/2021.findings-emnlp.351) |  | 0 | Natural language processing systems such as dialogue agents should be able to reason about other people’s beliefs, intentions and desires. This capability, called theory of mind (ToM), is crucial, as it allows a model to predict and interpret the needs of users based on their mental states. A recent line of research evaluates the ToM capability of existing memory-augmented neural models through question-answering. These models perform poorly on false belief tasks where beliefs differ from... | Akshatha Arodi, Jackie Chi Kit Cheung |  |
| 396 |  |  [Detect and Perturb: Neutral Rewriting of Biased and Sensitive Text via Gradient-based Decoding](https://doi.org/10.18653/v1/2021.findings-emnlp.352) |  | 0 | Written language carries explicit and implicit biases that can distract from meaningful signals. For example, letters of reference may describe male and female candidates differently, or their writing style may indirectly reveal demographic characteristics. At best, such biases distract from the meaningful content of the text; at worst they can lead to unfair outcomes. We investigate the challenge of re-generating input sentences to ‘neutralize’ sensitive attributes while maintaining the... | Zexue He, Bodhisattwa Prasad Majumder, Julian J. McAuley |  |
| 397 |  |  [HyperExpan: Taxonomy Expansion with Hyperbolic Representation Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.353) |  | 0 | Taxonomies are valuable resources for many applications, but the limited coverage due to the expensive manual curation process hinders their general applicability. Prior works attempt to automatically expand existing taxonomies to improve their coverage by learning concept embeddings in Euclidean space, while taxonomies, inherently hierarchical, more naturally align with the geometric properties of a hyperbolic space. In this paper, we present HyperExpan, a taxonomy expansion algorithm that... | Mingyu Derek Ma, Muhao Chen, TeLin Wu, Nanyun Peng |  |
| 398 |  |  [Want To Reduce Labeling Cost? GPT-3 Can Help](https://doi.org/10.18653/v1/2021.findings-emnlp.354) |  | 0 | Data annotation is a time-consuming and labor-intensive process for many NLP tasks. Although there exist various methods to produce pseudo data labels, they are often task-specific and require a decent amount of labeled data to start with. Recently, the immense language model GPT-3 with 170 billion parameters has achieved tremendous improvement across many few-shot learning tasks. In this paper, we explore ways to leverage GPT-3 as a low-cost data labeler to train other models. We find that to... | Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, Michael Zeng |  |
| 399 |  |  [Written Justifications are Key to Aggregate Crowdsourced Forecasts](https://doi.org/10.18653/v1/2021.findings-emnlp.355) |  | 0 | This paper demonstrates that aggregating crowdsourced forecasts benefits from modeling the written justifications provided by forecasters. Our experiments show that the majority and weighted vote baselines are competitive, and that the written justifications are beneficial to call a question throughout its life except in the last quarter. We also conduct an error analysis shedding light into the characteristics that make a justification unreliable. | Saketh Kotamraju, Eduardo Blanco |  |
| 400 |  |  [Cleaning Dirty Books: Post-OCR Processing for Previously Scanned Texts](https://doi.org/10.18653/v1/2021.findings-emnlp.356) |  | 0 | Substantial amounts of work are required to clean large collections of digitized books for NLP analysis, both because of the presence of errors in the scanned text and the presence of duplicate volumes in the corpora. In this paper, we consider the issue of deduplication in the presence of optical character recognition (OCR) errors. We present methods to handle these errors, evaluated on a collection of 19,347 texts from the Project Gutenberg dataset and 96,635 texts from the HathiTrust... | Allen Kim, Charuta Pethe, Naoya Inoue, Steven Skiena |  |
| 401 |  |  [Bag of Tricks for Optimizing Transformer Efficiency](https://doi.org/10.18653/v1/2021.findings-emnlp.357) |  | 0 | Improving Transformer efficiency has become increasingly attractive recently. A wide range of methods has been proposed, e.g., pruning, quantization, new architectures and etc. But these methods are either sophisticated in implementation or dependent on hardware. In this paper, we show that the efficiency of Transformer can be improved by combining some simple and hardware-agnostic methods, including tuning hyper-parameters, better design choices and training strategies. On the WMT news... | Ye Lin, Yanyang Li, Tong Xiao, Jingbo Zhu |  |
| 402 |  |  [Non-Parametric Unsupervised Domain Adaptation for Neural Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.358) |  | 0 | Recently, kNN-MT (Khandelwal et al., 2020) has shown the promising capability of directly incorporating the pre-trained neural machine translation (NMT) model with domain-specific token-level k-nearest-neighbor (kNN) retrieval to achieve domain adaptation without retraining. Despite being conceptually attractive, it heavily relies on high-quality in-domain parallel corpora, limiting its capability on unsupervised domain adaptation, where in-domain parallel corpora are scarce or nonexistent. In... | Xin Zheng, Zhirui Zhang, Shujian Huang, Boxing Chen, Jun Xie, Weihua Luo, Jiajun Chen |  |
| 403 |  |  [The Topic Confusion Task: A Novel Evaluation Scenario for Authorship Attribution](https://doi.org/10.18653/v1/2021.findings-emnlp.359) |  | 0 | Authorship attribution is the problem of identifying the most plausible author of an anonymous text from a set of candidate authors. Researchers have investigated same-topic and cross-topic scenarios of authorship attribution, which differ according to whether new, unseen topics are used in the testing phase. However, neither scenario allows us to explain whether errors are caused by failure to capture authorship writing style or by the topic shift. Motivated by this, we propose the topic... | Malik H. Altakrori, Jackie Chi Kit Cheung, Benjamin C. M. Fung |  |
| 404 |  |  [Micromodels for Efficient, Explainable, and Reusable Systems: A Case Study on Mental Health](https://doi.org/10.18653/v1/2021.findings-emnlp.360) |  | 0 | Many statistical models have high accuracy on test benchmarks, but are not explainable, struggle in low-resource scenarios, cannot be reused for multiple tasks, and cannot easily integrate domain expertise. These factors limit their use, particularly in settings such as mental health, where it is difficult to annotate datasets and model outputs have significant impact. We introduce a micromodel architecture to address these challenges. Our approach allows researchers to build interpretable... | Andrew Lee, Jonathan K. Kummerfeld, Larry An, Rada Mihalcea |  |
| 405 |  |  [Discovering Explanatory Sentences in Legal Case Decisions Using Pre-trained Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.361) |  | 0 | Legal texts routinely use concepts that are difficult to understand. Lawyers elaborate on the meaning of such concepts by, among other things, carefully investigating how they have been used in the past. Finding text snippets that mention a particular concept in a useful way is tedious, time-consuming, and hence expensive. We assembled a data set of 26,959 sentences, coming from legal case decisions, and labeled them in terms of their usefulness for explaining selected legal concepts. Using the... | Jaromír Savelka, Kevin D. Ashley |  |
| 406 |  |  [FCM: A Fine-grained Comparison Model for Multi-turn Dialogue Reasoning](https://doi.org/10.18653/v1/2021.findings-emnlp.362) |  | 0 | Despite the success of neural dialogue systems in achieving high performance on the leader-board, they cannot meet users’ requirements in practice, due to their poor reasoning skills. The underlying reason is that most neural dialogue models only capture the syntactic and semantic information, but fail to model the logical consistency between the dialogue history and the generated response. Recently, a new multi-turn dialogue reasoning task has been proposed, to facilitate dialogue reasoning... | Xu Wang, Hainan Zhang, Shuai Zhao, Yanyan Zou, Hongshen Chen, Zhuoye Ding, Bo Cheng, Yanyan Lan |  |
| 407 |  |  [Reference-based Weak Supervision for Answer Sentence Selection using Web Data](https://doi.org/10.18653/v1/2021.findings-emnlp.363) |  | 0 | Answer Sentence Selection (AS2) models are core components of efficient retrieval-based Question Answering (QA) systems. We present the Reference-based Weak Supervision (RWS), a fully automatic large-scale data pipeline that harvests high-quality weakly- supervised answer sentences from Web data, only requiring a question-reference pair as input. We evaluated the quality of the RWS-derived data by training TANDA models, which are the state of the art for AS2. Our results show that the data... | Vivek Krishnamurthy, Thuy Vu, Alessandro Moschitti |  |
| 408 |  |  [A Deep Decomposable Model for Disentangling Syntax and Semantics in Sentence Representation](https://doi.org/10.18653/v1/2021.findings-emnlp.364) |  | 0 | Recently, disentanglement based on a generative adversarial network or a variational autoencoder has significantly advanced the performance of diverse applications in CV and NLP domains. Nevertheless, those models still work on coarse levels in the disentanglement of closely related properties, such as syntax and semantics in human languages. This paper introduces a deep decomposable model based on VAE to disentangle syntax and semantics by using total correlation penalties on KL divergences.... | Dingcheng Li, Hongliang Fei, Shaogang Ren, Ping Li |  |
| 409 |  |  [Improved Word Sense Disambiguation with Enhanced Sense Representations](https://doi.org/10.18653/v1/2021.findings-emnlp.365) |  | 0 | Current state-of-the-art supervised word sense disambiguation (WSD) systems (such as GlossBERT and bi-encoder model) yield surprisingly good results by purely leveraging pre-trained language models and short dictionary definitions (or glosses) of the different word senses. While concise and intuitive, the sense gloss is just one of many ways to provide information about word senses. In this paper, we focus on enhancing the sense representations via incorporating synonyms, example phrases or... | Yang Song, Xin Cai Ong, Hwee Tou Ng, Qian Lin |  |
| 410 |  |  [Rethinking Zero-shot Neural Machine Translation: From a Perspective of Latent Variables](https://doi.org/10.18653/v1/2021.findings-emnlp.366) |  | 0 | Zero-shot translation, directly translating between language pairs unseen in training, is a promising capability of multilingual neural machine translation (NMT). However, it usually suffers from capturing spurious correlations between the output language and language invariant semantics due to the maximum likelihood training objective, leading to poor transfer performance on zero-shot translation. In this paper, we introduce a denoising autoencoder objective based on pivot language into... | Weizhi Wang, Zhirui Zhang, Yichao Du, Boxing Chen, Jun Xie, Weihua Luo |  |
| 411 |  |  [FastCorrect 2: Fast Error Correction on Multiple Candidates for Automatic Speech Recognition](https://doi.org/10.18653/v1/2021.findings-emnlp.367) |  | 0 | Error correction is widely used in automatic speech recognition (ASR) to post-process the generated sentence, and can further reduce the word error rate (WER). Although multiple candidates are generated by an ASR system through beam search, current error correction approaches can only correct one sentence at a time, failing to leverage the voting effect from multiple candidates to better detect and correct error tokens. In this work, we propose FastCorrect 2, an error correction model that... | Yichong Leng, Xu Tan, Rui Wang, Linchen Zhu, Jin Xu, Wenjie Liu, Linquan Liu, XiangYang Li, Tao Qin, Edward Lin, TieYan Liu |  |
| 412 |  |  [Task-Oriented Clustering for Dialogues](https://doi.org/10.18653/v1/2021.findings-emnlp.368) |  | 0 | A reliable clustering algorithm for task-oriented dialogues can help developer analysis and define dialogue tasks efficiently. It is challenging to directly apply prior normal text clustering algorithms for task-oriented dialogues, due to the inherent differences between them, such as coreference, omission and diversity expression. In this paper, we propose a Dialogue Task Clustering Network model for task-oriented clustering. The proposed model combines context-aware utterance representations... | Chenxu Lv, Hengtong Lu, Shuyu Lei, Huixing Jiang, Wei Wu, Caixia Yuan, Xiaojie Wang |  |
| 413 |  |  [Mitigating Data Poisoning in Text Classification with Differential Privacy](https://doi.org/10.18653/v1/2021.findings-emnlp.369) |  | 0 | NLP models are vulnerable to data poisoning attacks. One type of attack can plant a backdoor in a model by injecting poisoned examples in training, causing the victim model to misclassify test instances which include a specific pattern. Although defences exist to counter these attacks, they are specific to an attack type or pattern. In this paper, we propose a generic defence mechanism by making the training process robust to poisoning attacks through gradient shaping methods, based on... | Chang Xu, Jun Wang, Francisco Guzmán, Benjamin I. P. Rubinstein, Trevor Cohn |  |
| 414 |  |  [Does Vision-and-Language Pretraining Improve Lexical Grounding?](https://doi.org/10.18653/v1/2021.findings-emnlp.370) |  | 0 | Linguistic representations derived from text alone have been criticized for their lack of grounding, i.e., connecting words to their meanings in the physical world. Vision-and- Language (VL) models, trained jointly on text and image or video data, have been offered as a response to such criticisms. However, while VL pretraining has shown success on multimodal tasks such as visual question answering, it is not yet known how the internal linguistic representations themselves compare to their... | Tian Yun, Chen Sun, Ellie Pavlick |  |
| 415 |  |  [Character-based PCFG Induction for Modeling the Syntactic Acquisition of Morphologically Rich Languages](https://doi.org/10.18653/v1/2021.findings-emnlp.371) |  | 0 | Unsupervised PCFG induction models, which build syntactic structures from raw text, can be used to evaluate the extent to which syntactic knowledge can be acquired from distributional information alone. However, many state-of-the-art PCFG induction models are word-based, meaning that they cannot directly inspect functional affixes, which may provide crucial information for syntactic acquisition in child learners. This work first introduces a neural PCFG induction model that allows a clean... | Lifeng Jin, ByungDoh Oh, William Schuler |  |
| 416 |  |  [Block-wise Word Embedding Compression Revisited: Better Weighting and Structuring](https://doi.org/10.18653/v1/2021.findings-emnlp.372) |  | 0 | Word embedding is essential for neural network models for various natural language processing tasks. Since the word embedding usually has a considerable size, in order to deploy a neural network model having it on edge devices, it should be effectively compressed. There was a study for proposing a block-wise low-rank approximation method for word embedding, called GroupReduce. Even if their structure is effective, the properties behind the concept of the block-wise word embedding compression... | JongRyul Lee, YongJu Lee, YongHyuk Moon |  |
| 417 |  |  [Switch Point biased Self-Training: Re-purposing Pretrained Models for Code-Switching](https://doi.org/10.18653/v1/2021.findings-emnlp.373) |  | 0 | Code-switching (CS), a ubiquitous phenomenon due to the ease of communication it offers in multilingual communities still remains an understudied problem in language processing. The primary reasons behind this are: (1) minimal efforts in leveraging large pretrained multilingual models, and (2) the lack of annotated data. The distinguishing case of low performance of multilingual models in CS is the intra-sentence mixing of languages leading to switch points. We first benchmark two sequence... | Parul Chopra, Sai Krishna Rallabandi, Alan W. Black, Khyathi Raghavi Chandu |  |
| 418 |  |  [Influence Tuning: Demoting Spurious Correlations via Instance Attribution and Instance-Driven Updates](https://doi.org/10.18653/v1/2021.findings-emnlp.374) |  | 0 | Among the most critical limitations of deep learning NLP models are their lack of interpretability, and their reliance on spurious correlations. Prior work proposed various approaches to interpreting the black-box models to unveil the spurious correlations, but the research was primarily used in human-computer interaction scenarios. It still remains underexplored whether or how such model interpretations can be used to automatically “unlearn” confounding features. In this work, we propose... | Xiaochuang Han, Yulia Tsvetkov |  |
| 419 |  |  [Learning Task Sampling Policy for Multitask Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.375) |  | 0 | It has been shown that training multi-task models with auxiliary tasks can improve the target task quality through cross-task transfer. However, the importance of each auxiliary task to the primary task is likely not known a priori. While the importance weights of auxiliary tasks can be manually tuned, it becomes practically infeasible with the number of tasks scaling up. To address this, we propose a search method that automatically assigns importance weights. We formulate it as a... | Dhanasekar Sundararaman, Henry Tsai, KuangHuei Lee, Iulia Turc, Lawrence Carin |  |
| 420 |  |  [Competing Independent Modules for Knowledge Integration and Optimization](https://doi.org/10.18653/v1/2021.findings-emnlp.376) |  | 0 | This paper presents a neural framework of untied independent modules, used here for integrating off the shelf knowledge sources such as language models, lexica, POS information, and dependency relations. Each knowledge source is implemented as an independent component that can interact and share information with other knowledge sources. We report proof of concept experiments for several standard sentiment analysis tasks and show that the knowledge sources interoperate effectively without... | Parsa Bagherzadeh, Sabine Bergler |  |
| 421 |  |  [An Exploratory Study on Long Dialogue Summarization: What Works and What's Next](https://doi.org/10.18653/v1/2021.findings-emnlp.377) |  | 0 | Dialogue summarization helps readers capture salient information from long conversations in meetings, interviews, and TV series. However, real-world dialogues pose a great challenge to current summarization models, as the dialogue length typically exceeds the input limits imposed by recent transformer-based pre-trained models, and the interactive nature of dialogues makes relevant information more context-dependent and sparsely distributed than news articles. In this work, we perform a... | Yusen Zhang, Ansong Ni, Tao Yu, Rui Zhang, Chenguang Zhu, Budhaditya Deb, Asli Celikyilmaz, Ahmed Hassan Awadallah, Dragomir R. Radev |  |
| 422 |  |  [Improving Text Auto-Completion with Next Phrase Prediction](https://doi.org/10.18653/v1/2021.findings-emnlp.378) |  | 0 | Language models such as GPT-2 have performed well on constructing syntactically sound sentences for text auto-completion tasks. However, such models often require considerable training effort to adapt to specific writing domains (e.g., medical). In this paper, we propose an intermediate training strategy to enhance pre-trained language models’ performance in the text auto-completion task and fastly adapt them to specific domains. Our strategy includes a novel self-supervised training objective... | DongHo Lee, Zhiqiang Hu, Roy KaWei Lee |  |
| 423 |  |  [MOMENTA: A Multimodal Framework for Detecting Harmful Memes and Their Targets](https://doi.org/10.18653/v1/2021.findings-emnlp.379) |  | 0 | Internet memes have become powerful means to transmit political, psychological, and socio-cultural ideas. Although memes are typically humorous, recent days have witnessed an escalation of harmful memes used for trolling, cyberbullying, and abuse. Detecting such memes is challenging as they can be highly satirical and cryptic. Moreover, while previous work has focused on specific aspects of memes such as hate speech and propaganda, there has been little work on harm in general. Here, we aim to... | Shraman Pramanick, Shivam Sharma, Dimitar Dimitrov, Md. Shad Akhtar, Preslav Nakov, Tanmoy Chakraborty |  |
| 424 |  |  [NICE: Neural Image Commenting with Empathy](https://doi.org/10.18653/v1/2021.findings-emnlp.380) |  | 0 | Emotion and empathy are examples of human qualities lacking in many human-machine interactions. The goal of our work is to generate engaging dialogue grounded in a user-shared image with increased emotion and empathy while minimizing socially inappropriate or offensive outputs. We release the Neural Image Commenting with Empathy (NICE) dataset consisting of almost two million images and the corresponding human-generated comments, a set of human annotations, and baseline performance on a range... | Kezhen Chen, Qiuyuan Huang, Daniel McDuff, Xiang Gao, Hamid Palangi, Jianfeng Wang, Kenneth D. Forbus, Jianfeng Gao |  |
| 425 |  |  [HAConvGNN: Hierarchical Attention Based Convolutional Graph Neural Network for Code Documentation Generation in Jupyter Notebooks](https://doi.org/10.18653/v1/2021.findings-emnlp.381) |  | 0 | Jupyter notebook allows data scientists to write machine learning code together with its documentation in cells. In this paper, we propose a new task of code documentation generation (CDG) for computational notebooks. In contrast to the previous CDG tasks which focus on generating documentation for single code snippets, in a computational notebook, one documentation in a markdown cell often corresponds to multiple code cells, and these code cells have an inherent structure. We proposed a new... | Xuye Liu, Dakuo Wang, April Yi Wang, Yufang Hou, Lingfei Wu |  |
| 426 |  |  [A multilabel approach to morphosyntactic probing](https://doi.org/10.18653/v1/2021.findings-emnlp.382) |  | 0 | We propose using a multilabel probing task to assess the morphosyntactic representations of multilingual word embeddings. This tweak on canonical probing makes it easy to explore morphosyntactic representations, both holistically and at the level of individual features (e.g., gender, number, case), and leads more naturally to the study of how language models handle co-occurring features (e.g., agreement phenomena). We demonstrate this task with multilingual BERT (Devlin et al., 2018), training... | Naomi Tachikawa Shapiro, Amandalynne Paullada, Shane SteinertThrelkeld |  |
| 427 |  |  [Co-Teaching Student-Model through Submission Results of Shared Task](https://doi.org/10.18653/v1/2021.findings-emnlp.383) |  | 0 | Shared tasks have a long history and have become the mainstream of NLP research. Most of the shared tasks require participants to submit only system outputs and descriptions. It is uncommon for the shared task to request submission of the system itself because of the license issues and implementation differences. Therefore, many systems are abandoned without being used in real applications or contributing to better systems. In this research, we propose a scheme to utilize all those systems... | Kouta Nakayama, Shuhei Kurita, Akio Kobayashi, Yukino Baba, Satoshi Sekine |  |
| 428 |  |  [KLMo: Knowledge Graph Enhanced Pretrained Language Model with Fine-Grained Relationships](https://doi.org/10.18653/v1/2021.findings-emnlp.384) |  | 0 | Interactions between entities in knowledge graph (KG) provide rich knowledge for language representation learning. However, existing knowledge-enhanced pretrained language models (PLMs) only focus on entity information and ignore the fine-grained relationships between entities. In this work, we propose to incorporate KG (including both entities and relations) into the language learning process to obtain KG-enhanced pretrained Language Model, namely KLMo. Specifically, a novel knowledge... | Lei He, Suncong Zheng, Tao Yang, Feng Zhang |  |
| 429 |  |  [Do We Know What We Don't Know? Studying Unanswerable Questions beyond SQuAD 2.0](https://doi.org/10.18653/v1/2021.findings-emnlp.385) |  | 0 | Understanding when a text snippet does not provide a sought after information is an essential part of natural language utnderstanding. Recent work (SQuAD 2.0; Rajpurkar et al., 2018) has attempted to make some progress in this direction by enriching the SQuAD dataset for the Extractive QA task with unanswerable questions. However, as we show, the performance of a top system trained on SQuAD 2.0 drops considerably in out-of-domain scenarios, limiting its use in practical situations. In order to... | Elior Sulem, Jamaal Hay, Dan Roth |  |
| 430 |  |  [Glyph Enhanced Chinese Character Pre-Training for Lexical Sememe Prediction](https://doi.org/10.18653/v1/2021.findings-emnlp.386) |  | 0 | Sememes are defined as the atomic units to describe the semantic meaning of concepts. Due to the difficulty of manually annotating sememes and the inconsistency of annotations between experts, the lexical sememe prediction task has been proposed. However, previous methods heavily rely on word or character embeddings, and ignore the fine-grained information. In this paper, we propose a novel pre-training method which is designed to better incorporate the internal information of Chinese... | Boer Lyu, Lu Chen, Kai Yu |  |
| 431 |  |  [Active Learning for Rumor Identification on Social Media](https://doi.org/10.18653/v1/2021.findings-emnlp.387) |  | 0 | Social media has emerged as a key channel for seeking information. Online users spend several hours reading, posting, and searching for news on microblogging platforms daily. However, this could act as a double-edged sword especially when not all information online is reliable. Moreover, the inherently unmoderated nature of social media renders identifying unverified information ever more challenging. Most of the existing approaches for rumor tracking are not scalable because of their... | Parsa Farinneya, Mohammad Mahdi Abdollah Pour, Sardar Hamidian, Mona T. Diab |  |
| 432 |  |  [Cross-Domain Data Integration for Named Entity Disambiguation in Biomedical Text](https://doi.org/10.18653/v1/2021.findings-emnlp.388) |  | 0 | Named entity disambiguation (NED), which involves mapping textual mentions to structured entities, is particularly challenging in the medical domain due to the presence of rare entities. Existing approaches are limited by the presence of coarse-grained structural resources in biomedical knowledge bases as well as the use of training datasets that provide low coverage over uncommon resources. In this work, we address these issues by proposing a cross-domain data integration method that transfers... | Maya Varma, Laurel J. Orr, Sen Wu, Megan Leszczynski, Xiao Ling, Christopher Ré |  |
| 433 |  |  [Self-Training using Rules of Grammar for Few-Shot NLU](https://doi.org/10.18653/v1/2021.findings-emnlp.389) |  | 0 | We tackle the problem of self-training networks for NLU in low-resource environment—few labeled data and lots of unlabeled data. The effectiveness of self-training is a result of increasing the amount of training data while training. Yet it becomes less effective in low-resource settings due to unreliable labels predicted by the teacher model on unlabeled data. Rules of grammar, which describe the grammatical structure of data, have been used in NLU for better explainability. We propose to use... | Joonghyuk Hahn, Hyunjoon Cheon, Kyuyeol Han, Cheongjae Lee, Junseok Kim, YoSub Han |  |
| 434 |  |  [Aspect-based Sentiment Analysis in Question Answering Forums](https://doi.org/10.18653/v1/2021.findings-emnlp.390) |  | 0 | Aspect-based sentiment analysis (ABSA) typically focuses on extracting aspects and predicting their sentiments on individual sentences such as customer reviews. Recently, another kind of opinion sharing platform, namely question answering (QA) forum, has received increasing popularity, which accumulates a large number of user opinions towards various aspects. This motivates us to investigate the task of ABSA on QA forums (ABSA-QA), aiming to jointly detect the discussed aspects and their... | Wenxuan Zhang, Yang Deng, Xin Li, Lidong Bing, Wai Lam |  |
| 435 |  |  [ForumSum: A Multi-Speaker Conversation Summarization Dataset](https://doi.org/10.18653/v1/2021.findings-emnlp.391) |  | 0 | Abstractive summarization quality had large improvements since recent language pretraining techniques. However, currently there is a lack of datasets for the growing needs of conversation summarization applications. Thus we collected ForumSum, a diverse and high-quality conversation summarization dataset with human written summaries. The conversations in ForumSum dataset are collected from a wide variety of internet forums. To make the dataset easily expandable, we also release the process of... | Misha Khalman, Yao Zhao, Mohammad Saleh |  |
| 436 |  |  [Question Answering over Electronic Devices: A New Benchmark Dataset and a Multi-Task Learning based QA Framework](https://doi.org/10.18653/v1/2021.findings-emnlp.392) |  | 0 | Answering questions asked from instructional corpora such as E-manuals, recipe books, etc., has been far less studied than open-domain factoid context-based question answering. This can be primarily attributed to the absence of standard benchmark datasets. In this paper, we meticulously create a large amount of data connected with E-manuals and develop a suitable algorithm to exploit it. We collect E-Manual Corpus, a huge corpus of 307,957 E-manuals, and pretrain RoBERTa on this large corpus.... | Abhilash Nandy, Soumya Sharma, Shubham Maddhashiya, Kapil Sachdeva, Pawan Goyal, Niloy Ganguly |  |
| 437 |  |  [Comprehensive Punctuation Restoration for English and Polish](https://doi.org/10.18653/v1/2021.findings-emnlp.393) |  | 0 | Punctuation restoration is a fundamental requirement for the readability of text derived from Automatic Speech Recognition (ASR) systems. Most contemporary solutions are limited to predicting only a few of the most frequently occurring marks, such as periods, commas, and question marks - and only one per word. However, in written language, we deal with a much larger number of punctuation characters (such as parentheses, hyphens, etc.), and their combinations (like parenthesis followed by a... | Michal Pogoda, Tomasz Walkowiak |  |
| 438 |  |  [Syntactically Diverse Adversarial Network for Knowledge-Grounded Conversation Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.394) |  | 0 | Generative conversation systems tend to produce meaningless and generic responses, which significantly reduce the user experience. In order to generate informative and diverse responses, recent studies proposed to fuse knowledge to improve informativeness and adopt latent variables to enhance the diversity. However, utilizing latent variables will lead to the inaccuracy of knowledge in the responses, and the dissemination of wrong knowledge will mislead the communicators. To address this... | Fuwei Cui, Hui Di, Hongjie Ren, Kazushige Ouchi, Ze Liu, Jinan Xu |  |
| 439 |  |  [QACE: Asking Questions to Evaluate an Image Caption](https://doi.org/10.18653/v1/2021.findings-emnlp.395) |  | 0 | In this paper we propose QACE, a new metric based on Question Answering for Caption Evaluation to evaluate image captioning based on Question Generation(QG) and Question Answering(QA) systems. QACE generates questions on the evaluated caption and check its content by asking the questions on either the reference caption or the source image. We first develop QACE_Ref that compares the answers of the evaluated caption to its reference, and report competitive results with the state-of-the-art... | Hwanhee Lee, Thomas Scialom, Seunghyun Yoon, Franck Dernoncourt, Kyomin Jung |  |
| 440 |  |  [Secoco: Self-Correcting Encoding for Neural Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.396) |  | 0 | This paper presents Self-correcting Encoding (Secoco), a framework that effectively deals with noisy input for robust neural machine translation by introducing self-correcting predictors. Different from previous robust approaches, Secoco enables NMT to explicitly correct noisy inputs and delete specific errors simultaneously with the translation decoding process. Secoco is able to achieve significant improvements over strong baselines on two real-world test sets and a benchmark WMT dataset with... | Tao Wang, Chengqi Zhao, Mingxuan Wang, Lei Li, Hang Li, Deyi Xiong |  |
| 441 |  |  [Simple or Complex? Complexity-controllable Question Generation with Soft Templates and Deep Mixture of Experts Model](https://doi.org/10.18653/v1/2021.findings-emnlp.397) |  | 0 | The ability to generate natural-language questions with controlled complexity levels is highly desirable as it further expands the applicability of question generation. In this paper, we propose an end-to-end neural complexity-controllable question generation model, which incorporates a mixture of experts (MoE) as the selector of soft templates to improve the accuracy of complexity control and the quality of generated questions. The soft templates capture question similarity while avoiding the... | Sheng Bi, Xiya Cheng, YuanFang Li, Lizhen Qu, Shirong Shen, Guilin Qi, Lu Pan, Yinlin Jiang |  |
| 442 |  |  [Predicting Anti-Asian Hateful Users on Twitter during COVID-19](https://doi.org/10.18653/v1/2021.findings-emnlp.398) |  | 0 | We investigate predictors of anti-Asian hate among Twitter users throughout COVID-19. With the rise of xenophobia and polarization that has accompanied widespread social media usage in many nations, online hate has become a major social issue, attracting many researchers. Here, we apply natural language processing techniques to characterize social media users who began to post anti-Asian hate messages during COVID-19. We compare two user groups—those who posted anti-Asian slurs and those who... | Jisun An, Haewoon Kwak, Claire Seungeun Lee, Bogang Jun, YongYeol Ahn |  |
| 443 |  |  [Fine-grained Typing of Emerging Entities in Microblogs](https://doi.org/10.18653/v1/2021.findings-emnlp.399) |  | 0 | Analyzing microblogs where we post what we experience enables us to perform various applications such as social-trend analysis and entity recommendation. To track emerging trends in a variety of areas, we want to categorize information on emerging entities (e.g., Avatar 2) in microblog posts according to their types (e.g., Film). We thus introduce a new entity typing task that assigns a fine-grained type to each emerging entity when a burst of posts containing that entity is first observed in a... | Satoshi Akasaki, Naoki Yoshinaga, Masashi Toyoda |  |
| 444 |  |  [Data-Efficient Language Shaped Few-shot Image Classification](https://doi.org/10.18653/v1/2021.findings-emnlp.400) |  | 0 | Many existing works have demonstrated that language is a helpful guider for image understanding by neural networks. We focus on a language-shaped learning problem in a few-shot setting, i.e., using language to improve few-shot image classification when language descriptions are only available during training. We propose a data-efficient method that can make the best usage of the few-shot images and the language available only in training. Experimental results on dataset ShapeWorld and Birds... | Zhenwen Liang, Xiangliang Zhang |  |
| 445 |  |  [Beyond Glass-Box Features: Uncertainty Quantification Enhanced Quality Estimation for Neural Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.401) |  | 0 | Quality Estimation (QE) plays an essential role in applications of Machine Translation (MT). Traditionally, a QE system accepts the original source text and translation from a black-box MT system as input. Recently, a few studies indicate that as a by-product of translation, QE benefits from the model and training data’s information of the MT system where the translations come from, and it is called the “glass-box QE”. In this paper, we extend the definition of “glass-box QE” generally to... | Ke Wang, Yangbin Shi, Jiayi Wang, Yuqi Zhang, Yu Zhao, Xiaolin Zheng |  |
| 446 |  |  [Fight Fire with Fire: Fine-tuning Hate Detectors using Large Samples of Generated Hate Speech](https://doi.org/10.18653/v1/2021.findings-emnlp.402) |  | 0 | Automatic hate speech detection is hampered by the scarcity of labeled datasetd, leading to poor generalization. We employ pretrained language models (LMs) to alleviate this data bottleneck. We utilize the GPT LM for generating large amounts of synthetic hate speech sequences from available labeled examples, and leverage the generated data in fine-tuning large pretrained LMs on hate detection. An empirical study using the models of BERT, RoBERTa and ALBERT, shows that this approach improves... | Tomer Wullach, Amir Adler, Einat Minkov |  |
| 447 |  |  [AutoEQA: Auto-Encoding Questions for Extractive Question Answering](https://doi.org/10.18653/v1/2021.findings-emnlp.403) |  | 0 | There has been a significant progress in the field of Extractive Question Answering (EQA) in the recent years. However, most of them are reliant on annotations of answer-spans in the corresponding passages. In this work, we address the problem of EQA when no annotations are present for the answer span, i.e., when the dataset contains only questions and corresponding passages. Our method is based on auto-encoding of the question that performs a question answering task during encoding and a... | Stalin Varanasi, Saadullah Amin, Guenter Neumann |  |
| 448 |  |  [A Multi-label Multi-hop Relation Detection Model based on Relation-aware Sequence Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.404) |  | 0 | Multi-hop relation detection in Knowledge Base Question Answering (KBQA) aims at retrieving the relation path starting from the topic entity to the answer node based on a given question, where the relation path may comprise multiple relations. Most of the existing methods treat it as a single-label learning problem while ignoring the fact that for some complex questions, there exist multiple correct relation paths in knowledge bases. Therefore, in this paper, multi-hop relation detection is... | Linhai Zhang, Deyu Zhou, Chao Lin, Yulan He |  |
| 449 |  |  [Don't Discard All the Biased Instances: Investigating a Core Assumption in Dataset Bias Mitigation Techniques](https://doi.org/10.18653/v1/2021.findings-emnlp.405) |  | 0 | Existing techniques for mitigating dataset bias often leverage a biased model to identify biased instances. The role of these biased instances is then reduced during the training of the main model to enhance its robustness to out-of-distribution data. A common core assumption of these techniques is that the main model handles biased instances similarly to the biased model, in that it will resort to biases whenever available. In this paper, we show that this assumption does not hold in general.... | Hossein Amirkhani, Mohammad Taher Pilehvar |  |
| 450 |  |  [Stacked AMR Parsing with Silver Data](https://doi.org/10.18653/v1/2021.findings-emnlp.406) |  | 0 | Lacking sufficient human-annotated data is one main challenge for abstract meaning representation (AMR) parsing. To alleviate this problem, previous works usually make use of silver data or pre-trained language models. In particular, one recent seq-to-seq work directly fine-tunes AMR graph sequences on the encoder-decoder pre-trained language model and achieves new state-of-the-art results, outperforming previous works by a large margin. However, it makes the decoding relatively slower. In this... | Qingrong Xia, Zhenghua Li, Rui Wang, Min Zhang |  |
| 451 |  |  [Speculative Sampling in Variational Autoencoders for Dialogue Response Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.407) |  | 0 | Variational autoencoders have been studied as a promising approach to model one-to-many mappings from context to response in chat response generation. However, they often fail to learn proper mappings. One of the reasons for this failure is the discrepancy between a response and a latent variable sampled from an approximated distribution in training. Inappropriately sampled latent variables hinder models from constructing a modulated latent space. As a result, the models stop handling... | Shoetsu Sato, Naoki Yoshinaga, Masashi Toyoda, Masaru Kitsuregawa |  |
| 452 |  |  [Perceived and Intended Sarcasm Detection with Graph Attention Networks](https://doi.org/10.18653/v1/2021.findings-emnlp.408) |  | 0 | Existing sarcasm detection systems focus on exploiting linguistic markers, context, or user-level priors. However, social studies suggest that the relationship between the author and the audience can be equally relevant for the sarcasm usage and interpretation. In this work, we propose a framework jointly leveraging (1) a user context from their historical tweets together with (2) the social information from a user’s neighborhood in an interaction graph, to contextualize the interpretation of... | Joan Plepi, Lucie Flek |  |
| 453 |  |  [Contrastive Representation Learning for Exemplar-Guided Paraphrase Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.409) |  | 0 | Exemplar-Guided Paraphrase Generation (EGPG) aims to generate a target sentence which conforms to the style of the given exemplar while encapsulating the content information of the source sentence. In this paper, we propose a new method with the goal of learning a better representation of the style and the content. This method is mainly motivated by the recent success of contrastive learning which has demonstrated its power in unsupervised feature extraction tasks. The idea is to design two... | Haoran Yang, Wai Lam, Piji Li |  |
| 454 |  |  [MAD-G: Multilingual Adapter Generation for Efficient Cross-Lingual Transfer](https://doi.org/10.18653/v1/2021.findings-emnlp.410) |  | 0 | Adapter modules have emerged as a general parameter-efficient means to specialize a pretrained encoder to new domains. Massively multilingual transformers (MMTs) have particularly benefited from additional training of language-specific adapters. However, this approach is not viable for the vast majority of languages, due to limitations in their corpus size or compute budgets. In this work, we propose MAD-G (Multilingual ADapter Generation), which contextually generates language adapters from... | Alan Ansell, Edoardo Maria Ponti, Jonas Pfeiffer, Sebastian Ruder, Goran Glavas, Ivan Vulic, Anna Korhonen |  |
| 455 |  |  [Sustainable Modular Debiasing of Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.411) |  | 0 | Unfair stereotypical biases (e.g., gender, racial, or religious biases) encoded in modern pretrained language models (PLMs) have negative ethical implications for widespread adoption of state-of-the-art language technology. To remedy for this, a wide range of debiasing techniques have recently been introduced to remove such stereotypical biases from PLMs. Existing debiasing methods, however, directly modify all of the PLMs parameters, which – besides being computationally expensive – comes with... | Anne Lauscher, Tobias Lüken, Goran Glavas |  |
| 456 |  |  [A Divide-And-Conquer Approach for Multi-label Multi-hop Relation Detection in Knowledge Base Question Answering](https://doi.org/10.18653/v1/2021.findings-emnlp.412) |  | 0 | Relation detection in knowledge base question answering, aims to identify the path(s) of relations starting from the topic entity node that is linked to the answer node in knowledge graph. Such path might consist of multiple relations, which we call multi-hop. Moreover, for a single question, there may exist multiple relation paths to the correct answer, which we call multi-label. However, most of existing approaches only detect one single path to obtain the answer without considering other... | Deyu Zhou, Yanzheng Xiang, Linhai Zhang, Chenchen Ye, QianWen Zhang, Yunbo Cao |  |
| 457 |  |  [Counterfactual Adversarial Learning with Representation Interpolation](https://doi.org/10.18653/v1/2021.findings-emnlp.413) |  | 0 | Deep learning models exhibit a preference for statistical fitting over logical reasoning. Spurious correlations might be memorized when there exists statistical bias in training data, which severely limits the model performance especially in small data scenarios. In this work, we introduce Counterfactual Adversarial Training framework (CAT) to tackle the problem from a causality perspective. Particularly, for a specific sample, CAT first generates a counterfactual representation through latent... | Wei Wang, Boxin Wang, Ning Shi, Jinfeng Li, Bingyu Zhu, Xiangyu Liu, Rong Zhang |  |
| 458 |  |  ['Just What do You Think You're Doing, Dave?' A Checklist for Responsible Data Use in NLP](https://doi.org/10.18653/v1/2021.findings-emnlp.414) |  | 0 | A key part of the NLP ethics movement is responsible use of data, but exactly what that means or how it can be best achieved remain unclear. This position paper discusses the core legal and ethical principles for collection and sharing of textual data, and the tensions between them. We propose a potential checklist for responsible data (re-)use that could both standardise the peer review of conference submissions, as well as enable a more in-depth view of published research across the... | Anna Rogers, Timothy Baldwin, Kobi Leins |  |
| 459 |  |  [Counter-Contrastive Learning for Language GANs](https://doi.org/10.18653/v1/2021.findings-emnlp.415) |  | 0 | Generative Adversarial Networks (GANs) have achieved great success in image synthesis, but have proven to be difficult to generate natural language. Challenges arise from the uninformative learning signals passed from the discriminator. In other words, the poor learning signals limit the learning capacity for generating languages with rich structures and semantics. In this paper, we propose to adopt the counter-contrastive learning (CCL) method to support the generator’s training in language... | Yekun Chai, Haidong Zhang, Qiyue Yin, Junge Zhang |  |
| 460 |  |  [Incorporating Circumstances into Narrative Event Prediction](https://doi.org/10.18653/v1/2021.findings-emnlp.416) |  | 0 | The narrative event prediction aims to predict what happens after a sequence of events, which is essential to modeling sophisticated real-world events. Existing studies focus on mining the inter-events relationships while ignoring how the events happened, which we called circumstances. With our observation, the event circumstances indicate what will happen next. To incorporate event circumstances into the narrative event prediction, we propose the CircEvent, which adopts the two multi-head... | Shichao Wang, Xiangrui Cai, Hongbin Wang, Xiaojie Yuan |  |
| 461 |  |  [MultiFix: Learning to Repair Multiple Errors by Optimal Alignment Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.417) |  | 0 | We consider the problem of learning to repair erroneous C programs by learning optimal alignments with correct programs. Since the previous approaches fix a single error in a line, it is inevitable to iterate the fixing process until no errors remain. In this work, we propose a novel sequence-to-sequence learning framework for fixing multiple program errors at a time. We introduce the edit-distance-based data labeling approach for program error correction. Instead of labeling a program repair... | HyeonTae Seo, YoSub Han, SangKi Ko |  |
| 462 |  |  [HOTTER: Hierarchical Optimal Topic Transport with Explanatory Context Representations](https://doi.org/10.18653/v1/2021.findings-emnlp.418) |  | 0 | Natural language processing (NLP) is often the backbone of today’s systems for user interactions, information retrieval and others. Many of such NLP applications rely on specialized learned representations (e.g. neural word embeddings, topic models) that improve the ability to reason about the relationships between documents of a corpus. Paired with the progress in learned representations, the similarity metrics used to compare representations of documents are also evolving, with numerous... | Sabine Wehnert, Christian Scheel, Simona SzakácsBehling, Maret Nieländer, Patrick Mielke, Ernesto William De Luca |  |
| 463 |  |  [Grammatical Error Correction with Contrastive Learning in Low Error Density Domains](https://doi.org/10.18653/v1/2021.findings-emnlp.419) |  | 0 | Although grammatical error correction (GEC) has achieved good performance on texts written by learners of English as a second language, performance on low error density domains where texts are written by English speakers of varying levels of proficiency can still be improved. In this paper, we propose a contrastive learning approach to encourage the GEC model to assign a higher probability to a correct sentence while reducing the probability of incorrect sentences that the model tends to... | Hannan Cao, Wenmian Yang, Hwee Tou Ng |  |
| 464 |  |  [Improving Unsupervised Commonsense Reasoning Using Knowledge-Enabled Natural Language Inference](https://doi.org/10.18653/v1/2021.findings-emnlp.420) |  | 0 | Recent methods based on pre-trained language models have shown strong supervised performance on commonsense reasoning. However, they rely on expensive data annotation and time-consuming training. Thus, we focus on unsupervised commonsense reasoning. We show the effectiveness of using a common framework, Natural Language Inference (NLI), to solve diverse commonsense reasoning tasks. By leveraging transfer learning from large NLI datasets, and injecting crucial knowledge from commonsense sources... | Canming Huang, Weinan He, Yongmei Liu |  |
| 465 |  |  [Does Putting a Linguist in the Loop Improve NLU Data Collection?](https://doi.org/10.18653/v1/2021.findings-emnlp.421) |  | 0 | Many crowdsourced NLP datasets contain systematic artifacts that are identified only after data collection is complete. Earlier identification of these issues should make it easier to create high-quality training and evaluation data. We attempt this by evaluating protocols in which expert linguists work ‘in the loop’ during data collection to identify and address these issues by adjusting task instructions and incentives. Using natural language inference as a test case, we compare three data... | Alicia Parrish, William Huang, Omar Agha, SooHwan Lee, Nikita Nangia, Alex Warstadt, Karmanya Aggarwal, Emily Allaway, Tal Linzen, Samuel R. Bowman |  |
| 466 |  |  [Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding](https://doi.org/10.18653/v1/2021.findings-emnlp.422) |  | 0 | Large-scale, pre-trained language models (LMs) have achieved human-level performance on a breadth of language understanding tasks. However, evaluations only based on end task performance shed little light on machines’ true ability in language understanding and reasoning. In this paper, we highlight the importance of evaluating the underlying reasoning process in addition to end performance. Toward this goal, we introduce Tiered Reasoning for Intuitive Physics (TRIP), a novel commonsense... | Shane Storks, Qiaozi Gao, Yichi Zhang, Joyce Chai |  |
| 467 |  |  [Making Heads and Tails of Models with Marginal Calibration for Sparse Tagsets](https://doi.org/10.18653/v1/2021.findings-emnlp.423) |  | 0 | For interpreting the behavior of a probabilistic model, it is useful to measure a model’s calibration—the extent to which it produces reliable confidence scores. We address the open problem of calibration for tagging models with sparse tagsets, and recommend strategies to measure and reduce calibration error (CE) in such models. We show that several post-hoc recalibration techniques all reduce calibration error across the marginal distribution for two existing sequence taggers. Moreover, we... | Michael Kranzlein, Nelson F. Liu, Nathan Schneider |  |
| 468 |  |  [GeDi: Generative Discriminator Guided Sequence Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.424) |  | 0 |  | Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq R. Joty, Richard Socher, Nazneen Fatema Rajani |  |
| 469 |  |  [Frontmatter](https://aclanthology.org/2021.emnlp-main.0) |  | 0 |  |  |  |
| 470 |  |  [AligNART: Non-autoregressive Neural Machine Translation by Jointly Learning to Estimate Alignment and Translate](https://doi.org/10.18653/v1/2021.emnlp-main.1) |  | 0 | Non-autoregressive neural machine translation (NART) models suffer from the multi-modality problem which causes translation inconsistency such as token repetition. Most recent approaches have attempted to solve this problem by implicitly modeling dependencies between outputs. In this paper, we introduce AligNART, which leverages full alignment information to explicitly reduce the modality of the target distribution. AligNART divides the machine translation task into (i) alignment estimation and... | Jongyoon Song, Sungwon Kim, Sungroh Yoon |  |
| 471 |  |  [Zero-Shot Cross-Lingual Transfer of Neural Machine Translation with Multilingual Pretrained Encoders](https://doi.org/10.18653/v1/2021.emnlp-main.2) |  | 0 | Previous work mainly focuses on improving cross-lingual transfer for NLU tasks with a multilingual pretrained encoder (MPE), or improving the performance on supervised machine translation with BERT. However, it is under-explored that whether the MPE can help to facilitate the cross-lingual transferability of NMT model. In this paper, we focus on a zero-shot cross-lingual transfer task in NMT. In this task, the NMT model is trained with parallel dataset of only one language pair and an... | Guanhua Chen, Shuming Ma, Yun Chen, Li Dong, Dongdong Zhang, Jia Pan, Wenping Wang, Furu Wei |  |
| 472 |  |  [ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora](https://doi.org/10.18653/v1/2021.emnlp-main.3) |  | 0 | Recent studies have demonstrated that pre-trained cross-lingual models achieve impressive performance in downstream cross-lingual tasks. This improvement benefits from learning a large amount of monolingual and parallel corpora. Although it is generally acknowledged that parallel corpora are critical for improving the model performance, existing methods are often constrained by the size of parallel corpora, especially for low-resource languages. In this paper, we propose Ernie-M, a new training... | Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang |  |
| 473 |  |  [Cross Attention Augmented Transducer Networks for Simultaneous Translation](https://doi.org/10.18653/v1/2021.emnlp-main.4) |  | 0 | This paper proposes a novel architecture, Cross Attention Augmented Transducer (CAAT), for simultaneous translation. The framework aims to jointly optimize the policy and translation models. To effectively consider all possible READ-WRITE simultaneous translation action paths, we adapt the online automatic speech recognition (ASR) model, RNN-T, but remove the strong monotonic constraint, which is critical for the translation task to consider reordering. To make CAAT work, we introduce a novel... | Dan Liu, Mengge Du, Xiaoxi Li, Ya Li, Enhong Chen |  |
| 474 |  |  [Translating Headers of Tabular Data: A Pilot Study of Schema Translation](https://doi.org/10.18653/v1/2021.emnlp-main.5) |  | 0 | Schema translation is the task of automatically translating headers of tabular data from one language to another. High-quality schema translation plays an important role in cross-lingual table searching, understanding and analysis. Despite its importance, schema translation is not well studied in the community, and state-of-the-art neural machine translation models cannot work well on this task because of two intrinsic differences between plain text and tabular data: morphological difference... | Kunrui Zhu, Yan Gao, Jiaqi Guo, JianGuang Lou |  |
| 475 |  |  [Towards Making the Most of Dialogue Characteristics for Neural Chat Translation](https://doi.org/10.18653/v1/2021.emnlp-main.6) |  | 0 | Neural Chat Translation (NCT) aims to translate conversational text between speakers of different languages. Despite the promising performance of sentence-level and context-aware neural machine translation models, there still remain limitations in current NCT models because the inherent dialogue characteristics of chat, such as dialogue coherence and speaker personality, are neglected. In this paper, we propose to promote the chat translation by introducing the modeling of dialogue... | Yunlong Liang, Chulun Zhou, Fandong Meng, Jinan Xu, Yufeng Chen, Jinsong Su, Jie Zhou |  |
| 476 |  |  [Low-Resource Dialogue Summarization with Domain-Agnostic Multi-Source Pretraining](https://doi.org/10.18653/v1/2021.emnlp-main.7) |  | 0 | With the rapid increase in the volume of dialogue data from daily life, there is a growing demand for dialogue summarization. Unfortunately, training a large summarization model is generally infeasible due to the inadequacy of dialogue data with annotated summaries. Most existing works for low-resource dialogue summarization directly pretrain models in other domains, e.g., the news domain, but they generally neglect the huge difference between dialogues and conventional articles. To bridge the... | Yicheng Zou, Bolin Zhu, Xingwu Hu, Tao Gui, Qi Zhang |  |
| 477 |  |  [Controllable Neural Dialogue Summarization with Personal Named Entity Planning](https://doi.org/10.18653/v1/2021.emnlp-main.8) |  | 0 | In this paper, we propose a controllable neural generation framework that can flexibly guide dialogue summarization with personal named entity planning. The conditional sequences are modulated to decide what types of information or what perspective to focus on when forming summaries to tackle the under-constrained problem in summarization tasks. This framework supports two types of use cases: (1) Comprehensive Perspective, which is a general-purpose case with no user-preference specified,... | Zhengyuan Liu, Nancy F. Chen |  |
| 478 |  |  [Fine-grained Factual Consistency Assessment for Abstractive Summarization Models](https://doi.org/10.18653/v1/2021.emnlp-main.9) |  | 0 | Factual inconsistencies existed in the output of abstractive summarization models with original documents are frequently presented. Fact consistency assessment requires the reasoning capability to find subtle clues to identify whether a model-generated summary is consistent with the original document. This paper proposes a fine-grained two-stage Fact Consistency assessment framework for Summarization models (SumFC). Given a document and a summary sentence, in the first stage, SumFC selects the... | Sen Zhang, Jianwei Niu, Chuyuan Wei |  |
| 479 |  |  [Decision-Focused Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.10) |  | 0 | Relevance in summarization is typically de- fined based on textual information alone, without incorporating insights about a particular decision. As a result, to support risk analysis of pancreatic cancer, summaries of medical notes may include irrelevant information such as a knee injury. We propose a novel problem, decision-focused summarization, where the goal is to summarize relevant information for a decision. We leverage a predictive model that makes the decision based on the full text to... | ChaoChun Hsu, Chenhao Tan |  |
| 480 |  |  [Multiplex Graph Neural Network for Extractive Text Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.11) |  | 0 | Extractive text summarization aims at extracting the most representative sentences from a given document as its summary. To extract a good summary from a long text document, sentence embedding plays an important role. Recent studies have leveraged graph neural networks to capture the inter-sentential relationship (e.g., the discourse graph) within the documents to learn contextual sentence embedding. However, those approaches neither consider multiple types of inter-sentential relationships... | Baoyu Jing, Zeyu You, Tao Yang, Wei Fan, Hanghang Tong |  |
| 481 |  |  [A Thorough Evaluation of Task-Specific Pretraining for Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.12) |  | 0 | Task-agnostic pretraining objectives like masked language models or corrupted span prediction are applicable to a wide range of NLP downstream tasks (Raffel et al.,2019), but are outperformed by task-specific pretraining objectives like predicting extracted gap sentences on summarization (Zhang et al.,2020). We compare three summarization specific pretraining objectives with the task agnostic corrupted span prediction pretraining in controlled study. We also extend our study to a low resource... | Sascha Rothe, Joshua Maynez, Shashi Narayan |  |
| 482 |  |  [HETFORMER: Heterogeneous Transformer with Sparse Attention for Long-Text Extractive Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.13) |  | 0 | To capture the semantic graph structure from raw text, most existing summarization approaches are built on GNNs with a pre-trained model. However, these methods suffer from cumbersome procedures and inefficient computations for long-text documents. To mitigate these issues, this paper proposes HetFormer, a Transformer-based pre-trained model with multi-granularity sparse attentions for long-text extractive summarization. Specifically, we model different types of semantic nodes in raw text as a... | Ye Liu, JianGuo Zhang, Yao Wan, Congying Xia, Lifang He, Philip S. Yu |  |
| 483 |  |  [Unsupervised Keyphrase Extraction by Jointly Modeling Local and Global Context](https://doi.org/10.18653/v1/2021.emnlp-main.14) |  | 0 | Embedding based methods are widely used for unsupervised keyphrase extraction (UKE) tasks. Generally, these methods simply calculate similarities between phrase embeddings and document embedding, which is insufficient to capture different context for a more effective UKE model. In this paper, we propose a novel method for UKE, where local and global contexts are jointly modeled. From a global view, we calculate the similarity between a certain phrase and the whole document in the vector space... | Xinnian Liang, Shuangzhi Wu, Mu Li, Zhoujun Li |  |
| 484 |  |  [Distantly Supervised Relation Extraction using Multi-Layer Revision Network and Confidence-based Multi-Instance Learning](https://doi.org/10.18653/v1/2021.emnlp-main.15) |  | 0 | Distantly supervised relation extraction is widely used in the construction of knowledge bases due to its high efficiency. However, the automatically obtained instances are of low quality with numerous irrelevant words. In addition, the strong assumption of distant supervision leads to the existence of noisy sentences in the sentence bags. In this paper, we propose a novel Multi-Layer Revision Network (MLRN) which alleviates the effects of word-level noise by emphasizing inner-sentence... | Xiangyu Lin, Tianyi Liu, Weijia Jia, Zhiguo Gong |  |
| 485 |  |  [Logic-level Evidence Retrieval and Graph-based Verification Network for Table-based Fact Verification](https://doi.org/10.18653/v1/2021.emnlp-main.16) |  | 0 | Table-based fact verification task aims to verify whether the given statement is supported by the given semi-structured table. Symbolic reasoning with logical operations plays a crucial role in this task. Existing methods leverage programs that contain rich logical information to enhance the verification process. However, due to the lack of fully supervised signals in the program generation process, spurious programs can be derived and employed, which leads to the inability of the model to... | Qi Shi, Yu Zhang, Qingyu Yin, Ting Liu |  |
| 486 |  |  [A Partition Filter Network for Joint Entity and Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.17) |  | 0 | In joint entity and relation extraction, existing work either sequentially encode task-specific features, leading to an imbalance in inter-task feature interaction where features extracted later have no direct contact with those that come first. Or they encode entity features and relation features in a parallel manner, meaning that feature representation learning for each task is largely independent of each other except for input sharing. We propose a partition filter network to model two-way... | Zhiheng Yan, Chong Zhang, Jinlan Fu, Qi Zhang, Zhongyu Wei |  |
| 487 |  |  [TEBNER: Domain Specific Named Entity Recognition with Type Expanded Boundary-aware Network](https://doi.org/10.18653/v1/2021.emnlp-main.18) |  | 0 | To alleviate label scarcity in Named Entity Recognition (NER) task, distantly supervised NER methods are widely applied to automatically label data and identify entities. Although the human effort is reduced, the generated incomplete and noisy annotations pose new challenges for learning effective neural models. In this paper, we propose a novel dictionary extension method which extracts new entities through the type expanded model. Moreover, we design a multi-granularity boundary-aware network... | Zheng Fang, Yanan Cao, Tai Li, Ruipeng Jia, Fang Fang, Yanmin Shang, Yuhai Lu |  |
| 488 |  |  [Beta Distribution Guided Aspect-aware Graph for Aspect Category Sentiment Analysis with Affective Knowledge](https://doi.org/10.18653/v1/2021.emnlp-main.19) |  | 0 | In this paper, we investigate the Aspect Category Sentiment Analysis (ACSA) task from a novel perspective by exploring a Beta Distribution guided aspect-aware graph construction based on external knowledge. That is, we are no longer entangled about how to laboriously search the sentiment clues for coarse-grained aspects from the context, but how to preferably find the words highly related to the aspects in the context and determine their importance based on the public knowledge base. In this... | Bin Liang, Hang Su, Rongdi Yin, Lin Gui, Min Yang, Qin Zhao, Xiaoqi Yu, Ruifeng Xu |  |
| 489 |  |  [DILBERT: Customized Pre-Training for Domain Adaptation with Category Shift, with an Application to Aspect Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.20) |  | 0 | The rise of pre-trained language models has yielded substantial progress in the vast majority of Natural Language Processing (NLP) tasks. However, a generic approach towards the pre-training procedure can naturally be sub-optimal in some cases. Particularly, fine-tuning a pre-trained language model on a source domain and then applying it to a different target domain, results in a sharp performance decline of the eventual classifier for many source-target domain pairs. Moreover, in some NLP... | Entony Lekhtman, Yftah Ziser, Roi Reichart |  |
| 490 |  |  [Improving Multimodal fusion via Mutual Dependency Maximisation](https://doi.org/10.18653/v1/2021.emnlp-main.21) |  | 0 | Multimodal sentiment analysis is a trending area of research, and multimodal fusion is one of its most active topic. Acknowledging humans communicate through a variety of channels (i.e visual, acoustic, linguistic), multimodal systems aim at integrating different unimodal representations into a synthetic one. So far, a consequent effort has been made on developing complex architectures allowing the fusion of these modalities. However, such systems are mainly trained by minimising simple losses... | Pierre Colombo, Emile Chapuis, Matthieu Labeau, Chloé Clavel |  |
| 491 |  |  [Learning Implicit Sentiment in Aspect-based Sentiment Analysis with Supervised Contrastive Pre-Training](https://doi.org/10.18653/v1/2021.emnlp-main.22) |  | 0 | Aspect-based sentiment analysis aims to identify the sentiment polarity of a specific aspect in product reviews. We notice that about 30% of reviews do not contain obvious opinion words, but still convey clear human-aware sentiment orientation, which is known as implicit sentiment. However, recent neural network-based approaches paid little attention to implicit sentiment entailed in the reviews. To overcome this issue, we adopt Supervised Contrastive Pre-training on large-scale... | Zhengyan Li, Yicheng Zou, Chong Zhang, Qi Zhang, Zhongyu Wei |  |
| 492 |  |  [Progressive Self-Training with Discriminator for Aspect Term Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.23) |  | 0 | Aspect term extraction aims to extract aspect terms from a review sentence that users have expressed opinions on. One of the remaining challenges for aspect term extraction resides in the lack of sufficient annotated data. While self-training is potentially an effective method to address this issue, the pseudo-labels it yields on unlabeled data could induce noise. In this paper, we use two means to alleviate the noise in the pseudo-labels. One is that inspired by the curriculum learning, we... | Qianlong Wang, Zhiyuan Wen, Qin Zhao, Min Yang, Ruifeng Xu |  |
| 493 |  |  [Reinforced Counterfactual Data Augmentation for Dual Sentiment Classification](https://doi.org/10.18653/v1/2021.emnlp-main.24) |  | 0 | Data augmentation and adversarial perturbation approaches have recently achieved promising results in solving the over-fitting problem in many natural language processing (NLP) tasks including sentiment classification. However, existing studies aimed to improve the generalization ability by augmenting the training data with synonymous examples or adding random noises to word embeddings, which cannot address the spurious association problem. In this work, we propose an end-to-end reinforcement... | Hao Chen, Rui Xia, Jianfei Yu |  |
| 494 |  |  [Idiosyncratic but not Arbitrary: Learning Idiolects in Online Registers Reveals Distinctive yet Consistent Individual Styles](https://doi.org/10.18653/v1/2021.emnlp-main.25) |  | 0 | An individual’s variation in writing style is often a function of both social and personal attributes. While structured social variation has been extensively studied, e.g., gender based variation, far less is known about how to characterize individual styles due to their idiosyncratic nature. We introduce a new approach to studying idiolects through a massive cross-author comparison to identify and encode stylistic features. The neural model achieves strong performance at authorship... | Jian Zhu, David Jurgens |  |
| 495 |  |  [Narrative Theory for Computational Narrative Understanding](https://doi.org/10.18653/v1/2021.emnlp-main.26) |  | 0 | Over the past decade, the field of natural language processing has developed a wide array of computational methods for reasoning about narrative, including summarization, commonsense inference, and event detection. While this work has brought an important empirical lens for examining narrative, it is by and large divorced from the large body of theoretical work on narrative within the humanities, social and cognitive sciences. In this position paper, we introduce the dominant theoretical... | Andrew Piper, Richard Jean So, David Bamman |  |
| 496 |  |  [(Mis)alignment Between Stance Expressed in Social Media Data and Public Opinion Surveys](https://doi.org/10.18653/v1/2021.emnlp-main.27) |  | 0 | Stance detection, which aims to determine whether an individual is for or against a target concept, promises to uncover public opinion from large streams of social media data. Yet even human annotation of social media content does not always capture “stance” as measured by public opinion polls. We demonstrate this by directly comparing an individual’s self-reported stance to the stance inferred from their social media data. Leveraging a longitudinal public opinion survey with respondent Twitter... | Kenneth Joseph, Sarah Shugars, Ryan J. Gallagher, Jon Green, Alexi Quintana Mathé, Zijian An, David Lazer |  |
| 497 |  |  [How Does Counterfactually Augmented Data Impact Models for Social Computing Constructs?](https://doi.org/10.18653/v1/2021.emnlp-main.28) |  | 0 | As NLP models are increasingly deployed in socially situated settings such as online abusive content detection, it is crucial to ensure that these models are robust. One way of improving model robustness is to generate counterfactually augmented data (CAD) for training models that can better learn to distinguish between core features and data artifacts. While models trained on this type of data have shown promising out-of-domain generalizability, it is still unclear what the sources of such... | Indira Sen, Mattia Samory, Fabian Flöck, Claudia Wagner, Isabelle Augenstein |  |
| 498 |  |  [Latent Hatred: A Benchmark for Understanding Implicit Hate Speech](https://doi.org/10.18653/v1/2021.emnlp-main.29) |  | 0 | Hate speech has grown significantly on social media, causing serious consequences for victims of all demographics. Despite much attention being paid to characterize and detect discriminatory speech, most work has focused on explicit or overt hate speech, failing to address a more pervasive form based on coded or indirect language. To fill this gap, this work introduces a theoretically-justified taxonomy of implicit hate speech and a benchmark corpus with fine-grained labels for each message and... | Mai ElSherief, Caleb Ziems, David Muchlinski, Vaishnavi Anupindi, Jordyn Seybolt, Munmun De Choudhury, Diyi Yang |  |
| 499 |  |  [Distilling Linguistic Context for Language Model Compression](https://doi.org/10.18653/v1/2021.emnlp-main.30) |  | 0 | A computationally expensive and memory intensive neural network lies behind the recent success of language representation learning. Knowledge distillation, a major technique for deploying such a vast language model in resource-scarce environments, transfers the knowledge on individual word representations learned without restrictions. In this paper, inspired by the recent observations that language representations are relatively positioned and have more semantic knowledge as a whole, we present... | Geondo Park, Gyeongman Kim, Eunho Yang |  |
| 500 |  |  [Dynamic Knowledge Distillation for Pre-trained Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.31) |  | 0 | Knowledge distillation (KD) has been proved effective for compressing large-scale pre-trained language models. However, existing methods conduct KD statically, e.g., the student model aligns its output distribution to that of a selected teacher model on the pre-defined training dataset. In this paper, we explore whether a dynamic knowledge distillation that empowers the student to adjust the learning procedure according to its competency, regarding the student performance and learning... | Lei Li, Yankai Lin, Shuhuai Ren, Peng Li, Jie Zhou, Xu Sun |  |
| 501 |  |  [Few-Shot Text Generation with Natural Language Instructions](https://doi.org/10.18653/v1/2021.emnlp-main.32) |  | 0 | Providing pretrained language models with simple task descriptions in natural language enables them to solve some tasks in a fully unsupervised fashion. Moreover, when combined with regular learning from examples, this idea yields impressive few-shot results for a wide range of text classification tasks. It is also a promising direction to improve data efficiency in generative settings, but there are several challenges to using a combination of task descriptions and example-based learning for... | Timo Schick, Hinrich Schütze |  |
| 502 |  |  [SOM-NCSCM : An Efficient Neural Chinese Sentence Compression Model Enhanced with Self-Organizing Map](https://doi.org/10.18653/v1/2021.emnlp-main.33) |  | 0 | Sentence Compression (SC), which aims to shorten sentences while retaining important words that express the essential meanings, has been studied for many years in many languages, especially in English. However, improvements on Chinese SC task are still quite few due to several difficulties: scarce of parallel corpora, different segmentation granularity of Chinese sentences, and imperfect performance of syntactic analyses. Furthermore, entire neural Chinese SC models have been under-investigated... | Kangli Zi, Shi Wang, Yu Liu, Jicun Li, Yanan Cao, Cungen Cao |  |
| 503 |  |  [Efficient Multi-Task Auxiliary Learning: Selecting Auxiliary Data by Feature Similarity](https://doi.org/10.18653/v1/2021.emnlp-main.34) |  | 0 | Multi-task auxiliary learning utilizes a set of relevant auxiliary tasks to improve the performance of a primary task. A common usage is to manually select multiple auxiliary tasks for multi-task learning on all data, which raises two issues: (1) selecting beneficial auxiliary tasks for a primary task is nontrivial; (2) when the auxiliary datasets are large, training on all data becomes time-expensive and impractical. Therefore, this paper focuses on addressing these problems and proposes a... | PoNien Kung, ShengSiang Yin, YiCheng Chen, TseHsuan Yang, YunNung Chen |  |
| 504 |  |  [GOLD: Improving Out-of-Scope Detection in Dialogues using Data Augmentation](https://doi.org/10.18653/v1/2021.emnlp-main.35) |  | 0 | Practical dialogue systems require robust methods of detecting out-of-scope (OOS) utterances to avoid conversational breakdowns and related failure modes. Directly training a model with labeled OOS examples yields reasonable performance, but obtaining such data is a resource-intensive process. To tackle this limited-data problem, previous methods focus on better modeling the distribution of in-scope (INS) examples. We introduce GOLD as an orthogonal technique that augments existing data to... | Derek Chen, Zhou Yu |  |
| 505 |  |  [Graph Based Network with Contextualized Representations of Turns in Dialogue](https://doi.org/10.18653/v1/2021.emnlp-main.36) |  | 0 | Dialogue-based relation extraction (RE) aims to extract relation(s) between two arguments that appear in a dialogue. Because dialogues have the characteristics of high personal pronoun occurrences and low information density, and since most relational facts in dialogues are not supported by any single sentence, dialogue-based relation extraction requires a comprehensive understanding of dialogue. In this paper, we propose the TUrn COntext awaRE Graph Convolutional Network (TUCORE-GCN) modeled... | Bongseok Lee, Yong Suk Choi |  |
| 506 |  |  [Automatically Exposing Problems with Neural Dialog Models](https://doi.org/10.18653/v1/2021.emnlp-main.37) |  | 0 | Neural dialog models are known to suffer from problems such as generating unsafe and inconsistent responses. Even though these problems are crucial and prevalent, they are mostly manually identified by model designers through interactions. Recently, some research instructs crowdworkers to goad the bots into triggering such problems. However, humans leverage superficial clues such as hate speech, while leaving systematic problems undercover. In this paper, we propose two methods including... | Dian Yu, Kenji Sagae |  |
| 507 |  |  [Event Coreference Data (Almost) for Free: Mining Hyperlinks from Online News](https://doi.org/10.18653/v1/2021.emnlp-main.38) |  | 0 | Cross-document event coreference resolution (CDCR) is the task of identifying which event mentions refer to the same events throughout a collection of documents. Annotating CDCR data is an arduous and expensive process, explaining why existing corpora are small and lack domain coverage. To overcome this bottleneck, we automatically extract event coreference data from hyperlinks in online news: When referring to a significant real-world event, writers often add a hyperlink to another article... | Michael Bugert, Iryna Gurevych |  |
| 508 |  |  [Inducing Stereotypical Character Roles from Plot Structure](https://doi.org/10.18653/v1/2021.emnlp-main.39) |  | 0 | Stereotypical character roles-also known as archetypes or dramatis personae-play an important function in narratives: they facilitate efficient communication with bundles of default characteristics and associations and ease understanding of those characters’ roles in the overall narrative. We present a fully unsupervised k-means clustering approach for learning stereotypical roles given only structural plot information. We demonstrate the technique on Vladimir Propp’s structural theory of... | Labiba Jahan, Rahul Mittal, Mark A. Finlayson |  |
| 509 |  |  [Multitask Semi-Supervised Learning for Class-Imbalanced Discourse Classification](https://doi.org/10.18653/v1/2021.emnlp-main.40) |  | 0 | As labeling schemas evolve over time, small differences can render datasets following older schemas unusable. This prevents researchers from building on top of previous annotation work and results in the existence, in discourse learning in particular, of many small class-imbalanced datasets. In this work, we show that a multitask learning approach can combine discourse datasets from similar and diverse domains to improve discourse classification. We show an improvement of 4.9% Micro F1-score... | Alexander Spangher, Jonathan May, SzRung Shiang, Lingjia Deng |  |
| 510 |  |  [Low Frequency Names Exhibit Bias and Overfitting in Contextualizing Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.41) |  | 0 | We use a dataset of U.S. first names with labels based on predominant gender and racial group to examine the effect of training corpus frequency on tokenization, contextualization, similarity to initial representation, and bias in BERT, GPT-2, T5, and XLNet. We show that predominantly female and non-white names are less frequent in the training corpora of these four language models. We find that infrequent names are more self-similar across contexts, with Spearman’s rho between frequency and... | Robert Wolfe, Aylin Caliskan |  |
| 511 |  |  [Mitigating Language-Dependent Ethnic Bias in BERT](https://doi.org/10.18653/v1/2021.emnlp-main.42) |  | 0 | In this paper, we study ethnic bias and how it varies across languages by analyzing and mitigating ethnic bias in monolingual BERT for English, German, Spanish, Korean, Turkish, and Chinese. To observe and quantify ethnic bias, we develop a novel metric called Categorical Bias score. Then we propose two methods for mitigation; first using a multilingual model, and second using contextual word alignment of two monolingual models. We compare our proposed methods with monolingual BERT and show... | Jaimeen Ahn, Alice Oh |  |
| 512 |  |  [Adversarial Scrubbing of Demographic Information for Text Classification](https://doi.org/10.18653/v1/2021.emnlp-main.43) |  | 0 | Contextual representations learned by language models can often encode undesirable attributes, like demographic associations of the users, while being trained for an unrelated target task. We aim to scrub such undesirable attributes and learn fair representations while maintaining performance on the target task. In this paper, we present an adversarial learning framework “Adversarial Scrubber” (AdS), to debias contextual representations. We perform theoretical analysis to show that our... | Somnath Basu Roy Chowdhury, Sayan Ghosh, Yiyuan Li, Junier Oliva, Shashank Srivastava, Snigdha Chaturvedi |  |
| 513 |  |  [Open-domain clarification question generation without question examples](https://doi.org/10.18653/v1/2021.emnlp-main.44) |  | 0 | An overarching goal of natural language processing is to enable machines to communicate seamlessly with humans. However, natural language can be ambiguous or unclear. In cases of uncertainty, humans engage in an interactive process known as repair: asking questions and seeking clarification until their uncertainty is resolved. We propose a framework for building a visually grounded question-asking model capable of producing polar (yes-no) clarification questions to resolve misunderstandings in... | Julia White, Gabriel Poesia, Robert X. D. Hawkins, Dorsa Sadigh, Noah D. Goodman |  |
| 514 |  |  [Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting](https://doi.org/10.18653/v1/2021.emnlp-main.45) |  | 0 | In this paper, we propose Sequence Span Rewriting (SSR), a self-supervised task for sequence-to-sequence (Seq2Seq) pre-training. SSR learns to refine the machine-generated imperfect text spans into ground truth text. SSR provides more fine-grained and informative supervision in addition to the original text-infilling objective. Compared to the prevalent text infilling objectives for Seq2Seq pre-training, SSR is naturally more consistent with many downstream generation tasks that require... | Wangchunshu Zhou, Tao Ge, Canwen Xu, Ke Xu, Furu Wei |  |
| 515 |  |  [Coarse2Fine: Fine-grained Text Classification on Coarsely-grained Annotated Data](https://doi.org/10.18653/v1/2021.emnlp-main.46) |  | 0 | Existing text classification methods mainly focus on a fixed label set, whereas many real-world applications require extending to new fine-grained classes as the number of samples per label increases. To accommodate such requirements, we introduce a new problem called coarse-to-fine grained classification, which aims to perform fine-grained classification on coarsely annotated data. Instead of asking for new fine-grained human annotations, we opt to leverage label surface names as the only... | Dheeraj Mekala, Varun Gangal, Jingbo Shang |  |
| 516 |  |  [Text2Mol: Cross-Modal Molecule Retrieval with Natural Language Queries](https://doi.org/10.18653/v1/2021.emnlp-main.47) |  | 0 | We propose a new task, Text2Mol, to retrieve molecules using natural language descriptions as queries. Natural language and molecules encode information in very different ways, which leads to the exciting but challenging problem of integrating these two very different modalities. Although some work has been done on text-based retrieval and structure-based retrieval, this new task requires integrating molecules and natural language more directly. Moreover, this can be viewed as an especially... | Carl Edwards, ChengXiang Zhai, Heng Ji |  |
| 517 |  |  [Classification of hierarchical text using geometric deep learning: the case of clinical trials corpus](https://doi.org/10.18653/v1/2021.emnlp-main.48) |  | 0 | We consider the hierarchical representation of documents as graphs and use geometric deep learning to classify them into different categories. While graph neural networks can efficiently handle the variable structure of hierarchical documents using the permutation invariant message passing operations, we show that we can gain extra performance improvements using our proposed selective graph pooling operation that arises from the fact that some parts of the hierarchy are invariable across... | Sohrab Ferdowsi, Nikolay Borissov, Julien Knafou, Poorya Amini, Douglas Teodoro |  |
| 518 |  |  [The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers](https://doi.org/10.18653/v1/2021.emnlp-main.49) |  | 0 | Recently, many datasets have been proposed to test the systematic generalization ability of neural networks. The companion baseline Transformers, typically trained with default hyper-parameters from standard tasks, are shown to fail dramatically. Here we demonstrate that by revisiting model configurations as basic as scaling of embeddings, early stopping, relative positional embedding, and Universal Transformer variants, we can drastically improve the performance of Transformers on systematic... | Róbert Csordás, Kazuki Irie, Jürgen Schmidhuber |  |
| 519 |  |  [Artificial Text Detection via Examining the Topology of Attention Maps](https://doi.org/10.18653/v1/2021.emnlp-main.50) |  | 0 | The impressive capabilities of recent generative models to create texts that are challenging to distinguish from the human-written ones can be misused for generating fake news, product reviews, and even abusive content. Despite the prominent performance of existing methods for artificial text detection, they still lack interpretability and robustness towards unseen models. To this end, we propose three novel types of interpretable topological features for this task based on Topological Data... | Laida Kushnareva, Daniil Cherniavskii, Vladislav Mikhailov, Ekaterina Artemova, Serguei Barannikov, Alexander Bernstein, Irina Piontkovskaya, Dmitri Piontkovski, Evgeny Burnaev |  |
| 520 |  |  [Active Learning by Acquiring Contrastive Examples](https://doi.org/10.18653/v1/2021.emnlp-main.51) |  | 0 | Common acquisition functions for active learning use either uncertainty or diversity sampling, aiming to select difficult and diverse data points from the pool of unlabeled data, respectively. In this work, leveraging the best of both worlds, we propose an acquisition function that opts for selecting contrastive examples, i.e. data points that are similar in the model feature space and yet the model outputs maximally different predictive likelihoods. We compare our approach, CAL (Contrastive... | Katerina Margatina, Giorgos Vernikos, Loïc Barrault, Nikolaos Aletras |  |
| 521 |  |  [Conditional Poisson Stochastic Beams](https://doi.org/10.18653/v1/2021.emnlp-main.52) |  | 0 | Beam search is the default decoding strategy for many sequence generation tasks in NLP. The set of approximate K-best items returned by the algorithm is a useful summary of the distribution for many applications; however, the candidates typically exhibit high overlap and may give a highly biased estimate for expectations under our model. These problems can be addressed by instead using stochastic decoding strategies. In this work, we propose a new method for turning beam search into a... | Clara Meister, Afra Amini, Tim Vieira, Ryan Cotterell |  |
| 522 |  |  [Building Adaptive Acceptability Classifiers for Neural NLG](https://doi.org/10.18653/v1/2021.emnlp-main.53) |  | 0 | We propose a novel framework to train models to classify acceptability of responses generated by natural language generation (NLG) models, improving upon existing sentence transformation and model-based approaches. An NLG response is considered acceptable if it is both semantically correct and grammatical. We don’t make use of any human references making the classifiers suitable for runtime deployment. Training data for the classifiers is obtained using a 2-stage approach of first generating... | Soumya Batra, Shashank Jain, Peyman Heidari, Ankit Arun, Catharine Youngs, Xintong Li, Pinar Donmez, Shawn Mei, Shiunzu Kuo, Vikas Bhardwaj, Anuj Kumar, Michael White |  |
| 523 |  |  [Moral Stories: Situated Reasoning about Norms, Intents, Actions, and their Consequences](https://doi.org/10.18653/v1/2021.emnlp-main.54) |  | 0 | In social settings, much of human behavior is governed by unspoken rules of conduct rooted in societal norms. For artificial systems to be fully integrated into social environments, adherence to such norms is a central prerequisite. To investigate whether language generation models can serve as behavioral priors for systems deployed in social settings, we evaluate their ability to generate action descriptions that achieve predefined goals under normative constraints. Moreover, we examine if... | Denis Emelin, Ronan Le Bras, Jena D. Hwang, Maxwell Forbes, Yejin Choi |  |
| 524 |  |  [Truth-Conditional Captions for Time Series Data](https://doi.org/10.18653/v1/2021.emnlp-main.55) |  | 0 | In this paper, we explore the task of automatically generating natural language descriptions of salient patterns in a time series, such as stock prices of a company over a week. A model for this task should be able to extract high-level patterns such as presence of a peak or a dip. While typical contemporary neural models with attention mechanisms can generate fluent output descriptions for this task, they often generate factually incorrect descriptions. We propose a computational model with a... | Harsh Jhamtani, Taylor BergKirkpatrick |  |
| 525 |  |  [Injecting Entity Types into Entity-Guided Text Generation](https://doi.org/10.18653/v1/2021.emnlp-main.56) |  | 0 | Recent successes in deep generative modeling have led to significant advances in natural language generation (NLG). Incorporating entities into neural generation models has demonstrated great improvements by assisting to infer the summary topic and to generate coherent content. To enhance the role of entity in NLG, in this paper, we aim to model the entity type in the decoding phase to generate contextual words accurately. We develop a novel NLG model to produce a target sequence based on a... | Xiangyu Dong, Wenhao Yu, Chenguang Zhu, Meng Jiang |  |
| 526 |  |  [Smelting Gold and Silver for Improved Multilingual AMR-to-Text Generation](https://doi.org/10.18653/v1/2021.emnlp-main.57) |  | 0 | Recent work on multilingual AMR-to-text generation has exclusively focused on data augmentation strategies that utilize silver AMR. However, this assumes a high quality of generated AMRs, potentially limiting the transferability to the target task. In this paper, we investigate different techniques for automatically generating AMR annotations, where we aim to study which source of information yields better multilingual results. Our models trained on gold AMR with silver (machine translated)... | Leonardo F. R. Ribeiro, Jonas Pfeiffer, Yue Zhang, Iryna Gurevych |  |
| 527 |  |  [Learning Compact Metrics for MT](https://doi.org/10.18653/v1/2021.emnlp-main.58) |  | 0 | Recent developments in machine translation and multilingual text generation have led researchers to adopt trained metrics such as COMET or BLEURT, which treat evaluation as a regression problem and use representations from multilingual pre-trained models such as XLM-RoBERTa or mBERT. Yet studies on related tasks suggest that these models are most efficient when they are large, which is costly and impractical for evaluation. We investigate the trade-off between multilinguality and model capacity... | Amy Pu, Hyung Won Chung, Ankur P. Parikh, Sebastian Gehrmann, Thibault Sellam |  |
| 528 |  |  [The Impact of Positional Encodings on Multilingual Compression](https://doi.org/10.18653/v1/2021.emnlp-main.59) |  | 0 | In order to preserve word-order information in a non-autoregressive setting, transformer architectures tend to include positional knowledge, by (for instance) adding positional encodings to token embeddings. Several modifications have been proposed over the sinusoidal positional encodings used in the original transformer architecture; these include, for instance, separating position encodings and token embeddings, or directly modifying attention weights based on the distance between word pairs.... | Vinit Ravishankar, Anders Søgaard |  |
| 529 |  |  [Disentangling Representations of Text by Masking Transformers](https://doi.org/10.18653/v1/2021.emnlp-main.60) |  | 0 | Representations from large pretrained models such as BERT encode a range of features into monolithic vectors, affording strong predictive accuracy across a range of downstream tasks. In this paper we explore whether it is possible to learn disentangled representations by identifying existing subnetworks within pretrained models that encode distinct, complementary aspects. Concretely, we learn binary masks over transformer weights or hidden units to uncover subsets of features that correlate... | Xiongyi Zhang, JanWillem van de Meent, Byron C. Wallace |  |
| 530 |  |  [Exploring the Role of BERT Token Representations to Explain Sentence Probing Results](https://doi.org/10.18653/v1/2021.emnlp-main.61) |  | 0 | Several studies have been carried out on revealing linguistic features captured by BERT. This is usually achieved by training a diagnostic classifier on the representations obtained from different layers of BERT. The subsequent classification accuracy is then interpreted as the ability of the model in encoding the corresponding linguistic property. Despite providing insights, these studies have left out the potential role of token representations. In this paper, we provide a more in-depth... | Hosein Mohebbi, Ali Modarressi, Mohammad Taher Pilehvar |  |
| 531 |  |  [Do Long-Range Language Models Actually Use Long-Range Context?](https://doi.org/10.18653/v1/2021.emnlp-main.62) |  | 0 | Language models are generally trained on short, truncated input sequences, which limits their ability to use discourse-level information present in long-range context to improve their predictions. Recent efforts to improve the efficiency of self-attention have led to a proliferation of long-range Transformer language models, which can process much longer sequences than models of the past. However, the ways in which such models take advantage of the long-range context remain unclear. In this... | Simeng Sun, Kalpesh Krishna, Andrew MattarellaMicke, Mohit Iyyer |  |
| 532 |  |  [The World of an Octopus: How Reporting Bias Influences a Language Model's Perception of Color](https://doi.org/10.18653/v1/2021.emnlp-main.63) |  | 0 | Recent work has raised concerns about the inherent limitations of text-only pretraining. In this paper, we first demonstrate that reporting bias, the tendency of people to not state the obvious, is one of the causes of this limitation, and then investigate to what extent multimodal training can mitigate this issue. To accomplish this, we 1) generate the Color Dataset (CoDa), a dataset of human-perceived color distributions for 521 common objects; 2) use CoDa to analyze and compare the color... | Cory Paik, Stéphane ArocaOuellette, Alessandro Roncone, Katharina Kann |  |
| 533 |  |  [SELFEXPLAIN: A Self-Explaining Architecture for Neural Text Classifiers](https://doi.org/10.18653/v1/2021.emnlp-main.64) |  | 0 | We introduce SelfExplain, a novel self-explaining model that explains a text classifier’s predictions using phrase-based concepts. SelfExplain augments existing neural classifiers by adding (1) a globally interpretable layer that identifies the most influential concepts in the training set for a given sample and (2) a locally interpretable layer that quantifies the contribution of each local input concept by computing a relevance score relative to the predicted label. Experiments across five... | Dheeraj Rajagopal, Vidhisha Balachandran, Eduard H. Hovy, Yulia Tsvetkov |  |
| 534 |  |  [Memory and Knowledge Augmented Language Models for Inferring Salience in Long-Form Stories](https://doi.org/10.18653/v1/2021.emnlp-main.65) |  | 0 | Measuring event salience is essential in the understanding of stories. This paper takes a recent unsupervised method for salience detection derived from Barthes Cardinal Functions and theories of surprise and applies it to longer narrative forms. We improve the standard transformer language model by incorporating an external knowledgebase (derived from Retrieval Augmented Generation) and adding a memory mechanism to enhance performance on longer works. We use a novel approach to derive salience... | David Wilmot, Frank Keller |  |
| 535 |  |  [Semantic Novelty Detection in Natural Language Descriptions](https://doi.org/10.18653/v1/2021.emnlp-main.66) |  | 0 | This paper proposes to study a fine-grained semantic novelty detection task, which can be illustrated with the following example. It is normal that a person walks a dog in the park, but if someone says “A man is walking a chicken in the park”, it is novel. Given a set of natural language descriptions of normal scenes, we want to identify descriptions of novel scenes. We are not aware of any existing work that solves the problem. Although existing novelty or anomaly detection algorithms are... | Nianzu Ma, Alexander Politowicz, Sahisnu Mazumder, Jiahua Chen, Bing Liu, Eric Robertson, Scott Grigsby |  |
| 536 |  |  [Jump-Starting Item Parameters for Adaptive Language Tests](https://doi.org/10.18653/v1/2021.emnlp-main.67) |  | 0 | A challenge in designing high-stakes language assessments is calibrating the test item difficulties, either a priori or from limited pilot test data. While prior work has addressed ‘cold start’ estimation of item difficulties without piloting, we devise a multi-task generalized linear model with BERT features to jump-start these estimates, rapidly improving their quality with as few as 500 test-takers and a small sample of item exposures (≈6 each) from a large item bank (≈4,000 items). Our... | Arya D. McCarthy, Kevin P. Yancey, Geoffrey T. LaFlair, Jesse Egbert, Manqian Liao, Burr Settles |  |
| 537 |  |  [Voice Query Auto Completion](https://doi.org/10.18653/v1/2021.emnlp-main.68) |  | 0 | Query auto completion (QAC) is the task of predicting a search engine user’s final query from their intermediate, incomplete query. In this paper, we extend QAC to the streaming voice search setting, where automatic speech recognition systems produce intermediate transcriptions as users speak. Naively applying existing methods fails because the intermediate transcriptions often don’t form prefixes or even substrings of the final transcription. To address this issue, we propose to condition QAC... | Raphael Tang, Karun Kumar, Kendra Chalkley, Ji Xin, Liming Zhang, Wenyan Li, Gefei Yang, Yajie Mao, Junho Shin, Geoffrey Craig Murray, Jimmy Lin |  |
| 538 |  |  [CoPHE: A Count-Preserving Hierarchical Evaluation Metric in Large-Scale Multi-Label Text Classification](https://doi.org/10.18653/v1/2021.emnlp-main.69) |  | 0 | Large-Scale Multi-Label Text Classification (LMTC) includes tasks with hierarchical label spaces, such as automatic assignment of ICD-9 codes to discharge summaries. Performance of models in prior art is evaluated with standard precision, recall, and F1 measures without regard for the rich hierarchical structure. In this work we argue for hierarchical evaluation of the predictions of neural LMTC models. With the example of the ICD-9 ontology we describe a structural issue in the representation... | Matús Falis, Hang Dong, Alexandra Birch, Beatrice Alex |  |
| 539 |  |  [Learning Universal Authorship Representations](https://doi.org/10.18653/v1/2021.emnlp-main.70) |  | 0 | Determining whether two documents were composed by the same author, also known as authorship verification, has traditionally been tackled using statistical methods. Recently, authorship representations learned using neural networks have been found to outperform alternatives, particularly in large-scale settings involving hundreds of thousands of authors. But do such representations learned in a particular domain transfer to other domains? Or are these representations inherently entangled with... | Rafael A. Rivera Soto, Olivia Elizabeth Miano, Juanita Ordonez, Barry Y. Chen, Aleem Khan, Marcus Bishop, Nicholas Andrews |  |
| 540 |  |  [Predicting emergent linguistic compositions through time: Syntactic frame extension via multimodal chaining](https://doi.org/10.18653/v1/2021.emnlp-main.71) |  | 0 | Natural language relies on a finite lexicon to express an unbounded set of emerging ideas. One result of this tension is the formation of new compositions, such that existing linguistic units can be combined with emerging items into novel expressions. We develop a framework that exploits the cognitive mechanisms of chaining and multimodal knowledge to predict emergent compositional expressions through time. We present the syntactic frame extension model (SFEM) that draws on the theory of... | Lei Yu, Yang Xu |  |
| 541 |  |  [Frequency Effects on Syntactic Rule Learning in Transformers](https://doi.org/10.18653/v1/2021.emnlp-main.72) |  | 0 | Pre-trained language models perform well on a variety of linguistic tasks that require symbolic reasoning, raising the question of whether such models implicitly represent abstract symbols and rules. We investigate this question using the case study of BERT’s performance on English subject–verb agreement. Unlike prior work, we train multiple instances of BERT from scratch, allowing us to perform a series of controlled interventions at pre-training time. We show that BERT often generalizes well... | Jason Wei, Dan Garrette, Tal Linzen, Ellie Pavlick |  |
| 542 |  |  [A surprisal-duration trade-off across and within the world's languages](https://doi.org/10.18653/v1/2021.emnlp-main.73) |  | 0 | While there exist scores of natural languages, each with its unique features and idiosyncrasies, they all share a unifying theme: enabling human communication. We may thus reasonably predict that human cognition shapes how these languages evolve and are used. Assuming that the capacity to process information is roughly constant across human populations, we expect a surprisal–duration trade-off to arise both across and within languages. We analyse this trade-off using a corpus of 600 languages... | Tiago Pimentel, Clara Meister, Elizabeth Salesky, Simone Teufel, Damián E. Blasi, Ryan Cotterell |  |
| 543 |  |  [Revisiting the Uniform Information Density Hypothesis](https://doi.org/10.18653/v1/2021.emnlp-main.74) |  | 0 | The uniform information density (UID) hypothesis posits a preference among language users for utterances structured such that information is distributed uniformly across a signal. While its implications on language production have been well explored, the hypothesis potentially makes predictions about language comprehension and linguistic acceptability as well. Further, it is unclear how uniformity in a linguistic signal—or lack thereof—should be measured, and over which linguistic unit, e.g.,... | Clara Meister, Tiago Pimentel, Patrick Haller, Lena A. Jäger, Ryan Cotterell, Roger Levy |  |
| 544 |  |  [Condenser: a Pre-training Architecture for Dense Retrieval](https://doi.org/10.18653/v1/2021.emnlp-main.75) |  | 0 | Pre-trained Transformer language models (LM) have become go-to text representation encoders. Prior research fine-tunes deep LMs to encode text sequences such as sentences and passages into single dense vector representations for efficient text comparison and retrieval. However, dense encoders require a lot of data and sophisticated techniques to effectively train and suffer in low data situations. This paper finds a key reason is that standard LMs’ internal attention structure is not... | Luyu Gao, Jamie Callan |  |
| 545 |  |  [Monitoring geometrical properties of word embeddings for detecting the emergence of new topics](https://doi.org/10.18653/v1/2021.emnlp-main.76) |  | 0 | Slow emerging topic detection is a task between event detection, where we aggregate behaviors of different words on short period of time, and language evolution, where we monitor their long term evolution. In this work, we tackle the problem of early detection of slowly emerging new topics. To this end, we gather evidence of weak signals at the word level. We propose to monitor the behavior of words representation in an embedding space and use one of its geometrical properties to characterize... | Clément Christophe, Julien Velcin, Jairo Cugliari, Manel Boumghar, Philippe Suignard |  |
| 546 |  |  [Contextualized Query Embeddings for Conversational Search](https://doi.org/10.18653/v1/2021.emnlp-main.77) |  | 0 | This paper describes a compact and effective model for low-latency passage retrieval in conversational search based on learned dense representations. Prior to our work, the state-of-the-art approach uses a multi-stage pipeline comprising conversational query reformulation and information retrieval modules. Despite its effectiveness, such a pipeline often includes multiple neural models that require long inference times. In addition, independently optimizing each module ignores dependencies... | ShengChieh Lin, JhengHong Yang, Jimmy Lin |  |
| 547 |  |  [Ultra-High Dimensional Sparse Representations with Binarization for Efficient Text Retrieval](https://doi.org/10.18653/v1/2021.emnlp-main.78) |  | 0 | The semantic matching capabilities of neural information retrieval can ameliorate synonymy and polysemy problems of symbolic approaches. However, neural models’ dense representations are more suitable for re-ranking, due to their inefficiency. Sparse representations, either in symbolic or latent form, are more efficient with an inverted index. Taking the merits of the sparse and dense representations, we propose an ultra-high dimensional (UHD) representation scheme equipped with directly... | Kyoungrok Jang, Junmo Kang, Giwon Hong, SungHyon Myaeng, Joohee Park, Taewon Yoon, HeeCheol Seo |  |
| 548 |  |  [IR like a SIR: Sense-enhanced Information Retrieval for Multiple Languages](https://doi.org/10.18653/v1/2021.emnlp-main.79) |  | 0 | With the advent of contextualized embeddings, attention towards neural ranking approaches for Information Retrieval increased considerably. However, two aspects have remained largely neglected: i) queries usually consist of few keywords only, which increases ambiguity and makes their contextualization harder, and ii) performing neural ranking on non-English documents is still cumbersome due to shortage of labeled datasets. In this paper we present SIR (Sense-enhanced Information Retrieval) to... | Rexhina Blloshmi, Tommaso Pasini, Niccolò Campolungo, Somnath Banerjee, Roberto Navigli, Gabriella Pasi |  |
| 549 |  |  [Neural Attention-Aware Hierarchical Topic Model](https://doi.org/10.18653/v1/2021.emnlp-main.80) |  | 0 | Neural topic models (NTMs) apply deep neural networks to topic modelling. Despite their success, NTMs generally ignore two important aspects: (1) only document-level word count information is utilized for the training, while more fine-grained sentence-level information is ignored, and (2) external semantic knowledge regarding documents, sentences and words are not exploited for the training. To address these issues, we propose a variational autoencoder (VAE) NTM model that jointly reconstructs... | Yuan Jin, He Zhao, Ming Liu, Lan Du, Wray L. Buntine |  |
| 550 |  |  [Relational World Knowledge Representation in Contextual Language Models: A Review](https://doi.org/10.18653/v1/2021.emnlp-main.81) |  | 0 | Relational knowledge bases (KBs) are commonly used to represent world knowledge in machines. However, while advantageous for their high degree of precision and interpretability, KBs are usually organized according to manually-defined schemas, which limit their expressiveness and require significant human efforts to engineer and maintain. In this review, we take a natural language processing perspective to these limitations, examining how they may be addressed in part by training deep contextual... | Tara Safavi, Danai Koutra |  |
| 551 |  |  [Certified Robustness to Programmable Transformations in LSTMs](https://doi.org/10.18653/v1/2021.emnlp-main.82) |  | 0 | Deep neural networks for natural language processing are fragile in the face of adversarial examples—small input perturbations, like synonym substitution or word duplication, which cause a neural network to change its prediction. We present an approach to certifying the robustness of LSTMs (and extensions of LSTMs) and training models that can be efficiently certified. Our approach can certify robustness to intractably large perturbation spaces defined programmatically in a language of string... | Yuhao Zhang, Aws Albarghouthi, Loris D'Antoni |  |
| 552 |  |  [ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.83) |  | 0 | Automatic construction of relevant Knowledge Bases (KBs) from text, and generation of semantically meaningful text from KBs are both long-standing goals in Machine Learning. In this paper, we present ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning to improve performance. Graph linearization enables us to re-frame both tasks as a sequence to sequence generation problem regardless of the generative direction, which in turn allows the use of Reinforcement... | Pierre L. Dognin, Inkit Padhi, Igor Melnyk, Payel Das |  |
| 553 |  |  [Contrastive Out-of-Distribution Detection for Pretrained Transformers](https://doi.org/10.18653/v1/2021.emnlp-main.84) |  | 0 | Pretrained Transformers achieve remarkable performance when training and test data are from the same distribution. However, in real-world scenarios, the model often faces out-of-distribution (OOD) instances that can cause severe semantic shift problems at inference time. Therefore, in practice, a reliable model should identify such instances, and then either reject them during inference or pass them over to models that handle another distribution. In this paper, we develop an unsupervised OOD... | Wenxuan Zhou, Fangyu Liu, Muhao Chen |  |
| 554 |  |  [MindCraft: Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks](https://doi.org/10.18653/v1/2021.emnlp-main.85) |  | 0 | An ideal integration of autonomous agents in a human world implies that they are able to collaborate on human terms. In particular, theory of mind plays an important role in maintaining common ground during human collaboration and communication. To enable theory of mind modeling in situated interactions, we introduce a fine-grained dataset of collaborative tasks performed by pairs of human subjects in the 3D virtual blocks world of Minecraft. It provides information that captures partners’... | CristianPaul Bara, Sky CHWang, Joyce Chai |  |
| 555 |  |  [Detecting Speaker Personas from Conversational Texts](https://doi.org/10.18653/v1/2021.emnlp-main.86) |  | 0 | Personas are useful for dialogue response prediction. However, the personas used in current studies are pre-defined and hard to obtain before a conversation. To tackle this issue, we study a new task, named Speaker Persona Detection (SPD), which aims to detect speaker personas based on the plain conversational text. In this task, a best-matched persona is searched out from candidates given the conversational text. This is a many-to-many semantic matching task because both contexts and personas... | JiaChen Gu, ZhenHua Ling, Yu Wu, Quan Liu, Zhigang Chen, Xiaodan Zhu |  |
| 556 |  |  [Cross-lingual Intermediate Fine-tuning improves Dialogue State Tracking](https://doi.org/10.18653/v1/2021.emnlp-main.87) |  | 0 | Recent progress in task-oriented neural dialogue systems is largely focused on a handful of languages, as annotation of training data is tedious and expensive. Machine translation has been used to make systems multilingual, but this can introduce a pipeline of errors. Another promising solution is using cross-lingual transfer learning through pretrained multilingual models. Existing methods train multilingual models with additional code-mixed task data or refine the cross-lingual... | Nikita Moghe, Mark Steedman, Alexandra Birch |  |
| 557 |  |  [ConvFiT: Conversational Fine-Tuning of Pretrained Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.88) |  | 0 | Transformer-based language models (LMs) pretrained on large text collections are proven to store a wealth of semantic knowledge. However, 1) they are not effective as sentence encoders when used off-the-shelf, and 2) thus typically lag behind conversationally pretrained (e.g., via response selection) encoders on conversational tasks such as intent detection (ID). In this work, we propose ConvFiT, a simple and efficient two-stage procedure which turns any pretrained LM into a universal... | Ivan Vulic, PeiHao Su, Samuel Coope, Daniela Gerz, Pawel Budzianowski, Iñigo Casanueva, Nikola Mrksic, TsungHsien Wen |  |
| 558 |  |  [We've had this conversation before: A Novel Approach to Measuring Dialog Similarity](https://doi.org/10.18653/v1/2021.emnlp-main.89) |  | 0 | Dialog is a core building block of human natural language interactions. It contains multi-party utterances used to convey information from one party to another in a dynamic and evolving manner. The ability to compare dialogs is beneficial in many real world use cases, such as conversation analytics for contact center calls and virtual agent design. We propose a novel adaptation of the edit distance metric to the scenario of dialog similarity. Our approach takes into account various conversation... | Ofer Lavi, Ella Rabinovich, Segev Shlomov, David Boaz, Inbal Ronen, Ateret AnabyTavor |  |
| 559 |  |  [Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental NLU](https://doi.org/10.18653/v1/2021.emnlp-main.90) |  | 0 | Incremental processing allows interactive systems to respond based on partial inputs, which is a desirable property e.g. in dialogue agents. The currently popular Transformer architecture inherently processes sequences as a whole, abstracting away the notion of time. Recent work attempts to apply Transformers incrementally via restart-incrementality by repeatedly feeding, to an unchanged model, increasingly longer input prefixes to produce partial outputs. However, this approach is... | Patrick Kahardipraja, Brielen Madureira, David Schlangen |  |
| 560 |  |  [Feedback Attribution for Counterfactual Bandit Learning in Multi-Domain Spoken Language Understanding](https://doi.org/10.18653/v1/2021.emnlp-main.91) |  | 0 | With counterfactual bandit learning, models can be trained based on positive and negative feedback received for historical predictions, with no labeled data needed. Such feedback is often available in real-world dialog systems, however, the modularized architecture commonly used in large-scale systems prevents the direct application of such algorithms. In this paper, we study the feedback attribution problem that arises when using counterfactual bandit learning for multi-domain spoken language... | Tobias Falke, Patrick Lehnen |  |
| 561 |  |  [Label Verbalization and Entailment for Effective Zero and Few-Shot Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.92) |  | 0 | Relation extraction systems require large amounts of labeled examples which are costly to annotate. In this work we reformulate relation extraction as an entailment task, with simple, hand-made, verbalizations of relations produced in less than 15 min per relation. The system relies on a pretrained textual entailment engine which is run as-is (no training examples, zero-shot) or further fine-tuned on labeled examples (few-shot or fully trained). In our experiments on TACRED we attain 63% F1... | Oscar Sainz, Oier Lopez de Lacalle, Gorka Labaka, Ander Barrena, Eneko Agirre |  |
| 562 |  |  [Extend, don't rebuild: Phrasing conditional graph modification as autoregressive sequence labelling](https://doi.org/10.18653/v1/2021.emnlp-main.93) |  | 0 | Deriving and modifying graphs from natural language text has become a versatile basis technology for information extraction with applications in many subfields, such as semantic parsing or knowledge graph construction. A recent work used this technique for modifying scene graphs (He et al. 2020), by first encoding the original graph and then generating the modified one based on this encoding. In this work, we show that we can considerably increase performance on this problem by phrasing it as... | Leon Weber, Jannes Münchmeyer, Samuele Garda, Ulf Leser |  |
| 563 |  |  [Zero-Shot Information Extraction as a Unified Text-to-Triple Translation](https://doi.org/10.18653/v1/2021.emnlp-main.94) |  | 0 | We cast a suite of information extraction tasks into a text-to-triple translation framework. Instead of solving each task relying on task-specific datasets and models, we formalize the task as a translation between task-specific input text and output triples. By taking the task-specific input, we enable a task-agnostic translation by leveraging the latent knowledge that a pre-trained language model has about the task. We further demonstrate that a simple pre-training task of predicting which... | Chenguang Wang, Xiao Liu, Zui Chen, Haoyun Hong, Jie Tang, Dawn Song |  |
| 564 |  |  [Learning Logic Rules for Document-Level Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.95) |  | 0 | Document-level relation extraction aims to identify relations between entities in a whole document. Prior efforts to capture long-range dependencies have relied heavily on implicitly powerful representations learned through (graph) neural networks, which makes the model less transparent. To tackle this challenge, in this paper, we propose LogiRE, a novel probabilistic model for document-level relation extraction by learning logic rules. LogiRE treats logic rules as latent variables and consists... | Dongyu Ru, Changzhi Sun, Jiangtao Feng, Lin Qiu, Hao Zhou, Weinan Zhang, Yong Yu, Lei Li |  |
| 565 |  |  [A Large-Scale Dataset for Empathetic Response Generation](https://doi.org/10.18653/v1/2021.emnlp-main.96) |  | 0 | Recent development in NLP shows a strong trend towards refining pre-trained models with a domain-specific dataset. This is especially the case for response generation where emotion plays an important role. However, existing empathetic datasets remain small, delaying research efforts in this area, for example, the development of emotion-aware chatbots. One main technical challenge has been the cost of manually annotating dialogues with the right emotion labels. In this paper, we describe a... | Anuradha Welivita, Yubo Xie, Pearl Pu |  |
| 566 |  |  [The Perils of Using Mechanical Turk to Evaluate Open-Ended Text Generation](https://doi.org/10.18653/v1/2021.emnlp-main.97) |  | 0 | Recent text generation research has increasingly focused on open-ended domains such as story and poetry generation. Because models built for such tasks are difficult to evaluate automatically, most researchers in the space justify their modeling choices by collecting crowdsourced human judgments of text quality (e.g., Likert scores of coherence or grammaticality) from Amazon Mechanical Turk (AMT). In this paper, we first conduct a survey of 45 open-ended text generation papers and find that the... | Marzena Karpinska, Nader Akoury, Mohit Iyyer |  |
| 567 |  |  [Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus](https://doi.org/10.18653/v1/2021.emnlp-main.98) |  | 0 | Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of... | Jesse Dodge, Maarten Sap, Ana Marasovic, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, Matt Gardner |  |
| 568 |  |  [AfroMT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages](https://doi.org/10.18653/v1/2021.emnlp-main.99) |  | 0 | Reproducible benchmarks are crucial in driving progress of machine translation research. However, existing machine translation benchmarks have been mostly limited to high-resource or well-represented languages. Despite an increasing interest in low-resource machine translation, there are no standardized reproducible benchmarks for many African languages, many of which are used by millions of speakers but have less digitized textual data. To tackle these challenges, we propose AfroMT, a... | Machel Reid, Junjie Hu, Graham Neubig, Yutaka Matsuo |  |
| 569 |  |  [Evaluating the Evaluation Metrics for Style Transfer: A Case Study in Multilingual Formality Transfer](https://doi.org/10.18653/v1/2021.emnlp-main.100) |  | 0 | While the field of style transfer (ST) has been growing rapidly, it has been hampered by a lack of standardized practices for automatic evaluation. In this paper, we evaluate leading automatic metrics on the oft-researched task of formality style transfer. Unlike previous evaluations, which focus solely on English, we expand our focus to Brazilian-Portuguese, French, and Italian, making this work the first multilingual evaluation of metrics in ST. We outline best practices for automatic... | Eleftheria Briakou, Sweta Agrawal, Joel R. Tetreault, Marine Carpuat |  |
| 570 |  |  [MS-Mentions: Consistently Annotating Entity Mentions in Materials Science Procedural Text](https://doi.org/10.18653/v1/2021.emnlp-main.101) |  | 0 | Material science synthesis procedures are a promising domain for scientific NLP, as proper modeling of these recipes could provide insight into new ways of creating materials. However, a fundamental challenge in building information extraction models for material science synthesis procedures is getting accurate labels for the materials, operations, and other entities of those procedures. We present a new corpus of entity mention annotations over 595 Material Science synthesis procedural texts... | Tim O'Gorman, Zach Jensen, Sheshera Mysore, Kevin Huang, Rubayyat Mahbub, Elsa A. Olivetti, Andrew McCallum |  |
| 571 |  |  [Understanding Politics via Contextualized Discourse Processing](https://doi.org/10.18653/v1/2021.emnlp-main.102) |  | 0 | Politicians often have underlying agendas when reacting to events. Arguments in contexts of various events reflect a fairly consistent set of agendas for a given entity. In spite of recent advances in Pretrained Language Models, those text representations are not designed to capture such nuanced patterns. In this paper, we propose a Compositional Reader model consisting of encoder and composer modules, that captures and leverages such information to generate more effective representations for... | Rajkumar Pujari, Dan Goldwasser |  |
| 572 |  |  [Conundrums in Event Coreference Resolution: Making Sense of the State of the Art](https://doi.org/10.18653/v1/2021.emnlp-main.103) |  | 0 | Despite recent promising results on the application of span-based models for event reference interpretation, there is a lack of understanding of what has been improved. We present an empirical analysis of a state-of-the-art span-based event reference systems with the goal of providing the general NLP audience with a better understanding of the state of the art and reference researchers with directions for future research. | Jing Lu, Vincent Ng |  |
| 573 |  |  [Weakly supervised discourse segmentation for multiparty oral conversations](https://doi.org/10.18653/v1/2021.emnlp-main.104) |  | 0 | Discourse segmentation, the first step of discourse analysis, has been shown to improve results for text summarization, translation and other NLP tasks. While segmentation models for written text tend to perform well, they are not directly applicable to spontaneous, oral conversation, which has linguistic features foreign to written text. Segmentation is less studied for this type of language, where annotated data is scarce, and existing corpora more heterogeneous. We develop a weak supervision... | Lila Gravellier, Julie Hunter, Philippe Muller, Thomas Pellegrini, Isabelle Ferrané |  |
| 574 |  |  [Narrative Embedding: Re-Contextualization Through Attention](https://doi.org/10.18653/v1/2021.emnlp-main.105) |  | 0 | Narrative analysis is becoming increasingly important for a number of linguistic tasks including summarization, knowledge extraction, and question answering. We present a novel approach for narrative event representation using attention to re-contextualize events across the whole story. Comparing to previous analysis we find an unexpected attachment of event semantics to predicate tokens within a popular transformer model. We test the utility of our approach on narrative completion prediction,... | Sean Wilner, Daniel Woolridge, Madeleine Glick |  |
| 575 |  |  [Focus on what matters: Applying Discourse Coherence Theory to Cross Document Coreference](https://doi.org/10.18653/v1/2021.emnlp-main.106) |  | 0 | Performing event and entity coreference resolution across documents vastly increases the number of candidate mentions, making it intractable to do the full n2 pairwise comparisons. Existing approaches simplify by considering coreference only within document clusters, but this fails to handle inter-cluster coreference, common in many applications. As a result cross-document coreference algorithms are rarely applied to downstream tasks. We draw on an insight from discourse coherence theory:... | William Held, Dan Iter, Dan Jurafsky |  |
| 576 |  |  [Salience-Aware Event Chain Modeling for Narrative Understanding](https://doi.org/10.18653/v1/2021.emnlp-main.107) |  | 0 | Storytelling, whether via fables, news reports, documentaries, or memoirs, can be thought of as the communication of interesting and related events that, taken together, form a concrete process. It is desirable to extract the event chains that represent such processes. However, this extraction remains a challenging problem. We posit that this is due to the nature of the texts from which chains are discovered. Natural language text interleaves a narrative of concrete, salient events with... | Xiyang Zhang, Muhao Chen, Jonathan May |  |
| 577 |  |  [Asking It All: Generating Contextualized Questions for any Semantic Role](https://doi.org/10.18653/v1/2021.emnlp-main.108) |  | 0 | Asking questions about a situation is an inherent step towards understanding it. To this end, we introduce the task of role question generation, which, given a predicate mention and a passage, requires producing a set of questions asking about all possible semantic roles of the predicate. We develop a two-stage model for this task, which first produces a context-independent question prototype for each role and then revises it to be contextually appropriate for the passage. Unlike most existing... | Valentina Pyatkin, Paul Roit, Julian Michael, Yoav Goldberg, Reut Tsarfaty, Ido Dagan |  |
| 578 |  |  [Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders](https://doi.org/10.18653/v1/2021.emnlp-main.109) |  | 0 | Previous work has indicated that pretrained Masked Language Models (MLMs) are not effective as universal lexical and sentence encoders off-the-shelf, i.e., without further task-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks using annotated task data. In this work, we demonstrate that it is possible to turn MLMs into effective lexical and sentence encoders even without any additional data, relying simply on self-supervision. We propose an extremely simple, fast, and... | Fangyu Liu, Ivan Vulic, Anna Korhonen, Nigel Collier |  |
| 579 |  |  [RuleBERT: Teaching Soft Rules to Pre-Trained Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.110) |  | 0 | While pre-trained language models (PLMs) are the go-to solution to tackle many natural language processing problems, they are still very limited in their ability to capture and to use common-sense knowledge. In fact, even if information is available in the form of approximate (soft) logical rules, it is not clear how to transfer it to a PLM in order to improve its performance for deductive reasoning tasks. Here, we aim to bridge this gap by teaching PLMs how to reason with soft Horn rules. We... | Mohammed Saeed, Naser Ahmadi, Preslav Nakov, Paolo Papotti |  |
| 580 |  |  [Stepmothers are mean and academics are pretentious: What do pretrained language models learn about you?](https://doi.org/10.18653/v1/2021.emnlp-main.111) |  | 0 | In this paper, we investigate what types of stereotypical information are captured by pretrained language models. We present the first dataset comprising stereotypical attributes of a range of social groups and propose a method to elicit stereotypes encoded by pretrained language models in an unsupervised fashion. Moreover, we link the emergent stereotypes to their manifestation as basic emotions as a means to study their emotional effects in a more generalized manner. To demonstrate how our... | Rochelle Choenni, Ekaterina Shutova, Robert van Rooij |  |
| 581 |  |  [ConSeC: Word Sense Disambiguation as Continuous Sense Comprehension](https://doi.org/10.18653/v1/2021.emnlp-main.112) |  | 0 | Supervised systems have nowadays become the standard recipe for Word Sense Disambiguation (WSD), with Transformer-based language models as their primary ingredient. However, while these systems have certainly attained unprecedented performances, virtually all of them operate under the constraining assumption that, given a context, each word can be disambiguated individually with no account of the other sense choices. To address this limitation and drop this assumption, we propose CONtinuous... | Edoardo Barba, Luigi Procopio, Roberto Navigli |  |
| 582 |  |  [Shortcutted Commonsense: Data Spuriousness in Deep Learning of Commonsense Reasoning](https://doi.org/10.18653/v1/2021.emnlp-main.113) |  | 0 | Commonsense is a quintessential human capacity that has been a core challenge to Artificial Intelligence since its inception. Impressive results in Natural Language Processing tasks, including in commonsense reasoning, have consistently been achieved with Transformer neural language models, even matching or surpassing human performance in some benchmarks. Recently, some of these advances have been called into question: so called data artifacts in the training data have been made evident as... | Ruben Branco, António Branco, João António Rodrigues, João Ricardo Silva |  |
| 583 |  |  [When differential privacy meets NLP: The devil is in the detail](https://doi.org/10.18653/v1/2021.emnlp-main.114) |  | 0 | Differential privacy provides a formal approach to privacy of individuals. Applications of differential privacy in various scenarios, such as protecting users’ original utterances, must satisfy certain mathematical properties. Our contribution is a formal analysis of ADePT, a differentially private auto-encoder for text rewriting (Krishna et al, 2021). ADePT achieves promising results on downstream tasks while providing tight privacy guarantees. Our proof reveals that ADePT is not... | Ivan Habernal |  |
| 584 |  |  [Achieving Model Robustness through Discrete Adversarial Training](https://doi.org/10.18653/v1/2021.emnlp-main.115) |  | 0 | Discrete adversarial attacks are symbolic perturbations to a language input that preserve the output label but lead to a prediction error. While such attacks have been extensively explored for the purpose of evaluating model robustness, their utility for improving robustness has been limited to offline augmentation only. Concretely, given a trained model, attacks are used to generate perturbed (adversarial) examples, and the model is re-trained exactly once. In this work, we address this gap... | Maor Ivgi, Jonathan Berant |  |
| 585 |  |  [Debiasing Methods in Natural Language Understanding Make Bias More Accessible](https://doi.org/10.18653/v1/2021.emnlp-main.116) |  | 0 | Model robustness to bias is often determined by the generalization on carefully designed out-of-distribution datasets. Recent debiasing methods in natural language understanding (NLU) improve performance on such datasets by pressuring models into making unbiased predictions. An underlying assumption behind such methods is that this also leads to the discovery of more robust features in the model’s inner representations. We propose a general probing-based framework that allows for post-hoc... | Michael Mendelson, Yonatan Belinkov |  |
| 586 |  |  [Evaluating the Robustness of Neural Language Models to Input Perturbations](https://doi.org/10.18653/v1/2021.emnlp-main.117) |  | 0 | High-performance neural language models have obtained state-of-the-art results on a wide range of Natural Language Processing (NLP) tasks. However, results for common benchmark datasets often do not reflect model reliability and robustness when applied to noisy, real-world data. In this study, we design and implement various types of character-level and word-level perturbation methods to simulate realistic scenarios in which input texts may be slightly noisy or different from the data... | Milad Moradi, Matthias Samwald |  |
| 587 |  |  [How much pretraining data do language models need to learn syntax?](https://doi.org/10.18653/v1/2021.emnlp-main.118) |  | 0 | Transformers-based pretrained language models achieve outstanding results in many well-known NLU benchmarks. However, while pretraining methods are very convenient, they are expensive in terms of time and resources. This calls for a study of the impact of pretraining data size on the knowledge of the models. We explore this impact on the syntactic capabilities of RoBERTa, using models trained on incremental sizes of raw text data. First, we use syntactic structural probes to determine whether... | Laura PérezMayos, Miguel Ballesteros, Leo Wanner |  |
| 588 |  |  [Sorting through the noise: Testing robustness of information processing in pre-trained language models](https://doi.org/10.18653/v1/2021.emnlp-main.119) |  | 0 | Pre-trained LMs have shown impressive performance on downstream NLP tasks, but we have yet to establish a clear understanding of their sophistication when it comes to processing, retaining, and applying information presented in their input. In this paper we tackle a component of this question by examining robustness of models’ ability to deploy relevant context information in the face of distracting content. We present models with cloze tasks requiring use of critical context information, and... | Lalchand Pandia, Allyson Ettinger |  |
| 589 |  |  [Contrastive Explanations for Model Interpretability](https://doi.org/10.18653/v1/2021.emnlp-main.120) |  | 0 | Contrastive explanations clarify why an event occurred in contrast to another. They are inherently intuitive to humans to both produce and comprehend. We propose a method to produce contrastive explanations in the latent space, via a projection of the input representation, such that only the features that differentiate two potential decisions are captured. Our modification allows model behavior to consider only contrastive reasoning, and uncover which aspects of the input are useful for and... | Alon Jacovi, Swabha Swayamdipta, Shauli Ravfogel, Yanai Elazar, Yejin Choi, Yoav Goldberg |  |
| 590 |  |  [On the Transferability of Adversarial Attacks against Neural Text Classifier](https://doi.org/10.18653/v1/2021.emnlp-main.121) |  | 0 | Deep neural networks are vulnerable to adversarial attacks, where a small perturbation to an input alters the model prediction. In many cases, malicious inputs intentionally crafted for one model can fool another model. In this paper, we present the first study to systematically investigate the transferability of adversarial examples for text classification models and explore how various factors, including network architecture, tokenization scheme, word embedding, and model capacity, affect the... | Liping Yuan, Xiaoqing Zheng, Yi Zhou, ChoJui Hsieh, KaiWei Chang |  |
| 591 |  |  [Conditional probing: measuring usable information beyond a baseline](https://doi.org/10.18653/v1/2021.emnlp-main.122) |  | 0 | Probing experiments investigate the extent to which neural representations make properties—like part-of-speech—predictable. One suggests that a representation encodes a property if probing that representation produces higher accuracy than probing a baseline representation like non-contextual word embeddings. Instead of using baselines as a point of comparison, we’re interested in measuring information that is contained in the representation but not in the baseline. For example, current methods... | John Hewitt, Kawin Ethayarajh, Percy Liang, Christopher D. Manning |  |
| 592 |  |  [GFST: Gender-Filtered Self-Training for More Accurate Gender in Translation](https://doi.org/10.18653/v1/2021.emnlp-main.123) |  | 0 | Targeted evaluations have found that machine translation systems often output incorrect gender in translations, even when the gender is clear from context. Furthermore, these incorrectly gendered translations have the potential to reflect or amplify social biases. We propose gender-filtered self-training (GFST) to improve gender translation accuracy on unambiguously gendered inputs. Our GFST approach uses a source monolingual corpus and an initial model to generate gender-specific... | Prafulla Kumar Choubey, Anna Currey, Prashant Mathur, Georgiana Dinu |  |
| 593 |  |  ["Wikily" Supervised Neural Translation Tailored to Cross-Lingual Tasks](https://doi.org/10.18653/v1/2021.emnlp-main.124) |  | 0 | We present a simple but effective approach for leveraging Wikipedia for neural machine translation as well as cross-lingual tasks of image captioning and dependency parsing without using any direct supervision from external parallel data or supervised models in the target language. We show that first sentences and titles of linked Wikipedia pages, as well as cross-lingual image captions, are strong signals for a seed parallel data to extract bilingual dictionaries and cross-lingual word... | Mohammad Sadegh Rasooli, Chris CallisonBurch, Derry Tanti Wijaya |  |
| 594 |  |  [mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs](https://doi.org/10.18653/v1/2021.emnlp-main.125) |  | 0 | Multilingual T5 pretrains a sequence-to-sequence model on massive monolingual texts, which has shown promising results on many cross-lingual tasks. In this paper, we improve multilingual text-to-text transfer Transformer with translation pairs (mT6). Specifically, we explore three cross-lingual text-to-text pre-training tasks, namely, machine translation, translation pair span corruption, and translation span corruption. In addition, we propose a partially non-autoregressive objective for... | Zewen Chi, Li Dong, Shuming Ma, Shaohan Huang, Saksham Singhal, XianLing Mao, Heyan Huang, Xia Song, Furu Wei |  |
| 595 |  |  [Improving Zero-Shot Cross-Lingual Transfer Learning via Robust Training](https://doi.org/10.18653/v1/2021.emnlp-main.126) |  | 0 | Pre-trained multilingual language encoders, such as multilingual BERT and XLM-R, show great potential for zero-shot cross-lingual transfer. However, these multilingual encoders do not precisely align words and phrases across languages. Especially, learning alignments in the multilingual embedding space usually requires sentence-level or word-level parallel corpora, which are expensive to be obtained for low-resource languages. An alternative is to make the multilingual encoders more robust;... | KuanHao Huang, Wasi Uddin Ahmad, Nanyun Peng, KaiWei Chang |  |
| 596 |  |  [Speechformer: Reducing Information Loss in Direct Speech Translation](https://doi.org/10.18653/v1/2021.emnlp-main.127) |  | 0 | Transformer-based models have gained increasing popularity achieving state-of-the-art performance in many research fields including speech translation. However, Transformer’s quadratic complexity with respect to the input sequence length prevents its adoption as is with audio signals, which are typically represented by long sequences. Current solutions resort to an initial sub-optimal compression based on a fixed sampling of raw audio features. Therefore, potentially useful linguistic... | Sara Papi, Marco Gaido, Matteo Negri, Marco Turchi |  |
| 597 |  |  [Is "moby dick" a Whale or a Bird? Named Entities and Terminology in Speech Translation](https://doi.org/10.18653/v1/2021.emnlp-main.128) |  | 0 | Automatic translation systems are known to struggle with rare words. Among these, named entities (NEs) and domain-specific terms are crucial, since errors in their translation can lead to severe meaning distortions. Despite their importance, previous speech translation (ST) studies have neglected them, also due to the dearth of publicly available resources tailored to their specific evaluation. To fill this gap, we i) present the first systematic analysis of the behavior of state-of-the-art ST... | Marco Gaido, Susana Rodríguez, Matteo Negri, Luisa Bentivogli, Marco Turchi |  |
| 598 |  |  [HintedBT: Augmenting Back-Translation with Quality and Transliteration Hints](https://doi.org/10.18653/v1/2021.emnlp-main.129) |  | 0 | Back-translation (BT) of target monolingual corpora is a widely used data augmentation strategy for neural machine translation (NMT), especially for low-resource language pairs. To improve effectiveness of the available BT data, we introduce HintedBT—a family of techniques which provides hints (through tags) to the encoder and decoder. First, we propose a novel method of using both high and low quality BT data by providing hints (as source tags on the encoder) to the model about the quality of... | Sahana Ramnath, Melvin Johnson, Abhirut Gupta, Aravindan Raghuveer |  |
| 599 |  |  [Translation-based Supervision for Policy Generation in Simultaneous Neural Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.130) |  | 0 | In simultaneous machine translation, finding an agent with the optimal action sequence of reads and writes that maintain a high level of translation quality while minimizing the average lag in producing target tokens remains an extremely challenging problem. We propose a novel supervised learning approach for training an agent that can detect the minimum number of reads required for generating each target token by comparing simultaneous translations against full-sentence translations during... | Ashkan Alinejad, Hassan S. Shavarani, Anoop Sarkar |  |
| 600 |  |  [Nearest Neighbour Few-Shot Learning for Cross-lingual Classification](https://doi.org/10.18653/v1/2021.emnlp-main.131) |  | 0 | Even though large pre-trained multilingual models (e.g. mBERT, XLM-R) have led to significant performance gains on a wide range of cross-lingual NLP tasks, success on many downstream tasks still relies on the availability of sufficient annotated data. Traditional fine-tuning of pre-trained models using only a few target samples can cause over-fitting. This can be quite limiting as most languages in the world are under-resourced. In this work, we investigate cross-lingual adaptation using a... | M. Saiful Bari, Batool Haider, Saab Mansour |  |
| 601 |  |  [Cross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.132) |  | 0 | We study the power of cross-attention in the Transformer architecture within the context of transfer learning for machine translation, and extend the findings of studies into cross-attention when training from scratch. We conduct a series of experiments through fine-tuning a translation model on data where either the source or target language has changed. These experiments reveal that fine-tuning only the cross-attention parameters is nearly as effective as fine-tuning all parameters (i.e., the... | Mozhdeh Gheini, Xiang Ren, Jonathan May |  |
| 602 |  |  [Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent](https://doi.org/10.18653/v1/2021.emnlp-main.133) |  | 0 | The capacity of neural networks like the widely adopted transformer is known to be very high. Evidence is emerging that they learn successfully due to inductive bias in the training routine, typically a variant of gradient descent (GD). To better understand this bias, we study the tendency for transformer parameters to grow in magnitude (ℓ2 norm) during training, and its implications for the emergent representations within self attention layers. Empirically, we document norm growth in the... | William Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, Noah A. Smith |  |
| 603 |  |  [Foreseeing the Benefits of Incidental Supervision](https://doi.org/10.18653/v1/2021.emnlp-main.134) |  | 0 | Real-world applications often require improved models by leveraging \*a range of cheap incidental supervision signals\*. These could include partial labels, noisy labels, knowledge-based constraints, and cross-domain or cross-task annotations – all having statistical associations with gold annotations but not exactly the same. However, we currently lack a principled way to measure the benefits of these signals to a given target task, and the common practice of evaluating these benefits is... | Hangfeng He, Mingyuan Zhang, Qiang Ning, Dan Roth |  |
| 604 |  |  [Competency Problems: On Finding and Removing Artifacts in Language Data](https://doi.org/10.18653/v1/2021.emnlp-main.135) |  | 0 | Much recent work in NLP has documented dataset artifacts, bias, and spurious correlations between input features and output labels. However, how to tell which features have “spurious” instead of legitimate correlations is typically left unspecified. In this work we argue that for complex language understanding tasks, all simple feature correlations are spurious, and we formalize this notion into a class of problems which we call competency problems. For example, the word “amazing” on its own... | Matt Gardner, William Merrill, Jesse Dodge, Matthew E. Peters, Alexis Ross, Sameer Singh, Noah A. Smith |  |
| 605 |  |  [Knowledge-Aware Meta-learning for Low-Resource Text Classification](https://doi.org/10.18653/v1/2021.emnlp-main.136) |  | 0 | Meta-learning has achieved great success in leveraging the historical learned knowledge to facilitate the learning process of the new task. However, merely learning the knowledge from the historical tasks, adopted by current meta-learning algorithms, may not generalize well to testing tasks when they are not well-supported by training tasks. This paper studies a low-resource text classification problem and bridges the gap between meta-training and meta-testing tasks by leveraging the external... | Huaxiu Yao, Yingxin Wu, Maruan AlShedivat, Eric P. Xing |  |
| 606 |  |  [Sentence Bottleneck Autoencoders from Transformer Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.137) |  | 0 | Representation learning for text via pretraining a language model on a large corpus has become a standard starting point for building NLP systems. This approach stands in contrast to autoencoders, also trained on raw text, but with the objective of learning to encode each input as a vector that allows full reconstruction. Autoencoders are attractive because of their latent space structure and generative properties. We therefore explore the construction of a sentence-level autoencoder from a... | Ivan Montero, Nikolaos Pappas, Noah A. Smith |  |
| 607 |  |  [Efficient Contrastive Learning via Novel Data Augmentation and Curriculum Learning](https://doi.org/10.18653/v1/2021.emnlp-main.138) |  | 0 | We introduce EfficientCL, a memory-efficient continual pretraining method that applies contrastive learning with novel data augmentation and curriculum learning. For data augmentation, we stack two types of operation sequentially: cutoff and PCA jittering. While pretraining steps proceed, we apply curriculum learning by incrementing the augmentation degree for each difficulty step. After data augmentation is finished, contrastive learning is applied on projected embeddings of original and... | Seonghyeon Ye, Jiseon Kim, Alice Oh |  |
| 608 |  |  [CR-Walker: Tree-Structured Graph Reasoning and Dialog Acts for Conversational Recommendation](https://doi.org/10.18653/v1/2021.emnlp-main.139) |  | 0 | Growing interests have been attracted in Conversational Recommender Systems (CRS), which explore user preference through conversational interactions in order to make appropriate recommendation. However, there is still a lack of ability in existing CRS to (1) traverse multiple reasoning paths over background knowledge to introduce relevant items and attributes, and (2) arrange selected entities appropriately under current system intents to control response generation. To address these issues, we... | Wenchang Ma, Ryuichi Takanobu, Minlie Huang |  |
| 609 |  |  [DIALKI: Knowledge Identification in Conversational Systems through Dialogue-Document Contextualization](https://doi.org/10.18653/v1/2021.emnlp-main.140) |  | 0 | Identifying relevant knowledge to be used in conversational systems that are grounded in long documents is critical to effective response generation. We introduce a knowledge identification model that leverages the document structure to provide dialogue-contextualized passage encodings and better locate knowledge relevant to the conversation. An auxiliary loss captures the history of dialogue-document connections. We demonstrate the effectiveness of our model on two document-grounded... | Zeqiu Wu, BoRu Lu, Hannaneh Hajishirzi, Mari Ostendorf |  |
| 610 |  |  [Iconary: A Pictionary-Based Game for Testing Multimodal Communication with Drawings and Text](https://doi.org/10.18653/v1/2021.emnlp-main.141) |  | 0 | Communicating with humans is challenging for AIs because it requires a shared understanding of the world, complex semantics (e.g., metaphors or analogies), and at times multi-modal gestures (e.g., pointing with a finger, or an arrow in a diagram). We investigate these challenges in the context of Iconary, a collaborative game of drawing and guessing based on Pictionary, that poses a novel challenge for the research community. In Iconary, a Guesser tries to identify a phrase that a Drawer is... | Christopher Clark, Jordi Salvador, Dustin Schwenk, Derrick Bonafilia, Mark Yatskar, Eric Kolve, Alvaro Herrasti, Jonghyun Choi, Sachin Mehta, Sam Skjonsberg, Carissa Schoenick, Aaron Sarnat, Hannaneh Hajishirzi, Aniruddha Kembhavi, Oren Etzioni, Ali Farhadi |  |
| 611 |  |  [Self-training Improves Pre-training for Few-shot Learning in Task-oriented Dialog Systems](https://doi.org/10.18653/v1/2021.emnlp-main.142) |  | 0 | As the labeling cost for different modules in task-oriented dialog (ToD) systems is expensive, a major challenge is to train different modules with the least amount of labeled data. Recently, large-scale pre-trained language models, have shown promising results for few-shot learning in ToD. In this paper, we devise a self-training approach to utilize the abundant unlabeled dialog data to further improve state-of-the-art pre-trained models in few-shot learning scenarios for ToD systems.... | Fei Mi, Wanhao Zhou, Lingjing Kong, Fengyu Cai, Minlie Huang, Boi Faltings |  |
| 612 |  |  [Contextual Rephrase Detection for Reducing Friction in Dialogue Systems](https://doi.org/10.18653/v1/2021.emnlp-main.143) |  | 0 | For voice assistants like Alexa, Google Assistant, and Siri, correctly interpreting users’ intentions is of utmost importance. However, users sometimes experience friction with these assistants, caused by errors from different system components or user errors such as slips of the tongue. Users tend to rephrase their queries until they get a satisfactory response. Rephrase detection is used to identify the rephrases and has long been treated as a task with pairwise input, which does not fully... | Zhuoyi Wang, Saurabh Gupta, Jie Hao, Xing Fan, Dingcheng Li, Alexander Hanbo Li, Chenlei Guo |  |
| 613 |  |  [Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning](https://doi.org/10.18653/v1/2021.emnlp-main.144) |  | 0 | In this work, we focus on a more challenging few-shot intent detection scenario where many intents are fine-grained and semantically similar. We present a simple yet effective few-shot intent detection schema via contrastive pre-training and fine-tuning. Specifically, we first conduct self-supervised contrastive pre-training on collected intent datasets, which implicitly learns to discriminate semantically similar utterances without using any labels. We then perform few-shot intent detection... | JianGuo Zhang, Trung Bui, Seunghyun Yoon, Xiang Chen, Zhiwei Liu, Congying Xia, Quan Hung Tran, Walter Chang, Philip S. Yu |  |
| 614 |  |  ["It doesn't look good for a date": Transforming Critiques into Preferences for Conversational Recommendation Systems](https://doi.org/10.18653/v1/2021.emnlp-main.145) |  | 0 | Conversations aimed at determining good recommendations are iterative in nature. People often express their preferences in terms of a critique of the current recommendation (e.g., “It doesn’t look good for a date”), requiring some degree of common sense for a preference to be inferred. In this work, we present a method for transforming a user critique into a positive preference (e.g., “I prefer more romantic”) in order to retrieve reviews pertaining to potentially better recommendations (e.g.,... | Victor S. Bursztyn, Jennifer Healey, Nedim Lipka, Eunyee Koh, Doug Downey, Larry Birnbaum |  |
| 615 |  |  [AttentionRank: Unsupervised Keyphrase Extraction using Self and Cross Attentions](https://doi.org/10.18653/v1/2021.emnlp-main.146) |  | 0 | Keyword or keyphrase extraction is to identify words or phrases presenting the main topics of a document. This paper proposes the AttentionRank, a hybrid attention model, to identify keyphrases from a document in an unsupervised manner. AttentionRank calculates self-attention and cross-attention using a pre-trained language model. The self-attention is designed to determine the importance of a candidate within the context of a sentence. The cross-attention is calculated to identify the semantic... | Haoran Ding, Xiao Luo |  |
| 616 |  |  [Unsupervised Relation Extraction: A Variational Autoencoder Approach](https://doi.org/10.18653/v1/2021.emnlp-main.147) |  | 0 | Unsupervised relation extraction works by clustering entity pairs that have the same relations in the text. Some existing variational autoencoder (VAE)-based approaches train the relation extraction model as an encoder that generates relation classifications. A decoder is trained along with the encoder to reconstruct the encoder input based on the encoder-generated relation classifications. These classifications are a latent variable so they are required to follow a pre-defined prior... | Chenhan Yuan, Hoda Eldardiry |  |
| 617 |  |  [Robust Retrieval Augmented Generation for Zero-shot Slot Filling](https://doi.org/10.18653/v1/2021.emnlp-main.148) |  | 0 | Automatically inducing high quality knowledge graphs from a given collection of documents still remains a challenging problem in AI. One way to make headway for this problem is through advancements in a related task known as slot filling. In this task, given an entity query in form of [Entity, Slot, ?], a system is asked to ‘fill’ the slot by generating or extracting the missing value exploiting evidence extracted from relevant passage(s) in the given document collection. The recent works in... | Michael R. Glass, Gaetano Rossiello, Md. Faisal Mahbub Chowdhury, Alfio Gliozzo |  |
| 618 |  |  [Everything Is All It Takes: A Multipronged Strategy for Zero-Shot Cross-Lingual Information Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.149) |  | 0 | Zero-shot cross-lingual information extraction (IE) describes the construction of an IE model for some target language, given existing annotations exclusively in some other language, typically English. While the advance of pretrained multilingual encoders suggests an easy optimism of “train on English, run on any language”, we find through a thorough exploration and extension of techniques that a combination of approaches, both new and old, leads to better performance than any one cross-lingual... | Mahsa Yarmohammadi, Shijie Wu, Marc Marone, Haoran Xu, Seth Ebner, Guanghui Qin, Yunmo Chen, Jialiang Guo, Craig Harman, Kenton Murray, Aaron Steven White, Mark Dredze, Benjamin Van Durme |  |
| 619 |  |  [Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies](https://doi.org/10.18653/v1/2021.emnlp-main.150) |  | 0 | Gender is widely discussed in the context of language tasks and when examining the stereotypes propagated by language models. However, current discussions primarily treat gender as binary, which can perpetuate harms such as the cyclical erasure of non-binary gender identities. These harms are driven by model and dataset biases, which are consequences of the non-recognition and lack of understanding of non-binary genders in society. In this paper, we explain the complexity of gender and language... | Sunipa Dev, Masoud Monajatipoor, Anaelia Ovalle, Arjun Subramonian, Jeff M. Phillips, KaiWei Chang |  |
| 620 |  |  [Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search](https://doi.org/10.18653/v1/2021.emnlp-main.151) |  | 0 | Internet search affects people’s cognition of the world, so mitigating biases in search results and learning fair models is imperative for social good. We study a unique gender bias in image search in this work: the search images are often gender-imbalanced for gender-neutral natural language queries. We diagnose two typical image search models, the specialized model trained on in-domain datasets and the generalized representation model pre-trained on massive image and text data across the... | Jialu Wang, Yang Liu, Xin Eric Wang |  |
| 621 |  |  [Style Pooling: Automatic Text Style Obfuscation for Improved Classification Fairness](https://doi.org/10.18653/v1/2021.emnlp-main.152) |  | 0 | Text style can reveal sensitive attributes of the author (e.g. age and race) to the reader, which can, in turn, lead to privacy violations and bias in both human and algorithmic decisions based on text. For example, the style of writing in job applications might reveal protected attributes of the candidate which could lead to bias in hiring decisions, regardless of whether hiring decisions are made algorithmically or by humans. We propose a VAE-based framework that obfuscates stylistic features... | Fatemehsadat Mireshghallah, Taylor BergKirkpatrick |  |
| 622 |  |  [Modeling Disclosive Transparency in NLP Application Descriptions](https://doi.org/10.18653/v1/2021.emnlp-main.153) |  | 0 | Broader disclosive transparency—truth and clarity in communication regarding the function of AI systems—is widely considered desirable. Unfortunately, it is a nebulous concept, difficult to both define and quantify. This is problematic, as previous work has demonstrated possible trade-offs and negative consequences to disclosive transparency, such as a confusion effect, where “too much information” clouds a reader’s understanding of what a system description means. Disclosive transparency’s... | Michael Saxon, Sharon Levy, Xinyi Wang, Alon Albalak, William Yang Wang |  |
| 623 |  |  [Reconstruction Attack on Instance Encoding for Language Understanding](https://doi.org/10.18653/v1/2021.emnlp-main.154) |  | 0 | A private learning scheme TextHide was recently proposed to protect the private text data during the training phase via so-called instance encoding. We propose a novel reconstruction attack to break TextHide by recovering the private training data, and thus unveil the privacy risks of instance encoding. We have experimentally validated the effectiveness of the reconstruction attack with two commonly-used datasets for sentence classification. Our attack would advance the development of privacy... | Shangyu Xie, Yuan Hong |  |
| 624 |  |  [Fairness-aware Class Imbalanced Learning](https://doi.org/10.18653/v1/2021.emnlp-main.155) |  | 0 | Class imbalance is a common challenge in many NLP tasks, and has clear connections to bias, in that bias in training data often leads to higher accuracy for majority groups at the expense of minority groups. However there has traditionally been a disconnect between research on class-imbalanced learning and mitigating bias, and only recently have the two been looked at through a common lens. In this work we evaluate long-tail learning methods for tweet sentiment and occupation classification,... | Shivashankar Subramanian, Afshin Rahimi, Timothy Baldwin, Trevor Cohn, Lea Frermann |  |
| 625 |  |  [CRYPTOGRU: Low Latency Privacy-Preserving Text Analysis With GRU](https://doi.org/10.18653/v1/2021.emnlp-main.156) |  | 0 | Homomorphic encryption (HE) and garbled circuit (GC) provide the protection for users’ privacy. However, simply mixing the HE and GC in RNN models suffer from long inference latency due to slow activation functions. In this paper, we present a novel hybrid structure of HE and GC gated recurrent unit (GRU) network, , for low-latency secure inferences. replaces computationally expensive GC-based tanh with fast GC-based ReLU, and then quantizes sigmoid and ReLU to smaller bit-length to accelerate... | Bo Feng, Qian Lou, Lei Jiang, Geoffrey C. Fox |  |
| 626 |  |  [Local Word Discovery for Interactive Transcription](https://doi.org/10.18653/v1/2021.emnlp-main.157) |  | 0 | Human expertise and the participation of speech communities are essential factors in the success of technologies for low-resource languages. Accordingly, we propose a new computational task which is tuned to the available knowledge and interests in an Indigenous community, and which supports the construction of high quality texts and lexicons. The task is illustrated for Kunwinjku, a morphologically-complex Australian language. We combine a finite state implementation of a published grammar... | William Lane, Steven Bird |  |
| 627 |  |  [Segment, Mask, and Predict: Augmenting Chinese Word Segmentation with Self-Supervision](https://doi.org/10.18653/v1/2021.emnlp-main.158) |  | 0 | Recent state-of-the-art (SOTA) effective neural network methods and fine-tuning methods based on pre-trained models (PTM) have been used in Chinese word segmentation (CWS), and they achieve great results. However, previous works focus on training the models with the fixed corpus at every iteration. The intermediate generated information is also valuable. Besides, the robustness of the previous neural methods is limited by the large-scale annotated data. There are a few noises in the annotated... | Mieradilijiang Maimaiti, Yang Liu, Yuanhang Zheng, Gang Chen, Kaiyu Huang, Ji Zhang, Huanbo Luan, Maosong Sun |  |
| 628 |  |  [Minimal Supervision for Morphological Inflection](https://doi.org/10.18653/v1/2021.emnlp-main.159) |  | 0 | Neural models for the various flavours of morphological reinflection tasks have proven to be extremely accurate given ample labeled data, yet labeled data may be slow and costly to obtain. In this work we aim to overcome this annotation bottleneck by bootstrapping labeled data from a seed as small as five labeled inflection tables, accompanied by a large bulk of unlabeled text. Our bootstrapping method exploits the orthographic and semantic regularities in morphological systems in a two-phased... | Omer Goldman, Reut Tsarfaty |  |
| 629 |  |  [Fast WordPiece Tokenization](https://doi.org/10.18653/v1/2021.emnlp-main.160) |  | 0 | Tokenization is a fundamental preprocessing step for almost all NLP tasks. In this paper, we propose efficient algorithms for the WordPiece tokenization used in BERT, from single-word tokenization to general text (e.g., sentence) tokenization. When tokenizing a single word, WordPiece uses a longest-match-first strategy, known as maximum matching. The best known algorithms so far are O(nˆ2) (where n is the input length) or O(nm) (where m is the maximum vocabulary token length). We propose a... | Xinying Song, Alex Salcianu, Yang Song, Dave Dopson, Denny Zhou |  |
| 630 |  |  [You should evaluate your language model on marginal likelihood over tokenisations](https://doi.org/10.18653/v1/2021.emnlp-main.161) |  | 0 | Neural language models typically tokenise input text into sub-word units to achieve an open vocabulary. The standard approach is to use a single canonical tokenisation at both train and test time. We suggest that this approach is unsatisfactory and may bottleneck our evaluation of language model performance. Using only the one-best tokenisation ignores tokeniser uncertainty over alternative tokenisations, which may hurt model out-of-domain performance. In this paper, we argue that instead,... | Kris Cao, Laura Rimell |  |
| 631 |  |  [Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning](https://doi.org/10.18653/v1/2021.emnlp-main.162) |  | 0 | Commonsense is defined as the knowledge on which everyone agrees. However, certain types of commonsense knowledge are correlated with culture and geographic locations and they are only shared locally. For example, the scenes of wedding ceremonies vary across regions due to different customs influenced by historical and religious factors. Such regional characteristics, however, are generally omitted in prior work. In this paper, we construct a Geo-Diverse Visual Commonsense Reasoning dataset... | Da Yin, Liunian Harold Li, Ziniu Hu, Nanyun Peng, KaiWei Chang |  |
| 632 |  |  [Reference-Centric Models for Grounded Collaborative Dialogue](https://doi.org/10.18653/v1/2021.emnlp-main.163) |  | 0 | We present a grounded neural dialogue model that successfully collaborates with people in a partially-observable reference game. We focus on a setting where two agents each observe an overlapping part of a world context and need to identify and agree on some object they share. Therefore, the agents should pool their information and communicate pragmatically to solve the task. Our dialogue agent accurately grounds referents from the partner’s utterances using a structured reference resolver,... | Daniel Fried, Justin T. Chiu, Dan Klein |  |
| 633 |  |  [CrossVQA: Scalably Generating Benchmarks for Systematically Testing VQA Generalization](https://doi.org/10.18653/v1/2021.emnlp-main.164) |  | 0 | One challenge in evaluating visual question answering (VQA) models in the cross-dataset adaptation setting is that the distribution shifts are multi-modal, making it difficult to identify if it is the shifts in visual or language features that play a key role. In this paper, we propose a semi-automatic framework for generating disentangled shifts by introducing a controllable visual question-answer generation (VQAG) module that is capable of generating highly-relevant and diverse... | Arjun R. Akula, Soravit Changpinyo, Boqing Gong, Piyush Sharma, SongChun Zhu, Radu Soricut |  |
| 634 |  |  [Visual Goal-Step Inference using wikiHow](https://doi.org/10.18653/v1/2021.emnlp-main.165) |  | 0 | Understanding what sequence of steps are needed to complete a goal can help artificial intelligence systems reason about human activities. Past work in NLP has examined the task of goal-step inference for text. We introduce the visual analogue. We propose the Visual Goal-Step Inference (VGSI) task, where a model is given a textual goal and must choose which of four images represents a plausible step towards that goal. With a new dataset harvested from wikiHow consisting of 772,277 images... | Yue Yang, Artemis Panagopoulou, Qing Lyu, Li Zhang, Mark Yatskar, Chris CallisonBurch |  |
| 635 |  |  [Systematic Generalization on gSCAN: What is Nearly Solved and What is Next?](https://doi.org/10.18653/v1/2021.emnlp-main.166) |  | 0 | We analyze the grounded SCAN (gSCAN) benchmark, which was recently proposed to study systematic generalization for grounded language understanding. First, we study which aspects of the original benchmark can be solved by commonly used methods in multi-modal research. We find that a general-purpose Transformer-based model with cross-modal attention achieves strong performance on a majority of the gSCAN splits, surprisingly outperforming more specialized approaches from prior work. Furthermore,... | Linlu Qiu, Hexiang Hu, Bowen Zhang, Peter Shaw, Fei Sha |  |
| 636 |  |  [Effect of Visual Extensions on Natural Language Understanding in Vision-and-Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.167) |  | 0 | A method for creating a vision-and-language (V&L) model is to extend a language model through structural modifications and V&L pre-training. Such an extension aims to make a V&L model inherit the capability of natural language understanding (NLU) from the original language model. To see how well this is achieved, we propose to evaluate V&L models using an NLU benchmark (GLUE). We compare five V&L models, including single-stream and dual-stream models, trained with the same pre-training.... | Taichi Iki, Akiko Aizawa |  |
| 637 |  |  [Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding](https://doi.org/10.18653/v1/2021.emnlp-main.168) |  | 0 | Dialogue systems powered by large pre-trained language models exhibit an innate ability to deliver fluent and natural-sounding responses. Despite their impressive performance, these models are fitful and can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving faithfulness and reducing hallucination of neural dialogue systems to known facts supplied by a Knowledge Graph (KG). We propose Neural Path Hunter which... | Nouha Dziri, Andrea Madotto, Osmar Zaïane, Avishek Joey Bose |  |
| 638 |  |  [Thinking Clearly, Talking Fast: Concept-Guided Non-Autoregressive Generation for Open-Domain Dialogue Systems](https://doi.org/10.18653/v1/2021.emnlp-main.169) |  | 0 | Human dialogue contains evolving concepts, and speakers naturally associate multiple concepts to compose a response. However, current dialogue models with the seq2seq framework lack the ability to effectively manage concept transitions and can hardly introduce multiple concepts to responses in a sequential decoding manner. To facilitate a controllable and coherent dialogue, in this work, we devise a concept-guided non-autoregressive model (CG-nAR) for open-domain dialogue generation. The... | Yicheng Zou, Zhihua Liu, Xingwu Hu, Qi Zhang |  |
| 639 |  |  [Perspective-taking and Pragmatics for Generating Empathetic Responses Focused on Emotion Causes](https://doi.org/10.18653/v1/2021.emnlp-main.170) |  | 0 | Empathy is a complex cognitive ability based on the reasoning of others’ affective states. In order to better understand others and express stronger empathy in dialogues, we argue that two issues must be tackled at the same time: (i) identifying which word is the cause for the other’s emotion from his or her utterance and (ii) reflecting those specific words in the response generation. However, previous approaches for recognizing emotion cause words in text require sub-utterance level... | Hyunwoo Kim, Byeongchang Kim, Gunhee Kim |  |
| 640 |  |  [Generation and Extraction Combined Dialogue State Tracking with Hierarchical Ontology Integration](https://doi.org/10.18653/v1/2021.emnlp-main.171) |  | 0 | Recently, the focus of dialogue state tracking has expanded from single domain to multiple domains. The task is characterized by the shared slots between domains. As the scenario gets more complex, the out-of-vocabulary problem also becomes severer. Current models are not satisfactory for solving the challenges of ontology integration between domains and out-of-vocabulary problems. To address the problem, we explore the hierarchical semantic of ontology and enhance the interrelation between... | Xinmeng Li, Qian Li, Wansen Wu, Quanjun Yin |  |
| 641 |  |  [CoLV: A Collaborative Latent Variable Model for Knowledge-Grounded Dialogue Generation](https://doi.org/10.18653/v1/2021.emnlp-main.172) |  | 0 | Knowledge-grounded dialogue generation has achieved promising performance with the engagement of external knowledge sources. Typical approaches towards this task usually perform relatively independent two sub-tasks, i.e., knowledge selection and knowledge-aware response generation. In this paper, in order to improve the diversity of both knowledge selection and knowledge-aware response generation, we propose a collaborative latent variable (CoLV) model to integrate these two aspects... | Haolan Zhan, Lei Shen, Hongshen Chen, Hainan Zhang |  |
| 642 |  |  [A Three-Stage Learning Framework for Low-Resource Knowledge-Grounded Dialogue Generation](https://doi.org/10.18653/v1/2021.emnlp-main.173) |  | 0 | Neural conversation models have shown great potentials towards generating fluent and informative responses by introducing external background knowledge. Nevertheless, it is laborious to construct such knowledge-grounded dialogues, and existing models usually perform poorly when transfer to new domains with limited training samples. Therefore, building a knowledge-grounded dialogue system under the low-resource setting is a still crucial issue. In this paper, we propose a novel three-stage... | Shilei Liu, Xiaofeng Zhao, Bochao Li, Feiliang Ren, Longhui Zhang, Shujuan Yin |  |
| 643 |  |  [Intention Reasoning Network for Multi-Domain End-to-end Task-Oriented Dialogue](https://doi.org/10.18653/v1/2021.emnlp-main.174) |  | 0 | Recent years has witnessed the remarkable success in end-to-end task-oriented dialog system, especially when incorporating external knowledge information. However, the quality of most existing models’ generated response is still limited, mainly due to their lack of fine-grained reasoning on deterministic knowledge (w.r.t. conceptual tokens), which makes them difficult to capture the concept shifts and identify user’s real intention in cross-task scenarios. To address these issues, we propose a... | Zhiyuan Ma, Jianjun Li, Zezheng Zhang, Guohui Li, Yongjing Cheng |  |
| 644 |  |  [More is Better: Enhancing Open-Domain Dialogue Generation via Multi-Source Heterogeneous Knowledge](https://doi.org/10.18653/v1/2021.emnlp-main.175) |  | 0 | Despite achieving remarkable performance, previous knowledge-enhanced works usually only use a single-source homogeneous knowledge base of limited knowledge coverage. Thus, they often degenerate into traditional methods because not all dialogues can be linked with knowledge entries. This paper proposes a novel dialogue generation model, MSKE-Dialog, to solve this issue with three unique advantages: (1) Rather than only one, MSKE-Dialog can simultaneously leverage multiple heterogeneous... | Sixing Wu, Ying Li, Minghui Wang, Dawei Zhang, Yang Zhou, Zhonghai Wu |  |
| 645 |  |  [Domain-Lifelong Learning for Dialogue State Tracking via Knowledge Preservation Networks](https://doi.org/10.18653/v1/2021.emnlp-main.176) |  | 0 | Dialogue state tracking (DST), which estimates user goals given a dialogue context, is an essential component of task-oriented dialogue systems. Conventional DST models are usually trained offline, which requires a fixed dataset prepared in advance. This paradigm is often impractical in real-world applications since online dialogue systems usually involve continually emerging new data and domains. Therefore, this paper explores Domain-Lifelong Learning for Dialogue State Tracking (DLL-DST),... | Qingbin Liu, Pengfei Cao, Cao Liu, Jiansong Chen, Xunliang Cai, Fan Yang, Shizhu He, Kang Liu, Jun Zhao |  |
| 646 |  |  [CSAGN: Conversational Structure Aware Graph Network for Conversational Semantic Role Labeling](https://doi.org/10.18653/v1/2021.emnlp-main.177) |  | 0 | Conversational semantic role labeling (CSRL) is believed to be a crucial step towards dialogue understanding. However, it remains a major challenge for existing CSRL parser to handle conversational structural information. In this paper, we present a simple and effective architecture for CSRL which aims to address this problem. Our model is based on a conversational structure aware graph network which explicitly encodes the speaker dependent information. We also propose a multi-task learning... | Han Wu, Kun Xu, Linqi Song |  |
| 647 |  |  [Different Strokes for Different Folks: Investigating Appropriate Further Pre-training Approaches for Diverse Dialogue Tasks](https://doi.org/10.18653/v1/2021.emnlp-main.178) |  | 0 | Loading models pre-trained on the large-scale corpus in the general domain and fine-tuning them on specific downstream tasks is gradually becoming a paradigm in Natural Language Processing. Previous investigations prove that introducing a further pre-training phase between pre-training and fine-tuning phases to adapt the model on the domain-specific unlabeled data can bring positive effects. However, most of these further pre-training works just keep running the conventional pre-training task,... | Yao Qiu, Jinchao Zhang, Jie Zhou |  |
| 648 |  |  [Knowledge Enhanced Fine-Tuning for Better Handling Unseen Entities in Dialogue Generation](https://doi.org/10.18653/v1/2021.emnlp-main.179) |  | 0 | Although pre-training models have achieved great success in dialogue generation, their performance drops dramatically when the input contains an entity that does not appear in pre-training and fine-tuning datasets (unseen entity). To address this issue, existing methods leverage an external knowledge base to generate appropriate responses. In real-world practical, the entity may not be included by the knowledge base or suffer from the precision of knowledge retrieval. To deal with this problem,... | Leyang Cui, Yu Wu, Shujie Liu, Yue Zhang |  |
| 649 |  |  [An Evaluation Dataset and Strategy for Building Robust Multi-turn Response Selection Model](https://doi.org/10.18653/v1/2021.emnlp-main.180) |  | 0 | Multi-turn response selection models have recently shown comparable performance to humans in several benchmark datasets. However, in the real environment, these models often have weaknesses, such as making incorrect predictions based heavily on superficial patterns without a comprehensive understanding of the context. For example, these models often give a high score to the wrong response candidate containing several keywords related to the context but using the inconsistent tense. In this... | Kijong Han, Seojin Lee, Donghun Lee |  |
| 650 |  |  [Unsupervised Conversation Disentanglement through Co-Training](https://doi.org/10.18653/v1/2021.emnlp-main.181) |  | 0 | Conversation disentanglement aims to separate intermingled messages into detached sessions, which is a fundamental task in understanding multi-party conversations. Existing work on conversation disentanglement relies heavily upon human-annotated datasets, which is expensive to obtain in practice. In this work, we explore training a conversation disentanglement model without referencing any human annotations. Our method is built upon the deep co-training algorithm, which consists of two neural... | Hui Liu, Zhan Shi, Xiaodan Zhu |  |
| 651 |  |  [Don't be Contradicted with Anything! CI-ToD: Towards Benchmarking Consistency for Task-oriented Dialogue System](https://doi.org/10.18653/v1/2021.emnlp-main.182) |  | 0 | Consistency Identification has obtained remarkable success on open-domain dialogue, which can be used for preventing inconsistent response generation. However, in contrast to the rapid development in open-domain dialogue, few efforts have been made to the task-oriented dialogue direction. In this paper, we argue that consistency problem is more urgent in task-oriented domain. To facilitate the research, we introduce CI-ToD, a novel dataset for Consistency Identification in Task-oriented Dialog... | Libo Qin, Tianbao Xie, Shijue Huang, Qiguang Chen, Xiao Xu, Wanxiang Che |  |
| 652 |  |  [Transferable Persona-Grounded Dialogues via Grounded Minimal Edits](https://doi.org/10.18653/v1/2021.emnlp-main.183) |  | 0 | Grounded dialogue models generate responses that are grounded on certain concepts. Limited by the distribution of grounded dialogue data, models trained on such data face the transferability challenges in terms of the data distribution and the type of grounded concepts. To address the challenges, we propose the grounded minimal editing framework, which minimally edits existing responses to be grounded on the given concept. Focusing on personas, we propose Grounded Minimal Editor (GME), which... | Chen Henry Wu, Yinhe Zheng, Xiaoxi Mao, Minlie Huang |  |
| 653 |  |  [EARL: Informative Knowledge-Grounded Conversation Generation with Entity-Agnostic Representation Learning](https://doi.org/10.18653/v1/2021.emnlp-main.184) |  | 0 | Generating informative and appropriate responses is challenging but important for building human-like dialogue systems. Although various knowledge-grounded conversation models have been proposed, these models have limitations in utilizing knowledge that infrequently occurs in the training data, not to mention integrating unseen knowledge into conversation generation. In this paper, we propose an Entity-Agnostic Representation Learning (EARL) method to introduce knowledge graphs to informative... | Hao Zhou, Minlie Huang, Yong Liu, Wei Chen, Xiaoyan Zhu |  |
| 654 |  |  [DialogueCSE: Dialogue-based Contrastive Learning of Sentence Embeddings](https://doi.org/10.18653/v1/2021.emnlp-main.185) |  | 0 | Learning sentence embeddings from dialogues has drawn increasing attention due to its low annotation cost and high domain adaptability. Conventional approaches employ the siamese-network for this task, which obtains the sentence embeddings through modeling the context-response semantic relevance by applying a feed-forward network on top of the sentence encoders. However, as the semantic textual similarity is commonly measured through the element-wise distance metrics (e.g. cosine and L2... | Che Liu, Rui Wang, Jinghua Liu, Jian Sun, Fei Huang, Luo Si |  |
| 655 |  |  [Improving Graph-based Sentence Ordering with Iteratively Predicted Pairwise Orderings](https://doi.org/10.18653/v1/2021.emnlp-main.186) |  | 0 | Dominant sentence ordering models can be classified into pairwise ordering models and set-to-sequence models. However, there is little attempt to combine these two types of models, which inituitively possess complementary advantages. In this paper, we propose a novel sentence ordering framework which introduces two classifiers to make better use of pairwise orderings for graph-based sentence ordering (Yin et al. 2019, 2021). Specially, given an initial sentence-entity graph, we first introduce... | Shaopeng Lai, Ante Wang, Fandong Meng, Jie Zhou, Yubin Ge, Jiali Zeng, Junfeng Yao, Degen Huang, Jinsong Su |  |
| 656 |  |  [Not Just Classification: Recognizing Implicit Discourse Relation on Joint Modeling of Classification and Generation](https://doi.org/10.18653/v1/2021.emnlp-main.187) |  | 0 | Implicit discourse relation recognition (IDRR) is a critical task in discourse analysis. Previous studies only regard it as a classification task and lack an in-depth understanding of the semantics of different relations. Therefore, we first view IDRR as a generation task and further propose a method joint modeling of the classification and generation. Specifically, we propose a joint model, CG-T5, to recognize the relation label and generate the target sentence containing the meaning of... | Feng Jiang, Yaxin Fan, Xiaomin Chu, Peifeng Li, Qiaoming Zhu |  |
| 657 |  |  [A Language Model-based Generative Classifier for Sentence-level Discourse Parsing](https://doi.org/10.18653/v1/2021.emnlp-main.188) |  | 0 | Discourse segmentation and sentence-level discourse parsing play important roles for various NLP tasks to consider textual coherence. Despite recent achievements in both tasks, there is still room for improvement due to the scarcity of labeled data. To solve the problem, we propose a language model-based generative classifier (LMGC) for using more information from labels by treating the labels as an input while enhancing label representations by embedding descriptions for each label. Moreover,... | Ying Zhang, Hidetaka Kamigaito, Manabu Okumura |  |
| 658 |  |  [Multimodal Phased Transformer for Sentiment Analysis](https://doi.org/10.18653/v1/2021.emnlp-main.189) |  | 0 | Multimodal Transformers achieve superior performance in multimodal learning tasks. However, the quadratic complexity of the self-attention mechanism in Transformers limits their deployment in low-resource devices and makes their inference and training computationally expensive. We propose multimodal Sparse Phased Transformer (SPT) to alleviate the problem of self-attention complexity and memory footprint. SPT uses a sampling function to generate a sparse attention matrix and compress a long... | Junyan Cheng, Iordanis Fostiropoulos, Barry W. Boehm, Mohammad Soleymani |  |
| 659 |  |  [Hierarchical Multi-label Text Classification with Horizontal and Vertical Category Correlations](https://doi.org/10.18653/v1/2021.emnlp-main.190) |  | 0 | Hierarchical multi-label text classification (HMTC) deals with the challenging task where an instance can be assigned to multiple hierarchically structured categories at the same time. The majority of prior studies either focus on reducing the HMTC task into a flat multi-label problem ignoring the vertical category correlations or exploiting the dependencies across different hierarchical levels without considering the horizontal correlations among categories at the same level, which inevitably... | Linli Xu, Sijie Teng, Ruoyu Zhao, Junliang Guo, Chi Xiao, Deqiang Jiang, Bo Ren |  |
| 660 |  |  [RankNAS: Efficient Neural Architecture Search by Pairwise Ranking](https://doi.org/10.18653/v1/2021.emnlp-main.191) |  | 0 | This paper addresses the efficiency challenge of Neural Architecture Search (NAS) by formulating the task as a ranking problem. Previous methods require numerous training examples to estimate the accurate performance of architectures, although the actual goal is to find the distinction between “good” and “bad” candidates. Here we do not resort to performance predictors. Instead, we propose a performance ranking method (RankNAS) via pairwise ranking. It enables efficient architecture search... | Chi Hu, Chenglong Wang, Xiangnan Ma, Xia Meng, Yinqiao Li, Tong Xiao, Jingbo Zhu, Changliang Li |  |
| 661 |  |  [FLiText: A Faster and Lighter Semi-Supervised Text Classification with Convolution Networks](https://doi.org/10.18653/v1/2021.emnlp-main.192) |  | 0 | In natural language processing (NLP), state-of-the-art (SOTA) semi-supervised learning (SSL) frameworks have shown great performance on deep pre-trained language models such as BERT, and are expected to significantly reduce the demand for manual labeling. However, our empirical studies indicate that these frameworks are not suitable for lightweight models such as TextCNN, LSTM and etc. In this work, we develop a new SSL framework called FLiText, which stands for Faster and Lighter... | Chen Liu, Mengchao Zhang, Zhibing Fu, Panpan Hou, Yu Li |  |
| 662 |  |  [Evaluating Debiasing Techniques for Intersectional Biases](https://doi.org/10.18653/v1/2021.emnlp-main.193) |  | 0 | Bias is pervasive for NLP models, motivating the development of automatic debiasing techniques. Evaluation of NLP debiasing methods has largely been limited to binary attributes in isolation, e.g., debiasing with respect to binary gender or race, however many corpora involve multiple such attributes, possibly with higher cardinality. In this paper we argue that a truly fair model must consider ‘gerrymandering’ groups which comprise not only single attributes, but also intersectional groups. We... | Shivashankar Subramanian, Xudong Han, Timothy Baldwin, Trevor Cohn, Lea Frermann |  |
| 663 |  |  [Definition Modelling for Appropriate Specificity](https://doi.org/10.18653/v1/2021.emnlp-main.194) |  | 0 | Definition generation techniques aim to generate a definition of a target word or phrase given a context. In previous studies, researchers have faced various issues such as the out-of-vocabulary problem and over/under-specificity problems. Over-specific definitions present narrow word meanings, whereas under-specific definitions present general and context-insensitive meanings. Herein, we propose a method for definition generation with appropriate specificity. The proposed method addresses the... | Han Huang, Tomoyuki Kajiwara, Yuki Arase |  |
| 664 |  |  [Transductive Learning for Unsupervised Text Style Transfer](https://doi.org/10.18653/v1/2021.emnlp-main.195) |  | 0 | Unsupervised style transfer models are mainly based on an inductive learning approach, which represents the style as embeddings, decoder parameters, or discriminator parameters and directly applies these general rules to the test cases. However, the lacking of parallel corpus hinders the ability of these inductive learning methods on this task. As a result, it is likely to cause severe inconsistent style expressions, like ‘the salad is rude’. To tackle this problem, we propose a novel... | Fei Xiao, Liang Pang, Yanyan Lan, Yan Wang, Huawei Shen, Xueqi Cheng |  |
| 665 |  |  [Integrating Semantic Scenario and Word Relations for Abstractive Sentence Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.196) |  | 0 | Recently graph-based methods have been adopted for Abstractive Text Summarization. However, existing graph-based methods only consider either word relations or structure information, which neglect the correlation between them. To simultaneously capture the word relations and structure information from sentences, we propose a novel Dual Graph network for Abstractive Sentence Summarization. Specifically, we first construct semantic scenario graph and semantic word relation graph based on... | Yong Guan, Shaoru Guo, Ru Li, Xiaoli Li, Hu Zhang |  |
| 666 |  |  [Coupling Context Modeling with Zero Pronoun Recovering for Document-Level Natural Language Generation](https://doi.org/10.18653/v1/2021.emnlp-main.197) |  | 0 | Natural language generation (NLG) tasks on pro-drop languages are known to suffer from zero pronoun (ZP) problems, and the problems remain challenging due to the scarcity of ZP-annotated NLG corpora. In this case, we propose a highly adaptive two-stage approach to couple context modeling with ZP recovering to mitigate the ZP problem in NLG tasks. Notably, we frame the recovery process in a task-supervised fashion where the ZP representation recovering capability is learned during the NLG task... | Xin Tan, Longyin Zhang, Guodong Zhou |  |
| 667 |  |  [Adaptive Bridge between Training and Inference for Dialogue Generation](https://doi.org/10.18653/v1/2021.emnlp-main.198) |  | 0 | Although exposure bias has been widely studied in some NLP tasks, it faces its unique challenges in dialogue response generation, the representative one-to-various generation scenario. In real human dialogue, there are many appropriate responses for the same context, not only with different expressions, but also with different topics. Therefore, due to the much bigger gap between various ground-truth responses and the generated synthetic response, exposure bias is more challenging in dialogue... | Haoran Xu, Hainan Zhang, Yanyan Zou, Hongshen Chen, Zhuoye Ding, Yanyan Lan |  |
| 668 |  |  [ConRPG: Paraphrase Generation using Contexts as Regularizer](https://doi.org/10.18653/v1/2021.emnlp-main.199) |  | 0 | A long-standing issue with paraphrase generation is the lack of reliable supervision signals. In this paper, we propose a new unsupervised paradigm for paraphrase generation based on the assumption that the probabilities of generating two sentences with the same meaning given the same context should be the same. Inspired by this fundamental idea, we propose a pipelined system which consists of paraphrase candidate generation based on contextual language models, candidate filtering using scoring... | Yuxian Meng, Xiang Ao, Qing He, Xiaofei Sun, Qinghong Han, Fei Wu, Chun Fan, Jiwei Li |  |
| 669 |  |  [Building the Directed Semantic Graph for Coherent Long Text Generation](https://doi.org/10.18653/v1/2021.emnlp-main.200) |  | 0 | Generating long text conditionally depending on the short input text has recently attracted more and more research efforts. Most existing approaches focus more on introducing extra knowledge to supplement the short input text, but ignore the coherence issue of the generated texts. To address aforementioned research issue, this paper proposes a novel two-stage approach to generate coherent long text. Particularly, we first build a document-level path for each output text with each sentence... | Ziao Wang, Xiaofeng Zhang, Hongwei Du |  |
| 670 |  |  [Iterative GNN-based Decoder for Question Generation](https://doi.org/10.18653/v1/2021.emnlp-main.201) |  | 0 | Natural question generation (QG) aims to generate questions from a passage, and generated questions are answered from the passage. Most models with state-of-the-art performance model the previously generated text at each decoding step. However, (1) they ignore the rich structure information that is hidden in the previously generated text. (2) they ignore the impact of copied words on the passage. We perceive that information in previously generated words serves as auxiliary information in... | Zichu Fei, Qi Zhang, Yaqian Zhou |  |
| 671 |  |  [Asking Questions Like Educational Experts: Automatically Generating Question-Answer Pairs on Real-World Examination Data](https://doi.org/10.18653/v1/2021.emnlp-main.202) |  | 0 | Generating high quality question-answer pairs is a hard but meaningful task. Although previous works have achieved great results on answer-aware question generation, it is difficult to apply them into practical application in the education field. This paper for the first time addresses the question-answer pair generation task on the real-world examination data, and proposes a new unified framework on RACE. To capture the important information of the input passage we first automatically generate... | Fanyi Qu, Xin Jia, Yunfang Wu |  |
| 672 |  |  [Syntactically-Informed Unsupervised Paraphrasing with Non-Parallel Data](https://doi.org/10.18653/v1/2021.emnlp-main.203) |  | 0 | Previous works on syntactically controlled paraphrase generation heavily rely on large-scale parallel paraphrase data that is not easily available for many languages and domains. In this paper, we take this research direction to the extreme and investigate whether it is possible to learn syntactically controlled paraphrase generation with nonparallel data. We propose a syntactically-informed unsupervised paraphrasing model based on conditional variational auto-encoder (VAE) which can generate... | Erguang Yang, Mingtong Liu, Deyi Xiong, Yujie Zhang, Yao Meng, Changjian Hu, Jinan Xu, Yufeng Chen |  |
| 673 |  |  [Exploring Task Difficulty for Few-Shot Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.204) |  | 0 | Few-shot relation extraction (FSRE) focuses on recognizing novel relations by learning with merely a handful of annotated instances. Meta-learning has been widely adopted for such a task, which trains on randomly generated few-shot tasks to learn generic data representations. Despite impressive results achieved, existing models still perform suboptimally when handling hard FSRE tasks, where the relations are fine-grained and similar to each other. We argue this is largely because existing... | Jiale Han, Bo Cheng, Wei Lu |  |
| 674 |  |  [MuVER: Improving First-Stage Entity Retrieval with Multi-View Entity Representations](https://doi.org/10.18653/v1/2021.emnlp-main.205) |  | 0 | Entity retrieval, which aims at disambiguating mentions to canonical entities from massive KBs, is essential for many tasks in natural language processing. Recent progress in entity retrieval shows that the dual-encoder structure is a powerful and efficient framework to nominate candidates if entities are only identified by descriptions. However, they ignore the property that meanings of entity mentions diverge in different contexts and are related to various portions of descriptions, which are... | Xinyin Ma, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, Weiming Lu |  |
| 675 |  |  [Treasures Outside Contexts: Improving Event Detection via Global Statistics](https://doi.org/10.18653/v1/2021.emnlp-main.206) |  | 0 | Event detection (ED) aims at identifying event instances of specified types in given texts, which has been formalized as a sequence labeling task. As far as we know, existing neural-based ED models make decisions relying entirely on the contextual semantic features of each word in the inputted text, which we find is easy to be confused by the varied contexts in the test stage. To this end, we come up with the idea of introducing a set of statistical features from word-event co-occurrence... | Rui Li, Wenlin Zhao, Cheng Yang, Sen Su |  |
| 676 |  |  [Uncertain Local-to-Global Networks for Document-Level Event Factuality Identification](https://doi.org/10.18653/v1/2021.emnlp-main.207) |  | 0 | Event factuality indicates the degree of certainty about whether an event occurs in the real world. Existing studies mainly focus on identifying event factuality at sentence level, which easily leads to conflicts between different mentions of the same event. To this end, we study the problem of document-level event factuality identification, which determines the event factuality from the view of a document. For this task, we need to consider two important characteristics: Local Uncertainty and... | Pengfei Cao, Yubo Chen, Yuqing Yang, Kang Liu, Jun Zhao |  |
| 677 |  |  [A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling](https://doi.org/10.18653/v1/2021.emnlp-main.208) |  | 0 | Table filling based relational triple extraction methods are attracting growing research interests due to their promising performance and their abilities on extracting triples from complex sentences. However, this kind of methods are far from their full potential because most of them only focus on using local features but ignore the global associations of relations and of token pairs, which increases the possibility of overlooking some important information during triple extraction. To overcome... | Feiliang Ren, Longhui Zhang, Shujuan Yin, Xiaofeng Zhao, Shilei Liu, Bochao Li, Yaduo Liu |  |
| 678 |  |  [Structure-Augmented Keyphrase Generation](https://doi.org/10.18653/v1/2021.emnlp-main.209) |  | 0 | This paper studies the keyphrase generation (KG) task for scenarios where structure plays an important role. For example, a scientific publication consists of a short title and a long body, where the title can be used for de-emphasizing unimportant details in the body. Similarly, for short social media posts (, tweets), scarce context can be augmented from titles, though often missing. Our contribution is generating/augmenting structure then injecting these information in the encoding, using... | Jihyuk Kim, Myeongho Jeong, Seungtaek Choi, Seungwon Hwang |  |
| 679 |  |  [An Empirical Study on Multiple Information Sources for Zero-Shot Fine-Grained Entity Typing](https://doi.org/10.18653/v1/2021.emnlp-main.210) |  | 0 | Auxiliary information from multiple sources has been demonstrated to be effective in zero-shot fine-grained entity typing (ZFET). However, there lacks a comprehensive understanding about how to make better use of the existing information sources and how they affect the performance of ZFET. In this paper, we empirically study three kinds of auxiliary information: context consistency, type hierarchy and background knowledge (e.g., prototypes and descriptions) of types, and propose a multi-source... | Yi Chen, Haiyun Jiang, Lemao Liu, Shuming Shi, Chuang Fan, Min Yang, Ruifeng Xu |  |
| 680 |  |  [DyLex: Incorporating Dynamic Lexicons into BERT for Sequence Labeling](https://doi.org/10.18653/v1/2021.emnlp-main.211) |  | 0 | Incorporating lexical knowledge into deep learning models has been proved to be very effective for sequence labeling tasks. However, previous works commonly have difficulty dealing with large-scale dynamic lexicons which often cause excessive matching noise and problems of frequent updates. In this paper, we propose DyLex, a plug-in lexicon incorporation approach for BERT based sequence labeling tasks. Instead of leveraging embeddings of words in the lexicon as in conventional methods, we adopt... | Baojun Wang, Zhao Zhang, Kun Xu, GuangYuan Hao, Yuyang Zhang, Lifeng Shang, Linlin Li, Xiao Chen, Xin Jiang, Qun Liu |  |
| 681 |  |  [MapRE: An Effective Semantic Mapping Approach for Low-resource Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.212) |  | 0 | Neural relation extraction models have shown promising results in recent years; however, the model performance drops dramatically given only a few training samples. Recent works try leveraging the advance in few-shot learning to solve the low resource problem, where they train label-agnostic models to directly compare the semantic similarities among context sentences in the embedding space. However, the label-aware information, i.e., the relation label that contains the semantic knowledge of... | Manqing Dong, Chunguang Pan, Zhipeng Luo |  |
| 682 |  |  [Heterogeneous Graph Neural Networks for Keyphrase Generation](https://doi.org/10.18653/v1/2021.emnlp-main.213) |  | 0 | The encoder–decoder framework achieves state-of-the-art results in keyphrase generation (KG) tasks by predicting both present keyphrases that appear in the source document and absent keyphrases that do not. However, relying solely on the source document can result in generating uncontrollable and inaccurate absent keyphrases. To address these problems, we propose a novel graph-based method that can capture explicit knowledge from related references. Our model first retrieves some... | Jiacheng Ye, Ruijian Cai, Tao Gui, Qi Zhang |  |
| 683 |  |  [Machine Reading Comprehension as Data Augmentation: A Case Study on Implicit Event Argument Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.214) |  | 0 | Implicit event argument extraction (EAE) is a crucial document-level information extraction task that aims to identify event arguments beyond the sentence level. Despite many efforts for this task, the lack of enough training data has long impeded the study. In this paper, we take a new perspective to address the data sparsity issue faced by implicit EAE, by bridging the task with machine reading comprehension (MRC). Particularly, we devise two data augmentation regimes via MRC, including: 1)... | Jian Liu, Yufeng Chen, Jinan Xu |  |
| 684 |  |  [Importance Estimation from Multiple Perspectives for Keyphrase Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.215) |  | 0 | Keyphrase extraction is a fundamental task in Natural Language Processing, which usually contains two main parts: candidate keyphrase extraction and keyphrase importance estimation. From the view of human understanding documents, we typically measure the importance of phrase according to its syntactic accuracy, information saliency, and concept consistency simultaneously. However, most existing keyphrase extraction approaches only focus on the part of them, which leads to biased results. In... | Mingyang Song, Liping Jing, Lin Xiao |  |
| 685 |  |  [Gradient Imitation Reinforcement Learning for Low Resource Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.216) |  | 0 | Low-resource Relation Extraction (LRE) aims to extract relation facts from limited labeled corpora when human annotation is scarce. Existing works either utilize self-training scheme to generate pseudo labels that will cause the gradual drift problem, or leverage meta-learning scheme which does not solicit feedback explicitly. To alleviate selection bias due to the lack of feedback loops in existing LRE learning paradigms, we developed a Gradient Imitation Reinforcement Learning method to... | Xuming Hu, Chenwei Zhang, Yawen Yang, Xiaohe Li, Li Lin, Lijie Wen, Philip S. Yu |  |
| 686 |  |  [Low-resource Taxonomy Enrichment with Pretrained Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.217) |  | 0 | Taxonomies are symbolic representations of hierarchical relationships between terms or entities. While taxonomies are useful in broad applications, manually updating or maintaining them is labor-intensive and difficult to scale in practice. Conventional supervised methods for this enrichment task fail to find optimal parents of new terms in low-resource settings where only small taxonomies are available because of overfitting to hierarchical relationships in the taxonomies. To tackle the... | Kunihiro Takeoka, Kosuke Akimoto, Masafumi Oyamada |  |
| 687 |  |  [Entity Relation Extraction as Dependency Parsing in Visually Rich Documents](https://doi.org/10.18653/v1/2021.emnlp-main.218) |  | 0 | Previous works on key information extraction from visually rich documents (VRDs) mainly focus on labeling the text within each bounding box (i.e.,semantic entity), while the relations in-between are largely unexplored. In this paper, we adapt the popular dependency parsing model, the biaffine parser, to this entity relation extraction task. Being different from the original dependency parsing model which recognizes dependency relations between words, we identify relations between groups of... | Yue Zhang, Bo Zhang, Rui Wang, Junjie Cao, Chen Li, Zuyi Bao |  |
| 688 |  |  [Synchronous Dual Network with Cross-Type Attention for Joint Entity and Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.219) |  | 0 | Joint entity and relation extraction is challenging due to the complex interaction of interaction between named entity recognition and relation extraction. Although most existing works tend to jointly train these two tasks through a shared network, they fail to fully utilize the interdependence between entity types and relation types. In this paper, we design a novel synchronous dual network (SDN) with cross-type attention via separately and interactively considering the entity types and... | Hui Wu, Xiaodong Shi |  |
| 689 |  |  [Less is More: Pretrain a Strong Siamese Encoder for Dense Text Retrieval Using a Weak Decoder](https://doi.org/10.18653/v1/2021.emnlp-main.220) |  | 0 | Dense retrieval requires high-quality text sequence embeddings to support effective search in the representation space. Autoencoder-based language models are appealing in dense retrieval as they train the encoder to output high-quality embedding that can reconstruct the input texts. However, in this paper, we provide theoretical analyses and show empirically that an autoencoder language model with a low reconstruction loss may not provide good sequence representations because the decoder may... | Shuqi Lu, Di He, Chenyan Xiong, Guolin Ke, Waleed Malik, Zhicheng Dou, Paul Bennett, TieYan Liu, Arnold Overwijk |  |
| 690 |  |  [TransPrompt: Towards an Automatic Transferable Prompting Framework for Few-shot Text Classification](https://doi.org/10.18653/v1/2021.emnlp-main.221) |  | 0 | Recent studies have shown that prompts improve the performance of large pre-trained language models for few-shot text classification. Yet, it is unclear how the prompting knowledge can be transferred across similar NLP tasks for the purpose of mutual reinforcement. Based on continuous prompt embeddings, we propose TransPrompt, a transferable prompting framework for few-shot learning across similar tasks. In TransPrompt, we employ a multi-task meta-knowledge acquisition procedure to train a... | Chengyu Wang, Jianing Wang, Minghui Qiu, Jun Huang, Ming Gao |  |
| 691 |  |  [Weakly-supervised Text Classification Based on Keyword Graph](https://doi.org/10.18653/v1/2021.emnlp-main.222) |  | 0 | Weakly-supervised text classification has received much attention in recent years for it can alleviate the heavy burden of annotating massive data. Among them, keyword-driven methods are the mainstream where user-provided keywords are exploited to generate pseudo-labels for unlabeled texts. However, existing methods treat keywords independently, thus ignore the correlation among them, which should be useful if properly exploited. In this paper, we propose a novel framework called ClassKG to... | Lu Zhang, Jiandong Ding, Yi Xu, Yingyao Liu, Shuigeng Zhou |  |
| 692 |  |  [Efficient-FedRec: Efficient Federated Learning Framework for Privacy-Preserving News Recommendation](https://doi.org/10.18653/v1/2021.emnlp-main.223) |  | 0 | News recommendation is critical for personalized news access. Most existing news recommendation methods rely on centralized storage of users’ historical news click behavior data, which may lead to privacy concerns and hazards. Federated Learning is a privacy-preserving framework for multiple clients to collaboratively train models without sharing their private data. However, the computation and communication cost of directly learning many existing news recommendation models in a federated way... | Jingwei Yi, Fangzhao Wu, Chuhan Wu, Ruixuan Liu, Guangzhong Sun, Xing Xie |  |
| 693 |  |  [RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking](https://doi.org/10.18653/v1/2021.emnlp-main.224) |  | 0 | In various natural language processing tasks, passage retrieval and passage re-ranking are two key procedures in finding and ranking relevant information. Since both the two procedures contribute to the final performance, it is important to jointly optimize them in order to achieve mutual improvement. In this paper, we propose a novel joint training approach for dense passage retrieval and passage reranking. A major contribution is that we introduce the dynamic listwise distillation, where we... | Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Qiaoqiao She, Hua Wu, Haifeng Wang, JiRong Wen |  |
| 694 |  |  [Dealing with Typos for BERT-based Passage Retrieval and Ranking](https://doi.org/10.18653/v1/2021.emnlp-main.225) |  | 0 | Passage retrieval and ranking is a key task in open-domain question answering and information retrieval. Current effective approaches mostly rely on pre-trained deep language model-based retrievers and rankers. These methods have been shown to effectively model the semantic matching between queries and passages, also in presence of keyword mismatch, i.e. passages that are relevant to a query but do not contain important query keywords. In this paper we consider the Dense Retriever (DR), a... | Shengyao Zhuang, Guido Zuccon |  |
| 695 |  |  [From Alignment to Assignment: Frustratingly Simple Unsupervised Entity Alignment](https://doi.org/10.18653/v1/2021.emnlp-main.226) |  | 0 | Cross-lingual entity alignment (EA) aims to find the equivalent entities between crosslingual KGs (Knowledge Graphs), which is a crucial step for integrating KGs. Recently, many GNN-based EA methods are proposed and show decent performance improvements on several public datasets. However, existing GNN-based EA methods inevitably inherit poor interpretability and low efficiency from neural networks. Motivated by the isomorphic assumption of GNN-based methods, we successfully transform the... | Xin Mao, Wenting Wang, Yuanbin Wu, Man Lan |  |
| 696 |  |  [Simple and Effective Unsupervised Redundancy Elimination to Compress Dense Vectors for Passage Retrieval](https://doi.org/10.18653/v1/2021.emnlp-main.227) |  | 0 | Recent work has shown that dense passage retrieval techniques achieve better ranking accuracy in open-domain question answering compared to sparse retrieval techniques such as BM25, but at the cost of large space and memory requirements. In this paper, we analyze the redundancy present in encoded dense vectors and show that the default dimension of 768 is unnecessarily large. To improve space efficiency, we propose a simple unsupervised compression pipeline that consists of principal component... | Xueguang Ma, Minghan Li, Kai Sun, Ji Xin, Jimmy Lin |  |
| 697 |  |  [Relation Extraction with Word Graphs from N-grams](https://doi.org/10.18653/v1/2021.emnlp-main.228) |  | 0 | Most recent studies for relation extraction (RE) leverage the dependency tree of the input sentence to incorporate syntax-driven contextual information to improve model performance, with little attention paid to the limitation where high-quality dependency parsers in most cases unavailable, especially for in-domain scenarios. To address this limitation, in this paper, we propose attentive graph convolutional networks (A-GCN) to improve neural RE methods with an unsupervised manner to build the... | Han Qin, Yuanhe Tian, Yan Song |  |
| 698 |  |  [A Bayesian Framework for Information-Theoretic Probing](https://doi.org/10.18653/v1/2021.emnlp-main.229) |  | 0 | Pimentel et al. (2020) recently analysed probing from an information-theoretic perspective. They argue that probing should be seen as approximating a mutual information. This led to the rather unintuitive conclusion that representations encode exactly the same information about a target task as the original sentences. The mutual information, however, assumes the true probability distribution of a pair of random variables is known, leading to unintuitive results in settings where it is not. This... | Tiago Pimentel, Ryan Cotterell |  |
| 699 |  |  [Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little](https://doi.org/10.18653/v1/2021.emnlp-main.230) |  | 0 | A possible explanation for the impressive performance of masked language model (MLM) pre-training is that such models have learned to represent the syntactic structures prevalent in classical NLP pipelines. In this paper, we propose a different explanation: MLMs succeed on downstream tasks almost entirely due to their ability to model higher-order word co-occurrence statistics. To demonstrate this, we pre-train MLMs on sentences with randomly shuffled word order, and show that these models... | Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, Douwe Kiela |  |
| 700 |  |  [What's Hidden in a One-layer Randomly Weighted Transformer?](https://doi.org/10.18653/v1/2021.emnlp-main.231) |  | 0 | We demonstrate that, hidden within one-layer randomly weighted neural networks, there exist subnetworks that can achieve impressive performance, without ever modifying the weight initializations, on machine translation tasks. To find subnetworks for one-layer randomly weighted neural networks, we apply different binary masks to the same weight matrix to generate different layers. Hidden within a one-layer randomly weighted Transformer, we find that subnetworks that can achieve 29.45/17.29 BLEU... | Sheng Shen, Zhewei Yao, Douwe Kiela, Kurt Keutzer, Michael W. Mahoney |  |
| 701 |  |  [Rethinking Denoised Auto-Encoding in Language Pre-Training](https://doi.org/10.18653/v1/2021.emnlp-main.232) |  | 0 | Pre-trained self-supervised models such as BERT have achieved striking success in learning sequence representations, especially for natural language processing. These models typically corrupt the given sequences with certain types of noise, such as masking, shuffling, or substitution, and then try to recover the original input. However, such pre-training approaches are prone to learning representations that are covariant with the noise, leading to the discrepancy between the pre-training and... | Fuli Luo, Pengcheng Yang, Shicheng Li, Xuancheng Ren, Xu Sun, Songfang Huang, Fei Huang |  |
| 702 |  |  [Lifelong Explainer for Lifelong Learners](https://doi.org/10.18653/v1/2021.emnlp-main.233) |  | 0 | Lifelong Learning (LL) black-box models are dynamic in that they keep learning from new tasks and constantly update their parameters. Owing to the need to utilize information from previously seen tasks, and capture commonalities in potentially diverse data, it is hard for automatic explanation methods to explain the outcomes of these models. In addition, existing explanation methods, e.g., LIME, which are computationally expensive when explaining a static black-box model, are even more... | Xuelin Situ, Sameen Maruf, Ingrid Zukerman, Cécile Paris, Gholamreza Haffari |  |
| 703 |  |  [Linguistic Dependencies and Statistical Dependence](https://doi.org/10.18653/v1/2021.emnlp-main.234) |  | 0 | Are pairs of words that tend to occur together also likely to stand in a linguistic dependency? This empirical question is motivated by a long history of literature in cognitive science, psycholinguistics, and NLP. In this work we contribute an extensive analysis of the relationship between linguistic dependencies and statistical dependence between words. Improving on previous work, we introduce the use of large pretrained language models to compute contextualized estimates of the pointwise... | Jacob Louis Hoover, Wenyu Du, Alessandro Sordoni, Timothy J. O'Donnell |  |
| 704 |  |  [Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars](https://doi.org/10.18653/v1/2021.emnlp-main.235) |  | 0 | In computational linguistics, it has been shown that hierarchical structures make language models (LMs) more human-like. However, the previous literature has been agnostic about a parsing strategy of the hierarchical models. In this paper, we investigated whether hierarchical structures make LMs more human-like, and if so, which parsing strategy is most cognitively plausible. In order to address this question, we evaluated three LMs against human reading times in Japanese with head-final... | Ryo Yoshida, Hiroshi Noji, Yohei Oseki |  |
| 705 |  |  [A Simple and Effective Positional Encoding for Transformers](https://doi.org/10.18653/v1/2021.emnlp-main.236) |  | 0 | Transformer models are permutation equivariant. To supply the order and type information of the input tokens, position and segment embeddings are usually added to the input. Recent works proposed variations of positional encodings with relative position encodings achieving better performance. Our analysis shows that the gain actually comes from moving positional information to attention layer from the input. Motivated by this, we introduce Decoupled Positional Attention for Transformers (DIET),... | PuChin Chen, Henry Tsai, Srinadh Bhojanapalli, Hyung Won Chung, YinWen Chang, ChunSung Ferng |  |
| 706 |  |  [Explore Better Relative Position Embeddings from Encoding Perspective for Transformer Models](https://doi.org/10.18653/v1/2021.emnlp-main.237) |  | 0 | Relative position embedding (RPE) is a successful method to explicitly and efficaciously encode position information into Transformer models. In this paper, we investigate the potential problems in Shaw-RPE and XL-RPE, which are the most representative and prevalent RPEs, and propose two novel RPEs called Low-level Fine-grained High-level Coarse-grained (LFHC) RPE and Gaussian Cumulative Distribution Function (GCDF) RPE. LFHC-RPE is an improvement of Shaw-RPE, which enhances the perception... | Anlin Qu, Jianwei Niu, Shasha Mo |  |
| 707 |  |  [Adversarial Mixing Policy for Relaxing Locally Linear Constraints in Mixup](https://doi.org/10.18653/v1/2021.emnlp-main.238) |  | 0 | Mixup is a recent regularizer for current deep classification networks. Through training a neural network on convex combinations of pairs of examples and their labels, it imposes locally linear constraints on the model’s input space. However, such strict linear constraints often lead to under-fitting which degrades the effects of regularization. Noticeably, this issue is getting more serious when the resource is extremely limited. To address these issues, we propose the Adversarial Mixing... | Guang Liu, Yuzhao Mao, Hailong Huang, Weiguo Gao, Xuan Li |  |
| 708 |  |  [Is this the end of the gold standard? A straightforward reference-less grammatical error correction metric](https://doi.org/10.18653/v1/2021.emnlp-main.239) |  | 0 | It is difficult to rank and evaluate the performance of grammatical error correction (GEC) systems, as a sentence can be rewritten in numerous correct ways. A number of GEC metrics have been used to evaluate proposed GEC systems; however, each system relies on either a comparison with one or more reference texts—in what is known as the gold standard for reference-based metrics—or a separate annotated dataset to fine-tune the reference-less metric. Reference-based systems have a low correlation... | Md Asadul Islam, Enrico Magnani |  |
| 709 |  |  [Augmenting BERT-style Models with Predictive Coding to Improve Discourse-level Representations](https://doi.org/10.18653/v1/2021.emnlp-main.240) |  | 0 | Current language models are usually trained using a self-supervised scheme, where the main focus is learning representations at the word or sentence level. However, there has been limited progress in generating useful discourse-level representations. In this work, we propose to use ideas from predictive coding theory to augment BERT-style language models with a mechanism that allows them to learn suitable discourse-level representations. As a result, our proposed approach is able to predict... | Vladimir Araujo, Andrés Villa, Marcelo Mendoza, MarieFrancine Moens, Alvaro Soto |  |
| 710 |  |  [Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning](https://doi.org/10.18653/v1/2021.emnlp-main.241) |  | 0 | Pre-Trained Models have been widely applied and recently proved vulnerable under backdoor attacks: the released pre-trained weights can be maliciously poisoned with certain triggers. When the triggers are activated, even the fine-tuned model will predict pre-defined labels, causing a security threat. These backdoors generated by the poisoning methods can be erased by changing hyper-parameters during fine-tuning or detected by finding the triggers. In this paper, we propose a stronger... | Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, Xipeng Qiu |  |
| 711 |  |  [GAML-BERT: Improving BERT Early Exiting by Gradient Aligned Mutual Learning](https://doi.org/10.18653/v1/2021.emnlp-main.242) |  | 0 | In this work, we propose a novel framework, Gradient Aligned Mutual Learning BERT (GAML-BERT), for improving the early exiting of BERT. GAML-BERT’s contributions are two-fold. We conduct a set of pilot experiments, which shows that mutual knowledge distillation between a shallow exit and a deep exit leads to better performances for both. From this observation, we use mutual learning to improve BERT’s early exiting performances, that is, we ask each exit of a multi-exit BERT to distill knowledge... | Wei Zhu, Xiaoling Wang, Yuan Ni, Guotong Xie |  |
| 712 |  |  [The Power of Scale for Parameter-Efficient Prompt Tuning](https://doi.org/10.18653/v1/2021.emnlp-main.243) |  | 0 | In this work, we explore “prompt tuning,” a simple yet effective mechanism for learning “soft prompts” to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3’s few-shot learning by a large margin. More remarkably, through ablations on model size using T5,... | Brian Lester, Rami AlRfou, Noah Constant |  |
| 713 |  |  [Scalable Font Reconstruction with Dual Latent Manifolds](https://doi.org/10.18653/v1/2021.emnlp-main.244) |  | 0 | We propose a deep generative model that performs typography analysis and font reconstruction by learning disentangled manifolds of both font style and character shape. Our approach enables us to massively scale up the number of character types we can effectively model compared to previous methods. Specifically, we infer separate latent variables representing character and font via a pair of inference networks which take as input sets of glyphs that either all share a character type, or belong... | Nikita Srivatsan, Si Wu, Jonathan T. Barron, Taylor BergKirkpatrick |  |
| 714 |  |  [Neuro-Symbolic Approaches for Text-Based Policy Learning](https://doi.org/10.18653/v1/2021.emnlp-main.245) |  | 0 | Text-Based Games (TBGs) have emerged as important testbeds for reinforcement learning (RL) in the natural language domain. Previous methods using LSTM-based action policies are uninterpretable and often overfit the training games showing poor performance to unseen test games. We present SymboLic Action policy for Textual Environments (SLATE), that learns interpretable action policy rules from symbolic abstractions of textual observations for improved generalization. We outline a method for... | Subhajit Chaudhury, Prithviraj Sen, Masaki Ono, Daiki Kimura, Michiaki Tatsubori, Asim Munawar |  |
| 715 |  |  [Layer-wise Model Pruning based on Mutual Information](https://doi.org/10.18653/v1/2021.emnlp-main.246) |  | 0 | Inspired by mutual information (MI) based feature selection in SVMs and logistic regression, in this paper, we propose MI-based layer-wise pruning: for each layer of a multi-layer neural network, neurons with higher values of MI with respect to preserved neurons in the upper layer are preserved. Starting from the top softmax layer, layer-wise pruning proceeds in a top-down fashion until reaching the bottom word embedding layer. The proposed pruning strategy offers merits over weight-based... | Chun Fan, Jiwei Li, Tianwei Zhang, Xiang Ao, Fei Wu, Yuxian Meng, Xiaofei Sun |  |
| 716 |  |  [Hierarchical Heterogeneous Graph Representation Learning for Short Text Classification](https://doi.org/10.18653/v1/2021.emnlp-main.247) |  | 0 | Short text classification is a fundamental task in natural language processing. It is hard due to the lack of context information and labeled data in practice. In this paper, we propose a new method called SHINE, which is based on graph neural network (GNN), for short text classification. First, we model the short text dataset as a hierarchical heterogeneous graph consisting of word-level component graphs which introduce more semantic and syntactic information. Then, we dynamically learn a... | Yaqing Wang, Song Wang, Quanming Yao, Dejing Dou |  |
| 717 |  |  [kFolden: k-Fold Ensemble for Out-Of-Distribution Detection](https://doi.org/10.18653/v1/2021.emnlp-main.248) |  | 0 | Out-of-Distribution (OOD) detection is an important problem in natural language processing (NLP). In this work, we propose a simple yet effective framework kFolden, which mimics the behaviors of OOD detection during training without the use of any external data. For a task with k training labels, kFolden induces k sub-models, each of which is trained on a subset with k-1 categories with the left category masked unknown to the sub-model. Exposing an unknown label to the sub-model during... | Xiaoya Li, Jiwei Li, Xiaofei Sun, Chun Fan, Tianwei Zhang, Fei Wu, Yuxian Meng, Jun Zhang |  |
| 718 |  |  [Frustratingly Simple Pretraining Alternatives to Masked Language Modeling](https://doi.org/10.18653/v1/2021.emnlp-main.249) |  | 0 | Masked language modeling (MLM), a self-supervised pretraining objective, is widely used in natural language processing for learning text representations. MLM trains a model to predict a random sample of input tokens that have been replaced by a [MASK] placeholder in a multi-class setting over the entire vocabulary. When pretraining, it is common to use alongside MLM other auxiliary objectives on the token or sequence level to improve downstream performance (e.g. next sentence prediction).... | Atsuki Yamaguchi, George Chrysostomou, Katerina Margatina, Nikolaos Aletras |  |
| 719 |  |  [HRKD: Hierarchical Relational Knowledge Distillation for Cross-domain Language Model Compression](https://doi.org/10.18653/v1/2021.emnlp-main.250) |  | 0 | On many natural language processing tasks, large pre-trained language models (PLMs) have shown overwhelming performances compared with traditional neural network methods. Nevertheless, their huge model size and low inference speed have hindered the deployment on resource-limited devices in practice. In this paper, we target to compress PLMs with knowledge distillation, and propose a hierarchical relational knowledge distillation (HRKD) method to capture both hierarchical and domain relational... | Chenhe Dong, Yaliang Li, Ying Shen, Minghui Qiu |  |
| 720 |  |  [Searching for an Effective Defender: Benchmarking Defense against Adversarial Word Substitution](https://doi.org/10.18653/v1/2021.emnlp-main.251) |  | 0 | Recent studies have shown that deep neural network-based models are vulnerable to intentionally crafted adversarial examples, and various methods have been proposed to defend against adversarial word-substitution attacks for neural NLP models. However, there is a lack of systematic study on comparing different defense approaches under the same attacking setting. In this paper, we seek to fill the gap of systematic studies through comprehensive researches on understanding the behavior of neural... | Zongyi Li, Jianhan Xu, Jiehang Zeng, Linyang Li, Xiaoqing Zheng, Qi Zhang, KaiWei Chang, ChoJui Hsieh |  |
| 721 |  |  [Re-embedding Difficult Samples via Mutual Information Constrained Semantically Oversampling for Imbalanced Text Classification](https://doi.org/10.18653/v1/2021.emnlp-main.252) |  | 0 | Difficult samples of the minority class in imbalanced text classification are usually hard to be classified as they are embedded into an overlapping semantic region with the majority class. In this paper, we propose a Mutual Information constrained Semantically Oversampling framework (MISO) that can generate anchor instances to help the backbone network determine the re-embedding position of a non-overlapping representation for each difficult sample. MISO consists of (1) a semantic fusion... | Jiachen Tian, Shizhan Chen, Xiaowang Zhang, Zhiyong Feng, Deyi Xiong, Shaojuan Wu, Chunliu Dou |  |
| 722 |  |  [Beyond Text: Incorporating Metadata and Label Structure for Multi-Label Document Classification using Heterogeneous Graphs](https://doi.org/10.18653/v1/2021.emnlp-main.253) |  | 0 | Multi-label document classification, associating one document instance with a set of relevant labels, is attracting more and more research attention. Existing methods explore the incorporation of information beyond text, such as document metadata or label structure. These approaches however either simply utilize the semantic information of metadata or employ the predefined parent-child label hierarchy, ignoring the heterogeneous graphical structures of metadata and labels, which we believe are... | Chenchen Ye, Linhai Zhang, Yulan He, Deyu Zhou, Jie Wu |  |
| 723 |  |  [Natural Language Processing Meets Quantum Physics: A Survey and Categorization](https://doi.org/10.18653/v1/2021.emnlp-main.254) |  | 0 | Recent research has investigated quantum NLP, designing algorithms that process natural language in quantum computers, and also quantum-inspired algorithms that improve NLP performance on classical computers. In this survey, we review representative methods at the intersection of NLP and quantum physics in the past ten years, categorizing them according to the use of quantum theory, the linguistic targets that are modeled, and the downstream application. The literature review ends with a... | Sixuan Wu, Jian Li, Peng Zhang, Yue Zhang |  |
| 724 |  |  [MetaTS: Meta Teacher-Student Network for Multilingual Sequence Labeling with Minimal Supervision](https://doi.org/10.18653/v1/2021.emnlp-main.255) |  | 0 | Sequence labeling aims to predict a fine-grained sequence of labels for the text. However, such formulation hinders the effectiveness of supervised methods due to the lack of token-level annotated data. This is exacerbated when we meet a diverse range of languages. In this work, we explore multilingual sequence labeling with minimal supervision using a single unified model for multiple languages. Specifically, we propose a Meta Teacher-Student (MetaTS) Network, a novel meta learning method to... | Zheng Li, Danqing Zhang, Tianyu Cao, Ying Wei, Yiwei Song, Bing Yin |  |
| 725 |  |  [Neural Machine Translation with Heterogeneous Topic Knowledge Embeddings](https://doi.org/10.18653/v1/2021.emnlp-main.256) |  | 0 | Neural Machine Translation (NMT) has shown a strong ability to utilize local context to disambiguate the meaning of words. However, it remains a challenge for NMT to leverage broader context information like topics. In this paper, we propose heterogeneous ways of embedding topic information at the sentence level into an NMT model to improve translation performance. Specifically, the topic information can be incorporated as pre-encoder topic embedding, post-encoder topic embedding, and decoder... | Weixuan Wang, Wei Peng, Meng Zhang, Qun Liu |  |
| 726 |  |  [Allocating Large Vocabulary Capacity for Cross-Lingual Language Model Pre-Training](https://doi.org/10.18653/v1/2021.emnlp-main.257) |  | 0 | Compared to monolingual models, cross-lingual models usually require a more expressive vocabulary to represent all languages adequately. We find that many languages are under-represented in recent cross-lingual language models due to the limited vocabulary capacity. To this end, we propose an algorithm VoCap to determine the desired vocabulary capacity of each language. However, increasing the vocabulary size significantly slows down the pre-training speed. In order to address the issues, we... | Bo Zheng, Li Dong, Shaohan Huang, Saksham Singhal, Wanxiang Che, Ting Liu, Xia Song, Furu Wei |  |
| 727 |  |  [Recurrent Attention for Neural Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.258) |  | 0 | Recent research questions the importance of the dot-product self-attention in Transformer models and shows that most attention heads learn simple positional patterns. In this paper, we push further in this research line and propose a novel substitute mechanism for self-attention: Recurrent AtteNtion (RAN) . RAN directly learns attention weights without any token-to-token interaction and further improves their capacity by layer-to-layer interaction. Across an extensive set of experiments on 10... | Jiali Zeng, Shuangzhi Wu, Yongjing Yin, Yufan Jiang, Mu Li |  |
| 728 |  |  [Learning from Multiple Noisy Augmented Data Sets for Better Cross-Lingual Spoken Language Understanding](https://doi.org/10.18653/v1/2021.emnlp-main.259) |  | 0 | Lack of training data presents a grand challenge to scaling out spoken language understanding (SLU) to low-resource languages. Although various data augmentation approaches have been proposed to synthesize training data in low-resource target languages, the augmented data sets are often noisy, and thus impede the performance of SLU models. In this paper we focus on mitigating noise in augmented data. We develop a denoising training approach. Multiple models are trained with data produced by... | Yingmei Guo, Linjun Shou, Jian Pei, Ming Gong, Mingxing Xu, Zhiyong Wu, Daxin Jiang |  |
| 729 |  |  [Enlivening Redundant Heads in Multi-head Self-attention for Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.260) |  | 0 | Multi-head self-attention recently attracts enormous interest owing to its specialized functions, significant parallelizable computation, and flexible extensibility. However, very recent empirical studies show that some self-attention heads make little contribution and can be pruned as redundant heads. This work takes a novel perspective of identifying and then vitalizing redundant heads. We propose a redundant head enlivening (RHE) method to precisely identify redundant heads, and then... | Tianfu Zhang, Heyan Huang, Chong Feng, Longbing Cao |  |
| 730 |  |  [Unsupervised Neural Machine Translation with Universal Grammar](https://doi.org/10.18653/v1/2021.emnlp-main.261) |  | 0 | Machine translation usually relies on parallel corpora to provide parallel signals for training. The advent of unsupervised machine translation has brought machine translation away from this reliance, though performance still lags behind traditional supervised machine translation. In unsupervised machine translation, the model seeks symmetric language similarities as a source of weak parallel signal to achieve translation. Chomsky’s Universal Grammar theory postulates that grammar is an innate... | Zuchao Li, Masao Utiyama, Eiichiro Sumita, Hai Zhao |  |
| 731 |  |  [Encouraging Lexical Translation Consistency for Document-Level Neural Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.262) |  | 0 | Recently a number of approaches have been proposed to improve translation performance for document-level neural machine translation (NMT). However, few are focusing on the subject of lexical translation consistency. In this paper we apply “one translation per discourse” in NMT, and aim to encourage lexical translation consistency for document-level NMT. This is done by first obtaining a word link for each source word in a document, which tells the positions where the source word appears. Then... | Xinglin Lyu, Junhui Li, Zhengxian Gong, Min Zhang |  |
| 732 |  |  [Improving Neural Machine Translation by Bidirectional Training](https://doi.org/10.18653/v1/2021.emnlp-main.263) |  | 0 | We present a simple and effective pretraining strategy – bidirectional training (BiT) for neural machine translation. Specifically, we bidirectionally update the model parameters at the early stage and then tune the model normally. To achieve bidirectional updating, we simply reconstruct the training samples from “src→tgt” to “src+tgt→tgt+src” without any complicated model modifications. Notably, our approach does not increase any parameters or training steps, requiring the parallel data... | Liang Ding, Di Wu, Dacheng Tao |  |
| 733 |  |  [Scheduled Sampling Based on Decoding Steps for Neural Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.264) |  | 0 | Scheduled sampling is widely used to mitigate the exposure bias problem for neural machine translation. Its core motivation is to simulate the inference scene during training by replacing ground-truth tokens with predicted tokens, thus bridging the gap between training and inference. However, vanilla scheduled sampling is merely based on training steps and equally treats all decoding steps. Namely, it simulates an inference scene with uniform error rates, which disobeys the real inference... | Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, Jie Zhou |  |
| 734 |  |  [Learning to Rewrite for Non-Autoregressive Neural Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.265) |  | 0 | Non-autoregressive neural machine translation, which decomposes the dependence on previous target tokens from the inputs of the decoder, has achieved impressive inference speedup but at the cost of inferior accuracy. Previous works employ iterative decoding to improve the translation by applying multiple refinement iterations. However, a serious drawback is that these approaches expose the serious weakness in recognizing the erroneous translation pieces. In this paper, we propose an... | Xinwei Geng, Xiaocheng Feng, Bing Qin |  |
| 735 |  |  [SHAPE : Shifted Absolute Position Embedding for Transformers](https://doi.org/10.18653/v1/2021.emnlp-main.266) |  | 0 | Position representation is crucial for building position-aware representations in Transformers. Existing position representations suffer from a lack of generalization to test data with unseen lengths or high computational cost. We investigate shifted absolute position embedding (SHAPE) to address both issues. The basic idea of SHAPE is to achieve shift invariance, which is a key property of recent successful position representations, by randomly shifting absolute positions during training. We... | Shun Kiyono, Sosuke Kobayashi, Jun Suzuki, Kentaro Inui |  |
| 736 |  |  [Self-Supervised Quality Estimation for Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.267) |  | 0 | Quality estimation (QE) of machine translation (MT) aims to evaluate the quality of machine-translated sentences without references and is important in practical applications of MT. Training QE models require massive parallel data with hand-crafted quality annotations, which are time-consuming and labor-intensive to obtain. To address the issue of the absence of annotated training data, previous studies attempt to develop unsupervised QE methods. However, very few of them can be applied to both... | Yuanhang Zheng, Zhixing Tan, Meng Zhang, Mieradilijiang Maimaiti, Huanbo Luan, Maosong Sun, Qun Liu, Yang Liu |  |
| 737 |  |  [Generalised Unsupervised Domain Adaptation of Neural Machine Translation with Cross-Lingual Data Selection](https://doi.org/10.18653/v1/2021.emnlp-main.268) |  | 0 | This paper considers the unsupervised domain adaptation problem for neural machine translation (NMT), where we assume the access to only monolingual text in either the source or target language in the new domain. We propose a cross-lingual data selection method to extract in-domain sentences in the missing language side from a large generic monolingual corpus. Our proposed method trains an adaptive layer on top of multilingual BERT by contrastive learning to align the representation between the... | ThuyTrang Vu, Xuanli He, Dinh Q. Phung, Gholamreza Haffari |  |
| 738 |  |  [STANKER: Stacking Network based on Level-grained Attention-masked BERT for Rumor Detection on Social Media](https://doi.org/10.18653/v1/2021.emnlp-main.269) |  | 0 | Rumor detection on social media puts pre-trained language models (LMs), such as BERT, and auxiliary features, such as comments, into use. However, on the one hand, rumor detection datasets in Chinese companies with comments are rare; on the other hand, intensive interaction of attention on Transformer-based models like BERT may hinder performance improvement. To alleviate these problems, we build a new Chinese microblog dataset named Weibo20 by collecting posts and associated comments from Sina... | Dongning Rao, Xin Miao, Zhihua Jiang, Ran Li |  |
| 739 |  |  [ActiveEA: Active Learning for Neural Entity Alignment](https://doi.org/10.18653/v1/2021.emnlp-main.270) |  | 0 | Entity Alignment (EA) aims to match equivalent entities across different Knowledge Graphs (KGs) and is an essential step of KG fusion. Current mainstream methods – neural EA models – rely on training with seed alignment, i.e., a set of pre-aligned entity pairs which are very costly to annotate. In this paper, we devise a novel Active Learning (AL) framework for neural EA, aiming to create highly informative seed alignment to obtain more effective EA models with less annotation cost. Our... | Bing Liu, Harrisen Scells, Guido Zuccon, Wen Hua, Genghong Zhao |  |
| 740 |  |  [Cost-effective End-to-end Information Extraction for Semi-structured Document Images](https://doi.org/10.18653/v1/2021.emnlp-main.271) |  | 0 | A real-world information extraction (IE) system for semi-structured document images often involves a long pipeline of multiple modules, whose complexity dramatically increases its development and maintenance cost. One can instead consider an end-to-end model that directly maps the input to the target output and simplify the entire process. However, such generation approach is known to lead to unstable performance if not designed carefully. Here we present our recent effort on transitioning from... | Wonseok Hwang, Hyunji Lee, Jinyeong Yim, Geewook Kim, Minjoon Seo |  |
| 741 |  |  [Improving Math Word Problems with Pre-trained Knowledge and Hierarchical Reasoning](https://doi.org/10.18653/v1/2021.emnlp-main.272) |  | 0 | The recent algorithms for math word problems (MWP) neglect to use outside knowledge not present in the problems. Most of them only capture the word-level relationship and ignore to build hierarchical reasoning like the human being for mining the contextual structure between words and sentences. In this paper, we propose a Reasoning with Pre-trained Knowledge and Hierarchical Structure (RPKHS) network, which contains a pre-trained knowledge encoder and a hierarchical reasoning encoder. Firstly,... | Weijiang Yu, Yingpeng Wen, Fudan Zheng, Nong Xiao |  |
| 742 |  |  [GraphMR: Graph Neural Network for Mathematical Reasoning](https://doi.org/10.18653/v1/2021.emnlp-main.273) |  | 0 | Mathematical reasoning aims to infer satisfiable solutions based on the given mathematics questions. Previous natural language processing researches have proven the effectiveness of sequence-to-sequence (Seq2Seq) or related variants on mathematics solving. However, few works have been able to explore structural or syntactic information hidden in expressions (e.g., precedence and associativity). This dissertation set out to investigate the usefulness of such untapped information for neural... | Weijie Feng, Binbin Liu, Dongpeng Xu, Qilong Zheng, Yun Xu |  |
| 743 |  |  [What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers](https://doi.org/10.18653/v1/2021.emnlp-main.274) |  | 0 | GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our... | Boseop Kim, HyoungSeok Kim, SangWoo Lee, Gichang Lee, DongHyun Kwak, Dong Hyeon Jeon, Sunghyun Park, Sungju Kim, Seonhoon Kim, Dongpil Seo, Heungsub Lee, Minyoung Jeong, Sungjae Lee, Minsub Kim, SukHyun Ko, Seokhun Kim, Taeyong Park, Jinuk Kim, Soyoung Kang, NaHyeon Ryu, Kang Min Yoo, Minsuk Chang, Soobin Suh, Sookyo In, Jinseong Park, Kyungduk Kim, Hiun Kim, Jisu Jeong, Yong Goo Yeo, Donghoon Ham, Dongju Park, Min Young Lee, Jaewook Kang, Inho Kang, JungWoo Ha, WooMyoung Park, Nako Sung |  |
| 744 |  |  [APIRecX: Cross-Library API Recommendation via Pre-Trained Language Model](https://doi.org/10.18653/v1/2021.emnlp-main.275) |  | 0 | For programmers, learning the usage of APIs (Application Programming Interfaces) of a software library is important yet difficult. API recommendation tools can help developers use APIs by recommending which APIs to be used next given the APIs that have been written. Traditionally, language models such as N-gram are applied to API recommendation. However, because the software libraries keep changing and new libraries keep emerging, new APIs are common. These new APIs can be seen as OOV (out of... | Yuning Kang, Zan Wang, Hongyu Zhang, Junjie Chen, Hanmo You |  |
| 745 |  |  [GMH: A General Multi-hop Reasoning Model for KG Completion](https://doi.org/10.18653/v1/2021.emnlp-main.276) |  | 0 | Knowledge graphs are essential for numerous downstream natural language processing applications, but are typically incomplete with many facts missing. This results in research efforts on multi-hop reasoning task, which can be formulated as a search process and current models typically perform short distance reasoning. However, the long-distance reasoning is also vital with the ability to connect the superficially unrelated entities. To the best of our knowledge, there lacks a general framework... | Yao Zhang, Hongru Liang, Adam Jatowt, Wenqiang Lei, Xin Wei, Ning Jiang, Zhenglu Yang |  |
| 746 |  |  [BPM_MT: Enhanced Backchannel Prediction Model using Multi-Task Learning](https://doi.org/10.18653/v1/2021.emnlp-main.277) |  | 0 | Backchannel (BC), a short reaction signal of a listener to a speaker’s utterances, helps to improve the quality of the conversation. Several studies have been conducted to predict BC in conversation; however, the utilization of advanced natural language processing techniques using lexical information presented in the utterances of a speaker has been less considered. To address this limitation, we present a BC prediction model called BPM_MT (Backchannel prediction model with multitask learning),... | Jin Yea Jang, San Kim, Minyoung Jung, Saim Shin, Gahgene Gweon |  |
| 747 |  |  [Graphine: A Dataset for Graph-aware Terminology Definition Generation](https://doi.org/10.18653/v1/2021.emnlp-main.278) |  | 0 | Precisely defining the terminology is the first step in scientific communication. Developing neural text generation models for definition generation can circumvent the labor-intensity curation, further accelerating scientific discovery. Unfortunately, the lack of large-scale terminology definition dataset hinders the process toward definition generation. In this paper, we present a large-scale terminology definition dataset Graphine covering 2,010,648 terminology definition pairs, spanning 227... | Zequn Liu, Shukai Wang, Yiyang Gu, Ruiyi Zhang, Ming Zhang, Sheng Wang |  |
| 748 |  |  [Leveraging Order-Free Tag Relations for Context-Aware Recommendation](https://doi.org/10.18653/v1/2021.emnlp-main.279) |  | 0 | Tag recommendation relies on either a ranking function for top-k tags or an autoregressive generation method. However, the previous methods neglect one of two seemingly conflicting yet desirable characteristics of a tag set: orderlessness and inter-dependency. While the ranking approach fails to address the inter-dependency among tags when they are ranked, the autoregressive approach fails to take orderlessness into account because it is designed to utilize sequential relations among tokens. We... | Junmo Kang, Jeonghwan Kim, Suwon Shin, SungHyon Myaeng |  |
| 749 |  |  [End-to-End Conversational Search for Online Shopping with Utterance Transfer](https://doi.org/10.18653/v1/2021.emnlp-main.280) |  | 0 | Successful conversational search systems can present natural, adaptive and interactive shopping experience for online shopping customers. However, building such systems from scratch faces real word challenges from both imperfect product schema/knowledge and lack of training dialog data. In this work we first propose ConvSearch, an end-to-end conversational search system that deeply combines the dialog system with search. It leverages the text profile to retrieve products, which is more robust... | Liqiang Xiao, Jun Ma, Xin Luna Dong, Pascual MartínezGómez, Nasser Zalmout, Wei Chen, Tong Zhao, Hao He, Yaohui Jin |  |
| 750 |  |  [Self-Supervised Curriculum Learning for Spelling Error Correction](https://doi.org/10.18653/v1/2021.emnlp-main.281) |  | 0 | Spelling Error Correction (SEC) that requires high-level language understanding is a challenging but useful task. Current SEC approaches normally leverage a pre-training then fine-tuning procedure that treats data equally. By contrast, Curriculum Learning (CL) utilizes training data differently during training and has shown its effectiveness in improving both performance and training efficiency in many other NLP tasks. In NMT, a model’s performance has been shown sensitive to the difficulty of... | Zifa Gan, Hongfei Xu, Hongying Zan |  |
| 751 |  |  [Fix-Filter-Fix: Intuitively Connect Any Models for Effective Bug Fixing](https://doi.org/10.18653/v1/2021.emnlp-main.282) |  | 0 | Locating and fixing bugs is a time-consuming task. Most neural machine translation (NMT) based approaches for automatically bug fixing lack generality and do not make full use of the rich information in the source code. In NMT-based bug fixing, we find some predicted code identical to the input buggy code (called unchanged fix) in NMT-based approaches due to high similarity between buggy and fixed code (e.g., the difference may only appear in one particular line). Obviously, unchanged fix is... | Haiwen Hong, Jingfeng Zhang, Yin Zhang, Yao Wan, Yulei Sui |  |
| 752 |  |  [Neuro-Symbolic Reinforcement Learning with First-Order Logic](https://doi.org/10.18653/v1/2021.emnlp-main.283) |  | 0 | Deep reinforcement learning (RL) methods often require many trials before convergence, and no direct interpretability of trained policies is provided. In order to achieve fast convergence and interpretability for the policy in RL, we propose a novel RL method for text-based games with a recent neuro-symbolic framework called Logical Neural Network, which can learn symbolic and interpretable rules in their differentiable network. The method is first to extract first-order logical facts from text... | Daiki Kimura, Masaki Ono, Subhajit Chaudhury, Ryosuke Kohita, Akifumi Wachi, Don Joven Agravante, Michiaki Tatsubori, Asim Munawar, Alexander Gray |  |
| 753 |  |  [Biomedical Concept Normalization by Leveraging Hypernyms](https://doi.org/10.18653/v1/2021.emnlp-main.284) |  | 0 | Biomedical Concept Normalization (BCN) is widely used in biomedical text processing as a fundamental module. Owing to numerous surface variants of biomedical concepts, BCN still remains challenging and unsolved. In this paper, we exploit biomedical concept hypernyms to facilitate BCN. We propose Biomedical Concept Normalizer with Hypernyms (BCNH), a novel framework that adopts list-wise training to make use of both hypernyms and synonyms, and also employs norm constraint on the representation... | Cheng Yan, Yuanzhe Zhang, Kang Liu, Jun Zhao, Yafei Shi, Shengping Liu |  |
| 754 |  |  [Leveraging Capsule Routing to Associate Knowledge with Medical Literature Hierarchically](https://doi.org/10.18653/v1/2021.emnlp-main.285) |  | 0 | Integrating knowledge into text is a promising way to enrich text representation, especially in the medical field. However, undifferentiated knowledge not only confuses the text representation but also imports unexpected noises. In this paper, to alleviate this problem, we propose leveraging capsule routing to associate knowledge with medical literature hierarchically (called HiCapsRKL). Firstly, HiCapsRKL extracts two empirically designed text fragments from medical literature and encodes them... | Xin Liu, Qingcai Chen, Junying Chen, Wenxiu Zhou, Tingyu Liu, Xinlan Yang, Weihua Peng |  |
| 755 |  |  [Label-Enhanced Hierarchical Contextualized Representation for Sequential Metaphor Identification](https://doi.org/10.18653/v1/2021.emnlp-main.286) |  | 0 | Recent metaphor identification approaches mainly consider the contextual text features within a sentence or introduce external linguistic features to the model. But they usually ignore the extra information that the data can provide, such as the contextual metaphor information and broader discourse information. In this paper, we propose a model augmented with hierarchical contextualized representation to extract more information from both sentence-level and discourse-level. At the sentence... | Shuqun Li, Liang Yang, Weidong He, Shiqi Zhang, Jingjie Zeng, Hongfei Lin |  |
| 756 |  |  [SpellBERT: A Lightweight Pretrained Model for Chinese Spelling Check](https://doi.org/10.18653/v1/2021.emnlp-main.287) |  | 0 | Chinese Spelling Check (CSC) is to detect and correct Chinese spelling errors. Many models utilize a predefined confusion set to learn a mapping between correct characters and its visually similar or phonetically similar misuses but the mapping may be out-of-domain. To that end, we propose SpellBERT, a pretrained model with graph-based extra features and independent on confusion set. To explicitly capture the two erroneous patterns, we employ a graph neural network to introduce radical and... | Tuo Ji, Hang Yan, Xipeng Qiu |  |
| 757 |  |  [Automated Generation of Accurate & Fluent Medical X-ray Reports](https://doi.org/10.18653/v1/2021.emnlp-main.288) |  | 0 | Our paper aims to automate the generation of medical reports from chest X-ray image inputs, a critical yet time-consuming task for radiologists. Existing medical report generation efforts emphasize producing human-readable reports, yet the generated text may not be well aligned to the clinical facts. Our generated medical reports, on the other hand, are fluent and, more importantly, clinically accurate. This is achieved by our fully differentiable and end-to-end paradigm that contains three... | Hoang T. N. Nguyen, Dong Nie, Taivanbat Badamdorj, Yujie Liu, Yingying Zhu, Jason Truong, Li Cheng |  |
| 758 |  |  [Enhancing Document Ranking with Task-adaptive Training and Segmented Token Recovery Mechanism](https://doi.org/10.18653/v1/2021.emnlp-main.289) |  | 0 | In this paper, we propose a new ranking model DR-BERT, which improves the Document Retrieval (DR) task by a task-adaptive training process and a Segmented Token Recovery Mechanism (STRM). In the task-adaptive training, we first pre-train DR-BERT to be domain-adaptive and then make the two-phase fine-tuning. In the first-phase fine-tuning, the model learns query-document matching patterns regarding different query types in a pointwise way. Next, in the second-phase fine-tuning, the model learns... | Xingwu Sun, Yanling Cui, Hongyin Tang, Fuzheng Zhang, Beihong Jin, Shi Wang |  |
| 759 |  |  [Abstract, Rationale, Stance: A Joint Model for Scientific Claim Verification](https://doi.org/10.18653/v1/2021.emnlp-main.290) |  | 0 | Scientific claim verification can help the researchers to easily find the target scientific papers with the sentence evidence from a large corpus for the given claim. Some existing works propose pipeline models on the three tasks of abstract retrieval, rationale selection and stance prediction. Such works have the problems of error propagation among the modules in the pipeline and lack of sharing valuable information among modules. We thus propose an approach, named as ARSJoint, that jointly... | Zhiwei Zhang, Jiyi Li, Fumiyo Fukumoto, Yanming Ye |  |
| 760 |  |  [A Fine-Grained Domain Adaption Model for Joint Word Segmentation and POS Tagging](https://doi.org/10.18653/v1/2021.emnlp-main.291) |  | 0 | Domain adaption for word segmentation and POS tagging is a challenging problem for Chinese lexical processing. Self-training is one promising solution for it, which struggles to construct a set of high-quality pseudo training instances for the target domain. Previous work usually assumes a universal source-to-target adaption to collect such pseudo corpus, ignoring the different gaps from the target sentences to the source domain. In this work, we start from joint word segmentation and POS... | Peijie Jiang, Dingkun Long, Yueheng Sun, Meishan Zhang, Guangwei Xu, Pengjun Xie |  |
| 761 |  |  [Answering Open-Domain Questions of Varying Reasoning Steps from Text](https://doi.org/10.18653/v1/2021.emnlp-main.292) |  | 0 | We develop a unified system to answer directly from text open-domain questions that may require a varying number of retrieval steps. We employ a single multi-task transformer model to perform all the necessary subtasks—retrieving supporting facts, reranking them, and predicting the answer from all retrieved documents—in an iterative fashion. We avoid crucial assumptions of previous work that do not transfer well to real-world settings, including exploiting knowledge of the fixed number of... | Peng Qi, Haejun Lee, Tg Sido, Christopher D. Manning |  |
| 762 |  |  [Adaptive Information Seeking for Open-Domain Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.293) |  | 0 | Information seeking is an essential step for open-domain question answering to efficiently gather evidence from a large corpus. Recently, iterative approaches have been proven to be effective for complex questions, by recursively retrieving new evidence at each step. However, almost all existing iterative approaches use predefined strategies, either applying the same retrieval function multiple times or fixing the order of different retrieval functions, which cannot fulfill the diverse... | Yunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, Xueqi Cheng |  |
| 763 |  |  [Mapping probability word problems to executable representations](https://doi.org/10.18653/v1/2021.emnlp-main.294) |  | 0 | While solving math word problems automatically has received considerable attention in the NLP community, few works have addressed probability word problems specifically. In this paper, we employ and analyse various neural models for answering such word problems. In a two-step approach, the problem text is first mapped to a formal representation in a declarative language using a sequence-to-sequence model, and then the resulting representation is executed using a probabilistic programming system... | Simon Suster, Pieter Fivez, Pietro Totis, Angelika Kimmig, Jesse Davis, Luc De Raedt, Walter Daelemans |  |
| 764 |  |  [Enhancing Multiple-choice Machine Reading Comprehension by Punishing Illogical Interpretations](https://doi.org/10.18653/v1/2021.emnlp-main.295) |  | 0 | Machine Reading Comprehension (MRC), which requires a machine to answer questions given the relevant documents, is an important way to test machines’ ability to understand human language. Multiple-choice MRC is one of the most studied tasks in MRC due to the convenience of evaluation and the flexibility of answer format. Post-hoc interpretation aims to explain a trained model and reveal how the model arrives at the prediction. One of the most important interpretation forms is to attribute model... | Yiming Ju, Yuanzhe Zhang, Zhixing Tian, Kang Liu, Xiaohuan Cao, Wenting Zhao, Jinlong Li, Jun Zhao |  |
| 765 |  |  [Large-Scale Relation Learning for Question Answering over Knowledge Bases with Pre-trained Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.296) |  | 0 | The key challenge of question answering over knowledge bases (KBQA) is the inconsistency between the natural language questions and the reasoning paths in the knowledge base (KB). Recent graph-based KBQA methods are good at grasping the topological structure of the graph but often ignore the textual information carried by the nodes and edges. Meanwhile, pre-trained language models learn massive open-world knowledge from the large corpus, but it is in the natural language form and not... | Yuanmeng Yan, Rumei Li, Sirui Wang, Hongzhi Zhang, Daoguang Zan, Fuzheng Zhang, Wei Wu, Weiran Xu |  |
| 766 |  |  [Phrase Retrieval Learns Passage Retrieval, Too](https://doi.org/10.18653/v1/2021.emnlp-main.297) |  | 0 | Dense retrieval methods have shown great promise over sparse retrieval methods in a range of NLP problems. Among them, dense phrase retrieval—the most fine-grained retrieval unit—is appealing because phrases can be directly used as the output for question answering and slot filling tasks. In this work, we follow the intuition that retrieving phrases naturally entails retrieving larger text blocks and study whether phrase retrieval can serve as the basis for coarse-level retrieval including... | Jinhyuk Lee, Alexander Wettig, Danqi Chen |  |
| 767 |  |  [Neural Natural Logic Inference for Interpretable Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.298) |  | 0 | Many open-domain question answering problems can be cast as a textual entailment task, where a question and candidate answers are concatenated to form hypotheses. A QA system then determines if the supporting knowledge bases, regarded as potential premises, entail the hypotheses. In this paper, we investigate a neural-symbolic QA approach that integrates natural logic reasoning within deep learning architectures, towards developing effective and yet explainable question answering models. The... | Jihao Shi, Xiao Ding, Li Du, Ting Liu, Bing Qin |  |
| 768 |  |  [Smoothing Dialogue States for Open Conversational Machine Reading](https://doi.org/10.18653/v1/2021.emnlp-main.299) |  | 0 | Conversational machine reading (CMR) requires machines to communicate with humans through multi-turn interactions between two salient dialogue states of decision making and question generation processes. In open CMR settings, as the more realistic scenario, the retrieved background knowledge would be noisy, which results in severe challenges in the information transmission. Existing studies commonly train independent or pipeline systems for the two subtasks. However, those methods are trivial... | Zhuosheng Zhang, Siru Ouyang, Hai Zhao, Masao Utiyama, Eiichiro Sumita |  |
| 769 |  |  [FinQA: A Dataset of Numerical Reasoning over Financial Data](https://doi.org/10.18653/v1/2021.emnlp-main.300) |  | 0 | The sheer volume of financial statements makes it difficult for humans to access and analyze a business’s financials. Robust numerical reasoning likewise faces unique challenges in this domain. In this work, we focus on answering deep questions over financial data, aiming to automate the analysis of a large corpus of financial documents. In contrast to existing tasks on general domain, the finance domain includes complex numerical reasoning and understanding of heterogeneous representations. To... | Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, TingHao Kenneth Huang, Bryan R. Routledge, William Yang Wang |  |
| 770 |  |  [FiD-Ex: Improving Sequence-to-Sequence Models for Extractive Rationale Generation](https://doi.org/10.18653/v1/2021.emnlp-main.301) |  | 0 | Natural language (NL) explanations of model predictions are gaining popularity as a means to understand and verify decisions made by large black-box pre-trained models, for tasks such as Question Answering (QA) and Fact Verification. Recently, pre-trained sequence to sequence (seq2seq) models have proven to be very effective in jointly making predictions, as well as generating NL explanations. However, these models have many shortcomings; they can fabricate explanations even for incorrect... | Kushal Lakhotia, Bhargavi Paranjape, Asish Ghoshal, Scott Yih, Yashar Mehdad, Srini Iyer |  |
| 771 |  |  [RockNER: A Simple Method to Create Adversarial Examples for Evaluating the Robustness of Named Entity Recognition Models](https://doi.org/10.18653/v1/2021.emnlp-main.302) |  | 0 | To audit the robustness of named entity recognition (NER) models, we propose RockNER, a simple yet effective method to create natural adversarial examples. Specifically, at the entity level, we replace target entities with other entities of the same semantic class in Wikidata; at the context level, we use pre-trained language models (e.g., BERT) to generate word substitutions. Together, the two levels of at- tack produce natural adversarial examples that result in a shifted distribution from... | Bill Yuchen Lin, Wenyang Gao, Jun Yan, Ryan Moreno, Xiang Ren |  |
| 772 |  |  [Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI](https://doi.org/10.18653/v1/2021.emnlp-main.303) |  | 0 | Recently, language models (LMs) have achieved significant performance on many NLU tasks, which has spurred widespread interest for their possible applications in the scientific and social area. However, LMs have faced much criticism of whether they are truly capable of reasoning in NLU. In this work, we propose a diagnostic method for first-order logic (FOL) reasoning with a new proposed benchmark, LogicNLI. LogicNLI is an NLI-style dataset that effectively disentangles the target FOL reasoning... | Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, Yaohui Jin |  |
| 773 |  |  [Constructing a Psychometric Testbed for Fair Natural Language Processing](https://doi.org/10.18653/v1/2021.emnlp-main.304) |  | 0 | Psychometric measures of ability, attitudes, perceptions, and beliefs are crucial for understanding user behavior in various contexts including health, security, e-commerce, and finance. Traditionally, psychometric dimensions have been measured and collected using survey-based methods. Inferring such constructs from user-generated text could allow timely, unobtrusive collection and analysis. In this paper we describe our efforts to construct a corpus for psychometric natural language processing... | Ahmed Abbasi, David G. Dobolyi, John P. Lalor, Richard G. Netemeyer, Kendall Smith, Yi Yang |  |
| 774 |  |  [COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval](https://doi.org/10.18653/v1/2021.emnlp-main.305) |  | 0 | We present a large, challenging dataset, COUGH, for COVID-19 FAQ retrieval. Similar to a standard FAQ dataset, COUGH consists of three parts: FAQ Bank, Query Bank and Relevance Set. The FAQ Bank contains ~16K FAQ items scraped from 55 credible websites (e.g., CDC and WHO). For evaluation, we introduce Query Bank and Relevance Set, where the former contains 1,236 human-paraphrased queries while the latter contains ~32 human-annotated FAQ items for each query. We analyze COUGH by testing... | Xinliang Frederick Zhang, Heming Sun, Xiang Yue, Simon M. Lin, Huan Sun |  |
| 775 |  |  [Chinese WPLC: A Chinese Dataset for Evaluating Pretrained Language Models on Word Prediction Given Long-Range Context](https://doi.org/10.18653/v1/2021.emnlp-main.306) |  | 0 | This paper presents a Chinese dataset for evaluating pretrained language models on Word Prediction given Long-term Context (Chinese WPLC). We propose both automatic and manual selection strategies tailored to Chinese to guarantee that target words in passages collected from over 69K novels can only be predicted with long-term context beyond the scope of sentences containing the target words. Dataset analysis reveals that the types of target words range from common nouns to Chinese 4-character... | Huibin Ge, Chenxi Sun, Deyi Xiong, Qun Liu |  |
| 776 |  |  [WinoLogic: A Zero-Shot Logic-based Diagnostic Dataset for Winograd Schema Challenge](https://doi.org/10.18653/v1/2021.emnlp-main.307) |  | 0 | The recent success of neural language models (NLMs) on the Winograd Schema Challenge has called for further investigation of the commonsense reasoning ability of these models. Previous diagnostic datasets rely on crowd-sourcing which fails to provide coherent commonsense crucial for solving WSC problems. To better evaluate NLMs, we propose a logic-based framework that focuses on high-quality commonsense knowledge. Specifically, we identify and collect formal knowledge formulas verified by... | Weinan He, Canming Huang, Yongmei Liu, Xiaodan Zhu |  |
| 777 |  |  [Pseudo Zero Pronoun Resolution Improves Zero Anaphora Resolution](https://doi.org/10.18653/v1/2021.emnlp-main.308) |  | 0 | Masked language models (MLMs) have contributed to drastic performance improvements with regard to zero anaphora resolution (ZAR). To further improve this approach, in this study, we made two proposals. The first is a new pretraining task that trains MLMs on anaphoric relations with explicit supervision, and the second proposal is a new finetuning method that remedies a notorious issue, the pretrain-finetune discrepancy. Our experiments on Japanese ZAR demonstrated that our two proposals boost... | Ryuto Konno, Shun Kiyono, Yuichiroh Matsubayashi, Hiroki Ouchi, Kentaro Inui |  |
| 778 |  |  [Aligning Cross-lingual Sentence Representations with Dual Momentum Contrast](https://doi.org/10.18653/v1/2021.emnlp-main.309) |  | 0 | In this paper, we propose to align sentence representations from different languages into a unified embedding space, where semantic similarities (both cross-lingual and monolingual) can be computed with a simple dot product. Pre-trained language models are fine-tuned with the translation ranking task. Existing work (Feng et al., 2020) uses sentences within the same batch as negatives, which can suffer from the issue of easy negatives. We adapt MoCo (He et al., 2020) to further improve the... | Liang Wang, Wei Zhao, Jingming Liu |  |
| 779 |  |  [Total Recall: a Customized Continual Learning Method for Neural Semantic Parsers](https://doi.org/10.18653/v1/2021.emnlp-main.310) |  | 0 | This paper investigates continual learning for semantic parsing. In this setting, a neural semantic parser learns tasks sequentially without accessing full training data from previous tasks. Direct application of the SOTA continual learning algorithms to this problem fails to achieve comparable performance with re-training models with all seen tasks because they have not considered the special properties of structured outputs yielded by semantic parsers. Therefore, we propose TotalRecall, a... | Zhuang Li, Lizhen Qu, Gholamreza Haffari |  |
| 780 |  |  [Exophoric Pronoun Resolution in Dialogues with Topic Regularization](https://doi.org/10.18653/v1/2021.emnlp-main.311) |  | 0 | Resolving pronouns to their referents has long been studied as a fundamental natural language understanding problem. Previous works on pronoun coreference resolution (PCR) mostly focus on resolving pronouns to mentions in text while ignoring the exophoric scenario. Exophoric pronouns are common in daily communications, where speakers may directly use pronouns to refer to some objects present in the environment without introducing the objects first. Although such objects are not mentioned in the... | Xintong Yu, Hongming Zhang, Yangqiu Song, Changshui Zhang, Kun Xu, Dong Yu |  |
| 781 |  |  [Context-Aware Interaction Network for Question Matching](https://doi.org/10.18653/v1/2021.emnlp-main.312) |  | 0 | Impressive milestones have been achieved in text matching by adopting a cross-attention mechanism to capture pertinent semantic connections between two sentence representations. However, regular cross-attention focuses on word-level links between the two input sequences, neglecting the importance of contextual information. We propose a context-aware interaction network (COIN) to properly align two sequences and infer their semantic relationship. Specifically, each interaction block includes (1)... | Zhe Hu, Zuohui Fu, Yu Yin, Gerard de Melo |  |
| 782 |  |  [TEMP: Taxonomy Expansion with Dynamic Margin Loss through Taxonomy-Paths](https://doi.org/10.18653/v1/2021.emnlp-main.313) |  | 0 | As an essential form of knowledge representation, taxonomies are widely used in various downstream natural language processing tasks. However, with the continuously rising of new concepts, many existing taxonomies are unable to maintain coverage by manual expansion. In this paper, we propose TEMP, a self-supervised taxonomy expansion method, which predicts the position of new concepts by ranking the generated taxonomy-paths. For the first time, TEMP employs pre-trained contextual encoders in... | Zichen Liu, Hongyuan Xu, Yanlong Wen, Ning Jiang, Haiying Wu, Xiaojie Yuan |  |
| 783 |  |  [A Graph-Based Neural Model for End-to-End Frame Semantic Parsing](https://doi.org/10.18653/v1/2021.emnlp-main.314) |  | 0 | Frame semantic parsing is a semantic analysis task based on FrameNet which has received great attention recently. The task usually involves three subtasks sequentially: (1) target identification, (2) frame classification and (3) semantic role labeling. The three subtasks are closely related while previous studies model them individually, which ignores their intern connections and meanwhile induces error propagation problem. In this work, we propose an end-to-end neural model to tackle the task... | Zhichao Lin, Yueheng Sun, Meishan Zhang |  |
| 784 |  |  [Virtual Data Augmentation: A Robust and General Framework for Fine-tuning Pre-trained Models](https://doi.org/10.18653/v1/2021.emnlp-main.315) |  | 0 | Recent works have shown that powerful pre-trained language models (PLM) can be fooled by small perturbations or intentional attacks. To solve this issue, various data augmentation techniques are proposed to improve the robustness of PLMs. However, it is still challenging to augment semantically relevant examples with sufficient diversity. In this work, we present Virtual Data Augmentation (VDA), a general framework for robustly fine-tuning PLMs. Based on the original token embeddings, we... | Kun Zhou, Wayne Xin Zhao, Sirui Wang, Fuzheng Zhang, Wei Wu, JiRong Wen |  |
| 785 |  |  [CATE: A Contrastive Pre-trained Model for Metaphor Detection with Semi-supervised Learning](https://doi.org/10.18653/v1/2021.emnlp-main.316) |  | 0 | Metaphors are ubiquitous in natural language, and detecting them requires contextual reasoning about whether a semantic incongruence actually exists. Most existing work addresses this problem using pre-trained contextualized models. Despite their success, these models require a large amount of labeled data and are not linguistically-based. In this paper, we proposed a ContrAstive pre-Trained modEl (CATE) for metaphor detection with semi-supervised learning. Our model first uses a pre-trained... | Zhenxi Lin, Qianli Ma, Jiangyue Yan, Jieyu Chen |  |
| 786 |  |  [To be Closer: Learning to Link up Aspects with Opinions](https://doi.org/10.18653/v1/2021.emnlp-main.317) |  | 0 | Dependency parse trees are helpful for discovering the opinion words in aspect-based sentiment analysis (ABSA) (CITATION). However, the trees obtained from off-the-shelf dependency parsers are static, and could be sub-optimal in ABSA. This is because the syntactic trees are not designed for capturing the interactions between opinion words and aspect words. In this work, we aim to shorten the distance between aspects and corresponding opinion words by learning an aspect-centric tree structure.... | Yuxiang Zhou, Lejian Liao, Yang Gao, Zhanming Jie, Wei Lu |  |
| 787 |  |  [Seeking Common but Distinguishing Difference, A Joint Aspect-based Sentiment Analysis Model](https://doi.org/10.18653/v1/2021.emnlp-main.318) |  | 0 | Aspect-based sentiment analysis (ABSA) task consists of three typical subtasks: aspect term extraction, opinion term extraction, and sentiment polarity classification. These three subtasks are usually performed jointly to save resources and reduce the error propagation in the pipeline. However, most of the existing joint models only focus on the benefits of encoder sharing between subtasks but ignore the difference. Therefore, we propose a joint ABSA model, which not only enjoys the benefits of... | Hongjiang Jing, Zuchao Li, Hai Zhao, Shu Jiang |  |
| 788 |  |  [Argument Pair Extraction with Mutual Guidance and Inter-sentence Relation Graph](https://doi.org/10.18653/v1/2021.emnlp-main.319) |  | 0 | Argument pair extraction (APE) aims to extract interactive argument pairs from two passages of a discussion. Previous work studied this task in the context of peer review and rebuttal, and decomposed it into a sequence labeling task and a sentence relation classification task. However, despite the promising performance, such an approach obtains the argument pairs implicitly by the two decomposed tasks, lacking explicitly modeling of the argument-level interactions between argument pairs. In... | Jianzhu Bao, Bin Liang, Jingyi Sun, Yice Zhang, Min Yang, Ruifeng Xu |  |
| 789 |  |  [Emotion Inference in Multi-Turn Conversations with Addressee-Aware Module and Ensemble Strategy](https://doi.org/10.18653/v1/2021.emnlp-main.320) |  | 0 | Emotion inference in multi-turn conversations aims to predict the participant’s emotion in the next upcoming turn without knowing the participant’s response yet, and is a necessary step for applications such as dialogue planning. However, it is a severe challenge to perceive and reason about the future feelings of participants, due to the lack of utterance information from the future. Moreover, it is crucial for emotion inference to capture the characteristics of emotional propagation in... | Dayu Li, Xiaodan Zhu, Yang Li, Suge Wang, Deyu Li, Jian Liao, Jianxing Zheng |  |
| 790 |  |  [Improving Federated Learning for Aspect-based Sentiment Analysis via Topic Memories](https://doi.org/10.18653/v1/2021.emnlp-main.321) |  | 0 | Aspect-based sentiment analysis (ABSA) predicts the sentiment polarity towards a particular aspect term in a sentence, which is an important task in real-world applications. To perform ABSA, the trained model is required to have a good understanding of the contextual information, especially the particular patterns that suggest the sentiment polarity. However, these patterns typically vary in different sentences, especially when the sentences come from different sources (domains), which makes... | Han Qin, Guimin Chen, Yuanhe Tian, Yan Song |  |
| 791 |  |  [Comparative Opinion Quintuple Extraction from Product Reviews](https://doi.org/10.18653/v1/2021.emnlp-main.322) |  | 0 | As an important task in opinion mining, comparative opinion mining aims to identify comparative sentences from product reviews, extract the comparative elements, and obtain the corresponding comparative opinion tuples. However, most previous studies simply regarded comparative tuple extraction as comparative element extraction, but ignored the fact that many comparative sentences may contain multiple comparisons. The comparative opinion tuples defined in these studies also failed to explicitly... | Ziheng Liu, Rui Xia, Jianfei Yu |  |
| 792 |  |  [CTAL: Pre-training Cross-modal Transformer for Audio-and-Language Representations](https://doi.org/10.18653/v1/2021.emnlp-main.323) |  | 0 | Existing audio-language task-specific predictive approaches focus on building complicated late-fusion mechanisms. However, these models are facing challenges of overfitting with limited labels and low model generalization abilities. In this paper, we present a Cross-modal Transformer for Audio-and-Language, i.e., CTAL, which aims to learn the intra-modality and inter-modality connections between audio and language through two proxy tasks on a large amount of audio-and-language pairs: masked... | Hang Li, Wenbiao Ding, Yu Kang, Tianqiao Liu, Zhongqin Wu, Zitao Liu |  |
| 793 |  |  [Relation-aware Video Reading Comprehension for Temporal Language Grounding](https://doi.org/10.18653/v1/2021.emnlp-main.324) |  | 0 | Temporal language grounding in videos aims to localize the temporal span relevant to the given query sentence. Previous methods treat it either as a boundary regression task or a span extraction task. This paper will formulate temporal language grounding into video reading comprehension and propose a Relation-aware Network (RaNet) to address it. This framework aims to select a video moment choice from the predefined answer set with the aid of coarse-and-fine choice-query interaction and... | Jialin Gao, Xin Sun, Mengmeng Xu, Xi Zhou, Bernard Ghanem |  |
| 794 |  |  [Mutual-Learning Improves End-to-End Speech Translation](https://doi.org/10.18653/v1/2021.emnlp-main.325) |  | 0 | A currently popular research area in end-to-end speech translation is the use of knowledge distillation from a machine translation (MT) task to improve the speech translation (ST) task. However, such scenario obviously only allows one way transfer, which is limited by the performance of the teacher model. Therefore, We hypothesis that the knowledge distillation-based approaches are sub-optimal. In this paper, we propose an alternative–a trainable mutual-learning scenario, where the MT and the... | Jiawei Zhao, Wei Luo, Boxing Chen, Andrew Gilman |  |
| 795 |  |  [Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.326) |  | 0 | Multimodal abstractive summarization (MAS) models that summarize videos (vision modality) and their corresponding transcripts (text modality) are able to extract the essential information from massive multimodal data on the Internet. Recently, large-scale generative pre-trained language models (GPLMs) have been shown to be effective in text generation tasks. However, existing MAS models cannot leverage GPLMs’ powerful generation ability. To fill this research gap, we aim to study two research... | Tiezheng Yu, Wenliang Dai, Zihan Liu, Pascale Fung |  |
| 796 |  |  [Natural Language Video Localization with Learnable Moment Proposals](https://doi.org/10.18653/v1/2021.emnlp-main.327) |  | 0 | Given an untrimmed video and a natural language query, Natural Language Video Localization (NLVL) aims to identify the video moment described by query. To address this task, existing methods can be roughly grouped into two groups: 1) propose-and-rank models first define a set of hand-designed moment candidates and then find out the best-matching one. 2) proposal-free models directly predict two temporal boundaries of the referential moment from frames. Currently, almost all the propose-and-rank... | Shaoning Xiao, Long Chen, Jian Shao, Yueting Zhuang, Jun Xiao |  |
| 797 |  |  [Language-Aligned Waypoint (LAW) Supervision for Vision-and-Language Navigation in Continuous Environments](https://doi.org/10.18653/v1/2021.emnlp-main.328) |  | 0 | In the Vision-and-Language Navigation (VLN) task an embodied agent navigates a 3D environment, following natural language instructions. A challenge in this task is how to handle ‘off the path’ scenarios where an agent veers from a reference path. Prior work supervises the agent with actions based on the shortest path from the agent’s location to the goal, but such goal-oriented supervision is often not in alignment with the instruction. Furthermore, the evaluation metrics employed by prior work... | Sonia Raychaudhuri, Saim Wani, Shivansh Patel, Unnat Jain, Angel X. Chang |  |
| 798 |  |  [How to leverage the multimodal EHR data for better medical prediction?](https://doi.org/10.18653/v1/2021.emnlp-main.329) |  | 0 | Healthcare is becoming a more and more important research topic recently. With the growing data in the healthcare domain, it offers a great opportunity for deep learning to improve the quality of service and reduce costs. However, the complexity of electronic health records (EHR) data is a challenge for the application of deep learning. Specifically, the data produced in the hospital admissions are monitored by the EHR system, which includes structured data like daily body temperature and... | Bo Yang, Lijun Wu |  |
| 799 |  |  [Considering Nested Tree Structure in Sentence Extractive Summarization with Pre-trained Transformer](https://doi.org/10.18653/v1/2021.emnlp-main.330) |  | 0 | Sentence extractive summarization shortens a document by selecting sentences for a summary while preserving its important contents. However, constructing a coherent and informative summary is difficult using a pre-trained BERT-based encoder since it is not explicitly trained for representing the information of sentences in a document. We propose a nested tree-based extractive summarization model on RoBERTa (NeRoBERTa), where nested tree structures consist of syntactic and discourse trees in a... | Jingun Kwon, Naoki Kobayashi, Hidetaka Kamigaito, Manabu Okumura |  |
| 800 |  |  [Frame Semantic-Enhanced Sentence Modeling for Sentence-level Extractive Text Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.331) |  | 0 | Sentence-level extractive text summarization aims to select important sentences from a given document. However, it is very challenging to model the importance of sentences. In this paper, we propose a novel Frame Semantic-Enhanced Sentence Modeling for Extractive Summarization, which leverages Frame semantics to model sentences from both intra-sentence level and inter-sentence level, facilitating the text summarization task. In particular, intra-sentence level semantics leverage Frames and... | Yong Guan, Shaoru Guo, Ru Li, Xiaoli Li, Hongye Tan |  |
| 801 |  |  [CAST: Enhancing Code Summarization with Hierarchical Splitting and Reconstruction of Abstract Syntax Trees](https://doi.org/10.18653/v1/2021.emnlp-main.332) |  | 0 | Code summarization aims to generate concise natural language descriptions of source code, which can help improve program comprehension and maintenance. Recent studies show that syntactic and structural information extracted from abstract syntax trees (ASTs) is conducive to summary generation. However, existing approaches fail to fully capture the rich information in ASTs because of the large size/depth of ASTs. In this paper, we propose a novel model CAST that hierarchically splits and... | Ensheng Shi, Yanlin Wang, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, Hongbin Sun |  |
| 802 |  |  [SgSum: Transforming Multi-document Summarization into Sub-graph Selection](https://doi.org/10.18653/v1/2021.emnlp-main.333) |  | 0 | Most of existing extractive multi-document summarization (MDS) methods score each sentence individually and extract salient sentences one by one to compose a summary, which have two main drawbacks: (1) neglecting both the intra and cross-document relations between sentences; (2) neglecting the coherence and conciseness of the whole summary. In this paper, we propose a novel MDS framework (SgSum) to formulate the MDS task as a sub-graph selection problem, in which source documents are regarded... | Moye Chen, Wei Li, Jiachen Liu, Xinyan Xiao, Hua Wu, Haifeng Wang |  |
| 803 |  |  [Event Graph based Sentence Fusion](https://doi.org/10.18653/v1/2021.emnlp-main.334) |  | 0 | Sentence fusion is a conditional generation task that merges several related sentences into a coherent one, which can be deemed as a summary sentence. The importance of sentence fusion has long been recognized by communities in natural language generation, especially in text summarization. It remains challenging for a state-of-the-art neural abstractive summarization model to generate a well-integrated summary sentence. In this paper, we explore the effective sentence fusion method in the... | Ruifeng Yuan, Zili Wang, Wenjie Li |  |
| 804 |  |  [Transformer-based Lexically Constrained Headline Generation](https://doi.org/10.18653/v1/2021.emnlp-main.335) |  | 0 | This paper explores a variant of automatic headline generation methods, where a generated headline is required to include a given phrase such as a company or a product name. Previous methods using Transformer-based models generate a headline including a given phrase by providing the encoder with additional information corresponding to the given phrase. However, these methods cannot always include the phrase in the generated headline. Inspired by previous RNN-based methods generating token... | Kosuke Yamada, Yuta Hitomi, Hideaki Tamori, Ryohei Sasano, Naoaki Okazaki, Kentaro Inui, Koichi Takeda |  |
| 805 |  |  [Learn to Copy from the Copying History: Correlational Copy Network for Abstractive Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.336) |  | 0 | The copying mechanism has had considerable success in abstractive summarization, facilitating models to directly copy words from the input text to the output summary. Existing works mostly employ encoder-decoder attention, which applies copying at each time step independently of the former ones. However, this may sometimes lead to incomplete copying. In this paper, we propose a novel copying scheme named Correlational Copying Network (CoCoNet) that enhances the standard copying mechanism by... | Haoran Li, Song Xu, Peng Yuan, Yujia Wang, Youzheng Wu, Xiaodong He, Bowen Zhou |  |
| 806 |  |  [Gradient-Based Adversarial Factual Consistency Evaluation for Abstractive Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.337) |  | 0 | Neural abstractive summarization systems have gained significant progress in recent years. However, abstractive summarization often produce inconsisitent statements or false facts. How to automatically generate highly abstract yet factually correct summaries? In this paper, we proposed an efficient weak-supervised adversarial data augmentation approach to form the factual consistency dataset. Based on the artificial dataset, we train an evaluation model that can not only make accurate and... | Zhiyuan Zeng, Jiaze Chen, Weiran Xu, Lei Li |  |
| 807 |  |  [Word Reordering for Zero-shot Cross-lingual Structured Prediction](https://doi.org/10.18653/v1/2021.emnlp-main.338) |  | 0 | Adapting word order from one language to another is a key problem in cross-lingual structured prediction. Current sentence encoders (e.g., RNN, Transformer with position embeddings) are usually word order sensitive. Even with uniform word form representations (MUSE, mBERT), word order discrepancies may hurt the adaptation of models. In this paper, we build structured prediction models with bag-of-words inputs, and introduce a new reordering module to organizing words following the source... | Tao Ji, Yong Jiang, Tao Wang, Zhongqiang Huang, Fei Huang, Yuanbin Wu, Xiaoling Wang |  |
| 808 |  |  [A Unified Encoding of Structures in Transition Systems](https://doi.org/10.18653/v1/2021.emnlp-main.339) |  | 0 | Transition systems usually contain various dynamic structures (e.g., stacks, buffers). An ideal transition-based model should encode these structures completely and efficiently. Previous works relying on templates or neural network structures either only encode partial structure information or suffer from computation efficiency. In this paper, we propose a novel attention-based encoder unifying representation of all structures in a transition system. Specifically, we separate two views of items... | Tao Ji, Yong Jiang, Tao Wang, Zhongqiang Huang, Fei Huang, Yuanbin Wu, Xiaoling Wang |  |
| 809 |  |  [Improving Unsupervised Question Answering via Summarization-Informed Question Generation](https://doi.org/10.18653/v1/2021.emnlp-main.340) |  | 0 | Question Generation (QG) is the task of generating a plausible question for a given <passage, answer> pair. Template-based QG uses linguistically-informed heuristics to transform declarative sentences into interrogatives, whereas supervised QG uses existing Question Answering (QA) datasets to train a system to generate a question given a passage and an answer. A disadvantage of the heuristic approach is that the generated questions are heavily tied to their declarative counterparts. A... | Chenyang Lyu, Lifeng Shang, Yvette Graham, Jennifer Foster, Xin Jiang, Qun Liu |  |
| 810 |  |  [TransferNet: An Effective and Transparent Framework for Multi-hop Question Answering over Relation Graph](https://doi.org/10.18653/v1/2021.emnlp-main.341) |  | 0 | Multi-hop Question Answering (QA) is a challenging task because it requires precise reasoning with entity relations at every step towards the answer. The relations can be represented in terms of labels in knowledge graph (e.g., spouse) or text in text corpus (e.g., they have been married for 26 years). Existing models usually infer the answer by predicting the sequential relation path or aggregating the hidden graph features. The former is hard to optimize, and the latter lacks... | Jiaxin Shi, Shulin Cao, Lei Hou, Juanzi Li, Hanwang Zhang |  |
| 811 |  |  [Topic Transferable Table Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.342) |  | 0 | Weakly-supervised table question-answering (TableQA) models have achieved state-of-art performance by using pre-trained BERT transformer to jointly encoding a question and a table to produce structured query for the question. However, in practical settings TableQA systems are deployed over table corpora having topic and word distributions quite distinct from BERT’s pretraining corpus. In this work we simulate the practical topic shift scenario by designing novel challenge benchmarks WikiSQL-TS... | Saneem A. Chemmengath, Vishwajeet Kumar, Samarth Bharadwaj, Jaydeep Sen, Mustafa Canim, Soumen Chakrabarti, Alfio Gliozzo, Karthik Sankaranarayanan |  |
| 812 |  |  [WebSRC: A Dataset for Web-Based Structural Reading Comprehension](https://doi.org/10.18653/v1/2021.emnlp-main.343) |  | 0 | Web search is an essential way for humans to obtain information, but it’s still a great challenge for machines to understand the contents of web pages. In this paper, we introduce the task of web-based structural reading comprehension. Given a web page and a question about it, the task is to find an answer from the web page. This task requires a system not only to understand the semantics of texts but also the structure of the web page. Moreover, we proposed WebSRC, a novel Web-based Structural... | Xingyu Chen, Zihan Zhao, Lu Chen, Jiabao Ji, Danyang Zhang, Ao Luo, Yuxuan Xiong, Kai Yu |  |
| 813 |  |  [Cryptonite: A Cryptic Crossword Benchmark for Extreme Ambiguity in Language](https://doi.org/10.18653/v1/2021.emnlp-main.344) |  | 0 | Current NLP datasets targeting ambiguity can be solved by a native speaker with relative ease. We present Cryptonite, a large-scale dataset based on cryptic crosswords, which is both linguistically complex and naturally sourced. Each example in Cryptonite is a cryptic clue, a short phrase or sentence with a misleading surface reading, whose solving requires disambiguating semantic, syntactic, and phonetic wordplays, as well as world knowledge. Cryptic clues pose a challenge even for experienced... | Avia Efrat, Uri Shaham, Dan Kilman, Omer Levy |  |
| 814 |  |  [End-to-End Entity Resolution and Question Answering Using Differentiable Knowledge Graphs](https://doi.org/10.18653/v1/2021.emnlp-main.345) |  | 0 | Recently, end-to-end (E2E) trained models for question answering over knowledge graphs (KGQA) have delivered promising results using only a weakly supervised dataset. However, these models are trained and evaluated in a setting where hand-annotated question entities are supplied to the model, leaving the important and non-trivial task of entity resolution (ER) outside the scope of E2E learning. In this work, we extend the boundaries of E2E learning for KGQA to include the training of an ER... | Amir Saffari, Armin Oliya, Priyanka Sen, Tom Ayoola |  |
| 815 |  |  [Improving Query Graph Generation for Complex Question Answering over Knowledge Base](https://doi.org/10.18653/v1/2021.emnlp-main.346) |  | 0 | Most of the existing Knowledge-based Question Answering (KBQA) methods first learn to map the given question to a query graph, and then convert the graph to an executable query to find the answer. The query graph is typically expanded progressively from the topic entity based on a sequence prediction model. In this paper, we propose a new solution to query graph generation that works in the opposite manner: we start with the entire knowledge base and gradually shrink it to the desired query... | Kechen Qin, Cheng Li, Virgil Pavlu, Javed A. Aslam |  |
| 816 |  |  [DiscoDVT: Generating Long Text with Discourse-Aware Discrete Variational Transformer](https://doi.org/10.18653/v1/2021.emnlp-main.347) |  | 0 | Despite the recent advances in applying pre-trained language models to generate high-quality texts, generating long passages that maintain long-range coherence is yet challenging for these models. In this paper, we propose DiscoDVT, a discourse-aware discrete variational Transformer to tackle the incoherence issue. DiscoDVT learns a discrete variable sequence that summarizes the global structure of the text and then applies it to guide the generation process at each decoding step. To further... | Haozhe Ji, Minlie Huang |  |
| 817 |  |  [Mathematical Word Problem Generation from Commonsense Knowledge Graph and Equations](https://doi.org/10.18653/v1/2021.emnlp-main.348) |  | 0 | There is an increasing interest in the use of mathematical word problem (MWP) generation in educational assessment. Different from standard natural question generation, MWP generation needs to maintain the underlying mathematical operations between quantities and variables, while at the same time ensuring the relevance between the output and the given topic. To address above problem, we develop an end-to-end neural model to generate diverse MWPs in real-world scenarios from commonsense... | Tianqiao Liu, Qiang Fang, Wenbiao Ding, Hang Li, Zhongqin Wu, Zitao Liu |  |
| 818 |  |  [Generic resources are what you need: Style transfer tasks without task-specific parallel training data](https://doi.org/10.18653/v1/2021.emnlp-main.349) |  | 0 | Style transfer aims to rewrite a source text in a different target style while preserving its content. We propose a novel approach to this task that leverages generic resources, and without using any task-specific parallel (source–target) data outperforms existing unsupervised approaches on the two most popular style transfer tasks: formality transfer and polarity swap. In practice, we adopt a multi-step procedure which builds on a generic pre-trained sequence-to-sequence model (BART). First,... | Huiyuan Lai, Antonio Toral, Malvina Nissim |  |
| 819 |  |  [Revisiting Pivot-Based Paraphrase Generation: Language Is Not the Only Optional Pivot](https://doi.org/10.18653/v1/2021.emnlp-main.350) |  | 0 | Paraphrases refer to texts that convey the same meaning with different expression forms. Pivot-based methods, also known as the round-trip translation, have shown promising results in generating high-quality paraphrases. However, existing pivot-based methods all rely on language as the pivot, where large-scale, high-quality parallel bilingual texts are required. In this paper, we explore the feasibility of using semantic and syntactic representations as the pivot for paraphrase generation.... | Yitao Cai, Yue Cao, Xiaojun Wan |  |
| 820 |  |  [Structural Adapters in Pretrained Language Models for AMR-to-Text Generation](https://doi.org/10.18653/v1/2021.emnlp-main.351) |  | 0 | Pretrained language models (PLM) have recently advanced graph-to-text generation, where the input graph is linearized into a sequence and fed into the PLM to obtain its representation. However, efficiently encoding the graph structure in PLMs is challenging because such models were pretrained on natural language, and modeling structured data may lead to catastrophic forgetting of distributional knowledge. In this paper, we propose StructAdapt, an adapter method to encode graph structure into... | Leonardo F. R. Ribeiro, Yue Zhang, Iryna Gurevych |  |
| 821 |  |  [Data-to-text Generation by Splicing Together Nearest Neighbors](https://doi.org/10.18653/v1/2021.emnlp-main.352) |  | 0 | We propose to tackle data-to-text generation tasks by directly splicing together retrieved segments of text from “neighbor” source-target pairs. Unlike recent work that conditions on retrieved neighbors but generates text token-by-token, left-to-right, we learn a policy that directly manipulates segments of neighbor text, by inserting or replacing them in partially constructed generations. Standard techniques for training such a policy require an oracle derivation for each generation, and we... | Sam Wiseman, Arturs Backurs, Karl Stratos |  |
| 822 |  |  [Contextualize Knowledge Bases with Transformer for End-to-end Task-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2021.emnlp-main.353) |  | 0 | Incorporating knowledge bases (KB) into end-to-end task-oriented dialogue systems is challenging, since it requires to properly represent the entity of KB, which is associated with its KB context and dialogue context. The existing works represent the entity with only perceiving a part of its KB context, which can lead to the less effective representation due to the information loss, and adversely favor KB reasoning and response generation. To tackle this issue, we explore to fully contextualize... | Yanjie Gou, Yinjie Lei, Lingqiao Liu, Yong Dai, Chunxu Shen |  |
| 823 |  |  [Efficient Dialogue Complementary Policy Learning via Deep Q-network Policy and Episodic Memory Policy](https://doi.org/10.18653/v1/2021.emnlp-main.354) |  | 0 | Deep reinforcement learning has shown great potential in training dialogue policies. However, its favorable performance comes at the cost of many rounds of interaction. Most of the existing dialogue policy methods rely on a single learning system, while the human brain has two specialized learning and memory systems, supporting to find good solutions without requiring copious examples. Inspired by the human brain, this paper proposes a novel complementary policy learning (CPL) framework, which... | Yangyang Zhao, Zhenyu Wang, Changxi Zhu, Shihan Wang |  |
| 824 |  |  [CRFR: Improving Conversational Recommender Systems via Flexible Fragments Reasoning on Knowledge Graphs](https://doi.org/10.18653/v1/2021.emnlp-main.355) |  | 0 | Although paths of user interests shift in knowledge graphs (KGs) can benefit conversational recommender systems (CRS), explicit reasoning on KGs has not been well considered in CRS, due to the complex of high-order and incomplete paths. We propose CRFR, which effectively does explicit multi-hop reasoning on KGs with a conversational context-based reinforcement learning model. Considering the incompleteness of KGs, instead of learning single complete reasoning path, CRFR flexibly learns multiple... | Jinfeng Zhou, Bo Wang, Ruifang He, Yuexian Hou |  |
| 825 |  |  [DuRecDial 2.0: A Bilingual Parallel Corpus for Conversational Recommendation](https://doi.org/10.18653/v1/2021.emnlp-main.356) |  | 0 | In this paper, we provide a bilingual parallel human-to-human recommendation dialog dataset (DuRecDial 2.0) to enable researchers to explore a challenging task of multilingual and cross-lingual conversational recommendation. The difference between DuRecDial 2.0 and existing conversational recommendation datasets is that the data item (Profile, Goal, Knowledge, Context, Response) in DuRecDial 2.0 is annotated in two languages, both English and Chinese, while other datasets are built with the... | Zeming Liu, Haifeng Wang, Zhengyu Niu, Hua Wu, Wanxiang Che |  |
| 826 |  |  [End-to-End Learning of Flowchart Grounded Task-Oriented Dialogs](https://doi.org/10.18653/v1/2021.emnlp-main.357) |  | 0 | We propose a novel problem within end-to-end learning of task oriented dialogs (TOD), in which the dialog system mimics a troubleshooting agent who helps a user by diagnosing their problem (e.g., car not starting). Such dialogs are grounded in domain-specific flowcharts, which the agent is supposed to follow during the conversation. Our task exposes novel technical challenges for neural TOD, such as grounding an utterance to the flowchart without explicit annotation, referring to additional... | Dinesh Raghu, Shantanu Agarwal, Sachindra Joshi, Mausam |  |
| 827 |  |  [Dimensional Emotion Detection from Categorical Emotion](https://doi.org/10.18653/v1/2021.emnlp-main.358) |  | 0 | We present a model to predict fine-grained emotions along the continuous dimensions of valence, arousal, and dominance (VAD) with a corpus with categorical emotion annotations. Our model is trained by minimizing the EMD (Earth Mover’s Distance) loss between the predicted VAD score distribution and the categorical emotion distributions sorted along VAD, and it can simultaneously classify the emotion categories and predict the VAD scores for a given sentence. We use pre-trained RoBERTa-Large and... | Sungjoon Park, Jiseon Kim, Seonghyeon Ye, Jaeyeol Jeon, Heeyoung Park, Alice Oh |  |
| 828 |  |  [Not All Negatives are Equal: Label-Aware Contrastive Loss for Fine-grained Text Classification](https://doi.org/10.18653/v1/2021.emnlp-main.359) |  | 0 | Fine-grained classification involves dealing with datasets with larger number of classes with subtle differences between them. Guiding the model to focus on differentiating dimensions between these commonly confusable classes is key to improving performance on fine-grained tasks. In this work, we analyse the contrastive fine-tuning of pre-trained language models on two fine-grained text classification tasks, emotion classification and sentiment analysis. We adaptively embed class relationships... | Varsha Suresh, Desmond C. Ong |  |
| 829 |  |  [Joint Multi-modal Aspect-Sentiment Analysis with Auxiliary Cross-modal Relation Detection](https://doi.org/10.18653/v1/2021.emnlp-main.360) |  | 0 | Aspect terms extraction (ATE) and aspect sentiment classification (ASC) are two fundamental and fine-grained sub-tasks in aspect-level sentiment analysis (ALSA). In the textual analysis, joint extracting both aspect terms and sentiment polarities has been drawn much attention due to the better applications than individual sub-task. However, in the multi-modal scenario, the existing studies are limited to handle each sub-task independently, which fails to model the innate connection between the... | Xincheng Ju, Dong Zhang, Rong Xiao, Junhui Li, Shoushan Li, Min Zhang, Guodong Zhou |  |
| 830 |  |  [Solving Aspect Category Sentiment Analysis as a Text Generation Task](https://doi.org/10.18653/v1/2021.emnlp-main.361) |  | 0 | Aspect category sentiment analysis has attracted increasing research attention. The dominant methods make use of pre-trained language models by learning effective aspect category-specific representations, and adding specific output layers to its pre-trained representation. We consider a more direct way of making use of pre-trained language models, by casting the ACSA tasks into natural language generation tasks, using natural language sentences to represent the output. Our method allows more... | Jian Liu, Zhiyang Teng, Leyang Cui, Hanmeng Liu, Yue Zhang |  |
| 831 |  |  [Semantics-Preserved Data Augmentation for Aspect-Based Sentiment Analysis](https://doi.org/10.18653/v1/2021.emnlp-main.362) |  | 0 | Both the issues of data deficiencies and semantic consistency are important for data augmentation. Most of previous methods address the first issue, but ignore the second one. In the cases of aspect-based sentiment analysis, violation of the above issues may change the aspect and sentiment polarity. In this paper, we propose a semantics-preservation data augmentation approach by considering the importance of each word in a textual sequence according to the related aspects and sentiments. We... | TingWei Hsu, ChungChi Chen, HenHsen Huang, HsinHsi Chen |  |
| 832 |  |  [The Effect of Round-Trip Translation on Fairness in Sentiment Analysis](https://doi.org/10.18653/v1/2021.emnlp-main.363) |  | 0 | Sentiment analysis systems have been shown to exhibit sensitivity to protected attributes. Round-trip translation, on the other hand, has been shown to normalize text. We explore the impact of round-trip translation on the demographic parity of sentiment classifiers and show how round-trip translation consistently improves classification fairness at test time (reducing up to 47% of between-group gaps). We also explore the idea of retraining sentiment classifiers on round-trip-translated data. | Jonathan Gabel Christiansen, Mathias Gammelgaard, Anders Søgaard |  |
| 833 |  |  [CHoRaL: Collecting Humor Reaction Labels from Millions of Social Media Users](https://doi.org/10.18653/v1/2021.emnlp-main.364) |  | 0 | Humor detection has gained attention in recent years due to the desire to understand user-generated content with figurative language. However, substantial individual and cultural differences in humor perception make it very difficult to collect a large-scale humor dataset with reliable humor labels. We propose CHoRaL, a framework to generate perceived humor labels on Facebook posts, using the naturally available user reactions to these posts with no manual annotation needed. CHoRaL provides... | Zixiaofan Yang, Shayan Hooshmand, Julia Hirschberg |  |
| 834 |  |  [CSDS: A Fine-Grained Chinese Dataset for Customer Service Dialogue Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.365) |  | 0 | Dialogue summarization has drawn much attention recently. Especially in the customer service domain, agents could use dialogue summaries to help boost their works by quickly knowing customer’s issues and service progress. These applications require summaries to contain the perspective of a single speaker and have a clear topic flow structure, while neither are available in existing datasets. Therefore, in this paper, we introduce a novel Chinese dataset for Customer Service Dialogue... | Haitao Lin, Liqun Ma, Junnan Zhu, Lu Xiang, Yu Zhou, Jiajun Zhang, Chengqing Zong |  |
| 835 |  |  [CodRED: A Cross-Document Relation Extraction Dataset for Acquiring Knowledge in the Wild](https://doi.org/10.18653/v1/2021.emnlp-main.366) |  | 0 | Existing relation extraction (RE) methods typically focus on extracting relational facts between entity pairs within single sentences or documents. However, a large quantity of relational facts in knowledge bases can only be inferred across documents in practice. In this work, we present the problem of cross-document RE, making an initial step towards knowledge acquisition in the wild. To facilitate the research, we construct the first human-annotated cross-document RE dataset CodRED. Compared... | Yuan Yao, Jiaju Du, Yankai Lin, Peng Li, Zhiyuan Liu, Jie Zhou, Maosong Sun |  |
| 836 |  |  [Building and Evaluating Open-Domain Dialogue Corpora with Clarifying Questions](https://doi.org/10.18653/v1/2021.emnlp-main.367) |  | 0 | Enabling open-domain dialogue systems to ask clarifying questions when appropriate is an important direction for improving the quality of the system response. Namely, for cases when a user request is not specific enough for a conversation system to provide an answer right away, it is desirable to ask a clarifying question to increase the chances of retrieving a satisfying answer. To address the problem of ‘asking clarifying questions in open-domain dialogues’: (1) we collect and release a new... | Mohammad Aliannejadi, Julia Kiseleva, Aleksandr Chuklin, Jeff Dalton, Mikhail Burtsev |  |
| 837 |  |  [We Need to Talk About train-dev-test Splits](https://doi.org/10.18653/v1/2021.emnlp-main.368) |  | 0 | Standard train-dev-test splits used to benchmark multiple models against each other are ubiquitously used in Natural Language Processing (NLP). In this setup, the train data is used for training the model, the development set for evaluating different versions of the proposed model(s) during development, and the test set to confirm the answers to the main research question(s). However, the introduction of neural networks in NLP has led to a different use of these standard splits; the development... | Rob van der Goot |  |
| 838 |  |  [PhoMT: A High-Quality and Large-Scale Benchmark Dataset for Vietnamese-English Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.369) |  | 0 | We introduce a high-quality and large-scale Vietnamese-English parallel dataset of 3.02M sentence pairs, which is 2.9M pairs larger than the benchmark Vietnamese-English machine translation corpus IWSLT15. We conduct experiments comparing strong neural baselines and well-known automatic translation engines on our dataset and find that in both automatic and human evaluations: the best performance is obtained by fine-tuning the pre-trained sequence-to-sequence denoising auto-encoder mBART. To our... | Long Doan, Linh The Nguyen, Nguyen Luong Tran, Thai Hoang, Dat Quoc Nguyen |  |
| 839 |  |  [Lying Through One's Teeth: A Study on Verbal Leakage Cues](https://doi.org/10.18653/v1/2021.emnlp-main.370) |  | 0 | Although many studies use the LIWC lexicon to show the existence of verbal leakage cues in lie detection datasets, none mention how verbal leakage cues are influenced by means of data collection, or the impact thereof on the performance of models. In this paper, we study verbal leakage cues to understand the effect of the data construction method on their significance, and examine the relationship between such cues and models’ validity. The LIWC word-category dominance scores of seven lie... | MinHsuan Yeh, LunWei Ku |  |
| 840 |  |  [Multi-granularity Textual Adversarial Attack with Behavior Cloning](https://doi.org/10.18653/v1/2021.emnlp-main.371) |  | 0 | Recently, the textual adversarial attack models become increasingly popular due to their successful in estimating the robustness of NLP models. However, existing works have obvious deficiencies. (1)They usually consider only a single granularity of modification strategies (e.g. word-level or sentence-level), which is insufficient to explore the holistic textual space for generation; (2) They need to query victim models hundreds of times to make a successful attack, which is highly inefficient... | Yangyi Chen, Jin Su, Wei Wei |  |
| 841 |  |  [All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality](https://doi.org/10.18653/v1/2021.emnlp-main.372) |  | 0 | Similarity measures are a vital tool for understanding how language models represent and process language. Standard representational similarity measures such as cosine similarity and Euclidean distance have been successfully used in static word embedding models to understand how words cluster in semantic space. Recently, these measures have been applied to embeddings from contextualized models such as BERT and GPT-2. In this work, we call into question the informativity of such measures for... | William Timkey, Marten van Schijndel |  |
| 842 |  |  [Incorporating Residual and Normalization Layers into Analysis of Masked Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.373) |  | 0 | Transformer architecture has become ubiquitous in the natural language processing field. To interpret the Transformer-based models, their attention patterns have been extensively analyzed. However, the Transformer architecture is not only composed of the multi-head attention; other components can also contribute to Transformers’ progressive performance. In this study, we extended the scope of the analysis of Transformers from solely the attention patterns to the whole attention block, i.e.,... | Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, Kentaro Inui |  |
| 843 |  |  [Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer](https://doi.org/10.18653/v1/2021.emnlp-main.374) |  | 0 | Adversarial attacks and backdoor attacks are two common security threats that hang over deep learning. Both of them harness task-irrelevant features of data in their implementation. Text style is a feature that is naturally irrelevant to most NLP tasks, and thus suitable for adversarial and backdoor attacks. In this paper, we make the first attempt to conduct adversarial and backdoor attacks based on text style transfer, which is aimed at altering the style of a sentence while preserving its... | Fanchao Qi, Yangyi Chen, Xurui Zhang, Mukai Li, Zhiyuan Liu, Maosong Sun |  |
| 844 |  |  [Sociolectal Analysis of Pretrained Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.375) |  | 0 | Using data from English cloze tests, in which subjects also self-reported their gender, age, education, and race, we examine performance differences of pretrained language models across demographic groups, defined by these (protected) attributes. We demonstrate wide performance gaps across demographic groups and show that pretrained language models systematically disfavor young non-white male speakers; i.e., not only do pretrained language models learn social biases (stereotypical associations)... | Sheng Zhang, Xin Zhang, Weiming Zhang, Anders Søgaard |  |
| 845 |  |  [Examining Cross-lingual Contextual Embeddings with Orthogonal Structural Probes](https://doi.org/10.18653/v1/2021.emnlp-main.376) |  | 0 | State-of-the-art contextual embeddings are obtained from large language models available only for a few languages. For others, we need to learn representations using a multilingual model. There is an ongoing debate on whether multilingual embeddings can be aligned in a space shared across many languages. The novel Orthogonal Structural Probe (Limisiewicz and Mareček, 2021) allows us to answer this question for specific linguistic features and learn a projection based only on mono-lingual... | Tomasz Limisiewicz, David Marecek |  |
| 846 |  |  [Are Transformers a Modern Version of ELIZA? Observations on French Object Verb Agreement](https://doi.org/10.18653/v1/2021.emnlp-main.377) |  | 0 | Many recent works have demonstrated that unsupervised sentence representations of neural networks encode syntactic information by observing that neural language models are able to predict the agreement between a verb and its subject. We take a critical look at this line of research by showing that it is possible to achieve high accuracy on this agreement task with simple surface heuristics, indicating a possible flaw in our assessment of neural networks’ syntactic ability. Our fine-grained... | Bingzhi Li, Guillaume Wisniewski, Benoît Crabbé |  |
| 847 |  |  [Fine-grained Entity Typing via Label Reasoning](https://doi.org/10.18653/v1/2021.emnlp-main.378) |  | 0 | Conventional entity typing approaches are based on independent classification paradigms, which make them difficult to recognize inter-dependent, long-tailed and fine-grained entity types. In this paper, we argue that the implicitly entailed extrinsic and intrinsic dependencies between labels can provide critical knowledge to tackle the above challenges. To this end, we propose Label Reasoning Network(LRN), which sequentially reasons fine-grained entity labels by discovering and exploiting label... | Qing Liu, Hongyu Lin, Xinyan Xiao, Xianpei Han, Le Sun, Hua Wu |  |
| 848 |  |  [Enhanced Language Representation with Label Knowledge for Span Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.379) |  | 0 | Span extraction, aiming to extract text spans (such as words or phrases) from plain text, is a fundamental process in Information Extraction. Recent works introduce the label knowledge to enhance the text representation by formalizing the span extraction task into a question answering problem (QA Formalization), which achieves state-of-the-art performance. However, such a QA Formalization does not fully exploit the label knowledge and causes a dramatic decrease in efficiency of... | Pan Yang, Xin Cong, Zhenyu Sun, Xingwu Liu |  |
| 849 |  |  [PRIDE: Predicting Relationships in Conversations](https://doi.org/10.18653/v1/2021.emnlp-main.380) |  | 0 | Automatically extracting interpersonal relationships of conversation interlocutors can enrich personal knowledge bases to enhance personalized search, recommenders and chatbots. To infer speakers’ relationships from dialogues we propose PRIDE, a neural multi-label classifier, based on BERT and Transformer for creating a conversation representation. PRIDE utilizes dialogue structure and augments it with external knowledge about speaker features and conversation style. Unlike prior works, we... | Anna Tigunova, Paramita Mirza, Andrew Yates, Gerhard Weikum |  |
| 850 |  |  [Extracting Fine-Grained Knowledge Graphs of Scientific Claims: Dataset and Transformer-Based Results](https://doi.org/10.18653/v1/2021.emnlp-main.381) |  | 0 | Recent transformer-based approaches demonstrate promising results on relational scientific information extraction. Existing datasets focus on high-level description of how research is carried out. Instead we focus on the subtleties of how experimental associations are presented by building SciClaim, a dataset of scientific claims drawn from Social and Behavior Science (SBS), PubMed, and CORD-19 papers. Our novel graph annotation schema incorporates not only coarse-grained entity spans as nodes... | Ian H. Magnusson, Scott E. Friedman |  |
| 851 |  |  [Sequential Cross-Document Coreference Resolution](https://doi.org/10.18653/v1/2021.emnlp-main.382) |  | 0 | Relating entities and events in text is a key component of natural language understanding. Cross-document coreference resolution, in particular, is important for the growing interest in multi-document analysis tasks. In this work we propose a new model that extends the efficient sequential prediction paradigm for coreference resolution to cross-document settings and achieves competitive results for both entity and event coreference while providing strong evidence of the efficacy of both... | Emily Allaway, Shuai Wang, Miguel Ballesteros |  |
| 852 |  |  [Mixture-of-Partitions: Infusing Large Biomedical Knowledge Graphs into BERT](https://doi.org/10.18653/v1/2021.emnlp-main.383) |  | 0 | Infusing factual knowledge into pre-trained models is fundamental for many knowledge-intensive tasks. In this paper, we proposed Mixture-of-Partitions (MoP), an infusion approach that can handle a very large knowledge graph (KG) by partitioning it into smaller sub-graphs and infusing their specific knowledge into various BERT models using lightweight adapters. To leverage the overall factual knowledge for a target task, these sub-graph adapters are further fine-tuned along with the underlying... | Zaiqiao Meng, Fangyu Liu, Thomas Hikaru Clark, Ehsan Shareghi, Nigel Collier |  |
| 853 |  |  [Filling the Gaps in Ancient Akkadian Texts: A Masked Language Modelling Approach](https://doi.org/10.18653/v1/2021.emnlp-main.384) |  | 0 | We present models which complete missing text given transliterations of ancient Mesopotamian documents, originally written on cuneiform clay tablets (2500 BCE - 100 CE). Due to the tablets’ deterioration, scholars often rely on contextual cues to manually fill in missing parts in the text in a subjective and time-consuming process. We identify that this challenge can be formulated as a masked language modelling task, used mostly as a pretraining objective for contextualized language models.... | Koren Lazar, Benny Saret, Asaf Yehudai, Wayne Horowitz, Nathan Wasserman, Gabriel Stanovsky |  |
| 854 |  |  [AVocaDo: Strategy for Adapting Vocabulary to Downstream Domain](https://doi.org/10.18653/v1/2021.emnlp-main.385) |  | 0 | During the fine-tuning phase of transfer learning, the pretrained vocabulary remains unchanged, while model parameters are updated. The vocabulary generated based on the pretrained data is suboptimal for downstream data when domain discrepancy exists. We propose to consider the vocabulary as an optimizable parameter, allowing us to update the vocabulary by expanding it with domain specific vocabulary based on a tokenization statistic. Furthermore, we preserve the embeddings of the added words... | Jimin Hong, Taehee Kim, Hyesu Lim, Jaegul Choo |  |
| 855 |  |  [Can We Improve Model Robustness through Secondary Attribute Counterfactuals?](https://doi.org/10.18653/v1/2021.emnlp-main.386) |  | 0 | Developing robust NLP models that perform well on many, even small, slices of data is a significant but important challenge, with implications from fairness to general reliability. To this end, recent research has explored how models rely on spurious correlations, and how counterfactual data augmentation (CDA) can mitigate such issues. In this paper we study how and why modeling counterfactuals over multiple attributes can go significantly further in improving model performance. We propose RDI,... | Ananth Balashankar, Xuezhi Wang, Ben Packer, Nithum Thain, Ed H. Chi, Alex Beutel |  |
| 856 |  |  [Long-Range Modeling of Source Code Files with eWASH: Extended Window Access by Syntax Hierarchy](https://doi.org/10.18653/v1/2021.emnlp-main.387) |  | 0 | Statistical language modeling and translation with transformers have found many successful applications in program understanding and generation tasks, setting high benchmarks for tools in modern software development environments. The finite context window of these neural models means, however, that they will be unable to leverage the entire relevant context of large files and packages for any given task. While there are many efforts to extend the context window, we introduce an... | Colin B. Clement, Shuai Lu, Xiaoyu Liu, Michele Tufano, Dawn Drain, Nan Duan, Neel Sundaresan, Alexey Svyatkovskiy |  |
| 857 |  |  [Can Language Models be Biomedical Knowledge Bases?](https://doi.org/10.18653/v1/2021.emnlp-main.388) |  | 0 | Pre-trained language models (LMs) have become ubiquitous in solving various natural language processing (NLP) tasks. There has been increasing interest in what knowledge these LMs contain and how we can extract that knowledge, treating LMs as knowledge bases (KBs). While there has been much work on probing LMs in the general domain, there has been little attention to whether these powerful LMs can be used as domain-specific KBs. To this end, we create the BioLAMA benchmark, which is comprised... | Mujeen Sung, Jinhyuk Lee, Sean S. Yi, Minji Jeon, Sungdong Kim, Jaewoo Kang |  |
| 858 |  |  [LayoutReader: Pre-training of Text and Layout for Reading Order Detection](https://doi.org/10.18653/v1/2021.emnlp-main.389) |  | 0 | Reading order detection is the cornerstone to understanding visually-rich documents (e.g., receipts and forms). Unfortunately, no existing work took advantage of advanced deep learning models because it is too laborious to annotate a large enough dataset. We observe that the reading order of WORD documents is embedded in their XML metadata; meanwhile, it is easy to convert WORD documents to PDFs or images. Therefore, in an automated manner, we construct ReadingBank, a benchmark dataset that... | Zilong Wang, Yiheng Xu, Lei Cui, Jingbo Shang, Furu Wei |  |
| 859 |  |  [Region under Discussion for visual dialog](https://doi.org/10.18653/v1/2021.emnlp-main.390) |  | 0 | Visual Dialog is assumed to require the dialog history to generate correct responses during a dialog. However, it is not clear from previous work how dialog history is needed for visual dialog. In this paper we define what it means for a visual question to require dialog history and we release a subset of the Guesswhat?! questions for which their dialog history completely changes their responses. We propose a novel interpretable representation that visually grounds dialog history: the Region... | Mauricio Mazuecos, Franco M. Luque, Jorge Sánchez, Hernán Maina, Thomas Vadora, Luciana Benotti |  |
| 860 |  |  [Learning grounded word meaning representations on similarity graphs](https://doi.org/10.18653/v1/2021.emnlp-main.391) |  | 0 | This paper introduces a novel approach to learn visually grounded meaning representations of words as low-dimensional node embeddings on an underlying graph hierarchy. The lower level of the hierarchy models modality-specific word representations, conditioned to another modality, through dedicated but communicating graphs, while the higher level puts these representations together on a single graph to learn a representation jointly from both modalities. The topology of each graph models... | Mariella Dimiccoli, Herwig Wendt, Pau Batlle Franch |  |
| 861 |  |  [WhyAct: Identifying Action Reasons in Lifestyle Vlogs](https://doi.org/10.18653/v1/2021.emnlp-main.392) |  | 0 | We aim to automatically identify human action reasons in online videos. We focus on the widespread genre of lifestyle vlogs, in which people perform actions while verbally describing them. We introduce and make publicly available the WhyAct dataset, consisting of 1,077 visual actions manually annotated with their reasons. We describe a multimodal model that leverages visual and textual information to automatically infer the reasons corresponding to an action presented in the video. | Oana Ignat, Santiago Castro, Hanwen Miao, Weiji Li, Rada Mihalcea |  |
| 862 |  |  [Genre as Weak Supervision for Cross-lingual Dependency Parsing](https://doi.org/10.18653/v1/2021.emnlp-main.393) |  | 0 | Recent work has shown that monolingual masked language models learn to represent data-driven notions of language variation which can be used for domain-targeted training data selection. Dataset genre labels are already frequently available, yet remain largely unexplored in cross-lingual setups. We harness this genre metadata as a weak supervision signal for targeted data selection in zero-shot dependency parsing. Specifically, we project treebank-level genre information to the finer-grained... | Max MüllerEberstein, Rob van der Goot, Barbara Plank |  |
| 863 |  |  [On the Relation between Syntactic Divergence and Zero-Shot Performance](https://doi.org/10.18653/v1/2021.emnlp-main.394) |  | 0 | We explore the link between the extent to which syntactic relations are preserved in translation and the ease of correctly constructing a parse tree in a zero-shot setting. While previous work suggests such a relation, it tends to focus on the macro level and not on the level of individual edges—a gap we aim to address. As a test case, we take the transfer of Universal Dependencies (UD) parsing from English to a diverse set of languages and conduct two sets of experiments. In one, we analyze... | Ofir Arviv, Dmitry Nikolaev, Taelin Karidi, Omri Abend |  |
| 864 |  |  [Improved Latent Tree Induction with Distant Supervision via Span Constraints](https://doi.org/10.18653/v1/2021.emnlp-main.395) |  | 0 | For over thirty years, researchers have developed and analyzed methods for latent tree induction as an approach for unsupervised syntactic parsing. Nonetheless, modern systems still do not perform well enough compared to their supervised counterparts to have any practical use as structural annotation of text. In this work, we present a technique that uses distant supervision in the form of span constraints (i.e. phrase bracketing) to improve performance in unsupervised constituency parsing.... | Zhiyang Xu, Andrew Drozdov, JayYoon Lee, Tim O'Gorman, Subendhu Rongali, Dylan Finkbeiner, Shilpa Suresh, Mohit Iyyer, Andrew McCallum |  |
| 865 |  |  [Aligning Multidimensional Worldviews and Discovering Ideological Differences](https://doi.org/10.18653/v1/2021.emnlp-main.396) |  | 0 | The Internet is home to thousands of communities, each with their own unique worldview and associated ideological differences. With new communities constantly emerging and serving as ideological birthplaces, battlegrounds, and bunkers, it is critical to develop a framework for understanding worldviews and ideological distinction. Most existing work, however, takes a predetermined view based on political polarization: the “right vs. left” dichotomy of U.S. politics. In reality, both political... | Jeremiah Milbauer, Adarsh Mathew, James Evans |  |
| 866 |  |  [Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts](https://doi.org/10.18653/v1/2021.emnlp-main.397) |  | 0 | Dialogue models trained on human conversations inadvertently learn to generate toxic responses. In addition to producing explicitly offensive utterances, these models can also implicitly insult a group or individual by aligning themselves with an offensive statement. To better understand the dynamics of contextually offensive language, we investigate the stance of dialogue model responses in offensive Reddit conversations. Specifically, we create ToxiChat, a crowd-annotated dataset of 2,000... | Ashutosh Baheti, Maarten Sap, Alan Ritter, Mark O. Riedl |  |
| 867 |  |  [Multi-Modal Open-Domain Dialogue](https://doi.org/10.18653/v1/2021.emnlp-main.398) |  | 0 | Recent work in open-domain conversational agents has demonstrated that significant improvements in humanness and user preference can be achieved via massive scaling in both pre-training data and model size (Adiwardana et al., 2020; Roller et al., 2020). However, if we want to build agents with human-like abilities, we must expand beyond handling just text. A particularly important topic is the ability to see images and communicate about what is perceived. With the goal of getting humans to... | Kurt Shuster, Eric Michael Smith, Da Ju, Jason Weston |  |
| 868 |  |  [A Label-Aware BERT Attention Network for Zero-Shot Multi-Intent Detection in Spoken Language Understanding](https://doi.org/10.18653/v1/2021.emnlp-main.399) |  | 0 | With the early success of query-answer assistants such as Alexa and Siri, research attempts to expand system capabilities of handling service automation are now abundant. However, preliminary systems have quickly found the inadequacy in relying on simple classification techniques to effectively accomplish the automation task. The main challenge is that the dialogue often involves complexity in user’s intents (or purposes) which are multiproned, subject to spontaneous change, and difficult to... | TingWei Wu, Ruolin Su, BiingHwang Juang |  |
| 869 |  |  [Zero-Shot Dialogue Disentanglement by Self-Supervised Entangled Response Selection](https://doi.org/10.18653/v1/2021.emnlp-main.400) |  | 0 | Dialogue disentanglement aims to group utterances in a long and multi-participant dialogue into threads. This is useful for discourse analysis and downstream applications such as dialogue response selection, where it can be the first step to construct a clean context/response set. Unfortunately, labeling all reply-to links takes quadratic effort w.r.t the number of utterances: an annotator must check all preceding utterances to identify the one to which the current utterance is a reply. In this... | TaChung Chi, Alexander I. Rudnicky |  |
| 870 |  |  [SIMMC 2.0: A Task-oriented Dialog Dataset for Immersive Multimodal Conversations](https://doi.org/10.18653/v1/2021.emnlp-main.401) |  | 0 | Next generation task-oriented dialog systems need to understand conversational contexts with their perceived surroundings, to effectively help users in the real-world multimodal environment. Existing task-oriented dialog datasets aimed towards virtual assistance fall short and do not situate the dialog in the user’s multimodal context. To overcome, we present a new dataset for Situated and Interactive Multimodal Conversations, SIMMC 2.0, which includes 11K task-oriented user<->assistant dialogs... | Satwik Kottur, Seungwhan Moon, Alborz Geramifard, Babak Damavandi |  |
| 871 |  |  [RAST: Domain-Robust Dialogue Rewriting as Sequence Tagging](https://doi.org/10.18653/v1/2021.emnlp-main.402) |  | 0 | The task of dialogue rewriting aims to reconstruct the latest dialogue utterance by copying the missing content from the dialogue context. Until now, the existing models for this task suffer from the robustness issue, i.e., performances drop dramatically when testing on a different dataset. We address this robustness issue by proposing a novel sequence-tagging-based model so that the search space is significantly reduced, yet the core of this task is still well covered. As a common issue of... | Jie Hao, Linfeng Song, Liwei Wang, Kun Xu, Zhaopeng Tu, Dong Yu |  |
| 872 |  |  [MRF-Chat: Improving Dialogue with Markov Random Fields](https://doi.org/10.18653/v1/2021.emnlp-main.403) |  | 0 | Recent state-of-the-art approaches in open-domain dialogue include training end-to-end deep-learning models to learn various conversational features like emotional content of response, symbolic transitions of dialogue contexts in a knowledge graph and persona of the agent and the user, among others. While neural models have shown reasonable results, modelling the cognitive processes that humans use when conversing with each other may improve the agent’s quality of responses. A key element of... | Ishaan Grover, Matthew Huggins, Cynthia Breazeal, Hae Won Park |  |
| 873 |  |  [Dialogue State Tracking with a Language Model using Schema-Driven Prompting](https://doi.org/10.18653/v1/2021.emnlp-main.404) |  | 0 | Task-oriented conversational systems often use dialogue state tracking to represent the user’s intentions, which involves filling in values of pre-defined slots. Many approaches have been proposed, often using task-specific architectures with special-purpose classifiers. Recently, good results have been obtained using more general architectures based on pretrained language models. Here, we introduce a new variation of the language modeling approach that uses schema-driven prompting to provide... | ChiaHsuan Lee, Hao Cheng, Mari Ostendorf |  |
| 874 |  |  [Signed Coreference Resolution](https://doi.org/10.18653/v1/2021.emnlp-main.405) |  | 0 | Coreference resolution is key to many natural language processing tasks and yet has been relatively unexplored in Sign Language Processing. In signed languages, space is primarily used to establish reference. Solving coreference resolution for signed languages would not only enable higher-level Sign Language Processing systems, but also enhance our understanding of language in different modalities and of situated references, which are key problems in studying grounded language. In this paper,... | Kayo Yin, Kenneth DeHaan, Malihe Alikhani |  |
| 875 |  |  [Consistent Accelerated Inference via Confident Adaptive Transformers](https://doi.org/10.18653/v1/2021.emnlp-main.406) |  | 0 | We develop a novel approach for confidently accelerating inference in the large and expensive multilayer Transformers that are now ubiquitous in natural language processing (NLP). Amortized or approximate computational methods increase efficiency, but can come with unpredictable performance costs. In this work, we present CATs – Confident Adaptive Transformers – in which we simultaneously increase computational efficiency, while guaranteeing a specifiable degree of consistency with the original... | Tal Schuster, Adam Fisch, Tommi S. Jaakkola, Regina Barzilay |  |
| 876 |  |  [Improving and Simplifying Pattern Exploiting Training](https://doi.org/10.18653/v1/2021.emnlp-main.407) |  | 0 | Recently, pre-trained language models (LMs) have achieved strong performance when fine-tuned on difficult benchmarks like SuperGLUE. However, performance can suffer when there are very few labeled examples available for fine-tuning. Pattern Exploiting Training (PET) is a recent approach that leverages patterns for few-shot learning. However, PET uses task-specific unlabeled data. In this paper, we focus on few-shot learning without any unlabeled data and introduce ADAPET, which modifies PET’s... | Derek Tam, Rakesh R. Menon, Mohit Bansal, Shashank Srivastava, Colin Raffel |  |
| 877 |  |  [Unsupervised Data Augmentation with Naive Augmentation and without Unlabeled Data](https://doi.org/10.18653/v1/2021.emnlp-main.408) |  | 0 | Unsupervised Data Augmentation (UDA) is a semisupervised technique that applies a consistency loss to penalize differences between a model’s predictions on (a) observed (unlabeled) examples; and (b) corresponding ‘noised’ examples produced via data augmentation. While UDA has gained popularity for text classification, open questions linger over which design decisions are necessary and how to extend the method to sequence labeling tasks. In this paper, we re-examine UDA and demonstrate its... | David Lowell, Brian E. Howard, Zachary C. Lipton, Byron C. Wallace |  |
| 878 |  |  [Pre-train or Annotate? Domain Adaptation with a Constrained Budget](https://doi.org/10.18653/v1/2021.emnlp-main.409) |  | 0 | Recent work has demonstrated that pre-training in-domain language models can boost performance when adapting to a new domain. However, the costs associated with pre-training raise an important question: given a fixed budget, what steps should an NLP practitioner take to maximize performance? In this paper, we study domain adaptation under budget constraints, and approach it as a customer choice problem between data annotation and pre-training. Specifically, we measure the annotation cost of... | Fan Bai, Alan Ritter, Wei Xu |  |
| 879 |  |  [Lawyers are Dishonest? Quantifying Representational Harms in Commonsense Knowledge Resources](https://doi.org/10.18653/v1/2021.emnlp-main.410) |  | 0 | Warning: this paper contains content that may be offensive or upsetting. Commonsense knowledge bases (CSKB) are increasingly used for various natural language processing tasks. Since CSKBs are mostly human-generated and may reflect societal biases, it is important to ensure that such biases are not conflated with the notion of commonsense. Here we focus on two widely used CSKBs, ConceptNet and GenericsKB, and establish the presence of bias in the form of two types of representational harms,... | Ninareh Mehrabi, Pei Zhou, Fred Morstatter, Jay Pujara, Xiang Ren, Aram Galstyan |  |
| 880 |  |  [OSCaR: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings](https://doi.org/10.18653/v1/2021.emnlp-main.411) |  | 0 | Language representations are known to carry stereotypical biases and, as a result, lead to biased predictions in downstream tasks. While existing methods are effective at mitigating biases by linear projection, such methods are too aggressive: they not only remove bias, but also erase valuable information from word embeddings. We develop new measures for evaluating specific information retention that demonstrate the tradeoff between bias removal and information retention. To address this... | Sunipa Dev, Tao Li, Jeff M. Phillips, Vivek Srikumar |  |
| 881 |  |  [Sentence-Permuted Paragraph Generation](https://doi.org/10.18653/v1/2021.emnlp-main.412) |  | 0 | Generating paragraphs of diverse contents is important in many applications. Existing generation models produce similar contents from homogenized contexts due to the fixed left-to-right sentence order. Our idea is permuting the sentence orders to improve the content diversity of multi-sentence paragraph. We propose a novel framework PermGen whose objective is to maximize the expected log-likelihood of output paragraph distributions with respect to all possible sentence orders. PermGen uses... | Wenhao Yu, Chenguang Zhu, Tong Zhao, Zhichun Guo, Meng Jiang |  |
| 882 |  |  [Extract, Denoise and Enforce: Evaluating and Improving Concept Preservation for Text-to-Text Generation](https://doi.org/10.18653/v1/2021.emnlp-main.413) |  | 0 | Prior studies on text-to-text generation typically assume that the model could figure out what to attend to in the input and what to include in the output via seq2seq learning, with only the parallel training data and no additional guidance. However, it remains unclear whether current models can preserve important concepts in the source input, as seq2seq learning does not have explicit focus on the concepts and commonly used evaluation metrics also treat them equally important as other tokens.... | Yuning Mao, Wenchang Ma, Deren Lei, Jiawei Han, Xiang Ren |  |
| 883 |  |  [Paraphrase Generation: A Survey of the State of the Art](https://doi.org/10.18653/v1/2021.emnlp-main.414) |  | 0 | This paper focuses on paraphrase generation,which is a widely studied natural language generation task in NLP. With the development of neural models, paraphrase generation research has exhibited a gradual shift to neural methods in the recent years. This has provided architectures for contextualized representation of an input text and generating fluent, diverseand human-like paraphrases. This paper surveys various approaches to paraphrase generation with a main focus on neural methods. | Jianing Zhou, Suma Bhat |  |
| 884 |  |  [Exposure Bias versus Self-Recovery: Are Distortions Really Incremental for Autoregressive Text Generation?](https://doi.org/10.18653/v1/2021.emnlp-main.415) |  | 0 | Exposure bias has been regarded as a central problem for auto-regressive language models (LM). It claims that teacher forcing would cause the test-time generation to be incrementally distorted due to the training-generation discrepancy. Although a lot of algorithms have been proposed to avoid teacher forcing and therefore alleviate exposure bias, there is little work showing how serious the exposure bias problem actually is. In this work, we focus on the task of open-ended language generation,... | Tianxing He, Jingzhao Zhang, Zhiming Zhou, James R. Glass |  |
| 885 |  |  [Generating Self-Contained and Summary-Centric Question Answer Pairs via Differentiable Reward Imitation Learning](https://doi.org/10.18653/v1/2021.emnlp-main.416) |  | 0 | Motivated by suggested question generation in conversational news recommendation systems, we propose a model for generating question-answer pairs (QA pairs) with self-contained, summary-centric questions and length-constrained, article-summarizing answers. We begin by collecting a new dataset of news articles with questions as titles and pairing them with summaries of varying length. This dataset is used to learn a QA pair generation model producing summaries as answers that balance brevity... | Li Zhou, Kevin Small, Yong Zhang, Sandeep Atluri |  |
| 886 |  |  [Unsupervised Paraphrasing with Pretrained Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.417) |  | 0 | Paraphrase generation has benefited extensively from recent progress in the designing of training objectives and model architectures. However, previous explorations have largely focused on supervised methods, which require a large amount of labeled data that is costly to collect. To address this drawback, we adopt a transfer learning approach and propose a training pipeline that enables pre-trained language models to generate high-quality paraphrases in an unsupervised setting. Our recipe... | Tong Niu, Semih Yavuz, Yingbo Zhou, Nitish Shirish Keskar, Huan Wang, Caiming Xiong |  |
| 887 |  |  [Profanity-Avoiding Training Framework for Seq2seq Models with Certified Robustness](https://doi.org/10.18653/v1/2021.emnlp-main.418) |  | 0 | Seq2seq models have demonstrated their incredible effectiveness in a large variety of applications. However, recent research has shown that inappropriate language in training samples and well-designed testing cases can induce seq2seq models to output profanity. These outputs may potentially hurt the usability of seq2seq models and make the end-users feel offended. To address this problem, we propose a training framework with certified robustness to eliminate the causes that trigger the... | Hengtong Zhang, Tianhang Zheng, Yaliang Li, Jing Gao, Lu Su, Bo Li |  |
| 888 |  |  [Journalistic Guidelines Aware News Image Captioning](https://doi.org/10.18653/v1/2021.emnlp-main.419) |  | 0 | The task of news article image captioning aims to generate descriptive and informative captions for news article images. Unlike conventional image captions that simply describe the content of the image in general terms, news image captions follow journalistic guidelines and rely heavily on named entities to describe the image content, often drawing context from the whole article they are associated with. In this work, we propose a new approach to this task, motivated by caption guidelines that... | Xuewen Yang, Svebor Karaman, Joel R. Tetreault, Alejandro Jaimes |  |
| 889 |  |  [AESOP: Paraphrase Generation with Adaptive Syntactic Control](https://doi.org/10.18653/v1/2021.emnlp-main.420) |  | 0 | We propose to control paraphrase generation through carefully chosen target syntactic structures to generate more proper and higher quality paraphrases. Our model, AESOP, leverages a pretrained language model and adds deliberately chosen syntactical control via a retrieval-based selection module to generate fluent paraphrases. Experiments show that AESOP achieves state-of-the-art performances on semantic preservation and syntactic conformation on two benchmark datasets with ground-truth... | Jiao Sun, Xuezhe Ma, Nanyun Peng |  |
| 890 |  |  [Refocusing on Relevance: Personalization in NLG](https://doi.org/10.18653/v1/2021.emnlp-main.421) |  | 0 | Many NLG tasks such as summarization, dialogue response, or open domain question answering, focus primarily on a source text in order to generate a target response. This standard approach falls short, however, when a user’s intent or context of work is not easily recoverable based solely on that source text– a scenario that we argue is more of the rule than the exception. In this work, we argue that NLG systems in general should place a much higher level of emphasis on making use of additional... | Shiran Dudy, Steven Bedrick, Bonnie Webber |  |
| 891 |  |  [The Future is not One-dimensional: Complex Event Schema Induction by Graph Modeling for Event Prediction](https://doi.org/10.18653/v1/2021.emnlp-main.422) |  | 0 | Event schemas encode knowledge of stereotypical structures of events and their connections. As events unfold, schemas are crucial to act as a scaffolding. Previous work on event schema induction focuses either on atomic events or linear temporal event sequences, ignoring the interplay between events via arguments and argument relations. We introduce a new concept of Temporal Complex Event Schema: a graph-based schema representation that encompasses events, arguments, temporal connections and... | Manling Li, Sha Li, Zhenhailong Wang, Lifu Huang, Kyunghyun Cho, Heng Ji, Jiawei Han, Clare R. Voss |  |
| 892 |  |  [Learning Constraints and Descriptive Segmentation for Subevent Detection](https://doi.org/10.18653/v1/2021.emnlp-main.423) |  | 0 | Event mentions in text correspond to real-world events of varying degrees of granularity. The task of subevent detection aims to resolve this granularity issue, recognizing the membership of multi-granular events in event complexes. Since knowing the span of descriptive contexts of event complexes helps infer the membership of events, we propose the task of event-based text segmentation (EventSeg) as an auxiliary task to improve the learning for subevent detection. To bridge the two tasks... | Haoyu Wang, Hongming Zhang, Muhao Chen, Dan Roth |  |
| 893 |  |  [ChemNER: Fine-Grained Chemistry Named Entity Recognition with Ontology-Guided Distant Supervision](https://doi.org/10.18653/v1/2021.emnlp-main.424) |  | 0 | Scientific literature analysis needs fine-grained named entity recognition (NER) to provide a wide range of information for scientific discovery. For example, chemistry research needs to study dozens to hundreds of distinct, fine-grained entity types, making consistent and accurate annotation difficult even for crowds of domain experts. On the other hand, domain-specific ontologies and knowledge bases (KBs) can be easily accessed, constructed, or integrated, which makes distant supervision... | Xuan Wang, Vivian Hu, Xiangchen Song, Shweta Garg, Jinfeng Xiao, Jiawei Han |  |
| 894 |  |  [Moving on from OntoNotes: Coreference Resolution Model Transfer](https://doi.org/10.18653/v1/2021.emnlp-main.425) |  | 0 | Academic neural models for coreference resolution (coref) are typically trained on a single dataset, OntoNotes, and model improvements are benchmarked on that same dataset. However, real-world applications of coref depend on the annotation guidelines and the domain of the target dataset, which often differ from those of OntoNotes. We aim to quantify transferability of coref models based on the number of annotated documents available in the target dataset. We examine eleven target datasets and... | Patrick Xia, Benjamin Van Durme |  |
| 895 |  |  [Document-level Entity-based Extraction as Template Generation](https://doi.org/10.18653/v1/2021.emnlp-main.426) |  | 0 | Document-level entity-based extraction (EE), aiming at extracting entity-centric information such as entity roles and entity relations, is key to automatic knowledge acquisition from text corpora for various domains. Most document-level EE systems build extractive models, which struggle to model long-term dependencies among entities at the document level. To address this issue, we propose a generative framework for two document-level EE tasks: role-filler entity extraction (REE) and relation... | KungHsiang Huang, Sam Tang, Nanyun Peng |  |
| 896 |  |  [Learning Prototype Representations Across Few-Shot Tasks for Event Detection](https://doi.org/10.18653/v1/2021.emnlp-main.427) |  | 0 | We address the sampling bias and outlier issues in few-shot learning for event detection, a subtask of information extraction. We propose to model the relations between training tasks in episodic few-shot learning by introducing cross-task prototypes. We further propose to enforce prediction consistency among classifiers across tasks to make the model more robust to outliers. Our extensive experiment shows a consistent improvement on three few-shot learning datasets. The findings suggest that... | Viet Dac Lai, Franck Dernoncourt, Thien Huu Nguyen |  |
| 897 |  |  [Lifelong Event Detection with Knowledge Transfer](https://doi.org/10.18653/v1/2021.emnlp-main.428) |  | 0 | Traditional supervised Information Extraction (IE) methods can extract structured knowledge elements from unstructured data, but it is limited to a pre-defined target ontology. In reality, the ontology of interest may change over time, adding emergent new types or more fine-grained subtypes. We propose a new lifelong learning framework to address this challenge. We focus on lifelong event detection as an exemplar case and propose a new problem formulation that is also generalizable to other IE... | Pengfei Yu, Heng Ji, Prem Natarajan |  |
| 898 |  |  [Modular Self-Supervision for Document-Level Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.429) |  | 0 | Extracting relations across large text spans has been relatively underexplored in NLP, but it is particularly important for high-value domains such as biomedicine, where obtaining high recall of the latest findings is crucial for practical applications. Compared to conventional information extraction confined to short text spans, document-level relation extraction faces additional challenges in both inference and learning. Given longer text spans, state-of-the-art neural architectures are less... | Sheng Zhang, Cliff Wong, Naoto Usuyama, Sarthak Jain, Tristan Naumann, Hoifung Poon |  |
| 899 |  |  [Unsupervised Paraphrasing Consistency Training for Low Resource Named Entity Recognition](https://doi.org/10.18653/v1/2021.emnlp-main.430) |  | 0 | Unsupervised consistency training is a way of semi-supervised learning that encourages consistency in model predictions between the original and augmented data. For Named Entity Recognition (NER), existing approaches augment the input sequence with token replacement, assuming annotations on the replaced positions unchanged. In this paper, we explore the use of paraphrasing as a more principled data augmentation scheme for NER unsupervised consistency training. Specifically, we convert... | Rui Wang, Ricardo Henao |  |
| 900 |  |  [Fine-grained Entity Typing without Knowledge Base](https://doi.org/10.18653/v1/2021.emnlp-main.431) |  | 0 | Existing work on Fine-grained Entity Typing (FET) typically trains automatic models on the datasets obtained by using Knowledge Bases (KB) as distant supervision. However, the reliance on KB means this training setting can be hampered by the lack of or the incompleteness of the KB. To alleviate this limitation, we propose a novel setting for training FET models: FET without accessing any knowledge base. Under this setting, we propose a two-step framework to train FET models. In the first step,... | Jing Qian, Yibin Liu, Lemao Liu, Yangming Li, Haiyun Jiang, Haisong Zhang, Shuming Shi |  |
| 901 |  |  [Adversarial Attack against Cross-lingual Knowledge Graph Alignment](https://doi.org/10.18653/v1/2021.emnlp-main.432) |  | 0 | Recent literatures have shown that knowledge graph (KG) learning models are highly vulnerable to adversarial attacks. However, there is still a paucity of vulnerability analyses of cross-lingual entity alignment under adversarial attacks. This paper proposes an adversarial attack model with two novel attack techniques to perturb the KG structure and degrade the quality of deep cross-lingual entity alignment. First, an entity density maximization method is employed to hide the attacked entities... | Zeru Zhang, Zijie Zhang, Yang Zhou, Lingfei Wu, Sixing Wu, Xiaoying Han, Dejing Dou, Tianshi Che, Da Yan |  |
| 902 |  |  [Towards Realistic Few-Shot Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.433) |  | 0 | In recent years, few-shot models have been applied successfully to a variety of NLP tasks. Han et al. (2018) introduced a few-shot learning framework for relation classification, and since then, several models have surpassed human performance on this task, leading to the impression that few-shot relation classification is solved. In this paper we take a deeper look at the efficacy of strong few-shot classification models in the more common relation extraction setting, and show that typical... | Sam Brody, Sichao Wu, Adrian Benton |  |
| 903 |  |  [Data Augmentation for Cross-Domain Named Entity Recognition](https://doi.org/10.18653/v1/2021.emnlp-main.434) |  | 0 | Current work in named entity recognition (NER) shows that data augmentation techniques can produce more robust models. However, most existing techniques focus on augmenting in-domain data in low-resource scenarios where annotated data is quite limited. In this work, we take this research direction to the opposite and study cross-domain data augmentation for the NER task. We investigate the possibility of leveraging data from high-resource domains by projecting it into the low-resource domains.... | Shuguang Chen, Gustavo Aguilar, Leonardo Neves, Thamar Solorio |  |
| 904 |  |  [Incorporating medical knowledge in BERT for clinical relation extraction](https://doi.org/10.18653/v1/2021.emnlp-main.435) |  | 0 | In recent years pre-trained language models (PLM) such as BERT have proven to be very effective in diverse NLP tasks such as Information Extraction, Sentiment Analysis and Question Answering. Trained with massive general-domain text, these pre-trained language models capture rich syntactic, semantic and discourse information in the text. However, due to the differences between general and specific domain text (e.g., Wikipedia versus clinic notes), these models may not be ideal for... | Arpita Roy, Shimei Pan |  |
| 905 |  |  [ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning](https://doi.org/10.18653/v1/2021.emnlp-main.436) |  | 0 | While pre-trained language models (PTLMs) have achieved noticeable success on many NLP tasks, they still struggle for tasks that require event temporal reasoning, which is essential for event-centric applications. We present a continual pre-training approach that equips PTLMs with targeted knowledge about event temporal relations. We design self-supervised learning objectives to recover masked-out event and temporal indicators and to discriminate sentences from their corrupted counterparts... | Rujun Han, Xiang Ren, Nanyun Peng |  |
| 906 |  |  [Learning from Noisy Labels for Entity-Centric Information Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.437) |  | 0 | Recent information extraction approaches have relied on training deep neural models. However, such models can easily overfit noisy labels and suffer from performance degradation. While it is very costly to filter noisy labels in large learning resources, recent studies show that such labels take more training steps to be memorized and are more frequently forgotten than clean labels, therefore are identifiable in training. Motivated by such properties, we propose a simple co-regularization... | Wenxuan Zhou, Muhao Chen |  |
| 907 |  |  [Extracting Material Property Measurement Data from Scientific Articles](https://doi.org/10.18653/v1/2021.emnlp-main.438) |  | 0 | Machine learning-based prediction of material properties is often hampered by the lack of sufficiently large training data sets. The majority of such measurement data is embedded in scientific literature and the ability to automatically extract these data is essential to support the development of reliable property prediction methods. In this work, we describe a methodology for developing an automatic property extraction framework using material solubility as the target property. We create a... | Gihan Panapitiya, Fred Parks, Jonathan Sepulveda, Emily Saldanha |  |
| 908 |  |  [Modeling Document-Level Context for Event Detection via Important Context Selection](https://doi.org/10.18653/v1/2021.emnlp-main.439) |  | 0 | The task of Event Detection (ED) in Information Extraction aims to recognize and classify trigger words of events in text. The recent progress has featured advanced transformer-based language models (e.g., BERT) as a critical component in state-of-the-art models for ED. However, the length limit for input texts is a barrier for such ED models as they cannot encode long-range document-level context that has been shown to be beneficial for ED. To address this issue, we propose a novel method to... | Amir Pouran Ben Veyseh, Minh Van Nguyen, Nghia Trung Ngo, Bonan Min, Thien Huu Nguyen |  |
| 909 |  |  [Crosslingual Transfer Learning for Relation and Event Extraction via Word Category and Class Alignments](https://doi.org/10.18653/v1/2021.emnlp-main.440) |  | 0 | Previous work on crosslingual Relation and Event Extraction (REE) suffers from the monolingual bias issue due to the training of models on only the source language data. An approach to overcome this issue is to use unlabeled data in the target language to aid the alignment of crosslingual representations, i.e., via fooling a language discriminator. However, as this approach does not condition on class information, a target language example of a class could be incorrectly aligned to a source... | Minh Van Nguyen, Tuan Ngo Nguyen, Bonan Min, Thien Huu Nguyen |  |
| 910 |  |  [Corpus-based Open-Domain Event Type Induction](https://doi.org/10.18653/v1/2021.emnlp-main.441) |  | 0 | Traditional event extraction methods require predefined event types and their corresponding annotations to learn event extractors. These prerequisites are often hard to be satisfied in real-world applications. This work presents a corpus-based open-domain event type induction method that automatically discovers a set of event types from a given corpus. As events of the same type could be expressed in multiple ways, we propose to represent each event type as a cluster of <predicate sense, object... | Jiaming Shen, Yunyi Zhang, Heng Ji, Jiawei Han |  |
| 911 |  |  [PDALN: Progressive Domain Adaptation over a Pre-trained Model for Low-Resource Cross-Domain Named Entity Recognition](https://doi.org/10.18653/v1/2021.emnlp-main.442) |  | 0 | Cross-domain Named Entity Recognition (NER) transfers the NER knowledge from high-resource domains to the low-resource target domain. Due to limited labeled resources and domain shift, cross-domain NER is a challenging task. To address these challenges, we propose a progressive domain adaptation Knowledge Distillation (KD) approach – PDALN. It achieves superior domain adaptability by employing three components: (1) Adaptive data augmentation techniques, which alleviate cross-domain gap and... | Tao Zhang, Congying Xia, Philip S. Yu, Zhiwei Liu, Shu Zhao |  |
| 912 |  |  [Multi-Vector Attention Models for Deep Re-ranking](https://doi.org/10.18653/v1/2021.emnlp-main.443) |  | 0 | Large-scale document retrieval systems often utilize two styles of neural network models which live at two different ends of the joint computation vs. accuracy spectrum. The first style is dual encoder (or two-tower) models, where the query and document representations are computed completely independently and combined with a simple dot product operation. The second style is cross-attention models, where the query and document features are concatenated in the input layer and all computation is... | Giulio Zhou, Jacob Devlin |  |
| 913 |  |  [Toward Deconfounding the Effect of Entity Demographics for Question Answering Accuracy](https://doi.org/10.18653/v1/2021.emnlp-main.444) |  | 0 | The goal of question answering (QA) is to answer _any_ question. However, major QA datasets have skewed distributions over gender, profession, and nationality. Despite that skew, an analysis of model accuracy reveals little evidence that accuracy is lower for people based on gender or nationality; instead, there is more variation on professions (question topic) and question ambiguity. But QA’s lack of representation could itself hide evidence of bias, necessitating QA datasets that better... | Maharshi Gor, Kellie Webster, Jordan L. BoydGraber |  |
| 914 |  |  [Exploring Strategies for Generalizable Commonsense Reasoning with Pre-trained Models](https://doi.org/10.18653/v1/2021.emnlp-main.445) |  | 0 | Commonsense reasoning benchmarks have been largely solved by fine-tuning language models. The downside is that fine-tuning may cause models to overfit to task-specific data and thereby forget their knowledge gained during pre-training. Recent works only propose lightweight model updates as models may already possess useful knowledge from past experience, but a challenge remains in understanding what parts and to what extent models should be refined for a given task. In this paper, we... | Kaixin Ma, Filip Ilievski, Jonathan Francis, Satoru Ozaki, Eric Nyberg, Alessandro Oltramari |  |
| 915 |  |  [Transformer Feed-Forward Layers Are Key-Value Memories](https://doi.org/10.18653/v1/2021.emnlp-main.446) |  | 0 | Feed-forward layers constitute two-thirds of a transformer model’s parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow... | Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy |  |
| 916 |  |  [Connecting Attributions and QA Model Behavior on Realistic Counterfactuals](https://doi.org/10.18653/v1/2021.emnlp-main.447) |  | 0 | When a model attribution technique highlights a particular part of the input, a user might understand this highlight as making a statement about counterfactuals (Miller, 2019): if that part of the input were to change, the model’s prediction might change as well. This paper investigates how well different attribution techniques align with this assumption on realistic counterfactuals in the case of reading comprehension (RC). RC is a particularly challenging test case, as token-level... | Xi Ye, Rohan Nair, Greg Durrett |  |
| 917 |  |  [How Do Neural Sequence Models Generalize? Local and Global Cues for Out-of-Distribution Prediction](https://doi.org/10.18653/v1/2021.emnlp-main.448) |  | 0 | After a neural sequence model encounters an unexpected token, can its behavior be predicted? We show that RNN and transformer language models exhibit structured, consistent generalization in out-of-distribution contexts. We begin by introducing two idealized models of generalization in next-word prediction: a lexical context model in which generalization is consistent with the last word observed, and a syntactic context model in which generalization is consistent with the global structure of... | D. Anthony Bau, Jacob Andreas |  |
| 918 |  |  [Comparing Text Representations: A Theory-Driven Approach](https://doi.org/10.18653/v1/2021.emnlp-main.449) |  | 0 | Much of the progress in contemporary NLP has come from learning representations, such as masked language model (MLM) contextual embeddings, that turn challenging problems into simple classification tasks. But how do we quantify and explain this effect? We adapt general tools from computational learning theory to fit the specific characteristics of text datasets and present a method to evaluate the compatibility between representations and tasks. Even though many tasks can be easily solved with... | Gregory Yauney, David Mimno |  |
| 919 |  |  [Human Rationales as Attribution Priors for Explainable Stance Detection](https://doi.org/10.18653/v1/2021.emnlp-main.450) |  | 0 | As NLP systems become better at detecting opinions and beliefs from text, it is important to ensure not only that models are accurate but also that they arrive at their predictions in ways that align with human reasoning. In this work, we present a method for imparting human-like rationalization to a stance detection model using crowdsourced annotations on a small fraction of the training data. We show that in a data-scarce setting, our approach can improve the reasoning of a state-of-the-art... | Sahil Jayaram, Emily Allaway |  |
| 920 |  |  [The Stem Cell Hypothesis: Dilemma behind Multi-Task Learning with Transformer Encoders](https://doi.org/10.18653/v1/2021.emnlp-main.451) |  | 0 | Multi-task learning with transformer encoders (MTL) has emerged as a powerful technique to improve performance on closely-related tasks for both accuracy and efficiency while a question still remains whether or not it would perform as well on tasks that are distinct in nature. We first present MTL results on five NLP tasks, POS, NER, DEP, CON, and SRL, and depict its deficiency over single-task learning. We then conduct an extensive pruning analysis to show that a certain set of attention heads... | Han He, Jinho D. Choi |  |
| 921 |  |  [Text Counterfactuals via Latent Optimization and Shapley-Guided Search](https://doi.org/10.18653/v1/2021.emnlp-main.452) |  | 0 | We study the problem of generating counterfactual text for a classifier as a means for understanding and debugging classification. Given a textual input and a classification model, we aim to minimally alter the text to change the model’s prediction. White-box approaches have been successfully applied to similar problems in vision where one can directly optimize the continuous input. Optimization-based approaches become difficult in the language domain due to the discrete nature of text. We... | Xiaoli Z. Fern, Quintin Pope |  |
| 922 |  |  ["Average" Approximates "First Principal Component"? An Empirical Analysis on Representations from Neural Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.453) |  | 0 | Contextualized representations based on neural language models have furthered the state of the art in various NLP tasks. Despite its great success, the nature of such representations remains a mystery. In this paper, we present an empirical property of these representations—”average” approximates “first principal component”. Specifically, experiments show that the average of these representations shares almost the same direction as the first principal component of the matrix whose columns are... | Zihan Wang, Chengyu Dong, Jingbo Shang |  |
| 923 |  |  [Controlled Evaluation of Grammatical Knowledge in Mandarin Chinese Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.454) |  | 0 | Prior work has shown that structural supervision helps English language models learn generalizations about syntactic phenomena such as subject-verb agreement. However, it remains unclear if such an inductive bias would also improve language models’ ability to learn grammatical dependencies in typologically different languages. Here we investigate this question in Mandarin Chinese, which has a logographic, largely syllable-based writing system; different word order; and sparser morphology than... | Yiwen Wang, Jennifer Hu, Roger Levy, Peng Qian |  |
| 924 |  |  [GradTS: A Gradient-Based Automatic Auxiliary Task Selection Method Based on Transformer Networks](https://doi.org/10.18653/v1/2021.emnlp-main.455) |  | 0 | A key problem in multi-task learning (MTL) research is how to select high-quality auxiliary tasks automatically. This paper presents GradTS, an automatic auxiliary task selection method based on gradient calculation in Transformer-based models. Compared to AUTOSEM, a strong baseline method, GradTS improves the performance of MT-DNN with a bert-base-cased backend model, from 0.33% to 17.93% on 8 natural language understanding (NLU) tasks in the GLUE benchmarks. GradTS is also time-saving since... | Weicheng Ma, Renze Lou, Kai Zhang, Lili Wang, Soroush Vosoughi |  |
| 925 |  |  [NegatER: Unsupervised Discovery of Negatives in Commonsense Knowledge Bases](https://doi.org/10.18653/v1/2021.emnlp-main.456) |  | 0 | Codifying commonsense knowledge in machines is a longstanding goal of artificial intelligence. Recently, much progress toward this goal has been made with automatic knowledge base (KB) construction techniques. However, such techniques focus primarily on the acquisition of positive (true) KB statements, even though negative (false) statements are often also important for discriminative reasoning over commonsense KBs. As a first step toward the latter, this paper proposes NegatER, a framework... | Tara Safavi, Jing Zhu, Danai Koutra |  |
| 926 |  |  [Instance-adaptive training with noise-robust losses against noisy labels](https://doi.org/10.18653/v1/2021.emnlp-main.457) |  | 0 | In order to alleviate the huge demand for annotated datasets for different tasks, many recent natural language processing datasets have adopted automated pipelines for fast-tracking usable data. However, model training with such datasets poses a challenge because popular optimization objectives are not robust to label noise induced in the annotation generation process. Several noise-robust losses have been proposed and evaluated on tasks in computer vision, but they generally use a single... | Lifeng Jin, Linfeng Song, Kun Xu, Dong Yu |  |
| 927 |  |  [Distributionally Robust Multilingual Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.458) |  | 0 | Multilingual neural machine translation (MNMT) learns to translate multiple language pairs with a single model, potentially improving both the accuracy and the memory-efficiency of deployed models. However, the heavy data imbalance between languages hinders the model from performing uniformly across language pairs. In this paper, we propose a new learning objective for MNMT based on distributionally robust optimization, which minimizes the worst-case expected loss over the set of language... | Chunting Zhou, Daniel Levy, Xian Li, Marjan Ghazvininejad, Graham Neubig |  |
| 928 |  |  [Model Selection for Cross-lingual Transfer](https://doi.org/10.18653/v1/2021.emnlp-main.459) |  | 0 | Transformers that are pre-trained on multilingual corpora, such as, mBERT and XLM-RoBERTa, have achieved impressive cross-lingual transfer capabilities. In the zero-shot transfer setting, only English training data is used, and the fine-tuned model is evaluated on another target language. While this works surprisingly well, substantial variance has been observed in target language performance between different fine-tuning runs, and in the zero-shot setup, no target-language development data is... | Yang Chen, Alan Ritter |  |
| 929 |  |  [Continual Few-Shot Learning for Text Classification](https://doi.org/10.18653/v1/2021.emnlp-main.460) |  | 0 | Natural Language Processing (NLP) is increasingly relying on general end-to-end systems that need to handle many different linguistic phenomena and nuances. For example, a Natural Language Inference (NLI) system has to recognize sentiment, handle numbers, perform coreference, etc. Our solutions to complex problems are still far from perfect, so it is important to create systems that can learn to correct mistakes quickly, incrementally, and with little training data. In this work, we propose a... | Ramakanth Pasunuru, Veselin Stoyanov, Mohit Bansal |  |
| 930 |  |  [Efficient Nearest Neighbor Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.461) |  | 0 | Non-parametric neural language models (NLMs) learn predictive distributions of text utilizing an external datastore, which allows them to learn through explicitly memorizing the training datapoints. While effective, these models often require retrieval from a large datastore at test time, significantly increasing the inference overhead and thus limiting the deployment of non-parametric NLMs in practical applications. In this paper, we take the recently proposed k-nearest neighbors language... | Junxian He, Graham Neubig, Taylor BergKirkpatrick |  |
| 931 |  |  [STraTA: Self-Training with Task Augmentation for Better Few-shot Learning](https://doi.org/10.18653/v1/2021.emnlp-main.462) |  | 0 | Despite their recent successes in tackling many NLP tasks, large-scale pre-trained language models do not perform as well in few-shot settings where only a handful of training examples are available. To address this shortcoming, we propose STraTA, which stands for Self-Training with Task Augmentation, an approach that builds on two key ideas for effective leverage of unlabeled data. First, STraTA uses task augmentation, a novel technique that synthesizes a large amount of data for... | Tu Vu, MinhThang Luong, Quoc V. Le, Grady Simon, Mohit Iyyer |  |
| 932 |  |  [TADPOLE: Task ADapted Pre-Training via AnOmaLy DEtection](https://doi.org/10.18653/v1/2021.emnlp-main.463) |  | 0 | The paradigm of pre-training followed by finetuning has become a standard procedure for NLP tasks, with a known problem of domain shift between the pre-training and downstream corpus. Previous works have tried to mitigate this problem with additional pre-training, either on the downstream corpus itself when it is large enough, or on a manually curated unlabeled corpus of a similar domain. In this paper, we address the problem for the case when the downstream corpus is too small for additional... | Vivek Madan, Ashish Khetan, Zohar Karnin |  |
| 933 |  |  [Gradient-based Adversarial Attacks against Text Transformers](https://doi.org/10.18653/v1/2021.emnlp-main.464) |  | 0 | We propose the first general-purpose gradient-based adversarial attack against transformer models. Instead of searching for a single adversarial example, we search for a distribution of adversarial examples parameterized by a continuous-valued matrix, hence enabling gradient-based optimization. We empirically demonstrate that our white-box attack attains state-of-the-art attack performance on a variety of natural language tasks, outperforming prior work in terms of adversarial success rate with... | Chuan Guo, Alexandre Sablayrolles, Hervé Jégou, Douwe Kiela |  |
| 934 |  |  [Do Transformer Modifications Transfer Across Implementations and Applications?](https://doi.org/10.18653/v1/2021.emnlp-main.465) |  | 0 | The research community has proposed copious modifications to the Transformer architecture since it was introduced over three years ago, relatively few of which have seen widespread adoption. In this paper, we comprehensively evaluate many of these modifications in a shared experimental setting that covers most of the common uses of the Transformer in natural language processing. Surprisingly, we find that most modifications do not meaningfully improve performance. Furthermore, most of the... | Sharan Narang, Hyung Won Chung, Yi Tay, Liam Fedus, Thibault Févry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi Zhou, Wei Li, Nan Ding, Jake Marcus, Adam Roberts, Colin Raffel |  |
| 935 |  |  [Paired Examples as Indirect Supervision in Latent Decision Models](https://doi.org/10.18653/v1/2021.emnlp-main.466) |  | 0 | Compositional, structured models are appealing because they explicitly decompose problems and provide interpretable intermediate outputs that give confidence that the model is not simply latching onto data artifacts. Learning these models is challenging, however, because end-task supervision only provides a weak indirect signal on what values the latent decisions should take. This often results in the model failing to learn to perform the intermediate tasks correctly. In this work, we introduce... | Nitish Gupta, Sameer Singh, Matt Gardner, Dan Roth |  |
| 936 |  |  [Pairwise Supervised Contrastive Learning of Sentence Representations](https://doi.org/10.18653/v1/2021.emnlp-main.467) |  | 0 | Many recent successes in sentence representation learning have been achieved by simply fine-tuning on the Natural Language Inference (NLI) datasets with triplet loss or siamese loss. Nevertheless, they share a common weakness: sentences in a contradiction pair are not necessarily from different semantic categories. Therefore, optimizing the semantic entailment and contradiction reasoning objective alone is inadequate to capture the high-level semantic structure. The drawback is compounded by... | Dejiao Zhang, ShangWen Li, Wei Xiao, Henghui Zhu, Ramesh Nallapati, Andrew O. Arnold, Bing Xiang |  |
| 937 |  |  [Muppet: Massive Multi-task Representations with Pre-Finetuning](https://doi.org/10.18653/v1/2021.emnlp-main.468) |  | 0 | We propose pre-finetuning, an additional large-scale learning stage between language model pre-training and fine-tuning. Pre-finetuning is massively multi-task learning (around 50 datasets, over 4.8 million total labeled examples), and is designed to encourage learning of representations that generalize better to many different tasks. We show that pre-finetuning consistently improves performance for pretrained discriminators (e.g. RoBERTa) and generation models (e.g. BART) on a wide range of... | Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, Sonal Gupta |  |
| 938 |  |  [Diverse Distributions of Self-Supervised Tasks for Meta-Learning in NLP](https://doi.org/10.18653/v1/2021.emnlp-main.469) |  | 0 | Meta-learning considers the problem of learning an efficient learning process that can leverage its past experience to accurately solve new tasks. However, the efficacy of meta-learning crucially depends on the distribution of tasks available for training, and this is often assumed to be known a priori or constructed from limited supervised datasets. In this work, we aim to provide task distributions for meta-learning by considering self-supervised tasks automatically proposed from unlabeled... | Trapit Bansal, Karthick Prasad Gunasekaran, Tong Wang, Tsendsuren Munkhdalai, Andrew McCallum |  |
| 939 |  |  [A Simple and Effective Method To Eliminate the Self Language Bias in Multilingual Representations](https://doi.org/10.18653/v1/2021.emnlp-main.470) |  | 0 | Language agnostic and semantic-language information isolation is an emerging research direction for multilingual representations models. We explore this problem from a novel angle of geometric algebra and semantic space. A simple but highly effective method “Language Information Removal (LIR)” factors out language identity information from semantic related components in multilingual representations pre-trained on multi-monolingual data. A post-training and model-agnostic method, LIR only uses... | Ziyi Yang, Yinfei Yang, Daniel Cer, Eric Darve |  |
| 940 |  |  [A Massively Multilingual Analysis of Cross-linguality in Shared Embedding Space](https://doi.org/10.18653/v1/2021.emnlp-main.471) |  | 0 | In cross-lingual language models, representations for many different languages live in the same space. Here, we investigate the linguistic and non-linguistic factors affecting sentence-level alignment in cross-lingual pretrained language models for 101 languages and 5,050 language pairs. Using BERT-based LaBSE and BiLSTM-based LASER as our models, and the Bible as our corpus, we compute a task-based measure of cross-lingual alignment in the form of bitext retrieval performance, as well as four... | Alexander Jones, William Yang Wang, Kyle Mahowald |  |
| 941 |  |  [Frustratingly Simple but Surprisingly Strong: Using Language-Independent Features for Zero-shot Cross-lingual Semantic Parsing](https://doi.org/10.18653/v1/2021.emnlp-main.472) |  | 0 | The availability of corpora has led to significant advances in training semantic parsers in English. Unfortunately, for languages other than English, annotated data is limited and so is the performance of the developed parsers. Recently, pretrained multilingual models have been proven useful for zero-shot cross-lingual transfer in many NLP tasks. What else does it require to apply a parser trained in English to other languages for zero-shot cross-lingual semantic parsing? Will simple... | Jingfeng Yang, Federico Fancellu, Bonnie Webber, Diyi Yang |  |
| 942 |  |  [Improving Simultaneous Translation by Incorporating Pseudo-References with Fewer Reorderings](https://doi.org/10.18653/v1/2021.emnlp-main.473) |  | 0 | Simultaneous translation is vastly different from full-sentence translation, in the sense that it starts translation before the source sentence ends, with only a few words delay. However, due to the lack of large-scale, high-quality simultaneous translation datasets, most such systems are still trained on conventional full-sentence bitexts. This is far from ideal for the simultaneous scenario due to the abundance of unnecessary long-distance reorderings in those bitexts. We propose a novel... | JunKun Chen, Renjie Zheng, Atsuhito Kita, Mingbo Ma, Liang Huang |  |
| 943 |  |  [Classification-based Quality Estimation: Small and Efficient Models for Real-world Applications](https://doi.org/10.18653/v1/2021.emnlp-main.474) |  | 0 | Sentence-level Quality estimation (QE) of machine translation is traditionally formulated as a regression task, and the performance of QE models is typically measured by Pearson correlation with human labels. Recent QE models have achieved previously-unseen levels of correlation with human judgments, but they rely on large multilingual contextualized language models that are computationally expensive and make them infeasible for real-world applications. In this work, we evaluate several model... | Shuo Sun, Ahmed ElKishky, Vishrav Chaudhary, James Cross, Lucia Specia, Francisco Guzmán |  |
| 944 |  |  [A Large-Scale Study of Machine Translation in Turkic Languages](https://doi.org/10.18653/v1/2021.emnlp-main.475) |  | 0 | Recent advances in neural machine translation (NMT) have pushed the quality of machine translation systems to the point where they are becoming widely adopted to build competitive systems. However, there is still a large number of languages that are yet to reap the benefits of NMT. In this paper, we provide the first large-scale case study of the practical application of MT in the Turkic language family in order to realize the gains of NMT for Turkic languages under high-resource to extremely... | Jamshidbek Mirzakhalov, Anoop Babu, Duygu Ataman, Sherzod Kariev, Francis M. Tyers, Otabek Abduraufov, Mammad Hajili, Sardana Ivanova, Abror Khaytbaev, Antonio Laverghetta Jr., Behzodbek Moydinboyev, Esra Onal, Shaxnoza Pulatova, Ahsan Wahab, Orhan Firat, Sriram Chellappan |  |
| 945 |  |  [Analyzing the Surprising Variability in Word Embedding Stability Across Languages](https://doi.org/10.18653/v1/2021.emnlp-main.476) |  | 0 | Word embeddings are powerful representations that form the foundation of many natural language processing architectures, both in English and in other languages. To gain further insight into word embeddings, we explore their stability (e.g., overlap between the nearest neighbors of a word in different embedding spaces) in diverse languages. We discuss linguistic properties that are related to stability, drawing out insights about correlations with affixing, language gender systems, and other... | Laura Burdick, Jonathan K. Kummerfeld, Rada Mihalcea |  |
| 946 |  |  [Rule-based Morphological Inflection Improves Neural Terminology Translation](https://doi.org/10.18653/v1/2021.emnlp-main.477) |  | 0 | Current approaches to incorporating terminology constraints in machine translation (MT) typically assume that the constraint terms are provided in their correct morphological forms. This limits their application to real-world scenarios where constraint terms are provided as lemmas. In this paper, we introduce a modular framework for incorporating lemma constraints in neural MT (NMT) in which linguistic knowledge and diverse types of NMT models can be flexibly applied. It is based on a novel... | Weijia Xu, Marine Carpuat |  |
| 947 |  |  [Data and Parameter Scaling Laws for Neural Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.478) |  | 0 | We observe that the development cross-entropy loss of supervised neural machine translation models scales like a power law with the amount of training data and the number of non-embedding parameters in the model. We discuss some practical implications of these results, such as predicting BLEU achieved by large scale models and predicting the ROI of labeling data in low-resource language pairs. | Mitchell A. Gordon, Kevin Duh, Jared Kaplan |  |
| 948 |  |  [Good-Enough Example Extrapolation](https://doi.org/10.18653/v1/2021.emnlp-main.479) |  | 0 | This paper asks whether extrapolating the hidden space distribution of text examples from one class onto another is a valid inductive bias for data augmentation. To operationalize this question, I propose a simple data augmentation protocol called “good-enough example extrapolation” (GE3). GE3 is lightweight and has no hyperparameters. Applied to three text classification datasets for various data imbalance scenarios, GE3 improves performance more than upsampling and other hidden-space data... | Jason Wei |  |
| 949 |  |  [Learning to Selectively Learn for Weakly-supervised Paraphrase Generation](https://doi.org/10.18653/v1/2021.emnlp-main.480) |  | 0 | Paraphrase generation is a longstanding NLP task that has diverse applications on downstream NLP tasks. However, the effectiveness of existing efforts predominantly relies on large amounts of golden labeled data. Though unsupervised endeavors have been proposed to alleviate this issue, they may fail to generate meaningful paraphrases due to the lack of supervision signals. In this work, we go beyond the existing paradigms and propose a novel approach to generate high-quality paraphrases with... | Kaize Ding, Dingcheng Li, Alexander Hanbo Li, Xing Fan, Chenlei Guo, Yang Liu, Huan Liu |  |
| 950 |  |  [Effective Convolutional Attention Network for Multi-label Clinical Document Classification](https://doi.org/10.18653/v1/2021.emnlp-main.481) |  | 0 | Multi-label document classification (MLDC) problems can be challenging, especially for long documents with a large label set and a long-tail distribution over labels. In this paper, we present an effective convolutional attention network for the MLDC problem with a focus on medical code prediction from clinical documents. Our innovations are three-fold: (1) we utilize a deep convolution-based encoder with the squeeze-and-excitation networks and residual networks to aggregate the information... | Yang Liu, Hua Cheng, Russell Klopfer, Matthew R. Gormley, Thomas Schaaf |  |
| 951 |  |  [Contrastive Code Representation Learning](https://doi.org/10.18653/v1/2021.emnlp-main.482) |  | 0 | Recent work learns contextual representations of source code by reconstructing tokens from their context. For downstream semantic understanding tasks like code clone detection, these representations should ideally capture program functionality. However, we show that the popular reconstruction-based RoBERTa model is sensitive to source code edits, even when the edits preserve semantics. We propose ContraCode: a contrastive pre-training task that learns code functionality, not form. ContraCode... | Paras Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel, Joseph Gonzalez, Ion Stoica |  |
| 952 |  |  [IGA: An Intent-Guided Authoring Assistant](https://doi.org/10.18653/v1/2021.emnlp-main.483) |  | 0 | While large-scale pretrained language models have significantly improved writing assistance functionalities such as autocomplete, more complex and controllable writing assistants have yet to be explored. We leverage advances in language modeling to build an interactive writing assistant that generates and rephrases text according to fine-grained author specifications. Users provide input to our Intent-Guided Assistant (IGA) in the form of text interspersed with tags that correspond to specific... | Simeng Sun, Wenlong Zhao, Varun Manjunatha, Rajiv Jain, Vlad I. Morariu, Franck Dernoncourt, Balaji Vasan Srinivasan, Mohit Iyyer |  |
| 953 |  |  [Math Word Problem Generation with Mathematical Consistency and Problem Context Constraints](https://doi.org/10.18653/v1/2021.emnlp-main.484) |  | 0 | We study the problem of generating arithmetic math word problems (MWPs) given a math equation that specifies the mathematical computation and a context that specifies the problem scenario. Existing approaches are prone to generating MWPs that are either mathematically invalid or have unsatisfactory language quality. They also either ignore the context or require manual specification of a problem template, which compromises the diversity of the generated MWPs. In this paper, we develop a novel... | Zichao Wang, Andrew S. Lan, Richard G. Baraniuk |  |
| 954 |  |  [Navigating the Kaleidoscope of COVID-19 Misinformation Using Deep Learning](https://doi.org/10.18653/v1/2021.emnlp-main.485) |  | 0 | Irrespective of the success of the deep learning-based mixed-domain transfer learning approach for solving various Natural Language Processing tasks, it does not lend a generalizable solution for detecting misinformation from COVID-19 social media data. Due to the inherent complexity of this type of data, caused by its dynamic (context evolves rapidly), nuanced (misinformation types are often ambiguous), and diverse (skewed, fine-grained, and overlapping categories) nature, it is imperative for... | Yuanzhi Chen, Mohammad Rashedul Hasan |  |
| 955 |  |  [Detecting Health Advice in Medical Research Literature](https://doi.org/10.18653/v1/2021.emnlp-main.486) |  | 0 | Health and medical researchers often give clinical and policy recommendations to inform health practice and public health policy. However, no current health information system supports the direct retrieval of health advice. This study fills the gap by developing and validating an NLP-based prediction model for identifying health advice in research publications. We annotated a corpus of 6,000 sentences extracted from structured abstracts in PubMed publications as ‘“strong advice”, “weak advice”,... | Yingya Li, Jun Wang, Bei Yu |  |
| 956 |  |  [A Semantic Feature-Wise Transformation Relation Network for Automatic Short Answer Grading](https://doi.org/10.18653/v1/2021.emnlp-main.487) |  | 0 | Automatic short answer grading (ASAG) is the task of assessing students’ short natural language responses to objective questions. It is a crucial component of new education platforms, and could support more wide-spread use of constructed response questions to replace cognitively less challenging multiple choice questions. We propose a Semantic Feature-wise transformation Relation Network (SFRN) that exploits the multiple components of ASAG datasets more effectively. SFRN captures relational... | Zhaohui Li, Yajur Tomar, Rebecca J. Passonneau |  |
| 957 |  |  [Evaluating Scholarly Impact: Towards Content-Aware Bibliometrics](https://doi.org/10.18653/v1/2021.emnlp-main.488) |  | 0 | Quantitatively measuring the impact-related aspects of scientific, engineering, and technological (SET) innovations is a fundamental problem with broad applications. Traditional citation-based measures for assessing the impact of innovations and related entities do not take into account the content of the publications. This limits their ability to provide rigorous quality-related metrics because they cannot account for the reasons that led to a citation. We present approaches to estimate... | Saurav Manchanda, George Karypis |  |
| 958 |  |  [A Scalable Framework for Learning From Implicit User Feedback to Improve Natural Language Understanding in Large-Scale Conversational AI Systems](https://doi.org/10.18653/v1/2021.emnlp-main.489) |  | 0 | Natural Language Understanding (NLU) is an established component within a conversational AI or digital assistant system, and it is responsible for producing semantic understanding of a user request. We propose a scalable and automatic approach for improving NLU in a large-scale conversational AI system by leveraging implicit user feedback, with an insight that user interaction data and dialog context have rich information embedded from which user satisfaction and intention can be inferred. In... | Sunghyun Park, Han Li, Ameen Patel, Sidharth Mudgal, Sungjin Lee, YoungBum Kim, Spyros Matsoukas, Ruhi Sarikaya |  |
| 959 |  |  [Summarize-then-Answer: Generating Concise Explanations for Multi-hop Reading Comprehension](https://doi.org/10.18653/v1/2021.emnlp-main.490) |  | 0 | How can we generate concise explanations for multi-hop Reading Comprehension (RC)? The current strategies of identifying supporting sentences can be seen as an extractive question-focused summarization of the input text. However, these extractive explanations are not necessarily concise i.e. not minimally sufficient for answering a question. Instead, we advocate for an abstractive approach, where we propose to generate a question-focused, abstractive summary of input paragraphs and then feed it... | Naoya Inoue, Harsh Trivedi, Steven Sinha, Niranjan Balasubramanian, Kentaro Inui |  |
| 960 |  |  [FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models](https://doi.org/10.18653/v1/2021.emnlp-main.491) |  | 0 | The task of learning from only a few examples (called a few-shot setting) is of key importance and relevance to a real-world setting. For question answering (QA), the current state-of-the-art pre-trained models typically need fine-tuning on tens of thousands of examples to obtain good results. Their performance degrades significantly in a few-shot setting (< 100 examples). To address this, we propose a simple fine-tuning framework that leverages pre-trained text-to-text models and is directly... | Rakesh Chada, Pradeep Natarajan |  |
| 961 |  |  [Multi-stage Training with Improved Negative Contrast for Neural Passage Retrieval](https://doi.org/10.18653/v1/2021.emnlp-main.492) |  | 0 | In the context of neural passage retrieval, we study three promising techniques: synthetic data generation, negative sampling, and fusion. We systematically investigate how these techniques contribute to the performance of the retrieval system and how they complement each other. We propose a multi-stage framework comprising of pre-training with synthetic data, fine-tuning with labeled data, and negative sampling at both stages. We study six negative sampling strategies and apply them to the... | Jing Lu, Gustavo Hernández Ábrego, Ji Ma, Jianmo Ni, Yinfei Yang |  |
| 962 |  |  [Perhaps PTLMs Should Go to School - A Task to Assess Open Book and Closed Book QA](https://doi.org/10.18653/v1/2021.emnlp-main.493) |  | 0 | Our goal is to deliver a new task and leaderboard to stimulate research on question answering and pre-trained language models (PTLMs) to understand a significant instructional document, e.g., an introductory college textbook or a manual. PTLMs have shown great success in many question-answering tasks, given significant supervised training, but much less so in zero-shot settings. We propose a new task that includes two college-level introductory texts in the social sciences (American Government... | Manuel R. Ciosici, Joe Cecil, DongHo Lee, Alex Hedges, Marjorie Freedman, Ralph M. Weischedel |  |
| 963 |  |  [ReasonBERT: Pre-trained to Reason with Distant Supervision](https://doi.org/10.18653/v1/2021.emnlp-main.494) |  | 0 | We present ReasonBert, a pre-training method that augments language models with the ability to reason over long-range relations and multiple, possibly hybrid contexts. Unlike existing pre-training methods that only harvest learning signals from local contexts of naturally occurring texts, we propose a generalized notion of distant supervision to automatically connect multiple pieces of text and tables to create pre-training examples that require long-range reasoning. Different types of... | Xiang Deng, Yu Su, Alyssa Lees, You Wu, Cong Yu, Huan Sun |  |
| 964 |  |  [Single-dataset Experts for Multi-dataset Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.495) |  | 0 | Many datasets have been created for training reading comprehension models, and a natural question is whether we can combine them to build models that (1) perform better on all of the training datasets and (2) generalize and transfer better to new datasets. Prior work has addressed this goal by training one network simultaneously on multiple datasets, which works well on average but is prone to over- or under-fitting different sub- distributions and might transfer worse compared to source models... | Dan Friedman, Ben Dodge, Danqi Chen |  |
| 965 |  |  [Simple Entity-Centric Questions Challenge Dense Retrievers](https://doi.org/10.18653/v1/2021.emnlp-main.496) |  | 0 | Open-domain question answering has exploded in popularity recently due to the success of dense retrieval models, which have surpassed sparse models using only a few supervised training examples. However, in this paper, we demonstrate current dense models are not yet the holy grail of retrieval. We first construct EntityQuestions, a set of simple, entity-rich questions based on facts from Wikidata (e.g., “Where was Arve Furset born?”), and observe that dense retrievers drastically under-perform... | Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, Danqi Chen |  |
| 966 |  |  [Mitigating False-Negative Contexts in Multi-document Question Answering with Retrieval Marginalization](https://doi.org/10.18653/v1/2021.emnlp-main.497) |  | 0 | Question Answering (QA) tasks requiring information from multiple documents often rely on a retrieval model to identify relevant information for reasoning. The retrieval model is typically trained to maximize the likelihood of the labeled supporting evidence. However, when retrieving from large text corpora such as Wikipedia, the correct answer can often be obtained from multiple evidence candidates. Moreover, not all such candidates are labeled as positive during annotation, rendering the... | Ansong Ni, Matt Gardner, Pradeep Dasigi |  |
| 967 |  |  [MultiDoc2Dial: Modeling Dialogues Grounded in Multiple Documents](https://doi.org/10.18653/v1/2021.emnlp-main.498) |  | 0 | We propose MultiDoc2Dial, a new task and dataset on modeling goal-oriented dialogues grounded in multiple documents. Most previous works treat document-grounded dialogue modeling as machine reading comprehension task based on a single given document or passage. In this work, we aim to address more realistic scenarios where a goal-oriented information-seeking conversation involves multiple topics, and hence is grounded on different documents. To facilitate such task, we introduce a new dataset... | Song Feng, Siva Sankalp Patel, Hui Wan, Sachindra Joshi |  |
| 968 |  |  [GupShup: Summarizing Open-Domain Code-Switched Conversations](https://doi.org/10.18653/v1/2021.emnlp-main.499) |  | 0 | Code-switching is the communication phenomenon where the speakers switch between different languages during a conversation. With the widespread adoption of conversational agents and chat platforms, code-switching has become an integral part of written conversations in many multi-lingual communities worldwide. Therefore, it is essential to develop techniques for understanding and summarizing these conversations. Towards this objective, we introduce the task of abstractive summarization of... | Laiba Mehnaz, Debanjan Mahata, Rakesh Gosangi, Uma Sushmitha Gunturi, Riya Jain, Gauri Gupta, Amardeep Kumar, Isabelle G. Lee, Anish Acharya, Rajiv Ratn Shah |  |
| 969 |  |  [BiSECT: Learning to Split and Rephrase Sentences with Bitexts](https://doi.org/10.18653/v1/2021.emnlp-main.500) |  | 0 | An important task in NLP applications such as sentence simplification is the ability to take a long, complex sentence and split it into shorter sentences, rephrasing as necessary. We introduce a novel dataset and a new model for this ‘split and rephrase’ task. Our BiSECT training data consists of 1 million long English sentences paired with shorter, meaning-equivalent English sentences. We obtain these by extracting 1-2 sentence alignments in bilingual parallel corpora and then using machine... | Joongwon Kim, Mounica Maddela, Reno Kriz, Wei Xu, Chris CallisonBurch |  |
| 970 |  |  [Data Collection vs. Knowledge Graph Completion: What is Needed to Improve Coverage?](https://doi.org/10.18653/v1/2021.emnlp-main.501) |  | 0 | This survey/position paper discusses ways to improve coverage of resources such as WordNet. Rapp estimated correlations, rho, between corpus statistics and pyscholinguistic norms. rho improves with quantity (corpus size) and quality (balance). 1M words is enough for simple estimates (unigram frequencies), but at least 100x more is required for good estimates of word associations and embeddings. Given such estimates, WordNet’s coverage is remarkable. WordNet was developed on SemCor, a small... | Kenneth Church, Yuchen Bian |  |
| 971 |  |  [Universal Sentence Representation Learning with Conditional Masked Language Model](https://doi.org/10.18653/v1/2021.emnlp-main.502) |  | 0 | This paper presents a novel training method, Conditional Masked Language Modeling (CMLM), to effectively learn sentence representations on large scale unlabeled corpora. CMLM integrates sentence representation learning into MLM training by conditioning on the encoded vectors of adjacent sentences. Our English CMLM model achieves state-of-the-art performance on SentEval, even outperforming models learned using supervised signals. As a fully unsupervised learning method, CMLM can be conveniently... | Ziyi Yang, Yinfei Yang, Daniel Cer, Jax Law, Eric Darve |  |
| 972 |  |  [On the Benefit of Syntactic Supervision for Cross-lingual Transfer in Semantic Role Labeling](https://doi.org/10.18653/v1/2021.emnlp-main.503) |  | 0 | Although recent developments in neural architectures and pre-trained representations have greatly increased state-of-the-art model performance on fully-supervised semantic role labeling (SRL), the task remains challenging for languages where supervised SRL training data are not abundant. Cross-lingual learning can improve performance in this setting by transferring knowledge from high-resource languages to low-resource ones. Moreover, we hypothesize that annotations of syntactic dependencies... | Zhisong Zhang, Emma Strubell, Eduard H. Hovy |  |
| 973 |  |  [Implicit Premise Generation with Discourse-aware Commonsense Knowledge Models](https://doi.org/10.18653/v1/2021.emnlp-main.504) |  | 0 | Enthymemes are defined as arguments where a premise or conclusion is left implicit. We tackle the task of generating the implicit premise in an enthymeme, which requires not only an understanding of the stated conclusion and premise but also additional inferences that could depend on commonsense knowledge. The largest available dataset for enthymemes (Habernal et al., 2018) consists of 1.7k samples, which is not large enough to train a neural text generation model. To address this issue, we... | Tuhin Chakrabarty, Aadit Trivedi, Smaranda Muresan |  |
| 974 |  |  [Inducing Transformer's Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks](https://doi.org/10.18653/v1/2021.emnlp-main.505) |  | 0 | Systematic compositionality is an essential mechanism in human language, allowing the recombination of known parts to create novel expressions. However, existing neural models have been shown to lack this basic ability in learning symbolic structures. Motivated by the failure of a Transformer model on the SCAN compositionality challenge (Lake and Baroni, 2018), which requires parsing a command into actions, we propose two auxiliary sequence prediction tasks as additional training supervision.... | Yichen Jiang, Mohit Bansal |  |
| 975 |  |  [Flexible Generation of Natural Language Deductions](https://doi.org/10.18653/v1/2021.emnlp-main.506) |  | 0 | An interpretable system for open-domain reasoning needs to express its reasoning process in a transparent form. Natural language is an attractive representation for this purpose — it is both highly expressive and easy for humans to understand. However, manipulating natural language statements in logically consistent ways is hard: models must cope with variation in how meaning is expressed while remaining precise. In this paper, we describe ParaPattern, a method for building models to generate... | Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, Greg Durrett |  |
| 976 |  |  [Structure-aware Fine-tuning of Sequence-to-sequence Transformers for Transition-based AMR Parsing](https://doi.org/10.18653/v1/2021.emnlp-main.507) |  | 0 | Predicting linearized Abstract Meaning Representation (AMR) graphs using pre-trained sequence-to-sequence Transformer models has recently led to large improvements on AMR parsing benchmarks. These parsers are simple and avoid explicit modeling of structure but lack desirable properties such as graph well-formedness guarantees or built-in graph-sentence alignments. In this work we explore the integration of general pre-trained sequence-to-sequence language models and a structure-aware... | Jiawei Zhou, Tahira Naseem, Ramón Fernandez Astudillo, YoungSuk Lee, Radu Florian, Salim Roukos |  |
| 977 |  |  [Think about it! Improving defeasible reasoning by first modeling the question scenario](https://doi.org/10.18653/v1/2021.emnlp-main.508) |  | 0 | Defeasible reasoning is the mode of reasoning where conclusions can be overturned by taking into account new evidence. Existing cognitive science literature on defeasible reasoning suggests that a person forms a “mental model” of the problem scenario before answering questions. Our research goal asks whether neural models can similarly benefit from envisioning the question scenario before answering a defeasible query. Our approach is, given a question, to have a model first create a graph of... | Aman Madaan, Niket Tandon, Dheeraj Rajagopal, Peter Clark, Yiming Yang, Eduard H. Hovy |  |
| 978 |  |  [Open Aspect Target Sentiment Classification with Natural Language Prompts](https://doi.org/10.18653/v1/2021.emnlp-main.509) |  | 0 | For many business applications, we often seek to analyze sentiments associated with any arbitrary aspects of commercial products, despite having a very limited amount of labels or even without any labels at all. However, existing aspect target sentiment classification (ATSC) models are not trainable if annotated datasets are not available. Even with labeled data, they fall short of reaching satisfactory performance. To address this, we propose simple approaches that better solve ATSC with... | Ronald Seoh, Ian Birle, Mrinal Tak, HawShiuan Chang, Brian Pinette, Alfred Hough |  |
| 979 |  |  [Does BERT Learn as Humans Perceive? Understanding Linguistic Styles through Lexica](https://doi.org/10.18653/v1/2021.emnlp-main.510) |  | 0 | People convey their intention and attitude through linguistic styles of the text that they write. In this study, we investigate lexicon usages across styles throughout two lenses: human perception and machine word importance, since words differ in the strength of the stylistic cues that they provide. To collect labels of human perception, we curate a new dataset, Hummingbird, on top of benchmarking style datasets. We have crowd workers highlight the representative words in the text that makes... | Shirley Anugrah Hayati, Dongyeop Kang, Lyle H. Ungar |  |
| 980 |  |  [Improving Stance Detection with Multi-Dataset Learning and Knowledge Distillation](https://doi.org/10.18653/v1/2021.emnlp-main.511) |  | 0 | Stance detection determines whether the author of a text is in favor of, against or neutral to a specific target and provides valuable insights into important events such as legalization of abortion. Despite significant progress on this task, one of the remaining challenges is the scarcity of annotations. Besides, most previous works focused on a hard-label training in which meaningful similarities among categories are discarded during training. To address these challenges, first, we evaluate a... | Yingjie Li, Chenye Zhao, Cornelia Caragea |  |
| 981 |  |  [Discovering the Unknown Knowns: Turning Implicit Knowledge in the Dataset into Explicit Training Examples for Visual Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.512) |  | 0 | Visual question answering (VQA) is challenging not only because the model has to handle multi-modal information, but also because it is just so hard to collect sufficient training examples — there are too many questions one can ask about an image. As a result, a VQA model trained solely on human-annotated examples could easily over-fit specific question styles or image contents that are being asked, leaving the model largely ignorant about the sheer diversity of questions. Existing methods... | Jihyung Kil, Cheng Zhang, Dong Xuan, WeiLun Chao |  |
| 982 |  |  [Improving Pre-trained Vision-and-Language Embeddings for Phrase Grounding](https://doi.org/10.18653/v1/2021.emnlp-main.513) |  | 0 | Phrase grounding aims to map textual phrases to their associated image regions, which can be a prerequisite for multimodal reasoning and can benefit tasks requiring identifying objects based on language. With pre-trained vision-and-language models achieving impressive performance across tasks, it remains unclear if we can directly utilize their learned embeddings for phrase grounding without fine-tuning. To this end, we propose a method to extract matched phrase-region pairs from pre-trained... | ZiYi Dou, Nanyun Peng |  |
| 983 |  |  [Sequential Randomized Smoothing for Adversarially Robust Speech Recognition](https://doi.org/10.18653/v1/2021.emnlp-main.514) |  | 0 | While Automatic Speech Recognition has been shown to be vulnerable to adversarial attacks, defenses against these attacks are still lagging. Existing, naive defenses can be partially broken with an adaptive attack. In classification tasks, the Randomized Smoothing paradigm has been shown to be effective at defending models. However, it is difficult to apply this paradigm to ASR tasks, due to their complexity and the sequential nature of their outputs. Our paper overcomes some of these... | Raphaël Olivier, Bhiksha Raj |  |
| 984 |  |  [Hitting your MARQ: Multimodal ARgument Quality Assessment in Long Debate Video](https://doi.org/10.18653/v1/2021.emnlp-main.515) |  | 0 | The combination of gestures, intonations, and textual content plays a key role in argument delivery. However, the current literature mostly considers textual content while assessing the quality of an argument, and it is limited to datasets containing short sequences (18-48 words). In this paper, we study argument quality assessment in a multimodal context, and experiment on DBATES, a publicly available dataset of long debate videos. First, we propose a set of interpretable debate centric... | Md. Kamrul Hasan, James Spann, Masum Hasan, Md. Saiful Islam, Kurtis Haut, Rada Mihalcea, Ehsan Hoque |  |
| 985 |  |  [Mind the Context: The Impact of Contextualization in Neural Module Networks for Grounding Visual Referring Expressions](https://doi.org/10.18653/v1/2021.emnlp-main.516) |  | 0 | Neural module networks (NMN) are a popular approach for grounding visual referring expressions. Prior implementations of NMN use pre-defined and fixed textual inputs in their module instantiation. This necessitates a large number of modules as they lack the ability to share weights and exploit associations between similar textual contexts (e.g. “dark cube on the left” vs. “black cube on the left”). In this work, we address these limitations and evaluate the impact of contextual clues in... | Arjun R. Akula, Spandana Gella, Keze Wang, SongChun Zhu, Siva Reddy |  |
| 986 |  |  [Weakly-Supervised Visual-Retriever-Reader for Knowledge-based Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.517) |  | 0 | Knowledge-based visual question answering (VQA) requires answering questions with external knowledge in addition to the content of images. One dataset that is mostly used in evaluating knowledge-based VQA is OK-VQA, but it lacks a gold standard knowledge corpus for retrieval. Existing work leverage different knowledge bases (e.g., ConceptNet and Wikipedia) to obtain external knowledge. Because of varying knowledge bases, it is hard to fairly compare models’ performance. To address this issue,... | Man Luo, Yankai Zeng, Pratyay Banerjee, Chitta Baral |  |
| 987 |  |  [NDH-Full: Learning and Evaluating Navigational Agents on Full-Length Dialogue](https://doi.org/10.18653/v1/2021.emnlp-main.518) |  | 0 | Communication between human and mobile agents is getting increasingly important as such agents are widely deployed in our daily lives. Vision-and-Dialogue Navigation is one of the tasks that evaluate the agent’s ability to interact with humans for assistance and navigate based on natural language responses. In this paper, we explore the Navigation from Dialogue History (NDH) task, which is based on the Cooperative Vision-and-Dialogue Navigation (CVDN) dataset, and present a state-of-the-art... | Hyounghun Kim, Jialu Li, Mohit Bansal |  |
| 988 |  |  [Timeline Summarization based on Event Graph Compression via Time-Aware Optimal Transport](https://doi.org/10.18653/v1/2021.emnlp-main.519) |  | 0 | Timeline Summarization identifies major events from a news collection and describes them following temporal order, with key dates tagged. Previous methods generally generate summaries separately for each date after they determine the key dates of events. These methods overlook the events’ intra-structures (arguments) and inter-structures (event-event connections). Following a different route, we propose to represent the news articles as an event-graph, thus the summarization becomes compressing... | Manling Li, Tengfei Ma, Mo Yu, Lingfei Wu, Tian Gao, Heng Ji, Kathleen R. McKeown |  |
| 989 |  |  [StreamHover: Livestream Transcript Summarization and Annotation](https://doi.org/10.18653/v1/2021.emnlp-main.520) |  | 0 | With the explosive growth of livestream broadcasting, there is an urgent need for new summarization technology that enables us to create a preview of streamed content and tap into this wealth of knowledge. However, the problem is nontrivial due to the informal nature of spoken language. Further, there has been a shortage of annotated datasets that are necessary for transcript summarization. In this paper, we present StreamHover, a framework for annotating and summarizing livestream transcripts.... | Sangwoo Cho, Franck Dernoncourt, Tim Ganter, Trung Bui, Nedim Lipka, Walter Chang, Hailin Jin, Jonathan Brandt, Hassan Foroosh, Fei Liu |  |
| 990 |  |  [Cross-Register Projection for Headline Part of Speech Tagging](https://doi.org/10.18653/v1/2021.emnlp-main.521) |  | 0 | Part of speech (POS) tagging is a familiar NLP task. State of the art taggers routinely achieve token-level accuracies of over 97% on news body text, evidence that the problem is well understood. However, the register of English news headlines, “headlinese”, is very different from the register of long-form text, causing POS tagging models to underperform on headlines. In this work, we automatically annotate news headlines with POS tags by projecting predicted tags from corresponding sentences... | Adrian Benton, Hanyang Li, Igor Malioutov |  |
| 991 |  |  [Editing Factual Knowledge in Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.522) |  | 0 | The factual knowledge acquired during pre-training and stored in the parameters of Language Models (LMs) can be useful in downstream tasks (e.g., question answering or textual inference). However, some facts can be incorrectly induced or become obsolete over time. We present KnowledgeEditor, a method which can be used to edit this knowledge and, thus, fix ‘bugs’ or unexpected predictions without the need for expensive re-training or fine-tuning. Besides being computationally efficient,... | Nicola De Cao, Wilker Aziz, Ivan Titov |  |
| 992 |  |  [Sparse Attention with Linear Units](https://doi.org/10.18653/v1/2021.emnlp-main.523) |  | 0 | Recently, it has been argued that encoder-decoder models can be made more interpretable by replacing the softmax function in the attention with its sparse variants. In this work, we introduce a novel, simple method for achieving sparsity in attention: we replace the softmax activation with a ReLU, and show that sparsity naturally emerges from such a formulation. Training stability is achieved with layer normalization with either a specialized initialization or an additional gating function. Our... | Biao Zhang, Ivan Titov, Rico Sennrich |  |
| 993 |  |  [Knowledge Base Completion Meets Transfer Learning](https://doi.org/10.18653/v1/2021.emnlp-main.524) |  | 0 | The aim of knowledge base completion is to predict unseen facts from existing facts in knowledge bases. In this work, we introduce the first approach for transfer of knowledge from one collection of facts to another without the need for entity or relation matching. The method works for both canonicalized knowledge bases and uncanonicalized or open knowledge bases, i.e., knowledge bases where more than one copy of a real-world entity or relation may exist. Such knowledge bases are a natural... | Vid Kocijan, Thomas Lukasiewicz |  |
| 994 |  |  [SPECTRA: Sparse Structured Text Rationalization](https://doi.org/10.18653/v1/2021.emnlp-main.525) |  | 0 | Selective rationalization aims to produce decisions along with rationales (e.g., text highlights or word alignments between two sentences). Commonly, rationales are modeled as stochastic binary masks, requiring sampling-based gradient estimators, which complicates training and requires careful hyperparameter tuning. Sparse attention mechanisms are a deterministic alternative, but they lack a way to regularize the rationale extraction (e.g., to control the sparsity of a text highlight or the... | Nuno Miguel Guerreiro, André F. T. Martins |  |
| 995 |  |  [Towards Zero-Shot Knowledge Distillation for Natural Language Processing](https://doi.org/10.18653/v1/2021.emnlp-main.526) |  | 0 | Knowledge distillation (KD) is a common knowledge transfer algorithm used for model compression across a variety of deep learning based natural language processing (NLP) solutions. In its regular manifestations, KD requires access to the teacher’s training data for knowledge transfer to the student network. However, privacy concerns, data regulations and proprietary reasons may prevent access to such data. We present, to the best of our knowledge, the first work on Zero-shot Knowledge... | Ahmad Rashid, Vasileios Lioutas, Abbas Ghaddar, Mehdi Rezagholizadeh |  |
| 996 |  |  [Adversarial Regularization as Stackelberg Game: An Unrolled Optimization Approach](https://doi.org/10.18653/v1/2021.emnlp-main.527) |  | 0 | Adversarial regularization has been shown to improve the generalization performance of deep learning models in various natural language processing tasks. Existing works usually formulate the method as a zero-sum game, which is solved by alternating gradient descent/ascent algorithms. Such a formulation treats the adversarial and the defending players equally, which is undesirable because only the defending player contributes to the generalization performance. To address this issue, we propose... | Simiao Zuo, Chen Liang, Haoming Jiang, Xiaodong Liu, Pengcheng He, Jianfeng Gao, Weizhu Chen, Tuo Zhao |  |
| 997 |  |  [Aspect-Controllable Opinion Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.528) |  | 0 | Recent work on opinion summarization produces general summaries based on a set of input reviews and the popularity of opinions expressed in them. In this paper, we propose an approach that allows the generation of customized summaries based on aspect queries (e.g., describing the location and room of a hotel). Using a review corpus, we create a synthetic training dataset of (review, summary) pairs enriched with aspect controllers which are induced by a multi-instance learning model that... | Reinald Kim Amplayo, Stefanos Angelidis, Mirella Lapata |  |
| 998 |  |  [QuestEval: Summarization Asks for Fact-based Evaluation](https://doi.org/10.18653/v1/2021.emnlp-main.529) |  | 0 | Summarization evaluation remains an open research problem: current metrics such as ROUGE are known to be limited and to correlate poorly with human judgments. To alleviate this issue, recent work has proposed evaluation metrics which rely on question answering models to assess whether a summary contains all the relevant information in its source document. Though promising, the proposed approaches have so far failed to correlate better than ROUGE with human judgments. In this paper, we extend... | Thomas Scialom, PaulAlexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, Patrick Gallinari |  |
| 999 |  |  [Simple Conversational Data Augmentation for Semi-supervised Abstractive Dialogue Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.530) |  | 0 | Abstractive conversation summarization has received growing attention while most current state-of-the-art summarization models heavily rely on human-annotated summaries. To reduce the dependence on labeled summaries, in this work, we present a simple yet effective set of Conversational Data Augmentation (CODA) methods for semi-supervised abstractive conversation summarization, such as random swapping/deletion to perturb the discourse relations inside conversations, dialogue-acts-guided... | Jiaao Chen, Diyi Yang |  |
| 1000 |  |  [Finding a Balanced Degree of Automation for Summary Evaluation](https://doi.org/10.18653/v1/2021.emnlp-main.531) |  | 0 | Human evaluation for summarization tasks is reliable but brings in issues of reproducibility and high costs. Automatic metrics are cheap and reproducible but sometimes poorly correlated with human judgment. In this work, we propose flexible semiautomatic to automatic summary evaluation metrics, following the Pyramid human evaluation method. Semi-automatic Lite2Pyramid retains the reusable human-labeled Summary Content Units (SCUs) for reference(s) but replaces the manual work of judging SCUs’... | Shiyue Zhang, Mohit Bansal |  |
| 1001 |  |  [CLIFF: Contrastive Learning for Improving Faithfulness and Factuality in Abstractive Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.532) |  | 0 | We study generating abstractive summaries that are faithful and factually consistent with the given articles. A novel contrastive learning formulation is presented, which leverages both reference summaries, as positive training data, and automatically generated erroneous summaries, as negative training data, to train summarization systems that are better at distinguishing between them. We further design four types of strategies for creating negative samples, to resemble errors made commonly by... | Shuyang Cao, Lu Wang |  |
| 1002 |  |  [Multilingual Unsupervised Neural Machine Translation with Denoising Adapters](https://doi.org/10.18653/v1/2021.emnlp-main.533) |  | 0 | We consider the problem of multilingual unsupervised machine translation, translating to and from languages that only have monolingual data by using auxiliary parallel language pairs. For this problem the standard procedure so far to leverage the monolingual data is _back-translation_, which is computationally costly and hard to tune. In this paper we propose instead to use _denoising adapters_, adapter layers with a denoising objective, on top of pre-trained mBART-50. In addition to the... | Ahmet Üstün, Alexandre Berard, Laurent Besacier, Matthias Gallé |  |
| 1003 |  |  [BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.534) |  | 0 | The success of bidirectional encoders using masked language models, such as BERT, on numerous natural language processing tasks has prompted researchers to attempt to incorporate these pre-trained models into neural machine translation (NMT) systems. However, proposed methods for incorporating pre-trained models are non-trivial and mainly focus on BERT, which lacks a comparison of the impact that other pre-trained models may have on translation performance. In this paper, we demonstrate that... | Haoran Xu, Benjamin Van Durme, Kenton W. Murray |  |
| 1004 |  |  [Controlling Machine Translation for Multiple Attributes with Additive Interventions](https://doi.org/10.18653/v1/2021.emnlp-main.535) |  | 0 | Fine-grained control of machine translation (MT) outputs along multiple attributes is critical for many modern MT applications and is a requirement for gaining users’ trust. A standard approach for exerting control in MT is to prepend the input with a special tag to signal the desired output attribute. Despite its simplicity, attribute tagging has several drawbacks: continuous values must be binned into discrete categories, which is unnatural for certain applications; interference between... | Andrea Schioppa, David Vilar, Artem Sokolov, Katja Filippova |  |
| 1005 |  |  [A Generative Framework for Simultaneous Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.536) |  | 0 | We propose a generative framework for simultaneous machine translation. Conventional approaches use a fixed number of source words to translate or learn dynamic policies for the number of source words by reinforcement learning. Here we formulate simultaneous translation as a structural sequence-to-sequence learning problem. A latent variable is introduced to model read or translate actions at every time step, which is then integrated out to consider all the possible translation policies. A... | Yishu Miao, Phil Blunsom, Lucia Specia |  |
| 1006 |  |  [It Is Not As Good As You Think! Evaluating Simultaneous Machine Translation on Interpretation Data](https://doi.org/10.18653/v1/2021.emnlp-main.537) |  | 0 | Most existing simultaneous machine translation (SiMT) systems are trained and evaluated on offline translation corpora. We argue that SiMT systems should be trained and tested on real interpretation data. To illustrate this argument, we propose an interpretation test set and conduct a realistic evaluation of SiMT trained on offline translations. Our results, on our test set along with 3 existing smaller scale language pairs, highlight the difference of up-to 13.83 BLEU score when SiMT models... | Jinming Zhao, Philip Arthur, Gholamreza Haffari, Trevor Cohn, Ehsan Shareghi |  |
| 1007 |  |  [Boosting Cross-Lingual Transfer via Self-Learning with Uncertainty Estimation](https://doi.org/10.18653/v1/2021.emnlp-main.538) |  | 0 | Recent multilingual pre-trained language models have achieved remarkable zero-shot performance, where the model is only finetuned on one source language and directly evaluated on target languages. In this work, we propose a self-learning framework that further utilizes unlabeled data of target languages, combined with uncertainty estimation in the process to select high-quality silver labels. Three different uncertainties are adapted and analyzed specifically for the cross lingual transfer:... | Liyan Xu, Xuchao Zhang, Xujiang Zhao, Haifeng Chen, Feng Chen, Jinho D. Choi |  |
| 1008 |  |  [Levenshtein Training for Word-level Quality Estimation](https://doi.org/10.18653/v1/2021.emnlp-main.539) |  | 0 | We propose a novel scheme to use the Levenshtein Transformer to perform the task of word-level quality estimation. A Levenshtein Transformer is a natural fit for this task: trained to perform decoding in an iterative manner, a Levenshtein Transformer can learn to post-edit without explicit supervision. To further minimize the mismatch between the translation task and the word-level QE task, we propose a two-stage transfer learning procedure on both augmented data and human post-editing data. We... | Shuoyang Ding, Marcin JunczysDowmunt, Matt Post, Philipp Koehn |  |
| 1009 |  |  [Interactive Machine Comprehension with Dynamic Knowledge Graphs](https://doi.org/10.18653/v1/2021.emnlp-main.540) |  | 0 | Interactive machine reading comprehension (iMRC) is machine comprehension tasks where knowledge sources are partially observable. An agent must interact with an environment sequentially to gather necessary knowledge in order to answer a question. We hypothesize that graph representations are good inductive biases, which can serve as an agent’s memory mechanism in iMRC tasks. We explore four different categories of graphs that can capture text information at various levels. We describe methods... | Xingdi Yuan |  |
| 1010 |  |  [Residual Adapters for Parameter-Efficient ASR Adaptation to Atypical and Accented Speech](https://doi.org/10.18653/v1/2021.emnlp-main.541) |  | 0 | Automatic Speech Recognition (ASR) systems are often optimized to work best for speakers with canonical speech patterns. Unfortunately, these systems perform poorly when tested on atypical speech and heavily accented speech. It has previously been shown that personalization through model fine-tuning substantially improves performance. However, maintaining such large models per speaker is costly and difficult to scale. We show that by adding a relatively small number of extra parameters to the... | Katrin Tomanek, Vicky Zayats, Dirk Padfield, Kara Vaillancourt, Fadi Biadsy |  |
| 1011 |  |  [Visual News: Benchmark and Challenges in News Image Captioning](https://doi.org/10.18653/v1/2021.emnlp-main.542) |  | 0 | We propose Visual News Captioner, an entity-aware model for the task of news image captioning. We also introduce Visual News, a large-scale benchmark consisting of more than one million news images along with associated news articles, image captions, author information, and other metadata. Unlike the standard image captioning task, news images depict situations where people, locations, and events are of paramount importance. Our proposed method can effectively combine visual and textual... | Fuxiao Liu, Yinghan Wang, Tianlu Wang, Vicente Ordonez |  |
| 1012 |  |  [Integrating Visuospatial, Linguistic, and Commonsense Structure into Story Visualization](https://doi.org/10.18653/v1/2021.emnlp-main.543) |  | 0 | While much research has been done in text-to-image synthesis, little work has been done to explore the usage of linguistic structure of the input text. Such information is even more important for story visualization since its inputs have an explicit narrative structure that needs to be translated into an image sequence (or visual story). Prior work in this domain has shown that there is ample room for improvement in the generated image sequence in terms of visual quality, consistency and... | Adyasha Maharana, Mohit Bansal |  |
| 1013 |  |  [VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding](https://doi.org/10.18653/v1/2021.emnlp-main.544) |  | 0 | We present VideoCLIP, a contrastive approach to pre-train a unified model for zero-shot video and text understanding, without using any labels on downstream tasks. VideoCLIP trains a transformer for video and text by contrasting temporally overlapping positive video-text pairs with hard negatives from nearest neighbor retrieval. Our experiments on a diverse series of downstream tasks, including sequence-level text-video retrieval, VideoQA, token-level action localization, and action... | Hu Xu, Gargi Ghosh, PoYao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, Christoph Feichtenhofer |  |
| 1014 |  |  [NewsCLIPpings: Automatic Generation of Out-of-Context Multimodal Media](https://doi.org/10.18653/v1/2021.emnlp-main.545) |  | 0 | Online misinformation is a prevalent societal issue, with adversaries relying on tools ranging from cheap fakes to sophisticated deep fakes. We are motivated by the threat scenario where an image is used out of context to support a certain narrative. While some prior datasets for detecting image-text inconsistency generate samples via text manipulation, we propose a dataset where both image and text are unmanipulated but mismatched. We introduce several strategies for automatically retrieving... | Grace Luo, Trevor Darrell, Anna Rohrbach |  |
| 1015 |  |  [Powering Comparative Classification with Sentiment Analysis via Domain Adaptive Knowledge Transfer](https://doi.org/10.18653/v1/2021.emnlp-main.546) |  | 0 | We study Comparative Preference Classification (CPC) which aims at predicting whether a preference comparison exists between two entities in a given sentence and, if so, which entity is preferred over the other. High-quality CPC models can significantly benefit applications such as comparative question answering and review-based recommendation. Among the existing approaches, non-deep learning methods suffer from inferior performances. The state-of-the-art graph neural network-based ED-GAT (Ma... | Zeyu Li, Yilong Qin, Zihan Liu, Wei Wang |  |
| 1016 |  |  [Tribrid: Stance Classification with Neural Inconsistency Detection](https://doi.org/10.18653/v1/2021.emnlp-main.547) |  | 0 | We study the problem of performing automatic stance classification on social media with neural architectures such as BERT. Although these architectures deliver impressive results, their level is not yet comparable to the one of humans and they might produce errors that have a significant impact on the downstream task (e.g., fact-checking). To improve the performance, we present a new neural architecture where the input also includes automatically generated negated perspectives over a given... | Song Yang, Jacopo Urbani |  |
| 1017 |  |  [SYSML: StYlometry with Structure and Multitask Learning: Implications for Darknet Forum Migrant Analysis](https://doi.org/10.18653/v1/2021.emnlp-main.548) |  | 0 | Darknet market forums are frequently used to exchange illegal goods and services between parties who use encryption to conceal their identities. The Tor network is used to host these markets, which guarantees additional anonymization from IP and location tracking, making it challenging to link across malicious users using multiple accounts (sybils). Additionally, users migrate to new forums when one is closed further increasing the difficulty of linking users across multiple forums. We develop... | Pranav Maneriker, Yuntian He, Srinivasan Parthasarathy |  |
| 1018 |  |  [Few-Shot Emotion Recognition in Conversation with Sequential Prototypical Networks](https://doi.org/10.18653/v1/2021.emnlp-main.549) |  | 0 | Several recent studies on dyadic human-human interactions have been done on conversations without specific business objectives. However, many companies might benefit from studies dedicated to more precise environments such as after sales services or customer satisfaction surveys. In this work, we place ourselves in the scope of a live chat customer service in which we want to detect emotions and their evolution in the conversation flow. This context leads to multiple challenges that range from... | Gaël Guibon, Matthieu Labeau, Hélène Flamein, Luce Lefeuvre, Chloé Clavel |  |
| 1019 |  |  [CLASSIC: Continual and Contrastive Learning of Aspect Sentiment Classification Tasks](https://doi.org/10.18653/v1/2021.emnlp-main.550) |  | 0 | This paper studies continual learning (CL) of a sequence of aspect sentiment classification (ASC) tasks in a particular CL setting called domain incremental learning (DIL). Each task is from a different domain or product. The DIL setting is particularly suited to ASC because in testing the system needs not know the task/domain to which the test data belongs. To our knowledge, this setting has not been studied before for ASC. This paper proposes a novel model called CLASSIC. The key novelty is a... | Zixuan Ke, Bing Liu, Hu Xu, Lei Shu |  |
| 1020 |  |  [Implicit Sentiment Analysis with Event-centered Text Representation](https://doi.org/10.18653/v1/2021.emnlp-main.551) |  | 0 | Implicit sentiment analysis, aiming at detecting the sentiment of a sentence without sentiment words, has become an attractive research topic in recent years. In this paper, we focus on event-centric implicit sentiment analysis that utilizes the sentiment-aware event contained in a sentence to infer its sentiment polarity. Most existing methods in implicit sentiment analysis simply view noun phrases or entities in text as events or indirectly model events with sophisticated models. Since events... | Deyu Zhou, Jianan Wang, Linhai Zhang, Yulan He |  |
| 1021 |  |  [SimCSE: Simple Contrastive Learning of Sentence Embeddings](https://doi.org/10.18653/v1/2021.emnlp-main.552) |  | 0 | This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation... | Tianyu Gao, Xingcheng Yao, Danqi Chen |  |
| 1022 |  |  [When is Wall a Pared and when a Muro?: Extracting Rules Governing Lexical Selection](https://doi.org/10.18653/v1/2021.emnlp-main.553) |  | 0 | Learning fine-grained distinctions between vocabulary items is a key challenge in learning a new language. For example, the noun “wall” has different lexical manifestations in Spanish – “pared” refers to an indoor wall while “muro” refers to an outside wall. However, this variety of lexical distinction may not be obvious to non-native learners unless the distinction is explained in such a way. In this work, we present a method for automatically identifying fine-grained lexical distinctions, and... | Aditi Chaudhary, Kayo Yin, Antonios Anastasopoulos, Graham Neubig |  |
| 1023 |  |  [Aligning Actions Across Recipe Graphs](https://doi.org/10.18653/v1/2021.emnlp-main.554) |  | 0 | Recipe texts are an idiosyncratic form of instructional language that pose unique challenges for automatic understanding. One challenge is that a cooking step in one recipe can be explained in another recipe in different words, at a different level of abstraction, or not at all. Previous work has annotated correspondences between recipe instructions at the sentence level, often glossing over important correspondences between cooking steps across recipes. We present a novel and fully-parsed... | Lucia Donatelli, Theresa Schmidt, Debanjali Biswas, Arne Köhn, Fangzhou Zhai, Alexander Koller |  |
| 1024 |  |  [Generating Datasets with Pretrained Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.555) |  | 0 | To obtain high-quality sentence embeddings from pretrained language models (PLMs), they must either be augmented with additional pretraining objectives or finetuned on a large set of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to generate suitable datasets of sufficient size. In this paper, we show how PLMs can be leveraged to obtain high-quality sentence embeddings without the need for labeled data, finetuning or modifications... | Timo Schick, Hinrich Schütze |  |
| 1025 |  |  [Continuous Entailment Patterns for Lexical Inference in Context](https://doi.org/10.18653/v1/2021.emnlp-main.556) |  | 0 | Combining a pretrained language model (PLM) with textual patterns has been shown to help in both zero- and few-shot settings. For zero-shot performance, it makes sense to design patterns that closely resemble the text seen during self-supervised pretraining because the model has never seen anything else. Supervised training allows for more flexibility. If we allow for tokens outside the PLM’s vocabulary, patterns can be adapted more flexibly to a PLM’s idiosyncrasies. Contrasting patterns where... | Martin Schmitt, Hinrich Schütze |  |
| 1026 |  |  [Numeracy enhances the Literacy of Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.557) |  | 0 | Specialized number representations in NLP have shown improvements on numerical reasoning tasks like arithmetic word problems and masked number prediction. But humans also use numeracy to make better sense of world concepts, e.g., you can seat 5 people in your ‘room’ but not 500. Does a better grasp of numbers improve a model’s understanding of other concepts and words? This paper studies the effect of using six different number encoders on the task of masked word prediction (MWP), as a proxy... | Avijit Thawani, Jay Pujara, Filip Ilievski |  |
| 1027 |  |  [Students Who Study Together Learn Better: On the Importance of Collective Knowledge Distillation for Domain Transfer in Fact Verification](https://doi.org/10.18653/v1/2021.emnlp-main.558) |  | 0 | While neural networks produce state-of-the- art performance in several NLP tasks, they generally depend heavily on lexicalized information, which transfer poorly between domains. Previous works have proposed delexicalization as a form of knowledge distillation to reduce the dependency on such lexical artifacts. However, a critical unsolved issue that remains is how much delexicalization to apply: a little helps reduce overfitting, but too much discards useful information. We propose Group... | Mitch Paul Mithun, Sandeep Suntwal, Mihai Surdeanu |  |
| 1028 |  |  [MultiEURLEX - A multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer](https://doi.org/10.18653/v1/2021.emnlp-main.559) |  | 0 | We introduce MULTI-EURLEX, a new multilingual dataset for topic classification of legal documents. The dataset comprises 65k European Union (EU) laws, officially translated in 23 languages, annotated with multiple labels from the EUROVOC taxonomy. We highlight the effect of temporal concept drift and the importance of chronological, instead of random splits. We use the dataset as a testbed for zero-shot cross-lingual transfer, where we exploit annotated training documents in one language... | Ilias Chalkidis, Manos Fergadiotis, Ion Androutsopoulos |  |
| 1029 |  |  [Joint Passage Ranking for Diverse Multi-Answer Retrieval](https://doi.org/10.18653/v1/2021.emnlp-main.560) |  | 0 | We study multi-answer retrieval, an under-explored problem that requires retrieving passages to cover multiple distinct answers for a given question. This task requires joint modeling of retrieved passages, as models should not repeatedly retrieve passages containing the same answer at the cost of missing a different valid answer. Prior work focusing on single-answer retrieval is limited as it cannot reason about the set of passages jointly. In this paper, we introduce JPR, a joint passage... | Sewon Min, Kenton Lee, MingWei Chang, Kristina Toutanova, Hannaneh Hajishirzi |  |
| 1030 |  |  [Generative Context Pair Selection for Multi-hop Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.561) |  | 0 | Compositional reasoning tasks such as multi-hop question answering require models to learn how to make latent decisions using only weak supervision from the final answer. Crowdsourced datasets gathered for these tasks, however, often contain only a slice of the underlying task distribution, which can induce unanticipated biases such as shallow word overlap between the question and context. Recent works have shown that discriminative training results in models that exploit these underlying... | Dheeru Dua, Cícero Nogueira dos Santos, Patrick Ng, Ben Athiwaratkun, Bing Xiang, Matt Gardner, Sameer Singh |  |
| 1031 |  |  [Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.562) |  | 0 | Coupled with the availability of large scale datasets, deep learning architectures have enabled rapid progress on the Question Answering task. However, most of those datasets are in English, and the performances of state-of-the-art multilingual models are significantly lower when evaluated on non-English data. Due to high data collection costs, it is not realistic to obtain annotated data for each language one desires to support. We propose a method to improve the Cross-lingual Question... | Arij Riabi, Thomas Scialom, Rachel Keraron, Benoît Sagot, Djamé Seddah, Jacopo Staiano |  |
| 1032 |  |  [Have You Seen That Number? Investigating Extrapolation in Question Answering Models](https://doi.org/10.18653/v1/2021.emnlp-main.563) |  | 0 | Numerical reasoning in machine reading comprehension (MRC) has shown drastic improvements over the past few years. While the previous models for numerical MRC are able to interpolate the learned numerical reasoning capabilities, it is not clear whether they can perform just as well on numbers unseen in the training dataset. Our work rigorously tests state-of-the-art models on DROP, a numerical MRC dataset, to see if they can handle passages that contain out-of-range numbers. One of the key... | Jeonghwan Kim, Giwon Hong, KyungMin Kim, Junmo Kang, SungHyon Myaeng |  |
| 1033 |  |  [Surface Form Competition: Why the Highest Probability Answer Isn't Always Right](https://doi.org/10.18653/v1/2021.emnlp-main.564) |  | 0 | Large language models have shown promising results in zero-shot settings. For example, they can perform multiple choice tasks simply by conditioning on a question and selecting the answer with the highest probability. However, ranking by string probability can be problematic due to surface form competition—wherein different surface forms compete for probability mass, even if they represent the same underlying concept in a given context, e.g. “computer” and “PC.” Since probability mass is... | Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, Luke Zettlemoyer |  |
| 1034 |  |  [Entity-Based Knowledge Conflicts in Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.565) |  | 0 | Knowledge-dependent tasks typically use two sources of knowledge: parametric, learned at training time, and contextual, given as a passage at inference time. To understand how models use these sources together, we formalize the problem of knowledge conflicts, where the contextual information contradicts the learned information. Analyzing the behaviour of popular models, we measure their over-reliance on memorized information (the cause of hallucinations), and uncover important factors that... | Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, Sameer Singh |  |
| 1035 |  |  [Back-Training excels Self-Training at Unsupervised Domain Adaptation of Question Generation and Passage Retrieval](https://doi.org/10.18653/v1/2021.emnlp-main.566) |  | 0 | In this work, we introduce back-training, an alternative to self-training for unsupervised domain adaptation (UDA). While self-training generates synthetic training data where natural inputs are aligned with noisy outputs, back-training results in natural outputs aligned with noisy inputs. This significantly reduces the gap between target domain and synthetic data distribution, and reduces model overfitting to source domain. We run UDA experiments on question generation and passage retrieval... | Devang Kulshreshtha, Robert Belfer, Iulian Vlad Serban, Siva Reddy |  |
| 1036 |  |  [DWUG: A large Resource of Diachronic Word Usage Graphs in Four Languages](https://doi.org/10.18653/v1/2021.emnlp-main.567) |  | 0 | Word meaning is notoriously difficult to capture, both synchronically and diachronically. In this paper, we describe the creation of the largest resource of graded contextualized, diachronic word meaning annotation in four different languages, based on 100,000 human semantic proximity judgments. We describe in detail the multi-round incremental annotation process, the choice for a clustering algorithm to group usages into senses, and possible – diachronic and synchronic – uses for this dataset. | Dominik Schlechtweg, Nina Tahmasebi, Simon Hengchen, Haim Dubossarsky, Barbara McGillivray |  |
| 1037 |  |  [I Wish I Would Have Loved This One, But I Didn't - A Multilingual Dataset for Counterfactual Detection in Product Review](https://doi.org/10.18653/v1/2021.emnlp-main.568) |  | 0 | Counterfactual statements describe events that did not or cannot take place. We consider the problem of counterfactual detection (CFD) in product reviews. For this purpose, we annotate a multilingual CFD dataset from Amazon product reviews covering counterfactual statements written in English, German, and Japanese languages. The dataset is unique as it contains counterfactuals in multiple languages, covers a new application area of e-commerce reviews, and provides high quality professional... | James O'Neill, Polina Rozenshtein, Ryuichi Kiryo, Motoko Kubota, Danushka Bollegala |  |
| 1038 |  |  [Does It Capture STEL? A Modular, Similarity-based Linguistic Style Evaluation Framework](https://doi.org/10.18653/v1/2021.emnlp-main.569) |  | 0 | Style is an integral part of natural language. However, evaluation methods for style measures are rare, often task-specific and usually do not control for content. We propose the modular, fine-grained and content-controlled similarity-based STyle EvaLuation framework (STEL) to test the performance of any model that can compare two sentences on style. We illustrate STEL with two general dimensions of style (formal/informal and simple/complex) as well as two specific characteristics of style... | Anna Wegmann, Dong Nguyen |  |
| 1039 |  |  [Evaluating the Morphosyntactic Well-formedness of Generated Texts](https://doi.org/10.18653/v1/2021.emnlp-main.570) |  | 0 | Text generation systems are ubiquitous in natural language processing applications. However, evaluation of these systems remains a challenge, especially in multilingual settings. In this paper, we propose L’AMBRE – a metric to evaluate the morphosyntactic well-formedness of text using its dependency parse and morphosyntactic rules of the language. We present a way to automatically extract various rules governing morphosyntax directly from dependency treebanks. To tackle the noisy outputs from... | Adithya Pratapa, Antonios Anastasopoulos, Shruti Rijhwani, Aditi Chaudhary, David R. Mortensen, Graham Neubig, Yulia Tsvetkov |  |
| 1040 |  |  [AM2iCo: Evaluating Word Meaning in Context across Low-Resource Languages with Adversarial Examples](https://doi.org/10.18653/v1/2021.emnlp-main.571) |  | 0 | Capturing word meaning in context and distinguishing between correspondences and variations across languages is key to building successful multilingual and cross-lingual text representation models. However, existing multilingual evaluation datasets that evaluate lexical semantics “in-context” have various limitations. In particular, 1) their language coverage is restricted to high-resource languages and skewed in favor of only a few language families and areas, 2) a design that makes the task... | Qianchu Liu, Edoardo Maria Ponti, Diana McCarthy, Ivan Vulic, Anna Korhonen |  |
| 1041 |  |  [CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP](https://doi.org/10.18653/v1/2021.emnlp-main.572) |  | 0 | Humans can learn a new language task efficiently with only few examples, by leveraging their knowledge obtained when learning prior tasks. In this paper, we explore whether and how such cross-task generalization ability can be acquired, and further applied to build better few-shot learners across diverse NLP tasks. We introduce CrossFit, a problem setup for studying cross-task generalization ability, which standardizes seen/unseen task partitions, data access during different learning stages,... | Qinyuan Ye, Bill Yuchen Lin, Xiang Ren |  |
| 1042 |  |  [On the Influence of Masking Policies in Intermediate Pre-training](https://doi.org/10.18653/v1/2021.emnlp-main.573) |  | 0 | Current NLP models are predominantly trained through a two-stage “pre-train then fine-tune” pipeline. Prior work has shown that inserting an intermediate pre-training stage, using heuristic masking policies for masked language modeling (MLM), can significantly improve final performance. However, it is still unclear (1) in what cases such intermediate pre-training is helpful, (2) whether hand-crafted heuristic objectives are optimal for a given task, and (3) whether a masking policy designed for... | Qinyuan Ye, Belinda Z. Li, Sinong Wang, Benjamin Bolte, Hao Ma, Wentau Yih, Xiang Ren, Madian Khabsa |  |
| 1043 |  |  [ValNorm Quantifies Semantics to Reveal Consistent Valence Biases Across Languages and Over Centuries](https://doi.org/10.18653/v1/2021.emnlp-main.574) |  | 0 | Word embeddings learn implicit biases from linguistic regularities captured by word co-occurrence statistics. By extending methods that quantify human-like biases in word embeddings, we introduce ValNorm, a novel intrinsic evaluation task and method to quantify the valence dimension of affect in human-rated word sets from social psychology. We apply ValNorm on static word embeddings from seven languages (Chinese, English, German, Polish, Portuguese, Spanish, and Turkish) and from historical... | Autumn Toney, Aylin Caliskan |  |
| 1044 |  |  [Perturbation CheckLists for Evaluating NLG Evaluation Metrics](https://doi.org/10.18653/v1/2021.emnlp-main.575) |  | 0 | Natural Language Generation (NLG) evaluation is a multifaceted task requiring assessment of multiple desirable criteria, e.g., fluency, coherency, coverage, relevance, adequacy, overall quality, etc. Across existing datasets for 6 NLG tasks, we observe that the human evaluation scores on these multiple criteria are often not correlated. For example, there is a very low correlation between human scores on fluency and data coverage for the task of structured data to text generation. This suggests... | Ananya B. Sai, Tanay Dixit, Dev Yashpal Sheth, Sreyas Mohan, Mitesh M. Khapra |  |
| 1045 |  |  [Robust Open-Vocabulary Translation from Visual Text Representations](https://doi.org/10.18653/v1/2021.emnlp-main.576) |  | 0 | Machine translation models have discrete vocabularies and commonly use subword segmentation techniques to achieve an ‘open vocabulary.’ This approach relies on consistent and correct underlying unicode sequences, and makes models susceptible to degradation from common types of noise and variation. Motivated by the robustness of human language processing, we propose the use of visual text representations, which dispense with a finite set of text embeddings in favor of continuous vocabularies... | Elizabeth Salesky, David Etter, Matt Post |  |
| 1046 |  |  [Don't Go Far Off: An Empirical Study on Neural Poetry Translation](https://doi.org/10.18653/v1/2021.emnlp-main.577) |  | 0 | Despite constant improvements in machine translation quality, automatic poetry translation remains a challenging problem due to the lack of open-sourced parallel poetic corpora, and to the intrinsic complexities involved in preserving the semantics, style and figurative nature of poetry. We present an empirical investigation for poetry translation along several dimensions: 1) size and style of training data (poetic vs. non-poetic), including a zero-shot setup; 2) bilingual vs. multilingual... | Tuhin Chakrabarty, Arkadiy Saakyan, Smaranda Muresan |  |
| 1047 |  |  [Improving Multilingual Translation by Representation and Gradient Regularization](https://doi.org/10.18653/v1/2021.emnlp-main.578) |  | 0 | Multilingual Neural Machine Translation (NMT) enables one model to serve all translation directions, including ones that are unseen during training, i.e. zero-shot translation. Despite being theoretically attractive, current models often produce low quality translations – commonly failing to even produce outputs in the right target language. In this work, we observe that off-target translation is dominant even in strong multilingual systems, trained on massive multilingual corpora. To address... | Yilin Yang, Akiko Eriguchi, Alexandre Muzio, Prasad Tadepalli, Stefan Lee, Hany Hassan |  |
| 1048 |  |  [Learning Kernel-Smoothed Machine Translation with Retrieved Examples](https://doi.org/10.18653/v1/2021.emnlp-main.579) |  | 0 | How to effectively adapt neural machine translation (NMT) models according to emerging cases without retraining? Despite the great success of neural machine translation, updating the deployed models online remains a challenge. Existing non-parametric approaches that retrieve similar examples from a database to guide the translation process are promising but are prone to overfit the retrieved examples. However, non-parametric methods are prone to overfit the retrieved examples. In this work, we... | Qingnan Jiang, Mingxuan Wang, Jun Cao, Shanbo Cheng, Shujian Huang, Lei Li |  |
| 1049 |  |  [Uncertainty-Aware Balancing for Multilingual and Multi-Domain Neural Machine Translation Training](https://doi.org/10.18653/v1/2021.emnlp-main.580) |  | 0 | Learning multilingual and multi-domain translation model is challenging as the heterogeneous and imbalanced data make the model converge inconsistently over different corpora in real world. One common practice is to adjust the share of each corpus in the training, so that the learning process is balanced and low-resource cases can benefit from the high resource ones. However, automatic balancing methods usually depend on the intra- and inter-dataset characteristics, which is usually agnostic or... | Minghao Wu, Yitong Li, Meng Zhang, Liangyou Li, Gholamreza Haffari, Qun Liu |  |
| 1050 |  |  [Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy](https://doi.org/10.18653/v1/2021.emnlp-main.581) |  | 0 | Simultaneous machine translation (SiMT) generates translation before reading the entire source sentence and hence it has to trade off between translation quality and latency. To fulfill the requirements of different translation quality and latency in practical applications, the previous methods usually need to train multiple SiMT models for different latency levels, resulting in large computational costs. In this paper, we propose a universal SiMT model with Mixture-of-Experts Wait-k Policy to... | Shaolei Zhang, Yang Feng |  |
| 1051 |  |  [How much coffee was consumed during EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI](https://doi.org/10.18653/v1/2021.emnlp-main.582) |  | 0 | Many real-world problems require the combined application of multiple reasoning abilities—employing suitable abstractions, commonsense knowledge, and creative synthesis of problem-solving strategies. To help advance AI systems towards such capabilities, we propose a new reasoning challenge, namely Fermi Problems (FPs), which are questions whose answers can only be approximately estimated because their precise computation is either impractical or impossible. For example, “How much would the sea... | Ashwin Kalyan, Abhinav Kumar, Arjun Chandrasekaran, Ashish Sabharwal, Peter Clark |  |
| 1052 |  |  [Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.583) |  | 0 | In this paper we propose a novel approach towards improving the efficiency of Question Answering (QA) systems by filtering out questions that will not be answered by them. This is based on an interesting new finding: the answer confidence scores of state-of-the-art QA systems can be approximated well by models solely using the input question text. This enables preemptive filtering of questions that are not answered by the system due to their answer confidence scores being lower than the system... | Siddhant Garg, Alessandro Moschitti |  |
| 1053 |  |  [Learning with Instance Bundles for Reading Comprehension](https://doi.org/10.18653/v1/2021.emnlp-main.584) |  | 0 | When training most modern reading comprehension models, all the questions associated with a context are treated as being independent from each other. However, closely related questions and their corresponding answers are not independent, and leveraging these relationships could provide a strong supervision signal to a model. Drawing on ideas from contrastive estimation, we introduce several new supervision losses that compare question-answer scores across multiple related instances.... | Dheeru Dua, Pradeep Dasigi, Sameer Singh, Matt Gardner |  |
| 1054 |  |  [Explaining Answers with Entailment Trees](https://doi.org/10.18653/v1/2021.emnlp-main.585) |  | 0 | Our goal, in the context of open-domain textual question-answering (QA), is to explain answers by showing the line of reasoning from what is known to the answer, rather than simply showing a fragment of textual evidence (a “rationale”). If this could be done, new opportunities for understanding and debugging the system’s reasoning become possible. Our approach is to generate explanations in the form of entailment trees, namely a tree of multipremise entailment steps from facts that are known,... | Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, Peter Clark |  |
| 1055 |  |  [SituatedQA: Incorporating Extra-Linguistic Contexts into QA](https://doi.org/10.18653/v1/2021.emnlp-main.586) |  | 0 | Answers to the same question may change depending on the extra-linguistic contexts (when and where the question was asked). To study this challenge, we introduce SituatedQA, an open-retrieval QA dataset where systems must produce the correct answer to a question given the temporal or geographical context. To construct SituatedQA, we first identify such questions in existing QA datasets. We find that a significant proportion of information seeking questions have context-dependent answers (e.g.... | Michael J. Q. Zhang, Eunsol Choi |  |
| 1056 |  |  [ConvAbuse: Data, Analysis, and Benchmarks for Nuanced Detection in Conversational AI](https://doi.org/10.18653/v1/2021.emnlp-main.587) |  | 0 | We present the first English corpus study on abusive language towards three conversational AI systems gathered ‘in the wild’: an open-domain social bot, a rule-based chatbot, and a task-based system. To account for the complexity of the task, we take a more ‘nuanced’ approach where our ConvAI dataset reflects fine-grained notions of abuse, as well as views from multiple expert annotators. We find that the distribution of abuse is vastly different compared to other commonly used datasets, with... | Amanda Cercas Curry, Gavin Abercrombie, Verena Rieser |  |
| 1057 |  |  [Conversational Multi-Hop Reasoning with Neural Commonsense Knowledge and Symbolic Logic Rules](https://doi.org/10.18653/v1/2021.emnlp-main.588) |  | 0 | One of the challenges faced by conversational agents is their inability to identify unstated presumptions of their users’ commands, a task trivial for humans due to their common sense. In this paper, we propose a zero-shot commonsense reasoning system for conversational agents in an attempt to achieve this. Our reasoner uncovers unstated presumptions from user commands satisfying a general template of if-(state), then-(action), because-(goal). Our reasoner uses a state-of-the-art... | Forough Arabshahi, Jennifer Lee, Antoine Bosselut, Yejin Choi, Tom M. Mitchell |  |
| 1058 |  |  [Towards Automatic Evaluation of Dialog Systems: A Model-Free Off-Policy Evaluation Approach](https://doi.org/10.18653/v1/2021.emnlp-main.589) |  | 0 | Reliable automatic evaluation of dialogue systems under an interactive environment has long been overdue. An ideal environment for evaluating dialog systems, also known as the Turing test, needs to involve human interaction, which is usually not affordable for large-scale experiments. Though researchers have attempted to use metrics for language generation tasks (e.g., perplexity, BLEU) or some model-based reinforcement learning methods (e.g., self-play evaluation) for automatic evaluation,... | Haoming Jiang, Bo Dai, Mengjiao Yang, Tuo Zhao, Wei Wei |  |
| 1059 |  |  [Continual Learning in Task-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2021.emnlp-main.590) |  | 0 | Continual learning in task-oriented dialogue systems allows the system to add new domains and functionalities overtime after deployment, without incurring the high cost of retraining the whole system each time. In this paper, we propose a first-ever continual learning benchmark for task-oriented dialogue systems with 37 domains to be learned continuously in both modularized and end-to-end learning settings. In addition, we implement and compare multiple existing continual learning baselines,... | Andrea Madotto, Zhaojiang Lin, Zhenpeng Zhou, Seungwhan Moon, Paul A. Crook, Bing Liu, Zhou Yu, Eunjoon Cho, Pascale Fung, Zhiguang Wang |  |
| 1060 |  |  [Multilingual and Cross-Lingual Intent Detection from Spoken Data](https://doi.org/10.18653/v1/2021.emnlp-main.591) |  | 0 | We present a systematic study on multilingual and cross-lingual intent detection (ID) from spoken data. The study leverages a new resource put forth in this work, termed MInDS-14, a first training and evaluation resource for the ID task with spoken data. It covers 14 intents extracted from a commercial system in the e-banking domain, associated with spoken examples in 14 diverse language varieties. Our key results indicate that combining machine translation models with state-of-the-art... | Daniela Gerz, PeiHao Su, Razvan Kusztos, Avishek Mondal, Michal Lis, Eshan Singhal, Nikola Mrksic, TsungHsien Wen, Ivan Vulic |  |
| 1061 |  |  [Investigating Robustness of Dialog Models to Popular Figurative Language Constructs](https://doi.org/10.18653/v1/2021.emnlp-main.592) |  | 0 | Humans often employ figurative language use in communication, including during interactions with dialog systems. Thus, it is important for real-world dialog systems to be able to handle popular figurative language constructs like metaphor and simile. In this work, we analyze the performance of existing dialog models in situations where the input dialog context exhibits use of figurative language. We observe large gaps in handling of figurative language when evaluating the models on two open... | Harsh Jhamtani, Varun Gangal, Eduard H. Hovy, Taylor BergKirkpatrick |  |
| 1062 |  |  [Effective Sequence-to-Sequence Dialogue State Tracking](https://doi.org/10.18653/v1/2021.emnlp-main.593) |  | 0 | Sequence-to-sequence models have been applied to a wide variety of NLP tasks, but how to properly use them for dialogue state tracking has not been systematically investigated. In this paper, we study this problem from the perspectives of pre-training objectives as well as the formats of context representations. We demonstrate that the choice of pre-training objective makes a significant difference to the state tracking quality. In particular, we find that masked span prediction is more... | Jeffrey Zhao, Mahdis Mahdieh, Ye Zhang, Yuan Cao, Yonghui Wu |  |
| 1063 |  |  [MS\^2: Multi-Document Summarization of Medical Studies](https://doi.org/10.18653/v1/2021.emnlp-main.594) |  | 0 | To assess the effectiveness of any medical intervention, researchers must conduct a time-intensive and manual literature review. NLP systems can help to automate or assist in parts of this expensive process. In support of this goal, we release MSˆ2 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence... | Jay DeYoung, Iz Beltagy, Madeleine van Zuylen, Bailey Kuehl, Lucy Lu Wang |  |
| 1064 |  |  [CLIPScore: A Reference-free Evaluation Metric for Image Captioning](https://doi.org/10.18653/v1/2021.emnlp-main.595) |  | 0 | Image captioning has conventionally relied on reference-based automatic evaluations, where machine captions are compared against captions written by humans. This is in contrast to the reference-free manner in which humans assess caption quality. In this paper, we report the surprising empirical finding that CLIP (Radford et al., 2021), a cross-modal model pretrained on 400M image+caption pairs from the web, can be used for robust automatic evaluation of image captioning without the need for... | Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, Yejin Choi |  |
| 1065 |  |  [On the Challenges of Evaluating Compositional Explanations in Multi-Hop Inference: Relevance, Completeness, and Expert Ratings](https://doi.org/10.18653/v1/2021.emnlp-main.596) |  | 0 | Building compositional explanations requires models to combine two or more facts that, together, describe why the answer to a question is correct. Typically, these “multi-hop” explanations are evaluated relative to one (or a small number of) gold explanations. In this work, we show these evaluations substantially underestimate model performance, both in terms of the relevance of included facts, as well as the completeness of model-generated explanations, because models regularly discover and... | Peter Jansen, Kelly J. Smith, Dan Moreno, Huitzilin Ortiz |  |
| 1066 |  |  [ESTER: A Machine Reading Comprehension Dataset for Reasoning about Event Semantic Relations](https://doi.org/10.18653/v1/2021.emnlp-main.597) |  | 0 | Understanding how events are semantically related to each other is the essence of reading comprehension. Recent event-centric reading comprehension datasets focus mostly on event arguments or temporal relations. While these tasks partially evaluate machines’ ability of narrative understanding, human-like reading comprehension requires the capability to process event-based information beyond arguments and temporal reasoning. For example, to understand causality between events, we need to infer... | Rujun Han, IHung Hsu, Jiao Sun, Julia Baylon, Qiang Ning, Dan Roth, Nanyun Peng |  |
| 1067 |  |  [RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms](https://doi.org/10.18653/v1/2021.emnlp-main.598) |  | 0 | Pre-trained language models (PTLMs) have achieved impressive performance on commonsense inference benchmarks, but their ability to employ commonsense to make robust inferences, which is crucial for effective communications with humans, is debated. In the pursuit of advancing fluid human-AI communication, we propose a new challenge, RICA: Robust Inference using Commonsense Axioms, that evaluates robust commonsense inference despite textual perturbations. To generate data for this challenge, we... | Pei Zhou, Rahul Khanna, Seyeon Lee, Bill Yuchen Lin, Daniel Ho, Jay Pujara, Xiang Ren |  |
| 1068 |  |  [Compression, Transduction, and Creation: A Unified Framework for Evaluating Natural Language Generation](https://doi.org/10.18653/v1/2021.emnlp-main.599) |  | 0 | Natural language generation (NLG) spans a broad range of tasks, each of which serves for specific objectives and desires different properties of generated text. The complexity makes automatic evaluation of NLG particularly challenging. Previous work has typically focused on a single task and developed individual evaluation metrics based on specific intuitions. In this paper, we propose a unifying perspective based on the nature of information change in NLG tasks, including compression (e.g.,... | Mingkai Deng, Bowen Tan, Zhengzhong Liu, Eric P. Xing, Zhiting Hu |  |
| 1069 |  |  [MATE: Multi-view Attention for Table Transformer Efficiency](https://doi.org/10.18653/v1/2021.emnlp-main.600) |  | 0 | This work presents a sparse-attention Transformer architecture for modeling documents that contain large tables. Tables are ubiquitous on the web, and are rich in information. However, more than 20% of relational tables on the web have 20 or more rows (Cafarella et al., 2008), and these large tables present a challenge for current Transformer models, which are typically limited to 512 tokens. Here we propose MATE, a novel Transformer architecture designed to model the structure of web tables.... | Julian Martin Eisenschlos, Maharshi Gor, Thomas Müller, William W. Cohen |  |
| 1070 |  |  [Learning with Different Amounts of Annotation: From Zero to Many Labels](https://doi.org/10.18653/v1/2021.emnlp-main.601) |  | 0 | Training NLP systems typically assumes access to annotated data that has a single human label per example. Given imperfect labeling from annotators and inherent ambiguity of language, we hypothesize that single label is not sufficient to learn the spectrum of language interpretation. We explore new annotation distribution schemes, assigning multiple labels per example for a small subset of training examples. Introducing such multi label examples at the cost of annotating fewer examples brings... | Shujian Zhang, Chengyue Gong, Eunsol Choi |  |
| 1071 |  |  [When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute](https://doi.org/10.18653/v1/2021.emnlp-main.602) |  | 0 | Large language models have become increasingly difficult to train because of the growing computation time and cost. In this work, we present SRU++, a highly-efficient architecture that combines fast recurrence and attention for sequence modeling. SRU++ exhibits strong modeling capacity and training efficiency. On standard language modeling tasks such as Enwik8, Wiki-103 and Billion Word datasets, our model obtains better bits-per-character and perplexity while using 3x-10x less training cost... | Tao Lei |  |
| 1072 |  |  [Universal-KD: Attention-based Output-Grounded Intermediate Layer Knowledge Distillation](https://doi.org/10.18653/v1/2021.emnlp-main.603) |  | 0 | Intermediate layer matching is shown as an effective approach for improving knowledge distillation (KD). However, this technique applies matching in the hidden spaces of two different networks (i.e. student and teacher), which lacks clear interpretability. Moreover, intermediate layer KD cannot easily deal with other problems such as layer mapping search and architecture mismatch (i.e. it requires the teacher and student to be of the same model type). To tackle the aforementioned problems all... | Yimeng Wu, Mehdi Rezagholizadeh, Abbas Ghaddar, Md. Akmal Haidar, Ali Ghodsi |  |
| 1073 |  |  [Highly Parallel Autoregressive Entity Linking with Discriminative Correction](https://doi.org/10.18653/v1/2021.emnlp-main.604) |  | 0 | Generative approaches have been recently shown to be effective for both Entity Disambiguation and Entity Linking (i.e., joint mention detection and disambiguation). However, the previously proposed autoregressive formulation for EL suffers from i) high computational cost due to a complex (deep) decoder, ii) non-parallelizable decoding that scales with the source sequence length, and iii) the need for training on a large amount of data. In this work, we propose a very efficient approach that... | Nicola De Cao, Wilker Aziz, Ivan Titov |  |
| 1074 |  |  [Word-Level Coreference Resolution](https://doi.org/10.18653/v1/2021.emnlp-main.605) |  | 0 | Recent coreference resolution models rely heavily on span representations to find coreference links between word spans. As the number of spans is O(n2) in the length of text and the number of potential links is O(n4), various pruning techniques are necessary to make this approach computationally feasible. We propose instead to consider coreference links between individual words rather than word spans and then reconstruct the word spans. This reduces the complexity of the coreference model to... | Vladimir Dobrovolskii |  |
| 1075 |  |  [A Secure and Efficient Federated Learning Framework for NLP](https://doi.org/10.18653/v1/2021.emnlp-main.606) |  | 0 | In this work, we consider the problem of designing secure and efficient federated learning (FL) frameworks for NLP. Existing solutions under this literature either consider a trusted aggregator or require heavy-weight cryptographic primitives, which makes the performance significantly degraded. Moreover, many existing secure FL designs work only under the restrictive assumption that none of the clients can be dropped out from the training protocol. To tackle these problems, we propose SEFL, a... | Chenghong Wang, Jieren Deng, Xianrui Meng, Yijue Wang, Ji Li, Sheng Lin, Shuo Han, Fei Miao, Sanguthevar Rajasekaran, Caiwen Ding |  |
| 1076 |  |  [Controllable Semantic Parsing via Retrieval Augmentation](https://doi.org/10.18653/v1/2021.emnlp-main.607) |  | 0 | In practical applications of semantic parsing, we often want to rapidly change the behavior of the parser, such as enabling it to handle queries in a new domain, or changing its predictions on certain targeted queries. While we can introduce new training examples exhibiting the target behavior, a mechanism for enacting such behavior changes without expensive model re-training would be preferable. To this end, we propose ControllAble Semantic Parser via Exemplar Retrieval (CASPER). Given an... | Panupong Pasupat, Yuan Zhang, Kelvin Guu |  |
| 1077 |  |  [Constrained Language Models Yield Few-Shot Semantic Parsers](https://doi.org/10.18653/v1/2021.emnlp-main.608) |  | 0 | We explore the use of large pretrained language models as few-shot semantic parsers. The goal in semantic parsing is to generate a structured meaning representation given a natural language input. However, language models are trained to generate natural language. To bridge the gap, we use language models to paraphrase inputs into a controlled sublanguage resembling English that can be automatically mapped to a target meaning representation. Our results demonstrate that with only a small amount... | Richard Shin, Christopher H. Lin, Sam Thomson, Charles Chen, Subhro Roy, Emmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, Benjamin Van Durme |  |
| 1078 |  |  [ExplaGraphs: An Explanation Graph Generation Task for Structured Commonsense Reasoning](https://doi.org/10.18653/v1/2021.emnlp-main.609) |  | 0 | Recent commonsense-reasoning tasks are typically discriminative in nature, where a model answers a multiple-choice question for a certain context. Discriminative tasks are limiting because they fail to adequately evaluate the model’s ability to reason and explain predictions with underlying commonsense knowledge. They also allow such models to use reasoning shortcuts and not be “right for the right reasons”. In this work, we present ExplaGraphs, a new generative and structured... | Swarnadeep Saha, Prateek Yadav, Lisa Bauer, Mohit Bansal |  |
| 1079 |  |  [Connect-the-Dots: Bridging Semantics between Words and Definitions via Aligning Word Sense Inventories](https://doi.org/10.18653/v1/2021.emnlp-main.610) |  | 0 | Word Sense Disambiguation (WSD) aims to automatically identify the exact meaning of one word according to its context. Existing supervised models struggle to make correct predictions on rare word senses due to limited training data and can only select the best definition sentence from one predefined word sense inventory (e.g., WordNet). To address the data sparsity problem and generalize the model to be independent of one predefined inventory, we propose a gloss alignment algorithm that can... | Wenlin Yao, Xiaoman Pan, Lifeng Jin, Jianshu Chen, Dian Yu, Dong Yu |  |
| 1080 |  |  [LM-Critic: Language Models for Unsupervised Grammatical Error Correction](https://doi.org/10.18653/v1/2021.emnlp-main.611) |  | 0 | Grammatical error correction (GEC) requires a set of labeled ungrammatical / grammatical sentence pairs for training, but obtaining such annotation can be prohibitively expensive. Recently, the Break-It-Fix-It (BIFI) framework has demonstrated strong results on learning to repair a broken program without any labeled examples, but this relies on a perfect critic (e.g., a compiler) that returns whether an example is valid or not, which does not exist for the GEC task. In this work, we show how to... | Michihiro Yasunaga, Jure Leskovec, Percy Liang |  |
| 1081 |  |  [Language-agnostic Representation from Multilingual Sentence Encoders for Cross-lingual Similarity Estimation](https://doi.org/10.18653/v1/2021.emnlp-main.612) |  | 0 | We propose a method to distill a language-agnostic meaning embedding from a multilingual sentence encoder. By removing language-specific information from the original embedding, we retrieve an embedding that fully represents the sentence’s meaning. The proposed method relies only on parallel corpora without any human annotations. Our meaning embedding allows efficient cross-lingual sentence similarity estimation by simple cosine similarity calculation. Experimental results on both quality... | Nattapong Tiyajamorn, Tomoyuki Kajiwara, Yuki Arase, Makoto Onizuka |  |
| 1082 |  |  [Classifying Dyads for Militarized Conflict Analysis](https://doi.org/10.18653/v1/2021.emnlp-main.613) |  | 0 | Understanding the origins of militarized conflict is a complex, yet important undertaking. Existing research seeks to build this understanding by considering bi-lateral relationships between entity pairs (dyadic causes) and multi-lateral relationships among multiple entities (systemic causes). The aim of this work is to compare these two causes in terms of how they correlate with conflict between two entities. We do this by devising a set of textual and graph-based features which represent each... | Niklas Stoehr, Lucas Torroba Hennigen, Samin Ahbab, Robert West, Ryan Cotterell |  |
| 1083 |  |  [Point-of-Interest Type Prediction using Text and Images](https://doi.org/10.18653/v1/2021.emnlp-main.614) |  | 0 | Point-of-interest (POI) type prediction is the task of inferring the type of a place from where a social media post was shared. Inferring a POI’s type is useful for studies in computational social science including sociolinguistics, geosemiotics, and cultural geography, and has applications in geosocial networking technologies such as recommendation and visualization systems. Prior efforts in POI type prediction focus solely on text, without taking visual information into account. However in... | Danae Sánchez Villegas, Nikolaos Aletras |  |
| 1084 |  |  [Come hither or go away? Recognising pre-electoral coalition signals in the news](https://doi.org/10.18653/v1/2021.emnlp-main.615) |  | 0 | In this paper, we introduce the task of political coalition signal prediction from text, that is, the task of recognizing from the news coverage leading up to an election the (un)willingness of political parties to form a government coalition. We decompose our problem into two related, but distinct tasks: (i) predicting whether a reported statement from a politician or a journalist refers to a potential coalition and (ii) predicting the polarity of the signal – namely, whether the speaker is in... | Ines Rehbein, Simone Paolo Ponzetto, Anna Adendorf, Oke Bahnsen, Lukas Stoetzer, Heiner Stuckenschmidt |  |
| 1085 |  |  [#HowYouTagTweets: Learning User Hashtagging Preferences via Personalized Topic Attention](https://doi.org/10.18653/v1/2021.emnlp-main.616) |  | 0 | Millions of hashtags are created on social media every day to cross-refer messages concerning similar topics. To help people find the topics they want to discuss, this paper characterizes a user’s hashtagging preferences via predicting how likely they will post with a hashtag. It is hypothesized that one’s interests in a hashtag are related with what they said before (user history) and the existing posts present the hashtag (hashtag contexts). These factors are married in the deep semantic... | Yuji Zhang, Yubo Zhang, Chunpu Xu, Jing Li, Ziyan Jiang, Baolin Peng |  |
| 1086 |  |  [Learning Neural Templates for Recommender Dialogue System](https://doi.org/10.18653/v1/2021.emnlp-main.617) |  | 0 | The task of Conversational Recommendation System (CRS), i.e., recommender dialog system, aims to recommend precise items to users through natural language interactions. Though recent end-to-end neural models have shown promising progress on this task, two key challenges still remain. First, the recommended items cannot be always incorporated into the generated response precisely and appropriately. Second, only the items mentioned in the training corpus have a chance to be recommended in the... | Zujie Liang, Huang Hu, Can Xu, Jian Miao, Yingying He, Yining Chen, Xiubo Geng, Fan Liang, Daxin Jiang |  |
| 1087 |  |  [Proxy Indicators for the Quality of Open-domain Dialogues](https://doi.org/10.18653/v1/2021.emnlp-main.618) |  | 0 | The automatic evaluation of open-domain dialogues remains a largely unsolved challenge. Despite the abundance of work done in the field, human judges have to evaluate dialogues’ quality. As a consequence, performing such evaluations at scale is usually expensive. This work investigates using a deep-learning model trained on the General Language Understanding Evaluation (GLUE) benchmark to serve as a quality indication of open-domain dialogues. The aim is to use the various GLUE tasks as... | Rostislav Nedelchev, Jens Lehmann, Ricardo Usbeck |  |
| 1088 |  |  [$Q^2$: Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.619) |  | 0 | Neural knowledge-grounded generative models for dialogue often produce content that is factually inconsistent with the knowledge they rely on, making them unreliable and limiting their applicability. Inspired by recent work on evaluating factual consistency in abstractive summarization, we propose an automatic evaluation metric for factual consistency in knowledge-grounded dialogue using automatic question generation and question answering. Our metric, denoted Q2, compares answer spans using... | Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, Omri Abend |  |
| 1089 |  |  [Knowledge-Aware Graph-Enhanced GPT-2 for Dialogue State Tracking](https://doi.org/10.18653/v1/2021.emnlp-main.620) |  | 0 | Dialogue State Tracking is central to multi-domain task-oriented dialogue systems, responsible for extracting information from user utterances. We present a novel hybrid architecture that augments GPT-2 with representations derived from Graph Attention Networks in such a way to allow causal, sequential prediction of slot values. The model architecture captures inter-slot relationships and dependencies across domains that otherwise can be lost in sequential prediction. We report improvements in... | Weizhe Lin, BoHsiang Tseng, Bill Byrne |  |
| 1090 |  |  [A Collaborative Multi-agent Reinforcement Learning Framework for Dialog Action Decomposition](https://doi.org/10.18653/v1/2021.emnlp-main.621) |  | 0 | Most reinforcement learning methods for dialog policy learning train a centralized agent that selects a predefined joint action concatenating domain name, intent type, and slot name. The centralized dialog agent suffers from a great many user-agent interaction requirements due to the large action space. Besides, designing the concatenated actions is laborious to engineers and maybe struggled with edge cases. To solve these problems, we model the dialog policy learning problem with a novel... | Huimin Wang, KamFai Wong |  |
| 1091 |  |  [Zero-Shot Dialogue State Tracking via Cross-Task Transfer](https://doi.org/10.18653/v1/2021.emnlp-main.622) |  | 0 | Zero-shot transfer learning for dialogue state tracking (DST) enables us to handle a variety of task-oriented dialogue domains without the expense of collecting in-domain data. In this work, we propose to transfer the cross-task knowledge from general question answering (QA) corpora for the zero-shot DST task. Specifically, we propose TransferQA, a transferable generative QA model that seamlessly combines extractive QA and multi-choice QA via a text-to-text transformer framework, and tracks... | Zhaojiang Lin, Bing Liu, Andrea Madotto, Seungwhan Moon, Zhenpeng Zhou, Paul A. Crook, Zhiguang Wang, Zhou Yu, Eunjoon Cho, Rajen Subba, Pascale Fung |  |
| 1092 |  |  [Uncertainty Measures in Neural Belief Tracking and the Effects on Dialogue Policy Performance](https://doi.org/10.18653/v1/2021.emnlp-main.623) |  | 0 | The ability to identify and resolve uncertainty is crucial for the robustness of a dialogue system. Indeed, this has been confirmed empirically on systems that utilise Bayesian approaches to dialogue belief tracking. However, such systems consider only confidence estimates and have difficulty scaling to more complex settings. Neural dialogue systems, on the other hand, rarely take uncertainties into account. They are therefore overconfident in their decisions and less robust. Moreover, the... | Carel van Niekerk, Andrey Malinin, Christian Geishauser, Michael Heck, HsienChin Lin, Nurul Lubis, Shutong Feng, Milica Gasic |  |
| 1093 |  |  [Dynamic Forecasting of Conversation Derailment](https://doi.org/10.18653/v1/2021.emnlp-main.624) |  | 0 | Online conversations can sometimes take a turn for the worse, either due to systematic cultural differences, accidental misunderstandings, or mere malice. Automatically forecasting derailment in public online conversations provides an opportunity to take early action to moderate it. Previous work in this space is limited, and we extend it in several ways. We apply a pretrained language encoder to the task, which outperforms earlier approaches. We further experiment with shifting the training... | Yova Kementchedjhieva, Anders Søgaard |  |
| 1094 |  |  [A Semantic Filter Based on Relations for Knowledge Graph Completion](https://doi.org/10.18653/v1/2021.emnlp-main.625) |  | 0 | Knowledge graph embedding, representing entities and relations in the knowledge graphs with high-dimensional vectors, has made significant progress in link prediction. More researchers have explored the representational capabilities of models in recent years. That is, they investigate better representational models to fit symmetry/antisymmetry and combination relationships. The current embedding models are more inclined to utilize the identical vector for the same entity in various triples to... | Zongwei Liang, Junan Yang, Hui Liu, KeJu Huang |  |
| 1095 |  |  [AdapterDrop: On the Efficiency of Adapters in Transformers](https://doi.org/10.18653/v1/2021.emnlp-main.626) |  | 0 | Transformer models are expensive to fine-tune, slow for inference, and have large storage requirements. Recent approaches tackle these shortcomings by training smaller models, dynamically reducing the model size, and by training light-weight adapters. In this paper, we propose AdapterDrop, removing adapters from lower transformer layers during training and inference, which incorporates concepts from all three directions. We show that AdapterDrop can dynamically reduce the computational overhead... | Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, Iryna Gurevych |  |
| 1096 |  |  [Understanding and Overcoming the Challenges of Efficient Transformer Quantization](https://doi.org/10.18653/v1/2021.emnlp-main.627) |  | 0 | Transformer-based architectures have become the de-facto standard models for a wide range of Natural Language Processing tasks. However, their memory footprint and high latency are prohibitive for efficient deployment and inference on resource-limited devices. In this work, we explore quantization for transformers. We show that transformers have unique quantization challenges – namely, high dynamic activation ranges that are difficult to represent with a low bit fixed-point format. We establish... | Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort |  |
| 1097 |  |  [CAPE: Context-Aware Private Embeddings for Private Language Learning](https://doi.org/10.18653/v1/2021.emnlp-main.628) |  | 0 | Neural language models have contributed to state-of-the-art results in a number of downstream applications including sentiment analysis, intent classification and others. However, obtaining text representations or embeddings using these models risks encoding personally identifiable information learned from language and context cues that may lead to privacy leaks. To ameliorate this issue, we propose Context-Aware Private Embeddings (CAPE), a novel approach which combines differential privacy... | Richard Plant, Dimitra Gkatzia, Valerio Giuffrida |  |
| 1098 |  |  [Text Detoxification using Large Pre-trained Neural Models](https://doi.org/10.18653/v1/2021.emnlp-main.629) |  | 0 | We present two novel unsupervised methods for eliminating toxicity in text. Our first method combines two recent ideas: (1) guidance of the generation process with small style-conditional language models and (2) use of paraphrasing models to perform style transfer. We use a well-performing paraphraser guided by style-trained language models to keep the text content and remove toxicity. Our second method uses BERT to replace toxic words with their non-offensive synonyms. We make the method more... | David Dale, Anton Voronov, Daryna Dementieva, Varvara Logacheva, Olga Kozlova, Nikita Semenov, Alexander Panchenko |  |
| 1099 |  |  [Document-Level Text Simplification: Dataset, Criteria and Baseline](https://doi.org/10.18653/v1/2021.emnlp-main.630) |  | 0 | Text simplification is a valuable technique. However, current research is limited to sentence simplification. In this paper, we define and investigate a new task of document-level text simplification, which aims to simplify a document consisting of multiple sentences. Based on Wikipedia dumps, we first construct a large-scale dataset named D-Wikipedia and perform analysis and human evaluation on it to show that the dataset is reliable. Then, we propose a new automatic evaluation metric called... | Renliang Sun, Hanqi Jin, Xiaojun Wan |  |
| 1100 |  |  [A Bag of Tricks for Dialogue Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.631) |  | 0 | Dialogue summarization comes with its own peculiar challenges as opposed to news or scientific articles summarization. In this work, we explore four different challenges of the task: handling and differentiating parts of the dialogue belonging to multiple speakers, negation understanding, reasoning about the situation, and informal language understanding. Using a pretrained sequence-to-sequence language model, we explore speaker name substitution, negation scope highlighting, multi-task... | Muhammad Khalifa, Miguel Ballesteros, Kathleen R. McKeown |  |
| 1101 |  |  [Paraphrasing Compound Nominalizations](https://doi.org/10.18653/v1/2021.emnlp-main.632) |  | 0 | A nominalization uses a deverbal noun to describe an event associated with its underlying verb. Commonly found in academic and formal texts, nominalizations can be difficult to interpret because of ambiguous semantic relations between the deverbal noun and its arguments. Our goal is to interpret nominalizations by generating clausal paraphrases. We address compound nominalizations with both nominal and adjectival modifiers, as well as prepositional phrases. In evaluations on a number of... | John Lee, Ho Hung Lim, Carol Webster |  |
| 1102 |  |  [Data-QuestEval: A Referenceless Metric for Data-to-Text Semantic Evaluation](https://doi.org/10.18653/v1/2021.emnlp-main.633) |  | 0 | QuestEval is a reference-less metric used in text-to-text tasks, that compares the generated summaries directly to the source text, by automatically asking and answering questions. Its adaptation to Data-to-Text tasks is not straightforward, as it requires multimodal Question Generation and Answering systems on the considered tasks, which are seldom available. To this purpose, we propose a method to build synthetic multimodal corpora enabling to train multimodal components for a data-QuestEval... | Clément Rebuffel, Thomas Scialom, Laure Soulier, Benjamin Piwowarski, Sylvain Lamprier, Jacopo Staiano, Geoffrey Scoutheeten, Patrick Gallinari |  |
| 1103 |  |  [Low-Rank Subspaces for Unsupervised Entity Linking](https://doi.org/10.18653/v1/2021.emnlp-main.634) |  | 0 | Entity linking is an important problem with many applications. Most previous solutions were designed for settings where annotated training data is available, which is, however, not the case in numerous domains. We propose a light-weight and scalable entity linking method, Eigenthemes, that relies solely on the availability of entity names and a referent knowledge base. Eigenthemes exploits the fact that the entities that are truly mentioned in a document (the “gold entities”) tend to form a... | Akhil Arora, Alberto GarcíaDurán, Robert West |  |
| 1104 |  |  [TDEER: An Efficient Translating Decoding Schema for Joint Extraction of Entities and Relations](https://doi.org/10.18653/v1/2021.emnlp-main.635) |  | 0 | Joint extraction of entities and relations from unstructured texts to form factual triples is a fundamental task of constructing a Knowledge Base (KB). A common method is to decode triples by predicting entity pairs to obtain the corresponding relation. However, it is still challenging to handle this task efficiently, especially for the overlapping triple problem. To address such a problem, this paper proposes a novel efficient entities and relations extraction model called TDEER, which stands... | Xianming Li, Xiaotian Luo, Chenghao Dong, Daichuan Yang, Beidi Luan, Zhen He |  |
| 1105 |  |  [Extracting Event Temporal Relations via Hyperbolic Geometry](https://doi.org/10.18653/v1/2021.emnlp-main.636) |  | 0 | Detecting events and their evolution through time is a crucial task in natural language understanding. Recent neural approaches to event temporal relation extraction typically map events to embeddings in the Euclidean space and train a classifier to detect temporal relations between event pairs. However, embeddings in the Euclidean space cannot capture richer asymmetric relations such as event temporal relations. We thus propose to embed events into hyperbolic spaces, which are intrinsically... | Xingwei Tan, Gabriele Pergola, Yulan He |  |
| 1106 |  |  [Honey or Poison? Solving the Trigger Curse in Few-shot Event Detection via Causal Intervention](https://doi.org/10.18653/v1/2021.emnlp-main.637) |  | 0 | Event detection has long been troubled by the trigger curse: overfitting the trigger will harm the generalization ability while underfitting it will hurt the detection performance. This problem is even more severe in few-shot scenario. In this paper, we identify and solve the trigger curse problem in few-shot event detection (FSED) from a causal view. By formulating FSED with a structural causal model (SCM), we found that the trigger is a confounder of the context and the result, which makes... | Jiawei Chen, Hongyu Lin, Xianpei Han, Le Sun |  |
| 1107 |  |  [Back to the Basics: A Quantitative Analysis of Statistical and Graph-Based Term Weighting Schemes for Keyword Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.638) |  | 0 | Term weighting schemes are widely used in Natural Language Processing and Information Retrieval. In particular, term weighting is the basis for keyword extraction. However, there are relatively few evaluation studies that shed light about the strengths and shortcomings of each weighting scheme. In fact, in most cases researchers and practitioners resort to the well-known tf-idf as default, despite the existence of other suitable alternatives, including graph-based models. In this paper, we... | Asahi Ushio, Federico Liberatore, José CamachoCollados |  |
| 1108 |  |  [Time-dependent Entity Embedding is not All You Need: A Re-evaluation of Temporal Knowledge Graph Completion Models under a Unified Framework](https://doi.org/10.18653/v1/2021.emnlp-main.639) |  | 0 | Various temporal knowledge graph (KG) completion models have been proposed in the recent literature. The models usually contain two parts, a temporal embedding layer and a score function derived from existing static KG modeling approaches. Since the approaches differ along several dimensions, including different score functions and training strategies, the individual contributions of different temporal embedding techniques to model performance are not always clear. In this work, we... | Zhen Han, Gengyuan Zhang, Yunpu Ma, Volker Tresp |  |
| 1109 |  |  [Matching-oriented Embedding Quantization For Ad-hoc Retrieval](https://doi.org/10.18653/v1/2021.emnlp-main.640) |  | 0 | Product quantization (PQ) is a widely used technique for ad-hoc retrieval. Recent studies propose supervised PQ, where the embedding and quantization models can be jointly trained with supervised learning. However, there is a lack of appropriate formulation of the joint training objective; thus, the improvements over previous non-supervised baselines are limited in reality. In this work, we propose the Matching-oriented Product Quantization (MoPQ), where a novel objective Multinoulli... | Shitao Xiao, Zheng Liu, Yingxia Shao, Defu Lian, Xing Xie |  |
| 1110 |  |  [Efficient Mind-Map Generation via Sequence-to-Graph and Reinforced Graph Refinement](https://doi.org/10.18653/v1/2021.emnlp-main.641) |  | 0 | A mind-map is a diagram that represents the central concept and key ideas in a hierarchical way. Converting plain text into a mind-map will reveal its key semantic structure and be easier to understand. Given a document, the existing automatic mind-map generation method extracts the relationships of every sentence pair to generate the directed semantic graph for this document. The computation complexity increases exponentially with the length of the document. Moreover, it is difficult to... | Mengting Hu, Honglei Guo, Shiwan Zhao, Hang Gao, Zhong Su |  |
| 1111 |  |  [Deep Attention Diffusion Graph Neural Networks for Text Classification](https://doi.org/10.18653/v1/2021.emnlp-main.642) |  | 0 | Text classification is a fundamental task with broad applications in natural language processing. Recently, graph neural networks (GNNs) have attracted much attention due to their powerful representation ability. However, most existing methods for text classification based on GNNs consider only one-hop neighborhoods and low-frequency information within texts, which cannot fully utilize the rich context information of documents. Moreover, these models suffer from over-smoothing issues if many... | Yonghao Liu, Renchu Guan, Fausto Giunchiglia, Yanchun Liang, Xiaoyue Feng |  |
| 1112 |  |  [Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution](https://doi.org/10.18653/v1/2021.emnlp-main.643) |  | 0 | Multi-label text classification is a challenging task because it requires capturing label dependencies. It becomes even more challenging when class distribution is long-tailed. Resampling and re-weighting are common approaches used for addressing the class imbalance problem, however, they are not effective when there is label dependency besides class imbalance because they result in oversampling of common labels. Here, we introduce the application of balancing loss functions for multi-label... | Yi Huang, Buse Giledereli, Abdullatif Köksal, Arzucan Özgür, Elif Ozkirimli |  |
| 1113 |  |  [Bayesian Topic Regression for Causal Inference](https://doi.org/10.18653/v1/2021.emnlp-main.644) |  | 0 | Causal inference using observational text data is becoming increasingly popular in many research areas. This paper presents the Bayesian Topic Regression (BTR) model that uses both text and numerical information to model an outcome variable. It allows estimation of both discrete and continuous treatment effects. Furthermore, it allows for the inclusion of additional numerical confounding factors next to text data. To this end, we combine a supervised Bayesian topic model with a Bayesian... | Maximilian Ahrens, Julian Ashwin, JanPeter Calliess, Vu Nguyen |  |
| 1114 |  |  [Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience](https://doi.org/10.18653/v1/2021.emnlp-main.645) |  | 0 | Pretrained transformer-based models such as BERT have demonstrated state-of-the-art predictive performance when adapted into a range of natural language processing tasks. An open problem is how to improve the faithfulness of explanations (rationales) for the predictions of these models. In this paper, we hypothesize that salient information extracted a priori from the training data can complement the task-specific information learned by the model during fine-tuning on a downstream task. In this... | George Chrysostomou, Nikolaos Aletras |  |
| 1115 |  |  [What's in Your Head? Emergent Behaviour in Multi-Task Transformer Models](https://doi.org/10.18653/v1/2021.emnlp-main.646) |  | 0 | The primary paradigm for multi-task training in natural language processing is to represent the input with a shared pre-trained language model, and add a small, thin network (head) per task. Given an input, a target head is the head that is selected for outputting the final prediction. In this work, we examine the behaviour of non-target heads, that is, the output of heads when given input that belongs to a different task than the one they were trained for. We find that non-target heads exhibit... | Mor Geva, Uri Katz, Aviv BenArie, Jonathan Berant |  |
| 1116 |  |  [Don't Search for a Search Method - Simple Heuristics Suffice for Adversarial Text Attacks](https://doi.org/10.18653/v1/2021.emnlp-main.647) |  | 0 | Recently more attention has been given to adversarial attacks on neural networks for natural language processing (NLP). A central research topic has been the investigation of search algorithms and search constraints, accompanied by benchmark algorithms and tasks. We implement an algorithm inspired by zeroth order optimization-based attacks and compare with the benchmark results in the TextAttack framework. Surprisingly, we find that optimization-based methods do not yield any improvement in a... | Nathaniel Berger, Stefan Riezler, Sebastian Ebert, Artem Sokolov |  |
| 1117 |  |  [Adversarial Attacks on Knowledge Graph Embeddings via Instance Attribution Methods](https://doi.org/10.18653/v1/2021.emnlp-main.648) |  | 0 | Despite the widespread use of Knowledge Graph Embeddings (KGE), little is known about the security vulnerabilities that might disrupt their intended behaviour. We study data poisoning attacks against KGE models for link prediction. These attacks craft adversarial additions or deletions at training time to cause model failure at test time. To select adversarial deletions, we propose to use the model-agnostic instance attribution methods from Interpretable Machine Learning, which identify the... | Peru Bhardwaj, John D. Kelleher, Luca Costabello, Declan O'Sullivan |  |
| 1118 |  |  [Locke's Holiday: Belief Bias in Machine Reading](https://doi.org/10.18653/v1/2021.emnlp-main.649) |  | 0 | I highlight a simple failure mode of state-of-the-art machine reading systems: when contexts do not align with commonly shared beliefs. For example, machine reading systems fail to answer What did Elizabeth want? correctly in the context of ‘My kingdom for a cough drop, cried Queen Elizabeth.’ Biased by co-occurrence statistics in the training data of pretrained language models, systems predict my kingdom, rather than a cough drop. I argue such biases are analogous to human belief biases and... | Anders Søgaard |  |
| 1119 |  |  [Sequence Length is a Domain: Length-based Overfitting in Transformer Models](https://doi.org/10.18653/v1/2021.emnlp-main.650) |  | 0 | Transformer-based sequence-to-sequence architectures, while achieving state-of-the-art results on a large number of NLP tasks, can still suffer from overfitting during training. In practice, this is usually countered either by applying regularization methods (e.g. dropout, L2-regularization) or by providing huge amounts of training data. Additionally, Transformer and other architectures are known to struggle when generating very long sequences. For example, in machine translation, the... | Dusan Varis, Ondrej Bojar |  |
| 1120 |  |  [Contrasting Human- and Machine-Generated Word-Level Adversarial Examples for Text Classification](https://doi.org/10.18653/v1/2021.emnlp-main.651) |  | 0 | Research shows that natural language processing models are generally considered to be vulnerable to adversarial attacks; but recent work has drawn attention to the issue of validating these adversarial inputs against certain criteria (e.g., the preservation of semantics and grammaticality). Enforcing constraints to uphold such criteria may render attacks unsuccessful, raising the question of whether valid attacks are actually feasible. In this work, we investigate this through the lens of human... | Maximilian Mozes, Max Bartolo, Pontus Stenetorp, Bennett Kleinberg, Lewis D. Griffin |  |
| 1121 |  |  [Is Information Density Uniform in Task-Oriented Dialogues?](https://doi.org/10.18653/v1/2021.emnlp-main.652) |  | 0 | The Uniform Information Density principle states that speakers plan their utterances to reduce fluctuations in the density of the information transmitted. In this paper, we test whether, and within which contextual units this principle holds in task-oriented dialogues. We show that there is evidence supporting the principle in written dialogues where participants play a cooperative reference game as well as in spoken dialogues involving instruction giving and following. Our study underlines the... | Mario Giulianelli, Arabella Sinclair, Raquel Fernández |  |
| 1122 |  |  [On Homophony and Rényi Entropy](https://doi.org/10.18653/v1/2021.emnlp-main.653) |  | 0 | Homophony’s widespread presence in natural languages is a controversial topic. Recent theories of language optimality have tried to justify its prevalence, despite its negative effects on cognitive processing time, e.g., Piantadosi et al. (2012) argued homophony enables the reuse of efficient wordforms and is thus beneficial for languages. This hypothesis has recently been challenged by Trott and Bergen (2020), who posit that good wordforms are more often homophonous simply because they are... | Tiago Pimentel, Clara Meister, Simone Teufel, Ryan Cotterell |  |
| 1123 |  |  [Synthetic Textual Features for the Large-Scale Detection of Basic-level Categories in English and Mandarin](https://doi.org/10.18653/v1/2021.emnlp-main.654) |  | 0 | Basic-level categories (BLC) are an important psycholinguistic concept introduced by Rosch et al. (1976); they are defined as the most inclusive categories for which a concrete mental image of the category as a whole can be formed, and also as those categories which are acquired early in life. Rosch’s original algorithm for detecting BLC (called cue-validity) is based on the availability of semantic features such as “has tail” for “cat”, and has remained untested at large. An at-scale algorithm... | Yiwen Chen, Simone Teufel |  |
| 1124 |  |  [TimeTraveler: Reinforcement Learning for Temporal Knowledge Graph Forecasting](https://doi.org/10.18653/v1/2021.emnlp-main.655) |  | 0 | Temporal knowledge graph (TKG) reasoning is a crucial task that has gained increasing research interest in recent years. Most existing methods focus on reasoning at past timestamps to complete the missing facts, and there are only a few works of reasoning on known TKGs to forecast future facts. Compared with the completion task, the forecasting task is more difficult that faces two main challenges: (1) how to effectively model the time information to handle future timestamps? (2) how to make... | Haohai Sun, Jialun Zhong, Yunpu Ma, Zhen Han, Kun He |  |
| 1125 |  |  [Code-switched inspired losses for spoken dialog representations](https://doi.org/10.18653/v1/2021.emnlp-main.656) |  | 0 | Spoken dialogue systems need to be able to handle both multiple languages and multilinguality inside a conversation (e.g in case of code-switching). In this work, we introduce new pretraining losses tailored to learn generic multilingual spoken dialogue representations. The goal of these losses is to expose the model to code-switched language. In order to scale up training, we automatically build a pretraining corpus composed of multilingual conversations in five different languages (French,... | Pierre Colombo, Emile Chapuis, Matthieu Labeau, Chloé Clavel |  |
| 1126 |  |  [BiQUE: Biquaternionic Embeddings of Knowledge Graphs](https://doi.org/10.18653/v1/2021.emnlp-main.657) |  | 0 | Knowledge graph embeddings (KGEs) compactly encode multi-relational knowledge graphs (KGs). Existing KGE models rely on geometric operations to model relational patterns. Euclidean (circular) rotation is useful for modeling patterns such as symmetry, but cannot represent hierarchical semantics. In contrast, hyperbolic models are effective at modeling hierarchical relations, but do not perform as well on patterns on which circular rotation excels. It is crucial for KGE models to unify multiple... | Jia Guo, Stanley Kok |  |
| 1127 |  |  [Learning Neural Ordinary Equations for Forecasting Future Links on Temporal Knowledge Graphs](https://doi.org/10.18653/v1/2021.emnlp-main.658) |  | 0 | There has been an increasing interest in inferring future links on temporal knowledge graphs (KG). While links on temporal KGs vary continuously over time, the existing approaches model the temporal KGs in discrete state spaces. To this end, we propose a novel continuum model by extending the idea of neural ordinary differential equations (ODEs) to multi-relational graph convolutional networks. The proposed model preserves the continuous nature of dynamic multi-relational graph data and encodes... | Zhen Han, Zifeng Ding, Yunpu Ma, Yujia Gu, Volker Tresp |  |
| 1128 |  |  [RAP: Robustness-Aware Perturbations for Defending against Backdoor Attacks on NLP Models](https://doi.org/10.18653/v1/2021.emnlp-main.659) |  | 0 | Backdoor attacks, which maliciously control a well-trained model’s outputs of the instances with specific triggers, are recently shown to be serious threats to the safety of reusing deep neural networks (DNNs). In this work, we propose an efficient online defense mechanism based on robustness-aware perturbations. Specifically, by analyzing the backdoor training process, we point out that there exists a big gap of robustness between poisoned and clean samples. Motivated by this observation, we... | Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, Xu Sun |  |
| 1129 |  |  [FAME: Feature-Based Adversarial Meta-Embeddings for Robust Input Representations](https://doi.org/10.18653/v1/2021.emnlp-main.660) |  | 0 | Combining several embeddings typically improves performance in downstream tasks as different embeddings encode different information. It has been shown that even models using embeddings from transformers still benefit from the inclusion of standard word embeddings. However, the combination of embeddings of different types and dimensions is challenging. As an alternative to attention-based meta-embeddings, we propose feature-based adversarial meta-embeddings (FAME) with an attention function... | Lukas Lange, Heike Adel, Jannik Strötgen, Dietrich Klakow |  |
| 1130 |  |  [A Strong Baseline for Query Efficient Attacks in a Black Box Setting](https://doi.org/10.18653/v1/2021.emnlp-main.661) |  | 0 | Existing black box search methods have achieved high success rate in generating adversarial attacks against NLP models. However, such search methods are inefficient as they do not consider the amount of queries required to generate adversarial attacks. Also, prior attacks do not maintain a consistent search space while comparing different search methods. In this paper, we propose a query efficient attack strategy to generate plausible adversarial examples on text classification and entailment... | Rishabh Maheshwary, Saket Maheshwary, Vikram Pudi |  |
| 1131 |  |  [Machine Translation Decoding beyond Beam Search](https://doi.org/10.18653/v1/2021.emnlp-main.662) |  | 0 | Beam search is the go-to method for decoding auto-regressive machine translation models. While it yields consistent improvements in terms of BLEU, it is only concerned with finding outputs with high model likelihood, and is thus agnostic to whatever end metric or score practitioners care about. Our aim is to establish whether beam search can be replaced by a more powerful metric-driven search technique. To this end, we explore numerous decoding algorithms, including some which rely on a value... | Rémi Leblond, JeanBaptiste Alayrac, Laurent Sifre, Miruna Pislar, JeanBaptiste Lespiau, Ioannis Antonoglou, Karen Simonyan, Oriol Vinyals |  |
| 1132 |  |  [Document Graph for Neural Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.663) |  | 0 | Previous works have shown that contextual information can improve the performance of neural machine translation (NMT). However, most existing document-level NMT methods failed to leverage contexts beyond a few set of previous sentences. How to make use of the whole document as global contexts is still a challenge. To address this issue, we hypothesize that a document can be represented as a graph that connects relevant contexts regardless of their distances. We employ several types of... | Mingzhou Xu, Liangyou Li, Derek F. Wong, Qun Liu, Lidia S. Chao |  |
| 1133 |  |  [An Empirical Investigation of Word Alignment Supervision for Zero-Shot Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.664) |  | 0 | Zero-shot translations is a fascinating feature of Multilingual Neural Machine Translation (MNMT) systems. These MNMT models are usually trained on English-centric data, i.e. English either as the source or target language, and with a language label prepended to the input indicating the target language. However, recent work has highlighted several flaws of these models in zero-shot scenarios where language labels are ignored and the wrong language is generated or different runs show highly... | Alessandro Raganato, Raúl Vázquez, Mathias Creutz, Jörg Tiedemann |  |
| 1134 |  |  [Graph Algorithms for Multiparallel Word Alignment](https://doi.org/10.18653/v1/2021.emnlp-main.665) |  | 0 | With the advent of end-to-end deep learning approaches in machine translation, interest in word alignments initially decreased; however, they have again become a focus of research more recently. Alignments are useful for typological research, transferring formatting like markup to translated texts, and can be used in the decoding of machine translation systems. At the same time, massively multilingual processing is becoming an important NLP scenario, and pretrained language and machine... | Ayyoob Imani, Masoud Jalili Sabet, Lutfi Kerem Senel, Philipp Dufter, François Yvon, Hinrich Schütze |  |
| 1135 |  |  [Improving the Quality Trade-Off for Neural Machine Translation Multi-Domain Adaptation](https://doi.org/10.18653/v1/2021.emnlp-main.666) |  | 0 | Building neural machine translation systems to perform well on a specific target domain is a well-studied problem. Optimizing system performance for multiple, diverse target domains however remains a challenge. We study this problem in an adaptation setting where the goal is to preserve the existing system quality while incorporating data for domains that were not the focus of the original translation system. We find that we can improve over the performance trade-off offered by Elastic Weight... | Eva Hasler, Tobias Domhan, Jonay Trénous, Ke Tran, Bill Byrne, Felix Hieber |  |
| 1136 |  |  [Language Modeling, Lexical Translation, Reordering: The Training Process of NMT through the Lens of Classical SMT](https://doi.org/10.18653/v1/2021.emnlp-main.667) |  | 0 | Differently from the traditional statistical MT that decomposes the translation task into distinct separately learned components, neural machine translation uses a single neural network to model the entire translation process. Despite neural machine translation being de-facto standard, it is still not clear how NMT models acquire different competences over the course of training, and how this mirrors the different models in traditional SMT. In this work, we look at the competences related to... | Elena Voita, Rico Sennrich, Ivan Titov |  |
| 1137 |  |  [Effective Fine-Tuning Methods for Cross-lingual Adaptation](https://doi.org/10.18653/v1/2021.emnlp-main.668) |  | 0 | Large scale multilingual pre-trained language models have shown promising results in zero- and few-shot cross-lingual tasks. However, recent studies have shown their lack of generalizability when the languages are structurally dissimilar. In this work, we propose a novel fine-tuning method based on co-training that aims to learn more generalized semantic equivalences as a complementary to multilingual language modeling using the unlabeled data in the target language. We also propose an adaption... | Tao Yu, Shafiq R. Joty |  |
| 1138 |  |  [Rethinking Data Augmentation for Low-Resource Neural Machine Translation: A Multi-Task Learning Approach](https://doi.org/10.18653/v1/2021.emnlp-main.669) |  | 0 | In the context of neural machine translation, data augmentation (DA) techniques may be used for generating additional training samples when the available parallel data are scarce. Many DA approaches aim at expanding the support of the empirical data distribution by generating new sentence pairs that contain infrequent words, thus making it closer to the true data distribution of parallel sentences. In this paper, we propose to follow a completely different approach and present a multi-task DA... | Víctor M. SánchezCartagena, Miquel EsplàGomis, Juan Antonio PérezOrtiz, Felipe SánchezMartínez |  |
| 1139 |  |  [Wino-X: Multilingual Winograd Schemas for Commonsense Reasoning and Coreference Resolution](https://doi.org/10.18653/v1/2021.emnlp-main.670) |  | 0 | Winograd schemas are a well-established tool for evaluating coreference resolution (CoR) and commonsense reasoning (CSR) capabilities of computational models. So far, schemas remained largely confined to English, limiting their utility in multilingual settings. This work presents Wino-X, a parallel dataset of German, French, and Russian schemas, aligned with their English counterparts. We use this resource to investigate whether neural machine translation (NMT) models can perform CoR that... | Denis Emelin, Rico Sennrich |  |
| 1140 |  |  [One Source, Two Targets: Challenges and Rewards of Dual Decoding](https://doi.org/10.18653/v1/2021.emnlp-main.671) |  | 0 | Machine translation is generally understood as generating one target text from an input source document. In this paper, we consider a stronger requirement: to jointly generate two texts so that each output side effectively depends on the other. As we discuss, such a device serves several practical purposes, from multi-target machine translation to the generation of controlled variations of the target text. We present an analysis of possible implementations of dual decoding, and experiment with... | Jitao Xu, François Yvon |  |
| 1141 |  |  [Discrete and Soft Prompting for Multilingual Models](https://doi.org/10.18653/v1/2021.emnlp-main.672) |  | 0 | It has been shown for English that discrete and soft prompting perform strongly in few-shot learning with pretrained language models (PLMs). In this paper, we show that discrete and soft prompting perform better than finetuning in multilingual cases: Crosslingual transfer and in-language training of multilingual natural language inference. For example, with 48 English training examples, finetuning obtains 33.74% accuracy in crosslingual transfer, barely surpassing the majority baseline... | Mengjie Zhao, Hinrich Schütze |  |
| 1142 |  |  [Vision Matters When It Should: Sanity Checking Multimodal Machine Translation Models](https://doi.org/10.18653/v1/2021.emnlp-main.673) |  | 0 | Multimodal machine translation (MMT) systems have been shown to outperform their text-only neural machine translation (NMT) counterparts when visual context is available. However, recent studies have also shown that the performance of MMT models is only marginally impacted when the associated image is replaced with an unrelated image or noise, which suggests that the visual context might not be exploited by the model at all. We hypothesize that this might be caused by the nature of the commonly... | Jiaoda Li, Duygu Ataman, Rico Sennrich |  |
| 1143 |  |  [Efficient Inference for Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.674) |  | 0 | Multilingual NMT has become an attractive solution for MT deployment in production. But to match bilingual quality, it comes at the cost of larger and slower models. In this work, we consider several ways to make multilingual NMT faster at inference without degrading its quality. We experiment with several “light decoder” architectures in two 20-language multi-parallel settings: small-scale on TED Talks and large-scale on ParaCrawl. Our experiments demonstrate that combining a shallow decoder... | Alexandre Berard, Dain Lee, Stéphane Clinchant, Kweon Woo Jung, Vassilina Nikoulina |  |
| 1144 |  |  [Role of Language Relatedness in Multilingual Fine-tuning of Language Models: A Case Study in Indo-Aryan Languages](https://doi.org/10.18653/v1/2021.emnlp-main.675) |  | 0 | We explore the impact of leveraging the relatedness of languages that belong to the same family in NLP models using multilingual fine-tuning. We hypothesize and validate that multilingual fine-tuning of pre-trained language models can yield better performance on downstream NLP applications, compared to models fine-tuned on individual languages. A first of its kind detailed study is presented to track performance change as languages are added to a base language in a graded and greedy (in the... | Tejas I. Dhamecha, V. Rudra Murthy, Samarth Bharadwaj, Karthik Sankaranarayanan, Pushpak Bhattacharyya |  |
| 1145 |  |  [Comparing Feature-Engineering and Feature-Learning Approaches for Multilingual Translationese Classification](https://doi.org/10.18653/v1/2021.emnlp-main.676) |  | 0 | Traditional hand-crafted linguistically-informed features have often been used for distinguishing between translated and original non-translated texts. By contrast, to date, neural architectures without manual feature engineering have been less explored for this task. In this work, we (i) compare the traditional feature-engineering-based approach to the feature-learning-based one and (ii) analyse the neural architectures in order to investigate how well the hand-crafted features explain the... | Daria Pylypenko, Kwabena AmponsahKaakyire, Koel Dutta Chowdhury, Josef van Genabith, Cristina EspañaBonet |  |
| 1146 |  |  [Multi-Sentence Resampling: A Simple Approach to Alleviate Dataset Length Bias and Beam-Search Degradation](https://doi.org/10.18653/v1/2021.emnlp-main.677) |  | 0 | Neural Machine Translation (NMT) is known to suffer from a beam-search problem: after a certain point, increasing beam size causes an overall drop in translation quality. This effect is especially pronounced for long sentences. While much work was done analyzing this phenomenon, primarily for autoregressive NMT models, there is still no consensus on its underlying cause. In this work, we analyze errors that cause major quality degradation with large beams in NMT and Automatic Speech Recognition... | Ivan Provilkov, Andrey Malinin |  |
| 1147 |  |  [Cross-Policy Compliance Detection via Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.678) |  | 0 | Policy compliance detection is the task of ensuring that a scenario conforms to a policy (e.g. a claim is valid according to government rules or a post in an online platform conforms to community guidelines). This task has been previously instantiated as a form of textual entailment, which results in poor accuracy due to the complexity of the policies. In this paper we propose to address policy compliance detection via decomposing it into question answering, where questions check whether the... | Marzieh Saeidi, Majid Yazdani, Andreas Vlachos |  |
| 1148 |  |  [Meta-LMTC: Meta-Learning for Large-Scale Multi-Label Text Classification](https://doi.org/10.18653/v1/2021.emnlp-main.679) |  | 0 | Large-scale multi-label text classification (LMTC) tasks often face long-tailed label distributions, where many labels have few or even no training instances. Although current methods can exploit prior knowledge to handle these few/zero-shot labels, they neglect the meta-knowledge contained in the dataset that can guide models to learn with few samples. In this paper, for the first time, this problem is addressed from a meta-learning perspective. However, the simple extension of meta-learning... | Ran Wang, Xi'ao Su, Siyu Long, Xinyu Dai, Shujian Huang, Jiajun Chen |  |
| 1149 |  |  [Unsupervised Multi-View Post-OCR Error Correction With Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.680) |  | 0 | We investigate post-OCR correction in a setting where we have access to different OCR views of the same document. The goal of this study is to understand if a pretrained language model (LM) can be used in an unsupervised way to reconcile the different OCR views such that their combination contains fewer errors than each individual view. This approach is motivated by scenarios in which unconstrained text generation for error correction is too risky. We evaluated different pretrained LMs on two... | Harsh Gupta, Luciano Del Corro, Samuel Broscheit, Johannes Hoffart, Eliot Brenner |  |
| 1150 |  |  [Parallel Refinements for Lexically Constrained Text Generation with BART](https://doi.org/10.18653/v1/2021.emnlp-main.681) |  | 0 | Lexically constrained text generation aims to control the generated text by incorporating certain pre-specified keywords into the output. Previous work injects lexical constraints into the output by controlling the decoding process or refining the candidate output iteratively, which tends to generate generic or ungrammatical sentences, and has high computational complexity. To address these challenges, we proposed Constrained BART (CBART) for lexically constrained text generation. CBART... | Xingwei He |  |
| 1151 |  |  [BERT-Beta: A Proactive Probabilistic Approach to Text Moderation](https://doi.org/10.18653/v1/2021.emnlp-main.682) |  | 0 | Text moderation for user generated content, which helps to promote healthy interaction among users, has been widely studied and many machine learning models have been proposed. In this work, we explore an alternative perspective by augmenting reactive reviews with proactive forecasting. Specifically, we propose a new concept text toxicity propensity to characterize the extent to which a text tends to attract toxic comments. Beta regression is then introduced to do the probabilistic modeling,... | Fei Tan, Yifan Hu, Kevin Yen, Changwei Hu |  |
| 1152 |  |  [STaCK: Sentence Ordering with Temporal Commonsense Knowledge](https://doi.org/10.18653/v1/2021.emnlp-main.683) |  | 0 | Sentence order prediction is the task of finding the correct order of sentences in a randomly ordered document. Correctly ordering the sentences requires an understanding of coherence with respect to the chronological sequence of events described in the text. Document-level contextual understanding and commonsense knowledge centered around these events are often essential in uncovering this coherence and predicting the exact chronological order. In this paper, we introduce STaCK — a framework... | Deepanway Ghosal, Navonil Majumder, Rada Mihalcea, Soujanya Poria |  |
| 1153 |  |  [Preventing Author Profiling through Zero-Shot Multilingual Back-Translation](https://doi.org/10.18653/v1/2021.emnlp-main.684) |  | 0 | Documents as short as a single sentence may inadvertently reveal sensitive information about their authors, including e.g. their gender or ethnicity. Style transfer is an effective way of transforming texts in order to remove any information that enables author profiling. However, for a number of current state-of-the-art approaches the improved privacy is accompanied by an undesirable drop in the down-stream utility of the transformed data. In this paper, we propose a simple, zero-shot way to... | David Ifeoluwa Adelani, Miaoran Zhang, Xiaoyu Shen, Ali Davody, Thomas Kleinbauer, Dietrich Klakow |  |
| 1154 |  |  [CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation](https://doi.org/10.18653/v1/2021.emnlp-main.685) |  | 0 | Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a... | Yue Wang, Weishi Wang, Shafiq R. Joty, Steven C. H. Hoi |  |
| 1155 |  |  [Detect and Classify - Joint Span Detection and Classification for Health Outcomes](https://doi.org/10.18653/v1/2021.emnlp-main.686) |  | 0 | A health outcome is a measurement or an observation used to capture and assess the effect of a treatment. Automatic detection of health outcomes from text would undoubtedly speed up access to evidence necessary in healthcare decision making. Prior work on outcome detection has modelled this task as either (a) a sequence labelling task, where the goal is to detect which text spans describe health outcomes, or (b) a classification task, where the goal is to classify a text into a predefined set... | Micheal Abaho, Danushka Bollegala, Paula Williamson, Susanna Dodd |  |
| 1156 |  |  [Multi-Class Grammatical Error Detection for Correction: A Tale of Two Systems](https://doi.org/10.18653/v1/2021.emnlp-main.687) |  | 0 | In this paper, we show how a multi-class grammatical error detection (GED) system can be used to improve grammatical error correction (GEC) for English. Specifically, we first develop a new state-of-the-art binary detection system based on pre-trained ELECTRA, and then extend it to multi-class detection using different error type tagsets derived from the ERRANT framework. Output from this detection system is used as auxiliary input to fine-tune a novel encoder-decoder GEC model, and we... | Zheng Yuan, Shiva Taslimipoor, Christopher Davis, Christopher Bryant |  |
| 1157 |  |  [Towards Zero-shot Commonsense Reasoning with Self-supervised Refinement of Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.688) |  | 0 | Can we get existing language models and refine them for zero-shot commonsense reasoning? This paper presents an initial study exploring the feasibility of zero-shot commonsense reasoning for the Winograd Schema Challenge by formulating the task as self-supervised refinement of a pre-trained language model. In contrast to previous studies that rely on fine-tuning annotated datasets, we seek to boost conceptualization via loss landscape refinement. To this end, we propose a novel self-supervised... | Tassilo Klein, Moin Nabi |  |
| 1158 |  |  [To Share or not to Share: Predicting Sets of Sources for Model Transfer Learning](https://doi.org/10.18653/v1/2021.emnlp-main.689) |  | 0 | In low-resource settings, model transfer can help to overcome a lack of labeled data for many tasks and domains. However, predicting useful transfer sources is a challenging problem, as even the most similar sources might lead to unexpected negative transfer results. Thus, ranking methods based on task and text similarity — as suggested in prior work — may not be sufficient to identify promising sources. To tackle this problem, we propose a new approach to automatically determine which and how... | Lukas Lange, Jannik Strötgen, Heike Adel, Dietrich Klakow |  |
| 1159 |  |  [Self-Supervised Detection of Contextual Synonyms in a Multi-Class Setting: Phenotype Annotation Use Case](https://doi.org/10.18653/v1/2021.emnlp-main.690) |  | 0 | Contextualised word embeddings is a powerful tool to detect contextual synonyms. However, most of the current state-of-the-art (SOTA) deep learning concept extraction methods remain supervised and underexploit the potential of the context. In this paper, we propose a self-supervised pre-training approach which is able to detect contextual synonyms of concepts being training on the data created by shallow matching. We apply our methodology in the sparse multi-class setting (over 15,000 concepts)... | Jingqing Zhang, Luis Bolanos, Tong Li, Ashwani Tanwar, Guilherme Freire, Xian Yang, Julia Ive, Vibhor Gupta, Yike Guo |  |
| 1160 |  |  [ClauseRec: A Clause Recommendation Framework for AI-aided Contract Authoring](https://doi.org/10.18653/v1/2021.emnlp-main.691) |  | 0 | Contracts are a common type of legal document that frequent in several day-to-day business workflows. However, there has been very limited NLP research in processing such documents, and even lesser in generating them. These contracts are made up of clauses, and the unique nature of these clauses calls for specific methods to understand and generate such documents. In this paper, we introduce the task of clause recommendation, as a first step to aid and accelerate the authoring of contract... | Vinay Aggarwal, Aparna Garimella, Balaji Vasan Srinivasan, Anandhavelu Natarajan, Rajiv Jain |  |
| 1161 |  |  [Finnish Dialect Identification: The Effect of Audio and Text](https://doi.org/10.18653/v1/2021.emnlp-main.692) |  | 0 | Finnish is a language with multiple dialects that not only differ from each other in terms of accent (pronunciation) but also in terms of morphological forms and lexical choice. We present the first approach to automatically detect the dialect of a speaker based on a dialect transcript and transcript with audio recording in a dataset consisting of 23 different dialects. Our results show that the best accuracy is received by combining both of the modalities, as text only reaches to an overall... | Mika Hämäläinen, Khalid Alnajjar, Niko Partanen, Jack Rueter |  |
| 1162 |  |  [English Machine Reading Comprehension Datasets: A Survey](https://doi.org/10.18653/v1/2021.emnlp-main.693) |  | 0 | This paper surveys 60 English Machine Reading Comprehension datasets, with a view to providing a convenient resource for other researchers interested in this problem. We categorize the datasets according to their question and answer form and compare them across various dimensions including size, vocabulary, data source, method of creation, human performance level, and first question word. Our analysis reveals that Wikipedia is by far the most common data source and that there is a relative lack... | Daria Dzendzik, Jennifer Foster, Carl Vogel |  |
| 1163 |  |  [Expanding End-to-End Question Answering on Differentiable Knowledge Graphs with Intersection](https://doi.org/10.18653/v1/2021.emnlp-main.694) |  | 0 | End-to-end question answering using a differentiable knowledge graph is a promising technique that requires only weak supervision, produces interpretable results, and is fully differentiable. Previous implementations of this technique (Cohen et al, 2020) have focused on single-entity questions using a relation following operation. In this paper, we propose a model that explicitly handles multiple-entity questions by implementing a new intersection operation, which identifies the shared elements... | Priyanka Sen, Armin Oliya, Amir Saffari |  |
| 1164 |  |  [Structured Context and High-Coverage Grammar for Conversational Question Answering over Knowledge Graphs](https://doi.org/10.18653/v1/2021.emnlp-main.695) |  | 0 | We tackle the problem of weakly-supervised conversational Question Answering over large Knowledge Graphs using a neural semantic parsing approach. We introduce a new Logical Form (LF) grammar that can model a wide range of queries on the graph while remaining sufficiently simple to generate supervision data efficiently. Our Transformer-based model takes a JSON-like structure as input, allowing us to easily incorporate both Knowledge Graph and conversational contexts. This structured input is... | Pierre Marion, Pawel Krzysztof Nowak, Francesco Piccinno |  |
| 1165 |  |  [Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation](https://doi.org/10.18653/v1/2021.emnlp-main.696) |  | 0 | Despite recent progress, state-of-the-art question answering models remain vulnerable to a variety of adversarial attacks. While dynamic adversarial data collection, in which a human annotator tries to write examples that fool a model-in-the-loop, can improve model robustness, this process is expensive which limits the scale of the collected data. In this work, we are the first to use synthetic adversarial data generation to make question answering models more robust to human adversaries. We... | Max Bartolo, Tristan Thrush, Robin Jia, Sebastian Riedel, Pontus Stenetorp, Douwe Kiela |  |
| 1166 |  |  [BeliefBank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief](https://doi.org/10.18653/v1/2021.emnlp-main.697) |  | 0 | Although pretrained language models (PTLMs) contain significant amounts of world knowledge, they can still produce inconsistent answers to questions when probed, even after specialized training. As a result, it can be hard to identify what the model actually “believes” about the world, making it susceptible to inconsistent behavior and simple errors. Our goal is to reduce these problems. Our approach is to embed a PTLM in a broader system that also includes an evolving, symbolic memory of... | Nora Kassner, Oyvind Tafjord, Hinrich Schütze, Peter Clark |  |
| 1167 |  |  [MLEC-QA: A Chinese Multi-Choice Biomedical Question Answering Dataset](https://doi.org/10.18653/v1/2021.emnlp-main.698) |  | 0 | Question Answering (QA) has been successfully applied in scenarios of human-computer interaction such as chatbots and search engines. However, for the specific biomedical domain, QA systems are still immature due to expert-annotated datasets being limited by category and scale. In this paper, we present MLEC-QA, the largest-scale Chinese multi-choice biomedical QA dataset, collected from the National Medical Licensing Examination in China. The dataset is composed of five subsets with 136,236... | Jing Li, Shangping Zhong, Kaizhi Chen |  |
| 1168 |  |  [IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural Language Generation](https://doi.org/10.18653/v1/2021.emnlp-main.699) |  | 0 | Natural language generation (NLG) benchmarks provide an important avenue to measure progress and develop better NLG systems. Unfortunately, the lack of publicly available NLG benchmarks for low-resource languages poses a challenging barrier for building NLG systems that work well for languages with limited amounts of data. Here we introduce IndoNLG, the first benchmark to measure natural language generation (NLG) progress in three low-resource—yet widely spoken—languages of Indonesia:... | Samuel Cahyawijaya, Genta Indra Winata, Bryan Wilie, Karissa Vincentio, Xiaohong Li, Adhiguna Kuncoro, Sebastian Ruder, Zhi Yuan Lim, Syafri Bahar, Masayu Leylia Khodra, Ayu Purwarianti, Pascale Fung |  |
| 1169 |  |  [Is Multi-Hop Reasoning Really Explainable? Towards Benchmarking Reasoning Interpretability](https://doi.org/10.18653/v1/2021.emnlp-main.700) |  | 0 | Multi-hop reasoning has been widely studied in recent years to obtain more interpretable link prediction. However, we find in experiments that many paths given by these models are actually unreasonable, while little work has been done on interpretability evaluation for them. In this paper, we propose a unified framework to quantitatively evaluate the interpretability of multi-hop reasoning models so as to advance their development. In specific, we define three metrics, including path recall,... | Xin Lv, Yixin Cao, Lei Hou, Juanzi Li, Zhiyuan Liu, Yichi Zhang, Zelin Dai |  |
| 1170 |  |  [Global Explainability of BERT-Based Evaluation Metrics by Disentangling along Linguistic Factors](https://doi.org/10.18653/v1/2021.emnlp-main.701) |  | 0 | Evaluation metrics are a key ingredient for progress of text generation systems. In recent years, several BERT-based evaluation metrics have been proposed (including BERTScore, MoverScore, BLEURT, etc.) which correlate much better with human assessment of text generation quality than BLEU or ROUGE, invented two decades ago. However, little is known what these metrics, which are based on black-box language model representations, actually capture (it is typically assumed they model semantic... | Marvin Kaster, Wei Zhao, Steffen Eger |  |
| 1171 |  |  [Exploring Underexplored Limitations of Cross-Domain Text-to-SQL Generalization](https://doi.org/10.18653/v1/2021.emnlp-main.702) |  | 0 | Recently, there has been significant progress in studying neural networks for translating text descriptions into SQL queries under the zero-shot cross-domain setting. Despite achieving good performance on some public benchmarks, we observe that existing text-to-SQL models do not generalize when facing domain knowledge that does not frequently appear in the training data, which may render the worse prediction performance for unseen domains. In this work, we investigate the robustness of... | Yujian Gan, Xinyun Chen, Matthew Purver |  |
| 1172 |  |  [What happens if you treat ordinal ratings as interval data? Human evaluations in NLP are even more under-powered than you think](https://doi.org/10.18653/v1/2021.emnlp-main.703) |  | 0 | Previous work has shown that human evaluations in NLP are notoriously under-powered. Here, we argue that there are two common factors which make this problem even worse: NLP studies usually (a) treat ordinal data as interval data and (b) operate under high variance settings while the differences they are hoping to detect are often subtle. We demonstrate through simulation that ordinal mixed effects models are better able to detect small differences between models, especially in high variance... | David M. Howcroft, Verena Rieser |  |
| 1173 |  |  [NeuTral Rewriter: A Rule-Based and Neural Approach to Automatic Rewriting into Gender Neutral Alternatives](https://doi.org/10.18653/v1/2021.emnlp-main.704) |  | 0 | Recent years have seen an increasing need for gender-neutral and inclusive language. Within the field of NLP, there are various mono- and bilingual use cases where gender inclusive language is appropriate, if not preferred due to ambiguity or uncertainty in terms of the gender of referents. In this work, we present a rule-based and a neural approach to gender-neutral rewriting for English along with manually curated synthetic data (WinoBias+) and natural data (OpenSubtitles and Reddit)... | Eva Vanmassenhove, Chris Emmery, Dimitar Shterionov |  |
| 1174 |  |  [Benchmarking Commonsense Knowledge Base Population with an Effective Evaluation Dataset](https://doi.org/10.18653/v1/2021.emnlp-main.705) |  | 0 | Reasoning over commonsense knowledge bases (CSKB) whose elements are in the form of free-text is an important yet hard task in NLP. While CSKB completion only fills the missing links within the domain of the CSKB, CSKB population is alternatively proposed with the goal of reasoning unseen assertions from external resources. In this task, CSKBs are grounded to a large-scale eventuality (activity, state, and event) graph to discriminate whether novel triples from the eventuality graph are... | Tianqing Fang, Weiqi Wang, Sehyun Choi, Shibo Hao, Hongming Zhang, Yangqiu Song, Bin He |  |
| 1175 |  |  [Enhancing the Context Representation in Similarity-based Word Sense Disambiguation](https://doi.org/10.18653/v1/2021.emnlp-main.706) |  | 0 | In previous similarity-based WSD systems, studies have allocated much effort on learning comprehensive sense embeddings using contextual representations and knowledge sources. However, the context embedding of an ambiguous word is learned using only the sentence where the word appears, neglecting its global context. In this paper, we investigate the contribution of both word-level and sense-level global context of an ambiguous word for disambiguation. Experiments have shown that the... | Ming Wang, Jianzhang Zhang, Yinglin Wang |  |
| 1176 |  |  [Data Augmentation with Hierarchical SQL-to-Question Generation for Cross-domain Text-to-SQL Parsing](https://doi.org/10.18653/v1/2021.emnlp-main.707) |  | 0 | Data augmentation has attracted a lot of research attention in the deep learning era for its ability in alleviating data sparseness. The lack of labeled data for unseen evaluation databases is exactly the major challenge for cross-domain text-to-SQL parsing. Previous works either require human intervention to guarantee the quality of generated data, or fail to handle complex SQL queries. This paper presents a simple yet effective data augmentation framework. First, given a database, we... | Kun Wu, Lijie Wang, Zhenghua Li, Ao Zhang, Xinyan Xiao, Hua Wu, Min Zhang, Haifeng Wang |  |
| 1177 |  |  [SPARQLing Database Queries from Intermediate Question Decompositions](https://doi.org/10.18653/v1/2021.emnlp-main.708) |  | 0 | To translate natural language questions into executable database queries, most approaches rely on a fully annotated training set. Annotating a large dataset with queries is difficult as it requires query-language expertise. We reduce this burden using grounded in databases intermediate question representations. These representations are simpler to collect and were originally crowdsourced within the Break dataset (Wolfson et al., 2020). Our pipeline consists of two parts: a neural semantic... | Irina Saparina, Anton Osokin |  |
| 1178 |  |  [Time-aware Graph Neural Network for Entity Alignment between Temporal Knowledge Graphs](https://doi.org/10.18653/v1/2021.emnlp-main.709) |  | 0 | Entity alignment aims to identify equivalent entity pairs between different knowledge graphs (KGs). Recently, the availability of temporal KGs (TKGs) that contain time information created the need for reasoning over time in such TKGs. Existing embedding-based entity alignment approaches disregard time information that commonly exists in many large-scale KGs, leaving much room for improvement. In this paper, we focus on the task of aligning entity pairs between TKGs and propose a novel... | Chengjin Xu, Fenglong Su, Jens Lehmann |  |
| 1179 |  |  [Cross-Domain Label-Adaptive Stance Detection](https://doi.org/10.18653/v1/2021.emnlp-main.710) |  | 0 | Stance detection concerns the classification of a writer’s viewpoint towards a target. There are different task variants, e.g., stance of a tweet vs. a full article, or stance with respect to a claim vs. an (implicit) topic. Moreover, task definitions vary, which includes the label inventory, the data collection, and the annotation protocol. All these aspects hinder cross-domain studies, as they require changes to standard domain adaptation approaches. In this paper, we perform an in-depth... | Momchil Hardalov, Arnav Arora, Preslav Nakov, Isabelle Augenstein |  |
| 1180 |  |  [Text AutoAugment: Learning Compositional Augmentation Policy for Text Classification](https://doi.org/10.18653/v1/2021.emnlp-main.711) |  | 0 | Data augmentation aims to enrich training samples for alleviating the overfitting issue in low-resource or class-imbalanced situations. Traditional methods first devise task-specific operations such as Synonym Substitute, then preset the corresponding parameters such as the substitution rate artificially, which require a lot of prior knowledge and are prone to fall into the sub-optimum. Besides, the number of editing operations is limited in the previous methods, which decreases the diversity... | Shuhuai Ren, Jinchao Zhang, Lei Li, Xu Sun, Jie Zhou |  |
| 1181 |  |  [Distilling Relation Embeddings from Pretrained Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.712) |  | 0 | Pre-trained language models have been found to capture a surprisingly rich amount of lexical knowledge, ranging from commonsense properties of everyday concepts to detailed factual knowledge about named entities. Among others, this makes it possible to distill high-quality word vectors from pre-trained language models. However, it is currently unclear to what extent it is possible to distill relation embeddings, i.e. vectors that characterize the relationship between two words. Such relation... | Asahi Ushio, José CamachoCollados, Steven Schockaert |  |
| 1182 |  |  [Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning](https://doi.org/10.18653/v1/2021.emnlp-main.713) |  | 0 | Recent prompt-based approaches allow pretrained language models to achieve strong performances on few-shot finetuning by reformulating downstream tasks as a language modeling problem. In this work, we demonstrate that, despite its advantages on low data regimes, finetuned prompt-based models for sentence pair classification tasks still suffer from a common pitfall of adopting inference heuristics based on lexical overlap, e.g., models incorrectly assuming a sentence pair is of the same meaning... | Prasetya Ajie Utama, Nafise Sadat Moosavi, Victor Sanh, Iryna Gurevych |  |
| 1183 |  |  [A Differentiable Relaxation of Graph Segmentation and Alignment for AMR Parsing](https://doi.org/10.18653/v1/2021.emnlp-main.714) |  | 0 | Abstract Meaning Representations (AMR) are a broad-coverage semantic formalism which represents sentence meaning as a directed acyclic graph. To train most AMR parsers, one needs to segment the graph into subgraphs and align each such subgraph to a word in a sentence; this is normally done at preprocessing, relying on hand-crafted rules. In contrast, we treat both alignment and segmentation as latent variables in our model and induce them as part of end-to-end training. As marginalizing over... | Chunchuan Lyu, Shay B. Cohen, Ivan Titov |  |
| 1184 |  |  [Integrating Personalized PageRank into Neural Word Sense Disambiguation](https://doi.org/10.18653/v1/2021.emnlp-main.715) |  | 0 | Neural Word Sense Disambiguation (WSD) has recently been shown to benefit from the incorporation of pre-existing knowledge, such as that coming from the WordNet graph. However, state-of-the-art approaches have been successful in exploiting only the local structure of the graph, with only close neighbors of a given synset influencing the prediction. In this work, we improve a classification model by recomputing logits as a function of both the vanilla independently produced logits and the global... | Ahmed El Sheikh, Michele Bevilacqua, Roberto Navigli |  |
| 1185 |  |  [Cross-lingual Sentence Embedding using Multi-Task Learning](https://doi.org/10.18653/v1/2021.emnlp-main.716) |  | 0 | Multilingual sentence embeddings capture rich semantic information not only for measuring similarity between texts but also for catering to a broad range of downstream cross-lingual NLP tasks. State-of-the-art multilingual sentence embedding models require large parallel corpora to learn efficiently, which confines the scope of these models. In this paper, we propose a novel sentence embedding framework based on an unsupervised loss function for generating effective multilingual sentence... | Koustava Goswami, Sourav Dutta, Haytham Assem, Theodorus Fransen, John P. McCrae |  |
| 1186 |  |  [NB-MLM: Efficient Domain Adaptation of Masked Language Models for Sentiment Analysis](https://doi.org/10.18653/v1/2021.emnlp-main.717) |  | 0 | While Masked Language Models (MLM) are pre-trained on massive datasets, the additional training with the MLM objective on domain or task-specific data before fine-tuning for the final task is known to improve the final performance. This is usually referred to as the domain or task adaptation step. However, unlike the initial pre-training, this step is performed for each domain or task individually and is still rather slow, requiring several GPU days compared to several GPU hours required for... | Nikolay Arefyev, Dmitry Kharchev, Artem Shelmanov |  |
| 1187 |  |  [Revisiting Self-training for Few-shot Learning of Language Model](https://doi.org/10.18653/v1/2021.emnlp-main.718) |  | 0 | As unlabeled data carry rich task-relevant information, they are proven useful for few-shot learning of language model. The question is how to effectively make use of such data. In this work, we revisit the self-training technique for language model fine-tuning and present a state-of-the-art prompt-based few-shot learner, SFLM. Given two views of a text sample via weak and strong augmentation techniques, SFLM generates a pseudo label on the weakly augmented version. Then, the model predicts the... | Yiming Chen, Yan Zhang, Chen Zhang, Grandee Lee, Ran Cheng, Haizhou Li |  |
| 1188 |  |  [Bridging Perception, Memory, and Inference through Semantic Relations](https://doi.org/10.18653/v1/2021.emnlp-main.719) |  | 0 | There is a growing consensus that surface form alone does not enable models to learn meaning and gain language understanding. This warrants an interest in hybrid systems that combine the strengths of neural and symbolic methods. We favour triadic systems consisting of neural networks, knowledge bases, and inference engines. The network provides perception, that is, the interface between the system and its environment. The knowledge base provides explicit memory and thus immediate access to... | Johanna Björklund, Adam Dahlgren Lindström, Frank Drewes |  |
| 1189 |  |  [Unimodal and Crossmodal Refinement Network for Multimodal Sequence Fusion](https://doi.org/10.18653/v1/2021.emnlp-main.720) |  | 0 | Effective unimodal representation and complementary crossmodal representation fusion are both important in multimodal representation learning. Prior works often modulate one modal feature to another straightforwardly and thus, underutilizing both unimodal and crossmodal representation refinements, which incurs a bottleneck of performance improvement. In this paper, Unimodal and Crossmodal Refinement Network (UCRN) is proposed to enhance both unimodal and crossmodal representations.... | Xiaobao Guo, Adams WaiKin Kong, Huan Zhou, Xianfeng Wang, Min Wang |  |
| 1190 |  |  [YASO: A Targeted Sentiment Analysis Evaluation Dataset for Open-Domain Reviews](https://doi.org/10.18653/v1/2021.emnlp-main.721) |  | 0 | Current TSA evaluation in a cross-domain setup is restricted to the small set of review domains available in existing datasets. Such an evaluation is limited, and may not reflect true performance on sites like Amazon or Yelp that host diverse reviews from many domains. To address this gap, we present YASO – a new TSA evaluation dataset of open-domain user reviews. YASO contains 2,215 English sentences from dozens of review domains, annotated with target terms and their sentiment. Our analysis... | Matan Orbach, Orith ToledoRonen, Artem Spector, Ranit Aharonov, Yoav Katz, Noam Slonim |  |
| 1191 |  |  [An Empirical Study on Leveraging Position Embeddings for Target-oriented Opinion Words Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.722) |  | 0 | Target-oriented opinion words extraction (TOWE) (Fan et al., 2019b) is a new subtask of target-oriented sentiment analysis that aims to extract opinion words for a given aspect in text. Current state-of-the-art methods leverage position embeddings to capture the relative position of a word to the target. However, the performance of these methods depends on the ability to incorporate this information into word representations. In this paper, we explore a variety of text encoders based on... | Samuel Mensah, Kai Sun, Nikolaos Aletras |  |
| 1192 |  |  [Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis](https://doi.org/10.18653/v1/2021.emnlp-main.723) |  | 0 | In multimodal sentiment analysis (MSA), the performance of a model highly depends on the quality of synthesized embeddings. These embeddings are generated from the upstream process called multimodal fusion, which aims to extract and combine the input unimodal raw data to produce a richer multimodal representation. Previous work either back-propagates the task loss or manipulates the geometric property of feature spaces to produce favorable fusion results, which neglects the preservation of... | Wei Han, Hui Chen, Soujanya Poria |  |
| 1193 |  |  [BERT4GCN: Using BERT Intermediate Layers to Augment GCN for Aspect-based Sentiment Classification](https://doi.org/10.18653/v1/2021.emnlp-main.724) |  | 0 | Graph-based Aspect-based Sentiment Classification (ABSC) approaches have yielded state-of-the-art results, expecially when equipped with contextual word embedding from pre-training language models (PLMs). However, they ignore sequential features of the context and have not yet made the best of PLMs. In this paper, we propose a novel model, BERT4GCN, which integrates the grammatical sequential features from the PLM of BERT, and the syntactic knowledge from dependency graphs. BERT4GCN utilizes... | Zeguan Xiao, Jiarun Wu, Qingliang Chen, Congjian Deng |  |
| 1194 |  |  [Does Social Pressure Drive Persuasion in Online Fora?](https://doi.org/10.18653/v1/2021.emnlp-main.725) |  | 0 | Online forums such as ChangeMyView have been explored to research aspects of persuasion and argumentative quality in language. While previous research has focused on arguments between a view-holder and a persuader, we explore the premise that apart from the merits of arguments, persuasion is influenced by the ambient social community. We hypothesize that comments from the rest of the community can either affirm the original view or implicitly exert pressure to change it. We develop a structured... | Ayush Jain, Shashank Srivastava |  |
| 1195 |  |  [Aspect Sentiment Quad Prediction as Paraphrase Generation](https://doi.org/10.18653/v1/2021.emnlp-main.726) |  | 0 | Aspect-based sentiment analysis (ABSA) has been extensively studied in recent years, which typically involves four fundamental sentiment elements, including the aspect category, aspect term, opinion term, and sentiment polarity. Existing studies usually consider the detection of partial sentiment elements, instead of predicting the four elements in one shot. In this work, we introduce the Aspect Sentiment Quad Prediction (ASQP) task, aiming to jointly detect all sentiment elements in quads for... | Wenxuan Zhang, Yang Deng, Xin Li, Yifei Yuan, Lidong Bing, Wai Lam |  |
| 1196 |  |  [Cross-lingual Aspect-based Sentiment Analysis with Aspect Term Code-Switching](https://doi.org/10.18653/v1/2021.emnlp-main.727) |  | 0 | Many efforts have been made in solving the Aspect-based sentiment analysis (ABSA) task. While most existing studies focus on English texts, handling ABSA in resource-poor languages remains a challenging problem. In this paper, we consider the unsupervised cross-lingual transfer for the ABSA task, where only labeled data in the source language is available and we aim at transferring its knowledge to the target language having no labeled data. To this end, we propose an alignment-free label... | Wenxuan Zhang, Ruidan He, Haiyun Peng, Lidong Bing, Wai Lam |  |
| 1197 |  |  [Towards Label-Agnostic Emotion Embeddings](https://doi.org/10.18653/v1/2021.emnlp-main.728) |  | 0 | Research in emotion analysis is scattered across different label formats (e.g., polarity types, basic emotion categories, and affective dimensions), linguistic levels (word vs. sentence vs. discourse), and, of course, (few well-resourced but much more under-resourced) natural languages and text genres (e.g., product reviews, tweets, news). The resulting heterogeneity makes data and software developed under these conflicting constraints hard to compare and challenging to integrate. To resolve... | Sven Buechel, Luise Modersohn, Udo Hahn |  |
| 1198 |  |  [Collaborative Learning of Bidirectional Decoders for Unsupervised Text Style Transfer](https://doi.org/10.18653/v1/2021.emnlp-main.729) |  | 0 | Unsupervised text style transfer aims to alter the underlying style of the text to a desired value while keeping its style-independent semantics, without the support of parallel training corpora. Existing methods struggle to achieve both high style conversion rate and low content loss, exhibiting the over-transfer and under-transfer problems. We attribute these problems to the conflicting driving forces of the style conversion goal and content preservation goal. In this paper, we propose a... | Yun Ma, Yangbin Chen, Xudong Mao, Qing Li |  |
| 1199 |  |  [Exploring Non-Autoregressive Text Style Transfer](https://doi.org/10.18653/v1/2021.emnlp-main.730) |  | 0 | In this paper, we explore Non-AutoRegressive (NAR) decoding for unsupervised text style transfer. We first propose a base NAR model by directly adapting the common training scheme from its AutoRegressive (AR) counterpart. Despite the faster inference speed over the AR model, this NAR model sacrifices its transfer performance due to the lack of conditional dependence between output tokens. To this end, we investigate three techniques, i.e., knowledge distillation, contrastive learning, and... | Yun Ma, Qing Li |  |
| 1200 |  |  [PASTE: A Tagging-Free Decoding Framework Using Pointer Networks for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.731) |  | 0 | Aspect Sentiment Triplet Extraction (ASTE) deals with extracting opinion triplets, consisting of an opinion target or aspect, its associated sentiment, and the corresponding opinion term/span explaining the rationale behind the sentiment. Existing research efforts are majorly tagging-based. Among the methods taking a sequence tagging approach, some fail to capture the strong interdependence between the three opinion factors, whereas others fall short of identifying triplets with overlapping... | Rajdeep Mukherjee, Tapas Nayak, Yash Butala, Sourangshu Bhattacharya, Pawan Goyal |  |
| 1201 |  |  [Adaptive Proposal Generation Network for Temporal Sentence Localization in Videos](https://doi.org/10.18653/v1/2021.emnlp-main.732) |  | 0 | We address the problem of temporal sentence localization in videos (TSLV). Traditional methods follow a top-down framework which localizes the target segment with pre-defined segment proposals. Although they have achieved decent performance, the proposals are handcrafted and redundant. Recently, bottom-up framework attracts increasing attention due to its superior efficiency. It directly predicts the probabilities for each frame as a boundary. However, the performance of bottom-up model is... | Daizong Liu, Xiaoye Qu, Jianfeng Dong, Pan Zhou |  |
| 1202 |  |  [Progressively Guide to Attend: An Iterative Alignment Framework for Temporal Sentence Grounding](https://doi.org/10.18653/v1/2021.emnlp-main.733) |  | 0 | A key solution to temporal sentence grounding (TSG) exists in how to learn effective alignment between vision and language features extracted from an untrimmed video and a sentence description. Existing methods mainly leverage vanilla soft attention to perform the alignment in a single-step process. However, such single-step attention is insufficient in practice, since complicated relations between inter- and intra-modality are usually obtained through multi-step reasoning. In this paper, we... | Daizong Liu, Xiaoye Qu, Pan Zhou |  |
| 1203 |  |  [Language Models are Few-Shot Butlers](https://doi.org/10.18653/v1/2021.emnlp-main.734) |  | 0 | Pretrained language models demonstrate strong performance in most NLP tasks when fine-tuned on small task-specific datasets. Hence, these autoregressive models constitute ideal agents to operate in text-based environments where language understanding and generative capabilities are essential. Nonetheless, collecting expert demonstrations in such environments is a time-consuming endeavour. We introduce a two-stage procedure to learn from a small set of demonstrations and further improve by... | Vincent Micheli, François Fleuret |  |
| 1204 |  |  [R\^3Net: Relation-embedded Representation Reconstruction Network for Change Captioning](https://doi.org/10.18653/v1/2021.emnlp-main.735) |  | 0 | Change captioning is to use a natural language sentence to describe the fine-grained disagreement between two similar images. Viewpoint change is the most typical distractor in this task, because it changes the scale and location of the objects and overwhelms the representation of real change. In this paper, we propose a Relation-embedded Representation Reconstruction Network (Rˆ3Net) to explicitly distinguish the real change from the large amount of clutter and irrelevant changes.... | Yunbin Tu, Liang Li, Chenggang Yan, Shengxiang Gao, Zhengtao Yu |  |
| 1205 |  |  [Looking for Confirmations: An Effective and Human-Like Visual Dialogue Strategy](https://doi.org/10.18653/v1/2021.emnlp-main.736) |  | 0 | Generating goal-oriented questions in Visual Dialogue tasks is a challenging and longstanding problem. State-Of-The-Art systems are shown to generate questions that, although grammatically correct, often lack an effective strategy and sound unnatural to humans. Inspired by the cognitive literature on information search and cross-situational word learning, we design Confirm-it, a model based on a beam search re-ranking algorithm that guides an effective goal-oriented strategy by asking questions... | Alberto Testoni, Raffaella Bernardi |  |
| 1206 |  |  [A Unified Speaker Adaptation Approach for ASR](https://doi.org/10.18653/v1/2021.emnlp-main.737) |  | 0 | Transformer models have been used in automatic speech recognition (ASR) successfully and yields state-of-the-art results. However, its performance is still affected by speaker mismatch between training and test data. Further finetuning a trained model with target speaker data is the most natural approach for adaptation, but it takes a lot of compute and may cause catastrophic forgetting to the existing speakers. In this work, we propose a unified speaker adaptation approach consisting of... | Yingzhu Zhao, Chongjia Ni, CheungChi Leung, Shafiq R. Joty, Eng Siong Chng, Bin Ma |  |
| 1207 |  |  [Caption Enriched Samples for Improving Hateful Memes Detection](https://doi.org/10.18653/v1/2021.emnlp-main.738) |  | 0 | The recently introduced hateful meme challenge demonstrates the difficulty of determining whether a meme is hateful or not. Specifically, both unimodal language models and multimodal vision-language models cannot reach the human level of performance. Motivated by the need to model the contrast between the image content and the overlayed text, we suggest applying an off-the-shelf image captioning tool in order to capture the first. We demonstrate that the incorporation of such automatic captions... | Efrat Blaier, Itzik Malkiel, Lior Wolf |  |
| 1208 |  |  [Sparsity and Sentence Structure in Encoder-Decoder Attention of Summarization Systems](https://doi.org/10.18653/v1/2021.emnlp-main.739) |  | 0 | Transformer models have achieved state-of-the-art results in a wide range of NLP tasks including summarization. Training and inference using large transformer models can be computationally expensive. Previous work has focused on one important bottleneck, the quadratic self-attention mechanism in the encoder. Modified encoder architectures such as LED or LoBART use local attention patterns to address this problem for summarization. In contrast, this work focuses on the transformer’s... | Potsawee Manakul, Mark J. F. Gales |  |
| 1209 |  |  [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://doi.org/10.18653/v1/2021.emnlp-main.740) |  | 0 | Inductive transfer learning has taken the entire NLP field by storm, with models such as BERT and BART setting new state of the art on countless NLU tasks. However, most of the available models and research have been conducted for English. In this work, we introduce BARThez, the first large-scale pretrained seq2seq model for French. Being based on BART, BARThez is particularly well-suited for generative tasks. We evaluate BARThez on five discriminative tasks from the FLUE benchmark and two... | Moussa Kamal Eddine, Antoine J.P. Tixier, Michalis Vazirgiannis |  |
| 1210 |  |  [ARMAN: Pre-training with Semantically Selecting and Reordering of Sentences for Persian Abstractive Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.741) |  | 0 | Abstractive text summarization is one of the areas influenced by the emergence of pre-trained language models. Current pre-training works in abstractive summarization give more points to the summaries with more words in common with the main text and pay less attention to the semantic similarity between generated sentences and the original document. We propose ARMAN, a Transformer-based encoder-decoder model pre-trained with three novel objectives to address this issue. In ARMAN, salient... | Alireza Salemi, Emad Kebriaei, Ghazal Neisi Minaei, Azadeh Shakery |  |
| 1211 |  |  [Models and Datasets for Cross-Lingual Summarisation](https://doi.org/10.18653/v1/2021.emnlp-main.742) |  | 0 | We present a cross-lingual summarisation corpus with long documents in a source language associated with multi-sentence summaries in a target language. The corpus covers twelve language pairs and directions for four European languages, namely Czech, English, French and German, and the methodology for its creation can be applied to several other languages. We derive cross-lingual document-summary instances from Wikipedia by combining lead paragraphs and articles’ bodies from language aligned... | Laura PerezBeltrachini, Mirella Lapata |  |
| 1212 |  |  [Learning Opinion Summarizers by Selecting Informative Reviews](https://doi.org/10.18653/v1/2021.emnlp-main.743) |  | 0 | Opinion summarization has been traditionally approached with unsupervised, weakly-supervised and few-shot learning techniques. In this work, we collect a large dataset of summaries paired with user reviews for over 31,000 products, enabling supervised training. However, the number of reviews per product is large (320 on average), making summarization – and especially training a summarizer – impractical. Moreover, the content of many reviews is not reflected in the human-written summaries, and,... | Arthur Brazinskas, Mirella Lapata, Ivan Titov |  |
| 1213 |  |  [Enriching and Controlling Global Semantics for Text Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.744) |  | 0 | Recently, Transformer-based models have been proven effective in the abstractive summarization task by creating fluent and informative summaries. Nevertheless, these models still suffer from the short-range dependency problem, causing them to produce summaries that miss the key points of document. In this paper, we attempt to address this issue by introducing a neural topic model empowered with normalizing flow to capture the global semantics of the document, which are then integrated into the... | Thong Nguyen, Anh Tuan Luu, Truc Lu, Tho Quan |  |
| 1214 |  |  [Revisiting Tri-training of Dependency Parsers](https://doi.org/10.18653/v1/2021.emnlp-main.745) |  | 0 | We compare two orthogonal semi-supervised learning techniques, namely tri-training and pretrained word embeddings, in the task of dependency parsing. We explore language-specific FastText and ELMo embeddings and multilingual BERT embeddings. We focus on a low resource scenario as semi-supervised learning can be expected to have the most impact here. Based on treebank size and available ELMo models, we select Hungarian, Uyghur (a zero-shot language for mBERT) and Vietnamese. Furthermore, we... | Joachim Wagner, Jennifer Foster |  |
| 1215 |  |  [Bridge to Target Domain by Prototypical Contrastive Learning and Label Confusion: Re-explore Zero-Shot Learning for Slot Filling](https://doi.org/10.18653/v1/2021.emnlp-main.746) |  | 0 | Zero-shot cross-domain slot filling alleviates the data dependence in the case of data scarcity in the target domain, which has aroused extensive research. However, as most of the existing methods do not achieve effective knowledge transfer to the target domain, they just fit the distribution of the seen slot and show poor performance on unseen slot in the target domain. To solve this, we propose a novel approach based on prototypical contrastive learning with a dynamic label confusion strategy... | Liwen Wang, Xuefeng Li, Jiachi Liu, Keqing He, Yuanmeng Yan, Weiran Xu |  |
| 1216 |  |  [Neuralizing Regular Expressions for Slot Filling](https://doi.org/10.18653/v1/2021.emnlp-main.747) |  | 0 | Neural models and symbolic rules such as regular expressions have their respective merits and weaknesses. In this paper, we study the integration of the two approaches for the slot filling task by converting regular expressions into neural networks. Specifically, we first convert regular expressions into a special form of finite-state transducers, then unfold its approximate inference algorithm as a bidirectional recurrent neural model that performs slot filling via sequence labeling.... | Chengyue Jiang, Zijian Jin, Kewei Tu |  |
| 1217 |  |  [Causal Direction of Data Collection Matters: Implications of Causal and Anticausal Learning for NLP](https://doi.org/10.18653/v1/2021.emnlp-main.748) |  | 0 | The principle of independent causal mechanisms (ICM) states that generative processes of real world data consist of independent modules which do not influence or inform each other. While this idea has led to fruitful developments in the field of causal inference, it is not widely-known in the NLP community. In this work, we argue that the causal direction of the data collection process bears nontrivial implications that can explain a number of published NLP findings, such as differences in... | Zhijing Jin, Julius von Kügelgen, Jingwei Ni, Tejas Vaidhya, Ayush Kaushal, Mrinmaya Sachan, Bernhard Schölkopf |  |
| 1218 |  |  [Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning](https://doi.org/10.18653/v1/2021.emnlp-main.749) |  | 0 | Recent pretrained language models extend from millions to billions of parameters. Thus the need to fine-tune an extremely large pretrained model with a limited training corpus arises in various downstream tasks. In this paper, we propose a straightforward yet effective fine-tuning technique, Child-Tuning, which updates a subset of parameters (called child network) of large pretrained models via strategically masking out the gradients of the non-child network during the backward process.... | Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao Chang, Songfang Huang, Fei Huang |  |
| 1219 |  |  [Knowledge Graph Representation Learning using Ordinary Differential Equations](https://doi.org/10.18653/v1/2021.emnlp-main.750) |  | 0 | Knowledge Graph Embeddings (KGEs) have shown promising performance on link prediction tasks by mapping the entities and relations from a knowledge graph into a geometric space. The capability of KGEs in preserving graph characteristics including structural aspects and semantics, highly depends on the design of their score function, as well as the inherited abilities from the underlying geometry. Many KGEs use the Euclidean geometry which renders them incapable of preserving complex structures... | Mojtaba Nayyeri, Chengjin Xu, Franca Hoffmann, Mirza Mohtashim Alam, Jens Lehmann, Sahar Vahdati |  |
| 1220 |  |  [KnowMAN: Weakly Supervised Multinomial Adversarial Networks](https://doi.org/10.18653/v1/2021.emnlp-main.751) |  | 0 | The absence of labeled data for training neural models is often addressed by leveraging knowledge about the specific task, resulting in heuristic but noisy labels. The knowledge is captured in labeling functions, which detect certain regularities or patterns in the training samples and annotate corresponding labels for training. This process of weakly supervised training may result in an over-reliance on the signals captured by the labeling functions and hinder models to exploit other signals... | Luisa März, Ehsaneddin Asgari, Fabienne Braune, Franziska Zimmermann, Benjamin Roth |  |
| 1221 |  |  [ONION: A Simple and Effective Defense Against Textual Backdoor Attacks](https://doi.org/10.18653/v1/2021.emnlp-main.752) |  | 0 | Backdoor attacks are a kind of emergent training-time threat to deep neural networks (DNNs). They can manipulate the output of DNNs and possess high insidiousness. In the field of natural language processing, some attack methods have been proposed and achieve very high attack success rates on multiple popular models. Nevertheless, there are few studies on defending against textual backdoor attacks. In this paper, we propose a simple and effective textual backdoor defense named ONION, which is... | Fanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao, Zhiyuan Liu, Maosong Sun |  |
| 1222 |  |  [Value-aware Approximate Attention](https://doi.org/10.18653/v1/2021.emnlp-main.753) |  | 0 | Following the success of dot-product attention in Transformers, numerous approximations have been recently proposed to address its quadratic complexity with respect to the input length. However, all approximations thus far have ignored the contribution of the \*value vectors\* to the quality of approximation. In this work, we argue that research efforts should be directed towards approximating the true output of the attention sub-layer, which includes the value vectors. We propose a value-aware... | Ankit Gupta, Jonathan Berant |  |
| 1223 |  |  [Contrastive Domain Adaptation for Question Answering using Limited Text Corpora](https://doi.org/10.18653/v1/2021.emnlp-main.754) |  | 0 | Question generation has recently shown impressive results in customizing question answering (QA) systems to new domains. These approaches circumvent the need for manually annotated training data from the new domain and, instead, generate synthetic question-answer pairs that are used for training. However, existing methods for question generation rely on large amounts of synthetically generated datasets and costly computational resources, which render these techniques widely inaccessible when... | Zhenrui Yue, Bernhard Kratzwald, Stefan Feuerriegel |  |
| 1224 |  |  [Case-based Reasoning for Natural Language Queries over Knowledge Bases](https://doi.org/10.18653/v1/2021.emnlp-main.755) |  | 0 | It is often challenging to solve a complex problem from scratch, but much easier if we can access other similar problems with their solutions — a paradigm known as case-based reasoning (CBR). We propose a neuro-symbolic CBR approach (CBR-KBQA) for question answering over large knowledge bases. CBR-KBQA consists of a nonparametric memory that stores cases (question and logical forms) and a parametric model that can generate a logical form for a new question by retrieving cases that are relevant... | Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez, Jay Yoon Lee, Lizhen Tan, Lazaros Polymenakos, Andrew McCallum |  |
| 1225 |  |  [Distantly-Supervised Dense Retrieval Enables Open-Domain Question Answering without Evidence Annotation](https://doi.org/10.18653/v1/2021.emnlp-main.756) |  | 0 | Open-domain question answering answers a question based on evidence retrieved from a large corpus. State-of-the-art neural approaches require intermediate evidence annotations for training. However, such intermediate annotations are expensive, and methods that rely on them cannot transfer to the more common setting, where only question–answer pairs are available. This paper investigates whether models can learn to find evidence from a large corpus, with only distant supervision from answer... | Chen Zhao, Chenyan Xiong, Jordan L. BoydGraber, Hal Daumé III |  |
| 1226 |  |  [What's in a Name? Answer Equivalence For Open-Domain Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.757) |  | 0 | A flaw in QA evaluation is that annotations often only provide one gold answer. Thus, model predictions semantically equivalent to the answer but superficially different are considered incorrect. This work explores mining alias entities from knowledge bases and using them as additional gold answers (i.e., equivalent answers). We incorporate answers for two settings: evaluation with additional answers and model training with equivalent answers. We analyse three QA benchmarks: Natural Questions,... | Chenglei Si, Chen Zhao, Jordan L. BoydGraber |  |
| 1227 |  |  [Evaluation Paradigms in Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.758) |  | 0 | Question answering (QA) primarily descends from two branches of research: (1) Alan Turing’s investigation of machine intelligence at Manchester University and (2) Cyril Cleverdon’s comparison of library card catalog indices at Cranfield University. This position paper names and distinguishes these paradigms. Despite substantial overlap, subtle but significant distinctions exert an outsize influence on research. While one evaluation paradigm values creating more intelligent QA systems, the other... | Pedro Rodriguez, Jordan L. BoydGraber |  |
| 1228 |  |  [Numerical reasoning in machine reading comprehension tasks: are we there yet?](https://doi.org/10.18653/v1/2021.emnlp-main.759) |  | 0 | Numerical reasoning based machine reading comprehension is a task that involves reading comprehension along with using arithmetic operations such as addition, subtraction, sorting and counting. The DROP benchmark (Dua et al., 2019) is a recent dataset that has inspired the design of NLP models aimed at solving this task. The current standings of these models in the DROP leaderboard, over standard metrics, suggests that the models have achieved near-human performance. However, does this mean... | Hadeel AlNegheimish, Pranava Madhyastha, Alessandra Russo |  |
| 1229 |  |  [Set Generation Networks for End-to-End Knowledge Base Population](https://doi.org/10.18653/v1/2021.emnlp-main.760) |  | 0 | The task of knowledge base population (KBP) aims to discover facts about entities from texts and expand a knowledge base with these facts. Previous studies shape end-to-end KBP as a machine translation task, which is required to convert unordered fact into a sequence according to a pre-specified order. However, the facts stated in a sentence are unordered in essence. In this paper, we formulate end-to-end KBP as a direct set generation problem, avoiding considering the order of multiple facts.... | Dianbo Sui, Chenhao Wang, Yubo Chen, Kang Liu, Jun Zhao, Wei Bi |  |
| 1230 |  |  [Knowing False Negatives: An Adversarial Training Method for Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.761) |  | 0 | Distantly supervised relation extraction (RE) automatically aligns unstructured text with relation instances in a knowledge base (KB). Due to the incompleteness of current KBs, sentences implying certain relations may be annotated as N/A instances, which causes the so-called false negative (FN) problem. Current RE methods usually overlook this problem, inducing improper biases in both training and testing procedures. To address this issue, we propose a two-stage approach. First, it finds out... | Kailong Hao, Botao Yu, Wei Hu |  |
| 1231 |  |  [Progressive Adversarial Learning for Bootstrapping: A Case Study on Entity Set Expansion](https://doi.org/10.18653/v1/2021.emnlp-main.762) |  | 0 | Bootstrapping has become the mainstream method for entity set expansion. Conventional bootstrapping methods mostly define the expansion boundary using seed-based distance metrics, which heavily depend on the quality of selected seeds and are hard to be adjusted due to the extremely sparse supervision. In this paper, we propose BootstrapGAN, a new learning method for bootstrapping which jointly models the bootstrapping process and the boundary learning process in a GAN framework. Specifically,... | Lingyong Yan, Xianpei Han, Le Sun |  |
| 1232 |  |  [Uncovering Main Causalities for Long-tailed Information Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.763) |  | 0 | Information Extraction (IE) aims to extract structural information from unstructured texts. In practice, long-tailed distributions caused by the selection bias of a dataset may lead to incorrect correlations, also known as spurious correlations, between entities and labels in the conventional likelihood models. This motivates us to propose counterfactual IE (CFIE), a novel framework that aims to uncover the main causalities behind data in the view of causal inference. Specifically, 1) we first... | Guoshun Nan, Jiaqi Zeng, Rui Qiao, Zhijiang Guo, Wei Lu |  |
| 1233 |  |  [Maximal Clique Based Non-Autoregressive Open Information Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.764) |  | 0 | Open Information Extraction (OpenIE) aims to discover textual facts from a given sentence. In essence, the facts contained in plain text are unordered. However, the popular OpenIE systems usually output facts sequentially in the way of predicting the next fact conditioned on the previous decoded ones, which enforce an unnecessary order on the facts and involve the error accumulation between autoregressive steps. To break this bottleneck, we propose MacroIE, a novel non-autoregressive framework... | Bowen Yu, Yucheng Wang, Tingwen Liu, Hongsong Zhu, Limin Sun, Bin Wang |  |
| 1234 |  |  [A Relation-Oriented Clustering Method for Open Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.765) |  | 0 | The clustering-based unsupervised relation discovery method has gradually become one of the important methods of open relation extraction (OpenRE). However, high-dimensional vectors can encode complex linguistic information which leads to the problem that the derived clusters cannot explicitly align with the relational semantic classes. In this work, we propose a relation-oriented clustering model and use it to identify the novel relations in the unlabeled data. Specifically, to enable the... | Jun Zhao, Tao Gui, Qi Zhang, Yaqian Zhou |  |
| 1235 |  |  [Exploring Methods for Generating Feedback Comments for Writing Learning](https://doi.org/10.18653/v1/2021.emnlp-main.766) |  | 0 | The task of generating explanatory notes for language learners is known as feedback comment generation. Although various generation techniques are available, little is known about which methods are appropriate for this task. Nagata (2019) demonstrates the effectiveness of neural-retrieval-based methods in generating feedback comments for preposition use. Retrieval-based methods have limitations in that they can only output feedback comments existing in a given training data. Furthermore,... | Kazuaki Hanawa, Ryo Nagata, Kentaro Inui |  |
| 1236 |  |  [A Role-Selected Sharing Network for Joint Machine-Human Chatting Handoff and Service Satisfaction Analysis](https://doi.org/10.18653/v1/2021.emnlp-main.767) |  | 0 | Chatbot is increasingly thriving in different domains, however, because of unexpected discourse complexity and training data sparseness, its potential distrust hatches vital apprehension. Recently, Machine-Human Chatting Handoff (MHCH), predicting chatbot failure and enabling human-algorithm collaboration to enhance chatbot quality, has attracted increasing attention from industry and academia. In this study, we propose a novel model, Role-Selected Sharing Network (RSSN), which integrates both... | Jiawei Liu, Kaisong Song, Yangyang Kang, Guoxiu He, Zhuoren Jiang, Changlong Sun, Wei Lu, Xiaozhong Liu |  |
| 1237 |  |  [Meta Distant Transfer Learning for Pre-trained Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.768) |  | 0 | With the wide availability of Pre-trained Language Models (PLMs), multi-task fine-tuning across domains has been extensively applied. For tasks related to distant domains with different class label sets, PLMs may memorize non-transferable knowledge for the target domain and suffer from negative transfer. Inspired by meta-learning, we propose the Meta Distant Transfer Learning (Meta-DTL) framework to learn the cross-task knowledge for PLM-based methods. Meta-DTL first employs task representation... | Chengyu Wang, Haojie Pan, Minghui Qiu, Jun Huang, Fei Yang, Yin Zhang |  |
| 1238 |  |  [UniKER: A Unified Framework for Combining Embedding and Definite Horn Rule Reasoning for Knowledge Graph Inference](https://doi.org/10.18653/v1/2021.emnlp-main.769) |  | 0 | Knowledge graph inference has been studied extensively due to its wide applications. It has been addressed by two lines of research, i.e., the more traditional logical rule reasoning and the more recent knowledge graph embedding (KGE). Several attempts have been made to combine KGE and logical rules for better knowledge graph inference. Unfortunately, they either simply treat logical rules as additional constraints into KGE loss or use probabilistic model to approximate the exact logical... | Kewei Cheng, Ziqing Yang, Ming Zhang, Yizhou Sun |  |
| 1239 |  |  [Wasserstein Selective Transfer Learning for Cross-domain Text Mining](https://doi.org/10.18653/v1/2021.emnlp-main.770) |  | 0 | Transfer learning (TL) seeks to improve the learning of a data-scarce target domain by using information from source domains. However, the source and target domains usually have different data distributions, which may lead to negative transfer. To alleviate this issue, we propose a Wasserstein Selective Transfer Learning (WSTL) method. Specifically, the proposed method considers a reinforced selector to select helpful data for transfer learning. We further use a Wasserstein-based discriminator... | Lingyun Feng, Minghui Qiu, Yaliang Li, Haitao Zheng, Ying Shen |  |
| 1240 |  |  [Jointly Learning to Repair Code and Generate Commit Message](https://doi.org/10.18653/v1/2021.emnlp-main.771) |  | 0 | We propose a novel task of jointly repairing program codes and generating commit messages. Code repair and commit message generation are two essential and related tasks for software development. However, existing work usually performs the two tasks independently. We construct a multilingual triple dataset including buggy code, fixed code, and commit messages for this novel task. We first introduce a cascaded method with two models, one is to generate the fixed code first, and the other... | Jiaqi Bai, Long Zhou, Ambrosio Blanco, Shujie Liu, Furu Wei, Ming Zhou, Zhoujun Li |  |
| 1241 |  |  [Inflate and Shrink: Enriching and Reducing Interactions for Fast Text-Image Retrieval](https://doi.org/10.18653/v1/2021.emnlp-main.772) |  | 0 | By exploiting the cross-modal attention, cross-BERT methods have achieved state-of-the-art accuracy in cross-modal retrieval. Nevertheless, the heavy text-image interactions in the cross-BERT model are prohibitively slow for large-scale retrieval. Late-interaction methods trade off retrieval accuracy and efficiency by exploiting cross-modal interaction only in the late stage, attaining a satisfactory retrieval speed. In this work, we propose an inflating and shrinking approach to further boost... | Haoliang Liu, Tan Yu, Ping Li |  |
| 1242 |  |  [On Pursuit of Designing Multi-modal Transformer for Video Grounding](https://doi.org/10.18653/v1/2021.emnlp-main.773) |  | 0 | Video grounding aims to localize the temporal segment corresponding to a sentence query from an untrimmed video. Almost all existing video grounding methods fall into two frameworks: 1) Top-down model: It predefines a set of segment candidates and then conducts segment classification and regression. 2) Bottom-up model: It directly predicts frame-wise probabilities of the referential segment boundaries. However, all these methods are not end-to-end, i.e., they always rely on some time-consuming... | Meng Cao, Long Chen, Mike Zheng Shou, Can Zhang, Yuexian Zou |  |
| 1243 |  |  [COVR: A Test-Bed for Visually Grounded Compositional Generalization with Real Images](https://doi.org/10.18653/v1/2021.emnlp-main.774) |  | 0 | While interest in models that generalize at test time to new compositions has risen in recent years, benchmarks in the visually-grounded domain have thus far been restricted to synthetic images. In this work, we propose COVR, a new test-bed for visually-grounded compositional generalization with real images. To create COVR, we use real images annotated with scene graphs, and propose an almost fully automatic procedure for generating question-answer pairs along with a set of context images. COVR... | Ben Bogin, Shivanshu Gupta, Matt Gardner, Jonathan Berant |  |
| 1244 |  |  [Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers](https://doi.org/10.18653/v1/2021.emnlp-main.775) |  | 0 | Pretrained vision-and-language BERTs aim to learn representations that combine information from both modalities. We propose a diagnostic method based on cross-modal input ablation to assess the extent to which these models actually integrate cross-modal information. This method involves ablating inputs from one modality, either entirely or selectively based on cross-modal grounding alignments, and evaluating the model prediction performance on the other modality. Model performance is measured... | Stella Frank, Emanuele Bugliarello, Desmond Elliott |  |
| 1245 |  |  [HypMix: Hyperbolic Interpolative Data Augmentation](https://doi.org/10.18653/v1/2021.emnlp-main.776) |  | 0 | Interpolation-based regularisation methods for data augmentation have proven to be effective for various tasks and modalities. These methods involve performing mathematical operations over the raw input samples or their latent states representations - vectors that often possess complex hierarchical geometries. However, these operations are performed in the Euclidean space, simplifying these representations, which may lead to distorted and noisy interpolations. We propose HypMix, a novel model-,... | Ramit Sawhney, Megh Thakkar, Shivam Agarwal, Di Jin, Diyi Yang, Lucie Flek |  |
| 1246 |  |  [Integrating Deep Event-Level and Script-Level Information for Script Event Prediction](https://doi.org/10.18653/v1/2021.emnlp-main.777) |  | 0 | Scripts are structured sequences of events together with the participants, which are extracted from the texts. Script event prediction aims to predict the subsequent event given the historical events in the script. Two kinds of information facilitate this task, namely, the event-level information and the script-level information. At the event level, existing studies view an event as a verb with its participants, while neglecting other useful properties, such as the state of the participants. At... | Long Bai, Saiping Guan, Jiafeng Guo, Zixuan Li, Xiaolong Jin, Xueqi Cheng |  |
| 1247 |  |  [QA-Align: Representing Cross-Text Content Overlap by Aligning Question-Answer Propositions](https://doi.org/10.18653/v1/2021.emnlp-main.778) |  | 0 | Multi-text applications, such as multi-document summarization, are typically required to model redundancies across related texts. Current methods confronting consolidation struggle to fuse overlapping information. In order to explicitly represent content overlap, we propose to align predicate-argument relations across texts, providing a potential scaffold for information consolidation. We go beyond clustering coreferring mentions, and instead model overlap with respect to redundancy at a... | Daniela Brook Weiss, Paul Roit, Ayal Klein, Ori Ernst, Ido Dagan |  |
| 1248 |  |  [PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.779) |  | 0 | Large pre-trained language models for textual data have an unconstrained output space; at each decoding step, they can produce any of 10,000s of sub-word tokens. When fine-tuned to target constrained formal languages like SQL, these models often generate invalid code, rendering it unusable. We propose PICARD (code available at https://github.com/ElementAI/picard), a method for constraining auto-regressive decoders of language models through incremental parsing. PICARD helps to find valid output... | Torsten Scholak, Nathan Schucher, Dzmitry Bahdanau |  |
| 1249 |  |  [Exploiting Twitter as Source of Large Corpora of Weakly Similar Pairs for Semantic Sentence Embeddings](https://doi.org/10.18653/v1/2021.emnlp-main.780) |  | 0 | Semantic sentence embeddings are usually supervisedly built minimizing distances between pairs of embeddings of sentences labelled as semantically similar by annotators. Since big labelled datasets are rare, in particular for non-English languages, and expensive, recent studies focus on unsupervised approaches that require not-paired input sentences. We instead propose a language-independent approach to build large datasets of pairs of informal texts weakly similar, without manual human effort,... | Marco Di Giovanni, Marco Brambilla |  |
| 1250 |  |  [Guilt by Association: Emotion Intensities in Lexical Representations](https://doi.org/10.18653/v1/2021.emnlp-main.781) |  | 0 | What do linguistic models reveal about the emotions associated with words? In this study, we consider the task of estimating word-level emotion intensity scores for specific emotions, exploring unsupervised, supervised, and finally a self-supervised method of extracting emotional associations from pretrained vectors and models. Overall, we find that linguistic models carry substantial potential for inducing fine-grained emotion intensity scores, showing a far higher correlation with human... | Shahab Raji, Gerard de Melo |  |
| 1251 |  |  [Using Sociolinguistic Variables to Reveal Changing Attitudes Towards Sexuality and Gender](https://doi.org/10.18653/v1/2021.emnlp-main.782) |  | 0 | Individuals signal aspects of their identity and beliefs through linguistic choices. Studying these choices in aggregate allows us to examine large-scale attitude shifts within a population. Here, we develop computational methods to study word choice within a sociolinguistic lexical variable—alternate words used to express the same concept—in order to test for change in the United States towards sexuality and gender. We examine two variables: i) referents to significant others, such as the word... | Sky CHWang, David Jurgens |  |
| 1252 |  |  [Identifying Morality Frames in Political Tweets using Relational Learning](https://doi.org/10.18653/v1/2021.emnlp-main.783) |  | 0 | Extracting moral sentiment from text is a vital component in understanding public opinion, social movements, and policy decisions. The Moral Foundation Theory identifies five moral foundations, each associated with a positive and negative polarity. However, moral sentiment is often motivated by its targets, which can correspond to individuals or collective entities. In this paper, we introduce morality frames, a representation framework for organizing moral attitudes directed at different... | Shamik Roy, Maria Leonor Pacheco, Dan Goldwasser |  |
| 1253 |  |  [Measuring Sentence-Level and Aspect-Level (Un)certainty in Science Communications](https://doi.org/10.18653/v1/2021.emnlp-main.784) |  | 0 | Certainty and uncertainty are fundamental to science communication. Hedges have widely been used as proxies for uncertainty. However, certainty is a complex construct, with authors expressing not only the degree but the type and aspects of uncertainty in order to give the reader a certain impression of what is known. Here, we introduce a new study of certainty that models both the level and the aspects of certainty in scientific findings. Using a new dataset of 2167 annotated scientific... | Jiaxin Pei, David Jurgens |  |
| 1254 |  |  [Assessing the Reliability of Word Embedding Gender Bias Measures](https://doi.org/10.18653/v1/2021.emnlp-main.785) |  | 0 | Various measures have been proposed to quantify human-like social biases in word embeddings. However, bias scores based on these measures can suffer from measurement error. One indication of measurement quality is reliability, concerning the extent to which a measure produces consistent results. In this paper, we assess three types of reliability of word embedding gender bias measures, namely test-retest reliability, inter-rater consistency and internal consistency. Specifically, we investigate... | Yupei Du, Qixiang Fang, Dong Nguyen |  |
| 1255 |  |  [Rumor Detection on Twitter with Claim-Guided Hierarchical Graph Attention Networks](https://doi.org/10.18653/v1/2021.emnlp-main.786) |  | 0 | Rumors are rampant in the era of social media. Conversation structures provide valuable clues to differentiate between real and fake claims. However, existing rumor detection methods are either limited to the strict relation of user responses or oversimplify the conversation structure. In this study, to substantially reinforces the interaction of user opinions while alleviating the negative impact imposed by irrelevant posts, we first represent the conversation thread as an undirected... | Hongzhan Lin, Jing Ma, Mingfei Cheng, Zhiwei Yang, Liangliang Chen, Guang Chen |  |
| 1256 |  |  [Learning Bill Similarity with Annotated and Augmented Corpora of Bills](https://doi.org/10.18653/v1/2021.emnlp-main.787) |  | 0 | Bill writing is a critical element of representative democracy. However, it is often overlooked that most legislative bills are derived, or even directly copied, from other bills. Despite the significance of bill-to-bill linkages for understanding the legislative process, existing approaches fail to address semantic similarities across bills, let alone reordering or paraphrasing which are prevalent in legal document writing. In this paper, we overcome these limitations by proposing a 5-class... | Jiseon Kim, Elden Griggs, In Song Kim, Alice Oh |  |
| 1257 |  |  [SWEAT: Scoring Polarization of Topics across Different Corpora](https://doi.org/10.18653/v1/2021.emnlp-main.788) |  | 0 | Understanding differences of viewpoints across corpora is a fundamental task for computational social sciences. In this paper, we propose the Sliced Word Embedding Association Test (SWEAT), a novel statistical measure to compute the relative polarization of a topical wordset across two distributional representations. To this end, SWEAT uses two additional wordsets, deemed to have opposite valence, to represent two different poles. We validate our approach and illustrate a case study to show the... | Federico Bianchi, Marco Marelli, Paolo Nicoli, Matteo Palmonari |  |
| 1258 |  |  ["So You Think You're Funny?": Rating the Humour Quotient in Standup Comedy](https://doi.org/10.18653/v1/2021.emnlp-main.789) |  | 0 | Computational Humour (CH) has attracted the interest of Natural Language Processing and Computational Linguistics communities. Creating datasets for automatic measurement of humour quotient is difficult due to multiple possible interpretations of the content. In this work, we create a multi-modal humour-annotated dataset (~40 hours) using stand-up comedy clips. We devise a novel scoring mechanism to annotate the training data with a humour quotient score using the audience’s laughter. The... | Anirudh Mittal, Pranav Jeevan, Prerak Gandhi, Diptesh Kanojia, Pushpak Bhattacharyya |  |
| 1259 |  |  ["Was it "stated" or was it "claimed"?: How linguistic bias affects generative language models](https://doi.org/10.18653/v1/2021.emnlp-main.790) |  | 0 | People use language in subtle and nuanced ways to convey their beliefs. For instance, saying claimed instead of said casts doubt on the truthfulness of the underlying proposition, thus representing the author’s opinion on the matter. Several works have identified such linguistic classes of words that occur frequently in natural language text and are bias-inducing by virtue of their framing effects. In this paper, we test whether generative language models (including GPT-2 (CITATION) are... | Roma Patel, Ellie Pavlick |  |
| 1260 |  |  [PAUSE: Positive and Annealed Unlabeled Sentence Embedding](https://doi.org/10.18653/v1/2021.emnlp-main.791) |  | 0 | Sentence embedding refers to a set of effective and versatile techniques for converting raw text into numerical vector representations that can be used in a wide range of natural language processing (NLP) applications. The majority of these techniques are either supervised or unsupervised. Compared to the unsupervised methods, the supervised ones make less assumptions about optimization objectives and usually achieve better results. However, the training requires a large amount of labeled... | Lele Cao, Emil Larsson, Vilhelm von Ehrenheim, Dhiana Deva Cavalcanti Rocha, Anna Martin, Sonja Horn |  |
| 1261 |  |  [A Simple Geometric Method for Cross-Lingual Linguistic Transformations with Pre-trained Autoencoders](https://doi.org/10.18653/v1/2021.emnlp-main.792) |  | 0 | Powerful sentence encoders trained for multiple languages are on the rise. These systems are capable of embedding a wide range of linguistic properties into vector representations. While explicit probing tasks can be used to verify the presence of specific linguistic properties, it is unclear whether the vector representations can be manipulated to indirectly steer such properties. For efficient learning, we investigate the use of a geometric mapping in embedding space to transform linguistic... | Maarten De Raedt, Fréderic Godin, Pieter Buteneers, Chris Develder, Thomas Demeester |  |
| 1262 |  |  [An Information-Theoretic Characterization of Morphological Fusion](https://doi.org/10.18653/v1/2021.emnlp-main.793) |  | 0 | Linguistic typology generally divides synthetic languages into groups based on their morphological fusion. However, this measure has long been thought to be best considered a matter of degree. We present an information-theoretic measure, called informational fusion, to quantify the degree of fusion of a given set of morphological features in a surface form, which naturally provides such a graded scale. Informational fusion is able to encapsulate not only concatenative, but also nonconcatenative... | Neil Rathi, Michael Hahn, Richard Futrell |  |
| 1263 |  |  [The Effect of Efficient Messaging and Input Variability on Neural-Agent Iterated Language Learning](https://doi.org/10.18653/v1/2021.emnlp-main.794) |  | 0 | Natural languages display a trade-off among different strategies to convey syntactic structure, such as word order or inflection. This trade-off, however, has not appeared in recent simulations of iterated language learning with neural network agents (Chaabouni et al., 2019b). We re-evaluate this result in light of three factors that play an important role in comparable experiments from the Language Evolution field: (i) speaker bias towards efficient messaging, (ii) non systematic input... | Yuchen Lian, Arianna Bisazza, Tessa Verhoef |  |
| 1264 |  |  [On Classifying whether Two Texts are on the Same Side of an Argument](https://doi.org/10.18653/v1/2021.emnlp-main.795) |  | 0 | To ease the difficulty of argument stance classification, the task of same side stance classification (S3C) has been proposed. In contrast to actual stance classification, which requires a substantial amount of domain knowledge to identify whether an argument is in favor or against a certain issue, it is argued that, for S3C, only argument similarity within stances needs to be learned to successfully solve the task. We evaluate several transformer-based approaches on the dataset of the recent... | Erik Körner, Gregor Wiedemann, Ahmad Dawar Hakimi, Gerhard Heyer, Martin Potthast |  |
| 1265 |  |  [Chinese Opinion Role Labeling with Corpus Translation: A Pivot Study](https://doi.org/10.18653/v1/2021.emnlp-main.796) |  | 0 | Opinion Role Labeling (ORL), aiming to identify the key roles of opinion, has received increasing interest. Unlike most of the previous works focusing on the English language, in this paper, we present the first work of Chinese ORL. We construct a Chinese dataset by manually translating and projecting annotations from a standard English MPQA dataset. Then, we investigate the effectiveness of cross-lingual transfer methods, including model transfer and corpus translation. We exploit multilingual... | Ranran Zhen, Rui Wang, Guohong Fu, Chengguo Lv, Meishan Zhang |  |
| 1266 |  |  [MassiveSumm: a very large-scale, very multilingual, news summarisation dataset](https://doi.org/10.18653/v1/2021.emnlp-main.797) |  | 0 | Current research in automatic summarisation is unapologetically anglo-centered–a persistent state-of-affairs, which also predates neural net approaches. High-quality automatic summarisation datasets are notoriously expensive to create, posing a challenge for any language. However, with digitalisation, archiving, and social media advertising of newswire articles, recent work has shown how, with careful methodology application, large-scale datasets can now be simply gathered instead of written.... | Daniel Varab, Natalie Schluter |  |
| 1267 |  |  [AUTOSUMM: Automatic Model Creation for Text Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.798) |  | 0 | Recent efforts to develop deep learning models for text generation tasks such as extractive and abstractive summarization have resulted in state-of-the-art performances on various datasets. However, obtaining the best model configuration for a given dataset requires an extensive knowledge of deep learning specifics like model architecture, tuning parameters etc., and is often extremely challenging for a non-expert. In this paper, we propose methods to automatically create deep learning models... | Sharmila Reddy Nangi, Atharv Tyagi, Jay Mundra, Sagnik Mukherjee, Raj Snehal, Niyati Chhaya, Aparna Garimella |  |
| 1268 |  |  [Investigating the Helpfulness of Word-Level Quality Estimation for Post-Editing Machine Translation Output](https://doi.org/10.18653/v1/2021.emnlp-main.799) |  | 0 | Compared to fully manual translation, post-editing (PE) machine translation (MT) output can save time and reduce errors. Automatic word-level quality estimation (QE) aims to predict the correctness of words in MT output and holds great promise to aid PE by flagging problematic output. Quality of QE is crucial, as incorrect QE might lead to translators missing errors or wasting time on already correct MT output. Achieving accurate automatic word-level QE is very hard, and it is currently not... | Raksha Shenoy, Nico Herbig, Antonio Krüger, Josef van Genabith |  |
| 1269 |  |  [UNKs Everywhere: Adapting Multilingual Language Models to New Scripts](https://doi.org/10.18653/v1/2021.emnlp-main.800) |  | 0 | Massively multilingual language models such as multilingual BERT offer state-of-the-art cross-lingual transfer performance on a range of NLP tasks. However, due to limited capacity and large differences in pretraining data sizes, there is a profound performance gap between resource-rich and resource-poor target languages. The ultimate challenge is dealing with under-resourced languages not covered at all by the models and written in scripts unseen during pretraining. In this work, we propose a... | Jonas Pfeiffer, Ivan Vulic, Iryna Gurevych, Sebastian Ruder |  |
| 1270 |  |  [Neural Machine Translation Quality and Post-Editing Performance](https://doi.org/10.18653/v1/2021.emnlp-main.801) |  | 0 | We test the natural expectation that using MT in professional translation saves human processing time. The last such study was carried out by Sanchez-Torron and Koehn (2016) with phrase-based MT, artificially reducing the translation quality. In contrast, we focus on neural MT (NMT) of high quality, which has become the state-of-the-art approach since then and also got adopted by most translation companies. Through an experimental study involving over 30 professional translators for English ->... | Vilém Zouhar, Martin Popel, Ondrej Bojar, Ales Tamchyna |  |
| 1271 |  |  [XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation](https://doi.org/10.18653/v1/2021.emnlp-main.802) |  | 0 | Machine learning has brought striking advances in multilingual natural language processing capabilities over the past year. For example, the latest techniques have improved the state-of-the-art performance on the XTREME multilingual benchmark by more than 13 points. While a sizeable gap to human-level performance remains, improvements have been easier to achieve in some tasks than in others. This paper analyzes the current state of cross-lingual transfer learning and summarizes some lessons... | Sebastian Ruder, Noah Constant, Jan A. Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Dan Garrette, Graham Neubig, Melvin Johnson |  |
| 1272 |  |  [Contrastive Conditioning for Assessing Disambiguation in MT: A Case Study of Distilled Bias](https://doi.org/10.18653/v1/2021.emnlp-main.803) |  | 0 | Lexical disambiguation is a major challenge for machine translation systems, especially if some senses of a word are trained less often than others. Identifying patterns of overgeneralization requires evaluation methods that are both reliable and scalable. We propose contrastive conditioning as a reference-free black-box method for detecting disambiguation errors. Specifically, we score the quality of a translation by conditioning on variants of the source that provide contrastive... | Jannis Vamvas, Rico Sennrich |  |
| 1273 |  |  [Measuring Association Between Labels and Free-Text Rationales](https://doi.org/10.18653/v1/2021.emnlp-main.804) |  | 0 | In interpretable NLP, we require faithful rationales that reflect the model’s decision-making process for an explained instance. While prior work focuses on extractive rationales (a subset of the input words), we investigate their less-studied counterpart: free-text natural language rationales. We demonstrate that \*pipelines\*, models for faithful rationalization on information-extraction style tasks, do not work as well on “reasoning” tasks requiring free-text rationales. We turn to models... | Sarah Wiegreffe, Ana Marasovic, Noah A. Smith |  |
| 1274 |  |  [Discretized Integrated Gradients for Explaining Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.805) |  | 0 | As a prominent attribution-based explanation algorithm, Integrated Gradients (IG) is widely adopted due to its desirable explanation axioms and the ease of gradient computation. It measures feature importance by averaging the model’s output gradient interpolated along a straight-line path in the input data space. However, such straight-line interpolated points are not representative of text data due to the inherent discreteness of the word embedding space. This questions the faithfulness of the... | Soumya Sanyal, Xiang Ren |  |
| 1275 |  |  [Putting Words in BERT's Mouth: Navigating Contextualized Vector Spaces with Pseudowords](https://doi.org/10.18653/v1/2021.emnlp-main.806) |  | 0 | We present a method for exploring regions around individual points in a contextualized vector space (particularly, BERT space), as a way to investigate how these regions correspond to word senses. By inducing a contextualized “pseudoword” vector as a stand-in for a static embedding in the input layer, and then performing masked prediction of a word in the sentence, we are able to investigate the geometry of the BERT-space in a controlled manner around individual instances. Using our method on a... | Taelin Karidi, Yichu Zhou, Nathan Schneider, Omri Abend, Vivek Srikumar |  |
| 1276 |  |  [Rationales for Sequential Predictions](https://doi.org/10.18653/v1/2021.emnlp-main.807) |  | 0 | Sequence models are a critical component of modern NLP systems, but their predictions are difficult to explain. We consider model explanations though rationales, subsets of context that can explain individual model predictions. We find sequential rationales by solving a combinatorial optimization: the best rationale is the smallest subset of input tokens that would predict the same output as the full sequence. Enumerating all subsets is intractable, so we propose an efficient greedy algorithm... | Keyon Vafa, Yuntian Deng, David M. Blei, Alexander M. Rush |  |
| 1277 |  |  [FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging](https://doi.org/10.18653/v1/2021.emnlp-main.808) |  | 0 | Influence functions approximate the “influences” of training data-points for test predictions and have a wide variety of applications. Despite the popularity, their computational cost does not scale well with model and training data size. We present FastIF, a set of simple modifications to influence functions that significantly improves their run-time. We use k-Nearest Neighbors (kNN) to narrow the search space down to a subset of good candidate data points, identify the configurations that... | Han Guo, Nazneen Rajani, Peter Hase, Mohit Bansal, Caiming Xiong |  |
| 1278 |  |  [Studying word order through iterative shuffling](https://doi.org/10.18653/v1/2021.emnlp-main.809) |  | 0 | As neural language models approach human performance on NLP benchmark tasks, their advances are widely seen as evidence of an increasingly complex understanding of syntax. This view rests upon a hypothesis that has not yet been empirically tested: that word order encodes meaning essential to performing these tasks. We refute this hypothesis in many cases: in the GLUE suite and in various genres of English text, the words in a sentence or phrase can rarely be permuted to form a phrase carrying... | Nikolay Malkin, Sameera Lanka, Pranav Goel, Nebojsa Jojic |  |
| 1279 |  |  [Distantly-Supervised Named Entity Recognition with Noise-Robust Learning and Language Model Augmented Self-Training](https://doi.org/10.18653/v1/2021.emnlp-main.810) |  | 0 | We study the problem of training named entity recognition (NER) models using only distantly-labeled data, which can be automatically obtained by matching entity mentions in the raw text with entity types in a knowledge base. The biggest challenge of distantly-supervised NER is that the distant supervision may induce incomplete and noisy labels, rendering the straightforward application of supervised learning ineffective. In this paper, we propose (1) a noise-robust learning scheme comprised of... | Yu Meng, Yunyi Zhang, Jiaxin Huang, Xuan Wang, Yu Zhang, Heng Ji, Jiawei Han |  |
| 1280 |  |  [Open Knowledge Graphs Canonicalization using Variational Autoencoders](https://doi.org/10.18653/v1/2021.emnlp-main.811) |  | 0 | Noun phrases and Relation phrases in open knowledge graphs are not canonicalized, leading to an explosion of redundant and ambiguous subject-relation-object triples. Existing approaches to solve this problem take a two-step approach. First, they generate embedding representations for both noun and relation phrases, then a clustering algorithm is used to group them using the embeddings as features. In this work, we propose Canonicalizing Using Variational AutoEncoders and Side Information... | Sarthak Dash, Gaetano Rossiello, Nandana Mihindukulasooriya, Sugato Bagchi, Alfio Gliozzo |  |
| 1281 |  |  [HittER: Hierarchical Transformers for Knowledge Graph Embeddings](https://doi.org/10.18653/v1/2021.emnlp-main.812) |  | 0 | This paper examines the challenging problem of learning representations of entities and relations in a complex multi-relational knowledge graph. We propose HittER, a Hierarchical Transformer model to jointly learn Entity-relation composition and Relational contextualization based on a source entity’s neighborhood. Our proposed model consists of two different Transformer blocks: the bottom block extracts features of each entity-relation pair in the local neighborhood of the source entity and the... | Sanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao, Ruofei Zhang, Yangfeng Ji |  |
| 1282 |  |  [Few-Shot Named Entity Recognition: An Empirical Baseline Study](https://doi.org/10.18653/v1/2021.emnlp-main.813) |  | 0 | This paper presents an empirical study to efficiently build named entity recognition (NER) systems when a small amount of in-domain labeled data is available. Based upon recent Transformer-based self-supervised pre-trained language models (PLMs), we investigate three orthogonal schemes to improve model generalization ability in few-shot settings: (1) meta-learning to construct prototypes for different entity types, (2) task-specific supervised pre-training on noisy web data to extract... | Jiaxin Huang, Chunyuan Li, Krishan Subudhi, Damien Jose, Shobana Balakrishnan, Weizhu Chen, Baolin Peng, Jianfeng Gao, Jiawei Han |  |
| 1283 |  |  [XLEnt: Mining a Large Cross-lingual Entity Dataset with Lexical-Semantic-Phonetic Word Alignment](https://doi.org/10.18653/v1/2021.emnlp-main.814) |  | 0 | Cross-lingual named-entity lexica are an important resource to multilingual NLP tasks such as machine translation and cross-lingual wikification. While knowledge bases contain a large number of entities in high-resource languages such as English and French, corresponding entities for lower-resource languages are often missing. To address this, we propose Lexical-Semantic-Phonetic Align (LSP-Align), a technique to automatically mine cross-lingual entity lexica from mined web data. We demonstrate... | Ahmed ElKishky, Adithya Renduchintala, James Cross, Francisco Guzmán, Philipp Koehn |  |
| 1284 |  |  [Utilizing Relative Event Time to Enhance Event-Event Temporal Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.815) |  | 0 | Event time is one of the most important features for event-event temporal relation extraction. However, explicit event time information in text is sparse. For example, only about 20% of event mentions in TimeBank-Dense have event-time links. In this paper, we propose a joint model for event-event temporal relation classification and an auxiliary task, relative event time prediction, which predicts the event time as real numbers. We adopt the Stack-Propagation framework to incorporate predicted... | Haoyang Wen, Heng Ji |  |
| 1285 |  |  [Separating Retention from Extraction in the Evaluation of End-to-end Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.816) |  | 0 | State-of-the-art NLP models can adopt shallow heuristics that limit their generalization capability (McCoy et al., 2019). Such heuristics include lexical overlap with the training set in Named-Entity Recognition (Taille et al., 2020) and Event or Type heuristics in Relation Extraction (Rosenman et al., 2020). In the more realistic end-to-end RE setting, we can expect yet another heuristic: the mere retention of training relation triples. In this paper we propose two experiments confirming that... | Bruno Taillé, Vincent Guigue, Geoffrey Scoutheeten, Patrick Gallinari |  |
| 1286 |  |  [Automatic Text Evaluation through the Lens of Wasserstein Barycenters](https://doi.org/10.18653/v1/2021.emnlp-main.817) |  | 0 | A new metric BaryScore to evaluate text generation based on deep contextualized embeddings (e.g., BERT, Roberta, ELMo) is introduced. This metric is motivated by a new framework relying on optimal transport tools, i.e., Wasserstein distance and barycenter. By modelling the layer output of deep contextualized embeddings as a probability distribution rather than by a vector embedding; this framework provides a natural way to aggregate the different outputs through the Wasserstein space topology.... | Pierre Colombo, Guillaume Staerman, Chloé Clavel, Pablo Piantanida |  |
| 1287 |  |  [Visually Grounded Reasoning across Languages and Cultures](https://doi.org/10.18653/v1/2021.emnlp-main.818) |  | 0 | The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western European bias. Therefore, we devise a new protocol to construct an ImageNet-style hierarchy... | Fangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti, Siva Reddy, Nigel Collier, Desmond Elliott |  |
| 1288 |  |  [Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema](https://doi.org/10.18653/v1/2021.emnlp-main.819) |  | 0 | The Winograd Schema (WS) has been proposed as a test for measuring commonsense capabilities of models. Recently, pre-trained language model-based approaches have boosted performance on some WS benchmarks but the source of improvement is still not clear. This paper suggests that the apparent progress on WS may not necessarily reflect progress in commonsense reasoning. To support this claim, we first show that the current evaluation method of WS is sub-optimal and propose a modification that uses... | Yanai Elazar, Hongming Zhang, Yoav Goldberg, Dan Roth |  |
| 1289 |  |  [Robustness Evaluation of Entity Disambiguation Using Prior Probes: the Case of Entity Overshadowing](https://doi.org/10.18653/v1/2021.emnlp-main.820) |  | 0 | Entity disambiguation (ED) is the last step of entity linking (EL), when candidate entities are reranked according to the context they appear in. All datasets for training and evaluating models for EL consist of convenience samples, such as news articles and tweets, that propagate the prior probability bias of the entity distribution towards more frequently occurring entities. It was shown that the performance of the EL systems on such datasets is overestimated since it is possible to obtain... | Vera Provatorova, Samarth Bhargav, Svitlana Vakulenko, Evangelos Kanoulas |  |
| 1290 |  |  [IndoNLI: A Natural Language Inference Dataset for Indonesian](https://doi.org/10.18653/v1/2021.emnlp-main.821) |  | 0 | We present IndoNLI, the first human-elicited NLI dataset for Indonesian. We adapt the data collection protocol for MNLI and collect ~18K sentence pairs annotated by crowd workers and experts. The expert-annotated data is used exclusively as a test set. It is designed to provide a challenging test-bed for Indonesian NLI by explicitly incorporating various linguistic phenomena such as numerical reasoning, structural changes, idioms, or temporal and spatial reasoning. Experiment results show that... | Rahmad Mahendra, Alham Fikri Aji, Samuel Louvan, Fahrurrozi Rahman, Clara Vania |  |
| 1291 |  |  [Agreeing to Disagree: Annotating Offensive Language Datasets with Annotators' Disagreement](https://doi.org/10.18653/v1/2021.emnlp-main.822) |  | 0 | Since state-of-the-art approaches to offensive language detection rely on supervised learning, it is crucial to quickly adapt them to the continuously evolving scenario of social media. While several approaches have been proposed to tackle the problem from an algorithmic perspective, so to reduce the need for annotated data, less attention has been paid to the quality of these data. Following a trend that has emerged recently, we focus on the level of agreement among annotators while selecting... | Elisa Leonardelli, Stefano Menini, Alessio Palmero Aprosio, Marco Guerini, Sara Tonelli |  |
| 1292 |  |  [A Root of a Problem: Optimizing Single-Root Dependency Parsing](https://doi.org/10.18653/v1/2021.emnlp-main.823) |  | 0 | We describe two approaches to single-root dependency parsing that yield significant speed ups in such parsing. One approach has been previously used in dependency parsers in practice, but remains undocumented in the parsing literature, and is considered a heuristic. We show that this approach actually finds the optimal dependency tree. The second approach relies on simple reweighting of the inference graph being input to the dependency parser and has an optimal running time. Here, we again show... | Milos Stanojevic, Shay B. Cohen |  |
| 1293 |  |  [Efficient Sampling of Dependency Structure](https://doi.org/10.18653/v1/2021.emnlp-main.824) |  | 0 | Probabilistic distributions over spanning trees in directed graphs are a fundamental model of dependency structure in natural language processing, syntactic dependency trees. In NLP, dependency trees often have an additional root constraint: only one edge may emanate from the root. However, no sampling algorithm has been presented in the literature to account for this additional constraint. In this paper, we adapt two spanning tree sampling algorithms to faithfully sample dependency trees from... | Ran Zmigrod, Tim Vieira, Ryan Cotterell |  |
| 1294 |  |  [Reducing Discontinuous to Continuous Parsing with Pointer Network Reordering](https://doi.org/10.18653/v1/2021.emnlp-main.825) |  | 0 | Discontinuous constituent parsers have always lagged behind continuous approaches in terms of accuracy and speed, as the presence of constituents with discontinuous yield introduces extra complexity to the task. However, a discontinuous tree can be converted into a continuous variant by reordering tokens. Based on that, we propose to reduce discontinuous parsing to a continuous problem, which can then be directly solved by any off-the-shelf continuous parser. To that end, we develop a Pointer... | Daniel FernándezGonzález, Carlos GómezRodríguez |  |
| 1295 |  |  [A New Representation for Span-based CCG Parsing](https://doi.org/10.18653/v1/2021.emnlp-main.826) |  | 0 | This paper proposes a new representation for CCG derivations. CCG derivations are represented as trees whose nodes are labeled with categories strictly restricted by CCG rule schemata. This characteristic is not suitable for span-based parsing models because they predict node labels independently. In other words, span-based models may generate invalid CCG derivations that violate the rule schemata. Our proposed representation decomposes CCG derivations into several independent pieces and... | Yoshihide Kato, Shigeki Matsubara |  |
| 1296 |  |  [What to Pre-Train on? Efficient Intermediate Task Selection](https://doi.org/10.18653/v1/2021.emnlp-main.827) |  | 0 | Intermediate task fine-tuning has been shown to culminate in large transfer gains across many NLP tasks. With an abundance of candidate datasets as well as pre-trained language models, it has become infeasible to experiment with all combinations to find the best transfer setting. In this work, we provide a comprehensive comparison of different methods for efficiently identifying beneficial tasks for intermediate transfer learning. We focus on parameter and computationally efficient adapter... | Clifton Poth, Jonas Pfeiffer, Andreas Rücklé, Iryna Gurevych |  |
| 1297 |  |  [PermuteFormer: Efficient Relative Position Encoding for Long Sequences](https://doi.org/10.18653/v1/2021.emnlp-main.828) |  | 0 | A recent variation of Transformer, Performer, scales Transformer to longer sequences with a linear attention mechanism. However, it is not compatible with relative position encoding, which has advantages over absolute position encoding. In this paper, we discuss possible ways to add relative position encoding to Performer. Based on the analysis, we propose PermuteFormer, a Performer-based model with relative position encoding that scales linearly on long sequences. PermuteFormer applies... | Peng Chen |  |
| 1298 |  |  [Block Pruning For Faster Transformers](https://doi.org/10.18653/v1/2021.emnlp-main.829) |  | 0 | Pre-training has improved model accuracy for both classification and generation tasks at the cost of introducing much larger and slower models. Pruning methods have proven to be an effective way of reducing model size, whereas distillation methods are proven for speeding up inference. We introduce a block pruning approach targeting both small and fast models. Our approach extends structured methods by considering blocks of any size and integrates this structure into the movement pruning... | François Lagunas, Ella Charlaix, Victor Sanh, Alexander M. Rush |  |
| 1299 |  |  [Finetuning Pretrained Transformers into RNNs](https://doi.org/10.18653/v1/2021.emnlp-main.830) |  | 0 | Transformers have outperformed recurrent neural networks (RNNs) in natural language generation. But this comes with a signifi- cant computational cost, as the attention mechanism’s complexity scales quadratically with sequence length. Efficient transformer variants have received increasing interest in recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature... | Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, Noah A. Smith |  |
| 1300 |  |  [How to Train BERT with an Academic Budget](https://doi.org/10.18653/v1/2021.emnlp-main.831) |  | 0 | While large language models a la BERT are used ubiquitously in NLP, pretraining them is considered a luxury that only a few well-funded industry labs can afford. How can one train such models with a more modest budget? We present a recipe for pretraining a masked language model in 24 hours using a single low-end deep learning server. We demonstrate that through a combination of software optimizations, design choices, and hyperparameter tuning, it is possible to produce models that are... | Peter Izsak, Moshe Berchansky, Omer Levy |  |
| 1301 |  |  [Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression](https://doi.org/10.18653/v1/2021.emnlp-main.832) |  | 0 | Recent studies on compression of pretrained language models (e.g., BERT) usually use preserved accuracy as the metric for evaluation. In this paper, we propose two new metrics, label loyalty and probability loyalty that measure how closely a compressed model (i.e., student) mimics the original model (i.e., teacher). We also explore the effect of compression with regard to robustness under adversarial attacks. We benchmark quantization, pruning, knowledge distillation and progressive module... | Canwen Xu, Wangchunshu Zhou, Tao Ge, Ke Xu, Julian J. McAuley, Furu Wei |  |
| 1302 |  |  [IndoBERTweet: A Pretrained Language Model for Indonesian Twitter with Effective Domain-Specific Vocabulary Initialization](https://doi.org/10.18653/v1/2021.emnlp-main.833) |  | 0 | We present IndoBERTweet, the first large-scale pretrained model for Indonesian Twitter that is trained by extending a monolingually-trained Indonesian BERT model with additive domain-specific vocabulary. We focus in particular on efficient model adaptation under vocabulary mismatch, and benchmark different ways of initializing the BERT embedding layer for new word types. We find that initializing with the average BERT subword embedding makes pretraining five times faster, and is more effective... | Fajri Koto, Jey Han Lau, Timothy Baldwin |  |
| 1303 |  |  [Pushing on Text Readability Assessment: A Transformer Meets Handcrafted Linguistic Features](https://doi.org/10.18653/v1/2021.emnlp-main.834) |  | 0 | We report two essential improvements in readability assessment: 1. three novel features in advanced semantics and 2. the timely evidence that traditional ML models (e.g. Random Forest, using handcrafted features) can combine with transformers (e.g. RoBERTa) to augment model performance. First, we explore suitable transformers and traditional ML models. Then, we extract 255 handcrafted linguistic features using self-developed extraction software. Finally, we assemble those to create several... | Bruce W. Lee, Yoo Sung Jang, Jason HyungJong Lee |  |
| 1304 |  |  [Types of Out-of-Distribution Texts and How to Detect Them](https://doi.org/10.18653/v1/2021.emnlp-main.835) |  | 0 | Despite agreement on the importance of detecting out-of-distribution (OOD) examples, there is little consensus on the formal definition of the distribution shifts of OOD examples and how to best detect them. We categorize these examples as exhibiting a background shift or semantic shift, and find that the two major approaches to OOD detection, calibration and density estimation (language modeling for text), have distinct behavior on these types of OOD data. Across 14 pairs of in-distribution... | Udit Arora, William Huang, He He |  |
| 1305 |  |  [Self-training with Few-shot Rationalization](https://doi.org/10.18653/v1/2021.emnlp-main.836) |  | 0 | While pre-trained language models have obtained state-of-the-art performance for several natural language understanding tasks, they are quite opaque in terms of their decision-making process. While some recent works focus on rationalizing neural predictions by highlighting salient concepts in the text as justifications or rationales, they rely on thousands of labeled training examples for both task labels as well as annotated rationales for every instance. Such extensive large-scale annotations... | Meghana Moorthy Bhat, Alessandro Sordoni, Subhabrata Mukherjee |  |
| 1306 |  |  [MTAdam: Automatic Balancing of Multiple Training Loss Terms](https://doi.org/10.18653/v1/2021.emnlp-main.837) |  | 0 | When training neural models, it is common to combine multiple loss terms. The balancing of these terms requires considerable human effort and is computationally demanding. Moreover, the optimal trade-off between the loss terms can change as training progresses, e.g., for adversarial terms. In this work, we generalize the Adam optimization algorithm to handle multiple loss terms. The guiding principle is that for every layer, the gradient magnitude of the terms should be balanced. To this end,... | Itzik Malkiel, Lior Wolf |  |
| 1307 |  |  [Softmax Tree: An Accurate, Fast Classifier When the Number of Classes Is Large](https://doi.org/10.18653/v1/2021.emnlp-main.838) |  | 0 | Classification problems having thousands or more classes naturally occur in NLP, for example language models or document classification. A softmax or one-vs-all classifier naturally handles many classes, but it is very slow at inference time, because every class score must be calculated to find the top class. We propose the “softmax tree”, consisting of a binary tree having sparse hyperplanes at the decision nodes (which make hard, not soft, decisions) and small softmax classifiers at the... | Arman Zharmagambetov, Magzhan Gabidolla, Miguel Á. CarreiraPerpiñán |  |
| 1308 |  |  [Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning](https://doi.org/10.18653/v1/2021.emnlp-main.839) |  | 0 | Distantly supervised named entity recognition (DS-NER) efficiently reduces labor costs but meanwhile intrinsically suffers from the label noise due to the strong assumption of distant supervision. Typically, the wrongly labeled instances comprise numbers of incomplete and inaccurate annotations, while most prior denoising works are only concerned with one kind of noise and fail to fully explore useful information in the training set. To address this issue, we propose a robust learning paradigm... | Xinghua Zhang, Bowen Yu, Tingwen Liu, Zhenyu Zhang, Jiawei Sheng, Mengge Xue, Hongbo Xu |  |
| 1309 |  |  [Multivalent Entailment Graphs for Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.840) |  | 0 | Drawing inferences between open-domain natural language predicates is a necessity for true language understanding. There has been much progress in unsupervised learning of entailment graphs for this purpose. We make three contributions: (1) we reinterpret the Distributional Inclusion Hypothesis to model entailment between predicates of different valencies, like DEFEAT(Biden, Trump) entails WIN(Biden); (2) we actualize this theory by learning unsupervised Multivalent Entailment Graphs of... | Nick McKenna, Liane Guillou, Mohammad Javad Hosseini, Sander Bijl de Vroe, Mark Johnson, Mark Steedman |  |
| 1310 |  |  [Is Everything in Order? A Simple Way to Order Sentences](https://doi.org/10.18653/v1/2021.emnlp-main.841) |  | 0 | The task of organizing a shuffled set of sentences into a coherent text has been used to evaluate a machine’s understanding of causal and temporal relations. We formulate the sentence ordering task as a conditional text-to-marker generation problem. We present Reorder-BART (Re-BART) that leverages a pre-trained Transformer-based model to identify a coherent order for a given set of shuffled sentences. The model takes a set of shuffled sentences with sentence-specific markers as input and... | Somnath Basu Roy Chowdhury, Faeze Brahman, Snigdha Chaturvedi |  |
| 1311 |  |  [VeeAlign: Multifaceted Context Representation Using Dual Attention for Ontology Alignment](https://doi.org/10.18653/v1/2021.emnlp-main.842) |  | 0 | Ontology Alignment is an important research problem applied to various fields such as data integration, data transfer, data preparation, etc. State-of-the-art (SOTA) Ontology Alignment systems typically use naive domain-dependent approaches with handcrafted rules or domain-specific architectures, making them unscalable and inefficient. In this work, we propose VeeAlign, a Deep Learning based model that uses a novel dual-attention mechanism to compute the contextualized representation of a... | Vivek Iyer, Arvind Agarwal, Harshit Kumar |  |
| 1312 |  |  [Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization](https://doi.org/10.18653/v1/2021.emnlp-main.843) |  | 0 | Modern semantic parsers suffer from two principal limitations. First, training requires expensive collection of utterance-program pairs. Second, semantic parsers fail to generalize at test time to new compositions/structures that have not been observed during training. Recent research has shown that automatic generation of synthetic utterance-program pairs can alleviate the first problem, but its potential for the second has thus far been under-explored. In this work, we investigate automatic... | Inbar Oren, Jonathan Herzig, Jonathan Berant |  |
| 1313 |  |  [GeneSis: A Generative Approach to Substitutes in Context](https://doi.org/10.18653/v1/2021.emnlp-main.844) |  | 0 | The lexical substitution task aims at generating a list of suitable replacements for a target word in context, ideally keeping the meaning of the modified text unchanged. While its usage has increased in recent years, the paucity of annotated data prevents the finetuning of neural models on the task, hindering the full fruition of recently introduced powerful architectures such as language models. Furthermore, lexical substitution is usually evaluated in a framework that is strictly bound to a... | Caterina Lacerra, Rocco Tripodi, Roberto Navigli |  |
| 1314 |  |  [Semi-Supervised Exaggeration Detection of Health Science Press Releases](https://doi.org/10.18653/v1/2021.emnlp-main.845) |  | 0 | Public trust in science depends on honest and factual communication of scientific papers. However, recent studies have demonstrated a tendency of news media to misrepresent scientific papers by exaggerating their findings. Given this, we present a formalization of and study into the problem of exaggeration detection in science communication. While there are an abundance of scientific papers and popular media articles written about them, very rarely do the articles include a direct link to the... | Dustin Wright, Isabelle Augenstein |  |
| 1315 |  |  [Phrase-BERT: Improved Phrase Embeddings from BERT with an Application to Corpus Exploration](https://doi.org/10.18653/v1/2021.emnlp-main.846) |  | 0 | Phrase representations derived from BERT often do not exhibit complex phrasal compositionality, as the model relies instead on lexical similarity to determine semantic relatedness. In this paper, we propose a contrastive fine-tuning objective that enables BERT to produce more powerful phrase embeddings. Our approach (Phrase-BERT) relies on a dataset of diverse phrasal paraphrases, which is automatically generated using a paraphrase generation model, as well as a large-scale dataset of phrases... | Shufan Wang, Laure Thompson, Mohit Iyyer |  |
| 1316 |  |  [Detecting Contact-Induced Semantic Shifts: What Can Embedding-Based Methods Do in Practice?](https://doi.org/10.18653/v1/2021.emnlp-main.847) |  | 0 | This study investigates the applicability of semantic change detection methods in descriptively oriented linguistic research. It specifically focuses on contact-induced semantic shifts in Quebec English. We contrast synchronic data from different regions in order to identify the meanings that are specific to Quebec and potentially related to language contact. Type-level embeddings are used to detect new semantic shifts, and token-level embeddings to isolate regionally specific occurrences. We... | Filip Miletic, Anne PrzewoznyDesriaux, Ludovic Tanguy |  |
