# EMNLP2021

## 会议论文列表

本会议共有 1316 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [Frontmatter](https://aclanthology.org/2021.emnlp-demo.0) |  | 0 |  | Heike Adel, Shuming Shi |  |
| 2 |  |  [MiSS: An Assistant for Multi-Style Simultaneous Translation](https://doi.org/10.18653/v1/2021.emnlp-demo.1) |  | 0 | In this paper, we present MiSS, an assistant for multi-style simultaneous translation. Our proposed translation system has five key features: highly accurate translation, simultaneous translation, translation for multiple text styles, back-translation for translation quality evaluation, and... | Zuchao Li, Kevin Parnow, Masao Utiyama, Eiichiro Sumita, Hai Zhao |  |
| 3 |  |  [Automatic Construction of Enterprise Knowledge Base](https://doi.org/10.18653/v1/2021.emnlp-demo.2) |  | 0 | In this paper, we present an automatic knowledge base construction system from large scale enterprise documents with minimal efforts of human intervention. In the design and deployment of such a knowledge mining system for enterprise, we faced several challenges including data distributional shift,... | Junyi Chai, Yujie He, Homa Hashemi, Bing Li, Daraksha Parveen, Ranganath Kondapally, Wenjin Xu |  |
| 4 |  |  [LightTag: Text Annotation Platform](https://doi.org/10.18653/v1/2021.emnlp-demo.3) |  | 0 | Text annotation tools assume that their user’s goal is to create a labeled corpus. However,users view annotation as a necessary evil on the way to deliver business value through NLP.Thus an annotation tool should optimize for the throughput of the global NLP process, not only the productivity of... | Tal Perry |  |
| 5 |  |  [TransIns: Document Translation with Markup Reinsertion](https://doi.org/10.18653/v1/2021.emnlp-demo.4) |  | 0 | For many use cases, it is required that MT does not just translate raw text, but complex formatted documents (e.g. websites, slides, spreadsheets) and the result of the translation should reflect the formatting. This is challenging, as markup can be nested, apply to spans contiguous in source but... | Jörg Steffen, Josef van Genabith |  |
| 6 |  |  [ET: A Workstation for Querying, Editing and Evaluating Annotated Corpora](https://doi.org/10.18653/v1/2021.emnlp-demo.5) |  | 0 | In this paper we explore the functionalities of ET, a suite designed to support linguistic research and natural language processing tasks using corpora annotated in the CoNLL-U format. These goals are achieved by two integrated environments – Interrogatório, an environment for querying and editing... | Elvis de Souza, Cláudia Freitas |  |
| 7 |  |  [N-LTP: An Open-source Neural Language Technology Platform for Chinese](https://doi.org/10.18653/v1/2021.emnlp-demo.6) |  | 0 | We introduce N-LTP, an open-source neural language technology platform supporting six fundamental Chinese NLP tasks: lexical analysis (Chinese word segmentation, part-of-speech tagging, and named entity recognition), syntactic parsing (dependency parsing), and semantic parsing (semantic dependency... | Wanxiang Che, Yunlong Feng, Libo Qin, Ting Liu |  |
| 8 |  |  [COMBO: State-of-the-Art Morphosyntactic Analysis](https://doi.org/10.18653/v1/2021.emnlp-demo.7) |  | 0 | We introduce COMBO – a fully neural NLP system for accurate part-of-speech tagging, morphological analysis, lemmatisation, and (enhanced) dependency parsing. It predicts categorical morphosyntactic features whilst also exposes their vector representations, extracted from hidden layers. COMBO is an... | Mateusz Klimaszewski, Alina Wróblewska |  |
| 9 |  |  [ExcavatorCovid: Extracting Events and Relations from Text Corpora for Temporal and Causal Analysis for COVID-19](https://doi.org/10.18653/v1/2021.emnlp-demo.8) |  | 0 | Timely responses from policy makers to mitigate the impact of the COVID-19 pandemic rely on a comprehensive grasp of events, their causes, and their impacts. These events are reported at such a speed and scale as to be overwhelming. In this paper, we present ExcavatorCovid, a machine reading system... | Bonan Min, Benjamin Rozonoyer, Haoling Qiu, Alexander Zamanian, Nianwen Xue, Jessica MacBride |  |
| 10 |  |  [KOAS: Korean Text Offensiveness Analysis System](https://doi.org/10.18653/v1/2021.emnlp-demo.9) |  | 0 | Warning: This manuscript contains a certain level of offensive expression. As communication through social media platforms has grown immensely, the increasing prevalence of offensive language online has become a critical problem. Notably in Korea, one of the countries with the highest Internet... | SanHee Park, KangMin Kim, Seonhee Cho, JunHyung Park, Hyuntae Park, Hyuna Kim, Seongwon Chung, SangKeun Lee |  |
| 11 |  |  [RepGraph: Visualising and Analysing Meaning Representation Graphs](https://doi.org/10.18653/v1/2021.emnlp-demo.10) |  | 0 | We present RepGraph, an open source visualisation and analysis tool for meaning representation graphs. Graph-based meaning representations provide rich semantic annotations, but visualising them clearly is more challenging than for fully lexicalized representations. Our application provides a... | Jaron Cohen, Roy Cohen, Edan Toledo, Jan Buys |  |
| 12 |  |  [Thermostat: A Large Collection of NLP Model Explanations and Analysis Tools](https://doi.org/10.18653/v1/2021.emnlp-demo.11) |  | 0 | In the language domain, as in other domains, neural explainability takes an ever more important role, with feature attribution methods on the forefront. Many such methods require considerable computational resources and expert knowledge about implementation details and parameter choices. To... | Nils Feldhus, Robert Schwarzenberg, Sebastian Möller |  |
| 13 |  |  [LMdiff: A Visual Diff Tool to Compare Language Models](https://doi.org/10.18653/v1/2021.emnlp-demo.12) |  | 0 | While different language models are ubiquitous in NLP, it is hard to contrast their outputs and identify which contexts one can handle better than the other. To address this question, we introduce LMdiff, a tool that visually compares probability distributions of two models that differ, e.g.,... | Hendrik Strobelt, Benjamin Hoover, Arvind Satyanarayan, Sebastian Gehrmann |  |
| 14 |  |  [Semantic Context Path Labeling for Semantic Exploration of User Reviews](https://doi.org/10.18653/v1/2021.emnlp-demo.13) |  | 0 | In this paper we present a prototype demonstrator showcasing a novel method to perform semantic exploration of user reviews. The system enables effective navigation in a rich contextual semantic schema with a large number of structured classes indicating relevant information. In order to identify... | Salah AïtMokhtar, Caroline Brun, Yves Hoppenot, Ágnes Sándor |  |
| 15 |  |  [Beyond Accuracy: A Consolidated Tool for Visual Question Answering Benchmarking](https://doi.org/10.18653/v1/2021.emnlp-demo.14) |  | 0 | On the way towards general Visual Question Answering (VQA) systems that are able to answer arbitrary questions, the need arises for evaluation beyond single-metric leaderboards for specific datasets. To this end, we propose a browser-based benchmarking tool for researchers and challenge organizers,... | Dirk Väth, Pascal Tilli, Ngoc Thang Vu |  |
| 16 |  |  [Athena 2.0: Contextualized Dialogue Management for an Alexa Prize SocialBot](https://doi.org/10.18653/v1/2021.emnlp-demo.15) |  | 0 | Athena 2.0 is an Alexa Prize SocialBot that has been a finalist in the last two Alexa Prize Grand Challenges. One reason for Athena’s success is its novel dialogue management strategy, which allows it to dynamically construct dialogues and responses from component modules, leading to novel... | Marilyn A. Walker, Vrindavan Harrison, Juraj Juraska, Lena Reed, Kevin Bowden, Wen Cui, Omkar Patil, Adwait Ratnaparkhi |  |
| 17 |  |  [SPRING Goes Online: End-to-End AMR Parsing and Generation](https://doi.org/10.18653/v1/2021.emnlp-demo.16) |  | 0 | In this paper we present SPRING Online Services, a Web interface and RESTful APIs for our state-of-the-art AMR parsing and generation system, SPRING (Symmetric PaRsIng aNd Generation). The Web interface has been developed to be easily used by the Natural Language Processing community, as well as by... | Rexhina Blloshmi, Michele Bevilacqua, Edoardo Fabiano, Valentina Caruso, Roberto Navigli |  |
| 18 |  |  [fairseq S\^2: A Scalable and Integrable Speech Synthesis Toolkit](https://doi.org/10.18653/v1/2021.emnlp-demo.17) |  | 0 | This paper presents fairseq Sˆ2, a fairseq extension for speech synthesis. We implement a number of autoregressive (AR) and non-AR text-to-speech models, and their multi-speaker variants. To enable training speech synthesis models with less curated data, a number of preprocessing tools are built... | Changhan Wang, WeiNing Hsu, Yossi Adi, Adam Polyak, Ann Lee, PengJen Chen, Jiatao Gu, Juan Pino |  |
| 19 |  |  [Press Freedom Monitor: Detection of Reported Press and Media Freedom Violations in Twitter and News Articles](https://doi.org/10.18653/v1/2021.emnlp-demo.18) |  | 0 | Freedom of the press and media is of vital importance for democratically organised states and open societies. We introduce the Press Freedom Monitor, a tool that aims to detect reported press and media freedom violations in news articles and tweets. It is used by press and media freedom... | Tariq Yousef, Antje Schlaf, Janos Borst, Andreas Niekler, Gerhard Heyer |  |
| 20 |  |  [UMR-Writer: A Web Application for Annotating Uniform Meaning Representations](https://doi.org/10.18653/v1/2021.emnlp-demo.19) |  | 0 | We present UMR-Writer, a web-based application for annotating Uniform Meaning Representations (UMR), a graph-based, cross-linguistically applicable semantic representation developed recently to support the development of interpretable natural language applications that require deep semantic... | Jin Zhao, Nianwen Xue, Jens E. L. Van Gysel, Jinho D. Choi |  |
| 21 |  |  [TranslateLocally: Blazing-fast translation running on the local CPU](https://doi.org/10.18653/v1/2021.emnlp-demo.20) |  | 0 | Every day, millions of people sacrifice their privacy and browsing habits in exchange for online machine translation. Companies and governments with confidentiality requirements often ban online translation or pay a premium to disable logging. To bring control back to the end user and demonstrate... | Nikolay Bogoychev, Jelmer van der Linde, Kenneth Heafield |  |
| 22 |  |  [Datasets: A Community Library for Natural Language Processing](https://doi.org/10.18653/v1/2021.emnlp-demo.21) |  | 0 | The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces,... | Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Sasko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillanMajor, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, François Lagunas, Alexander M. Rush, Thomas Wolf |  |
| 23 |  |  [Summary Explorer: Visualizing the State of the Art in Text Summarization](https://doi.org/10.18653/v1/2021.emnlp-demo.22) |  | 0 | This paper introduces Summary Explorer, a new tool to support the manual inspection of text summarization systems by compiling the outputs of 55 state-of-the-art single document summarization approaches on three benchmark datasets, and visually exploring them during a qualitative assessment. The... | Shahbaz Syed, Tariq Yousef, Khalid Al Khatib, Stefan Jänicke, Martin Potthast |  |
| 24 |  |  [MeetDot: Videoconferencing with Live Translation Captions](https://doi.org/10.18653/v1/2021.emnlp-demo.23) |  | 0 | We present MeetDot, a videoconferencing system with live translation captions overlaid on screen. The system aims to facilitate conversation between people who speak different languages, thereby reducing communication barriers between multilingual participants. Currently, our system supports speech... | Arkady Arkhangorodsky, Christopher Chu, Scot Fang, Yiqi Huang, Denglin Jiang, Ajay Nagesh, Boliang Zhang, Kevin Knight |  |
| 25 |  |  [Box Embeddings: An open-source library for representation learning using geometric structures](https://doi.org/10.18653/v1/2021.emnlp-demo.24) |  | 0 | A fundamental component to the success of modern representation learning is the ease of performing various vector operations. Recently, objects with more geometric structure (eg. distributions, complex or hyperbolic vectors, or regions such as cones, disks, or boxes) have been explored for their... | Tejas Chheda, Purujit Goyal, Trang Tran, Dhruvesh Patel, Michael Boratko, Shib Sankar Dasgupta, Andrew McCallum |  |
| 26 |  |  [LexiClean: An annotation tool for rapid multi-task lexical normalisation](https://doi.org/10.18653/v1/2021.emnlp-demo.25) |  | 0 | NLP systems are often challenged by difficulties arising from noisy, non-standard, and domain specific corpora. The task of lexical normalisation aims to standardise such corpora, but currently lacks suitable tools to acquire high-quality annotated data to support deep learning based approaches. In... | Tyler Bikaun, Tim French, Melinda Hodkiewicz, Michael Stewart, Wei Liu |  |
| 27 |  |  [T3-Vis: visual analytic for Training and fine-Tuning Transformers in NLP](https://doi.org/10.18653/v1/2021.emnlp-demo.26) |  | 0 | Transformers are the dominant architecture in NLP, but their training and fine-tuning is still very challenging. In this paper, we present the design and implementation of a visual analytic framework for assisting researchers in such process, by providing them with valuable insights about the... | Raymond Li, Wen Xiao, Lanjun Wang, Hyeju Jang, Giuseppe Carenini |  |
| 28 |  |  [DomiKnowS: A Library for Integration of Symbolic Domain Knowledge in Deep Learning](https://doi.org/10.18653/v1/2021.emnlp-demo.27) |  | 0 | We demonstrate a library for the integration of domain knowledge in deep learning architectures. Using this library, the structure of the data is expressed symbolically via graph declarations and the logical constraints over outputs or latent variables can be seamlessly added to the deep models.... | Hossein Rajaby Faghihi, Quan Guo, Andrzej Uszok, Aliakbar Nafar, Parisa Kordjamshidi |  |
| 29 |  |  [OpenFraming: Open-sourced Tool for Computational Framing Analysis of Multilingual Data](https://doi.org/10.18653/v1/2021.emnlp-demo.28) |  | 0 | When journalists cover a news story, they can cover the story from multiple angles or perspectives. These perspectives are called “frames,” and usage of one frame or another may influence public perception and opinion of the issue at hand. We develop a web-based system for analyzing frames in... | Vibhu Bhatia, Vidya Prasad Akavoor, Sejin Paik, Lei Guo, Mona Jalal, Alyssa Hasegawa Smith, David Assefa Tofu, Edward Edberg Halim, Yimeng Sun, Margrit Betke, Prakash Ishwar, Derry Tanti Wijaya |  |
| 30 |  |  [IrEne-viz: Visualizing Energy Consumption of Transformer Models](https://doi.org/10.18653/v1/2021.emnlp-demo.29) |  | 0 | IrEne is an energy prediction system that accurately predicts the interpretable inference energy consumption of a wide range of Transformer-based NLP models. We present the IrEne-viz tool, an online platform for visualizing and exploring energy consumption of various Transformer-based models... | Yash Kumar Lal, Reetu Singh, Harsh Trivedi, Qingqing Cao, Aruna Balasubramanian, Niranjan Balasubramanian |  |
| 31 |  |  [Open-Domain Question-Answering for COVID-19 and Other Emergent Domains](https://doi.org/10.18653/v1/2021.emnlp-demo.30) |  | 0 | Since late 2019, COVID-19 has quickly emerged as the newest biomedical domain, resulting in a surge of new information. As with other emergent domains, the discussion surrounding the topic has been rapidly changing, leading to the spread of misinformation. This has created the need for a public... | Sharon Levy, Kevin Mo, Wenhan Xiong, William Yang Wang |  |
| 32 |  |  [Project Debater APIs: Decomposing the AI Grand Challenge](https://doi.org/10.18653/v1/2021.emnlp-demo.31) |  | 0 | Project Debater was revealed in 2019 as the first AI system that can debate human experts on complex topics. Engaging in a live debate requires a diverse set of skills, and Project Debater has been developed accordingly as a collection of components, each designed to perform a specific subtask.... | Roy BarHaim, Yoav Kantor, Elad Venezian, Yoav Katz, Noam Slonim |  |
| 33 |  |  [CroAno : A Crowd Annotation Platform for Improving Label Consistency of Chinese NER Dataset](https://doi.org/10.18653/v1/2021.emnlp-demo.32) |  | 0 | In this paper, we introduce CroAno, a web-based crowd annotation platform for the Chinese named entity recognition (NER). Besides some basic features for crowd annotation like fast tagging and data management, CroAno provides a systematic solution for improving label consistency of Chinese NER... | Baoli Zhang, Zhucong Li, Zhen Gan, Yubo Chen, Jing Wan, Kang Liu, Jun Zhao, Shengping Liu, Yafei Shi |  |
| 34 |  |  [iFacetSum: Coreference-based Interactive Faceted Summarization for Multi-Document Exploration](https://doi.org/10.18653/v1/2021.emnlp-demo.33) |  | 0 | We introduce iFᴀᴄᴇᴛSᴜᴍ, a web application for exploring topical document collections. iFᴀᴄᴇᴛSᴜᴍ integrates interactive summarization together with faceted search, by providing a novel faceted navigation scheme that yields abstractive summaries for the user’s selections. This approach offers both a... | Eran Hirsch, Alon Eirew, Ori Shapira, Avi Caciularu, Arie Cattan, Ori Ernst, Ramakanth Pasunuru, Hadar Ronen, Mohit Bansal, Ido Dagan |  |
| 35 |  |  [AMuSE-WSD: An All-in-one Multilingual System for Easy Word Sense Disambiguation](https://doi.org/10.18653/v1/2021.emnlp-demo.34) |  | 0 | Over the past few years, Word Sense Disambiguation (WSD) has received renewed interest: recently proposed systems have shown the remarkable effectiveness of deep learning techniques in this task, especially when aided by modern pretrained language models. Unfortunately, such systems are still not... | Riccardo Orlando, Simone Conia, Fabrizio Brignone, Francesco Cecconi, Roberto Navigli |  |
| 36 |  |  [SeqAttack: On Adversarial Attacks for Named Entity Recognition](https://doi.org/10.18653/v1/2021.emnlp-demo.35) |  | 0 | Named Entity Recognition is a fundamental task in information extraction and is an essential element for various Natural Language Processing pipelines. Adversarial attacks have been shown to greatly affect the performance of text classification systems but knowledge about their effectiveness... | Walter Simoncini, Gerasimos Spanakis |  |
| 37 |  |  [InVeRo-XL: Making Cross-Lingual Semantic Role Labeling Accessible with Intelligible Verbs and Roles](https://doi.org/10.18653/v1/2021.emnlp-demo.36) |  | 0 | Notwithstanding the growing interest in cross-lingual techniques for Natural Language Processing, there has been a surprisingly small number of efforts aimed at the development of easy-to-use tools for cross-lingual Semantic Role Labeling. In this paper, we fill this gap and present InVeRo-XL, an... | Simone Conia, Riccardo Orlando, Fabrizio Brignone, Francesco Cecconi, Roberto Navigli |  |
| 38 |  |  [SummerTime: Text Summarization Toolkit for Non-experts](https://doi.org/10.18653/v1/2021.emnlp-demo.37) |  | 0 | Recent advances in summarization provide models that can generate summaries of higher quality. Such models now exist for a number of summarization tasks, including query-based summarization, dialogue summarization, and multi-document summarization. While such models and tasks are rapidly growing in... | Ansong Ni, Zhangir Azerbayev, Mutethia Mutuma, Troy Feng, Yusen Zhang, Tao Yu, Ahmed Hassan Awadallah, Dragomir R. Radev |  |
| 39 |  |  [Chandler: An Explainable Sarcastic Response Generator](https://doi.org/10.18653/v1/2021.emnlp-demo.38) |  | 0 | We introduce Chandler, a system that generates sarcastic responses to a given utterance. Previous sarcasm generators assume the intended meaning that sarcasm conceals is the opposite of the literal meaning. We argue that this traditional theory of sarcasm provides a grounding that is neither... | Silviu Oprea, Steven R. Wilson, Walid Magdy |  |
| 40 |  |  [TabPert : An Effective Platform for Tabular Perturbation](https://doi.org/10.18653/v1/2021.emnlp-demo.39) |  | 0 | To grasp the true reasoning ability, the Natural Language Inference model should be evaluated on counterfactual data. TabPert facilitates this by generation of such counterfactual data for assessing model tabular reasoning issues. TabPert allows the user to update a table, change the hypothesis,... | Nupur Jain, Vivek Gupta, Anshul Rai, Gaurav Kumar |  |
| 41 |  |  [DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature](https://doi.org/10.18653/v1/2021.emnlp-demo.40) |  | 0 | In this work, we present to the NLP community, and to the wider research community as a whole, an application for the diachronic analysis of research corpora. We open source an easy-to-use tool coined DRIFT, which allows researchers to track research trends and development over the years. The... | Abheesht Sharma, Gunjan Chhablani, Harshit Pandey, Rajaswa Patil |  |
| 42 |  |  [FAST: Fast Annotation tool for SmarT devices](https://doi.org/10.18653/v1/2021.emnlp-demo.41) |  | 0 | Working with a wide range of annotators with the same attributes is crucial, as in real-world applications. Although such application cases often use crowd-sourcing mechanisms to gather a variety of annotators, most real-world users use mobile devices. In this paper, we propose “FAST,” an... | Shunyo Kawamoto, Yu Sawai, Kohei Wakimoto, Peinan Zhang |  |
| 43 |  |  [deepQuest-py: Large and Distilled Models for Quality Estimation](https://doi.org/10.18653/v1/2021.emnlp-demo.42) |  | 0 | We introduce deepQuest-py, a framework for training and evaluation of large and light-weight models for Quality Estimation (QE). deepQuest-py provides access to (1) state-of-the-art models based on pre-trained Transformers for sentence-level and word-level QE; (2) light-weight and efficient... | Fernando AlvaManchego, Abiola Obamuyide, Amit Gajbhiye, Frédéric Blain, Marina Fomicheva, Lucia Specia |  |
| 44 |  |  [Frontmatter](https://aclanthology.org/2021.findings-emnlp.0) |  | 0 |  |  |  |
| 45 |  |  [K-PLUG: Knowledge-injected Pre-trained Language Model for Natural Language Understanding and Generation in E-Commerce](https://doi.org/10.18653/v1/2021.findings-emnlp.1) |  | 0 | Existing pre-trained language models (PLMs) have demonstrated the effectiveness of self-supervised learning for a broad range of natural language processing (NLP) tasks. However, most of them are not explicitly aware of domain-specific knowledge, which is essential for downstream tasks in many... | Song Xu, Haoran Li, Peng Yuan, Yujia Wang, Youzheng Wu, Xiaodong He, Ying Liu, Bowen Zhou |  |
| 46 |  |  [Extracting Topics with Simultaneous Word Co-occurrence and Semantic Correlation Graphs: Neural Topic Modeling for Short Texts](https://doi.org/10.18653/v1/2021.findings-emnlp.2) |  | 0 | Short text nowadays has become a more fashionable form of text data, e.g., Twitter posts, news titles, and product reviews. Extracting semantic topics from short texts plays a significant role in a wide spectrum of NLP applications, and neural topic modeling is now a major tool to achieve it.... | Yiming Wang, Ximing Li, Xiaotang Zhou, Jihong Ouyang |  |
| 47 |  |  [Self-supervised Contrastive Cross-Modality Representation Learning for Spoken Question Answering](https://doi.org/10.18653/v1/2021.findings-emnlp.3) |  | 0 | Spoken question answering (SQA) requires fine-grained understanding of both spoken documents and questions for the optimal answer prediction. In this paper, we propose novel training schemes for spoken question answering with a self-supervised training stage and a contrastive representation... | Chenyu You, Nuo Chen, Yuexian Zou |  |
| 48 |  |  [Language Clustering for Multilingual Named Entity Recognition](https://doi.org/10.18653/v1/2021.findings-emnlp.4) |  | 0 | Recent work in multilingual natural language processing has shown progress in various tasks such as natural language inference and joint multilingual translation. Despite success in learning across many languages, challenges arise where multilingual training regimes often boost performance on some... | Kyle Shaffer |  |
| 49 |  |  [Neural News Recommendation with Collaborative News Encoding and Structural User Encoding](https://doi.org/10.18653/v1/2021.findings-emnlp.5) |  | 0 | Automatic news recommendation has gained much attention from the academic community and industry. Recent studies reveal that the key to this task lies within the effective representation learning of both news and users. Existing works typically encode news title and content separately while... | Zhiming Mao, Xingshan Zeng, KamFai Wong |  |
| 50 |  |  [Self-Teaching Machines to Read and Comprehend with Large-Scale Multi-Subject Question-Answering Data](https://doi.org/10.18653/v1/2021.findings-emnlp.6) |  | 0 | Despite considerable progress, most machine reading comprehension (MRC) tasks still lack sufficient training data to fully exploit powerful deep neural network models with millions of parameters, and it is laborious, expensive, and time-consuming to create large-scale, high-quality MRC data through... | Dian Yu, Kai Sun, Dong Yu, Claire Cardie |  |
| 51 |  |  [A Web Scale Entity Extraction System](https://doi.org/10.18653/v1/2021.findings-emnlp.7) |  | 0 | Understanding the semantic meaning of content on the web through the lens of entities and concepts has many practical advantages. However, when building large-scale entity extraction systems, practitioners are facing unique challenges involving finding the best ways to leverage the scale and... | Xuanting Cai, Quanbin Ma, Jianyu Liu, Pan Li, Qi Zeng, Zhengkan Yang, Pushkar Tripathi |  |
| 52 |  |  [Joint Multimedia Event Extraction from Video and Article](https://doi.org/10.18653/v1/2021.findings-emnlp.8) |  | 0 | Visual and textual modalities contribute complementary information about events described in multimedia documents. Videos contain rich dynamics and detailed unfoldings of events, while text describes more high-level and abstract concepts. However, existing event extraction methods either do not... | Brian Chen, Xudong Lin, Christopher Thomas, Manling Li, Shoya Yoshida, Lovish Chum, Heng Ji, ShihFu Chang |  |
| 53 |  |  [Fine-grained Semantic Alignment Network for Weakly Supervised Temporal Language Grounding](https://doi.org/10.18653/v1/2021.findings-emnlp.9) |  | 0 | Temporal language grounding (TLG) aims to localize a video segment in an untrimmed video based on a natural language description. To alleviate the expensive cost of manual annotations for temporal boundary labels,we are dedicated to the weakly supervised setting, where only video-level descriptions... | Yuechen Wang, Wengang Zhou, Houqiang Li |  |
| 54 |  |  [Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation](https://doi.org/10.18653/v1/2021.findings-emnlp.10) |  | 0 | Despite significant progress has been achieved in text summarization, factual inconsistency in generated summaries still severely limits its practical applications. Among the key factors to ensure factual consistency, a reliable automatic evaluation metric is the first and the most crucial one.... | Yuexiang Xie, Fei Sun, Yang Deng, Yaliang Li, Bolin Ding |  |
| 55 |  |  [Cross-Modal Retrieval Augmentation for Multi-Modal Classification](https://doi.org/10.18653/v1/2021.findings-emnlp.11) |  | 0 | Recent advances in using retrieval components over external knowledge sources have shown impressive results for a variety of downstream tasks in natural language processing. Here, we explore the use of unstructured external knowledge sources of images and their corresponding captions for improving... | Shir Gur, Natalia Neverova, Christopher Stauffer, SerNam Lim, Douwe Kiela, Austin Reiter |  |
| 56 |  |  [HiTRANS: A Hierarchical Transformer Network for Nested Named Entity Recognition](https://doi.org/10.18653/v1/2021.findings-emnlp.12) |  | 0 | Nested Named Entity Recognition (NNER) has been extensively studied, aiming to identify all nested entities from potential spans (i.e., one or more continuous tokens). However, recent studies for NNER either focus on tedious tagging schemas or utilize complex structures, which fail to learn... | Zhiwei Yang, Jing Ma, Hechang Chen, Yunke Zhang, Yi Chang |  |
| 57 |  |  [Improving Embedding-based Large-scale Retrieval via Label Enhancement](https://doi.org/10.18653/v1/2021.findings-emnlp.13) |  | 0 | Current embedding-based large-scale retrieval models are trained with 0-1 hard label that indicates whether a query is relevant to a document, ignoring rich information of the relevance degree. This paper proposes to improve embedding-based retrieval from the perspective of better characterizing... | Peiyang Liu, Xi Wang, Sen Wang, Wei Ye, Xiangyu Xi, Shikun Zhang |  |
| 58 |  |  [Improving Privacy Guarantee and Efficiency of Latent Dirichlet Allocation Model Training Under Differential Privacy](https://doi.org/10.18653/v1/2021.findings-emnlp.14) |  | 0 | Latent Dirichlet allocation (LDA), a widely used topic model, is often employed as a fundamental tool for text analysis in various applications. However, the training process of the LDA model typically requires massive text corpus data. On one hand, such massive data may expose private information... | Tao Huang, Hong Chen |  |
| 59 |  |  [Generating Mammography Reports from Multi-view Mammograms with BERT](https://doi.org/10.18653/v1/2021.findings-emnlp.15) |  | 0 | Writing mammography reports can be error-prone and time-consuming for radiologists. In this paper we propose a method to generate mammography reports given four images, corresponding to the four views used in screening mammography. To the best of our knowledge our work represents the first attempt... | Alexander Yalunin, Elena Sokolova, Ilya Burenko, Alexander Ponomarchuk, Olga Puchkova, Dmitriy Umerenkov |  |
| 60 |  |  [Euphemistic Phrase Detection by Masked Language Model](https://doi.org/10.18653/v1/2021.findings-emnlp.16) |  | 0 | It is a well-known approach for fringe groups and organizations to use euphemisms—ordinary-sounding and innocent-looking words with a secret meaning—to conceal what they are discussing. For instance, drug dealers often use “pot” for marijuana and “avocado” for heroin. From a social media content... | Wanzheng Zhu, Suma Bhat |  |
| 61 |  |  [Decomposing Complex Questions Makes Multi-Hop QA Easier and More Interpretable](https://doi.org/10.18653/v1/2021.findings-emnlp.17) |  | 0 | Multi-hop QA requires the machine to answer complex questions through finding multiple clues and reasoning, and provide explanatory evidence to demonstrate the machine’s reasoning process. We propose Relation Extractor-Reader and Comparator (RERC), a three-stage framework based on complex question... | Ruiliu Fu, Han Wang, Xuejun Zhang, Jun Zhou, Yonghong Yan |  |
| 62 |  |  [Segmenting Natural Language Sentences via Lexical Unit Analysis](https://doi.org/10.18653/v1/2021.findings-emnlp.18) |  | 0 | The span-based model enjoys great popularity in recent works of sequence segmentation. However, each of these methods suffers from its own defects, such as invalid predictions. In this work, we introduce a unified span-based model, lexical unit analysis (LUA), that addresses all these matters.... | Yangming Li, Lemao Liu, Shuming Shi |  |
| 63 |  |  [Dense Hierarchical Retrieval for Open-domain Question Answering](https://doi.org/10.18653/v1/2021.findings-emnlp.19) |  | 0 | Dense neural text retrieval has achieved promising results on open-domain Question Answering (QA), where latent representations of questions and passages are exploited for maximum inner product search in the retrieval process. However, current dense retrievers require splitting documents into short... | Ye Liu, Kazuma Hashimoto, Yingbo Zhou, Semih Yavuz, Caiming Xiong, Philip S. Yu |  |
| 64 |  |  [Visually Grounded Concept Composition](https://doi.org/10.18653/v1/2021.findings-emnlp.20) |  | 0 | We investigate ways to compose complex concepts in texts from primitive ones while grounding them in images. We propose Concept and Relation Graph (CRG), which builds on top of constituency analysis and consists of recursively combined concepts with predicate functions. Meanwhile, we propose a... | Bowen Zhang, Hexiang Hu, Linlu Qiu, Peter Shaw, Fei Sha |  |
| 65 |  |  [Compositional Networks Enable Systematic Generalization for Grounded Language Understanding](https://doi.org/10.18653/v1/2021.findings-emnlp.21) |  | 0 | Humans are remarkably flexible when understanding new sentences that include combinations of concepts they have never encountered before. Recent work has shown that while deep networks can mimic some human language abilities when presented with novel sentences, systematic variation uncovers the... | YenLing Kuo, Boris Katz, Andrei Barbu |  |
| 66 |  |  [An Unsupervised Method for Building Sentence Simplification Corpora in Multiple Languages](https://doi.org/10.18653/v1/2021.findings-emnlp.22) |  | 0 | The availability of parallel sentence simplification (SS) is scarce for neural SS modelings. We propose an unsupervised method to build SS corpora from large-scale bilingual translation corpora, alleviating the need for SS supervised corpora. Our method is motivated by the following two findings:... | Xinyu Lu, Jipeng Qiang, Yun Li, Yunhao Yuan, Yi Zhu |  |
| 67 |  |  [WhiteningBERT: An Easy Unsupervised Sentence Embedding Approach](https://doi.org/10.18653/v1/2021.findings-emnlp.23) |  | 0 | Producing the embedding of a sentence in anunsupervised way is valuable to natural language matching and retrieval problems in practice. In this work, we conduct a thorough examination of pretrained model based unsupervised sentence embeddings. We study on fourpretrained models and conduct massive... | Junjie Huang, Duyu Tang, Wanjun Zhong, Shuai Lu, Linjun Shou, Ming Gong, Daxin Jiang, Nan Duan |  |
| 68 |  |  [TWEETSUMM - A Dialog Summarization Dataset for Customer Service](https://doi.org/10.18653/v1/2021.findings-emnlp.24) |  | 0 | In a typical customer service chat scenario, customers contact a support center to ask for help or raise complaints, and human agents try to solve the issues. In most cases, at the end of the conversation, agents are asked to write a short summary emphasizing the problem and the proposed solution,... | Guy Feigenblat, R. Chulaka Gunasekara, Benjamin Sznajder, Sachindra Joshi, David Konopnicki, Ranit Aharonov |  |
| 69 |  |  [Discourse-Based Sentence Splitting](https://doi.org/10.18653/v1/2021.findings-emnlp.25) |  | 0 | Sentence splitting involves the segmentation of a sentence into two or more shorter sentences. It is a key component of sentence simplification, has been shown to help human comprehension and is a useful preprocessing step for NLP tasks such as summarisation and relation extraction. While several... | Liam Cripwell, Joël Legrand, Claire Gardent |  |
| 70 |  |  [Multi-Task Dense Retrieval via Model Uncertainty Fusion for Open-Domain Question Answering](https://doi.org/10.18653/v1/2021.findings-emnlp.26) |  | 0 | Multi-task dense retrieval models can be used to retrieve documents from a common corpus (e.g., Wikipedia) for different open-domain question-answering (QA) tasks. However, Karpukhin et al. (2020) shows that jointly learning different QA tasks with one dense model is not always beneficial due to... | Minghan Li, Ming Li, Kun Xiong, Jimmy Lin |  |
| 71 |  |  [Mining the Cause of Political Decision-Making from Social Media: A Case Study of COVID-19 Policies across the US States](https://doi.org/10.18653/v1/2021.findings-emnlp.27) |  | 0 | Mining the causes of political decision-making is an active research area in the field of political science. In the past, most studies have focused on long-term policies that are collected over several decades of time, and have primarily relied on surveys as the main source of predictors. However,... | Zhijing Jin, Zeyu Peng, Tejas Vaidhya, Bernhard Schölkopf, Rada Mihalcea |  |
| 72 |  |  [Self-Attention Graph Residual Convolutional Networks for Event Detection with dependency relations](https://doi.org/10.18653/v1/2021.findings-emnlp.28) |  | 0 | Event detection (ED) task aims to classify events by identifying key event trigger words embedded in a piece of text. Previous research have proved the validity of fusing syntactic dependency relations into Graph Convolutional Networks(GCN). While existing GCN-based methods explore latent... | Anan Liu, Ning Xu, Haozhe Liu |  |
| 73 |  |  [Mixup Decoding for Diverse Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.29) |  | 0 | Diverse machine translation aims at generating various target language translations for a given source language sentence. To leverage the linear relationship in the sentence latent space introduced by the mixup training, we propose a novel method, MixDiversity, to generate different translations... | Jicheng Li, Pengzhi Gao, Xuanfu Wu, Yang Feng, Zhongjun He, Hua Wu, Haifeng Wang |  |
| 74 |  |  [An Alignment-Agnostic Model for Chinese Text Error Correction](https://doi.org/10.18653/v1/2021.findings-emnlp.30) |  | 0 | This paper investigates how to correct Chinese text errors with types of mistaken, missing and redundant characters, which are common for Chinese native speakers. Most existing models based on detect-correct framework can correct mistaken characters, but cannot handle missing or redundant... | Liying Zheng, Yue Deng, Weishun Song, Liang Xu, Jing Xiao |  |
| 75 |  |  [Reasoning Visual Dialog with Sparse Graph Learning and Knowledge Transfer](https://doi.org/10.18653/v1/2021.findings-emnlp.31) |  | 0 | Visual dialog is a task of answering a sequence of questions grounded in an image using the previous dialog history as context. In this paper, we study how to address two fundamental challenges for this task: (1) reasoning over underlying semantic structures among dialog rounds and (2) identifying... | GiCheon Kang, Junseok Park, Hwaran Lee, ByoungTak Zhang, JinHwa Kim |  |
| 76 |  |  [Exploring Sentence Community for Document-Level Event Extraction](https://doi.org/10.18653/v1/2021.findings-emnlp.32) |  | 0 | Document-level event extraction is critical to various natural language processing tasks for providing structured information. Existing approaches by sequential modeling neglect the complex logic structures for long texts. In this paper, we leverage the entity interactions and sentence interactions... | Yusheng Huang, Weijia Jia |  |
| 77 |  |  [A Model of Cross-Lingual Knowledge-Grounded Response Generation for Open-Domain Dialogue Systems](https://doi.org/10.18653/v1/2021.findings-emnlp.33) |  | 0 | Research on open-domain dialogue systems that allow free topics is challenging in the field of natural language processing (NLP). The performance of the dialogue system has been improved recently by the method utilizing dialogue-related knowledge; however, non-English dialogue systems suffer from... | San Kim, Jin Yea Jang, Minyoung Jung, Saim Shin |  |
| 78 |  |  [WHOSe Heritage: Classification of UNESCO World Heritage Statements of "Outstanding Universal Value" with Soft Labels](https://doi.org/10.18653/v1/2021.findings-emnlp.34) |  | 0 | The UNESCO World Heritage List (WHL) includes the exceptionally valuable cultural and natural heritage to be preserved for mankind. Evaluating and justifying the Outstanding Universal Value (OUV) is essential for each site inscribed in the WHL, and yet a complex task, even for experts, since the... | Nan Bai, Renqian Luo, Pirouz Nourian, Ana Pereira Roders |  |
| 79 |  |  [P-INT: A Path-based Interaction Model for Few-shot Knowledge Graph Completion](https://doi.org/10.18653/v1/2021.findings-emnlp.35) |  | 0 | Few-shot knowledge graph completion is to infer the unknown facts (i.e., query head-tail entity pairs) of a given relation with only a few observed reference entity pairs. Its general process is to first encode the implicit relation of an entity pair and then match the relation of a query entity... | Jingwen Xu, Jing Zhang, Xirui Ke, Yuxiao Dong, Hong Chen, Cuiping Li, Yongbin Liu |  |
| 80 |  |  [Cartography Active Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.36) |  | 0 | We propose Cartography Active Learning (CAL), a novel Active Learning (AL) algorithm that exploits the behavior of the model on individual instances during training as a proxy to find the most informative instances for labeling. CAL is inspired by data maps, which were recently proposed to derive... | Mike Zhang, Barbara Plank |  |
| 81 |  |  [Beyond Reptile: Meta-Learned Dot-Product Maximization between Gradients for Improved Single-Task Regularization](https://doi.org/10.18653/v1/2021.findings-emnlp.37) |  | 0 | Meta-learning algorithms such as MAML, Reptile, and FOMAML have led to improved performance of several neural models. The primary difference between standard gradient descent and these meta-learning approaches is that they contain as a small component the gradient for maximizing dot-product between... | Akhil Kedia, Sai Chetan Chinthakindi, Wonho Ryu |  |
| 82 |  |  [GooAQ: Open Question Answering with Diverse Answer Types](https://doi.org/10.18653/v1/2021.findings-emnlp.38) |  | 0 | While day-to-day questions come with a variety of answer types, the current question-answering (QA) literature has failed to adequately address the answer diversity of questions. To this end, we present GooAQ, a large-scale dataset with a variety of answer types. This dataset contains over 5... | Daniel Khashabi, Amos Ng, Tushar Khot, Ashish Sabharwal, Hannaneh Hajishirzi, Chris CallisonBurch |  |
| 83 |  |  [Attention Weights in Transformer NMT Fail Aligning Words Between Sequences but Largely Explain Model Predictions](https://doi.org/10.18653/v1/2021.findings-emnlp.39) |  | 0 | This work proposes an extensive analysis of the Transformer architecture in the Neural Machine Translation (NMT) setting. Focusing on the encoder-decoder attention mechanism, we prove that attention weights systematically make alignment errors by relying mainly on uninformative tokens from the... | Javier Ferrando, Marta R. Costajussà |  |
| 84 |  |  [BFClass: A Backdoor-free Text Classification Framework](https://doi.org/10.18653/v1/2021.findings-emnlp.40) |  | 0 | Backdoor attack introduces artificial vulnerabilities into the model by poisoning a subset of the training data via injecting triggers and modifying labels. Various trigger design strategies have been explored to attack text classifiers, however, defending such attacks remains an open problem. In... | Zichao Li, Dheeraj Mekala, Chengyu Dong, Jingbo Shang |  |
| 85 |  |  [Multilingual Chart-based Constituency Parse Extraction from Pre-trained Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.41) |  | 0 | As it has been unveiled that pre-trained language models (PLMs) are to some extent capable of recognizing syntactic concepts in natural language, much effort has been made to develop a method for extracting complete (binary) parses from PLMs without training separate parsers. We improve upon this... | Taeuk Kim, Bowen Li, Sanggoo Lee |  |
| 86 |  |  [Hyperbolic Geometry is Not Necessary: Lightweight Euclidean-Based Models for Low-Dimensional Knowledge Graph Embeddings](https://doi.org/10.18653/v1/2021.findings-emnlp.42) |  | 0 | Recent knowledge graph embedding (KGE) models based on hyperbolic geometry have shown great potential in a low-dimensional embedding space. However, the necessity of hyperbolic space in KGE is still questionable, because the calculation based on hyperbolic geometry is much more complicated than... | Kai Wang, Yu Liu, Dan Lin, Michael Sheng |  |
| 87 |  |  [CascadeBERT: Accelerating Inference of Pre-trained Language Models via Calibrated Complete Models Cascade](https://doi.org/10.18653/v1/2021.findings-emnlp.43) |  | 0 | Dynamic early exiting aims to accelerate the inference of pre-trained language models (PLMs) by emitting predictions in internal layers without passing through the entire model. In this paper, we empirically analyze the working mechanism of dynamic early exiting and find that it faces a performance... | Lei Li, Yankai Lin, Deli Chen, Shuhuai Ren, Peng Li, Jie Zhou, Xu Sun |  |
| 88 |  |  [Semi-supervised Relation Extraction via Incremental Meta Self-Training](https://doi.org/10.18653/v1/2021.findings-emnlp.44) |  | 0 | To alleviate human efforts from obtaining large-scale annotations, Semi-Supervised Relation Extraction methods aim to leverage unlabeled data in addition to learning from limited samples. Existing self-training methods suffer from the gradual drift problem, where noisy pseudo labels on unlabeled... | Xuming Hu, Chenwei Zhang, Fukun Ma, Chenyao Liu, Lijie Wen, Philip S. Yu |  |
| 89 |  |  [Keyphrase Generation with Fine-Grained Evaluation-Guided Reinforcement Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.45) |  | 0 | Aiming to generate a set of keyphrases, Keyphrase Generation (KG) is a classical task for capturing the central idea from a given document. Based on Seq2Seq models, the previous reinforcement learning framework on KG tasks utilizes the evaluation metrics to further improve the well-trained neural... | Yichao Luo, Yige Xu, Jiacheng Ye, Xipeng Qiu, Qi Zhang |  |
| 90 |  |  [Improving Knowledge Graph Embedding Using Affine Transformations of Entities Corresponding to Each Relation](https://doi.org/10.18653/v1/2021.findings-emnlp.46) |  | 0 | To find a suitable embedding for a knowledge graph remains a big challenge nowadays. By using previous knowledge graph embedding methods, every entity in a knowledge graph is usually represented as a k-dimensional vector. As we know, an affine transformation can be expressed in the form of a matrix... | Jinfa Yang, Yongjie Shi, Xin Tong, Robin Wang, Taiyan Chen, Xianghua Ying |  |
| 91 |  |  [Using Question Answering Rewards to Improve Abstractive Summarization](https://doi.org/10.18653/v1/2021.findings-emnlp.47) |  | 0 | Neural abstractive summarization models have drastically improved in the recent years. However, the summaries generated by these models generally suffer from issues such as: not capturing the critical facts in source documents, and containing facts that are inconsistent with the source documents.... | R. Chulaka Gunasekara, Guy Feigenblat, Benjamin Sznajder, Ranit Aharonov, Sachindra Joshi |  |
| 92 |  |  [Effect Generation Based on Causal Reasoning](https://doi.org/10.18653/v1/2021.findings-emnlp.48) |  | 0 | Causal reasoning aims to predict the future scenarios that may be caused by the observed actions. However, existing causal reasoning methods deal with causalities on the word level. In this paper, we propose a novel event-level causal reasoning method and demonstrate its use in the task of effect... | Feiteng Mu, Wenjie Li, Zhipeng Xie |  |
| 93 |  |  [Distilling Word Meaning in Context from Pre-trained Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.49) |  | 0 | In this study, we propose a self-supervised learning method that distils representations of word meaning in context from a pre-trained masked language model. Word representations are the basis for context-aware lexical semantics and unsupervised semantic textual similarity (STS) estimation. A... | Yuki Arase, Tomoyuki Kajiwara |  |
| 94 |  |  [Unseen Entity Handling in Complex Question Answering over Knowledge Base via Language Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.50) |  | 0 | Complex question answering over knowledge base remains as a challenging task because it involves reasoning over multiple pieces of information, including intermediate entities/relations and other constraints. Previous methods simplify the SPARQL query of a question into such forms as a list or a... | Xin Huang, JungJae Kim, Bowei Zou |  |
| 95 |  |  [Bidirectional Hierarchical Attention Networks based on Document-level Context for Emotion Cause Extraction](https://doi.org/10.18653/v1/2021.findings-emnlp.51) |  | 0 | Emotion cause extraction (ECE) aims to extract the causes behind the certain emotion in text. Some works related to the ECE task have been published and attracted lots of attention in recent years. However, these methods neglect two major issues: 1) pay few attentions to the effect of... | Guimin Hu, Guangming Lu, Yi Zhao |  |
| 96 |  |  [Distantly Supervised Relation Extraction in Federated Settings](https://doi.org/10.18653/v1/2021.findings-emnlp.52) |  | 0 | In relation extraction, distant supervision is widely used to automatically label a large-scale training dataset by aligning a knowledge base with unstructured text. Most existing studies in this field have assumed there is a great deal of centralized unstructured text. However, in practice, texts... | Dianbo Sui, Yubo Chen, Kang Liu, Jun Zhao |  |
| 97 |  |  [Casting the Same Sentiment Classification Problem](https://doi.org/10.18653/v1/2021.findings-emnlp.53) |  | 0 | We introduce and study a problem variant of sentiment analysis, namely the “same sentiment classification problem”, where, given a pair of texts, the task is to determine if they have the same sentiment, disregarding the actual sentiment polarity. Among other things, our goal is to enable a more... | Erik Körner, Ahmad Dawar Hakimi, Gerhard Heyer, Martin Potthast |  |
| 98 |  |  [Detecting Compositionally Out-of-Distribution Examples in Semantic Parsing](https://doi.org/10.18653/v1/2021.findings-emnlp.54) |  | 0 | While neural networks are ubiquitous in state-of-the-art semantic parsers, it has been shown that most standard models suffer from dramatic performance losses when faced with compositionally out-of-distribution (OOD) data. Recently several methods have been proposed to improve compositional... | Denis Lukovnikov, Sina Däubener, Asja Fischer |  |
| 99 |  |  [Saliency-based Multi-View Mixed Language Training for Zero-shot Cross-lingual Classification](https://doi.org/10.18653/v1/2021.findings-emnlp.55) |  | 0 | Recent multilingual pre-trained models, like XLM-RoBERTa (XLM-R), have been demonstrated effective in many cross-lingual tasks. However, there are still gaps between the contextualized representations of similar words in different languages. To solve this problem, we propose a novel framework named... | Siyu Lai, Hui Huang, Dong Jing, Yufeng Chen, Jinan Xu, Jian Liu |  |
| 100 |  |  [Fighting the COVID-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society](https://doi.org/10.18653/v1/2021.findings-emnlp.56) |  | 0 | With the emergence of the COVID-19 pandemic, the political and the medical aspects of disinformation merged as the problem got elevated to a whole new level to become the first global infodemic. Fighting this infodemic has been declared one of the most important focus areas of the World Health... | Firoj Alam, Shaden Shaar, Fahim Dalvi, Hassan Sajjad, Alex Nikolov, Hamdy Mubarak, Giovanni Da San Martino, Ahmed Abdelali, Nadir Durrani, Kareem Darwish, Abdulaziz AlHomaid, Wajdi Zaghouani, Tommaso Caselli, Gijs Danoe, Friso Stolk, Britt Bruntink, Preslav Nakov |  |
| 101 |  |  [FANATIC: FAst Noise-Aware TopIc Clustering](https://doi.org/10.18653/v1/2021.findings-emnlp.57) |  | 0 | Extracting salient topics from a collection of documents can be a challenging task when a) the amount of data is large, b) the number of topics is not known a priori, and/or c) “topic noise” is present. We define “topic noise” as the collection of documents that are irrelevant to any coherent topic... | Ari Silburt, Anja Subasic, Evan Thompson, Carmeline Dsilva, Tarec Fares |  |
| 102 |  |  [Stream-level Latency Evaluation for Simultaneous Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.58) |  | 0 | Simultaneous machine translation has recently gained traction thanks to significant quality improvements and the advent of streaming applications. Simultaneous translation systems need to find a trade-off between translation quality and response time, and with this purpose multiple latency measures... | Javier IranzoSánchez, Jorge Civera Saiz, Alfons Juan |  |
| 103 |  |  [TSDAE: Using Transformer-based Sequential Denoising Auto-Encoderfor Unsupervised Sentence Embedding Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.59) |  | 0 | Learning sentence embeddings often requires a large amount of labeled data. However, for most tasks and domains, labeled data is seldom available and creating it is expensive. In this work, we present a new state-of-the-art unsupervised method based on pre-trained Transformers and Sequential... | Kexin Wang, Nils Reimers, Iryna Gurevych |  |
| 104 |  |  [How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?](https://doi.org/10.18653/v1/2021.findings-emnlp.60) |  | 0 | Data-driven subword segmentation has become the default strategy for open-vocabulary machine translation and other NLP tasks, but may not be sufficiently generic for optimal learning of non-concatenative morphology. We design a test suite to evaluate segmentation strategies on different types of... | Chantal Amrhein, Rico Sennrich |  |
| 105 |  |  [Rethinking Why Intermediate-Task Fine-Tuning Works](https://doi.org/10.18653/v1/2021.findings-emnlp.61) |  | 0 | Supplementary Training on Intermediate Labeled-data Tasks (STILT) is a widely applied technique, which first fine-tunes the pretrained language models on an intermediate task before on the target task of interest. While STILT is able to further improve the performance of pretrained language models,... | TingYun Chang, ChiJen Lu |  |
| 106 |  |  [Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.62) |  | 0 | The ability to continuously expand knowledge over time and utilize it to rapidly generalize to new tasks is a key feature of human linguistic intelligence. Existing models that pursue rapid generalization to new tasks (e.g., few-shot learning methods), however, are mostly trained in a single shot... | Xisen Jin, Bill Yuchen Lin, Mohammad Rostami, Xiang Ren |  |
| 107 |  |  [Efficient Test Time Adapter Ensembling for Low-resource Language Varieties](https://doi.org/10.18653/v1/2021.findings-emnlp.63) |  | 0 | Adapters are light-weight modules that allow parameter-efficient fine-tuning of pretrained models. Specialized language and task adapters have recently been proposed to facilitate cross-lingual transfer of multilingual pretrained models (Pfeiffer et al., 2020b). However, this approach requires... | Xinyi Wang, Yulia Tsvetkov, Sebastian Ruder, Graham Neubig |  |
| 108 |  |  [An Analysis of Euclidean vs. Graph-Based Framing for Bilingual Lexicon Induction from Word Embedding Spaces](https://doi.org/10.18653/v1/2021.findings-emnlp.64) |  | 0 | Much recent work in bilingual lexicon induction (BLI) views word embeddings as vectors in Euclidean space. As such, BLI is typically solved by finding a linear transformation that maps embeddings to a common space. Alternatively, word embeddings may be understood as nodes in a weighted graph. This... | Kelly Marchisio, Youngser Park, Ali SaadEldin, Anton Alyakin, Kevin Duh, Carey E. Priebe, Philipp Koehn |  |
| 109 |  |  [How to Select One Among All ? An Empirical Study Towards the Robustness of Knowledge Distillation in Natural Language Understanding](https://doi.org/10.18653/v1/2021.findings-emnlp.65) |  | 0 | Knowledge Distillation (KD) is a model compression algorithm that helps transfer the knowledge in a large neural network into a smaller one. Even though KD has shown promise on a wide range of Natural Language Processing (NLP) applications, little is understood about how one KD algorithm compares... | Tianda Li, Ahmad Rashid, Aref Jafari, Pranav Sharma, Ali Ghodsi, Mehdi Rezagholizadeh |  |
| 110 |  |  [Recommend for a Reason: Unlocking the Power of Unsupervised Aspect-Sentiment Co-Extraction](https://doi.org/10.18653/v1/2021.findings-emnlp.66) |  | 0 | Compliments and concerns in reviews are valuable for understanding users’ shopping interests and their opinions with respect to specific aspects of certain items. Existing review-based recommenders favor large and complex language encoders that can only learn latent and uninterpretable text... | Zeyu Li, Wei Cheng, Reema Kshetramade, John Houser, Haifeng Chen, Wei Wang |  |
| 111 |  |  [Learning Hard Retrieval Decoder Attention for Transformers](https://doi.org/10.18653/v1/2021.findings-emnlp.67) |  | 0 | The Transformer translation model is based on the multi-head attention mechanism, which can be parallelized easily. The multi-head attention network performs the scaled dot-product attention function in parallel, empowering the model by jointly attending to information from different representation... | Hongfei Xu, Qiuhui Liu, Josef van Genabith, Deyi Xiong |  |
| 112 |  |  [Recall and Learn: A Memory-augmented Solver for Math Word Problems](https://doi.org/10.18653/v1/2021.findings-emnlp.68) |  | 0 | In this article, we tackle the math word problem, namely, automatically answering a mathematical problem according to its textual description. Although recent methods have demonstrated their promising results, most of these methods are based on template-based generation scheme which results in... | Shifeng Huang, Jiawei Wang, Jiao Xu, Da Cao, Ming Yang |  |
| 113 |  |  [An Uncertainty-Aware Encoder for Aspect Detection](https://doi.org/10.18653/v1/2021.findings-emnlp.69) |  | 0 | Aspect detection is a fundamental task in opinion mining. Previous works use seed words either as priors of topic models, as anchors to guide the learning of aspects, or as features of aspect classifiers. This paper presents a novel weakly-supervised method to exploit seed words for aspect... | ThiNhung Nguyen, KiemHieu Nguyen, YoungIn Song, TuanDung Cao |  |
| 114 |  |  [Improving Empathetic Response Generation by Recognizing Emotion Cause in Conversations](https://doi.org/10.18653/v1/2021.findings-emnlp.70) |  | 0 | Current approaches to empathetic response generation focus on learning a model to predict an emotion label and generate a response based on this label and have achieved promising results. However, the emotion cause, an essential factor for empathetic responding, is ignored. The emotion cause is a... | Jun Gao, Yuhan Liu, Haolin Deng, Wei Wang, Yu Cao, Jiachen Du, Ruifeng Xu |  |
| 115 |  |  [Probing Across Time: What Does RoBERTa Know and When?](https://doi.org/10.18653/v1/2021.findings-emnlp.71) |  | 0 | Models of language trained on very large corpora have been demonstrated useful for natural language processing. As fixed artifacts, they have become the object of intense study, with many researchers “probing” the extent to which they acquire and readily demonstrate linguistic abstractions, factual... | Zeyu Liu, Yizhong Wang, Jungo Kasai, Hannaneh Hajishirzi, Noah A. Smith |  |
| 116 |  |  [Knowledge-Guided Paraphrase Identification](https://doi.org/10.18653/v1/2021.findings-emnlp.72) |  | 0 | Paraphrase identification (PI), a fundamental task in natural language processing, is to identify whether two sentences express the same or similar meaning, which is a binary classification problem. Recently, BERT-like pre-trained language models have been a popular choice for the frameworks of... | Haoyu Wang, Fenglong Ma, Yaqing Wang, Jing Gao |  |
| 117 |  |  [R2-D2: A Modular Baseline for Open-Domain Question Answering](https://doi.org/10.18653/v1/2021.findings-emnlp.73) |  | 0 | This work presents a novel four-stage open-domain QA pipeline R2-D2 (Rank twice, reaD twice). The pipeline is composed of a retriever, passage reranker, extractive reader, generative reader and a mechanism that aggregates the final prediction from all system’s components. We demonstrate its... | Martin Fajcik, Martin Docekal, Karel Ondrej, Pavel Smrz |  |
| 118 |  |  [What Does Your Smile Mean? Jointly Detecting Multi-Modal Sarcasm and Sentiment Using Quantum Probability](https://doi.org/10.18653/v1/2021.findings-emnlp.74) |  | 0 | Sarcasm and sentiment embody intrinsic uncertainty of human cognition, making joint detection of multi-modal sarcasm and sentiment a challenging task. In view of the advantages of quantum probability (QP) in modeling such uncertainty, this paper explores the potential of QP as a mathematical... | Yaochen Liu, Yazhou Zhang, Qiuchi Li, Benyou Wang, Dawei Song |  |
| 119 |  |  [Discovering Representation Sprachbund For Multilingual Pre-Training](https://doi.org/10.18653/v1/2021.findings-emnlp.75) |  | 0 | Multilingual pre-trained models have demonstrated their effectiveness in many multilingual NLP tasks and enabled zero-shot or few-shot transfer from high-resource languages to low-resource ones. However, due to significant typological differences and contradictions between some languages, such... | Yimin Fan, Yaobo Liang, Alexandre Muzio, Hany Hassan, Houqiang Li, Ming Zhou, Nan Duan |  |
| 120 |  |  [Plan-then-Generate: Controlled Data-to-Text Generation via Planning](https://doi.org/10.18653/v1/2021.findings-emnlp.76) |  | 0 | Recent developments in neural networks have led to the advance in data-to-text generation. However, the lack of ability of neural models to control the structure of generated output can be limiting in certain real-world applications. In this study, we propose a novel Plan-then-Generate (PlanGen)... | Yixuan Su, David Vandyke, Sihui Wang, Yimai Fang, Nigel Collier |  |
| 121 |  |  [Few-Shot Table-to-Text Generation with Prototype Memory](https://doi.org/10.18653/v1/2021.findings-emnlp.77) |  | 0 | Neural table-to-text generation models have achieved remarkable progress on an array of tasks. However, due to the data-hungry nature of neural models, their performances strongly rely on large-scale training examples, limiting their applicability in real-world applications. To address this, we... | Yixuan Su, Zaiqiao Meng, Simon Baker, Nigel Collier |  |
| 122 |  |  [Leveraging Word-Formation Knowledge for Chinese Word Sense Disambiguation](https://doi.org/10.18653/v1/2021.findings-emnlp.78) |  | 0 | In parataxis languages like Chinese, word meanings are constructed using specific word-formations, which can help to disambiguate word senses. However, such knowledge is rarely explored in previous word sense disambiguation (WSD) methods. In this paper, we propose to leverage word-formation... | Hua Zheng, Lei Li, Damai Dai, Deli Chen, Tianyu Liu, Xu Sun, Yang Liu |  |
| 123 |  |  [Exploiting Curriculum Learning in Unsupervised Neural Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.79) |  | 0 | Back-translation (BT) has become one of the de facto components in unsupervised neural machine translation (UNMT), and it explicitly makes UNMT have translation ability. However, all the pseudo bi-texts generated by BT are treated equally as clean data during optimization without considering the... | Jinliang Lu, Jiajun Zhang |  |
| 124 |  |  [Robust Fragment-Based Framework for Cross-lingual Sentence Retrieval](https://doi.org/10.18653/v1/2021.findings-emnlp.80) |  | 0 | Cross-lingual Sentence Retrieval (CLSR) aims at retrieving parallel sentence pairs that are translations of each other from a multilingual set of comparable documents. The retrieved parallel sentence pairs can be used in other downstream NLP tasks such as machine translation and cross-lingual word... | Nattapol Trijakwanich, Peerat Limkonchotiwat, Raheem Sarwar, Wannaphong Phatthiyaphaibun, Ekapol Chuangsuwanich, Sarana Nutanong |  |
| 125 |  |  [Towards Improving Adversarial Training of NLP Models](https://doi.org/10.18653/v1/2021.findings-emnlp.81) |  | 0 | Adversarial training, a method for learning robust deep neural networks, constructs adversarial examples during training. However, recent methods for generating NLP adversarial examples involve combinatorial search and expensive sentence encoders for constraining the generated instances. As a... | Jin Yong Yoo, Yanjun Qi |  |
| 126 |  |  [To Protect and To Serve? Analyzing Entity-Centric Framing of Police Violence](https://doi.org/10.18653/v1/2021.findings-emnlp.82) |  | 0 | Framing has significant but subtle effects on public opinion and policy. We propose an NLP framework to measure entity-centric frames. We use it to understand media coverage on police violence in the United States in a new Police Violence Frames Corpus of 82k news articles spanning 7k police... | Caleb Ziems, Diyi Yang |  |
| 127 |  |  [Calibrate your listeners! Robust communication-based training for pragmatic speakers](https://doi.org/10.18653/v1/2021.findings-emnlp.83) |  | 0 | To be good conversational partners, natural language processing (NLP) systems should be trained to produce contextually useful utterances. Prior work has investigated training NLP systems with communication-based objectives, where a neural listener stands in as a communication partner. However,... | Rose E. Wang, Julia White, Jesse Mu, Noah D. Goodman |  |
| 128 |  |  [When Retriever-Reader Meets Scenario-Based Multiple-Choice Questions](https://doi.org/10.18653/v1/2021.findings-emnlp.84) |  | 0 | Scenario-based question answering (SQA) requires retrieving and reading paragraphs from a large corpus to answer a question which is contextualized by a long scenario description. Since a scenario contains both keyphrases for retrieval and much noise, retrieval for SQA is extremely difficult.... | Zixian Huang, Ao Wu, Yulin Shen, Gong Cheng, Yuzhong Qu |  |
| 129 |  |  [Structured abbreviation expansion in context](https://doi.org/10.18653/v1/2021.findings-emnlp.85) |  | 0 | Ad hoc abbreviations are commonly found in informal communication channels that favor shorter messages. We consider the task of reversing these abbreviations in context to recover normalized, expanded versions of abbreviated messages. The problem is related to, but distinct from, spelling... | Kyle Gorman, Christo Kirov, Brian Roark, Richard Sproat |  |
| 130 |  |  [Task-adaptive Pre-training and Self-training are Complementary for Natural Language Understanding](https://doi.org/10.18653/v1/2021.findings-emnlp.86) |  | 0 | Task-adaptive pre-training (TAPT) and Self-training (ST) have emerged as the major semi-supervised approaches to improve natural language understanding (NLU) tasks with massive amount of unlabeled data. However, it’s unclear whether they learn similar representations or they can be effectively... | Shiyang Li, Semih Yavuz, Wenhu Chen, Xifeng Yan |  |
| 131 |  |  [CNNBiF: CNN-based Bigram Features for Named Entity Recognition](https://doi.org/10.18653/v1/2021.findings-emnlp.87) |  | 0 | Transformer models fine-tuned with a sequence labeling objective have become the dominant choice for named entity recognition tasks. However, a self-attention mechanism with unconstrained length can fail to fully capture local dependencies, particularly when training data is limited. In this paper,... | Chul Sung, Vaibhava Goel, Etienne Marcheret, Steven J. Rennie, David Nahamoo |  |
| 132 |  |  [Compositional Generalization via Semantic Tagging](https://doi.org/10.18653/v1/2021.findings-emnlp.88) |  | 0 | Although neural sequence-to-sequence models have been successfully applied to semantic parsing, they fail at compositional generalization, i.e., they are unable to systematically generalize to unseen compositions of seen components. Motivated by traditional semantic parsing where compositionality... | Hao Zheng, Mirella Lapata |  |
| 133 |  |  [Towards Document-Level Paraphrase Generation with Sentence Rewriting and Reordering](https://doi.org/10.18653/v1/2021.findings-emnlp.89) |  | 0 | Paraphrase generation is an important task in natural language processing. Previous works focus on sentence-level paraphrase generation, while ignoring document-level paraphrase generation, which is a more challenging and valuable task. In this paper, we explore the task of document-level... | Zhe Lin, Yitao Cai, Xiaojun Wan |  |
| 134 |  |  [Exploring Decomposition for Table-based Fact Verification](https://doi.org/10.18653/v1/2021.findings-emnlp.90) |  | 0 | Fact verification based on structured data is challenging as it requires models to understand both natural language and symbolic operations performed over tables. Although pre-trained language models have demonstrated a strong capability in verifying simple statements, they struggle with complex... | Xiaoyu Yang, Xiaodan Zhu |  |
| 135 |  |  [Diversity and Consistency: Exploring Visual Question-Answer Pair Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.91) |  | 0 | Although showing promising values to downstream applications, generating question and answer together is under-explored. In this paper, we introduce a novel task that targets question-answer pair generation from visual images. It requires not only generating diverse question-answer pairs but also... | Sen Yang, Qingyu Zhou, Dawei Feng, Yang Liu, Chao Li, Yunbo Cao, Dongsheng Li |  |
| 136 |  |  [Entity-level Cross-modal Learning Improves Multi-modal Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.92) |  | 0 | Multi-modal machine translation (MMT) aims at improving translation performance by incorporating visual information. Most of the studies leverage the visual information through integrating the global image features as auxiliary input or decoding by attending to relevant local regions of the image.... | Xin Huang, Jiajun Zhang, Chengqing Zong |  |
| 137 |  |  [Learning to Ground Visual Objects for Visual Dialog](https://doi.org/10.18653/v1/2021.findings-emnlp.93) |  | 0 | Visual dialog is challenging since it needs to answer a series of coherent questions based on understanding the visual environment. How to ground related visual objects is one of the key problems. Previous studies utilize the question and history to attend to the image and achieve satisfactory... | Feilong Chen, Xiuyi Chen, Can Xu, Daxin Jiang |  |
| 138 |  |  [KERS: A Knowledge-Enhanced Framework for Recommendation Dialog Systems with Multiple Subgoals](https://doi.org/10.18653/v1/2021.findings-emnlp.94) |  | 0 | Recommendation dialogs require the system to build a social bond with users to gain trust and develop affinity in order to increase the chance of a successful recommendation. It is beneficial to divide up, such conversations with multiple subgoals (such as social chat, question answering,... | Jun Zhang, Yan Yang, Chencai Chen, Liang He, Zhou Yu |  |
| 139 |  |  [Less Is More: Domain Adaptation with Lottery Ticket for Reading Comprehension](https://doi.org/10.18653/v1/2021.findings-emnlp.95) |  | 0 | In this paper, we propose a simple few-shot domain adaptation paradigm for reading comprehension. We first identify the lottery subnetwork structure within the Transformer-based source domain model via gradual magnitude pruning. Then, we only fine-tune the lottery subnetwork, a small fraction of... | Haichao Zhu, Zekun Wang, Heng Zhang, Ming Liu, Sendong Zhao, Bing Qin |  |
| 140 |  |  [Effectiveness of Pre-training for Few-shot Intent Classification](https://doi.org/10.18653/v1/2021.findings-emnlp.96) |  | 0 | This paper investigates the effectiveness of pre-training for few-shot intent classification. While existing paradigms commonly further pre-train language models such as BERT on a vast amount of unlabeled corpus, we find it highly effective and efficient to simply fine-tune BERT with a small set of... | Haode Zhang, Yuwei Zhang, LiMing Zhan, Jiaxin Chen, Guangyuan Shi, XiaoMing Wu, Albert Y. S. Lam |  |
| 141 |  |  [Improving Abstractive Dialogue Summarization with Hierarchical Pretraining and Topic Segment](https://doi.org/10.18653/v1/2021.findings-emnlp.97) |  | 0 | With the increasing abundance of meeting transcripts, meeting summary has attracted more and more attention from researchers. The unsupervised pre-training method based on transformer structure combined with fine-tuning of downstream tasks has achieved great success in the field of text... | Mengnan Qi, Hao Liu, Yuzhuo Fu, Ting Liu |  |
| 142 |  |  [Learning to Answer Psychological Questionnaire for Personality Detection](https://doi.org/10.18653/v1/2021.findings-emnlp.98) |  | 0 | Existing text-based personality detection research mostly relies on data-driven approaches to implicitly capture personality cues in online posts, lacking the guidance of psychological knowledge. Psychological questionnaire, which contains a series of dedicated questions highly related to... | Feifan Yang, Tao Yang, Xiaojun Quan, Qinliang Su |  |
| 143 |  |  [Exploiting Reasoning Chains for Multi-hop Science Question Answering](https://doi.org/10.18653/v1/2021.findings-emnlp.99) |  | 0 | We propose a novel Chain Guided Retriever-reader (CGR) framework to model the reasoning chain for multi-hop Science Question Answering. Our framework is capable of performing explainable reasoning without the need of any corpus-specific annotations, such as the ground-truth reasoning chain, or... | Weiwen Xu, Yang Deng, Huihui Zhang, Deng Cai, Wai Lam |  |
| 144 |  |  [Winnowing Knowledge for Multi-choice Question Answering](https://doi.org/10.18653/v1/2021.findings-emnlp.100) |  | 0 | We tackle multi-choice question answering. Acquiring related commonsense knowledge to the question and options facilitates the recognition of the correct answer. However, the current reasoning models suffer from the noises in the retrieved knowledge. In this paper, we propose a novel encoding... | Yeqiu Li, Bowei Zou, Zhifeng Li, Ai Ti Aw, Yu Hong, Qiaoming Zhu |  |
| 145 |  |  [Neural Media Bias Detection Using Distant Supervision With BABE - Bias Annotations By Experts](https://doi.org/10.18653/v1/2021.findings-emnlp.101) |  | 0 | Media coverage has a substantial effect on the public perception of events. Nevertheless, media outlets are often biased. One way to bias news articles is by altering the word choice. The automatic identification of bias by word choice is challenging, primarily due to the lack of a gold standard... | Timo Spinde, Manuel Plank, JanDavid Krieger, Terry Ruas, Bela Gipp, Akiko Aizawa |  |
| 146 |  |  [Learning and Evaluating a Differentially Private Pre-trained Language Model](https://doi.org/10.18653/v1/2021.findings-emnlp.102) |  | 0 | Contextual language models have led to significantly better results, especially when pre-trained on the same data as the downstream task. While this additional pre-training usually improves performance, it can lead to information leakage and therefore risks the privacy of individuals mentioned in... | Shlomo Hoory, Amir Feder, Avichai Tendler, Sofia Erell, Alon PeledCohen, Itay Laish, Hootan Nakhost, Uri Stemmer, Ayelet Benjamini, Avinatan Hassidim, Yossi Matias |  |
| 147 |  |  [Simulated Chats for Building Dialog Systems: Learning to Generate Conversations from Instructions](https://doi.org/10.18653/v1/2021.findings-emnlp.103) |  | 0 | Popular dialog datasets such as MultiWOZ are created by providing crowd workers an instruction, expressed in natural language, that describes the task to be accomplished. Crowd workers play the role of a user and an agent to generate dialogs to accomplish tasks involving booking restaurant tables,... | Biswesh Mohapatra, Gaurav Pandey, Danish Contractor, Sachindra Joshi |  |
| 148 |  |  [Past, Present, and Future: Conversational Emotion Recognition through Structural Modeling of Psychological Knowledge](https://doi.org/10.18653/v1/2021.findings-emnlp.104) |  | 0 | Conversational Emotion Recognition (CER) is a task to predict the emotion of an utterance in the context of a conversation. Although modeling the conversational context and interactions between speakers has been studied broadly, it is important to consider the speaker’s psychological state, which... | Jiangnan Li, Zheng Lin, Peng Fu, Weiping Wang |  |
| 149 |  |  [An unsupervised framework for tracing textual sources of moral change](https://doi.org/10.18653/v1/2021.findings-emnlp.105) |  | 0 | Morality plays an important role in social well-being, but people’s moral perception is not stable and changes over time. Recent advances in natural language processing have shown that text is an effective medium for informing moral change, but no attempt has been made to quantify the origins of... | Aida Ramezani, Zining Zhu, Frank Rudzicz, Yang Xu |  |
| 150 |  |  [Topic-Aware Contrastive Learning for Abstractive Dialogue Summarization](https://doi.org/10.18653/v1/2021.findings-emnlp.106) |  | 0 | Unlike well-structured text, such as news reports and encyclopedia articles, dialogue content often comes from two or more interlocutors, exchanging information with each other. In such a scenario, the topic of a conversation can vary upon progression and the key information for a certain topic is... | Junpeng Liu, Yanyan Zou, Hainan Zhang, Hongshen Chen, Zhuoye Ding, Caixia Yuan, Xiaojie Wang |  |
| 151 |  |  [TWT: Table with Written Text for Controlled Data-to-Text Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.107) |  | 0 | Large pre-trained neural models have recently shown remarkable progress in text generation. In this paper, we propose to generate text conditioned on the structured data (table) and a prefix (the written text) by leveraging the pre-trained models. We present a new data-to-text dataset, Table with... | Tongliang Li, Lei Fang, JianGuang Lou, Zhoujun Li |  |
| 152 |  |  [ArabicTransformer: Efficient Large Arabic Language Model with Funnel Transformer and ELECTRA Objective](https://doi.org/10.18653/v1/2021.findings-emnlp.108) |  | 0 | Pre-training Transformer-based models such as BERT and ELECTRA on a collection of Arabic corpora, demonstrated by both AraBERT and AraELECTRA, shows an impressive result on downstream tasks. However, pre-training Transformer-based language models is computationally expensive, especially for... | Sultan Alrowili, Vijay Shanker |  |
| 153 |  |  [Which is Making the Contribution: Modulating Unimodal and Cross-modal Dynamics for Multimodal Sentiment Analysis](https://doi.org/10.18653/v1/2021.findings-emnlp.109) |  | 0 | Multimodal sentiment analysis (MSA) draws increasing attention with the availability of multimodal data. The boost in performance of MSA models is mainly hindered by two problems. On the one hand, recent MSA works mostly focus on learning cross-modal dynamics, but neglect to explore an optimal... | Ying Zeng, Sijie Mai, Haifeng Hu |  |
| 154 |  |  [CVAE-based Re-anchoring for Implicit Discourse Relation Classification](https://doi.org/10.18653/v1/2021.findings-emnlp.110) |  | 0 | Training implicit discourse relation classifiers suffers from data sparsity. Variational AutoEncoder (VAE) appears to be the proper solution. It is because ideally VAE is capable of generating inexhaustible varying samples, and this facilitates selective data augmentation. However, our experiments... | Zujun Dou, Yu Hong, Yu Sun, Guodong Zhou |  |
| 155 |  |  [Combining Curriculum Learning and Knowledge Distillation for Dialogue Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.111) |  | 0 | Curriculum learning, a machine training strategy that feeds training instances to the model from easy to hard, has been proven to facilitate the dialogue generation task. Meanwhile, knowledge distillation, a knowledge transformation methodology among teachers and students networks can yield... | Qingqing Zhu, Xiuying Chen, Pengfei Wu, Junfei Liu, Dongyan Zhao |  |
| 156 |  |  [Improving End-to-End Task-Oriented Dialog System with A Simple Auxiliary Task](https://doi.org/10.18653/v1/2021.findings-emnlp.112) |  | 0 | The paradigm of leveraging large pre-trained language models has made significant progress on benchmarks on task-oriented dialogue (TOD) systems. In this paper, we combine this paradigm with multi-task learning framework for end-to-end TOD modeling by adopting span prediction as an auxiliary task.... | Yohan Lee |  |
| 157 |  |  [EDTC: A Corpus for Discourse-Level Topic Chain Parsing](https://doi.org/10.18653/v1/2021.findings-emnlp.113) |  | 0 | Discourse analysis has long been known to be fundamental in natural language processing. In this research, we present our insight on discourse-level topic chain (DTC) parsing which aims at discovering new topics and investigating how these topics evolve over time within an article. To address the... | Longyin Zhang, Xin Tan, Fang Kong, Guodong Zhou |  |
| 158 |  |  [Multilingual Neural Machine Translation: Can Linguistic Hierarchies Help?](https://doi.org/10.18653/v1/2021.findings-emnlp.114) |  | 0 | Multilingual Neural Machine Translation (MNMT) trains a single NMT model that supports translation between multiple languages, rather than training separate models for different languages. Learning a single model can enhance the low-resource translation by leveraging data from multiple languages.... | Fahimeh Saleh, Wray L. Buntine, Gholamreza Haffari, Lan Du |  |
| 159 |  |  [Self Question-answering: Aspect-based Sentiment Analysis by Role Flipped Machine Reading Comprehension](https://doi.org/10.18653/v1/2021.findings-emnlp.115) |  | 0 | The pivot for the unified Aspect-based Sentiment Analysis (ABSA) is to couple aspect terms with their corresponding opinion terms, which might further derive easier sentiment predictions. In this paper, we investigate the unified ABSA task from the perspective of Machine Reading Comprehension (MRC)... | Guoxin Yu, Jiwei Li, Ling Luo, Yuxian Meng, Xiang Ao, Qing He |  |
| 160 |  |  [Generalization in Text-based Games via Hierarchical Reinforcement Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.116) |  | 0 | Deep reinforcement learning provides a promising approach for text-based games in studying natural language communication between humans and artificial agents. However, the generalization still remains a big challenge as the agents depend critically on the complexity and variety of training tasks.... | Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Chengqi Zhang |  |
| 161 |  |  [A Finer-grain Universal Dialogue Semantic Structures based Model For Abstractive Dialogue Summarization](https://doi.org/10.18653/v1/2021.findings-emnlp.117) |  | 0 | Although abstractive summarization models have achieved impressive results on document summarization tasks, their performance on dialogue modeling is much less satisfactory due to the crude and straight methods for dialogue encoding. To address this question, we propose a novel end-to-end... | Yuejie Lei, Fujia Zheng, Yuanmeng Yan, Keqing He, Weiran Xu |  |
| 162 |  |  [Constructing contrastive samples via summarization for text classification with limited annotations](https://doi.org/10.18653/v1/2021.findings-emnlp.118) |  | 0 | Contrastive Learning has emerged as a powerful representation learning method and facilitates various downstream tasks especially when supervised data is limited. How to construct efficient contrastive samples through data augmentation is key to its success. Unlike vision tasks, the data... | Yangkai Du, Tengfei Ma, Lingfei Wu, Fangli Xu, Xuhong Zhang, Bo Long, Shouling Ji |  |
| 163 |  |  [End-to-end Neural Information Status Classification](https://doi.org/10.18653/v1/2021.findings-emnlp.119) |  | 0 | Most previous studies on information status (IS) classification and bridging anaphora recognition assume that the gold mention or syntactic tree information is given (Hou et al., 2013; Roesiger et al., 2018; Hou, 2020; Yu and Poesio, 2020). In this paper, we propose an end-to-end neural approach... | Yufang Hou |  |
| 164 |  |  [EventKE: Event-Enhanced Knowledge Graph Embedding](https://doi.org/10.18653/v1/2021.findings-emnlp.120) |  | 0 | Relations in most of the traditional knowledge graphs (KGs) only reflect static and factual connections, but fail to represent the dynamic activities and state changes about entities. In this paper, we emphasize the importance of incorporating events in KG representation learning, and propose an... | Zixuan Zhang, Hongwei Wang, Han Zhao, Hanghang Tong, Heng Ji |  |
| 165 |  |  [Modeling Concentrated Cross-Attention for Neural Machine Translation with Gaussian Mixture Model](https://doi.org/10.18653/v1/2021.findings-emnlp.121) |  | 0 | Cross-attention is an important component of neural machine translation (NMT), which is always realized by dot-product attention in previous methods. However, dot-product attention only considers the pair-wise correlation between words, resulting in dispersion when dealing with long sentences and... | Shaolei Zhang, Yang Feng |  |
| 166 |  |  [Inconsistency Matters: A Knowledge-guided Dual-inconsistency Network for Multi-modal Rumor Detection](https://doi.org/10.18653/v1/2021.findings-emnlp.122) |  | 0 | Rumor spreaders are increasingly utilizing multimedia content to attract the attention and trust of news consumers. Though a set of rumor detection models have exploited the multi-modal data, they seldom consider the inconsistent relationships among images and texts. Moreover, they also fail to... | Mengzhu Sun, Xi Zhang, Jianqiang Ma, Yazheng Liu |  |
| 167 |  |  [EfficientBERT: Progressively Searching Multilayer Perceptron via Warm-up Knowledge Distillation](https://doi.org/10.18653/v1/2021.findings-emnlp.123) |  | 0 | Pre-trained language models have shown remarkable results on various NLP tasks. Nevertheless, due to their bulky size and slow inference speed, it is hard to deploy them on edge devices. In this paper, we have a critical insight that improving the feed-forward network (FFN) in BERT has a higher... | Chenhe Dong, Guangrun Wang, Hang Xu, Jiefeng Peng, Xiaozhe Ren, Xiaodan Liang |  |
| 168 |  |  [Uni-FedRec: A Unified Privacy-Preserving News Recommendation Framework for Model Training and Online Serving](https://doi.org/10.18653/v1/2021.findings-emnlp.124) |  | 0 | News recommendation techniques can help users on news platforms obtain their preferred news information. Most existing news recommendation methods rely on centrally stored user behavior data to train models and serve users. However, user data is usually highly privacy-sensitive, and centrally... | Tao Qi, Fangzhao Wu, Chuhan Wu, Yongfeng Huang, Xing Xie |  |
| 169 |  |  [Mapping Language to Programs using Multiple Reward Components with Inverse Reinforcement Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.125) |  | 0 | Mapping natural language instructions to programs that computers can process is a fundamental challenge. Existing approaches focus on likelihood-based training or using reinforcement learning to fine-tune models based on a single reward. In this paper, we pose program generation from language as... | Sayan Ghosh, Shashank Srivastava |  |
| 170 |  |  [Topic-Guided Abstractive Multi-Document Summarization](https://doi.org/10.18653/v1/2021.findings-emnlp.126) |  | 0 | A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and... | Peng Cui, Le Hu |  |
| 171 |  |  [An Edge-Enhanced Hierarchical Graph-to-Tree Network for Math Word Problem Solving](https://doi.org/10.18653/v1/2021.findings-emnlp.127) |  | 0 | Math word problem solving has attracted considerable research interest in recent years. Previous works have shown the effectiveness of utilizing graph neural networks to capture the relationships in the problem. However, these works did not carefully take the edge label information and the... | Qinzhuo Wu, Qi Zhang, Zhongyu Wei |  |
| 172 |  |  [SciXGen: A Scientific Paper Dataset for Context-Aware Text Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.128) |  | 0 | Generating texts in scientific papers requires not only capturing the content contained within the given input but also frequently acquiring the external information called context. We push forward the scientific text generation by proposing a new task, namely context-aware text generation in the... | Hong Chen, Hiroya Takamura, Hideki Nakayama |  |
| 173 |  |  [Don't Miss the Potential Customers! Retrieving Similar Ads to Improve User Targeting](https://doi.org/10.18653/v1/2021.findings-emnlp.129) |  | 0 | User targeting is an essential task in the modern advertising industry: given a package of ads for a particular category of products (e.g., green tea), identify the online users to whom the ad package should be targeted. A (ad package specific) user targeting model is typically trained using... | Yi Feng, Ting Wang, Chuanyi Li, Vincent Ng, Jidong Ge, Bin Luo, Yucheng Hu, Xiaopeng Zhang |  |
| 174 |  |  [Cross-lingual Transfer for Text Classification with Dictionary-based Heterogeneous Graph](https://doi.org/10.18653/v1/2021.findings-emnlp.130) |  | 0 | In cross-lingual text classification, it is required that task-specific training data in high-resource source languages are available, where the task is identical to that of a low-resource target language. However, collecting such training data can be infeasible because of the labeling cost, task... | Nuttapong Chairatanakul, Noppayut Sriwatanasakdi, Nontawat Charoenphakdee, Xin Liu, Tsuyoshi Murata |  |
| 175 |  |  [Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.131) |  | 0 | Distantly supervised named entity recognition (DS-NER) efficiently reduces labor costs but meanwhile intrinsically suffers from the label noise due to the strong assumption of distant supervision. Typically, the wrongly labeled instances comprise numbers of incomplete and inaccurate annotations,... | Xinghua Zhang, Bowen Yu, Tingwen Liu, Zhenyu Zhang, Jiawei Sheng, Mengge Xue, Hongbo Xu |  |
| 176 |  |  [Entity-Based Semantic Adequacy for Data-to-Text Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.132) |  | 0 | While powerful pre-trained language models have improved the fluency of text generation models, semantic adequacy -the ability to generate text that is semantically faithful to the input- remains an unsolved issue. In this paper, we introduce a novel automatic evaluation metric, Entity-Based... | Juliette Faille, Albert Gatt, Claire Gardent |  |
| 177 |  |  [MiRANews: Dataset and Benchmarks for Multi-Resource-Assisted News Summarization](https://doi.org/10.18653/v1/2021.findings-emnlp.133) |  | 0 | One of the most challenging aspects of current single-document news summarization is that the summary often contains ‘extrinsic hallucinations’, i.e., facts that are not present in the source document, which are often derived via world knowledge. This causes summarisation systems to act more like... | Xinnuo Xu, Ondrej Dusek, Shashi Narayan, Verena Rieser, Ioannis Konstas |  |
| 178 |  |  [A Conditional Generative Matching Model for Multi-lingual Reply Suggestion](https://doi.org/10.18653/v1/2021.findings-emnlp.134) |  | 0 | We study the problem of multilingual automated reply suggestions (RS) model serving many languages simultaneously. Multilingual models are often challenged by model capacity and severe data distribution skew across languages. While prior works largely focus on monolingual models, we propose... | Budhaditya Deb, Guoqing Zheng, Milad Shokouhi, Ahmed Hassan Awadallah |  |
| 179 |  |  [Rethinking Sentiment Style Transfer](https://doi.org/10.18653/v1/2021.findings-emnlp.135) |  | 0 | Though remarkable efforts have been made in non-parallel text style transfer, the evaluation system is unsatisfactory. It always evaluates over samples from only one checkpoint of the model and compares three metrics, i.e., transfer accuracy, BLEU score, and PPL score. In this paper, we argue the... | Ping Yu, Yang Zhao, Chunyuan Li, Changyou Chen |  |
| 180 |  |  [HypoGen: Hyperbole Generation with Commonsense and Counterfactual Knowledge](https://doi.org/10.18653/v1/2021.findings-emnlp.136) |  | 0 | A hyperbole is an intentional and creative exaggeration not to be taken literally. Despite its ubiquity in daily life, the computational explorations of hyperboles are scarce. In this paper, we tackle the under-explored and challenging task: sentence-level hyperbole generation. We start with a... | Yufei Tian, Arvind Krishna Sridhar, Nanyun Peng |  |
| 181 |  |  [Profiling News Discourse Structure Using Explicit Subtopic Structures Guided Critics](https://doi.org/10.18653/v1/2021.findings-emnlp.137) |  | 0 | We present an actor-critic framework to induce subtopical structures in a news article for news discourse profiling. The model uses multiple critics that act according to known subtopic structures while the actor aims to outperform them. The content structures constitute sentences that represent... | Prafulla Kumar Choubey, Ruihong Huang |  |
| 182 |  |  [ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection](https://doi.org/10.18653/v1/2021.findings-emnlp.138) |  | 0 | The ability to detect Out-of-Domain (OOD) inputs has been a critical requirement in many real-world NLP applications. For example, intent classification in dialogue systems. The reason is that the inclusion of unsupported OOD inputs may lead to catastrophic failure of systems. However, it remains... | Iftitahu Ni'mah, Meng Fang, Vlado Menkovski, Mykola Pechenizkiy |  |
| 183 |  |  [Learning from Language Description: Low-shot Named Entity Recognition via Decomposed Framework](https://doi.org/10.18653/v1/2021.findings-emnlp.139) |  | 0 | In this work, we study the problem of named entity recognition (NER) in a low resource scenario, focusing on few-shot and zero-shot settings. Built upon large-scale pre-trained language models, we propose a novel NER framework, namely SpanNER, which learns from natural language supervision and... | Yaqing Wang, Haoda Chu, Chao Zhang, Jing Gao |  |
| 184 |  |  [BERT might be Overkill: A Tiny but Effective Biomedical Entity Linker based on Residual Convolutional Neural Networks](https://doi.org/10.18653/v1/2021.findings-emnlp.140) |  | 0 | Biomedical entity linking is the task of linking entity mentions in a biomedical document to referent entities in a knowledge base. Recently, many BERT-based models have been introduced for the task. While these models achieve competitive results on many datasets, they are computationally expensive... | Tuan Manh Lai, Heng Ji, ChengXiang Zhai |  |
| 185 |  |  [Char2Subword: Extending the Subword Embedding Space Using Robust Character Compositionality](https://doi.org/10.18653/v1/2021.findings-emnlp.141) |  | 0 | Byte-pair encoding (BPE) is a ubiquitous algorithm in the subword tokenization process of language models as it provides multiple benefits. However, this process is solely based on pre-training data statistics, making it hard for the tokenizer to handle infrequent spellings. On the other hand,... | Gustavo Aguilar, Bryan McCann, Tong Niu, Nazneen Rajani, Nitish Shirish Keskar, Thamar Solorio |  |
| 186 |  |  [Exploring Multitask Learning for Low-Resource Abstractive Summarization](https://doi.org/10.18653/v1/2021.findings-emnlp.142) |  | 0 | This paper explores the effect of using multitask learning for abstractive summarization in the context of small training corpora. In particular, we incorporate four different tasks (extractive summarization, language modeling, concept detection, and paraphrase detection) both individually and in... | Ahmed Magooda, Diane J. Litman, Mohamed Elaraby |  |
| 187 |  |  [Conical Classification For Efficient One-Class Topic Determination](https://doi.org/10.18653/v1/2021.findings-emnlp.143) |  | 0 | As the Internet grows in size, so does the amount of text based information that exists. For many application spaces it is paramount to isolate and identify texts that relate to a particular topic. While one-class classification would be ideal for such analysis, there is a relative lack of research... | Sameer Khanna |  |
| 188 |  |  [Improving Dialogue State Tracking with Turn-based Loss Function and Sequential Data Augmentation](https://doi.org/10.18653/v1/2021.findings-emnlp.144) |  | 0 | While state-of-the-art Dialogue State Tracking (DST) models show promising results, all of them rely on a traditional cross-entropy loss function during the training process, which may not be optimal for improving the joint goal accuracy. Although several approaches recently propose augmenting the... | Jarana Manotumruksa, Jeff Dalton, Edgar Meij, Emine Yilmaz |  |
| 189 |  |  [TIAGE: A Benchmark for Topic-Shift Aware Dialog Modeling](https://doi.org/10.18653/v1/2021.findings-emnlp.145) |  | 0 | Human conversations naturally evolve around different topics and fluently move between them. In research on dialog systems, the ability to actively and smoothly transition to new topics is often ignored. In this paper we introduce TIAGE, a new topic-shift aware dialog benchmark constructed... | Huiyuan Xie, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu, Ann A. Copestake |  |
| 190 |  |  [Optimal Neural Program Synthesis from Multimodal Specifications](https://doi.org/10.18653/v1/2021.findings-emnlp.146) |  | 0 | Multimodal program synthesis, which leverages different types of user input to synthesize a desired program, is an attractive way to scale program synthesis to challenging settings; however, it requires integrating noisy signals from the user, like natural language, with hard constraints on the... | Xi Ye, Qiaochu Chen, Isil Dillig, Greg Durrett |  |
| 191 |  |  [Sent2Span: Span Detection for PICO Extraction in the Biomedical Text without Span Annotations](https://doi.org/10.18653/v1/2021.findings-emnlp.147) |  | 0 | The rapid growth in published clinical trials makes it difficult to maintain up-to-date systematic reviews, which require finding all relevant trials. This leads to policy and practice decisions based on out-of-date, incomplete, and biased subsets of available clinical evidence. Extracting and then... | Shifeng Liu, Yifang Sun, Bing Li, Wei Wang, Florence T. Bourgeois, Adam G. Dunn |  |
| 192 |  |  [When in Doubt: Improving Classification Performance with Alternating Normalization](https://doi.org/10.18653/v1/2021.findings-emnlp.148) |  | 0 | We introduce Classification with Alternating Normalization (CAN), a non-parametric post-processing step for classification. CAN improves classification accuracy for challenging examples by re-adjusting their predicted class probability distribution using the predicted class distributions of... | Menglin Jia, Austin Reiter, SerNam Lim, Yoav Artzi, Claire Cardie |  |
| 193 |  |  [APGN: Adversarial and Parameter Generation Networks for Multi-Source Cross-Domain Dependency Parsing](https://doi.org/10.18653/v1/2021.findings-emnlp.149) |  | 0 | Thanks to the strong representation learning capability of deep learning, especially pre-training techniques with language model loss, dependency parsing has achieved great performance boost in the in-domain scenario with abundant labeled training data for target domains. However, the parsing... | Ying Li, Meishan Zhang, Zhenghua Li, Min Zhang, Zhefeng Wang, Baoxing Huai, Nicholas Jing Yuan |  |
| 194 |  |  ["Let Your Characters Tell Their Story": A Dataset for Character-Centric Narrative Understanding](https://doi.org/10.18653/v1/2021.findings-emnlp.150) |  | 0 | When reading a literary piece, readers often make inferences about various characters’ roles, personalities, relationships, intents, actions, etc. While humans can readily draw upon their past experiences to build such a character-centric view of the narrative, understanding characters in... | Faeze Brahman, Meng Huang, Oyvind Tafjord, Chao Zhao, Mrinmaya Sachan, Snigdha Chaturvedi |  |
| 195 |  |  [Towards Developing a Multilingual and Code-Mixed Visual Question Answering System by Knowledge Distillation](https://doi.org/10.18653/v1/2021.findings-emnlp.151) |  | 0 | Pre-trained language-vision models have shown remarkable performance on the visual question answering (VQA) task. However, most pre-trained models are trained by only considering monolingual learning, especially the resource-rich language like English. Training such models for multilingual setups... | Humair Raj Khan, Deepak Gupta, Asif Ekbal |  |
| 196 |  |  [An Iterative Multi-Knowledge Transfer Network for Aspect-Based Sentiment Analysis](https://doi.org/10.18653/v1/2021.findings-emnlp.152) |  | 0 | Aspect-based sentiment analysis (ABSA) mainly involves three subtasks: aspect term extraction, opinion term extraction, and aspect-level sentiment classification, which are typically handled in a separate or joint manner. However, previous approaches do not well exploit the interactive relations... | Yunlong Liang, Fandong Meng, Jinchao Zhang, Yufeng Chen, Jinan Xu, Jie Zhou |  |
| 197 |  |  [Semantic Alignment with Calibrated Similarity for Multilingual Sentence Embedding](https://doi.org/10.18653/v1/2021.findings-emnlp.153) |  | 0 | Measuring the similarity score between a pair of sentences in different languages is the essential requisite for multilingual sentence embedding methods. Predicting the similarity score consists of two sub-tasks, which are monolingual similarity evaluation and multilingual sentence retrieval.... | Jiyeon Ham, EunSol Kim |  |
| 198 |  |  [fBERT: A Neural Transformer for Identifying Offensive Content](https://doi.org/10.18653/v1/2021.findings-emnlp.154) |  | 0 | Transformer-based models such as BERT, XLNET, and XLM-R have achieved state-of-the-art performance across various NLP tasks including the identification of offensive language and hate speech, an important problem in social media. In this paper, we present fBERT, a BERT model retrained on SOLID, the... | Diptanu Sarkar, Marcos Zampieri, Tharindu Ranasinghe, Alexander G. Ororbia II |  |
| 199 |  |  [WIKIBIAS: Detecting Multi-Span Subjective Biases in Language](https://doi.org/10.18653/v1/2021.findings-emnlp.155) |  | 0 | Biases continue to be prevalent in modern text and media, especially subjective bias – a special type of bias that introduces improper attitudes or presents a statement with the presupposition of truth. To tackle the problem of detecting and further mitigating subjective bias, we introduce a... | Yang Zhong, Jingfeng Yang, Wei Xu, Diyi Yang |  |
| 200 |  |  [UnClE: Explicitly Leveraging Semantic Similarity to Reduce the Parameters of Word Embeddings](https://doi.org/10.18653/v1/2021.findings-emnlp.156) |  | 0 | Natural language processing (NLP) models often require a massive number of parameters for word embeddings, which limits their application on mobile devices. Researchers have employed many approaches, e.g. adaptive inputs, to reduce the parameters of word embeddings. However, existing methods rarely... | Zhi Li, Yuchen Zhai, Chengyu Wang, Minghui Qiu, Kailiang Li, Yin Zhang |  |
| 201 |  |  [Grounded Graph Decoding improves Compositional Generalization in Question Answering](https://doi.org/10.18653/v1/2021.findings-emnlp.157) |  | 0 | Question answering models struggle to generalize to novel compositions of training patterns. Current end-to-end models learn a flat input embedding which can lose input syntax context. Prior approaches improve generalization by learning permutation invariant models, but these methods do not scale... | Yu Gai, Paras Jain, Wendi Zhang, Joseph Gonzalez, Dawn Song, Ion Stoica |  |
| 202 |  |  [Enhancing Visual Dialog Questioner with Entity-based Strategy Learning and Augmented Guesser](https://doi.org/10.18653/v1/2021.findings-emnlp.158) |  | 0 | Considering the importance of building a good Visual Dialog (VD) Questioner, many researchers study the topic under a Q-Bot-A-Bot image-guessing game setting, where the Questioner needs to raise a series of questions to collect information of an undisclosed image. Despite progress has been made in... | Duo Zheng, Zipeng Xu, Fandong Meng, Xiaojie Wang, Jiaan Wang, Jie Zhou |  |
| 203 |  |  [A Pretraining Numerical Reasoning Model for Ordinal Constrained Question Answering on Knowledge Base](https://doi.org/10.18653/v1/2021.findings-emnlp.159) |  | 0 | Knowledge Base Question Answering (KBQA) is to answer natural language questions posed over knowledge bases (KBs). This paper targets at empowering the IR-based KBQA models with the ability of numerical reasoning for answering ordinal constrained questions. A major challenge is the lack of explicit... | Yu Feng, Jing Zhang, Gaole He, Wayne Xin Zhao, Lemao Liu, Quan Liu, Cuiping Li, Hong Chen |  |
| 204 |  |  [RoR: Read-over-Read for Long Document Machine Reading Comprehension](https://doi.org/10.18653/v1/2021.findings-emnlp.160) |  | 0 | Transformer-based pre-trained models, such as BERT, have achieved remarkable results on machine reading comprehension. However, due to the constraint of encoding length (e.g., 512 WordPiece tokens), a long document is usually split into multiple chunks that are independently read. It results in the... | Jing Zhao, Junwei Bao, Yifan Wang, Yongwei Zhou, Youzheng Wu, Xiaodong He, Bowen Zhou |  |
| 205 |  |  [Span Pointer Networks for Non-Autoregressive Task-Oriented Semantic Parsing](https://doi.org/10.18653/v1/2021.findings-emnlp.161) |  | 0 | An effective recipe for building seq2seq, non-autoregressive, task-oriented parsers to map utterances to semantic frames proceeds in three steps: encoding an utterance x, predicting a frame’s length \|y\|, and decoding a \|y\|-sized frame with utterance and ontology tokens. Though empirically... | Akshat Shrivastava, Pierce Chuang, Arun Babu, Shrey Desai, Abhinav Arora, Alexander Zotov, Ahmed Aly |  |
| 206 |  |  [Language Resource Efficient Learning for Captioning](https://doi.org/10.18653/v1/2021.findings-emnlp.162) |  | 0 | Due to complex cognitive and inferential efforts involved in the manual generation of one caption per image/video input, the human annotation resources are very limited for captioning tasks. We define language resource efficient as reaching the same performance with fewer annotated captions per... | Jia Chen, Yike Wu, Shiwan Zhao, Qin Jin |  |
| 207 |  |  [Translation as Cross-Domain Knowledge: Attention Augmentation for Unsupervised Cross-Domain Segmenting and Labeling Tasks](https://doi.org/10.18653/v1/2021.findings-emnlp.163) |  | 0 | The nature of no word delimiter or inflection that can indicate segment boundaries or word semantics increases the difficulty of Chinese text understanding, and also intensifies the demand for word-level semantic knowledge to accomplish the tagging goal in Chinese segmenting and labeling tasks.... | Ruixuan Luo, Yi Zhang, Sishuo Chen, Xu Sun |  |
| 208 |  |  [ContractNLI: A Dataset for Document-level Natural Language Inference for Contracts](https://doi.org/10.18653/v1/2021.findings-emnlp.164) |  | 0 | Reviewing contracts is a time-consuming procedure that incurs large expenses to companies and social inequality to those who cannot afford it. In this work, we propose “document-level natural language inference (NLI) for contracts”, a novel, real-world application of NLI that addresses such... | Yuta Koreeda, Christopher D. Manning |  |
| 209 |  |  [Japanese Zero Anaphora Resolution Can Benefit from Parallel Texts Through Neural Transfer Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.165) |  | 0 | Parallel texts of Japanese and a non-pro-drop language have the potential of improving the performance of Japanese zero anaphora resolution (ZAR) because pronouns dropped in the former are usually mentioned explicitly in the latter. However, rule-based cross-lingual transfer is hampered by error... | Masato Umakoshi, Yugo Murawaki, Sadao Kurohashi |  |
| 210 |  |  [Grouped-Attention for Content-Selection and Content-Plan Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.166) |  | 0 | Content-planning is an essential part of data-to-text generation to determine the order of data mentioned in generated texts. Recent neural data-to-text generation models employ Pointer Networks to explicitly learn content-plan given a set of attributes as input. They use LSTM to encode the input,... | Bayu Distiawan Trisedya, Xiaojie Wang, Jianzhong Qi, Rui Zhang, Qingjun Cui |  |
| 211 |  |  [An Explicit-Joint and Supervised-Contrastive Learning Framework for Few-Shot Intent Classification and Slot Filling](https://doi.org/10.18653/v1/2021.findings-emnlp.167) |  | 0 | Intent classification (IC) and slot filling (SF) are critical building blocks in task-oriented dialogue systems. These two tasks are closely-related and can flourish each other. Since only a few utterances can be utilized for identifying fast-emerging new intents and slots, data scarcity issue... | Han Liu, Feng Zhang, Xiaotong Zhang, Siyang Zhao, Xianchao Zhang |  |
| 212 |  |  [Retrieve, Discriminate and Rewrite: A Simple and Effective Framework for Obtaining Affective Response in Retrieval-Based Chatbots](https://doi.org/10.18653/v1/2021.findings-emnlp.168) |  | 0 | Obtaining affective response is a key step in building empathetic dialogue systems. This task has been studied a lot in generation-based chatbots, but the related research in retrieval-based chatbots is still in the early stage. Existing works in retrieval-based chatbots are based on... | Xin Lu, Yijian Tian, Yanyan Zhao, Bing Qin |  |
| 213 |  |  [Span Fine-tuning for Pre-trained Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.169) |  | 0 | Pre-trained language models (PrLM) have to carefully manage input units when training on a very large text with a vocabulary consisting of millions of words. Previous works have shown that incorporating span-level information over consecutive words in pre-training could further improve the... | Rongzhou Bao, Zhuosheng Zhang, Hai Zhao |  |
| 214 |  |  [DIRECT: Direct and Indirect Responses in Conversational Text Corpus](https://doi.org/10.18653/v1/2021.findings-emnlp.170) |  | 0 | We create a large-scale dialogue corpus that provides pragmatic paraphrases to advance technology for understanding the underlying intentions of users. While neural conversation models acquire the ability to generate fluent responses through training on a dialogue corpus, previous corpora have... | Junya Takayama, Tomoyuki Kajiwara, Yuki Arase |  |
| 215 |  |  [Retrieval, Analogy, and Composition: A framework for Compositional Generalization in Image Captioning](https://doi.org/10.18653/v1/2021.findings-emnlp.171) |  | 0 | Image captioning systems are expected to have the ability to combine individual concepts when describing scenes with concept combinations that are not observed during training. In spite of significant progress in image captioning with the help of the autoregressive generation framework, current... | Zhan Shi, Hui Liu, Martin Renqiang Min, Christopher Malon, Li Erran Li, Xiaodan Zhu |  |
| 216 |  |  [TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.172) |  | 0 | Recent progress in generative language models has enabled machines to generate astonishingly realistic texts. While there are many legitimate applications of such models, there is also a rising need to distinguish machine-generated texts from human-written ones (e.g., fake news detection). However,... | Adaku Uchendu, Zeyu Ma, Thai Le, Rui Zhang, Dongwon Lee |  |
| 217 |  |  [Say 'YES' to Positivity: Detecting Toxic Language in Workplace Communications](https://doi.org/10.18653/v1/2021.findings-emnlp.173) |  | 0 | Workplace communication (e.g. email, chat, etc.) is a central part of enterprise productivity. Healthy conversations are crucial for creating an inclusive environment and maintaining harmony in an organization. Toxic communications at the workplace can negatively impact overall job satisfaction and... | Meghana Moorthy Bhat, Saghar Hosseini, Ahmed Hassan Awadallah, Paul N. Bennett, Weisheng Li |  |
| 218 |  |  [Natural SQL: Making SQL Easier to Infer from Natural Language Specifications](https://doi.org/10.18653/v1/2021.findings-emnlp.174) |  | 0 | Addressing the mismatch between natural language descriptions and the corresponding SQL queries is a key challenge for text-to-SQL translation. To bridge this gap, we propose an SQL intermediate representation (IR) called Natural SQL (NatSQL). Specifically, NatSQL preserves the core functionalities... | Yujian Gan, Xinyun Chen, Jinxia Xie, Matthew Purver, John R. Woodward, John H. Drake, Qiaofu Zhang |  |
| 219 |  |  [Mitigating Data Scarceness through Data Synthesis, Augmentation and Curriculum for Abstractive Summarization](https://doi.org/10.18653/v1/2021.findings-emnlp.175) |  | 0 | This paper explores three simple data manipulation techniques (synthesis, augmentation, curriculum) for improving abstractive summarization models without the need for any additional data. We introduce a method of data synthesis with paraphrasing, a data augmentation technique with sample mixing,... | Ahmed Magooda, Diane J. Litman |  |
| 220 |  |  [Self- and Pseudo-self-supervised Prediction of Speaker and Key-utterance for Multi-party Dialogue Reading Comprehension](https://doi.org/10.18653/v1/2021.findings-emnlp.176) |  | 0 | Multi-party dialogue machine reading comprehension (MRC) brings tremendous challenge since it involves multiple speakers at one dialogue, resulting in intricate speaker information flows and noisy dialogue contexts. To alleviate such difficulties, previous models focus on how to incorporate these... | Yiyang Li, Hai Zhao |  |
| 221 |  |  [Few-Shot Novel Concept Learning for Semantic Parsing](https://doi.org/10.18653/v1/2021.findings-emnlp.177) |  | 0 | Humans are capable of learning novel concepts from very few examples; in contrast, state-of-the-art machine learning algorithms typically need thousands of examples to do so. In this paper, we propose an algorithm for learning novel concepts by representing them as programs over existing concepts.... | Soham Dan, Osbert Bastani, Dan Roth |  |
| 222 |  |  [Compositional Data and Task Augmentation for Instruction Following](https://doi.org/10.18653/v1/2021.findings-emnlp.178) |  | 0 | Executing natural language instructions in a physically grounded domain requires a model that understands both spatial concepts such as “left of” and “above”, and the compositional language used to identify landmarks and articulate instructions relative to them. In this paper, we study instruction... | Soham Dan, Xinran Han, Dan Roth |  |
| 223 |  |  [Are Factuality Checkers Reliable? Adversarial Meta-evaluation of Factuality in Summarization](https://doi.org/10.18653/v1/2021.findings-emnlp.179) |  | 0 | With the continuous upgrading of the summarization systems driven by deep neural networks, researchers have higher requirements on the quality of the generated summaries, which should be not only fluent and informative but also factually correct. As a result, the field of factual evaluation has... | Yiran Chen, Pengfei Liu, Xipeng Qiu |  |
| 224 |  |  [On the Effects of Transformer Size on In- and Out-of-Domain Calibration](https://doi.org/10.18653/v1/2021.findings-emnlp.180) |  | 0 | Large, pre-trained transformer language models, which are pervasive in natural language processing tasks, are notoriously expensive to train. To reduce the cost of training such large models, prior work has developed smaller, more compact models which achieves a significant speedup in training time... | Soham Dan, Dan Roth |  |
| 225 |  |  [Detecting Polarized Topics Using Partisanship-aware Contextualized Topic Embeddings](https://doi.org/10.18653/v1/2021.findings-emnlp.181) |  | 0 | Growing polarization of the news media has been blamed for fanning disagreement, controversy and even violence. Early identification of polarized topics is thus an urgent matter that can help mitigate conflict. However, accurate measurement of topic-wise polarization is still an open research... | Zihao He, Negar Mokhberian, António Câmara, Andrés Abeliuk, Kristina Lerman |  |
| 226 |  |  [GenerativeRE: Incorporating a Novel Copy Mechanism and Pretrained Model for Joint Entity and Relation Extraction](https://doi.org/10.18653/v1/2021.findings-emnlp.182) |  | 0 | Previous neural Seq2Seq models have shown the effectiveness for jointly extracting relation triplets. However, most of these models suffer from incompletion and disorder problems when they extract multi-token entities from input sentences. To tackle these problems, we propose a generative,... | Jiarun Cao, Sophia Ananiadou |  |
| 227 |  |  [Re-entry Prediction for Online Conversations via Self-Supervised Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.183) |  | 0 | In recent years, world business in online discussions and opinion sharing on social media is booming. Re-entry prediction task is thus proposed to help people keep track of the discussions which they wish to continue. Nevertheless, existing works only focus on exploiting chatting history and... | Lingzhi Wang, Xingshan Zeng, Huang Hu, KamFai Wong, Daxin Jiang |  |
| 228 |  |  [proScript: Partially Ordered Scripts Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.184) |  | 0 | Scripts – prototypical event sequences describing everyday activities – have been shown to help understand narratives by providing expectations, resolving ambiguity, and filling in unstated information. However, to date they have proved hard to author or extract from text. In this work, we... | Keisuke Sakaguchi, Chandra Bhagavatula, Ronan Le Bras, Niket Tandon, Peter Clark, Yejin Choi |  |
| 229 |  |  [Speaker Turn Modeling for Dialogue Act Classification](https://doi.org/10.18653/v1/2021.findings-emnlp.185) |  | 0 | Dialogue Act (DA) classification is the task of classifying utterances with respect to the function they serve in a dialogue. Existing approaches to DA classification model utterances without incorporating the turn changes among speakers throughout the dialogue, therefore treating it no different... | Zihao He, Leili Tavabi, Kristina Lerman, Mohammad Soleymani |  |
| 230 |  |  [Unsupervised Domain Adaptation Method with Semantic-Structural Alignment for Dependency Parsing](https://doi.org/10.18653/v1/2021.findings-emnlp.186) |  | 0 | Unsupervised cross-domain dependency parsing is to accomplish domain adaptation for dependency parsing without using labeled data in target domain. Existing methods are often of the pseudo-annotation type, which generates data through self-annotation of the base model and performing iterative... | Boda Lin, Mingzheng Li, Si Li, Yong Luo |  |
| 231 |  |  [Devil's Advocate: Novel Boosting Ensemble Method from Psychological Findings for Text Classification](https://doi.org/10.18653/v1/2021.findings-emnlp.187) |  | 0 | We present a new form of ensemble method–Devil’s Advocate, which uses a deliberately dissenting model to force other submodels within the ensemble to better collaborate. Our method consists of two different training settings: one follows the conventional training process (Norm), and the other is... | Hwiyeol Jo, Jaeseo Lim, ByoungTak Zhang |  |
| 232 |  |  [SideControl: Controlled Open-domain Dialogue Generation via Additive Side Networks](https://doi.org/10.18653/v1/2021.findings-emnlp.188) |  | 0 | Transformer-based pre-trained language models boost the performance of open-domain dialogue systems. Prior works leverage Transformer-based pre-trained language models to generate texts with desired attributes in two general approaches: (1) gradient-based methods: updating all latent... | Wanyu Du, Yangfeng Ji |  |
| 233 |  |  [Is BERT a Cross-Disciplinary Knowledge Learner? A Surprising Finding of Pre-trained Models' Transferability](https://doi.org/10.18653/v1/2021.findings-emnlp.189) |  | 0 | This paper investigates whether the power of the models pre-trained on text data, such as BERT, can be transferred to general token sequence classification applications. To verify pre-trained models’ transferability, we test the pre-trained models on text classification tasks with meanings of... | WeiTsung Kao, Hungyi Lee |  |
| 234 |  |  [Geo-BERT Pre-training Model for Query Rewriting in POI Search](https://doi.org/10.18653/v1/2021.findings-emnlp.190) |  | 0 | Query Rewriting (QR) is proposed to solve the problem of the word mismatch between queries and documents in Web search. Existing approaches usually model QR with an end-to-end sequence-to-sequence (seq2seq) model. The state-of-the-art Transformer-based models can effectively learn textual semantics... | Xiao Liu, Juan Hu, Qi Shen, Huan Chen |  |
| 235 |  |  [Leveraging Bidding Graphs for Advertiser-Aware Relevance Modeling in Sponsored Search](https://doi.org/10.18653/v1/2021.findings-emnlp.191) |  | 0 | Recently, sponsored search has become one of the most lucrative channels for marketing. As the fundamental basis of sponsored search, relevance modeling has attracted increasing attention due to the tremendous practical value. Most existing methods solely rely on the query-keyword pairs. However,... | Shuxian Bi, Chaozhuo Li, Xiao Han, Zheng Liu, Xing Xie, Haizhen Huang, Zengxuan Wen |  |
| 236 |  |  [GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation](https://doi.org/10.18653/v1/2021.findings-emnlp.192) |  | 0 | Large-scale language models such as GPT-3 are excellent few-shot learners, allowing them to be controlled via natural text prompts. Recent studies report that prompt-based direct classification eliminates the need for fine-tuning but lacks data and inference scalability. This paper proposes a novel... | Kang Min Yoo, Dongju Park, Jaewook Kang, SangWoo Lee, WooMyoung Park |  |
| 237 |  |  [Context-aware Entity Typing in Knowledge Graphs](https://doi.org/10.18653/v1/2021.findings-emnlp.193) |  | 0 | Knowledge graph entity typing aims to infer entities’ missing types in knowledge graphs which is an important but under-explored issue. This paper proposes a novel method for this task by utilizing entities’ contextual information. Specifically, we design two inference mechanisms: i) N2T:... | Weiran Pan, Wei Wei, XianLing Mao |  |
| 238 |  |  [Attribute Alignment: Controlling Text Generation from Pre-trained Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.194) |  | 0 | Large language models benefit from training with a large amount of unlabeled text, which gives them increasingly fluent and diverse generation capabilities. However, using these models for text generation that takes into account target attributes, such as sentiment polarity or specific topics,... | Dian Yu, Zhou Yu, Kenji Sagae |  |
| 239 |  |  [Generate & Rank: A Multi-task Framework for Math Word Problems](https://doi.org/10.18653/v1/2021.findings-emnlp.195) |  | 0 | Math word problem (MWP) is a challenging and critical task in natural language processing. Many recent studies formalize MWP as a generation task and have adopted sequence-to-sequence models to transform problem descriptions to mathematical expressions. However, mathematical expressions are prone... | Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, Qun Liu |  |
| 240 |  |  [MIRTT: Learning Multimodal Interaction Representations from Trilinear Transformers for Visual Question Answering](https://doi.org/10.18653/v1/2021.findings-emnlp.196) |  | 0 | In Visual Question Answering (VQA), existing bilinear methods focus on the interaction between images and questions. As a result, the answers are either spliced into the questions or utilized as labels only for classification. On the other hand, trilinear models such as the CTI model efficiently... | Junjie Wang, Yatai Ji, Jiaqi Sun, Yujiu Yang, Tetsuya Sakai |  |
| 241 |  |  [UniteD-SRL: A Unified Dataset for Span- and Dependency-Based Multilingual and Cross-Lingual Semantic Role Labeling](https://doi.org/10.18653/v1/2021.findings-emnlp.197) |  | 0 | Multilingual and cross-lingual Semantic Role Labeling (SRL) have recently garnered increasing attention as multilingual text representation techniques have become more effective and widely available. While recent work has attained growing success, results on gold multilingual benchmarks are still... | Rocco Tripodi, Simone Conia, Roberto Navigli |  |
| 242 |  |  [Enhancing Dual-Encoders with Question and Answer Cross-Embeddings for Answer Retrieval](https://doi.org/10.18653/v1/2021.findings-emnlp.198) |  | 0 | Dual-Encoders is a promising mechanism for answer retrieval in question answering (QA) systems. Currently most conventional Dual-Encoders learn the semantic representations of questions and answers merely through matching score. Researchers proposed to introduce the QA interaction features in... | Yanmeng Wang, Jun Bai, Ye Wang, Jianfei Zhang, Wenge Rong, Zongcheng Ji, Shaojun Wang, Jing Xiao |  |
| 243 |  |  [A Neural Graph-based Local Coherence Model](https://doi.org/10.18653/v1/2021.findings-emnlp.199) |  | 0 | Entity grids and entity graphs are two frameworks for modeling local coherence. These frameworks represent entity relations between sentences and then extract features from such representations to encode coherence. The benefits of convolutional neural models for extracting informative features from... | Mohsen Mesgar, Leonardo F. R. Ribeiro, Iryna Gurevych |  |
| 244 |  |  [GiBERT: Enhancing BERT with Linguistic Information using a Lightweight Gated Injection Method](https://doi.org/10.18653/v1/2021.findings-emnlp.200) |  | 0 | Large pre-trained language models such as BERT have been the driving force behind recent improvements across many NLP tasks. However, BERT is only trained to predict missing words – either through masking or next sentence prediction – and has no knowledge of lexical, syntactic or semantic... | Nicole Peinelt, Marek Rei, Maria Liakata |  |
| 245 |  |  [RollingLDA: An Update Algorithm of Latent Dirichlet Allocation to Construct Consistent Time Series from Textual Data](https://doi.org/10.18653/v1/2021.findings-emnlp.201) |  | 0 | We propose a rolling version of the Latent Dirichlet Allocation, called RollingLDA. By a sequential approach, it enables the construction of LDA-based time series of topics that are consistent with previous states of LDA models. After an initial modeling, updates can be computed efficiently,... | Jonas Rieger, Carsten Jentsch, Jörg Rahnenführer |  |
| 246 |  |  [What If Sentence-hood is Hard to Define: A Case Study in Chinese Reading Comprehension](https://doi.org/10.18653/v1/2021.findings-emnlp.202) |  | 0 | Machine reading comprehension (MRC) is a challenging NLP task for it requires to carefully deal with all linguistic granularities from word, sentence to passage. For extractive MRC, the answer span has been shown mostly determined by key evidence linguistic units, in which it is a sentence in most... | Jiawei Wang, Hai Zhao, Yinggong Zhao, Libin Shen |  |
| 247 |  |  [Refining BERT Embeddings for Document Hashing via Mutual Information Maximization](https://doi.org/10.18653/v1/2021.findings-emnlp.203) |  | 0 | Existing unsupervised document hashing methods are mostly established on generative models. Due to the difficulties of capturing long dependency structures, these methods rarely model the raw documents directly, but instead to model the features extracted from them (e.g. bag-of-words (BOG), TFIDF).... | Zijing Ou, Qinliang Su, Jianxing Yu, Ruihui Zhao, Yefeng Zheng, Bang Liu |  |
| 248 |  |  [REBEL: Relation Extraction By End-to-end Language generation](https://doi.org/10.18653/v1/2021.findings-emnlp.204) |  | 0 | Extracting relation triplets from raw text is a crucial task in Information Extraction, enabling multiple applications such as populating or validating knowledge bases, factchecking, and other downstream tasks. However, it usually involves multiple-step pipelines that propagate errors or are... | PereLluís Huguet Cabot, Roberto Navigli |  |
| 249 |  |  [Wine is not v i n. On the Compatibility of Tokenizations across Languages](https://doi.org/10.18653/v1/2021.findings-emnlp.205) |  | 0 | The size of the vocabulary is a central design choice in large pretrained language models, with respect to both performance and memory requirements. Typically, subword tokenization algorithms such as byte pair encoding and WordPiece are used. In this work, we investigate the compatibility of... | Antonis Maronikolakis, Philipp Dufter, Hinrich Schütze |  |
| 250 |  |  [Temporal Adaptation of BERT and Performance on Downstream Document Classification: Insights from Social Media](https://doi.org/10.18653/v1/2021.findings-emnlp.206) |  | 0 | Language use differs between domains and even within a domain, language use changes over time. For pre-trained language models like BERT, domain adaptation through continued pre-training has been shown to improve performance on in-domain downstream tasks. In this article, we investigate whether... | Paul Röttger, Janet B. Pierrehumbert |  |
| 251 |  |  [Skim-Attention: Learning to Focus via Document Layout](https://doi.org/10.18653/v1/2021.findings-emnlp.207) |  | 0 | Transformer-based pre-training techniques of text and layout have proven effective in a number of document understanding tasks. Despite this success, multimodal pre-training models suffer from very high computational and memory costs. Motivated by human reading strategies, this paper presents... | Laura Nguyen, Thomas Scialom, Jacopo Staiano, Benjamin Piwowarski |  |
| 252 |  |  [Attention-based Contrastive Learning for Winograd Schemas](https://doi.org/10.18653/v1/2021.findings-emnlp.208) |  | 0 | Self-supervised learning has recently attracted considerable attention in the NLP community for its ability to learn discriminative features using a contrastive objective. This paper investigates whether contrastive learning can be extended to Transfomer attention to tackling the Winograd Schema... | Tassilo Klein, Moin Nabi |  |
| 253 |  |  [Give the Truth: Incorporate Semantic Slot into Abstractive Dialogue Summarization](https://doi.org/10.18653/v1/2021.findings-emnlp.209) |  | 0 | Abstractive dialogue summarization suffers from a lots of factual errors, which are due to scattered salient elements in the multi-speaker information interaction process. In this work, we design a heterogeneous semantic slot graph with a slot-level mask cross-attention to enhance the slot features... | Lulu Zhao, Weihao Zeng, Weiran Xu, Jun Guo |  |
| 254 |  |  [Challenges in Detoxifying Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.210) |  | 0 | Large language models (LM) generate remarkably fluent text and can be efficiently adapted across NLP tasks. Measuring and guaranteeing the quality of generated text in terms of safety is imperative for deploying LMs in the real world; to this end, prior work often relies on automatic evaluation of... | Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, PoSen Huang |  |
| 255 |  |  [Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.211) |  | 0 | Recent works have found evidence of gender bias in models of machine translation and coreference resolution using mostly synthetic diagnostic datasets. While these quantify bias in a controlled experiment, they often do so on a small scale and consist mostly of artificial, out-of-distribution... | Shahar Levy, Koren Lazar, Gabriel Stanovsky |  |
| 256 |  |  [Competence-based Curriculum Learning for Multilingual Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.212) |  | 0 | Currently, multilingual machine translation is receiving more and more attention since it brings better performance for low resource languages (LRLs) and saves more space. However, existing multilingual machine translation models face a severe challenge: imbalance. As a result, the translation... | Mingliang Zhang, Fandong Meng, Yunhai Tong, Jie Zhou |  |
| 257 |  |  [Informed Sampling for Diversity in Concept-to-Text NLG](https://doi.org/10.18653/v1/2021.findings-emnlp.213) |  | 0 | Deep-learning models for language generation tasks tend to produce repetitive output. Various methods have been proposed to encourage lexical diversity during decoding, but this often comes at a cost to the perceived fluency and adequacy of the output. In this work, we propose to ameliorate this... | Giulio Zhou, Gerasimos Lampouras |  |
| 258 |  |  [Novel Natural Language Summarization of Program Code via Leveraging Multiple Input Representations](https://doi.org/10.18653/v1/2021.findings-emnlp.214) |  | 0 | The lack of description of a given program code acts as a big hurdle to those developers new to the code base for its understanding. To tackle this problem, previous work on code summarization, the task of automatically generating code description given a piece of code reported that an auxiliary... | Fuxiang Chen, Mijung Kim, Jaegul Choo |  |
| 259 |  |  [WikiNEuRal: Combined Neural and Knowledge-based Silver Data Creation for Multilingual NER](https://doi.org/10.18653/v1/2021.findings-emnlp.215) |  | 0 | Multilingual Named Entity Recognition (NER) is a key intermediate task which is needed in many areas of NLP. In this paper, we address the well-known issue of data scarcity in NER, especially relevant when moving to a multilingual scenario, and go beyond current approaches to the creation of... | Simone Tedeschi, Valentino Maiorca, Niccolò Campolungo, Francesco Cecconi, Roberto Navigli |  |
| 260 |  |  [Beyond Grammatical Error Correction: Improving L1-influenced research writing in English using pre-trained encoder-decoder models](https://doi.org/10.18653/v1/2021.findings-emnlp.216) |  | 0 | In this paper, we present a new method for training a writing improvement model adapted to the writer’s first language (L1) that goes beyond grammatical error correction (GEC). Without using annotated training data, we rely solely on pre-trained language models fine-tuned with parallel corpora of... | Gustavo Zomer, Ana FrankenbergGarcia |  |
| 261 |  |  [Classification and Geotemporal Analysis of Quality-of-Life Issues in Tenant Reviews](https://doi.org/10.18653/v1/2021.findings-emnlp.217) |  | 0 | Online tenant reviews of multifamily residential properties present a unique source of information for commercial real estate investing and research. Real estate professionals frequently read tenant reviews to uncover property-related issues that are otherwise difficult to detect, a process that is... | Adam Haber, Zeev Waks |  |
| 262 |  |  [Probing Pre-trained Language Models for Semantic Attributes and their Values](https://doi.org/10.18653/v1/2021.findings-emnlp.218) |  | 0 | Pretrained language models (PTLMs) yield state-of-the-art performance on many natural language processing tasks, including syntax, semantics and commonsense. In this paper, we focus on identifying to what extent do PTLMs capture semantic attributes and their values, e.g., the correlation between... | Meriem Beloucif, Chris Biemann |  |
| 263 |  |  [Uncovering the Limits of Text-based Emotion Detection](https://doi.org/10.18653/v1/2021.findings-emnlp.219) |  | 0 | Identifying emotions from text is crucial for a variety of real world tasks. We consider the two largest now-available corpora for emotion classification: GoEmotions, with 58k messages labelled by readers, and Vent, with 33M writer-labelled messages. We design a benchmark and evaluate several... | Nurudin AlvarezGonzalez, Andreas Kaltenbrunner, Vicenç Gómez |  |
| 264 |  |  [Named Entity Recognition for Entity Linking: What Works and What's Next](https://doi.org/10.18653/v1/2021.findings-emnlp.220) |  | 0 | Entity Linking (EL) systems have achieved impressive results on standard benchmarks mainly thanks to the contextualized representations provided by recent pretrained language models. However, such systems still require massive amounts of data – millions of labeled examples – to perform at their... | Simone Tedeschi, Simone Conia, Francesco Cecconi, Roberto Navigli |  |
| 265 |  |  [Learning Numeracy: A Simple Yet Effective Number Embedding Approach Using Knowledge Graph](https://doi.org/10.18653/v1/2021.findings-emnlp.221) |  | 0 | Numeracy plays a key role in natural language understanding. However, existing NLP approaches, not only traditional word2vec approach or contextualized transformer-based language models, fail to learn numeracy. As the result, the performance of these models is limited when they are applied to... | Hanyu Duan, Yi Yang, Kar Yan Tam |  |
| 266 |  |  [Weakly Supervised Semantic Parsing by Learning from Mistakes](https://doi.org/10.18653/v1/2021.findings-emnlp.222) |  | 0 | Weakly supervised semantic parsing (WSP) aims at training a parser via utterance-denotation pairs. This task is challenging because it requires (1) searching consistent logical forms in a huge space; and (2) dealing with spurious logical forms. In this work, we propose Learning from Mistakes (LFM),... | Jiaqi Guo, JianGuang Lou, Ting Liu, Dongmei Zhang |  |
| 267 |  |  [CodeQA: A Question Answering Dataset for Source Code Comprehension](https://doi.org/10.18653/v1/2021.findings-emnlp.223) |  | 0 | We propose CodeQA, a free-form question answering dataset for the purpose of source code comprehension: given a code snippet and a question, a textual answer is required to be generated. CodeQA contains a Java dataset with 119,778 question-answer pairs and a Python dataset with 70,085... | Chenxiao Liu, Xiaojun Wan |  |
| 268 |  |  [Subword Mapping and Anchoring across Languages](https://doi.org/10.18653/v1/2021.findings-emnlp.224) |  | 0 | State-of-the-art multilingual systems rely on shared vocabularies that sufficiently cover all considered languages. To this end, a simple and frequently used approach makes use of subword vocabularies constructed jointly over several languages. We hypothesize that such vocabularies are suboptimal... | Giorgos Vernikos, Andrei PopescuBelis |  |
| 269 |  |  [CDLM: Cross-Document Language Modeling](https://doi.org/10.18653/v1/2021.findings-emnlp.225) |  | 0 | We introduce a new pretraining approach geared for multi-document language modeling, incorporating two key ideas into the masked language modeling self-supervised objective. First, instead of considering documents in isolation, we pretrain over sets of multiple related documents, encouraging the... | Avi Caciularu, Arman Cohan, Iz Beltagy, Matthew E. Peters, Arie Cattan, Ido Dagan |  |
| 270 |  |  [Patterns of Polysemy and Homonymy in Contextualised Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.226) |  | 0 | One of the central aspects of contextualised language models is that they should be able to distinguish the meaning of lexically ambiguous words by their contexts. In this paper we investigate the extent to which the contextualised embeddings of word forms that display multiplicity of sense reflect... | Janosch Haber, Massimo Poesio |  |
| 271 |  |  [Cross-Lingual Leveled Reading Based on Language-Invariant Features](https://doi.org/10.18653/v1/2021.findings-emnlp.227) |  | 0 | Leveled reading (LR) aims to automatically classify texts by the cognitive levels of readers, which is fundamental in providing appropriate reading materials regarding different reading capabilities. However, most state-of-the-art LR methods rely on the availability of copious annotated resources,... | Simin Rao, Hua Zheng, Sujian Li |  |
| 272 |  |  [Controlled Neural Sentence-Level Reframing of News Articles](https://doi.org/10.18653/v1/2021.findings-emnlp.228) |  | 0 | Framing a news article means to portray the reported event from a specific perspective, e.g., from an economic or a health perspective. Reframing means to change this perspective. Depending on the audience or the submessage, reframing can become necessary to achieve the desired effect on the... | WeiFan Chen, Khalid Al Khatib, Benno Stein, Henning Wachsmuth |  |
| 273 |  |  [DialogueTRM: Exploring Multi-Modal Emotional Dynamics in a Conversation](https://doi.org/10.18653/v1/2021.findings-emnlp.229) |  | 0 | Emotion dynamics formulates principles explaining the emotional fluctuation during conversations. Recent studies explore the emotion dynamics from the self and inter-personal dependencies, however, ignoring the temporal and spatial dependencies in the situation of multi-modal conversations. To... | Yuzhao Mao, Guang Liu, Xiaojie Wang, Weiguo Gao, Xuan Li |  |
| 274 |  |  [Adversarial Examples for Evaluating Math Word Problem Solvers](https://doi.org/10.18653/v1/2021.findings-emnlp.230) |  | 0 | Standard accuracy metrics have shown that Math Word Problem (MWP) solvers have achieved high performance on benchmark datasets. However, the extent to which existing MWP solvers truly understand language and its relation with numbers is still unclear. In this paper, we generate adversarial attacks... | Vivek Kumar, Rishabh Maheshwary, Vikram Pudi |  |
| 275 |  |  [Improving Numerical Reasoning Skills in the Modular Approach for Complex Question Answering on Text](https://doi.org/10.18653/v1/2021.findings-emnlp.231) |  | 0 | Numerical reasoning skills are essential for complex question answering (CQA) over text. It requires opertaions including counting, comparison, addition and subtraction. A successful approach to CQA on text, Neural Module Networks (NMNs), follows the programmer-interpreter paradigm and leverages... | Xiaoyu Guo, YuanFang Li, Gholamreza Haffari |  |
| 276 |  |  [Retrieval Augmented Code Generation and Summarization](https://doi.org/10.18653/v1/2021.findings-emnlp.232) |  | 0 | Software developers write a lot of source code and documentation during software development. Intrinsically, developers often recall parts of source code or code summaries that they had written in the past while implementing software or documenting them. To mimic developers’ code or summary... | Md. Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, KaiWei Chang |  |
| 277 |  |  [Multilingual Translation via Grafting Pre-trained Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.233) |  | 0 | Can pre-trained BERT for one language and GPT for another be glued together to translate texts? Self-supervised training using only monolingual data has led to the success of pre-trained (masked) language models in many NLP tasks. However, directly connecting BERT as an encoder and GPT as a decoder... | Zewei Sun, Mingxuan Wang, Lei Li |  |
| 278 |  |  [AEDA: An Easier Data Augmentation Technique for Text Classification](https://doi.org/10.18653/v1/2021.findings-emnlp.234) |  | 0 | This paper proposes AEDA (An Easier Data Augmentation) technique to help improve the performance on text classification tasks. AEDA includes only random insertion of punctuation marks into the original text. This is an easier technique to implement for data augmentation than EDA method (Wei and... | Akbar Karimi, Leonardo Rossi, Andrea Prati |  |
| 279 |  |  [A Comprehensive Comparison of Word Embeddings in Event & Entity Coreference Resolution](https://doi.org/10.18653/v1/2021.findings-emnlp.235) |  | 0 | Coreference Resolution is an important NLP task and most state-of-the-art methods rely on word embeddings for word representation. However, one issue that has been largely overlooked in literature is that of comparing the performance of different embeddings across and within families. Therefore, we... | Judicael Poumay, Ashwin Ittoo |  |
| 280 |  |  [Wav-BERT: Cooperative Acoustic and Linguistic Representation Learning for Low-Resource Speech Recognition](https://doi.org/10.18653/v1/2021.findings-emnlp.236) |  | 0 | Unifying acoustic and linguistic representation learning has become increasingly crucial to transfer the knowledge learned on the abundance of high-resource language data for low-resource speech recognition. Existing approaches simply cascade pre-trained acoustic and language models to learn the... | Guolin Zheng, Yubei Xiao, Ke Gong, Pan Zhou, Xiaodan Liang, Liang Lin |  |
| 281 |  |  [Multilingual AMR Parsing with Noisy Knowledge Distillation](https://doi.org/10.18653/v1/2021.findings-emnlp.237) |  | 0 | We study multilingual AMR parsing from the perspective of knowledge distillation, where the aim is to learn and improve a multilingual AMR parser by using an existing English parser as its teacher. We constrain our exploration in a strict multilingual setting: there is but one model to parse all... | Deng Cai, Xin Li, Jackie ChunSing Ho, Lidong Bing, Wai Lam |  |
| 282 |  |  [Open-Domain Contextual Link Prediction and its Complementarity with Entailment Graphs](https://doi.org/10.18653/v1/2021.findings-emnlp.238) |  | 0 | An open-domain knowledge graph (KG) has entities as nodes and natural language relations as edges, and is constructed by extracting (subject, relation, object) triples from text. The task of open-domain link prediction is to infer missing relations in the KG. Previous work has used standard link... | Mohammad Javad Hosseini, Shay B. Cohen, Mark Johnson, Mark Steedman |  |
| 283 |  |  [Analysis of Language Change in Collaborative Instruction Following](https://doi.org/10.18653/v1/2021.findings-emnlp.239) |  | 0 | We analyze language change over time in a collaborative, goal-oriented instructional task, where utility-maximizing participants form conventions and increase their expertise. Prior work studied such scenarios mostly in the context of reference games, and consistently found that language complexity... | Anna Effenberger, Rhia Singh, Eva Yan, Alane Suhr, Yoav Artzi |  |
| 284 |  |  [Counter-Interference Adapter for Multilingual Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.240) |  | 0 | Developing a unified multilingual model has been a long pursuing goal for machine translation. However, existing approaches suffer from performance degradation - a single multilingual model is inferior to separately trained bilingual ones on rich-resource languages. We conjecture that such a... | Yaoming Zhu, Jiangtao Feng, Chengqi Zhao, Mingxuan Wang, Lei Li |  |
| 285 |  |  [Progressive Transformer-Based Generation of Radiology Reports](https://doi.org/10.18653/v1/2021.findings-emnlp.241) |  | 0 | Inspired by Curriculum Learning, we propose a consecutive (i.e., image-to-text-to-text) generation framework where we divide the problem of radiology report generation into two steps. Contrary to generating the full radiology report from the image at once, the model generates global concepts from... | Farhad Nooralahzadeh, Nicolas Perez Gonzalez, Thomas Frauenfelder, Koji Fujimoto, Michael Krauthammer |  |
| 286 |  |  ["Be nice to your wife! The restaurants are closed": Can Gender Stereotype Detection Improve Sexism Classification?](https://doi.org/10.18653/v1/2021.findings-emnlp.242) |  | 0 | In this paper, we focus on the detection of sexist hate speech against women in tweets studying for the first time the impact of gender stereotype detection on sexism classification. We propose: (1) the first dataset annotated for gender stereotype detection, (2) a new method for data augmentation... | Patricia Chiril, Farah Benamara, Véronique Moriceau |  |
| 287 |  |  [Automatic Discrimination between Inherited and Borrowed Latin Words in Romance Languages](https://doi.org/10.18653/v1/2021.findings-emnlp.243) |  | 0 | In this paper, we address the problem of automatically discriminating between inherited and borrowed Latin words. We introduce a new dataset and investigate the case of Romance languages (Romanian, Italian, French, Spanish, Portuguese and Catalan), where words directly inherited from Latin coexist... | Alina Maria Cristea, Liviu P. Dinu, Simona Georgescu, MihneaLucian Mihai, Ana Sabina Uban |  |
| 288 |  |  [Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections](https://doi.org/10.18653/v1/2021.findings-emnlp.244) |  | 0 | Large pre-trained language models (LMs) such as GPT-3 have acquired a surprising ability to perform zero-shot learning. For example, to classify sentiment without any training examples, we can “prompt” the LM with the review and the label description “Does the user like this movie?”, and ask... | Ruiqi Zhong, Kristy Lee, Zheng Zhang, Dan Klein |  |
| 289 |  |  [Knowledge-Interactive Network with Sentiment Polarity Intensity-Aware Multi-Task Learning for Emotion Recognition in Conversations](https://doi.org/10.18653/v1/2021.findings-emnlp.245) |  | 0 | Emotion Recognition in Conversation (ERC) has gained much attention from the NLP community recently. Some models concentrate on leveraging commonsense knowledge or multi-task learning to help complicated emotional reasoning. However, these models neglect direct utterance-knowledge interaction. In... | Yunhe Xie, Kailai Yang, Chengjie Sun, Bingquan Liu, Zhenzhou Ji |  |
| 290 |  |  [Minimizing Annotation Effort via Max-Volume Spectral Sampling](https://doi.org/10.18653/v1/2021.findings-emnlp.246) |  | 0 | We address the annotation data bottleneck for sequence classification. Specifically we ask the question: if one has a budget of N annotations, which samples should we select for annotation? The solution we propose looks for diversity in the selected sample, by maximizing the amount of information... | Ariadna Quattoni, Xavier Carreras |  |
| 291 |  |  [On the Complementarity between Pre-Training and Back-Translation for Neural Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.247) |  | 0 | Pre-training (PT) and back-translation (BT) are two simple and powerful methods to utilize monolingual data for improving the model performance of neural machine translation (NMT). This paper takes the first step to investigate the complementarity between PT and BT. We introduce two probing tasks... | Xuebo Liu, Longyue Wang, Derek F. Wong, Liang Ding, Lidia S. Chao, Shuming Shi, Zhaopeng Tu |  |
| 292 |  |  [Lexicon-Based Graph Convolutional Network for Chinese Word Segmentation](https://doi.org/10.18653/v1/2021.findings-emnlp.248) |  | 0 | Precise information of word boundary can alleviate the problem of lexical ambiguity to improve the performance of natural language processing (NLP) tasks. Thus, Chinese word segmentation (CWS) is a fundamental task in NLP. Due to the development of pre-trained language models (PLM), pre-trained... | Kaiyu Huang, Hao Yu, Junpeng Liu, Wei Liu, Jingxiang Cao, Degen Huang |  |
| 293 |  |  [KFCNet: Knowledge Filtering and Contrastive Learning for Generative Commonsense Reasoning](https://doi.org/10.18653/v1/2021.findings-emnlp.249) |  | 0 | Pre-trained language models have led to substantial gains over a broad range of natural language processing (NLP) tasks, but have been shown to have limitations for natural language generation tasks with high-quality requirements on the output, such as commonsense generation and ad keyword... | Haonan Li, Yeyun Gong, Jian Jiao, Ruofei Zhang, Timothy Baldwin, Nan Duan |  |
| 294 |  |  [Monolingual and Cross-Lingual Acceptability Judgments with the Italian CoLA corpus](https://doi.org/10.18653/v1/2021.findings-emnlp.250) |  | 0 | The development of automated approaches to linguistic acceptability has been greatly fostered by the availability of the English CoLA corpus, which has also been included in the widely used GLUE benchmark. However, this kind of research for languages other than English, as well as the analysis of... | Daniela Trotta, Raffaele Guarasci, Elisa Leonardelli, Sara Tonelli |  |
| 295 |  |  [Hyperbolic Hierarchy-Aware Knowledge Graph Embedding for Link Prediction](https://doi.org/10.18653/v1/2021.findings-emnlp.251) |  | 0 | Knowledge graph embedding (KGE) using low-dimensional representations to predict missing information is widely applied in knowledge completion. Existing embedding methods are mostly built on Euclidean space, which are difficult to handle hierarchical structures. Hyperbolic embedding methods have... | Zhe Pan, Peng Wang |  |
| 296 |  |  [A Discourse-Aware Graph Neural Network for Emotion Recognition in Multi-Party Conversation](https://doi.org/10.18653/v1/2021.findings-emnlp.252) |  | 0 | Emotion recognition in multi-party conversation (ERMC) is becoming increasingly popular as an emerging research topic in natural language processing. Prior research focuses on exploring sequential information but ignores the discourse structures of conversations. In this paper, we investigate the... | Yang Sun, Nan Yu, Guohong Fu |  |
| 297 |  |  [MeLT: Message-Level Transformer with Masked Document Representations as Pre-Training for Stance Detection](https://doi.org/10.18653/v1/2021.findings-emnlp.253) |  | 0 | Much of natural language processing is focused on leveraging large capacity language models, typically trained over single messages with a task of predicting one or more tokens. However, modeling human language at higher-levels of context (i.e., sequences of messages) is under-explored. In stance... | Matthew Matero, Nikita Soni, Niranjan Balasubramanian, H. Andrew Schwartz |  |
| 298 |  |  [LMSOC: An Approach for Socially Sensitive Pretraining](https://doi.org/10.18653/v1/2021.findings-emnlp.254) |  | 0 | While large-scale pretrained language models have been shown to learn effective linguistic representations for many NLP tasks, there remain many real-world contextual aspects of language that current approaches do not capture. For instance, consider a cloze test “I enjoyed the _____ game this... | Vivek Kulkarni, Shubhanshu Mishra, Aria Haghighi |  |
| 299 |  |  [Extract, Integrate, Compete: Towards Verification Style Reading Comprehension](https://doi.org/10.18653/v1/2021.findings-emnlp.255) |  | 0 | In this paper, we present a new verification style reading comprehension dataset named VGaokao from Chinese Language tests of Gaokao. Different from existing efforts, the new dataset is originally designed for native speakers’ evaluation, thus requiring more advanced language understanding skills.... | Chen Zhang, Yuxuan Lai, Yansong Feng, Dongyan Zhao |  |
| 300 |  |  [Comparing learnability of two dependency schemes: 'semantic' (UD) and 'syntactic' (SUD)](https://doi.org/10.18653/v1/2021.findings-emnlp.256) |  | 0 | This paper contributes to the thread of research on the learnability of different dependency annotation schemes: one (‘semantic’) favouring content words as heads of dependency relations and the other (‘syntactic’) favouring syntactic heads. Several studies have lent support to the idea that... | Ryszard Tuora, Adam Przepiórkowski, Aleksander Leczkowski |  |
| 301 |  |  [Argumentation-Driven Evidence Association in Criminal Cases](https://doi.org/10.18653/v1/2021.findings-emnlp.257) |  | 0 | Evidence association in criminal cases is dividing a set of judicial evidence into several non-overlapping subsets, improving the interpretability and legality of conviction. Observably, evidence divided into the same subset usually supports the same claim. Therefore, we propose an... | Yefei Teng, Wenhan Chao |  |
| 302 |  |  [Eliminating Sentiment Bias for Aspect-Level Sentiment Classification with Unsupervised Opinion Extraction](https://doi.org/10.18653/v1/2021.findings-emnlp.258) |  | 0 | Aspect-level sentiment classification (ALSC) aims at identifying the sentiment polarity of a specified aspect in a sentence. ALSC is a practical setting in aspect-based sentiment analysis due to no opinion term labeling needed, but it fails to interpret why a sentiment polarity is derived for the... | Bo Wang, Tao Shen, Guodong Long, Tianyi Zhou, Yi Chang |  |
| 303 |  |  [Data Efficient Masked Language Modeling for Vision and Language](https://doi.org/10.18653/v1/2021.findings-emnlp.259) |  | 0 | Masked language modeling (MLM) is one of the key sub-tasks in vision-language pretraining. In the cross-modal setting, tokens in the sentence are masked at random, and the model predicts the masked tokens given the image and the text. In this paper, we observe several key disadvantages of MLM in... | Yonatan Bitton, Michael Elhadad, Gabriel Stanovsky, Roy Schwartz |  |
| 304 |  |  [Improving Multilingual Neural Machine Translation with Auxiliary Source Languages](https://doi.org/10.18653/v1/2021.findings-emnlp.260) |  | 0 | Multilingual neural machine translation models typically handle one source language at a time. However, prior work has shown that translating from multiple source languages improves translation quality. Different from existing approaches on multi-source translation that are limited to the test... | Weijia Xu, Yuwei Yin, Shuming Ma, Dongdong Zhang, Haoyang Huang |  |
| 305 |  |  [How Does Fine-tuning Affect the Geometry of Embedding Space: A Case Study on Isotropy](https://doi.org/10.18653/v1/2021.findings-emnlp.261) |  | 0 | It is widely accepted that fine-tuning pre-trained language models usually brings about performance improvements in downstream tasks. However, there are limited studies on the reasons behind this effectiveness, particularly from the viewpoint of structural changes in the embedding space. Trying to... | Sara Rajaee, Mohammad Taher Pilehvar |  |
| 306 |  |  [Locality Preserving Sentence Encoding](https://doi.org/10.18653/v1/2021.findings-emnlp.262) |  | 0 | Although researches on word embeddings have made great progress in recent years, many tasks in natural language processing are on the sentence level. Thus, it is essential to learn sentence embeddings. Recently, Sentence BERT (SBERT) is proposed to learn embeddings on the sentence level, and it... | Changrong Min, Yonghe Chu, Liang Yang, Bo Xu, Hongfei Lin |  |
| 307 |  |  [Knowledge Representation Learning with Contrastive Completion Coding](https://doi.org/10.18653/v1/2021.findings-emnlp.263) |  | 0 | Knowledge representation learning (KRL) has been used in plenty of knowledge-driven tasks. Despite fruitfully progress, existing methods still suffer from the immaturity on tackling potentially-imperfect knowledge graphs and highly-imbalanced positive-negative instances during training, both of... | Bo Ouyang, Wenbing Huang, Runfa Chen, Zhixing Tan, Yang Liu, Maosong Sun, Jihong Zhu |  |
| 308 |  |  [Knowledge-Enhanced Evidence Retrieval for Counterargument Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.264) |  | 0 | Finding counterevidence to statements is key to many tasks, including counterargument generation. We build a system that, given a statement, retrieves counterevidence from diverse sources on the Web. At the core of this system is a natural language inference (NLI) model that determines whether a... | Yohan Jo, Haneul Yoo, JinYeong Bak, Alice Oh, Chris Reed, Eduard H. Hovy |  |
| 309 |  |  [Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model](https://doi.org/10.18653/v1/2021.findings-emnlp.265) |  | 0 | The transformer-based pre-trained language models have been tremendously successful in most of the conventional NLP tasks. But they often struggle in those tasks where numerical understanding is required. Some possible reasons can be the tokenizers and pre-training objectives which are not... | Kuntal Kumar Pal, Chitta Baral |  |
| 310 |  |  [Modeling Mathematical Notation Semantics in Academic Papers](https://doi.org/10.18653/v1/2021.findings-emnlp.266) |  | 0 | Natural language models often fall short when understanding and generating mathematical notation. What is not clear is whether these shortcomings are due to fundamental limitations of the models, or the absence of appropriate tasks. In this paper, we explore the extent to which natural language... | Hwiyeol Jo, Dongyeop Kang, Andrew Head, Marti A. Hearst |  |
| 311 |  |  [Unpacking the Interdependent Systems of Discrimination: Ableist Bias in NLP Systems through an Intersectional Lens](https://doi.org/10.18653/v1/2021.findings-emnlp.267) |  | 0 | Much of the world’s population experiences some form of disability during their lifetime. Caution must be exercised while designing natural language processing (NLP) systems to prevent systems from inadvertently perpetuating ableist bias against people with disabilities, i.e., prejudice that favors... | Saad Hassan, Matt Huenerfauth, Cecilia Ovesdotter Alm |  |
| 312 |  |  [Constructing Emotional Consensus and Utilizing Unpaired Data for Empathetic Dialogue Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.268) |  | 0 | Researches on dialogue empathy aim to endow an agent with the capacity of accurate understanding and proper responding for emotions. Existing models for empathetic dialogue generation focus on the emotion flow in one direction, that is, from the context to response. We argue that conducting an... | Lei Shen, Jinchao Zhang, Jiao Ou, Xiaofang Zhao, Jie Zhou |  |
| 313 |  |  [Automatic rule generation for time expression normalization](https://doi.org/10.18653/v1/2021.findings-emnlp.269) |  | 0 | The understanding of time expressions includes two sub-tasks: recognition and normalization. In recent years, significant progress has been made in the recognition of time expressions while research on normalization has lagged behind. Existing SOTA normalization methods highly rely on rules or... | Wentao Ding, Jianhao Chen, Jinmao Li, Yuzhong Qu |  |
| 314 |  |  [RW-KD: Sample-wise Loss Terms Re-Weighting for Knowledge Distillation](https://doi.org/10.18653/v1/2021.findings-emnlp.270) |  | 0 | Knowledge Distillation (KD) is extensively used in Natural Language Processing to compress the pre-training and task-specific fine-tuning phases of large neural language models. A student model is trained to minimize a convex combination of the prediction loss over the labels and another over the... | Peng Lu, Abbas Ghaddar, Ahmad Rashid, Mehdi Rezagholizadeh, Ali Ghodsi, Philippe Langlais |  |
| 315 |  |  [Visual Cues and Error Correction for Translation Robustness](https://doi.org/10.18653/v1/2021.findings-emnlp.271) |  | 0 | Neural Machine Translation models are sensitive to noise in the input texts, such as misspelled words and ungrammatical constructions. Existing robustness techniques generally fail when faced with unseen types of noise and their performance degrades on clean texts. In this paper, we focus on three... | Zhenhao Li, Marek Rei, Lucia Specia |  |
| 316 |  |  [Beyond the Tip of the Iceberg: Assessing Coherence of Text Classifiers](https://doi.org/10.18653/v1/2021.findings-emnlp.272) |  | 0 | As large-scale, pre-trained language models achieve human-level and superhuman accuracy on existing language understanding tasks, statistical bias in benchmark data and probing studies have recently called into question their true capabilities. For a more informative evaluation than accuracy on... | Shane Storks, Joyce Chai |  |
| 317 |  |  [Does Pretraining for Summarization Require Knowledge Transfer?](https://doi.org/10.18653/v1/2021.findings-emnlp.273) |  | 0 | Pretraining techniques leveraging enormous datasets have driven recent advances in text summarization. While folk explanations suggest that knowledge transfer accounts for pretraining’s benefits, little is known about why it works or what makes a pretraining task or dataset suitable. In this paper,... | Kundan Krishna, Jeffrey P. Bigham, Zachary C. Lipton |  |
| 318 |  |  [Bandits Don't Follow Rules: Balancing Multi-Facet Machine Translation with Multi-Armed Bandits](https://doi.org/10.18653/v1/2021.findings-emnlp.274) |  | 0 | Training data for machine translation (MT) is often sourced from a multitude of large corpora that are multi-faceted in nature, e.g. containing contents from multiple domains or different levels of quality or complexity. Naturally, these facets do not occur with equal frequency, nor are they... | Julia Kreutzer, David Vilar, Artem Sokolov |  |
| 319 |  |  [Sometimes We Want Ungrammatical Translations](https://doi.org/10.18653/v1/2021.findings-emnlp.275) |  | 0 | Rapid progress in Neural Machine Translation (NMT) systems over the last few years has focused primarily on improving translation quality, and as a secondary focus, improving robustness to perturbations (e.g. spelling). While performance and robustness are important objectives, by over-focusing on... | Prasanna Parthasarathi, Koustuv Sinha, Joelle Pineau, Adina Williams |  |
| 320 |  |  [An animated picture says at least a thousand words: Selecting Gif-based Replies in Multimodal Dialog](https://doi.org/10.18653/v1/2021.findings-emnlp.276) |  | 0 | Online conversations include more than just text. Increasingly, image-based responses such as memes and animated gifs serve as culturally recognized and often humorous responses in conversation. However, while NLP has broadened to multimodal models, conversational dialog systems have largely... | Xingyao Wang, David Jurgens |  |
| 321 |  |  [SciCap: Generating Captions for Scientific Figures](https://doi.org/10.18653/v1/2021.findings-emnlp.277) |  | 0 | Researchers use figures to communicate rich, complex information in scientific papers. The captions of these figures are critical to conveying effective messages. However, low-quality figure captions commonly occur in scientific articles and may decrease understanding. In this paper, we propose an... | TingYao Hsu, C. Lee Giles, TingHao Kenneth Huang |  |
| 322 |  |  [SentNoB: A Dataset for Analysing Sentiment on Noisy Bangla Texts](https://doi.org/10.18653/v1/2021.findings-emnlp.278) |  | 0 | In this paper, we propose an annotated sentiment analysis dataset made of informally written Bangla texts. This dataset comprises public comments on news and videos collected from social media covering 13 different domains, including politics, education, and agriculture. These comments are labeled... | Khondoker Ittehadul Islam, Sudipta Kar, Md Saiful Islam, Mohammad Ruhul Amin |  |
| 323 |  |  [Translate & Fill: Improving Zero-Shot Multilingual Semantic Parsing with Synthetic Data](https://doi.org/10.18653/v1/2021.findings-emnlp.279) |  | 0 | While multilingual pretrained language models (LMs) fine-tuned on a single language have shown substantial cross-lingual task transfer capabilities, there is still a wide performance gap in semantic parsing tasks when target language supervision is available. In this paper, we propose a novel... | Massimo Nicosia, Zhongdi Qu, Yasemin Altun |  |
| 324 |  |  [NewsBERT: Distilling Pre-trained Language Model for Intelligent News Application](https://doi.org/10.18653/v1/2021.findings-emnlp.280) |  | 0 | Pre-trained language models (PLMs) like BERT have made great progress in NLP. News articles usually contain rich textual information, and PLMs have the potentials to enhance news text modeling for various intelligent news applications like news recommendation and retrieval. However, most existing... | Chuhan Wu, Fangzhao Wu, Yang Yu, Tao Qi, Yongfeng Huang, Qi Liu |  |
| 325 |  |  [SD-QA: Spoken Dialectal Question Answering for the Real World](https://doi.org/10.18653/v1/2021.findings-emnlp.281) |  | 0 | Question answering (QA) systems are now available through numerous commercial applications for a wide variety of domains, serving millions of users that interact with them via speech interfaces. However, current benchmarks in QA research do not account for the errors that speech recognition models... | Fahim Faisal, Sharlina Keshava, Md Mahfuz Ibn Alam, Antonios Anastasopoulos |  |
| 326 |  |  [The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.282) |  | 0 | A “bigger is better” explosion in the number of parameters in deep neural networks has made it increasingly challenging to make state-of-the-art networks accessible in compute-restricted environments. Compression techniques have taken on renewed importance as a way to bridge the gap. However,... | Orevaoghene Ahia, Julia Kreutzer, Sara Hooker |  |
| 327 |  |  [Transformer over Pre-trained Transformer for Neural Text Segmentation with Enhanced Topic Coherence](https://doi.org/10.18653/v1/2021.findings-emnlp.283) |  | 0 | This paper proposes a transformer over transformer framework, called Transformerˆ2, to perform neural text segmentation. It consists of two components: bottom-level sentence encoders using pre-trained transformers, and an upper-level transformer-based segmentation model based on the sentence... | Kelvin Lo, Yuan Jin, Weicong Tan, Ming Liu, Lan Du, Wray L. Buntine |  |
| 328 |  |  [Self-Supervised Neural Topic Modeling](https://doi.org/10.18653/v1/2021.findings-emnlp.284) |  | 0 | Topic models are useful tools for analyzing and interpreting the main underlying themes of large corpora of text. Most topic models rely on word co-occurrence for computing a topic, i.e., a weighted set of words that together represent a high-level semantic concept. In this paper, we propose a new... | Seyed Ali Bahrainian, Martin Jaggi, Carsten Eickhoff |  |
| 329 |  |  [Coreference-aware Surprisal Predicts Brain Response](https://doi.org/10.18653/v1/2021.findings-emnlp.285) |  | 0 | Recent evidence supports a role for coreference processing in guiding human expectations about upcoming words during reading, based on covariation between reading times and word surprisal estimated by a coreference-aware semantic processing model (Jaffe et al. 2020).The present study reproduces and... | Evan Jaffe, ByungDoh Oh, William Schuler |  |
| 330 |  |  [Distilling the Knowledge of Large-scale Generative Models into Retrieval Models for Efficient Open-domain Conversation](https://doi.org/10.18653/v1/2021.findings-emnlp.286) |  | 0 | Despite the remarkable performance of large-scale generative models in open-domain conversation, they are known to be less practical for building real-time conversation systems due to high latency. On the other hand, retrieval models could return responses with much lower latency but show inferior... | Beomsu Kim, Seokjun Seo, Seungju Han, Enkhbayar Erdenee, Buru Chang |  |
| 331 |  |  [Modeling Users and Online Communities for Abuse Detection: A Position on Ethics and Explainability](https://doi.org/10.18653/v1/2021.findings-emnlp.287) |  | 0 | Abuse on the Internet is an important societal problem of our time. Millions of Internet users face harassment, racism, personal attacks, and other types of abuse across various platforms. The psychological effects of abuse on individuals can be profound and lasting. Consequently, over the past few... | Pushkar Mishra, Helen Yannakoudakis, Ekaterina Shutova |  |
| 332 |  |  [Detecting Community Sensitive Norm Violations in Online Conversations](https://doi.org/10.18653/v1/2021.findings-emnlp.288) |  | 0 | Online platforms and communities establish their own norms that govern what behavior is acceptable within the community. Substantial effort in NLP has focused on identifying unacceptable behaviors and, recently, on forecasting them before they occur. However, these efforts have largely focused on... | Chan Young Park, Julia Mendelsohn, Karthik Radhakrishnan, Kinjal Jain, Tushar Kanakagiri, David Jurgens, Yulia Tsvetkov |  |
| 333 |  |  [SupCL-Seq: Supervised Contrastive Learning for Downstream Optimized Sequence Representations](https://doi.org/10.18653/v1/2021.findings-emnlp.289) |  | 0 | While contrastive learning is proven to be an effective training strategy in computer vision, Natural Language Processing (NLP) is only recently adopting it as a self-supervised alternative to Masked Language Modeling (MLM) for improving sequence representations. This paper introduces SupCL-Seq,... | Hooman Sedghamiz, Shivam Raval, Enrico Santus, Tuka Alhanai, Mohammad M. Ghassemi |  |
| 334 |  |  [mDAPT: Multilingual Domain Adaptive Pretraining in a Single Model](https://doi.org/10.18653/v1/2021.findings-emnlp.290) |  | 0 | Domain adaptive pretraining, i.e. the continued unsupervised pretraining of a language model on domain-specific text, improves the modelling of text for downstream tasks within the domain. Numerous real-world applications are based on domain-specific text, e.g. working with financial or biomedical... | Rasmus Kær Jørgensen, Mareike Hartmann, Xiang Dai, Desmond Elliott |  |
| 335 |  |  [COSMic: A Coherence-Aware Generation Metric for Image Descriptions](https://doi.org/10.18653/v1/2021.findings-emnlp.291) |  | 0 | Developers of text generation models rely on automated evaluation metrics as a stand-in for slow and expensive manual evaluations. However, image captioning metrics have struggled to give accurate learned estimates of the semantic and pragmatic success of output text. We address this weakness by... | Mert Inan, Piyush Sharma, Baber Khalid, Radu Soricut, Matthew Stone, Malihe Alikhani |  |
| 336 |  |  [Relation-Guided Pre-Training for Open-Domain Question Answering](https://doi.org/10.18653/v1/2021.findings-emnlp.292) |  | 0 | Answering complex open-domain questions requires understanding the latent relations between involving entities. However, we found that the existing QA datasets are extremely imbalanced in some types of relations, which hurts the generalization performance over questions with long-tail relations. To... | Ziniu Hu, Yizhou Sun, KaiWei Chang |  |
| 337 |  |  [MURAL: Multimodal, Multitask Representations Across Languages](https://doi.org/10.18653/v1/2021.findings-emnlp.293) |  | 0 | Both image-caption pairs and translation pairs provide the means to learn deep representations of and connections between languages. We use both types of pairs in MURAL (MUltimodal, MUltitask Representations Across Languages), a dual encoder that solves two tasks: 1) image-text matching and 2)... | Aashi Jain, Mandy Guo, Krishna Srinivasan, Ting Chen, Sneha Kudugunta, Chao Jia, Yinfei Yang, Jason Baldridge |  |
| 338 |  |  [AStitchInLanguageModels: Dataset and Methods for the Exploration of Idiomaticity in Pre-Trained Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.294) |  | 0 | Despite their success in a variety of NLP tasks, pre-trained language models, due to their heavy reliance on compositionality, fail in effectively capturing the meanings of multiword expressions (MWEs), especially idioms. Therefore, datasets and methods to improve the representation of MWEs are... | Harish Tayyar Madabushi, Edward GowSmith, Carolina Scarton, Aline Villavicencio |  |
| 339 |  |  [Refine and Imitate: Reducing Repetition and Inconsistency in Persuasion Dialogues via Reinforcement Learning and Human Demonstration](https://doi.org/10.18653/v1/2021.findings-emnlp.295) |  | 0 | Persuasion dialogue system reflects the machine’s ability to make strategic moves beyond verbal communication, and therefore differentiates itself from task-oriented or open-domain dialogues and has its own unique values. However, the repetition and inconsistency problems still persist in dialogue... | Weiyan Shi, Yu Li, Saurav Sahay, Zhou Yu |  |
| 340 |  |  [A Computational Exploration of Pejorative Language in Social Media](https://doi.org/10.18653/v1/2021.findings-emnlp.296) |  | 0 | In this paper we study pejorative language, an under-explored topic in computational linguistics. Unlike existing models of offensive language and hate speech, pejorative language manifests itself primarily at the lexical level, and describes a word that is used with a negative connotation, making... | Liviu P. Dinu, IoanBogdan Iordache, Ana Sabina Uban, Marcos Zampieri |  |
| 341 |  |  [Evidence-based Fact-Checking of Health-related Claims](https://doi.org/10.18653/v1/2021.findings-emnlp.297) |  | 0 | The task of verifying the truthfulness of claims in textual documents, or fact-checking, has received significant attention in recent years. Many existing evidence-based factchecking datasets contain synthetic claims and the models trained on these data might not be able to verify real-world... | Mourad Sarrouti, Asma Ben Abacha, Yassine Mrabet, Dina DemnerFushman |  |
| 342 |  |  [Learning and Analyzing Generation Order for Undirected Sequence Models](https://doi.org/10.18653/v1/2021.findings-emnlp.298) |  | 0 | Undirected neural sequence models have achieved performance competitive with the state-of-the-art directed sequence models that generate monotonically from left to right in machine translation tasks. In this work, we train a policy that learns the generation order for a pre-trained, undirected... | Yichen Jiang, Mohit Bansal |  |
| 343 |  |  [Automatic Bilingual Markup Transfer](https://doi.org/10.18653/v1/2021.findings-emnlp.299) |  | 0 | We describe the task of bilingual markup transfer, which involves placing markup tags from a source sentence into a fixed target translation. This task arises in practice when a human translator generates the target translation without markup, and then the system infers the placement of markup... | Thomas Zenkel, Joern Wuebker, John DeNero |  |
| 344 |  |  [Exploring a Unified Sequence-To-Sequence Transformer for Medical Product Safety Monitoring in Social Media](https://doi.org/10.18653/v1/2021.findings-emnlp.300) |  | 0 | Adverse Events (AE) are harmful events resulting from the use of medical products. Although social media may be crucial for early AE detection, the sheer scale of this data makes it logistically intractable to analyze using human agents, with NLP representing the only low-cost and scalable... | Shivam Raval, Hooman Sedghamiz, Enrico Santus, Tuka Alhanai, Mohammad M. Ghassemi, Emmanuele Chersoni |  |
| 345 |  |  [Disentangling Generative Factors in Natural Language with Discrete Variational Autoencoders](https://doi.org/10.18653/v1/2021.findings-emnlp.301) |  | 0 | The ability of learning disentangled representations represents a major step for interpretable NLP systems as it allows latent linguistic features to be controlled. Most approaches to disentanglement rely on continuous variables, both for images and text. We argue that despite being suitable for... | Giangiacomo Mercatali, André Freitas |  |
| 346 |  |  [MSD: Saliency-aware Knowledge Distillation for Multimodal Understanding](https://doi.org/10.18653/v1/2021.findings-emnlp.302) |  | 0 | To reduce a model size but retain performance, we often rely on knowledge distillation (KD) which transfers knowledge from a large “teacher” model to a smaller “student” model. However, KD on multimodal datasets such as vision-language tasks is relatively unexplored, and digesting multimodal... | Woojeong Jin, Maziar Sanjabi, Shaoliang Nie, Liang Tan, Xiang Ren, Hamed Firooz |  |
| 347 |  |  [Do UD Trees Match Mention Spans in Coreference Annotations?](https://doi.org/10.18653/v1/2021.findings-emnlp.303) |  | 0 | One can find dozens of data resources for various languages in which coreference - a relation between two or more expressions that refer to the same real-world entity - is manually annotated. One could also assume that such expressions usually constitute syntactically meaningful units; however,... | Martin Popel, Zdenek Zabokrtský, Anna Nedoluzhko, Michal Novák, Daniel Zeman |  |
| 348 |  |  [Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference](https://doi.org/10.18653/v1/2021.findings-emnlp.304) |  | 0 | Sparse Mixture-of-Experts (MoE) has been a successful approach for scaling multilingual translation models to billions of parameters without a proportional increase in training computation. However, MoE models are prohibitively large and practitioners often resort to methods such as distillation... | Sneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, MinhThang Luong, Orhan Firat |  |
| 349 |  |  [TAG: Gradient Attack on Transformer-based Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.305) |  | 0 | Although distributed learning has increasingly gained attention in terms of effectively utilizing local devices for data privacy enhancement, recent studies show that publicly shared gradients in the training process can reveal the private training data (gradient leakage) to a third-party. We have,... | Jieren Deng, Yijue Wang, Ji Li, Chenghong Wang, Chao Shang, Hang Liu, Sanguthevar Rajasekaran, Caiwen Ding |  |
| 350 |  |  [Generating Realistic Natural Language Counterfactuals](https://doi.org/10.18653/v1/2021.findings-emnlp.306) |  | 0 | Counterfactuals are a valuable means for understanding decisions made by ML systems. However, the counterfactuals generated by the methods currently available for natural language text are either unrealistic or introduce imperceptible changes. We propose CounterfactualGAN: a method that combines a... | Marcel Robeer, Floris Bex, Ad Feelders |  |
| 351 |  |  [Unsupervised Chunking as Syntactic Structure Induction with a Knowledge-Transfer Approach](https://doi.org/10.18653/v1/2021.findings-emnlp.307) |  | 0 | In this paper, we address unsupervised chunking as a new task of syntactic structure induction, which is helpful for understanding the linguistic structures of human languages as well as processing low-resource languages. We propose a knowledge-transfer approach that heuristically induces chunk... | Anup Anand Deshmukh, Qianqiu Zhang, Ming Li, Jimmy Lin, Lili Mou |  |
| 352 |  |  [Model-based analysis of brain activity reveals the hierarchy of language in 305 subjects](https://doi.org/10.18653/v1/2021.findings-emnlp.308) |  | 0 | A popular approach to decompose the neural bases of language consists in correlating, across individuals, the brain responses to different stimuli (e.g. regular speech versus scrambled words, sentences, or paragraphs). Although successful, this ‘model-free’ approach necessitates the acquisition of... | Charlotte Caucheteux, Alexandre Gramfort, JeanRemi King |  |
| 353 |  |  [Gated Transformer for Robust De-noised Sequence-to-Sequence Modelling](https://doi.org/10.18653/v1/2021.findings-emnlp.309) |  | 0 | Robust sequence-to-sequence modelling is an essential task in the real world where the inputs are often noisy. Both user-generated and machine generated inputs contain various kinds of noises in the form of spelling mistakes, grammatical errors, character recognition errors, all of which impact... | Ayan Sengupta, Amit Kumar, Sourabh Kumar Bhattacharjee, Suman Roy |  |
| 354 |  |  [Token-wise Curriculum Learning for Neural Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.310) |  | 0 | Existing curriculum learning approaches to Neural Machine Translation (NMT) require sampling sufficient amounts of “easy” samples from training data at the early training stage. This is not always achievable for low-resource languages where the amount of training data is limited. To address such a... | Chen Liang, Haoming Jiang, Xiaodong Liu, Pengcheng He, Weizhu Chen, Jianfeng Gao, Tuo Zhao |  |
| 355 |  |  [RelDiff: Enriching Knowledge Graph Relation Representations for Sensitivity Classification](https://doi.org/10.18653/v1/2021.findings-emnlp.311) |  | 0 | The relationships that exist between entities can be a reliable indicator for classifying sensitive information, such as commercially sensitive information. For example, the relation person-IsDirectorOf-company can indicate whether an individual’s salary should be considered as sensitive personal... | Hitarth Narvala, Graham McDonald, Iadh Ounis |  |
| 356 |  |  [Post-Editing Extractive Summaries by Definiteness Prediction](https://doi.org/10.18653/v1/2021.findings-emnlp.312) |  | 0 | Extractive summarization has been the mainstay of automatic summarization for decades. Despite all the progress, extractive summarizers still suffer from shortcomings including coreference issues arising from extracting sentences away from their original context in the source document. This affects... | Jad Kabbara, Jackie Chi Kit Cheung |  |
| 357 |  |  [Leveraging Pretrained Models for Automatic Summarization of Doctor-Patient Conversations](https://doi.org/10.18653/v1/2021.findings-emnlp.313) |  | 0 | Fine-tuning pretrained models for automatically summarizing doctor-patient conversation transcripts presents many challenges: limited training data, significant domain shift, long and noisy transcripts, and high target summary variability. In this paper, we explore the feasibility of using... | Longxiang Zhang, Renato Negrinho, Arindam Ghosh, Vasudevan Jagannathan, Hamid Reza Hassanzadeh, Thomas Schaaf, Matthew R. Gormley |  |
| 358 |  |  [Distilling Knowledge for Empathy Detection](https://doi.org/10.18653/v1/2021.findings-emnlp.314) |  | 0 | Empathy is the link between self and others. Detecting and understanding empathy is a key element for improving human-machine interaction. However, annotating data for detecting empathy at a large scale is a challenging task. This paper employs multi-task training with knowledge distillation to... | Mahshid Hosseini, Cornelia Caragea |  |
| 359 |  |  [Adapting Entities across Languages and Cultures](https://doi.org/10.18653/v1/2021.findings-emnlp.315) |  | 0 | How would you explain Bill Gates to a German? He is associated with founding a company in the United States, so perhaps the German founder Carl Benz could stand in for Gates in those contexts. This type of translation is called adaptation in the translation community. Until now, this task has not... | Denis Peskov, Viktor Hangya, Jordan L. BoydGraber, Alexander Fraser |  |
| 360 |  |  [ODIST: Open World Classification via Distributionally Shifted Instances](https://doi.org/10.18653/v1/2021.findings-emnlp.316) |  | 0 | In this work, we address the open-world classification problem with a method called ODIST, open world classification via distributionally shifted instances. This novel and straightforward method can create out-of-domain instances from the in-domain training instances with the help of a pre-trained... | Lei Shu, Yassine Benajiba, Saab Mansour, Yi Zhang |  |
| 361 |  |  [LAMAD: A Linguistic Attentional Model for Arabic Text Diacritization](https://doi.org/10.18653/v1/2021.findings-emnlp.317) |  | 0 | In Arabic Language, diacritics are used to specify meanings as well as pronunciations. However, diacritics are often omitted from written texts, which increases the number of possible meanings and pronunciations. This leads to an ambiguous text and makes the computational process on undiacritized... | Raeed AlSabri, Jianliang Gao |  |
| 362 |  |  [Sequence-to-Lattice Models for Fast Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.318) |  | 0 | Non-autoregressive machine translation (NAT) approaches enable fast generation by utilizing parallelizable generative processes. The remaining bottleneck in these models is their decoder layers; unfortunately unlike in autoregressive models (Kasai et al., 2020), removing decoder layers from NAT... | Yuntian Deng, Alexander M. Rush |  |
| 363 |  |  [Towards Realistic Single-Task Continuous Learning Research for NER](https://doi.org/10.18653/v1/2021.findings-emnlp.319) |  | 0 | There is an increasing interest in continuous learning (CL), as data privacy is becoming a priority for real-world machine learning applications. Meanwhile, there is still a lack of academic NLP benchmarks that are applicable for realistic CL settings, which is a major challenge for the advancement... | Justin Payan, Yuval Merhav, He Xie, Satyapriya Krishna, Anil Ramakrishna, Mukund Sridhar, Rahul Gupta |  |
| 364 |  |  [Retrieval Augmentation Reduces Hallucination in Conversation](https://doi.org/10.18653/v1/2021.findings-emnlp.320) |  | 0 | Despite showing increasingly human-like conversational abilities, state-of-the-art dialogue models often suffer from factual incorrectness and hallucination of knowledge (Roller et al., 2020). In this work we explore the use of neural-retrieval-in-the-loop architectures - recently shown to be... | Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, Jason Weston |  |
| 365 |  |  [Towards Automatic Bias Detection in Knowledge Graphs](https://doi.org/10.18653/v1/2021.findings-emnlp.321) |  | 0 | With the recent surge in social applications relying on knowledge graphs, the need for techniques to ensure fairness in KG based methods is becoming increasingly evident. Previous works have demonstrated that KGs are prone to various social biases, and have proposed multiple methods for debiasing... | Daphna Keidar, Mian Zhong, Ce Zhang, Yash Raj Shrestha, Bibek Paudel |  |
| 366 |  |  [Searching for More Efficient Dynamic Programs](https://doi.org/10.18653/v1/2021.findings-emnlp.322) |  | 0 | Computational models of human language often involve combinatorial problems. For instance, a probabilistic parser may marginalize over exponentially many trees to make predictions. Algorithms for such problems often employ dynamic programming and are not always unique. Finding one with optimal... | Tim Vieira, Ryan Cotterell, Jason Eisner |  |
| 367 |  |  [Revisiting Robust Neural Machine Translation: A Transformer Case Study](https://doi.org/10.18653/v1/2021.findings-emnlp.323) |  | 0 | Transformers have brought a remarkable improvement in the performance of neural machine translation (NMT) systems but they could be surprisingly vulnerable to noise. In this work, we try to investigate how noise breaks Transformers and if there exist solutions to deal with such issues. There is a... | Peyman Passban, Puneeth S. M. Saladi, Qun Liu |  |
| 368 |  |  [Can NLI Models Verify QA Systems' Predictions?](https://doi.org/10.18653/v1/2021.findings-emnlp.324) |  | 0 | To build robust question answering systems, we need the ability to verify whether answers to questions are truly correct, not just “good enough” in the context of imperfect QA datasets. We explore the use of natural language inference (NLI) as a way to achieve this goal, as NLI inherently requires... | Jifan Chen, Eunsol Choi, Greg Durrett |  |
| 369 |  |  [Parameter-Efficient Domain Knowledge Integration from Multiple Sources for Biomedical Pre-trained Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.325) |  | 0 | Domain-specific pre-trained language models (PLMs) have achieved great success over various downstream tasks in different domains. However, existing domain-specific PLMs mostly rely on self-supervised learning over large amounts of domain text, without explicitly integrating domain-specific... | Qiuhao Lu, Dejing Dou, Thien Huu Nguyen |  |
| 370 |  |  [Uncovering Implicit Gender Bias in Narratives through Commonsense Inference](https://doi.org/10.18653/v1/2021.findings-emnlp.326) |  | 0 | Pre-trained language models learn socially harmful biases from their training corpora, and may repeat these biases when used for generation. We study gender biases associated with the protagonist in model-generated stories. Such biases may be expressed either explicitly (“women can’t park”) or... | Tenghao Huang, Faeze Brahman, Vered Shwartz, Snigdha Chaturvedi |  |
| 371 |  |  [Contrastive Document Representation Learning with Graph Attention Networks](https://doi.org/10.18653/v1/2021.findings-emnlp.327) |  | 0 | Recent progress in pretrained Transformer-based language models has shown great success in learning contextual representation of text. However, due to the quadratic self-attention complexity, most of the pretrained Transformers models can only handle relatively short text. It is still a challenge... | Peng Xu, Xinchi Chen, Xiaofei Ma, Zhiheng Huang, Bing Xiang |  |
| 372 |  |  [Convex Aggregation for Opinion Summarization](https://doi.org/10.18653/v1/2021.findings-emnlp.328) |  | 0 | Recent advances in text autoencoders have significantly improved the quality of the latent space, which enables models to generate grammatical and consistent text from aggregated latent vectors. As a successful application of this property, unsupervised opinion summarization models generate a... | Hayate Iso, Xiaolan Wang, Yoshihiko Suhara, Stefanos Angelidis, WangChiew Tan |  |
| 373 |  |  [Using Optimal Transport as Alignment Objective for fine-tuning Multilingual Contextualized Embeddings](https://doi.org/10.18653/v1/2021.findings-emnlp.329) |  | 0 | Recent studies have proposed different methods to improve multilingual word representations in contextualized settings including techniques that align between source and target embedding spaces. For contextualized embeddings, alignment becomes more complex as we additionally take context into... | Sawsan Alqahtani, Garima Lalwani, Yi Zhang, Salvatore Romeo, Saab Mansour |  |
| 374 |  |  [Uncertainty-Aware Machine Translation Evaluation](https://doi.org/10.18653/v1/2021.findings-emnlp.330) |  | 0 | Several neural-based metrics have been recently proposed to evaluate machine translation quality. However, all of them resort to point estimates, which provide limited information at segment level. This is made worse as they are trained on noisy, biased and scarce human judgements, often resulting... | Taisiya Glushkova, Chrysoula Zerva, Ricardo Rei, André F. T. Martins |  |
| 375 |  |  [Neural Unification for Logic Reasoning over Natural Language](https://doi.org/10.18653/v1/2021.findings-emnlp.331) |  | 0 | Automated Theorem Proving (ATP) deals with the development of computer programs being able to show that some conjectures (queries) are a logical consequence of a set of axioms (facts and rules). There exists several successful ATPs where conjectures and axioms are formally provided (e.g. formalised... | Gabriele Picco, Thanh Lam Hoang, Marco Luca Sbodio, Vanessa López |  |
| 376 |  |  [From None to Severe: Predicting Severity in Movie Scripts](https://doi.org/10.18653/v1/2021.findings-emnlp.332) |  | 0 | In this paper, we introduce the task of predicting severity of age-restricted aspects of movie content based solely on the dialogue script. We first investigate categorizing the ordinal severity of movies on 5 aspects: Sex, Violence, Profanity, Substance consumption, and Frightening scenes. The... | Yigeng Zhang, Mahsa Shafaei, Fabio A. González, Thamar Solorio |  |
| 377 |  |  [Benchmarking Meta-embeddings: What Works and What Does Not](https://doi.org/10.18653/v1/2021.findings-emnlp.333) |  | 0 | In the last few years, several methods have been proposed to build meta-embeddings. The general aim was to obtain new representations integrating complementary knowledge from different source pre-trained embeddings thereby improving their overall quality. However, previous meta-embeddings have been... | Iker GarcíaFerrero, Rodrigo Agerri, German Rigau |  |
| 378 |  |  [A Plug-and-Play Method for Controlled Text Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.334) |  | 0 | Large pre-trained language models have repeatedly shown their ability to produce fluent text. Yet even when starting from a prompt, generation can continue in many plausible directions. Current decoding methods with the goal of controlling generation, e.g., to ensure specific words are included,... | Damian Pascual, Beni Egressy, Clara Meister, Ryan Cotterell, Roger Wattenhofer |  |
| 379 |  |  [A Corpus-based Syntactic Analysis of Two-termed Unlike Coordination](https://doi.org/10.18653/v1/2021.findings-emnlp.335) |  | 0 | Coordination is a phenomenon of language that conjoins two or more terms or phrases using a coordinating conjunction. Although coordination has been explored extensively in the linguistics literature, the rules and constraints that govern its structure are still largely elusive and widely debated... | Julie Kallini, Christiane Fellbaum |  |
| 380 |  |  [Weakly Supervised Contrastive Learning for Chest X-Ray Report Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.336) |  | 0 | Radiology report generation aims at generating descriptive text from radiology images automatically, which may present an opportunity to improve radiology reporting and interpretation. A typical setting consists of training encoder-decoder models on image-report pairs with a cross entropy loss,... | An Yan, Zexue He, Xing Lu, Jiang Du, Eric Y. Chang, Amilcare Gentili, Julian J. McAuley, ChunNan Hsu |  |
| 381 |  |  [NUANCED: Natural Utterance Annotation for Nuanced Conversation with Estimated Distributions](https://doi.org/10.18653/v1/2021.findings-emnlp.337) |  | 0 | Existing conversational systems are mostly agent-centric, which assumes the user utterances will closely follow the system ontology. However, in real-world scenarios, it is highly desirable that users can speak freely and naturally. In this work, we attempt to build a user-centric dialogue system... | Zhiyu Chen, Honglei Liu, Hu Xu, Seungwhan Moon, Hao Zhou, Bing Liu |  |
| 382 |  |  [Table-based Fact Verification With Salience-aware Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.338) |  | 0 | Tables provide valuable knowledge that can be used to verify textual statements. While a number of works have considered table-based fact verification, direct alignments of tabular data with tokens in textual statements are rarely available. Moreover, training a generalized fact verification model... | Fei Wang, Kexuan Sun, Jay Pujara, Pedro A. Szekely, Muhao Chen |  |
| 383 |  |  [Detecting Frames in News Headlines and Lead Images in U.S. Gun Violence Coverage](https://doi.org/10.18653/v1/2021.findings-emnlp.339) |  | 0 | News media structure their reporting of events or issues using certain perspectives. When describing an incident involving gun violence, for example, some journalists may focus on mental health or gun regulation, while others may emphasize the discussion of gun rights. Such perspectives are called... | Isidora Chara Tourni, Lei Guo, Taufiq Husada Daryanto, Fabian Zhafransyah, Edward Edberg Halim, Mona Jalal, Boqi Chen, Sha Lai, Hengchang Hu, Margrit Betke, Prakash Ishwar, Derry Tanti Wijaya |  |
| 384 |  |  [Multi-task Learning to Enable Location Mention Identification in the Early Hours of a Crisis Event](https://doi.org/10.18653/v1/2021.findings-emnlp.340) |  | 0 | Training a robust and reliable deep learning model requires a large amount of data. In the crisis domain, building deep learning models to identify actionable information from the huge influx of data posted by eyewitnesses of crisis events on social media, in a time-critical manner, is central for... | Sarthak Khanal, Doina Caragea |  |
| 385 |  |  [Graph-Based Decoding for Task Oriented Semantic Parsing](https://doi.org/10.18653/v1/2021.findings-emnlp.341) |  | 0 | The dominant paradigm for semantic parsing in recent years is to formulate parsing as a sequence-to-sequence task, generating predictions with auto-regressive sequence decoders. In this work, we explore an alternative paradigm. We formulate semantic parsing as a dependency parsing task, applying... | Jeremy R. Cole, Nanjiang Jiang, Panupong Pasupat, Luheng He, Peter Shaw |  |
| 386 |  |  [Expected Validation Performance and Estimation of a Random Variable's Maximum](https://doi.org/10.18653/v1/2021.findings-emnlp.342) |  | 0 | Research in NLP is often supported by experimental results, and improved reporting of such results can lead to better understanding and more reproducible science. In this paper we analyze three statistical estimators for expected validation performance, a tool used for reporting performance (e.g.,... | Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, Noah A. Smith |  |
| 387 |  |  [How May I Help You? Using Neural Text Simplification to Improve Downstream NLP Tasks](https://doi.org/10.18653/v1/2021.findings-emnlp.343) |  | 0 | The general goal of text simplification (TS) is to reduce text complexity for human consumption. In this paper, we investigate another potential use of neural TS: assisting machines performing natural language processing (NLP) tasks. We evaluate the use of neural TS in two ways: simplifying input... | Hoang Van, Zheng Tang, Mihai Surdeanu |  |
| 388 |  |  [Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers](https://doi.org/10.18653/v1/2021.findings-emnlp.344) |  | 0 | Transformers have shown improved performance when compared to previous architectures for sequence processing such as RNNs. Despite their sizeable performance gains, as recently suggested, the model is computationally expensive to train and with a high parameter budget. In light of this, we explore... | Machel Reid, Edison MarreseTaylor, Yutaka Matsuo |  |
| 389 |  |  [Leveraging Information Bottleneck for Scientific Document Summarization](https://doi.org/10.18653/v1/2021.findings-emnlp.345) |  | 0 | This paper presents an unsupervised extractive approach to summarize scientific long documents based on the Information Bottleneck principle. Inspired by previous work which uses the Information Bottleneck principle for sentence compression, we extend it to document level summarization with two... | Jiaxin Ju, Ming Liu, Huan Yee Koh, Yuan Jin, Lan Du, Shirui Pan |  |
| 390 |  |  [Reconsidering the Past: Optimizing Hidden States in Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.346) |  | 0 | We present Hidden-State Optimization (HSO), a gradient-based method for improving the performance of transformer language models at inference time. Similar to dynamic evaluation (Krause et al., 2018), HSO computes the gradient of the log-probability the language model assigns to an evaluation text,... | Davis Yoshida, Kevin Gimpel |  |
| 391 |  |  [Attend, Memorize and Generate: Towards Faithful Table-to-Text Generation in Few Shots](https://doi.org/10.18653/v1/2021.findings-emnlp.347) |  | 0 | Few-shot table-to-text generation is a task of composing fluent and faithful sentences to convey table content using limited data. Despite many efforts having been made towards generating impressive fluent sentences by fine-tuning powerful pre-trained language models, the faithfulness of generated... | Wenting Zhao, Ye Liu, Yao Wan, Philip S. Yu |  |
| 392 |  |  [ARCH: Efficient Adversarial Regularized Training with Caching](https://doi.org/10.18653/v1/2021.findings-emnlp.348) |  | 0 | Adversarial regularization can improve model generalization in many natural language processing tasks. However, conventional approaches are computationally expensive since they need to generate a perturbation for each sample in each epoch. We propose a new adversarial regularization method ARCH... | Simiao Zuo, Chen Liang, Haoming Jiang, Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, Tuo Zhao |  |
| 393 |  |  [Probing Commonsense Explanation in Dialogue Response Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.349) |  | 0 | Humans use commonsense reasoning (CSR) implicitly to produce natural and coherent responses in conversations. Aiming to close the gap between current response generation (RG) models and human communication abilities, we want to understand why RG models respond as they do by probing RG model’s... | Pei Zhou, Pegah Jandaghi, Hyundong Cho, Bill Yuchen Lin, Jay Pujara, Xiang Ren |  |
| 394 |  |  [NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset](https://doi.org/10.18653/v1/2021.findings-emnlp.350) |  | 0 | While diverse question answering (QA) datasets have been proposed and contributed significantly to the development of deep learning models for QA tasks, the existing datasets fall short in two aspects. First, we lack QA datasets covering complex questions that involve answers as well as the... | Qiyuan Zhang, Lei Wang, Sicheng Yu, Shuohang Wang, Yang Wang, Jing Jiang, EePeng Lim |  |
| 395 |  |  [Textual Time Travel: A Temporally Informed Approach to Theory of Mind](https://doi.org/10.18653/v1/2021.findings-emnlp.351) |  | 0 | Natural language processing systems such as dialogue agents should be able to reason about other people’s beliefs, intentions and desires. This capability, called theory of mind (ToM), is crucial, as it allows a model to predict and interpret the needs of users based on their mental states. A... | Akshatha Arodi, Jackie Chi Kit Cheung |  |
| 396 |  |  [Detect and Perturb: Neutral Rewriting of Biased and Sensitive Text via Gradient-based Decoding](https://doi.org/10.18653/v1/2021.findings-emnlp.352) |  | 0 | Written language carries explicit and implicit biases that can distract from meaningful signals. For example, letters of reference may describe male and female candidates differently, or their writing style may indirectly reveal demographic characteristics. At best, such biases distract from the... | Zexue He, Bodhisattwa Prasad Majumder, Julian J. McAuley |  |
| 397 |  |  [HyperExpan: Taxonomy Expansion with Hyperbolic Representation Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.353) |  | 0 | Taxonomies are valuable resources for many applications, but the limited coverage due to the expensive manual curation process hinders their general applicability. Prior works attempt to automatically expand existing taxonomies to improve their coverage by learning concept embeddings in Euclidean... | Mingyu Derek Ma, Muhao Chen, TeLin Wu, Nanyun Peng |  |
| 398 |  |  [Want To Reduce Labeling Cost? GPT-3 Can Help](https://doi.org/10.18653/v1/2021.findings-emnlp.354) |  | 0 | Data annotation is a time-consuming and labor-intensive process for many NLP tasks. Although there exist various methods to produce pseudo data labels, they are often task-specific and require a decent amount of labeled data to start with. Recently, the immense language model GPT-3 with 170 billion... | Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, Michael Zeng |  |
| 399 |  |  [Written Justifications are Key to Aggregate Crowdsourced Forecasts](https://doi.org/10.18653/v1/2021.findings-emnlp.355) |  | 0 | This paper demonstrates that aggregating crowdsourced forecasts benefits from modeling the written justifications provided by forecasters. Our experiments show that the majority and weighted vote baselines are competitive, and that the written justifications are beneficial to call a question... | Saketh Kotamraju, Eduardo Blanco |  |
| 400 |  |  [Cleaning Dirty Books: Post-OCR Processing for Previously Scanned Texts](https://doi.org/10.18653/v1/2021.findings-emnlp.356) |  | 0 | Substantial amounts of work are required to clean large collections of digitized books for NLP analysis, both because of the presence of errors in the scanned text and the presence of duplicate volumes in the corpora. In this paper, we consider the issue of deduplication in the presence of optical... | Allen Kim, Charuta Pethe, Naoya Inoue, Steven Skiena |  |
| 401 |  |  [Bag of Tricks for Optimizing Transformer Efficiency](https://doi.org/10.18653/v1/2021.findings-emnlp.357) |  | 0 | Improving Transformer efficiency has become increasingly attractive recently. A wide range of methods has been proposed, e.g., pruning, quantization, new architectures and etc. But these methods are either sophisticated in implementation or dependent on hardware. In this paper, we show that the... | Ye Lin, Yanyang Li, Tong Xiao, Jingbo Zhu |  |
| 402 |  |  [Non-Parametric Unsupervised Domain Adaptation for Neural Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.358) |  | 0 | Recently, kNN-MT (Khandelwal et al., 2020) has shown the promising capability of directly incorporating the pre-trained neural machine translation (NMT) model with domain-specific token-level k-nearest-neighbor (kNN) retrieval to achieve domain adaptation without retraining. Despite being... | Xin Zheng, Zhirui Zhang, Shujian Huang, Boxing Chen, Jun Xie, Weihua Luo, Jiajun Chen |  |
| 403 |  |  [The Topic Confusion Task: A Novel Evaluation Scenario for Authorship Attribution](https://doi.org/10.18653/v1/2021.findings-emnlp.359) |  | 0 | Authorship attribution is the problem of identifying the most plausible author of an anonymous text from a set of candidate authors. Researchers have investigated same-topic and cross-topic scenarios of authorship attribution, which differ according to whether new, unseen topics are used in the... | Malik H. Altakrori, Jackie Chi Kit Cheung, Benjamin C. M. Fung |  |
| 404 |  |  [Micromodels for Efficient, Explainable, and Reusable Systems: A Case Study on Mental Health](https://doi.org/10.18653/v1/2021.findings-emnlp.360) |  | 0 | Many statistical models have high accuracy on test benchmarks, but are not explainable, struggle in low-resource scenarios, cannot be reused for multiple tasks, and cannot easily integrate domain expertise. These factors limit their use, particularly in settings such as mental health, where it is... | Andrew Lee, Jonathan K. Kummerfeld, Larry An, Rada Mihalcea |  |
| 405 |  |  [Discovering Explanatory Sentences in Legal Case Decisions Using Pre-trained Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.361) |  | 0 | Legal texts routinely use concepts that are difficult to understand. Lawyers elaborate on the meaning of such concepts by, among other things, carefully investigating how they have been used in the past. Finding text snippets that mention a particular concept in a useful way is tedious,... | Jaromír Savelka, Kevin D. Ashley |  |
| 406 |  |  [FCM: A Fine-grained Comparison Model for Multi-turn Dialogue Reasoning](https://doi.org/10.18653/v1/2021.findings-emnlp.362) |  | 0 | Despite the success of neural dialogue systems in achieving high performance on the leader-board, they cannot meet users’ requirements in practice, due to their poor reasoning skills. The underlying reason is that most neural dialogue models only capture the syntactic and semantic information, but... | Xu Wang, Hainan Zhang, Shuai Zhao, Yanyan Zou, Hongshen Chen, Zhuoye Ding, Bo Cheng, Yanyan Lan |  |
| 407 |  |  [Reference-based Weak Supervision for Answer Sentence Selection using Web Data](https://doi.org/10.18653/v1/2021.findings-emnlp.363) |  | 0 | Answer Sentence Selection (AS2) models are core components of efficient retrieval-based Question Answering (QA) systems. We present the Reference-based Weak Supervision (RWS), a fully automatic large-scale data pipeline that harvests high-quality weakly- supervised answer sentences from Web data,... | Vivek Krishnamurthy, Thuy Vu, Alessandro Moschitti |  |
| 408 |  |  [A Deep Decomposable Model for Disentangling Syntax and Semantics in Sentence Representation](https://doi.org/10.18653/v1/2021.findings-emnlp.364) |  | 0 | Recently, disentanglement based on a generative adversarial network or a variational autoencoder has significantly advanced the performance of diverse applications in CV and NLP domains. Nevertheless, those models still work on coarse levels in the disentanglement of closely related properties,... | Dingcheng Li, Hongliang Fei, Shaogang Ren, Ping Li |  |
| 409 |  |  [Improved Word Sense Disambiguation with Enhanced Sense Representations](https://doi.org/10.18653/v1/2021.findings-emnlp.365) |  | 0 | Current state-of-the-art supervised word sense disambiguation (WSD) systems (such as GlossBERT and bi-encoder model) yield surprisingly good results by purely leveraging pre-trained language models and short dictionary definitions (or glosses) of the different word senses. While concise and... | Yang Song, Xin Cai Ong, Hwee Tou Ng, Qian Lin |  |
| 410 |  |  [Rethinking Zero-shot Neural Machine Translation: From a Perspective of Latent Variables](https://doi.org/10.18653/v1/2021.findings-emnlp.366) |  | 0 | Zero-shot translation, directly translating between language pairs unseen in training, is a promising capability of multilingual neural machine translation (NMT). However, it usually suffers from capturing spurious correlations between the output language and language invariant semantics due to the... | Weizhi Wang, Zhirui Zhang, Yichao Du, Boxing Chen, Jun Xie, Weihua Luo |  |
| 411 |  |  [FastCorrect 2: Fast Error Correction on Multiple Candidates for Automatic Speech Recognition](https://doi.org/10.18653/v1/2021.findings-emnlp.367) |  | 0 | Error correction is widely used in automatic speech recognition (ASR) to post-process the generated sentence, and can further reduce the word error rate (WER). Although multiple candidates are generated by an ASR system through beam search, current error correction approaches can only correct one... | Yichong Leng, Xu Tan, Rui Wang, Linchen Zhu, Jin Xu, Wenjie Liu, Linquan Liu, XiangYang Li, Tao Qin, Edward Lin, TieYan Liu |  |
| 412 |  |  [Task-Oriented Clustering for Dialogues](https://doi.org/10.18653/v1/2021.findings-emnlp.368) |  | 0 | A reliable clustering algorithm for task-oriented dialogues can help developer analysis and define dialogue tasks efficiently. It is challenging to directly apply prior normal text clustering algorithms for task-oriented dialogues, due to the inherent differences between them, such as coreference,... | Chenxu Lv, Hengtong Lu, Shuyu Lei, Huixing Jiang, Wei Wu, Caixia Yuan, Xiaojie Wang |  |
| 413 |  |  [Mitigating Data Poisoning in Text Classification with Differential Privacy](https://doi.org/10.18653/v1/2021.findings-emnlp.369) |  | 0 | NLP models are vulnerable to data poisoning attacks. One type of attack can plant a backdoor in a model by injecting poisoned examples in training, causing the victim model to misclassify test instances which include a specific pattern. Although defences exist to counter these attacks, they are... | Chang Xu, Jun Wang, Francisco Guzmán, Benjamin I. P. Rubinstein, Trevor Cohn |  |
| 414 |  |  [Does Vision-and-Language Pretraining Improve Lexical Grounding?](https://doi.org/10.18653/v1/2021.findings-emnlp.370) |  | 0 | Linguistic representations derived from text alone have been criticized for their lack of grounding, i.e., connecting words to their meanings in the physical world. Vision-and- Language (VL) models, trained jointly on text and image or video data, have been offered as a response to such criticisms.... | Tian Yun, Chen Sun, Ellie Pavlick |  |
| 415 |  |  [Character-based PCFG Induction for Modeling the Syntactic Acquisition of Morphologically Rich Languages](https://doi.org/10.18653/v1/2021.findings-emnlp.371) |  | 0 | Unsupervised PCFG induction models, which build syntactic structures from raw text, can be used to evaluate the extent to which syntactic knowledge can be acquired from distributional information alone. However, many state-of-the-art PCFG induction models are word-based, meaning that they cannot... | Lifeng Jin, ByungDoh Oh, William Schuler |  |
| 416 |  |  [Block-wise Word Embedding Compression Revisited: Better Weighting and Structuring](https://doi.org/10.18653/v1/2021.findings-emnlp.372) |  | 0 | Word embedding is essential for neural network models for various natural language processing tasks. Since the word embedding usually has a considerable size, in order to deploy a neural network model having it on edge devices, it should be effectively compressed. There was a study for proposing a... | JongRyul Lee, YongJu Lee, YongHyuk Moon |  |
| 417 |  |  [Switch Point biased Self-Training: Re-purposing Pretrained Models for Code-Switching](https://doi.org/10.18653/v1/2021.findings-emnlp.373) |  | 0 | Code-switching (CS), a ubiquitous phenomenon due to the ease of communication it offers in multilingual communities still remains an understudied problem in language processing. The primary reasons behind this are: (1) minimal efforts in leveraging large pretrained multilingual models, and (2) the... | Parul Chopra, Sai Krishna Rallabandi, Alan W. Black, Khyathi Raghavi Chandu |  |
| 418 |  |  [Influence Tuning: Demoting Spurious Correlations via Instance Attribution and Instance-Driven Updates](https://doi.org/10.18653/v1/2021.findings-emnlp.374) |  | 0 | Among the most critical limitations of deep learning NLP models are their lack of interpretability, and their reliance on spurious correlations. Prior work proposed various approaches to interpreting the black-box models to unveil the spurious correlations, but the research was primarily used in... | Xiaochuang Han, Yulia Tsvetkov |  |
| 419 |  |  [Learning Task Sampling Policy for Multitask Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.375) |  | 0 | It has been shown that training multi-task models with auxiliary tasks can improve the target task quality through cross-task transfer. However, the importance of each auxiliary task to the primary task is likely not known a priori. While the importance weights of auxiliary tasks can be manually... | Dhanasekar Sundararaman, Henry Tsai, KuangHuei Lee, Iulia Turc, Lawrence Carin |  |
| 420 |  |  [Competing Independent Modules for Knowledge Integration and Optimization](https://doi.org/10.18653/v1/2021.findings-emnlp.376) |  | 0 | This paper presents a neural framework of untied independent modules, used here for integrating off the shelf knowledge sources such as language models, lexica, POS information, and dependency relations. Each knowledge source is implemented as an independent component that can interact and share... | Parsa Bagherzadeh, Sabine Bergler |  |
| 421 |  |  [An Exploratory Study on Long Dialogue Summarization: What Works and What's Next](https://doi.org/10.18653/v1/2021.findings-emnlp.377) |  | 0 | Dialogue summarization helps readers capture salient information from long conversations in meetings, interviews, and TV series. However, real-world dialogues pose a great challenge to current summarization models, as the dialogue length typically exceeds the input limits imposed by recent... | Yusen Zhang, Ansong Ni, Tao Yu, Rui Zhang, Chenguang Zhu, Budhaditya Deb, Asli Celikyilmaz, Ahmed Hassan Awadallah, Dragomir R. Radev |  |
| 422 |  |  [Improving Text Auto-Completion with Next Phrase Prediction](https://doi.org/10.18653/v1/2021.findings-emnlp.378) |  | 0 | Language models such as GPT-2 have performed well on constructing syntactically sound sentences for text auto-completion tasks. However, such models often require considerable training effort to adapt to specific writing domains (e.g., medical). In this paper, we propose an intermediate training... | DongHo Lee, Zhiqiang Hu, Roy KaWei Lee |  |
| 423 |  |  [MOMENTA: A Multimodal Framework for Detecting Harmful Memes and Their Targets](https://doi.org/10.18653/v1/2021.findings-emnlp.379) |  | 0 | Internet memes have become powerful means to transmit political, psychological, and socio-cultural ideas. Although memes are typically humorous, recent days have witnessed an escalation of harmful memes used for trolling, cyberbullying, and abuse. Detecting such memes is challenging as they can be... | Shraman Pramanick, Shivam Sharma, Dimitar Dimitrov, Md. Shad Akhtar, Preslav Nakov, Tanmoy Chakraborty |  |
| 424 |  |  [NICE: Neural Image Commenting with Empathy](https://doi.org/10.18653/v1/2021.findings-emnlp.380) |  | 0 | Emotion and empathy are examples of human qualities lacking in many human-machine interactions. The goal of our work is to generate engaging dialogue grounded in a user-shared image with increased emotion and empathy while minimizing socially inappropriate or offensive outputs. We release the... | Kezhen Chen, Qiuyuan Huang, Daniel McDuff, Xiang Gao, Hamid Palangi, Jianfeng Wang, Kenneth D. Forbus, Jianfeng Gao |  |
| 425 |  |  [HAConvGNN: Hierarchical Attention Based Convolutional Graph Neural Network for Code Documentation Generation in Jupyter Notebooks](https://doi.org/10.18653/v1/2021.findings-emnlp.381) |  | 0 | Jupyter notebook allows data scientists to write machine learning code together with its documentation in cells. In this paper, we propose a new task of code documentation generation (CDG) for computational notebooks. In contrast to the previous CDG tasks which focus on generating documentation for... | Xuye Liu, Dakuo Wang, April Yi Wang, Yufang Hou, Lingfei Wu |  |
| 426 |  |  [A multilabel approach to morphosyntactic probing](https://doi.org/10.18653/v1/2021.findings-emnlp.382) |  | 0 | We propose using a multilabel probing task to assess the morphosyntactic representations of multilingual word embeddings. This tweak on canonical probing makes it easy to explore morphosyntactic representations, both holistically and at the level of individual features (e.g., gender, number, case),... | Naomi Tachikawa Shapiro, Amandalynne Paullada, Shane SteinertThrelkeld |  |
| 427 |  |  [Co-Teaching Student-Model through Submission Results of Shared Task](https://doi.org/10.18653/v1/2021.findings-emnlp.383) |  | 0 | Shared tasks have a long history and have become the mainstream of NLP research. Most of the shared tasks require participants to submit only system outputs and descriptions. It is uncommon for the shared task to request submission of the system itself because of the license issues and... | Kouta Nakayama, Shuhei Kurita, Akio Kobayashi, Yukino Baba, Satoshi Sekine |  |
| 428 |  |  [KLMo: Knowledge Graph Enhanced Pretrained Language Model with Fine-Grained Relationships](https://doi.org/10.18653/v1/2021.findings-emnlp.384) |  | 0 | Interactions between entities in knowledge graph (KG) provide rich knowledge for language representation learning. However, existing knowledge-enhanced pretrained language models (PLMs) only focus on entity information and ignore the fine-grained relationships between entities. In this work, we... | Lei He, Suncong Zheng, Tao Yang, Feng Zhang |  |
| 429 |  |  [Do We Know What We Don't Know? Studying Unanswerable Questions beyond SQuAD 2.0](https://doi.org/10.18653/v1/2021.findings-emnlp.385) |  | 0 | Understanding when a text snippet does not provide a sought after information is an essential part of natural language utnderstanding. Recent work (SQuAD 2.0; Rajpurkar et al., 2018) has attempted to make some progress in this direction by enriching the SQuAD dataset for the Extractive QA task with... | Elior Sulem, Jamaal Hay, Dan Roth |  |
| 430 |  |  [Glyph Enhanced Chinese Character Pre-Training for Lexical Sememe Prediction](https://doi.org/10.18653/v1/2021.findings-emnlp.386) |  | 0 | Sememes are defined as the atomic units to describe the semantic meaning of concepts. Due to the difficulty of manually annotating sememes and the inconsistency of annotations between experts, the lexical sememe prediction task has been proposed. However, previous methods heavily rely on word or... | Boer Lyu, Lu Chen, Kai Yu |  |
| 431 |  |  [Active Learning for Rumor Identification on Social Media](https://doi.org/10.18653/v1/2021.findings-emnlp.387) |  | 0 | Social media has emerged as a key channel for seeking information. Online users spend several hours reading, posting, and searching for news on microblogging platforms daily. However, this could act as a double-edged sword especially when not all information online is reliable. Moreover, the... | Parsa Farinneya, Mohammad Mahdi Abdollah Pour, Sardar Hamidian, Mona T. Diab |  |
| 432 |  |  [Cross-Domain Data Integration for Named Entity Disambiguation in Biomedical Text](https://doi.org/10.18653/v1/2021.findings-emnlp.388) |  | 0 | Named entity disambiguation (NED), which involves mapping textual mentions to structured entities, is particularly challenging in the medical domain due to the presence of rare entities. Existing approaches are limited by the presence of coarse-grained structural resources in biomedical knowledge... | Maya Varma, Laurel J. Orr, Sen Wu, Megan Leszczynski, Xiao Ling, Christopher Ré |  |
| 433 |  |  [Self-Training using Rules of Grammar for Few-Shot NLU](https://doi.org/10.18653/v1/2021.findings-emnlp.389) |  | 0 | We tackle the problem of self-training networks for NLU in low-resource environment—few labeled data and lots of unlabeled data. The effectiveness of self-training is a result of increasing the amount of training data while training. Yet it becomes less effective in low-resource settings due to... | Joonghyuk Hahn, Hyunjoon Cheon, Kyuyeol Han, Cheongjae Lee, Junseok Kim, YoSub Han |  |
| 434 |  |  [Aspect-based Sentiment Analysis in Question Answering Forums](https://doi.org/10.18653/v1/2021.findings-emnlp.390) |  | 0 | Aspect-based sentiment analysis (ABSA) typically focuses on extracting aspects and predicting their sentiments on individual sentences such as customer reviews. Recently, another kind of opinion sharing platform, namely question answering (QA) forum, has received increasing popularity, which... | Wenxuan Zhang, Yang Deng, Xin Li, Lidong Bing, Wai Lam |  |
| 435 |  |  [ForumSum: A Multi-Speaker Conversation Summarization Dataset](https://doi.org/10.18653/v1/2021.findings-emnlp.391) |  | 0 | Abstractive summarization quality had large improvements since recent language pretraining techniques. However, currently there is a lack of datasets for the growing needs of conversation summarization applications. Thus we collected ForumSum, a diverse and high-quality conversation summarization... | Misha Khalman, Yao Zhao, Mohammad Saleh |  |
| 436 |  |  [Question Answering over Electronic Devices: A New Benchmark Dataset and a Multi-Task Learning based QA Framework](https://doi.org/10.18653/v1/2021.findings-emnlp.392) |  | 0 | Answering questions asked from instructional corpora such as E-manuals, recipe books, etc., has been far less studied than open-domain factoid context-based question answering. This can be primarily attributed to the absence of standard benchmark datasets. In this paper, we meticulously create a... | Abhilash Nandy, Soumya Sharma, Shubham Maddhashiya, Kapil Sachdeva, Pawan Goyal, Niloy Ganguly |  |
| 437 |  |  [Comprehensive Punctuation Restoration for English and Polish](https://doi.org/10.18653/v1/2021.findings-emnlp.393) |  | 0 | Punctuation restoration is a fundamental requirement for the readability of text derived from Automatic Speech Recognition (ASR) systems. Most contemporary solutions are limited to predicting only a few of the most frequently occurring marks, such as periods, commas, and question marks - and only... | Michal Pogoda, Tomasz Walkowiak |  |
| 438 |  |  [Syntactically Diverse Adversarial Network for Knowledge-Grounded Conversation Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.394) |  | 0 | Generative conversation systems tend to produce meaningless and generic responses, which significantly reduce the user experience. In order to generate informative and diverse responses, recent studies proposed to fuse knowledge to improve informativeness and adopt latent variables to enhance the... | Fuwei Cui, Hui Di, Hongjie Ren, Kazushige Ouchi, Ze Liu, Jinan Xu |  |
| 439 |  |  [QACE: Asking Questions to Evaluate an Image Caption](https://doi.org/10.18653/v1/2021.findings-emnlp.395) |  | 0 | In this paper we propose QACE, a new metric based on Question Answering for Caption Evaluation to evaluate image captioning based on Question Generation(QG) and Question Answering(QA) systems. QACE generates questions on the evaluated caption and check its content by asking the questions on either... | Hwanhee Lee, Thomas Scialom, Seunghyun Yoon, Franck Dernoncourt, Kyomin Jung |  |
| 440 |  |  [Secoco: Self-Correcting Encoding for Neural Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.396) |  | 0 | This paper presents Self-correcting Encoding (Secoco), a framework that effectively deals with noisy input for robust neural machine translation by introducing self-correcting predictors. Different from previous robust approaches, Secoco enables NMT to explicitly correct noisy inputs and delete... | Tao Wang, Chengqi Zhao, Mingxuan Wang, Lei Li, Hang Li, Deyi Xiong |  |
| 441 |  |  [Simple or Complex? Complexity-controllable Question Generation with Soft Templates and Deep Mixture of Experts Model](https://doi.org/10.18653/v1/2021.findings-emnlp.397) |  | 0 | The ability to generate natural-language questions with controlled complexity levels is highly desirable as it further expands the applicability of question generation. In this paper, we propose an end-to-end neural complexity-controllable question generation model, which incorporates a mixture of... | Sheng Bi, Xiya Cheng, YuanFang Li, Lizhen Qu, Shirong Shen, Guilin Qi, Lu Pan, Yinlin Jiang |  |
| 442 |  |  [Predicting Anti-Asian Hateful Users on Twitter during COVID-19](https://doi.org/10.18653/v1/2021.findings-emnlp.398) |  | 0 | We investigate predictors of anti-Asian hate among Twitter users throughout COVID-19. With the rise of xenophobia and polarization that has accompanied widespread social media usage in many nations, online hate has become a major social issue, attracting many researchers. Here, we apply natural... | Jisun An, Haewoon Kwak, Claire Seungeun Lee, Bogang Jun, YongYeol Ahn |  |
| 443 |  |  [Fine-grained Typing of Emerging Entities in Microblogs](https://doi.org/10.18653/v1/2021.findings-emnlp.399) |  | 0 | Analyzing microblogs where we post what we experience enables us to perform various applications such as social-trend analysis and entity recommendation. To track emerging trends in a variety of areas, we want to categorize information on emerging entities (e.g., Avatar 2) in microblog posts... | Satoshi Akasaki, Naoki Yoshinaga, Masashi Toyoda |  |
| 444 |  |  [Data-Efficient Language Shaped Few-shot Image Classification](https://doi.org/10.18653/v1/2021.findings-emnlp.400) |  | 0 | Many existing works have demonstrated that language is a helpful guider for image understanding by neural networks. We focus on a language-shaped learning problem in a few-shot setting, i.e., using language to improve few-shot image classification when language descriptions are only available... | Zhenwen Liang, Xiangliang Zhang |  |
| 445 |  |  [Beyond Glass-Box Features: Uncertainty Quantification Enhanced Quality Estimation for Neural Machine Translation](https://doi.org/10.18653/v1/2021.findings-emnlp.401) |  | 0 | Quality Estimation (QE) plays an essential role in applications of Machine Translation (MT). Traditionally, a QE system accepts the original source text and translation from a black-box MT system as input. Recently, a few studies indicate that as a by-product of translation, QE benefits from the... | Ke Wang, Yangbin Shi, Jiayi Wang, Yuqi Zhang, Yu Zhao, Xiaolin Zheng |  |
| 446 |  |  [Fight Fire with Fire: Fine-tuning Hate Detectors using Large Samples of Generated Hate Speech](https://doi.org/10.18653/v1/2021.findings-emnlp.402) |  | 0 | Automatic hate speech detection is hampered by the scarcity of labeled datasetd, leading to poor generalization. We employ pretrained language models (LMs) to alleviate this data bottleneck. We utilize the GPT LM for generating large amounts of synthetic hate speech sequences from available labeled... | Tomer Wullach, Amir Adler, Einat Minkov |  |
| 447 |  |  [AutoEQA: Auto-Encoding Questions for Extractive Question Answering](https://doi.org/10.18653/v1/2021.findings-emnlp.403) |  | 0 | There has been a significant progress in the field of Extractive Question Answering (EQA) in the recent years. However, most of them are reliant on annotations of answer-spans in the corresponding passages. In this work, we address the problem of EQA when no annotations are present for the answer... | Stalin Varanasi, Saadullah Amin, Guenter Neumann |  |
| 448 |  |  [A Multi-label Multi-hop Relation Detection Model based on Relation-aware Sequence Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.404) |  | 0 | Multi-hop relation detection in Knowledge Base Question Answering (KBQA) aims at retrieving the relation path starting from the topic entity to the answer node based on a given question, where the relation path may comprise multiple relations. Most of the existing methods treat it as a single-label... | Linhai Zhang, Deyu Zhou, Chao Lin, Yulan He |  |
| 449 |  |  [Don't Discard All the Biased Instances: Investigating a Core Assumption in Dataset Bias Mitigation Techniques](https://doi.org/10.18653/v1/2021.findings-emnlp.405) |  | 0 | Existing techniques for mitigating dataset bias often leverage a biased model to identify biased instances. The role of these biased instances is then reduced during the training of the main model to enhance its robustness to out-of-distribution data. A common core assumption of these techniques is... | Hossein Amirkhani, Mohammad Taher Pilehvar |  |
| 450 |  |  [Stacked AMR Parsing with Silver Data](https://doi.org/10.18653/v1/2021.findings-emnlp.406) |  | 0 | Lacking sufficient human-annotated data is one main challenge for abstract meaning representation (AMR) parsing. To alleviate this problem, previous works usually make use of silver data or pre-trained language models. In particular, one recent seq-to-seq work directly fine-tunes AMR graph... | Qingrong Xia, Zhenghua Li, Rui Wang, Min Zhang |  |
| 451 |  |  [Speculative Sampling in Variational Autoencoders for Dialogue Response Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.407) |  | 0 | Variational autoencoders have been studied as a promising approach to model one-to-many mappings from context to response in chat response generation. However, they often fail to learn proper mappings. One of the reasons for this failure is the discrepancy between a response and a latent variable... | Shoetsu Sato, Naoki Yoshinaga, Masashi Toyoda, Masaru Kitsuregawa |  |
| 452 |  |  [Perceived and Intended Sarcasm Detection with Graph Attention Networks](https://doi.org/10.18653/v1/2021.findings-emnlp.408) |  | 0 | Existing sarcasm detection systems focus on exploiting linguistic markers, context, or user-level priors. However, social studies suggest that the relationship between the author and the audience can be equally relevant for the sarcasm usage and interpretation. In this work, we propose a framework... | Joan Plepi, Lucie Flek |  |
| 453 |  |  [Contrastive Representation Learning for Exemplar-Guided Paraphrase Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.409) |  | 0 | Exemplar-Guided Paraphrase Generation (EGPG) aims to generate a target sentence which conforms to the style of the given exemplar while encapsulating the content information of the source sentence. In this paper, we propose a new method with the goal of learning a better representation of the style... | Haoran Yang, Wai Lam, Piji Li |  |
| 454 |  |  [MAD-G: Multilingual Adapter Generation for Efficient Cross-Lingual Transfer](https://doi.org/10.18653/v1/2021.findings-emnlp.410) |  | 0 | Adapter modules have emerged as a general parameter-efficient means to specialize a pretrained encoder to new domains. Massively multilingual transformers (MMTs) have particularly benefited from additional training of language-specific adapters. However, this approach is not viable for the vast... | Alan Ansell, Edoardo Maria Ponti, Jonas Pfeiffer, Sebastian Ruder, Goran Glavas, Ivan Vulic, Anna Korhonen |  |
| 455 |  |  [Sustainable Modular Debiasing of Language Models](https://doi.org/10.18653/v1/2021.findings-emnlp.411) |  | 0 | Unfair stereotypical biases (e.g., gender, racial, or religious biases) encoded in modern pretrained language models (PLMs) have negative ethical implications for widespread adoption of state-of-the-art language technology. To remedy for this, a wide range of debiasing techniques have recently been... | Anne Lauscher, Tobias Lüken, Goran Glavas |  |
| 456 |  |  [A Divide-And-Conquer Approach for Multi-label Multi-hop Relation Detection in Knowledge Base Question Answering](https://doi.org/10.18653/v1/2021.findings-emnlp.412) |  | 0 | Relation detection in knowledge base question answering, aims to identify the path(s) of relations starting from the topic entity node that is linked to the answer node in knowledge graph. Such path might consist of multiple relations, which we call multi-hop. Moreover, for a single question, there... | Deyu Zhou, Yanzheng Xiang, Linhai Zhang, Chenchen Ye, QianWen Zhang, Yunbo Cao |  |
| 457 |  |  [Counterfactual Adversarial Learning with Representation Interpolation](https://doi.org/10.18653/v1/2021.findings-emnlp.413) |  | 0 | Deep learning models exhibit a preference for statistical fitting over logical reasoning. Spurious correlations might be memorized when there exists statistical bias in training data, which severely limits the model performance especially in small data scenarios. In this work, we introduce... | Wei Wang, Boxin Wang, Ning Shi, Jinfeng Li, Bingyu Zhu, Xiangyu Liu, Rong Zhang |  |
| 458 |  |  ['Just What do You Think You're Doing, Dave?' A Checklist for Responsible Data Use in NLP](https://doi.org/10.18653/v1/2021.findings-emnlp.414) |  | 0 | A key part of the NLP ethics movement is responsible use of data, but exactly what that means or how it can be best achieved remain unclear. This position paper discusses the core legal and ethical principles for collection and sharing of textual data, and the tensions between them. We propose a... | Anna Rogers, Timothy Baldwin, Kobi Leins |  |
| 459 |  |  [Counter-Contrastive Learning for Language GANs](https://doi.org/10.18653/v1/2021.findings-emnlp.415) |  | 0 | Generative Adversarial Networks (GANs) have achieved great success in image synthesis, but have proven to be difficult to generate natural language. Challenges arise from the uninformative learning signals passed from the discriminator. In other words, the poor learning signals limit the learning... | Yekun Chai, Haidong Zhang, Qiyue Yin, Junge Zhang |  |
| 460 |  |  [Incorporating Circumstances into Narrative Event Prediction](https://doi.org/10.18653/v1/2021.findings-emnlp.416) |  | 0 | The narrative event prediction aims to predict what happens after a sequence of events, which is essential to modeling sophisticated real-world events. Existing studies focus on mining the inter-events relationships while ignoring how the events happened, which we called circumstances. With our... | Shichao Wang, Xiangrui Cai, Hongbin Wang, Xiaojie Yuan |  |
| 461 |  |  [MultiFix: Learning to Repair Multiple Errors by Optimal Alignment Learning](https://doi.org/10.18653/v1/2021.findings-emnlp.417) |  | 0 | We consider the problem of learning to repair erroneous C programs by learning optimal alignments with correct programs. Since the previous approaches fix a single error in a line, it is inevitable to iterate the fixing process until no errors remain. In this work, we propose a novel... | HyeonTae Seo, YoSub Han, SangKi Ko |  |
| 462 |  |  [HOTTER: Hierarchical Optimal Topic Transport with Explanatory Context Representations](https://doi.org/10.18653/v1/2021.findings-emnlp.418) |  | 0 | Natural language processing (NLP) is often the backbone of today’s systems for user interactions, information retrieval and others. Many of such NLP applications rely on specialized learned representations (e.g. neural word embeddings, topic models) that improve the ability to reason about the... | Sabine Wehnert, Christian Scheel, Simona SzakácsBehling, Maret Nieländer, Patrick Mielke, Ernesto William De Luca |  |
| 463 |  |  [Grammatical Error Correction with Contrastive Learning in Low Error Density Domains](https://doi.org/10.18653/v1/2021.findings-emnlp.419) |  | 0 | Although grammatical error correction (GEC) has achieved good performance on texts written by learners of English as a second language, performance on low error density domains where texts are written by English speakers of varying levels of proficiency can still be improved. In this paper, we... | Hannan Cao, Wenmian Yang, Hwee Tou Ng |  |
| 464 |  |  [Improving Unsupervised Commonsense Reasoning Using Knowledge-Enabled Natural Language Inference](https://doi.org/10.18653/v1/2021.findings-emnlp.420) |  | 0 | Recent methods based on pre-trained language models have shown strong supervised performance on commonsense reasoning. However, they rely on expensive data annotation and time-consuming training. Thus, we focus on unsupervised commonsense reasoning. We show the effectiveness of using a common... | Canming Huang, Weinan He, Yongmei Liu |  |
| 465 |  |  [Does Putting a Linguist in the Loop Improve NLU Data Collection?](https://doi.org/10.18653/v1/2021.findings-emnlp.421) |  | 0 | Many crowdsourced NLP datasets contain systematic artifacts that are identified only after data collection is complete. Earlier identification of these issues should make it easier to create high-quality training and evaluation data. We attempt this by evaluating protocols in which expert linguists... | Alicia Parrish, William Huang, Omar Agha, SooHwan Lee, Nikita Nangia, Alex Warstadt, Karmanya Aggarwal, Emily Allaway, Tal Linzen, Samuel R. Bowman |  |
| 466 |  |  [Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding](https://doi.org/10.18653/v1/2021.findings-emnlp.422) |  | 0 | Large-scale, pre-trained language models (LMs) have achieved human-level performance on a breadth of language understanding tasks. However, evaluations only based on end task performance shed little light on machines’ true ability in language understanding and reasoning. In this paper, we highlight... | Shane Storks, Qiaozi Gao, Yichi Zhang, Joyce Chai |  |
| 467 |  |  [Making Heads and Tails of Models with Marginal Calibration for Sparse Tagsets](https://doi.org/10.18653/v1/2021.findings-emnlp.423) |  | 0 | For interpreting the behavior of a probabilistic model, it is useful to measure a model’s calibration—the extent to which it produces reliable confidence scores. We address the open problem of calibration for tagging models with sparse tagsets, and recommend strategies to measure and reduce... | Michael Kranzlein, Nelson F. Liu, Nathan Schneider |  |
| 468 |  |  [GeDi: Generative Discriminator Guided Sequence Generation](https://doi.org/10.18653/v1/2021.findings-emnlp.424) |  | 0 |  | Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq R. Joty, Richard Socher, Nazneen Fatema Rajani |  |
| 469 |  |  [Frontmatter](https://aclanthology.org/2021.emnlp-main.0) |  | 0 |  |  |  |
| 470 |  |  [AligNART: Non-autoregressive Neural Machine Translation by Jointly Learning to Estimate Alignment and Translate](https://doi.org/10.18653/v1/2021.emnlp-main.1) |  | 0 | Non-autoregressive neural machine translation (NART) models suffer from the multi-modality problem which causes translation inconsistency such as token repetition. Most recent approaches have attempted to solve this problem by implicitly modeling dependencies between outputs. In this paper, we... | Jongyoon Song, Sungwon Kim, Sungroh Yoon |  |
| 471 |  |  [Zero-Shot Cross-Lingual Transfer of Neural Machine Translation with Multilingual Pretrained Encoders](https://doi.org/10.18653/v1/2021.emnlp-main.2) |  | 0 | Previous work mainly focuses on improving cross-lingual transfer for NLU tasks with a multilingual pretrained encoder (MPE), or improving the performance on supervised machine translation with BERT. However, it is under-explored that whether the MPE can help to facilitate the cross-lingual... | Guanhua Chen, Shuming Ma, Yun Chen, Li Dong, Dongdong Zhang, Jia Pan, Wenping Wang, Furu Wei |  |
| 472 |  |  [ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora](https://doi.org/10.18653/v1/2021.emnlp-main.3) |  | 0 | Recent studies have demonstrated that pre-trained cross-lingual models achieve impressive performance in downstream cross-lingual tasks. This improvement benefits from learning a large amount of monolingual and parallel corpora. Although it is generally acknowledged that parallel corpora are... | Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang |  |
| 473 |  |  [Cross Attention Augmented Transducer Networks for Simultaneous Translation](https://doi.org/10.18653/v1/2021.emnlp-main.4) |  | 0 | This paper proposes a novel architecture, Cross Attention Augmented Transducer (CAAT), for simultaneous translation. The framework aims to jointly optimize the policy and translation models. To effectively consider all possible READ-WRITE simultaneous translation action paths, we adapt the online... | Dan Liu, Mengge Du, Xiaoxi Li, Ya Li, Enhong Chen |  |
| 474 |  |  [Translating Headers of Tabular Data: A Pilot Study of Schema Translation](https://doi.org/10.18653/v1/2021.emnlp-main.5) |  | 0 | Schema translation is the task of automatically translating headers of tabular data from one language to another. High-quality schema translation plays an important role in cross-lingual table searching, understanding and analysis. Despite its importance, schema translation is not well studied in... | Kunrui Zhu, Yan Gao, Jiaqi Guo, JianGuang Lou |  |
| 475 |  |  [Towards Making the Most of Dialogue Characteristics for Neural Chat Translation](https://doi.org/10.18653/v1/2021.emnlp-main.6) |  | 0 | Neural Chat Translation (NCT) aims to translate conversational text between speakers of different languages. Despite the promising performance of sentence-level and context-aware neural machine translation models, there still remain limitations in current NCT models because the inherent dialogue... | Yunlong Liang, Chulun Zhou, Fandong Meng, Jinan Xu, Yufeng Chen, Jinsong Su, Jie Zhou |  |
| 476 |  |  [Low-Resource Dialogue Summarization with Domain-Agnostic Multi-Source Pretraining](https://doi.org/10.18653/v1/2021.emnlp-main.7) |  | 0 | With the rapid increase in the volume of dialogue data from daily life, there is a growing demand for dialogue summarization. Unfortunately, training a large summarization model is generally infeasible due to the inadequacy of dialogue data with annotated summaries. Most existing works for... | Yicheng Zou, Bolin Zhu, Xingwu Hu, Tao Gui, Qi Zhang |  |
| 477 |  |  [Controllable Neural Dialogue Summarization with Personal Named Entity Planning](https://doi.org/10.18653/v1/2021.emnlp-main.8) |  | 0 | In this paper, we propose a controllable neural generation framework that can flexibly guide dialogue summarization with personal named entity planning. The conditional sequences are modulated to decide what types of information or what perspective to focus on when forming summaries to tackle the... | Zhengyuan Liu, Nancy F. Chen |  |
| 478 |  |  [Fine-grained Factual Consistency Assessment for Abstractive Summarization Models](https://doi.org/10.18653/v1/2021.emnlp-main.9) |  | 0 | Factual inconsistencies existed in the output of abstractive summarization models with original documents are frequently presented. Fact consistency assessment requires the reasoning capability to find subtle clues to identify whether a model-generated summary is consistent with the original... | Sen Zhang, Jianwei Niu, Chuyuan Wei |  |
| 479 |  |  [Decision-Focused Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.10) |  | 0 | Relevance in summarization is typically de- fined based on textual information alone, without incorporating insights about a particular decision. As a result, to support risk analysis of pancreatic cancer, summaries of medical notes may include irrelevant information such as a knee injury. We... | ChaoChun Hsu, Chenhao Tan |  |
| 480 |  |  [Multiplex Graph Neural Network for Extractive Text Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.11) |  | 0 | Extractive text summarization aims at extracting the most representative sentences from a given document as its summary. To extract a good summary from a long text document, sentence embedding plays an important role. Recent studies have leveraged graph neural networks to capture the... | Baoyu Jing, Zeyu You, Tao Yang, Wei Fan, Hanghang Tong |  |
| 481 |  |  [A Thorough Evaluation of Task-Specific Pretraining for Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.12) |  | 0 | Task-agnostic pretraining objectives like masked language models or corrupted span prediction are applicable to a wide range of NLP downstream tasks (Raffel et al.,2019), but are outperformed by task-specific pretraining objectives like predicting extracted gap sentences on summarization (Zhang et... | Sascha Rothe, Joshua Maynez, Shashi Narayan |  |
| 482 |  |  [HETFORMER: Heterogeneous Transformer with Sparse Attention for Long-Text Extractive Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.13) |  | 0 | To capture the semantic graph structure from raw text, most existing summarization approaches are built on GNNs with a pre-trained model. However, these methods suffer from cumbersome procedures and inefficient computations for long-text documents. To mitigate these issues, this paper proposes... | Ye Liu, JianGuo Zhang, Yao Wan, Congying Xia, Lifang He, Philip S. Yu |  |
| 483 |  |  [Unsupervised Keyphrase Extraction by Jointly Modeling Local and Global Context](https://doi.org/10.18653/v1/2021.emnlp-main.14) |  | 0 | Embedding based methods are widely used for unsupervised keyphrase extraction (UKE) tasks. Generally, these methods simply calculate similarities between phrase embeddings and document embedding, which is insufficient to capture different context for a more effective UKE model. In this paper, we... | Xinnian Liang, Shuangzhi Wu, Mu Li, Zhoujun Li |  |
| 484 |  |  [Distantly Supervised Relation Extraction using Multi-Layer Revision Network and Confidence-based Multi-Instance Learning](https://doi.org/10.18653/v1/2021.emnlp-main.15) |  | 0 | Distantly supervised relation extraction is widely used in the construction of knowledge bases due to its high efficiency. However, the automatically obtained instances are of low quality with numerous irrelevant words. In addition, the strong assumption of distant supervision leads to the... | Xiangyu Lin, Tianyi Liu, Weijia Jia, Zhiguo Gong |  |
| 485 |  |  [Logic-level Evidence Retrieval and Graph-based Verification Network for Table-based Fact Verification](https://doi.org/10.18653/v1/2021.emnlp-main.16) |  | 0 | Table-based fact verification task aims to verify whether the given statement is supported by the given semi-structured table. Symbolic reasoning with logical operations plays a crucial role in this task. Existing methods leverage programs that contain rich logical information to enhance the... | Qi Shi, Yu Zhang, Qingyu Yin, Ting Liu |  |
| 486 |  |  [A Partition Filter Network for Joint Entity and Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.17) |  | 0 | In joint entity and relation extraction, existing work either sequentially encode task-specific features, leading to an imbalance in inter-task feature interaction where features extracted later have no direct contact with those that come first. Or they encode entity features and relation features... | Zhiheng Yan, Chong Zhang, Jinlan Fu, Qi Zhang, Zhongyu Wei |  |
| 487 |  |  [TEBNER: Domain Specific Named Entity Recognition with Type Expanded Boundary-aware Network](https://doi.org/10.18653/v1/2021.emnlp-main.18) |  | 0 | To alleviate label scarcity in Named Entity Recognition (NER) task, distantly supervised NER methods are widely applied to automatically label data and identify entities. Although the human effort is reduced, the generated incomplete and noisy annotations pose new challenges for learning effective... | Zheng Fang, Yanan Cao, Tai Li, Ruipeng Jia, Fang Fang, Yanmin Shang, Yuhai Lu |  |
| 488 |  |  [Beta Distribution Guided Aspect-aware Graph for Aspect Category Sentiment Analysis with Affective Knowledge](https://doi.org/10.18653/v1/2021.emnlp-main.19) |  | 0 | In this paper, we investigate the Aspect Category Sentiment Analysis (ACSA) task from a novel perspective by exploring a Beta Distribution guided aspect-aware graph construction based on external knowledge. That is, we are no longer entangled about how to laboriously search the sentiment clues for... | Bin Liang, Hang Su, Rongdi Yin, Lin Gui, Min Yang, Qin Zhao, Xiaoqi Yu, Ruifeng Xu |  |
| 489 |  |  [DILBERT: Customized Pre-Training for Domain Adaptation with Category Shift, with an Application to Aspect Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.20) |  | 0 | The rise of pre-trained language models has yielded substantial progress in the vast majority of Natural Language Processing (NLP) tasks. However, a generic approach towards the pre-training procedure can naturally be sub-optimal in some cases. Particularly, fine-tuning a pre-trained language model... | Entony Lekhtman, Yftah Ziser, Roi Reichart |  |
| 490 |  |  [Improving Multimodal fusion via Mutual Dependency Maximisation](https://doi.org/10.18653/v1/2021.emnlp-main.21) |  | 0 | Multimodal sentiment analysis is a trending area of research, and multimodal fusion is one of its most active topic. Acknowledging humans communicate through a variety of channels (i.e visual, acoustic, linguistic), multimodal systems aim at integrating different unimodal representations into a... | Pierre Colombo, Emile Chapuis, Matthieu Labeau, Chloé Clavel |  |
| 491 |  |  [Learning Implicit Sentiment in Aspect-based Sentiment Analysis with Supervised Contrastive Pre-Training](https://doi.org/10.18653/v1/2021.emnlp-main.22) |  | 0 | Aspect-based sentiment analysis aims to identify the sentiment polarity of a specific aspect in product reviews. We notice that about 30% of reviews do not contain obvious opinion words, but still convey clear human-aware sentiment orientation, which is known as implicit sentiment. However, recent... | Zhengyan Li, Yicheng Zou, Chong Zhang, Qi Zhang, Zhongyu Wei |  |
| 492 |  |  [Progressive Self-Training with Discriminator for Aspect Term Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.23) |  | 0 | Aspect term extraction aims to extract aspect terms from a review sentence that users have expressed opinions on. One of the remaining challenges for aspect term extraction resides in the lack of sufficient annotated data. While self-training is potentially an effective method to address this... | Qianlong Wang, Zhiyuan Wen, Qin Zhao, Min Yang, Ruifeng Xu |  |
| 493 |  |  [Reinforced Counterfactual Data Augmentation for Dual Sentiment Classification](https://doi.org/10.18653/v1/2021.emnlp-main.24) |  | 0 | Data augmentation and adversarial perturbation approaches have recently achieved promising results in solving the over-fitting problem in many natural language processing (NLP) tasks including sentiment classification. However, existing studies aimed to improve the generalization ability by... | Hao Chen, Rui Xia, Jianfei Yu |  |
| 494 |  |  [Idiosyncratic but not Arbitrary: Learning Idiolects in Online Registers Reveals Distinctive yet Consistent Individual Styles](https://doi.org/10.18653/v1/2021.emnlp-main.25) |  | 0 | An individual’s variation in writing style is often a function of both social and personal attributes. While structured social variation has been extensively studied, e.g., gender based variation, far less is known about how to characterize individual styles due to their idiosyncratic nature. We... | Jian Zhu, David Jurgens |  |
| 495 |  |  [Narrative Theory for Computational Narrative Understanding](https://doi.org/10.18653/v1/2021.emnlp-main.26) |  | 0 | Over the past decade, the field of natural language processing has developed a wide array of computational methods for reasoning about narrative, including summarization, commonsense inference, and event detection. While this work has brought an important empirical lens for examining narrative, it... | Andrew Piper, Richard Jean So, David Bamman |  |
| 496 |  |  [(Mis)alignment Between Stance Expressed in Social Media Data and Public Opinion Surveys](https://doi.org/10.18653/v1/2021.emnlp-main.27) |  | 0 | Stance detection, which aims to determine whether an individual is for or against a target concept, promises to uncover public opinion from large streams of social media data. Yet even human annotation of social media content does not always capture “stance” as measured by public opinion polls. We... | Kenneth Joseph, Sarah Shugars, Ryan J. Gallagher, Jon Green, Alexi Quintana Mathé, Zijian An, David Lazer |  |
| 497 |  |  [How Does Counterfactually Augmented Data Impact Models for Social Computing Constructs?](https://doi.org/10.18653/v1/2021.emnlp-main.28) |  | 0 | As NLP models are increasingly deployed in socially situated settings such as online abusive content detection, it is crucial to ensure that these models are robust. One way of improving model robustness is to generate counterfactually augmented data (CAD) for training models that can better learn... | Indira Sen, Mattia Samory, Fabian Flöck, Claudia Wagner, Isabelle Augenstein |  |
| 498 |  |  [Latent Hatred: A Benchmark for Understanding Implicit Hate Speech](https://doi.org/10.18653/v1/2021.emnlp-main.29) |  | 0 | Hate speech has grown significantly on social media, causing serious consequences for victims of all demographics. Despite much attention being paid to characterize and detect discriminatory speech, most work has focused on explicit or overt hate speech, failing to address a more pervasive form... | Mai ElSherief, Caleb Ziems, David Muchlinski, Vaishnavi Anupindi, Jordyn Seybolt, Munmun De Choudhury, Diyi Yang |  |
| 499 |  |  [Distilling Linguistic Context for Language Model Compression](https://doi.org/10.18653/v1/2021.emnlp-main.30) |  | 0 | A computationally expensive and memory intensive neural network lies behind the recent success of language representation learning. Knowledge distillation, a major technique for deploying such a vast language model in resource-scarce environments, transfers the knowledge on individual word... | Geondo Park, Gyeongman Kim, Eunho Yang |  |
| 500 |  |  [Dynamic Knowledge Distillation for Pre-trained Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.31) |  | 0 | Knowledge distillation (KD) has been proved effective for compressing large-scale pre-trained language models. However, existing methods conduct KD statically, e.g., the student model aligns its output distribution to that of a selected teacher model on the pre-defined training dataset. In this... | Lei Li, Yankai Lin, Shuhuai Ren, Peng Li, Jie Zhou, Xu Sun |  |
| 501 |  |  [Few-Shot Text Generation with Natural Language Instructions](https://doi.org/10.18653/v1/2021.emnlp-main.32) |  | 0 | Providing pretrained language models with simple task descriptions in natural language enables them to solve some tasks in a fully unsupervised fashion. Moreover, when combined with regular learning from examples, this idea yields impressive few-shot results for a wide range of text classification... | Timo Schick, Hinrich Schütze |  |
| 502 |  |  [SOM-NCSCM : An Efficient Neural Chinese Sentence Compression Model Enhanced with Self-Organizing Map](https://doi.org/10.18653/v1/2021.emnlp-main.33) |  | 0 | Sentence Compression (SC), which aims to shorten sentences while retaining important words that express the essential meanings, has been studied for many years in many languages, especially in English. However, improvements on Chinese SC task are still quite few due to several difficulties: scarce... | Kangli Zi, Shi Wang, Yu Liu, Jicun Li, Yanan Cao, Cungen Cao |  |
| 503 |  |  [Efficient Multi-Task Auxiliary Learning: Selecting Auxiliary Data by Feature Similarity](https://doi.org/10.18653/v1/2021.emnlp-main.34) |  | 0 | Multi-task auxiliary learning utilizes a set of relevant auxiliary tasks to improve the performance of a primary task. A common usage is to manually select multiple auxiliary tasks for multi-task learning on all data, which raises two issues: (1) selecting beneficial auxiliary tasks for a primary... | PoNien Kung, ShengSiang Yin, YiCheng Chen, TseHsuan Yang, YunNung Chen |  |
| 504 |  |  [GOLD: Improving Out-of-Scope Detection in Dialogues using Data Augmentation](https://doi.org/10.18653/v1/2021.emnlp-main.35) |  | 0 | Practical dialogue systems require robust methods of detecting out-of-scope (OOS) utterances to avoid conversational breakdowns and related failure modes. Directly training a model with labeled OOS examples yields reasonable performance, but obtaining such data is a resource-intensive process. To... | Derek Chen, Zhou Yu |  |
| 505 |  |  [Graph Based Network with Contextualized Representations of Turns in Dialogue](https://doi.org/10.18653/v1/2021.emnlp-main.36) |  | 0 | Dialogue-based relation extraction (RE) aims to extract relation(s) between two arguments that appear in a dialogue. Because dialogues have the characteristics of high personal pronoun occurrences and low information density, and since most relational facts in dialogues are not supported by any... | Bongseok Lee, Yong Suk Choi |  |
| 506 |  |  [Automatically Exposing Problems with Neural Dialog Models](https://doi.org/10.18653/v1/2021.emnlp-main.37) |  | 0 | Neural dialog models are known to suffer from problems such as generating unsafe and inconsistent responses. Even though these problems are crucial and prevalent, they are mostly manually identified by model designers through interactions. Recently, some research instructs crowdworkers to goad the... | Dian Yu, Kenji Sagae |  |
| 507 |  |  [Event Coreference Data (Almost) for Free: Mining Hyperlinks from Online News](https://doi.org/10.18653/v1/2021.emnlp-main.38) |  | 0 | Cross-document event coreference resolution (CDCR) is the task of identifying which event mentions refer to the same events throughout a collection of documents. Annotating CDCR data is an arduous and expensive process, explaining why existing corpora are small and lack domain coverage. To overcome... | Michael Bugert, Iryna Gurevych |  |
| 508 |  |  [Inducing Stereotypical Character Roles from Plot Structure](https://doi.org/10.18653/v1/2021.emnlp-main.39) |  | 0 | Stereotypical character roles-also known as archetypes or dramatis personae-play an important function in narratives: they facilitate efficient communication with bundles of default characteristics and associations and ease understanding of those characters’ roles in the overall narrative. We... | Labiba Jahan, Rahul Mittal, Mark A. Finlayson |  |
| 509 |  |  [Multitask Semi-Supervised Learning for Class-Imbalanced Discourse Classification](https://doi.org/10.18653/v1/2021.emnlp-main.40) |  | 0 | As labeling schemas evolve over time, small differences can render datasets following older schemas unusable. This prevents researchers from building on top of previous annotation work and results in the existence, in discourse learning in particular, of many small class-imbalanced datasets. In... | Alexander Spangher, Jonathan May, SzRung Shiang, Lingjia Deng |  |
| 510 |  |  [Low Frequency Names Exhibit Bias and Overfitting in Contextualizing Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.41) |  | 0 | We use a dataset of U.S. first names with labels based on predominant gender and racial group to examine the effect of training corpus frequency on tokenization, contextualization, similarity to initial representation, and bias in BERT, GPT-2, T5, and XLNet. We show that predominantly female and... | Robert Wolfe, Aylin Caliskan |  |
| 511 |  |  [Mitigating Language-Dependent Ethnic Bias in BERT](https://doi.org/10.18653/v1/2021.emnlp-main.42) |  | 0 | In this paper, we study ethnic bias and how it varies across languages by analyzing and mitigating ethnic bias in monolingual BERT for English, German, Spanish, Korean, Turkish, and Chinese. To observe and quantify ethnic bias, we develop a novel metric called Categorical Bias score. Then we... | Jaimeen Ahn, Alice Oh |  |
| 512 |  |  [Adversarial Scrubbing of Demographic Information for Text Classification](https://doi.org/10.18653/v1/2021.emnlp-main.43) |  | 0 | Contextual representations learned by language models can often encode undesirable attributes, like demographic associations of the users, while being trained for an unrelated target task. We aim to scrub such undesirable attributes and learn fair representations while maintaining performance on... | Somnath Basu Roy Chowdhury, Sayan Ghosh, Yiyuan Li, Junier Oliva, Shashank Srivastava, Snigdha Chaturvedi |  |
| 513 |  |  [Open-domain clarification question generation without question examples](https://doi.org/10.18653/v1/2021.emnlp-main.44) |  | 0 | An overarching goal of natural language processing is to enable machines to communicate seamlessly with humans. However, natural language can be ambiguous or unclear. In cases of uncertainty, humans engage in an interactive process known as repair: asking questions and seeking clarification until... | Julia White, Gabriel Poesia, Robert X. D. Hawkins, Dorsa Sadigh, Noah D. Goodman |  |
| 514 |  |  [Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting](https://doi.org/10.18653/v1/2021.emnlp-main.45) |  | 0 | In this paper, we propose Sequence Span Rewriting (SSR), a self-supervised task for sequence-to-sequence (Seq2Seq) pre-training. SSR learns to refine the machine-generated imperfect text spans into ground truth text. SSR provides more fine-grained and informative supervision in addition to the... | Wangchunshu Zhou, Tao Ge, Canwen Xu, Ke Xu, Furu Wei |  |
| 515 |  |  [Coarse2Fine: Fine-grained Text Classification on Coarsely-grained Annotated Data](https://doi.org/10.18653/v1/2021.emnlp-main.46) |  | 0 | Existing text classification methods mainly focus on a fixed label set, whereas many real-world applications require extending to new fine-grained classes as the number of samples per label increases. To accommodate such requirements, we introduce a new problem called coarse-to-fine grained... | Dheeraj Mekala, Varun Gangal, Jingbo Shang |  |
| 516 |  |  [Text2Mol: Cross-Modal Molecule Retrieval with Natural Language Queries](https://doi.org/10.18653/v1/2021.emnlp-main.47) |  | 0 | We propose a new task, Text2Mol, to retrieve molecules using natural language descriptions as queries. Natural language and molecules encode information in very different ways, which leads to the exciting but challenging problem of integrating these two very different modalities. Although some work... | Carl Edwards, ChengXiang Zhai, Heng Ji |  |
| 517 |  |  [Classification of hierarchical text using geometric deep learning: the case of clinical trials corpus](https://doi.org/10.18653/v1/2021.emnlp-main.48) |  | 0 | We consider the hierarchical representation of documents as graphs and use geometric deep learning to classify them into different categories. While graph neural networks can efficiently handle the variable structure of hierarchical documents using the permutation invariant message passing... | Sohrab Ferdowsi, Nikolay Borissov, Julien Knafou, Poorya Amini, Douglas Teodoro |  |
| 518 |  |  [The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers](https://doi.org/10.18653/v1/2021.emnlp-main.49) |  | 0 | Recently, many datasets have been proposed to test the systematic generalization ability of neural networks. The companion baseline Transformers, typically trained with default hyper-parameters from standard tasks, are shown to fail dramatically. Here we demonstrate that by revisiting model... | Róbert Csordás, Kazuki Irie, Jürgen Schmidhuber |  |
| 519 |  |  [Artificial Text Detection via Examining the Topology of Attention Maps](https://doi.org/10.18653/v1/2021.emnlp-main.50) |  | 0 | The impressive capabilities of recent generative models to create texts that are challenging to distinguish from the human-written ones can be misused for generating fake news, product reviews, and even abusive content. Despite the prominent performance of existing methods for artificial text... | Laida Kushnareva, Daniil Cherniavskii, Vladislav Mikhailov, Ekaterina Artemova, Serguei Barannikov, Alexander Bernstein, Irina Piontkovskaya, Dmitri Piontkovski, Evgeny Burnaev |  |
| 520 |  |  [Active Learning by Acquiring Contrastive Examples](https://doi.org/10.18653/v1/2021.emnlp-main.51) |  | 0 | Common acquisition functions for active learning use either uncertainty or diversity sampling, aiming to select difficult and diverse data points from the pool of unlabeled data, respectively. In this work, leveraging the best of both worlds, we propose an acquisition function that opts for... | Katerina Margatina, Giorgos Vernikos, Loïc Barrault, Nikolaos Aletras |  |
| 521 |  |  [Conditional Poisson Stochastic Beams](https://doi.org/10.18653/v1/2021.emnlp-main.52) |  | 0 | Beam search is the default decoding strategy for many sequence generation tasks in NLP. The set of approximate K-best items returned by the algorithm is a useful summary of the distribution for many applications; however, the candidates typically exhibit high overlap and may give a highly biased... | Clara Meister, Afra Amini, Tim Vieira, Ryan Cotterell |  |
| 522 |  |  [Building Adaptive Acceptability Classifiers for Neural NLG](https://doi.org/10.18653/v1/2021.emnlp-main.53) |  | 0 | We propose a novel framework to train models to classify acceptability of responses generated by natural language generation (NLG) models, improving upon existing sentence transformation and model-based approaches. An NLG response is considered acceptable if it is both semantically correct and... | Soumya Batra, Shashank Jain, Peyman Heidari, Ankit Arun, Catharine Youngs, Xintong Li, Pinar Donmez, Shawn Mei, Shiunzu Kuo, Vikas Bhardwaj, Anuj Kumar, Michael White |  |
| 523 |  |  [Moral Stories: Situated Reasoning about Norms, Intents, Actions, and their Consequences](https://doi.org/10.18653/v1/2021.emnlp-main.54) |  | 0 | In social settings, much of human behavior is governed by unspoken rules of conduct rooted in societal norms. For artificial systems to be fully integrated into social environments, adherence to such norms is a central prerequisite. To investigate whether language generation models can serve as... | Denis Emelin, Ronan Le Bras, Jena D. Hwang, Maxwell Forbes, Yejin Choi |  |
| 524 |  |  [Truth-Conditional Captions for Time Series Data](https://doi.org/10.18653/v1/2021.emnlp-main.55) |  | 0 | In this paper, we explore the task of automatically generating natural language descriptions of salient patterns in a time series, such as stock prices of a company over a week. A model for this task should be able to extract high-level patterns such as presence of a peak or a dip. While typical... | Harsh Jhamtani, Taylor BergKirkpatrick |  |
| 525 |  |  [Injecting Entity Types into Entity-Guided Text Generation](https://doi.org/10.18653/v1/2021.emnlp-main.56) |  | 0 | Recent successes in deep generative modeling have led to significant advances in natural language generation (NLG). Incorporating entities into neural generation models has demonstrated great improvements by assisting to infer the summary topic and to generate coherent content. To enhance the role... | Xiangyu Dong, Wenhao Yu, Chenguang Zhu, Meng Jiang |  |
| 526 |  |  [Smelting Gold and Silver for Improved Multilingual AMR-to-Text Generation](https://doi.org/10.18653/v1/2021.emnlp-main.57) |  | 0 | Recent work on multilingual AMR-to-text generation has exclusively focused on data augmentation strategies that utilize silver AMR. However, this assumes a high quality of generated AMRs, potentially limiting the transferability to the target task. In this paper, we investigate different techniques... | Leonardo F. R. Ribeiro, Jonas Pfeiffer, Yue Zhang, Iryna Gurevych |  |
| 527 |  |  [Learning Compact Metrics for MT](https://doi.org/10.18653/v1/2021.emnlp-main.58) |  | 0 | Recent developments in machine translation and multilingual text generation have led researchers to adopt trained metrics such as COMET or BLEURT, which treat evaluation as a regression problem and use representations from multilingual pre-trained models such as XLM-RoBERTa or mBERT. Yet studies on... | Amy Pu, Hyung Won Chung, Ankur P. Parikh, Sebastian Gehrmann, Thibault Sellam |  |
| 528 |  |  [The Impact of Positional Encodings on Multilingual Compression](https://doi.org/10.18653/v1/2021.emnlp-main.59) |  | 0 | In order to preserve word-order information in a non-autoregressive setting, transformer architectures tend to include positional knowledge, by (for instance) adding positional encodings to token embeddings. Several modifications have been proposed over the sinusoidal positional encodings used in... | Vinit Ravishankar, Anders Søgaard |  |
| 529 |  |  [Disentangling Representations of Text by Masking Transformers](https://doi.org/10.18653/v1/2021.emnlp-main.60) |  | 0 | Representations from large pretrained models such as BERT encode a range of features into monolithic vectors, affording strong predictive accuracy across a range of downstream tasks. In this paper we explore whether it is possible to learn disentangled representations by identifying existing... | Xiongyi Zhang, JanWillem van de Meent, Byron C. Wallace |  |
| 530 |  |  [Exploring the Role of BERT Token Representations to Explain Sentence Probing Results](https://doi.org/10.18653/v1/2021.emnlp-main.61) |  | 0 | Several studies have been carried out on revealing linguistic features captured by BERT. This is usually achieved by training a diagnostic classifier on the representations obtained from different layers of BERT. The subsequent classification accuracy is then interpreted as the ability of the model... | Hosein Mohebbi, Ali Modarressi, Mohammad Taher Pilehvar |  |
| 531 |  |  [Do Long-Range Language Models Actually Use Long-Range Context?](https://doi.org/10.18653/v1/2021.emnlp-main.62) |  | 0 | Language models are generally trained on short, truncated input sequences, which limits their ability to use discourse-level information present in long-range context to improve their predictions. Recent efforts to improve the efficiency of self-attention have led to a proliferation of long-range... | Simeng Sun, Kalpesh Krishna, Andrew MattarellaMicke, Mohit Iyyer |  |
| 532 |  |  [The World of an Octopus: How Reporting Bias Influences a Language Model's Perception of Color](https://doi.org/10.18653/v1/2021.emnlp-main.63) |  | 0 | Recent work has raised concerns about the inherent limitations of text-only pretraining. In this paper, we first demonstrate that reporting bias, the tendency of people to not state the obvious, is one of the causes of this limitation, and then investigate to what extent multimodal training can... | Cory Paik, Stéphane ArocaOuellette, Alessandro Roncone, Katharina Kann |  |
| 533 |  |  [SELFEXPLAIN: A Self-Explaining Architecture for Neural Text Classifiers](https://doi.org/10.18653/v1/2021.emnlp-main.64) |  | 0 | We introduce SelfExplain, a novel self-explaining model that explains a text classifier’s predictions using phrase-based concepts. SelfExplain augments existing neural classifiers by adding (1) a globally interpretable layer that identifies the most influential concepts in the training set for a... | Dheeraj Rajagopal, Vidhisha Balachandran, Eduard H. Hovy, Yulia Tsvetkov |  |
| 534 |  |  [Memory and Knowledge Augmented Language Models for Inferring Salience in Long-Form Stories](https://doi.org/10.18653/v1/2021.emnlp-main.65) |  | 0 | Measuring event salience is essential in the understanding of stories. This paper takes a recent unsupervised method for salience detection derived from Barthes Cardinal Functions and theories of surprise and applies it to longer narrative forms. We improve the standard transformer language model... | David Wilmot, Frank Keller |  |
| 535 |  |  [Semantic Novelty Detection in Natural Language Descriptions](https://doi.org/10.18653/v1/2021.emnlp-main.66) |  | 0 | This paper proposes to study a fine-grained semantic novelty detection task, which can be illustrated with the following example. It is normal that a person walks a dog in the park, but if someone says “A man is walking a chicken in the park”, it is novel. Given a set of natural language... | Nianzu Ma, Alexander Politowicz, Sahisnu Mazumder, Jiahua Chen, Bing Liu, Eric Robertson, Scott Grigsby |  |
| 536 |  |  [Jump-Starting Item Parameters for Adaptive Language Tests](https://doi.org/10.18653/v1/2021.emnlp-main.67) |  | 0 | A challenge in designing high-stakes language assessments is calibrating the test item difficulties, either a priori or from limited pilot test data. While prior work has addressed ‘cold start’ estimation of item difficulties without piloting, we devise a multi-task generalized linear model with... | Arya D. McCarthy, Kevin P. Yancey, Geoffrey T. LaFlair, Jesse Egbert, Manqian Liao, Burr Settles |  |
| 537 |  |  [Voice Query Auto Completion](https://doi.org/10.18653/v1/2021.emnlp-main.68) |  | 0 | Query auto completion (QAC) is the task of predicting a search engine user’s final query from their intermediate, incomplete query. In this paper, we extend QAC to the streaming voice search setting, where automatic speech recognition systems produce intermediate transcriptions as users speak.... | Raphael Tang, Karun Kumar, Kendra Chalkley, Ji Xin, Liming Zhang, Wenyan Li, Gefei Yang, Yajie Mao, Junho Shin, Geoffrey Craig Murray, Jimmy Lin |  |
| 538 |  |  [CoPHE: A Count-Preserving Hierarchical Evaluation Metric in Large-Scale Multi-Label Text Classification](https://doi.org/10.18653/v1/2021.emnlp-main.69) |  | 0 | Large-Scale Multi-Label Text Classification (LMTC) includes tasks with hierarchical label spaces, such as automatic assignment of ICD-9 codes to discharge summaries. Performance of models in prior art is evaluated with standard precision, recall, and F1 measures without regard for the rich... | Matús Falis, Hang Dong, Alexandra Birch, Beatrice Alex |  |
| 539 |  |  [Learning Universal Authorship Representations](https://doi.org/10.18653/v1/2021.emnlp-main.70) |  | 0 | Determining whether two documents were composed by the same author, also known as authorship verification, has traditionally been tackled using statistical methods. Recently, authorship representations learned using neural networks have been found to outperform alternatives, particularly in... | Rafael A. Rivera Soto, Olivia Elizabeth Miano, Juanita Ordonez, Barry Y. Chen, Aleem Khan, Marcus Bishop, Nicholas Andrews |  |
| 540 |  |  [Predicting emergent linguistic compositions through time: Syntactic frame extension via multimodal chaining](https://doi.org/10.18653/v1/2021.emnlp-main.71) |  | 0 | Natural language relies on a finite lexicon to express an unbounded set of emerging ideas. One result of this tension is the formation of new compositions, such that existing linguistic units can be combined with emerging items into novel expressions. We develop a framework that exploits the... | Lei Yu, Yang Xu |  |
| 541 |  |  [Frequency Effects on Syntactic Rule Learning in Transformers](https://doi.org/10.18653/v1/2021.emnlp-main.72) |  | 0 | Pre-trained language models perform well on a variety of linguistic tasks that require symbolic reasoning, raising the question of whether such models implicitly represent abstract symbols and rules. We investigate this question using the case study of BERT’s performance on English subject–verb... | Jason Wei, Dan Garrette, Tal Linzen, Ellie Pavlick |  |
| 542 |  |  [A surprisal-duration trade-off across and within the world's languages](https://doi.org/10.18653/v1/2021.emnlp-main.73) |  | 0 | While there exist scores of natural languages, each with its unique features and idiosyncrasies, they all share a unifying theme: enabling human communication. We may thus reasonably predict that human cognition shapes how these languages evolve and are used. Assuming that the capacity to process... | Tiago Pimentel, Clara Meister, Elizabeth Salesky, Simone Teufel, Damián E. Blasi, Ryan Cotterell |  |
| 543 |  |  [Revisiting the Uniform Information Density Hypothesis](https://doi.org/10.18653/v1/2021.emnlp-main.74) |  | 0 | The uniform information density (UID) hypothesis posits a preference among language users for utterances structured such that information is distributed uniformly across a signal. While its implications on language production have been well explored, the hypothesis potentially makes predictions... | Clara Meister, Tiago Pimentel, Patrick Haller, Lena A. Jäger, Ryan Cotterell, Roger Levy |  |
| 544 |  |  [Condenser: a Pre-training Architecture for Dense Retrieval](https://doi.org/10.18653/v1/2021.emnlp-main.75) |  | 0 | Pre-trained Transformer language models (LM) have become go-to text representation encoders. Prior research fine-tunes deep LMs to encode text sequences such as sentences and passages into single dense vector representations for efficient text comparison and retrieval. However, dense encoders... | Luyu Gao, Jamie Callan |  |
| 545 |  |  [Monitoring geometrical properties of word embeddings for detecting the emergence of new topics](https://doi.org/10.18653/v1/2021.emnlp-main.76) |  | 0 | Slow emerging topic detection is a task between event detection, where we aggregate behaviors of different words on short period of time, and language evolution, where we monitor their long term evolution. In this work, we tackle the problem of early detection of slowly emerging new topics. To this... | Clément Christophe, Julien Velcin, Jairo Cugliari, Manel Boumghar, Philippe Suignard |  |
| 546 |  |  [Contextualized Query Embeddings for Conversational Search](https://doi.org/10.18653/v1/2021.emnlp-main.77) |  | 0 | This paper describes a compact and effective model for low-latency passage retrieval in conversational search based on learned dense representations. Prior to our work, the state-of-the-art approach uses a multi-stage pipeline comprising conversational query reformulation and information retrieval... | ShengChieh Lin, JhengHong Yang, Jimmy Lin |  |
| 547 |  |  [Ultra-High Dimensional Sparse Representations with Binarization for Efficient Text Retrieval](https://doi.org/10.18653/v1/2021.emnlp-main.78) |  | 0 | The semantic matching capabilities of neural information retrieval can ameliorate synonymy and polysemy problems of symbolic approaches. However, neural models’ dense representations are more suitable for re-ranking, due to their inefficiency. Sparse representations, either in symbolic or latent... | Kyoungrok Jang, Junmo Kang, Giwon Hong, SungHyon Myaeng, Joohee Park, Taewon Yoon, HeeCheol Seo |  |
| 548 |  |  [IR like a SIR: Sense-enhanced Information Retrieval for Multiple Languages](https://doi.org/10.18653/v1/2021.emnlp-main.79) |  | 0 | With the advent of contextualized embeddings, attention towards neural ranking approaches for Information Retrieval increased considerably. However, two aspects have remained largely neglected: i) queries usually consist of few keywords only, which increases ambiguity and makes their... | Rexhina Blloshmi, Tommaso Pasini, Niccolò Campolungo, Somnath Banerjee, Roberto Navigli, Gabriella Pasi |  |
| 549 |  |  [Neural Attention-Aware Hierarchical Topic Model](https://doi.org/10.18653/v1/2021.emnlp-main.80) |  | 0 | Neural topic models (NTMs) apply deep neural networks to topic modelling. Despite their success, NTMs generally ignore two important aspects: (1) only document-level word count information is utilized for the training, while more fine-grained sentence-level information is ignored, and (2) external... | Yuan Jin, He Zhao, Ming Liu, Lan Du, Wray L. Buntine |  |
| 550 |  |  [Relational World Knowledge Representation in Contextual Language Models: A Review](https://doi.org/10.18653/v1/2021.emnlp-main.81) |  | 0 | Relational knowledge bases (KBs) are commonly used to represent world knowledge in machines. However, while advantageous for their high degree of precision and interpretability, KBs are usually organized according to manually-defined schemas, which limit their expressiveness and require significant... | Tara Safavi, Danai Koutra |  |
| 551 |  |  [Certified Robustness to Programmable Transformations in LSTMs](https://doi.org/10.18653/v1/2021.emnlp-main.82) |  | 0 | Deep neural networks for natural language processing are fragile in the face of adversarial examples—small input perturbations, like synonym substitution or word duplication, which cause a neural network to change its prediction. We present an approach to certifying the robustness of LSTMs (and... | Yuhao Zhang, Aws Albarghouthi, Loris D'Antoni |  |
| 552 |  |  [ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.83) |  | 0 | Automatic construction of relevant Knowledge Bases (KBs) from text, and generation of semantically meaningful text from KBs are both long-standing goals in Machine Learning. In this paper, we present ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning to improve... | Pierre L. Dognin, Inkit Padhi, Igor Melnyk, Payel Das |  |
| 553 |  |  [Contrastive Out-of-Distribution Detection for Pretrained Transformers](https://doi.org/10.18653/v1/2021.emnlp-main.84) |  | 0 | Pretrained Transformers achieve remarkable performance when training and test data are from the same distribution. However, in real-world scenarios, the model often faces out-of-distribution (OOD) instances that can cause severe semantic shift problems at inference time. Therefore, in practice, a... | Wenxuan Zhou, Fangyu Liu, Muhao Chen |  |
| 554 |  |  [MindCraft: Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks](https://doi.org/10.18653/v1/2021.emnlp-main.85) |  | 0 | An ideal integration of autonomous agents in a human world implies that they are able to collaborate on human terms. In particular, theory of mind plays an important role in maintaining common ground during human collaboration and communication. To enable theory of mind modeling in situated... | CristianPaul Bara, Sky CHWang, Joyce Chai |  |
| 555 |  |  [Detecting Speaker Personas from Conversational Texts](https://doi.org/10.18653/v1/2021.emnlp-main.86) |  | 0 | Personas are useful for dialogue response prediction. However, the personas used in current studies are pre-defined and hard to obtain before a conversation. To tackle this issue, we study a new task, named Speaker Persona Detection (SPD), which aims to detect speaker personas based on the plain... | JiaChen Gu, ZhenHua Ling, Yu Wu, Quan Liu, Zhigang Chen, Xiaodan Zhu |  |
| 556 |  |  [Cross-lingual Intermediate Fine-tuning improves Dialogue State Tracking](https://doi.org/10.18653/v1/2021.emnlp-main.87) |  | 0 | Recent progress in task-oriented neural dialogue systems is largely focused on a handful of languages, as annotation of training data is tedious and expensive. Machine translation has been used to make systems multilingual, but this can introduce a pipeline of errors. Another promising solution is... | Nikita Moghe, Mark Steedman, Alexandra Birch |  |
| 557 |  |  [ConvFiT: Conversational Fine-Tuning of Pretrained Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.88) |  | 0 | Transformer-based language models (LMs) pretrained on large text collections are proven to store a wealth of semantic knowledge. However, 1) they are not effective as sentence encoders when used off-the-shelf, and 2) thus typically lag behind conversationally pretrained (e.g., via response... | Ivan Vulic, PeiHao Su, Samuel Coope, Daniela Gerz, Pawel Budzianowski, Iñigo Casanueva, Nikola Mrksic, TsungHsien Wen |  |
| 558 |  |  [We've had this conversation before: A Novel Approach to Measuring Dialog Similarity](https://doi.org/10.18653/v1/2021.emnlp-main.89) |  | 0 | Dialog is a core building block of human natural language interactions. It contains multi-party utterances used to convey information from one party to another in a dynamic and evolving manner. The ability to compare dialogs is beneficial in many real world use cases, such as conversation analytics... | Ofer Lavi, Ella Rabinovich, Segev Shlomov, David Boaz, Inbal Ronen, Ateret AnabyTavor |  |
| 559 |  |  [Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental NLU](https://doi.org/10.18653/v1/2021.emnlp-main.90) |  | 0 | Incremental processing allows interactive systems to respond based on partial inputs, which is a desirable property e.g. in dialogue agents. The currently popular Transformer architecture inherently processes sequences as a whole, abstracting away the notion of time. Recent work attempts to apply... | Patrick Kahardipraja, Brielen Madureira, David Schlangen |  |
| 560 |  |  [Feedback Attribution for Counterfactual Bandit Learning in Multi-Domain Spoken Language Understanding](https://doi.org/10.18653/v1/2021.emnlp-main.91) |  | 0 | With counterfactual bandit learning, models can be trained based on positive and negative feedback received for historical predictions, with no labeled data needed. Such feedback is often available in real-world dialog systems, however, the modularized architecture commonly used in large-scale... | Tobias Falke, Patrick Lehnen |  |
| 561 |  |  [Label Verbalization and Entailment for Effective Zero and Few-Shot Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.92) |  | 0 | Relation extraction systems require large amounts of labeled examples which are costly to annotate. In this work we reformulate relation extraction as an entailment task, with simple, hand-made, verbalizations of relations produced in less than 15 min per relation. The system relies on a pretrained... | Oscar Sainz, Oier Lopez de Lacalle, Gorka Labaka, Ander Barrena, Eneko Agirre |  |
| 562 |  |  [Extend, don't rebuild: Phrasing conditional graph modification as autoregressive sequence labelling](https://doi.org/10.18653/v1/2021.emnlp-main.93) |  | 0 | Deriving and modifying graphs from natural language text has become a versatile basis technology for information extraction with applications in many subfields, such as semantic parsing or knowledge graph construction. A recent work used this technique for modifying scene graphs (He et al. 2020),... | Leon Weber, Jannes Münchmeyer, Samuele Garda, Ulf Leser |  |
| 563 |  |  [Zero-Shot Information Extraction as a Unified Text-to-Triple Translation](https://doi.org/10.18653/v1/2021.emnlp-main.94) |  | 0 | We cast a suite of information extraction tasks into a text-to-triple translation framework. Instead of solving each task relying on task-specific datasets and models, we formalize the task as a translation between task-specific input text and output triples. By taking the task-specific input, we... | Chenguang Wang, Xiao Liu, Zui Chen, Haoyun Hong, Jie Tang, Dawn Song |  |
| 564 |  |  [Learning Logic Rules for Document-Level Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.95) |  | 0 | Document-level relation extraction aims to identify relations between entities in a whole document. Prior efforts to capture long-range dependencies have relied heavily on implicitly powerful representations learned through (graph) neural networks, which makes the model less transparent. To tackle... | Dongyu Ru, Changzhi Sun, Jiangtao Feng, Lin Qiu, Hao Zhou, Weinan Zhang, Yong Yu, Lei Li |  |
| 565 |  |  [A Large-Scale Dataset for Empathetic Response Generation](https://doi.org/10.18653/v1/2021.emnlp-main.96) |  | 0 | Recent development in NLP shows a strong trend towards refining pre-trained models with a domain-specific dataset. This is especially the case for response generation where emotion plays an important role. However, existing empathetic datasets remain small, delaying research efforts in this area,... | Anuradha Welivita, Yubo Xie, Pearl Pu |  |
| 566 |  |  [The Perils of Using Mechanical Turk to Evaluate Open-Ended Text Generation](https://doi.org/10.18653/v1/2021.emnlp-main.97) |  | 0 | Recent text generation research has increasingly focused on open-ended domains such as story and poetry generation. Because models built for such tasks are difficult to evaluate automatically, most researchers in the space justify their modeling choices by collecting crowdsourced human judgments of... | Marzena Karpinska, Nader Akoury, Mohit Iyyer |  |
| 567 |  |  [Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus](https://doi.org/10.18653/v1/2021.emnlp-main.98) |  | 0 | Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal... | Jesse Dodge, Maarten Sap, Ana Marasovic, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, Matt Gardner |  |
| 568 |  |  [AfroMT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages](https://doi.org/10.18653/v1/2021.emnlp-main.99) |  | 0 | Reproducible benchmarks are crucial in driving progress of machine translation research. However, existing machine translation benchmarks have been mostly limited to high-resource or well-represented languages. Despite an increasing interest in low-resource machine translation, there are no... | Machel Reid, Junjie Hu, Graham Neubig, Yutaka Matsuo |  |
| 569 |  |  [Evaluating the Evaluation Metrics for Style Transfer: A Case Study in Multilingual Formality Transfer](https://doi.org/10.18653/v1/2021.emnlp-main.100) |  | 0 | While the field of style transfer (ST) has been growing rapidly, it has been hampered by a lack of standardized practices for automatic evaluation. In this paper, we evaluate leading automatic metrics on the oft-researched task of formality style transfer. Unlike previous evaluations, which focus... | Eleftheria Briakou, Sweta Agrawal, Joel R. Tetreault, Marine Carpuat |  |
| 570 |  |  [MS-Mentions: Consistently Annotating Entity Mentions in Materials Science Procedural Text](https://doi.org/10.18653/v1/2021.emnlp-main.101) |  | 0 | Material science synthesis procedures are a promising domain for scientific NLP, as proper modeling of these recipes could provide insight into new ways of creating materials. However, a fundamental challenge in building information extraction models for material science synthesis procedures is... | Tim O'Gorman, Zach Jensen, Sheshera Mysore, Kevin Huang, Rubayyat Mahbub, Elsa A. Olivetti, Andrew McCallum |  |
| 571 |  |  [Understanding Politics via Contextualized Discourse Processing](https://doi.org/10.18653/v1/2021.emnlp-main.102) |  | 0 | Politicians often have underlying agendas when reacting to events. Arguments in contexts of various events reflect a fairly consistent set of agendas for a given entity. In spite of recent advances in Pretrained Language Models, those text representations are not designed to capture such nuanced... | Rajkumar Pujari, Dan Goldwasser |  |
| 572 |  |  [Conundrums in Event Coreference Resolution: Making Sense of the State of the Art](https://doi.org/10.18653/v1/2021.emnlp-main.103) |  | 0 | Despite recent promising results on the application of span-based models for event reference interpretation, there is a lack of understanding of what has been improved. We present an empirical analysis of a state-of-the-art span-based event reference systems with the goal of providing the general... | Jing Lu, Vincent Ng |  |
| 573 |  |  [Weakly supervised discourse segmentation for multiparty oral conversations](https://doi.org/10.18653/v1/2021.emnlp-main.104) |  | 0 | Discourse segmentation, the first step of discourse analysis, has been shown to improve results for text summarization, translation and other NLP tasks. While segmentation models for written text tend to perform well, they are not directly applicable to spontaneous, oral conversation, which has... | Lila Gravellier, Julie Hunter, Philippe Muller, Thomas Pellegrini, Isabelle Ferrané |  |
| 574 |  |  [Narrative Embedding: Re-Contextualization Through Attention](https://doi.org/10.18653/v1/2021.emnlp-main.105) |  | 0 | Narrative analysis is becoming increasingly important for a number of linguistic tasks including summarization, knowledge extraction, and question answering. We present a novel approach for narrative event representation using attention to re-contextualize events across the whole story. Comparing... | Sean Wilner, Daniel Woolridge, Madeleine Glick |  |
| 575 |  |  [Focus on what matters: Applying Discourse Coherence Theory to Cross Document Coreference](https://doi.org/10.18653/v1/2021.emnlp-main.106) |  | 0 | Performing event and entity coreference resolution across documents vastly increases the number of candidate mentions, making it intractable to do the full n2 pairwise comparisons. Existing approaches simplify by considering coreference only within document clusters, but this fails to handle... | William Held, Dan Iter, Dan Jurafsky |  |
| 576 |  |  [Salience-Aware Event Chain Modeling for Narrative Understanding](https://doi.org/10.18653/v1/2021.emnlp-main.107) |  | 0 | Storytelling, whether via fables, news reports, documentaries, or memoirs, can be thought of as the communication of interesting and related events that, taken together, form a concrete process. It is desirable to extract the event chains that represent such processes. However, this extraction... | Xiyang Zhang, Muhao Chen, Jonathan May |  |
| 577 |  |  [Asking It All: Generating Contextualized Questions for any Semantic Role](https://doi.org/10.18653/v1/2021.emnlp-main.108) |  | 0 | Asking questions about a situation is an inherent step towards understanding it. To this end, we introduce the task of role question generation, which, given a predicate mention and a passage, requires producing a set of questions asking about all possible semantic roles of the predicate. We... | Valentina Pyatkin, Paul Roit, Julian Michael, Yoav Goldberg, Reut Tsarfaty, Ido Dagan |  |
| 578 |  |  [Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders](https://doi.org/10.18653/v1/2021.emnlp-main.109) |  | 0 | Previous work has indicated that pretrained Masked Language Models (MLMs) are not effective as universal lexical and sentence encoders off-the-shelf, i.e., without further task-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks using annotated task data. In this work, we... | Fangyu Liu, Ivan Vulic, Anna Korhonen, Nigel Collier |  |
| 579 |  |  [RuleBERT: Teaching Soft Rules to Pre-Trained Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.110) |  | 0 | While pre-trained language models (PLMs) are the go-to solution to tackle many natural language processing problems, they are still very limited in their ability to capture and to use common-sense knowledge. In fact, even if information is available in the form of approximate (soft) logical rules,... | Mohammed Saeed, Naser Ahmadi, Preslav Nakov, Paolo Papotti |  |
| 580 |  |  [Stepmothers are mean and academics are pretentious: What do pretrained language models learn about you?](https://doi.org/10.18653/v1/2021.emnlp-main.111) |  | 0 | In this paper, we investigate what types of stereotypical information are captured by pretrained language models. We present the first dataset comprising stereotypical attributes of a range of social groups and propose a method to elicit stereotypes encoded by pretrained language models in an... | Rochelle Choenni, Ekaterina Shutova, Robert van Rooij |  |
| 581 |  |  [ConSeC: Word Sense Disambiguation as Continuous Sense Comprehension](https://doi.org/10.18653/v1/2021.emnlp-main.112) |  | 0 | Supervised systems have nowadays become the standard recipe for Word Sense Disambiguation (WSD), with Transformer-based language models as their primary ingredient. However, while these systems have certainly attained unprecedented performances, virtually all of them operate under the constraining... | Edoardo Barba, Luigi Procopio, Roberto Navigli |  |
| 582 |  |  [Shortcutted Commonsense: Data Spuriousness in Deep Learning of Commonsense Reasoning](https://doi.org/10.18653/v1/2021.emnlp-main.113) |  | 0 | Commonsense is a quintessential human capacity that has been a core challenge to Artificial Intelligence since its inception. Impressive results in Natural Language Processing tasks, including in commonsense reasoning, have consistently been achieved with Transformer neural language models, even... | Ruben Branco, António Branco, João António Rodrigues, João Ricardo Silva |  |
| 583 |  |  [When differential privacy meets NLP: The devil is in the detail](https://doi.org/10.18653/v1/2021.emnlp-main.114) |  | 0 | Differential privacy provides a formal approach to privacy of individuals. Applications of differential privacy in various scenarios, such as protecting users’ original utterances, must satisfy certain mathematical properties. Our contribution is a formal analysis of ADePT, a differentially private... | Ivan Habernal |  |
| 584 |  |  [Achieving Model Robustness through Discrete Adversarial Training](https://doi.org/10.18653/v1/2021.emnlp-main.115) |  | 0 | Discrete adversarial attacks are symbolic perturbations to a language input that preserve the output label but lead to a prediction error. While such attacks have been extensively explored for the purpose of evaluating model robustness, their utility for improving robustness has been limited to... | Maor Ivgi, Jonathan Berant |  |
| 585 |  |  [Debiasing Methods in Natural Language Understanding Make Bias More Accessible](https://doi.org/10.18653/v1/2021.emnlp-main.116) |  | 0 | Model robustness to bias is often determined by the generalization on carefully designed out-of-distribution datasets. Recent debiasing methods in natural language understanding (NLU) improve performance on such datasets by pressuring models into making unbiased predictions. An underlying... | Michael Mendelson, Yonatan Belinkov |  |
| 586 |  |  [Evaluating the Robustness of Neural Language Models to Input Perturbations](https://doi.org/10.18653/v1/2021.emnlp-main.117) |  | 0 | High-performance neural language models have obtained state-of-the-art results on a wide range of Natural Language Processing (NLP) tasks. However, results for common benchmark datasets often do not reflect model reliability and robustness when applied to noisy, real-world data. In this study, we... | Milad Moradi, Matthias Samwald |  |
| 587 |  |  [How much pretraining data do language models need to learn syntax?](https://doi.org/10.18653/v1/2021.emnlp-main.118) |  | 0 | Transformers-based pretrained language models achieve outstanding results in many well-known NLU benchmarks. However, while pretraining methods are very convenient, they are expensive in terms of time and resources. This calls for a study of the impact of pretraining data size on the knowledge of... | Laura PérezMayos, Miguel Ballesteros, Leo Wanner |  |
| 588 |  |  [Sorting through the noise: Testing robustness of information processing in pre-trained language models](https://doi.org/10.18653/v1/2021.emnlp-main.119) |  | 0 | Pre-trained LMs have shown impressive performance on downstream NLP tasks, but we have yet to establish a clear understanding of their sophistication when it comes to processing, retaining, and applying information presented in their input. In this paper we tackle a component of this question by... | Lalchand Pandia, Allyson Ettinger |  |
| 589 |  |  [Contrastive Explanations for Model Interpretability](https://doi.org/10.18653/v1/2021.emnlp-main.120) |  | 0 | Contrastive explanations clarify why an event occurred in contrast to another. They are inherently intuitive to humans to both produce and comprehend. We propose a method to produce contrastive explanations in the latent space, via a projection of the input representation, such that only the... | Alon Jacovi, Swabha Swayamdipta, Shauli Ravfogel, Yanai Elazar, Yejin Choi, Yoav Goldberg |  |
| 590 |  |  [On the Transferability of Adversarial Attacks against Neural Text Classifier](https://doi.org/10.18653/v1/2021.emnlp-main.121) |  | 0 | Deep neural networks are vulnerable to adversarial attacks, where a small perturbation to an input alters the model prediction. In many cases, malicious inputs intentionally crafted for one model can fool another model. In this paper, we present the first study to systematically investigate the... | Liping Yuan, Xiaoqing Zheng, Yi Zhou, ChoJui Hsieh, KaiWei Chang |  |
| 591 |  |  [Conditional probing: measuring usable information beyond a baseline](https://doi.org/10.18653/v1/2021.emnlp-main.122) |  | 0 | Probing experiments investigate the extent to which neural representations make properties—like part-of-speech—predictable. One suggests that a representation encodes a property if probing that representation produces higher accuracy than probing a baseline representation like non-contextual word... | John Hewitt, Kawin Ethayarajh, Percy Liang, Christopher D. Manning |  |
| 592 |  |  [GFST: Gender-Filtered Self-Training for More Accurate Gender in Translation](https://doi.org/10.18653/v1/2021.emnlp-main.123) |  | 0 | Targeted evaluations have found that machine translation systems often output incorrect gender in translations, even when the gender is clear from context. Furthermore, these incorrectly gendered translations have the potential to reflect or amplify social biases. We propose gender-filtered... | Prafulla Kumar Choubey, Anna Currey, Prashant Mathur, Georgiana Dinu |  |
| 593 |  |  ["Wikily" Supervised Neural Translation Tailored to Cross-Lingual Tasks](https://doi.org/10.18653/v1/2021.emnlp-main.124) |  | 0 | We present a simple but effective approach for leveraging Wikipedia for neural machine translation as well as cross-lingual tasks of image captioning and dependency parsing without using any direct supervision from external parallel data or supervised models in the target language. We show that... | Mohammad Sadegh Rasooli, Chris CallisonBurch, Derry Tanti Wijaya |  |
| 594 |  |  [mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs](https://doi.org/10.18653/v1/2021.emnlp-main.125) |  | 0 | Multilingual T5 pretrains a sequence-to-sequence model on massive monolingual texts, which has shown promising results on many cross-lingual tasks. In this paper, we improve multilingual text-to-text transfer Transformer with translation pairs (mT6). Specifically, we explore three cross-lingual... | Zewen Chi, Li Dong, Shuming Ma, Shaohan Huang, Saksham Singhal, XianLing Mao, Heyan Huang, Xia Song, Furu Wei |  |
| 595 |  |  [Improving Zero-Shot Cross-Lingual Transfer Learning via Robust Training](https://doi.org/10.18653/v1/2021.emnlp-main.126) |  | 0 | Pre-trained multilingual language encoders, such as multilingual BERT and XLM-R, show great potential for zero-shot cross-lingual transfer. However, these multilingual encoders do not precisely align words and phrases across languages. Especially, learning alignments in the multilingual embedding... | KuanHao Huang, Wasi Uddin Ahmad, Nanyun Peng, KaiWei Chang |  |
| 596 |  |  [Speechformer: Reducing Information Loss in Direct Speech Translation](https://doi.org/10.18653/v1/2021.emnlp-main.127) |  | 0 | Transformer-based models have gained increasing popularity achieving state-of-the-art performance in many research fields including speech translation. However, Transformer’s quadratic complexity with respect to the input sequence length prevents its adoption as is with audio signals, which are... | Sara Papi, Marco Gaido, Matteo Negri, Marco Turchi |  |
| 597 |  |  [Is "moby dick" a Whale or a Bird? Named Entities and Terminology in Speech Translation](https://doi.org/10.18653/v1/2021.emnlp-main.128) |  | 0 | Automatic translation systems are known to struggle with rare words. Among these, named entities (NEs) and domain-specific terms are crucial, since errors in their translation can lead to severe meaning distortions. Despite their importance, previous speech translation (ST) studies have neglected... | Marco Gaido, Susana Rodríguez, Matteo Negri, Luisa Bentivogli, Marco Turchi |  |
| 598 |  |  [HintedBT: Augmenting Back-Translation with Quality and Transliteration Hints](https://doi.org/10.18653/v1/2021.emnlp-main.129) |  | 0 | Back-translation (BT) of target monolingual corpora is a widely used data augmentation strategy for neural machine translation (NMT), especially for low-resource language pairs. To improve effectiveness of the available BT data, we introduce HintedBT—a family of techniques which provides hints... | Sahana Ramnath, Melvin Johnson, Abhirut Gupta, Aravindan Raghuveer |  |
| 599 |  |  [Translation-based Supervision for Policy Generation in Simultaneous Neural Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.130) |  | 0 | In simultaneous machine translation, finding an agent with the optimal action sequence of reads and writes that maintain a high level of translation quality while minimizing the average lag in producing target tokens remains an extremely challenging problem. We propose a novel supervised learning... | Ashkan Alinejad, Hassan S. Shavarani, Anoop Sarkar |  |
| 600 |  |  [Nearest Neighbour Few-Shot Learning for Cross-lingual Classification](https://doi.org/10.18653/v1/2021.emnlp-main.131) |  | 0 | Even though large pre-trained multilingual models (e.g. mBERT, XLM-R) have led to significant performance gains on a wide range of cross-lingual NLP tasks, success on many downstream tasks still relies on the availability of sufficient annotated data. Traditional fine-tuning of pre-trained models... | M. Saiful Bari, Batool Haider, Saab Mansour |  |
| 601 |  |  [Cross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.132) |  | 0 | We study the power of cross-attention in the Transformer architecture within the context of transfer learning for machine translation, and extend the findings of studies into cross-attention when training from scratch. We conduct a series of experiments through fine-tuning a translation model on... | Mozhdeh Gheini, Xiang Ren, Jonathan May |  |
| 602 |  |  [Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent](https://doi.org/10.18653/v1/2021.emnlp-main.133) |  | 0 | The capacity of neural networks like the widely adopted transformer is known to be very high. Evidence is emerging that they learn successfully due to inductive bias in the training routine, typically a variant of gradient descent (GD). To better understand this bias, we study the tendency for... | William Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, Noah A. Smith |  |
| 603 |  |  [Foreseeing the Benefits of Incidental Supervision](https://doi.org/10.18653/v1/2021.emnlp-main.134) |  | 0 | Real-world applications often require improved models by leveraging \*a range of cheap incidental supervision signals\*. These could include partial labels, noisy labels, knowledge-based constraints, and cross-domain or cross-task annotations – all having statistical associations with gold... | Hangfeng He, Mingyuan Zhang, Qiang Ning, Dan Roth |  |
| 604 |  |  [Competency Problems: On Finding and Removing Artifacts in Language Data](https://doi.org/10.18653/v1/2021.emnlp-main.135) |  | 0 | Much recent work in NLP has documented dataset artifacts, bias, and spurious correlations between input features and output labels. However, how to tell which features have “spurious” instead of legitimate correlations is typically left unspecified. In this work we argue that for complex language... | Matt Gardner, William Merrill, Jesse Dodge, Matthew E. Peters, Alexis Ross, Sameer Singh, Noah A. Smith |  |
| 605 |  |  [Knowledge-Aware Meta-learning for Low-Resource Text Classification](https://doi.org/10.18653/v1/2021.emnlp-main.136) |  | 0 | Meta-learning has achieved great success in leveraging the historical learned knowledge to facilitate the learning process of the new task. However, merely learning the knowledge from the historical tasks, adopted by current meta-learning algorithms, may not generalize well to testing tasks when... | Huaxiu Yao, Yingxin Wu, Maruan AlShedivat, Eric P. Xing |  |
| 606 |  |  [Sentence Bottleneck Autoencoders from Transformer Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.137) |  | 0 | Representation learning for text via pretraining a language model on a large corpus has become a standard starting point for building NLP systems. This approach stands in contrast to autoencoders, also trained on raw text, but with the objective of learning to encode each input as a vector that... | Ivan Montero, Nikolaos Pappas, Noah A. Smith |  |
| 607 |  |  [Efficient Contrastive Learning via Novel Data Augmentation and Curriculum Learning](https://doi.org/10.18653/v1/2021.emnlp-main.138) |  | 0 | We introduce EfficientCL, a memory-efficient continual pretraining method that applies contrastive learning with novel data augmentation and curriculum learning. For data augmentation, we stack two types of operation sequentially: cutoff and PCA jittering. While pretraining steps proceed, we apply... | Seonghyeon Ye, Jiseon Kim, Alice Oh |  |
| 608 |  |  [CR-Walker: Tree-Structured Graph Reasoning and Dialog Acts for Conversational Recommendation](https://doi.org/10.18653/v1/2021.emnlp-main.139) |  | 0 | Growing interests have been attracted in Conversational Recommender Systems (CRS), which explore user preference through conversational interactions in order to make appropriate recommendation. However, there is still a lack of ability in existing CRS to (1) traverse multiple reasoning paths over... | Wenchang Ma, Ryuichi Takanobu, Minlie Huang |  |
| 609 |  |  [DIALKI: Knowledge Identification in Conversational Systems through Dialogue-Document Contextualization](https://doi.org/10.18653/v1/2021.emnlp-main.140) |  | 0 | Identifying relevant knowledge to be used in conversational systems that are grounded in long documents is critical to effective response generation. We introduce a knowledge identification model that leverages the document structure to provide dialogue-contextualized passage encodings and better... | Zeqiu Wu, BoRu Lu, Hannaneh Hajishirzi, Mari Ostendorf |  |
| 610 |  |  [Iconary: A Pictionary-Based Game for Testing Multimodal Communication with Drawings and Text](https://doi.org/10.18653/v1/2021.emnlp-main.141) |  | 0 | Communicating with humans is challenging for AIs because it requires a shared understanding of the world, complex semantics (e.g., metaphors or analogies), and at times multi-modal gestures (e.g., pointing with a finger, or an arrow in a diagram). We investigate these challenges in the context of... | Christopher Clark, Jordi Salvador, Dustin Schwenk, Derrick Bonafilia, Mark Yatskar, Eric Kolve, Alvaro Herrasti, Jonghyun Choi, Sachin Mehta, Sam Skjonsberg, Carissa Schoenick, Aaron Sarnat, Hannaneh Hajishirzi, Aniruddha Kembhavi, Oren Etzioni, Ali Farhadi |  |
| 611 |  |  [Self-training Improves Pre-training for Few-shot Learning in Task-oriented Dialog Systems](https://doi.org/10.18653/v1/2021.emnlp-main.142) |  | 0 | As the labeling cost for different modules in task-oriented dialog (ToD) systems is expensive, a major challenge is to train different modules with the least amount of labeled data. Recently, large-scale pre-trained language models, have shown promising results for few-shot learning in ToD. In this... | Fei Mi, Wanhao Zhou, Lingjing Kong, Fengyu Cai, Minlie Huang, Boi Faltings |  |
| 612 |  |  [Contextual Rephrase Detection for Reducing Friction in Dialogue Systems](https://doi.org/10.18653/v1/2021.emnlp-main.143) |  | 0 | For voice assistants like Alexa, Google Assistant, and Siri, correctly interpreting users’ intentions is of utmost importance. However, users sometimes experience friction with these assistants, caused by errors from different system components or user errors such as slips of the tongue. Users tend... | Zhuoyi Wang, Saurabh Gupta, Jie Hao, Xing Fan, Dingcheng Li, Alexander Hanbo Li, Chenlei Guo |  |
| 613 |  |  [Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning](https://doi.org/10.18653/v1/2021.emnlp-main.144) |  | 0 | In this work, we focus on a more challenging few-shot intent detection scenario where many intents are fine-grained and semantically similar. We present a simple yet effective few-shot intent detection schema via contrastive pre-training and fine-tuning. Specifically, we first conduct... | JianGuo Zhang, Trung Bui, Seunghyun Yoon, Xiang Chen, Zhiwei Liu, Congying Xia, Quan Hung Tran, Walter Chang, Philip S. Yu |  |
| 614 |  |  ["It doesn't look good for a date": Transforming Critiques into Preferences for Conversational Recommendation Systems](https://doi.org/10.18653/v1/2021.emnlp-main.145) |  | 0 | Conversations aimed at determining good recommendations are iterative in nature. People often express their preferences in terms of a critique of the current recommendation (e.g., “It doesn’t look good for a date”), requiring some degree of common sense for a preference to be inferred. In this... | Victor S. Bursztyn, Jennifer Healey, Nedim Lipka, Eunyee Koh, Doug Downey, Larry Birnbaum |  |
| 615 |  |  [AttentionRank: Unsupervised Keyphrase Extraction using Self and Cross Attentions](https://doi.org/10.18653/v1/2021.emnlp-main.146) |  | 0 | Keyword or keyphrase extraction is to identify words or phrases presenting the main topics of a document. This paper proposes the AttentionRank, a hybrid attention model, to identify keyphrases from a document in an unsupervised manner. AttentionRank calculates self-attention and cross-attention... | Haoran Ding, Xiao Luo |  |
| 616 |  |  [Unsupervised Relation Extraction: A Variational Autoencoder Approach](https://doi.org/10.18653/v1/2021.emnlp-main.147) |  | 0 | Unsupervised relation extraction works by clustering entity pairs that have the same relations in the text. Some existing variational autoencoder (VAE)-based approaches train the relation extraction model as an encoder that generates relation classifications. A decoder is trained along with the... | Chenhan Yuan, Hoda Eldardiry |  |
| 617 |  |  [Robust Retrieval Augmented Generation for Zero-shot Slot Filling](https://doi.org/10.18653/v1/2021.emnlp-main.148) |  | 0 | Automatically inducing high quality knowledge graphs from a given collection of documents still remains a challenging problem in AI. One way to make headway for this problem is through advancements in a related task known as slot filling. In this task, given an entity query in form of [Entity,... | Michael R. Glass, Gaetano Rossiello, Md. Faisal Mahbub Chowdhury, Alfio Gliozzo |  |
| 618 |  |  [Everything Is All It Takes: A Multipronged Strategy for Zero-Shot Cross-Lingual Information Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.149) |  | 0 | Zero-shot cross-lingual information extraction (IE) describes the construction of an IE model for some target language, given existing annotations exclusively in some other language, typically English. While the advance of pretrained multilingual encoders suggests an easy optimism of “train on... | Mahsa Yarmohammadi, Shijie Wu, Marc Marone, Haoran Xu, Seth Ebner, Guanghui Qin, Yunmo Chen, Jialiang Guo, Craig Harman, Kenton Murray, Aaron Steven White, Mark Dredze, Benjamin Van Durme |  |
| 619 |  |  [Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies](https://doi.org/10.18653/v1/2021.emnlp-main.150) |  | 0 | Gender is widely discussed in the context of language tasks and when examining the stereotypes propagated by language models. However, current discussions primarily treat gender as binary, which can perpetuate harms such as the cyclical erasure of non-binary gender identities. These harms are... | Sunipa Dev, Masoud Monajatipoor, Anaelia Ovalle, Arjun Subramonian, Jeff M. Phillips, KaiWei Chang |  |
| 620 |  |  [Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search](https://doi.org/10.18653/v1/2021.emnlp-main.151) |  | 0 | Internet search affects people’s cognition of the world, so mitigating biases in search results and learning fair models is imperative for social good. We study a unique gender bias in image search in this work: the search images are often gender-imbalanced for gender-neutral natural language... | Jialu Wang, Yang Liu, Xin Eric Wang |  |
| 621 |  |  [Style Pooling: Automatic Text Style Obfuscation for Improved Classification Fairness](https://doi.org/10.18653/v1/2021.emnlp-main.152) |  | 0 | Text style can reveal sensitive attributes of the author (e.g. age and race) to the reader, which can, in turn, lead to privacy violations and bias in both human and algorithmic decisions based on text. For example, the style of writing in job applications might reveal protected attributes of the... | Fatemehsadat Mireshghallah, Taylor BergKirkpatrick |  |
| 622 |  |  [Modeling Disclosive Transparency in NLP Application Descriptions](https://doi.org/10.18653/v1/2021.emnlp-main.153) |  | 0 | Broader disclosive transparency—truth and clarity in communication regarding the function of AI systems—is widely considered desirable. Unfortunately, it is a nebulous concept, difficult to both define and quantify. This is problematic, as previous work has demonstrated possible trade-offs and... | Michael Saxon, Sharon Levy, Xinyi Wang, Alon Albalak, William Yang Wang |  |
| 623 |  |  [Reconstruction Attack on Instance Encoding for Language Understanding](https://doi.org/10.18653/v1/2021.emnlp-main.154) |  | 0 | A private learning scheme TextHide was recently proposed to protect the private text data during the training phase via so-called instance encoding. We propose a novel reconstruction attack to break TextHide by recovering the private training data, and thus unveil the privacy risks of instance... | Shangyu Xie, Yuan Hong |  |
| 624 |  |  [Fairness-aware Class Imbalanced Learning](https://doi.org/10.18653/v1/2021.emnlp-main.155) |  | 0 | Class imbalance is a common challenge in many NLP tasks, and has clear connections to bias, in that bias in training data often leads to higher accuracy for majority groups at the expense of minority groups. However there has traditionally been a disconnect between research on class-imbalanced... | Shivashankar Subramanian, Afshin Rahimi, Timothy Baldwin, Trevor Cohn, Lea Frermann |  |
| 625 |  |  [CRYPTOGRU: Low Latency Privacy-Preserving Text Analysis With GRU](https://doi.org/10.18653/v1/2021.emnlp-main.156) |  | 0 | Homomorphic encryption (HE) and garbled circuit (GC) provide the protection for users’ privacy. However, simply mixing the HE and GC in RNN models suffer from long inference latency due to slow activation functions. In this paper, we present a novel hybrid structure of HE and GC gated recurrent... | Bo Feng, Qian Lou, Lei Jiang, Geoffrey C. Fox |  |
| 626 |  |  [Local Word Discovery for Interactive Transcription](https://doi.org/10.18653/v1/2021.emnlp-main.157) |  | 0 | Human expertise and the participation of speech communities are essential factors in the success of technologies for low-resource languages. Accordingly, we propose a new computational task which is tuned to the available knowledge and interests in an Indigenous community, and which supports the... | William Lane, Steven Bird |  |
| 627 |  |  [Segment, Mask, and Predict: Augmenting Chinese Word Segmentation with Self-Supervision](https://doi.org/10.18653/v1/2021.emnlp-main.158) |  | 0 | Recent state-of-the-art (SOTA) effective neural network methods and fine-tuning methods based on pre-trained models (PTM) have been used in Chinese word segmentation (CWS), and they achieve great results. However, previous works focus on training the models with the fixed corpus at every iteration.... | Mieradilijiang Maimaiti, Yang Liu, Yuanhang Zheng, Gang Chen, Kaiyu Huang, Ji Zhang, Huanbo Luan, Maosong Sun |  |
| 628 |  |  [Minimal Supervision for Morphological Inflection](https://doi.org/10.18653/v1/2021.emnlp-main.159) |  | 0 | Neural models for the various flavours of morphological reinflection tasks have proven to be extremely accurate given ample labeled data, yet labeled data may be slow and costly to obtain. In this work we aim to overcome this annotation bottleneck by bootstrapping labeled data from a seed as small... | Omer Goldman, Reut Tsarfaty |  |
| 629 |  |  [Fast WordPiece Tokenization](https://doi.org/10.18653/v1/2021.emnlp-main.160) |  | 0 | Tokenization is a fundamental preprocessing step for almost all NLP tasks. In this paper, we propose efficient algorithms for the WordPiece tokenization used in BERT, from single-word tokenization to general text (e.g., sentence) tokenization. When tokenizing a single word, WordPiece uses a... | Xinying Song, Alex Salcianu, Yang Song, Dave Dopson, Denny Zhou |  |
| 630 |  |  [You should evaluate your language model on marginal likelihood over tokenisations](https://doi.org/10.18653/v1/2021.emnlp-main.161) |  | 0 | Neural language models typically tokenise input text into sub-word units to achieve an open vocabulary. The standard approach is to use a single canonical tokenisation at both train and test time. We suggest that this approach is unsatisfactory and may bottleneck our evaluation of language model... | Kris Cao, Laura Rimell |  |
| 631 |  |  [Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning](https://doi.org/10.18653/v1/2021.emnlp-main.162) |  | 0 | Commonsense is defined as the knowledge on which everyone agrees. However, certain types of commonsense knowledge are correlated with culture and geographic locations and they are only shared locally. For example, the scenes of wedding ceremonies vary across regions due to different customs... | Da Yin, Liunian Harold Li, Ziniu Hu, Nanyun Peng, KaiWei Chang |  |
| 632 |  |  [Reference-Centric Models for Grounded Collaborative Dialogue](https://doi.org/10.18653/v1/2021.emnlp-main.163) |  | 0 | We present a grounded neural dialogue model that successfully collaborates with people in a partially-observable reference game. We focus on a setting where two agents each observe an overlapping part of a world context and need to identify and agree on some object they share. Therefore, the agents... | Daniel Fried, Justin T. Chiu, Dan Klein |  |
| 633 |  |  [CrossVQA: Scalably Generating Benchmarks for Systematically Testing VQA Generalization](https://doi.org/10.18653/v1/2021.emnlp-main.164) |  | 0 | One challenge in evaluating visual question answering (VQA) models in the cross-dataset adaptation setting is that the distribution shifts are multi-modal, making it difficult to identify if it is the shifts in visual or language features that play a key role. In this paper, we propose a... | Arjun R. Akula, Soravit Changpinyo, Boqing Gong, Piyush Sharma, SongChun Zhu, Radu Soricut |  |
| 634 |  |  [Visual Goal-Step Inference using wikiHow](https://doi.org/10.18653/v1/2021.emnlp-main.165) |  | 0 | Understanding what sequence of steps are needed to complete a goal can help artificial intelligence systems reason about human activities. Past work in NLP has examined the task of goal-step inference for text. We introduce the visual analogue. We propose the Visual Goal-Step Inference (VGSI) task,... | Yue Yang, Artemis Panagopoulou, Qing Lyu, Li Zhang, Mark Yatskar, Chris CallisonBurch |  |
| 635 |  |  [Systematic Generalization on gSCAN: What is Nearly Solved and What is Next?](https://doi.org/10.18653/v1/2021.emnlp-main.166) |  | 0 | We analyze the grounded SCAN (gSCAN) benchmark, which was recently proposed to study systematic generalization for grounded language understanding. First, we study which aspects of the original benchmark can be solved by commonly used methods in multi-modal research. We find that a general-purpose... | Linlu Qiu, Hexiang Hu, Bowen Zhang, Peter Shaw, Fei Sha |  |
| 636 |  |  [Effect of Visual Extensions on Natural Language Understanding in Vision-and-Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.167) |  | 0 | A method for creating a vision-and-language (V&L) model is to extend a language model through structural modifications and V&L pre-training. Such an extension aims to make a V&L model inherit the capability of natural language understanding (NLU) from the original language model. To see how well... | Taichi Iki, Akiko Aizawa |  |
| 637 |  |  [Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding](https://doi.org/10.18653/v1/2021.emnlp-main.168) |  | 0 | Dialogue systems powered by large pre-trained language models exhibit an innate ability to deliver fluent and natural-sounding responses. Despite their impressive performance, these models are fitful and can often generate factually incorrect statements impeding their widespread adoption. In this... | Nouha Dziri, Andrea Madotto, Osmar Zaïane, Avishek Joey Bose |  |
| 638 |  |  [Thinking Clearly, Talking Fast: Concept-Guided Non-Autoregressive Generation for Open-Domain Dialogue Systems](https://doi.org/10.18653/v1/2021.emnlp-main.169) |  | 0 | Human dialogue contains evolving concepts, and speakers naturally associate multiple concepts to compose a response. However, current dialogue models with the seq2seq framework lack the ability to effectively manage concept transitions and can hardly introduce multiple concepts to responses in a... | Yicheng Zou, Zhihua Liu, Xingwu Hu, Qi Zhang |  |
| 639 |  |  [Perspective-taking and Pragmatics for Generating Empathetic Responses Focused on Emotion Causes](https://doi.org/10.18653/v1/2021.emnlp-main.170) |  | 0 | Empathy is a complex cognitive ability based on the reasoning of others’ affective states. In order to better understand others and express stronger empathy in dialogues, we argue that two issues must be tackled at the same time: (i) identifying which word is the cause for the other’s emotion from... | Hyunwoo Kim, Byeongchang Kim, Gunhee Kim |  |
| 640 |  |  [Generation and Extraction Combined Dialogue State Tracking with Hierarchical Ontology Integration](https://doi.org/10.18653/v1/2021.emnlp-main.171) |  | 0 | Recently, the focus of dialogue state tracking has expanded from single domain to multiple domains. The task is characterized by the shared slots between domains. As the scenario gets more complex, the out-of-vocabulary problem also becomes severer. Current models are not satisfactory for solving... | Xinmeng Li, Qian Li, Wansen Wu, Quanjun Yin |  |
| 641 |  |  [CoLV: A Collaborative Latent Variable Model for Knowledge-Grounded Dialogue Generation](https://doi.org/10.18653/v1/2021.emnlp-main.172) |  | 0 | Knowledge-grounded dialogue generation has achieved promising performance with the engagement of external knowledge sources. Typical approaches towards this task usually perform relatively independent two sub-tasks, i.e., knowledge selection and knowledge-aware response generation. In this paper,... | Haolan Zhan, Lei Shen, Hongshen Chen, Hainan Zhang |  |
| 642 |  |  [A Three-Stage Learning Framework for Low-Resource Knowledge-Grounded Dialogue Generation](https://doi.org/10.18653/v1/2021.emnlp-main.173) |  | 0 | Neural conversation models have shown great potentials towards generating fluent and informative responses by introducing external background knowledge. Nevertheless, it is laborious to construct such knowledge-grounded dialogues, and existing models usually perform poorly when transfer to new... | Shilei Liu, Xiaofeng Zhao, Bochao Li, Feiliang Ren, Longhui Zhang, Shujuan Yin |  |
| 643 |  |  [Intention Reasoning Network for Multi-Domain End-to-end Task-Oriented Dialogue](https://doi.org/10.18653/v1/2021.emnlp-main.174) |  | 0 | Recent years has witnessed the remarkable success in end-to-end task-oriented dialog system, especially when incorporating external knowledge information. However, the quality of most existing models’ generated response is still limited, mainly due to their lack of fine-grained reasoning on... | Zhiyuan Ma, Jianjun Li, Zezheng Zhang, Guohui Li, Yongjing Cheng |  |
| 644 |  |  [More is Better: Enhancing Open-Domain Dialogue Generation via Multi-Source Heterogeneous Knowledge](https://doi.org/10.18653/v1/2021.emnlp-main.175) |  | 0 | Despite achieving remarkable performance, previous knowledge-enhanced works usually only use a single-source homogeneous knowledge base of limited knowledge coverage. Thus, they often degenerate into traditional methods because not all dialogues can be linked with knowledge entries. This paper... | Sixing Wu, Ying Li, Minghui Wang, Dawei Zhang, Yang Zhou, Zhonghai Wu |  |
| 645 |  |  [Domain-Lifelong Learning for Dialogue State Tracking via Knowledge Preservation Networks](https://doi.org/10.18653/v1/2021.emnlp-main.176) |  | 0 | Dialogue state tracking (DST), which estimates user goals given a dialogue context, is an essential component of task-oriented dialogue systems. Conventional DST models are usually trained offline, which requires a fixed dataset prepared in advance. This paradigm is often impractical in real-world... | Qingbin Liu, Pengfei Cao, Cao Liu, Jiansong Chen, Xunliang Cai, Fan Yang, Shizhu He, Kang Liu, Jun Zhao |  |
| 646 |  |  [CSAGN: Conversational Structure Aware Graph Network for Conversational Semantic Role Labeling](https://doi.org/10.18653/v1/2021.emnlp-main.177) |  | 0 | Conversational semantic role labeling (CSRL) is believed to be a crucial step towards dialogue understanding. However, it remains a major challenge for existing CSRL parser to handle conversational structural information. In this paper, we present a simple and effective architecture for CSRL which... | Han Wu, Kun Xu, Linqi Song |  |
| 647 |  |  [Different Strokes for Different Folks: Investigating Appropriate Further Pre-training Approaches for Diverse Dialogue Tasks](https://doi.org/10.18653/v1/2021.emnlp-main.178) |  | 0 | Loading models pre-trained on the large-scale corpus in the general domain and fine-tuning them on specific downstream tasks is gradually becoming a paradigm in Natural Language Processing. Previous investigations prove that introducing a further pre-training phase between pre-training and... | Yao Qiu, Jinchao Zhang, Jie Zhou |  |
| 648 |  |  [Knowledge Enhanced Fine-Tuning for Better Handling Unseen Entities in Dialogue Generation](https://doi.org/10.18653/v1/2021.emnlp-main.179) |  | 0 | Although pre-training models have achieved great success in dialogue generation, their performance drops dramatically when the input contains an entity that does not appear in pre-training and fine-tuning datasets (unseen entity). To address this issue, existing methods leverage an external... | Leyang Cui, Yu Wu, Shujie Liu, Yue Zhang |  |
| 649 |  |  [An Evaluation Dataset and Strategy for Building Robust Multi-turn Response Selection Model](https://doi.org/10.18653/v1/2021.emnlp-main.180) |  | 0 | Multi-turn response selection models have recently shown comparable performance to humans in several benchmark datasets. However, in the real environment, these models often have weaknesses, such as making incorrect predictions based heavily on superficial patterns without a comprehensive... | Kijong Han, Seojin Lee, Donghun Lee |  |
| 650 |  |  [Unsupervised Conversation Disentanglement through Co-Training](https://doi.org/10.18653/v1/2021.emnlp-main.181) |  | 0 | Conversation disentanglement aims to separate intermingled messages into detached sessions, which is a fundamental task in understanding multi-party conversations. Existing work on conversation disentanglement relies heavily upon human-annotated datasets, which is expensive to obtain in practice.... | Hui Liu, Zhan Shi, Xiaodan Zhu |  |
| 651 |  |  [Don't be Contradicted with Anything! CI-ToD: Towards Benchmarking Consistency for Task-oriented Dialogue System](https://doi.org/10.18653/v1/2021.emnlp-main.182) |  | 0 | Consistency Identification has obtained remarkable success on open-domain dialogue, which can be used for preventing inconsistent response generation. However, in contrast to the rapid development in open-domain dialogue, few efforts have been made to the task-oriented dialogue direction. In this... | Libo Qin, Tianbao Xie, Shijue Huang, Qiguang Chen, Xiao Xu, Wanxiang Che |  |
| 652 |  |  [Transferable Persona-Grounded Dialogues via Grounded Minimal Edits](https://doi.org/10.18653/v1/2021.emnlp-main.183) |  | 0 | Grounded dialogue models generate responses that are grounded on certain concepts. Limited by the distribution of grounded dialogue data, models trained on such data face the transferability challenges in terms of the data distribution and the type of grounded concepts. To address the challenges,... | Chen Henry Wu, Yinhe Zheng, Xiaoxi Mao, Minlie Huang |  |
| 653 |  |  [EARL: Informative Knowledge-Grounded Conversation Generation with Entity-Agnostic Representation Learning](https://doi.org/10.18653/v1/2021.emnlp-main.184) |  | 0 | Generating informative and appropriate responses is challenging but important for building human-like dialogue systems. Although various knowledge-grounded conversation models have been proposed, these models have limitations in utilizing knowledge that infrequently occurs in the training data, not... | Hao Zhou, Minlie Huang, Yong Liu, Wei Chen, Xiaoyan Zhu |  |
| 654 |  |  [DialogueCSE: Dialogue-based Contrastive Learning of Sentence Embeddings](https://doi.org/10.18653/v1/2021.emnlp-main.185) |  | 0 | Learning sentence embeddings from dialogues has drawn increasing attention due to its low annotation cost and high domain adaptability. Conventional approaches employ the siamese-network for this task, which obtains the sentence embeddings through modeling the context-response semantic relevance by... | Che Liu, Rui Wang, Jinghua Liu, Jian Sun, Fei Huang, Luo Si |  |
| 655 |  |  [Improving Graph-based Sentence Ordering with Iteratively Predicted Pairwise Orderings](https://doi.org/10.18653/v1/2021.emnlp-main.186) |  | 0 | Dominant sentence ordering models can be classified into pairwise ordering models and set-to-sequence models. However, there is little attempt to combine these two types of models, which inituitively possess complementary advantages. In this paper, we propose a novel sentence ordering framework... | Shaopeng Lai, Ante Wang, Fandong Meng, Jie Zhou, Yubin Ge, Jiali Zeng, Junfeng Yao, Degen Huang, Jinsong Su |  |
| 656 |  |  [Not Just Classification: Recognizing Implicit Discourse Relation on Joint Modeling of Classification and Generation](https://doi.org/10.18653/v1/2021.emnlp-main.187) |  | 0 | Implicit discourse relation recognition (IDRR) is a critical task in discourse analysis. Previous studies only regard it as a classification task and lack an in-depth understanding of the semantics of different relations. Therefore, we first view IDRR as a generation task and further propose a... | Feng Jiang, Yaxin Fan, Xiaomin Chu, Peifeng Li, Qiaoming Zhu |  |
| 657 |  |  [A Language Model-based Generative Classifier for Sentence-level Discourse Parsing](https://doi.org/10.18653/v1/2021.emnlp-main.188) |  | 0 | Discourse segmentation and sentence-level discourse parsing play important roles for various NLP tasks to consider textual coherence. Despite recent achievements in both tasks, there is still room for improvement due to the scarcity of labeled data. To solve the problem, we propose a language... | Ying Zhang, Hidetaka Kamigaito, Manabu Okumura |  |
| 658 |  |  [Multimodal Phased Transformer for Sentiment Analysis](https://doi.org/10.18653/v1/2021.emnlp-main.189) |  | 0 | Multimodal Transformers achieve superior performance in multimodal learning tasks. However, the quadratic complexity of the self-attention mechanism in Transformers limits their deployment in low-resource devices and makes their inference and training computationally expensive. We propose... | Junyan Cheng, Iordanis Fostiropoulos, Barry W. Boehm, Mohammad Soleymani |  |
| 659 |  |  [Hierarchical Multi-label Text Classification with Horizontal and Vertical Category Correlations](https://doi.org/10.18653/v1/2021.emnlp-main.190) |  | 0 | Hierarchical multi-label text classification (HMTC) deals with the challenging task where an instance can be assigned to multiple hierarchically structured categories at the same time. The majority of prior studies either focus on reducing the HMTC task into a flat multi-label problem ignoring the... | Linli Xu, Sijie Teng, Ruoyu Zhao, Junliang Guo, Chi Xiao, Deqiang Jiang, Bo Ren |  |
| 660 |  |  [RankNAS: Efficient Neural Architecture Search by Pairwise Ranking](https://doi.org/10.18653/v1/2021.emnlp-main.191) |  | 0 | This paper addresses the efficiency challenge of Neural Architecture Search (NAS) by formulating the task as a ranking problem. Previous methods require numerous training examples to estimate the accurate performance of architectures, although the actual goal is to find the distinction between... | Chi Hu, Chenglong Wang, Xiangnan Ma, Xia Meng, Yinqiao Li, Tong Xiao, Jingbo Zhu, Changliang Li |  |
| 661 |  |  [FLiText: A Faster and Lighter Semi-Supervised Text Classification with Convolution Networks](https://doi.org/10.18653/v1/2021.emnlp-main.192) |  | 0 | In natural language processing (NLP), state-of-the-art (SOTA) semi-supervised learning (SSL) frameworks have shown great performance on deep pre-trained language models such as BERT, and are expected to significantly reduce the demand for manual labeling. However, our empirical studies indicate... | Chen Liu, Mengchao Zhang, Zhibing Fu, Panpan Hou, Yu Li |  |
| 662 |  |  [Evaluating Debiasing Techniques for Intersectional Biases](https://doi.org/10.18653/v1/2021.emnlp-main.193) |  | 0 | Bias is pervasive for NLP models, motivating the development of automatic debiasing techniques. Evaluation of NLP debiasing methods has largely been limited to binary attributes in isolation, e.g., debiasing with respect to binary gender or race, however many corpora involve multiple such... | Shivashankar Subramanian, Xudong Han, Timothy Baldwin, Trevor Cohn, Lea Frermann |  |
| 663 |  |  [Definition Modelling for Appropriate Specificity](https://doi.org/10.18653/v1/2021.emnlp-main.194) |  | 0 | Definition generation techniques aim to generate a definition of a target word or phrase given a context. In previous studies, researchers have faced various issues such as the out-of-vocabulary problem and over/under-specificity problems. Over-specific definitions present narrow word meanings,... | Han Huang, Tomoyuki Kajiwara, Yuki Arase |  |
| 664 |  |  [Transductive Learning for Unsupervised Text Style Transfer](https://doi.org/10.18653/v1/2021.emnlp-main.195) |  | 0 | Unsupervised style transfer models are mainly based on an inductive learning approach, which represents the style as embeddings, decoder parameters, or discriminator parameters and directly applies these general rules to the test cases. However, the lacking of parallel corpus hinders the ability of... | Fei Xiao, Liang Pang, Yanyan Lan, Yan Wang, Huawei Shen, Xueqi Cheng |  |
| 665 |  |  [Integrating Semantic Scenario and Word Relations for Abstractive Sentence Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.196) |  | 0 | Recently graph-based methods have been adopted for Abstractive Text Summarization. However, existing graph-based methods only consider either word relations or structure information, which neglect the correlation between them. To simultaneously capture the word relations and structure information... | Yong Guan, Shaoru Guo, Ru Li, Xiaoli Li, Hu Zhang |  |
| 666 |  |  [Coupling Context Modeling with Zero Pronoun Recovering for Document-Level Natural Language Generation](https://doi.org/10.18653/v1/2021.emnlp-main.197) |  | 0 | Natural language generation (NLG) tasks on pro-drop languages are known to suffer from zero pronoun (ZP) problems, and the problems remain challenging due to the scarcity of ZP-annotated NLG corpora. In this case, we propose a highly adaptive two-stage approach to couple context modeling with ZP... | Xin Tan, Longyin Zhang, Guodong Zhou |  |
| 667 |  |  [Adaptive Bridge between Training and Inference for Dialogue Generation](https://doi.org/10.18653/v1/2021.emnlp-main.198) |  | 0 | Although exposure bias has been widely studied in some NLP tasks, it faces its unique challenges in dialogue response generation, the representative one-to-various generation scenario. In real human dialogue, there are many appropriate responses for the same context, not only with different... | Haoran Xu, Hainan Zhang, Yanyan Zou, Hongshen Chen, Zhuoye Ding, Yanyan Lan |  |
| 668 |  |  [ConRPG: Paraphrase Generation using Contexts as Regularizer](https://doi.org/10.18653/v1/2021.emnlp-main.199) |  | 0 | A long-standing issue with paraphrase generation is the lack of reliable supervision signals. In this paper, we propose a new unsupervised paradigm for paraphrase generation based on the assumption that the probabilities of generating two sentences with the same meaning given the same context... | Yuxian Meng, Xiang Ao, Qing He, Xiaofei Sun, Qinghong Han, Fei Wu, Chun Fan, Jiwei Li |  |
| 669 |  |  [Building the Directed Semantic Graph for Coherent Long Text Generation](https://doi.org/10.18653/v1/2021.emnlp-main.200) |  | 0 | Generating long text conditionally depending on the short input text has recently attracted more and more research efforts. Most existing approaches focus more on introducing extra knowledge to supplement the short input text, but ignore the coherence issue of the generated texts. To address... | Ziao Wang, Xiaofeng Zhang, Hongwei Du |  |
| 670 |  |  [Iterative GNN-based Decoder for Question Generation](https://doi.org/10.18653/v1/2021.emnlp-main.201) |  | 0 | Natural question generation (QG) aims to generate questions from a passage, and generated questions are answered from the passage. Most models with state-of-the-art performance model the previously generated text at each decoding step. However, (1) they ignore the rich structure information that is... | Zichu Fei, Qi Zhang, Yaqian Zhou |  |
| 671 |  |  [Asking Questions Like Educational Experts: Automatically Generating Question-Answer Pairs on Real-World Examination Data](https://doi.org/10.18653/v1/2021.emnlp-main.202) |  | 0 | Generating high quality question-answer pairs is a hard but meaningful task. Although previous works have achieved great results on answer-aware question generation, it is difficult to apply them into practical application in the education field. This paper for the first time addresses the... | Fanyi Qu, Xin Jia, Yunfang Wu |  |
| 672 |  |  [Syntactically-Informed Unsupervised Paraphrasing with Non-Parallel Data](https://doi.org/10.18653/v1/2021.emnlp-main.203) |  | 0 | Previous works on syntactically controlled paraphrase generation heavily rely on large-scale parallel paraphrase data that is not easily available for many languages and domains. In this paper, we take this research direction to the extreme and investigate whether it is possible to learn... | Erguang Yang, Mingtong Liu, Deyi Xiong, Yujie Zhang, Yao Meng, Changjian Hu, Jinan Xu, Yufeng Chen |  |
| 673 |  |  [Exploring Task Difficulty for Few-Shot Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.204) |  | 0 | Few-shot relation extraction (FSRE) focuses on recognizing novel relations by learning with merely a handful of annotated instances. Meta-learning has been widely adopted for such a task, which trains on randomly generated few-shot tasks to learn generic data representations. Despite impressive... | Jiale Han, Bo Cheng, Wei Lu |  |
| 674 |  |  [MuVER: Improving First-Stage Entity Retrieval with Multi-View Entity Representations](https://doi.org/10.18653/v1/2021.emnlp-main.205) |  | 0 | Entity retrieval, which aims at disambiguating mentions to canonical entities from massive KBs, is essential for many tasks in natural language processing. Recent progress in entity retrieval shows that the dual-encoder structure is a powerful and efficient framework to nominate candidates if... | Xinyin Ma, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, Weiming Lu |  |
| 675 |  |  [Treasures Outside Contexts: Improving Event Detection via Global Statistics](https://doi.org/10.18653/v1/2021.emnlp-main.206) |  | 0 | Event detection (ED) aims at identifying event instances of specified types in given texts, which has been formalized as a sequence labeling task. As far as we know, existing neural-based ED models make decisions relying entirely on the contextual semantic features of each word in the inputted... | Rui Li, Wenlin Zhao, Cheng Yang, Sen Su |  |
| 676 |  |  [Uncertain Local-to-Global Networks for Document-Level Event Factuality Identification](https://doi.org/10.18653/v1/2021.emnlp-main.207) |  | 0 | Event factuality indicates the degree of certainty about whether an event occurs in the real world. Existing studies mainly focus on identifying event factuality at sentence level, which easily leads to conflicts between different mentions of the same event. To this end, we study the problem of... | Pengfei Cao, Yubo Chen, Yuqing Yang, Kang Liu, Jun Zhao |  |
| 677 |  |  [A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling](https://doi.org/10.18653/v1/2021.emnlp-main.208) |  | 0 | Table filling based relational triple extraction methods are attracting growing research interests due to their promising performance and their abilities on extracting triples from complex sentences. However, this kind of methods are far from their full potential because most of them only focus on... | Feiliang Ren, Longhui Zhang, Shujuan Yin, Xiaofeng Zhao, Shilei Liu, Bochao Li, Yaduo Liu |  |
| 678 |  |  [Structure-Augmented Keyphrase Generation](https://doi.org/10.18653/v1/2021.emnlp-main.209) |  | 0 | This paper studies the keyphrase generation (KG) task for scenarios where structure plays an important role. For example, a scientific publication consists of a short title and a long body, where the title can be used for de-emphasizing unimportant details in the body. Similarly, for short social... | Jihyuk Kim, Myeongho Jeong, Seungtaek Choi, Seungwon Hwang |  |
| 679 |  |  [An Empirical Study on Multiple Information Sources for Zero-Shot Fine-Grained Entity Typing](https://doi.org/10.18653/v1/2021.emnlp-main.210) |  | 0 | Auxiliary information from multiple sources has been demonstrated to be effective in zero-shot fine-grained entity typing (ZFET). However, there lacks a comprehensive understanding about how to make better use of the existing information sources and how they affect the performance of ZFET. In this... | Yi Chen, Haiyun Jiang, Lemao Liu, Shuming Shi, Chuang Fan, Min Yang, Ruifeng Xu |  |
| 680 |  |  [DyLex: Incorporating Dynamic Lexicons into BERT for Sequence Labeling](https://doi.org/10.18653/v1/2021.emnlp-main.211) |  | 0 | Incorporating lexical knowledge into deep learning models has been proved to be very effective for sequence labeling tasks. However, previous works commonly have difficulty dealing with large-scale dynamic lexicons which often cause excessive matching noise and problems of frequent updates. In this... | Baojun Wang, Zhao Zhang, Kun Xu, GuangYuan Hao, Yuyang Zhang, Lifeng Shang, Linlin Li, Xiao Chen, Xin Jiang, Qun Liu |  |
| 681 |  |  [MapRE: An Effective Semantic Mapping Approach for Low-resource Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.212) |  | 0 | Neural relation extraction models have shown promising results in recent years; however, the model performance drops dramatically given only a few training samples. Recent works try leveraging the advance in few-shot learning to solve the low resource problem, where they train label-agnostic models... | Manqing Dong, Chunguang Pan, Zhipeng Luo |  |
| 682 |  |  [Heterogeneous Graph Neural Networks for Keyphrase Generation](https://doi.org/10.18653/v1/2021.emnlp-main.213) |  | 0 | The encoder–decoder framework achieves state-of-the-art results in keyphrase generation (KG) tasks by predicting both present keyphrases that appear in the source document and absent keyphrases that do not. However, relying solely on the source document can result in generating uncontrollable and... | Jiacheng Ye, Ruijian Cai, Tao Gui, Qi Zhang |  |
| 683 |  |  [Machine Reading Comprehension as Data Augmentation: A Case Study on Implicit Event Argument Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.214) |  | 0 | Implicit event argument extraction (EAE) is a crucial document-level information extraction task that aims to identify event arguments beyond the sentence level. Despite many efforts for this task, the lack of enough training data has long impeded the study. In this paper, we take a new perspective... | Jian Liu, Yufeng Chen, Jinan Xu |  |
| 684 |  |  [Importance Estimation from Multiple Perspectives for Keyphrase Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.215) |  | 0 | Keyphrase extraction is a fundamental task in Natural Language Processing, which usually contains two main parts: candidate keyphrase extraction and keyphrase importance estimation. From the view of human understanding documents, we typically measure the importance of phrase according to its... | Mingyang Song, Liping Jing, Lin Xiao |  |
| 685 |  |  [Gradient Imitation Reinforcement Learning for Low Resource Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.216) |  | 0 | Low-resource Relation Extraction (LRE) aims to extract relation facts from limited labeled corpora when human annotation is scarce. Existing works either utilize self-training scheme to generate pseudo labels that will cause the gradual drift problem, or leverage meta-learning scheme which does not... | Xuming Hu, Chenwei Zhang, Yawen Yang, Xiaohe Li, Li Lin, Lijie Wen, Philip S. Yu |  |
| 686 |  |  [Low-resource Taxonomy Enrichment with Pretrained Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.217) |  | 0 | Taxonomies are symbolic representations of hierarchical relationships between terms or entities. While taxonomies are useful in broad applications, manually updating or maintaining them is labor-intensive and difficult to scale in practice. Conventional supervised methods for this enrichment task... | Kunihiro Takeoka, Kosuke Akimoto, Masafumi Oyamada |  |
| 687 |  |  [Entity Relation Extraction as Dependency Parsing in Visually Rich Documents](https://doi.org/10.18653/v1/2021.emnlp-main.218) |  | 0 | Previous works on key information extraction from visually rich documents (VRDs) mainly focus on labeling the text within each bounding box (i.e.,semantic entity), while the relations in-between are largely unexplored. In this paper, we adapt the popular dependency parsing model, the biaffine... | Yue Zhang, Bo Zhang, Rui Wang, Junjie Cao, Chen Li, Zuyi Bao |  |
| 688 |  |  [Synchronous Dual Network with Cross-Type Attention for Joint Entity and Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.219) |  | 0 | Joint entity and relation extraction is challenging due to the complex interaction of interaction between named entity recognition and relation extraction. Although most existing works tend to jointly train these two tasks through a shared network, they fail to fully utilize the interdependence... | Hui Wu, Xiaodong Shi |  |
| 689 |  |  [Less is More: Pretrain a Strong Siamese Encoder for Dense Text Retrieval Using a Weak Decoder](https://doi.org/10.18653/v1/2021.emnlp-main.220) |  | 0 | Dense retrieval requires high-quality text sequence embeddings to support effective search in the representation space. Autoencoder-based language models are appealing in dense retrieval as they train the encoder to output high-quality embedding that can reconstruct the input texts. However, in... | Shuqi Lu, Di He, Chenyan Xiong, Guolin Ke, Waleed Malik, Zhicheng Dou, Paul Bennett, TieYan Liu, Arnold Overwijk |  |
| 690 |  |  [TransPrompt: Towards an Automatic Transferable Prompting Framework for Few-shot Text Classification](https://doi.org/10.18653/v1/2021.emnlp-main.221) |  | 0 | Recent studies have shown that prompts improve the performance of large pre-trained language models for few-shot text classification. Yet, it is unclear how the prompting knowledge can be transferred across similar NLP tasks for the purpose of mutual reinforcement. Based on continuous prompt... | Chengyu Wang, Jianing Wang, Minghui Qiu, Jun Huang, Ming Gao |  |
| 691 |  |  [Weakly-supervised Text Classification Based on Keyword Graph](https://doi.org/10.18653/v1/2021.emnlp-main.222) |  | 0 | Weakly-supervised text classification has received much attention in recent years for it can alleviate the heavy burden of annotating massive data. Among them, keyword-driven methods are the mainstream where user-provided keywords are exploited to generate pseudo-labels for unlabeled texts.... | Lu Zhang, Jiandong Ding, Yi Xu, Yingyao Liu, Shuigeng Zhou |  |
| 692 |  |  [Efficient-FedRec: Efficient Federated Learning Framework for Privacy-Preserving News Recommendation](https://doi.org/10.18653/v1/2021.emnlp-main.223) |  | 0 | News recommendation is critical for personalized news access. Most existing news recommendation methods rely on centralized storage of users’ historical news click behavior data, which may lead to privacy concerns and hazards. Federated Learning is a privacy-preserving framework for multiple... | Jingwei Yi, Fangzhao Wu, Chuhan Wu, Ruixuan Liu, Guangzhong Sun, Xing Xie |  |
| 693 |  |  [RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking](https://doi.org/10.18653/v1/2021.emnlp-main.224) |  | 0 | In various natural language processing tasks, passage retrieval and passage re-ranking are two key procedures in finding and ranking relevant information. Since both the two procedures contribute to the final performance, it is important to jointly optimize them in order to achieve mutual... | Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Qiaoqiao She, Hua Wu, Haifeng Wang, JiRong Wen |  |
| 694 |  |  [Dealing with Typos for BERT-based Passage Retrieval and Ranking](https://doi.org/10.18653/v1/2021.emnlp-main.225) |  | 0 | Passage retrieval and ranking is a key task in open-domain question answering and information retrieval. Current effective approaches mostly rely on pre-trained deep language model-based retrievers and rankers. These methods have been shown to effectively model the semantic matching between queries... | Shengyao Zhuang, Guido Zuccon |  |
| 695 |  |  [From Alignment to Assignment: Frustratingly Simple Unsupervised Entity Alignment](https://doi.org/10.18653/v1/2021.emnlp-main.226) |  | 0 | Cross-lingual entity alignment (EA) aims to find the equivalent entities between crosslingual KGs (Knowledge Graphs), which is a crucial step for integrating KGs. Recently, many GNN-based EA methods are proposed and show decent performance improvements on several public datasets. However, existing... | Xin Mao, Wenting Wang, Yuanbin Wu, Man Lan |  |
| 696 |  |  [Simple and Effective Unsupervised Redundancy Elimination to Compress Dense Vectors for Passage Retrieval](https://doi.org/10.18653/v1/2021.emnlp-main.227) |  | 0 | Recent work has shown that dense passage retrieval techniques achieve better ranking accuracy in open-domain question answering compared to sparse retrieval techniques such as BM25, but at the cost of large space and memory requirements. In this paper, we analyze the redundancy present in encoded... | Xueguang Ma, Minghan Li, Kai Sun, Ji Xin, Jimmy Lin |  |
| 697 |  |  [Relation Extraction with Word Graphs from N-grams](https://doi.org/10.18653/v1/2021.emnlp-main.228) |  | 0 | Most recent studies for relation extraction (RE) leverage the dependency tree of the input sentence to incorporate syntax-driven contextual information to improve model performance, with little attention paid to the limitation where high-quality dependency parsers in most cases unavailable,... | Han Qin, Yuanhe Tian, Yan Song |  |
| 698 |  |  [A Bayesian Framework for Information-Theoretic Probing](https://doi.org/10.18653/v1/2021.emnlp-main.229) |  | 0 | Pimentel et al. (2020) recently analysed probing from an information-theoretic perspective. They argue that probing should be seen as approximating a mutual information. This led to the rather unintuitive conclusion that representations encode exactly the same information about a target task as the... | Tiago Pimentel, Ryan Cotterell |  |
| 699 |  |  [Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little](https://doi.org/10.18653/v1/2021.emnlp-main.230) |  | 0 | A possible explanation for the impressive performance of masked language model (MLM) pre-training is that such models have learned to represent the syntactic structures prevalent in classical NLP pipelines. In this paper, we propose a different explanation: MLMs succeed on downstream tasks almost... | Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, Douwe Kiela |  |
| 700 |  |  [What's Hidden in a One-layer Randomly Weighted Transformer?](https://doi.org/10.18653/v1/2021.emnlp-main.231) |  | 0 | We demonstrate that, hidden within one-layer randomly weighted neural networks, there exist subnetworks that can achieve impressive performance, without ever modifying the weight initializations, on machine translation tasks. To find subnetworks for one-layer randomly weighted neural networks, we... | Sheng Shen, Zhewei Yao, Douwe Kiela, Kurt Keutzer, Michael W. Mahoney |  |
| 701 |  |  [Rethinking Denoised Auto-Encoding in Language Pre-Training](https://doi.org/10.18653/v1/2021.emnlp-main.232) |  | 0 | Pre-trained self-supervised models such as BERT have achieved striking success in learning sequence representations, especially for natural language processing. These models typically corrupt the given sequences with certain types of noise, such as masking, shuffling, or substitution, and then try... | Fuli Luo, Pengcheng Yang, Shicheng Li, Xuancheng Ren, Xu Sun, Songfang Huang, Fei Huang |  |
| 702 |  |  [Lifelong Explainer for Lifelong Learners](https://doi.org/10.18653/v1/2021.emnlp-main.233) |  | 0 | Lifelong Learning (LL) black-box models are dynamic in that they keep learning from new tasks and constantly update their parameters. Owing to the need to utilize information from previously seen tasks, and capture commonalities in potentially diverse data, it is hard for automatic explanation... | Xuelin Situ, Sameen Maruf, Ingrid Zukerman, Cécile Paris, Gholamreza Haffari |  |
| 703 |  |  [Linguistic Dependencies and Statistical Dependence](https://doi.org/10.18653/v1/2021.emnlp-main.234) |  | 0 | Are pairs of words that tend to occur together also likely to stand in a linguistic dependency? This empirical question is motivated by a long history of literature in cognitive science, psycholinguistics, and NLP. In this work we contribute an extensive analysis of the relationship between... | Jacob Louis Hoover, Wenyu Du, Alessandro Sordoni, Timothy J. O'Donnell |  |
| 704 |  |  [Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars](https://doi.org/10.18653/v1/2021.emnlp-main.235) |  | 0 | In computational linguistics, it has been shown that hierarchical structures make language models (LMs) more human-like. However, the previous literature has been agnostic about a parsing strategy of the hierarchical models. In this paper, we investigated whether hierarchical structures make LMs... | Ryo Yoshida, Hiroshi Noji, Yohei Oseki |  |
| 705 |  |  [A Simple and Effective Positional Encoding for Transformers](https://doi.org/10.18653/v1/2021.emnlp-main.236) |  | 0 | Transformer models are permutation equivariant. To supply the order and type information of the input tokens, position and segment embeddings are usually added to the input. Recent works proposed variations of positional encodings with relative position encodings achieving better performance. Our... | PuChin Chen, Henry Tsai, Srinadh Bhojanapalli, Hyung Won Chung, YinWen Chang, ChunSung Ferng |  |
| 706 |  |  [Explore Better Relative Position Embeddings from Encoding Perspective for Transformer Models](https://doi.org/10.18653/v1/2021.emnlp-main.237) |  | 0 | Relative position embedding (RPE) is a successful method to explicitly and efficaciously encode position information into Transformer models. In this paper, we investigate the potential problems in Shaw-RPE and XL-RPE, which are the most representative and prevalent RPEs, and propose two novel RPEs... | Anlin Qu, Jianwei Niu, Shasha Mo |  |
| 707 |  |  [Adversarial Mixing Policy for Relaxing Locally Linear Constraints in Mixup](https://doi.org/10.18653/v1/2021.emnlp-main.238) |  | 0 | Mixup is a recent regularizer for current deep classification networks. Through training a neural network on convex combinations of pairs of examples and their labels, it imposes locally linear constraints on the model’s input space. However, such strict linear constraints often lead to... | Guang Liu, Yuzhao Mao, Hailong Huang, Weiguo Gao, Xuan Li |  |
| 708 |  |  [Is this the end of the gold standard? A straightforward reference-less grammatical error correction metric](https://doi.org/10.18653/v1/2021.emnlp-main.239) |  | 0 | It is difficult to rank and evaluate the performance of grammatical error correction (GEC) systems, as a sentence can be rewritten in numerous correct ways. A number of GEC metrics have been used to evaluate proposed GEC systems; however, each system relies on either a comparison with one or more... | Md Asadul Islam, Enrico Magnani |  |
| 709 |  |  [Augmenting BERT-style Models with Predictive Coding to Improve Discourse-level Representations](https://doi.org/10.18653/v1/2021.emnlp-main.240) |  | 0 | Current language models are usually trained using a self-supervised scheme, where the main focus is learning representations at the word or sentence level. However, there has been limited progress in generating useful discourse-level representations. In this work, we propose to use ideas from... | Vladimir Araujo, Andrés Villa, Marcelo Mendoza, MarieFrancine Moens, Alvaro Soto |  |
| 710 |  |  [Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning](https://doi.org/10.18653/v1/2021.emnlp-main.241) |  | 0 | Pre-Trained Models have been widely applied and recently proved vulnerable under backdoor attacks: the released pre-trained weights can be maliciously poisoned with certain triggers. When the triggers are activated, even the fine-tuned model will predict pre-defined labels, causing a security... | Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, Xipeng Qiu |  |
| 711 |  |  [GAML-BERT: Improving BERT Early Exiting by Gradient Aligned Mutual Learning](https://doi.org/10.18653/v1/2021.emnlp-main.242) |  | 0 | In this work, we propose a novel framework, Gradient Aligned Mutual Learning BERT (GAML-BERT), for improving the early exiting of BERT. GAML-BERT’s contributions are two-fold. We conduct a set of pilot experiments, which shows that mutual knowledge distillation between a shallow exit and a deep... | Wei Zhu, Xiaoling Wang, Yuan Ni, Guotong Xie |  |
| 712 |  |  [The Power of Scale for Parameter-Efficient Prompt Tuning](https://doi.org/10.18653/v1/2021.emnlp-main.243) |  | 0 | In this work, we explore “prompt tuning,” a simple yet effective mechanism for learning “soft prompts” to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to... | Brian Lester, Rami AlRfou, Noah Constant |  |
| 713 |  |  [Scalable Font Reconstruction with Dual Latent Manifolds](https://doi.org/10.18653/v1/2021.emnlp-main.244) |  | 0 | We propose a deep generative model that performs typography analysis and font reconstruction by learning disentangled manifolds of both font style and character shape. Our approach enables us to massively scale up the number of character types we can effectively model compared to previous methods.... | Nikita Srivatsan, Si Wu, Jonathan T. Barron, Taylor BergKirkpatrick |  |
| 714 |  |  [Neuro-Symbolic Approaches for Text-Based Policy Learning](https://doi.org/10.18653/v1/2021.emnlp-main.245) |  | 0 | Text-Based Games (TBGs) have emerged as important testbeds for reinforcement learning (RL) in the natural language domain. Previous methods using LSTM-based action policies are uninterpretable and often overfit the training games showing poor performance to unseen test games. We present SymboLic... | Subhajit Chaudhury, Prithviraj Sen, Masaki Ono, Daiki Kimura, Michiaki Tatsubori, Asim Munawar |  |
| 715 |  |  [Layer-wise Model Pruning based on Mutual Information](https://doi.org/10.18653/v1/2021.emnlp-main.246) |  | 0 | Inspired by mutual information (MI) based feature selection in SVMs and logistic regression, in this paper, we propose MI-based layer-wise pruning: for each layer of a multi-layer neural network, neurons with higher values of MI with respect to preserved neurons in the upper layer are preserved.... | Chun Fan, Jiwei Li, Tianwei Zhang, Xiang Ao, Fei Wu, Yuxian Meng, Xiaofei Sun |  |
| 716 |  |  [Hierarchical Heterogeneous Graph Representation Learning for Short Text Classification](https://doi.org/10.18653/v1/2021.emnlp-main.247) |  | 0 | Short text classification is a fundamental task in natural language processing. It is hard due to the lack of context information and labeled data in practice. In this paper, we propose a new method called SHINE, which is based on graph neural network (GNN), for short text classification. First, we... | Yaqing Wang, Song Wang, Quanming Yao, Dejing Dou |  |
| 717 |  |  [kFolden: k-Fold Ensemble for Out-Of-Distribution Detection](https://doi.org/10.18653/v1/2021.emnlp-main.248) |  | 0 | Out-of-Distribution (OOD) detection is an important problem in natural language processing (NLP). In this work, we propose a simple yet effective framework kFolden, which mimics the behaviors of OOD detection during training without the use of any external data. For a task with k training labels,... | Xiaoya Li, Jiwei Li, Xiaofei Sun, Chun Fan, Tianwei Zhang, Fei Wu, Yuxian Meng, Jun Zhang |  |
| 718 |  |  [Frustratingly Simple Pretraining Alternatives to Masked Language Modeling](https://doi.org/10.18653/v1/2021.emnlp-main.249) |  | 0 | Masked language modeling (MLM), a self-supervised pretraining objective, is widely used in natural language processing for learning text representations. MLM trains a model to predict a random sample of input tokens that have been replaced by a [MASK] placeholder in a multi-class setting over the... | Atsuki Yamaguchi, George Chrysostomou, Katerina Margatina, Nikolaos Aletras |  |
| 719 |  |  [HRKD: Hierarchical Relational Knowledge Distillation for Cross-domain Language Model Compression](https://doi.org/10.18653/v1/2021.emnlp-main.250) |  | 0 | On many natural language processing tasks, large pre-trained language models (PLMs) have shown overwhelming performances compared with traditional neural network methods. Nevertheless, their huge model size and low inference speed have hindered the deployment on resource-limited devices in... | Chenhe Dong, Yaliang Li, Ying Shen, Minghui Qiu |  |
| 720 |  |  [Searching for an Effective Defender: Benchmarking Defense against Adversarial Word Substitution](https://doi.org/10.18653/v1/2021.emnlp-main.251) |  | 0 | Recent studies have shown that deep neural network-based models are vulnerable to intentionally crafted adversarial examples, and various methods have been proposed to defend against adversarial word-substitution attacks for neural NLP models. However, there is a lack of systematic study on... | Zongyi Li, Jianhan Xu, Jiehang Zeng, Linyang Li, Xiaoqing Zheng, Qi Zhang, KaiWei Chang, ChoJui Hsieh |  |
| 721 |  |  [Re-embedding Difficult Samples via Mutual Information Constrained Semantically Oversampling for Imbalanced Text Classification](https://doi.org/10.18653/v1/2021.emnlp-main.252) |  | 0 | Difficult samples of the minority class in imbalanced text classification are usually hard to be classified as they are embedded into an overlapping semantic region with the majority class. In this paper, we propose a Mutual Information constrained Semantically Oversampling framework (MISO) that... | Jiachen Tian, Shizhan Chen, Xiaowang Zhang, Zhiyong Feng, Deyi Xiong, Shaojuan Wu, Chunliu Dou |  |
| 722 |  |  [Beyond Text: Incorporating Metadata and Label Structure for Multi-Label Document Classification using Heterogeneous Graphs](https://doi.org/10.18653/v1/2021.emnlp-main.253) |  | 0 | Multi-label document classification, associating one document instance with a set of relevant labels, is attracting more and more research attention. Existing methods explore the incorporation of information beyond text, such as document metadata or label structure. These approaches however either... | Chenchen Ye, Linhai Zhang, Yulan He, Deyu Zhou, Jie Wu |  |
| 723 |  |  [Natural Language Processing Meets Quantum Physics: A Survey and Categorization](https://doi.org/10.18653/v1/2021.emnlp-main.254) |  | 0 | Recent research has investigated quantum NLP, designing algorithms that process natural language in quantum computers, and also quantum-inspired algorithms that improve NLP performance on classical computers. In this survey, we review representative methods at the intersection of NLP and quantum... | Sixuan Wu, Jian Li, Peng Zhang, Yue Zhang |  |
| 724 |  |  [MetaTS: Meta Teacher-Student Network for Multilingual Sequence Labeling with Minimal Supervision](https://doi.org/10.18653/v1/2021.emnlp-main.255) |  | 0 | Sequence labeling aims to predict a fine-grained sequence of labels for the text. However, such formulation hinders the effectiveness of supervised methods due to the lack of token-level annotated data. This is exacerbated when we meet a diverse range of languages. In this work, we explore... | Zheng Li, Danqing Zhang, Tianyu Cao, Ying Wei, Yiwei Song, Bing Yin |  |
| 725 |  |  [Neural Machine Translation with Heterogeneous Topic Knowledge Embeddings](https://doi.org/10.18653/v1/2021.emnlp-main.256) |  | 0 | Neural Machine Translation (NMT) has shown a strong ability to utilize local context to disambiguate the meaning of words. However, it remains a challenge for NMT to leverage broader context information like topics. In this paper, we propose heterogeneous ways of embedding topic information at the... | Weixuan Wang, Wei Peng, Meng Zhang, Qun Liu |  |
| 726 |  |  [Allocating Large Vocabulary Capacity for Cross-Lingual Language Model Pre-Training](https://doi.org/10.18653/v1/2021.emnlp-main.257) |  | 0 | Compared to monolingual models, cross-lingual models usually require a more expressive vocabulary to represent all languages adequately. We find that many languages are under-represented in recent cross-lingual language models due to the limited vocabulary capacity. To this end, we propose an... | Bo Zheng, Li Dong, Shaohan Huang, Saksham Singhal, Wanxiang Che, Ting Liu, Xia Song, Furu Wei |  |
| 727 |  |  [Recurrent Attention for Neural Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.258) |  | 0 | Recent research questions the importance of the dot-product self-attention in Transformer models and shows that most attention heads learn simple positional patterns. In this paper, we push further in this research line and propose a novel substitute mechanism for self-attention: Recurrent... | Jiali Zeng, Shuangzhi Wu, Yongjing Yin, Yufan Jiang, Mu Li |  |
| 728 |  |  [Learning from Multiple Noisy Augmented Data Sets for Better Cross-Lingual Spoken Language Understanding](https://doi.org/10.18653/v1/2021.emnlp-main.259) |  | 0 | Lack of training data presents a grand challenge to scaling out spoken language understanding (SLU) to low-resource languages. Although various data augmentation approaches have been proposed to synthesize training data in low-resource target languages, the augmented data sets are often noisy, and... | Yingmei Guo, Linjun Shou, Jian Pei, Ming Gong, Mingxing Xu, Zhiyong Wu, Daxin Jiang |  |
| 729 |  |  [Enlivening Redundant Heads in Multi-head Self-attention for Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.260) |  | 0 | Multi-head self-attention recently attracts enormous interest owing to its specialized functions, significant parallelizable computation, and flexible extensibility. However, very recent empirical studies show that some self-attention heads make little contribution and can be pruned as redundant... | Tianfu Zhang, Heyan Huang, Chong Feng, Longbing Cao |  |
| 730 |  |  [Unsupervised Neural Machine Translation with Universal Grammar](https://doi.org/10.18653/v1/2021.emnlp-main.261) |  | 0 | Machine translation usually relies on parallel corpora to provide parallel signals for training. The advent of unsupervised machine translation has brought machine translation away from this reliance, though performance still lags behind traditional supervised machine translation. In unsupervised... | Zuchao Li, Masao Utiyama, Eiichiro Sumita, Hai Zhao |  |
| 731 |  |  [Encouraging Lexical Translation Consistency for Document-Level Neural Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.262) |  | 0 | Recently a number of approaches have been proposed to improve translation performance for document-level neural machine translation (NMT). However, few are focusing on the subject of lexical translation consistency. In this paper we apply “one translation per discourse” in NMT, and aim to encourage... | Xinglin Lyu, Junhui Li, Zhengxian Gong, Min Zhang |  |
| 732 |  |  [Improving Neural Machine Translation by Bidirectional Training](https://doi.org/10.18653/v1/2021.emnlp-main.263) |  | 0 | We present a simple and effective pretraining strategy – bidirectional training (BiT) for neural machine translation. Specifically, we bidirectionally update the model parameters at the early stage and then tune the model normally. To achieve bidirectional updating, we simply reconstruct the... | Liang Ding, Di Wu, Dacheng Tao |  |
| 733 |  |  [Scheduled Sampling Based on Decoding Steps for Neural Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.264) |  | 0 | Scheduled sampling is widely used to mitigate the exposure bias problem for neural machine translation. Its core motivation is to simulate the inference scene during training by replacing ground-truth tokens with predicted tokens, thus bridging the gap between training and inference. However,... | Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, Jie Zhou |  |
| 734 |  |  [Learning to Rewrite for Non-Autoregressive Neural Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.265) |  | 0 | Non-autoregressive neural machine translation, which decomposes the dependence on previous target tokens from the inputs of the decoder, has achieved impressive inference speedup but at the cost of inferior accuracy. Previous works employ iterative decoding to improve the translation by applying... | Xinwei Geng, Xiaocheng Feng, Bing Qin |  |
| 735 |  |  [SHAPE : Shifted Absolute Position Embedding for Transformers](https://doi.org/10.18653/v1/2021.emnlp-main.266) |  | 0 | Position representation is crucial for building position-aware representations in Transformers. Existing position representations suffer from a lack of generalization to test data with unseen lengths or high computational cost. We investigate shifted absolute position embedding (SHAPE) to address... | Shun Kiyono, Sosuke Kobayashi, Jun Suzuki, Kentaro Inui |  |
| 736 |  |  [Self-Supervised Quality Estimation for Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.267) |  | 0 | Quality estimation (QE) of machine translation (MT) aims to evaluate the quality of machine-translated sentences without references and is important in practical applications of MT. Training QE models require massive parallel data with hand-crafted quality annotations, which are time-consuming and... | Yuanhang Zheng, Zhixing Tan, Meng Zhang, Mieradilijiang Maimaiti, Huanbo Luan, Maosong Sun, Qun Liu, Yang Liu |  |
| 737 |  |  [Generalised Unsupervised Domain Adaptation of Neural Machine Translation with Cross-Lingual Data Selection](https://doi.org/10.18653/v1/2021.emnlp-main.268) |  | 0 | This paper considers the unsupervised domain adaptation problem for neural machine translation (NMT), where we assume the access to only monolingual text in either the source or target language in the new domain. We propose a cross-lingual data selection method to extract in-domain sentences in the... | ThuyTrang Vu, Xuanli He, Dinh Q. Phung, Gholamreza Haffari |  |
| 738 |  |  [STANKER: Stacking Network based on Level-grained Attention-masked BERT for Rumor Detection on Social Media](https://doi.org/10.18653/v1/2021.emnlp-main.269) |  | 0 | Rumor detection on social media puts pre-trained language models (LMs), such as BERT, and auxiliary features, such as comments, into use. However, on the one hand, rumor detection datasets in Chinese companies with comments are rare; on the other hand, intensive interaction of attention on... | Dongning Rao, Xin Miao, Zhihua Jiang, Ran Li |  |
| 739 |  |  [ActiveEA: Active Learning for Neural Entity Alignment](https://doi.org/10.18653/v1/2021.emnlp-main.270) |  | 0 | Entity Alignment (EA) aims to match equivalent entities across different Knowledge Graphs (KGs) and is an essential step of KG fusion. Current mainstream methods – neural EA models – rely on training with seed alignment, i.e., a set of pre-aligned entity pairs which are very costly to annotate. In... | Bing Liu, Harrisen Scells, Guido Zuccon, Wen Hua, Genghong Zhao |  |
| 740 |  |  [Cost-effective End-to-end Information Extraction for Semi-structured Document Images](https://doi.org/10.18653/v1/2021.emnlp-main.271) |  | 0 | A real-world information extraction (IE) system for semi-structured document images often involves a long pipeline of multiple modules, whose complexity dramatically increases its development and maintenance cost. One can instead consider an end-to-end model that directly maps the input to the... | Wonseok Hwang, Hyunji Lee, Jinyeong Yim, Geewook Kim, Minjoon Seo |  |
| 741 |  |  [Improving Math Word Problems with Pre-trained Knowledge and Hierarchical Reasoning](https://doi.org/10.18653/v1/2021.emnlp-main.272) |  | 0 | The recent algorithms for math word problems (MWP) neglect to use outside knowledge not present in the problems. Most of them only capture the word-level relationship and ignore to build hierarchical reasoning like the human being for mining the contextual structure between words and sentences. In... | Weijiang Yu, Yingpeng Wen, Fudan Zheng, Nong Xiao |  |
| 742 |  |  [GraphMR: Graph Neural Network for Mathematical Reasoning](https://doi.org/10.18653/v1/2021.emnlp-main.273) |  | 0 | Mathematical reasoning aims to infer satisfiable solutions based on the given mathematics questions. Previous natural language processing researches have proven the effectiveness of sequence-to-sequence (Seq2Seq) or related variants on mathematics solving. However, few works have been able to... | Weijie Feng, Binbin Liu, Dongpeng Xu, Qilong Zheng, Yun Xu |  |
| 743 |  |  [What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers](https://doi.org/10.18653/v1/2021.emnlp-main.274) |  | 0 | GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently... | Boseop Kim, HyoungSeok Kim, SangWoo Lee, Gichang Lee, DongHyun Kwak, Dong Hyeon Jeon, Sunghyun Park, Sungju Kim, Seonhoon Kim, Dongpil Seo, Heungsub Lee, Minyoung Jeong, Sungjae Lee, Minsub Kim, SukHyun Ko, Seokhun Kim, Taeyong Park, Jinuk Kim, Soyoung Kang, NaHyeon Ryu, Kang Min Yoo, Minsuk Chang, Soobin Suh, Sookyo In, Jinseong Park, Kyungduk Kim, Hiun Kim, Jisu Jeong, Yong Goo Yeo, Donghoon Ham, Dongju Park, Min Young Lee, Jaewook Kang, Inho Kang, JungWoo Ha, WooMyoung Park, Nako Sung |  |
| 744 |  |  [APIRecX: Cross-Library API Recommendation via Pre-Trained Language Model](https://doi.org/10.18653/v1/2021.emnlp-main.275) |  | 0 | For programmers, learning the usage of APIs (Application Programming Interfaces) of a software library is important yet difficult. API recommendation tools can help developers use APIs by recommending which APIs to be used next given the APIs that have been written. Traditionally, language models... | Yuning Kang, Zan Wang, Hongyu Zhang, Junjie Chen, Hanmo You |  |
| 745 |  |  [GMH: A General Multi-hop Reasoning Model for KG Completion](https://doi.org/10.18653/v1/2021.emnlp-main.276) |  | 0 | Knowledge graphs are essential for numerous downstream natural language processing applications, but are typically incomplete with many facts missing. This results in research efforts on multi-hop reasoning task, which can be formulated as a search process and current models typically perform short... | Yao Zhang, Hongru Liang, Adam Jatowt, Wenqiang Lei, Xin Wei, Ning Jiang, Zhenglu Yang |  |
| 746 |  |  [BPM_MT: Enhanced Backchannel Prediction Model using Multi-Task Learning](https://doi.org/10.18653/v1/2021.emnlp-main.277) |  | 0 | Backchannel (BC), a short reaction signal of a listener to a speaker’s utterances, helps to improve the quality of the conversation. Several studies have been conducted to predict BC in conversation; however, the utilization of advanced natural language processing techniques using lexical... | Jin Yea Jang, San Kim, Minyoung Jung, Saim Shin, Gahgene Gweon |  |
| 747 |  |  [Graphine: A Dataset for Graph-aware Terminology Definition Generation](https://doi.org/10.18653/v1/2021.emnlp-main.278) |  | 0 | Precisely defining the terminology is the first step in scientific communication. Developing neural text generation models for definition generation can circumvent the labor-intensity curation, further accelerating scientific discovery. Unfortunately, the lack of large-scale terminology definition... | Zequn Liu, Shukai Wang, Yiyang Gu, Ruiyi Zhang, Ming Zhang, Sheng Wang |  |
| 748 |  |  [Leveraging Order-Free Tag Relations for Context-Aware Recommendation](https://doi.org/10.18653/v1/2021.emnlp-main.279) |  | 0 | Tag recommendation relies on either a ranking function for top-k tags or an autoregressive generation method. However, the previous methods neglect one of two seemingly conflicting yet desirable characteristics of a tag set: orderlessness and inter-dependency. While the ranking approach fails to... | Junmo Kang, Jeonghwan Kim, Suwon Shin, SungHyon Myaeng |  |
| 749 |  |  [End-to-End Conversational Search for Online Shopping with Utterance Transfer](https://doi.org/10.18653/v1/2021.emnlp-main.280) |  | 0 | Successful conversational search systems can present natural, adaptive and interactive shopping experience for online shopping customers. However, building such systems from scratch faces real word challenges from both imperfect product schema/knowledge and lack of training dialog data. In this... | Liqiang Xiao, Jun Ma, Xin Luna Dong, Pascual MartínezGómez, Nasser Zalmout, Wei Chen, Tong Zhao, Hao He, Yaohui Jin |  |
| 750 |  |  [Self-Supervised Curriculum Learning for Spelling Error Correction](https://doi.org/10.18653/v1/2021.emnlp-main.281) |  | 0 | Spelling Error Correction (SEC) that requires high-level language understanding is a challenging but useful task. Current SEC approaches normally leverage a pre-training then fine-tuning procedure that treats data equally. By contrast, Curriculum Learning (CL) utilizes training data differently... | Zifa Gan, Hongfei Xu, Hongying Zan |  |
| 751 |  |  [Fix-Filter-Fix: Intuitively Connect Any Models for Effective Bug Fixing](https://doi.org/10.18653/v1/2021.emnlp-main.282) |  | 0 | Locating and fixing bugs is a time-consuming task. Most neural machine translation (NMT) based approaches for automatically bug fixing lack generality and do not make full use of the rich information in the source code. In NMT-based bug fixing, we find some predicted code identical to the input... | Haiwen Hong, Jingfeng Zhang, Yin Zhang, Yao Wan, Yulei Sui |  |
| 752 |  |  [Neuro-Symbolic Reinforcement Learning with First-Order Logic](https://doi.org/10.18653/v1/2021.emnlp-main.283) |  | 0 | Deep reinforcement learning (RL) methods often require many trials before convergence, and no direct interpretability of trained policies is provided. In order to achieve fast convergence and interpretability for the policy in RL, we propose a novel RL method for text-based games with a recent... | Daiki Kimura, Masaki Ono, Subhajit Chaudhury, Ryosuke Kohita, Akifumi Wachi, Don Joven Agravante, Michiaki Tatsubori, Asim Munawar, Alexander Gray |  |
| 753 |  |  [Biomedical Concept Normalization by Leveraging Hypernyms](https://doi.org/10.18653/v1/2021.emnlp-main.284) |  | 0 | Biomedical Concept Normalization (BCN) is widely used in biomedical text processing as a fundamental module. Owing to numerous surface variants of biomedical concepts, BCN still remains challenging and unsolved. In this paper, we exploit biomedical concept hypernyms to facilitate BCN. We propose... | Cheng Yan, Yuanzhe Zhang, Kang Liu, Jun Zhao, Yafei Shi, Shengping Liu |  |
| 754 |  |  [Leveraging Capsule Routing to Associate Knowledge with Medical Literature Hierarchically](https://doi.org/10.18653/v1/2021.emnlp-main.285) |  | 0 | Integrating knowledge into text is a promising way to enrich text representation, especially in the medical field. However, undifferentiated knowledge not only confuses the text representation but also imports unexpected noises. In this paper, to alleviate this problem, we propose leveraging... | Xin Liu, Qingcai Chen, Junying Chen, Wenxiu Zhou, Tingyu Liu, Xinlan Yang, Weihua Peng |  |
| 755 |  |  [Label-Enhanced Hierarchical Contextualized Representation for Sequential Metaphor Identification](https://doi.org/10.18653/v1/2021.emnlp-main.286) |  | 0 | Recent metaphor identification approaches mainly consider the contextual text features within a sentence or introduce external linguistic features to the model. But they usually ignore the extra information that the data can provide, such as the contextual metaphor information and broader discourse... | Shuqun Li, Liang Yang, Weidong He, Shiqi Zhang, Jingjie Zeng, Hongfei Lin |  |
| 756 |  |  [SpellBERT: A Lightweight Pretrained Model for Chinese Spelling Check](https://doi.org/10.18653/v1/2021.emnlp-main.287) |  | 0 | Chinese Spelling Check (CSC) is to detect and correct Chinese spelling errors. Many models utilize a predefined confusion set to learn a mapping between correct characters and its visually similar or phonetically similar misuses but the mapping may be out-of-domain. To that end, we propose... | Tuo Ji, Hang Yan, Xipeng Qiu |  |
| 757 |  |  [Automated Generation of Accurate & Fluent Medical X-ray Reports](https://doi.org/10.18653/v1/2021.emnlp-main.288) |  | 0 | Our paper aims to automate the generation of medical reports from chest X-ray image inputs, a critical yet time-consuming task for radiologists. Existing medical report generation efforts emphasize producing human-readable reports, yet the generated text may not be well aligned to the clinical... | Hoang T. N. Nguyen, Dong Nie, Taivanbat Badamdorj, Yujie Liu, Yingying Zhu, Jason Truong, Li Cheng |  |
| 758 |  |  [Enhancing Document Ranking with Task-adaptive Training and Segmented Token Recovery Mechanism](https://doi.org/10.18653/v1/2021.emnlp-main.289) |  | 0 | In this paper, we propose a new ranking model DR-BERT, which improves the Document Retrieval (DR) task by a task-adaptive training process and a Segmented Token Recovery Mechanism (STRM). In the task-adaptive training, we first pre-train DR-BERT to be domain-adaptive and then make the two-phase... | Xingwu Sun, Yanling Cui, Hongyin Tang, Fuzheng Zhang, Beihong Jin, Shi Wang |  |
| 759 |  |  [Abstract, Rationale, Stance: A Joint Model for Scientific Claim Verification](https://doi.org/10.18653/v1/2021.emnlp-main.290) |  | 0 | Scientific claim verification can help the researchers to easily find the target scientific papers with the sentence evidence from a large corpus for the given claim. Some existing works propose pipeline models on the three tasks of abstract retrieval, rationale selection and stance prediction.... | Zhiwei Zhang, Jiyi Li, Fumiyo Fukumoto, Yanming Ye |  |
| 760 |  |  [A Fine-Grained Domain Adaption Model for Joint Word Segmentation and POS Tagging](https://doi.org/10.18653/v1/2021.emnlp-main.291) |  | 0 | Domain adaption for word segmentation and POS tagging is a challenging problem for Chinese lexical processing. Self-training is one promising solution for it, which struggles to construct a set of high-quality pseudo training instances for the target domain. Previous work usually assumes a... | Peijie Jiang, Dingkun Long, Yueheng Sun, Meishan Zhang, Guangwei Xu, Pengjun Xie |  |
| 761 |  |  [Answering Open-Domain Questions of Varying Reasoning Steps from Text](https://doi.org/10.18653/v1/2021.emnlp-main.292) |  | 0 | We develop a unified system to answer directly from text open-domain questions that may require a varying number of retrieval steps. We employ a single multi-task transformer model to perform all the necessary subtasks—retrieving supporting facts, reranking them, and predicting the answer from all... | Peng Qi, Haejun Lee, Tg Sido, Christopher D. Manning |  |
| 762 |  |  [Adaptive Information Seeking for Open-Domain Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.293) |  | 0 | Information seeking is an essential step for open-domain question answering to efficiently gather evidence from a large corpus. Recently, iterative approaches have been proven to be effective for complex questions, by recursively retrieving new evidence at each step. However, almost all existing... | Yunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, Xueqi Cheng |  |
| 763 |  |  [Mapping probability word problems to executable representations](https://doi.org/10.18653/v1/2021.emnlp-main.294) |  | 0 | While solving math word problems automatically has received considerable attention in the NLP community, few works have addressed probability word problems specifically. In this paper, we employ and analyse various neural models for answering such word problems. In a two-step approach, the problem... | Simon Suster, Pieter Fivez, Pietro Totis, Angelika Kimmig, Jesse Davis, Luc De Raedt, Walter Daelemans |  |
| 764 |  |  [Enhancing Multiple-choice Machine Reading Comprehension by Punishing Illogical Interpretations](https://doi.org/10.18653/v1/2021.emnlp-main.295) |  | 0 | Machine Reading Comprehension (MRC), which requires a machine to answer questions given the relevant documents, is an important way to test machines’ ability to understand human language. Multiple-choice MRC is one of the most studied tasks in MRC due to the convenience of evaluation and the... | Yiming Ju, Yuanzhe Zhang, Zhixing Tian, Kang Liu, Xiaohuan Cao, Wenting Zhao, Jinlong Li, Jun Zhao |  |
| 765 |  |  [Large-Scale Relation Learning for Question Answering over Knowledge Bases with Pre-trained Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.296) |  | 0 | The key challenge of question answering over knowledge bases (KBQA) is the inconsistency between the natural language questions and the reasoning paths in the knowledge base (KB). Recent graph-based KBQA methods are good at grasping the topological structure of the graph but often ignore the... | Yuanmeng Yan, Rumei Li, Sirui Wang, Hongzhi Zhang, Daoguang Zan, Fuzheng Zhang, Wei Wu, Weiran Xu |  |
| 766 |  |  [Phrase Retrieval Learns Passage Retrieval, Too](https://doi.org/10.18653/v1/2021.emnlp-main.297) |  | 0 | Dense retrieval methods have shown great promise over sparse retrieval methods in a range of NLP problems. Among them, dense phrase retrieval—the most fine-grained retrieval unit—is appealing because phrases can be directly used as the output for question answering and slot filling tasks. In this... | Jinhyuk Lee, Alexander Wettig, Danqi Chen |  |
| 767 |  |  [Neural Natural Logic Inference for Interpretable Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.298) |  | 0 | Many open-domain question answering problems can be cast as a textual entailment task, where a question and candidate answers are concatenated to form hypotheses. A QA system then determines if the supporting knowledge bases, regarded as potential premises, entail the hypotheses. In this paper, we... | Jihao Shi, Xiao Ding, Li Du, Ting Liu, Bing Qin |  |
| 768 |  |  [Smoothing Dialogue States for Open Conversational Machine Reading](https://doi.org/10.18653/v1/2021.emnlp-main.299) |  | 0 | Conversational machine reading (CMR) requires machines to communicate with humans through multi-turn interactions between two salient dialogue states of decision making and question generation processes. In open CMR settings, as the more realistic scenario, the retrieved background knowledge would... | Zhuosheng Zhang, Siru Ouyang, Hai Zhao, Masao Utiyama, Eiichiro Sumita |  |
| 769 |  |  [FinQA: A Dataset of Numerical Reasoning over Financial Data](https://doi.org/10.18653/v1/2021.emnlp-main.300) |  | 0 | The sheer volume of financial statements makes it difficult for humans to access and analyze a business’s financials. Robust numerical reasoning likewise faces unique challenges in this domain. In this work, we focus on answering deep questions over financial data, aiming to automate the analysis... | Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, TingHao Kenneth Huang, Bryan R. Routledge, William Yang Wang |  |
| 770 |  |  [FiD-Ex: Improving Sequence-to-Sequence Models for Extractive Rationale Generation](https://doi.org/10.18653/v1/2021.emnlp-main.301) |  | 0 | Natural language (NL) explanations of model predictions are gaining popularity as a means to understand and verify decisions made by large black-box pre-trained models, for tasks such as Question Answering (QA) and Fact Verification. Recently, pre-trained sequence to sequence (seq2seq) models have... | Kushal Lakhotia, Bhargavi Paranjape, Asish Ghoshal, Scott Yih, Yashar Mehdad, Srini Iyer |  |
| 771 |  |  [RockNER: A Simple Method to Create Adversarial Examples for Evaluating the Robustness of Named Entity Recognition Models](https://doi.org/10.18653/v1/2021.emnlp-main.302) |  | 0 | To audit the robustness of named entity recognition (NER) models, we propose RockNER, a simple yet effective method to create natural adversarial examples. Specifically, at the entity level, we replace target entities with other entities of the same semantic class in Wikidata; at the context level,... | Bill Yuchen Lin, Wenyang Gao, Jun Yan, Ryan Moreno, Xiang Ren |  |
| 772 |  |  [Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI](https://doi.org/10.18653/v1/2021.emnlp-main.303) |  | 0 | Recently, language models (LMs) have achieved significant performance on many NLU tasks, which has spurred widespread interest for their possible applications in the scientific and social area. However, LMs have faced much criticism of whether they are truly capable of reasoning in NLU. In this... | Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, Yaohui Jin |  |
| 773 |  |  [Constructing a Psychometric Testbed for Fair Natural Language Processing](https://doi.org/10.18653/v1/2021.emnlp-main.304) |  | 0 | Psychometric measures of ability, attitudes, perceptions, and beliefs are crucial for understanding user behavior in various contexts including health, security, e-commerce, and finance. Traditionally, psychometric dimensions have been measured and collected using survey-based methods. Inferring... | Ahmed Abbasi, David G. Dobolyi, John P. Lalor, Richard G. Netemeyer, Kendall Smith, Yi Yang |  |
| 774 |  |  [COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval](https://doi.org/10.18653/v1/2021.emnlp-main.305) |  | 0 | We present a large, challenging dataset, COUGH, for COVID-19 FAQ retrieval. Similar to a standard FAQ dataset, COUGH consists of three parts: FAQ Bank, Query Bank and Relevance Set. The FAQ Bank contains ~16K FAQ items scraped from 55 credible websites (e.g., CDC and WHO). For evaluation, we... | Xinliang Frederick Zhang, Heming Sun, Xiang Yue, Simon M. Lin, Huan Sun |  |
| 775 |  |  [Chinese WPLC: A Chinese Dataset for Evaluating Pretrained Language Models on Word Prediction Given Long-Range Context](https://doi.org/10.18653/v1/2021.emnlp-main.306) |  | 0 | This paper presents a Chinese dataset for evaluating pretrained language models on Word Prediction given Long-term Context (Chinese WPLC). We propose both automatic and manual selection strategies tailored to Chinese to guarantee that target words in passages collected from over 69K novels can only... | Huibin Ge, Chenxi Sun, Deyi Xiong, Qun Liu |  |
| 776 |  |  [WinoLogic: A Zero-Shot Logic-based Diagnostic Dataset for Winograd Schema Challenge](https://doi.org/10.18653/v1/2021.emnlp-main.307) |  | 0 | The recent success of neural language models (NLMs) on the Winograd Schema Challenge has called for further investigation of the commonsense reasoning ability of these models. Previous diagnostic datasets rely on crowd-sourcing which fails to provide coherent commonsense crucial for solving WSC... | Weinan He, Canming Huang, Yongmei Liu, Xiaodan Zhu |  |
| 777 |  |  [Pseudo Zero Pronoun Resolution Improves Zero Anaphora Resolution](https://doi.org/10.18653/v1/2021.emnlp-main.308) |  | 0 | Masked language models (MLMs) have contributed to drastic performance improvements with regard to zero anaphora resolution (ZAR). To further improve this approach, in this study, we made two proposals. The first is a new pretraining task that trains MLMs on anaphoric relations with explicit... | Ryuto Konno, Shun Kiyono, Yuichiroh Matsubayashi, Hiroki Ouchi, Kentaro Inui |  |
| 778 |  |  [Aligning Cross-lingual Sentence Representations with Dual Momentum Contrast](https://doi.org/10.18653/v1/2021.emnlp-main.309) |  | 0 | In this paper, we propose to align sentence representations from different languages into a unified embedding space, where semantic similarities (both cross-lingual and monolingual) can be computed with a simple dot product. Pre-trained language models are fine-tuned with the translation ranking... | Liang Wang, Wei Zhao, Jingming Liu |  |
| 779 |  |  [Total Recall: a Customized Continual Learning Method for Neural Semantic Parsers](https://doi.org/10.18653/v1/2021.emnlp-main.310) |  | 0 | This paper investigates continual learning for semantic parsing. In this setting, a neural semantic parser learns tasks sequentially without accessing full training data from previous tasks. Direct application of the SOTA continual learning algorithms to this problem fails to achieve comparable... | Zhuang Li, Lizhen Qu, Gholamreza Haffari |  |
| 780 |  |  [Exophoric Pronoun Resolution in Dialogues with Topic Regularization](https://doi.org/10.18653/v1/2021.emnlp-main.311) |  | 0 | Resolving pronouns to their referents has long been studied as a fundamental natural language understanding problem. Previous works on pronoun coreference resolution (PCR) mostly focus on resolving pronouns to mentions in text while ignoring the exophoric scenario. Exophoric pronouns are common in... | Xintong Yu, Hongming Zhang, Yangqiu Song, Changshui Zhang, Kun Xu, Dong Yu |  |
| 781 |  |  [Context-Aware Interaction Network for Question Matching](https://doi.org/10.18653/v1/2021.emnlp-main.312) |  | 0 | Impressive milestones have been achieved in text matching by adopting a cross-attention mechanism to capture pertinent semantic connections between two sentence representations. However, regular cross-attention focuses on word-level links between the two input sequences, neglecting the importance... | Zhe Hu, Zuohui Fu, Yu Yin, Gerard de Melo |  |
| 782 |  |  [TEMP: Taxonomy Expansion with Dynamic Margin Loss through Taxonomy-Paths](https://doi.org/10.18653/v1/2021.emnlp-main.313) |  | 0 | As an essential form of knowledge representation, taxonomies are widely used in various downstream natural language processing tasks. However, with the continuously rising of new concepts, many existing taxonomies are unable to maintain coverage by manual expansion. In this paper, we propose TEMP,... | Zichen Liu, Hongyuan Xu, Yanlong Wen, Ning Jiang, Haiying Wu, Xiaojie Yuan |  |
| 783 |  |  [A Graph-Based Neural Model for End-to-End Frame Semantic Parsing](https://doi.org/10.18653/v1/2021.emnlp-main.314) |  | 0 | Frame semantic parsing is a semantic analysis task based on FrameNet which has received great attention recently. The task usually involves three subtasks sequentially: (1) target identification, (2) frame classification and (3) semantic role labeling. The three subtasks are closely related while... | Zhichao Lin, Yueheng Sun, Meishan Zhang |  |
| 784 |  |  [Virtual Data Augmentation: A Robust and General Framework for Fine-tuning Pre-trained Models](https://doi.org/10.18653/v1/2021.emnlp-main.315) |  | 0 | Recent works have shown that powerful pre-trained language models (PLM) can be fooled by small perturbations or intentional attacks. To solve this issue, various data augmentation techniques are proposed to improve the robustness of PLMs. However, it is still challenging to augment semantically... | Kun Zhou, Wayne Xin Zhao, Sirui Wang, Fuzheng Zhang, Wei Wu, JiRong Wen |  |
| 785 |  |  [CATE: A Contrastive Pre-trained Model for Metaphor Detection with Semi-supervised Learning](https://doi.org/10.18653/v1/2021.emnlp-main.316) |  | 0 | Metaphors are ubiquitous in natural language, and detecting them requires contextual reasoning about whether a semantic incongruence actually exists. Most existing work addresses this problem using pre-trained contextualized models. Despite their success, these models require a large amount of... | Zhenxi Lin, Qianli Ma, Jiangyue Yan, Jieyu Chen |  |
| 786 |  |  [To be Closer: Learning to Link up Aspects with Opinions](https://doi.org/10.18653/v1/2021.emnlp-main.317) |  | 0 | Dependency parse trees are helpful for discovering the opinion words in aspect-based sentiment analysis (ABSA) (CITATION). However, the trees obtained from off-the-shelf dependency parsers are static, and could be sub-optimal in ABSA. This is because the syntactic trees are not designed for... | Yuxiang Zhou, Lejian Liao, Yang Gao, Zhanming Jie, Wei Lu |  |
| 787 |  |  [Seeking Common but Distinguishing Difference, A Joint Aspect-based Sentiment Analysis Model](https://doi.org/10.18653/v1/2021.emnlp-main.318) |  | 0 | Aspect-based sentiment analysis (ABSA) task consists of three typical subtasks: aspect term extraction, opinion term extraction, and sentiment polarity classification. These three subtasks are usually performed jointly to save resources and reduce the error propagation in the pipeline. However,... | Hongjiang Jing, Zuchao Li, Hai Zhao, Shu Jiang |  |
| 788 |  |  [Argument Pair Extraction with Mutual Guidance and Inter-sentence Relation Graph](https://doi.org/10.18653/v1/2021.emnlp-main.319) |  | 0 | Argument pair extraction (APE) aims to extract interactive argument pairs from two passages of a discussion. Previous work studied this task in the context of peer review and rebuttal, and decomposed it into a sequence labeling task and a sentence relation classification task. However, despite the... | Jianzhu Bao, Bin Liang, Jingyi Sun, Yice Zhang, Min Yang, Ruifeng Xu |  |
| 789 |  |  [Emotion Inference in Multi-Turn Conversations with Addressee-Aware Module and Ensemble Strategy](https://doi.org/10.18653/v1/2021.emnlp-main.320) |  | 0 | Emotion inference in multi-turn conversations aims to predict the participant’s emotion in the next upcoming turn without knowing the participant’s response yet, and is a necessary step for applications such as dialogue planning. However, it is a severe challenge to perceive and reason about the... | Dayu Li, Xiaodan Zhu, Yang Li, Suge Wang, Deyu Li, Jian Liao, Jianxing Zheng |  |
| 790 |  |  [Improving Federated Learning for Aspect-based Sentiment Analysis via Topic Memories](https://doi.org/10.18653/v1/2021.emnlp-main.321) |  | 0 | Aspect-based sentiment analysis (ABSA) predicts the sentiment polarity towards a particular aspect term in a sentence, which is an important task in real-world applications. To perform ABSA, the trained model is required to have a good understanding of the contextual information, especially the... | Han Qin, Guimin Chen, Yuanhe Tian, Yan Song |  |
| 791 |  |  [Comparative Opinion Quintuple Extraction from Product Reviews](https://doi.org/10.18653/v1/2021.emnlp-main.322) |  | 0 | As an important task in opinion mining, comparative opinion mining aims to identify comparative sentences from product reviews, extract the comparative elements, and obtain the corresponding comparative opinion tuples. However, most previous studies simply regarded comparative tuple extraction as... | Ziheng Liu, Rui Xia, Jianfei Yu |  |
| 792 |  |  [CTAL: Pre-training Cross-modal Transformer for Audio-and-Language Representations](https://doi.org/10.18653/v1/2021.emnlp-main.323) |  | 0 | Existing audio-language task-specific predictive approaches focus on building complicated late-fusion mechanisms. However, these models are facing challenges of overfitting with limited labels and low model generalization abilities. In this paper, we present a Cross-modal Transformer for... | Hang Li, Wenbiao Ding, Yu Kang, Tianqiao Liu, Zhongqin Wu, Zitao Liu |  |
| 793 |  |  [Relation-aware Video Reading Comprehension for Temporal Language Grounding](https://doi.org/10.18653/v1/2021.emnlp-main.324) |  | 0 | Temporal language grounding in videos aims to localize the temporal span relevant to the given query sentence. Previous methods treat it either as a boundary regression task or a span extraction task. This paper will formulate temporal language grounding into video reading comprehension and propose... | Jialin Gao, Xin Sun, Mengmeng Xu, Xi Zhou, Bernard Ghanem |  |
| 794 |  |  [Mutual-Learning Improves End-to-End Speech Translation](https://doi.org/10.18653/v1/2021.emnlp-main.325) |  | 0 | A currently popular research area in end-to-end speech translation is the use of knowledge distillation from a machine translation (MT) task to improve the speech translation (ST) task. However, such scenario obviously only allows one way transfer, which is limited by the performance of the teacher... | Jiawei Zhao, Wei Luo, Boxing Chen, Andrew Gilman |  |
| 795 |  |  [Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.326) |  | 0 | Multimodal abstractive summarization (MAS) models that summarize videos (vision modality) and their corresponding transcripts (text modality) are able to extract the essential information from massive multimodal data on the Internet. Recently, large-scale generative pre-trained language models... | Tiezheng Yu, Wenliang Dai, Zihan Liu, Pascale Fung |  |
| 796 |  |  [Natural Language Video Localization with Learnable Moment Proposals](https://doi.org/10.18653/v1/2021.emnlp-main.327) |  | 0 | Given an untrimmed video and a natural language query, Natural Language Video Localization (NLVL) aims to identify the video moment described by query. To address this task, existing methods can be roughly grouped into two groups: 1) propose-and-rank models first define a set of hand-designed... | Shaoning Xiao, Long Chen, Jian Shao, Yueting Zhuang, Jun Xiao |  |
| 797 |  |  [Language-Aligned Waypoint (LAW) Supervision for Vision-and-Language Navigation in Continuous Environments](https://doi.org/10.18653/v1/2021.emnlp-main.328) |  | 0 | In the Vision-and-Language Navigation (VLN) task an embodied agent navigates a 3D environment, following natural language instructions. A challenge in this task is how to handle ‘off the path’ scenarios where an agent veers from a reference path. Prior work supervises the agent with actions based... | Sonia Raychaudhuri, Saim Wani, Shivansh Patel, Unnat Jain, Angel X. Chang |  |
| 798 |  |  [How to leverage the multimodal EHR data for better medical prediction?](https://doi.org/10.18653/v1/2021.emnlp-main.329) |  | 0 | Healthcare is becoming a more and more important research topic recently. With the growing data in the healthcare domain, it offers a great opportunity for deep learning to improve the quality of service and reduce costs. However, the complexity of electronic health records (EHR) data is a... | Bo Yang, Lijun Wu |  |
| 799 |  |  [Considering Nested Tree Structure in Sentence Extractive Summarization with Pre-trained Transformer](https://doi.org/10.18653/v1/2021.emnlp-main.330) |  | 0 | Sentence extractive summarization shortens a document by selecting sentences for a summary while preserving its important contents. However, constructing a coherent and informative summary is difficult using a pre-trained BERT-based encoder since it is not explicitly trained for representing the... | Jingun Kwon, Naoki Kobayashi, Hidetaka Kamigaito, Manabu Okumura |  |
| 800 |  |  [Frame Semantic-Enhanced Sentence Modeling for Sentence-level Extractive Text Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.331) |  | 0 | Sentence-level extractive text summarization aims to select important sentences from a given document. However, it is very challenging to model the importance of sentences. In this paper, we propose a novel Frame Semantic-Enhanced Sentence Modeling for Extractive Summarization, which leverages... | Yong Guan, Shaoru Guo, Ru Li, Xiaoli Li, Hongye Tan |  |
| 801 |  |  [CAST: Enhancing Code Summarization with Hierarchical Splitting and Reconstruction of Abstract Syntax Trees](https://doi.org/10.18653/v1/2021.emnlp-main.332) |  | 0 | Code summarization aims to generate concise natural language descriptions of source code, which can help improve program comprehension and maintenance. Recent studies show that syntactic and structural information extracted from abstract syntax trees (ASTs) is conducive to summary generation.... | Ensheng Shi, Yanlin Wang, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, Hongbin Sun |  |
| 802 |  |  [SgSum: Transforming Multi-document Summarization into Sub-graph Selection](https://doi.org/10.18653/v1/2021.emnlp-main.333) |  | 0 | Most of existing extractive multi-document summarization (MDS) methods score each sentence individually and extract salient sentences one by one to compose a summary, which have two main drawbacks: (1) neglecting both the intra and cross-document relations between sentences; (2) neglecting the... | Moye Chen, Wei Li, Jiachen Liu, Xinyan Xiao, Hua Wu, Haifeng Wang |  |
| 803 |  |  [Event Graph based Sentence Fusion](https://doi.org/10.18653/v1/2021.emnlp-main.334) |  | 0 | Sentence fusion is a conditional generation task that merges several related sentences into a coherent one, which can be deemed as a summary sentence. The importance of sentence fusion has long been recognized by communities in natural language generation, especially in text summarization. It... | Ruifeng Yuan, Zili Wang, Wenjie Li |  |
| 804 |  |  [Transformer-based Lexically Constrained Headline Generation](https://doi.org/10.18653/v1/2021.emnlp-main.335) |  | 0 | This paper explores a variant of automatic headline generation methods, where a generated headline is required to include a given phrase such as a company or a product name. Previous methods using Transformer-based models generate a headline including a given phrase by providing the encoder with... | Kosuke Yamada, Yuta Hitomi, Hideaki Tamori, Ryohei Sasano, Naoaki Okazaki, Kentaro Inui, Koichi Takeda |  |
| 805 |  |  [Learn to Copy from the Copying History: Correlational Copy Network for Abstractive Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.336) |  | 0 | The copying mechanism has had considerable success in abstractive summarization, facilitating models to directly copy words from the input text to the output summary. Existing works mostly employ encoder-decoder attention, which applies copying at each time step independently of the former ones.... | Haoran Li, Song Xu, Peng Yuan, Yujia Wang, Youzheng Wu, Xiaodong He, Bowen Zhou |  |
| 806 |  |  [Gradient-Based Adversarial Factual Consistency Evaluation for Abstractive Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.337) |  | 0 | Neural abstractive summarization systems have gained significant progress in recent years. However, abstractive summarization often produce inconsisitent statements or false facts. How to automatically generate highly abstract yet factually correct summaries? In this paper, we proposed an efficient... | Zhiyuan Zeng, Jiaze Chen, Weiran Xu, Lei Li |  |
| 807 |  |  [Word Reordering for Zero-shot Cross-lingual Structured Prediction](https://doi.org/10.18653/v1/2021.emnlp-main.338) |  | 0 | Adapting word order from one language to another is a key problem in cross-lingual structured prediction. Current sentence encoders (e.g., RNN, Transformer with position embeddings) are usually word order sensitive. Even with uniform word form representations (MUSE, mBERT), word order discrepancies... | Tao Ji, Yong Jiang, Tao Wang, Zhongqiang Huang, Fei Huang, Yuanbin Wu, Xiaoling Wang |  |
| 808 |  |  [A Unified Encoding of Structures in Transition Systems](https://doi.org/10.18653/v1/2021.emnlp-main.339) |  | 0 | Transition systems usually contain various dynamic structures (e.g., stacks, buffers). An ideal transition-based model should encode these structures completely and efficiently. Previous works relying on templates or neural network structures either only encode partial structure information or... | Tao Ji, Yong Jiang, Tao Wang, Zhongqiang Huang, Fei Huang, Yuanbin Wu, Xiaoling Wang |  |
| 809 |  |  [Improving Unsupervised Question Answering via Summarization-Informed Question Generation](https://doi.org/10.18653/v1/2021.emnlp-main.340) |  | 0 | Question Generation (QG) is the task of generating a plausible question for a given <passage, answer> pair. Template-based QG uses linguistically-informed heuristics to transform declarative sentences into interrogatives, whereas supervised QG uses existing Question Answering (QA) datasets to train... | Chenyang Lyu, Lifeng Shang, Yvette Graham, Jennifer Foster, Xin Jiang, Qun Liu |  |
| 810 |  |  [TransferNet: An Effective and Transparent Framework for Multi-hop Question Answering over Relation Graph](https://doi.org/10.18653/v1/2021.emnlp-main.341) |  | 0 | Multi-hop Question Answering (QA) is a challenging task because it requires precise reasoning with entity relations at every step towards the answer. The relations can be represented in terms of labels in knowledge graph (e.g., spouse) or text in text corpus (e.g., they have been married for 26... | Jiaxin Shi, Shulin Cao, Lei Hou, Juanzi Li, Hanwang Zhang |  |
| 811 |  |  [Topic Transferable Table Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.342) |  | 0 | Weakly-supervised table question-answering (TableQA) models have achieved state-of-art performance by using pre-trained BERT transformer to jointly encoding a question and a table to produce structured query for the question. However, in practical settings TableQA systems are deployed over table... | Saneem A. Chemmengath, Vishwajeet Kumar, Samarth Bharadwaj, Jaydeep Sen, Mustafa Canim, Soumen Chakrabarti, Alfio Gliozzo, Karthik Sankaranarayanan |  |
| 812 |  |  [WebSRC: A Dataset for Web-Based Structural Reading Comprehension](https://doi.org/10.18653/v1/2021.emnlp-main.343) |  | 0 | Web search is an essential way for humans to obtain information, but it’s still a great challenge for machines to understand the contents of web pages. In this paper, we introduce the task of web-based structural reading comprehension. Given a web page and a question about it, the task is to find... | Xingyu Chen, Zihan Zhao, Lu Chen, Jiabao Ji, Danyang Zhang, Ao Luo, Yuxuan Xiong, Kai Yu |  |
| 813 |  |  [Cryptonite: A Cryptic Crossword Benchmark for Extreme Ambiguity in Language](https://doi.org/10.18653/v1/2021.emnlp-main.344) |  | 0 | Current NLP datasets targeting ambiguity can be solved by a native speaker with relative ease. We present Cryptonite, a large-scale dataset based on cryptic crosswords, which is both linguistically complex and naturally sourced. Each example in Cryptonite is a cryptic clue, a short phrase or... | Avia Efrat, Uri Shaham, Dan Kilman, Omer Levy |  |
| 814 |  |  [End-to-End Entity Resolution and Question Answering Using Differentiable Knowledge Graphs](https://doi.org/10.18653/v1/2021.emnlp-main.345) |  | 0 | Recently, end-to-end (E2E) trained models for question answering over knowledge graphs (KGQA) have delivered promising results using only a weakly supervised dataset. However, these models are trained and evaluated in a setting where hand-annotated question entities are supplied to the model,... | Amir Saffari, Armin Oliya, Priyanka Sen, Tom Ayoola |  |
| 815 |  |  [Improving Query Graph Generation for Complex Question Answering over Knowledge Base](https://doi.org/10.18653/v1/2021.emnlp-main.346) |  | 0 | Most of the existing Knowledge-based Question Answering (KBQA) methods first learn to map the given question to a query graph, and then convert the graph to an executable query to find the answer. The query graph is typically expanded progressively from the topic entity based on a sequence... | Kechen Qin, Cheng Li, Virgil Pavlu, Javed A. Aslam |  |
| 816 |  |  [DiscoDVT: Generating Long Text with Discourse-Aware Discrete Variational Transformer](https://doi.org/10.18653/v1/2021.emnlp-main.347) |  | 0 | Despite the recent advances in applying pre-trained language models to generate high-quality texts, generating long passages that maintain long-range coherence is yet challenging for these models. In this paper, we propose DiscoDVT, a discourse-aware discrete variational Transformer to tackle the... | Haozhe Ji, Minlie Huang |  |
| 817 |  |  [Mathematical Word Problem Generation from Commonsense Knowledge Graph and Equations](https://doi.org/10.18653/v1/2021.emnlp-main.348) |  | 0 | There is an increasing interest in the use of mathematical word problem (MWP) generation in educational assessment. Different from standard natural question generation, MWP generation needs to maintain the underlying mathematical operations between quantities and variables, while at the same time... | Tianqiao Liu, Qiang Fang, Wenbiao Ding, Hang Li, Zhongqin Wu, Zitao Liu |  |
| 818 |  |  [Generic resources are what you need: Style transfer tasks without task-specific parallel training data](https://doi.org/10.18653/v1/2021.emnlp-main.349) |  | 0 | Style transfer aims to rewrite a source text in a different target style while preserving its content. We propose a novel approach to this task that leverages generic resources, and without using any task-specific parallel (source–target) data outperforms existing unsupervised approaches on the two... | Huiyuan Lai, Antonio Toral, Malvina Nissim |  |
| 819 |  |  [Revisiting Pivot-Based Paraphrase Generation: Language Is Not the Only Optional Pivot](https://doi.org/10.18653/v1/2021.emnlp-main.350) |  | 0 | Paraphrases refer to texts that convey the same meaning with different expression forms. Pivot-based methods, also known as the round-trip translation, have shown promising results in generating high-quality paraphrases. However, existing pivot-based methods all rely on language as the pivot, where... | Yitao Cai, Yue Cao, Xiaojun Wan |  |
| 820 |  |  [Structural Adapters in Pretrained Language Models for AMR-to-Text Generation](https://doi.org/10.18653/v1/2021.emnlp-main.351) |  | 0 | Pretrained language models (PLM) have recently advanced graph-to-text generation, where the input graph is linearized into a sequence and fed into the PLM to obtain its representation. However, efficiently encoding the graph structure in PLMs is challenging because such models were pretrained on... | Leonardo F. R. Ribeiro, Yue Zhang, Iryna Gurevych |  |
| 821 |  |  [Data-to-text Generation by Splicing Together Nearest Neighbors](https://doi.org/10.18653/v1/2021.emnlp-main.352) |  | 0 | We propose to tackle data-to-text generation tasks by directly splicing together retrieved segments of text from “neighbor” source-target pairs. Unlike recent work that conditions on retrieved neighbors but generates text token-by-token, left-to-right, we learn a policy that directly manipulates... | Sam Wiseman, Arturs Backurs, Karl Stratos |  |
| 822 |  |  [Contextualize Knowledge Bases with Transformer for End-to-end Task-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2021.emnlp-main.353) |  | 0 | Incorporating knowledge bases (KB) into end-to-end task-oriented dialogue systems is challenging, since it requires to properly represent the entity of KB, which is associated with its KB context and dialogue context. The existing works represent the entity with only perceiving a part of its KB... | Yanjie Gou, Yinjie Lei, Lingqiao Liu, Yong Dai, Chunxu Shen |  |
| 823 |  |  [Efficient Dialogue Complementary Policy Learning via Deep Q-network Policy and Episodic Memory Policy](https://doi.org/10.18653/v1/2021.emnlp-main.354) |  | 0 | Deep reinforcement learning has shown great potential in training dialogue policies. However, its favorable performance comes at the cost of many rounds of interaction. Most of the existing dialogue policy methods rely on a single learning system, while the human brain has two specialized learning... | Yangyang Zhao, Zhenyu Wang, Changxi Zhu, Shihan Wang |  |
| 824 |  |  [CRFR: Improving Conversational Recommender Systems via Flexible Fragments Reasoning on Knowledge Graphs](https://doi.org/10.18653/v1/2021.emnlp-main.355) |  | 0 | Although paths of user interests shift in knowledge graphs (KGs) can benefit conversational recommender systems (CRS), explicit reasoning on KGs has not been well considered in CRS, due to the complex of high-order and incomplete paths. We propose CRFR, which effectively does explicit multi-hop... | Jinfeng Zhou, Bo Wang, Ruifang He, Yuexian Hou |  |
| 825 |  |  [DuRecDial 2.0: A Bilingual Parallel Corpus for Conversational Recommendation](https://doi.org/10.18653/v1/2021.emnlp-main.356) |  | 0 | In this paper, we provide a bilingual parallel human-to-human recommendation dialog dataset (DuRecDial 2.0) to enable researchers to explore a challenging task of multilingual and cross-lingual conversational recommendation. The difference between DuRecDial 2.0 and existing conversational... | Zeming Liu, Haifeng Wang, Zhengyu Niu, Hua Wu, Wanxiang Che |  |
| 826 |  |  [End-to-End Learning of Flowchart Grounded Task-Oriented Dialogs](https://doi.org/10.18653/v1/2021.emnlp-main.357) |  | 0 | We propose a novel problem within end-to-end learning of task oriented dialogs (TOD), in which the dialog system mimics a troubleshooting agent who helps a user by diagnosing their problem (e.g., car not starting). Such dialogs are grounded in domain-specific flowcharts, which the agent is supposed... | Dinesh Raghu, Shantanu Agarwal, Sachindra Joshi, Mausam |  |
| 827 |  |  [Dimensional Emotion Detection from Categorical Emotion](https://doi.org/10.18653/v1/2021.emnlp-main.358) |  | 0 | We present a model to predict fine-grained emotions along the continuous dimensions of valence, arousal, and dominance (VAD) with a corpus with categorical emotion annotations. Our model is trained by minimizing the EMD (Earth Mover’s Distance) loss between the predicted VAD score distribution and... | Sungjoon Park, Jiseon Kim, Seonghyeon Ye, Jaeyeol Jeon, Heeyoung Park, Alice Oh |  |
| 828 |  |  [Not All Negatives are Equal: Label-Aware Contrastive Loss for Fine-grained Text Classification](https://doi.org/10.18653/v1/2021.emnlp-main.359) |  | 0 | Fine-grained classification involves dealing with datasets with larger number of classes with subtle differences between them. Guiding the model to focus on differentiating dimensions between these commonly confusable classes is key to improving performance on fine-grained tasks. In this work, we... | Varsha Suresh, Desmond C. Ong |  |
| 829 |  |  [Joint Multi-modal Aspect-Sentiment Analysis with Auxiliary Cross-modal Relation Detection](https://doi.org/10.18653/v1/2021.emnlp-main.360) |  | 0 | Aspect terms extraction (ATE) and aspect sentiment classification (ASC) are two fundamental and fine-grained sub-tasks in aspect-level sentiment analysis (ALSA). In the textual analysis, joint extracting both aspect terms and sentiment polarities has been drawn much attention due to the better... | Xincheng Ju, Dong Zhang, Rong Xiao, Junhui Li, Shoushan Li, Min Zhang, Guodong Zhou |  |
| 830 |  |  [Solving Aspect Category Sentiment Analysis as a Text Generation Task](https://doi.org/10.18653/v1/2021.emnlp-main.361) |  | 0 | Aspect category sentiment analysis has attracted increasing research attention. The dominant methods make use of pre-trained language models by learning effective aspect category-specific representations, and adding specific output layers to its pre-trained representation. We consider a more direct... | Jian Liu, Zhiyang Teng, Leyang Cui, Hanmeng Liu, Yue Zhang |  |
| 831 |  |  [Semantics-Preserved Data Augmentation for Aspect-Based Sentiment Analysis](https://doi.org/10.18653/v1/2021.emnlp-main.362) |  | 0 | Both the issues of data deficiencies and semantic consistency are important for data augmentation. Most of previous methods address the first issue, but ignore the second one. In the cases of aspect-based sentiment analysis, violation of the above issues may change the aspect and sentiment... | TingWei Hsu, ChungChi Chen, HenHsen Huang, HsinHsi Chen |  |
| 832 |  |  [The Effect of Round-Trip Translation on Fairness in Sentiment Analysis](https://doi.org/10.18653/v1/2021.emnlp-main.363) |  | 0 | Sentiment analysis systems have been shown to exhibit sensitivity to protected attributes. Round-trip translation, on the other hand, has been shown to normalize text. We explore the impact of round-trip translation on the demographic parity of sentiment classifiers and show how round-trip... | Jonathan Gabel Christiansen, Mathias Gammelgaard, Anders Søgaard |  |
| 833 |  |  [CHoRaL: Collecting Humor Reaction Labels from Millions of Social Media Users](https://doi.org/10.18653/v1/2021.emnlp-main.364) |  | 0 | Humor detection has gained attention in recent years due to the desire to understand user-generated content with figurative language. However, substantial individual and cultural differences in humor perception make it very difficult to collect a large-scale humor dataset with reliable humor... | Zixiaofan Yang, Shayan Hooshmand, Julia Hirschberg |  |
| 834 |  |  [CSDS: A Fine-Grained Chinese Dataset for Customer Service Dialogue Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.365) |  | 0 | Dialogue summarization has drawn much attention recently. Especially in the customer service domain, agents could use dialogue summaries to help boost their works by quickly knowing customer’s issues and service progress. These applications require summaries to contain the perspective of a single... | Haitao Lin, Liqun Ma, Junnan Zhu, Lu Xiang, Yu Zhou, Jiajun Zhang, Chengqing Zong |  |
| 835 |  |  [CodRED: A Cross-Document Relation Extraction Dataset for Acquiring Knowledge in the Wild](https://doi.org/10.18653/v1/2021.emnlp-main.366) |  | 0 | Existing relation extraction (RE) methods typically focus on extracting relational facts between entity pairs within single sentences or documents. However, a large quantity of relational facts in knowledge bases can only be inferred across documents in practice. In this work, we present the... | Yuan Yao, Jiaju Du, Yankai Lin, Peng Li, Zhiyuan Liu, Jie Zhou, Maosong Sun |  |
| 836 |  |  [Building and Evaluating Open-Domain Dialogue Corpora with Clarifying Questions](https://doi.org/10.18653/v1/2021.emnlp-main.367) |  | 0 | Enabling open-domain dialogue systems to ask clarifying questions when appropriate is an important direction for improving the quality of the system response. Namely, for cases when a user request is not specific enough for a conversation system to provide an answer right away, it is desirable to... | Mohammad Aliannejadi, Julia Kiseleva, Aleksandr Chuklin, Jeff Dalton, Mikhail Burtsev |  |
| 837 |  |  [We Need to Talk About train-dev-test Splits](https://doi.org/10.18653/v1/2021.emnlp-main.368) |  | 0 | Standard train-dev-test splits used to benchmark multiple models against each other are ubiquitously used in Natural Language Processing (NLP). In this setup, the train data is used for training the model, the development set for evaluating different versions of the proposed model(s) during... | Rob van der Goot |  |
| 838 |  |  [PhoMT: A High-Quality and Large-Scale Benchmark Dataset for Vietnamese-English Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.369) |  | 0 | We introduce a high-quality and large-scale Vietnamese-English parallel dataset of 3.02M sentence pairs, which is 2.9M pairs larger than the benchmark Vietnamese-English machine translation corpus IWSLT15. We conduct experiments comparing strong neural baselines and well-known automatic translation... | Long Doan, Linh The Nguyen, Nguyen Luong Tran, Thai Hoang, Dat Quoc Nguyen |  |
| 839 |  |  [Lying Through One's Teeth: A Study on Verbal Leakage Cues](https://doi.org/10.18653/v1/2021.emnlp-main.370) |  | 0 | Although many studies use the LIWC lexicon to show the existence of verbal leakage cues in lie detection datasets, none mention how verbal leakage cues are influenced by means of data collection, or the impact thereof on the performance of models. In this paper, we study verbal leakage cues to... | MinHsuan Yeh, LunWei Ku |  |
| 840 |  |  [Multi-granularity Textual Adversarial Attack with Behavior Cloning](https://doi.org/10.18653/v1/2021.emnlp-main.371) |  | 0 | Recently, the textual adversarial attack models become increasingly popular due to their successful in estimating the robustness of NLP models. However, existing works have obvious deficiencies. (1)They usually consider only a single granularity of modification strategies (e.g. word-level or... | Yangyi Chen, Jin Su, Wei Wei |  |
| 841 |  |  [All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality](https://doi.org/10.18653/v1/2021.emnlp-main.372) |  | 0 | Similarity measures are a vital tool for understanding how language models represent and process language. Standard representational similarity measures such as cosine similarity and Euclidean distance have been successfully used in static word embedding models to understand how words cluster in... | William Timkey, Marten van Schijndel |  |
| 842 |  |  [Incorporating Residual and Normalization Layers into Analysis of Masked Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.373) |  | 0 | Transformer architecture has become ubiquitous in the natural language processing field. To interpret the Transformer-based models, their attention patterns have been extensively analyzed. However, the Transformer architecture is not only composed of the multi-head attention; other components can... | Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, Kentaro Inui |  |
| 843 |  |  [Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer](https://doi.org/10.18653/v1/2021.emnlp-main.374) |  | 0 | Adversarial attacks and backdoor attacks are two common security threats that hang over deep learning. Both of them harness task-irrelevant features of data in their implementation. Text style is a feature that is naturally irrelevant to most NLP tasks, and thus suitable for adversarial and... | Fanchao Qi, Yangyi Chen, Xurui Zhang, Mukai Li, Zhiyuan Liu, Maosong Sun |  |
| 844 |  |  [Sociolectal Analysis of Pretrained Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.375) |  | 0 | Using data from English cloze tests, in which subjects also self-reported their gender, age, education, and race, we examine performance differences of pretrained language models across demographic groups, defined by these (protected) attributes. We demonstrate wide performance gaps across... | Sheng Zhang, Xin Zhang, Weiming Zhang, Anders Søgaard |  |
| 845 |  |  [Examining Cross-lingual Contextual Embeddings with Orthogonal Structural Probes](https://doi.org/10.18653/v1/2021.emnlp-main.376) |  | 0 | State-of-the-art contextual embeddings are obtained from large language models available only for a few languages. For others, we need to learn representations using a multilingual model. There is an ongoing debate on whether multilingual embeddings can be aligned in a space shared across many... | Tomasz Limisiewicz, David Marecek |  |
| 846 |  |  [Are Transformers a Modern Version of ELIZA? Observations on French Object Verb Agreement](https://doi.org/10.18653/v1/2021.emnlp-main.377) |  | 0 | Many recent works have demonstrated that unsupervised sentence representations of neural networks encode syntactic information by observing that neural language models are able to predict the agreement between a verb and its subject. We take a critical look at this line of research by showing that... | Bingzhi Li, Guillaume Wisniewski, Benoît Crabbé |  |
| 847 |  |  [Fine-grained Entity Typing via Label Reasoning](https://doi.org/10.18653/v1/2021.emnlp-main.378) |  | 0 | Conventional entity typing approaches are based on independent classification paradigms, which make them difficult to recognize inter-dependent, long-tailed and fine-grained entity types. In this paper, we argue that the implicitly entailed extrinsic and intrinsic dependencies between labels can... | Qing Liu, Hongyu Lin, Xinyan Xiao, Xianpei Han, Le Sun, Hua Wu |  |
| 848 |  |  [Enhanced Language Representation with Label Knowledge for Span Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.379) |  | 0 | Span extraction, aiming to extract text spans (such as words or phrases) from plain text, is a fundamental process in Information Extraction. Recent works introduce the label knowledge to enhance the text representation by formalizing the span extraction task into a question answering problem (QA... | Pan Yang, Xin Cong, Zhenyu Sun, Xingwu Liu |  |
| 849 |  |  [PRIDE: Predicting Relationships in Conversations](https://doi.org/10.18653/v1/2021.emnlp-main.380) |  | 0 | Automatically extracting interpersonal relationships of conversation interlocutors can enrich personal knowledge bases to enhance personalized search, recommenders and chatbots. To infer speakers’ relationships from dialogues we propose PRIDE, a neural multi-label classifier, based on BERT and... | Anna Tigunova, Paramita Mirza, Andrew Yates, Gerhard Weikum |  |
| 850 |  |  [Extracting Fine-Grained Knowledge Graphs of Scientific Claims: Dataset and Transformer-Based Results](https://doi.org/10.18653/v1/2021.emnlp-main.381) |  | 0 | Recent transformer-based approaches demonstrate promising results on relational scientific information extraction. Existing datasets focus on high-level description of how research is carried out. Instead we focus on the subtleties of how experimental associations are presented by building... | Ian H. Magnusson, Scott E. Friedman |  |
| 851 |  |  [Sequential Cross-Document Coreference Resolution](https://doi.org/10.18653/v1/2021.emnlp-main.382) |  | 0 | Relating entities and events in text is a key component of natural language understanding. Cross-document coreference resolution, in particular, is important for the growing interest in multi-document analysis tasks. In this work we propose a new model that extends the efficient sequential... | Emily Allaway, Shuai Wang, Miguel Ballesteros |  |
| 852 |  |  [Mixture-of-Partitions: Infusing Large Biomedical Knowledge Graphs into BERT](https://doi.org/10.18653/v1/2021.emnlp-main.383) |  | 0 | Infusing factual knowledge into pre-trained models is fundamental for many knowledge-intensive tasks. In this paper, we proposed Mixture-of-Partitions (MoP), an infusion approach that can handle a very large knowledge graph (KG) by partitioning it into smaller sub-graphs and infusing their specific... | Zaiqiao Meng, Fangyu Liu, Thomas Hikaru Clark, Ehsan Shareghi, Nigel Collier |  |
| 853 |  |  [Filling the Gaps in Ancient Akkadian Texts: A Masked Language Modelling Approach](https://doi.org/10.18653/v1/2021.emnlp-main.384) |  | 0 | We present models which complete missing text given transliterations of ancient Mesopotamian documents, originally written on cuneiform clay tablets (2500 BCE - 100 CE). Due to the tablets’ deterioration, scholars often rely on contextual cues to manually fill in missing parts in the text in a... | Koren Lazar, Benny Saret, Asaf Yehudai, Wayne Horowitz, Nathan Wasserman, Gabriel Stanovsky |  |
| 854 |  |  [AVocaDo: Strategy for Adapting Vocabulary to Downstream Domain](https://doi.org/10.18653/v1/2021.emnlp-main.385) |  | 0 | During the fine-tuning phase of transfer learning, the pretrained vocabulary remains unchanged, while model parameters are updated. The vocabulary generated based on the pretrained data is suboptimal for downstream data when domain discrepancy exists. We propose to consider the vocabulary as an... | Jimin Hong, Taehee Kim, Hyesu Lim, Jaegul Choo |  |
| 855 |  |  [Can We Improve Model Robustness through Secondary Attribute Counterfactuals?](https://doi.org/10.18653/v1/2021.emnlp-main.386) |  | 0 | Developing robust NLP models that perform well on many, even small, slices of data is a significant but important challenge, with implications from fairness to general reliability. To this end, recent research has explored how models rely on spurious correlations, and how counterfactual data... | Ananth Balashankar, Xuezhi Wang, Ben Packer, Nithum Thain, Ed H. Chi, Alex Beutel |  |
| 856 |  |  [Long-Range Modeling of Source Code Files with eWASH: Extended Window Access by Syntax Hierarchy](https://doi.org/10.18653/v1/2021.emnlp-main.387) |  | 0 | Statistical language modeling and translation with transformers have found many successful applications in program understanding and generation tasks, setting high benchmarks for tools in modern software development environments. The finite context window of these neural models means, however, that... | Colin B. Clement, Shuai Lu, Xiaoyu Liu, Michele Tufano, Dawn Drain, Nan Duan, Neel Sundaresan, Alexey Svyatkovskiy |  |
| 857 |  |  [Can Language Models be Biomedical Knowledge Bases?](https://doi.org/10.18653/v1/2021.emnlp-main.388) |  | 0 | Pre-trained language models (LMs) have become ubiquitous in solving various natural language processing (NLP) tasks. There has been increasing interest in what knowledge these LMs contain and how we can extract that knowledge, treating LMs as knowledge bases (KBs). While there has been much work on... | Mujeen Sung, Jinhyuk Lee, Sean S. Yi, Minji Jeon, Sungdong Kim, Jaewoo Kang |  |
| 858 |  |  [LayoutReader: Pre-training of Text and Layout for Reading Order Detection](https://doi.org/10.18653/v1/2021.emnlp-main.389) |  | 0 | Reading order detection is the cornerstone to understanding visually-rich documents (e.g., receipts and forms). Unfortunately, no existing work took advantage of advanced deep learning models because it is too laborious to annotate a large enough dataset. We observe that the reading order of WORD... | Zilong Wang, Yiheng Xu, Lei Cui, Jingbo Shang, Furu Wei |  |
| 859 |  |  [Region under Discussion for visual dialog](https://doi.org/10.18653/v1/2021.emnlp-main.390) |  | 0 | Visual Dialog is assumed to require the dialog history to generate correct responses during a dialog. However, it is not clear from previous work how dialog history is needed for visual dialog. In this paper we define what it means for a visual question to require dialog history and we release a... | Mauricio Mazuecos, Franco M. Luque, Jorge Sánchez, Hernán Maina, Thomas Vadora, Luciana Benotti |  |
| 860 |  |  [Learning grounded word meaning representations on similarity graphs](https://doi.org/10.18653/v1/2021.emnlp-main.391) |  | 0 | This paper introduces a novel approach to learn visually grounded meaning representations of words as low-dimensional node embeddings on an underlying graph hierarchy. The lower level of the hierarchy models modality-specific word representations, conditioned to another modality, through dedicated... | Mariella Dimiccoli, Herwig Wendt, Pau Batlle Franch |  |
| 861 |  |  [WhyAct: Identifying Action Reasons in Lifestyle Vlogs](https://doi.org/10.18653/v1/2021.emnlp-main.392) |  | 0 | We aim to automatically identify human action reasons in online videos. We focus on the widespread genre of lifestyle vlogs, in which people perform actions while verbally describing them. We introduce and make publicly available the WhyAct dataset, consisting of 1,077 visual actions manually... | Oana Ignat, Santiago Castro, Hanwen Miao, Weiji Li, Rada Mihalcea |  |
| 862 |  |  [Genre as Weak Supervision for Cross-lingual Dependency Parsing](https://doi.org/10.18653/v1/2021.emnlp-main.393) |  | 0 | Recent work has shown that monolingual masked language models learn to represent data-driven notions of language variation which can be used for domain-targeted training data selection. Dataset genre labels are already frequently available, yet remain largely unexplored in cross-lingual setups. We... | Max MüllerEberstein, Rob van der Goot, Barbara Plank |  |
| 863 |  |  [On the Relation between Syntactic Divergence and Zero-Shot Performance](https://doi.org/10.18653/v1/2021.emnlp-main.394) |  | 0 | We explore the link between the extent to which syntactic relations are preserved in translation and the ease of correctly constructing a parse tree in a zero-shot setting. While previous work suggests such a relation, it tends to focus on the macro level and not on the level of individual edges—a... | Ofir Arviv, Dmitry Nikolaev, Taelin Karidi, Omri Abend |  |
| 864 |  |  [Improved Latent Tree Induction with Distant Supervision via Span Constraints](https://doi.org/10.18653/v1/2021.emnlp-main.395) |  | 0 | For over thirty years, researchers have developed and analyzed methods for latent tree induction as an approach for unsupervised syntactic parsing. Nonetheless, modern systems still do not perform well enough compared to their supervised counterparts to have any practical use as structural... | Zhiyang Xu, Andrew Drozdov, JayYoon Lee, Tim O'Gorman, Subendhu Rongali, Dylan Finkbeiner, Shilpa Suresh, Mohit Iyyer, Andrew McCallum |  |
| 865 |  |  [Aligning Multidimensional Worldviews and Discovering Ideological Differences](https://doi.org/10.18653/v1/2021.emnlp-main.396) |  | 0 | The Internet is home to thousands of communities, each with their own unique worldview and associated ideological differences. With new communities constantly emerging and serving as ideological birthplaces, battlegrounds, and bunkers, it is critical to develop a framework for understanding... | Jeremiah Milbauer, Adarsh Mathew, James Evans |  |
| 866 |  |  [Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts](https://doi.org/10.18653/v1/2021.emnlp-main.397) |  | 0 | Dialogue models trained on human conversations inadvertently learn to generate toxic responses. In addition to producing explicitly offensive utterances, these models can also implicitly insult a group or individual by aligning themselves with an offensive statement. To better understand the... | Ashutosh Baheti, Maarten Sap, Alan Ritter, Mark O. Riedl |  |
| 867 |  |  [Multi-Modal Open-Domain Dialogue](https://doi.org/10.18653/v1/2021.emnlp-main.398) |  | 0 | Recent work in open-domain conversational agents has demonstrated that significant improvements in humanness and user preference can be achieved via massive scaling in both pre-training data and model size (Adiwardana et al., 2020; Roller et al., 2020). However, if we want to build agents with... | Kurt Shuster, Eric Michael Smith, Da Ju, Jason Weston |  |
| 868 |  |  [A Label-Aware BERT Attention Network for Zero-Shot Multi-Intent Detection in Spoken Language Understanding](https://doi.org/10.18653/v1/2021.emnlp-main.399) |  | 0 | With the early success of query-answer assistants such as Alexa and Siri, research attempts to expand system capabilities of handling service automation are now abundant. However, preliminary systems have quickly found the inadequacy in relying on simple classification techniques to effectively... | TingWei Wu, Ruolin Su, BiingHwang Juang |  |
| 869 |  |  [Zero-Shot Dialogue Disentanglement by Self-Supervised Entangled Response Selection](https://doi.org/10.18653/v1/2021.emnlp-main.400) |  | 0 | Dialogue disentanglement aims to group utterances in a long and multi-participant dialogue into threads. This is useful for discourse analysis and downstream applications such as dialogue response selection, where it can be the first step to construct a clean context/response set. Unfortunately,... | TaChung Chi, Alexander I. Rudnicky |  |
| 870 |  |  [SIMMC 2.0: A Task-oriented Dialog Dataset for Immersive Multimodal Conversations](https://doi.org/10.18653/v1/2021.emnlp-main.401) |  | 0 | Next generation task-oriented dialog systems need to understand conversational contexts with their perceived surroundings, to effectively help users in the real-world multimodal environment. Existing task-oriented dialog datasets aimed towards virtual assistance fall short and do not situate the... | Satwik Kottur, Seungwhan Moon, Alborz Geramifard, Babak Damavandi |  |
| 871 |  |  [RAST: Domain-Robust Dialogue Rewriting as Sequence Tagging](https://doi.org/10.18653/v1/2021.emnlp-main.402) |  | 0 | The task of dialogue rewriting aims to reconstruct the latest dialogue utterance by copying the missing content from the dialogue context. Until now, the existing models for this task suffer from the robustness issue, i.e., performances drop dramatically when testing on a different dataset. We... | Jie Hao, Linfeng Song, Liwei Wang, Kun Xu, Zhaopeng Tu, Dong Yu |  |
| 872 |  |  [MRF-Chat: Improving Dialogue with Markov Random Fields](https://doi.org/10.18653/v1/2021.emnlp-main.403) |  | 0 | Recent state-of-the-art approaches in open-domain dialogue include training end-to-end deep-learning models to learn various conversational features like emotional content of response, symbolic transitions of dialogue contexts in a knowledge graph and persona of the agent and the user, among... | Ishaan Grover, Matthew Huggins, Cynthia Breazeal, Hae Won Park |  |
| 873 |  |  [Dialogue State Tracking with a Language Model using Schema-Driven Prompting](https://doi.org/10.18653/v1/2021.emnlp-main.404) |  | 0 | Task-oriented conversational systems often use dialogue state tracking to represent the user’s intentions, which involves filling in values of pre-defined slots. Many approaches have been proposed, often using task-specific architectures with special-purpose classifiers. Recently, good results have... | ChiaHsuan Lee, Hao Cheng, Mari Ostendorf |  |
| 874 |  |  [Signed Coreference Resolution](https://doi.org/10.18653/v1/2021.emnlp-main.405) |  | 0 | Coreference resolution is key to many natural language processing tasks and yet has been relatively unexplored in Sign Language Processing. In signed languages, space is primarily used to establish reference. Solving coreference resolution for signed languages would not only enable higher-level... | Kayo Yin, Kenneth DeHaan, Malihe Alikhani |  |
| 875 |  |  [Consistent Accelerated Inference via Confident Adaptive Transformers](https://doi.org/10.18653/v1/2021.emnlp-main.406) |  | 0 | We develop a novel approach for confidently accelerating inference in the large and expensive multilayer Transformers that are now ubiquitous in natural language processing (NLP). Amortized or approximate computational methods increase efficiency, but can come with unpredictable performance costs.... | Tal Schuster, Adam Fisch, Tommi S. Jaakkola, Regina Barzilay |  |
| 876 |  |  [Improving and Simplifying Pattern Exploiting Training](https://doi.org/10.18653/v1/2021.emnlp-main.407) |  | 0 | Recently, pre-trained language models (LMs) have achieved strong performance when fine-tuned on difficult benchmarks like SuperGLUE. However, performance can suffer when there are very few labeled examples available for fine-tuning. Pattern Exploiting Training (PET) is a recent approach that... | Derek Tam, Rakesh R. Menon, Mohit Bansal, Shashank Srivastava, Colin Raffel |  |
| 877 |  |  [Unsupervised Data Augmentation with Naive Augmentation and without Unlabeled Data](https://doi.org/10.18653/v1/2021.emnlp-main.408) |  | 0 | Unsupervised Data Augmentation (UDA) is a semisupervised technique that applies a consistency loss to penalize differences between a model’s predictions on (a) observed (unlabeled) examples; and (b) corresponding ‘noised’ examples produced via data augmentation. While UDA has gained popularity for... | David Lowell, Brian E. Howard, Zachary C. Lipton, Byron C. Wallace |  |
| 878 |  |  [Pre-train or Annotate? Domain Adaptation with a Constrained Budget](https://doi.org/10.18653/v1/2021.emnlp-main.409) |  | 0 | Recent work has demonstrated that pre-training in-domain language models can boost performance when adapting to a new domain. However, the costs associated with pre-training raise an important question: given a fixed budget, what steps should an NLP practitioner take to maximize performance? In... | Fan Bai, Alan Ritter, Wei Xu |  |
| 879 |  |  [Lawyers are Dishonest? Quantifying Representational Harms in Commonsense Knowledge Resources](https://doi.org/10.18653/v1/2021.emnlp-main.410) |  | 0 | Warning: this paper contains content that may be offensive or upsetting. Commonsense knowledge bases (CSKB) are increasingly used for various natural language processing tasks. Since CSKBs are mostly human-generated and may reflect societal biases, it is important to ensure that such biases are not... | Ninareh Mehrabi, Pei Zhou, Fred Morstatter, Jay Pujara, Xiang Ren, Aram Galstyan |  |
| 880 |  |  [OSCaR: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings](https://doi.org/10.18653/v1/2021.emnlp-main.411) |  | 0 | Language representations are known to carry stereotypical biases and, as a result, lead to biased predictions in downstream tasks. While existing methods are effective at mitigating biases by linear projection, such methods are too aggressive: they not only remove bias, but also erase valuable... | Sunipa Dev, Tao Li, Jeff M. Phillips, Vivek Srikumar |  |
| 881 |  |  [Sentence-Permuted Paragraph Generation](https://doi.org/10.18653/v1/2021.emnlp-main.412) |  | 0 | Generating paragraphs of diverse contents is important in many applications. Existing generation models produce similar contents from homogenized contexts due to the fixed left-to-right sentence order. Our idea is permuting the sentence orders to improve the content diversity of multi-sentence... | Wenhao Yu, Chenguang Zhu, Tong Zhao, Zhichun Guo, Meng Jiang |  |
| 882 |  |  [Extract, Denoise and Enforce: Evaluating and Improving Concept Preservation for Text-to-Text Generation](https://doi.org/10.18653/v1/2021.emnlp-main.413) |  | 0 | Prior studies on text-to-text generation typically assume that the model could figure out what to attend to in the input and what to include in the output via seq2seq learning, with only the parallel training data and no additional guidance. However, it remains unclear whether current models can... | Yuning Mao, Wenchang Ma, Deren Lei, Jiawei Han, Xiang Ren |  |
| 883 |  |  [Paraphrase Generation: A Survey of the State of the Art](https://doi.org/10.18653/v1/2021.emnlp-main.414) |  | 0 | This paper focuses on paraphrase generation,which is a widely studied natural language generation task in NLP. With the development of neural models, paraphrase generation research has exhibited a gradual shift to neural methods in the recent years. This has provided architectures for... | Jianing Zhou, Suma Bhat |  |
| 884 |  |  [Exposure Bias versus Self-Recovery: Are Distortions Really Incremental for Autoregressive Text Generation?](https://doi.org/10.18653/v1/2021.emnlp-main.415) |  | 0 | Exposure bias has been regarded as a central problem for auto-regressive language models (LM). It claims that teacher forcing would cause the test-time generation to be incrementally distorted due to the training-generation discrepancy. Although a lot of algorithms have been proposed to avoid... | Tianxing He, Jingzhao Zhang, Zhiming Zhou, James R. Glass |  |
| 885 |  |  [Generating Self-Contained and Summary-Centric Question Answer Pairs via Differentiable Reward Imitation Learning](https://doi.org/10.18653/v1/2021.emnlp-main.416) |  | 0 | Motivated by suggested question generation in conversational news recommendation systems, we propose a model for generating question-answer pairs (QA pairs) with self-contained, summary-centric questions and length-constrained, article-summarizing answers. We begin by collecting a new dataset of... | Li Zhou, Kevin Small, Yong Zhang, Sandeep Atluri |  |
| 886 |  |  [Unsupervised Paraphrasing with Pretrained Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.417) |  | 0 | Paraphrase generation has benefited extensively from recent progress in the designing of training objectives and model architectures. However, previous explorations have largely focused on supervised methods, which require a large amount of labeled data that is costly to collect. To address this... | Tong Niu, Semih Yavuz, Yingbo Zhou, Nitish Shirish Keskar, Huan Wang, Caiming Xiong |  |
| 887 |  |  [Profanity-Avoiding Training Framework for Seq2seq Models with Certified Robustness](https://doi.org/10.18653/v1/2021.emnlp-main.418) |  | 0 | Seq2seq models have demonstrated their incredible effectiveness in a large variety of applications. However, recent research has shown that inappropriate language in training samples and well-designed testing cases can induce seq2seq models to output profanity. These outputs may potentially hurt... | Hengtong Zhang, Tianhang Zheng, Yaliang Li, Jing Gao, Lu Su, Bo Li |  |
| 888 |  |  [Journalistic Guidelines Aware News Image Captioning](https://doi.org/10.18653/v1/2021.emnlp-main.419) |  | 0 | The task of news article image captioning aims to generate descriptive and informative captions for news article images. Unlike conventional image captions that simply describe the content of the image in general terms, news image captions follow journalistic guidelines and rely heavily on named... | Xuewen Yang, Svebor Karaman, Joel R. Tetreault, Alejandro Jaimes |  |
| 889 |  |  [AESOP: Paraphrase Generation with Adaptive Syntactic Control](https://doi.org/10.18653/v1/2021.emnlp-main.420) |  | 0 | We propose to control paraphrase generation through carefully chosen target syntactic structures to generate more proper and higher quality paraphrases. Our model, AESOP, leverages a pretrained language model and adds deliberately chosen syntactical control via a retrieval-based selection module to... | Jiao Sun, Xuezhe Ma, Nanyun Peng |  |
| 890 |  |  [Refocusing on Relevance: Personalization in NLG](https://doi.org/10.18653/v1/2021.emnlp-main.421) |  | 0 | Many NLG tasks such as summarization, dialogue response, or open domain question answering, focus primarily on a source text in order to generate a target response. This standard approach falls short, however, when a user’s intent or context of work is not easily recoverable based solely on that... | Shiran Dudy, Steven Bedrick, Bonnie Webber |  |
| 891 |  |  [The Future is not One-dimensional: Complex Event Schema Induction by Graph Modeling for Event Prediction](https://doi.org/10.18653/v1/2021.emnlp-main.422) |  | 0 | Event schemas encode knowledge of stereotypical structures of events and their connections. As events unfold, schemas are crucial to act as a scaffolding. Previous work on event schema induction focuses either on atomic events or linear temporal event sequences, ignoring the interplay between... | Manling Li, Sha Li, Zhenhailong Wang, Lifu Huang, Kyunghyun Cho, Heng Ji, Jiawei Han, Clare R. Voss |  |
| 892 |  |  [Learning Constraints and Descriptive Segmentation for Subevent Detection](https://doi.org/10.18653/v1/2021.emnlp-main.423) |  | 0 | Event mentions in text correspond to real-world events of varying degrees of granularity. The task of subevent detection aims to resolve this granularity issue, recognizing the membership of multi-granular events in event complexes. Since knowing the span of descriptive contexts of event complexes... | Haoyu Wang, Hongming Zhang, Muhao Chen, Dan Roth |  |
| 893 |  |  [ChemNER: Fine-Grained Chemistry Named Entity Recognition with Ontology-Guided Distant Supervision](https://doi.org/10.18653/v1/2021.emnlp-main.424) |  | 0 | Scientific literature analysis needs fine-grained named entity recognition (NER) to provide a wide range of information for scientific discovery. For example, chemistry research needs to study dozens to hundreds of distinct, fine-grained entity types, making consistent and accurate annotation... | Xuan Wang, Vivian Hu, Xiangchen Song, Shweta Garg, Jinfeng Xiao, Jiawei Han |  |
| 894 |  |  [Moving on from OntoNotes: Coreference Resolution Model Transfer](https://doi.org/10.18653/v1/2021.emnlp-main.425) |  | 0 | Academic neural models for coreference resolution (coref) are typically trained on a single dataset, OntoNotes, and model improvements are benchmarked on that same dataset. However, real-world applications of coref depend on the annotation guidelines and the domain of the target dataset, which... | Patrick Xia, Benjamin Van Durme |  |
| 895 |  |  [Document-level Entity-based Extraction as Template Generation](https://doi.org/10.18653/v1/2021.emnlp-main.426) |  | 0 | Document-level entity-based extraction (EE), aiming at extracting entity-centric information such as entity roles and entity relations, is key to automatic knowledge acquisition from text corpora for various domains. Most document-level EE systems build extractive models, which struggle to model... | KungHsiang Huang, Sam Tang, Nanyun Peng |  |
| 896 |  |  [Learning Prototype Representations Across Few-Shot Tasks for Event Detection](https://doi.org/10.18653/v1/2021.emnlp-main.427) |  | 0 | We address the sampling bias and outlier issues in few-shot learning for event detection, a subtask of information extraction. We propose to model the relations between training tasks in episodic few-shot learning by introducing cross-task prototypes. We further propose to enforce prediction... | Viet Dac Lai, Franck Dernoncourt, Thien Huu Nguyen |  |
| 897 |  |  [Lifelong Event Detection with Knowledge Transfer](https://doi.org/10.18653/v1/2021.emnlp-main.428) |  | 0 | Traditional supervised Information Extraction (IE) methods can extract structured knowledge elements from unstructured data, but it is limited to a pre-defined target ontology. In reality, the ontology of interest may change over time, adding emergent new types or more fine-grained subtypes. We... | Pengfei Yu, Heng Ji, Prem Natarajan |  |
| 898 |  |  [Modular Self-Supervision for Document-Level Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.429) |  | 0 | Extracting relations across large text spans has been relatively underexplored in NLP, but it is particularly important for high-value domains such as biomedicine, where obtaining high recall of the latest findings is crucial for practical applications. Compared to conventional information... | Sheng Zhang, Cliff Wong, Naoto Usuyama, Sarthak Jain, Tristan Naumann, Hoifung Poon |  |
| 899 |  |  [Unsupervised Paraphrasing Consistency Training for Low Resource Named Entity Recognition](https://doi.org/10.18653/v1/2021.emnlp-main.430) |  | 0 | Unsupervised consistency training is a way of semi-supervised learning that encourages consistency in model predictions between the original and augmented data. For Named Entity Recognition (NER), existing approaches augment the input sequence with token replacement, assuming annotations on the... | Rui Wang, Ricardo Henao |  |
| 900 |  |  [Fine-grained Entity Typing without Knowledge Base](https://doi.org/10.18653/v1/2021.emnlp-main.431) |  | 0 | Existing work on Fine-grained Entity Typing (FET) typically trains automatic models on the datasets obtained by using Knowledge Bases (KB) as distant supervision. However, the reliance on KB means this training setting can be hampered by the lack of or the incompleteness of the KB. To alleviate... | Jing Qian, Yibin Liu, Lemao Liu, Yangming Li, Haiyun Jiang, Haisong Zhang, Shuming Shi |  |
| 901 |  |  [Adversarial Attack against Cross-lingual Knowledge Graph Alignment](https://doi.org/10.18653/v1/2021.emnlp-main.432) |  | 0 | Recent literatures have shown that knowledge graph (KG) learning models are highly vulnerable to adversarial attacks. However, there is still a paucity of vulnerability analyses of cross-lingual entity alignment under adversarial attacks. This paper proposes an adversarial attack model with two... | Zeru Zhang, Zijie Zhang, Yang Zhou, Lingfei Wu, Sixing Wu, Xiaoying Han, Dejing Dou, Tianshi Che, Da Yan |  |
| 902 |  |  [Towards Realistic Few-Shot Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.433) |  | 0 | In recent years, few-shot models have been applied successfully to a variety of NLP tasks. Han et al. (2018) introduced a few-shot learning framework for relation classification, and since then, several models have surpassed human performance on this task, leading to the impression that few-shot... | Sam Brody, Sichao Wu, Adrian Benton |  |
| 903 |  |  [Data Augmentation for Cross-Domain Named Entity Recognition](https://doi.org/10.18653/v1/2021.emnlp-main.434) |  | 0 | Current work in named entity recognition (NER) shows that data augmentation techniques can produce more robust models. However, most existing techniques focus on augmenting in-domain data in low-resource scenarios where annotated data is quite limited. In this work, we take this research direction... | Shuguang Chen, Gustavo Aguilar, Leonardo Neves, Thamar Solorio |  |
| 904 |  |  [Incorporating medical knowledge in BERT for clinical relation extraction](https://doi.org/10.18653/v1/2021.emnlp-main.435) |  | 0 | In recent years pre-trained language models (PLM) such as BERT have proven to be very effective in diverse NLP tasks such as Information Extraction, Sentiment Analysis and Question Answering. Trained with massive general-domain text, these pre-trained language models capture rich syntactic,... | Arpita Roy, Shimei Pan |  |
| 905 |  |  [ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning](https://doi.org/10.18653/v1/2021.emnlp-main.436) |  | 0 | While pre-trained language models (PTLMs) have achieved noticeable success on many NLP tasks, they still struggle for tasks that require event temporal reasoning, which is essential for event-centric applications. We present a continual pre-training approach that equips PTLMs with targeted... | Rujun Han, Xiang Ren, Nanyun Peng |  |
| 906 |  |  [Learning from Noisy Labels for Entity-Centric Information Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.437) |  | 0 | Recent information extraction approaches have relied on training deep neural models. However, such models can easily overfit noisy labels and suffer from performance degradation. While it is very costly to filter noisy labels in large learning resources, recent studies show that such labels take... | Wenxuan Zhou, Muhao Chen |  |
| 907 |  |  [Extracting Material Property Measurement Data from Scientific Articles](https://doi.org/10.18653/v1/2021.emnlp-main.438) |  | 0 | Machine learning-based prediction of material properties is often hampered by the lack of sufficiently large training data sets. The majority of such measurement data is embedded in scientific literature and the ability to automatically extract these data is essential to support the development of... | Gihan Panapitiya, Fred Parks, Jonathan Sepulveda, Emily Saldanha |  |
| 908 |  |  [Modeling Document-Level Context for Event Detection via Important Context Selection](https://doi.org/10.18653/v1/2021.emnlp-main.439) |  | 0 | The task of Event Detection (ED) in Information Extraction aims to recognize and classify trigger words of events in text. The recent progress has featured advanced transformer-based language models (e.g., BERT) as a critical component in state-of-the-art models for ED. However, the length limit... | Amir Pouran Ben Veyseh, Minh Van Nguyen, Nghia Trung Ngo, Bonan Min, Thien Huu Nguyen |  |
| 909 |  |  [Crosslingual Transfer Learning for Relation and Event Extraction via Word Category and Class Alignments](https://doi.org/10.18653/v1/2021.emnlp-main.440) |  | 0 | Previous work on crosslingual Relation and Event Extraction (REE) suffers from the monolingual bias issue due to the training of models on only the source language data. An approach to overcome this issue is to use unlabeled data in the target language to aid the alignment of crosslingual... | Minh Van Nguyen, Tuan Ngo Nguyen, Bonan Min, Thien Huu Nguyen |  |
| 910 |  |  [Corpus-based Open-Domain Event Type Induction](https://doi.org/10.18653/v1/2021.emnlp-main.441) |  | 0 | Traditional event extraction methods require predefined event types and their corresponding annotations to learn event extractors. These prerequisites are often hard to be satisfied in real-world applications. This work presents a corpus-based open-domain event type induction method that... | Jiaming Shen, Yunyi Zhang, Heng Ji, Jiawei Han |  |
| 911 |  |  [PDALN: Progressive Domain Adaptation over a Pre-trained Model for Low-Resource Cross-Domain Named Entity Recognition](https://doi.org/10.18653/v1/2021.emnlp-main.442) |  | 0 | Cross-domain Named Entity Recognition (NER) transfers the NER knowledge from high-resource domains to the low-resource target domain. Due to limited labeled resources and domain shift, cross-domain NER is a challenging task. To address these challenges, we propose a progressive domain adaptation... | Tao Zhang, Congying Xia, Philip S. Yu, Zhiwei Liu, Shu Zhao |  |
| 912 |  |  [Multi-Vector Attention Models for Deep Re-ranking](https://doi.org/10.18653/v1/2021.emnlp-main.443) |  | 0 | Large-scale document retrieval systems often utilize two styles of neural network models which live at two different ends of the joint computation vs. accuracy spectrum. The first style is dual encoder (or two-tower) models, where the query and document representations are computed completely... | Giulio Zhou, Jacob Devlin |  |
| 913 |  |  [Toward Deconfounding the Effect of Entity Demographics for Question Answering Accuracy](https://doi.org/10.18653/v1/2021.emnlp-main.444) |  | 0 | The goal of question answering (QA) is to answer _any_ question. However, major QA datasets have skewed distributions over gender, profession, and nationality. Despite that skew, an analysis of model accuracy reveals little evidence that accuracy is lower for people based on gender or nationality;... | Maharshi Gor, Kellie Webster, Jordan L. BoydGraber |  |
| 914 |  |  [Exploring Strategies for Generalizable Commonsense Reasoning with Pre-trained Models](https://doi.org/10.18653/v1/2021.emnlp-main.445) |  | 0 | Commonsense reasoning benchmarks have been largely solved by fine-tuning language models. The downside is that fine-tuning may cause models to overfit to task-specific data and thereby forget their knowledge gained during pre-training. Recent works only propose lightweight model updates as models... | Kaixin Ma, Filip Ilievski, Jonathan Francis, Satoru Ozaki, Eric Nyberg, Alessandro Oltramari |  |
| 915 |  |  [Transformer Feed-Forward Layers Are Key-Value Memories](https://doi.org/10.18653/v1/2021.emnlp-main.446) |  | 0 | Feed-forward layers constitute two-thirds of a transformer model’s parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training... | Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy |  |
| 916 |  |  [Connecting Attributions and QA Model Behavior on Realistic Counterfactuals](https://doi.org/10.18653/v1/2021.emnlp-main.447) |  | 0 | When a model attribution technique highlights a particular part of the input, a user might understand this highlight as making a statement about counterfactuals (Miller, 2019): if that part of the input were to change, the model’s prediction might change as well. This paper investigates how well... | Xi Ye, Rohan Nair, Greg Durrett |  |
| 917 |  |  [How Do Neural Sequence Models Generalize? Local and Global Cues for Out-of-Distribution Prediction](https://doi.org/10.18653/v1/2021.emnlp-main.448) |  | 0 | After a neural sequence model encounters an unexpected token, can its behavior be predicted? We show that RNN and transformer language models exhibit structured, consistent generalization in out-of-distribution contexts. We begin by introducing two idealized models of generalization in next-word... | D. Anthony Bau, Jacob Andreas |  |
| 918 |  |  [Comparing Text Representations: A Theory-Driven Approach](https://doi.org/10.18653/v1/2021.emnlp-main.449) |  | 0 | Much of the progress in contemporary NLP has come from learning representations, such as masked language model (MLM) contextual embeddings, that turn challenging problems into simple classification tasks. But how do we quantify and explain this effect? We adapt general tools from computational... | Gregory Yauney, David Mimno |  |
| 919 |  |  [Human Rationales as Attribution Priors for Explainable Stance Detection](https://doi.org/10.18653/v1/2021.emnlp-main.450) |  | 0 | As NLP systems become better at detecting opinions and beliefs from text, it is important to ensure not only that models are accurate but also that they arrive at their predictions in ways that align with human reasoning. In this work, we present a method for imparting human-like rationalization to... | Sahil Jayaram, Emily Allaway |  |
| 920 |  |  [The Stem Cell Hypothesis: Dilemma behind Multi-Task Learning with Transformer Encoders](https://doi.org/10.18653/v1/2021.emnlp-main.451) |  | 0 | Multi-task learning with transformer encoders (MTL) has emerged as a powerful technique to improve performance on closely-related tasks for both accuracy and efficiency while a question still remains whether or not it would perform as well on tasks that are distinct in nature. We first present MTL... | Han He, Jinho D. Choi |  |
| 921 |  |  [Text Counterfactuals via Latent Optimization and Shapley-Guided Search](https://doi.org/10.18653/v1/2021.emnlp-main.452) |  | 0 | We study the problem of generating counterfactual text for a classifier as a means for understanding and debugging classification. Given a textual input and a classification model, we aim to minimally alter the text to change the model’s prediction. White-box approaches have been successfully... | Xiaoli Z. Fern, Quintin Pope |  |
| 922 |  |  ["Average" Approximates "First Principal Component"? An Empirical Analysis on Representations from Neural Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.453) |  | 0 | Contextualized representations based on neural language models have furthered the state of the art in various NLP tasks. Despite its great success, the nature of such representations remains a mystery. In this paper, we present an empirical property of these representations—”average” approximates... | Zihan Wang, Chengyu Dong, Jingbo Shang |  |
| 923 |  |  [Controlled Evaluation of Grammatical Knowledge in Mandarin Chinese Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.454) |  | 0 | Prior work has shown that structural supervision helps English language models learn generalizations about syntactic phenomena such as subject-verb agreement. However, it remains unclear if such an inductive bias would also improve language models’ ability to learn grammatical dependencies in... | Yiwen Wang, Jennifer Hu, Roger Levy, Peng Qian |  |
| 924 |  |  [GradTS: A Gradient-Based Automatic Auxiliary Task Selection Method Based on Transformer Networks](https://doi.org/10.18653/v1/2021.emnlp-main.455) |  | 0 | A key problem in multi-task learning (MTL) research is how to select high-quality auxiliary tasks automatically. This paper presents GradTS, an automatic auxiliary task selection method based on gradient calculation in Transformer-based models. Compared to AUTOSEM, a strong baseline method, GradTS... | Weicheng Ma, Renze Lou, Kai Zhang, Lili Wang, Soroush Vosoughi |  |
| 925 |  |  [NegatER: Unsupervised Discovery of Negatives in Commonsense Knowledge Bases](https://doi.org/10.18653/v1/2021.emnlp-main.456) |  | 0 | Codifying commonsense knowledge in machines is a longstanding goal of artificial intelligence. Recently, much progress toward this goal has been made with automatic knowledge base (KB) construction techniques. However, such techniques focus primarily on the acquisition of positive (true) KB... | Tara Safavi, Jing Zhu, Danai Koutra |  |
| 926 |  |  [Instance-adaptive training with noise-robust losses against noisy labels](https://doi.org/10.18653/v1/2021.emnlp-main.457) |  | 0 | In order to alleviate the huge demand for annotated datasets for different tasks, many recent natural language processing datasets have adopted automated pipelines for fast-tracking usable data. However, model training with such datasets poses a challenge because popular optimization objectives are... | Lifeng Jin, Linfeng Song, Kun Xu, Dong Yu |  |
| 927 |  |  [Distributionally Robust Multilingual Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.458) |  | 0 | Multilingual neural machine translation (MNMT) learns to translate multiple language pairs with a single model, potentially improving both the accuracy and the memory-efficiency of deployed models. However, the heavy data imbalance between languages hinders the model from performing uniformly... | Chunting Zhou, Daniel Levy, Xian Li, Marjan Ghazvininejad, Graham Neubig |  |
| 928 |  |  [Model Selection for Cross-lingual Transfer](https://doi.org/10.18653/v1/2021.emnlp-main.459) |  | 0 | Transformers that are pre-trained on multilingual corpora, such as, mBERT and XLM-RoBERTa, have achieved impressive cross-lingual transfer capabilities. In the zero-shot transfer setting, only English training data is used, and the fine-tuned model is evaluated on another target language. While... | Yang Chen, Alan Ritter |  |
| 929 |  |  [Continual Few-Shot Learning for Text Classification](https://doi.org/10.18653/v1/2021.emnlp-main.460) |  | 0 | Natural Language Processing (NLP) is increasingly relying on general end-to-end systems that need to handle many different linguistic phenomena and nuances. For example, a Natural Language Inference (NLI) system has to recognize sentiment, handle numbers, perform coreference, etc. Our solutions to... | Ramakanth Pasunuru, Veselin Stoyanov, Mohit Bansal |  |
| 930 |  |  [Efficient Nearest Neighbor Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.461) |  | 0 | Non-parametric neural language models (NLMs) learn predictive distributions of text utilizing an external datastore, which allows them to learn through explicitly memorizing the training datapoints. While effective, these models often require retrieval from a large datastore at test time,... | Junxian He, Graham Neubig, Taylor BergKirkpatrick |  |
| 931 |  |  [STraTA: Self-Training with Task Augmentation for Better Few-shot Learning](https://doi.org/10.18653/v1/2021.emnlp-main.462) |  | 0 | Despite their recent successes in tackling many NLP tasks, large-scale pre-trained language models do not perform as well in few-shot settings where only a handful of training examples are available. To address this shortcoming, we propose STraTA, which stands for Self-Training with Task... | Tu Vu, MinhThang Luong, Quoc V. Le, Grady Simon, Mohit Iyyer |  |
| 932 |  |  [TADPOLE: Task ADapted Pre-Training via AnOmaLy DEtection](https://doi.org/10.18653/v1/2021.emnlp-main.463) |  | 0 | The paradigm of pre-training followed by finetuning has become a standard procedure for NLP tasks, with a known problem of domain shift between the pre-training and downstream corpus. Previous works have tried to mitigate this problem with additional pre-training, either on the downstream corpus... | Vivek Madan, Ashish Khetan, Zohar Karnin |  |
| 933 |  |  [Gradient-based Adversarial Attacks against Text Transformers](https://doi.org/10.18653/v1/2021.emnlp-main.464) |  | 0 | We propose the first general-purpose gradient-based adversarial attack against transformer models. Instead of searching for a single adversarial example, we search for a distribution of adversarial examples parameterized by a continuous-valued matrix, hence enabling gradient-based optimization. We... | Chuan Guo, Alexandre Sablayrolles, Hervé Jégou, Douwe Kiela |  |
| 934 |  |  [Do Transformer Modifications Transfer Across Implementations and Applications?](https://doi.org/10.18653/v1/2021.emnlp-main.465) |  | 0 | The research community has proposed copious modifications to the Transformer architecture since it was introduced over three years ago, relatively few of which have seen widespread adoption. In this paper, we comprehensively evaluate many of these modifications in a shared experimental setting that... | Sharan Narang, Hyung Won Chung, Yi Tay, Liam Fedus, Thibault Févry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi Zhou, Wei Li, Nan Ding, Jake Marcus, Adam Roberts, Colin Raffel |  |
| 935 |  |  [Paired Examples as Indirect Supervision in Latent Decision Models](https://doi.org/10.18653/v1/2021.emnlp-main.466) |  | 0 | Compositional, structured models are appealing because they explicitly decompose problems and provide interpretable intermediate outputs that give confidence that the model is not simply latching onto data artifacts. Learning these models is challenging, however, because end-task supervision only... | Nitish Gupta, Sameer Singh, Matt Gardner, Dan Roth |  |
| 936 |  |  [Pairwise Supervised Contrastive Learning of Sentence Representations](https://doi.org/10.18653/v1/2021.emnlp-main.467) |  | 0 | Many recent successes in sentence representation learning have been achieved by simply fine-tuning on the Natural Language Inference (NLI) datasets with triplet loss or siamese loss. Nevertheless, they share a common weakness: sentences in a contradiction pair are not necessarily from different... | Dejiao Zhang, ShangWen Li, Wei Xiao, Henghui Zhu, Ramesh Nallapati, Andrew O. Arnold, Bing Xiang |  |
| 937 |  |  [Muppet: Massive Multi-task Representations with Pre-Finetuning](https://doi.org/10.18653/v1/2021.emnlp-main.468) |  | 0 | We propose pre-finetuning, an additional large-scale learning stage between language model pre-training and fine-tuning. Pre-finetuning is massively multi-task learning (around 50 datasets, over 4.8 million total labeled examples), and is designed to encourage learning of representations that... | Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, Sonal Gupta |  |
| 938 |  |  [Diverse Distributions of Self-Supervised Tasks for Meta-Learning in NLP](https://doi.org/10.18653/v1/2021.emnlp-main.469) |  | 0 | Meta-learning considers the problem of learning an efficient learning process that can leverage its past experience to accurately solve new tasks. However, the efficacy of meta-learning crucially depends on the distribution of tasks available for training, and this is often assumed to be known a... | Trapit Bansal, Karthick Prasad Gunasekaran, Tong Wang, Tsendsuren Munkhdalai, Andrew McCallum |  |
| 939 |  |  [A Simple and Effective Method To Eliminate the Self Language Bias in Multilingual Representations](https://doi.org/10.18653/v1/2021.emnlp-main.470) |  | 0 | Language agnostic and semantic-language information isolation is an emerging research direction for multilingual representations models. We explore this problem from a novel angle of geometric algebra and semantic space. A simple but highly effective method “Language Information Removal (LIR)”... | Ziyi Yang, Yinfei Yang, Daniel Cer, Eric Darve |  |
| 940 |  |  [A Massively Multilingual Analysis of Cross-linguality in Shared Embedding Space](https://doi.org/10.18653/v1/2021.emnlp-main.471) |  | 0 | In cross-lingual language models, representations for many different languages live in the same space. Here, we investigate the linguistic and non-linguistic factors affecting sentence-level alignment in cross-lingual pretrained language models for 101 languages and 5,050 language pairs. Using... | Alexander Jones, William Yang Wang, Kyle Mahowald |  |
| 941 |  |  [Frustratingly Simple but Surprisingly Strong: Using Language-Independent Features for Zero-shot Cross-lingual Semantic Parsing](https://doi.org/10.18653/v1/2021.emnlp-main.472) |  | 0 | The availability of corpora has led to significant advances in training semantic parsers in English. Unfortunately, for languages other than English, annotated data is limited and so is the performance of the developed parsers. Recently, pretrained multilingual models have been proven useful for... | Jingfeng Yang, Federico Fancellu, Bonnie Webber, Diyi Yang |  |
| 942 |  |  [Improving Simultaneous Translation by Incorporating Pseudo-References with Fewer Reorderings](https://doi.org/10.18653/v1/2021.emnlp-main.473) |  | 0 | Simultaneous translation is vastly different from full-sentence translation, in the sense that it starts translation before the source sentence ends, with only a few words delay. However, due to the lack of large-scale, high-quality simultaneous translation datasets, most such systems are still... | JunKun Chen, Renjie Zheng, Atsuhito Kita, Mingbo Ma, Liang Huang |  |
| 943 |  |  [Classification-based Quality Estimation: Small and Efficient Models for Real-world Applications](https://doi.org/10.18653/v1/2021.emnlp-main.474) |  | 0 | Sentence-level Quality estimation (QE) of machine translation is traditionally formulated as a regression task, and the performance of QE models is typically measured by Pearson correlation with human labels. Recent QE models have achieved previously-unseen levels of correlation with human... | Shuo Sun, Ahmed ElKishky, Vishrav Chaudhary, James Cross, Lucia Specia, Francisco Guzmán |  |
| 944 |  |  [A Large-Scale Study of Machine Translation in Turkic Languages](https://doi.org/10.18653/v1/2021.emnlp-main.475) |  | 0 | Recent advances in neural machine translation (NMT) have pushed the quality of machine translation systems to the point where they are becoming widely adopted to build competitive systems. However, there is still a large number of languages that are yet to reap the benefits of NMT. In this paper,... | Jamshidbek Mirzakhalov, Anoop Babu, Duygu Ataman, Sherzod Kariev, Francis M. Tyers, Otabek Abduraufov, Mammad Hajili, Sardana Ivanova, Abror Khaytbaev, Antonio Laverghetta Jr., Behzodbek Moydinboyev, Esra Onal, Shaxnoza Pulatova, Ahsan Wahab, Orhan Firat, Sriram Chellappan |  |
| 945 |  |  [Analyzing the Surprising Variability in Word Embedding Stability Across Languages](https://doi.org/10.18653/v1/2021.emnlp-main.476) |  | 0 | Word embeddings are powerful representations that form the foundation of many natural language processing architectures, both in English and in other languages. To gain further insight into word embeddings, we explore their stability (e.g., overlap between the nearest neighbors of a word in... | Laura Burdick, Jonathan K. Kummerfeld, Rada Mihalcea |  |
| 946 |  |  [Rule-based Morphological Inflection Improves Neural Terminology Translation](https://doi.org/10.18653/v1/2021.emnlp-main.477) |  | 0 | Current approaches to incorporating terminology constraints in machine translation (MT) typically assume that the constraint terms are provided in their correct morphological forms. This limits their application to real-world scenarios where constraint terms are provided as lemmas. In this paper,... | Weijia Xu, Marine Carpuat |  |
| 947 |  |  [Data and Parameter Scaling Laws for Neural Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.478) |  | 0 | We observe that the development cross-entropy loss of supervised neural machine translation models scales like a power law with the amount of training data and the number of non-embedding parameters in the model. We discuss some practical implications of these results, such as predicting BLEU... | Mitchell A. Gordon, Kevin Duh, Jared Kaplan |  |
| 948 |  |  [Good-Enough Example Extrapolation](https://doi.org/10.18653/v1/2021.emnlp-main.479) |  | 0 | This paper asks whether extrapolating the hidden space distribution of text examples from one class onto another is a valid inductive bias for data augmentation. To operationalize this question, I propose a simple data augmentation protocol called “good-enough example extrapolation” (GE3). GE3 is... | Jason Wei |  |
| 949 |  |  [Learning to Selectively Learn for Weakly-supervised Paraphrase Generation](https://doi.org/10.18653/v1/2021.emnlp-main.480) |  | 0 | Paraphrase generation is a longstanding NLP task that has diverse applications on downstream NLP tasks. However, the effectiveness of existing efforts predominantly relies on large amounts of golden labeled data. Though unsupervised endeavors have been proposed to alleviate this issue, they may... | Kaize Ding, Dingcheng Li, Alexander Hanbo Li, Xing Fan, Chenlei Guo, Yang Liu, Huan Liu |  |
| 950 |  |  [Effective Convolutional Attention Network for Multi-label Clinical Document Classification](https://doi.org/10.18653/v1/2021.emnlp-main.481) |  | 0 | Multi-label document classification (MLDC) problems can be challenging, especially for long documents with a large label set and a long-tail distribution over labels. In this paper, we present an effective convolutional attention network for the MLDC problem with a focus on medical code prediction... | Yang Liu, Hua Cheng, Russell Klopfer, Matthew R. Gormley, Thomas Schaaf |  |
| 951 |  |  [Contrastive Code Representation Learning](https://doi.org/10.18653/v1/2021.emnlp-main.482) |  | 0 | Recent work learns contextual representations of source code by reconstructing tokens from their context. For downstream semantic understanding tasks like code clone detection, these representations should ideally capture program functionality. However, we show that the popular reconstruction-based... | Paras Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel, Joseph Gonzalez, Ion Stoica |  |
| 952 |  |  [IGA: An Intent-Guided Authoring Assistant](https://doi.org/10.18653/v1/2021.emnlp-main.483) |  | 0 | While large-scale pretrained language models have significantly improved writing assistance functionalities such as autocomplete, more complex and controllable writing assistants have yet to be explored. We leverage advances in language modeling to build an interactive writing assistant that... | Simeng Sun, Wenlong Zhao, Varun Manjunatha, Rajiv Jain, Vlad I. Morariu, Franck Dernoncourt, Balaji Vasan Srinivasan, Mohit Iyyer |  |
| 953 |  |  [Math Word Problem Generation with Mathematical Consistency and Problem Context Constraints](https://doi.org/10.18653/v1/2021.emnlp-main.484) |  | 0 | We study the problem of generating arithmetic math word problems (MWPs) given a math equation that specifies the mathematical computation and a context that specifies the problem scenario. Existing approaches are prone to generating MWPs that are either mathematically invalid or have unsatisfactory... | Zichao Wang, Andrew S. Lan, Richard G. Baraniuk |  |
| 954 |  |  [Navigating the Kaleidoscope of COVID-19 Misinformation Using Deep Learning](https://doi.org/10.18653/v1/2021.emnlp-main.485) |  | 0 | Irrespective of the success of the deep learning-based mixed-domain transfer learning approach for solving various Natural Language Processing tasks, it does not lend a generalizable solution for detecting misinformation from COVID-19 social media data. Due to the inherent complexity of this type... | Yuanzhi Chen, Mohammad Rashedul Hasan |  |
| 955 |  |  [Detecting Health Advice in Medical Research Literature](https://doi.org/10.18653/v1/2021.emnlp-main.486) |  | 0 | Health and medical researchers often give clinical and policy recommendations to inform health practice and public health policy. However, no current health information system supports the direct retrieval of health advice. This study fills the gap by developing and validating an NLP-based... | Yingya Li, Jun Wang, Bei Yu |  |
| 956 |  |  [A Semantic Feature-Wise Transformation Relation Network for Automatic Short Answer Grading](https://doi.org/10.18653/v1/2021.emnlp-main.487) |  | 0 | Automatic short answer grading (ASAG) is the task of assessing students’ short natural language responses to objective questions. It is a crucial component of new education platforms, and could support more wide-spread use of constructed response questions to replace cognitively less challenging... | Zhaohui Li, Yajur Tomar, Rebecca J. Passonneau |  |
| 957 |  |  [Evaluating Scholarly Impact: Towards Content-Aware Bibliometrics](https://doi.org/10.18653/v1/2021.emnlp-main.488) |  | 0 | Quantitatively measuring the impact-related aspects of scientific, engineering, and technological (SET) innovations is a fundamental problem with broad applications. Traditional citation-based measures for assessing the impact of innovations and related entities do not take into account the content... | Saurav Manchanda, George Karypis |  |
| 958 |  |  [A Scalable Framework for Learning From Implicit User Feedback to Improve Natural Language Understanding in Large-Scale Conversational AI Systems](https://doi.org/10.18653/v1/2021.emnlp-main.489) |  | 0 | Natural Language Understanding (NLU) is an established component within a conversational AI or digital assistant system, and it is responsible for producing semantic understanding of a user request. We propose a scalable and automatic approach for improving NLU in a large-scale conversational AI... | Sunghyun Park, Han Li, Ameen Patel, Sidharth Mudgal, Sungjin Lee, YoungBum Kim, Spyros Matsoukas, Ruhi Sarikaya |  |
| 959 |  |  [Summarize-then-Answer: Generating Concise Explanations for Multi-hop Reading Comprehension](https://doi.org/10.18653/v1/2021.emnlp-main.490) |  | 0 | How can we generate concise explanations for multi-hop Reading Comprehension (RC)? The current strategies of identifying supporting sentences can be seen as an extractive question-focused summarization of the input text. However, these extractive explanations are not necessarily concise i.e. not... | Naoya Inoue, Harsh Trivedi, Steven Sinha, Niranjan Balasubramanian, Kentaro Inui |  |
| 960 |  |  [FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models](https://doi.org/10.18653/v1/2021.emnlp-main.491) |  | 0 | The task of learning from only a few examples (called a few-shot setting) is of key importance and relevance to a real-world setting. For question answering (QA), the current state-of-the-art pre-trained models typically need fine-tuning on tens of thousands of examples to obtain good results.... | Rakesh Chada, Pradeep Natarajan |  |
| 961 |  |  [Multi-stage Training with Improved Negative Contrast for Neural Passage Retrieval](https://doi.org/10.18653/v1/2021.emnlp-main.492) |  | 0 | In the context of neural passage retrieval, we study three promising techniques: synthetic data generation, negative sampling, and fusion. We systematically investigate how these techniques contribute to the performance of the retrieval system and how they complement each other. We propose a... | Jing Lu, Gustavo Hernández Ábrego, Ji Ma, Jianmo Ni, Yinfei Yang |  |
| 962 |  |  [Perhaps PTLMs Should Go to School - A Task to Assess Open Book and Closed Book QA](https://doi.org/10.18653/v1/2021.emnlp-main.493) |  | 0 | Our goal is to deliver a new task and leaderboard to stimulate research on question answering and pre-trained language models (PTLMs) to understand a significant instructional document, e.g., an introductory college textbook or a manual. PTLMs have shown great success in many question-answering... | Manuel R. Ciosici, Joe Cecil, DongHo Lee, Alex Hedges, Marjorie Freedman, Ralph M. Weischedel |  |
| 963 |  |  [ReasonBERT: Pre-trained to Reason with Distant Supervision](https://doi.org/10.18653/v1/2021.emnlp-main.494) |  | 0 | We present ReasonBert, a pre-training method that augments language models with the ability to reason over long-range relations and multiple, possibly hybrid contexts. Unlike existing pre-training methods that only harvest learning signals from local contexts of naturally occurring texts, we... | Xiang Deng, Yu Su, Alyssa Lees, You Wu, Cong Yu, Huan Sun |  |
| 964 |  |  [Single-dataset Experts for Multi-dataset Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.495) |  | 0 | Many datasets have been created for training reading comprehension models, and a natural question is whether we can combine them to build models that (1) perform better on all of the training datasets and (2) generalize and transfer better to new datasets. Prior work has addressed this goal by... | Dan Friedman, Ben Dodge, Danqi Chen |  |
| 965 |  |  [Simple Entity-Centric Questions Challenge Dense Retrievers](https://doi.org/10.18653/v1/2021.emnlp-main.496) |  | 0 | Open-domain question answering has exploded in popularity recently due to the success of dense retrieval models, which have surpassed sparse models using only a few supervised training examples. However, in this paper, we demonstrate current dense models are not yet the holy grail of retrieval. We... | Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, Danqi Chen |  |
| 966 |  |  [Mitigating False-Negative Contexts in Multi-document Question Answering with Retrieval Marginalization](https://doi.org/10.18653/v1/2021.emnlp-main.497) |  | 0 | Question Answering (QA) tasks requiring information from multiple documents often rely on a retrieval model to identify relevant information for reasoning. The retrieval model is typically trained to maximize the likelihood of the labeled supporting evidence. However, when retrieving from large... | Ansong Ni, Matt Gardner, Pradeep Dasigi |  |
| 967 |  |  [MultiDoc2Dial: Modeling Dialogues Grounded in Multiple Documents](https://doi.org/10.18653/v1/2021.emnlp-main.498) |  | 0 | We propose MultiDoc2Dial, a new task and dataset on modeling goal-oriented dialogues grounded in multiple documents. Most previous works treat document-grounded dialogue modeling as machine reading comprehension task based on a single given document or passage. In this work, we aim to address more... | Song Feng, Siva Sankalp Patel, Hui Wan, Sachindra Joshi |  |
| 968 |  |  [GupShup: Summarizing Open-Domain Code-Switched Conversations](https://doi.org/10.18653/v1/2021.emnlp-main.499) |  | 0 | Code-switching is the communication phenomenon where the speakers switch between different languages during a conversation. With the widespread adoption of conversational agents and chat platforms, code-switching has become an integral part of written conversations in many multi-lingual communities... | Laiba Mehnaz, Debanjan Mahata, Rakesh Gosangi, Uma Sushmitha Gunturi, Riya Jain, Gauri Gupta, Amardeep Kumar, Isabelle G. Lee, Anish Acharya, Rajiv Ratn Shah |  |
| 969 |  |  [BiSECT: Learning to Split and Rephrase Sentences with Bitexts](https://doi.org/10.18653/v1/2021.emnlp-main.500) |  | 0 | An important task in NLP applications such as sentence simplification is the ability to take a long, complex sentence and split it into shorter sentences, rephrasing as necessary. We introduce a novel dataset and a new model for this ‘split and rephrase’ task. Our BiSECT training data consists of 1... | Joongwon Kim, Mounica Maddela, Reno Kriz, Wei Xu, Chris CallisonBurch |  |
| 970 |  |  [Data Collection vs. Knowledge Graph Completion: What is Needed to Improve Coverage?](https://doi.org/10.18653/v1/2021.emnlp-main.501) |  | 0 | This survey/position paper discusses ways to improve coverage of resources such as WordNet. Rapp estimated correlations, rho, between corpus statistics and pyscholinguistic norms. rho improves with quantity (corpus size) and quality (balance). 1M words is enough for simple estimates (unigram... | Kenneth Church, Yuchen Bian |  |
| 971 |  |  [Universal Sentence Representation Learning with Conditional Masked Language Model](https://doi.org/10.18653/v1/2021.emnlp-main.502) |  | 0 | This paper presents a novel training method, Conditional Masked Language Modeling (CMLM), to effectively learn sentence representations on large scale unlabeled corpora. CMLM integrates sentence representation learning into MLM training by conditioning on the encoded vectors of adjacent sentences.... | Ziyi Yang, Yinfei Yang, Daniel Cer, Jax Law, Eric Darve |  |
| 972 |  |  [On the Benefit of Syntactic Supervision for Cross-lingual Transfer in Semantic Role Labeling](https://doi.org/10.18653/v1/2021.emnlp-main.503) |  | 0 | Although recent developments in neural architectures and pre-trained representations have greatly increased state-of-the-art model performance on fully-supervised semantic role labeling (SRL), the task remains challenging for languages where supervised SRL training data are not abundant.... | Zhisong Zhang, Emma Strubell, Eduard H. Hovy |  |
| 973 |  |  [Implicit Premise Generation with Discourse-aware Commonsense Knowledge Models](https://doi.org/10.18653/v1/2021.emnlp-main.504) |  | 0 | Enthymemes are defined as arguments where a premise or conclusion is left implicit. We tackle the task of generating the implicit premise in an enthymeme, which requires not only an understanding of the stated conclusion and premise but also additional inferences that could depend on commonsense... | Tuhin Chakrabarty, Aadit Trivedi, Smaranda Muresan |  |
| 974 |  |  [Inducing Transformer's Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks](https://doi.org/10.18653/v1/2021.emnlp-main.505) |  | 0 | Systematic compositionality is an essential mechanism in human language, allowing the recombination of known parts to create novel expressions. However, existing neural models have been shown to lack this basic ability in learning symbolic structures. Motivated by the failure of a Transformer model... | Yichen Jiang, Mohit Bansal |  |
| 975 |  |  [Flexible Generation of Natural Language Deductions](https://doi.org/10.18653/v1/2021.emnlp-main.506) |  | 0 | An interpretable system for open-domain reasoning needs to express its reasoning process in a transparent form. Natural language is an attractive representation for this purpose — it is both highly expressive and easy for humans to understand. However, manipulating natural language statements in... | Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, Greg Durrett |  |
| 976 |  |  [Structure-aware Fine-tuning of Sequence-to-sequence Transformers for Transition-based AMR Parsing](https://doi.org/10.18653/v1/2021.emnlp-main.507) |  | 0 | Predicting linearized Abstract Meaning Representation (AMR) graphs using pre-trained sequence-to-sequence Transformer models has recently led to large improvements on AMR parsing benchmarks. These parsers are simple and avoid explicit modeling of structure but lack desirable properties such as... | Jiawei Zhou, Tahira Naseem, Ramón Fernandez Astudillo, YoungSuk Lee, Radu Florian, Salim Roukos |  |
| 977 |  |  [Think about it! Improving defeasible reasoning by first modeling the question scenario](https://doi.org/10.18653/v1/2021.emnlp-main.508) |  | 0 | Defeasible reasoning is the mode of reasoning where conclusions can be overturned by taking into account new evidence. Existing cognitive science literature on defeasible reasoning suggests that a person forms a “mental model” of the problem scenario before answering questions. Our research goal... | Aman Madaan, Niket Tandon, Dheeraj Rajagopal, Peter Clark, Yiming Yang, Eduard H. Hovy |  |
| 978 |  |  [Open Aspect Target Sentiment Classification with Natural Language Prompts](https://doi.org/10.18653/v1/2021.emnlp-main.509) |  | 0 | For many business applications, we often seek to analyze sentiments associated with any arbitrary aspects of commercial products, despite having a very limited amount of labels or even without any labels at all. However, existing aspect target sentiment classification (ATSC) models are not... | Ronald Seoh, Ian Birle, Mrinal Tak, HawShiuan Chang, Brian Pinette, Alfred Hough |  |
| 979 |  |  [Does BERT Learn as Humans Perceive? Understanding Linguistic Styles through Lexica](https://doi.org/10.18653/v1/2021.emnlp-main.510) |  | 0 | People convey their intention and attitude through linguistic styles of the text that they write. In this study, we investigate lexicon usages across styles throughout two lenses: human perception and machine word importance, since words differ in the strength of the stylistic cues that they... | Shirley Anugrah Hayati, Dongyeop Kang, Lyle H. Ungar |  |
| 980 |  |  [Improving Stance Detection with Multi-Dataset Learning and Knowledge Distillation](https://doi.org/10.18653/v1/2021.emnlp-main.511) |  | 0 | Stance detection determines whether the author of a text is in favor of, against or neutral to a specific target and provides valuable insights into important events such as legalization of abortion. Despite significant progress on this task, one of the remaining challenges is the scarcity of... | Yingjie Li, Chenye Zhao, Cornelia Caragea |  |
| 981 |  |  [Discovering the Unknown Knowns: Turning Implicit Knowledge in the Dataset into Explicit Training Examples for Visual Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.512) |  | 0 | Visual question answering (VQA) is challenging not only because the model has to handle multi-modal information, but also because it is just so hard to collect sufficient training examples — there are too many questions one can ask about an image. As a result, a VQA model trained solely on... | Jihyung Kil, Cheng Zhang, Dong Xuan, WeiLun Chao |  |
| 982 |  |  [Improving Pre-trained Vision-and-Language Embeddings for Phrase Grounding](https://doi.org/10.18653/v1/2021.emnlp-main.513) |  | 0 | Phrase grounding aims to map textual phrases to their associated image regions, which can be a prerequisite for multimodal reasoning and can benefit tasks requiring identifying objects based on language. With pre-trained vision-and-language models achieving impressive performance across tasks, it... | ZiYi Dou, Nanyun Peng |  |
| 983 |  |  [Sequential Randomized Smoothing for Adversarially Robust Speech Recognition](https://doi.org/10.18653/v1/2021.emnlp-main.514) |  | 0 | While Automatic Speech Recognition has been shown to be vulnerable to adversarial attacks, defenses against these attacks are still lagging. Existing, naive defenses can be partially broken with an adaptive attack. In classification tasks, the Randomized Smoothing paradigm has been shown to be... | Raphaël Olivier, Bhiksha Raj |  |
| 984 |  |  [Hitting your MARQ: Multimodal ARgument Quality Assessment in Long Debate Video](https://doi.org/10.18653/v1/2021.emnlp-main.515) |  | 0 | The combination of gestures, intonations, and textual content plays a key role in argument delivery. However, the current literature mostly considers textual content while assessing the quality of an argument, and it is limited to datasets containing short sequences (18-48 words). In this paper, we... | Md. Kamrul Hasan, James Spann, Masum Hasan, Md. Saiful Islam, Kurtis Haut, Rada Mihalcea, Ehsan Hoque |  |
| 985 |  |  [Mind the Context: The Impact of Contextualization in Neural Module Networks for Grounding Visual Referring Expressions](https://doi.org/10.18653/v1/2021.emnlp-main.516) |  | 0 | Neural module networks (NMN) are a popular approach for grounding visual referring expressions. Prior implementations of NMN use pre-defined and fixed textual inputs in their module instantiation. This necessitates a large number of modules as they lack the ability to share weights and exploit... | Arjun R. Akula, Spandana Gella, Keze Wang, SongChun Zhu, Siva Reddy |  |
| 986 |  |  [Weakly-Supervised Visual-Retriever-Reader for Knowledge-based Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.517) |  | 0 | Knowledge-based visual question answering (VQA) requires answering questions with external knowledge in addition to the content of images. One dataset that is mostly used in evaluating knowledge-based VQA is OK-VQA, but it lacks a gold standard knowledge corpus for retrieval. Existing work leverage... | Man Luo, Yankai Zeng, Pratyay Banerjee, Chitta Baral |  |
| 987 |  |  [NDH-Full: Learning and Evaluating Navigational Agents on Full-Length Dialogue](https://doi.org/10.18653/v1/2021.emnlp-main.518) |  | 0 | Communication between human and mobile agents is getting increasingly important as such agents are widely deployed in our daily lives. Vision-and-Dialogue Navigation is one of the tasks that evaluate the agent’s ability to interact with humans for assistance and navigate based on natural language... | Hyounghun Kim, Jialu Li, Mohit Bansal |  |
| 988 |  |  [Timeline Summarization based on Event Graph Compression via Time-Aware Optimal Transport](https://doi.org/10.18653/v1/2021.emnlp-main.519) |  | 0 | Timeline Summarization identifies major events from a news collection and describes them following temporal order, with key dates tagged. Previous methods generally generate summaries separately for each date after they determine the key dates of events. These methods overlook the events’... | Manling Li, Tengfei Ma, Mo Yu, Lingfei Wu, Tian Gao, Heng Ji, Kathleen R. McKeown |  |
| 989 |  |  [StreamHover: Livestream Transcript Summarization and Annotation](https://doi.org/10.18653/v1/2021.emnlp-main.520) |  | 0 | With the explosive growth of livestream broadcasting, there is an urgent need for new summarization technology that enables us to create a preview of streamed content and tap into this wealth of knowledge. However, the problem is nontrivial due to the informal nature of spoken language. Further,... | Sangwoo Cho, Franck Dernoncourt, Tim Ganter, Trung Bui, Nedim Lipka, Walter Chang, Hailin Jin, Jonathan Brandt, Hassan Foroosh, Fei Liu |  |
| 990 |  |  [Cross-Register Projection for Headline Part of Speech Tagging](https://doi.org/10.18653/v1/2021.emnlp-main.521) |  | 0 | Part of speech (POS) tagging is a familiar NLP task. State of the art taggers routinely achieve token-level accuracies of over 97% on news body text, evidence that the problem is well understood. However, the register of English news headlines, “headlinese”, is very different from the register of... | Adrian Benton, Hanyang Li, Igor Malioutov |  |
| 991 |  |  [Editing Factual Knowledge in Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.522) |  | 0 | The factual knowledge acquired during pre-training and stored in the parameters of Language Models (LMs) can be useful in downstream tasks (e.g., question answering or textual inference). However, some facts can be incorrectly induced or become obsolete over time. We present KnowledgeEditor, a... | Nicola De Cao, Wilker Aziz, Ivan Titov |  |
| 992 |  |  [Sparse Attention with Linear Units](https://doi.org/10.18653/v1/2021.emnlp-main.523) |  | 0 | Recently, it has been argued that encoder-decoder models can be made more interpretable by replacing the softmax function in the attention with its sparse variants. In this work, we introduce a novel, simple method for achieving sparsity in attention: we replace the softmax activation with a ReLU,... | Biao Zhang, Ivan Titov, Rico Sennrich |  |
| 993 |  |  [Knowledge Base Completion Meets Transfer Learning](https://doi.org/10.18653/v1/2021.emnlp-main.524) |  | 0 | The aim of knowledge base completion is to predict unseen facts from existing facts in knowledge bases. In this work, we introduce the first approach for transfer of knowledge from one collection of facts to another without the need for entity or relation matching. The method works for both... | Vid Kocijan, Thomas Lukasiewicz |  |
| 994 |  |  [SPECTRA: Sparse Structured Text Rationalization](https://doi.org/10.18653/v1/2021.emnlp-main.525) |  | 0 | Selective rationalization aims to produce decisions along with rationales (e.g., text highlights or word alignments between two sentences). Commonly, rationales are modeled as stochastic binary masks, requiring sampling-based gradient estimators, which complicates training and requires careful... | Nuno Miguel Guerreiro, André F. T. Martins |  |
| 995 |  |  [Towards Zero-Shot Knowledge Distillation for Natural Language Processing](https://doi.org/10.18653/v1/2021.emnlp-main.526) |  | 0 | Knowledge distillation (KD) is a common knowledge transfer algorithm used for model compression across a variety of deep learning based natural language processing (NLP) solutions. In its regular manifestations, KD requires access to the teacher’s training data for knowledge transfer to the student... | Ahmad Rashid, Vasileios Lioutas, Abbas Ghaddar, Mehdi Rezagholizadeh |  |
| 996 |  |  [Adversarial Regularization as Stackelberg Game: An Unrolled Optimization Approach](https://doi.org/10.18653/v1/2021.emnlp-main.527) |  | 0 | Adversarial regularization has been shown to improve the generalization performance of deep learning models in various natural language processing tasks. Existing works usually formulate the method as a zero-sum game, which is solved by alternating gradient descent/ascent algorithms. Such a... | Simiao Zuo, Chen Liang, Haoming Jiang, Xiaodong Liu, Pengcheng He, Jianfeng Gao, Weizhu Chen, Tuo Zhao |  |
| 997 |  |  [Aspect-Controllable Opinion Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.528) |  | 0 | Recent work on opinion summarization produces general summaries based on a set of input reviews and the popularity of opinions expressed in them. In this paper, we propose an approach that allows the generation of customized summaries based on aspect queries (e.g., describing the location and room... | Reinald Kim Amplayo, Stefanos Angelidis, Mirella Lapata |  |
| 998 |  |  [QuestEval: Summarization Asks for Fact-based Evaluation](https://doi.org/10.18653/v1/2021.emnlp-main.529) |  | 0 | Summarization evaluation remains an open research problem: current metrics such as ROUGE are known to be limited and to correlate poorly with human judgments. To alleviate this issue, recent work has proposed evaluation metrics which rely on question answering models to assess whether a summary... | Thomas Scialom, PaulAlexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, Patrick Gallinari |  |
| 999 |  |  [Simple Conversational Data Augmentation for Semi-supervised Abstractive Dialogue Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.530) |  | 0 | Abstractive conversation summarization has received growing attention while most current state-of-the-art summarization models heavily rely on human-annotated summaries. To reduce the dependence on labeled summaries, in this work, we present a simple yet effective set of Conversational Data... | Jiaao Chen, Diyi Yang |  |
| 1000 |  |  [Finding a Balanced Degree of Automation for Summary Evaluation](https://doi.org/10.18653/v1/2021.emnlp-main.531) |  | 0 | Human evaluation for summarization tasks is reliable but brings in issues of reproducibility and high costs. Automatic metrics are cheap and reproducible but sometimes poorly correlated with human judgment. In this work, we propose flexible semiautomatic to automatic summary evaluation metrics,... | Shiyue Zhang, Mohit Bansal |  |
| 1001 |  |  [CLIFF: Contrastive Learning for Improving Faithfulness and Factuality in Abstractive Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.532) |  | 0 | We study generating abstractive summaries that are faithful and factually consistent with the given articles. A novel contrastive learning formulation is presented, which leverages both reference summaries, as positive training data, and automatically generated erroneous summaries, as negative... | Shuyang Cao, Lu Wang |  |
| 1002 |  |  [Multilingual Unsupervised Neural Machine Translation with Denoising Adapters](https://doi.org/10.18653/v1/2021.emnlp-main.533) |  | 0 | We consider the problem of multilingual unsupervised machine translation, translating to and from languages that only have monolingual data by using auxiliary parallel language pairs. For this problem the standard procedure so far to leverage the monolingual data is _back-translation_, which is... | Ahmet Üstün, Alexandre Berard, Laurent Besacier, Matthias Gallé |  |
| 1003 |  |  [BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.534) |  | 0 | The success of bidirectional encoders using masked language models, such as BERT, on numerous natural language processing tasks has prompted researchers to attempt to incorporate these pre-trained models into neural machine translation (NMT) systems. However, proposed methods for incorporating... | Haoran Xu, Benjamin Van Durme, Kenton W. Murray |  |
| 1004 |  |  [Controlling Machine Translation for Multiple Attributes with Additive Interventions](https://doi.org/10.18653/v1/2021.emnlp-main.535) |  | 0 | Fine-grained control of machine translation (MT) outputs along multiple attributes is critical for many modern MT applications and is a requirement for gaining users’ trust. A standard approach for exerting control in MT is to prepend the input with a special tag to signal the desired output... | Andrea Schioppa, David Vilar, Artem Sokolov, Katja Filippova |  |
| 1005 |  |  [A Generative Framework for Simultaneous Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.536) |  | 0 | We propose a generative framework for simultaneous machine translation. Conventional approaches use a fixed number of source words to translate or learn dynamic policies for the number of source words by reinforcement learning. Here we formulate simultaneous translation as a structural... | Yishu Miao, Phil Blunsom, Lucia Specia |  |
| 1006 |  |  [It Is Not As Good As You Think! Evaluating Simultaneous Machine Translation on Interpretation Data](https://doi.org/10.18653/v1/2021.emnlp-main.537) |  | 0 | Most existing simultaneous machine translation (SiMT) systems are trained and evaluated on offline translation corpora. We argue that SiMT systems should be trained and tested on real interpretation data. To illustrate this argument, we propose an interpretation test set and conduct a realistic... | Jinming Zhao, Philip Arthur, Gholamreza Haffari, Trevor Cohn, Ehsan Shareghi |  |
| 1007 |  |  [Boosting Cross-Lingual Transfer via Self-Learning with Uncertainty Estimation](https://doi.org/10.18653/v1/2021.emnlp-main.538) |  | 0 | Recent multilingual pre-trained language models have achieved remarkable zero-shot performance, where the model is only finetuned on one source language and directly evaluated on target languages. In this work, we propose a self-learning framework that further utilizes unlabeled data of target... | Liyan Xu, Xuchao Zhang, Xujiang Zhao, Haifeng Chen, Feng Chen, Jinho D. Choi |  |
| 1008 |  |  [Levenshtein Training for Word-level Quality Estimation](https://doi.org/10.18653/v1/2021.emnlp-main.539) |  | 0 | We propose a novel scheme to use the Levenshtein Transformer to perform the task of word-level quality estimation. A Levenshtein Transformer is a natural fit for this task: trained to perform decoding in an iterative manner, a Levenshtein Transformer can learn to post-edit without explicit... | Shuoyang Ding, Marcin JunczysDowmunt, Matt Post, Philipp Koehn |  |
| 1009 |  |  [Interactive Machine Comprehension with Dynamic Knowledge Graphs](https://doi.org/10.18653/v1/2021.emnlp-main.540) |  | 0 | Interactive machine reading comprehension (iMRC) is machine comprehension tasks where knowledge sources are partially observable. An agent must interact with an environment sequentially to gather necessary knowledge in order to answer a question. We hypothesize that graph representations are good... | Xingdi Yuan |  |
| 1010 |  |  [Residual Adapters for Parameter-Efficient ASR Adaptation to Atypical and Accented Speech](https://doi.org/10.18653/v1/2021.emnlp-main.541) |  | 0 | Automatic Speech Recognition (ASR) systems are often optimized to work best for speakers with canonical speech patterns. Unfortunately, these systems perform poorly when tested on atypical speech and heavily accented speech. It has previously been shown that personalization through model... | Katrin Tomanek, Vicky Zayats, Dirk Padfield, Kara Vaillancourt, Fadi Biadsy |  |
| 1011 |  |  [Visual News: Benchmark and Challenges in News Image Captioning](https://doi.org/10.18653/v1/2021.emnlp-main.542) |  | 0 | We propose Visual News Captioner, an entity-aware model for the task of news image captioning. We also introduce Visual News, a large-scale benchmark consisting of more than one million news images along with associated news articles, image captions, author information, and other metadata. Unlike... | Fuxiao Liu, Yinghan Wang, Tianlu Wang, Vicente Ordonez |  |
| 1012 |  |  [Integrating Visuospatial, Linguistic, and Commonsense Structure into Story Visualization](https://doi.org/10.18653/v1/2021.emnlp-main.543) |  | 0 | While much research has been done in text-to-image synthesis, little work has been done to explore the usage of linguistic structure of the input text. Such information is even more important for story visualization since its inputs have an explicit narrative structure that needs to be translated... | Adyasha Maharana, Mohit Bansal |  |
| 1013 |  |  [VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding](https://doi.org/10.18653/v1/2021.emnlp-main.544) |  | 0 | We present VideoCLIP, a contrastive approach to pre-train a unified model for zero-shot video and text understanding, without using any labels on downstream tasks. VideoCLIP trains a transformer for video and text by contrasting temporally overlapping positive video-text pairs with hard negatives... | Hu Xu, Gargi Ghosh, PoYao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, Christoph Feichtenhofer |  |
| 1014 |  |  [NewsCLIPpings: Automatic Generation of Out-of-Context Multimodal Media](https://doi.org/10.18653/v1/2021.emnlp-main.545) |  | 0 | Online misinformation is a prevalent societal issue, with adversaries relying on tools ranging from cheap fakes to sophisticated deep fakes. We are motivated by the threat scenario where an image is used out of context to support a certain narrative. While some prior datasets for detecting... | Grace Luo, Trevor Darrell, Anna Rohrbach |  |
| 1015 |  |  [Powering Comparative Classification with Sentiment Analysis via Domain Adaptive Knowledge Transfer](https://doi.org/10.18653/v1/2021.emnlp-main.546) |  | 0 | We study Comparative Preference Classification (CPC) which aims at predicting whether a preference comparison exists between two entities in a given sentence and, if so, which entity is preferred over the other. High-quality CPC models can significantly benefit applications such as comparative... | Zeyu Li, Yilong Qin, Zihan Liu, Wei Wang |  |
| 1016 |  |  [Tribrid: Stance Classification with Neural Inconsistency Detection](https://doi.org/10.18653/v1/2021.emnlp-main.547) |  | 0 | We study the problem of performing automatic stance classification on social media with neural architectures such as BERT. Although these architectures deliver impressive results, their level is not yet comparable to the one of humans and they might produce errors that have a significant impact on... | Song Yang, Jacopo Urbani |  |
| 1017 |  |  [SYSML: StYlometry with Structure and Multitask Learning: Implications for Darknet Forum Migrant Analysis](https://doi.org/10.18653/v1/2021.emnlp-main.548) |  | 0 | Darknet market forums are frequently used to exchange illegal goods and services between parties who use encryption to conceal their identities. The Tor network is used to host these markets, which guarantees additional anonymization from IP and location tracking, making it challenging to link... | Pranav Maneriker, Yuntian He, Srinivasan Parthasarathy |  |
| 1018 |  |  [Few-Shot Emotion Recognition in Conversation with Sequential Prototypical Networks](https://doi.org/10.18653/v1/2021.emnlp-main.549) |  | 0 | Several recent studies on dyadic human-human interactions have been done on conversations without specific business objectives. However, many companies might benefit from studies dedicated to more precise environments such as after sales services or customer satisfaction surveys. In this work, we... | Gaël Guibon, Matthieu Labeau, Hélène Flamein, Luce Lefeuvre, Chloé Clavel |  |
| 1019 |  |  [CLASSIC: Continual and Contrastive Learning of Aspect Sentiment Classification Tasks](https://doi.org/10.18653/v1/2021.emnlp-main.550) |  | 0 | This paper studies continual learning (CL) of a sequence of aspect sentiment classification (ASC) tasks in a particular CL setting called domain incremental learning (DIL). Each task is from a different domain or product. The DIL setting is particularly suited to ASC because in testing the system... | Zixuan Ke, Bing Liu, Hu Xu, Lei Shu |  |
| 1020 |  |  [Implicit Sentiment Analysis with Event-centered Text Representation](https://doi.org/10.18653/v1/2021.emnlp-main.551) |  | 0 | Implicit sentiment analysis, aiming at detecting the sentiment of a sentence without sentiment words, has become an attractive research topic in recent years. In this paper, we focus on event-centric implicit sentiment analysis that utilizes the sentiment-aware event contained in a sentence to... | Deyu Zhou, Jianan Wang, Linhai Zhang, Yulan He |  |
| 1021 |  |  [SimCSE: Simple Contrastive Learning of Sentence Embeddings](https://doi.org/10.18653/v1/2021.emnlp-main.552) |  | 0 | This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise.... | Tianyu Gao, Xingcheng Yao, Danqi Chen |  |
| 1022 |  |  [When is Wall a Pared and when a Muro?: Extracting Rules Governing Lexical Selection](https://doi.org/10.18653/v1/2021.emnlp-main.553) |  | 0 | Learning fine-grained distinctions between vocabulary items is a key challenge in learning a new language. For example, the noun “wall” has different lexical manifestations in Spanish – “pared” refers to an indoor wall while “muro” refers to an outside wall. However, this variety of lexical... | Aditi Chaudhary, Kayo Yin, Antonios Anastasopoulos, Graham Neubig |  |
| 1023 |  |  [Aligning Actions Across Recipe Graphs](https://doi.org/10.18653/v1/2021.emnlp-main.554) |  | 0 | Recipe texts are an idiosyncratic form of instructional language that pose unique challenges for automatic understanding. One challenge is that a cooking step in one recipe can be explained in another recipe in different words, at a different level of abstraction, or not at all. Previous work has... | Lucia Donatelli, Theresa Schmidt, Debanjali Biswas, Arne Köhn, Fangzhou Zhai, Alexander Koller |  |
| 1024 |  |  [Generating Datasets with Pretrained Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.555) |  | 0 | To obtain high-quality sentence embeddings from pretrained language models (PLMs), they must either be augmented with additional pretraining objectives or finetuned on a large set of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to... | Timo Schick, Hinrich Schütze |  |
| 1025 |  |  [Continuous Entailment Patterns for Lexical Inference in Context](https://doi.org/10.18653/v1/2021.emnlp-main.556) |  | 0 | Combining a pretrained language model (PLM) with textual patterns has been shown to help in both zero- and few-shot settings. For zero-shot performance, it makes sense to design patterns that closely resemble the text seen during self-supervised pretraining because the model has never seen anything... | Martin Schmitt, Hinrich Schütze |  |
| 1026 |  |  [Numeracy enhances the Literacy of Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.557) |  | 0 | Specialized number representations in NLP have shown improvements on numerical reasoning tasks like arithmetic word problems and masked number prediction. But humans also use numeracy to make better sense of world concepts, e.g., you can seat 5 people in your ‘room’ but not 500. Does a better grasp... | Avijit Thawani, Jay Pujara, Filip Ilievski |  |
| 1027 |  |  [Students Who Study Together Learn Better: On the Importance of Collective Knowledge Distillation for Domain Transfer in Fact Verification](https://doi.org/10.18653/v1/2021.emnlp-main.558) |  | 0 | While neural networks produce state-of-the- art performance in several NLP tasks, they generally depend heavily on lexicalized information, which transfer poorly between domains. Previous works have proposed delexicalization as a form of knowledge distillation to reduce the dependency on such... | Mitch Paul Mithun, Sandeep Suntwal, Mihai Surdeanu |  |
| 1028 |  |  [MultiEURLEX - A multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer](https://doi.org/10.18653/v1/2021.emnlp-main.559) |  | 0 | We introduce MULTI-EURLEX, a new multilingual dataset for topic classification of legal documents. The dataset comprises 65k European Union (EU) laws, officially translated in 23 languages, annotated with multiple labels from the EUROVOC taxonomy. We highlight the effect of temporal concept drift... | Ilias Chalkidis, Manos Fergadiotis, Ion Androutsopoulos |  |
| 1029 |  |  [Joint Passage Ranking for Diverse Multi-Answer Retrieval](https://doi.org/10.18653/v1/2021.emnlp-main.560) |  | 0 | We study multi-answer retrieval, an under-explored problem that requires retrieving passages to cover multiple distinct answers for a given question. This task requires joint modeling of retrieved passages, as models should not repeatedly retrieve passages containing the same answer at the cost of... | Sewon Min, Kenton Lee, MingWei Chang, Kristina Toutanova, Hannaneh Hajishirzi |  |
| 1030 |  |  [Generative Context Pair Selection for Multi-hop Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.561) |  | 0 | Compositional reasoning tasks such as multi-hop question answering require models to learn how to make latent decisions using only weak supervision from the final answer. Crowdsourced datasets gathered for these tasks, however, often contain only a slice of the underlying task distribution, which... | Dheeru Dua, Cícero Nogueira dos Santos, Patrick Ng, Ben Athiwaratkun, Bing Xiang, Matt Gardner, Sameer Singh |  |
| 1031 |  |  [Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.562) |  | 0 | Coupled with the availability of large scale datasets, deep learning architectures have enabled rapid progress on the Question Answering task. However, most of those datasets are in English, and the performances of state-of-the-art multilingual models are significantly lower when evaluated on... | Arij Riabi, Thomas Scialom, Rachel Keraron, Benoît Sagot, Djamé Seddah, Jacopo Staiano |  |
| 1032 |  |  [Have You Seen That Number? Investigating Extrapolation in Question Answering Models](https://doi.org/10.18653/v1/2021.emnlp-main.563) |  | 0 | Numerical reasoning in machine reading comprehension (MRC) has shown drastic improvements over the past few years. While the previous models for numerical MRC are able to interpolate the learned numerical reasoning capabilities, it is not clear whether they can perform just as well on numbers... | Jeonghwan Kim, Giwon Hong, KyungMin Kim, Junmo Kang, SungHyon Myaeng |  |
| 1033 |  |  [Surface Form Competition: Why the Highest Probability Answer Isn't Always Right](https://doi.org/10.18653/v1/2021.emnlp-main.564) |  | 0 | Large language models have shown promising results in zero-shot settings. For example, they can perform multiple choice tasks simply by conditioning on a question and selecting the answer with the highest probability. However, ranking by string probability can be problematic due to surface form... | Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, Luke Zettlemoyer |  |
| 1034 |  |  [Entity-Based Knowledge Conflicts in Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.565) |  | 0 | Knowledge-dependent tasks typically use two sources of knowledge: parametric, learned at training time, and contextual, given as a passage at inference time. To understand how models use these sources together, we formalize the problem of knowledge conflicts, where the contextual information... | Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, Sameer Singh |  |
| 1035 |  |  [Back-Training excels Self-Training at Unsupervised Domain Adaptation of Question Generation and Passage Retrieval](https://doi.org/10.18653/v1/2021.emnlp-main.566) |  | 0 | In this work, we introduce back-training, an alternative to self-training for unsupervised domain adaptation (UDA). While self-training generates synthetic training data where natural inputs are aligned with noisy outputs, back-training results in natural outputs aligned with noisy inputs. This... | Devang Kulshreshtha, Robert Belfer, Iulian Vlad Serban, Siva Reddy |  |
| 1036 |  |  [DWUG: A large Resource of Diachronic Word Usage Graphs in Four Languages](https://doi.org/10.18653/v1/2021.emnlp-main.567) |  | 0 | Word meaning is notoriously difficult to capture, both synchronically and diachronically. In this paper, we describe the creation of the largest resource of graded contextualized, diachronic word meaning annotation in four different languages, based on 100,000 human semantic proximity judgments. We... | Dominik Schlechtweg, Nina Tahmasebi, Simon Hengchen, Haim Dubossarsky, Barbara McGillivray |  |
| 1037 |  |  [I Wish I Would Have Loved This One, But I Didn't - A Multilingual Dataset for Counterfactual Detection in Product Review](https://doi.org/10.18653/v1/2021.emnlp-main.568) |  | 0 | Counterfactual statements describe events that did not or cannot take place. We consider the problem of counterfactual detection (CFD) in product reviews. For this purpose, we annotate a multilingual CFD dataset from Amazon product reviews covering counterfactual statements written in English,... | James O'Neill, Polina Rozenshtein, Ryuichi Kiryo, Motoko Kubota, Danushka Bollegala |  |
| 1038 |  |  [Does It Capture STEL? A Modular, Similarity-based Linguistic Style Evaluation Framework](https://doi.org/10.18653/v1/2021.emnlp-main.569) |  | 0 | Style is an integral part of natural language. However, evaluation methods for style measures are rare, often task-specific and usually do not control for content. We propose the modular, fine-grained and content-controlled similarity-based STyle EvaLuation framework (STEL) to test the performance... | Anna Wegmann, Dong Nguyen |  |
| 1039 |  |  [Evaluating the Morphosyntactic Well-formedness of Generated Texts](https://doi.org/10.18653/v1/2021.emnlp-main.570) |  | 0 | Text generation systems are ubiquitous in natural language processing applications. However, evaluation of these systems remains a challenge, especially in multilingual settings. In this paper, we propose L’AMBRE – a metric to evaluate the morphosyntactic well-formedness of text using its... | Adithya Pratapa, Antonios Anastasopoulos, Shruti Rijhwani, Aditi Chaudhary, David R. Mortensen, Graham Neubig, Yulia Tsvetkov |  |
| 1040 |  |  [AM2iCo: Evaluating Word Meaning in Context across Low-Resource Languages with Adversarial Examples](https://doi.org/10.18653/v1/2021.emnlp-main.571) |  | 0 | Capturing word meaning in context and distinguishing between correspondences and variations across languages is key to building successful multilingual and cross-lingual text representation models. However, existing multilingual evaluation datasets that evaluate lexical semantics “in-context” have... | Qianchu Liu, Edoardo Maria Ponti, Diana McCarthy, Ivan Vulic, Anna Korhonen |  |
| 1041 |  |  [CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP](https://doi.org/10.18653/v1/2021.emnlp-main.572) |  | 0 | Humans can learn a new language task efficiently with only few examples, by leveraging their knowledge obtained when learning prior tasks. In this paper, we explore whether and how such cross-task generalization ability can be acquired, and further applied to build better few-shot learners across... | Qinyuan Ye, Bill Yuchen Lin, Xiang Ren |  |
| 1042 |  |  [On the Influence of Masking Policies in Intermediate Pre-training](https://doi.org/10.18653/v1/2021.emnlp-main.573) |  | 0 | Current NLP models are predominantly trained through a two-stage “pre-train then fine-tune” pipeline. Prior work has shown that inserting an intermediate pre-training stage, using heuristic masking policies for masked language modeling (MLM), can significantly improve final performance. However, it... | Qinyuan Ye, Belinda Z. Li, Sinong Wang, Benjamin Bolte, Hao Ma, Wentau Yih, Xiang Ren, Madian Khabsa |  |
| 1043 |  |  [ValNorm Quantifies Semantics to Reveal Consistent Valence Biases Across Languages and Over Centuries](https://doi.org/10.18653/v1/2021.emnlp-main.574) |  | 0 | Word embeddings learn implicit biases from linguistic regularities captured by word co-occurrence statistics. By extending methods that quantify human-like biases in word embeddings, we introduce ValNorm, a novel intrinsic evaluation task and method to quantify the valence dimension of affect in... | Autumn Toney, Aylin Caliskan |  |
| 1044 |  |  [Perturbation CheckLists for Evaluating NLG Evaluation Metrics](https://doi.org/10.18653/v1/2021.emnlp-main.575) |  | 0 | Natural Language Generation (NLG) evaluation is a multifaceted task requiring assessment of multiple desirable criteria, e.g., fluency, coherency, coverage, relevance, adequacy, overall quality, etc. Across existing datasets for 6 NLG tasks, we observe that the human evaluation scores on these... | Ananya B. Sai, Tanay Dixit, Dev Yashpal Sheth, Sreyas Mohan, Mitesh M. Khapra |  |
| 1045 |  |  [Robust Open-Vocabulary Translation from Visual Text Representations](https://doi.org/10.18653/v1/2021.emnlp-main.576) |  | 0 | Machine translation models have discrete vocabularies and commonly use subword segmentation techniques to achieve an ‘open vocabulary.’ This approach relies on consistent and correct underlying unicode sequences, and makes models susceptible to degradation from common types of noise and variation.... | Elizabeth Salesky, David Etter, Matt Post |  |
| 1046 |  |  [Don't Go Far Off: An Empirical Study on Neural Poetry Translation](https://doi.org/10.18653/v1/2021.emnlp-main.577) |  | 0 | Despite constant improvements in machine translation quality, automatic poetry translation remains a challenging problem due to the lack of open-sourced parallel poetic corpora, and to the intrinsic complexities involved in preserving the semantics, style and figurative nature of poetry. We present... | Tuhin Chakrabarty, Arkadiy Saakyan, Smaranda Muresan |  |
| 1047 |  |  [Improving Multilingual Translation by Representation and Gradient Regularization](https://doi.org/10.18653/v1/2021.emnlp-main.578) |  | 0 | Multilingual Neural Machine Translation (NMT) enables one model to serve all translation directions, including ones that are unseen during training, i.e. zero-shot translation. Despite being theoretically attractive, current models often produce low quality translations – commonly failing to even... | Yilin Yang, Akiko Eriguchi, Alexandre Muzio, Prasad Tadepalli, Stefan Lee, Hany Hassan |  |
| 1048 |  |  [Learning Kernel-Smoothed Machine Translation with Retrieved Examples](https://doi.org/10.18653/v1/2021.emnlp-main.579) |  | 0 | How to effectively adapt neural machine translation (NMT) models according to emerging cases without retraining? Despite the great success of neural machine translation, updating the deployed models online remains a challenge. Existing non-parametric approaches that retrieve similar examples from a... | Qingnan Jiang, Mingxuan Wang, Jun Cao, Shanbo Cheng, Shujian Huang, Lei Li |  |
| 1049 |  |  [Uncertainty-Aware Balancing for Multilingual and Multi-Domain Neural Machine Translation Training](https://doi.org/10.18653/v1/2021.emnlp-main.580) |  | 0 | Learning multilingual and multi-domain translation model is challenging as the heterogeneous and imbalanced data make the model converge inconsistently over different corpora in real world. One common practice is to adjust the share of each corpus in the training, so that the learning process is... | Minghao Wu, Yitong Li, Meng Zhang, Liangyou Li, Gholamreza Haffari, Qun Liu |  |
| 1050 |  |  [Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy](https://doi.org/10.18653/v1/2021.emnlp-main.581) |  | 0 | Simultaneous machine translation (SiMT) generates translation before reading the entire source sentence and hence it has to trade off between translation quality and latency. To fulfill the requirements of different translation quality and latency in practical applications, the previous methods... | Shaolei Zhang, Yang Feng |  |
| 1051 |  |  [How much coffee was consumed during EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI](https://doi.org/10.18653/v1/2021.emnlp-main.582) |  | 0 | Many real-world problems require the combined application of multiple reasoning abilities—employing suitable abstractions, commonsense knowledge, and creative synthesis of problem-solving strategies. To help advance AI systems towards such capabilities, we propose a new reasoning challenge, namely... | Ashwin Kalyan, Abhinav Kumar, Arjun Chandrasekaran, Ashish Sabharwal, Peter Clark |  |
| 1052 |  |  [Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.583) |  | 0 | In this paper we propose a novel approach towards improving the efficiency of Question Answering (QA) systems by filtering out questions that will not be answered by them. This is based on an interesting new finding: the answer confidence scores of state-of-the-art QA systems can be approximated... | Siddhant Garg, Alessandro Moschitti |  |
| 1053 |  |  [Learning with Instance Bundles for Reading Comprehension](https://doi.org/10.18653/v1/2021.emnlp-main.584) |  | 0 | When training most modern reading comprehension models, all the questions associated with a context are treated as being independent from each other. However, closely related questions and their corresponding answers are not independent, and leveraging these relationships could provide a strong... | Dheeru Dua, Pradeep Dasigi, Sameer Singh, Matt Gardner |  |
| 1054 |  |  [Explaining Answers with Entailment Trees](https://doi.org/10.18653/v1/2021.emnlp-main.585) |  | 0 | Our goal, in the context of open-domain textual question-answering (QA), is to explain answers by showing the line of reasoning from what is known to the answer, rather than simply showing a fragment of textual evidence (a “rationale”). If this could be done, new opportunities for understanding and... | Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, Peter Clark |  |
| 1055 |  |  [SituatedQA: Incorporating Extra-Linguistic Contexts into QA](https://doi.org/10.18653/v1/2021.emnlp-main.586) |  | 0 | Answers to the same question may change depending on the extra-linguistic contexts (when and where the question was asked). To study this challenge, we introduce SituatedQA, an open-retrieval QA dataset where systems must produce the correct answer to a question given the temporal or geographical... | Michael J. Q. Zhang, Eunsol Choi |  |
| 1056 |  |  [ConvAbuse: Data, Analysis, and Benchmarks for Nuanced Detection in Conversational AI](https://doi.org/10.18653/v1/2021.emnlp-main.587) |  | 0 | We present the first English corpus study on abusive language towards three conversational AI systems gathered ‘in the wild’: an open-domain social bot, a rule-based chatbot, and a task-based system. To account for the complexity of the task, we take a more ‘nuanced’ approach where our ConvAI... | Amanda Cercas Curry, Gavin Abercrombie, Verena Rieser |  |
| 1057 |  |  [Conversational Multi-Hop Reasoning with Neural Commonsense Knowledge and Symbolic Logic Rules](https://doi.org/10.18653/v1/2021.emnlp-main.588) |  | 0 | One of the challenges faced by conversational agents is their inability to identify unstated presumptions of their users’ commands, a task trivial for humans due to their common sense. In this paper, we propose a zero-shot commonsense reasoning system for conversational agents in an attempt to... | Forough Arabshahi, Jennifer Lee, Antoine Bosselut, Yejin Choi, Tom M. Mitchell |  |
| 1058 |  |  [Towards Automatic Evaluation of Dialog Systems: A Model-Free Off-Policy Evaluation Approach](https://doi.org/10.18653/v1/2021.emnlp-main.589) |  | 0 | Reliable automatic evaluation of dialogue systems under an interactive environment has long been overdue. An ideal environment for evaluating dialog systems, also known as the Turing test, needs to involve human interaction, which is usually not affordable for large-scale experiments. Though... | Haoming Jiang, Bo Dai, Mengjiao Yang, Tuo Zhao, Wei Wei |  |
| 1059 |  |  [Continual Learning in Task-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2021.emnlp-main.590) |  | 0 | Continual learning in task-oriented dialogue systems allows the system to add new domains and functionalities overtime after deployment, without incurring the high cost of retraining the whole system each time. In this paper, we propose a first-ever continual learning benchmark for task-oriented... | Andrea Madotto, Zhaojiang Lin, Zhenpeng Zhou, Seungwhan Moon, Paul A. Crook, Bing Liu, Zhou Yu, Eunjoon Cho, Pascale Fung, Zhiguang Wang |  |
| 1060 |  |  [Multilingual and Cross-Lingual Intent Detection from Spoken Data](https://doi.org/10.18653/v1/2021.emnlp-main.591) |  | 0 | We present a systematic study on multilingual and cross-lingual intent detection (ID) from spoken data. The study leverages a new resource put forth in this work, termed MInDS-14, a first training and evaluation resource for the ID task with spoken data. It covers 14 intents extracted from a... | Daniela Gerz, PeiHao Su, Razvan Kusztos, Avishek Mondal, Michal Lis, Eshan Singhal, Nikola Mrksic, TsungHsien Wen, Ivan Vulic |  |
| 1061 |  |  [Investigating Robustness of Dialog Models to Popular Figurative Language Constructs](https://doi.org/10.18653/v1/2021.emnlp-main.592) |  | 0 | Humans often employ figurative language use in communication, including during interactions with dialog systems. Thus, it is important for real-world dialog systems to be able to handle popular figurative language constructs like metaphor and simile. In this work, we analyze the performance of... | Harsh Jhamtani, Varun Gangal, Eduard H. Hovy, Taylor BergKirkpatrick |  |
| 1062 |  |  [Effective Sequence-to-Sequence Dialogue State Tracking](https://doi.org/10.18653/v1/2021.emnlp-main.593) |  | 0 | Sequence-to-sequence models have been applied to a wide variety of NLP tasks, but how to properly use them for dialogue state tracking has not been systematically investigated. In this paper, we study this problem from the perspectives of pre-training objectives as well as the formats of context... | Jeffrey Zhao, Mahdis Mahdieh, Ye Zhang, Yuan Cao, Yonghui Wu |  |
| 1063 |  |  [MS\^2: Multi-Document Summarization of Medical Studies](https://doi.org/10.18653/v1/2021.emnlp-main.594) |  | 0 | To assess the effectiveness of any medical intervention, researchers must conduct a time-intensive and manual literature review. NLP systems can help to automate or assist in parts of this expensive process. In support of this goal, we release MSˆ2 (Multi-Document Summarization of Medical Studies),... | Jay DeYoung, Iz Beltagy, Madeleine van Zuylen, Bailey Kuehl, Lucy Lu Wang |  |
| 1064 |  |  [CLIPScore: A Reference-free Evaluation Metric for Image Captioning](https://doi.org/10.18653/v1/2021.emnlp-main.595) |  | 0 | Image captioning has conventionally relied on reference-based automatic evaluations, where machine captions are compared against captions written by humans. This is in contrast to the reference-free manner in which humans assess caption quality. In this paper, we report the surprising empirical... | Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, Yejin Choi |  |
| 1065 |  |  [On the Challenges of Evaluating Compositional Explanations in Multi-Hop Inference: Relevance, Completeness, and Expert Ratings](https://doi.org/10.18653/v1/2021.emnlp-main.596) |  | 0 | Building compositional explanations requires models to combine two or more facts that, together, describe why the answer to a question is correct. Typically, these “multi-hop” explanations are evaluated relative to one (or a small number of) gold explanations. In this work, we show these... | Peter Jansen, Kelly J. Smith, Dan Moreno, Huitzilin Ortiz |  |
| 1066 |  |  [ESTER: A Machine Reading Comprehension Dataset for Reasoning about Event Semantic Relations](https://doi.org/10.18653/v1/2021.emnlp-main.597) |  | 0 | Understanding how events are semantically related to each other is the essence of reading comprehension. Recent event-centric reading comprehension datasets focus mostly on event arguments or temporal relations. While these tasks partially evaluate machines’ ability of narrative understanding,... | Rujun Han, IHung Hsu, Jiao Sun, Julia Baylon, Qiang Ning, Dan Roth, Nanyun Peng |  |
| 1067 |  |  [RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms](https://doi.org/10.18653/v1/2021.emnlp-main.598) |  | 0 | Pre-trained language models (PTLMs) have achieved impressive performance on commonsense inference benchmarks, but their ability to employ commonsense to make robust inferences, which is crucial for effective communications with humans, is debated. In the pursuit of advancing fluid human-AI... | Pei Zhou, Rahul Khanna, Seyeon Lee, Bill Yuchen Lin, Daniel Ho, Jay Pujara, Xiang Ren |  |
| 1068 |  |  [Compression, Transduction, and Creation: A Unified Framework for Evaluating Natural Language Generation](https://doi.org/10.18653/v1/2021.emnlp-main.599) |  | 0 | Natural language generation (NLG) spans a broad range of tasks, each of which serves for specific objectives and desires different properties of generated text. The complexity makes automatic evaluation of NLG particularly challenging. Previous work has typically focused on a single task and... | Mingkai Deng, Bowen Tan, Zhengzhong Liu, Eric P. Xing, Zhiting Hu |  |
| 1069 |  |  [MATE: Multi-view Attention for Table Transformer Efficiency](https://doi.org/10.18653/v1/2021.emnlp-main.600) |  | 0 | This work presents a sparse-attention Transformer architecture for modeling documents that contain large tables. Tables are ubiquitous on the web, and are rich in information. However, more than 20% of relational tables on the web have 20 or more rows (Cafarella et al., 2008), and these large... | Julian Martin Eisenschlos, Maharshi Gor, Thomas Müller, William W. Cohen |  |
| 1070 |  |  [Learning with Different Amounts of Annotation: From Zero to Many Labels](https://doi.org/10.18653/v1/2021.emnlp-main.601) |  | 0 | Training NLP systems typically assumes access to annotated data that has a single human label per example. Given imperfect labeling from annotators and inherent ambiguity of language, we hypothesize that single label is not sufficient to learn the spectrum of language interpretation. We explore new... | Shujian Zhang, Chengyue Gong, Eunsol Choi |  |
| 1071 |  |  [When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute](https://doi.org/10.18653/v1/2021.emnlp-main.602) |  | 0 | Large language models have become increasingly difficult to train because of the growing computation time and cost. In this work, we present SRU++, a highly-efficient architecture that combines fast recurrence and attention for sequence modeling. SRU++ exhibits strong modeling capacity and training... | Tao Lei |  |
| 1072 |  |  [Universal-KD: Attention-based Output-Grounded Intermediate Layer Knowledge Distillation](https://doi.org/10.18653/v1/2021.emnlp-main.603) |  | 0 | Intermediate layer matching is shown as an effective approach for improving knowledge distillation (KD). However, this technique applies matching in the hidden spaces of two different networks (i.e. student and teacher), which lacks clear interpretability. Moreover, intermediate layer KD cannot... | Yimeng Wu, Mehdi Rezagholizadeh, Abbas Ghaddar, Md. Akmal Haidar, Ali Ghodsi |  |
| 1073 |  |  [Highly Parallel Autoregressive Entity Linking with Discriminative Correction](https://doi.org/10.18653/v1/2021.emnlp-main.604) |  | 0 | Generative approaches have been recently shown to be effective for both Entity Disambiguation and Entity Linking (i.e., joint mention detection and disambiguation). However, the previously proposed autoregressive formulation for EL suffers from i) high computational cost due to a complex (deep)... | Nicola De Cao, Wilker Aziz, Ivan Titov |  |
| 1074 |  |  [Word-Level Coreference Resolution](https://doi.org/10.18653/v1/2021.emnlp-main.605) |  | 0 | Recent coreference resolution models rely heavily on span representations to find coreference links between word spans. As the number of spans is O(n2) in the length of text and the number of potential links is O(n4), various pruning techniques are necessary to make this approach computationally... | Vladimir Dobrovolskii |  |
| 1075 |  |  [A Secure and Efficient Federated Learning Framework for NLP](https://doi.org/10.18653/v1/2021.emnlp-main.606) |  | 0 | In this work, we consider the problem of designing secure and efficient federated learning (FL) frameworks for NLP. Existing solutions under this literature either consider a trusted aggregator or require heavy-weight cryptographic primitives, which makes the performance significantly degraded.... | Chenghong Wang, Jieren Deng, Xianrui Meng, Yijue Wang, Ji Li, Sheng Lin, Shuo Han, Fei Miao, Sanguthevar Rajasekaran, Caiwen Ding |  |
| 1076 |  |  [Controllable Semantic Parsing via Retrieval Augmentation](https://doi.org/10.18653/v1/2021.emnlp-main.607) |  | 0 | In practical applications of semantic parsing, we often want to rapidly change the behavior of the parser, such as enabling it to handle queries in a new domain, or changing its predictions on certain targeted queries. While we can introduce new training examples exhibiting the target behavior, a... | Panupong Pasupat, Yuan Zhang, Kelvin Guu |  |
| 1077 |  |  [Constrained Language Models Yield Few-Shot Semantic Parsers](https://doi.org/10.18653/v1/2021.emnlp-main.608) |  | 0 | We explore the use of large pretrained language models as few-shot semantic parsers. The goal in semantic parsing is to generate a structured meaning representation given a natural language input. However, language models are trained to generate natural language. To bridge the gap, we use language... | Richard Shin, Christopher H. Lin, Sam Thomson, Charles Chen, Subhro Roy, Emmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, Benjamin Van Durme |  |
| 1078 |  |  [ExplaGraphs: An Explanation Graph Generation Task for Structured Commonsense Reasoning](https://doi.org/10.18653/v1/2021.emnlp-main.609) |  | 0 | Recent commonsense-reasoning tasks are typically discriminative in nature, where a model answers a multiple-choice question for a certain context. Discriminative tasks are limiting because they fail to adequately evaluate the model’s ability to reason and explain predictions with underlying... | Swarnadeep Saha, Prateek Yadav, Lisa Bauer, Mohit Bansal |  |
| 1079 |  |  [Connect-the-Dots: Bridging Semantics between Words and Definitions via Aligning Word Sense Inventories](https://doi.org/10.18653/v1/2021.emnlp-main.610) |  | 0 | Word Sense Disambiguation (WSD) aims to automatically identify the exact meaning of one word according to its context. Existing supervised models struggle to make correct predictions on rare word senses due to limited training data and can only select the best definition sentence from one... | Wenlin Yao, Xiaoman Pan, Lifeng Jin, Jianshu Chen, Dian Yu, Dong Yu |  |
| 1080 |  |  [LM-Critic: Language Models for Unsupervised Grammatical Error Correction](https://doi.org/10.18653/v1/2021.emnlp-main.611) |  | 0 | Grammatical error correction (GEC) requires a set of labeled ungrammatical / grammatical sentence pairs for training, but obtaining such annotation can be prohibitively expensive. Recently, the Break-It-Fix-It (BIFI) framework has demonstrated strong results on learning to repair a broken program... | Michihiro Yasunaga, Jure Leskovec, Percy Liang |  |
| 1081 |  |  [Language-agnostic Representation from Multilingual Sentence Encoders for Cross-lingual Similarity Estimation](https://doi.org/10.18653/v1/2021.emnlp-main.612) |  | 0 | We propose a method to distill a language-agnostic meaning embedding from a multilingual sentence encoder. By removing language-specific information from the original embedding, we retrieve an embedding that fully represents the sentence’s meaning. The proposed method relies only on parallel... | Nattapong Tiyajamorn, Tomoyuki Kajiwara, Yuki Arase, Makoto Onizuka |  |
| 1082 |  |  [Classifying Dyads for Militarized Conflict Analysis](https://doi.org/10.18653/v1/2021.emnlp-main.613) |  | 0 | Understanding the origins of militarized conflict is a complex, yet important undertaking. Existing research seeks to build this understanding by considering bi-lateral relationships between entity pairs (dyadic causes) and multi-lateral relationships among multiple entities (systemic causes). The... | Niklas Stoehr, Lucas Torroba Hennigen, Samin Ahbab, Robert West, Ryan Cotterell |  |
| 1083 |  |  [Point-of-Interest Type Prediction using Text and Images](https://doi.org/10.18653/v1/2021.emnlp-main.614) |  | 0 | Point-of-interest (POI) type prediction is the task of inferring the type of a place from where a social media post was shared. Inferring a POI’s type is useful for studies in computational social science including sociolinguistics, geosemiotics, and cultural geography, and has applications in... | Danae Sánchez Villegas, Nikolaos Aletras |  |
| 1084 |  |  [Come hither or go away? Recognising pre-electoral coalition signals in the news](https://doi.org/10.18653/v1/2021.emnlp-main.615) |  | 0 | In this paper, we introduce the task of political coalition signal prediction from text, that is, the task of recognizing from the news coverage leading up to an election the (un)willingness of political parties to form a government coalition. We decompose our problem into two related, but distinct... | Ines Rehbein, Simone Paolo Ponzetto, Anna Adendorf, Oke Bahnsen, Lukas Stoetzer, Heiner Stuckenschmidt |  |
| 1085 |  |  [#HowYouTagTweets: Learning User Hashtagging Preferences via Personalized Topic Attention](https://doi.org/10.18653/v1/2021.emnlp-main.616) |  | 0 | Millions of hashtags are created on social media every day to cross-refer messages concerning similar topics. To help people find the topics they want to discuss, this paper characterizes a user’s hashtagging preferences via predicting how likely they will post with a hashtag. It is hypothesized... | Yuji Zhang, Yubo Zhang, Chunpu Xu, Jing Li, Ziyan Jiang, Baolin Peng |  |
| 1086 |  |  [Learning Neural Templates for Recommender Dialogue System](https://doi.org/10.18653/v1/2021.emnlp-main.617) |  | 0 | The task of Conversational Recommendation System (CRS), i.e., recommender dialog system, aims to recommend precise items to users through natural language interactions. Though recent end-to-end neural models have shown promising progress on this task, two key challenges still remain. First, the... | Zujie Liang, Huang Hu, Can Xu, Jian Miao, Yingying He, Yining Chen, Xiubo Geng, Fan Liang, Daxin Jiang |  |
| 1087 |  |  [Proxy Indicators for the Quality of Open-domain Dialogues](https://doi.org/10.18653/v1/2021.emnlp-main.618) |  | 0 | The automatic evaluation of open-domain dialogues remains a largely unsolved challenge. Despite the abundance of work done in the field, human judges have to evaluate dialogues’ quality. As a consequence, performing such evaluations at scale is usually expensive. This work investigates using a... | Rostislav Nedelchev, Jens Lehmann, Ricardo Usbeck |  |
| 1088 |  |  [$Q^2$: Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.619) |  | 0 | Neural knowledge-grounded generative models for dialogue often produce content that is factually inconsistent with the knowledge they rely on, making them unreliable and limiting their applicability. Inspired by recent work on evaluating factual consistency in abstractive summarization, we propose... | Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, Omri Abend |  |
| 1089 |  |  [Knowledge-Aware Graph-Enhanced GPT-2 for Dialogue State Tracking](https://doi.org/10.18653/v1/2021.emnlp-main.620) |  | 0 | Dialogue State Tracking is central to multi-domain task-oriented dialogue systems, responsible for extracting information from user utterances. We present a novel hybrid architecture that augments GPT-2 with representations derived from Graph Attention Networks in such a way to allow causal,... | Weizhe Lin, BoHsiang Tseng, Bill Byrne |  |
| 1090 |  |  [A Collaborative Multi-agent Reinforcement Learning Framework for Dialog Action Decomposition](https://doi.org/10.18653/v1/2021.emnlp-main.621) |  | 0 | Most reinforcement learning methods for dialog policy learning train a centralized agent that selects a predefined joint action concatenating domain name, intent type, and slot name. The centralized dialog agent suffers from a great many user-agent interaction requirements due to the large action... | Huimin Wang, KamFai Wong |  |
| 1091 |  |  [Zero-Shot Dialogue State Tracking via Cross-Task Transfer](https://doi.org/10.18653/v1/2021.emnlp-main.622) |  | 0 | Zero-shot transfer learning for dialogue state tracking (DST) enables us to handle a variety of task-oriented dialogue domains without the expense of collecting in-domain data. In this work, we propose to transfer the cross-task knowledge from general question answering (QA) corpora for the... | Zhaojiang Lin, Bing Liu, Andrea Madotto, Seungwhan Moon, Zhenpeng Zhou, Paul A. Crook, Zhiguang Wang, Zhou Yu, Eunjoon Cho, Rajen Subba, Pascale Fung |  |
| 1092 |  |  [Uncertainty Measures in Neural Belief Tracking and the Effects on Dialogue Policy Performance](https://doi.org/10.18653/v1/2021.emnlp-main.623) |  | 0 | The ability to identify and resolve uncertainty is crucial for the robustness of a dialogue system. Indeed, this has been confirmed empirically on systems that utilise Bayesian approaches to dialogue belief tracking. However, such systems consider only confidence estimates and have difficulty... | Carel van Niekerk, Andrey Malinin, Christian Geishauser, Michael Heck, HsienChin Lin, Nurul Lubis, Shutong Feng, Milica Gasic |  |
| 1093 |  |  [Dynamic Forecasting of Conversation Derailment](https://doi.org/10.18653/v1/2021.emnlp-main.624) |  | 0 | Online conversations can sometimes take a turn for the worse, either due to systematic cultural differences, accidental misunderstandings, or mere malice. Automatically forecasting derailment in public online conversations provides an opportunity to take early action to moderate it. Previous work... | Yova Kementchedjhieva, Anders Søgaard |  |
| 1094 |  |  [A Semantic Filter Based on Relations for Knowledge Graph Completion](https://doi.org/10.18653/v1/2021.emnlp-main.625) |  | 0 | Knowledge graph embedding, representing entities and relations in the knowledge graphs with high-dimensional vectors, has made significant progress in link prediction. More researchers have explored the representational capabilities of models in recent years. That is, they investigate better... | Zongwei Liang, Junan Yang, Hui Liu, KeJu Huang |  |
| 1095 |  |  [AdapterDrop: On the Efficiency of Adapters in Transformers](https://doi.org/10.18653/v1/2021.emnlp-main.626) |  | 0 | Transformer models are expensive to fine-tune, slow for inference, and have large storage requirements. Recent approaches tackle these shortcomings by training smaller models, dynamically reducing the model size, and by training light-weight adapters. In this paper, we propose AdapterDrop, removing... | Andreas Rücklé, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, Iryna Gurevych |  |
| 1096 |  |  [Understanding and Overcoming the Challenges of Efficient Transformer Quantization](https://doi.org/10.18653/v1/2021.emnlp-main.627) |  | 0 | Transformer-based architectures have become the de-facto standard models for a wide range of Natural Language Processing tasks. However, their memory footprint and high latency are prohibitive for efficient deployment and inference on resource-limited devices. In this work, we explore quantization... | Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort |  |
| 1097 |  |  [CAPE: Context-Aware Private Embeddings for Private Language Learning](https://doi.org/10.18653/v1/2021.emnlp-main.628) |  | 0 | Neural language models have contributed to state-of-the-art results in a number of downstream applications including sentiment analysis, intent classification and others. However, obtaining text representations or embeddings using these models risks encoding personally identifiable information... | Richard Plant, Dimitra Gkatzia, Valerio Giuffrida |  |
| 1098 |  |  [Text Detoxification using Large Pre-trained Neural Models](https://doi.org/10.18653/v1/2021.emnlp-main.629) |  | 0 | We present two novel unsupervised methods for eliminating toxicity in text. Our first method combines two recent ideas: (1) guidance of the generation process with small style-conditional language models and (2) use of paraphrasing models to perform style transfer. We use a well-performing... | David Dale, Anton Voronov, Daryna Dementieva, Varvara Logacheva, Olga Kozlova, Nikita Semenov, Alexander Panchenko |  |
| 1099 |  |  [Document-Level Text Simplification: Dataset, Criteria and Baseline](https://doi.org/10.18653/v1/2021.emnlp-main.630) |  | 0 | Text simplification is a valuable technique. However, current research is limited to sentence simplification. In this paper, we define and investigate a new task of document-level text simplification, which aims to simplify a document consisting of multiple sentences. Based on Wikipedia dumps, we... | Renliang Sun, Hanqi Jin, Xiaojun Wan |  |
| 1100 |  |  [A Bag of Tricks for Dialogue Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.631) |  | 0 | Dialogue summarization comes with its own peculiar challenges as opposed to news or scientific articles summarization. In this work, we explore four different challenges of the task: handling and differentiating parts of the dialogue belonging to multiple speakers, negation understanding, reasoning... | Muhammad Khalifa, Miguel Ballesteros, Kathleen R. McKeown |  |
| 1101 |  |  [Paraphrasing Compound Nominalizations](https://doi.org/10.18653/v1/2021.emnlp-main.632) |  | 0 | A nominalization uses a deverbal noun to describe an event associated with its underlying verb. Commonly found in academic and formal texts, nominalizations can be difficult to interpret because of ambiguous semantic relations between the deverbal noun and its arguments. Our goal is to interpret... | John Lee, Ho Hung Lim, Carol Webster |  |
| 1102 |  |  [Data-QuestEval: A Referenceless Metric for Data-to-Text Semantic Evaluation](https://doi.org/10.18653/v1/2021.emnlp-main.633) |  | 0 | QuestEval is a reference-less metric used in text-to-text tasks, that compares the generated summaries directly to the source text, by automatically asking and answering questions. Its adaptation to Data-to-Text tasks is not straightforward, as it requires multimodal Question Generation and... | Clément Rebuffel, Thomas Scialom, Laure Soulier, Benjamin Piwowarski, Sylvain Lamprier, Jacopo Staiano, Geoffrey Scoutheeten, Patrick Gallinari |  |
| 1103 |  |  [Low-Rank Subspaces for Unsupervised Entity Linking](https://doi.org/10.18653/v1/2021.emnlp-main.634) |  | 0 | Entity linking is an important problem with many applications. Most previous solutions were designed for settings where annotated training data is available, which is, however, not the case in numerous domains. We propose a light-weight and scalable entity linking method, Eigenthemes, that relies... | Akhil Arora, Alberto GarcíaDurán, Robert West |  |
| 1104 |  |  [TDEER: An Efficient Translating Decoding Schema for Joint Extraction of Entities and Relations](https://doi.org/10.18653/v1/2021.emnlp-main.635) |  | 0 | Joint extraction of entities and relations from unstructured texts to form factual triples is a fundamental task of constructing a Knowledge Base (KB). A common method is to decode triples by predicting entity pairs to obtain the corresponding relation. However, it is still challenging to handle... | Xianming Li, Xiaotian Luo, Chenghao Dong, Daichuan Yang, Beidi Luan, Zhen He |  |
| 1105 |  |  [Extracting Event Temporal Relations via Hyperbolic Geometry](https://doi.org/10.18653/v1/2021.emnlp-main.636) |  | 0 | Detecting events and their evolution through time is a crucial task in natural language understanding. Recent neural approaches to event temporal relation extraction typically map events to embeddings in the Euclidean space and train a classifier to detect temporal relations between event pairs.... | Xingwei Tan, Gabriele Pergola, Yulan He |  |
| 1106 |  |  [Honey or Poison? Solving the Trigger Curse in Few-shot Event Detection via Causal Intervention](https://doi.org/10.18653/v1/2021.emnlp-main.637) |  | 0 | Event detection has long been troubled by the trigger curse: overfitting the trigger will harm the generalization ability while underfitting it will hurt the detection performance. This problem is even more severe in few-shot scenario. In this paper, we identify and solve the trigger curse problem... | Jiawei Chen, Hongyu Lin, Xianpei Han, Le Sun |  |
| 1107 |  |  [Back to the Basics: A Quantitative Analysis of Statistical and Graph-Based Term Weighting Schemes for Keyword Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.638) |  | 0 | Term weighting schemes are widely used in Natural Language Processing and Information Retrieval. In particular, term weighting is the basis for keyword extraction. However, there are relatively few evaluation studies that shed light about the strengths and shortcomings of each weighting scheme. In... | Asahi Ushio, Federico Liberatore, José CamachoCollados |  |
| 1108 |  |  [Time-dependent Entity Embedding is not All You Need: A Re-evaluation of Temporal Knowledge Graph Completion Models under a Unified Framework](https://doi.org/10.18653/v1/2021.emnlp-main.639) |  | 0 | Various temporal knowledge graph (KG) completion models have been proposed in the recent literature. The models usually contain two parts, a temporal embedding layer and a score function derived from existing static KG modeling approaches. Since the approaches differ along several dimensions,... | Zhen Han, Gengyuan Zhang, Yunpu Ma, Volker Tresp |  |
| 1109 |  |  [Matching-oriented Embedding Quantization For Ad-hoc Retrieval](https://doi.org/10.18653/v1/2021.emnlp-main.640) |  | 0 | Product quantization (PQ) is a widely used technique for ad-hoc retrieval. Recent studies propose supervised PQ, where the embedding and quantization models can be jointly trained with supervised learning. However, there is a lack of appropriate formulation of the joint training objective; thus,... | Shitao Xiao, Zheng Liu, Yingxia Shao, Defu Lian, Xing Xie |  |
| 1110 |  |  [Efficient Mind-Map Generation via Sequence-to-Graph and Reinforced Graph Refinement](https://doi.org/10.18653/v1/2021.emnlp-main.641) |  | 0 | A mind-map is a diagram that represents the central concept and key ideas in a hierarchical way. Converting plain text into a mind-map will reveal its key semantic structure and be easier to understand. Given a document, the existing automatic mind-map generation method extracts the relationships... | Mengting Hu, Honglei Guo, Shiwan Zhao, Hang Gao, Zhong Su |  |
| 1111 |  |  [Deep Attention Diffusion Graph Neural Networks for Text Classification](https://doi.org/10.18653/v1/2021.emnlp-main.642) |  | 0 | Text classification is a fundamental task with broad applications in natural language processing. Recently, graph neural networks (GNNs) have attracted much attention due to their powerful representation ability. However, most existing methods for text classification based on GNNs consider only... | Yonghao Liu, Renchu Guan, Fausto Giunchiglia, Yanchun Liang, Xiaoyue Feng |  |
| 1112 |  |  [Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution](https://doi.org/10.18653/v1/2021.emnlp-main.643) |  | 0 | Multi-label text classification is a challenging task because it requires capturing label dependencies. It becomes even more challenging when class distribution is long-tailed. Resampling and re-weighting are common approaches used for addressing the class imbalance problem, however, they are not... | Yi Huang, Buse Giledereli, Abdullatif Köksal, Arzucan Özgür, Elif Ozkirimli |  |
| 1113 |  |  [Bayesian Topic Regression for Causal Inference](https://doi.org/10.18653/v1/2021.emnlp-main.644) |  | 0 | Causal inference using observational text data is becoming increasingly popular in many research areas. This paper presents the Bayesian Topic Regression (BTR) model that uses both text and numerical information to model an outcome variable. It allows estimation of both discrete and continuous... | Maximilian Ahrens, Julian Ashwin, JanPeter Calliess, Vu Nguyen |  |
| 1114 |  |  [Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience](https://doi.org/10.18653/v1/2021.emnlp-main.645) |  | 0 | Pretrained transformer-based models such as BERT have demonstrated state-of-the-art predictive performance when adapted into a range of natural language processing tasks. An open problem is how to improve the faithfulness of explanations (rationales) for the predictions of these models. In this... | George Chrysostomou, Nikolaos Aletras |  |
| 1115 |  |  [What's in Your Head? Emergent Behaviour in Multi-Task Transformer Models](https://doi.org/10.18653/v1/2021.emnlp-main.646) |  | 0 | The primary paradigm for multi-task training in natural language processing is to represent the input with a shared pre-trained language model, and add a small, thin network (head) per task. Given an input, a target head is the head that is selected for outputting the final prediction. In this... | Mor Geva, Uri Katz, Aviv BenArie, Jonathan Berant |  |
| 1116 |  |  [Don't Search for a Search Method - Simple Heuristics Suffice for Adversarial Text Attacks](https://doi.org/10.18653/v1/2021.emnlp-main.647) |  | 0 | Recently more attention has been given to adversarial attacks on neural networks for natural language processing (NLP). A central research topic has been the investigation of search algorithms and search constraints, accompanied by benchmark algorithms and tasks. We implement an algorithm inspired... | Nathaniel Berger, Stefan Riezler, Sebastian Ebert, Artem Sokolov |  |
| 1117 |  |  [Adversarial Attacks on Knowledge Graph Embeddings via Instance Attribution Methods](https://doi.org/10.18653/v1/2021.emnlp-main.648) |  | 0 | Despite the widespread use of Knowledge Graph Embeddings (KGE), little is known about the security vulnerabilities that might disrupt their intended behaviour. We study data poisoning attacks against KGE models for link prediction. These attacks craft adversarial additions or deletions at training... | Peru Bhardwaj, John D. Kelleher, Luca Costabello, Declan O'Sullivan |  |
| 1118 |  |  [Locke's Holiday: Belief Bias in Machine Reading](https://doi.org/10.18653/v1/2021.emnlp-main.649) |  | 0 | I highlight a simple failure mode of state-of-the-art machine reading systems: when contexts do not align with commonly shared beliefs. For example, machine reading systems fail to answer What did Elizabeth want? correctly in the context of ‘My kingdom for a cough drop, cried Queen Elizabeth.’... | Anders Søgaard |  |
| 1119 |  |  [Sequence Length is a Domain: Length-based Overfitting in Transformer Models](https://doi.org/10.18653/v1/2021.emnlp-main.650) |  | 0 | Transformer-based sequence-to-sequence architectures, while achieving state-of-the-art results on a large number of NLP tasks, can still suffer from overfitting during training. In practice, this is usually countered either by applying regularization methods (e.g. dropout, L2-regularization) or by... | Dusan Varis, Ondrej Bojar |  |
| 1120 |  |  [Contrasting Human- and Machine-Generated Word-Level Adversarial Examples for Text Classification](https://doi.org/10.18653/v1/2021.emnlp-main.651) |  | 0 | Research shows that natural language processing models are generally considered to be vulnerable to adversarial attacks; but recent work has drawn attention to the issue of validating these adversarial inputs against certain criteria (e.g., the preservation of semantics and grammaticality).... | Maximilian Mozes, Max Bartolo, Pontus Stenetorp, Bennett Kleinberg, Lewis D. Griffin |  |
| 1121 |  |  [Is Information Density Uniform in Task-Oriented Dialogues?](https://doi.org/10.18653/v1/2021.emnlp-main.652) |  | 0 | The Uniform Information Density principle states that speakers plan their utterances to reduce fluctuations in the density of the information transmitted. In this paper, we test whether, and within which contextual units this principle holds in task-oriented dialogues. We show that there is... | Mario Giulianelli, Arabella Sinclair, Raquel Fernández |  |
| 1122 |  |  [On Homophony and Rényi Entropy](https://doi.org/10.18653/v1/2021.emnlp-main.653) |  | 0 | Homophony’s widespread presence in natural languages is a controversial topic. Recent theories of language optimality have tried to justify its prevalence, despite its negative effects on cognitive processing time, e.g., Piantadosi et al. (2012) argued homophony enables the reuse of efficient... | Tiago Pimentel, Clara Meister, Simone Teufel, Ryan Cotterell |  |
| 1123 |  |  [Synthetic Textual Features for the Large-Scale Detection of Basic-level Categories in English and Mandarin](https://doi.org/10.18653/v1/2021.emnlp-main.654) |  | 0 | Basic-level categories (BLC) are an important psycholinguistic concept introduced by Rosch et al. (1976); they are defined as the most inclusive categories for which a concrete mental image of the category as a whole can be formed, and also as those categories which are acquired early in life.... | Yiwen Chen, Simone Teufel |  |
| 1124 |  |  [TimeTraveler: Reinforcement Learning for Temporal Knowledge Graph Forecasting](https://doi.org/10.18653/v1/2021.emnlp-main.655) |  | 0 | Temporal knowledge graph (TKG) reasoning is a crucial task that has gained increasing research interest in recent years. Most existing methods focus on reasoning at past timestamps to complete the missing facts, and there are only a few works of reasoning on known TKGs to forecast future facts.... | Haohai Sun, Jialun Zhong, Yunpu Ma, Zhen Han, Kun He |  |
| 1125 |  |  [Code-switched inspired losses for spoken dialog representations](https://doi.org/10.18653/v1/2021.emnlp-main.656) |  | 0 | Spoken dialogue systems need to be able to handle both multiple languages and multilinguality inside a conversation (e.g in case of code-switching). In this work, we introduce new pretraining losses tailored to learn generic multilingual spoken dialogue representations. The goal of these losses is... | Pierre Colombo, Emile Chapuis, Matthieu Labeau, Chloé Clavel |  |
| 1126 |  |  [BiQUE: Biquaternionic Embeddings of Knowledge Graphs](https://doi.org/10.18653/v1/2021.emnlp-main.657) |  | 0 | Knowledge graph embeddings (KGEs) compactly encode multi-relational knowledge graphs (KGs). Existing KGE models rely on geometric operations to model relational patterns. Euclidean (circular) rotation is useful for modeling patterns such as symmetry, but cannot represent hierarchical semantics. In... | Jia Guo, Stanley Kok |  |
| 1127 |  |  [Learning Neural Ordinary Equations for Forecasting Future Links on Temporal Knowledge Graphs](https://doi.org/10.18653/v1/2021.emnlp-main.658) |  | 0 | There has been an increasing interest in inferring future links on temporal knowledge graphs (KG). While links on temporal KGs vary continuously over time, the existing approaches model the temporal KGs in discrete state spaces. To this end, we propose a novel continuum model by extending the idea... | Zhen Han, Zifeng Ding, Yunpu Ma, Yujia Gu, Volker Tresp |  |
| 1128 |  |  [RAP: Robustness-Aware Perturbations for Defending against Backdoor Attacks on NLP Models](https://doi.org/10.18653/v1/2021.emnlp-main.659) |  | 0 | Backdoor attacks, which maliciously control a well-trained model’s outputs of the instances with specific triggers, are recently shown to be serious threats to the safety of reusing deep neural networks (DNNs). In this work, we propose an efficient online defense mechanism based on robustness-aware... | Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, Xu Sun |  |
| 1129 |  |  [FAME: Feature-Based Adversarial Meta-Embeddings for Robust Input Representations](https://doi.org/10.18653/v1/2021.emnlp-main.660) |  | 0 | Combining several embeddings typically improves performance in downstream tasks as different embeddings encode different information. It has been shown that even models using embeddings from transformers still benefit from the inclusion of standard word embeddings. However, the combination of... | Lukas Lange, Heike Adel, Jannik Strötgen, Dietrich Klakow |  |
| 1130 |  |  [A Strong Baseline for Query Efficient Attacks in a Black Box Setting](https://doi.org/10.18653/v1/2021.emnlp-main.661) |  | 0 | Existing black box search methods have achieved high success rate in generating adversarial attacks against NLP models. However, such search methods are inefficient as they do not consider the amount of queries required to generate adversarial attacks. Also, prior attacks do not maintain a... | Rishabh Maheshwary, Saket Maheshwary, Vikram Pudi |  |
| 1131 |  |  [Machine Translation Decoding beyond Beam Search](https://doi.org/10.18653/v1/2021.emnlp-main.662) |  | 0 | Beam search is the go-to method for decoding auto-regressive machine translation models. While it yields consistent improvements in terms of BLEU, it is only concerned with finding outputs with high model likelihood, and is thus agnostic to whatever end metric or score practitioners care about. Our... | Rémi Leblond, JeanBaptiste Alayrac, Laurent Sifre, Miruna Pislar, JeanBaptiste Lespiau, Ioannis Antonoglou, Karen Simonyan, Oriol Vinyals |  |
| 1132 |  |  [Document Graph for Neural Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.663) |  | 0 | Previous works have shown that contextual information can improve the performance of neural machine translation (NMT). However, most existing document-level NMT methods failed to leverage contexts beyond a few set of previous sentences. How to make use of the whole document as global contexts is... | Mingzhou Xu, Liangyou Li, Derek F. Wong, Qun Liu, Lidia S. Chao |  |
| 1133 |  |  [An Empirical Investigation of Word Alignment Supervision for Zero-Shot Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.664) |  | 0 | Zero-shot translations is a fascinating feature of Multilingual Neural Machine Translation (MNMT) systems. These MNMT models are usually trained on English-centric data, i.e. English either as the source or target language, and with a language label prepended to the input indicating the target... | Alessandro Raganato, Raúl Vázquez, Mathias Creutz, Jörg Tiedemann |  |
| 1134 |  |  [Graph Algorithms for Multiparallel Word Alignment](https://doi.org/10.18653/v1/2021.emnlp-main.665) |  | 0 | With the advent of end-to-end deep learning approaches in machine translation, interest in word alignments initially decreased; however, they have again become a focus of research more recently. Alignments are useful for typological research, transferring formatting like markup to translated texts,... | Ayyoob Imani, Masoud Jalili Sabet, Lutfi Kerem Senel, Philipp Dufter, François Yvon, Hinrich Schütze |  |
| 1135 |  |  [Improving the Quality Trade-Off for Neural Machine Translation Multi-Domain Adaptation](https://doi.org/10.18653/v1/2021.emnlp-main.666) |  | 0 | Building neural machine translation systems to perform well on a specific target domain is a well-studied problem. Optimizing system performance for multiple, diverse target domains however remains a challenge. We study this problem in an adaptation setting where the goal is to preserve the... | Eva Hasler, Tobias Domhan, Jonay Trénous, Ke Tran, Bill Byrne, Felix Hieber |  |
| 1136 |  |  [Language Modeling, Lexical Translation, Reordering: The Training Process of NMT through the Lens of Classical SMT](https://doi.org/10.18653/v1/2021.emnlp-main.667) |  | 0 | Differently from the traditional statistical MT that decomposes the translation task into distinct separately learned components, neural machine translation uses a single neural network to model the entire translation process. Despite neural machine translation being de-facto standard, it is still... | Elena Voita, Rico Sennrich, Ivan Titov |  |
| 1137 |  |  [Effective Fine-Tuning Methods for Cross-lingual Adaptation](https://doi.org/10.18653/v1/2021.emnlp-main.668) |  | 0 | Large scale multilingual pre-trained language models have shown promising results in zero- and few-shot cross-lingual tasks. However, recent studies have shown their lack of generalizability when the languages are structurally dissimilar. In this work, we propose a novel fine-tuning method based on... | Tao Yu, Shafiq R. Joty |  |
| 1138 |  |  [Rethinking Data Augmentation for Low-Resource Neural Machine Translation: A Multi-Task Learning Approach](https://doi.org/10.18653/v1/2021.emnlp-main.669) |  | 0 | In the context of neural machine translation, data augmentation (DA) techniques may be used for generating additional training samples when the available parallel data are scarce. Many DA approaches aim at expanding the support of the empirical data distribution by generating new sentence pairs... | Víctor M. SánchezCartagena, Miquel EsplàGomis, Juan Antonio PérezOrtiz, Felipe SánchezMartínez |  |
| 1139 |  |  [Wino-X: Multilingual Winograd Schemas for Commonsense Reasoning and Coreference Resolution](https://doi.org/10.18653/v1/2021.emnlp-main.670) |  | 0 | Winograd schemas are a well-established tool for evaluating coreference resolution (CoR) and commonsense reasoning (CSR) capabilities of computational models. So far, schemas remained largely confined to English, limiting their utility in multilingual settings. This work presents Wino-X, a parallel... | Denis Emelin, Rico Sennrich |  |
| 1140 |  |  [One Source, Two Targets: Challenges and Rewards of Dual Decoding](https://doi.org/10.18653/v1/2021.emnlp-main.671) |  | 0 | Machine translation is generally understood as generating one target text from an input source document. In this paper, we consider a stronger requirement: to jointly generate two texts so that each output side effectively depends on the other. As we discuss, such a device serves several practical... | Jitao Xu, François Yvon |  |
| 1141 |  |  [Discrete and Soft Prompting for Multilingual Models](https://doi.org/10.18653/v1/2021.emnlp-main.672) |  | 0 | It has been shown for English that discrete and soft prompting perform strongly in few-shot learning with pretrained language models (PLMs). In this paper, we show that discrete and soft prompting perform better than finetuning in multilingual cases: Crosslingual transfer and in-language training... | Mengjie Zhao, Hinrich Schütze |  |
| 1142 |  |  [Vision Matters When It Should: Sanity Checking Multimodal Machine Translation Models](https://doi.org/10.18653/v1/2021.emnlp-main.673) |  | 0 | Multimodal machine translation (MMT) systems have been shown to outperform their text-only neural machine translation (NMT) counterparts when visual context is available. However, recent studies have also shown that the performance of MMT models is only marginally impacted when the associated image... | Jiaoda Li, Duygu Ataman, Rico Sennrich |  |
| 1143 |  |  [Efficient Inference for Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2021.emnlp-main.674) |  | 0 | Multilingual NMT has become an attractive solution for MT deployment in production. But to match bilingual quality, it comes at the cost of larger and slower models. In this work, we consider several ways to make multilingual NMT faster at inference without degrading its quality. We experiment with... | Alexandre Berard, Dain Lee, Stéphane Clinchant, Kweon Woo Jung, Vassilina Nikoulina |  |
| 1144 |  |  [Role of Language Relatedness in Multilingual Fine-tuning of Language Models: A Case Study in Indo-Aryan Languages](https://doi.org/10.18653/v1/2021.emnlp-main.675) |  | 0 | We explore the impact of leveraging the relatedness of languages that belong to the same family in NLP models using multilingual fine-tuning. We hypothesize and validate that multilingual fine-tuning of pre-trained language models can yield better performance on downstream NLP applications,... | Tejas I. Dhamecha, V. Rudra Murthy, Samarth Bharadwaj, Karthik Sankaranarayanan, Pushpak Bhattacharyya |  |
| 1145 |  |  [Comparing Feature-Engineering and Feature-Learning Approaches for Multilingual Translationese Classification](https://doi.org/10.18653/v1/2021.emnlp-main.676) |  | 0 | Traditional hand-crafted linguistically-informed features have often been used for distinguishing between translated and original non-translated texts. By contrast, to date, neural architectures without manual feature engineering have been less explored for this task. In this work, we (i) compare... | Daria Pylypenko, Kwabena AmponsahKaakyire, Koel Dutta Chowdhury, Josef van Genabith, Cristina EspañaBonet |  |
| 1146 |  |  [Multi-Sentence Resampling: A Simple Approach to Alleviate Dataset Length Bias and Beam-Search Degradation](https://doi.org/10.18653/v1/2021.emnlp-main.677) |  | 0 | Neural Machine Translation (NMT) is known to suffer from a beam-search problem: after a certain point, increasing beam size causes an overall drop in translation quality. This effect is especially pronounced for long sentences. While much work was done analyzing this phenomenon, primarily for... | Ivan Provilkov, Andrey Malinin |  |
| 1147 |  |  [Cross-Policy Compliance Detection via Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.678) |  | 0 | Policy compliance detection is the task of ensuring that a scenario conforms to a policy (e.g. a claim is valid according to government rules or a post in an online platform conforms to community guidelines). This task has been previously instantiated as a form of textual entailment, which results... | Marzieh Saeidi, Majid Yazdani, Andreas Vlachos |  |
| 1148 |  |  [Meta-LMTC: Meta-Learning for Large-Scale Multi-Label Text Classification](https://doi.org/10.18653/v1/2021.emnlp-main.679) |  | 0 | Large-scale multi-label text classification (LMTC) tasks often face long-tailed label distributions, where many labels have few or even no training instances. Although current methods can exploit prior knowledge to handle these few/zero-shot labels, they neglect the meta-knowledge contained in the... | Ran Wang, Xi'ao Su, Siyu Long, Xinyu Dai, Shujian Huang, Jiajun Chen |  |
| 1149 |  |  [Unsupervised Multi-View Post-OCR Error Correction With Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.680) |  | 0 | We investigate post-OCR correction in a setting where we have access to different OCR views of the same document. The goal of this study is to understand if a pretrained language model (LM) can be used in an unsupervised way to reconcile the different OCR views such that their combination contains... | Harsh Gupta, Luciano Del Corro, Samuel Broscheit, Johannes Hoffart, Eliot Brenner |  |
| 1150 |  |  [Parallel Refinements for Lexically Constrained Text Generation with BART](https://doi.org/10.18653/v1/2021.emnlp-main.681) |  | 0 | Lexically constrained text generation aims to control the generated text by incorporating certain pre-specified keywords into the output. Previous work injects lexical constraints into the output by controlling the decoding process or refining the candidate output iteratively, which tends to... | Xingwei He |  |
| 1151 |  |  [BERT-Beta: A Proactive Probabilistic Approach to Text Moderation](https://doi.org/10.18653/v1/2021.emnlp-main.682) |  | 0 | Text moderation for user generated content, which helps to promote healthy interaction among users, has been widely studied and many machine learning models have been proposed. In this work, we explore an alternative perspective by augmenting reactive reviews with proactive forecasting.... | Fei Tan, Yifan Hu, Kevin Yen, Changwei Hu |  |
| 1152 |  |  [STaCK: Sentence Ordering with Temporal Commonsense Knowledge](https://doi.org/10.18653/v1/2021.emnlp-main.683) |  | 0 | Sentence order prediction is the task of finding the correct order of sentences in a randomly ordered document. Correctly ordering the sentences requires an understanding of coherence with respect to the chronological sequence of events described in the text. Document-level contextual understanding... | Deepanway Ghosal, Navonil Majumder, Rada Mihalcea, Soujanya Poria |  |
| 1153 |  |  [Preventing Author Profiling through Zero-Shot Multilingual Back-Translation](https://doi.org/10.18653/v1/2021.emnlp-main.684) |  | 0 | Documents as short as a single sentence may inadvertently reveal sensitive information about their authors, including e.g. their gender or ethnicity. Style transfer is an effective way of transforming texts in order to remove any information that enables author profiling. However, for a number of... | David Ifeoluwa Adelani, Miaoran Zhang, Xiaoyu Shen, Ali Davody, Thomas Kleinbauer, Dietrich Klakow |  |
| 1154 |  |  [CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation](https://doi.org/10.18653/v1/2021.emnlp-main.685) |  | 0 | Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training... | Yue Wang, Weishi Wang, Shafiq R. Joty, Steven C. H. Hoi |  |
| 1155 |  |  [Detect and Classify - Joint Span Detection and Classification for Health Outcomes](https://doi.org/10.18653/v1/2021.emnlp-main.686) |  | 0 | A health outcome is a measurement or an observation used to capture and assess the effect of a treatment. Automatic detection of health outcomes from text would undoubtedly speed up access to evidence necessary in healthcare decision making. Prior work on outcome detection has modelled this task as... | Micheal Abaho, Danushka Bollegala, Paula Williamson, Susanna Dodd |  |
| 1156 |  |  [Multi-Class Grammatical Error Detection for Correction: A Tale of Two Systems](https://doi.org/10.18653/v1/2021.emnlp-main.687) |  | 0 | In this paper, we show how a multi-class grammatical error detection (GED) system can be used to improve grammatical error correction (GEC) for English. Specifically, we first develop a new state-of-the-art binary detection system based on pre-trained ELECTRA, and then extend it to multi-class... | Zheng Yuan, Shiva Taslimipoor, Christopher Davis, Christopher Bryant |  |
| 1157 |  |  [Towards Zero-shot Commonsense Reasoning with Self-supervised Refinement of Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.688) |  | 0 | Can we get existing language models and refine them for zero-shot commonsense reasoning? This paper presents an initial study exploring the feasibility of zero-shot commonsense reasoning for the Winograd Schema Challenge by formulating the task as self-supervised refinement of a pre-trained... | Tassilo Klein, Moin Nabi |  |
| 1158 |  |  [To Share or not to Share: Predicting Sets of Sources for Model Transfer Learning](https://doi.org/10.18653/v1/2021.emnlp-main.689) |  | 0 | In low-resource settings, model transfer can help to overcome a lack of labeled data for many tasks and domains. However, predicting useful transfer sources is a challenging problem, as even the most similar sources might lead to unexpected negative transfer results. Thus, ranking methods based on... | Lukas Lange, Jannik Strötgen, Heike Adel, Dietrich Klakow |  |
| 1159 |  |  [Self-Supervised Detection of Contextual Synonyms in a Multi-Class Setting: Phenotype Annotation Use Case](https://doi.org/10.18653/v1/2021.emnlp-main.690) |  | 0 | Contextualised word embeddings is a powerful tool to detect contextual synonyms. However, most of the current state-of-the-art (SOTA) deep learning concept extraction methods remain supervised and underexploit the potential of the context. In this paper, we propose a self-supervised pre-training... | Jingqing Zhang, Luis Bolanos, Tong Li, Ashwani Tanwar, Guilherme Freire, Xian Yang, Julia Ive, Vibhor Gupta, Yike Guo |  |
| 1160 |  |  [ClauseRec: A Clause Recommendation Framework for AI-aided Contract Authoring](https://doi.org/10.18653/v1/2021.emnlp-main.691) |  | 0 | Contracts are a common type of legal document that frequent in several day-to-day business workflows. However, there has been very limited NLP research in processing such documents, and even lesser in generating them. These contracts are made up of clauses, and the unique nature of these clauses... | Vinay Aggarwal, Aparna Garimella, Balaji Vasan Srinivasan, Anandhavelu Natarajan, Rajiv Jain |  |
| 1161 |  |  [Finnish Dialect Identification: The Effect of Audio and Text](https://doi.org/10.18653/v1/2021.emnlp-main.692) |  | 0 | Finnish is a language with multiple dialects that not only differ from each other in terms of accent (pronunciation) but also in terms of morphological forms and lexical choice. We present the first approach to automatically detect the dialect of a speaker based on a dialect transcript and... | Mika Hämäläinen, Khalid Alnajjar, Niko Partanen, Jack Rueter |  |
| 1162 |  |  [English Machine Reading Comprehension Datasets: A Survey](https://doi.org/10.18653/v1/2021.emnlp-main.693) |  | 0 | This paper surveys 60 English Machine Reading Comprehension datasets, with a view to providing a convenient resource for other researchers interested in this problem. We categorize the datasets according to their question and answer form and compare them across various dimensions including size,... | Daria Dzendzik, Jennifer Foster, Carl Vogel |  |
| 1163 |  |  [Expanding End-to-End Question Answering on Differentiable Knowledge Graphs with Intersection](https://doi.org/10.18653/v1/2021.emnlp-main.694) |  | 0 | End-to-end question answering using a differentiable knowledge graph is a promising technique that requires only weak supervision, produces interpretable results, and is fully differentiable. Previous implementations of this technique (Cohen et al, 2020) have focused on single-entity questions... | Priyanka Sen, Armin Oliya, Amir Saffari |  |
| 1164 |  |  [Structured Context and High-Coverage Grammar for Conversational Question Answering over Knowledge Graphs](https://doi.org/10.18653/v1/2021.emnlp-main.695) |  | 0 | We tackle the problem of weakly-supervised conversational Question Answering over large Knowledge Graphs using a neural semantic parsing approach. We introduce a new Logical Form (LF) grammar that can model a wide range of queries on the graph while remaining sufficiently simple to generate... | Pierre Marion, Pawel Krzysztof Nowak, Francesco Piccinno |  |
| 1165 |  |  [Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation](https://doi.org/10.18653/v1/2021.emnlp-main.696) |  | 0 | Despite recent progress, state-of-the-art question answering models remain vulnerable to a variety of adversarial attacks. While dynamic adversarial data collection, in which a human annotator tries to write examples that fool a model-in-the-loop, can improve model robustness, this process is... | Max Bartolo, Tristan Thrush, Robin Jia, Sebastian Riedel, Pontus Stenetorp, Douwe Kiela |  |
| 1166 |  |  [BeliefBank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief](https://doi.org/10.18653/v1/2021.emnlp-main.697) |  | 0 | Although pretrained language models (PTLMs) contain significant amounts of world knowledge, they can still produce inconsistent answers to questions when probed, even after specialized training. As a result, it can be hard to identify what the model actually “believes” about the world, making it... | Nora Kassner, Oyvind Tafjord, Hinrich Schütze, Peter Clark |  |
| 1167 |  |  [MLEC-QA: A Chinese Multi-Choice Biomedical Question Answering Dataset](https://doi.org/10.18653/v1/2021.emnlp-main.698) |  | 0 | Question Answering (QA) has been successfully applied in scenarios of human-computer interaction such as chatbots and search engines. However, for the specific biomedical domain, QA systems are still immature due to expert-annotated datasets being limited by category and scale. In this paper, we... | Jing Li, Shangping Zhong, Kaizhi Chen |  |
| 1168 |  |  [IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural Language Generation](https://doi.org/10.18653/v1/2021.emnlp-main.699) |  | 0 | Natural language generation (NLG) benchmarks provide an important avenue to measure progress and develop better NLG systems. Unfortunately, the lack of publicly available NLG benchmarks for low-resource languages poses a challenging barrier for building NLG systems that work well for languages with... | Samuel Cahyawijaya, Genta Indra Winata, Bryan Wilie, Karissa Vincentio, Xiaohong Li, Adhiguna Kuncoro, Sebastian Ruder, Zhi Yuan Lim, Syafri Bahar, Masayu Leylia Khodra, Ayu Purwarianti, Pascale Fung |  |
| 1169 |  |  [Is Multi-Hop Reasoning Really Explainable? Towards Benchmarking Reasoning Interpretability](https://doi.org/10.18653/v1/2021.emnlp-main.700) |  | 0 | Multi-hop reasoning has been widely studied in recent years to obtain more interpretable link prediction. However, we find in experiments that many paths given by these models are actually unreasonable, while little work has been done on interpretability evaluation for them. In this paper, we... | Xin Lv, Yixin Cao, Lei Hou, Juanzi Li, Zhiyuan Liu, Yichi Zhang, Zelin Dai |  |
| 1170 |  |  [Global Explainability of BERT-Based Evaluation Metrics by Disentangling along Linguistic Factors](https://doi.org/10.18653/v1/2021.emnlp-main.701) |  | 0 | Evaluation metrics are a key ingredient for progress of text generation systems. In recent years, several BERT-based evaluation metrics have been proposed (including BERTScore, MoverScore, BLEURT, etc.) which correlate much better with human assessment of text generation quality than BLEU or ROUGE,... | Marvin Kaster, Wei Zhao, Steffen Eger |  |
| 1171 |  |  [Exploring Underexplored Limitations of Cross-Domain Text-to-SQL Generalization](https://doi.org/10.18653/v1/2021.emnlp-main.702) |  | 0 | Recently, there has been significant progress in studying neural networks for translating text descriptions into SQL queries under the zero-shot cross-domain setting. Despite achieving good performance on some public benchmarks, we observe that existing text-to-SQL models do not generalize when... | Yujian Gan, Xinyun Chen, Matthew Purver |  |
| 1172 |  |  [What happens if you treat ordinal ratings as interval data? Human evaluations in NLP are even more under-powered than you think](https://doi.org/10.18653/v1/2021.emnlp-main.703) |  | 0 | Previous work has shown that human evaluations in NLP are notoriously under-powered. Here, we argue that there are two common factors which make this problem even worse: NLP studies usually (a) treat ordinal data as interval data and (b) operate under high variance settings while the differences... | David M. Howcroft, Verena Rieser |  |
| 1173 |  |  [NeuTral Rewriter: A Rule-Based and Neural Approach to Automatic Rewriting into Gender Neutral Alternatives](https://doi.org/10.18653/v1/2021.emnlp-main.704) |  | 0 | Recent years have seen an increasing need for gender-neutral and inclusive language. Within the field of NLP, there are various mono- and bilingual use cases where gender inclusive language is appropriate, if not preferred due to ambiguity or uncertainty in terms of the gender of referents. In this... | Eva Vanmassenhove, Chris Emmery, Dimitar Shterionov |  |
| 1174 |  |  [Benchmarking Commonsense Knowledge Base Population with an Effective Evaluation Dataset](https://doi.org/10.18653/v1/2021.emnlp-main.705) |  | 0 | Reasoning over commonsense knowledge bases (CSKB) whose elements are in the form of free-text is an important yet hard task in NLP. While CSKB completion only fills the missing links within the domain of the CSKB, CSKB population is alternatively proposed with the goal of reasoning unseen... | Tianqing Fang, Weiqi Wang, Sehyun Choi, Shibo Hao, Hongming Zhang, Yangqiu Song, Bin He |  |
| 1175 |  |  [Enhancing the Context Representation in Similarity-based Word Sense Disambiguation](https://doi.org/10.18653/v1/2021.emnlp-main.706) |  | 0 | In previous similarity-based WSD systems, studies have allocated much effort on learning comprehensive sense embeddings using contextual representations and knowledge sources. However, the context embedding of an ambiguous word is learned using only the sentence where the word appears, neglecting... | Ming Wang, Jianzhang Zhang, Yinglin Wang |  |
| 1176 |  |  [Data Augmentation with Hierarchical SQL-to-Question Generation for Cross-domain Text-to-SQL Parsing](https://doi.org/10.18653/v1/2021.emnlp-main.707) |  | 0 | Data augmentation has attracted a lot of research attention in the deep learning era for its ability in alleviating data sparseness. The lack of labeled data for unseen evaluation databases is exactly the major challenge for cross-domain text-to-SQL parsing. Previous works either require human... | Kun Wu, Lijie Wang, Zhenghua Li, Ao Zhang, Xinyan Xiao, Hua Wu, Min Zhang, Haifeng Wang |  |
| 1177 |  |  [SPARQLing Database Queries from Intermediate Question Decompositions](https://doi.org/10.18653/v1/2021.emnlp-main.708) |  | 0 | To translate natural language questions into executable database queries, most approaches rely on a fully annotated training set. Annotating a large dataset with queries is difficult as it requires query-language expertise. We reduce this burden using grounded in databases intermediate question... | Irina Saparina, Anton Osokin |  |
| 1178 |  |  [Time-aware Graph Neural Network for Entity Alignment between Temporal Knowledge Graphs](https://doi.org/10.18653/v1/2021.emnlp-main.709) |  | 0 | Entity alignment aims to identify equivalent entity pairs between different knowledge graphs (KGs). Recently, the availability of temporal KGs (TKGs) that contain time information created the need for reasoning over time in such TKGs. Existing embedding-based entity alignment approaches disregard... | Chengjin Xu, Fenglong Su, Jens Lehmann |  |
| 1179 |  |  [Cross-Domain Label-Adaptive Stance Detection](https://doi.org/10.18653/v1/2021.emnlp-main.710) |  | 0 | Stance detection concerns the classification of a writer’s viewpoint towards a target. There are different task variants, e.g., stance of a tweet vs. a full article, or stance with respect to a claim vs. an (implicit) topic. Moreover, task definitions vary, which includes the label inventory, the... | Momchil Hardalov, Arnav Arora, Preslav Nakov, Isabelle Augenstein |  |
| 1180 |  |  [Text AutoAugment: Learning Compositional Augmentation Policy for Text Classification](https://doi.org/10.18653/v1/2021.emnlp-main.711) |  | 0 | Data augmentation aims to enrich training samples for alleviating the overfitting issue in low-resource or class-imbalanced situations. Traditional methods first devise task-specific operations such as Synonym Substitute, then preset the corresponding parameters such as the substitution rate... | Shuhuai Ren, Jinchao Zhang, Lei Li, Xu Sun, Jie Zhou |  |
| 1181 |  |  [Distilling Relation Embeddings from Pretrained Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.712) |  | 0 | Pre-trained language models have been found to capture a surprisingly rich amount of lexical knowledge, ranging from commonsense properties of everyday concepts to detailed factual knowledge about named entities. Among others, this makes it possible to distill high-quality word vectors from... | Asahi Ushio, José CamachoCollados, Steven Schockaert |  |
| 1182 |  |  [Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning](https://doi.org/10.18653/v1/2021.emnlp-main.713) |  | 0 | Recent prompt-based approaches allow pretrained language models to achieve strong performances on few-shot finetuning by reformulating downstream tasks as a language modeling problem. In this work, we demonstrate that, despite its advantages on low data regimes, finetuned prompt-based models for... | Prasetya Ajie Utama, Nafise Sadat Moosavi, Victor Sanh, Iryna Gurevych |  |
| 1183 |  |  [A Differentiable Relaxation of Graph Segmentation and Alignment for AMR Parsing](https://doi.org/10.18653/v1/2021.emnlp-main.714) |  | 0 | Abstract Meaning Representations (AMR) are a broad-coverage semantic formalism which represents sentence meaning as a directed acyclic graph. To train most AMR parsers, one needs to segment the graph into subgraphs and align each such subgraph to a word in a sentence; this is normally done at... | Chunchuan Lyu, Shay B. Cohen, Ivan Titov |  |
| 1184 |  |  [Integrating Personalized PageRank into Neural Word Sense Disambiguation](https://doi.org/10.18653/v1/2021.emnlp-main.715) |  | 0 | Neural Word Sense Disambiguation (WSD) has recently been shown to benefit from the incorporation of pre-existing knowledge, such as that coming from the WordNet graph. However, state-of-the-art approaches have been successful in exploiting only the local structure of the graph, with only close... | Ahmed El Sheikh, Michele Bevilacqua, Roberto Navigli |  |
| 1185 |  |  [Cross-lingual Sentence Embedding using Multi-Task Learning](https://doi.org/10.18653/v1/2021.emnlp-main.716) |  | 0 | Multilingual sentence embeddings capture rich semantic information not only for measuring similarity between texts but also for catering to a broad range of downstream cross-lingual NLP tasks. State-of-the-art multilingual sentence embedding models require large parallel corpora to learn... | Koustava Goswami, Sourav Dutta, Haytham Assem, Theodorus Fransen, John P. McCrae |  |
| 1186 |  |  [NB-MLM: Efficient Domain Adaptation of Masked Language Models for Sentiment Analysis](https://doi.org/10.18653/v1/2021.emnlp-main.717) |  | 0 | While Masked Language Models (MLM) are pre-trained on massive datasets, the additional training with the MLM objective on domain or task-specific data before fine-tuning for the final task is known to improve the final performance. This is usually referred to as the domain or task adaptation step.... | Nikolay Arefyev, Dmitry Kharchev, Artem Shelmanov |  |
| 1187 |  |  [Revisiting Self-training for Few-shot Learning of Language Model](https://doi.org/10.18653/v1/2021.emnlp-main.718) |  | 0 | As unlabeled data carry rich task-relevant information, they are proven useful for few-shot learning of language model. The question is how to effectively make use of such data. In this work, we revisit the self-training technique for language model fine-tuning and present a state-of-the-art... | Yiming Chen, Yan Zhang, Chen Zhang, Grandee Lee, Ran Cheng, Haizhou Li |  |
| 1188 |  |  [Bridging Perception, Memory, and Inference through Semantic Relations](https://doi.org/10.18653/v1/2021.emnlp-main.719) |  | 0 | There is a growing consensus that surface form alone does not enable models to learn meaning and gain language understanding. This warrants an interest in hybrid systems that combine the strengths of neural and symbolic methods. We favour triadic systems consisting of neural networks, knowledge... | Johanna Björklund, Adam Dahlgren Lindström, Frank Drewes |  |
| 1189 |  |  [Unimodal and Crossmodal Refinement Network for Multimodal Sequence Fusion](https://doi.org/10.18653/v1/2021.emnlp-main.720) |  | 0 | Effective unimodal representation and complementary crossmodal representation fusion are both important in multimodal representation learning. Prior works often modulate one modal feature to another straightforwardly and thus, underutilizing both unimodal and crossmodal representation refinements,... | Xiaobao Guo, Adams WaiKin Kong, Huan Zhou, Xianfeng Wang, Min Wang |  |
| 1190 |  |  [YASO: A Targeted Sentiment Analysis Evaluation Dataset for Open-Domain Reviews](https://doi.org/10.18653/v1/2021.emnlp-main.721) |  | 0 | Current TSA evaluation in a cross-domain setup is restricted to the small set of review domains available in existing datasets. Such an evaluation is limited, and may not reflect true performance on sites like Amazon or Yelp that host diverse reviews from many domains. To address this gap, we... | Matan Orbach, Orith ToledoRonen, Artem Spector, Ranit Aharonov, Yoav Katz, Noam Slonim |  |
| 1191 |  |  [An Empirical Study on Leveraging Position Embeddings for Target-oriented Opinion Words Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.722) |  | 0 | Target-oriented opinion words extraction (TOWE) (Fan et al., 2019b) is a new subtask of target-oriented sentiment analysis that aims to extract opinion words for a given aspect in text. Current state-of-the-art methods leverage position embeddings to capture the relative position of a word to the... | Samuel Mensah, Kai Sun, Nikolaos Aletras |  |
| 1192 |  |  [Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis](https://doi.org/10.18653/v1/2021.emnlp-main.723) |  | 0 | In multimodal sentiment analysis (MSA), the performance of a model highly depends on the quality of synthesized embeddings. These embeddings are generated from the upstream process called multimodal fusion, which aims to extract and combine the input unimodal raw data to produce a richer multimodal... | Wei Han, Hui Chen, Soujanya Poria |  |
| 1193 |  |  [BERT4GCN: Using BERT Intermediate Layers to Augment GCN for Aspect-based Sentiment Classification](https://doi.org/10.18653/v1/2021.emnlp-main.724) |  | 0 | Graph-based Aspect-based Sentiment Classification (ABSC) approaches have yielded state-of-the-art results, expecially when equipped with contextual word embedding from pre-training language models (PLMs). However, they ignore sequential features of the context and have not yet made the best of... | Zeguan Xiao, Jiarun Wu, Qingliang Chen, Congjian Deng |  |
| 1194 |  |  [Does Social Pressure Drive Persuasion in Online Fora?](https://doi.org/10.18653/v1/2021.emnlp-main.725) |  | 0 | Online forums such as ChangeMyView have been explored to research aspects of persuasion and argumentative quality in language. While previous research has focused on arguments between a view-holder and a persuader, we explore the premise that apart from the merits of arguments, persuasion is... | Ayush Jain, Shashank Srivastava |  |
| 1195 |  |  [Aspect Sentiment Quad Prediction as Paraphrase Generation](https://doi.org/10.18653/v1/2021.emnlp-main.726) |  | 0 | Aspect-based sentiment analysis (ABSA) has been extensively studied in recent years, which typically involves four fundamental sentiment elements, including the aspect category, aspect term, opinion term, and sentiment polarity. Existing studies usually consider the detection of partial sentiment... | Wenxuan Zhang, Yang Deng, Xin Li, Yifei Yuan, Lidong Bing, Wai Lam |  |
| 1196 |  |  [Cross-lingual Aspect-based Sentiment Analysis with Aspect Term Code-Switching](https://doi.org/10.18653/v1/2021.emnlp-main.727) |  | 0 | Many efforts have been made in solving the Aspect-based sentiment analysis (ABSA) task. While most existing studies focus on English texts, handling ABSA in resource-poor languages remains a challenging problem. In this paper, we consider the unsupervised cross-lingual transfer for the ABSA task,... | Wenxuan Zhang, Ruidan He, Haiyun Peng, Lidong Bing, Wai Lam |  |
| 1197 |  |  [Towards Label-Agnostic Emotion Embeddings](https://doi.org/10.18653/v1/2021.emnlp-main.728) |  | 0 | Research in emotion analysis is scattered across different label formats (e.g., polarity types, basic emotion categories, and affective dimensions), linguistic levels (word vs. sentence vs. discourse), and, of course, (few well-resourced but much more under-resourced) natural languages and text... | Sven Buechel, Luise Modersohn, Udo Hahn |  |
| 1198 |  |  [Collaborative Learning of Bidirectional Decoders for Unsupervised Text Style Transfer](https://doi.org/10.18653/v1/2021.emnlp-main.729) |  | 0 | Unsupervised text style transfer aims to alter the underlying style of the text to a desired value while keeping its style-independent semantics, without the support of parallel training corpora. Existing methods struggle to achieve both high style conversion rate and low content loss, exhibiting... | Yun Ma, Yangbin Chen, Xudong Mao, Qing Li |  |
| 1199 |  |  [Exploring Non-Autoregressive Text Style Transfer](https://doi.org/10.18653/v1/2021.emnlp-main.730) |  | 0 | In this paper, we explore Non-AutoRegressive (NAR) decoding for unsupervised text style transfer. We first propose a base NAR model by directly adapting the common training scheme from its AutoRegressive (AR) counterpart. Despite the faster inference speed over the AR model, this NAR model... | Yun Ma, Qing Li |  |
| 1200 |  |  [PASTE: A Tagging-Free Decoding Framework Using Pointer Networks for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.731) |  | 0 | Aspect Sentiment Triplet Extraction (ASTE) deals with extracting opinion triplets, consisting of an opinion target or aspect, its associated sentiment, and the corresponding opinion term/span explaining the rationale behind the sentiment. Existing research efforts are majorly tagging-based. Among... | Rajdeep Mukherjee, Tapas Nayak, Yash Butala, Sourangshu Bhattacharya, Pawan Goyal |  |
| 1201 |  |  [Adaptive Proposal Generation Network for Temporal Sentence Localization in Videos](https://doi.org/10.18653/v1/2021.emnlp-main.732) |  | 0 | We address the problem of temporal sentence localization in videos (TSLV). Traditional methods follow a top-down framework which localizes the target segment with pre-defined segment proposals. Although they have achieved decent performance, the proposals are handcrafted and redundant. Recently,... | Daizong Liu, Xiaoye Qu, Jianfeng Dong, Pan Zhou |  |
| 1202 |  |  [Progressively Guide to Attend: An Iterative Alignment Framework for Temporal Sentence Grounding](https://doi.org/10.18653/v1/2021.emnlp-main.733) |  | 0 | A key solution to temporal sentence grounding (TSG) exists in how to learn effective alignment between vision and language features extracted from an untrimmed video and a sentence description. Existing methods mainly leverage vanilla soft attention to perform the alignment in a single-step... | Daizong Liu, Xiaoye Qu, Pan Zhou |  |
| 1203 |  |  [Language Models are Few-Shot Butlers](https://doi.org/10.18653/v1/2021.emnlp-main.734) |  | 0 | Pretrained language models demonstrate strong performance in most NLP tasks when fine-tuned on small task-specific datasets. Hence, these autoregressive models constitute ideal agents to operate in text-based environments where language understanding and generative capabilities are essential.... | Vincent Micheli, François Fleuret |  |
| 1204 |  |  [R\^3Net: Relation-embedded Representation Reconstruction Network for Change Captioning](https://doi.org/10.18653/v1/2021.emnlp-main.735) |  | 0 | Change captioning is to use a natural language sentence to describe the fine-grained disagreement between two similar images. Viewpoint change is the most typical distractor in this task, because it changes the scale and location of the objects and overwhelms the representation of real change. In... | Yunbin Tu, Liang Li, Chenggang Yan, Shengxiang Gao, Zhengtao Yu |  |
| 1205 |  |  [Looking for Confirmations: An Effective and Human-Like Visual Dialogue Strategy](https://doi.org/10.18653/v1/2021.emnlp-main.736) |  | 0 | Generating goal-oriented questions in Visual Dialogue tasks is a challenging and longstanding problem. State-Of-The-Art systems are shown to generate questions that, although grammatically correct, often lack an effective strategy and sound unnatural to humans. Inspired by the cognitive literature... | Alberto Testoni, Raffaella Bernardi |  |
| 1206 |  |  [A Unified Speaker Adaptation Approach for ASR](https://doi.org/10.18653/v1/2021.emnlp-main.737) |  | 0 | Transformer models have been used in automatic speech recognition (ASR) successfully and yields state-of-the-art results. However, its performance is still affected by speaker mismatch between training and test data. Further finetuning a trained model with target speaker data is the most natural... | Yingzhu Zhao, Chongjia Ni, CheungChi Leung, Shafiq R. Joty, Eng Siong Chng, Bin Ma |  |
| 1207 |  |  [Caption Enriched Samples for Improving Hateful Memes Detection](https://doi.org/10.18653/v1/2021.emnlp-main.738) |  | 0 | The recently introduced hateful meme challenge demonstrates the difficulty of determining whether a meme is hateful or not. Specifically, both unimodal language models and multimodal vision-language models cannot reach the human level of performance. Motivated by the need to model the contrast... | Efrat Blaier, Itzik Malkiel, Lior Wolf |  |
| 1208 |  |  [Sparsity and Sentence Structure in Encoder-Decoder Attention of Summarization Systems](https://doi.org/10.18653/v1/2021.emnlp-main.739) |  | 0 | Transformer models have achieved state-of-the-art results in a wide range of NLP tasks including summarization. Training and inference using large transformer models can be computationally expensive. Previous work has focused on one important bottleneck, the quadratic self-attention mechanism in... | Potsawee Manakul, Mark J. F. Gales |  |
| 1209 |  |  [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://doi.org/10.18653/v1/2021.emnlp-main.740) |  | 0 | Inductive transfer learning has taken the entire NLP field by storm, with models such as BERT and BART setting new state of the art on countless NLU tasks. However, most of the available models and research have been conducted for English. In this work, we introduce BARThez, the first large-scale... | Moussa Kamal Eddine, Antoine J.P. Tixier, Michalis Vazirgiannis |  |
| 1210 |  |  [ARMAN: Pre-training with Semantically Selecting and Reordering of Sentences for Persian Abstractive Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.741) |  | 0 | Abstractive text summarization is one of the areas influenced by the emergence of pre-trained language models. Current pre-training works in abstractive summarization give more points to the summaries with more words in common with the main text and pay less attention to the semantic similarity... | Alireza Salemi, Emad Kebriaei, Ghazal Neisi Minaei, Azadeh Shakery |  |
| 1211 |  |  [Models and Datasets for Cross-Lingual Summarisation](https://doi.org/10.18653/v1/2021.emnlp-main.742) |  | 0 | We present a cross-lingual summarisation corpus with long documents in a source language associated with multi-sentence summaries in a target language. The corpus covers twelve language pairs and directions for four European languages, namely Czech, English, French and German, and the methodology... | Laura PerezBeltrachini, Mirella Lapata |  |
| 1212 |  |  [Learning Opinion Summarizers by Selecting Informative Reviews](https://doi.org/10.18653/v1/2021.emnlp-main.743) |  | 0 | Opinion summarization has been traditionally approached with unsupervised, weakly-supervised and few-shot learning techniques. In this work, we collect a large dataset of summaries paired with user reviews for over 31,000 products, enabling supervised training. However, the number of reviews per... | Arthur Brazinskas, Mirella Lapata, Ivan Titov |  |
| 1213 |  |  [Enriching and Controlling Global Semantics for Text Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.744) |  | 0 | Recently, Transformer-based models have been proven effective in the abstractive summarization task by creating fluent and informative summaries. Nevertheless, these models still suffer from the short-range dependency problem, causing them to produce summaries that miss the key points of document.... | Thong Nguyen, Anh Tuan Luu, Truc Lu, Tho Quan |  |
| 1214 |  |  [Revisiting Tri-training of Dependency Parsers](https://doi.org/10.18653/v1/2021.emnlp-main.745) |  | 0 | We compare two orthogonal semi-supervised learning techniques, namely tri-training and pretrained word embeddings, in the task of dependency parsing. We explore language-specific FastText and ELMo embeddings and multilingual BERT embeddings. We focus on a low resource scenario as semi-supervised... | Joachim Wagner, Jennifer Foster |  |
| 1215 |  |  [Bridge to Target Domain by Prototypical Contrastive Learning and Label Confusion: Re-explore Zero-Shot Learning for Slot Filling](https://doi.org/10.18653/v1/2021.emnlp-main.746) |  | 0 | Zero-shot cross-domain slot filling alleviates the data dependence in the case of data scarcity in the target domain, which has aroused extensive research. However, as most of the existing methods do not achieve effective knowledge transfer to the target domain, they just fit the distribution of... | Liwen Wang, Xuefeng Li, Jiachi Liu, Keqing He, Yuanmeng Yan, Weiran Xu |  |
| 1216 |  |  [Neuralizing Regular Expressions for Slot Filling](https://doi.org/10.18653/v1/2021.emnlp-main.747) |  | 0 | Neural models and symbolic rules such as regular expressions have their respective merits and weaknesses. In this paper, we study the integration of the two approaches for the slot filling task by converting regular expressions into neural networks. Specifically, we first convert regular... | Chengyue Jiang, Zijian Jin, Kewei Tu |  |
| 1217 |  |  [Causal Direction of Data Collection Matters: Implications of Causal and Anticausal Learning for NLP](https://doi.org/10.18653/v1/2021.emnlp-main.748) |  | 0 | The principle of independent causal mechanisms (ICM) states that generative processes of real world data consist of independent modules which do not influence or inform each other. While this idea has led to fruitful developments in the field of causal inference, it is not widely-known in the NLP... | Zhijing Jin, Julius von Kügelgen, Jingwei Ni, Tejas Vaidhya, Ayush Kaushal, Mrinmaya Sachan, Bernhard Schölkopf |  |
| 1218 |  |  [Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning](https://doi.org/10.18653/v1/2021.emnlp-main.749) |  | 0 | Recent pretrained language models extend from millions to billions of parameters. Thus the need to fine-tune an extremely large pretrained model with a limited training corpus arises in various downstream tasks. In this paper, we propose a straightforward yet effective fine-tuning technique,... | Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao Chang, Songfang Huang, Fei Huang |  |
| 1219 |  |  [Knowledge Graph Representation Learning using Ordinary Differential Equations](https://doi.org/10.18653/v1/2021.emnlp-main.750) |  | 0 | Knowledge Graph Embeddings (KGEs) have shown promising performance on link prediction tasks by mapping the entities and relations from a knowledge graph into a geometric space. The capability of KGEs in preserving graph characteristics including structural aspects and semantics, highly depends on... | Mojtaba Nayyeri, Chengjin Xu, Franca Hoffmann, Mirza Mohtashim Alam, Jens Lehmann, Sahar Vahdati |  |
| 1220 |  |  [KnowMAN: Weakly Supervised Multinomial Adversarial Networks](https://doi.org/10.18653/v1/2021.emnlp-main.751) |  | 0 | The absence of labeled data for training neural models is often addressed by leveraging knowledge about the specific task, resulting in heuristic but noisy labels. The knowledge is captured in labeling functions, which detect certain regularities or patterns in the training samples and annotate... | Luisa März, Ehsaneddin Asgari, Fabienne Braune, Franziska Zimmermann, Benjamin Roth |  |
| 1221 |  |  [ONION: A Simple and Effective Defense Against Textual Backdoor Attacks](https://doi.org/10.18653/v1/2021.emnlp-main.752) |  | 0 | Backdoor attacks are a kind of emergent training-time threat to deep neural networks (DNNs). They can manipulate the output of DNNs and possess high insidiousness. In the field of natural language processing, some attack methods have been proposed and achieve very high attack success rates on... | Fanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao, Zhiyuan Liu, Maosong Sun |  |
| 1222 |  |  [Value-aware Approximate Attention](https://doi.org/10.18653/v1/2021.emnlp-main.753) |  | 0 | Following the success of dot-product attention in Transformers, numerous approximations have been recently proposed to address its quadratic complexity with respect to the input length. However, all approximations thus far have ignored the contribution of the \*value vectors\* to the quality of... | Ankit Gupta, Jonathan Berant |  |
| 1223 |  |  [Contrastive Domain Adaptation for Question Answering using Limited Text Corpora](https://doi.org/10.18653/v1/2021.emnlp-main.754) |  | 0 | Question generation has recently shown impressive results in customizing question answering (QA) systems to new domains. These approaches circumvent the need for manually annotated training data from the new domain and, instead, generate synthetic question-answer pairs that are used for training.... | Zhenrui Yue, Bernhard Kratzwald, Stefan Feuerriegel |  |
| 1224 |  |  [Case-based Reasoning for Natural Language Queries over Knowledge Bases](https://doi.org/10.18653/v1/2021.emnlp-main.755) |  | 0 | It is often challenging to solve a complex problem from scratch, but much easier if we can access other similar problems with their solutions — a paradigm known as case-based reasoning (CBR). We propose a neuro-symbolic CBR approach (CBR-KBQA) for question answering over large knowledge bases.... | Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez, Jay Yoon Lee, Lizhen Tan, Lazaros Polymenakos, Andrew McCallum |  |
| 1225 |  |  [Distantly-Supervised Dense Retrieval Enables Open-Domain Question Answering without Evidence Annotation](https://doi.org/10.18653/v1/2021.emnlp-main.756) |  | 0 | Open-domain question answering answers a question based on evidence retrieved from a large corpus. State-of-the-art neural approaches require intermediate evidence annotations for training. However, such intermediate annotations are expensive, and methods that rely on them cannot transfer to the... | Chen Zhao, Chenyan Xiong, Jordan L. BoydGraber, Hal Daumé III |  |
| 1226 |  |  [What's in a Name? Answer Equivalence For Open-Domain Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.757) |  | 0 | A flaw in QA evaluation is that annotations often only provide one gold answer. Thus, model predictions semantically equivalent to the answer but superficially different are considered incorrect. This work explores mining alias entities from knowledge bases and using them as additional gold answers... | Chenglei Si, Chen Zhao, Jordan L. BoydGraber |  |
| 1227 |  |  [Evaluation Paradigms in Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.758) |  | 0 | Question answering (QA) primarily descends from two branches of research: (1) Alan Turing’s investigation of machine intelligence at Manchester University and (2) Cyril Cleverdon’s comparison of library card catalog indices at Cranfield University. This position paper names and distinguishes these... | Pedro Rodriguez, Jordan L. BoydGraber |  |
| 1228 |  |  [Numerical reasoning in machine reading comprehension tasks: are we there yet?](https://doi.org/10.18653/v1/2021.emnlp-main.759) |  | 0 | Numerical reasoning based machine reading comprehension is a task that involves reading comprehension along with using arithmetic operations such as addition, subtraction, sorting and counting. The DROP benchmark (Dua et al., 2019) is a recent dataset that has inspired the design of NLP models... | Hadeel AlNegheimish, Pranava Madhyastha, Alessandra Russo |  |
| 1229 |  |  [Set Generation Networks for End-to-End Knowledge Base Population](https://doi.org/10.18653/v1/2021.emnlp-main.760) |  | 0 | The task of knowledge base population (KBP) aims to discover facts about entities from texts and expand a knowledge base with these facts. Previous studies shape end-to-end KBP as a machine translation task, which is required to convert unordered fact into a sequence according to a pre-specified... | Dianbo Sui, Chenhao Wang, Yubo Chen, Kang Liu, Jun Zhao, Wei Bi |  |
| 1230 |  |  [Knowing False Negatives: An Adversarial Training Method for Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.761) |  | 0 | Distantly supervised relation extraction (RE) automatically aligns unstructured text with relation instances in a knowledge base (KB). Due to the incompleteness of current KBs, sentences implying certain relations may be annotated as N/A instances, which causes the so-called false negative (FN)... | Kailong Hao, Botao Yu, Wei Hu |  |
| 1231 |  |  [Progressive Adversarial Learning for Bootstrapping: A Case Study on Entity Set Expansion](https://doi.org/10.18653/v1/2021.emnlp-main.762) |  | 0 | Bootstrapping has become the mainstream method for entity set expansion. Conventional bootstrapping methods mostly define the expansion boundary using seed-based distance metrics, which heavily depend on the quality of selected seeds and are hard to be adjusted due to the extremely sparse... | Lingyong Yan, Xianpei Han, Le Sun |  |
| 1232 |  |  [Uncovering Main Causalities for Long-tailed Information Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.763) |  | 0 | Information Extraction (IE) aims to extract structural information from unstructured texts. In practice, long-tailed distributions caused by the selection bias of a dataset may lead to incorrect correlations, also known as spurious correlations, between entities and labels in the conventional... | Guoshun Nan, Jiaqi Zeng, Rui Qiao, Zhijiang Guo, Wei Lu |  |
| 1233 |  |  [Maximal Clique Based Non-Autoregressive Open Information Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.764) |  | 0 | Open Information Extraction (OpenIE) aims to discover textual facts from a given sentence. In essence, the facts contained in plain text are unordered. However, the popular OpenIE systems usually output facts sequentially in the way of predicting the next fact conditioned on the previous decoded... | Bowen Yu, Yucheng Wang, Tingwen Liu, Hongsong Zhu, Limin Sun, Bin Wang |  |
| 1234 |  |  [A Relation-Oriented Clustering Method for Open Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.765) |  | 0 | The clustering-based unsupervised relation discovery method has gradually become one of the important methods of open relation extraction (OpenRE). However, high-dimensional vectors can encode complex linguistic information which leads to the problem that the derived clusters cannot explicitly... | Jun Zhao, Tao Gui, Qi Zhang, Yaqian Zhou |  |
| 1235 |  |  [Exploring Methods for Generating Feedback Comments for Writing Learning](https://doi.org/10.18653/v1/2021.emnlp-main.766) |  | 0 | The task of generating explanatory notes for language learners is known as feedback comment generation. Although various generation techniques are available, little is known about which methods are appropriate for this task. Nagata (2019) demonstrates the effectiveness of neural-retrieval-based... | Kazuaki Hanawa, Ryo Nagata, Kentaro Inui |  |
| 1236 |  |  [A Role-Selected Sharing Network for Joint Machine-Human Chatting Handoff and Service Satisfaction Analysis](https://doi.org/10.18653/v1/2021.emnlp-main.767) |  | 0 | Chatbot is increasingly thriving in different domains, however, because of unexpected discourse complexity and training data sparseness, its potential distrust hatches vital apprehension. Recently, Machine-Human Chatting Handoff (MHCH), predicting chatbot failure and enabling human-algorithm... | Jiawei Liu, Kaisong Song, Yangyang Kang, Guoxiu He, Zhuoren Jiang, Changlong Sun, Wei Lu, Xiaozhong Liu |  |
| 1237 |  |  [Meta Distant Transfer Learning for Pre-trained Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.768) |  | 0 | With the wide availability of Pre-trained Language Models (PLMs), multi-task fine-tuning across domains has been extensively applied. For tasks related to distant domains with different class label sets, PLMs may memorize non-transferable knowledge for the target domain and suffer from negative... | Chengyu Wang, Haojie Pan, Minghui Qiu, Jun Huang, Fei Yang, Yin Zhang |  |
| 1238 |  |  [UniKER: A Unified Framework for Combining Embedding and Definite Horn Rule Reasoning for Knowledge Graph Inference](https://doi.org/10.18653/v1/2021.emnlp-main.769) |  | 0 | Knowledge graph inference has been studied extensively due to its wide applications. It has been addressed by two lines of research, i.e., the more traditional logical rule reasoning and the more recent knowledge graph embedding (KGE). Several attempts have been made to combine KGE and logical... | Kewei Cheng, Ziqing Yang, Ming Zhang, Yizhou Sun |  |
| 1239 |  |  [Wasserstein Selective Transfer Learning for Cross-domain Text Mining](https://doi.org/10.18653/v1/2021.emnlp-main.770) |  | 0 | Transfer learning (TL) seeks to improve the learning of a data-scarce target domain by using information from source domains. However, the source and target domains usually have different data distributions, which may lead to negative transfer. To alleviate this issue, we propose a Wasserstein... | Lingyun Feng, Minghui Qiu, Yaliang Li, Haitao Zheng, Ying Shen |  |
| 1240 |  |  [Jointly Learning to Repair Code and Generate Commit Message](https://doi.org/10.18653/v1/2021.emnlp-main.771) |  | 0 | We propose a novel task of jointly repairing program codes and generating commit messages. Code repair and commit message generation are two essential and related tasks for software development. However, existing work usually performs the two tasks independently. We construct a multilingual triple... | Jiaqi Bai, Long Zhou, Ambrosio Blanco, Shujie Liu, Furu Wei, Ming Zhou, Zhoujun Li |  |
| 1241 |  |  [Inflate and Shrink: Enriching and Reducing Interactions for Fast Text-Image Retrieval](https://doi.org/10.18653/v1/2021.emnlp-main.772) |  | 0 | By exploiting the cross-modal attention, cross-BERT methods have achieved state-of-the-art accuracy in cross-modal retrieval. Nevertheless, the heavy text-image interactions in the cross-BERT model are prohibitively slow for large-scale retrieval. Late-interaction methods trade off retrieval... | Haoliang Liu, Tan Yu, Ping Li |  |
| 1242 |  |  [On Pursuit of Designing Multi-modal Transformer for Video Grounding](https://doi.org/10.18653/v1/2021.emnlp-main.773) |  | 0 | Video grounding aims to localize the temporal segment corresponding to a sentence query from an untrimmed video. Almost all existing video grounding methods fall into two frameworks: 1) Top-down model: It predefines a set of segment candidates and then conducts segment classification and... | Meng Cao, Long Chen, Mike Zheng Shou, Can Zhang, Yuexian Zou |  |
| 1243 |  |  [COVR: A Test-Bed for Visually Grounded Compositional Generalization with Real Images](https://doi.org/10.18653/v1/2021.emnlp-main.774) |  | 0 | While interest in models that generalize at test time to new compositions has risen in recent years, benchmarks in the visually-grounded domain have thus far been restricted to synthetic images. In this work, we propose COVR, a new test-bed for visually-grounded compositional generalization with... | Ben Bogin, Shivanshu Gupta, Matt Gardner, Jonathan Berant |  |
| 1244 |  |  [Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers](https://doi.org/10.18653/v1/2021.emnlp-main.775) |  | 0 | Pretrained vision-and-language BERTs aim to learn representations that combine information from both modalities. We propose a diagnostic method based on cross-modal input ablation to assess the extent to which these models actually integrate cross-modal information. This method involves ablating... | Stella Frank, Emanuele Bugliarello, Desmond Elliott |  |
| 1245 |  |  [HypMix: Hyperbolic Interpolative Data Augmentation](https://doi.org/10.18653/v1/2021.emnlp-main.776) |  | 0 | Interpolation-based regularisation methods for data augmentation have proven to be effective for various tasks and modalities. These methods involve performing mathematical operations over the raw input samples or their latent states representations - vectors that often possess complex hierarchical... | Ramit Sawhney, Megh Thakkar, Shivam Agarwal, Di Jin, Diyi Yang, Lucie Flek |  |
| 1246 |  |  [Integrating Deep Event-Level and Script-Level Information for Script Event Prediction](https://doi.org/10.18653/v1/2021.emnlp-main.777) |  | 0 | Scripts are structured sequences of events together with the participants, which are extracted from the texts. Script event prediction aims to predict the subsequent event given the historical events in the script. Two kinds of information facilitate this task, namely, the event-level information... | Long Bai, Saiping Guan, Jiafeng Guo, Zixuan Li, Xiaolong Jin, Xueqi Cheng |  |
| 1247 |  |  [QA-Align: Representing Cross-Text Content Overlap by Aligning Question-Answer Propositions](https://doi.org/10.18653/v1/2021.emnlp-main.778) |  | 0 | Multi-text applications, such as multi-document summarization, are typically required to model redundancies across related texts. Current methods confronting consolidation struggle to fuse overlapping information. In order to explicitly represent content overlap, we propose to align... | Daniela Brook Weiss, Paul Roit, Ayal Klein, Ori Ernst, Ido Dagan |  |
| 1248 |  |  [PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.779) |  | 0 | Large pre-trained language models for textual data have an unconstrained output space; at each decoding step, they can produce any of 10,000s of sub-word tokens. When fine-tuned to target constrained formal languages like SQL, these models often generate invalid code, rendering it unusable. We... | Torsten Scholak, Nathan Schucher, Dzmitry Bahdanau |  |
| 1249 |  |  [Exploiting Twitter as Source of Large Corpora of Weakly Similar Pairs for Semantic Sentence Embeddings](https://doi.org/10.18653/v1/2021.emnlp-main.780) |  | 0 | Semantic sentence embeddings are usually supervisedly built minimizing distances between pairs of embeddings of sentences labelled as semantically similar by annotators. Since big labelled datasets are rare, in particular for non-English languages, and expensive, recent studies focus on... | Marco Di Giovanni, Marco Brambilla |  |
| 1250 |  |  [Guilt by Association: Emotion Intensities in Lexical Representations](https://doi.org/10.18653/v1/2021.emnlp-main.781) |  | 0 | What do linguistic models reveal about the emotions associated with words? In this study, we consider the task of estimating word-level emotion intensity scores for specific emotions, exploring unsupervised, supervised, and finally a self-supervised method of extracting emotional associations from... | Shahab Raji, Gerard de Melo |  |
| 1251 |  |  [Using Sociolinguistic Variables to Reveal Changing Attitudes Towards Sexuality and Gender](https://doi.org/10.18653/v1/2021.emnlp-main.782) |  | 0 | Individuals signal aspects of their identity and beliefs through linguistic choices. Studying these choices in aggregate allows us to examine large-scale attitude shifts within a population. Here, we develop computational methods to study word choice within a sociolinguistic lexical... | Sky CHWang, David Jurgens |  |
| 1252 |  |  [Identifying Morality Frames in Political Tweets using Relational Learning](https://doi.org/10.18653/v1/2021.emnlp-main.783) |  | 0 | Extracting moral sentiment from text is a vital component in understanding public opinion, social movements, and policy decisions. The Moral Foundation Theory identifies five moral foundations, each associated with a positive and negative polarity. However, moral sentiment is often motivated by its... | Shamik Roy, Maria Leonor Pacheco, Dan Goldwasser |  |
| 1253 |  |  [Measuring Sentence-Level and Aspect-Level (Un)certainty in Science Communications](https://doi.org/10.18653/v1/2021.emnlp-main.784) |  | 0 | Certainty and uncertainty are fundamental to science communication. Hedges have widely been used as proxies for uncertainty. However, certainty is a complex construct, with authors expressing not only the degree but the type and aspects of uncertainty in order to give the reader a certain... | Jiaxin Pei, David Jurgens |  |
| 1254 |  |  [Assessing the Reliability of Word Embedding Gender Bias Measures](https://doi.org/10.18653/v1/2021.emnlp-main.785) |  | 0 | Various measures have been proposed to quantify human-like social biases in word embeddings. However, bias scores based on these measures can suffer from measurement error. One indication of measurement quality is reliability, concerning the extent to which a measure produces consistent results. In... | Yupei Du, Qixiang Fang, Dong Nguyen |  |
| 1255 |  |  [Rumor Detection on Twitter with Claim-Guided Hierarchical Graph Attention Networks](https://doi.org/10.18653/v1/2021.emnlp-main.786) |  | 0 | Rumors are rampant in the era of social media. Conversation structures provide valuable clues to differentiate between real and fake claims. However, existing rumor detection methods are either limited to the strict relation of user responses or oversimplify the conversation structure. In this... | Hongzhan Lin, Jing Ma, Mingfei Cheng, Zhiwei Yang, Liangliang Chen, Guang Chen |  |
| 1256 |  |  [Learning Bill Similarity with Annotated and Augmented Corpora of Bills](https://doi.org/10.18653/v1/2021.emnlp-main.787) |  | 0 | Bill writing is a critical element of representative democracy. However, it is often overlooked that most legislative bills are derived, or even directly copied, from other bills. Despite the significance of bill-to-bill linkages for understanding the legislative process, existing approaches fail... | Jiseon Kim, Elden Griggs, In Song Kim, Alice Oh |  |
| 1257 |  |  [SWEAT: Scoring Polarization of Topics across Different Corpora](https://doi.org/10.18653/v1/2021.emnlp-main.788) |  | 0 | Understanding differences of viewpoints across corpora is a fundamental task for computational social sciences. In this paper, we propose the Sliced Word Embedding Association Test (SWEAT), a novel statistical measure to compute the relative polarization of a topical wordset across two... | Federico Bianchi, Marco Marelli, Paolo Nicoli, Matteo Palmonari |  |
| 1258 |  |  ["So You Think You're Funny?": Rating the Humour Quotient in Standup Comedy](https://doi.org/10.18653/v1/2021.emnlp-main.789) |  | 0 | Computational Humour (CH) has attracted the interest of Natural Language Processing and Computational Linguistics communities. Creating datasets for automatic measurement of humour quotient is difficult due to multiple possible interpretations of the content. In this work, we create a multi-modal... | Anirudh Mittal, Pranav Jeevan, Prerak Gandhi, Diptesh Kanojia, Pushpak Bhattacharyya |  |
| 1259 |  |  ["Was it "stated" or was it "claimed"?: How linguistic bias affects generative language models](https://doi.org/10.18653/v1/2021.emnlp-main.790) |  | 0 | People use language in subtle and nuanced ways to convey their beliefs. For instance, saying claimed instead of said casts doubt on the truthfulness of the underlying proposition, thus representing the author’s opinion on the matter. Several works have identified such linguistic classes of words... | Roma Patel, Ellie Pavlick |  |
| 1260 |  |  [PAUSE: Positive and Annealed Unlabeled Sentence Embedding](https://doi.org/10.18653/v1/2021.emnlp-main.791) |  | 0 | Sentence embedding refers to a set of effective and versatile techniques for converting raw text into numerical vector representations that can be used in a wide range of natural language processing (NLP) applications. The majority of these techniques are either supervised or unsupervised. Compared... | Lele Cao, Emil Larsson, Vilhelm von Ehrenheim, Dhiana Deva Cavalcanti Rocha, Anna Martin, Sonja Horn |  |
| 1261 |  |  [A Simple Geometric Method for Cross-Lingual Linguistic Transformations with Pre-trained Autoencoders](https://doi.org/10.18653/v1/2021.emnlp-main.792) |  | 0 | Powerful sentence encoders trained for multiple languages are on the rise. These systems are capable of embedding a wide range of linguistic properties into vector representations. While explicit probing tasks can be used to verify the presence of specific linguistic properties, it is unclear... | Maarten De Raedt, Fréderic Godin, Pieter Buteneers, Chris Develder, Thomas Demeester |  |
| 1262 |  |  [An Information-Theoretic Characterization of Morphological Fusion](https://doi.org/10.18653/v1/2021.emnlp-main.793) |  | 0 | Linguistic typology generally divides synthetic languages into groups based on their morphological fusion. However, this measure has long been thought to be best considered a matter of degree. We present an information-theoretic measure, called informational fusion, to quantify the degree of fusion... | Neil Rathi, Michael Hahn, Richard Futrell |  |
| 1263 |  |  [The Effect of Efficient Messaging and Input Variability on Neural-Agent Iterated Language Learning](https://doi.org/10.18653/v1/2021.emnlp-main.794) |  | 0 | Natural languages display a trade-off among different strategies to convey syntactic structure, such as word order or inflection. This trade-off, however, has not appeared in recent simulations of iterated language learning with neural network agents (Chaabouni et al., 2019b). We re-evaluate this... | Yuchen Lian, Arianna Bisazza, Tessa Verhoef |  |
| 1264 |  |  [On Classifying whether Two Texts are on the Same Side of an Argument](https://doi.org/10.18653/v1/2021.emnlp-main.795) |  | 0 | To ease the difficulty of argument stance classification, the task of same side stance classification (S3C) has been proposed. In contrast to actual stance classification, which requires a substantial amount of domain knowledge to identify whether an argument is in favor or against a certain issue,... | Erik Körner, Gregor Wiedemann, Ahmad Dawar Hakimi, Gerhard Heyer, Martin Potthast |  |
| 1265 |  |  [Chinese Opinion Role Labeling with Corpus Translation: A Pivot Study](https://doi.org/10.18653/v1/2021.emnlp-main.796) |  | 0 | Opinion Role Labeling (ORL), aiming to identify the key roles of opinion, has received increasing interest. Unlike most of the previous works focusing on the English language, in this paper, we present the first work of Chinese ORL. We construct a Chinese dataset by manually translating and... | Ranran Zhen, Rui Wang, Guohong Fu, Chengguo Lv, Meishan Zhang |  |
| 1266 |  |  [MassiveSumm: a very large-scale, very multilingual, news summarisation dataset](https://doi.org/10.18653/v1/2021.emnlp-main.797) |  | 0 | Current research in automatic summarisation is unapologetically anglo-centered–a persistent state-of-affairs, which also predates neural net approaches. High-quality automatic summarisation datasets are notoriously expensive to create, posing a challenge for any language. However, with... | Daniel Varab, Natalie Schluter |  |
| 1267 |  |  [AUTOSUMM: Automatic Model Creation for Text Summarization](https://doi.org/10.18653/v1/2021.emnlp-main.798) |  | 0 | Recent efforts to develop deep learning models for text generation tasks such as extractive and abstractive summarization have resulted in state-of-the-art performances on various datasets. However, obtaining the best model configuration for a given dataset requires an extensive knowledge of deep... | Sharmila Reddy Nangi, Atharv Tyagi, Jay Mundra, Sagnik Mukherjee, Raj Snehal, Niyati Chhaya, Aparna Garimella |  |
| 1268 |  |  [Investigating the Helpfulness of Word-Level Quality Estimation for Post-Editing Machine Translation Output](https://doi.org/10.18653/v1/2021.emnlp-main.799) |  | 0 | Compared to fully manual translation, post-editing (PE) machine translation (MT) output can save time and reduce errors. Automatic word-level quality estimation (QE) aims to predict the correctness of words in MT output and holds great promise to aid PE by flagging problematic output. Quality of QE... | Raksha Shenoy, Nico Herbig, Antonio Krüger, Josef van Genabith |  |
| 1269 |  |  [UNKs Everywhere: Adapting Multilingual Language Models to New Scripts](https://doi.org/10.18653/v1/2021.emnlp-main.800) |  | 0 | Massively multilingual language models such as multilingual BERT offer state-of-the-art cross-lingual transfer performance on a range of NLP tasks. However, due to limited capacity and large differences in pretraining data sizes, there is a profound performance gap between resource-rich and... | Jonas Pfeiffer, Ivan Vulic, Iryna Gurevych, Sebastian Ruder |  |
| 1270 |  |  [Neural Machine Translation Quality and Post-Editing Performance](https://doi.org/10.18653/v1/2021.emnlp-main.801) |  | 0 | We test the natural expectation that using MT in professional translation saves human processing time. The last such study was carried out by Sanchez-Torron and Koehn (2016) with phrase-based MT, artificially reducing the translation quality. In contrast, we focus on neural MT (NMT) of high... | Vilém Zouhar, Martin Popel, Ondrej Bojar, Ales Tamchyna |  |
| 1271 |  |  [XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation](https://doi.org/10.18653/v1/2021.emnlp-main.802) |  | 0 | Machine learning has brought striking advances in multilingual natural language processing capabilities over the past year. For example, the latest techniques have improved the state-of-the-art performance on the XTREME multilingual benchmark by more than 13 points. While a sizeable gap to... | Sebastian Ruder, Noah Constant, Jan A. Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Dan Garrette, Graham Neubig, Melvin Johnson |  |
| 1272 |  |  [Contrastive Conditioning for Assessing Disambiguation in MT: A Case Study of Distilled Bias](https://doi.org/10.18653/v1/2021.emnlp-main.803) |  | 0 | Lexical disambiguation is a major challenge for machine translation systems, especially if some senses of a word are trained less often than others. Identifying patterns of overgeneralization requires evaluation methods that are both reliable and scalable. We propose contrastive conditioning as a... | Jannis Vamvas, Rico Sennrich |  |
| 1273 |  |  [Measuring Association Between Labels and Free-Text Rationales](https://doi.org/10.18653/v1/2021.emnlp-main.804) |  | 0 | In interpretable NLP, we require faithful rationales that reflect the model’s decision-making process for an explained instance. While prior work focuses on extractive rationales (a subset of the input words), we investigate their less-studied counterpart: free-text natural language rationales. We... | Sarah Wiegreffe, Ana Marasovic, Noah A. Smith |  |
| 1274 |  |  [Discretized Integrated Gradients for Explaining Language Models](https://doi.org/10.18653/v1/2021.emnlp-main.805) |  | 0 | As a prominent attribution-based explanation algorithm, Integrated Gradients (IG) is widely adopted due to its desirable explanation axioms and the ease of gradient computation. It measures feature importance by averaging the model’s output gradient interpolated along a straight-line path in the... | Soumya Sanyal, Xiang Ren |  |
| 1275 |  |  [Putting Words in BERT's Mouth: Navigating Contextualized Vector Spaces with Pseudowords](https://doi.org/10.18653/v1/2021.emnlp-main.806) |  | 0 | We present a method for exploring regions around individual points in a contextualized vector space (particularly, BERT space), as a way to investigate how these regions correspond to word senses. By inducing a contextualized “pseudoword” vector as a stand-in for a static embedding in the input... | Taelin Karidi, Yichu Zhou, Nathan Schneider, Omri Abend, Vivek Srikumar |  |
| 1276 |  |  [Rationales for Sequential Predictions](https://doi.org/10.18653/v1/2021.emnlp-main.807) |  | 0 | Sequence models are a critical component of modern NLP systems, but their predictions are difficult to explain. We consider model explanations though rationales, subsets of context that can explain individual model predictions. We find sequential rationales by solving a combinatorial optimization:... | Keyon Vafa, Yuntian Deng, David M. Blei, Alexander M. Rush |  |
| 1277 |  |  [FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging](https://doi.org/10.18653/v1/2021.emnlp-main.808) |  | 0 | Influence functions approximate the “influences” of training data-points for test predictions and have a wide variety of applications. Despite the popularity, their computational cost does not scale well with model and training data size. We present FastIF, a set of simple modifications to... | Han Guo, Nazneen Rajani, Peter Hase, Mohit Bansal, Caiming Xiong |  |
| 1278 |  |  [Studying word order through iterative shuffling](https://doi.org/10.18653/v1/2021.emnlp-main.809) |  | 0 | As neural language models approach human performance on NLP benchmark tasks, their advances are widely seen as evidence of an increasingly complex understanding of syntax. This view rests upon a hypothesis that has not yet been empirically tested: that word order encodes meaning essential to... | Nikolay Malkin, Sameera Lanka, Pranav Goel, Nebojsa Jojic |  |
| 1279 |  |  [Distantly-Supervised Named Entity Recognition with Noise-Robust Learning and Language Model Augmented Self-Training](https://doi.org/10.18653/v1/2021.emnlp-main.810) |  | 0 | We study the problem of training named entity recognition (NER) models using only distantly-labeled data, which can be automatically obtained by matching entity mentions in the raw text with entity types in a knowledge base. The biggest challenge of distantly-supervised NER is that the distant... | Yu Meng, Yunyi Zhang, Jiaxin Huang, Xuan Wang, Yu Zhang, Heng Ji, Jiawei Han |  |
| 1280 |  |  [Open Knowledge Graphs Canonicalization using Variational Autoencoders](https://doi.org/10.18653/v1/2021.emnlp-main.811) |  | 0 | Noun phrases and Relation phrases in open knowledge graphs are not canonicalized, leading to an explosion of redundant and ambiguous subject-relation-object triples. Existing approaches to solve this problem take a two-step approach. First, they generate embedding representations for both noun and... | Sarthak Dash, Gaetano Rossiello, Nandana Mihindukulasooriya, Sugato Bagchi, Alfio Gliozzo |  |
| 1281 |  |  [HittER: Hierarchical Transformers for Knowledge Graph Embeddings](https://doi.org/10.18653/v1/2021.emnlp-main.812) |  | 0 | This paper examines the challenging problem of learning representations of entities and relations in a complex multi-relational knowledge graph. We propose HittER, a Hierarchical Transformer model to jointly learn Entity-relation composition and Relational contextualization based on a source... | Sanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao, Ruofei Zhang, Yangfeng Ji |  |
| 1282 |  |  [Few-Shot Named Entity Recognition: An Empirical Baseline Study](https://doi.org/10.18653/v1/2021.emnlp-main.813) |  | 0 | This paper presents an empirical study to efficiently build named entity recognition (NER) systems when a small amount of in-domain labeled data is available. Based upon recent Transformer-based self-supervised pre-trained language models (PLMs), we investigate three orthogonal schemes to improve... | Jiaxin Huang, Chunyuan Li, Krishan Subudhi, Damien Jose, Shobana Balakrishnan, Weizhu Chen, Baolin Peng, Jianfeng Gao, Jiawei Han |  |
| 1283 |  |  [XLEnt: Mining a Large Cross-lingual Entity Dataset with Lexical-Semantic-Phonetic Word Alignment](https://doi.org/10.18653/v1/2021.emnlp-main.814) |  | 0 | Cross-lingual named-entity lexica are an important resource to multilingual NLP tasks such as machine translation and cross-lingual wikification. While knowledge bases contain a large number of entities in high-resource languages such as English and French, corresponding entities for lower-resource... | Ahmed ElKishky, Adithya Renduchintala, James Cross, Francisco Guzmán, Philipp Koehn |  |
| 1284 |  |  [Utilizing Relative Event Time to Enhance Event-Event Temporal Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.815) |  | 0 | Event time is one of the most important features for event-event temporal relation extraction. However, explicit event time information in text is sparse. For example, only about 20% of event mentions in TimeBank-Dense have event-time links. In this paper, we propose a joint model for event-event... | Haoyang Wen, Heng Ji |  |
| 1285 |  |  [Separating Retention from Extraction in the Evaluation of End-to-end Relation Extraction](https://doi.org/10.18653/v1/2021.emnlp-main.816) |  | 0 | State-of-the-art NLP models can adopt shallow heuristics that limit their generalization capability (McCoy et al., 2019). Such heuristics include lexical overlap with the training set in Named-Entity Recognition (Taille et al., 2020) and Event or Type heuristics in Relation Extraction (Rosenman et... | Bruno Taillé, Vincent Guigue, Geoffrey Scoutheeten, Patrick Gallinari |  |
| 1286 |  |  [Automatic Text Evaluation through the Lens of Wasserstein Barycenters](https://doi.org/10.18653/v1/2021.emnlp-main.817) |  | 0 | A new metric BaryScore to evaluate text generation based on deep contextualized embeddings (e.g., BERT, Roberta, ELMo) is introduced. This metric is motivated by a new framework relying on optimal transport tools, i.e., Wasserstein distance and barycenter. By modelling the layer output of deep... | Pierre Colombo, Guillaume Staerman, Chloé Clavel, Pablo Piantanida |  |
| 1287 |  |  [Visually Grounded Reasoning across Languages and Cultures](https://doi.org/10.18653/v1/2021.emnlp-main.818) |  | 0 | The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical... | Fangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti, Siva Reddy, Nigel Collier, Desmond Elliott |  |
| 1288 |  |  [Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema](https://doi.org/10.18653/v1/2021.emnlp-main.819) |  | 0 | The Winograd Schema (WS) has been proposed as a test for measuring commonsense capabilities of models. Recently, pre-trained language model-based approaches have boosted performance on some WS benchmarks but the source of improvement is still not clear. This paper suggests that the apparent... | Yanai Elazar, Hongming Zhang, Yoav Goldberg, Dan Roth |  |
| 1289 |  |  [Robustness Evaluation of Entity Disambiguation Using Prior Probes: the Case of Entity Overshadowing](https://doi.org/10.18653/v1/2021.emnlp-main.820) |  | 0 | Entity disambiguation (ED) is the last step of entity linking (EL), when candidate entities are reranked according to the context they appear in. All datasets for training and evaluating models for EL consist of convenience samples, such as news articles and tweets, that propagate the prior... | Vera Provatorova, Samarth Bhargav, Svitlana Vakulenko, Evangelos Kanoulas |  |
| 1290 |  |  [IndoNLI: A Natural Language Inference Dataset for Indonesian](https://doi.org/10.18653/v1/2021.emnlp-main.821) |  | 0 | We present IndoNLI, the first human-elicited NLI dataset for Indonesian. We adapt the data collection protocol for MNLI and collect ~18K sentence pairs annotated by crowd workers and experts. The expert-annotated data is used exclusively as a test set. It is designed to provide a challenging... | Rahmad Mahendra, Alham Fikri Aji, Samuel Louvan, Fahrurrozi Rahman, Clara Vania |  |
| 1291 |  |  [Agreeing to Disagree: Annotating Offensive Language Datasets with Annotators' Disagreement](https://doi.org/10.18653/v1/2021.emnlp-main.822) |  | 0 | Since state-of-the-art approaches to offensive language detection rely on supervised learning, it is crucial to quickly adapt them to the continuously evolving scenario of social media. While several approaches have been proposed to tackle the problem from an algorithmic perspective, so to reduce... | Elisa Leonardelli, Stefano Menini, Alessio Palmero Aprosio, Marco Guerini, Sara Tonelli |  |
| 1292 |  |  [A Root of a Problem: Optimizing Single-Root Dependency Parsing](https://doi.org/10.18653/v1/2021.emnlp-main.823) |  | 0 | We describe two approaches to single-root dependency parsing that yield significant speed ups in such parsing. One approach has been previously used in dependency parsers in practice, but remains undocumented in the parsing literature, and is considered a heuristic. We show that this approach... | Milos Stanojevic, Shay B. Cohen |  |
| 1293 |  |  [Efficient Sampling of Dependency Structure](https://doi.org/10.18653/v1/2021.emnlp-main.824) |  | 0 | Probabilistic distributions over spanning trees in directed graphs are a fundamental model of dependency structure in natural language processing, syntactic dependency trees. In NLP, dependency trees often have an additional root constraint: only one edge may emanate from the root. However, no... | Ran Zmigrod, Tim Vieira, Ryan Cotterell |  |
| 1294 |  |  [Reducing Discontinuous to Continuous Parsing with Pointer Network Reordering](https://doi.org/10.18653/v1/2021.emnlp-main.825) |  | 0 | Discontinuous constituent parsers have always lagged behind continuous approaches in terms of accuracy and speed, as the presence of constituents with discontinuous yield introduces extra complexity to the task. However, a discontinuous tree can be converted into a continuous variant by reordering... | Daniel FernándezGonzález, Carlos GómezRodríguez |  |
| 1295 |  |  [A New Representation for Span-based CCG Parsing](https://doi.org/10.18653/v1/2021.emnlp-main.826) |  | 0 | This paper proposes a new representation for CCG derivations. CCG derivations are represented as trees whose nodes are labeled with categories strictly restricted by CCG rule schemata. This characteristic is not suitable for span-based parsing models because they predict node labels independently.... | Yoshihide Kato, Shigeki Matsubara |  |
| 1296 |  |  [What to Pre-Train on? Efficient Intermediate Task Selection](https://doi.org/10.18653/v1/2021.emnlp-main.827) |  | 0 | Intermediate task fine-tuning has been shown to culminate in large transfer gains across many NLP tasks. With an abundance of candidate datasets as well as pre-trained language models, it has become infeasible to experiment with all combinations to find the best transfer setting. In this work, we... | Clifton Poth, Jonas Pfeiffer, Andreas Rücklé, Iryna Gurevych |  |
| 1297 |  |  [PermuteFormer: Efficient Relative Position Encoding for Long Sequences](https://doi.org/10.18653/v1/2021.emnlp-main.828) |  | 0 | A recent variation of Transformer, Performer, scales Transformer to longer sequences with a linear attention mechanism. However, it is not compatible with relative position encoding, which has advantages over absolute position encoding. In this paper, we discuss possible ways to add relative... | Peng Chen |  |
| 1298 |  |  [Block Pruning For Faster Transformers](https://doi.org/10.18653/v1/2021.emnlp-main.829) |  | 0 | Pre-training has improved model accuracy for both classification and generation tasks at the cost of introducing much larger and slower models. Pruning methods have proven to be an effective way of reducing model size, whereas distillation methods are proven for speeding up inference. We introduce... | François Lagunas, Ella Charlaix, Victor Sanh, Alexander M. Rush |  |
| 1299 |  |  [Finetuning Pretrained Transformers into RNNs](https://doi.org/10.18653/v1/2021.emnlp-main.830) |  | 0 | Transformers have outperformed recurrent neural networks (RNNs) in natural language generation. But this comes with a signifi- cant computational cost, as the attention mechanism’s complexity scales quadratically with sequence length. Efficient transformer variants have received increasing interest... | Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, Noah A. Smith |  |
| 1300 |  |  [How to Train BERT with an Academic Budget](https://doi.org/10.18653/v1/2021.emnlp-main.831) |  | 0 | While large language models a la BERT are used ubiquitously in NLP, pretraining them is considered a luxury that only a few well-funded industry labs can afford. How can one train such models with a more modest budget? We present a recipe for pretraining a masked language model in 24 hours using a... | Peter Izsak, Moshe Berchansky, Omer Levy |  |
| 1301 |  |  [Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression](https://doi.org/10.18653/v1/2021.emnlp-main.832) |  | 0 | Recent studies on compression of pretrained language models (e.g., BERT) usually use preserved accuracy as the metric for evaluation. In this paper, we propose two new metrics, label loyalty and probability loyalty that measure how closely a compressed model (i.e., student) mimics the original... | Canwen Xu, Wangchunshu Zhou, Tao Ge, Ke Xu, Julian J. McAuley, Furu Wei |  |
| 1302 |  |  [IndoBERTweet: A Pretrained Language Model for Indonesian Twitter with Effective Domain-Specific Vocabulary Initialization](https://doi.org/10.18653/v1/2021.emnlp-main.833) |  | 0 | We present IndoBERTweet, the first large-scale pretrained model for Indonesian Twitter that is trained by extending a monolingually-trained Indonesian BERT model with additive domain-specific vocabulary. We focus in particular on efficient model adaptation under vocabulary mismatch, and benchmark... | Fajri Koto, Jey Han Lau, Timothy Baldwin |  |
| 1303 |  |  [Pushing on Text Readability Assessment: A Transformer Meets Handcrafted Linguistic Features](https://doi.org/10.18653/v1/2021.emnlp-main.834) |  | 0 | We report two essential improvements in readability assessment: 1. three novel features in advanced semantics and 2. the timely evidence that traditional ML models (e.g. Random Forest, using handcrafted features) can combine with transformers (e.g. RoBERTa) to augment model performance. First, we... | Bruce W. Lee, Yoo Sung Jang, Jason HyungJong Lee |  |
| 1304 |  |  [Types of Out-of-Distribution Texts and How to Detect Them](https://doi.org/10.18653/v1/2021.emnlp-main.835) |  | 0 | Despite agreement on the importance of detecting out-of-distribution (OOD) examples, there is little consensus on the formal definition of the distribution shifts of OOD examples and how to best detect them. We categorize these examples as exhibiting a background shift or semantic shift, and find... | Udit Arora, William Huang, He He |  |
| 1305 |  |  [Self-training with Few-shot Rationalization](https://doi.org/10.18653/v1/2021.emnlp-main.836) |  | 0 | While pre-trained language models have obtained state-of-the-art performance for several natural language understanding tasks, they are quite opaque in terms of their decision-making process. While some recent works focus on rationalizing neural predictions by highlighting salient concepts in the... | Meghana Moorthy Bhat, Alessandro Sordoni, Subhabrata Mukherjee |  |
| 1306 |  |  [MTAdam: Automatic Balancing of Multiple Training Loss Terms](https://doi.org/10.18653/v1/2021.emnlp-main.837) |  | 0 | When training neural models, it is common to combine multiple loss terms. The balancing of these terms requires considerable human effort and is computationally demanding. Moreover, the optimal trade-off between the loss terms can change as training progresses, e.g., for adversarial terms. In this... | Itzik Malkiel, Lior Wolf |  |
| 1307 |  |  [Softmax Tree: An Accurate, Fast Classifier When the Number of Classes Is Large](https://doi.org/10.18653/v1/2021.emnlp-main.838) |  | 0 | Classification problems having thousands or more classes naturally occur in NLP, for example language models or document classification. A softmax or one-vs-all classifier naturally handles many classes, but it is very slow at inference time, because every class score must be calculated to find the... | Arman Zharmagambetov, Magzhan Gabidolla, Miguel Á. CarreiraPerpiñán |  |
| 1308 |  |  [Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning](https://doi.org/10.18653/v1/2021.emnlp-main.839) |  | 0 | Distantly supervised named entity recognition (DS-NER) efficiently reduces labor costs but meanwhile intrinsically suffers from the label noise due to the strong assumption of distant supervision. Typically, the wrongly labeled instances comprise numbers of incomplete and inaccurate annotations,... | Xinghua Zhang, Bowen Yu, Tingwen Liu, Zhenyu Zhang, Jiawei Sheng, Mengge Xue, Hongbo Xu |  |
| 1309 |  |  [Multivalent Entailment Graphs for Question Answering](https://doi.org/10.18653/v1/2021.emnlp-main.840) |  | 0 | Drawing inferences between open-domain natural language predicates is a necessity for true language understanding. There has been much progress in unsupervised learning of entailment graphs for this purpose. We make three contributions: (1) we reinterpret the Distributional Inclusion Hypothesis to... | Nick McKenna, Liane Guillou, Mohammad Javad Hosseini, Sander Bijl de Vroe, Mark Johnson, Mark Steedman |  |
| 1310 |  |  [Is Everything in Order? A Simple Way to Order Sentences](https://doi.org/10.18653/v1/2021.emnlp-main.841) |  | 0 | The task of organizing a shuffled set of sentences into a coherent text has been used to evaluate a machine’s understanding of causal and temporal relations. We formulate the sentence ordering task as a conditional text-to-marker generation problem. We present Reorder-BART (Re-BART) that leverages... | Somnath Basu Roy Chowdhury, Faeze Brahman, Snigdha Chaturvedi |  |
| 1311 |  |  [VeeAlign: Multifaceted Context Representation Using Dual Attention for Ontology Alignment](https://doi.org/10.18653/v1/2021.emnlp-main.842) |  | 0 | Ontology Alignment is an important research problem applied to various fields such as data integration, data transfer, data preparation, etc. State-of-the-art (SOTA) Ontology Alignment systems typically use naive domain-dependent approaches with handcrafted rules or domain-specific architectures,... | Vivek Iyer, Arvind Agarwal, Harshit Kumar |  |
| 1312 |  |  [Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization](https://doi.org/10.18653/v1/2021.emnlp-main.843) |  | 0 | Modern semantic parsers suffer from two principal limitations. First, training requires expensive collection of utterance-program pairs. Second, semantic parsers fail to generalize at test time to new compositions/structures that have not been observed during training. Recent research has shown... | Inbar Oren, Jonathan Herzig, Jonathan Berant |  |
| 1313 |  |  [GeneSis: A Generative Approach to Substitutes in Context](https://doi.org/10.18653/v1/2021.emnlp-main.844) |  | 0 | The lexical substitution task aims at generating a list of suitable replacements for a target word in context, ideally keeping the meaning of the modified text unchanged. While its usage has increased in recent years, the paucity of annotated data prevents the finetuning of neural models on the... | Caterina Lacerra, Rocco Tripodi, Roberto Navigli |  |
| 1314 |  |  [Semi-Supervised Exaggeration Detection of Health Science Press Releases](https://doi.org/10.18653/v1/2021.emnlp-main.845) |  | 0 | Public trust in science depends on honest and factual communication of scientific papers. However, recent studies have demonstrated a tendency of news media to misrepresent scientific papers by exaggerating their findings. Given this, we present a formalization of and study into the problem of... | Dustin Wright, Isabelle Augenstein |  |
| 1315 |  |  [Phrase-BERT: Improved Phrase Embeddings from BERT with an Application to Corpus Exploration](https://doi.org/10.18653/v1/2021.emnlp-main.846) |  | 0 | Phrase representations derived from BERT often do not exhibit complex phrasal compositionality, as the model relies instead on lexical similarity to determine semantic relatedness. In this paper, we propose a contrastive fine-tuning objective that enables BERT to produce more powerful phrase... | Shufan Wang, Laure Thompson, Mohit Iyyer |  |
| 1316 |  |  [Detecting Contact-Induced Semantic Shifts: What Can Embedding-Based Methods Do in Practice?](https://doi.org/10.18653/v1/2021.emnlp-main.847) |  | 0 | This study investigates the applicability of semantic change detection methods in descriptively oriented linguistic research. It specifically focuses on contact-induced semantic shifts in Quebec English. We contrast synchronic data from different regions in order to identify the meanings that are... | Filip Miletic, Anne PrzewoznyDesriaux, Ludovic Tanguy |  |
