# EMNLP2020

## 会议论文列表

本会议共有 1236 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [OpenUE: An Open Toolkit of Universal Extraction from Text](https://doi.org/10.18653/v1/2020.emnlp-demos.1) |  | 0 | Natural language processing covers a wide variety of tasks with token-level or sentence-level understandings. In this paper, we provide a simple insight that most tasks can be represented in a single universal extraction format. We introduce a prototype model and provide an open-source and... | Ningyu Zhang, Shumin Deng, Zhen Bi, Haiyang Yu, Jiacheng Yang, Mosha Chen, Fei Huang, Wei Zhang, Huajun Chen |  |
| 2 |  |  [BERTweet: A pre-trained language model for English Tweets](https://doi.org/10.18653/v1/2020.emnlp-demos.2) |  | 0 | We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong... | Dat Quoc Nguyen, Thanh Vu, Anh Tuan Nguyen |  |
| 3 |  |  [NeuralQA: A Usable Library for Question Answering (Contextual Query Expansion + BERT) on Large Datasets](https://doi.org/10.18653/v1/2020.emnlp-demos.3) |  | 0 | Existing tools for Question Answering (QA) have challenges that limit their use in practice. They can be complex to set up or integrate with existing infrastructure, do not offer configurable interactive interfaces, and do not cover the full set of subtasks that frequently comprise the QA pipeline... | Victor Dibia |  |
| 4 |  |  [Wikipedia2Vec: An Efficient Toolkit for Learning and Visualizing the Embeddings of Words and Entities from Wikipedia](https://doi.org/10.18653/v1/2020.emnlp-demos.4) |  | 0 | The embeddings of entities in a large knowledge base (e.g., Wikipedia) are highly beneficial for solving various natural language tasks that involve real world knowledge. In this paper, we present Wikipedia2Vec, a Python-based open-source tool for learning the embeddings of words and entities from... | Ikuya Yamada, Akari Asai, Jin Sakuma, Hiroyuki Shindo, Hideaki Takeda, Yoshiyasu Takefuji, Yuji Matsumoto |  |
| 5 |  |  [ARES: A Reading Comprehension Ensembling Service](https://doi.org/10.18653/v1/2020.emnlp-demos.5) |  | 0 | We introduce ARES (A Reading Comprehension Ensembling Service): a novel Machine Reading Comprehension (MRC) demonstration system which utilizes an ensemble of models to increase F1 by 2.3 points. While many of the top leaderboard submissions in popular MRC benchmarks such as the Stanford Question... | Anthony Ferritto, Lin Pan, Rishav Chakravarti, Salim Roukos, Radu Florian, J. William Murdock, Avirup Sil |  |
| 6 |  |  [Transformers: State-of-the-Art Natural Language Processing](https://doi.org/10.18653/v1/2020.emnlp-demos.6) |  | 0 | Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of... | Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush |  |
| 7 |  |  [AdapterHub: A Framework for Adapting Transformers](https://doi.org/10.18653/v1/2020.emnlp-demos.7) |  | 0 | The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of millions or billions of parameters. Storing and sharing such large trained models is expensive, slow, and time-consuming, which impedes progress towards more general and versatile NLP methods that... | Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vulic, Sebastian Ruder, Kyunghyun Cho, Iryna Gurevych |  |
| 8 |  |  [HUMAN: Hierarchical Universal Modular ANnotator](https://doi.org/10.18653/v1/2020.emnlp-demos.8) |  | 0 | A lot of real-world phenomena are complex and cannot be captured by single task annotations. This causes a need for subsequent annotations, with interdependent questions and answers describing the nature of the subject at hand. Even in the case a phenomenon is easily captured by a single task, the... | Moritz Wolf, Dana Ruiter, Ashwin Geet D'Sa, Liane Reiners, Jan Alexandersson, Dietrich Klakow |  |
| 9 |  |  [DeezyMatch: A Flexible Deep Learning Approach to Fuzzy String Matching](https://doi.org/10.18653/v1/2020.emnlp-demos.9) |  | 0 | We present DeezyMatch, a free, open-source software library written in Python for fuzzy string matching and candidate ranking. Its pair classifier supports various deep neural network architectures for training new classifiers and for fine-tuning a pretrained model, which paves the way for transfer... | Kasra Hosseini, Federico Nanni, Mariona Coll Ardanuy |  |
| 10 |  |  [CoSaTa: A Constraint Satisfaction Solver and Interpreted Language for Semi-Structured Tables of Sentences](https://doi.org/10.18653/v1/2020.emnlp-demos.10) |  | 0 | This work presents CoSaTa, an intuitive constraint satisfaction solver and interpreted language for knowledge bases of semi-structured tables expressed as text. The stand-alone CoSaTa solver allows easily expressing complex compositional “inference patterns” for how knowledge from different tables... | Peter Jansen |  |
| 11 |  |  [InVeRo: Making Semantic Role Labeling Accessible with Intelligible Verbs and Roles](https://doi.org/10.18653/v1/2020.emnlp-demos.11) |  | 0 | Semantic Role Labeling (SRL) is deeply dependent on complex linguistic resources and sophisticated neural models, which makes the task difficult to approach for non-experts. To address this issue we present a new platform named Intelligible Verbs and Roles (InVeRo). This platform provides access to... | Simone Conia, Fabrizio Brignone, Davide Zanfardino, Roberto Navigli |  |
| 12 |  |  [Youling: an AI-assisted Lyrics Creation System](https://doi.org/10.18653/v1/2020.emnlp-demos.12) |  | 0 | Recently, a variety of neural models have been proposed for lyrics generation. However, most previous work completes the generation process in a single pass with little human intervention. We believe that lyrics creation is a creative process with human intelligence centered. AI should play a role... | Rongsheng Zhang, Xiaoxi Mao, Le Li, Lin Jiang, Lin Chen, Zhiwei Hu, Yadong Xi, Changjie Fan, Minlie Huang |  |
| 13 |  |  [A Technical Question Answering System with Transfer Learning](https://doi.org/10.18653/v1/2020.emnlp-demos.13) |  | 0 | In recent years, the need for community technical question-answering sites has increased significantly. However, it is often expensive for human experts to provide timely and helpful responses on those forums. We develop TransTQA, which is a novel system that offers automatic responses by retrieving... | Wenhao Yu, Lingfei Wu, Yu Deng, Ruchi Mahindru, Qingkai Zeng, Sinem Güven, Meng Jiang |  |
| 14 |  |  [ENTYFI: A System for Fine-grained Entity Typing in Fictional Texts](https://doi.org/10.18653/v1/2020.emnlp-demos.14) |  | 0 | Fiction and fantasy are archetypes of long-tail domains that lack suitable NLP methodologies and tools. We present ENTYFI, a web-based system for fine-grained typing of entity mentions in fictional texts. It builds on 205 automatically induced high-quality type systems for popular fictional domains,... | Cuong Xuan Chu, Simon Razniewski, Gerhard Weikum |  |
| 15 |  |  [The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models](https://doi.org/10.18653/v1/2020.emnlp-demos.15) |  | 0 | We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior: Why did my model make this prediction? When does it perform poorly? What happens under a controlled change in the input?... | Ian Tenney, James Wexler, Jasmijn Bastings, Tolga Bolukbasi, Andy Coenen, Sebastian Gehrmann, Ellen Jiang, Mahima Pushkarna, Carey Radebaugh, Emily Reif, Ann Yuan |  |
| 16 |  |  [TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP](https://doi.org/10.18653/v1/2020.emnlp-demos.16) |  | 0 | While there has been substantial research using adversarial attacks to analyze NLP models, each attack is implemented in its own code repository. It remains challenging to develop NLP attacks and utilize them to improve model performance. This paper introduces TextAttack, a Python framework for... | John X. Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, Yanjun Qi |  |
| 17 |  |  [Easy, Reproducible and Quality-Controlled Data Collection with CROWDAQ](https://doi.org/10.18653/v1/2020.emnlp-demos.17) |  | 0 | High-quality and large-scale data are key to success for AI systems. However, large-scale data annotation efforts are often confronted with a set of common challenges: (1) designing a user-friendly annotation interface; (2) training enough annotators efficiently; and (3) reproducibility. To address... | Qiang Ning, Hao Wu, Pradeep Dasigi, Dheeru Dua, Matt Gardner, Robert L. Logan IV, Ana Marasovic, Zhen Nie |  |
| 18 |  |  [SciSight: Combining faceted navigation and research group detection for COVID-19 exploratory scientific search](https://doi.org/10.18653/v1/2020.emnlp-demos.18) |  | 0 | The COVID-19 pandemic has sparked unprecedented mobilization of scientists, generating a deluge of papers that makes it hard for researchers to keep track and explore new directions. Search engines are designed for targeted queries, not for discovery of connections across a corpus. In this paper, we... | Tom Hope, Jason Portenoy, Kishore Vasan, Jonathan Borchardt, Eric Horvitz, Daniel S. Weld, Marti A. Hearst, Jevin West |  |
| 19 |  |  [SIMULEVAL: An Evaluation Toolkit for Simultaneous Translation](https://doi.org/10.18653/v1/2020.emnlp-demos.19) |  | 0 | Simultaneous translation on both text and speech focuses on a real-time and low-latency scenario where the model starts translating before reading the complete source input. Evaluating simultaneous translation models is more complex than offline models because the latency is another factor to... | Xutai Ma, Mohammad Javad Dousti, Changhan Wang, Jiatao Gu, Juan Miguel Pino |  |
| 20 |  |  [Agent Assist through Conversation Analysis](https://doi.org/10.18653/v1/2020.emnlp-demos.20) |  | 0 | Customer support agents play a crucial role as an interface between an organization and its end-users. We propose CAIRAA: Conversational Approach to Information Retrieval for Agent Assistance, to reduce the cognitive workload of support agents who engage with users through conversation systems.... | Kshitij Fadnis, Nathaniel Mills, Jatin Ganhotra, Haggai Roitman, Gaurav Pandey, Doron Cohen, Yosi Mass, Shai Erera, R. Chulaka Gunasekara, Danish Contractor, Siva Sankalp Patel, Q. Vera Liao, Sachindra Joshi, Luis A. Lastras, David Konopnicki |  |
| 21 |  |  [NeuSpell: A Neural Spelling Correction Toolkit](https://doi.org/10.18653/v1/2020.emnlp-demos.21) |  | 0 | We introduce NeuSpell, an open-source toolkit for spelling correction in English. Our toolkit comprises ten different models, and benchmarks them on naturally occurring misspellings from multiple sources. We find that many systems do not adequately leverage the context around the misspelt token. To... | Sai Muralidhar Jayanthi, Danish Pruthi, Graham Neubig |  |
| 22 |  |  [LibKGE - A knowledge graph embedding library for reproducible research](https://doi.org/10.18653/v1/2020.emnlp-demos.22) |  | 0 | LibKGE ( https://github.com/uma-pi1/kge ) is an open-source PyTorch-based library for training, hyperparameter optimization, and evaluation of knowledge graph embedding models for link prediction. The key goals of LibKGE are to enable reproducible research, to provide a framework for comprehensive... | Samuel Broscheit, Daniel Ruffinelli, Adrian Kochsiek, Patrick Betz, Rainer Gemulla |  |
| 23 |  |  [WantWords: An Open-source Online Reverse Dictionary System](https://doi.org/10.18653/v1/2020.emnlp-demos.23) |  | 0 | A reverse dictionary takes descriptions of words as input and outputs words semantically matching the input descriptions. Reverse dictionaries have great practical value such as solving the tip-of-the-tongue problem and helping new language learners. There have been some online reverse dictionary... | Fanchao Qi, Lei Zhang, Yanhui Yang, Zhiyuan Liu, Maosong Sun |  |
| 24 |  |  [BENNERD: A Neural Named Entity Linking System for COVID-19](https://doi.org/10.18653/v1/2020.emnlp-demos.24) |  | 0 | We present a biomedical entity linking (EL) system BENNERD that detects named enti- ties in text and links them to the unified medical language system (UMLS) knowledge base (KB) entries to facilitate the corona virus disease 2019 (COVID-19) research. BEN- NERD mainly covers biomedical domain, es-... | Mohammad Golam Sohrab, Khoa Duong, Makoto Miwa, Goran Topic, Ikeda Masami, Hiroya Takamura |  |
| 25 |  |  [RoFT: A Tool for Evaluating Human Detection of Machine-Generated Text](https://doi.org/10.18653/v1/2020.emnlp-demos.25) |  | 0 | In recent years, large neural networks for natural language generation (NLG) have made leaps and bounds in their ability to generate fluent text. However, the tasks of evaluating quality differences between NLG systems and understanding how humans perceive the generated text remain both crucial and... | Liam Dugan, Daphne Ippolito, Arun Kirubarajan, Chris CallisonBurch |  |
| 26 |  |  [A Data-Centric Framework for Composable NLP Workflows](https://doi.org/10.18653/v1/2020.emnlp-demos.26) |  | 0 | Empirical natural language processing (NLP) systems in application domains (e.g., healthcare, finance, education) involve interoperation among multiple components, ranging from data ingestion, human annotation, to text retrieval, analysis, generation, and visualization. We establish a unified... | Zhengzhong Liu, Guanxiong Ding, Avinash Bukkittu, Mansi Gupta, Pengzhi Gao, Atif Ahmed, Shikun Zhang, Xin Gao, Swapnil Singhavi, Linwei Li, Wei Wei, Zecong Hu, Haoran Shi, Xiaodan Liang, Teruko Mitamura, Eric P. Xing, Zhiting Hu |  |
| 27 |  |  [CoRefi: A Crowd Sourcing Suite for Coreference Annotation](https://doi.org/10.18653/v1/2020.emnlp-demos.27) |  | 0 | Coreference annotation is an important, yet expensive and time consuming, task, which often involved expert annotators trained on complex decision guidelines. To enable cheaper and more efficient annotation, we present CoRefi, a web-based coreference annotation suite, oriented for crowdsourcing.... | Aaron Bornstein, Arie Cattan, Ido Dagan |  |
| 28 |  |  [Langsmith: An Interactive Academic Text Revision System](https://doi.org/10.18653/v1/2020.emnlp-demos.28) |  | 0 | Despite the current diversity and inclusion initiatives in the academic community, researchers with a non-native command of English still face significant obstacles when writing papers in English. This paper presents the Langsmith editor, which assists inexperienced, non-native researchers to write... | Takumi Ito, Tatsuki Kuribayashi, Masatoshi Hidaka, Jun Suzuki, Kentaro Inui |  |
| 29 |  |  [IsOBS: An Information System for Oracle Bone Script](https://doi.org/10.18653/v1/2020.emnlp-demos.29) |  | 0 | Oracle bone script (OBS) is the earliest known ancient Chinese writing system and the ancestor of modern Chinese. As the Chinese writing system is the oldest continuously-used system in the world, the study of OBS plays an important role in both linguistic and historical research. In order to... | Xu Han, Yuzhuo Bai, Keyue Qiu, Zhiyuan Liu, Maosong Sun |  |
| 30 |  |  [Machine Reasoning: Technology, Dilemma and Future](https://doi.org/10.18653/v1/2020.emnlp-tutorials.1) |  | 0 | Machine reasoning research aims to build interpretable AI systems that can solve problems or draw conclusions from what they are told (i.e. facts and observations) and already know (i.e. models, common sense and knowledge) under certain constraints. In this tutorial, we will (1) describe the... | Nan Duan, Duyu Tang, Ming Zhou |  |
| 31 |  |  [Fact-Checking, Fake News, Propaganda, and Media Bias: Truth Seeking in the Post-Truth Era](https://doi.org/10.18653/v1/2020.emnlp-tutorials.2) |  | 0 | The rise of social media has democratized content creation and has made it easy for everybody to share and spread information online. On the positive side, this has given rise to citizen journalism, thus enabling much faster dissemination of information compared to what was possible with newspapers,... | Preslav Nakov, Giovanni Da San Martino |  |
| 32 |  |  [Interpreting Predictions of NLP Models](https://doi.org/10.18653/v1/2020.emnlp-tutorials.3) |  | 0 | Although neural NLP models are highly expressive and empirically successful, they also systematically fail in counterintuitive ways and are opaque in their decision-making process. This tutorial will provide a background on interpretation techniques, i.e., methods for explaining the predictions of... | Eric Wallace, Matt Gardner, Sameer Singh |  |
| 33 |  |  [High Performance Natural Language Processing](https://doi.org/10.18653/v1/2020.emnlp-tutorials.4) |  | 0 | Scale has played a central role in the rapid progress natural language processing has enjoyed in recent years. While benchmarks are dominated by ever larger models, efficient hardware use is critical for their widespread adoption and further progress in the field. In this cutting-edge tutorial, we... | Gabriel Ilharco, Cesar Ilharco, Iulia Turc, Tim Dettmers, Felipe Ferreira, Kenton Lee |  |
| 34 |  |  [Representation, Learning and Reasoning on Spatial Language for Downstream NLP Tasks](https://doi.org/10.18653/v1/2020.emnlp-tutorials.5) |  | 0 | Understating spatial semantics expressed in natural language can become highly complex in real-world applications. This includes applications of language grounding, navigation, visual question answering, and more generic human-machine interaction and dialogue systems. In many of such downstream... | Parisa Kordjamshidi, James Pustejovsky, MarieFrancine Moens |  |
| 35 |  |  [Simultaneous Translation](https://doi.org/10.18653/v1/2020.emnlp-tutorials.6) |  | 0 | Simultaneous translation, which performs translation concurrently with the source speech, is widely useful in many scenarios such as international conferences, negotiations, press releases, legal proceedings, and medicine. This problem has long been considered one of the hardest problems in AI and... | Liang Huang, Colin Cherry, Mingbo Ma, Naveen Arivazhagan, Zhongjun He |  |
| 36 |  |  [The Amazing World of Neural Language Generation](https://doi.org/10.18653/v1/2020.emnlp-tutorials.7) |  | 0 | Neural Language Generation (NLG) – using neural network models to generate coherent text – is among the most promising methods for automated text creation. Recent years have seen a paradigm shift in neural text generation, caused by the advances in deep contextual language modeling (e.g., LSTMs,... | Yangfeng Ji, Antoine Bosselut, Thomas Wolf, Asli Celikyilmaz |  |
| 37 |  |  [Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020](https://aclanthology.org/volumes/2020.findings-emnlp/) |  | 0 |  | Trevor Cohn, Yulan He, Yang Liu |  |
| 38 |  |  [Fully Quantized Transformer for Machine Translation](https://doi.org/10.18653/v1/2020.findings-emnlp.1) |  | 0 | State-of-the-art neural machine translation methods employ massive amounts of parameters. Drastically reducing computational costs of such methods without affecting performance has been up to this point unsuccessful. To this end, we propose FullyQT: an all-inclusive quantization strategy for the... | Gabriele Prato, Ella Charlaix, Mehdi Rezagholizadeh |  |
| 39 |  |  [Summarizing Chinese Medical Answer with Graph Convolution Networks and Question-focused Dual Attention](https://doi.org/10.18653/v1/2020.findings-emnlp.2) |  | 0 | Online search engines are a popular source of medical information for users, where users can enter questions and obtain relevant answers. It is desirable to generate answer summaries for online search engines, particularly summaries that can reveal direct answers to questions. Moreover, answer... | Ningyu Zhang, Shumin Deng, Juan Li, Xi Chen, Wei Zhang, Huajun Chen |  |
| 40 |  |  [Stay Hungry, Stay Focused: Generating Informative and Specific Questions in Information-Seeking Conversations](https://doi.org/10.18653/v1/2020.findings-emnlp.3) |  | 0 | We investigate the problem of generating informative questions in information-asymmetric conversations. Unlike previous work on question generation which largely assumes knowledge of what the answer might be, we are interested in the scenario where the questioner is not given the context from which... | Peng Qi, Yuhao Zhang, Christopher D. Manning |  |
| 41 |  |  [Adapting BERT for Word Sense Disambiguation with Gloss Selection Objective and Example Sentences](https://doi.org/10.18653/v1/2020.findings-emnlp.4) |  | 0 | Domain adaptation or transfer learning using pre-trained language models such as BERT has proven to be an effective approach for many natural language processing tasks. In this work, we propose to formulate word sense disambiguation as a relevance ranking task, and fine-tune BERT on sequence-pair... | Boon Peng Yap, Andrew Koh, Eng Siong Chng |  |
| 42 |  |  [Adversarial Text Generation via Sequence Contrast Discrimination](https://doi.org/10.18653/v1/2020.findings-emnlp.5) |  | 0 | In this paper, we propose a sequence contrast loss driven text generation framework, which learns the difference between real texts and generated texts and uses that difference. Specifically, our discriminator contains a discriminative sequence generator instead of a binary classifier, and measures... | Ke Wang, Xiaojun Wan |  |
| 43 |  |  [GRACE: Gradient Harmonized and Cascaded Labeling for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2020.findings-emnlp.6) |  | 0 | In this paper, we focus on the imbalance issue, which is rarely studied in aspect term extraction and aspect sentiment classification when regarding them as sequence labeling tasks. Besides, previous works usually ignore the interaction between aspect terms when labeling polarities. We propose a... | Huaishao Luo, Lei Ji, Tianrui Li, Daxin Jiang, Nan Duan |  |
| 44 |  |  [Reducing Sentiment Bias in Language Models via Counterfactual Evaluation](https://doi.org/10.18653/v1/2020.findings-emnlp.7) |  | 0 | Advances in language modeling architectures and the availability of large text corpora have driven progress in automatic text generation. While this results in models capable of generating coherent texts, it also prompts models to internalize social biases present in the training corpus. This paper... | PoSen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae, Vishal Maini, Dani Yogatama, Pushmeet Kohli |  |
| 45 |  |  [Improving Text Understanding via Deep Syntax-Semantics Communication](https://doi.org/10.18653/v1/2020.findings-emnlp.8) |  | 0 | Recent studies show that integrating syntactic tree models with sequential semantic models can bring improved task performance, while these methods mostly employ shallow integration of syntax and semantics. In this paper, we propose a deep neural communication model between syntax and semantics to... | Hao Fei, Yafeng Ren, Donghong Ji |  |
| 46 |  |  [GRUEN for Evaluating Linguistic Quality of Generated Text](https://doi.org/10.18653/v1/2020.findings-emnlp.9) |  | 0 | Automatic evaluation metrics are indispensable for evaluating generated text. To date, these metrics have focused almost exclusively on the content selection aspect of the system output, ignoring the linguistic quality aspect altogether. We bridge this gap by proposing GRUEN for evaluating... | Wanzheng Zhu, Suma Bhat |  |
| 47 |  |  [A Greedy Bit-flip Training Algorithm for Binarized Knowledge Graph Embeddings](https://doi.org/10.18653/v1/2020.findings-emnlp.10) |  | 0 | This paper presents a simple and effective discrete optimization method for training binarized knowledge graph embedding model B-CP. Unlike the prior work using a SGD-based method and quantization of real-valued vectors, the proposed method directly optimizes binary embedding vectors by a series of... | Katsuhiko Hayashi, Koki Kishimoto, Masashi Shimbo |  |
| 48 |  |  [Difference-aware Knowledge Selection for Knowledge-grounded Conversation Generation](https://doi.org/10.18653/v1/2020.findings-emnlp.11) |  | 0 | In a multi-turn knowledge-grounded dialog, the difference between the knowledge selected at different turns usually provides potential clues to knowledge selection, which has been largely neglected in previous research. In this paper, we propose a difference-aware knowledge selection method. It... | Chujie Zheng, Yunbo Cao, Daxin Jiang, Minlie Huang |  |
| 49 |  |  [An Attentive Recurrent Model for Incremental Prediction of Sentence-final Verbs](https://doi.org/10.18653/v1/2020.findings-emnlp.12) |  | 0 | Verb prediction is important for understanding human processing of verb-final languages, with practical applications to real-time simultaneous interpretation from verb-final to verb-medial languages. While previous approaches use classical statistical models, we introduce an attention-based neural... | Wenyan Li, Alvin Grissom II, Jordan L. BoydGraber |  |
| 50 |  |  [Transformer-GCRF: Recovering Chinese Dropped Pronouns with General Conditional Random Fields](https://doi.org/10.18653/v1/2020.findings-emnlp.13) |  | 0 | Pronouns are often dropped in Chinese conversations and recovering the dropped pronouns is important for NLP applications such as Machine Translation. Existing approaches usually formulate this as a sequence labeling task of predicting whether there is a dropped pronoun before each token and its... | Jingxuan Yang, Kerui Xu, Jun Xu, Si Li, Sheng Gao, Jun Guo, JiRong Wen, Nianwen Xue |  |
| 51 |  |  [Neural Speed Reading Audited](https://doi.org/10.18653/v1/2020.findings-emnlp.14) |  | 0 | Several approaches to neural speed reading have been presented at major NLP and machine learning conferences in 2017–20; i.e., “human-inspired” recurrent network architectures that learn to “read” text faster by skipping irrelevant words, typically optimizing the joint objective of minimizing... | Anders Søgaard |  |
| 52 |  |  [Converting the Point of View of Message Spoken to Virtual Assistants](https://doi.org/10.18653/v1/2020.findings-emnlp.15) |  | 0 | Virtual Assistants can be quite literal at times. If the user says “tell Bob I love him,” most virtual assistants will extract the message “I love him” and send it to the user’s contact named Bob, rather than properly converting the message to “I love you.” We designed a system to allow virtual... | Gunhee Lee, Vera Zu, Sai Srujana Buddi, Dennis Liang, Purva Kulkarni, Jack G. M. Fitzgerald |  |
| 53 |  |  [Robustness to Modification with Shared Words in Paraphrase Identification](https://doi.org/10.18653/v1/2020.findings-emnlp.16) |  | 0 | Revealing the robustness issues of natural language processing models and improving their robustness is important to their performance under difficult situations. In this paper, we study the robustness of paraphrase identification models from a new perspective – via modification with shared words,... | Zhouxing Shi, Minlie Huang |  |
| 54 |  |  [Few-shot Natural Language Generation for Task-Oriented Dialog](https://doi.org/10.18653/v1/2020.findings-emnlp.17) |  | 0 | As a crucial component in task-oriented dialog systems, the Natural Language Generation (NLG) module converts a dialog act represented in a semantic form into a response in natural language. The success of traditional template-based or statistical models typically relies on heavily annotated data,... | Baolin Peng, Chenguang Zhu, Chunyuan Li, Xiujun Li, Jinchao Li, Michael Zeng, Jianfeng Gao |  |
| 55 |  |  [Mimic and Conquer: Heterogeneous Tree Structure Distillation for Syntactic NLP](https://doi.org/10.18653/v1/2020.findings-emnlp.18) |  | 0 | Syntax has been shown useful for various NLP tasks, while existing work mostly encodes singleton syntactic tree using one hierarchical neural network. In this paper, we investigate a simple and effective method, Knowledge Distillation, to integrate heterogeneous structure knowledge into a unified... | Hao Fei, Yafeng Ren, Donghong Ji |  |
| 56 |  |  [A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining](https://doi.org/10.18653/v1/2020.findings-emnlp.19) |  | 0 | With the abundance of automatic meeting transcripts, meeting summarization is of great interest to both participants and other parties. Traditional methods of summarizing meetings depend on complex multi-step pipelines that make joint optimization intractable. Meanwhile, there are a handful of deep... | Chenguang Zhu, Ruochen Xu, Michael Zeng, Xuedong Huang |  |
| 57 |  |  [Active Testing: An Unbiased Evaluation Method for Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2020.findings-emnlp.20) |  | 0 | Distant supervision has been a widely used method for neural relation extraction for its convenience of automatically labeling datasets. However, existing works on distantly supervised relation extraction suffer from the low quality of test set, which leads to considerable biased performance... | Pengshuai Li, Xinsong Zhang, Weijia Jia, Wei Zhao |  |
| 58 |  |  [Semantic Matching via Optimal Partial Transport](https://doi.org/10.18653/v1/2020.findings-emnlp.21) |  | 0 | In sequence-to-sequence models, classical optimal transport (OT) can be applied to semantically match generated sentences with target sentences. However, in non-parallel settings, target sentences are usually unavailable. To tackle this issue without losing the benefits of classical OT, we present a... | Ruiyi Zhang, Changyou Chen, Xinyuan Zhang, Ke Bai, Lawrence Carin |  |
| 59 |  |  [How Decoding Strategies Affect the Verifiability of Generated Text](https://doi.org/10.18653/v1/2020.findings-emnlp.22) |  | 0 | Recent progress in pre-trained language models led to systems that are able to generate text of an increasingly high quality. While several works have investigated the fluency and grammatical correctness of such models, it is still unclear to which extent the generated text is consistent with... | Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis Plachouras, Fabrizio Silvestri, Sebastian Riedel |  |
| 60 |  |  [Minimize Exposure Bias of Seq2Seq Models in Joint Entity and Relation Extraction](https://doi.org/10.18653/v1/2020.findings-emnlp.23) |  | 0 | Joint entity and relation extraction aims to extract relation triplets from plain text directly. Prior work leverages Sequence-to-Sequence (Seq2Seq) models for triplet sequence generation. However, Seq2Seq enforces an unnecessary order on the unordered triplets and involves a large decoding length... | Ranran Haoran Zhang, Qianying Liu, Aysa Xuemo Fan, Heng Ji, Daojian Zeng, Fei Cheng, Daisuke Kawahara, Sadao Kurohashi |  |
| 61 |  |  [Gradient-based Analysis of NLP Models is Manipulable](https://doi.org/10.18653/v1/2020.findings-emnlp.24) |  | 0 | Gradient-based analysis methods, such as saliency map visualizations and adversarial input perturbations, have found widespread use in interpreting neural NLP models due to their simplicity, flexibility, and most importantly, the fact that they directly reflect the model internals. In this paper,... | Junlin Wang, Jens Tuyls, Eric Wallace, Sameer Singh |  |
| 62 |  |  [Pretrain-KGE: Learning Knowledge Representation from Pretrained Language Models](https://doi.org/10.18653/v1/2020.findings-emnlp.25) |  | 0 | Conventional knowledge graph embedding (KGE) often suffers from limited knowledge representation, leading to performance degradation especially on the low-resource problem. To remedy this, we propose to enrich knowledge representation via pretrained language models by leveraging world knowledge from... | Zhiyuan Zhang, Xiaoqian Liu, Yi Zhang, Qi Su, Xu Sun, Bin He |  |
| 63 |  |  [A Self-Refinement Strategy for Noise Reduction in Grammatical Error Correction](https://doi.org/10.18653/v1/2020.findings-emnlp.26) |  | 0 | Existing approaches for grammatical error correction (GEC) largely rely on supervised learning with manually created GEC datasets. However, there has been little focus on verifying and ensuring the quality of the datasets, and on how lower-quality data might affect GEC performance. We indeed found... | Masato Mita, Shun Kiyono, Masahiro Kaneko, Jun Suzuki, Kentaro Inui |  |
| 64 |  |  [Understanding tables with intermediate pre-training](https://doi.org/10.18653/v1/2020.findings-emnlp.27) |  | 0 | Table entailment, the binary classification task of finding if a sentence is supported or refuted by the content of a table, requires parsing language and table structure as well as numerical and discrete reasoning. While there is extensive work on textual entailment, table entailment is less well... | Julian Martin Eisenschlos, Syrine Krichene, Thomas Müller |  |
| 65 |  |  [Enhance Robustness of Sequence Labelling with Masked Adversarial Training](https://doi.org/10.18653/v1/2020.findings-emnlp.28) |  | 0 | Adversarial training (AT) has shown strong regularization effects on deep learning algorithms by introducing small input perturbations to improve model robustness. In language tasks, adversarial training brings word-level robustness by adding input noise, which is beneficial for text classification.... | Luoxin Chen, Xinyue Liu, Weitong Ruan, Jianhua Lu |  |
| 66 |  |  [Multilingual Argument Mining: Datasets and Analysis](https://doi.org/10.18653/v1/2020.findings-emnlp.29) |  | 0 | The growing interest in argument mining and computational argumentation brings with it a plethora of Natural Language Understanding (NLU) tasks and corresponding datasets. However, as with many other NLU tasks, the dominant language is English, with resources in other languages being few and far... | Orith ToledoRonen, Matan Orbach, Yonatan Bilu, Artem Spector, Noam Slonim |  |
| 67 |  |  [Improving Grammatical Error Correction with Machine Translation Pairs](https://doi.org/10.18653/v1/2020.findings-emnlp.30) |  | 0 | We propose a novel data synthesis method to generate diverse error-corrected sentence pairs for improving grammatical error correction, which is based on a pair of machine translation models (e.g., Chinese to English) of different qualities (i.e., poor and good). The poor translation model can... | Wangchunshu Zhou, Tao Ge, Chang Mu, Ke Xu, Furu Wei, Ming Zhou |  |
| 68 |  |  [Machines Getting with the Program: Understanding Intent Arguments of Non-Canonical Directives](https://doi.org/10.18653/v1/2020.findings-emnlp.31) |  | 0 | Modern dialog managers face the challenge of having to fulfill human-level conversational skills as part of common user expectations, including but not limited to discourse with no clear objective. Along with these requirements, agents are expected to extrapolate intent from the user’s dialogue even... | WonIk Cho, Young Ki Moon, Sangwhan Moon, Seok Min Kim, Nam Soo Kim |  |
| 69 |  |  [The RELX Dataset and Matching the Multilingual Blanks for Cross-lingual Relation Classification](https://doi.org/10.18653/v1/2020.findings-emnlp.32) |  | 0 | Relation classification is one of the key topics in information extraction, which can be used to construct knowledge bases or to provide useful information for question answering. Current approaches for relation classification are mainly focused on the English language and require lots of training... | Abdullatif Köksal, Arzucan Özgür |  |
| 70 |  |  [Control, Generate, Augment: A Scalable Framework for Multi-Attribute Text Generation](https://doi.org/10.18653/v1/2020.findings-emnlp.33) |  | 0 | We introduce CGA, a conditional VAE architecture, to control, generate, and augment text. CGA is able to generate natural English sentences controlling multiple semantic and syntactic attributes by combining adversarial learning with a context-aware loss and a cyclical word dropout routine. We... | Giuseppe Russo, Nora Hollenstein, Claudiu Cristian Musat, Ce Zhang |  |
| 71 |  |  [Open-Ended Visual Question Answering by Multi-Modal Domain Adaptation](https://doi.org/10.18653/v1/2020.findings-emnlp.34) |  | 0 | We study the problem of visual question answering (VQA) in images by exploiting supervised domain adaptation, where there is a large amount of labeled data in the source domain but only limited labeled data in the target domain, with the goal to train a good target model. A straightforward solution... | Yiming Xu, Lin Chen, Zhongwei Cheng, Lixin Duan, Jiebo Luo |  |
| 72 |  |  [Dual Low-Rank Multimodal Fusion](https://doi.org/10.18653/v1/2020.findings-emnlp.35) |  | 0 | Tensor-based fusion methods have been proven effective in multimodal fusion tasks. However, existing tensor-based methods make a poor use of the fine-grained temporal dynamics of multimodal sequential features. Motivated by this observation, this paper proposes a novel multimodal fusion method... | Tao Jin, Siyu Huang, Yingming Li, Zhongfei Zhang |  |
| 73 |  |  [Contextual Modulation for Relation-Level Metaphor Identification](https://doi.org/10.18653/v1/2020.findings-emnlp.36) |  | 0 | Identifying metaphors in text is very challenging and requires comprehending the underlying comparison. The automation of this cognitive process has gained wide attention lately. However, the majority of existing approaches concentrate on word-level identification by treating the task as either... | Omnia Zayed, John P. McCrae, Paul Buitelaar |  |
| 74 |  |  [Context-aware Stand-alone Neural Spelling Correction](https://doi.org/10.18653/v1/2020.findings-emnlp.37) |  | 0 | Existing natural language processing systems are vulnerable to noisy inputs resulting from misspellings. On the contrary, humans can easily infer the corresponding correct words from their misspellings and surrounding context. Inspired by this, we address the stand-alone spelling correction problem,... | Xiangci Li, Hairong Liu, Liang Huang |  |
| 75 |  |  [A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels](https://doi.org/10.18653/v1/2020.findings-emnlp.38) |  | 0 | Resources for Semantic Role Labeling (SRL) are typically annotated by experts at great expense. Prior attempts to develop crowdsourcing methods have either had low accuracy or required substantial expert annotation. We propose a new multi-stage crowd workflow that substantially reduces expert... | Youxuan Jiang, Huaiyu Zhu, Jonathan K. Kummerfeld, Yunyao Li, Walter S. Lasecki |  |
| 76 |  |  [KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding](https://doi.org/10.18653/v1/2020.findings-emnlp.39) |  | 0 | Natural language inference (NLI) and semantic textual similarity (STS) are key tasks in natural language understanding (NLU). Although several benchmark datasets for those tasks have been released in English and a few other languages, there are no publicly available NLI or STS datasets in the Korean... | Jiyeon Ham, Yo Joong Choe, Kyubyong Park, Ilji Choi, Hyungjoon Soh |  |
| 77 |  |  [Dialogue Generation on Infrequent Sentence Functions via Structured Meta-Learning](https://doi.org/10.18653/v1/2020.findings-emnlp.40) |  | 0 | Sentence function is an important linguistic feature indicating the communicative purpose in uttering a sentence. Incorporating sentence functions into conversations has shown improvements in the quality of generated responses. However, the number of utterances for different types of fine-grained... | Yifan Gao, Piji Li, Wei Bi, Xiaojiang Liu, Michael R. Lyu, Irwin King |  |
| 78 |  |  [Exploring Versatile Generative Language Model Via Parameter-Efficient Transfer Learning](https://doi.org/10.18653/v1/2020.findings-emnlp.41) |  | 0 | Fine-tuning pre-trained generative language models to down-stream language generation tasks has shown promising results. However, this comes with the cost of having a single, large model for each task, which is not ideal in low-memory/power scenarios (e.g., mobile). In this paper, we propose an... | Zhaojiang Lin, Andrea Madotto, Pascale Fung |  |
| 79 |  |  [A Fully Hyperbolic Neural Model for Hierarchical Multi-class Classification](https://doi.org/10.18653/v1/2020.findings-emnlp.42) |  | 0 | Label inventories for fine-grained entity typing have grown in size and complexity. Nonetheless, they exhibit a hierarchical structure. Hyperbolic spaces offer a mathematically appealing approach for learning hierarchical representations of symbolic data. However, it is not clear how to integrate... | Federico López, Michael Strube |  |
| 80 |  |  [Claim Check-Worthiness Detection as Positive Unlabelled Learning](https://doi.org/10.18653/v1/2020.findings-emnlp.43) |  | 0 | As the first step of automatic fact checking, claim check-worthiness detection is a critical component of fact checking systems. There are multiple lines of research which study this problem: check-worthiness ranking from political speeches and debates, rumour detection on Twitter, and citation... | Dustin Wright, Isabelle Augenstein |  |
| 81 |  |  [ConceptBert: Concept-Aware Representation for Visual Question Answering](https://doi.org/10.18653/v1/2020.findings-emnlp.44) |  | 0 | Visual Question Answering (VQA) is a challenging task that has received increasing attention from both the computer vision and the natural language processing communities. A VQA model combines visual and textual features in order to answer questions grounded in an image. Current works in VQA focus... | François Gardères, Maryam Ziaeefard, Baptiste Abeloos, Freddy Lécué |  |
| 82 |  |  [Bootstrapping a Crosslingual Semantic Parser](https://doi.org/10.18653/v1/2020.findings-emnlp.45) |  | 0 | Recent progress in semantic parsing scarcely considers languages other than English but professional translation can be prohibitively expensive. We adapt a semantic parser trained on a single language, such as English, to new languages and multiple domains with minimal annotation. We query if... | Tom Sherborne, Yumo Xu, Mirella Lapata |  |
| 83 |  |  [Revisiting Representation Degeneration Problem in Language Modeling](https://doi.org/10.18653/v1/2020.findings-emnlp.46) |  | 0 | Weight tying is now a common setting in many language generation tasks such as language modeling and machine translation. However, a recent study reveals that there is a potential flaw in weight tying. They find that the learned word embeddings are likely to degenerate and lie in a narrow cone when... | Zhong Zhang, Chongming Gao, Cong Xu, Rui Miao, Qinli Yang, Junming Shao |  |
| 84 |  |  [The workweek is the best time to start a family - A Study of GPT-2 Based Claim Generation](https://doi.org/10.18653/v1/2020.findings-emnlp.47) |  | 0 | Argument generation is a challenging task whose research is timely considering its potential impact on social media and the dissemination of information. Here we suggest a pipeline based on GPT-2 for generating coherent claims, and explore the types of claims that it produces, and their veracity,... | Shai Gretz, Yonatan Bilu, Edo CohenKarlik, Noam Slonim |  |
| 85 |  |  [Dynamic Data Selection for Curriculum Learning via Ability Estimation](https://doi.org/10.18653/v1/2020.findings-emnlp.48) |  | 0 | Curriculum learning methods typically rely on heuristics to estimate the difficulty of training examples or the ability of the model. In this work, we propose replacing difficulty heuristics with learned difficulty parameters. We also propose Dynamic Data selection for Curriculum Learning via... | John P. Lalor, Hong Yu |  |
| 86 |  |  [Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation](https://doi.org/10.18653/v1/2020.findings-emnlp.49) |  | 0 | Transformer-based models have brought a radical change to neural machine translation. A key feature of the Transformer architecture is the so-called multi-head attention mechanism, which allows the model to focus simultaneously on different parts of the input. However, recent works have shown that... | Alessandro Raganato, Yves Scherrer, Jörg Tiedemann |  |
| 87 |  |  [ZEST: Zero-shot Learning from Text Descriptions using Textual Similarity and Visual Summarization](https://doi.org/10.18653/v1/2020.findings-emnlp.50) |  | 0 | We study the problem of recognizing visual entities from the textual descriptions of their classes. Specifically, given birds’ images with free-text descriptions of their species, we learn to classify images of previously-unseen species based on specie descriptions. This setup has been studied in... | Tzuf PazArgaman, Reut Tsarfaty, Gal Chechik, Yuval Atzmon |  |
| 88 |  |  [Few-Shot Multi-Hop Relation Reasoning over Knowledge Bases](https://doi.org/10.18653/v1/2020.findings-emnlp.51) |  | 0 | Multi-hop relation reasoning over knowledge base is to generate effective and interpretable relation prediction through reasoning paths. The current methods usually require sufficient training data (fact triples) for each query relation, impairing their performances over few-shot relations (with... | Chuxu Zhang, Lu Yu, Mandana Saebi, Meng Jiang, Nitesh V. Chawla |  |
| 89 |  |  [Sentiment Analysis with Weighted Graph Convolutional Networks](https://doi.org/10.18653/v1/2020.findings-emnlp.52) |  | 0 | Syntactic information is essential for both sentiment analysis(SA) and aspect-based sentiment analysis(ABSA). Previous work has already achieved great progress utilizing Graph Convolutional Network(GCN) over dependency tree of a sentence. However, these models do not fully exploit the syntactic... | Fanyu Meng, Junlan Feng, Danping Yin, Si Chen, Min Hu |  |
| 90 |  |  [PBoS: Probabilistic Bag-of-Subwords for Generalizing Word Embedding](https://doi.org/10.18653/v1/2020.findings-emnlp.53) |  | 0 | We look into the task of generalizing word embeddings: given a set of pre-trained word vectors over a finite vocabulary, the goal is to predict embedding vectors for out-of-vocabulary words, without extra contextual information. We rely solely on the spellings of words and propose a model, along... | Jinman Zhao, Shawn Zhong, Xiaomin Zhang, Yingyu Liang |  |
| 91 |  |  [Interpretable Entity Representations through Large-Scale Typing](https://doi.org/10.18653/v1/2020.findings-emnlp.54) |  | 0 | In standard methodology for natural language processing, entities in text are typically embedded in dense vector spaces with pre-trained models. The embeddings produced this way are effective when fed into downstream models, but they require end-task fine-tuning and are fundamentally difficult to... | Yasumasa Onoe, Greg Durrett |  |
| 92 |  |  [Empirical Studies of Institutional Federated Learning For Natural Language Processing](https://doi.org/10.18653/v1/2020.findings-emnlp.55) |  | 0 | Federated learning has sparkled new interests in the deep learning society to make use of isolated data sources from independent institutes. With the development of novel training tools, we have successfully deployed federated natural language processing networks on GPU-enabled server clusters. This... | Xinghua Zhu, Jianzong Wang, Zhenhou Hong, Jing Xiao |  |
| 93 |  |  [NeuReduce: Reducing Mixed Boolean-Arithmetic Expressions by Recurrent Neural Network](https://doi.org/10.18653/v1/2020.findings-emnlp.56) |  | 0 | Mixed Boolean-Arithmetic (MBA) expressions involve both arithmetic calculation (e.g.,plus, minus, multiply) and bitwise computation (e.g., and, or, negate, xor). MBA expressions have been widely applied in software obfuscation, transforming programs from a simple form to a complex form. MBA... | Weijie Feng, Binbin Liu, Dongpeng Xu, Qilong Zheng, Yun Xu |  |
| 94 |  |  [From Language to Language-ish: How Brain-Like is an LSTM's Representation of Atypical Language Stimuli?](https://doi.org/10.18653/v1/2020.findings-emnlp.57) |  | 0 | The representations generated by many models of language (word embeddings, recurrent neural networks and transformers) correlate to brain activity recorded while people read. However, these decoding results are usually based on the brain’s reaction to syntactically and semantically sound language... | Maryam Hashemzadeh, Greta Kaufeld, Martha White, Andrea E. Martin, Alona Fyshe |  |
| 95 |  |  [Revisiting Pre-Trained Models for Chinese Natural Language Processing](https://doi.org/10.18653/v1/2020.findings-emnlp.58) |  | 0 | Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks, and consecutive variants have been proposed to further improve the performance of the pre-trained language models. In this paper, we target on revisiting Chinese pre-trained... | Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, Guoping Hu |  |
| 96 |  |  [Cascaded Semantic and Positional Self-Attention Network for Document Classification](https://doi.org/10.18653/v1/2020.findings-emnlp.59) |  | 0 | Transformers have shown great success in learning representations for language modelling. However, an open challenge still remains on how to systematically aggregate semantic information (word embedding) with positional (or temporal) information (word orders). In this work, we propose a new... | Juyong Jiang, Jie Zhang, Kai Zhang |  |
| 97 |  |  [Toward Recognizing More Entity Types in NER: An Efficient Implementation using Only Entity Lexicons](https://doi.org/10.18653/v1/2020.findings-emnlp.60) |  | 0 | In this work, we explore the way to quickly adjust an existing named entity recognition (NER) system to make it capable of recognizing entity types not defined in the system. As an illustrative example, consider the case that a NER system has been built to recognize person and organization names,... | Minlong Peng, Ruotian Ma, Qi Zhang, Lujun Zhao, Mengxi Wei, Changlong Sun, Xuanjing Huang |  |
| 98 |  |  [From Disjoint Sets to Parallel Data to Train Seq2Seq Models for Sentiment Transfer](https://doi.org/10.18653/v1/2020.findings-emnlp.61) |  | 0 | We present a method for creating parallel data to train Seq2Seq neural networks for sentiment transfer. Most systems for this task, which can be viewed as monolingual machine translation (MT), have relied on unsupervised methods, such as Generative Adversarial Networks (GANs)-inspired approaches,... | Paulo R. Cavalin, Marisa Vasconcelos, Marcelo Grave, Claudio S. Pinhanez, Victor Henrique Alves Ribeiro |  |
| 99 |  |  [Learning to Stop: A Simple yet Effective Approach to Urban Vision-Language Navigation](https://doi.org/10.18653/v1/2020.findings-emnlp.62) |  | 0 | Vision-and-Language Navigation (VLN) is a natural language grounding task where an agent learns to follow language instructions and navigate to specified destinations in real-world environments. A key challenge is to recognize and stop at the correct location, especially for complicated outdoor... | Jiannan Xiang, Xin Wang, William Yang Wang |  |
| 100 |  |  [Document Ranking with a Pretrained Sequence-to-Sequence Model](https://doi.org/10.18653/v1/2020.findings-emnlp.63) |  | 0 | This work proposes the use of a pretrained sequence-to-sequence model for document ranking. Our approach is fundamentally different from a commonly adopted classification-based formulation based on encoder-only pretrained transformer architectures such as BERT. We show how a sequence-to-sequence... | Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, Jimmy Lin |  |
| 101 |  |  [Pruning Redundant Mappings in Transformer Models via Spectral-Normalized Identity Prior](https://doi.org/10.18653/v1/2020.findings-emnlp.64) |  | 0 | Traditional (unstructured) pruning methods for a Transformer model focus on regularizing the individual weights by penalizing them toward zero. In this work, we explore spectral-normalized identity priors (SNIP), a structured pruning approach which penalizes an entire residual module in a... | Zi Lin, Jeremiah Z. Liu, Zi Yang, Nan Hua, Dan Roth |  |
| 102 |  |  [Rethinking Self-Attention: Towards Interpretability in Neural Parsing](https://doi.org/10.18653/v1/2020.findings-emnlp.65) |  | 0 | Attention mechanisms have improved the performance of NLP tasks while allowing models to remain explainable. Self-attention is currently widely used, however interpretability is difficult due to the numerous attention distributions. Recent work has shown that model representations can benefit from... | Khalil Mrini, Franck Dernoncourt, Quan Hung Tran, Trung Bui, Walter Chang, Ndapa Nakashole |  |
| 103 |  |  [PolicyQA: A Reading Comprehension Dataset for Privacy Policies](https://doi.org/10.18653/v1/2020.findings-emnlp.66) |  | 0 | Privacy policy documents are long and verbose. A question answering (QA) system can assist users in finding the information that is relevant and important to them. Prior studies in this domain frame the QA task as retrieving the most relevant text segment or a list of sentences from the policy... | Wasi Uddin Ahmad, Jianfeng Chi, Yuan Tian, KaiWei Chang |  |
| 104 |  |  [A Linguistic Analysis of Visually Grounded Dialogues Based on Spatial Expressions](https://doi.org/10.18653/v1/2020.findings-emnlp.67) |  | 0 | Recent models achieve promising results in visually grounded dialogues. However, existing datasets often contain undesirable biases and lack sophisticated linguistic analyses, which make it difficult to understand how well current models recognize their precise linguistic structures. To address this... | Takuma Udagawa, Takato Yamazaki, Akiko Aizawa |  |
| 105 |  |  [Efficient Context and Schema Fusion Networks for Multi-Domain Dialogue State Tracking](https://doi.org/10.18653/v1/2020.findings-emnlp.68) |  | 0 | Dialogue state tracking (DST) aims at estimating the current dialogue state given all the preceding conversation. For multi-domain DST, the data sparsity problem is a major obstacle due to increased numbers of state candidates and dialogue lengths. To encode the dialogue context efficiently, we... | Su Zhu, Jieyu Li, Lu Chen, Kai Yu |  |
| 106 |  |  [Syntactic and Semantic-driven Learning for Open Information Extraction](https://doi.org/10.18653/v1/2020.findings-emnlp.69) |  | 0 | One of the biggest bottlenecks in building accurate, high coverage neural open IE systems is the need for large labelled corpora. The diversity of open domain corpora and the variety of natural language expressions further exacerbate this problem. In this paper, we propose a syntactic and... | Jialong Tang, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, Xinyan Xiao, Hua Wu |  |
| 107 |  |  [Group-wise Contrastive Learning for Neural Dialogue Generation](https://doi.org/10.18653/v1/2020.findings-emnlp.70) |  | 0 | Neural dialogue response generation has gained much popularity in recent years. Maximum Likelihood Estimation (MLE) objective is widely adopted in existing dialogue model learning. However, models trained with MLE objective function are plagued by the low-diversity issue when it comes to the... | Hengyi Cai, Hongshen Chen, Yonghao Song, Zhuoye Ding, Yongjun Bao, Weipeng Yan, Xiaofang Zhao |  |
| 108 |  |  [E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT](https://doi.org/10.18653/v1/2020.findings-emnlp.71) |  | 0 | We present a novel way of injecting factual knowledge about entities into the pretrained BERT model (Devlin et al., 2019): We align Wikipedia2Vec entity vectors (Yamada et al., 2016) with BERT’s native wordpiece vector space and use the aligned entity vectors as if they were wordpiece vectors. The... | Nina Pörner, Ulli Waltinger, Hinrich Schütze |  |
| 109 |  |  [A Multi-task Learning Framework for Opinion Triplet Extraction](https://doi.org/10.18653/v1/2020.findings-emnlp.72) |  | 0 | The state-of-the-art Aspect-based Sentiment Analysis (ABSA) approaches are mainly based on either detecting aspect terms and their corresponding sentiment polarities, or co-extracting aspect and opinion terms. However, the extraction of aspect-sentiment pairs lacks opinion terms as a reference,... | Chen Zhang, Qiuchi Li, Dawei Song, Benyou Wang |  |
| 110 |  |  [Event Extraction as Multi-turn Question Answering](https://doi.org/10.18653/v1/2020.findings-emnlp.73) |  | 0 | Event extraction, which aims to identify event triggers of pre-defined event types and their arguments of specific roles, is a challenging task in NLP. Most traditional approaches formulate this task as classification problems, with event types or argument roles taken as golden labels. Such... | Fayuan Li, Weihua Peng, Yuguang Chen, Quan Wang, Lu Pan, Yajuan Lyu, Yong Zhu |  |
| 111 |  |  [Improving QA Generalization by Concurrent Modeling of Multiple Biases](https://doi.org/10.18653/v1/2020.findings-emnlp.74) |  | 0 | Existing NLP datasets contain various biases that models can easily exploit to achieve high performances on the corresponding evaluation sets. However, focusing on dataset-specific biases limits their ability to learn more generalizable knowledge about the task from more general data patterns. In... | Mingzhu Wu, Nafise Sadat Moosavi, Andreas Rücklé, Iryna Gurevych |  |
| 112 |  |  [Actor-Double-Critic: Incorporating Model-Based Critic for Task-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2020.findings-emnlp.75) |  | 0 | In order to improve the sample-efficiency of deep reinforcement learning (DRL), we implemented imagination augmented agent (I2A) in spoken dialogue systems (SDS). Although I2A achieves a higher success rate than baselines by augmenting predicted future into a policy network, its complicated... | YenChen Wu, BoHsiang Tseng, Milica Gasic |  |
| 113 |  |  [Controlled Hallucinations: Learning to Generate Faithfully from Noisy Data](https://doi.org/10.18653/v1/2020.findings-emnlp.76) |  | 0 | Neural text generation (data- or text-to-text) demonstrates remarkable performance when training data is abundant which for many applications is not the case. To collect a large corpus of parallel data, heuristic rules are often used but they inevitably let noise into the data, such as phrases in... | Katja Filippova |  |
| 114 |  |  [Sequential Span Classification with Neural Semi-Markov CRFs for Biomedical Abstracts](https://doi.org/10.18653/v1/2020.findings-emnlp.77) |  | 0 | Dividing biomedical abstracts into several segments with rhetorical roles is essential for supporting researchers’ information access in the biomedical domain. Conventional methods have regarded the task as a sequence labeling task based on sequential sentence classification, i.e., they assign a... | Kosuke Yamada, Tsutomu Hirao, Ryohei Sasano, Koichi Takeda, Masaaki Nagata |  |
| 115 |  |  [Where to Submit? Helping Researchers to Choose the Right Venue](https://doi.org/10.18653/v1/2020.findings-emnlp.78) |  | 0 | Whenever researchers write a paper, the same question occurs: “Where to submit?” In this work, we introduce WTS, an open and interpretable NLP system that recommends conferences and journals to researchers based on the title, abstract, and/or keywords of a given paper. We adapt the TextCNN... | Konstantin Kobs, Tobias Koopmann, Albin Zehe, David Fernes, Philipp Krop, Andreas Hotho |  |
| 116 |  |  [AirConcierge: Generating Task-Oriented Dialogue via Efficient Large-Scale Knowledge Retrieval](https://doi.org/10.18653/v1/2020.findings-emnlp.79) |  | 0 | Despite recent success in neural task-oriented dialogue systems, developing such a real-world system involves accessing large-scale knowledge bases (KBs), which cannot be simply encoded by neural approaches, such as memory network mechanisms. To alleviate the above problem, we propose , an... | ChiehYang Chen, PeiHsin Wang, ShihChieh Chang, DaCheng Juan, Wei Wei, JiaYu Pan |  |
| 117 |  |  [DocStruct: A Multimodal Method to Extract Hierarchy Structure in Document for General Form Understanding](https://doi.org/10.18653/v1/2020.findings-emnlp.80) |  | 0 | Form understanding depends on both textual contents and organizational structure. Although modern OCR performs well, it is still challenging to realize general form understanding because forms are commonly used and of various formats. The table detection and handcrafted features in previous works... | Zilong Wang, Mingjie Zhan, Xuebo Liu, Ding Liang |  |
| 118 |  |  [Pretrained Language Models for Dialogue Generation with Multiple Input Sources](https://doi.org/10.18653/v1/2020.findings-emnlp.81) |  | 0 | Large-scale pretrained language models have achieved outstanding performance on natural language understanding tasks. However, it is still under investigating how to apply them to dialogue generation tasks, especially those with responses conditioned on multiple sources. Previous work simply... | Yu Cao, Wei Bi, Meng Fang, Dacheng Tao |  |
| 119 |  |  [A Study in Improving BLEU Reference Coverage with Diverse Automatic Paraphrasing](https://doi.org/10.18653/v1/2020.findings-emnlp.82) |  | 0 | We investigate a long-perceived shortcoming in the typical use of BLEU: its reliance on a single reference. Using modern neural paraphrasing techniques, we study whether automatically generating additional \*diverse\* references can provide better coverage of the space of valid translations and... | Rachel Bawden, Biao Zhang, Lisa Yankovskaya, Andre Tättar, Matt Post |  |
| 120 |  |  [Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study](https://doi.org/10.18653/v1/2020.findings-emnlp.83) |  | 0 | Multilingual BERT (mBERT) has shown reasonable capability for zero-shot cross-lingual transfer when fine-tuned on downstream tasks. Since mBERT is not pre-trained with explicit cross-lingual supervision, transfer performance can further be improved by aligning mBERT with cross-lingual signal. Prior... | Saurabh Kulshreshtha, José Luis Redondo García, ChingYun Chang |  |
| 121 |  |  [Hybrid Emoji-Based Masked Language Models for Zero-Shot Abusive Language Detection](https://doi.org/10.18653/v1/2020.findings-emnlp.84) |  | 0 | Recent studies have demonstrated the effectiveness of cross-lingual language model pre-training on different NLP tasks, such as natural language inference and machine translation. In our work, we test this approach on social media data, which are particularly challenging to process within this... | Michele Corazza, Stefano Menini, Elena Cabrio, Sara Tonelli, Serena Villata |  |
| 122 |  |  [SeNsER: Learning Cross-Building Sensor Metadata Tagger](https://doi.org/10.18653/v1/2020.findings-emnlp.85) |  | 0 | Sensor metadata tagging, akin to the named entity recognition task, provides key contextual information (e.g., measurement type and location) about sensors for running smart building applications. Unfortunately, sensor metadata in different buildings often follows distinct naming conventions.... | Yang Jiao, Jiacheng Li, Jiaman Wu, Dezhi Hong, Rajesh Gupta, Jingbo Shang |  |
| 123 |  |  [Persian Ezafe Recognition Using Transformers and Its Role in Part-Of-Speech Tagging](https://doi.org/10.18653/v1/2020.findings-emnlp.86) |  | 0 | Ezafe is a grammatical particle in some Iranian languages that links two words together. Regardless of the important information it conveys, it is almost always not indicated in Persian script, resulting in mistakes in reading complex sentences and errors in natural language processing tasks. In... | Ehsan Doostmohammadi, Minoo Nassajian, Adel Rahimi |  |
| 124 |  |  [Scene Graph Modification Based on Natural Language Commands](https://doi.org/10.18653/v1/2020.findings-emnlp.87) |  | 0 | Structured representations like graphs and parse trees play a crucial role in many Natural Language Processing systems. In recent years, the advancements in multi-turn user interfaces necessitate the need for controlling and updating these structured representations given new sources of information.... | Xuanli He, Quan Hung Tran, Gholamreza Haffari, Walter Chang, Zhe Lin, Trung Bui, Franck Dernoncourt, Nhan Dam |  |
| 125 |  |  [LiMiT: The Literal Motion in Text Dataset](https://doi.org/10.18653/v1/2020.findings-emnlp.88) |  | 0 | Motion recognition is one of the basic cognitive capabilities of many life forms, yet identifying motion of physical entities in natural language have not been explored extensively and empirically. We present the Literal-Motion-in-Text (LiMiT) dataset, a large human-annotated collection of English... | Irene Manotas, Ngoc Phuoc An Vo, Vadim Sheinin |  |
| 126 |  |  [Transition-based Parsing with Stack-Transformers](https://doi.org/10.18653/v1/2020.findings-emnlp.89) |  | 0 | Modeling the parser state is key to good performance in transition-based parsing. Recurrent Neural Networks considerably improved the performance of transition-based systems by modelling the global state, e.g. stack-LSTM parsers, or local state modeling of contextualized features, e.g. Bi-LSTM... | Ramón Fernandez Astudillo, Miguel Ballesteros, Tahira Naseem, Austin Blodgett, Radu Florian |  |
| 127 |  |  [G-DAug: Generative Data Augmentation for Commonsense Reasoning](https://doi.org/10.18653/v1/2020.findings-emnlp.90) |  | 0 | Recent advances in commonsense reasoning depend on large-scale human-annotated training sets to achieve peak performance. However, manual curation of training sets is expensive and has been shown to introduce annotation artifacts that neural models can readily exploit and overfit to. We propose a... | Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, JiPing Wang, Chandra Bhagavatula, Yejin Choi, Doug Downey |  |
| 128 |  |  [HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data](https://doi.org/10.18653/v1/2020.findings-emnlp.91) |  | 0 | Existing question answering datasets focus on dealing with homogeneous information, based either only on text or KB/Table information alone. However, as human knowledge is distributed over heterogeneous forms, using homogeneous information alone might lead to severe coverage problems. To fill in the... | Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, William Yang Wang |  |
| 129 |  |  [PhoBERT: Pre-trained language models for Vietnamese](https://doi.org/10.18653/v1/2020.findings-emnlp.92) |  | 0 | We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the first public large-scale monolingual language models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent best pre-trained multilingual model XLM-R (Conneau et al., 2020) and... | Dat Quoc Nguyen, Anh Tuan Nguyen |  |
| 130 |  |  [ESTeR: Combining Word Co-occurrences and Word Associations for Unsupervised Emotion Detection](https://doi.org/10.18653/v1/2020.findings-emnlp.93) |  | 0 | Accurate detection of emotions in user- generated text was shown to have several applications for e-commerce, public well-being, and disaster management. Currently, the state-of-the-art performance for emotion detection in text is obtained using complex, deep learning models trained on... | Sujatha Das Gollapalli, Polina Rozenshtein, SeeKiong Ng |  |
| 131 |  |  [Make Templates Smarter: A Template Based Data2Text System Powered by Text Stitch Model](https://doi.org/10.18653/v1/2020.findings-emnlp.94) |  | 0 | Neural network (NN) based data2text models achieve state-of-the-art (SOTA) performance in most metrics, but they sometimes drop or modify the information in the input, and it is hard to control the generation contents. Moreover, it requires paired training data that are usually expensive to collect.... | Bingfeng Luo, Zuo Bai, Kunfeng Lai, Jianping Shen |  |
| 132 |  |  [GCDST: A Graph-based and Copy-augmented Multi-domain Dialogue State Tracking](https://doi.org/10.18653/v1/2020.findings-emnlp.95) |  | 0 | As an essential component of task-oriented dialogue systems, Dialogue State Tracking (DST) takes charge of estimating user intentions and requests in dialogue contexts and extracting substantial goals (states) from user utterances to help the downstream modules to determine the next actions of... | Peng Wu, Bowei Zou, Ridong Jiang, AiTi Aw |  |
| 133 |  |  [Incorporating Stylistic Lexical Preferences in Generative Language Models](https://doi.org/10.18653/v1/2020.findings-emnlp.96) |  | 0 | While recent advances in language modeling has resulted in powerful generation models, their generation style remains implicitly dependent on the training data and can not emulate a specific target style. Leveraging the generative capabilities of a transformer-based language models, we present an... | Hrituraj Singh, Gaurav Verma, Balaji Vasan Srinivasan |  |
| 134 |  |  [Why do you think that? Exploring faithful sentence-level rationales without supervision](https://doi.org/10.18653/v1/2020.findings-emnlp.97) |  | 0 | Evaluating the trustworthiness of a model’s prediction is essential for differentiating between ‘right for the right reasons’ and ‘right for the wrong reasons’. Identifying textual spans that determine the target label, known as faithful rationales, usually relies on pipeline approaches or... | Max Glockner, Ivan Habernal, Iryna Gurevych |  |
| 135 |  |  [Semi-Supervised Learning for Video Captioning](https://doi.org/10.18653/v1/2020.findings-emnlp.98) |  | 0 | Deep neural networks have made great success on video captioning in supervised learning setting. However, annotating videos with descriptions is very expensive and time-consuming. If the video captioning algorithm can benefit from a large number of unlabeled videos, the cost of annotation can be... | Ke Lin, Zhuoxin Gan, Liwei Wang |  |
| 136 |  |  [Multi2OIE: Multilingual Open Information Extraction based on Multi-Head Attention with BERT](https://doi.org/10.18653/v1/2020.findings-emnlp.99) |  | 0 | In this paper, we propose Multi2OIE, which performs open information extraction (open IE) by combining BERT with multi-head attention. Our model is a sequence-labeling system with an efficient and effective argument extraction method. We use a query, key, and value setting inspired by the Multimodal... | Youngbin Ro, Yukyung Lee, Pilsung Kang |  |
| 137 |  |  [LGPSolver - Solving Logic Grid Puzzles Automatically](https://doi.org/10.18653/v1/2020.findings-emnlp.100) |  | 0 | Logic grid puzzle (LGP) is a type of word problem where the task is to solve a problem in logic. Constraints for the problem are given in the form of textual clues. Once these clues are transformed into formal logic, a deductive reasoning process provides the solution. Solving logic grid puzzles in... | Elgun Jabrayilzade, Selma Tekir |  |
| 138 |  |  [Using the Past Knowledge to Improve Sentiment Classification](https://doi.org/10.18653/v1/2020.findings-emnlp.101) |  | 0 | This paper studies sentiment classification in the lifelong learning setting that incrementally learns a sequence of sentiment classification tasks. It proposes a new lifelong learning model (called L2PG) that can retain and selectively transfer the knowledge learned in the past to help learn the... | Qi Qin, Wenpeng Hu, Bing Liu |  |
| 139 |  |  [High-order Semantic Role Labeling](https://doi.org/10.18653/v1/2020.findings-emnlp.102) |  | 0 | Semantic role labeling is primarily used to identify predicates, arguments, and their semantic relationships. Due to the limitations of modeling methods and the conditions of pre-identified predicates, previous work has focused on the relationships between predicates and arguments and the... | Zuchao Li, Hai Zhao, Rui Wang, Kevin Parnow |  |
| 140 |  |  [Undersensitivity in Neural Reading Comprehension](https://doi.org/10.18653/v1/2020.findings-emnlp.103) |  | 0 | Current reading comprehension methods generalise well to in-distribution test sets, yet perform poorly on adversarially selected data. Prior work on adversarial inputs typically studies model oversensitivity: semantically invariant text perturbations that cause a model’s prediction to change. Here... | Johannes Welbl, Pasquale Minervini, Max Bartolo, Pontus Stenetorp, Sebastian Riedel |  |
| 141 |  |  [HyperText: Endowing FastText with Hyperbolic Geometry](https://doi.org/10.18653/v1/2020.findings-emnlp.104) |  | 0 | Natural language data exhibit tree-like hierarchical structures such as the hypernym-hyponym hierarchy in WordNet. FastText, as the state-of-the-art text classifier based on shallow neural network in Euclidean space, may not represent such hierarchies precisely with limited representation capacity.... | Yudong Zhu, Di Zhou, Jinghui Xiao, Xin Jiang, Xiao Chen, Qun Liu |  |
| 142 |  |  [AutoETER: Automated Entity Type Representation with Relation-Aware Attention for Knowledge Graph Embedding](https://doi.org/10.18653/v1/2020.findings-emnlp.105) |  | 0 | Recent advances in Knowledge Graph Embedding (KGE) allow for representing entities and relations in continuous vector spaces. Some traditional KGE models leveraging additional type information can improve the representation of entities which however totally rely on the explicit types or neglect the... | Guanglin Niu, Bo Li, Yongfei Zhang, Shiliang Pu, Jingyang Li |  |
| 143 |  |  [Learning Robust and Multilingual Speech Representations](https://doi.org/10.18653/v1/2020.findings-emnlp.106) |  | 0 | Unsupervised speech representation learning has shown remarkable success at finding representations that correlate with phonetic structures and improve downstream speech recognition performance. However, most research has been focused on evaluating the representations in terms of their ability to... | Kazuya Kawakami, Luyu Wang, Chris Dyer, Phil Blunsom, Aäron van den Oord |  |
| 144 |  |  [FQuAD: French Question Answering Dataset](https://doi.org/10.18653/v1/2020.findings-emnlp.107) |  | 0 | Recent advances in the field of language modeling have improved state-of-the-art results on many Natural Language Processing tasks. Among them, Reading Comprehension has made significant progress over the past few years. However, most results are reported in English since labeled resources available... | Martin d'Hoffschmidt, Wacim Belblidia, Quentin Heinrich, Tom Brendlé, Maxime Vidal |  |
| 145 |  |  [Semantic Matching and Aggregation Network for Few-shot Intent Detection](https://doi.org/10.18653/v1/2020.findings-emnlp.108) |  | 0 | Few-shot Intent Detection is challenging due to the scarcity of available annotated utterances. Although recent works demonstrate that multi-level matching plays an important role in transferring learned knowledge from seen training classes to novel testing classes, they rely on a static similarity... | Hoang Nguyen, Chenwei Zhang, Congying Xia, Philip S. Yu |  |
| 146 |  |  [Quantifying the Contextualization of Word Representations with Semantic Class Probing](https://doi.org/10.18653/v1/2020.findings-emnlp.109) |  | 0 | Pretrained language models achieve state-of-the-art results on many NLP tasks, but there are still many open questions about how and why they work so well. We investigate the contextualization of words in BERT. We quantify the amount of contextualization, i.e., how well words are interpreted in... | Mengjie Zhao, Philipp Dufter, Yadollah Yaghoobzadeh, Hinrich Schütze |  |
| 147 |  |  [Learning to Generate Clinically Coherent Chest X-Ray Reports](https://doi.org/10.18653/v1/2020.findings-emnlp.110) |  | 0 | Automated radiology report generation has the potential to reduce the time clinicians spend manually reviewing radiographs and streamline clinical care. However, past work has shown that typical abstractive methods tend to produce fluent, but clinically incorrect radiology reports. In this work, we... | Justin R. Lovelace, Bobak Mortazavi |  |
| 148 |  |  [FELIX: Flexible Text Editing Through Tagging and Insertion](https://doi.org/10.18653/v1/2020.findings-emnlp.111) |  | 0 | We present FELIX – a flexible text-editing approach for generation, designed to derive maximum benefit from the ideas of decoding with bi-directional contexts and self-supervised pretraining. In contrast to conventional sequenceto-sequence (seq2seq) models, FELIX is efficient in low-resource... | Jonathan Mallinson, Aliaksei Severyn, Eric Malmi, Guillermo Garrido |  |
| 149 |  |  [What Can We Do to Improve Peer Review in NLP?](https://doi.org/10.18653/v1/2020.findings-emnlp.112) |  | 0 | Peer review is our best tool for judging the quality of conference submissions, but it is becoming increasingly spurious. We argue that a part of the problem is that the reviewers and area chairs face a poorly defined task forcing apples-to-oranges comparisons. There are several potential ways... | Anna Rogers, Isabelle Augenstein |  |
| 150 |  |  [Unsupervised Relation Extraction from Language Models using Constrained Cloze Completion](https://doi.org/10.18653/v1/2020.findings-emnlp.113) |  | 0 | We show that state-of-the-art self-supervised language models can be readily used to extract relations from a corpus without the need to train a fine-tuned extractive head. We introduce RE-Flex, a simple framework that performs constrained cloze completion over pretrained language models to perform... | Ankur Goswami, Akshata Bhat, Hadar Ohana, Theodoros Rekatsinas |  |
| 151 |  |  [Biomedical Event Extraction on Graph Edge-conditioned Attention Networks with Hierarchical Knowledge Graphs](https://doi.org/10.18653/v1/2020.findings-emnlp.114) |  | 0 | Biomedical event extraction is critical in understanding biomolecular interactions described in scientific corpus. One of the main challenges is to identify nested structured events that are associated with non-indicative trigger words. We propose to incorporate domain knowledge from Unified Medical... | KungHsiang Huang, Mu Yang, Nanyun Peng |  |
| 152 |  |  [Constraint Satisfaction Driven Natural Language Generation: A Tree Search Embedded MCMC Approach](https://doi.org/10.18653/v1/2020.findings-emnlp.115) |  | 0 | Generating natural language under complex constraints is a principled formulation towards controllable text generation. We present a framework to allow specification of combinatorial constraints for sentence generation. We propose TSMC, an efficient method to generate high likelihood sentences with... | Maosen Zhang, Nan Jiang, Lei Li, Yexiang Xue |  |
| 153 |  |  [Examining the Ordering of Rhetorical Strategies in Persuasive Requests](https://doi.org/10.18653/v1/2020.findings-emnlp.116) |  | 0 | Interpreting how persuasive language influences audiences has implications across many domains like advertising, argumentation, and propaganda. Persuasion relies on more than a message’s content. Arranging the order of the message itself (i.e., ordering specific rhetorical strategies) also plays an... | Omar Shaikh, Jiaao Chen, Jon SaadFalcon, Polo Chau, Diyi Yang |  |
| 154 |  |  [Evaluating Models' Local Decision Boundaries via Contrast Sets](https://doi.org/10.18653/v1/2020.findings-emnlp.117) |  | 0 | Standard test sets for supervised learning evaluate in-distribution generalization. Unfortunately, when a dataset has systematic gaps (e.g., annotation artifacts), these evaluations are misleading: a model can learn simple decision rules that perform well on the test set but do not capture the... | Matt Gardner, Yoav Artzi, Victoria Basmova, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, Ben Zhou |  |
| 155 |  |  [Parsing with Multilingual BERT, a Small Treebank, and a Small Corpus](https://doi.org/10.18653/v1/2020.findings-emnlp.118) |  | 0 | Pretrained multilingual contextual representations have shown great success, but due to the limits of their pretraining data, their benefits do not apply equally to all language varieties. This presents a challenge for language varieties unfamiliar to these models, whose labeled and unlabeled data... | Ethan C. Chau, Lucy H. Lin, Noah A. Smith |  |
| 156 |  |  [OptSLA: an Optimization-Based Approach for Sequential Label Aggregation](https://doi.org/10.18653/v1/2020.findings-emnlp.119) |  | 0 | The need for the annotated training dataset on which data-hungry machine learning algorithms feed has increased dramatically with advanced acclaim of machine learning applications. To annotate the data, people with domain expertise are needed, but they are seldom available and expensive to hire.... | Nasim Sabetpour, Adithya Kulkarni, Qi Li |  |
| 157 |  |  [Optimizing Word Segmentation for Downstream Task](https://doi.org/10.18653/v1/2020.findings-emnlp.120) |  | 0 | In traditional NLP, we tokenize a given sentence as a preprocessing, and thus the tokenization is unrelated to a target downstream task. To address this issue, we propose a novel method to explore a tokenization which is appropriate for the downstream task. Our proposed method, optimizing... | Tatsuya Hiraoka, Sho Takase, Kei Uchiumi, Atsushi Keyaki, Naoaki Okazaki |  |
| 158 |  |  [Dynamically Updating Event Representations for Temporal Relation Classification with Multi-category Learning](https://doi.org/10.18653/v1/2020.findings-emnlp.121) |  | 0 | Temporal relation classification is the pair-wise task for identifying the relation of a temporal link (TLINKs) between two mentions, i.e. event, time and document creation time (DCT). It leads to two crucial limits: 1) Two TLINKs involving a common mention do not share information. 2) Existing... | Fei Cheng, Masayuki Asahara, Ichiro Kobayashi, Sadao Kurohashi |  |
| 159 |  |  [A Compare Aggregate Transformer for Understanding Document-grounded Dialogue](https://doi.org/10.18653/v1/2020.findings-emnlp.122) |  | 0 | Unstructured documents serving as external knowledge of the dialogues help to generate more informative responses. Previous research focused on knowledge selection (KS) in the document with dialogue. However, dialogue history that is not related to the current dialogue may introduce noise in the KS... | Longxuan Ma, WeiNan Zhang, Runxin Sun, Ting Liu |  |
| 160 |  |  [TextHide: Tackling Data Privacy for Language Understanding Tasks](https://doi.org/10.18653/v1/2020.findings-emnlp.123) |  | 0 | An unsolved challenge in distributed or federated learning is to effectively mitigate privacy risks without slowing down training or reducing accuracy. In this paper, we propose TextHide aiming at addressing this challenge for natural language understanding tasks. It requires all participants to add... | Yangsibo Huang, Zhao Song, Danqi Chen, Kai Li, Sanjeev Arora |  |
| 161 |  |  [Modeling Intra and Inter-modality Incongruity for Multi-Modal Sarcasm Detection](https://doi.org/10.18653/v1/2020.findings-emnlp.124) |  | 0 | Sarcasm is a pervasive phenomenon in today’s social media platforms such as Twitter and Reddit. These platforms allow users to create multi-modal messages, including texts, images, and videos. Existing multi-modal sarcasm detection methods either simply concatenate the features from multi modalities... | Hongliang Pan, Zheng Lin, Peng Fu, Yatao Qi, Weiping Wang |  |
| 162 |  |  [Investigating Transferability in Pretrained Language Models](https://doi.org/10.18653/v1/2020.findings-emnlp.125) |  | 0 | How does language model pretraining help transfer learning? We consider a simple ablation technique for determining the impact of each pretrained layer on transfer task performance. This method, partial reinitialization, involves replacing different layers of a pretrained model with random weights,... | Alex Tamkin, Trisha Singh, Davide Giovanardi, Noah D. Goodman |  |
| 163 |  |  [Improving Knowledge-Aware Dialogue Response Generation by Using Human-Written Prototype Dialogues](https://doi.org/10.18653/v1/2020.findings-emnlp.126) |  | 0 | Incorporating commonsense knowledge can alleviate the issue of generating generic responses in open-domain generative dialogue systems. However, selecting knowledge facts for the dialogue context is still a challenge. The widely used approach Entity Name Matching always retrieves irrelevant facts... | Sixing Wu, Ying Li, Dawei Zhang, Zhonghai Wu |  |
| 164 |  |  [Filtering before Iteratively Referring for Knowledge-Grounded Response Selection in Retrieval-Based Chatbots](https://doi.org/10.18653/v1/2020.findings-emnlp.127) |  | 0 | The challenges of building knowledge-grounded retrieval-based chatbots lie in how to ground a conversation on its background knowledge and how to match response candidates with both context and knowledge simultaneously. This paper proposes a method named Filtering before Iteratively REferring (FIRE)... | JiaChen Gu, ZhenHua Ling, Quan Liu, Zhigang Chen, Xiaodan Zhu |  |
| 165 |  |  [Privacy-Preserving News Recommendation Model Learning](https://doi.org/10.18653/v1/2020.findings-emnlp.128) |  | 0 | News recommendation aims to display news articles to users based on their personal interest. Existing news recommendation methods rely on centralized storage of user behavior data for model training, which may lead to privacy concerns and risks due to the privacy-sensitive nature of user behaviors.... | Tao Qi, Fangzhao Wu, Chuhan Wu, Yongfeng Huang, Xing Xie |  |
| 166 |  |  [exBERT: Extending Pre-trained Models with Domain-specific Vocabulary Under Constrained Training Resources](https://doi.org/10.18653/v1/2020.findings-emnlp.129) |  | 0 | We introduce exBERT, a training method to extend BERT pre-trained models from a general domain to a new pre-trained model for a specific domain with a new additive vocabulary under constrained training resources (i.e., constrained computation and data). exBERT uses a small extension module to learn... | Wen Tai, H. T. Kung, Xin Dong, Marcus Z. Comiter, ChangFu Kuo |  |
| 167 |  |  [Balancing via Generation for Multi-Class Text Classification Improvement](https://doi.org/10.18653/v1/2020.findings-emnlp.130) |  | 0 | Data balancing is a known technique for improving the performance of classification tasks. In this work we define a novel balancing-viageneration framework termed BalaGen. BalaGen consists of a flexible balancing policy coupled with a text generation mechanism. Combined, these two techniques can be... | Naama Tepper, Esther Goldbraich, Naama Zwerdling, George Kour, Ateret AnabyTavor, Boaz Carmeli |  |
| 168 |  |  [Conditional Neural Generation using Sub-Aspect Functions for Extractive News Summarization](https://doi.org/10.18653/v1/2020.findings-emnlp.131) |  | 0 | Much progress has been made in text summarization, fueled by neural architectures using large-scale training corpora. However, in the news domain, neural models easily overfit by leveraging position-related features due to the prevalence of the inverted pyramid writing style. In addition, there is... | Zhengyuan Liu, Ke Shi, Nancy F. Chen |  |
| 169 |  |  [Research Replication Prediction Using Weakly Supervised Learning](https://doi.org/10.18653/v1/2020.findings-emnlp.132) |  | 0 | Knowing whether a published research result can be replicated is important. Carrying out direct replication of published research incurs a high cost. There are efforts tried to use machine learning aided methods to predict scientific claims’ replicability. However, existing machine learning aided... | Tianyi Luo, Xingyu Li, Hainan Wang, Yang Liu |  |
| 170 |  |  [Open Domain Question Answering based on Text Enhanced Knowledge Graph with Hyperedge Infusion](https://doi.org/10.18653/v1/2020.findings-emnlp.133) |  | 0 | The incompleteness of knowledge base (KB) is a vital factor limiting the performance of question answering (QA). This paper proposes a novel QA method by leveraging text information to enhance the incomplete KB. The model enriches the entity representation through semantic information contained in... | Jiale Han, Bo Cheng, Xu Wang |  |
| 171 |  |  [Inexpensive Domain Adaptation of Pretrained Language Models: Case Studies on Biomedical NER and Covid-19 QA](https://doi.org/10.18653/v1/2020.findings-emnlp.134) |  | 0 | Domain adaptation of Pretrained Language Models (PTLMs) is typically achieved by unsupervised pretraining on target-domain text. While successful, this approach is expensive in terms of hardware, runtime and CO 2 emissions. Here, we propose a cheaper alternative: We train Word2Vec on target-domain... | Nina Pörner, Ulli Waltinger, Hinrich Schütze |  |
| 172 |  |  [Semantically Driven Sentence Fusion: Modeling and Evaluation](https://doi.org/10.18653/v1/2020.findings-emnlp.135) |  | 0 | Sentence fusion is the task of joining related sentences into coherent text. Current training and evaluation schemes for this task are based on single reference ground-truths and do not account for valid fusion variants. We show that this hinders models from robustly capturing the semantic... | Eyal BenDavid, Orgad Keller, Eric Malmi, Idan Szpektor, Roi Reichart |  |
| 173 |  |  [Pseudo-Bidirectional Decoding for Local Sequence Transduction](https://doi.org/10.18653/v1/2020.findings-emnlp.136) |  | 0 | Local sequence transduction (LST) tasks are sequence transduction tasks where there exists massive overlapping between the source and target sequences, such as grammatical error correction and spell or OCR correction. Motivated by this characteristic of LST tasks, we propose Pseudo-Bidirectional... | Wangchunshu Zhou, Tao Ge, Ke Xu |  |
| 174 |  |  [Predicting Responses to Psychological Questionnaires from Participants' Social Media Posts and Question Text Embeddings](https://doi.org/10.18653/v1/2020.findings-emnlp.137) |  | 0 | Psychologists routinely assess people’s emotions and traits, such as their personality, by collecting their responses to survey questionnaires. Such assessments can be costly in terms of both time and money, and often lack generalizability, as existing data cannot be used to predict responses for... | Huy Vu, Suhaib Abdurahman, Sudeep Bhatia, Lyle H. Ungar |  |
| 175 |  |  [Will it Unblend?](https://doi.org/10.18653/v1/2020.findings-emnlp.138) |  | 0 | Natural language processing systems often struggle with out-of-vocabulary (OOV) terms, which do not appear in training data. Blends, such as “innoventor”, are one particularly challenging class of OOV, as they are formed by fusing together two or more bases that relate to the intended meaning in... | Yuval Pinter, Cassandra L. Jacobs, Jacob Eisenstein |  |
| 176 |  |  [CodeBERT: A Pre-Trained Model for Programming and Natural Languages](https://doi.org/10.18653/v1/2020.findings-emnlp.139) |  | 0 | We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop CodeBERT with... | Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, Ming Zhou |  |
| 177 |  |  [StyleDGPT: Stylized Response Generation with Pre-trained Language Models](https://doi.org/10.18653/v1/2020.findings-emnlp.140) |  | 0 | Generating responses following a desired style has great potentials to extend applications of open-domain dialogue systems, yet is refrained by lacking of parallel data for training. In this work, we explore the challenging task with pre-trained language models that have brought breakthrough to... | Ze Yang, Wei Wu, Can Xu, Xinnian Liang, Jiaqi Bai, Liran Wang, Wei Wang, Zhoujun Li |  |
| 178 |  |  [Enhancing Automated Essay Scoring Performance via Cohesion Measurement and Combination of Regression and Ranking](https://doi.org/10.18653/v1/2020.findings-emnlp.141) |  | 0 | Automated Essay Scoring (AES) is a critical text regression task that automatically assigns scores to essays based on their writing quality. Recently, the performance of sentence prediction tasks has been largely improved by using Pre-trained Language Models via fusing representations from different... | Ruosong Yang, Jiannong Cao, Zhiyuan Wen, Youzheng Wu, Xiaodong He |  |
| 179 |  |  [Neural Dialogue State Tracking with Temporally Expressive Networks](https://doi.org/10.18653/v1/2020.findings-emnlp.142) |  | 0 | Dialogue state tracking (DST) is an important part of a spoken dialogue system. Existing DST models either ignore temporal feature dependencies across dialogue turns or fail to explicitly model temporal state dependencies in a dialogue. In this work, we propose Temporally Expressive Networks (TEN)... | Junfan Chen, Richong Zhang, Yongyi Mao, Jie Xu |  |
| 180 |  |  [Inferring about fraudulent collusion risk on Brazilian public works contracts in official texts using a Bi-LSTM approach](https://doi.org/10.18653/v1/2020.findings-emnlp.143) |  | 0 | Public works procurements move US$ 10 billion yearly in Brazil and are a preferred field for collusion and fraud. Federal Police and audit agencies investigate collusion (bid-rigging), over-pricing, and delivery fraud in this field and efforts have been employed to early detect fraud and collusion... | Marcos C. Lima, Roberta Silva, Felipe Lopes de Souza Mendes, Leonardo Rebouças de Carvalho, Aletéia P. F. Araújo, Flavio de Barros Vidal |  |
| 181 |  |  [Record-to-Text Generation with Style Imitation](https://doi.org/10.18653/v1/2020.findings-emnlp.144) |  | 0 | Recent neural approaches to data-to-text generation have mostly focused on improving content fidelity while lacking explicit control over writing styles (e.g., sentence structures, word choices). More traditional systems use templates to determine the realization of text. Yet manual or automatic... | Shuai Lin, Wentao Wang, Zichao Yang, Xiaodan Liang, Frank F. Xu, Eric P. Xing, Zhiting Hu |  |
| 182 |  |  [Teaching Machine Comprehension with Compositional Explanations](https://doi.org/10.18653/v1/2020.findings-emnlp.145) |  | 0 | Advances in machine reading comprehension (MRC) rely heavily on the collection of large scale human-annotated examples in the form of (question, paragraph, answer) triples. In contrast, humans are typically able to generalize with only a few examples, relying on deeper underlying world knowledge,... | Qinyuan Ye, Xiao Huang, Elizabeth Boschee, Xiang Ren |  |
| 183 |  |  [A Knowledge-driven Approach to Classifying Object and Attribute Coreferences in Opinion Mining](https://doi.org/10.18653/v1/2020.findings-emnlp.146) |  | 0 | Classifying and resolving coreferences of objects (e.g., product names) and attributes (e.g., product aspects) in opinionated reviews is crucial for improving the opinion mining performance. However, the task is challenging as one often needs to consider domain-specific knowledge (e.g., iPad is a... | Jiahua Chen, Shuai Wang, Sahisnu Mazumder, Bing Liu |  |
| 184 |  |  [SimAlign: High Quality Word Alignments without Parallel Training Data using Static and Contextualized Embeddings](https://doi.org/10.18653/v1/2020.findings-emnlp.147) |  | 0 | Word alignments are useful for tasks like statistical and neural machine translation (NMT) and cross-lingual annotation projection. Statistical word aligners perform well, as do methods that extract alignments jointly with translations in NMT. However, most approaches require parallel training data... | Masoud Jalili Sabet, Philipp Dufter, François Yvon, Hinrich Schütze |  |
| 185 |  |  [TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification](https://doi.org/10.18653/v1/2020.findings-emnlp.148) |  | 0 | The experimental landscape in natural language processing for social media is too fragmented. Each year, new shared tasks and datasets are proposed, ranging from classics like sentiment analysis to irony detection or emoji prediction. Therefore, it is unclear what the current state of the art is, as... | Francesco Barbieri, José CamachoCollados, Luis Espinosa Anke, Leonardo Neves |  |
| 186 |  |  [Octa: Omissions and Conflicts in Target-Aspect Sentiment Analysis](https://doi.org/10.18653/v1/2020.findings-emnlp.149) |  | 0 | Sentiments in opinionated text are often determined by both aspects and target words (or targets). We observe that targets and aspects interrelate in subtle ways, often yielding conflicting sentiments. Thus, a naive aggregation of sentiments from aspects and targets treated separately, as in... | Zhe Zhang, ChungWei Hang, Munindar P. Singh |  |
| 187 |  |  [On the Language Neutrality of Pre-trained Multilingual Representations](https://doi.org/10.18653/v1/2020.findings-emnlp.150) |  | 0 | Multilingual contextual embeddings, such as multilingual BERT and XLM-RoBERTa, have proved useful for many multi-lingual tasks. Previous work probed the cross-linguality of the representations indirectly using zero-shot transfer learning on morphological and syntactic tasks. We instead investigate... | Jindrich Libovický, Rudolf Rosa, Alexander Fraser |  |
| 188 |  |  [Cost-effective Selection of Pretraining Data: A Case Study of Pretraining BERT on Social Media](https://doi.org/10.18653/v1/2020.findings-emnlp.151) |  | 0 | Recent studies on domain-specific BERT models show that effectiveness on downstream tasks can be improved when models are pretrained on in-domain data. Often, the pretraining data used in these models are selected based on their subject matter, e.g., biology or computer science. Given the range of... | Xiang Dai, Sarvnaz Karimi, Ben Hachey, Cécile Paris |  |
| 189 |  |  [TopicBERT for Energy Efficient Document Classification](https://doi.org/10.18653/v1/2020.findings-emnlp.152) |  | 0 | Prior research notes that BERT’s computational cost grows quadratically with sequence length thus leading to longer training times, higher GPU memory constraints and carbon emissions. While recent work seeks to address these scalability issues at pre-training, these issues are also prominent in... | Yatin Chaudhary, Pankaj Gupta, Khushbu Saxena, Vivek Kulkarni, Thomas A. Runkler, Hinrich Schütze |  |
| 190 |  |  [Improving Constituency Parsing with Span Attention](https://doi.org/10.18653/v1/2020.findings-emnlp.153) |  | 0 | Constituency parsing is a fundamental and important task for natural language understanding, where a good representation of contextual information can help this task. N-grams, which is a conventional type of feature for contextual information, have been demonstrated to be useful in many tasks, and... | Yuanhe Tian, Yan Song, Fei Xia, Tong Zhang |  |
| 191 |  |  [Optimizing BERT for Unlabeled Text-Based Items Similarity](https://doi.org/10.18653/v1/2020.findings-emnlp.154) |  | 0 | Language models that utilize extensive self-supervised pre-training from unlabeled text, have recently shown to significantly advance the state-of-the-art performance in a variety of language understanding tasks. However, it is yet unclear if and how these recent models can be harnessed for... | Itzik Malkiel, Oren Barkan, Avi Caciularu, Noam Razin, Ori Katz, Noam Koenigstein |  |
| 192 |  |  [Multi-Agent Mutual Learning at Sentence-Level and Token-Level for Neural Machine Translation](https://doi.org/10.18653/v1/2020.findings-emnlp.155) |  | 0 | Mutual learning, where multiple agents learn collaboratively and teach one another, has been shown to be an effective way to distill knowledge for image classification tasks. In this paper, we extend mutual learning to the machine translation task and operate at both the sentence-level and the... | Baohao Liao, Yingbo Gao, Hermann Ney |  |
| 193 |  |  [DomBERT: Domain-oriented Language Model for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2020.findings-emnlp.156) |  | 0 | This paper focuses on learning domain-oriented language models driven by end tasks, which aims to combine the worlds of both general-purpose language models (such as ELMo and BERT) and domain-specific language understanding. We propose DomBERT, an extension of BERT to learn from both in-domain... | Hu Xu, Bing Liu, Lei Shu, Philip S. Yu |  |
| 194 |  |  [RMM: A Recursive Mental Model for Dialog Navigation](https://doi.org/10.18653/v1/2020.findings-emnlp.157) |  | 0 | Language-guided robots must be able to both ask humans questions and understand answers. Much existing work focuses only on the latter. In this paper, we go beyond instruction following and introduce a two-agent task where one agent navigates and asks questions that a second, guiding agent answers.... | Homero Roman Roman, Yonatan Bisk, Jesse Thomason, Asli Celikyilmaz, Jianfeng Gao |  |
| 195 |  |  [Will this Idea Spread Beyond Academia? Understanding Knowledge Transfer of Scientific Concepts across Text Corpora](https://doi.org/10.18653/v1/2020.findings-emnlp.158) |  | 0 | What kind of basic research ideas are more likely to get applied in practice? There is a long line of research investigating patterns of knowledge transfer, but it generally focuses on documents as the unit of analysis and follow their transfer into practice for a specific scientific domain. Here we... | Hancheng Cao, Mengjie Cheng, Zhepeng Cen, Daniel A. McFarland, Xiang Ren |  |
| 196 |  |  [Recurrent Inference in Text Editing](https://doi.org/10.18653/v1/2020.findings-emnlp.159) |  | 0 | In neural text editing, prevalent sequence-to-sequence based approaches directly map the unedited text either to the edited text or the editing operations, in which the performance is degraded by the limited source text encoding and long, varying decoding steps. To address this problem, we propose a... | Ning Shi, Ziheng Zeng, Haotian Zhang, Yichen Gong |  |
| 197 |  |  [An Empirical Exploration of Local Ordering Pre-training for Structured Learning](https://doi.org/10.18653/v1/2020.findings-emnlp.160) |  | 0 | Recently, pre-training contextualized encoders with language model (LM) objectives has been shown an effective semi-supervised method for structured prediction. In this work, we empirically explore an alternative pre-training method for contextualized encoders. Instead of predicting words in LMs, we... | Zhisong Zhang, Xiang Kong, Lori S. Levin, Eduard H. Hovy |  |
| 198 |  |  [Unsupervised Extractive Summarization by Pre-training Hierarchical Transformers](https://doi.org/10.18653/v1/2020.findings-emnlp.161) |  | 0 | Unsupervised extractive document summarization aims to select important sentences from a document without using labeled summaries during training. Existing methods are mostly graph-based with sentences as nodes and edge weights measured by sentence similarities. In this work, we find that... | Shusheng Xu, Xingxing Zhang, Yi Wu, Furu Wei, Ming Zhou |  |
| 199 |  |  [Active Learning Approaches to Enhancing Neural Machine Translation: An Empirical Study](https://doi.org/10.18653/v1/2020.findings-emnlp.162) |  | 0 | Active learning is an efficient approach for mitigating data dependency when training neural machine translation (NMT) models. In this paper, we explore new training frameworks by incorporating active learning into various techniques such as transfer learning and iterative back-translation (IBT)... | Yuekai Zhao, Haoran Zhang, Shuchang Zhou, Zhihua Zhang |  |
| 200 |  |  [Towards Fine-Grained Transfer: An Adaptive Graph-Interactive Framework for Joint Multiple Intent Detection and Slot Filling](https://doi.org/10.18653/v1/2020.findings-emnlp.163) |  | 0 | In real-world scenarios, users usually have multiple intents in the same utterance. Unfortunately, most spoken language understanding (SLU) models either mainly focused on the single intent scenario, or simply incorporated an overall intent context vector for all tokens, ignoring the fine-grained... | Libo Qin, Xiao Xu, Wanxiang Che, Ting Liu |  |
| 201 |  |  [Continual Learning Long Short Term Memory](https://doi.org/10.18653/v1/2020.findings-emnlp.164) |  | 0 | Catastrophic forgetting in neural networks indicates the performance decreasing of deep learning models on previous tasks while learning new tasks. To address this problem, we propose a novel Continual Learning Long Short Term Memory (CL-LSTM) cell in Recurrent Neural Network (RNN) in this paper.... | Xin Guo, Yu Tian, Qinghan Xue, Panos Lampropoulos, Steven Eliuk, Kenneth E. Barner, Xiaolong Wang |  |
| 202 |  |  [CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning](https://doi.org/10.18653/v1/2020.findings-emnlp.165) |  | 0 | Recently, large-scale pre-trained language models have demonstrated impressive performance on several commonsense-reasoning benchmark datasets. However, building machines with commonsense to compose realistically plausible sentences remains challenging. In this paper, we present a constrained text... | Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, Xiang Ren |  |
| 203 |  |  [Constrained Decoding for Computationally Efficient Named Entity Recognition Taggers](https://doi.org/10.18653/v1/2020.findings-emnlp.166) |  | 0 | Current state-of-the-art models for named entity recognition (NER) are neural models with a conditional random field (CRF) as the final layer. Entities are represented as per-token labels with a special structure in order to decode them into spans. Current work eschews prior knowledge of how the... | Brian Lester, Daniel Pressel, Amy Hemmeter, Sagnik Ray Choudhury, Srinivas Bangalore |  |
| 204 |  |  [On the Potential of Lexico-logical Alignments for Semantic Parsing to SQL Queries](https://doi.org/10.18653/v1/2020.findings-emnlp.167) |  | 0 | Large-scale semantic parsing datasets annotated with logical forms have enabled major advances in supervised approaches. But can richer supervision help even more? To explore the utility of fine-grained, lexical-level supervision, we introduce SQUALL, a dataset that enriches 11,276... | Tianze Shi, Chen Zhao, Jordan L. BoydGraber, Hal Daumé III, Lillian Lee |  |
| 205 |  |  [TED: A Pretrained Unsupervised Summarization Model with Theme Modeling and Denoising](https://doi.org/10.18653/v1/2020.findings-emnlp.168) |  | 0 | Text summarization aims to extract essential information from a piece of text and transform the text into a concise version. Existing unsupervised abstractive summarization models leverage recurrent neural networks framework while the recently proposed transformer exhibits much more capability.... | Ziyi Yang, Chenguang Zhu, Robert Gmyr, Michael Zeng, Xuedong Huang, Eric Darve |  |
| 206 |  |  [Improving End-to-End Bangla Speech Recognition with Semi-supervised Training](https://doi.org/10.18653/v1/2020.findings-emnlp.169) |  | 0 | Automatic speech recognition systems usually require large annotated speech corpus for training. The manual annotation of a large corpus is very difficult. It can be very helpful to use unsupervised and semi-supervised learning methods in addition to supervised learning. In this work, we focus on... | Nafis Sadeq, Nafis Tahmid Chowdhury, Farhan Tanvir Utshaw, Shafayat Ahmed, Muhammad Abdullah Adnan |  |
| 207 |  |  [No Gestures Left Behind: Learning Relationships between Spoken Language and Freeform Gestures](https://doi.org/10.18653/v1/2020.findings-emnlp.170) |  | 0 | We study relationships between spoken language and co-speech gestures in context of two key challenges. First, distributions of text and gestures are inherently skewed making it important to model the long tail. Second, gesture predictions are made at a subword level, making it important to learn... | Chaitanya Ahuja, Dong Won Lee, Ryo Ishii, LouisPhilippe Morency |  |
| 208 |  |  [UnifiedQA: Crossing Format Boundaries With a Single QA System](https://doi.org/10.18653/v1/2020.findings-emnlp.171) |  | 0 | Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary,... | Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, Hannaneh Hajishirzi |  |
| 209 |  |  [Robust and Interpretable Grounding of Spatial References with Relation Networks](https://doi.org/10.18653/v1/2020.findings-emnlp.172) |  | 0 | Learning representations of spatial references in natural language is a key challenge in tasks like autonomous navigation and robotic manipulation. Recent work has investigated various neural architectures for learning multi-modal representations for spatial concepts. However, the lack of explicit... | TsungYen Yang, Andrew S. Lan, Karthik Narasimhan |  |
| 210 |  |  [Pragmatic Issue-Sensitive Image Captioning](https://doi.org/10.18653/v1/2020.findings-emnlp.173) |  | 0 | Image captioning systems need to produce texts that are not only true but also relevant in that they are properly aligned with the current issues. For instance, in a newspaper article about a sports event, a caption that not only identifies the player in a picture but also comments on their... | Allen Nie, Reuben CohnGordon, Christopher Potts |  |
| 211 |  |  [PTUM: Pre-training User Model from Unlabeled User Behaviors via Self-supervision](https://doi.org/10.18653/v1/2020.findings-emnlp.174) |  | 0 | User modeling is critical for many personalized web services. Many existing methods model users based on their behaviors and the labeled data of target tasks. However, these methods cannot exploit useful information in unlabeled user behavior data, and their performance may be not optimal when... | Chuhan Wu, Fangzhao Wu, Tao Qi, Jianxun Lian, Yongfeng Huang, Xing Xie |  |
| 212 |  |  [Adversarial Subword Regularization for Robust Neural Machine Translation](https://doi.org/10.18653/v1/2020.findings-emnlp.175) |  | 0 | Exposing diverse subword segmentations to neural machine translation (NMT) models often improves the robustness of machine translation as NMT models can experience various subword candidates. However, the diversification of subword segmentations mostly relies on the pre-trained subword language... | Jungsoo Park, Mujeen Sung, Jinhyuk Lee, Jaewoo Kang |  |
| 213 |  |  [Learning Visual-Semantic Embeddings for Reporting Abnormal Findings on Chest X-rays](https://doi.org/10.18653/v1/2020.findings-emnlp.176) |  | 0 | Automatic medical image report generation has drawn growing attention due to its potential to alleviate radiologists’ workload. Existing work on report generation often trains encoder-decoder networks to generate complete reports. However, such models are affected by data bias (e.g. label imbalance)... | Jianmo Ni, ChunNan Hsu, Amilcare Gentili, Julian J. McAuley |  |
| 214 |  |  [SynET: Synonym Expansion using Transitivity](https://doi.org/10.18653/v1/2020.findings-emnlp.177) |  | 0 | In this paper, we study a new task of synonym expansion using transitivity, and propose a novel approach named SynET, which considers both the contexts of two given synonym pairs. It introduces an auxiliary task to reduce the impact of noisy sentences, and proposes a Multi-Perspective Entity... | Jiale Yu, Yongliang Shen, Xinyin Ma, Chenghao Jia, Chen Chen, Weiming Lu |  |
| 215 |  |  [Scheduled DropHead: A Regularization Method for Transformer Models](https://doi.org/10.18653/v1/2020.findings-emnlp.178) |  | 0 | We introduce DropHead, a structured dropout method specifically designed for regularizing the multi-head attention mechanism which is a key component of transformer. In contrast to the conventional dropout mechanism which randomly drops units or connections, DropHead drops entire attention heads... | Wangchunshu Zhou, Tao Ge, Furu Wei, Ming Zhou, Ke Xu |  |
| 216 |  |  [Multi-Turn Dialogue Generation in E-Commerce Platform with the Context of Historical Dialogue](https://doi.org/10.18653/v1/2020.findings-emnlp.179) |  | 0 | As an important research topic, customer service dialogue generation tends to generate generic seller responses by leveraging current dialogue information. In this study, we propose a novel and extensible dialogue generation method by leveraging sellers’ historical dialogue information, which can be... | Weisheng Zhang, Kaisong Song, Yangyang Kang, Zhongqing Wang, Changlong Sun, Xiaozhong Liu, Shoushan Li, Min Zhang, Luo Si |  |
| 217 |  |  [Automatically Identifying Gender Issues in Machine Translation using Perturbations](https://doi.org/10.18653/v1/2020.findings-emnlp.180) |  | 0 | The successful application of neural methods to machine translation has realized huge quality advances for the community. With these improvements, many have noted outstanding challenges, including the modeling and treatment of gendered language. While previous studies have identified issues using... | Hila Gonen, Kellie Webster |  |
| 218 |  |  [Ruler: Data Programming by Demonstration for Document Labeling](https://doi.org/10.18653/v1/2020.findings-emnlp.181) |  | 0 | Data programming aims to reduce the cost of curating training data by encoding domain knowledge as labeling functions over source data. As such it not only requires domain expertise but also programming experience, a skill that many subject matter experts lack. Additionally, generating functions by... | Sara Evensen, Chang Ge, Çagatay Demiralp |  |
| 219 |  |  [Dual Reconstruction: a Unifying Objective for Semi-Supervised Neural Machine Translation](https://doi.org/10.18653/v1/2020.findings-emnlp.182) |  | 0 | While Iterative Back-Translation and Dual Learning effectively incorporate monolingual training data in neural machine translation, they use different objectives and heuristic gradient approximation strategies, and have not been extensively compared. We introduce a novel dual reconstruction... | Weijia Xu, Xing Niu, Marine Carpuat |  |
| 220 |  |  [Focus-Constrained Attention Mechanism for CVAE-based Response Generation](https://doi.org/10.18653/v1/2020.findings-emnlp.183) |  | 0 | To model diverse responses for a given post, one promising way is to introduce a latent variable into Seq2Seq models. The latent variable is supposed to capture the discourse-level information and encourage the informativeness of target responses. However, such discourse-level information is often... | Zhi Cui, Yanran Li, Jiayi Zhang, Jianwei Cui, Chen Wei, Bin Wang |  |
| 221 |  |  [Chunk-based Chinese Spelling Check with Global Optimization](https://doi.org/10.18653/v1/2020.findings-emnlp.184) |  | 0 | Chinese spelling check is a challenging task due to the characteristics of the Chinese language, such as the large character set, no word boundary, and short word length. On the one hand, most of the previous works only consider corrections with similar character pronunciation or shape, failing to... | Zuyi Bao, Chen Li, Rui Wang |  |
| 222 |  |  [Multi-pretraining for Large-scale Text Classification](https://doi.org/10.18653/v1/2020.findings-emnlp.185) |  | 0 | Deep neural network-based pretraining methods have achieved impressive results in many natural language processing tasks including text classification. However, their applicability to large-scale text classification with numerous categories (e.g., several thousands) is yet to be well-studied, where... | KangMin Kim, Bumsu Hyeon, Yeachan Kim, JunHyung Park, SangKeun Lee |  |
| 223 |  |  [End-to-End Speech Recognition and Disfluency Removal](https://doi.org/10.18653/v1/2020.findings-emnlp.186) |  | 0 | Disfluency detection is usually an intermediate step between an automatic speech recognition (ASR) system and a downstream task. By contrast, this paper aims to investigate the task of end-to-end speech recognition and disfluency removal. We specifically explore whether it is possible to train an... | Paria Jamshid Lou, Mark Johnson |  |
| 224 |  |  [Characterizing the Value of Information in Medical Notes](https://doi.org/10.18653/v1/2020.findings-emnlp.187) |  | 0 | Machine learning models depend on the quality of input data. As electronic health records are widely adopted, the amount of data in health care is growing, along with complaints about the quality of medical notes. We use two prediction tasks, readmission prediction and in-hospital mortality... | ChaoChun Hsu, Shantanu Karnwal, Sendhil Mullainathan, Ziad Obermeyer, Chenhao Tan |  |
| 225 |  |  [KLearn: Background Knowledge Inference from Summarization Data](https://doi.org/10.18653/v1/2020.findings-emnlp.188) |  | 0 | The goal of text summarization is to compress documents to the relevant information while excluding background information already known to the receiver. So far, summarization researchers have given considerably more attention to relevance than to background knowledge. In contrast, this work puts... | Maxime Peyrard, Robert West |  |
| 226 |  |  [Extracting Chemical-Protein Interactions via Calibrated Deep Neural Network and Self-training](https://doi.org/10.18653/v1/2020.findings-emnlp.189) |  | 0 | The extraction of interactions between chemicals and proteins from several biomedical articles is important in many fields of biomedical research such as drug development and prediction of drug side effects. Several natural language processing methods, including deep neural network (DNN) models,... | Dongha Choi, Hyunju Lee |  |
| 227 |  |  [Logic2Text: High-Fidelity Natural Language Generation from Logical Forms](https://doi.org/10.18653/v1/2020.findings-emnlp.190) |  | 0 | Previous studies on Natural Language Generation (NLG) from structured data have primarily focused on surface-level descriptions of record sequences. However, for complex structured data, e.g., multi-row tables, it is often desirable for an NLG system to describe interesting facts from logical... | Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou Zhou, Yunkai Zhang, Sairam Sundaresan, William Yang Wang |  |
| 228 |  |  [MedICaT: A Dataset of Medical Images, Captions, and Textual References](https://doi.org/10.18653/v1/2020.findings-emnlp.191) |  | 0 | Understanding the relationship between figures and text is key to scientific document understanding. Medical figures in particular are quite complex, often consisting of several subfigures (75% of figures in our dataset), with detailed text describing their content. Previous work studying figures in... | Sanjay Subramanian, Lucy Lu Wang, Ben Bogin, Sachin Mehta, Madeleine van Zuylen, Sravanthi Parasa, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi |  |
| 229 |  |  [TSDG: Content-aware Neural Response Generation with Two-stage Decoding Process](https://doi.org/10.18653/v1/2020.findings-emnlp.192) |  | 0 | Neural response generative models have achieved remarkable progress in recent years but tend to yield irrelevant and uninformative responses. One of the reasons is that encoder-decoder based models always use a single decoder to generate a complete response at a stroke. This tends to generate... | Junsheng Kong, Zhicheng Zhong, Yi Cai, Xin Wu, Da Ren |  |
| 230 |  |  [Unsupervised Cross-Lingual Adaptation of Dependency Parsers Using CRF Autoencoders](https://doi.org/10.18653/v1/2020.findings-emnlp.193) |  | 0 | We consider the task of cross-lingual adaptation of dependency parsers without annotated target corpora and parallel corpora. Previous work either directly applies a discriminative source parser to the target language, ignoring unannotated target corpora, or employs an unsupervised generative parser... | Zhao Li, Kewei Tu |  |
| 231 |  |  [Diversify Question Generation with Continuous Content Selectors and Question Type Modeling](https://doi.org/10.18653/v1/2020.findings-emnlp.194) |  | 0 | Generating questions based on answers and relevant contexts is a challenging task. Recent work mainly pays attention to the quality of a single generated question. However, question generation is actually a one-to-many problem, as it is possible to raise questions with different focuses on contexts... | Zhen Wang, Siwei Rao, Jie Zhang, Zhen Qin, Guangjian Tian, Jun Wang |  |
| 232 |  |  [Participatory Research for Low-resourced Machine Translation: A Case Study in African Languages](https://doi.org/10.18653/v1/2020.findings-emnlp.195) |  | 0 | Research in NLP lacks geographic diversity, and the question of how NLP can be scaled to low-resourced languages has not yet been adequately solved. ‘Low-resourced’-ness is a complex problem going beyond data availability and reflects systemic problems in society. In this paper, we focus on the task... | Wilhelmina Nekoto, Vukosi Marivate, Tshinondiwa Matsila, Timi E. Fasubaa, Taiwo Fagbohungbe, Solomon Oluwole Akinola, Shamsuddeen Hassan Muhammad, Salomon Kabongo Kabenamualu, Salomey Osei, Freshia Sackey, Rubungo Andre Niyongabo, Ricky Macharm, Perez Ogayo, Orevaoghene Ahia, Musie Meressa Berhe, Mofetoluwa Adeyemi, Masabata MokgesiSelinga, Lawrence Okegbemi, Laura Martinus, Kolawole Tajudeen, Kevin Degila, Kelechi Ogueji, Kathleen Siminyu, Julia Kreutzer, Jason Webster, Jamiil Toure Ali, Jade Z. Abbott, Iroro Orife, Ignatius Ezeani, Idris Abdulkabir Dangana, Herman Kamper, Hady Elsahar, Goodness Duru, Ghollah Kioko, Espoir Murhabazi, Elan Van Biljon, Daniel Whitenack, Christopher Onyefuluchi, Chris Chinenye Emezue, Bonaventure F. P. Dossou, Blessing K. Sibanda, Blessing Itoro Bassey, Ayodele Olabiyi, Arshath Ramkilowan, Alp Öktem, Adewale Akinfaderin, Abdallah Bashir |  |
| 233 |  |  [ConveRT: Efficient and Accurate Conversational Representations from Transformers](https://doi.org/10.18653/v1/2020.findings-emnlp.196) |  | 0 | General-purpose pretrained sentence encoders such as BERT are not ideal for real-world conversational AI applications; they are computationally heavy, slow, and expensive to train. We propose ConveRT (Conversational Representations from Transformers), a pretraining framework for conversational tasks... | Matthew Henderson, Iñigo Casanueva, Nikola Mrksic, PeiHao Su, TsungHsien Wen, Ivan Vulic |  |
| 234 |  |  [Computer Assisted Translation with Neural Quality Estimation and Auotmatic Post-Editing](https://doi.org/10.18653/v1/2020.findings-emnlp.197) |  | 0 | With the advent of neural machine translation, there has been a marked shift towards leveraging and consuming the machine translation results. However, the gap between machine translation systems and human translators needs to be manually closed by post-editing. In this paper, we propose an... | Ke Wang, Jiayi Wang, Niyu Ge, Yangbin Shi, Yu Zhao, Kai Fan |  |
| 235 |  |  [Zero-Shot Rationalization by Multi-Task Transfer Learning from Question Answering](https://doi.org/10.18653/v1/2020.findings-emnlp.198) |  | 0 | Extracting rationales can help human understand which information the model utilizes and how it makes the prediction towards better interpretability. However, annotating rationales requires much effort and only few datasets contain such labeled rationales, making supervised learning for... | PoNien Kung, TseHsuan Yang, YiCheng Chen, ShengSiang Yin, YunNung Chen |  |
| 236 |  |  [The Role of Reentrancies in Abstract Meaning Representation Parsing](https://doi.org/10.18653/v1/2020.findings-emnlp.199) |  | 0 | Abstract Meaning Representation (AMR) parsing aims at converting sentences into AMR representations. These are graphs and not trees because AMR supports reentrancies (nodes with more than one parent). Following previous findings on the importance of reen- trancies for AMR, we empirically find and... | Marco Damonte, Ida Szubert, Shay B. Cohen, Mark Steedman |  |
| 237 |  |  [Cross-Lingual Suicidal-Oriented Word Embedding toward Suicide Prevention](https://doi.org/10.18653/v1/2020.findings-emnlp.200) |  | 0 | Early intervention for suicide risks with social media data has increasingly received great attention. Using a suicide dictionary created by mental health experts is one of the effective ways to detect suicidal ideation. However, little attention has been paid to validate whether and how the... | Daeun Lee, Soyoung Park, Jiwon Kang, Daejin Choi, Jinyoung Han |  |
| 238 |  |  [Service-oriented Text-to-SQL Parsing](https://doi.org/10.18653/v1/2020.findings-emnlp.201) |  | 0 | The information retrieval from relational database requires professionals who has an understanding of structural query language such as SQL. TEXT2SQL models apply natural language inference to enable user interacting the database via natural language utterance. Current TEXT2SQL models normally focus... | Wangsu Hu, Jilei Tian |  |
| 239 |  |  [Reinforcement Learning with Imbalanced Dataset for Data-to-Text Medical Report Generation](https://doi.org/10.18653/v1/2020.findings-emnlp.202) |  | 0 | Automated generation of medical reports that describe the findings in the medical images helps radiologists by alleviating their workload. Medical report generation system should generate correct and concise reports. However, data imbalance makes it difficult to train models accurately. Medical... | Toru Nishino, Ryota Ozaki, Yohei Momoki, Tomoki Taniguchi, Ryuji Kano, Norihisa Nakano, Yuki Tagawa, Motoki Taniguchi, Tomoko Ohkuma, Keigo Nakamura |  |
| 240 |  |  [Reducing the Frequency of Hallucinated Quantities in Abstractive Summaries](https://doi.org/10.18653/v1/2020.findings-emnlp.203) |  | 0 | It is well-known that abstractive summaries are subject to hallucination—including material that is not supported by the original text. While summaries can be made hallucination-free by limiting them to general phrases, such summaries would fail to be very informative. Alternatively, one can try to... | Zheng Zhao, Shay B. Cohen, Bonnie Webber |  |
| 241 |  |  [Rethinking Topic Modelling: From Document-Space to Term-Space](https://doi.org/10.18653/v1/2020.findings-emnlp.204) |  | 0 | This paper problematizes the reliance on documents as the basic notion for defining term interactions in standard topic models. As an alternative to this practice, we reformulate topic distributions as latent factors in term similarity space. We exemplify the idea using a number of standard word... | Magnus Sahlgren |  |
| 242 |  |  [Sparse and Decorrelated Representations for Stable Zero-shot NMT](https://doi.org/10.18653/v1/2020.findings-emnlp.205) |  | 0 | Using a single encoder and decoder for all directions and training with English-centric data is a popular scheme for multilingual NMT. However, zero-shot translation under this scheme is vulnerable to changes in training conditions, as the model degenerates by decoding non-English texts into English... | Bokyung Son, Sungwon Lyu |  |
| 243 |  |  [A Semi-supervised Approach to Generate the Code-Mixed Text using Pre-trained Encoder and Transfer Learning](https://doi.org/10.18653/v1/2020.findings-emnlp.206) |  | 0 | Code-mixing, the interleaving of two or more languages within a sentence or discourse is ubiquitous in multilingual societies. The lack of code-mixed training data is one of the major concerns for the development of end-to-end neural network-based models to be deployed for a variety of natural... | Deepak Gupta, Asif Ekbal, Pushpak Bhattacharyya |  |
| 244 |  |  [Integrating Graph Contextualized Knowledge into Pre-trained Language Models](https://doi.org/10.18653/v1/2020.findings-emnlp.207) |  | 0 | Complex node interactions are common in knowledge graphs (KGs), and these interactions can be considered as contextualized knowledge exists in the topological structure of KGs. Traditional knowledge representation learning (KRL) methods usually treat a single triple as a training unit, neglecting... | Bin He, Di Zhou, Jinghui Xiao, Xin Jiang, Qun Liu, Nicholas Jing Yuan, Tong Xu |  |
| 245 |  |  [Recursive Top-Down Production for Sentence Generation with Latent Trees](https://doi.org/10.18653/v1/2020.findings-emnlp.208) |  | 0 | We model the recursive production property of context-free grammars for natural and synthetic languages. To this end, we present a dynamic programming algorithm that marginalises over latent binary tree structures with N leaves, allowing us to compute the likelihood of a sequence of N tokens under a... | Shawn Tan, Yikang Shen, Alessandro Sordoni, Aaron C. Courville, Timothy J. O'Donnell |  |
| 246 |  |  [Guided Dialogue Policy Learning without Adversarial Learning in the Loop](https://doi.org/10.18653/v1/2020.findings-emnlp.209) |  | 0 | Reinforcement learning methods have emerged as a popular choice for training an efficient and effective dialogue policy. However, these methods suffer from sparse and unstable reward signals returned by a user simulator only when a dialogue finishes. Besides, the reward signal is manually designed... | Ziming Li, Sungjin Lee, Baolin Peng, Jinchao Li, Julia Kiseleva, Maarten de Rijke, Shahin Shayandeh, Jianfeng Gao |  |
| 247 |  |  [MultiDM-GCN: Aspect-Guided Response Generation in Multi-Domain Multi-Modal Dialogue System using Graph Convolution Network](https://doi.org/10.18653/v1/2020.findings-emnlp.210) |  | 0 | In the recent past, dialogue systems have gained immense popularity and have become ubiquitous. During conversations, humans not only rely on languages but seek contextual information through visual contents as well. In every task-oriented dialogue system, the user is guided by the different aspects... | Mauajama Firdaus, Nidhi Thakur, Asif Ekbal |  |
| 248 |  |  [Edge-Enhanced Graph Convolution Networks for Event Detection with Syntactic Relation](https://doi.org/10.18653/v1/2020.findings-emnlp.211) |  | 0 | Event detection (ED), a key subtask of information extraction, aims to recognize instances of specific event types in text. Previous studies on the task have verified the effectiveness of integrating syntactic dependency into graph convolutional networks. However, these methods usually ignore... | Shiyao Cui, Bowen Yu, Tingwen Liu, Zhenyu Zhang, Xuebin Wang, Jinqiao Shi |  |
| 249 |  |  [Semi-supervised Formality Style Transfer using Language Model Discriminator and Mutual Information Maximization](https://doi.org/10.18653/v1/2020.findings-emnlp.212) |  | 0 | Formality style transfer is the task of converting informal sentences to grammatically-correct formal sentences, which can be used to improve performance of many downstream NLP tasks. In this work, we propose a semi-supervised formality style transfer model that utilizes a language model-based... | Kunal Chawla, Diyi Yang |  |
| 250 |  |  [Differentially Private Representation for NLP: Formal Guarantee and An Empirical Study on Privacy and Fairness](https://doi.org/10.18653/v1/2020.findings-emnlp.213) |  | 0 | It has been demonstrated that hidden representation learned by deep model can encode private information of the input, hence can be exploited to recover such information with reasonable accuracy. To address this issue, we propose a novel approach called Differentially Private Neural Representation... | Lingjuan Lyu, Xuanli He, Yitong Li |  |
| 251 |  |  [Helpful or Hierarchical? Predicting the Communicative Strategies of Chat Participants, and their Impact on Success](https://doi.org/10.18653/v1/2020.findings-emnlp.214) |  | 0 | When interacting with each other, we motivate, advise, inform, show love or power towards our peers. However, the way we interact may also hold some indication on how successful we are, as people often try to help each other to achieve their goals. We study the chat interactions of thousands of... | Farzana Rashid, Tommaso Fornaciari, Dirk Hovy, Eduardo Blanco, Fernando VegaRedondo |  |
| 252 |  |  [Learning Knowledge Bases with Parameters for Task-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2020.findings-emnlp.215) |  | 0 | Task-oriented dialogue systems are either modularized with separate dialogue state tracking (DST) and management steps or end-to-end trainable. In either case, the knowledge base (KB) plays an essential role in fulfilling user requests. Modularized systems rely on DST to interact with the KB, which... | Andrea Madotto, Samuel Cahyawijaya, Genta Indra Winata, Yan Xu, Zihan Liu, Zhaojiang Lin, Pascale Fung |  |
| 253 |  |  [Generalizing Open Domain Fact Extraction and Verification to COVID-FACT thorough In-Domain Language Modeling](https://doi.org/10.18653/v1/2020.findings-emnlp.216) |  | 0 | With the epidemic of COVID-19, verifying the scientifically false online information, such as fake news and maliciously fabricated statements, has become crucial. However, the lack of training data in the scientific domain limits the performance of fact verification models. This paper proposes an... | Zhenghao Liu, Chenyan Xiong, Zhuyun Dai, Si Sun, Maosong Sun, Zhiyuan Liu |  |
| 254 |  |  [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://doi.org/10.18653/v1/2020.findings-emnlp.217) |  | 0 | This paper presents a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of optimizing one-step-ahead prediction in the traditional... | Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang, Ming Zhou |  |
| 255 |  |  [DivGAN: Towards Diverse Paraphrase Generation via Diversified Generative Adversarial Network](https://doi.org/10.18653/v1/2020.findings-emnlp.218) |  | 0 | Paraphrases refer to texts that convey the same meaning with different expression forms. Traditional seq2seq-based models on paraphrase generation mainly focus on the fidelity while ignoring the diversity of outputs. In this paper, we propose a deep generative model to generate diverse paraphrases.... | Yue Cao, Xiaojun Wan |  |
| 256 |  |  [Plug-and-Play Conversational Models](https://doi.org/10.18653/v1/2020.findings-emnlp.219) |  | 0 | There has been considerable progress made towards conversational models that generate coherent and fluent responses; however, this often involves training large language models on large dialogue datasets, such as Reddit. These large conversational models provide little control over the generated... | Andrea Madotto, Etsuko Ishii, Zhaojiang Lin, Sumanth Dathathri, Pascale Fung |  |
| 257 |  |  [Event-Driven Learning of Systematic Behaviours in Stock Markets](https://doi.org/10.18653/v1/2020.findings-emnlp.220) |  | 0 | It is reported that financial news, especially financial events expressed in news, provide information to investors’ long/short decisions and influence the movements of stock markets. Motivated by this, we leverage financial event streams to train a classification neural network that detects latent... | Xianchao Wu |  |
| 258 |  |  [You could have said that instead: Improving Chatbots with Natural Language Feedback](https://doi.org/10.18653/v1/2020.findings-emnlp.221) |  | 0 | The ubiquitous nature of dialogue systems and their interaction with users generate an enormous amount of data. Can we improve chatbots using this data? A self-feeding chatbot improves itself by asking natural language feedback when a user is dissatisfied with its response and uses this feedback as... | Makesh Narsimhan Sreedhar, Kun Ni, Siva Reddy |  |
| 259 |  |  [Adapting Coreference Resolution to Twitter Conversations](https://doi.org/10.18653/v1/2020.findings-emnlp.222) |  | 0 | The performance of standard coreference resolution is known to drop significantly on Twitter texts. We improve the performance of the (Lee et al., 2018) system, which is originally trained on OntoNotes, by retraining on manually-annotated Twitter conversation data. Further experiments by combining... | Berfin Aktas, Veronika Solopova, Annalena Kohnert, Manfred Stede |  |
| 260 |  |  [On Romanization for Model Transfer Between Scripts in Neural Machine Translation](https://doi.org/10.18653/v1/2020.findings-emnlp.223) |  | 0 | Transfer learning is a popular strategy to improve the quality of low-resource machine translation. For an optimal transfer of the embedding layer, the child and parent model should share a substantial part of the vocabulary. This is not the case when transferring to languages with a different... | Chantal Amrhein, Rico Sennrich |  |
| 261 |  |  [COSMIC: COmmonSense knowledge for eMotion Identification in Conversations](https://doi.org/10.18653/v1/2020.findings-emnlp.224) |  | 0 | In this paper, we address the task of utterance level emotion recognition in conversations using commonsense knowledge. We propose COSMIC, a new framework that incorporates different elements of commonsense such as mental states, events, and causal relations, and build upon them to learn... | Deepanway Ghosal, Navonil Majumder, Alexander F. Gelbukh, Rada Mihalcea, Soujanya Poria |  |
| 262 |  |  [Improving Compositional Generalization in Semantic Parsing](https://doi.org/10.18653/v1/2020.findings-emnlp.225) |  | 0 | Generalization of models to out-of-distribution (OOD) data has captured tremendous attention recently. Specifically, compositional generalization, i.e., whether a model generalizes to new structures built of components observed during training, has sparked substantial interest. In this work, we... | Inbar Oren, Jonathan Herzig, Nitish Gupta, Matt Gardner, Jonathan Berant |  |
| 263 |  |  [Answer Span Correction in Machine Reading Comprehension](https://doi.org/10.18653/v1/2020.findings-emnlp.226) |  | 0 | Answer validation in machine reading comprehension (MRC) consists of verifying an extracted answer against an input context and question pair. Previous work has looked at re-assessing the “answerability” of the question given the extracted answer. Here we address a different problem: the tendency of... | Revanth Gangi Reddy, Md. Arafat Sultan, Efsun Sarioglu Kayi, Rong Zhang, Vittorio Castelli, Avirup Sil |  |
| 264 |  |  [On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers](https://doi.org/10.18653/v1/2020.findings-emnlp.227) |  | 0 | Fine-tuning pre-trained contextualized embedding models has become an integral part of the NLP pipeline. At the same time, probing has emerged as a way to investigate the linguistic knowledge captured by pre-trained models. Very little is, however, understood about how fine-tuning affects the... | Marius Mosbach, Anna Khokhlova, Michael A. Hedderich, Dietrich Klakow |  |
| 265 |  |  [Zero-shot Entity Linking with Efficient Long Range Sequence Modeling](https://doi.org/10.18653/v1/2020.findings-emnlp.228) |  | 0 | This paper considers the problem of zero-shot entity linking, in which a link in the test time may not present in training. Following the prevailing BERT-based research efforts, we find a simple yet effective way is to expand the long-range sequence modeling. Unlike many previous methods, our method... | Zonghai Yao, Liangliang Cao, Huapu Pan |  |
| 266 |  |  [How Does Context Matter? On the Robustness of Event Detection with Context-Selective Mask Generalization](https://doi.org/10.18653/v1/2020.findings-emnlp.229) |  | 0 | Event detection (ED) aims to identify and classify event triggers in texts, which is a crucial subtask of event extraction (EE). Despite many advances in ED, the existing studies are typically centered on improving the overall performance of an ED model, which rarely consider the robustness of an ED... | Jian Liu, Yubo Chen, Kang Liu, Yantao Jia, Zhicheng Sheng |  |
| 267 |  |  [Adaptive Feature Selection for End-to-End Speech Translation](https://doi.org/10.18653/v1/2020.findings-emnlp.230) |  | 0 | Information in speech signals is not evenly distributed, making it an additional challenge for end-to-end (E2E) speech translation (ST) to learn to focus on informative features. In this paper, we propose adaptive feature selection (AFS) for encoder-decoder based E2E ST. We first pre-train an ASR... | Biao Zhang, Ivan Titov, Barry Haddow, Rico Sennrich |  |
| 268 |  |  [Abstractive Multi-Document Summarization via Joint Learning with Single-Document Summarization](https://doi.org/10.18653/v1/2020.findings-emnlp.231) |  | 0 | Single-document and multi-document summarizations are very closely related in both task definition and solution method. In this work, we propose to improve neural abstractive multi-document summarization by jointly learning an abstractive single-document summarizer. We build a unified model for... | Hanqi Jin, Xiaojun Wan |  |
| 269 |  |  [Blockwise Self-Attention for Long Document Understanding](https://doi.org/10.18653/v1/2020.findings-emnlp.232) |  | 0 | We present BlockBERT, a lightweight and efficient BERT model for better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training/inference time, which also enables attention heads to... | Jiezhong Qiu, Hao Ma, Omer Levy, Wentau Yih, Sinong Wang, Jie Tang |  |
| 270 |  |  [Unsupervised Few-Bits Semantic Hashing with Implicit Topics Modeling](https://doi.org/10.18653/v1/2020.findings-emnlp.233) |  | 0 | Semantic hashing is a powerful paradigm for representing texts as compact binary hash codes. The explosion of short text data has spurred the demand of few-bits hashing. However, the performance of existing semantic hashing methods cannot be guaranteed when applied to few-bits hashing because of... | Fanghua Ye, Jarana Manotumruksa, Emine Yilmaz |  |
| 271 |  |  [Grid Tagging Scheme for End-to-End Fine-grained Opinion Extraction](https://doi.org/10.18653/v1/2020.findings-emnlp.234) |  | 0 | Aspect-oriented Fine-grained Opinion Extraction (AFOE) aims at extracting aspect terms and opinion terms from review in the form of opinion pairs or additionally extracting sentiment polarity of aspect term to form opinion triplet. Because of containing several opinion factors, the complete AFOE... | Zhen Wu, Chengcan Ying, Fei Zhao, Zhifang Fan, Xinyu Dai, Rui Xia |  |
| 272 |  |  [Learning Numeral Embedding](https://doi.org/10.18653/v1/2020.findings-emnlp.235) |  | 0 | Word embedding is an essential building block for deep learning methods for natural language processing. Although word embedding has been extensively studied over the years, the problem of how to effectively embed numerals, a special subset of words, is still underexplored. Existing word embedding... | Chengyue Jiang, Zhonglin Nian, Kaihao Guo, Shanbo Chu, Yinggong Zhao, Libin Shen, Kewei Tu |  |
| 273 |  |  [An Investigation of Potential Function Designs for Neural CRF](https://doi.org/10.18653/v1/2020.findings-emnlp.236) |  | 0 | The neural linear-chain CRF model is one of the most widely-used approach to sequence labeling. In this paper, we investigate a series of increasingly expressive potential functions for neural CRF models, which not only integrate the emission and transition functions, but also explicitly take the... | Zechuan Hu, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, Kewei Tu |  |
| 274 |  |  [Fast End-to-end Coreference Resolution for Korean](https://doi.org/10.18653/v1/2020.findings-emnlp.237) |  | 0 | Recently, end-to-end neural network-based approaches have shown significant improvements over traditional pipeline-based models in English coreference resolution. However, such advancements came at a cost of computational complexity and recent works have not focused on tackling this problem. Hence,... | Cheoneum Park, Jamin Shin, Sungjoon Park, Joonho Lim, Changki Lee |  |
| 275 |  |  [Toward Stance-based Personas for Opinionated Dialogues](https://doi.org/10.18653/v1/2020.findings-emnlp.238) |  | 0 | In the context of chit-chat dialogues it has been shown that endowing systems with a persona profile is important to produce more coherent and meaningful conversations. Still, the representation of such personas has thus far been limited to a fact-based representation (e.g. “I have two cats.”). We... | Thomas Scialom, Serra Sinem Tekiroglu, Jacopo Staiano, Marco Guerini |  |
| 276 |  |  [Hierarchical Pre-training for Sequence Labelling in Spoken Dialog](https://doi.org/10.18653/v1/2020.findings-emnlp.239) |  | 0 | Sequence labelling tasks like Dialog Act and Emotion/Sentiment identification are a key component of spoken dialog systems. In this work, we propose a new approach to learn generic representations adapted to spoken dialog, which we evaluate on a new benchmark we call Sequence labellIng evaLuatIon... | Emile Chapuis, Pierre Colombo, Matteo Manica, Matthieu Labeau, Chloé Clavel |  |
| 277 |  |  [Extending Multilingual BERT to Low-Resource Languages](https://doi.org/10.18653/v1/2020.findings-emnlp.240) |  | 0 | Multilingual BERT (M-BERT) has been a huge success in both supervised and zero-shot cross-lingual transfer learning. However, this success is focused only on the top 104 languages in Wikipedia it was trained on. In this paper, we propose a simple but effective approach to extend M-BERT E-MBERT so it... | Zihan Wang, Karthikeyan K, Stephen Mayhew, Dan Roth |  |
| 278 |  |  [Out-of-Sample Representation Learning for Knowledge Graphs](https://doi.org/10.18653/v1/2020.findings-emnlp.241) |  | 0 | Many important problems can be formulated as reasoning in knowledge graphs. Representation learning has proved extremely effective for transductive reasoning, in which one needs to make new predictions for already observed entities. This is true for both attributed graphs(where each entity has an... | Marjan Albooyeh, Rishab Goel, Seyed Mehran Kazemi |  |
| 279 |  |  [Fine-Grained Grounding for Multimodal Speech Recognition](https://doi.org/10.18653/v1/2020.findings-emnlp.242) |  | 0 | Multimodal automatic speech recognition systems integrate information from images to improve speech recognition quality, by grounding the speech in the visual context. While visual signals have been shown to be useful for recovering entities that have been masked in the audio, these models should be... | Tejas Srinivasan, Ramon Sanabria, Florian Metze, Desmond Elliott |  |
| 280 |  |  [Unsupervised Expressive Rules Provide Explainability and Assist Human Experts Grasping New Domains](https://doi.org/10.18653/v1/2020.findings-emnlp.243) |  | 0 | Approaching new data can be quite deterrent; you do not know how your categories of interest are realized in it, commonly, there is no labeled data at hand, and the performance of domain adaptation methods is unsatisfactory. Aiming to assist domain experts in their first steps into a new task over a... | Eyal Shnarch, Leshem Choshen, Guy Moshkowich, Ranit Aharonov, Noam Slonim |  |
| 281 |  |  [Textual supervision for visually grounded spoken language understanding](https://doi.org/10.18653/v1/2020.findings-emnlp.244) |  | 0 | Visually-grounded models of spoken language understanding extract semantic information directly from speech, without relying on transcriptions. This is useful for low-resource languages, where transcriptions can be expensive or impossible to obtain. Recent work showed that these models can be... | Bertrand Higy, Desmond Elliott, Grzegorz Chrupala |  |
| 282 |  |  [Universal Dependencies according to BERT: both more specific and more general](https://doi.org/10.18653/v1/2020.findings-emnlp.245) |  | 0 | This work focuses on analyzing the form and extent of syntactic abstraction captured by BERT by extracting labeled dependency trees from self-attentions. Previous work showed that individual BERT heads tend to encode particular dependency relation types. We extend these findings by explicitly... | Tomasz Limisiewicz, David Marecek, Rudolf Rosa |  |
| 283 |  |  [Visual Objects As Context: Exploiting Visual Objects for Lexical Entailment](https://doi.org/10.18653/v1/2020.findings-emnlp.246) |  | 0 | We propose a new word representation method derived from visual objects in associated images to tackle the lexical entailment task. Although it has been shown that the Distributional Informativeness Hypothesis (DIH) holds on text, in which the DIH assumes that a context surrounding a hyponym is more... | Masayasu Muraoka, Tetsuya Nasukawa, Bishwaranjan Bhattacharjee |  |
| 284 |  |  [Learning to Plan and Realize Separately for Open-Ended Dialogue Systems](https://doi.org/10.18653/v1/2020.findings-emnlp.247) |  | 0 | Achieving true human-like ability to conduct a conversation remains an elusive goal for open-ended dialogue systems. We posit this is because extant approaches towards natural language generation (NLG) are typically construed as end-to-end architectures that do not adequately model human generation... | Sashank Santhanam, Zhuo Cheng, Brodie Mather, Bonnie J. Dorr, Archna Bhatia, Bryanna Hebenstreit, Alan Zemel, Adam Dalton, Tomek Strzalkowski, Samira Shaikh |  |
| 285 |  |  [Be Different to Be Better! A Benchmark to Leverage the Complementarity of Language and Vision](https://doi.org/10.18653/v1/2020.findings-emnlp.248) |  | 0 | This paper introduces BD2BB, a novel language and vision benchmark that requires multimodal models combine complementary information from the two modalities. Recently, impressive progress has been made to develop universal multimodal encoders suitable for virtually any language and vision tasks.... | Sandro Pezzelle, Claudio Greco, Greta Gandolfi, Eleonora Gualdoni, Raffaella Bernardi |  |
| 286 |  |  [Cross-Lingual Training of Neural Models for Document Ranking](https://doi.org/10.18653/v1/2020.findings-emnlp.249) |  | 0 | We tackle the challenge of cross-lingual training of neural document ranking models for mono-lingual retrieval, specifically leveraging relevance judgments in English to improve search in non-English languages. Our work successfully applies multi-lingual BERT (mBERT) to document ranking and... | Peng Shi, He Bai, Jimmy Lin |  |
| 287 |  |  [Improving Word Embedding Factorization for Compression using Distilled Nonlinear Neural Decomposition](https://doi.org/10.18653/v1/2020.findings-emnlp.250) |  | 0 | Word-embeddings are vital components of Natural Language Processing (NLP) models and have been extensively explored. However, they consume a lot of memory which poses a challenge for edge deployment. Embedding matrices, typically, contain most of the parameters for language models and about a third... | Vasileios Lioutas, Ahmad Rashid, Krtin Kumar, Md. Akmal Haidar, Mehdi Rezagholizadeh |  |
| 288 |  |  [PharmMT: A Neural Machine Translation Approach to Simplify Prescription Directions](https://doi.org/10.18653/v1/2020.findings-emnlp.251) |  | 0 | The language used by physicians and health professionals in prescription directions includes medical jargon and implicit directives and causes much confusion among patients. Human intervention to simplify the language at the pharmacies may introduce additional errors that can lead to potentially... | Jiazhao Li, Corey A. Lester, Xinyan Zhao, Yuting Ding, Yun Jiang, V. G. Vinod Vydiswaran |  |
| 289 |  |  [LSTMS Compose - and Learn - Bottom-Up](https://doi.org/10.18653/v1/2020.findings-emnlp.252) |  | 0 | Recent work in NLP shows that LSTM language models capture compositional structure in language data. In contrast to existing work, we consider the learning process that leads to compositional behavior. For a closer look at how an LSTM’s sequential representations are composed hierarchically, we... | Naomi Saphra, Adam Lopez |  |
| 290 |  |  [Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs](https://doi.org/10.18653/v1/2020.findings-emnlp.253) |  | 0 | Natural language rationales could provide intuitive, higher-level explanations that are easily understandable by humans, complementing the more broadly studied lower-level explanations based on gradients or attention weights. We present the first study focused on generating natural language... | Ana Marasovic, Chandra Bhagavatula, Jae Sung Park, Ronan Le Bras, Noah A. Smith, Yejin Choi |  |
| 291 |  |  [Corpora Evaluation and System Bias detection in Multi Document Summarization](https://doi.org/10.18653/v1/2020.findings-emnlp.254) |  | 0 | Multi-document summarization (MDS) is the task of reflecting key points from any set of documents into a concise text paragraph. In the past, it has been used to aggregate news, tweets, product reviews, etc. from various sources. Owing to no standard definition of the task, we encounter a plethora... | Alvin Dey, Tanya Chowdhury, Yash Kumar Atri, Tanmoy Chakraborty |  |
| 292 |  |  [Graph-to-Tree Neural Networks for Learning Structured Input-Output Translation with Applications to Semantic Parsing and Math Word Problem](https://doi.org/10.18653/v1/2020.findings-emnlp.255) |  | 0 | The celebrated Seq2Seq technique and its numerous variants achieve excellent performance on many tasks such as neural machine translation, semantic parsing, and math word problem solving. However, these models either only consider input objects as sequences while ignoring the important structural... | Shucheng Li, Lingfei Wu, Shiwei Feng, Fangli Xu, Fengyuan Xu, Sheng Zhong |  |
| 293 |  |  [Target Conditioning for One-to-Many Generation](https://doi.org/10.18653/v1/2020.findings-emnlp.256) |  | 0 | Neural Machine Translation (NMT) models often lack diversity in their generated translations, even when paired with search algorithm, like beam search. A challenge is that the diversity in translations are caused by the variability in the target language, and cannot be inferred from the source... | MarieAnne Lachaux, Armand Joulin, Guillaume Lample |  |
| 294 |  |  [Can Pre-training help VQA with Lexical Variations?](https://doi.org/10.18653/v1/2020.findings-emnlp.257) |  | 0 | Rephrasings or paraphrases are sentences with similar meanings expressed in different ways. Visual Question Answering (VQA) models are closing the gap with the oracle performance for datasets like VQA2.0. However, these models fail to perform well on rephrasings of a question, which raises some... | Shailza Jolly, Shubham Kapoor |  |
| 295 |  |  [FENAS: Flexible and Expressive Neural Architecture Search](https://doi.org/10.18653/v1/2020.findings-emnlp.258) |  | 0 | Architecture search is the automatic process of designing the model or cell structure that is optimal for the given dataset or task. Recently, this approach has shown good improvements in terms of performance (tested on language modeling and image classification) with reasonable training speed using... | Ramakanth Pasunuru, Mohit Bansal |  |
| 296 |  |  [Inferring symmetry in natural language](https://doi.org/10.18653/v1/2020.findings-emnlp.259) |  | 0 | We present a methodological framework for inferring symmetry of verb predicates in natural language. Empirical work on predicate symmetry has taken two main approaches. The feature-based approach focuses on linguistic features pertaining to symmetry. The context-based approach denies the existence... | Chelsea Tanchip, Lei Yu, Aotao Xu, Yang Xu |  |
| 297 |  |  [A Concise Model for Multi-Criteria Chinese Word Segmentation with Transformer Encoder](https://doi.org/10.18653/v1/2020.findings-emnlp.260) |  | 0 | Multi-criteria Chinese word segmentation (MCCWS) aims to exploit the relations among the multiple heterogeneous segmentation criteria and further improve the performance of each single criterion. Previous work usually regards MCCWS as different tasks, which are learned together under the multi-task... | Xipeng Qiu, Hengzhi Pei, Hang Yan, Xuanjing Huang |  |
| 298 |  |  [LEGAL-BERT: "Preparing the Muppets for Court'"](https://doi.org/10.18653/v1/2020.findings-emnlp.261) |  | 0 | BERT has achieved impressive performance in several NLP tasks. However, there has been limited investigation on its adaptation guidelines in specialised domains. Here we focus on the legal domain, where we explore several approaches for applying BERT models to downstream legal tasks, evaluating on... | Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, Ion Androutsopoulos |  |
| 299 |  |  [Enhancing Content Planning for Table-to-Text Generation with Data Understanding and Verification](https://doi.org/10.18653/v1/2020.findings-emnlp.262) |  | 0 | Neural table-to-text models, which select and order salient data, as well as verbalizing them fluently via surface realization, have achieved promising progress. Based on results from previous work, the performance bottleneck of current models lies in the stage of content planing (selecting and... | Heng Gong, Wei Bi, Xiaocheng Feng, Bing Qin, Xiaojiang Liu, Ting Liu |  |
| 300 |  |  [Contextual Text Style Transfer](https://doi.org/10.18653/v1/2020.findings-emnlp.263) |  | 0 | We introduce a new task, Contextual Text Style Transfer - translating a sentence into a desired style with its surrounding context taken into account. This brings two key challenges to existing style transfer approaches: (I) how to preserve the semantic meaning of target sentence and its consistency... | Yu Cheng, Zhe Gan, Yizhe Zhang, Oussama Elachqar, Dianqi Li, Jingjing Liu |  |
| 301 |  |  [DiPair: Fast and Accurate Distillation for Trillion-ScaleText Matching and Pair Modeling](https://doi.org/10.18653/v1/2020.findings-emnlp.264) |  | 0 | Pre-trained models like BERT ((Devlin et al., 2018) have dominated NLP / IR applications such as single sentence classification, text pair classification, and question answering. However, deploying these models in real systems is highly non-trivial due to their exorbitant computational costs. A... | Jiecao Chen, Liu Yang, Karthik Raman, Michael Bendersky, JungJung Yeh, Yun Zhou, Marc Najork, Danyang Cai, Ehsan Emadzadeh |  |
| 302 |  |  [Cross-Lingual Dependency Parsing by POS-Guided Word Reordering](https://doi.org/10.18653/v1/2020.findings-emnlp.265) |  | 0 | We propose a novel approach to cross-lingual dependency parsing based on word reordering. The words in each sentence of a source language corpus are rearranged to meet the word order in a target language under the guidance of a part-of-speech based language model (LM). To obtain the highest... | Lu Liu, Yi Zhou, Jianhan Xu, Xiaoqing Zheng, KaiWei Chang, Xuanjing Huang |  |
| 303 |  |  [Assessing Robustness of Text Classification through Maximal Safe Radius Computation](https://doi.org/10.18653/v1/2020.findings-emnlp.266) |  | 0 | Neural network NLP models are vulnerable to small modifications of the input that maintain the original meaning but result in a different prediction. In this paper, we focus on robustness of text classification against word substitutions, aiming to provide guarantees that the model prediction does... | Emanuele La Malfa, Min Wu, Luca Laurenti, Benjie Wang, Anthony Hartshorn, Marta Kwiatkowska |  |
| 304 |  |  [Social Commonsense Reasoning with Multi-Head Knowledge Attention](https://doi.org/10.18653/v1/2020.findings-emnlp.267) |  | 0 | Social Commonsense Reasoning requires understanding of text, knowledge about social events and their pragmatic implications, as well as commonsense reasoning skills. In this work we propose a novel multi-head knowledge attention model that encodes semi-structured commonsense inference rules and... | Debjit Paul, Anette Frank |  |
| 305 |  |  [TurnGPT: a Transformer-based Language Model for Predicting Turn-taking in Spoken Dialog](https://doi.org/10.18653/v1/2020.findings-emnlp.268) |  | 0 | Syntactic and pragmatic completeness is known to be important for turn-taking prediction, but so far machine learning models of turn-taking have used such linguistic information in a limited way. In this paper, we introduce TurnGPT, a transformer-based language model for predicting turn-shifts in... | Erik Ekstedt, Gabriel Skantze |  |
| 306 |  |  [A little goes a long way: Improving toxic language classification despite data scarcity](https://doi.org/10.18653/v1/2020.findings-emnlp.269) |  | 0 | Detection of some types of toxic language is hampered by extreme scarcity of labeled training data. Data augmentation – generating new synthetic data from a labeled seed dataset – can help. The efficacy of data augmentation on toxic language classification has not been fully explored. We present the... | Mika Juuti, Tommi Gröndahl, Adrian Flanagan, N. Asokan |  |
| 307 |  |  [An Instance Level Approach for Shallow Semantic Parsing in Scientific Procedural Text](https://doi.org/10.18653/v1/2020.findings-emnlp.270) |  | 0 | In specific domains, such as procedural scientific text, human labeled data for shallow semantic parsing is especially limited and expensive to create. Fortunately, such specific domains often use rather formulaic writing, such that the different ways of expressing relations in a small number of... | Daivik Swarup, Ahsaas Bajaj, Sheshera Mysore, Tim O'Gorman, Rajarshi Das, Andrew McCallum |  |
| 308 |  |  [General Purpose Text Embeddings from Pre-trained Language Models for Scalable Inference](https://doi.org/10.18653/v1/2020.findings-emnlp.271) |  | 0 | The state of the art on many NLP tasks is currently achieved by large pre-trained language models, which require a considerable amount of computation. We aim to reduce the inference cost in a setting where many different predictions are made on a single piece of text. In that case, computational... | Jingfei Du, Myle Ott, Haoran Li, Xing Zhou, Veselin Stoyanov |  |
| 309 |  |  [Learning to Model and Ignore Dataset Bias with Mixed Capacity Ensembles](https://doi.org/10.18653/v1/2020.findings-emnlp.272) |  | 0 | Many datasets have been shown to contain incidental correlations created by idiosyncrasies in the data collection process. For example, sentence entailment datasets can have spurious word-class correlations if nearly all contradiction sentences contain the word “not”, and image recognition datasets... | Christopher Clark, Mark Yatskar, Luke Zettlemoyer |  |
| 310 |  |  [Learning to Generalize for Sequential Decision Making](https://doi.org/10.18653/v1/2020.findings-emnlp.273) |  | 0 | We consider problems of making sequences of decisions to accomplish tasks, interacting via the medium of language. These problems are often tackled with reinforcement learning approaches. We find that these models do not generalize well when applied to novel task domains. However, the large amount... | Xusen Yin, Ralph M. Weischedel, Jonathan May |  |
| 311 |  |  [Effective Crowd-Annotation of Participants, Interventions, and Outcomes in the Text of Clinical Trial Reports](https://doi.org/10.18653/v1/2020.findings-emnlp.274) |  | 0 | The search for Participants, Interventions, and Outcomes (PIO) in clinical trial reports is a critical task in Evidence Based Medicine. For an automatic PIO extraction, high-quality corpora are needed. Obtaining such a corpus from crowdworkers, however, has been shown to be ineffective since (i)... | Markus Zlabinger, Marta Sabou, Sebastian Hofstätter, Allan Hanbury |  |
| 312 |  |  [Adversarial Grammatical Error Correction](https://doi.org/10.18653/v1/2020.findings-emnlp.275) |  | 0 | Recent works in Grammatical Error Correction (GEC) have leveraged the progress in Neural Machine Translation (NMT), to learn rewrites from parallel corpora of grammatically incorrect and corrected sentences, achieving state-of-the-art results. At the same time, Generative Adversarial Networks (GANs)... | Vipul Raheja, Dimitris Alikaniotis |  |
| 313 |  |  [On Long-Tailed Phenomena in Neural Machine Translation](https://doi.org/10.18653/v1/2020.findings-emnlp.276) |  | 0 | State-of-the-art Neural Machine Translation (NMT) models struggle with generating low-frequency tokens, tackling which remains a major challenge. The analysis of long-tailed phenomena in the context of structured prediction tasks is further hindered by the added complexities of search during... | Vikas Raunak, Siddharth Dalmia, Vivek Gupta, Florian Metze |  |
| 314 |  |  [Knowing What You Know: Calibrating Dialogue Belief State Distributions via Ensembles](https://doi.org/10.18653/v1/2020.findings-emnlp.277) |  | 0 | The ability to accurately track what happens during a conversation is essential for the performance of a dialogue system. Current state-of-the-art multi-domain dialogue state trackers achieve just over 55% accuracy on the current go-to benchmark, which means that in almost every second dialogue turn... | Carel van Niekerk, Michael Heck, Christian Geishauser, HsienChin Lin, Nurul Lubis, Marco Moresi, Milica Gasic |  |
| 315 |  |  [Domain Adversarial Fine-Tuning as an Effective Regularizer](https://doi.org/10.18653/v1/2020.findings-emnlp.278) |  | 0 | In Natural Language Processing (NLP), pretrained language models (LMs) that are transferred to downstream tasks have been recently shown to achieve state-of-the-art results. However, standard fine-tuning can degrade the general-domain representations captured during pretraining. To address this... | Giorgos Vernikos, Katerina Margatina, Alexandra Chronopoulou, Ion Androutsopoulos |  |
| 316 |  |  [CLAR: A Cross-Lingual Argument Regularizer for Semantic Role Labeling](https://doi.org/10.18653/v1/2020.findings-emnlp.279) |  | 0 | Semantic role labeling (SRL) identifies predicate-argument structure(s) in a given sentence. Although different languages have different argument annotations, polyglot training, the idea of training one model on multiple languages, has previously been shown to outperform monolingual baselines,... | Ishan Jindal, Yunyao Li, Siddhartha Brahma, Huaiyu Zhu |  |
| 317 |  |  [Neutralizing Gender Bias in Word Embedding with Latent Disentanglement and Counterfactual Generation](https://doi.org/10.18653/v1/2020.findings-emnlp.280) |  | 0 | Recent research demonstrates that word embeddings, trained on the human-generated corpus, have strong gender biases in embedding spaces, and these biases can result in the discriminative results from the various downstream tasks. Whereas the previous methods project word embeddings into a linear... | Seungjae Shin, Kyungwoo Song, JoonHo Jang, Hyemi Kim, Weonyoung Joo, IlChul Moon |  |
| 318 |  |  [Towards Domain-Independent Text Structuring Trainable on Large Discourse Treebanks](https://doi.org/10.18653/v1/2020.findings-emnlp.281) |  | 0 | Text structuring is a fundamental step in NLG, especially when generating multi-sentential text. With the goal of fostering more general and data-driven approaches to text structuring, we propose the new and domain-independent NLG task of structuring and ordering a (possibly large) set of EDUs. We... | Grigorii Guz, Giuseppe Carenini |  |
| 319 |  |  [Data Annealing for Informal Language Understanding Tasks](https://doi.org/10.18653/v1/2020.findings-emnlp.282) |  | 0 | There is a huge performance gap between formal and informal language understanding tasks. The recent pre-trained models that improved formal language understanding tasks did not achieve a comparable result on informal language. We propose data annealing transfer learning procedure to bridge the... | Jing Gu, Zhou Yu |  |
| 320 |  |  [A Multilingual View of Unsupervised Machine Translation](https://doi.org/10.18653/v1/2020.findings-emnlp.283) |  | 0 | We present a probabilistic framework for multilingual neural machine translation that encompasses supervised and unsupervised setups, focusing on unsupervised translation. In addition to studying the vanilla case where there is only monolingual data available, we propose a novel setup where one... | Xavier Garcia, Pierre Foret, Thibault Sellam, Ankur P. Parikh |  |
| 321 |  |  [An Evaluation Method for Diachronic Word Sense Induction](https://doi.org/10.18653/v1/2020.findings-emnlp.284) |  | 0 | The task of Diachronic Word Sense Induction (DWSI) aims to identify the meaning of words from their context, taking the temporal dimension into account. In this paper we propose an evaluation method based on large-scale time-stamped annotated biomedical data, and a range of evaluation measures... | Ashjan Alsulaimani, Erwan Moreau, Carl Vogel |  |
| 322 |  |  [Integrating Task Specific Information into Pretrained Language Models for Low Resource Fine Tuning](https://doi.org/10.18653/v1/2020.findings-emnlp.285) |  | 0 | Pretrained Language Models (PLMs) have improved the performance of natural language understanding in recent years. Such models are pretrained on large corpora, which encode the general prior knowledge of natural languages but are agnostic to information characteristic of downstream tasks. This often... | Rui Wang, Shijing Si, Guoyin Wang, Lei Zhang, Lawrence Carin, Ricardo Henao |  |
| 323 |  |  [Efficient Transformer-based Large Scale Language Representations using Hardware-friendly Block Structured Pruning](https://doi.org/10.18653/v1/2020.findings-emnlp.286) |  | 0 | Pretrained large-scale language models have increasingly demonstrated high accuracy on many natural language processing (NLP) tasks. However, the limited weight storage and computational speed on hardware platforms have impeded the popularity of pretrained models, especially in the era of edge... | Bingbing Li, Zhenglun Kong, Tianyun Zhang, Ji Li, Zhengang Li, Hang Liu, Caiwen Ding |  |
| 324 |  |  [KoBE: Knowledge-Based Machine Translation Evaluation](https://doi.org/10.18653/v1/2020.findings-emnlp.287) |  | 0 | We propose a simple and effective method for machine translation evaluation which does not require reference translations. Our approach is based on (1) grounding the entity mentions found in each source sentence and candidate translation against a large-scale multilingual knowledge base, and (2)... | Zorik Gekhman, Roee Aharoni, Genady Beryozkin, Markus Freitag, Wolfgang Macherey |  |
| 325 |  |  [Pushing the Limits of AMR Parsing with Self-Learning](https://doi.org/10.18653/v1/2020.findings-emnlp.288) |  | 0 | Abstract Meaning Representation (AMR) parsing has experienced a notable growth in performance in the last two years, due both to the impact of transfer learning and the development of novel architectures specific to AMR. At the same time, self-learning techniques have helped push the performance... | YoungSuk Lee, Ramón Fernandez Astudillo, Tahira Naseem, Revanth Gangi Reddy, Radu Florian, Salim Roukos |  |
| 326 |  |  [Towards Zero Shot Conditional Summarization with Adaptive Multi-task Fine-Tuning](https://doi.org/10.18653/v1/2020.findings-emnlp.289) |  | 0 | Automatic summarization research has traditionally focused on providing high quality general-purpose summaries of documents. However, there are many applications which require more specific summaries, such as supporting question answering or topic-based literature discovery. In this paper we study... | Travis R. Goodwin, Max E. Savery, Dina DemnerFushman |  |
| 327 |  |  [Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer](https://doi.org/10.18653/v1/2020.findings-emnlp.290) |  | 0 | Predicting missing facts in a knowledge graph(KG) is a crucial task in knowledge base construction and reasoning, and it has been the subject of much research in recent works us-ing KG embeddings. While existing KG embedding approaches mainly learn and predict facts within a single KG, a more... | Xuelu Chen, Muhao Chen, Changjun Fan, Ankith Uppunda, Yizhou Sun, Carlo Zaniolo |  |
| 328 |  |  [Towards Controllable Biases in Language Generation](https://doi.org/10.18653/v1/2020.findings-emnlp.291) |  | 0 | We present a general approach towards controllable societal biases in natural language generation (NLG). Building upon the idea of adversarial triggers, we develop a method to induce societal biases in generated text when input prompts contain mentions of specific demographic groups. We then analyze... | Emily Sheng, KaiWei Chang, Prem Natarajan, Nanyun Peng |  |
| 329 |  |  [RobBERT: a Dutch RoBERTa-based Language Model](https://doi.org/10.18653/v1/2020.findings-emnlp.292) |  | 0 | Pre-trained language models have been dominating the field of natural language processing in recent years, and have led to significant performance gains for various complex natural language tasks. One of the most prominent pre-trained language models is BERT, which was released as an English as well... | Pieter Delobelle, Thomas Winters, Bettina Berendt |  |
| 330 |  |  [Regularization of Distinct Strategies for Unsupervised Question Generation](https://doi.org/10.18653/v1/2020.findings-emnlp.293) |  | 0 | Unsupervised question answering (UQA) has been proposed to avoid the high cost of creating high-quality datasets for QA. One approach to UQA is to train a QA model with questions generated automatically. However, the generated questions are either too similar to a word sequence in the context or too... | Junmo Kang, Giwon Hong, Haritz Puerto San Roman, SungHyon Myaeng |  |
| 331 |  |  [Graph-to-Graph Transformer for Transition-based Dependency Parsing](https://doi.org/10.18653/v1/2020.findings-emnlp.294) |  | 0 | We propose the Graph2Graph Transformer architecture for conditioning on and predicting arbitrary graphs, and apply it to the challenging task of transition-based dependency parsing. After proposing two novel Transformer models of transition-based dependency parsing as strong baselines, we show that... | Alireza Mohammadshahi, James Henderson |  |
| 332 |  |  [WER we are and WER we think we are](https://doi.org/10.18653/v1/2020.findings-emnlp.295) |  | 0 | Natural language processing of conversational speech requires the availability of high-quality transcripts. In this paper, we express our skepticism towards the recent reports of very low Word Error Rates (WERs) achieved by modern Automatic Speech Recognition (ASR) systems on benchmark datasets. We... | Piotr Szymanski, Piotr Zelasko, Mikolaj Morzy, Adrian Szymczak, Marzena ZylaHoppe, Joanna Banaszczak, Lukasz Augustyniak, Jan Mizgajski, Yishay Carmiel |  |
| 333 |  |  [DeSMOG: Detecting Stance in Media On Global Warming](https://doi.org/10.18653/v1/2020.findings-emnlp.296) |  | 0 | Citing opinions is a powerful yet understudied strategy in argumentation. For example, an environmental activist might say, “Leading scientists agree that global warming is a serious concern,” framing a clause which affirms their own stance (“that global warming is serious”) as an opinion endorsed... | Yiwei Luo, Dallas Card, Dan Jurafsky |  |
| 334 |  |  [A Novel Challenge Set for Hebrew Morphological Disambiguation and Diacritics Restoration](https://doi.org/10.18653/v1/2020.findings-emnlp.297) |  | 0 | One of the primary tasks of morphological parsers is the disambiguation of homographs. Particularly difficult are cases of unbalanced ambiguity, where one of the possible analyses is far more frequent than the others. In such cases, there may not exist sufficient examples of the minority analyses in... | Avi Shmidman, Joshua Guedalia, Shaltiel Shmidman, Moshe Koppel, Reut Tsarfaty |  |
| 335 |  |  [Improve Transformer Models with Better Relative Position Embeddings](https://doi.org/10.18653/v1/2020.findings-emnlp.298) |  | 0 | The transformer model has demonstrated superior results on NLP tasks including machine translation and question answering. In this paper, we argue that the position information is not fully utilized in existing work. For example, the initial proposal of a sinusoid embedding is fixed and not... | Zhiheng Huang, Davis Liang, Peng Xu, Bing Xiang |  |
| 336 |  |  [A Sentiment-Controllable Topic-to-Essay Generator with Topic Knowledge Graph](https://doi.org/10.18653/v1/2020.findings-emnlp.299) |  | 0 | Generating a vivid, novel, and diverse essay with only several given topic words is a promising task of natural language generation. Previous work in this task exists two challenging problems: neglect of sentiment beneath the text and insufficient utilization of topic-related knowledge. Therefore,... | Lin Qiao, Jianhao Yan, Fandong Meng, Zhendong Yang, Jie Zhou |  |
| 337 |  |  [What-if I ask you to explain: Explaining the effects of perturbations in procedural text](https://doi.org/10.18653/v1/2020.findings-emnlp.300) |  | 0 | Our goal is to explain the effects of perturbations in procedural text, e.g., given a passage describing a rabbit’s life cycle, explain why illness (the perturbation) may reduce the rabbit population (the effect). Although modern systems are able to solve the original prediction task well (e.g.,... | Dheeraj Rajagopal, Niket Tandon, Peter Clark, Bhavana Dalvi, Eduard H. Hovy |  |
| 338 |  |  [RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://doi.org/10.18653/v1/2020.findings-emnlp.301) |  | 0 | Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation... | Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, Noah A. Smith |  |
| 339 |  |  [Improving Event Duration Prediction via Time-aware Pre-training](https://doi.org/10.18653/v1/2020.findings-emnlp.302) |  | 0 | End-to-end models in NLP rarely encode external world knowledge about length of time. We introduce two effective models for duration prediction, which incorporate external knowledge by reading temporal-related news sentences (time-aware pre-training). Specifically, one model predicts the range/unit... | Zonglin Yang, Xinya Du, Alexander M. Rush, Claire Cardie |  |
| 340 |  |  [Composed Variational Natural Language Generation for Few-shot Intents](https://doi.org/10.18653/v1/2020.findings-emnlp.303) |  | 0 | In this paper, we focus on generating training examples for few-shot intents in the realistic imbalanced scenario. To build connections between existing many-shot intents and few-shot intents, we consider an intent as a combination of a domain and an action, and propose a composed variational... | Congying Xia, Caiming Xiong, Philip S. Yu, Richard Socher |  |
| 341 |  |  [Document Reranking for Precision Medicine with Neural Matching and Faceted Summarization](https://doi.org/10.18653/v1/2020.findings-emnlp.304) |  | 0 | Information retrieval (IR) for precision medicine (PM) often involves looking for multiple pieces of evidence that characterize a patient case. This typically includes at least the name of a condition and a genetic variation that applies to the patient. Other factors such as demographic attributes,... | Jiho Noh, Ramakanth Kavuluru |  |
| 342 |  |  [On the Importance of Adaptive Data Collection for Extremely Imbalanced Pairwise Tasks](https://doi.org/10.18653/v1/2020.findings-emnlp.305) |  | 0 | Many pairwise classification tasks, such as paraphrase detection and open-domain question answering, naturally have extreme label imbalance (e.g., 99.99% of examples are negatives). In contrast, many recent datasets heuristically choose examples to ensure label balance. We show that these heuristics... | Stephen Mussmann, Robin Jia, Percy Liang |  |
| 343 |  |  [A Dual-Attention Network for Joint Named Entity Recognition and Sentence Classification of Adverse Drug Events](https://doi.org/10.18653/v1/2020.findings-emnlp.306) |  | 0 | An adverse drug event (ADE) is an injury resulting from medical intervention related to a drug. Automatic ADE detection from text is either fine-grained (ADE entity recognition) or coarse-grained (ADE assertive sentence classification), with limited efforts leveraging inter-dependencies among the... | Susmitha Wunnava, Xiao Qin, Tabassum Kakar, Xiangnan Kong, Elke A. Rundensteiner |  |
| 344 |  |  [BERT-kNN: Adding a kNN Search Component to Pretrained Language Models for Better QA](https://doi.org/10.18653/v1/2020.findings-emnlp.307) |  | 0 | Khandelwal et al. (2020) use a k-nearest-neighbor (kNN) component to improve language model performance. We show that this idea is beneficial for open-domain question answering (QA). To improve the recall of facts encountered during training, we combine BERT (Devlin et al., 2019) with a traditional... | Nora Kassner, Hinrich Schütze |  |
| 345 |  |  [Identifying spurious correlations for robust text classification](https://doi.org/10.18653/v1/2020.findings-emnlp.308) |  | 0 | The predictions of text classifiers are often driven by spurious correlations – e.g., the term “Spielberg” correlates with positively reviewed movies, even though the term itself does not semantically convey a positive sentiment. In this paper, we propose a method to distinguish spurious and genuine... | Zhao Wang, Aron Culotta |  |
| 346 |  |  [HoVer: A Dataset for Many-Hop Fact Extraction And Claim Verification](https://doi.org/10.18653/v1/2020.findings-emnlp.309) |  | 0 | We introduce HoVer (HOppy VERification), a dataset for many-hop evidence extraction and fact verification. It challenges models to extract facts from several Wikipedia articles that are relevant to a claim and classify whether the claim is supported or not-supported by the facts. In HoVer, the... | Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Kumar Singh, Mohit Bansal |  |
| 347 |  |  [Continual Learning for Natural Language Generation in Task-oriented Dialog Systems](https://doi.org/10.18653/v1/2020.findings-emnlp.310) |  | 0 | Natural language generation (NLG) is an essential component of task-oriented dialog systems. Despite the recent success of neural approaches for NLG, they are typically developed in an offline manner for particular domains. To better fit real-life applications where new data come in a stream, we... | Fei Mi, Liangwei Chen, Mengjie Zhao, Minlie Huang, Boi Faltings |  |
| 348 |  |  [UNQOVERing Stereotypical Biases via Underspecified Questions](https://doi.org/10.18653/v1/2020.findings-emnlp.311) |  | 0 | While language embeddings have been shown to have stereotyping biases, how these biases affect downstream question answering (QA) models remains unexplored. We present UNQOVER, a general framework to probe and quantify biases through underspecified questions. We show that a naive use of model scores... | Tao Li, Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Vivek Srikumar |  |
| 349 |  |  [A Semantics-based Approach to Disclosure Classification in User-Generated Online Content](https://doi.org/10.18653/v1/2020.findings-emnlp.312) |  | 0 | As users engage in public discourse, the rate of voluntarily disclosed personal information has seen a steep increase. So-called self-disclosure can result in a number of privacy concerns. Users are often unaware of the sheer amount of personal information they share across online forums,... | Chandan Akiti, Anna Cinzia Squicciarini, Sarah Michele Rajtmajer |  |
| 350 |  |  [Mining Knowledge for Natural Language Inference from Wikipedia Categories](https://doi.org/10.18653/v1/2020.findings-emnlp.313) |  | 0 | Accurate lexical entailment (LE) and natural language inference (NLI) often require large quantities of costly annotations. To alleviate the need for labeled data, we introduce WikiNLI: a resource for improving model performance on NLI and LE tasks. It contains 428,899 pairs of phrases constructed... | Mingda Chen, Zewei Chu, Karl Stratos, Kevin Gimpel |  |
| 351 |  |  [OCNLI: Original Chinese Natural Language Inference](https://doi.org/10.18653/v1/2020.findings-emnlp.314) |  | 0 | Despite the tremendous recent progress on natural language inference (NLI), driven largely by large-scale investment in new datasets (e.g.,SNLI, MNLI) and advances in modeling, most progress has been limited to English due to a lack of reliable datasets for most of the world’s languages. In this... | Hai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra Kübler, Lawrence S. Moss |  |
| 352 |  |  [Unsupervised Domain Adaptation for Cross-lingual Text Labeling](https://doi.org/10.18653/v1/2020.findings-emnlp.315) |  | 0 | Unsupervised domain adaptation addresses the problem of leveraging labeled data in a source domain to learn a well-performing model in a target domain where labels are unavailable. In this paper, we improve upon a recent theoretical work (Zhang et al., 2019b) and adopt the Margin Disparity... | Dejiao Zhang, Ramesh Nallapati, Henghui Zhu, Feng Nan, Cícero Nogueira dos Santos, Kathleen R. McKeown, Bing Xiang |  |
| 353 |  |  [Rethinking Supervised Learning and Reinforcement Learning in Task-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2020.findings-emnlp.316) |  | 0 | Dialogue policy learning for task-oriented dialogue systems has enjoyed great progress recently mostly through employing reinforcement learning methods. However, these approaches have become very sophisticated. It is time to re-evaluate it. Are we really making progress developing dialogue agents... | Ziming Li, Julia Kiseleva, Maarten de Rijke |  |
| 354 |  |  [What do we expect from Multiple-choice QA Systems?](https://doi.org/10.18653/v1/2020.findings-emnlp.317) |  | 0 | The recent success of machine learning systems on various QA datasets could be interpreted as a significant improvement in models’ language understanding abilities. However, using various perturbations, multiple recent works have shown that good performance on a dataset might not indicate... | Krunal Shah, Nitish Gupta, Dan Roth |  |
| 355 |  |  [Resource-Enhanced Neural Model for Event Argument Extraction](https://doi.org/10.18653/v1/2020.findings-emnlp.318) |  | 0 | Event argument extraction (EAE) aims to identify the arguments of an event and classify the roles that those arguments play. Despite great efforts made in prior work, there remain many challenges: (1) Data scarcity. (2) Capturing the long-range dependency, specifically, the connection between an... | Jie Ma, Shuai Wang, Rishita Anubhai, Miguel Ballesteros, Yaser AlOnaizan |  |
| 356 |  |  [Improving Target-side Lexical Transfer in Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2020.findings-emnlp.319) |  | 0 | To improve the performance of Neural Machine Translation (NMT) for low-resource languages (LRL), one effective strategy is to leverage parallel data from a related high-resource language (HRL). However, multilingual data has been found more beneficial for NMT models that translate from the LRL to a... | Luyu Gao, Xinyi Wang, Graham Neubig |  |
| 357 |  |  [Accurate Polyglot Semantic Parsing With DAG Grammars](https://doi.org/10.18653/v1/2020.findings-emnlp.320) |  | 0 | Semantic parses are directed acyclic graphs (DAGs), but in practice most parsers treat them as strings or trees, mainly because models that predict graphs are far less understood. This simplification, however, comes at a cost: there is no guarantee that the output is a well-formed graph. A recent... | Federico Fancellu, Ákos Kádár, Ran Zhang, Afsaneh Fazly |  |
| 358 |  |  [Approximation of Response Knowledge Retrieval in Knowledge-grounded Dialogue Generation](https://doi.org/10.18653/v1/2020.findings-emnlp.321) |  | 0 | This paper is concerned with improving dialogue generation models through injection of knowledge, e.g., content relevant to the post that can increase the quality of responses. Past research extends the training of the generative models by incorporating statistical properties of posts, responses and... | Wen Zheng, Natasa MilicFrayling, Ke Zhou |  |
| 359 |  |  [Evaluating Factuality in Generation with Dependency-level Entailment](https://doi.org/10.18653/v1/2020.findings-emnlp.322) |  | 0 | Despite significant progress in text generation models, a serious limitation is their tendency to produce text that is factually inconsistent with information in the input. Recent work has studied whether textual entailment systems can be used to identify factual errors; however, these... | Tanya Goyal, Greg Durrett |  |
| 360 |  |  [Cross-Lingual Text Classification with Minimal Resources by Transferring a Sparse Teacher](https://doi.org/10.18653/v1/2020.findings-emnlp.323) |  | 0 | Cross-lingual text classification alleviates the need for manually labeled documents in a target language by leveraging labeled documents from other languages. Existing approaches for transferring supervision across languages require expensive cross-lingual resources, such as parallel corpora, while... | Giannis Karamanolakis, Daniel Hsu, Luis Gravano |  |
| 361 |  |  [A Multi-Persona Chatbot for Hotline Counselor Training](https://doi.org/10.18653/v1/2020.findings-emnlp.324) |  | 0 | Suicide prevention hotline counselors aid individuals during difficult times through millions of calls and chats. A chatbot cannot safely replace a counselor, but we explore whether a chatbot can be developed to help train human counselors. Such a system needs to simulate intimate situations across... | Orianna Demasi, Yu Li, Zhou Yu |  |
| 362 |  |  [Narrative Text Generation with a Latent Discrete Plan](https://doi.org/10.18653/v1/2020.findings-emnlp.325) |  | 0 | Past work on story generation has demonstrated the usefulness of conditioning on a generation plan to generate coherent stories. However, these approaches have used heuristics or off-the-shelf models to first tag training stories with the desired type of plan, and then train generation models in a... | Harsh Jhamtani, Taylor BergKirkpatrick |  |
| 363 |  |  [Graph Transformer Networks with Syntactic and Semantic Structures for Event Argument Extraction](https://doi.org/10.18653/v1/2020.findings-emnlp.326) |  | 0 | The goal of Event Argument Extraction (EAE) is to find the role of each entity mention for a given event trigger word. It has been shown in the previous works that the syntactic structures of the sentences are helpful for the deep learning models for EAE. However, a major problem in such prior works... | Amir Pouran Ben Veyseh, Tuan Ngo Nguyen, Thien Huu Nguyen |  |
| 364 |  |  [The Box is in the Pen: Evaluating Commonsense Reasoning in Neural Machine Translation](https://doi.org/10.18653/v1/2020.findings-emnlp.327) |  | 0 | Does neural machine translation yield translations that are congenial with common sense? In this paper, we present a test suite to evaluate the commonsense reasoning capability of neural machine translation. The test suite consists of three test sets, covering lexical and contextless/contextual... | Jie He, Tao Wang, Deyi Xiong, Qun Liu |  |
| 365 |  |  [Using Visual Feature Space as a Pivot Across Languages](https://doi.org/10.18653/v1/2020.findings-emnlp.328) |  | 0 | Our work aims to leverage visual feature space to pass information across languages. We show that models trained to generate textual captions in more than one language conditioned on an input image can leverage their jointly trained feature space during inference to pivot across languages. We... | Ziyan Yang, Leticia PintoAlva, Franck Dernoncourt, Vicente Ordonez |  |
| 366 |  |  [An Empirical Study of Cross-Dataset Evaluation for Neural Summarization Systems](https://doi.org/10.18653/v1/2020.findings-emnlp.329) |  | 0 | Neural network-based models augmented with unsupervised pre-trained knowledge have achieved impressive performance on text summarization. However, most existing evaluation methods are limited to an in-domain setting, where summarizers are trained and evaluated on the same dataset. We argue that this... | Yiran Chen, Pengfei Liu, Ming Zhong, ZiYi Dou, Danqing Wang, Xipeng Qiu, Xuanjing Huang |  |
| 367 |  |  [Attending to Long-Distance Document Context for Sequence Labeling](https://doi.org/10.18653/v1/2020.findings-emnlp.330) |  | 0 | We present in this work a method for incorporating global context in long documents when making local decisions in sequence labeling problems like NER. Inspired by work in featurized log-linear models (Chieu and Ng, 2002; Sutton and McCallum, 2004), our model learns to attend to multiple mentions of... | Matthew Jörke, Jon Gillick, Matthew Sims, David Bamman |  |
| 368 |  |  [Global Bootstrapping Neural Network for Entity Set Expansion](https://doi.org/10.18653/v1/2020.findings-emnlp.331) |  | 0 | Bootstrapping for entity set expansion (ESE) has been studied for a long period, which expands new entities using only a few seed entities as supervision. Recent end-to-end bootstrapping approaches have shown their advantages in information capturing and bootstrapping process modeling. However, due... | Lingyong Yan, Xianpei Han, Ben He, Le Sun |  |
| 369 |  |  [Document Classification for COVID-19 Literature](https://doi.org/10.18653/v1/2020.findings-emnlp.332) |  | 0 | The global pandemic has made it more important than ever to quickly and accurately retrieve relevant scientific literature for effective consumption by researchers in a wide range of fields. We provide an analysis of several multi-label document classification models on the LitCovid dataset, a... | Bernal Jimenez Gutierrez, Jucheng Zeng, Dongdong Zhang, Ping Zhang, Yu Su |  |
| 370 |  |  [Adversarial Augmentation Policy Search for Domain and Cross-Lingual Generalization in Reading Comprehension](https://doi.org/10.18653/v1/2020.findings-emnlp.333) |  | 0 | Reading comprehension models often overfit to nuances of training datasets and fail at adversarial evaluation. Training with adversarially augmented dataset improves robustness against those adversarial attacks but hurts generalization of the models. In this work, we present several effective... | Adyasha Maharana, Mohit Bansal |  |
| 371 |  |  [Denoising Multi-Source Weak Supervision for Neural Text Classification](https://doi.org/10.18653/v1/2020.findings-emnlp.334) |  | 0 | We study the problem of learning neural text classifiers without using any labeled data, but only easy-to-provide rules as multiple weak supervision sources. This problem is challenging because rule-induced weak labels are often noisy and incomplete. To address these two challenges, we design a... | Wendi Ren, Yinghao Li, Hanting Su, David Kartchner, Cassie S. Mitchell, Chao Zhang |  |
| 372 |  |  [Dr. Summarize: Global Summarization of Medical Dialogue by Exploiting Local Structures](https://doi.org/10.18653/v1/2020.findings-emnlp.335) |  | 0 | Understanding a medical conversation between a patient and a physician poses unique natural language understanding challenge since it combines elements of standard open-ended conversation with very domain-specific elements that require expertise and medical knowledge. Summarization of medical... | Anirudh Joshi, Namit Katariya, Xavier Amatriain, Anitha Kannan |  |
| 373 |  |  [Generating Accurate EHR Assessment from Medical Graph](https://doi.org/10.18653/v1/2020.findings-emnlp.336) |  | 0 | One of the fundamental goals of artificial intelligence is to build computer-based expert systems. Inferring clinical diagnoses to generate a clinical assessment during a patient encounter is a crucial step towards building a medical diagnostic system. Previous works were mainly based on either... | Zhichao Yang, Hong Yu |  |
| 374 |  |  [Do Models of Mental Health Based on Social Media Data Generalize?](https://doi.org/10.18653/v1/2020.findings-emnlp.337) |  | 0 | Proxy-based methods for annotating mental health status in social media have grown popular in computational research due to their ability to gather large training samples. However, an emerging body of literature has raised new concerns regarding the validity of these types of methods for use in... | Keith Harrigian, Carlos Alejandro Aguirre, Mark Dredze |  |
| 375 |  |  [Context Analysis for Pre-trained Masked Language Models](https://doi.org/10.18653/v1/2020.findings-emnlp.338) |  | 0 | Pre-trained language models that learn contextualized word representations from a large un-annotated corpus have become a standard component for many state-of-the-art NLP systems. Despite their successful applications in various downstream NLP tasks, the extent of contextual impact on the word... | YiAn Lai, Garima Lalwani, Yi Zhang |  |
| 376 |  |  [Controllable Text Generation with Focused Variation](https://doi.org/10.18653/v1/2020.findings-emnlp.339) |  | 0 | This work introduces Focused-Variation Network (FVN), a novel model to control language generation. The main problems in previous controlled language generation models range from the difficulty of generating text according to the given attributes, to the lack of diversity of the generated texts. FVN... | Lei Shu, Alexandros Papangelis, YiChia Wang, Gökhan Tür, Hu Xu, Zhaleh Feizollahi, Bing Liu, Piero Molino |  |
| 377 |  |  [Modeling Preconditions in Text with a Crowd-sourced Dataset](https://doi.org/10.18653/v1/2020.findings-emnlp.340) |  | 0 | Preconditions provide a form of logical connection between events that explains why some events occur together and information that is complementary to the more widely studied relations such as causation, temporal ordering, entailment, and discourse relations. Modeling preconditions in text has been... | Heeyoung Kwon, Mahnaz Koupaee, Pratyush Singh, Gargi Sawhney, Anmol Shukla, Keerthi Kumar Kallur, Nathanael Chambers, Niranjan Balasubramanian |  |
| 378 |  |  [Reevaluating Adversarial Examples in Natural Language](https://doi.org/10.18653/v1/2020.findings-emnlp.341) |  | 0 | State-of-the-art attacks on NLP models lack a shared definition of a what constitutes a successful attack. We distill ideas from past work into a unified framework: a successful natural language adversarial example is a perturbation that fools the model and follows some linguistic constraints. We... | John X. Morris, Eli Lifland, Jack Lanchantin, Yangfeng Ji, Yanjun Qi |  |
| 379 |  |  [Question Answering with Long Multiple-Span Answers](https://doi.org/10.18653/v1/2020.findings-emnlp.342) |  | 0 | Answering questions in many real-world applications often requires complex and precise information excerpted from texts spanned across a long document. However, currently no such annotated dataset is publicly available, which hinders the development of neural question-answering (QA) systems. To this... | Ming Zhu, Aman Ahuja, DaCheng Juan, Wei Wei, Chandan K. Reddy |  |
| 380 |  |  [Inserting Information Bottleneck for Attribution in Transformers](https://doi.org/10.18653/v1/2020.findings-emnlp.343) |  | 0 | Pretrained transformers achieve the state of the art across tasks in natural language processing, motivating researchers to investigate their inner mechanisms. One common direction is to understand what features are important for prediction. In this paper, we apply information bottlenecks to analyze... | Zhiying Jiang, Raphael Tang, Ji Xin, Jimmy Lin |  |
| 381 |  |  [Event-Related Bias Removal for Real-time Disaster Events](https://doi.org/10.18653/v1/2020.findings-emnlp.344) |  | 0 | Social media has become an important tool to share information about crisis events such as natural disasters and mass attacks. Detecting actionable posts that contain useful information requires rapid analysis of huge volumes of data in real-time. This poses a complex problem due to the large amount... | Salvador Medina Maza, Evangelia Spiliopoulou, Eduard H. Hovy, Alexander G. Hauptmann |  |
| 382 |  |  [It's not a Non-Issue: Negation as a Source of Error in Machine Translation](https://doi.org/10.18653/v1/2020.findings-emnlp.345) |  | 0 | As machine translation (MT) systems progress at a rapid pace, questions of their adequacy linger. In this study we focus on negation, a universal, core property of human language that significantly affects the semantics of an utterance. We investigate whether translating negation is an issue for... | Md Mosharaf Hossain, Antonios Anastasopoulos, Eduardo Blanco, Alexis Palmer |  |
| 383 |  |  [Incremental Text-to-Speech Synthesis with Prefix-to-Prefix Framework](https://doi.org/10.18653/v1/2020.findings-emnlp.346) |  | 0 | Text-to-speech synthesis (TTS) has witnessed rapid progress in recent years, where neural methods became capable of producing audios with high naturalness. However, these efforts still suffer from two types of latencies: (a) the computational latency (synthesizing time), which grows linearly with... | Mingbo Ma, Baigong Zheng, Kaibo Liu, Renjie Zheng, Hairong Liu, Kainan Peng, Kenneth Church, Liang Huang |  |
| 384 |  |  [Joint Turn and Dialogue level User Satisfaction Estimation on Mulit-Domain Conversations](https://doi.org/10.18653/v1/2020.findings-emnlp.347) |  | 0 | Dialogue level quality estimation is vital for optimizing data driven dialogue management. Current automated methods to estimate turn and dialogue level user satisfaction employ hand-crafted features and rely on complex annotation schemes, which reduce the generalizability of the trained models. We... | Praveen Kumar Bodigutla, Aditya Tiwari, Spyros Matsoukas, Josep VallsVargas, Lazaros Polymenakos |  |
| 385 |  |  [ArraMon: A Joint Navigation-Assembly Instruction Interpretation Task in Dynamic Environments](https://doi.org/10.18653/v1/2020.findings-emnlp.348) |  | 0 | For embodied agents, navigation is an important ability but not an isolated goal. Agents are also expected to perform specific tasks after reaching the target location, such as picking up objects and assembling them into a particular arrangement. We combine Vision-andLanguage Navigation, assembling... | Hyounghun Kim, Abhaysinh Zala, Graham Burri, Hao Tan, Mohit Bansal |  |
| 386 |  |  [Fluent and Low-latency Simultaneous Speech-to-Speech Translation with Self-adaptive Training](https://doi.org/10.18653/v1/2020.findings-emnlp.349) |  | 0 | Simultaneous speech-to-speech translation is an extremely challenging but widely useful scenario that aims to generate target-language speech only a few seconds behind the source-language speech. In addition, we have to continuously translate a speech of multiple sentences, but all recent solutions... | Renjie Zheng, Mingbo Ma, Baigong Zheng, Kaibo Liu, Jiahong Yuan, Kenneth Church, Liang Huang |  |
| 387 |  |  [Towards Context-Aware Code Comment Generation](https://doi.org/10.18653/v1/2020.findings-emnlp.350) |  | 0 | Code comments are vital for software maintenance and comprehension, but many software projects suffer from the lack of meaningful and up-to-date comments in practice. This paper presents a novel approach to automatically generate code comments at a function level by targeting object-oriented... | Xiaohan Yu, Quzhe Huang, Zheng Wang, Yansong Feng, Dongyan Zhao |  |
| 388 |  |  [MCMH: Learning Multi-Chain Multi-Hop Rules for Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2020.findings-emnlp.351) |  | 0 | Multi-hop reasoning approaches over knowledge graphs infer a missing relationship between entities with a multi-hop rule, which corresponds to a chain of relationships. We extend existing works to consider a generalized form of multi-hop rules, where each rule is a set of relation chains. To learn... | Lu Zhang, Mo Yu, Tian Gao, Yue Yu |  |
| 389 |  |  [Finding the Optimal Vocabulary Size for Neural Machine Translation](https://doi.org/10.18653/v1/2020.findings-emnlp.352) |  | 0 | We cast neural machine translation (NMT) as a classification task in an autoregressive setting and analyze the limitations of both classification and autoregression components. Classifiers are known to perform better with balanced class distributions during training. Since the Zipfian nature of... | Thamme Gowda, Jonathan May |  |
| 390 |  |  [Weakly- and Semi-supervised Evidence Extraction](https://doi.org/10.18653/v1/2020.findings-emnlp.353) |  | 0 | For many prediction tasks, stakeholders desire not only predictions but also supporting evidence that a human can use to verify its correctness. However, in practice, evidence annotations may only be available for a minority of training examples (if available at all). In this paper, we propose new... | Danish Pruthi, Bhuwan Dhingra, Graham Neubig, Zachary C. Lipton |  |
| 391 |  |  [Making Information Seeking Easier: An Improved Pipeline for Conversational Search](https://doi.org/10.18653/v1/2020.findings-emnlp.354) |  | 0 | This paper presents a highly effective pipeline for passage retrieval in a conversational search setting. The pipeline comprises of two components: Conversational Term Selection (CTS) and Multi-View Reranking (MVR). CTS is responsible for performing the first-stage of passage retrieval. Given an... | Vaibhav Kumar, Jamie Callan |  |
| 392 |  |  [Generalizable and Explainable Dialogue Generation via Explicit Action Learning](https://doi.org/10.18653/v1/2020.findings-emnlp.355) |  | 0 | Response generation for task-oriented dialogues implicitly optimizes two objectives at the same time: task completion and language quality. Conditioned response generation serves as an effective approach to separately and better optimize these two objectives. Such an approach relies on system action... | Xinting Huang, Jianzhong Qi, Yu Sun, Rui Zhang |  |
| 393 |  |  [More Embeddings, Better Sequence Labelers?](https://doi.org/10.18653/v1/2020.findings-emnlp.356) |  | 0 | Recent work proposes a family of contextual embeddings that significantly improves the accuracy of sequence labelers over non-contextual embeddings. However, there is no definite conclusion on whether we can build better sequence labelers by combining different kinds of embeddings in various... | Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, Kewei Tu |  |
| 394 |  |  [NLP Service APIs and Models for Efficient Registration of New Clients](https://doi.org/10.18653/v1/2020.findings-emnlp.357) |  | 0 | State-of-the-art NLP inference uses enormous neural architectures and models trained for GPU-months, well beyond the reach of most consumers of NLP. This has led to one-size-fits-all public API-based NLP service models by major AI companies, serving millions of clients. They cannot afford... | Sahil Shah, Vihari Piratla, Soumen Chakrabarti, Sunita Sarawagi |  |
| 395 |  |  [Effects of Naturalistic Variation in Goal-Oriented Dialog](https://doi.org/10.18653/v1/2020.findings-emnlp.358) |  | 0 | Existing benchmarks used to evaluate the performance of end-to-end neural dialog systems lack a key component: natural variation present in human conversations. Most datasets are constructed through crowdsourcing, where the crowd workers follow a fixed template of instructions while enacting the... | Jatin Ganhotra, Robert Moore, Sachindra Joshi, Kahini Wadhawan |  |
| 396 |  |  [Determining Event Outcomes: The Case of #fail](https://doi.org/10.18653/v1/2020.findings-emnlp.359) |  | 0 | This paper targets the task of determining event outcomes in social media. We work with tweets containing either #cookingFail or #bakingFail, and show that many of the events described in them resulted in something edible. Tweets that contain images are more likely to result in edible albeit... | Srikala Murugan, Dhivya Chinnappa, Eduardo Blanco |  |
| 397 |  |  [WikiLingua: A New Benchmark Dataset for Multilingual Abstractive Summarization](https://doi.org/10.18653/v1/2020.findings-emnlp.360) |  | 0 | We introduce WikiLingua, a large-scale, multilingual dataset for the evaluation of cross-lingual abstractive summarization systems. We extract article and summary pairs in 18 languages from WikiHow, a high quality, collaborative resource of how-to guides on a diverse set of topics written by human... | Faisal Ladhak, Esin Durmus, Claire Cardie, Kathleen R. McKeown |  |
| 398 |  |  [Adversarial Training for Code Retrieval with Question-Description Relevance Regularization](https://doi.org/10.18653/v1/2020.findings-emnlp.361) |  | 0 | Code retrieval is a key task aiming to match natural and programming languages. In this work, we propose adversarial learning for code retrieval, that is regularized by question-description relevance. First, we adapt a simple adversarial learning technique to generate difficult code snippets given... | Jie Zhao, Huan Sun |  |
| 399 |  |  [Large Product Key Memory for Pre-trained Language Models](https://doi.org/10.18653/v1/2020.findings-emnlp.362) |  | 0 | Product key memory (PKM) proposed by Lample et al. (2019) enables to improve prediction accuracy by increasing model capacity efficiently with insignificant computational overhead. However, their empirical application is only limited to causal language modeling. Motivated by the recent success of... | Gyuwan Kim, TaeHwan Jung |  |
| 400 |  |  [Temporal Reasoning in Natural Language Inference](https://doi.org/10.18653/v1/2020.findings-emnlp.363) |  | 0 | We introduce five new natural language inference (NLI) datasets focused on temporal reasoning. We recast four existing datasets annotated for event duration—how long an event lasts—and event ordering—how events are temporally arranged—into more than one million NLI examples. We use these datasets to... | Siddharth Vashishtha, Adam Poliak, Yash Kumar Lal, Benjamin Van Durme, Aaron Steven White |  |
| 401 |  |  [A Pilot Study of Text-to-SQL Semantic Parsing for Vietnamese](https://doi.org/10.18653/v1/2020.findings-emnlp.364) |  | 0 | Semantic parsing is an important NLP task. However, Vietnamese is a low-resource language in this research area. In this paper, we present the first public large-scale Text-to-SQL semantic parsing dataset for Vietnamese. We extend and evaluate two strong semantic parsing baselines EditSQL (Zhang et... | Anh Tuan Nguyen, Mai Hoang Dao, Dat Quoc Nguyen |  |
| 402 |  |  [STANDER: An Expert-Annotated Dataset for News Stance Detection and Evidence Retrieval](https://doi.org/10.18653/v1/2020.findings-emnlp.365) |  | 0 | We present a new challenging news dataset that targets both stance detection (SD) and fine-grained evidence retrieval (ER). With its 3,291 expert-annotated articles, the dataset constitutes a high-quality benchmark for future research in SD and multi-task learning. We provide a detailed description... | Costanza Conforti, Jakob Berndt, Mohammad Taher Pilehvar, Chryssi Giannitsarou, Flavio Toxvaerd, Nigel Collier |  |
| 403 |  |  [An Empirical Methodology for Detecting and Prioritizing Needs during Crisis Events](https://doi.org/10.18653/v1/2020.findings-emnlp.366) |  | 0 | In times of crisis, identifying essential needs is crucial to providing appropriate resources and services to affected entities. Social media platforms such as Twitter contain a vast amount of information about the general public’s needs. However, the sparsity of information and the amount of noisy... | Maria Janina Sarol, Ly Dinh, Rezvaneh Rezapour, ChiehLi Chin, Pingjing Yang, Jana Diesner |  |
| 404 |  |  [SupMMD: A Sentence Importance Model for Extractive Summarisation using Maximum Mean Discrepancy](https://doi.org/10.18653/v1/2020.findings-emnlp.367) |  | 0 | Most work on multi-document summarization has focused on generic summarization of information present in each individual document set. However, the under-explored setting of update summarization, where the goal is to identify the new information present in each set, is of equal practical interest... | Umanga Bista, Alexander Patrick Mathews, Aditya Krishna Menon, Lexing Xie |  |
| 405 |  |  [Towards Low-Resource Semi-Supervised Dialogue Generation with Meta-Learning](https://doi.org/10.18653/v1/2020.findings-emnlp.368) |  | 0 | In this paper, we propose a meta-learning based semi-supervised explicit dialogue state tracker (SEDST) for neural dialogue generation, denoted as MEDST. Our main motivation is to further bridge the chasm between the need for high accuracy dialogue state tracker and the common reality that only... | Yi Huang, Junlan Feng, Shuo Ma, Xiaoyu Du, Xiaoting Wu |  |
| 406 |  |  [Connecting the Dots: A Knowledgeable Path Generator for Commonsense Question Answering](https://doi.org/10.18653/v1/2020.findings-emnlp.369) |  | 0 | Commonsense question answering (QA) requires background knowledge which is not explicitly stated in a given context. Prior works use commonsense knowledge graphs (KGs) to obtain this knowledge for reasoning. However, relying entirely on these KGs may not suffice, considering their limited coverage... | Peifeng Wang, Nanyun Peng, Filip Ilievski, Pedro A. Szekely, Xiang Ren |  |
| 407 |  |  [No Answer is Better Than Wrong Answer: A Reflection Model for Document Level Machine Reading Comprehension](https://doi.org/10.18653/v1/2020.findings-emnlp.370) |  | 0 | The Natural Questions (NQ) benchmark set brings new challenges to Machine Reading Comprehension: the answers are not only at different levels of granularity (long and short), but also of richer types (including no-answer, yes/no, single-span and multi-span). In this paper, we target at this... | Xuguang Wang, Linjun Shou, Ming Gong, Nan Duan, Daxin Jiang |  |
| 408 |  |  [Reference Language based Unsupervised Neural Machine Translation](https://doi.org/10.18653/v1/2020.findings-emnlp.371) |  | 0 | Exploiting a common language as an auxiliary for better translation has a long tradition in machine translation and lets supervised learning-based machine translation enjoy the enhancement delivered by the well-used pivot language in the absence of a source language to target language parallel... | Zuchao Li, Hai Zhao, Rui Wang, Masao Utiyama, Eiichiro Sumita |  |
| 409 |  |  [TinyBERT: Distilling BERT for Natural Language Understanding](https://doi.org/10.18653/v1/2020.findings-emnlp.372) |  | 0 | Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate... | Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu |  |
| 410 |  |  [Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder](https://doi.org/10.18653/v1/2020.findings-emnlp.373) |  | 0 | This paper demonstrates a fatal vulnerability in natural language inference (NLI) and text classification systems. More concretely, we present a ‘backdoor poisoning’ attack on NLP models. Our poisoning attack utilizes conditional adversarially regularized autoencoder (CARA) to generate poisoned... | Alvin Chan, Yi Tay, YewSoon Ong, Aston Zhang |  |
| 411 |  |  [#Turki$hTweets: A Benchmark Dataset for Turkish Text Correction](https://doi.org/10.18653/v1/2020.findings-emnlp.374) |  | 0 | #Turki$hTweets is a benchmark dataset for the task of correcting the user misspellings, with the purpose of introducing the first public Turkish dataset in this area. #Turki$hTweets provides correct/incorrect word annotations with a detailed misspelling category formulation based on the real user... | Asiye Tuba Koksal, Ozge Bozal, Emre Yürekli, Gizem Gezici |  |
| 412 |  |  [Assessing Human-Parity in Machine Translation on the Segment Level](https://doi.org/10.18653/v1/2020.findings-emnlp.375) |  | 0 | Recent machine translation shared tasks have shown top-performing systems to tie or in some cases even outperform human translation. Such conclusions about system and human performance are, however, based on estimates aggregated from scores collected over large test sets of translations and... | Yvette Graham, Christian Federmann, Maria Eskevich, Barry Haddow |  |
| 413 |  |  [Multichannel Generative Language Model: Learning All Possible Factorizations Within and Across Channels](https://doi.org/10.18653/v1/2020.findings-emnlp.376) |  | 0 | A channel corresponds to a viewpoint or transformation of an underlying meaning. A pair of parallel sentences in English and French express the same underlying meaning, but through two separate channels corresponding to their languages. In this work, we present the Multichannel Generative Language... | Harris Chan, Jamie Kiros, William Chan |  |
| 414 |  |  [Factorized Transformer for Multi-Domain Neural Machine Translation](https://doi.org/10.18653/v1/2020.findings-emnlp.377) |  | 0 | Multi-Domain Neural Machine Translation (NMT) aims at building a single system that performs well on a range of target domains. However, along with the extreme diversity of cross-domain wording and phrasing style, the imperfections of training data distribution and the inherent defects of the... | Yongchao Deng, Hongfei Yu, Heng Yu, Xiangyu Duan, Weihua Luo |  |
| 415 |  |  [Improving Named Entity Recognition with Attentive Ensemble of Syntactic Information](https://doi.org/10.18653/v1/2020.findings-emnlp.378) |  | 0 | Named entity recognition (NER) is highly sensitive to sentential syntactic and semantic properties where entities may be extracted according to how they are used and placed in the running text. To model such properties, one could rely on existing resources to providing helpful knowledge to the NER... | Yuyang Nie, Yuanhe Tian, Yan Song, Xiang Ao, Xiang Wan |  |
| 416 |  |  [Query-Key Normalization for Transformers](https://doi.org/10.18653/v1/2020.findings-emnlp.379) |  | 0 | Low-resource language translation is a challenging but socially valuable NLP task. Building on recent work adapting the Transformer’s normalization to this setting, we propose QKNorm, a normalization technique that modifies the attention mechanism to make the softmax function less prone to arbitrary... | Alex Henry, Prudhvi Raj Dachapally, Shubham Shantaram Pawar, Yuxuan Chen |  |
| 417 |  |  [Contract Discovery: Dataset and a Few-shot Semantic Retrieval Challenge with Competitive Baselines](https://doi.org/10.18653/v1/2020.findings-emnlp.380) |  | 0 | We propose a new shared task of semantic retrieval from legal texts, in which a so-called contract discovery is to be performed – where legal clauses are extracted from documents, given a few examples of similar clauses from other legal acts. The task differs substantially from conventional NLI and... | Lukasz Borchmann, Dawid Wisniewski, Andrzej Gretkowski, Izabela Kosmala, Dawid Jurkiewicz, Lukasz Szalkiewicz, Gabriela Palka, Karol Kaczmarek, Agnieszka Kaliska, Filip Gralinski |  |
| 418 |  |  [Vocabulary Adaptation for Domain Adaptation in Neural Machine Translation](https://doi.org/10.18653/v1/2020.findings-emnlp.381) |  | 0 | Neural network methods exhibit strong performance only in a few resource-rich domains. Practitioners therefore employ domain adaptation from resource-rich domains that are, in most cases, distant from the target domain. Domain adaptation between distant domains (e.g., movie subtitles and research... | Shoetsu Sato, Jin Sakuma, Naoki Yoshinaga, Masashi Toyoda, Masaru Kitsuregawa |  |
| 419 |  |  [A Shared-Private Representation Model with Coarse-to-Fine Extraction for Target Sentiment Analysis](https://doi.org/10.18653/v1/2020.findings-emnlp.382) |  | 0 | Target sentiment analysis aims to detect opinion targets along with recognizing their sentiment polarities from a sentence. Some models with span-based labeling have achieved promising results in this task. However, the relation between the target extraction task and the target classification task... | Peiqin Lin, Meng Yang |  |
| 420 |  |  [Detecting Media Bias in News Articles using Gaussian Bias Distributions](https://doi.org/10.18653/v1/2020.findings-emnlp.383) |  | 0 | Media plays an important role in shaping public opinion. Biased media can influence people in undesirable directions and hence should be unmasked as such. We observe that feature-based and neural text classification approaches which rely only on the distribution of low-level lexical information fail... | WeiFan Chen, Khalid Al Khatib, Benno Stein, Henning Wachsmuth |  |
| 421 |  |  [How Can Self-Attention Networks Recognize Dyck-n Languages?](https://doi.org/10.18653/v1/2020.findings-emnlp.384) |  | 0 | We focus on the recognition of Dyck-n (Dn) languages with self-attention (SA) networks, which has been deemed to be a difficult task for these networks. We compare the performance of two variants of SA, one with a starting symbol (SA+) and one without (SA-). Our results show that SA+ is able to... | Javid Ebrahimi, Dhruv Gelda, Wei Zhang |  |
| 422 |  |  [Training Flexible Depth Model by Multi-Task Learning for Neural Machine Translation](https://doi.org/10.18653/v1/2020.findings-emnlp.385) |  | 0 | The standard neural machine translation model can only decode with the same depth configuration as training. Restricted by this feature, we have to deploy models of various sizes to maintain the same translation latency, because the hardware conditions on different terminal devices (e.g., mobile... | Qiang Wang, Tong Xiao, Jingbo Zhu |  |
| 423 |  |  [Looking inside Noun Compounds: Unsupervised Prepositional and Free Paraphrasing using Language Models](https://doi.org/10.18653/v1/2020.findings-emnlp.386) |  | 0 | A noun compound is a sequence of contiguous nouns that acts as a single noun, although the predicate denoting the semantic relation between its components is dropped. Noun Compound Interpretation is the task of uncovering the relation, in the form of a preposition or a free paraphrase. Prepositional... | Girishkumar Ponkiya, V. Rudra Murthy, Pushpak Bhattacharyya, Girish Keshav Palshikar |  |
| 424 |  |  [The birth of Romanian BERT](https://doi.org/10.18653/v1/2020.findings-emnlp.387) |  | 0 | Large-scale pretrained language models have become ubiquitous in Natural Language Processing. However, most of these models are available either in high-resource languages, in particular English, or as multilingual models that compromise performance on individual languages for coverage. This paper... | Stefan Daniel Dumitrescu, AndreiMarius Avram, Sampo Pyysalo |  |
| 425 |  |  [BERT for Monolingual and Cross-Lingual Reverse Dictionary](https://doi.org/10.18653/v1/2020.findings-emnlp.388) |  | 0 | Reverse dictionary is the task to find the proper target word given the word description. In this paper, we tried to incorporate BERT into this task. However, since BERT is based on the byte-pair-encoding (BPE) subword encoding, it is nontrivial to make BERT generate a word given the description. We... | Hang Yan, Xiaonan Li, Xipeng Qiu, Bocao Deng |  |
| 426 |  |  [What's so special about BERT's layers? A closer look at the NLP pipeline in monolingual and multilingual models](https://doi.org/10.18653/v1/2020.findings-emnlp.389) |  | 0 | Peeking into the inner workings of BERT has shown that its layers resemble the classical NLP pipeline, with progressively more complex tasks being concentrated in later layers. To investigate to what extent these results also hold for a language other than English, we probe a Dutch BERT-based model... | Wietse de Vries, Andreas van Cranenburgh, Malvina Nissim |  |
| 427 |  |  [Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?](https://doi.org/10.18653/v1/2020.findings-emnlp.390) |  | 0 | Data collection for natural language (NL) understanding tasks has increasingly included human explanations alongside data points, allowing past works to introduce models that both perform a task and generate NL explanations for their outputs. Yet to date, model-generated explanations have been... | Peter Hase, Shiyue Zhang, Harry Xie, Mohit Bansal |  |
| 428 |  |  [A Pointer Network Architecture for Joint Morphological Segmentation and Tagging](https://doi.org/10.18653/v1/2020.findings-emnlp.391) |  | 0 | Morphologically Rich Languages (MRLs) such as Arabic, Hebrew and Turkish often require Morphological Disambiguation (MD), i.e., the prediction of morphological decomposition of tokens into morphemes, early in the pipeline. Neural MD may be addressed as a simple pipeline, where segmentation is... | Amit Seker, Reut Tsarfaty |  |
| 429 |  |  [Beyond Language: Learning Commonsense from Images for Reasoning](https://doi.org/10.18653/v1/2020.findings-emnlp.392) |  | 0 | This paper proposes a novel approach to learn commonsense from images, instead of limited raw texts or costly constructed knowledge bases, for the commonsense reasoning problem in NLP. Our motivation comes from the fact that an image is worth a thousand words, where richer scene information could be... | Wanqing Cui, Yanyan Lan, Liang Pang, Jiafeng Guo, Xueqi Cheng |  |
| 430 |  |  [A BERT-based Distractor Generation Scheme with Multi-tasking and Negative Answer Training Strategies](https://doi.org/10.18653/v1/2020.findings-emnlp.393) |  | 0 | In this paper, we investigate the following two limitations for the existing distractor generation (DG) methods. First, the quality of the existing DG methods are still far from practical use. There are still room for DG quality improvement. Second, the existing DG designs are mainly for single... | HoLam Chung, YingHong Chan, YaoChung Fan |  |
| 431 |  |  [How Effective is Task-Agnostic Data Augmentation for Pretrained Transformers?](https://doi.org/10.18653/v1/2020.findings-emnlp.394) |  | 0 | Task-agnostic forms of data augmentation have proven widely effective in computer vision, even on pretrained models. In NLP similar results are reported most commonly for low data regimes, non-pretrained models, or situationally for pretrained models. In this paper we ask how effective these... | Shayne Longpre, Yu Wang, Chris DuBois |  |
| 432 |  |  [Visually-Grounded Planning without Vision: Language Models Infer Detailed Plans from High-level Instructions](https://doi.org/10.18653/v1/2020.findings-emnlp.395) |  | 0 | The recently proposed ALFRED challenge task aims for a virtual robotic agent to complete complex multi-step everyday tasks in a virtual home environment from high-level natural language directives, such as “put a hot piece of bread on a plate”. Currently, the best-performing models are able to... | Peter A. Jansen |  |
| 433 |  |  [Consistent Response Generation with Controlled Specificity](https://doi.org/10.18653/v1/2020.findings-emnlp.396) |  | 0 | We propose a method to control the specificity of responses while maintaining the consistency with the utterances. We first design a metric based on pointwise mutual information, which measures the co-occurrence degree between an utterance and a response. To control the specificity of generated... | Junya Takayama, Yuki Arase |  |
| 434 |  |  [Internal and External Pressures on Language Emergence: Least Effort, Object Constancy and Frequency](https://doi.org/10.18653/v1/2020.findings-emnlp.397) |  | 0 | In previous work, artificial agents were shown to achieve almost perfect accuracy in referential games where they have to communicate to identify images. Nevertheless, the resulting communication protocols rarely display salient features of natural languages, such as compositionality. In this paper,... | Diana Rodríguez Luna, Edoardo Maria Ponti, Dieuwke Hupkes, Elia Bruni |  |
| 435 |  |  [Parsing All: Syntax and Semantics, Dependencies and Spans](https://doi.org/10.18653/v1/2020.findings-emnlp.398) |  | 0 | Both syntactic and semantic structures are key linguistic contextual clues, in which parsing the latter has been well shown beneficial from parsing the former. However, few works ever made an attempt to let semantic parsing help syntactic parsing. As linguistic representation formalisms, both syntax... | Junru Zhou, Zuchao Li, Hai Zhao |  |
| 436 |  |  [LIMIT-BERT : Linguistics Informed Multi-Task BERT](https://doi.org/10.18653/v1/2020.findings-emnlp.399) |  | 0 | In this paper, we present Linguistics Informed Multi-Task BERT (LIMIT-BERT) for learning language representations across multiple linguistics tasks by Multi-Task Learning. LIMIT-BERT includes five key linguistics tasks: Part-Of-Speech (POS) tags, constituent and dependency syntactic parsing, span... | Junru Zhou, Zhuosheng Zhang, Hai Zhao, Shuailiang Zhang |  |
| 437 |  |  [Improving Limited Labeled Dialogue State Tracking with Self-Supervision](https://doi.org/10.18653/v1/2020.findings-emnlp.400) |  | 0 | Existing dialogue state tracking (DST) models require plenty of labeled data. However, collecting high-quality labels is costly, especially when the number of domains increases. In this paper, we address a practical DST problem that is rarely discussed, i.e., learning efficiently with limited... | ChienSheng Wu, Steven C. H. Hoi, Caiming Xiong |  |
| 438 |  |  [On the Branching Bias of Syntax Extracted from Pre-trained Language Models](https://doi.org/10.18653/v1/2020.findings-emnlp.401) |  | 0 | Many efforts have been devoted to extracting constituency trees from pre-trained language models, often proceeding in two stages: feature definition and parsing. However, this kind of methods may suffer from the branching bias issue, which will inflate the performances on languages with the same... | Huayang Li, Lemao Liu, Guoping Huang, Shuming Shi |  |
| 439 |  |  [The Pragmatics behind Politics: Modelling Metaphor, Framing and Emotion in Political Discourse](https://doi.org/10.18653/v1/2020.findings-emnlp.402) |  | 0 | There has been an increased interest in modelling political discourse within the natural language processing (NLP) community, in tasks such as political bias and misinformation detection, among others. Metaphor-rich and emotion-eliciting communication strategies are ubiquitous in political rhetoric,... | PereLluís Huguet Cabot, Verna Dankers, David Abadi, Agneta Fischer, Ekaterina Shutova |  |
| 440 |  |  [SMRTer Chatbots: Improving Non-Task-Oriented Dialog with Simulated Multi-Reference Training](https://doi.org/10.18653/v1/2020.findings-emnlp.403) |  | 0 | Non-task-oriented dialog models suffer from poor quality and non-diverse responses. To overcome limited conversational data, we apply Simulated Multiple Reference Training (SMRT; Khayrallah et al., 2020), and use a paraphraser to simulate multiple responses per training prompt. We find SMRT improves... | Huda Khayrallah, João Sedoc |  |
| 441 |  |  [PrivNet: Safeguarding Private Attributes in Transfer Learning for Recommendation](https://doi.org/10.18653/v1/2020.findings-emnlp.404) |  | 0 | Transfer learning is an effective technique to improve a target recommender system with the knowledge from a source domain. Existing research focuses on the recommendation performance of the target domain while ignores the privacy leakage of the source domain. The transferred knowledge, however, may... | Guangneng Hu, Qiang Yang |  |
| 442 |  |  [Learning to Learn to Disambiguate: Meta-Learning for Few-Shot Word Sense Disambiguation](https://doi.org/10.18653/v1/2020.findings-emnlp.405) |  | 0 | The success of deep learning methods hinges on the availability of large training datasets annotated for the task of interest. In contrast to human intelligence, these methods lack versatility and struggle to learn and adapt quickly to new tasks, where labeled data is scarce. Meta-learning aims to... | Nithin Holla, Pushkar Mishra, Helen Yannakoudakis, Ekaterina Shutova |  |
| 443 |  |  [An Empirical Investigation of Beam-Aware Training in Supertagging](https://doi.org/10.18653/v1/2020.findings-emnlp.406) |  | 0 | Structured prediction is often approached by training a locally normalized model with maximum likelihood and decoding approximately with beam search. This approach leads to mismatches as, during training, the model is not exposed to its mistakes and does not use beam search. Beam-aware training aims... | Renato Negrinho, Matthew R. Gormley, Geoffrey J. Gordon |  |
| 444 |  |  [Improving Aspect-based Sentiment Analysis with Gated Graph Convolutional Networks and Syntax-based Regulation](https://doi.org/10.18653/v1/2020.findings-emnlp.407) |  | 0 | Aspect-based Sentiment Analysis (ABSA) seeks to predict the sentiment polarity of a sentence toward a specific aspect. Recently, it has been shown that dependency trees can be integrated into deep learning models to produce the state-of-the-art performance for ABSA. However, these models tend to... | Amir Pouran Ben Veyseh, Nasim Nouri, Franck Dernoncourt, Quan Hung Tran, Dejing Dou, Thien Huu Nguyen |  |
| 445 |  |  [Decoding language spatial relations to 2D spatial arrangements](https://doi.org/10.18653/v1/2020.findings-emnlp.408) |  | 0 | We address the problem of multimodal spatial understanding by decoding a set of language-expressed spatial relations to a set of 2D spatial arrangements in a multi-object and multi-relationship setting. We frame the task as arranging a scene of clip-arts given a textual description. We propose a... | Gorjan Radevski, Guillem Collell, MarieFrancine Moens, Tinne Tuytelaars |  |
| 446 |  |  [The Dots Have Their Values: Exploiting the Node-Edge Connections in Graph-based Neural Models for Document-level Relation Extraction](https://doi.org/10.18653/v1/2020.findings-emnlp.409) |  | 0 | The goal of Document-level Relation Extraction (DRE) is to recognize the relations between entity mentions that can span beyond sentence boundary. The current state-of-the-art method for this problem has involved the graph-based edge-oriented model where the entity mentions, entities, and sentences... | Hieu Minh Tran, Trung Minh Nguyen, Thien Huu Nguyen |  |
| 447 |  |  [Why and when should you pool? Analyzing Pooling in Recurrent Architectures](https://doi.org/10.18653/v1/2020.findings-emnlp.410) |  | 0 | Pooling-based recurrent neural architectures consistently outperform their counterparts without pooling on sequence classification tasks. However, the reasons for their enhanced performance are largely unexamined. In this work, we examine three commonly used pooling techniques (mean-pooling,... | Pratyush Maini, Keshav Kolluru, Danish Pruthi, Mausam |  |
| 448 |  |  [Structural and Functional Decomposition for Personality Image Captioning in a Communication Game](https://doi.org/10.18653/v1/2020.findings-emnlp.411) |  | 0 | Personality image captioning (PIC) aims to describe an image with a natural language caption given a personality trait. In this work, we introduce a novel formulation for PIC based on a communication game between a speaker and a listener. The speaker attempts to generate natural language captions... | Minh Thu Nguyen, Duy Phung, Minh Hoai, Thien Huu Nguyen |  |
| 449 |  |  [Long Document Ranking with Query-Directed Sparse Transformer](https://doi.org/10.18653/v1/2020.findings-emnlp.412) |  | 0 | The computing cost of transformer self-attention often necessitates breaking long documents to fit in pretrained models in document ranking tasks. In this paper, we design Query-Directed Sparse attention that induces IR-axiomatic structures in transformer self-attention. Our model, QDS-Transformer,... | JyunYu Jiang, Chenyan Xiong, ChiaJung Lee, Wei Wang |  |
| 450 |  |  [Visuo-Lingustic Question Answering (VLQA) Challenge](https://doi.org/10.18653/v1/2020.findings-emnlp.413) |  | 0 | Understanding images and text together is an important aspect of cognition and building advanced Artificial Intelligence (AI) systems. As a community, we have achieved good benchmarks over language and vision domains separately, however joint reasoning is still a challenge for state-of-the-art... | Shailaja Keyur Sampat, Yezhou Yang, Chitta Baral |  |
| 451 |  |  [Byte Pair Encoding is Suboptimal for Language Model Pretraining](https://doi.org/10.18653/v1/2020.findings-emnlp.414) |  | 0 | The success of pretrained transformer language models (LMs) in natural language processing has led to a wide range of pretraining setups. In particular, these models employ a variety of subword tokenization methods, most notably byte-pair encoding (BPE) (Sennrich et al., 2016; Gage, 1994), the... | Kaj Bostrom, Greg Durrett |  |
| 452 |  |  [Exploring BERT's sensitivity to lexical cues using tests from semantic priming](https://doi.org/10.18653/v1/2020.findings-emnlp.415) |  | 0 | Models trained to estimate word probabilities in context have become ubiquitous in natural language processing. How do these models use lexical cues in context to inform their word probabilities? To answer this question, we present a case study analyzing the pre-trained BERT model with tests... | Kanishka Misra, Allyson Ettinger, Julia Rayz |  |
| 453 |  |  [Multi-hop Question Generation with Graph Convolutional Network](https://doi.org/10.18653/v1/2020.findings-emnlp.416) |  | 0 | Multi-hop Question Generation (QG) aims to generate answer-related questions by aggregating and reasoning over multiple scattered evidence from different paragraphs. It is a more challenging yet under-explored task compared to conventional single-hop QG, where the questions are generated from the... | Dan Su, Yan Xu, Wenliang Dai, Ziwei Ji, Tiezheng Yu, Pascale Fung |  |
| 454 |  |  [MMFT-BERT: Multimodal Fusion Transformer with BERT Encodings for Visual Question Answering](https://doi.org/10.18653/v1/2020.findings-emnlp.417) |  | 0 | We present MMFT-BERT(MultiModal FusionTransformer with BERT encodings), to solve Visual Question Answering (VQA) ensuring individual and combined processing of multiple input modalities. Our approach benefits from processing multimodal data (video and text) adopting the BERT encodings individually... | Aisha Urooj Khan, Amir Mazaheri, Niels da Vitoria Lobo, Mubarak Shah |  |
| 455 |  |  [Thinking Like a Skeptic: Defeasible Inference in Natural Language](https://doi.org/10.18653/v1/2020.findings-emnlp.418) |  | 0 | Defeasible inference is a mode of reasoning in which an inference (X is a bird, therefore X flies) may be weakened or overturned in light of new evidence (X is a penguin). Though long recognized in classical AI and philosophy, defeasible inference has not been extensively studied in the context of... | Rachel Rudinger, Vered Shwartz, Jena D. Hwang, Chandra Bhagavatula, Maxwell Forbes, Ronan Le Bras, Noah A. Smith, Yejin Choi |  |
| 456 |  |  [Guiding Attention for Self-Supervised Learning with Transformers](https://doi.org/10.18653/v1/2020.findings-emnlp.419) |  | 0 | In this paper, we propose a simple and effective technique to allow for efficient self-supervised learning with bi-directional Transformers. Our approach is motivated by recent studies demonstrating that self-attention patterns in trained models contain a majority of non-linguistic regularities. We... | Ameet Deshpande, Karthik Narasimhan |  |
| 457 |  |  [Language-Conditioned Feature Pyramids for Visual Selection Tasks](https://doi.org/10.18653/v1/2020.findings-emnlp.420) |  | 0 | Referring expression comprehension, which is the ability to locate language to an object in an image, plays an important role in creating common ground. Many models that fuse visual and linguistic features have been proposed. However, few models consider the fusion of linguistic features with... | Taichi Iki, Akiko Aizawa |  |
| 458 |  |  [Learning to Classify Human Needs of Events from Category Definitions with Prototypical Instantiation](https://doi.org/10.18653/v1/2020.findings-emnlp.421) |  | 0 | We study the problem of learning an event classifier from human needs category descriptions, which is challenging due to: (1) the use of highly abstract concepts in natural language descriptions, (2) the difficulty of choosing key concepts. To tackle these two challenges, we propose LeaPI, a... | Haibo Ding, Zhe Feng |  |
| 459 |  |  [Automatic Term Name Generation for Gene Ontology: Task and Dataset](https://doi.org/10.18653/v1/2020.findings-emnlp.422) |  | 0 | Terms contained in Gene Ontology (GO) have been widely used in biology and bio-medicine. Most previous research focuses on inferring new GO terms, while the term names that reflect the gene function are still named by the experts. To fill this gap, we propose a novel task, namely term name... | Yanjian Zhang, Qin Chen, Yiteng Zhang, Zhongyu Wei, Yixu Gao, Jiajie Peng, Zengfeng Huang, Weijian Sun, Xuanjing Huang |  |
| 460 |  |  [Compressing Transformer-Based Semantic Parsing Models using Compositional Code Embeddings](https://doi.org/10.18653/v1/2020.findings-emnlp.423) |  | 0 | The current state-of-the-art task-oriented semantic parsing models use BERT or RoBERTa as pretrained encoders; these models have huge memory footprints. This poses a challenge to their deployment for voice assistants such as Amazon Alexa and Google Assistant on edge devices with limited memory... | Prafull Prakash, Saurabh Kumar Shashidhar, Wenlong Zhao, Subendhu Rongali, Haidar Khan, Michael Kayser |  |
| 461 |  |  [BERT-QE: Contextualized Query Expansion for Document Re-ranking](https://doi.org/10.18653/v1/2020.findings-emnlp.424) |  | 0 | Query expansion aims to mitigate the mismatch between the language used in a query and in a document. However, query expansion methods can suffer from introducing non-relevant information when expanding the query. To bridge this gap, inspired by recent advances in applying contextualized models like... | Zhi Zheng, Kai Hui, Ben He, Xianpei Han, Le Sun, Andrew Yates |  |
| 462 |  |  [ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations](https://doi.org/10.18653/v1/2020.findings-emnlp.425) |  | 0 | The pre-training of text encoders normally processes text as a sequence of tokens corresponding to small text units, such as word pieces in English and characters in Chinese. It omits information carried by larger text granularity, and thus the encoders cannot easily adapt to certain combinations of... | Shizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang, Yonggang Wang |  |
| 463 |  |  [Finding Friends and Flipping Frenemies: Automatic Paraphrase Dataset Augmentation Using Graph Theory](https://doi.org/10.18653/v1/2020.findings-emnlp.426) |  | 0 | Most NLP datasets are manually labeled, so suffer from inconsistent labeling or limited size. We propose methods for automatically improving datasets by viewing them as graphs with expected semantic properties. We construct a paraphrase graph from the provided sentence pair labels, and create an... | Hannah Chen, Yangfeng Ji, David E. Evans |  |
| 464 |  |  [Probabilistic Case-based Reasoning in Knowledge Bases](https://doi.org/10.18653/v1/2020.findings-emnlp.427) |  | 0 | A case-based reasoning (CBR) system solves a new problem by retrieving ‘cases’ that are similar to the given problem. If such a system can achieve high accuracy, it is appealing owing to its simplicity, interpretability, and scalability. In this paper, we demonstrate that such a system is achievable... | Rajarshi Das, Ameya Godbole, Nicholas Monath, Manzil Zaheer, Andrew McCallum |  |
| 465 |  |  [TLDR: Extreme Summarization of Scientific Documents](https://doi.org/10.18653/v1/2020.findings-emnlp.428) |  | 0 | We introduce TLDR generation, a new form of extreme summarization, for scientific papers. TLDR generation involves high source compression and requires expert background knowledge and understanding of complex domain-specific language. To facilitate study on this task, we introduce SCITLDR, a new... | Isabel Cachola, Kyle Lo, Arman Cohan, Daniel S. Weld |  |
| 466 |  |  [Tri-Train: Automatic Pre-Fine Tuning between Pre-Training and Fine-Tuning for SciNER](https://doi.org/10.18653/v1/2020.findings-emnlp.429) |  | 0 | The training process of scientific NER models is commonly performed in two steps: i) Pre-training a language model by self-supervised tasks on huge data and ii) fine-tune training with small labelled data. The success of the strategy depends on the relevance between the data domains and between the... | Qingkai Zeng, Wenhao Yu, Mengxia Yu, Tianwen Jiang, Tim Weninger, Meng Jiang |  |
| 467 |  |  [Hierarchical Region Learning for Nested Named Entity Recognition](https://doi.org/10.18653/v1/2020.findings-emnlp.430) |  | 0 | Named Entity Recognition (NER) is deeply explored and widely used in various tasks. Usually, some entity mentions are nested in other entities, which leads to the nested NER problem. Leading region based models face both the efficiency and effectiveness challenge due to the high subsequence... | Xinwei Long, Shuzi Niu, Yucheng Li |  |
| 468 |  |  [Understanding User Resistance Strategies in Persuasive Conversations](https://doi.org/10.18653/v1/2020.findings-emnlp.431) |  | 0 | Persuasive dialog systems have various usages, such as donation persuasion and physical exercise persuasion. Previous persuasive dialog systems research mostly focused on analyzing the persuader’s strategies and paid little attention to the persuadee (user). However, understanding and addressing... | Youzhi Tian, Weiyan Shi, Chen Li, Zhou Yu |  |
| 469 |  |  [On the Sub-Layer Functionalities of Transformer Decoder](https://doi.org/10.18653/v1/2020.findings-emnlp.432) |  | 0 | There have been significant efforts to interpret the encoder of Transformer-based encoder-decoder architectures for neural machine translation (NMT); meanwhile, the decoder remains largely unexamined despite its critical role. During translation, the decoder must predict output tokens by considering... | Yilin Yang, Longyue Wang, Shuming Shi, Prasad Tadepalli, Stefan Lee, Zhaopeng Tu |  |
| 470 |  |  [Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation](https://doi.org/10.18653/v1/2020.findings-emnlp.433) |  | 0 | The deployment of widely used Transformer architecture is challenging because of heavy computation load and memory overhead during inference, especially when the target device is limited in computational resources such as mobile or edge devices. Quantization is an effective technique to address such... | Insoo Chung, Byeongwook Kim, Yoonjung Choi, Se Jung Kwon, Yongkweon Jeon, Baeseong Park, Sangha Kim, Dongsoo Lee |  |
| 471 |  |  [Robust Backed-off Estimation of Out-of-Vocabulary Embeddings](https://doi.org/10.18653/v1/2020.findings-emnlp.434) |  | 0 | Out-of-vocabulary (oov) words cause serious troubles in solving natural language tasks with a neural network. Existing approaches to this problem resort to using subwords, which are shorter and more ambiguous units than words, in order to represent oov words with a bag of subwords. In this study,... | Nobukazu Fukuda, Naoki Yoshinaga, Masaru Kitsuregawa |  |
| 472 |  |  [Exploiting Unsupervised Data for Emotion Recognition in Conversations](https://doi.org/10.18653/v1/2020.findings-emnlp.435) |  | 0 | Emotion Recognition in Conversations (ERC) aims to predict the emotional state of speakers in conversations, which is essentially a text classification task. Unlike the sentence-level text classification problem, the available supervised data for the ERC task is limited, which potentially prevents... | Wenxiang Jiao, Michael R. Lyu, Irwin King |  |
| 473 |  |  [Tensorized Embedding Layers](https://doi.org/10.18653/v1/2020.findings-emnlp.436) |  | 0 | The embedding layers transforming input words into real vectors are the key components of deep neural networks used in natural language processing. However, when the vocabulary is large, the corresponding weight matrices can be enormous, which precludes their deployment in a limited resource... | Oleksii Hrinchuk, Valentin Khrulkov, Leyla Mirvakhabova, Elena Orlova, Ivan V. Oseledets |  |
| 474 |  |  [Speaker or Listener? The Role of a Dialogue Agent](https://doi.org/10.18653/v1/2020.findings-emnlp.437) |  | 0 | For decades, chitchat bots are designed as a listener to passively answer what people ask. This passive and relatively simple dialogue mechanism gains less attention from humans and consumes the interests of human beings rapidly. Therefore some recent researches attempt to endow the bots with... | Yafei Liu, Hongjin Qian, Hengpeng Xu, Jinmao Wei |  |
| 475 |  |  [Bridging Textual and Tabular Data for Cross-Domain Text-to-SQL Semantic Parsing](https://doi.org/10.18653/v1/2020.findings-emnlp.438) |  | 0 | We present BRIDGE, a powerful sequential architecture for modeling dependencies between natural language questions and relational databases in cross-DB semantic parsing. BRIDGE represents the question and DB schema in a tagged sequence where a subset of the fields are augmented with cell values... | Xi Victoria Lin, Richard Socher, Caiming Xiong |  |
| 476 |  |  [Do Language Embeddings capture Scales?](https://doi.org/10.18653/v1/2020.findings-emnlp.439) |  | 0 | Pretrained Language Models (LMs) have been shown to possess significant linguistic, common sense and factual knowledge. One form of knowledge that has not been studied yet in this context is information about the scalar magnitudes of objects. We show that pretrained language models capture a... | Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, Dan Roth |  |
| 477 |  |  [Paraphrasing vs Coreferring: Two Sides of the Same Coin](https://doi.org/10.18653/v1/2020.findings-emnlp.440) |  | 0 | We study the potential synergy between two different NLP tasks, both confronting predicate lexical variability: identifying predicate paraphrases, and event coreference resolution. First, we used annotations from an event coreference dataset as distant supervision to re-score heuristically-extracted... | Yehudit Meged, Avi Caciularu, Vered Shwartz, Ido Dagan |  |
| 478 |  |  [Active Sentence Learning by Adversarial Uncertainty Sampling in Discrete Space](https://doi.org/10.18653/v1/2020.findings-emnlp.441) |  | 0 | Active learning for sentence understanding aims at discovering informative unlabeled data for annotation and therefore reducing the demand for labeled data. We argue that the typical uncertainty sampling method for active learning is time-consuming and can hardly work in real-time, which may lead to... | Dongyu Ru, Jiangtao Feng, Lin Qiu, Hao Zhou, Mingxuan Wang, Weinan Zhang, Yong Yu, Lei Li |  |
| 479 |  |  [Coming to Terms: Automatic Formation of Neologisms in Hebrew](https://doi.org/10.18653/v1/2020.findings-emnlp.442) |  | 0 | Spoken languages are ever-changing, with new words entering them all the time. However, coming up with new words (neologisms) today relies exclusively on human creativity. In this paper we propose a system to automatically suggest neologisms. We focus on the Hebrew language as a test case due to the... | Moran Mizrahi, Stav Yardeni Seelig, Dafna Shahaf |  |
| 480 |  |  [Dual Inference for Improving Language Understanding and Generation](https://doi.org/10.18653/v1/2020.findings-emnlp.443) |  | 0 | Natural language understanding (NLU) and Natural language generation (NLG) tasks hold a strong dual relationship, where NLU aims at predicting semantic labels based on natural language utterances and NLG does the opposite. The prior work mainly focused on exploiting the duality in model training in... | ShangYu Su, YungSung Chuang, YunNung Chen |  |
| 481 |  |  [Joint Intent Detection and Entity Linking on Spatial Domain Queries](https://doi.org/10.18653/v1/2020.findings-emnlp.444) |  | 0 | Continuous efforts have been devoted to language understanding (LU) for conversational queries with the fast and wide-spread popularity of voice assistants. In this paper, we first study the LU problem in the spatial domain, which is a critical problem for providing location-based services by voice... | Lei Zhang, Runze Wang, Jingbo Zhou, Jingsong Yu, Zhenhua Ling, Hui Xiong |  |
| 482 |  |  [iNLPSuite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages](https://doi.org/10.18653/v1/2020.findings-emnlp.445) |  | 0 | In this paper, we introduce NLP resources for 11 major Indian languages from two major language families. These resources include: (a) large-scale sentence-level monolingual corpora, (b) pre-trained word embeddings, (c) pre-trained language models, and (d) multiple NLU evaluation datasets (IndicGLUE... | Divyanshu Kakwani, Anoop Kunchukuttan, Satish Golla, Gokul N. C., Avik Bhattacharyya, Mitesh M. Khapra, Pratyush Kumar |  |
| 483 |  |  [Weakly-Supervised Modeling of Contextualized Event Embedding for Discourse Relations](https://doi.org/10.18653/v1/2020.findings-emnlp.446) |  | 0 | Representing, and reasoning over, long narratives requires models that can deal with complex event structures connected through multiple relationship types. This paper suggests to represent this type of information as a narrative graph and learn contextualized event representations over it using a... | ITa Lee, Maria Leonor Pacheco, Dan Goldwasser |  |
| 484 |  |  [Enhancing Generalization in Natural Language Inference by Syntax](https://doi.org/10.18653/v1/2020.findings-emnlp.447) |  | 0 | Pre-trained language models such as BERT have achieved the state-of-the-art performance on natural language inference (NLI). However, it has been shown that such models can be tricked by variations of surface patterns such as syntax. We investigate the use of dependency trees to enhance the... | Qi He, Han Wang, Yue Zhang |  |
| 485 |  |  [Detecting Attackable Sentences in Arguments](https://doi.org/10.18653/v1/2020.emnlp-main.1) |  | 0 | Finding attackable sentences in an argument is the first step toward successful refutation in argumentation. We present a first large-scale analysis of sentence attackability in online arguments. We analyze driving reasons for attacks in argumentation and identify relevant characteristics of... | Yohan Jo, Seojin Bang, Emaad A. Manzoor, Eduard H. Hovy, Chris Reed |  |
| 486 |  |  [Extracting Implicitly Asserted Propositions in Argumentation](https://doi.org/10.18653/v1/2020.emnlp-main.2) |  | 0 | Argumentation accommodates various rhetorical devices, such as questions, reported speech, and imperatives. These rhetorical tools usually assert argumentatively relevant propositions rather implicitly, so understanding their true meaning is key to understanding certain arguments properly. However,... | Yohan Jo, Jacky Visser, Chris Reed, Eduard H. Hovy |  |
| 487 |  |  [Quantitative argument summarization and beyond: Cross-domain key point analysis](https://doi.org/10.18653/v1/2020.emnlp-main.3) |  | 0 | When summarizing a collection of views, arguments or opinions on some topic, it is often desirable not only to extract the most salient points, but also to quantify their prevalence. Work on multi-document summarization has traditionally focused on creating textual summaries, which lack this... | Roy BarHaim, Yoav Kantor, Lilach Eden, Roni Friedman, Dan Lahav, Noam Slonim |  |
| 488 |  |  [Unsupervised stance detection for arguments from consequences](https://doi.org/10.18653/v1/2020.emnlp-main.4) |  | 0 | Social media platforms have become an essential venue for online deliberation where users discuss arguments, debate, and form opinions. In this paper, we propose an unsupervised method to detect the stance of argumentative claims with respect to a topic. Most related work focuses on topic-specific... | Jonathan Kobbe, Ioana Hulpus, Heiner Stuckenschmidt |  |
| 489 |  |  [BLEU might be Guilty but References are not Innocent](https://doi.org/10.18653/v1/2020.emnlp-main.5) |  | 0 | The quality of automatic metrics for machine translation has been increasingly called into question, especially for high-quality systems. This paper demonstrates that, while choice of metric is important, the nature of the references is also critical. We study different methods to collect references... | Markus Freitag, David Grangier, Isaac Caswell |  |
| 490 |  |  [Statistical Power and Translationese in Machine Translation Evaluation](https://doi.org/10.18653/v1/2020.emnlp-main.6) |  | 0 | The term translationese has been used to describe features of translated text, and in this paper, we provide detailed analysis of potential adverse effects of translationese on machine translation evaluation. Our analysis shows differences in conclusions drawn from evaluations that include... | Yvette Graham, Barry Haddow, Philipp Koehn |  |
| 491 |  |  [Simulated multiple reference training improves low-resource machine translation](https://doi.org/10.18653/v1/2020.emnlp-main.7) |  | 0 | Many valid translations exist for a given sentence, yet machine translation (MT) is trained with a single reference translation, exacerbating data sparsity in low-resource settings. We introduce Simulated Multiple Reference Training (SMRT), a novel MT training method that approximates the full space... | Huda Khayrallah, Brian Thompson, Matt Post, Philipp Koehn |  |
| 492 |  |  [Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing](https://doi.org/10.18653/v1/2020.emnlp-main.8) |  | 0 | We frame the task of machine translation evaluation as one of scoring machine translation output with a sequence-to-sequence paraphraser, conditioned on a human reference. We propose training the paraphraser as a multilingual NMT system, treating paraphrasing as a zero-shot translation task (e.g.,... | Brian Thompson, Matt Post |  |
| 493 |  |  [PRover: Proof Generation for Interpretable Reasoning over Rules](https://doi.org/10.18653/v1/2020.emnlp-main.9) |  | 0 | Recent work by Clark et al. (2020) shows that transformers can act as “soft theorem provers” by answering questions over explicitly provided knowledge in natural language. In our work, we take a step closer to emulating formal theorem provers, by proposing PRover, an interpretable transformer-based... | Swarnadeep Saha, Sayan Ghosh, Shashank Srivastava, Mohit Bansal |  |
| 494 |  |  [Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering](https://doi.org/10.18653/v1/2020.emnlp-main.10) |  | 0 | Despite the rapid progress in multihop question-answering (QA), models still have trouble explaining why an answer is correct, with limited explanation training data available to learn from. To address this, we introduce three explanation datasets in which explanations formed from corpus facts are... | Harsh Jhamtani, Peter Clark |  |
| 495 |  |  [Self-Supervised Knowledge Triplet Learning for Zero-Shot Question Answering](https://doi.org/10.18653/v1/2020.emnlp-main.11) |  | 0 | The aim of all Question Answering (QA) systems is to generalize to unseen questions. Current supervised methods are reliant on expensive data annotation. Moreover, such annotations can introduce unintended annotator bias, making systems focus more on the bias than the actual task. This work proposes... | Pratyay Banerjee, Chitta Baral |  |
| 496 |  |  [More Bang for Your Buck: Natural Perturbation for Robust Question Answering](https://doi.org/10.18653/v1/2020.emnlp-main.12) |  | 0 | Deep learning models for linguistic tasks require large training datasets, which are expensive to create. As an alternative to the traditional approach of creating new instances by repeating the process of creating one instance, we propose doing so by first collecting a set of seed examples and then... | Daniel Khashabi, Tushar Khot, Ashish Sabharwal |  |
| 497 |  |  [A matter of framing: The impact of linguistic formalism on probing results](https://doi.org/10.18653/v1/2020.emnlp-main.13) |  | 0 | Deep pre-trained contextualized encoders like BERT demonstrate remarkable performance on a range of downstream tasks. A recent line of research in probing investigates the linguistic knowledge implicitly learned by these models during pre-training. While most work in probing operates on the task... | Ilia Kuznetsov, Iryna Gurevych |  |
| 498 |  |  [Information-Theoretic Probing with Minimum Description Length](https://doi.org/10.18653/v1/2020.emnlp-main.14) |  | 0 | To measure how well pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier trained to predict the property from the representations. Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect... | Elena Voita, Ivan Titov |  |
| 499 |  |  [Intrinsic Probing through Dimension Selection](https://doi.org/10.18653/v1/2020.emnlp-main.15) |  | 0 | Most modern NLP systems make use of pre-trained contextual representations that attain astonishingly high performance on a variety of tasks. Such high performance should not be possible unless some form of linguistic structure inheres in these representations, and a wealth of research has sprung up... | Lucas Torroba Hennigen, Adina Williams, Ryan Cotterell |  |
| 500 |  |  [Learning Which Features Matter: RoBERTa Acquires a Preference for Linguistic Generalizations (Eventually)](https://doi.org/10.18653/v1/2020.emnlp-main.16) |  | 0 | One reason pretraining on self-supervised linguistic tasks is effective is that it teaches models features that are helpful for language understanding. However, we want pretrained models to learn not only to represent linguistic features, but also to use those features preferentially during... | Alex Warstadt, Yian Zhang, Xiaocheng Li, Haokun Liu, Samuel R. Bowman |  |
| 501 |  |  [Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference](https://doi.org/10.18653/v1/2020.emnlp-main.17) |  | 0 | The neural attention mechanism plays an important role in many natural language processing applications. In particular, multi-head attention extends single-head attention by allowing a model to jointly attend information from different perspectives. However, without explicit constraining, multi-head... | Bang An, Jie Lyu, Zhenyi Wang, Chunyuan Li, Changwei Hu, Fei Tan, Ruiyi Zhang, Yifan Hu, Changyou Chen |  |
| 502 |  |  [KERMIT: Complementing Transformer Architectures with Encoders of Explicit Syntactic Interpretations](https://doi.org/10.18653/v1/2020.emnlp-main.18) |  | 0 | Syntactic parsers have dominated natural language understanding for decades. Yet, their syntactic interpretations are losing centrality in downstream tasks due to the success of large-scale textual representation learners. In this paper, we propose KERMIT (Kernel-inspired Encoder with Recursive... | Fabio Massimo Zanzotto, Andrea Santilli, Leonardo Ranaldi, Dario Onorati, Pierfrancesco Tommasino, Francesca Fallucchi |  |
| 503 |  |  [ETC: Encoding Long and Structured Inputs in Transformers](https://doi.org/10.18653/v1/2020.emnlp-main.19) |  | 0 | Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, “Extended Transformer Construction” (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input... | Joshua Ainslie, Santiago Ontañón, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, Li Yang |  |
| 504 |  |  [Pre-Training Transformers as Energy-Based Cloze Models](https://doi.org/10.18653/v1/2020.emnlp-main.20) |  | 0 | We introduce Electric, an energy-based cloze model for representation learning over text. Like BERT, it is a conditional generative model of tokens given their contexts. However, Electric does not use masking or output a full distribution over tokens that could occur in a context. Instead, it... | Kevin Clark, MinhThang Luong, Quoc V. Le, Christopher D. Manning |  |
| 505 |  |  [Calibration of Pre-trained Transformers](https://doi.org/10.18653/v1/2020.emnlp-main.21) |  | 0 | Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models’ posterior probabilities provide an accurate empirical measure of how likely the model... | Shrey Desai, Greg Durrett |  |
| 506 |  |  [Near-imperceptible Neural Linguistic Steganography via Self-Adjusting Arithmetic Coding](https://doi.org/10.18653/v1/2020.emnlp-main.22) |  | 0 | Linguistic steganography studies how to hide secret messages in natural language cover texts. Traditional methods aim to transform a secret message into an innocent text via lexical substitution or syntactical modification. Recently, advances in neural language models (LMs) enable us to directly... | Jiaming Shen, Heng Ji, Jiawei Han |  |
| 507 |  |  [Multi-Dimensional Gender Bias Classification](https://doi.org/10.18653/v1/2020.emnlp-main.23) |  | 0 | Machine learning models are trained to find patterns in data. NLP models can inadvertently learn socially undesirable patterns when training on gender biased text. In this work, we propose a novel, general framework that decomposes gender bias in text along several pragmatic and semantic dimensions:... | Emily Dinan, Angela Fan, Ledell Wu, Jason Weston, Douwe Kiela, Adina Williams |  |
| 508 |  |  [FIND: Human-in-the-Loop Debugging Deep Text Classifiers](https://doi.org/10.18653/v1/2020.emnlp-main.24) |  | 0 | Since obtaining a perfect training dataset (i.e., a dataset which is considerably large, unbiased, and well-representative of unseen cases) is hardly possible, many real-world text classifiers are trained on the available, yet imperfect, datasets. These classifiers are thus likely to have... | Piyawat Lertvittayakumjorn, Lucia Specia, Francesca Toni |  |
| 509 |  |  [Conversational Document Prediction to Assist Customer Care Agents](https://doi.org/10.18653/v1/2020.emnlp-main.25) |  | 0 | A frequent pattern in customer care conversations is the agents responding with appropriate webpage URLs that address users’ needs. We study the task of predicting the documents that customer care agents can use to facilitate users’ needs. We also introduce a new public dataset which supports the... | Jatin Ganhotra, Haggai Roitman, Doron Cohen, Nathaniel Mills, R. Chulaka Gunasekara, Yosi Mass, Sachindra Joshi, Luis A. Lastras, David Konopnicki |  |
| 510 |  |  [Incremental Processing in the Age of Non-Incremental Encoders: An Empirical Assessment of Bidirectional Models for Incremental NLU](https://doi.org/10.18653/v1/2020.emnlp-main.26) |  | 0 | While humans process language incrementally, the best language encoders currently used in NLP do not. Both bidirectional LSTMs and Transformers assume that the sequence that is to be encoded is available in full, to be processed either forwards and backwards (BiLSTMs) or as a whole (Transformers).... | Brielen Madureira, David Schlangen |  |
| 511 |  |  [Augmented Natural Language for Generative Sequence Labeling](https://doi.org/10.18653/v1/2020.emnlp-main.27) |  | 0 | We propose a generative framework for joint sequence labeling and sentence-level classification. Our model performs multiple sequence labeling tasks at once using a single, shared natural language output space. Unlike prior discriminative methods, our model naturally incorporates label semantics and... | Ben Athiwaratkun, Cícero Nogueira dos Santos, Jason Krone, Bing Xiang |  |
| 512 |  |  [Dialogue Response Ranking Training with Large-Scale Human Feedback Data](https://doi.org/10.18653/v1/2020.emnlp-main.28) |  | 0 | Existing open-domain dialog models are generally trained to minimize the perplexity of target human responses. However, some human replies are more engaging than others, spawning more followup interactions. Current conversational models are increasingly capable of producing turns that are... | Xiang Gao, Yizhe Zhang, Michel Galley, Chris Brockett, Bill Dolan |  |
| 513 |  |  [Semantic Evaluation for Text-to-SQL with Distilled Test Suites](https://doi.org/10.18653/v1/2020.emnlp-main.29) |  | 0 | We propose test suite accuracy to approximate semantic accuracy for Text-to-SQL models. Our method distills a small test suite of databases that achieves high code coverage for the gold query from a large number of randomly generated databases. At evaluation time, it computes the denotation accuracy... | Ruiqi Zhong, Tao Yu, Dan Klein |  |
| 514 |  |  [Cross-Thought for Sentence Encoder Pre-training](https://doi.org/10.18653/v1/2020.emnlp-main.30) |  | 0 | In this paper, we propose Cross-Thought, a novel approach to pre-training sequence encoder, which is instrumental in building reusable sequence embeddings for large-scale NLP tasks such as question answering. Instead of using the original signals of full sentences, we train a Transformer-based... | Shuohang Wang, Yuwei Fang, Siqi Sun, Zhe Gan, Yu Cheng, Jingjing Liu, Jing Jiang |  |
| 515 |  |  [AutoQA: From Databases To QA Semantic Parsers With Only Synthetic Training Data](https://doi.org/10.18653/v1/2020.emnlp-main.31) |  | 0 | We propose AutoQA, a methodology and toolkit to generate semantic parsers that answer questions on databases, with no manual effort. Given a database schema and its data, AutoQA automatically generates a large set of high-quality questions for training that covers different database operations. It... | Silei Xu, Sina J. Semnani, Giovanni Campagna, Monica S. Lam |  |
| 516 |  |  [A Spectral Method for Unsupervised Multi-Document Summarization](https://doi.org/10.18653/v1/2020.emnlp-main.32) |  | 0 | Multi-document summarization (MDS) aims at producing a good-quality summary for several related documents. In this paper, we propose a spectral-based hypothesis, which states that the goodness of summary candidate is closely linked to its so-called spectral impact. Here spectral impact considers the... | Kexiang Wang, Baobao Chang, Zhifang Sui |  |
| 517 |  |  [What Have We Achieved on Text Summarization?](https://doi.org/10.18653/v1/2020.emnlp-main.33) |  | 0 | Deep learning has led to significant improvement in text summarization with various methods investigated and improved ROUGE scores reported over the years. However, gaps still exist between summaries produced by automatic summarizers and human professionals. Aiming to gain more understanding of... | Dandan Huang, Leyang Cui, Sen Yang, Guangsheng Bao, Kun Wang, Jun Xie, Yue Zhang |  |
| 518 |  |  [Q-learning with Language Model for Edit-based Unsupervised Summarization](https://doi.org/10.18653/v1/2020.emnlp-main.34) |  | 0 | Unsupervised methods are promising for abstractive textsummarization in that the parallel corpora is not required. However, their performance is still far from being satisfied, therefore research on promising solutions is on-going. In this paper, we propose a new approach based on Q-learning with an... | Ryosuke Kohita, Akifumi Wachi, Yang Zhao, Ryuki Tachibana |  |
| 519 |  |  [Friendly Topic Assistant for Transformer Based Abstractive Summarization](https://doi.org/10.18653/v1/2020.emnlp-main.35) |  | 0 | Abstractive document summarization is a comprehensive task including document understanding and summary generation, in which area Transformer-based models have achieved the state-of-the-art performance. Compared with Transformers, topic models are better at learning explicit document semantics, and... | Zhengjue Wang, Zhibin Duan, Hao Zhang, Chaojie Wang, Long Tian, Bo Chen, Mingyuan Zhou |  |
| 520 |  |  [Contrastive Distillation on Intermediate Representations for Language Model Compression](https://doi.org/10.18653/v1/2020.emnlp-main.36) |  | 0 | Existing language model compression methods mostly use a simple L_2 loss to distill knowledge in the intermediate representations of a large BERT model to a smaller one. Although widely used, this objective by design assumes that all the dimensions of hidden representations are independent, failing... | Siqi Sun, Zhe Gan, Yuwei Fang, Yu Cheng, Shuohang Wang, Jingjing Liu |  |
| 521 |  |  [TernaryBERT: Distillation-aware Ultra-low Bit BERT](https://doi.org/10.18653/v1/2020.emnlp-main.37) |  | 0 | Transformer-based pre-training models like BERT have achieved remarkable performance in many natural language processing tasks. However, these models are both computation and memory expensive, hindering their deployment to resource-constrained devices. In this work, we propose TernaryBERT, which... | Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, Qun Liu |  |
| 522 |  |  [Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks](https://doi.org/10.18653/v1/2020.emnlp-main.38) |  | 0 | Self-supervised pre-training of transformer models has revolutionized NLP applications. Such pre-training with language modeling objectives provides a useful initial point for parameters that generalize well to new tasks with fine-tuning. However, fine-tuning is still data inefficient — when there... | Trapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai, Andrew McCallum |  |
| 523 |  |  [Efficient Meta Lifelong-Learning with Limited Memory](https://doi.org/10.18653/v1/2020.emnlp-main.39) |  | 0 | Current natural language processing models work well on a single task, yet they often fail to continuously learn new tasks without forgetting previous ones as they are re-trained throughout their lifetime, a challenge known as lifelong learning. State-of-the-art lifelong language learning methods... | Zirui Wang, Sanket Vaibhav Mehta, Barnabás Póczos, Jaime G. Carbonell |  |
| 524 |  |  [Don't Use English Dev: On the Zero-Shot Cross-Lingual Evaluation of Contextual Embeddings](https://doi.org/10.18653/v1/2020.emnlp-main.40) |  | 0 | Multilingual contextual embeddings have demonstrated state-of-the-art performance in zero-shot cross-lingual transfer learning, where multilingual BERT is fine-tuned on one source language and evaluated on a different target language. However, published results for mBERT zero-shot accuracy vary as... | Phillip Keung, Yichao Lu, Julian Salazar, Vikas Bhardwaj |  |
| 525 |  |  [A Supervised Word Alignment Method based on Cross-Language Span Prediction using Multilingual BERT](https://doi.org/10.18653/v1/2020.emnlp-main.41) |  | 0 | We present a novel supervised word alignment method based on cross-language span prediction. We first formalize a word alignment problem as a collection of independent predictions from a token in the source sentence to a span in the target sentence. Since this step is equivalent to a SQuAD v2.0... | Masaaki Nagata, Katsuki Chousa, Masaaki Nishino |  |
| 526 |  |  [Accurate Word Alignment Induction from Neural Machine Translation](https://doi.org/10.18653/v1/2020.emnlp-main.42) |  | 0 | Despite its original goal to jointly learn to align and translate, prior researches suggest that Transformer captures poor word alignments through its attention mechanism. In this paper, we show that attention weights do capture accurate word alignments and propose two novel word alignment induction... | Yun Chen, Yang Liu, Guanhua Chen, Xin Jiang, Qun Liu |  |
| 527 |  |  [ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization](https://doi.org/10.18653/v1/2020.emnlp-main.43) |  | 0 | Cherokee is a highly endangered Native American language spoken by the Cherokee people. The Cherokee culture is deeply embedded in its language. However, there are approximately only 2,000 fluent first language Cherokee speakers remaining in the world and the number is declining every year. To help... | Shiyue Zhang, Benjamin Frey, Mohit Bansal |  |
| 528 |  |  [Unsupervised Discovery of Implicit Gender Bias](https://doi.org/10.18653/v1/2020.emnlp-main.44) |  | 0 | Despite their prevalence in society, social biases are difficult to identify, primarily because human judgements in this domain can be unreliable. We take an unsupervised approach to identifying gender bias against women at a comment level and present a model that can surface text likely to contain... | Anjalie Field, Yulia Tsvetkov |  |
| 529 |  |  [Condolence and Empathy in Online Communities](https://doi.org/10.18653/v1/2020.emnlp-main.45) |  | 0 | Offering condolence is a natural reaction to hearing someone’s distress. Individuals frequently express distress in social media, where some communities can provide support. However, not all condolence is equal—trite responses offer little actual support despite their good intentions. Here, we... | Naitian Zhou, David Jurgens |  |
| 530 |  |  [An Embedding Model for Estimating Legislative Preferences from the Frequency and Sentiment of Tweets](https://doi.org/10.18653/v1/2020.emnlp-main.46) |  | 0 | Legislator preferences are typically represented as measures of general ideology estimated from roll call votes on legislation, potentially masking important nuances in legislators’ political attitudes. In this paper we introduce a method of measuring more specific legislator attitudes using an... | Gregory Spell, Brian Guay, Sunshine Hillygus, Lawrence Carin |  |
| 531 |  |  [Measuring Information Propagation in Literary Social Networks](https://doi.org/10.18653/v1/2020.emnlp-main.47) |  | 0 | We present the task of modeling information propagation in literature, in which we seek to identify pieces of information passing from character A to character B to character C, only given a description of their activity in text. We describe a new pipeline for measuring information propagation in... | Matthew Sims, David Bamman |  |
| 532 |  |  [Social Chemistry 101: Learning to Reason about Social and Moral Norms](https://doi.org/10.18653/v1/2020.emnlp-main.48) |  | 0 | Social norms—the unspoken commonsense rules about acceptable social behavior—are crucial in understanding the underlying causes and intents of people’s actions in narratives. For example, underlying an action such as “wanting to call cops on my neighbor” are social norms that inform our conduct,... | Maxwell Forbes, Jena D. Hwang, Vered Shwartz, Maarten Sap, Yejin Choi |  |
| 533 |  |  [Event Extraction by Answering (Almost) Natural Questions](https://doi.org/10.18653/v1/2020.emnlp-main.49) |  | 0 | The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid... | Xinya Du, Claire Cardie |  |
| 534 |  |  [Connecting the Dots: Event Graph Schema Induction with Path Language Modeling](https://doi.org/10.18653/v1/2020.emnlp-main.50) |  | 0 | Event schemas can guide our understanding and ability to make predictions with respect to what might happen next. We propose a new Event Graph Schema, where two event types are connected through multiple paths involving entities that fill important roles in a coherent story. We then introduce Path... | Manling Li, Qi Zeng, Ying Lin, Kyunghyun Cho, Heng Ji, Jonathan May, Nathanael Chambers, Clare R. Voss |  |
| 535 |  |  [Joint Constrained Learning for Event-Event Relation Extraction](https://doi.org/10.18653/v1/2020.emnlp-main.51) |  | 0 | Understanding natural language involves recognizing how multiple event mentions structurally and temporally interact with each other. In this process, one can induce event complexes that organize multi-granular events with temporal order and membership relations interweaving among them. Due to the... | Haoyu Wang, Muhao Chen, Hongming Zhang, Dan Roth |  |
| 536 |  |  [Incremental Event Detection via Knowledge Consolidation Networks](https://doi.org/10.18653/v1/2020.emnlp-main.52) |  | 0 | Conventional approaches to event detection usually require a fixed set of pre-defined event types. Such a requirement is often challenged in real-world applications, as new events continually occur. Due to huge computation cost and storage budge, it is infeasible to store all previous data and... | Pengfei Cao, Yubo Chen, Jun Zhao, Taifeng Wang |  |
| 537 |  |  [Semi-supervised New Event Type Induction and Event Detection](https://doi.org/10.18653/v1/2020.emnlp-main.53) |  | 0 | Most previous event extraction studies assume a set of target event types and corresponding event annotations are given, which could be very expensive. In this paper, we work on a new task of semi-supervised event type induction, aiming to automatically discover a set of unseen types from a given... | Lifu Huang, Heng Ji |  |
| 538 |  |  [Language Generation with Multi-Hop Reasoning on Commonsense Knowledge Graph](https://doi.org/10.18653/v1/2020.emnlp-main.54) |  | 0 | Despite the success of generative pre-trained language models on a series of text generation tasks, they still suffer in cases where reasoning over underlying commonsense knowledge is required during generation. Existing approaches that integrate commonsense knowledge into generative pre-trained... | Haozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, Xiaoyan Zhu, Minlie Huang |  |
| 539 |  |  [Reformulating Unsupervised Style Transfer as Paraphrase Generation](https://doi.org/10.18653/v1/2020.emnlp-main.55) |  | 0 | Modern NLP defines the task of style transfer as modifying the style of a given sentence without appreciably changing its semantics, which implies that the outputs of style transfer systems should be paraphrases of their inputs. However, many existing systems purportedly designed for style transfer... | Kalpesh Krishna, John Wieting, Mohit Iyyer |  |
| 540 |  |  [De-Biased Court's View Generation with Causality](https://doi.org/10.18653/v1/2020.emnlp-main.56) |  | 0 | Court’s view generation is a novel but essential task for legal AI, aiming at improving the interpretability of judgment prediction results and enabling automatic legal document generation. While prior text-to-text natural language generation (NLG) approaches can be used to address this problem,... | Yiquan Wu, Kun Kuang, Yating Zhang, Xiaozhong Liu, Changlong Sun, Jun Xiao, Yueting Zhuang, Luo Si, Fei Wu |  |
| 541 |  |  [PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation](https://doi.org/10.18653/v1/2020.emnlp-main.57) |  | 0 | Pre-trained Transformers have enabled impressive breakthroughs in generating long and fluent text, yet their outputs are often “rambling” without coherently arranged content. In this work, we present a novel content-controlled text generation framework, PAIR, with planning and iterative refinement,... | Xinyu Hua, Lu Wang |  |
| 542 |  |  [Back to the Future: Unsupervised Backprop-based Decoding for Counterfactual and Abductive Commonsense Reasoning](https://doi.org/10.18653/v1/2020.emnlp-main.58) |  | 0 | Abductive and counterfactual reasoning, core abilities of everyday human cognition, require reasoning about what might have happened at time t, while conditioning on multiple contexts from the relative past and future. However, simultaneous incorporation of past and future contexts using generative... | Lianhui Qin, Vered Shwartz, Peter West, Chandra Bhagavatula, Jena D. Hwang, Ronan Le Bras, Antoine Bosselut, Yejin Choi |  |
| 543 |  |  [Where Are You? Localization from Embodied Dialog](https://doi.org/10.18653/v1/2020.emnlp-main.59) |  | 0 | We present WHERE ARE YOU? (WAY), a dataset of ~6k dialogs in which two humans – an Observer and a Locator – complete a cooperative localization task. The Observer is spawned at random in a 3D environment and can navigate from first-person views while answering questions from the Locator. The Locator... | Meera Hahn, Jacob Krantz, Dhruv Batra, Devi Parikh, James M. Rehg, Stefan Lee, Peter Anderson |  |
| 544 |  |  [Learning to Represent Image and Text with Denotation Graph](https://doi.org/10.18653/v1/2020.emnlp-main.60) |  | 0 | Learning to fuse vision and language information and representing them is an important research problem with many applications. Recent progresses have leveraged the ideas of pre-training (from language modeling) and attention layers in Transformers to learn representation from datasets containing... | Bowen Zhang, Hexiang Hu, Vihan Jain, Eugene Ie, Fei Sha |  |
| 545 |  |  [Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning](https://doi.org/10.18653/v1/2020.emnlp-main.61) |  | 0 | Captioning is a crucial and challenging task for video understanding. In videos that involve active agents such as humans, the agent’s actions can bring about myriad changes in the scene. Observable changes such as movements, manipulations, and transformations of the objects in the scene, are... | Zhiyuan Fang, Tejas Gokhale, Pratyay Banerjee, Chitta Baral, Yezhou Yang |  |
| 546 |  |  [Does my multimodal model learn cross-modal interactions? It's harder to tell than you might think!](https://doi.org/10.18653/v1/2020.emnlp-main.62) |  | 0 | Modeling expressive cross-modal interactions seems crucial in multimodal tasks, such as visual question answering. However, sometimes high-performing black-box algorithms turn out to be mostly exploiting unimodal signals in the data. We propose a new diagnostic tool, empirical multimodally-additive... | Jack Hessel, Lillian Lee |  |
| 547 |  |  [MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering](https://doi.org/10.18653/v1/2020.emnlp-main.63) |  | 0 | While progress has been made on the visual question answering leaderboards, models often utilize spurious correlations and priors in datasets under the i.i.d. setting. As such, evaluation on out-of-distribution (OOD) test samples has emerged as a proxy for generalization. In this paper, we present... | Tejas Gokhale, Pratyay Banerjee, Chitta Baral, Yezhou Yang |  |
| 548 |  |  [Mitigating Gender Bias for Neural Dialogue Generation with Adversarial Learning](https://doi.org/10.18653/v1/2020.emnlp-main.64) |  | 0 | Dialogue systems play an increasingly important role in various aspects of our daily life. It is evident from recent research that dialogue systems trained on human conversation data are biased. In particular, they can produce responses that reflect people’s gender prejudice. Many debiasing methods... | Haochen Liu, Wentao Wang, Yiqi Wang, Hui Liu, Zitao Liu, Jiliang Tang |  |
| 549 |  |  [Will I Sound Like Me? Improving Persona Consistency in Dialogues through Pragmatic Self-Consciousness](https://doi.org/10.18653/v1/2020.emnlp-main.65) |  | 0 | We explore the task of improving persona consistency of dialogue agents. Recent models tackling consistency often train with additional Natural Language Inference (NLI) labels or attach trained extra modules to the generative agent for maintaining consistency. However, such additional labels and... | Hyunwoo Kim, Byeongchang Kim, Gunhee Kim |  |
| 550 |  |  [TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue](https://doi.org/10.18653/v1/2020.emnlp-main.66) |  | 0 | The underlying difference of linguistic patterns between general text and task-oriented dialogue makes existing pre-trained language models less useful in practice. In this work, we unify nine human-human and multi-turn task-oriented dialogue datasets for language modeling. To better model dialogue... | ChienSheng Wu, Steven C. H. Hoi, Richard Socher, Caiming Xiong |  |
| 551 |  |  [RiSAWOZ: A Large-Scale Multi-Domain Wizard-of-Oz Dataset with Rich Semantic Annotations for Task-Oriented Dialogue Modeling](https://doi.org/10.18653/v1/2020.emnlp-main.67) |  | 0 | In order to alleviate the shortage of multi-domain data and to capture discourse phenomena for task-oriented dialogue modeling, we propose RiSAWOZ, a large-scale multi-domain Chinese Wizard-of-Oz dataset with Rich Semantic Annotations. RiSAWOZ contains 11.2K human-to-human (H2H) multi-turn... | Jun Quan, Shian Zhang, Qian Cao, Zizhong Li, Deyi Xiong |  |
| 552 |  |  [Filtering Noisy Dialogue Corpora by Connectivity and Content Relatedness](https://doi.org/10.18653/v1/2020.emnlp-main.68) |  | 0 | Large-scale dialogue datasets have recently become available for training neural dialogue agents. However, these datasets have been reported to contain a non-negligible number of unacceptable utterance pairs. In this paper, we propose a method for scoring the quality of utterance pairs in terms of... | Reina Akama, Sho Yokoi, Jun Suzuki, Kentaro Inui |  |
| 553 |  |  [Latent Geographical Factors for Analyzing the Evolution of Dialects in Contact](https://doi.org/10.18653/v1/2020.emnlp-main.69) |  | 0 | Analyzing the evolution of dialects remains a challenging problem because contact phenomena hinder the application of the standard tree model. Previous statistical approaches to this problem resort to admixture analysis, where each dialect is seen as a mixture of latent ancestral populations.... | Yugo Murawaki |  |
| 554 |  |  [Predicting Reference: What do Language Models Learn about Discourse Models?](https://doi.org/10.18653/v1/2020.emnlp-main.70) |  | 0 | Whereas there is a growing literature that probes neural language models to assess the degree to which they have latently acquired grammatical knowledge, little if any research has investigated their acquisition of discourse modeling ability. We address this question by drawing on a rich... | Shiva Upadhye, Leon Bergen, Andrew Kehler |  |
| 555 |  |  [Word class flexibility: A deep contextualized approach](https://doi.org/10.18653/v1/2020.emnlp-main.71) |  | 0 | Word class flexibility refers to the phenomenon whereby a single word form is used across different grammatical categories. Extensive work in linguistic typology has sought to characterize word class flexibility across languages, but quantifying this phenomenon accurately and at scale has been... | Bai Li, Guillaume Thomas, Yang Xu, Frank Rudzicz |  |
| 556 |  |  [Shallow-to-Deep Training for Neural Machine Translation](https://doi.org/10.18653/v1/2020.emnlp-main.72) |  | 0 | Deep encoders have been proven to be effective in improving neural machine translation (NMT) systems, but training an extremely deep encoder is time consuming. Moreover, why deep models help NMT is an open question. In this paper, we investigate the behavior of a well-tuned deep Transformer system.... | Bei Li, Ziyang Wang, Hui Liu, Yufan Jiang, Quan Du, Tong Xiao, Huizhen Wang, Jingbo Zhu |  |
| 557 |  |  [Iterative Refinement in the Continuous Space for Non-Autoregressive Neural Machine Translation](https://doi.org/10.18653/v1/2020.emnlp-main.73) |  | 0 | We propose an efficient inference procedure for non-autoregressive machine translation that iteratively refines translation purely in the continuous space. Given a continuous latent variable model for machine translation (Shu et al., 2020), we train an inference network to approximate the gradient... | Jason Lee, Raphael Shu, Kyunghyun Cho |  |
| 558 |  |  [Why Skip If You Can Combine: A Simple Knowledge Distillation Technique for Intermediate Layers](https://doi.org/10.18653/v1/2020.emnlp-main.74) |  | 0 | With the growth of computing power neural machine translation (NMT) models also grow accordingly and become better. However, they also become harder to deploy on edge devices due to memory constraints. To cope with this problem, a common practice is to distill knowledge from a large and... | Yimeng Wu, Peyman Passban, Mehdi Rezagholizadeh, Qun Liu |  |
| 559 |  |  [Multi-task Learning for Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2020.emnlp-main.75) |  | 0 | While monolingual data has been shown to be useful in improving bilingual neural machine translation (NMT), effectively and efficiently leveraging monolingual data for Multilingual NMT (MNMT) systems is a less explored area. In this work, we propose a multi-task learning (MTL) framework that jointly... | Yiren Wang, ChengXiang Zhai, Hany Hassan |  |
| 560 |  |  [Token-level Adaptive Training for Neural Machine Translation](https://doi.org/10.18653/v1/2020.emnlp-main.76) |  | 0 | There exists a token imbalance phenomenon in natural language as different tokens appear with different frequencies, which leads to different learning difficulties for tokens in Neural Machine Translation (NMT). The vanilla NMT model usually adopts trivial equal-weighted objectives for target tokens... | Shuhao Gu, Jinchao Zhang, Fandong Meng, Yang Feng, Wanying Xie, Jie Zhou, Dong Yu |  |
| 561 |  |  [Multi-Unit Transformers for Neural Machine Translation](https://doi.org/10.18653/v1/2020.emnlp-main.77) |  | 0 | Transformer models achieve remarkable success in Neural Machine Translation. Many efforts have been devoted to deepening the Transformer by stacking several units (i.e., a combination of Multihead Attentions and FFN) in a cascade, while the investigation over multiple parallel units draws little... | Jianhao Yan, Fandong Meng, Jie Zhou |  |
| 562 |  |  [On the Sparsity of Neural Machine Translation Models](https://doi.org/10.18653/v1/2020.emnlp-main.78) |  | 0 | Modern neural machine translation (NMT) models employ a large number of parameters, which leads to serious over-parameterization and typically causes the underutilization of computational resources. In response to this problem, we empirically investigate whether the redundant parameters can be... | Yong Wang, Longyue Wang, Victor O. K. Li, Zhaopeng Tu |  |
| 563 |  |  [Incorporating a Local Translation Mechanism into Non-autoregressive Translation](https://doi.org/10.18653/v1/2020.emnlp-main.79) |  | 0 | In this work, we introduce a novel local autoregressive translation (LAT) mechanism into non-autoregressive translation (NAT) models so as to capture local dependencies among target outputs. Specifically, for each target decoding position, instead of only one token, we predict a short sequence of... | Xiang Kong, Zhisong Zhang, Eduard H. Hovy |  |
| 564 |  |  [Self-Paced Learning for Neural Machine Translation](https://doi.org/10.18653/v1/2020.emnlp-main.80) |  | 0 | Recent studies have proven that the training of neural machine translation (NMT) can be facilitated by mimicking the learning process of humans. Nevertheless, achievements of such kind of curriculum learning rely on the quality of artificial schedule drawn up with the handcrafted features, e.g.... | Yu Wan, Baosong Yang, Derek F. Wong, Yikai Zhou, Lidia S. Chao, Haibo Zhang, Boxing Chen |  |
| 565 |  |  [Long-Short Term Masking Transformer: A Simple but Effective Baseline for Document-level Neural Machine Translation](https://doi.org/10.18653/v1/2020.emnlp-main.81) |  | 0 | Many document-level neural machine translation (NMT) systems have explored the utility of context-aware architecture, usually requiring an increasing number of parameters and computational complexity. However, few attention is paid to the baseline model. In this paper, we research extensively the... | Pei Zhang, Boxing Chen, Niyu Ge, Kai Fan |  |
| 566 |  |  [Generating Diverse Translation from Model Distribution with Dropout](https://doi.org/10.18653/v1/2020.emnlp-main.82) |  | 0 | Despite the improvement of translation quality, neural machine translation (NMT) often suffers from the lack of diversity in its generation. In this paper, we propose to generate diverse translations by deriving a large number of possible models with Bayesian modelling and sampling models from them... | Xuanfu Wu, Yang Feng, Chenze Shao |  |
| 567 |  |  [Non-Autoregressive Machine Translation with Latent Alignments](https://doi.org/10.18653/v1/2020.emnlp-main.83) |  | 0 | This paper presents two strong methods, CTC and Imputer, for non-autoregressive machine translation that model latent alignments with dynamic programming. We revisit CTC for machine translation and demonstrate that a simple CTC model can achieve state-of-the-art for single-step non-autoregressive... | Chitwan Saharia, William Chan, Saurabh Saxena, Mohammad Norouzi |  |
| 568 |  |  [Look at the First Sentence: Position Bias in Question Answering](https://doi.org/10.18653/v1/2020.emnlp-main.84) |  | 0 | Many extractive question answering models are trained to predict start and end positions of answers. The choice of predicting answers as positions is mainly due to its simplicity and effectiveness. In this study, we hypothesize that when the distribution of the answer positions is highly skewed in... | Miyoung Ko, Jinhyuk Lee, Hyunjae Kim, Gangwoo Kim, Jaewoo Kang |  |
| 569 |  |  [ProtoQA: A Question Answering Dataset for Prototypical Common-Sense Reasoning](https://doi.org/10.18653/v1/2020.emnlp-main.85) |  | 0 | Given questions regarding some prototypical situation — such as Name something that people usually do before they leave the house for work? — a human can easily answer them via acquired experiences. There can be multiple right answers for such questions, with some more common for a situation than... | Michael Boratko, Xiang Li, Tim O'Gorman, Rajarshi Das, Dan Le, Andrew McCallum |  |
| 570 |  |  [IIRC: A Dataset of Incomplete Information Reading Comprehension Questions](https://doi.org/10.18653/v1/2020.emnlp-main.86) |  | 0 | Humans often have to read multiple documents to address their information needs. However, most existing reading comprehension (RC) tasks only focus on questions for which the contexts provide all the information required to answer them, thus not evaluating a system’s performance at identifying a... | James Ferguson, Matt Gardner, Hannaneh Hajishirzi, Tushar Khot, Pradeep Dasigi |  |
| 571 |  |  [Unsupervised Adaptation of Question Answering Systems via Generative Self-training](https://doi.org/10.18653/v1/2020.emnlp-main.87) |  | 0 | BERT-era question answering systems have recently achieved impressive performance on several question-answering (QA) tasks. These systems are based on representations that have been pre-trained on self-supervised tasks such as word masking and sentence entailment, using massive amounts of data.... | Steven J. Rennie, Etienne Marcheret, Neil Mallinar, David Nahamoo, Vaibhava Goel |  |
| 572 |  |  [TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions](https://doi.org/10.18653/v1/2020.emnlp-main.88) |  | 0 | A critical part of reading is being able to understand the temporal relationships between events described in a passage of text, even when those relationships are not explicitly stated. However, current machine reading comprehension benchmarks have practically no questions that test temporal... | Qiang Ning, Hao Wu, Rujun Han, Nanyun Peng, Matt Gardner, Dan Roth |  |
| 573 |  |  [ToTTo: A Controlled Table-To-Text Generation Dataset](https://doi.org/10.18653/v1/2020.emnlp-main.89) |  | 0 | We present ToTTo, an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description. To obtain generated targets that are natural but also... | Ankur P. Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, Dipanjan Das |  |
| 574 |  |  [ENT-DESC: Entity Description Generation by Exploring Knowledge Graph](https://doi.org/10.18653/v1/2020.emnlp-main.90) |  | 0 | Previous works on knowledge-to-text generation take as input a few RDF triples or key-value pairs conveying the knowledge of some entities to generate a natural language description. Existing datasets, such as WIKIBIO, WebNLG, and E2E, basically have a good alignment between an input triple/pair set... | Liying Cheng, Dekun Wu, Lidong Bing, Yan Zhang, Zhanming Jie, Wei Lu, Luo Si |  |
| 575 |  |  [Small but Mighty: New Benchmarks for Split and Rephrase](https://doi.org/10.18653/v1/2020.emnlp-main.91) |  | 0 | Split and Rephrase is a text simplification task of rewriting a complex sentence into simpler ones. As a relatively new task, it is paramount to ensure the soundness of its evaluation benchmark and metric. We find that the widely used benchmark dataset universally contains easily exploitable... | Li Zhang, Huaiyu Zhu, Siddhartha Brahma, Yunyao Li |  |
| 576 |  |  [Online Back-Parsing for AMR-to-Text Generation](https://doi.org/10.18653/v1/2020.emnlp-main.92) |  | 0 | AMR-to-text generation aims to recover a text containing the same meaning as an input AMR graph. Current research develops increasingly powerful graph encoders to better represent AMR graphs, with decoders based on standard language modeling being used to generate outputs. We propose a decoder that... | Xuefeng Bai, Linfeng Song, Yue Zhang |  |
| 577 |  |  [Reading Between the Lines: Exploring Infilling in Visual Narratives](https://doi.org/10.18653/v1/2020.emnlp-main.93) |  | 0 | Generating long form narratives such as stories and procedures from multiple modalities has been a long standing dream for artificial intelligence. In this regard, there is often crucial subtext that is derived from the surrounding contexts. The general seq2seq training methods render the models... | Khyathi Raghavi Chandu, RuoPing Dong, Alan W. Black |  |
| 578 |  |  [Acrostic Poem Generation](https://doi.org/10.18653/v1/2020.emnlp-main.94) |  | 0 | We propose a new task in the area of computational creativity: acrostic poem generation in English. Acrostic poems are poems that contain a hidden message; typically, the first letter of each line spells out a word or short phrase. We define the task as a generation task with multiple constraints:... | Rajat Agarwal, Katharina Kann |  |
| 579 |  |  [Local Additivity Based Data Augmentation for Semi-supervised NER](https://doi.org/10.18653/v1/2020.emnlp-main.95) |  | 0 | Named Entity Recognition (NER) is one of the first stages in deep language understanding yet current NER models heavily rely on human-annotated data. In this work, to alleviate the dependence on labeled data, we propose a Local Additivity based Data Augmentation (LADA) method for semi-supervised... | Jiaao Chen, Zhenghui Wang, Ran Tian, Zichao Yang, Diyi Yang |  |
| 580 |  |  [Grounded Compositional Outputs for Adaptive Language Modeling](https://doi.org/10.18653/v1/2020.emnlp-main.96) |  | 0 | Language models have emerged as a central component across NLP, and a great deal of progress depends on the ability to cheaply adapt them (e.g., through finetuning) to new domains and tasks. A language model’s vocabulary—typically selected before training and permanently fixed later—affects its size... | Nikolaos Pappas, Phoebe Mulcaire, Noah A. Smith |  |
| 581 |  |  [SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness](https://doi.org/10.18653/v1/2020.emnlp-main.97) |  | 0 | Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to generate new examples that stay on the underlying... | Nathan Ng, Kyunghyun Cho, Marzyeh Ghassemi |  |
| 582 |  |  [SetConv: A New Approach for Learning from Imbalanced Data](https://doi.org/10.18653/v1/2020.emnlp-main.98) |  | 0 | For many real-world classification problems, e.g., sentiment classification, most existing machine learning methods are biased towards the majority class when the Imbalance Ratio (IR) is high. To address this problem, we propose a set convolution (SetConv) operation and an episodic training strategy... | Yang Gao, YiFan Li, Yu Lin, Charu C. Aggarwal, Latifur Khan |  |
| 583 |  |  [Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering](https://doi.org/10.18653/v1/2020.emnlp-main.99) |  | 0 | Existing work on augmenting question answering (QA) models with external knowledge (e.g., knowledge graphs) either struggle to model multi-hop relations efficiently, or lack transparency into the model’s prediction rationale. In this paper, we propose a novel knowledge-aware approach that equips... | Yanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng Wang, Jun Yan, Xiang Ren |  |
| 584 |  |  [Improving Bilingual Lexicon Induction for Low Frequency Words](https://doi.org/10.18653/v1/2020.emnlp-main.100) |  | 0 | This paper designs a Monolingual Lexicon Induction task and observes that two factors accompany the degraded accuracy of bilingual lexicon induction for rare words. First, a diminishing margin between similarities in low frequency regime, and secondly, exacerbated hubness at low frequency. Based on... | Jiaji Huang, Xingyu Cai, Kenneth Church |  |
| 585 |  |  [Learning VAE-LDA Models with Rounded Reparameterization Trick](https://doi.org/10.18653/v1/2020.emnlp-main.101) |  | 0 | The introduction of VAE provides an efficient framework for the learning of generative models, including generative topic models. However, when the topic model is a Latent Dirichlet Allocation (LDA) model, a central technique of VAE, the reparameterization trick, fails to be applicable. This is... | Runzhi Tian, Yongyi Mao, Richong Zhang |  |
| 586 |  |  [Calibrated Language Model Fine-Tuning for In- and Out-of-Distribution Data](https://doi.org/10.18653/v1/2020.emnlp-main.102) |  | 0 | Fine-tuned pre-trained language models can suffer from severe miscalibration for both in-distribution and out-of-distribution (OOD) data due to over-parameterization. To mitigate this issue, we propose a regularized fine-tuning method. Our method introduces two types of regularization for better... | Lingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie Lyu, Tuo Zhao, Chao Zhang |  |
| 587 |  |  [Scaling Hidden Markov Language Models](https://doi.org/10.18653/v1/2020.emnlp-main.103) |  | 0 | The hidden Markov model (HMM) is a fundamental tool for sequence modeling that cleanly separates the hidden state from the emission structure. However, this separation makes it difficult to fit HMMs to large datasets in modern NLP, and they have fallen out of use due to very poor performance... | Justin T. Chiu, Alexander M. Rush |  |
| 588 |  |  [Coding Textual Inputs Boosts the Accuracy of Neural Networks](https://doi.org/10.18653/v1/2020.emnlp-main.104) |  | 0 | Natural Language Processing (NLP) tasks are usually performed word by word on textual inputs. We can use arbitrary symbols to represent the linguistic meaning of a word and use these symbols as inputs. As “alternatives” to a text representation, we introduce Soundex, MetaPhone, NYSIIS, logogram to... | Abdul Rafae Khan, Jia Xu, Weiwei Sun |  |
| 589 |  |  [Learning from Task Descriptions](https://doi.org/10.18653/v1/2020.emnlp-main.105) |  | 0 | Typically, machine learning systems solve new tasks by training on thousands of examples. In contrast, humans can solve new tasks by reading some instructions, with perhaps an example or two. To take a step toward closing this gap, we introduce a framework for developing NLP systems that solve new... | Orion Weller, Nicholas Lourie, Matt Gardner, Matthew E. Peters |  |
| 590 |  |  [Hashtags, Emotions, and Comments: A Large-Scale Dataset to Understand Fine-Grained Social Emotions to Online Topics](https://doi.org/10.18653/v1/2020.emnlp-main.106) |  | 0 | This paper studies social emotions to online discussion topics. While most prior work focus on emotions from writers, we investigate readers’ responses and explore the public feelings to an online topic. A large-scale dataset is collected from Chinese microblog Sina Weibo with over 13 thousand... | Keyang Ding, Jing Li, Yuji Zhang |  |
| 591 |  |  [Named Entity Recognition for Social Media Texts with Semantic Augmentation](https://doi.org/10.18653/v1/2020.emnlp-main.107) |  | 0 | Existing approaches for named entity recognition suffer from data sparsity problems when conducted on short and informal texts, especially user-generated social media content. Semantic augmentation is a potential way to alleviate this problem. Given that rich semantic information is implicitly... | Yuyang Nie, Yuanhe Tian, Xiang Wan, Yan Song, Bo Dai |  |
| 592 |  |  [Coupled Hierarchical Transformer for Stance-Aware Rumor Verification in Social Media Conversations](https://doi.org/10.18653/v1/2020.emnlp-main.108) |  | 0 | The prevalent use of social media enables rapid spread of rumors on a massive scale, which leads to the emerging need of automatic rumor verification (RV). A number of previous studies focus on leveraging stance classification to enhance RV with multi-task learning (MTL) methods. However, most of... | Jianfei Yu, Jing Jiang, Ling Min Serena Khoo, Hai Leong Chieu, Rui Xia |  |
| 593 |  |  [Social Media Attributions in the Context of Water Crisis](https://doi.org/10.18653/v1/2020.emnlp-main.109) |  | 0 | Attribution of natural disasters/collective misfortune is a widely-studied political science problem. However, such studies typically rely on surveys, or expert opinions, or external signals such as voting outcomes. In this paper, we explore the viability of using unstructured, noisy social media... | Rupak Sarkar, Sayantan Mahinder, Hirak Sarkar, Ashiqur R. KhudaBukhsh |  |
| 594 |  |  [On the Reliability and Validity of Detecting Approval of Political Actors in Tweets](https://doi.org/10.18653/v1/2020.emnlp-main.110) |  | 0 | Social media sites like Twitter possess the potential to complement surveys that measure political opinions and, more specifically, political actors’ approval. However, new challenges related to the reliability and validity of social-media-based estimates arise. Various sentiment analysis and stance... | Indira Sen, Fabian Flöck, Claudia Wagner |  |
| 595 |  |  [Towards Medical Machine Reading Comprehension with Structural Knowledge and Plain Text](https://doi.org/10.18653/v1/2020.emnlp-main.111) |  | 0 | Machine reading comprehension (MRC) has achieved significant progress on the open domain in recent years, mainly due to large-scale pre-trained language models. However, it performs much worse in specific domains such as the medical field due to the lack of extensive training data and professional... | Dongfang Li, Baotian Hu, Qingcai Chen, Weihua Peng, Anqi Wang |  |
| 596 |  |  [Generating Radiology Reports via Memory-driven Transformer](https://doi.org/10.18653/v1/2020.emnlp-main.112) |  | 0 | Medical imaging is frequently used in clinical practice and trials for diagnosis and treatment. Writing imaging reports is time-consuming and can be error-prone for inexperienced radiologists. Therefore, automatically generating radiology reports is highly desired to lighten the workload of... | Zhihong Chen, Yan Song, TsungHui Chang, Xiang Wan |  |
| 597 |  |  [Planning and Generating Natural and Diverse Disfluent Texts as Augmentation for Disfluency Detection](https://doi.org/10.18653/v1/2020.emnlp-main.113) |  | 0 | Existing approaches to disfluency detection heavily depend on human-annotated data. Numbers of data augmentation methods have been proposed to alleviate the dependence on labeled data. However, current augmentation approaches such as random insertion or repetition fail to resemble training corpus... | Jingfeng Yang, Diyi Yang, Zhaoran Ma |  |
| 598 |  |  [Predicting Clinical Trial Results by Implicit Evidence Integration](https://doi.org/10.18653/v1/2020.emnlp-main.114) |  | 0 | Clinical trials provide essential guidance for practicing Evidence-Based Medicine, though often accompanying with unendurable costs and risks. To optimize the design of clinical trials, we introduce a novel Clinical Trial Result Prediction (CTRP) task. In the CTRP framework, a model takes a... | Qiao Jin, Chuanqi Tan, Mosha Chen, Xiaozhong Liu, Songfang Huang |  |
| 599 |  |  [Explainable Clinical Decision Support from Text](https://doi.org/10.18653/v1/2020.emnlp-main.115) |  | 0 | Clinical prediction models often use structured variables and provide outcomes that are not readily interpretable by clinicians. Further, free-text medical notes may contain information not immediately available in structured variables. We propose a hierarchical CNN-transformer model with explicit... | Jinyue Feng, Chantal Shaib, Frank Rudzicz |  |
| 600 |  |  [A Knowledge-driven Generative Model for Multi-implication Chinese Medical Procedure Entity Normalization](https://doi.org/10.18653/v1/2020.emnlp-main.116) |  | 0 | Medical entity normalization, which links medical mentions in the text to entities in knowledge bases, is an important research topic in medical natural language processing. In this paper, we focus on Chinese medical procedure entity normalization. However, nonstandard Chinese expressions and... | Jinghui Yan, Yining Wang, Lu Xiang, Yu Zhou, Chengqing Zong |  |
| 601 |  |  [Combining Automatic Labelers and Expert Annotations for Accurate Radiology Report Labeling Using BERT](https://doi.org/10.18653/v1/2020.emnlp-main.117) |  | 0 | The extraction of labels from radiology text reports enables large-scale training of medical imaging models. Existing approaches to report labeling typically rely either on sophisticated feature engineering based on medical domain knowledge or manual annotations by experts. In this work, we... | Akshay Smit, Saahil Jain, Pranav Rajpurkar, Anuj Pareek, Andrew Y. Ng, Matthew P. Lungren |  |
| 602 |  |  [Benchmarking Meaning Representations in Neural Semantic Parsing](https://doi.org/10.18653/v1/2020.emnlp-main.118) |  | 0 | Meaning representation is an important component of semantic parsing. Although researchers have designed a lot of meaning representations, recent work focuses on only a few of them. Thus, the impact of meaning representation on semantic parsing is less understood. Furthermore, existing work’s... | Jiaqi Guo, Qian Liu, JianGuang Lou, Zhenwen Li, Xueqing Liu, Tao Xie, Ting Liu |  |
| 603 |  |  [Analogous Process Structure Induction for Sub-event Sequence Prediction](https://doi.org/10.18653/v1/2020.emnlp-main.119) |  | 0 | Computational and cognitive studies of event understanding suggest that identifying, comprehending, and predicting events depend on having structured representations of a sequence of events and on conceptualizing (abstracting) its components into (soft) event categories. Thus, knowledge about a... | Hongming Zhang, Muhao Chen, Haoyu Wang, Yangqiu Song, Dan Roth |  |
| 604 |  |  [SLM: Learning a Discourse Language Representation with Sentence Unshuffling](https://doi.org/10.18653/v1/2020.emnlp-main.120) |  | 0 | We introduce Sentence-level Language Modeling, a new pre-training objective for learning a discourse language representation in a fully self-supervised manner. Recent pre-training methods in NLP focus on learning either bottom or top-level language representations: contextualized word... | Haejun Lee, Drew A. Hudson, Kangwook Lee, Christopher D. Manning |  |
| 605 |  |  [Detecting Fine-Grained Cross-Lingual Semantic Divergences without Supervision by Learning to Rank](https://doi.org/10.18653/v1/2020.emnlp-main.121) |  | 0 | Detecting fine-grained differences in content conveyed in different languages matters for cross-lingual NLP and multilingual corpora analysis, but it is a challenging machine learning problem since annotation is expensive and hard to scale. This work improves the prediction and annotation of... | Eleftheria Briakou, Marine Carpuat |  |
| 606 |  |  [A Bilingual Generative Transformer for Semantic Sentence Embedding](https://doi.org/10.18653/v1/2020.emnlp-main.122) |  | 0 | Semantic sentence embedding models encode natural language sentences into vectors, such that closeness in embedding space indicates closeness in the semantics between the sentences. Bilingual data offers a useful signal for learning such embeddings: properties shared by both sentences in a... | John Wieting, Graham Neubig, Taylor BergKirkpatrick |  |
| 607 |  |  [Semantically Inspired AMR Alignment for the Portuguese Language](https://doi.org/10.18653/v1/2020.emnlp-main.123) |  | 0 | Abstract Meaning Representation (AMR) is a graph-based semantic formalism where the nodes are concepts and edges are relations among them. Most of AMR parsing methods require alignment between the nodes of the graph and the words of the sentence. However, this alignment is not provided by manual... | Rafael T. Anchiêta, Thiago A. S. Pardo |  |
| 608 |  |  [An Unsupervised Sentence Embedding Method by Mutual Information Maximization](https://doi.org/10.18653/v1/2020.emnlp-main.124) |  | 0 | BERT is inefficient for sentence-pair tasks such as clustering or semantic search as it needs to evaluate combinatorially many sentence pairs which is very time-consuming. Sentence BERT (SBERT) attempted to solve this challenge by learning semantically meaningful representations of single sentences,... | Yan Zhang, Ruidan He, Zuozhu Liu, Kwan Hui Lim, Lidong Bing |  |
| 609 |  |  [Compositional Phrase Alignment and Beyond](https://doi.org/10.18653/v1/2020.emnlp-main.125) |  | 0 | Phrase alignment is the basis for modelling sentence pair interactions, such as paraphrase and textual entailment recognition. Most phrase alignments are compositional processes such that an alignment of a phrase pair is constructed based on the alignments of their child phrases. Nonetheless,... | Yuki Arase, Jun'ichi Tsujii |  |
| 610 |  |  [Table Fact Verification with Structure-Aware Transformer](https://doi.org/10.18653/v1/2020.emnlp-main.126) |  | 0 | Verifying fact on semi-structured evidence like tables requires the ability to encode structural information and perform symbolic reasoning. Pre-trained language models trained on natural language could not be directly applied to encode tables, because simply linearizing tables into sequences will... | Hongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi Cao, Fuzheng Zhang, Zhongyuan Wang |  |
| 611 |  |  [Double Graph Based Reasoning for Document-level Relation Extraction](https://doi.org/10.18653/v1/2020.emnlp-main.127) |  | 0 | Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across paragraphs. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN), a method to... | Shuang Zeng, Runxin Xu, Baobao Chang, Lei Li |  |
| 612 |  |  [Event Extraction as Machine Reading Comprehension](https://doi.org/10.18653/v1/2020.emnlp-main.128) |  | 0 | Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts. Previous methods for EE typically model it as a classification task, which are usually prone to the data scarcity problem. In this paper, we propose a new learning paradigm of EE, by... | Jian Liu, Yubo Chen, Kang Liu, Wei Bi, Xiaojiang Liu |  |
| 613 |  |  [MAVEN: A Massive General Domain Event Detection Dataset](https://doi.org/10.18653/v1/2020.emnlp-main.129) |  | 0 | Event detection (ED), which means identifying event trigger words and classifying event types, is the first and most fundamental step for extracting event knowledge from plain text. Most existing datasets exhibit the following issues that limit further development of ED: (1) Data scarcity. Existing... | Xiaozhi Wang, Ziqi Wang, Xu Han, Wangyi Jiang, Rong Han, Zhiyuan Liu, Juanzi Li, Peng Li, Yankai Lin, Jie Zhou |  |
| 614 |  |  [Knowledge Graph Alignment with Entity-Pair Embedding](https://doi.org/10.18653/v1/2020.emnlp-main.130) |  | 0 | Knowledge Graph (KG) alignment is to match entities in different KGs, which is important to knowledge fusion and integration. Recently, a number of embedding-based approaches for KG alignment have been proposed and achieved promising results. These approaches first embed entities in low-dimensional... | Zhichun Wang, Jinjian Yang, Xiaoju Ye |  |
| 615 |  |  [Adaptive Attentional Network for Few-Shot Knowledge Graph Completion](https://doi.org/10.18653/v1/2020.emnlp-main.131) |  | 0 | Few-shot Knowledge Graph (KG) completion is a focus of current research, where each task aims at querying unseen facts of a relation given its few-shot reference entity pairs. Recent attempts solve this problem by learning static representations of entities and references, ignoring their dynamic... | Jiawei Sheng, Shu Guo, Zhenyu Chen, Juwei Yue, Lihong Wang, Tingwen Liu, Hongbo Xu |  |
| 616 |  |  [Pre-training Entity Relation Encoder with Intra-span and Inter-span Information](https://doi.org/10.18653/v1/2020.emnlp-main.132) |  | 0 | In this paper, we integrate span-related information into pre-trained encoder for entity relation extraction task. Instead of using general-purpose sentence encoder (e.g., existing universal pre-trained models), we introduce a span encoder and a span pair encoder to the pre-training network, which... | Yijun Wang, Changzhi Sun, Yuanbin Wu, Junchi Yan, Peng Gao, Guotong Xie |  |
| 617 |  |  [Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders](https://doi.org/10.18653/v1/2020.emnlp-main.133) |  | 0 | Named entity recognition and relation extraction are two important fundamental problems. Joint learning algorithms have been proposed to solve both tasks simultaneously, and many of them cast the joint task as a table-filling problem. However, they typically focused on learning a single encoder... | Jue Wang, Wei Lu |  |
| 618 |  |  [Beyond [CLS] through Ranking by Generation](https://doi.org/10.18653/v1/2020.emnlp-main.134) |  | 0 | Generative models for Information Retrieval, where ranking of documents is viewed as the task of generating a query from a document’s language model, were very successful in various IR tasks in the past. However, with the advent of modern deep neural networks, attention has shifted to discriminative... | Cícero Nogueira dos Santos, Xiaofei Ma, Ramesh Nallapati, Zhiheng Huang, Bing Xiang |  |
| 619 |  |  [Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too!](https://doi.org/10.18653/v1/2020.emnlp-main.135) |  | 0 | Topic models are a useful analysis tool to uncover the underlying themes within document collections. The dominant approach is to use probabilistic topic models that posit a generative story, but in this paper we propose an alternative way to obtain topics: clustering pre-trained word embeddings... | Suzanna Sia, Ayush Dalmia, Sabrina J. Mielke |  |
| 620 |  |  [Multi-document Summarization with Maximal Marginal Relevance-guided Reinforcement Learning](https://doi.org/10.18653/v1/2020.emnlp-main.136) |  | 0 | While neural sequence learning methods have made significant progress in single-document summarization (SDS), they produce unsatisfactory results on multi-document summarization (MDS). We observe two major challenges when adapting SDS advances to MDS: (1) MDS involves larger search space and yet... | Yuning Mao, Yanru Qu, Yiqing Xie, Xiang Ren, Jiawei Han |  |
| 621 |  |  [Improving Neural Topic Models using Knowledge Distillation](https://doi.org/10.18653/v1/2020.emnlp-main.137) |  | 0 | Topic models are often used to identify human-interpretable topics to help make sense of large document collections. We use knowledge distillation to combine the best attributes of probabilistic topic models and pretrained transformers. Our modular method can be straightforwardly applied with any... | Alexander Miserlis Hoyle, Pranav Goel, Philip Resnik |  |
| 622 |  |  [Short Text Topic Modeling with Topic Distribution Quantization and Negative Sampling Decoder](https://doi.org/10.18653/v1/2020.emnlp-main.138) |  | 0 | Topic models have been prevailing for many years on discovering latent semantics while modeling long documents. However, for short texts they generally suffer from data sparsity because of extremely limited word co-occurrences; thus tend to yield repetitive or trivial topics with low quality. In... | Xiaobao Wu, Chunping Li, Yan Zhu, Yishu Miao |  |
| 623 |  |  [Querying Across Genres for Medical Claims in News](https://doi.org/10.18653/v1/2020.emnlp-main.139) |  | 0 | We present a query-based biomedical information retrieval task across two vastly different genres – newswire and research literature – where the goal is to find the research publication that supports the primary claim made in a health-related news article. For this task, we present a new dataset of... | Chaoyuan Zuo, Narayan Acharya, Ritwik Banerjee |  |
| 624 |  |  [Incorporating Multimodal Information in Open-Domain Web Keyphrase Extraction](https://doi.org/10.18653/v1/2020.emnlp-main.140) |  | 0 | Open-domain Keyphrase extraction (KPE) on the Web is a fundamental yet complex NLP task with a wide range of practical applications within the field of Information Retrieval. In contrast to other document types, web page designs are intended for easy navigation and information finding. Effective... | Yansen Wang, Zhen Fan, Carolyn P. Rosé |  |
| 625 |  |  [CMU-MOSEAS: A Multimodal Language Dataset for Spanish, Portuguese, German and French](https://doi.org/10.18653/v1/2020.emnlp-main.141) |  | 0 | Modeling multimodal language is a core research area in natural language processing. While languages such as English have relatively large multimodal language resources, other widely spoken languages across the globe have few or no large-scale datasets in this area. This disproportionately affects... | AmirAli Bagher Zadeh, Yansheng Cao, Smon Hessner, Paul Pu Liang, Soujanya Poria, LouisPhilippe Morency |  |
| 626 |  |  [Combining Self-Training and Self-Supervised Learning for Unsupervised Disfluency Detection](https://doi.org/10.18653/v1/2020.emnlp-main.142) |  | 0 | Most existing approaches to disfluency detection heavily rely on human-annotated corpora, which is expensive to obtain in practice. There have been several proposals to alleviate this issue with, for instance, self-supervised learning techniques, but they still require human-annotated corpora. In... | Shaolei Wang, Zhongyuan Wang, Wanxiang Che, Ting Liu |  |
| 627 |  |  [Multimodal Routing: Improving Local and Global Interpretability of Multimodal Language Analysis](https://doi.org/10.18653/v1/2020.emnlp-main.143) |  | 0 | The human language can be expressed through multiple sources of information known as modalities, including tones of voice, facial gestures, and spoken language. Recent multimodal learning with strong performances on human-centric tasks such as sentiment analysis and emotion recognition are often... | YaoHung Hubert Tsai, Martin Q. Ma, Muqiao Yang, Ruslan Salakhutdinov, LouisPhilippe Morency |  |
| 628 |  |  [Multistage Fusion with Forget Gate for Multimodal Summarization in Open-Domain Videos](https://doi.org/10.18653/v1/2020.emnlp-main.144) |  | 0 | Multimodal summarization for open-domain videos is an emerging task, aiming to generate a summary from multisource information (video, audio, transcript). Despite the success of recent multiencoder-decoder frameworks on this task, existing methods lack fine-grained multimodality interactions of... | Nayu Liu, Xian Sun, Hongfeng Yu, Wenkai Zhang, Guangluan Xu |  |
| 629 |  |  [BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded Dialogues](https://doi.org/10.18653/v1/2020.emnlp-main.145) |  | 0 | Video-grounded dialogues are very challenging due to (i) the complexity of videos which contain both spatial and temporal variations, and (ii) the complexity of user utterances which query different segments and/or different objects in videos over multiple dialogue turns. However, existing... | Hung Le, Doyen Sahoo, Nancy F. Chen, Steven C. H. Hoi |  |
| 630 |  |  [UniConv: A Unified Conversational Neural Architecture for Multi-domain Task-oriented Dialogues](https://doi.org/10.18653/v1/2020.emnlp-main.146) |  | 0 | Building an end-to-end conversational agent for multi-domain task-oriented dialogues has been an open challenge for two main reasons. First, tracking dialogue states of multiple domains is non-trivial as the dialogue agent must obtain complete states from all relevant domains, some of which might... | Hung Le, Doyen Sahoo, Chenghao Liu, Nancy F. Chen, Steven C. H. Hoi |  |
| 631 |  |  [GraphDialog: Integrating Graph Knowledge into End-to-End Task-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2020.emnlp-main.147) |  | 0 | End-to-end task-oriented dialogue systems aim to generate system responses directly from plain text inputs. There are two challenges for such systems: one is how to effectively incorporate external knowledge bases (KBs) into the learning framework; the other is how to accurately capture the... | Shiquan Yang, Rui Zhang, Sarah M. Erfani |  |
| 632 |  |  [Structured Attention for Unsupervised Dialogue Structure Induction](https://doi.org/10.18653/v1/2020.emnlp-main.148) |  | 0 | Inducing a meaningful structural representation from one or a set of dialogues is a crucial but challenging task in computational linguistics. Advancement made in this area is critical for dialogue system design and discourse analysis. It can also be extended to solve grammatical inference. In this... | Liang Qiu, Yizhou Zhao, Weiyan Shi, Yuan Liang, Feng Shi, Tao Yuan, Zhou Yu, SongChun Zhu |  |
| 633 |  |  [Cross Copy Network for Dialogue Generation](https://doi.org/10.18653/v1/2020.emnlp-main.149) |  | 0 | In the past few years, audiences from different fields witness the achievements of sequence-to-sequence models (e.g., LSTM+attention, Pointer Generator Networks and Transformer) to enhance dialogue content generation. While content fluency and accuracy often serve as the major indicators for model... | Changzhen Ji, Xin Zhou, Yating Zhang, Xiaozhong Liu, Changlong Sun, Conghui Zhu, Tiejun Zhao |  |
| 634 |  |  [Multi-turn Response Selection using Dialogue Dependency Relations](https://doi.org/10.18653/v1/2020.emnlp-main.150) |  | 0 | Multi-turn response selection is a task designed for developing dialogue agents. The performance on this task has a remarkable improvement with pre-trained language models. However, these models simply concatenate the turns in dialogue history as the input and largely ignore the dependencies between... | Qi Jia, Yizhu Liu, Siyu Ren, Kenny Q. Zhu, Haifeng Tang |  |
| 635 |  |  [Parallel Interactive Networks for Multi-Domain Dialogue State Generation](https://doi.org/10.18653/v1/2020.emnlp-main.151) |  | 0 | The dependencies between system and user utterances in the same turn and across different turns are not fully considered in existing multidomain dialogue state tracking (MDST) models. In this study, we argue that the incorporation of these dependencies is crucial for the design of MDST and propose... | Junfan Chen, Richong Zhang, Yongyi Mao, Jie Xu |  |
| 636 |  |  [SlotRefine: A Fast Non-Autoregressive Model for Joint Intent Detection and Slot Filling](https://doi.org/10.18653/v1/2020.emnlp-main.152) |  | 0 | Slot filling and intent detection are two main tasks in spoken language understanding (SLU) system. In this paper, we propose a novel non-autoregressive model named SlotRefine for joint intent detection and slot filling. Besides, we design a novel two-pass iteration mechanism to handle the... | Di Wu, Liang Ding, Fan Lu, Jian Xie |  |
| 637 |  |  [An Information Bottleneck Approach for Controlling Conciseness in Rationale Extraction](https://doi.org/10.18653/v1/2020.emnlp-main.153) |  | 0 | Decisions of complex models for language understanding can be explained by limiting the inputs they are provided to a relevant subsequence of the original text — a rationale. Models that condition predictions on a concise rationale, while being more interpretable, tend to be less accurate than... | Bhargavi Paranjape, Mandar Joshi, John Thickstun, Hannaneh Hajishirzi, Luke Zettlemoyer |  |
| 638 |  |  [CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models](https://doi.org/10.18653/v1/2020.emnlp-main.154) |  | 0 | Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To... | Nikita Nangia, Clara Vania, Rasika Bhalerao, Samuel R. Bowman |  |
| 639 |  |  [LOGAN: Local Group Bias Detection by Clustering](https://doi.org/10.18653/v1/2020.emnlp-main.155) |  | 0 | Machine learning techniques have been widely used in natural language processing (NLP). However, as revealed by many recent studies, machine learning models often inherit and amplify the societal biases in data. Various metrics have been proposed to quantify biases in model predictions. In... | Jieyu Zhao, KaiWei Chang |  |
| 640 |  |  [RNNs can generate bounded hierarchical languages with optimal memory](https://doi.org/10.18653/v1/2020.emnlp-main.156) |  | 0 | Recurrent neural networks empirically generate natural language with high syntactic fidelity. However, their success is not well-understood theoretically. We provide theoretical insight into this success, proving in a finite-precision setting that RNNs can efficiently generate bounded hierarchical... | John Hewitt, Michael Hahn, Surya Ganguli, Percy Liang, Christopher D. Manning |  |
| 641 |  |  [Detecting Independent Pronoun Bias with Partially-Synthetic Data Generation](https://doi.org/10.18653/v1/2020.emnlp-main.157) |  | 0 | We report that state-of-the-art parsers consistently failed to identify “hers” and “theirs” as pronouns but identified the masculine equivalent “his”. We find that the same biases exist in recent language models like BERT. While some of the bias comes from known sources, like training data with... | Robert Munro, Alex Morrison |  |
| 642 |  |  [Visually Grounded Continual Learning of Compositional Phrases](https://doi.org/10.18653/v1/2020.emnlp-main.158) |  | 0 | Humans acquire language continually with much more limited access to data samples at a time, as compared to contemporary NLP systems. To study this human-like language acquisition ability, we present VisCOLL, a visually grounded language learning task, which simulates the continual acquisition of... | Xisen Jin, Junyi Du, Arka Sadhu, Ram Nevatia, Xiang Ren |  |
| 643 |  |  [MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase Grounding](https://doi.org/10.18653/v1/2020.emnlp-main.159) |  | 0 | Phrase localization is a task that studies the mapping from textual phrases to regions of an image. Given difficulties in annotating phrase-to-object datasets at scale, we develop a Multimodal Alignment Framework (MAF) to leverage more widely-available caption-image datasets, which can then be used... | Qinxin Wang, Hao Tan, Sheng Shen, Michael W. Mahoney, Zhewei Yao |  |
| 644 |  |  [Domain-Specific Lexical Grounding in Noisy Visual-Textual Documents](https://doi.org/10.18653/v1/2020.emnlp-main.160) |  | 0 | Images can give us insights into the contextual meanings of words, but current image-text grounding approaches require detailed annotations. Such granular annotation is rare, expensive, and unavailable in most domain-specific contexts. In contrast, unlabeled multi-image, multi-sentence documents are... | Gregory Yauney, Jack Hessel, David Mimno |  |
| 645 |  |  [HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training](https://doi.org/10.18653/v1/2020.emnlp-main.161) |  | 0 | We present HERO, a novel framework for large-scale video+language omni-representation learning. HERO encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a Cross-modal Transformer via multimodal fusion, and global video context is captured by a... | Linjie Li, YenChun Chen, Yu Cheng, Zhe Gan, Licheng Yu, Jingjing Liu |  |
| 646 |  |  [Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision](https://doi.org/10.18653/v1/2020.emnlp-main.162) |  | 0 | Humans learn language by listening, speaking, writing, reading, and also, via interaction with the multimodal real world. Existing language pre-training frameworks show the effectiveness of text-only self-supervision while we explore the idea of a visually-supervised language model in this paper. We... | Hao Tan, Mohit Bansal |  |
| 647 |  |  [Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News](https://doi.org/10.18653/v1/2020.emnlp-main.163) |  | 0 | Large-scale dissemination of disinformation online intended to mislead or deceive the general population is a major societal problem. Rapid progression in image, video, and natural language generative models has only exacerbated this situation and intensified our need for an effective defense... | Reuben Tan, Bryan A. Plummer, Kate Saenko |  |
| 648 |  |  [Enhancing Aspect Term Extraction with Soft Prototypes](https://doi.org/10.18653/v1/2020.emnlp-main.164) |  | 0 | Aspect term extraction (ATE) aims to extract aspect terms from a review sentence that users have expressed opinions on. Existing studies mostly focus on designing neural sequence taggers to extract linguistic features from the token level. However, since the aspect terms and context words usually... | Zhuang Chen, Tieyun Qian |  |
| 649 |  |  [FedED: Federated Learning via Ensemble Distillation for Medical Relation Extraction](https://doi.org/10.18653/v1/2020.emnlp-main.165) |  | 0 | Unlike other domains, medical texts are inevitably accompanied by private information, so sharing or copying these texts is strictly restricted. However, training a medical relation extraction model requires collecting these privacy-sensitive texts and storing them on one machine, which comes in... | Dianbo Sui, Yubo Chen, Jun Zhao, Yantao Jia, Yuantao Xie, Weijian Sun |  |
| 650 |  |  [Multimodal Joint Attribute Prediction and Value Extraction for E-commerce Product](https://doi.org/10.18653/v1/2020.emnlp-main.166) |  | 0 | Product attribute values are essential in many e-commerce scenarios, such as customer service robots, product recommendations, and product retrieval. While in the real world, the attribute values of a product are usually incomplete and vary over time, which greatly hinders the practical... | Tiangang Zhu, Yue Wang, Haoran Li, Youzheng Wu, Xiaodong He, Bowen Zhou |  |
| 651 |  |  [A Predicate-Function-Argument Annotation of Natural Language for Open-Domain Information eXpression](https://doi.org/10.18653/v1/2020.emnlp-main.167) |  | 0 | Existing OIE (Open Information Extraction) algorithms are independent of each other such that there exist lots of redundant works; the featured strategies are not reusable and not adaptive to new tasks. This paper proposes a new pipeline to build OIE systems, where an Open-domain Information... | Mingming Sun, Wenyue Hua, Zoey Liu, Xin Wang, Kangjie Zheng, Ping Li |  |
| 652 |  |  [Retrofitting Structure-aware Transformer Language Model for End Tasks](https://doi.org/10.18653/v1/2020.emnlp-main.168) |  | 0 | We consider retrofitting structure-aware Transformer language model for facilitating end tasks by proposing to exploit syntactic distance to encode both the phrasal constituency and dependency connection into the language model. A middle-layer structural learning strategy is leveraged for structure... | Hao Fei, Yafeng Ren, Donghong Ji |  |
| 653 |  |  [Lightweight, Dynamic Graph Convolutional Networks for AMR-to-Text Generation](https://doi.org/10.18653/v1/2020.emnlp-main.169) |  | 0 | AMR-to-text generation is used to transduce Abstract Meaning Representation structures (AMR) into text. A key challenge in this task is to efficiently learn effective graph representations. Previously, Graph Convolution Networks (GCNs) were used to encode input AMRs, however, vanilla GCNs are not... | Yan Zhang, Zhijiang Guo, Zhiyang Teng, Wei Lu, Shay B. Cohen, Zuozhu Liu, Lidong Bing |  |
| 654 |  |  [If beam search is the answer, what was the question?](https://doi.org/10.18653/v1/2020.emnlp-main.170) |  | 0 | Quite surprisingly, exact maximum a posteriori (MAP) decoding of neural language generators frequently leads to low-quality results. Rather, most state-of-the-art results on language generation tasks are attained using beam search despite its overwhelmingly high search error rate. This implies that... | Clara Meister, Ryan Cotterell, Tim Vieira |  |
| 655 |  |  [Understanding the Mechanics of SPIGOT: Surrogate Gradients for Latent Structure Learning](https://doi.org/10.18653/v1/2020.emnlp-main.171) |  | 0 | Latent structure models are a powerful tool for modeling language data: they can mitigate the error propagation and annotation bottleneck in pipeline systems, while simultaneously uncovering linguistic insights about the data. One challenge with end-to-end training of these models is the argmax... | Tsvetomila Mihaylova, Vlad Niculae, André F. T. Martins |  |
| 656 |  |  [Is the Best Better? Bayesian Statistical Model Comparison for Natural Language Processing](https://doi.org/10.18653/v1/2020.emnlp-main.172) |  | 0 | Recent work raises concerns about the use of standard splits to compare natural language processing models. We propose a Bayesian statistical model comparison technique which uses k-fold cross-validation across multiple data sets to estimate the likelihood that one model will outperform the other,... | Piotr Szymanski, Kyle Gorman |  |
| 657 |  |  [Exploring Logically Dependent Multi-task Learning with Causal Inference](https://doi.org/10.18653/v1/2020.emnlp-main.173) |  | 0 | Previous studies have shown that hierarchical multi-task learning (MTL) can utilize task dependencies by stacking encoders and outperform democratic MTL. However, stacking encoders only considers the dependencies of feature representations and ignores the label dependencies in logically dependent... | Wenqing Chen, Jidong Tian, Liqiang Xiao, Hao He, Yaohui Jin |  |
| 658 |  |  [Masking as an Efficient Alternative to Finetuning for Pretrained Language Models](https://doi.org/10.18653/v1/2020.emnlp-main.174) |  | 0 | We present an efficient method of utilizing pretrained language models, where we learn selective binary masks for pretrained weights in lieu of modifying them through finetuning. Extensive evaluations of masking BERT, RoBERTa, and DistilBERT on eleven diverse NLP tasks show that our masking scheme... | Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, Hinrich Schütze |  |
| 659 |  |  [Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning](https://doi.org/10.18653/v1/2020.emnlp-main.175) |  | 0 | Document-level neural machine translation has yielded attractive improvements. However, majority of existing methods roughly use all context sentences in a fixed scope. They neglect the fact that different source sentences need different sizes of context. To address this problem, we propose an... | Xiaomian Kang, Yang Zhao, Jiajun Zhang, Chengqing Zong |  |
| 660 |  |  [Data Rejuvenation: Exploiting Inactive Training Examples for Neural Machine Translation](https://doi.org/10.18653/v1/2020.emnlp-main.176) |  | 0 | Large-scale training datasets lie at the core of the recent success of neural machine translation (NMT) models. However, the complex patterns and potential noises in the large-scale data make training NMT models difficult. In this work, we explore to identify the inactive training examples which... | Wenxiang Jiao, Xing Wang, Shilin He, Irwin King, Michael R. Lyu, Zhaopeng Tu |  |
| 661 |  |  [Pronoun-Targeted Fine-tuning for NMT with Hybrid Losses](https://doi.org/10.18653/v1/2020.emnlp-main.177) |  | 0 | Popular Neural Machine Translation model training uses strategies like backtranslation to improve BLEU scores, requiring large amounts of additional data and training. We introduce a class of conditional generative-discriminative hybrid losses that we use to fine-tune a trained machine translation... | Prathyusha Jwalapuram, Shafiq R. Joty, Youlin Shen |  |
| 662 |  |  [Learning Adaptive Segmentation Policy for Simultaneous Translation](https://doi.org/10.18653/v1/2020.emnlp-main.178) |  | 0 | Balancing accuracy and latency is a great challenge for simultaneous translation. To achieve high accuracy, the model usually needs to wait for more streaming text before translation, which results in increased latency. However, keeping low latency would probably hurt accuracy. Therefore, it is... | Ruiqing Zhang, Chuanqiang Zhang, Zhongjun He, Hua Wu, Haifeng Wang |  |
| 663 |  |  [Learn to Cross-lingual Transfer with Meta Graph Learning Across Heterogeneous Languages](https://doi.org/10.18653/v1/2020.emnlp-main.179) |  | 0 | Recent emergence of multilingual pre-training language model (mPLM) has enabled breakthroughs on various downstream cross-lingual transfer (CLT) tasks. However, mPLM-based methods usually involve two problems: (1) simply fine-tuning may not adapt general-purpose multilingual representations to be... | Zheng Li, Mukul Kumar, William Headden, Bing Yin, Ying Wei, Yu Zhang, Qiang Yang |  |
| 664 |  |  [UDapter: Language Adaptation for Truly Universal Dependency Parsing](https://doi.org/10.18653/v1/2020.emnlp-main.180) |  | 0 | Recent advances in multilingual dependency parsing have brought the idea of a truly universal parser closer to reality. However, cross-language interference and restrained model capacity remain major obstacles. To address this, we propose a novel multilingual task adaptation approach based on... | Ahmet Üstün, Arianna Bisazza, Gosse Bouma, Gertjan van Noord |  |
| 665 |  |  [Uncertainty-Aware Label Refinement for Sequence Labeling](https://doi.org/10.18653/v1/2020.emnlp-main.181) |  | 0 | Conditional random fields (CRF) for label decoding has become ubiquitous in sequence labeling tasks. However, the local label dependencies and inefficient Viterbi decoding have always been a problem to be solved. In this work, we introduce a novel two-stage label decoding framework to model... | Tao Gui, Jiacheng Ye, Qi Zhang, Zhengyan Li, Zichu Fei, Yeyun Gong, Xuanjing Huang |  |
| 666 |  |  [Adversarial Attack and Defense of Structured Prediction Models](https://doi.org/10.18653/v1/2020.emnlp-main.182) |  | 0 | Building an effective adversarial attacker and elaborating on countermeasures for adversarial attacks for natural language processing (NLP) have attracted a lot of research in recent years. However, most of the existing approaches focus on classification problems. In this paper, we investigate... | Wenjuan Han, Liwen Zhang, Yong Jiang, Kewei Tu |  |
| 667 |  |  [Position-Aware Tagging for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2020.emnlp-main.183) |  | 0 | Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting the triplets of target entities, their associated sentiment, and opinion spans explaining the reason for the sentiment. Existing research efforts mostly solve this problem using pipeline approaches, which break the triplet... | Lu Xu, Hao Li, Wei Lu, Lidong Bing |  |
| 668 |  |  [Simultaneous Machine Translation with Visual Context](https://doi.org/10.18653/v1/2020.emnlp-main.184) |  | 0 | Simultaneous machine translation (SiMT) aims to translate a continuous input text stream into another language with the lowest latency and highest quality possible. The translation thus has to start with an incomplete source text, which is read progressively, creating the need for anticipation. In... | Ozan Caglayan, Julia Ive, Veneta Haralampieva, Pranava Madhyastha, Loïc Barrault, Lucia Specia |  |
| 669 |  |  [XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning](https://doi.org/10.18653/v1/2020.emnlp-main.185) |  | 0 | In order to simulate human language capacity, natural language processing systems must be able to reason about the dynamics of everyday situations, including their possible causes and effects. Moreover, they should be able to generalise the acquired world knowledge to new languages, modulo cultural... | Edoardo Maria Ponti, Goran Glavas, Olga Majewska, Qianchu Liu, Ivan Vulic, Anna Korhonen |  |
| 670 |  |  [The Secret is in the Spectra: Predicting Cross-lingual Task Performance with Spectral Similarity Measures](https://doi.org/10.18653/v1/2020.emnlp-main.186) |  | 0 | Performance in cross-lingual NLP tasks is impacted by the (dis)similarity of languages at hand: e.g., previous work has suggested there is a connection between the expected success of bilingual lexicon induction (BLI) and the assumption of (approximate) isomorphism between monolingual embedding... | Haim Dubossarsky, Ivan Vulic, Roi Reichart, Anna Korhonen |  |
| 671 |  |  [Bridging Linguistic Typology and Multilingual Machine Translation with Multi-View Language Representations](https://doi.org/10.18653/v1/2020.emnlp-main.187) |  | 0 | Sparse language vectors from linguistic typology databases and learned embeddings from tasks like multilingual machine translation have been investigated in isolation, without analysing how they could benefit from each other’s language characterisation. We propose to fuse both views using singular... | Arturo Oncevay, Barry Haddow, Alexandra Birch |  |
| 672 |  |  [AnswerFact: Fact Checking in Product Question Answering](https://doi.org/10.18653/v1/2020.emnlp-main.188) |  | 0 | Product-related question answering platforms nowadays are widely employed in many E-commerce sites, providing a convenient way for potential customers to address their concerns during online shopping. However, the misinformation in the answers on those platforms poses unprecedented challenges for... | Wenxuan Zhang, Yang Deng, Jing Ma, Wai Lam |  |
| 673 |  |  [Context-Aware Answer Extraction in Question Answering](https://doi.org/10.18653/v1/2020.emnlp-main.189) |  | 0 | Extractive QA models have shown very promising performance in predicting the correct answer to a question for a given passage. However, they sometimes result in predicting the correct answer text but in a context irrelevant to the given question. This discrepancy becomes especially important as the... | Yeon Seonwoo, JiHoon Kim, JungWoo Ha, Alice Oh |  |
| 674 |  |  [What do Models Learn from Question Answering Datasets?](https://doi.org/10.18653/v1/2020.emnlp-main.190) |  | 0 | While models have reached superhuman performance on popular question answering (QA) datasets such as SQuAD, they have yet to outperform humans on the task of question answering itself. In this paper, we investigate if models are learning reading comprehension from QA datasets by evaluating... | Priyanka Sen, Amir Saffari |  |
| 675 |  |  [Discern: Discourse-Aware Entailment Reasoning Network for Conversational Machine Reading](https://doi.org/10.18653/v1/2020.emnlp-main.191) |  | 0 | Document interpretation and dialog understanding are the two major challenges for conversational machine reading. In this work, we propose “Discern”, a discourse-aware entailment reasoning network to strengthen the connection and enhance the understanding of both document and dialog. Specifically,... | Yifan Gao, ChienSheng Wu, Jingjing Li, Shafiq R. Joty, Steven C. H. Hoi, Caiming Xiong, Irwin King, Michael R. Lyu |  |
| 676 |  |  [A Method for Building a Commonsense Inference Dataset based on Basic Events](https://doi.org/10.18653/v1/2020.emnlp-main.192) |  | 0 | We present a scalable, low-bias, and low-cost method for building a commonsense inference dataset that combines automatic extraction from a corpus and crowdsourcing. Each problem is a multiple-choice question that asks contingency between basic events. We applied the proposed method to a Japanese... | Kazumasa Omura, Daisuke Kawahara, Sadao Kurohashi |  |
| 677 |  |  [Neural Deepfake Detection with Factual Structure of Text](https://doi.org/10.18653/v1/2020.emnlp-main.193) |  | 0 | Deepfake detection, the task of automatically discriminating machine-generated text, is increasingly critical with recent advances in natural language generative models. Existing approaches to deepfake detection typically represent documents with coarse-grained representations. However, they... | Wanjun Zhong, Duyu Tang, Zenan Xu, Ruize Wang, Nan Duan, Ming Zhou, Jiahai Wang, Jian Yin |  |
| 678 |  |  [MultiCQA: Zero-Shot Transfer of Self-Supervised Text Matching Models on a Massive Scale](https://doi.org/10.18653/v1/2020.emnlp-main.194) |  | 0 | We study the zero-shot transfer capabilities of text matching models on a massive scale, by self-supervised training on 140 source domains from community question answering forums in English. We investigate the model performances on nine benchmarks of answer selection and question similarity tasks,... | Andreas Rücklé, Jonas Pfeiffer, Iryna Gurevych |  |
| 679 |  |  [XL-AMR: Enabling Cross-Lingual AMR Parsing with Transfer Learning Techniques](https://doi.org/10.18653/v1/2020.emnlp-main.195) |  | 0 | Abstract Meaning Representation (AMR) is a popular formalism of natural language that represents the meaning of a sentence as a semantic graph. It is agnostic about how to derive meanings from strings and for this reason it lends itself well to the encoding of semantics across languages. However,... | Rexhina Blloshmi, Rocco Tripodi, Roberto Navigli |  |
| 680 |  |  [Improving AMR Parsing with Sequence-to-Sequence Pre-training](https://doi.org/10.18653/v1/2020.emnlp-main.196) |  | 0 | In the literature, the research on abstract meaning representation (AMR) parsing is much restricted by the size of human-curated dataset which is critical to build an AMR parser with good performance. To alleviate such data size restriction, pre-trained models have been drawing more and more... | Dongqin Xu, Junhui Li, Muhua Zhu, Min Zhang, Guodong Zhou |  |
| 681 |  |  [Hate-Speech and Offensive Language Detection in Roman Urdu](https://doi.org/10.18653/v1/2020.emnlp-main.197) |  | 0 | The task of automatic hate-speech and offensive language detection in social media content is of utmost importance due to its implications in unprejudiced society concerning race, gender, or religion. Existing research in this area, however, is mainly focused on the English language, limiting the... | Hammad Rizwan, Muhammad Haroon Shakeel, Asim Karim |  |
| 682 |  |  [Suicidal Risk Detection for Military Personnel](https://doi.org/10.18653/v1/2020.emnlp-main.198) |  | 0 | We analyze social media for detecting the suicidal risk of military personnel, which is especially crucial for countries with compulsory military service such as the Republic of Korea. From a widely-used Korean social Q&A site, we collect posts containing military-relevant content written by... | Sungjoon Park, KiWoong Park, Jaimeen Ahn, Alice Oh |  |
| 683 |  |  [Comparative Evaluation of Label-Agnostic Selection Bias in Multilingual Hate Speech Datasets](https://doi.org/10.18653/v1/2020.emnlp-main.199) |  | 0 | Work on bias in hate speech typically aims to improve classification performance while relatively overlooking the quality of the data. We examine selection bias in hate speech in a language and label independent fashion. We first use topic models to discover latent semantics in eleven hate speech... | Nedjma Ousidhoum, Yangqiu Song, DitYan Yeung |  |
| 684 |  |  [HENIN: Learning Heterogeneous Neural Interaction Networks for Explainable Cyberbullying Detection on Social Media](https://doi.org/10.18653/v1/2020.emnlp-main.200) |  | 0 | In the computational detection of cyberbullying, existing work largely focused on building generic classifiers that rely exclusively on text analysis of social media sessions. Despite their empirical success, we argue that a critical missing piece is the model explainability, i.e., why a particular... | HsinYu Chen, ChengTe Li |  |
| 685 |  |  [Reactive Supervision: A New Method for Collecting Sarcasm Data](https://doi.org/10.18653/v1/2020.emnlp-main.201) |  | 0 | Sarcasm detection is an important task in affective computing, requiring large amounts of labeled data. We introduce reactive supervision, a novel data collection method that utilizes the dynamics of online conversations to overcome the limitations of existing data collection techniques. We use the... | Boaz Shmueli, LunWei Ku, Soumya Ray |  |
| 686 |  |  [Self-Induced Curriculum Learning in Self-Supervised Neural Machine Translation](https://doi.org/10.18653/v1/2020.emnlp-main.202) |  | 0 | Self-supervised neural machine translation (SSNMT) jointly learns to identify and select suitable training data from comparable (rather than parallel) corpora and to translate, in a way that the two tasks support each other in a virtuous circle. In this study, we provide an in-depth analysis of the... | Dana Ruiter, Josef van Genabith, Cristina EspañaBonet |  |
| 687 |  |  [Towards Reasonably-Sized Character-Level Transformer NMT by Finetuning Subword Systems](https://doi.org/10.18653/v1/2020.emnlp-main.203) |  | 0 | Applying the Transformer architecture on the character level usually requires very deep architectures that are difficult and slow to train. These problems can be partially overcome by incorporating a segmentation into tokens in the model. We show that by initially training a subword model and then... | Jindrich Libovický, Alexander Fraser |  |
| 688 |  |  [Transfer Learning and Distant Supervision for Multilingual Transformer Models: A Study on African Languages](https://doi.org/10.18653/v1/2020.emnlp-main.204) |  | 0 | Multilingual transformer models like mBERT and XLM-RoBERTa have obtained great improvements for many NLP tasks on a variety of languages. However, recent works also showed that results from high-resource languages could not be easily transferred to realistic, low-resource scenarios. In this work, we... | Michael A. Hedderich, David Ifeoluwa Adelani, Dawei Zhu, Jesujoba O. Alabi, Udia Markus, Dietrich Klakow |  |
| 689 |  |  [Translation Quality Estimation by Jointly Learning to Score and Rank](https://doi.org/10.18653/v1/2020.emnlp-main.205) |  | 0 | The translation quality estimation (QE) task, particularly the QE as a Metric task, aims to evaluate the general quality of a translation based on the translation and the source sentence without using reference translations. Supervised learning of this QE task requires human evaluation of... | Jingyi Zhang, Josef van Genabith |  |
| 690 |  |  [Direct Segmentation Models for Streaming Speech Translation](https://doi.org/10.18653/v1/2020.emnlp-main.206) |  | 0 | The cascade approach to Speech Translation (ST) is based on a pipeline that concatenates an Automatic Speech Recognition (ASR) system followed by a Machine Translation (MT) system. These systems are usually connected by a segmenter that splits the ASR output into hopefully, semantically... | Javier IranzoSánchez, Adrià GiménezPastor, Joan Albert SilvestreCerdà, Pau BaqueroArnal, Jorge Civera Saiz, Alfons Juan |  |
| 691 |  |  [Not Low-Resource Anymore: Aligner Ensembling, Batch Filtering, and New Datasets for Bengali-English Machine Translation](https://doi.org/10.18653/v1/2020.emnlp-main.207) |  | 0 | Despite being the seventh most widely spoken language in the world, Bengali has received much less attention in machine translation literature due to being low in resources. Most publicly available parallel corpora for Bengali are not large enough; and have rather poor quality, mostly because of... | Tahmid Hasan, Abhik Bhattacharjee, Kazi Samin, Masum Hasan, Madhusudan Basak, M. Sohel Rahman, Rifat Shahriyar |  |
| 692 |  |  [CSP: Code-Switching Pre-training for Neural Machine Translation](https://doi.org/10.18653/v1/2020.emnlp-main.208) |  | 0 | This paper proposes a new pre-training method, called Code-Switching Pre-training (CSP for short) for Neural Machine Translation (NMT). Unlike traditional pre-training method which randomly masks some fragments of the input sentence, the proposed CSP randomly replaces some words in the source... | Zhen Yang, Bojie Hu, Ambyera Han, Shen Huang, Qi Ju |  |
| 693 |  |  [Type B Reflexivization as an Unambiguous Testbed for Multilingual Multi-Task Gender Bias](https://doi.org/10.18653/v1/2020.emnlp-main.209) |  | 0 | The one-sided focus on English in previous studies of gender bias in NLP misses out on opportunities in other languages: English challenge datasets such as GAP and WinoGender highlight model preferences that are “hallucinatory”, e.g., disambiguating gender-ambiguous occurrences of ‘doctor’ as male... | Ana Valeria GonzálezGarduño, Maria Barrett, Rasmus Hvingelby, Kellie Webster, Anders Søgaard |  |
| 694 |  |  [Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information](https://doi.org/10.18653/v1/2020.emnlp-main.210) |  | 0 | We investigate the following question for machine translation (MT): can we develop a single universal MT model to serve as the common seed and obtain derivative and improved models on arbitrary language pairs? We propose mRASP, an approach to pre-train a universal multilingual neural machine... | Zehui Lin, Xiao Pan, Mingxuan Wang, Xipeng Qiu, Jiangtao Feng, Hao Zhou, Lei Li |  |
| 695 |  |  [Losing Heads in the Lottery: Pruning Transformer Attention in Neural Machine Translation](https://doi.org/10.18653/v1/2020.emnlp-main.211) |  | 0 | The attention mechanism is the crucial component of the transformer architecture. Recent research shows that most attention heads are not confident in their decisions and can be pruned. However, removing them before training a model results in lower quality. In this paper, we apply the lottery... | Maximiliana Behnke, Kenneth Heafield |  |
| 696 |  |  [Towards Enhancing Faithfulness for Neural Machine Translation](https://doi.org/10.18653/v1/2020.emnlp-main.212) |  | 0 | Neural machine translation (NMT) has achieved great success due to the ability to generate high-quality sentences. Compared with human translations, one of the drawbacks of current NMT is that translations are not usually faithful to the input, e.g., omitting information or generating unrelated... | Rongxiang Weng, Heng Yu, Xiangpeng Wei, Weihua Luo |  |
| 697 |  |  [COMET: A Neural Framework for MT Evaluation](https://doi.org/10.18653/v1/2020.emnlp-main.213) |  | 0 | We present COMET, a neural framework for training multilingual machine translation evaluation models which obtains new state-of-the-art levels of correlation with human judgements. Our framework leverages recent breakthroughs in cross-lingual pretrained language modeling resulting in highly... | Ricardo Rei, Craig Stewart, Ana C. Farinha, Alon Lavie |  |
| 698 |  |  [Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT](https://doi.org/10.18653/v1/2020.emnlp-main.214) |  | 0 | Using a language model (LM) pretrained on two languages with large monolingual data in order to initialize an unsupervised neural machine translation (UNMT) system yields state-of-the-art results. When limited data is available for one language, however, this method leads to poor translations. We... | Alexandra Chronopoulou, Dario Stojanovski, Alexander M. Fraser |  |
| 699 |  |  [LNMap: Departures from Isomorphic Assumption in Bilingual Lexicon Induction Through Non-Linear Mapping in Latent Space](https://doi.org/10.18653/v1/2020.emnlp-main.215) |  | 0 | Most of the successful and predominant methods for Bilingual Lexicon Induction (BLI) are mapping-based, where a linear mapping function is learned with the assumption that the word embedding spaces of different languages exhibit similar geometric structures (i.e. approximately isomorphic). However,... | Tasnim Mohiuddin, M. Saiful Bari, Shafiq Rayhan Joty |  |
| 700 |  |  [Uncertainty-Aware Semantic Augmentation for Neural Machine Translation](https://doi.org/10.18653/v1/2020.emnlp-main.216) |  | 0 | As a sequence-to-sequence generation task, neural machine translation (NMT) naturally contains intrinsic uncertainty, where a single sentence in one language has multiple valid counterparts in the other. However, the dominant methods for NMT only observe one of them from the parallel corpora for the... | Xiangpeng Wei, Heng Yu, Yue Hu, Rongxiang Weng, Luxi Xing, Weihua Luo |  |
| 701 |  |  [Can Automatic Post-Editing Improve NMT?](https://doi.org/10.18653/v1/2020.emnlp-main.217) |  | 0 | Automatic post-editing (APE) aims to improve machine translations, thereby reducing human post-editing effort. APE has had notable success when used with statistical machine translation (SMT) systems but has not been as successful over neural machine translation (NMT) systems. This has raised... | Shamil Chollampatt, Raymond Hendy Susanto, Liling Tan, Ewa Szymanska |  |
| 702 |  |  [Parsing Gapping Constructions Based on Grammatical and Semantic Roles](https://doi.org/10.18653/v1/2020.emnlp-main.218) |  | 0 | A gapping construction consists of a coordinated structure where redundant elements are elided from all but one conjuncts. This paper proposes a method of parsing sentences with gapping to recover elided elements. The proposed method is based on constituent trees annotated with grammatical and... | Yoshihide Kato, Shigeki Matsubara |  |
| 703 |  |  [Span-based discontinuous constituency parsing: a family of exact chart-based algorithms with time complexities from O(n\^6) down to O(n\^3)](https://doi.org/10.18653/v1/2020.emnlp-main.219) |  | 0 | We introduce a novel chart-based algorithm for span-based parsing of discontinuous constituency trees of block degree two, including ill-nested structures. In particular, we show that we can build variants of our parser with smaller search spaces and time complexities ranging from O(nˆ6) down to... | Caio Corro |  |
| 704 |  |  [Some Languages Seem Easier to Parse Because Their Treebanks Leak](https://doi.org/10.18653/v1/2020.emnlp-main.220) |  | 0 | Cross-language differences in (universal) dependency parsing performance are mostly attributed to treebank size, average sentence length, average dependency length, morphological complexity, and domain differences. We point at a factor not previously discussed: If we abstract away from words and... | Anders Søgaard |  |
| 705 |  |  [Discontinuous Constituent Parsing as Sequence Labeling](https://doi.org/10.18653/v1/2020.emnlp-main.221) |  | 0 | This paper reduces discontinuous parsing to sequence labeling. It first shows that existing reductions for constituent parsing as labeling do not support discontinuities. Second, it fills this gap and proposes to encode tree discontinuities as nearly ordered permutations of the input sequence.... | David Vilares, Carlos GómezRodríguez |  |
| 706 |  |  [Modularized Syntactic Neural Networks for Sentence Classification](https://doi.org/10.18653/v1/2020.emnlp-main.222) |  | 0 | This paper focuses on tree-based modeling for the sentence classification task. In existing works, aggregating on a syntax tree usually considers local information of sub-trees. In contrast, in addition to the local information, our proposed Modularized Syntactic Neural Network (MSNN) utilizes the... | Haiyan Wu, Ying Liu, Shaoyun Shi |  |
| 707 |  |  [TED-CDB: A Large-Scale Chinese Discourse Relation Dataset on TED Talks](https://doi.org/10.18653/v1/2020.emnlp-main.223) |  | 0 | As different genres are known to differ in their communicative properties and as previously, for Chinese, discourse relations have only been annotated over news text, we have created the TED-CDB dataset. TED-CDB comprises a large set of TED talks in Chinese that have been manually annotated... | Wanqiu Long, Bonnie Webber, Deyi Xiong |  |
| 708 |  |  [QADiscourse - Discourse Relations as QA Pairs: Representation, Crowdsourcing and Baselines](https://doi.org/10.18653/v1/2020.emnlp-main.224) |  | 0 | Discourse relations describe how two propositions relate to one another, and identifying them automatically is an integral part of natural language understanding. However, annotating discourse relations typically requires expert annotators. Recently, different semantic aspects of a sentence have... | Valentina Pyatkin, Ayal Klein, Reut Tsarfaty, Ido Dagan |  |
| 709 |  |  [Discourse Self-Attention for Discourse Element Identification in Argumentative Student Essays](https://doi.org/10.18653/v1/2020.emnlp-main.225) |  | 0 | This paper proposes to adapt self-attention to discourse level for modeling discourse elements in argumentative student essays. Specifically, we focus on two issues. First, we propose structural sentence positional encodings to explicitly represent sentence positions. Second, we propose to use... | Wei Song, Ziyao Song, Ruiji Fu, Lizhen Liu, Miaomiao Cheng, Ting Liu |  |
| 710 |  |  [MEGATRON-CNTRL: Controllable Story Generation with External Knowledge Using Large-Scale Language Models](https://doi.org/10.18653/v1/2020.emnlp-main.226) |  | 0 | Existing pre-trained large language models have shown unparalleled generative capabilities. However, they are not controllable. In this paper, we propose MEGATRON-CNTRL, a novel framework that uses large-scale language models and adds control to text generation by incorporating an external knowledge... | Peng Xu, Mostofa Patwary, Mohammad Shoeybi, Raul Puri, Pascale Fung, Anima Anandkumar, Bryan Catanzaro |  |
| 711 |  |  [Incomplete Utterance Rewriting as Semantic Segmentation](https://doi.org/10.18653/v1/2020.emnlp-main.227) |  | 0 | Recent years the task of incomplete utterance rewriting has raised a large attention. Previous works usually shape it as a machine translation task and employ sequence to sequence based architecture with copy mechanism. In this paper, we present a novel and extensive approach, which formulates it as... | Qian Liu, Bei Chen, JianGuang Lou, Bin Zhou, Dongmei Zhang |  |
| 712 |  |  [Improving Grammatical Error Correction Models with Purpose-Built Adversarial Examples](https://doi.org/10.18653/v1/2020.emnlp-main.228) |  | 0 | A sequence-to-sequence (seq2seq) learning with neural networks empirically shows to be an effective framework for grammatical error correction (GEC), which takes a sentence with errors as input and outputs the corrected one. However, the performance of GEC models with the seq2seq framework heavily... | Lihao Wang, Xiaoqing Zheng |  |
| 713 |  |  [Homophonic Pun Generation with Lexically Constrained Rewriting](https://doi.org/10.18653/v1/2020.emnlp-main.229) |  | 0 | Punning is a creative way to make conversation enjoyable and literary writing elegant. In this paper, we focus on the task of generating a pun sentence given a pair of homophones. We first find the constraint words supporting the semantic incongruity for a sentence. Then we rewrite the sentence with... | Zhiwei Yu, Hongyu Zang, Xiaojun Wan |  |
| 714 |  |  [How to Make Neural Natural Language Generation as Reliable as Templates in Task-Oriented Dialogue](https://doi.org/10.18653/v1/2020.emnlp-main.230) |  | 0 | Neural Natural Language Generation (NLG) systems are well known for their unreliability. To overcome this issue, we propose a data augmentation approach which allows us to restrict the output of a network and guarantee reliability. While this restriction means generation will be less diverse than if... | Henry Elder, Alexander O'Connor, Jennifer Foster |  |
| 715 |  |  [Multilingual AMR-to-Text Generation](https://doi.org/10.18653/v1/2020.emnlp-main.231) |  | 0 | Generating text from structured data is challenging because it requires bridging the gap between (i) structure and natural language (NL) and (ii) semantically underspecified input and fully specified NL output. Multilingual generation brings in an additional challenge: that of generating into... | Angela Fan, Claire Gardent |  |
| 716 |  |  [Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation](https://doi.org/10.18653/v1/2020.emnlp-main.232) |  | 0 | Bolukbasi et al. (2016) presents one of the first gender bias mitigation techniques for word embeddings. Their method takes pre-trained word embeddings as input and attempts to isolate a linear subspace that captures most of the gender bias in the embeddings. As judged by an analogical evaluation... | Francisco Vargas, Ryan Cotterell |  |
| 717 |  |  [Lifelong Language Knowledge Distillation](https://doi.org/10.18653/v1/2020.emnlp-main.233) |  | 0 | It is challenging to perform lifelong language learning (LLL) on a stream of different tasks without any performance degradation comparing to the multi-task counterparts. To address this issue, we present Lifelong Language Knowledge Distillation (L2KD), a simple but efficient method that can be... | YungSung Chuang, ShangYu Su, YunNung Chen |  |
| 718 |  |  [Sparse Parallel Training of Hierarchical Dirichlet Process Topic Models](https://doi.org/10.18653/v1/2020.emnlp-main.234) |  | 0 | To scale non-parametric extensions of probabilistic topic models such as Latent Dirichlet allocation to larger data sets, practitioners rely increasingly on parallel and distributed systems. In this work, we study data-parallel training for the hierarchical Dirichlet process (HDP) topic model. Based... | Alexander Terenin, Måns Magnusson, Leif Jonsson |  |
| 719 |  |  [Multi-label Few/Zero-shot Learning with Knowledge Aggregated from Multiple Label Graphs](https://doi.org/10.18653/v1/2020.emnlp-main.235) |  | 0 | Few/Zero-shot learning is a big challenge of many classifications tasks, where a classifier is required to recognise instances of classes that have very few or even no training samples. It becomes more difficult in multi-label classification, where each instance is labelled with more than one class.... | Jueqing Lu, Lan Du, Ming Liu, Joanna Dipnall |  |
| 720 |  |  [Word Rotator's Distance](https://doi.org/10.18653/v1/2020.emnlp-main.236) |  | 0 | One key principle for assessing textual similarity is measuring the degree of semantic overlap between texts by considering the word alignment. Such alignment-based approaches are both intuitive and interpretable; however, they are empirically inferior to the simple cosine similarity between... | Sho Yokoi, Ryo Takahashi, Reina Akama, Jun Suzuki, Kentaro Inui |  |
| 721 |  |  [Disentangle-based Continual Graph Representation Learning](https://doi.org/10.18653/v1/2020.emnlp-main.237) |  | 0 | Graph embedding (GE) methods embed nodes (and/or edges) in graph into a low-dimensional semantic space, and have shown its effectiveness in modeling multi-relational data. However, existing GE models are not practical in real-world applications since it overlooked the streaming nature of incoming... | Xiaoyu Kou, Yankai Lin, Shaobo Liu, Peng Li, Jie Zhou, Yan Zhang |  |
| 722 |  |  [Semi-Supervised Bilingual Lexicon Induction with Two-way Interaction](https://doi.org/10.18653/v1/2020.emnlp-main.238) |  | 0 | Semi-supervision is a promising paradigm for Bilingual Lexicon Induction (BLI) with limited annotations. However, previous semisupervised methods do not fully utilize the knowledge hidden in annotated and nonannotated data, which hinders further improvement of their performance. In this paper, we... | Xu Zhao, Zihao Wang, Hao Wu, Yong Zhang |  |
| 723 |  |  [Wasserstein Distance Regularized Sequence Representation for Text Matching in Asymmetrical Domains](https://doi.org/10.18653/v1/2020.emnlp-main.239) |  | 0 | One approach to matching texts from asymmetrical domains is projecting the input sequences into a common semantic space as feature vectors upon which the matching function can be readily defined and learned. In real-world matching practices, it is often observed that with the training goes on, the... | Weijie Yu, Chen Xu, Jun Xu, Liang Pang, Xiaopeng Gao, Xiaozhao Wang, JiRong Wen |  |
| 724 |  |  [A Simple Approach to Learning Unsupervised Multilingual Embeddings](https://doi.org/10.18653/v1/2020.emnlp-main.240) |  | 0 | Recent progress on unsupervised cross-lingual embeddings in the bilingual setting has given the impetus to learning a shared embedding space for several languages. A popular framework to solve the latter problem is to solve the following two sub-problems jointly: 1) learning unsupervised word... | Pratik Jawanpuria, Mayank Meghwanshi, Bamdev Mishra |  |
| 725 |  |  [Bootstrapped Q-learning with Context Relevant Observation Pruning to Generalize in Text-based Games](https://doi.org/10.18653/v1/2020.emnlp-main.241) |  | 0 | We show that Reinforcement Learning (RL) methods for solving Text-Based Games (TBGs) often fail to generalize on unseen games, especially in small data regimes. To address this issue, we propose Context Relevant Episodic State Truncation (CREST) for irrelevant token removal in observation text for... | Subhajit Chaudhury, Daiki Kimura, Kartik Talamadupula, Michiaki Tatsubori, Asim Munawar, Ryuki Tachibana |  |
| 726 |  |  [BERT-EMD: Many-to-Many Layer Mapping for BERT Compression with Earth Mover's Distance](https://doi.org/10.18653/v1/2020.emnlp-main.242) |  | 0 | Pre-trained language models (e.g., BERT) have achieved significant success in various natural language processing (NLP) tasks. However, high storage and computational costs obstruct pre-trained language models to be effectively deployed on resource-constrained devices. In this paper, we propose a... | Jianquan Li, Xiaokang Liu, Honghong Zhao, Ruifeng Xu, Min Yang, Yaohong Jin |  |
| 727 |  |  [Slot Attention with Value Normalization for Multi-Domain Dialogue State Tracking](https://doi.org/10.18653/v1/2020.emnlp-main.243) |  | 0 | Incompleteness of domain ontology and unavailability of some values are two inevitable problems of dialogue state tracking (DST). Existing approaches generally fall into two extremes: choosing models without ontology or embedding ontology in models leading to over-dependence. In this paper, we... | Yexiang Wang, Yi Guo, Siqi Zhu |  |
| 728 |  |  [Don't Read Too Much Into It: Adaptive Computation for Open-Domain Question Answering](https://doi.org/10.18653/v1/2020.emnlp-main.244) |  | 0 | Most approaches to Open-Domain Question Answering consist of a light-weight retriever that selects a set of candidate passages, and a computationally expensive reader that examines the passages to identify the correct answer. Previous works have shown that as the number of retrieved passages... | Yuxiang Wu, Sebastian Riedel, Pasquale Minervini, Pontus Stenetorp |  |
| 729 |  |  [Multi-Step Inference for Reasoning Over Paragraphs](https://doi.org/10.18653/v1/2020.emnlp-main.245) |  | 0 | Complex reasoning over text requires understanding and chaining together free-form predicates and logical connectives. Prior work has largely tried to do this either symbolically or with black-box transformers. We present a middle ground between these two extremes: a compositional model reminiscent... | Jiangming Liu, Matt Gardner, Shay B. Cohen, Mirella Lapata |  |
| 730 |  |  [Learning a Cost-Effective Annotation Policy for Question Answering](https://doi.org/10.18653/v1/2020.emnlp-main.246) |  | 0 | State-of-the-art question answering (QA) relies upon large amounts of training data for which labeling is time consuming and thus expensive. For this reason, customizing QA systems is challenging. As a remedy, we propose a novel framework for annotating QA datasets that entails learning a... | Bernhard Kratzwald, Stefan Feuerriegel, Huan Sun |  |
| 731 |  |  [Scene Restoring for Narrative Machine Reading Comprehension](https://doi.org/10.18653/v1/2020.emnlp-main.247) |  | 0 | This paper focuses on machine reading comprehension for narrative passages. Narrative passages usually describe a chain of events. When reading this kind of passage, humans tend to restore a scene according to the text with their prior knowledge, which helps them understand the passage... | Zhixing Tian, Yuanzhe Zhang, Kang Liu, Jun Zhao, Yantao Jia, Zhicheng Sheng |  |
| 732 |  |  [A Simple and Effective Model for Answering Multi-span Questions](https://doi.org/10.18653/v1/2020.emnlp-main.248) |  | 0 | Models for reading comprehension (RC) commonly restrict their output space to the set of all single contiguous spans from the input, in order to alleviate the learning problem and avoid the need for a model that generates text explicitly. However, forcing an answer to be a single span can be... | Elad Segal, Avia Efrat, Mor Shoham, Amir Globerson, Jonathan Berant |  |
| 733 |  |  [Top-Rank-Focused Adaptive Vote Collection for the Evaluation of Domain-Specific Semantic Models](https://doi.org/10.18653/v1/2020.emnlp-main.249) |  | 0 | The growth of domain-specific applications of semantic models, boosted by the recent achievements of unsupervised embedding learning algorithms, demands domain-specific evaluation datasets. In many cases, content-based recommenders being a prime example, these models are required to rank words or... | Pierangelo Lombardo, Alessio Boiardi, Luca Colombo, Angelo Schiavone, Nicolò Tamagnone |  |
| 734 |  |  [Meta Fine-Tuning Neural Language Models for Multi-Domain Text Mining](https://doi.org/10.18653/v1/2020.emnlp-main.250) |  | 0 | Pre-trained neural language models bring significant improvement for various NLP tasks, by fine-tuning the models on task-specific training sets. During fine-tuning, the parameters are initialized from pre-trained models directly, which ignores how the learning process of similar NLP tasks in... | Chengyu Wang, Minghui Qiu, Jun Huang, Xiaofeng He |  |
| 735 |  |  [Incorporating Behavioral Hypotheses for Query Generation](https://doi.org/10.18653/v1/2020.emnlp-main.251) |  | 0 | Generative neural networks have been shown effective on query suggestion. Commonly posed as a conditional generation problem, the task aims to leverage earlier inputs from users in a search session to predict queries that they will likely issue at a later time. User inputs come in various forms such... | RueyCheng Chen, ChiaJung Lee |  |
| 736 |  |  [Conditional Causal Relationships between Emotions and Causes in Texts](https://doi.org/10.18653/v1/2020.emnlp-main.252) |  | 0 | The causal relationships between emotions and causes in text have recently received a lot of attention. Most of the existing works focus on the extraction of the causally related clauses from documents. However, none of these works has considered the possibility that the causal relationships among... | Xinhong Chen, Qing Li, Jianping Wang |  |
| 737 |  |  [COMETA: A Corpus for Medical Entity Linking in the Social Media](https://doi.org/10.18653/v1/2020.emnlp-main.253) |  | 0 | Whilst there has been growing progress in Entity Linking (EL) for general language, existing datasets fail to address the complex nature of health terminology in layman’s language. Meanwhile, there is a growing need for applications that can understand the public’s voice in the health domain. To... | Marco Basaldella, Fangyu Liu, Ehsan Shareghi, Nigel Collier |  |
| 738 |  |  [Pareto Probing: Trading Off Accuracy for Complexity](https://doi.org/10.18653/v1/2020.emnlp-main.254) |  | 0 | The question of how to probe contextual word representations in a way that is principled and useful has seen significant recent attention. In our contribution to this discussion, we argue, first, for a probe metric that reflects the trade-off between probe complexity and performance: the Pareto... | Tiago Pimentel, Naomi Saphra, Adina Williams, Ryan Cotterell |  |
| 739 |  |  [Interpretation of NLP models through input marginalization](https://doi.org/10.18653/v1/2020.emnlp-main.255) |  | 0 | To demystify the “black box” property of deep neural networks for natural language processing (NLP), several methods have been proposed to interpret their predictions by measuring the change in prediction probability after erasing each token of an input. Since existing methods replace each token... | Siwon Kim, Jihun Yi, Eunji Kim, Sungroh Yoon |  |
| 740 |  |  [Generating Label Cohesive and Well-Formed Adversarial Claims](https://doi.org/10.18653/v1/2020.emnlp-main.256) |  | 0 | Adversarial attacks reveal important vulnerabilities and flaws of trained models. One potent type of attack are universal adversarial triggers, which are individual n-grams that, when appended to instances of a class under attack, can trick a model into predicting a target class. However, for... | Pepa Atanasova, Dustin Wright, Isabelle Augenstein |  |
| 741 |  |  [Are All Good Word Vector Spaces Isomorphic?](https://doi.org/10.18653/v1/2020.emnlp-main.257) |  | 0 | Existing algorithms for aligning cross-lingual word vector spaces assume that vector spaces are approximately isomorphic. As a result, they perform poorly or fail completely on non-isomorphic spaces. Such non-isomorphism has been hypothesised to result from typological differences between languages.... | Ivan Vulic, Sebastian Ruder, Anders Søgaard |  |
| 742 |  |  [Cold-Start and Interpretability: Turning Regular Expressions into Trainable Recurrent Neural Networks](https://doi.org/10.18653/v1/2020.emnlp-main.258) |  | 0 | Neural networks can achieve impressive performance on many natural language processing applications, but they typically need large labeled data for training and are not easily interpretable. On the other hand, symbolic rules such as regular expressions are interpretable, require no training, and... | Chengyue Jiang, Yinggong Zhao, Shanbo Chu, Libin Shen, Kewei Tu |  |
| 743 |  |  [When BERT Plays the Lottery, All Tickets Are Winning](https://doi.org/10.18653/v1/2020.emnlp-main.259) |  | 0 | Large Transformer-based models were shown to be reducible to a smaller number of self-attention heads and layers. We consider this phenomenon from the perspective of the lottery ticket hypothesis, using both structured and magnitude pruning. For fine-tuned BERT, we show that (a) it is possible to... | Sai Prasanna, Anna Rogers, Anna Rumshisky |  |
| 744 |  |  [On the weak link between importance and prunability of attention heads](https://doi.org/10.18653/v1/2020.emnlp-main.260) |  | 0 | Given the success of Transformer-based models, two directions of study have emerged: interpreting role of individual attention heads and down-sizing the models for efficiency. Our work straddles these two streams: We analyse the importance of basing pruning strategies on the interpreted role of the... | Aakriti Budhraja, Madhura Pande, Preksha Nema, Pratyush Kumar, Mitesh M. Khapra |  |
| 745 |  |  [Towards Interpreting BERT for Reading Comprehension Based QA](https://doi.org/10.18653/v1/2020.emnlp-main.261) |  | 0 | BERT and its variants have achieved state-of-the-art performance in various NLP tasks. Since then, various works have been proposed to analyze the linguistic information being captured in BERT. However, the current works do not provide an insight into how BERT is able to achieve near human-level... | Sahana Ramnath, Preksha Nema, Deep Sahni, Mitesh M. Khapra |  |
| 746 |  |  [How do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking](https://doi.org/10.18653/v1/2020.emnlp-main.262) |  | 0 | Attribution methods assess the contribution of inputs to the model prediction. One way to do so is erasure: a subset of inputs is considered irrelevant if it can be removed without affecting the prediction. Though conceptually simple, erasure’s objective is intractable and approximate search remains... | Nicola De Cao, Michael Sejr Schlichtkrull, Wilker Aziz, Ivan Titov |  |
| 747 |  |  [A Diagnostic Study of Explainability Techniques for Text Classification](https://doi.org/10.18653/v1/2020.emnlp-main.263) |  | 0 | Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity. Efforts to make the rationales behind the models’ predictions transparent have inspired an abundance of new explainability techniques. Provided with an... | Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, Isabelle Augenstein |  |
| 748 |  |  [STL-CQA: Structure-based Transformers with Localization and Encoding for Chart Question Answering](https://doi.org/10.18653/v1/2020.emnlp-main.264) |  | 0 | Chart Question Answering (CQA) is the task of answering natural language questions about visualisations in the chart image. Recent solutions, inspired by VQA approaches, rely on image-based attention for question/answering while ignoring the inherent chart structure. We propose STL-CQA which... | Hrituraj Singh, Sumit Shekhar |  |
| 749 |  |  [Learning to Contrast the Counterfactual Samples for Robust Visual Question Answering](https://doi.org/10.18653/v1/2020.emnlp-main.265) |  | 0 | In the task of Visual Question Answering (VQA), most state-of-the-art models tend to learn spurious correlations in the training set and achieve poor performance in out-of-distribution test data. Some methods of generating counterfactual samples have been proposed to alleviate this problem. However,... | Zujie Liang, Weitao Jiang, Haifeng Hu, Jiaying Zhu |  |
| 750 |  |  [Learning Physical Common Sense as Knowledge Graph Completion via BERT Data Augmentation and Constrained Tucker Factorization](https://doi.org/10.18653/v1/2020.emnlp-main.266) |  | 0 | Physical common sense plays an essential role in the cognition abilities of robots for human-robot interaction. Machine learning methods have shown promising results on physical commonsense learning in natural language processing but still suffer from model generalization. In this paper, we... | Zhenjie Zhao, Evangelos E. Papalexakis, Xiaojuan Ma |  |
| 751 |  |  [A Visually-grounded First-person Dialogue Dataset with Verbal and Non-verbal Responses](https://doi.org/10.18653/v1/2020.emnlp-main.267) |  | 0 | In real-world dialogue, first-person visual information about where the other speakers are and what they are paying attention to is crucial to understand their intentions. Non-verbal responses also play an important role in social interactions. In this paper, we propose a visually-grounded... | Hisashi Kamezawa, Noriki Nishida, Nobuyuki Shimizu, Takashi Miyazaki, Hideki Nakayama |  |
| 752 |  |  [Cross-Media Keyphrase Prediction: A Unified Framework with Multi-Modality Multi-Head Attention and Image Wordings](https://doi.org/10.18653/v1/2020.emnlp-main.268) |  | 0 | Social media produces large amounts of contents every day. To help users quickly capture what they need, keyphrase prediction is receiving a growing attention. Nevertheless, most prior efforts focus on text modeling, largely ignoring the rich features embedded in the matching images. In this work,... | Yue Wang, Jing Li, Michael R. Lyu, Irwin King |  |
| 753 |  |  [VD-BERT: A Unified Vision and Dialog Transformer with BERT](https://doi.org/10.18653/v1/2020.emnlp-main.269) |  | 0 | Visual dialog is a challenging vision-language task, where a dialog agent needs to answer a series of questions through reasoning on the image content and dialog history. Prior work has mostly focused on various attention mechanisms to model such intricate interactions. By contrast, in this work, we... | Yue Wang, Shafiq R. Joty, Michael R. Lyu, Irwin King, Caiming Xiong, Steven C. H. Hoi |  |
| 754 |  |  [The Grammar of Emergent Languages](https://doi.org/10.18653/v1/2020.emnlp-main.270) |  | 0 | In this paper, we consider the syntactic properties of languages emerged in referential games, using unsupervised grammar induction (UGI) techniques originally designed to analyse natural language. We show that the considered UGI techniques are appropriate to analyse emergent languages and we then... | Oskar van der Wal, Silvan de Boer, Elia Bruni, Dieuwke Hupkes |  |
| 755 |  |  [Sub-Instruction Aware Vision-and-Language Navigation](https://doi.org/10.18653/v1/2020.emnlp-main.271) |  | 0 | Vision-and-language navigation requires an agent to navigate through a real 3D environment following natural language instructions. Despite significant advances, few previous works are able to fully utilize the strong correspondence between the visual and textual sequences. Meanwhile, due to the... | Yicong Hong, Cristian Rodriguez Opazo, Qi Wu, Stephen Gould |  |
| 756 |  |  [Knowledge-Grounded Dialogue Generation with Pre-trained Language Models](https://doi.org/10.18653/v1/2020.emnlp-main.272) |  | 0 | We study knowledge-grounded dialogue generation with pre-trained language models. To leverage the redundant external knowledge under capacity constraint, we propose equipping response generation defined by a pre-trained language model with a knowledge selection module, and an unsupervised approach... | Xueliang Zhao, Wei Wu, Can Xu, Chongyang Tao, Dongyan Zhao, Rui Yan |  |
| 757 |  |  [MinTL: Minimalist Transfer Learning for Task-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2020.emnlp-main.273) |  | 0 | In this paper, we propose Minimalist Transfer Learning (MinTL) to simplify the system design process of task-oriented dialogue systems and alleviate the over-dependency on annotated data. MinTL is a simple yet effective transfer learning framework, which allows us to plug-and-play pre-trained... | Zhaojiang Lin, Andrea Madotto, Genta Indra Winata, Pascale Fung |  |
| 758 |  |  [Variational Hierarchical Dialog Autoencoder for Dialog State Tracking Data Augmentation](https://doi.org/10.18653/v1/2020.emnlp-main.274) |  | 0 | Recent works have shown that generative data augmentation, where synthetic samples generated from deep generative models complement the training dataset, benefit NLP tasks. In this work, we extend this approach to the task of dialog state tracking for goaloriented dialogs. Due to the inherent... | Kang Min Yoo, Hanbit Lee, Franck Dernoncourt, Trung Bui, Walter Chang, Sanggoo Lee |  |
| 759 |  |  [Bridging the Gap between Prior and Posterior Knowledge Selection for Knowledge-Grounded Dialogue Generation](https://doi.org/10.18653/v1/2020.emnlp-main.275) |  | 0 | Knowledge selection plays an important role in knowledge-grounded dialogue, which is a challenging task to generate more informative responses by leveraging external knowledge. Recently, latent variable models have been proposed to deal with the diversity of knowledge selection by using both prior... | Xiuyi Chen, Fandong Meng, Peng Li, Feilong Chen, Shuang Xu, Bo Xu, Jie Zhou |  |
| 760 |  |  [Counterfactual Off-Policy Training for Neural Dialogue Generation](https://doi.org/10.18653/v1/2020.emnlp-main.276) |  | 0 | Open-domain dialogue generation suffers from the data insufficiency problem due to the vast size of potential responses. In this paper, we propose to explore potential responses by counterfactual reasoning. Given an observed response, the counterfactual reasoning model automatically infers the... | Qingfu Zhu, WeiNan Zhang, Ting Liu, William Yang Wang |  |
| 761 |  |  [Dialogue Distillation: Open-Domain Dialogue Augmentation Using Unpaired Data](https://doi.org/10.18653/v1/2020.emnlp-main.277) |  | 0 | Recent advances in open-domain dialogue systems rely on the success of neural models that are trained on large-scale data. However, collecting large-scale dialogue data is usually time-consuming and labor-intensive. To address this data dilemma, we propose a novel data augmentation method for... | Rongsheng Zhang, Yinhe Zheng, Jianzhi Shao, Xiaoxi Mao, Yadong Xi, Minlie Huang |  |
| 762 |  |  [Task-Completion Dialogue Policy Learning via Monte Carlo Tree Search with Dueling Network](https://doi.org/10.18653/v1/2020.emnlp-main.278) |  | 0 | We introduce a framework of Monte Carlo Tree Search with Double-q Dueling network (MCTS-DDU) for task-completion dialogue policy learning. Different from the previous deep model-based reinforcement learning methods, which uses background planning and may suffer from low-quality simulated... | Sihan Wang, Kaijie Zhou, Kunfeng Lai, Jianping Shen |  |
| 763 |  |  [Learning a Simple and Effective Model for Multi-turn Response Generation with Auxiliary Tasks](https://doi.org/10.18653/v1/2020.emnlp-main.279) |  | 0 | We study multi-turn response generation for open-domain dialogues. The existing state-of-the-art addresses the problem with deep neural architectures. While these models improved response quality, their complexity also hinders the application of the models in real systems. In this work, we pursue a... | Yufan Zhao, Can Xu, Wei Wu |  |
| 764 |  |  [AttnIO: Knowledge Graph Exploration with In-and-Out Attention Flow for Knowledge-Grounded Dialogue](https://doi.org/10.18653/v1/2020.emnlp-main.280) |  | 0 | Retrieving the proper knowledge relevant to conversational context is an important challenge in dialogue systems, to engage users with more informative response. Several recent works propose to formulate this knowledge selection problem as a path traversal over an external knowledge graph (KG), but... | Jaehun Jung, Bokyung Son, Sungwon Lyu |  |
| 765 |  |  [Amalgamating Knowledge from Two Teachers for Task-oriented Dialogue System with Adversarial Training](https://doi.org/10.18653/v1/2020.emnlp-main.281) |  | 0 | The challenge of both achieving task completion by querying the knowledge base and generating human-like responses for task-oriented dialogue systems is attracting increasing research attention. In this paper, we propose a “Two-Teacher One-Student” learning framework (TTOS) for task-oriented... | Wanwei He, Min Yang, Rui Yan, Chengming Li, Ying Shen, Ruifeng Xu |  |
| 766 |  |  [Task-oriented Domain-specific Meta-Embedding for Text Classification](https://doi.org/10.18653/v1/2020.emnlp-main.282) |  | 0 | Meta-embedding learning, which combines complementary information in different word embeddings, have shown superior performances across different Natural Language Processing tasks. However, domain-specific knowledge is still ignored by existing meta-embedding methods, which results in unstable... | Xin Wu, Yi Cai, Kai Yang, Tao Wang, Qing Li |  |
| 767 |  |  [Don't Neglect the Obvious: On the Role of Unambiguous Words in Word Sense Disambiguation](https://doi.org/10.18653/v1/2020.emnlp-main.283) |  | 0 | State-of-the-art methods for Word Sense Disambiguation (WSD) combine two different features: the power of pre-trained language models and a propagation method to extend the coverage of such models. This propagation is needed as current sense-annotated corpora lack coverage of many instances in the... | Daniel Loureiro, José CamachoCollados |  |
| 768 |  |  [Within-Between Lexical Relation Classification](https://doi.org/10.18653/v1/2020.emnlp-main.284) |  | 0 | We propose the novel Within-Between Relation model for recognizing lexical-semantic relations between words. Our model integrates relational and distributional signals, forming an effective sub-space representation for each relation. We show that the proposed model is competitive and outperforms... | Oren Barkan, Avi Caciularu, Ido Dagan |  |
| 769 |  |  [With More Contexts Comes Better Performance: Contextualized Sense Embeddings for All-Round Word Sense Disambiguation](https://doi.org/10.18653/v1/2020.emnlp-main.285) |  | 0 | Contextualized word embeddings have been employed effectively across several tasks in Natural Language Processing, as they have proved to carry useful semantic information. However, it is still hard to link them to structured sources of knowledge. In this paper we present ARES (context-AwaRe... | Bianca Scarlini, Tommaso Pasini, Roberto Navigli |  |
| 770 |  |  [Convolution over Hierarchical Syntactic and Lexical Graphs for Aspect Level Sentiment Analysis](https://doi.org/10.18653/v1/2020.emnlp-main.286) |  | 0 | The state-of-the-art methods in aspect-level sentiment classification have leveraged the graph based models to incorporate the syntactic structure of a sentence. While being effective, these methods ignore the corpus level word co-occurrence information, which reflect the collocations in linguistics... | Mi Zhang, Tieyun Qian |  |
| 771 |  |  [Multi-Instance Multi-Label Learning Networks for Aspect-Category Sentiment Analysis](https://doi.org/10.18653/v1/2020.emnlp-main.287) |  | 0 | Aspect-category sentiment analysis (ACSA) aims to predict sentiment polarities of sentences with respect to given aspect categories. To detect the sentiment toward a particular aspect category in a sentence, most previous methods first generate an aspect category-specific sentence representation for... | Yuncong Li, Cunxiang Yin, Shenghua Zhong, Xu Pan |  |
| 772 |  |  [Aspect Sentiment Classification with Aspect-Specific Opinion Spans](https://doi.org/10.18653/v1/2020.emnlp-main.288) |  | 0 | Aspect based sentiment analysis, predicting sentiment polarity of given aspects, has drawn extensive attention. Previous attention-based models emphasize using aspect semantics to help extract opinion features for classification. However, these works are either not able to capture opinion spans as a... | Lu Xu, Lidong Bing, Wei Lu, Fei Huang |  |
| 773 |  |  [Emotion-Cause Pair Extraction as Sequence Labeling Based on A Novel Tagging Scheme](https://doi.org/10.18653/v1/2020.emnlp-main.289) |  | 0 | The task of emotion-cause pair extraction deals with finding all emotions and the corresponding causes in unannotated emotion texts. Most recent studies are based on the likelihood of Cartesian product among all clause candidates, resulting in a high computational cost. Targeting this issue, we... | Chaofa Yuan, Chuang Fan, Jianzhu Bao, Ruifeng Xu |  |
| 774 |  |  [End-to-End Emotion-Cause Pair Extraction based on Sliding Window Multi-Label Learning](https://doi.org/10.18653/v1/2020.emnlp-main.290) |  | 0 | Emotion-cause pair extraction (ECPE) is a new task that aims to extract the potential pairs of emotions and their corresponding causes in a document. The existing methods first perform emotion extraction and cause extraction independently, and then perform emotion-cause pairing and filtering.... | Zixiang Ding, Rui Xia, Jianfei Yu |  |
| 775 |  |  [Multi-modal Multi-label Emotion Detection with Modality and Label Dependence](https://doi.org/10.18653/v1/2020.emnlp-main.291) |  | 0 | As an important research issue in the natural language processing community, multi-label emotion detection has been drawing more and more attention in the last few years. However, almost all existing studies focus on one modality (e.g., textual modality). In this paper, we focus on multi-label... | Dong Zhang, Xincheng Ju, Junhui Li, Shoushan Li, Qiaoming Zhu, Guodong Zhou |  |
| 776 |  |  [Tasty Burgers, Soggy Fries: Probing Aspect Robustness in Aspect-Based Sentiment Analysis](https://doi.org/10.18653/v1/2020.emnlp-main.292) |  | 0 | Aspect-based sentiment analysis (ABSA) aims to predict the sentiment towards a specific aspect in the text. However, existing ABSA test sets cannot be used to probe whether a model can distinguish the sentiment of the target aspect from the non-target aspects. To solve this problem, we develop a... | Xiaoyu Xing, Zhijing Jin, Di Jin, Bingning Wang, Qi Zhang, Xuanjing Huang |  |
| 777 |  |  [Modeling Content Importance for Summarization with Pre-trained Language Models](https://doi.org/10.18653/v1/2020.emnlp-main.293) |  | 0 | Modeling content importance is an essential yet challenging task for summarization. Previous work is mostly based on statistical methods that estimate word-level salience, which does not consider semantics and larger context when quantifying importance. It is thus hard for these methods to... | Liqiang Xiao, Lu Wang, Hao He, Yaohui Jin |  |
| 778 |  |  [Unsupervised Reference-Free Summary Quality Evaluation via Contrastive Learning](https://doi.org/10.18653/v1/2020.emnlp-main.294) |  | 0 | Evaluation of a document summarization system has been a critical factor to impact the success of the summarization task. Previous approaches, such as ROUGE, mainly consider the informativeness of the assessed summary and require human-generated references for each test summary. In this work, we... | Hanlu Wu, Tengfei Ma, Lingfei Wu, Tariro Manyumwa, Shouling Ji |  |
| 779 |  |  [Neural Extractive Summarization with Hierarchical Attentive Heterogeneous Graph Network](https://doi.org/10.18653/v1/2020.emnlp-main.295) |  | 0 | Sentence-level extractive text summarization is substantially a node classification task of network mining, adhering to the informative components and concise representations. There are lots of redundant phrases between extracted sentences, but it is difficult to model them exactly by the general... | Ruipeng Jia, Yanan Cao, Hengzhu Tang, Fang Fang, Cong Cao, Shi Wang |  |
| 780 |  |  [Coarse-to-Fine Query Focused Multi-Document Summarization](https://doi.org/10.18653/v1/2020.emnlp-main.296) |  | 0 | We consider the problem of better modeling query-cluster interactions to facilitate query focused multi-document summarization. Due to the lack of training data, existing work relies heavily on retrieval-style methods for assembling query relevant summaries. We propose a coarse-to-fine modeling... | Yumo Xu, Mirella Lapata |  |
| 781 |  |  [Pre-training for Abstractive Document Summarization by Reinstating Source Text](https://doi.org/10.18653/v1/2020.emnlp-main.297) |  | 0 | Abstractive document summarization is usually modeled as a sequence-to-sequence (SEQ2SEQ) learning problem. Unfortunately, training large SEQ2SEQ based summarization models on limited supervised summarization data is challenging. This paper presents three sequence-to-sequence pre-training (in... | Yanyan Zou, Xingxing Zhang, Wei Lu, Furu Wei, Ming Zhou |  |
| 782 |  |  [Learning from Context or Names? An Empirical Study on Neural Relation Extraction](https://doi.org/10.18653/v1/2020.emnlp-main.298) |  | 0 | Neural models have achieved remarkable success on relation extraction (RE) benchmarks. However, there is no clear understanding what information in text affects existing RE models to make decisions and how to further improve the performance of these models. To this end, we empirically study the... | Hao Peng, Tianyu Gao, Xu Han, Yankai Lin, Peng Li, Zhiyuan Liu, Maosong Sun, Jie Zhou |  |
| 783 |  |  [SelfORE: Self-supervised Relational Feature Learning for Open Relation Extraction](https://doi.org/10.18653/v1/2020.emnlp-main.299) |  | 0 | Open relation extraction is the task of extracting open-domain relation facts from natural language sentences. Existing works either utilize heuristics or distant-supervised annotations to train a supervised classifier over pre-defined relations, or adopt unsupervised methods with additional... | Xuming Hu, Lijie Wen, Yusong Xu, Chenwei Zhang, Philip S. Yu |  |
| 784 |  |  [Denoising Relation Extraction from Document-level Distant Supervision](https://doi.org/10.18653/v1/2020.emnlp-main.300) |  | 0 | Distant supervision (DS) has been widely adopted to generate auto-labeled data for sentence-level relation extraction (RE) and achieved great results. However, the existing success of DS cannot be directly transferred to more challenging document-level relation extraction (DocRE), as the inevitable... | Chaojun Xiao, Yuan Yao, Ruobing Xie, Xu Han, Zhiyuan Liu, Maosong Sun, Fen Lin, Leyu Lin |  |
| 785 |  |  [Let's Stop Incorrect Comparisons in End-to-end Relation Extraction!](https://doi.org/10.18653/v1/2020.emnlp-main.301) |  | 0 | Despite efforts to distinguish three different evaluation setups (Bekoulis et al., 2018), numerous end-to-end Relation Extraction (RE) articles present unreliable performance comparison to previous work. In this paper, we first identify several patterns of invalid comparisons in published papers and... | Bruno Taillé, Vincent Guigue, Geoffrey Scoutheeten, Patrick Gallinari |  |
| 786 |  |  [Exposing Shallow Heuristics of Relation Extraction Models with Challenge Data](https://doi.org/10.18653/v1/2020.emnlp-main.302) |  | 0 | The process of collecting and annotating training data may introduce distribution artifacts which may limit the ability of models to learn correct generalization behavior. We identify failure modes of SOTA relation extraction (RE) models trained on TACRED, which we attribute to limitations in the... | Shachar Rosenman, Alon Jacovi, Yoav Goldberg |  |
| 787 |  |  [Global-to-Local Neural Networks for Document-Level Relation Extraction](https://doi.org/10.18653/v1/2020.emnlp-main.303) |  | 0 | Relation extraction (RE) aims to identify the semantic relations between named entities in text. Recent years have witnessed it raised to the document level, which requires complex reasoning with entities and mentions throughout an entire document. In this paper, we propose a novel model to... | Difeng Wang, Wei Hu, Ermei Cao, Weijian Sun |  |
| 788 |  |  [Recurrent Interaction Network for Jointly Extracting Entities and Classifying Relations](https://doi.org/10.18653/v1/2020.emnlp-main.304) |  | 0 | The idea of using multi-task learning approaches to address the joint extraction of entity and relation is motivated by the relatedness between the entity recognition task and the relation classification task. Existing methods using multi-task learning techniques to address the problem learn... | Kai Sun, Richong Zhang, Samuel Mensah, Yongyi Mao, Xudong Liu |  |
| 789 |  |  [Temporal Knowledge Base Completion: New Algorithms and Evaluation Protocols](https://doi.org/10.18653/v1/2020.emnlp-main.305) |  | 0 | Research on temporal knowledge bases, which associate a relational fact (s,r,o) with a validity time period (or time instant), is in its early days. Our work considers predicting missing entities (link prediction) and missing time intervals (time prediction) as joint Temporal Knowledge Base... | Prachi Jain, Sushant Rathi, Mausam, Soumen Chakrabarti |  |
| 790 |  |  [OpenIE6: Iterative Grid Labeling and Coordination Analysis for Open Information Extraction](https://doi.org/10.18653/v1/2020.emnlp-main.306) |  | 0 | A recent state-of-the-art neural open information extraction (OpenIE) system generates extractions iteratively, requiring repeated encoding of partial outputs. This comes at a significant computational cost. On the other hand,sequence labeling approaches for OpenIE are much faster, but worse in... | Keshav Kolluru, Vaibhav Adlakha, Samarth Aggarwal, Mausam, Soumen Chakrabarti |  |
| 791 |  |  [Public Sentiment Drift Analysis Based on Hierarchical Variational Auto-encoder](https://doi.org/10.18653/v1/2020.emnlp-main.307) |  | 0 | Detecting public sentiment drift is a challenging task due to sentiment change over time. Existing methods first build a classification model using historical data and subsequently detect drift if the model performs much worse on new data. In this paper, we focus on distribution learning by... | Wenyue Zhang, Xiaoli Li, Yang Li, Suge Wang, Deyu Li, Jian Liao, Jianxing Zheng |  |
| 792 |  |  [Point to the Expression: Solving Algebraic Word Problems using the Expression-Pointer Transformer Model](https://doi.org/10.18653/v1/2020.emnlp-main.308) |  | 0 | Solving algebraic word problems has recently emerged as an important natural language processing task. To solve algebraic word problems, recent studies suggested neural models that generate solution equations by using ‘Op (operator/operand)’ tokens as a unit of input/output. However, such a neural... | Bugeun Kim, Kyung Seo Ki, Donggeon Lee, Gahgene Gweon |  |
| 793 |  |  [Semantically-Aligned Universal Tree-Structured Solver for Math Word Problems](https://doi.org/10.18653/v1/2020.emnlp-main.309) |  | 0 | A practical automatic textual math word problems (MWPs) solver should be able to solve various textual MWPs while most existing works only focused on one-unknown linear MWPs. Herein, we propose a simple but efficient method called Universal Expression Tree (UET) to make the first attempt to... | Jinghui Qin, Lihui Lin, Xiaodan Liang, Rumin Zhang, Liang Lin |  |
| 794 |  |  [Neural Topic Modeling by Incorporating Document Relationship Graph](https://doi.org/10.18653/v1/2020.emnlp-main.310) |  | 0 | Graph Neural Networks (GNNs) that capture the relationships between graph nodes via message passing have been a hot research direction in the natural language processing community. In this paper, we propose Graph Topic Model (GTM), a GNN based neural topic model that represents a corpus as a... | Deyu Zhou, Xuemeng Hu, Rui Wang |  |
| 795 |  |  [Routing Enforced Generative Model for Recipe Generation](https://doi.org/10.18653/v1/2020.emnlp-main.311) |  | 0 | One of the most challenging part of recipe generation is to deal with the complex restrictions among the input ingredients. Previous researches simplify the problem by treating the inputs independently and generating recipes containing as much information as possible. In this work, we propose a... | Zhiwei Yu, Hongyu Zang, Xiaojun Wan |  |
| 796 |  |  [Assessing the Helpfulness of Learning Materials with Inference-Based Learner-Like Agent](https://doi.org/10.18653/v1/2020.emnlp-main.312) |  | 0 | Many English-as-a-second language learners have trouble using near-synonym words (e.g., small vs.little; briefly vs.shortly) correctly, and often look for example sentences to learn how two nearly synonymous terms differ. Prior work uses hand-crafted scores to recommend sentences but has difficulty... | YunHsuan Jen, ChiehYang Huang, MeiHua Chen, TingHao K. Huang, LunWei Ku |  |
| 797 |  |  [Selection and Generation: Learning towards Multi-Product Advertisement Post Generation](https://doi.org/10.18653/v1/2020.emnlp-main.313) |  | 0 | As the E-commerce thrives, high-quality online advertising copywriting has attracted more and more attention. Different from the advertising copywriting for a single product, an advertisement (AD) post includes an attractive topic that meets the customer needs and description copywriting about... | Zhangming Chan, Yuchi Zhang, Xiuying Chen, Shen Gao, Zhiqiang Zhang, Dongyan Zhao, Rui Yan |  |
| 798 |  |  [Form2Seq : A Framework for Higher-Order Form Structure Extraction](https://doi.org/10.18653/v1/2020.emnlp-main.314) |  | 0 | Document structure extraction has been a widely researched area for decades with recent works performing it as a semantic segmentation task over document images using fully-convolution networks. Such methods are limited by image resolution due to which they fail to disambiguate structures in dense... | Milan Aggarwal, Hiresh Gupta, Mausoom Sarkar, Balaji Krishnamurthy |  |
| 799 |  |  [Domain Adaptation of Thai Word Segmentation Models using Stacked Ensemble](https://doi.org/10.18653/v1/2020.emnlp-main.315) |  | 0 | Like many Natural Language Processing tasks, Thai word segmentation is domain-dependent. Researchers have been relying on transfer learning to adapt an existing model to a new domain. However, this approach is inapplicable to cases where we can interact with only input and output layers of the... | Peerat Limkonchotiwat, Wannaphong Phatthiyaphaibun, Raheem Sarwar, Ekapol Chuangsuwanich, Sarana Nutanong |  |
| 800 |  |  [DagoBERT: Generating Derivational Morphology with a Pretrained Language Model](https://doi.org/10.18653/v1/2020.emnlp-main.316) |  | 0 | Can pretrained language models (PLMs) generate derivationally complex words? We present the first study investigating this question, taking BERT as the example PLM. We examine BERT’s derivational capabilities in different settings, ranging from using the unmodified pretrained model to full... | Valentin Hofmann, Janet B. Pierrehumbert, Hinrich Schütze |  |
| 801 |  |  [Attention Is All You Need for Chinese Word Segmentation](https://doi.org/10.18653/v1/2020.emnlp-main.317) |  | 0 | Taking greedy decoding algorithm as it should be, this work focuses on further strengthening the model itself for Chinese word segmentation (CWS), which results in an even more fast and more accurate CWS model. Our model consists of an attention only stacked encoder and a light enough decoder for... | Sufeng Duan, Hai Zhao |  |
| 802 |  |  [A Joint Multiple Criteria Model in Transfer Learning for Cross-domain Chinese Word Segmentation](https://doi.org/10.18653/v1/2020.emnlp-main.318) |  | 0 | Word-level information is important in natural language processing (NLP), especially for the Chinese language due to its high linguistic complexity. Chinese word segmentation (CWS) is an essential task for Chinese downstream NLP tasks. Existing methods have already achieved a competitive performance... | Kaiyu Huang, Degen Huang, Zhuang Liu, Fengran Mo |  |
| 803 |  |  [Alignment-free Cross-lingual Semantic Role Labeling](https://doi.org/10.18653/v1/2020.emnlp-main.319) |  | 0 | Cross-lingual semantic role labeling (SRL) aims at leveraging resources in a source language to minimize the effort required to construct annotations or models for a new target language. Recent approaches rely on word alignments, machine translation engines, or preprocessing tools such as parsers or... | Rui Cai, Mirella Lapata |  |
| 804 |  |  [Leveraging Declarative Knowledge in Text and First-Order Logic for Fine-Grained Propaganda Detection](https://doi.org/10.18653/v1/2020.emnlp-main.320) |  | 0 | We study the detection of propagandistic text fragments in news articles. Instead of merely learning from input-output datapoints in training data, we introduce an approach to inject declarative knowledge of fine-grained propaganda techniques. Specifically, we leverage the declarative knowledge... | Ruize Wang, Duyu Tang, Nan Duan, Wanjun Zhong, Zhongyu Wei, Xuanjing Huang, Daxin Jiang, Ming Zhou |  |
| 805 |  |  [X-SRL: A Parallel Cross-Lingual Semantic Role Labeling Dataset](https://doi.org/10.18653/v1/2020.emnlp-main.321) |  | 0 | Even though SRL is researched for many languages, major improvements have mostly been obtained for English, for which more resources are available. In fact, existing multilingual SRL datasets contain disparate annotation styles or come from different domains, hampering generalization in multilingual... | Angel Daza, Anette Frank |  |
| 806 |  |  [Graph Convolutions over Constituent Trees for Syntax-Aware Semantic Role Labeling](https://doi.org/10.18653/v1/2020.emnlp-main.322) |  | 0 | Semantic role labeling (SRL) is the task of identifying predicates and labeling argument spans with semantic roles. Even though most semantic-role formalisms are built upon constituent syntax, and only syntactic constituents can be labeled as arguments (e.g., FrameNet and PropBank), all the recent... | Diego Marcheggiani, Ivan Titov |  |
| 807 |  |  [Fast semantic parsing with well-typedness guarantees](https://doi.org/10.18653/v1/2020.emnlp-main.323) |  | 0 | AM dependency parsing is a linguistically principled method for neural semantic parsing with high accuracy across multiple graphbanks. It relies on a type system that models semantic valency but makes existing parsers slow. We describe an A\* parser and a transition-based parser for AM dependency... | Matthias Lindemann, Jonas Groschwitz, Alexander Koller |  |
| 808 |  |  [Improving Out-of-Scope Detection in Intent Classification by Using Embeddings of the Word Graph Space of the Classes](https://doi.org/10.18653/v1/2020.emnlp-main.324) |  | 0 | This paper explores how intent classification can be improved by representing the class labels not as a discrete set of symbols but as a space where the word graphs associated to each class are mapped using typical graph embedding techniques. The approach, inspired by a previous algorithm used for... | Paulo R. Cavalin, Victor Henrique Alves Ribeiro, Ana Paula Appel, Claudio S. Pinhanez |  |
| 809 |  |  [Supervised Seeded Iterated Learning for Interactive Language Learning](https://doi.org/10.18653/v1/2020.emnlp-main.325) |  | 0 | Language drift has been one of the major obstacles to train language models through interaction. When word-based conversational agents are trained towards completing a task, they tend to invent their language rather than leveraging natural language. In recent literature, two general methods... | Yuchen Lu, Soumye Singhal, Florian Strub, Olivier Pietquin, Aaron C. Courville |  |
| 810 |  |  [Spot The Bot: A Robust and Efficient Framework for the Evaluation of Conversational Dialogue Systems](https://doi.org/10.18653/v1/2020.emnlp-main.326) |  | 0 | The lack of time efficient and reliable evalu-ation methods is hampering the development of conversational dialogue systems (chat bots). Evaluations that require humans to converse with chat bots are time and cost intensive, put high cognitive demands on the human judges, and tend to yield low... | Jan Deriu, Don Tuggener, Pius von Däniken, Jon Ander Campos, Álvaro Rodrigo, Thiziri Belkacem, Aitor Soroa, Eneko Agirre, Mark Cieliebak |  |
| 811 |  |  [Human-centric dialog training via offline reinforcement learning](https://doi.org/10.18653/v1/2020.emnlp-main.327) |  | 0 | How can we train a dialog model to produce better conversations by learning from human feedback, without the risk of humans teaching it harmful chat behaviors? We start by hosting models online, and gather human feedback from real-time, open-ended conversations, which we then use to train and... | Natasha Jaques, Judy Hanwen Shen, Asma Ghandeharioun, Craig Ferguson, Àgata Lapedriza, Noah Jones, Shixiang Gu, Rosalind W. Picard |  |
| 812 |  |  [Speakers Fill Lexical Semantic Gaps with Context](https://doi.org/10.18653/v1/2020.emnlp-main.328) |  | 0 | Lexical ambiguity is widespread in language, allowing for the reuse of economical word forms and therefore making language more efficient. If ambiguous words cannot be disambiguated from context, however, this gain in efficiency might make language less clear—resulting in frequent miscommunication.... | Tiago Pimentel, Rowan Hall Maudslay, Damián E. Blasi, Ryan Cotterell |  |
| 813 |  |  [Investigating Cross-Linguistic Adjective Ordering Tendencies with a Latent-Variable Model](https://doi.org/10.18653/v1/2020.emnlp-main.329) |  | 0 | Across languages, multiple consecutive adjectives modifying a noun (e.g. “the big red dog”) follow certain unmarked ordering rules. While explanatory accounts have been put forward, much of the work done in this area has relied primarily on the intuitive judgment of native speakers, rather than on... | Jun Yen Leung, Guy Emerson, Ryan Cotterell |  |
| 814 |  |  [Surprisal Predicts Code-Switching in Chinese-English Bilingual Text](https://doi.org/10.18653/v1/2020.emnlp-main.330) |  | 0 | Why do bilinguals switch languages within a sentence? The present observational study asks whether word surprisal and word entropy predict code-switching in bilingual written conversation. We describe and model a new dataset of Chinese-English text with 1476 clean code-switched sentences, translated... | Jesús Calvillo, Le Fang, Jeremy R. Cole, David Reitter |  |
| 815 |  |  [Word Frequency Does Not Predict Grammatical Knowledge in Language Models](https://doi.org/10.18653/v1/2020.emnlp-main.331) |  | 0 | Neural language models learn, to varying degrees of accuracy, the grammatical properties of natural languages. In this work, we investigate whether there are systematic sources of variation in the language models’ accuracy. Focusing on subject-verb agreement and reflexive anaphora, we find that... | Charles Yu, Ryan Sie, Nico Tedeschi, Leon Bergen |  |
| 816 |  |  [Improving Word Sense Disambiguation with Translations](https://doi.org/10.18653/v1/2020.emnlp-main.332) |  | 0 | It has been conjectured that multilingual information can help monolingual word sense disambiguation (WSD). However, existing WSD systems rarely consider multilingual information, and no effective method has been proposed for improving WSD by generating translations. In this paper, we present a... | Yixing Luan, Bradley Hauer, Lili Mou, Grzegorz Kondrak |  |
| 817 |  |  [Towards Better Context-aware Lexical Semantics: Adjusting Contextualized Representations through Static Anchors](https://doi.org/10.18653/v1/2020.emnlp-main.333) |  | 0 | One of the most powerful features of contextualized models is their dynamic embeddings for words in context, leading to state-of-the-art representations for context-aware lexical semantics. In this paper, we present a post-processing technique that enhances these representations by learning a... | Qianchu Liu, Diana McCarthy, Anna Korhonen |  |
| 818 |  |  [Compositional Demographic Word Embeddings](https://doi.org/10.18653/v1/2020.emnlp-main.334) |  | 0 | Word embeddings are usually derived from corpora containing text from many individuals, thus leading to general purpose representations rather than individually personalized representations. While personalized embeddings can be useful to improve language model performance and other language... | Charles Welch, Jonathan K. Kummerfeld, Verónica PérezRosas, Rada Mihalcea |  |
| 819 |  |  [Do "Undocumented Workers" == "Illegal Aliens"? Differentiating Denotation and Connotation in Vector Spaces](https://doi.org/10.18653/v1/2020.emnlp-main.335) |  | 0 | In politics, neologisms are frequently invented for partisan objectives. For example, “undocumented workers” and “illegal aliens” refer to the same group of people (i.e., they have the same denotation), but they carry clearly different connotations. Examples like these have traditionally posed a... | Albert Webson, Zhizhong Chen, Carsten Eickhoff, Ellie Pavlick |  |
| 820 |  |  [Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization](https://doi.org/10.18653/v1/2020.emnlp-main.336) |  | 0 | Text summarization is one of the most challenging and interesting problems in NLP. Although much attention has been paid to summarizing structured text like news reports or encyclopedia articles, summarizing conversations—an essential part of human-human/machine interaction where most important... | Jiaao Chen, Diyi Yang |  |
| 821 |  |  [Few-Shot Learning for Opinion Summarization](https://doi.org/10.18653/v1/2020.emnlp-main.337) |  | 0 | Opinion summarization is the automatic creation of text reflecting subjective information expressed in multiple documents, such as user reviews of a product. The task is practically important and has attracted a lot of attention. However, due to the high cost of summary production, datasets large... | Arthur Brazinskas, Mirella Lapata, Ivan Titov |  |
| 822 |  |  [Learning to Fuse Sentences with Transformers for Summarization](https://doi.org/10.18653/v1/2020.emnlp-main.338) |  | 0 | The ability to fuse sentences is highly attractive for summarization systems because it is an essential step to produce succinct abstracts. However, to date, summarizers can fail on fusing sentences. They tend to produce few summary sentences by fusion or generate incorrect fusions that lead the... | Logan Lebanoff, Franck Dernoncourt, Doo Soon Kim, Lidan Wang, Walter Chang, Fei Liu |  |
| 823 |  |  [Stepwise Extractive Summarization and Planning with Structured Transformers](https://doi.org/10.18653/v1/2020.emnlp-main.339) |  | 0 | We propose encoder-centric stepwise models for extractive summarization using structured transformers – HiBERT and Extended Transformers. We enable stepwise summarization by injecting the previously generated summary into the structured transformer as an auxiliary sub-structure. Our models are not... | Shashi Narayan, Joshua Maynez, Jakub Adámek, Daniele Pighin, Blaz Bratanic, Ryan T. McDonald |  |
| 824 |  |  [CLIRMatrix: A massively large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval](https://doi.org/10.18653/v1/2020.emnlp-main.340) |  | 0 | We present CLIRMatrix, a massively large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval extracted automatically from Wikipedia. CLIRMatrix comprises (1) BI-139, a bilingual dataset of queries in one language matched with relevant documents in another... | Shuo Sun, Kevin Duh |  |
| 825 |  |  [SLEDGE-Z: A Zero-Shot Baseline for COVID-19 Literature Search](https://doi.org/10.18653/v1/2020.emnlp-main.341) |  | 0 | With worldwide concerns surrounding the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), there is a rapidly growing body of scientific literature on the virus. Clinicians, researchers, and policy-makers need to be able to search these articles effectively. In this work, we present a... | Sean MacAvaney, Arman Cohan, Nazli Goharian |  |
| 826 |  |  [Modularized Transfomer-based Ranking Framework](https://doi.org/10.18653/v1/2020.emnlp-main.342) |  | 0 | Recent innovations in Transformer-based ranking models have advanced the state-of-the-art in information retrieval. However, these Transformers are computationally expensive, and their opaque hidden states make it hard to understand the ranking process. In this work, we modularize the Transformer... | Luyu Gao, Zhuyun Dai, Jamie Callan |  |
| 827 |  |  [Ad-hoc Document Retrieval using Weak-Supervision with BERT and GPT2](https://doi.org/10.18653/v1/2020.emnlp-main.343) |  | 0 | We describe a weakly-supervised method for training deep learning models for the task of ad-hoc document retrieval. Our method is based on generative and discriminative models that are trained using weak-supervision just from the documents in the corpus. We present an end-to-end retrieval system... | Yosi Mass, Haggai Roitman |  |
| 828 |  |  [Adversarial Semantic Collisions](https://doi.org/10.18653/v1/2020.emnlp-main.344) |  | 0 | We study semantic collisions: texts that are semantically unrelated but judged as similar by NLP models. We develop gradient-based approaches for generating semantic collisions and demonstrate that state-of-the-art models for many tasks which rely on analyzing the meaning and similarity of... | Congzheng Song, Alexander M. Rush, Vitaly Shmatikov |  |
| 829 |  |  [Learning Explainable Linguistic Expressions with Neural Inductive Logic Programming for Sentence Classification](https://doi.org/10.18653/v1/2020.emnlp-main.345) |  | 0 | Interpretability of predictive models is becoming increasingly important with growing adoption in the real-world. We present RuleNN, a neural network architecture for learning transparent models for sentence classification. The models are in the form of rules expressed in first-order logic, a... | Prithviraj Sen, Marina Danilevsky, Yunyao Li, Siddhartha Brahma, Matthias Boehm, Laura Chiticariu, Rajasekar Krishnamurthy |  |
| 830 |  |  [AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts](https://doi.org/10.18653/v1/2020.emnlp-main.346) |  | 0 | The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the... | Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, Sameer Singh |  |
| 831 |  |  [Learning Variational Word Masks to Improve the Interpretability of Neural Text Classifiers](https://doi.org/10.18653/v1/2020.emnlp-main.347) |  | 0 | To build an interpretable neural text classifier, most of the prior work has focused on designing inherently interpretable models or finding faithful explanations. A new line of work on improving model interpretability has just started, and many existing methods require either prior information or... | Hanjie Chen, Yangfeng Ji |  |
| 832 |  |  [Sparse Text Generation](https://doi.org/10.18653/v1/2020.emnlp-main.348) |  | 0 | Current state-of-the-art text generators build on powerful language models such as GPT-2, achieving impressive performance. However, to avoid degenerate text, they require sampling from a modified softmax, via temperature parameters or ad-hoc truncation techniques, as in top-k or nucleus sampling.... | Pedro Henrique Martins, Zita Marinho, André F. T. Martins |  |
| 833 |  |  [PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking](https://doi.org/10.18653/v1/2020.emnlp-main.349) |  | 0 | We propose the task of outline-conditioned story generation: given an outline as a set of phrases that describe key characters and events to appear in a story, the task is to generate a coherent narrative that is consistent with the provided outline. This task is challenging as the input only... | Hannah Rashkin, Asli Celikyilmaz, Yejin Choi, Jianfeng Gao |  |
| 834 |  |  [Do sequence-to-sequence VAEs learn global features of sentences?](https://doi.org/10.18653/v1/2020.emnlp-main.350) |  | 0 | Autoregressive language models are powerful and relatively easy to train. However, these models are usually trained without explicit conditioning labels and do not offer easy ways to control global aspects such as sentiment or topic during generation. Bowman & al. 2016 adapted the Variational... | Tom Bosc, Pascal Vincent |  |
| 835 |  |  [Content Planning for Neural Story Generation with Aristotelian Rescoring](https://doi.org/10.18653/v1/2020.emnlp-main.351) |  | 0 | Long-form narrative text generated from large language models manages a fluent impersonation of human writing, but only at the local sentence level, and lacks structure or global cohesion. We posit that many of the problems of story generation can be addressed via high-quality content planning, and... | Seraphina GoldfarbTarrant, Tuhin Chakrabarty, Ralph M. Weischedel, Nanyun Peng |  |
| 836 |  |  [Generating Dialogue Responses from a Semantic Latent Space](https://doi.org/10.18653/v1/2020.emnlp-main.352) |  | 0 | Existing open-domain dialogue generation models are usually trained to mimic the gold response in the training set using cross-entropy loss on the vocabulary. However, a good response does not need to resemble the gold response, since there are multiple possible responses to a given prompt. In this... | WeiJen Ko, Avik Ray, Yilin Shen, Hongxia Jin |  |
| 837 |  |  [Refer, Reuse, Reduce: Generating Subsequent References in Visual and Conversational Contexts](https://doi.org/10.18653/v1/2020.emnlp-main.353) |  | 0 | Dialogue participants often refer to entities or situations repeatedly within a conversation, which contributes to its cohesiveness. Subsequent references exploit the common ground accumulated by the interlocutors and hence have several interesting properties, namely, they tend to be shorter and... | Ece Takmaz, Mario Giulianelli, Sandro Pezzelle, Arabella Sinclair, Raquel Fernández |  |
| 838 |  |  [Visually Grounded Compound PCFGs](https://doi.org/10.18653/v1/2020.emnlp-main.354) |  | 0 | Exploiting visual groundings for language understanding has recently been drawing much attention. In this work, we study visually grounded grammar induction and learn a constituency parser from both unlabeled text and its visual groundings. Existing work on this task (Shi et al., 2019) optimizes a... | Yanpeng Zhao, Ivan Titov |  |
| 839 |  |  [ALICE: Active Learning with Contrastive Natural Language Explanations](https://doi.org/10.18653/v1/2020.emnlp-main.355) |  | 0 | Training a supervised neural network classifier typically requires many annotated training samples. Collecting and annotating a large number of data points are costly and sometimes even infeasible. Traditional annotation process uses a low-bandwidth human-machine communication interface:... | Weixin Liang, James Zou, Zhou Yu |  |
| 840 |  |  [Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding](https://doi.org/10.18653/v1/2020.emnlp-main.356) |  | 0 | We introduce Room-Across-Room (RxR), a new Vision-and-Language Navigation (VLN) dataset. RxR is multilingual (English, Hindi, and Telugu) and larger (more paths and instructions) than other VLN datasets. It emphasizes the role of language in VLN by addressing known biases in paths and eliciting more... | Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, Jason Baldridge |  |
| 841 |  |  [SSCR: Iterative Language-Based Image Editing via Self-Supervised Counterfactual Reasoning](https://doi.org/10.18653/v1/2020.emnlp-main.357) |  | 0 | Iterative Language-Based Image Editing (ILBIE) tasks follow iterative instructions to edit images step by step. Data scarcity is a significant issue for ILBIE as it is challenging to collect large-scale examples of images before and after instruction-based changes. Yet, humans still accomplish these... | TsuJui Fu, Xin Wang, Scott T. Grafton, Miguel P. Eckstein, William Yang Wang |  |
| 842 |  |  [Identifying Elements Essential for BERT's Multilinguality](https://doi.org/10.18653/v1/2020.emnlp-main.358) |  | 0 | It has been shown that multilingual BERT (mBERT) yields high quality multilingual representations and enables effective zero-shot transfer. This is surprising given that mBERT does not use any crosslingual signal during training. While recent literature has studied this phenomenon, the reasons for... | Philipp Dufter, Hinrich Schütze |  |
| 843 |  |  [On Negative Interference in Multilingual Models: Findings and A Meta-Learning Treatment](https://doi.org/10.18653/v1/2020.emnlp-main.359) |  | 0 | Modern multilingual models are trained on concatenated text from multiple languages in hopes of conferring benefits to each (positive transfer), with the most pronounced benefits accruing to low-resource languages. However, recent work has shown that this approach can degrade performance on... | Zirui Wang, Zachary C. Lipton, Yulia Tsvetkov |  |
| 844 |  |  [Pre-tokenization of Multi-word Expressions in Cross-lingual Word Embeddings](https://doi.org/10.18653/v1/2020.emnlp-main.360) |  | 0 | Cross-lingual word embedding (CWE) algorithms represent words in multiple languages in a unified vector space. Multi-Word Expressions (MWE) are common in every language. When training word embeddings, each component word of an MWE gets its own separate embedding, and thus, MWEs are not translated by... | Naoki Otani, Satoru Ozaki, Xingyuan Zhao, Yucen Li, Micaelah St Johns, Lori S. Levin |  |
| 845 |  |  [Monolingual Adapters for Zero-Shot Neural Machine Translation](https://doi.org/10.18653/v1/2020.emnlp-main.361) |  | 0 | We propose a novel adapter layer formalism for adapting multilingual models. They are more parameter-efficient than existing adapter layers while obtaining as good or better performance. The layers are specific to one language (as opposed to bilingual adapters) allowing to compose them and... | Jerin Philip, Alexandre Berard, Matthias Gallé, Laurent Besacier |  |
| 846 |  |  [Do Explicit Alignments Robustly Improve Multilingual Encoders?](https://doi.org/10.18653/v1/2020.emnlp-main.362) |  | 0 | Multilingual BERT (mBERT), XLM-RoBERTa (XLMR) and other unsupervised multilingual encoders can effectively learn cross-lingual representation. Explicit alignment objectives based on bitexts like Europarl or MultiUN have been shown to further improve these representations. However, word-level... | Shijie Wu, Mark Dredze |  |
| 847 |  |  [From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers](https://doi.org/10.18653/v1/2020.emnlp-main.363) |  | 0 | Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with... | Anne Lauscher, Vinit Ravishankar, Ivan Vulic, Goran Glavas |  |
| 848 |  |  [Distilling Multiple Domains for Neural Machine Translation](https://doi.org/10.18653/v1/2020.emnlp-main.364) |  | 0 | Neural machine translation achieves impressive results in high-resource conditions, but performance often suffers when the input domain is low-resource. The standard practice of adapting a separate model for each domain of interest does not scale well in practice from both a quality perspective... | Anna Currey, Prashant Mathur, Georgiana Dinu |  |
| 849 |  |  [Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation](https://doi.org/10.18653/v1/2020.emnlp-main.365) |  | 0 | We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should be mapped to the same location in the vector... | Nils Reimers, Iryna Gurevych |  |
| 850 |  |  [A Streaming Approach For Efficient Batched Beam Search](https://doi.org/10.18653/v1/2020.emnlp-main.366) |  | 0 | We propose an efficient batching strategy for variable-length decoding on GPU architectures. During decoding, when candidates terminate or are pruned according to heuristics, our streaming approach periodically “refills” the batch before proceeding with a selected subset of candidates. We apply our... | Kevin Yang, Violet Yao, John DeNero, Dan Klein |  |
| 851 |  |  [Improving Multilingual Models with Language-Clustered Vocabularies](https://doi.org/10.18653/v1/2020.emnlp-main.367) |  | 0 | State-of-the-art multilingual models depend on vocabularies that cover all of the languages the model will expect to see at inference time, but the standard methods for generating those vocabularies are not ideal for massively multilingual applications. In this work, we introduce a novel procedure... | Hyung Won Chung, Dan Garrette, Kiat Chuan Tan, Jason Riesa |  |
| 852 |  |  [Zero-Shot Cross-Lingual Transfer with Meta Learning](https://doi.org/10.18653/v1/2020.emnlp-main.368) |  | 0 | Learning what to share between tasks has become a topic of great importance, as strategic sharing of knowledge has been shown to improve downstream task performance. This is particularly important for multilingual applications, as most languages in the world are under-resourced. Here, we consider... | Farhad Nooralahzadeh, Giannis Bekoulis, Johannes Bjerva, Isabelle Augenstein |  |
| 853 |  |  [The Multilingual Amazon Reviews Corpus](https://doi.org/10.18653/v1/2020.emnlp-main.369) |  | 0 | We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale collection of Amazon reviews for multilingual text classification. The corpus contains reviews in English, Japanese, German, French, Spanish, and Chinese, which were collected between 2015 and 2019. Each record in the dataset... | Phillip Keung, Yichao Lu, György Szarvas, Noah A. Smith |  |
| 854 |  |  [GLUCOSE: GeneraLized and COntextualized Story Explanations](https://doi.org/10.18653/v1/2020.emnlp-main.370) |  | 0 | When humans read or listen, they make implicit commonsense inferences that frame their understanding of what happened and why. As a step toward AI systems that can build similar mental models, we introduce GLUCOSE, a large-scale dataset of implicit commonsense causal knowledge, encoded as causal... | Nasrin Mostafazadeh, Aditya Kalyanpur, Lori Moon, David W. Buchanan, Lauren Berkowitz, Or Biran, Jennifer ChuCarroll |  |
| 855 |  |  [Character-level Representations Improve DRS-based Semantic Parsing Even in the Age of BERT](https://doi.org/10.18653/v1/2020.emnlp-main.371) |  | 0 | We combine character-level and contextual language model representations to improve performance on Discourse Representation Structure parsing. Character representations can easily be added in a sequence-to-sequence model in either one encoder or as a fully separate encoder, with improvements that... | Rik van Noord, Antonio Toral, Johan Bos |  |
| 856 |  |  [Infusing Disease Knowledge into BERT for Health Question Answering, Medical Inference and Disease Name Recognition](https://doi.org/10.18653/v1/2020.emnlp-main.372) |  | 0 | Knowledge of a disease includes information of various aspects of the disease, such as signs and symptoms, diagnosis and treatment. This disease knowledge is critical for many health-related and biomedical tasks, including consumer health question answering, medical language inference and disease... | Yun He, Ziwei Zhu, Yin Zhang, Qin Chen, James Caverlee |  |
| 857 |  |  [Unsupervised Commonsense Question Answering with Self-Talk](https://doi.org/10.18653/v1/2020.emnlp-main.373) |  | 0 | Natural language understanding involves reading between the lines with implicit background knowledge. Current systems either rely on pre-trained language models as the sole implicit source of world knowledge, or resort to external knowledge bases (KBs) to incorporate additional relevant knowledge.... | Vered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi |  |
| 858 |  |  [Reasoning about Goals, Steps, and Temporal Ordering with WikiHow](https://doi.org/10.18653/v1/2020.emnlp-main.374) |  | 0 | We propose a suite of reasoning tasks on two types of relations between procedural events: goal-step relations (“learn poses” is a step in the larger goal of “doing yoga”) and step-step temporal relations (“buy a yoga mat” typically precedes “learn poses”). We introduce a dataset targeting these two... | Li Zhang, Qing Lyu, Chris CallisonBurch |  |
| 859 |  |  [Structural Supervision Improves Few-Shot Learning and Syntactic Generalization in Neural Language Models](https://doi.org/10.18653/v1/2020.emnlp-main.375) |  | 0 | Humans can learn structural properties about a word from minimal experience, and deploy their learned syntactic representations uniformly in different grammatical contexts. We assess the ability of modern neural language models to reproduce this behavior in English and evaluate the effect of... | Ethan Wilcox, Peng Qian, Richard Futrell, Ryosuke Kohita, Roger Levy, Miguel Ballesteros |  |
| 860 |  |  [Investigating representations of verb bias in neural language models](https://doi.org/10.18653/v1/2020.emnlp-main.376) |  | 0 | Languages typically provide more than one grammatical construction to express certain types of messages. A speaker’s choice of construction is known to depend on multiple factors, including the choice of main verb – a phenomenon known as verb bias. Here we introduce DAIS, a large benchmark dataset... | Robert X. D. Hawkins, Takateru Yamakoshi, Thomas L. Griffiths, Adele E. Goldberg |  |
| 861 |  |  [Generating Image Descriptions via Sequential Cross-Modal Alignment Guided by Human Gaze](https://doi.org/10.18653/v1/2020.emnlp-main.377) |  | 0 | When speakers describe an image, they tend to look at objects before mentioning them. In this paper, we investigate such sequential cross-modal alignment by modelling the image description generation process computationally. We take as our starting point a state-of-the-art image captioning system... | Ece Takmaz, Sandro Pezzelle, Lisa Beinborn, Raquel Fernández |  |
| 862 |  |  [Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space](https://doi.org/10.18653/v1/2020.emnlp-main.378) |  | 0 | When trained effectively, the Variational Autoencoder (VAE) can be both a powerful generative model and an effective representation learning framework for natural language. In this paper, we propose the first large-scale language VAE model Optimus (Organizing sentences via Pre-Trained Modeling of a... | Chunyuan Li, Xiang Gao, Yuan Li, Baolin Peng, Xiujun Li, Yizhe Zhang, Jianfeng Gao |  |
| 863 |  |  [BioMegatron: Larger Biomedical Domain Language Model](https://doi.org/10.18653/v1/2020.emnlp-main.379) |  | 0 | There has been an influx of biomedical domain-specific language models, showing language models pre-trained on biomedical text perform better on biomedical domain benchmarks than those trained on general domain text corpora such as Wikipedia and Books. Yet, most works do not study the factors... | HooChang Shin, Yang Zhang, Evelina Bakhturina, Raul Puri, Mostofa Patwary, Mohammad Shoeybi, Raghav Mani |  |
| 864 |  |  [Text Segmentation by Cross Segment Attention](https://doi.org/10.18653/v1/2020.emnlp-main.380) |  | 0 | Document and discourse segmentation are two fundamental NLP tasks pertaining to breaking up text into constituents, which are commonly used to help downstream tasks such as information retrieval or text summarization. In this work, we propose three transformer-based architectures and provide... | Michal Lukasik, Boris Dadachev, Kishore Papineni, Gonçalo Simões |  |
| 865 |  |  [RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark](https://doi.org/10.18653/v1/2020.emnlp-main.381) |  | 0 | In this paper, we introduce an advanced Russian general language understanding evaluation benchmark – Russian SuperGLUE. Recent advances in the field of universal language models and transformers require the development of a methodology for their broad diagnostics and testing for general... | Tatiana Shavrina, Alena Fenogenova, Anton A. Emelyanov, Denis Shevelev, Ekaterina Artemova, Valentin Malykh, Vladislav Mikhailov, Maria Tikhonova, Andrey Chertok, Andrey Evlampiev |  |
| 866 |  |  [An Empirical Study of Pre-trained Transformers for Arabic Information Extraction](https://doi.org/10.18653/v1/2020.emnlp-main.382) |  | 0 | Multilingual pre-trained Transformers, such as mBERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020a), have been shown to enable effective cross-lingual zero-shot transfer. However, their performance on Arabic information extraction (IE) tasks is not very well studied. In this paper, we... | Wuwei Lan, Yang Chen, Wei Xu, Alan Ritter |  |
| 867 |  |  [TNT: Text Normalization based Pre-training of Transformers for Content Moderation](https://doi.org/10.18653/v1/2020.emnlp-main.383) |  | 0 | In this work, we present a new language pre-training model TNT (Text Normalization based pre-training of Transformers) for content moderation. Inspired by the masking strategy and text normalization, TNT is developed to learn language representation by training transformers to reconstruct text from... | Fei Tan, Yifan Hu, Changwei Hu, Keqian Li, Kevin Yen |  |
| 868 |  |  [Methods for Numeracy-Preserving Word Embeddings](https://doi.org/10.18653/v1/2020.emnlp-main.384) |  | 0 | Word embedding models are typically able to capture the semantics of words via the distributional hypothesis, but fail to capture the numerical properties of numbers that appear in the text. This leads to problems with numerical reasoning involving tasks such as question answering. We propose a new... | Dhanasekar Sundararaman, Shijing Si, Vivek Subramanian, Guoyin Wang, Devamanyu Hazarika, Lawrence Carin |  |
| 869 |  |  [An Empirical Investigation of Contextualized Number Prediction](https://doi.org/10.18653/v1/2020.emnlp-main.385) |  | 0 | We conduct a large scale empirical investigation of contextualized number prediction in running text. Specifically, we consider two tasks: (1)masked number prediction– predict-ing a missing numerical value within a sentence, and (2)numerical anomaly detection–detecting an errorful numeric value... | Taylor BergKirkpatrick, Daniel Spokoyny |  |
| 870 |  |  [Modeling the Music Genre Perception across Language-Bound Cultures](https://doi.org/10.18653/v1/2020.emnlp-main.386) |  | 0 | The music genre perception expressed through human annotations of artists or albums varies significantly across language-bound cultures. These variations cannot be modeled as mere translations since we also need to account for cultural differences in the music genre perception. In this work, we... | Elena V. Epure, Guillaume Salha, Manuel Moussallam, Romain Hennequin |  |
| 871 |  |  [Joint Estimation and Analysis of Risk Behavior Ratings in Movie Scripts](https://doi.org/10.18653/v1/2020.emnlp-main.387) |  | 0 | Exposure to violent, sexual, or substance-abuse content in media increases the willingness of children and adolescents to imitate similar behaviors. Computational methods that identify portrayals of risk behaviors from audio-visual cues are limited in their applicability to films in post-production,... | Victor R. Martinez, Krishna Somandepalli, Yalda T. Uhls, Shrikanth Narayanan |  |
| 872 |  |  [Keep it Surprisingly Simple: A Simple First Order Graph Based Parsing Model for Joint Morphosyntactic Parsing in Sanskrit](https://doi.org/10.18653/v1/2020.emnlp-main.388) |  | 0 | Morphologically rich languages seem to benefit from joint processing of morphology and syntax, as compared to pipeline architectures. We propose a graph-based model for joint morphological parsing and dependency parsing in Sanskrit. Here, we extend the Energy based model framework (Krishna et al.,... | Amrith Krishna, Ashim Gupta, Deepak Garasangi, Pavankumar Satuluri, Pawan Goyal |  |
| 873 |  |  [Unsupervised Parsing via Constituency Tests](https://doi.org/10.18653/v1/2020.emnlp-main.389) |  | 0 | We propose a method for unsupervised parsing based on the linguistic notion of a constituency test. One type of constituency test involves modifying the sentence via some transformation (e.g. replacing the span with a pronoun) and then judging the result (e.g. checking if it is grammatical).... | Steven Cao, Nikita Kitaev, Dan Klein |  |
| 874 |  |  [Please Mind the Root: Decoding Arborescences for Dependency Parsing](https://doi.org/10.18653/v1/2020.emnlp-main.390) |  | 0 | The connection between dependency trees and spanning trees is exploited by the NLP community to train and to decode graph-based dependency parsers. However, the NLP literature has missed an important difference between the two structures: only one edge may emanate from the root in a dependency tree.... | Ran Zmigrod, Tim Vieira, Ryan Cotterell |  |
| 875 |  |  [Unsupervised Cross-Lingual Part-of-Speech Tagging for Truly Low-Resource Scenarios](https://doi.org/10.18653/v1/2020.emnlp-main.391) |  | 0 | We describe a fully unsupervised cross-lingual transfer approach for part-of-speech (POS) tagging under a truly low resource scenario. We assume access to parallel translations between the target language and one or more source languages for which POS taggers are available. We use the Bible as... | Ramy Eskander, Smaranda Muresan, Michael Collins |  |
| 876 |  |  [Unsupervised Parsing with S-DIORA: Single Tree Encoding for Deep Inside-Outside Recursive Autoencoders](https://doi.org/10.18653/v1/2020.emnlp-main.392) |  | 0 | The deep inside-outside recursive autoencoder (DIORA; Drozdov et al. 2019) is a self-supervised neural model that learns to induce syntactic tree structures for input sentences \*without access to labeled training data\*. In this paper, we discover that while DIORA exhaustively encodes all possible... | Andrew Drozdov, Subendhu Rongali, YiPei Chen, Tim O'Gorman, Mohit Iyyer, Andrew McCallum |  |
| 877 |  |  [Utility is in the Eye of the User: A Critique of NLP Leaderboards](https://doi.org/10.18653/v1/2020.emnlp-main.393) |  | 0 | Benchmarks such as GLUE have helped drive advances in NLP by incentivizing the creation of more accurate models. While this leaderboard paradigm has been remarkably successful, a historical focus on performance-based evaluation has been at the expense of other qualities that the NLP community values... | Kawin Ethayarajh, Dan Jurafsky |  |
| 878 |  |  [An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training](https://doi.org/10.18653/v1/2020.emnlp-main.394) |  | 0 | Pre-training large language models has become a standard in the natural language processing community. Such models are pre-trained on generic data (e.g. BookCorpus and English Wikipedia) and often fine-tuned on tasks in the same domain. However, in order to achieve state-of-the-art performance on... | Kristjan Arumae, Qing Sun, Parminder Bhatia |  |
| 879 |  |  [Analyzing Individual Neurons in Pre-trained Language Models](https://doi.org/10.18653/v1/2020.emnlp-main.395) |  | 0 | While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep NLP models, very little attention has been paid towards individual neurons. We carry outa neuron-level analysis using core linguistic tasks of predicting morphology,... | Nadir Durrani, Hassan Sajjad, Fahim Dalvi, Yonatan Belinkov |  |
| 880 |  |  [Dissecting Span Identification Tasks with Performance Prediction](https://doi.org/10.18653/v1/2020.emnlp-main.396) |  | 0 | Span identification (in short, span ID) tasks such as chunking, NER, or code-switching detection, ask models to identify and classify relevant spans in a text. Despite being a staple of NLP, and sharing a common structure, there is little insight on how these tasks’ properties influence their... | Sean Papay, Roman Klinger, Sebastian Padó |  |
| 881 |  |  [Assessing Phrasal Representation and Composition in Transformers](https://doi.org/10.18653/v1/2020.emnlp-main.397) |  | 0 | Deep transformer models have pushed performance on NLP tasks to new limits, suggesting sophisticated treatment of complex linguistic inputs, such as phrases. However, we have limited understanding of how these models handle representation of phrases, and whether this reflects sophisticated... | Lang Yu, Allyson Ettinger |  |
| 882 |  |  [Analyzing Redundancy in Pretrained Transformer Models](https://doi.org/10.18653/v1/2020.emnlp-main.398) |  | 0 | Transformer-based deep NLP models are trained using hundreds of millions of parameters, limiting their applicability in computationally constrained environments. In this paper, we study the cause of these limitations by defining a notion of Redundancy, which we categorize into two classes: General... | Fahim Dalvi, Hassan Sajjad, Nadir Durrani, Yonatan Belinkov |  |
| 883 |  |  [Be More with Less: Hypergraph Attention Networks for Inductive Text Classification](https://doi.org/10.18653/v1/2020.emnlp-main.399) |  | 0 | Text classification is a critical research topic with broad applications in natural language processing. Recently, graph neural networks (GNNs) have received increasing attention in the research community and demonstrated their promising results on this canonical task. Despite the success, their... | Kaize Ding, Jianling Wang, Jundong Li, Dingcheng Li, Huan Liu |  |
| 884 |  |  [Entities as Experts: Sparse Memory Access with Entity Supervision](https://doi.org/10.18653/v1/2020.emnlp-main.400) |  | 0 | We focus on the problem of capturing declarative knowledge about entities in the learned parameters of a language model. We introduce a new model—Entities as Experts (EaE)—that can access distinct memories of the entities mentioned in a piece of text. Unlike previous efforts to integrate entity... | Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, Tom Kwiatkowski |  |
| 885 |  |  [H2KGAT: Hierarchical Hyperbolic Knowledge Graph Attention Network](https://aclanthology.org/2020.emnlp-main.401/) |  | 0 |  | Shen Wang, Xiaokai Wei, Cícero Nogueira dos Santos, Zhiguo Wang, Ramesh Nallapati, Andrew O. Arnold, Bing Xiang, Philip S. Yu |  |
| 886 |  |  [Does the Objective Matter? Comparing Training Objectives for Pronoun Resolution](https://doi.org/10.18653/v1/2020.emnlp-main.402) |  | 0 | Hard cases of pronoun resolution have been used as a long-standing benchmark for commonsense reasoning. In the recent literature, pre-trained language models have been used to obtain state-of-the-art results on pronoun resolution. Overall, four categories of training and evaluation objectives have... | Yordan Yordanov, OanaMaria Camburu, Vid Kocijan, Thomas Lukasiewicz |  |
| 887 |  |  [On Losses for Modern Language Models](https://doi.org/10.18653/v1/2020.emnlp-main.403) |  | 0 | BERT set many state-of-the-art results over varied NLU benchmarks by pre-training over two tasks: masked language modelling (MLM) and next sentence prediction (NSP), the latter of which has been highly criticized. In this paper, we 1) clarify NSP’s effect on BERT pre-training, 2) explore fourteen... | Stephane ArocaOuellette, Frank Rudzicz |  |
| 888 |  |  [We Can Detect Your Bias: Predicting the Political Ideology of News Articles](https://doi.org/10.18653/v1/2020.emnlp-main.404) |  | 0 | We explore the task of predicting the leading political ideology or bias of news articles. First, we collect and release a large dataset of 34,737 articles that were manually annotated for political ideology –left, center, or right–, which is well-balanced across both topics and media. We further... | Ramy Baly, Giovanni Da San Martino, James R. Glass, Preslav Nakov |  |
| 889 |  |  [Semantic Label Smoothing for Sequence to Sequence Problems](https://doi.org/10.18653/v1/2020.emnlp-main.405) |  | 0 | Label smoothing has been shown to be an effective regularization strategy in classification, that prevents overfitting and helps in label de-noising. However, extending such methods directly to seq2seq settings, such as Machine Translation, is challenging: the large target output space of such... | Michal Lukasik, Himanshu Jain, Aditya Krishna Menon, Seungyeon Kim, Srinadh Bhojanapalli, Felix X. Yu, Sanjiv Kumar |  |
| 890 |  |  [Training for Gibbs Sampling on Conditional Random Fields with Neural Scoring Factors](https://doi.org/10.18653/v1/2020.emnlp-main.406) |  | 0 | Most recent improvements in NLP come from changes to the neural network architectures modeling the text input. Yet, state-of-the-art models often rely on simple approaches to model the label space, e.g. bigram Conditional Random Fields (CRFs) in sequence tagging. More expressive graphical models are... | Sida Gao, Matthew R. Gormley |  |
| 891 |  |  [Multilevel Text Alignment with Cross-Document Attention](https://doi.org/10.18653/v1/2020.emnlp-main.407) |  | 0 | Text alignment finds application in tasks such as citation recommendation and plagiarism detection. Existing alignment methods operate at a single, predefined level and cannot learn to align texts at, for example, sentence and document levels. We propose a new learning approach that equips... | Xuhui Zhou, Nikolaos Pappas, Noah A. Smith |  |
| 892 |  |  [Conversational Semantic Parsing](https://doi.org/10.18653/v1/2020.emnlp-main.408) |  | 0 | The structured representation for semantic parsing in task-oriented assistant systems is geared towards simple understanding of one-turn queries. Due to the limitations of the representation, the session-based properties such as co-reference resolution and context carryover are processed downstream... | Armen Aghajanyan, Jean Maillard, Akshat Shrivastava, Keith Diedrick, Michael Haeger, Haoran Li, Yashar Mehdad, Veselin Stoyanov, Anuj Kumar, Mike Lewis, Sonal Gupta |  |
| 893 |  |  [Probing Task-Oriented Dialogue Representation from Language Models](https://doi.org/10.18653/v1/2020.emnlp-main.409) |  | 0 | This paper investigates pre-trained language models to find out which model intrinsically carries the most informative representation for task-oriented dialogue tasks. We approach the problem from two aspects: supervised classifier probe and unsupervised mutual information probe. We fine-tune a... | ChienSheng Wu, Caiming Xiong |  |
| 894 |  |  [End-to-End Slot Alignment and Recognition for Cross-Lingual NLU](https://doi.org/10.18653/v1/2020.emnlp-main.410) |  | 0 | Natural language understanding (NLU) in the context of goal-oriented dialog systems typically includes intent classification and slot labeling tasks. Existing methods to expand an NLU system to new languages use machine translation with slot label projection from source to the translated utterances,... | Weijia Xu, Batool Haider, Saab Mansour |  |
| 895 |  |  [Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference](https://doi.org/10.18653/v1/2020.emnlp-main.411) |  | 0 | Intent detection is one of the core components of goal-oriented dialog systems, and detecting out-of-scope (OOS) intents is also a practically important skill. Few-shot learning is attracting much attention to mitigate data scarcity, but OOS detection becomes even more challenging. In this paper, we... | JianGuo Zhang, Kazuma Hashimoto, Wenhao Liu, ChienSheng Wu, Yao Wan, Philip S. Yu, Richard Socher, Caiming Xiong |  |
| 896 |  |  [Simple Data Augmentation with the Mask Token Improves Domain Adaptation for Dialog Act Tagging](https://doi.org/10.18653/v1/2020.emnlp-main.412) |  | 0 | The concept of Dialogue Act (DA) is universal across different task-oriented dialogue domains - the act of “request” carries the same speaker intention whether it is for restaurant reservation or flight booking. However, DA taggers trained on one domain do not generalize well to other domains, which... | Semih Yavuz, Kazuma Hashimoto, Wenhao Liu, Nitish Shirish Keskar, Richard Socher, Caiming Xiong |  |
| 897 |  |  [Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic Parsing](https://doi.org/10.18653/v1/2020.emnlp-main.413) |  | 0 | Task-oriented semantic parsing is a critical component of virtual assistants, which is responsible for understanding the user’s intents (set reminder, play music, etc.). Recent advances in deep learning have enabled several approaches to successfully parse more complex queries (Gupta et al., 2018;... | Xilun Chen, Asish Ghoshal, Yashar Mehdad, Luke Zettlemoyer, Sonal Gupta |  |
| 898 |  |  [Sound Natural: Content Rephrasing in Dialog Systems](https://doi.org/10.18653/v1/2020.emnlp-main.414) |  | 0 | We introduce a new task of rephrasing for a more natural virtual assistant. Currently, virtual assistants work in the paradigm of intent-slot tagging and the slot values are directly passed as-is to the execution engine. However, this setup fails in some scenarios such as messaging when the query... | Arash Einolghozati, Anchit Gupta, Keith Diedrick, Sonal Gupta |  |
| 899 |  |  [Zero-Shot Crosslingual Sentence Simplification](https://doi.org/10.18653/v1/2020.emnlp-main.415) |  | 0 | Sentence simplification aims to make sentences easier to read and understand. Recent approaches have shown promising results with encoder-decoder models trained on large amounts of parallel data which often only exists in English. We propose a zero-shot modeling framework which transfers... | Jonathan Mallinson, Rico Sennrich, Mirella Lapata |  |
| 900 |  |  [Facilitating the Communication of Politeness through Fine-Grained Paraphrasing](https://doi.org/10.18653/v1/2020.emnlp-main.416) |  | 0 | Aided by technology, people are increasingly able to communicate across geographical, cultural, and language barriers. This ability also results in new challenges, as interlocutors need to adapt their communication approaches to increasingly diverse circumstances. In this work, we take the first... | Liye Fu, Susan R. Fussell, Cristian DanescuNiculescuMizil |  |
| 901 |  |  [CAT-Gen: Improving Robustness in NLP Models via Controlled Adversarial Text Generation](https://doi.org/10.18653/v1/2020.emnlp-main.417) |  | 0 | NLP models are shown to suffer from robustness issues, i.e., a model’s prediction can be easily changed under small perturbations to the input. In this work, we present a Controlled Adversarial Text Generation (CAT-Gen) model that, given an input text, generates adversarial texts through... | Tianlu Wang, Xuezhi Wang, Yao Qin, Ben Packer, Kang Li, Jilin Chen, Alex Beutel, Ed H. Chi |  |
| 902 |  |  [Seq2Edits: Sequence Transduction Using Span-level Edit Operations](https://doi.org/10.18653/v1/2020.emnlp-main.418) |  | 0 | We propose Seq2Edits, an open-vocabulary approach to sequence editing for natural language processing (NLP) tasks with a high degree of overlap between input and output texts. In this approach, each sequence-to-sequence transduction is represented as a sequence of edit operations, where each... | Felix Stahlberg, Shankar Kumar |  |
| 903 |  |  [Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies](https://doi.org/10.18653/v1/2020.emnlp-main.419) |  | 0 | We study the degree to which neural sequence-to-sequence models exhibit fine-grained controllability when performing natural language generation from a meaning representation. Using two task-oriented dialogue generation benchmarks, we systematically compare the effect of four input linearization... | Chris Kedzie, Kathleen R. McKeown |  |
| 904 |  |  [Blank Language Models](https://doi.org/10.18653/v1/2020.emnlp-main.420) |  | 0 | We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks. The blanks control which part of the sequence to expand, making BLM ideal for a variety of text editing and rewriting tasks. The model can start from a single blank or partially... | Tianxiao Shen, Victor Quach, Regina Barzilay, Tommi S. Jaakkola |  |
| 905 |  |  [COD3S: Diverse Generation with Discrete Semantic Signatures](https://doi.org/10.18653/v1/2020.emnlp-main.421) |  | 0 | We present COD3S, a novel method for generating semantically diverse sentences using neural sequence-to-sequence (seq2seq) models. Conditioned on an input, seq2seqs typically produce semantically and syntactically homogeneous sets of sentences and thus perform poorly on one-to-many sequence... | Nathaniel Weir, João Sedoc, Benjamin Van Durme |  |
| 906 |  |  [Automatic Extraction of Rules Governing Morphological Agreement](https://doi.org/10.18653/v1/2020.emnlp-main.422) |  | 0 | Creating a descriptive grammar of a language is an indispensable step for language documentation and preservation. However, at the same time it is a tedious, time-consuming task. In this paper, we take steps towards automating this process by devising an automated framework for extracting a... | Aditi Chaudhary, Antonios Anastasopoulos, Adithya Pratapa, David R. Mortensen, Zaid Sheikh, Yulia Tsvetkov, Graham Neubig |  |
| 907 |  |  [Tackling the Low-resource Challenge for Canonical Segmentation](https://doi.org/10.18653/v1/2020.emnlp-main.423) |  | 0 | Canonical morphological segmentation consists of dividing words into their standardized morphemes. Here, we are interested in approaches for the task when training data is limited. We compare model performance in a simulated low-resource setting for the high-resource languages German, English, and... | Manuel Mager, Özlem Çetinoglu, Katharina Kann |  |
| 908 |  |  [IGT2P: From Interlinear Glossed Texts to Paradigms](https://doi.org/10.18653/v1/2020.emnlp-main.424) |  | 0 | An intermediate step in the linguistic analysis of an under-documented language is to find and organize inflected forms that are attested in natural speech. From this data, linguists generate unseen inflected word forms in order to test hypotheses about the language’s inflectional patterns and to... | Sarah R. Moeller, Ling Liu, Changbing Yang, Katharina Kann, Mans Hulden |  |
| 909 |  |  [A Computational Approach to Understanding Empathy Expressed in Text-Based Mental Health Support](https://doi.org/10.18653/v1/2020.emnlp-main.425) |  | 0 | Empathy is critical to successful mental health support. Empathy measurement has predominantly occurred in synchronous, face-to-face settings, and may not translate to asynchronous, text-based contexts. Because millions of people use text-based platforms for mental health support, understanding... | Ashish Sharma, Adam S. Miner, David C. Atkins, Tim Althoff |  |
| 910 |  |  [Modeling Protagonist Emotions for Emotion-Aware Storytelling](https://doi.org/10.18653/v1/2020.emnlp-main.426) |  | 0 | Emotions and their evolution play a central role in creating a captivating story. In this paper, we present the first study on modeling the emotional trajectory of the protagonist in neural storytelling. We design methods that generate stories that adhere to given story titles and desired emotion... | Faeze Brahman, Snigdha Chaturvedi |  |
| 911 |  |  [Help! Need Advice on Identifying Advice](https://doi.org/10.18653/v1/2020.emnlp-main.427) |  | 0 | Humans use language to accomplish a wide variety of tasks - asking for and giving advice being one of them. In online advice forums, advice is mixed in with non-advice, like emotional support, and is sometimes stated explicitly, sometimes implicitly. Understanding the language of advice would equip... | Venkata Subrahmanyan Govindarajan, Benjamin T. Chen, Rebecca Warholic, Katrin Erk, Junyi Jessy Li |  |
| 912 |  |  [Quantifying Intimacy in Language](https://doi.org/10.18653/v1/2020.emnlp-main.428) |  | 0 | Intimacy is a fundamental aspect of how we relate to others in social settings. Language encodes the social information of intimacy through both topics and other more subtle cues (such as linguistic hedging and swearing). Here, we introduce a new computational framework for studying expressions of... | Jiaxin Pei, David Jurgens |  |
| 913 |  |  [Writing Strategies for Science Communication: Data and Computational Analysis](https://doi.org/10.18653/v1/2020.emnlp-main.429) |  | 0 | Communicating complex scientific ideas without misleading or overwhelming the public is challenging. While science communication guides exist, they rarely offer empirical evidence for how their strategies are used in practice. Writing strategies that can be automatically recognized could greatly... | Tal August, Lauren Kim, Katharina Reinecke, Noah A. Smith |  |
| 914 |  |  [Weakly Supervised Subevent Knowledge Acquisition](https://doi.org/10.18653/v1/2020.emnlp-main.430) |  | 0 | Subevents elaborate an event and widely exist in event descriptions. Subevent knowledge is useful for discourse analysis and event-centric applications. Acknowledging the scarcity of subevent knowledge, we propose a weakly supervised approach to extract subevent relation tuples from text and build... | Wenlin Yao, Zeyu Dai, Maitreyi Ramaswamy, Bonan Min, Ruihong Huang |  |
| 915 |  |  [Biomedical Event Extraction as Sequence Labeling](https://doi.org/10.18653/v1/2020.emnlp-main.431) |  | 0 | We introduce Biomedical Event Extraction as Sequence Labeling (BeeSL), a joint end-to-end neural information extraction model. BeeSL recasts the task as sequence labeling, taking advantage of a multi-label aware encoding strategy and jointly modeling the intermediate tasks via multi-task learning.... | Alan Ramponi, Rob van der Goot, Rosario Lombardo, Barbara Plank |  |
| 916 |  |  [Annotating Temporal Dependency Graphs via Crowdsourcing](https://doi.org/10.18653/v1/2020.emnlp-main.432) |  | 0 | We present the construction of a corpus of 500 Wikinews articles annotated with temporal dependency graphs (TDGs) that can be used to train systems to understand temporal relations in text. We argue that temporal dependency graphs, built on previous research on narrative times and temporal anaphora,... | Jiarui Yao, Haoling Qiu, Bonan Min, Nianwen Xue |  |
| 917 |  |  [Introducing a New Dataset for Event Detection in Cybersecurity Texts](https://doi.org/10.18653/v1/2020.emnlp-main.433) |  | 0 | Detecting cybersecurity events is necessary to keep us informed about the fast growing number of such events reported in text. In this work, we focus on the task of event detection (ED) to identify event trigger words for the cybersecurity domain. In particular, to facilitate the future research, we... | Hieu Man Duc Trong, DucTrong Le, Amir Pouran Ben Veyseh, Thuat Nguyen, Thien Huu Nguyen |  |
| 918 |  |  [CHARM: Inferring Personal Attributes from Conversations](https://doi.org/10.18653/v1/2020.emnlp-main.434) |  | 0 | Personal knowledge about users’ professions, hobbies, favorite food, and travel preferences, among others, is a valuable asset for individualized AI, such as recommenders or chatbots. Conversations in social media, such as Reddit, are a rich source of data for inferring personal facts. Prior work... | Anna Tigunova, Andrew Yates, Paramita Mirza, Gerhard Weikum |  |
| 919 |  |  [Event Detection: Gate Diversity and Syntactic Importance Scores for Graph Convolution Neural Networks](https://doi.org/10.18653/v1/2020.emnlp-main.435) |  | 0 | Recent studies on event detection (ED) have shown that the syntactic dependency graph can be employed in graph convolution neural networks (GCN) to achieve state-of-the-art performance. However, the computation of the hidden vectors in such graph-based models is agnostic to the trigger candidate... | Viet Dac Lai, Tuan Ngo Nguyen, Thien Huu Nguyen |  |
| 920 |  |  [Severing the Edge Between Before and After: Neural Architectures for Temporal Ordering of Events](https://doi.org/10.18653/v1/2020.emnlp-main.436) |  | 0 | In this paper, we propose a neural architecture and a set of training methods for ordering events by predicting temporal relations. Our proposed models receive a pair of events within a span of text as input and they identify temporal relations (Before, After, Equal, Vague) between them. Given that... | Miguel Ballesteros, Rishita Anubhai, Shuai Wang, Nima Pourdamghani, Yogarshi Vyas, Jie Ma, Parminder Bhatia, Kathleen R. McKeown, Yaser AlOnaizan |  |
| 921 |  |  [How Much Knowledge Can You Pack Into the Parameters of a Language Model?](https://doi.org/10.18653/v1/2020.emnlp-main.437) |  | 0 | It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without... | Adam Roberts, Colin Raffel, Noam Shazeer |  |
| 922 |  |  [EXAMS: A Multi-subject High School Examinations Dataset for Cross-lingual and Multilingual Question Answering](https://doi.org/10.18653/v1/2020.emnlp-main.438) |  | 0 | We propose EXAMS – a new benchmark dataset for cross-lingual and multilingual question answering for high school examinations. We collected more than 24,000 high-quality high school exam questions in 16 languages, covering 8 language families and 24 school subjects from Natural Sciences and Social... | Momchil Hardalov, Todor Mihaylov, Dimitrina Zlatkova, Yoan Dinkov, Ivan Koychev, Preslav Nakov |  |
| 923 |  |  [End-to-End Synthetic Data Generation for Domain Adaptation of Question Answering Systems](https://doi.org/10.18653/v1/2020.emnlp-main.439) |  | 0 | We propose an end-to-end approach for synthetic QA data generation. Our model comprises a single transformer-based encoder-decoder network that is trained end-to-end to generate both answers and questions. In a nutshell, we feed a passage to the encoder and ask the decoder to generate a question and... | Siamak Shakeri, Cícero Nogueira dos Santos, Henghui Zhu, Patrick Ng, Feng Nan, Zhiguo Wang, Ramesh Nallapati, Bing Xiang |  |
| 924 |  |  [Multi-Stage Pre-training for Low-Resource Domain Adaptation](https://doi.org/10.18653/v1/2020.emnlp-main.440) |  | 0 | Transfer learning techniques are particularly useful for NLP tasks where a sizable amount of high-quality annotated data is difficult to obtain. Current approaches directly adapt a pretrained language model (LM) on in-domain text before fine-tuning to downstream tasks. We show that extending the... | Rong Zhang, Revanth Gangi Reddy, Md. Arafat Sultan, Vittorio Castelli, Anthony Ferritto, Radu Florian, Efsun Sarioglu Kayi, Salim Roukos, Avirup Sil, Todd Ward |  |
| 925 |  |  [ISAAQ - Mastering Textbook Questions with Pre-trained Transformers and Bottom-Up and Top-Down Attention](https://doi.org/10.18653/v1/2020.emnlp-main.441) |  | 0 | Textbook Question Answering is a complex task in the intersection of Machine Comprehension and Visual Question Answering that requires reasoning with multimodal information from text and diagrams. For the first time, this paper taps on the potential of transformer language models and bottom-up and... | José Manuél GómezPérez, Raúl Ortega |  |
| 926 |  |  [SubjQA: A Dataset for Subjectivity and Review Comprehension](https://doi.org/10.18653/v1/2020.emnlp-main.442) |  | 0 | Subjectivity is the expression of internal opinions or beliefs which cannot be objectively observed or verified, and has been shown to be important for sentiment analysis and word-sense disambiguation. Furthermore, subjectivity is an important aspect of user-generated data. In spite of this,... | Johannes Bjerva, Nikita Bhutani, Behzad Golshan, WangChiew Tan, Isabelle Augenstein |  |
| 927 |  |  [Widget Captioning: Generating Natural Language Description for Mobile User Interface Elements](https://doi.org/10.18653/v1/2020.emnlp-main.443) |  | 0 | Natural language descriptions of user interface (UI) elements such as alternative text are crucial for accessibility and language-based interaction in general. Yet, these descriptions are constantly missing in mobile UIs. We propose widget captioning, a novel task for automatically generating... | Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, Zhiwei Guan |  |
| 928 |  |  [Unsupervised Natural Language Inference via Decoupled Multimodal Contrastive Learning](https://doi.org/10.18653/v1/2020.emnlp-main.444) |  | 0 | We propose to solve the natural language inference problem without any supervision from the inference labels via task-agnostic multimodal pretraining. Although recent studies of multimodal self-supervised learning also represent the linguistic and visual context, their encoders for different... | Wanyun Cui, Guangyu Zheng, Wei Wang |  |
| 929 |  |  [Digital Voicing of Silent Speech](https://doi.org/10.18653/v1/2020.emnlp-main.445) |  | 0 | In this paper, we consider the task of digitally voicing silent speech, where silently mouthed words are converted to audible speech based on electromyography (EMG) sensor measurements that capture muscle impulses. While prior work has focused on training speech synthesis models from EMG collected... | David Gaddy, Dan Klein |  |
| 930 |  |  [Imitation Attacks and Defenses for Black-box Machine Translation Systems](https://doi.org/10.18653/v1/2020.emnlp-main.446) |  | 0 | Adversaries may look to steal or attack black-box NLP systems, either for financial gain or to exploit model errors. One setting of particular interest is machine translation (MT), where models have high commercial value and errors can be costly. We investigate possible exploitations of black-box MT... | Eric Wallace, Mitchell Stern, Dawn Song |  |
| 931 |  |  [Sequence-Level Mixed Sample Data Augmentation](https://doi.org/10.18653/v1/2020.emnlp-main.447) |  | 0 | Despite their empirical success, neural networks still have difficulty capturing compositional aspects of natural language. This work proposes a simple data augmentation approach to encourage compositional behavior in neural models for sequence-to-sequence problems. Our approach, SeqMix, creates new... | Demi Guo, Yoon Kim, Alexander M. Rush |  |
| 932 |  |  [Consistency of a Recurrent Language Model With Respect to Incomplete Decoding](https://doi.org/10.18653/v1/2020.emnlp-main.448) |  | 0 | Despite strong performance on a variety of tasks, neural sequence models trained with maximum likelihood have been shown to exhibit issues such as length bias and degenerate repetition. We study the related issue of receiving infinite-length sequences from a recurrent language model when using... | Sean Welleck, Ilia Kulikov, Jaedeok Kim, Richard Yuanzhe Pang, Kyunghyun Cho |  |
| 933 |  |  [An Exploration of Arbitrary-Order Sequence Labeling via Energy-Based Inference Networks](https://doi.org/10.18653/v1/2020.emnlp-main.449) |  | 0 | Many tasks in natural language processing involve predicting structured outputs, e.g., sequence labeling, semantic role labeling, parsing, and machine translation. Researchers are increasingly applying deep representation learning to these problems, but the structured component of these approaches... | Lifu Tu, Tianyu Liu, Kevin Gimpel |  |
| 934 |  |  [Ensemble Distillation for Structured Prediction: Calibrated, Accurate, Fast - Choose Three](https://doi.org/10.18653/v1/2020.emnlp-main.450) |  | 0 | Modern neural networks do not always produce well-calibrated predictions, even when trained with a proper scoring function such as cross-entropy. In classification settings, simple methods such as isotonic regression or temperature scaling may be used in conjunction with a held-out dataset to... | Steven Reich, David Mueller, Nicholas Andrews |  |
| 935 |  |  [Inducing Target-Specific Latent Structures for Aspect Sentiment Classification](https://doi.org/10.18653/v1/2020.emnlp-main.451) |  | 0 | Aspect-level sentiment analysis aims to recognize the sentiment polarity of an aspect or a target in a comment. Recently, graph convolutional networks based on linguistic dependency trees have been studied for this task. However, the dependency parsing accuracy of commercial product comments or... | Chenhua Chen, Zhiyang Teng, Yue Zhang |  |
| 936 |  |  [Affective Event Classification with Discourse-enhanced Self-training](https://doi.org/10.18653/v1/2020.emnlp-main.452) |  | 0 | Prior research has recognized the need to associate affective polarities with events and has produced several techniques and lexical resources for identifying affective events. Our research introduces new classification models to assign affective polarity to event phrases. First, we present a... | Yuan Zhuang, Tianyu Jiang, Ellen Riloff |  |
| 937 |  |  [Deep Weighted MaxSAT for Aspect-based Opinion Extraction](https://doi.org/10.18653/v1/2020.emnlp-main.453) |  | 0 | Though deep learning has achieved significant success in various NLP tasks, most deep learning models lack the capability of encoding explicit domain knowledge to model complex causal relationships among different types of variables. On the other hand, logic rules offer a compact expression to... | Meixi Wu, Wenya Wang, Sinno Jialin Pan |  |
| 938 |  |  [Multi-view Story Characterization from Movie Plot Synopses and Reviews](https://doi.org/10.18653/v1/2020.emnlp-main.454) |  | 0 | This paper considers the problem of characterizing stories by inferring properties such as theme and style using written synopses and reviews of movies. We experiment with a multi-label dataset of movie synopses and a tagset representing various attributes of stories (e.g., genre, type of events).... | Sudipta Kar, Gustavo Aguilar, Mirella Lapata, Thamar Solorio |  |
| 939 |  |  [Mind Your Inflections! Improving NLP for Non-Standard Englishes with Base-Inflection Encoding](https://doi.org/10.18653/v1/2020.emnlp-main.455) |  | 0 | Inflectional variation is a common feature of World Englishes such as Colloquial Singapore English and African American Vernacular English. Although comprehension by human readers is usually unimpaired by non-standard inflections, current NLP systems are not yet robust. We propose Base-Inflection... | Samson Tan, Shafiq R. Joty, Lav R. Varshney, MinYen Kan |  |
| 940 |  |  [Measuring the Similarity of Grammatical Gender Systems by Comparing Partitions](https://doi.org/10.18653/v1/2020.emnlp-main.456) |  | 0 | A grammatical gender system divides a lexicon into a small number of relatively fixed grammatical categories. How similar are these gender systems across languages? To quantify the similarity, we define gender systems extensionally, thereby reducing the problem of comparisons between languages’... | Arya D. McCarthy, Adina Williams, Shijia Liu, David Yarowsky, Ryan Cotterell |  |
| 941 |  |  [RethinkCWS: Is Chinese Word Segmentation a Solved Task?](https://doi.org/10.18653/v1/2020.emnlp-main.457) |  | 0 | The performance of the Chinese Word Segmentation (CWS) systems has gradually reached a plateau with the rapid development of deep neural networks, especially the successful use of large pre-trained models. In this paper, we take stock of what we have achieved and rethink what’s left in the CWS task.... | Jinlan Fu, Pengfei Liu, Qi Zhang, Xuanjing Huang |  |
| 942 |  |  [Learning to Pronounce Chinese Without a Pronunciation Dictionary](https://doi.org/10.18653/v1/2020.emnlp-main.458) |  | 0 | We demonstrate a program that learns to pronounce Chinese text in Mandarin, without a pronunciation dictionary. From non-parallel streams of Chinese characters and Chinese pinyin syllables, it establishes a many-to-many mapping between characters and pronunciations. Using unsupervised methods, the... | Christopher Chu, Scot Fang, Kevin Knight |  |
| 943 |  |  [Dynamic Anticipation and Completion for Multi-Hop Reasoning over Sparse Knowledge Graph](https://doi.org/10.18653/v1/2020.emnlp-main.459) |  | 0 | Multi-hop reasoning has been widely studied in recent years to seek an effective and interpretable method for knowledge graph (KG) completion. Most previous reasoning methods are designed for dense KGs with enough paths between entities, but cannot work well on those sparse KGs that only contain... | Xin Lv, Xu Han, Lei Hou, Juanzi Li, Zhiyuan Liu, Wei Zhang, Yichi Zhang, Hao Kong, Suhui Wu |  |
| 944 |  |  [Knowledge Association with Hyperbolic Knowledge Graph Embeddings](https://doi.org/10.18653/v1/2020.emnlp-main.460) |  | 0 | Capturing associations for knowledge graphs (KGs) through entity alignment, entity type inference and other related tasks benefits NLP applications with comprehensive knowledge representations. Recent related methods built on Euclidean embeddings are challenged by the hierarchical structures and... | Zequn Sun, Muhao Chen, Wei Hu, Chengming Wang, Jian Dai, Wei Zhang |  |
| 945 |  |  [Domain Knowledge Empowered Structured Neural Net for End-to-End Event Temporal Relation Extraction](https://doi.org/10.18653/v1/2020.emnlp-main.461) |  | 0 | Extracting event temporal relations is a critical task for information extraction and plays an important role in natural language understanding. Prior systems leverage deep learning and pre-trained language models to improve the performance of the task. However, these systems often suffer from two... | Rujun Han, Yichao Zhou, Nanyun Peng |  |
| 946 |  |  [TeMP: Temporal Message Passing for Temporal Knowledge Graph Completion](https://doi.org/10.18653/v1/2020.emnlp-main.462) |  | 0 | Inferring missing facts in temporal knowledge graphs (TKGs) is a fundamental and challenging task. Previous works have approached this problem by augmenting methods for static knowledge graphs to leverage time-dependent representations. However, these methods do not explicitly leverage multi-hop... | Jiapeng Wu, Meng Cao, Jackie Chi Kit Cheung, William L. Hamilton |  |
| 947 |  |  [Understanding the Difficulty of Training Transformers](https://doi.org/10.18653/v1/2020.emnlp-main.463) |  | 0 | Transformers have proved effective in many NLP tasks. However, their training requires non-trivial efforts regarding carefully designing cutting-edge optimizers and learning rate schedulers (e.g., conventional SGD fails to train Transformers effectively). Our objective here is to understand __what... | Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, Jiawei Han |  |
| 948 |  |  [An Empirical Study of Generation Order for Machine Translation](https://doi.org/10.18653/v1/2020.emnlp-main.464) |  | 0 | In this work, we present an empirical study of generation order for machine translation. Building on recent advances in insertion-based modeling, we first introduce a soft order-reward framework that enables us to train models to follow arbitrary oracle generation policies. We then make use of this... | William Chan, Mitchell Stern, Jamie Kiros, Jakob Uszkoreit |  |
| 949 |  |  [Inference Strategies for Machine Translation with Conditional Masking](https://doi.org/10.18653/v1/2020.emnlp-main.465) |  | 0 | Conditional masked language model (CMLM) training has proven successful for non-autoregressive and semi-autoregressive sequence generation tasks, such as machine translation. Given a trained CMLM, however, it is not clear what the best inference strategy is. We formulate masked inference as a... | Julia Kreutzer, George F. Foster, Colin Cherry |  |
| 950 |  |  [AmbigQA: Answering Ambiguous Open-domain Questions](https://doi.org/10.18653/v1/2020.emnlp-main.466) |  | 0 | Ambiguity is inherent to open-domain question answering; especially when exploring new topics, it can be difficult to ask questions that have a single, unambiguous answer. In this paper, we introduce AmbigQA, a new open-domain question answering task which involves finding every plausible answer,... | Sewon Min, Julian Michael, Hannaneh Hajishirzi, Luke Zettlemoyer |  |
| 951 |  |  [Tell Me How to Ask Again: Question Data Augmentation with Controllable Rewriting in Continuous Space](https://doi.org/10.18653/v1/2020.emnlp-main.467) |  | 0 | In this paper, we propose a novel data augmentation method, referred to as Controllable Rewriting based Question Data Augmentation (CRQDA), for machine reading comprehension (MRC), question generation, and question-answering natural language inference tasks. We treat the question data augmentation... | Dayiheng Liu, Yeyun Gong, Jie Fu, Yu Yan, Jiusheng Chen, Jiancheng Lv, Nan Duan, Ming Zhou |  |
| 952 |  |  [Training Question Answering Models From Synthetic Data](https://doi.org/10.18653/v1/2020.emnlp-main.468) |  | 0 | Question and answer generation is a data augmentation method that aims to improve question answering (QA) models given the limited amount of human labeled data. However, a considerable gap remains between synthetic and human-generated question-answer pairs. This work aims to narrow this gap by... | Raul Puri, Ryan Spring, Mohammad Shoeybi, Mostofa Patwary, Bryan Catanzaro |  |
| 953 |  |  [Few-Shot Complex Knowledge Base Question Answering via Meta Reinforcement Learning](https://doi.org/10.18653/v1/2020.emnlp-main.469) |  | 0 | Complex question-answering (CQA) involves answering complex natural-language questions on a knowledge base (KB). However, the conventional neural program induction (NPI) approach exhibits uneven performance when the questions have different types, harboring inherently different characteristics,... | Yuncheng Hua, YuanFang Li, Gholamreza Haffari, Guilin Qi, Tongtong Wu |  |
| 954 |  |  [Multilingual Offensive Language Identification with Cross-lingual Embeddings](https://doi.org/10.18653/v1/2020.emnlp-main.470) |  | 0 | Offensive content is pervasive in social media and a reason for concern to companies and government organizations. Several studies have been recently published investigating methods to detect the various forms of such content (e.g. hate speech, cyberbulling, and cyberaggression). The clear majority... | Tharindu Ranasinghe, Marcos Zampieri |  |
| 955 |  |  [Solving Historical Dictionary Codes with a Neural Language Model](https://doi.org/10.18653/v1/2020.emnlp-main.471) |  | 0 | We solve difficult word-based substitution codes by constructing a decoding lattice and searching that lattice with a neural language model. We apply our method to a set of enciphered letters exchanged between US Army General James Wilkinson and agents of the Spanish Crown in the late 1700s and... | Christopher Chu, Raphael Valenti, Kevin Knight |  |
| 956 |  |  [Toward Micro-Dialect Identification in Diaglossic and Code-Switched Environments](https://doi.org/10.18653/v1/2020.emnlp-main.472) |  | 0 | Although prediction of dialects is an important language processing task, with a wide range of applications, existing work is largely limited to coarse-grained varieties. Inspired by geolocation research, we propose the novel task of Micro-Dialect Identification (MDI) and introduce MARBERT, a new... | Muhammad AbdulMageed, Chiyu Zhang, AbdelRahim A. Elmadany, Lyle H. Ungar |  |
| 957 |  |  [Investigating African-American Vernacular English in Transformer-Based Text Generation](https://doi.org/10.18653/v1/2020.emnlp-main.473) |  | 0 | The growth of social media has encouraged the written use of African American Vernacular English (AAVE), which has traditionally been used only in oral contexts. However, NLP models have historically been developed using dominant English varieties, such as Standard American English (SAE), due to... | Sophie Groenwold, Lily Ou, Aesha Parekh, Samhita Honnavalli, Sharon Levy, Diba Mirza, William Yang Wang |  |
| 958 |  |  [Iterative Domain-Repaired Back-Translation](https://doi.org/10.18653/v1/2020.emnlp-main.474) |  | 0 | In this paper, we focus on the domain-specific translation with low resources, where in-domain parallel corpora are scarce or nonexistent. One common and effective strategy for this case is exploiting in-domain monolingual data with the back-translation method. However, the synthetic parallel data... | HaoRan Wei, Zhirui Zhang, Boxing Chen, Weihua Luo |  |
| 959 |  |  [Dynamic Data Selection and Weighting for Iterative Back-Translation](https://doi.org/10.18653/v1/2020.emnlp-main.475) |  | 0 | Back-translation has proven to be an effective method to utilize monolingual data in neural machine translation (NMT), and iteratively conducting back-translation can further improve the model performance. Selecting which monolingual data to back-translate is crucial, as we require that the... | ZiYi Dou, Antonios Anastasopoulos, Graham Neubig |  |
| 960 |  |  [Revisiting Modularized Multilingual NMT to Meet Industrial Demands](https://doi.org/10.18653/v1/2020.emnlp-main.476) |  | 0 | The complete sharing of parameters for multilingual translation (1-1) has been the mainstream approach in current research. However, degraded performance due to the capacity bottleneck and low maintainability hinders its extensive adoption in industries. In this study, we revisit the multilingual... | Sungwon Lyu, Bokyung Son, Kichang Yang, Jaekyoung Bae |  |
| 961 |  |  [LAReQA: Language-Agnostic Answer Retrieval from a Multilingual Pool](https://doi.org/10.18653/v1/2020.emnlp-main.477) |  | 0 | We present LAReQA, a challenging new benchmark for language-agnostic answer retrieval from a multilingual candidate pool. Unlike previous cross-lingual tasks, LAReQA tests for “strong” cross-lingual alignment, requiring semantically related cross-language pairs to be closer in representation space... | Uma Roy, Noah Constant, Rami AlRfou, Aditya Barua, Aaron Phillips, Yinfei Yang |  |
| 962 |  |  [OCR Post Correction for Endangered Language Texts](https://doi.org/10.18653/v1/2020.emnlp-main.478) |  | 0 | There is little to no data available to build natural language processing models for most endangered languages. However, textual data in these languages often exists in formats that are not machine-readable, such as paper books and scanned images. In this work, we address the task of extracting text... | Shruti Rijhwani, Antonios Anastasopoulos, Graham Neubig |  |
| 963 |  |  [X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models](https://doi.org/10.18653/v1/2020.emnlp-main.479) |  | 0 | Language models (LMs) have proven surprisingly successful at capturing factual knowledge by completing cloze-style fill-in-the-blank questions such as “Punta Cana is located in _.” However, while knowledge is both written and queried in many languages, studies on LMs’ factual representation ability... | Zhengbao Jiang, Antonios Anastasopoulos, Jun Araki, Haibo Ding, Graham Neubig |  |
| 964 |  |  [CCAligned: A Massive Collection of Cross-Lingual Web-Document Pairs](https://doi.org/10.18653/v1/2020.emnlp-main.480) |  | 0 | Cross-lingual document alignment aims to identify pairs of documents in two distinct languages that are of comparable content or translations of each other. In this paper, we exploit the signals embedded in URLs to label web documents at scale with an average precision of 94.5% across different... | Ahmed ElKishky, Vishrav Chaudhary, Francisco Guzmán, Philipp Koehn |  |
| 965 |  |  [Localizing Open-Ontology QA Semantic Parsers in a Day Using Machine Translation](https://doi.org/10.18653/v1/2020.emnlp-main.481) |  | 0 | We propose Semantic Parser Localizer (SPL), a toolkit that leverages Neural Machine Translation (NMT) systems to localize a semantic parser for a new language. Our methodology is to (1) generate training data automatically in the target language by augmenting machine-translated datasets with local... | Mehrad Moradshahi, Giovanni Campagna, Sina J. Semnani, Silei Xu, Monica S. Lam |  |
| 966 |  |  [Interactive Refinement of Cross-Lingual Word Embeddings](https://doi.org/10.18653/v1/2020.emnlp-main.482) |  | 0 | Cross-lingual word embeddings transfer knowledge between languages: models trained on high-resource languages can predict in low-resource languages. We introduce CLIME, an interactive system to quickly refine cross-lingual word embeddings for a given classification problem. First, CLIME ranks words... | Michelle Yuan, Mozhi Zhang, Benjamin Van Durme, Leah Findlater, Jordan L. BoydGraber |  |
| 967 |  |  [Exploiting Sentence Order in Document Alignment](https://doi.org/10.18653/v1/2020.emnlp-main.483) |  | 0 | We present a simple document alignment method that incorporates sentence order information in both candidate generation and candidate re-scoring. Our method results in 61% relative reduction in error compared to the best previously published result on the WMT16 document alignment shared task. Our... | Brian Thompson, Philipp Koehn |  |
| 968 |  |  [XGLUE: A New Benchmark Datasetfor Cross-lingual Pre-training, Understanding and Generation](https://doi.org/10.18653/v1/2020.emnlp-main.484) |  | 0 | In this paper, we introduce XGLUE, a new benchmark dataset to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora, and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to GLUE (Wang et al.,2019), which is labeled in English and... | Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang, Rahul Agrawal, Edward Cui, Sining Wei, Taroon Bharti, Ying Qiao, JiunHung Chen, Winnie Wu, Shuguang Liu, Fan Yang, Daniel Campos, Rangan Majumder, Ming Zhou |  |
| 969 |  |  [AIN: Fast and Accurate Sequence Labeling with Approximate Inference Network](https://doi.org/10.18653/v1/2020.emnlp-main.485) |  | 0 | The linear-chain Conditional Random Field (CRF) model is one of the most widely-used neural sequence labeling approaches. Exact probabilistic inference algorithms such as the forward-backward and Viterbi algorithms are typically applied in training and prediction stages of the CRF model. However,... | Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, Kewei Tu |  |
| 970 |  |  [HIT: Nested Named Entity Recognition via Head-Tail Pair and Token Interaction](https://doi.org/10.18653/v1/2020.emnlp-main.486) |  | 0 | Named Entity Recognition (NER) is a fundamental task in natural language processing. In order to identify entities with nested structure, many sophisticated methods have been recently developed based on either the traditional sequence labeling approaches or directed hypergraph structures. Despite... | Yu Wang, Yun Li, Hanghang Tong, Ziye Zhu |  |
| 971 |  |  [Supertagging Combinatory Categorial Grammar with Attentive Graph Convolutional Networks](https://doi.org/10.18653/v1/2020.emnlp-main.487) |  | 0 | Supertagging is conventionally regarded as an important task for combinatory categorial grammar (CCG) parsing, where effective modeling of contextual information is highly important to this task. However, existing studies have made limited efforts to leverage contextual features except for applying... | Yuanhe Tian, Yan Song, Fei Xia |  |
| 972 |  |  [DAGA: Data Augmentation with a Generation Approach forLow-resource Tagging Tasks](https://doi.org/10.18653/v1/2020.emnlp-main.488) |  | 0 | Data augmentation techniques have been widely used to improve machine learning performance as they facilitate generalization. In this work, we propose a novel augmentation method to generate high quality synthetic data for low-resource tagging tasks with language models trained on the linearized... | Bosheng Ding, Linlin Liu, Lidong Bing, Canasai Kruengkrai, Thien Hai Nguyen, Shafiq R. Joty, Luo Si, Chunyan Miao |  |
| 973 |  |  [Interpretable Multi-dataset Evaluation for Named Entity Recognition](https://doi.org/10.18653/v1/2020.emnlp-main.489) |  | 0 | With the proliferation of models for natural language processing tasks, it is even harder to understand the differences between models and their relative merits. Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 does not tell us why or how particular methods... | Jinlan Fu, Pengfei Liu, Graham Neubig |  |
| 974 |  |  [Adversarial Semantic Decoupling for Recognizing Open-Vocabulary Slots](https://doi.org/10.18653/v1/2020.emnlp-main.490) |  | 0 | Open-vocabulary slots, such as file name, album name, or schedule title, significantly degrade the performance of neural-based slot filling models since these slots can take on values from a virtually unlimited set and have no semantic restriction nor a length limit. In this paper, we propose a... | Yuanmeng Yan, Keqing He, Hong Xu, Sihong Liu, Fanyu Meng, Min Hu, Weiran Xu |  |
| 975 |  |  [Plug and Play Autoencoders for Conditional Text Generation](https://doi.org/10.18653/v1/2020.emnlp-main.491) |  | 0 | Text autoencoders are commonly used for conditional generation tasks such as style transfer. We propose methods which are plug and play, where any pretrained autoencoder can be used, and only require learning a mapping within the autoencoder’s embedding space, training embedding-to-embedding... | Florian Mai, Nikolaos Pappas, Ivan Montero, Noah A. Smith, James Henderson |  |
| 976 |  |  [Structure Aware Negative Sampling in Knowledge Graphs](https://doi.org/10.18653/v1/2020.emnlp-main.492) |  | 0 | Learning low-dimensional representations for entities and relations in knowledge graphs using contrastive estimation represents a scalable and effective method for inferring connectivity patterns. A crucial aspect of contrastive learning approaches is the choice of corruption distribution that... | Kian Ahrabian, Aarash Feizi, Yasmin Salehi, William L. Hamilton, Avishek Joey Bose |  |
| 977 |  |  [Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation](https://doi.org/10.18653/v1/2020.emnlp-main.493) |  | 0 | We propose a method to automatically generate a domain- and task-adaptive maskings of the given text for self-supervised pre-training, such that we can effectively adapt the language model to a particular target task (e.g. question answering). Specifically, we present a novel reinforcement... | Minki Kang, Moonsu Han, Sung Ju Hwang |  |
| 978 |  |  [Autoregressive Knowledge Distillation through Imitation Learning](https://doi.org/10.18653/v1/2020.emnlp-main.494) |  | 0 | The performance of autoregressive models on natural language generation tasks has dramatically improved due to the adoption of deep, self-attentive architectures. However, these gains have come at the cost of hindering inference speed, making state-of-the-art models cumbersome to deploy in... | Alexander Lin, Jeremy Wohlwend, Howard Chen, Tao Lei |  |
| 979 |  |  [T3: Tree-Autoencoder Constrained Adversarial Text Generation for Targeted Attack](https://doi.org/10.18653/v1/2020.emnlp-main.495) |  | 0 | Adversarial attacks against natural language processing systems, which perform seemingly innocuous modifications to inputs, can induce arbitrary mistakes to the target models. Though raised great concerns, such adversarial attacks can be leveraged to estimate the robustness of NLP models. Compared... | Boxin Wang, Hengzhi Pei, Boyuan Pan, Qian Chen, Shuohang Wang, Bo Li |  |
| 980 |  |  [Structured Pruning of Large Language Models](https://doi.org/10.18653/v1/2020.emnlp-main.496) |  | 0 | Large language models have recently achieved state of the art performance across a wide variety of natural language tasks. Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises an interesting question: do language models need... | Ziheng Wang, Jeremy Wohlwend, Tao Lei |  |
| 981 |  |  [Effective Unsupervised Domain Adaptation with Adversarially Trained Language Models](https://doi.org/10.18653/v1/2020.emnlp-main.497) |  | 0 | Recent work has shown the importance of adaptation of broad-coverage contextualised embedding models on the domain of the target task of interest. Current self-supervised adaptation methods are simplistic, as the training signal comes from a small percentage of randomly masked-out tokens. In this... | ThuyTrang Vu, Dinh Phung, Gholamreza Haffari |  |
| 982 |  |  [BAE: BERT-based Adversarial Examples for Text Classification](https://doi.org/10.18653/v1/2020.emnlp-main.498) |  | 0 | Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model. Recent works in NLP use rule-based synonym replacement strategies to generate adversarial examples. These strategies can... | Siddhant Garg, Goutham Ramakrishnan |  |
| 983 |  |  [Adversarial Self-Supervised Data-Free Distillation for Text Classification](https://doi.org/10.18653/v1/2020.emnlp-main.499) |  | 0 | Large pre-trained transformer-based language models have achieved impressive results on a wide range of NLP tasks. In the past few years, Knowledge Distillation(KD) has become a popular paradigm to compress a computationally expensive model to a resource-efficient lightweight model. However, most KD... | Xinyin Ma, Yongliang Shen, Gongfan Fang, Chen Chen, Chenghao Jia, Weiming Lu |  |
| 984 |  |  [BERT-ATTACK: Adversarial Attack Against BERT Using BERT](https://doi.org/10.18653/v1/2020.emnlp-main.500) |  | 0 | Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually adopt heuristic... | Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, Xipeng Qiu |  |
| 985 |  |  [The Thieves on Sesame Street are Polyglots - Extracting Multilingual Models from Monolingual APIs](https://doi.org/10.18653/v1/2020.emnlp-main.501) |  | 0 | Pre-training in natural language processing makes it easier for an adversary with only query access to a victim model to reconstruct a local copy of the victim by training with gibberish input data paired with the victim’s labels for that data. We discover that this extraction process extends to... | Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, Richard Socher |  |
| 986 |  |  [When Hearst Is not Enough: Improving Hypernymy Detection from Corpus with Distributional Models](https://doi.org/10.18653/v1/2020.emnlp-main.502) |  | 0 | We address hypernymy detection, i.e., whether an is-a relationship exists between words (x ,y), with the help of large textual corpora. Most conventional approaches to this task have been categorized to be either pattern-based or distributional. Recent studies suggest that pattern-based ones are... | Changlong Yu, Jialong Han, Peifeng Wang, Yangqiu Song, Hongming Zhang, Wilfred Ng, Shuming Shi |  |
| 987 |  |  [Interpreting Open-Domain Modifiers: Decomposition of Wikipedia Categories into Disambiguated Property-Value Pairs](https://doi.org/10.18653/v1/2020.emnlp-main.503) |  | 0 | This paper proposes an open-domain method for automatically annotating modifier constituents (20th-century’) within Wikipedia categories (20th-century male writers) with properties (date of birth). The annotations offer a semantically-anchored understanding of the role of the constituents in... | Marius Pasca |  |
| 988 |  |  [A Synset Relation-enhanced Framework with a Try-again Mechanism for Word Sense Disambiguation](https://doi.org/10.18653/v1/2020.emnlp-main.504) |  | 0 | Contextual embeddings are proved to be overwhelmingly effective to the task of Word Sense Disambiguation (WSD) compared with other sense representation techniques. However, these embeddings fail to embed sense knowledge in semantic networks. In this paper, we propose a Synset Relation-Enhanced... | Ming Wang, Yinglin Wang |  |
| 989 |  |  [Diverse, Controllable, and Keyphrase-Aware: A Corpus and Method for News Multi-Headline Generation](https://doi.org/10.18653/v1/2020.emnlp-main.505) |  | 0 | News headline generation aims to produce a short sentence to attract readers to read the news. One news article often contains multiple keyphrases that are of interest to different users, which can naturally have multiple reasonable headlines. However, most existing methods focus on the single... | Dayiheng Liu, Yeyun Gong, Yu Yan, Jie Fu, Bo Shao, Daxin Jiang, Jiancheng Lv, Nan Duan |  |
| 990 |  |  [Factual Error Correction for Abstractive Summarization Models](https://doi.org/10.18653/v1/2020.emnlp-main.506) |  | 0 | Neural abstractive summarization systems have achieved promising progress, thanks to the availability of large-scale datasets and models pre-trained with self-supervised methods. However, ensuring the factual consistency of the generated summaries for abstractive summarization systems is a... | Meng Cao, Yue Dong, Jiapeng Wu, Jackie Chi Kit Cheung |  |
| 991 |  |  [Compressive Summarization with Plausibility and Salience Modeling](https://doi.org/10.18653/v1/2020.emnlp-main.507) |  | 0 | Compressive summarization systems typically rely on a seed set of syntactic rules to determine under what circumstances deleting a span is permissible, then learn which compressions to actually apply by optimizing for ROUGE. In this work, we propose to relax these explicit syntactic constraints on... | Shrey Desai, Jiacheng Xu, Greg Durrett |  |
| 992 |  |  [Understanding Neural Abstractive Summarization Models via Uncertainty](https://doi.org/10.18653/v1/2020.emnlp-main.508) |  | 0 | An advantage of seq2seq abstractive summarization models is that they generate text in a free-form manner, but this flexibility makes it difficult to interpret model behavior. In this work, we analyze summarization decoders in both blackbox and whitebox ways by studying on the entropy, or... | Jiacheng Xu, Shrey Desai, Greg Durrett |  |
| 993 |  |  [Better Highlighting: Creating Sub-Sentence Summary Highlights](https://doi.org/10.18653/v1/2020.emnlp-main.509) |  | 0 | Amongst the best means to summarize is highlighting. In this paper, we aim to generate summary highlights to be overlaid on the original documents to make it easier for readers to sift through a large amount of text. The method allows summaries to be understood in context to prevent a summarizer... | Sangwoo Cho, Kaiqiang Song, Chen Li, Dong Yu, Hassan Foroosh, Fei Liu |  |
| 994 |  |  [Summarizing Text on Any Aspects: A Knowledge-Informed Weakly-Supervised Approach](https://doi.org/10.18653/v1/2020.emnlp-main.510) |  | 0 | Given a document and a target aspect (e.g., a topic of interest), aspect-based abstractive summarization attempts to generate a summary with respect to the aspect. Previous studies usually assume a small pre-defined set of aspects and fall short of summarizing on other diverse topics. In this work,... | Bowen Tan, Lianhui Qin, Eric P. Xing, Zhiting Hu |  |
| 995 |  |  [BERT-enhanced Relational Sentence Ordering Network](https://doi.org/10.18653/v1/2020.emnlp-main.511) |  | 0 | In this paper, we introduce a novel BERT-enhanced Relational Sentence Ordering Network (referred to as BRSON) by leveraging BERT for capturing better dependency relationship among sentences to enhance the coherence modeling for the entire paragraph. In particular, we develop a new Relational Pointer... | Baiyun Cui, Yingming Li, Zhongfei Zhang |  |
| 996 |  |  [Online Conversation Disentanglement with Pointer Networks](https://doi.org/10.18653/v1/2020.emnlp-main.512) |  | 0 | Huge amounts of textual conversations occur online every day, where multiple conversations take place concurrently. Interleaved conversations lead to difficulties in not only following the ongoing discussions but also extracting relevant information from simultaneous messages. Conversation... | Tao Yu, Shafiq R. Joty |  |
| 997 |  |  [VCDM: Leveraging Variational Bi-encoding and Deep Contextualized Word Representations for Improved Definition Modeling](https://doi.org/10.18653/v1/2020.emnlp-main.513) |  | 0 | In this paper, we tackle the task of definition modeling, where the goal is to learn to generate definitions of words and phrases. Existing approaches for this task are discriminative, combining distributional and lexical semantics in an implicit rather than direct way. To tackle this issue we... | Machel Reid, Edison MarreseTaylor, Yutaka Matsuo |  |
| 998 |  |  [Coarse-to-Fine Pre-training for Named Entity Recognition](https://doi.org/10.18653/v1/2020.emnlp-main.514) |  | 0 | More recently, Named Entity Recognition hasachieved great advances aided by pre-trainingapproaches such as BERT. However, currentpre-training techniques focus on building lan-guage modeling objectives to learn a gen-eral representation, ignoring the named entity-related knowledge. To this end, we... | Mengge Xue, Bowen Yu, Zhenyu Zhang, Tingwen Liu, Yue Zhang, Bin Wang |  |
| 999 |  |  [Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment](https://doi.org/10.18653/v1/2020.emnlp-main.515) |  | 0 | Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich content by linking the equivalent entities from various KGs. GNN-based EA methods present promising performance by modeling the KG structure defined by relation triples. However, attribute triples can also provide crucial... | Zhiyuan Liu, Yixin Cao, Liangming Pan, Juanzi Li, TatSeng Chua |  |
| 1000 |  |  [Simple and Effective Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning](https://doi.org/10.18653/v1/2020.emnlp-main.516) |  | 0 | We present a simple few-shot named entity recognition (NER) system based on nearest neighbor learning and structured inference. Our system uses a supervised NER model trained on the source domain, as a feature extractor. Across several test domains, we show that a nearest neighbor classifier in this... | Yi Yang, Arzoo Katiyar |  |
| 1001 |  |  [Learning Structured Representations of Entity Names using ActiveLearning and Weak Supervision](https://doi.org/10.18653/v1/2020.emnlp-main.517) |  | 0 | Structured representations of entity names are useful for many entity-related tasks such as entity normalization and variant generation. Learning the implicit structured representations of entity names without context and external knowledge is particularly challenging. In this paper, we present a... | Kun Qian, Poornima Chozhiyath Raman, Yunyao Li, Lucian Popa |  |
| 1002 |  |  [Entity Enhanced BERT Pre-training for Chinese NER](https://doi.org/10.18653/v1/2020.emnlp-main.518) |  | 0 | Character-level BERT pre-trained in Chinese suffers a limitation of lacking lexicon information, which shows effectiveness for Chinese NER. To integrate the lexicon into pre-trained LMs for Chinese NER, we investigate a semi-supervised entity enhanced BERT pre-training method. In particular, we... | Chen Jia, Yuefeng Shi, Qinrong Yang, Yue Zhang |  |
| 1003 |  |  [Scalable Zero-shot Entity Linking with Dense Entity Retrieval](https://doi.org/10.18653/v1/2020.emnlp-main.519) |  | 0 | This paper introduces a conceptually simple, scalable, and highly effective BERT-based entity linking model, along with an extensive evaluation of its accuracy-speed trade-off. We present a two-stage zero-shot linking algorithm, where each entity is defined only by a short textual description. The... | Ledell Wu, Fabio Petroni, Martin Josifoski, Sebastian Riedel, Luke Zettlemoyer |  |
| 1004 |  |  [A Dataset for Tracking Entities in Open Domain Procedural Text](https://doi.org/10.18653/v1/2020.emnlp-main.520) |  | 0 | We present the first dataset for tracking state changes in procedural text from arbitrary domains by using an unrestricted (open) vocabulary. For example, in a text describing fog removal using potatoes, a car window may transition between being foggy, sticky, opaque, and clear. Previous... | Niket Tandon, Keisuke Sakaguchi, Bhavana Dalvi, Dheeraj Rajagopal, Peter Clark, Michal Guerquin, Kyle Richardson, Eduard H. Hovy |  |
| 1005 |  |  [Design Challenges in Low-resource Cross-lingual Entity Linking](https://doi.org/10.18653/v1/2020.emnlp-main.521) |  | 0 | Cross-lingual Entity Linking (XEL), the problem of grounding mentions of entities in a foreign language text into an English knowledge base such as Wikipedia, has seen a lot of research in recent years, with a range of promising techniques. However, current techniques do not rise to the challenges... | Xingyu Fu, Weijia Shi, Xiaodong Yu, Zian Zhao, Dan Roth |  |
| 1006 |  |  [Efficient One-Pass End-to-End Entity Linking for Questions](https://doi.org/10.18653/v1/2020.emnlp-main.522) |  | 0 | We present ELQ, a fast end-to-end entity linking model for questions, which uses a biencoder to jointly perform mention detection and linking in one pass. Evaluated on WebQSP and GraphQuestions with extended annotations that cover multiple entities per question, ELQ outperforms the previous state of... | Belinda Z. Li, Sewon Min, Srinivasan Iyer, Yashar Mehdad, Wentau Yih |  |
| 1007 |  |  [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://doi.org/10.18653/v1/2020.emnlp-main.523) |  | 0 | Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens,... | Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto |  |
| 1008 |  |  [Generating similes effortlessly like a Pro: A Style Transfer Approach for Simile Generation](https://doi.org/10.18653/v1/2020.emnlp-main.524) |  | 0 | Literary tropes, from poetry to stories, are at the crux of human imagination and communication. Figurative language such as a simile go beyond plain expressions to give readers new insights and inspirations. In this paper, we tackle the problem of simile generation. Generating a simile requires... | Tuhin Chakrabarty, Smaranda Muresan, Nanyun Peng |  |
| 1009 |  |  [STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation](https://doi.org/10.18653/v1/2020.emnlp-main.525) |  | 0 | Systems for story generation are asked to produce plausible and enjoyable stories given an input context. This task is underspecified, as a vast number of diverse stories can originate from a single input. The large output space makes it difficult to build and evaluate story generation models, as... | Nader Akoury, Shufan Wang, Josh Whiting, Stephen Hood, Nanyun Peng, Mohit Iyyer |  |
| 1010 |  |  [Substance over Style: Document-Level Targeted Content Transfer](https://doi.org/10.18653/v1/2020.emnlp-main.526) |  | 0 | Existing language models excel at writing from scratch, but many real-world scenarios require rewriting an existing document to fit a set of constraints. Although sentence-level rewriting has been fairly well-studied, little work has addressed the challenge of rewriting an entire document... | Allison Hegel, Sudha Rao, Asli Celikyilmaz, Bill Dolan |  |
| 1011 |  |  [Template Guided Text Generation for Task-Oriented Dialogue](https://doi.org/10.18653/v1/2020.emnlp-main.527) |  | 0 | Virtual assistants such as Google Assistant, Amazon Alexa, and Apple Siri enable users to interact with a large number of services and APIs on the web using natural language. In this work, we investigate two methods for Natural Language Generation (NLG) using a single domain-independent model across... | Mihir Kale, Abhinav Rastogi |  |
| 1012 |  |  [MOCHA: A Dataset for Training and Evaluating Generative Reading Comprehension Metrics](https://doi.org/10.18653/v1/2020.emnlp-main.528) |  | 0 | Posing reading comprehension as a generation problem provides a great deal of flexibility, allowing for open-ended questions with few restrictions on possible answers. However, progress is impeded by existing generation metrics, which rely on token overlap and are agnostic to the nuances of reading... | Anthony Chen, Gabriel Stanovsky, Sameer Singh, Matt Gardner |  |
| 1013 |  |  [Plan ahead: Self-Supervised Text Planning for Paragraph Completion Task](https://doi.org/10.18653/v1/2020.emnlp-main.529) |  | 0 | Despite the recent success of contextualized language models on various NLP tasks, language model itself cannot capture textual coherence of a long, multi-sentence document (e.g., a paragraph). Humans often make structural decisions on what and how to say about before making utterances. Guiding... | Dongyeop Kang, Eduard H. Hovy |  |
| 1014 |  |  [Inquisitive Question Generation for High Level Text Comprehension](https://doi.org/10.18653/v1/2020.emnlp-main.530) |  | 0 | Inquisitive probing questions come naturally to humans in a variety of settings, but is a challenging task for automatic systems. One natural type of question to ask tries to fill a gap in knowledge during text comprehension, like reading a news article: we might ask about background information,... | WeiJen Ko, TeYuan Chen, Yiyan Huang, Greg Durrett, Junyi Jessy Li |  |
| 1015 |  |  [Towards Persona-Based Empathetic Conversational Models](https://doi.org/10.18653/v1/2020.emnlp-main.531) |  | 0 | Empathetic conversational models have been shown to improve user satisfaction and task outcomes in numerous domains. In Psychology, persona has been shown to be highly correlated to personality, which in turn influences empathy. In addition, our empirical analysis also suggests that persona plays an... | Peixiang Zhong, Chen Zhang, Hao Wang, Yong Liu, Chunyan Miao |  |
| 1016 |  |  [Personal Information Leakage Detection in Conversations](https://doi.org/10.18653/v1/2020.emnlp-main.532) |  | 0 | The global market size of conversational assistants (chatbots) is expected to grow to USD 9.4 billion by 2024, according to MarketsandMarkets. Despite the wide use of chatbots, leakage of personal information through chatbots poses serious privacy concerns for their users. In this work, we propose... | Qiongkai Xu, Lizhen Qu, Zeyu Gao, Gholamreza Haffari |  |
| 1017 |  |  [Response Selection for Multi-Party Conversations with Dynamic Topic Tracking](https://doi.org/10.18653/v1/2020.emnlp-main.533) |  | 0 | While participants in a multi-party multi-turn conversation simultaneously engage in multiple conversation topics, existing response selection methods are developed mainly focusing on a two-party single-conversation scenario. Hence, the prolongation and transition of conversation topics are ignored... | Weishi Wang, Steven C. H. Hoi, Shafiq R. Joty |  |
| 1018 |  |  [Regularizing Dialogue Generation by Imitating Implicit Scenarios](https://doi.org/10.18653/v1/2020.emnlp-main.534) |  | 0 | Human dialogues are scenario-based and appropriate responses generally relate to the latent context knowledge entailed by the specific scenario. To enable responses that are more meaningful and context-specific, we propose to improve generative dialogue systems from the scenario perspective, where... | Shaoxiong Feng, Xuancheng Ren, Hongshen Chen, Bin Sun, Kan Li, Xu Sun |  |
| 1019 |  |  [MovieChats: Chat like Humans in a Closed Domain](https://doi.org/10.18653/v1/2020.emnlp-main.535) |  | 0 | Being able to perform in-depth chat with humans in a closed domain is a precondition before an open-domain chatbot can be ever claimed. In this work, we take a close look at the movie domain and present a large-scale high-quality corpus with fine-grained annotations in hope of pushing the limit of... | Hui Su, Xiaoyu Shen, Xiao Zhou, Zheng Zhang, Ernie Chang, Cheng Zhang, Cheng Niu, Jie Zhou |  |
| 1020 |  |  [Conundrums in Entity Coreference Resolution: Making Sense of the State of the Art](https://doi.org/10.18653/v1/2020.emnlp-main.536) |  | 0 | Despite the significant progress on entity coreference resolution observed in recent years, there is a general lack of understanding of what has been improved. We present an empirical analysis of state-of-the-art resolvers with the goal of providing the general NLP audience with a better... | Jing Lu, Vincent Ng |  |
| 1021 |  |  [Semantic Role Labeling Guided Multi-turn Dialogue ReWriter](https://doi.org/10.18653/v1/2020.emnlp-main.537) |  | 0 | For multi-turn dialogue rewriting, the capacity of effectively modeling the linguistic knowledge in dialog context and getting ride of the noises is essential to improve its performance. Existing attentive models attend to all words without prior focus, which results in inaccurate concentration on... | Kun Xu, Haochen Tan, Linfeng Song, Han Wu, Haisong Zhang, Linqi Song, Dong Yu |  |
| 1022 |  |  [Continuity of Topic, Interaction, and Query: Learning to Quote in Online Conversations](https://doi.org/10.18653/v1/2020.emnlp-main.538) |  | 0 | Quotations are crucial for successful explanations and persuasions in interpersonal communications. However, finding what to quote in a conversation is challenging for both humans and machines. This work studies automatic quotation generation in an online conversation and explores how language... | Lingzhi Wang, Jing Li, Xingshan Zeng, Haisong Zhang, KamFai Wong |  |
| 1023 |  |  [Profile Consistency Identification for Open-domain Dialogue Agents](https://doi.org/10.18653/v1/2020.emnlp-main.539) |  | 0 | Maintaining a consistent attribute profile is crucial for dialogue agents to naturally converse with humans. Existing studies on improving attribute consistency mainly explored how to incorporate attribute information in the responses, but few efforts have been made to identify the consistency... | Haoyu Song, Yan Wang, WeiNan Zhang, Zhengyu Zhao, Ting Liu, Xiaojiang Liu |  |
| 1024 |  |  [An Element-aware Multi-representation Model for Law Article Prediction](https://doi.org/10.18653/v1/2020.emnlp-main.540) |  | 0 | Existing works have proved that using law articles as external knowledge can improve the performance of the Legal Judgment Prediction. However, they do not fully use law article information and most of the current work is only for single label samples. In this paper, we propose a Law Article... | Huilin Zhong, Junsheng Zhou, Weiguang Qu, Yunfei Long, Yanhui Gu |  |
| 1025 |  |  [Recurrent Event Network: Autoregressive Structure Inferenceover Temporal Knowledge Graphs](https://doi.org/10.18653/v1/2020.emnlp-main.541) |  | 0 | Knowledge graph reasoning is a critical task in natural language processing. The task becomes more challenging on temporal knowledge graphs, where each fact is associated with a timestamp. Most existing methods focus on reasoning at past timestamps and they are not able to predict facts happening in... | Woojeong Jin, Meng Qu, Xisen Jin, Xiang Ren |  |
| 1026 |  |  [Multi-resolution Annotations for Emoji Prediction](https://doi.org/10.18653/v1/2020.emnlp-main.542) |  | 0 | Emojis are able to express various linguistic components, including emotions, sentiments, events, etc. Predicting the proper emojis associated with text provides a way to summarize the text accurately, and it has been proven to be a good auxiliary task to many Natural Language Understanding (NLU)... | Weicheng Ma, Ruibo Liu, Lili Wang, Soroush Vosoughi |  |
| 1027 |  |  [Less is More: Attention Supervision with Counterfactuals for Text Classification](https://doi.org/10.18653/v1/2020.emnlp-main.543) |  | 0 | We aim to leverage human and machine intelligence together for attention supervision. Specifically, we show that human annotation cost can be kept reasonably low, while its quality can be enhanced by machine self-supervision. Specifically, for this goal, we explore the advantage of counterfactual... | Seungtaek Choi, Haeju Park, Jinyoung Yeo, Seungwon Hwang |  |
| 1028 |  |  [MODE-LSTM: A Parameter-efficient Recurrent Network with Multi-Scale for Sentence Classification](https://doi.org/10.18653/v1/2020.emnlp-main.544) |  | 0 | The central problem of sentence classification is to extract multi-scale n-gram features for understanding the semantic meaning of sentences. Most existing models tackle this problem by stacking CNN and RNN models, which easily leads to feature redundancy and overfitting because of relatively... | Qianli Ma, Zhenxi Lin, Jiangyue Yan, Zipeng Chen, Liuhong Yu |  |
| 1029 |  |  [HSCNN: A Hybrid-Siamese Convolutional Neural Network for Extremely Imbalanced Multi-label Text Classification](https://doi.org/10.18653/v1/2020.emnlp-main.545) |  | 0 | The data imbalance problem is a crucial issue for the multi-label text classification. Some existing works tackle it by proposing imbalanced loss objectives instead of the vanilla cross-entropy loss, but their performances remain limited in the cases of extremely imbalanced data. We propose a hybrid... | Wenshuo Yang, Jiyi Li, Fumiyo Fukumoto, Yanming Ye |  |
| 1030 |  |  [Multi-Stage Pre-training for Automated Chinese Essay Scoring](https://doi.org/10.18653/v1/2020.emnlp-main.546) |  | 0 | This paper proposes a pre-training based automated Chinese essay scoring method. The method involves three components: weakly supervised pre-training, supervised cross- prompt fine-tuning and supervised target- prompt fine-tuning. An essay scorer is first pre- trained on a large essay dataset... | Wei Song, Kai Zhang, Ruiji Fu, Lizhen Liu, Ting Liu, Miaomiao Cheng |  |
| 1031 |  |  [Multi-hop Inference for Question-driven Summarization](https://doi.org/10.18653/v1/2020.emnlp-main.547) |  | 0 | Question-driven summarization has been recently studied as an effective approach to summarizing the source document to produce concise but informative answers for non-factoid questions. In this work, we propose a novel question-driven abstractive summarization method, Multi-hop Selective Generator... | Yang Deng, Wenxuan Zhang, Wai Lam |  |
| 1032 |  |  [Towards Interpretable Reasoning over Paragraph Effects in Situation](https://doi.org/10.18653/v1/2020.emnlp-main.548) |  | 0 | We focus on the task of reasoning over paragraph effects in situation, which requires a model to understand the cause and effect described in a background paragraph, and apply the knowledge to a novel situation. Existing works ignore the complicated reasoning process and solve it with a one-step... | Mucheng Ren, Xiubo Geng, Tao Qin, Heyan Huang, Daxin Jiang |  |
| 1033 |  |  [Question Directed Graph Attention Network for Numerical Reasoning over Text](https://doi.org/10.18653/v1/2020.emnlp-main.549) |  | 0 | Numerical reasoning over texts, such as addition, subtraction, sorting and counting, is a challenging machine reading comprehension task, since it requires both natural language understanding and arithmetic computation. To address this challenge, we propose a heterogeneous graph representation for... | Kunlong Chen, Weidi Xu, Xingyi Cheng, Zou Xiaochuan, Yuyu Zhang, Le Song, Taifeng Wang, Yuan Qi, Wei Chu |  |
| 1034 |  |  [Dense Passage Retrieval for Open-Domain Question Answering](https://doi.org/10.18653/v1/2020.emnlp-main.550) |  | 0 | Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone,... | Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wentau Yih |  |
| 1035 |  |  [Distilling Structured Knowledge for Text-Based Relational Reasoning](https://doi.org/10.18653/v1/2020.emnlp-main.551) |  | 0 | There is an increasing interest in developing text-based relational reasoning systems, which are capable of systematically reasoning about the relationships between entities mentioned in a text. However, there remains a substantial performance gap between NLP models for relational reasoning and... | Jin Dong, MarcAntoine Rondeau, William L. Hamilton |  |
| 1036 |  |  [Asking without Telling: Exploring Latent Ontologies in Contextual Representations](https://doi.org/10.18653/v1/2020.emnlp-main.552) |  | 0 | The success of pretrained contextual encoders, such as ELMo and BERT, has brought a great deal of interest in what these models learn: do they, without explicit supervision, learn to encode meaningful notions of linguistic structure? If so, how is this structure encoded? To investigate this, we... | Julian Michael, Jan A. Botha, Ian Tenney |  |
| 1037 |  |  [Pretrained Language Model Embryology: The Birth of ALBERT](https://doi.org/10.18653/v1/2020.emnlp-main.553) |  | 0 | While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a... | David ChengHan Chiang, SungFeng Huang, Hungyi Lee |  |
| 1038 |  |  [Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models](https://doi.org/10.18653/v1/2020.emnlp-main.554) |  | 0 | We propose transfer learning as a method for analyzing the encoding of grammatical structure in neural language models. We train LSTMs on non-linguistic data and evaluate their performance on natural language to assess which kinds of data induce generalizable structural features that LSTMs can use... | Isabel Papadimitriou, Dan Jurafsky |  |
| 1039 |  |  [What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding](https://doi.org/10.18653/v1/2020.emnlp-main.555) |  | 0 | In recent years, pre-trained Transformers have dominated the majority of NLP benchmark tasks. Many variants of pre-trained Transformers have kept breaking out, and most focus on designing different pre-training objectives or variants of self-attention. Embedding the position information in the... | YuAn Wang, YunNung Chen |  |
| 1040 |  |  ["You are grounded!": Latent Name Artifacts in Pre-trained Language Models](https://doi.org/10.18653/v1/2020.emnlp-main.556) |  | 0 | Pre-trained language models (LMs) may perpetuate biases originating in their training corpus to downstream models. We focus on artifacts associated with the representation of given names (e.g., Donald), which, depending on the corpus, may be associated with specific entities, as indicated by next... | Vered Shwartz, Rachel Rudinger, Oyvind Tafjord |  |
| 1041 |  |  [Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models](https://doi.org/10.18653/v1/2020.emnlp-main.557) |  | 0 | Recent works show that pre-trained language models (PTLMs), such as BERT, possess certain commonsense and factual knowledge. They suggest that it is promising to use PTLMs as “neural knowledge bases” via predicting masked words. Surprisingly, we find that this may not work for numerical commonsense... | Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, Xiang Ren |  |
| 1042 |  |  [Grounded Adaptation for Zero-shot Executable Semantic Parsing](https://doi.org/10.18653/v1/2020.emnlp-main.558) |  | 0 | We propose Grounded Adaptation for Zeroshot Executable Semantic Parsing (GAZP) to adapt an existing semantic parser to new environments (e.g. new database schemas). GAZP combines a forward semantic parser with a backward utterance generator to synthesize data (e.g. utterances and SQL queries) in the... | Victor Zhong, Mike Lewis, Sida I. Wang, Luke Zettlemoyer |  |
| 1043 |  |  [An Imitation Game for Learning Semantic Parsers from User Interaction](https://doi.org/10.18653/v1/2020.emnlp-main.559) |  | 0 | Despite the widely successful applications, bootstrapping and fine-tuning semantic parsers are still a tedious process with challenges such as costly data annotation and privacy risks. In this paper, we suggest an alternative, human-in-the-loop methodology for learning semantic parsers directly from... | Ziyu Yao, Yiqi Tang, Wentau Yih, Huan Sun, Yu Su |  |
| 1044 |  |  [IGSQL: Database Schema Interaction Graph Based Neural Model for Context-Dependent Text-to-SQL Generation](https://doi.org/10.18653/v1/2020.emnlp-main.560) |  | 0 | Context-dependent text-to-SQL task has drawn much attention in recent years. Previous models on context-dependent text-to-SQL task only concentrate on utilizing historic user inputs. In this work, in addition to using encoders to capture historic information of user inputs, we propose a database... | Yitao Cai, Xiaojun Wan |  |
| 1045 |  |  ["What Do You Mean by That?" A Parser-Independent Interactive Approach for Enhancing Text-to-SQL](https://doi.org/10.18653/v1/2020.emnlp-main.561) |  | 0 | In Natural Language Interfaces to Databases systems, the text-to-SQL technique allows users to query databases by using natural language questions. Though significant progress in this area has been made recently, most parsers may fall short when they are deployed in real systems. One main reason... | Yuntao Li, Bei Chen, Qian Liu, Yan Gao, JianGuang Lou, Yan Zhang, Dongmei Zhang |  |
| 1046 |  |  [DuSQL: A Large-Scale and Pragmatic Chinese Text-to-SQL Dataset](https://doi.org/10.18653/v1/2020.emnlp-main.562) |  | 0 | Due to the lack of labeled data, previous research on text-to-SQL parsing mainly focuses on English. Representative English datasets include ATIS, WikiSQL, Spider, etc. This paper presents DuSQL, a larges-scale and pragmatic Chinese dataset for the cross-domain text-to-SQL task, containing 200... | Lijie Wang, Ao Zhang, Kun Wu, Ke Sun, Zhenghua Li, Hua Wu, Min Zhang, Haifeng Wang |  |
| 1047 |  |  [Mention Extraction and Linking for SQL Query Generation](https://doi.org/10.18653/v1/2020.emnlp-main.563) |  | 0 | On the WikiSQL benchmark, state-of-the-art text-to-SQL systems typically take a slot- filling approach by building several dedicated models for each type of slots. Such modularized systems are not only complex but also of limited capacity for capturing inter-dependencies among SQL clauses. To solve... | Jianqiang Ma, Zeyu Yan, Shuai Pang, Yang Zhang, Jianping Shen |  |
| 1048 |  |  [Re-examining the Role of Schema Linking in Text-to-SQL](https://doi.org/10.18653/v1/2020.emnlp-main.564) |  | 0 | In existing sophisticated text-to-SQL models, schema linking is often considered as a simple, minor component, belying its importance. By providing a schema linking corpus based on the Spider text-to-SQL dataset, we systematically study the role of schema linking. We also build a simple BERT-based... | Wenqiang Lei, Weixin Wang, Zhixin Ma, Tian Gan, Wei Lu, MinYen Kan, TatSeng Chua |  |
| 1049 |  |  [A Multi-Task Incremental Learning Framework with Category Name Embedding for Aspect-Category Sentiment Analysis](https://doi.org/10.18653/v1/2020.emnlp-main.565) |  | 0 | (T)ACSA tasks, including aspect-category sentiment analysis (ACSA) and targeted aspect-category sentiment analysis (TACSA), aims at identifying sentiment polarity on predefined categories. Incremental learning on new categories is necessary for (T)ACSA real applications. Though current multi-task... | Zehui Dai, Cheng Peng, Huajie Chen, Yadong Ding |  |
| 1050 |  |  [Train No Evil: Selective Masking for Task-Guided Pre-Training](https://doi.org/10.18653/v1/2020.emnlp-main.566) |  | 0 | Recently, pre-trained language models mostly follow the pre-train-then-fine-tuning paradigm and have achieved great performance on various downstream tasks. However, since the pre-training stage is typically task-agnostic and the fine-tuning stage usually suffers from insufficient supervised data,... | Yuxian Gu, Zhengyan Zhang, Xiaozhi Wang, Zhiyuan Liu, Maosong Sun |  |
| 1051 |  |  [SentiLARE: Sentiment-Aware Language Representation Learning with Linguistic Knowledge](https://doi.org/10.18653/v1/2020.emnlp-main.567) |  | 0 | Most of the existing pre-trained language representation models neglect to consider the linguistic knowledge of texts, which can promote language understanding in NLP tasks. To benefit the downstream tasks in sentiment analysis, we propose a novel language representation model called SentiLARE,... | Pei Ke, Haozhe Ji, Siyang Liu, Xiaoyan Zhu, Minlie Huang |  |
| 1052 |  |  [Weakly-Supervised Aspect-Based Sentiment Analysis via Joint Aspect-Sentiment Topic Embedding](https://doi.org/10.18653/v1/2020.emnlp-main.568) |  | 0 | Aspect-based sentiment analysis of review texts is of great value for understanding user feedback in a fine-grained manner. It has in general two sub-tasks: (i) extracting aspects from each review, and (ii) classifying aspect-based reviews by sentiment polarity. In this paper, we propose a... | Jiaxin Huang, Yu Meng, Fang Guo, Heng Ji, Jiawei Han |  |
| 1053 |  |  [APE: Argument Pair Extraction from Peer Review and Rebuttal via Multi-task Learning](https://doi.org/10.18653/v1/2020.emnlp-main.569) |  | 0 | Peer review and rebuttal, with rich interactions and argumentative discussions in between, are naturally a good resource to mine arguments. However, few works study both of them simultaneously. In this paper, we introduce a new argument pair extraction (APE) task on peer review and rebuttal in order... | Liying Cheng, Lidong Bing, Qian Yu, Wei Lu, Luo Si |  |
| 1054 |  |  [Diversified Multiple Instance Learning for Document-Level Multi-Aspect Sentiment Classification](https://doi.org/10.18653/v1/2020.emnlp-main.570) |  | 0 | Neural Document-level Multi-aspect Sentiment Classification (DMSC) usually requires a lot of manual aspect-level sentiment annotations, which is time-consuming and laborious. As document-level sentiment labeled data are widely available from online service, it is valuable to perform DMSC with such... | Yunjie Ji, Hao Liu, Bolei He, Xinyan Xiao, Hua Wu, Yanhua Yu |  |
| 1055 |  |  [Identifying Exaggerated Language](https://doi.org/10.18653/v1/2020.emnlp-main.571) |  | 0 | While hyperbole is one of the most prevalent rhetorical devices, it is arguably one of the least studied devices in the figurative language processing community. We contribute to the study of hyperbole by (1) creating a corpus focusing on sentence-level hyperbole detection, (2) performing a... | Li Kong, Chuanyi Li, Jidong Ge, Bin Luo, Vincent Ng |  |
| 1056 |  |  [Unified Feature and Instance Based Domain Adaptation for Aspect-Based Sentiment Analysis](https://doi.org/10.18653/v1/2020.emnlp-main.572) |  | 0 | The supervised models for aspect-based sentiment analysis (ABSA) rely heavily on labeled data. However, fine-grained labeled data are scarce for the ABSA task. To alleviate the dependence on labeled data, prior works mainly focused on feature-based adaptation, which used the domain-shared knowledge... | Chenggong Gong, Jianfei Yu, Rui Xia |  |
| 1057 |  |  [Compositional and Lexical Semantics in RoBERTa, BERT and DistilBERT: A Case Study on CoQA](https://doi.org/10.18653/v1/2020.emnlp-main.573) |  | 0 | Many NLP tasks have benefited from transferring knowledge from contextualized word embeddings, however the picture of what type of knowledge is transferred is incomplete. This paper studies the types of linguistic phenomena accounted for by language models in the context of a Conversational Question... | Ieva Staliunaite, Ignacio Iacobacci |  |
| 1058 |  |  [Attention is Not Only a Weight: Analyzing Transformers with Vector Norms](https://doi.org/10.18653/v1/2020.emnlp-main.574) |  | 0 | Attention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on analyzing the parallels between attention weights... | Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, Kentaro Inui |  |
| 1059 |  |  [F1 is Not Enough! Models and Evaluation Towards User-Centered Explainable Question Answering](https://doi.org/10.18653/v1/2020.emnlp-main.575) |  | 0 | Explainable question answering systems predict an answer together with an explanation showing why the answer has been selected. The goal is to enable users to assess the correctness of the system and understand its reasoning process. However, we show that current models and evaluation settings have... | Hendrik Schuff, Heike Adel, Ngoc Thang Vu |  |
| 1060 |  |  [On the Ability and Limitations of Transformers to Recognize Formal Languages](https://doi.org/10.18653/v1/2020.emnlp-main.576) |  | 0 | Transformers have supplanted recurrent models in a large number of NLP tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that LSTMs generalize very well on regular languages and have close connections with counter... | Satwik Bhattamishra, Kabir Ahuja, Navin Goyal |  |
| 1061 |  |  [An Unsupervised Joint System for Text Generation from Knowledge Graphs and Semantic Parsing](https://doi.org/10.18653/v1/2020.emnlp-main.577) |  | 0 | Knowledge graphs (KGs) can vary greatly from one domain to another. Therefore supervised approaches to both graph-to-text generation and text-to-graph knowledge extraction (semantic parsing) will always suffer from a shortage of domain-specific parallel graph-text data; at the same time, adapting a... | Martin Schmitt, Sahand Sharifzadeh, Volker Tresp, Hinrich Schütze |  |
| 1062 |  |  [DGST: a Dual-Generator Network for Text Style Transfer](https://doi.org/10.18653/v1/2020.emnlp-main.578) |  | 0 | We propose DGST, a novel and simple Dual-Generator network architecture for text Style Transfer. Our model employs two generators only, and does not rely on any discriminators or parallel corpus for training. Both quantitative and qualitative experiments on the Yelp and IMDb datasets show that our... | Xiao Li, Guanyi Chen, Chenghua Lin, Ruizhe Li |  |
| 1063 |  |  [A Knowledge-Aware Sequence-to-Tree Network for Math Word Problem Solving](https://doi.org/10.18653/v1/2020.emnlp-main.579) |  | 0 | With the advancements in natural language processing tasks, math word problem solving has received increasing attention. Previous methods have achieved promising results but ignore background common-sense knowledge not directly provided by the problem. In addition, during generation, they focus on... | Qinzhuo Wu, Qi Zhang, Jinlan Fu, Xuanjing Huang |  |
| 1064 |  |  [Generating Fact Checking Briefs](https://doi.org/10.18653/v1/2020.emnlp-main.580) |  | 0 | Fact checking at scale is difficult—while the number of active fact checking websites is growing, it remains too small for the needs of the contemporary media ecosystem. However, despite good intentions, contributions from volunteers are often error-prone, and thus in practice restricted to claim... | Angela Fan, Aleksandra Piktus, Fabio Petroni, Guillaume Wenzek, Marzieh Saeidi, Andreas Vlachos, Antoine Bordes, Sebastian Riedel |  |
| 1065 |  |  [Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction](https://doi.org/10.18653/v1/2020.emnlp-main.581) |  | 0 | We propose a novel language-independent approach to improve the efficiency for Grammatical Error Correction (GEC) by dividing the task into two subtasks: Erroneous Span Detection (ESD) and Erroneous Span Correction (ESC). ESD identifies grammatically incorrect text spans with an efficient sequence... | Mengyun Chen, Tao Ge, Xingxing Zhang, Furu Wei, Ming Zhou |  |
| 1066 |  |  [Coreferential Reasoning Learning for Language Representation](https://doi.org/10.18653/v1/2020.emnlp-main.582) |  | 0 | Language representation models such as BERT could effectively capture contextual semantic information from plain text, and have been proved to achieve promising results in lots of downstream NLP tasks with appropriate fine-tuning. However, most existing language representation models cannot... | Deming Ye, Yankai Lin, Jiaju Du, Zhenghao Liu, Peng Li, Maosong Sun, Zhiyuan Liu |  |
| 1067 |  |  [Is Graph Structure Necessary for Multi-hop Question Answering?](https://doi.org/10.18653/v1/2020.emnlp-main.583) |  | 0 | Recently, attempting to model texts as graph structure and introducing graph neural networks to deal with it has become a trend in many NLP research areas. In this paper, we investigate whether the graph structure is necessary for textual multi-hop reasoning. Our analysis is centered on HotpotQA. We... | Nan Shao, Yiming Cui, Ting Liu, Shijin Wang, Guoping Hu |  |
| 1068 |  |  [XL-WiC: A Multilingual Benchmark for Evaluating Semantic Contextualization](https://doi.org/10.18653/v1/2020.emnlp-main.584) |  | 0 | The ability to correctly model distinct meanings of a word is crucial for the effectiveness of semantic representation techniques. However, most existing evaluation benchmarks for assessing this criterion are tied to sense inventories (usually WordNet), restricting their usage to a small subset of... | Alessandro Raganato, Tommaso Pasini, José CamachoCollados, Mohammad Taher Pilehvar |  |
| 1069 |  |  [Generationary or "How We Went beyond Word Sense Inventories and Learned to Gloss"](https://doi.org/10.18653/v1/2020.emnlp-main.585) |  | 0 | Mainstream computational lexical semantics embraces the assumption that word senses can be represented as discrete items of a predefined inventory. In this paper we show this needs not be the case, and propose a unified model that is able to produce contextually appropriate definitions. In our... | Michele Bevilacqua, Marco Maru, Roberto Navigli |  |
| 1070 |  |  [Probing Pretrained Language Models for Lexical Semantics](https://doi.org/10.18653/v1/2020.emnlp-main.586) |  | 0 | The success of large pretrained language models (LMs) such as BERT and RoBERTa has sparked interest in probing their representations, in order to unveil what types of knowledge they implicitly capture. While prior research focused on morphosyntactic, semantic, and world knowledge, it remains unclear... | Ivan Vulic, Edoardo Maria Ponti, Robert Litschko, Goran Glavas, Anna Korhonen |  |
| 1071 |  |  [Cross-lingual Spoken Language Understanding with Regularized Representation Alignment](https://doi.org/10.18653/v1/2020.emnlp-main.587) |  | 0 | Despite the promising results of current cross-lingual models for spoken language understanding systems, they still suffer from imperfect cross-lingual representation alignments between the source and target languages, which makes the performance sub-optimal. To cope with this issue, we propose a... | Zihan Liu, Genta Indra Winata, Peng Xu, Zhaojiang Lin, Pascale Fung |  |
| 1072 |  |  [SLURP: A Spoken Language Understanding Resource Package](https://doi.org/10.18653/v1/2020.emnlp-main.588) |  | 0 | Spoken Language Understanding infers semantic meaning directly from audio data, and thus promises to reduce error propagation and misunderstandings in end-user applications. However, publicly available SLU resources are limited. In this paper, we release SLURP, a new SLU package containing the... | Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, Verena Rieser |  |
| 1073 |  |  [Neural Conversational QA: Learning to Reason vs Exploiting Patterns](https://doi.org/10.18653/v1/2020.emnlp-main.589) |  | 0 | Neural Conversational QA tasks such as ShARC require systems to answer questions based on the contents of a given passage. On studying recent state-of-the-art models on the ShARC QA task, we found indications that the model(s) learn spurious clues/patterns in the data-set. Further, a heuristic-based... | Nikhil Verma, Abhishek Sharma, Dhiraj Madan, Danish Contractor, Harshit Kumar, Sachindra Joshi |  |
| 1074 |  |  [Counterfactual Generator: A Weakly-Supervised Method for Named Entity Recognition](https://doi.org/10.18653/v1/2020.emnlp-main.590) |  | 0 | Past progress on neural models has proven that named entity recognition is no longer a problem if we have enough labeled data. However, collecting enough data and annotating them are labor-intensive, time-consuming, and expensive. In this paper, we decompose the sentence into two parts: entity and... | Xiangji Zeng, Yunliang Li, Yuchen Zhai, Yin Zhang |  |
| 1075 |  |  [Understanding Procedural Text using Interactive Entity Networks](https://doi.org/10.18653/v1/2020.emnlp-main.591) |  | 0 | The task of procedural text comprehension aims to understand the dynamic nature of entities/objects in a process. Here, the key is to track how the entities interact with each other and how their states are changing along the procedure. Recent efforts have made great progress to track multiple... | Jizhi Tang, Yansong Feng, Dongyan Zhao |  |
| 1076 |  |  [A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained Model Lead to the Promised Land?](https://doi.org/10.18653/v1/2020.emnlp-main.592) |  | 0 | Fine-tuning pretrained model has achieved promising performance on standard NER benchmarks. Generally, these benchmarks are blessed with strong name regularity, high mention coverage and sufficient context diversity. Unfortunately, when scaling NER to open situations, these advantages may no longer... | Hongyu Lin, Yaojie Lu, Jialong Tang, Xianpei Han, Le Sun, Zhicheng Wei, Nicholas Jing Yuan |  |
| 1077 |  |  [DyERNIE: Dynamic Evolution of Riemannian Manifold Embeddings for Temporal Knowledge Graph Completion](https://doi.org/10.18653/v1/2020.emnlp-main.593) |  | 0 | There has recently been increasing interest in learning representations of temporal knowledge graphs (KGs), which record the dynamic relationships between entities over time. Temporal KGs often exhibit multiple simultaneous non-Euclidean structures, such as hierarchical and cyclic structures.... | Zhen Han, Peng Chen, Yunpu Ma, Volker Tresp |  |
| 1078 |  |  [Embedding Words in Non-Vector Space with Unsupervised Graph Learning](https://doi.org/10.18653/v1/2020.emnlp-main.594) |  | 0 | It has become a de-facto standard to represent words as elements of a vector space (word2vec, GloVe). While this approach is convenient, it is unnatural for language: words form a graph with a latent hierarchical structure, and this structure has to be revealed and encoded by word embeddings. We... | Max Ryabinin, Sergei Popov, Liudmila Prokhorenkova, Elena Voita |  |
| 1079 |  |  [Debiasing knowledge graph embeddings](https://doi.org/10.18653/v1/2020.emnlp-main.595) |  | 0 | It has been shown that knowledge graph embeddings encode potentially harmful social biases, such as the information that women are more likely to be nurses, and men more likely to be bankers. As graph embeddings begin to be used more widely in NLP pipelines, there is a need to develop training... | Joseph Fisher, Arpit Mittal, Dave Palfrey, Christos Christodoulopoulos |  |
| 1080 |  |  [Message Passing for Hyper-Relational Knowledge Graphs](https://doi.org/10.18653/v1/2020.emnlp-main.596) |  | 0 | Hyper-relational knowledge graphs (KGs) (e.g., Wikidata) enable associating additional key-value pairs along with the main triple to disambiguate, or restrict the validity of a fact. In this work, we propose a message passing based graph encoder - StarE capable of modeling such hyper-relational KGs.... | Mikhail Galkin, Priyansh Trivedi, Gaurav Maheshwari, Ricardo Usbeck, Jens Lehmann |  |
| 1081 |  |  [Relation-aware Graph Attention Networks with Relational Position Encodings for Emotion Recognition in Conversations](https://doi.org/10.18653/v1/2020.emnlp-main.597) |  | 0 | Interest in emotion recognition in conversations (ERC) has been increasing in various fields, because it can be used to analyze user behaviors and detect fake news. Many recent ERC methods use graph-based neural networks to take the relationships between the utterances of the speakers into account.... | Taichi Ishiwatari, Yuki Yasuda, Taro Miyazaki, Jun Goto |  |
| 1082 |  |  [BERT Knows Punta Cana is not just beautiful, it's gorgeous: Ranking Scalar Adjectives with Contextualised Representations](https://doi.org/10.18653/v1/2020.emnlp-main.598) |  | 0 | Adjectives like pretty, beautiful and gorgeous describe positive properties of the nouns they modify but with different intensity. These differences are important for natural language understanding and reasoning. We propose a novel BERT-based approach to intensity detection for scalar adjectives. We... | Aina Garí Soler, Marianna Apidianaki |  |
| 1083 |  |  [Feature Adaptation of Pre-Trained Language Models across Languages and Domains with Robust Self-Training](https://doi.org/10.18653/v1/2020.emnlp-main.599) |  | 0 | Adapting pre-trained language models (PrLMs) (e.g., BERT) to new domains has gained much attention recently. Instead of fine-tuning PrLMs as done in most previous work, we investigate how to adapt the features of PrLMs to new domains without fine-tuning. We explore unsupervised domain adaptation... | Hai Ye, Qingyu Tan, Ruidan He, Juntao Li, Hwee Tou Ng, Lidong Bing |  |
| 1084 |  |  [Textual Data Augmentation for Efficient Active Learning on Tiny Datasets](https://doi.org/10.18653/v1/2020.emnlp-main.600) |  | 0 | In this paper we propose a novel data augmentation approach where guided outputs of a language generation model, e.g. GPT-2, when labeled, can improve the performance of text classifiers through an active learning process. We transform the data generation task into an optimization problem which... | Husam Quteineh, Spyridon Samothrakis, Richard F. E. Sutcliffe |  |
| 1085 |  |  ["I'd rather just go to bed": Understanding Indirect Answers](https://doi.org/10.18653/v1/2020.emnlp-main.601) |  | 0 | We revisit a pragmatic inference problem in dialog: Understanding indirect responses to questions. Humans can interpret ‘I’m starving.’ in response to ‘Hungry?’, even without direct cue words such as ‘yes’ and ‘no’. In dialog systems, allowing natural responses rather than closed vocabularies would... | Annie Louis, Dan Roth, Filip Radlinski |  |
| 1086 |  |  [PowerTransformer: Unsupervised Controllable Revision for Biased Language Correction](https://doi.org/10.18653/v1/2020.emnlp-main.602) |  | 0 | Unconscious biases continue to be prevalent in modern text and media, calling for algorithms that can assist writers with bias correction. For example, a female character in a story is often portrayed as passive and powerless (“_She daydreams about being a doctor_”) while a man is portrayed as more... | Xinyao Ma, Maarten Sap, Hannah Rashkin, Yejin Choi |  |
| 1087 |  |  [MEGA RST Discourse Treebanks with Structure and Nuclearity from Scalable Distant Sentiment Supervision](https://doi.org/10.18653/v1/2020.emnlp-main.603) |  | 0 | The lack of large and diverse discourse treebanks hinders the application of data-driven approaches, such as deep-learning, to RST-style discourse parsing. In this work, we present a novel scalable methodology to automatically generate discourse treebanks using distant supervision from sentiment... | Patrick Huber, Giuseppe Carenini |  |
| 1088 |  |  [Centering-based Neural Coherence Modeling with Hierarchical Discourse Segments](https://doi.org/10.18653/v1/2020.emnlp-main.604) |  | 0 | Previous neural coherence models have focused on identifying semantic relations between adjacent sentences. However, they do not have the means to exploit structural information. In this work, we propose a coherence model which takes discourse structural information into account without relying on... | Sungho Jeon, Michael Strube |  |
| 1089 |  |  [Keeping Up Appearances: Computational Modeling of Face Acts in Persuasion Oriented Discussions](https://doi.org/10.18653/v1/2020.emnlp-main.605) |  | 0 | The notion of face refers to the public self-image of an individual that emerges both from the individual’s own actions as well as from the interaction with others. Modeling face and understanding its state changes throughout a conversation is critical to the study of maintenance of basic human... | Ritam Dutt, Rishabh Joshi, Carolyn P. Rosé |  |
| 1090 |  |  [HABERTOR: An Efficient and Effective Deep Hatespeech Detector](https://doi.org/10.18653/v1/2020.emnlp-main.606) |  | 0 | We present our HABERTOR model for detecting hatespeech in large scale user-generated content. Inspired by the recent success of the BERT model, we propose several modifications to BERT to enhance the performance on the downstream hatespeech classification task. HABERTOR inherits BERT’s architecture,... | Thanh Tran, Yifan Hu, Changwei Hu, Kevin Yen, Fei Tan, Kyumin Lee, Se Rim Park |  |
| 1091 |  |  [An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels](https://doi.org/10.18653/v1/2020.emnlp-main.607) |  | 0 | Large-scale Multi-label Text Classification (LMTC) has a wide range of Natural Language Processing (NLP) applications and presents interesting challenges. First, not all labels are well represented in the training set, due to the very large label set and the skewed label distributions of datasets.... | Ilias Chalkidis, Manos Fergadiotis, Sotiris Kotitsas, Prodromos Malakasiotis, Nikolaos Aletras, Ion Androutsopoulos |  |
| 1092 |  |  [Which \*BERT? A Survey Organizing Contextualized Encoders](https://doi.org/10.18653/v1/2020.emnlp-main.608) |  | 0 | Pretrained contextualized text encoders are now a staple of the NLP community. We present a survey on language representation learning with the aim of consolidating a series of shared lessons learned across a variety of recent efforts. While significant advancements continue at a rapid pace, we find... | Patrick Xia, Shijie Wu, Benjamin Van Durme |  |
| 1093 |  |  [Fact or Fiction: Verifying Scientific Claims](https://doi.org/10.18653/v1/2020.emnlp-main.609) |  | 0 | We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K... | David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, Hannaneh Hajishirzi |  |
| 1094 |  |  [Semantic Role Labeling as Syntactic Dependency Parsing](https://doi.org/10.18653/v1/2020.emnlp-main.610) |  | 0 | We reduce the task of (span-based) PropBank-style semantic role labeling (SRL) to syntactic dependency parsing. Our approach is motivated by our empirical analysis that shows three common syntactic patterns account for over 98% of the SRL annotations for both English and Chinese data. Based on this... | Tianze Shi, Igor Malioutov, Ozan Irsoy |  |
| 1095 |  |  [PARADE: A New Dataset for Paraphrase Identification Requiring Computer Science Domain Knowledge](https://doi.org/10.18653/v1/2020.emnlp-main.611) |  | 0 | We present a new benchmark dataset called PARADE for paraphrase identification that requires specialized domain knowledge. PARADE contains paraphrases that overlap very little at the lexical and syntactic level but are semantically equivalent based on computer science domain knowledge, as well as... | Yun He, Zhuoer Wang, Yin Zhang, Ruihong Huang, James Caverlee |  |
| 1096 |  |  [Causal Inference of Script Knowledge](https://doi.org/10.18653/v1/2020.emnlp-main.612) |  | 0 | When does a sequence of events define an everyday scenario and how can this knowledge be induced from text? Prior works in inducing such scripts have relied on, in one form or another, measures of correlation between instances of events in a corpus. We argue from both a conceptual and practical... | Noah Weber, Rachel Rudinger, Benjamin Van Durme |  |
| 1097 |  |  [Towards Debiasing NLU Models from Unknown Biases](https://doi.org/10.18653/v1/2020.emnlp-main.613) |  | 0 | NLU models often exploit biases to achieve high dataset-specific performance without properly learning the intended task. Recently proposed debiasing methods are shown to be effective in mitigating this tendency. However, these methods rely on a major assumption that the types of bias should be... | Prasetya Ajie Utama, Nafise Sadat Moosavi, Iryna Gurevych |  |
| 1098 |  |  [On the Role of Supervision in Unsupervised Constituency Parsing](https://doi.org/10.18653/v1/2020.emnlp-main.614) |  | 0 | We analyze several recent unsupervised constituency parsing models, which are tuned with respect to the parsing F1 score on the Wall Street Journal (WSJ) development set (1,700 sentences). We introduce strong baselines for them, by training an existing supervised parsing model (Kitaev and Klein,... | Haoyue Shi, Karen Livescu, Kevin Gimpel |  |
| 1099 |  |  [Language Model Prior for Low-Resource Neural Machine Translation](https://doi.org/10.18653/v1/2020.emnlp-main.615) |  | 0 | The scarcity of large parallel corpora is an important obstacle for neural machine translation. A common solution is to exploit the knowledge of language models (LM) trained on abundant monolingual data. In this work, we propose a novel approach to incorporate a LM as prior in a neural translation... | Christos Baziotis, Barry Haddow, Alexandra Birch |  |
| 1100 |  |  [Detecting Word Sense Disambiguation Biases in Machine Translation for Model-Agnostic Adversarial Attacks](https://doi.org/10.18653/v1/2020.emnlp-main.616) |  | 0 | Word sense disambiguation is a well-known source of translation errors in NMT. We posit that some of the incorrect disambiguation choices are due to models’ over-reliance on dataset artifacts found in training data, specifically superficial word co-occurrences, rather than a deeper understanding of... | Denis Emelin, Ivan Titov, Rico Sennrich |  |
| 1101 |  |  [MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer](https://doi.org/10.18653/v1/2020.emnlp-main.617) |  | 0 | The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance... | Jonas Pfeiffer, Ivan Vulic, Iryna Gurevych, Sebastian Ruder |  |
| 1102 |  |  [Translation Artifacts in Cross-lingual Transfer Learning](https://doi.org/10.18653/v1/2020.emnlp-main.618) |  | 0 | Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique.... | Mikel Artetxe, Gorka Labaka, Eneko Agirre |  |
| 1103 |  |  [A Time-Aware Transformer Based Model for Suicide Ideation Detection on Social Media](https://doi.org/10.18653/v1/2020.emnlp-main.619) |  | 0 | Social media’s ubiquity fosters a space for users to exhibit suicidal thoughts outside of traditional clinical settings. Understanding the build-up of such ideation is critical for the identification of at-risk users and suicide prevention. Suicide ideation is often linked to a history of mental... | Ramit Sawhney, Harshit Joshi, Saumya Gandhi, Rajiv Ratn Shah |  |
| 1104 |  |  [Weakly Supervised Learning of Nuanced Frames for Analyzing Polarization in News Media](https://doi.org/10.18653/v1/2020.emnlp-main.620) |  | 0 | In this paper, we suggest a minimally supervised approach for identifying nuanced frames in news article coverage of politically divisive topics. We suggest to break the broad policy frames suggested by Boydstun et al., 2014 into fine-grained subframes which can capture differences in political... | Shamik Roy, Dan Goldwasser |  |
| 1105 |  |  [Where Are the Facts? Searching for Fact-checked Information to Alleviate the Spread of Fake News](https://doi.org/10.18653/v1/2020.emnlp-main.621) |  | 0 | Although many fact-checking systems have been developed in academia and industry, fake news is still proliferating on social media. These systems mostly focus on fact-checking but usually neglect online users who are the main drivers of the spread of misinformation. How can we use fact-checked... | Nguyen Vo, Kyumin Lee |  |
| 1106 |  |  [Fortifying Toxic Speech Detectors Against Veiled Toxicity](https://doi.org/10.18653/v1/2020.emnlp-main.622) |  | 0 | Modern toxic speech detectors are incompetent in recognizing disguised offensive language, such as adversarial attacks that deliberately avoid known toxic lexicons, or manifestations of implicit bias. Building a large annotated dataset for such veiled toxicity can be very expensive. In this work, we... | Xiaochuang Han, Yulia Tsvetkov |  |
| 1107 |  |  [Explainable Automated Fact-Checking for Public Health Claims](https://doi.org/10.18653/v1/2020.emnlp-main.623) |  | 0 | Fact-checking is the task of verifying the veracity of claims by assessing their assertions against credible evidence. The vast majority of fact-checking studies focus exclusively on political claims. Very little research explores fact-checking for other topics, specifically subject matters for... | Neema Kotonya, Francesca Toni |  |
| 1108 |  |  [Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning](https://doi.org/10.18653/v1/2020.emnlp-main.624) |  | 0 | Interactive Fiction (IF) games with real human-written natural language texts provide a new natural evaluation for language understanding techniques. In contrast to previous text games with mostly synthetic texts, IF games pose language understanding challenges on the human-written textual... | Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Murray Campbell, Shiyu Chang |  |
| 1109 |  |  [DORB: Dynamically Optimizing Multiple Rewards with Bandits](https://doi.org/10.18653/v1/2020.emnlp-main.625) |  | 0 | Policy gradients-based reinforcement learning has proven to be a promising approach for directly optimizing non-differentiable evaluation metrics for language generation tasks. However, optimizing for a specific metric reward leads to improvements in mostly that metric only, suggesting that the... | Ramakanth Pasunuru, Han Guo, Mohit Bansal |  |
| 1110 |  |  [MedFilter: Improving Extraction of Task-relevant Utterances through Integration of Discourse Structure and Ontological Knowledge](https://doi.org/10.18653/v1/2020.emnlp-main.626) |  | 0 | Information extraction from conversational data is particularly challenging because the task-centric nature of conversation allows for effective communication of implicit information by humans, but is challenging for machines. The challenges may differ between utterances depending on the role of the... | Sopan Khosla, Shikhar Vashishth, Jill Fain Lehman, Carolyn P. Rosé |  |
| 1111 |  |  [Hierarchical Evidence Set Modeling for Automated Fact Extraction and Verification](https://doi.org/10.18653/v1/2020.emnlp-main.627) |  | 0 | Automated fact extraction and verification is a challenging task that involves finding relevant evidence sentences from a reliable corpus to verify the truthfulness of a claim. Existing models either (i) concatenate all the evidence sentences, leading to the inclusion of redundant and noisy... | Shyam Subramanian, Kyumin Lee |  |
| 1112 |  |  [Program Enhanced Fact Verification with Verbalization and Graph Attention Network](https://doi.org/10.18653/v1/2020.emnlp-main.628) |  | 0 | Performing fact verification based on structured data is important for many real-life applications and is a challenging research problem, particularly when it involves both symbolic operations and informal inference based on language understanding. In this paper, we present a Program-enhanced... | Xiaoyu Yang, Feng Nie, Yufei Feng, Quan Liu, Zhigang Chen, Xiaodan Zhu |  |
| 1113 |  |  [Constrained Fact Verification for FEVER](https://doi.org/10.18653/v1/2020.emnlp-main.629) |  | 0 | Fact-verification systems are well explored in the NLP literature with growing attention owing to shared tasks like FEVER. Though the task requires reasoning on extracted evidence to verify a claim’s factuality, there is little work on understanding the reasoning process. In this work, we propose a... | Adithya Pratapa, Sai Muralidhar Jayanthi, Kavya Nerella |  |
| 1114 |  |  [Entity Linking in 100 Languages](https://doi.org/10.18653/v1/2020.emnlp-main.630) |  | 0 | We propose a new formulation for multilingual entity linking, where language-specific mentions resolve to a language-agnostic Knowledge Base. We train a dual encoder in this new setting, building on prior work with improved feature representation, negative mining, and an auxiliary entity-pairing... | Jan A. Botha, Zifei Shan, Daniel Gillick |  |
| 1115 |  |  [PatchBERT: Just-in-Time, Out-of-Vocabulary Patching](https://doi.org/10.18653/v1/2020.emnlp-main.631) |  | 0 | Large scale pre-trained language models have shown groundbreaking performance improvements for transfer learning in the domain of natural language processing. In our paper, we study a pre-trained multilingual BERT model and analyze the OOV rate on downstream tasks, how it introduces information... | Sangwhan Moon, Naoaki Okazaki |  |
| 1116 |  |  [On the importance of pre-training data volume for compact language models](https://doi.org/10.18653/v1/2020.emnlp-main.632) |  | 0 | Recent advances in language modeling have led to computationally intensive and resource-demanding state-of-the-art models. In an effort towards sustainable practices, we study the impact of pre-training data volume on compact language models. Multiple BERT-based models are trained on gradually... | Vincent Micheli, Martin d'Hoffschmidt, François Fleuret |  |
| 1117 |  |  [BERT-of-Theseus: Compressing BERT by Progressive Module Replacing](https://doi.org/10.18653/v1/2020.emnlp-main.633) |  | 0 | In this paper, we propose a novel model compression approach to effectively compress BERT by progressive module replacing. Our approach first divides the original BERT into several modules and builds their compact substitutes. Then, we randomly replace the original modules with their substitutes to... | Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, Ming Zhou |  |
| 1118 |  |  [Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting](https://doi.org/10.18653/v1/2020.emnlp-main.634) |  | 0 | Deep pretrained language models have achieved great success in the way of pretraining first and then fine-tuning. But such a sequential transfer learning paradigm often confronts the catastrophic forgetting problem and leads to sub-optimal performance. To fine-tune with less forgetting, we propose a... | Sanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu, Xiangzhan Yu |  |
| 1119 |  |  [Exploring and Predicting Transferability across NLP Tasks](https://doi.org/10.18653/v1/2020.emnlp-main.635) |  | 0 | Recent advances in NLP demonstrate the effectiveness of training large-scale language models and transferring them to downstream tasks. Can fine-tuning these models on tasks other than language modeling further improve performance? In this paper, we conduct an extensive study of the transferability... | Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam Trischler, Andrew MattarellaMicke, Subhransu Maji, Mohit Iyyer |  |
| 1120 |  |  [To BERT or Not to BERT: Comparing Task-specific and Task-agnostic Semi-Supervised Approaches for Sequence Tagging](https://doi.org/10.18653/v1/2020.emnlp-main.636) |  | 0 | Leveraging large amounts of unlabeled data using Transformer-like architectures, like BERT, has gained popularity in recent times owing to their effectiveness in learning general representations that can then be further fine-tuned for downstream tasks to much success. However, training these models... | Kasturi Bhattacharjee, Miguel Ballesteros, Rishita Anubhai, Smaranda Muresan, Jie Ma, Faisal Ladhak, Yaser AlOnaizan |  |
| 1121 |  |  [Cold-start Active Learning through Self-supervised Language Modeling](https://doi.org/10.18653/v1/2020.emnlp-main.637) |  | 0 | Active learning strives to reduce annotation costs by choosing the most critical examples to label. Typically, the active learning strategy is contingent on the classification model. For instance, uncertainty sampling depends on poorly calibrated model confidence scores. In the cold-start setting,... | Michelle Yuan, HsuanTien Lin, Jordan L. BoydGraber |  |
| 1122 |  |  [Active Learning for BERT: An Empirical Study](https://doi.org/10.18653/v1/2020.emnlp-main.638) |  | 0 | Real world scenarios present a challenge for text classification, since labels are usually expensive and the data is often characterized by class imbalance. Active Learning (AL) is a ubiquitous paradigm to cope with data scarcity. Recently, pre-trained NLP models, and BERT in particular, are... | Liat EinDor, Alon Halfon, Ariel Gera, Eyal Shnarch, Lena Dankin, Leshem Choshen, Marina Danilevsky, Ranit Aharonov, Yoav Katz, Noam Slonim |  |
| 1123 |  |  [Transformer Based Multi-Source Domain Adaptation](https://doi.org/10.18653/v1/2020.emnlp-main.639) |  | 0 | In practical machine learning settings, the data on which a model must make predictions often come from a different distribution than the data it was trained on. Here, we investigate the problem of unsupervised multi-source domain adaptation, where a model is trained on labelled data from multiple... | Dustin Wright, Isabelle Augenstein |  |
| 1124 |  |  [Vector-Vector-Matrix Architecture: A Novel Hardware-Aware Framework for Low-Latency Inference in NLP Applications](https://doi.org/10.18653/v1/2020.emnlp-main.640) |  | 0 | Deep neural networks have become the standard approach to building reliable Natural Language Processing (NLP) applications, ranging from Neural Machine Translation (NMT) to dialogue systems. However, improving accuracy by increasing the model size requires a large number of hardware computations,... | Matthew Khoury, Rumen Dangovski, Longwu Ou, Preslav Nakov, Yichen Shen, Li Jing |  |
| 1125 |  |  [The importance of fillers for text representations of speech transcripts](https://doi.org/10.18653/v1/2020.emnlp-main.641) |  | 0 | While being an essential component of spoken language, fillers (e.g. “um” or “uh”) often remain overlooked in Spoken Language Understanding (SLU) tasks. We explore the possibility of representing them with deep contextualised embeddings, showing improvements on modelling spoken language and two... | Tanvi Dinkar, Pierre Colombo, Matthieu Labeau, Chloé Clavel |  |
| 1126 |  |  [The role of context in neural pitch accent detection in English](https://doi.org/10.18653/v1/2020.emnlp-main.642) |  | 0 | Prosody is a rich information source in natural language, serving as a marker for phenomena such as contrast. In order to make this information available to downstream tasks, we need a way to detect prosodic events in speech. We propose a new model for pitch accent detection, inspired by the work of... | Elizabeth Nielsen, Mark Steedman, Sharon Goldwater |  |
| 1127 |  |  [VolTAGE: Volatility Forecasting via Text Audio Fusion with Graph Convolution Networks for Earnings Calls](https://doi.org/10.18653/v1/2020.emnlp-main.643) |  | 0 | Natural language processing has recently made stock movement forecasting and volatility forecasting advances, leading to improved financial forecasting. Transcripts of companies’ earnings calls are well studied for risk modeling, offering unique investment insight into stock performance. However,... | Ramit Sawhney, Piyush Khanna, Arshiya Aggarwal, Taru Jain, Puneet Mathur, Rajiv Ratn Shah |  |
| 1128 |  |  [Effectively pretraining a speech translation decoder with Machine Translation data](https://doi.org/10.18653/v1/2020.emnlp-main.644) |  | 0 | Directly translating from speech to text using an end-to-end approach is still challenging for many language pairs due to insufficient data. Although pretraining the encoder parameters using the Automatic Speech Recognition (ASR) task improves the results in low resource settings, attempting to use... | Ashkan Alinejad, Anoop Sarkar |  |
| 1129 |  |  [A Preliminary Exploration of GANs for Keyphrase Generation](https://doi.org/10.18653/v1/2020.emnlp-main.645) |  | 0 | We introduce a new keyphrase generation approach using Generative Adversarial Networks (GANs). For a given document, the generator produces a sequence of keyphrases, and the discriminator distinguishes between human-curated and machine-generated keyphrases. We evaluated this approach on standard... | Avinash Swaminathan, Haimin Zhang, Debanjan Mahata, Rakesh Gosangi, Rajiv Ratn Shah, Amanda Stent |  |
| 1130 |  |  [TESA: A Task in Entity Semantic Aggregation for Abstractive Summarization](https://doi.org/10.18653/v1/2020.emnlp-main.646) |  | 0 | Human-written texts contain frequent generalizations and semantic aggregation of content. In a document, they may refer to a pair of named entities such as ‘London’ and ‘Paris’ with different expressions: “the major cities”, “the capital cities” and “two European cities”. Yet generation, especially,... | Clément Jumel, Annie Louis, Jackie Chi Kit Cheung |  |
| 1131 |  |  [MLSUM: The Multilingual Summarization Corpus](https://doi.org/10.18653/v1/2020.emnlp-main.647) |  | 0 | We present MLSUM, the first large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages – namely, French, German, Spanish, Russian, Turkish. Together with English news articles from the popular CNN/Daily mail... | Thomas Scialom, PaulAlexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano |  |
| 1132 |  |  [Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles](https://doi.org/10.18653/v1/2020.emnlp-main.648) |  | 0 | Multi-document summarization is a challenging task for which there exists little large-scale datasets. We propose Multi-XScience, a large-scale multi-document summarization dataset created from scientific articles. Multi-XScience introduces a challenging multi-document summarization task: writing... | Yao Lu, Yue Dong, Laurent Charlin |  |
| 1133 |  |  [Intrinsic Evaluation of Summarization Datasets](https://doi.org/10.18653/v1/2020.emnlp-main.649) |  | 0 | High quality data forms the bedrock for building meaningful statistical models in NLP. Consequently, data quality must be evaluated either during dataset construction or \*post hoc\*. Almost all popular summarization datasets are drawn from natural sources and do not come with inherent quality... | Rishi Bommasani, Claire Cardie |  |
| 1134 |  |  [Iterative Feature Mining for Constraint-Based Data Collection to Increase Data Diversity and Model Robustness](https://doi.org/10.18653/v1/2020.emnlp-main.650) |  | 0 | Diverse data is crucial for training robust models, but crowdsourced text often lacks diversity as workers tend to write simple variations from prompts. We propose a general approach for guiding workers to write more diverse text by iteratively constraining their writing. We show how prior workflows... | Stefan Larson, Anthony Zheng, Anish Mahendran, Rishi Tekriwal, Adrian Cheung, Eric Guldan, Kevin Leach, Jonathan K. Kummerfeld |  |
| 1135 |  |  [Conversational Semantic Parsing for Dialog State Tracking](https://doi.org/10.18653/v1/2020.emnlp-main.651) |  | 0 | We consider a new perspective on dialog state tracking (DST), the task of estimating a user’s goal through the course of a dialog. By formulating DST as a semantic parsing task over hierarchical representations, we can incorporate semantic compositionality, cross-domain knowledge sharing and... | Jianpeng Cheng, Devang Agrawal, Héctor Martínez Alonso, Shruti Bhargava, Joris Driesen, Federico Flego, Dain Kaplan, Dimitri Kartsaklis, Lin Li, Dhivya Piraviperumal, Jason D. Williams, Hong Yu, Diarmuid Ó Séaghdha, Anders Johannsen |  |
| 1136 |  |  [doc2dial: A Goal-Oriented Document-Grounded Dialogue Dataset](https://doi.org/10.18653/v1/2020.emnlp-main.652) |  | 0 | We introduce doc2dial, a new dataset of goal-oriented dialogues that are grounded in the associated documents. Inspired by how the authors compose documents for guiding end users, we first construct dialogue flows based on the content elements that corresponds to higher-level relations across text... | Song Feng, Hui Wan, R. Chulaka Gunasekara, Siva Sankalp Patel, Sachindra Joshi, Luis A. Lastras |  |
| 1137 |  |  [Interview: Large-scale Modeling of Media Dialog with Discourse Patterns and Knowledge Grounding](https://doi.org/10.18653/v1/2020.emnlp-main.653) |  | 0 | In this work, we perform the first large-scale analysis of discourse in media dialog and its impact on generative modeling of dialog turns, with a focus on interrogative patterns and use of external knowledge. Discourse analysis can help us understand modes of persuasion, entertainment, and... | Bodhisattwa Prasad Majumder, Shuyang Li, Jianmo Ni, Julian J. McAuley |  |
| 1138 |  |  [INSPIRED: Toward Sociable Recommendation Dialog Systems](https://doi.org/10.18653/v1/2020.emnlp-main.654) |  | 0 | In recommendation dialogs, humans commonly disclose their preference and make recommendations in a friendly manner. However, this is a challenge when developing a sociable recommendation dialog system, due to the lack of dialog dataset annotated with such sociable strategies. Therefore, we present... | Shirley Anugrah Hayati, Dongyeop Kang, Qingxiaoyang Zhu, Weiyan Shi, Zhou Yu |  |
| 1139 |  |  [Information Seeking in the Spirit of Learning: A Dataset for Conversational Curiosity](https://doi.org/10.18653/v1/2020.emnlp-main.655) |  | 0 | Open-ended human learning and information-seeking are increasingly mediated by digital assistants. However, such systems often ignore the user’s pre-existing knowledge. Assuming a correlation between engagement and user responses such as “liking” messages or asking followup questions, we design a... | Pedro Rodriguez, Paul A. Crook, Seungwhan Moon, Zhiguang Wang |  |
| 1140 |  |  [Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation](https://doi.org/10.18653/v1/2020.emnlp-main.656) |  | 0 | Social biases present in data are often directly reflected in the predictions of models trained on that data. We analyze gender bias in dialogue data, and examine how this bias is not only replicated, but is also amplified in subsequent generative chit-chat dialogue models. We measure gender bias in... | Emily Dinan, Angela Fan, Adina Williams, Jack Urbanek, Douwe Kiela, Jason Weston |  |
| 1141 |  |  [Discriminatively-Tuned Generative Classifiers for Robust Natural Language Inference](https://doi.org/10.18653/v1/2020.emnlp-main.657) |  | 0 | While discriminative neural network classifiers are generally preferred, recent work has shown advantages of generative classifiers in term of data efficiency and robustness. In this paper, we focus on natural language inference (NLI). We propose GenNLI, a generative classifier for NLI tasks, and... | Xiaoan Ding, Tianyu Liu, Baobao Chang, Zhifang Sui, Kevin Gimpel |  |
| 1142 |  |  [New Protocols and Negative Results for Textual Entailment Data Collection](https://doi.org/10.18653/v1/2020.emnlp-main.658) |  | 0 | Natural language inference (NLI) data has proven useful in benchmarking and, especially, as pretraining data for tasks requiring language understanding. However, the crowdsourcing protocol that was used to collect this data has known issues and was not explicitly optimized for either of these... | Samuel R. Bowman, Jennimaria Palomaki, Livio Baldini Soares, Emily Pitler |  |
| 1143 |  |  [The Curse of Performance Instability in Analysis Datasets: Consequences, Source, and Suggestions](https://doi.org/10.18653/v1/2020.emnlp-main.659) |  | 0 | We find that the performance of state-of-the-art models on Natural Language Inference (NLI) and Reading Comprehension (RC) analysis/stress sets can be highly unstable. This raises three questions: (1) How will the instability affect the reliability of the conclusions drawn based on these analysis... | Xiang Zhou, Yixin Nie, Hao Tan, Mohit Bansal |  |
| 1144 |  |  [Universal Natural Language Processing with Limited Annotations: Try Few-shot Textual Entailment as a Start](https://doi.org/10.18653/v1/2020.emnlp-main.660) |  | 0 | A standard way to address different NLP problems is by first constructing a problem-specific dataset, then building a model to fit this dataset. To build the ultimate artificial intelligence, we desire a single machine that can handle diverse new problems, for which task-specific annotations are... | Wenpeng Yin, Nazneen Fatema Rajani, Dragomir R. Radev, Richard Socher, Caiming Xiong |  |
| 1145 |  |  [ConjNLI: Natural Language Inference Over Conjunctive Sentences](https://doi.org/10.18653/v1/2020.emnlp-main.661) |  | 0 | Reasoning about conjuncts in conjunctive sentences is important for a deeper understanding of conjunctions in English and also how their usages and semantics differ from conjunctive and disjunctive boolean logic. Existing NLI stress tests do not consider non-boolean usages of conjunctions and use... | Swarnadeep Saha, Yixin Nie, Mohit Bansal |  |
| 1146 |  |  [Data and Representation for Turkish Natural Language Inference](https://doi.org/10.18653/v1/2020.emnlp-main.662) |  | 0 | Large annotated datasets in NLP are overwhelmingly in English. This is an obstacle to progress in other languages. Unfortunately, obtaining new annotated resources for each task in each language would be prohibitively expensive. At the same time, commercial machine translation systems are now... | Emrah Budur, Riza Özçelik, Tunga Gungor, Christopher Potts |  |
| 1147 |  |  [Multitask Learning for Cross-Lingual Transfer of Broad-coverage Semantic Dependencies](https://doi.org/10.18653/v1/2020.emnlp-main.663) |  | 0 | We describe a method for developing broad-coverage semantic dependency parsers for languages for which no semantically annotated resource is available. We leverage a multitask learning framework coupled with annotation projection. We use syntactic parsing as the auxiliary task in our multitask... | Maryam Aminian, Mohammad Sadegh Rasooli, Mona T. Diab |  |
| 1148 |  |  [Precise Task Formalization Matters in Winograd Schema Evaluations](https://doi.org/10.18653/v1/2020.emnlp-main.664) |  | 0 | Performance on the Winograd Schema Challenge (WSC), a respected English commonsense reasoning benchmark, recently rocketed from chance accuracy to 89% on the SuperGLUE leaderboard, with relatively little corroborating evidence of a correspondingly large improvement in reasoning ability. We... | Haokun Liu, William Huang, Dhara A. Mungra, Samuel R. Bowman |  |
| 1149 |  |  [Avoiding the Hypothesis-Only Bias in Natural Language Inference via Ensemble Adversarial Training](https://doi.org/10.18653/v1/2020.emnlp-main.665) |  | 0 | Natural Language Inference (NLI) datasets contain annotation artefacts resulting in spurious correlations between the natural language utterances and their respective entailment classes. These artefacts are exploited by neural networks even when only considering the hypothesis and ignoring the... | Joe Stacey, Pasquale Minervini, Haim Dubossarsky, Sebastian Riedel, Tim Rocktäschel |  |
| 1150 |  |  [SynSetExpan: An Iterative Framework for Joint Entity Set Expansion and Synonym Discovery](https://doi.org/10.18653/v1/2020.emnlp-main.666) |  | 0 | Entity set expansion and synonym discovery are two critical NLP tasks. Previous studies accomplish them separately, without exploring their interdependencies. In this work, we hypothesize that these two tasks are tightly coupled because two synonymous entities tend to have a similar likelihood of... | Jiaming Shen, Wenda Qiu, Jingbo Shang, Michelle Vanni, Xiang Ren, Jiawei Han |  |
| 1151 |  |  [Evaluating the Calibration of Knowledge Graph Embeddings for Trustworthy Link Prediction](https://doi.org/10.18653/v1/2020.emnlp-main.667) |  | 0 | Little is known about the trustworthiness of predictions made by knowledge graph embedding (KGE) models. In this paper we take initial steps toward this direction by investigating the calibration of KGE models, or the extent to which they output confidence scores that reflect the expected... | Tara Safavi, Danai Koutra, Edgar Meij |  |
| 1152 |  |  [Text Graph Transformer for Document Classification](https://doi.org/10.18653/v1/2020.emnlp-main.668) |  | 0 | Text classification is a fundamental problem in natural language processing. Recent studies applied graph neural network (GNN) techniques to capture global word co-occurrence in a corpus. However, previous works are not scalable to large-sized corpus and ignore the heterogeneity of the text graph.... | Haopeng Zhang, Jiawei Zhang |  |
| 1153 |  |  [CoDEx: A Comprehensive Knowledge Graph Completion Benchmark](https://doi.org/10.18653/v1/2020.emnlp-main.669) |  | 0 | We present CoDEx, a set of knowledge graph completion datasets extracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. In terms of scope, CoDEx comprises three knowledge graphs varying in size and structure,... | Tara Safavi, Danai Koutra |  |
| 1154 |  |  [META: Metadata-Empowered Weak Supervision for Text Classification](https://doi.org/10.18653/v1/2020.emnlp-main.670) |  | 0 | Recent advances in weakly supervised learning enable training high-quality text classifiers by only providing a few user-provided seed words. Existing methods mainly use text data alone to generate pseudo-labels despite the fact that metadata information (e.g., author and timestamp) is widely... | Dheeraj Mekala, Xinyang Zhang, Jingbo Shang |  |
| 1155 |  |  [Towards More Accurate Uncertainty Estimation In Text Classification](https://doi.org/10.18653/v1/2020.emnlp-main.671) |  | 0 | The uncertainty measurement of classified results is especially important in areas requiring limited human resources for higher accuracy. For instance, data-driven algorithms diagnosing diseases need accurate uncertainty score to decide whether additional but limited quantity of experts are needed... | Jianfeng He, Xuchao Zhang, Shuo Lei, Zhiqian Chen, Fanglan Chen, Abdulaziz Alhamadani, Bei Xiao, ChangTien Lu |  |
| 1156 |  |  [Chapter Captor: Text Segmentation in Novels](https://doi.org/10.18653/v1/2020.emnlp-main.672) |  | 0 | Books are typically segmented into chapters and sections, representing coherent sub-narratives and topics. We investigate the task of predicting chapter boundaries, as a proxy for the general task of segmenting long texts. We build a Project Gutenberg chapter segmentation data set of 9,126 English... | Charuta Pethe, Allen Kim, Steven Skiena |  |
| 1157 |  |  [Authorship Attribution for Neural Text Generation](https://doi.org/10.18653/v1/2020.emnlp-main.673) |  | 0 | In recent years, the task of generating realistic short and long texts have made tremendous advancements. In particular, several recently proposed neural network-based language models have demonstrated their astonishing capabilities to generate texts that are challenging to distinguish from... | Adaku Uchendu, Thai Le, Kai Shu, Dongwon Lee |  |
| 1158 |  |  [NwQM: A neural quality assessment framework for Wikipedia](https://doi.org/10.18653/v1/2020.emnlp-main.674) |  | 0 | Millions of people irrespective of socioeconomic and demographic backgrounds, depend on Wikipedia articles everyday for keeping themselves informed regarding popular as well as obscure topics. Articles have been categorized by editors into several quality classes, which indicate their reliability as... | Bhanu Prakash Reddy Guda, Sasi Bhushan Seelaboyina, Soumya Sarkar, Animesh Mukherjee |  |
| 1159 |  |  [Towards Modeling Revision Requirements in wikiHow Instructions](https://doi.org/10.18653/v1/2020.emnlp-main.675) |  | 0 | wikiHow is a resource of how-to guidesthat describe the steps necessary to accomplish a goal. Guides in this resource are regularly edited by a community of users, who try to improve instructions in terms of style, clarity and correctness. In this work, we test whether the need for such edits can be... | Irshad Bhat, Talita Anthonio, Michael Roth |  |
| 1160 |  |  [Deep Attentive Learning for Stock Movement Prediction From Social Media Text and Company Correlations](https://doi.org/10.18653/v1/2020.emnlp-main.676) |  | 0 | In the financial domain, risk modeling and profit generation heavily rely on the sophisticated and intricate stock movement prediction task. Stock forecasting is complex, given the stochastic dynamics and non-stationary behavior of the market. Stock movements are influenced by varied factors beyond... | Ramit Sawhney, Shivam Agarwal, Arnav Wadhwa, Rajiv Ratn Shah |  |
| 1161 |  |  [Natural Language Processing for Achieving Sustainable Development: the Case of Neural Labelling to Enhance Community Profiling](https://doi.org/10.18653/v1/2020.emnlp-main.677) |  | 0 | In recent years, there has been an increasing interest in the application of Artificial Intelligence – and especially Machine Learning – to the field of Sustainable Development (SD). However, until now, NLP has not been systematically applied in this context. In this paper, we show the high... | Costanza Conforti, Stephanie Hirmer, Dai Morgan, Marco Basaldella, Yau Ben Or |  |
| 1162 |  |  [To Schedule or not to Schedule: Extracting Task Specific Temporal Entities and Associated Negation Constraints](https://doi.org/10.18653/v1/2020.emnlp-main.678) |  | 0 | State of the art research for date-time entity extraction from text is task agnostic. Consequently, while the methods proposed in literature perform well for generic date-time extraction from texts, they don’t fare as well on task specific date-time entity extraction where only a subset of the... | Barun Patra, Chala Fufa, Pamela Bhattacharya, Charles Lee |  |
| 1163 |  |  [Competence-Level Prediction and Resume & Job Description Matching Using Context-Aware Transformer Models](https://doi.org/10.18653/v1/2020.emnlp-main.679) |  | 0 | This paper presents a comprehensive study on resume classification to reduce the time and labor needed to screen an overwhelming number of applications significantly, while improving the selection of suitable candidates. A total of 6,492 resumes are extracted from 24,933 job applications for 252... | Changmao Li, Elaine Fisher, Rebecca Thomas, Steve Pittard, Vicki Hertzberg, Jinho D. Choi |  |
| 1164 |  |  [Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses](https://doi.org/10.18653/v1/2020.emnlp-main.680) |  | 0 | Evaluation of grammatical error correction (GEC) systems has primarily focused on essays written by non-native learners of English, which however is only part of the full spectrum of GEC applications. We aim to broaden the target domain of GEC and release CWEB, a new benchmark for GEC consisting of... | Simon Flachs, Ophélie Lacroix, Helen Yannakoudakis, Marek Rei, Anders Søgaard |  |
| 1165 |  |  [Deconstructing word embedding algorithms](https://doi.org/10.18653/v1/2020.emnlp-main.681) |  | 0 | Word embeddings are reliable feature representations of words used to obtain high quality results for various NLP applications. Uncontextualized word embeddings are used in many NLP tasks today, especially in resource-limited settings where high memory capacity and GPUs are not available. Given the... | Kian KenyonDean, Edward Newell, Jackie Chi Kit Cheung |  |
| 1166 |  |  [Sequential Modelling of the Evolution of Word Representations for Semantic Change Detection](https://doi.org/10.18653/v1/2020.emnlp-main.682) |  | 0 | Semantic change detection concerns the task of identifying words whose meaning has changed over time. Current state-of-the-art approaches operating on neural embeddings detect the level of semantic change in a word by comparing its vector representation in two distinct time periods, without... | Adam Tsakalidis, Maria Liakata |  |
| 1167 |  |  [Sparsity Makes Sense: Word Sense Disambiguation Using Sparse Contextualized Word Representations](https://doi.org/10.18653/v1/2020.emnlp-main.683) |  | 0 | In this paper, we demonstrate that by utilizing sparse word representations, it becomes possible to surpass the results of more complex task-specific models on the task of fine-grained all-words word sense disambiguation. Our proposed algorithm relies on an overcomplete set of semantic basis vectors... | Gábor Berend |  |
| 1168 |  |  [Exploring Semantic Capacity of Terms](https://doi.org/10.18653/v1/2020.emnlp-main.684) |  | 0 | We introduce and study semantic capacity of terms. For example, the semantic capacity of artificial intelligence is higher than that of linear regression since artificial intelligence possesses a broader meaning scope. Understanding semantic capacity of terms will help many downstream tasks in... | Jie Huang, Zilong Wang, Kevin Chang, WenMei Hwu, Jinjun Xiong |  |
| 1169 |  |  [Learning to Ignore: Long Document Coreference with Bounded Memory Neural Networks](https://doi.org/10.18653/v1/2020.emnlp-main.685) |  | 0 | Long document coreference resolution remains a challenging task due to the large memory and runtime requirements of current models. Recent work doing incremental coreference resolution using just the global representation of entities shows practical benefits but requires keeping all entities in... | Shubham Toshniwal, Sam Wiseman, Allyson Ettinger, Karen Livescu, Kevin Gimpel |  |
| 1170 |  |  [Revealing the Myth of Higher-Order Inference in Coreference Resolution](https://doi.org/10.18653/v1/2020.emnlp-main.686) |  | 0 | This paper analyzes the impact of higher-order inference (HOI) on the task of coreference resolution. HOI has been adapted by almost all recent coreference resolution models without taking much investigation on its true effectiveness over representation learning. To make a comprehensive analysis, we... | Liyan Xu, Jinho D. Choi |  |
| 1171 |  |  [Pre-training Mention Representations in Coreference Models](https://doi.org/10.18653/v1/2020.emnlp-main.687) |  | 0 | Collecting labeled data for coreference resolution is a challenging task, requiring skilled annotators. It is thus desirable to develop coreference resolution models that can make use of unlabeled data. Here we provide such an approach for the powerful class of neural coreference models. These... | Yuval Varkel, Amir Globerson |  |
| 1172 |  |  [Learning Collaborative Agents with Rule Guidance for Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2020.emnlp-main.688) |  | 0 | Walk-based models have shown their advantages in knowledge graph (KG) reasoning by achieving decent performance while providing interpretable decisions. However, the sparse reward signals offered by the KG during a traversal are often insufficient to guide a sophisticated walk-based reinforcement... | Deren Lei, Gangrong Jiang, Xiaotao Gu, Kexuan Sun, Yuning Mao, Xiang Ren |  |
| 1173 |  |  [Exploring Contextualized Neural Language Models for Temporal Dependency Parsing](https://doi.org/10.18653/v1/2020.emnlp-main.689) |  | 0 | Extracting temporal relations between events and time expressions has many applications such as constructing event timelines and time-related question answering. It is a challenging problem which requires syntactic and semantic information at sentence or discourse levels, which may be captured by... | Hayley Ross, Jonathon Cai, Bonan Min |  |
| 1174 |  |  [Systematic Comparison of Neural Architectures and Training Approaches for Open Information Extraction](https://doi.org/10.18653/v1/2020.emnlp-main.690) |  | 0 | The goal of open information extraction (OIE) is to extract facts from natural language text, and to represent them as structured triples of the form <subject,predicate, object>. For example, given the sentence “Beethoven composed the Ode to Joy.”, we are expected to extract the triple <Beethoven,... | Patrick Hohenecker, Frank Mtumbuka, Vid Kocijan, Thomas Lukasiewicz |  |
| 1175 |  |  [SeqMix: Augmenting Active Sequence Labeling via Sequence Mixup](https://doi.org/10.18653/v1/2020.emnlp-main.691) |  | 0 | Active learning is an important technique for low-resource sequence labeling tasks. However, current active sequence labeling methods use the queried samples alone in each iteration, which is an inefficient way of leveraging human annotations. We propose a simple but effective data augmentation... | Rongzhi Zhang, Yue Yu, Chao Zhang |  |
| 1176 |  |  [AxCell: Automatic Extraction of Results from Machine Learning Papers](https://doi.org/10.18653/v1/2020.emnlp-main.692) |  | 0 | Tracking progress in machine learning has become increasingly difficult with the recent explosion in the number of papers. In this paper, we present AxCell, an automatic machine learning pipeline for extracting results from papers. AxCell uses several novel components, including a table segmentation... | Marcin Kardas, Piotr Czapla, Pontus Stenetorp, Sebastian Ruder, Sebastian Riedel, Ross Taylor, Robert Stojnic |  |
| 1177 |  |  [Knowledge-guided Open Attribute Value Extraction with Reinforcement Learning](https://doi.org/10.18653/v1/2020.emnlp-main.693) |  | 0 | Open attribute value extraction for emerging entities is an important but challenging task. A lot of previous works formulate the problem as a question-answering (QA) task. While the collections of articles from web corpus provide updated information about the emerging entities, the retrieved texts... | Ye Liu, Sheng Zhang, Rui Song, Suo Feng, Yanghua Xiao |  |
| 1178 |  |  [DualTKB: A Dual Learning Bridge between Text and Knowledge Base](https://doi.org/10.18653/v1/2020.emnlp-main.694) |  | 0 | In this work, we present a dual learning approach for unsupervised text to path and path to text transfers in Commonsense Knowledge Bases (KBs). We investigate the impact of weak supervision by creating a weakly supervised dataset and show that even a slight amount of supervision can significantly... | Pierre L. Dognin, Igor Melnyk, Inkit Padhi, Cícero Nogueira dos Santos, Payel Das |  |
| 1179 |  |  [Incremental Neural Coreference Resolution in Constant Memory](https://doi.org/10.18653/v1/2020.emnlp-main.695) |  | 0 | We investigate modeling coreference resolution under a fixed memory constraint by extending an incremental clustering algorithm to utilize contextualized encoders and neural components. Given a new sentence, our end-to-end algorithm proposes and scores each mention span against explicit entity... | Patrick Xia, João Sedoc, Benjamin Van Durme |  |
| 1180 |  |  [Improving Low Compute Language Modeling with In-Domain Embedding Initialisation](https://doi.org/10.18653/v1/2020.emnlp-main.696) |  | 0 | Many NLP applications, such as biomedical data and technical support, have 10-100 million tokens of in-domain data and limited computational resources for learning from it. How should we train a language model in this scenario? Most language modeling research considers either a small dataset with a... | Charles Welch, Rada Mihalcea, Jonathan K. Kummerfeld |  |
| 1181 |  |  [KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation](https://doi.org/10.18653/v1/2020.emnlp-main.697) |  | 0 | Data-to-text generation has recently attracted substantial interests due to its wide applications. Existing methods have shown impressive performance on an array of tasks. However, they rely on a significant amount of labeled data for each task, which is costly to acquire and thus limits their... | Wenhu Chen, Yu Su, Xifeng Yan, William Yang Wang |  |
| 1182 |  |  [POINTER: Constrained Progressive Text Generation via Insertion-based Generative Pre-training](https://doi.org/10.18653/v1/2020.emnlp-main.698) |  | 0 | Large-scale pre-trained language models, such as BERT and GPT-2, have achieved excellent performance in language representation learning and free-form text generation. However, these models cannot be directly employed to generate text under specified lexical constraints. To address this challenge,... | Yizhe Zhang, Guoyin Wang, Chunyuan Li, Zhe Gan, Chris Brockett, Bill Dolan |  |
| 1183 |  |  [Unsupervised Text Style Transfer with Padded Masked Language Models](https://doi.org/10.18653/v1/2020.emnlp-main.699) |  | 0 | We propose Masker, an unsupervised text-editing method for style transfer. To tackle cases when no parallel source–target pairs are available, we train masked language models (MLMs) for both the source and the target domain. Then we find the text spans where the two models disagree the most in terms... | Eric Malmi, Aliaksei Severyn, Sascha Rothe |  |
| 1184 |  |  [PALM: Pre-training an Autoencoding&Autoregressive Language Model for Context-conditioned Generation](https://doi.org/10.18653/v1/2020.emnlp-main.700) |  | 0 | Self-supervised pre-training, such as BERT, MASS and BART, has emerged as a powerful technique for natural language understanding and generation. Existing pre-training techniques employ autoencoding and/or autoregressive objectives to train Transformer-based models by recovering original word tokens... | Bin Bi, Chenliang Li, Chen Wu, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, Luo Si |  |
| 1185 |  |  [Gradient-guided Unsupervised Lexically Constrained Text Generation](https://doi.org/10.18653/v1/2020.emnlp-main.701) |  | 0 | Lexically constrained generation requires the target sentence to satisfy some lexical constraints, such as containing some specific words or being the paraphrase to a given sentence, which is very important in many real-world natural language generation applications. Previous works usually apply... | Lei Sha |  |
| 1186 |  |  [TeaForN: Teacher-Forcing with N-grams](https://doi.org/10.18653/v1/2020.emnlp-main.702) |  | 0 | Sequence generation models trained with teacher-forcing suffer from issues related to exposure bias and lack of differentiability across timesteps. Our proposed method, Teacher-Forcing with N-grams (TeaForN), addresses both these problems directly, through the use of a stack of N decoders trained to... | Sebastian Goodman, Nan Ding, Radu Soricut |  |
| 1187 |  |  [Experience Grounds Language](https://doi.org/10.18653/v1/2020.emnlp-main.703) |  | 0 | Language understanding research is held back by a failure to relate language to the physical world it describes and to the social interactions it facilitates. Despite the incredible effectiveness of language processing models to tackle tasks after being trained on text alone, successful linguistic... | Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, Joseph P. Turian |  |
| 1188 |  |  [Keep CALM and Explore: Language Models for Action Generation in Text-based Games](https://doi.org/10.18653/v1/2020.emnlp-main.704) |  | 0 | Text-based games present a unique challenge for autonomous agents to operate in natural language and handle enormous action spaces. In this paper, we propose the Contextual Action Language Model (CALM) to generate a compact set of action candidates at each game state. Our key insight is to train... | Shunyu Yao, Rohan Rao, Matthew J. Hausknecht, Karthik Narasimhan |  |
| 1189 |  |  [CapWAP: Image Captioning with a Purpose](https://doi.org/10.18653/v1/2020.emnlp-main.705) |  | 0 | The traditional image captioning task uses generic reference captions to provide textual information about images. Different user populations, however, will care about different visual aspects of images. In this paper, we propose a new task, Captioning with A Purpose (CapWAP). Our goal is to develop... | Adam Fisch, Kenton Lee, MingWei Chang, Jonathan H. Clark, Regina Barzilay |  |
| 1190 |  |  [What is More Likely to Happen Next? Video-and-Language Future Event Prediction](https://doi.org/10.18653/v1/2020.emnlp-main.706) |  | 0 | Given a video with aligned dialogue, people can often infer what is more likely to happen next. Making such predictions requires not only a deep understanding of the rich dynamics underlying the video and dialogue, but also a significant amount of commonsense knowledge. In this work, we explore... | Jie Lei, Licheng Yu, Tamara L. Berg, Mohit Bansal |  |
| 1191 |  |  [X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers](https://doi.org/10.18653/v1/2020.emnlp-main.707) |  | 0 | Mirroring the success of masked language models, vision-and-language counterparts like VILBERT, LXMERT and UNITER have achieved state of the art performance on a variety of multimodal discriminative tasks like visual question answering and visual grounding. Recent work has also successfully adapted... | Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi, Aniruddha Kembhavi |  |
| 1192 |  |  [Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations](https://doi.org/10.18653/v1/2020.emnlp-main.708) |  | 0 | A major challenge in visually grounded language generation is to build robust benchmark datasets and models that can generalize well in real-world settings. To do this, it is critical to ensure that our evaluation protocols are correct, and benchmarks are reliable. In this work, we set forth to... | Wanrong Zhu, Xin Wang, Pradyumna Narayana, Kazoo Sone, Sugato Basu, William Yang Wang |  |
| 1193 |  |  [Beyond Instructional Videos: Probing for More Diverse Visual-Textual Grounding on YouTube](https://doi.org/10.18653/v1/2020.emnlp-main.709) |  | 0 | Pretraining from unlabelled web videos has quickly become the de-facto means of achieving high performance on many video understanding tasks. Features are learned via prediction of grounded relationships between visual content and automatic speech recognition (ASR) tokens. However, prior pretraining... | Jack Hessel, Zhenhai Zhu, Bo Pang, Radu Soricut |  |
| 1194 |  |  [Hierarchical Graph Network for Multi-hop Question Answering](https://doi.org/10.18653/v1/2020.emnlp-main.710) |  | 0 | In this paper, we present Hierarchical Graph Network (HGN) for multi-hop question answering. To aggregate clues from scattered texts across multiple paragraphs, a hierarchical graph is created by constructing nodes on different levels of granularity (questions, paragraphs, sentences, entities), the... | Yuwei Fang, Siqi Sun, Zhe Gan, Rohit Pillai, Shuohang Wang, Jingjing Liu |  |
| 1195 |  |  [A Simple Yet Strong Pipeline for HotpotQA](https://doi.org/10.18653/v1/2020.emnlp-main.711) |  | 0 | State-of-the-art models for multi-hop question answering typically augment large-scale language models like BERT with additional, intuitively useful capabilities such as named entity recognition, graph-based reasoning, and question decomposition. However, does their strong performance on popular... | Dirk Groeneveld, Tushar Khot, Mausam, Ashish Sabharwal |  |
| 1196 |  |  [Is Multihop QA in DiRe Condition? Measuring and Reducing Disconnected Reasoning](https://doi.org/10.18653/v1/2020.emnlp-main.712) |  | 0 | Has there been real progress in multi-hop question-answering? Models often exploit dataset artifacts to produce correct answers, without connecting information across multiple supporting facts. This limits our ability to measure true progress and defeats the purpose of building multi-hop QA... | Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal |  |
| 1197 |  |  [Unsupervised Question Decomposition for Question Answering](https://doi.org/10.18653/v1/2020.emnlp-main.713) |  | 0 | We aim to improve question answering (QA) by decomposing hard questions into simpler sub-questions that existing QA systems are capable of answering. Since labeling questions with decompositions is cumbersome, we take an unsupervised approach to produce sub-questions, also enabling us to leverage... | Ethan Perez, Patrick Lewis, Wentau Yih, Kyunghyun Cho, Douwe Kiela |  |
| 1198 |  |  [SRLGRN: Semantic Role Labeling Graph Reasoning Network](https://doi.org/10.18653/v1/2020.emnlp-main.714) |  | 0 | This work deals with the challenge of learning and reasoning over multi-hop question answering (QA). We propose a graph reasoning network based on the semantic structure of the sentences to learn cross paragraph reasoning paths and find the supporting facts and the answer jointly. The proposed graph... | Chen Zheng, Parisa Kordjamshidi |  |
| 1199 |  |  [CancerEmo: A Dataset for Fine-Grained Emotion Detection](https://doi.org/10.18653/v1/2020.emnlp-main.715) |  | 0 | Emotions are an important element of human nature, often affecting the overall wellbeing of a person. Therefore, it is no surprise that the health domain is a valuable area of interest for emotion detection, as it can provide medical staff or caregivers with essential information about patients.... | Tiberiu Sosea, Cornelia Caragea |  |
| 1200 |  |  [Exploring the Role of Argument Structure in Online Debate Persuasion](https://doi.org/10.18653/v1/2020.emnlp-main.716) |  | 0 | Online debate forums provide users a platform to express their opinions on controversial topics while being exposed to opinions from diverse set of viewpoints. Existing work in Natural Language Processing (NLP) has shown that linguistic features extracted from the debate text and features encoding... | Jialu Li, Esin Durmus, Claire Cardie |  |
| 1201 |  |  [Zero-Shot Stance Detection: A Dataset and Model using Generalized Topic Representations](https://doi.org/10.18653/v1/2020.emnlp-main.717) |  | 0 | Stance detection is an important component of understanding hidden influences in everyday life. Since there are thousands of potential topics to take a stance on, most with little to no training data, we focus on zero-shot stance detection: classifying stance from no training examples. In this... | Emily Allaway, Kathleen R. McKeown |  |
| 1202 |  |  [Sentiment Analysis of Tweets using Heterogeneous Multi-layer Network Representation and Embedding](https://doi.org/10.18653/v1/2020.emnlp-main.718) |  | 0 | Sentiment classification on tweets often needs to deal with the problems of under-specificity, noise, and multilingual content. This study proposes a heterogeneous multi-layer network-based representation of tweets to generate multiple representations of a tweet and address the above issues. The... | Loitongbam Gyanendro Singh, Anasua Mitra, Sanasam Ranbir Singh |  |
| 1203 |  |  [Introducing Syntactic Structures into Target Opinion Word Extraction with Deep Learning](https://doi.org/10.18653/v1/2020.emnlp-main.719) |  | 0 | Targeted opinion word extraction (TOWE) is a sub-task of aspect based sentiment analysis (ABSA) which aims to find the opinion words for a given aspect-term in a sentence. Despite their success for TOWE, the current deep learning models fail to exploit the syntactic information of the sentences that... | Amir Pouran Ben Veyseh, Nasim Nouri, Franck Dernoncourt, Dejing Dou, Thien Huu Nguyen |  |
| 1204 |  |  [EmoTag1200: Understanding the Association between Emojis and Emotions](https://doi.org/10.18653/v1/2020.emnlp-main.720) |  | 0 | Given the growing ubiquity of emojis in language, there is a need for methods and resources that shed light on their meaning and communicative role. One conspicuous aspect of emojis is their use to convey affect in ways that may otherwise be non-trivial to achieve. In this paper, we seek to explore... | Abu Awal Md Shoeb, Gerard de Melo |  |
| 1205 |  |  [MIME: MIMicking Emotions for Empathetic Response Generation](https://doi.org/10.18653/v1/2020.emnlp-main.721) |  | 0 | Current approaches to empathetic response generation view the set of emotions expressed in the input text as a flat structure, where all the emotions are treated uniformly. We argue that empathetic responses often mimic the emotion of the user to a varying degree, depending on its positivity or... | Navonil Majumder, Pengfei Hong, Shanshan Peng, Jiankun Lu, Deepanway Ghosal, Alexander F. Gelbukh, Rada Mihalcea, Soujanya Poria |  |
| 1206 |  |  [Exploiting Structured Knowledge in Text via Graph-Guided Representation Learning](https://doi.org/10.18653/v1/2020.emnlp-main.722) |  | 0 | In this work, we aim at equipping pre-trained language models with structured knowledge. We present two self-supervised tasks learning over raw text with the guidance from knowledge graphs. Building upon entity-level masked language models, our first contribution is an entity masking scheme that... | Tao Shen, Yi Mao, Pengcheng He, Guodong Long, Adam Trischler, Weizhu Chen |  |
| 1207 |  |  [Named Entity Recognition Only from Word Embeddings](https://doi.org/10.18653/v1/2020.emnlp-main.723) |  | 0 | Deep neural network models have helped named entity recognition achieve amazing performance without handcrafting features. However, existing systems require large amounts of human annotated training data. Efforts have been made to replace human annotations with external knowledge (e.g., NE... | Ying Luo, Hai Zhao, Junlang Zhan |  |
| 1208 |  |  [Text Classification Using Label Names Only: A Language Model Self-Training Approach](https://doi.org/10.18653/v1/2020.emnlp-main.724) |  | 0 | Current text classification methods typically require a good number of human-labeled documents as training data, which can be costly and difficult to obtain in real applications. Humans can perform classification without seeing any labeled examples but only based on a small set of words describing... | Yu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong, Heng Ji, Chao Zhang, Jiawei Han |  |
| 1209 |  |  [Neural Topic Modeling with Cycle-Consistent Adversarial Training](https://doi.org/10.18653/v1/2020.emnlp-main.725) |  | 0 | Advances on deep generative models have attracted significant research interest in neural topic modeling. The recently proposed Adversarial-neural Topic Model models topics with an adversarially trained generator network and employs Dirichlet prior to capture the semantic patterns in latent topics.... | Xuemeng Hu, Rui Wang, Deyu Zhou, Yuxuan Xiong |  |
| 1210 |  |  [Data Boost: Text Data Augmentation Through Reinforcement Learning Guided Conditional Generation](https://doi.org/10.18653/v1/2020.emnlp-main.726) |  | 0 | Data augmentation is proven to be effective in many NLU tasks, especially for those suffering from data scarcity. In this paper, we present a powerful and easy to deploy text augmentation framework, Data Boost, which augments data through reinforcement learning guided conditional generation. We... | Ruibo Liu, Guangxuan Xu, Chenyan Jia, Weicheng Ma, Lili Wang, Soroush Vosoughi |  |
| 1211 |  |  [A State-independent and Time-evolving Network for Early Rumor Detection in Social Media](https://doi.org/10.18653/v1/2020.emnlp-main.727) |  | 0 | In this paper, we study automatic rumor detection for in social media at the event level where an event consists of a sequence of posts organized according to the posting time. It is common that the state of an event is dynamically evolving. However, most of the existing methods to this task ignored... | Rui Xia, Kaizhou Xuan, Jianfei Yu |  |
| 1212 |  |  [PyMT5: multi-mode translation of natural language and Python code with transformers](https://doi.org/10.18653/v1/2020.emnlp-main.728) |  | 0 | Simultaneously modeling source code and natural language has many exciting applications in automated software development and understanding. Pursuant to achieving such technology, we introduce PyMT5, the Python method text-to-text transfer transformer, which is trained to translate between all pairs... | Colin B. Clement, Dawn Drain, Jonathan Timcheck, Alexey Svyatkovskiy, Neel Sundaresan |  |
| 1213 |  |  [PathQG: Neural Question Generation from Facts](https://doi.org/10.18653/v1/2020.emnlp-main.729) |  | 0 | Existing research for question generation encodes the input text as a sequence of tokens without explicitly modeling fact information. These models tend to generate irrelevant and uninformative questions. In this paper, we explore to incorporate facts in the text for question generation in a... | Siyuan Wang, Zhongyu Wei, Zhihao Fan, Zengfeng Huang, Weijian Sun, Qi Zhang, Xuanjing Huang |  |
| 1214 |  |  [What time is it? Temporal Analysis of Novels](https://doi.org/10.18653/v1/2020.emnlp-main.730) |  | 0 | Recognizing the flow of time in a story is a crucial aspect of understanding it. Prior work related to time has primarily focused on identifying temporal expressions or relative sequencing of events, but here we propose computationally annotating each line of a book with wall clock times, even in... | Allen Kim, Charuta Pethe, Steven Skiena |  |
| 1215 |  |  [COGS: A Compositional Generalization Challenge Based on Semantic Interpretation](https://doi.org/10.18653/v1/2020.emnlp-main.731) |  | 0 | Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based... | Najoung Kim, Tal Linzen |  |
| 1216 |  |  [An Analysis of Natural Language Inference Benchmarks through the Lens of Negation](https://doi.org/10.18653/v1/2020.emnlp-main.732) |  | 0 | Negation is underrepresented in existing natural language inference benchmarks. Additionally, one can often ignore the few negations in existing benchmarks and still make the right inference judgments. In this paper, we present a new benchmark for natural language inference in which negation plays a... | Md Mosharaf Hossain, Venelin Kovatchev, Pranoy Dutta, Tiffany Kao, Elizabeth Wei, Eduardo Blanco |  |
| 1217 |  |  [On the Sentence Embeddings from Pre-trained Language Models](https://doi.org/10.18653/v1/2020.emnlp-main.733) |  | 0 | Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the... | Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, Lei Li |  |
| 1218 |  |  [What Can We Learn from Collective Human Opinions on Natural Language Inference Data?](https://doi.org/10.18653/v1/2020.emnlp-main.734) |  | 0 | Despite the subjective nature of many NLP tasks, most NLU evaluations have focused on using the majority label with presumably high agreement as the ground truth. Less attention has been paid to the distribution of human opinions. We collect ChaosNLI, a dataset with a total of 464,500 annotations to... | Yixin Nie, Xiang Zhou, Mohit Bansal |  |
| 1219 |  |  [Improving Text Generation with Student-Forcing Optimal Transport](https://doi.org/10.18653/v1/2020.emnlp-main.735) |  | 0 | Neural language models are often trained with maximum likelihood estimation (MLE), where the next word is generated conditioned on the ground-truth word tokens. During testing, however, the model is instead conditioned on previously generated tokens, resulting in what is termed exposure bias. To... | Jianqiao Li, Chunyuan Li, Guoyin Wang, Hao Fu, YuhChen Lin, Liqun Chen, Yizhe Zhang, Chenyang Tao, Ruiyi Zhang, Wenlin Wang, Dinghan Shen, Qian Yang, Lawrence Carin |  |
| 1220 |  |  [UNION: An Unreferenced Metric for Evaluating Open-ended Story Generation](https://doi.org/10.18653/v1/2020.emnlp-main.736) |  | 0 | Despite the success of existing referenced metrics (e.g., BLEU and MoverScore), they correlate poorly with human judgments for open-ended text generation including story or dialog generation because of the notorious one-to-many issue: there are many plausible outputs for the same input, which may... | Jian Guan, Minlie Huang |  |
| 1221 |  |  [F\^2-Softmax: Diversifying Neural Text Generation via Frequency Factorized Softmax](https://doi.org/10.18653/v1/2020.emnlp-main.737) |  | 0 | Despite recent advances in neural text generation, encoding the rich diversity in human language remains elusive. We argue that the sub-optimal text generation is mainly attributable to the imbalanced token distribution, which particularly misdirects the learning model when trained with the... | ByungJu Choi, Jimin Hong, David Keetae Park, Sang Wan Lee |  |
| 1222 |  |  [Partially-Aligned Data-to-Text Generation with Distant Supervision](https://doi.org/10.18653/v1/2020.emnlp-main.738) |  | 0 | The Data-to-Text task aims to generate human-readable text for describing some given structured data enabling more interpretability. However, the typical generation task is confined to a few particular domains since it requires well-aligned data which is difficult and expensive to obtain. Using... | Zihao Fu, Bei Shi, Wai Lam, Lidong Bing, Zhiyuan Liu |  |
| 1223 |  |  [Like hiking? You probably enjoy nature: Persona-grounded Dialog with Commonsense Expansions](https://doi.org/10.18653/v1/2020.emnlp-main.739) |  | 0 | Existing persona-grounded dialog models often fail to capture simple implications of given persona descriptions, something which humans are able to do seamlessly. For example, state-of-the-art models cannot infer that interest in hiking might imply love for nature or longing for a break. In this... | Bodhisattwa Prasad Majumder, Harsh Jhamtani, Taylor BergKirkpatrick, Julian J. McAuley |  |
| 1224 |  |  [A Probabilistic End-To-End Task-Oriented Dialog Model with Latent Belief States towards Semi-Supervised Learning](https://doi.org/10.18653/v1/2020.emnlp-main.740) |  | 0 | Structured belief states are crucial for user goal tracking and database query in task-oriented dialog systems. However, training belief trackers often requires expensive turn-level annotations of every user utterance. In this paper we aim at alleviating the reliance on belief state labels in... | Yichi Zhang, Zhijian Ou, Min Hu, Junlan Feng |  |
| 1225 |  |  [The World is Not Binary: Learning to Rank with Grayscale Data for Dialogue Response Selection](https://doi.org/10.18653/v1/2020.emnlp-main.741) |  | 0 | Response selection plays a vital role in building retrieval-based conversation systems. Despite that response selection is naturally a learning-to-rank problem, most prior works take a point-wise view and train binary classifiers for this task: each response candidate is labeled either relevant... | Zibo Lin, Deng Cai, Yan Wang, Xiaojiang Liu, Haitao Zheng, Shuming Shi |  |
| 1226 |  |  [GRADE: Automatic Graph-Enhanced Coherence Metric for Evaluating Open-Domain Dialogue Systems](https://doi.org/10.18653/v1/2020.emnlp-main.742) |  | 0 | Automatically evaluating dialogue coherence is a challenging but high-demand ability for developing high-quality open-domain dialogue systems. However, current evaluation metrics consider only surface features or utterance-level semantics, without explicitly considering the fine-grained topic... | Lishan Huang, Zheng Ye, Jinghui Qin, Liang Lin, Xiaodan Liang |  |
| 1227 |  |  [MedDialog: Large-scale Medical Dialogue Datasets](https://doi.org/10.18653/v1/2020.emnlp-main.743) |  | 0 | Medical dialogue systems are promising in assisting in telemedicine to increase access to healthcare services, improve the quality of patient care, and reduce medical costs. To facilitate the research and development of medical dialogue systems, we build large-scale medical dialogue datasets –... | Guangtao Zeng, Wenmian Yang, Zeqian Ju, Yue Yang, Sicheng Wang, Ruisi Zhang, Meng Zhou, Jiaqi Zeng, Xiangyu Dong, Ruoyu Zhang, Hongchao Fang, Penghui Zhu, Shu Chen, Pengtao Xie |  |
| 1228 |  |  [An information theoretic view on selecting linguistic probes](https://doi.org/10.18653/v1/2020.emnlp-main.744) |  | 0 | There is increasing interest in assessing the linguistic knowledge encoded in neural representations. A popular approach is to attach a diagnostic classifier – or ”probe” – to perform supervised classification from internal representations. However, how to select a good probe is in debate. Hewitt... | Zining Zhu, Frank Rudzicz |  |
| 1229 |  |  [With Little Power Comes Great Responsibility](https://doi.org/10.18653/v1/2020.emnlp-main.745) |  | 0 | Despite its importance to experimental design, statistical power (the probability that, given a real effect, an experiment will reject the null hypothesis) has largely been ignored by the NLP community. Underpowered experiments make it more difficult to discern the difference between statistical... | Dallas Card, Peter Henderson, Urvashi Khandelwal, Robin Jia, Kyle Mahowald, Dan Jurafsky |  |
| 1230 |  |  [Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics](https://doi.org/10.18653/v1/2020.emnlp-main.746) |  | 0 | Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps—a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the... | Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A. Smith, Yejin Choi |  |
| 1231 |  |  [Evaluating and Characterizing Human Rationales](https://doi.org/10.18653/v1/2020.emnlp-main.747) |  | 0 | Two main approaches for evaluating the quality of machine-generated rationales are: 1) using human rationales as a gold standard; and 2) automated metrics based on how rationales affect model behavior. An open question, however, is how human rationales fare with these automatic metrics. Analyzing a... | Samuel Carton, Anirudh Rathore, Chenhao Tan |  |
| 1232 |  |  [On Extractive and Abstractive Neural Document Summarization with Transformer Language Models](https://doi.org/10.18653/v1/2020.emnlp-main.748) |  | 0 | We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information... | Jonathan Pilault, Raymond Li, Sandeep Subramanian, Chris Pal |  |
| 1233 |  |  [Multi-Fact Correction in Abstractive Text Summarization](https://doi.org/10.18653/v1/2020.emnlp-main.749) |  | 0 | Pre-trained neural abstractive summarization systems have dominated extractive strategies on news summarization performance, at least in terms of ROUGE. However, system-generated abstractive summaries often face the pitfall of factual inconsistency: generating incorrect facts with respect to the... | Yue Dong, Shuohang Wang, Zhe Gan, Yu Cheng, Jackie Chi Kit Cheung, Jingjing Liu |  |
| 1234 |  |  [Evaluating the Factual Consistency of Abstractive Text Summarization](https://doi.org/10.18653/v1/2020.emnlp-main.750) |  | 0 | The most common metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and generated... | Wojciech Kryscinski, Bryan McCann, Caiming Xiong, Richard Socher |  |
| 1235 |  |  [Re-evaluating Evaluation in Text Summarization](https://doi.org/10.18653/v1/2020.emnlp-main.751) |  | 0 | Automated evaluation metrics as a stand-in for manual evaluation are an essential part of the development of text-generation tasks such as text summarization. However, while the field has progressed, our standard metrics have not – for nearly 20 years ROUGE has been the standard evaluation in most... | Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei Liu, Graham Neubig |  |
| 1236 |  |  [VMSMO: Learning to Generate Multimodal Summary for Video-based News Articles](https://doi.org/10.18653/v1/2020.emnlp-main.752) |  | 0 | A popular multimedia news format nowadays is providing users with a lively video and a corresponding news article, which is employed by influential news media including CNN, BBC, and social media including Twitter and Weibo. In such a case, automatically choosing a proper cover frame of the video... | Mingzhe Li, Xiuying Chen, Shen Gao, Zhangming Chan, Dongyan Zhao, Rui Yan |  |
